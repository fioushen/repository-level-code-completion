{"filename": "train_deephase.py", "chunked_list": ["import os\n\tfrom argparse import ArgumentParser\n\timport pytorch_lightning as pl\n\tfrom pytorch_lightning import Trainer\n\tfrom pytorch_lightning import loggers\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\tfrom pytorch_lightning.profiler import SimpleProfiler\n\tfrom pytorch_lightning.utilities.seed import seed_everything\n\tfrom src.Datasets.DeepPhaseDataModule import Style100DataModule\n\tfrom src.Datasets.Style100Processor import StyleLoader, Swap100StyJoints\n", "from src.Net.DeepPhaseNet import DeepPhaseNet, Application\n\tfrom src.utils import BVH_mod as BVH\n\tfrom src.utils.locate_model import locate_model\n\tfrom src.utils.motion_process import subsample\n\t#from src.Datasets.DataSetProperty import lafan1_property,cmu_property\n\tdef setup_seed(seed:int):\n\t    seed_everything(seed,True)\n\tdef test_model():\n\t    dict = {}\n\t    dict['limit_train_batches'] = 1.\n", "    dict['limit_val_batches'] = 1.\n\t    return dict\n\tdef detect_nan_par():\n\t    '''track_grad_norm\": 'inf'''\n\t    return { \"detect_anomaly\":True}\n\tdef select_gpu_par():\n\t    return {\"accelerator\":'gpu', \"auto_select_gpus\":True, \"devices\":-1}\n\tdef create_common_states(prefix:str):\n\t    log_name = prefix+'/'\n\t    '''test upload'''\n", "    parser = ArgumentParser()\n\t    parser.add_argument(\"--dev_run\", action=\"store_true\")\n\t    parser.add_argument(\"--version\", type=str, default=\"-1\")\n\t    parser.add_argument(\"--resume\", action=\"store_true\")\n\t    parser.add_argument(\"--n_phases\",type=int,default=10)\n\t    parser.add_argument(\"--epoch\",type=str,default = '')\n\t    parser.add_argument(\"--test\",action=\"store_true\")\n\t    args = parser.parse_args()\n\t    ckpt_path = \"results/\"\n\t    if (args.version != \"-1\"):\n", "        version = args.version\n\t    else:\n\t        version = None\n\t    '''Create Loggers tensorboard'''\n\t    if args.dev_run:\n\t        log_name += \"dev_run\"\n\t    else:\n\t        log_name += \"myResults\"\n\t    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"tensorboard_logs/\", name=log_name, version=version)\n\t    ckpt_path = os.path.join(ckpt_path, log_name, str(tb_logger.version))\n", "    if (args.resume == True):\n\t        resume_from_checkpoint = os.path.join(os.path.join(ckpt_path, \"last.ckpt\"))  # results/version/last.ckpt\"\n\t    else:\n\t        resume_from_checkpoint = None\n\t    checkpoint_callback = [ModelCheckpoint(dirpath=ckpt_path + \"/\", save_top_k=-1, save_last=True, every_n_epochs=1,save_weights_only=True),\n\t                           ModelCheckpoint(dirpath=ckpt_path + \"/\", save_top_k=1, monitor=\"val_loss\", save_last=False, every_n_epochs=1)]\n\t    '''Train'''\n\t    checkpoint_callback[0].CHECKPOINT_NAME_LAST = \"last\"\n\t    profiler = SimpleProfiler()\n\t    trainer_dict = {\n", "        \"callbacks\":checkpoint_callback,\n\t        \"profiler\":profiler,\n\t        \"logger\":tb_logger\n\t    }\n\t    return args,trainer_dict,resume_from_checkpoint,ckpt_path\n\tdef read_style_bvh(style,content,clip=None):\n\t    swap_joints = Swap100StyJoints()\n\t    anim = BVH.read_bvh(os.path.join(\"MotionData/100STYLE/\",style,style+\"_\"+content+\".bvh\"),remove_joints=swap_joints)\n\t    if (clip != None):\n\t        anim.quats = anim.quats[clip[0]:clip[1], ...]\n", "        anim.hip_pos = anim.hip_pos[clip[0]:clip[1], ...]\n\t    anim = subsample(anim,ratio=2)\n\t    return anim\n\tdef training_style100():\n\t    args, trainer_dict, resume_from_checkpoint, ckpt_path = create_common_states(\"deephase_sty\")\n\t    '''Create the model'''\n\t    frequency = 30\n\t    window = 61\n\t    style_loader = StyleLoader()\n\t    batch_size = 32\n", "    data_module = Style100DataModule( batch_size=batch_size,shuffle=True,data_loader=style_loader,window_size=window)\n\t    model = DeepPhaseNet(args.n_phases, data_module.skeleton, window, 1.0 / frequency,batch_size=batch_size)  # or model = pl.LightningModule().load_from_checkpoint(PATH)\n\t    if (args.test == False):\n\t        if (args.dev_run):\n\t            trainer = Trainer(**trainer_dict, **test_model(),\n\t                              **select_gpu_par(), precision=32,\n\t                              log_every_n_steps=50, flush_logs_every_n_steps=500, max_epochs=30,\n\t                              weights_summary='full', auto_lr_find=True)\n\t        else:\n\t            trainer = Trainer(**trainer_dict, max_epochs=500, **select_gpu_par(), log_every_n_steps=50,#limit_train_batches=0.1,\n", "                              flush_logs_every_n_steps=500, resume_from_checkpoint=resume_from_checkpoint)\n\t        trainer.fit(model, datamodule=data_module)\n\t    # trainer.test(ckpt_path='best')\n\t    else:\n\t        anim = read_style_bvh(\"WildArms\", \"FW\",[509,1009])\n\t        check_file = ckpt_path + \"/\"\n\t        modelfile = locate_model(check_file, args.epoch)\n\t        model = DeepPhaseNet.load_from_checkpoint(modelfile)\n\t        model = model.cuda()\n\t        data_module.setup()\n", "        app = Application(model, data_module)\n\t        app = app.float()\n\t        anim = subsample(anim, 1)\n\t        app.setAnim(anim)\n\t        app.forward()\n\t        BVH.save_bvh(\"source.bvh\",anim)\n\tdef readBVH(filename,dataset_property):\n\t    remove_joints = (dataset_property['remove_joints'])\n\t    if (remove_joints != None):\n\t        remove_joints = remove_joints()\n", "    filename = dataset_property[\"test_path\"] + filename\n\t    return BVH.read_bvh(filename, remove_joints=remove_joints, Tpose=-1, remove_gap=dataset_property['remove_gap'])\n\tif __name__ == '__main__':\n\t    setup_seed(3407)\n\t    training_style100()\n"]}
{"filename": "train_styleVAE.py", "chunked_list": ["#import argparse\n\timport copy\n\timport os\n\timport re\n\tfrom argparse import ArgumentParser\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom pytorch_lightning import Trainer\n\tfrom pytorch_lightning import loggers\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n", "from pytorch_lightning.profiler import SimpleProfiler\n\tfrom pytorch_lightning.utilities.seed import seed_everything\n\tfrom src.Datasets.BaseLoader import WindowBasedLoader\n\tfrom src.Net.StyleVAENet import StyleVAENet\n\tfrom src.utils import BVH_mod as BVH\n\tdef setup_seed(seed:int):\n\t    seed_everything(seed,True)\n\tdef test_model():\n\t    dict = {}\n\t    #dict['fast_dev_run'] = 1 # only run 1 train, val, test batch and program ends\n", "    dict['limit_train_batches'] = 0.1\n\t    dict['limit_val_batches'] = 0.7\n\t    return dict\n\tdef detect_nan_par():\n\t    '''track_grad_norm\": 'inf'''\n\t    return { \"detect_anomaly\":True}\n\tdef select_gpu_par():\n\t    return {\"accelerator\":'gpu', \"auto_select_gpus\":True, \"devices\":-1}\n\tdef create_common_states(prefix:str):\n\t    log_name = prefix+'/'\n", "    '''test upload'''\n\t    parser = ArgumentParser()\n\t    parser.add_argument(\"--dev_run\", action=\"store_true\")\n\t    parser.add_argument(\"--version\", type=str, default=\"-1\")\n\t    parser.add_argument(\"--epoch\",type=str,default=\"last\")\n\t    parser.add_argument(\"--resume\", action=\"store_true\")\n\t    parser.add_argument(\"--test\",action=\"store_true\")\n\t    args = parser.parse_args()\n\t    ckpt_path_prefix = \"results/\"\n\t    if (args.version != \"-1\"):\n", "        version = args.version\n\t    else:\n\t        version = None\n\t    '''Create Loggers tensorboard'''\n\t    if args.dev_run:\n\t        log_name += \"dev_run\"\n\t    else:\n\t        log_name += \"myResults\"\n\t    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"tensorboard_logs/\", name=log_name, version=None)\n\t    load_ckpt_path = os.path.join(ckpt_path_prefix, prefix+'/myResults', str(version))\n", "    save_ckpt_path = os.path.join(ckpt_path_prefix, log_name, str(tb_logger.version))\n\t    if (args.resume == True):\n\t        check_file = load_ckpt_path+\"/\"\n\t        if (args.epoch == \"last\"):\n\t            check_file += \"last.ckpt\"\n\t        else:\n\t            dirs = os.listdir(check_file)\n\t            for dir in dirs:\n\t                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n\t                out = re.findall(st, dir)\n", "                if (len(out) > 0):\n\t                    check_file += out[0]\n\t                    print(check_file)\n\t                    break\n\t        resume_from_checkpoint = check_file  # results/version/last.ckpt\"\n\t    else:\n\t        resume_from_checkpoint = None\n\t    checkpoint_callback = [ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=-1, save_last=True, every_n_epochs=5),\n\t                           ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=1, monitor=\"val_loss\", save_last=False, every_n_epochs=1,save_weights_only=True),\n\t                          # EMA(0.99)\n", "                           ]\n\t    '''Train'''\n\t    checkpoint_callback[0].CHECKPOINT_NAME_LAST = \"last\"\n\t    profiler = SimpleProfiler()#PyTorchProfiler(filename=\"profiler\")\n\t    trainer_dict = {\n\t        \"callbacks\":checkpoint_callback,\n\t        \"profiler\":profiler,\n\t        \"logger\":tb_logger\n\t    }\n\t    return args,trainer_dict,resume_from_checkpoint,load_ckpt_path\n", "def training_style100():\n\t    from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n\t    from src.Datasets.Style100Processor import StyleLoader\n\t    from src.Net.StyleVAENet import Application,VAEMode\n\t    prefix = \"StyleVAE2\"\n\t    data_set = \"style100\"\n\t    prefix += \"_\" + data_set\n\t    args, trainer_dict, resume_from_checkpoint, ckpt_path = create_common_states(prefix)\n\t    resume_from_checkpoint = None\n\t    loader = WindowBasedLoader(61, 21, 1)\n", "    dt = 1. / 30.\n\t    phase_dim = 10\n\t    phase_file = \"+phase_gv10\"\n\t    latent_size = 32\n\t    net_mode = VAEMode.SINGLE\n\t    batch_size = 32\n\t    if (args.test == False):\n\t        '''Create the model'''\n\t        style_loader = StyleLoader()\n\t        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name=None, dt=dt, batch_size=batch_size, mirror=0.0)  # when apply phase, should avoid mirror\n", "        model = StyleVAENet(data_module.skeleton,  phase_dim=phase_dim, latent_size=latent_size,batch_size=batch_size,mode='pretrain',net_mode=net_mode)\n\t        if (args.dev_run):\n\t            trainer = Trainer(**trainer_dict, **test_model(),\n\t                              **select_gpu_par(), precision=32, reload_dataloaders_every_n_epochs=1,#gradient_clip_val=1.0,#**detect_nan_par(),\n\t                              log_every_n_steps=5, flush_logs_every_n_steps=10,\n\t                              weights_summary='full')\n\t        else:\n\t            trainer = Trainer(**trainer_dict, max_epochs=10000, reload_dataloaders_every_n_epochs=1,gradient_clip_val=1.0,#**detect_nan_par(),\n\t                              **select_gpu_par(), log_every_n_steps=50,\n\t                              flush_logs_every_n_steps=100)\n", "        trainer.fit(model, datamodule=data_module)\n\t    else:\n\t        style_loader = StyleLoader()\n\t        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),None, dt=dt, batch_size=batch_size, mirror=0.0)\n\t        data_module.setup()\n\t        check_file = ckpt_path + \"/\"\n\t        if (args.epoch == \"last\"):\n\t            check_file += \"last.ckpt\"\n\t            print(check_file)\n\t        else:\n", "            dirs = os.listdir(check_file)\n\t            for dir in dirs:\n\t                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n\t                out = re.findall(st, dir)\n\t                if (len(out) > 0):\n\t                    check_file += out[0]\n\t                    print(check_file)\n\t                    break\n\t        model = StyleVAENet.load_from_checkpoint(check_file, moe_decoder=None,pose_channels=6,net_mode=net_mode,strict=False)\n\t        model = model.cuda()\n", "        src_motion = data_module.test_set.dataset[\"HighKnees\"][0]\n\t        source = BVH.read_bvh(\"source.bvh\")\n\t        '''check if space can produce netural space: encoding=False, style=kick'''\n\t        data_module.mirror = 0\n\t        model = model.cpu()\n\t        model.eval()\n\t        app = Application(model, data_module)\n\t        app = app.float()\n\t        app.setSource(src_motion)\n\t        output = copy.deepcopy(source)\n", "        output.hip_pos, output.quats = app.forward(seed=3000,encoding=True)\n\t        BVH.save_bvh(\"test_net.bvh\", output)\n\t        source.hip_pos, source.quats = app.get_source()\n\t        BVH.save_bvh(\"source.bvh\", source)\n\t        torch.save(model, ckpt_path + \"/m_save_model_\" + str(args.epoch))\n\tif __name__ == '__main__':\n\t    setup_seed(3407)\n\t    training_style100()\n"]}
{"filename": "add_phase_to_dataset.py", "chunked_list": ["import torch\n\timport src.Datasets.BaseLoader as mBaseLoader\n\tfrom src.Datasets.DeepPhaseDataModule import DeephaseDataSet, Style100DataModule\n\tfrom src.Datasets.Style100Processor import StyleLoader\n\tfrom src.Net.DeepPhaseNet import Application\n\tfrom src.Net.DeepPhaseNet import DeepPhaseNet\n\tclass PhaseMotionStyle100Processor(mBaseLoader.BasedDataProcessor):\n\t    def __init__(self,window,dt,model_path:str):\n\t        from src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\n\t        super(PhaseMotionStyle100Processor, self).__init__()\n", "        self.processor = DeepPhaseProcessor(dt)\n\t        #self.processor = DeepPhaseProcessorPCA(dt)\n\t        #self.attribute = 'pos'#'gv'\n\t        self.window = window\n\t        self.model = DeepPhaseNet.load_from_checkpoint(model_path,style_loader=None)\n\t    def __call__(self, dict,skeleton,motion_datalaoder= None):\n\t        offsets, hip_pos, quats = dict[\"offsets\"],dict[\"hip_pos\"],dict[\"quats\"]\n\t        style_loader = StyleLoader()\n\t        data_module = Style100DataModule(batch_size=32, shuffle=True, data_loader=style_loader, window_size=self.window)\n\t        # data_module.setup()# load std\n", "        #stat = style_loader.load_part_to_binary(\"deepphase_vp_statistics\")\n\t        app = Application(self.model, data_module)\n\t        self.app = app.float()\n\t        gv = self.processor(dict,skeleton,style_loader)['gv']\n\t        gv = torch.from_numpy(gv).cuda()\n\t        phase = {key:[] for key in [\"A\",\"S\",\"B\",\"F\"]}\n\t        h=[]\n\t        q=[]\n\t        o=[]\n\t        for i in range(len(offsets)):\n", "            print(\"{} in {},length:{}\".format(i,len(offsets),hip_pos[i].shape[0]))\n\t            if(hip_pos[i].shape[0]<=self.window): #gv = hip_pos[i].shape[0]-1\n\t                continue\n\t            dataset = DeephaseDataSet([gv[i]], self.window)\n\t            print(\"dataset length: {}\".format(len(dataset)))\n\t            if(len(dataset)==0):\n\t                continue\n\t            self.app.Net.to(\"cuda\")\n\t            phases = self.app.calculate_statistic_for_dataset(dataset)\n\t            key_frame = self.window // 2   # 61th or 31th,\n", "            use_pos= False\n\t            if(use_pos):\n\t                clip = lambda x:x[key_frame:-key_frame]\n\t            else:\n\t                '''gv的第60帧实际上是第61帧减60，我们应该保留第61帧'''\n\t                clip = lambda x: x[key_frame+1:-key_frame+1]\n\t            # o[i] = clip(o[i])\n\t            o.append(offsets[i])\n\t            h.append(clip(hip_pos[i]))\n\t            q.append(clip(quats[i]))\n", "            #offsets[i]=None\n\t            #hip_pos[i]=None\n\t            #quats[i]=None\n\t            for key in phases:\n\t                phase[key].append(phases[key])\n\t        return {\"offsets\": o, \"hip_pos\": h, \"quats\": q, **phase}\n\tdef add_phase_to_100Style(info):\n\t    phase_processor = PhaseMotionStyle100Processor(info[\"window\"], info['dt'], info[\"model_path\"])\n\t    bloader = mBaseLoader.StreamBasedLoader(1)\n\t    style_loader = StyleLoader()\n", "    style_loader.setup(bloader,mBaseLoader.BasedDataProcessor())\n\t    style_loader.process_from_binary()\n\t    def add_phase(motions):\n\t        for style in motions.keys():\n\t            print(style+\"----------\")\n\t            for content in motions[style].keys():\n\t                print(content)\n\t                motions[style][content] = phase_processor(motions[style][content],style_loader.skeleton)\n\t        return motions\n\t    style_loader.train_motions = add_phase(style_loader.train_motions)\n", "    style_loader.test_motions = add_phase(style_loader.test_motions)\n\t    style_loader.save_dataset(\"+phase_gv10\")\n\t    # style_loader.process_from_binary(argument=False)\n\t    # style_loader.train_motions = add_phase(style_loader.train_motions)\n\t    # style_loader.test_motions = add_phase(style_loader.test_motions)\n\t    # style_loader.save_dataset(\"no_augement+phase_gv10\")"]}
{"filename": "process_dataset.py", "chunked_list": ["import os\n\timport src.Datasets.BaseLoader as mBaseLoader\n\tfrom src.Datasets.BatchProcessor import BatchRotateYCenterXZ\n\timport torch\n\timport numpy as np\n\timport random\n\tfrom src.Datasets.Style100Processor import StyleLoader,Swap100StyJoints,bvh_to_binary,save_skeleton\n\timport src.utils.BVH_mod as BVH\n\tfrom src.utils.motion_process import subsample\n\tfrom src.Datasets.BaseLoader import BasedDataProcessor,BasedLoader,DataSetType,MotionDataLoader,WindowBasedLoader\n", "class TransitionProcessor(BasedDataProcessor):\n\t    def __init__(self,ref_id):\n\t        super(TransitionProcessor, self).__init__()\n\t        self.process = BatchRotateYCenterXZ()\n\t        self.ref_id = ref_id\n\t    def __call__(self, dict, skeleton,motionDataLoader,ratio=1.0):\n\t        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n\t        stat = self._calculate_stat(offsets,hip_pos,quats,skeleton,ratio)\n\t        return {\"offsets\":offsets,\"hip_pos\":hip_pos,\"quats\":quats,**stat}\n\t    def _concat(self,local_quat,offsets,hip_pos,ratio=0.2):\n", "        if(ratio>=1.0):\n\t            local_quat = torch.from_numpy(np.concatenate(local_quat, axis=0))\n\t            offsets = torch.from_numpy(np.concatenate((offsets), axis=0))\n\t            hip_pos = torch.from_numpy(np.concatenate((hip_pos), axis=0))\n\t        else:\n\t            length = int(len(local_quat)*ratio)+1\n\t            idx = []\n\t            for i in range(length):\n\t                idx.append(random.randrange(0,len(local_quat)))\n\t            sample = lambda x:[x[i] for i in idx]\n", "            local_quat = torch.from_numpy(np.concatenate(sample(local_quat), axis=0))\n\t            offsets = torch.from_numpy(np.concatenate(sample(offsets), axis=0))\n\t            hip_pos =torch.from_numpy(np.concatenate(sample(hip_pos), axis=0))\n\t        return local_quat,offsets,hip_pos\n\t    def calculate_pos_statistic(self, pos):\n\t        '''pos:N,T,J,3'''\n\t        mean = np.mean(pos, axis=(0, 1))\n\t        std = np.std(pos, axis=(0, 1))\n\t        std = np.mean(std)\n\t        return mean, std\n", "    def calculate_rotation_mean(self,rotation):\n\t        mean = np.mean(rotation,axis=(0,1))\n\t        std = np.std(rotation,axis=(0,1))\n\t        std = np.mean(std)\n\t        return mean ,std\n\t    def calculate_statistic(self, local_pos,local_rot):\n\t        pos_mean, pos_std = self.calculate_pos_statistic(local_pos[:,:,1:,:].cpu().numpy())\n\t        vel_mean, vel_std = self.calculate_pos_statistic((local_pos[:, 1:, 1:, :] - local_pos[:, :-1, 1:, :]).cpu().numpy())\n\t        hipv_mean, hipv_std = self.calculate_pos_statistic((local_pos[:, 1:, 0:1, :] - local_pos[:, :-1, 0:1, :]).cpu().numpy())\n\t        rot_mean,rot_std = self.calculate_rotation_mean(local_rot[:,:,1:,:].cpu().numpy())\n", "        rotv_mean,rotv_std = self.calculate_rotation_mean((local_rot[:,1:,1:,:]-local_rot[:,:-1,1:,:]).cpu().numpy())\n\t        hipr_mean, hipr_std = self.calculate_rotation_mean(local_rot[:, :, 0:1, :].cpu().numpy())\n\t        hiprv_mean, hiprv_std = self.calculate_rotation_mean((local_rot[:, 1:, 0:1, :] - local_rot[:, :-1, 0:1, :]).cpu().numpy())\n\t        return {\"pos_stat\": [pos_mean, pos_std], \"rot_stat\":[rot_mean,rot_std],\"vel_stat\":[vel_mean,vel_std],\n\t                \"rotv_stat\":[rotv_mean,rotv_std],\"hipv_stat\":[hipv_mean,hipv_std],\"hipr_stat\":[hipr_mean,hipr_std],\"hiprv_stat\":[hiprv_mean,hiprv_std]}\n\t    def _calculate_stat(self,offsets,hip_pos,local_quat,skeleton,ratio):\n\t        local_quat, offsets, hip_pos = self._concat(local_quat,offsets,hip_pos,ratio)\n\t        global_positions, global_rotations = skeleton.forward_kinematics(local_quat, offsets, hip_pos)\n\t        local_pos,local_rot = self.process(global_positions,local_quat, self.ref_id)\n\t        return self.calculate_statistic(local_pos,local_rot)\n", "def read_style_bvh(style,content,clip=None):\n\t    swap_joints = Swap100StyJoints()\n\t    anim = BVH.read_bvh(os.path.join(\"MotionData/100STYLE/\",style,style+\"_\"+content+\".bvh\"),remove_joints=swap_joints)\n\t    if (clip != None):\n\t        anim.quats = anim.quats[clip[0]:clip[1], ...]\n\t        anim.hip_pos = anim.hip_pos[clip[0]:clip[1], ...]\n\t    anim = subsample(anim,ratio=2)\n\t    return anim\n\tdef processStyle100Benchmark( window, overlap):\n\t    style_loader = StyleLoader()\n", "    processor = None\n\t    bloader = mBaseLoader.WindowBasedLoader(window=window, overlap=overlap, subsample=1)\n\t    style_loader.setup(bloader, processor)\n\t    style_loader.load_dataset(\"+phase_gv10\")\n\t    def split_window(motions):\n\t        for style in motions.keys():\n\t            styles = []\n\t            # print(style)\n\t            if len(motions[style].keys()):\n\t                dict = motions[style].copy()\n", "            for content in motions[style].keys():\n\t                motions[style][content] = bloader.append_dicts(motions[style][content])\n\t            for content in dict.keys():\n\t                if dict[content]['hip_pos'][0].shape[0]>=120:\n\t                    o = dict[content]['offsets'][0]\n\t                    h = dict[content]['hip_pos'][0][0:120]\n\t                    q = dict[content]['quats'][0][0:120]\n\t                    styles.append({\"offsets\": o, \"hip_pos\": h, \"quats\": q})\n\t            motions[style]['style'] = styles\n\t        result = {}\n", "        for style_name in motions.keys():\n\t            # print(motions.keys())\n\t            o, h, q, a, s, b, f = [], [], [], [], [], [], []\n\t            for content_name in motions[style_name]:\n\t                if content_name == 'style':\n\t                    continue\n\t                dict = motions[style_name][content_name]\n\t                o += dict['offsets']\n\t                h += dict['hip_pos']\n\t                q += dict['quats']\n", "                a += dict['A']\n\t                s += dict['S']\n\t                b += dict['B']\n\t                f += dict['F']\n\t            # i += 1\n\t            style = motions[style_name]['style']\n\t            motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f}\n\t            result[style_name] = {\"motion\":motion, \"style\":style}\n\t        return result\n\t    style_loader.test_dict = split_window(style_loader.test_motions)\n", "    style_loader.save_to_binary(\"style100_benchmark_65_25\", style_loader.test_dict)\n\tdef processTransitionPhaseDatasetForStyle100(window,overlap):\n\t    style_loader = StyleLoader()\n\t    window_loader = mBaseLoader.WindowBasedLoader(window, overlap, 1)\n\t    processor = None #MotionPuzzleProcessor()\n\t    style_loader.setup(window_loader, processor)\n\t    style_loader.load_dataset(\"+phase_gv10\")\n\t    def split_window(motions):\n\t        #motions = style_loader.all_motions\n\t        for style in motions.keys():\n", "            for content in motions[style].keys():\n\t                motions[style][content] = window_loader.append_dicts(motions[style][content])\n\t        return motions\n\t    style_loader.train_motions = split_window(style_loader.train_motions)\n\t    style_loader.test_motions = split_window(style_loader.test_motions)\n\t    #style_loader.save_part_to_binary(\"motionpuzzle_statistics\", [\"pos_stat\", \"vel_stat\", \"rot_stat\"])\n\t    style_loader.save_dataset(\"+phase_gv10\" + window_loader.get_postfix_str())\n\t    print()\n\tdef processDeepPhaseForStyle100(window,overlap):\n\t    from src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\n", "    style_loader = StyleLoader()\n\t    window_loader = mBaseLoader.WindowBasedLoader(window,overlap,1)\n\t    processor = DeepPhaseProcessor(1./30)\n\t    style_loader.setup(window_loader,processor)\n\t    style_loader.process_from_binary()\n\t    style_loader.save_train_test_dataset(\"deep_phase_gv\")\n\tdef splitStyle100TrainTestSet():\n\t    style_loader = StyleLoader()\n\t    print(\"Divide the data set to train set and test set\")\n\t    style_loader.split_from_binary()\n", "    print(\"argument datasets\")\n\t    style_loader.augment_dataset()\n\t    print(\"down\")\n\tif __name__ == '__main__':\n\t    from argparse import ArgumentParser\n\t    parser = ArgumentParser()\n\t    parser.add_argument(\"--preprocess\", action=\"store_true\")\n\t    parser.add_argument(\"--train_phase_model\", action=\"store_true\")\n\t    parser.add_argument(\"--add_phase_to_dataset\", action=\"store_true\")\n\t    parser.add_argument(\"--model_path\",type=str,default=\"./results/deephase_sty/myResults/31/epoch=161-step=383778-v1.ckpt\")\n", "    parser.add_argument(\"--train_manifold_model\", action=\"store_true\")\n\t    parser.add_argument(\"--train_sampler_model\", action=\"store_true\")\n\t    parser.add_argument(\"--benchmarks\", action=\"store_true\")\n\t    args = parser.parse_args()\n\t    if(args.preprocess==True):\n\t        print(\"######## convert all bvh files to binary files################\")\n\t        bvh_to_binary()\n\t        save_skeleton()\n\t        print(\"\\nConvert down\\n\")\n\t        print(\"Divide the dataset to train set and test set, and then argument datasets.\")\n", "        splitStyle100TrainTestSet()\n\t    elif(args.train_phase_model==True):\n\t        processDeepPhaseForStyle100(62,2)\n\t    elif(args.add_phase_to_dataset==True):\n\t        from add_phase_to_dataset import add_phase_to_100Style\n\t        style100_info = {\n\t            \"model_path\":args.model_path,\n\t            \"dt\": 1. / 30,\n\t            \"window\": 61\n\t        }\n", "        add_phase_to_100Style(style100_info)\n\t    elif(args.train_manifold_model==True):\n\t        processTransitionPhaseDatasetForStyle100(61,21)\n\t    elif(args.train_sampler_model==True):\n\t        processTransitionPhaseDatasetForStyle100(120,0)\n\t    elif(args.benchmarks==True):\n\t        processStyle100Benchmark(65,25)\n"]}
{"filename": "Running_LongSeq.py", "chunked_list": ["import torch\n\tfrom pytorch3d.transforms import quaternion_apply, quaternion_multiply, quaternion_invert\n\tfrom src.Datasets.Style100Processor import StyleLoader\n\tfrom src.geometry.quaternions import or6d_to_quat, quat_to_or6D, from_to_1_0_0\n\tfrom src.utils import BVH_mod as BVH\n\tfrom src.utils.BVH_mod import Skeleton, find_secondary_axis\n\tdef load_model():\n\t    model = torch.load('./results/Transitionv2_style100/myResults/141/m_save_model_198')\n\t    return model\n\tdef load_dataSet():\n", "    loader = StyleLoader()\n\t    loader.load_dataset(\"+phase_gv10\")\n\t    return loader\n\tclass BatchRotateYCenterXZ(torch.nn.Module):\n\t    def __init__(self):\n\t        super(BatchRotateYCenterXZ, self).__init__()\n\t    def forward(self,global_positions,global_quats,ref_frame_id):\n\t        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n\t                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),dim=-1)\n\t        root_rotation = from_to_1_0_0(ref_vector)\n", "        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n\t        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n\t        global_positions = quaternion_apply(root_rotation, global_positions)\n\t        global_quats = quaternion_multiply(root_rotation, global_quats)\n\t        \"\"\" Local Space \"\"\"\n\t        return global_positions,global_quats, ref_hip,root_rotation\n\tclass TransformSeq():\n\t    def __init__(self):\n\t        self.trans = BatchRotateYCenterXZ()\n\t        pass\n", "    def transform(self,glb_pos,glb_quat):\n\t        pos,quats, ref_hip,root_rotation = self.trans(glb_pos,glb_quat,9)\n\t        self.ref_hip = ref_hip\n\t        self.inv_rot = quaternion_invert(root_rotation)\n\t        return pos,quats\n\t    def un_transform(self,glb_pos,glb_quat):\n\t        glb_pos = quaternion_apply(self.inv_rot,glb_pos)\n\t        glb_rot = quaternion_multiply(self.inv_rot,glb_quat)\n\t        glb_pos[...,[0,2]] = glb_pos[...,[0,2]]+self.ref_hip\n\t        return glb_pos,glb_rot\n", "class RunningLongSeq():\n\t    def __init__(self,model,X,Q,A,S,tar_X,tar_Q,pos_offset,skeleton):\n\t        self.window = 60\n\t        self.source_idx = 0\n\t        self.out_idx = 0\n\t        self.X = X.cuda()\n\t        self.Q = Q.cuda()\n\t        self.pos_offset = pos_offset.cuda()\n\t        self.tar_pos, self.tar_rot = skeleton.forward_kinematics(tar_Q[:, :120].cuda(),  self.pos_offset, tar_X[:, :120].cuda())\n\t        self.skeleton = skeleton\n", "        self.transform = TransformSeq()\n\t        self.model = model.cuda()\n\t        self.phases = model.phase_op.phaseManifold(A, S)\n\t        self.outX = X[:,:10].cuda()\n\t        self.outQ = Q[:,:10].cuda()\n\t        self.outPhase = self.phases[:,:10].cuda()\n\t        tar_id = [50,90,130,170,210,250,290,330]\n\t        self.time = [40,40,40,40,40,80,40,40]\n\t        get_tar_property = lambda property: [property[:,tar_id[i]-2:tar_id[i]] for i in range(len(tar_id))]\n\t        self.X_tar = get_tar_property(self.X)\n", "        self.Q_tar = get_tar_property(self.Q)\n\t        self.X_tar[3][:, :, :, [0, 2]] = self.X_tar[2][:, :, :, [0]] + 20\n\t        self.tar_id = 0\n\t        pass\n\t    def next_seq(self,length):\n\t        X = self.outX[:,self.out_idx:self.out_idx+10]\n\t        Q = self.outQ[:,self.out_idx:self.out_idx+10]\n\t        phases = self.phases[:,self.out_idx:self.out_idx+10]#self.outPhase[:,self.out_idx:self.out_idx+10]\n\t        X_tar = self.X_tar[self.tar_id]#torch.cat((self.X_tar[self.tar_id]),dim=1)\n\t        Q_tar = self.Q_tar[self.tar_id]#torch.cat((self.Q_tar[self.tar_id]),dim=1)\n", "        X = torch.cat((X,X_tar),dim=1)\n\t        Q = torch.cat((Q,Q_tar),dim=1)\n\t        gp,gq = self.skeleton.forward_kinematics(Q,self.pos_offset,X)\n\t        gp,gq = self.transform.transform(gp,gq)\n\t        gq,gp,pred_phase = synthesize(self.model,gp,gq,phases,self.tar_pos,self.tar_rot,self.pos_offset,self.skeleton,length,12)\n\t        gp,gq = self.transform.un_transform(gp,gq)\n\t        X,Q = self.skeleton.inverse_kinematics(gq,gp)\n\t        self.outX = torch.cat((self.outX,X),dim=1)\n\t        self.outQ = torch.cat((self.outQ,Q),dim=1)\n\t        self.outPhase = torch.cat((self.outPhase,pred_phase),dim=1)\n", "        self.out_idx += length\n\t        self.tar_id+=1\n\t    def iteration(self):\n\t      #  time = [40,40,40,40,40,80,40]\n\t        for i in range(len(self.X_tar)):\n\t            self.next_seq(self.time[i])\n\t    def get_source(self):\n\t        return self.X.cpu().squeeze(0).numpy()[10:],self.Q.cpu().squeeze(0).numpy()[10:]\n\t    def get_results(self):\n\t        return self.outX.cpu().squeeze(0).numpy()[10:],self.outQ.cpu().squeeze(0).numpy()[10:]\n", "    def get_target(self):\n\t        Xs = []\n\t        Qs = []\n\t        for i in range(len(self.X_tar)):\n\t            X = self.X_tar[i][0,1:]\n\t            Q = self.Q_tar[i][0,1:]\n\t            X = X.repeat(self.time[i],1,1)\n\t            Q = Q.repeat(self.time[i],1,1)\n\t            Xs.append(X)\n\t            Qs.append(Q)\n", "        Xs = torch.cat(Xs,dim=0)\n\t        Qs = torch.cat(Qs,dim=0)\n\t        return Xs.cpu().numpy(),Qs.cpu().numpy()\n\tdef synthesize(model, gp, gq, phases, tar_pos, tar_quat, pos_offset, skeleton: Skeleton, length, target_id, ifnoise=False):\n\t    model = model.eval()\n\t    model = model.cuda()\n\t    #quats = Q\n\t    offsets = pos_offset\n\t  #  hip_pos = X\n\t   # gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n", "    loc_rot = quat_to_or6D(gq)\n\t    if ifnoise:\n\t        noise = None\n\t    else:\n\t        noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device).cuda()\n\t    tar_quat = quat_to_or6D(tar_quat)\n\t    target_style = model.get_film_code(tar_pos.cuda(), tar_quat.cuda())  # use random style seq\n\t    # target_style = model.get_film_code(gp.cuda(), loc_rot.cuda())\n\t   # F = S[:, 1:] - S[:, :-1]\n\t  #  F = model.phase_op.remove_F_discontiny(F)\n", "   # F = F / model.phase_op.dt\n\t   # phases = model.phase_op.phaseManifold(A, S)\n\t    pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), None,\n\t                                                            None,\n\t                                                            target_style, noise, start_id=10, target_id=target_id,\n\t                                                            length=length, phase_schedule=1.)\n\t    pred_pos, pred_rot = pred_pos, pred_rot\n\t    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n\t    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n\t    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n", "    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n\t    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n\t    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :])\n\t    phases = pred_phase[3]\n\t    return GQ, GX, phases\n\tif __name__ ==\"__main__\":\n\t    model = load_model()\n\t    loader = load_dataSet()\n\t    anim = BVH.read_bvh(\"source.bvh\")\n\t    motions = loader.train_motions['LeftHop'][\"BR\"]\n", "    tar_motions = loader.train_motions['Neutral'][\"FR\"]\n\t    def extract_property(motions):\n\t        X = torch.from_numpy(motions['hip_pos'][0]).unsqueeze(0)\n\t        Q = torch.from_numpy(motions['quats'][0]).unsqueeze(0)\n\t        A = torch.from_numpy(motions['A'][0]).unsqueeze(0)/0.1\n\t        S = torch.from_numpy(motions['S'][0]).unsqueeze(0)\n\t        return X,Q,A,S\n\t    X, Q, A, S = extract_property(motions)\n\t    pos_offset = torch.from_numpy(motions['offsets'][0]).unsqueeze(0)\n\t    with torch.no_grad():\n", "        running_machine = RunningLongSeq(model,X,Q,A,S,X,Q,pos_offset,anim.skeleton)\n\t        running_machine.iteration()\n\t        anim.hip_pos,anim.quats = running_machine.get_source()\n\t        BVH.save_bvh(\"source.bvh\",anim)\n\t        anim.hip_pos,anim.quats = running_machine.get_results()\n\t        BVH.save_bvh(\"results.bvh\",anim)\n\t        anim.hip_pos, anim.quats = running_machine.get_target()\n\t        BVH.save_bvh(\"target.bvh\", anim)\n"]}
{"filename": "benchmark.py", "chunked_list": ["import time\n\timport pickle\n\tfrom src.Datasets.BatchProcessor import BatchProcessDatav2\n\tfrom src.geometry.quaternions import or6d_to_quat, quat_to_or6D, from_to_1_0_0\n\tfrom scipy import linalg\n\tfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply\n\tfrom torch.utils.data import DataLoader, Dataset\n\timport matplotlib.pyplot as plt\n\timport os\n\tfrom src.utils.BVH_mod import Skeleton, find_secondary_axis\n", "from src.utils.np_vector import interpolate_local, remove_quat_discontinuities\n\tfrom src.Datasets.Style100Processor import StyleLoader\n\timport src.Datasets.BaseLoader as mBaseLoader\n\timport torch\n\timport numpy as np\n\timport random\n\timport torch.nn.functional as F\n\tdef load_model(args):\n\t    model_dict, function_dict = {}, {}\n\t    model_dict[args.model_name] = torch.load(args.model_path)\n", "    if(hasattr(model_dict[args.model_name],\"predict_phase\")):\n\t        model_dict[args.model_name].predict_phase = False\n\t    model_dict[args.model_name].test = True\n\t    function_dict[args.model_name] = eval_sample\n\t    return model_dict, function_dict\n\tdef eval_sample(model, X, Q, A, S, tar_pos, tar_quat, pos_offset, skeleton: Skeleton, length, target_id, ifnoise=False):\n\t    model = model.eval()\n\t    model = model.cuda()\n\t    quats = Q\n\t    offsets = pos_offset\n", "    hip_pos = X\n\t    gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n\t    loc_rot = quat_to_or6D(gq)\n\t    if ifnoise:\n\t        noise = None\n\t    else:\n\t        noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device).cuda()\n\t    tar_quat = quat_to_or6D(tar_quat)\n\t    target_style = model.get_film_code(tar_pos.cuda(), tar_quat.cuda())   # use random style seq\n\t    # target_style = model.get_film_code(gp.cuda(), loc_rot.cuda())\n", "    F = S[:, 1:] - S[:, :-1]\n\t    F = model.phase_op.remove_F_discontiny(F)\n\t    F = F / model.phase_op.dt\n\t    phases = model.phase_op.phaseManifold(A, S)\n\t    if(hasattr(model,\"predict_phase\") and model.predict_phase):\n\t        pred_pos, pred_rot, pred_phase, _,_ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n\t                                                            F.cuda(),\n\t                                                            target_style, noise, start_id=10, target_id=target_id,\n\t                                                            length=length, phase_schedule=1.)\n\t    else:\n", "        pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n\t                                                            F.cuda(),\n\t                                                            target_style, noise, start_id=10, target_id=target_id,\n\t                                                            length=length, phase_schedule=1.)\n\t    pred_pos, pred_rot = pred_pos, pred_rot\n\t    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n\t    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n\t    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n\t    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n\t    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n", "    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :])\n\t    return GQ, GX\n\tdef renderplot(name, ylabel, lengths, res):\n\t    for key, l in res:\n\t        if l == lengths[0]:\n\t            result = [res[(key, n)] for n in lengths]\n\t            plt.plot(lengths, result, label=key)\n\t    plt.xlabel('Lengths')\n\t    plt.ylabel(ylabel)\n\t    plt.title(name)\n", "    plt.legend()\n\t    plt.savefig(name + '.png')\n\t    plt.close()\n\tdef calculate_fid(embeddings, gt_embeddings):\n\t    if type(embeddings) == torch.Tensor:\n\t        embeddings = embeddings.detach().cpu().numpy()\n\t        gt_embeddings = gt_embeddings.detach().cpu().numpy()\n\t    mu1 = np.mean(embeddings, axis=0)\n\t    sigma1 = np.cov(embeddings, rowvar=False)\n\t    mu2 = np.mean(gt_embeddings, axis=0)\n", "    sigma2 = np.cov(gt_embeddings, rowvar=False)\n\t    return calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n\tdef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n\t    \"\"\"Numpy implementation of the Frechet Distance.\n\t    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n\t    and X_2 ~ N(mu_2, C_2) is\n\t            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n\t    Stable version by Dougal J. Sutherland.\n\t    Params:\n\t    -- mu1   : Numpy array containing the activations of a layer of the\n", "               inception net (like returned by the function 'get_predictions')\n\t               for generated samples.\n\t    -- mu2   : The sample mean over activations, precalculated on an\n\t               representative data set.\n\t    -- sigma1: The covariance matrix over activations for generated samples.\n\t    -- sigma2: The covariance matrix over activations, precalculated on an\n\t               representative data set.\n\t    Returns:\n\t    --   : The Frechet Distance.\n\t    \"\"\"\n", "    mu1 = np.atleast_1d(mu1)\n\t    mu2 = np.atleast_1d(mu2)\n\t    sigma1 = np.atleast_2d(sigma1)\n\t    sigma2 = np.atleast_2d(sigma2)\n\t    assert mu1.shape == mu2.shape, \\\n\t        'Training and test mean vectors have different lengths'\n\t    assert sigma1.shape == sigma2.shape, \\\n\t        'Training and test covariances have different dimensions'\n\t    diff = mu1 - mu2\n\t    # Product might be almost singular\n", "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False, blocksize=1024)\n\t    if not np.isfinite(covmean).all():\n\t        msg = ('fid calculation produces singular product; '\n\t               'adding %s to diagonal of cov estimates') % eps\n\t        print(msg)\n\t        offset = np.eye(sigma1.shape[0]) * eps\n\t        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\t    # Numerical error might give slight imaginary component\n\t    if np.iscomplexobj(covmean):\n\t        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n", "            m = np.max(np.abs(covmean.imag))\n\t            raise ValueError('Imaginary component {}'.format(m))\n\t        covmean = covmean.real\n\t    tr_covmean = np.trace(covmean)\n\t    return (diff.dot(diff) + np.trace(sigma1)\n\t            + np.trace(sigma2) - 2 * tr_covmean)\n\tclass BatchRotateYCenterXZ(torch.nn.Module):\n\t    def __init__(self):\n\t        super(BatchRotateYCenterXZ, self).__init__()\n\t    def forward(self, global_positions, global_quats, ref_frame_id):\n", "        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id + 1, 5:6, :] - global_positions[:,\n\t                                                                                              ref_frame_id:ref_frame_id + 1,\n\t                                                                                              1:2, :],\n\t                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),\n\t                                 dim=-1)\n\t        root_rotation = from_to_1_0_0(ref_vector)\n\t        # center\n\t        ref_hip = torch.mean(global_positions[:, :, 0:1, [0, 2]], dim=(1), keepdim=True)\n\t        global_positions[..., [0, 2]] = global_positions[..., [0, 2]] - ref_hip\n\t        global_positions = quaternion_apply(root_rotation, global_positions)\n", "        global_quats = quaternion_multiply(root_rotation, global_quats)\n\t        return global_positions, global_quats\n\tclass BenchmarkDataSet(Dataset):\n\t    def __init__(self, data, style_keys):\n\t        super(BenchmarkDataSet, self).__init__()\n\t        o, h, q, a, s, b, f, style = [], [], [], [], [], [], [], []\n\t        for style_name in style_keys:\n\t            dict = data[style_name]['motion']\n\t            o += dict['offsets']\n\t            h += dict['hip_pos']\n", "            q += dict['quats']\n\t            a += dict['A']\n\t            s += dict['S']\n\t            b += dict['B']\n\t            f += dict['F']\n\t            for i in range(len(dict['offsets'])):\n\t                style.append(random.sample(data[style_name]['style'], 1)[0])\n\t        motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f, \"style\": style}\n\t        self.data = motion\n\t    def __getitem__(self, item):\n", "        keys = [\"hip_pos\", \"quats\", \"offsets\", \"A\", \"S\", \"B\", \"F\"]\n\t        dict = {key: self.data[key][item][0] for key in keys}\n\t        dict[\"style\"] = self.data[\"style\"][item]\n\t        return {**dict}\n\t    def __len__(self):\n\t        return len(self.data[\"offsets\"])\n\tdef skating_loss(pred_seq):\n\t    num_joints = 23\n\t    # pred_seq = pred_seq.transpose(1, 2)\n\t    pred_seq = pred_seq.view(pred_seq.shape[0], pred_seq.shape[1], num_joints, 3)\n", "    foot_seq = pred_seq[:, :, [3, 4, 7, 8], :]\n\t    v = torch.sqrt(\n\t        torch.sum((foot_seq[:, 1:, :, [0, 1, 2]] - foot_seq[:, :-1, :, [0, 1, 2]]) ** 2, dim=3, keepdim=True))\n\t    #  v[v<1]=0\n\t    # v=torch.abs(foot_seq[:,1:,:,[0,2]]-foot_seq[:,:-1,:,[0,2]])\n\t    ratio = torch.abs(foot_seq[:, 1:, :, 1:2]) / 2.5  # 2.5\n\t    exp = torch.clamp(2 - torch.pow(2, ratio), 0, 1)\n\t    c = np.sum(exp.detach().numpy() > 0)\n\t    s = (v * exp)\n\t    c = max(c, 1)\n", "    m = torch.sum(s) / c\n\t    # m = torch.max(s)\n\t    return m\n\tclass Condition():\n\t    def __init__(self, length, x, t):\n\t        self.length = length\n\t        self.x = x\n\t        self.t = t\n\t    def get_name(self):\n\t        return \"{}_{}_{}\".format(self.length, self.x, self.t)\n", "class GetLoss():\n\t    def __init__(self):\n\t        pass\n\t    def __call__(self, data, mA, mB):\n\t        return 0.0\n\tdef flatjoints(x):\n\t    \"\"\"\n\t    Shorthand for a common reshape pattern. Collapses all but the two first dimensions of a tensor.\n\t    :param x: Data tensor of at least 3 dimensions.\n\t    :return: The flattened tensor.\n", "    \"\"\"\n\t    return x.reshape((x.shape[0], x.shape[1], -1))\n\tdef fast_npss(gt_seq, pred_seq):\n\t    \"\"\"\n\t    Computes Normalized Power Spectrum Similarity (NPSS).\n\t    This is the metric proposed by Gropalakrishnan et al (2019).\n\t    This implementation uses numpy parallelism for improved performance.\n\t    :param gt_seq: ground-truth array of shape : (Batchsize, Timesteps, Dimension)\n\t    :param pred_seq: shape : (Batchsize, Timesteps, Dimension)\n\t    :return: The average npss metric for the batch\n", "    \"\"\"\n\t    # Fourier coefficients along the time dimension\n\t    gt_fourier_coeffs = np.real(np.fft.fft(gt_seq, axis=1))\n\t    pred_fourier_coeffs = np.real(np.fft.fft(pred_seq, axis=1))\n\t    # Square of the Fourier coefficients\n\t    gt_power = np.square(gt_fourier_coeffs)\n\t    pred_power = np.square(pred_fourier_coeffs)\n\t    # Sum of powers over time dimension\n\t    gt_total_power = np.sum(gt_power, axis=1)\n\t    pred_total_power = np.sum(pred_power, axis=1)\n", "    # Normalize powers with totals\n\t    gt_norm_power = gt_power / gt_total_power[:, np.newaxis, :]\n\t    pred_norm_power = pred_power / pred_total_power[:, np.newaxis, :]\n\t    # Cumulative sum over time\n\t    cdf_gt_power = np.cumsum(gt_norm_power, axis=1)\n\t    cdf_pred_power = np.cumsum(pred_norm_power, axis=1)\n\t    # Earth mover distance\n\t    emd = np.linalg.norm((cdf_pred_power - cdf_gt_power), ord=1, axis=1)\n\t    # Weighted EMD\n\t    power_weighted_emd = np.average(emd, weights=gt_total_power)\n", "    return power_weighted_emd\n\tclass GetLastRotLoss(GetLoss):\n\t    def __init__(self):\n\t        super(GetLastRotLoss, self).__init__()\n\t    def __call__(self, data, mA, mB):\n\t        a = data[mA]['rot']\n\t        b = data[mB]['rot']\n\t        return torch.mean(torch.sqrt(\n\t            torch.sum((a[:, -1:, ...] - b[:, -1:, ...]) ** 2.0, dim=(2, 3))))\n\tclass GetNPSS(GetLoss):\n", "    def __init__(self):\n\t        super(GetNPSS, self).__init__()\n\t    def __call__(self, data, mA, mB):\n\t        a = data[mA]['rot']\n\t        b = data[mB]['rot']\n\t        return fast_npss(flatjoints(a), flatjoints(b))\n\tclass GetLastPosLoss(GetLoss):\n\t    def __init__(self, x_mean, x_std):\n\t        super(GetLastPosLoss, self).__init__()\n\t        self.x_mean = x_mean\n", "        self.x_std = x_std\n\t    def __call__(self, data, mA, mB):\n\t        a = data[mA]['pos'].flatten(-2, -1)\n\t        b = data[mB]['pos'].flatten(-2, -1)\n\t        a = (a - self.x_mean) / self.x_std\n\t        b = (b - self.x_mean) / self.x_std\n\t        return torch.mean(torch.sqrt(\n\t            torch.sum((a[:, -1:, ...] - b[:, -1:, ...]) ** 2.0, dim=(2))))\n\tclass GetPosLoss(GetLoss):\n\t    def __init__(self, x_mean, x_std):\n", "        super(GetPosLoss, self).__init__()\n\t        self.x_mean = x_mean\n\t        self.x_std = x_std\n\t    def __call__(self, data, mA, mB):\n\t        a = data[mA]['pos'].flatten(-2, -1)\n\t        b = data[mB]['pos'].flatten(-2, -1)\n\t        a = (a - self.x_mean) / self.x_std\n\t        b = (b - self.x_mean) / self.x_std\n\t        return torch.mean(torch.sqrt(\n\t            torch.sum((a[:, :, ...] - b[:, :, ...]) ** 2.0, dim=(2))))\n", "class GetContactLoss(GetLoss):\n\t    def __init__(self):\n\t        super(GetContactLoss, self).__init__()\n\t    def __call__(self, data, mA, mB=None):\n\t        return skating_loss(data[mA]['pos'])\n\tdef calculate_diversity(data):\n\t    loss_function = torch.nn.MSELoss()\n\t    N = data.shape[-1]\n\t    loss = 0\n\t    for i in range(N):\n", "        for j in range(i + 1, N):\n\t            loss += loss_function(data[..., i], data[..., j]).detach().cpu().numpy()\n\t    loss /= (N * N / 2)\n\t    return loss\n\tclass GetDiversity(GetLoss):\n\t    def __init__(self, style_seq):\n\t        super(GetDiversity, self).__init__()\n\t        self.style_seq = style_seq\n\t    def __call__(self, data, mA, mB=None):\n\t        pos = data[mA]['pos']\n", "        DIVs = []\n\t        start_id = 0\n\t        for length in self.style_seq:\n\t            if length:\n\t                pos_seq = pos[start_id:start_id + length]\n\t                div = calculate_diversity(pos_seq)\n\t            else:\n\t                div = 0\n\t            DIVs.append(div)\n\t            start_id += length\n", "        dic = dict(zip(range(1, len(DIVs) + 1), DIVs))\n\t        for key in dic.keys():\n\t            print(dic[key])\n\t        DIVs = np.array(DIVs)\n\t        weight = np.array(self.style_seq)\n\t        # mask = self.style_seq != 0\n\t        # weight = self.style_seq[mask]\n\t        avg_div = np.average(DIVs, weights=weight)\n\t        return avg_div\n\tclass GetFID(GetLoss):\n", "    def __init__(self, style_seq):\n\t        super(GetFID, self).__init__()\n\t        self.style_seq = style_seq\n\t        self.norm_factor = 1\n\t    def __call__(self, data, mA, mB):\n\t        if mA == mB:\n\t            return 0.0\n\t        import time\n\t        t = time.time()\n\t        FIDs = []\n", "        start_id_noaug = 0\n\t        start_id_aug = 0\n\t        # latentA = data[mB]['latent'][start_id:start_id + self.style_seq[0]]\n\t        for i, length_noaug in enumerate(self.style_seq):\n\t            if length_noaug:\n\t                length_aug = length_noaug\n\t                # latentA = data[mB]['latent'][start_id:start_id + length//2].flatten(1, -1)\n\t                # latentB = data[mB]['latent'][start_id + length//2:start_id + length].flatten(1, -1)\n\t                latentA = data[mA]['latent'][start_id_noaug:start_id_noaug + length_noaug] / self.norm_factor\n\t                # latentA = latentA.flatten(1, -1)\n", "                # latentA = latentA.flatten(0, 1)  # v\n\t                latentA = latentA[:, :, :, 0].transpose(1, 2).flatten(0, 1)  # latent\n\t                # latentA = latentA.flatten(0, 1).flatten(1, 2)  # phase\n\t                latentB = data[mB]['latent'][start_id_aug:start_id_aug + length_aug] / self.norm_factor\n\t                latentB = latentB[:, :, :, 0].transpose(1, 2).flatten(0, 1)  # latent\n\t                # latentB = latentB.flatten(0, 1).flatten(1, 2)  # phase\n\t                # latentB = latentB.flatten(0, 1)   # v\n\t                # latentB = latentB.flatten(1, -1)\n\t                fid = calculate_fid(latentA, latentB)\n\t            else:\n", "                fid = 0\n\t            FIDs.append(fid)\n\t            start_id_noaug += length_noaug\n\t            start_id_aug += length_aug\n\t        round_func = lambda x: round(x, 2)\n\t        dic = dict(zip(range(1, len(FIDs) + 1), map(round_func, FIDs)))\n\t        for key in dic.keys():\n\t            print(dic[key])\n\t        FIDs = np.array(FIDs)\n\t        weight = np.array(self.style_seq)\n", "        # mask = self.style_seq != 0\n\t        # weight = self.style_seq[mask]\n\t        avg_fid = np.average(FIDs, weights=weight)\n\t        max_fid = np.max(FIDs)\n\t        print(avg_fid)\n\t        print(max_fid)\n\t        print(f'cost : {time.time() - t:.4f}s')\n\t        # return [avg_fid, max_fid]\n\t        return avg_fid\n\tclass GetAcc(GetLoss):\n", "    def __init__(self, gt_label, style_seq):\n\t        super(GetAcc, self).__init__()\n\t        self.style_seq = style_seq\n\t        self.gt_label = gt_label\n\t        pass\n\t    def __call__(self, data, mA, mB):\n\t        print(mA)\n\t        start_id_noaug = 0\n\t        correct = 0\n\t        correct_list = []\n", "        for i, length_noaug in enumerate(self.style_seq):\n\t            gt_label = self.gt_label[i]\n\t            if length_noaug:\n\t                pred_label = data[mA]['label'][start_id_noaug:start_id_noaug + length_noaug]\n\t                correct_num = ((pred_label == gt_label).sum()).detach().cpu().numpy()\n\t                correct += correct_num\n\t                correct_list.append(correct_num / length_noaug)\n\t                start_id_noaug += length_noaug\n\t            else:\n\t                correct_list.append(0.)\n", "        for i in correct_list:\n\t            print(i)\n\t        print('###########################')\n\t        return correct / sum(self.style_seq)\n\tdef reconstruct_motion(models, function, X, Q, A, S, tar_pos, tar_quat, offsets, skeleton: Skeleton,\n\t                       condition: Condition, n_past=10, n_future=10):\n\t    # \"\"\"\n\t    # Evaluate naive baselines (zero-velocity and interpolation) for transition generation on given data.\n\t    # :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n\t    # :param Q: Local quaternions array of shape (B, T, J, 4)\n", "    # :param x_mean : Mean vector of local positions of shape (1, J*3, 1)\n\t    # :param x_std: Standard deviation vector of local positions (1, J*3, 1)\n\t    # :param offsets: Local bone offsets tensor of shape (1, 1, J-1, 3)\n\t    # :param parents: List of bone parents indices defining the hierarchy\n\t    # :param out_path: optional path for saving the results\n\t    # :param n_past: Number of frames used as past context\n\t    # :param n_future: Number of frames used as future context (only the first frame is used as the target)\n\t    # :return: Results dictionary\n\t    # \"\"\"\n\t    data = {}\n", "    torch.cuda.empty_cache()\n\t    location_x = condition.x\n\t    duration_t = condition.t\n\t    n_trans = int(condition.length * duration_t)\n\t    target_id = condition.length + n_past\n\t    X[:, 12:, 0, [0, 2]] = X[:, 12:, 0, [0, 2]] + (\n\t            X[:, target_id:target_id + 1, 0, [0, 2]] - X[:, 10:11, 0, [0, 2]]) * location_x\n\t    # Format the data for the current transition lengths. The number of samples and the offset stays unchanged.\n\t    curr_window = target_id + n_future\n\t    curr_x = X[:, :curr_window, ...]\n", "    curr_q = Q[:, :curr_window, ...]\n\t    global_poses, global_quats = skeleton.forward_kinematics(curr_q, offsets, curr_x)\n\t    def remove_disconti(quats):\n\t        ref_quat = global_quats[:, n_past - 1:n_past]\n\t        return remove_quat_discontinuities(torch.cat((ref_quat, quats), dim=1))[:, 1:]\n\t    def get_gt_pose(n_past, target_id):\n\t        trans_gt_global_poses, trans_gt_global_quats = global_poses[:, n_past: target_id, ...], global_quats[:,\n\t                                                                                                n_past: target_id, ...]\n\t        trans_gt_global_quats = remove_disconti(trans_gt_global_quats)\n\t        return trans_gt_global_poses, trans_gt_global_quats\n", "    gt_global_pos, gt_global_quats = get_gt_pose(n_past, target_id)\n\t    data['gt'] = {}\n\t    data['gt']['pos'] = gt_global_pos\n\t    data['gt']['rot'] = gt_global_quats\n\t    data['gt']['offsets'] = offsets\n\t    for key in models:\n\t        resQ, resX = function[key](models[key], X, Q, A, S, tar_pos, tar_quat, offsets, skeleton, n_trans, target_id)\n\t        resQ = remove_disconti(resQ)\n\t        data[key] = {}\n\t        data[key]['rot'] = resQ\n", "        data[key]['pos'] = resX\n\t        # data[key]['phase'] = F.normalize(phase[3], dim=-1)\n\t        data[key][\"offsets\"] = offsets\n\t        ############################# calculate diversity #################################\n\t        # data['gt']['phase'] = F.normalize(gt_phase, dim=-1)\n\t        # data['div_test'] = {}\n\t        # num_samples = 10\n\t        # N,T,J,C = X.shape\n\t        # X = X.repeat(num_samples,1,1,1)\n\t        # Q = Q.repeat(num_samples,1,1,1)\n", "        # A = A.repeat(num_samples,1,1,1)\n\t        # S = S.repeat(num_samples,1,1,1)\n\t        # tar_pos = tar_pos.repeat(num_samples,1,1,1)\n\t        # tar_quat = tar_quat.repeat(num_samples,1,1,1)\n\t        # offsets = offsets.repeat(num_samples,1,1)\n\t        # x, q = [], []\n\t        # resQ, resX = function[key](models[key], X, Q, A, S, tar_pos, tar_quat, offsets, skeleton, n_trans,\n\t        #                         target_id, ifnoise=True)\n\t        #\n\t        # data['div_test']['pos'] = resX.view(num_samples,N,resX.shape[1],resX.shape[2],3).permute(1,2,3,4,0).cpu()\n", "        # data['div_test']['rot'] = resQ.view(num_samples,N,resX.shape[1],resX.shape[2],4).permute(1,2,3,4,0).cpu()\n\t        ####################################################################################\n\t    return data, global_poses[:, n_past - 1:n_past, :, :], global_quats[:, n_past - 1:n_past, :, :]\n\tdef save_data_to_binary(path, data):\n\t    f = open('./benchmark_result/'+ path, \"wb+\")\n\t    pos = {}\n\t    for condition in data.keys():\n\t        pos[condition] = {}\n\t        for method in data[condition].keys():\n\t            pos[condition][method] = data[condition][method]['pos']\n", "            data[condition][method]['pos'] = data[condition][method]['pos'][:,:,0:1,:]#b,t,1,3\n\t    pickle.dump(data, f)\n\t    for condition in data.keys():\n\t        for method in data[condition].keys():\n\t            data[condition][method]['pos'] = pos[condition][method]\n\t    f.close()\n\tdef load_data_from_binary(path, skeleton:Skeleton):\n\t    # condition{method{data}}\n\t    f = open('./benchmark_result/'+path, \"rb\")\n\t    data = pickle.load(f)\n", "    for condition in data.keys():\n\t        for method in data[condition].keys():\n\t            pos = data[condition][method]['pos'] #b,t,1,3\n\t            rot = data[condition][method]['rot'] #b,t,23,4\n\t            offsets = data[condition][method]['offsets']\n\t            data[condition][method]['pos'] = skeleton.global_rot_to_global_pos(rot, offsets, pos)\n\t    f.close()\n\t    return data\n\tdef print_result(metrics, out_path=None):\n\t    def format(dt, name, value):\n", "        s = \"{0: <16}\"\n\t        for i in range(len(value)):\n\t            s += \" & {\" + str(i + 1) + \":\" + dt + \"}\"\n\t        return s.format(name, *value)\n\t    print()\n\t    for metric_name in metrics.keys():\n\t        if metric_name != \"NPSS\":\n\t            print(\"=== {} losses ===\".format(metric_name))\n\t            conditions = list(metrics[metric_name].keys())\n\t            print(format(\"<6\", \"Conditions\", conditions))\n", "            all_methods = list(metrics[metric_name][conditions[0]].keys())\n\t            for method_name in all_methods:\n\t                method_metric = [metrics[metric_name][condition][method_name] for condition in conditions]\n\t                print(format(\"6.3f\", method_name, method_metric))\n\t        else:\n\t            print(\"=== NPSS ===\".format(metric_name))\n\t            conditions = list(metrics[metric_name].keys())\n\t            print(format(\"<6\", \"Conditions\", conditions))\n\t            all_methods = list(metrics[metric_name][conditions[0]].keys())\n\t            for method_name in all_methods:\n", "                method_metric = [metrics[metric_name][condition][method_name] for condition in conditions]\n\t                # print(method_name)\n\t                # print(method_metric)\n\t                print(format(\"6.5f\", method_name, method_metric))\n\t    if out_path is not None:\n\t        res_txt_file = open(os.path.join(out_path, 'benchmark.txt'), \"a\")\n\t        for metric_name in metrics.keys():\n\t            res_txt_file.write(\"\\n=== {} losses ===\\n\".format(metric_name))\n\t            conditions = list(metrics[metric_name].keys())\n\t            res_txt_file.write(format(\"<6\", \"Conditions\", conditions) + \"\\n\")\n", "            all_methods = list(metrics[metric_name][conditions[0]].keys())\n\t            for method_name in all_methods:\n\t                method_metric = [metrics[metric_name][condition][method_name] for condition in conditions]\n\t                res_txt_file.write(format(\"6.3f\", method_name, method_metric) + \"\\n\")\n\t        print(\"\\n\")\n\t        res_txt_file.close()\n\tdef get_vel(pos):\n\t    return pos[:, 1:] - pos[:, :-1]\n\tdef get_gt_latent(style_encoder, rot, pos, batch_size=1000):\n\t    glb_vel, glb_pos, glb_rot, root_rotation = BatchProcessDatav2().forward(rot, pos)\n", "    stard_id = 0\n\t    data_size = glb_vel.shape[0]\n\t    init = True\n\t    while stard_id < data_size:\n\t        length = min(batch_size, data_size - stard_id)\n\t        mp_batch = {}\n\t        mp_batch['glb_rot'] = quat_to_or6D(glb_rot[stard_id: stard_id + length]).cuda()\n\t        mp_batch['glb_pos'] = glb_pos[stard_id: stard_id + length].cuda()\n\t        mp_batch['glb_vel'] = glb_vel[stard_id: stard_id + length].cuda()\n\t        latent = style_encoder.cal_latent(mp_batch).cpu()\n", "        if init:\n\t            output = torch.empty((data_size,) + latent.shape[1:])\n\t            init = False\n\t        output[stard_id: stard_id + length] = latent\n\t        stard_id += length\n\t    return output\n\tdef calculate_stat(conditions, dataLoader, data_size, function, models, skeleton, load_from_dict = False,data_name = None, window_size=1500):\n\t    ##################################\n\t    # condition{method{data}}\n\t    if load_from_dict:\n", "        print('load from calculated stat ...')\n\t        t = time.time()\n\t        outputs = load_data_from_binary(data_name, skeleton)\n\t        print('loading data costs {} s'.format(time.time() - t))\n\t    else:\n\t        outputs = {}\n\t        with torch.no_grad():\n\t            for condition in conditions:\n\t                print('Reconstructing motions for condition: length={}, x={}, t={} ...'.format(condition.length, condition.x,condition.t))\n\t                start_id = 0\n", "                outputs[condition.get_name()] = {}\n\t                outputs[condition.get_name()] = {\"gt\": {}, \"div_test\": {}}  # , \"interp\":{}, \"zerov\":{}}\n\t                output = outputs[condition.get_name()]\n\t                for key in models.keys():\n\t                    output[key] = {}\n\t                for batch in dataLoader:\n\t                    \"\"\"\n\t                    :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n\t                    :param Q: Local quaternions array of shape (B, T, J, 4)\n\t                    \"\"\"\n", "                    t1 = time.time()\n\t                    A = batch['A'] / 0.1\n\t                    S = batch['S']\n\t                    gp, gq = skeleton.forward_kinematics(batch['quats'], batch['offsets'], batch['hip_pos'])\n\t                    tar_gp, tar_gq = skeleton.forward_kinematics(batch['style']['quats'], batch['style']['offsets'],\n\t                                                                 batch['style']['hip_pos'])\n\t                    local_pos, global_quat = BatchRotateYCenterXZ().forward(gp, gq, 10)\n\t                    tar_pos, tar_quat = BatchRotateYCenterXZ().forward(tar_gp, tar_gq, 10)\n\t                    local_quat = skeleton.inverse_kinematics_quats(global_quat)\n\t                    local_quat = (remove_quat_discontinuities(local_quat.cpu()))\n", "                    hip_pos = local_pos[:, :, 0:1, :]\n\t                    data, last_pos, last_rot = reconstruct_motion(models, function, hip_pos.cuda(), local_quat.cuda(),\n\t                                                                  A.cuda(), S.cuda(), tar_pos.cuda(),\n\t                                                                  tar_quat.cuda(), batch['offsets'].cuda(), skeleton,\n\t                                                                  condition)\n\t                    t2 = time.time()\n\t                    if start_id == 0:\n\t                        for method in data.keys():\n\t                            for prop in data[method].keys():\n\t                                output[method][prop] = torch.empty((data_size,) + data[method][prop].shape[1:])\n", "                    for method in data.keys():\n\t                        batch_size = data[method]['pos'].shape[0]\n\t                        props = list(data[method].keys())\n\t                        for prop in props:\n\t                            output[method][prop][start_id:start_id + batch_size] = data[method][prop]\n\t                            del data[method][prop]\n\t                    print(\"batch_id : {} - {}\".format(start_id, start_id + batch_size))\n\t                    start_id += batch_size\n\t                    print(\"time cost t2 - t1 : {}\".format(t2 - t1))\n\t            # print('saving data to binary file ...')\n", "            # save_data_to_binary(data_name, outputs)\n\t    for condition in conditions:\n\t        if condition.length == 40:\n\t            print('Reconstructing latent for condition: length={}, x={}, t={} ...'.format(condition.length, condition.x, condition.t))\n\t            t = time.time()\n\t            data = outputs[condition.get_name()]\n\t            for method in data.keys():\n\t                if method == \"div_test\":# or method == \"gt\":\n\t                    continue\n\t                start_id = 0\n", "                length = data[method]['rot'].shape[0]\n\t                while start_id < length:\n\t                    window = min(window_size, length-start_id)\n\t                    rot, pos = data[method]['rot'][start_id:start_id+window], data[method]['pos'][start_id:start_id+window]\n\t                    # rot, pos = torch.cat((last_rot, data[method]['rot']), dim=1), torch.cat((last_pos, data[method]['pos']), dim=1)\n\t                    # glb_vel,glb_pos,glb_rot,root_rotation = BatchProcessDatav2().forward(data[method]['rot'], data[method]['pos'])\n\t                    glb_vel, glb_pos, glb_rot, root_rotation = BatchProcessDatav2().forward(rot, pos)\n\t                    # data[method][\"latent\"] = glb_vel.flatten(-2,-1).cpu()    # use vel\n\t                    mp_batch = {}\n\t                    mp_batch['glb_rot'] = quat_to_or6D(glb_rot).cuda()\n", "                    mp_batch['glb_pos'] = glb_pos.cuda()\n\t                    mp_batch['glb_vel'] = glb_vel.cuda()\n\t                    # latent = style_encoder.cal_latent(mp_batch).cpu() # use latent\n\t                    # label = style_encoder.cal_label(mp_batch).cpu()\n\t                    # if start_id == 0:\n\t                    #     data[method]['latent'] = torch.empty((length,) + latent.shape[1:])\n\t                    #     # data[method]['label'] = torch.empty((length,) + label.shape[1:])\n\t                    #\n\t                    # data[method]['latent'][start_id:start_id + window] = latent\n\t                    # data[method]['label'][start_id:start_id + window] = label\n", "                    # del latent\n\t                    # del latent\n\t                    # del label\n\t                    start_id += window\n\t            print('cost {} s'.format(time.time() - t))\n\t    return outputs\n\tdef calculate_benchmark(data, benchmarks):\n\t    metrics = {}\n\t    for metric_key in benchmarks.keys():\n\t        print('Computing errors for {}'.format(metric_key))\n", "        metrics[metric_key] = {}\n\t        if metric_key == \"Diversity\":\n\t            for condition in data.keys():\n\t                print(\"condition : {}\".format(condition))\n\t                metrics[metric_key][condition] = {}\n\t                metric = benchmarks[metric_key]\n\t                metrics[metric_key][condition][\"div_test\"] = metric(data[condition], \"div_test\")\n\t        else:\n\t            for condition in data.keys():\n\t                print(\"condition : {}\".format(condition))\n", "                # if metric_key == 'FID' and condition.length != 40:\n\t                #     continue\n\t                metrics[metric_key][condition] = {}\n\t                metric = benchmarks[metric_key]\n\t                for method in data[condition].keys():\n\t                    if method == \"div_test\":\n\t                        continue\n\t                    metrics[metric_key][condition][method] = metric(data[condition], method, \"gt\")\n\t    return metrics\n\tdef benchmarks(args,load_stat=False, data_name = None):\n", "    # set dataset\n\t    style_start = 0\n\t    style_end = 90\n\t    batch_size = 500\n\t    style_loader = StyleLoader()\n\t    print('loading dataset ...')\n\t    stat_file = 'style100_benchmark_65_25'\n\t    style_loader.load_from_binary(stat_file)\n\t    style_loader.load_skeleton_only()\n\t    skeleton = style_loader.skeleton\n", "    stat_dict = style_loader.load_part_to_binary(\"style100_benchmark_stat\")\n\t    mean, std = stat_dict['pos_stat']\n\t    mean, std = torch.from_numpy(mean).view(23 * 3), torch.from_numpy(std).view(23 * 3)\n\t    # set style\n\t    style_keys = list(style_loader.all_motions.keys())[style_start:style_end]\n\t    dataSet = BenchmarkDataSet(style_loader.all_motions, style_keys)\n\t    data_size = len(dataSet)\n\t    dataLoader = DataLoader(dataSet, batch_size=batch_size, num_workers=0, shuffle=False)\n\t    style_seqs = [len(style_loader.all_motions[style_key]['motion']['quats']) for style_key in style_keys]\n\t    gt_style = []\n", "    models, function = load_model(args)\n\t    for key in models:\n\t        for param in models[key].parameters():\n\t            param.requires_grad = False\n\t    # set condition and metrics for the benchmarks\n\t    conditions, metrics = Set_Condition_Metric(gt_style, mean, std, style_seqs)\n\t    outputs = calculate_stat(conditions, dataLoader, data_size, function, models, skeleton, load_from_dict=load_stat, data_name=data_name)\n\t    metrics_stat = calculate_benchmark(outputs, metrics)\n\t    print_result(metrics_stat, './')\n\tdef Set_Condition_Metric(gt_style, mean, std, style_seqs):\n", "    pos_loss = GetLastPosLoss(mean, std)\n\t    gp_loss = GetPosLoss(mean,std)\n\t    rot_loss = GetLastRotLoss()\n\t    contact_loss = GetContactLoss()\n\t    npss_loss = GetNPSS()\n\t    fid = GetFID(style_seqs)\n\t    diversity = GetDiversity(style_seqs)\n\t    acc = GetAcc(gt_style, style_seqs)\n\t    # set metrics\n\t    # for example: metrics = {\"Diversity\" : diversity}#{\"LastPos\": pos_loss, \"Contact\": contact_loss}\n", "    metrics = {\"NPSS\": npss_loss, \"gp\": gp_loss, \"Contact\": contact_loss}\n\t    # set conditions\n\t    conditions = [Condition(10, 0, 1), Condition(20, 0, 1), Condition(40, 0, 1),]\n\t    return conditions, metrics\n\tif __name__ == '__main__':\n\t    random.seed(0)\n\t    from argparse import ArgumentParser\n\t    parser = ArgumentParser()\n\t    parser.add_argument(\"--model_path\", type=str,default='./results/Transitionv2_style100/myResults/117/m_save_model_205')\n\t    parser.add_argument(\"--model_name\",type=str,default=\"RSMT\")\n", "    args = parser.parse_args()\n\t   # args.model_path = './results/Transitionv2_style100/myResults/128/m_save_model_last'\n\t    benchmarks(args,load_stat=False, data_name='benchmark_test_data.dat')"]}
{"filename": "benchmarkStyle100_withStyle.py", "chunked_list": ["import os\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport torch\n\tfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply\n\tfrom torch.utils.data import DataLoader, Dataset\n\timport src.Datasets.BaseLoader as mBaseLoader\n\timport src.Net.TransitionNet\n\tfrom src.Datasets.BaseLoader import BasedDataProcessor\n\tfrom src.Net.TransitionPhaseNet import TransitionNet_phase\n", "from src.geometry.quaternions import quat_inv, quat_mul, quat_mul_vec, from_to_1_0_0\n\tfrom src.utils.BVH_mod import Skeleton\n\tfrom src.utils.np_vector import interpolate_local, remove_quat_discontinuities\n\tclass BatchRotateYCenterXZ(torch.nn.Module):\n\t    def __init__(self):\n\t        super(BatchRotateYCenterXZ, self).__init__()\n\t    def forward(self,global_positions,global_quats,ref_frame_id):\n\t        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n\t                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),dim=-1)\n\t        root_rotation = from_to_1_0_0(ref_vector)\n", "        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n\t        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n\t        global_positions = quaternion_apply(root_rotation, global_positions)\n\t        global_quats = quaternion_multiply(root_rotation, global_quats)\n\t        return global_positions,global_quats\n\tclass BenchmarkProcessor(BasedDataProcessor):\n\t    def __init__(self):\n\t        super(BenchmarkProcessor, self).__init__()\n\t        # self.process = BatchRotateYCenterXZ()\n\t    def __call__(self, dict, skeleton, motionDataLoader, ratio=1.0):\n", "        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n\t        stat = self._calculate_stat(offsets, hip_pos, quats, skeleton, 1.0)\n\t        return {\"offsets\": offsets, \"hip_pos\": hip_pos, \"quats\": quats, **stat}\n\t    def _concat(self, local_quat, offsets, hip_pos, ratio=0.2):\n\t        local_quat = torch.from_numpy(np.concatenate((local_quat), axis=0))\n\t        offsets = torch.from_numpy(np.concatenate((offsets), axis=0))\n\t        hip_pos = torch.from_numpy(np.concatenate((hip_pos), axis=0))\n\t        return local_quat, offsets, hip_pos\n\t    def calculate_pos_statistic(self, pos):\n\t        '''pos:N,T,J,3'''\n", "        mean = np.mean(pos, axis=(0, 1))\n\t        std = np.std(pos, axis=(0, 1))\n\t        return mean, std\n\t    def _calculate_stat(self, offsets, hip_pos, local_quat, skeleton, ratio):\n\t        local_quat, offsets, hip_pos = self._concat(local_quat, offsets, hip_pos, ratio)\n\t        local_quat = local_quat.to(torch.float64)\n\t        offsets = offsets.to(torch.float64)\n\t        hip_pos = hip_pos.to(torch.float64)\n\t        global_positions, global_rotations = skeleton.forward_kinematics(local_quat, offsets, hip_pos)\n\t        global_positions, global_quats, root_rotation = BatchRotateYCenterXZ().forward(global_positions,\n", "                                                                                       global_rotations, 10)\n\t        # global_positions = torch.cat((local_positions[:,:,0:1,:],local_positions[:,:,1:]+local_positions[:,:,0:1,:]),dim=-2)\n\t        pos_mean, pos_std = self.calculate_pos_statistic(global_positions.cpu().numpy())\n\t        pos_mean.astype(np.float)\n\t        pos_std.astype(np.float)\n\t        return {\"pos_stat\": [pos_mean, pos_std]}\n\tclass BenchmarkDataSet(Dataset):\n\t    def __init__(self, data, style_keys):\n\t        super(BenchmarkDataSet, self).__init__()\n\t        o, h, q, a, s, b, f, style = [], [], [], [], [], [], [], []\n", "        for style_name in style_keys:\n\t            dict = data[style_name]['motion']\n\t            o += dict['offsets']\n\t            h += dict['hip_pos']\n\t            q += dict['quats']\n\t            a += dict['A']\n\t            s += dict['S']\n\t            b += dict['B']\n\t            f += dict['F']\n\t            for i in range(len(dict['offsets'])):\n", "                style.append( data[style_name]['style'])\n\t        motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f, \"style\":style}\n\t        self.data = motion\n\t        # motions = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f}\n\t    def __getitem__(self, item):\n\t        keys = [\"hip_pos\",\"quats\",\"offsets\",\"A\",\"S\",\"B\",\"F\"]\n\t        dict = {key: self.data[key][item][0] for key in keys}\n\t        dict[\"style\"] = self.data[\"style\"][item]\n\t        return {**dict}\n\t    def __len__(self):\n", "        return len(self.data[\"offsets\"])\n\tdef eval_sample(model, X, Q, x_mean, x_std, pos_offset, skeleton: Skeleton, length, param):\n\t    # FOR EXAMPLE\n\t    model = model.eval()\n\t    # target frame\n\t    target_id = None\n\t    if \"target_id\" in param:\n\t        target_id = param[\"target_id\"]\n\t    pred_hips = torch.zeros(size=(X.shape[0], length, 1, 3))\n\t    pred_quats = torch.zeros(size=(X.shape[0], length, 22, 4))\n", "    GX, GQ = skeleton.forward_kinematics(pred_quats, pos_offset, pred_hips)\n\t    x_mean = x_mean.view(skeleton.num_joints, 3)\n\t    x_std = x_std.view(skeleton.num_joints, 3)\n\t    # normalized\n\t    GX = (GX - x_mean.flatten(-2, -1)) / x_std.flatten(-2, -1)\n\t    GX = GX.transpose(1, 2)\n\t    return GQ, GX\n\tdef load_model():\n\t    model_dict, function_dict, param_dict = {}, {}, {}\n\t    model_dict['Transition'] = torch.load('./results/Transitionv2_style100/myResults/117/m_save_model_205')\n", "    function_dict['Transition'] = src.Net.TransitionPhaseNet.eval_sample\n\t    return model_dict, function_dict, param_dict\n\tdef catPosition(pos, pos_offset):\n\t    pos = pos.view(pos.shape[0], pos.shape[1], 1, 3)\n\t    return torch.cat((pos, pos_offset), 2)\n\tdef torch_fk(lrot, lpos, parents, squeeze=True):\n\t    \"\"\"\n\t    Performs Forward Kinematics (FK) on local quaternions and local positions to retrieve global representations\n\t    :param lrot: tensor of local quaternions with shape (..., Nb of joints, 4)\n\t    :param lpos: tensor of local positions with shape (..., Nb of joints, 3)\n", "    :param parents: list of parents indices\n\t    :return: tuple of tensors of global quaternion, global positions\n\t    \"\"\"\n\t    gp, gr = [lpos[..., :1, :]], [lrot[..., :1, :]]\n\t    for i in range(1, len(parents)):\n\t        gp.append(quat_mul_vec(gr[parents[i]], lpos[..., i:i + 1, :]) + gp[parents[i]])\n\t        gr.append(quat_mul(gr[parents[i]], lrot[..., i:i + 1, :]))\n\t    rot, pos = torch.cat(gr, dim=-2), torch.cat(gp, dim=-2)\n\t    if (squeeze):\n\t        rot = rot.reshape(lrot.shape[0], lrot.shape[1], -1)\n", "        pos = pos.reshape(lrot.shape[0], lrot.shape[1], -1)\n\t    return rot, pos\n\tdef torch_quat_normalize(x, eps=1e-12):\n\t    assert x.shape[-1] == 3 or x.shape[-1] == 4 or x.shape[-1] == 2\n\t    # lgth =  torch.sqrt(torch.sum(x*x,dim=-1,keepdim=True))\n\t    lgth = torch.norm(x, dim=-1, keepdim=True)\n\t    return x / (lgth + eps)\n\tdef torch_ik_rot(grot, parents):\n\t    return torch.cat((\n\t        grot[..., :1, :],\n", "        quat_mul(quat_inv(grot[..., parents[1:], :]), grot[..., 1:, :]),\n\t    ), dim=-2)\n\tdef skating_loss(pred_seq):\n\t    num_joints = 23\n\t    pred_seq = pred_seq.transpose(1, 2)\n\t    pred_seq = pred_seq.view(pred_seq.shape[0], pred_seq.shape[1], num_joints, 3)\n\t    foot_seq = pred_seq[:, :, [3, 4, 7, 8], :]\n\t    v = torch.sqrt(\n\t        torch.sum((foot_seq[:, 1:, :, [0, 1, 2]] - foot_seq[:, :-1, :, [0, 1, 2]]) ** 2, dim=3, keepdim=True))\n\t    #  v[v<1]=0\n", "    # v=torch.abs(foot_seq[:,1:,:,[0,2]]-foot_seq[:,:-1,:,[0,2]])\n\t    ratio = torch.abs(foot_seq[:, 1:, :, 1:2]) / 2.5  # 2.5\n\t    exp = torch.clamp(2 - torch.pow(2, ratio), 0, 1)\n\t    c = np.sum(exp.detach().numpy() > 0)\n\t    s = (v * exp)\n\t    c = max(c, 1)\n\t    m = torch.sum(s) / c\n\t    # m = torch.max(s)\n\t    return m\n\tdef fast_npss(gt_seq, pred_seq):\n", "    \"\"\"\n\t    Computes Normalized Power Spectrum Similarity (NPSS).\n\t    This is the metric proposed by Gropalakrishnan et al (2019).\n\t    This implementation uses numpy parallelism for improved performance.\n\t    :param gt_seq: ground-truth array of shape : (Batchsize, Timesteps, Dimension)\n\t    :param pred_seq: shape : (Batchsize, Timesteps, Dimension)\n\t    :return: The average npss metric for the batch\n\t    \"\"\"\n\t    # Fourier coefficients along the time dimension\n\t    gt_fourier_coeffs = np.real(np.fft.fft(gt_seq, axis=1))\n", "    pred_fourier_coeffs = np.real(np.fft.fft(pred_seq, axis=1))\n\t    # Square of the Fourier coefficients\n\t    gt_power = np.square(gt_fourier_coeffs)\n\t    pred_power = np.square(pred_fourier_coeffs)\n\t    # Sum of powers over time dimension\n\t    gt_total_power = np.sum(gt_power, axis=1)\n\t    pred_total_power = np.sum(pred_power, axis=1)\n\t    # Normalize powers with totals\n\t    gt_norm_power = gt_power / gt_total_power[:, np.newaxis, :]\n\t    pred_norm_power = pred_power / pred_total_power[:, np.newaxis, :]\n", "    # Cumulative sum over time\n\t    cdf_gt_power = np.cumsum(gt_norm_power, axis=1)\n\t    cdf_pred_power = np.cumsum(pred_norm_power, axis=1)\n\t    # Earth mover distance\n\t    emd = np.linalg.norm((cdf_pred_power - cdf_gt_power), ord=1, axis=1)\n\t    # Weighted EMD\n\t    power_weighted_emd = np.average(emd, weights=gt_total_power)\n\t    return power_weighted_emd\n\tdef flatjoints(x):\n\t    \"\"\"\n", "    Shorthand for a common reshape pattern. Collapses all but the two first dimensions of a tensor.\n\t    :param x: Data tensor of at least 3 dimensions.\n\t    :return: The flattened tensor.\n\t    \"\"\"\n\t    return x.reshape((x.shape[0], x.shape[1], -1))\n\tdef benchmark_interpolation(models, function, params, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton: Skeleton, trans_lengths,\n\t                            benchmarks, n_past=10, n_future=10):\n\t    \"\"\"\n\t    Evaluate naive baselines (zero-velocity and interpolation) for transition generation on given data.\n\t    :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n", "    :param Q: Local quaternions array of shape (B, T, J, 4)\n\t    :param x_mean : Mean vector of local positions of shape (1, J*3, 1)\n\t    :param x_std: Standard deviation vector of local positions (1, J*3, 1)\n\t    :param offsets: Local bone offsets tensor of shape (1, 1, J-1, 3)\n\t    :param parents: List of bone parents indices defining the hierarchy\n\t    :param out_path: optional path for saving the results\n\t    :param n_past: Number of frames used as past context\n\t    :param n_future: Number of frames used as future context (only the first frame is used as the target)\n\t    :return: Results dictionary\n\t    \"\"\"\n", "    # trans_lengths = [ 30,30,30,30]\n\t    n_joints = skeleton.num_joints\n\t    res_quat = {}\n\t    res_pos = {}\n\t    res_npss = {}\n\t    res_contact = {}\n\t    resultQ = {}\n\t    resultX = {}\n\t    for n_trans in trans_lengths:\n\t        torch.cuda.empty_cache()\n", "        print('Computing errors for transition length = {}...'.format(n_trans))\n\t        target_id = n_trans + n_past\n\t        # Format the data for the current transition lengths. The number of samples and the offset stays unchanged.\n\t        curr_window = n_trans + n_past + n_future\n\t        curr_x = X[:, :curr_window, ...]\n\t        curr_q = Q[:, :curr_window, ...]\n\t        # ref_quat = Q[:,curr_window-1:curr_window]\n\t        # pos_offset = offsets.unsqueeze(1)[:,:,1:,:]\n\t        batchsize = curr_x.shape[0]\n\t        gt_local_quats = curr_q\n", "        gt_roots = curr_x  # torch.unsqueeze(curr_x,dim=2)\n\t        gt_local_poses = gt_roots  # torch.cat((gt_roots, gt_offsets), dim=2)\n\t        global_poses, global_quats = skeleton.forward_kinematics(gt_local_quats, offsets, gt_local_poses)\n\t        ref_quat = global_quats[:, n_past - 1:n_past]\n\t        trans_gt_local_poses = gt_local_poses[:, n_past: n_past + n_trans, ...]\n\t        trans_gt_local_quats = gt_local_quats[:, n_past: n_past + n_trans, ...]\n\t        # Local to global with Forward Kinematics (FK)\n\t        trans_gt_global_poses, trans_gt_global_quats = skeleton.forward_kinematics(trans_gt_local_quats, offsets,\n\t                                                                                   trans_gt_local_poses)  # torch_fk(trans_gt_local_quats, trans_gt_local_poses, skeleton.parents)\n\t        trans_gt_global_poses = trans_gt_global_poses.reshape(\n", "            (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        def remove_disconti(quats):\n\t            return remove_quat_discontinuities(torch.cat((ref_quat, quats), dim=1))[:, 1:]\n\t        trans_gt_global_quats = remove_disconti(trans_gt_global_quats)\n\t        # Normalize\n\t        trans_gt_global_poses = (trans_gt_global_poses - x_mean) / x_std\n\t        # trans_gt_global_poses = trans_gt_global_poses.reshape(\n\t        #     (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        # Zero-velocity pos/quats\n\t        zerov_trans_local_quats, zerov_trans_local_poses = torch.zeros_like(trans_gt_local_quats), torch.zeros_like(\n", "            trans_gt_local_poses)\n\t        zerov_trans_local_quats[:, :, :, :] = gt_local_quats[:, n_past - 1:n_past, :, :]\n\t        zerov_trans_local_poses[:, :, :, :] = gt_local_poses[:, n_past - 1:n_past, :, :]\n\t        # To global\n\t        trans_zerov_global_poses, trans_zerov_global_quats = skeleton.forward_kinematics(zerov_trans_local_quats,\n\t                                                                                         offsets,\n\t                                                                                         zerov_trans_local_poses)  # torch_fk(zerov_trans_local_quats, zerov_trans_local_poses, skeleton.parents)\n\t        trans_zerov_global_poses = trans_zerov_global_poses.reshape(\n\t            (trans_zerov_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        # Normalize\n", "        trans_zerov_global_poses = (trans_zerov_global_poses - x_mean) / x_std\n\t        trans_zerov_global_quats = remove_disconti(trans_zerov_global_quats)\n\t        # trans_zerov_global_poses = trans_zerov_global_poses.reshape(\n\t        #     (trans_zerov_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        resultQ['zerov'] = trans_zerov_global_quats\n\t        resultX['zerov'] = trans_zerov_global_poses\n\t        # Interpolation pos/quats\n\t        r, q = curr_x[:, :, :], curr_q\n\t        r = r.cpu().data.numpy()\n\t        q = q.cpu().data.numpy()\n", "        \"\"\"duration test\"\"\"\n\t        inter_root, inter_local_quats = interpolate_local(r, q, n_trans, n_trans + n_past)\n\t        # inter_root, inter_local_quats = interpolate_local(X.unsqueeze(dim=2).data.numpy(), Q.data.numpy(), n_trans, 30+n_past)\n\t        inter_root = torch.Tensor(inter_root)\n\t        inter_local_quats = torch.Tensor(inter_local_quats)\n\t        trans_inter_root = inter_root[:, 1:-1, :, :]\n\t        inter_local_quats = inter_local_quats[:, 1:-1, :, :]\n\t        # To global\n\t        trans_interp_global_poses, trans_interp_global_quats = skeleton.forward_kinematics(inter_local_quats, offsets,\n\t                                                                                           trans_inter_root)  # torch_fk(inter_local_quats, trans_inter_local_poses, skeleton.parents)\n", "        trans_interp_global_poses = trans_interp_global_poses.reshape(\n\t            (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        # Normalize\n\t        trans_interp_global_poses = (trans_interp_global_poses - x_mean) / x_std\n\t        # trans_interp_global_poses = trans_interp_global_poses.reshape(\n\t        #     (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        resultQ['interp'] = trans_interp_global_quats\n\t        resultX['interp'] = trans_interp_global_poses\n\t        # robust motion in between pos/quats\n\t        # model,model_input,x_mean,x_std,pos_offset,parents,length, param)\n", "        for key in models:\n\t            if (key in params):\n\t                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, n_trans, params[key])\n\t            else:\n\t                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, n_trans, {})\n\t            resultQ[key] = resQ\n\t            resultX[key] = resX\n\t        # Local quaternion loss\n\t        if (\"gq\" in benchmarks):\n\t            for key in resultQ:\n", "                resultQ[key] = remove_disconti(\n\t                    resultQ[key].view(batchsize, n_trans, n_joints, 4))  # .flatten(start_dim=-2,end_dim=-1)\n\t            trans_gt_global_quats = remove_disconti(\n\t                trans_gt_global_quats.view(batchsize, n_trans, n_joints, 4))  # .flatten(start_dim=-2,end_dim=-1)\n\t            for key in resultQ:\n\t                res_quat[(key, n_trans)] = torch.mean(\n\t                    torch.sqrt(torch.sum((resultQ[key] - trans_gt_global_quats) ** 2.0, dim=(2, 3))))\n\t        if (\"gp\" in benchmarks):\n\t            for key in resultX:\n\t                res_pos[(key, n_trans)] = torch.mean(\n", "                    torch.sqrt(torch.sum((resultX[key] - trans_gt_global_poses) ** 2.0, dim=1)))\n\t        # NPSS loss on global quaternions\n\t        if (\"npss\" in benchmarks):\n\t            for key in resultQ:\n\t                resultQ[key] = resultQ[key].cpu().data.numpy()\n\t            for key in resultQ:\n\t                res_npss[(key, n_trans)] = fast_npss(flatjoints(trans_gt_global_quats), flatjoints(resultQ[key]))\n\t        # foot skating artifacts\n\t        if (\"contact\" in benchmarks):\n\t            for key in resultX:\n", "                # resultX[key] = resultX[key].reshape(\n\t                #     (resultX[key].shape[0], -1, n_joints , 3))\n\t                resultX[key] = resultX[key] * x_std + x_mean\n\t            # trans_gt_global_poses = trans_gt_global_poses.reshape(\n\t            #     (trans_gt_global_poses.shape[0], -1, n_joints, 3))\n\t            trans_gt_global_poses_unnomarilized = trans_gt_global_poses * x_std + x_mean\n\t            res_contact[('gt', n_trans)] = skating_loss(trans_gt_global_poses_unnomarilized)\n\t            for key in resultX:\n\t                res_contact[(key, n_trans)] = skating_loss(resultX[key])\n\t    ### === Global quat losses ===\n", "    return res_quat, res_pos, res_npss, res_contact\n\tdef print_result(res_quat, res_pos, res_npss, res_contact, trans_lengths, out_path=None):\n\t    def format(dt, name, value):\n\t        s = \"{0: <16}\"\n\t        for i in range(len(value)):\n\t            s += \" & {\" + str(i + 1) + \":\" + dt + \"}\"\n\t        return s.format(name, *value)\n\t    print()\n\t    print(\"=== Global quat losses ===\")\n\t    print(format(\"6d\", \"Lengths\", trans_lengths))\n", "    for key, l in res_quat:\n\t        if (l == trans_lengths[0]):\n\t            avg_loss = [res_quat[(key, n)] for n in trans_lengths]\n\t            print(format(\"6.3f\", key, avg_loss))\n\t    print()\n\t    renderplot('GlobalQuatLosses', 'loss', trans_lengths, res_quat)\n\t    ### === Global pos losses ===\n\t    print(\"=== Global pos losses ===\")\n\t    print(format(\"6d\", \"Lengths\", trans_lengths))\n\t    for key, l in res_pos:\n", "        if l == trans_lengths[0]:\n\t            print(format(\"6.3f\", key, [res_pos[(key, n)] for n in trans_lengths]))\n\t    print()\n\t    renderplot('GlobalPositionLosses', 'loss', trans_lengths, res_pos)\n\t    ### === NPSS on global quats ===\n\t    print(\"=== NPSS on global quats ===\")\n\t    print(format(\"6.4f\", \"Lengths\", trans_lengths))\n\t    for key, l in res_npss:\n\t        if l == trans_lengths[0]:\n\t            print(format(\"6.5f\", key, [res_npss[(key, n)] for n in trans_lengths]))\n", "    renderplot('NPSSonGlobalQuats', 'NPSS', trans_lengths, res_npss)\n\t    ### === Contact loss ===\n\t    print(\"=== Contact loss ===\")\n\t    print(format(\"6d\", \"Lengths\", trans_lengths))\n\t    for key, l in res_contact:\n\t        if l == trans_lengths[0]:\n\t            print(format(\"6.3f\", key, [res_contact[(key, n)] for n in trans_lengths]))\n\t    renderplot('ContactLoss', 'loss', trans_lengths, res_contact)\n\t    # Write to file is desired\n\t    if out_path is not None:\n", "        res_txt_file = open(os.path.join(out_path, 'h36m_transitions_benchmark.txt'), \"a\")\n\t        res_txt_file.write(\"=== Global quat losses ===\\n\")\n\t        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n\t        for key in res_quat:\n\t            res_txt_file.write(format(\"6.3f\", key, [res_quat[(key, n)] for n in trans_lengths]) + \"\\n\")\n\t        res_txt_file.write(\"\\n\")\n\t        res_txt_file.write(\"=== Global pos losses ===\" + \"\\n\")\n\t        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n\t        for key in res_pos:\n\t            res_txt_file.write(format(\"6.3f\", key, [res_pos[(key, n)] for n in trans_lengths]) + \"\\n\")\n", "        res_txt_file.write(\"\\n\")\n\t        res_txt_file.write(\"=== NPSS on global quats ===\" + \"\\n\")\n\t        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n\t        for key in res_npss:\n\t            res_txt_file.write(format(\"6.3f\", key, [res_npss[(key, n)] for n in trans_lengths]) + \"\\n\")\n\t        res_txt_file.write(\"\\n\")\n\t        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n\t        for key in res_contact:\n\t            res_txt_file.write(format(\"6.3f\", key, [res_contact[(key, n)] for n in trans_lengths]) + \"\\n\")\n\t        print(\"\\n\")\n", "        res_txt_file.close()\n\tfrom src.Datasets.Style100Processor import StyleLoader\n\tdef benchmarks():\n\t    loader = mBaseLoader.WindowBasedLoader(65, 25, 1)\n\t    # motionloader = mBaseLoader.MotionDataLoader(lafan1_property)\n\t    style_loader = StyleLoader()\n\t    processor = BenchmarkProcessor()\n\t    style_loader.setup(loader, processor)\n\t    stat_dict = style_loader.load_part_to_binary( \"style100_benchmark_stat\")\n\t    mean, std = stat_dict['pos_stat']  # ,motionloader.test_dict['pos_std'] #load train x_mean, x_std  size (J * 3)\n", "    mean, std = torch.from_numpy(mean).view(23*3,1), torch.from_numpy(std).view(23*3,1)\n\t    style_loader.load_from_binary( \"style100_benchmark_\" + loader.get_postfix_str())\n\t    #style_loader.load_from_binary( \"test+phase_gv10_ori__61_21\" )\n\t    style_keys = list(style_loader.all_motions.keys())[0:90]\n\t    dataSet = BenchmarkDataSet(style_loader.all_motions,style_keys)\n\t    # dataLoader = DataLoader(dataSet, batch_size=len(dataSet),num_workers=0)\n\t    dataLoader = DataLoader(dataSet, 100, num_workers=0)\n\t    style_loader.load_skeleton_only()\n\t    skeleton = style_loader.skeleton\n\t    segment = 1\n", "    num_joints = 22\n\t    benchmarks = [\"gq\", \"gp\", \"npss\", \"contact\"]\n\t    models, function, params = load_model()\n\t    trans_lengths = [5, 15, 30]\n\t    res_quat = None\n\t    for key in models:\n\t        for param in models[key].parameters():\n\t            param.requires_grad = False\n\t    iter = 0\n\t    with torch.no_grad():\n", "        for batch in dataLoader:\n\t            \"\"\"\n\t            :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n\t            :param Q: Local quaternions array of shape (B, T, J, 4)\n\t            \"\"\"\n\t            iter += 1\n\t            X = batch['hip_pos']\n\t            A = batch['A']/0.1\n\t            S = batch['S']\n\t            B = batch['B']\n", "            F = batch['F']\n\t            gp, gq = skeleton.forward_kinematics(batch['quats'], batch['offsets'], batch['hip_pos'])\n\t            tar_gp, tar_gq = skeleton.forward_kinematics(batch['style']['quats'], batch['style']['offsets'], batch['style']['hip_pos'])\n\t            hip_pos = batch['hip_pos']\n\t            local_quat = batch['quats']\n\t            local_pos, global_quat = BatchRotateYCenterXZ().forward(gp, gq, 10)\n\t            tar_pos, tar_quat = BatchRotateYCenterXZ().forward(tar_gp, tar_gq, 10)\n\t            local_quat = skeleton.inverse_kinematics_quats(global_quat)\n\t            local_quat = (remove_quat_discontinuities(local_quat.cpu()))\n\t            hip_pos = local_pos[:, :, 0:1, :]\n", "            quat, pos, npss, contact = benchmark_interpolation(models, function, params, hip_pos, local_quat, A, S, tar_pos, tar_quat, mean, std,\n\t                                                               batch['offsets'], skeleton, trans_lengths, benchmarks)\n\t            if (res_quat == None):\n\t                res_quat, res_pos, res_npss, res_contact = quat, pos, npss, contact\n\t            else:\n\t                for key in res_quat:\n\t                    res_quat[key] += quat[key]\n\t                    res_pos[key] += pos[key]\n\t                    res_npss[key] += npss[key]\n\t                    res_contact[key] += contact[key]\n", "    for key in res_quat:\n\t        res_quat[key] /= iter\n\t        res_pos[key] /= iter\n\t        res_npss[key] /= iter\n\t        res_contact[key] /= iter\n\t    print_result(res_quat, res_pos, res_npss, res_contact, trans_lengths)\n\tdef renderplot(name, ylabel, lengths, res):\n\t    # plt.plot(lengths, interp, label='Interp')\n\t    for key, l in res:\n\t        if l == lengths[0]:\n", "            result = [res[(key, n)] for n in lengths]\n\t            plt.plot(lengths, result, label=key)\n\t    plt.xlabel('Lengths')\n\t    plt.ylabel(ylabel)\n\t    plt.title(name)\n\t    plt.legend()\n\t    plt.savefig(name + '.png')\n\t    plt.close()\n\tdef duration_interpolation(models, function, params, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton: Skeleton,\n\t                            benchmarks, trans_lengths, n_past=10, n_future=10):\n", "    \"\"\"\n\t    Evaluate naive baselines (zero-velocity and interpolation) for transition generation on given data.\n\t    :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n\t    :param Q: Local quaternions array of shape (B, T, J, 4)\n\t    :param x_mean : Mean vector of local positions of shape (1, J*3, 1)\n\t    :param x_std: Standard deviation vector of local positions (1, J*3, 1)\n\t    :param offsets: Local bone offsets tensor of shape (1, 1, J-1, 3)\n\t    :param parents: List of bone parents indices defining the hierarchy\n\t    :param out_path: optional path for saving the results\n\t    :param n_past: Number of frames used as past context\n", "    :param n_future: Number of frames used as future context (only the first frame is used as the target)\n\t    :return: Results dictionary\n\t    \"\"\"\n\t    # trans_lengths = [ 30,30,30,30]\n\t    n_joints = skeleton.num_joints\n\t    res_quat = {}\n\t    res_pos = {}\n\t    res_npss = {}\n\t    res_contact = {}\n\t    resultQ = {}\n", "    resultX = {}\n\t    # trans_lengths = [8,15,60,3000]\n\t    n_trans = 30\n\t    for length in trans_lengths:\n\t        torch.cuda.empty_cache()\n\t        print('Computing errors for transition length = {}...'.format(length))\n\t        target_id = n_trans + n_past\n\t        # Format the data for the current transition lengths. The number of samples and the offset stays unchanged.\n\t        curr_window = n_trans + n_past + n_future\n\t        curr_x = X[:, :curr_window, ...]\n", "        curr_q = Q[:, :curr_window, ...]\n\t        # ref_quat = Q[:,curr_window-1:curr_window]\n\t        # pos_offset = offsets.unsqueeze(1)[:,:,1:,:]\n\t        batchsize = curr_x.shape[0]\n\t        # gt_offsets = pos_offset#.expand(curr_x.shape[0], curr_x.shape[1], num_joints - 1, 3)\n\t        # trans_offsets=gt_offsets[:,0: 1,...].repeat(1,n_trans,1,1)\n\t        # Ground-truth positions/quats/eulers\n\t        gt_local_quats = curr_q\n\t        gt_roots = curr_x  # torch.unsqueeze(curr_x,dim=2)\n\t        gt_local_poses = gt_roots  # torch.cat((gt_roots, gt_offsets), dim=2)\n", "        global_poses, global_quats = skeleton.forward_kinematics(gt_local_quats, offsets, gt_local_poses)\n\t        ref_quat = global_quats[:, n_past - 1:n_past]\n\t        trans_gt_local_poses = gt_local_poses[:, n_past: n_past + n_trans, ...]\n\t        trans_gt_local_quats = gt_local_quats[:, n_past: n_past + n_trans, ...]\n\t        # Local to global with Forward Kinematics (FK)\n\t        trans_gt_global_poses, trans_gt_global_quats = skeleton.forward_kinematics(trans_gt_local_quats, offsets,\n\t                                                                                   trans_gt_local_poses)  # torch_fk(trans_gt_local_quats, trans_gt_local_poses, skeleton.parents)\n\t        trans_gt_global_poses = trans_gt_global_poses.reshape(\n\t            (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        def remove_disconti(quats):\n", "            return remove_quat_discontinuities(torch.cat((ref_quat, quats), dim=1))[:, 1:]\n\t        trans_gt_global_quats = remove_disconti(trans_gt_global_quats)\n\t        # Normalize\n\t        trans_gt_global_poses = (trans_gt_global_poses - x_mean) / x_std\n\t        # trans_gt_global_poses = trans_gt_global_poses.reshape(\n\t        #     (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        # Interpolation pos/quats\n\t        r, q = curr_x[:, :, :], curr_q\n\t        r = r.cpu().data.numpy()\n\t        q = q.cpu().data.numpy()\n", "        \"\"\"duration test\"\"\"\n\t        inter_root, inter_local_quats = interpolate_local(r, q, length, n_trans + n_past)\n\t        # inter_root, inter_local_quats = interpolate_local(X.unsqueeze(dim=2).data.numpy(), Q.data.numpy(), n_trans, 30+n_past)\n\t        inter_root = torch.Tensor(inter_root)\n\t        inter_local_quats = torch.Tensor(inter_local_quats)\n\t        trans_inter_root = inter_root[:, 1:-1, :, :]\n\t        inter_local_quats = inter_local_quats[:, 1:-1, :, :]\n\t        # To global\n\t        trans_interp_global_poses, trans_interp_global_quats = skeleton.forward_kinematics(inter_local_quats, offsets,\n\t                                                                                           trans_inter_root)  # torch_fk(inter_local_quats, trans_inter_local_poses, skeleton.parents)\n", "        trans_interp_global_poses = trans_interp_global_poses.reshape(\n\t            (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        # Normalize\n\t        trans_interp_global_poses = (trans_interp_global_poses - x_mean) / x_std\n\t        # trans_interp_global_poses = trans_interp_global_poses.reshape(\n\t        #     (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\t        resultQ['interp'] = trans_interp_global_quats\n\t        resultX['interp'] = trans_interp_global_poses\n\t        # robust motion in between pos/quats\n\t        # model,model_input,x_mean,x_std,pos_offset,parents,length, param)\n", "        for key in models:\n\t            if (key in params):\n\t                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, length, params[key])\n\t            else:\n\t                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, length, {})\n\t            resultQ[key] = resQ\n\t            resultX[key] = resX\n\t        # foot skating artifacts\n\t        if (\"contact\" in benchmarks):\n\t            for key in resultX:\n", "                # resultX[key] = resultX[key].reshape(\n\t                #     (resultX[key].shape[0], -1, n_joints , 3))\n\t                resultX[key] = resultX[key] * x_std + x_mean\n\t            # trans_gt_global_poses = trans_gt_global_poses.reshape(\n\t            #     (trans_gt_global_poses.shape[0], -1, n_joints, 3))\n\t            trans_gt_global_poses_unnomarilized = trans_gt_global_poses * x_std + x_mean\n\t            res_contact[('gt', length)] = skating_loss(trans_gt_global_poses_unnomarilized)\n\t            for key in resultX:\n\t                res_contact[(key, length)] = skating_loss(resultX[key])\n\t    ### === Global quat losses ===\n", "    return  res_contact\n\tdef duration_test():\n\t    loader = mBaseLoader.WindowBasedLoader(65, 25, 1)\n\t    # motionloader = mBaseLoader.MotionDataLoader(lafan1_property)\n\t    style_loader = StyleLoader()\n\t    #    processor = TransitionProcessor(10)\n\t    processor = BenchmarkProcessor()\n\t    style_loader.setup(loader, processor)\n\t    stat_dict = style_loader.load_part_to_binary( \"style100_benchmark_stat\")\n\t    mean, std = stat_dict['pos_stat']  # ,motionloader.test_dict['pos_std'] #load train x_mean, x_std  size (J * 3)\n", "    mean, std = torch.from_numpy(mean).view(23*3,1), torch.from_numpy(std).view(23*3,1)\n\t    style_loader.load_from_binary( \"style100_benchmark_\" + loader.get_postfix_str())\n\t    #style_loader.load_from_binary( \"test+phase_gv10_ori__61_21\" )\n\t    style_keys = list(style_loader.all_motions.keys())[0:90]\n\t    dataSet = BenchmarkDataSet(style_loader.all_motions,style_keys)\n\t    dataLoader = DataLoader(dataSet, batch_size=len(dataSet),num_workers=0)\n\t    style_loader.load_skeleton_only()\n\t    skeleton = style_loader.skeleton\n\t    benchmarks = [\"contact\"]\n\t    models, function, params = load_model()\n", "    # trans_lengths = [8, 15, 60, 120]\n\t    trans_lengths = [30]\n\t    res_contact = None\n\t    for key in models:\n\t        for param in models[key].parameters():\n\t            param.requires_grad = False\n\t    iter = 0\n\t    with torch.no_grad():\n\t        for batch in dataLoader:\n\t            \"\"\"\n", "            :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n\t            :param Q: Local quaternions array of shape (B, T, J, 4)\n\t            \"\"\"\n\t            iter += 1\n\t            X = batch['hip_pos']\n\t            A = batch['A']/0.1\n\t            S = batch['S']\n\t            B = batch['B']\n\t            F = batch['F']\n\t            gp, gq = skeleton.forward_kinematics(batch['quats'], batch['offsets'], batch['hip_pos'])\n", "            tar_gp, tar_gq = skeleton.forward_kinematics(batch['style']['quats'], batch['style']['offsets'], batch['style']['hip_pos'])\n\t            hip_pos = batch['hip_pos']\n\t            local_quat = batch['quats']\n\t            # X=X.cpu()\n\t            # Q=Q.view(Q.shape[0],Q.shape[1],num_joints,4).cpu()\n\t            local_pos, global_quat = BatchRotateYCenterXZ().forward(gp, gq, 10)\n\t            local_pos[:, 12:, :, [0, 2]] = local_pos[:, 12:, :, [0, 2]] + (\n\t                    local_pos[:, 40:41, 0:1, [0, 2]] - local_pos[:, 10:11, 0:1, [0, 2]]) * 2\n\t            tar_pos, tar_quat = BatchRotateYCenterXZ().forward(tar_gp, tar_gq, 10)\n\t            local_quat = skeleton.inverse_kinematics_quats(global_quat)\n", "            local_quat = (remove_quat_discontinuities(local_quat.cpu()))\n\t            hip_pos = local_pos[:, :, 0:1, :]\n\t            contact = duration_interpolation(models, function, params, hip_pos, local_quat, A, S, tar_pos, tar_quat, mean, std,\n\t                                                               batch['offsets'], skeleton, benchmarks,trans_lengths)\n\t            if (res_contact == None):\n\t                res_contact = contact\n\t            else:\n\t                for key in res_contact:\n\t                    res_contact[key] += contact[key]\n\t    for key in res_contact:\n", "        res_contact[key] /= iter\n\t    def format(dt, name, value):\n\t        s = \"{0: <16}\"\n\t        for i in range(len(value)):\n\t            s += \" & {\" + str(i + 1) + \":\" + dt + \"}\"\n\t        return s.format(name, *value)\n\t    print(\"=== Contact loss ===\")\n\t    print(format(\"6d\", \"Lengths\", trans_lengths))\n\t    for key, l in res_contact:\n\t        if l == trans_lengths[0]:\n", "            print(format(\"6.3f\", key, [res_contact[(key, n)] for n in trans_lengths]))\n\tif __name__ == '__main__':\n\t    benchmarks()\n\t    # duration_test()"]}
{"filename": "train_transitionNet.py", "chunked_list": ["import copy\n\timport os\n\timport re\n\tfrom argparse import ArgumentParser\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom pytorch_lightning import Trainer\n\tfrom pytorch_lightning import loggers\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\tfrom pytorch_lightning.profiler import SimpleProfiler\n", "from pytorch_lightning.utilities.seed import seed_everything\n\tfrom src.Datasets.BaseLoader import WindowBasedLoader\n\tfrom src.Datasets.Style100Processor import StyleLoader, Swap100StyJoints\n\tfrom src.utils import BVH_mod as BVH\n\tfrom src.utils.motion_process import subsample\n\tdef setup_seed(seed:int):\n\t    seed_everything(seed,True)\n\tdef test_model():\n\t    dict = {}\n\t    #dict['fast_dev_run'] = 1 # only run 1 train, val, test batch and program ends\n", "    dict['limit_train_batches'] = 1.\n\t    dict['limit_val_batches'] = 1.\n\t    return dict\n\tdef detect_nan_par():\n\t    '''track_grad_norm\": 'inf'''\n\t    return { \"detect_anomaly\":True}\n\tdef select_gpu_par():\n\t    return {\"accelerator\":'gpu', \"auto_select_gpus\":True, \"devices\":-1}\n\tdef create_common_states(prefix:str):\n\t    log_name = prefix+'/'\n", "    '''test upload'''\n\t    parser = ArgumentParser()\n\t    parser.add_argument(\"--dev_run\", action=\"store_true\")\n\t    parser.add_argument(\"--version\", type=str, default=\"-1\")\n\t    parser.add_argument(\"--epoch\",type=str,default=\"last\")\n\t    parser.add_argument(\"--resume\", action=\"store_true\")\n\t    parser.add_argument(\"--test\",action=\"store_true\")\n\t    parser.add_argument(\"--moe_model\",type=str,default=\"./results/StyleVAE2_style100/myResults/55/m_save_model_332\")\n\t    parser.add_argument(\"--pretrained\",action=\"store_true\")\n\t    parser.add_argument(\"--predict_phase\",action=\"store_true\")\n", "    args = parser.parse_args()\n\t    ckpt_path_prefix = \"results/\"\n\t    if (args.version != \"-1\"):\n\t        version = args.version\n\t    else:\n\t        version = None\n\t    '''Create Loggers tensorboard'''\n\t    if args.dev_run:\n\t        log_name += \"dev_run\"\n\t    else:\n", "        log_name += \"myResults\"\n\t    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"tensorboard_logs/\", name=log_name, version=None)\n\t    #load_ckpt_path = os.path.join(ckpt_path_prefix, prefix+'/myResults', str(version))\n\t    load_ckpt_path = os.path.join(ckpt_path_prefix, prefix+'/myResults', str(version))\n\t    save_ckpt_path = os.path.join(ckpt_path_prefix, log_name, str(tb_logger.version))\n\t    if (args.resume == True):\n\t        check_file = load_ckpt_path+\"/\"\n\t        if (args.epoch == \"last\"):\n\t            check_file += \"last.ckpt\"\n\t        else:\n", "            dirs = os.listdir(check_file)\n\t            for dir in dirs:\n\t                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n\t                out = re.findall(st, dir)\n\t                if (len(out) > 0):\n\t                    check_file += out[0]\n\t                    print(check_file)\n\t                    break\n\t        resume_from_checkpoint = check_file  # results/version/last.ckpt\"\n\t    else:\n", "        resume_from_checkpoint = None\n\t    checkpoint_callback = [ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=-1, save_last=False, every_n_epochs=2,save_weights_only=True),\n\t                           ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=1, monitor=\"val_loss\", save_last=True, every_n_epochs=1,save_weights_only=True),\n\t                          # EMA(0.99)\n\t                           ]\n\t    '''Train'''\n\t    checkpoint_callback[0].CHECKPOINT_NAME_LAST = \"last\"\n\t    profiler = SimpleProfiler()#PyTorchProfiler(filename=\"profiler\")\n\t    trainer_dict = {\n\t        \"callbacks\":checkpoint_callback,\n", "        \"profiler\":profiler,\n\t        \"logger\":tb_logger\n\t    }\n\t    return args,trainer_dict,load_ckpt_path\n\tdef read_style_bvh(style,content,clip=None):\n\t    swap_joints = Swap100StyJoints()\n\t    anim = BVH.read_bvh(os.path.join(\"MotionData/100STYLE/\",style,style+\"_\"+content+\".bvh\"),remove_joints=swap_joints)\n\t    if (clip != None):\n\t        anim.quats = anim.quats[clip[0]:clip[1], ...]\n\t        anim.hip_pos = anim.hip_pos[clip[0]:clip[1], ...]\n", "    anim = subsample(anim,ratio=2)\n\t    return anim\n\tdef training_style100_phase():\n\t    from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n\t    from src.Net.TransitionPhaseNet import TransitionNet_phase,Application_phase\n\t    prefix = \"Transitionv2\"\n\t    data_set = \"style100\"\n\t    prefix += \"_\" + data_set\n\t    args, trainer_dict, ckpt_path = create_common_states(prefix)\n\t    moe_net = torch.load(args.moe_model)\n", "    if(args.pretrained==True):\n\t        from src.utils.locate_model import locate_model\n\t        pretrained_file = locate_model(ckpt_path+\"/\",args.epoch)\n\t        pre_trained = torch.load(pretrained_file)\n\t    else:\n\t        pre_trained = None\n\t    loader = WindowBasedLoader(61, 21, 1)\n\t    dt = 1. / 30.\n\t    phase_dim = 10\n\t    phase_file = \"+phase_gv10\"\n", "    style_file_name = phase_file + WindowBasedLoader(120,0,1).get_postfix_str()\n\t    if (args.test == False):\n\t        '''Create the model'''\n\t        style_loader = StyleLoader()\n\t        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name, dt=dt,\n\t                                         batch_size=32,mirror=0.0) # when apply phase, should avoid mirror\n\t        stat = style_loader.load_part_to_binary(\"motion_statistics\")\n\t        mode = \"pretrain\"\n\t        model = TransitionNet_phase(moe_net, data_module.skeleton, pose_channels=9,stat=stat ,phase_dim=phase_dim,\n\t                               dt=dt,mode=mode,pretrained_model=pre_trained,predict_phase=args.predict_phase)\n", "        if (args.dev_run):\n\t            trainer = Trainer(**trainer_dict, **test_model(),\n\t                              **select_gpu_par(), precision=32,reload_dataloaders_every_n_epochs=1,\n\t                              log_every_n_steps=5, flush_logs_every_n_steps=10,\n\t                              weights_summary='full')\n\t        else:\n\t            trainer = Trainer(**trainer_dict, max_epochs=10000,reload_dataloaders_every_n_epochs=1,gradient_clip_val=1.0,\n\t                              **select_gpu_par(), log_every_n_steps=50,check_val_every_n_epoch=2,\n\t                              flush_logs_every_n_steps=100)\n\t        trainer.fit(model, datamodule=data_module)\n", "    else:\n\t        style_loader = StyleLoader()\n\t        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name, dt=dt,batch_size=32,mirror=0.0)\n\t        data_module.setup()\n\t        check_file = ckpt_path + \"/\"\n\t        if (args.epoch == \"last\"):\n\t            check_file += \"last.ckpt\"\n\t            print(check_file)\n\t        else:\n\t            dirs = os.listdir(check_file)\n", "            for dir in dirs:\n\t                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n\t                out = re.findall(st, dir)\n\t                if (len(out) > 0):\n\t                    check_file += out[0]\n\t                    print(check_file)\n\t                    break\n\t        model = TransitionNet_phase.load_from_checkpoint(check_file, moe_decoder=moe_net, pose_channels=9,phase_dim=phase_dim,\n\t                               dt=dt,mode='fine_tune',strict=False)\n\t        model = model.cuda()\n", "        data_module.mirror = 0\n\t        app = Application_phase(model, data_module)\n\t        model.eval()\n\t        app = app.float()\n\t        key = \"HighKnees\"\n\t        sty_key = \"HighKnees\"\n\t        cid = 61\n\t        sid = 4\n\t        src_motion = app.data_module.test_set.dataset[key][cid]\n\t        target_motion = app.data_module.test_set_sty.dataset[sty_key][sid]\n", "        app.setSource(src_motion)\n\t        app.setTarget(target_motion)\n\t        source = BVH.read_bvh(\"source.bvh\")\n\t        output = copy.deepcopy(source)\n\t        output.hip_pos, output.quats = app.forward(t=2., x=0.)\n\t        BVH.save_bvh(\"test_net.bvh\", output)\n\t        output.hip_pos, output.quats = app.get_source()\n\t        BVH.save_bvh(\"source.bvh\", output)\n\t        torch.save(model, ckpt_path + \"/m_save_model_\" + str(args.epoch))\n\tif __name__ == '__main__':\n", "    setup_seed(3407)\n\t    training_style100_phase()\n"]}
{"filename": "src/__init__.py", "chunked_list": ["import os\n\timport sys\n\tBASEPATH = os.path.dirname(__file__)\n\tsys.path.insert(0, BASEPATH)"]}
{"filename": "src/Datasets/BaseLoader.py", "chunked_list": ["import os\n\timport pickle\n\timport random\n\timport numpy as np\n\timport src.utils.BVH_mod as BVH\n\tfrom src.utils.motion_process import subsample\n\tfrom enum import Enum\n\tclass DataSetType(Enum):\n\t    Train = 0\n\t    Test = 1\n", "'''Loader decides the data structure in the memory'''\n\tdef subsample_dict(data,ratio):\n\t    if(ratio<=1):\n\t        return data\n\t    for key in data.keys():\n\t        if(key!=\"offsets\"):\n\t            data[key] = data[key][::ratio,...]\n\t    return data\n\tclass BasedLoader():\n\t    def __init__(self,sample_ratio:int):\n", "        self.sample_ratio = sample_ratio\n\t    def _append(self, offsets, hip_pos, local_quat):\n\t       # return {\"offsets\":[offsets],\"hip_pos\":[hip_pos],\"quat\":[local_quat]}\n\t        return [offsets],[hip_pos],[local_quat]\n\t        pass\n\t    def _append_dict(self,dict,non_temporal_keys):\n\t        return dict\n\t    def _subsample(self,anim):\n\t        if(self.sample_ratio>1):\n\t            anim = subsample(anim,self.sample_ratio)\n", "        return anim\n\t    def load_anim(self, anim):\n\t        anim = self._subsample(anim)\n\t        return self._append(anim.offsets,anim.hip_pos,anim.quats)\n\t    def load_data(self,offsets,hip_pos,quats):\n\t        return self._append(offsets,hip_pos,quats)\n\t    def load_dict(self,data,non_temporal_keys=[\"offsets\"]):\n\t        data = subsample_dict(data,self.sample_ratio)\n\t        return self._append_dict(data,non_temporal_keys)\n\tclass WindowBasedLoader(BasedLoader):\n", "    def __init__(self,window,overlap,subsample):\n\t        super(WindowBasedLoader, self).__init__(subsample)\n\t        self.window = window\n\t        self.overlap = overlap\n\t    def _append(self,offsets, hip_pos, local_quat):\n\t        # Sliding windows\n\t        step = self.window - self.overlap\n\t        i = 0\n\t        o=[]\n\t        h=[]\n", "        q=[]\n\t        while i + self.window < local_quat.shape[0]:\n\t            clip = lambda x: x[i:i + self.window , ...]\n\t            o.append(offsets[np.newaxis, :, :].astype(np.float32))\n\t            h.append(clip(hip_pos)[np.newaxis, ...].astype(np.float32))\n\t            q.append(clip(local_quat)[np.newaxis, ...].astype(np.float32))\n\t            i += step\n\t        return o,h,q\n\t    def append_dicts(self,dict,non_temporal_keys=[\"offsets\"]):\n\t        temporal_keys = [i for i in dict.keys() if i not in non_temporal_keys]\n", "        length = len(dict[temporal_keys[0]])\n\t        output = {key:[] for key in dict.keys()}\n\t        for i in range(length):\n\t            x = {key:dict[key][i] for key in dict.keys()}\n\t            x = self._append_dict(x,non_temporal_keys)\n\t            for key in dict.keys():\n\t                output[key] = output[key]+x[key]\n\t        return output\n\t    def _append_dict(self,dict,non_temporal_keys=[\"offsets\"]):\n\t        step = self.window - self.overlap\n", "        temporal_keys = [i for i in dict.keys() if i not in non_temporal_keys]\n\t        length = dict[temporal_keys[0]].shape[0]\n\t        i = 0\n\t        output = {key:[] for key in dict.keys()}\n\t        while i + self.window < length:\n\t            clip = lambda x: x[i:i + self.window, ...]\n\t            for key in temporal_keys:\n\t                output[key].append(clip(dict[key])[np.newaxis,:,:].astype(np.float32))\n\t            for key in non_temporal_keys:\n\t                output[key].append(dict[key][np.newaxis,:,:].astype(np.float32))\n", "            i += step\n\t        return output\n\t    def get_postfix_str(self):\n\t        return \"_\"+str(self.window)+\"_\"+str(self.overlap)\n\tclass StreamBasedLoader(BasedLoader):\n\t    def __init__(self,sample_ratio):\n\t        super(StreamBasedLoader, self).__init__(sample_ratio)\n\t        pass\n\t'''Processor decides the training data '''\n\tclass BasedDataProcessor():\n", "    def __init__(self):\n\t        pass\n\t    def _concat(self,offsets,hip_pos,quats):\n\t        offsets = np.concatenate(offsets,axis = 0)\n\t        local_quat = np.concatenate(quats,axis = 0)\n\t        hip_pos = np.concatenate(hip_pos,axis = 0)\n\t        return offsets,hip_pos,local_quat\n\t    def __call__(self, dict,skeleton,motion_data_loader):\n\t        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n\t        return {\"offsets\":offsets,\"hip_pos\":hip_pos,\"quats\":quats}\n", "'''读文件的工具类，如果从bvh文件中读取，则需要配置loader和processor，以做简单的数据处理和元数据计算'''\n\tclass MotionDataLoader():\n\t    def __init__(self,property:dict):\n\t       # self.dict = None\n\t        self.loader= self.processor = None\n\t        self.skeleton = None\n\t        self.dataset_property = property\n\t        self.file_prefix = []\n\t    def setup(self,loader:BasedLoader=None,processor:BasedDataProcessor=None):\n\t        self.loader = loader\n", "        self.processor = processor\n\t    def get_path(self,name:DataSetType):\n\t        if(name==DataSetType.Train):\n\t            return self.dataset_property[\"train_path\"]\n\t        elif(name == DataSetType.Test):\n\t            return self.dataset_property['test_path']\n\t        else:\n\t            assert False\n\t    def _set_dict(self,name:DataSetType,dict):\n\t        if (name == DataSetType.Train):\n", "            self.train_dict = dict\n\t        else:\n\t            self.test_dict = dict\n\t    def get_dict(self,name:DataSetType):\n\t        if (name == DataSetType.Train):\n\t            return self.train_dict\n\t        else:\n\t            return self.test_dict\n\t    def _process_set(self,name:DataSetType):\n\t        path = self.get_path(name)\n", "        remove_joints = (self.dataset_property['remove_joints'])\n\t        if (remove_joints != None):\n\t            remove_joints = remove_joints()\n\t        self.files = []\n\t        self.files = collect_bvh_files(path, self.files)\n\t        self.file_prefix = [0]\n\t        list_count = 0\n\t        o = []\n\t        h = []\n\t        q = []\n", "        # buffer = DataBuffer()\n\t        for file in self.files:\n\t            if file.endswith(\".bvh\"):\n\t                offsets, hip_pos, quats = self._load_from_bvh(file,remove_joints)\n\t                list_count += len(hip_pos)\n\t                self.file_prefix.append(list_count)\n\t                o+=(offsets)\n\t                h+=(hip_pos)\n\t                q+=(quats)\n\t        assert list_count == len(q)\n", "        data = {\"offsets\":o,\"hip_pos\":h,\"quats\":q}\n\t        self._set_dict(name,self.processor(data,self.skeleton,self))\n\t        return o,h,q\n\t    def load_from_bvh_list(self,name:DataSetType):\n\t        # data_file = os.path.join(path,\"data\")\n\t        if(name == DataSetType.Test and self.get_path(DataSetType.Train)==self.get_path(DataSetType.Test)):\n\t            assert False\n\t        return self._process_set(name)\n\t    def load_from_bvh_file(self,name:DataSetType,file):\n\t        path = self.get_path(name)\n", "        remove_joints = (self.dataset_property['remove_joints'])\n\t        if (remove_joints != None):\n\t            remove_joints = remove_joints()\n\t        o = []\n\t        h = []\n\t        q = []\n\t        offsets,hip_pos,quats = self._load_from_bvh(os.path.join(path,file),remove_joints)\n\t        o += (offsets)\n\t        h += (hip_pos)\n\t        q += (quats)\n", "        self._set_dict(name,self.processor(o,h,q,self.skeleton,self))\n\t    def _load_from_bvh(self,file,remove_joints):\n\t        if (self.loader == None or self.processor == None):\n\t            assert False\n\t        #print(file)\n\t        anim = BVH.read_bvh(file, remove_joints=remove_joints, Tpose=-1, remove_gap=self.dataset_property['remove_gap'])\n\t        if (self.skeleton == None):\n\t            self.skeleton = anim.skeleton\n\t        return self.loader.load_anim(anim)\n\t    def get_skeleton(self):\n", "        if(self.skeleton==None):\n\t            self._load_skeleton(self.get_path(DataSetType.Train))\n\t    def _load_skeleton(self,path):\n\t        f = open(path +'/'+ 'skeleton', 'rb')\n\t        self.skeleton = pickle.load(f)\n\t        f.close()\n\t    def _save_skeleton(self,path):\n\t        f = open(path +'/'+ 'skeleton', 'wb+')\n\t        pickle.dump(self.skeleton, f)\n\t        f.close()\n", "    def load_skeleton_only(self,name:DataSetType):\n\t        path = self.get_path(name)\n\t        self._load_skeleton(path)\n\t    def save_part_to_binary(self,name:DataSetType,filename,keys):\n\t        path = self.get_path(name)\n\t        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n\t        dict = self.get_dict(name)\n\t        dict = {key:dict[key] for key in keys}\n\t        f = open(path + '/' + parseName + filename + \".dat\", \"wb+\")\n\t        pickle.dump(dict, f)\n", "        f.close()\n\t    def load_part_from_binary(self,name:DataSetType,filename):\n\t        path = self.get_path(name)\n\t        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n\t        f = open(path + '/' + parseName + filename + \".dat\", 'rb')\n\t        dict = pickle.load(f)\n\t        f.close()\n\t        return dict\n\t    def save_to_binary(self,name:DataSetType,filename):\n\t        path = self.get_path(name)\n", "        self._save_skeleton(path)\n\t        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n\t        dict = self.get_dict(name)\n\t        f = open(path+'/'+parseName+filename+\".dat\",\"wb+\")\n\t        pickle.dump(dict,f)\n\t        f.close()\n\t    def load_from_binary(self,name:DataSetType,filename):\n\t        path = self.get_path(name)\n\t        self._load_skeleton(path)\n\t        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n", "        f = open(path + '/' + parseName + filename + \".dat\", 'rb')\n\t        dict = pickle.load(f)\n\t        f.close()\n\t        self._set_dict(name,dict)\n\tdef collect_bvh_files(path,l:list):\n\t    print(path)\n\t    files = os.listdir(path)\n\t    for file in files:\n\t        file = os.path.join(path,file)\n\t        if os.path.isdir(file):\n", "            l = collect_bvh_files(file,l)\n\t        elif file.endswith(\".bvh\"):\n\t            l.append(file)\n\t    return l\n"]}
{"filename": "src/Datasets/DeepPhaseDataModule.py", "chunked_list": ["import numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\timport torch.utils.data\n\tfrom torch.utils.data import DataLoader\n\tfrom src.Datasets.BaseDataSet import StreamDataSetHelper\n\tfrom src.Datasets.BaseLoader import BasedDataProcessor\n\tfrom src.Datasets.BatchProcessor import BatchProcessData, BatchProcessDatav2\n\tfrom src.Datasets.Style100Processor import StyleLoader\n\tclass DeepPhaseProcessor(BasedDataProcessor):\n", "    def __init__(self,dt):\n\t        super(DeepPhaseProcessor, self).__init__()\n\t        self.process = BatchProcessData()\n\t        self.dt = dt\n\t    def gpu_fk(self,offsets,hip_pos,local_quat,skeleton):\n\t        quat = torch.from_numpy(local_quat).float().cuda()\n\t        offsets = torch.from_numpy(offsets).float().cuda()\n\t        hip_pos = torch.from_numpy(hip_pos).float().cuda()\n\t        gp,gq = skeleton.forward_kinematics(quat,offsets,hip_pos)\n\t        return gp,gq[...,0:1,:]\n", "    def transform_single(self,offsets,hip_pos,local_quat,skeleton):\n\t        gp,gq = self.gpu_fk(offsets,hip_pos,local_quat,skeleton)\n\t        dict = self.process(gq.unsqueeze(0),gp.unsqueeze(0))\n\t        lp = dict['local_pos'][0]\n\t        relative_gv = (lp[:,1:]-lp[:,:-1])/self.dt\n\t        torch.cuda.empty_cache()\n\t        return relative_gv\n\t    #((V_i in R_i) - (V_(i - 1) in R_(i - 1))) / dt,\n\t    def __call__(self, dict,skeleton,motionDataLoader):\n\t        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n", "        dict = {\"gv\":[]}\n\t        if(len(offsets)>1):\n\t            offsets = np.concatenate(offsets,axis=0)\n\t            hip_pos = np.concatenate(hip_pos,axis=0)\n\t            quats = np.concatenate(quats,axis=0)\n\t        else:\n\t            offsets = offsets[0][np.newaxis,...]\n\t            hip_pos = hip_pos[0][np.newaxis,...]\n\t            quats = quats[0][np.newaxis,...]\n\t        gv = self.transform_single(offsets, hip_pos,quats, skeleton)\n", "        gv = gv.cpu().numpy()\n\t        dict['gv']=gv\n\t        return dict\n\tclass DeepPhaseProcessorv2(BasedDataProcessor):\n\t    def __init__(self,dt):\n\t        super(DeepPhaseProcessorv2, self).__init__()\n\t        self.process = BatchProcessDatav2()\n\t        self.dt = dt\n\t    def gpu_fk(self,offsets,hip_pos,local_quat,skeleton):\n\t        quat = torch.from_numpy(local_quat).float().cuda()\n", "        offsets = torch.from_numpy(offsets).float().cuda()\n\t        hip_pos = torch.from_numpy(hip_pos).float().cuda()\n\t        gp,gq = skeleton.forward_kinematics(quat,offsets,hip_pos)\n\t        return gp,gq[...,0:1,:]\n\t    def transform_single(self,offsets,hip_pos,local_quat,skeleton):\n\t        gp,gq = self.gpu_fk(offsets,hip_pos,local_quat,skeleton)\n\t        glb_vel,glb_pos,glb_rot,root_rotatioin = self.process.forward(gq.unsqueeze(0),gp.unsqueeze(0))\n\t        relative_gv = glb_vel[0]/self.dt\n\t        torch.cuda.empty_cache()\n\t        return relative_gv\n", "    #((V_i in R_i) - (V_(i - 1) in R_(i - 1))) / dt,\n\t    def __call__(self, dict,skeleton,motionDataLoader):\n\t        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n\t        dict = {\"gv\":[]}\n\t        all_std = 0\n\t        length = 0\n\t        for i in range(len(offsets)):\n\t            N,J,D = quats[i].shape\n\t            dict[\"gv\"].append(self.transform_single(offsets[i],hip_pos[i],quats[i],skeleton))\n\t            all_std = all_std + dict['gv'][-1].std() * dict['gv'][-1].shape[0]\n", "            length = length + dict['gv'][-1].shape[0]\n\t        all_std = all_std/length\n\t        dict['std'] = all_std\n\t        return dict\n\tclass DeephaseDatasetWindow(torch.utils.data.Dataset):\n\t    def __init__(self,buffer):\n\t        self.buffer = buffer\n\t    def __getitem__(self, item):\n\t        return self.buffer[item]\n\t    def __len__(self):\n", "        return self.buffer.shape[0]\n\tclass DeephaseDataSet(torch.utils.data.Dataset):\n\t    def __init__(self,buffer,window_size):\n\t        self.window = window_size\n\t        # first remove sequence less than window\n\t        idx = [i for i in range(len(buffer)) if buffer[i].shape[0]>=window_size]\n\t        buffer = [buffer[i] for i in idx]\n\t        self.streamer = StreamDataSetHelper(buffer,window_size)\n\t        self.buffer = buffer\n\t    def get_window(self,item):\n", "        idx, frame_idx = self.streamer[item]\n\t        return idx,[frame_idx,frame_idx+self.window]\n\t    def __getitem__(self, item):\n\t        idx,frame_idx = self.streamer[item]\n\t        return self.buffer[idx][frame_idx:frame_idx+self.window]\n\t    def __len__(self):\n\t        return len(self.streamer)\n\tclass Style100DataModule(pl.LightningDataModule):\n\t    def __init__(self,batch_size,shuffle,data_loader:StyleLoader,window_size):\n\t        super(Style100DataModule, self).__init__()\n", "        self.batch_size = batch_size\n\t        self.shuffle = shuffle\n\t        self.data_loader = data_loader\n\t        data_loader.load_skeleton_only()\n\t        self.skeleton = data_loader.skeleton\n\t        self.window = window_size\n\t    def setup(self,stage: [str] = None) :\n\t        use_gv=True\n\t        if(use_gv):\n\t            self.data_loader.load_train_test_dataset(\"deep_phase_gv\")\n", "            self.test_set = DeephaseDatasetWindow(self.data_loader.test_dict['gv'])\n\t            self.train_set = DeephaseDatasetWindow(self.data_loader.train_dict['gv'])\n\t            self.scale = 1.\n\t        else:\n\t            self.data_loader.load_train_test_dataset(\"deep_phase_pca\")\n\t            self.test_set = DeephaseDataSet(self.data_loader.test_dict['pos'], self.window)\n\t            self.train_set = DeephaseDataSet(self.data_loader.train_dict['pos'], self.window)\n\t            self.scale = 1.\n\t        self.val_set = self.test_set\n\t    def train_dataloader(self):\n", "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=self.shuffle, num_workers=0, drop_last=True)\n\t    def val_dataloader(self):  # 需要shuffle确保相同batch内的风格不相同\n\t        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=0, drop_last=True)\n\t    def test_dataloader(self):\n\t        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=16, drop_last=True)\n\t    def on_after_batch_transfer(self, batch, dataloader_idx: int) :\n\t        self.scale= 1.\n\t        return on_after_batch_transfer(batch,self.scale)\n\tdef on_after_batch_transfer(batch,scale):\n\t    use_gv = True\n", "    if (use_gv):\n\t        batch = batch.permute(0, 2, 3, 1).contiguous()\n\t        B, J, D, N = batch.shape\n\t        batch = batch/30.\n\t        mean = torch.mean(batch, dim=(-1), keepdim=True)\n\t        batch = batch - mean\n\t    batch = batch.to(torch.float)\n\t    return batch\n"]}
{"filename": "src/Datasets/StyleVAE_DataModule.py", "chunked_list": ["import numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.data import DataLoader\n\tfrom src.Datasets.BatchProcessor import BatchRotateYCenterXZ\n\tfrom src.Datasets.Style100Processor import StyleLoader\n\tfrom src.Datasets.augmentation import BatchMirror\n\tfrom src.Module.PhaseModule import PhaseOperator\n\tclass Style100Dataset_phase(torch.utils.data.Dataset):\n", "    def __init__(self, dataset:dict,batch_size,is_train,keep_equal=True):\n\t        self.is_train = is_train\n\t        self.dataset = dataset\n\t        self.n_styles = len(self.dataset.keys())\n\t        self.style_ids = list(self.dataset.keys())\n\t        self.expand_()\n\t        self.batch_size = batch_size\n\t        self.equal_style = keep_equal\n\t        size = [len(self.dataset[key]) for key in self.dataset]\n\t        if (keep_equal):\n", "            min_size = min(size)\n\t            min_batches = min_size//batch_size\n\t            self.sizes = [min_batches for i in range(self.n_styles)]\n\t            self.len = min_batches*self.n_styles\n\t        else:\n\t            self.sizes = [size[i]//batch_size for i in range(self.n_styles)]\n\t            self.len = sum(self.sizes)\n\t        self.expand_dataset = {\"data\":[],\"sty\":[]}\n\t        self.shuffle_()\n\t        self.style_to_idx = {}\n", "        for i in range(len(self.style_ids)):\n\t            self.style_to_idx[self.style_ids[i]]=i\n\t        self.style_batch_idx = [0 for i in range(len(self.style_ids))]\n\t        self.style_batch_start = [0]\n\t        for i in range(len(self.sizes)):\n\t            self.style_batch_start.append(self.sizes[i]+self.style_batch_start[i])\n\t        pass\n\t    def get_style_batch_for_train(self,style):\n\t        style_id = self.style_to_idx[style]\n\t        self.style_batch_idx[style_id] += 1\n", "        assert(self.sizes[style_id]>0)\n\t        if(self.style_batch_idx[style_id]>=self.sizes[style_id]):\n\t            self.style_batch_idx[style_id] = 0\n\t        start_idx = self.style_batch_start[style_id]\n\t        idx = self.style_batch_idx[style_id]\n\t        item = start_idx+idx\n\t        return self.expand_dataset['data'][item],self.expand_dataset['sty'][item]\n\t    def get_style_batch(self,style,style_id,batch_size):\n\t        motions = self.dataset[style]\n\t        length = len(motions)\n", "        idx = np.arange(0,length)\n\t        np.random.shuffle(idx)\n\t        sub_idx = idx[:batch_size]\n\t        sub_motions = [motions[j] for j in sub_idx]\n\t        for i in range(len(sub_motions)):\n\t            dict = {key:torch.from_numpy(sub_motions[i][3][key]).unsqueeze(0).cuda() for key in sub_motions[i][3].keys()}\n\t            sub_motions[i] = [torch.from_numpy(sub_motions[i][j]).unsqueeze(0).cuda() for j in range(3)]+[dict]\n\t        return {\"data\":sub_motions,'sty':style_id}\n\t    def expand_(self):\n\t        for style in self.dataset:\n", "            motions = self.dataset[style]\n\t            expand_ = lambda key : sum([motions[con][key] for con in motions],[])\n\t            q = expand_('quats')\n\t            o = expand_('offsets')\n\t            h = expand_('hip_pos')\n\t            A = expand_(\"A\")\n\t            S = expand_(\"S\")\n\t            B = expand_(\"B\")\n\t            F = expand_(\"F\")\n\t            self.dataset[style] = [(q[i],o[i],h[i],{\"A\":A[i],\"S\":S[i],\"B\":B[i],\"F\":F[i]}) for i in range(len(q))]#\n", "    def shuffle_(self):\n\t        #self.expand_dataset['data'].clear()\n\t        #self.expand_dataset['sty'].clear()\n\t        self.expand_dataset = {\"data\": [], \"sty\": []}\n\t        for style_id,style in enumerate(self.dataset):\n\t            motions = self.dataset[style]\n\t            length = len(motions)\n\t            idx = np.arange(0,length)\n\t            np.random.shuffle(idx)\n\t            sub_motions = []\n", "            for i in range(0,len(idx),self.batch_size):\n\t                if(i+self.batch_size>len(idx)):\n\t                    break\n\t                sub_idx = idx[i:i+self.batch_size]\n\t                sub_motions.append([motions[j] for j in sub_idx])\n\t            self.expand_dataset['data']+=(sub_motions[:self.sizes[style_id]])\n\t            self.expand_dataset['sty']+=[self.style_ids[style_id] for i in range(self.sizes[style_id])]\n\t    def __getitem__(self, item):\n\t        return self.expand_dataset['data'][item],self.expand_dataset['sty'][item],self.is_train\n\t    def __len__(self):\n", "        return self.len\n\tclass StyleVAE_DataModule(pl.LightningDataModule):\n\t    def __init__(self,dataloader:StyleLoader,filename,style_file_name,dt, batch_size = 32,shuffle=True,mirror=0.,use_phase = True):\n\t        super(StyleVAE_DataModule, self).__init__()\n\t        self.loader = dataloader\n\t        self.data_file = filename\n\t        self.style_file_name = style_file_name\n\t        self.batch_size = batch_size\n\t        self.shuffle = shuffle\n\t        self.loader.load_skeleton_only()\n", "        self.skeleton = self.loader.skeleton\n\t        self.processor = BatchRotateYCenterXZ()\n\t        self.phase_op = PhaseOperator(dt)\n\t        self.mirror = mirror\n\t        self.batch_mirror = BatchMirror(self.skeleton, mirror_prob=mirror)\n\t        self.use_phase = use_phase\n\t        if(style_file_name==None):\n\t            self.use_sty = False\n\t        else:\n\t            self.use_sty=True\n", "    def prepare_data(self) -> None:\n\t        # download, tokenize, etc\n\t        pass\n\t    def setup(self, stage: [str] = None) -> None:\n\t        self.loader.load_dataset(self.data_file)\n\t        self.train_set = Style100Dataset_phase(self.loader.train_motions, self.batch_size,True)\n\t        self.test_set = Style100Dataset_phase(self.loader.test_motions, self.batch_size,False, keep_equal=False)\n\t        self.val_set = self.test_set\n\t        if(self.use_sty):\n\t            self.loader.load_dataset(self.style_file_name)\n", "            self.train_set_sty = Style100Dataset_phase(self.loader.train_motions, self.batch_size,True,keep_equal=False)\n\t            self.test_set_sty = Style100Dataset_phase(self.loader.test_motions, 4,False, keep_equal=False)\n\t            self.val_set = self.test_set\n\t        pass\n\t    def train_dataloader(self):\n\t        torch.cuda.empty_cache()\n\t        self.train_set.shuffle_()\n\t        cons = DataLoader(self.train_set, batch_size=1, shuffle=self.shuffle, num_workers=0, drop_last=False)\n\t        return cons\n\t    def val_dataloader(self):  # 需要shuffle确保相同batch内的风格不相同\n", "        cons = DataLoader(self.test_set, batch_size=1, shuffle=self.shuffle, num_workers=0, drop_last=False)\n\t        return cons\n\t    def test_dataloader(self):\n\t        cons = DataLoader(self.test_set, batch_size=1, shuffle=self.shuffle, num_workers=0,\n\t                          drop_last=False)\n\t        return cons\n\t    def transfer_mannual(self, batch, dataloader_idx: int, use_phase=True,use_sty=True):\n\t        def get_data(batch, idx):\n\t            data = [batch[0][i][idx].squeeze(1) for i in range(len(batch[0]))]\n\t            data = torch.cat(data, dim=0)\n", "            return data\n\t        def get_phase(batch, str):\n\t            data = [batch[0][i][3][str].squeeze(1) for i in range(len(batch[0]))]\n\t            data = torch.cat(data, dim=0)\n\t            return data\n\t        if ('con' in batch):\n\t            quat = torch.cat((get_data(batch['con'], 0), get_data(batch['sty'], 0)), dim=0)\n\t            hip_pos = torch.cat((get_data(batch['con'], 2), get_data(batch['sty'], 2)), dim=0)\n\t            offsets = torch.cat((get_data(batch['con'], 1), get_data(batch['sty'], 1)), dim=0)\n\t            sty = [batch['con'][1], batch['sty'][1]]\n", "            if (use_phase):\n\t                A = torch.cat((get_phase(batch['con'], \"A\"), get_phase(batch['sty'], 'A')), dim=0)\n\t                S = torch.cat((get_phase(batch['con'], \"S\"), get_phase(batch['sty'], 'S')), dim=0)\n\t                A =  A/0.1\n\t                phase = self.phase_op.phaseManifold(A, S)\n\t            else:\n\t                phase = None\n\t                A = S = None\n\t        else:\n\t            hip_pos = get_data(batch, 2)\n", "            quat = get_data(batch, 0)\n\t            offsets = get_data(batch, 1)\n\t            if (use_phase):\n\t                A = get_phase(batch, 'A')  # ['A']\n\t                S = get_phase(batch, 'S')  # ['S']\n\t                A = A/0.1\n\t                phase = self.phase_op.phaseManifold(A, S)\n\t            else:\n\t                A=S=None\n\t                phase = None\n", "            sty = [batch[1]]\n\t        gp, gq = self.skeleton.forward_kinematics(quat, offsets, hip_pos)\n\t        if(use_sty):\n\t            style_name = sty[0][0]\n\t            is_train = batch[2][0]\n\t            if(is_train==False):# is not train\n\t                style_batch = self.test_set_sty.get_style_batch_for_train(style_name)\n\t            else:\n\t                style_batch = self.train_set_sty.get_style_batch_for_train(style_name)\n\t            assert(style_batch[1]==style_name)\n", "            def get_style_batch(batch,idx):\n\t                style_hip = [batch[0][i][idx] for i in range(len(batch[0]))]\n\t                style_hip = np.concatenate(style_hip,axis=0)\n\t                return torch.from_numpy(style_hip).to(gp.device)\n\t            style_hip = get_style_batch(style_batch,2)\n\t            style_quat = get_style_batch(style_batch, 0)\n\t            style_offsets = get_style_batch(style_batch, 1)\n\t            style_gp,style_gq = self.skeleton.forward_kinematics(style_quat,style_offsets,style_hip)\n\t            if(is_train==False):# the test set is too small so there is not enough batch\n\t                style_gp = style_gp.unsqueeze(0).expand((8,)+style_gp.shape).flatten(0,1)\n", "                style_gq = style_gq.unsqueeze(0).expand((8,)+style_gq.shape).flatten(0,1)\n\t        else:\n\t            style_gp = style_gq = None\n\t        local_pos, local_rot = self.processor.forward(gp, gq, 10)\n\t        return {\"local_pos\": local_pos, \"local_rot\": local_rot, \"offsets\": offsets, \"label\": sty, \"phase\": phase,'A':A,'S':S,\"sty_pos\":style_gp,\"sty_rot\":style_gq}\n\t    def on_after_batch_transfer(self, batch, dataloader_idx: int) :\n\t        return self.transfer_mannual(batch,dataloader_idx,self.use_phase,use_sty=self.use_sty)\n"]}
{"filename": "src/Datasets/__init__.py", "chunked_list": []}
{"filename": "src/Datasets/augmentation.py", "chunked_list": ["import random\n\tfrom typing import Optional\n\timport numpy as np\n\timport torch\n\tfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply, quaternion_invert\n\tfrom src.geometry.quaternions import slerp, from_to_1_0_0\n\tfrom src.geometry.vector import normalize_vector\n\tfrom src.utils.BVH_mod import Skeleton\n\tdef find_Yrotation_to_align_with_Xplus(q):\n\t    '''Warning: the quat must be along y axis'''\n", "    \"\"\"\n\t    :param q: Quats tensor for current rotations (B, 4)\n\t    :return y_rotation: Quats tensor of rotations to apply to q to align with X+\n\t    \"\"\"\n\t    shape = list(q.shape)\n\t    q = q.reshape(-1,4)\n\t    mask = torch.tensor(np.array([[1.0, 0.0, 1.0]]), dtype=q.dtype,device=q.device).expand(q.shape[0], -1)\n\t    ref_vector = torch.tensor(np.array([[1.0, 0.0, 0.0]]), dtype=q.dtype,device=q.device).expand(q.shape[0], -1)\n\t    forward = mask * quaternion_apply(q, ref_vector)\n\t    #forward = normalize_vector(forward)\n", "    #target =  torch.tensor(np.array([[1, 0, 0]]), dtype=torch.float,device=q.device).expand(q.shape[0], -1)\n\t    #y_rotation = normalize_vector(from_to_quaternion(forward, target))\n\t    y_rotation = normalize_vector(from_to_1_0_0(forward))\n\t    return y_rotation.view(shape)\n\tdef angle_between_quats_along_y(from_quat:torch.Tensor,to_quat:torch.Tensor):\n\t    \"\"\"\n\t        Quats tensor for current rotations (B, 4)\n\t        :return\n\t    \"\"\"\n\t    #mask = np.tile(np.array([[1.0, 0.0, 1.0]], dtype=np.float),(from_quat.shape[0],1))\n", "    initial_direction = torch.tensor(np.array([[0.0, 0.0, 1.0]]), dtype=torch.float,device=from_quat.device).expand(from_quat.shape[:-1]+(3,))\n\t    quat = quaternion_multiply(to_quat,quaternion_invert(from_quat))\n\t    dir = quaternion_apply(quat,initial_direction)\n\t    return torch.atan2(dir[...,0],dir[...,2])\n\tclass TemporalScale(torch.nn.Module):\n\t    def __init__(self,prob):\n\t        super(TemporalScale, self).__init__()\n\t        self.prob = prob\n\t    def forward(self,hip_pos,quat):\n\t        if (random.random() > self.prob):\n", "            return hip_pos, quat\n\t            # hip_pos = batch['hip_pos']\n\t            # quat = batch['quat']\n\t        batch_size = hip_pos.shape[0]\n\t        T = hip_pos.shape[1]\n\t        num_joints = quat.shape[2]\n\t        delta_t = random.randint(T // 2, T)\n\t        start_t = random.randint(0, T - delta_t)\n\t        hip_pos = hip_pos[:, start_t:delta_t + start_t, :, :]\n\t        quat = quat[:, start_t:delta_t + start_t, :, :]\n", "        if delta_t < 0.75 * T:\n\t            gamma = random.random() + 1  # gamma \\in [1,2]\n\t        else:\n\t            gamma = random.random() * 0.5 + 0.5  # gamma \\in [0.5,1]\n\t        new_delta_t = int(delta_t * gamma)\n\t        new_hip_pos = torch.zeros((batch_size, new_delta_t, 1, 3), device=hip_pos.device, dtype=hip_pos.dtype)\n\t        new_quat = torch.zeros((batch_size, new_delta_t, num_joints, 4), device=hip_pos.device, dtype=hip_pos.dtype)\n\t        '''scale'''\n\t        ratio = delta_t / new_delta_t\n\t        idx = torch.arange(0, new_delta_t, device=quat.device).type(torch.long)\n", "        ori_frame_step = idx * ratio\n\t        start_frame = ori_frame_step.to(torch.long)\n\t        neighbor_frame = start_frame + 1\n\t        neighbor_frame = torch.where(neighbor_frame >= quat.shape[1], neighbor_frame - 1, neighbor_frame)\n\t        weight = start_frame - ori_frame_step + 1\n\t        weight = weight.view(1, -1, 1, 1)\n\t        new_quat[:, idx, :, :] = slerp(quat[:, start_frame, :, :], quat[:, neighbor_frame, :, :], 1 - weight)\n\t        new_hip_pos[:, idx, :, :] = hip_pos[:, start_frame, :, :] * weight + (1 - weight) * hip_pos[:, neighbor_frame,:, :]\n\t        '''padding or cutting'''\n\t        if new_delta_t > T:\n", "            new_quat = new_quat[:, :T, :, :]\n\t            new_hip_pos = new_hip_pos[:, :T, :, :]\n\t        # elif new_delta_t < self.T:\n\t        #     new_quat = pad_to_window(new_quat, window=self.T)\n\t        #     new_hip_pos = pad_to_window(new_hip_pos, window=self.T)\n\t        return new_hip_pos, new_quat\n\tclass BatchMirror(torch.nn.Module):\n\t    def __init__(self, skeleton, mirror_prob=0.5):\n\t        super().__init__()\n\t        self.mirroring = Mirror(skeleton=skeleton,dtype=torch.float)\n", "        self.mirror_probability = mirror_prob\n\t        self.nb_joints = skeleton.num_joints\n\t    def forward(self, global_pos,global_rot):\n\t        if torch.rand(1)[0] < self.mirror_probability:\n\t            batch_size = global_pos.shape[0]\n\t            original_positions = global_pos.reshape(-1, self.nb_joints, 3)\n\t            original_rotations = global_rot.reshape(-1, self.nb_joints, 4)\n\t            mirrored_positions, mirrored_rotations = self.mirroring.forward(original_positions, original_rotations)\n\t            global_pos = mirrored_positions.view(batch_size, -1, self.nb_joints, 3)\n\t            global_rot = mirrored_rotations.view(batch_size, -1, self.nb_joints, 4)\n", "        return global_pos,global_rot\n\tclass Mirror(torch.nn.Module):\n\t    def __init__(self, skeleton: Skeleton,dtype:torch.dtype):\n\t        super().__init__()\n\t        self.register_buffer('reflection_matrix', torch.eye(3,dtype=dtype))\n\t        # quaternions must be in w, x, y, z order\n\t        if(skeleton.sys==\"x\"):# x\n\t            self.reflection_matrix[0, 0] = -1\n\t            self.register_buffer('quat_indices', torch.tensor([2, 3]))\n\t        elif(skeleton.sys=='z'):# z\n", "            self.reflection_matrix[2, 2] = -1\n\t            self.register_buffer('quat_indices', torch.tensor([1, 2]))\n\t        elif(skeleton.sys=='y'):\n\t            self.reflection_matrix[1,1] = -1\n\t            self.register_buffer('quat_indices', torch.tensor([1, 3]))\n\t        else:\n\t            assert(0,\"can't mirror\")\n\t        self.register_buffer('swap_index_list', torch.LongTensor(skeleton.bone_pair_indices))\n\t    def forward(self, joint_positions: torch.Tensor = None, joint_rotations: torch.Tensor = None):#t position in global space\n\t        self.reflection_matrix = self.reflection_matrix.to(joint_positions.device)\n", "        if joint_positions is None:\n\t            new_positions = None\n\t        else:\n\t            new_positions = self._swap_tensor(joint_positions)\n\t            new_positions = torch.matmul(self.reflection_matrix, new_positions.permute(1, 2, 0)).permute(2, 0, 1)\n\t            new_positions = new_positions.contiguous()\n\t        if joint_rotations is None:\n\t            new_rotations = None\n\t        else:\n\t            new_rotations = self._swap_tensor(joint_rotations)\n", "            new_rotations[:, :, self.quat_indices] *= -1\n\t            new_rotations = new_rotations.contiguous()\n\t        return new_positions, new_rotations\n\t    def _swap_tensor(self, tensor: torch.Tensor) -> torch.Tensor:\n\t        return tensor[:, self.swap_index_list, :].view_as(tensor)\n"]}
{"filename": "src/Datasets/Style100Processor.py", "chunked_list": ["import pickle\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom src.Datasets.BaseLoader import BasedLoader, BasedDataProcessor\n\tfrom src.utils.BVH_mod import read_bvh\n\tfrom src.utils.motion_process import subsample\n\tclass Swap100StyJoints():\n\t    def __call__(self,quats,offsets,parents,names):\n\t        order = [\"Hips\",\n", "                 \"LeftHip\",\"LeftKnee\",\"LeftAnkle\",\"LeftToe\",\n\t                 \"RightHip\",\"RightKnee\",\"RightAnkle\",\"RightToe\",\n\t                 \"Chest\",\"Chest2\",\"Chest3\",\"Chest4\",\"Neck\",\"Head\",\n\t                 \"LeftCollar\",\"LeftShoulder\",\"LeftElbow\",\"LeftWrist\",\n\t                 \"RightCollar\",\"RightShoulder\",\"RightElbow\",\"RightWrist\"]\n\t        #order = {name:i for i,name in enumerate(order)}\n\t        ori_order = {name:i for i,name in enumerate(names)}\n\t        tar_order = {name:i for i,name in enumerate(order)}\n\t        n_quats = np.empty_like(quats)\n\t        n_offsets = np.empty_like(offsets)\n", "        n_parents = parents.copy()\n\t        for i,name in enumerate(order):\n\t            or_idx = ori_order[name]\n\t            n_quats[:, i] = quats[:, or_idx]\n\t            n_offsets[ i] = offsets[ or_idx]\n\t            if(i!=or_idx):\n\t                par_name = names[parents[or_idx]]\n\t                par_id = tar_order[par_name]\n\t            else:\n\t                par_id = parents[or_idx]\n", "            n_parents[i] = par_id\n\t        return n_quats,n_offsets,n_parents,order\n\t#dataset_list = pd.read_csv(root_dir+\"Dataset_List.csv\")\n\t#anim = read_bvh(\"./MotionData/lafan1/train/aiming1_subject4.bvh\")\n\t#root_dir = \"./MotionData/100STYLE/\"\n\t#anim = read_bvh(root_dir+\"Aeroplane/Aeroplane_BR.bvh\",remove_joints=Swap100StyJoints())\n\t#save_bvh(\"example.bvh\",anim)\n\tdef bvh_to_binary():\n\t    root_dir = \"./MotionData/100STYLE/\"\n\t    frame_cuts = pd.read_csv(root_dir + \"Frame_Cuts.csv\")\n", "    n_styles = len(frame_cuts.STYLE_NAME)\n\t    style_name = [frame_cuts.STYLE_NAME[i] for i in range(n_styles)]\n\t    content_name = [\"BR\", \"BW\", \"FR\", \"FW\", \"ID\", \"SR\", \"SW\", \"TR1\", \"TR2\", \"TR3\"]\n\t    def extractSeqRange(start,end):\n\t        start = start.astype('Int64')\n\t        end = end.astype('Int64')\n\t        return [[(start[i]),(end[i])] for i in range(len(start))]\n\t    content_range = {name:extractSeqRange(frame_cuts[name+\"_START\"],frame_cuts[name+\"_STOP\"]) for name in content_name}\n\t    def clip_anim(anim,start,end):\n\t        anim.quats = anim.quats[start:end]\n", "        anim.hip_pos = anim.hip_pos[start:end]\n\t        return anim\n\t    for i in range(n_styles):\n\t        anim_style = {}\n\t        folder = root_dir + style_name[i] + \"/\"\n\t        for content in content_name:\n\t            ran = content_range[content][i]\n\t            if(type(content_range[content][i][0])!=type(pd.NA)):\n\t                file = folder+style_name[i]+\"_\"+content+\".bvh\"\n\t                anim = read_bvh(file,remove_joints=Swap100StyJoints())\n", "                anim = clip_anim(anim,ran[0],ran[1])\n\t                anim = subsample(anim,2)\n\t                anim_style[content] = {\"quats\":anim.quats.astype(np.float32),\"offsets\":anim.offsets.astype(np.float32),\"hips\":anim.hip_pos.astype(np.float32)}\n\t        f = open(folder+\"binary.dat\",\"wb+\")\n\t        pickle.dump(anim_style,f)\n\t        f.close()\n\tdef save_skeleton():\n\t    root_dir = \"./MotionData/100STYLE/\"\n\t    anim = read_bvh(root_dir+\"Aeroplane/Aeroplane_BR.bvh\",remove_joints=Swap100StyJoints())\n\t    f = open(root_dir+\"skeleton\",\"wb+\")\n", "    pickle.dump(anim.skeleton,f)\n\t    f.close()\n\tdef read_binary():\n\t    root_dir = \"./MotionData/100STYLE/\"\n\t    frame_cuts = pd.read_csv(root_dir + \"Frame_Cuts.csv\")\n\t    n_styles = len(frame_cuts.STYLE_NAME)\n\t    style_name = [frame_cuts.STYLE_NAME[i] for i in range(n_styles)]\n\t    content_name = [\"BR\", \"BW\", \"FR\", \"FW\", \"ID\", \"SR\", \"SW\", \"TR1\", \"TR2\", \"TR3\"]\n\t    motion_styles={}\n\t    for i in range(n_styles):\n", "        #anim_style = {}\n\t        folder = root_dir + style_name[i] + \"/\"\n\t        f = open(folder + \"binary.dat\", \"rb\")\n\t        anim_style = pickle.load(f)\n\t        f.close()\n\t        motion_styles[style_name[i]]=anim_style\n\t    return motion_styles,style_name\n\t#bvh_to_binary()\n\t#save_skeleton()\n\tclass StyleLoader():\n", "    def __init__(self,root_dir = \"./MotionData/100STYLE/\"):\n\t        self.root_dir = root_dir\n\t    def setup(self,loader:BasedLoader,processor:BasedDataProcessor):\n\t        self.loader = loader\n\t        self.processor = processor\n\t    def save_part_to_binary(self,filename,keys):\n\t        path = self.root_dir\n\t        dict = {key:self.train_dict[key] for key in keys}\n\t        f = open(path + '/' + filename + \".dat\", \"wb+\")\n\t        pickle.dump(dict, f)\n", "        f.close()\n\t    def load_part_to_binary(self,filename):\n\t        path = \"./\"#self.root_dir\n\t        f = open(path + '/' + filename + \".dat\", \"rb\")\n\t        stat = pickle.load(f)\n\t        f.close()\n\t        return stat\n\t    # dataset: all motions, the motions are splited into windows\n\t    def save_dataset(self,filename):\n\t        path = self.root_dir\n", "        f = open(path + '/train' + filename + \".dat\", \"wb\")\n\t        pickle.dump(self.train_motions, f)\n\t        f.close()\n\t        f = open(path + '/test' + filename + \".dat\", \"wb\")\n\t        pickle.dump(self.test_motions, f)\n\t        f.close()\n\t    def load_dataset(self,filename):\n\t        path = self.root_dir\n\t        f = open(path + '/train' + filename + \".dat\", \"rb\")\n\t        self.train_motions = pickle.load(f)\n", "        f.close()\n\t        f = open(path + '/test' + filename + \".dat\", \"rb\")\n\t        self.test_motions = pickle.load(f)\n\t        f.close()\n\t    # save train set and test set\n\t    def save_train_test_dataset(self,filename):\n\t        path = self.root_dir\n\t        dict = {\"train\": self.train_dict, \"test\": self.test_dict}\n\t        f = open(path+'/'+filename+\".dat\",\"wb+\")\n\t        pickle.dump(dict,f)\n", "        f.close()\n\t    def load_train_test_dataset(self,filename):\n\t        path = self.root_dir\n\t        f = open(path + '/' + filename + \".dat\", \"rb\")\n\t        dict = pickle.load( f)#{\"train\": self.train_dict, \"test\": self.test_dict}\n\t        f.close()\n\t        self.train_dict = dict['train']\n\t        self.test_dict = dict['test']\n\t    def load_style_code(self,filename):\n\t        path = self.root_dir\n", "        f = open(path + '/' + filename + \"_stycode.dat\", \"rb\")\n\t        self.style_codes = pickle.load(f)\n\t        f.close()\n\t    def save_style_code(self,filename,style_codes):\n\t        path = self.root_dir\n\t        f = open(path + '/' + filename + \"_stycode.dat\", \"wb+\")\n\t        pickle.dump(style_codes, f)\n\t        f.close()\n\t    def load_from_binary(self,filename):\n\t        path = self.root_dir\n", "        f = open(path + '/' + filename + \".dat\", \"rb\")\n\t        self.all_motions = pickle.load(f)\n\t        f.close()\n\t    def save_to_binary(self,filename,all_motions):\n\t        path = self.root_dir\n\t        f = open(path + '/' + filename + \".dat\", \"wb+\")\n\t        pickle.dump(all_motions, f)\n\t        f.close()\n\t    def augment_dataset(self):\n\t        from src.Datasets.augmentation import TemporalScale,BatchMirror\n", "        folder = \"./MotionData/100STYLE/\"\n\t        self._load_skeleton(folder)\n\t        mirror = BatchMirror(self.skeleton,1.)\n\t        scale = TemporalScale(1.)\n\t        def augment_motions(motions):\n\t            for style in motions.keys():\n\t                content_keys = list(motions[style].keys())\n\t                for content in content_keys:\n\t                    seq = motions[style][content]\n\t                    quats = torch.from_numpy(seq['quats']).unsqueeze(0).float().cuda()\n", "                    offsets = torch.from_numpy(seq['offsets']).unsqueeze(0).float().cuda()\n\t                    hips = torch.from_numpy(seq['hips']).unsqueeze(0).float().cuda()\n\t                    # mirror\n\t                    gp,gq = self.skeleton.forward_kinematics(quats,offsets,hips)\n\t                    gp,gq = mirror(gp,gq)\n\t                    mirror_hips,mirror_quats = self.skeleton.inverse_kinematics(gq,gp)\n\t                    mirror_hips,mirror_quats = mirror_hips.squeeze(0).cpu().numpy(),mirror_quats.squeeze(0).cpu().numpy()\n\t                    motions[style][\"mr_\"+content] = {\"quats\":mirror_quats,\"offsets\":seq['offsets'],\"hips\":mirror_hips}\n\t                    # scale\n\t                    sc_hips,sc_quats = scale(hips,quats)\n", "                    sc_hips, sc_quats = sc_hips.squeeze(0).cpu().numpy(), sc_quats.squeeze(0).cpu().numpy()\n\t                    motions[style][\"sca_\" + content] = {\"quats\": sc_quats, \"offsets\": seq['offsets'], \"hips\": sc_hips}\n\t            return motions\n\t        f = open(folder + \"train_binary.dat\", 'rb')\n\t        self.train_motions = pickle.load(f)\n\t        f.close()\n\t        f = open(folder + 'test_binary.dat', 'rb')\n\t        self.test_motions = pickle.load(f)\n\t        f.close()\n\t        self.train_motions = augment_motions(self.train_motions)\n", "        self.test_motions = augment_motions(self.test_motions)\n\t        f = open(folder + \"train_binary_agument.dat\", \"wb\")\n\t        pickle.dump(self.train_motions, f)\n\t        f.close()\n\t        f = open(folder + \"test_binary_agument.dat\", \"wb\")\n\t        pickle.dump(self.test_motions, f)\n\t        f.close()\n\t    def split_from_binary(self):\n\t        folder = \"./MotionData/100STYLE/\"\n\t        self.load_skeleton_only()\n", "        self.all_motions, self.style_names = read_binary()\n\t        self.train_motions = {}\n\t        self.test_motions = {}\n\t        # 镜像数据集：\n\t        from src.Datasets.BatchProcessor import BatchMirror\n\t        batch_mirror = BatchMirror(self.skeleton, 1.)\n\t        for style in self.style_names[:-10]:\n\t            self.train_motions[style]={}\n\t            self.test_motions[style]={}\n\t            for content in self.all_motions[style].keys():\n", "                seq = self.all_motions[style][content]\n\t                length = seq['quats'].shape[0]\n\t                if(length>2000):\n\t                    test_length = length//10\n\t                    self.train_motions[style][content]={}\n\t                    self.train_motions[style][content]['quats'] = seq['quats'][:-test_length]\n\t                    self.train_motions[style][content]['offsets'] = seq['offsets']#[:-test_length]\n\t                    self.train_motions[style][content]['hips'] = seq['hips'][:-test_length]\n\t                    self.test_motions[style][content]={}\n\t                    self.test_motions[style][content]['quats'] = seq['quats'][-test_length:]\n", "                    self.test_motions[style][content]['offsets'] = seq['offsets']#[-test_length:]\n\t                    self.test_motions[style][content]['hips'] = seq['hips'][-test_length:]\n\t                else:\n\t                    self.train_motions[style][content] = seq\n\t        for style in self.style_names[-10:]:\n\t            self.test_motions[style] = self.all_motions[style]\n\t        f = open(folder + \"train_binary.dat\", \"wb\")\n\t        pickle.dump(self.train_motions,f)\n\t        f.close()\n\t        f = open(folder+\"test_binary.dat\",\"wb\")\n", "        pickle.dump(self.test_motions,f)\n\t        f.close()\n\t    # read from each file\n\t    def process_from_binary(self,argument = True):\n\t        folder = \"./MotionData/100STYLE/\"\n\t        if(argument):\n\t            f = open(folder+\"train_binary_agument.dat\",'rb')\n\t        else:\n\t            f = open(folder+\"train_binary.dat\",'rb')\n\t        self.train_motions = pickle.load(f)\n", "        f.close()\n\t        if (argument):\n\t            f = open(folder + \"test_binary_agument.dat\", 'rb')\n\t        else:\n\t            f = open(folder + \"test_binary.dat\", 'rb')\n\t        self.test_motions = pickle.load(f)\n\t        f.close()\n\t        self.train_dict = self._process_dataset(self.train_motions)\n\t        self.test_dict = self._process_dataset(self.test_motions)\n\t        print(\"process done\")\n", "        #self.loader.load_data()\n\t    def _process_dataset(self,motions):\n\t        o, h, q, s = [], [], [], []\n\t        for style_name in motions.keys():\n\t            for content_name in motions[style_name]:\n\t                dict = motions[style_name][content_name]\n\t                off, hip, quat = self.loader.load_data(dict['offsets'], dict['hips'], dict['quats'])\n\t                dict['offsets'], dict['hip_pos'], dict['quats'] = off, hip, quat\n\t                del dict['hips']\n\t                o += off\n", "                h += hip\n\t                q += quat\n\t                s += [style_name for i in range(2*len(off))]\n\t                # motions[style_name][content_name] = dict\n\t        self.load_skeleton_only()\n\t        train_set = self.processor({\"offsets\": o, \"hip_pos\": h, \"quats\": q}, self.skeleton, self)\n\t        train_set[\"style\"] = s\n\t        return train_set\n\t    def _load_skeleton(self,path):\n\t        f = open(path +'/'+ 'skeleton', 'rb')\n", "        self.skeleton = pickle.load(f)\n\t        f.close()\n\t    def load_skeleton_only(self):\n\t        self._load_skeleton(self.root_dir)\n"]}
{"filename": "src/Datasets/BatchProcessor.py", "chunked_list": ["import random\n\timport numpy as np\n\timport pytorch3d\n\timport torch.utils.data\n\tfrom pytorch3d.transforms import quaternion_apply, quaternion_multiply\n\tfrom src.Datasets.BaseLoader import BasedDataProcessor\n\tfrom src.Datasets.augmentation import angle_between_quats_along_y\n\tfrom src.geometry.quaternions import quat_to_or6D, or6d_to_quat, from_to_1_0_0\n\tclass BatchProcessDatav2(torch.nn.Module):\n\t    def __init__(self):\n", "        super(BatchProcessDatav2, self).__init__()\n\t    def forward(self,glb_rot,glb_pos):\n\t        dir = glb_pos[..., 5:6, :] - glb_pos[..., 1:2, :]\n\t        ref_vector = torch.cat((-dir[...,2:3],torch.zeros_like(dir[...,1:2]),dir[...,0:1]),dim=-1)\n\t        root_rotation = from_to_1_0_0(ref_vector)\n\t        glb_vel = quaternion_apply(root_rotation[:,:-1], glb_pos[:,1:]-glb_pos[:,:-1])\n\t        glb_rot = quaternion_multiply(root_rotation,glb_rot)\n\t        glb_pos = glb_pos.clone()\n\t        glb_pos[...,[0,2]] = glb_pos[...,[0,2]]-glb_pos[...,0:1,[0,2]]\n\t        glb_pos = quaternion_apply(root_rotation,glb_pos)\n", "        return glb_vel,glb_pos,glb_rot,root_rotation\n\tclass BatchUnProcessDatav2(torch.nn.Module):\n\t    # we don't need use it in training\n\t    def __init__(self):\n\t        super(BatchUnProcessDatav2, self).__init__()\n\t    def forward(self,glb_pos,glb_rot,glb_vel,root_rotation):\n\t        # glb_pos: N,T,J,3\n\t        inverse_rot = pytorch3d.transforms.quaternion_invert(root_rotation)\n\t        glb_pos = quaternion_apply(inverse_rot,glb_pos)\n\t        out_pos = torch.empty(size=glb_rot.shape[:-1]+(3,),dtype=glb_pos.dtype,device=glb_pos.device)\n", "        out_pos[:,0:1,:,:] = glb_pos[:,0:1,:,:]\n\t        glb_vel = quaternion_apply(inverse_rot[:,:-1],glb_vel)\n\t        for i in range(0,glb_vel.shape[1]):\n\t            out_pos[:,i+1,...] = out_pos[:,i]+glb_vel[:,i]\n\t        glb_rot = quaternion_multiply(inverse_rot,glb_rot)\n\t        #glb_pos = out_pos\n\t        glb_pos[...,[0,2]]=out_pos[:,:,0:1,[0,2]]+glb_pos[...,[0,2]]\n\t        return  glb_pos,glb_rot\n\tclass BatchProcessData(torch.nn.Module):#(B,T,J,dim)\n\t    def __init__(self):\n", "        super(BatchProcessData, self).__init__()\n\t    def forward(self,global_rotations,global_positions):\n\t        ref_vector = torch.cross(global_positions[...,5:6,:]-global_positions[...,1:2,:],torch.tensor([0,1,0],dtype=global_positions.dtype,device=global_positions.device),dim=-1)\n\t        root_rotation = from_to_1_0_0(ref_vector)\n\t        \"\"\" Local Space \"\"\"\n\t        local_positions = global_positions.clone()\n\t        local_positions[..., 0] = local_positions[..., 0] - local_positions[..., 0:1, 0]\n\t        local_positions[...,2] = local_positions[..., 2] - local_positions[..., 0:1, 2]\n\t        local_positions = quaternion_apply(root_rotation, local_positions)\n\t        local_velocities = quaternion_apply(root_rotation[:,:-1], (global_positions[:,1:] - global_positions[:,:-1]))\n", "        local_rotations = quaternion_multiply(root_rotation, global_rotations)\n\t        local_rotations = quat_to_or6D(local_rotations)\n\t        if torch.isnan(local_rotations).any():\n\t            print(\"6D\")\n\t            assert False\n\t        root_global_velocity_XZ = global_positions[:,1:, 0:1] - global_positions[:,:-1, 0:1]\n\t        root_global_velocity_XZ[..., 1] = 0\n\t        root_velocity = quaternion_apply(root_rotation[:,:-1, :], (root_global_velocity_XZ))\n\t        #root_rvelocity = angle_between_quats_along_y(global_rotations[:,:-1, 0:1, :], global_rotations[:,1:, 0:1, :])\n\t        root_rvelocity = angle_between_quats_along_y(root_rotation[:,1:, 0:1, :], root_rotation[:,:-1, 0:1, :])\n", "        dict = {\"root_rotation\":root_rotation,\n\t            \"local_pos\":local_positions[:,:,...],\"local_vel\":local_velocities,\"local_rot\":local_rotations[:,:,...],\"root_vel\":root_velocity[...,[0,2]],\"root_rvel\":root_rvelocity.unsqueeze(-1)}\n\t        return dict\n\tclass UnProcessData(torch.nn.Module):\n\t    def __init__(self):\n\t        super(UnProcessData, self).__init__()\n\t    def get_global_rotation(self,root_rvelocity):\n\t        '''root_rvelocity:  N,T,...,C'''\n\t        #r = torch.zeros_like(root_rvelocity)  # BxTx1 or Bx1 or BxTx1x1\n\t        shape = list(root_rvelocity.shape)\n", "        shape[1]+=1\n\t        r = torch.zeros(shape,device=root_rvelocity.device)\n\t        for i in range(1, r.shape[1]):\n\t                r[:, i] = r[:, i - 1] + root_rvelocity[:, i - 1]\n\t        shape = [1 for i in range(r.dim())]\n\t        shape[-1] = 3\n\t        axis = torch.zeros(size = shape,device=r.device)\n\t        axis[...,0:3] = torch.tensor([0,1,0])\n\t        rotation = pytorch3d.transforms.axis_angle_to_quaternion(axis * r)  # B,T,1,4\n\t        return rotation\n", "    def get_global_xz(self,global_hip_rot,hip_velocity):\n\t        root_velocity3D = torch.zeros(size=(hip_velocity.shape[0], hip_velocity.shape[1], 1, 3),device=global_hip_rot.device,dtype=hip_velocity.dtype)\n\t        root_velocity3D[..., [0, 2]] = hip_velocity.clone()\n\t        root_velocity3D = quaternion_apply(global_hip_rot[:,:-1,...], root_velocity3D)\n\t        shape = list(root_velocity3D.shape)\n\t        shape[1]+=1\n\t        root_pos_XZ = torch.zeros(shape,device=root_velocity3D.device)\n\t        for i in range(1, (root_velocity3D.shape[1]+1)):\n\t            root_pos_XZ[:, i, 0, :] = root_pos_XZ[:, i - 1, 0, :] + root_velocity3D[:, i - 1, 0, :]\n\t        return root_pos_XZ\n", "    def forward(self,batch):\n\t        num_joints = batch['local_pos'].shape[-2]\n\t        batch_size = batch['local_pos'].shape[0]\n\t        root_rvelocity = batch['root_rvel']\n\t        local_positions = batch['local_pos']\n\t        local_rotations = batch['local_rot']\n\t        local_rotations = or6d_to_quat(local_rotations)\n\t        root_velocity = batch['root_vel']\n\t        rotation = self.get_global_rotation(root_rvelocity)\n\t        '''transform to global rotation'''\n", "        global_rot = quaternion_multiply(rotation,local_rotations)\n\t        relative_pos = quaternion_apply(rotation,local_positions)\n\t        global_pos_xz = self.get_global_xz(rotation,root_velocity)\n\t        '''transform to global pos'''\n\t        global_pos = relative_pos + global_pos_xz\n\t        #return relative_pos,local_rotations\n\t       # global_pos = local_positions\n\t      #  global_rot = local_rotations\n\t        batch['global_pos'] = global_pos\n\t        batch['global_rot'] = global_rot\n", "        return global_pos,global_rot\n\tclass BatchRotateYCenterXZ(torch.nn.Module):\n\t    def __init__(self):\n\t        super(BatchRotateYCenterXZ, self).__init__()\n\t    def forward(self,global_positions,global_quats,ref_frame_id):\n\t        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n\t                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device).view(1,1,1,3),dim=-1)\n\t        root_rotation = from_to_1_0_0(ref_vector)\n\t        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n\t        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n", "        global_positions = quaternion_apply(root_rotation, global_positions)\n\t        global_quats = quaternion_multiply(root_rotation, global_quats)\n\t        \"\"\" Local Space \"\"\"\n\t        return global_positions,global_quats\n"]}
{"filename": "src/Datasets/BaseDataSet.py", "chunked_list": ["import torch\n\timport torch.utils.data\n\tclass StreamDataSetHelper():\n\t    '''first dimsion of data is seq length'''\n\t    def __init__(self,example_data_list,window_size = 1):\n\t        self.length = 0\n\t        self.prefix_list = [0]\n\t        for i in range(len(example_data_list)):\n\t            self.length+=example_data_list[i].shape[0]-window_size+1\n\t            self.prefix_list.append(self.length)\n", "    def __binary_search(self,idx,start,end):\n\t        middle = (start+end)//2\n\t        if(self.prefix_list[middle]<=idx and self.prefix_list[middle+1]>idx):\n\t            return middle\n\t        elif(self.prefix_list[middle+1]<=idx):\n\t            return self.__binary_search(idx,middle+1,end)\n\t        elif(self.prefix_list[middle]>idx):\n\t            return self.__binary_search(idx,start,middle)\n\t        else:\n\t            assert False\n", "    def __len__(self):\n\t        return self.length\n\t    def __getitem__(self, item):\n\t        results = self.__binary_search(item,0,len(self.prefix_list)-1)\n\t        assert results>=0\n\t        return results,item-self.prefix_list[results]\n"]}
{"filename": "src/Net/DeepPhaseNet.py", "chunked_list": ["from torch import nn\n\tfrom torch._lowrank import pca_lowrank\n\timport pytorch_lightning as pl\n\tfrom src.Net.CommonOperation import CommonOperator\n\tfrom src.utils.Drawer import Drawer\n\tfrom src.Datasets.DeepPhaseDataModule import DeephaseDataSet\n\timport numpy as np\n\tfrom torch.optim.optimizer import Optimizer\n\tclass AdamW(Optimizer):\n\t    \"\"\"Implements Adam algorithm.\n", "    Arguments:\n\t        params (iterable): iterable of parameters to optimize or dicts defining\n\t            parameter groups\n\t        lr (float, optional): learning rate (default: 1e-3)\n\t        betas (Tuple[float, float], optional): coefficients used for computing\n\t            running averages of gradient and its square (default: (0.9, 0.999))\n\t        eps (float, optional): term added to the denominator to improve\n\t            numerical stability (default: 1e-8)\n\t        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\t        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n", "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\t    \"\"\"\n\t    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n\t                 weight_decay=0, amsgrad=False):\n\t        if not 0.0 <= betas[0] < 1.0:\n\t            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n\t        if not 0.0 <= betas[1] < 1.0:\n\t            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n\t        defaults = dict(lr=lr, betas=betas, eps=eps,\n\t                        weight_decay=weight_decay, amsgrad=amsgrad)\n", "        #super(AdamW, self).__init__(params, defaults)\n\t        super().__init__(params, defaults)\n\t    def step(self, closure=None):\n\t        \"\"\"Performs a single optimization step.\n\t        Arguments:\n\t            closure (callable, optional): A closure that reevaluates the model\n\t                and returns the loss.\n\t        \"\"\"\n\t        loss = None\n\t        if closure is not None:\n", "            loss = closure()\n\t        for group in self.param_groups:\n\t            for p in group['params']:\n\t                if p.grad is None:\n\t                    continue\n\t                grad = p.grad.data\n\t                if grad.is_sparse:\n\t                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\t                amsgrad = group['amsgrad']\n\t                state = self.state[p]\n", "                # State initialization\n\t                if len(state) == 0:\n\t                    state['step'] = 0\n\t                    # Exponential moving average of gradient values\n\t                    state['exp_avg'] = torch.zeros_like(p.data)\n\t                    # Exponential moving average of squared gradient values\n\t                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\t                    if amsgrad:\n\t                        # Maintains max of all exp. moving avg. of sq. grad. values\n\t                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n", "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n\t                if amsgrad:\n\t                    max_exp_avg_sq = state['max_exp_avg_sq']\n\t                beta1, beta2 = group['betas']\n\t                state['step'] += 1\n\t                # Decay the first and second moment running average coefficient\n\t                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\t                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\t                if amsgrad:\n\t                    # Maintains the maximum of all 2nd moment running avg. till now\n", "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n\t                    # Use the max. for normalizing running avg. of gradient\n\t                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n\t                else:\n\t                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\t                bias_correction1 = 1 - beta1 ** state['step']\n\t                bias_correction2 = 1 - beta2 ** state['step']\n\t                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\t                p.data.mul_(1 - group['weight_decay']).addcdiv_(exp_avg, denom, value=-step_size)\n\t        return loss\n", "from torch.optim import Optimizer\n\tfrom torch.optim.lr_scheduler import _LRScheduler\n\timport math\n\timport torch\n\tclass ReduceMaxLROnRestart:\n\t    def __init__(self, ratio=0.75):\n\t        self.ratio = ratio\n\t    def __call__(self, eta_min, eta_max):\n\t        return eta_min, eta_max * self.ratio\n\tclass ExpReduceMaxLROnIteration:\n", "    def __init__(self, gamma=1):\n\t        self.gamma = gamma\n\t    def __call__(self, eta_min, eta_max, iterations):\n\t        return eta_min, eta_max * self.gamma ** iterations\n\tclass CosinePolicy:\n\t    def __call__(self, t_cur, restart_period):\n\t        return 0.5 * (1. + math.cos(math.pi *\n\t                                    (t_cur / restart_period)))\n\tclass ArccosinePolicy:\n\t    def __call__(self, t_cur, restart_period):\n", "        return (math.acos(max(-1, min(1, 2 * t_cur\n\t                                      / restart_period - 1))) / math.pi)\n\tclass TriangularPolicy:\n\t    def __init__(self, triangular_step=0.5):\n\t        self.triangular_step = triangular_step\n\t    def __call__(self, t_cur, restart_period):\n\t        inflection_point = self.triangular_step * restart_period\n\t        point_of_triangle = (t_cur / inflection_point\n\t                             if t_cur < inflection_point\n\t                             else 1.0 - (t_cur - inflection_point)\n", "                             / (restart_period - inflection_point))\n\t        return point_of_triangle\n\tclass CyclicLRWithRestarts(_LRScheduler):\n\t    \"\"\"Decays learning rate with cosine annealing, normalizes weight decay\n\t    hyperparameter value, implements restarts.\n\t    https://arxiv.org/abs/1711.05101\n\t    Args:\n\t        optimizer (Optimizer): Wrapped optimizer.\n\t        batch_size: minibatch size\n\t        epoch_size: training samples per epoch\n", "        restart_period: epoch count in the first restart period\n\t        t_mult: multiplication factor by which the next restart period will expand/shrink\n\t        policy: [\"cosine\", \"arccosine\", \"triangular\", \"triangular2\", \"exp_range\"]\n\t        min_lr: minimum allowed learning rate\n\t        verbose: print a message on every restart\n\t        gamma: exponent used in \"exp_range\" policy\n\t        eta_on_restart_cb: callback executed on every restart, adjusts max or min lr\n\t        eta_on_iteration_cb: callback executed on every iteration, adjusts max or min lr\n\t        triangular_step: adjusts ratio of increasing/decreasing phases for triangular policy\n\t    Example:\n", "        >>> scheduler = CyclicLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\n\t        >>> for epoch in range(100):\n\t        >>>     scheduler.step()\n\t        >>>     train(...)\n\t        >>>         ...\n\t        >>>         optimizer.zero_grad()\n\t        >>>         loss.backward()\n\t        >>>         optimizer.step()\n\t        >>>         scheduler.batch_step()\n\t        >>>     validate(...)\n", "    \"\"\"\n\t    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\n\t                 t_mult=2, last_epoch=-1, verbose=False,\n\t                 policy=\"cosine\", policy_fn=None, min_lr=1e-7,\n\t                 eta_on_restart_cb=None, eta_on_iteration_cb=None,\n\t                 gamma=1.0, triangular_step=0.5):\n\t        if not isinstance(optimizer, Optimizer):\n\t            raise TypeError('{} is not an Optimizer'.format(\n\t                type(optimizer).__name__))\n\t        self.optimizer = optimizer\n", "        if last_epoch == -1:\n\t            for group in optimizer.param_groups:\n\t                group.setdefault('initial_lr', group['lr'])\n\t                group.setdefault('minimum_lr', min_lr)\n\t        else:\n\t            for i, group in enumerate(optimizer.param_groups):\n\t                if 'initial_lr' not in group:\n\t                    raise KeyError(\"param 'initial_lr' is not specified \"\n\t                                   \"in param_groups[{}] when resuming an\"\n\t                                   \" optimizer\".format(i))\n", "        self.base_lrs = [group['initial_lr'] for group\n\t                         in optimizer.param_groups]\n\t        self.min_lrs = [group['minimum_lr'] for group\n\t                        in optimizer.param_groups]\n\t        self.base_weight_decays = [group['weight_decay'] for group\n\t                                   in optimizer.param_groups]\n\t        self.policy = policy\n\t        self.eta_on_restart_cb = eta_on_restart_cb\n\t        self.eta_on_iteration_cb = eta_on_iteration_cb\n\t        if policy_fn is not None:\n", "            self.policy_fn = policy_fn\n\t        elif self.policy == \"cosine\":\n\t            self.policy_fn = CosinePolicy()\n\t        elif self.policy == \"arccosine\":\n\t            self.policy_fn = ArccosinePolicy()\n\t        elif self.policy == \"triangular\":\n\t            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n\t        elif self.policy == \"triangular2\":\n\t            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n\t            self.eta_on_restart_cb = ReduceMaxLROnRestart(ratio=0.5)\n", "        elif self.policy == \"exp_range\":\n\t            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n\t            self.eta_on_iteration_cb = ExpReduceMaxLROnIteration(gamma=gamma)\n\t        self.last_epoch = last_epoch\n\t        self.batch_size = batch_size\n\t        self.epoch_size = epoch_size\n\t        self.iteration = 0\n\t        self.total_iterations = 0\n\t        self.t_mult = t_mult\n\t        self.verbose = verbose\n", "        self.restart_period = math.ceil(restart_period)\n\t        self.restarts = 0\n\t        self.t_epoch = -1\n\t        self.epoch = -1\n\t        self.eta_min = 0\n\t        self.eta_max = 1\n\t        self.end_of_period = False\n\t        self.batch_increments = []\n\t        self._set_batch_increment()\n\t    def _on_restart(self):\n", "        if self.eta_on_restart_cb is not None:\n\t            self.eta_min, self.eta_max = self.eta_on_restart_cb(self.eta_min,\n\t                                                                self.eta_max)\n\t    def _on_iteration(self):\n\t        if self.eta_on_iteration_cb is not None:\n\t            self.eta_min, self.eta_max = self.eta_on_iteration_cb(self.eta_min,\n\t                                                                  self.eta_max,\n\t                                                                  self.total_iterations)\n\t    def get_lr(self, t_cur):\n\t        eta_t = (self.eta_min + (self.eta_max - self.eta_min)\n", "                 * self.policy_fn(t_cur, self.restart_period))\n\t        weight_decay_norm_multi = math.sqrt(self.batch_size /\n\t                                            (self.epoch_size *\n\t                                             self.restart_period))\n\t        lrs = [min_lr + (base_lr - min_lr) * eta_t for base_lr, min_lr\n\t               in zip(self.base_lrs, self.min_lrs)]\n\t        weight_decays = [base_weight_decay * eta_t * weight_decay_norm_multi\n\t                         for base_weight_decay in self.base_weight_decays]\n\t        if (self.t_epoch + 1) % self.restart_period < self.t_epoch:\n\t            self.end_of_period = True\n", "        if self.t_epoch % self.restart_period < self.t_epoch:\n\t            if self.verbose:\n\t                print(\"Restart {} at epoch {}\".format(self.restarts + 1,\n\t                                                      self.last_epoch))\n\t            self.restart_period = math.ceil(self.restart_period * self.t_mult)\n\t            self.restarts += 1\n\t            self.t_epoch = 0\n\t            self._on_restart()\n\t            self.end_of_period = False\n\t        return zip(lrs, weight_decays)\n", "    def _set_batch_increment(self):\n\t        d, r = divmod(self.epoch_size, self.batch_size)\n\t        batches_in_epoch = d + 2 if r > 0 else d + 1\n\t        self.iteration = 0\n\t        self.batch_increments = torch.linspace(0, 1, batches_in_epoch).tolist()\n\t    def step(self):\n\t        self.last_epoch += 1\n\t        self.t_epoch += 1\n\t        self._set_batch_increment()\n\t        self.batch_step()\n", "    def batch_step(self):\n\t        try:\n\t            t_cur = self.t_epoch + self.batch_increments[self.iteration]\n\t            self._on_iteration()\n\t            self.iteration += 1\n\t            self.total_iterations += 1\n\t        except (IndexError):\n\t            raise StopIteration(\"Epoch size and batch size used in the \"\n\t                                \"training loop and while initializing \"\n\t                                \"scheduler should be the same.\")\n", "        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,\n\t                                                   self.get_lr(t_cur)):\n\t            param_group['lr'] = lr\n\t            param_group['weight_decay'] = weight_decay\n\t        return lr\n\tdef PhaseManifold(A,S):\n\t    shape = list(A.shape)\n\t    shape[-1]*=2\n\t    output = torch.empty((shape))\n\t    output[...,::2] = A*torch.cos(2*torch.pi*S)\n", "    output[...,1::2] = A*torch.sin(2*torch.pi*S)\n\t    return output\n\tclass PAE_AI4Animation(nn.Module):\n\t    def __init__(self,n_phases, n_joints,length, key_range=1., window=2.0):\n\t        super(PAE_AI4Animation, self).__init__()\n\t        embedding_channels = n_phases\n\t        input_channels = (n_joints)*3\n\t        time_range = length\n\t        self.n_phases = n_phases\n\t        self.input_channels = input_channels\n", "        self.embedding_channels = embedding_channels\n\t        self.time_range = time_range\n\t        self.key_range = key_range\n\t        self.window = window\n\t        self.time_scale = key_range / time_range\n\t        self.tpi = nn.Parameter(torch.from_numpy(np.array([2.0 * np.pi], dtype=np.float32)), requires_grad=False)\n\t        self.args = nn.Parameter(\n\t            torch.from_numpy(np.linspace(-self.window / 2, self.window / 2, self.time_range, dtype=np.float32)),\n\t            requires_grad=False)\n\t        self.freqs = nn.Parameter(torch.fft.rfftfreq(time_range)[1:] * (time_range * self.time_scale) / self.window,\n", "                               requires_grad=False)  # Remove DC frequency\n\t        intermediate_channels = int(input_channels / 3)\n\t        self.conv1 = nn.Conv1d(input_channels, intermediate_channels, time_range, stride=1,\n\t                               padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True, padding_mode='zeros')\n\t        self.bn_conv1 = nn.BatchNorm1d(num_features=intermediate_channels)\n\t        self.conv2 = nn.Conv1d(intermediate_channels, embedding_channels, time_range, stride=1,\n\t                               padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True, padding_mode='zeros')\n\t        self.bn_conv2 = nn.BatchNorm1d(num_features=embedding_channels)\n\t        self.fc = torch.nn.ModuleList()\n\t        self.bn = torch.nn.ModuleList()\n", "        for i in range(embedding_channels):\n\t            self.fc.append(nn.Linear(time_range, 2))\n\t            self.bn.append(nn.BatchNorm1d(num_features=2))\n\t        self.parallel_fc0 = nn.Linear(time_range,embedding_channels)\n\t        self.parallel_fc1 = nn.Linear(time_range,embedding_channels)\n\t        self.deconv1 = nn.Conv1d(embedding_channels, intermediate_channels, time_range, stride=1,\n\t                                 padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True,\n\t                                 padding_mode='zeros')\n\t        self.bn_deconv1 = nn.BatchNorm1d(num_features=intermediate_channels)\n\t        self.deconv2 = nn.Conv1d(intermediate_channels, input_channels, time_range, stride=1,\n", "                                 padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True,\n\t                                 padding_mode='zeros')\n\t    def atan2(self, y, x):\n\t        tpi = self.tpi\n\t        ans = torch.atan(y / x)\n\t        ans = torch.where((x < 0) * (y >= 0), ans + 0.5 * tpi, ans)\n\t        ans = torch.where((x < 0) * (y < 0), ans - 0.5 * tpi, ans)\n\t        return ans\n\t    # Returns the frequency for a function over a time window in s\n\t    def FFT(self, function, dim):\n", "        rfft = torch.fft.rfft(function, dim=dim)\n\t        magnitudes = rfft.abs()\n\t        spectrum = magnitudes[:, :, 1:]  # Spectrum without DC component\n\t        power = spectrum ** 2\n\t        # Frequency\n\t        freq = torch.sum(self.freqs * power, dim=dim) / torch.sum(power, dim=dim)\n\t        freq = freq / self.time_scale\n\t        # Amplitude\n\t        amp = 2 * torch.sqrt(torch.sum(power, dim=dim)) / self.time_range\n\t        # Offset\n", "        offset = rfft.real[:, :, 0] / self.time_range  # DC component\n\t        return freq, amp, offset\n\t    def forward(self, x):\n\t        y = x\n\t        # Signal Embedding\n\t        y = y.reshape(y.shape[0], self.input_channels, self.time_range)\n\t        y = self.conv1(y)\n\t        y = self.bn_conv1(y)\n\t        y = torch.tanh(y)\n\t        y = self.conv2(y)\n", "        y = self.bn_conv2(y)\n\t        y = torch.tanh(y)\n\t        latent = y  # Save latent for returning\n\t        # Frequency, Amplitude, Offset\n\t        f, a, b = self.FFT(y, dim=2)\n\t        # Phase\n\t        sx = self.parallel_fc0(y).diagonal(dim1=-2,dim2=-1).unsqueeze(-1).contiguous()\n\t        sy = self.parallel_fc1(y).diagonal(dim1=-2,dim2=-1).unsqueeze(-1).contiguous()\n\t        v = torch.cat([sx,sy],dim=-1) # B x M x 2\n\t        tv = torch.empty_like(v)\n", "        for i in range(self.embedding_channels):\n\t            tv[:,i,:] = self.bn[i](v[:,i,:])\n\t        p = self.atan2(tv[:,:,1],tv[:,:,0])/self.tpi\n\t        #################### the original code ####################\n\t        # p = torch.empty((y.shape[0], self.embedding_channels), dtype=torch.float32, device=y.device)\n\t        # for i in range(self.embedding_channels):\n\t        #     v = self.fc[i](y[:, i, :])\n\t        #     v = self.bn[i](v)\n\t        #     p[:, i] = self.atan2(v[:, 1], v[:, 0]) / self.tpi\n\t        ###########################################################\n", "        # Parameters\n\t        p = p.unsqueeze(2)\n\t        f = f.unsqueeze(2)\n\t        a = a.unsqueeze(2)\n\t        b = b.unsqueeze(2)\n\t        params = [p, f, a, b]  # Save parameters for returning\n\t        # Latent Reconstruction\n\t        y = a * torch.sin(self.tpi * (f * self.args + p)) + b\n\t        signal = y  # Save signal for returning\n\t        # Signal Reconstruction\n", "        y = self.deconv1(y)\n\t        y = self.bn_deconv1(y)\n\t        y = torch.tanh(y)\n\t        y = self.deconv2(y)\n\t        return y, p, a, f, b\n\tclass DeepPhaseNet(pl.LightningModule):\n\t    def __init__(self,n_phase,skeleton,length,dt,batch_size):\n\t        super(DeepPhaseNet, self).__init__()\n\t       # self.automatic_optimization = False\n\t        self.save_hyperparameters(ignore=['style_loader'])\n", "        self.lr = 1e-3\n\t        self.weight_decay = 1e-4\n\t        self.dt = dt\n\t        self.skeleton  = skeleton\n\t        self.model = PAE_AI4Animation(n_phase,skeleton.num_joints,length)\n\t        self.mse_loss = nn.MSELoss()\n\t        self.oper = CommonOperator(batch_size)\n\t    def transform_to_pca(self,input):\n\t        # input: N,T,J,D\n\t        input = input*self.dt\n", "        std = torch.std(input,dim=[-2,-1],keepdim=True)\n\t        mean = torch.mean(input,dim=[-2,-1],keepdim=True)\n\t        input = (input-mean)/std\n\t        input = input.flatten(1,2)\n\t        return input\n\t    def forward(self,input):\n\t        input = input.flatten(1,2)\n\t        Y,S,A,F,B = self.model(input)\n\t        loss ={ \"loss\":self.mse_loss(input,Y)}\n\t        return loss,input,Y\n", "    def training_step(self, batch,batch_idx):\n\t        loss,_,_ = self.forward(batch)\n\t        self.oper.log_dict(self,loss,\"train_\")\n\t        return loss['loss']\n\t    def validation_step(self, batch,batch_idx) :\n\t        loss,input,Y = self.forward(batch)\n\t        self.oper.log_dict(self, loss, \"val_\")\n\t        return loss['loss']\n\t    def test_step(self,batch,batch_idx):\n\t        loss = self.forward(batch)\n", "        self.oper.log_dict(self,loss,\"test_\")\n\t        return loss['loss']\n\t    def configure_optimizers(self):\n\t        optimizer = AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n\t        scheduler = CyclicLRWithRestarts(optimizer=optimizer, batch_size=32,\n\t                                          epoch_size=75830, restart_period=10,\n\t                                          t_mult=2, policy=\"cosine\", verbose=True)\n\t        return [optimizer], {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n\t'''增加一个loss计算，确保输入和网络训练的时候是一致的'''\n\tfrom src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\n", "class Application(nn.Module):\n\t    def __init__(self,Net:DeepPhaseNet,datamodule):\n\t        super(Application, self).__init__()\n\t        self.Net = Net\n\t        self.drawer = Drawer()\n\t        self.module = datamodule\n\t        self.processor = DeepPhaseProcessor (1/30.)\n\t        self.window = datamodule.window\n\t    def draw_phase(self,label,input,channel,feature):\n\t        '''input: Batch x M x 1'''\n", "        self.drawer.ax[channel,feature].set_xlabel(label)\n\t        self.drawer.draw(input.cpu().numpy(),channel=channel,feature=feature)\n\t    def draw_dict(self,input:dict):\n\t        size = len(input)\n\t        print(self.Net.model.n_phases)\n\t        self.drawer.create_canvas(self.Net.model.n_phases,size)\n\t        feature_id = 0\n\t        for key in input.keys():\n\t            data = input[key]\n\t            for phase in range(data.shape[1]):\n", "                self.draw_phase(key,data[:,phase],phase,feature_id)\n\t            feature_id+=1\n\t    def _transform_anim(self,anim):\n\t        offsets,hip_pos,quats = anim.offsets,anim.hip_pos,anim.quats#self.loader(anim)\n\t        offsets = np.expand_dims(offsets,axis=0)\n\t        hip_pos = np.expand_dims(hip_pos,axis=0)\n\t        quats = np.expand_dims(quats,axis=0)\n\t        gv = self.processor.transform_single(offsets,hip_pos,quats,self.module.skeleton)\n\t        batch = gv\n\t        return batch\n", "    def setAnim(self,anim):\n\t        self.input = self._transform_anim(anim).to(torch.float32)#.cpu()\n\t        self.dataset = DeephaseDataSet(self.input,self.window)\n\t    def forward_window(self,batch):\n\t        input = batch\n\t        input = self.module.on_after_batch_transfer(input,0)\n\t        return self.calculate_phase(input)\n\t    def calculate_FFT(self,embedding):\n\t        N = embedding.shape[-1]\n\t        model = self.Net.model\n", "        A, F, B = model.fft_layer(embedding, N)\n\t        return A,F,B\n\t    def calculate_phase(self,x):\n\t        N = x.shape[-1]\n\t        model = self.Net.model\n\t        self.Net = self.Net.to(self.Net.device)\n\t        x = x.to(self.Net.device)\n\t        self.Net.eval()\n\t        x = x.flatten(1,2 )\n\t        Y,S,A,F,B = model.forward(x)\n", "        print(\"mseLoss:{}\".format(self.Net.mse_loss(Y,x)))\n\t        return {\n\t            \"S\":S,\"A\":A,\"F\":F,\"B\":B\n\t        }\n\t    def calculate_statistic_for_dataset(self,dataset):\n\t        self.Net.eval()\n\t        with torch.no_grad():\n\t            phases = {\"F\": [], \"A\": [], \"S\": [], \"B\": []}\n\t            step = 1\n\t            length = len(dataset)\n", "            input = []\n\t            for i in range(length):\n\t                input.append(dataset[i].unsqueeze(0))\n\t            batch = torch.cat(input, 0)\n\t            batch = batch.to(\"cuda\")\n\t            phase = self.forward_window(batch)\n\t            for key in phase.keys():\n\t                phase[key] = phase[key].cpu().numpy()\n\t            return phase\n\t    def forward(self):\n", "        import matplotlib.pyplot as plt\n\t        self.Net.eval()\n\t        with torch.no_grad():\n\t            start = 0\n\t            length = max(self.window,len(self.dataset)-self.window-10)\n\t            input = []\n\t            for i in range(length):\n\t                input.append(self.dataset[i+start].unsqueeze(0))\n\t            batch = torch.cat(input,0)\n\t            phase = self.forward_window(batch)\n", "            self.draw_dict(phase)\n\t            self.drawer.show()\n\t            phase = PhaseManifold(phase['A'][:,:,0],phase['S'][:,:,0])\n\t            U,S,V = pca_lowrank(phase)\n\t            proj = torch.matmul(phase,V)\n\t            proj = proj[:,:2]\n\t            c = np.arange(0,proj.shape[0],step=1.)\n\t            plt.scatter(proj[:,0],proj[:,1],c=c)\n\t            plt.show()\n"]}
{"filename": "src/Net/CommonOperation.py", "chunked_list": ["from torch import nn\n\timport pytorch_lightning as pl\n\timport torch\n\timport math\n\tclass CommonOperator():\n\t    def __init__(self,batch_size):\n\t        self.batch_size = batch_size\n\t        self.steps_per_epoch = None\n\t        pass\n\t    def add_prefix_to_loss(self, loss:dict, prefix:str):\n", "        output = {}\n\t        for key, value in loss.items():\n\t            if(type(value)==torch.Tensor):\n\t                output[prefix + key] = value.detach()\n\t            else:\n\t                output[prefix + key] = value\n\t        return output\n\t    def set_lr(self,lr,optimizer):\n\t        for pg in optimizer.param_groups:\n\t            pg['lr'] = lr\n", "    def log_dict(self,obj:pl.LightningModule,loss:dict,prefix:str):\n\t        loss_ = self.add_prefix_to_loss(loss,prefix)\n\t        obj.log_dict(loss_,prog_bar=True,logger=True,batch_size=self.batch_size)\n\t        return loss_\n\t    def add_weight_decay(self,model,lr, weight_decay=1e-5, skip_list=()):\n\t        decay = []\n\t        no_decay = []\n\t        for name, param in model.named_parameters():\n\t            if not param.requires_grad:\n\t                continue\n", "            if len(param.shape) == 1 or name in skip_list:\n\t                no_decay.append(param)\n\t            else:\n\t                decay.append(param)\n\t        return [\n\t            {'params': no_decay, 'weight_decay': 0.,\"lr\":lr},\n\t            {'params': decay, 'weight_decay': weight_decay,\"lr\":lr}]\n\t    def num_training_steps(self,obj:pl.LightningModule) -> int:\n\t        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n\t        # warning: will call train_dataloader()\n", "        if(self.steps_per_epoch==None):\n\t            dataset = obj.trainer._data_connector._train_dataloader_source.dataloader()\n\t            self.steps_per_epoch=len(dataset)//obj.trainer.accumulate_grad_batches\n\t        return self.steps_per_epoch\n\t    def get_progress(self,obj:pl.LightningModule,target_epoch:float,start_epoch=0.):\n\t        steps_per_epoch = self.num_training_steps(obj)\n\t        return max(min((obj.global_step-start_epoch*steps_per_epoch)/(target_epoch*steps_per_epoch-start_epoch*steps_per_epoch),1.0),0.0)\n\t    def collect_models(self,obj:nn.Module):\n\t        models = []\n\t        for i, module in obj._modules.items():\n", "            if (sum(p.numel() for p in module.parameters()) > 0):\n\t                models.append(i)\n\t        return models\n"]}
{"filename": "src/Net/__init__.py", "chunked_list": ["import os\n\timport sys\n\tBASEPATH = os.path.dirname(__file__)\n\tsys.path.insert(0, BASEPATH)"]}
{"filename": "src/Net/StyleVAENet.py", "chunked_list": ["import random\n\timport numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom torch import nn\n\tfrom src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n\tfrom src.Module.MoEModule import MultipleExpertsLinear\n\tfrom src.Module.PhaseModule import PhaseOperator\n\tfrom src.Net.CommonOperation import CommonOperator\n\t_EPS32 = torch.finfo(torch.float32).eps\n", "_EPS32_2 = _EPS32*2\n\tfrom src.Module.VAEModule import VAE_Linear\n\tfrom src.geometry.quaternions import quat_to_or6D,or6d_to_quat\n\tfrom src.Datasets.BatchProcessor import BatchProcessDatav2\n\tclass MoeGateDecoder(nn.Module):\n\t    def __init__(self,  style_dims, n_joints,n_pos_joints, condition_size, phase_dim, latent_size, num_experts):\n\t        super(MoeGateDecoder, self).__init__()\n\t        out_channels = 6 * n_joints + n_pos_joints*3# + phase_dim*4\n\t        gate_in = phase_dim*2 + latent_size#+ condition_size\n\t        self.gate = nn.Sequential(nn.Linear(gate_in, 128), nn.ELU(), nn.Linear(128, 128),nn.ELU(), nn.Linear(128, num_experts), nn.Softmax(-1))\n", "        self.linears = nn.ModuleList([MultipleExpertsLinear(condition_size+latent_size, 512, num_experts), MultipleExpertsLinear(512+latent_size, 512, num_experts)])\n\t        self.act = nn.ModuleList([nn.ELU(), nn.ELU(), nn.ELU()])\n\t        self.mlp = MultipleExpertsLinear(512, out_channels, num_experts)\n\t        self.phase_dims = phase_dim\n\t        self.out_channels = out_channels\n\t    def forward(self,  latent, condition, phase):\n\t        '''input: N,C->decoder'''\n\t        '''phase: N,C->gate network'''\n\t        '''x: pose+latent'''\n\t        x = condition\n", "        coefficients = self.gate(torch.cat((phase.flatten(-2,-1),latent),dim=-1))  # self.gate(torch.cat((x,hn),dim=-1))#self.gate(torch.cat((condition,hn,phase.flatten(-2,-1)),dim=-1))#self.gate(torch.cat((hn,condition),dim=-1))##self.gate(torch.cat((phase.flatten(-2,-1),contact),dim=-1))###\n\t        x = torch.cat((x,latent),dim=-1)\n\t        x = self.linears[0](x,coefficients)\n\t        x = self.act[0](x)\n\t        x = torch.cat((x, latent), dim=-1)\n\t        x = self.linears[1](x,coefficients)\n\t        x = self.act[1](x)\n\t        out = self.mlp(x,coefficients)\n\t        pred_pose = out\n\t        return pred_pose,coefficients\n", "from enum import Enum\n\tclass VAEMode(Enum):\n\t        MULTI = 1\n\t        SINGLE = 2\n\tclass MultiVAEOperator():\n\t    def __init__(self):\n\t        pass\n\tclass SingleVAEOperator():\n\t    def __init__(self):\n\t        pass\n", "    def shift_encode_two_frame(this,self,last_v,last_rots,nxt_v,nxt_rots,contact,last_phase,next_phase):\n\t        co_input = torch.cat((last_v, last_rots, nxt_v, nxt_rots,last_phase.flatten(-2,-1),next_phase.flatten(-2,-1)), dim=-1)\n\t        z,mu,log_var = self.embedding_encoder(co_input)\n\t        return z,mu,log_var\n\tclass IAN_FilmGenerator2(nn.Module):\n\t    def __init__(self,in_channel):\n\t        super(IAN_FilmGenerator2, self).__init__()\n\t        self.conv_pre = nn.Conv1d(in_channel,512,1)\n\t        self.conv_pre2 = nn.Conv1d(512,512,3)\n\t        self.conv0 = nn.Conv1d(512,512,3)\n", "        self.conv1 = nn.Conv1d(512,512,5)\n\t        self.act = nn.ReLU()\n\t    def forward(self,x):\n\t        # hope transformer can aggregate the sharp feature, different from the netural pose\n\t        # from motion\n\t        x = self.conv_pre(x)\n\t        x = self.act(x)\n\t        x = self.conv_pre2(x)\n\t        x = self.act(x)\n\t        x = self.conv0(x)\n", "        x = self.act(x)\n\t        x = self.conv1(x)\n\t        x = self.act(x)\n\t        y2 = x\n\t        y2 = y2.transpose(1,2)\n\t        return [y2]\n\tclass EmbeddingTwoFrameEncoder(nn.Module):\n\t    def __init__(self,num_joints,num_pos_joints,latent_dims,phase_dims):\n\t        super(EmbeddingTwoFrameEncoder, self).__init__()\n\t        in_channels = (6*num_joints+6*num_pos_joints)*2 #+latent_dims\n", "        self.linears = nn.ModuleList([nn.Linear(in_channels,512),nn.Linear(512,512),nn.Linear(512,512)])\n\t        self.act = nn.ModuleList([nn.ELU(),nn.ELU(),nn.ELU()])\n\t        self.mlp = VAE_Linear(512,latent_dims)\n\t    def forward(self, condition):\n\t        '''input: N,C->decoder'''\n\t        '''phase: N,C->gate network'''\n\t        '''x: pose+latent'''\n\t        x = condition\n\t        x = self.linears[0](x)\n\t        x = self.act[0](x)\n", "        x = self.linears[1](x)\n\t        x = self.act[1](x)\n\t        x = self.linears[2](x)\n\t        x = self.act[2](x)\n\t        latent,mu,log_var = self.mlp(x)\n\t        return latent,mu,log_var\n\tclass StyleVAENet(pl.LightningModule):\n\t    def __init__(self, skeleton, phase_dim:int=20,latent_size = 64,batch_size=64,mode=\"pretrain\",net_mode=VAEMode.SINGLE):\n\t        style_level_dim = [512,512]\n\t        self.rot_rep_idx = [1, 5, 9, 10, 11, 12, 13, 15, 19]\n", "        self.pos_rep_idx = [idx for idx in np.arange(0, skeleton.num_joints) if idx not in self.rot_rep_idx]\n\t        '''input channel: n_joints*dimensions'''\n\t        super(StyleVAENet, self).__init__()\n\t        self.lr = self.init_lr = 1e-3\n\t        self.automatic_optimization = True\n\t        self.dt = 1./30.\n\t        self.phase_op = PhaseOperator(self.dt)\n\t        self.style_level_dim = style_level_dim\n\t        self.save_hyperparameters(ignore=['mode','net_mode'])\n\t        self.skeleton = skeleton\n", "        self.mode = mode\n\t        self.net_mode = net_mode\n\t        self.vae_op = SingleVAEOperator() if net_mode==VAEMode.SINGLE else MultiVAEOperator()\n\t        self.batch_processor = BatchProcessDatav2()\n\t        self.embedding_encoder = EmbeddingTwoFrameEncoder(skeleton.num_joints, len(self.pos_rep_idx), latent_size,phase_dim)\n\t        self.decoder = MoeGateDecoder(style_level_dim, skeleton.num_joints,len(self.pos_rep_idx),  9+len(self.pos_rep_idx)*6+self.skeleton.num_joints*6, phase_dim, latent_size, num_experts=8)\n\t        self.l1Loss = nn.L1Loss()\n\t        self.mse_loss = nn.MSELoss()\n\t        self.common_operator = CommonOperator(batch_size=batch_size)\n\t        self.scheduled_prob = 1.0\n", "        self.latent_size = latent_size\n\t        self.style_level_dim = style_level_dim\n\t        self.sigma = 0.3\n\t        self.initialize = True\n\t    def transform_batch_to_VAE(self, batch):\n\t        local_pos, local_rot = batch['local_pos'], batch['local_rot']\n\t        edge_len = torch.norm(batch['offsets'][:, 1:], dim=-1, keepdim=True)\n\t        return local_pos, quat_to_or6D(local_rot), edge_len, (batch['phase'])\n\t    def kl_loss(self, mu, log_var):\n\t        return -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp()) / np.prod(mu.shape)\n", "    def shift_running(self, local_pos, local_rots, phases, As, Ss, contacts, style_code):\n\t        '''pose: N,T,J,9'''\n\t        '''hip: N,T,3'''\n\t        '''phases: N,T,M'''\n\t        '''style_code: [N,C,T,J| N,C,T,J]'''\n\t        N, T, n_joints, C = local_pos.shape\n\t        output_pos = torch.empty(size=(N, T - 2, n_joints, 3), device=local_pos.device)\n\t        output_rot = torch.empty(size=(N, T - 2, n_joints, 6), device=local_rots.device)\n\t        output_mu = torch.empty(size=(N, T - 2, self.latent_size), device=local_rots.device)\n\t        local_pos = local_pos[:, :, self.pos_rep_idx]\n", "        # last_S = Ss[:,0]\n\t        output_phase = torch.empty(size=(N, T - 2, phases.shape[-2], 2), device=phases.device)\n\t        output_A = torch.empty(size=(N, T - 2, phases.shape[-2], 1), device=phases.device)\n\t        output_F = torch.empty(size=(N, T - 2, phases.shape[-2], 1), device=phases.device)\n\t        output_sphase = torch.empty(size=(N, T - 2, phases.shape[-2], 2), device=phases.device)\n\t        last_g_pos, last_g_rot = local_pos[:, 1], local_rots[:, 1]\n\t        last_g_v = local_pos[:, 1] - local_pos[:, 0]\n\t        last_phase = phases[:, 1]\n\t        kl_loss = 0.\n\t        step = 0.\n", "        for t in range(1, T - 1):  # slice+1: we discard the first frame\n\t            last_rel_pos = (last_g_pos - last_g_pos[:, 0:1]).flatten(-2,-1)\n\t            last_l_v = last_g_v.flatten(-2, -1)\n\t            last_l_rot = last_g_rot.flatten(-2,-1)\n\t            next_rel_pos = (local_pos[:, t + 1] - local_pos[:, t+1, 0:1]).flatten(-2,-1)\n\t            next_l_v = (local_pos[:, t + 1] - last_g_pos).flatten(-2, -1)\n\t            next_l_rot = local_rots[:, t + 1].flatten(-2, -1)\n\t            hip_l_v = next_l_v[..., 0:3]\n\t            hip_l_r = next_l_rot[..., 0:6].clone()\n\t            condition_no_style = torch.cat((last_rel_pos, last_l_v, hip_l_v, last_l_rot, hip_l_r), dim=-1)\n", "            embedding_input = torch.cat( (last_rel_pos, next_rel_pos, last_l_v, next_l_v, last_l_rot, next_l_rot), dim=-1)\n\t            latent, mu, log_var = self.embedding_encoder( embedding_input)\n\t            output_mu[:, t - 1] = latent\n\t            kl_loss = kl_loss + self.kl_loss(mu, log_var)\n\t            step += 1\n\t            pred_pose_, coefficients = self.decoder(latent, condition_no_style, phases[:,t+1])\n\t            pred_l_v, pred_l_rot_v = pred_pose_[..., :len(self.pos_rep_idx) * 3].view(-1, len(self.pos_rep_idx),3), pred_pose_[..., len(self.pos_rep_idx) * 3:].view( -1, self.skeleton.num_joints, 6)\n\t            last_l_rot = last_l_rot.view(-1, self.skeleton.num_joints, 6)\n\t            pred_g_v = pred_l_v\n\t            pred_g_v[:, 0] = local_pos[:, t + 1, 0] - last_g_pos[:, 0]\n", "            pred_pos = pred_g_v + last_g_pos\n\t            pred_rot = pred_l_rot_v + last_l_rot\n\t            pred_rot[:, 0:1] = local_rots[:, t + 1, 0:1]\n\t            output_pos[:, t - 1, self.pos_rep_idx] = pred_pos\n\t            output_rot[:, t - 1] = pred_rot\n\t            last_g_pos, last_g_rot, last_g_v = pred_pos, pred_rot, pred_g_v\n\t        if (step > 0):\n\t            kl_loss = kl_loss / step\n\t        return output_pos, output_rot, kl_loss, [output_phase, output_A, output_F, output_sphase]\n\t    def regu_pose(self, pos, edge_len, rot):\n", "        import src.geometry.inverse_kinematics as ik\n\t        from src.geometry.quaternions import normalized_or6d\n\t        # In our setting, the spine idx can not be affected by this function because the bone length is constant\n\t        pos = ik.scale_pos_to_bonelen(pos, edge_len, self.skeleton._level_joints, self.skeleton._level_joints_parents)\n\t        rot = normalized_or6d(rot)\n\t        return pos, rot\n\t    def get_gt_contact(self,gt_v):\n\t        eps = 0.5\n\t        eps_bound = 1.\n\t        def contact_smooth(x, idx, eps, bound):\n", "            # x must be in range of [eps,bound]\n\t            x = (x - eps) / (bound - eps)\n\t            x2 = x * x\n\t            x3 = x2 * x\n\t            contact = 2 * (x3) - 3 * (x2) + 1\n\t            return contact * idx\n\t        key_joint = [3, 4, 7, 8]\n\t        gt_fv = gt_v[:, :, key_joint]\n\t        gt_fv = torch.sum(gt_fv.pow(2), dim=-1).sqrt()\n\t        # first we need gt_contact\n", "        idx = (gt_fv < eps)  # * (pred_fv>gt_fv) # get penality idx\n\t        idx_smooth = (gt_fv >= eps) * (gt_fv < eps_bound)\n\t        gt_contact = contact_smooth(gt_fv, idx_smooth, eps, eps_bound) + torch.ones_like(gt_fv) * idx\n\t        return gt_contact\n\t    def contact_foot_loss(self,contact,pred_v):\n\t        key_joints = [3, 4, 7, 8]\n\t        pred_fv = pred_v[:, :, key_joints]\n\t        pred_fv = (torch.sum(pred_fv.pow(2), dim=-1) + 1e-6)  # .sqrt()\n\t        contact_idx = torch.where(contact < 0.01, 0, contact)\n\t        # torch.where(contact>=0.01,-contact,0) # only those who is 100% fixed, we don't apply position constrain\n", "        contact_num = torch.count_nonzero(contact_idx)\n\t        pred_fv = pred_fv * contact_idx\n\t        contact_loss = pred_fv.sum() / max(contact_num, 1)\n\t        return contact_loss\n\t    def rot_to_pos(self,rot,offsets,hip):\n\t        if (rot.shape[-1] == 6):\n\t            rot = or6d_to_quat(rot)\n\t        rot_pos = self.skeleton.global_rot_to_global_pos(rot, offsets, hip)\n\t        return rot_pos\n\t    def phase_loss(self,gt_phase,gt_A,gt_F,phase,A,F,sphase):\n", "        amp_scale = 1.\n\t        loss = {\"phase\": self.mse_loss(gt_phase*amp_scale,phase*amp_scale),\"A\":self.mse_loss(gt_A*amp_scale,A*amp_scale),\"F\":self.mse_loss(gt_F,F),\"slerp_phase\":self.mse_loss(gt_phase*amp_scale,sphase*amp_scale)}\n\t        return loss\n\t    def shared_forward_single(self,batch,base_epoch = 30,edge_mean =21.):\n\t        N = batch['local_pos'].shape[0] // 2\n\t        local_pos, local_rots, edge_len, phases = self.transform_batch_to_VAE(batch)\n\t        A = batch['A']\n\t        S = batch['S']\n\t        src_code = None\n\t        self.length = 25\n", "        start_idx = np.random.randint(0, 60-self.length-1)\n\t        end_idx = start_idx + self.length\n\t        local_pos = local_pos[:, start_idx:end_idx]\n\t        local_rots = local_rots[:, start_idx:end_idx]\n\t        phases = phases[:, start_idx:end_idx]\n\t        A = A[:, start_idx:end_idx]\n\t        S = S[:, start_idx:end_idx]\n\t        F = S[:,1:]-S[:,:-1]\n\t        F = self.phase_op.remove_F_discontiny(F)\n\t        F = F / self.phase_op.dt\n", "        src_pos = local_pos[:N]\n\t        src_rots = local_rots[:N]\n\t        src_edge_len = edge_len[:N]\n\t        src_phases = phases[:N]\n\t        src_A = A[:N]\n\t        src_F = F[:N]\n\t        ########################################################################################\n\t        pred_pos, pred_rot,kl, pred_phase = self.shift_running(src_pos, src_rots,src_phases, src_A, src_F,None,style_code=src_code)\n\t        rot_pos = self.rot_to_pos(pred_rot,batch['offsets'][:N],pred_pos[:,:,0:1])\n\t        pred_pos[:,:,self.rot_rep_idx] = rot_pos[:, :, self.rot_rep_idx]\n", "        if (self.stage != \"training\" or random.random() < 0.1):\n\t            pred_pos, pred_rot = self.regu_pose(pred_pos, src_edge_len, pred_rot)\n\t        # in loss function, we consider the end effector's position more\n\t        gt_contact = self.get_gt_contact(src_pos[:,3:,:] - src_pos[:,2:-1,:]).detach()\n\t        contact_loss = self.contact_foot_loss(gt_contact,pred_pos[:,1:]-pred_pos[:,:-1])\n\t        pos_loss = self.mse_loss(src_pos[:,2:2+self.length,:]/edge_mean,pred_pos[:,:,:]/edge_mean)\n\t        rot_loss = self.mse_loss(src_rots[:,2:2+self.length],pred_rot)\n\t        vae_loss = {\"pos\":pos_loss,\"rot\":rot_loss,\"kl\":kl,\"ct\":contact_loss}\n\t        epoch = self.common_operator.get_progress(self,1,0)\n\t        if(epoch>=1):\n", "            vae_loss['loss'] = vae_loss['pos'] + vae_loss['rot'] + kl*0.001 + vae_loss[\"ct\"]*0.1# + (vae_loss[\"phase\"] + vae_loss['A'] + vae_loss['F']+ vae_loss['slerp_phase']) * 0.5\n\t        else:\n\t            vae_loss['loss'] = vae_loss['pos'] + vae_loss['rot'] + kl * 0.001\n\t        return vae_loss\n\t    def validation_step(self, batch, batch_idx):\n\t        self.scheduled_prob = 1.\n\t        self.scheduled_phase = 1.\n\t        self.length = 30\n\t        self.stage = \"validation\"\n\t        loss = self.shared_forward_single(batch)\n", "        self.common_operator.log_dict(self, loss, \"val_\")\n\t        return loss['loss']\n\t    def test_step(self, batch, batch_idx):\n\t        self.stage = \"test\"\n\t        loss = self.shared_forward_single(batch)\n\t        self.common_operator.log_dict(self, loss, \"test_\")\n\t        return loss\n\t    def update_lr(self):\n\t        base_epoch = 20\n\t        if (self.mode == 'fine_tune'):\n", "            self.scheduled_prob = 1.\n\t            # first 20 epoch ,we increase lr to self.lr\n\t            if (self.current_epoch < base_epoch):\n\t                progress = self.common_operator.get_progress(self, base_epoch, 0)\n\t                lr = self.lr * progress\n\t            else:\n\t                progress = self.common_operator.get_progress(self, 400, base_epoch)\n\t                lr = (1 - progress)*self.lr+progress*1e-5\n\t            opt = self.optimizers()\n\t            self.common_operator.set_lr(lr, opt)\n", "            self.log(\"lr\", lr, logger=True)\n\t        else:\n\t            lr = self.lr\n\t            #first 40 epoch, we use the original lr\n\t            #then we decay the lr to zero until 200 epoch\n\t            if(self.mode=='second'):\n\t                base_epoch = 0\n\t            progress = self.common_operator.get_progress(self, 300, base_epoch)\n\t            decay = (1 - progress)*self.lr+progress*1e-5\n\t            lr = decay\n", "            opt = self.optimizers()\n\t            self.common_operator.set_lr(lr, opt)\n\t            self.log(\"lr\", lr, logger=True)\n\t    def training_step(self, batch, batch_idx):\n\t        self.stage = \"training\"\n\t        self.scheduled_prob = self.common_operator.get_progress(self,target_epoch=50,start_epoch=0)\n\t        if(self.mode!=\"pretrain\"):\n\t            self.scheduled_prob = 1.\n\t        self.update_lr()\n\t        loss = self.shared_forward_single(batch)\n", "        self.common_operator.log_dict(self, loss, \"train_\")\n\t        self.log(\"prob\",self.scheduled_prob,logger=True)\n\t        self.log('prob_ph',self.scheduled_phase,logger=True)\n\t        return loss['loss']\n\t    def configure_optimizers(self):\n\t        models = self.common_operator.collect_models(self)\n\t        trained_model = []\n\t        def weight_decay(model_name, lr, weight_decay):\n\t            trained_model.append(model_name)\n\t            model = getattr(self, model_name)\n", "            return self.common_operator.add_weight_decay(model, lr, weight_decay)\n\t        lr = self.lr\n\t        params = weight_decay(\"decoder\", lr, 0) + weight_decay(\"embedding_encoder\",lr,0)\n\t        optimizer = torch.optim.Adam(params, lr=self.lr, betas=(0.5, 0.9),amsgrad=True)\n\t        non_train_model = [i for i in models if i not in trained_model]\n\t        if (len(non_train_model) > 0):\n\t            print(\"warning:there are some models not trained:{}\".format(non_train_model))\n\t            input(\"Please confirm that by Enter\")\n\t        if(self.mode=='fine_tune'):\n\t            return [optimizer]\n", "        else:\n\t            return [optimizer]\n\tfrom src.utils.BVH_mod import find_secondary_axis\n\tclass Application(nn.Module):\n\t    def __init__(self,net:StyleVAENet,data_module:StyleVAE_DataModule):\n\t        super(Application, self).__init__()\n\t        self.Net = net\n\t        self.data_module = data_module\n\t        self.src_batch = None\n\t        self.target_batch = None\n", "        self.skeleton = data_module.skeleton\n\t    def transform_batch(self, motion, label):\n\t        batch = [[], label]\n\t        batch[0] = [[motion[0], motion[1], motion[2]]]\n\t        for i, value in enumerate(batch[0][0]):\n\t            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n\t        for key in motion[3].keys():\n\t            motion[3][key] = torch.from_numpy(motion[3][key]).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n\t        batch[0][0].append(motion[3])\n\t        batch = self.data_module.transfer_mannual(batch, 0, use_phase=True,use_sty=False)\n", "        return batch\n\t    def transform_anim(self, anim, label):\n\t        batch = [[], label]\n\t        batch[0] = [[anim.quats, anim.offsets, anim.hip_pos]]\n\t        for i, value in enumerate(batch[0][0]):\n\t            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n\t        batch = self.data_module.transfer_mannual(batch, 0, use_phase=False,use_sty=False)\n\t        return batch\n\t    def setSource(self, anim):\n\t        if (type(anim) == tuple):\n", "            self.src_batch = self.transform_batch(anim, 0)\n\t        else:\n\t            self.src_batch = self.transform_anim(anim, 0)\n\t        self.offsets = self.src_batch['offsets']\n\t        self.tangent = find_secondary_axis(self.offsets)\n\t    def _get_transform_ori_motion(self, batch):\n\t        global_pos = batch['local_pos']\n\t        global_rot = batch['local_rot']\n\t        # global_pos,global_rot = self.skeleton.forward_kinematics(local_rot,self.offsets,global_pos[:,:,0:1,:])\n\t        global_rot = self.skeleton.inverse_pos_to_rot(global_rot, global_pos, self.offsets, self.tangent)\n", "        # lp, lq = self.skeleton.inverse_kinematics((global_rot), (global_pos))\n\t        # or\n\t        lp, lq = self.skeleton.inverse_kinematics(global_rot, global_pos)\n\t        return lp[0].cpu().numpy(), lq[0].cpu().numpy()\n\t    def get_source(self):\n\t        batch = {'local_pos':self.src_batch['local_pos'][:,16:46],\"local_rot\":self.src_batch['local_rot'][:,16:46]}\n\t        return self._get_transform_ori_motion(batch)\n\t    def draw_foot_vel(self, pos, pred_pos):\n\t        import matplotlib.pyplot as plt\n\t        def draw(pos,ax,key_joints):\n", "            foot_vel = ((pos[:, 1:, key_joints] - pos[:, :-1, key_joints]) ** 2).sum(dim=-1).sqrt()\n\t            ax.plot(foot_vel[0, :].cpu())\n\t        fig,ax = plt.subplots(2,2,figsize=(2*1.5,2*1.))\n\t        joints = [17,18,21,22]\n\t        ax[0,0].set_xlabel(\"3\")\n\t        draw(pos,ax[0,0],joints[0])\n\t        draw(pred_pos,ax[0,0],joints[0])\n\t        ax[0, 1].set_xlabel(\"4\")\n\t        draw(pos, ax[0, 1], joints[1])\n\t        draw(pred_pos, ax[0, 1], joints[1])\n", "        ax[1, 0].set_xlabel(\"7\")\n\t        draw(pos, ax[1, 0], joints[2])\n\t        draw(pred_pos, ax[1, 0], joints[2])\n\t        ax[1, 1].set_xlabel(\"8\")\n\t        draw(pos, ax[1, 1], joints[3])\n\t        draw(pred_pos, ax[1, 1], joints[3])\n\t        plt.show()\n\t    def forward(self,seed,encoding=True):\n\t        import matplotlib.pyplot as plt\n\t        from torch._lowrank import pca_lowrank\n", "        self.Net.eval()\n\t        with torch.no_grad():\n\t            loc_pos, loc_rot, edge_len, phases= self.Net.transform_batch_to_VAE(self.src_batch)\n\t            A = self.src_batch['A']\n\t            S = self.src_batch['S']\n\t            F = S[:,1:]-S[:,:-1]\n\t            F = self.Net.phase_op.remove_F_discontiny(F)\n\t            F = F / self.Net.phase_op.dt\n\t            torch.random.manual_seed(seed)\n\t            pred_pos, pred_rot,kl,pred_phase = self.Net.shift_running(loc_pos, loc_rot, phases,A,F, None,\n", "                                                                                        None)\n\t            def draw_projection(pred_mu):\n\t                U, S, V = pca_lowrank(pred_mu)\n\t                proj = torch.matmul(pred_mu, V)\n\t                proj = proj[:, :2]\n\t                c = np.arange(0, proj.shape[0], step=1.)\n\t                proj = proj.cpu()\n\t                plt.scatter(proj[:, 0], proj[:, 1], c=c)\n\t                plt.show()\n\t            draw_projection(phases[0].squeeze(0).flatten(-2,-1))\n", "            loss = {}\n\t            rot_pos = self.Net.rot_to_pos(pred_rot, self.src_batch['offsets'], pred_pos[:, :, 0:1])\n\t            pred_pos[:, :, self.Net.rot_rep_idx] = rot_pos[:, :, self.Net.rot_rep_idx]\n\t            # output_pos,output_rot = pred_pos,pred_rot\n\t            output_pos, output_rot = self.Net.regu_pose(pred_pos, edge_len, pred_rot)\n\t            loss['pos']=self.Net.mse_loss(output_pos[:]/21,loc_pos[:,2:]/21)\n\t            loss['rot']=self.Net.mse_loss(output_rot[:,],loc_rot[:,2:])\n\t            print(loss)\n\t            output_pos = torch.cat((loc_pos[:, :2, ...], output_pos), dim=1)\n\t            output_rot = torch.cat((loc_rot[:, :2, ...], output_rot), dim=1)\n", "            output_rot = or6d_to_quat(output_rot)\n\t            batch = {}\n\t            batch['local_rot'] = output_rot#or6d_to_quat(output_rot)\n\t            batch['local_pos'] = output_pos\n\t            self.draw_foot_vel(loc_pos[:,2:], output_pos)\n\t            return self._get_transform_ori_motion(batch)\n"]}
{"filename": "src/Net/TransitionNet.py", "chunked_list": ["import math\n\timport random\n\timport torch\n\tfrom torch import nn\n\tclass PosEncoding(nn.Module):\n\t    def positional_encoding(self,tta, basis=10000., dimensions=256):\n\t        z = torch.zeros(dimensions, device='cuda')  # .type(torch.float16)\n\t        indices = torch.arange(0, dimensions, 2, device='cuda').type(torch.float32)\n\t        z[indices.long()] = torch.sin(tta / torch.pow(basis, indices / dimensions))\n\t        z[indices.long() + 1] = torch.cos(tta / torch.pow(basis, indices / dimensions))\n", "        return z\n\t    def __init__(self,max_tta,dims):\n\t        super(PosEncoding, self).__init__()\n\t        ttaEncoding = torch.empty(max_tta, dims)\n\t        for tta in range(max_tta):\n\t            ttaEncoding[tta] = self.positional_encoding(tta,basis=10000.,dimensions=dims)\n\t        self.register_buffer(\"ttaEncoding\", ttaEncoding)\n\t    def forward(self,id):\n\t        return self.ttaEncoding[id]\n\tdef add_pos_info(latent,embedings,tta,t_max):\n", "    t = min(tta, t_max)\n\t    pos = embedings(t)\n\t    #pos = torch.cat((pos,pos,pos),dim=-1)\n\t    return latent+pos\n\tdef multi_concat(context_state,context_offset,context_target,embeddings,embeddings512,noise_per_sequence,tta,t_max):\n\t    t = torch.where(tta < t_max, tta, t_max)\n\t    embeddings_vector = []\n\t    embeddings512_vector = []\n\t    for i in range(t.shape[0]):\n\t        embeddings_vector.append(embeddings(t[i]).view(1,1,-1))\n", "        embeddings512_vector.append(embeddings512(t[i]).view(1,1,-1))\n\t    embeddings_vector = torch.cat(embeddings_vector,dim=1) # 1, 8 ,256\n\t    embeddings512_vector = torch.cat(embeddings512_vector,dim=1)\n\t    h_target = context_target + embeddings_vector\n\t    h_state = context_state + embeddings512_vector\n\t    h_offset = context_offset + embeddings_vector\n\t    lambda_tar = torch.where(t>30,1.,torch.where(t<5.,0,(t-5.)/25.))\n\t    # if tta >= 30:\n\t    #     lambda_tar = 1.\n\t    # elif tta < 5:\n", "    #     lambda_tar = 0.\n\t    # else:\n\t    #     lambda_tar = (tta - 5.) / 25.\n\t    h_target = torch.cat((h_offset, h_target), dim=-1)\n\t    h_target = h_target + lambda_tar.view(1,-1,1) * noise_per_sequence.unsqueeze(1)\n\t    return torch.cat((h_state, h_target), dim=-1), h_target\n\tdef concat(context_state,context_offset,context_target,embeddings,embeddings512,noise_per_sequence,tta,t_max):\n\t#    t = torch.min(tta,t_max) # MAXFRAME+10-5\n\t    t = min(tta,t_max)\n\t    h_target = context_target + embeddings(t)\n", "    h_state = context_state + embeddings512(t)\n\t    h_offset = context_offset + embeddings(t)\n\t    if tta >= 30:\n\t        lambda_tar = 1.\n\t    elif tta < 5:\n\t        lambda_tar = 0.\n\t    else:\n\t        lambda_tar = (tta - 5.) / 25.\n\t    h_target = torch.cat((h_offset, h_target), dim=-1)\n\t    h_target = h_target + lambda_tar * noise_per_sequence\n", "    return torch.cat((h_state,h_target),dim=-1),h_target\n\tclass SeqScheduler():\n\t    def __init__(self,initial_seq,max_seq):\n\t        self.initial_seq = initial_seq\n\t        self.max_seq = max_seq\n\t       # self.epoch = epoch\n\t    def progress(self,t:float):\n\t        t = min(max(t,0.),1.)\n\t        out = (self.max_seq-self.initial_seq)*t+self.initial_seq\n\t        return int(out)\n", "    def range(self,t:float):\n\t        upper = self.progress(t)\n\t        return random.randint(self.initial_seq,upper)\n\tclass ATNBlock(nn.Module):\n\t    def __init__(self,content_dims,style_dims):\n\t        super(ATNBlock, self).__init__()\n\t        in_ch = content_dims\n\t        self.in_ch = content_dims\n\t        self.sty_ch = style_dims\n\t        self.f = nn.Linear(content_dims, style_dims)\n", "        self.g = nn.Linear(style_dims, style_dims)\n\t        self.h = nn.Linear(style_dims, style_dims)\n\t        self.sm = nn.Softmax(dim=-2)\n\t        self.k = nn.Linear(style_dims, in_ch)\n\t        self.norm = nn.InstanceNorm2d(style_dims,affine=False)\n\t        self.norm_content = nn.InstanceNorm1d(content_dims,affine=False) #LayerNorm(keep the same as AdaIn)\n\t       # self.s = []\n\t    def forward(self, fs, fd,pos, first):\n\t        #return fd\n\t        N,T,C = fs.shape\n", "        N,C = fd.shape\n\t        x = fd\n\t        s_sty = fs\n\t        # N,C : fd.shape\n\t        # N,C,T : fs.shape\n\t        b = s_sty.shape[0]\n\t        F = self.f(self.norm_content(x)).unsqueeze(-1) # N,C,1\n\t        if(first):\n\t            G = self.g(self.norm(s_sty)) #N,T,C\n\t            self.G = G.view(b, -1, self.sty_ch)  # N,T,C\n", "            #s_sty_pos = s_sty+pos_embedding.view(1,C,1)\n\t            self.H = self.h(s_sty).transpose(1,2) #N,C,T\n\t        #H = H.view(b, self.sty_ch, -1)  # N,C,T\n\t        F = F.view(b, self.sty_ch, -1) #N,C,1\n\t        S = torch.bmm(self.G,F) #N,T,1\n\t        S = self.sm(S/math.sqrt(self.G.shape[-1]))\n\t        # self.s.append(S)\n\t        O = torch.bmm(self.H, S) # N,C,1\n\t        O = O.view(x.shape[:-1]+(self.sty_ch,))\n\t        O = self.k(O)\n", "        O += x\n\t        return O\n\tclass AdaInNorm2D(nn.Module):\n\t    r\"\"\"MLP(fs.mean(),fs.std()) -> instanceNorm(fd)\"\"\"\n\t    def __init__(self,style_dims,content_dim,n_joints=0):\n\t        super(AdaInNorm2D, self).__init__()\n\t        self.affine2 = nn.Linear(style_dims, style_dims)\n\t        self.act = nn.ELU()#nn.LeakyReLU(0.2)\n\t        self.affine3 = nn.Linear(style_dims, style_dims)\n\t        self.affine4 = nn.Linear(style_dims, content_dim * 2)\n", "        self.norm = nn.InstanceNorm1d(512)\n\t        self.dropout = nn.Dropout(0.1)\n\t    def forward(self, s, d ,pos_emedding,first):\n\t        if(first):\n\t            N,T,C = s.shape\n\t            s = torch.mean(s,dim=1) #N,C\n\t            s = self.affine2(s)\n\t            s = self.act(s)\n\t            s = self.dropout(s)\n\t            s = self.affine4(s)\n", "            self.gamma, self.beta = torch.chunk(s, chunks=2, dim=1)\n\t        d = self.norm(d)\n\t        return (1 + self.gamma) * d + self.beta\n"]}
{"filename": "src/Net/TransitionPhaseNet.py", "chunked_list": ["import random\n\timport numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom torch import nn\n\tfrom src.Datasets.BatchProcessor import BatchProcessDatav2\n\tfrom src.Module.Utilities import PLU\n\tfrom src.Net.CommonOperation import CommonOperator\n\t# from src.Datasets.TransitionDataModule import Transition_DataModule, BatchRotateYCenterXZ\n\tfrom src.geometry.quaternions import normalized_or6d\n", "from src.geometry.quaternions import quat_to_or6D, or6d_to_quat\n\tfrom src.utils.BVH_mod import Skeleton, find_secondary_axis\n\tdef eval_sample(model, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, pos_offset, skeleton: Skeleton, length, param):\n\t    # FOR EXAMPLE\n\t    model = model.eval()\n\t    model = model.cuda()\n\t    quats = Q\n\t    offsets = pos_offset\n\t    hip_pos = X\n\t    dict = {\"hip_pos\": X, 'offsets': Q, 'quats': Q}\n", "    gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n\t    loc_rot = quat_to_or6D(gq)\n\t    if \"target_id\" in param:\n\t        target_id = param[\"target_id\"]\n\t    else:\n\t        target_id = length + 10\n\t    noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device)\n\t    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n\t    tar_quat = quat_to_or6D(tar_quat)\n\t    target_style = model.get_film_code(tar_pos.cuda(),tar_quat.cuda())\n", "    F = S[:, 1:] - S[:, :-1]\n\t    F = model.phase_op.remove_F_discontiny(F)\n\t    F = F / model.phase_op.dt\n\t    phases = model.phase_op.phaseManifold(A, S)\n\t    if(model.predict_phase==True):\n\t     pred_pos, pred_rot, pred_phase, _,_ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(), F.cuda(), target_style, None,\n\t                                                               start_id=10,\n\t                                                               target_id=target_id, length=length,\n\t                                                               phase_schedule=1.)\n\t    else:\n", "        pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n\t                                                                   F.cuda(), target_style, None,\n\t                                                                   start_id=10,\n\t                                                                   target_id=target_id, length=length,\n\t                                                                   phase_schedule=1.)\n\t    pred_pos,pred_rot = pred_pos.cpu(),pred_rot.cpu()\n\t    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n\t    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n\t    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n\t    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n", "    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n\t    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :]).flatten(-2, -1)\n\t    x_mean = x_mean.view(skeleton.num_joints, 3)\n\t    x_std = x_std.view(skeleton.num_joints, 3)\n\t    GX = (GX - x_mean.flatten(-2, -1)) / x_std.flatten(-2, -1)\n\t    GX = GX.transpose(1, 2)\n\t    return GQ, GX\n\tfrom src.Module.PhaseModule import PhaseOperator\n\tclass StateEncoder(nn.Module):\n\t    def __init__(self,inchannels):\n", "        super(StateEncoder, self).__init__()\n\t        self.layers = torch.nn.ModuleList([torch.nn.Linear(inchannels, 512),torch.nn.Linear(512,256)])\n\t        self.acts = torch.nn.ModuleList([PLU(),PLU()])\n\t    def forward(self,x):\n\t        for i in range(len(self.layers)):\n\t            x = self.layers[i](x)\n\t            x = self.acts[i](x)\n\t        return x\n\tclass StyleEmbedding(torch.nn.Module):\n\t    def __init__(self,in_channels):\n", "        super(StyleEmbedding, self).__init__()\n\t        from src.Net.TransitionNet import AdaInNorm2D,ATNBlock\n\t        self.adains = nn.ModuleList([AdaInNorm2D(512, 512, 0)])\n\t        self.atns = nn.ModuleList([ATNBlock(512, 512)])\n\t        self.linears = nn.ModuleList([nn.Linear(in_channels, 512)])\n\t        self.act = nn.ELU()\n\t    def FILM(self, idx, s, x,pos_encoding, first):\n\t        x = self.adains[idx](s, x,pos_encoding, first)\n\t        x = self.act(x)\n\t        x = self.atns[idx](s, x,pos_encoding, first)\n", "        return x\n\t    def forward(self, fs, condition,pos_encoding, first):\n\t        '''input: N,C->decoder'''\n\t        '''phase: N,C->gate network'''\n\t        '''x: pose+latent'''\n\t        # just for sure it has style\n\t        x = condition\n\t        x = self.linears[0](x)\n\t        x = self.FILM(0,fs[0],x,None,first)\n\t        x = self.act(x)\n", "        return x\n\tclass MoeStylePhasePredictor(nn.Module):\n\t    def __init__(self, rnn_size, phase_dim, num_experts):\n\t        from src.Net.TransitionNet import AdaInNorm2D,ATNBlock\n\t        from src.Module.PhaseModule import PhaseSegement\n\t        super(MoeStylePhasePredictor, self).__init__()\n\t        self.linears = nn.ModuleList([nn.Linear(rnn_size+phase_dim*2+512,rnn_size),nn.Linear(rnn_size,512),nn.Linear(512,512)])\n\t        self.adains = nn.ModuleList([AdaInNorm2D(512,512,0)])\n\t        self.atns = nn.ModuleList([ATNBlock(512, 512)])\n\t        self.act = nn.ModuleList([nn.ELU(), nn.ELU(), nn.ELU()])\n", "        self.mlp = nn.Linear(512,phase_dim*4+9+32)\n\t        self.phase_predcitor = PhaseSegement(phase_dim)\n\t        self.phase_dims = phase_dim\n\t    def FILM(self, idx, s, x, pos_encoding, first):\n\t        x = self.adains[idx](s, x, pos_encoding, first)\n\t        x = self.act[idx](x)\n\t        x = self.atns[idx](s, x, pos_encoding, first)\n\t        return x\n\t    def forward(self, fs, condition, phase,offset_latent,first):\n\t        '''input: N,C->decoder'''\n", "        '''phase: N,C->gate network'''\n\t        '''x: pose+latent'''\n\t        x = condition\n\t        x = torch.cat((x,phase.flatten(-2,-1),offset_latent),dim=-1)\n\t        x = self.linears[0](x)\n\t        x = self.act[0](x)\n\t        x = self.linears[1](x)\n\t        x = self.FILM(0,fs[0],x,None,first)\n\t        x = self.act[1](x)\n\t        x = self.linears[2](x)\n", "        x = self.act[2](x)\n\t        out = self.mlp(x)\n\t        phase,hip,latent = out[:,:self.phase_dims*4],out[:,self.phase_dims*4:self.phase_dims*4+9],out[:,self.phase_dims*4+9:]\n\t        phase, A, F = self.phase_predcitor(phase)\n\t        hip_v,hip_rv = hip[:,:3],hip[:,3:]\n\t        return phase, A, F, hip_v,hip_rv,latent\n\tclass PredictInitialState(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.conv_layer = nn.Sequential(\n", "            nn.Conv1d(1024,1024,5),\n\t           # nn.AvgPool1d(2),\n\t            nn.ReLU(),\n\t            nn.Dropout(0.5),\n\t            nn.Conv1d(1024, 1024, 3),\n\t          #  nn.AvgPool1d(2),\n\t            nn.ReLU(),\n\t            nn.Dropout(0.5),\n\t            nn.Conv1d(1024, 1024, 2),\n\t            nn.ReLU()\n", "        )\n\t        #self.transform = nn.Transformer(1024)\n\t       # self.bn = lambda v:(v[:,0]**2+v[:,1]**2)\n\t        self.mlp = nn.Linear(1024,10*3)\n\t        self.phase_op = PhaseOperator(1 / 30.)\n\t    def forward(self, x):\n\t        def bn(v):\n\t            len = torch.sqrt(v[...,0]**2+v[...,1]**2+1e-6)\n\t            return v/len.unsqueeze(-1)\n\t        x = x.transpose(1,2)\n", "        x = self.conv_layer(x)\n\t        x = x.squeeze(-1)\n\t        # x = self.transform(x,x)\n\t        # x = x[:,-1]\n\t        out = self.mlp(x)\n\t        phase = out#.view(out.shape[0],10,3)\n\t        A,S = phase[:,:10].unsqueeze(-1),phase[:,10:30].unsqueeze(-1)\n\t        S = S.view(S.shape[0],10,2)\n\t        S = bn(S)\n\t        S = torch.atan2(S[...,1],S[...,0]).unsqueeze(-1)/self.phase_op.tpi\n", "       #  S = torch.atan2(S[:,:10],S[:,10:])/3.14159\n\t        phase = self.phase_op.phaseManifold(A,S)\n\t        return  phase,A,S\n\tclass TransitionNet_phase(pl.LightningModule):\n\t    \"\"\"Sequence-to-sequence model for human motion prediction\"\"\"\n\t    def __init__(self, moe_decoder: nn.Module,skeleton, pose_channels,stat,\n\t                  dt,  phase_dim=20, rnn_size=1024, dropout=0.3, past_seq=10,mode='pretrain',predict_phase=False,pretrained_model=None):\n\t        from src.Net.StyleVAENet import IAN_FilmGenerator2\n\t        from src.Net.TransitionNet import SeqScheduler,PosEncoding\n\t        super(TransitionNet_phase, self).__init__()\n", "        self.rot_rep_idx = [1, 5, 9, 10, 11, 12, 13, 15, 19]\n\t        self.pos_rep_idx = [idx for idx in np.arange(0, skeleton.num_joints) if idx not in self.rot_rep_idx]\n\t        self.mode = mode\n\t        self.test = True\n\t        self.predict_phase = predict_phase\n\t        pos_mean, pos_std = stat['pos_stat']\n\t        vel_mean, vel_std = stat['vel_stat']\n\t        rot_mean, rot_std = stat[\"rot_stat\"]\n\t        register_numpy = lambda x, s: self.register_buffer(s, torch.from_numpy(x).float())\n\t        register_scalar = lambda x, s: self.register_buffer(s, torch.Tensor([x]).float())\n", "        register_numpy(vel_mean, \"vel_mean\")\n\t        register_numpy(pos_mean, \"pos_mean\")\n\t        register_scalar(vel_std, \"vel_std\")\n\t        register_numpy(rot_mean, \"rot_mean\")\n\t        register_scalar(rot_std, \"rot_std\")\n\t        register_scalar(pos_std, \"pos_std\")\n\t        self.automatic_optimization = True\n\t        max_seq_length = 40\n\t        self.seq_scheduler = SeqScheduler(20, 40)\n\t        self.seq_scheduler2 = SeqScheduler(5,20)\n", "        self.max_seq = max_seq_length\n\t        self.past_seq = past_seq\n\t        self.learned_embedding = False\n\t        self.batch_processor = BatchProcessDatav2()\n\t        self.phase_dim = phase_dim\n\t        self.dt = dt\n\t        #self.VAE_op = VAE_Pose_Operator(skeleton)\n\t        self.phase_op = PhaseOperator(dt)\n\t        self.lr = self.init_lr = 1e-3\n\t        self.normal_method = 'zscore'\n", "        self.input_channel = pose_channels\n\t        self.save_hyperparameters(ignore=['moe_decoder','mode','pretrained_model'])\n\t        self.skeleton = skeleton\n\t        num_joints = skeleton.num_joints\n\t        self.num_joints = num_joints\n\t        self.state_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*6 # +phase_dim*2# c_t, root velocity, q_t\n\t        self.offset_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*3  # offset from target, root offset, pose offset\n\t        self.target_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*6 # + phase_dim*2# q_target\n\t        self.dropout = dropout\n\t        self.state_encoder = StateEncoder(self.state_input_size)\n", "        self.offset_encoder = StateEncoder(self.offset_input_size)\n\t        self.target_encoder = StateEncoder(self.target_input_size)\n\t        self.embedding_style = StyleEmbedding(256)\n\t        self.LSTMCell = torch.nn.LSTMCell(1024, rnn_size)\n\t        self.initial_state_predictor = PredictInitialState()\n\t        self.pose_channels = pose_channels\n\t        self.phase_predictor = MoeStylePhasePredictor(rnn_size, phase_dim,8)\n\t        self.film_generator = IAN_FilmGenerator2(12 * 22)\n\t        self.decoder = moe_decoder.decoder\n\t        self.sigma_target = 0.5\n", "        self.max_tta = past_seq + max_seq_length - 5\n\t        if (self.learned_embedding):\n\t            self.embedding = nn.Embedding(self.max_tta + 1, 256)\n\t            self.embedding512 = nn.Embedding(self.max_tta+1,512)\n\t        else:\n\t            self.embedding = PosEncoding(self.max_tta + 1, 256)\n\t            self.embedding512 = PosEncoding(self.max_tta + 1, 512)\n\t        if (pretrained_model != None):\n\t            if(type(pretrained_model)==dict):\n\t                self.load_state_dict(pretrained_model['state_dict'], strict=False)\n", "            else:\n\t                self.load_state_dict(pretrained_model.state_dict(), strict=False)\n\t        self.l1_loss = nn.L1Loss()\n\t        self.mse_loss = nn.MSELoss()\n\t        self.common_operator = CommonOperator(batch_size=32)\n\t    def target_encoding(self, target_rots,target_pos, target_v):\n\t        return self.target_encoder(torch.cat((target_pos.flatten(-2,-1),target_v.flatten(-2,-1),target_rots.flatten(-2,-1)), dim=-1))\n\t    def state_encoding(self, pos,vel, rot):\n\t        return self.state_encoder(torch.cat((pos.flatten(-2,-1),vel.flatten(-2,-1), rot.flatten(-2,-1)), dim=-1))\n\t    def offset_encoding(self, pos, rot):\n", "        return self.offset_encoder(torch.cat((pos.flatten(-2,-1),rot.flatten(-2,-1)), dim=-1))\n\t    def regu_pose(self, pos, edge_len, rot):\n\t        import src.geometry.inverse_kinematics as ik\n\t        pos = ik.scale_pos_to_bonelen(pos, edge_len, self.skeleton._level_joints, self.skeleton._level_joints_parents)\n\t        rot = normalized_or6d(rot)\n\t        return pos, rot\n\t    def shift_running(self, local_pos, local_rots,phases,As,Fs, style_code, noise_per_sequence, start_id, target_id,length,phase_schedule=1.):\n\t        from src.Net.TransitionNet import concat\n\t        if not length:\n\t            length = target_id - start_id\n", "        '''pose: N,T,J,9'''\n\t        '''hip: N,T,3'''\n\t        '''phases: N,T,M'''\n\t        '''style_code: [N,C,T,J| N,C,T,J]'''\n\t        J = local_pos.shape[-2]\n\t        N, T, J, C = local_pos.shape\n\t        device = local_pos.device\n\t        output_pos = torch.empty(size=(N, length,  (J),3), device=device)\n\t        output_rot = torch.empty(size=(N, length,  (J),6), device=device)\n\t        output_phase = torch.empty(size=(N, length, phases.shape[-2], 2), device=phases.device)\n", "        output_sphase = torch.empty(size=(N, length, phases.shape[-2], 2), device=phases.device)\n\t        output_A = torch.empty(size=(N, length, phases.shape[-2], 1), device=phases.device)\n\t        output_F = torch.empty(size=(N, length, phases.shape[-2], 1), device=phases.device)\n\t        #\n\t        hn = torch.zeros(N, 1024, device=device)\n\t        cn = torch.zeros(N, 1024, device=device)\n\t        if (noise_per_sequence == None):\n\t            noise_per_sequence = torch.normal(0, 0.5, size=(N, 512), device=device)\n\t        offset_t = torch.scalar_tensor(start_id + length - 1, dtype=torch.int32, device=device)\n\t        tmax = torch.scalar_tensor(self.max_tta, dtype=torch.int32, device=device)\n", "        local_pos = local_pos[:,:,self.pos_rep_idx]\n\t        last_g_v = local_pos[:, 1] - local_pos[:, 0]\n\t        target_g_pos, target_g_rots = local_pos[:, target_id-1, :], local_rots[:,target_id-1, :]\n\t        target_g_v = local_pos[:,target_id-1]-local_pos[:,target_id-2]\n\t        last_g_pos, last_g_rot  = local_pos[:, 1, :], local_rots[:, 1, :]\n\t        latent_loss = 0.\n\t        target_latent = self.target_encoding(target_g_rots, target_g_pos-target_g_pos[:,0:1], target_g_v)\n\t        encode_first = True\n\t        if(hasattr(self,\"predict_phase\")==False or self.predict_phase==False):\n\t            for i in range(1,start_id-1):\n", "                last_l_v, target_l_v = last_g_v, target_g_v\n\t                last_l_pos, target_l_pos = last_g_pos, target_g_pos\n\t                last_l_rot, target_l_rot = last_g_rot, target_g_rots\n\t                offset_pos = target_l_pos - last_l_pos\n\t                offset_rot = target_l_rot - last_l_rot\n\t                offset_t = offset_t - 1\n\t                state_latent = self.state_encoding(last_l_pos-last_l_pos[:,0:1], last_l_v, last_l_rot)\n\t                offset_latent = self.offset_encoding(offset_pos, offset_rot)\n\t                state_latent = self.embedding_style(style_code, state_latent, None, encode_first)\n\t                latent,h_target = concat(state_latent, offset_latent, target_latent, self.embedding,self.embedding512, noise_per_sequence, offset_t, tmax)\n", "                encode_first = False\n\t                (hn, cn) = self.LSTMCell(latent, (hn, cn))\n\t                last_g_pos,last_g_rot  = local_pos[:,i+1],local_rots[:,i+1]\n\t                last_g_v = local_pos[:,i+1]-local_pos[:,i]\n\t                last_phase = phases[:,i+1]\n\t        else:\n\t            last_l_v, target_l_v = local_pos[:,1:start_id-1]-local_pos[:,0:start_id-2], target_g_v.repeat(1,start_id-2,1,1)\n\t            last_l_pos,target_l_pos = local_pos[:,1:start_id-1],target_g_pos.unsqueeze(1).repeat(1,start_id-2,1,1)\n\t            last_l_rot,target_l_rot = local_rots[:,1:start_id-1],target_g_rots.unsqueeze(1).repeat(1,start_id-2,1,1)\n\t            offset_pos = target_l_pos-last_l_pos\n", "            offset_rot = target_l_rot-last_l_rot\n\t            last_l_pos = last_l_pos.flatten(0,1)\n\t            last_l_v = last_l_v.flatten(0,1)\n\t            last_l_rot = last_l_rot.flatten(0,1)\n\t            offset_pos = offset_pos.flatten(0,1)\n\t            offset_rot = offset_rot.flatten(0,1)\n\t            state_latent = self.state_encoding(last_l_pos - last_l_pos[:, 0:1], last_l_v, last_l_rot)\n\t            offset_latent = self.offset_encoding(offset_pos, offset_rot)\n\t            style_code[0] = style_code[0].unsqueeze(1).repeat(1,start_id-2,1,1).flatten(0,1)\n\t            state_latent = self.embedding_style(style_code, state_latent, None, first=True)\n", "            target_latent = target_latent.unsqueeze(1).repeat(1,start_id-2,1).flatten(0,1)\n\t            ## recover\n\t            recover = lambda x: x.view((N,start_id-2)+x.shape[1:])\n\t            state_latent = recover(state_latent)\n\t            offset_latent = recover(offset_latent)\n\t            target_latent = recover(target_latent)\n\t            style_code[0] = recover(style_code[0])\n\t            offset_ts = []\n\t            for i in range(1,start_id-1):\n\t                offset_t = offset_t - 1\n", "                offset_ts.append(offset_t.view(1))\n\t            offset_ts = torch.cat(offset_ts,dim=0)\n\t            from src.Net.TransitionNet import multi_concat\n\t            latent, h_target = multi_concat(state_latent, offset_latent, target_latent, self.embedding, self.embedding512,noise_per_sequence,offset_ts, tmax)\n\t            pre_phase, preA, preS = self.initial_state_predictor(latent)\n\t            style_code[0] = style_code[0][:,0]\n\t            target_latent = target_latent[:,0]\n\t            # prepare for predicting the first frame\n\t            last_g_pos,last_g_rot  = local_pos[:,start_id-1],local_rots[:,start_id-1]\n\t            last_g_v = local_pos[:,start_id-1]-local_pos[:,start_id-2]\n", "            last_phase = pre_phase\n\t           # last_phase = phases[:,start_id-1]\n\t            for i in range(1,start_id-1):\n\t                (hn, cn) = self.LSTMCell(latent[:,i-1], (hn, cn))\n\t        first = True\n\t        step = 0\n\t        for i in range(start_id-1, start_id-1+length):\n\t            last_l_v, target_l_v = last_g_v, target_g_v\n\t            last_l_pos, target_l_pos = last_g_pos, target_g_pos\n\t            last_l_rot, target_l_rot = last_g_rot, target_g_rots\n", "            offset_pos = target_l_pos - last_l_pos\n\t            offset_rot = target_l_rot - last_l_rot\n\t            offset_t = offset_t - 1\n\t            state_latent = self.state_encoding(last_l_pos - last_l_pos[:, 0:1], last_l_v, last_l_rot)\n\t            offset_latent = self.offset_encoding(offset_pos, offset_rot)\n\t            state_latent = self.embedding_style(style_code, state_latent, None, encode_first)\n\t            encode_first=False\n\t            latent,h_target = concat(state_latent, offset_latent, target_latent, self.embedding,self.embedding512, noise_per_sequence, offset_t,tmax)\n\t            (hn, cn) = self.LSTMCell(latent, (hn, cn))\n\t            input_clip = hn\n", "            pred_phase,pred_A,pred_F,hip_l_v,hip_l_rv,latent = self.phase_predictor(style_code,input_clip,last_phase,h_target,first)\n\t            hip_l_r = hip_l_rv + last_l_rot[:,0]\n\t            condition_no_style = torch.cat(((last_l_pos - last_l_pos[:, 0:1]).flatten(-2,-1), last_l_v.flatten(-2,-1), hip_l_v, last_l_rot.flatten(-2,-1), hip_l_r), dim=-1)\n\t            nxt_phase = self.phase_op.next_phase(last_phase, pred_A, pred_F)\n\t            slerp_phase = self.phase_op.slerp(nxt_phase, pred_phase)\n\t            pred_pose_, coefficients = self.decoder(latent, condition_no_style,slerp_phase)\n\t            pred_l_v, pred_l_rot_v = pred_pose_[..., :len(self.pos_rep_idx) * 3], pred_pose_[..., len(self.pos_rep_idx) * 3:]\n\t            pred_l_v = pred_l_v.view(-1,len(self.pos_rep_idx),3)\n\t            pred_l_rot_v = pred_l_rot_v.view(-1, self.skeleton.num_joints, 6)\n\t            pred_g_v = pred_l_v\n", "            pred_g_v[:,0] = hip_l_v\n\t            pred_rot = pred_l_rot_v+last_l_rot\n\t            pred_rot[:,0] = hip_l_r\n\t            pred_pos = pred_g_v + last_g_pos\n\t            output_pos[:,step, self.pos_rep_idx] = pred_pos\n\t            output_rot[:, step] = pred_rot\n\t            output_phase[:,step] = pred_phase\n\t            output_A[:,step] = pred_A\n\t            output_F[:,step] = pred_F\n\t            output_sphase[:,step] = slerp_phase\n", "            last_g_pos, last_g_rot = pred_pos, pred_rot\n\t            last_g_v = pred_g_v\n\t            last_phase = slerp_phase\n\t            first = False\n\t            step+=1\n\t        latent_loss = latent_loss/length\n\t        #output = output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss#,output_hip_pos,output_hip_rot#, output_phase,output_A,output_F\n\t        if(hasattr(self,\"predict_phase\") and self.predict_phase):\n\t            return output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss,[pre_phase,preA,preS]\n\t        else:\n", "            return output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss\n\t    def phase_loss(self, gt_phase, gt_A, gt_F, phase, A, F, sphase):\n\t        loss = {\"phase\": self.mse_loss(gt_phase, phase), \"A\": self.mse_loss(gt_A, A), \"F\": self.mse_loss(gt_F, F),\n\t                \"slerp_phase\": self.mse_loss(gt_phase, sphase)}\n\t        return loss\n\t    def weight_sum_loss(self, loss, weight):\n\t        l = 0\n\t        for key in loss.keys():\n\t            l = l + loss[key] * weight[key] if key in weight.keys() else l + loss[key]\n\t        return l\n", "    def rot_to_pos(self,rot,offsets,hip):\n\t        if (rot.shape[-1] == 6):\n\t            rot = or6d_to_quat(rot)\n\t        rot_pos = self.skeleton.global_rot_to_global_pos(rot, offsets, hip)\n\t        return rot_pos\n\t    def get_gt_contact(self,gt_v):\n\t        eps = 0.5\n\t        eps_bound = 1.\n\t        def contact_smooth(x, idx, eps, bound):\n\t            # x must be in range of [eps,bound]\n", "            x = (x - eps) / (bound - eps)\n\t            x2 = x * x\n\t            x3 = x2 * x\n\t            contact = 2 * (x3) - 3 * (x2) + 1\n\t            return contact * idx\n\t        key_joint = [3, 4, 7, 8]\n\t        gt_fv = gt_v[:, :, key_joint]\n\t        gt_fv = torch.sum(gt_fv.pow(2), dim=-1).sqrt()\n\t        # first we need gt_contact\n\t        idx = (gt_fv < eps)  # * (pred_fv>gt_fv) # get penality idx\n", "        idx_smooth = (gt_fv >= eps) * (gt_fv < eps_bound)\n\t        gt_contact = contact_smooth(gt_fv, idx_smooth, eps, eps_bound) + torch.ones_like(gt_fv) * idx\n\t        return gt_contact\n\t    def contact_foot_loss(self,contact,pred_v):\n\t        key_joints = [3, 4, 7, 8]\n\t        pred_fv = pred_v[:, :, key_joints]\n\t        pred_fv = (torch.sum(pred_fv.pow(2), dim=-1) + 1e-6)  # .sqrt()\n\t        contact_idx = torch.where(contact < 0.01, 0, contact)\n\t        # torch.where(contact>=0.01,-contact,0) # only those who is 100% fixed, we don't apply position constrain\n\t        contact_num = torch.count_nonzero(contact_idx)\n", "        pred_fv = pred_fv * contact_idx\n\t        contact_loss = pred_fv.sum() / max(contact_num, 1)\n\t        return contact_loss\n\t    def shared_forward(self, batch, seq, optimizer=None, d_optimizer=None):\n\t        N = batch['local_pos'].shape[0] #// 2\n\t        style_code = self.get_film_code(batch['sty_pos'][:N], batch['sty_rot'][:N])\n\t        A = batch['A']\n\t        S = batch['S']\n\t        F = S[:, 1:] - S[:, :-1]\n\t        F = self.phase_op.remove_F_discontiny(F)\n", "        F = F / self.phase_op.dt\n\t        local_pos, local_rots, edge_len, phases = self.transform_batch_to_VAE(batch)\n\t        # source\n\t        local_pos = local_pos[:N]\n\t        local_rots = local_rots[:N]\n\t        edge_len = edge_len[:N]\n\t        phases = phases[:N]\n\t        A = A[:N]\n\t        F = F[:N]\n\t        offsets = batch['offsets'][:N]\n", "        noise=None\n\t        start_id = 10\n\t        target_id = 10+seq\n\t        output = self.shift_running(local_pos, local_rots, phases, A, F,style_code, noise,\n\t                                                                                start_id=start_id, target_id=target_id,length=seq,\n\t                                                                                phase_schedule=self.schedule_phase)\n\t        if(self.predict_phase):\n\t            pred_pos, pred_rot, pred_phase, latent_loss, first_phase = output\n\t        else:\n\t            pred_pos, pred_rot, pred_phase, latent_loss = output\n", "        rot_pos = self.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n\t        pred_pos[:, :, self.rot_rep_idx] = rot_pos[:, :, self.rot_rep_idx]\n\t        if (self.test == True or random.random() < 0.1):\n\t            pred_pos, pred_rot = self.regu_pose(pred_pos, edge_len, pred_rot)\n\t        edge_mean = 21.\n\t        glb_loss = self.l1_loss(local_pos[:, start_id:target_id, :]/edge_mean, pred_pos[:, :, :]/edge_mean)\n\t        glb_rot_loss = self.l1_loss(local_rots[:, start_id:target_id], pred_rot)\n\t        last_pos_loss = self.l1_loss(local_pos[:,target_id-1,:]/edge_mean,pred_pos[:,-1]/edge_mean)\n\t        last_rot_loss = self.l1_loss(local_rots[:,target_id-1,:],pred_rot[:,-1])\n\t        gt_contact = self.get_gt_contact(local_pos[:,start_id+1:target_id]-local_pos[:,start_id:target_id-1])\n", "        contact_loss = self.contact_foot_loss(gt_contact,pred_pos[:,1:]-pred_pos[:,:-1])\n\t        phase_loss = self.phase_loss(phases[:, start_id:target_id], A[:, start_id:target_id], F[:, start_id-1:target_id-1], pred_phase[0][:, :],pred_phase[1][:, :], pred_phase[2][:, :], pred_phase[3])\n\t        loss = {\"glb_pos\": glb_loss, \"glb_rot\": glb_rot_loss, \"lst_pos\": last_pos_loss, \"lst_rot\": last_rot_loss,\n\t                **phase_loss, \"fv\": contact_loss}\n\t        if(self.predict_phase):\n\t            pre_phase,pre_A,pre_S = first_phase\n\t            first_phase_loss = {\"f_ph\": self.mse_loss(phases[:,start_id-1], pre_phase), \"f_A\": self.mse_loss(A[:,start_id-1], pre_A), \"f_S\": self.mse_loss(S[:,start_id-1], pre_S)}\n\t            loss = {**loss,**first_phase_loss}\n\t        new_weight = {\"glb_pos\":1.,\"glb_rot\":1., \"fv\":0.01,\"lst_pos\":1.0,\"lst_rot\":0.5,\"phase\": 0.5, \"A\": 0.5, \"F\": 0.5, \"slerp_phase\": 0.5,\"f_ph\":0.5,\"f_A\":0.5,\"f_S\":0.5}\n\t        loss['loss'] = self.weight_sum_loss(loss, new_weight)\n", "        return loss, pred_pos, pred_rot\n\t    def un_normalized(self, batch):\n\t        batch['glb_vel'] = batch[\"glb_vel\"] * self.vel_std + self.vel_mean\n\t        batch['glb_rot'] = batch['glb_rot'] * self.rot_std+self.rot_mean\n\t        batch['glb_pos'] = batch['glb_pos'] * self.pos_std+self.pos_mean\n\t        return batch\n\t    def normalized(self, batch):\n\t        batch['glb_rot'] = (batch['glb_rot']-self.rot_mean)/self.rot_std\n\t        batch['glb_vel'] = (batch['glb_vel']-self.vel_mean)/self.vel_std\n\t        batch['glb_pos'] = (batch['glb_pos']-self.pos_mean)/self.pos_std\n", "        return batch\n\t    def transform_batch_to_filmEncoder(self,glb_pos,glb_rot):\n\t        glb_vel, glb_pos, glb_rot, root_rotation = self.batch_processor.forward(glb_rot, glb_pos)\n\t        glb_rot = quat_to_or6D(glb_rot)\n\t        batch = {'glb_vel': glb_vel, 'glb_rot': glb_rot, 'glb_pos': glb_pos}\n\t        batch = self.normalized(batch)\n\t        batch = {key: batch[key][:, :, 1:] for key in batch.keys()}\n\t        return self.transform_batch_to_input(batch)\n\t    def transform_batch_to_input(self,batch):\n\t        glb_vel, glb_rot, glb_pos = batch['glb_vel'],batch['glb_rot'], batch['glb_pos']\n", "        data = torch.cat((glb_pos[:,1:],glb_vel,glb_rot[:,1:]),dim=-1)\n\t        data  = data.permute(0,3,1,2).contiguous() # N,C,T,J\n\t        return data\n\t    def get_film_code(self,glb_pos,glb_rot):\n\t        if (glb_rot.shape[-1] == 6):\n\t            glb_rot = or6d_to_quat(glb_rot)\n\t        data = self.transform_batch_to_filmEncoder(glb_pos, glb_rot)\n\t        data = data.transpose(-2, -1).contiguous().flatten(1, 2)\n\t        if self.test ==False:\n\t            idx = torch.randperm(data.shape[0])\n", "        else:\n\t            idx = torch.arange(data.shape[0])\n\t        return self.film_generator(data[idx])#,idx\n\t    def transform_batch_to_VAE(self, batch):\n\t        local_pos, local_rot = batch['local_pos'], batch['local_rot']\n\t        edge_len = torch.norm(batch['offsets'][:, 1:], dim=-1, keepdim=True)\n\t        return local_pos, quat_to_or6D(local_rot), edge_len,batch['phase']\n\t    def validation_step(self, batch, batch_idx):\n\t        self.scheduled_prob = 1.\n\t        self.schedule_phase = 1.\n", "        self.style_phase = 1.\n\t        self.test =True\n\t        loss,pred_pos,pred_rot = self.shared_forward(batch, self.seq_scheduler.max_seq)\n\t        self.common_operator.log_dict(self, loss, \"val_\")\n\t        return loss\n\t    def test_step(self, batch, batch_idx):\n\t        self.test=True\n\t        self.scheduled_prob = 1.\n\t        self.schedule_phase = 1.\n\t        self.style_phase = 1.\n", "        loss, pred_pos, pred_rot = self.shared_forward(batch, self.seq_scheduler.max_seq)\n\t        self.common_operator.log_dict(self, loss, \"test_\")\n\t        return loss\n\t    def training_step(self, batch, batch_idx):\n\t        self.test =False\n\t        if(self.mode=='pretrain'):\n\t            progress = self.common_operator.get_progress(self, target_epoch=3,start_epoch=0)\n\t            self.schedule_phase = self.common_operator.get_progress(self,target_epoch=4,start_epoch=1)\n\t            self.style_phase = self.common_operator.get_progress(self,target_epoch=6,start_epoch=3)\n\t        else:\n", "            progress = 1\n\t            self.schedule_phase = 1.\n\t        length = self.seq_scheduler.range(progress)\n\t        '''calculate loss'''\n\t        loss,pred_pos,pred_rot = self.shared_forward(batch, length)\n\t        self.common_operator.log_dict(self, loss, \"train_\")\n\t        self.log(\"length\", length, logger=True)\n\t        return loss['loss']\n\t    def configure_optimizers(self):\n\t        models = self.common_operator.collect_models(self)\n", "        trained_model = []\n\t        for param in self.parameters():\n\t            param.requires_grad = False\n\t        def weight_decay(model_name,lr, weight_decay):\n\t            trained_model.append(model_name)\n\t            model = getattr(self,model_name)\n\t            for param in model.parameters():\n\t                param.requires_grad = True\n\t            return self.common_operator.add_weight_decay(model,lr, weight_decay)\n\t        lr = self.lr\n", "        if(self.mode=='pretrain' and self.predict_phase==False):\n\t            params = weight_decay(\"film_generator\",lr,1e-4)+weight_decay(\"target_encoder\", lr, 0) + weight_decay(\"embedding_style\",lr,0)+\\\n\t                     weight_decay(\"offset_encoder\", lr, 0) +\\\n\t                     weight_decay(\"state_encoder\", lr, 0) + \\\n\t                     weight_decay(\"LSTMCell\", lr, 0) + weight_decay(\"phase_predictor\",lr,0)#+weight_decay(\"decoder\",lr,0)\n\t        elif(self.predict_phase == True):\n\t            params = weight_decay(\"initial_state_predictor\",lr,1e-4)\n\t        elif(self.mode=='fine_tune'):\n\t            lr = self.lr*0.1\n\t            params = weight_decay(\"film_generator\", lr, 1e-4) + weight_decay(\"target_encoder\", lr, 0) + weight_decay(\n", "                \"embedding_style\", lr, 0) + \\\n\t                     weight_decay(\"offset_encoder\", lr, 0) + \\\n\t                     weight_decay(\"state_encoder\", lr, 0) + \\\n\t                     weight_decay(\"LSTMCell\", lr, 0) + weight_decay(\"phase_predictor\", lr, 0)\n\t        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.5, 0.9), amsgrad=True)\n\t        non_train_model=[i for i in models if i not in trained_model]\n\t        if(len(non_train_model)>0):\n\t            import warnings\n\t            warnings.warn(\"warning:there are some models not trained:{}\".format(non_train_model))\n\t        return [optimizer]\n", "from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n\tclass Application_phase(nn.Module):\n\t    def __init__(self, net: TransitionNet_phase, data_module: StyleVAE_DataModule):\n\t        super(Application_phase, self).__init__()\n\t        self.Net = net\n\t        self.data_module = data_module\n\t        self.data_loader = data_module.loader\n\t        self.src_batch = None\n\t        self.target_batch = None\n\t        self.skeleton = data_module.skeleton\n", "    def transform_batch(self,motion,label):\n\t        batch = [[],label]\n\t        batch[0] = [[motion[0],motion[1],motion[2]]]\n\t        for i, value in enumerate(batch[0][0]):\n\t            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n\t        phase = {}\n\t        for key in motion[3].keys():\n\t            phase[key] = torch.from_numpy(motion[3][key]).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n\t        batch[0][0].append(phase)\n\t        batch = self.data_module.transfer_mannual(batch,0,use_phase=True,use_sty=False)\n", "        return batch\n\t    def transform_anim(self, anim, label):\n\t        batch = [[], label]\n\t        batch[0] = [[anim.quats, anim.offsets, anim.hip_pos]]\n\t        for i, value in enumerate(batch[0][0]):\n\t            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n\t        batch = self.data_module.transfer_mannual(batch, 0,use_phase=False)\n\t        # batch = normalized(self.Net,batch,True)\n\t        return batch\n\t    def setSource(self, anim):\n", "        if(type(anim)==tuple):\n\t            self.src_batch = self.transform_batch(anim,0)\n\t        else:\n\t            self.src_batch = self.transform_anim(anim, 0)\n\t        self.offsets = self.src_batch['offsets']\n\t        self.tangent = find_secondary_axis(self.offsets)\n\t    def setTarget(self, anim):\n\t        if (type(anim) == tuple):\n\t            self.target_anim = self.transform_batch(anim, 2)\n\t        else:\n", "            self.target_anim = self.transform_anim(anim, 2)\n\t    def _get_transform_ori_motion(self, batch):\n\t        global_pos = batch['local_pos']\n\t        global_rot = batch['local_rot']\n\t        global_rot = self.skeleton.inverse_pos_to_rot(global_rot, global_pos, self.offsets, self.tangent)\n\t        lp, lq = self.skeleton.inverse_kinematics(global_rot, global_pos)\n\t        return lp[0].cpu().numpy(), lq[0].cpu().numpy()\n\t    def get_source(self):\n\t        return self._get_transform_ori_motion(self.src_batch)\n\t    def get_target(self):\n", "        return self._get_transform_ori_motion(self.target_anim)\n\t    def forward(self,t,x):\n\t        self.Net.schedule_phase = 1.\n\t        self.Net.style_phase = 1.\n\t        seq = self.Net.seq_scheduler.max_seq\n\t        # seq = 10\n\t        self.Net.eval()\n\t        with torch.no_grad():\n\t            loc_pos, loc_rot, edge_len,phases = self.Net.transform_batch_to_VAE(self.src_batch)\n\t            tar_pos,tar_rot ,_,_= self.Net.transform_batch_to_VAE(self.target_anim)\n", "            target_style = self.Net.get_film_code(tar_pos,tar_rot)\n\t            A = self.src_batch['A']\n\t            S = self.src_batch['S']\n\t            F = S[:,1:]-S[:,:-1]\n\t            F = self.Net.phase_op.remove_F_discontiny(F)\n\t            F = F/self.Net.phase_op.dt\n\t            if x !=1 :\n\t                loc_pos[:, 12:, :, [0, 2]] = loc_pos[:, 12:, :, [0, 2]] + (\n\t                            loc_pos[:, 10 + seq, 0, [0, 2]] - loc_pos[:, 10, 0, [0, 2]]) * x\n\t            if(self.Net.predict_phase):\n", "                pred_pos, pred_rot, pred_phase, _,_ = self.Net.shift_running(loc_pos, loc_rot, phases, A, F,\n\t                                                                          target_style, None, start_id=10,\n\t                                                                          target_id=10 + seq, length=int(seq * t),\n\t                                                                          phase_schedule=1.)\n\t            else:\n\t                pred_pos, pred_rot,pred_phase,_= self.Net.shift_running(loc_pos, loc_rot, phases,A,F,target_style, None, start_id=10,\n\t                                                            target_id=10 + seq,length = int(seq*t) ,phase_schedule=1.)\n\t            rot_pos = self.Net.rot_to_pos(pred_rot,self.src_batch['offsets'],pred_pos[:,:,0:1])\n\t            pred_pos[:,:,self.Net.rot_rep_idx] = rot_pos[:,:,self.Net.rot_rep_idx]\n\t            output_pos, output_rot = self.Net.regu_pose(pred_pos, edge_len, pred_rot)\n", "            output_pos = torch.cat((loc_pos[:, :10, ...], output_pos, loc_pos[:, 10 + seq:, ...]), dim=1)\n\t            output_rot = torch.cat((loc_rot[:, :10, ...], output_rot, loc_rot[:, 10 + seq:, ...]), dim=1)\n\t            batch = {}\n\t            batch['local_rot'] = or6d_to_quat(output_rot)\n\t            batch['local_pos'] = output_pos\n\t            return self._get_transform_ori_motion(batch)\n\t            # output = self.src\n"]}
{"filename": "src/utils/locate_model.py", "chunked_list": ["import re\n\timport os\n\tdef locate_model(check_file:str,epoch):\n\t    if(epoch=='last'):\n\t        check_file+=\"last.ckpt\"\n\t        return check_file\n\t    dirs = os.listdir(check_file)\n\t    for dir in dirs:\n\t        st = \"epoch=\" + epoch + \"-step=\\d+.ckpt\"\n\t        out = re.findall(st, dir)\n", "        if (len(out) > 0):\n\t            check_file += out[0]\n\t            print(check_file)\n\t            return check_file\n"]}
{"filename": "src/utils/motion_process.py", "chunked_list": ["import numpy as np\n\tfrom np_vector import quat_mul,quat_mul_vec,quat_inv,normalize,quat_between\n\tfrom BVH_mod import  Anim\n\tdef angle_between_quats_along_y(from_quat,to_quat):\n\t    \"\"\"\n\t        Quats tensor for current rotations (B, 4)\n\t        :return\n\t    \"\"\"\n\t    #mask = np.tile(np.array([[1.0, 0.0, 1.0]], dtype=np.float),(from_quat.shape[0],1))\n\t    initial_direction = np.array([[0.0, 0.0, 1.0]], dtype=np.float)\n", "    quat = quat_mul(to_quat,quat_inv(from_quat))\n\t    dir = quat_mul_vec(quat,initial_direction)\n\t   # dir[...,1] = 0\n\t   # dir = normalize(dir)\n\t    return np.arctan2(dir[...,0],dir[...,2])\n\tdef find_Yrotation_to_align_with_Xplus(q):\n\t    \"\"\"\n\t    :param q: Quats tensor for current rotations (B, 4)\n\t    :return y_rotation: Quats tensor of rotations to apply to q to align with X+\n\t    \"\"\"\n", "    mask = np.array([[1.0, 0.0, 1.0]], dtype=np.float)#(q.shape[0], -1)\n\t    mask = np.tile(mask,(q.shape[0],q.shape[1],1))\n\t    '''用来度量quat的方向'''\n\t    initial_direction = np.tile(np.array([[1.0, 0.0, 0.0]], dtype=np.float),(q.shape[0],q.shape[1],1))\n\t    forward = mask * quat_mul_vec(q, initial_direction)\n\t    forward = normalize(forward)\n\t    y_rotation = normalize(quat_between(forward, np.array([[1, 0, 0]])))\n\t    return y_rotation\n\t'''We keep batch version only'''\n\tdef extract_feet_contacts(pos:np.array , lfoot_idx, rfoot_idx, velfactor=0.02):\n", "    assert len(pos.shape)==4\n\t    \"\"\"\n\t    Extracts binary tensors of feet contacts\n\t    :param pos: tensor of global positions of shape (Timesteps, Joints, 3)\n\t    :param lfoot_idx: indices list of left foot joints\n\t    :param rfoot_idx: indices list of right foot joints\n\t    :param velfactor: velocity threshold to consider a joint moving or not\n\t    :return: float tensors of left foot contacts and right foot contacts\n\t    \"\"\"\n\t    lfoot_xyz = (pos[:,1:, lfoot_idx, :] - pos[:,:-1, lfoot_idx, :]) ** 2\n", "    contacts_l = (np.sum(lfoot_xyz, axis=-1) < velfactor).astype(np.float)\n\t    rfoot_xyz = (pos[:,1:, rfoot_idx, :] - pos[:,:-1, rfoot_idx, :]) ** 2\n\t    contacts_r = ((np.sum(rfoot_xyz, axis=-1)) < velfactor).astype(np.float)\n\t    # Duplicate the last frame for shape consistency\n\t    contacts_l = np.concatenate([contacts_l, contacts_l[:,-1:]], axis=1)\n\t    contacts_r = np.concatenate([contacts_r, contacts_r[:,-1:]], axis=1)\n\t    return [contacts_l,contacts_r]\n\tdef subsample(anim:Anim,ratio = 2):\n\t    anim.quats = anim.quats[::ratio,...]\n\t    anim.hip_pos = anim.hip_pos[::ratio,...]\n", "    return anim"]}
{"filename": "src/utils/np_vector.py", "chunked_list": ["import numpy\n\timport numpy as np\n\timport pytorch3d.transforms\n\t_FLOAT_EPS = np.finfo(np.float32).eps\n\t_EPS4 = _FLOAT_EPS * 4.0\n\t_EPS16 = _FLOAT_EPS * 16.0\n\tdef clamp_mean(x,axis):\n\t    x = np.asarray(x,dtype=np.float64)\n\t    return np.mean(x,axis=axis)\n\tdef clamp_std(x,axis):\n", "    x = np.asarray(x,dtype=np.float64)\n\t    std = np.std(x,axis=axis)\n\t    return np.where(np.abs(std) < _EPS4, _EPS4, std)\n\tdef length(x, axis=-1, keepdims=True):\n\t    \"\"\"\n\t    Computes vector norm along a tensor axis(axes)\n\t    :param x: tensor\n\t    :param axis: axis(axes) along which to compute the norm\n\t    :param keepdims: indicates if the dimension(s) on axis should be kept\n\t    :return: The length or vector of lengths.\n", "    \"\"\"\n\t    lgth = np.sqrt(np.sum(x * x, axis=axis, keepdims=keepdims))\n\t    return lgth\n\tdef normalize(x, axis=-1):\n\t    \"\"\"\n\t    Normalizes a tensor over some axis (axes)\n\t    :param x: data tensor\n\t    :param axis: axis(axes) along which to compute the norm\n\t    :param eps: epsilon to prevent numerical instabilities\n\t    :return: The normalized tensor\n", "    \"\"\"\n\t    x = np.array(x,dtype=np.float64)\n\t    res = x / (length(x, axis=axis) + _EPS4)\n\t    return res\n\tdef quat_normalize(x):\n\t    \"\"\"\n\t    Normalizes a quaternion tensor\n\t    :param x: data tensor\n\t    :param eps: epsilon to prevent numerical instabilities\n\t    :return: The normalized quaternions tensor\n", "    \"\"\"\n\t    res = normalize(x)\n\t    return res\n\tdef angle_axis_to_quat(angle, axis):\n\t    \"\"\"\n\t    Converts from and angle-axis representation to a quaternion representation\n\t    :param angle: angles tensor\n\t    :param axis: axis tensor\n\t    :return: quaternion tensor\n\t    \"\"\"\n", "    c = np.cos(angle / 2.0)[..., np.newaxis]\n\t    s = np.sin(angle / 2.0)[..., np.newaxis]\n\t    q = np.concatenate([c, s * axis], axis=-1)\n\t    return q\n\tdef euler_to_quat(e, order='zyx'):\n\t    \"\"\"\n\t    Converts from an euler representation to a quaternion representation\n\t    :param e: euler tensor\n\t    :param order: order of euler rotations\n\t    :return: quaternion tensor\n", "    \"\"\"\n\t    axis = {\n\t        'x': np.asarray([1, 0, 0], dtype=np.float32),\n\t        'y': np.asarray([0, 1, 0], dtype=np.float32),\n\t        'z': np.asarray([0, 0, 1], dtype=np.float32)}\n\t    q0 = angle_axis_to_quat(e[..., 0], axis[order[0]])\n\t    q1 = angle_axis_to_quat(e[..., 1], axis[order[1]])\n\t    q2 = angle_axis_to_quat(e[..., 2], axis[order[2]])\n\t    return quat_mul(q0, quat_mul(q1, q2))\n\tdef quat_inv(q):\n", "    \"\"\"\n\t    Inverts a tensor of quaternions\n\t    :param q: quaternion tensor\n\t    :return: tensor of inverted quaternions\n\t    \"\"\"\n\t    res = np.asarray([1, -1, -1, -1], dtype=np.float32) * q\n\t    return res\n\t# def quat_fk(lrot, lpos, parents):\n\t#     \"\"\"\n\t#     Performs Forward Kinematics (FK) on local quaternions and local positions to retrieve global representations\n", "#\n\t#     :param lrot: tensor of local quaternions with shape (..., Nb of joints, 4)\n\t#     :param lpos: tensor of local positions with shape (..., Nb of joints, 3)\n\t#     :param parents: list of parents indices\n\t#     :return: tuple of tensors of global quaternion, global positions\n\t#     \"\"\"\n\t#     gp, gr = [lpos[..., :1, :]], [lrot[..., :1, :]]\n\t#     for i in range(1, len(parents)):\n\t#         gp.append(quat_mul_vec(gr[parents[i]], lpos[..., i:i+1, :]) + gp[parents[i]])\n\t#         gr.append(quat_mul    (gr[parents[i]], lrot[..., i:i+1, :]))\n", "#\n\t#     res = np.concatenate(gr, axis=-2), np.concatenate(gp, axis=-2)\n\t#     return res\n\t#\n\t#\n\t# def quat_ik(grot, gpos, parents):\n\t#     \"\"\"\n\t#     Performs Inverse Kinematics (IK) on global quaternions and global positions to retrieve local representations\n\t#\n\t#     :param grot: tensor of global quaternions with shape (..., Nb of joints, 4)\n", "#     :param gpos: tensor of global positions with shape (..., Nb of joints, 3)\n\t#     :param parents: list of parents indices\n\t#     :return: tuple of tensors of local quaternion, local positions\n\t#     \"\"\"\n\t#     res = [\n\t#         np.concatenate([\n\t#             grot[..., :1, :],\n\t#             quat_mul(quat_inv(grot[..., parents[1:], :]), grot[..., 1:, :]),\n\t#         ], axis=-2),\n\t#         np.concatenate([\n", "#             gpos[..., :1, :],\n\t#             quat_mul_vec(\n\t#                 quat_inv(grot[..., parents[1:], :]),\n\t#                 gpos[..., 1:, :] - gpos[..., parents[1:], :]),\n\t#         ], axis=-2)\n\t#     ]\n\t#\n\t#     return res\n\tdef quat_mul(x, y):\n\t    \"\"\"\n", "    Performs quaternion multiplication on arrays of quaternions\n\t    :param x: tensor of quaternions of shape (..., Nb of joints, 4)\n\t    :param y: tensor of quaternions of shape (..., Nb of joints, 4)\n\t    :return: The resulting quaternions\n\t    \"\"\"\n\t    x0, x1, x2, x3 = x[..., 0:1], x[..., 1:2], x[..., 2:3], x[..., 3:4]\n\t    y0, y1, y2, y3 = y[..., 0:1], y[..., 1:2], y[..., 2:3], y[..., 3:4]\n\t    res = np.concatenate([\n\t        y0 * x0 - y1 * x1 - y2 * x2 - y3 * x3,\n\t        y0 * x1 + y1 * x0 - y2 * x3 + y3 * x2,\n", "        y0 * x2 + y1 * x3 + y2 * x0 - y3 * x1,\n\t        y0 * x3 - y1 * x2 + y2 * x1 + y3 * x0], axis=-1)\n\t    return res\n\tdef quat_mul_vec(q, x):\n\t    \"\"\"\n\t    Performs multiplication of an array of 3D vectors by an array of quaternions (rotation).\n\t    :param q: tensor of quaternions of shape (..., Nb of joints, 4)\n\t    :param x: tensor of vectors of shape (..., Nb of joints, 3)\n\t    :return: the resulting array of rotated vectors\n\t    \"\"\"\n", "    t = 2.0 * np.cross(q[..., 1:], x)\n\t    res = x + q[..., 0][..., np.newaxis] * t + np.cross(q[..., 1:], t)\n\t    return res\n\tdef quat_slerp(x, y, a):\n\t    \"\"\"\n\t    Perfroms spherical linear interpolation (SLERP) between x and y, with proportion a\n\t    :param x: quaternion tensor\n\t    :param y: quaternion tensor\n\t    :param a: indicator (between 0 and 1) of completion of the interpolation.\n\t    :return: tensor of interpolation results\n", "    \"\"\"\n\t    len = np.sum(x * y, axis=-1)\n\t    neg = len < 0.0\n\t    len[neg] = -len[neg]\n\t    y[neg] = -y[neg]\n\t    a = np.zeros_like(x[..., 0]) + a\n\t    amount0 = np.zeros(a.shape)\n\t    amount1 = np.zeros(a.shape)\n\t    linear = (1.0 - len) < 0.01\n\t    omegas = np.arccos(len[~linear])\n", "    sinoms = np.sin(omegas)\n\t    amount0[linear] = 1.0 - a[linear]\n\t    amount0[~linear] = np.sin((1.0 - a[~linear]) * omegas) / sinoms\n\t    amount1[linear] = a[linear]\n\t    amount1[~linear] = np.sin(a[~linear] * omegas) / sinoms\n\t    res = amount0[..., np.newaxis] * x + amount1[..., np.newaxis] * y\n\t    return res\n\tdef quat_between(x, y):\n\t    \"\"\"\n\t    Quaternion rotations between two 3D-vector arrays\n", "    :param x: tensor of 3D vectors\n\t    :param y: tensor of 3D vetcors\n\t    :return: tensor of quaternions\n\t    \"\"\"\n\t    res = np.concatenate([\n\t        np.sqrt(np.sum(x * x, axis=-1) * np.sum(y * y, axis=-1))[..., np.newaxis] +\n\t        np.sum(x * y, axis=-1)[..., np.newaxis],\n\t        np.cross(x, y)], axis=-1)\n\t    return res\n\tdef interpolate_local(lcl_r_mb,lcl_q_mb,length,target):\n", "    # Extract last past frame and target frame\n\t    start_lcl_r_mb = lcl_r_mb[:, 10 - 1, :, :][:, None, :, :]  # (B, 1, J, 3)\n\t    end_lcl_r_mb = lcl_r_mb[:, target, :, :][:, None, :, :]\n\t    start_lcl_q_mb = lcl_q_mb[:, 10 - 1, :, :]\n\t    end_lcl_q_mb = lcl_q_mb[:, target, :, :]\n\t    # LERP Local Positions:\n\t    n_trans = length\n\t    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n\t    offset = end_lcl_r_mb - start_lcl_r_mb\n\t    const_trans = np.tile(start_lcl_r_mb, [1, n_trans + 2, 1, 1])\n", "    inter_lcl_r_mb = const_trans + (interp_ws)[None, :, None, None] * offset\n\t    # SLERP Local Quats:\n\t    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n\t    inter_lcl_q_mb = np.stack(\n\t        [(quat_normalize(quat_slerp(quat_normalize(start_lcl_q_mb), quat_normalize(end_lcl_q_mb), w))) for w in\n\t         interp_ws], axis=1)\n\t    return inter_lcl_r_mb, inter_lcl_q_mb\n\tdef interpolate_local_(lcl_r_mb, lcl_q_mb, n_past, n_future):\n\t    \"\"\"\n\t    Performs interpolation between 2 frames of an animation sequence.\n", "    The 2 frames are indirectly specified through n_past and n_future.\n\t    SLERP is performed on the quaternions\n\t    LERP is performed on the root's positions.\n\t    :param lcl_r_mb:  Local/Global root positions (B, T, 1, 3)\n\t    :param lcl_q_mb:  Local quaternions (B, T, J, 4)\n\t    :param n_past:    Number of frames of past context\n\t    :param n_future:  Number of frames of future context\n\t    :return: Interpolated root and quats\n\t    \"\"\"\n\t    # Extract last past frame and target frame\n", "    start_lcl_r_mb = lcl_r_mb[:, n_past - 1, :, :][:, None, :, :]  # (B, 1, J, 3)\n\t    end_lcl_r_mb = lcl_r_mb[:, -n_future, :, :][:, None, :, :]\n\t    start_lcl_q_mb = lcl_q_mb[:, n_past - 1, :, :]\n\t    end_lcl_q_mb = lcl_q_mb[:, -n_future, :, :]\n\t    # LERP Local Positions:\n\t    n_trans = lcl_r_mb.shape[1] - (n_past + n_future)\n\t    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n\t    offset = end_lcl_r_mb - start_lcl_r_mb\n\t    const_trans    = np.tile(start_lcl_r_mb, [1, n_trans + 2, 1, 1])\n\t    inter_lcl_r_mb = const_trans + (interp_ws)[None, :, None, None] * offset\n", "    # SLERP Local Quats:\n\t    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n\t    inter_lcl_q_mb = np.stack(\n\t        [(quat_normalize(quat_slerp(quat_normalize(start_lcl_q_mb), quat_normalize(end_lcl_q_mb), w))) for w in\n\t         interp_ws], axis=1)\n\t    return inter_lcl_r_mb, inter_lcl_q_mb\n\timport torch\n\tdef remove_quat_discontinuities(rotations:torch.tensor):\n\t    \"\"\"\n\t    Removing quat discontinuities on the time dimension (removing flips)\n", "    :param rotations: Array of quaternions of shape (T, J, 4)\n\t    :return: The processed array without quaternion inversion.\n\t    \"\"\"\n\t    rotations = rotations.clone()\n\t    rots_inv = -rotations\n\t    for i in range(1, rotations.shape[1]):\n\t        replace_mask = torch.sum(rotations[:, i - 1:i, ...] * rotations[:, i:i + 1, ...],\n\t                                 dim=-1, keepdim=True) < \\\n\t                       torch.sum(rotations[:, i - 1:i, ...] * rots_inv[:, i:i + 1, ...],\n\t                                 dim=-1, keepdim=True)\n", "        replace_mask = replace_mask.squeeze(1).type_as(rotations)\n\t        rotations[:, i, ...] = replace_mask * rots_inv[:, i, ...] + (1.0 - replace_mask) * rotations[:, i, ...]\n\t    return rotations\n\t# Orient the data according to the las past keframe\n\t# def rotate_at_frame(X, Q, parents, n_past=10):\n\t#     \"\"\"\n\t#     Re-orients the animation data according to the last frame of past context.\n\t#\n\t#     :param X: tensor of local positions of shape (Batchsize, Timesteps, Joints, 3)\n\t#     :param Q: tensor of local quaternions (Batchsize, Timesteps, Joints, 4)\n", "#     :param parents: list of parents' indices\n\t#     :param n_past: number of frames in the past context\n\t#     :return: The rotated positions X and quaternions Q\n\t#     \"\"\"\n\t#     # Get global quats and global poses (FK)\n\t#     global_q, global_x = quat_fk(Q, X, parents)\n\t#\n\t#     key_glob_Q = global_q[:, n_past - 1: n_past, 0:1, :]  # (B, 1, 1, 4)\n\t#     forward = np.array([1, 0, 1])[np.newaxis, np.newaxis, np.newaxis, :] \\\n\t#                  * quat_mul_vec(key_glob_Q, np.array([0, 1, 0])[np.newaxis, np.newaxis, np.newaxis, :])\n", "#     forward = normalize(forward)\n\t#     yrot = quat_normalize(quat_between(np.array([1, 0, 0]), forward))#1,0,0\n\t#     new_glob_Q = quat_mul(quat_inv(yrot), global_q)\n\t#     new_glob_X = quat_mul_vec(quat_inv(yrot), global_x)\n\t#\n\t#     # back to local quat-pos\n\t#     Q, X = quat_ik(new_glob_Q, new_glob_X, parents)\n\t#\n\t#     return X, Q\n\tdef extract_feet_contacts(pos, lfoot_idx, rfoot_idx, velfactor=0.02):\n", "    \"\"\"\n\t    Extracts binary tensors of feet contacts\n\t    :param pos: tensor of global positions of shape (Timesteps, Joints, 3)\n\t    :param lfoot_idx: indices list of left foot joints\n\t    :param rfoot_idx: indices list of right foot joints\n\t    :param velfactor: velocity threshold to consider a joint moving or not\n\t    :return: binary tensors of left foot contacts and right foot contacts\n\t    \"\"\"\n\t    lfoot_xyz = (pos[1:, lfoot_idx, :] - pos[:-1, lfoot_idx, :]) ** 2\n\t    contacts_l = (np.sqrt(np.sum(lfoot_xyz, axis=-1)) < velfactor)\n", "    rfoot_xyz = (pos[1:, rfoot_idx, :] - pos[:-1, rfoot_idx, :]) ** 2\n\t    contacts_r = (np.sqrt(np.sum(rfoot_xyz, axis=-1)) < velfactor)\n\t    # Duplicate the last frame for shape consistency\n\t    contacts_l = np.concatenate([contacts_l, contacts_l[-1:]], axis=0)\n\t    contacts_r = np.concatenate([contacts_r, contacts_r[-1:]], axis=0)\n\t    return contacts_l, contacts_r\n\t##########\n\t# My own code\n\t#########\n\t# axis sequences for Euler angles\n", "def quat_to_mat(quat):\n\t    \"\"\" Convert Quaternion to Euler Angles.  See rotation.py for notes \"\"\"\n\t    quat = np.asarray(quat, dtype=np.float64)\n\t    assert quat.shape[-1] == 4, \"Invalid shape quat {}\".format(quat.shape)\n\t    w, x, y, z = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]\n\t    Nq = np.sum(quat * quat, axis=-1)\n\t    s = 2.0 / Nq\n\t    X, Y, Z = x * s, y * s, z * s\n\t    wX, wY, wZ = w * X, w * Y, w * Z\n\t    xX, xY, xZ = x * X, x * Y, x * Z\n", "    yY, yZ, zZ = y * Y, y * Z, z * Z\n\t    mat = np.empty(quat.shape[:-1] + (3, 3), dtype=np.float64)\n\t    mat[..., 0, 0] = 1.0 - (yY + zZ)\n\t    mat[..., 0, 1] = xY - wZ\n\t    mat[..., 0, 2] = xZ + wY\n\t    mat[..., 1, 0] = xY + wZ\n\t    mat[..., 1, 1] = 1.0 - (xX + zZ)\n\t    mat[..., 1, 2] = yZ - wX\n\t    mat[..., 2, 0] = xZ - wY\n\t    mat[..., 2, 1] = yZ + wX\n", "    mat[..., 2, 2] = 1.0 - (xX + yY)\n\t    return np.where((Nq > _FLOAT_EPS)[..., np.newaxis, np.newaxis], mat, np.eye(3))\n\tdef mat_to_euler(mat):\n\t    \"\"\" Convert Rotation Matrix to Euler Angles.  See rotation.py for notes \"\"\"\n\t    mat = np.asarray(mat, dtype=np.float64)\n\t    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\t    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])\n\t    condition = cy > _EPS4\n\t    euler = np.empty(mat.shape[:-1], dtype=np.float64)\n\t    euler[..., 2] = np.where(condition,\n", "                             -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),\n\t                             -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]))\n\t    euler[..., 1] = np.where(condition,\n\t                             -np.arctan2(-mat[..., 0, 2], cy),\n\t                             -np.arctan2(-mat[..., 0, 2], cy))\n\t    euler[..., 0] = np.where(condition,\n\t                             -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]),\n\t                             0.0)\n\t    return euler\n\tdef quat_to_euler(quat):\n", "    \"\"\" Convert Quaternion to Euler Angles.  See rotation.py for notes \"\"\"\n\t    return mat_to_euler(quat_to_mat(quat))\n"]}
{"filename": "src/utils/__init__.py", "chunked_list": ["import os\n\timport sys\n\tBASEPATH = os.path.dirname(__file__)\n\tsys.path.insert(0, BASEPATH)"]}
{"filename": "src/utils/Drawer.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport matplotlib\n\t#matplotlib.use('TKAgg')\n\tclass Drawer():\n\t    def __init__(self):\n\t        str_dict = {\"point\":\"o\",}\n\t        pass\n\t    def create_canvas(self,m,n):\n\t        self.fig,self.ax = plt.subplots(m,n,figsize=(m,n*2))\n\t    def get_window(self,window):\n", "        return self.ax[window]\n\t    def draw(self,*args,channel,feature):\n\t        self.ax[channel,feature].plot(*args)\n\t    def scatter(self,*args,channel,feature):\n\t        self.ax[channel,feature].scatter(*args)\n\t    def show(self):\n\t        plt.show()"]}
{"filename": "src/utils/BVH_mod.py", "chunked_list": ["import re\n\timport numpy as np\n\timport src.geometry.forward_kinematics as fk\n\timport src.geometry.inverse_kinematics as ik\n\timport torch\n\tfrom src.geometry.vector import find_secondary_axis\n\tfrom np_vector import euler_to_quat,remove_quat_discontinuities,quat_to_euler\n\timport pytorch3d.transforms as trans\n\tchannelmap = {\n\t    'Xrotation' : 'x',\n", "    'Yrotation' : 'y',\n\t    'Zrotation' : 'z'   \n\t}\n\tchannelmap_inv = {\n\t    'x': 'Xrotation',\n\t    'y': 'Yrotation',\n\t    'z': 'Zrotation',\n\t}\n\tordermap = {\n\t    'x' : 0,\n", "    'y' : 1,\n\t    'z' : 2,\n\t}\n\tdef find_sys_with_prefix(names,name,l_prefix,r_prefix):\n\t    if (name.lower().startswith(l_prefix)):\n\t        l_joint_name = name[len(l_prefix):]\n\t        for rName in names:\n\t            if(rName.lower().startswith(r_prefix)):\n\t                r_joint_name = rName[len(r_prefix):]\n\t                if(l_joint_name == r_joint_name):\n", "                    return rName\n\t    return \"\"\n\tdef find_sys(names,name):\n\t    sys_part = find_sys_with_prefix(names,name,\"l\",\"r\")\n\t    if(sys_part==\"\"):\n\t        sys_part = find_sys_with_prefix(names, name, \"left\", \"right\")\n\t    return sys_part\n\tdef find_level_recursive(map,parent,node,level_joint,level_joint_parent,level):\n\t    if(level not in level_joint):\n\t        level_joint[level]=[]\n", "    if(level not in level_joint_parent):\n\t        level_joint_parent[level]=[]\n\t    level_joint[level].append(node)\n\t    level_joint_parent[level].append(parent)\n\t    for child_id in map[node][\"children\"]:\n\t        find_level_recursive(map,node,child_id,level_joint,level_joint_parent,level+1)\n\tdef find_link_recursive(map,node,link):\n\t    for child_id in map[node][\"children\"]:\n\t        link.append((node,child_id))\n\t        find_link_recursive(map,child_id,link)\n", "class Skeleton(object):\n\t    def __init__(self,parents,names,offsets,symmetry=None):\n\t        self.num_joints = len(parents)\n\t        self.root = 0\n\t        self.parents = parents\n\t        self.names = names\n\t        self.hierarchy = {}\n\t        self.name_to_id = {}\n\t        self.__build_name_map()\n\t        self.bone_pair_indices = symmetry\n", "        self._level_joints = []\n\t        self._level_joints_parents=[]\n\t        if(symmetry==None):\n\t            self.__predict_sysmetry()\n\t        self.predict_sysmetry_axis(offsets)\n\t        self.__build_hierarchy()\n\t        self.__build_level_map()\n\t        self.self_link = [(i,i) for i in range(len(parents))]\n\t        self.neighbor_link =[]\n\t        find_link_recursive(self.hierarchy,self.root,self.neighbor_link)\n", "        self.build_end_effector()\n\t    def __eq__(self, other):\n\t        if(other==None):\n\t            return False\n\t        return self.parents == other.parents and self.names == other.names and self.bone_pair_indices == other.bone_pair_indices\n\t    def build_end_effector(self):\n\t        self.multi_children_idx = []\n\t        self.end_effector_idx = []\n\t        for i in range(len(self.names)):\n\t            child = [*self.hierarchy[i]['children'].keys()]\n", "            if(len(child)>1):\n\t                self.multi_children_idx.append(i)\n\t            elif(len(child)==0):\n\t                self.end_effector_idx.append(i)\n\t    def __build_name_map(self):\n\t        self.hierarchy[-1] = {\"name\":\"\"}\n\t        self.name_to_id[''] = -1\n\t        for idx, name in enumerate(self.names):\n\t            self.hierarchy[idx] = {\"name\": name}\n\t            self.name_to_id[name] = idx\n", "    def predict_sysmetry_axis(self,offsets):\n\t        offsets = offsets[self.bone_pair_indices]-offsets\n\t        x = np.max(np.abs(offsets)[...,0])\n\t        y = np.max(np.abs(offsets)[...,1])\n\t        z = np.max(np.abs(offsets)[...,2])\n\t        if(x>y and x>z):\n\t            self.sys = \"x\"\n\t        elif(y>z and y>x):\n\t            self.sys = 'y'\n\t        elif(z>y and z>x):\n", "            self.sys = 'z'\n\t        else:\n\t            assert(0,\"Unpredicition\")\n\t    def __predict_sysmetry(self):\n\t        '''根据骨骼名字猜测对称骨骼idx，可能不准确，如果没有对称骨骼，则该位置为-1'''\n\t        self.bone_pair_indices = [-1 for _ in range(len(self.parents))]\n\t        for idx, name in enumerate(self.names):\n\t            if(idx in self.bone_pair_indices):\n\t                continue\n\t            sys_name = (find_sys(self.names,name))\n", "            sys_idx = self.name_to_id[sys_name]\n\t            self.bone_pair_indices[idx] = sys_idx\n\t            if(sys_idx==-1):\n\t                self.bone_pair_indices[idx] = idx\n\t            else:\n\t                self.bone_pair_indices[sys_idx] = idx\n\t    def __build_hierarchy(self):\n\t        '''构建骨骼名字，idx，parent,children，对称等索引'''\n\t        mapping = self.hierarchy\n\t        for idx, (name,par_id) in enumerate(zip(self.names, self.parents)):\n", "            children_id = []\n\t            children = {}\n\t            for child_id,child_parent in enumerate(self.parents):\n\t                if(child_parent==idx):\n\t                    children_id.append(child_id)\n\t                    children[child_id] = mapping[child_id]\n\t            mapping[idx][\"parent\"] = mapping[par_id]\n\t            mapping[idx][\"children\"] = children\n\t            if(self.bone_pair_indices[idx]!=-1):\n\t                mapping[idx][\"sys\"] = mapping[self.bone_pair_indices[idx]]\n", "    def __build_level_map(self):\n\t        level_joint = {}\n\t        level_joint_parent = {}\n\t        find_level_recursive(self.hierarchy,-1,self.root,level_joint,level_joint_parent,0)\n\t        for i in range(len(level_joint)):\n\t            self._level_joints.append(level_joint[i])\n\t            self._level_joints_parents.append(level_joint_parent[i])\n\t    def forward_kinematics(self,local_rotations: torch.Tensor, local_offsets: torch.Tensor,hip_positions:torch.Tensor):\n\t        #gp,gq =  fk.forward_kinematics_quats(local_rotations, local_offsets,hip_positions, self._level_joints, self._level_joints_parents)\n\t        gp,gq =  fk.forward_kinematics_quats(local_rotations, local_offsets,hip_positions, self.parents)\n", "        return gp,gq\n\t    def inverse_kinematics_quats(self,joints_global_rotation:torch.Tensor):\n\t        return ik.inverse_kinematics_only_quats(joints_global_rotation, self.parents)\n\t    def inverse_kinematics(self,joints_global_rotations: torch.Tensor, joints_global_positions: torch.Tensor,output_other_dim = False):\n\t        lp,lq =  ik.inverse_kinematics_quats(joints_global_rotations, joints_global_positions, self.parents)\n\t        if(output_other_dim):\n\t            return lp,lq\n\t        else:\n\t            return lp[...,0:1,:],lq\n\t    def global_rot_to_global_pos(self,global_rot,offsets,hip):\n", "        # quick version of fk(ik()), only for global_rotation\n\t        pos_shape = global_rot.shape[:-1]+(3,)\n\t        offsets = offsets.unsqueeze(-3).expand(pos_shape)\n\t        #hip = torch.zeros_like(offsets[...,:1,:])\n\t        gp = torch.empty(pos_shape,dtype=global_rot.dtype,device = global_rot.device)#[hip]\n\t        gp[...,0:1,:] = hip[...,0:1,:]\n\t        offset = trans.quaternion_apply(global_rot[..., self.parents[1:], :], offsets[..., 1:, :])\n\t        for level in range(1, len(self._level_joints)):\n\t            local_bone_indices = self._level_joints[level]\n\t            local_bone_indices_1 = [i-1 for i in local_bone_indices]\n", "            parent_bone_indices = self._level_joints_parents[level]\n\t            gp[...,local_bone_indices,:] = offset[...,local_bone_indices_1,:]+gp[...,parent_bone_indices,:]\n\t        # for i in range(1, len(self.parents)):\n\t        #     gp.append(offset[..., i-1:i, :] + gp[self.parents[i]])\n\t        # gp = torch.cat(gp, dim=-2)\n\t        return gp\n\t    def inverse_pos_to_rot(self,joints_global_rotations,joints_global_positions,offsets,tangent):\n\t        edge_len = torch.norm(offsets[:,1:],dim=-1,keepdim=True)\n\t        joints_global_positions = ik.scale_pos_to_bonelen(joints_global_positions,edge_len,self._level_joints,self._level_joints_parents)\n\t        parent_rot = self.pos_to_parent_rot(joints_global_rotations,joints_global_positions,offsets,tangent)\n", "        out = self.reduce_parent_rot(parent_rot)\n\t        special_idx = [*self.end_effector_idx,*self.multi_children_idx,*[9,10,11,12,13,15,19]]\n\t        out[...,special_idx,:] = joints_global_rotations[...,special_idx,:]\n\t        return out\n\t    def pos_to_parent_rot(self,joints_global_rotations:torch.Tensor,joints_global_positions:torch.Tensor,local_offsets:torch.Tensor,secondary_offsets:torch.Tensor):\n\t        return ik.change_pos_to_rot(joints_global_rotations, joints_global_positions, local_offsets, secondary_offsets,self.parents)\n\t    def reduce_parent_rot(self,parent_rot):\n\t        shape = list(parent_rot.shape)\n\t        shape[-2] += 1\n\t        rot = torch.empty(size=shape,device=parent_rot.device,dtype=parent_rot.dtype)\n", "        #children = [[*self.hierarchy[i]['children'].keys()] for i in range(len(self.names))]\n\t        idx = []\n\t        child_idx = []\n\t        for i in range(len(self.names)):\n\t            child = [*self.hierarchy[i]['children'].keys()]\n\t            if(len(child)==1):\n\t                idx.append(i)\n\t                child_idx.append(child[0]-1)\n\t        rot[...,idx,:] = parent_rot[...,child_idx,:]\n\t            # else:\n", "            #     rot[..., i, :] = torch.zeros_like(rot[..., i, :])\n\t        return rot\n\tclass SkeletonLevel():\n\t    def __init__(self,n_joints,edges,root,body_parts,last_n_joints, pooling_list=None,unpooling_list=None):\n\t        self.n_joints = n_joints\n\t        self.last_n_joints = last_n_joints\n\t        self.edges = edges\n\t        self.pooling_list = pooling_list\n\t        self.unpooling_list = unpooling_list\n\t        self.root = root\n", "        self.body_parts = body_parts\n\tclass Anim(object):\n\t    \"\"\"\n\t    A very basic animation object\n\t    \"\"\"\n\t    def __init__(self, quats, pos, offsets, parents, names,remove_joints = None,Tpose = -1,remove_gap:float = 0.,filename= \"\"):\n\t        \"\"\"\n\t        :param quats: local quaternions tensor\n\t        :param pos: local positions tensor\n\t        :param offsets: local joint offsets\n", "        :param parents: bone hierarchy\n\t        :param names: bone names\n\t        :param valid_names: useful for remove unnecessary joints\n\t        \"\"\"\n\t        from src.Datasets.BatchProcessor import BatchProcessData\n\t        process = BatchProcessData()\n\t        self.quats = quats\n\t        self.hip_pos = pos\n\t        self.offsets = offsets\n\t        self.parents = parents\n", "        self.names = names\n\t        if(remove_joints!=None):\n\t            self.quats,self.offsets,self.parents,self.names = remove_joints(self.quats,self.offsets,self.parents,self.names)\n\t        self.skeleton = Skeleton(self.parents,self.names,self.offsets)\n\t        if(remove_gap>0.):\n\t            gp,gq = self.skeleton.forward_kinematics(torch.from_numpy(self.quats),torch.from_numpy(self.offsets),torch.from_numpy(self.hip_pos))\n\t            idx = check_continuty(gp,remove_gap)\n\t            #print(filename)\n\t            idx = list(set(idx))\n\t            idx.sort()\n", "            dict = process(gq.unsqueeze(0),gp.unsqueeze(0))\n\t            lp = dict['local_pos'][0]\n\t            idx1 = check_continuty(lp,remove_gap)\n\t            idx1 = list(set(idx1))\n\t            idx1.sort()\n\t            if(idx[-1]>10 or idx1[-1]>10):\n\t                print(filename)\n\t                print()\n\t            idx = max(idx[-1],idx1[-1])+1\n\t            self.quats = self.quats[idx:]\n", "            self.hip_pos = self.hip_pos[idx:]\n\t        self.secondary_offsets = find_secondary_axis(torch.from_numpy(self.offsets)).numpy()\n\t    def forward_kinamatics(self):\n\t        return self.skeleton.forward_kinematics(torch.from_numpy(self.quats),torch.from_numpy(self.offsets),torch.from_numpy(self.hip_pos))\n\tdef check_continuty(x,threshold):\n\t    nei = (x[1:]-x[:-1]).abs()\n\t    index = np.where(nei>threshold)[0]\n\t    index = (set(index))\n\t    vel = (nei[1:]-nei[:-1]).abs()\n\t    id = np.where(vel>threshold/10)[0]\n", "    id = (set(id))\n\t    index = index.intersection(id)\n\t    index = list(index)\n\t    index.sort()\n\t    if (len(index) == 0):\n\t        return [-1]\n\t    seq_start = index[-1]+1\n\t    if(seq_start>10):\n\t        print({key:float(nei[key].max()) for key in index})\n\t       # print(\"remove gap:{}, max gap{}, remove gap{}\".format(seq_start,nei.max(),nei[seq_start-1].max()))\n", "    return index\n\tdef remove_zero_seq(hip_pos):\n\t    '''把一段序列中hip_pos在原点之前的子序列移除'''\n\t    '''[seq,1,3]'''\n\t    index = np.where(np.dot(np.abs(hip_pos),np.array([1,1,1]))==0)\n\t    if(index[0].shape[0]==0):\n\t        return 0\n\t    seq_start = index[0][-1]\n\t    return seq_start\n\tdef read_bvh(filename, start=None, end=None, order=None,remove_joints = None,Tpose = -1,remove_gap :float=0.):\n", "    print(filename)\n\t    \"\"\"\n\t    Reads a BVH file and extracts animation information.\n\t    :param filename: BVh filename\n\t    :param start: start frame\n\t    :param end: end frame\n\t    :param order: order of euler rotations\n\t    :return: A simple Anim object conatining the extracted information.\n\t    \"\"\"\n\t    f = open(filename, \"r\")\n", "    i = 0\n\t    active = -1\n\t    end_site = False\n\t    names = []\n\t    orients = np.array([]).reshape((0, 4))\n\t    offsets = np.array([]).reshape((0, 3))\n\t    parents = np.array([], dtype=int)\n\t    # Parse the  file, line by line\n\t    for line in f:\n\t        if \"HIERARCHY\" in line: continue\n", "        if \"MOTION\" in line: continue\n\t        rmatch = re.match(r\"ROOT (\\w+)\", line)\n\t        if rmatch:\n\t            names.append(rmatch.group(1))\n\t            offsets = np.append(offsets, np.array([[0, 0, 0]]), axis=0)\n\t            orients = np.append(orients, np.array([[1, 0, 0, 0]]), axis=0)\n\t            parents = np.append(parents, active)\n\t            active = (len(parents) - 1)\n\t            continue\n\t        if \"{\" in line: continue\n", "        if \"}\" in line:\n\t            if end_site:\n\t                end_site = False\n\t            else:\n\t                active = parents[active]\n\t            continue\n\t        offmatch = re.match(r\"\\s*OFFSET\\s+([\\-\\d\\.e]+)\\s+([\\-\\d\\.e]+)\\s+([\\-\\d\\.e]+)\", line)\n\t        if offmatch:\n\t            if not end_site:\n\t                offsets[active] = np.array([list(map(float, offmatch.groups()))])\n", "            continue\n\t        chanmatch = re.match(r\"\\s*CHANNELS\\s+(\\d+)\", line)\n\t        if chanmatch:\n\t            channels = int(chanmatch.group(1))\n\t            if order is None:\n\t                channelis = 0 if channels == 3 else 3\n\t                channelie = 3 if channels == 3 else 6\n\t                parts = line.split()[2 + channelis:2 + channelie]\n\t                if any([p not in channelmap for p in parts]):\n\t                    continue\n", "                order = \"\".join([channelmap[p] for p in parts])\n\t            continue\n\t        jmatch = re.match(\"\\s*JOINT\\s+(\\w+)\", line)\n\t        if jmatch:\n\t            names.append(jmatch.group(1))\n\t            offsets = np.append(offsets, np.array([[0, 0, 0]]), axis=0)\n\t            orients = np.append(orients, np.array([[1, 0, 0, 0]]), axis=0)\n\t            parents = np.append(parents, active)\n\t            active = (len(parents) - 1)\n\t            continue\n", "        if \"End Site\" in line:\n\t            end_site = True\n\t            continue\n\t        fmatch = re.match(\"\\s*Frames:\\s+(\\d+)\", line)\n\t        if fmatch:\n\t            if start and end:\n\t                fnum = (end - start) - 1\n\t            else:\n\t                fnum = int(fmatch.group(1))\n\t            positions = offsets[np.newaxis].repeat(fnum, axis=0)\n", "            rotations = np.zeros((fnum, len(orients), 3))\n\t            continue\n\t        fmatch = re.match(\"\\s*Frame Time:\\s+([\\d\\.]+)\", line)\n\t        if fmatch:\n\t            frametime = float(fmatch.group(1))\n\t            continue\n\t        if (start and end) and (i < start or i >= end - 1):\n\t            i += 1\n\t            continue\n\t        dmatch = line.strip().split(' ')\n", "        if dmatch:\n\t            data_block = np.array(list(map(float, dmatch)))\n\t            N = len(parents)\n\t            fi = i - start if start else i\n\t            if channels == 3:\n\t                positions[fi, 0:1] = data_block[0:3]\n\t                rotations[fi, :] = data_block[3:].reshape(N, 3)\n\t            elif channels == 6:\n\t                data_block = data_block.reshape(N, 6)\n\t                positions[fi, :] = data_block[:, 0:3]\n", "                rotations[fi, :] = data_block[:, 3:6]\n\t            elif channels == 9:\n\t                positions[fi, 0] = data_block[0:3]\n\t                data_block = data_block[3:].reshape(N - 1, 9)\n\t                rotations[fi, 1:] = data_block[:, 3:6]\n\t                positions[fi, 1:] += data_block[:, 0:3] * data_block[:, 6:9]\n\t            else:\n\t                raise Exception(\"Too many channels! %i\" % channels)\n\t            i += 1\n\t    f.close()\n", "    rotations = euler_to_quat(np.radians(rotations), order=order)\n\t    rotations = remove_quat_discontinuities(torch.from_numpy(rotations).unsqueeze(0))[0].numpy()\n\t    return Anim(rotations, positions[...,0:1,:], offsets, parents, names,remove_joints,Tpose=Tpose,remove_gap=remove_gap,filename=filename)\n\t#anim.pos: only root position writen into file, anim.offset: write all joints to file\n\tdef save_bvh(filename, anim, names=None, frametime=1.0 / 30.0, order='xyz', positions=False, orients=True):\n\t    \"\"\"\n\t    Saves an Animation to file as BVH\n\t    Parameters\n\t    ----------\n\t    filename: str\n", "        File to be saved to\n\t    anim : Animation\n\t        Animation to save\n\t    names : [str]\n\t        List of joint names\n\t    order : str\n\t        Optional Specifier for joint order.\n\t        Given as string E.G 'xyz', 'zxy'\n\t    frametime : float\n\t        Optional Animation Frame time\n", "    positions : bool\n\t        Optional specfier to save bone\n\t        positions for each frame\n\t    orients : bool\n\t        Multiply joint orients to the rotations\n\t        before saving.\n\t    \"\"\"\n\t    if names is None:\n\t        if anim.names is None:\n\t            names = [\"joint_\" + str(i) for i in range(len(anim.parents))]\n", "        else:\n\t            names = anim.names\n\t    print(filename)\n\t    with open(filename, 'w') as f:\n\t        t = \"\"\n\t        f.write(\"%sHIERARCHY\\n\" % t)\n\t        f.write(\"%sROOT %s\\n\" % (t, names[0]))\n\t        f.write(\"%s{\\n\" % t)\n\t        t += '\\t'\n\t        f.write(\"%sOFFSET %f %f %f\\n\" % (t, anim.offsets[0, 0], anim.offsets[0, 1], anim.offsets[0, 2]))\n", "        f.write(\"%sCHANNELS 6 Xposition Yposition Zposition %s %s %s \\n\" %\n\t                (t, channelmap_inv[order[0]], channelmap_inv[order[1]], channelmap_inv[order[2]]))\n\t        num_joints=anim.parents.shape[0]\n\t        assert num_joints==anim.offsets.shape[0]\n\t        seq_length=anim.quats.shape[0]\n\t        assert seq_length==anim.hip_pos.shape[0]\n\t        for i in range(anim.parents.shape[0]):#iterate joints\n\t            if anim.parents[i] == 0:\n\t                t = save_joint(f, anim, names, t, i, order=order, positions=positions)\n\t        t = t[:-1]\n", "        f.write(\"%s}\\n\" % t)\n\t        f.write(\"MOTION\\n\")\n\t        f.write(\"Frames: %i\\n\" % anim.quats.shape[0]);\n\t        f.write(\"Frame Time: %f\\n\" % frametime);\n\t        # if orients:\n\t        #    rots = np.degrees((-anim.orients[np.newaxis] * anim.rotations).euler(order=order[::-1]))\n\t        # else:\n\t        #    rots = np.degrees(anim.rotations.euler(order=order[::-1]))\n\t        rots = np.degrees(quat_to_euler(anim.quats))\n\t        poss = anim.hip_pos\n", "        if len(poss.shape)==2:\n\t            poss=poss[:,np.newaxis,:]\n\t        for i in range(poss.shape[0]):#frame\n\t            for j in range(anim.parents.shape[0]):#joints\n\t                if positions or j == 0:\n\t                    f.write(\"%f %f %f %f %f %f \" % (\n\t                        poss[i, j, 0], poss[i, j, 1], poss[i, j, 2],\n\t                        rots[i, j, ordermap[order[0]]], rots[i, j, ordermap[order[1]]], rots[i, j, ordermap[order[2]]]))\n\t                else:\n\t                    f.write(\"%f %f %f \" % (\n", "                        rots[i, j, ordermap[order[0]]], rots[i, j, ordermap[order[1]]], rots[i, j, ordermap[order[2]]]))\n\t            f.write(\"\\n\")\n\tdef save_joint(f, anim, names, t, i, order='zyx', positions=False):\n\t    f.write(\"%sJOINT %s\\n\" % (t, names[i]))\n\t    f.write(\"%s{\\n\" % t)\n\t    t += '\\t'\n\t    f.write(\"%sOFFSET %f %f %f\\n\" % (t, anim.offsets[i, 0], anim.offsets[i, 1], anim.offsets[i, 2]))\n\t    if positions:\n\t        f.write(\"%sCHANNELS 6 Xposition Yposition Zposition %s %s %s \\n\" % (t,\n\t                                                                            channelmap_inv[order[0]],\n", "                                                                            channelmap_inv[order[1]],\n\t                                                                            channelmap_inv[order[2]]))\n\t    else:\n\t        f.write(\"%sCHANNELS 3 %s %s %s\\n\" % (t,\n\t                                             channelmap_inv[order[0]], channelmap_inv[order[1]],\n\t                                             channelmap_inv[order[2]]))\n\t    end_site = True\n\t    for j in range(anim.parents.shape[0]):\n\t        if anim.parents[j] == i:\n\t            t = save_joint(f, anim, names, t, j, order=order, positions=positions)\n", "            end_site = False\n\t    if end_site:\n\t        f.write(\"%sEnd Site\\n\" % t)\n\t        f.write(\"%s{\\n\" % t)\n\t        t += '\\t'\n\t        f.write(\"%sOFFSET %f %f %f\\n\" % (t, 0.0, 0.0, 0.0))\n\t        t = t[:-1]\n\t        f.write(\"%s}\\n\" % t)\n\t    t = t[:-1]\n\t    f.write(\"%s}\\n\" % t)\n", "    return t\n"]}
{"filename": "src/Module/Utilities.py", "chunked_list": ["from torch import nn\n\timport torch\n\tclass PLU(torch.nn.Module):\n\t  def __init__(self,w=None,c=None):\n\t    super(PLU, self).__init__()\n\t    if w==None:\n\t      self.w=torch.nn.Parameter(torch.Tensor([0.1,]))\n\t    else:\n\t      self.w=torch.nn.Parameter(torch.Tensor([w,]))\n\t    self.w.requires_grad=True\n", "    if(c == None):\n\t      self.c=torch.nn.Parameter(torch.Tensor([1]))\n\t    else:\n\t      self.c=torch.nn.Parameter(torch.Tensor([c]))\n\t    self.c.requires_grad=True\n\t  def forward(self, x):\n\t        # max(w(x+c)-c,min(w(x-c)+c,x))\n\t        return torch.max(self.w * (x + self.c) - self.c, torch.min(self.w * (x - self.c) + self.c, x))\n\t''' from [Liu et al.2019]'''\n"]}
{"filename": "src/Module/__init__.py", "chunked_list": ["import os\n\timport sys\n\tBASEPATH = os.path.dirname(__file__)\n\tsys.path.insert(0, BASEPATH)"]}
{"filename": "src/Module/MoEModule.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport math\n\tclass MultipleExpertsLinear(nn.Module):\n\t    def __init__(self,in_channels,out_channels,nums):\n\t        super(MultipleExpertsLinear, self).__init__()\n\t        self.num_experts = nums\n\t        weight_list = []\n\t        bias_list = []\n\t        for i in range(nums):\n", "            weight = nn.Parameter(torch.empty(out_channels,in_channels))\n\t            bias = nn.Parameter(torch.empty(out_channels))\n\t            weight,bias = self.reset_parameter(weight,bias)\n\t            weight_list.append(weight.unsqueeze(0))\n\t            bias_list.append(bias.unsqueeze(0))\n\t        self.weight = nn.Parameter(torch.cat(weight_list,dim=0))\n\t        self.bias = nn.Parameter(torch.cat(bias_list,dim=0))\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        #self.linear = torch.nn.Linear(in_channels,out_channels)\n", "    def reset_parameter(self,weight,bias):\n\t        nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n\t        if bias is not None:\n\t            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weight)\n\t            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n\t            nn.init.uniform_(bias, -bound, bound)\n\t        return weight,bias\n\t    def forward(self,x,coefficients):\n\t        '''coefficients: N,num_experts'''\n\t        '''x: N,C'''\n", "        mixed_weight = torch.einsum(\"bc,cmn->bmn\",(coefficients,self.weight))\n\t        mixed_bias = torch.matmul(coefficients,self.bias).unsqueeze(2)# N, out_channels, 1\n\t        x = x.unsqueeze(2)  # N,1,in_channels\n\t        out = torch.baddbmm(mixed_bias,mixed_weight,x).squeeze(2)\n\t        return out\n"]}
{"filename": "src/Module/PhaseModule.py", "chunked_list": ["import torch\n\tfrom src.Module.Utilities import PLU\n\tclass PhaseSegement(torch.nn.Module):\n\t    def __init__(self,phase_dims):\n\t        super(PhaseSegement, self).__init__()\n\t        self.phase_dims = phase_dims\n\t        self.A_eps = torch.scalar_tensor(1e-6)\n\t        self.F_act = torch.nn.Tanh()\n\t    def forward(self,out):\n\t        # x:[...,4*phase_dims]\n", "        phase, A, F = out[..., :2 * self.phase_dims], out[..., 2 * self.phase_dims:3 * self.phase_dims], out[...,3 * self.phase_dims:4 * self.phase_dims]\n\t        return phase.view(phase.shape[:-1]+(self.phase_dims,2)), A.unsqueeze(-1),F.unsqueeze(-1)\n\tclass PhaseOperator():\n\t    def __init__(self,dt):\n\t        self.dt = dt\n\t        self._EPS32 = torch.finfo(torch.float32).eps\n\t        self.tpi = 2 * torch.pi\n\t        #self.phase_dim = phase_dim\n\t        pass\n\t    def phaseManifold(self,A,S):\n", "        assert A.shape[-1] == 1\n\t        out = torch.empty(size=(A.shape[:-1]+(2,)),device=A.device,dtype=A.dtype)\n\t        out[...,0:1] = A*torch.cos(self.tpi * S)\n\t        out[...,1:2] = A*torch.sin(self.tpi * S)\n\t        return out\n\t    def remove_F_discontiny(self,F):\n\t        F = torch.where(F <= -0.5, 1 + F, F)\n\t        F = torch.where(F >= 0.5, F - 1, F)\n\t        return F\n\t    def getA(self,phase):\n", "        # phase should be (...,phase_dim,2)\n\t        #phase = phase.view(phase.shape[:-1]+(self.phase_dim, 2))\n\t        A = torch.norm(phase, dim=-1,keepdim=True)\n\t        return A\n\t    def normalized_phase(self,p):\n\t        A0 = torch.norm(p, dim=-1, keepdim=True)\n\t        A0 = torch.where(A0.abs() < self._EPS32, self._EPS32, A0)\n\t        return p/A0\n\t    def next_phase(self,last_phase,A,F):\n\t        theta = self.tpi * F * self.dt\n", "        #dA = self.dt * A\n\t        #last_phase = last_phase.view(last_phase.shape[0], self.phase_dim, 2)\n\t        cos = torch.cos(theta)\n\t        sin = torch.sin(theta)\n\t        R = torch.stack([cos, sin, -sin, cos], dim=-1).reshape(cos.shape[:-1] + (2, 2))  # N,10,2,2\n\t        # normalized\n\t        baseA = torch.norm(last_phase,dim=-1,keepdim=True)\n\t        baseA = torch.clamp_min(baseA,self._EPS32)\n\t        last_phase = last_phase/baseA\n\t        pred_phase = torch.matmul(last_phase.unsqueeze(dim=-2), R).squeeze(dim=-2)\n", "        pred_phase = A*pred_phase\n\t        # if (torch.isnan(last_phase).any() or torch.isinf(last_phase).any()):\n\t        #     print(\"nan in last_phase\")\n\t        # elif (torch.isnan(R).any()):\n\t        #     print(\"nan in R\")\n\t        # elif(torch.isnan(pred_phase).any()):\n\t        #     print(\"nan in pred_phase\")\n\t        #pred_phase = pred_phase/baseA\n\t        #pred_phase = pred_phase * (dA+baseA)\n\t        return pred_phase\n", "    def slerp(self,p0,p1):\n\t        # only work for t = 0.5, if p0 \\approx -p1, the results might have numerical error, we assume p0 is similar to p1\n\t        A0 = torch.norm(p0,dim=-1,keepdim=True)\n\t        A1 = torch.norm(p1,dim=-1,keepdim=True)\n\t        A0 = torch.where(A0.abs()<self._EPS32,self._EPS32,A0)\n\t        A1 = torch.where(A1.abs()<self._EPS32,self._EPS32,A1)\n\t        # normalized\n\t        p0 = p0/A0\n\t        p1 = p1/A1\n\t        p = self.normalized_phase(0.5*(p0+p1))\n", "        A = 0.5*(A0+A1)\n\t        return A*p"]}
{"filename": "src/Module/VAEModule.py", "chunked_list": ["import torch\n\tfrom torch import  nn\n\timport torch.nn.functional as F\n\tfrom src.geometry.quaternions import normalized_or6d\n\t_EPS32 = torch.finfo(torch.float32).eps\n\tclass VAE_Conv(nn.Module):\n\t    def __init__(self,in_channels,out_channels,kernel_size):\n\t        super(VAE_Conv, self).__init__()\n\t        assert kernel_size%2==1\n\t        padding = kernel_size//2\n", "        self.latent_size = out_channels\n\t        self.encode = nn.Conv1d(in_channels,2*out_channels,kernel_size,padding=padding,padding_mode=\"reflect\")\n\t    def reparameterize(self,mu,log_sigma,epsilon = None):\n\t        std = torch.exp(0.5*log_sigma)\n\t        if epsilon==None:\n\t            epsilon = torch.randn_like(std)\n\t        return mu+std*epsilon\n\t    def forward(self,input):\n\t        x = self.encode(input)\n\t        mu = x[:,:self.latent_size]\n", "        log_var = x[:,self.latent_size:]\n\t        return self.reparameterize(mu,log_var),mu,log_var\n\tclass VAE_Linear(nn.Module):\n\t    def __init__(self,in_channels,out_channels,output_ori=True):\n\t        super(VAE_Linear, self).__init__()\n\t        self.output_ori = output_ori\n\t        self.encode = nn.Linear(in_channels,2*out_channels)\n\t        self.N = out_channels\n\t        #self.encode_logvar = nn.Linear(in_channels,out_channels)\n\t    def reparameterize(self,mu,log_sigma,epsilon = None):\n", "        std = torch.exp(0.5*log_sigma)\n\t        if epsilon==None:\n\t            epsilon = torch.randn_like(std)\n\t        return mu+std*epsilon\n\t    def forward(self,input):\n\t        mu_logvar = self.encode(input)\n\t        mu,logvar = mu_logvar[:,:self.N],mu_logvar[:,self.N:]\n\t        #logvar = self.encode_logvar(input)\n\t        if(self.output_ori):\n\t            return self.reparameterize(mu,logvar),mu,logvar\n", "        else:\n\t            return self.reparameterize(mu,logvar)\n"]}
{"filename": "src/geometry/rotations.py", "chunked_list": ["from pytorch3d.transforms import rotation_6d_to_matrix, quaternion_to_matrix\n\timport torch.nn.functional as F\n\tfrom src.geometry.vector import cross_product, normalize_vector\n\timport torch\n\timport numpy as np\n\t# m: batch*3*3\n\t# out: batch*4*4\n\tdef get_4x4_rotation_matrix_from_3x3_rotation_matrix(m):\n\t    batch_size = m.shape[0]\n\t    row4 = torch.autograd.Variable(torch.zeros(batch_size, 1, 3).type_as(m))\n", "    m43 = torch.cat((m, row4), 1)  # batch*4,3\n\t    col4 = torch.autograd.Variable(torch.zeros(batch_size, 4, 1).type_as(m))\n\t    col4[:, 3, 0] = col4[:, 3, 0] + 1\n\t    out = torch.cat((m43, col4), 2)  # batch*4*4\n\t    return out\n\t# matrices batch*3*3\n\t# both matrix are orthogonal rotation matrices\n\t# out theta between 0 to 180 degree batch\n\tdef compute_geodesic_distance_from_two_matrices(m1, m2):\n\t    m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3\n", "    theta = compute_angle_from_rotation_matrix(m)\n\t    # theta = torch.min(theta, 2*np.pi - theta)\n\t    return theta\n\t# matrices batch*3*3\n\t# both matrix are orthogonal rotation matrices\n\t# out theta between 0 to 180 degree batch\n\tdef compute_angle_from_rotation_matrix(m):\n\t    batch = m.shape[0]\n\t    cos = (m[:, 0, 0] + m[:, 1, 1] + m[:, 2, 2] - 1) / 2\n\t    cos = torch.min(cos, torch.autograd.Variable(torch.ones(batch).type_as(m)))\n", "    cos = torch.max(cos, torch.autograd.Variable(torch.ones(batch).type_as(m)) * -1)\n\t    theta = torch.acos(cos)\n\t    return theta\n\t# axis: size (batch, 3)\n\t# output quat order is w, x, y, z\n\tdef get_random_rotation_around_axis(axis, return_quaternion=False):\n\t    batch = axis.shape[0]\n\t    axis = normalize_vector(axis)  # batch*3\n\t    theta = torch.FloatTensor(axis.shape[0]).uniform_(-np.pi, np.pi).type_as(axis)  # [0, pi] #[-180, 180]\n\t    sin = torch.sin(theta)\n", "    qw = torch.cos(theta)\n\t    qx = axis[:, 0] * sin\n\t    qy = axis[:, 1] * sin\n\t    qz = axis[:, 2] * sin\n\t    quaternion = torch.cat((qw.view(batch, 1), qx.view(batch, 1), qy.view(batch, 1), qz.view(batch, 1)), 1)\n\t    matrix = quaternion_to_matrix(quaternion)\n\t    if (return_quaternion == True):\n\t        return matrix, quaternion\n\t    else:\n\t        return matrix\n", "# axisAngle batch*4 angle, x,y,z\n\t# output quat order is w, x, y, z\n\tdef get_random_rotation_matrices_around_random_axis(batch, return_quaternion=False):\n\t    axis = torch.autograd.Variable(torch.randn(batch.shape[0], 3).type_as(batch))\n\t    return get_random_rotation_around_axis(axis, return_quaternion=return_quaternion)\n\t# matrices batch*3*3\n\t# both matrix are orthogonal rotation matrices\n\t# out theta between 0 to 180 degree batch\n\tdef compute_geodesic_distance_from_two_matrices(m1, m2):\n\t    batch = m1.shape[0]\n", "    m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3\n\t    cos = (m[:, 0, 0] + m[:, 1, 1] + m[:, 2, 2] - 1) / 2\n\t    cos = torch.min(cos, torch.autograd.Variable(torch.ones(batch).type_as(m1)))\n\t    cos = torch.max(cos, torch.autograd.Variable(torch.ones(batch).type_as(m1)) * -1)\n\t    theta = torch.acos(cos)\n\t    # theta = torch.min(theta, 2*np.pi - theta)\n\t    return theta\n\tdef geodesic_loss(gt_r_matrix, out_r_matrix):\n\t    theta = compute_geodesic_distance_from_two_matrices(gt_r_matrix, out_r_matrix)\n\t    error = theta.mean()\n", "    return error\n\tdef geodesic_loss_matrix3x3_matrix3x3(gt_r_matrix, out_r_matrix):\n\t    return geodesic_loss(gt_r_matrix, out_r_matrix)\n\tdef geodesic_loss_quat_ortho6d(quaternions, ortho6d):\n\t    assert quaternions.shape[-1] == 4, \"quaternions should have the last dimension length be 4\"\n\t    assert ortho6d.shape[-1] == 6, \"ortho6d should have the last dimension length be 6\"\n\t    # quat -> matrix3x3\n\t    matrix3x3_a = quaternion_to_matrix(quaternions)\n\t    # ortho6d -> matrix3x3\n\t    matrix3x3_b = rotation_6d_to_matrix(ortho6d)\n", "    return geodesic_loss_matrix3x3_matrix3x3(matrix3x3_a, matrix3x3_b)\n\tdef rotation_6d_to_matrix_no_cross(d6: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"\n\t    This is the pytorch3d implementation of the conversion,\n\t    but avoids the torch.cross() operator that cannot be exported with opset_version >= 11\n\t    \"\"\"\n\t    a1, a2 = d6[..., :3], d6[..., 3:]\n\t    b1 = F.normalize(a1, dim=-1)\n\t    b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n\t    b2 = F.normalize(b2, dim=-1)\n", "    b3 = cross_product(b1, b2)\n\t    return torch.stack((b1, b2, b3), dim=-2)\n"]}
{"filename": "src/geometry/inverse_kinematics.py", "chunked_list": ["from typing import List\n\timport torch\n\timport numpy as np\n\tfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply, quaternion_invert\n\t# performs differentiable forward kinematics\n\t# local_transforms: local transform matrix of each transform (B, J, 4, 4)\n\t# level_transforms: a list of hierarchy levels, ordered by distance to the root transforms in the hierarchy\n\t#                   each elements of the list is a list transform indexes\n\t#                   for instance level_transforms[3] contains a list of indices of all the transforms that are 3 levels\n\t#                   below the root transforms the indices should match these of the provided local transforms\n", "#                   for instance if level_transforms[4, 1] is equal to 7, it means that local_transform[:, 7, :, :]\n\t#                   contains the local matrix of a transform that is 4 levels deeper than the root transform\n\t#                   (ie there are 4 transforms above it in the hierarchy)\n\t# level_transform_parents: similar to level_transforms but contains the parent indices\n\t#                   for instance if level_transform_parents[4, 1] is equal to 5, it means that local_transform[:, 5, :, :]\n\t#                   contains the local matrix of the parent transform of the transform contained in level_transforms[4, 1]\n\t# out: world transform matrix of each transform in the batch (B, J, 4, 4). Order is the same as input\n\tdef invert_transform_hierarchy(global_transforms: torch.Tensor, level_transforms: List[List[int]],\n\t                               level_transform_parents: List[List[int]]):\n\t    # used to store local transforms\n", "    local_transforms = global_transforms.clone()\n\t    # then process all children transforms\n\t    for level in range(len(level_transforms)-1, 0, -1):\n\t        parent_bone_indices = level_transform_parents[level]\n\t        local_bone_indices = level_transforms[level]\n\t        parent_level_transforms = global_transforms[..., parent_bone_indices, :, :]\n\t        local_level_transforms = global_transforms[..., local_bone_indices, :, :]\n\t        local_matrix = torch.matmul(torch.inverse(parent_level_transforms),\n\t                                    local_level_transforms)\n\t        local_transforms[..., local_bone_indices, :, :] = local_matrix.type_as(local_transforms)\n", "    return local_transforms\n\t# Compute a transform matrix combining a rotation and translation (rotation is applied first)\n\t# translations: translation vectors (N, 3)\n\t# rotations: rotation matrices (N, 3, 3)\n\t# out: transform matrix (N, 4, 4)\n\tdef get_transform_matrix(translations: torch.Tensor, rotations: torch.Tensor):    \n\t    out_shape = np.array(rotations.shape)\n\t    out_shape[-2:] = 4    \n\t    out = torch.zeros(list(out_shape)).type_as(rotations)\n\t    out[..., 0:3, 0:3] = rotations\n", "    out[..., 0:3, 3] = translations\n\t    out[..., 3, 3] = 1.0\n\t    return out\n\t# performs differentiable inverse kinematics\n\t# global_rotations: global rotation matrices of each joint in the batch (B, J, 3, 3)\n\t# global_offset: local position offset of each joint in the batch (B, J, 3).\n\t#               This corresponds to the offset with respect to the parent joint when no rotation is applied\n\t# level_joints: a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n\t#               each elements of the list is a list transform indexes\n\t#               for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n", "#               below the root joints the indices should match these of the provided local rotations\n\t#               for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n\t#               contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n\t#               (ie there are 4 joints above it in the hierarchy)\n\t# level_joint_parents: similar to level_transforms but contains the parent indices\n\t#                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n\t#                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n\tdef inverse_kinematics(global_rotations: torch.Tensor, global_offsets: torch.Tensor,\n\t                       level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n\t    # compute global transformation matrix for each joints\n", "    global_transforms = get_transform_matrix(translations=global_offsets, rotations=global_rotations)\n\t    # used to store local transforms\n\t    local_transforms = global_transforms.clone()\n\t    # then process all children transforms\n\t    for level in range(len(level_joints)-1, 0, -1):\n\t        parent_bone_indices = level_joint_parents[level]\n\t        local_bone_indices = level_joints[level]\n\t        parent_level_transforms = global_transforms[..., parent_bone_indices, :, :]\n\t        local_level_transforms = global_transforms[..., local_bone_indices, :, :]\n\t        local_matrix = torch.matmul(torch.inverse(parent_level_transforms),\n", "                                    local_level_transforms)\n\t        local_transforms[..., local_bone_indices, :, :] = local_matrix.type_as(local_transforms)\n\t    return local_transforms\n\tdef inverse_kinematics_only_quats(joints_global_rotations: torch.Tensor, parents):\n\t    \"\"\"\n\t    Performs differentiable inverse kinematics with quaternion-based rotations\n\t    :param joints_global_rotations: tensor of global quaternions of shape (..., J, 4)\n\t    :param joints_global_positions: tensor of global positions of shape (..., J, 3)\n\t    :param a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n\t                           each elements of the list is a list transform indexes\n", "                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n\t                           below the root joints the indices should match these of the provided local rotations\n\t                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n\t                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n\t                           (ie there are 4 joints above it in the hierarchy)\n\t    :param level_joint_parents: similar to level_transforms but contains the parent indices\n\t                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n\t                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n\t    :return:\n\t        joints_local_positions: tensor of local bone offsets of shape (..., J, 3)\n", "        joints_local_rotations: tensor of local quaternions of shape (..., J, 4)\n\t    \"\"\"\n\t    joint_local_quats = joints_global_rotations.clone()\n\t    invert_global_rotations = quaternion_invert(joint_local_quats)\n\t    joint_local_quats = torch.cat((\n\t        joint_local_quats[..., :1, :],\n\t        quaternion_multiply(invert_global_rotations[..., parents[1:], :], joint_local_quats[..., 1:, :]),\n\t    ), dim=-2)\n\t    return joint_local_quats\n\tdef inverse_kinematics_quats(joints_global_rotations: torch.Tensor,joints_global_positions, parents):\n", "    \"\"\"\n\t    Performs differentiable inverse kinematics with quaternion-based rotations\n\t    :param joints_global_rotations: tensor of global quaternions of shape (..., J, 4)\n\t    :param joints_global_positions: tensor of global positions of shape (..., J, 3)\n\t    :param a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n\t                           each elements of the list is a list transform indexes\n\t                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n\t                           below the root joints the indices should match these of the provided local rotations\n\t                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n\t                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n", "                           (ie there are 4 joints above it in the hierarchy)\n\t    :param level_joint_parents: similar to level_transforms but contains the parent indices\n\t                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n\t                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n\t    :return:\n\t        joints_local_positions: tensor of local bone offsets of shape (..., J, 3)\n\t        joints_local_rotations: tensor of local quaternions of shape (..., J, 4)\n\t    \"\"\"\n\t    joint_local_quats = joints_global_rotations.clone()\n\t    invert_global_rotations = quaternion_invert(joint_local_quats)\n", "    joint_local_quats = torch.cat((\n\t        joint_local_quats[..., :1, :],\n\t        quaternion_multiply(invert_global_rotations[..., parents[1:], :], joint_local_quats[..., 1:, :]),\n\t    ), dim=-2)\n\t    joint_local_pos = torch.cat([joints_global_positions[...,:1,:],\n\t                                 quaternion_apply(invert_global_rotations[..., parents[1:], :],joints_global_positions[...,1:,:]-joints_global_positions[...,parents[1:],:])],dim=-2)\n\t    return joint_local_pos,joint_local_quats\n\tfrom src.geometry.vector import normalize_vector\n\tfrom src.geometry.quaternions import from_to_quaternion,quat_to_or6D\n\tdef scale_pos_to_bonelen(glb_pos:torch.Tensor,edge_len:torch.Tensor,level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n", "    edge_len = edge_len.unsqueeze(1).expand(glb_pos.shape[:2]+(glb_pos.shape[2]-1,3))\n\t    _EPS32 = torch.finfo(torch.float32).eps\n\t    #for level in range(len(level_joints)-1,0,-1):\n\t    # import modifications\n\t    for level in range(1,len(level_joints)):\n\t        parent_bone_indices = level_joint_parents[level]\n\t        local_bone_indices = level_joints[level]\n\t        local_bone_indices_1 = [i-1 for i in local_bone_indices]\n\t        parent_pos = glb_pos[:,:,parent_bone_indices]\n\t        local_pos = glb_pos[:,:,local_bone_indices]\n", "        dir = local_pos - parent_pos\n\t        length = torch.norm(dir,dim=-1,keepdim=True)\n\t        length = torch.where(length<_EPS32,_EPS32,length)\n\t        local_pos = dir/length*edge_len[:,:,local_bone_indices_1]+parent_pos\n\t        glb_pos[:,:,local_bone_indices] = local_pos\n\t    return glb_pos\n\tdef change_pos_to_rot(joints_global_rotations: torch.Tensor, joints_global_positions: torch.Tensor,offsets:torch.Tensor,secondary_offsets:torch.Tensor,parent:list):\n\t    \"\"\"\n\t    Performs differentiable inverse kinematics with quaternion-based rotations\n\t    :param joints_global_rotations: tensor of global quaternions of shape (..., J, 4)\n", "    :param joints_global_positions: tensor of global positions of shape (..., J, 3)\n\t    :param a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n\t                           each elements of the list is a list transform indexes\n\t                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n\t                           below the root joints the indices should match these of the provided local rotations\n\t                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n\t                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n\t                           (ie there are 4 joints above it in the hierarchy)\n\t    :param level_joint_parents: similar to level_transforms but contains the parent indices\n\t                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n", "                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n\t    :return:\n\t        joints_local_positions: tensor of local bone offsets of shape (..., J, 3)\n\t        joints_local_rotations: tensor of local quaternions of shape (..., J, 4)\n\t    \"\"\"\n\t    #joint_local_positions = joints_global_positions.clone()\n\t    joint_local_quats = joints_global_rotations.clone()\n\t    joint_offset = joints_global_positions[...,1:,:] - joints_global_positions[...,parent[1:],:]\n\t    joint_offset = normalize_vector(joint_offset)\n\t    offsets = offsets[...,1:,:].unsqueeze(-3).expand(joint_offset.shape)\n", "    offsets = normalize_vector(offsets)\n\t    secondary_offsets = secondary_offsets[...,1:,:].unsqueeze(-3).expand(joint_offset.shape)\n\t    joints_global_rotations = from_to_quaternion(offsets,joint_offset)\n\t    # find plane P of joint offset\n\t    tangent = quaternion_apply(joints_global_rotations, secondary_offsets)\n\t    #co_tangent = normalize_vector(torch.cross(joint_offset,tangent))\n\t    # project the secondary offsets to plane\n\t    init_ore = quaternion_apply(joint_local_quats[...,parent[1:],:],secondary_offsets)\n\t    dot = lambda x,y: (x*y).sum(-1,keepdim=True)\n\t    init_ore = init_ore - dot(init_ore,joint_offset)*joint_offset\n", "    #now init_ore and tangent should be in plane P\n\t    rot = from_to_quaternion(tangent,init_ore)\n\t    joints_global_rotations = quaternion_multiply(rot,joints_global_rotations)\n\t    return joints_global_rotations\n"]}
{"filename": "src/geometry/quaternions.py", "chunked_list": ["import torch\n\timport torch.nn.functional as F\n\tfrom src.geometry.vector import normalize_vector\n\timport numpy as np\n\timport pytorch3d.transforms\n\tdef rotation_6d_to_matrix_no_normalized(d6: torch.Tensor) -> torch.Tensor:\n\t    a1, a2 = d6[..., :3], d6[..., 3:]\n\t    # b1 = F.normalize(a1, dim=-1)\n\t    # b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n\t    # b2 = F.normalize(b2, dim=-1)\n", "    # b3 = torch.cross(b1, b2, dim=-1)\n\t    return torch.stack((a1, a2), dim=-2)\n\tdef rotation6d_multiply(r1,r2):\n\t    r1 = pytorch3d.transforms.rotation_6d_to_matrix(r1)\n\t    r2 = pytorch3d.transforms.rotation_6d_to_matrix(r2)\n\t    return pytorch3d.transforms.matrix_to_rotation_6d(torch.matmul(r1,r2))\n\tdef rotation6d_apply(r,p):\n\t    #p :...3\n\t    r = pytorch3d.transforms.rotation_6d_to_matrix(r)\n\t    p = p.unsqueeze(-1)#...3,1\n", "    return torch.matmul(r,p).squeeze(-1)\n\tdef rotation6d_inverse(r):\n\t    r = pytorch3d.transforms.rotation_6d_to_matrix(r)\n\t    inv_r = torch.transpose(r,-2,-1)\n\t    return pytorch3d.transforms.matrix_to_rotation_6d(inv_r)\n\tdef quat_to_or6D(quat):\n\t    assert(quat.shape[-1]==4)\n\t    return pytorch3d.transforms.matrix_to_rotation_6d(pytorch3d.transforms.quaternion_to_matrix(quat))\n\tdef or6d_to_quat(mat):\n\t    assert (mat.shape[-1] == 6)\n", "    return pytorch3d.transforms.matrix_to_quaternion(pytorch3d.transforms.rotation_6d_to_matrix(mat))\n\tdef normalized_or6d(d6):\n\t    a1, a2 = d6[..., :3], d6[..., 3:]\n\t    b1 = F.normalize(a1, dim=-1)\n\t    b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n\t    b2 = F.normalize(b2, dim=-1)\n\t    return torch.cat((b1, b2), dim=-1)\n\t#移除四元数不连续的问题\n\tdef remove_quat_discontinuities(rotations):\n\t    rotations = rotations.clone()\n", "    rots_inv = -rotations\n\t    for i in range(1, rotations.shape[1]):\n\t        replace_mask = torch.sum(rotations[:, i-1:i, ...] * rotations[:, i:i+1, ...], \n\t                                 dim=-1, keepdim=True) < \\\n\t                       torch.sum(rotations[:, i-1:i, ...] * rots_inv[:, i:i+1, ...], \n\t                                 dim=-1, keepdim=True)\n\t        replace_mask = replace_mask.squeeze(1).type_as(rotations)\n\t        rotations[:, i, ...] = replace_mask * rots_inv[:, i, ...] + (1.0 - replace_mask) * rotations[:, i, ...]\n\t    return rotations\n\tdef from_to_1_0_0(v_from):\n", "    '''v_from: B,3'''\n\t    v_to_unit = torch.tensor(np.array([[1, 0, 0]]), dtype=v_from.dtype, device=v_from.device).expand(v_from.shape)\n\t    y = torch.tensor(np.array([[0,1,0]]),dtype=v_from.dtype,device=v_from.device).expand(v_from.shape)\n\t    z_to_unit = torch.tensor(np.array([[0,0,1]]),dtype=v_from.dtype,device=v_from.device).expand(v_from.shape)\n\t    v_from_unit = normalize_vector(v_from)\n\t    z_from_unit = normalize_vector(torch.cross(v_from_unit,y,dim=-1))\n\t    to_transform = torch.stack([v_to_unit,y,z_to_unit],dim=-1)\n\t    from_transform = torch.stack([v_from_unit,y,z_from_unit],dim=-1)\n\t    shape = to_transform.shape\n\t    to_transform = to_transform.view(-1,3,3)\n", "    from_transform = from_transform.view(-1,3,3)\n\t    r = torch.matmul(to_transform,from_transform.transpose(1,2)).view(shape)\n\t    rq = pytorch3d.transforms.matrix_to_quaternion(r)\n\t    # w = (v_from_unit * v_to_unit).sum(dim=1) + 1\n\t    # '''can't cross if two directions are exactly inverse'''\n\t    # xyz = torch.cross(v_from_unit, v_to_unit, dim=1)\n\t    # '''if exactly inverse, the rotation should be (0,0,1,0), around yaxis 180'''\n\t    # xyz[...,1] = torch.where(w==0,torch.tensor([1],dtype=v_from.dtype,device=xyz.device),xyz[...,1])\n\t    # q = torch.cat([w.unsqueeze(1), xyz], dim=1)\n\t    return rq\n", "# returns quaternion so that v_from rotated by this quaternion equals v_to\n\t# v_... are vectors of size (..., 3)\n\t# returns quaternion in w, x, y, z order, of size (..., 4)\n\t# note: such a rotation is not unique, there is an infinite number of solutions\n\t# this implementation returns the shortest arc\n\tdef from_to_quaternion(v_from, v_to):\n\t    v_from_unit = normalize_vector(v_from)\n\t    v_to_unit = normalize_vector(v_to)\n\t    w = (v_from_unit * v_to_unit).sum(dim=-1)+1\n\t    '''can't cross if two directions are exactly inverse'''\n", "    xyz = torch.cross(v_from_unit, v_to_unit, dim=-1)\n\t    q = torch.cat([w.unsqueeze(-1), xyz], dim=-1)\n\t    return normalize_vector(q)\n\tdef quat_inv(quat):\n\t    return pytorch3d.transforms.quaternion_invert(quat)\n\tdef quat_mul(quat0,quat1):\n\t    return pytorch3d.transforms.quaternion_multiply(quat0,quat1)\n\tdef quat_mul_vec(quat,vec):\n\t    return pytorch3d.transforms.quaternion_apply(quat,vec)\n\tdef slerp(q0, q1, t):\n", "    \"\"\"\n\t    Spherical Linear Interpolation of quaternions\n\t    https://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/slerp/index.htm\n\t    :param q0: Start quats (w, x, y, z) : shape = (B, J, 4)\n\t    :param q1: End quats (w, x, y, z) : shape = (B, J, 4)\n\t    :param t:  Step (in [0, 1]) : shape = (B, J, 1)\n\t    :return: Interpolated quat (w, x, y, z) : shape = (B, J, 4)\n\t    \"\"\"\n\t  #  q0 = q0.unsqueeze(1)\n\t  #  q1 = q1.unsqueeze(1)\n", "    # Dot product\n\t    q = q0*q1\n\t    cos_half_theta = torch.sum(q, dim=-1, keepdim=True)\n\t   # t = t.view(1,-1,1,1)\n\t    # Make sure we take the shortest path :\n\t    q1_antipodal = -q1\n\t    q1 = torch.where(cos_half_theta < 0, q1_antipodal, q1)\n\t    cos_half_theta = torch.where(cos_half_theta < 0,-cos_half_theta,cos_half_theta)\n\t    half_theta = torch.acos(cos_half_theta)\n\t    # torch.sin must be safer here\n", "    sin_half_theta = torch.sqrt(1.0 - cos_half_theta * cos_half_theta)\n\t    ratio_a = torch.sin((1 - t) * half_theta) / sin_half_theta\n\t    ratio_b = torch.sin(t * half_theta) / sin_half_theta\n\t    qt = ratio_a * q0 + ratio_b * q1    \n\t    # If the angle was constant, prevent nans by picking the original quat:\n\t    qt = torch.where(torch.abs(cos_half_theta) >= 1.0-1e-8, q0, qt)\n\t    return qt\n"]}
{"filename": "src/geometry/__init__.py", "chunked_list": []}
{"filename": "src/geometry/vector.py", "chunked_list": ["import torch\n\timport numpy as np\n\t_EPS64 = torch.finfo(torch.float64).eps\n\t_4EPS64 = _EPS64 * 4.0\n\t_EPS32 = torch.finfo(torch.float32).eps\n\t_4EPS32 = _EPS32 * 4.0\n\t# batch*n\n\tdef find_secondary_axis(v):\n\t    v = normalize_vector(v)\n\t    refx = torch.tensor(np.array([[1, 0, 0]]), dtype=v.dtype, device=v.device).expand(v.shape)\n", "    refy =  torch.tensor(np.array([[0, 1, 0]]), dtype=v.dtype, device=v.device).expand(v.shape)\n\t    dot = lambda x,y:(x*y).sum(-1,keepdim=True)\n\t    refxv,refyv = dot(v,refx).abs(),dot(v,refy).abs()    # find proper axis\n\t    ref = torch.where(refxv>refyv,refy,refx)\n\t    pred = torch.cross(v,ref,dim=-1)\n\t    pred = normalize_vector(pred)\n\t    return pred\n\tdef normalize_vector(v, return_mag=False):\n\t    if(v.dtype==torch.float64):\n\t        eps = _EPS64\n", "    else:\n\t        eps = _EPS32\n\t    v_mag = torch.norm(v,dim=-1,keepdim=True)# torch.sqrt(v.pow(2).sum(-1,keepdim=True))#+eps\n\t    #v_mag = torch.max(v_mag, torch.autograd.Variable(torch.FloatTensor([eps]).type_as(v)))\n\t    v = v / v_mag\n\t    if return_mag:\n\t        return v, v_mag[:, 0]\n\t    else:\n\t        return v\n\tdef cross_product(u, v):\n", "    \"\"\"\n\t    Cross operation on batched vectors of shape (..., 3)\n\t    \"\"\"\n\t    i = u[..., 1] * v[..., 2] - u[..., 2] * v[..., 1]\n\t    j = u[..., 2] * v[..., 0] - u[..., 0] * v[..., 2]\n\t    k = u[..., 0] * v[..., 1] - u[..., 1] * v[..., 0]\n\t    out = torch.cat((i.unsqueeze(-1), j.unsqueeze(-1), k.unsqueeze(-1)), dim=-1)\n\t    return out\n"]}
{"filename": "src/geometry/forward_kinematics.py", "chunked_list": ["from typing import List\n\timport torch\n\tfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply\n\tdef forward_transform_hierarchy(local_transforms: torch.Tensor, level_transforms: List[List[int]],\n\t                                level_transform_parents: List[List[int]]):\n\t    # used to store world transforms\n\t    world_transforms = torch.zeros_like(local_transforms)\n\t    # initialize root transforms\n\t    world_transforms[:, level_transforms[0]] = local_transforms[:, level_transforms[0], :, :]\n\t    # then process all children transforms\n", "    for level in range(1, len(level_transforms)):\n\t        parent_bone_indices = level_transform_parents[level]\n\t        local_bone_indices = level_transforms[level]\n\t        parent_level_transforms = world_transforms[:, parent_bone_indices]\n\t        local_level_transforms = local_transforms[:, local_bone_indices]\n\t        global_matrix = torch.matmul(parent_level_transforms, local_level_transforms)\n\t        world_transforms[:, local_bone_indices] = global_matrix.type_as(world_transforms)\n\t    return world_transforms\n\t# Compute a transform matrix combining a rotation and translation (rotation is applied first)\n\t# translations: translation vectors (N, 3)\n", "# rotations: rotation matrices (N, 3, 3)\n\t# out: transform matrix (N, 4, 4)\n\tdef get_transform_matrix(translations: torch.Tensor, rotations: torch.Tensor):\n\t    batch_size = rotations.shape[0]\n\t    out = torch.zeros((batch_size, 4, 4)).type_as(rotations)\n\t    out[:, 0:3, 0:3] = rotations\n\t    out[:, 0:3, 3] = translations\n\t    out[:, 3, 3] = 1.0\n\t    return out\n\tdef forward_kinematics(local_rotations: torch.Tensor, local_offsets: torch.Tensor,\n", "                       level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n\t    batch_size = local_rotations.shape[0]\n\t    joints_nbr = local_rotations.shape[1]\n\t    # compute local transformation matrix for each joints\n\t    local_transforms = get_transform_matrix(translations=local_offsets.view(-1, 3), rotations=local_rotations.view(-1, 3, 3))\n\t    local_transforms = local_transforms.view(batch_size, joints_nbr, 4, 4)\n\t    return forward_transform_hierarchy(local_transforms, level_joints, level_joint_parents)\n\tdef forward_kinematics_quats(local_rotations: torch.Tensor, local_offsets: torch.Tensor, hip_positions:torch.Tensor,\n\t                       parents):\n\t    \"\"\"\n", "    Performs FK to retrieve global positions and rotations in quaternion format\n\t    :param local_rotations: tensor of local rotations of shape (..., J, 4)\n\t    :param local_offsets: tensor of local bone offsets of shape (..., J, 3)\n\t    :param level_joints:  a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n\t                           each elements of the list is a list transform indexes\n\t                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n\t                           below the root joints the indices should match these of the provided local rotations\n\t                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n\t                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n\t                           (ie there are 4 joints above it in the hierarchy)\n", "    :param level_joint_parents: similar to level_transforms but contains the parent indices\n\t                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n\t                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n\t    :return:\n\t        global_positions : tensor of global bone positions of shape (..., J, 3)\n\t        global_quats     : tensor of global bone quaternions of shape (..., J, 4)\n\t    \"\"\"\n\t    quat_shape = list(local_rotations.shape)\n\t    pos_shape = quat_shape.copy()\n\t    pos_shape[-1] = 3\n", "    global_quats = local_rotations.clone()\n\t    local_offsets = local_offsets.unsqueeze(-3).expand(pos_shape)\n\t    global_positions = [hip_positions]\n\t    global_quats = [global_quats[...,:1,:]]\n\t    #global_positions[...,0:1,:] = hip_positions.clone()\n\t    for level in range(1, len(parents)):\n\t        global_positions.append( quaternion_apply(global_quats[parents[level]], local_offsets[..., level:level+1, :]) + global_positions[parents[level]])\n\t        global_quats.append( quaternion_multiply(global_quats[parents[level]], local_rotations[..., level:level+1, :]))\n\t    global_positions = torch.cat(global_positions,dim=-2)\n\t    global_quats = torch.cat(global_quats,dim=-2)\n", "    return global_positions, global_quats\n"]}
