{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(name='ugle', version='1.0', packages=find_packages())\n"]}
{"filename": "main.py", "chunked_list": ["import ugle\n\timport ugle.utils as utils\n\tfrom ugle.logger import log\n\tfrom ugle.trainer import MyLibrarySniffingClass\n\tfrom omegaconf import OmegaConf, DictConfig, open_dict\n\timport argparse\n\timport psutil\n\timport time\n\tfrom os.path import isfile\n\timport pickle\n", "def neural_run(override_model: str = None,\n\t               override_dataset: str = None,\n\t               override_cfg: DictConfig = None) -> dict:\n\t    \"\"\"\n\t    runs a GNN neural experiment\n\t    :param override_model: name of model to override default config\n\t    :param override_dataset: name of the dataset to override default config\n\t    :param override_cfg: override config options\n\t    :return results: results from the study\n\t    \"\"\"\n", "    # load model config\n\t    cfg = utils.load_model_config(override_model=override_model, override_cfg=override_cfg)\n\t    if override_dataset:\n\t        cfg.dataset = override_dataset\n\t    if cfg.trainer.load_existing_test and 'default' not in override_model:\n\t        # try load the pickle file from previous study \n\t        hpo_path = f\"{cfg.trainer.load_hps_path}{cfg.dataset}_{cfg.model}.pkl\"\n\t        if isfile(hpo_path):\n\t            log.info(f'loading hpo args: {hpo_path}')\n\t            previously_found = pickle.load(open(hpo_path, \"rb\"))\n", "            cfg.previous_results = previously_found.results\n\t        # if this doesn't exist then just use the default parameters\n\t        else: \n\t            log.info(f'loading default args')\n\t            found_args = OmegaConf.load(f'ugle/configs/models/{cfg.model}/{cfg.model}_default.yaml')\n\t            with open_dict(cfg):\n\t                cfg.args = OmegaConf.merge(cfg.args, found_args)\n\t        cfg.trainer.only_testing = True\n\t    # create trainer object defined in models and init with config\n\t    Trainer = getattr(getattr(ugle.models, cfg.model), f\"{cfg.model}_trainer\")(cfg)\n", "    start_time = time.time()\n\t    # memory profiling max memory requires other class\n\t    if 'memory' in Trainer.cfg.trainer.test_metrics:\n\t        # train model\n\t        start_mem = psutil.virtual_memory().active\n\t        mythread = MyLibrarySniffingClass(Trainer.eval)\n\t        mythread.start()\n\t        delta_mem = 0\n\t        max_memory = 0\n\t        memory_usage_refresh = .001  # Seconds\n", "        while True:\n\t            time.sleep(memory_usage_refresh)\n\t            delta_mem = psutil.virtual_memory().active - start_mem\n\t            if delta_mem > max_memory:\n\t                max_memory = delta_mem\n\t                max_percent = psutil.virtual_memory().percent\n\t            # Check to see if the library call is complete\n\t            if mythread.isShutdown():\n\t                break\n\t        max_memory /= 1024.0 ** 2\n", "        log.info(f\"MAX Memory Usage in MB: {max_memory:.2f}\")\n\t        log.info(f\"Max useage %: {max_percent}\")\n\t        results = mythread.results\n\t        results['memory'] = max_memory\n\t    else:\n\t        # train and evaluate model\n\t        results = Trainer.eval()\n\t    log.info(f\"Total Time for {cfg.model} {cfg.dataset}: {round(time.time() - start_time, 3)}s\")\n\t    return results\n\tif __name__ == \"__main__\":\n", "    parser = argparse.ArgumentParser(description='which model to run')\n\t    parser.add_argument('--model', type=str, default='daegc',\n\t                        help='the name of the model to run')\n\t    parser.add_argument('--dataset', type=str, default='cora',\n\t                        help='the name of the dataset to run model on')\n\t    parser.add_argument('--seed', type=str, default=42,\n\t                        help='the number random seed to train on')\n\t    parser.add_argument('--gpu', type=str, default=\"0\",\n\t                        help='the gpu to train on')\n\t    parser.add_argument('--load_existing_test', action='store_true',\n", "                        help='load best parameters available')\n\t    parsed = parser.parse_args()\n\t    study_cfg = OmegaConf.create({\"args\": {\"random_seed\": int(parsed.seed)},\n\t                                  \"trainer\": {\"gpu\": int(parsed.gpu), \n\t                                              \"load_existing_test\": parsed.load_existing_test}})\n\t    if ugle.utils.is_neural(parsed.model):\n\t        results = neural_run(override_model=parsed.model,\n\t                             override_dataset=parsed.dataset,\n\t                             override_cfg=study_cfg)"]}
{"filename": "model_evaluations.py", "chunked_list": ["from main import neural_run\n\tfrom omegaconf import OmegaConf, DictConfig\n\timport ugle\n\timport argparse\n\tfrom ugle.logger import log\n\timport pickle\n\tfrom os.path import exists\n\tfrom os import makedirs\n\tfrom copy import deepcopy\n\tdef run_study(study_override_cfg: DictConfig, algorithm: str, dataset: str, seeds: list):\n", "    \"\"\"\n\t    runs a study of one algorithm on one dataset over multiple seeds\n\t    :param study_override_cfg: study configuration object\n\t    :param algorithm: string of algorithm to test on\n\t    :param dataset: string of the dataset to test on\n\t    :param seeds: list of seeds to test over\n\t    :return average_results: the results for each metric averaged over the seeds\n\t    \"\"\"\n\t    study_cfg = study_override_cfg.copy()\n\t    average_results = ugle.utils.create_study_tracker(len(seeds), study_cfg.trainer.test_metrics)\n", "    study_results = OmegaConf.create({'dataset': dataset,\n\t                                      'model': algorithm,\n\t                                      'average_results': {},\n\t                                      'results': []})\n\t    # repeat training over all seeds\n\t    for idx, seed in enumerate(seeds):\n\t        study_cfg.args.random_seed = seed\n\t        log.info(f'Study -- {algorithm}:{dataset}:Seed({seed})')\n\t        # test results stores the results of one algorithm run\n\t        if ugle.utils.is_neural(algorithm):\n", "            results = neural_run(override_model=algorithm,\n\t                                 override_dataset=dataset,\n\t                                 override_cfg=study_cfg)\n\t        # save study output\n\t        study_results.results.append({'seed': seed, 'study_output': results})\n\t        average_results = ugle.utils.collate_study_results(average_results, results, idx)\n\t        # use first seed hyperparameters and train/test on remaining\n\t        if idx == 0:\n\t            study_cfg.previous_results = results\n\t    # average results stores the average calculation of statistics\n", "    average_results = ugle.utils.calc_average_results(average_results)\n\t    study_results.average_results = average_results\n\t    # save the result of the study\n\t    if not exists(study_cfg.trainer.results_path):\n\t        makedirs(study_cfg.trainer.results_path)\n\t    save_path = f\"{study_cfg.trainer.results_path}{dataset}_{algorithm}\"\n\t    pickle.dump(study_results, open(f\"{save_path}.pkl\", \"wb\"))\n\t    # after all results have been collected, get best result seed, retrain and save\n\t    if study_cfg.trainer.retrain_on_each_dataset:\n\t        args, best_seed = ugle.utils.get_best_args(study_results.results, study_cfg.trainer.model_resolution_metric)\n", "        study_cfg.args = args\n\t        study_cfg.args.random_seed = best_seed\n\t        study_cfg.trainer.only_testing = True\n\t        study_cfg.trainer.save_model = True\n\t        _ = neural_run(override_model=algorithm,\n\t                        override_dataset=dataset,\n\t                        override_cfg=study_cfg)\n\t    return average_results\n\tdef run_experiment(exp_cfg_name: str):\n\t    \"\"\"\n", "    run experiments which consists of multiple models and datasets\n\t    :param exp_cfg_name: location of the yaml file containing experiment configuration\n\t    \"\"\"\n\t    # load experiment config\n\t    log.info(f'loading experiment: {exp_cfg_name}')\n\t    exp_cfg = OmegaConf.load('ugle/configs/experiments/exp_cfg_template.yaml')\n\t    exp_cfg = ugle.utils.merge_yaml(exp_cfg, exp_cfg_name)\n\t    # iterate\n\t    if exp_cfg.special_training.split_addition_percentage:\n\t        log.info('Special Experiment: Removes percentage from whole dataset')\n", "        saved_cfg = deepcopy(exp_cfg)\n\t        special_runs = deepcopy(exp_cfg.study_override_cfg.trainer.split_addition_percentage)\n\t    elif exp_cfg.study_override_cfg.trainer.retrain_on_each_dataset: \n\t        log.info('Special Experiment: Finetuning on each new dataset given')\n\t        special_runs = [-1]\n\t        iterations_before_fine_tuning = len(exp_cfg.algorithms) - 1\n\t        if not exists(exp_cfg.study_override_cfg.trainer.models_path):\n\t            makedirs(exp_cfg.study_override_cfg.trainer.models_path)\n\t    elif exp_cfg.study_override_cfg.trainer.same_init_hpo:\n\t        log.info('Special Experiment: Same initilisation of Parameters')\n", "        special_runs = [-1]\n\t        if not exists(exp_cfg.study_override_cfg.trainer.models_path):\n\t            makedirs(exp_cfg.study_override_cfg.trainer.models_path)\n\t    else:\n\t        special_runs = [-1]\n\t    save_path = exp_cfg.study_override_cfg.trainer.results_path\n\t    for special_var in special_runs:\n\t        if special_var != -1:\n\t            log.info(f'Experiment: {special_var}% of the entire dataset')\n\t            exp_cfg = deepcopy(saved_cfg)\n", "            exp_cfg.study_override_cfg.trainer.split_addition_percentage = special_var\n\t            exp_cfg.study_override_cfg.trainer.results_path += str(special_var).replace('.', '') + '/'\n\t            if not exists(exp_cfg.study_override_cfg.trainer.results_path):\n\t                makedirs(exp_cfg.study_override_cfg.trainer.results_path)\n\t        # creating experiment iterator\n\t        experiment_tracker = ugle.utils.create_experiment_tracker(exp_cfg)\n\t        experiments_cpu = []\n\t        for exp_num, experiment in enumerate(experiment_tracker):\n\t            log.debug(f'starting new experiment ... ...')\n\t            log.debug(f'testing dataset: {experiment.dataset}')\n", "            log.debug(f'testing algorithm: {experiment.algorithm}')\n\t            if exp_cfg.study_override_cfg.trainer.retrain_on_each_dataset:\n\t                if exp_num > iterations_before_fine_tuning:\n\t                    exp_cfg.study_override_cfg.trainer.finetuning_new_dataset = True\n\t                    exp_cfg.study_override_cfg.trainer.only_testing = False\n\t                    exp_cfg.study_override_cfg.trainer.save_model = False\n\t            try:\n\t                # run experiment\n\t                experiment_results = run_study(exp_cfg.study_override_cfg,\n\t                                                experiment.algorithm,\n", "                                                experiment.dataset,\n\t                                                exp_cfg.seeds)\n\t                # save result in experiment tracker\n\t                experiment.results = experiment_results\n\t                ugle.utils.save_experiment_tracker(experiment_tracker, save_path)\n\t            # if breaks then tests on cpu\n\t            except Exception as e:\n\t                log.exception(str(e))\n\t                log.info(f\"adding to cpu fallback test\")\n\t                experiments_cpu.append(experiment)\n", "        # run all experiments that didn't work on gpu\n\t        if experiments_cpu and exp_cfg.run_cpu_fallback:\n\t            log.info(f'launching cpu fallback experiments')\n\t            exp_cfg.study_override_cfg.trainer.gpu = -1\n\t            for experiment in experiments_cpu:\n\t                log.debug(f'starting new experiment ...')\n\t                log.debug(f'testing dataset: {experiment.dataset}')\n\t                log.debug(f'testing algorithm: {experiment.algorithm}')\n\t                # run experiment\n\t                experiment_results = run_study(exp_cfg.study_override_cfg,\n", "                                                experiment.algorithm,\n\t                                                experiment.dataset,\n\t                                                exp_cfg.seeds)\n\t                # save result in experiment tracker\n\t                experiment.results = experiment_results\n\t                ugle.utils.save_experiment_tracker(experiment_tracker, save_path)\n\t        else:\n\t            if experiments_cpu:\n\t                log.info('The following combinations lead to OOM')\n\t                for experiment in experiments_cpu:\n", "                    log.info(f'{experiment.dataset} : {experiment.algorithm}')\n\t        ugle.utils.display_evaluation_results(experiment_tracker)\n\t    return\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description='parsing the experiment to run')\n\t    parser.add_argument('-ec', '--experiment_config', type=str, required=True,\n\t                        help='the location of the experiment config')\n\t    parsed = parser.parse_args()\n\t    run_experiment(parsed.experiment_config)\n"]}
{"filename": "transfer_results.py", "chunked_list": ["import subprocess\n\timport os\n\tugle_path = os.path.dirname(os.path.realpath(__file__))\n\tdef search_results(folder, filename):\n\t    for root, dirs, files in os.walk(f'{ugle_path}/{folder}'):\n\t        if filename in files:\n\t            return os.path.join(root, filename)\n\t    return None\n\tdef attempt_file_transfer(filename, folder):\n\t    command = f\"scp -r blue_server:~/ugle/{folder}{filename} {folder}\"\n", "    res = subprocess.call(command, shell=True)\n\t    if res != 1:\n\t        print(f'File Transferred: {filename}')\n\t    else:\n\t        print(f'No Result: {filename}')\n\t    return\n\tdef update_progress_results(datasets, algorithms, folder):\n\t    for dataset in datasets:\n\t        for algo in algorithms:\n\t            filename = f\"{dataset}_{algo}.pkl\"\n", "            result = search_results(folder, filename)\n\t            if result:\n\t                print(f'Found: {filename}')\n\t            else:\n\t                attempt_file_transfer(filename, folder)\n\t    return\n\talgorithms = ['daegc', 'dgi', 'dmon', 'grace', 'mvgrl', 'selfgnn', 'sublime', 'bgrl', 'vgaer', 'cagc']\n\tdatasets = ['cora', 'citeseer', 'dblp', 'bat', 'eat', 'texas', 'wisc', 'cornell', 'uat', 'amac', 'amap']\n\tfolder = './updated_results/'\n\tupdate_progress_results(datasets, algorithms, folder)\n", "default_algos = ['daegc_default', 'dgi_default', 'dmon_default', 'grace_default', 'mvgrl_default', 'selfgnn_default',\n\t                 'sublime_default', 'bgrl_default', 'vgaer_default', 'cagc_default']\n\tdefault_folder = './new_default/'\n\tupdate_progress_results(datasets, default_algos, default_folder)"]}
{"filename": "tests/testing_env.py", "chunked_list": ["import ugle\n\tfrom model_evaluations import run_experiment\n\tfrom main import neural_run\n\tdef test_loading_real_data():\n\t    features, label, training_adj, testing_adj = ugle.datasets.load_real_graph_data('cora', 0.5, False)\n\t    features, label, training_adj, testing_adj = ugle.datasets.load_real_graph_data('facebook', 0.5, False)\n\t    return\n\tdef test_neural():\n\t    neural_run(override_model='grace_default')\n\t    neural_run(override_model='dmon_default')\n", "    neural_run(override_model='daegc_default')\n\t    neural_run(override_model='mvgrl_default')\n\t    neural_run(override_model='bgrl_default')\n\t    neural_run(override_model='selfgnn_default')\n\t    neural_run(override_model='sublime_default')\n\t    neural_run(override_model='vgaer_default')\n\tdef test_pipeline():\n\t    # test that pipeline fallsback to cpu correctly\n\t    run_experiment('ugle/configs/testing/min_cpu_fallback.yaml')\n\t    # test that pipeline works\n", "    run_experiment('ugle/configs/testing/min_working_pipeline_config.yaml')\n\tdef test_multi_objective():\n\t    run_experiment('ugle/configs/testing/min_multi_objective.yaml')\n\t    run_experiment('ugle/configs/testing/min_multi_hpo_non_neural.yaml')"]}
{"filename": "ugle/gnn_architecture.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tclass mvgrl_Discriminator(nn.Module):\n\t    def __init__(self, n_h):\n\t        super(mvgrl_Discriminator, self).__init__()\n\t        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\t        for m in self.modules():\n\t            self.weights_init(m)\n\t    def weights_init(self, m):\n", "        if isinstance(m, nn.Bilinear):\n\t            torch.nn.init.xavier_uniform_(m.weight.data)\n\t            if m.bias is not None:\n\t                m.bias.data.fill_(0.0)\n\t    def forward(self, c1, c2, h1, h2, h3, h4, s_bias1=None, s_bias2=None):\n\t        c_x1 = torch.unsqueeze(c1, 1)\n\t        c_x1 = c_x1.expand_as(h1).contiguous()\n\t        c_x2 = torch.unsqueeze(c2, 1)\n\t        c_x2 = c_x2.expand_as(h2).contiguous()\n\t        # positive\n", "        sc_1 = torch.squeeze(self.f_k(h2, c_x1), 2)\n\t        sc_2 = torch.squeeze(self.f_k(h1, c_x2), 2)\n\t        # negetive\n\t        sc_3 = torch.squeeze(self.f_k(h4, c_x1), 2)\n\t        sc_4 = torch.squeeze(self.f_k(h3, c_x2), 2)\n\t        logits = torch.cat((sc_1, sc_2, sc_3, sc_4), 1)\n\t        return logits\n\tclass Discriminator(nn.Module):\n\t    def __init__(self, n_h):\n\t        super(Discriminator, self).__init__()\n", "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\t        for m in self.modules():\n\t            self.weights_init(m)\n\t    def weights_init(self, m):\n\t        if isinstance(m, nn.Bilinear):\n\t            torch.nn.init.xavier_uniform_(m.weight.data)\n\t            if m.bias is not None:\n\t                m.bias.data.fill_(0.0)\n\t    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n\t        c_x = torch.unsqueeze(c, 1)\n", "        c_x = c_x.expand_as(h_pl)\n\t        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n\t        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n\t        if s_bias1 is not None:\n\t            sc_1 += s_bias1\n\t        if s_bias2 is not None:\n\t            sc_2 += s_bias2\n\t        logits = torch.cat((sc_1, sc_2), 1)\n\t        return logits\n\t# Applies an average on seq, of shape (batch, nodes, features)\n", "# While taking into account the masking of msk\n\tclass AvgReadout(nn.Module):\n\t    def __init__(self):\n\t        super(AvgReadout, self).__init__()\n\t    def forward(self, seq, msk):\n\t        if msk is None:\n\t            return torch.mean(seq, 1)\n\t        else:\n\t            msk = torch.unsqueeze(msk, -1)\n\t            return torch.sum(seq * msk, 1) / torch.sum(msk)\n", "class GATLayer(nn.Module):\n\t    \"\"\"\n\t    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n\t    \"\"\"\n\t    def __init__(self, in_features, out_features, alpha=0.2):\n\t        super(GATLayer, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.alpha = alpha\n\t        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n", "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n\t        self.a_self = nn.Parameter(torch.zeros(size=(out_features, 1)))\n\t        nn.init.xavier_uniform_(self.a_self.data, gain=1.414)\n\t        self.a_neighs = nn.Parameter(torch.zeros(size=(out_features, 1)))\n\t        nn.init.xavier_uniform_(self.a_neighs.data, gain=1.414)\n\t        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\t    def forward(self, input, adj, M, concat=True):\n\t        h = torch.mm(input, self.W)\n\t        attn_for_self = torch.mm(h, self.a_self)  # (N,1)\n\t        attn_for_neighs = torch.mm(h, self.a_neighs)  # (N,1)\n", "        attn_dense = attn_for_self + torch.transpose(attn_for_neighs, 0, 1)\n\t        attn_dense = torch.mul(attn_dense, M)\n\t        attn_dense = self.leakyrelu(attn_dense)  # (N,N)\n\t        zero_vec = -9e15 * torch.ones_like(adj)\n\t        adj = torch.where(adj > 0, attn_dense, zero_vec)\n\t        attention = F.softmax(adj, dim=1)\n\t        h_prime = torch.matmul(attention, h)\n\t        if concat:\n\t            return F.elu(h_prime)\n\t        else:\n", "            return h_prime\n\t    def __repr__(self):\n\t        return (\n\t            self.__class__.__name__\n\t            + \" (\"\n\t            + str(self.in_features)\n\t            + \" -> \"\n\t            + str(self.out_features)\n\t            + \")\"\n\t        )\n", "class GAT(nn.Module):\n\t    def __init__(self, num_features, hidden_size, embedding_size, alpha):\n\t        super(GAT, self).__init__()\n\t        self.hidden_size = hidden_size\n\t        self.embedding_size = embedding_size\n\t        self.alpha = alpha\n\t        self.conv1 = GATLayer(num_features, hidden_size, alpha)\n\t        self.conv2 = GATLayer(hidden_size, embedding_size, alpha)\n\t    def forward(self, x, adj, M):\n\t        h = self.conv1(x, adj, M)\n", "        h = self.conv2(h, adj, M)\n\t        z = F.normalize(h, p=2, dim=1)\n\t        A_pred = self.dot_product_decode(z)\n\t        return A_pred, z\n\t    def dot_product_decode(self, Z):\n\t        A_pred = torch.sigmoid(torch.matmul(Z, Z.t()))\n\t        return A_pred\n\tclass GCN(nn.Module):\n\t    def __init__(self, in_ft, out_ft, act, bias=True, skip=False):\n\t        super(GCN, self).__init__()\n", "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n\t        if act == 'prelu':\n\t            self.act = nn.PReLU()\n\t        elif act == 'selu':\n\t            self.act = nn.SELU()\n\t        else:\n\t            self.act = act\n\t        if bias:\n\t            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n\t            torch.nn.init.normal_(self.bias, mean=0.0, std=0.1)\n", "        else:\n\t            self.register_parameter('bias', None)\n\t        if skip:\n\t            self.skip = nn.Parameter(torch.FloatTensor(out_ft))\n\t            torch.nn.init.normal_(self.skip, mean=0.0, std=0.1)\n\t        else:\n\t            self.register_parameter('skip', None)\n\t        for m in self.modules():\n\t            self.weights_init(m)\n\t    def weights_init(self, m):\n", "        if isinstance(m, nn.Linear):\n\t            torch.nn.init.xavier_uniform_(m.weight.data)\n\t            if m.bias is not None:\n\t                m.bias.data.fill_(0.0)\n\t    # Shape of seq: (batch, nodes, features)\n\t    def forward(self, seq, adj, sparse=False, skip=False):\n\t        seq_fts = self.fc(seq)\n\t        if skip:\n\t            skip_out = seq_fts * self.skip\n\t        if sparse:\n", "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n\t        else:\n\t            out = torch.bmm(adj, seq_fts)\n\t        if skip:\n\t            out += skip_out\n\t        if self.bias is not None:\n\t            out += self.bias\n\t        return self.act(out)"]}
{"filename": "ugle/process.py", "chunked_list": ["import numpy as np\n\timport scipy.sparse as sp\n\timport torch\n\tfrom scipy.linalg import fractional_matrix_power, inv\n\tfrom sklearn.metrics import f1_score, normalized_mutual_info_score\n\tfrom scipy.optimize import linear_sum_assignment\n\timport math\n\timport warnings\n\tSMOOTH_K_TOLERANCE = 1e-5\n\tMIN_K_DIST_SCALE = 1e-3\n", "NPY_INFINITY = np.inf\n\tdef compute_ppr(adj: np.ndarray, alpha: int = 0.2, self_loop: bool = True):\n\t    \"\"\"\n\t    computes ppr diffusion\n\t    \"\"\"\n\t    if self_loop:\n\t        adj = adj + np.eye(adj.shape[0])  # A^ = A + I_n\n\t    d = np.diag(np.sum(adj, 1))  # D^ = Sigma A^_ii\n\t    dinv = fractional_matrix_power(d, -0.5)  # D^(-1/2)\n\t    at = np.matmul(np.matmul(dinv, adj), dinv)  # A~ = D^(-1/2) x A^ x D^(-1/2)\n", "    return alpha * inv((np.eye(adj.shape[0]) - (1 - alpha) * at))  # a(I_n-(1-a)A~)^-1\n\tdef modularity(adjacency: np.ndarray, preds: np.ndarray) -> float:\n\t    \"\"\"\n\t    computes modularity\n\t    \"\"\"\n\t    adjacency = sp.coo_matrix(adjacency).tocsr()\n\t    degrees = adjacency.sum(axis=0).A1\n\t    m = degrees.sum()\n\t    result = 0\n\t    for cluster_id in np.unique(preds):\n", "        cluster_indices = np.where(preds == cluster_id)[0]\n\t        adj_submatrix = adjacency[cluster_indices, :][:, cluster_indices]\n\t        degrees_submatrix = degrees[cluster_indices]\n\t        result += np.sum(adj_submatrix) - (np.sum(degrees_submatrix) ** 2) / m\n\t    return result / m\n\tdef conductance(adjacency: np.ndarray, preds: np.ndarray) -> float:\n\t    \"\"\"\n\t    computes conductance\n\t    \"\"\"\n\t    if len(np.unique(preds)) == 1:\n", "        return 1.\n\t    inter = 0\n\t    intra = 0\n\t    cluster_idx = np.zeros(adjacency.shape[0], dtype=bool)\n\t    for cluster_id in np.unique(preds):\n\t        cluster_idx[:] = 0\n\t        cluster_idx[np.where(preds == cluster_id)[0]] = 1\n\t        adj_submatrix = adjacency[cluster_idx, :]\n\t        inter += np.sum(adj_submatrix[:, cluster_idx])\n\t        intra += np.sum(adj_submatrix[:, ~cluster_idx])\n", "    return intra / (inter + intra)\n\tdef preds_eval(labels: np.ndarray, preds: np.ndarray, sf=4, adj: np.ndarray = None, metrics=['nmi', 'f1']) -> tuple[\n\t    dict, np.ndarray]:\n\t    \"\"\"\n\t    evaluates predictions given metrics\n\t    \"\"\"\n\t    # returns the correct predictions to match labels\n\t    eval_preds, _ = hungarian_algorithm(labels, preds)\n\t    results = {}\n\t    if 'nmi' in metrics:\n", "        nmi = normalized_mutual_info_score(labels, eval_preds)\n\t        results['nmi'] = float(round(nmi, sf))\n\t    if 'f1' in metrics:\n\t        f1 = f1_score(labels, eval_preds, average='macro')\n\t        results['f1'] = float(round(f1, sf))\n\t    if 'snmi' in metrics:\n\t        true_num_clusters = len(np.unique(labels))\n\t        found_num_clusters = len(np.unique(preds))\n\t        scaling_k = np.exp(-(np.abs(true_num_clusters - found_num_clusters) / true_num_clusters))\n\t        snmi = scaling_k * nmi\n", "        results['snmi'] = float(round(snmi, sf))\n\t    if 'modularity' in metrics:\n\t        assert adj is not None, 'adj not provided'\n\t        results['modularity'] = float(round(modularity(adj, eval_preds), sf))\n\t    if 'conductance' in metrics:\n\t        assert adj is not None, 'adj not provided'\n\t        results['conductance'] = float(round(conductance(adj, eval_preds), sf))\n\t    if 'n_clusters' in metrics:\n\t        results['n_clusters'] = len(np.unique(preds))\n\t    return results, eval_preds\n", "def hungarian_algorithm(labels: np.ndarray, preds: np.ndarray, col_ind=None):\n\t    \"\"\"\n\t    hungarian algorithm for prediction reassignment\n\t    \"\"\"\n\t    labels = labels.astype(np.int64)\n\t    assert preds.size == labels.size\n\t    D = max(preds.max(), labels.max()) + 1\n\t    w = np.zeros((D, D), dtype=np.int64)\n\t    for i in range(preds.size):\n\t        w[preds[i], labels[i]] += 1\n", "    if col_ind is None:\n\t        row_ind, col_ind = linear_sum_assignment(w.max() - w)\n\t        preds = col_ind[preds]\n\t    else:\n\t        preds = [col_ind[int(i)] for i in preds]\n\t        preds = np.asarray(preds)\n\t    return preds, col_ind\n\tdef preprocess_features(features):\n\t    \"\"\"\n\t    Row-normalize feature matrix and convert to tuple representation\n", "    \"\"\"\n\t    rowsum = np.array(features.sum(1))\n\t    nonzero_indexes = np.argwhere(rowsum != 0).flatten()\n\t    r_inv = np.zeros_like(rowsum, dtype=float)\n\t    r_inv[nonzero_indexes] = np.power(rowsum[nonzero_indexes], -1)\n\t    r_inv[np.isinf(r_inv)] = 0.\n\t    r_inv[np.isnan(r_inv)] = 0.\n\t    r_mat_inv = sp.diags(r_inv)\n\t    features = r_mat_inv.dot(features)\n\t    if isinstance(features, np.ndarray):\n", "        return features\n\t    else:\n\t        return features.todense(), sparse_to_tuple(features)\n\tdef normalize_adj(adj):\n\t    \"\"\"\n\t    Symmetrically normalize adjacency matrix.\n\t    \"\"\"\n\t    adj = sp.coo_matrix(adj)\n\t    rowsum = np.array(adj.sum(1))\n\t    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n", "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n\t    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n\t    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\tdef sparse_to_tuple(sparse_mx, insert_batch=False):\n\t    \"\"\"\n\t    Convert sparse matrix to tuple representation.\n\t    Set insert_batch=True if you want to insert a batch dimension.\n\t    \"\"\"\n\t    def to_tuple(mx):\n\t        if not sp.isspmatrix_coo(mx):\n", "            mx = mx.tocoo()\n\t        if insert_batch:\n\t            coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n\t            values = mx.data\n\t            shape = (1,) + mx.shape\n\t        else:\n\t            coords = np.vstack((mx.row, mx.col)).transpose()\n\t            values = mx.data\n\t            shape = mx.shape\n\t        return coords, values, shape\n", "    if isinstance(sparse_mx, list):\n\t        for i in range(len(sparse_mx)):\n\t            sparse_mx[i] = to_tuple(sparse_mx[i])\n\t    else:\n\t        sparse_mx = to_tuple(sparse_mx)\n\t    return sparse_mx\n\tdef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n\t    \"\"\"\n\t    Convert a scipy sparse matrix to a torch sparse tensor.\n\t    \"\"\"\n", "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n\t    indices = torch.from_numpy(\n\t        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n\t    values = torch.from_numpy(sparse_mx.data)\n\t    shape = torch.Size(sparse_mx.shape)\n\t    return torch.sparse.FloatTensor(indices, values, shape)\n\tdef euclidean_distance(point1: np.ndarray, point2: np.ndarray):\n\t    # calculate the Euclidean distance\n\t    point_iterable = zip(point1, point2)\n\t    distance = math.sqrt(sum([(y - x) ** 2 for x, y in point_iterable]))\n", "    return distance"]}
{"filename": "ugle/logger.py", "chunked_list": ["import numpy as np\n\timport os\n\timport logging\n\treal_path = os.path.dirname(os.path.realpath(__file__))\n\tugle_path, _ = os.path.split(real_path)\n\tclass CustomFormatter(logging.Formatter):\n\t    COLOR_CODE = {\n\t        'DEBUG': '\\033[94m',    # Blue\n\t        'INFO': '\\033[92m',     # Green\n\t        'WARNING': '\\033[93m',  # Yellow\n", "        'ERROR': '\\033[91m',    # Red\n\t        'CRITICAL': '\\033[95m'  # Purple\n\t    }\n\t    RESET_CODE = '\\033[0m'  # Reset to default color\n\t    def format(self, record):\n\t        # Get the original formatted message\n\t        original_message = super().format(record)\n\t        # Shorten the file path\n\t        record.pathname = os.path.basename(record.pathname)\n\t        original_message = super().format(record)\n", "        # Apply color to the log level\n\t        log_level = record.levelname\n\t        color_code = self.COLOR_CODE.get(log_level, '')\n\t        colored_message = f\"{color_code}{original_message}{self.RESET_CODE}\"\n\t        return colored_message\n\tdef create_logger():\n\t    \"\"\"\n\t    :return log: the loging object\n\t    \"\"\"\n\t    # Create a file handler (logs will be stored in \"my_log_file.log\")\n", "    pokemon_names = np.load(open(ugle_path + '/data/pokemon_names.npy', 'rb'), allow_pickle=True)\n\t    pokemon_names = [element for element in pokemon_names if all(char.isalpha() for char in element.lower())]\n\t    log_path = f'{ugle_path}/logs/'\n\t    if not os.path.exists(log_path):\n\t        os.mkdir(log_path)\n\t    latest_file_index = len(os.listdir(log_path))\n\t    poke_name = np.random.choice(pokemon_names)\n\t    uid = f'{latest_file_index}-{poke_name}'\n\t    # Set up the logger\n\t    logger = logging.getLogger(uid)\n", "    logger.setLevel(logging.INFO)\n\t    # Create a stream handler (console output)\n\t    stream_handler = logging.StreamHandler()\n\t    stream_handler.setLevel(logging.INFO)\n\t    file_handler = logging.FileHandler(filename= log_path + uid)\n\t    file_handler.setLevel(logging.DEBUG)\n\t    # Set the custom formatters to the handlers\n\t    color_formatter = CustomFormatter(fmt=\"[%(levelname)s:%(name)s: %(asctime)s: %(module)s.%(funcName)s] %(message)s\", \n\t                                      datefmt=\"%d/%m--%I:%M:%S\")\n\t    stream_handler.setFormatter(color_formatter)\n", "    file_handler.setFormatter(color_formatter)\n\t    # Add the handlers to the logger\n\t    logger.addHandler(stream_handler)\n\t    logger.addHandler(file_handler)\n\t    return logger\n\tlog = create_logger()\n"]}
{"filename": "ugle/__init__.py", "chunked_list": ["import importlib\n\timport pkgutil\n\tdef import_submodules(package, recursive=True):\n\t    \"\"\" Import all submodules of a module, recursively, including subpackages\n\t    :param package: package (name or actual module)\n\t    :type package: str | module\n\t    :rtype: dict[str, types.ModuleType]\n\t    \"\"\"\n\t    if isinstance(package, str):\n\t        package = importlib.import_module(package)\n", "    results = {}\n\t    for loader, name, is_pkg in pkgutil.walk_packages(package.__path__):\n\t        full_name = package.__name__ + '.' + name\n\t        results[full_name] = importlib.import_module(full_name)\n\t        if recursive and is_pkg:\n\t            results.update(import_submodules(full_name))\n\t    return results\n\timport_submodules(__name__)\n"]}
{"filename": "ugle/utils.py", "chunked_list": ["from omegaconf import OmegaConf, open_dict, DictConfig\n\tfrom optuna import Study\n\tfrom typing import Tuple\n\timport random\n\timport torch\n\timport numpy as np\n\timport optuna\n\timport os\n\timport pickle\n\tfrom ugle.logger import log\n", "neural_algorithms = ['daegc', 'dgi', 'dmon', 'grace', 'mvgrl', 'selfgnn', 'sublime', 'bgrl', 'vgaer', 'cagc', 'igo']\n\tdef load_model_config(override_model: str = None, override_cfg: DictConfig = None) -> DictConfig:\n\t    \"\"\"\n\t    loads the base model config, model specific config, then override_cfg, then the umap processing\n\t    investigation config options if relevant\n\t    :param override_model: name of the model to load config for\n\t    :param override_cfg: config to override options on loaded config\n\t    :return config: config for running experiments\n\t    \"\"\"\n\t    config = OmegaConf.load('ugle/configs/config.yaml')\n", "    if override_model:\n\t        config.model = override_model\n\t    config_name = config.model\n\t    model_name = config.model\n\t    # model_path structure edit if testing\n\t    if config_name.__contains__('_'):\n\t        model_name, name_ext = config_name.split(\"_\")\n\t        config.model = model_name\n\t        model_path = f'ugle/configs/models/{model_name}/{config_name}.yaml'\n\t    else:\n", "        model_path = f'ugle/configs/models/{model_name}/{model_name}.yaml'\n\t    config = merge_yaml(config, model_path)\n\t    # model_eval override\n\t    if override_cfg:\n\t        with open_dict(config):\n\t            config = OmegaConf.merge(config, override_cfg)\n\t    return config\n\tdef process_study_cfg_parameters(config: DictConfig) -> DictConfig:\n\t    \"\"\"\n\t    creates optimisation directions for study\n", "    :param config: config object\n\t    :return config: process config\n\t    \"\"\"\n\t    opt_directions = []\n\t    if len(config.trainer.valid_metrics) > 1:\n\t        config.trainer.multi_objective_study = True\n\t    else:\n\t        config.trainer.multi_objective_study = False\n\t    for metric in config.trainer.valid_metrics:\n\t        if metric == 'nmi' or metric == 'f1' or metric == 'modularity':\n", "            opt_directions.append('maximize')\n\t        else:\n\t            opt_directions.append('minimize')\n\t    config.trainer.optimisation_directions = opt_directions\n\t    return config\n\tdef extract_best_trials_info(study: Study, valid_metrics: list) -> Tuple[list, list]:\n\t    \"\"\"\n\t    extracts the best trial from a study for each given metric and associated trial\n\t    :param study: the study object\n\t    :param valid_metrics: the validation metrics \n", "    :return best_values: the best values for each metric\n\t    :return associated_trail: the associated trial with each best value\n\t    \"\"\"\n\t    best_values = study.best_trials[0].values\n\t    associated_trial = [study.best_trials[0].number] * len(valid_metrics)\n\t    if len(study.best_trials) > 1:\n\t        for a_best_trial in study.best_trials[1:]:\n\t            for i, bval in enumerate(a_best_trial.values):\n\t                if (bval > best_values[i] and study.directions[i].name == 'MAXIMIZE') or (\n\t                        bval < best_values[i] and study.directions[i].name == 'MINIMIZE'):\n", "                    best_values[i] = bval\n\t                    associated_trial[i] = a_best_trial.number\n\t    return best_values, associated_trial\n\tdef merge_yaml(config: DictConfig, yaml_str: str) -> DictConfig:\n\t    \"\"\"\n\t    merges config object with config specified in yaml path str\n\t    :param config: config object to be merged\n\t    :param yaml_str: path location of .yaml string to be merged\n\t    :return config: merged config\n\t    \"\"\"\n", "    yaml_dict = OmegaConf.load(yaml_str)\n\t    with open_dict(config):\n\t        config = OmegaConf.merge(config, yaml_dict)\n\t    return config\n\tdef set_random(random_seed: int):\n\t    \"\"\"\n\t    sets the random seed\n\t    \"\"\"\n\t    random.seed(random_seed)\n\t    np.random.seed(random_seed)\n", "    torch.manual_seed(random_seed)\n\t    torch.cuda.manual_seed_all(random_seed)\n\t    return\n\tdef set_device(gpu: int):\n\t    \"\"\"\n\t    returns the correct device\n\t    \"\"\"\n\t    if gpu != -1 and torch.cuda.is_available():\n\t        device = f'cuda:{gpu}'\n\t    else:\n", "        device = 'cpu'\n\t    return device\n\tdef sample_hyperparameters(trial: optuna.trial.Trial, args: DictConfig, prune_params=None) -> DictConfig:\n\t    \"\"\"\n\t    iterates through the args configuration, if an item is a list then suggests a value based on\n\t    the optuna trial instance\n\t    :param trial: instance of trial for suggestion parameters\n\t    :param args: config dictionary where list\n\t    :return: new config with values replaced where list is given\n\t    \"\"\"\n", "    vars_to_set = []\n\t    vals_to_set = []\n\t    for k, v in args.items_ex(resolve=False):\n\t        if not OmegaConf.is_list(v):\n\t            continue\n\t        else:\n\t            trial_sample = trial.suggest_categorical(k, OmegaConf.to_object(v))\n\t            setattr(args, k, trial_sample)\n\t            vars_to_set.append(k)\n\t            vals_to_set.append(trial_sample)\n", "    if prune_params:\n\t        repeated = prune_params.check_params()\n\t    for var, val in zip(vars_to_set, vals_to_set):\n\t        log.info(f\"args.{var}={val}\")\n\t    return args\n\tdef assign_test_params(config: DictConfig, best_params: dict) -> DictConfig:\n\t    \"\"\"\n\t    assigns the best params from the hyperparameter selection and assigns test config settings\n\t    :param config: original config for training\n\t    :param best_params: the best hyperparameters from training\n", "    :return: config for training then testing\n\t    \"\"\"\n\t    cfg = config.copy()\n\t    # assigns the best parameters from hp to config\n\t    for key, value in cfg.args.items_ex(resolve=False):\n\t        if not OmegaConf.is_list(value):\n\t            continue\n\t        else:\n\t            cfg.args[key] = best_params[key]\n\t    # assigns parameters for testing\n", "    cfg.trainer.train_to_valid_split = 1.0\n\t    cfg.trainer.n_valid_splits = 1\n\t    cfg.trainer.currently_testing = True\n\t    return cfg\n\tdef is_neural(model_name: str) -> bool:\n\t    \"\"\"\n\t    checks if an algorithm is neural or non_neural\n\t    :param model_name: the model name string\n\t    :return True if neural, False if not\n\t    \"\"\"\n", "    if model_name.__contains__('_'):\n\t        adjusted_model_name, _ = model_name.split(\"_\")\n\t    else:\n\t        adjusted_model_name = 'this is definitely not a name of an algorithm'\n\t    if model_name in neural_algorithms or adjusted_model_name in neural_algorithms:\n\t        return True\n\t    else:\n\t        raise ValueError(f\"Algorithm {model_name} is not implemented\")\n\tdef collate_study_results(average_results: dict, results: dict, val_idx: int) -> dict:\n\t    \"\"\"\n", "    converts study results from results\n\t    :param average_results: dict of metrics with numpy arrays of results across training runs\n\t    :param results: dictionary of results\n\t    :param val_idx: no. training run\n\t    :return:\n\t    \"\"\"\n\t    for trial in results:\n\t        for metric, value in trial[\"results\"].items():\n\t            average_results[metric][val_idx] = value\n\t    return average_results\n", "def create_study_tracker(k_splits: int, metrics: list) -> dict:\n\t    \"\"\"\n\t    creates a study tracker for multiple seeds\n\t    :param k_splits: the number of seeds\n\t    :param metrics: the metrics to create a tracker for\n\t    :return results: the results tracker\n\t    \"\"\"\n\t    results = {}\n\t    for metric in metrics:\n\t        results[metric] = np.zeros(k_splits)\n", "    return results\n\tdef create_experiment_tracker(exp_cfg: DictConfig) -> list:\n\t    \"\"\"\n\t    creates the experiment tracker to track results\n\t    :param exp_cfg: experiment config\n\t    :experiment_tracker: list of results objects that store results from individual experiment\n\t    \"\"\"\n\t    experiment_tracker = []\n\t    for dataset in exp_cfg.datasets:\n\t        for algorithm in exp_cfg.algorithms:\n", "            experiment_tracker.append(OmegaConf.create(\n\t                            {'dataset': dataset,\n\t                             'algorithm': algorithm,\n\t                             'seeds': exp_cfg.seeds,\n\t                             'results': {}\n\t                             }))\n\t    return experiment_tracker\n\tdef display_saved_results(results_path: str = \"./results\"):\n\t    \"\"\"\n\t    displays results saved in memory under given directory in latex form\n", "    :param results_path: path where results are located\n\t    \"\"\"\n\t    for subdir, dirs, files in os.walk(results_path):\n\t        for file in files:\n\t            log.info(f\"{file}\")\n\t            exp_result_path = os.path.join(subdir, file)\n\t            exp_result = pickle.load(open(exp_result_path, \"rb\"))\n\t            for experiment in exp_result:\n\t                if hasattr(experiment, 'algorithm'):\n\t                    log.info(f\"{experiment.dataset}\")\n", "                    try:\n\t                        display_latex_results(experiment.algorithm_identifier, experiment.results)\n\t                    except:\n\t                        print(experiment.results)\n\t    return\n\tdef display_evaluation_results(experiment_tracker: list):\n\t    \"\"\"\n\t    displays the evaluation results in table latex form\n\t    :param experiment_tracker: list of results from experiment\n\t    \"\"\"\n", "    for experiment in experiment_tracker:\n\t        try: \n\t            display_latex_results(experiment.algorithm, experiment.results, experiment.dataset)\n\t        except:\n\t            pass\n\t    return\n\tdef display_latex_results(method: str, exp_result: dict, dataset):\n\t    \"\"\"\n\t    prints out latex results of experiment\n\t    :param method: method string\n", "    :param exp_result: dictionary of results\n\t    \"\"\"\n\t    display_str = f'{method} {dataset}'\n\t    if not is_neural(method):\n\t        n_clusters = int(exp_result['n_clusters_mean'])\n\t        display_str += f' ({n_clusters}) '\n\t    if 'memory_max' in exp_result.keys():\n\t        memory = float(exp_result['memory_max'])\n\t        display_str += f' ({memory}) '\n\t    metric_string = extract_result_values(exp_result)\n", "    display_str += metric_string\n\t    print(display_str)\n\t    return\n\tdef extract_result_values(exp_result, metrics=['f1', 'nmi', 'modularity', 'conductance']) -> str:\n\t    \"\"\"\n\t    extracts the results into a strjing ready to print for latex display\n\t    if std values exist then they are also used\n\t    :param exp_result: dictionary of results\n\t    :param metrics: list of metrics in results\n\t    :return metric_string: string to print given results\n", "    \"\"\"\n\t    metric_string = ''\n\t    for metric in metrics:\n\t        metric_string += ' & '\n\t        metric_mean = f'{metric}_mean'\n\t        metric_val = exp_result[metric_mean]\n\t        metric_string += str(metric_val)\n\t        metric_std = f'{metric}_std'\n\t        if metric_std in exp_result.keys():\n\t            std = str(exp_result[metric_std])\n", "            metric_string += f'\\pm {std}'\n\t    metric_string += ' \\\\\\\\'\n\t    return metric_string\n\tdef save_experiment_tracker(result_tracker: list, results_path: str):\n\t    \"\"\"\n\t    pickle saves result tracker to results path\n\t    :param result_tracker: list of experiment results\n\t    :param results_path: the path to save results in\n\t    \"\"\"\n\t    if not os.path.exists(results_path):\n", "        os.makedirs(results_path)\n\t    experiment_identifier = log.name\n\t    log.info(f'saving to {results_path}{experiment_identifier}')\n\t    pickle.dump(result_tracker, open(f\"{results_path}{experiment_identifier}.pkl\", \"wb\"))\n\t    return\n\tdef calc_average_results(results_tracker: dict) -> dict:\n\t    \"\"\"\n\t    calculates the average of the performance statistic to an appropriate s.f.\n\t    :param results_tracker: contains results of over seeds\n\t    :return average_results: the average of results over all seeds\n", "    \"\"\"\n\t    average_results = {}\n\t    for stat, values in results_tracker.items():\n\t        if stat != 'memory':\n\t            average_results[f'{stat}_mean'] = float(np.format_float_positional(np.mean(values, axis=0),\n\t                                                                               precision=4,\n\t                                                                               unique=False,\n\t                                                                               fractional=False))\n\t            if len(values) > 1:\n\t                average_results[f'{stat}_std'] = float(np.format_float_positional(np.std(values, axis=0),\n", "                                                                                  precision=2,\n\t                                                                                  unique=False,\n\t                                                                                  fractional=False))\n\t        else:\n\t            average_results[f'memory_max'] = float(np.format_float_positional(np.max(values, axis=0),\n\t                                                                              precision=7,\n\t                                                                              unique=False,\n\t                                                                              fractional=False))\n\t    return average_results\n\tdef get_best_args(results, model_resolution_metric):\n", "    best = -1.\n\t    for seed_res in results: \n\t        for metric_res in seed_res['study_output']:\n\t            if model_resolution_metric in metric_res['metrics'] and best < metric_res['results'][model_resolution_metric]: \n\t                best = metric_res['results'][model_resolution_metric]\n\t                best_seed = seed_res['seed']\n\t                args = metric_res['args']\n\t    return args, best_seed\n"]}
{"filename": "ugle/datasets.py", "chunked_list": ["import os\n\tfrom omegaconf import OmegaConf, DictConfig\n\timport numpy as np\n\tfrom karateclub.dataset import GraphReader\n\timport networkx as nx\n\timport zipfile\n\timport gdown\n\tfrom pathlib import Path\n\timport shutil\n\timport torch\n", "import copy\n\timport random\n\timport scipy.sparse as sp\n\timport plotly.graph_objects as go\n\tfrom typing import Union\n\tfrom ugle.logger import log, ugle_path\n\tfrom typing import Tuple\n\tfrom torch_geometric.utils import to_dense_adj, stochastic_blockmodel_graph\n\timport torch\n\tgoogle_store_datasets = ['acm', 'amac', 'amap', 'bat', 'citeseer', 'cora', 'cocs', 'dblp', 'eat', 'uat', 'pubmed',\n", "                         'cite', 'corafull', 'texas', 'wisc', 'film', 'cornell']\n\tkarate_club_datasets = ['facebook', 'twitch', 'wikipedia', 'github', 'lastfm', 'deezer']\n\tall_datasets = (google_store_datasets + karate_club_datasets)\n\tdef check_data_presence(dataset_name: str) -> bool:\n\t    \"\"\"\n\t    checks for dataset presence in local memory\n\t    :param dataset_name: dataset name to check\n\t    \"\"\"\n\t    dataset_path = ugle_path + f'/data/{dataset_name}'\n\t    if not os.path.exists(dataset_path):\n", "        return False\n\t    elif not os.path.exists(f'{dataset_path}/{dataset_name}_feat.npy'):\n\t        return False\n\t    elif not os.path.exists(f'{dataset_path}/{dataset_name}_label.npy'):\n\t        return False\n\t    elif not os.path.exists(f'{dataset_path}/{dataset_name}_adj.npy'):\n\t        return False\n\t    else:\n\t        return True\n\tdef download_graph_data(dataset_name: str) -> bool:\n", "    \"\"\"\n\t    downloads a graph dataset\n\t    :param dataset_name: name of the dataset to download\n\t    :return True if successful\n\t    \"\"\"\n\t    log.info(f'downloading {dataset_name}')\n\t    download_link_path = ugle_path + '/data/download_links.yaml'\n\t    download_links = OmegaConf.load(download_link_path)\n\t    url = download_links[dataset_name]\n\t    dataset_path = ugle_path + f'/data/{dataset_name}'\n", "    if not os.path.exists(dataset_path):\n\t        os.mkdir(dataset_path)\n\t    dataset_zip_path = dataset_path + f'/{dataset_name}.zip'\n\t    gdown.download(url=url, output=dataset_zip_path, quiet=False, fuzzy=True)\n\t    log.info('finished downloading')\n\t    # extract the zip file\n\t    log.info('extracting dataset')\n\t    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n\t        zip_ref.printdir()\n\t        zip_ref.extractall(dataset_path)\n", "    log.info('extraction complete')\n\t    # correct the path dir\n\t    extended_path = f'{dataset_path}/{dataset_name}'\n\t    dataset_path += '/'\n\t    if os.path.exists(extended_path):\n\t        log.info('extraction to wrong location')\n\t        for subdir, dirs, files in os.walk(extended_path):\n\t            for file in files:\n\t                extract_path = os.path.join(subdir, file)\n\t                file = Path(file)\n", "                out_path = os.path.join(dataset_path, file)\n\t                log.info(f'Extracting {extract_path} to ... {out_path} ...')\n\t                shutil.move(Path(extract_path), Path(out_path))\n\t        shutil.rmtree(extended_path)\n\t    return True\n\tdef load_real_graph_data(dataset_name: str, test_split: float = 0.5, split_scheme: str = 'drop_edges',\n\t                         split_addition=None) -> Tuple[\n\t    np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\t    \"\"\"\n\t    loads the graph dataset and splits the adjacency matrix into two\n", "    :param dataset_name: name of the dataset\n\t    :param test_split: percentage of edges to keep\n\t    :return features, label, train_adj, adjacency: loaded graph data\n\t    \"\"\"\n\t    assert dataset_name in all_datasets, f\"{dataset_name} not a real dataset\"\n\t    if dataset_name in google_store_datasets:\n\t        if not check_data_presence(dataset_name):\n\t            extraction_success = download_graph_data(dataset_name)\n\t            assert extraction_success, f'download/extraction of dataset {dataset_name} failed'\n\t            assert check_data_presence(dataset_name)\n", "        dataset_path = ugle_path + f'/data/{dataset_name}/{dataset_name}'\n\t        features = np.load(dataset_path + \"_feat.npy\", allow_pickle=True)\n\t        label = np.load(dataset_path + \"_label.npy\", allow_pickle=True)\n\t        adjacency = np.load(dataset_path + \"_adj.npy\", allow_pickle=True)\n\t    elif dataset_name in karate_club_datasets:\n\t        loader = GraphReader(dataset_name)\n\t        features = loader.get_features().todense()\n\t        label = loader.get_target()\n\t        adjacency = nx.to_numpy_matrix(loader.get_graph())\n\t    if split_addition:\n", "        adjacency, _ = aug_drop_adj(adjacency, drop_percent=1-split_addition, split_adj=False)\n\t    log.debug('splitting dataset into training/testing')\n\t    train_adj, test_adj = split_adj(adjacency, test_split, split_scheme)\n\t    return features, label, train_adj, test_adj\n\tdef compute_datasets_info(dataset_names: list, visualise: bool=False):\n\t    \"\"\"\n\t    computes the information about dataset statistics\n\t    :param dataset_names: list of datasets to look at\n\t    \"\"\"\n\t    clustering_x_data = []\n", "    closeness_y_data = []\n\t    for dataset_name in dataset_names:\n\t        features, label, train_adjacency, test_adjacency = load_real_graph_data(dataset_name, test_split=1.)\n\t        display_string = dataset_name + ' & '  # name\n\t        display_string += str(train_adjacency.shape[0]) + ' & '  # n_nodes\n\t        display_string += str(features.shape[1]) + ' & '  # n_features\n\t        display_string += str(int(np.nonzero(train_adjacency)[0].shape[0]/2)) + ' & '  # n_edges\n\t        display_string += str(len(np.unique(label))) + ' & '  # n_classes\n\t        nx_g = nx.Graph(train_adjacency)\n\t        clustering = nx.average_clustering(nx_g)\n", "        cercania = nx.closeness_centrality(nx_g)\n\t        cercania = np.mean(list(cercania.values()))\n\t        clustering_x_data.append(clustering)\n\t        closeness_y_data.append(cercania)\n\t        display_string += str(round(clustering, 3)) + ' & '  # clustering coefficient\n\t        display_string += str(round(cercania, 3)) + ' \\\\\\\\'  # closeness centrality\n\t        if visualise:\n\t            print(display_string)\n\t    if visualise:\n\t        _ = display_figure_dataset_stats(clustering_x_data, closeness_y_data, dataset_names)\n", "    return clustering_x_data, closeness_y_data\n\tdef display_figure_dataset_stats(x_data: list, y_data: list, datasets: list):\n\t    \"\"\"\n\t    function to display dataset statistics on a graph\n\t    :param x_data: clustering coefficient data for x-axis\n\t    :param y_data: closeness centrality data for y-axis\n\t    :param datasets: list of datasets metrics were computed over\n\t    :return fig: figure to be displayed\n\t    \"\"\"\n\t    fig = go.Figure()\n", "    fig.add_trace(go.Scatter(\n\t        x=x_data, y=y_data, text=datasets, textposition=\"top center\",\n\t        color_discrete_sequence=['red']\n\t    ))\n\t    # layout options\n\t    layout = dict(\n\t                font=dict(\n\t                    size=18),\n\t              plot_bgcolor='white',\n\t              paper_bgcolor='white',\n", "              margin=dict(t=10, b=10, l=10, r=10, pad=0),\n\t              xaxis=dict(title='Average Clustering Coefficient',\n\t                         linecolor='black',\n\t                         showgrid=False,\n\t                         showticklabels=False,\n\t                         mirror=True),\n\t              yaxis=dict(title='Mean Closeness Centrality',\n\t                         linecolor='black',\n\t                         showgrid=False,\n\t                         showticklabels=False,\n", "                         mirror=True))\n\t    fig.update_layout(layout)\n\t    # save figure\n\t    if not os.path.exists(\"images\"):\n\t        os.mkdir(\"images\")\n\t    fig.write_image(\"images/dataset_stats.png\")\n\t    return fig\n\tdef aug_drop_features(input_feature: Union[np.ndarray, torch.Tensor], drop_percent: float = 0.2):\n\t    \"\"\"\n\t    augmentation by randomly masking features for every node\n", "    :param input_feature: feature matrix\n\t    :param drop_percent: percent that any feature is dropped\n\t    :return aug_feature: augmented feature matrix\n\t    \"\"\"\n\t    node_num = input_feature.shape[1]\n\t    mask_num = int(node_num * drop_percent)\n\t    node_idx = [i for i in range(node_num)]\n\t    mask_idx = random.sample(node_idx, mask_num)\n\t    aug_feature = copy.deepcopy(input_feature)\n\t    if type(aug_feature) == np.ndarray:\n", "        zeros = np.zeros_like(aug_feature[0][0])\n\t    else:\n\t        zeros = torch.zeros_like(aug_feature[0][0])\n\t    for j in mask_idx:\n\t        aug_feature[0][j] = zeros\n\t    return aug_feature\n\tdef aug_drop_adj(input_adj: np.ndarray, drop_percent: float = 0.2, split_adj: bool = False):\n\t    \"\"\"\n\t    augmentation by randomly dropping edges with given probability\n\t    :param input_adj: input adjacency matrix\n", "    :param drop_percent: percent that any edge is dropped\n\t    :return aug_adj: augmented adjacency matrix\n\t    \"\"\"\n\t    index_list = input_adj.nonzero()\n\t    row_idx = index_list[0].tolist()\n\t    col_idx = index_list[1].tolist()\n\t    index_list = []\n\t    for i in range(len(row_idx)):\n\t        index_list.append((row_idx[i], col_idx[i]))\n\t    edge_num = int(len(row_idx))\n", "    add_drop_num = int(edge_num * drop_percent)\n\t    aug_adj = copy.deepcopy(input_adj.tolist())\n\t    else_adj = np.zeros_like(input_adj)\n\t    edge_idx = list(np.arange(edge_num))\n\t    drop_idx = random.sample(edge_idx, add_drop_num)\n\t    n_drop = len(drop_idx)\n\t    log.debug(f'dropping {n_drop} edges from {edge_num}')\n\t    for i in drop_idx:\n\t        aug_adj[index_list[i][0]][index_list[i][1]] = 0\n\t        aug_adj[index_list[i][1]][index_list[i][0]] = 0\n", "        else_adj[index_list[i][0]][index_list[i][1]] = 1\n\t        else_adj[index_list[i][1]][index_list[i][0]] = 1\n\t    aug_adj = np.array(aug_adj)\n\t    if split_adj: \n\t        return aug_adj, else_adj\n\t    else:\n\t        return aug_adj, input_adj\n\tdef numpy_to_edge_index(adjacency: np.ndarray):\n\t    \"\"\"\n\t    converts adjacency in numpy array form to an array of active edges\n", "    :param adjacency: input adjacency matrix\n\t    :return adjacency: adjacency matrix update form\n\t    \"\"\"\n\t    adj_label = sp.coo_matrix(adjacency)\n\t    adj_label = adj_label.todok()\n\t    outwards = [i[0] for i in adj_label.keys()]\n\t    inwards = [i[1] for i in adj_label.keys()]\n\t    adjacency = np.array([outwards, inwards], dtype=np.int)\n\t    return adjacency\n\tclass Augmentations:\n", "    \"\"\"\n\t    A utility for graph data augmentation\n\t    \"\"\"\n\t    def __init__(self, method='gdc'):\n\t        methods = {\"split\", \"standardize\", \"katz\"}\n\t        assert method in methods\n\t        self.method = method\n\t    @staticmethod\n\t    def _split(features, permute=True):\n\t        \"\"\"\n", "        Data augmentation is build by spliting data along the feature dimension.\n\t        :param data: the data object to be augmented\n\t        :param permute: Whether to permute along the feature dimension\n\t        \"\"\"\n\t        perm = np.random.permutation(features.shape[1]) if permute else np.arange(features.shape[1])\n\t        features = features[:, perm]\n\t        size = features.shape[1] // 2\n\t        x1 = features[:, :size]\n\t        x2 = features[:, size:]\n\t        return x1, x2\n", "    @staticmethod\n\t    def _standardize(features):\n\t        \"\"\"\n\t        Applies a zscore node feature data augmentation.\n\t        :param data: The data to be augmented\n\t        :return: a new augmented instance of the input data\n\t        \"\"\"\n\t        mean, std = features.mean(), features.std()\n\t        new_data = (features - mean) / (std + 10e-7)\n\t        return new_data\n", "    @staticmethod\n\t    def _katz(features, adjacency, beta=0.1, threshold=0.0001):\n\t        \"\"\"\n\t        Applies a Katz-index graph topology augmentation\n\t        :param data: The data to be augmented\n\t        :return: a new augmented instance of the input data\n\t        \"\"\"\n\t        n_nodes = features.shape[0]\n\t        a_hat = adjacency + sp.eye(n_nodes)\n\t        d_hat = sp.diags(\n", "            np.array(1 / np.sqrt(a_hat.sum(axis=1))).reshape(n_nodes))\n\t        a_hat = d_hat @ a_hat @ d_hat\n\t        temp = sp.eye(n_nodes) - beta * a_hat\n\t        h_katz = (sp.linalg.inv(temp.tocsc()) * beta * a_hat).toarray()\n\t        mask = (h_katz < threshold)\n\t        h_katz[mask] = 0.\n\t        edge_index = np.array(h_katz.nonzero())\n\t        edge_attr = torch.tensor(h_katz[h_katz.nonzero()], dtype=torch.float32)\n\t        return edge_index\n\t    def __call__(self, features, adjacency):\n", "        \"\"\"\n\t        Applies different data augmentation techniques\n\t        \"\"\"\n\t        if self.method == \"katz\":\n\t            aug_adjacency = self._katz(features, adjacency)\n\t            aug_adjacency = np.array([aug_adjacency[0], aug_adjacency[1]], dtype=np.int)\n\t            adjacency = adjacency.todense()\n\t            adjacency = numpy_to_edge_index(adjacency)\n\t            aug_features = features.copy()\n\t        elif self.method == 'split':\n", "            features, aug_features = self._split(features)\n\t            adjacency = adjacency.todense()\n\t            adjacency = numpy_to_edge_index(adjacency)\n\t            aug_adjacency = adjacency.copy()\n\t        elif self.method == \"standardize\":\n\t            aug_features = self._standardize(features)\n\t            adjacency = adjacency.todense()\n\t            adjacency = numpy_to_edge_index(adjacency)\n\t            aug_adjacency = adjacency.copy()\n\t        return features, adjacency, aug_features, aug_adjacency\n", "    def __str__(self):\n\t        return self.method.title()\n\tdef split(n, k):\n\t    d, r = divmod(n, k)\n\t    return [d + 1] * r + [d] * (k - r)\n\tdef create_synth_graph(n_nodes: int, n_features: int , n_clusters: int, adj_type: str, feature_type: str = 'random'):\n\t    if adj_type == 'disjoint':\n\t        probs = (np.identity(n_clusters)).tolist()\n\t    elif adj_type == 'random':\n\t        probs = (np.ones((n_clusters, n_clusters))/n_clusters).tolist()\n", "    elif adj_type == 'complete':\n\t        probs = np.ones((n_clusters, n_clusters)).tolist()\n\t    cluster_sizes = split(n_nodes, n_clusters)\n\t    adj = to_dense_adj(stochastic_blockmodel_graph(cluster_sizes, probs)).squeeze(0).numpy()\n\t    if feature_type == 'random':\n\t        features = torch.normal(mean=0, std=1, size=(n_nodes, n_features)).numpy()\n\t        features = np.where(features > 0., 1, 0)\n\t    elif feature_type == 'complete': \n\t        features = np.ones((n_nodes, n_features))\n\t    elif feature_type == 'disjoint':\n", "        features = np.zeros((n_nodes, n_features))\n\t        feature_dims_fo_cluster = split(n_features, n_clusters)\n\t        start_feat = 0\n\t        end_feat = feature_dims_fo_cluster[0]\n\t        start_clus = 0\n\t        end_clus = cluster_sizes[0]\n\t        for i in range(len(feature_dims_fo_cluster)):\n\t            features[start_clus:end_clus, start_feat:end_feat] = np.ones_like(features[start_clus:end_clus, start_feat:end_feat])\n\t            if i == len(feature_dims_fo_cluster) - 1:\n\t                break\n", "            start_feat += feature_dims_fo_cluster[i]\n\t            end_feat += feature_dims_fo_cluster[i+1]\n\t            start_clus += cluster_sizes[i]\n\t            end_clus += cluster_sizes[i+1]\n\t    labels = []\n\t    for i in range(n_clusters):\n\t        labels.extend([i] * cluster_sizes[i])\n\t    labels = np.array(labels)\n\t    return adj, features.astype(float), labels\n\tdef split_adj(adj, percent, split_scheme):\n", "    if split_scheme == 'drop_edges':\n\t        # drops edges from dataset to form new adj \n\t        if percent != 1.:\n\t            train_adjacency, validation_adjacency = aug_drop_adj(adj, drop_percent=1 - percent, split_adj=False)\n\t        else:\n\t            train_adjacency = adj\n\t            validation_adjacency = copy.deepcopy(adj)\n\t    elif split_scheme == 'split_edges':\n\t        # splits the adj via the edges so that no edges in both \n\t        if percent != 1.:\n", "            train_adjacency, validation_adjacency = aug_drop_adj(adj, drop_percent=1 - percent, split_adj=True)\n\t        else:\n\t            train_adjacency = adj\n\t            validation_adjacency = copy.deepcopy(adj)\n\t    elif split_scheme == 'all_edges':\n\t        # makes the adj fully connected \n\t        train_adjacency = np.ones_like(adj)\n\t        validation_adjacency = np.ones_like(adj)\n\t    elif split_scheme == 'no_edges':\n\t        # makes the adj completely unconnected \n", "        train_adjacency = np.zeros_like(adj)\n\t        validation_adjacency = np.zeros_like(adj)\n\t    return train_adjacency, validation_adjacency"]}
{"filename": "ugle/helper.py", "chunked_list": ["from omegaconf import OmegaConf\n\tfrom ugle.process import euclidean_distance\n\timport pickle\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\tfrom scipy.stats import wasserstein_distance, gaussian_kde\n\timport os\n\timport matplotlib\n\tfrom ugle.logger import ugle_path\n\tfrom copy import deepcopy\n", "def search_results(folder, filename):\n\t    for root, dirs, files in os.walk(f'{ugle_path}/{folder}'):\n\t        if filename in files:\n\t            return os.path.join(root, filename)\n\t    return None\n\tdef get_all_results_from_storage(datasets: list, algorithms: list, folder: str, empty: str = 'minus_ten',\n\t                                 collect_all: bool = False):\n\t    if empty == 'minus_ten':\n\t        empty_result = OmegaConf.create(\n\t            {'conductance_mean': -10.0,\n", "             'conductance_std': -10.0,\n\t             'f1_mean': -10.0,\n\t             'f1_std': -10.0,\n\t             'modularity_mean': -10.0,\n\t             'modularity_std': -10.0,\n\t             'nmi_mean': -10.0,\n\t             'nmi_std': -10.0,\n\t             })\n\t    elif empty == 'zeros':\n\t        empty_result = OmegaConf.create(\n", "            {'conductance_mean': 0.0,\n\t             'conductance_std': 0.0,\n\t             'f1_mean': 0.0,\n\t             'f1_std': 0.0,\n\t             'modularity_mean': 0.0,\n\t             'modularity_std': 0.0,\n\t             'nmi_mean': 0.0,\n\t             'nmi_std': 0.0,\n\t             })\n\t    result_holder = OmegaConf.create({})\n", "    for dataset in datasets:\n\t        for algo in algorithms:\n\t            filename = f\"{dataset}_{algo}.pkl\"\n\t            file_found = search_results(folder, filename)\n\t            if file_found:\n\t                result = pickle.load(open(file_found, \"rb\"))\n\t                if collect_all:\n\t                    # parse seed\n\t                    return_results = np.zeros(shape=(4, 10))\n\t                    # go thru every seed\n", "                    for i, seed_result in enumerate(result.results):\n\t                        # go thru every best hps configuration\n\t                        for metric_result in seed_result.study_output:\n\t                            # go thru each best metric in configuration\n\t                            for metric in metric_result.metrics:\n\t                                # add result to correct place\n\t                                if metric == 'f1':\n\t                                    return_results[0, i] = metric_result.results[metric]\n\t                                elif metric == 'nmi':\n\t                                    return_results[1, i] = metric_result.results[metric]\n", "                                elif metric == 'modularity':\n\t                                    return_results[2, i] = metric_result.results[metric]\n\t                                elif metric == 'conductance':\n\t                                    return_results[3, i] = metric_result.results[metric]\n\t                    result_holder[f\"{dataset}_{algo}\"] = return_results.tolist()\n\t                else:\n\t                    result_holder[f\"{dataset}_{algo}\"] = result.average_results\n\t            else:\n\t                if collect_all:\n\t                    if empty == 'minus_ten':\n", "                        place_holder = np.ones(shape=(4, 10)) * -10\n\t                        result_holder[f\"{dataset}_{algo}\"] = place_holder.tolist()\n\t                    elif empty == 'zeros':\n\t                        result_holder[f\"{dataset}_{algo}\"] = np.zeros(shape=(4, 10))\n\t                else:\n\t                    result_holder[f\"{dataset}_{algo}\"] = empty_result\n\t    return result_holder\n\tdef get_values_from_results_holder(result_holder, dataset_name, metric_name, return_std=False):\n\t    metric_values = []\n\t    if return_std:\n", "        std_values = []\n\t    for k, v in result_holder.items():\n\t        if k.__contains__(dataset_name):\n\t            metric_values.append(v[f'{metric_name}_mean'])\n\t            if return_std:\n\t                std_values.append(v[f'{metric_name}_std'])\n\t    if return_std:\n\t        return metric_values, std_values\n\t    else:\n\t        return metric_values\n", "def make_test_performance_object(datasets, algorithms, metrics, seeds, folder):\n\t    # get results object\n\t    result_object = np.zeros(shape=(len(datasets), len(algorithms), len(metrics), len(seeds)))\n\t    try:\n\t        result_holder = get_all_results_from_storage(datasets, algorithms, folder, collect_all=True)\n\t    except:\n\t        return result_object\n\t    for d, dataset in enumerate(datasets):\n\t        for a, algo in enumerate(algorithms):\n\t            result_object[d, a] = result_holder[f\"{dataset}_{algo}\"]\n", "    return result_object\n\tdef calculate_abs_std(result_object, datasets, metrics):\n\t    # calculate deviation over each seed\n\t    std_object = np.zeros(shape=np.shape(result_object)[:-1])\n\t    for d, _ in enumerate(datasets):\n\t        for m, _ in enumerate(metrics):\n\t            std_object[d, :, m] = np.std(result_object[d, :, m, :] , axis=1)\n\t    return std_object\n\tdef calculate_ranking_performance(result_object, datasets, metrics, seeds, scale_metrics=False, calc_ave_first=False):\n\t    if calc_ave_first:\n", "        # calculate ranking on each seed\n\t        ranking_object = np.zeros(shape=np.shape(result_object)[:-1])\n\t        for d, _ in enumerate(datasets):\n\t            for m, metric_name in enumerate(metrics):\n\t                metric_values = np.mean(result_object[d, :, m, :] , axis=1)\n\t                last_place_zero = np.argwhere(np.array(metric_values) == -10).flatten()\n\t                if metric_name != 'conductance':\n\t                    ranking_of_algorithms = np.flip(np.argsort(metric_values)) + 1\n\t                else:\n\t                    ranking_of_algorithms = np.argsort(metric_values) + 1\n", "                ranking_of_algorithms[last_place_zero] = len(ranking_of_algorithms)\n\t                if scale_metrics:\n\t                    ranking_of_algorithms = scale_metric_values(ranking_of_algorithms, metric_values, metric_name)\n\t                ranking_object[d, :, m] = ranking_of_algorithms\n\t    else:\n\t        # calculate ranking on each seed\n\t        ranking_object = np.zeros_like(result_object)\n\t        for d, _ in enumerate(datasets):\n\t            for m, metric_name in enumerate(metrics):\n\t                for s, _, in enumerate(seeds):\n", "                    metric_values = result_object[d, :, m, s]\n\t                    last_place_zero = np.argwhere(np.array(metric_values) == -10).flatten()\n\t                    if metric_name != 'conductance':\n\t                        ranking_of_algorithms = np.flip(np.argsort(metric_values)) + 1\n\t                    else:\n\t                        ranking_of_algorithms = np.argsort(metric_values) + 1\n\t                    ranking_of_algorithms[last_place_zero] = len(ranking_of_algorithms)\n\t                    if scale_metrics:\n\t                        ranking_of_algorithms = scale_metric_values(ranking_of_algorithms, metric_values, metric_name)\n\t                    ranking_object[d, :, m, s] = ranking_of_algorithms\n", "    return ranking_object\n\tdef create_result_bar_chart(dataset_name, algorithms, folder, default_algos, default_folder, ax=None):\n\t    \"\"\"\n\t    displays the results in matplotlib with dashed borders for original comparison on single dataset\n\t    :param hpo_results: hyperparameter results\n\t    :param default_results: default parameter results\n\t    :param dataset_name: the dataset on which the results were gathered\n\t    :param ax: optional input axis\n\t    :return ax: axis on which figure is displayed\n\t    \"\"\"\n", "    if not ax:\n\t        fig, ax = plt.subplots(figsize=(10, 5))\n\t    #alt_colours = ['#dc143c', '#0bb5ff', '#2ca02c', '#800080']\n\t    alt_colours = ['C2', 'C0', 'C1', 'C3']\n\t    # extract key arrays for results\n\t    result_holder = get_all_results_from_storage([dataset_name], algorithms, folder, empty='zeros')\n\t    default_result_holder = get_all_results_from_storage([dataset_name], default_algos, default_folder, empty='zeros')\n\t    f1, f1_std = get_values_from_results_holder(result_holder, dataset_name, 'f1', return_std=True)\n\t    nmi, nmi_std = get_values_from_results_holder(result_holder, dataset_name, 'nmi', return_std=True)\n\t    modularity, modularity_std = get_values_from_results_holder(result_holder, dataset_name, 'modularity',\n", "                                                                return_std=True)\n\t    conductance, conductance_std = get_values_from_results_holder(result_holder, dataset_name, 'conductance',\n\t                                                                  return_std=True)\n\t    default_f1, default_f1_std = get_values_from_results_holder(default_result_holder, dataset_name, 'f1',\n\t                                                                return_std=True)\n\t    default_nmi, default_nmi_std = get_values_from_results_holder(default_result_holder, dataset_name, 'nmi',\n\t                                                                  return_std=True)\n\t    default_modularity, default_modularity_std = get_values_from_results_holder(default_result_holder, dataset_name,\n\t                                                                                'modularity',\n\t                                                                                return_std=True)\n", "    default_conductance, default_conductance_std = get_values_from_results_holder(default_result_holder, dataset_name,\n\t                                                                                  'conductance',\n\t                                                                                  return_std=True)\n\t    bar_width = 1 / 4\n\t    x_axis_names = np.arange(len(algorithms))\n\t    # plot hyperparameter results in full colour\n\t    ax.bar(x_axis_names, f1, yerr=f1_std,\n\t           width=bar_width, facecolor=alt_colours[0], alpha=0.9, linewidth=0, label='f1')\n\t    ax.bar(x_axis_names + bar_width, nmi, yerr=nmi_std,\n\t           width=bar_width, facecolor=alt_colours[1], alpha=0.9, linewidth=0, label='nmi')\n", "    ax.bar(x_axis_names + (2 * bar_width), modularity, yerr=modularity_std,\n\t           width=bar_width, facecolor=alt_colours[2], alpha=0.9, linewidth=0, label='modularity')\n\t    ax.bar(x_axis_names + (3 * bar_width), conductance, yerr=conductance_std,\n\t           width=bar_width, facecolor=alt_colours[3], alpha=0.9, linewidth=0, label='conductance')\n\t    # plot default parameters bars in dashed lines\n\t    blank_colours = np.zeros(4)\n\t    ax.bar(x_axis_names, default_f1, width=bar_width,\n\t           facecolor=blank_colours, edgecolor='black', linewidth=2, linestyle='--', label='default values')\n\t    ax.bar(x_axis_names + bar_width, default_nmi, width=bar_width,\n\t           facecolor=blank_colours, edgecolor='black', linewidth=2, linestyle='--')\n", "    ax.bar(x_axis_names + (2 * bar_width), default_modularity, width=bar_width,\n\t           facecolor=blank_colours, edgecolor='black', linewidth=2, linestyle='--')\n\t    ax.bar(x_axis_names + (3 * bar_width), default_conductance,\n\t           facecolor=blank_colours, width=bar_width, edgecolor='black', linewidth=2, linestyle='--')\n\t    # create the tick labels for axis\n\t    ax.set_xticks(x_axis_names - 0.5 * bar_width)\n\t    ax.set_xticklabels(algorithms, ha='left', rotation=-45, position=(-0.3, 0))\n\t    ax.set_axisbelow(True)\n\t    # Axis styling.\n\t    ax.spines['top'].set_visible(False)\n", "    ax.spines['right'].set_visible(False)\n\t    ax.spines['left'].set_visible(False)\n\t    ax.spines['bottom'].set_color('#DDDDDD')\n\t    ax.yaxis.grid(True, color='#EEEEEE')\n\t    ax.xaxis.grid(False)\n\t    # tighten the layout\n\t    ax.set_title(dataset_name, y=0.95, fontsize=98)\n\t    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n\t        item.set_fontsize(42)\n\t    return ax\n", "def create_dataset_landscape_fig(ax, title, datasets, algorithms, metrics, seeds, folder, scale_metrics=False, calc_ave_first=False):\n\t    # get datasest information\n\t    try:\n\t        clustering = pickle.load(open(f\"{ugle_path}/dataset_stats/clustering.pkl\", \"rb\"))\n\t        closeness = pickle.load(open(f\"{ugle_path}/dataset_stats/closeness.pkl\", \"rb\"))\n\t    except:\n\t        from ugle.datasets import compute_datasets_info\n\t        clustering, closeness = compute_datasets_info(datasets)\n\t        pickle.dump(clustering, open(f\"{ugle_path}/dataset_stats/clustering.pkl\", \"wb\"))\n\t        pickle.dump(closeness, open(f\"{ugle_path}/dataset_stats/closeness.pkl\", \"wb\"))\n", "    # make data landscape\n\t    dataset_points = np.array([clustering, closeness])\n\t    n_dataset = len(clustering)\n\t    n_points = 512\n\t    x_range = 0.8\n\t    y_range = 0.6\n\t    X, Y = np.meshgrid(np.linspace(0, x_range, n_points), np.linspace(0, y_range, n_points))\n\t    xy = np.vstack((X.flatten(), Y.flatten())).T\n\t    # find the idx of the dataset point closest in the landscape\n\t    all_distances = np.zeros((n_dataset, np.shape(xy)[0]))\n", "    for i in range(n_dataset):\n\t        all_distances[i, :] = [euclidean_distance(x, dataset_points[:, i]) for x in xy]\n\t    closest_point = np.argsort(all_distances, axis=0)[0]\n\t    Z = closest_point.reshape((n_points, n_points))\n\t    # get results\n\t    result_object = make_test_performance_object(datasets, algorithms, metrics, seeds, folder)\n\t    ranking_object = calculate_ranking_performance(result_object, datasets, metrics, seeds, scale_metrics=scale_metrics, calc_ave_first=calc_ave_first)\n\t    if calc_ave_first:\n\t        # calculate the ranking averages \n\t        ranking_over_datasets = np.mean(ranking_object, axis=2)\n", "    else:\n\t        # calculate the ranking averages \n\t        ranking_over_datasets = np.mean(ranking_object, axis=(2, 3))\n\t    # get best algorithm for each dataset\n\t    best_algo_idx_over_datasets = np.argsort(np.array(ranking_over_datasets), axis=1)[:, 0]\n\t    # work out best algorithm for any point on dataset landscape\n\t    best_algo_over_space = best_algo_idx_over_datasets[Z]\n\t    n_best_algos = len(np.unique(best_algo_over_space))\n\t    # scale the best algorithm space to work nicely with visualisation\n\t    dataset_replace = np.array(list(range(n_dataset)))\n", "    dataset_replace[np.unique(best_algo_over_space)] = np.array(list(range(n_best_algos)))\n\t    scaled_best_algo_over_space = dataset_replace[best_algo_over_space]\n\t    # get colouring for the visualisation\n\t    cmap = plt.get_cmap('Blues', n_best_algos)\n\t    cax = ax.imshow(np.flip(scaled_best_algo_over_space, axis=0), cmap=cmap, interpolation='nearest',\n\t                    extent=[0, x_range, 0, y_range])\n\t    # add color bar\n\t    best_algo_list = [algorithms[i] for i in np.unique(best_algo_over_space)]\n\t    best_algo_list = [albel.split(\"_\")[0] for albel in best_algo_list]\n\t    if len(best_algo_list) == 2:\n", "        tick_spacing = [0.25, 0.75]\n\t    elif len(best_algo_list) == 3:\n\t        tick_spacing = [0.33, 1., 1.66]\n\t    else:\n\t        tick_spacing = np.linspace(0.5, n_best_algos - 1.5, n_best_algos)\n\t    cbar = plt.colorbar(cax, ticks=range(n_best_algos), orientation=\"vertical\", shrink=0.8)\n\t    cbar.set_ticks(tick_spacing)\n\t    cbar.ax.set_yticklabels(best_algo_list, fontsize=14, ha='left', rotation=-40)\n\t    # add dataset locations\n\t    ax.scatter(clustering, closeness, marker='x', s=20, color='black', label='datasets')\n", "    # add nice visual elements\n\t    ax.set_xlabel('Clustering Coefficient', fontsize=16)\n\t    ax.set_ylabel('Closeness Centrality', fontsize=16)\n\t    ax.set_title(title, fontsize=22, pad=15)\n\t    ax.tick_params(axis='y', labelsize=18)\n\t    ax.tick_params(axis='x', labelsize=18)\n\t    return ax\n\tdef create_ranking_charts(datasets: list, algorithms: list, metrics: list, seeds: list, folder: str, title_name: str, \n\t    scale_metrics: bool=True, calc_ave_first: bool=False, set_legend: bool=True, ax=None, ax1=None, fig=None, fig1=None):\n\t    # init axis\n", "    if not ax and not fig:\n\t        fig, ax = plt.subplots(figsize=(6, 6))\n\t    elif (ax and not fig) or (fig and not ax):\n\t        print('please provide both fig and ax or neither')\n\t    if not ax1:\n\t        fig1, ax1 = plt.subplots(figsize=(6, 6))\n\t    elif (ax1 and not fig1) or (fig1 and not ax1):\n\t        print('please provide both fig1 and ax1 or neither')\n\t    # fetch results\n\t    result_object = make_test_performance_object(datasets, algorithms, metrics, seeds, folder)\n", "    # calculate ranking \n\t    ranking_object = calculate_ranking_performance(result_object, datasets, metrics, seeds, scale_metrics=scale_metrics, calc_ave_first=calc_ave_first)\n\t    # calculate overrall ranking and sensitivity + order\n\t    if calc_ave_first:\n\t        overrall_ranking = np.mean(ranking_object, axis=(0, 2))\n\t        # calculate the ranking averages \n\t        ranking_over_metrics = np.mean(ranking_object, axis=0)\n\t        ranking_over_datasets = np.mean(ranking_object, axis=2)\n\t    else:\n\t        overrall_ranking = np.mean(ranking_object, axis=(0, 2, 3))\n", "        # calculate the ranking averages \n\t        ranking_over_metrics = np.mean(ranking_object, axis=(0, 3))\n\t        ranking_over_datasets = np.mean(ranking_object, axis=(2, 3))\n\t    algorithms = np.array(algorithms)\n\t    ranking_order = np.argsort(overrall_ranking)\n\t    algo_labels = algorithms[ranking_order]\n\t    algo_labels = [albel.split(\"_\")[0] for albel in algo_labels]\n\t    # plot the by metric ranking\n\t    for rank_on_metric_ave_over_dataset, metric in zip(ranking_over_metrics.T, metrics):\n\t        ax.plot(algo_labels, rank_on_metric_ave_over_dataset[ranking_order],\n", "                marker=\"o\", label=metric, alpha=0.5, zorder=20)\n\t    # plot the overrall ranking\n\t    ax.scatter(algo_labels, overrall_ranking[ranking_order],\n\t               marker=\"x\", c='black', s=20, label='average \\noverall rank', zorder=100)\n\t    # set legend\n\t    if set_legend:\n\t        ax.legend(loc='upper center', fontsize=14, ncol=2, bbox_to_anchor=(0.5, -0.35))\n\t    # configure y axis\n\t    ax.set_ylim(0, 10.5)\n\t    ax.set_ylabel('Algorithm ranking\\naveraged over dataset', fontsize=14)\n", "    ax.tick_params(axis='y', labelsize=18)\n\t    # configure x axis\n\t    plt.setp(ax.xaxis.get_majorticklabels(), ha='left', rotation=-40, fontsize=14)\n\t    # create offset transform by 5 points in x direction\n\t    dx = 5 / 72.\n\t    dy = 0 / 72.\n\t    offset = matplotlib.transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)\n\t    # apply offset transform to all x ticklabels.\n\t    for label in ax.xaxis.get_majorticklabels():\n\t        label.set_transform(label.get_transform() - offset)\n", "    # set title\n\t    ax.set_title(f'{title_name}', fontsize=20)\n\t    plt.tight_layout()\n\t    # plot the by dataset ranking\n\t    colours = plt.get_cmap('tab20').colors\n\t    for rank_on_dataset_ave_over_metric, dataset, c in zip(ranking_over_datasets, datasets, colours):\n\t        ax1.plot(algo_labels, rank_on_dataset_ave_over_metric[ranking_order],\n\t                 marker=\"o\", label=dataset, alpha=0.5, zorder=20, c=c)\n\t    # plot the overrall ranking\n\t    ax1.scatter(algo_labels, overrall_ranking[ranking_order],\n", "                marker=\"x\", c='black', s=20, label='average \\noverall rank', zorder=100)\n\t    # set legend\n\t    if set_legend:\n\t        ax1.legend(loc='upper center', fontsize=14, ncol=4, bbox_to_anchor=(0.5, -0.35))\n\t    # set y axis\n\t    ax1.set_ylim(0, 10.5)\n\t    ax1.set_ylabel('Algorithm ranking\\naveraged over metric', fontsize=14)\n\t    ax1.tick_params(axis='y', labelsize=20)\n\t    # set x axis\n\t    plt.setp(ax1.xaxis.get_majorticklabels(), ha='left', rotation=-40, fontsize=14)\n", "    # create offset transform by 5 points in x direction\n\t    dx = 5 / 72.\n\t    dy = 0 / 72.\n\t    offset = matplotlib.transforms.ScaledTranslation(dx, dy, fig1.dpi_scale_trans)\n\t    # apply offset transform to all x ticklabels.\n\t    for label in ax1.xaxis.get_majorticklabels():\n\t        label.set_transform(label.get_transform() - offset)\n\t    # set title\n\t    ax1.set_title(f'{title_name}', fontsize=20)\n\t    plt.tight_layout()\n", "    return ax, ax1\n\tdef create_rand_dist_fig(ax, title, datasets, algorithms, metrics, seeds, folder, scale_metrics=False, calc_ave_first=False, set_legend=False):\n\t    if not ax:\n\t        fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n\t    x_axis = np.arange(0, len(algorithms), 0.001)\n\t    result_object = make_test_performance_object(datasets, algorithms, metrics, seeds, folder)\n\t    ranking_object = calculate_ranking_performance(result_object, datasets, metrics, seeds, calc_ave_first=calc_ave_first, scale_metrics=scale_metrics)\n\t    if calc_ave_first:\n\t        ave_axis = [0, 2]\n\t    else:\n", "        ave_axis = [0, 2, 3]\n\t    n_ranks = 1\n\t    for axis in ave_axis:\n\t        n_ranks *= ranking_object.shape[axis]\n\t    # average rank over dataset and metric + sensitivity over seed\n\t    all_ranks_per_algo = np.zeros(shape=(len(algorithms), n_ranks))\n\t    for i, algo in enumerate(algorithms):\n\t        all_ranks_per_algo[i] = ranking_object[:, i, :].flatten()\n\t    # calculate amount of overlap\n\t    out = 0.\n", "    for i, _ in enumerate(algorithms):\n\t        for j, _ in enumerate(algorithms):\n\t            if i != j:\n\t                out += wasserstein_distance(all_ranks_per_algo[i], all_ranks_per_algo[j])\n\t    # calculate averages\n\t    div_out = (len(algorithms) * (len(algorithms) -1))/2\n\t    ave_overlap = str(round(out/div_out, 3))\n\t    with plt.xkcd():\n\t        # plot the lines\n\t        for j, algo_ranks in enumerate(all_ranks_per_algo):\n", "            kde = gaussian_kde(algo_ranks)\n\t            y_axis = kde.evaluate(x_axis)\n\t            ax.plot(x_axis, y_axis, label=algorithms[j], zorder=10)\n\t        if set_legend:\n\t            ax.legend(loc='best', fontsize=16, ncol=3)\n\t            #ax.legend(loc='upper center', fontsize=14, ncol=3, bbox_to_anchor=(0.5, -0.35))\n\t            #ax.set_xlabel('algorithm rank distribution over all tests', fontsize=18)\n\t        #ax.set_ybound(0, 3)\n\t        ax.set_xbound(1, 10)\n\t        ax.set_ylabel('probability density', fontsize=18)\n", "        ax.set_xlabel(r'kde estimatation of rank distribution $f_{j}(r)$', fontsize=18)\n\t        #ax.text(0.4, 0.85, ave_overlap_text, fontsize=20, transform=ax.transAxes, zorder=1000)\n\t        ax.tick_params(axis='x', labelsize=18)\n\t        ax.tick_params(axis='y', labelsize=18)\n\t        ax.set_title(title + ave_overlap, fontsize=20)\n\t    return ax\n\tdef create_big_figure(datasets, algorithms, folder, default_algos, default_folder):\n\t    \"\"\"\n\t    creates figure for all datasets tested comparing default and hpo results\n\t    \"\"\"\n", "    # create holder figure\n\t    nrows, ncols = 4, 3\n\t    col_n, row_n = 0, 0\n\t    fig, axs = plt.subplots(nrows, ncols, figsize=(54, 78))\n\t    for dataset_name in datasets:\n\t        # create a figure on the axis\n\t        if row_n == nrows:\n\t            col_n += 1\n\t            row_n = 0\n\t        axs[row_n, col_n] = create_result_bar_chart(dataset_name, algorithms, folder, default_algos, default_folder, axs[row_n, col_n])\n", "        row_n += 1\n\t    axs[row_n, col_n].spines['top'].set_visible(False)\n\t    axs[row_n, col_n].spines['bottom'].set_visible(False)\n\t    axs[row_n, col_n].spines['left'].set_visible(False)\n\t    axs[row_n, col_n].spines['right'].set_visible(False)\n\t    axs[row_n, col_n].set_xticks([])\n\t    axs[row_n, col_n].set_yticks([])\n\t    axs[0, 0].legend()\n\t    for item in axs[0, 0].get_legend().get_texts():\n\t        item.set_fontsize(48)\n", "    fig.tight_layout()\n\t    fig.savefig(f\"{ugle_path}/figures/new_hpo_investigation.png\", bbox_inches='tight')\n\t    return\n\tdef create_algo_selection_on_dataset_landscape(datasets, algorithms, default_algos, metrics, seeds, folder, default_folder, titles):\n\t    fig, ax = plt.subplots(2, 2, figsize=(15, 7.5))\n\t    titles[0] += r' $(\\mathcal{\\hat{T}}_{(dhp)})$'\n\t    titles[1] += r' $(\\mathcal{\\hat{T}}_{(hpo)})$'\n\t    titles[2] += r' $(\\mathcal{T}_{(dhp)})$'\n\t    titles[3] += r' $(\\mathcal{T}_{(hpo)})$'\n\t    ax[0, 0] = create_dataset_landscape_fig(ax[0, 0], titles[0], datasets, default_algos, metrics, seeds, default_folder, calc_ave_first=True)\n", "    ax[0, 1] = create_dataset_landscape_fig(ax[0, 1], titles[1], datasets, algorithms, metrics, seeds, folder, calc_ave_first=True)\n\t    ax[1, 0] = create_dataset_landscape_fig(ax[1, 0], titles[2], datasets, default_algos, metrics, seeds, default_folder)\n\t    ax[1, 1] = create_dataset_landscape_fig(ax[1, 1], titles[3], datasets, algorithms, metrics, seeds, folder)\n\t    fig.tight_layout()\n\t    fig.savefig(f\"{ugle_path}/figures/new_dataset_landscape_comparison.png\")\n\t    return\n\tdef create_comparison_figures(datasets: list, algorithms: list, metrics: list, seeds: list, folder: str,\n\t                              default_algos: list, default_folder: str, titles: list):\n\t    # create holder figure\n\t    nrows, ncols = 2, 2\n", "    fig0, axs0 = plt.subplots(nrows, ncols, figsize=(15, 7.5))\n\t    # create holder figure\n\t    nrows, ncols = 2, 2\n\t    fig1, axs1 = plt.subplots(nrows, ncols, figsize=(15, 7.5))\n\t    titles[0] += r' $(\\mathcal{\\hat{T}}_{(dhp)})$'\n\t    titles[1] += r' $(\\mathcal{\\hat{T}}_{(hpo)})$'\n\t    titles[2] += r' $(\\mathcal{T}_{(dhp)})$'\n\t    titles[3] += r' $(\\mathcal{T}_{(hpo)})$'\n\t    axs0[0, 0], axs1[0, 0] = create_ranking_charts(datasets, default_algos, metrics, seeds, default_folder,\n\t                                           title_name=titles[0],\n", "                                           scale_metrics=False, calc_ave_first=True, set_legend=False, ax=axs0[0, 0], ax1=axs1[0, 0], fig=fig0,\n\t                                           fig1=fig1)\n\t    axs0[0, 1], axs1[0, 1] = create_ranking_charts(datasets, algorithms, metrics, seeds, folder, title_name=titles[1],\n\t                                           scale_metrics=False, calc_ave_first=True, set_legend=False, ax=axs0[0, 1], ax1=axs1[0, 1], fig=fig0,\n\t                                           fig1=fig1)\n\t    axs0[1, 0], axs1[1, 0] = create_ranking_charts(datasets, default_algos, metrics, seeds, default_folder,\n\t                                           title_name=titles[2],\n\t                                           scale_metrics=False, set_legend=False, ax=axs0[1, 0], ax1=axs1[1, 0], fig=fig0,\n\t                                           fig1=fig1)\n\t    axs0[1, 1], axs1[1, 1] = create_ranking_charts(datasets, algorithms, metrics, seeds, folder, title_name=titles[3],\n", "                                           scale_metrics=False, set_legend=True, ax=axs0[1, 1], ax1=axs1[1, 1], fig=fig0,\n\t                                           fig1=fig1)\n\t    fig0.tight_layout()\n\t    fig1.tight_layout()\n\t    fig0.savefig(f\"{ugle_path}/figures/new_ranking_comparison_metrics.png\", bbox_inches='tight')\n\t    fig1.savefig(f\"{ugle_path}/figures/new_ranking_comparison_datasets.png\", bbox_inches='tight')\n\t    return\n\tdef create_rand_dist_comparison(datasets: list, algorithms: list, metrics: list, seeds: list, folder: str,\n\t                                default_algos: list, default_folder: str, titles: list):\n\t    # create holder figure\n", "    nrows, ncols = 2, 2\n\t    fig, ax = plt.subplots(nrows, ncols, figsize=(15, 7.5))\n\t    titles[0] += r' $\\Omega(\\mathcal{\\hat{T}}_{(dhp)})$ : '\n\t    titles[1] += r' $\\Omega(\\mathcal{\\hat{T}}_{(hpo)})$ : '\n\t    titles[2] += r' $\\Omega(\\mathcal{T}_{(dhp)})$ : '\n\t    titles[3] += r' $\\Omega(\\mathcal{T}_{(hpo)})$ : '\n\t    ax[0, 0] = create_rand_dist_fig(ax[0, 0], titles[0], datasets, default_algos, metrics, seeds, default_folder, calc_ave_first=True)\n\t    ax[0, 1] = create_rand_dist_fig(ax[0, 1], titles[1], datasets, algorithms, metrics, seeds, folder, calc_ave_first=True)\n\t    ax[1, 0] = create_rand_dist_fig(ax[1, 0], titles[2], datasets, default_algos, metrics, seeds, default_folder)\n\t    ax[1, 1] = create_rand_dist_fig(ax[1, 1], titles[3], datasets, algorithms, metrics, seeds, folder, set_legend=True)\n", "    fig.tight_layout()\n\t    fig.savefig(f'{ugle_path}/figures/new_rand_dist_comparison.png', bbox_inches='tight')\n\t    return\n\tdef create_all_paper_figures(datasets, algorithms, metrics, seeds, folder, default_folder, default_algos):\n\t    titles = ['a) Default HPs w/ AveSeed', 'b) HPO w/ AveSeed', 'c) Default HPs w/ SeedRanking', 'd) HPO w/ SeedRanking']\n\t    create_rand_dist_comparison(datasets, algorithms, metrics, seeds, folder, default_algos, default_folder, deepcopy(titles))\n\t    create_algo_selection_on_dataset_landscape(datasets, algorithms, default_algos, metrics, seeds, folder, default_folder, deepcopy(titles))\n\t    create_comparison_figures(datasets, algorithms, metrics, seeds, folder, default_algos, default_folder, deepcopy(titles))\n\t    create_big_figure(datasets, algorithms, folder, default_algos, default_folder)\n\t    return\n", "#algorithms = ['daegc', 'dgi', 'dmon', 'grace', 'mvgrl', 'selfgnn', 'sublime', 'bgrl', 'vgaer', 'cagc']\n\t#datasets = ['cora', 'citeseer', 'dblp', 'bat', 'eat', 'texas', 'wisc', 'cornell', 'uat', 'amac', 'amap']\n\t#metrics = ['nmi', 'modularity', 'f1', 'conductance']\n\t#folder = './progress_results/'\n\t#seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n\t#default_algos = ['daegc_default', 'dgi_default', 'dmon_default', 'grace_default', 'mvgrl_default', 'selfgnn_default',\n\t#                 'sublime_default', 'bgrl_default', 'vgaer_default', 'cagc_default']\n\t#default_folder = './new_default/'\n\t#create_all_paper_figures(datasets, algorithms, metrics, seeds, folder, default_folder, default_algos)\n\t# creates a tikz figure to show someone\n", "#fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n\t#ax = create_rand_dist_fig(ax, r'Framework Rank Distinction Coefficient $\\Omega(\\mathcal{\\hat{T}}_{(hpo)}) : $', datasets, algorithms, metrics, seeds, folder, calc_ave_first=True, set_legend=True)\n\t#fig.tight_layout()\n\t#fig.savefig(f'{ugle_path}/figures/tkiz_fig.png', bbox_inches='tight')\n\tdef rank_values(scores):\n\t    ranks = []\n\t    for score in scores:\n\t       ranks.append(scores.index(score) + 1)\n\t    return ranks\n\tdef calculate_ranking_coefficient_over_dataset(result_object, algorithms, dataset_idx, metrics):\n", "    ranking_object = np.mean(result_object, axis=-1)[dataset_idx, :, :]\n\t    n_ranks = len(metrics)\n\t    all_ranks_per_algo = np.zeros(shape=(len(algorithms), n_ranks))\n\t    for m, metric_name in enumerate(metrics):\n\t        if metric_name != 'conductance':\n\t            all_ranks_per_algo[:, m] = rank_values(np.flip(np.sort(ranking_object[:, m])).tolist())\n\t        else:\n\t            all_ranks_per_algo[:, m] = rank_values(np.sort(ranking_object[:, m]).tolist())\n\t    coeff = 0.\n\t    for i, _ in enumerate(algorithms):\n", "        for j, _ in enumerate(algorithms):\n\t            if i != j:\n\t                coeff += wasserstein_distance(all_ranks_per_algo[i], all_ranks_per_algo[j])\n\t    # calculate averages\n\t    div_out = (len(algorithms) * (len(algorithms) -1 ))/2\n\t    coeff = round(coeff/div_out, 3)  \n\t    return coeff \n\tdef create_synth_figure():\n\t    synth_datasets_2 = ['synth_disjoint_disjoint_2', 'synth_disjoint_random_2', 'synth_disjoint_complete_2',\n\t                    'synth_random_disjoint_2', 'synth_random_random_2', 'synth_random_complete_2',\n", "                    'synth_complete_disjoint_2', 'synth_complete_random_2', 'synth_complete_complete_2']\n\t    synth_datasets = ['synth_disjoint_disjoint_4', 'synth_disjoint_random_4', 'synth_disjoint_complete_4',\n\t                    'synth_random_disjoint_4', 'synth_random_random_4', 'synth_random_complete_4',\n\t                    'synth_complete_disjoint_4', 'synth_complete_random_4', 'synth_complete_complete_4']\n\t    synth_algorithms = ['dgi_default', 'daegc_default', 'dmon_default', 'grace_default', 'sublime_default', 'bgrl_default', 'vgaer_default']\n\t    synth_folder = './synth_defaults/'\n\t    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n\t    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n\t    # fetch results\n\t    result_object = make_test_performance_object(synth_datasets, synth_algorithms, metrics, seeds, synth_folder)\n", "    # create graph subfigures \n\t    nrows, ncols = 3, 3\n\t    fig, axes = plt.subplots(nrows, ncols, figsize=(9, 9))\n\t    for i, ax in enumerate(axes.flat):\n\t        coeff = calculate_ranking_coefficient_over_dataset(result_object, synth_algorithms, i, metrics)\n\t        title = (\" \").join(synth_datasets[i].split(\"_\")[1:3])\n\t        title = 'adj: ' + synth_datasets[i].split(\"_\")[1] + \" feat: \" + synth_datasets[i].split(\"_\")[2]\n\t        ax.set_title(f\"{title}: {str(coeff)}\" )\n\t        alt_colours = ['C3', 'C0', 'C2', 'C1']\n\t        bar_width = 1 / 4\n", "        x_axis_names = np.arange(len(synth_algorithms))\n\t        # plot results\n\t        for m, metric in enumerate(metrics):\n\t            metric_vals = []\n\t            metric_std = []\n\t            for a, algo in enumerate(synth_algorithms):\n\t                metric_vals.append(np.mean(result_object[i, a, m, :]))\n\t                metric_std.append(np.std(result_object[i, a, m, :]))\n\t            ax.bar(x_axis_names + (m * bar_width), metric_vals, yerr=metric_std, \n\t                   width=bar_width, facecolor=alt_colours[m], alpha=0.9, linewidth=0, label=metric)\n", "        ax.set_xticks(x_axis_names - 0.5 * bar_width)\n\t        ax.set_xticklabels(synth_algorithms, ha='left', rotation=-45, position=(-0.5, 0.0))\n\t        ax.set_axisbelow(True)\n\t        ax.set_ylim(0.0, 1.0)\n\t        ax.axhline(y=0.5, color='k', linestyle='-') \n\t    plt.tight_layout()\n\t    plt.savefig('synth_4_default.png', bbox_inches='tight')\n\t    print('done')\n\tdef create_trainpercent_figure():\n\t    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n", "    datasets = ['citeseer', 'cora', 'dblp', 'texas', 'wisc', 'cornell', 'amac']\n\t    algorithms = ['dgi', 'daegc', 'dmon', 'grace', 'sublime', 'bgrl', 'vgaer']\n\t    percents = ['01', '03', '05', '07', '09']\n\t    percents_labels = ['0.1', '0.3', '0.5', '0.7', '0.9']\n\t    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n\t    folder = './revamp_train_percent/'\n\t    nrows, ncols = len(algorithms), len(datasets)\n\t    fig, axes = plt.subplots(nrows, ncols, figsize=(7.5, 10))\n\t    res_objs = []\n\t    for pct in percents:\n", "        res_objs.append(make_test_performance_object(datasets, algorithms, metrics, seeds, folder + pct + '/'))\n\t    res_objs = np.stack(res_objs, axis=0)\n\t    for d, dataset in enumerate(datasets):\n\t        for a, algo in enumerate(algorithms):\n\t            ax = axes[a, d]\n\t            ax.set_title(f'{algo} -- {dataset}')\n\t            alt_colours = ['C3', 'C0', 'C2', 'C1']\n\t            bar_width = 1 / 4\n\t            x_axis_names = np.arange(len(percents))\n\t            # plot results\n", "            for m, metric in enumerate(metrics):\n\t                metric_vals = []\n\t                metric_std = []\n\t                for p in range(len(percents)):\n\t                    metric_vals.append(np.mean(res_objs[p, d, a, m, :]))\n\t                    metric_std.append(np.std(res_objs[p, d, a, m, :]))\n\t                ax.bar(x_axis_names + (m * bar_width), metric_vals, yerr=metric_std, \n\t                    width=bar_width, facecolor=alt_colours[m], alpha=0.9, linewidth=0, label=metric)\n\t            ax.set_xticks(x_axis_names - 0.5 * bar_width)\n\t            ax.set_xticklabels(percents_labels, ha='left', rotation=-45, position=(-0.5, 0.0))\n", "            ax.set_axisbelow(True)\n\t            ax.set_ylim(0.0, 1.0)\n\t    plt.tight_layout()\n\t    plt.show()\n\t    print('done')\n\t#create_trainpercent_figure()\n\t#create_synth_figure()\n\tdef calc_correlation():\n\t    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n\t    datasets = ['citeseer', 'cora', 'dblp', 'texas', 'wisc', 'cornell']\n", "    algorithms = ['dgi_default', 'daegc_default', 'dmon_default', 'grace_default', 'sublime_default', 'bgrl_default', 'vgaer_default']\n\t    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n\t    folder = './results/q1_default_predict_super/'\n\t    mod_results = []\n\t    con_results = []\n\t    for dataset in datasets:\n\t        for algo in algorithms:\n\t            filename = f\"{dataset}_{algo}.pkl\"\n\t            file_found = search_results(folder, filename)\n\t            if file_found:\n", "                result = pickle.load(open(file_found, \"rb\"))\n\t            for seed_result in result.results:\n\t                for metric_result in seed_result.study_output:\n\t                    if 'modularity' in metric_result.metrics:\n\t                       mod_results.append([metric_result.results['modularity'], metric_result.results['f1'], metric_result.results['nmi']])\n\t                    if 'conductance' in metric_result.metrics:\n\t                       con_results.append([metric_result.results['conductance'], metric_result.results['f1'], metric_result.results['nmi']])\n\t    mod_results = np.asarray(mod_results)\n\t    con_results = np.asarray(con_results)\n\t    mod_f1 = np.corrcoef(mod_results[:, 0], mod_results[:, 1])[0,1]\n", "    mod_nmi = np.corrcoef(mod_results[:, 0], mod_results[:, 2])[0,1]\n\t    con_f1 = np.corrcoef(con_results[:, 0], con_results[:, 1])[0,1]\n\t    con_nmi = np.corrcoef(con_results[:, 0], con_results[:, 2])[0,1]\n\t    print('Correlation Coefficients: ')\n\t    print(f'Modularity --> F1: {mod_f1:.3f}')\n\t    print(f'Modularity --> NMI: {mod_nmi:.3f}')\n\t    print(f'Conductance --> F1: {con_f1:.3f}')\n\t    print(f'Conductance --> NMI: {con_nmi:.3f}')\n\tdef calc_synth_results():\n\t    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n", "    datasets = ['synth_disjoint_disjoint_2', 'synth_disjoint_random_2', 'synth_disjoint_complete_2',\n\t                    'synth_random_disjoint_2', 'synth_random_random_2', 'synth_random_complete_2',\n\t                    'synth_complete_disjoint_2', 'synth_complete_random_2', 'synth_complete_complete_2']\n\t    algorithms = ['dgi_default', 'daegc_default', 'dmon_default', 'grace_default', 'sublime_default', 'bgrl_default', 'vgaer_default']\n\t    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n\t    folder = './results/synth_defaults/'\n\t    mod_results = []\n\t    con_results = []\n\t    for dataset in datasets:\n\t        for algo in algorithms:\n", "            filename = f\"{dataset}_{algo}.pkl\"\n\t            file_found = search_results(folder, filename)\n\t            if file_found:\n\t                result = pickle.load(open(file_found, \"rb\"))\n\t            for seed_result in result.results:\n\t                for metric_result in seed_result.study_output:\n\t                    if 'modularity' in metric_result.metrics:\n\t                       mod_results.append([metric_result.results['modularity'], metric_result.results['f1'], metric_result.results['nmi']])\n\t                    if 'conductance' in metric_result.metrics:\n\t                       con_results.append([metric_result.results['conductance'], metric_result.results['f1'], metric_result.results['nmi']])\n", "    mod_results = np.asarray(mod_results)\n\t    con_results = np.asarray(con_results)\n\t#calc_synth_results()"]}
{"filename": "ugle/trainer.py", "chunked_list": ["import ugle\n\timport ugle.utils as utils\n\timport ugle.datasets as datasets\n\tfrom ugle.logger import log\n\timport time\n\tfrom omegaconf import OmegaConf, DictConfig, ListConfig\n\timport numpy as np\n\timport optuna\n\tfrom optuna import Trial, Study\n\tfrom optuna.samplers import TPESampler\n", "from optuna.pruners import HyperbandPruner\n\tfrom optuna.trial import TrialState\n\timport threading\n\timport time\n\tfrom alive_progress import alive_it\n\timport copy\n\timport torch\n\tfrom typing import Dict, List, Optional, Tuple\n\tfrom collections import defaultdict\n\timport optuna\n", "import warnings\n\tfrom os.path import exists\n\tfrom os import makedirs\n\tfrom optuna.exceptions import ExperimentalWarning\n\twarnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n\toptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n\t# https://stackoverflow.com/questions/9850995/tracking-maximum-memory-usage-by-a-python-function\n\tclass StoppableThread(threading.Thread):\n\t    def __init__(self):\n\t        super(StoppableThread, self).__init__()\n", "        self.daemon = True\n\t        self.__monitor = threading.Event()\n\t        self.__monitor.set()\n\t        self.__has_shutdown = False\n\t    def run(self):\n\t        '''Overloads the threading.Thread.run'''\n\t        # Call the User's Startup functions\n\t        self.startup()\n\t        # Loop until the thread is stopped\n\t        while self.isRunning():\n", "            self.mainloop()\n\t        # Clean up\n\t        self.cleanup()\n\t        # Flag to the outside world that the thread has exited\n\t        # AND that the cleanup is complete\n\t        self.__has_shutdown = True\n\t    def stop(self):\n\t        self.__monitor.clear()\n\t    def isRunning(self):\n\t        return self.__monitor.isSet()\n", "    def isShutdown(self):\n\t        return self.__has_shutdown\n\t    ###############################\n\t    ### User Defined Functions ####\n\t    ###############################\n\t    def mainloop(self):\n\t        '''\n\t        Expected to be overwritten in a subclass!!\n\t        Note that Stoppable while(1) is handled in the built in \"run\".\n\t        '''\n", "        pass\n\t    def startup(self):\n\t        '''Expected to be overwritten in a subclass!!'''\n\t        pass\n\t    def cleanup(self):\n\t        '''Expected to be overwritten in a subclass!!'''\n\t        pass\n\tclass MyLibrarySniffingClass(StoppableThread):\n\t    def __init__(self, target_lib_call):\n\t        super(MyLibrarySniffingClass, self).__init__()\n", "        self.target_function = target_lib_call\n\t        self.results = None\n\t    def startup(self):\n\t        # Overload the startup function\n\t        log.info(\"Starting Memory Calculation\")\n\t    def cleanup(self):\n\t        # Overload the cleanup function\n\t        log.info(\"Ending Memory Tracking\")\n\t    def mainloop(self):\n\t        # Start the library Call\n", "        self.results = self.target_function()\n\t        # Kill the thread when complete\n\t        self.stop()\n\tclass StopWhenMaxTrialsHit:\n\t    def __init__(self, max_n_trials: int, max_n_pruned: int):\n\t        self.max_n_trials = max_n_trials\n\t        self.max_pruned = max_n_pruned\n\t        self.completed_trials = 0\n\t        self.pruned_in_a_row = 0\n\t    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n", "        if trial.state == optuna.trial.TrialState.COMPLETE:\n\t            self.completed_trials += 1\n\t            self.pruned_in_a_row = 0\n\t        elif trial.state == optuna.trial.TrialState.PRUNED:\n\t            self.pruned_in_a_row += 1\n\t        if self.completed_trials >= self.max_n_trials:\n\t            log.info('Stopping Study for Reaching Max Number of Trials')\n\t            study.stop()\n\t        if self.pruned_in_a_row >= self.max_pruned:\n\t            log.info('Stopping Study for Reaching Max Number of Pruned Trials in a Row')\n", "            study.stop()\n\tclass ParamRepeatPruner:\n\t    \"\"\"Prunes repeated trials, which means trials with the same parameters won't waste time/resources.\"\"\"\n\t    def __init__(\n\t        self,\n\t        study: optuna.study.Study,\n\t        repeats_max: int = 0,\n\t        should_compare_states: List[TrialState] = [TrialState.COMPLETE],\n\t        compare_unfinished: bool = True,\n\t    ):\n", "        \"\"\"\n\t        Args:\n\t            study (optuna.study.Study): Study of the trials.\n\t            repeats_max (int, optional): Instead of prunning all of them (not repeating trials at all, repeats_max=0) you can choose to repeat them up to a certain number of times, useful if your optimization function is not deterministic and gives slightly different results for the same params. Defaults to 0.\n\t            should_compare_states (List[TrialState], optional): By default it only skips the trial if the paremeters are equal to existing COMPLETE trials, so it repeats possible existing FAILed and PRUNED trials. If you also want to skip these trials then use [TrialState.COMPLETE,TrialState.FAIL,TrialState.PRUNED] for example. Defaults to [TrialState.COMPLETE].\n\t            compare_unfinished (bool, optional): Unfinished trials (e.g. `RUNNING`) are treated like COMPLETE ones, if you don't want this behavior change this to False. Defaults to True.\n\t        \"\"\"\n\t        self.should_compare_states = should_compare_states\n\t        self.repeats_max = repeats_max\n\t        self.repeats: Dict[int, List[int]] = defaultdict(lambda: [], {})\n", "        self.unfinished_repeats: Dict[int, List[int]] = defaultdict(lambda: [], {})\n\t        self.compare_unfinished = compare_unfinished\n\t        self.study = study\n\t    @property\n\t    def study(self) -> Optional[optuna.study.Study]:\n\t        return self._study\n\t    @study.setter\n\t    def study(self, study):\n\t        self._study = study\n\t        if self.study is not None:\n", "            self.register_existing_trials()\n\t    def register_existing_trials(self):\n\t        \"\"\"In case of studies with existing trials, it counts existing repeats\"\"\"\n\t        trials = self.study.trials\n\t        trial_n = len(trials)\n\t        for trial_idx, trial_past in enumerate(self.study.trials[1:]):\n\t            self.check_params(trial_past, False, -trial_n + trial_idx)\n\t    def prune(self):\n\t        self.check_params()\n\t    def should_compare(self, state):\n", "        return any(state == state_comp for state_comp in self.should_compare_states)\n\t    def clean_unfinised_trials(self):\n\t        trials = self.study.trials\n\t        finished = []\n\t        for key, value in self.unfinished_repeats.items():\n\t            if self.should_compare(trials[key].state):\n\t                for t in value:\n\t                    self.repeats[key].append(t)\n\t                finished.append(key)\n\t        for f in finished:\n", "            del self.unfinished_repeats[f]\n\t    def check_params(\n\t        self,\n\t        trial: Optional[optuna.trial.BaseTrial] = None,\n\t        prune_existing=True,\n\t        ignore_last_trial: Optional[int] = None,\n\t    ):\n\t        if self.study is None:\n\t            return\n\t        trials = self.study.trials\n", "        if trial is None:\n\t            trial = trials[-1]\n\t            ignore_last_trial = -1\n\t        self.clean_unfinised_trials()\n\t        self.repeated_idx = -1\n\t        self.repeated_number = -1\n\t        for idx_p, trial_past in enumerate(trials[:ignore_last_trial]):\n\t            should_compare = self.should_compare(trial_past.state)\n\t            should_compare |= (\n\t                self.compare_unfinished and not trial_past.state.is_finished()\n", "            )\n\t            if should_compare and trial.params == trial_past.params:\n\t                if not trial_past.state.is_finished():\n\t                    self.unfinished_repeats[trial_past.number].append(trial.number)\n\t                    continue\n\t                self.repeated_idx = idx_p\n\t                self.repeated_number = trial_past.number\n\t                break\n\t        if self.repeated_number > -1:\n\t            self.repeats[self.repeated_number].append(trial.number)\n", "        if len(self.repeats[self.repeated_number]) > self.repeats_max:\n\t            if prune_existing:\n\t                log.info('Pruning Trial for Suggesting Duplicate Parameters')\n\t                raise optuna.exceptions.TrialPruned()\n\t        return self.repeated_number\n\t    def get_value_of_repeats(\n\t        self, repeated_number: int, func=lambda value_list: np.mean(value_list)\n\t    ):\n\t        if self.study is None:\n\t            raise ValueError(\"No study registered.\")\n", "        trials = self.study.trials\n\t        values = (\n\t            trials[repeated_number].value,\n\t            *(\n\t                trials[tn].value\n\t                for tn in self.repeats[repeated_number]\n\t                if trials[tn].value is not None\n\t            ),\n\t        )\n\t        return func(values)\n", "def log_trial_result(trial: Trial, results: dict, valid_metrics: list, multi_objective_study: bool):\n\t    \"\"\"\n\t    logs the results for a trial\n\t    :param trial: trial object\n\t    :param results: result dictionary for trial\n\t    :param valid_metrics: validation metrics used\n\t    :param multi_objective_study: boolean whether under multi-objective study or not\n\t    \"\"\"\n\t    if not multi_objective_study:\n\t        # log validation results\n", "        trial_value = results[valid_metrics[0]]\n\t        log.info(f'Trial {trial.number} finished. Validation result || {valid_metrics[0]}: {trial_value} ||')\n\t        # log best trial comparison\n\t        new_best_message = f'New best trial {trial.number}'\n\t        if trial.number > 0:\n\t            if trial_value > trial.study.best_value and trial.study.direction.name == 'MAXIMIZE':\n\t                log.info(new_best_message)\n\t            elif trial_value < trial.study.best_value and trial.study.direction.name == 'MINIMIZE':\n\t                log.info(new_best_message)\n\t            else:\n", "                log.info(\n\t                    f'Trial {trial.number} finished. Best trial is {trial.study.best_trial.number} with {valid_metrics[0]}: {trial.study.best_value}')\n\t        else:\n\t            log.info(new_best_message)\n\t    else:\n\t        # log trial results\n\t        right_order_results = [results[k] for k in valid_metrics]\n\t        to_log_trial_values = ''.join(\n\t            f'| {metric}: {right_order_results[i]} |' for i, metric in enumerate(valid_metrics))\n\t        log.info(f'Trial {trial.number} finished. Validation result |{to_log_trial_values}|')\n", "        # log best trial comparison\n\t        if trial.number > 0:\n\t            # get best values for each metric across the best trials\n\t            best_values, associated_trial = ugle.utils.extract_best_trials_info(trial.study, valid_metrics)\n\t            # compare best values for each trial with new values found\n\t            improved_metrics = []\n\t            for i, metric_result in enumerate(right_order_results):\n\t                if (metric_result > best_values[i] and trial.study.directions[i].name == 'MAXIMIZE') or \\\n\t                        (metric_result < best_values[i] and trial.study.directions[i].name == 'MINIMIZE'):\n\t                    best_values[i] = metric_result\n", "                    improved_metrics.append(valid_metrics[i])\n\t                    associated_trial[i] = trial.number\n\t            # log best trial and value for each metric\n\t            if improved_metrics:\n\t                improved_metrics_str = ''.join(f'{metric}, ' for metric in improved_metrics)\n\t                improved_metrics_str = improved_metrics_str[:improved_metrics_str.rfind(',')]\n\t                log.info(f'New Best trial for metrics: {improved_metrics_str}')\n\t            else:\n\t                log.info('Trial worse than existing across all metrics')\n\t            best_so_far = ''.join(\n", "                f'trial {associated_trial[i]} ({metric}: {best_values[i]}), ' for i, metric in enumerate(valid_metrics))\n\t            best_so_far = best_so_far[:best_so_far.rfind(',')]\n\t            log.info(f'Best results so far: {best_so_far}')\n\t    return\n\tclass ugleTrainer:\n\t    def __init__(self, cfg: DictConfig):\n\t        super(ugleTrainer, self).__init__()\n\t        _device = ugle.utils.set_device(cfg.trainer.gpu)\n\t        if _device == 'cpu':\n\t            self.device_name = _device\n", "        else:\n\t            self.device_name = cfg.trainer.gpu\n\t        self.device = torch.device(_device)\n\t        utils.set_random(cfg.args.random_seed)\n\t        if cfg.trainer.show_config:\n\t            log.info(OmegaConf.to_yaml(cfg.args))\n\t        cfg = ugle.utils.process_study_cfg_parameters(cfg)\n\t        self.cfg = cfg\n\t        if not exists(cfg.trainer.models_path):\n\t            makedirs(cfg.trainer.models_path)\n", "        self.progress_bar = None\n\t        self.model = None\n\t        self.loss_function = None\n\t        self.scheduler = None\n\t        self.optimizers = None\n\t        self.patience_wait = 0\n\t        self.best_loss = 1e9\n\t        self.current_epoch = 0\n\t    def load_database(self):\n\t        log.info('loading dataset')\n", "        if 'synth' not in self.cfg.dataset:\n\t            # loads and splits the dataset\n\t            features, label, train_adjacency, test_adjacency = datasets.load_real_graph_data(\n\t                self.cfg.dataset,\n\t                self.cfg.trainer.training_to_testing_split,\n\t                self.cfg.trainer.split_scheme,\n\t                self.cfg.trainer.split_addition_percentage)\n\t        else:\n\t            _, adj_type, feature_type, n_clusters = self.cfg.dataset.split(\"_\")\n\t            n_clusters = int(n_clusters)\n", "            adjacency, features, label = datasets.create_synth_graph(n_nodes=1000, n_features=500, n_clusters=n_clusters, \n\t                                                                     adj_type=adj_type, feature_type=feature_type)\n\t            train_adjacency, test_adjacency = datasets.split_adj(adjacency,\n\t                                                                 self.cfg.trainer.training_to_testing_split,\n\t                                                                 self.cfg.trainer.split_scheme)\n\t        # extract database relevant info\n\t        if not self.cfg.args.n_clusters:\n\t            self.cfg.args.n_clusters = np.unique(label).shape[0]\n\t        self.cfg.args.n_nodes = features.shape[0]\n\t        self.cfg.args.n_features = features.shape[1]\n", "        return features, label, train_adjacency, test_adjacency\n\t    def eval(self):\n\t        # loads the database to train on\n\t        features, label, validation_adjacency, test_adjacency = self.load_database()\n\t        # creates store for range of hyperparameters optimised over\n\t        self.cfg.hypersaved_args = copy.deepcopy(self.cfg.args)\n\t        log.debug('splitting dataset into train/validation')\n\t        train_adjacency, validation_adjacency = datasets.split_adj(validation_adjacency, \n\t                                                          self.cfg.trainer.train_to_valid_split, \n\t                                                          self.cfg.trainer.split_scheme)\n", "        # process data for training\n\t        processed_data = self.preprocess_data(features, train_adjacency)\n\t        processed_valid_data = self.preprocess_data(features, validation_adjacency)\n\t        if not self.cfg.trainer.only_testing:\n\t            optuna.logging.disable_default_handler()\n\t            # creates the hpo study\n\t            if self.cfg.trainer.multi_objective_study:\n\t                study = optuna.create_study(study_name=f'{self.cfg.model}_{self.cfg.dataset}',\n\t                                            directions=self.cfg.trainer.optimisation_directions,\n\t                                            sampler=TPESampler(seed=self.cfg.args.random_seed, multivariate=True, group=True))\n", "            else:\n\t                study = optuna.create_study(study_name=f'{self.cfg.model}_{self.cfg.dataset}',\n\t                                            direction=self.cfg.trainer.optimisation_directions[0],\n\t                                            sampler=TPESampler(seed=self.cfg.args.random_seed))\n\t            log.info(f\"A new hyperparameter study created: {study.study_name}\")\n\t            study_stop_cb = StopWhenMaxTrialsHit(self.cfg.trainer.n_trials_hyperopt, self.cfg.trainer.max_n_pruned)\n\t            prune_params = ParamRepeatPruner(study)\n\t            study.optimize(lambda trial: self.train(trial, self.cfg.args,\n\t                                                    label,\n\t                                                    features,\n", "                                                    processed_data,\n\t                                                    validation_adjacency,\n\t                                                    processed_valid_data,\n\t                                                    prune_params=prune_params),\n\t                                n_trials=2*self.cfg.trainer.n_trials_hyperopt,\n\t                                callbacks=[study_stop_cb])\n\t            # assigns test parameters found in the study\n\t            if not self.cfg.trainer.multi_objective_study:\n\t                params_to_assign = study.best_trial.params\n\t                if params_to_assign != {}:\n", "                    log.info('Best Hyperparameters found: ')\n\t                    for hp_key, hp_val in params_to_assign.items():\n\t                        log.info(f'{hp_key} : {hp_val}')\n\t                self.cfg = utils.assign_test_params(self.cfg, params_to_assign)\n\t        processed_test_data = self.preprocess_data(features, test_adjacency)\n\t        # retrains the model on the validation adj and evaluates test performance\n\t        if not self.cfg.trainer.multi_objective_study:\n\t            self.cfg.trainer.calc_time = False\n\t            log.debug('Retraining model')\n\t            self.train(None, self.cfg.args, label, features, processed_data, validation_adjacency, processed_valid_data)\n", "            results = self.testing_loop(label, features, test_adjacency, processed_test_data,\n\t                                        self.cfg.trainer.test_metrics)\n\t            # log test results\n\t            right_order_results = [results[k] for k in self.cfg.trainer.valid_metrics]\n\t            to_log_trial_values = ''.join(f'| {metric}: {right_order_results[i]} |' for i, metric in\n\t                                          enumerate(self.cfg.trainer.valid_metrics))\n\t            log.info(f'Test results |{to_log_trial_values}|')\n\t            objective_results = {'metrics': self.cfg.trainer.valid_metrics[0],\n\t                                 'results': results,\n\t                                 'args': params_to_assign}\n", "        else:\n\t            if not self.cfg.trainer.only_testing:\n\t                # best hp found for metrics\n\t                best_values, associated_trial = ugle.utils.extract_best_trials_info(study, self.cfg.trainer.valid_metrics)\n\t                unique_trials = list(np.unique(np.array(associated_trial)))\n\t            elif not self.cfg.get(\"previous_results\", False):\n\t                unique_trials = [-1]\n\t            else:\n\t                unique_trials = list(range(len(self.cfg.previous_results)))\n\t            objective_results = []\n", "            for idx, best_trial_id in enumerate(unique_trials):\n\t                if not self.cfg.trainer.only_testing:\n\t                    # find the metrics for which trial is best at\n\t                    best_at_metrics = [metric for i, metric in enumerate(self.cfg.trainer.valid_metrics) if\n\t                                       associated_trial[i] == best_trial_id]\n\t                    best_hp_params = [trial for trial in study.best_trials if trial.number == best_trial_id][0].params\n\t                elif best_trial_id != -1:\n\t                    best_hp_params = self.cfg.previous_results[idx].args\n\t                    best_at_metrics = self.cfg.previous_results[idx].metrics\n\t                else:\n", "                    best_at_metrics = self.cfg.trainer.test_metrics\n\t                    best_hp_params = self.cfg.args\n\t                if best_trial_id != -1:\n\t                    # log the best hyperparameters and metrics at which they are best\n\t                    best_at_metrics_str = ''.join(f'{metric}, ' for metric in best_at_metrics)\n\t                    best_at_metrics_str = best_at_metrics_str[:best_at_metrics_str.rfind(',')]\n\t                    test_metrics = ''.join(f'{metric}_' for metric in best_at_metrics)\n\t                    test_metrics = test_metrics[:test_metrics.rfind(',')]\n\t                    log.info(f'Using best hyperparameters for metrics {best_at_metrics_str}: ')\n\t                    if not self.cfg.trainer.only_testing:\n", "                        for hp_key, hp_val in best_hp_params.items():\n\t                            log.info(f'{hp_key} : {hp_val}')\n\t                    # assign hyperparameters for the metric optimised over\n\t                    if self.cfg.trainer.finetuning_new_dataset or self.cfg.trainer.same_init_hpo:\n\t                        args_to_overwrite = list(set(self.cfg.args.keys()).intersection(self.cfg.trainer.args_cant_finetune))\n\t                        saved_args = OmegaConf.load(f'ugle/configs/models/{self.cfg.model}/{self.cfg.model}_default.yaml')\n\t                        for k in args_to_overwrite:\n\t                            best_hp_params[k] = saved_args.args[k]\n\t                    self.cfg = utils.assign_test_params(self.cfg, best_hp_params)\n\t                # do testing\n", "                self.cfg.trainer.calc_time = False\n\t                log.debug('Retraining model')\n\t                validation_results = self.train(None, self.cfg.args, label, features, processed_data, validation_adjacency,\n\t                           processed_valid_data)\n\t                results = self.testing_loop(label, features, test_adjacency, processed_test_data,\n\t                                            self.cfg.trainer.test_metrics)\n\t                # log test results\n\t                right_order_results = [results[k] for k in self.cfg.trainer.test_metrics]\n\t                to_log_trial_values = ''.join(f'| {metric}: {right_order_results[i]} |' for i, metric in\n\t                                              enumerate(self.cfg.trainer.test_metrics))\n", "                log.info(f'Test results |{to_log_trial_values}|')\n\t                objective_results.append({'metrics': best_at_metrics,\n\t                                          'results': results,\n\t                                          'args': best_hp_params})\n\t                if self.cfg.trainer.save_validation:\n\t                    objective_results[-1]['validation_results'] = validation_results\n\t                # re init the args object for assign_test params\n\t                self.cfg.args = copy.deepcopy(self.cfg.hypersaved_args)\n\t                if self.cfg.trainer.save_model and self.cfg.trainer.model_resolution_metric in best_at_metrics:\n\t                    log.info(f'Saving best version of model for {best_at_metrics}')\n", "                    torch.save({\"model\": self.model.state_dict(),\n\t                                \"args\": best_hp_params},\n\t                               f\"{self.cfg.trainer.models_path}{self.cfg.model}_{test_metrics}.pt\")\n\t        return objective_results\n\t    def train(self, trial: Trial, args: DictConfig, label: np.ndarray, features: np.ndarray, processed_data: tuple,\n\t              validation_adjacency: np.ndarray, processed_valid_data: tuple, prune_params=None):\n\t        timings = np.zeros(2)\n\t        start = time.time()\n\t        # configuration of args if hyperparameter optimisation phase\n\t        if trial is not None:\n", "            log.info(f'Launching Trial {trial.number}')\n\t            # if finetuning or reusing init procedure then just use default architecture sizes\n\t            if self.cfg.trainer.finetuning_new_dataset or self.cfg.trainer.same_init_hpo:\n\t                args_to_overwrite = list(set(args.keys()).intersection(self.cfg.trainer.args_cant_finetune))\n\t                saved_args = OmegaConf.load(f'ugle/configs/models/{self.cfg.model}/{self.cfg.model}_default.yaml')\n\t                for k in args_to_overwrite:\n\t                    args[k] = saved_args.args[k]\n\t            self.cfg.args = ugle.utils.sample_hyperparameters(trial, args, prune_params)\n\t        # process model creation\n\t        processed_data = self.move_to_activedevice(processed_data)\n", "        self.training_preprocessing(self.cfg.args, processed_data)\n\t        self.model.train()\n\t        if self.cfg.trainer.finetuning_new_dataset:\n\t            log.info('Loading pretrained model')\n\t            self.model.load_state_dict(torch.load(f\"{self.cfg.trainer.models_path}{self.cfg.model}.pt\")['model'])\n\t            self.model.to(self.device)\n\t        if trial is not None:\n\t            if self.cfg.trainer.same_init_hpo and trial.number == 0:\n\t                log.info(\"Saving initilisation of parameters\")\n\t                torch.save(self.model.state_dict(), f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_init.pt\")\n", "            if self.cfg.trainer.same_init_hpo: \n\t                log.info(\"Loading same initilisation of parameters\")\n\t                self.model.load_state_dict(torch.load(f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_init.pt\"))\n\t                self.model.to(self.device)\n\t        # create training loop\n\t        self.progress_bar = alive_it(range(self.cfg.args.max_epoch))\n\t        best_so_far = np.zeros(len((self.cfg.trainer.valid_metrics)))\n\t        best_epochs = np.zeros(len((self.cfg.trainer.valid_metrics)), dtype=int)\n\t        best_so_far[self.cfg.trainer.valid_metrics.index('conductance')] = 1.\n\t        patience_waiting = np.zeros((len(self.cfg.trainer.valid_metrics)), dtype=int)\n", "        for self.current_epoch in self.progress_bar:\n\t            timings[0] += time.time() - start\n\t            start = time.time()\n\t            # check if validation time\n\t            if self.current_epoch % self.cfg.trainer.validate_every_nepochs == 0:\n\t                # put in validation mode \n\t                processed_data = self.move_to_cpudevice(processed_data)\n\t                results = self.testing_loop(label, features, validation_adjacency, processed_valid_data,\n\t                                    self.cfg.trainer.valid_metrics)\n\t                # put data back into training mode\n", "                processed_data = self.move_to_activedevice(processed_data)\n\t                # check better for each metric\n\t                for m, metric in enumerate(self.cfg.trainer.valid_metrics):\n\t                    if (results[metric] > best_so_far[m] and metric != 'conductance') or (results[metric] < best_so_far[m] and metric == 'conductance'):\n\t                        best_so_far[m] = results[metric]\n\t                        best_epochs[m] = self.current_epoch\n\t                        # save model for this metric\n\t                        torch.save({\"model\": self.model.state_dict(),\n\t                                    \"args\": self.cfg.args},\n\t                                    f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_{metric}.pt\")\n", "                        patience_waiting[m] = 0\n\t                    else:\n\t                        patience_waiting[m] += 1\n\t            else: \n\t                patience_waiting += 1\n\t            timings[1] += time.time() - start\n\t            start = time.time()\n\t            # compute training iteration \n\t            loss, data_returned = self.training_epoch_iter(self.cfg.args, processed_data)\n\t            if data_returned:\n", "                processed_data = data_returned\n\t            # optimise\n\t            for opt in self.optimizers:\n\t                opt.zero_grad()\n\t            loss.backward()\n\t            for opt in self.optimizers:\n\t                opt.step()\n\t            if self.scheduler:\n\t                self.scheduler.step()\n\t            # update progress bar\n", "            self.progress_bar.title = f'Training {self.cfg.model} on {self.cfg.dataset} ... Epoch {self.current_epoch}: Loss={loss.item():.4f}'\n\t            # if all metrics hit patience then end\n\t            if patience_waiting.all() >= self.cfg.args.patience:\n\t                log.info(f'Early stopping at epoch {self.current_epoch}!')\n\t                break\n\t        timings[0] += time.time() - start\n\t        start = time.time()    \n\t        # compute final validation \n\t        processed_data = self.move_to_cpudevice(processed_data)\n\t        return_results = {}\n", "        if self.cfg.trainer.save_validation:\n\t            valid_results = {}\n\t        for m, metric in enumerate(self.cfg.trainer.valid_metrics):\n\t            log.info(f'Best model for {metric} at epoch {best_epochs[m]}')\n\t            self.model.load_state_dict(torch.load(f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_{metric}.pt\")['model'])\n\t            self.model.to(self.device)\n\t            results = self.testing_loop(label, features, validation_adjacency, processed_valid_data,\n\t                                        self.cfg.trainer.valid_metrics)\n\t            return_results[metric] = results[metric]\n\t            if self.cfg.trainer.save_validation:\n", "                results = self.testing_loop(label, features, validation_adjacency, processed_valid_data,\n\t                                        self.cfg.trainer.test_metrics)\n\t                valid_results[metric] = results\n\t        timings[1] += time.time() - start\n\t        start = time.time()\n\t        log.info(f\"Time Training {round(timings[0], 3)}s\")\n\t        log.info(f\"Time Validating {round(timings[1], 3)}s\")\n\t        if trial is None:\n\t            if not self.cfg.trainer.save_validation:\n\t                return\n", "            else:\n\t                return valid_results\n\t        else:\n\t            self.cfg.args = copy.deepcopy(self.cfg.hypersaved_args)\n\t            log_trial_result(trial, return_results, self.cfg.trainer.valid_metrics, self.cfg.trainer.multi_objective_study)\n\t            if not self.cfg.trainer.multi_objective_study:\n\t                return return_results[self.cfg.trainer.valid_metrics[0]]\n\t            else:\n\t                right_order_results = [return_results[k] for k in self.cfg.trainer.valid_metrics]\n\t                return tuple(right_order_results)\n", "    def testing_loop(self, label: np.ndarray, features: np.ndarray, adjacency: np.ndarray, processed_data: tuple, eval_metrics: ListConfig):\n\t        \"\"\"\n\t        testing loop which processes the testing data, run through the model to get predictions\n\t        evaluate those predictions\n\t        \"\"\"\n\t        self.model.eval()\n\t        processed_data = self.move_to_activedevice(processed_data)\n\t        preds = self.test(processed_data)\n\t        processed_data = self.move_to_cpudevice(processed_data)\n\t        results, eval_preds = ugle.process.preds_eval(label,\n", "                                                      preds,\n\t                                                      sf=4,\n\t                                                      adj=adjacency,\n\t                                                      metrics=eval_metrics)\n\t        self.model.train()\n\t        return results\n\t    def preprocess_data(self, features: np.ndarray, adjacency: np.ndarray) -> tuple:\n\t        log.error('NO PROCESSING STEP IMPLEMENTED FOR MODEL')\n\t        pass\n\t    def test(self, processed_data: tuple) -> np.ndarray:\n", "        log.error('NO TESTING STEP IMPLEMENTED')\n\t        pass\n\t    def training_preprocessing(self, args: DictConfig, processed_data: tuple):\n\t        \"\"\"\n\t        preparation steps of training\n\t        \"\"\"\n\t        log.error('NO TRAINING PREPROCESSING PROCEDURE')\n\t        pass\n\t    def training_epoch_iter(self, args: DictConfig, processed_data: tuple):\n\t        \"\"\"\n", "        this is what happens inside the iteration of epochs\n\t        \"\"\"\n\t        log.error('NO TRAINING ITERATION LOOP')\n\t        pass\n\t    def move_to_cpudevice(self, data):\n\t        return tuple(databite.to(torch.device(\"cpu\"), non_blocking=True) if torch.is_tensor(databite) else databite for databite in data)\n\t    def move_to_activedevice(self, data):\n\t        return tuple(databite.to(self.device, non_blocking=True)  if torch.is_tensor(databite) else databite for databite in data)\n"]}
{"filename": "ugle/models/selfgnn.py", "chunked_list": ["# https://github.com/zekarias-tilahun/SelfGNN\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom fast_pytorch_kmeans import KMeans\n\timport scipy.sparse as sp\n\tfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv\n\tfrom functools import wraps\n\timport copy\n\timport ugle\n", "from ugle.trainer import ugleTrainer\n\tclass EMA:\n\t    def __init__(self, beta):\n\t        super().__init__()\n\t        self.beta = beta\n\t    def update_average(self, old, new):\n\t        if old is None:\n\t            return new\n\t        return old * self.beta + (1 - self.beta) * new\n\tdef loss_fn(x, y):\n", "    x = F.normalize(x, dim=-1, p=2)\n\t    y = F.normalize(y, dim=-1, p=2)\n\t    return 2 - 2 * (x * y).sum(dim=-1)\n\tdef singleton(cache_key):\n\t    def inner_fn(fn):\n\t        @wraps(fn)\n\t        def wrapper(self, *args, **kwargs):\n\t            instance = getattr(self, cache_key)\n\t            if instance is not None:\n\t                return instance\n", "            instance = fn(self, *args, **kwargs)\n\t            setattr(self, cache_key, instance)\n\t            return instance\n\t        return wrapper\n\t    return inner_fn\n\tdef update_moving_average(ema_updater, ma_model, current_model):\n\t    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n\t        old_weight, up_weight = ma_params.data, current_params.data\n\t        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n\tdef set_requires_grad(model, val):\n", "    for p in model.parameters():\n\t        p.requires_grad = val\n\tclass Normalize(nn.Module):\n\t    def __init__(self, dim=None, method=\"batch\"):\n\t        super().__init__()\n\t        method = None if dim is None else method\n\t        if method == \"batch\":\n\t            self.norm = nn.BatchNorm1d(dim)\n\t        elif method == \"layer\":\n\t            self.norm = nn.LayerNorm(dim)\n", "        else:  # No norm => identity\n\t            self.norm = lambda x: x\n\t    def forward(self, x):\n\t        return self.norm(x)\n\tclass Encoder(nn.Module):\n\t    def __init__(self, args, layers, heads, dropout=None):\n\t        super().__init__()\n\t        rep_dim = layers[-1]\n\t        self.gnn_type = args.gnn_type\n\t        self.dropout = dropout\n", "        self.project = args.prj_head_norm\n\t        self.stacked_gnn = get_encoder(args.gnn_type, layers, heads, args.concat)\n\t        self.encoder_norm = Normalize(dim=rep_dim, method=args.encoder_norm)\n\t        if self.project != \"no\":\n\t            self.projection_head = nn.Sequential(\n\t                nn.Linear(rep_dim, rep_dim),\n\t                Normalize(dim=rep_dim, method=args.prj_head_norm),\n\t                nn.ReLU(inplace=True), nn.Dropout(dropout))\n\t    def forward(self, x, edge_index, edge_weight=None):\n\t        for i, gnn in enumerate(self.stacked_gnn):\n", "            if self.gnn_type == \"gat\" or self.gnn_type == \"sage\":\n\t                x = gnn(x, edge_index)\n\t            else:\n\t                x = gnn(x, edge_index, edge_weight=edge_weight)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.encoder_norm(x)\n\t        return x, (self.projection_head(x) if self.project != \"no\" else None)\n\tclass SelfGNN(nn.Module):\n\t    def __init__(self, args, layers, heads, dropout=0.0, moving_average_decay=0.99):\n\t        super().__init__()\n", "        self.student_encoder = Encoder(args, layers=layers, heads=heads, dropout=dropout)\n\t        self.teacher_encoder = None\n\t        self.teacher_ema_updater = EMA(moving_average_decay)\n\t        rep_dim = layers[-1]\n\t        self.student_predictor = nn.Sequential(\n\t            nn.Linear(rep_dim, rep_dim),\n\t            Normalize(dim=rep_dim, method=args.prd_head_norm),\n\t            nn.ReLU(inplace=True),\n\t            nn.Dropout(dropout))\n\t    @singleton('teacher_encoder')\n", "    def _get_teacher_encoder(self):\n\t        teacher_encoder = copy.deepcopy(self.student_encoder)\n\t        set_requires_grad(teacher_encoder, False)\n\t        return teacher_encoder\n\t    def reset_moving_average(self):\n\t        del self.teacher_encoder\n\t        self.teacher_encoder = None\n\t    def update_moving_average(self):\n\t        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n\t        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n", "    def encode(self, x, edge_index, edge_weight=None, encoder=None):\n\t        encoder = self.student_encoder if encoder is None else encoder\n\t        encoder.train(self.training)\n\t        return encoder(x, edge_index, edge_weight)\n\t    def forward(self, x1, x2, edge_index_v1, edge_index_v2, edge_weight_v1=None, edge_weight_v2=None):\n\t        \"\"\"\n\t        Apply student network on both views\n\t        v<x>_rep is the output of the stacked GNN\n\t        v<x>_student is the output of the student projection head, if used, otherwise is just a reference to v<x>_rep\n\t        \"\"\"\n", "        v1_enc = self.encode(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n\t        v1_rep, v1_student = v1_enc if v1_enc[1] is not None else (v1_enc[0], v1_enc[0])\n\t        v2_enc = self.encode(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\t        v2_rep, v2_student = v2_enc if v2_enc[1] is not None else (v2_enc[0], v2_enc[0])\n\t        \"\"\"\n\t        Apply the student predictor both views using the outputs from the previous phase \n\t        (after the stacked GNN or projection head - if there is one)\n\t        \"\"\"\n\t        v1_pred = self.student_predictor(v1_student)\n\t        v2_pred = self.student_predictor(v2_student)\n", "        \"\"\"\n\t        Apply the same procedure on the teacher network as in the student network except the predictor.\n\t        \"\"\"\n\t        with torch.no_grad():\n\t            teacher_encoder = self._get_teacher_encoder()\n\t            v1_enc = self.encode(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1, encoder=teacher_encoder)\n\t            v1_teacher = v1_enc[1] if v1_enc[1] is not None else v1_enc[0]\n\t            v2_enc = self.encode(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2, encoder=teacher_encoder)\n\t            v2_teacher = v2_enc[1] if v2_enc[1] is not None else v2_enc[0]\n\t        \"\"\"\n", "        Compute symmetric loss (once based on view1 (v1) as input to the student and then using view2 (v2))\n\t        \"\"\"\n\t        loss1 = loss_fn(v1_pred, v2_teacher.detach())\n\t        loss2 = loss_fn(v2_pred, v1_teacher.detach())\n\t        loss = loss1 + loss2\n\t        return v1_rep, v2_rep, loss.mean()\n\tdef get_encoder(gnn_type, layers, heads, concat):\n\t    \"\"\"\n\t    Builds the GNN backbone as required\n\t    \"\"\"\n", "    if gnn_type == \"gcn\":\n\t        return nn.ModuleList([GCNConv(layers[i - 1], layers[i]) for i in range(1, len(layers))])\n\t    elif gnn_type == \"sage\":\n\t        return nn.ModuleList([SAGEConv(layers[i - 1], layers[i]) for i in range(1, len(layers))])\n\t    elif gnn_type == \"gat\":\n\t        return nn.ModuleList(\n\t            [GATConv(layers[i - 1], layers[i] // heads[i - 1], heads=heads[i - 1], concat=concat)\n\t             for i in range(1, len(layers))])\n\tclass selfgnn_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n", "        adjacency = sp.csr_matrix(adjacency)\n\t        augmentation = ugle.datasets.Augmentations(method=self.cfg.args.aug)\n\t        features, adjacency, aug_features, aug_adjacency = augmentation(features, adjacency)\n\t        features = torch.FloatTensor(features)\n\t        adjacency = torch.LongTensor(adjacency)\n\t        aug_features = torch.FloatTensor(aug_features)\n\t        aug_adjacency = torch.LongTensor(aug_adjacency)\n\t        diff = abs(aug_features.shape[1] - features.shape[1])\n\t        if diff > 0:\n\t            \"\"\"\n", "            Data augmentation on the features could lead to mismatch between the shape of the two views,\n\t            hence the smaller view should be padded with zero. (smaller_data is a reference, changes will\n\t            reflect on the original data)\n\t            \"\"\"\n\t            which_small = 'features' if features.shape[1] < aug_features.shape[1] else 'aug_features'\n\t            smaller_data = features if which_small == 'features' else aug_features\n\t            smaller_data = F.pad(smaller_data, pad=(0, diff))\n\t            if which_small == 'features':\n\t                features = smaller_data\n\t            else:\n", "                aug_features = smaller_data\n\t            features = F.normalize(features)\n\t            aug_features = F.normalize(aug_features)\n\t            self.cfg.args.n_features = features.shape[1]\n\t        return features, adjacency, aug_features, aug_adjacency\n\t    def training_preprocessing(self, args, processed_data):\n\t        layers = [args.n_features, args.layer1, args.layer2]\n\t        heads = [args.head1, args.head2]\n\t        self.model = SelfGNN(args=args, layers=layers, heads=heads).to(self.device)\n\t        optimizer = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n", "        self.optimizers = [optimizer]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        features, adjacency, aug_features, aug_adjacency = processed_data\n\t        v1_output, v2_output, loss = self.model(\n\t            x1=features, x2=aug_features, edge_index_v1=adjacency, edge_index_v2=aug_adjacency)\n\t        self.model.update_moving_average()\n\t        return loss, None\n\t    def test(self, processed_data):\n\t        features, adjacency, aug_features, aug_adjacency = processed_data\n", "        self.model.eval()\n\t        v1_output, v2_output, _ = self.model(\n\t            x1=features, x2=aug_features, edge_index_v1=adjacency,\n\t            edge_index_v2=aug_adjacency)\n\t        emb = torch.cat([v1_output, v2_output], dim=1).detach()\n\t        emb = emb.squeeze(0)\n\t        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n\t        preds = kmeans.fit_predict(emb).cpu().numpy()\n\t        return preds\n"]}
{"filename": "ugle/models/cagc.py", "chunked_list": ["# https://github.com/wangtong627/CAGC/\n\timport torch\n\timport torch.nn as nn\n\tfrom torch_geometric.nn import GATConv\n\tfrom ugle.trainer import ugleTrainer\n\timport numpy as np\n\tfrom sklearn import cluster\n\timport scipy.sparse as sp\n\tfrom scipy.sparse.linalg import svds\n\tfrom sklearn.preprocessing import normalize\n", "import torch.nn.functional as F\n\tdef knn_fast(X, k):\n\t    X = F.normalize(X, dim=1, p=2)\n\t    similarities = torch.mm(X, X.t())\n\t    vals, inds = similarities.topk(k=k + 1, dim=-1)\n\t    return inds\n\tdef sim(z1: torch.Tensor, z2: torch.Tensor):\n\t    z1 = F.normalize(z1)\n\t    z2 = F.normalize(z2)\n\t    return torch.mm(z1, z2.t())\n", "def semi_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float):\n\t    f = lambda x: torch.exp(x / tau)\n\t    refl_sim = f(sim(z1, z1))\n\t    between_sim = f(sim(z1, z2))\n\t    return -torch.log(\n\t        between_sim.diag()\n\t        / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n\tdef instanceloss(z1: torch.Tensor, z2: torch.Tensor, tau: float, mean: bool = True):\n\t    l1 = semi_loss(z1, z2, tau)\n\t    l2 = semi_loss(z2, z1, tau)\n", "    ret = (l1 + l2) * 0.5\n\t    ret = ret.mean() if mean else ret.sum()\n\t    return ret\n\tdef knbrsloss(H, k, n_nodes, tau_knbrs, device):\n\t    indices = knn_fast(H, k)\n\t    f = lambda x: torch.exp(x / tau_knbrs)\n\t    refl_sim = f(sim(H, H))\n\t    ind2 = indices[:, 1:]\n\t    V = torch.gather(refl_sim, 1, ind2)\n\t    ret = -torch.log(\n", "        V.sum(1) / (refl_sim.sum(1) - refl_sim.diag()))\n\t    ret = ret.mean()\n\t    return ret\n\tdef post_proC(C, K, d=6, alpha=8):\n\t    # C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n\t    C = 0.5 * (C + C.T)\n\t    r = d * K + 1\n\t    U, S, _ = svds(C, r, v0=np.ones(C.shape[0]))\n\t    U = U[:, ::-1]\n\t    S = np.sqrt(S[::-1])\n", "    S = np.diag(S)\n\t    U = U.dot(S)\n\t    U = normalize(U, norm='l2', axis=1)\n\t    Z = U.dot(U.T)\n\t    Z = Z * (Z > 0)\n\t    L = np.abs(Z ** alpha)\n\t    L = L / L.max()\n\t    L = 0.5 * (L + L.T)\n\t    spectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',\n\t                                          assign_labels='discretize')\n", "    spectral.fit(L)\n\t    grp = spectral.fit_predict(L) + 1\n\t    return grp, L\n\tdef thrC(C, ro):\n\t    if ro < 1:\n\t        N = C.shape[1]\n\t        Cp = np.zeros((N, N))\n\t        S = np.abs(np.sort(-np.abs(C), axis=0))\n\t        Ind = np.argsort(-np.abs(C), axis=0)\n\t        for i in range(N):\n", "            cL1 = np.sum(S[:, i]).astype(float)\n\t            stop = False\n\t            csum = 0\n\t            t = 0\n\t            while stop == False:\n\t                csum = csum + S[t, i]\n\t                if csum > ro * cL1:\n\t                    stop = True\n\t                    Cp[Ind[0:t + 1, i], i] = C[Ind[0:t + 1, i], i]\n\t                t = t + 1\n", "    else:\n\t        Cp = C\n\t    return Cp\n\tclass Encoder(nn.Module):\n\t    def __init__(self, in_channels: int, out_channels: int, activation, base_model, k: int = 2, skip=False):\n\t        super(Encoder, self).__init__()\n\t        self.base_model = base_model\n\t        assert k >= 2\n\t        self.k = k\n\t        self.skip = skip\n", "        if not self.skip:\n\t            self.conv = [base_model(in_channels, 2 * out_channels).jittable()]\n\t            for _ in range(1, k - 1):\n\t                self.conv.append(base_model(1 * out_channels, 1 * out_channels))\n\t            self.conv.append(base_model(2 * out_channels, out_channels))\n\t            self.conv = nn.ModuleList(self.conv)\n\t            self.activation = activation\n\t        else:\n\t            self.fc_skip = nn.Linear(in_channels, out_channels)\n\t            self.conv = [base_model(in_channels, out_channels)]\n", "            for _ in range(1, k):\n\t                self.conv.append(base_model(out_channels, out_channels))\n\t            self.conv = nn.ModuleList(self.conv)\n\t            self.activation = activation\n\t    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n\t        if not self.skip:\n\t            for i in range(self.k):\n\t                x = self.activation(self.conv[i](x, edge_index))\n\t            return x\n\t        else:\n", "            h = self.activation(self.conv[0](x, edge_index))\n\t            hs = [self.fc_skip(x), h]\n\t            for i in range(1, self.k):\n\t                u = sum(hs)\n\t                hs.append(self.activation(self.conv[i](u, edge_index)))\n\t            return hs[-1]\n\tclass Decoder(nn.Module):\n\t    def __init__(self, in_channels: int, out_channels: int, activation, base_model, k: int = 2, skip=False):\n\t        super(Decoder, self).__init__()\n\t        self.base_model = base_model\n", "        assert k >= 2\n\t        self.k = k\n\t        self.skip = skip\n\t        if not self.skip:\n\t            self.conv = [base_model(in_channels, 2 * in_channels).jittable()]\n\t            for _ in range(1, k - 1):\n\t                self.conv.append(base_model(1 * in_channels, 1 * in_channels))\n\t            self.conv.append(base_model(2 * in_channels, out_channels))\n\t            self.conv = nn.ModuleList(self.conv)\n\t            self.activation = activation\n", "        else:\n\t            self.fc_skip = nn.Linear(in_channels, out_channels)\n\t            self.conv = [base_model(in_channels, in_channels)]\n\t            for _ in range(1, k - 1):\n\t                self.conv = [base_model(in_channels, in_channels)]\n\t            self.conv.append(base_model(in_channels, out_channels))\n\t            self.conv = nn.ModuleList(self.conv)\n\t            self.activation = activation\n\t    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n\t        if not self.skip:\n", "            for i in range(self.k):\n\t                x = self.activation(self.conv[i](x, edge_index))\n\t            return x\n\t        else:\n\t            h = self.activation(self.conv[0](x, edge_index))\n\t            hs = [self.fc_skip(x), h]\n\t            for i in range(1, self.k):\n\t                u = sum(hs)\n\t                hs.append(self.activation(self.conv[i](u, edge_index)))\n\t            return hs[-1]\n", "class Model(nn.Module):\n\t    def __init__(self, encoder: Encoder, decoder: Decoder, num_sample: int, device):\n\t        super(Model, self).__init__()\n\t        self.device = device\n\t        self.n = num_sample\n\t        self.encoder: Encoder = encoder\n\t        self.decoder: Decoder = decoder\n\t        self.Coefficient = nn.Parameter(1.0e-8 * torch.ones(self.n, self.n, dtype=torch.float32),\n\t                                        requires_grad=True).to(self.device)\n\t    def forward(self, x, edge_index):\n", "        # self expression layer, reshape to vectors, multiply Coefficient, then reshape back\n\t        H = self.encoder(x, edge_index)\n\t        CH = torch.matmul(self.Coefficient, H)\n\t        X_ = self.decoder(CH, edge_index)\n\t        return H, CH, self.Coefficient, X_\n\tclass cagc_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        adj_label = sp.coo_matrix(adjacency)\n\t        adj_label = adj_label.todok()\n\t        outwards = [i[0] for i in adj_label.keys()]\n", "        inwards = [i[1] for i in adj_label.keys()]\n\t        adj = torch.from_numpy(np.array([outwards, inwards], dtype=np.int))\n\t        data = torch.FloatTensor(features)\n\t        self.cfg.args.alpha = max(0.4 - (self.cfg.args.n_clusters - 1) / 10 * 0.1, 0.1)\n\t        self.cfg.hypersaved_args.alpha = self.cfg.args.alpha\n\t        return data, adj\n\t    def training_preprocessing(self, args, processed_data):\n\t        activation = nn.PReLU()\n\t        encoder = Encoder(args.n_features, args.num_hidden, activation,\n\t                          base_model=GATConv, k=args.num_layers).to(self.device)\n", "        decoder = Decoder(args.num_hidden, args.n_features, activation,\n\t                          base_model=GATConv, k=args.num_layers).to(self.device)\n\t        self.model = Model(encoder, decoder, args.n_nodes, self.device).to(self.device)\n\t        optimizer = torch.optim.Adam(\n\t            self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        self.optimizers = [optimizer]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        data, adj = processed_data\n\t        H, CH, Coefficient, X_ = self.model(data, adj)\n", "        loss_knbrs = knbrsloss(H, 10, args.n_nodes, args.tau_knbrs, self.device)\n\t        rec_loss = torch.sum(torch.pow(data - X_, 2))\n\t        loss_instance = instanceloss(H, CH, args.tau)\n\t        loss_coef = torch.sum(torch.pow(Coefficient, 2))\n\t        loss = (args.loss_instance * loss_instance) + (args.loss_knbrs * loss_knbrs) + (args.loss_coef * loss_coef) \\\n\t               + (args.rec_loss * rec_loss)\n\t        return loss, None\n\t    def test(self, processed_data):\n\t        data, adj = processed_data\n\t        self.model.eval()\n", "        _ , _, Coefficient, _ = self.model(data, adj)\n\t        # get C\n\t        C = Coefficient.detach().to('cpu').numpy()\n\t        commonZ = thrC(C, self.cfg.args.alpha)\n\t        preds, _ = post_proC(commonZ, self.cfg.args.n_clusters)\n\t        return preds\n"]}
{"filename": "ugle/models/dmon.py", "chunked_list": ["# https://github.com/google-research/google-research/blob/master/graph_embedding/dmon/dmon.py\n\timport ugle\n\timport scipy.sparse as sp\n\tfrom ugle.trainer import ugleTrainer\n\timport torch.nn as nn\n\timport torch\n\tfrom ugle.gnn_architecture import GCN\n\timport math\n\tfrom collections import OrderedDict\n\tclass DMoN(nn.Module):\n", "    def __init__(self,\n\t                 args,\n\t                 act='selu',\n\t                 do_unpooling=False):\n\t        \"\"\"Initializes the layer with specified parameters.\"\"\"\n\t        super(DMoN, self).__init__()\n\t        self.args = args\n\t        self.n_clusters = args.n_clusters\n\t        self.orthogonality_regularization = args.orthogonality_regularization\n\t        self.cluster_size_regularization = args.cluster_size_regularization\n", "        self.dropout_rate = args.dropout_rate\n\t        self.do_unpooling = do_unpooling\n\t        self.gcn = GCN(args.n_features, args.architecture, act=act, skip=True)\n\t        self.transform = nn.Sequential(OrderedDict([\n\t            ('layer1', nn.Linear(args.architecture, args.n_clusters)),\n\t            ('dropout', nn.Dropout(args.dropout_rate)),\n\t        ]))\n\t        def init_weights(m):\n\t            if isinstance(m, nn.Linear):\n\t                nn.init.orthogonal_(m.weight.data, gain=math.sqrt(2))\n", "                if m.bias is not None:\n\t                    m.bias.data.fill_(0.0)\n\t        self.transform.apply(init_weights)\n\t        return\n\t    def forward(self, graph, graph_normalised, features, extra_loss=False):\n\t        gcn_out = self.gcn(features, graph_normalised, sparse=True, skip=True)\n\t        assignments = self.transform(gcn_out).squeeze(0)\n\t        assignments = nn.functional.softmax(assignments, dim=1)\n\t        n_edges = graph._nnz()\n\t        degrees = torch.sparse.sum(graph, dim=0)._values().unsqueeze(1)\n", "        graph_pooled = torch.spmm(torch.spmm(graph, assignments).T, assignments)\n\t        normalizer_left = torch.spmm(assignments.T, degrees)\n\t        normalizer_right = torch.spmm(assignments.T, degrees).T\n\t        normalizer = torch.spmm(normalizer_left, normalizer_right) / 2 / n_edges\n\t        spectral_loss = - torch.trace(graph_pooled - normalizer) / 2 / n_edges\n\t        loss = spectral_loss\n\t        if extra_loss:\n\t            pairwise = torch.spmm(assignments.T, assignments)\n\t            identity = torch.eye(self.n_clusters).to(pairwise.device)\n\t            orthogonality_loss = torch.norm(pairwise / torch.norm(pairwise) -\n", "                                         identity / math.sqrt(float(self.n_clusters)))\n\t            orthogonality_loss *= self.orthogonality_regularization\n\t            loss += orthogonality_loss\n\t            cluster_loss = torch.norm(torch.sum(pairwise, dim=1)) / self.args.n_nodes * math.sqrt(float(self.n_clusters)) - 1\n\t            cluster_loss *= self.cluster_size_regularization\n\t            loss += cluster_loss\n\t        else:\n\t            cluster_sizes = torch.sum(assignments, dim=0)\n\t            cluster_loss = torch.norm(cluster_sizes) / self.args.n_nodes * math.sqrt(float(self.n_clusters)) - 1\n\t            cluster_loss *= self.cluster_size_regularization\n", "            loss += cluster_loss\n\t        return loss\n\t    def embed(self, graph, graph_normalised, features):\n\t        gcn_out = self.gcn(features, graph_normalised, sparse=True)\n\t        assignments = self.transform(gcn_out).squeeze(0)\n\t        assignments = nn.functional.softmax(assignments, dim=1)\n\t        return assignments\n\tclass dmon_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        features = torch.FloatTensor(features)\n", "        adjacency = adjacency + sp.eye(adjacency.shape[0])\n\t        adj_label = adjacency.copy()\n\t        adjacency = ugle.process.normalize_adj(adjacency)\n\t        graph_normalised = ugle.process.sparse_mx_to_torch_sparse_tensor(adjacency)\n\t        adj_label = sp.coo_matrix(adj_label)\n\t        graph = ugle.process.sparse_mx_to_torch_sparse_tensor(adj_label)\n\t        return graph, graph_normalised, features\n\t    def training_preprocessing(self, args, processed_data):\n\t        self.model = DMoN(args).to(self.device)\n\t        optimiser = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n", "        self.optimizers = [optimiser]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        graph, graph_normalised, features = processed_data\n\t        loss = self.model(graph, graph_normalised, features)\n\t        return loss, None\n\t    def test(self, processed_data):\n\t        graph, graph_normalised, features = processed_data\n\t        with torch.no_grad():\n\t            assignments = self.model.embed(graph, graph_normalised, features)\n", "            preds = assignments.detach().cpu().numpy().argmax(axis=1)\n\t        return preds\n"]}
{"filename": "ugle/models/sublime.py", "chunked_list": ["# https://github.com/GRAND-Lab/SUBLIME\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn import Sequential, Linear, ReLU\n\timport torch.nn.functional as F\n\timport numpy as np\n\timport copy\n\tfrom fast_pytorch_kmeans import KMeans\n\tfrom ugle.trainer import ugleTrainer\n\tEOS = 1e-10\n", "class GCNConv_dense(nn.Module):\n\t    def __init__(self, input_size, output_size):\n\t        super(GCNConv_dense, self).__init__()\n\t        self.linear = nn.Linear(input_size, output_size)\n\t    def init_para(self):\n\t        self.linear.reset_parameters()\n\t    def forward(self, input, A, sparse=False):\n\t        hidden = self.linear(input)\n\t        output = torch.matmul(A, hidden)\n\t        return output\n", "class GCN(nn.Module):\n\t    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, dropout_adj, Adj, sparse):\n\t        super(GCN, self).__init__()\n\t        self.layers = nn.ModuleList()\n\t        self.layers.append(GCNConv_dense(in_channels, hidden_channels))\n\t        for i in range(num_layers - 2):\n\t            self.layers.append(GCNConv_dense(hidden_channels, hidden_channels))\n\t        self.layers.append(GCNConv_dense(hidden_channels, out_channels))\n\t        self.dropout = dropout\n\t        self.dropout_adj_p = dropout_adj\n", "        self.Adj = Adj\n\t        self.Adj.requires_grad = False\n\t        self.dropout_adj = nn.Dropout(p=dropout_adj)\n\t    def forward(self, x):\n\t        Adj = self.dropout_adj(self.Adj)\n\t        for i, conv in enumerate(self.layers[:-1]):\n\t            x = conv(x, Adj)\n\t            x = F.relu(x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.layers[-1](x, Adj)\n", "        return x\n\tclass GraphEncoder(nn.Module):\n\t    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj):\n\t        super(GraphEncoder, self).__init__()\n\t        self.dropout = dropout\n\t        self.dropout_adj_p = dropout_adj\n\t        self.gnn_encoder_layers = nn.ModuleList()\n\t        self.gnn_encoder_layers.append(GCNConv_dense(in_dim, hidden_dim))\n\t        for _ in range(nlayers - 2):\n\t            self.gnn_encoder_layers.append(GCNConv_dense(hidden_dim, hidden_dim))\n", "        self.gnn_encoder_layers.append(GCNConv_dense(hidden_dim, emb_dim))\n\t        self.dropout_adj = nn.Dropout(p=dropout_adj)\n\t        self.proj_head = Sequential(Linear(emb_dim, proj_dim), ReLU(inplace=True),\n\t                                    Linear(proj_dim, proj_dim))\n\t    def forward(self, x, Adj_, branch=None):\n\t        Adj = self.dropout_adj(Adj_)\n\t        for conv in self.gnn_encoder_layers[:-1]:\n\t            x = conv(x, Adj)\n\t            x = F.relu(x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n", "        x = self.gnn_encoder_layers[-1](x, Adj)\n\t        z = self.proj_head(x)\n\t        return z, x\n\tclass GCL(nn.Module):\n\t    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj, sparse=False):\n\t        super(GCL, self).__init__()\n\t        self.encoder = GraphEncoder(nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj)\n\t    def forward(self, x, Adj_, branch=None):\n\t        z, embedding = self.encoder(x, Adj_, branch)\n\t        return z, embedding\n", "    @staticmethod\n\t    def calc_loss(x, x_aug, temperature=0.2, sym=True):\n\t        batch_size, _ = x.size()\n\t        x_abs = x.norm(dim=1)\n\t        x_aug_abs = x_aug.norm(dim=1)\n\t        sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / torch.einsum('i,j->ij', x_abs, x_aug_abs)\n\t        sim_matrix = torch.exp(sim_matrix / temperature)\n\t        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n\t        if sym:\n\t            loss_0 = pos_sim / (sim_matrix.sum(dim=0) - pos_sim)\n", "            loss_1 = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n\t            loss_0 = - torch.log(loss_0).mean()\n\t            loss_1 = - torch.log(loss_1).mean()\n\t            loss = (loss_0 + loss_1) / 2.0\n\t            return loss\n\t        else:\n\t            loss_1 = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n\t            loss_1 = - torch.log(loss_1).mean()\n\t            return loss_1\n\tclass MLP_learner(nn.Module):\n", "    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, act):\n\t        super(MLP_learner, self).__init__()\n\t        self.layers = nn.ModuleList()\n\t        if nlayers == 1:\n\t            self.layers.append(nn.Linear(isize, isize))\n\t        else:\n\t            self.layers.append(nn.Linear(isize, isize))\n\t            for _ in range(nlayers - 2):\n\t                self.layers.append(nn.Linear(isize, isize))\n\t            self.layers.append(nn.Linear(isize, isize))\n", "        self.input_dim = isize\n\t        self.output_dim = isize\n\t        self.k = k\n\t        self.knn_metric = knn_metric\n\t        self.non_linearity = 'relu'\n\t        self.param_init()\n\t        self.i = i\n\t        self.act = act\n\t    def internal_forward(self, h):\n\t        for i, layer in enumerate(self.layers):\n", "            h = layer(h)\n\t            if i != (len(self.layers) - 1):\n\t                if self.act == \"relu\":\n\t                    h = F.relu(h)\n\t                elif self.act == \"tanh\":\n\t                    h = F.tanh(h)\n\t        return h\n\t    def param_init(self):\n\t        for layer in self.layers:\n\t            layer.weight = nn.Parameter(torch.eye(self.input_dim))\n", "def cal_similarity_graph(node_embeddings):\n\t    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n\t    return similarity_graph\n\tdef apply_non_linearity(tensor, non_linearity, i):\n\t    if non_linearity == 'elu':\n\t        return F.elu(tensor * i - i) + 1\n\t    elif non_linearity == 'relu':\n\t        return F.relu(tensor)\n\t    elif non_linearity == 'none':\n\t        return tensor\n", "    else:\n\t        raise NameError('We dont support the non-linearity yet')\n\tdef symmetrize(adj):  # only for non-sparse\n\t    return (adj + adj.T) / 2\n\tdef normalize(adj, mode, sparse=False):\n\t    if mode == \"sym\":\n\t        inv_sqrt_degree = 1. / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)\n\t        return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]\n\t    elif mode == \"row\":\n\t        inv_degree = 1. / (adj.sum(dim=1, keepdim=False) + EOS)\n", "        return inv_degree[:, None] * adj\n\t    else:\n\t        exit(\"wrong norm mode\")\n\tdef split_batch(init_list, batch_size):\n\t    groups = zip(*(iter(init_list),) * batch_size)\n\t    end_list = [list(i) for i in groups]\n\t    count = len(init_list) % batch_size\n\t    end_list.append(init_list[-count:]) if count != 0 else end_list\n\t    return end_list\n\tdef get_feat_mask(features, mask_rate):\n", "    feat_node = features.shape[1]\n\t    mask = torch.zeros(features.shape)\n\t    samples = np.random.choice(feat_node, size=int(feat_node * mask_rate), replace=False)\n\t    mask[:, samples] = 1\n\t    return mask, samples\n\tclass sublime_trainer(ugleTrainer):\n\t    def knn_fast(self, X, k, b):\n\t        X = F.normalize(X, dim=1, p=2)\n\t        index = 0\n\t        values = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n", "        rows = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n\t        cols = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n\t        norm_row = torch.zeros(X.shape[0]).to(self.device)\n\t        norm_col = torch.zeros(X.shape[0]).to(self.device)\n\t        while index < X.shape[0]:\n\t            if (index + b) > (X.shape[0]):\n\t                end = X.shape[0]\n\t            else:\n\t                end = index + b\n\t            sub_tensor = X[index:index + b]\n", "            similarities = torch.mm(sub_tensor, X.t())\n\t            vals, inds = similarities.topk(k=k + 1, dim=-1)\n\t            values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n\t            cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n\t            rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end).view(-1, 1).repeat(1, k + 1).view(-1)\n\t            norm_row[index: end] = torch.sum(vals, dim=1)\n\t            norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n\t            index += b\n\t        norm = norm_row + norm_col\n\t        rows = rows.long()\n", "        cols = cols.long()\n\t        values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n\t        return rows, cols, values\n\t    def top_k(self, raw_graph, K):\n\t        values, indices = raw_graph.topk(k=int(K), dim=-1)\n\t        assert torch.max(indices) < raw_graph.shape[1]\n\t        mask = torch.zeros(raw_graph.shape).to(self.device)\n\t        mask[torch.arange(raw_graph.shape[0]).view(-1, 1), indices] = 1.\n\t        mask.requires_grad = False\n\t        sparse_graph = raw_graph * mask\n", "        return sparse_graph\n\t    def graph_learner_forward(self, features):\n\t        embeddings = self.graph_learner.internal_forward(features)\n\t        embeddings = F.normalize(embeddings, dim=1, p=2)\n\t        similarities = cal_similarity_graph(embeddings)\n\t        similarities = self.top_k(similarities, self.graph_learner.k + 1)\n\t        similarities = apply_non_linearity(similarities, self.graph_learner.non_linearity, self.graph_learner.i)\n\t        return similarities\n\t    def loss_gcl(self, model, features, anchor_adj):\n\t        # view 1: anchor graph\n", "        if self.cfg.args.maskfeat_rate_anchor:\n\t            mask_v1, _ = get_feat_mask(features, self.cfg.args.maskfeat_rate_anchor)\n\t            mask_v1 = mask_v1.to(self.device)\n\t            features_v1 = features * (1 - mask_v1)\n\t        else:\n\t            features_v1 = copy.deepcopy(features)\n\t        features_v1 = features_v1.to(self.device)\n\t        z1, _ = model(features_v1, anchor_adj, 'anchor')\n\t        # view 2: learned graph\n\t        if self.cfg.args.maskfeat_rate_learner:\n", "            mask, _ = get_feat_mask(features, self.cfg.args.maskfeat_rate_learner)\n\t            mask = mask.to(self.device)\n\t            features_v2 = features * (1 - mask)\n\t        else:\n\t            features_v2 = copy.deepcopy(features)\n\t        learned_adj = self.graph_learner_forward(features)\n\t        learned_adj = symmetrize(learned_adj)\n\t        learned_adj = normalize(learned_adj, 'sym')\n\t        z2, _ = model(features_v2, learned_adj, 'learner')\n\t        # compute loss\n", "        if self.cfg.args.contrast_batch_size:\n\t            node_idxs = list(range(self.cfg.agrs.n_nodes))\n\t            # random.shuffle(node_idxs)\n\t            batches = split_batch(node_idxs, self.cfg.args.contrast_batch_size)\n\t            loss = 0\n\t            for batch in batches:\n\t                weight = len(batch) / self.cfg.agrs.n_nodes\n\t                loss += model.calc_loss(z1[batch], z2[batch]) * weight\n\t        else:\n\t            loss = model.calc_loss(z1, z2)\n", "        return loss, learned_adj\n\t    def preprocess_data(self, features, adjacency):\n\t        anchor_adj_raw = torch.from_numpy(adjacency)\n\t        anchor_adj = normalize(anchor_adj_raw, 'sym')\n\t        features = torch.FloatTensor(features)\n\t        anchor_adj = anchor_adj.float()\n\t        return features, anchor_adj\n\t    def training_preprocessing(self, args, processed_data):\n\t        if args.learner_type == 'mlp':\n\t            self.graph_learner = MLP_learner(args.nlayers, args.n_features, args.k, args.sim_function, args.i, args.sparse,\n", "                                        args.activation_learner).to(self.device)\n\t        self.model = GCL(nlayers=args.nlayers, in_dim=args.n_features, hidden_dim=args.hidden_dim,\n\t                    emb_dim=args.rep_dim, proj_dim=args.proj_dim,\n\t                    dropout=args.dropout, dropout_adj=args.dropedge_rate).to(self.device)\n\t        optimizer_cl = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        optimizer_learner = torch.optim.Adam(self.graph_learner.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        self.optimizers = [optimizer_cl, optimizer_learner]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        features, anchor_adj = processed_data\n", "        loss, Adj = self.loss_gcl(self.model, features, anchor_adj)\n\t        # Structure Bootstrapping\n\t        if (1 - args.tau) and (args.c == 0 or self.current_epoch % args.c == 0):\n\t            anchor_adj = anchor_adj * args.tau + Adj.detach() * (1 - args.tau)\n\t        processed_data = (features, anchor_adj)\n\t        return loss, processed_data\n\t    def test(self, processed_data):\n\t        features, anchor_adj = processed_data\n\t        self.model.eval()\n\t        self.graph_learner.eval()\n", "        with torch.no_grad():\n\t            _, Adj = self.loss_gcl(self.model, features, anchor_adj)\n\t            _, embedding = self.model(features, Adj)\n\t            embedding = embedding.squeeze(0)\n\t            kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n\t            preds = kmeans.fit_predict(embedding).cpu().numpy()\n\t        return preds\n"]}
{"filename": "ugle/models/mvgrl.py", "chunked_list": ["# https://github.com/kavehhassani/mvgrl\n\timport torch\n\timport torch.nn as nn\n\timport ugle\n\timport scipy.sparse as sp\n\timport numpy as np\n\tfrom fast_pytorch_kmeans import KMeans\n\tfrom sklearn.preprocessing import MinMaxScaler\n\tfrom ugle.trainer import ugleTrainer\n\tfrom ugle.gnn_architecture import GCN, AvgReadout, mvgrl_Discriminator\n", "from ugle.process import sparse_mx_to_torch_sparse_tensor\n\tclass Model(nn.Module):\n\t    def __init__(self, n_in, n_h, act):\n\t        super(Model, self).__init__()\n\t        self.gcn1 = GCN(n_in, n_h, act)\n\t        self.gcn2 = GCN(n_in, n_h, act)\n\t        self.read = AvgReadout()\n\t        self.sigm = nn.Sigmoid()\n\t        self.disc = mvgrl_Discriminator(n_h)\n\t    def forward(self, seq1, seq2, adj, diff, sparse, msk, samp_bias1, samp_bias2):\n", "        h_1 = self.gcn1(seq1, adj, sparse)\n\t        c_1 = self.read(h_1, msk)\n\t        c_1 = self.sigm(c_1)\n\t        h_2 = self.gcn2(seq1, diff, sparse)\n\t        c_2 = self.read(h_2, msk)\n\t        c_2 = self.sigm(c_2)\n\t        h_3 = self.gcn1(seq2, adj, sparse)\n\t        h_4 = self.gcn2(seq2, diff, sparse)\n\t        ret = self.disc(c_1, c_2, h_1, h_2, h_3, h_4, samp_bias1, samp_bias2)\n\t        return ret, h_1, h_2\n", "    def embed(self, seq, adj, diff, sparse, msk):\n\t        h_1 = self.gcn1(seq, adj, sparse)\n\t        c = self.read(h_1, msk)\n\t        h_2 = self.gcn2(seq, diff, sparse)\n\t        return (h_1 + h_2).detach(), c.detach()\n\tclass mvgrl_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        epsilons = [1e-5, 1e-4, 1e-3, 1e-2]\n\t        diff_adj = ugle.process.compute_ppr(adjacency)\n\t        avg_degree = np.sum(adjacency) / adjacency.shape[0]\n", "        epsilon = epsilons[np.argmin([abs(avg_degree - np.argwhere(diff_adj >= e).shape[0] / diff_adj.shape[0])\n\t                                      for e in epsilons])]\n\t        diff_adj[diff_adj < epsilon] = 0.0\n\t        scaler = MinMaxScaler()\n\t        scaler.fit(diff_adj)\n\t        diff_adj = scaler.transform(diff_adj)\n\t        features = ugle.process.preprocess_features(features)\n\t        adjacency = ugle.process.normalize_adj(adjacency + sp.eye(adjacency.shape[0])).toarray()\n\t        return features, adjacency, diff_adj\n\t    def training_preprocessing(self, args, processed_data):\n", "        features, adj, diff_adj = processed_data\n\t        if adj.shape[-1] < args.sample_size:\n\t            args.sample_size = int(np.floor(adj.shape[-1] / 100.0) * 100)\n\t        self.model = Model(args.n_features, args.hid_units, args.activation).to(self.device)\n\t        optimizer = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        self.optimizers = [optimizer]\n\t        self.loss_function = nn.BCEWithLogitsLoss()\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        features, adj, diff_adj = processed_data\n", "        if adj.shape[-1] < self.cfg.args.sample_size:\n\t            self.cfg.args.sample_size = int(np.floor(adj.shape[-1] / 100.0) * 100)\n\t        lbl_1 = torch.ones(self.cfg.args.batch_size, self.cfg.args.sample_size * 2)\n\t        lbl_2 = torch.zeros(self.cfg.args.batch_size, self.cfg.args.sample_size * 2)\n\t        lbl = torch.cat((lbl_1, lbl_2), 1).to(self.device)\n\t        idx = np.random.randint(0, adj.shape[-1] - args.sample_size + 1, args.batch_size)\n\t        ba, bd, bf = [], [], []\n\t        for i in idx:\n\t            ba.append(adj[i: i + args.sample_size, i: i + args.sample_size])\n\t            bd.append(diff_adj[i: i + args.sample_size, i: i + args.sample_size])\n", "            bf.append(features[i: i + args.sample_size])\n\t        ba = np.asarray(ba).reshape(args.batch_size, args.sample_size, args.sample_size)\n\t        bd = np.array(bd).reshape(args.batch_size, args.sample_size, args.sample_size)\n\t        bf = np.array(bf).reshape(args.batch_size, args.sample_size, args.n_features)\n\t        if args.sparse:\n\t            ba = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(ba))\n\t            bd = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(bd))\n\t        else:\n\t            ba = torch.FloatTensor(ba)\n\t            bd = torch.FloatTensor(bd)\n", "        bf = torch.FloatTensor(bf)\n\t        idx = np.random.permutation(args.sample_size)\n\t        shuf_fts = bf[:, idx, :].to(self.device)\n\t        ba = ba.to(self.device)\n\t        bd = bd.to(self.device)\n\t        bf = bf.to(self.device)\n\t        logits, _, _ = self.model(bf, shuf_fts, ba, bd, args.sparse, None, None, None)\n\t        loss = self.loss_function(logits, lbl)\n\t        return loss, None\n\t    def test(self, processed_data):\n", "        features, adj, diff_adj = processed_data\n\t        if self.cfg.args.sparse:\n\t            adj = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(adj))\n\t            diff_adj = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(diff_adj))\n\t        features = torch.FloatTensor(features[np.newaxis]).to(self.device)\n\t        adj = torch.FloatTensor(adj[np.newaxis]).to(self.device)\n\t        diff_adj = torch.FloatTensor(diff_adj[np.newaxis]).to(self.device)\n\t        embeds, _ = self.model.embed(features, adj, diff_adj, self.cfg.args.sparse, None)\n\t        embeds = embeds.squeeze(0)\n\t        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n", "        preds = kmeans.fit_predict(embeds).cpu().numpy()\n\t        return preds\n"]}
{"filename": "ugle/models/__init__.py", "chunked_list": []}
{"filename": "ugle/models/daegc.py", "chunked_list": ["# code insipred from https://github.com/Tiger101010/DAEGC\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.nn.parameter import Parameter\n\tfrom torch.optim import Adam\n\tfrom fast_pytorch_kmeans import KMeans\n\timport scipy.sparse as sp\n\timport ugle\n", "from ugle.logger import log\n\tfrom ugle.trainer import ugleTrainer\n\tfrom ugle.gnn_architecture import GAT\n\tclass DAEGC(nn.Module):\n\t    def __init__(self, num_features, hidden_size, embedding_size, alpha, num_clusters, v=1):\n\t        super(DAEGC, self).__init__()\n\t        self.num_clusters = num_clusters\n\t        self.v = v\n\t        # get pretrain model\n\t        self.gat = GAT(num_features, hidden_size, embedding_size, alpha)\n", "        #self.gat.load_state_dict(torch.load(args.pretrain_path, map_location='cpu'))\n\t        # cluster layer\n\t        self.cluster_layer = Parameter(torch.Tensor(num_clusters, embedding_size))\n\t        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n\t    def forward(self, x, adj, M):\n\t        A_pred, z = self.gat(x, adj, M)\n\t        q = self.get_Q(z)\n\t        return A_pred, z, q\n\t    def get_Q(self, z):\n\t        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n", "        q = q.pow((self.v + 1.0) / 2.0)\n\t        q = (q.t() / torch.sum(q, 1)).t()\n\t        return q\n\tdef target_distribution(q):\n\t    weight = q ** 2 / q.sum(0)\n\t    return (weight.t() / weight.sum(1)).t()\n\tdef get_M(adj):\n\t    adj_numpy = adj.todense()\n\t    # t_order\n\t    t=2\n", "    row_l1_norms = np.linalg.norm(adj_numpy, ord=1, axis=1)\n\t    tran_prob = adj_numpy / row_l1_norms[:, None]\n\t    M_numpy = sum([np.linalg.matrix_power(tran_prob, i) for i in range(1, t + 1)]) / t\n\t    return torch.Tensor(M_numpy)\n\tclass daegc_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        adjacency = adjacency + sp.eye(adjacency.shape[0])\n\t        adj_label = adjacency.copy()\n\t        adjacency = ugle.process.normalize_adj(adjacency)\n\t        M = get_M(adjacency)\n", "        adj = torch.FloatTensor(adjacency.todense())\n\t        adj_label = torch.FloatTensor(adj_label)\n\t        features = torch.FloatTensor(features)\n\t        features[features != 0.] = 1.\n\t        return features, adj, adj_label, M\n\t    def training_preprocessing(self, args, processed_data):\n\t        features, adj, adj_label, M = processed_data\n\t        log.debug('creating model')\n\t        self.model = DAEGC(num_features=args.n_features, hidden_size=args.hidden_size,\n\t                      embedding_size=args.embedding_size, alpha=args.alpha, num_clusters=args.n_clusters).to(\n", "            self.device)\n\t        optimizer = Adam(self.model.gat.parameters(), lr=args.pre_lr, weight_decay=args.weight_decay)\n\t        log.debug('pretraining')\n\t        best_nmi = 0.\n\t        for pre_epoch in range(args.pre_epoch):\n\t            # training pass\n\t            self.model.train()\n\t            A_pred, z = self.model.gat(features, adj, M)\n\t            loss = F.binary_cross_entropy(A_pred.view(-1), adj_label.view(-1))\n\t            optimizer.zero_grad()\n", "            loss.backward()\n\t            optimizer.step()\n\t        log.debug('kmeans init estimate')\n\t        with torch.no_grad():\n\t            _, z = self.model.gat(features, adj, M)\n\t        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n\t        _ = kmeans.fit_predict(z).cpu().numpy()\n\t        self.model.cluster_layer.data = kmeans.centroids.clone().detach().to(self.device)\n\t        log.debug('model training')\n\t        optimizer = Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n", "        self.optimizers = [optimizer]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        if len(processed_data) == 4:\n\t            features, adj, adj_label, M = processed_data\n\t        else:\n\t            features, adj, adj_label, M, Q = processed_data\n\t        if self.current_epoch % args.update_interval == 0:\n\t            # update_interval\n\t            A_pred, z, Q = self.model(features, adj, M)\n", "            q = Q.detach().data.cpu().numpy().argmax(1)\n\t        A_pred, z, q = self.model(features, adj, M)\n\t        p = target_distribution(Q.detach())\n\t        kl_loss = F.kl_div(q.log(), p, reduction='batchmean')\n\t        re_loss = F.binary_cross_entropy(A_pred.view(-1), adj_label.view(-1))\n\t        loss = (args.kl_loss_const * kl_loss) + re_loss\n\t        processed_data = (features, adj, adj_label, M, Q)\n\t        return loss, processed_data\n\t    def test(self, processed_data):\n\t        with torch.no_grad():\n", "            features, adj, adj_label, M = processed_data\n\t            _, z, Q = self.model(features, adj, M)\n\t            preds = Q.detach().data.cpu().numpy().argmax(1)\n\t        return preds"]}
{"filename": "ugle/models/dgi.py", "chunked_list": ["# inspired by https://github.com/PetarV-/DGI\n\timport numpy as np\n\timport scipy.sparse as sp\n\timport torch\n\timport torch.nn as nn\n\tfrom fast_pytorch_kmeans import KMeans\n\timport ugle\n\tfrom ugle.trainer import ugleTrainer\n\tfrom ugle.gnn_architecture import GCN, AvgReadout, Discriminator\n\tclass DGI(nn.Module):\n", "    def __init__(self, n_in, n_h, activation):\n\t        super(DGI, self).__init__()\n\t        self.gcn = GCN(n_in, n_h, activation)\n\t        self.read = AvgReadout()\n\t        self.sigm = nn.Sigmoid()\n\t        self.disc = Discriminator(n_h)\n\t    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2):\n\t        h_1 = self.gcn(seq1, adj, sparse=True)\n\t        c = self.read(h_1, msk)\n\t        c = self.sigm(c)\n", "        h_2 = self.gcn(seq2, adj, sparse=True)\n\t        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n\t        return ret\n\t    # Detach the return variables\n\t    def embed(self, seq, adj, msk):\n\t        h_1 = self.gcn(seq, adj, sparse=True)\n\t        c = self.read(h_1, msk)\n\t        return h_1.detach(), c.detach()\n\tclass dgi_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n", "        adjacency = adjacency + sp.eye(adjacency.shape[0])\n\t        adjacency = ugle.process.normalize_adj(adjacency)\n\t        adj = ugle.process.sparse_mx_to_torch_sparse_tensor(adjacency)\n\t        features = ugle.process.preprocess_features(features)\n\t        features = torch.FloatTensor(features[np.newaxis])\n\t        return adj, features\n\t    def training_preprocessing(self, args, processed_data):\n\t        self.model = DGI(args.n_features, args.hid_units, args.nonlinearity).to(self.device)\n\t        optimiser = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        self.loss_function = nn.BCEWithLogitsLoss()\n", "        self.optimizers = [optimiser]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        adj, features = processed_data\n\t        idx = np.random.permutation(args.n_nodes)\n\t        shuf_fts = features[:, idx, :]\n\t        lbl_1 = torch.ones(args.batch_size, args.n_nodes)\n\t        lbl_2 = torch.zeros(args.batch_size, args.n_nodes)\n\t        lbl = torch.cat((lbl_1, lbl_2), 1)\n\t        shuf_fts = shuf_fts.to(self.device)\n", "        lbl = lbl.to(self.device)\n\t        logits = self.model(features, shuf_fts, adj, None, None, None)\n\t        loss = self.loss_function(logits, lbl)\n\t        return loss, None\n\t    def test(self, processed_data):\n\t        adj, features = processed_data\n\t        with torch.no_grad():\n\t            embeds, _ = self.model.embed(features, adj, None)\n\t            embeds = embeds.squeeze(0)\n\t        kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n", "        preds = kmeans.fit_predict(embeds).cpu().numpy()\n\t        return preds\n"]}
{"filename": "ugle/models/grace.py", "chunked_list": ["# https://github.com/CRIPAC-DIG/GRACE\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport ugle\n\timport scipy.sparse as sp\n\timport numpy as np\n\tfrom fast_pytorch_kmeans import KMeans\n\tfrom torch_geometric.utils import dropout_adj\n\tfrom torch_geometric.nn import GCNConv\n", "from ugle.trainer import ugleTrainer\n\tclass Encoder(torch.nn.Module):\n\t    def __init__(self, in_channels: int, out_channels: int, activation,\n\t                 base_model, k: int = 2):\n\t        super(Encoder, self).__init__()\n\t        self.base_model = base_model\n\t        assert k >= 2\n\t        self.k = k\n\t        self.conv = [base_model(in_channels, 2 * out_channels)]\n\t        for _ in range(1, k-1):\n", "            self.conv.append(base_model(2 * out_channels, 2 * out_channels))\n\t        self.conv.append(base_model(2 * out_channels, out_channels))\n\t        self.conv = nn.ModuleList(self.conv)\n\t        self.activation = activation\n\t    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n\t        for i in range(self.k):\n\t            x = self.activation(self.conv[i](x, edge_index))\n\t        return x\n\tclass Model(torch.nn.Module):\n\t    def __init__(self, encoder: Encoder, num_hidden: int, num_proj_hidden: int,\n", "                 tau: float = 0.5):\n\t        super(Model, self).__init__()\n\t        self.encoder: Encoder = encoder\n\t        self.tau: float = tau\n\t        self.fc1 = torch.nn.Linear(num_hidden, num_proj_hidden)\n\t        self.fc2 = torch.nn.Linear(num_proj_hidden, num_hidden)\n\t    def forward(self, x: torch.Tensor,\n\t                edge_index: torch.Tensor) -> torch.Tensor:\n\t        return self.encoder(x, edge_index)\n\t    def projection(self, z: torch.Tensor) -> torch.Tensor:\n", "        z = F.elu(self.fc1(z))\n\t        return self.fc2(z)\n\t    def sim(self, z1: torch.Tensor, z2: torch.Tensor):\n\t        z1 = F.normalize(z1)\n\t        z2 = F.normalize(z2)\n\t        return torch.mm(z1, z2.t())\n\t    def semi_loss(self, z1: torch.Tensor, z2: torch.Tensor):\n\t        f = lambda x: torch.exp(x / self.tau)\n\t        refl_sim = f(self.sim(z1, z1))\n\t        between_sim = f(self.sim(z1, z2))\n", "        return -torch.log(\n\t            between_sim.diag()\n\t            / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n\t    def batched_semi_loss(self, z1: torch.Tensor, z2: torch.Tensor,\n\t                          batch_size: int):\n\t        # Space complexity: O(BN) (semi_loss: O(N^2))\n\t        device = z1.device\n\t        num_nodes = z1.size(0)\n\t        num_batches = (num_nodes - 1) // batch_size + 1\n\t        f = lambda x: torch.exp(x / self.tau)\n", "        indices = torch.arange(0, num_nodes).to(device)\n\t        losses = []\n\t        for i in range(num_batches):\n\t            mask = indices[i * batch_size:(i + 1) * batch_size]\n\t            refl_sim = f(self.sim(z1[mask], z1))  # [B, N]\n\t            between_sim = f(self.sim(z1[mask], z2))  # [B, N]\n\t            losses.append(-torch.log(\n\t                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n\t                / (refl_sim.sum(1) + between_sim.sum(1)\n\t                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n", "        return torch.cat(losses)\n\t    def loss(self, z1: torch.Tensor, z2: torch.Tensor,\n\t             mean: bool = True, batch_size: int = 0):\n\t        h1 = self.projection(z1)\n\t        h2 = self.projection(z2)\n\t        if batch_size == 0:\n\t            l1 = self.semi_loss(h1, h2)\n\t            l2 = self.semi_loss(h2, h1)\n\t        else:\n\t            l1 = self.batched_semi_loss(h1, h2, batch_size)\n", "            l2 = self.batched_semi_loss(h2, h1, batch_size)\n\t        ret = (l1 + l2) * 0.5\n\t        ret = ret.mean() if mean else ret.sum()\n\t        return ret\n\tdef drop_feature(x, drop_prob):\n\t    drop_mask = torch.empty(\n\t        (x.size(1), ),\n\t        dtype=torch.float32,\n\t        device=x.device).uniform_(0, 1) < drop_prob\n\t    x = x.clone()\n", "    x[:, drop_mask] = 0\n\t    return x\n\tclass grace_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        adj_label = sp.coo_matrix(adjacency)\n\t        adj_label = adj_label.todok()\n\t        outwards = [i[0] for i in adj_label.keys()]\n\t        inwards = [i[1] for i in adj_label.keys()]\n\t        adj = torch.from_numpy(np.array([outwards, inwards], dtype=np.int))\n\t        data = torch.FloatTensor(features)\n", "        return data, adj\n\t    def training_preprocessing(self, args, processed_data):\n\t        activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[args.activation]\n\t        base_model = ({'GCNConv': GCNConv})[args.base_model]\n\t        encoder = Encoder(args.n_features, args.num_hidden, activation,\n\t                          base_model=base_model, k=args.num_layers).to(self.device)\n\t        self.model = Model(encoder, args.num_hidden, args.num_proj_hidden, args.tau).to(self.device)\n\t        optimizer = torch.optim.Adam(\n\t            self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        self.optimizers = [optimizer]\n", "        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        data, adj = processed_data\n\t        adj_1 = dropout_adj(adj, p=args.drop_edge_rate_1, force_undirected=True)[0]\n\t        adj_2 = dropout_adj(adj, p=args.drop_edge_rate_2, force_undirected=True)[0]\n\t        x_1 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_1)\n\t        x_2 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_2)\n\t        z1 = self.model(x_1, adj_1)\n\t        z2 = self.model(x_2, adj_2)\n\t        loss = self.model.loss(z1, z2, batch_size=0)\n", "        return loss, None\n\t    def test(self, processed_data):\n\t        data, adj = processed_data\n\t        self.model.eval()\n\t        z = self.model(data, adj)\n\t        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n\t        preds = kmeans.fit_predict(z).cpu().numpy()\n\t        return preds\n"]}
{"filename": "ugle/models/bgrl.py", "chunked_list": ["# https://github.com/Namkyeong/BGRL_Pytorch\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport numpy as np\n\tfrom fast_pytorch_kmeans import KMeans\n\timport scipy.sparse as sp\n\tfrom torch_geometric.nn import GCNConv\n\timport copy\n\tfrom torch_geometric.utils import dropout_adj\n", "import ugle\n\tfrom ugle.trainer import ugleTrainer\n\tclass EMA:\n\t    def __init__(self, beta, epochs):\n\t        super().__init__()\n\t        self.beta = beta\n\t        self.step = 0\n\t        self.total_steps = epochs\n\t    def update_average(self, old, new):\n\t        if old is None:\n", "            return new\n\t        beta = 1 - (1 - self.beta) * (np.cos(np.pi * self.step / self.total_steps) + 1) / 2.0\n\t        self.step += 1\n\t        return old * beta + (1 - beta) * new\n\tdef loss_fn(x, y):\n\t    x = F.normalize(x, dim=-1, p=2)\n\t    y = F.normalize(y, dim=-1, p=2)\n\t    return 2 - 2 * (x * y).sum(dim=-1)\n\tdef update_moving_average(ema_updater, ma_model, current_model):\n\t    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n", "        old_weight, up_weight = ma_params.data, current_params.data\n\t        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n\tdef set_requires_grad(model, val):\n\t    for p in model.parameters():\n\t        p.requires_grad = val\n\tclass Encoder(nn.Module):\n\t    def __init__(self, layer_config, dropout=None, project=False, **kwargs):\n\t        super().__init__()\n\t        self.conv1 = GCNConv(layer_config[0], layer_config[1])\n\t        self.bn1 = nn.BatchNorm1d(layer_config[1], momentum=0.01)\n", "        self.prelu1 = nn.PReLU()\n\t        self.conv2 = GCNConv(layer_config[1], layer_config[2])\n\t        self.bn2 = nn.BatchNorm1d(layer_config[2], momentum=0.01)\n\t        self.prelu2 = nn.PReLU()\n\t    def forward(self, x, edge_index, edge_weight=None):\n\t        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n\t        x = self.prelu1(self.bn1(x))\n\t        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n\t        x = self.prelu2(self.bn2(x))\n\t        return x\n", "def init_weights(m):\n\t    if type(m) == nn.Linear:\n\t        torch.nn.init.xavier_uniform_(m.weight)\n\t        m.bias.data.fill_(0.01)\n\tclass BGRL(nn.Module):\n\t    def __init__(self, layer_config, pred_hid, dropout=0.0, moving_average_decay=0.99, epochs=1000, **kwargs):\n\t        super().__init__()\n\t        self.student_encoder = Encoder(layer_config=layer_config, dropout=dropout, **kwargs)\n\t        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n\t        set_requires_grad(self.teacher_encoder, False)\n", "        self.teacher_ema_updater = EMA(moving_average_decay, epochs)\n\t        rep_dim = layer_config[-1]\n\t        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, pred_hid), nn.PReLU(), nn.Linear(pred_hid, rep_dim))\n\t        self.student_predictor.apply(init_weights)\n\t    def reset_moving_average(self):\n\t        del self.teacher_encoder\n\t        self.teacher_encoder = None\n\t    def update_moving_average(self):\n\t        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n\t        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n", "    def forward(self, x1, x2, edge_index_v1, edge_index_v2, edge_weight_v1=None, edge_weight_v2=None):\n\t        v1_student = self.student_encoder(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n\t        v2_student = self.student_encoder(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\t        v1_pred = self.student_predictor(v1_student)\n\t        v2_pred = self.student_predictor(v2_student)\n\t        with torch.no_grad():\n\t            v1_teacher = self.teacher_encoder(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n\t            v2_teacher = self.teacher_encoder(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\t        loss1 = loss_fn(v1_pred, v2_teacher.detach())\n\t        loss2 = loss_fn(v2_pred, v1_teacher.detach())\n", "        loss = loss1 + loss2\n\t        return v1_student, v2_student, loss.mean()\n\tclass bgrl_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        adj_label = sp.coo_matrix(adjacency)\n\t        adj_label = adj_label.todok()\n\t        outwards = [i[0] for i in adj_label.keys()]\n\t        inwards = [i[1] for i in adj_label.keys()]\n\t        adj = torch.from_numpy(np.array([outwards, inwards], dtype=int))\n\t        data = torch.FloatTensor(features)\n", "        return data, adj\n\t    def training_preprocessing(self, args, processed_data):\n\t        layers = [args.n_features, args.layer1, args.layer2]\n\t        self.model = BGRL(layer_config=layers, pred_hid=args.pred_hidden,\n\t                     dropout=args.dropout, epochs=args.max_epoch).to(self.device)\n\t        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=args.learning_rate, weight_decay=1e-5)\n\t        if args.max_epoch == 1000:\n\t            args.max_epoch += 1\n\t        scheduler = lambda epoch: epoch / 1000 if epoch < 1000 else (1 + np.cos((epoch - 1000) * np.pi / (args.max_epoch - 1000))) * 0.5\n\t        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n", "        self.scheduler = scheduler\n\t        self.optimizers = [optimizer]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n\t        data, adj = processed_data\n\t        self.model.update_moving_average()\n\t        adj_1 = dropout_adj(adj, p=args.drop_edge_rate_1, force_undirected=True)[0]\n\t        adj_2 = dropout_adj(adj, p=args.drop_edge_rate_2, force_undirected=True)[0]\n\t        x_1 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_1)\n\t        x_2 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_2)\n", "        v1_output, v2_output, loss = self.model(\n\t            x1=x_1, x2=x_2, edge_index_v1=adj_1, edge_index_v2=adj_2,\n\t            edge_weight_v1=None, edge_weight_v2=None)\n\t        return loss, None\n\t    def test(self, processed_data):\n\t        data, adj = processed_data\n\t        self.model.eval()\n\t        v1_output, v2_output, _ = self.model(\n\t                x1=data, x2=data, edge_index_v1=adj, edge_index_v2=adj,\n\t            edge_weight_v1=None,\n", "            edge_weight_v2=None)\n\t        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n\t        preds = kmeans.fit_predict(v1_output).cpu().numpy()\n\t        return preds"]}
{"filename": "ugle/models/vgaer.py", "chunked_list": ["# https://github.com/qcydm/VGAER/tree/main/VGAER_codes\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.nn.parameter import Parameter\n\tfrom fast_pytorch_kmeans import KMeans\n\timport math\n\tfrom ugle.trainer import ugleTrainer\n\tfrom copy import deepcopy\n\tclass GraphConvolution(nn.Module):\n", "    \"\"\"\n\t    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n\t    \"\"\"\n\t    def __init__(self, in_features, out_features, bias=True, act=\"tanh\"):\n\t        super(GraphConvolution, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n\t        if act == \"tanh\":\n\t            self.act = nn.Tanh()\n", "        if bias:\n\t            self.bias = Parameter(torch.FloatTensor(out_features))\n\t        else:\n\t            self.register_parameter('bias', None)\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n\t        stdv = 1. / math.sqrt(self.weight.size(1))\n\t        self.weight.data.uniform_(-stdv, stdv)\n\t        if self.bias is not None:\n\t            self.bias.data.uniform_(-stdv, stdv)\n", "    def forward(self, adj, input):\n\t        support = torch.mm(input, self.weight)\n\t        output = torch.spmm(adj, support)\n\t        if self.bias is not None:\n\t            output += self.bias\n\t        return self.act(output)\n\t    def __repr__(self):\n\t        return self.__class__.__name__ + ' (' \\\n\t               + str(self.in_features) + ' -> ' \\\n\t               + str(self.out_features) + ')'\n", "class InnerProductDecoder(nn.Module):\n\t    \"\"\"Decoder for using inner product for prediction.\"\"\"\n\t    def __init__(self, dropout, act=torch.sigmoid):\n\t        super(InnerProductDecoder, self).__init__()\n\t        self.dropout = dropout\n\t        self.act = act\n\t    def forward(self, z):\n\t        z = F.dropout(z, self.dropout, training=self.training)\n\t        B_hat = z @ z.t()\n\t        B_hat = F.sigmoid(B_hat)\n", "        return B_hat\n\tclass VGAERModel(nn.Module):\n\t    def __init__(self, in_dim, hidden1_dim, hidden2_dim, device):\n\t        super(VGAERModel, self).__init__()\n\t        self.in_dim = in_dim\n\t        self.hidden1_dim = hidden1_dim\n\t        self.hidden2_dim = hidden2_dim\n\t        layers = [GraphConvolution(self.in_dim, self.hidden1_dim, act=\"tanh\"),\n\t                  GraphConvolution(self.hidden1_dim, self.hidden2_dim, act=\"tanh\"),\n\t                  GraphConvolution(self.hidden1_dim, self.hidden2_dim, act=\"tanh\")]\n", "        self.layers = nn.ModuleList(layers)\n\t        self.device = device\n\t    def encoder(self, a_hat, features):\n\t        h = self.layers[0](a_hat, features)\n\t        self.mean = self.layers[1](a_hat, h)\n\t        self.log_std = self.layers[2](a_hat, h)\n\t        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(self.device)\n\t        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(self.device)\n\t        return sampled_z\n\t    def decoder(self, z):\n", "        adj_rec = torch.sigmoid(torch.matmul(z, z.t()))\n\t        return adj_rec\n\t    def forward(self, a_hat, features):\n\t        z = self.encoder(a_hat, features)\n\t        adj_rec = self.decoder(z)\n\t        return adj_rec, z\n\tdef compute_loss_para(adj):\n\t    try: \n\t        pos_weight = ((adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum())\n\t        norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n", "        weight_mask = adj.view(-1) == 1\n\t        weight_tensor = torch.ones(weight_mask.size(0))\n\t        weight_tensor[weight_mask] = pos_weight\n\t    except:\n\t        weight_tensor = torch.ones_like(adj.view(-1))\n\t        norm = 1.\n\t    return weight_tensor, norm\n\tclass vgaer_trainer(ugleTrainer):\n\t    def preprocess_data(self, features, adjacency):\n\t        A = torch.FloatTensor(adjacency)\n", "        A[A != 0] = 1\n\t        A_orig_ten = deepcopy(A)\n\t        # compute B matrix\n\t        K = 1 / (A.sum().item()) * (A.sum(dim=1).reshape(A.shape[0], 1) @ A.sum(dim=1).reshape(1, A.shape[0]))\n\t        feats = A - K\n\t        # compute A_hat matrix\n\t        A = A + torch.eye(A.shape[0])\n\t        D = torch.diag(torch.pow(A.sum(dim=1), -0.5))  # D = D^-1/2\n\t        A_hat = D @ A @ D\n\t        weight_tensor, norm = compute_loss_para(A)\n", "        weight_tensor = weight_tensor\n\t        A_hat = A_hat\n\t        feats = feats\n\t        return A_orig_ten, A_hat, feats, weight_tensor, norm\n\t    def training_preprocessing(self, args, processed_data):\n\t        self.model = VGAERModel(args.n_nodes, args.hidden1, args.hidden2, self.device).to(self.device)\n\t        optimizer = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\t        self.optimizers = [optimizer]\n\t        return\n\t    def training_epoch_iter(self, args, processed_data):\n", "        A_orig_ten, A_hat, feats, weight_tensor, norm = processed_data\n\t        recovered = self.model.forward(A_hat, feats)\n\t        logits = recovered[0]\n\t        hidemb = recovered[1]\n\t        logits = logits.clamp(min=0., max=1.)\n\t        loss = norm * F.binary_cross_entropy(logits.view(-1), A_orig_ten.view(-1), weight=weight_tensor)\n\t        kl_divergence = 0.5 / logits.size(0) * (\n\t                1 + 2 * self.model.log_std - self.model.mean ** 2 - torch.exp(self.model.log_std) ** 2).sum(\n\t            1).mean()\n\t        loss -= kl_divergence\n", "        return loss, None\n\t    def test(self, processed_data):\n\t        A_orig_ten, A_hat, feats, weight_tensor, norm = processed_data\n\t        self.model.eval()\n\t        recovered = self.model.forward(A_hat, feats)\n\t        emb = recovered[1]\n\t        emb = emb.float().clamp(torch.finfo(torch.float32).min, torch.finfo(torch.float32).max)\n\t        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n\t        preds = kmeans.fit_predict(emb).cpu().numpy()\n\t        return preds"]}
