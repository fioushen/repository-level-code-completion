{"filename": "setup.py", "chunked_list": ["from setuptools import setup\n\tsetup(\n\t    name='nninfo',\n\t    packages=['nninfo'],\n\t    version='1.0',\n\t    description='Deep Neural Network Information Toolkit',\n\t    classifiers=[\n\t        \"Programming Language :: Python\",\n\t        \"Programming Language :: Python :: 3\",\n\t        \"Development Status :: 5 - Production/Stable\",\n", "        \"Operating System :: POSIX :: Linux\",\n\t        \"Intended Audience :: Science/Research\",\n\t        \"Environment :: Console\",\n\t        \"Environment :: Other Environment\",\n\t        \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n\t        \"Topic :: Scientific/Engineering :: Physics\",\n\t        \"Topic :: Scientific/Engineering :: Information Analysis\",\n\t    ]\n\t)"]}
{"filename": "nninfo/experiment.py", "chunked_list": ["import os\n\timport re\n\tfrom pathlib import Path\n\timport numpy as np\n\timport torch\n\timport torch.utils.data\n\timport nninfo\n\tfrom nninfo.data_set import DataSet\n\tfrom nninfo.trainer import Trainer\n\tfrom nninfo.schedule import Schedule\n", "from nninfo.model.neural_network import NeuralNetwork, NeuronID, NoisyNeuralNetwork\n\tfrom nninfo.tasks.task_manager import TaskManager\n\tfrom nninfo.tester import Tester\n\tfrom nninfo.file_io import FileManager, CheckpointManager\n\tlog = nninfo.logger.get_logger(__name__)\n\tCONFIG_FILE_NAME = \"config.yaml\"\n\tclass Experiment:\n\t    \"\"\"\n\t    Manages the entire experiment, is directly in contact with the user script but also\n\t    the main components.\n", "    After connecting the components, user script should use preferably methods of this class.\n\t    1) is given an instance of TaskManager to feed data into the program and split the dataset\n\t       if necessary\n\t    2) is given an instance of NeuralNetwork in model.py that will be trained and tested on\n\t    3) is given an instance of Trainer that is responsible\n\t       for each chapter (predefined set of training epochs)\n\t       of the training process (gets data from TaskManager via dataset_name).\n\t    4) is given an instance of Tester that is called after each chunk of training is done\n\t       (gets data from TaskManager)\n\t    5) can be given an instance of Schedule. This makes automating the experiment easier.\n", "    6) creates an instance of CheckpointManager that stores the main experiment parameters\n\t       for loading afterwards. This can be used to\n\t       a) analyze the training afterwards\n\t       b) resume training from last or earlier chapters\n\t    \"\"\"\n\t    def __init__(self,\n\t                 experiment_id: str,\n\t                 network: NeuralNetwork,\n\t                 task: TaskManager,\n\t                 trainer: Trainer,\n", "                 tester: Tester,\n\t                 schedule: Schedule,\n\t                 _load: bool = False):\n\t        \"\"\"Creates a new Experiment instance from the given components.\n\t        Args:\n\t            experiment_id (str): Unique identifier for the experiment.\n\t            network (NeuralNetwork): NeuralNetwork instance.\n\t            task (TaskManager): TaskManager instance.\n\t            trainer (Trainer): Trainer instance.\n\t            tester (Tester): Tester instance.\n", "            schedule (Schedule): Schedule instance.\n\t            _load (bool, optional): Internal flag only. For loading an experiment, use Experiment.load().\n\t        \"\"\"\n\t        self._experiment_id = experiment_id\n\t        self._run_id = 0\n\t        self._experiment_dir = self._find_experiment_dir(experiment_id)\n\t        if not _load:\n\t            if os.path.exists(self._experiment_dir):\n\t                raise FileExistsError(f\"Experiment directory {self._experiment_dir} already exists.\"\n\t                                      \"Please choose a different experiment_id or use Experiment.load(experiment_id).\")\n", "            standard_dir_maker = FileManager(\n\t                \"../experiments/\", write=True\n\t            )\n\t            self._experiment_dir = standard_dir_maker.make_experiment_dir(\n\t                experiment_id, overwrite=False)\n\t        nninfo.logger.add_exp_file_handler(self._experiment_dir)\n\t        log.info(f\"Starting exp_{experiment_id}\")\n\t        # create checkpoint_manager that saves checkpoints to _experiment_dir for the entire experiment\n\t        self.checkpoint_manager = CheckpointManager(self._experiment_dir / \"checkpoints\")\n\t        self.checkpoint_manager.parent = self\n", "        self._set_components(network, task, trainer, tester, schedule)\n\t    @staticmethod\n\t    def _find_experiment_dir(experiment_id: str):\n\t        \"\"\"Finds the experiment directory based on the experiment id.\n\t        Args:\n\t            experiment_id (str): Unique identifier for the experiment.\n\t        Returns:\n\t            str: Path to the experiment directory.\n\t        \"\"\"\n\t        return Path(__file__).parent.parent / \"experiments\" / f\"exp_{experiment_id}\"\n", "    @staticmethod\n\t    def load(exp_id: str):\n\t        \"\"\"Loads an experiment from a file.\n\t        Args:\n\t            exp_id (str): Unique identifier for the experiment.\n\t        Returns:\n\t            Experiment: Experiment instance.\n\t        \"\"\"\n\t        experiment_dir = Experiment._find_experiment_dir(exp_id)\n\t        return Experiment.load_file(experiment_dir / CONFIG_FILE_NAME)\n", "    @staticmethod\n\t    def load_file(path: Path):\n\t        \"\"\"Loads an experiment from a file.\n\t        Args:\n\t            path (Path): Path to the experiment file.\n\t        Returns:\n\t            Experiment: Experiment instance.\n\t        \"\"\"\n\t        file_manager = FileManager(path.parent, read=True)\n\t        config = file_manager.read(path.name)\n", "        return Experiment.from_config(config)\n\t    @staticmethod\n\t    def from_config(config):\n\t        \"\"\"Loads an experiment from a config dict.\n\t        Args:\n\t            config (dict): Dictionary containing the experiment configuration.\n\t        Returns:\n\t            Experiment: Experiment instance.\n\t        \"\"\"\n\t        experiment_id = config[\"experiment_id\"]\n", "        network = NeuralNetwork.from_config(config[\"network\"])\n\t        task = TaskManager.from_config(config[\"task\"])\n\t        trainer = Trainer.from_config(config[\"trainer\"])\n\t        tester = Tester.from_config(config[\"tester\"])\n\t        schedule = Schedule.from_config(config[\"schedule\"])\n\t        experiment = Experiment(experiment_id, network,\n\t                                task, trainer, tester, schedule, _load=True)\n\t        trainer.initialize_components()\n\t        experiment.load_last_checkpoint()\n\t        return experiment\n", "    def to_config(self):\n\t        \"\"\"Creates a config dictionary from the experiment.\n\t        Returns:\n\t            dict: Dictionary containing the experiment configuration.\n\t        \"\"\"\n\t        config = {\n\t            \"experiment_id\": self._experiment_id,\n\t            \"network\": self._network.to_config(),\n\t            \"task\": self._task.to_config(),\n\t            \"trainer\": self._trainer.to_config(),\n", "            \"tester\": self._tester.to_config(),\n\t            \"schedule\": self._schedule.to_config()\n\t        }\n\t        return config\n\t    def load_last_checkpoint(self):\n\t        \"\"\"Loads the last checkpoint of the experiment.\"\"\"\n\t        try:\n\t            last_run = self.checkpoint_manager.get_last_run()\n\t        except ValueError:\n\t            raise\n", "        try:\n\t            last_chapter = self.checkpoint_manager.get_last_chapter_in_run(last_run)\n\t        except ValueError:\n\t            raise\n\t        self.load_checkpoint(last_run, last_chapter)\n\t    def load_checkpoint(self, run_id, chapter_id):\n\t        \"\"\"Loads a checkpoint of the experiment.\n\t        Args:\n\t            run_id (int): Run id of the checkpoint.\n\t            chapter_id (int): Chapter id of the checkpoint.\n", "        \"\"\"\n\t        checkpoint = self.checkpoint_manager.get_checkpoint(run_id, chapter_id)\n\t        self.network.load_state_dict(checkpoint[\"model_state_dict\"])\n\t        self.trainer.load_optimizer_state_dict(\n\t            checkpoint[\"optimizer_state_dict\"]\n\t        )\n\t        self.trainer.set_n_chapters_trained(checkpoint[\"chapter_id\"])\n\t        self.trainer.set_n_epochs_trained(checkpoint[\"epoch_id\"])\n\t        torch.set_rng_state(checkpoint[\"torch_seed\"])\n\t        np.random.set_state(checkpoint[\"numpy_seed\"])\n", "        log.info(f\"Successfully loaded checkpoint {run_id}-{chapter_id}.\")\n\t        self._run_id = checkpoint[\"run_id\"]\n\t    def save_components(self):\n\t        component_saver = FileManager(\n\t            self._experiment_dir, write=True)\n\t        config = self.to_config()\n\t        component_saver.write(config, CONFIG_FILE_NAME)\n\t    def run_following_schedule(self, continue_run=False, chapter_ends=None, use_cuda=False, use_ipex=False, compute_test_loss=None):\n\t        if chapter_ends is None:\n\t            if self._schedule is None:\n", "                log.error(\n\t                    \"You can only use run_following_schedule if you have \"\n\t                    + \"a schedule connected to the experiment or pass a schedule.\"\n\t                )\n\t                return\n\t            else:\n\t                chapter_ends = self._schedule.chapter_ends\n\t        if continue_run:\n\t            log.warning(\n\t                \"Continuing run {} at chapter {}.\".format(\n", "                    self.run_id, self.chapter_id)\n\t            )\n\t        else:\n\t            if self.chapter_id != 0 or self.epoch_id != 0:\n\t                log.error(\n\t                    \"You can only use run_following_schedule if you reset the training to a new run.\"\n\t                )\n\t                return\n\t        info = \"Starting training on run {} starting at chapter {}, epoch {}\".format(\n\t            self.run_id, self.chapter_id, self.epoch_id\n", "        )\n\t        log.info(info)\n\t        print(info)\n\t        for c in range(self.chapter_id, len(chapter_ends) - 1):\n\t            if chapter_ends[c] != self.epoch_id:\n\t                log.error(\n\t                    \"Error on continuing schedule,\"\n\t                    + \" schedule.chapter_ends[{}]={}\".format(c, chapter_ends[c])\n\t                    + \" and experiment's self.epoch_id={} \".format(self.epoch_id)\n\t                    + \"do not fit together.\"\n", "                )\n\t                raise ValueError\n\t            # running c+1:\n\t            n_epochs_chapter = chapter_ends[c + 1] - chapter_ends[c]\n\t            self._trainer.train_chapter(\n\t                n_epochs_chapter=n_epochs_chapter, use_cuda=use_cuda, use_ipex=use_ipex, compute_test_loss=compute_test_loss)\n\t    def continue_runs_following_schedule(self, runs_id_list, stop_epoch, schedule=None, use_cuda=False, compute_test_loss=None):\n\t        if schedule is None:\n\t            if self._schedule is None:\n\t                log.error(\n", "                    \"You can only use run_following_schedule if you have \"\n\t                    + \"a schedule connected to the experiment or pass a schedule.\"\n\t                )\n\t                return\n\t            else:\n\t                schedule = self._schedule\n\t        cut_off = np.argmax(\n\t            np.array(schedule.chapter_ends_continued) > stop_epoch)\n\t        chapter_ends = schedule.chapter_ends_continued[:cut_off]\n\t        for run_id in runs_id_list:\n", "            last_chapter = self.checkpoint_manager.get_last_chapter_in_run(run_id)\n\t            self.load_checkpoint(run_id, last_chapter)\n\t            self.run_following_schedule(\n\t                continue_run=True, chapter_ends=chapter_ends, use_cuda=use_cuda, compute_test_loss=compute_test_loss)\n\t    def rerun(self, n_runs, like_run_id=None):\n\t        \"\"\"\n\t        Reruns the experiment for a given number of runs. For doing this, it uses\n\t        the checkpoints of a previous run that are found in the checkpoints directory\n\t        and produces the same checkpoints with a new network initialization.\n\t        Args:\n", "            n_runs (int): Number of additional runs that should be performed.\n\t            like_run_id (int): Run id of the run that should be replicated. If not set,\n\t                defaults to run_id=0.\n\t        \"\"\"\n\t        log.info(\"Setting up rerun of experiment: n_runs=\" + str(n_runs))\n\t        if like_run_id is None:\n\t            like_run_id = 0\n\t        ckpt_list = self.checkpoint_manager.list_checkpoints(run_ids=[\n\t                                                              like_run_id])\n\t        ckpt_list = [ckpt[1] for ckpt in ckpt_list]\n", "        epoch_list = [self.schedule.get_epoch_for_chapter(c) for c in ckpt_list]\n\t        log.warning(\"Extracted schedule: \" + str(epoch_list))\n\t        last_run_id = self.checkpoint_manager.get_last_run()\n\t        for i in range(n_runs):\n\t            # getting everything to the same state as requested\n\t            self.load_checkpoint(run_id=like_run_id, chapter_id=0)\n\t            # get the new run_id\n\t            self._run_id = last_run_id + 1\n\t            # reinitialize the network with a new seed\n\t            self._network.init_weights(randomize_seed=True)\n", "            self.run_following_schedule(chapter_ends=epoch_list)\n\t            last_run_id = self._run_id\n\t    def save_checkpoint(self):\n\t        \"\"\"\n\t        Calls the CheckpointManager to save the current state of the network and the optimizer\n\t        (is necessary for optimizers that depend on their own past)\n\t        together with the state of random number generators (of numpy and torch).\n\t        \"\"\"\n\t        self.checkpoint_manager.save()\n\t    def _set_components(self, network, task, trainer, tester, schedule):\n", "        self._network = network\n\t        self._network.parent = self\n\t        self._task = task\n\t        self._task.parent = self\n\t        self._trainer = trainer\n\t        self._trainer.parent = self\n\t        self._tester = tester\n\t        self._tester.parent = self\n\t        self._schedule = schedule\n\t        self._schedule.parent = self\n", "    @property\n\t    def all_key_components_connected(self):\n\t        \"\"\"\n\t        Property (function that is disguised as an object variable)\n\t        that checks whether all components for this experiment\n\t        are already in place. (For now, all are needed to start the experiment,\n\t        this could be changed in the future though, for example Test might not\n\t        be relevant for every experiment.)\n\t        Returns:\n\t             (bool): All components are connected, True or False.\n", "        \"\"\"\n\t        all_comp_flag = True\n\t        if self._task is None:\n\t            log.info(\"Task still missing.\")\n\t            all_comp_flag = False\n\t        if self._network is None:\n\t            log.info(\"Network still missing.\")\n\t            all_comp_flag = False\n\t        if self._trainer is None:\n\t            log.info(\"Trainer still missing.\")\n", "            all_comp_flag = False\n\t        if self._tester is None:\n\t            log.info(\"Tester still missing.\")\n\t            all_comp_flag = False\n\t        return all_comp_flag\n\t    def capture_activations(\n\t            self,\n\t            dataset,\n\t            run_id,\n\t            chapter_id,\n", "            repeat_dataset=1,\n\t            batch_size=10 ** 4,\n\t            before_noise=False,\n\t            condition=None,\n\t            quantizer_params=None,\n\t        ):\n\t        \"\"\"\n\t        Captures activations for current network state for the given dataset.\n\t        Returns:\n\t            Iterator that yields dictionaries with keys X, Y, L1, L2, ...\n", "                and ndarrays of size (batch_size, ) containing activations\n\t        \"\"\"\n\t        # Load checkpoint for given run and chapter\n\t        self.load_checkpoint(run_id, chapter_id)\n\t        assert isinstance(self.network, NoisyNeuralNetwork) or before_noise == False, 'Extraction after noise only possible for noisy network!'\n\t        if not isinstance(dataset, DataSet):\n\t            dataset = self.task[dataset]\n\t        if not condition is None:\n\t            dataset = dataset.condition(len(dataset), condition)\n\t        feeder = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n", "        self.network.eval()\n\t        for x, y in feeder:\n\t            for _ in range(repeat_dataset):\n\t                if isinstance(self.network, NoisyNeuralNetwork):\n\t                    act_dict = self.network.extract_activations(\n\t                        x, before_noise=before_noise, quantizer_params=quantizer_params\n\t                    )\n\t                else:\n\t                    act_dict = self.network.extract_activations(\n\t                        x, quantizer_params=quantizer_params\n", "                    )\n\t                act_dict[\"X\"] = x.detach().numpy()\n\t                act_dict[\"Y\"] = y.detach().numpy()\n\t                # Add decision function. Index of maximum value of output layer. If multiple output neurons have the same activation, choose the first!\n\t                act_dict[\"Yhat\"] = np.argmax(\n\t                    act_dict[\"L{}\".format(len(act_dict) - 2)], axis=1\n\t                )\n\t                # Reshape to neuron id dicts NeuronID->[activations]\n\t                act_dict = {\n\t                    NeuronID(layer_id, (neuron_idx + 1,)): act_dict[layer_id][:, neuron_idx]\n", "                    if act_dict[layer_id].ndim > 1\n\t                    else act_dict[layer_id]\n\t                    for layer_id in act_dict\n\t                    for neuron_idx in range(\n\t                        act_dict[layer_id].shape[1]\n\t                        if act_dict[layer_id].ndim > 1\n\t                        else 1\n\t                    )\n\t                }\n\t                yield act_dict \n", "    @property\n\t    def experiment_dir(self):\n\t        return self._experiment_dir\n\t    @property\n\t    def id(self):\n\t        return self._experiment_id\n\t    @property\n\t    def run_id(self):\n\t        return self._run_id\n\t    @property\n", "    def chapter_id(self):\n\t        return self.trainer.n_chapters_trained\n\t    @property\n\t    def epoch_id(self):\n\t        return self.trainer.n_epochs_trained\n\t    @property\n\t    def network(self):\n\t        return self._network\n\t    @property\n\t    def trainer(self):\n", "        return self._trainer\n\t    @property\n\t    def task(self):\n\t        return self._task\n\t    @property\n\t    def tester(self):\n\t        return self._tester\n\t    @property\n\t    def schedule(self):\n\t        return self._schedule\n"]}
{"filename": "nninfo/exp_comp.py", "chunked_list": ["from abc import ABC\n\timport nninfo\n\tlog = nninfo.logger.get_logger(__name__)\n\tclass ExperimentComponent(ABC):\n\t    \"\"\"\n\t    Abstract class that defines parent property for each component of the experiment\n\t    (experiment is then the parent of each component, if they are connected).\n\t    \"\"\"\n\t    def __init__(self):\n\t        self._parent = None\n", "        super().__init__()\n\t    @property\n\t    def parent(self):\n\t        return self._parent\n\t    @parent.setter\n\t    def parent(self, parent):\n\t        if self.parent is not None:\n\t            if parent is not None:\n\t                log.warning(\n\t                    \"Parent of {} is changed to experiment {}.\".format(\n", "                        type(self)),\n\t                    parent.id,\n\t                )\n\t            else:\n\t                log.info(\"Parent of {} is removed.\".format(type(self)))\n\t        self._parent = parent\n"]}
{"filename": "nninfo/plot.py", "chunked_list": ["from math import comb\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport nninfo\n\t_dataset_display_names = {\n\t    'full_set/train': 'Train Set', 'full_set/test': 'Test Set'}\n\t##### Performance plots #####\n\tdef plot_accuracy(performance, dataset_name, ax, **kwargs):\n\t    # Get maximum epoch and format broken axis\n", "    max_epoch_exponent = int(np.ceil(np.log10(performance['epoch_id'].max())))\n\t    nninfo.plot.format_figure_broken_axis(ax, max_exp=max_epoch_exponent)\n\t    dataset_display_name = _dataset_display_names.get(\n\t        dataset_name, dataset_name)\n\t    kwargs = kwargs.copy()\n\t    kwargs.setdefault('lw', 1)\n\t    kwargs.setdefault('label', f'{dataset_display_name} Accuracy')\n\t    # Plot loss\n\t    nninfo.plot.plot_mean_and_interval(performance, ax, x=('epoch_id', ''), y=(\n\t        dataset_name, 'accuracy'), **kwargs)\n", "def plot_loss(performance, dataset_name, ax, **kwargs):\n\t    # Get maximum epoch and format broken axis\n\t    max_epoch_exponent = int(np.ceil(np.log10(performance['epoch_id'].max())))\n\t    nninfo.plot.format_figure_broken_axis(ax, max_exp=max_epoch_exponent)\n\t    dataset_display_name = _dataset_display_names.get(\n\t        dataset_name, dataset_name)\n\t    kwargs = kwargs.copy()\n\t    kwargs.setdefault('lw', 1)\n\t    kwargs.setdefault('label', f'{dataset_display_name} Accuracy')\n\t    # Plot loss\n", "    nninfo.plot.plot_mean_and_interval(performance, ax, x=('epoch_id', ''), y=(\n\t        dataset_name, 'loss'), **kwargs)\n\tdef plot_loss_accuracy(performance, ax, ax2, dataset_names=['full_set/train', 'full_set/test'], **kwargs):\n\t    \"\"\"\n\t    Plot loss and accuracy.\n\t    \"\"\"\n\t    max_epoch_exponent = int(np.ceil(np.log10(performance['epoch_id'].max())))\n\t    nninfo.plot.format_figure_broken_axis(ax, max_exp=max_epoch_exponent)\n\t    kwargs = kwargs.copy()\n\t    kwargs.setdefault('lw', 1)\n", "    for dataset_name in dataset_names:\n\t        dataset_display_name = _dataset_display_names.get(\n\t            dataset_name, dataset_name)\n\t        # Plot accuracy\n\t        nninfo.plot.plot_mean_and_interval(performance, ax, x=('epoch_id', ''), y=(\n\t            dataset_name, 'accuracy'), label=f'{dataset_display_name} Accuracy', **kwargs)\n\t        ax.set_xlabel(\"Epochs\")\n\t        ax.set_ylabel(\"Accuracy\")\n\t        ax.plot([], label=f'{dataset_display_name} Loss')\n\t        # Plot loss\n", "        ax2.plot([])  # advance color cycle\n\t        nninfo.plot.plot_mean_and_interval(performance, ax2, x=(\n\t            'epoch_id', ''), y=(dataset_name, 'loss'), **kwargs)\n\t        ax2.set_xlabel(\"Epochs\")\n\t        ax2.set_ylabel(\"Loss\")\n\t        max_epoch = performance['epoch_id'].max()\n\t        print(f'Avg. {dataset_display_name} accuracy at epoch {max_epoch}:',\n\t              performance[performance['epoch_id'] ==\n\t                          max_epoch][(dataset_name, 'accuracy')].mean(),\n\t              '+-',\n", "              performance[performance['epoch_id'] == max_epoch][(dataset_name, 'accuracy')].std(ddof=1))\n\t###### PID PLOTS ######\n\tdef plot_representational_complexity(pid_summary: pd.DataFrame, ax: plt.Axes, quantile_level=0.95, use_median=False, **kwargs):\n\t    max_epoch_exponent = int(np.ceil(np.log10(pid_summary['epoch_id'].max())))\n\t    format_figure_broken_axis(ax, max_epoch_exponent)\n\t    plot_mean_and_interval(\n\t        pid_summary, ax, x='epoch_id', y='representational_complexity', quantile_level=quantile_level, use_median=use_median, **kwargs)\n\tdef plot_degree_of_synergy_atoms(pid_summary: pd.DataFrame, ax: plt.Axes, quantile_level=0.95, use_median=False, **kwargs):\n\t    max_epoch_exponent = int(np.ceil(np.log10(pid_summary['epoch_id'].max())))\n\t    format_figure_broken_axis(ax, max_epoch_exponent)\n", "    max_degree = pid_summary['degree_of_synergy_atoms'].columns.max()\n\t    for degree in range(1, max_degree + 1):\n\t        plot_mean_and_interval(\n\t            pid_summary, ax, x=pid_summary['epoch_id'], y=('degree_of_synergy_atoms', degree), quantile_level=quantile_level, use_median=use_median, label=f'Deg. of syn. {degree}', **kwargs)\n\tdef plot_reing_directed_differences(reing_results: pd.DataFrame, ax: plt.Axes, quantile_level=0.95, use_median=False, labels=None, **kwargs):\n\t    max_epoch_exponent = int(np.ceil(np.log10(reing_results['epoch_id'].max())))\n\t    format_figure_broken_axis(ax, max_epoch_exponent)\n\t    # Find all columns that match the patterh 'C(k||k+1)'\n\t    for reing_column_name in reing_results.columns[reing_results.columns.str.match('C\\(\\d+\\|\\|\\d+\\)')]:\n\t        label = reing_column_name if labels is None else labels[reing_column_name]\n", "        plot_mean_and_interval(\n\t            reing_results, ax, x=reing_results['epoch_id'], y=reing_column_name, quantile_level=quantile_level, use_median=use_median, label=reing_column_name, **kwargs)\n\tdef plot_reing_complexity(reing_results: pd.DataFrame, ax: plt.Axes, quantile_level=0.95, use_median=False, **kwargs):\n\t    max_epoch_exponent = int(np.ceil(np.log10(reing_results['epoch_id'].max())))\n\t    format_figure_broken_axis(ax, max_epoch_exponent)\n\t    plot_mean_and_interval(\n\t        reing_results, ax, x=reing_results['epoch_id'], y='reing_complexity', quantile_level=quantile_level, use_median=use_median, **kwargs)\n\t##### HELPER FUNCTIONS ######\n\tdef plot_mean_and_interval(df, ax, x=('epoch_id', ''), y='c', use_median=False, quantile_level=0.95, zorder_shift=0, **kwargs):\n\t    df_center = df.groupby(x).median(\n", "    ) if use_median else df.groupby(df.epoch_id).mean()\n\t    df_high = df.groupby(df.epoch_id).quantile(.5 - quantile_level / 2)\n\t    df_low = df.groupby(df.epoch_id).quantile(.5 + quantile_level / 2)\n\t    kwargs.setdefault('label', '')\n\t    line = ax.plot(df_center[y], zorder=1+zorder_shift,\n\t                   solid_capstyle='butt', **kwargs)\n\t    ax.fill_between(df_low.index, df_low[y].values,\n\t                    df_high[y].values, color=line[-1].get_color(), alpha=0.3, zorder=0+zorder_shift, linewidth=0)\n\tdef format_figure_broken_axis(ax, max_exp=4):\n\t    ax.set_xscale('symlog', linthresh=1, linscale=.6)\n", "    ax.set_xlim(0, 10**max_exp)\n\t    ax.set_xticks([0] + [10**i for i in range(max_exp+1)])\n\t    ax.set_xticklabels(['$0$', '$1$'] + ['' if i %\n\t                                         2 == 1 else f'$10^{i}$' for i in range(1, max_exp+1)])\n\t    # Broken axis\n\t    d = .01\n\t    broken_x = 0.07\n\t    breakspacing = 0.015\n\t    ax.plot((broken_x-breakspacing*0.9, broken_x+breakspacing*0.9), (0, 0),\n\t            color='w', transform=ax.transAxes, clip_on=False, linewidth=.8, zorder=3)\n", "    ax.plot((broken_x-breakspacing*0.9, broken_x+breakspacing*0.9), (1, 1),\n\t            color='w', transform=ax.transAxes, clip_on=False, linewidth=.8, zorder=3)\n\t    kwargs = dict(transform=ax.transAxes, color='k',\n\t                  clip_on=False, linewidth=.8, zorder=4)\n\t    ax.plot((broken_x-d-breakspacing, broken_x+d -\n\t             breakspacing), (-3*d, +3*d), **kwargs)\n\t    ax.plot((broken_x-d-breakspacing, broken_x+d -\n\t             breakspacing), (1-3*d, 1+3*d), **kwargs)\n\t    ax.plot((broken_x-d+breakspacing, broken_x+d +\n\t             breakspacing), (-3*d, +3*d), **kwargs)\n", "    ax.plot((broken_x-d+breakspacing, broken_x+d +\n\t             breakspacing), (1-3*d, 1+3*d), **kwargs)"]}
{"filename": "nninfo/config.py", "chunked_list": ["from os import cpu_count\n\timport os\n\t# set the maximum number of workers, typically to the number of cpus minus 1\n\tcpus = cpu_count()\n\tif cpus is None:\n\t    raise OSError\n\telif cpus <= 2:\n\t    N_WORKERS = cpus\n\telse:\n\t    N_WORKERS = cpu_count()\n", "# for cluster safe usage uncomment:\n\tCLUSTER_MODE = int(os.environ.get(\"CLUSTER_MODE\", 0)) == 1\n\tif CLUSTER_MODE:\n\t    N_WORKERS = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", 1))\n\tprint(f'{N_WORKERS=}')"]}
{"filename": "nninfo/file_io.py", "chunked_list": ["# you may not need all of these, if you know your data file types\n\t# and the way FileManager handles them.\n\timport os, re, glob\n\tfrom pathlib import Path\n\timport shutil\n\timport pickle\n\timport json\n\timport ast\n\timport copy\n\timport numpy as np\n", "import scipy.io as io\n\timport torch\n\timport yaml\n\timport pandas as pd\n\tfrom filelock import FileLock\n\timport nninfo\n\tfrom .exp_comp import ExperimentComponent\n\tFILELOCK_TIMEOUT = -1\n\tlog = nninfo.logger.get_logger(__name__)\n\tEXPERIMENT_DIR_STANDARD = {\n", "    \"experiment_dir\": \"exp_{:04d}/\",\n\t    \"experiment_dir_str\": \"exp_{}/\",\n\t    \"checkpoints\": \"checkpoints/\",\n\t    \"measurements\": \"measurements/\",\n\t    \"plots\": \"plots/\",\n\t    \"logs\": \"log/\",\n\t    \"components\": \"components/\",\n\t}\n\tFILENAME_STANDARD = {\n\t    \"checkpoint\": \"ckpt_r{:06d}_c{:06d}_e{:012d}.pt\",\n", "    \"checkpoint_loading\": \"ckpt_r{:06d}_c{:06d}_e*.pt\",\n\t    \"checkpoint_loading_all_runs\": \"ckpt_r*_c{:06d}_e*.pt\",\n\t    \"checkpoint_loading_all_chapters\": \"ckpt_r{:06d}_c*_e*.pt\",\n\t    \"measurement_history\": \"measurement_history.json\",\n\t    \"measurement_file\": \"meas_r{:06d}_c{:06d}_e{:012d}.jsonl\",\n\t}\n\tCHECKPOINT_FILE_NAME_PATTERN = \"ckpt_r*_c*_e*.pt\"\n\tclass NoAliasDumper(yaml.SafeDumper):\n\t    def ignore_aliases(self, data):\n\t        return True\n", "class CheckpointManager(ExperimentComponent):\n\t    \"\"\"\n\t    Implemented following this:\n\t    https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3\n\t    Allows for storing and reloading of the network state,\n\t    optimizer state and random number generator states.\n\t    \"\"\"\n\t    def __init__(self, checkpoint_dir):\n\t        super().__init__()\n\t        self._checkpoint_dir = checkpoint_dir\n", "    def save(self, filename=None):\n\t        state = {\n\t            \"chapter_id\": self.parent.chapter_id,\n\t            \"model_state_dict\": self.parent.network.state_dict(),\n\t            \"optimizer_state_dict\": self.parent.trainer.optimizer_state_dict(),\n\t            \"epoch_id\": self.parent.epoch_id,\n\t            \"torch_seed\": torch.get_rng_state(),\n\t            \"numpy_seed\": np.random.get_state(),\n\t            \"run_id\": self.parent.run_id,\n\t        }\n", "        \"\"\"\n\t        Saves the current state of the experiment as a checkpoint\n\t        in the experiments/expXXXX/checkpoints/\n\t        directory. Outputs a message if succeeded.\n\t        \"\"\"\n\t        if filename is None:\n\t            filename = FILENAME_STANDARD[\"checkpoint\"]\n\t        final_filename = filename.format(\n\t            self.parent.run_id, self.parent.chapter_id, self.parent.epoch_id\n\t        )\n", "        torch.save(state, self._checkpoint_dir / final_filename)\n\t        log.info(\n\t            \"Successfully saved current state of the training as {}.\".format(\n\t                final_filename\n\t            )\n\t        )\n\t    def get_checkpoint_filename(self, run_id, chapter_id):\n\t        \"\"\"Returns the filename of the checkpoint with the given run_id and chapter_id.\n\t        Args:\n\t            run_id (int): run_id of the checkpoint\n", "            chapter_id (int): chapter_id of the checkpoint\n\t        Returns:\n\t            str: filename of the checkpoint\n\t        \"\"\"\n\t        file_name = FILENAME_STANDARD[\"checkpoint_loading\"].format(run_id, chapter_id)\n\t        file_paths = list(self._checkpoint_dir.glob(file_name))\n\t        if len(file_paths) == 0:\n\t            raise FileNotFoundError(f\"Could not find file {file_name}\")\n\t        elif len(file_paths) > 1:\n\t            raise RuntimeError(f\"Found more than one file matching {file_name}\")\n", "        file_path = file_paths[0]\n\t        return file_path\n\t    def get_checkpoint(self, run_id, chapter_id):\n\t        \"\"\"Returns the checkpoint with the given run_id and chapter_id.\n\t        Args:\n\t            run_id (int): run_id of the checkpoint\n\t            chapter_id (int): chapter_id of the checkpoint\n\t        Returns:\n\t            dict: checkpoint\n\t        \"\"\"\n", "        file_name = self.get_checkpoint_filename(run_id, chapter_id)\n\t        return torch.load(file_name)\n\t    def read(self, filename):\n\t        if self._checkpoint_loader is None:\n\t            self.init_loader_saver()\n\t        return self._checkpoint_loader.read(filename)\n\t    def list_all_checkpoints(self):\n\t        \"\"\"Collects all checkpoint files in the checkpoints directory.\n\t        Returns:\n\t            list: list of tuples (run_id, chapter_id)\n", "        \"\"\"\n\t        # Get all checkpoint files that match the pattern\n\t        checkpoints = self._checkpoint_dir.glob(CHECKPOINT_FILE_NAME_PATTERN)\n\t        checkpoints = [checkpoint.name for checkpoint in checkpoints]\n\t        # extract run_id and chapter_id from filename\n\t        checkpoints = [\n\t            (\n\t                int(re.findall(r\"r(\\d+)\", checkpoint)[0]),\n\t                int(re.findall(r\"c(\\d+)\", checkpoint)[0]),\n\t            )\n", "            for checkpoint in checkpoints\n\t        ]\n\t        return sorted(checkpoints)\n\t    def list_checkpoints(self, run_ids=None, chapter_ids=None):\n\t        checkpoints = self.list_all_checkpoints()\n\t        if run_ids is not None:\n\t            if isinstance(run_ids, int):\n\t                run_ids = [run_ids]\n\t            checkpoints = [checkpoint for checkpoint in checkpoints if checkpoint[0] in run_ids]\n\t        if chapter_ids is not None:\n", "            if isinstance(chapter_ids, int):\n\t                chapter_ids = [chapter_ids]\n\t            checkpoints = [checkpoint for checkpoint in checkpoints if checkpoint[1] in chapter_ids]\n\t        return checkpoints\n\t    def get_run_ids(self):\n\t        \"\"\"Returns a sorted list of all runs that have checkpoints.\"\"\"\n\t        return sorted(list(set(checkpoint[0] for checkpoint in self.list_all_checkpoints())))\n\t    def get_chapter_ids(self, run_id):\n\t        \"\"\"Returns a sorted list of all chapters that have checkpoints.\"\"\"\n\t        return sorted(list(set(checkpoint[1] for checkpoint in self.list_all_checkpoints() if checkpoint[0] == run_id)))\n", "    def get_last_run(self):\n\t        \"\"\"Returns the last run that has checkpoints.\"\"\"\n\t        runs = self.get_run_ids()\n\t        if len(runs) == 0:\n\t            raise ValueError(\"No runs found.\")\n\t        return max(runs)\n\t    def get_last_chapter_in_run(self, run_id):\n\t        \"\"\"Returns the last chapter in a run that has a checkpoint.\"\"\"\n\t        checkpoints = self.list_checkpoints(run_ids=[run_id])\n\t        if len(checkpoints) == 0:\n", "            raise ValueError(\"No checkpoints found for run_id {}\".format(run_id))\n\t        return max(checkpoints, key=lambda x: x[1])[1]\n\tclass MeasurementManager:\n\t    \"\"\"\n\t    Allows for storing and reloading of the network state,\n\t    optimizer state and random number generator states.\n\t    \"\"\"\n\t    def __init__(self, experiment_dir: Path, measurement_subdir=\"measurements/\"):\n\t        self._measurement_saver = FileManager(\n\t            experiment_dir / measurement_subdir, write=True\n", "        )\n\t        self._measurement_loader = FileManager(\n\t            experiment_dir / measurement_subdir, read=True\n\t        )\n\t        self._history_dict = None\n\t        lock = FileLock(FILENAME_STANDARD[\"measurement_history\"]+\".lock\", timeout=FILELOCK_TIMEOUT)\n\t        with lock.acquire():\n\t            self.load_history()\n\t    def load_history(self):\n\t        try:\n", "            self._history_dict = self._measurement_loader.read(\n\t                \"measurement_history.json\"\n\t            )\n\t        except FileNotFoundError:\n\t            log.info(\n\t                \"Measurement history not found. Creating new measurement_history.json\"\n\t            )\n\t            self._measurement_saver.write(dict(), \"measurement_history.json\")\n\t            self._history_dict = self._measurement_loader.read(\n\t                \"measurement_history.json\"\n", "            )\n\t    def get_next_measurement_id(self):\n\t        # This function could also look into the settings of old measurements in order not to\n\t        # repeat parts of or full measurements. At the moment it just finds the maximum of the\n\t        # other ids though\n\t        try:\n\t            id = int(max(self._history_dict, key=int)) + 1\n\t        except Exception as e:\n\t            id = 0\n\t        return id\n", "    def save(self, measurement, measurement_id=None):\n\t        \"\"\"\n\t        Save a measurement by appending it to the right file.\n\t        \"\"\"\n\t        lock = FileLock(FILENAME_STANDARD[\"measurement_history\"]+\".lock\", timeout=FILELOCK_TIMEOUT)\n\t        with lock.acquire():\n\t            self.load_history()\n\t            if measurement_id is None:\n\t                measurement_id = self.get_next_measurement_id()\n\t            save_dict = measurement.get_measurement_dict(measurement_id)\n", "            run_id = save_dict[\"run_id\"]\n\t            chapter_id = save_dict[\"chapter_id\"]\n\t            epoch_id = save_dict[\"epoch_id\"]\n\t            type_str = save_dict[\"measurement_type\"]\n\t            if type_str == \"pid\":\n\t                file_prefix = \"active_\"\n\t            elif type_str == \"fisher\":\n\t                file_prefix = \"structural_\"\n\t            elif type_str == \"weight\":\n\t                file_prefix = \"structural_\"\n", "            elif type_str == \"mi\":\n\t                file_prefix = \"active_\"\n\t            else:\n\t                raise NotImplementedError\n\t            self._measurement_saver.append(\n\t                save_dict,\n\t                file_prefix\n\t                + FILENAME_STANDARD[\"measurement_file\"].format(\n\t                    run_id, chapter_id, epoch_id\n\t                ),\n", "            )\n\t            if measurement_id in self._history_dict:\n\t                self._history_dict[measurement_id].append((run_id, chapter_id, epoch_id))\n\t            else:\n\t                self._history_dict[measurement_id] = [(run_id, chapter_id, epoch_id)]\n\t            self._measurement_saver.write(\n\t                self._history_dict, FILENAME_STANDARD[\"measurement_history\"]\n\t            )\n\t        return measurement_id\n\t    def load(self, filename=\"*.jsonl\", **kwargs):\n", "        file_list = self._measurement_loader.list_files_in_dir(filename)\n\t        row_list = []\n\t        for f in file_list:\n\t            row_list.extend(self._measurement_loader.read(f, **kwargs))\n\t        return pd.DataFrame(row_list)\n\t    @property\n\t    def history_dict(self):\n\t        self.load_history()\n\t        return copy.deepcopy(self._history_dict)\n\tclass FileManager:\n", "    def __init__(self, rel_path, read=False, write=False):\n\t        if (read and write) or (not read and not write):\n\t            log.error(\"FileManager should either read or write in one directory.\")\n\t            raise AttributeError\n\t        self._read = read\n\t        self._write = write\n\t        module_dir = os.path.dirname(__file__)\n\t        self._rel_path = rel_path\n\t        self._path = os.path.join(module_dir, rel_path)\n\t        self.tuple_as_str = False\n", "    def list_subdirs_in_dir(self):\n\t        d = self._path\n\t        return [\n\t            os.path.join(d, o)\n\t            for o in os.listdir(d)\n\t            if os.path.isdir(os.path.join(d, o))\n\t        ]\n\t    def list_files_in_dir(self, filename=None):\n\t        if filename is not None:\n\t            filepath = os.path.join(self._path, filename)\n", "            names = [os.path.basename(x) for x in glob.glob(filepath)]\n\t            return names\n\t        return os.listdir(self._path)\n\t    def find_file(self, filename):\n\t        # here glob is used, because it is nicer for a filename that includes a '*'\n\t        filepath = os.path.join(self._path, filename)\n\t        filepath_list = glob.glob(filepath)\n\t        if len(filepath_list) == 1:\n\t            new_filepath = filepath_list[0]\n\t        elif len(filepath_list) > 1:\n", "            log.warning(\"Multiple files meet this criterion: {}\".format(filepath_list))\n\t            idx = input(\"Specify index (0,1..)\")\n\t            log.info(\"Chose index {}\".format(idx))\n\t            new_filepath = filepath_list[idx]\n\t        else:\n\t            raise FileNotFoundError\n\t        return os.path.basename(new_filepath)\n\t    def write(self, data, filename):\n\t        if not self.write:\n\t            log.error(\"Not allowed to write.\")\n", "            raise PermissionError\n\t        ext = os.path.splitext(filename)[1]\n\t        if ext == \".pt\":\n\t            self._write_torch_state_pt(data, filename)\n\t        elif ext == \".jsonl\":\n\t            self._write_jsonl(data, filename)\n\t        elif ext == \".json\":\n\t            self._write_json(data, filename)\n\t        elif ext == \".npy\":\n\t            self._write_npy(data, filename)\n", "        elif ext == \".yaml\":\n\t            self._write_yaml(data, filename)\n\t        else:\n\t            raise NotImplementedError\n\t    def append(self, data, filename):\n\t        if not self.write:\n\t            log.error(\"Not allowed to write.\")\n\t            raise PermissionError\n\t        ext = os.path.splitext(filename)[1]\n\t        if ext == \".jsonl\":\n", "            self._write_jsonl(data, filename, append=True)\n\t        else:\n\t            log.error(\"Appending not implemented for file extension \" + ext)\n\t            raise NotImplementedError\n\t    def _write_torch_state_pt(self, state, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        torch.save(state, filepath)\n\t    def _write_jsonl(self, data, filename, append=False):\n\t        filepath = os.path.join(self._path, filename)\n\t        enc = MultiDimensionalArrayEncoder()\n", "        json_str = enc.encode(data)\n\t        if append:\n\t            with open(filepath, \"a\") as f:\n\t                f.write(json_str + \"\\n\")\n\t        else:\n\t            with open(filepath, \"w\") as f:\n\t                f.write(json_str + \"\\n\")\n\t    def _write_json(self, data, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        enc = MultiDimensionalArrayEncoder()\n", "        json_str = enc.encode(data)\n\t        with open(filepath, \"w\") as f:\n\t            f.write(json_str)\n\t    def _write_npy(self, data, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        np.save(filepath, data, allow_pickle=False)\n\t    def _write_yaml(self, data, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        with open(filepath, \"w\") as f:\n\t            yaml.dump(data, f, Dumper=NoAliasDumper, sort_keys=False)\n", "    def read(self, filename, line=None, **kwargs):\n\t        if not self.read:\n\t            log.error(\"Permission Error: Not allowed to read.\")\n\t            raise PermissionError\n\t        ext = os.path.splitext(filename)[1]\n\t        if ext == \".npy\":\n\t            return self._read_numpy(filename)\n\t        elif ext == \".mat\":\n\t            return self._read_mat(filename)\n\t        elif ext == \".pt\":\n", "            return self._read_torch_state_pt(filename)\n\t        elif ext == \".pkl\":\n\t            return self._read_pickle(filename)\n\t        elif ext == \".jsonl\":\n\t            return self._read_jsonl(filename, line, **kwargs)\n\t        elif ext == \".json\":\n\t            return self._read_json(filename, **kwargs)\n\t        elif ext == \".yaml\":\n\t            return self._read_yaml(filename, **kwargs)\n\t        else:\n", "            log.error(\"Filetype {} not supported.\".format(ext))\n\t            raise IOError\n\t    def _read_torch_state_pt(self, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        return torch.load(filepath)\n\t    def _read_numpy(self, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        return np.load(filepath)\n\t    def _read_mat(self, filename):\n\t        filepath = os.path.join(self._path, filename)\n", "        return io.loadmat(filepath)\n\t    def _read_pickle(self, filename):\n\t        filepath = os.path.join(self._path, filename)\n\t        with open(filepath, \"rb\") as f:\n\t            data = pickle.load(f)\n\t        return data\n\t    def _read_jsonl(self, filename, jsonl_line_id=None, **kwargs):\n\t        filepath = os.path.join(self._path, filename)\n\t        if \"tuple_as_str\" in kwargs:\n\t            self.tuple_as_str = kwargs[\"tuple_as_str\"]\n", "        else:\n\t            self.tuple_as_str = True\n\t        if jsonl_line_id is None:\n\t            row_list = []\n\t            with open(filepath, \"r\") as f:\n\t                for line in f:\n\t                    if line.startswith(\"{\"):\n\t                        json_line = json.loads(line, object_hook=self.hinted_tuple_hook)\n\t                        row_list.append(json_line)\n\t            return row_list\n", "        else:\n\t            json_string = \"\"\n\t            with open(filepath, \"r\") as f:\n\t                for i, line in enumerate(f):\n\t                    if i == jsonl_line_id - 1:\n\t                        json_string = line.rstrip(\"\\n\")\n\t                        break\n\t            return json.loads(json_string, object_hook=self.hinted_tuple_hook)\n\t    def _read_json(self, filename, **kwargs):\n\t        if \"tuple_as_str\" in kwargs:\n", "            self.tuple_as_str = kwargs[\"tuple_as_str\"]\n\t        else:\n\t            self.tuple_as_str = False\n\t        filepath = os.path.join(self._path, filename)\n\t        with open(filepath, \"r\") as f:\n\t            return json.load(f, object_hook=self.hinted_tuple_hook)\n\t    def _read_yaml(self, filename, **kwargs):\n\t        filepath = os.path.join(self._path, filename)\n\t        with open(filepath, \"r\") as f:\n\t            return yaml.safe_load(f)\n", "    def make_experiment_dir(self, id, overwrite=False):\n\t        if not self.write:\n\t            log.error(\"Not allowed to write.\")\n\t            raise PermissionError\n\t        experiment_dir = EXPERIMENT_DIR_STANDARD[\"experiment_dir_str\"].format(id)\n\t        experiment_dir_abs_path = os.path.join(self._path, experiment_dir)\n\t        experiment_dir_rel_path = os.path.join(self._rel_path, experiment_dir)\n\t        if os.path.exists(experiment_dir_abs_path):\n\t            log.warning(\n\t                \"Experiment directory with same id already exists: \"\n", "                + experiment_dir_abs_path\n\t            )\n\t            if overwrite:\n\t                log.warning(\"Overwrite existing experiment directory?\")\n\t                if input(\"Overwrite existing experiment directory? [y/n]\") == \"y\":\n\t                    #log.info(\n\t                    #    \"Overwriting experiment directory with id {} accepted by user.\".format(\n\t                    #        id\n\t                    #    )\n\t                    #)\n", "                    shutil.rmtree(experiment_dir_abs_path)\n\t                else:\n\t                    log.error(\"Overwriting not permitted by user.\")\n\t                    raise PermissionError\n\t            else:\n\t                log.error(\"Overwriting not set as argument when calling Experiment().\")\n\t                raise PermissionError\n\t        os.mkdir(experiment_dir_abs_path)\n\t        for key, value in EXPERIMENT_DIR_STANDARD.items():\n\t            if not key == \"experiment_dir\" and not key  == \"experiment_dir_str\":\n", "                new_dir = os.path.join(experiment_dir_abs_path, value)\n\t                os.mkdir(new_dir)\n\t        log.info(\n\t            \"Successfully made new experiment directory {} for experiment {}\".format(\n\t                experiment_dir_rel_path, str(id)\n\t            )\n\t        )\n\t        return Path(experiment_dir_abs_path)\n\t    def hinted_tuple_hook(self, obj):\n\t        \"\"\"\n", "        Helper function that is called for decoding of json lines files.\n\t        Whenever the json object that is\n\t        encoded has strings as key or value arguments that are strings, they are checked\n\t        for the prefix '__tuple__'. If such a tuple is found, the rest of the string\n\t        is decoded via ast.literal_eval and thereby turned into a tuple.\n\t        While this works fine for smaller files, it\n\t        might cause a significant slowdown for larger files. However, this does not hurt\n\t        too much the performance, since we are just appending to the measurement\n\t        files. Also, this is why in nninfo the\n\t        standard is, to have several json objects in one file, each on a seperate line.\n", "        Also keep in mind the potential security risk of ast.literal_eval.\n\t        \"\"\"\n\t        def parse(item):\n\t            if isinstance(item, str):\n\t                if item.startswith(\"__tuple__\"):\n\t                    if self.tuple_as_str:\n\t                        parsed = item.lstrip(\"__tuple__\")\n\t                    else:\n\t                        parsed = ast.literal_eval(item.lstrip(\"__tuple__\"))\n\t                elif item.startswith(\"__int__\"):\n", "                    parsed = ast.literal_eval(item.lstrip(\"__int__\"))\n\t                else:\n\t                    parsed = item\n\t            elif isinstance(item, list):\n\t                parsed = list()\n\t                for el in item:\n\t                    parsed.append(parse(el))\n\t            elif isinstance(item, dict):\n\t                parsed = dict()\n\t                for k, v in item.items():\n", "                    parsed_key = parse(k)\n\t                    parsed_value = parse(v)\n\t                    parsed[parsed_key] = parsed_value\n\t            else:\n\t                parsed = item\n\t            return parsed\n\t        return parse(obj)\n\tclass MultiDimensionalArrayEncoder(json.JSONEncoder):\n\t    \"\"\"\n\t    Encoder, inherits from json.JSONEncoder. Modifies it in the sense that it handles\n", "    tuples differently from lists. Tuples are packed into strings before writing to to\n\t    the file. Each of these str(tuple()) objects has a prefix \"__tuple__\" which identifies\n\t    it as a tuple. When loading the json object, these strings are decoded via the\n\t    hinted_tuple_hook.\n\t    Keyword arguments can be used to set the underlying JSONEncoder objects at __init__.\n\t    For example, skipkeys, ensure_ascii, check_circular, allow_nan, sort_keys, indent can\n\t    be set. However, for the functionality of nninfo, the JSON objects have to be in one\n\t    line each.\n\t    \"\"\"\n\t    def __init__(self, **kwargs):\n", "        super(MultiDimensionalArrayEncoder, self).__init__(**kwargs)\n\t    def encode(self, obj):\n\t        def hint_tuples(item, key=False):\n\t            if isinstance(item, tuple):\n\t                return \"__tuple__\" + str(item)\n\t            if isinstance(item, int) and key:\n\t                return \"__int__\" + str(item)\n\t            if isinstance(item, list):\n\t                return [hint_tuples(e) for e in item]\n\t            if isinstance(item, dict):\n", "                return {\n\t                    hint_tuples(key, key=True): hint_tuples(value)\n\t                    for key, value in item.items()\n\t                }\n\t            else:\n\t                return item\n\t        return super(MultiDimensionalArrayEncoder, self).encode(hint_tuples(obj))\n"]}
{"filename": "nninfo/data_set.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom numpy.random import Philox, Generator\n\tSUBSET_SYMBOL = \"/\"  # <dataset>/<subset>/<subsubset> etc.\n\tclass DataSet(torch.utils.data.Dataset):\n\t    def __init__(self, task, name):\n\t        self._task = task\n\t        self._name = name\n\t        self._subsets = []\n\t    @staticmethod\n", "    def from_config(task, config):\n\t        \"\"\"\n\t        Creates a new DataSet from a config dictionary\n\t        \"\"\"\n\t        if task.finite:\n\t            return CachedDataset.from_config(task, config)\n\t        else:\n\t            return LazyDataset.from_config(task, config)\n\t    def to_config(self):\n\t        \"\"\"\n", "        Returns a dictionary representation of the dataset tree\n\t        \"\"\"\n\t        d = dict(name=self._name)\n\t        if self._subsets:\n\t            d[\"subsets\"] = [subset.to_config() for subset in self._subsets]\n\t        return d\n\t    def _load_subsets_from_config_list(self, config_list):\n\t        self._subsets = [\n\t            SubSet.from_config(self, subset_config)\n\t            for subset_config in config_list\n", "        ]\n\t    def __str__(self, level=0):\n\t        \"\"\"\n\t        Recursive function that allows for printing of the Dataset Tree / Subset Branch.\n\t        Args:\n\t            level (int): level of branch\n\t        Returns:\n\t            str: Representation of this branch (Tree).\n\t        \"\"\"\n\t        ret = \"\\t\" * level + self.__repr__() + \"\\n\"\n", "        for subset in self._subsets:\n\t            ret += subset.__str__(level=level + 1)\n\t        return ret\n\t    def __repr__(self):\n\t        return (\n\t            self._name\n\t            + \": \\t\"\n\t            + str(len(self))\n\t            + \" elements.\"\n\t            + (\"(lazy)\" if isinstance(self, LazyDataset) else \"\")\n", "            + (\"(cached)\" if isinstance(self, CachedDataset) else \"\")\n\t        )\n\t    def find(self, dataset_name):\n\t        \"\"\"\n\t        Depth-first search for dataset_name in the dataset tree\n\t        \"\"\"\n\t        if self._name == dataset_name:\n\t            return self\n\t        else:\n\t            for subset in self._subsets:\n", "                result = subset.find(dataset_name)\n\t                if not result is None:\n\t                    return result\n\t            return None\n\t    def create_subset(self, name, subset_slice, random_split_seed=None):\n\t        subset = SubSet(self, name, subset_slice, random_split_seed=random_split_seed)\n\t        self._subsets.append(subset)\n\t    def train_test_val_random_split(self, train_len, test_len, val_len, seed):\n\t        total_len = train_len + test_len + val_len\n\t        if len(self) != total_len:\n", "            raise ValueError(\n\t                \"Split can only be performed if the subdatasets total\"\n\t                \"length matches with the length of the dataset\"\n\t            )\n\t        train_name = self._name + SUBSET_SYMBOL + \"train\"\n\t        test_name = self._name + SUBSET_SYMBOL + \"test\"\n\t        val_name = self._name + SUBSET_SYMBOL + \"val\"\n\t        # create subsets\n\t        self.create_subset(train_name, slice(None, train_len), random_split_seed=seed)\n\t        self.create_subset(test_name, slice(train_len, train_len + test_len), random_split_seed=seed)\n", "        self.create_subset(val_name, slice(train_len + test_len, None), random_split_seed=seed)\n\t        return [train_name, test_name, val_name]\n\t    def train_test_val_sequential_split(self, train_len, test_len, val_len):\n\t        assert train_len + test_len + val_len == len(self), 'Split can only be performed if the subsets comprise the whole set'\n\t        train_name = self._name + SUBSET_SYMBOL + \"train\"\n\t        test_name = self._name + SUBSET_SYMBOL + \"test\"\n\t        val_name = self._name + SUBSET_SYMBOL + \"val\"\n\t        self.create_subset(train_name, slice(None, train_len))\n\t        self.create_subset(test_name, slice(train_len, train_len + test_len))\n\t        self.create_subset(val_name, slice(train_len + test_len, None))\n", "    def one_class_split(self):\n\t        return self._class_wise_split(\"one_class\")\n\t    def all_but_one_class_split(self):\n\t        return self._class_wise_split(\"all_but_one_class\")\n\t    def multiple_class_split(self, class_list):\n\t        return self._class_wise_split(\"multiple\", class_list=class_list)\n\t    def _class_wise_split(self, method, class_list=None):\n\t        dataset_labels_np = self._y# if self is CachedDataset else np.array(self)[:, 1]\n\t        # TODO: Test this for one-hot-labels\n\t        classes = np.unique(dataset_labels_np)\n", "        return_name_list = []\n\t        if method == \"one_class\":\n\t            for cls_idx, cls in enumerate(classes):\n\t                keep_idx = np.where(dataset_labels_np == cls)[0]\n\t                cls_name = self._name + SUBSET_SYMBOL + \"class_\" + str(cls_idx)\n\t                self.create_subset(cls_name, keep_idx)\n\t                return_name_list.append(cls_name)\n\t        elif method == \"all_but_one_class\":\n\t            for cls_idx, cls in enumerate(classes):\n\t                keep_idx = np.where(dataset_labels_np != cls)[0]\n", "                cls_name = self._name + SUBSET_SYMBOL + \"not_class_\" + str(cls_idx)\n\t                self.create_subset(cls_name, keep_idx)\n\t                return_name_list.append(cls_name)\n\t        elif method == \"multiple\":\n\t            keep_idx_list = []\n\t            cls_name = self._name + SUBSET_SYMBOL + \"multiple\"\n\t            for el in class_list:\n\t                temp = np.where(dataset_labels_np == el)[0].tolist()\n\t                print(type(temp))\n\t                keep_idx_list.extend(temp)\n", "                cls_name += \"_\" + str(el)\n\t            keep_idx = np.array(keep_idx_list)\n\t            self.create_subset(cls_name, keep_idx)\n\t        else:\n\t            raise NotImplementedError\n\t        return return_name_list\n\t    @property\n\t    def name(self):\n\t        return self._name\n\t    @property\n", "    def task(self):\n\t        return self._task\n\t    @property\n\t    def subsets(self):\n\t        return self._subsets\n\tclass SubSet(DataSet):\n\t    \"\"\"\n\t    SubSet of data, that is directly connected to the original 'full_set', and takes its samples\n\t    from there. Never change the order of the parent Dataset!\n\t    \"\"\"\n", "    def __init__(self, parent, name, subset_slice, random_split_seed=None):\n\t        \"\"\"Defines a subset of the parent dataset, that is defined by the indices in parent_indices\n\t        Args:\n\t            parent (DataSet): Parent dataset\n\t            name (str): Name of the subset\n\t            subset_slice (slice): Slice of the parent indices that are used for this subset\n\t            random_split_seed (int, optional): Seed for shuffling the parent indices before slicing\n\t        \"\"\"\n\t        super().__init__(parent.task, name)\n\t        self._parent = parent\n", "        self._random_split_seed = random_split_seed\n\t        self._slice = subset_slice\n\t        if random_split_seed is not None:\n\t            generator = np.random.default_rng(random_split_seed)\n\t            parent_indices = generator.permutation(len(parent))\n\t        else:\n\t            parent_indices = np.arange(len(parent))\n\t        self._indices = parent_indices[subset_slice]\n\t    @staticmethod\n\t    def from_config(parent, config):\n", "        subsets = [SubSet.from_config(parent, subset_config) for subset_config in config.get('subsets', [])]\n\t        subset = SubSet(parent, config['name'], eval(config['subset_slice']), config.get('random_split_seed', None))\n\t        subset._subsets = subsets\n\t        return subset\n\t    def to_config(self):\n\t        d = dict(name=self._name, subset_slice=str(self._slice))\n\t        if self._random_split_seed is not None:\n\t            d['random_split_seed'] = self._random_split_seed\n\t        if self._subsets:\n\t            d['subsets'] = [subset.to_dict() for subset in self._subsets]\n", "        return d\n\t    def __getitem__(self, idx):\n\t        return self.parent[self._indices[idx]]\n\t    def __len__(self):\n\t        return len(self._indices)\n\t    @property\n\t    def parent(self):\n\t        return self._parent\n\t    @property\n\t    def indices(self):\n", "        return self._indices\n\tclass CachedDataset(DataSet):\n\t    def __init__(self, task, name):\n\t        super().__init__(task, name)\n\t        x, y = self._task.load_samples()\n\t        self._x, self._y = x, y\n\t    @staticmethod\n\t    def from_config(task, config):\n\t        dataset = CachedDataset(task, config['name'])\n\t        dataset._load_subsets_from_config_list(config.get('subsets', []))\n", "        return dataset\n\t    def __len__(self):\n\t        return self._x.shape[0]\n\t    def __getitem__(self, idx):\n\t        return self._x[idx], self._y[idx]\n\tclass LazyDataset(DataSet):\n\t    def __init__(self, task, name, length, seed=None, condition=None):\n\t        super().__init__(task, name)\n\t        assert not task.finite\n\t        self._len = length\n", "        if not seed is None:\n\t            self._seed = seed\n\t        else:\n\t            seed_generator = Generator(Philox())\n\t            self._seed = int(\n\t                seed_generator.integers(np.iinfo(np.int64).max)\n\t            )  # Has to be int to be json serializable\n\t        self._philox = Philox(seed)\n\t        self._philox_state = self._philox.state\n\t        self._rng = Generator(self._philox)\n", "        self._condition = condition\n\t    @staticmethod\n\t    def from_config(task, config):\n\t        dataset = LazyDataset(\n\t            task, config['name'], config['length'], config.get('seed', None), config.get('condition', None)\n\t        )\n\t        dataset._load_subsets_from_config_list(config.get('subsets', []))\n\t        return dataset\n\t    def to_config(self):\n\t        d = dict(\n", "            name=self._name,\n\t            length=self._len,\n\t            seed=self._seed,\n\t            condition=self._condition,\n\t        )\n\t        if self._subsets:\n\t            d[\"subsets\"] = [subset.to_dict() for subset in self._subsets]\n\t        return d\n\t    def extend(self, n_samples):\n\t        \"\"\"\n", "        Get a copy of the dataset with a different length\n\t        \"\"\"\n\t        name_ext = self._name + \"_ext\"\n\t        return LazyDataset(self._task, name_ext, n_samples, self._seed)\n\t    def condition(self, n_samples, condition):\n\t        \"\"\"\n\t        Get a copy of the dataset with a condition\n\t        \"\"\"\n\t        name_cond = self._name + \"_cond\"\n\t        return LazyDataset(\n", "            self._task, name_cond, n_samples, self._seed, condition=condition\n\t        )\n\t    def __len__(self):\n\t        return self._len\n\t    def __getitem__(self, idx):\n\t        if idx >= self._len:\n\t            raise IndexError\n\t        self._philox_state[\"state\"][\"counter\"][-1] = idx\n\t        self._philox.state = self._philox_state\n\t        return self._task.generate_sample(self._rng, condition=self._condition)\n"]}
{"filename": "nninfo/logger.py", "chunked_list": ["import logging\n\tdef get_logger(name):\n\t    # Create a custom logger\n\t    logger = logging.getLogger(name)\n\t    # Create handlers\n\t    c_handler = logging.StreamHandler()\n\t    # Create formatters and add it to handlers\n\t    c_format = logging.Formatter(\"%(name)s: %(message)s\")\n\t    c_handler.setFormatter(c_format)\n\t    c_handler.setLevel(logging.WARNING)\n", "    # Add handlers to the logger\n\t    logger.addHandler(c_handler)\n\t    return logger\n\tdef add_exp_file_handler(experiment_dir):\n\t    main_logger = logging.getLogger(\"nninfo\")\n\t    f_handler = logging.FileHandler(experiment_dir / \"log\" / \"exp_log.log\")\n\t    f_format = logging.Formatter(\n\t        \"%(asctime)s [%(name)-13.13s] [%(levelname)-5.5s]  %(message)s\"\n\t    )\n\t    f_handler.setFormatter(f_format)\n", "    f_handler.setLevel(logging.INFO)\n\t    # check if logger for this experiment exists already\n\t    if main_logger.handlers:\n\t        for h in main_logger.handlers:\n\t            if h.__dict__[\"baseFilename\"] == f_handler.baseFilename:\n\t                return\n\t    main_logger.addHandler(f_handler)\n\tdef remove_exp_file_handler(experiment_dir):\n\t    main_logger = logging.getLogger(\"nninfo\")\n\t    dummy = logging.FileHandler(experiment_dir + \"log/exp_log.log\")\n", "    for h in main_logger.handlers:\n\t        if h.__dict__[\"baseFilename\"] == dummy.baseFilename:\n\t            main_logger.handlers.remove(h)\n"]}
{"filename": "nninfo/__init__.py", "chunked_list": ["import logging\n\timport os\n\tfrom . import logger\n\tfrom . import config\n\tfrom . import experiment\n\tfrom . import trainer\n\tfrom . import tester\n\tfrom . import tasks\n\tfrom . import file_io\n\tfrom . import plot\n", "from . import data_set\n\tfrom . import analysis\n\tfrom . import schedule\n\tfrom .experiment import *\n\tfrom .trainer import *\n\tfrom .tester import *\n\tfrom .model.neural_network import *\n\tfrom .tasks import *\n\tmodule_dir = os.path.dirname(__file__) + \"/\"  # path to this file\n\tlogging.basicConfig(\n", "    level=logging.INFO,\n\t    format=\"%(asctime)s [%(name)-13.13s] [%(levelname)-5.5s]  %(message)s\",\n\t    filename=module_dir + \"../experiments/nninfo.log\",\n\t    filemode=\"a\",\n\t)\n\tlogging.getLogger(\"nninfo\").info(\"STARTUP NNINFO SESSION\")\n"]}
{"filename": "nninfo/utils.py", "chunked_list": ["import shutil\n\tfrom .experiment import Experiment\n\tdef remove_experiment(experiment_id, yes=False, silent=False):\n\t    \"\"\"\n\t    Removes an experiment from the database.\n\t    Args:\n\t        experiment_id (str): id of the experiment to be removed\n\t        yes (bool): if True, no confirmation is asked\n\t        silent (bool): if True, no error is raised if experiment does not exist\n\t    \"\"\"\n", "    experiment_dir = Experiment._find_experiment_dir(experiment_id)\n\t    if not experiment_dir.exists():\n\t        if silent:\n\t            print(\n\t                f'Experiment {experiment_id} does not exist at path {experiment_dir}. Skipping removal.')\n\t            return\n\t        raise ValueError(\n\t            f'Experiment {experiment_id} does not exist at path {experiment_dir}.')\n\t    if not yes:\n\t        print(\n", "            f'Are you sure you want to remove experiment {experiment_id} at path {experiment_dir}? (y/n)')\n\t        answer = input()\n\t        if answer != 'y':\n\t            return\n\t    # Recursively remove experiment directory\n\t    shutil.rmtree(experiment_dir)\n\t    print(f'Removed experiment {experiment_id} from database.')\n"]}
{"filename": "nninfo/tester.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import DataLoader\n\tfrom nninfo.exp_comp import ExperimentComponent\n\tfrom nninfo.model.quantization import quantizer_list_factory\n\tclass Tester(ExperimentComponent):\n\t    \"\"\"\n\t    Is called after each training chapter to perform predefined tests and save their results.\n\t    Args:\n\t        dataset_name (str): Name of the dataset in the TaskManagers dataset\n\t            dict that should be tested on.\n", "    \"\"\"\n\t    BATCH_SIZE = 10_000\n\t    def __init__(self, dataset_name):\n\t        super().__init__()\n\t        self._dataset_name = dataset_name\n\t        self._net = None\n\t        self._task = None\n\t    @staticmethod\n\t    def from_config(config):\n\t        return Tester(config[\"dataset_name\"])\n", "    def to_config(self):\n\t        return {\"dataset_name\": self._dataset_name}\n\t    def _get_output_activations(self, dataset_name=None, quantizer_params=None, quantizer=None):\n\t        self._net = self.parent.network\n\t        self._task = self.parent.task\n\t        if quantizer is None:\n\t            quantizer = quantizer_list_factory(\n\t                quantizer_params, self._net.get_limits_list())\n\t        self._net.eval()\n\t        if dataset_name is None:\n", "            dataset_name = self._dataset_name\n\t        feeder = DataLoader(\n\t            self._task[dataset_name], batch_size=self.BATCH_SIZE\n\t        )\n\t        with torch.no_grad():\n\t            for x_test, y_test in feeder:\n\t                yield self._net.forward(x_test, quantizer, apply_output_softmax=True), y_test\n\t    def compute_loss_and_accuracy(self, dataset_name=None, quantizer_params=None, quantizer=None):\n\t        activations_iter = self._get_output_activations(\n\t            dataset_name, quantizer_params, quantizer)\n", "        loss_fn = self.parent.trainer.loss\n\t        total_size = 0\n\t        correct = 0\n\t        test_loss = 0\n\t        for pred_y_test, y_test in activations_iter:\n\t            # Compute loss\n\t            loss = loss_fn(pred_y_test, y_test)\n\t            test_loss += loss.item() * pred_y_test.shape[0]\n\t            # Compute accuracy\n\t            if self.parent.task.task.y_dim > 1 and y_test.ndim == 1:\n", "                # One-hot-representation\n\t                decision = pred_y_test.argmax(dim=1)\n\t                correct += (decision == y_test).sum().item()\n\t            else:\n\t                # Binary output representations\n\t                decision = torch.round(pred_y_test)\n\t                correct += torch.all(decision == y_test, axis=1).sum().item()\n\t            total_size += pred_y_test.shape[0]\n\t        loss = test_loss / total_size\n\t        accuracy = correct / total_size\n", "        return loss, accuracy\n"]}
{"filename": "nninfo/schedule.py", "chunked_list": ["import numpy as np\n\timport nninfo\n\tclass Schedule:\n\t    \"\"\"\n\t    Can create epoch lists for preplanned experiment chapters. These chapters are the main\n\t    structure of the training period of the experiment and allow for spaced saving of\n\t    checkpoints.\n\t    The plan is to end a chapter of the experiment when a epoch contained in the chapter_ends\n\t    variable of this class is reached. This is not applied yet, but the class\n\t    is already able to create log spaced and lin spaced numbers of epochs,\n", "    which should then help with the actual experiment run. However, for the log-spaced chapter\n\t    planning, the number of chapter_ends can be lower than the number of chapters that are\n\t    given as n_chapter_wished.\n\t    Does not need to inherit from ExperimentComponent, because it is not calling anything else.\n\t    \"\"\"\n\t    def __init__(self):\n\t        self.chapter_ends = None\n\t        self.chapter_ends_continued = None\n\t    @staticmethod\n\t    def from_config(config):\n", "        \"\"\"\n\t        Creates a Schedule object from a config dictionary.\n\t        Args:\n\t            config (dict): Dictionary containing the config for the Schedule object.\n\t        Returns:\n\t            Schedule object.\n\t        \"\"\"\n\t        schedule = Schedule()\n\t        schedule.chapter_ends = config[\"chapter_ends\"]\n\t        schedule.chapter_ends_continued = config[\"chapter_ends_continued\"]\n", "        return schedule\n\t    def to_config(self):\n\t        \"\"\"\n\t        Creates a config dictionary from the Schedule object.\n\t        Returns:\n\t            Dictionary containing the config for the Schedule object.\n\t        \"\"\"\n\t        config = {\n\t            \"chapter_ends\": self.chapter_ends,\n\t            \"chapter_ends_continued\": self.chapter_ends_continued,\n", "        }\n\t        return config\n\t    def create_log_spaced_chapters(self, n_epochs, n_chapters_wished):\n\t        \"\"\"\n\t        Function that creates a list of numbers which are the epoch indices where chapters\n\t        are ended. The indices are created logarithmically spaced over the total number of\n\t        epochs for this experiment (n_epochs).\n\t        Args:\n\t            n_epochs (int): Total number of epochs for this experiment.\n\t            n_chapters_wished (int): Number of chapters the experiment should take to reach the\n", "                total number of epochs n_epochs.\n\t        Sets self.chapter_ends to a list of these indices (int).\n\t        Sets self.chapter_ends_continued to a list of a continued of chapter_ends\n\t        until n_epochs*n_epochs (int).\n\t        \"\"\"\n\t        def log_space(n_e, n_c):\n\t            end = np.log10(n_e)\n\t            epochs = np.logspace(0, end, n_c + 1, endpoint=True)\n\t            epochs = np.round(epochs).astype(int)\n\t            epochs = np.unique(epochs)\n", "            # add a 0 in the front for consistency\n\t            epochs = np.insert(epochs, 0, 0)\n\t            return epochs\n\t        self.chapter_ends = log_space(n_epochs, n_chapters_wished).tolist()\n\t        self.chapter_ends_continued = log_space(\n\t            n_epochs * n_epochs, n_chapters_wished * 2\n\t        ).tolist()\n\t    def create_lin_spaced_chapters(self, n_epochs, n_chapters_wished):\n\t        \"\"\"\n\t        Function that creates a list of numbers, which are the epoch indices where chapters\n", "        are ended. The indices are created linearly spaced over the total number of\n\t        epochs for this experiment (n_epochs).\n\t        Args:\n\t            n_epochs (int): Total number of epochs for this experiment.\n\t            n_chapters_wished (int): Number of chapters the experiment should take to reach the\n\t                total number of epochs n_epochs.\n\t        Sets self.chapter_ends to a list of these indices (int).\n\t        Sets self.chapter_ends_continued to a list of a continued of chapter_ends\n\t        until n_epochs*100 (int).\n\t        \"\"\"\n", "        def lin_space(n_e, n_c):\n\t            epochs = np.linspace(0, n_e, n_c + 1, endpoint=True)\n\t            epochs = np.round(epochs).astype(int)\n\t            epochs = np.unique(epochs)\n\t            return epochs\n\t        self.chapter_ends = lin_space(n_epochs, n_chapters_wished).tolist()\n\t        self.chapter_ends_continued = lin_space(\n\t            n_epochs * 100, n_chapters_wished * 100\n\t        ).tolist()\n\t    def get_epoch_for_chapter(self, chapter):\n", "        \"\"\"\n\t        Returns the epoch index for a given chapter index.\n\t        Args:\n\t            chapter (int): Index of the chapter.\n\t        Returns:\n\t            Epoch index (int).\n\t        \"\"\"\n\t        return self.chapter_ends_continued[chapter]\n\t    def save(self, path):\n\t        saver = nninfo.file_io.FileManager(path, write=True)\n", "        save_dict = {\n\t            \"chapter_ends\": self.chapter_ends,\n\t            \"chapter_ends_continued\": self.chapter_ends_continued,\n\t        }\n\t        saver.write(save_dict, \"schedule.json\")\n\t    def _load(self, path):\n\t        loader = nninfo.file_io.FileManager(path, read=True)\n\t        load_dict = loader.read(\"schedule.json\")\n\t        self.chapter_ends = load_dict[\"chapter_ends\"]\n\t        self.chapter_ends_continued = load_dict[\"chapter_ends_continued\"]\n", "    def __str__(self):\n\t        return str(self.chapter_ends)\n"]}
{"filename": "nninfo/trainer.py", "chunked_list": ["from typing import Optional\n\timport numpy as np\n\tfrom torch.utils.data import DataLoader\n\timport torch.optim as optim\n\timport torch.nn as nn\n\timport nninfo\n\tfrom nninfo.config import CLUSTER_MODE\n\tfrom nninfo.exp_comp import ExperimentComponent\n\tfrom nninfo.model.quantization import quantizer_list_factory\n\tlog = nninfo.logger.get_logger(__name__)\n", "# optimizers for pytorch\n\tOPTIMIZERS_PYTORCH = {\"SGD\": optim.SGD, \"Adam\": optim.Adam}\n\t# the losses that are available at the moment\n\tLOSSES_PYTORCH = {\n\t    \"BCELoss\": nn.BCELoss,\n\t    \"CELoss\": nn.CrossEntropyLoss,\n\t    \"MSELoss\": nn.MSELoss,\n\t}\n\tclass Trainer(ExperimentComponent):\n\t    \"\"\"\n", "    Trains the network using chapter structure.\n\t    Define your training settings here.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        dataset_name,\n\t        optim_str,\n\t        loss_str,\n\t        lr,\n\t        shuffle,\n", "        batch_size,\n\t        quantizer,\n\t        momentum=0\n\t    ):\n\t        \"\"\"\n\t        Sets training parameters. Is also called when loading parameters from file.\n\t        Args:\n\t            dataset_name (str): Name of the dataset in the TaskManagers dataset\n\t                dict that should be trained on.\n\t            optim_str (str): One of the optimizers available in constant OPTIMIZERS_PYTORCH.\n", "                It is easy to add new ones, if necessary, since most commonly used ones are\n\t                already implemented in pytorch.\n\t            loss_str (str): One of the losses available in LOSSES_PYTORCH.\n\t                It is easy to add new ones, if necessary, since most commonly used ones are\n\t                already implemented in pytorch.\n\t            lr (float): The learning rate that should be used for the training.\n\t            shuffle (bool): Whether to shuffle\n\t            batch_size (int): Number of samples from the dataset that should be used together\n\t                as a batch for one training step, for example in (Batch) Stochastic Gradient\n\t                Descent.\n", "            n_epochs_chapter (int): If the number of epochs per chapter is a constant it can\n\t                be also set here. Otherwise it must be passed each time train_chapter is\n\t                called.\n\t        \"\"\"\n\t        self._dataset_name = dataset_name\n\t        self._lr = lr\n\t        self._batch_size = batch_size\n\t        self._optim_str = optim_str\n\t        self._loss_str = loss_str\n\t        self._shuffle = shuffle\n", "        self._quantizer_params = quantizer\n\t        self._momentum = momentum\n\t        self._n_epochs_trained = 0\n\t        self._n_chapters_trained = 0\n\t        super().__init__()\n\t    @staticmethod\n\t    def from_config(config):\n\t        trainer = Trainer(\n\t            dataset_name=config[\"dataset_name\"],\n\t            optim_str=config[\"optim_str\"],\n", "            loss_str=config[\"loss_str\"],\n\t            lr=config[\"lr\"],\n\t            shuffle=config[\"shuffle\"],\n\t            batch_size=config[\"batch_size\"],\n\t            quantizer=config.get(\"quantizer\", None),\n\t            momentum=config.get(\"momentum\", 0)\n\t        )\n\t        return trainer\n\t    def to_config(self):\n\t        param_dict = {\n", "            \"dataset_name\": self._dataset_name,\n\t            \"optim_str\": self._optim_str,\n\t            \"batch_size\": self._batch_size,\n\t            \"shuffle\": self._shuffle,\n\t            \"lr\": self._lr,\n\t            \"loss_str\": self._loss_str,\n\t            \"n_epochs_trained\": self._n_epochs_trained,\n\t            \"n_chapters_trained\": self._n_chapters_trained,\n\t            \"quantizer\": self._quantizer_params,\n\t            \"momentum\": self._momentum\n", "        }\n\t        return param_dict\n\t    @property\n\t    def loss(self):\n\t        return self._loss\n\t    @property\n\t    def lr(self):\n\t        return self._lr\n\t    @property\n\t    def n_epochs_trained(self):\n", "        return self._n_epochs_trained\n\t    @property\n\t    def n_chapters_trained(self):\n\t        return self._n_chapters_trained\n\t    def set_n_epochs_trained(self, n_epochs_trained):\n\t        \"\"\"\n\t        Sets the number of epochs trained to a new value.\n\t        Should not be called by user, only by experiment.\n\t        \"\"\"\n\t        log.info(\"n_epochs_trained is changed from outside.\")\n", "        self._n_epochs_trained = n_epochs_trained\n\t    def set_n_chapters_trained(self, n_chapters_trained):\n\t        \"\"\"\n\t        Sets the number of epochs trained to a new value.\n\t        Should not be called by user, only by experiment.\n\t        \"\"\"\n\t        log.info(\"n_chapters_trained is changed from outside.\")\n\t        self._n_chapters_trained = n_chapters_trained\n\t    def optimizer_state_dict(self):\n\t        return self._optimizer.state_dict()\n", "    def load_optimizer_state_dict(self, opt_state_dict):\n\t        self._optimizer.load_state_dict(opt_state_dict)\n\t    def train_chapter(\n\t        self, use_cuda, use_ipex, n_epochs_chapter=None, compute_test_loss=Optional[bool]\n\t    ):\n\t        \"\"\"\n\t        Perform the training steps for a given number of epochs. If no n_epochs_chapter is given\n\t        it is expected to have already been set in set_training_parameters(..).\n\t        Args:\n\t            n_epochs_chapter (int):    Number of epochs to train for this chapter of the training.\n", "            compute_test_loss (bool):  Whether to compute the test loss after each epoch. When None is passed,\n\t                                        it is set to not CLUSTER_MODE.\n\t        \"\"\"\n\t        if compute_test_loss is None:\n\t            compute_test_loss = not CLUSTER_MODE\n\t        # make experiment components ready for training\n\t        self._start_chapter(use_ipex)\n\t        # set model to train mode\n\t        self._net.train()\n\t        if use_cuda and not next(self._net.parameters()).is_cuda:\n", "            print('Moving model to CUDA')\n\t            self._net.cuda()\n\t        # create a DataLoader that then feeds the chosen dataset into the network during training\n\t        feeder = DataLoader(\n\t            self._task[self._dataset_name],\n\t            batch_size=self._batch_size,\n\t            shuffle=self._shuffle,\n\t        )\n\t        # central training loop\n\t        for _ in range(n_epochs_chapter):\n", "            full_loss = 0\n\t            for local_x_batch, local_y_batch in feeder:\n\t                if use_cuda:\n\t                    local_x_batch = local_x_batch.cuda()\n\t                    local_y_batch = local_y_batch.cuda()\n\t                # zeroes the gradient buffers of all parameters\n\t                self._optimizer.zero_grad()\n\t                self._net.train()\n\t                pred_y = self._net(local_x_batch, quantizers=self._quantizer)\n\t                loss = self._loss(pred_y, local_y_batch)\n", "                loss.backward()\n\t                self._optimizer.step()\n\t                full_loss += loss.cpu().item() * len(local_y_batch)\n\t            self._n_epochs_trained += 1\n\t            print_str = (\n\t                \"trained epoch: \"\n\t                + str(self._n_epochs_trained)\n\t                + \"; train loss: \"\n\t                + str(np.sum(full_loss) / len(feeder.dataset))\n\t                + (f\"; test loss: {self._tester.compute_loss_and_accuracy(quantizer=self._quantizer)[0]}\" if compute_test_loss else \"\")\n", "            )\n\t            print(print_str)\n\t            log.info(print_str)\n\t        self._end_chapter()\n\t    def _start_chapter(self, use_ipex=False):\n\t        first_overall_epoch = self._n_epochs_trained == 0 and self.parent.run_id == 0\n\t        first_epoch_in_run = self._n_epochs_trained == 0\n\t        if first_overall_epoch:\n\t            self.initialize_components(use_ipex)\n\t            self.parent.save_components()\n", "        if first_epoch_in_run:\n\t            self.parent.save_checkpoint()\n\t        log.info(\"Started training chapter {}.\".format(self._n_chapters_trained + 1))\n\t    def initialize_components(self, use_ipex=False):\n\t        self._net = self.parent.network\n\t        if self._optim_str == \"SGD\":\n\t            self._optimizer = OPTIMIZERS_PYTORCH[self._optim_str](\n\t                self._net.parameters(), lr=self._lr, momentum=self._momentum\n\t            )\n\t        else:\n", "            self._optimizer = OPTIMIZERS_PYTORCH[self._optim_str](\n\t                self._net.parameters(), lr=self._lr\n\t            )\n\t        self._loss = LOSSES_PYTORCH[self._loss_str]()\n\t        self._task = self.parent.task\n\t        self._tester = self.parent.tester\n\t        self._quantizer = quantizer_list_factory(self._quantizer_params, self.parent.network.get_limits_list())\n\t        if use_ipex:\n\t            import intel_extension_for_pytorch as ipex # type: ignore\n\t            self._net, self._optimizer = ipex.optimize(self._net, optimizer=self._optimizer)\n", "    def _end_chapter(self):\n\t        self._n_chapters_trained += 1\n\t        log.info(\"Finished training chapter {}.\".format(self._n_chapters_trained))\n\t        print(\"Finished training chapter {}.\".format(self._n_chapters_trained))\n\t        self.parent.save_checkpoint()"]}
{"filename": "nninfo/postprocessing/pid_postprocessing.py", "chunked_list": ["from ast import literal_eval\n\tfrom itertools import chain, combinations\n\timport pandas as pd\n\tclass Antichain:\n\t    def __init__(self, antichain_string):\n\t        self._antichain_tuple = literal_eval(antichain_string)\n\t        assert isinstance(self._antichain_tuple, tuple)\n\t        assert all(isinstance(a, tuple) for a in self._antichain_tuple)\n\t        assert all(isinstance(l, int)\n\t                   for a in self._antichain_tuple for l in a)\n", "    def degree_of_synergy(self):\n\t        return min(len(a) for a in self._antichain_tuple)\n\t    def max_index(self):\n\t        return max(max(a) for a in self._antichain_tuple)\n\t    def __len__(self):\n\t        return len(self._antichain_tuple)\n\t    def __iter__(self):\n\t        return iter(self._antichain_tuple)\n\tdef _compute_degree_of_synergy_atoms(pid: pd.DataFrame):\n\t    degrees_of_synergy = [Antichain(antichain_string).degree_of_synergy()\n", "                          for antichain_string in pid.columns]\n\t    degree_of_synergy_atoms = pid.groupby(\n\t        degrees_of_synergy, axis=1).sum()\n\t    return degree_of_synergy_atoms\n\tdef _compute_mutual_information(pid: pd.DataFrame):\n\t    mutual_information = pid.sum(axis=1)\n\t    mutual_information.name = ''\n\t    return mutual_information\n\tdef _compute_representational_complexity(degree_of_synergy_atoms: pd.DataFrame, mutual_information: pd.DataFrame):\n\t    repr_compl = (degree_of_synergy_atoms *\n", "                  degree_of_synergy_atoms.columns).sum(axis=1) / mutual_information\n\t    repr_compl.name = ''\n\t    return repr_compl\n\tdef get_pid_summary_quantities(pid: pd.DataFrame, pid_part: str = 'avg_pid'):\n\t    \"\"\"\n\t    Compute summary quantities of a PID DataFrame.\n\t    \"\"\"\n\t    # Select the part of the PID to be used\n\t    pid_part = pid[pid_part]\n\t    degree_of_synergy_atoms = _compute_degree_of_synergy_atoms(pid_part)\n", "    mutual_information = _compute_mutual_information(pid_part)\n\t    repr_compl = _compute_representational_complexity(\n\t        degree_of_synergy_atoms, mutual_information)\n\t    # Combine into a DataFrame with a MultiIndex including run_id, chapter_id and epoch_id\n\t    summary_quantities = degree_of_synergy_atoms\n\t    summary_quantities.columns = pd.MultiIndex.from_arrays(\n\t        [['degree_of_synergy_atoms'] * len(degree_of_synergy_atoms.columns), degree_of_synergy_atoms.columns])\n\t    summary_quantities['mutual_information'] = mutual_information\n\t    summary_quantities['representational_complexity'] = repr_compl\n\t    summary_quantities['run_id'] = pid['run_id']\n", "    summary_quantities['chapter_id'] = pid['chapter_id']\n\t    summary_quantities['epoch_id'] = pid['epoch_id']\n\t    return summary_quantities\n\tdef _powerset(iterable):\n\t    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n\t    s = list(iterable)\n\t    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\tdef get_mi_or_entropy(pid: pd.DataFrame, pid_part: str = 'avg_pid'):\n\t    n = max(Antichain(antichain_string).max_index()\n\t            for antichain_string in pid.avg_pid.columns)\n", "    pid_part = pid[pid_part]\n\t    source_sets = list(_powerset(range(1, n + 1)))[1:]\n\t    df_entropies = pd.DataFrame()\n\t    for source_set in source_sets:\n\t        H = pd.Series(0, index=pid_part.index)\n\t        for achain_str in pid_part.columns:\n\t            achain = Antichain(achain_str)\n\t            for a in achain:\n\t                if set(a).issubset(source_set):\n\t                    H += pid_part[achain_str]\n", "                    break\n\t        df_entropies[str(source_set)] = H\n\t    return df_entropies"]}
{"filename": "nninfo/postprocessing/reing_postprocessing.py", "chunked_list": ["import pandas as pd\n\tdef get_directed_reing_complexity(reing_differences: pd.DataFrame):\n\t    n = sum(reing_differences.columns.str.match(r'C\\(\\d+\\|\\|\\d+\\)'))\n\t    complexity = pd.DataFrame()\n\t    complexity['run_id'] = reing_differences['run_id']\n\t    complexity['chapter_id'] = reing_differences['chapter_id']\n\t    complexity['epoch_id'] = reing_differences['epoch_id']\n\t    complexity['reing_complexity'] = sum(k * reing_differences[f'C({k}||{k+1})'] for k in range(1, n)) / sum(reing_differences[f'C({k}||{k+1})'] for k in range(1, n))\n\t    return complexity"]}
{"filename": "nninfo/model/quantization.py", "chunked_list": ["from abc import abstractmethod, ABC\n\tfrom dataclasses import dataclass\n\tfrom typing import Union, Tuple, List\n\timport numpy as np\n\timport torch\n\tLimits = Union[Tuple[float, float], str]\n\t@dataclass\n\tclass Quantizer(ABC):\n\t    \"\"\"\n\t        n_levels: Number of equidistant quantization levels\n", "    \"\"\"\n\t    n_levels: int\n\t    limits: Limits\n\t    @abstractmethod\n\t    def __call__(self, x: torch.Tensor):\n\t        pass\n\t@dataclass\n\tclass StochasticQuantizer(Quantizer):\n\t    \"\"\"Stochastically rounds either up or down to the nearest Q3 quantization level.\n\t    The probability of rounding up is proportional to the normalized remainder.\n", "    \"\"\"\n\t    def __post_init__(self):\n\t        # Make sure the limits are finite\n\t        assert np.isfinite(self.limits[0]) and np.isfinite(\n\t            self.limits[1]), f\"Quantization limits are not finite {self.limits}\"\n\t        self.scale = (self.limits[1] - self.limits[0]) / (self.n_levels - 1)\n\t    def __call__(self, x: torch.Tensor):\n\t        # Make sure all activations are within the limits\n\t        assert torch.all(x >= self.limits[0]) and torch.all(\n\t            x <= self.limits[1]), f\"Activations are not within the limits {self.limits}\"\n", "        x_normalized = (x - self.limits[0]) / self.scale\n\t        ceil_prob = torch.remainder(x_normalized, 1)\n\t        ceil_mask = torch.bernoulli(ceil_prob)\n\t        floored = FloorNoGradient.apply(\n\t            x_normalized) * self.scale + self.limits[0]\n\t        ceiled = CeilNoGradient.apply(\n\t            x_normalized) * self.scale + self.limits[0]\n\t        return torch.where(ceil_mask == 1, ceiled, floored)\n\t@dataclass\n\tclass Q3Quantizer(Quantizer):\n", "    \"\"\"The two outmost bins are only half the width than the others and round to the binning limits.\"\"\"\n\t    def __post_init__(self):\n\t        # Make sure the limits are finite\n\t        assert np.isfinite(self.limits[0]) and np.isfinite(\n\t            self.limits[1]), f\"Quantization limits are not finite {self.limits}\"\n\t        self.scale = (self.limits[1] - self.limits[0]) / (self.n_levels - 1)\n\t    def __call__(self, x: torch.Tensor):\n\t        # Make sure all activations are within the limits\n\t        assert torch.all(x >= self.limits[0]) and torch.all(\n\t            x <= self.limits[1]), f\"Activations are not within the limits {self.limits}\"\n", "        return torch.round((x - self.limits[0]) / self.scale) * self.scale + self.limits[0]\n\tclass CeilNoGradient(torch.autograd.Function):\n\t    @staticmethod\n\t    def forward(ctx, x):\n\t        return x.ceil()\n\t    @staticmethod\n\t    def backward(ctx, g):\n\t        return g\n\tclass FloorNoGradient(torch.autograd.Function):\n\t    @staticmethod\n", "    def forward(ctx, x):\n\t        return x.floor()\n\t    @staticmethod\n\t    def backward(ctx, g):\n\t        return g\n\tdef quantizer_factory(rounding_point: str, levels: int, limits: Limits) -> Quantizer:\n\t    if rounding_point == \"stochastic\":\n\t        return StochasticQuantizer(levels, limits)\n\t    elif rounding_point == \"center_saturating\":\n\t        return Q3Quantizer(levels, limits)\n", "    else:\n\t        raise NotImplementedError(\n\t            f'No quantizer with rounding point {rounding_point} is available.')\n\tdef quantizer_list_factory(quantizer_params: Union[None, dict, list], limits: List[Limits]) -> List[Quantizer]:\n\t    if quantizer_params is None:\n\t        return [lambda x: x] * len(limits)\n\t    if isinstance(quantizer_params, dict):\n\t        quantizer_param_list = [quantizer_params] * len(limits)\n\t    elif isinstance(quantizer_params, list):\n\t        quantizer_param_list = quantizer_params\n", "    return [(quantizer_factory(**params, limits=limits) if params is not None else (lambda x: x)) for params, limits in zip(quantizer_param_list, limits)]\n"]}
{"filename": "nninfo/model/neural_network.py", "chunked_list": ["import copy\n\tfrom dataclasses import dataclass, field\n\tfrom functools import cache\n\tfrom typing import Optional, Union, Tuple, List\n\tfrom ast import literal_eval\n\timport torch.nn as nn\n\timport torch\n\timport numpy as np\n\timport scipy as scp\n\timport yaml\n", "import nninfo\n\tfrom ..file_io import NoAliasDumper\n\tfrom .quantization import quantizer_list_factory\n\tLimits = Union[Tuple[float, float], str]\n\tclass RandomRotation(nn.Module):\n\t    def __init__(self, dim, rng_seed):\n\t        super(RandomRotation, self).__init__()\n\t        self.dim = dim\n\t        self.rng_seed = rng_seed\n\t        rng = np.random.default_rng(self.rng_seed)\n", "        self.rotation_matrix = torch.tensor(scp.stats.special_ortho_group.rvs(dim, random_state=rng), dtype=torch.float32)\n\t    def forward(self, x):\n\t        return torch.matmul(x, self.rotation_matrix)\n\tCONNECTION_LAYERS_PYTORCH = {\n\t    \"input\": lambda: (lambda x: x),\n\t    \"identity\": lambda: (lambda x: x),\n\t    \"linear\": nn.Linear,\n\t    \"dropout\": nn.Dropout,\n\t    \"maxpool2d\": nn.MaxPool2d,\n\t    \"conv2d\": nn.Conv2d,\n", "    \"flatten\": nn.Flatten,\n\t    \"random_rotation\": RandomRotation,\n\t}\n\tACTIV_FUNCS_PYTORCH = {\n\t    None: lambda: (lambda x: x),\n\t    \"input\": lambda: (lambda x:x),  \n\t    \"relu\": lambda: torch.relu,\n\t    \"tanh\": lambda: torch.tanh,\n\t    \"hardtanh\": lambda: nn.Hardtanh(),\n\t    \"sigmoid\": lambda: torch.sigmoid,\n", "    \"softmax\": lambda: torch.nn.functional.softmax,\n\t    \"log_softmax\": lambda: torch.nn.functional.log_softmax,\n\t    \"softmax_output\": lambda: (lambda x: x),\n\t}\n\tACTIV_FUNCS_BINNING_LIMITS = {\n\t    None: (-np.inf, np.inf),\n\t    \"input\": None,\n\t    \"relu\": (0.0, np.inf),\n\t    \"tanh\": (-1.0, 1.0),\n\t    \"hardtanh\": (-1.0, 1.0),\n", "    \"sigmoid\": (0.0, 1.0),\n\t    \"softmax\": (0.0, 1.0),\n\t    \"log_softmax\": (-np.inf, 0.0),\n\t    \"softmax_output\": (0.0, 1.0),\n\t}\n\tINITIALIZERS_PYTORCH = {\n\t    \"xavier\": nn.init.xavier_uniform_,\n\t    \"he_kaiming\": nn.init.kaiming_uniform_,\n\t    \"he_kaiming_normal\": nn.init.kaiming_normal_,\n\t}\n", "@dataclass\n\tclass LayerInfo:\n\t    \"\"\"A layer of a neural network.\n\t        Consists of a connection layer and an activation function.\n\t    \"\"\"\n\t    connection_layer: str\n\t    activation_function: str\n\t    connection_layer_kwargs: Optional[dict] = field(default_factory=dict)\n\t    activation_function_kwargs: Optional[dict] = field(default_factory=dict)\n\t    @staticmethod\n", "    def from_config(layer_dict):\n\t        return LayerInfo(\n\t            connection_layer=layer_dict[\"connection_layer\"],\n\t            connection_layer_kwargs=layer_dict[\"connection_layer_kwargs\"],\n\t            activation_function=layer_dict[\"activation_function\"],\n\t            activation_function_kwargs=layer_dict[\"activation_function_kwargs\"],\n\t        )\n\t    def to_config(self):\n\t        return {\n\t            \"connection_layer\": self.connection_layer,\n", "            \"connection_layer_kwargs\": self.connection_layer_kwargs,\n\t            \"activation_function\": self.activation_function,\n\t            \"activation_function_kwargs\": self.activation_function_kwargs,\n\t        }\n\t@dataclass(frozen=True)\n\tclass NeuronID():\n\t    \"\"\"Index of a neuron in a neural network.\n\t    Consists of a layer label and a neuron index.\n\t    The input layer is labeled \"X\", the output layer \"Y\",\n\t    and the hidden layers \"L1\", \"L2\", ...\n", "    \"\"\"\n\t    layer: str\n\t    index: Union[int, Tuple[int, ...]]\n\t    @staticmethod\n\t    def to_yaml(dumper, data):\n\t        # Dumps a NeuronID to a string in YAML.\n\t        return dumper.represent_scalar(\"!NeuronID\", f\"layer={data.layer}, index={data.index}\")\n\t    @staticmethod\n\t    def from_yaml(loader, node):\n\t        # Loads a NeuronID from a string in YAML.\n", "        value = loader.construct_scalar(node)\n\t        layer=value.split(\"layer=\")[1].split(\",\")[0]\n\t        index=value.split(\"index=\")[1]\n\t        index=literal_eval(index)\n\t        return NeuronID(layer, index)\n\tNoAliasDumper.add_representer(NeuronID, NeuronID.to_yaml)\n\tyaml.SafeLoader.add_constructor(\"!NeuronID\", NeuronID.from_yaml)\n\tclass NeuralNetwork(nninfo.exp_comp.ExperimentComponent, nn.Module):\n\t    \"\"\"\n\t    Model that is trained and analysed.\n", "    CUDA acceleration is not implemented yet, but will certainly be possible in the future.\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n\t            layer_infos: List[LayerInfo],\n\t            init_str,\n\t            **kwargs\n\t    ):\n\t        \"\"\"\n\t        Creates a new instance of NeuralNetwork and sets all structural parameters of the model.\n", "        Important comment: the external indexing of the layers is 1,...,n for convenience.\n\t        However, I could not find a way to use this indexing also for the inner structure. Maybe\n\t        we might change that in the future to avoid errors.\n\t        Args:\n\t        Keyword Args:\n\t            noise_stddev (float): in case of a noisy neural network\n\t        \"\"\"\n\t        # call pytorch super class\n\t        super().__init__()\n\t        self._layer_infos = layer_infos\n", "        self._params = kwargs\n\t        self.n_layers = len(layer_infos)\n\t        self._connection_layers = []\n\t        self._activ_funcs = []\n\t        self._module_list = nn.ModuleList()\n\t        for layer_info in layer_infos:\n\t            try:\n\t                activation_function_factory = ACTIV_FUNCS_PYTORCH[layer_info.activation_function]\n\t            except KeyError:\n\t                raise ValueError(f\"Activation function {layer_info.activation_function} not supported.\")\n", "            activation_function = activation_function_factory(**layer_info.activation_function_kwargs)\n\t            self._activ_funcs.append(activation_function)\n\t            try:\n\t                connection_layer_factory = CONNECTION_LAYERS_PYTORCH[layer_info.connection_layer]\n\t            except KeyError:\n\t                raise ValueError(f\"Connection layer {layer_info.connection_layer} not supported.\")\n\t            connection_layer = connection_layer_factory(**layer_info.connection_layer_kwargs)\n\t            self._connection_layers.append(connection_layer)\n\t            if isinstance(connection_layer, nn.Module):\n\t                self._module_list.append(connection_layer)\n", "        self._initializer = None\n\t        self._init_str = init_str\n\t        self.init_weights()\n\t    @staticmethod\n\t    def from_config(config):\n\t        \"\"\"\n\t        Creates a NeuralNetwork instance from a config dictionary.\n\t        Args:\n\t            config (dict): Dictionary with all the necessary information to create a NeuralNetwork instance.\n\t        Returns:\n", "            NeuralNetwork: Instance of NeuralNetwork with the given configuration.\n\t        \"\"\"\n\t        config = config.copy()\n\t        net_type = config.pop(\"net_type\")\n\t        layer_configs = [LayerInfo.from_config(layer_info) for layer_info in config.pop(\"layers\")]\n\t        if net_type == \"feedforward\":\n\t            return NeuralNetwork(layer_configs, **config)\n\t        elif net_type == \"noisy_feedforward\":\n\t            return NoisyNeuralNetwork(layer_configs, **config)\n\t        raise KeyError(f\"Unknown network type '{net_type}'\")\n", "    def to_config(self):\n\t        \"\"\"\n\t        Is called when the experiment saves all its components at the beginning of an experiment.\n\t        Returns:\n\t            dict: Dictionary with all the network settings necessary to create the network again\n\t                for rerunning and later investigation.\n\t        \"\"\"\n\t        param_dict = {\n\t            \"layers\": [layer_info.to_config() for layer_info in self._layer_infos],\n\t            \"net_type\": \"feedforward\",\n", "            \"init_str\": self._init_str,\n\t        }\n\t        return copy.deepcopy(param_dict)\n\t    @cache\n\t    def get_layer_sizes(self):\n\t        \"\"\"\n\t        Returns:\n\t            list of int: List of layer output sizes.\n\t        \"\"\"\n\t        shapes = [self.parent.task.x_dim]\n", "        for i in range(1, self.n_layers):\n\t            layer_info = self._layer_infos[i]\n\t            if \"out_features\" in layer_info.connection_layer_kwargs:\n\t                shapes.append(layer_info.connection_layer_kwargs[\"out_features\"])\n\t            elif \"out_channels\" in layer_info.connection_layer_kwargs:\n\t                shapes.append(layer_info.connection_layer_kwargs[\"out_channels\"])\n\t            else:\n\t                shapes.append(None)\n\t    def get_layer_structure_dict(self):\n\t        layer_dict = dict()\n", "        for i, size in enumerate(self.get_layer_sizes()):\n\t            if i == 0:\n\t                layer_label = \"X\"\n\t            else:\n\t                layer_label = \"L\" + str(i)\n\t            neuron_id_list = []\n\t            for n in range(size):\n\t                neuron_id_list.append(NeuronID(layer_label, n + 1))\n\t            layer_dict[layer_label] = neuron_id_list\n\t        layer_label = \"Y\"\n", "        neuron_id_list = []\n\t        for n in range(self.get_layer_sizes()[-1]):\n\t            neuron_id_list.append(NeuronID(layer_label, n + 1))\n\t        layer_dict[layer_label] = neuron_id_list\n\t        return layer_dict\n\t    def get_binning_limits(self):\n\t        \"\"\"\n\t        returns: {neuron_id -> (low, high) or \"binary\"}\n\t        \"\"\"\n\t        structure = self.get_layer_structure_dict()\n", "        limit_dict = dict()\n\t        for layer_label in structure:\n\t            for neuron_id in structure[layer_label]:\n\t                if layer_label == \"X\" or layer_label == \"Y\":\n\t                    limit_dict[neuron_id] = self.parent.task.get_binning_limits(\n\t                        layer_label\n\t                    )\n\t                elif layer_label.startswith(\"L\"):\n\t                    i = int(layer_label.lstrip(\"L\"))\n\t                    limit_dict[neuron_id] = ACTIV_FUNCS_BINNING_LIMITS[\n", "                        self._layer_infos[i].activation_function\n\t                    ]\n\t                else:\n\t                    raise NotImplementedError(\n\t                        \"Value \" + layer_label + \"not allowed for layer_label.\"\n\t                    )\n\t        return limit_dict\n\t    def get_limits_list(self) -> List[Limits]:\n\t        \"\"\"\n\t        Currently returns None for the input Limits\n", "        returns: [(low, high) or \"binary\"]\n\t        \"\"\"\n\t        return [ACTIV_FUNCS_BINNING_LIMITS[layer_info.activation_function] for layer_info in self._layer_infos]\n\t    def forward(self, x, quantizers, return_activations=False, apply_output_softmax=None):\n\t        \"\"\"\n\t        Forward pass for the network, given a batch.\n\t        Args:\n\t            x (torch tensor): batch from dataset to be fed into the network\n\t        Returns:\n\t            torch tensor: output of the network\n", "        \"\"\"\n\t        if apply_output_softmax is None:\n\t            apply_output_softmax = return_activations\n\t        if return_activations:\n\t            activations = {}\n\t        for i in range(self.n_layers):\n\t            x = self._connection_layers[i](x)\n\t            x = self._activ_funcs[i](x)\n\t            if apply_output_softmax and self._layer_infos[i].activation_function == 'softmax_output':\n\t                x = torch.softmax(x, dim=1)\n", "            x = quantizers[i](x)\n\t            if return_activations:\n\t                activations[\"L\" + str(i + 1)] = x.detach().numpy()\n\t        if return_activations:\n\t            return x, activations\n\t        return x\n\t    def extract_activations(self, x, quantizer_params):\n\t        \"\"\"\n\t        Extracts the activities using the input given. Used by Analysis. Outputs\n\t        activities together in a dictionary (because of variable sizes of the layers).\n", "        Args:\n\t            x (torch tensor): batch from dataset to calculate the activities on. Typically\n\t                feed the entire dataset in one large batch.\n\t            before_noise (bool): In a noisyNN, sample before or after applying noise\n\t        Returns:\n\t            dict: dictionary with each neuron_id as key,\n\t                labeled \"L1\",..,\"L<n>\",\n\t                where \"L1\" corresponds to the output of the first layer and \"L<n>\" to\n\t                the output of the final layer. Notice that this indexing is different\n\t                from the internal layer structures indices (they are\n", "                uncomfortable to change).\n\t        \"\"\"\n\t        test_quantizers = quantizer_list_factory(\n\t            quantizer_params, self.get_limits_list())\n\t        with torch.no_grad():\n\t            _, activations = self.forward(x, quantizers=test_quantizers, return_activations=True)\n\t        return activations\n\t    def init_weights(self, randomize_seed=False):\n\t        \"\"\"\n\t        Initialize the weights using the init_str that was set at initialization\n", "        (one of the initializers provided in INITIALIZERS_PYTORCH).\n\t        Args:\n\t            randomize_seed (bool): If true, the torch seed is reset before the initialization.\n\t        \"\"\"\n\t        self._initializer = INITIALIZERS_PYTORCH[self._init_str]\n\t        if randomize_seed:\n\t            torch.seed()\n\t        self.apply(self._init_weight_fct)\n\t    def _init_weight_fct(self, m):\n\t        if isinstance(m, nn.Linear):\n", "            if self._init_str == \"xavier\":\n\t                self._initializer(m.weight, gain=5./3.)\n\t            else:\n\t                self._initializer(m.weight)\n\t        elif isinstance(m, nn.Conv2d):\n\t            if self._init_str == \"xavier\":\n\t                self._initializer(m.weight, gain=np.sqrt(2))\n\t            else:\n\t                self._initializer(m.weight)\n\t        elif isinstance(m, nn.Sequential):\n", "            if len(m) == 2 and isinstance(m[0], nn.Linear):\n\t                if self._init_str == \"xavier\":\n\t                    self._initializer(m[0].weight, gain=5./3.)\n\t                else:\n\t                    self._initializer(m[0].weight)\n\t        elif not isinstance(m, (nn.ModuleList, NeuralNetwork, nn.MaxPool2d, nn.Flatten)):\n\t            raise NotImplementedError(\n\t                f\"Weight initialization for {m} is not implemented.\"\n\t            )\n\t    def __call__(self, x, quantizers):\n", "        return self.forward(x, quantizers)\n\t    def get_input_output_dimensions(self):\n\t        input_dim = self.get_layer_sizes()[0]\n\t        output_dim = self.get_layer_sizes()[-1]\n\t        return input_dim, output_dim\n\t    def neuron_ids(self, only_real_neurons=False):\n\t        \"\"\"\n\t        Create a simple list of all nodes of the network\n\t        (including input, target, bias nodes).\n\t        Args:\n", "            only_real_neurons: Whether you want to only include neurons\n\t                whose ids begin with 'L'. Default is False\n\t        Returns:\n\t            list: neuron ids\n\t        \"\"\"\n\t        names = []\n\t        if not only_real_neurons:\n\t            names.append((\"B\", (1,)))\n\t        for layer_name, neurons in self.get_layer_structure_dict().items():\n\t            if (not only_real_neurons) or (\n", "                only_real_neurons and layer_name.startswith(\"L\")\n\t            ):\n\t                for neuron in neurons:\n\t                    names.append(neuron)\n\t        return names\n\t    def connectome(self):\n\t        \"\"\"\n\t        Returns:\n\t            an empty connectome matrix\n\t        \"\"\"\n", "        neuron_list = self.neuron_ids()\n\t        connectome = [[]]\n\t        connectome[0].append(\"input_neuron_ids\")\n\t        connectome[0].extend(neuron_list)\n\t        for neuron in neuron_list:\n\t            connectome.append([neuron])\n\t            connectome[-1].extend([float(-1) for i in range(len(neuron_list))])\n\t        return connectome\n\t    def trainable_parameters(self):\n\t        \"\"\"\n", "        Create a list of all trainable parameters.\n\t        Ugly code still.\n\t        Returns:\n\t            list: List of all trainable parameters.\n\t                dim 0: parameters\n\t                dim 1: input neuron_id, output neuron_id, parameter_id\n\t        \"\"\"\n\t        connectome = self.connectome()\n\t        param_list = []\n\t        param_index = 0\n", "        for name, param in self.named_parameters():\n\t            if param.requires_grad:\n\t                _, internal_index, wb = name.split(\".\")\n\t                layer_index = int(internal_index) + 1\n\t                if wb == \"weight\":\n\t                    for i in range(param.shape[0]):\n\t                        for j in range(param.shape[1]):\n\t                            input_layer, output_layer = (\n\t                                \"L\" + str(layer_index - 1),\n\t                                \"L\" + str(layer_index),\n", "                            )\n\t                            if input_layer == \"L0\":\n\t                                input_layer = \"X\"\n\t                            k = connectome[0].index((input_layer, (j + 1,)))\n\t                            l = connectome[0].index((output_layer, (i + 1,)))\n\t                            connectome[k][l] = param_index\n\t                            param_list.append(\n\t                                [connectome[0][k], connectome[0][l], param_index]\n\t                            )\n\t                            param_index += 1\n", "                elif wb == \"bias\":\n\t                    for i in range(param.shape[0]):\n\t                        k = connectome[0].index((\"B\", (1,)))\n\t                        l = connectome[0].index(\n\t                            (\"L\" + str(layer_index), (i + 1,)))\n\t                        connectome[k][l] = param_index\n\t                        param_list.append(\n\t                            [connectome[0][k], connectome[0][l], param_index]\n\t                        )\n\t                        param_index += 1\n", "        return param_list\n\tclass NoisyNeuralNetwork(NeuralNetwork):\n\t    def forward(self, x, return_activations=False, save_before_noise=False, quantizer=None, apply_output_softmax=False):\n\t        if return_activations:\n\t            activations = {}\n\t        for i in range(self.n_layers):\n\t            x = self.layers[i](x)\n\t            x = self._activ_funcs[i](x)\n\t            if apply_output_softmax and self._activ_func_str[i] == 'softmax_output':\n\t                x = torch.softmax(x, dim=1)\n", "            # add gaussian noise to the layer with stddev noise_stddev\n\t            if return_activations and save_before_noise:\n\t                activations[\"L\" + str(i + 1)] = x.detach().numpy()\n\t            # if i != self.n_layers - 1:\n\t            limits = ACTIV_FUNCS_BINNING_LIMITS[self._activ_func_str[i]]\n\t            sampled_noise = torch.empty(x.shape).normal_(\n\t                mean=0, std=self._params[\"noise_stddev\"] * (limits[1]-limits[0])\n\t            )\n\t            x = x + sampled_noise\n\t            if return_activations and not save_before_noise:\n", "                activations[\"L\" + str(i + 1)] = x.detach().numpy()\n\t        if return_activations:\n\t            return x, activations\n\t        return x\n\t    def to_config(self):\n\t        param_dict = super.to_config()\n\t        param_dict[\"noise_stddev\"] = self._params[\"noise_stddev\"]\n\t        return param_dict\n"]}
{"filename": "nninfo/analysis/binning.py", "chunked_list": ["import numpy as np\n\tfrom tqdm import tqdm\n\tfrom abc import ABC, abstractmethod\n\tfrom collections import Counter\n\tfrom scipy.stats import norm\n\tfrom sxpid import SxPID\n\tfrom nninfo.config import CLUSTER_MODE\n\t__all__ = [\"Binning\"]\n\tclass Binning(ABC):\n\t    \"\"\"\n", "    Used for discretizing continuous activation data and produces a pmf\n\t    from the discretized values.\n\t    \"\"\"\n\t    def __init__(self, groupings, **kwargs):\n\t        \"\"\"\n\t        Args:\n\t            groupings:  List of PID variables, each defined as a list of neuron ids\n\t        \"\"\"\n\t        self._params = kwargs\n\t        self._groupings = groupings\n", "        self._counter = None\n\t        self.reset()\n\t    def reset(self):\n\t        self._counter = Counter()\n\t    @classmethod\n\t    def from_binning_method(\n\t        cls, binning_method, groupings, source_id_lists, target_id_list, **kwargs\n\t    ):\n\t        if binning_method == \"fixed_size\":\n\t            return FixedSizeBinning(groupings, **kwargs)\n", "        elif binning_method == \"goldfeld:fixed_size\":\n\t            return GoldfeldBinning(source_id_lists, target_id_list, **kwargs)\n\t        elif binning_method == \"none\":\n\t            return NoneBinnig(groupings, **kwargs)\n\t        else:\n\t            raise NotImplementedError\n\t    def apply(self, activations):\n\t        \"\"\"\n\t        Bins a batch of activations\n\t        activations: {neuron_id -> [activations]}\n", "        groupings: [[neuron_ids_source1/target], [neuron_ids_source2], ..., [neuron_ids_sourceN/target]]\n\t        \"\"\"\n\t        grouped_activations = zip(\n\t            *[\n\t                zip(\n\t                    *[\n\t                        self.discretize(activations[neuron_id], neuron_id)\n\t                        for neuron_id in grouping\n\t                    ]\n\t                )\n", "                for grouping in self._groupings\n\t            ]\n\t        )\n\t        self._counter.update(grouped_activations)\n\t    def get_pdf(self):\n\t        total = sum(self._counter.values())\n\t        return {rlz: count / total for (rlz, count) in self._counter.items()}\n\t    def get_params(self):\n\t        return {**self._params, \"binning_method\": self.get_binning_method()}\n\t    @abstractmethod\n", "    def discretize(self, activations, neuron_id):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def get_binning_method(self):\n\t        raise NotImplementedError\n\tclass NoneBinnig(Binning):\n\t    \"\"\"\n\t    Binning method that expects already discretized values and therefore skips the discretization step\n\t    \"\"\"\n\t    def __init__(self, groupings, **kwargs):\n", "        \"\"\"\n\t        Expected kwargs:\n\t            limits: {neuron_id -> (lower_limit, upper_limit) or \"binary\"}\n\t            n_bins: number of bins\n\t        \"\"\"\n\t        super().__init__(groupings, **kwargs)\n\t    def get_binning_method(self):\n\t        return \"none\"\n\t    def discretize(self, activations, neuron_id):\n\t        return activations\n", "class FixedSizeBinning(Binning):\n\t    def __init__(self, groupings, **kwargs):\n\t        \"\"\"\n\t        Expected kwargs:\n\t            limits: {neuron_id -> (lower_limit, upper_limit) or \"binary\"}\n\t            n_bins: number of bins\n\t        \"\"\"\n\t        super().__init__(groupings, **kwargs)\n\t    def get_binning_method(self):\n\t        return \"fixed_size\"\n", "    def discretize(self, activations, neuron_id):\n\t        \"\"\"\n\t        Args:\n\t            activations (numpy array): all activations\n\t            neuron_id: id of the neuron on which discretization should be performed\n\t        Returns: (numpy array) discretized version of the activations of neuron\n\t            with neuron_id\n\t        \"\"\"\n\t        limits = self._params[\"limits\"][neuron_id]\n\t        if limits == \"binary\":\n", "            activations = activations.astype(np.int64)\n\t            return activations\n\t        elif isinstance(limits, tuple) and limits[0] != -np.inf and limits[1] != np.inf:\n\t            binning_range = limits[1] - limits[0]\n\t            bin_size = binning_range / self._params[\"n_bins\"]\n\t            discretized = np.floor(activations / bin_size).astype(\"int\")\n\t        else:\n\t            raise NotImplementedError(\n\t                \"Applying discretization to layer with \"\n\t                + str(limits)\n", "                + \" limits is not possible\"\n\t            )\n\t        return discretized\n\tclass GoldfeldBinning(FixedSizeBinning):\n\t    def __init__(self, source_id_lists, target_id_list, **kwargs):\n\t        super().__init__(None, **kwargs)\n\t        \"\"\"\n\t        All sources are expected to be noisy, the target is assumed non-noisy.\n\t        Expected kwargs:\n\t            std_dev: standard deviation of gaussians\n", "            limits: {neuron_id -> (lower_limit, upper_limit) or \"binary\"}\n\t            n_bins: number of bins inside limits\n\t            extra_bin_factor: Determines how many bins to add outside of limits as 2 * ceil(bin_width * std_dev / bin_size)\n\t            prob_threshold: Events with probability mass lower than this threshold are excluded from PID for performance. pmf is normalized after exclusion.\n\t        \"\"\"\n\t        self._source_id_lists = source_id_lists\n\t        self._target_id_list = target_id_list\n\t        beta = self._params[\"std_dev\"]\n\t        self._bin_centers = []\n\t        extra_bin_factor = self._params[\"extra_bin_factor\"]\n", "        # Calculate center positions of bins of relevant neurons (i.e. those which appear in a grouping)\n\t        for grouping in source_id_lists:\n\t            for neuron_id in grouping:\n\t                limits = self._params[\"limits\"][neuron_id]\n\t                binning_range = limits[1] - limits[0]\n\t                bin_size = binning_range / self._params[\"n_bins\"]\n\t                n_extra_bins = int(np.ceil(extra_bin_factor * beta / bin_size))\n\t                neuron_bin_centers = (\n\t                    np.linspace(\n\t                        limits[0] - n_extra_bins * bin_size,\n", "                        limits[1] + n_extra_bins * bin_size,\n\t                        self._params[\"n_bins\"] + 2 * n_extra_bins,\n\t                        endpoint=False,\n\t                    )\n\t                    + bin_size / 2\n\t                )\n\t                self._bin_centers.append(neuron_bin_centers)\n\t        # Dictionary from target tuples to full ndarray source pmfs.\n\t        self._probability_dict = {}\n\t        # Full unnormalized pmf for all bin combinations\n", "        self._accumulated_probabilites = np.zeros(\n\t            shape=tuple(len(nbc) for nbc in self._bin_centers)\n\t        )\n\t        # Initialize normal distribution\n\t        self._normal = norm(loc=0, scale=beta).pdf\n\t    def get_binning_method(self):\n\t        return \"goldfeld:fixed_size\"\n\t    def apply(self, activations):\n\t        \"\"\"\n\t        Apply Goldfeld binnning to a batch of activations, i.e. evaluate multidimensional Gaussians \n", "        with the distance of the sample from the bin centers.\n\t        \"\"\"\n\t        # First digitize the non-noisy target varible\n\t        digitized_target = list(\n\t            zip(\n\t                *[\n\t                    self.discretize(activations[neuron_id], neuron_id)\n\t                    for neuron_id in self._target_id_list\n\t                ]\n\t            )\n", "        )\n\t        # Find relevant activations source, i.e. from neurons of interest for the requested PID groupings\n\t        relevant_activations = np.hstack(\n\t            tuple(\n\t                activations[neuron_id][:, np.newaxis]\n\t                for grouping in self._source_id_lists\n\t                for neuron_id in grouping\n\t            )\n\t        )\n\t        # For each sample, calculate the contributions to the accumulated_probabilities by\n", "        # evaluating 1D Gaussians for the dimensions separately and (outer) multiplying them.\n\t        # This exploits the fact that our evaluation points are on a cartesian grid and the Gaussian\n\t        # Factorizes over dimensions.\n\t        for i, sample in tqdm(\n\t            enumerate(relevant_activations),\n\t            total=len(relevant_activations),\n\t            disable=CLUSTER_MODE,\n\t        ):  # vectorize!\n\t            sample_contributions = 1\n\t            for neuron_sample, neuron_bin_centers in zip(sample, self._bin_centers):\n", "                neuron_marginal = self._normal(neuron_bin_centers - neuron_sample)\n\t                sample_contributions = np.multiply.outer(\n\t                    sample_contributions, neuron_marginal\n\t                )\n\t            if digitized_target[i] in self._probability_dict:\n\t                self._probability_dict[digitized_target[i]] += sample_contributions\n\t            else:\n\t                self._probability_dict[digitized_target[i]] = sample_contributions\n\t    def get_pdf(self):\n\t        \"\"\"\n", "        Applies the threshold, groups dimensions of _accumulated_probabilities according to groupings,\n\t        reencodes the accumulated_probabilities in a sparse matrix format (COO)\n\t        and normalizes it.\n\t        \"\"\"\n\t        # Find that a full pmf array would have after applying the variable grouping for PID analysis\n\t        grouped_shape = ()\n\t        i = 0\n\t        for grouping in self._source_id_lists:\n\t            group_bins = 1\n\t            for _ in grouping:\n", "                group_bins *= len(self._bin_centers[i])\n\t                i += 1\n\t            grouped_shape += (group_bins,)\n\t        grouped_shape += (len(self._probability_dict),)\n\t        # Create coordinate array according to shape and flatten it\n\t        coords = (\n\t            np.indices(grouped_shape).reshape(len(grouped_shape), -1).T\n\t        )  # shape=(n_samples, n_PID_variables)\n\t        flattened_pmf = np.stack(\n\t            list(self._probability_dict.values()), axis=-1\n", "        ).flatten()  # shape=n_samples\n\t        # normalize\n\t        flattened_pmf = flattened_pmf / flattened_pmf.sum()\n\t        # Apply threshold or find it first\n\t        if \"prob_threshold\" in self._params:\n\t            eps = self._params[\"prob_threshold\"]\n\t        else:\n\t            q = self._params[\n\t                \"min_incl_prob\"\n\t            ]  # Minimum total probability to be included in the PID\n", "            eps = self.find_thr(flattened_pmf, q)\n\t        above_threshold_indices = np.nonzero(flattened_pmf >= eps)\n\t        filtered_coords = coords[above_threshold_indices]\n\t        filtered_pmf = flattened_pmf[above_threshold_indices]\n\t        print(filtered_pmf.sum())\n\t        print(filtered_pmf.sum() - filtered_pmf.min())\n\t        # Normalize\n\t        normalized_pmf = filtered_pmf / np.sum(filtered_pmf)\n\t        print(\"Remaining total pmf:\", np.sum(filtered_pmf) / np.sum(flattened_pmf))\n\t        # Create SxPID pdf directly, without creating dictionary first. Make coords F-contiguous for performance reasons.\n", "        return SxPID.PDF(np.asfortranarray(filtered_coords), normalized_pmf)\n\t    def find_thr(self, a, q, bias_l=0, bias_g=0, first_call=True):\n\t        \"\"\"\n\t        Args:\n\t            a: normalized array of weights\n\t            q: quantile\n\t        Returns:\n\t            thr: largest threshold such that a[a >= thr].sum() >= q\n\t        \"\"\"\n\t        if len(a) == 1:\n", "            return a[0]\n\t        if len(a) == 2:\n\t            min = np.min(a)\n\t            max = np.max(a)\n\t            return max if max + bias_g >= q else min\n\t        pivot_idx = int(q * len(a)) if first_call else int(len(a) / 2)\n\t        partitioned = np.partition(a, kth=pivot_idx, axis=None)\n\t        sum_l = np.sum(partitioned[:pivot_idx]) + bias_l\n\t        sum_g = np.sum(partitioned[pivot_idx + 1 :]) + bias_g\n\t        if sum_l < 1 - q and sum_g < q:\n", "            return partitioned[pivot_idx]\n\t        if sum_g > q:\n\t            return self.find_thr(\n\t                partitioned[pivot_idx:],\n\t                q,\n\t                bias_l=sum_l,\n\t                bias_g=bias_g,\n\t                first_call=False,\n\t            )\n\t        else:\n", "            return self.find_thr(\n\t                partitioned[: pivot_idx + 1],\n\t                q,\n\t                bias_l=bias_l,\n\t                bias_g=sum_g,\n\t                first_call=False,\n\t            )\n"]}
{"filename": "nninfo/analysis/measurement.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import ClassVar, Literal\n\timport filelock\n\timport pandas as pd\n\timport yaml\n\tfrom ..file_io import NoAliasDumper\n\tfrom ..experiment import Experiment\n\tclass Measurement(ABC):\n\t    MEASUREMENT_CONFIG_FILE_NAME = \"config.yaml\"\n\t    MEASUREMENT_RESULTS_FILE_NAME = \"results.h5\"\n", "    MEASUREMENT_RESULTS_FILE_LOCK = \"results.lock\"\n\t    measurement_type: ClassVar[str]\n\t    def __init__(self,\n\t                 experiment: Experiment,\n\t                 measurement_id: str,\n\t                 dataset_name: str,\n\t                 dataset_kwargs: dict = None,\n\t                 quantizer_params: list[dict] = None,\n\t                 _load: bool = False):\n\t        self._experiment = experiment\n", "        self._measurement_id = measurement_id\n\t        self._quantizer_params = quantizer_params\n\t        self._dataset_name = dataset_name\n\t        self._dataset_kwargs = dataset_kwargs or {}\n\t        self._measurement_dir = experiment.experiment_dir / measurement_id\n\t        self._config_file = self._measurement_dir / self.MEASUREMENT_CONFIG_FILE_NAME\n\t        self._results_file = self._measurement_dir / self.MEASUREMENT_RESULTS_FILE_NAME\n\t        self._results_file_lock = self._measurement_dir / \\\n\t            self.MEASUREMENT_RESULTS_FILE_LOCK\n\t        if not _load:\n", "            self._create_dir_and_config()\n\t    def _create_dir_and_config(self):\n\t        try:\n\t            self._measurement_dir.mkdir(parents=True, exist_ok=False)\n\t        except FileExistsError:\n\t            if self._config_file.exists():\n\t                config = Measurement._load_yaml(self._config_file)\n\t                if config == self.to_config():\n\t                    return\n\t            raise FileExistsError(\n", "                \"The measurement directory already exists and the config does not match.\")\n\t        with open(self._config_file, \"w\") as f:\n\t            yaml.dump(self.to_config(), f, Dumper=NoAliasDumper)\n\t    def to_config(self):\n\t        return {\n\t            \"measurement_type\": self.measurement_type,\n\t            \"experiment_id\": self._experiment.id,\n\t            \"measurement_id\": self._measurement_id,\n\t            \"dataset_name\": self._dataset_name,\n\t            \"dataset_kwargs\": self._dataset_kwargs,\n", "            \"quantizer_params\": self._quantizer_params\n\t        }\n\t    @classmethod\n\t    def from_config(cls, experiment, config):\n\t        config = config.copy()\n\t        assert config.pop(\"measurement_type\") == cls.measurement_type, \\\n\t            \"The measurement type in the config does not match the measurement type of the class.\"\n\t        assert config.pop(\"experiment_id\") == experiment.id, \\\n\t            \"The experiment id in the config does not match the experiment id of the experiment.\"\n\t        return cls(experiment=experiment, **config, _load=True)\n", "    @staticmethod\n\t    def _load_yaml(file):\n\t        with open(file, \"r\") as f:\n\t            return yaml.safe_load(f)\n\t    @classmethod\n\t    def load(cls, experiment: Experiment, measurement_id: str):\n\t        config_file = experiment.experiment_dir / \\\n\t            measurement_id / cls.MEASUREMENT_CONFIG_FILE_NAME\n\t        config = cls._load_yaml(config_file)\n\t        return cls.from_config(experiment, config)\n", "    def save_results(self, run_id, chapter_id, results_df):\n\t        \"\"\"Appends the results to the hdf5 file in the corresponding measurement directory.\n\t        \"\"\"\n\t        # Add columns for run_ids, chapter_ids and epoch_ids\n\t        results_df.insert(loc=0, column=\"run_id\", value=run_id)\n\t        results_df.insert(loc=1, column=\"chapter_id\", value=chapter_id)\n\t        results_df.insert(loc=2, column=\"epoch_id\",\n\t                            value=self._experiment.schedule.get_epoch_for_chapter(chapter_id))\n\t        with filelock.FileLock(self._results_file_lock):\n\t            with pd.HDFStore(self._results_file) as store:\n", "                store.put(\"results\", results_df,\n\t                          append=True, format=\"table\")\n\t    def perform_measurements(self, run_ids: Literal['all'] | int | list[int], chapter_ids: Literal['all'] | int | list[int], exists_ok: bool = True):\n\t        checkpoint_ids = self._experiment.checkpoint_manager.list_all_checkpoints()\n\t        if not run_ids == 'all':\n\t            if isinstance(run_ids, int):\n\t                run_ids = [run_ids]\n\t            # Filter out checkpoint_ids that are not in run_ids\n\t            checkpoint_ids = [c for c in checkpoint_ids if c[0] in run_ids]\n\t        if not chapter_ids == 'all':\n", "            if isinstance(chapter_ids, int):\n\t                chapter_ids = [chapter_ids]\n\t            # Filter out checkpoint_ids that are not in chapter_ids\n\t            checkpoint_ids = [c for c in checkpoint_ids if c[1] in chapter_ids]\n\t        print(\n\t            f\"Performing measurements for {len(checkpoint_ids)} checkpoints: {checkpoint_ids}.\")\n\t        # Perform measurements for all run_ids and chapter_ids sequentially\n\t        for checkpoint_id in checkpoint_ids:\n\t            self.perform_measurement(checkpoint_id[0], checkpoint_id[1], exists_ok=exists_ok)\n\t    def perform_measurement(self, run_id: int, chapter_id: int, exists_ok: bool = True):\n", "        if not self._results_file.exists():\n\t            self._create_results_file()\n\t        # Check if there is already a result for this run_id and chapter_id\n\t        if self._check_if_result_exists(run_id, chapter_id):\n\t            if exists_ok:\n\t                print(\n\t                    f\"There is already a {self.measurement_type!r} result for run_id {run_id} and chapter_id {chapter_id}. Skipping.\")\n\t                return\n\t            raise ValueError(\n\t                f\"There is already a {self.measurement_type!r} result for run_id {run_id} and chapter_id {chapter_id}. Aborting.\")\n", "        print(\n\t            f\"Performing {self.measurement_type!r} measurement for run_id {run_id} and chapter_id {chapter_id}.\")\n\t        results = self._measure(run_id, chapter_id)\n\t        self.save_results(run_id, chapter_id, results)\n\t    def _create_results_file(self):\n\t        \"\"\"Creates the results file.\n\t        \"\"\"\n\t        with filelock.FileLock(self._results_file_lock):\n\t            with pd.HDFStore(self._results_file) as store:\n\t                store.put(\"results\", pd.DataFrame(),\n", "                          append=True, format=\"table\")\n\t    def _check_if_result_exists(self, run_id: int, chapter_id: int):\n\t        results = self.results\n\t        if results[(results[\"run_id\"] == run_id) & (results[\"chapter_id\"] == chapter_id)].shape[0] > 0:\n\t            return True\n\t    @abstractmethod\n\t    def _measure(self, *args, **kwargs):\n\t        raise NotImplementedError\n\t    @property\n\t    def results(self):\n", "        with filelock.FileLock(self._results_file_lock):\n\t            with pd.HDFStore(self._results_file) as store:\n\t                try:\n\t                    results_df = store.select(\"results\")\n\t                except KeyError:\n\t                    results_df = pd.DataFrame(columns=[\"run_id\", \"chapter_id\", \"epoch_id\"])\n\t        return results_df\n"]}
{"filename": "nninfo/analysis/__init__.py", "chunked_list": ["from .binning import *\n\tfrom .pid_measurement import *\n\tfrom .performance_measurement import *\n"]}
{"filename": "nninfo/analysis/reing.py", "chunked_list": ["\"\"\"Implementation of Reing et al. (2021) directed local differences between cohesion measures.\n\t\"\"\"\n\timport numpy as np\n\timport scipy.stats as stats\n\tfrom math import comb\n\tfrom itertools import chain, combinations\n\timport sxpid\n\tdef _get_entropy(pdf, var_id_list):\n\t    \"\"\"Calculate the entropy of a variable.\n\t    Parameters\n", "    ----------\n\t    pdf_dict : dict\n\t        Dictionary with keys being realization tuples and values their probability\n\t    var_id_list : list\n\t        List of variable IDs.\n\t    Returns\n\t    -------\n\t    float\n\t        Entropy of the variable.\n\t    \"\"\"\n", "    delete_id_list = [i for i in range(pdf.nVar) if i not in var_id_list]\n\t    marginalized_pdf = pdf.marginalize(*delete_id_list)\n\t    return stats.entropy(marginalized_pdf.probs, base=2)\n\tdef powerset(iterable):\n\t    \"\"\"Return the powerset of an iterable.\n\t    Parameters\n\t    ----------\n\t    iterable : iterable\n\t        Iterable to calculate the powerset of.\n\t    Returns\n", "    -------\n\t    list\n\t        List of all subsets of the iterable.\n\t    \"\"\"\n\t    s = list(iterable)\n\t    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\tdef compute_C_k(pdf_dict, k):\n\t    \"\"\"This definition is sligntly different than in the paper\n\t        compute_C_k(k) is equivalent to C_k(k+1) in the paper (Equation 9)\n\t        But the paper is wrong, because it uses C_k(k) in equation 10\n", "    \"\"\"\n\t    n = len(next(iter(pdf_dict.keys()))) - 1\n\t    # Create SxPID Pdf\n\t    pdf = sxpid.SxPID.PDF.from_dict(pdf_dict)\n\t    # Find all combinations of length k of the variables\n\t    source_combinations = list(combinations(range(n), k))\n\t    target_index = n\n\t    C_k = 1/comb(n, k) * sum([_get_entropy(pdf, source_combination + (target_index,))\n\t                            - _get_entropy(pdf, source_combination) for source_combination in source_combinations])\n\t    return C_k\n", "def compute_C_k_diffs(pdf_dict):\n\t    n = len(next(iter(pdf_dict.keys()))) - 1\n\t    C_k = [compute_C_k(pdf_dict, k) for k in range(0, n + 1)]\n\t    C_k_diffs = {f'C({k-1}||{k})':C_k[k-1] - C_k[k] for k in range(1, n + 1)}\n\t    return C_k_diffs"]}
{"filename": "nninfo/analysis/performance_measurement.py", "chunked_list": ["import pandas as pd\n\tfrom .measurement import Measurement\n\tfrom ..experiment import Experiment\n\tfrom .. import logger\n\tlog = logger.get_logger(__name__)\n\tclass PerformanceMeasurement(Measurement):\n\t    measurement_type = \"performance\"\n\t    \"\"\"\n\t    Computes loss and accuracy for a given dataset.\n\t    \"\"\"\n", "    def __init__(self,\n\t                 experiment: Experiment,\n\t                 dataset_names: str,\n\t                 measurement_id: str = 'performance',\n\t                 dataset_kwargs: dict = None,\n\t                 quantizer_params=None,\n\t                 _load=False,\n\t                 ):\n\t        self._dataset_names = dataset_names\n\t        super().__init__(\n", "            experiment=experiment,\n\t            measurement_id=measurement_id,\n\t            dataset_name=None,\n\t            dataset_kwargs=dataset_kwargs,\n\t            quantizer_params=quantizer_params,\n\t            _load=_load\n\t        )\n\t    def to_config(self):\n\t        config = super().to_config()\n\t        config[\"dataset_names\"] = self._dataset_names\n", "        return config\n\t    @classmethod\n\t    def from_config(cls, experiment, config):\n\t        config = config.copy()\n\t        del config[\"dataset_name\"]\n\t        return super().from_config(experiment, config)\n\t    def _measure(self, run_id, chapter_id):\n\t        self._experiment.load_checkpoint(run_id, chapter_id)\n\t        dfs = []\n\t        for dataset_name in self._dataset_names:\n", "            loss, acc = self._experiment.tester.compute_loss_and_accuracy(\n\t                dataset_name, self._quantizer_params)\n\t            dfs.append(pd.DataFrame({'loss': [loss], 'accuracy': [acc]}))\n\t        df = pd.concat(dfs, axis=1, keys=self._dataset_names)\n\t        return df\n"]}
{"filename": "nninfo/analysis/pid_measurement.py", "chunked_list": ["import copy\n\timport pandas as pd\n\tfrom sxpid import SxPID\n\tfrom broja2pid import BROJA_2PID\n\tfrom .measurement import Measurement\n\tfrom .binning import Binning\n\tfrom ..config import N_WORKERS, CLUSTER_MODE\n\tfrom ..experiment import Experiment\n\tfrom ..model.neural_network import NeuronID\n\tfrom .. import logger\n", "from .reing import compute_C_k_diffs\n\tlog = logger.get_logger(__name__)\n\tclass PIDMeasurement(Measurement):\n\t    measurement_type = \"pid\"\n\t    \"\"\"\n\t    A single PID Measurement, that can be saved into the file structure after computation.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 experiment: Experiment,\n\t                 measurement_id: str,\n", "                 dataset_name: str,\n\t                 pid_definition: str,\n\t                 target_id_list: list[NeuronID],\n\t                 source_id_lists: list[list[NeuronID]],\n\t                 binning_kwargs,\n\t                 pid_kwargs: dict = None,\n\t                 dataset_kwargs: dict = None,\n\t                 quantizer_params=None,\n\t                 _load=False,\n\t                 ):\n", "        self._pid_definition = pid_definition\n\t        self._pid_kwargs = pid_kwargs or {}\n\t        self.T = sorted(target_id_list)\n\t        self.S = [sorted(s) for s in source_id_lists]\n\t        self._n_sources = len(source_id_lists)\n\t        self._binning_kwargs = binning_kwargs or {}\n\t        super().__init__(\n\t            experiment=experiment,\n\t            measurement_id=measurement_id,\n\t            dataset_name=dataset_name,\n", "            dataset_kwargs=dataset_kwargs,\n\t            quantizer_params=quantizer_params,\n\t            _load=_load\n\t        )\n\t        binning_kwargs_copy = copy.deepcopy(binning_kwargs)\n\t        binning_method = binning_kwargs_copy.pop(\"binning_method\")\n\t        self._binning = Binning.from_binning_method(\n\t            binning_method,\n\t            source_id_lists + [target_id_list],\n\t            source_id_lists,\n", "            target_id_list,\n\t            **binning_kwargs_copy\n\t        )\n\t    def to_config(self):\n\t        config = super().to_config()\n\t        config[\"pid_definition\"] = self._pid_definition\n\t        config[\"pid_kwargs\"] = self._pid_kwargs\n\t        config[\"binning_kwargs\"] = self._binning_kwargs\n\t        config[\"target_id_list\"] = self.T\n\t        config[\"source_id_lists\"] = self.S\n", "        return config\n\t    def _measure(self, run_id, chapter_id):\n\t        # get the activations for the current run_id and chapter_id\n\t        activations_iter = self._experiment.capture_activations(\n\t            dataset=self._dataset_name,\n\t            run_id=run_id,\n\t            chapter_id=chapter_id,\n\t            repeat_dataset=self._dataset_kwargs.get(\"repeat_dataset\", 1),\n\t            before_noise=self._binning_kwargs.get(\"before_noise\", False),\n\t            quantizer_params=self._quantizer_params,\n", "        )\n\t        self._binning.reset()\n\t        for activations in activations_iter:\n\t            self._binning.apply(activations)\n\t        pdf_dict = self._binning.get_pdf()\n\t        try:\n\t            pid_function = self._pid_measures[self._pid_definition]\n\t        except KeyError:\n\t            raise NotImplementedError(\n\t                f\"PID definition {self._pid_definition} is not implemented.\"\n", "            )\n\t        results = pid_function(self, pdf_dict, **self._pid_kwargs)\n\t        return results\n\t    def _perform_sxpid(self, sxpid_pdf):\n\t        results = SxPID.pid(\n\t            sxpid_pdf, verbose=0 if CLUSTER_MODE else 2, n_threads=N_WORKERS, pointwise=False\n\t        )\n\t        # Convert results to a row in a dataframe. The multiindex column names are (\"informative\"/\"misinformative\"/\"average\", str(antichain))\n\t        inf = pd.DataFrame({str(k): [v[0]] for k, v in results.items()})\n\t        misinf = pd.DataFrame({str(k): [v[1]] for k, v in results.items()})\n", "        avg = pd.DataFrame({str(k): [v[2]] for k, v in results.items()})\n\t        return pd.concat([inf, misinf, avg], axis=1, keys=['inf_pid', 'misinf_pid', 'avg_pid'])\n\t    def _perform_brojapid(self, pdf_dict):\n\t        # BROJA-PID expects the target to be the first variable:\n\t        broja_pdf_dict = {(key[-1],) + key[:-1]\n\t                           : value for key, value in pdf_dict.items()}\n\t        results = BROJA_2PID.pid(\n\t            broja_pdf_dict, cone_solver=\"ECOS\"\n\t        )\n\t        avg_dict = {\n", "            '((1, 2,),)': [results[\"CI\"]],\n\t            '((1,),)': [results[\"UIY\"]],\n\t            '((2,),)': [results[\"UIZ\"]],\n\t            '((1,), (2,))': [results[\"SI\"]]\n\t        }\n\t        return pd.DataFrame(avg_dict)\n\t    def _perform_reing(self, reing_pdf):\n\t        results_dict = compute_C_k_diffs(reing_pdf)\n\t        return pd.DataFrame({str(k): [v] for k, v in results_dict.items()})\n\t    _pid_measures = {'sxpid': _perform_sxpid,\n", "                     'brojapid': _perform_brojapid,\n\t                     'reing': _perform_reing}\n"]}
{"filename": "nninfo/tasks/combined_mnist_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass CombinedMnistTask(Task):\n\t    task_id = \"combined_mnist_1d_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 20\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        fmnist = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=True)\n\t        fmnist_test = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=False)\n\t        x = torch.cat([mnist.data, fmnist.data, mnist_test.data,\n", "                       fmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, fmnist.targets + 10, mnist_test.targets,\n\t                       fmnist_test.targets + 10])\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/mnist_eight_binary_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass MnistEightBinaryTask(Task):\n\t    \"\"\"Mnist task but only with digits from 0-7\n\t    len train: 48200\n\t    len test: 48275\n\t    \"\"\"\n\t    task_id = \"mnist8_binary_dat\"\n\t    @property\n", "    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n\t        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n", "        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 3\n\t    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        qmnist_test = torchvision.datasets.QMNIST(\n", "            root=\"../\", what='test50k', download=True, train=False)\n\t        x = torch.cat([mnist.data, mnist_test.data,\n\t                       qmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, mnist_test.targets,\n\t                       qmnist_test.targets[:, 0]])\n\t        # Filter out digits 8 and 9\n\t        filter_mask = (y != 8) & (y != 9)\n\t        y = y[filter_mask]\n\t        x = x[filter_mask]\n\t        y_binary = self.binary(y, 3)\n", "        return x.type(torch.float32), y_binary.type(torch.float32)\n\t    def binary(self, x, bits):\n\t        mask = 2**torch.arange(bits).to(x.device, x.dtype)\n\t        return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()"]}
{"filename": "nninfo/tasks/mnist_shuffled.py", "chunked_list": ["import torch\n\timport torchvision\n\timport numpy as np\n\tfrom .task import Task\n\tclass Mnist1DShuffledTask(Task):\n\t    task_id = \"mnist_1d_shuffled_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n", "    def x_limits(self):\n\t        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n", "        return 10\n\t    def load_samples(self, component_dir):\n\t        mnist = torchvision.datasets.MNIST(root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(root=\"../\", download=True, train=False)\n\t        qmnist_test = torchvision.datasets.QMNIST(root=\"../\", what='test50k', download=True, train=False)\n\t        x = torch.cat([mnist.data, mnist_test.data, qmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, mnist_test.targets, qmnist_test.targets[:,0]])\n\t        # Shuffle y labels for test and train set separately\n\t        rng = np.random.default_rng(42 + self._kwargs[\"seed\"])\n\t        y[:60_000] = torch.tensor(rng.permutation(y[:60_000]))\n", "        y[60_000:] = torch.tensor(rng.permutation(y[60_000:]))\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/combined_mnist_octal_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task, octal_encode_label\n\tclass CombinedMnistOctalTask(Task):\n\t    task_id = \"combined_mnist_octal_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 2\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        fmnist = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=True)\n\t        fmnist_test = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=False)\n\t        x = torch.cat([mnist.data, fmnist.data, mnist_test.data,\n", "                       fmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, fmnist.targets + 10, mnist_test.targets,\n\t                       fmnist_test.targets + 10])\n\t        y_binary = octal_encode_label(y, 2)\n\t        return x.type(torch.float32), y_binary.type(torch.float32)"]}
{"filename": "nninfo/tasks/tishby_task.py", "chunked_list": ["import torch\n\tfrom ..file_io import FileManager\n\tfrom .task import Task\n\tclass TishbyTask(Task):\n\t    task_id = \"tishby_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return \"binary\"\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 12\n\t    @property\n\t    def y_dim(self):\n\t        return 1\n", "    def load_samples(self):\n\t        self._data_location = \"../data/Tishby_2017/\"\n\t        dataset_reader = FileManager(\n\t            self._data_location, read=True)\n\t        data_dict = dataset_reader.read(\"var_u.mat\")\n\t        x = data_dict[\"F\"]\n\t        y = data_dict[\"y\"].T\n\t        return torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)"]}
{"filename": "nninfo/tasks/task.py", "chunked_list": ["import numpy as np\n\timport torch.utils.data\n\timport torchvision\n\tfrom abc import ABC, abstractmethod\n\timport nninfo\n\tlog = nninfo.logger.get_logger(__name__)\n\tclass Task(ABC):\n\t    def __init__(self, **kwargs):\n\t        self._kwargs = kwargs\n\t    _subclasses = {}\n", "    @classmethod\n\t    def __init_subclass__(cls, **kwargs):\n\t        super().__init_subclass__(**kwargs)\n\t        cls._subclasses[cls.task_id] = cls\n\t    @staticmethod\n\t    def list_available_tasks():\n\t        return list(Task._subclasses.keys())\n\t    @property\n\t    def kwargs(self):\n\t        return self._kwargs\n", "    @property\n\t    @abstractmethod\n\t    def finite(self):\n\t        raise NotImplementedError\n\t    @property\n\t    @abstractmethod\n\t    def task_id(self):\n\t        raise NotImplementedError\n\t    @property\n\t    @abstractmethod\n", "    def x_limits(self):\n\t        raise NotImplementedError\n\t    @property\n\t    @abstractmethod\n\t    def y_limits(self):\n\t        raise NotImplementedError\n\t    @property\n\t    @abstractmethod\n\t    def x_dim(self):\n\t        \"\"\"\n", "        Returns the dimension of the feature component of a single sample\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @property\n\t    @abstractmethod\n\t    def y_dim(self):\n\t        \"\"\"\n\t        Returns the dimension of the label component of a single sample\n\t        \"\"\"\n\t        raise NotImplementedError\n", "    @classmethod\n\t    def from_id(cls, task_id, **kwargs):\n\t        return cls._subclasses[task_id](**kwargs)\n\t    def generate_sample(self, rng, condition=None):\n\t        raise NotImplementedError(\n\t            \"Finite-sample tasks do not support the generation of samples.\"\n\t        )\n\t    @abstractmethod\n\t    def load_samples(self):\n\t        raise NotImplementedError\n", "def binary_encode_label(y, bits):\n\t    \"\"\"Encode the label tensor y as a tensor of bits.\"\"\"\n\t    mask = 2**torch.arange(bits).to(y.device, y.dtype)\n\t    return y.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n\tdef quaternary_encode_label(y, quits):\n\t    \"\"\"Encode the label tensor y as a tensor of quits.\"\"\"\n\t    binary = binary_encode_label(y, 2*quits)\n\t    return binary[:, ::2] + 2 * binary[:, 1::2]\n\tdef octal_encode_label(y, octs):\n\t    \"\"\"Encode the label tensor y as a tensor of octs.\"\"\"\n", "    quaternary = quaternary_encode_label(y, 2*octs)\n\t    return quaternary[:, ::2] + 4 * quaternary[:, 1::2]"]}
{"filename": "nninfo/tasks/emnist_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass EMnist1DTask(Task):\n\t    task_id = \"emnist_1d_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 10\n", "    def load_samples(self):\n\t        emnist = torchvision.datasets.EMNIST(\n\t            root=\".../\", split='digits', download=True, train=True)\n\t        emnist_test = torchvision.datasets.EMNIST(\n\t            root=\"../\", split='digits', download=True, train=False)\n\t        x = torch.cat([torch.transpose(emnist.data, 1, 2), torch.transpose(\n\t            emnist_test.data, 1, 2)]).reshape(-1, 784) / 256\n\t        y = torch.cat([emnist.targets, emnist_test.targets])\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/mnist_binary_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass MnistBinaryTask(Task):\n\t    task_id = \"mnist_binary_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 4\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        qmnist_test = torchvision.datasets.QMNIST(\n\t            root=\"../\", what='test50k', download=True, train=False)\n\t        x = torch.cat([mnist.data, mnist_test.data,\n\t                       qmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, mnist_test.targets,\n", "                       qmnist_test.targets[:, 0]])\n\t        y_binary = self.binary(y, 4)\n\t        return x.type(torch.float32), y_binary.type(torch.float32)\n\t    def binary(self, x, bits):\n\t        mask = 2**torch.arange(bits).to(x.device, x.dtype)\n\t        return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()"]}
{"filename": "nninfo/tasks/fake_task.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom .task import Task\n\tclass FakeTask(Task):\n\t    task_id = \"fake_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return \"binary\"\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        if \"x_dim\" in self._kwargs:\n\t            x_dim = self._kwargs[\"x_dim\"]\n\t        else:\n\t            x_dim = 12\n", "        return x_dim\n\t    @property\n\t    def y_dim(self):\n\t        return 1\n\t    def load_samples(self):\n\t        n_bits = self.x_dim\n\t        x = _create_all_possible_n_bit_configurations(n_bits)\n\t        # effectively setting y with x_0=0 to 1\n\t        y = np.zeros(x.shape[0], dtype=np.int)\n\t        y[int(x.shape[0] / 2):] = y[int(x.shape[0] / 2):] + 1\n", "        y = y[:, np.newaxis]\n\t        return torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n\tdef _create_all_possible_n_bit_configurations(n_bits):\n\t    n_samples = 2 ** n_bits\n\t    # create all integer values\n\t    x_int = np.linspace(0, n_samples - 1, n_samples, endpoint=True, dtype=np.uint32)[\n\t        :, np.newaxis\n\t    ]\n\t    # unpack integer values into bits\n\t    x_bit = np.unpackbits(np.flip(x_int.view(\"uint8\")), axis=1)\n", "    # cut bits to x_dim dimensions\n\t    return x_bit[:, x_bit.shape[1] - n_bits:]"]}
{"filename": "nninfo/tasks/cifar10_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass CIFAR10Task(Task):\n\t    task_id = \"cifar10_1d_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 32 * 32 * 3\n\t    @property\n\t    def y_dim(self):\n\t        return 10\n", "    def load_samples(self):\n\t        cifar10_train = torchvision.datasets.CIFAR10(\n\t            root=\"../\", download=True, train=True)\n\t        cifar10_test = torchvision.datasets.CIFAR10(\n\t            root=\"../\", download=True, train=False)\n\t        x_train = torch.from_numpy(cifar10_train.data) / 1.\n\t        x_test = torch.from_numpy(cifar10_test.data) / 1.\n\t        x = torch.cat([x_train, x_test])\n\t        x = x - torch.mean(x_train, axis=[0, 1, 2])\n\t        x = x / torch.std(x_train, axis=[0, 1, 2])\n", "        x = torch.transpose(x, 1, 3)\n\t        y = torch.cat([torch.tensor(cifar10_train.targets),\n\t                      torch.tensor(cifar10_test.targets)])\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/combined_mnist_quaternary_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task, quaternary_encode_label\n\tclass CombinedMnistQuaternaryTask(Task):\n\t    task_id = \"combined_mnist_quaternary_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 3\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        fmnist = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=True)\n\t        fmnist_test = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=False)\n\t        x = torch.cat([mnist.data, fmnist.data, mnist_test.data,\n", "                       fmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, fmnist.targets + 10, mnist_test.targets,\n\t                       fmnist_test.targets + 10])\n\t        y_binary = quaternary_encode_label(y, 3)\n\t        return x.type(torch.float32), y_binary.type(torch.float32)"]}
{"filename": "nninfo/tasks/__init__.py", "chunked_list": ["import os\n\timport glob\n\t# Get a list of all Python module files in the current directory\n\tmodules = glob.glob(os.path.dirname(__file__) + \"/*.py\")\n\t__all__ = [os.path.basename(f)[:-3] for f in modules if os.path.isfile(f) and not f.startswith('_')]"]}
{"filename": "nninfo/tasks/fashion_mnist_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass FashionMnistTask(Task):\n\t    task_id = \"fashion_mnist_1d_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 10\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=False)\n\t        x = torch.cat([mnist.data, mnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, mnist_test.targets])\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/mnist_reduced_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass ReducedMnistTask(Task):\n\t    task_id = \"mnist_reduced_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 10\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        x = torch.cat([mnist.train_data, mnist_test.test_data]\n\t                      ).reshape(-1, 784)\n\t        y = torch.cat([mnist.train_labels, mnist_test.test_labels])\n\t        # Apply filter\n\t        f = (y == 3) | (y == 6) | (y == 8) | (y == 9)\n", "        x = x[f]\n\t        y = y[f]\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/parity_task.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom .task import Task\n\tclass ParityTask(Task):\n\t    task_id = \"parity\"\n\t    @property\n\t    def finite(self):\n\t        return False\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1) if self._kwargs[\"continuous\"] else \"binary\"\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return self._kwargs[\"n_bits\"]\n\t    @property\n\t    def y_dim(self):\n\t        return 1\n", "    def generate_sample(self, rng, condition=None):\n\t        n_bits = self._kwargs[\"n_bits\"]\n\t        if self._kwargs[\"continuous\"]:\n\t            x = rng.random(size=n_bits, dtype=np.float32)\n\t            y = (x >= 0.5).sum() % 2\n\t        else:\n\t            x = rng.integers(2, size=n_bits)\n\t            y = x.sum() % 2\n\t        return torch.tensor(x, dtype=torch.float), torch.tensor([y], dtype=torch.float)"]}
{"filename": "nninfo/tasks/xor_task.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom .task import Task\n\tclass XorTask(Task):\n\t    task_id = \"xor_dat\"\n\t    @property\n\t    def finite(self):\n\t        return False\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 2\n\t    @property\n\t    def y_dim(self):\n\t        return 1\n", "    def generate_sample(self, rng, condition=None):\n\t        x = rng.random(2, dtype=np.float32)\n\t        y = (x[0] >= 0.5) ^ (x[1] >= 0.5)\n\t        return x, torch.tensor([y], dtype=torch.float)"]}
{"filename": "nninfo/tasks/xor_misinfo_task.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom .task import Task\n\tclass XorTaskMissInfo(Task):\n\t    task_id = \"XorTaskMissInfo_dat\"\n\t    @property\n\t    def finite(self):\n\t        return False\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 3\n\t    @property\n\t    def y_dim(self):\n\t        return 1\n", "    def generate_sample(self, rng, condition=None):\n\t        x = rng.random(3, dtype=np.float32)\n\t        y = (x[0] >= 0.5) ^ (x[1] >= 0.5)\n\t        return x, torch.tensor([y], dtype=torch.float)"]}
{"filename": "nninfo/tasks/mnist1d_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task\n\tclass Mnist1DTask(Task):\n\t    task_id = \"mnist_1d_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 10\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        qmnist_test = torchvision.datasets.QMNIST(\n\t            root=\"../\", what='test50k', download=True, train=False)\n\t        x = torch.cat([mnist.data, mnist_test.data,\n\t                      qmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, mnist_test.targets,\n", "                      qmnist_test.targets[:, 0]])\n\t        return x.type(torch.float32), y.type(torch.long)"]}
{"filename": "nninfo/tasks/combined_mnist_binary_task.py", "chunked_list": ["import torch\n\timport torchvision.datasets\n\tfrom .task import Task, binary_encode_label\n\tclass CombinedMnistBinaryTask(Task):\n\t    task_id = \"combined_mnist_binary_dat\"\n\t    @property\n\t    def finite(self):\n\t        return True\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 784\n\t    @property\n\t    def y_dim(self):\n\t        return 5\n", "    def load_samples(self):\n\t        mnist = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=True)\n\t        mnist_test = torchvision.datasets.MNIST(\n\t            root=\"../\", download=True, train=False)\n\t        fmnist = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=True)\n\t        fmnist_test = torchvision.datasets.FashionMNIST(\n\t            root=\"../\", download=True, train=False)\n\t        x = torch.cat([mnist.data, fmnist.data, mnist_test.data,\n", "                       fmnist_test.data]).reshape(-1, 784) / 256.\n\t        y = torch.cat([mnist.targets, fmnist.targets + 10, mnist_test.targets,\n\t                       fmnist_test.targets + 10])\n\t        y_binary = binary_encode_label(y, 5)\n\t        return x.type(torch.float32), y_binary.type(torch.float32)"]}
{"filename": "nninfo/tasks/checkerboard_task.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom .task import Task\n\tclass CheckerboardTask(Task):\n\t    task_id = \"checkerboard\"\n\t    @property\n\t    def finite(self):\n\t        return False\n\t    @property\n\t    def x_limits(self):\n", "        return (0, 1)\n\t    @property\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    @property\n\t    def x_dim(self):\n\t        return 2\n\t    @property\n\t    def y_dim(self):\n\t        return 1\n", "    def generate_sample(self, rng, condition=None):\n\t        size = self._kwargs['size']\n\t        x = rng.random(2, dtype=np.float32)\n\t        y = (int(x[0] * size[0]) + int(x[1] * size[1])) % 2\n\t        return x, torch.tensor([y], dtype=torch.float)"]}
{"filename": "nninfo/tasks/rec_majority_task.py", "chunked_list": ["from .task import Task\n\tclass RecMajorityTask(Task):\n\t    task_id = \"rec_maj\"\n\t    def __init__(self, **kwargs):\n\t        \"\"\"\n\t        Expected kwargs:\n\t            voter_list: list of numbers of voters for each recursive layer\n\t        \"\"\"\n\t        super().__init__(self, kwargs)\n\t        assert \"voter_list\" in kwargs\n", "    def x_limits(self):\n\t        return \"binary\"\n\t    def y_limits(self):\n\t        return \"binary\"\n\t    def x_dim(self):\n\t        return self._kwargs[\"voter_list\"][0]\n\t    def y_dim(self):\n\t        return 1\n\t    def generate_sample(self, rng):\n\t        x = rng.integers(1, size=10)"]}
{"filename": "nninfo/tasks/task_manager.py", "chunked_list": ["from ..exp_comp import ExperimentComponent\n\tfrom ..data_set import DataSet, CachedDataset, LazyDataset\n\tfrom .task import Task\n\tclass TaskManager(ExperimentComponent):\n\t    \"\"\"\n\t    Helper class, handles a task with one 'full_set' dataset and several subsets,\n\t    which are then used by Trainer, Tester or Evaluation.\n\t    \"\"\"\n\t    def __init__(self, task_id=None, **kwargs):\n\t        \"\"\"\n", "        Creates a new instance of TaskManager. Loads the\n\t        dataset according to task_id.\n\t        Keyword Args:\n\t            task_id (str): one of the pre-implemented Tasks:\n\t                'tishby_dat', 'andreas_dat', 'fake_dat' etc.\n\t        Passes additional arguments on to dataset creation if given.\n\t        \"\"\"\n\t        super().__init__()\n\t        self._dataset = None\n\t        self._kwargs = kwargs\n", "        task_kwargs = kwargs.get(\"task_kwargs\", {})\n\t        self.task = Task.from_id(task_id, **task_kwargs)\n\t        if self.task.finite:\n\t            self._dataset = CachedDataset(self.task, \"full_set\")\n\t        else:\n\t            seed = kwargs[\"seed\"]\n\t            n_samples = kwargs[\"n_samples\"]\n\t            self._dataset = LazyDataset(self.task, \"full_set\", n_samples, seed)\n\t    @staticmethod\n\t    def from_config(config):\n", "        \"\"\"\n\t        Creates a new TaskManager from a config dictionary\n\t        \"\"\"\n\t        task_manager = TaskManager(\n\t            task_id=config[\"task_id\"],\n\t            **config[\"kwarg_dict\"],\n\t        )\n\t        task_manager._dataset = DataSet.from_config(task_manager.task, config[\"subsets\"])\n\t        return task_manager\n\t    def to_config(self):\n", "        \"\"\"\n\t        Creates a config dictionary from the TaskManager\n\t        \"\"\"\n\t        output_dict = {\n\t            \"task_id\": self.task.task_id,\n\t            \"kwarg_dict\": self._kwargs,\n\t            \"subsets\": self._dataset.to_config(),\n\t            \"task_kwargs\": self.task.kwargs,\n\t        }\n\t        return output_dict\n", "    def get_binning_limits(self, label):\n\t        if label == \"X\":\n\t            return self.task.x_limits\n\t        elif label == \"Y\":\n\t            return self.task.y_limits\n\t        else:\n\t            raise AttributeError\n\t    def get_input_output_dimensions(self):\n\t        return self.task.x_dim, self.task.y_dim\n\t    def __getitem__(self, dataset_name):\n", "        \"\"\"\n\t        Finds the dataset by the given name in the dataset tree\n\t        \"\"\"\n\t        return self._dataset.find(dataset_name)\n\t    def __str__(self, level=0):\n\t        ret = self._dataset.__str__()\n\t        return ret"]}
