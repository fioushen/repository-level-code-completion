{"filename": "setup.py", "chunked_list": ["import re\n\timport io\n\timport os\n\tfrom setuptools import setup, find_packages\n\twith open('requirements.txt') as f:\n\t    requirements = f.read()\n\twith open('README.md') as f:\n\t    readme = f.read()\n\t# Read the version from the __init__.py file without importing it\n\tdef read(*names, **kwargs):\n", "    with io.open(\n\t            os.path.join(os.path.dirname(__file__), *names),\n\t            encoding=kwargs.get(\"encoding\", \"utf8\")\n\t    ) as fp:\n\t        return fp.read()\n\tdef find_version(*file_paths):\n\t    version_file = read(*file_paths)\n\t    version_match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\",\n\t                              version_file, re.M)\n\t    if version_match:\n", "        return version_match.group(1)\n\t    raise RuntimeError(\"Unable to find version string.\")\n\tsetup(\n\t      name='buildings_bench',\n\t      version=find_version('buildings_bench', '__init__.py'),\n\t      description='Large-scale pretraining and benchmarking for short-term load forecasting.',\n\t      author='Patrick Emami',\n\t      author_email='Patrick.Emami@nrel.gov',\n\t      url=\"https://nrel.github.io/BuildingsBench/\",\n\t      long_description=readme,\n", "      long_description_content_type='text/markdown',\n\t      install_requires=requirements,\n\t      packages=find_packages(include=['buildings_bench',\n\t                                      'buildings_bench.data',\n\t                                      'buildings_bench.evaluation',\n\t                                      'buildings_bench.models'],\n\t                             exclude=['test']),\n\t      package_data={'buildings_bench': ['configs/*.toml']},\n\t      license='BSD 3-Clause',\n\t      python_requires='>=3.8',\n", "      extras_require={\n\t            'benchmark': ['transformers', 'wandb', 'properscoring', 'matplotlib', 'seaborn', 'jupyterlab']\n\t      },\n\t      keywords=['forecasting', 'energy', 'buildings', 'benchmark'],\n\t      classifiers=[\n\t            \"Development Status :: 3 - Alpha\",\n\t            \"Intended Audience :: Science/Research\",\n\t            \"License :: OSI Approved :: BSD License\",\n\t            \"Natural Language :: English\",\n\t            \"Programming Language :: Python :: 3\",\n", "            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n\t        ]\n\t)\n"]}
{"filename": "test/test_transforms.py", "chunked_list": ["import unittest\n\tfrom buildings_bench import transforms\n\tfrom pathlib import Path\n\timport os\n\timport numpy as np\n\timport torch\n\timport pandas as pd\n\tclass TestStandardScaler(unittest.TestCase):\n\t    def setUp(self):\n\t        self.ss = transforms.StandardScalerTransform()\n", "        save_dir = os.environ.get('BUILDINGS_BENCH', '')\n\t        self.ss.load(Path(save_dir) / 'metadata' / 'transforms')\n\t    def test_load_standard_scaler(self):\n\t        self.assertIsNotNone(self.ss.mean_, True)\n\t        self.assertIsNotNone(self.ss.std_, True)\n\t    def test_standard_scale(self):\n\t        x = torch.FloatTensor([[100.234], [0.234], [55.523]])\n\t        y = self.ss.transform(x)\n\t        z = self.ss.undo_transform(y)\n\t        self.assertTrue(torch.allclose(x, z, atol=1e-3))\n", "class TestBoxCox(unittest.TestCase):\n\t    def setUp(self):\n\t        self.bc = transforms.BoxCoxTransform()\n\t        metadata_dir = os.environ.get('BUILDINGS_BENCH', '')\n\t        self.bc.load(Path(metadata_dir) / 'metadata' / 'transforms')\n\t    def test_load_boxcox(self):\n\t        self.assertIsNotNone(self.bc.boxcox.lambdas_, True)\n\t    def test_boxcox(self):\n\t        x = torch.FloatTensor([[100.234], [0.234], [55.523]])\n\t        y = self.bc.transform(x)\n", "        z = self.bc.undo_transform(torch.from_numpy(y).float())\n\t        # assert allclose\n\t        self.assertTrue(torch.allclose(x, z, atol=1e-3))\n\tclass TestLatLonTransform(unittest.TestCase):\n\t    def setUp(self):\n\t        self.ll = transforms.LatLonTransform()\n\t    def test_load_latlon(self):\n\t        self.assertIsNotNone(self.ll.lat_means, True)\n\t        self.assertIsNotNone(self.ll.lon_means, True)\n\t        self.assertIsNotNone(self.ll.lat_stds, True)\n", "        self.assertIsNotNone(self.ll.lon_stds, True)\n\t    def test_latlon(self):\n\t        x = np.array([[100.234, 0.234], [0.234, 55.523], [55.523, 100.234]])\n\t        y = self.ll.transform_latlon(x)\n\t        z = self.ll.undo_transform(y)\n\t        self.assertTrue(np.allclose(x, z, atol=1e-3))\n\tclass TestTimestampTransform(unittest.TestCase):\n\t    def setUp(self):\n\t        self.tt = transforms.TimestampTransform()\n\t    def test_timestamp(self):\n", "        x = np.array(['2016-01-01 00:00:00', '2016-01-01 01:00:00'])\n\t        # convert x to dataframe\n\t        x = pd.DataFrame(x, columns=['timestamp'])\n\t        y = self.tt.transform(x.timestamp)\n\t        z = self.tt.undo_transform(y)\n\t        #print(x,y,z)\n\t        self.assertEqual(z[0,0], pd.to_datetime(x.timestamp).dt.dayofyear.values[0])\n\t        self.assertEqual(z[0,1], pd.to_datetime(x.timestamp).dt.dayofweek.values[0])\n\t        self.assertEqual(z[0,2], pd.to_datetime(x.timestamp).dt.hour.values[0])\n"]}
{"filename": "test/test_scoring_rules.py", "chunked_list": ["import unittest\n\timport numpy as np\n\timport torch\n\tfrom buildings_bench.evaluation.scoring_rules import ContinuousRankedProbabilityScore\n\tfrom buildings_bench.evaluation.scoring_rules import RankedProbabilityScore\n\tclass TestRPS(unittest.TestCase):\n\t    \"\"\"\n\t    Test the categorical ranked probability score\n\t    \"\"\"\n\t    def setUp(self):\n", "        np.random.seed(1984)\n\t    def test_rps(self):\n\t        rps = RankedProbabilityScore()\n\t        y_true = torch.from_numpy(np.random.randint(0, 10, size=(2, 3, 1)))\n\t        y_pred_logits = torch.from_numpy(np.random.normal(size=(2, 3, 10)))    \n\t        bin_values = torch.from_numpy(np.random.normal(size=(10,)))        \n\t        rps(None, y_true, y_pred_logits, bin_values)\n\t    def test_bin_widths(self):\n\t        rps = RankedProbabilityScore()\n\t        bin_values = torch.FloatTensor([1., 5., 7.])\n", "        y_true = torch.FloatTensor([[[0.], [1.]]]) # batch_size 1, seq_len 2, 1\n\t        y_pred_logits = torch.FloatTensor([[[0.9, 0.1, 0.], [0.1, 0.9, 0.]]]) # batch_size 1, seq_len 2, vocab_size 3\n\t        rps(None, y_true, y_pred_logits, bin_values)\n\t        rps.mean()\n\t        print(rps.value)\n\tclass TestContinuousRPS(unittest.TestCase):\n\t    \"\"\"\n\t    https://github.com/properscoring/properscoring/blob/master/properscoring/tests/test_crps.py\n\t    \"\"\"\n\t    def setUp(self):\n", "        np.random.seed(1983)\n\t        shape = (2, 3, 1)\n\t        self.mu = torch.from_numpy(np.random.normal(size=shape))\n\t        self.sig = torch.from_numpy(np.square(np.random.normal(size=shape)))\n\t        self.params = torch.concatenate([self.mu, self.sig], dim=-1)\n\t        self.obs = torch.from_numpy(np.random.normal(loc=self.mu, scale=self.sig, size=shape))\n\t        self.crps = ContinuousRankedProbabilityScore()\n\t    def test_continuous_rps(self):\n\t        self.crps(self.obs, None, self.params, None)\n\t        self.crps.mean()\n", "        print(self.crps.value)\n\t    def test_continuous_rps_correct(self):\n\t        from properscoring import crps_ensemble\n\t        from scipy import special\n\t        n = 1000\n\t        q = np.linspace(0. + 0.5 / n, 1. - 0.5 / n, n)\n\t        # convert to the corresponding normal deviates\n\t        normppf = special.ndtri\n\t        z = normppf(q)\n\t        sig = self.sig.squeeze(2).numpy()\n", "        mu = self.mu.squeeze(2).numpy()\n\t        forecasts = (z.reshape(-1, 1, 1) * sig) + mu\n\t        expected = crps_ensemble(self.obs.squeeze(2).numpy(),\n\t                                  forecasts, axis=0)\n\t        expected = expected.mean(0)\n\t        self.crps(self.obs, None, self.params, None)\n\t        self.crps.mean()\n\t        actual = self.crps.value\n\t        np.testing.assert_allclose(actual, expected, rtol=1e-4)\n"]}
{"filename": "test/test_datasets.py", "chunked_list": ["import unittest\n\tfrom buildings_bench import load_torch_dataset, load_pandas_dataset, load_pretraining\n\tfrom buildings_bench.data.datasets import PandasBuildingDatasetsFromCSV\n\tfrom buildings_bench import BuildingTypes\n\tfrom pathlib import Path\n\timport torch\n\timport os\n\tclass TestLoadDatasets(unittest.TestCase):\n\t    def test_dataset_not_in_registry(self):\n\t        with self.assertRaises(ValueError):\n", "            datasets = load_torch_dataset('not-a-dataset')\n\t    def test_load_buildings900k_test(self):\n\t        building_dataset_generator = load_torch_dataset('buildings-900k-test')\n\t        for building_name, building in building_dataset_generator:\n\t                print(f'building {building_name} dataset length: {len(building)}')\n\t                # create a dataloader for the building\n\t                building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t                for sample in building_dataloader:\n\t                    x = sample['load']\n\t                    break\n", "                break\n\t    def test_load_electricity(self):\n\t        building_dataset_generator = load_torch_dataset('electricity')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_sceaux(self):\n\t        building_dataset_generator = load_torch_dataset('sceaux')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_borealis(self):\n\t        building_dataset_generator = load_torch_dataset('borealis')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_bdg2_panther(self):\n\t        building_dataset_generator = load_torch_dataset('bdg-2:panther')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_bdg2_rat(self):\n\t        building_dataset_generator = load_torch_dataset('bdg-2:rat')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_ideal(self):\n\t        building_dataset_generator = load_torch_dataset('ideal')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_lcl(self):\n\t        building_dataset_generator = load_torch_dataset('lcl')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_smart(self):\n\t        building_dataset_generator = load_torch_dataset('smart')\n\t        for building_name, building in building_dataset_generator:\n\t            print(f'building {building_name} dataset length: {len(building)}')\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(building, batch_size=256, shuffle=False)\n\t            for sample in building_dataloader:\n\t                x = sample['load']\n\t                break\n", "            break\n\t    def test_load_pandas_datasets_direct(self):\n\t        dataset_path = Path(os.environ['BUILDINGS_BENCH'])\n\t        building_files = ['BDG-2/Bear_clean=2016', 'BDG-2/Bear_clean=2017']\n\t        datasets = PandasBuildingDatasetsFromCSV(\n\t                    dataset_path,\n\t                    building_files,\n\t                    [23.123, -32.1234],\n\t                    BuildingTypes.COMMERCIAL,\n\t                    features='transformer')\n", "        for building_name, building_dset in datasets:\n\t            print(f'building {building_name}, shape {len(building_dset)}')\n\t            break\n\t        datasets = PandasBuildingDatasetsFromCSV(\n\t                    dataset_path,\n\t                    building_files,\n\t                    [23.123, -32.1234],\n\t                    BuildingTypes.COMMERCIAL,\n\t                    features='engineered'\n\t                    )\n", "        for building_name, building_dset in datasets:\n\t            print(f'building {building_name}, shape {len(building_dset)}')\n\t            break\n\t    def test_load_pandas_datasets(self):\n\t        datasets = load_pandas_dataset('bdg-2:bear')\n\t        for building_name, building_dset in datasets:\n\t            print(f'building {building_name}, shape {len(building_dset)}')\n\t            break\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "test/test_metrics.py", "chunked_list": ["import unittest\n\tfrom buildings_bench import evaluation\n\timport torch \n\tclass TestMetrics(unittest.TestCase):\n\t    def setUp(self):\n\t        # Test torch tensor\n\t        self.y_true = torch.FloatTensor([1, 2, 3]).view(1,3,1)\n\t        self.y_pred = torch.FloatTensor([1, 2, 3]).view(1,3,1)\n\t    def test_mae(self):\n\t        mae = evaluation.metrics_factory('mae', \n", "                                         types=[evaluation.MetricType.SCALAR])\n\t        self.assertEqual(len(mae), 1)\n\t        mae = mae[0]\n\t        self.assertEqual(mae.name, 'mae-scalar')\n\t        self.assertEqual(mae.type, evaluation.MetricType.SCALAR)\n\t        self.assertEqual(mae.UNUSED_FLAG, True)\n\t        # Test call\n\t        mae(self.y_true, self.y_pred)\n\t        mae.mean()\n\t        self.assertEqual(\n", "            mae.value,\n\t            torch.FloatTensor([0])\n\t        )\n\t        self.assertEqual(mae.UNUSED_FLAG, False)\n\t    def test_cvrmse(self):\n\t        cvrmse = evaluation.metrics_factory('cvrmse',\n\t                                            types=[evaluation.MetricType.SCALAR])\n\t        self.assertEqual(len(cvrmse), 1)\n\t        cvrmse = cvrmse[0]\n\t        self.assertEqual(cvrmse.name, 'cvrmse-scalar')\n", "        self.assertEqual(cvrmse.type, evaluation.MetricType.SCALAR)\n\t        self.assertEqual(cvrmse.UNUSED_FLAG, True)\n\t        # Test call\n\t        cvrmse(self.y_true, self.y_pred)\n\t        cvrmse.mean()\n\t        self.assertEqual(\n\t            cvrmse.value,\n\t            torch.FloatTensor([0])\n\t        )\n\t        self.assertEqual(cvrmse.UNUSED_FLAG, False)\n", "    def test_mbe(self):\n\t        mbe = evaluation.metrics_factory('mbe',\n\t                                         types=[evaluation.MetricType.SCALAR])\n\t        self.assertEqual(len(mbe), 1)\n\t        mbe = mbe[0]\n\t        self.assertEqual(mbe.name, 'mbe-scalar')\n\t        self.assertEqual(mbe.type, evaluation.MetricType.SCALAR)\n\t        self.assertEqual(mbe.UNUSED_FLAG, True)\n\t        # Test call\n\t        mbe(self.y_true, self.y_pred)\n", "        mbe.mean()\n\t        self.assertEqual(\n\t            mbe.value,\n\t            torch.FloatTensor([0])\n\t        )\n\t        self.assertEqual(mbe.UNUSED_FLAG, False)"]}
{"filename": "test/test_managers.py", "chunked_list": ["import unittest\n\tfrom buildings_bench.evaluation.managers import MetricsManager, DatasetMetricsManager\n\tfrom buildings_bench.evaluation import metrics_factory\n\tfrom buildings_bench.evaluation import scoring_rule_factory\n\tfrom buildings_bench import BuildingTypes\n\timport torch\n\tclass TestMetricsManager(unittest.TestCase):\n\t    def test_create_dataset_metrics_manager_single_metric(self):\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae')\n", "        )\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.RESIDENTIAL]), 1)\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.COMMERCIAL]), 1)\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n\t    def test_create_dataset_metrics_manager_multiple_metrics(self):\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae') + metrics_factory('rmse')\n\t        )\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.RESIDENTIAL]), 2)\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.COMMERCIAL]), 2)\n", "        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][1].name, 'rmse-scalar')\n\t    def test_create_dataset_metrics_manager_with_scoring_rule(self):\n\t        metrics_manager = MetricsManager(\n\t            scoring_rule=scoring_rule_factory('rps')\n\t        )\n\t        self.assertEqual(len(metrics_manager.scoring_rules), 2)\n\t        self.assertEqual(metrics_manager.scoring_rules[BuildingTypes.RESIDENTIAL].name, 'rps')\n\t        self.assertEqual(metrics_manager.scoring_rules[BuildingTypes.COMMERCIAL].name, 'rps')\n\t    def test_create_dataset_metrics_manager_with_scoring_rule_and_metrics(self):\n", "        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae'),\n\t            scoring_rule=scoring_rule_factory('rps')\n\t        )\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.RESIDENTIAL]), 1)\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.COMMERCIAL]), 1)\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n\t        self.assertEqual(metrics_manager.scoring_rules[BuildingTypes.RESIDENTIAL].name, 'rps')\n\t        self.assertEqual(metrics_manager.scoring_rules[BuildingTypes.COMMERCIAL].name, 'rps')\n\t    def test_call_dataset_metrics_manager(self):\n", "        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae')\n\t        )\n\t        metrics_manager(\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False])\n\t        )\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n\t        metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].mean()\n", "        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].value, 0)\n\t    def test_update_loss_and_get_ppl_from_manager(self):\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae')\n\t        )\n\t        loss = torch.FloatTensor([1.0])\n\t        metrics_manager(\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False]),\n", "            loss=loss,\n\t        )\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n\t        metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].mean()\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].value, 0)\n\t        self.assertEqual(metrics_manager.get_ppl(), torch.exp( loss * 3 ) / 3)\n\t    def test_summary(self):\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae')\n\t        )\n", "        loss = torch.FloatTensor([1.0])\n\t        metrics_manager(\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False]),\n\t            loss = loss\n\t        )\n\t        summary = metrics_manager.summary(with_loss=True, with_ppl=True)\n\t        self.assertEqual(summary[BuildingTypes.RESIDENTIAL]['mae-scalar'].value, 0)\n\t        self.assertEqual(summary['ppl'], torch.exp( loss * 3 ) / 3)\n", "        self.assertEqual(summary['loss'], loss)\n\t    def test_reset(self):\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae')\n\t        )\n\t        loss = torch.FloatTensor([1.0])\n\t        metrics_manager(\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False]),\n", "            loss = loss\n\t        )\n\t        metrics_manager.reset()\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.RESIDENTIAL]), 1)\n\t        self.assertEqual(len(metrics_manager.metrics[BuildingTypes.COMMERCIAL]), 1)\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n\t        self.assertEqual(metrics_manager.accumulated_unnormalized_loss, 0)\n\t        metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].mean()\n\t        metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].mean()\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].UNUSED_FLAG, True)\n", "    def test_building_type(self):\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('mae')\n\t        )\n\t        metrics_manager(\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_type = BuildingTypes.RESIDENTIAL_INT\n\t        )\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].name, 'mae-scalar')\n", "        metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].mean()\n\t        self.assertEqual(metrics_manager.metrics[BuildingTypes.RESIDENTIAL][0].value, 0)\n\tclass TestBenchmarkMetricsManager(unittest.TestCase):\n\t    def test_create_benchmark_metrics_manager(self):\n\t        metrics_manager = DatasetMetricsManager()\n\t    def test_add_building_to_dataset_if_missing(self):\n\t        metrics_manager = DatasetMetricsManager()\n\t        metrics_manager.add_building_to_dataset_if_missing(\n\t            dataset_name='test',\n\t            building_id='0001')\n", "        buliding_mm = metrics_manager.get_building_from_dataset(\n\t            dataset_name='test',\n\t            building_id='0001')\n\t        # assert building_mm type is MetricsManager\n\t        self.assertEqual(type(buliding_mm), MetricsManager)\n\t    def test_call_benchmark_metrics_manager(self):\n\t        metrics_manager = DatasetMetricsManager()\n\t        metrics_manager.add_building_to_dataset_if_missing(\n\t            dataset_name='test',\n\t            building_id='0001')\n", "        metrics_manager(\n\t            dataset_name='test',\n\t            building_id='0001',\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False])\n\t        )\n\t        buliding_mm = metrics_manager.get_building_from_dataset(\n\t            dataset_name='test',\n\t            building_id='0001')\n", "        self.assertEqual(buliding_mm.metrics[BuildingTypes.RESIDENTIAL][0].name, 'cvrmse-scalar')\n\t        buliding_mm.metrics[BuildingTypes.RESIDENTIAL][0].mean()\n\t        self.assertEqual(buliding_mm.metrics[BuildingTypes.RESIDENTIAL][0].value, 0)\n\t    def test_update_loss_and_get_ppl_from_benchmark_manager(self):\n\t        metrics_manager = DatasetMetricsManager()\n\t        metrics_manager.add_building_to_dataset_if_missing(\n\t            dataset_name='test',\n\t            building_id='0001')\n\t        loss = torch.FloatTensor([1.0])\n\t        metrics_manager(\n", "            dataset_name='test',\n\t            building_id='0001',\n\t            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False]),\n\t            loss=loss,\n\t        )\n\t        buliding_mm = metrics_manager.get_building_from_dataset(\n\t            dataset_name='test',\n\t            building_id='0001')\n", "        self.assertEqual(buliding_mm.get_ppl(), torch.exp( loss * 3 ) / 3)\n\t    def test_benchmark_manager_summary(self):\n\t        import pandas as pd\n\t        metrics_manager = DatasetMetricsManager()\n\t        metrics_manager.add_building_to_dataset_if_missing(\n\t            dataset_name='test',\n\t            building_id='0001')\n\t        metrics_manager(\n\t            dataset_name='test',\n\t            building_id='0001',\n", "            y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t            building_types_mask=torch.BoolTensor([False]),\n\t        )\n\t        summary = metrics_manager.summary()\n\t        print(summary)\n"]}
{"filename": "test/__init__.py", "chunked_list": []}
{"filename": "test/test_tokenizer.py", "chunked_list": ["import unittest\n\tfrom buildings_bench import tokenizer\n\timport os\n\tfrom pathlib import Path \n\timport numpy as np\n\tclass TestTokenizer(unittest.TestCase):\n\t    \"\"\" Test the KMeans tokenizer\"\"\"\n\t    def test_load(self):\n\t        \"\"\" Test loading the tokenizer \"\"\"\n\t        transform_path = Path(os.environ.get('BUILDINGS_BENCH', '')) \\\n", "                              / 'metadata' / 'transforms'\n\t        load_quantizer = tokenizer.LoadQuantizer(with_merge=True, num_centroids=2274, device='cpu')\n\t        load_quantizer.load(transform_path)\n\t        self.assertEqual(load_quantizer.get_vocab_size(), 2274)\n\t    def test_transform_cpu(self):\n\t        \"\"\" Test the transform method on CPU.\"\"\"\n\t        transform_path = Path(os.environ.get('BUILDINGS_BENCH', '')) \\\n\t                              / 'metadata' / 'transforms'\n\t        load_quantizer = tokenizer.LoadQuantizer(with_merge=True, num_centroids=2274, device='cpu')\n\t        load_quantizer.load(transform_path)\n", "        x = np.array([[100.234], [0.234], [55.523]])\n\t        y = load_quantizer.transform(x)\n\t        z = load_quantizer.undo_transform(y)\n\t        #print(x,y,z)\n\t        self.assertTrue(np.allclose(x, z, atol=1))\n"]}
{"filename": "test/test_persistence.py", "chunked_list": ["import unittest\n\timport torch\n\timport numpy as np\n\tclass TestPersistence(unittest.TestCase):\n\t    def test_average_persistence(self):\n\t        from buildings_bench.models.persistence import AveragePersistence\n\t        context_len = 168\n\t        pred_len = 24\n\t        bsz = 10\n\t        seqlen = context_len + pred_len\n", "        x = {'load': torch.from_numpy(np.random.rand(bsz, seqlen, 1).astype(np.float32))}\n\t        ap = AveragePersistence(context_len=context_len, pred_len=pred_len)\n\t        y = ap(x)\n\t        y_mean = y[:, :, 0]\n\t        y_sigma = y[:, :, 1]\n\t        self.assertEqual(y_mean.shape, (bsz, pred_len))\n\t        self.assertEqual(y_sigma.shape, (bsz, pred_len))\n\t    def test_copy_last_persistence(self):\n\t        from buildings_bench.models.persistence import CopyLastDayPersistence\n\t        context_len = 168\n", "        pred_len = 24\n\t        bsz = 10\n\t        seqlen = context_len + pred_len\n\t        load = torch.from_numpy(np.random.rand(bsz, seqlen, 1).astype(np.float32))\n\t        last_day = load[:, -48:-24]\n\t        x = {'load': load}\n\t        ap = CopyLastDayPersistence(context_len=context_len, pred_len=pred_len)\n\t        y = ap(x)\n\t        self.assertEqual(y.shape, (bsz, pred_len, 1))\n\t        self.assertTrue( (last_day == y).all() )\n", "    def test_last_week_persistence(self):\n\t        from buildings_bench.models.persistence import CopyLastWeekPersistence\n\t        context_len = 168\n\t        pred_len = 24\n\t        bsz = 10\n\t        seqlen = context_len + pred_len\n\t        load = torch.from_numpy(np.random.rand(bsz, seqlen, 1).astype(np.float32))\n\t        last_week = load[:,0:24]\n\t        x = {'load': load}\n\t        ap = CopyLastWeekPersistence(context_len=context_len, pred_len=pred_len)\n", "        y = ap(x)\n\t        self.assertEqual(y.shape, (bsz, pred_len, 1))\n\t        self.assertTrue( (last_week == y).all() )\n\tdef test():\n\t    suite = unittest.TestLoader().loadTestsFromTestCase(TestPersistence)\n\t    unittest.TextTestRunner(verbosity=2).run(suite)\n"]}
{"filename": "scripts/pretrain.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom pathlib import Path\n\timport transformers\n\timport argparse \n\timport wandb\n\timport os\n\timport tomli\n\tfrom timeit import default_timer as timer\n\tfrom socket import gethostname\n", "from buildings_bench import utils\n\tfrom buildings_bench import BuildingTypes\n\tfrom buildings_bench import load_pretraining\n\tfrom buildings_bench.tokenizer import LoadQuantizer\n\tfrom buildings_bench.evaluation.managers import MetricsManager\n\tfrom buildings_bench.models import model_factory\n\tfrom buildings_bench.evaluation.metrics import MetricType\n\tfrom buildings_bench.evaluation import metrics_factory\n\tfrom buildings_bench.evaluation import scoring_rule_factory\n\tSCRIPT_PATH = Path(os.path.realpath(__file__)).parent\n", "@torch.no_grad()\n\tdef validation(model, val_dataloader, args, loss, load_transform, transform, inverse_transform, predict):\n\t    model.eval()\n\t    step = 0\n\t    if args.ignore_scoring_rules:\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('nrmse', types=[ MetricType.SCALAR, MetricType.HOUR_OF_DAY ]) \\\n\t                    + metrics_factory('nmae', types=[ MetricType.SCALAR, MetricType.HOUR_OF_DAY ])) \n\t    elif model.module.continuous_loads:\n\t        metrics_manager = MetricsManager(\n", "            metrics=metrics_factory('nrmse', types=[ MetricType.SCALAR, MetricType.HOUR_OF_DAY ]) \\\n\t                    + metrics_factory('nmae', types=[ MetricType.SCALAR, MetricType.HOUR_OF_DAY ]),\n\t            scoring_rule=scoring_rule_factory('crps')\n\t        ) \n\t    else:\n\t        metrics_manager = MetricsManager(\n\t            metrics=metrics_factory('nrmse', types=[ MetricType.SCALAR, MetricType.HOUR_OF_DAY ]) \\\n\t                + metrics_factory('nmae', types=[ MetricType.SCALAR, MetricType.HOUR_OF_DAY ]),\n\t            scoring_rule=scoring_rule_factory('rps')\n\t        )\n", "    for batch in val_dataloader:   \n\t        building_types_mask = batch['building_type'][:,0,0] == 1\n\t        for k,v in batch.items():\n\t           batch[k] = v.to(model.device)\n\t        continuous_load = batch['load'].clone()\n\t        continuous_targets = continuous_load[:, model.module.context_len:]\n\t        # Transform if needed\n\t        batch['load'] = transform(batch['load'])\n\t        targets = batch['load'][:, model.module.context_len:]\n\t        with torch.cuda.amp.autocast():\n", "            preds = model(batch)\n\t            batch_loss = loss(preds, targets)\n\t            predictions, distribution_params = predict(batch)\n\t        predictions = inverse_transform(predictions)\n\t        if args.apply_scaler_transform != '':\n\t            continuous_targets = inverse_transform(continuous_targets)\n\t            # unscale for crps\n\t            targets = inverse_transform(targets)\n\t            if args.apply_scaler_transform == 'standard':\n\t                mu = inverse_transform(distribution_params[:,:,0])\n", "                sigma = load_transform.undo_transform_std(distribution_params[:,:,1])\n\t                distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n\t            elif args.apply_scaler_transform == 'boxcox':\n\t                ######## approximate Gaussian in unscaled space ########\n\t                mu = inverse_transform(distribution_params[:,:,0])\n\t                muplussigma = inverse_transform(torch.sum(distribution_params,-1))\n\t                sigma = muplussigma - mu\n\t                muminussigma = inverse_transform(distribution_params[:,:,0] - distribution_params[:,:,1])\n\t                sigma = (sigma + (mu - muminussigma)) / 2\n\t                distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n", "        if not model.module.continuous_loads:\n\t            centroids = load_transform.kmeans.centroids.squeeze() \\\n\t                if args.tokenizer_without_merge else load_transform.merged_centroids\n\t        else:\n\t            centroids = None\n\t        metrics_manager(\n\t            continuous_targets,\n\t            predictions,\n\t            building_types_mask,\n\t            loss=batch_loss,\n", "            y_categories=targets,\n\t            y_distribution_params=distribution_params,\n\t            centroids=centroids\n\t        )\n\t        step += 1\n\t        # don't run for too long\n\t        if step == 500:\n\t           break\n\t    model.train()\n\t    summary = metrics_manager.summary(with_loss=True, with_ppl=True)\n", "    return summary['loss'], summary['ppl'], summary\n\tdef main(args, model_args):\n\t    utils.set_seed(args.random_seed)\n\t    # When running on the CuDNN backend, two further options must be set\n\t    torch.backends.cudnn.deterministic = True\n\t    # Optimize for fixed input sizes\n\t    torch.backends.cudnn.benchmark = False\n\t    ######################### DDP setup  #########################\n\t    # SLURM_LOCALID: gpu local rank (=0 as the first gpu of the node)\n\t    # SLURM_PROCID: gpu global rank (=4 as the fifth gpu among the 8)\n", "    # MASTER_ADDR and MASTER_PORT env variables should be set when calling this script\n\t    gpus_per_node = torch.cuda.device_count()    \n\t    args.world_size    = int(os.environ[\"WORLD_SIZE\"])\n\t    if args.disable_slurm:\n\t        local_rank     = int(os.environ[\"LOCAL_RANK\"])\n\t        args.rank      = local_rank\n\t    else:\n\t        args.rank      = int(os.environ[\"SLURM_PROCID\"])\n\t        print(f\"Hello from rank {args.rank} of {args.world_size} on {gethostname()} where there are\" \\\n\t            f\" {gpus_per_node} allocated GPUs per node.\", flush=True)\n", "        local_rank = args.rank - gpus_per_node * (args.rank // gpus_per_node)\n\t    print(f'About to call init_process_group on rank {args.rank} with local rank {local_rank}', flush=True)\n\t    torch.distributed.init_process_group(backend=args.dist_backend, \n\t                                        init_method=args.dist_url,\n\t                                        world_size=args.world_size,\n\t                                        rank=args.rank)\n\t    if args.rank == 0: print(f\"Group initialized? {torch.distributed.is_initialized()}\", flush=True)\n\t    torch.cuda.set_device(local_rank)\n\t    print(f'rank {args.rank} torch cuda available = ', torch.cuda.is_available(), flush=True)\n\t    print(f'rank {args.rank} torch cuda device count = ', torch.cuda.device_count(), flush=True)\n", "    print(f'rank {args.rank} torch cuda current device = ', torch.cuda.current_device(), flush=True)\n\t    print(f'rank {args.rank} torch cuda get_device_name = ', torch.cuda.get_device_name(0), flush=True)\n\t    print(f'rank {args.rank} torch threads = ', torch.get_num_threads(), flush=True)\n\t    checkpoint_dir = SCRIPT_PATH / '..' / 'checkpoints'\n\t    transform_path = Path(os.environ.get('BUILDINGS_BENCH', '')) / 'metadata' / 'transforms'\n\t    if args.rank == 0:\n\t        if not checkpoint_dir.exists():\n\t            os.makedirs(checkpoint_dir)\n\t        wandb_project = os.environ.get('WANDB_PROJECT', '')\n\t        if wandb_project == '':\n", "            print('WANDB_PROJECT environment variable not set, disabling wandb')\n\t            args.disable_wandb = True\n\t        if args.disable_wandb:\n\t            run = wandb.init(\n\t                project=wandb_project,\n\t                mode=\"disabled\",\n\t                config=args)\n\t        elif args.resume_from_checkpoint != '':\n\t            run = wandb.init(\n\t                id=args.wandb_run_id,\n", "                project=wandb_project,\n\t                notes=args.note,\n\t                resume=\"allow\",\n\t                config=args)\n\t        else:\n\t            run = wandb.init(\n\t                project=wandb_project,\n\t                notes=args.note,\n\t                config=args)\n\t    global_batch_size = args.world_size * args.batch_size\n", "    #################### Model setup ####################\n\t    model, loss, predict = model_factory(args.config, model_args)\n\t    model = model.to(local_rank)\n\t    print(f'rank {args.rank} number of trainable parameters is '\\\n\t          f'= {sum(p.numel() for p in model.parameters())}', flush=True)\n\t    #################### Dataset setup ####################\n\t    train_dataset = load_pretraining('buildings-900k-train',\n\t                                     args.num_buildings,\n\t                                     args.apply_scaler_transform,\n\t                                     transform_path)\n", "    val_dataset = load_pretraining('buildings-900k-val',\n\t                                   args.num_buildings,\n\t                                   args.apply_scaler_transform,\n\t                                   transform_path)\n\t    train_sampler = torch.utils.data.distributed.DistributedSampler(\n\t                                     dataset=train_dataset,\n\t                                     num_replicas=args.world_size,\n\t                                     rank=args.rank, shuffle=True)\n\t    val_sampler = torch.utils.data.distributed.DistributedSampler(\n\t                                     dataset=val_dataset,\n", "                                     num_replicas=args.world_size,\n\t                                     rank=args.rank, shuffle=True)\n\t    train_dataloader = torch.utils.data.DataLoader(\n\t        train_dataset, batch_size=args.batch_size, sampler=train_sampler,\n\t        drop_last=False, worker_init_fn=utils.worker_init_fn_eulp,\n\t        shuffle=(train_sampler is None), num_workers=args.num_workers, pin_memory=True)\n\t    val_dataloader = torch.utils.data.DataLoader(\n\t        val_dataset, batch_size=args.batch_size, sampler=val_sampler,\n\t        drop_last=False, worker_init_fn=utils.worker_init_fn_eulp,\n\t        shuffle=(val_sampler is None), num_workers=args.num_workers, pin_memory=True)\n", "    if not model.continuous_loads:\n\t        load_transform = LoadQuantizer(\n\t            with_merge=(not args.tokenizer_without_merge),\n\t            num_centroids=model.vocab_size,\n\t            device=f'cuda:{local_rank}')\n\t        load_transform.load(transform_path)\n\t    else:\n\t        load_transform = train_dataset.load_transform\n\t    if not model.continuous_loads: \n\t        transform = load_transform.transform\n", "        inverse_transform = load_transform.undo_transform\n\t    elif args.apply_scaler_transform != '':\n\t        transform = lambda x: x\n\t        inverse_transform = load_transform.undo_transform\n\t    else: # Continuous unscaled values\n\t        transform = lambda x: x\n\t        inverse_transform = lambda x: x\n\t    #################### Optimizer setup ##########################\n\t    # wrap model with DistributedDataParallel\n\t    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n", "    print(f'rank {args.rank} wrapped model in DDP', flush=True)\n\t    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n\t    if args.scheduler_steps > 0:\n\t        scheduler_steps = args.scheduler_steps\n\t    else:\n\t        scheduler_steps = args.num_epochs * (len(train_dataset) // global_batch_size)\n\t    scheduler = transformers.get_cosine_schedule_with_warmup(optimizer,\n\t                            num_warmup_steps=args.warmup_steps,\n\t                            num_training_steps=scheduler_steps)\n\t    scaler = torch.cuda.amp.GradScaler()\n", "    #################### Resume from checkpoint ####################\n\t    if args.resume_from_checkpoint != '':\n\t        model, optimizer, scheduler, step = utils.load_model_checkpoint(\n\t            checkpoint_dir / args.resume_from_checkpoint, model, optimizer, scheduler, local_rank)\n\t        seen_tokens = step * global_batch_size  * model.module.pred_len\n\t        if args.override_lr > 0.0:\n\t             # set the lr for every param group to the override value\n\t             for group in optimizer.param_groups:\n\t                 group['lr'] = args.override_lr\n\t                 group['initial_lr'] = args.override_lr\n", "             scheduler = transformers.get_cosine_schedule_with_warmup(optimizer,\n\t                             num_warmup_steps=0,\n\t                             num_training_steps=scheduler_steps)\n\t        #step -= 1\n\t    else:\n\t        step = 0\n\t        seen_tokens = 0\n\t        for p in model.parameters():\n\t            if p.dim() > 1:\n\t                nn.init.normal_(p, mean=0.0, std=args.init_scale)\n", "    #################### Training loop ##############################\n\t    best_val_loss = 1e9\n\t    start_epoch = step // (len(train_dataset) // global_batch_size)\n\t    # Save a checkpoint every 1B seen tokens\n\t    save_every = 1000000000 // (global_batch_size * model.module.pred_len )\n\t    # steps per epoch\n\t    steps_per_epoch = len(train_dataset) // global_batch_size\n\t    print(f'rank {args.rank} step {step} start_epoch {start_epoch} save_every = {save_every}', flush=True)\n\t    for epoch in range(start_epoch, args.num_epochs):\n\t        # fix sampling seed such that each gpu gets different part of dataset        \n", "        train_sampler.set_epoch(epoch)\n\t        val_sampler.set_epoch(epoch)    \n\t        model.train()\n\t        start_time = timer()\n\t        # NB step is the # of gradient updates\n\t        # batch_index is the number of batches processed\n\t        for batch in train_dataloader:\n\t            start_time = timer()\n\t            optimizer.zero_grad()\n\t            for k,v in batch.items():\n", "                batch[k] = v.to(model.device)\n\t            # Apply transform to load if needed\n\t            batch['load'] = transform(batch['load'])\n\t            # backwards is called in here\n\t            with torch.cuda.amp.autocast():\n\t                preds = model(batch)    \n\t                targets = batch['load'][:, model.module.context_len:]                \n\t                # preds are [bsz_sz, pred_len, vocab_size] if logits\n\t                # preds are [bsz_sz, pred_len, 2] if Gaussian\n\t                # preds are [bsz_sz, pred_len, 1] if MSE\n", "                # targets is [bsz_sz, pred_len, 1]\n\t                batch_loss = loss(preds, targets)\n\t            # Scale Gradients\n\t            scaler.scale(batch_loss).backward()\n\t            # Update Optimizer\n\t            scaler.step(optimizer)\n\t            scaler.update()\n\t            scheduler.step()\n\t            end_time = timer()\n\t            secs_per_step = end_time - start_time\n", "            # world_size * batch_size = global batch size with DDP training\n\t            seen_tokens += (global_batch_size * model.module.pred_len)\n\t            step += 1\n\t            ppl = torch.exp(batch_loss.detach())\n\t            if args.rank == 0 and step % 500 == 0:\n\t                wandb.log({\n\t                    'train/loss': batch_loss,\n\t                    'train/batch_ppl': ppl,\n\t                    'train/seen_tokens (M)': seen_tokens / 1000000,\n\t                    'train/secs_per_step': secs_per_step,\n", "                    'train/lr': optimizer.param_groups[0]['lr']\n\t                }, step=step)\n\t            if args.rank == 0 and step % min(steps_per_epoch,10000) == 0:\n\t                print(f'started validation at step {step}...')\n\t                val_loss, val_ppl, val_metrics = validation(model, val_dataloader, args, loss, load_transform,\n\t                                                            transform, inverse_transform, predict)\n\t                # only rank 0 needs to save model\n\t                if val_loss < best_val_loss:\n\t                    # delete old checkpoint\n\t                    if args.note != '':\n", "                        for f in checkpoint_dir.glob(f'ckpt-step-*-{args.note}-loss-{best_val_loss:.3f}.pt'):\n\t                            f.unlink()\n\t                    else:\n\t                        for f in checkpoint_dir.glob(f'ckpt-step-*-loss-{best_val_loss:.3f}.pt'):\n\t                            f.unlink()\n\t                    best_val_loss = val_loss\n\t                    if args.note != '':\n\t                        model_name = f'ckpt-step-{step}-{args.note}-loss-{best_val_loss:.3f}.pt'\n\t                    else:\n\t                        model_name = f'ckpt-step-{step}-loss-{best_val_loss:.3f}.pt'\n", "                    utils.save_model_checkpoint(model, optimizer, scheduler, step, checkpoint_dir / model_name)\n\t                for building_type in [BuildingTypes.RESIDENTIAL, BuildingTypes.COMMERCIAL]:\n\t                    for metric_name, metric_result in val_metrics[building_type].items():\n\t                        if metric_result.type == MetricType.SCALAR:\n\t                            wandb.log({f'val/{building_type}/{metric_name}' : metric_result.value }, step=step)\n\t                        else:\n\t                            # Create a wandb.Table for each hour of day metric then plot a line plot\n\t                            table = wandb.Table(columns=['time (hour)', metric_name])\n\t                            multi_hour_value = metric_result.value\n\t                            for row_idx in range(multi_hour_value.shape[0]):\n", "                                table.add_data(row_idx, multi_hour_value[row_idx].item())\n\t                            wandb.log({f'val/{building_type}/{metric_name}' : wandb.plot.line(\n\t                                table, \"time (hour)\", metric_name, title=f\"Time vs {metric_name}\")}, step=step)\n\t                wandb.log({\n\t                    'val/loss': val_loss,\n\t                    'val/ppl': val_ppl,\n\t                }, step=step)\n\t                print(f'finished validation at step {step}...')\n\t            # every 1B seen_tokens, save a checkpoint\n\t            if args.rank == 0 and step % save_every == 0:\n", "                if args.note != '':\n\t                    model_name = f'ckpt-step-{step}-{args.note}.pt'\n\t                else:\n\t                    model_name = f'ckpt-step-{step}.pt'\n\t                utils.save_model_checkpoint(model, optimizer, scheduler, step, checkpoint_dir / model_name)\n\t    torch.distributed.destroy_process_group()        \n\t    run.finish()\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    # Training hyperparams. If provided in config file, these will be overridden.\n", "    parser.add_argument('--config', type=str, default='', required=True,\n\t                        help='Path to your models TOML config file.')\n\t    parser.add_argument('--batch_size', type=int, default=256)\n\t    parser.add_argument('--lr', type=float, default=0.00006)\n\t    parser.add_argument('--override_lr', type=float, default=0.0,\n\t                        help='Override the learning rate with this value on resume')\n\t    parser.add_argument('--warmup_steps', type=int, default=10000)\n\t    parser.add_argument('--scheduler_steps', type=int, default=-1,\n\t                        help='Number of steps to decay lr to 0. '\n\t                             'Set to -1 to use num_epochs instead.')\n", "    parser.add_argument('--random_seed', type=int, default=99)\n\t    parser.add_argument('--ignore_scoring_rules', action='store_true',\n\t                        help='Do not compute a scoring rule for this model.')\n\t    parser.add_argument('--num_epochs', type=int, default=1,\n\t                         help='Number of epochs to train for.')\n\t    parser.add_argument('--resume_from_checkpoint', type=str, default='')\n\t    parser.add_argument('--tokenizer_without_merge', action='store_true', default=False, \n\t                        help='Use the tokenizer without merge. Default is False.')\n\t    parser.add_argument('--apply_scaler_transform', type=str, default='',\n\t                        choices=['', 'standard', 'boxcox'], \n", "                        help='Apply a scaler transform to the load values.')\n\t    # Wandb\n\t    parser.add_argument('--disable_wandb', action='store_true')\n\t    parser.add_argument('--note', type=str, default='',\n\t                        help='Note to append to model checkpoint name. '\n\t                        'Also used for wandb notes.')    \n\t    parser.add_argument('--wandb_run_id', type=str, default='')\n\t    # DDP\n\t    parser.add_argument('--disable_slurm', action='store_true')\n\t    parser.add_argument('--world-size', default=-1, type=int, \n", "                        help='number of nodes for distributed training')\n\t    parser.add_argument('--rank', default=-1, type=int, \n\t                        help='node rank for distributed training')\n\t    parser.add_argument('--dist-url', default='env://', type=str, \n\t                        help='url used to set up distributed training')\n\t    parser.add_argument('--dist-backend', default='nccl', type=str, \n\t                        help='distributed backend')\n\t    #parser.add_argument('--local_rank', default=-1, type=int, \n\t    #                    help='local rank for distributed training')\n\t    #parser.add_argument('--DDP_port', type=int, default=12345)\n", "    parser.add_argument('--num_workers', type=int, default=8)\n\t    # Variants\n\t    parser.add_argument('--num_buildings', type=int, default=-1,\n\t                        help='Number of buildings to use for training. '\n\t                             'Default is -1 which uses all buildings. ' \n\t                             'Options {1000, 10000, 100000}.')\n\t    args = parser.parse_args()\n\t    config_path = SCRIPT_PATH  / '..' / 'buildings_bench' / 'configs'\n\t    if (config_path / f'{args.config}.toml').exists():\n\t        toml_args = tomli.load(( config_path / f'{args.config}.toml').open('rb'))\n", "        model_args = toml_args['model']\n\t        if 'pretrain' in toml_args:\n\t            for k,v in toml_args['pretrain'].items():\n\t                if hasattr(args, k):\n\t                    print(f'Overriding argparse default for {k} with {v}')\n\t                setattr(args, k, v)\n\t        if not model_args['continuous_loads'] or 'apply_scaler_transform' not in args:\n\t            setattr(args, 'apply_scaler_transform', '')\n\t    else:\n\t        raise ValueError(f'Config {args.config}.toml not found.')\n", "    if not torch.cuda.is_available():\n\t        raise ValueError('CUDA is not available for pretraining!')\n\t    main(args, model_args)\n"]}
{"filename": "scripts/transfer_learning_lightgbm.py", "chunked_list": ["from pathlib import Path\n\timport os\n\timport argparse \n\tfrom buildings_bench import utils\n\timport pandas as pd\n\timport torch\n\tfrom lightgbm import LGBMRegressor\n\tfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n\tfrom buildings_bench import BuildingTypes\n\tfrom buildings_bench import load_pandas_dataset, benchmark_registry\n", "from buildings_bench.data.datasets import keep_buildings\n\tfrom buildings_bench import utils\n\tfrom buildings_bench.evaluation.managers import DatasetMetricsManager\n\tSCRIPT_PATH = Path(os.path.realpath(__file__)).parent\n\tdef transfer_learning(args, results_path: Path):\n\t    global benchmark_registry\n\t    lag = 168\n\t    # remove synthetic\n\t    benchmark_registry = [b for b in benchmark_registry if b != 'buildings-900k-test']\n\t    if args.benchmark[0] == 'all':\n", "        args.benchmark = benchmark_registry\n\t    metrics_manager = DatasetMetricsManager()\n\t    target_buildings = []\n\t    if args.subsample_buildings:\n\t        metadata_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'metadata')\n\t        with open(metadata_dir / 'transfer_learning_commercial_buildings.txt', 'r') as f:\n\t            target_buildings += f.read().splitlines()\n\t        with open(metadata_dir / 'transfer_learning_residential_buildings.txt', 'r') as f:\n\t            target_buildings += f.read().splitlines()\n\t    for dataset in args.benchmark:\n", "        dataset_generator = load_pandas_dataset(dataset, feature_set='engineered')\n\t        # Filter to target buildings\n\t        if len(target_buildings) > 0:\n\t            dataset_generator = keep_buildings(dataset_generator, target_buildings)\n\t        # For metrics management\n\t        if dataset_generator.building_type == BuildingTypes.COMMERCIAL:\n\t            building_types_mask = (BuildingTypes.COMMERCIAL_INT * torch.ones([1,24,1])).bool()\n\t        else:\n\t            building_types_mask = (BuildingTypes.RESIDENTIAL_INT * torch.ones([1,24,1])).bool()\n\t        for building_name, bldg_df in dataset_generator:\n", "            # if date range is less than 120 days, skip - 90 days training, 30+ days eval.\n\t            if len(bldg_df) < (args.num_training_days+30)*24:\n\t                print(f'{dataset} {building_name} has too few days {len(bldg_df)}')\n\t                continue\n\t            print(f'dataset {dataset} building {building_name}')\n\t            metrics_manager.add_building_to_dataset_if_missing(\n\t                 dataset, f'{building_name}',\n\t            )\n\t            # Split into fine-tuning and evaluation set by date\n\t            # Get the first month of data from bldg_df by index\n", "            start_timestamp = bldg_df.index[0]\n\t            end_timestamp = start_timestamp + pd.Timedelta(days=args.num_training_days)\n\t            historical_date_range = pd.date_range(start=start_timestamp, end=end_timestamp, freq='H')\n\t            training_set = bldg_df.loc[historical_date_range]\n\t            test_set = bldg_df.loc[~bldg_df.index.isin(historical_date_range)]\n\t            test_start_timestamp = test_set.index[0]\n\t            test_end_timestamp = test_start_timestamp + pd.Timedelta(days=180)\n\t            #test_date_range = pd.date_range(start=test_start_timestamp, end=test_end_timestamp, freq='H')\n\t            #test_set = test_set.loc[test_date_range]\n\t            test_set = test_set[test_set.index <= test_end_timestamp]\n", "            print(f'fine-tune set date range: {training_set.index[0]} {training_set.index[-1]}, '\n\t                  f'test set date range: {test_set.index[0]} {test_set.index[-1]}')\n\t            # train the model\n\t            forecaster = ForecasterAutoreg(\n\t                    regressor        = LGBMRegressor(max_depth=-1, n_estimators=100, n_jobs=24),\n\t                    lags             = lag\n\t                )\n\t            forecaster.fit(\n\t                y               = training_set['power'],\n\t                exog            = training_set[[key for key in training_set.keys() if key != 'power']]\n", "            )\n\t            pred_days = (len(test_set) - lag - 24) // 24\n\t            for i in range(pred_days):\n\t                seq_ptr =lag + 24 * i\n\t                last_window  = test_set.iloc[seq_ptr - lag : seq_ptr]\n\t                ground_truth = test_set.iloc[seq_ptr : seq_ptr + 24]\n\t                predictions = forecaster.predict(\n\t                    steps       = 24,\n\t                    last_window = last_window['power'],\n\t                    exog        = test_set[[key for key in test_set.keys() if key != 'power']]\n", "                )\n\t                metrics_manager(\n\t                    dataset,\n\t                    f'{building_name}',\n\t                    torch.from_numpy(ground_truth['power'].values).float().view(1,24,1),\n\t                    torch.from_numpy(predictions.values).float().view(1,24,1),\n\t                    building_types_mask\n\t                )\n\t    print('Generating summaries...')\n\t    variant_name = f':{args.variant_name}' if args.variant_name != '' else ''\n", "    metrics_file = results_path / f'TL_metrics_lightgbm{variant_name}.csv'\n\t    metrics_df = metrics_manager.summary()    \n\t    if metrics_file.exists():    \n\t        metrics_df.to_csv(metrics_file, mode='a', index=False, header=False)\n\t    else:\n\t        metrics_df.to_csv(metrics_file, index=False)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--results_path', type=str, default='results/')\n\t    parser.add_argument('--benchmark', nargs='+', type=str, default=['all'],\n", "                        help='Which datasets in the benchmark to run. Default is [\"all.\"] '\n\t                             'See the dataset registry in buildings_bench.data.__init__.py for options.')\n\t    parser.add_argument('--seed', type=int, default=1)\n\t    parser.add_argument('--variant_name', type=str, default='',\n\t                        help='Name of the variant. Optional. Used for results files.')\n\t    # Transfer learning - data\n\t    parser.add_argument('--num_training_days', type=int, default=180,\n\t                        help='Number of days for fine-tuning (last 30 used for early stopping)')\n\t    parser.add_argument('--subsample_buildings', action='store_true', default=False,\n\t                        help='Evaluate on a random subsample of 100 res/com buildings '\n", "                        ' (instead of all).')    \n\t    args = parser.parse_args()\n\t    utils.set_seed(args.seed)\n\t    results_path = Path(args.results_path)\n\t    results_path.mkdir(parents=True, exist_ok=True)\n\t    transfer_learning(args, results_path)\n"]}
{"filename": "scripts/create_index_files_with_less_buildings.py", "chunked_list": ["import argparse\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pyarrow.parquet as pq\n\timport random\n\timport glob \n\timport pandas as pd\n\timport os\n\tfrom tqdm import tqdm\n\tdef main(args):\n", "    random.seed(args.seed)\n\t    np.random.seed(args.seed)\n\t    dataset_path = Path(os.environ.get('BUILDINGS_BENCH', ''))\n\t    # Each line in the index file indicates a building and n \n\t    #   <building_type_and_year> <census_region> <puma_id> <building_id> <seq ptr>\n\t    #  e.g. <0-4> <0-4> G17031 23023 65\n\t    train_idx_file = open(dataset_path / 'metadata' / f'train_weekly_{args.num_buildings}.idx', 'w')\n\t    val_idx_file = open(dataset_path / 'metadata' / f'val_weekly_{args.num_buildings}.idx', 'w')\n\t    building_years = ['comstock_tmy3_release_1', 'resstock_tmy3_release_1', 'comstock_amy2018_release_1', 'resstock_amy2018_release_1'] \n\t    pumas = ['by_puma_midwest', 'by_puma_south', 'by_puma_northeast', 'by_puma_west']\n", "    # withhold 1 puma from each census region (all res and com buildingss) for test only\n\t    # midwest, south, northeast, west\n\t    # read withheld pumas from file\n\t    with open(dataset_path / 'metadata' / 'withheld_pumas.tsv', 'r') as f:\n\t        # tab separated file\n\t        line = f.readlines()[0]\n\t        withheld_pumas = line.strip('\\n').split('\\t')\n\t    print(f'Withheld pumas: {withheld_pumas}')\n\t    # 2 weeks heldout for val\n\t    train_tmy_timerange = (pd.Timestamp('2018-01-01'), pd.Timestamp('2018-12-31'))\n", "    train_amy2018_timerange = (pd.Timestamp('2018-01-01'), pd.Timestamp('2018-12-17'))\n\t    val_timerange = (pd.Timestamp('2018-12-17'), pd.Timestamp('2018-12-31'))\n\t    bldgs_per_bty = args.num_buildings // len(building_years)\n\t    for building_type_and_year, by in enumerate(building_years):\n\t        num_buildings = 0\n\t        by_path = dataset_path / 'Buildings-900K' / 'end-use-load-profiles-for-us-building-stock/2021' \\\n\t              / by / 'timeseries_individual_buildings'\n\t        if 'amy2018' in by:\n\t            train_hours = int((train_amy2018_timerange[1] - train_amy2018_timerange[0]).total_seconds() / 3600)\n\t            print(f'AMY2018 train hours: {train_hours}')\n", "            val_hours = int((val_timerange[1] - val_timerange[0]).total_seconds() / 3600)\n\t            print(f'AMY2018 Val hours: {val_hours}')\n\t        else:\n\t            train_hours = int((train_tmy_timerange[1] - train_tmy_timerange[0]).total_seconds() / 3600)\n\t            print(f'TMY train hours: {train_hours}')\n\t        # select buildings from census regions in random order,\n\t        # until we have enough buildings. alternatively, we could\n\t        # randomly sample the census region for each building.\n\t        census_region_idxs = [0, 1, 2, 3]\n\t        random.shuffle(census_region_idxs)\n", "        for census_region_idx in census_region_idxs: # census regions\n\t            pum_path = by_path / pumas[census_region_idx] / 'upgrade=0'\n\t            pum_files = glob.glob(str(pum_path / 'puma=*'))\n\t            # shuffle order of pum_files\n\t            random.shuffle(pum_files)\n\t            for pum_file in tqdm(pum_files):\n\t                try:\n\t                    bldg_ids = pq.read_table(pum_file).to_pandas().columns[1:]\n\t                except:\n\t                    print(f'Failed to read {pum_file}')\n", "                    import pdb; pdb.set_trace()\n\t                    continue\n\t                if not os.path.basename(pum_file) in withheld_pumas:\n\t                    # store building ids\n\t                    for bldg_id in bldg_ids:\n\t                        # train\n\t                        if num_buildings >= bldgs_per_bty:\n\t                            break\n\t                        bldg_id = bldg_id.zfill(6)\n\t                        # Sample the starting index between (0,24)\n", "                        s_start = np.random.randint(0, 24)\n\t                        for s_idx in range(s_start, train_hours - (args.context_len + args.pred_len), args.sliding_window_stride):                            \n\t                            seq_ptr = str(args.context_len + s_idx).zfill(4)  # largest seq ptr is < 10000\n\t                            # NB: We don't *need* \\n at the end of each line, but it makes it easier to count # of lines for dataloading\n\t                            linestr = f'{building_type_and_year}\\t{census_region_idx}\\t{os.path.basename(pum_file).split(\"=\")[1]}\\t{bldg_id}\\t{seq_ptr}\\n'\n\t                            assert len(linestr) == 26, f'linestr: {linestr}'\n\t                            train_idx_file.write(linestr)\n\t                        # val\n\t                        if 'amy2018' in by:\n\t                            s_start += train_hours \n", "                            for s_idx in range(s_start, (train_hours + val_hours) - (args.context_len + args.pred_len), args.sliding_window_stride):\n\t                                seq_ptr = str(args.context_len + s_idx).zfill(4)  # largest seq ptr is < 10000\n\t                                assert len(linestr) == 26, f'linestr: {linestr}'\n\t                                linestr = f'{building_type_and_year}\\t{census_region_idx}\\t{os.path.basename(pum_file).split(\"=\")[1]}\\t{bldg_id}\\t{seq_ptr}\\n'\n\t                                val_idx_file.write(linestr)\n\t                        num_buildings += 1\n\t                # for each puma in census region\n\t                if num_buildings >= bldgs_per_bty:\n\t                    break\n\t            # for each census region\n", "            if num_buildings >= bldgs_per_bty:\n\t                 break\n\t    # Close files\n\t    train_idx_file.close()\n\t    val_idx_file.close()\n\tif __name__ == '__main__':\n\t    args = argparse.ArgumentParser()\n\t    args.add_argument('--num_buildings', type=int, default=1000, required=False,)\n\t    args.add_argument('--seed', type=int, default=1, required=False,\n\t                        help='Random seed for KMeans and shuffling. Default: 1')\n", "    args.add_argument('--sliding_window_stride', type=int, default=24, required=False,\n\t                        help='Stride for sliding window to split timeseries into training examples. Default: 24 hours')\n\t    args.add_argument('--context_len', type=int, default=168, required=False,\n\t                                help='Length of context sequence. For handling year beginning and year end. Default: 168 hours')\n\t    args.add_argument('--pred_len', type=int, default=24, required=False,\n\t                                help='Length of prediction sequence. For handling year beginning and year end. Default: 24 hours')\n\t    args = args.parse_args()\n\t    main(args)\n"]}
{"filename": "scripts/fit_scaler_transforms.py", "chunked_list": ["import argparse\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pyarrow.parquet as pq\n\timport random\n\timport glob \n\timport os\n\tfrom buildings_bench.transforms import StandardScalerTransform, BoxCoxTransform \n\tdef main(args):\n\t    random.seed(args.seed)\n", "    np.random.seed(args.seed)\n\t    output_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'metadata')\n\t    # training set dir\n\t    time_series_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'Buildings-900K', 'end-use-load-profiles-for-us-building-stock', '2021')\n\t    building_years = ['comstock_tmy3_release_1', 'resstock_tmy3_release_1', 'comstock_amy2018_release_1', 'resstock_amy2018_release_1'] \n\t    pumas = ['by_puma_midwest', 'by_puma_south', 'by_puma_northeast', 'by_puma_west']\n\t    all_buildings = []\n\t    for by in building_years:\n\t        by_path = time_series_dir / by / 'timeseries_individual_buildings'\n\t        for pum in pumas:\n", "            pum_path = by_path / pum / 'upgrade=0'\n\t            # subsample pumas for faster quantization\n\t            pum_files = glob.glob(str(pum_path / 'puma=*'))\n\t            random.shuffle(pum_files)\n\t            # limit to 10 random pumas per\n\t            pum_files = pum_files[:10]\n\t            for pum_file in pum_files:\n\t                # load the parquet file and convert each column to a numpy array\n\t                #df = spark.read.parquet(pum_file)\n\t                df = pq.read_table(pum_file).to_pandas()\n", "                #df = df.toPandas()\n\t                # convert each column to a numpy array and stack vertically\n\t                all_buildings += [np.vstack([df[col].to_numpy() for col in df.columns if col != 'timestamp'])]\n\t    print('Fitting StandardScaler...')\n\t    ss = StandardScalerTransform()\n\t    ss.train(np.vstack(all_buildings))\n\t    ss.save(output_dir)\n\t    print('StandardScaler: ', ss.mean_, ss.std_)\n\t    print('Fitting BoxCox...')\n\t    bc = BoxCoxTransform()\n", "    bc.train(np.vstack(all_buildings))\n\t    bc.save(output_dir)\n\t    print('BoxCox: ', bc.boxcox.lambdas_)\n\tif __name__ == '__main__':\n\t    args = argparse.ArgumentParser()\n\t    args.add_argument('--seed', type=int, default=1, required=False,\n\t                        help='Random seed shuffling. Default: 1')\n\t    args = args.parse_args()\n\t    main(args)\n"]}
{"filename": "scripts/transfer_learning_torch.py", "chunked_list": ["from pathlib import Path\n\timport os\n\timport argparse \n\tfrom buildings_bench import utils\n\timport pandas as pd\n\timport torch\n\timport tomli \n\timport numpy as np\n\tfrom copy import deepcopy\n\tfrom buildings_bench import load_pandas_dataset, benchmark_registry\n", "from buildings_bench.data.datasets import PandasTransformerDataset\n\tfrom buildings_bench.data.datasets import keep_buildings\n\tfrom buildings_bench import utils\n\tfrom buildings_bench.tokenizer import LoadQuantizer\n\tfrom buildings_bench.evaluation.managers import DatasetMetricsManager\n\tfrom buildings_bench.models import model_factory\n\tfrom buildings_bench.evaluation import scoring_rule_factory\n\tSCRIPT_PATH = Path(os.path.realpath(__file__)).parent\n\tdef train(df_tr, df_val, args, model, transform, loss, lr, device):\n\t    torch_train_set = PandasTransformerDataset(df_tr, sliding_window=24)\n", "    train_dataloader = torch.utils.data.DataLoader(\n\t                                torch_train_set,\n\t                                batch_size=args.batch_size,\n\t                                num_workers=args.num_workers, \n\t                                shuffle=True)\n\t    torch_val_set = PandasTransformerDataset(df_val, sliding_window=24)\n\t    val_dataloader = torch.utils.data.DataLoader(\n\t                                torch_val_set,\n\t                                batch_size=args.batch_size,\n\t                                num_workers=args.num_workers,\n", "                                shuffle=False)\n\t    # Unfreeze the layer being fine-tuned\n\t    # and pass to the optimizer\n\t    params = model.unfreeze_and_get_parameters_for_finetuning()\n\t    if params is None:\n\t        raise ValueError('No parameters provided for fine-tuning. Did you mean to run with --eval_zero_shot?')\n\t    optimizer = torch.optim.AdamW(params, lr=lr) \n\t    model.train()\n\t    best_val = 100000\n\t    patience_counter = 0\n", "    # Fine tune the loaded model with frozen weights\n\t    for epoch in range(args.max_epochs):\n\t        losses = []\n\t        print(f'epoch {epoch}')\n\t        for batch in train_dataloader:\n\t            optimizer.zero_grad()\n\t            for k,v in batch.items():\n\t                batch[k] = v.to(device)\n\t            # Apply transform to load if needed\n\t            batch['load'] = transform(batch['load'])\n", "            with torch.cuda.amp.autocast():\n\t                preds = model(batch)\n\t                targets = batch['load'][:, model.context_len:]      \n\t                batch_loss = loss(preds, targets)\n\t            losses.append(batch_loss.item())\n\t            batch_loss.backward()\n\t            optimizer.step()\n\t        # run validation\n\t        losses_val = []\n\t        model.eval()\n", "        with torch.no_grad():\n\t            for batch in val_dataloader:\n\t                for k,v in batch.items():\n\t                    batch[k] = v.to(device)\n\t                # Apply transform to load if needed\n\t                batch['load'] = transform(batch['load'])\n\t                with torch.cuda.amp.autocast():\n\t                    preds = model(batch)\n\t                    targets = batch['load'][:, model.context_len:]\n\t                    batch_loss = loss(preds, targets)\n", "                losses_val.append(batch_loss.item())\n\t        epoch_val_loss = np.mean(losses_val)\n\t        model.train()\n\t        print('epoch train loss = ', np.mean(losses))\n\t        print('epoch val loss = ', np.mean(losses_val))\n\t        if epoch_val_loss > best_val:\n\t            patience_counter += 1\n\t        else:\n\t            patience_counter = 0\n\t            best_val = epoch_val_loss\n", "        if patience_counter > args.patience:\n\t            print(f'early stopping after epoch {epoch} with patience={args.patience}, best val loss {best_val}')\n\t            break\n\t    return model\n\tdef transfer_learning(args, model_args, results_path: Path):\n\t    global benchmark_registry\n\t    device = args.device\n\t    # load and configure the model for transfer learning\n\t    model, loss, _ = model_factory(args.config, model_args)\n\t    model = model.to(args.device)\n", "    transform_path = Path(os.environ.get('BUILDINGS_BENCH', '')) / 'metadata' / 'transforms'\n\t    if not model.continuous_loads:\n\t        load_transform = LoadQuantizer(\n\t            with_merge=(not args.tokenizer_without_merge),\n\t            num_centroids=model.vocab_size,\n\t            device='cuda:0' if 'cuda' in device else 'cpu')\n\t        load_transform.load(transform_path)\n\t    if args.checkpoint != '':\n\t        # By default, fine tune all layers\n\t        model.load_from_checkpoint(args.checkpoint)\n", "    model.train()\n\t    # remove synthetic\n\t    benchmark_registry = [b for b in benchmark_registry if b != 'buildings-900k-test']\n\t    if args.benchmark[0] == 'all':\n\t        args.benchmark = benchmark_registry\n\t    if args.ignore_scoring_rules:\n\t        metrics_manager = DatasetMetricsManager()\n\t    elif model.continuous_loads:\n\t        metrics_manager = DatasetMetricsManager(scoring_rule = scoring_rule_factory('crps'))\n\t    else:\n", "        metrics_manager = DatasetMetricsManager(scoring_rule = scoring_rule_factory('rps'))\n\t    target_buildings = []\n\t    if not args.dont_subsample_buildings:\n\t        metadata_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'metadata')\n\t        with open(metadata_dir / 'transfer_learning_commercial_buildings.txt', 'r') as f:\n\t            target_buildings += f.read().splitlines()\n\t        with open(metadata_dir / 'transfer_learning_residential_buildings.txt', 'r') as f:\n\t            target_buildings += f.read().splitlines()\n\t    for dataset in args.benchmark:\n\t        dataset_generator = load_pandas_dataset(dataset,\n", "                                                feature_set='transformer',\n\t                                                apply_scaler_transform=args.apply_scaler_transform,\n\t                                                scaler_transform_path=transform_path)\n\t        # Filter to target buildings\n\t        if len(target_buildings) > 0:\n\t            dataset_generator = keep_buildings(dataset_generator, target_buildings)\n\t        # Transforms\n\t        if not model.continuous_loads: \n\t            transform = load_transform.transform\n\t            inverse_transform = load_transform.undo_transform\n", "        elif args.apply_scaler_transform != '':\n\t            transform = lambda x: x\n\t            load_transform = dataset_generator.load_transform\n\t            inverse_transform = load_transform.undo_transform\n\t        else: # Continuous unscaled values\n\t            transform = lambda x: x\n\t            inverse_transform = lambda x: x\n\t        for building_name, bldg_df in dataset_generator:\n\t            # if date range is less than 120 days, skip - 90 days training, 30+ days eval.\n\t            if len(bldg_df) < (args.num_training_days+30)*24:\n", "                print(f'{dataset} {building_name} has too few days {len(bldg_df)}')\n\t                continue\n\t            print(f'dataset {dataset} building {building_name}')\n\t            metrics_manager.add_building_to_dataset_if_missing(\n\t                 dataset, building_name,\n\t            )\n\t            # Split into fine-tuning and evaluation set by date\n\t            # Get the first month of data from bldg_df by index\n\t            start_timestamp = bldg_df.index[0]\n\t            end_timestamp = start_timestamp + pd.Timedelta(days=args.num_training_days)\n", "            historical_date_range = pd.date_range(start=start_timestamp, end=end_timestamp, freq='H')\n\t            training_set = bldg_df.loc[historical_date_range]\n\t            training_start_timestamp = training_set.index[0]\n\t            training_end_timestamp = training_start_timestamp + pd.Timedelta(days=args.num_training_days-30)\n\t            train_date_range = pd.date_range(start=training_start_timestamp, end=training_end_timestamp, freq='H')\n\t            training_set_ = training_set.loc[train_date_range]\n\t            validation_set = training_set[~training_set.index.isin(train_date_range)]\n\t            test_set = bldg_df.loc[~bldg_df.index.isin(historical_date_range)]\n\t            test_start_timestamp = test_set.index[0]\n\t            test_end_timestamp = test_start_timestamp + pd.Timedelta(days=180)\n", "            #test_date_range = pd.date_range(start=test_start_timestamp, end=test_end_timestamp, freq='H')\n\t            #test_set = test_set.loc[test_date_range]\n\t            test_set = test_set[test_set.index <= test_end_timestamp]\n\t            print(f'fine-tune set date range: {training_set_.index[0]} {training_set_.index[-1]}, '\n\t                  f'test set date range: {test_set.index[0]} {test_set.index[-1]}')\n\t            if not args.eval_zero_shot:\n\t                # train the model\n\t                # we use deepcopy to avoid modifying the original model\n\t                tuned_model = train(training_set_, validation_set, args, deepcopy(model),\n\t                                    transform, loss, args.lr, args.device)\n", "            else:\n\t                tuned_model = deepcopy(model)\n\t            # do the evaluation on the building test data\n\t            torch_test_set = PandasTransformerDataset(test_set,\n\t                                                      sliding_window=24)\n\t            test_dataloader = torch.utils.data.DataLoader(\n\t                                        torch_test_set,\n\t                                        batch_size=360,\n\t                                        num_workers=args.num_workers,\n\t                                        shuffle=False)\n", "            tuned_model.eval()\n\t            with torch.no_grad():\n\t                for batch in test_dataloader:\n\t                    building_types_mask = batch['building_type'][:,0,0] == 1\n\t                    for k,v in batch.items():\n\t                        batch[k] = v.to(args.device)\n\t                    continuous_load = batch['load'].clone()\n\t                    continuous_targets = continuous_load[:, tuned_model.context_len:]\n\t                    # Transform if needed\n\t                    batch['load'] = transform(batch['load'])\n", "                    targets = batch['load'][:, tuned_model.context_len:]\n\t                    if args.device == 'cuda':\n\t                        with torch.cuda.amp.autocast():\n\t                            predictions, distribution_params = tuned_model.predict(batch)\n\t                    else:\n\t                        predictions, distribution_params = tuned_model.predict(batch)\n\t                    predictions = inverse_transform(predictions)\n\t                    if args.apply_scaler_transform != '':\n\t                        continuous_targets = inverse_transform(continuous_targets)\n\t                        targets = inverse_transform(targets)\n", "                        if args.apply_scaler_transform == 'standard':\n\t                            mu = inverse_transform(distribution_params[:,:,0])\n\t                            sigma = load_transform.undo_transform_std(distribution_params[:,:,1])\n\t                            distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n\t                        elif args.apply_scaler_transform == 'boxcox':\n\t                            ######## approximate Gaussian in unscaled space ########\n\t                            mu = inverse_transform(distribution_params[:,:,0])\n\t                            muplussigma = inverse_transform(torch.sum(distribution_params,-1))\n\t                            sigma = muplussigma - mu\n\t                            muminussigma = inverse_transform(distribution_params[:,:,0] - distribution_params[:,:,1])\n", "                            sigma = (sigma + (mu - muminussigma)) / 2\n\t                            distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n\t                    if not model.continuous_loads:\n\t                        centroids = load_transform.kmeans.centroids.squeeze() \\\n\t                            if args.tokenizer_without_merge else load_transform.merged_centroids\n\t                    else:\n\t                        centroids = None\n\t                    metrics_manager(\n\t                        dataset,\n\t                        building_name,\n", "                        continuous_targets,\n\t                        predictions,\n\t                        building_types_mask,\n\t                        y_categories=targets,\n\t                        y_distribution_params=distribution_params,\n\t                        centroids=centroids\n\t                    )\n\t    print('Generating summaries...')\n\t    variant_name = f':{args.variant_name}' if args.variant_name != '' else ''\n\t    metrics_file = results_path / f'TL_metrics_{args.config}{variant_name}.csv'\n", "    scoring_rule_file = results_path / f'TL_scoring_rule_{args.config}{variant_name}.csv'\n\t    if not args.ignore_scoring_rules:\n\t        metrics_df, scoring_rule_df = metrics_manager.summary()    \n\t        if metrics_file.exists():    \n\t            metrics_df.to_csv(metrics_file, mode='a', index=False, header=False)\n\t        else:\n\t            metrics_df.to_csv(metrics_file, index=False)\n\t        if scoring_rule_file.exists():\n\t            scoring_rule_df.to_csv(scoring_rule_file, mode='a', index=False, header=False)\n\t        else:\n", "            scoring_rule_df.to_csv(scoring_rule_file, index=False)\n\t    else:\n\t        metrics_df = metrics_manager.summary()    \n\t        if metrics_file.exists():    \n\t            metrics_df.to_csv(metrics_file, mode='a', index=False, header=False)\n\t        else:\n\t            metrics_df.to_csv(metrics_file, index=False)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--results_path', type=str, default='results/')\n", "    parser.add_argument('--config', type=str, default='', required=True)\n\t    parser.add_argument('--benchmark', nargs='+', type=str, default=['all'],\n\t                        help='Which datasets in the benchmark to run. Default is [\"all.\"] '\n\t                             'See the dataset registry in buildings_bench.data.__init__.py for options.')\n\t    parser.add_argument('--num_workers', type=int, default=2)\n\t    parser.add_argument('--seed', type=int, default=1)\n\t    parser.add_argument('--ignore_scoring_rules', action='store_true', help='Do not compute a scoring rule')\n\t    parser.add_argument('--variant_name', type=str, default='',\n\t                        help='Name of the variant. Optional. Used for results files.')\n\t    parser.add_argument('--device', type=str, default='cuda')\n", "    parser.add_argument('--tokenizer_without_merge', action='store_true', default=False, \n\t                        help='Use the tokenizer without merge. Default is False.')\n\t    parser.add_argument('--apply_scaler_transform', type=str, default='',\n\t                        choices=['', 'standard', 'boxcox'], \n\t                        help='Apply a scaler transform to the load values.')\n\t    # Transfer Learning - model\n\t    parser.add_argument('--checkpoint', type=str, default='',\n\t                        help='Path to a checkpoint to load.')\n\t    parser.add_argument('--max_epochs', type=int, default=25)\n\t    parser.add_argument('--lr', type=float, default=1e-6) # 1e-4, 1e-5, 1e-6\n", "    parser.add_argument('--batch_size', type=int, default=16)\n\t    parser.add_argument('--patience', type=int, default=2)\n\t    # Transfer learning - data\n\t    parser.add_argument('--num_training_days', type=int, default=180,\n\t                        help='Number of days for fine-tuning (last 30 used for early stopping)')\n\t    parser.add_argument('--dont_subsample_buildings', action='store_true', default=False,\n\t                        help='Evaluate on all instead of a subsample of 100 res/com buildings')    \n\t    parser.add_argument('--eval_zero_shot', action='store_true', default=False,\n\t                        help='Evaluate on the test data without fine-tuning, '\n\t                             'useful for getting baseline perf')\n", "    args = parser.parse_args()\n\t    utils.set_seed(args.seed)\n\t    config_path = SCRIPT_PATH  / '..' / 'buildings_bench' / 'configs'\n\t    if (config_path / f'{args.config}.toml').exists():\n\t            toml_args = tomli.load(( config_path / f'{args.config}.toml').open('rb'))\n\t            model_args = toml_args['model']\n\t            if 'transfer_learning' in toml_args:\n\t                for k,v in toml_args['transfer_learning'].items():\n\t                    setattr(args, k, v)\n\t            if not model_args['continuous_loads']:\n", "                setattr(args, 'apply_scaler_transform', '')\n\t    else:\n\t        raise ValueError(f'Config {args.config}.toml not found.')\n\t    results_path = Path(args.results_path)\n\t    results_path.mkdir(parents=True, exist_ok=True)\n\t    transfer_learning(args, model_args, results_path)\n"]}
{"filename": "scripts/fit_tokenizer.py", "chunked_list": ["import argparse\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pyarrow.parquet as pq\n\timport random\n\timport glob \n\timport os\n\tfrom buildings_bench.tokenizer import LoadQuantizer\n\tdef main(args):\n\t    random.seed(args.seed)\n", "    np.random.seed(args.seed)\n\t    output_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'metadata')\n\t    time_series_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'Buildings-900K', 'end-use-load-profiles-for-us-building-stock', '2021')    \n\t    building_years = ['comstock_tmy3_release_1', 'resstock_tmy3_release_1', 'comstock_amy2018_release_1', 'resstock_amy2018_release_1'] \n\t    pumas = ['by_puma_midwest', 'by_puma_south', 'by_puma_northeast', 'by_puma_west']\n\t    all_buildings = []\n\t    num_buildings = 0\n\t    for by in building_years:\n\t        by_path = time_series_dir / by / 'timeseries_individual_buildings'\n\t        for pum in pumas:\n", "            pum_path = by_path / pum / 'upgrade=0'\n\t            pum_files = glob.glob(str(pum_path / 'puma=*'))\n\t            # subsample pumas for faster quantization to 10 per census region\n\t            random.shuffle(pum_files)\n\t            pum_files = pum_files[:10]\n\t            for pum_file in pum_files:\n\t                # load the parquet file and convert each column to a numpy array\n\t                df = pq.read_table(pum_file).to_pandas()\n\t                # convert each column to a numpy array and stack vertically\n\t                all_buildings += [np.vstack([df[col].to_numpy() for col in df.columns if col != 'timestamp'])]\n", "                num_buildings += len(all_buildings[-1])\n\t    print(f'Loaded {num_buildings} buildings for tokenization')\n\t    lq = LoadQuantizer(args.seed, \n\t                       num_centroids=args.num_clusters,\n\t                       with_merge=(not args.without_merge),\n\t                       merge_threshold=args.merge_threshold,\n\t                       device=args.device)\n\t    lq.train(np.vstack(all_buildings))\n\t    lq.save(output_dir)\n\tif __name__ == '__main__':\n", "    args = argparse.ArgumentParser()\n\t    args.add_argument('--num_clusters', type=int, default=8192, required=False,\n\t                        help='Number of clusters for KMeans. Default: 8192')\n\t    args.add_argument('--without_merge', action='store_true',\n\t                        help='Do not merge clusters in KMeans. Default: False')\n\t    args.add_argument('--merge_threshold', type=float, default=0.01, required=False,\n\t                        help='Threshold for merging clusters during tokenization. Default: 0.01') \n\t    args.add_argument('--device', type=str, default='cuda:0', required=False,\n\t                            help='Device to use. Default: cuda:0')\n\t    args.add_argument('--seed', type=int, default=1, required=False,\n", "                        help='Random seed for KMeans and shuffling. Default: 1')\n\t    args = args.parse_args()\n\t    main(args)\n"]}
{"filename": "scripts/zero_shot.py", "chunked_list": ["import torch \n\tfrom pathlib import Path\n\timport argparse \n\timport os\n\timport tomli\n\tfrom buildings_bench import load_torch_dataset, benchmark_registry\n\tfrom buildings_bench import utils\n\tfrom buildings_bench.tokenizer import LoadQuantizer\n\tfrom buildings_bench.evaluation.managers import DatasetMetricsManager\n\tfrom buildings_bench.models import model_factory\n", "from buildings_bench.evaluation import scoring_rule_factory\n\tSCRIPT_PATH = Path(os.path.realpath(__file__)).parent\n\t@torch.no_grad()\n\tdef zero_shot_learning(args, model_args, results_path: Path):\n\t    device = args.device\n\t    model, _, predict = model_factory(args.config, model_args)\n\t    model = model.to(device)\n\t    transform_path = Path(os.environ.get('BUILDINGS_BENCH', '')) \\\n\t        / 'metadata' / 'transforms'\n\t    if not model.continuous_loads:   \n", "        load_transform = LoadQuantizer(\n\t            with_merge=(not args.tokenizer_without_merge),\n\t              num_centroids=model.vocab_size,\n\t              device='cuda:0' if 'cuda' in device else 'cpu')\n\t        load_transform.load(transform_path)\n\t    # Load from ckpts\n\t    if args.checkpoint != '':\n\t        model.load_from_checkpoint(args.checkpoint)\n\t    model.eval()\n\t    if args.benchmark[0] == 'all':\n", "        args.benchmark = benchmark_registry\n\t    if args.ignore_scoring_rules:\n\t        metrics_manager = DatasetMetricsManager()\n\t    elif model.continuous_loads:\n\t        metrics_manager = DatasetMetricsManager(scoring_rule = scoring_rule_factory('crps'))\n\t    else:\n\t        metrics_manager = DatasetMetricsManager(scoring_rule = scoring_rule_factory('rps'))\n\t    print(f'Evaluating model on test datasets {args.benchmark}...')\n\t    # Iterate over each dataset in the benchmark\n\t    for dataset_name in args.benchmark:\n", "        # Load the dataset generator\n\t        buildings_datasets_generator = load_torch_dataset(dataset_name,\n\t                                                    apply_scaler_transform=args.apply_scaler_transform,\n\t                                                    scaler_transform_path=transform_path)\n\t        # For each building\n\t        for building_name, building_dataset in buildings_datasets_generator:\n\t            print(f'dataset {dataset_name} building-year {building_name} '\n\t                    f'day-ahead forecasts {len(building_dataset)}')\n\t            metrics_manager.add_building_to_dataset_if_missing(\n\t                dataset_name, building_name,\n", "            )\n\t            if not model.continuous_loads: # Quantized loads\n\t                 transform = load_transform.transform\n\t                 inverse_transform = load_transform.undo_transform\n\t            elif args.apply_scaler_transform != '': # Scaling continuous values\n\t                 transform = lambda x: x \n\t                 if isinstance(building_dataset, torch.utils.data.ConcatDataset):\n\t                     load_transform = building_dataset.datasets[0].load_transform\n\t                     inverse_transform = load_transform.undo_transform\n\t                 else:\n", "                     load_transform = building_dataset.load_transform\n\t                     inverse_transform = load_transform.undo_transform\n\t            else: # Continuous unscaled values\n\t                 transform = lambda x: x\n\t                 inverse_transform = lambda x: x\n\t            # create a dataloader for the building\n\t            building_dataloader = torch.utils.data.DataLoader(\n\t                                    building_dataset,\n\t                                    batch_size=args.batch_size,\n\t                                    shuffle=False)\n", "            for batch in building_dataloader:\n\t                building_types_mask = batch['building_type'][:,0,0] == 1\n\t                for k,v in batch.items():\n\t                    batch[k] = v.to(device)\n\t                continuous_load = batch['load'].clone()\n\t                continuous_targets = continuous_load[:, model.context_len:]\n\t                # Transform if needed\n\t                batch['load'] = transform(batch['load'])\n\t                # These could be tokens or continuous\n\t                targets = batch['load'][:, model.context_len:]\n", "                if args.device == 'cuda':\n\t                    with torch.cuda.amp.autocast():\n\t                        predictions, distribution_params = predict(batch)\n\t                else:\n\t                    predictions, distribution_params = predict(batch)\n\t                predictions = inverse_transform(predictions)\n\t                if args.apply_scaler_transform != '':\n\t                    continuous_targets = inverse_transform(continuous_targets)\n\t                    # invert for crps\n\t                    targets = inverse_transform(targets)\n", "                    if args.apply_scaler_transform == 'standard':\n\t                        mu = inverse_transform(distribution_params[:,:,0])\n\t                        sigma = load_transform.undo_transform_std(distribution_params[:,:,1])\n\t                        distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n\t                    elif args.apply_scaler_transform == 'boxcox':\n\t                        ######## backproject approximate Gaussian in unscaled space ########\n\t                        mu = inverse_transform(distribution_params[:,:,0])\n\t                        muplussigma = inverse_transform(torch.sum(distribution_params,-1))\n\t                        sigma = muplussigma - mu\n\t                        muminussigma = inverse_transform(distribution_params[:,:,0] - distribution_params[:,:,1])\n", "                        sigma = (sigma + (mu - muminussigma)) / 2\n\t                        distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n\t                if not model.continuous_loads:\n\t                    centroids = load_transform.kmeans.centroids.squeeze() \\\n\t                        if args.tokenizer_without_merge else load_transform.merged_centroids\n\t                else:\n\t                    centroids = None\n\t                metrics_manager(\n\t                    dataset_name,\n\t                    building_name,\n", "                    continuous_targets,\n\t                    predictions,\n\t                    building_types_mask,\n\t                    y_categories=targets,\n\t                    y_distribution_params=distribution_params,\n\t                    centroids=centroids\n\t                )\n\t    print('Generating summaries...')\n\t    variant_name = f':{args.variant_name}' if args.variant_name != '' else ''\n\t    metrics_file = results_path / f'metrics_{args.config}{variant_name}.csv'\n", "    scoring_rule_file = results_path / f'scoring_rule_{args.config}{variant_name}.csv'\n\t    if not args.ignore_scoring_rules:\n\t        metrics_df, scoring_rule_df = metrics_manager.summary()    \n\t        if metrics_file.exists():    \n\t            metrics_df.to_csv(metrics_file, mode='a', index=False, header=False)\n\t        else:\n\t            metrics_df.to_csv(metrics_file, index=False)\n\t        if scoring_rule_file.exists():\n\t            scoring_rule_df.to_csv(scoring_rule_file, mode='a', index=False, header=False)\n\t        else:\n", "            scoring_rule_df.to_csv(scoring_rule_file, index=False)\n\t    else:\n\t        metrics_df = metrics_manager.summary()    \n\t        if metrics_file.exists():    \n\t            metrics_df.to_csv(metrics_file, mode='a', index=False, header=False)\n\t        else:\n\t            metrics_df.to_csv(metrics_file, index=False)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--results_path', type=str, default='results/')\n", "    parser.add_argument('--config', type=str, default='', required=True)\n\t    parser.add_argument('--benchmark', nargs='+', type=str, default=['all'],\n\t                        help='Which datasets in the benchmark to run. Default is [\"all.\"] '\n\t                             'See the dataset registry in buildings_bench.data.__init__.py for options.')\n\t    parser.add_argument('--num_workers', type=int, default=4)\n\t    parser.add_argument('--batch_size', type=int, default=360)\n\t    parser.add_argument('--seed', type=int, default=1)\n\t    parser.add_argument('--ignore_scoring_rules', action='store_true', \n\t                        help='Do not compute a scoring rule for this model.')\n\t    parser.add_argument('--checkpoint', type=str, default='',\n", "                        help='Path to a checkpoint to load. Optional. '\n\t                        ' One can also load a checkpoint from Wandb by specifying the run_id.')\n\t    parser.add_argument('--variant_name', type=str, default='',\n\t                        help='Name of the variant. Optional. Used for results files.')\n\t    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',\n\t                        help='Device to use. Default is cuda if available else cpu.')\n\t    parser.add_argument('--tokenizer_without_merge', action='store_true', default=False, \n\t                        help='Use the tokenizer without merge. Default is False.')\n\t    parser.add_argument('--apply_scaler_transform', type=str, default='',\n\t                        choices=['', 'standard', 'boxcox'], \n", "                        help='Apply a scaler transform to the load values.')\n\t    args = parser.parse_args()\n\t    utils.set_seed(args.seed)\n\t    config_path = SCRIPT_PATH  / '..' / 'buildings_bench' / 'configs'\n\t    if (config_path / f'{args.config}.toml').exists():\n\t        toml_args = tomli.load(( config_path / f'{args.config}.toml').open('rb'))\n\t        model_args = toml_args['model']\n\t        if 'zero_shot' in toml_args:\n\t            for k,v in toml_args['zero_shot'].items():\n\t                setattr(args, k, v)\n", "        if not model_args['continuous_loads'] or 'apply_scaler_transform' not in args:\n\t            setattr(args, 'apply_scaler_transform', '')\n\t    else:\n\t        raise ValueError(f'Config {args.config}.toml not found.')\n\t    results_path = Path(args.results_path)\n\t    results_path.mkdir(parents=True, exist_ok=True)\n\t    zero_shot_learning(args, model_args, results_path)\n"]}
{"filename": "scripts/subsample_buildings_for_transfer_learning.py", "chunked_list": ["\"\"\"\n\tRandomly select 100 residential and 100 commercial buildings \n\tfrom BuildingsBench for transfer learning.\n\t\"\"\"\n\tfrom buildings_bench import load_pandas_dataset, benchmark_registry\n\tfrom pathlib import Path\n\timport numpy as np\n\timport random\n\timport os\n\tdataset_path = Path(os.environ.get('BUILDINGS_BENCH', ''))\n", "datasets = [b for b in benchmark_registry if b != 'buildings-900k-test']\n\tdef consecutive(arr):\n\t    if len(arr) == 1:\n\t        return True\n\t    diffs = arr[1:] - arr[:-1]\n\t    if (diffs>1).any():\n\t        return False\n\t    return True\n\tresidential_buildings = set()\n\tcommercial_buildings = set()\n", "for dataset in datasets:\n\t    building_datasets_generator = load_pandas_dataset(dataset, dataset_path =dataset_path)\n\t    for building_name, building_dataset in building_datasets_generator:\n\t        years = building_dataset.index.year.unique()\n\t        years = np.sort(years)\n\t        # ensure the data we have across multiple years spans consecutive years,\n\t        # else we might try to use data from Dec 2014 and Jan 2016 for example.\n\t        # \n\t        # also ensure the building has at least ~210 days of data\n\t        if consecutive(years) and len(building_dataset) > (180+30)*24:\n", "            if building_datasets_generator.building_type == 'residential':\n\t                residential_buildings.add(building_name)\n\t            elif building_datasets_generator.building_type == 'commercial':\n\t                commercial_buildings.add(building_name)\n\t# sample 100 residential buildings\n\tresidential_buildings = list(residential_buildings)\n\tcommercial_buildings = list(commercial_buildings)\n\tnp.random.seed(100)\n\trandom.seed(100)\n\tresidential_buildings = random.sample(residential_buildings, 100)\n", "commercial_buildings = random.sample(commercial_buildings, 100)\n\twith open(dataset_path / 'metadata' / 'transfer_learning_residential_buildings.txt', 'w') as f:\n\t    for b in residential_buildings:\n\t        f.write(f'{b}\\n')\n\twith open(dataset_path / 'metadata' / 'transfer_learning_commercial_buildings.txt', 'w') as f:\n\t    for b in commercial_buildings:\n\t        f.write(f'{b}\\n')\n"]}
{"filename": "scripts/create_index_files.py", "chunked_list": ["import argparse\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pyarrow.parquet as pq\n\timport random\n\timport glob \n\timport pandas as pd\n\timport os\n\tfrom tqdm import tqdm\n\tdef main(args):\n", "    random.seed(args.seed)\n\t    np.random.seed(args.seed)\n\t    output_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'metadata')\n\t    time_series_dir = Path(os.environ.get('BUILDINGS_BENCH', ''), 'Buildings-900K', 'end-use-load-profiles-for-us-building-stock', '2021')\n\t    building_years = ['comstock_tmy3_release_1', 'resstock_tmy3_release_1', 'comstock_amy2018_release_1', 'resstock_amy2018_release_1'] \n\t    pumas = ['by_puma_midwest', 'by_puma_south', 'by_puma_northeast', 'by_puma_west']\n\t    # Each line in the index file indicates a building and n \n\t    #   <building_type_and_year> <census_region> <puma_id> <building_id> <seq ptr>\n\t    #  e.g. <0-4> <0-4> G17031 23023 65\n\t    train_idx_file = open(output_dir / f'train_weekly.idx', 'w')\n", "    val_idx_file = open(output_dir / f'val_weekly.idx', 'w')\n\t    # withhold 1 puma from each census region (all res and com buildingss) for test only\n\t    # midwest, south, northeast, west\n\t    # read withheld pumas from file\n\t    with open(Path(os.environ.get('BUILDINGS_BENCH','')) / 'metadata' / 'withheld_pumas.tsv', 'r') as f:\n\t        # tab separated file\n\t        line = f.readlines()[0]\n\t        withheld_pumas = line.strip('\\n').split('\\t')\n\t    print(f'Withheld pumas: {withheld_pumas}')\n\t    # 2 weeks heldout for val\n", "    train_tmy_timerange = (pd.Timestamp('2018-01-01'), pd.Timestamp('2018-12-31'))\n\t    train_amy2018_timerange = (pd.Timestamp('2018-01-01'), pd.Timestamp('2018-12-17'))\n\t    val_timerange = (pd.Timestamp('2018-12-17'), pd.Timestamp('2018-12-31'))\n\t    for building_type_and_year, by in enumerate(building_years):\n\t        by_path = time_series_dir / by / 'timeseries_individual_buildings'\n\t        if 'amy2018' in by:\n\t            train_hours = int((train_amy2018_timerange[1] - train_amy2018_timerange[0]).total_seconds() / 3600)\n\t            print(f'AMY2018 train hours: {train_hours}')\n\t            val_hours = int((val_timerange[1] - val_timerange[0]).total_seconds() / 3600)\n\t            print(f'AMY2018 Val hours: {val_hours}')\n", "        else:\n\t            train_hours = int((train_tmy_timerange[1] - train_tmy_timerange[0]).total_seconds() / 3600)\n\t            print(f'TMY train hours: {train_hours}')\n\t        for census_region_idx, pum in enumerate(pumas): # census regions\n\t            pum_path = by_path / pum / 'upgrade=0'\n\t            pum_files = glob.glob(str(pum_path / 'puma=*'))\n\t            for pum_file in tqdm(pum_files):\n\t                try:\n\t                    bldg_ids = pq.read_table(pum_file).to_pandas().columns[1:]\n\t                except:\n", "                    print(f'Failed to read {pum_file}')\n\t                    import pdb; pdb.set_trace()\n\t                    continue\n\t                # skip withheld pumas in train and val\n\t                if not os.path.basename(pum_file) in withheld_pumas:\n\t                    for bldg_id in bldg_ids:\n\t                        # train\n\t                        bldg_id = bldg_id.zfill(6)\n\t                        # Sample the starting index between (0,24)\n\t                        s_start = np.random.randint(0, 24)\n", "                        for s_idx in range(s_start, train_hours - (args.context_len + args.pred_len), args.sliding_window_stride):                            \n\t                            seq_ptr = str(args.context_len + s_idx).zfill(4)  # largest seq ptr is < 10000\n\t                            # NB: We don't *need* \\n at the end of each line, but it makes it easier to count # of lines for dataloading\n\t                            linestr = f'{building_type_and_year}\\t{census_region_idx}\\t{os.path.basename(pum_file).split(\"=\")[1]}\\t{bldg_id}\\t{seq_ptr}\\n'\n\t                            assert len(linestr) == 26, f'linestr: {linestr}'\n\t                            train_idx_file.write(linestr)\n\t                        # val\n\t                        if 'amy2018' in by:\n\t                            s_start += train_hours \n\t                            for s_idx in range(s_start, (train_hours + val_hours) - (args.context_len + args.pred_len), args.sliding_window_stride):\n", "                                seq_ptr = str(args.context_len + s_idx).zfill(4)  # largest seq ptr is < 10000\n\t                                assert len(linestr) == 26, f'linestr: {linestr}'\n\t                                linestr = f'{building_type_and_year}\\t{census_region_idx}\\t{os.path.basename(pum_file).split(\"=\")[1]}\\t{bldg_id}\\t{seq_ptr}\\n'\n\t                                val_idx_file.write(linestr)\n\t    # Close files\n\t    train_idx_file.close()\n\t    val_idx_file.close()\n\tif __name__ == '__main__':\n\t    args = argparse.ArgumentParser()\n\t    args.add_argument('--seed', type=int, default=1, required=False,\n", "                        help='Random seed for KMeans and shuffling. Default: 1')\n\t    args.add_argument('--sliding_window_stride', type=int, default=24, required=False,\n\t                        help='Stride for sliding window to split timeseries into training examples. Default: 24 hours')\n\t    args.add_argument('--context_len', type=int, default=168, required=False,\n\t                                help='Length of context sequence. For handling year beginning and year end. Default: 168 hours')\n\t    args.add_argument('--pred_len', type=int, default=24, required=False,\n\t                                help='Length of prediction sequence. For handling year beginning and year end. Default: 24 hours')\n\t    args = args.parse_args()\n\t    main(args)\n"]}
{"filename": "scripts/process_raw_data/download_and_process_buildingsbench.py", "chunked_list": ["\"\"\"\n\tBasic preprocessing steps for real building data\n\t1. Resample sub-hourly kW data to hourly kWh data\n\t2. Remove buildings with more than 10% of missing values\n\t3. Linearly interpolate missing hour data\n\t4. Save to csv\n\t\"\"\"\n\timport pandas as pd\n\timport numpy as np\n\timport glob\n", "from pathlib import Path \n\timport os\n\timport random\n\tdef download_electricity_data(savedir):\n\t    url = 'https://archive.ics.uci.edu/static/public/321/electricityloaddiagrams20112014.zip'\n\t    os.system(f'wget {url}')\n\t    os.system('unzip electricityloaddiagrams20112014.zip')\n\t    os.system('rm electricityloaddiagrams20112014.zip')\n\t    os.system(f'mv LD2011_2014.txt {savedir}')\n\tdef download_bdg2_data(savedir):\n", "    url = 'https://github.com/buds-lab/building-data-genome-project-2'\n\t    print(f'use git lfs to download the data from the git repo {url} '\n\t          f' then copy it to {savedir}')\n\tdef download_sceaux_data(savedir):\n\t    url = 'https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set'\n\t    print(f'Log into Kaggle then download the dataset from {url}. Unzip the file archive.zip and copy '\n\t          f'the file household_power_consumption.txt to {savedir}')\n\tdef download_smart_data(savedir):\n\t    url_Homes = ['https://lass.cs.umass.edu/smarttraces/2017/HomeA-electrical.tar.gz',\n\t                 'https://lass.cs.umass.edu/smarttraces/2017/HomeB-electrical.tar.gz',\n", "                 'https://lass.cs.umass.edu/smarttraces/2017/HomeC-electrical.tar.gz',\n\t                 'https://lass.cs.umass.edu/smarttraces/2017/HomeD-electrical.tar.gz',\n\t                 'https://lass.cs.umass.edu/smarttraces/2017/HomeF-electrical.tar.gz',\n\t                 'https://lass.cs.umass.edu/smarttraces/2017/HomeG-electrical.tar.gz',\n\t                 'https://lass.cs.umass.edu/smarttraces/2017/HomeH-electrical.tar.gz',\n\t    ]\n\t    # untar each \n\t    for url in url_Homes:\n\t        os.system(f'wget {url}')\n\t        os.system(f'tar -xzf {url.split(\"/\")[-1]}')\n", "        os.system(f'rm {url.split(\"/\")[-1]}')\n\t        # move each to savedir\n\t        os.system(f'mv {url.split(\"/\")[-1].split(\"-\")[0]} {savedir}')\n\tdef download_borealis_data(savedir):\n\t    url = 'https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP2/R4SVBF'\n\t    print(f'Navigate to {url} and download the .zip datasest. Unzip the file and copy '\n\t          f'the folder 6secs_load_measurement_dataset to {savedir}')\n\tdef download_ideal_data(savedir):\n\t    url = 'https://datashare.ed.ac.uk/bitstream/handle/10283/3647/household_sensors.zip'\n\t    os.system(f'wget {url}')\n", "    os.system(f'unzip household_sensors.zip')\n\t    os.system(f'rm household_sensors.zip')\n\t    os.system(f'mv sensordata {savedir}')\n\tdef download_lcl_data(savedir):\n\t    url = 'https://data.london.gov.uk/download/smartmeter-energy-use-data-in-london-households/04feba67-f1a3-4563-98d0-f3071e3d56d1/Partitioned%20LCL%20Data.zip'\n\t    os.system(f'wget {url}')\n\t    os.system(f'unzip Partitioned\\ LCL\\ Data.zip')\n\t    os.system(f'rm Partitioned\\ LCL\\ Data.zip')\n\t    os.system(f'mv Small\\ LCL\\ Data/ {savedir}')\n\tif __name__ == '__main__':\n", "    dataset_dir = Path(os.environ.get('BUILDINGS_BENCH', ''))\n\t    print('WARNING: This script will try to automatically download and process the datasets in BuildingsBench.'\n\t          ' Please be sure to have followed the instructions for downloading the data from the following datasets, '\n\t          'which are not able to be automatically downloaded: ')\n\t    print('>>> BDG-2')\n\t    download_bdg2_data(dataset_dir / 'BDG-2')\n\t    print('>>> Sceaux')\n\t    download_sceaux_data(dataset_dir / 'Sceaux')\n\t    print('>>> Borealis')\n\t    download_borealis_data(dataset_dir / 'Borealis')    \n", "    ########################################################\n\t    print('Electricity...')\n\t    ########################################################       \n\t    (dataset_dir / 'Electricity').mkdir(parents=True, exist_ok=True)\n\t    download_electricity_data(dataset_dir / 'Electricity')\n\t    df = pd.read_csv(dataset_dir / 'Electricity' / 'LD2011_2014.txt', sep=';', index_col=0)\n\t    df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n\t    df = df.asfreq('15min')\n\t    df = df.sort_index()\n\t    # Set the type of each column to float\n", "    # Convert commas in each string to a period\n\t    for col in df.columns:\n\t        # if column entries are type str\n\t        if df[col].dtype == 'object' and col != 'index':\n\t            df[col] = df[col].apply(lambda x: float(x.replace(',', '.') if type(x) is str else x))\n\t    df = df.astype('float32')\n\t    # Resample 15 min -> hourly\n\t    df = df.resample(rule='H', closed='left', label='right').mean()\n\t    # Group buildings by year\n\t    years = [2011, 2012, 2013, 2014]\n", "    for year in years:\n\t        bldgs = df[df.index.year == year]\n\t        # Replace zeros with nan for this dataset, which uses zeros to indicate missing values\n\t        bldgs = bldgs.replace(0, np.nan)\n\t        # Drop buildings with more than 10% of missing values\n\t        bldgs = bldgs.dropna(thresh=len(bldgs)*0.9, axis=1)\n\t        # linearly interpolate nan values\n\t        bldgs = bldgs.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both')\n\t        # Replace any remaining nan values with zeros (> 1 week)\n\t        bldgs = bldgs.fillna(0)\n", "        # Name the index column 'timestamp'\n\t        bldgs.index.name = 'timestamp'\n\t        bldgs = bldgs.asfreq('1H')\n\t        # Save to csv\n\t        bldgs.to_csv(dataset_dir / 'Electricity' / f'LD2011_2014_clean={year}.csv', index=True, header=True)\n\t    ########################################################\n\t    print('BDG-2...')\n\t    ########################################################\n\t    (dataset_dir / 'BDG-2').mkdir(parents=True, exist_ok=True)\n\t    try:\n", "        df = pd.read_csv(dataset_dir / 'electricity_cleaned.csv', sep=',')\n\t         # set timestamp as index\n\t        df = df.set_index('timestamp')\n\t        df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n\t        sites = ['Bear_', 'Fox_', 'Panther_', 'Rat_']\n\t        years = [2016, 2017]\n\t        for s in sites:\n\t            # regex filter for Panther\n\t            df_site = df.filter(regex=s)\n\t            for year in years:\n", "                bldgs = df_site[df_site.index.year == year]\n\t                # Drop columns with more than 10% of missing values\n\t                bldgs = bldgs.dropna(thresh=len(bldgs)*0.9, axis=1)\n\t                # linearly interpolate nan values (1 week limit)\n\t                bldgs = bldgs.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both')\n\t                # Replace any remaining nan values with zeros (> 1 week)\n\t                bldgs = bldgs.fillna(0)\n\t                sit = s.replace('_', '')\n\t                bldgs.to_csv(dataset_dir / 'BDG-2' / f'{sit}_clean={year}.csv', header=True, index=True)\n\t    except:\n", "        download_bdg2_data(dataset_dir / 'BDG-2')\n\t        print('skipping BDG-2...')\n\t    ########################################################\n\t    print('Sceaux...')\n\t    ########################################################\n\t    try:\n\t        (dataset_dir / 'Sceaux').mkdir(parents=True, exist_ok=True)\n\t        house = pd.read_csv(dataset_dir / 'Sceaux' / 'household_power_consumption.txt', sep=';', header=0)\n\t        # Combine Date and Time columns\n\t        house['timestamp'] = house['Date'] + ' ' + house['Time']\n", "        # Set timestamp as index\n\t        house = house.set_index('timestamp')\n\t        # Convert index to datetime\n\t        house.index = pd.to_datetime(house.index, format='%d/%m/%Y %H:%M:%S')\n\t        # Drop Date and Time columns\n\t        house = house.drop(['Date', 'Time'], axis=1)\n\t        # Replace ? values with NaN\n\t        house = house.replace('?', np.nan)\n\t        house = house.astype('float32')\n\t        house = house.asfreq('1min')\n", "        house = house.sort_index()\n\t        # resample to hourly data, ignoring nan values\n\t        house = house.resample(rule='H', closed='left', label='right').mean()\n\t        years = [2007, 2008, 2009, 2010]\n\t        for year in years:\n\t            h = house[house.index.year == year]\n\t            # Linearly interpolate missing values\n\t            h = h.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both')\n\t            # Fill remaining missing values with zeros\n\t            h = h.fillna(0)\n", "            # Keep Global_active_power column\n\t            h = h[['Global_active_power']]\n\t            # Save to csv\n\t            h.to_csv(dataset_dir / 'Sceaux' / f'Sceaux_clean={year}.csv', header=True, index=True)\n\t    except:\n\t        download_sceaux_data(dataset_dir / 'Sceaux')\n\t        print('skipping Sceaux...')\n\t    ########################################################\n\t    print('SMART...')\n\t    ########################################################\n", "    (dataset_dir / 'SMART').mkdir(parents=True, exist_ok=True)\n\t    download_smart_data(dataset_dir / 'SMART')\n\t    homes = [('HomeB', 'meter1'), ('HomeC', 'meter1'), ('HomeD', 'meter1'), ('HomeF', 'meter2'), ('HomeG', 'meter1'), ('HomeH', 'meter1')]\n\t    years = [2014, 2015, 2016]\n\t    power_column = 'use [kW]'\n\t    # Load data\n\t    for idx in range(len(homes)):\n\t        home = homes[idx][0]\n\t        meter = homes[idx][1]\n\t        for year in years:\n", "            try:\n\t                # join on timestamp index\n\t                house_ = pd.read_csv(dataset_dir / 'SMART' / f'{home}/{year}/{home}-{meter}_{year}.csv')\n\t            except:\n\t                continue\n\t            # Rename Date & Time to timestamp\n\t            house_ = house_.rename(columns={'Date & Time': 'timestamp'})\n\t            # Set timestamp as index\n\t            house_ = house_.set_index('timestamp')\n\t            # Convert index to datetime\n", "            house_.index = pd.to_datetime(house_.index, format='%Y-%m-%d %H:%M:%S')\n\t            # Replace missing values with NaN\n\t            house_ = house_.replace('?', np.nan)\n\t            # Resample to hourly\n\t            house_ = house_.resample(rule='H', closed='left', label='right').mean()\n\t            # only keep use [kW] column\n\t            if 'use [kW]' in house_.columns:\n\t                house_ = house_[['use [kW]']]\n\t                power_column = 'use [kW]'\n\t            if 'Usage [kW]' in house_.columns:\n", "                house_ = house_[['Usage [kW]']]\n\t                power_column = 'Usage [kW]'\n\t            house_ = house_.rename(columns={power_column: 'power'})\n\t            missing_frac = house_['power'].isnull().sum() / house_.shape[0]\n\t            # Calculate fraction of missing values\n\t            #print(f'Fraction of NaN values in {home}-{meter} {year}: {missing_frac}')\n\t            # Calculate fraction of 0's\n\t            zeros_frac = (house_['power'] == 0.0).sum() / house_.shape[0]\n\t            #print(f'Fraction of 0 values in {home}-{meter} {year}: {zeros_frac}')\n\t            if missing_frac <= 0.1 and zeros_frac <= 0.1:\n", "                # Linearly interpolate missing values\n\t                house_ = house_.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both') \n\t                # Fill remaining missing values with zeros\n\t                house_ = house_.fillna(0)\n\t                house_.to_csv(dataset_dir / 'SMART' / f'{home}_clean={year}.csv', header=True, index=True)\n\t    ########################################################\n\t    print('Borealis...')\n\t    ########################################################\n\t    (dataset_dir / 'Borealis').mkdir(parents=True, exist_ok=True)\n\t    try:\n", "        homes = glob.glob(str(dataset_dir / 'Borealis' / '6secs_load_measurement_dataset' / '*.csv'))\n\t        years = [2010, 2011, 2012]\n\t        # Load data\n\t        for home in homes:\n\t            house_ = pd.read_csv(dataset_dir / 'Borealis'  / '6secs_load_measurement_dataset' / f'{home}.csv')\n\t            # Set timestamp as index\n\t            house_ = house_.set_index('timestamp')\n\t            # drop row 1, which has 0 value\n\t            house_ = house_.drop(house_.index[0])\n\t            # Convert index to datetime\n", "            house_.index = pd.to_datetime(house_.index, format='%Y-%m-%d %H:%M:%S')\n\t            # only keep power column\n\t            house_ = house_[['power']]\n\t            # Resample to hourly\n\t            house_ = house_.resample(rule='H', closed='left', label='right').mean()\n\t            # convert from Wh to kHh\n\t            house_ = house_ / 1000\n\t            for year in years:\n\t                h = house_[house_.index.year == year]\n\t                # count number of rows\n", "                if h.shape[0] < 168: # if < 1 week, throw out building-year\n\t                    continue\n\t                missing_frac = h[\"power\"].isnull().sum() / h.shape[0]\n\t                if missing_frac <= 0.1:\n\t                    # Linearly interpolate\n\t                    h = h.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both')\n\t                    # Fill missing values with zeros\n\t                    h = h.fillna(0)\n\t                    # Save to csv\n\t                    h.to_csv(dataset_dir / 'Borealis' / f'{home}_clean={year}.csv', header=True, index=True)\n", "    except:\n\t        download_borealis_data(dataset_dir / 'Borealis')\n\t        print('skipping Borealis...')\n\t    ########################################################\n\t    print('IDEAL...')\n\t    ########################################################\n\t    (dataset_dir / 'IDEAL').mkdir(parents=True, exist_ok=True)\n\t    print('WARNING: the raw IDEAL dataset requires over 140GB of storage. Skip?')\n\t    skip = input('Skip IDEAL? (y/n): ')\n\t    if skip == 'y':\n", "        print('skipping IDEAL...')\n\t    else:\n\t        print('this may take a while...')\n\t        download_ideal_data(dataset_dir / 'IDEAL')\n\t        homes_gz = glob.glob(str(dataset_dir / 'IDEAL' / 'sensordata' / 'home*_electric-mains_electric-combined.csv.gz'))\n\t        # gunzip each file\n\t        for h in homes_gz:\n\t            os.system('gunzip ' + h)\n\t        homes = glob.glob(str(dataset_dir / 'IDEAL' / 'sensordata' / 'home*_electric-mains_electric-combined.csv'))\n\t        years = [2016, 2017, 2018]\n", "        for h in homes:\n\t            homeid = Path(h).name.split('_')[0]\n\t            home = pd.read_csv(h, names=['power'], index_col=0, header=None)\n\t            # drop the first row\n\t            home = home.drop(home.index[0])\n\t            # convert index to datetime\n\t            home.index = pd.to_datetime(home.index, format='%Y-%m-%d %H:%M:%S')\n\t            # convert from Wh to kHh\n\t            home = home / 1000\n\t            # resample to 1 hour\n", "            home = home.resample(rule='H', closed='left', label='right').mean() \n\t            for year in years:\n\t                ho = home[home.index.year == year]\n\t                if ho.shape[0] < 168: # skip empty\n\t                    continue\n\t                missing_frac = (ho['power'].isnull().sum() / ho.shape[0])\n\t                #print(f'{h} {year} missing frac = {missing_frac}')\n\t                if missing_frac <= 0.1:\n\t                    # linearly interpolate\n\t                    ho = ho.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both')\n", "                    # Replace remaining nan with 0\n\t                    ho = ho.fillna(0)\n\t                    # save\n\t                    ho.to_csv( dataset_dir / 'IDEAL' / f'{homeid}_clean={year}.csv', header=True, index=True)\n\t            del home\n\t    ########################################################\n\t    print('LCL...')\n\t    ########################################################\n\t    all_buildings = {'MAC001964', 'MAC003533', 'MAC003527', 'MAC005461', 'MAC000339', 'MAC000767', 'MAC001706', 'MAC003238', 'MAC000065', 'MAC002146', 'MAC001945', 'MAC003968', 'MAC001764', 'MAC000710', 'MAC000712', 'MAC005444', 'MAC000699', 'MAC000677', 'MAC005101', 'MAC000790', 'MAC002841', 'MAC005496', 'MAC003511', 'MAC002290', 'MAC004665', 'MAC005116', 'MAC000774', 'MAC003366', 'MAC004672', 'MAC000702', 'MAC002227', 'MAC002143', 'MAC000051', 'MAC001953', 'MAC000633', 'MAC002141', 'MAC005494', 'MAC005129', 'MAC004906', 'MAC000451', 'MAC003915', 'MAC000761', 'MAC000470', 'MAC000783', 'MAC005441', 'MAC001954', 'MAC000354', 'MAC002121', 'MAC004631', 'MAC000467', 'MAC000779', 'MAC003807', 'MAC001576', 'MAC003526', 'MAC005094', 'MAC004667', 'MAC000722', 'MAC005464', 'MAC001566', 'MAC003360', 'MAC005499', 'MAC004659', 'MAC000765', 'MAC003548', 'MAC002872', 'MAC003198', 'MAC004997', 'MAC003185', 'MAC005567', 'MAC000044', 'MAC001740', 'MAC005469', 'MAC005089', 'MAC004664', 'MAC001826', 'MAC005107', 'MAC005438', 'MAC000141', 'MAC000106', 'MAC002292', 'MAC003298', 'MAC003516', 'MAC005500', 'MAC000690', 'MAC003540', 'MAC002247', 'MAC003517', 'MAC003315', 'MAC005467', 'MAC004919', 'MAC003977', 'MAC000706', 'MAC003338', 'MAC001651', 'MAC000725', 'MAC005113', 'MAC003852', 'MAC001814', 'MAC003848', 'MAC000402', 'MAC001963', 'MAC000384', 'MAC004662', 'MAC001694', 'MAC000383', 'MAC004905', 'MAC001547', 'MAC002219', 'MAC004634', 'MAC002242', 'MAC005133', 'MAC005430', 'MAC005443', 'MAC004923', 'MAC003510', 'MAC002876', 'MAC000788', 'MAC000755', 'MAC000766', 'MAC005439', 'MAC003782', 'MAC001614', 'MAC000751', 'MAC003268', 'MAC001602', 'MAC003895', 'MAC001682', 'MAC005505', 'MAC000082', 'MAC002300', 'MAC004635', 'MAC002124', 'MAC001856', 'MAC002232', 'MAC003321', 'MAC001758', 'MAC005112', 'MAC005456', 'MAC004991', 'MAC002153', 'MAC001949', 'MAC003256', 'MAC003823', 'MAC000713', 'MAC001970', 'MAC001979', 'MAC003530', 'MAC004914', 'MAC001754', 'MAC000328', 'MAC000794', 'MAC005097', 'MAC001645', 'MAC000707', 'MAC004653', 'MAC005501', 'MAC003557', 'MAC001947', 'MAC002838', 'MAC000401', 'MAC002847', 'MAC003954', 'MAC000679', 'MAC001655', 'MAC000701', 'MAC001975', 'MAC001958', 'MAC004983', 'MAC005561', 'MAC000048', 'MAC000693', 'MAC000772', 'MAC002875', 'MAC004663', 'MAC004936', 'MAC001778', 'MAC004994', 'MAC004650', 'MAC001755', 'MAC004993', 'MAC004928', 'MAC000640', 'MAC003553', 'MAC000695', 'MAC001737', 'MAC004646', 'MAC000683', 'MAC005462', 'MAC005007', 'MAC003542', 'MAC005003', 'MAC003951', 'MAC001743', 'MAC002866', 'MAC000700', 'MAC001536', 'MAC000680', 'MAC003247', 'MAC000786', 'MAC000757', 'MAC002160', 'MAC004916', 'MAC005485', 'MAC001966', 'MAC005447', 'MAC000666', 'MAC000344', 'MAC003827', 'MAC001944', 'MAC001823', 'MAC001766', 'MAC002281', 'MAC003897', 'MAC003379', 'MAC003211', 'MAC002222', 'MAC001667', 'MAC004637', 'MAC000793', 'MAC001760', 'MAC001715', 'MAC004643', 'MAC004973', 'MAC002240', 'MAC004652', 'MAC000046', 'MAC003334', 'MAC003769', 'MAC003973', 'MAC000773', 'MAC002867', 'MAC002842', 'MAC004927', 'MAC001960', 'MAC005450', 'MAC004992', 'MAC000754', 'MAC001950', 'MAC003546', 'MAC000692', 'MAC005555', 'MAC000676', 'MAC000787', 'MAC004989', 'MAC000406', 'MAC002268', 'MAC001948', 'MAC001806', 'MAC000768', 'MAC004907', 'MAC002873', 'MAC001739', 'MAC002151', 'MAC004999', 'MAC000408', 'MAC005104', 'MAC001830', 'MAC001735', 'MAC004638', 'MAC003532', 'MAC004998', 'MAC000641', 'MAC003259', 'MAC000776', 'MAC005460', 'MAC005123', 'MAC000703', 'MAC003170', 'MAC001959', 'MAC005004', 'MAC003512', 'MAC001825', 'MAC005562', 'MAC005454', 'MAC000665', 'MAC001768', 'MAC000464', 'MAC001773', 'MAC001738', 'MAC001718', 'MAC005001', 'MAC004644', 'MAC001751', 'MAC001693', 'MAC003783', 'MAC001556', 'MAC003522', 'MAC005431', 'MAC000763', 'MAC003257', 'MAC002238', 'MAC002852', 'MAC003892', 'MAC003919', 'MAC004912', 'MAC000681', 'MAC002230', 'MAC000109', 'MAC001554', 'MAC001777', 'MAC005502', 'MAC003788', 'MAC001761', 'MAC001965', 'MAC002277', 'MAC002850', 'MAC003869', 'MAC004904', 'MAC002125', 'MAC005504', 'MAC000675', 'MAC003507', 'MAC000708', 'MAC000014', 'MAC002123', 'MAC000716', 'MAC002870', 'MAC003347', 'MAC002139', 'MAC003508', 'MAC004647', 'MAC004913', 'MAC003515', 'MAC004666', 'MAC005092', 'MAC000447', 'MAC003865', 'MAC005119', 'MAC001750', 'MAC003178', 'MAC003535', 'MAC000760', 'MAC001744', 'MAC005475', 'MAC003358', 'MAC003816', 'MAC000461', 'MAC000052', 'MAC000369', 'MAC002346', 'MAC005096', 'MAC001769', 'MAC001542', 'MAC003372', 'MAC005437', 'MAC002225', 'MAC001973', 'MAC002138', 'MAC003771', 'MAC004908', 'MAC003501', 'MAC000739', 'MAC000762', 'MAC001956', 'MAC005471', 'MAC005491', 'MAC001650', 'MAC003916', 'MAC000678', 'MAC002860', 'MAC002264', 'MAC003808', 'MAC000682', 'MAC000689', 'MAC004654', 'MAC003829', 'MAC000352', 'MAC001978', 'MAC000782', 'MAC002849', 'MAC002159', 'MAC004673', 'MAC000698', 'MAC005125', 'MAC005476', 'MAC002324', 'MAC002162', 'MAC000336', 'MAC002878', 'MAC003514', 'MAC003552', 'MAC003337', 'MAC002236', 'MAC002883', 'MAC005459', 'MAC002336', 'MAC000758', 'MAC005098', 'MAC004911', 'MAC005497', 'MAC002846', 'MAC003538', 'MAC005131', 'MAC000764', 'MAC003975', 'MAC003333', 'MAC002856', 'MAC003165', 'MAC003201', 'MAC001746', 'MAC000342', 'MAC001564', 'MAC001770', 'MAC002880', 'MAC002163', 'MAC003272', 'MAC002265', 'MAC003331', 'MAC004656', 'MAC003556', 'MAC002882', 'MAC005117', 'MAC000694', 'MAC005455', 'MAC005457', 'MAC002334', 'MAC001612', 'MAC000771', 'MAC001943', 'MAC003221', 'MAC004931', 'MAC002280', 'MAC002862', 'MAC002877', 'MAC002851', 'MAC000792', 'MAC003269', 'MAC004630', 'MAC005484', 'MAC000017', 'MAC000653', 'MAC002854', 'MAC001855', 'MAC002155', 'MAC005005', 'MAC002840', 'MAC000784', 'MAC005103', 'MAC005132', 'MAC003551', 'MAC005481', 'MAC002246', 'MAC001955', 'MAC002157', 'MAC005472', 'MAC005466', 'MAC004915', 'MAC002136', 'MAC005453', 'MAC004934', 'MAC001636', 'MAC005487', 'MAC000795', 'MAC002344', 'MAC001700', 'MAC001849', 'MAC001974', 'MAC001968', 'MAC002127', 'MAC001765', 'MAC005128', 'MAC003180', 'MAC001586', 'MAC001941', 'MAC002150', 'MAC003286', 'MAC001686', 'MAC005451', 'MAC003277', 'MAC003519', 'MAC003504', 'MAC003914', 'MAC004922', 'MAC005118', 'MAC004660', 'MAC001680', 'MAC002130', 'MAC005120', 'MAC002144', 'MAC003862', 'MAC002149', 'MAC000366', 'MAC000421', 'MAC004651', 'MAC003521', 'MAC003509', 'MAC000064', 'MAC003899', 'MAC002129', 'MAC003960', 'MAC003982', 'MAC004987', 'MAC005126', 'MAC000107', 'MAC000672', 'MAC001772', 'MAC002858', 'MAC000635', 'MAC005433', 'MAC000456', 'MAC003988', 'MAC005446', 'MAC000770', 'MAC000075', 'MAC003520', 'MAC004626', 'MAC000778', 'MAC000418', 'MAC003202', 'MAC001972', 'MAC002152', 'MAC005090', 'MAC002226', 'MAC005440', 'MAC000711', 'MAC004925', 'MAC004978', 'MAC005088', 'MAC001767', 'MAC005442', 'MAC000686', 'MAC004929', 'MAC001707', 'MAC002133', 'MAC005503', 'MAC003262', 'MAC001749', 'MAC000738', 'MAC002120', 'MAC004975', 'MAC000709', 'MAC001851', 'MAC000714', 'MAC002291', 'MAC003518', 'MAC004649', 'MAC000071', 'MAC001736', 'MAC002853', 'MAC004671', 'MAC003308', 'MAC001741', 'MAC001977', 'MAC002145', 'MAC001734', 'MAC004933', 'MAC005109', 'MAC003285', 'MAC001840', 'MAC000392', 'MAC000684', 'MAC001853', 'MAC000728', 'MAC003911', 'MAC001763', 'MAC002843', 'MAC005436', 'MAC001961', 'MAC000005', 'MAC000696', 'MAC001626', 'MAC003245', 'MAC005479', 'MAC001642', 'MAC003534', 'MAC004645', 'MAC000769', 'MAC000015', 'MAC001762', 'MAC000781', 'MAC004629', 'MAC001660', 'MAC001742', 'MAC003890', 'MAC005130', 'MAC003175', 'MAC004974', 'MAC004976', 'MAC004627', 'MAC001833', 'MAC004924', 'MAC000697', 'MAC002137', 'MAC002128', 'MAC001704', 'MAC001946', 'MAC004636', 'MAC004918', 'MAC000777', 'MAC001980', 'MAC004642', 'MAC005115', 'MAC003967', 'MAC001610', 'MAC005099', 'MAC001967', 'MAC000687', 'MAC001688', 'MAC003984', 'MAC005000', 'MAC003506', 'MAC002217', 'MAC001747', 'MAC002857', 'MAC004632', 'MAC001771', 'MAC000705', 'MAC000685', 'MAC005480', 'MAC000780', 'MAC001835', 'MAC000139', 'MAC005452', 'MAC003547', 'MAC004909', 'MAC000031', 'MAC004984', 'MAC001748', 'MAC002871', 'MAC003537', 'MAC000688', 'MAC000433', 'MAC005095', 'MAC004658', 'MAC002865', 'MAC003989', 'MAC002135', 'MAC001658', 'MAC002863', 'MAC003543', 'MAC005468', 'MAC001745', 'MAC004920', 'MAC000759', 'MAC005490', 'MAC000124', 'MAC001596', 'MAC004981', 'MAC005566', 'MAC001733', 'MAC001753', 'MAC004628', 'MAC003365', 'MAC004986', 'MAC004996', 'MAC001940', 'MAC005483', 'MAC005425', 'MAC003947', 'MAC000303', 'MAC005114', 'MAC001597', 'MAC001712', 'MAC001559', 'MAC002142', 'MAC003554', 'MAC001731', 'MAC005486', 'MAC002243', 'MAC005108', 'MAC005435', 'MAC001732', 'MAC001752', 'MAC001756', 'MAC002318', 'MAC001847', 'MAC005087', 'MAC005428', 'MAC004995', 'MAC001538', 'MAC005105', 'MAC000076', 'MAC002117', 'MAC003265', 'MAC005458', 'MAC000704', 'MAC004648', 'MAC004930', 'MAC002161', 'MAC005463', 'MAC003820', 'MAC004980', 'MAC004985', 'MAC005478', 'MAC000691', 'MAC001962', 'MAC005002', 'MAC001759', 'MAC005100', 'MAC003529', 'MAC000465', 'MAC003274', 'MAC005493', 'MAC000673', 'MAC001590', 'MAC003549', 'MAC000719', 'MAC001971', 'MAC001730', 'MAC002844', 'MAC002859', 'MAC001757', 'MAC002158', 'MAC003500', 'MAC003196', 'MAC000146', 'MAC004982', 'MAC000439', 'MAC003903', 'MAC004935', 'MAC000088', 'MAC000395', 'MAC005426'}\n\t    (dataset_dir / 'LCL').mkdir(parents=True, exist_ok=True)\n", "    download_lcl_data(dataset_dir / 'LCL')\n\t    all_LCL = glob.glob(str(dataset_dir / 'LCL' / 'Small LCL Data' / 'LCL-June2015v2*.csv'))\n\t    random.shuffle(all_LCL)\n\t    num_bldgs = 0\n\t    years = [2012, 2013]\n\t    for lcl_file in all_LCL:\n\t        df_ = pd.read_csv(lcl_file)\n\t        unique_bldgs = df_['LCLid'].unique()\n\t        # only keep buildings that are in all_buildings\n\t        unique_bldgs = all_buildings.intersection(set(unique_bldgs))\n", "        # remove buildings from all_buildings that are in unique_bldgs\n\t        all_buildings = all_buildings.difference(unique_bldgs)\n\t        for ub in unique_bldgs:\n\t            df = df_[df_.LCLid == ub].copy()\n\t            df['DateTime'] = pd.to_datetime(df['DateTime'])\n\t            # Set timestamp as index\n\t            df = df.set_index('DateTime')\n\t            df = df[~df.index.duplicated(keep='first')]\n\t            df = df.asfreq('30min')\n\t            df = df.rename(columns={'KWH/hh (per half hour) ': 'power'})\n", "            # only keep power column\n\t            df = df[['power']]\n\t            df = df.replace('Null', np.nan)\n\t            df = df.astype('float32')\n\t            df = df.resample(rule='H', closed='left', label='right').mean()\n\t            for year in years:\n\t                dfy = df[df.index.year == year]\n\t                if dfy.shape[0] < 168:\n\t                    continue\n\t                missing_frac = dfy['power'].isnull().sum() / dfy.shape[0]\n", "                if missing_frac <= 0.1:\n\t                    dfy = dfy.interpolate(method='linear', axis=0, limit=24*7, limit_direction='both')\n\t                    dfy = dfy.fillna(0.)\n\t                    dfy.to_csv(dataset_dir / 'LCL' / f'{ub}_clean={year}.csv')\n\t                    num_bldgs += 1\n\t            if num_bldgs == 713:\n\t                print(f'num bldgs: {num_bldgs}') \n\t                break\n\t        if num_bldgs == 713:\n\t            break\n"]}
{"filename": "scripts/process_raw_data/create_buildings900K.py", "chunked_list": ["import argparse\n\ttry:\n\t    import pyspark\n\texcept:\n\t    print('please install pyspark with `pip install pyspark`')\n\t    exit(1)\n\tfrom pyspark import SparkContext, SparkConf\n\tfrom pyspark.sql import SparkSession\n\timport pyspark.sql.functions as F\n\timport os\n", "import glob\n\tconf = SparkConf().setMaster(\"local[*]\").setAppName(\"pytorch\")\n\tconf.set(\"spark.executor.memory\", \"2g\")\n\tconf.set(\"spark.driver.memory\", \"64G\")\n\tconf.set(\"spark.executor.cores\", \"96\")\n\tconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n\tconf.set(\"spark.default.parallelism\", \"96\")\n\tconf.set(\"spark.local.dir\", \"/tmp/scratch/tmp\")\n\tsc =  SparkContext.getOrCreate(conf=conf)\n\tspark = SparkSession(sc)\n", "# Set the environment variable SPARK_LOCAL_DIRS\n\tos.environ['SPARK_LOCAL_DIRS'] = '/tmp/scratch/tmp'\n\t# Set the environment variable LOCAL_DIRS\n\tos.environ['SPARK_LOCAL_DIRS'] = '/tmp/scratch/tmp'\n\tdef main(args):\n\t    eulp_dir = os.path.join(args.eulp_dir, 'end-use-load-profiles-for-us-building-stock', '2021')\n\t    output_dir = args.output_dir\n\t    census_regions = ['by_puma_midwest', 'by_puma_northeast', 'by_puma_south', 'by_puma_west'] \n\t    years = ['tmy3', 'amy2018']\n\t    for cr in census_regions:\n", "        for year in years:\n\t            for res_or_com in ['resstock', 'comstock']:\n\t                list_of_pumas = glob.glob(os.path.join(eulp_dir,\n\t                                                       f'{res_or_com}_{year}_release_1',\n\t                                                       'timeseries_individual_buildings',\n\t                                                       cr,\n\t                                                       'upgrade=0',\n\t                                                       'puma=*'))\n\t                target_path = os.path.join(output_dir, 'end-use-load-profiles-for-us-building-stock' , '2021',\n\t                                            f'{res_or_com}_{year}_release_1',\n", "                                            'timeseries_individual_buildings',\n\t                                            cr,\n\t                                            'upgrade=0')\n\t                for puma_id in list_of_pumas:\n\t                    print(f'processing {puma_id} to store buildings as a parquet file...')\n\t                    target_puma_path = os.path.join(target_path, os.path.basename(puma_id))\n\t                    if not os.path.isdir(target_puma_path):\n\t                        os.makedirs(target_puma_path)\n\t                    else:\n\t                        print(f'{target_puma_path} already exists. Skipping...')\n", "                        continue\n\t                    df = spark.read.parquet(puma_id)\n\t                    # Just the datapoints we need  \n\t                    df = df.select(['`out.site_energy.total.energy_consumption`', 'timestamp', 'bldg_id'])\n\t                    # Average 15 min out.site_energy.total.energy_consumption by hour for each bldg_id\n\t                    df = df.withColumn('timestamp', F.date_trunc('hour', df['timestamp']))\n\t                    df = df.groupBy('timestamp', 'bldg_id').agg(F.avg('`out.site_energy.total.energy_consumption`').alias('total_energy_consumption'))\n\t                    # Group by timestamp and create a new column for each bldg_id\n\t                    df = df.groupBy('timestamp').pivot('bldg_id').agg(F.first('total_energy_consumption'))\n\t                    # fill na with 0\n", "                    df = df.fillna(0)\n\t                    df = df.repartition(1)\n\t                    # Save as dataset with parquet files\n\t                    df.write.option('header', True).mode('overwrite').parquet(target_puma_path)\n\tif __name__ == '__main__':\n\t    args = argparse.ArgumentParser()\n\t    args.add_argument('--eulp_dir', type=str, required=True,\n\t                        help='Path to raw EULP data.')\n\t    args.add_argument('--output_dir', type=str, required=True,\n\t                        help='Path to store the processed data.')\n", "    args = args.parse_args()\n\t    main(args)\n"]}
{"filename": "buildings_bench/tokenizer.py", "chunked_list": ["import faiss\n\timport faiss.contrib.torch_utils \n\tfrom typing import Union\n\timport numpy as np\n\tfrom pathlib import Path\n\timport torch\n\tclass LoadQuantizer:\n\t    \"\"\"Quantize load timeseries with KMeans. Merge centroids that are within a threshold.\"\"\"\n\t    def __init__(self, seed: int = 1, num_centroids: int = 2274,\n\t                       with_merge=False, merge_threshold=0.01, device: str = 'cpu'):\n", "        \"\"\"\n\t        Args:\n\t            seed (int): random seed. Default: 1.\n\t            num_centroids (int): number of centroids: Default: 2274.\n\t            with_merge (bool): whether to merge centroids that are within a threshold: Default: False.\n\t            merge_threshold (float): threshold for merging centroids. Default: 0.01 (kWh).\n\t            device (str): cpu or cuda. Default: cpu.\n\t        \"\"\"\n\t        self.seed = seed\n\t        self.K = num_centroids\n", "        self.with_merge = with_merge\n\t        self.merge_threshold = merge_threshold\n\t        self.kmeans = None\n\t        self.original_order_to_sorted_centroid_indices = None\n\t        self.original_order_to_merged_centroid_map = None\n\t        self.merged_centroids = None\n\t        self.device = device\n\t    def get_vocab_size(self) -> int:\n\t        if self.with_merge:\n\t            return len(self.merged_centroids)\n", "        return self.K\n\t    def train(self, sample: np.ndarray) -> None:\n\t        \"\"\"Fit KMeans to a subset of the data. \n\t        Optionally, merge centroids that are within a threshold.\n\t        Args:\n\t            sample (np.ndarray): shape [num_samples, 1]\n\t        \"\"\"\n\t        self.kmeans = faiss.Kmeans(d=1, k=self.K, niter=300, nredo=10,\n\t                     seed=self.seed, verbose=True, gpu=(True if 'cuda' in self.device else False))\n\t        # max is 256, min is 39 per centroid by defaul\n", "        # https://github.com/facebookresearch/faiss/blob/d5ca6f0a79aa382bb98d2221a8f61c1a200efa25/faiss/Clustering.cpp#L2t\n\t        self.kmeans.train(sample.reshape(-1,1))\n\t        if self.with_merge:\n\t            # sort the centroids and return the sorted indices\n\t            sorted_indices = np.argsort(self.kmeans.centroids.reshape(-1))\n\t            # sort the centroids\n\t            #unsorted_index_to_value_map = self.kmeans.centroids.copy()\n\t            index_to_value_map = self.kmeans.centroids[sorted_indices]\n\t            # create a map from the sorted indices to the original indices\n\t            self.original_order_to_sorted_centroid_indices = np.zeros_like(sorted_indices)\n", "            for i in range(len(sorted_indices)):\n\t                self.original_order_to_sorted_centroid_indices[sorted_indices[i]] = i\n\t            # Iterate over sorted centroids, merge centroids that are within a threshold\n\t            previous_centroid_value = index_to_value_map[0]\n\t            merge_classes = [0]\n\t            candidate_centroids = [previous_centroid_value]\n\t            # new list of centroids\n\t            self.merged_centroids = []\n\t            # list that maps the original centroid indices to the new centroid indices\n\t            candidate_index_list = []\n", "            #  Iterate over the sorted centroids.\n\t            for i in range(1,len(index_to_value_map)):\n\t                # If the current centroid is within a threshold of the previous centroid,\n\t                # add it to the merged class. \n\t                if index_to_value_map[i] - previous_centroid_value < self.merge_threshold:\n\t                    merge_classes += [i]\n\t                    candidate_centroids += [index_to_value_map[i]]\n\t                #  If the current centroid is not within a threshold of the previous centroid,\n\t                #  merge the set of candidate centroids, add the merged class indices to the\n\t                # candidate list, and update previous centroid value to point to current index\n", "                else:\n\t                    # I take the average of the centroids\n\t                    # being merged\n\t                    self.merged_centroids += [np.mean(candidate_centroids)]\n\t                    candidate_index_list += [merge_classes]\n\t                    # reset this with current index\n\t                    merge_classes = [i]\n\t                    candidate_centroids = [index_to_value_map[i]]\n\t                    previous_centroid_value = index_to_value_map[i]\n\t            # Add the last centroid to the new list of centroids and add the merged class to the merge list.\n", "            self.merged_centroids += [np.mean(candidate_centroids)]\n\t            candidate_index_list += [merge_classes]\n\t            self.merged_centroids = np.array(self.merged_centroids)\n\t            # Create a map from the original centroid index to the new centroid index\n\t            self.original_order_to_merged_centroid_map = np.zeros(len(index_to_value_map), dtype=np.int)\n\t            for i in range(len(candidate_index_list)):\n\t                for j in candidate_index_list[i]:\n\t                    self.original_order_to_merged_centroid_map[j] = i\n\t            print('Merged {} centroids to {} centroids'.format(len(index_to_value_map), len(self.merged_centroids)))\n\t            self.K = len(self.merged_centroids)\n", "    def save(self, output_path: Path) -> None:\n\t        if 'cuda' in self.device:\n\t            self.kmeans.index = faiss.index_gpu_to_cpu(self.kmeans.index)\n\t        chunk = faiss.serialize_index(self.kmeans.index)\n\t        np.save(output_path / \"kmeans_K={}.npy\".format(self.K), chunk)\n\t        np.save(output_path / \"kmeans_centroids_K={}.npy\".format(self.K), self.kmeans.centroids)\n\t        if self.with_merge:\n\t            np.save(output_path / \"kmeans_original_to_sorted_indices_K={}.npy\".format(self.K),\n\t                    self.original_order_to_sorted_centroid_indices)\n\t            np.save(output_path / \"kmeans_original_to_merged_map_K={}.npy\".format(self.K),\n", "                    self.original_order_to_merged_centroid_map)\n\t            np.save(output_path / \"kmeans_merged_centers_K={}.npy\".format(self.K),\n\t                    self.merged_centroids)\n\t    def load(self, saved_path: Path) -> None:\n\t        chunk = np.load(\n\t            saved_path / \"kmeans_K={}.npy\".format(self.K), allow_pickle=True)\n\t        self.kmeans = faiss.Kmeans(d=1, k=self.K, niter=200, nredo=20,\n\t                     seed=self.seed, verbose=True, gpu=(True if 'cuda' in self.device else False))\n\t        self.kmeans.index = faiss.deserialize_index(chunk)\n\t        self.kmeans.centroids = np.load(\n", "            saved_path / \"kmeans_centroids_K={}.npy\".format(self.K), allow_pickle=True)\n\t        if self.with_merge:\n\t            self.original_order_to_sorted_centroid_indices = np.load(\n\t                saved_path / \"kmeans_original_to_sorted_indices_K={}.npy\".format(self.K), allow_pickle=True)\n\t            self.original_order_to_merged_centroid_map = np.load(\n\t                saved_path / \"kmeans_original_to_merged_map_K={}.npy\".format(self.K), allow_pickle=True)\n\t            self.merged_centroids = np.load(\n\t                saved_path / \"kmeans_merged_centers_K={}.npy\".format(self.K), allow_pickle=True)\n\t            print(f'Loaded Kmeans quantizer with K={len(self.merged_centroids)}')\n\t        else:\n", "            print(f'Loaded Kmeans quantizer with K={self.K}')\n\t        if 'cuda' in self.device:\n\t            local_rank = int(self.device.split(':')[-1])\n\t            faiss_res = faiss.StandardGpuResources()\n\t            self.kmeans.index = faiss.index_cpu_to_gpu(faiss_res, local_rank, self.kmeans.index)\n\t            self.kmeans.centroids = torch.from_numpy(self.kmeans.centroids).float().to(self.device)\n\t            self.kmeans.centroids.squeeze()\n\t            if self.with_merge:\n\t                self.original_order_to_sorted_centroid_indices = torch.from_numpy( \n\t                    self.original_order_to_sorted_centroid_indices).long().to(self.device)\n", "                self.original_order_to_merged_centroid_map = torch.from_numpy(\n\t                    self.original_order_to_merged_centroid_map).long().to(self.device)\n\t                self.merged_centroids = torch.from_numpy(\n\t                    self.merged_centroids).float().to(self.device)\n\t                self.merged_centroids.squeeze()\n\t            print(f'Kmeans quantizer moved to GPU: {type(self.kmeans.index)}')\n\t    def transform(self, sample: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n\t        \"\"\"Quantize a sample of load values into a sequence of indices.\n\t        Args:\n\t            sample Union[np.ndarray, torch.Tensor]: of shape (n, 1) or (b,n,1). \n", "                type is numpy if device is cpu or torch Tensor if device is cuda.\n\t        Returns:\n\t            sample (Union[np.ndarray, torch.Tensor]): of shape (n, 1) or (b,n,1).\n\t        \"\"\"\n\t        init_shape = sample.shape\n\t        sample = sample.reshape(-1,1)\n\t        sample = self.kmeans.index.search(sample, 1)[1].reshape(-1)\n\t        if self.with_merge:\n\t            sample = self.original_order_to_sorted_centroid_indices[sample]\n\t            sample = self.original_order_to_merged_centroid_map[sample]\n", "        sample = sample.reshape(init_shape)\n\t        return sample\n\t    def undo_transform(self, sample: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n\t        \"\"\"Dequantize a sample of integer indices into a sequence of load values.\n\t        Args:\n\t            sample Union[np.ndarray, torch.Tensor]: of shape (n, 1) or (b,n,1). \n\t                type is numpy if device is cpu or torch Tensor if device is cuda.\n\t        Returns:\n\t            sample (Union[np.ndarray, torch.Tensor]): of shape (n, 1) or (b,n,1).\n\t        \"\"\"\n", "        init_shape = sample.shape\n\t        sample = sample.reshape(-1)\n\t        if self.with_merge:\n\t            sample = self.merged_centroids[sample]\n\t        else:\n\t            sample = self.kmeans.centroids[sample]\n\t        sample = sample.reshape(init_shape)\n\t        return sample\n"]}
{"filename": "buildings_bench/transforms.py", "chunked_list": ["import numpy as np\n\tfrom pathlib import Path\n\timport pandas as pd\n\timport torch\n\timport pickle as pkl\n\timport os\n\timport sklearn.preprocessing as preprocessing\n\tfrom typing import Union\n\tclass BoxCoxTransform:\n\t    \"\"\"A class that computes and applies the Box-Cox transform to data.\n", "    \"\"\"\n\t    def __init__(self, max_datapoints=1000000):\n\t        \"\"\"\n\t        Args:\n\t            max_datapoints (int): If the number of datapoints is greater than this, subsample.\n\t        \"\"\"\n\t        self.boxcox = None\n\t        self.max_datapoints = max_datapoints\n\t    def train(self, data: np.array) -> None:\n\t        \"\"\"Train the Box-Cox transform on the data with sklearn.preprocessing.PowerTransformer.\n", "        Args:\n\t            data (np.array): of shape (n, 1) or (b,n,1)\n\t        \"\"\"       \n\t        self.boxcox = preprocessing.PowerTransformer(method='box-cox', standardize=True)\n\t        data = data.flatten().reshape(-1,1)\n\t        if data.shape[0] > self.max_datapoints:\n\t            print(f'Box-Cox: subsampling {self.max_datapoints} datapoints')\n\t            data = data[np.random.choice(data.shape[0], self.max_datapoints, replace=False)]\n\t        self.boxcox.fit_transform(1e-6 + data)\n\t    def save(self, output_path: Path) -> None:\n", "        \"\"\"Save the Box-Cox transform\"\"\"\n\t        with open(output_path / \"boxcox.pkl\", 'wb') as f:\n\t            pkl.dump(self.boxcox, f)\n\t    def load(self, saved_path: Path) -> None:\n\t        \"\"\"Load the Box-Cox transform\"\"\"\n\t        with open(saved_path / \"boxcox.pkl\", 'rb') as f:\n\t            self.boxcox = pkl.load(f)\n\t    def transform(self, sample: np.ndarray) -> np.ndarray:\n\t        \"\"\"Transform a sample via Box-Cox.\n\t        Not ran on the GPU, so input/output are numpy arrays.\n", "        Args:\n\t            sample (np.ndarray): of shape (n, 1) or (b,n,1) \n\t        Returns:\n\t            transformed_sample (np.ndarray): of shape (n, 1) or (b,n,1)\n\t        \"\"\"\n\t        init_shape = sample.shape\n\t        return self.boxcox.transform(1e-6 + sample.flatten().reshape(-1,1)).reshape(init_shape)\n\t    def undo_transform(self, sample: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]: \n\t        \"\"\"Undo the transformation of a sample via Box-Cox\n\t        Args:\n", "            sample (np.ndarray) or (torch.LongTensor): of shape (n, 1) or (b,n,1). \n\t                numpy if device is cpu or torch Tensor if device is cuda.\n\t        Returns:\n\t            unscaled_sample (np.ndarray or torch.Tensor): of shape (n, 1) or (b,n,1).\n\t        \"\"\"\n\t        is_tensor = isinstance(sample, torch.Tensor)\n\t        # if torch.Tensor, convert to numpy first\n\t        if is_tensor:\n\t            device = sample.device\n\t            sample = sample.cpu().numpy()\n", "        init_shape = sample.shape       \n\t        sample = self.boxcox.inverse_transform(sample.flatten().reshape(-1,1)).reshape(init_shape)\n\t        # convert back to torch\n\t        if is_tensor:\n\t            sample = torch.from_numpy(sample).to(device)\n\t        return sample\n\tclass StandardScalerTransform:\n\t    \"\"\" A class that standardizes data by removing the mean and scaling to unit variance.\n\t    \"\"\"\n\t    def __init__(self, max_datapoints=1000000, device='cpu'):\n", "        \"\"\"\n\t        Args:\n\t            max_datapoints (int): If the number of datapoints is greater than this, subsample.\n\t            device (str): 'cpu' or 'cuda'\n\t        \"\"\"\n\t        self.mean_ = None\n\t        self.std_ = None\n\t        self.max_datapoints = max_datapoints\n\t        self.device=device\n\t    def train(self, data: np.array) -> None:\n", "        \"\"\"Train the StandardScaler transform on the data.\n\t        Args:\n\t            data (np.array): of shape (n, 1) or (b,n,1)\n\t        \"\"\"       \n\t        data = data.flatten().reshape(-1,1)\n\t        if data.shape[0] > self.max_datapoints:\n\t            print(f'Subsampling {self.max_datapoints} datapoints to fit StandardScalerTransform')\n\t            data = data[np.random.choice(data.shape[0], self.max_datapoints, replace=False)]\n\t        self.mean_ = torch.from_numpy(np.array([np.mean(data)])).float().to(self.device)\n\t        self.std_ = torch.from_numpy(np.array([np.std(data)])).float().to(self.device)\n", "    def save(self, output_path: Path) -> None:\n\t        \"\"\"Save the StandardScaler transform\"\"\"\n\t        mean_ = self.mean_.cpu().numpy().reshape(-1)\n\t        std_ = self.std_.cpu().numpy().reshape(-1)\n\t        np.save(output_path / \"standard_scaler.npy\", np.array([mean_, std_]))\n\t    def load(self, saved_path: Path) -> None:\n\t        \"\"\"Load the StandardScaler transform\"\"\"\n\t        x = np.load(saved_path / \"standard_scaler.npy\")\n\t        self.mean_ = torch.from_numpy(np.array([x[0]])).float().to(self.device)\n\t        self.std_ = torch.from_numpy(np.array([x[1]])).float().to(self.device)\n", "    def transform(self, sample: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:\n\t        \"\"\"Transform a sample via StandardScaler\n\t        Args:\n\t            sample (np.ndarray or torch.Tensor): shape (n, 1) or (b,n,1) \n\t        Returns:\n\t            transformed_samples (torch.Tensor): shape (n, 1) or (b,n,1)\n\t        \"\"\"\n\t        if isinstance(sample, np.ndarray):\n\t            sample = torch.from_numpy(sample).float().to(self.device)        \n\t        return (sample - self.mean_) / self.std_\n", "    def undo_transform(self, sample: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:\n\t        \"\"\"Undo the transformation of a sample via StandardScaler\n\t        Args:\n\t            sample (np.ndarray): of shape (n, 1) or (b,n,1) or torch.Tensor of shape (n, 1) or (b,n,1)\n\t        Returns:\n\t            unscaled_sample (torch.Tensor): of shape (n, 1) or (b,n,1)\n\t        \"\"\"\n\t        if isinstance(sample, np.ndarray):\n\t            sample = torch.from_numpy(sample).float().to(self.device)\n\t        return self.std_ * sample + self.mean_        \n", "    def undo_transform_std(self, scaled_std: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"Undo transform for standard deviation.\n\t        Args:\n\t            scaled_std (torch.Tensor): of shape (n, 1) or (b,n,1)\n\t        Returns:\n\t            unscaled_std (torch.Tensor): of shape (n, 1) or (b,n,1)\n\t        \"\"\"\n\t        return self.std_ * scaled_std\n\tclass LatLonTransform:\n\t    \"\"\"Pre-processing lat,lon data with standard normalization by Buildings-900K training set.\n", "    \"\"\"\n\t    def __init__(self):\n\t        metadata_path = Path(os.environ.get('BUILDINGS_BENCH','')) / 'metadata'\n\t        # Load withheld pumas\n\t        with open(metadata_path / 'withheld_pumas.tsv', 'r') as f:\n\t            # tab separated file\n\t            line = f.readlines()[0]\n\t            self.withheld_pumas = line.split('\\t')\n\t        census_regions = ['northeast', 'midwest', 'south', 'west']\n\t        self.puma_to_centroid = {}\n", "        lat_means, lat_stds = 0, 0\n\t        lon_means, lon_stds = 0, 0\n\t        for idx,cr in enumerate(census_regions):\n\t            cr_df = pd.read_csv(metadata_path / f'map_of_pumas_in_census_region_{idx+1}_{cr}.csv', header=0)\n\t            # keys are cr_df['GISJOIN'], values are (cr_df['latitude'], cr_df['longitude'])\n\t            self.puma_to_centroid.update(dict(zip(cr_df['GISJOIN'], zip(cr_df['latitude'], cr_df['longitude']))))\n\t            # Filter out withheld pumas\n\t            non_withheld_df = cr_df[~cr_df['GISJOIN'].isin(self.withheld_pumas)]\n\t            # Compute mean and std \n\t            lat_means += non_withheld_df['latitude'].mean()\n", "            lat_stds += non_withheld_df['latitude'].std()\n\t            lon_means += non_withheld_df['longitude'].mean()\n\t            lon_stds += non_withheld_df['longitude'].std()\n\t        self.lat_means = lat_means / 4\n\t        self.lon_means = lon_means / 4\n\t        self.lat_stds = lat_stds / 4\n\t        self.lon_stds = lon_stds / 4\n\t        # convert self.puma_to_centroid values to np.ndarray of shape (2,)\n\t        for k,v in self.puma_to_centroid.items():\n\t            # Normalize\n", "            v = (np.array(v, dtype=np.float32) - np.array([self.lat_means, self.lon_means])) / np.array([self.lat_stds, self.lon_stds])\n\t            self.puma_to_centroid[k] = v.astype(np.float32)\n\t    def transform_latlon(self, latlon: np.ndarray) -> np.ndarray:\n\t        \"\"\"Transform a raw Lat/Lon sample into a normalized Lat/Lon sample\n\t        Args:\n\t            latlon (np.ndarray): of shape (2,).\n\t        Returns:\n\t            transformed_latlon (np.ndarray): of shape (2,).\n\t        \"\"\"\n\t        return (latlon - np.array([self.lat_means, self.lon_means])) / np.array([self.lat_stds, self.lon_stds])\n", "    def undo_transform(self, normalized_latlon: np.ndarray) -> np.ndarray:\n\t        \"\"\"Undo the transformation of a sample\n\t        Args:\n\t            normalized_latlon (np.ndarray): of shape (n, 2) or (b,n,2).\n\t        Returns:\n\t            unnormalized_latlon (np.ndarray): of shape (n, 2) or (b,n,2).\n\t        \"\"\"\n\t        init_shape = normalized_latlon.shape\n\t        normalized_latlon = normalized_latlon.reshape(-1,2)\n\t        lat = normalized_latlon[:,0] * self.lat_stds + self.lat_means\n", "        lon = normalized_latlon[:,1] * self.lon_stds + self.lon_means\n\t        return np.stack([lat, lon], axis=1).reshape(init_shape)\n\t    def transform(self, puma_id: str) -> np.ndarray:\n\t        \"\"\"Look up a PUMA ID's normalized Lat/Lon centroid.\n\t        This is used in the Buildings-900K Dataset to look up a lat/lon\n\t        for each building's PUMA.\n\t        Args:\n\t            puma_id (str): PUMA ID\n\t        Returns:\n\t            centroid (np.ndarray): of shape (1,2)\n", "        \"\"\"\n\t        return self.puma_to_centroid[puma_id].reshape(1,2)\n\tclass TimestampTransform:\n\t    \"\"\"Extract timestamp features from a Pandas timestamp Series.\n\t    \"\"\"\n\t    def __init__(self, is_leap_year: bool = False):\n\t        \"\"\"\n\t        Args:\n\t            is_leap_year (bool): Whether the year of the building data is a leap year or not.\n\t        \"\"\"\n", "        self.day_year_normalization = 365 if is_leap_year else 364\n\t        self.hour_of_day_normalization = 23\n\t        self.day_of_week_normalization = 6\n\t    def transform(self, timestamp_series: pd.DataFrame) -> np.ndarray:\n\t        \"\"\"Extract timestamp features from a Pandas timestamp Series.\n\t        - Day of week (0-6)\n\t        - Day of year (0-364)\n\t        - Hour of day (0-23)\n\t        Args:\n\t            timestamp_series (pd.DataFrame): of shape (n,) or (b,n)\n", "        Returns:\n\t            time_features (np.ndarray): of shape (n,3) or (b,n,3)\n\t        \"\"\"\n\t        # If the input is a DatetimeIndex\n\t        if isinstance(timestamp_series, pd.DatetimeIndex):\n\t            timestamp_series = timestamp_series.to_series()\n\t        # Convert to datetime\n\t        timestamp_series = pd.to_datetime(timestamp_series)\n\t        # Extract features\n\t        day_of_week = timestamp_series.dt.dayofweek\n", "        day_of_year = timestamp_series.dt.dayofyear\n\t        hour_of_day = timestamp_series.dt.hour\n\t        time_features = np.stack([day_of_year / self.day_year_normalization,\n\t                         day_of_week / self.day_of_week_normalization,\n\t                         hour_of_day / self.hour_of_day_normalization], axis=1).astype(np.float32)\n\t        return time_features * 2 - 1\n\t    def undo_transform(self, time_features: np.ndarray) -> np.ndarray:\n\t        \"\"\"Convert normalized time features back to original time features\n\t        Args:\n\t            time_features (np.ndarray): of shape (n, 3) or (b,n,3)\n", "        Returns:\n\t            unnormalized_time_features (np.ndarray): of shape (n, 3) or (b,n,3)\n\t        \"\"\"\n\t        init_shape = time_features.shape\n\t        time_features = time_features.reshape(-1,3)\n\t        time_features = (time_features + 1) * 0.5\n\t        day_of_year = np.round(time_features[:,0] * self.day_year_normalization)\n\t        day_of_week = np.round(time_features[:,1] * self.day_of_week_normalization)\n\t        hour_of_day = np.round(time_features[:,2] * self.hour_of_day_normalization)\n\t        return np.stack([day_of_year, day_of_week, hour_of_day], axis=1).astype(np.int32).reshape(init_shape)\n"]}
{"filename": "buildings_bench/__init__.py", "chunked_list": ["from buildings_bench.evaluation.managers import BuildingTypes\n\tfrom buildings_bench.data import load_torch_dataset, load_pretraining, load_pandas_dataset\n\tfrom buildings_bench.data import benchmark_registry\n\t__version__ = \"0.1.0a3\""]}
{"filename": "buildings_bench/utils.py", "chunked_list": ["import numpy as np\n\timport random \n\timport torch \n\timport os \n\timport datetime \n\tdef set_seed(seed: int = 42) -> None:\n\t    \"\"\"Set random seed for reproducibility.\n\t    \"\"\"\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n", "    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\t    print(f\"Random seed set as {seed}\")\n\tdef save_model_checkpoint(model, optimizer, scheduler, step, path):\n\t    \"\"\"Save model checkpoint.\n\t    \"\"\"\n\t    checkpoint = {\n\t        'model': model.state_dict(),\n\t        'optimizer': optimizer.state_dict(),\n", "        'scheduler': scheduler.state_dict(),\n\t        'step': step\n\t    }\n\t    torch.save(checkpoint, path)\n\t    print(f\"Saved model checkpoint to {path}...\")\n\tdef load_model_checkpoint(path, model, optimizer, scheduler, local_rank):\n\t    \"\"\"Load model checkpoint.\n\t    \"\"\"\n\t    checkpoint = torch.load(path, map_location=f'cuda:{local_rank}')\n\t    model.load_state_dict(checkpoint['model'])\n", "    optimizer.load_state_dict(checkpoint['optimizer'])\n\t    scheduler.load_state_dict(checkpoint['scheduler'])\n\t    step = checkpoint['step']\n\t    print(f\"Loaded model checkpoint from {path}\")\n\t    return model, optimizer, scheduler, step\n\tdef worker_init_fn_eulp(worker_id):\n\t    \"\"\"Set random seed for each worker and init file pointer\n\t    for Buildings-900K dataset workers.\n\t    Args:\n\t        worker_id (int): worker id\n", "    \"\"\"\n\t    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\t    worker_info = torch.utils.data.get_worker_info()\n\t    worker_info.dataset.init_fp()\n\tdef time_features_to_datetime(time_features: np.ndarray,\n\t                              year: int) -> np.array:\n\t    \"\"\"\n\t    Convert time features to datetime objects.\n\t    Args:\n\t        time_features (np.ndarray): Array of time features.\n", "            [:,0] is day of year,\n\t            [:,1] is day of week,\n\t            [:,2] is hour of day.\n\t        year (int): Year to use for datetime objects.\n\t    Returns:\n\t        np.array: Array of datetime objects.\n\t    \"\"\"\n\t    day_of_year = time_features[:,0]\n\t    hour_of_day = time_features[:,2]\n\t    return np.array([datetime.datetime(year, 1, 1, 0, 0, 0) +   # January 1st\n", "                    datetime.timedelta(days=int(doy-1), hours=int(hod), minutes=0, seconds=0)\n\t                    for doy, hod in zip(day_of_year, hour_of_day)])   \n"]}
{"filename": "buildings_bench/evaluation/aggregate.py", "chunked_list": ["import pandas as pd\n\tfrom pathlib import Path \n\tfrom rliable import library as rly\n\timport numpy as np\n\tfrom buildings_bench import BuildingTypes\n\tdef return_aggregate_median(model_list, \n\t                            results_dir,\n\t                            experiment='zero_shot',\n\t                            metrics=['cvrmse'], \n\t                            exclude_simulated = True,\n", "                            only_simulated = False,\n\t                            oov_list = [],\n\t                            reps=50000):\n\t    \"\"\"Compute the aggregate median for a list of models and metrics over all buildings.\n\t    Also returns the stratified 95% boostrap CIs for the aggregate median.\n\t    Args:\n\t        model_list (list): List of models to compute aggregate median for.\n\t        results_dir (str): Path to directory containing results.\n\t        experiment (str, optional): Experiment type. Defaults to 'zero_shot'.\n\t            Options: 'zero_shot', 'transfer_learning'.\n", "        metrics (list, optional): List of metrics to compute aggregate median for. Defaults to ['cvrmse'].\n\t        exclude_simulated (bool, optional): Whether to exclude simulated data. Defaults to True.\n\t        only_simulated (bool, optional): Whether to only include simulated data. Defaults to False.\n\t        oov_list (list, optional): List of OOV buildings to exclude. Defaults to [].\n\t        reps (int, optional): Number of bootstrap replicates to use. Defaults to 50000.\n\t    Returns:\n\t        result_dict (Dict): Dictionary containing aggregate median and CIs for each metric and building type.\n\t    \"\"\"\n\t    result_dict = {}        \n\t    aggregate_func = lambda x : np.array([\n", "        np.median(x.reshape(-1))])\n\t    for building_type in [BuildingTypes.RESIDENTIAL, BuildingTypes.COMMERCIAL]:\n\t        result_dict[building_type] = {}\n\t        for metric in metrics:\n\t            result_dict[building_type][metric] = {}\n\t            if experiment == 'zero_shot' and (metric == 'rps' or metric == 'crps'):\n\t                prefix = 'scoring_rule'\n\t            elif experiment == 'transfer_learning' and (metric == 'rps' or metric == 'crps'):\n\t                prefix = 'TL_scoring_rule'\n\t            elif experiment == 'zero_shot':\n", "                prefix = 'metrics'\n\t            elif experiment == 'transfer_learning':\n\t                prefix = 'TL_metrics'\n\t            for model in model_list:\n\t                df = pd.read_csv(Path(results_dir) / f'{prefix}_{model}.csv')\n\t                if len(oov_list) > 0:\n\t                    # Remove OOV buildings\n\t                    df = df[~df['building_id'].str.contains('|'.join(oov_list))]\n\t                if exclude_simulated:\n\t                    # Exclude synthetic data\n", "                    df = df[~( (df['dataset'] == 'buildings-900k-test') | (df['dataset'] == 'buildings-1m-test') )]\n\t                elif only_simulated:\n\t                    df = df[ (df['dataset'] == 'buildings-900k-test') | (df['dataset'] == 'buildings-1m-test') ]\n\t                # if any df values are inf or nan\n\t                if df.isnull().values.any() or np.isinf(df.value).values.any():\n\t                    print(f'Warning: {model} has inf/nan values')\n\t                # REmove inf/nan values\n\t                df = df.replace(np.inf, np.nan)\n\t                df = df.dropna() \n\t                if metric != 'rps' and metric != 'crps':    \n", "                    result_dict[building_type][metric][model] = \\\n\t                        df[(df['metric'] == metric) & (df['building_type'] == building_type)]['value'].values.reshape(-1,1)\n\t                else:\n\t                    result_dict[building_type][metric][model] = \\\n\t                        df[df['building_type'] == building_type]['value'].values.reshape(-1,1)\n\t            aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n\t                result_dict[building_type][metric], aggregate_func, reps=reps)\n\t            result_dict[building_type][metric] = (aggregate_scores, aggregate_score_cis)\n\t    return result_dict"]}
{"filename": "buildings_bench/evaluation/metrics.py", "chunked_list": ["from typing import Callable\n\timport torch \n\timport abc\n\tclass MetricType:\n\t    \"\"\"Enum class for metric types.\n\t    Attributes:\n\t        SCALAR (str): A scalar metric.\n\t        HOUR_OF_DAY (str): A metric that is calculated for each hour of the day.\n\t    \"\"\"\n\t    SCALAR = 'scalar'\n", "    HOUR_OF_DAY = 'hour_of_day'\n\tclass BuildingsBenchMetric(metaclass=abc.ABCMeta):\n\t    \"\"\"An abstract class for all metrics.\n\t    The basic idea is to acculumate the errors etc. in a list and then\n\t    calculate the mean of the errors etc. at the end of the evaluation.\n\t    Calling the metric will add the error to the list of errors. Calling `.mean()`\n\t    will calculate the mean of the errors, populating the `.value` attribute.\n\t    Attributes:\n\t        name (str): The name of the metric.\n\t        type (MetricType): The type of the metric.\n", "        value (float): The value of the metric.\n\t    \"\"\"\n\t    def __init__(self, name: str, type: MetricType):\n\t        self.name = name\n\t        self.type = type\n\t        self.value = None\n\t    @abc.abstractmethod\n\t    def __call__(self, *args, **kwargs):\n\t        raise NotImplementedError()\n\t    @abc.abstractmethod\n", "    def reset(self):\n\t        raise NotImplementedError()\n\t    @abc.abstractmethod\n\t    def mean(self):\n\t        raise NotImplementedError()\n\tclass Metric(BuildingsBenchMetric):\n\t    \"\"\"A class that represents an error metric.  \n\t    Example:\n\t    ```python\n\t    rmse = Metric('rmse', MetricType.SCALAR, squared_error, sqrt=True)\n", "    mae = Metric('mae', MetricType.SCALAR, absolute_error)\n\t    nmae = Metric('nmae', MetricType.SCALAR, absolute_error, normalize=True)\n\t    cvrmse = Metric('cvrmse', MetricType.SCALAR, squared_error, normalize=True, sqrt=True)\n\t    nmbe = Metric('nmbe', MetricType.SCALAR, bias_error, normalize=True)\n\t    ```\n\t    \"\"\"\n\t    def __init__(self, name: str, type: MetricType, function: Callable, **kwargs):\n\t        \"\"\"\n\t        Args:\n\t            name (str): The name of the metric.\n", "            type (MetricType): The type of the metric.\n\t            function (Callable): A function that takes two tensors and returns a tensor.\n\t        Keyword Args:\n\t            normalize (bool): Whether to normalize the error.\n\t            sqrt (bool): Whether to take the square root of the error.\n\t        \"\"\"\n\t        super().__init__(name, type)\n\t        self.function = function\n\t        self.kwargs = kwargs\n\t        self.global_values = []\n", "        self.errors = []\n\t        self.UNUSED_FLAG = True\n\t    def __call__(self, y_true, y_pred) -> None:\n\t        \"\"\"\n\t        Args:\n\t            y_true (torch.Tensor): shape [batch_size, pred_len]\n\t            y_pred (torch.Tensor): shape [batch_size, pred_len]\n\t        \"\"\"\n\t        self.UNUSED_FLAG = False\n\t        self.errors += [self.function(y_true, y_pred)]\n", "        self.global_values += [y_true]\n\t    def reset(self) -> None:\n\t        \"\"\"Reset the metric.\"\"\"\n\t        self.global_values = []\n\t        self.errors = []\n\t        self.value = None\n\t        self.UNUSED_FLAG = True\n\t    def mean(self) -> None:\n\t        \"\"\"Calculate the mean of the error metric.\"\"\"\n\t        if self.UNUSED_FLAG:\n", "            # Returning a number >= 0 is undefined,\n\t            # because this metric is unused. -1\n\t            # is a flag to indicate this.\n\t            return\n\t        # When we concatenate errors and global values\n\t        # we want shape errors to be shape [batches, pred_len]\n\t        # and global values to be 1D\n\t        if self.errors[0].dim() == 1:\n\t            self.errors = [e.unsqueeze(0) for e in self.errors]\n\t        if self.global_values[0].dim() == 0:\n", "            self.global_values = [g.unsqueeze(0) for g in self.global_values]\n\t        all_errors = torch.concatenate(self.errors,0)\n\t        if self.type == MetricType.SCALAR:\n\t            mean = torch.mean(all_errors)\n\t        elif self.type == MetricType.HOUR_OF_DAY:\n\t            mean = torch.mean(all_errors, dim=0)\n\t        # for root mean error\n\t        if self.kwargs.get('sqrt', False):\n\t            mean = torch.sqrt(mean)\n\t        # normalize\n", "        if self.kwargs.get('normalize', False):\n\t            mean = mean / torch.mean(torch.concatenate(self.global_values,0))\n\t        self.value = mean\n\t################## METRICS ##################\n\tdef absolute_error(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"A PyTorch method that calculates the absolute error (AE) metric.\n\t    Args:\n\t        y_true (torch.Tensor): [batch, pred_len]\n\t        y_pred (torch.Tensor): [batch, pred_len]\n\t    Returns:\n", "        error (torch.Tensor): [batch, pred_len]\n\t    \"\"\"\n\t    return torch.abs(y_true - y_pred)\n\tdef squared_error(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"A PyTorch method that calculates the squared error (SE) metric.\n\t    Args:\n\t        y_true (torch.Tensor): [batch, pred_len]\n\t        y_pred (torch.Tensor): [batch, pred_len]\n\t    Returns:\n\t        error (torch.Tensor): [batch, pred_len]\n", "    \"\"\"\n\t    return torch.square(y_true - y_pred)\n\tdef bias_error(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"A PyTorch method that calculates the bias error (BE) metric.\n\t    Args:\n\t        y_true (torch.Tensor): [batch, pred_len]\n\t        y_pred (torch.Tensor): [batch, pred_len]\n\t    Returns:\n\t        error (torch.Tensor): [batch, pred_len]    \n\t    \"\"\"\n", "    return y_true - y_pred"]}
{"filename": "buildings_bench/evaluation/__init__.py", "chunked_list": ["from buildings_bench.evaluation.metrics import Metric, MetricType\n\tfrom buildings_bench.evaluation.metrics import absolute_error, squared_error, bias_error\n\tfrom buildings_bench.evaluation.scoring_rules import ScoringRule\n\tfrom buildings_bench.evaluation.scoring_rules import RankedProbabilityScore\n\tfrom buildings_bench.evaluation.scoring_rules import ContinuousRankedProbabilityScore\n\tfrom typing import List\n\tmetrics_registry = [\n\t    'rmse',\n\t    'mae',\n\t    'nrmse',\n", "    'nmae',\n\t    'mbe',\n\t    'nmbe',\n\t    'cvrmse'\n\t]\n\tscoring_rule_registry = [\n\t    'rps',\n\t    'crps'\n\t]\n\tdef metrics_factory(name: str,\n", "                    types: List[MetricType] = [MetricType.SCALAR]) -> List[Metric]:\n\t    \"\"\"\n\t    Create a metric from a name.\n\t    By default, will return a scalar metric.\n\t    Args:\n\t        name (str): The name of the metric.\n\t        types (List[MetricTypes]): The types of the metric.         \n\t    Returns:\n\t        metrics_list (List[Metric]): A list of metrics. \n\t    \"\"\"\n", "    assert name.lower() in metrics_registry, f'Invalid metric name: {name}'\n\t    if name.lower() == 'rmse':\n\t        return [ Metric(f'{name.lower()}-{type}', type, squared_error, sqrt=True) for type in types ]\n\t    elif name.lower() == 'mae':\n\t        return [ Metric(f'{name.lower()}-{type}', type, absolute_error) for type in types ]\n\t    elif name.lower() == 'nrmse':\n\t        return [ Metric(f'{name.lower()}-{type}', type, squared_error, normalize=True, sqrt=True) for type in types ]\n\t    elif name.lower() == 'nmae':\n\t        return [ Metric(f'{name.lower()}-{type}', type, absolute_error, normalize=True) for type in types ]\n\t    elif name.lower() == 'mbe':\n", "        return [ Metric(f'{name.lower()}-{type}', type, bias_error) for type in types ]\n\t    elif name.lower() == 'nmbe':\n\t        return [ Metric(f'{name.lower()}-{type}', type, bias_error, normalize=True) for type in types ]\n\t    elif name.lower() == 'cvrmse':\n\t        return [ Metric(f'{name.lower()}-{type}', type, squared_error, normalize=True, sqrt=True) for type in types ]\n\tdef scoring_rule_factory(name: str) -> ScoringRule:\n\t    \"\"\"Create a scoring rule from a name.\n\t    Args:\n\t        name (str): The name of the scoring rule.\n\t    Returns:\n", "        sr (ScoringRule): A scoring rule.\n\t    \"\"\"\n\t    assert name.lower() in scoring_rule_registry, f'Invalid scoring rule name: {name}'\n\t    if name.lower() == 'crps':\n\t        return ContinuousRankedProbabilityScore()\n\t    elif name.lower() == 'rps':\n\t        return RankedProbabilityScore()\n\tdef all_metrics_list() -> List[Metric]:\n\t    \"\"\"Returns all registered metrics.\n\t    Returns:\n", "        metrics_list (List[Metric]): A list of metrics.\n\t    \"\"\"\n\t    metrics_list = []\n\t    for metric in metrics_registry:\n\t        metrics_list += metrics_factory(metric, types=[MetricType.SCALAR, MetricType.HOUR_OF_DAY])\n\t    return metrics_list\n"]}
{"filename": "buildings_bench/evaluation/scoring_rules.py", "chunked_list": ["import torch \n\timport math \n\tfrom buildings_bench.evaluation.metrics import MetricType\n\tfrom buildings_bench.evaluation.metrics import BuildingsBenchMetric\n\tclass ScoringRule(BuildingsBenchMetric):\n\t    \"\"\"An abstract class for all scoring rules.\n\t    \"\"\"\n\t    def __init__(self, name: str):\n\t        super().__init__(name, MetricType.HOUR_OF_DAY)\n\t    def reset(self):\n", "        self.value = None\n\t    def __call__(self, **kwargs):\n\t        raise NotImplementedError()\n\t    def mean(self):\n\t        if self.value is None:\n\t            return\n\t        value = torch.stack(self.value,0)\n\t        self.value = torch.mean(value, 0)    \n\tclass RankedProbabilityScore(ScoringRule):\n\t    \"\"\"A class that calculates the ranked probability score (RPS) metric\n", "    for categorical distributions.\"\"\"\n\t    def __init__(self):\n\t        super().__init__(name='rps')\n\t    def rps(self, y_true, y_pred_logits, centroids) -> None:\n\t        \"\"\"A PyTorch method that calculates the ranked probability score metric\n\t           for categorical distributions.\n\t           Since the bin values are centroids of clusters along the real line,\n\t           we have to compute the width of the bins by summing the distance to\n\t           the left and right centroids of the bin (divided by 2), except for\n\t           the first and last bins, where we only need to sum the distance to\n", "           the right centroid of the first bin and the left centroid of the\n\t           last bin, respectively.\n\t        Args:\n\t            y_true (torch.Tensor): of shape [batch_size, seq_len, 1] categorical labels\n\t            y_pred_logits (torch.Tensor): of shape [batch_size, seq_len, vocab_size] logits\n\t            centroids (torch.Tensor): of shape [vocab_size]\n\t        \"\"\"\n\t        # Convert class labels y_true to one hot vectors [batch_size, seq_len, vocab_size]\n\t        y_true = torch.nn.functional.one_hot(y_true.squeeze(2).long(),\n\t                                             num_classes=centroids.shape[0]).to(y_pred_logits.device)\n", "        # Sort the values, logits, and y_true\n\t        centroids, indices = torch.sort(centroids, dim=-1)\n\t        y_pred_logits = y_pred_logits[:, :, indices]\n\t        y_true = y_true[:, :, indices]\n\t        softmax_preds = torch.softmax(y_pred_logits, dim=-1)\n\t        # Calculate the cumulative distribution function (CDF) of the predictions\n\t        cdf = torch.cumsum(softmax_preds, dim=-1)\n\t        y_true_cdf = torch.cumsum(y_true, dim=-1)\n\t        # Calculate the difference between the CDF and the true values\n\t        square = torch.square(cdf - y_true_cdf)\n", "        # Calculate the widths of the bins:\n\t        # we need to calculate\n\t        # half the distance to the right centroid and left centroid.\n\t        centroid_dist = centroids[1:] - centroids[:-1]\n\t        half_dists = centroid_dist / 2\n\t        widths = torch.unsqueeze(half_dists[1:] + half_dists[:-1], dim=0)\n\t        widths = torch.cat([\n\t            centroids[0].view(1, 1) + half_dists[0].view(1, 1),\n\t            widths,\n\t            half_dists[-1].view(1, 1)\n", "        ],dim=1)\n\t        # Calculate the RPS    \n\t        rps = torch.mean(torch.sum(square * widths, dim=-1), dim=0)  # [seq_len]\n\t        if self.value is None:\n\t            self.value = [rps]\n\t        else:\n\t            self.value += [rps]\n\t    def __call__(self, true_continuous, y_true, y_pred_logits, centroids):\n\t        self.rps(y_true, y_pred_logits, centroids)\n\tclass ContinuousRankedProbabilityScore(ScoringRule):\n", "    \"\"\"\n\t    A class that calculates the Gaussian continuous ranked probability score (CRPS) metric.\n\t    \"\"\"\n\t    def __init__(self):\n\t        super().__init__(name = 'crps')\n\t    def crps(self, true_continuous, y_pred_distribution_params) -> None:\n\t        \"\"\"Computes the Gaussian CRPS.\n\t        Args:\n\t            true_continuous (torch.Tensor): of shape [batch_size, seq_len, 1]\n\t            y_pred_distribution_params (torch.Tensor): of shape [batch_size, seq_len, 2]\n", "        \"\"\"\n\t        pred_mu = y_pred_distribution_params[:, :, 0].unsqueeze(-1)\n\t        pred_sigma = y_pred_distribution_params[:, :, 1].unsqueeze(-1)\n\t        # standardize the true values to N(0,1)\n\t        true_continuous = (true_continuous - pred_mu) / pred_sigma\n\t        # Calculate the cumulative distribution function (CDF) of the predictions\n\t        cdf = 0.5 * (1 + torch.erf(true_continuous / math.sqrt(2)))\n\t        # Calculate the pdf of the predictions\n\t        pdf = torch.exp(-torch.square(true_continuous) / 2) / math.sqrt(2 * math.pi)\n\t        # Calculate pi inv\n", "        pi_inv = 1 / math.sqrt(math.pi)\n\t        # CRPS\n\t        crps = pred_sigma * ( true_continuous * (2 * cdf - 1) + 2 * pdf - pi_inv)\n\t        crps = torch.mean(crps, dim=0).squeeze(1)  # [seq_len]\n\t        if self.value is None:\n\t            self.value = [crps]\n\t        else:\n\t            self.value += [crps]\n\t    def __call__(self, true_continuous, y_true, y_pred_distribution_params, centroids):\n\t        self.crps(true_continuous, y_pred_distribution_params)\n"]}
{"filename": "buildings_bench/evaluation/managers.py", "chunked_list": ["import torch\n\tfrom buildings_bench.evaluation import metrics_factory\n\tfrom buildings_bench.evaluation.metrics import Metric, MetricType\n\tfrom buildings_bench.evaluation.scoring_rules import ScoringRule\n\tfrom typing import List\n\timport pandas as pd\n\tfrom copy import deepcopy\n\tclass BuildingTypes:\n\t    \"\"\"Enum for supported types of buildings.\n\t    Attributes:\n", "        RESIDENTIAL (str): Residential building type.\n\t        COMMERCIAL (str): Commercial building type.\n\t        RESIDENTIAL_INT (int): Integer representation of residential building type (0).\n\t        COMMERCIAL_INT (int): Integer representation of commercial building type (1).\n\t    \"\"\"\n\t    RESIDENTIAL = 'residential'\n\t    COMMERCIAL = 'commercial'\n\t    RESIDENTIAL_INT = 0\n\t    COMMERCIAL_INT = 1\n\tclass MetricsManager:\n", "    \"\"\"A class that keeps track of all metrics (and a scoring rule)for one or more buildings.\n\t    Metrics are computed for each building type (residential and commercial).\n\t    Example:\n\t    ```python\n\t    from buildings_bench.evaluation.managers import MetricsManager\n\t    from buildings_bench.evaluation import metrics_factory\n\t    from buildings_bench import BuildingTypes\n\t    import torch\n\t    metrics_manager = MetricsManager(metrics=metrics_factory('cvrmse'))\n\t    metrics_manager(\n", "        y_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t        y_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\n\t        building_type = BuildingTypes.RESIDENTIAL_INT\n\t    )\n\t    for metric in metrics_manager.metrics[BuildingTypes.RESIDENTIAL]:\n\t        metric.mean()\n\t        print(metric.value) # prints tensor(0.)\n\t    ```\n\t    \"\"\"\n\t    def __init__(self, metrics: List[Metric] = None, scoring_rule: ScoringRule = None):\n", "        \"\"\"Initializes the MetricsManager.\n\t        Args:\n\t            metrics (List[Metric]): A list of metrics to compute for each\n\t                building type.\n\t            scoring_rule (ScoringRule): A scoring rule to compute for each\n\t                building type.\n\t        \"\"\"\n\t        self.metrics = {}\n\t        if not metrics is None:\n\t            self.metrics[BuildingTypes.RESIDENTIAL] = metrics\n", "            self.metrics[BuildingTypes.COMMERCIAL] = deepcopy(metrics)\n\t        self.scoring_rules = {}\n\t        if not scoring_rule is None:\n\t            self.scoring_rules[BuildingTypes.RESIDENTIAL] = scoring_rule\n\t            self.scoring_rules[BuildingTypes.COMMERCIAL] = deepcopy(scoring_rule)\n\t        self.accumulated_unnormalized_loss = 0\n\t        self.total_samples = 0\n\t    def _compute_all(self, y_true: torch.Tensor,\n\t                     y_pred: torch.Tensor,\n\t                     building_types_mask: torch.Tensor, **kwargs) -> None:\n", "        \"\"\"Computes all metrics and scoring rules for the given batch.\n\t        Args:\n\t            y_true (torch.Tensor): A tensor of shape [batch, pred_len, 1] with\n\t                the true values.\n\t            y_pred (torch.Tensor): A tensor of shape [batch, pred_len, 1] with\n\t                the predicted values.\n\t            building_types_mask (torch.Tensor): A tensor of shape [batch] with  \n\t                the building types of each sample.\n\t        \"\"\"\n\t        if len(self.metrics) > 0:\n", "            for building_type in [BuildingTypes.RESIDENTIAL, BuildingTypes.COMMERCIAL]:\n\t                if building_type == BuildingTypes.RESIDENTIAL:\n\t                    predictions = y_pred[~building_types_mask]\n\t                    if predictions.shape[0] == 0:\n\t                        continue\n\t                    targets = y_true[~building_types_mask]\n\t                elif building_type == BuildingTypes.COMMERCIAL:\n\t                    predictions = y_pred[building_types_mask]\n\t                    if predictions.shape[0] == 0:\n\t                        continue\n", "                    targets = y_true[building_types_mask]\n\t                for metric in self.metrics[building_type]:\n\t                    metric(targets, predictions)\n\t        if len(self.scoring_rules) > 0:\n\t            self._compute_scoring_rule(y_true,\n\t                                      kwargs['y_categories'],\n\t                                      kwargs['y_distribution_params'],\n\t                                      kwargs['centroids'],\n\t                                      building_types_mask)\n\t    def _compute_scoring_rule(self, \n", "                            true_continuous,\n\t                            true_categories,\n\t                            y_distribution_params,\n\t                            centroids,\n\t                            building_types_mask) -> None:\n\t        \"\"\"Compute the scoring rule.\n\t        Args:\n\t            true_continuous (torch.Tensor): The true continuous load values.\n\t                [bsz_sz, pred_len, 1]\n\t            true_categories (torch.Tensor): The true quantized load values.\n", "            y_distribution_params are [bsz_sz, pred_len, vocab_size] if logits,\n\t             or are [bsz_sz, pred_len, 2] if Gaussian\n\t            centroids (torch.Tensor): The bin values for the quantized distribution.\n\t            building_types_mask (torch.Tensor): \n\t        \"\"\"\n\t        if y_distribution_params is None:\n\t            raise ValueError('y_distribution_params must be provided to compute scoring rule.')\n\t        for building_type in [BuildingTypes.RESIDENTIAL, BuildingTypes.COMMERCIAL]:\n\t            if building_type == BuildingTypes.RESIDENTIAL:\n\t                true_continuous_by_type = true_continuous[~building_types_mask]\n", "                if true_continuous_by_type.shape[0] == 0:\n\t                    continue\n\t                self.scoring_rules[building_type](\n\t                    true_continuous_by_type,\n\t                    true_categories[~building_types_mask],\n\t                    y_distribution_params[~building_types_mask],\n\t                    centroids)\n\t            elif building_type == BuildingTypes.COMMERCIAL:\n\t                true_continuous_by_type = true_continuous[building_types_mask]\n\t                if true_continuous_by_type.shape[0] == 0:\n", "                    continue\n\t                self.scoring_rules[building_type](\n\t                    true_continuous_by_type,\n\t                    true_categories[building_types_mask],\n\t                    y_distribution_params[building_types_mask],\n\t                    centroids)\n\t    def _update_loss(self, loss, sample_size):\n\t        \"\"\"Updates the accumulated loss and total samples.\"\"\"\n\t        self.accumulated_unnormalized_loss += (loss * sample_size)\n\t        self.total_samples += sample_size\n", "    def get_ppl(self):\n\t        \"\"\"Returns the perplexity of the accumulated loss.\"\"\"\n\t        return torch.exp(self.accumulated_unnormalized_loss) / self.total_samples\n\t    def summary(self, with_loss=False, with_ppl=False):\n\t        \"\"\"Return a summary of the metrics for the dataset.\n\t        A summary maps keys to objects of type Metric or ScoringRule.\n\t        \"\"\"\n\t        summary = {}\n\t        for building_type in [BuildingTypes.RESIDENTIAL, BuildingTypes.COMMERCIAL]:\n\t            summary[building_type] = {}\n", "            if len(self.metrics) > 0:\n\t                for metric in self.metrics[building_type]:\n\t                    if not metric.UNUSED_FLAG:\n\t                        metric.mean()\n\t                        summary[building_type][metric.name] = metric\n\t            if len(self.scoring_rules) > 0: \n\t                if not self.scoring_rules[building_type].value is None: \n\t                    self.scoring_rules[building_type].mean()\n\t                    summary[building_type][self.scoring_rules[building_type].name] = \\\n\t                        self.scoring_rules[building_type]\n", "        if with_ppl and self.total_samples > 0:\n\t            summary['ppl'] = self.get_ppl()\n\t        if with_loss and self.total_samples > 0:\n\t            summary['loss'] = self.accumulated_unnormalized_loss / self.total_samples\n\t        return summary\n\t    def reset(self, loss: bool = True) -> None:\n\t        \"\"\"Reset the metrics.\"\"\"\n\t        for building_type in [BuildingTypes.RESIDENTIAL, BuildingTypes.COMMERCIAL]:\n\t            for metric in self.metrics[building_type]:\n\t                metric.reset()\n", "            if len(self.scoring_rules) > 0:\n\t                self.scoring_rules[building_type].reset()\n\t        if loss:\n\t            self.accumulated_unnormalized_loss = 0\n\t            self.total_samples = 0\n\t    def __call__(self, \n\t                 y_true: torch.Tensor,\n\t                 y_pred: torch.Tensor,\n\t                 building_types_mask: torch.Tensor = None,\n\t                 building_type: int = BuildingTypes.COMMERCIAL_INT,\n", "                 **kwargs):\n\t        \"\"\"Compute metrics for a batch of predictions.\n\t        Args:\n\t            y_true (torch.Tensor): The true (unscaled) load values. (continuous)\n\t                shape is [batch_size, pred_len, 1]\n\t            y_pred (torch.Tensor): The predicted (unscaled) load values. (continuous)\n\t                shape is [batch_size, pred_len, 1]\n\t            building_types_mask (torch.Tensor): \n\t                A boolean mask indicating the building type of each building.\n\t                True (1) if commercial, False (0). Shape is [batch_size].\n", "            building_type (int): The building type of the batch. Can be provided \n\t                instead of building_types_mask if all buildings are of the same type.\n\t        Keyword args:\n\t            y_categories (torch.Tensor): The true load values. (quantized)\n\t            y_distribution_params (torch.Tensor): logits, Gaussian params, etc.\n\t            centroids (torch.Tensor): The bin values for the quantized load.\n\t            loss (torch.Tensor): The loss for the batch.\n\t        \"\"\"\n\t        if building_types_mask is None:\n\t            building_types_mask = (building_type == BuildingTypes.COMMERCIAL_INT) * \\\n", "                                   torch.ones(y_true.shape[0], dtype=torch.bool, device=y_true.device)\n\t        self._compute_all(y_true, y_pred, building_types_mask, **kwargs)\n\t        if 'loss' in kwargs:\n\t            batch_size, pred_len, _ = y_true.shape\n\t            self._update_loss(kwargs['loss'], batch_size * pred_len)\n\tclass DatasetMetricsManager:\n\t    \"\"\"\n\t    A class that manages a MetricsManager for each building\n\t    in one or more benchmark datasets. \n\t    One DatasetMetricsManager can be used to keep track of all metrics\n", "    when evaluating a model on all of the benchmark's datasets.\n\t    This class wil create a Pandas Dataframe summary containing the metrics for each building.\n\t    Default metrics are NRMSE (CVRMSE), NMAE, NMBE.\n\t    \"\"\"\n\t    default_metrics = metrics_factory('cvrmse',\n\t                      types=[MetricType.SCALAR, MetricType.HOUR_OF_DAY]) \\\n\t                      + metrics_factory('nmbe', types=[MetricType.SCALAR, MetricType.HOUR_OF_DAY]) \\\n\t                      + metrics_factory('nmae', types=[MetricType.SCALAR, MetricType.HOUR_OF_DAY])\n\t    def __init__(self, \n\t                 metrics: List[Metric] = default_metrics,\n", "                 scoring_rule: ScoringRule = None):\n\t        self.metrics_list = metrics\n\t        self.scoring_rule = scoring_rule\n\t        self._metrics = {}\n\t    def get_building_from_dataset(self, dataset_name: str, building_id: str) -> None:\n\t        # Check if dataset exists\n\t        if dataset_name not in self._metrics:\n\t            return None\n\t        # Check if building exists\n\t        if building_id not in self._metrics[dataset_name]:\n", "            return None\n\t        return self._metrics[dataset_name][building_id]\n\t    def add_building_to_dataset_if_missing(self, dataset_name: str, building_id: str) -> None:\n\t        # Check if dataset exists\n\t        if dataset_name not in self._metrics:\n\t            self._metrics[dataset_name] = {}\n\t        # Check if building already exists\n\t        if building_id not in self._metrics[dataset_name]:\n\t            # Use deepcopy to pass new Metric and Scoring Rule objects\n\t            self._metrics[dataset_name][building_id] = \\\n", "                MetricsManager(deepcopy(self.metrics_list), deepcopy(self.scoring_rule))\n\t    def summary(self, dataset_name: str = None) -> pd.DataFrame:\n\t        \"\"\"Return a summary of the metrics for the dataset.\n\t        Args:\n\t            dataset_name (str): The name of the dataset to summarize. If None,\n\t                summarize all datasets.\n\t        Returns:\n\t            A Pandas dataframe with the following columns:\n\t                - dataset: The name of the dataset.\n\t                - building_id: The unique ID of the building.\n", "                - building_type: The type of the building.\n\t                - metric: The name of the metric.\n\t                - metric_type: The type of the metric. (scalar or hour_of_day)\n\t                - value: The value of the metric.\n\t        \"\"\"\n\t        summary = {}\n\t        if dataset_name is None: # summarize all datasets\n\t            for dataset_name in self._metrics.keys():\n\t                summary[dataset_name] = {}\n\t                for building_id in self._metrics[dataset_name].keys():\n", "                    summary[dataset_name][building_id] = \\\n\t                        self._metrics[dataset_name][building_id].summary()\n\t        else:\n\t            summary[dataset_name] = {}\n\t            for building_id in self._metrics[dataset_name].keys():\n\t                summary[dataset_name][building_id] = \\\n\t                    self._metrics[dataset_name][building_id].summary()\n\t        # to Pandas dataframe\n\t        columns = ['dataset', 'building_id', 'building_type', 'metric', 'metric_type', 'value']\n\t        rows = []\n", "        # for each dataset\n\t        for dataset_name in summary.keys():\n\t            # for each building\n\t            for building_id in summary[dataset_name].keys():\n\t                # for the building type \n\t                for building_type in summary[dataset_name][building_id].keys():\n\t                    # for each metric\n\t                    for metric_name in summary[dataset_name][building_id][building_type].keys():\n\t                        # if scoring rule, skip\n\t                        if self.scoring_rule and metric_name == self.scoring_rule.name:\n", "                            continue\n\t                        # if the metric is a scalar\n\t                        if summary[dataset_name][building_id][building_type][metric_name].type == MetricType.SCALAR:\n\t                            rows.append([dataset_name, building_id, building_type,\n\t                                         metric_name.split('-')[0], MetricType.SCALAR,\n\t                                         summary[dataset_name][building_id][building_type][metric_name].value.item()])\n\t                        # if the metric is a list of scalars\n\t                        elif summary[dataset_name][building_id][building_type][metric_name].type == MetricType.HOUR_OF_DAY:\n\t                            multi_hour_value = summary[dataset_name][building_id][building_type][metric_name].value\n\t                            for hour in range(multi_hour_value.shape[0]):\n", "                                rows.append([dataset_name, building_id,\n\t                                             building_type, metric_name.split('-')[0] + '_' + str(hour), \n\t                                             MetricType.HOUR_OF_DAY, multi_hour_value[hour].item()])\n\t        metric_df = pd.DataFrame(rows, columns=columns)\n\t        if self.scoring_rule:\n\t            columns = ['dataset', 'building_id', 'building_type', 'scoring_rule', 'value']\n\t            rows = []\n\t            for dataset_name in summary.keys():\n\t                for building_id in summary[dataset_name].keys():\n\t                    for building_type in summary[dataset_name][building_id].keys():\n", "                        if self.scoring_rule.name in summary[dataset_name][building_id][building_type]:\n\t                            score = summary[dataset_name][building_id][building_type][self.scoring_rule.name].value\n\t                            for hour in range(score.shape[0]):\n\t                                rows.append([dataset_name, building_id, building_type,\n\t                                            self.scoring_rule.name + '_' + str(hour), score[hour].item()])\n\t            scoring_rule_df = pd.DataFrame(rows, columns=columns)\n\t            return metric_df, scoring_rule_df\n\t        else:\n\t            return metric_df\n\t    def __call__(self, \n", "                 dataset_name: str,\n\t                 building_id: str,\n\t                 y_true: torch.Tensor, \n\t                 y_pred: torch.Tensor,\n\t                 building_types_mask: torch.Tensor = None,\n\t                 building_type: int = BuildingTypes.COMMERCIAL_INT,\n\t                 **kwargs) -> None:\n\t        \"\"\"Compute metrics for a batch of predictions for a single building in a dataset.\n\t        Args:\n\t            dataset_name (str): The name of the dataset.\n", "            building_id (str): The unique building identifier.\n\t            y_true (torch.Tensor): The true (unscaled) load values. (continuous)\n\t                shape is [batch_size, pred_len, 1]\n\t            y_pred (torch.Tensor): The predicted (unscaled) load values. (continuous)\n\t                shape is [batch_size, pred_len, 1]\n\t            building_types_mask (torch.Tensor): \n\t                A boolean mask indicating the building type of each building.\n\t                True (1) if commercial, False (0). Shape is [batch_size]. Default is None.\n\t            building_type (int): The building type of the batch. Can be provided \n\t                instead of building_types_mask if all buildings are of the same type.\n", "        Keyword args:\n\t            y_categories (torch.Tensor): The true load values. (quantized)\n\t            y_distribution_params (torch.Tensor): logits, Gaussian params, etc.\n\t            centroids (torch.Tensor): The bin values for the quantized load.\n\t            loss (torch.Tensor): The loss for the batch.        \n\t        \"\"\"\n\t        self.add_building_to_dataset_if_missing(dataset_name, building_id)\n\t        if building_types_mask is None:\n\t            building_types_mask = (building_type == BuildingTypes.COMMERCIAL_INT) * \\\n\t                                   torch.ones(y_true.shape[0], dtype=torch.bool, device=y_true.device)\n", "        self._metrics[dataset_name][building_id](y_true, y_pred, building_types_mask, **kwargs)\n"]}
{"filename": "buildings_bench/data/__init__.py", "chunked_list": ["from pathlib import Path\n\timport torch\n\timport tomli\n\timport os\n\tfrom buildings_bench.data.buildings900K import Buildings900K\n\tfrom buildings_bench.data.datasets import TorchBuildingDatasetsFromCSV\n\tfrom buildings_bench.data.datasets import TorchBuildingDatasetFromParquet\n\tfrom buildings_bench.data.datasets import PandasBuildingDatasetsFromCSV\n\tfrom buildings_bench import BuildingTypes\n\tfrom buildings_bench import transforms\n", "from typing import List, Union\n\tdataset_registry = [\n\t    'buildings-900k-train',\n\t    'buildings-900k-val',\n\t    'buildings-900k-test',\n\t    'sceaux',\n\t    'borealis',\n\t    'ideal',\n\t    'bdg-2:panther',\n\t    'bdg-2:fox',\n", "    'bdg-2:rat',\n\t    'bdg-2:bear',\n\t    'electricity',\n\t    'smart',\n\t    'lcl'\n\t]\n\tbenchmark_registry = [\n\t    'buildings-900k-test',\n\t    'sceaux',\n\t    'borealis',\n", "    'ideal',\n\t    'bdg-2:panther',\n\t    'bdg-2:fox',\n\t    'bdg-2:rat',\n\t    'bdg-2:bear',\n\t    'electricity',\n\t    'smart',\n\t    'lcl'\n\t]        \n\tdef parse_building_years_metadata(datapath: Path, dataset_name: str):\n", "    with open(datapath / 'metadata' / 'building_years.txt', 'r') as f:\n\t        building_years = f.readlines()\n\t    building_years = [building_year.strip() for building_year in building_years]\n\t    building_years = filter(lambda building_year: dataset_name in building_year.lower(), building_years)\n\t    return list(building_years)\n\tdef load_pretraining(\n\t        name: str,\n\t        num_buildings_ablation: int = -1,\n\t        apply_scaler_transform: str = '',\n\t        scaler_transform_path: Path = None,\n", "        context_len=168, # week\n\t        pred_len=24) -> torch.utils.data.Dataset:\n\t    r\"\"\"\n\t    Pre-training datasets: buildings-900k-train, buildings-900k-val\n\t    Args:\n\t        name (str): Name of the dataset to load.\n\t        num_buildings_ablation (int): Number of buildings to use for pre-training.\n\t                                        If -1, use all buildings.\n\t        apply_scaler_transform (str): If not using quantized load or unscaled loads,\n\t                                 applies a {boxcox,standard} scaling transform to the load. Default: ''.\n", "        scaler_transform_path (Path): Path to data for transform, e.g., pickled data for BoxCox transform.\n\t        context_len (int): Length of the context. Defaults to 168.\n\t        pred_len (int): Length of the prediction horizon. Defaults to 24.\n\t    Returns:\n\t        torch.utils.data.Dataset: Dataset for pretraining.\n\t    \"\"\"\n\t    dataset_path = Path(os.environ.get('BUILDINGS_BENCH', ''))\n\t    if not dataset_path.exists():\n\t        raise ValueError('BUILDINGS_BENCH environment variable not set')\n\t    if num_buildings_ablation > 0:\n", "        idx_file_suffix = f'_{num_buildings_ablation}'\n\t    else:\n\t        idx_file_suffix = ''\n\t    if name.lower() == 'buildings-900k-train':\n\t        idx_file = f'train_weekly{idx_file_suffix}.idx'\n\t        dataset = Buildings900K(dataset_path,\n\t                               idx_file,\n\t                               context_len=context_len,\n\t                               pred_len=pred_len,\n\t                               apply_scaler_transform=apply_scaler_transform,\n", "                               scaler_transform_path = scaler_transform_path)\n\t    elif name.lower() == 'buildings-900k-val':\n\t        idx_file = f'val_weekly{idx_file_suffix}.idx'\n\t        dataset = Buildings900K(dataset_path,\n\t                               idx_file,\n\t                               context_len=context_len,\n\t                               pred_len=pred_len,\n\t                               apply_scaler_transform=apply_scaler_transform,\n\t                               scaler_transform_path = scaler_transform_path)\n\t    return dataset\n", "def load_torch_dataset(\n\t        name: str,\n\t        dataset_path: Path = None,\n\t        apply_scaler_transform: str = '',\n\t        scaler_transform_path: Path = None,\n\t        context_len = 168,\n\t        pred_len = 24\n\t        ) -> Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]:\n\t    r\"\"\"Load datasets by name.\n\t    Args:\n", "        name (str): Name of the dataset to load.\n\t        dataset_path (Path): Path to the benchmark data. Optional.\n\t        apply_scaler_transform (str): If not using quantized load or unscaled loads,\n\t                                 applies a {boxcox,standard} scaling transform to the load. Default: ''.\n\t        scaler_transform_path (Path): Path to data for transform, e.g., pickled data for BoxCox transform.\n\t        context_len (int): Length of the context. Defaults to 168.\n\t        pred_len (int): Length of the prediction horizon. Defaults to 24.\n\t    Returns:\n\t        dataset (Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]): Dataset for benchmarking.\n\t    \"\"\"\n", "    if not dataset_path:\n\t        dataset_path = Path(os.environ.get('BUILDINGS_BENCH', ''))\n\t        if not dataset_path.exists():\n\t            raise ValueError('BUILDINGS_BENCH environment variable not set')\n\t    with open(dataset_path / 'metadata' / 'benchmark.toml', 'rb') as f:\n\t        metadata = tomli.load(f)['buildings_bench']\n\t    if name.lower() == 'buildings-900k-test':\n\t        spatial_lookup = transforms.LatLonTransform()\n\t        puma_files = list((dataset_path / 'Buildings-900K-test' / '2021').glob('*2018*/*/*/*/*/*.parquet'))\n\t        if len(puma_files) == 0:\n", "            raise ValueError(f'Could not find any Parquet files in '\n\t                             f' {str(dataset_path / \"Buildings-900K-test\" / \"2021\")}')\n\t        # to string\n\t        puma_files = [str(Path(pf).parent) for pf in puma_files]\n\t        puma_ids = [pf.split('puma=')[1] for pf in puma_files]\n\t        building_types = []\n\t        for pf in puma_files:\n\t            if 'res' in pf:\n\t                building_types += [BuildingTypes.RESIDENTIAL]\n\t            elif 'com' in pf:\n", "                building_types += [BuildingTypes.COMMERCIAL]\n\t        dataset_generator = TorchBuildingDatasetFromParquet(\n\t                                                         puma_files,\n\t                                                         [spatial_lookup.undo_transform( # pass unnormalized lat lon coords\n\t                                                            spatial_lookup.transform(pid)) for pid in puma_ids],\n\t                                                         building_types,\n\t                                                         context_len=context_len,\n\t                                                         pred_len=pred_len,\n\t                                                         apply_scaler_transform=apply_scaler_transform,\n\t                                                         scaler_transform_path = scaler_transform_path,\n", "                                                         leap_years=metadata['leap_years'])\n\t    elif ':' in name.lower():\n\t        name, subset = name.lower().split(':')\n\t        dataset_metadata = metadata[name.lower()]\n\t        all_by_files = parse_building_years_metadata(dataset_path, name.lower())\n\t        all_by_files = filter(lambda by_file: subset in by_file.lower(), all_by_files)\n\t        dataset_generator = TorchBuildingDatasetsFromCSV(dataset_path,\n\t                                                         all_by_files,\n\t                                                         dataset_metadata[subset]['latlon'],\n\t                                                         dataset_metadata[subset]['building_type'],\n", "                                                         context_len=context_len,\n\t                                                         pred_len=pred_len,\n\t                                                         apply_scaler_transform=apply_scaler_transform,\n\t                                                         scaler_transform_path = scaler_transform_path,\n\t                                                         leap_years=metadata['leap_years']) \n\t    elif name.lower() in benchmark_registry:\n\t        dataset_metadata = metadata[name.lower()]\n\t        all_by_files = parse_building_years_metadata(dataset_path, name.lower())\n\t        dataset_generator = TorchBuildingDatasetsFromCSV(dataset_path,\n\t                                                         all_by_files,\n", "                                                         dataset_metadata['latlon'],\n\t                                                         dataset_metadata['building_type'],\n\t                                                         context_len=context_len,\n\t                                                         pred_len=pred_len,\n\t                                                         apply_scaler_transform=apply_scaler_transform,\n\t                                                         scaler_transform_path = scaler_transform_path,\n\t                                                         leap_years=metadata['leap_years']) \n\t    else:\n\t        raise ValueError(f'Unknown dataset {name}')\n\t    return dataset_generator\n", "def load_pandas_dataset(\n\t        name: str,\n\t        dataset_path: Path = None,\n\t        feature_set: str = 'engineered',\n\t        apply_scaler_transform: str = '',\n\t        scaler_transform_path: Path = None) -> PandasBuildingDatasetsFromCSV:\n\t    \"\"\"\n\t    Load datasets by name.\n\t    Args:\n\t        name (str): Name of the dataset to load.\n", "        dataset_path (Path): Path to the benchmark data. Optional.\n\t        feature_set (str): Feature set to use. Default: 'engineered'.\n\t        apply_scaler_transform (str): If not using quantized load or unscaled loads,\n\t                                    applies a {boxcox,standard} scaling transform to the load. Default: ''. \n\t        scaler_transform_path (Path): Path to data for transform, e.g., pickled data for BoxCox transform.\n\t    Returns:\n\t        dataset (PandasBuildingDatasetsFromCSV): Generator of Pandas datasets for benchmarking.\n\t    \"\"\"\n\t    if not dataset_path:\n\t        dataset_path = Path(os.environ.get('BUILDINGS_BENCH', ''))\n", "        if not dataset_path.exists():\n\t            raise ValueError('BUILDINGS_BENCH environment variable not set')\n\t    if name.lower() == 'buildings-900k-test':\n\t        raise ValueError(f'{name.lower()} unavailable for now as pandas dataset')\n\t    with open(dataset_path / 'metadata' / 'benchmark.toml', 'rb') as f:\n\t        metadata = tomli.load(f)['buildings_bench']\n\t    if ':' in name.lower():\n\t        name, subset = name.lower().split(':')\n\t        dataset_metadata = metadata[name.lower()]\n\t        all_by_files = parse_building_years_metadata(dataset_path, name.lower())\n", "        all_by_files = filter(lambda by_file: subset in by_file.lower(), all_by_files)\n\t        building_type = dataset_metadata[subset]['building_type']\n\t        building_latlon = dataset_metadata[subset]['latlon']\n\t    else:\n\t        dataset_metadata = metadata[name.lower()]\n\t        all_by_files = parse_building_years_metadata(dataset_path, name.lower())\n\t        building_type = dataset_metadata['building_type']\n\t        building_latlon = dataset_metadata['latlon']\n\t    return PandasBuildingDatasetsFromCSV(\n\t            dataset_path,\n", "            all_by_files,\n\t            building_latlon,\n\t            building_type,\n\t            features=feature_set,\n\t            apply_scaler_transform = apply_scaler_transform,\n\t            scaler_transform_path = scaler_transform_path,\n\t            leap_years = metadata['leap_years'])\n"]}
{"filename": "buildings_bench/data/buildings900K.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom pathlib import Path\n\timport pyarrow.parquet as pq\n\timport buildings_bench.transforms as transforms\n\tfrom buildings_bench.transforms import BoxCoxTransform, StandardScalerTransform\n\tclass Buildings900K(torch.utils.data.Dataset):\n\t    r\"\"\"This is an indexed dataset for the Buildings-900K dataset.\n\t    It uses an index file to quickly load a sub-sequence from a time series in a multi-building\n\t    Parquet file. The index file is a tab separated file with the following columns:\n", "    0. Building-type-and-year (e.g., comstock_tmy3_release_1)\n\t    1. Census region (e.g., by_puma_midwest)\n\t    2. PUMA ID\n\t    3. Building ID\n\t    4. Hour of year pointer (e.g., 0070)\n\t    The sequence pointer is used to extract the slice\n\t    [pointer - context length : pointer + pred length] for a given building ID.\n\t    The time series are not stored chronologically and must be sorted by timestamp after loading.\n\t    Each dataloader worker has its own file pointer to the index file. This is to avoid\n\t    weird multiprocessing errors from sharing a file pointer. We 'seek' to the correct\n", "    line in the index file for random access.\n\t    With 4 workers, data loading with an indexed dataset requires about 30GB of RAM.\n\t    \"\"\"\n\t    def __init__(self, \n\t                dataset_path: Path,\n\t                index_file: str,\n\t                context_len: int = 168,\n\t                pred_len: int = 24,\n\t                apply_scaler_transform: str = '',\n\t                scaler_transform_path: Path = None):\n", "        \"\"\"\n\t        Args:\n\t            dataset_path (Path): Path to the pretraining dataset.\n\t            index_file (str): Name of the index file\n\t            context_len (int, optional): Length of the context. Defaults to 168. \n\t                The index file has to be generated with the same context length.\n\t            pred_len (int, optional): Length of the prediction horizon. Defaults to 24.\n\t                The index file has to be generated with the same pred length.\n\t            apply_scaler_transform (str, optional): Apply a scaler transform to the load. Defaults to ''.\n\t            scaler_transform_path (Path, optional): Path to the scaler transform. Defaults to None.\n", "        \"\"\"\n\t        self.dataset_path = dataset_path / 'Buildings-900K' / 'end-use-load-profiles-for-us-building-stock' / '2021'\n\t        self.metadata_path = dataset_path / 'metadata'\n\t        self.context_len = context_len\n\t        self.pred_len = pred_len\n\t        self.building_type_and_year = ['comstock_tmy3_release_1',\n\t                                       'resstock_tmy3_release_1',\n\t                                       'comstock_amy2018_release_1',\n\t                                       'resstock_amy2018_release_1']\n\t        self.census_regions = ['by_puma_midwest', 'by_puma_south', 'by_puma_northeast', 'by_puma_west']\n", "        self.index_file = self.metadata_path / index_file\n\t        self.index_fp = None\n\t        self.__read_index_file(self.index_file)\n\t        self.time_transform = transforms.TimestampTransform()\n\t        self.spatial_transform = transforms.LatLonTransform()\n\t        self.apply_scaler_transform = apply_scaler_transform\n\t        if self.apply_scaler_transform == 'boxcox':\n\t            self.load_transform = BoxCoxTransform()\n\t            self.load_transform.load(scaler_transform_path)\n\t        elif self.apply_scaler_transform == 'standard':\n", "            self.load_transform = StandardScalerTransform()\n\t            self.load_transform.load(scaler_transform_path)\n\t    def init_fp(self):\n\t        \"\"\"Each worker needs to open its own file pointer to avoid \n\t        weird multiprocessing errors from sharing a file pointer.\n\t        This is not called in the main process.\n\t        This is called in the DataLoader worker_init_fn.\n\t        The file is opened in binary mode which lets us disable buffering.\n\t        \"\"\"\n\t        self.index_fp = open(self.index_file, 'rb', buffering=0)\n", "        self.index_fp.seek(0)\n\t    def __read_index_file(self, index_file: Path) -> None:\n\t        \"\"\"Extract metadata from index file.\n\t        \"\"\"\n\t        # Fast solution to get the number of time series in index file\n\t        # https://pynative.com/python-count-number-of-lines-in-file/\n\t        def _count_generator(reader):\n\t            b = reader(1024 * 1024)\n\t            while b:\n\t                yield b\n", "                b = reader(1024 * 1024)\n\t        with open(index_file, 'rb') as fp:\n\t            c_generator = _count_generator(fp.raw.read)\n\t            # count each \\n\n\t            self.num_time_series = sum(buffer.count(b'\\n') for buffer in c_generator)\n\t        # Count the number of chars per line\n\t        with open(index_file, 'rb', buffering=0) as fp:\n\t            first_line = fp.readline()\n\t            self.chunk_size = len(first_line)\n\t        print(f'Counted {self.num_time_series} indices in index file.')\n", "    def __del__(self):\n\t        if self.index_fp:\n\t            self.index_fp.close()\n\t    def __len__(self):\n\t        return self.num_time_series\n\t    def __getitem__(self, idx):\n\t        # Open file pointer if not already open\n\t        if not self.index_fp:\n\t           self.index_fp = open(self.index_file, 'rb', buffering=0)\n\t           self.index_fp.seek(0)\n", "        # Get the index of the time series\n\t        self.index_fp.seek(idx * self.chunk_size, 0)\n\t        ts_idx = self.index_fp.read(self.chunk_size).decode('utf-8')\n\t        # Parse the index\n\t        ts_idx = ts_idx.strip('\\n').split('\\t')\n\t        # strip loading zeros\n\t        seq_ptr = ts_idx[-1]\n\t        seq_ptr = int(seq_ptr.lstrip('0')) if seq_ptr != '0000' else 0\n\t        # Building ID\n\t        bldg_id = ts_idx[3].lstrip('0')\n", "        # Select timestamp and building column\n\t        df = pq.read_table(str(self.dataset_path / self.building_type_and_year[int(ts_idx[0])]\n\t                           / 'timeseries_individual_buildings' / self.census_regions[int(ts_idx[1])]\n\t                           / 'upgrade=0' / f'puma={ts_idx[2]}'), columns=['timestamp', bldg_id])\n\t        # Order by timestamp\n\t        df = df.to_pandas().sort_values(by='timestamp')\n\t        # Slice each column from seq_ptr-context_len : seq_ptr + pred_len\n\t        time_features = self.time_transform.transform(df['timestamp'].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len ])\n\t        # Running faiss on CPU is  slower than on GPU, so we quantize loads later after data loading.\n\t        load_features = df[bldg_id].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len ].values.astype(np.float32)  # (context_len+pred_len,)\n", "        # For BoxCox transform\n\t        if self.apply_scaler_transform != '':\n\t            load_features = self.load_transform.transform(load_features)\n\t        latlon_features = self.spatial_transform.transform(ts_idx[2]).repeat(self.context_len + self.pred_len, axis=0) \n\t        # residential = 0 and commercial = 1\n\t        building_features = np.ones((self.context_len + self.pred_len,1), dtype=np.int32) * int(int(ts_idx[0]) % 2 == 0)\n\t        sample = {\n\t            'latitude': latlon_features[:, 0][...,None],\n\t            'longitude': latlon_features[:, 1][...,None],\n\t            'day_of_year': time_features[:, 0][...,None],\n", "            'day_of_week': time_features[:, 1][...,None],\n\t            'hour_of_day': time_features[:, 2][...,None],\n\t            'building_type': building_features,\n\t            'load': load_features[...,None]\n\t        }\n\t        return sample\n"]}
{"filename": "buildings_bench/data/datasets.py", "chunked_list": ["from pathlib import Path\n\timport torch\n\timport pandas as pd\n\timport numpy as np\n\tfrom typing import List, Union, Iterator, Tuple\n\timport buildings_bench.transforms as transforms\n\tfrom buildings_bench.transforms import BoxCoxTransform, StandardScalerTransform\n\tfrom buildings_bench import BuildingTypes\n\timport pyarrow.parquet as pq\n\tclass TorchBuildingDataset(torch.utils.data.Dataset):\n", "    \"\"\"PyTorch Dataset for a single building's Pandas Dataframe with a timestamp index and a 'power' column.\n\t    Used to iterate over mini-batches of 192-hour subsequences.\n\t    \"\"\"\n\t    def __init__(self, \n\t                dataframe: pd.DataFrame,\n\t                building_latlon: List[float],\n\t                building_type: BuildingTypes,\n\t                context_len: int = 168,\n\t                pred_len: int = 24,\n\t                sliding_window: int = 24,\n", "                apply_scaler_transform: str = '',\n\t                scaler_transform_path: Path = None,\n\t                is_leap_year = False):\n\t        \"\"\"\n\t        Args:\n\t            dataframe (pd.DataFrame): Pandas DataFrame with a timestamp index and a 'power' column.\n\t            building_latlon (List[float]): Latitude and longitude of the building.\n\t            building_type (BuildingTypes): Building type for the dataset.\n\t            context_len (int, optional): Length of context. Defaults to 168.\n\t            pred_len (int, optional): Length of prediction. Defaults to 24.\n", "            sliding_window (int, optional): Stride for sliding window to split timeseries into test samples. Defaults to 24.\n\t            apply_scaler_transform (str, optional): Apply scaler transform {boxcox,standard} to the load. Defaults to ''.\n\t            scaler_transform_path (Path, optional): Path to the pickled data for BoxCox transform. Defaults to None.\n\t            is_leap_year (bool, optional): Is the year a leap year? Defaults to False.\n\t        \"\"\"\n\t        self.df = dataframe        \n\t        self.building_type = building_type\n\t        self.context_len = context_len\n\t        self.pred_len = pred_len\n\t        self.sliding_window = sliding_window\n", "        self.apply_scaler_transform = apply_scaler_transform\n\t        self.normalized_latlon = transforms.LatLonTransform().transform_latlon(building_latlon)\n\t        self.time_transform = transforms.TimestampTransform(is_leap_year=is_leap_year)\n\t        if self.apply_scaler_transform == 'boxcox':\n\t            self.load_transform = BoxCoxTransform()\n\t            self.load_transform.load(scaler_transform_path)\n\t        elif self.apply_scaler_transform == 'standard':\n\t            self.load_transform = StandardScalerTransform()\n\t            self.load_transform.load(scaler_transform_path)\n\t    def __len__(self):\n", "        return (len(self.df) - self.context_len - self.pred_len) // self.sliding_window\n\t    def __getitem__(self, idx):\n\t        seq_ptr = self.context_len + self.sliding_window * idx\n\t        load_features = self.df['power'].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len].values.astype(np.float32)\n\t        if self.apply_scaler_transform != '':\n\t            load_features = self.load_transform.transform(load_features)\n\t        time_features = self.time_transform.transform(self.df.index[seq_ptr-self.context_len : seq_ptr+self.pred_len ])\n\t        latlon_features = self.normalized_latlon.reshape(1,2).repeat(self.context_len + self.pred_len, axis=0).astype(np.float32) \n\t        if self.building_type == BuildingTypes.RESIDENTIAL:\n\t            building_features = BuildingTypes.RESIDENTIAL_INT * np.ones((self.context_len + self.pred_len,1), dtype=np.int32)\n", "        elif self.building_type == BuildingTypes.COMMERCIAL:\n\t            building_features = BuildingTypes.COMMERCIAL_INT * np.ones((self.context_len + self.pred_len,1), dtype=np.int32)\n\t        sample = {\n\t            'latitude': latlon_features[:, 0][...,None],\n\t            'longitude': latlon_features[:, 1][...,None],\n\t            'day_of_year': time_features[:, 0][...,None],\n\t            'day_of_week': time_features[:, 1][...,None],\n\t            'hour_of_day': time_features[:, 2][...,None],\n\t            'building_type': building_features,\n\t            'load': load_features[...,None]\n", "        }\n\t        return sample\n\tclass TorchBuildingDatasetFromParquet:\n\t    \"\"\"Generate PyTorch Datasets out of Parquet files.\n\t    Each file has multiple buildings (with same Lat/Lon and building type) and\n\t    each building is a column. All time series are for the same year.     \n\t    Attributes:\n\t        building_datasets (dict): Maps unique building ids to a TorchBuildingDataset.   \n\t    \"\"\"\n\t    def __init__(self,\n", "                parquet_datasets: List[str],\n\t                building_latlons: List[List[float]],\n\t                building_types: List[BuildingTypes],\n\t                context_len: int = 168,\n\t                pred_len: int = 24,\n\t                sliding_window: int = 24,\n\t                apply_scaler_transform: str = '',\n\t                scaler_transform_path: Path = None,\n\t                leap_years: List[int] = None):\n\t        \"\"\"\n", "        Args:\n\t            parquet_datasets (List[str]): List of paths to a parquet file, each has a timestamp index and multiple columns, one per building.\n\t            building_latlons (List[List[float]]): List of latlons for each parquet file.\n\t            building_types (List[BuildingTypes]): List of building types for each parquet file.\n\t            context_len (int, optional): Length of context. Defaults to 168.\n\t            pred_len (int, optional): Length of prediction. Defaults to 24.\n\t            sliding_window (int, optional): Stride for sliding window to split timeseries into test samples. Defaults to 24.\n\t            apply_scaler_transform (str, optional): Apply scaler transform {boxcox,standard} to the load. Defaults to ''.\n\t            scaler_transform_path (Path, optional): Path to the pickled data for BoxCox transform. Defaults to None.\n\t            leap_years (List[int], optional): List of leap years. Defaults to None.\n", "        \"\"\"\n\t        self.building_datasets = {}\n\t        for parquet_data, building_latlon, building_type in zip(parquet_datasets, building_latlons, building_types):\n\t            df = pq.read_table(parquet_data)\n\t            # Order by timestamp\n\t            df = df.to_pandas().sort_values(by='timestamp')\n\t            # Set timestamp as the index\n\t            df.set_index('timestamp', inplace=True)\n\t            df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n\t            # split into multiple dataframes by column, keeping the index\n", "            dfs = np.split(df, df.shape[1], axis=1)\n\t            # for each column in the multi_building_dataset, create a BuildingYearDatasetFromCSV\n\t            for building_dataframe in dfs:\n\t                building_name = building_dataframe.columns[0]\n\t                year = building_dataframe.index[0].year\n\t                is_leap_year = True if year in leap_years else False\n\t                # remove the year for aggregating over all years, later\n\t                b_file = f'{building_type}_{Path(parquet_data).stem}/{building_name}'\n\t                #by_file = f'{building_type}_{Path(parquet_data).stem}/{building_name}_year={year}'\n\t                # rename column 1 to power\n", "                building_dataframe.rename(columns={building_dataframe.columns[0]: 'power'}, inplace=True)\n\t                self.building_datasets[b_file] = TorchBuildingDataset(building_dataframe, \n\t                                                                    building_latlon, \n\t                                                                    building_type, \n\t                                                                    context_len, \n\t                                                                    pred_len, \n\t                                                                    sliding_window,\n\t                                                                    apply_scaler_transform,\n\t                                                                    scaler_transform_path,\n\t                                                                    is_leap_year)\n", "    def __iter__(self) -> Iterator[Tuple[str, TorchBuildingDataset]]:\n\t        \"\"\"Generator to iterate over the building datasets.\n\t        Yields:\n\t            A pair of building id, TorchBuildingDataset objects. \n\t        \"\"\"\n\t        for building_name, building_dataset in self.building_datasets.items():\n\t            yield (building_name, building_dataset)\n\tclass TorchBuildingDatasetsFromCSV:\n\t    \"\"\"TorchBuildingDatasetsFromCSV\n\t    Generate PyTorch Datasets from a list of CSV files.\n", "    Attributes:\n\t        building_datasets (dict): Maps unique building ids to a list of tuples (year, TorchBuildingDataset). \n\t    \"\"\"\n\t    def __init__(self,\n\t                data_path: Path,\n\t                building_year_files: List[str],\n\t                building_latlon: List[float],\n\t                building_type: BuildingTypes,\n\t                context_len: int = 168,\n\t                pred_len: int = 24,\n", "                sliding_window: int = 24,\n\t                apply_scaler_transform: str = '',\n\t                scaler_transform_path: Path = None,\n\t                leap_years: List[int] = None):\n\t        \"\"\"\n\t        Args:\n\t            data_path (Path): Path to the dataset\n\t            building_year_files (List[str]): List of paths to a csv file, each has a timestamp index and multiple columns, one per building.\n\t            building_type (BuildingTypes): Building type for the dataset.\n\t            context_len (int, optional): Length of context. Defaults to 168.\n", "            pred_len (int, optional): Length of prediction sequence for the forecasting model. Defaults to 24.\n\t            sliding_window (int, optional): Stride for sliding window to split timeseries into test samples. Defaults to 24.\n\t            apply_scaler_transform (str, optional): Apply scaler transform {boxcox,standard} to the load. Defaults to ''.\n\t            scaler_transform_path (Path, optional): Path to the pickled data for BoxCox transform. Defaults to None.\n\t            leap_years (List[int], optional): List of leap years. Defaults to None.\n\t        \"\"\"\n\t        self.building_datasets = {}\n\t        self.building_type = building_type\n\t        for building_year_file in building_year_files:\n\t            name = building_year_file.split('_')[0].split('/')[1]\n", "            year = int(building_year_file.split('=')[1])\n\t            is_leap_year = True if year in leap_years else False\n\t            df = pd.read_csv(data_path / (building_year_file + '.csv'),\n\t                                                index_col=0, header=0, parse_dates=True)\n\t            df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n\t            df = df.sort_index()\n\t            if len(df.columns) > 1:\n\t                bldg_names = df.columns\n\t                # split into multiple dataframes by column, keeping the index\n\t                dfs = np.split(df, df.shape[1], axis=1)\n", "            else:\n\t                bldg_names = [name]\n\t                dfs = [df]\n\t            # for each bldg, create a TorchBuildingDatasetFromCSV\n\t            for bldg_name, bldg_df in zip(bldg_names, dfs):\n\t                bldg_df.rename(columns={bldg_df.columns[0]: 'power'}, inplace=True)\n\t                if not bldg_name in self.building_datasets:\n\t                    self.building_datasets[bldg_name] = []\n\t                self.building_datasets[bldg_name] += [(year, TorchBuildingDataset(bldg_df, \n\t                                                                                building_latlon, \n", "                                                                                building_type, \n\t                                                                                context_len, \n\t                                                                                pred_len, \n\t                                                                                sliding_window,\n\t                                                                                apply_scaler_transform,\n\t                                                                                scaler_transform_path,\n\t                                                                                is_leap_year))]\n\t    def __iter__(self) -> Iterator[Tuple[str, torch.utils.data.ConcatDataset]]:\n\t        \"\"\"A Generator for TorchBuildingDataset objects.\n\t        Yields:\n", "            A tuple of the building id and a ConcatDataset of the TorchBuildingDataset objects for all years.    \n\t        \"\"\"\n\t        for building_name, building_year_datasets in self.building_datasets.items():\n\t            building_year_datasets = sorted(building_year_datasets, key=lambda x: x[0])\n\t            building_dataset = torch.utils.data.ConcatDataset([\n\t                byd[1] for byd in building_year_datasets])\n\t            yield (building_name, building_dataset)\n\tclass PandasBuildingDatasetsFromCSV:\n\t    \"\"\"Generate Pandas Dataframes from a list of CSV files.\n\t    Create a dictionary of building datasets from a list of csv files.\n", "    Used as a generator to iterate over Pandas Dataframes for each building.\n\t    The Pandas Dataframe contain all of the years of data for the building.\n\t    Attributes:\n\t        building_datasets (dict): Maps unique building ids to a list of tuples (year, Dataframe).    \n\t    \"\"\"\n\t    def __init__(self, \n\t                data_path: Path,\n\t                building_year_files: List[str],\n\t                building_latlon: List[float],\n\t                building_type: BuildingTypes,\n", "                features: str = 'transformer',\n\t                apply_scaler_transform: str = '',\n\t                scaler_transform_path: Path = None,\n\t                leap_years: List[int] = []):\n\t        \"\"\"        \n\t        Args:\n\t            data_path (Path): Path to the dataset\n\t            building_year_files (List[str]): List of paths to a csv file, each has a timestamp index and multiple columns, one per building.\n\t            building_type (BuildingTypes): Building type for the dataset.\n\t            features (str, optional): Type of features to use. Defaults to 'transformer'. {'transformer','engineered'}\n", "                'transformer' features: load, latitude, longitude, hour of day, day of week, day of year, building type\n\t                'engineered' features are an expansive list of mainly calendar-based features, useful for traditional ML models.\n\t            apply_scaler_transform (str, optional): Apply scaler transform {boxcox,standard} to the load. Defaults to ''.\n\t            scaler_transform_path (Path, optional): Path to the pickled data for BoxCox transform. Defaults to None.\n\t            leap_years (List[int], optional): List of leap years. Defaults to None.\n\t        \"\"\"\n\t        self.building_type = building_type\n\t        self.features = features \n\t        self.apply_scaler_transform = apply_scaler_transform\n\t        self.leap_years = leap_years\n", "        if self.features == 'transformer':\n\t            self.normalized_latlon = transforms.LatLonTransform().transform_latlon(building_latlon)\n\t            if self.apply_scaler_transform == 'boxcox':\n\t                self.load_transform = BoxCoxTransform()\n\t                self.load_transform.load(scaler_transform_path)\n\t            elif self.apply_scaler_transform == 'standard':\n\t                self.load_transform = StandardScalerTransform()\n\t                self.load_transform.load(scaler_transform_path)\n\t        self.building_datasets = {}\n\t        for building_year_file in building_year_files:\n", "            #fullname = building_year_file.split('_')[0]\n\t            name = building_year_file.split('_')[0].split('/')[1]\n\t            year = int(building_year_file.split('=')[1])\n\t            # load the csv file\n\t            df = pd.read_csv(data_path / (building_year_file + '.csv'),\n\t                             index_col=0, header=0, parse_dates=True)\n\t            df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n\t            df = df.asfreq('H')\n\t            df = df.sort_index()\n\t            bldg_dfs =[]\n", "            # is multi-building file? \n\t            if len(df.columns) > 1:\n\t                bldg_names = df.columns\n\t                # split into multiple dataframes by column, keeping the index\n\t                bldg_dfs = np.split(df, df.shape[1], axis=1)\n\t            else:\n\t                bldg_names = [name]\n\t                bldg_dfs = [df]\n\t            for bldg_name,df in zip(bldg_names, bldg_dfs):\n\t                if self.features == 'engineered':\n", "                    self._prepare_data_with_engineered_features(bldg_name, df, year)\n\t                elif self.features == 'transformer':\n\t                    self._prepare_data_transformer(bldg_name, df, year)\n\t    def _prepare_data_with_engineered_features(self, bldg_name, df, year):\n\t        # rename column 1 to power\n\t        df.rename(columns={df.columns[0]: 'power'}, inplace=True)\n\t        # Create hour_of_day,.., etc columns\n\t        df[\"hour_x\"] = np.sin(np.radians((360/24) * df.index.hour))\n\t        df[\"hour_y\"] = np.cos(np.radians((360/24) * df.index.hour))\n\t        df[\"month_x\"] = np.sin(np.radians((360/12) * df.index.month))\n", "        df[\"month_y\"] = np.cos(np.radians((360/12) * df.index.month))\n\t        # add calendar-based variables as categorical data\n\t        # see https://colab.research.google.com/drive/1ZWpJY03xLIsUrlOzgTNHemKyLatMgKrp?usp=sharing#scrollTo=NJABd7ow5EHC\n\t        df[\"day_of_week\"] = df.index.weekday\n\t        df[\"hour_of_day\"] = df.index.hour\n\t        df[\"month_of_year\"] = df.index.month\n\t        df[\"weekend\"] = df.index.weekday.isin([5,6])\n\t        df= pd.get_dummies(df, columns=[\"day_of_week\", \"hour_of_day\", \"month_of_year\", \"weekend\"])\n\t        if bldg_name in self.building_datasets:\n\t            self.building_datasets[bldg_name] += [(year,df)]\n", "        else:\n\t            self.building_datasets[bldg_name] = [(year,df)]\n\t    def _prepare_data_transformer(self, bldg_name, df, year):\n\t        is_leap_year = True if year in self.leap_years else False            \n\t        time_transform = transforms.TimestampTransform(is_leap_year)\n\t        df.rename(columns={df.columns[0]: 'load'}, inplace=True)\n\t        if self.apply_scaler_transform != '':\n\t            df['load'] = self.load_transform.transform(df['load'].values)\n\t        # create a column called \"latitude\" and \"longitude\" with the normalized lat/lon\n\t        # of the same shape as 'load'\n", "        df[\"latitude\"] = self.normalized_latlon[0] * np.ones(df.shape[0])\n\t        df[\"longitude\"] = self.normalized_latlon[1] * np.ones(df.shape[0])\n\t        if self.building_type == BuildingTypes.RESIDENTIAL:\n\t            df[\"building_type\"] = BuildingTypes.RESIDENTIAL_INT * np.ones(df.shape[0])\n\t        elif self.building_type == BuildingTypes.COMMERCIAL:\n\t            df[\"building_type\"] = BuildingTypes.COMMERCIAL_INT * np.ones(df.shape[0])\n\t        time_features = time_transform.transform(df.index)\n\t        df[\"day_of_week\"] = time_features[:,1] * np.ones(df.shape[0])\n\t        df[\"hour_of_day\"] = time_features[:,2] * np.ones(df.shape[0])\n\t        df[\"day_of_year\"] = time_features[:, 0] * np.ones(df.shape[0])\n", "        if bldg_name in self.building_datasets:\n\t            self.building_datasets[bldg_name] += [(year,df)]\n\t        else:\n\t            self.building_datasets[bldg_name] = [(year,df)]\n\t    def __iter__(self) -> Iterator[Tuple[str, pd.DataFrame]]:\n\t        \"\"\"Generator for iterating over the dataset.\n\t        Yields:\n\t            A pair of building id and Pandas dataframe. \n\t                The dataframe has all years concatenated.    \n\t        \"\"\"\n", "        for building_id, building_dataset in self.building_datasets.items():\n\t            building_dataset = sorted(building_dataset, key=lambda x: x[0])\n\t            df = pd.concat([df[1] for df in building_dataset])\n\t            # fill missing values with 0\n\t            df = df.fillna(0) \n\t            yield (building_id, df)\n\tclass PandasTransformerDataset(torch.utils.data.Dataset):\n\t    \"\"\"Create a Torch Dataset out of a Pandas DataFrame.\n\t    Used to iterate over mini-batches of 192-hour sub-sequences.\n\t    \"\"\"\n", "    def __init__(self, \n\t                 df: pd.DataFrame,\n\t                 context_len: int = 168,\n\t                 pred_len: int = 24,\n\t                 sliding_window: int = 24):\n\t        \"\"\"\n\t        Args:\n\t            df (pd.DataFrame): Pandas DataFrame with columns: load, latitude, longitude, hour of day, day of week, day of year, building type\n\t            context_len (int, optional): Length of context.. Defaults to 168.\n\t            pred_len (int, optional): Length of prediction sequence for the forecasting model. Defaults to 24.\n", "            sliding_window (int, optional): Stride for sliding window to split timeseries into test samples. Defaults to 24.\n\t        \"\"\"\n\t        self.df = df\n\t        self.context_len = context_len\n\t        self.pred_len = pred_len\n\t        self.sliding_window = sliding_window\n\t    def __len__(self):\n\t        return (len(self.df) - self.context_len - self.pred_len) // self.sliding_window\n\t    def __getitem__(self, idx):\n\t        seq_ptr = self.context_len + self.sliding_window * idx\n", "        load_features = self.df['load'].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len].values.astype(np.float32)\n\t        building_features = self.df['building_type'].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len].values.astype(np.int32)\n\t        latlon_features = self.df[['latitude', 'longitude']].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len].values.astype(np.float32)\n\t        time_features = self.df[['day_of_year', 'day_of_week', 'hour_of_day']].iloc[seq_ptr-self.context_len : seq_ptr+self.pred_len].values.astype(np.float32)\n\t        sample = {\n\t            'latitude': latlon_features[:, 0][...,None],\n\t            'longitude': latlon_features[:, 1][...,None],\n\t            'day_of_year': time_features[:, 0][...,None],\n\t            'day_of_week': time_features[:, 1][...,None],\n\t            'hour_of_day': time_features[:, 2][...,None],\n", "            'building_type': building_features[...,None],\n\t            'load': load_features[...,None]\n\t        }\n\t        return sample\n\tdef keep_buildings(dataset_generator: Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet],\n\t                     building_ids: List[str]) -> Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]:\n\t    \"\"\"Remove all buildings *not* listed in building_ids from the building_datasets dictionary from the generator class.\n\t    Args:\n\t        dataset_generator (Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]): Dataset generator class.\n\t        building_ids (List[str]): List of building ids to keep.\n", "    Returns:\n\t        dataset_generator (Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]): Dataset generator \n\t            class with only the buildings listed in building_ids.\n\t    \"\"\"\n\t    for building_id in list(dataset_generator.building_datasets.keys()):\n\t        if building_id not in building_ids:\n\t            del dataset_generator.building_datasets[building_id]\n\t    return dataset_generator\n"]}
{"filename": "buildings_bench/models/linear_regression.py", "chunked_list": ["import torch\n\tfrom buildings_bench.models.base_model import BaseModel\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport numpy as np\n\tclass LinearRegression(BaseModel):\n\t    \"\"\"Linear regression model that does direct forecasting.\n\t    It has one weight W and one bias b. The output is computed as\n\t    y = Wx + b, where W is a matrix of shape [pred_len, context_len].\n\t    \"\"\"\n", "    def __init__(self, context_len=168, pred_len=24, continuous_loads=True):\n\t        super(LinearRegression, self).__init__(context_len, pred_len, continuous_loads)\n\t        self.Linear = nn.Linear(context_len, pred_len)\n\t    def forward(self, x):\n\t        x = x['load'][:,:self.context_len,:]\n\t        # src_series: [Batch, Input length, 1]\n\t        x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n\t        return x # [Batch, Output length, 1]\n\t    def loss(self, x, y):\n\t        return torch.nn.functional.mse_loss(x, y)\n", "    def predict(self, x):\n\t        out = self.forward(x)\n\t        return out, None\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n\t        for p in self.parameters():\n\t            p.requires_grad = True        \n\t        return self.parameters()\n\t    def load_from_checkpoint(self, checkpoint_path):\n\t        return None\n"]}
{"filename": "buildings_bench/models/persistence.py", "chunked_list": ["import torch\n\tfrom buildings_bench.models.base_model import BaseModel\n\tclass AveragePersistence(BaseModel):\n\t    \"\"\"Predict each hour as the average over each previous day.\n\t    \"\"\"\n\t    def __init__(self, context_len=168, pred_len=24, continuous_loads=True):\n\t        super().__init__(context_len, pred_len, continuous_loads)\n\t    def forward(self, x):\n\t        # [bsz x seqlen x 1]\n\t        src_series = x['load'][:,:self.context_len,:]\n", "        preds = []\n\t        stds = []\n\t        for i in range(self.pred_len):\n\t            preds += [ torch.mean(src_series[:,i::24,:], dim=1) ]\n\t            stds += [ torch.clamp(\n\t                torch.std(src_series[:,i::24,:], dim=1),\n\t                min=1e-3)]\n\t        return torch.cat([torch.stack(preds, 1), torch.stack(stds, 1)],2)\n\t    def loss(self, x, y):\n\t        return x, y\n", "    def predict(self, x):\n\t        out = self.forward(x)\n\t        return out[:,:,0].unsqueeze(-1), out\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n\t        return None\n\t    def load_from_checkpoint(self, checkpoint_path):\n\t        return None\n\tclass CopyLastDayPersistence(BaseModel):\n\t    \"\"\"Predict each hour as the same hour from the previous day.\n\t    \"\"\"\n", "    def __init__(self, context_len=168, pred_len=24, continuous_loads=True):\n\t        super().__init__(context_len, pred_len, continuous_loads)\n\t        assert self.context_len >= 24\n\t        assert self.pred_len >= 24\n\t    def forward(self, x):\n\t        # [bsz x seqlen x 1]\n\t        src_series = x['load'][:,:self.context_len,:]\n\t        return src_series[:, self.context_len-24:]\n\t    def loss(self, x, y):\n\t        return x, y\n", "    def predict(self, x):\n\t        return self.forward(x), None\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n\t        return None\n\t    def load_from_checkpoint(self, checkpoint_path):\n\t        return None\n\tclass CopyLastWeekPersistence(BaseModel):\n\t    \"\"\"Predict each hour as the same hour from the previous week.\n\t    \"\"\"\n\t    def __init__(self, context_len=168, pred_len=24, continuous_loads=True):\n", "        super().__init__(context_len, pred_len, continuous_loads)\n\t        assert self.context_len >= 168\n\t        assert self.pred_len >= 24\n\t    def forward(self, x):\n\t        # [bsz x seqlen x 1]\n\t        src_series = x['load'][:,:self.context_len,:]\n\t        return src_series[:, self.context_len-168 : self.context_len - 168 +24]\n\t    def loss(self, x, y):\n\t        return x, y\n\t    def predict(self, x):\n", "        return self.forward(x), None\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n\t        return None\n\t    def load_from_checkpoint(self, checkpoint_path):\n\t        return None\n"]}
{"filename": "buildings_bench/models/__init__.py", "chunked_list": ["# buildings_bench.models\n\timport torch\n\tfrom typing import Callable, Tuple, Dict\n\t# Import models here\n\tfrom buildings_bench.models.dlinear_regression import DLinearRegression\n\tfrom buildings_bench.models.linear_regression import LinearRegression\n\tfrom buildings_bench.models.transformers import LoadForecastingTransformer\n\tfrom buildings_bench.models.persistence import *\n\tmodel_registry = {\n\t    'TransformerWithTokenizer-L': LoadForecastingTransformer,\n", "    'TransformerWithTokenizer-M': LoadForecastingTransformer,\n\t    'TransformerWithTokenizer-S': LoadForecastingTransformer,\n\t    'TransformerWithTokenizer-L-ignore-spatial': LoadForecastingTransformer,\n\t    'TransformerWithTokenizer-L-8192': LoadForecastingTransformer,\n\t    'TransformerWithTokenizer-L-344': LoadForecastingTransformer,\n\t    'TransformerWithMSE': LoadForecastingTransformer,\n\t    'TransformerWithGaussian-L': LoadForecastingTransformer,\n\t    'TransformerWithGaussian-M': LoadForecastingTransformer,\n\t    'TransformerWithGaussian-S': LoadForecastingTransformer,\n\t    'AveragePersistence': AveragePersistence,\n", "    'CopyLastDayPersistence': CopyLastDayPersistence,\n\t    'CopyLastWeekPersistence': CopyLastWeekPersistence,\n\t    'LinearRegression': LinearRegression,\n\t    'DLinearRegression': DLinearRegression,\n\t    # Register your model here\n\t}\n\tdef model_factory(model_name: str, model_args: Dict) -> Tuple[torch.nn.Module, Callable, Callable]:\n\t    \"\"\"Instantiate and returns a model for the benchmark.\n\t    Returns the model itself,\n\t    the loss function to use, and the predict function.\n", "    The predict function should return a tuple of two tensors: \n\t    (point predictions, prediction distribution parameters) where\n\t    the distribution parameters may be, e.g., logits, or mean and variance.\n\t    Args:\n\t        model_name (str): Name of the model.\n\t        model_args (Dict): The keyword arguments for the model.\n\t    Returns:\n\t        model (torch.nn.Module): the instantiated model  \n\t        loss (Callable): loss function\n\t        predict (Callable): predict function\n", "    \"\"\"\n\t    assert model_name in model_registry.keys(), \\\n\t        f\"Model {model_name} not in registry: {model_registry.keys()}\"\n\t    model = model_registry[model_name](**model_args)\n\t    loss = model.loss\n\t    predict = model.predict\n\t    return model, loss, predict\n"]}
{"filename": "buildings_bench/models/base_model.py", "chunked_list": ["import abc\n\tfrom typing import Tuple, Dict, Union\n\tfrom pathlib import Path\n\timport torch\n\timport torch.nn as nn\n\tclass BaseModel(nn.Module, metaclass=abc.ABCMeta):\n\t    \"\"\"Base class for all models.\"\"\"\n\t    def __init__(self, context_len, pred_len, continuous_loads):\n\t        \"\"\"Init method for BaseModel.\n\t        Args:\n", "            context_len (int): length of context window\n\t            pred_len (int): length of prediction window\n\t            continuous_loads (bool): whether to use continuous load values\n\t        \"\"\"\n\t        super().__init__()\n\t        self.context_len = context_len\n\t        self.pred_len = pred_len\n\t        self.continuous_loads = continuous_loads\n\t    @abc.abstractmethod\n\t    def forward(self, x: Dict) -> Tuple[torch.Tensor, torch.Tensor]:\n", "        \"\"\"Forward pass. \n\t        Expected keys in x:\n\t            - 'load': torch.Tensor of shape (batch_size, seq_len, 1)\n\t            - 'building_type': torch.LongTensor of shape (batch_size, seq_len, 1)\n\t            - 'day_of_year': torch.FloatTensor of shape (batch_size, seq_len, 1)\n\t            - 'hour_of_day': torch.FloatTensor of shape (batch_size, seq_len, 1)\n\t            - 'day_of_week': torch.FloatTensor of shape (batch_size, seq_len, 1)\n\t            - 'latitude': torch.FloatTensor of shape (batch_size, seq_len, 1)\n\t            - 'longitude': torch.FloatTensor of shape (batch_size, seq_len, 1)\n\t        Args:\n", "            x (Dict): dictionary of input tensors\n\t        Returns:\n\t            predictions, distribution parameters (Tuple[torch.Tensor, torch.Tensor]): outputs\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    @abc.abstractmethod\n\t    def loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"A function for computing the loss.\n\t        Args:\n\t            x (torch.Tensor): preds of shape (batch_size, seq_len, 1)\n", "            y (torch.Tensor): targets of shape (batch_size, seq_len, 1)\n\t        Returns:\n\t            loss (torch.Tensor): scalar loss\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    @abc.abstractmethod \n\t    def predict(self, x: Dict) -> Tuple[torch.Tensor, torch.Tensor]:\n\t        \"\"\"A function for making a forecast on x with the model.\n\t        Args:\n\t            x (Dict): dictionary of input tensors\n", "        Returns:\n\t            predictions (torch.Tensor): of shape (batch_size, pred_len, 1)\n\t            distribution_parameters (torch.Tensor): of shape (batch_size, pred_len, -1)\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    @abc.abstractmethod\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n\t        \"\"\"For transfer learning. \n\t        - Set requires_grad=True for parameters being fine-tuned (if necessary)\n\t        - Return the parameters that should be fine-tuned.\n", "        \"\"\"\n\t        raise NotImplementedError()\n\t    @abc.abstractmethod\n\t    def load_from_checkpoint(self, checkpoint_path: Union[str, Path]):\n\t        \"\"\"Describes how to load the model from checkpoint_path.\"\"\"\n\t        raise NotImplementedError()\n"]}
{"filename": "buildings_bench/models/transformers.py", "chunked_list": ["import math\n\timport torch\n\tfrom typing import Tuple, Dict\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom torch.nn import Transformer\n\timport numpy as np\n\tfrom buildings_bench.models.base_model import BaseModel\n\tclass TokenEmbedding(nn.Module):\n\t    \"\"\"Helper Module to convert tensor of input\n", "       indices into corresponding tensor of token embeddings.\n\t    \"\"\"\n\t    def __init__(self, vocab_size: int, emb_size: int):\n\t        \"\"\"\n\t        Args:\n\t            vocab_size (int): number of quantized load values in the entire vocabulary.\n\t            emb_size (int): embedding size.\n\t        \"\"\"\n\t        super(TokenEmbedding, self).__init__()\n\t        self.embedding = nn.Embedding(vocab_size, emb_size)\n", "        self.emb_size = emb_size\n\t    def forward(self, tokens: torch.Tensor):\n\t        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\tclass PositionalEncoding(nn.Module):\n\t    \"\"\"Helper Module that adds positional encoding to the token embedding to\n\t       introduce a notion of order within a time-series.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 emb_size: int,\n\t                 dropout: float,\n", "                 maxlen: int = 500):\n\t        \"\"\"\n\t        Args:\n\t            emb_size (int): embedding size.\n\t            dropout (float): dropout rate.\n\t            maxlen (int): maximum possible length of the incoming time series.\n\t        \"\"\"\n\t        super(PositionalEncoding, self).__init__()\n\t        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n\t        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n", "        pos_embedding = torch.zeros((maxlen, emb_size))\n\t        pos_embedding[:, 0::2] = torch.sin(pos * den)\n\t        pos_embedding[:, 1::2] = torch.cos(pos * den)\n\t        pos_embedding = pos_embedding.unsqueeze(-2)\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.register_buffer('pos_embedding', pos_embedding)\n\t    def forward(self, token_embedding: torch.Tensor) -> torch.Tensor:\n\t        # batch first - use size(1)\n\t        # need to permute token embeddings from [batch_size, seqlen x emb_size] to [seqlen x batch_size, emb_size]\n\t        return self.dropout(token_embedding.permute(1,0,2) + self.pos_embedding[:token_embedding.size(1), :]).permute(1,0,2)\n", "class TimeSeriesSinusoidalPeriodicEmbedding(nn.Module):\n\t    \"\"\"This module produces a sinusoidal periodic embedding for a sequence of values in [-1, +1].\"\"\"\n\t    def __init__(self, embedding_dim: int) -> None:\n\t        \"\"\"\n\t        Args:\n\t            embedding_dim (int): embedding size.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.linear = nn.Linear(2, embedding_dim)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n", "        \"\"\"`x` is expected to be [batch_size, seqlen, 1].\"\"\"\n\t        with torch.no_grad():\n\t            x = torch.cat([torch.sin(np.pi * x), torch.cos(np.pi * x)], dim=2)\n\t        # [batch_size, seqlen x 2] --> [batch_size, seqlen, embedding_dim]\n\t        return self.linear(x)\n\tclass ZeroEmbedding(nn.Module):\n\t    \"\"\" Outputs zeros of the desired output dim.\"\"\"\n\t    def __init__(self, embedding_dim: int): \n\t        \"\"\"\n\t        Args:\n", "            embedding_dim (int): embedding size.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.embedding_dim = embedding_dim\n\t        self.zeros_embedding = nn.Parameter(\n\t            torch.zeros(1, 1, embedding_dim), requires_grad=False)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        \"\"\" `x` is expected to be [batch_size, seqlen, 1].\"\"\"\n\t        return self.zeros_embedding.repeat(x.shape[0], x.shape[1], 1)\n\tclass LoadForecastingTransformer(BaseModel):\n", "    \"\"\"\n\t    An encoder-decoder time series Transformer. Based on PyTorch nn.Transformer.\n\t    - Uses masking in the decoder to prevent the model from peeking into the future\n\t    - Uses N(0, 0.02) for weight initialization\n\t    - Trains with teacher forcing (i.e. the target is used as the input to the decoder)\n\t    - continuous_loads (True) just predict target values\n\t                     (False) categorical over quantized load values\n\t    \"\"\"\n\t    def __init__(self,\n\t                 context_len: int = 168,\n", "                 pred_len: int = 24,\n\t                 vocab_size = 2274,\n\t                 num_encoder_layers: int = 3,\n\t                 num_decoder_layers: int = 3,\n\t                 d_model: int = 256,\n\t                 nhead: int = 8,\n\t                 dim_feedforward: int = 256,\n\t                 dropout: float = 0.0,\n\t                 activation: str = 'gelu',\n\t                 continuous_loads = False,\n", "                 continuous_head = 'mse',\n\t                 ignore_spatial = False):\n\t        \"\"\"\n\t        Args:\n\t            context_len (int): length of the input sequence.\n\t            pred_len (int): length of the output sequence.\n\t            vocab_size (int): number of quantized load values in the entire vocabulary.\n\t            num_encoder_layers (int): number of encoder layers.\n\t            num_decoder_layers (int): number of decoder layers.\n\t            d_model (int): number of expected features in the encoder/decoder inputs.\n", "            nhead (int): number of heads in the multi-head attention models.\n\t            dim_feedforward (int): dimension of the feedforward network model.\n\t            dropout (float): dropout value.\n\t            activation (str): the activation function of encoder/decoder intermediate layer, relu or gelu.\n\t            continuous_loads (bool): whether inputs are continuous/to train the model to predict continuous values.\n\t            continuous_head (str): 'mse' or 'gaussian_nll'.\n\t            ignore_spatial (bool): whether to ignore the spatial features.\n\t        \"\"\"\n\t        super().__init__(context_len, pred_len, continuous_loads)\n\t        self.continuous_head = continuous_head\n", "        self.vocab_size = vocab_size\n\t        self.ignore_spatial = ignore_spatial\n\t        s = d_model // 256\n\t        self.transformer = Transformer(d_model=d_model,\n\t                                       nhead=nhead,\n\t                                       num_encoder_layers=num_encoder_layers,\n\t                                       num_decoder_layers=num_decoder_layers,\n\t                                       dim_feedforward=dim_feedforward,\n\t                                       dropout=dropout,\n\t                                       activation=activation,\n", "                                       batch_first=True)\n\t        if self.continuous_loads:\n\t            out_dim = 1 if self.continuous_head == 'mse' else 2\n\t            self.logits = nn.Linear(d_model, out_dim)\n\t            self.power_embedding = nn.Linear(1, 64 * s)\n\t        else:\n\t            self.logits = nn.Linear(d_model, self.vocab_size)\n\t            self.power_embedding = TokenEmbedding(self.vocab_size, 64 * s)\n\t        self.tgt_mask = self.transformer.generate_square_subsequent_mask(self.pred_len)\n\t        self.positional_encoding = PositionalEncoding(\n", "            d_model, dropout=dropout)\n\t        self.building_embedding = nn.Embedding(2, 32 * s)\n\t        self.lat_embedding = nn.Linear(1, 32 * s)\n\t        self.lon_embedding = nn.Linear(1, 32 * s)\n\t        if self.ignore_spatial:\n\t            self.lat_embedding = ZeroEmbedding(32 * s)\n\t            self.lon_embedding = ZeroEmbedding(32 * s)\n\t        self.day_of_year_encoding = TimeSeriesSinusoidalPeriodicEmbedding(32 * s) \n\t        self.day_of_week_encoding = TimeSeriesSinusoidalPeriodicEmbedding(32 * s)\n\t        self.hour_of_day_encoding = TimeSeriesSinusoidalPeriodicEmbedding(32 * s)\n", "    def to(self, device):\n\t        self.tgt_mask = self.tgt_mask.to(device)\n\t        return super().to(device)\n\t    def forward(self, x):\n\t        r\"\"\"Forward pass of the time series transformer. \n\t        Args:\n\t            x (Dict): dictionary of input tensors.\n\t        Returns:\n\t            logits (torch.Tensor): [batch_size, pred_len, vocab_size] if not continuous_loads,\n\t                                   [batch_size, pred_len, 1] if continuous_loads and continuous_head == 'mse', \n", "                                   [batch_size, pred_len, 2] if continuous_loads and continuous_head == 'gaussian_nll'.\n\t        \"\"\"\n\t        # [batch_size, seq_len, d_model]\n\t        time_series_embed = torch.cat([\n\t            self.lat_embedding(x['latitude']),\n\t            self.lon_embedding(x['longitude']),\n\t            self.building_embedding(x['building_type']).squeeze(2),\n\t            self.day_of_year_encoding(x['day_of_year']),\n\t            self.day_of_week_encoding(x['day_of_week']),\n\t            self.hour_of_day_encoding(x['hour_of_day']),\n", "            self.power_embedding(x['load']).squeeze(2),\n\t        ], dim=2)\n\t        # [batch_size, context_len, d_model]\n\t        src_series_inputs = time_series_embed[:, :self.context_len, :]\n\t        # [batch_size, pred_len, d_model]\n\t        # The last element of the target sequence is not used as input\n\t        # The last element of the source sequence is used as the initial decoder input\n\t        tgt_series_inputs = time_series_embed[:, self.context_len-1 : -1, :]\n\t        src_series_embed = self.positional_encoding(src_series_inputs)\n\t        tgt_series_embed = self.positional_encoding(tgt_series_inputs) \n", "        # The output of TransformerEncoder is the sequence from the last layer\n\t        # The shape will be [batch_size, context_len, d_model]\n\t        memory = self.transformer.encoder(src_series_embed, mask=None)\n\t        outs = self.transformer.decoder(tgt_series_embed, memory, tgt_mask=self.tgt_mask)\n\t        return self.logits(outs)\n\t    def predict(self, x: Dict) -> Tuple[torch.Tensor, torch.Tensor]:\n\t        return self.generate_sample(x, greedy=True)\n\t    def loss(self, x, y):\n\t        if self.continuous_loads and self.continuous_head == 'mse':\n\t            return F.mse_loss(x, y)\n", "        elif self.continuous_loads and self.continuous_head == 'gaussian_nll':\n\t            return F.gaussian_nll_loss(x[:, :, 0].unsqueeze(2), y,\n\t                                       F.softplus(x[:, :, 1].unsqueeze(2)) **2)\n\t        else:\n\t            return F.cross_entropy(x.reshape(-1, self.vocab_size),\n\t                                             y.long().reshape(-1))\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n\t        # for p in self.parameters():\n\t        #     p.requires_grad_(False)\n\t        # self.logits.requires_grad_(True)\n", "        # return self.logits.parameters()\n\t        return self.parameters()\n\t    def load_from_checkpoint(self, checkpoint_path):\n\t        stored_ckpt = torch.load(checkpoint_path)\n\t        model_state_dict = stored_ckpt['model']\n\t        new_state_dict = {}\n\t        for k,v in model_state_dict.items():\n\t            # remove string 'module.' from the key\n\t            if 'module.' in k:\n\t                new_state_dict[k.replace('module.', '')] = v\n", "            else:\n\t                new_state_dict[k] = v\n\t        self.load_state_dict(new_state_dict)    \n\t        print(f\"Loaded model checkpoint from {checkpoint_path}...\")\n\t    @torch.no_grad()\n\t    def generate_sample(self, \n\t                 x,\n\t                 temperature=1.0,\n\t                 greedy=False,\n\t                 num_samples=1):\n", "        \"\"\"Sample from the conditional distribution.\n\t        Use output of decoder at each prediction step as input to the next decoder step.\n\t        Implements greedy decoding and random temperature-controlled sampling.\n\t        Top-k sampling and nucleus sampling are deprecated.\n\t        Args:\n\t            x (Dict): dictionary of input tensors\n\t            temperature (float): temperature for sampling\n\t            greedy (bool): whether to use greedy decoding\n\t            num_samples (int): number of samples to generate\n\t        Returns:\n", "            predictions (torch.Tensor): of shape [batch_size, pred_len, 1] or shape [batch_size, num_samples, pred_len] if num_samples > 1.\n\t            distribution_parameters (torch.Tensor): of shape [batch_size, pred_len, 1]. Not returned if sampling.\n\t        \"\"\"\n\t        time_series_embed = torch.cat([\n\t            self.lat_embedding(x['latitude']),\n\t            self.lon_embedding(x['longitude']),\n\t            self.building_embedding(x['building_type']).squeeze(2),\n\t            self.day_of_year_encoding(x['day_of_year']),\n\t            self.day_of_week_encoding(x['day_of_week']),\n\t            self.hour_of_day_encoding(x['hour_of_day']),\n", "            self.power_embedding(x['load']).squeeze(2),\n\t        ], dim=2)\n\t        # [batch_size, context_len, d_model]\n\t        src_series_inputs = time_series_embed[:, :self.context_len, :]\n\t        tgt_series_inputs = time_series_embed[:, self.context_len-1 : -1, :]\n\t        src_series_embed = self.positional_encoding(src_series_inputs)\n\t        encoder_output = self.transformer.encoder(src_series_embed)\n\t        decoder_input = tgt_series_inputs[:, 0, :].unsqueeze(1)\n\t        if num_samples > 1 and not greedy:\n\t            # [batch_size, 1, emb_size] --> [batch_size * num_sampes, 1, emb_size]\n", "            decoder_input = decoder_input.repeat_interleave(num_samples, dim=0)\n\t            encoder_output = encoder_output.repeat_interleave(num_samples, dim=0)\n\t        all_preds, all_logits = [], []\n\t        for k in range(1, self.pred_len+1):\n\t            decoder_embed = self.positional_encoding(decoder_input)\n\t            tgt_mask = self.transformer.generate_square_subsequent_mask(k)\n\t            decoder_output = self.transformer.decoder(decoder_embed, encoder_output, tgt_mask.to(encoder_output.device))\n\t            # [batch_size, 1] if continuous (2 if head is gaussian_nll) or [batch_size, vocab_size] if not continuous_loads\n\t            outputs = self.logits(decoder_output[:, -1, :])\n\t            all_logits += [outputs.unsqueeze(1)]\n", "            if self.continuous_loads:\n\t                if self.continuous_head == 'mse':\n\t                    all_preds += [outputs] \n\t                elif self.continuous_head == 'gaussian_nll':\n\t                    if greedy:\n\t                        all_preds += [outputs[:, 0].unsqueeze(1)] # mean only\n\t                        outputs = all_preds[-1] # [batch_size, 1, 1]\n\t                    else:\n\t                        mean = outputs[:,0]\n\t                        std= torch.nn.functional.softplus(outputs[:,1])\n", "                        outputs = torch.distributions.normal.Normal(mean, std).sample().unsqueeze(1)\n\t                        all_preds += [outputs]    \n\t            elif not greedy:\n\t                # Sample from a Categorical distribution with logits outputs\n\t                all_preds += [torch.multinomial(torch.nn.functional.softmax(outputs/temperature, dim=1), 1)]\n\t                # change outputs to the predicted load tokens\n\t                outputs = all_preds[-1] # [batch_size * num_samples, 1]\n\t            else:\n\t                # outputs are [batch_size, vocab_size]\n\t                # Greedy decoding\n", "                all_preds += [outputs.argmax(dim=1).unsqueeze(1)]\n\t                # change outputs to the predicted load tokens\n\t                outputs = all_preds[-1]\n\t            # [batch_size, d_model]\n\t            if k < self.pred_len:\n\t                # [batch_size, d_model]\n\t                next_decoder_input = tgt_series_inputs[:, k]\n\t                if num_samples > 1 and not greedy:\n\t                    # [batch_size, d_model] --> [batch_size * num_samples, d_model]\n\t                    next_decoder_input = next_decoder_input.repeat_interleave(num_samples, dim=0)\n", "                # Use the embedding predicted load instead of the ground truth load\n\t                embedded_pred = self.power_embedding(outputs)  \n\t                if not self.continuous_loads:\n\t                    # [batch_size, 1, 1, 64*s] --> [batch_size, 64*s]\n\t                    embedded_pred = embedded_pred.squeeze(2).squeeze(1)\n\t                next_decoder_input = torch.cat([ next_decoder_input[:, :-embedded_pred.shape[-1]], embedded_pred ], dim=1)\n\t                # Append the next decoder input to the decoder input\n\t                decoder_input = torch.cat([decoder_input, next_decoder_input.unsqueeze(1)], dim=1)\n\t        if num_samples == 1 or greedy:\n\t            if self.continuous_head == 'gaussian_nll':\n", "                # [batch_size, pred_len, 2]\n\t                gaussian_params = torch.stack(all_logits,1)[:,:,0,:]\n\t                means = gaussian_params[:,:,0]\n\t                sigma = torch.nn.functional.softplus(gaussian_params[:,:,1])\n\t                return torch.stack(all_preds,1), torch.cat([means.unsqueeze(2), sigma.unsqueeze(2)],2)\n\t            else:\n\t                return torch.stack(all_preds,1), torch.stack(all_logits,1)[:,:,0,:]\n\t        else:\n\t            # [batch_size, num_samples, pred_len]\n\t            return torch.stack(all_preds,1).reshape(-1, num_samples, self.pred_len)"]}
{"filename": "buildings_bench/models/dlinear_regression.py", "chunked_list": ["import torch\n\tfrom buildings_bench.models.base_model import BaseModel\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport numpy as np\n\tclass moving_avg(nn.Module):\n\t    \"\"\"Moving average block to highlight the trend of time series\n\t    \"\"\"\n\t    def __init__(self, kernel_size, stride):\n\t        super(moving_avg, self).__init__()\n", "        self.kernel_size = kernel_size\n\t        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\t    def forward(self, x):\n\t        # padding on the both ends of time series\n\t        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n\t        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n\t        x = torch.cat([front, x, end], dim=1)\n\t        x = self.avg(x.permute(0, 2, 1))\n\t        x = x.permute(0, 2, 1)\n\t        return x\n", "class series_decomp(nn.Module):\n\t    \"\"\"Series decomposition block\n\t    \"\"\"\n\t    def __init__(self, kernel_size):\n\t        super(series_decomp, self).__init__()\n\t        self.moving_avg = moving_avg(kernel_size, stride=1)\n\t    def forward(self, x):\n\t        moving_mean = self.moving_avg(x)\n\t        res = x - moving_mean\n\t        return res, moving_mean\n", "class DLinearRegression(BaseModel):\n\t    \"\"\"\n\t    Decomposition-Linear\n\t    \"\"\"\n\t    def __init__(self,context_len=168, pred_len=24, continuous_loads=True):\n\t        super(DLinearRegression, self).__init__(context_len, pred_len, continuous_loads)\n\t        # Decompsition Kernel Size\n\t        kernel_size = 25\n\t        self.decompsition = series_decomp(kernel_size)\n\t        # self.individual = True\n", "        # self.channels = 1\n\t        self.Linear_Seasonal = nn.Linear(context_len, self.pred_len)\n\t        self.Linear_Trend = nn.Linear(context_len, self.pred_len)\n\t            # Use this two lines if you want to visualize the weights\n\t            # self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n\t            # self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n\t    def forward(self, x):\n\t        # x: [Batch, Input length, Channel]\n\t        src_series = x['load'][:,:self.context_len,:]\n\t        seasonal_init, trend_init = self.decompsition(src_series)\n", "        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)\n\t        seasonal_output = self.Linear_Seasonal(seasonal_init)\n\t        trend_output = self.Linear_Trend(trend_init)\n\t        x = seasonal_output + trend_output\n\t        return x.permute(0,2,1) # to [Batch, Output length, Channel]\n\t    def loss(self, x, y):\n\t        return torch.nn.functional.mse_loss(x,y)\n\t    def predict(self, x):\n\t        return self.forward(x), None\n\t    def unfreeze_and_get_parameters_for_finetuning(self):\n", "        for p in self.parameters():\n\t            p.requires_grad = True\n\t        return self.parameters()\n\t    def load_from_checkpoint(self, checkpoint_path):\n\t        return None\n"]}
