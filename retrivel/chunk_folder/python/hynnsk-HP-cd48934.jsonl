{"filename": "loss.py", "chunked_list": ["from typing import Tuple, Dict, Any\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport time\n\tdef tensor_correlation(a, b):\n\t    return torch.einsum(\"nchw,ncij->nhwij\", a, b)\n\tdef norm(t):\n\t    return F.normalize(t, dim=1, eps=1e-10)\n\tdef sample(t: torch.Tensor, coords: torch.Tensor):\n", "    return F.grid_sample(t, coords.permute(0, 2, 1, 3), padding_mode='border', align_corners=True)\n\t@torch.jit.script\n\tdef super_perm(size: int, device: torch.device):\n\t    perm = torch.randperm(size, device=device, dtype=torch.long)\n\t    perm[perm == torch.arange(size, device=device)] += 1\n\t    return perm % size\n\tclass StegoLoss(nn.Module):\n\t    def __init__(self,\n\t                 n_classes: int,\n\t                 cfg: dict,\n", "                 corr_weight: float = 1.0):\n\t        super().__init__()\n\t        self.n_classes = n_classes\n\t        self.corr_weight = corr_weight\n\t        self.corr_loss = ContrastiveCorrelationLoss(cfg)\n\t        self.linear_loss = LinearLoss(cfg)\n\t    def forward(self, model_input, model_output, model_pos_output=None, linear_output: torch.Tensor() = None,\n\t                cluster_output: torch.Tensor() = None) \\\n\t            -> Tuple[torch.Tensor, Dict[str, float]]:\n\t        img, label = model_input\n", "        # feats, code = model_output\n\t        feats = model_output[0]\n\t        code = model_output[1]\n\t        if self.corr_weight > 0:\n\t            # feats_pos, code_pos = model_pos_output\n\t            feats_pos = model_pos_output[0]\n\t            code_pos = model_pos_output[1]\n\t            corr_loss, corr_loss_dict = self.corr_loss(feats, feats_pos, code, code_pos)\n\t        else:\n\t            corr_loss_dict = {\"none\": 0}\n", "            corr_loss = torch.tensor(0, device=feats.device)\n\t        linear_loss = self.linear_loss(linear_output, label, self.n_classes)\n\t        cluster_loss = cluster_output[0]\n\t        loss = linear_loss + cluster_loss\n\t        loss_dict = {\"loss\": loss.item(), \"corr\": corr_loss.item(), \"linear\": linear_loss.item(),\n\t                     \"cluster\": cluster_loss.item()}\n\t        return loss, loss_dict, corr_loss_dict\n\tclass ContrastiveCorrelationLoss(nn.Module):\n\t    def __init__(self, cfg: dict):\n\t        super().__init__()\n", "        self.cfg = cfg\n\t    def standard_scale(self, t):\n\t        t1 = t - t.mean()\n\t        t2 = t1 / t1.std()\n\t        return t2\n\t    def helper(self, f1, f2, c1, c2, shift):\n\t        with torch.no_grad():\n\t            # Comes straight from backbone which is currently frozen. this saves mem.\n\t            fd = tensor_correlation(norm(f1), norm(f2))\n\t            if self.cfg[\"pointwise\"]:\n", "                old_mean = fd.mean()\n\t                fd -= fd.mean([3, 4], keepdim=True)\n\t                fd = fd - fd.mean() + old_mean\n\t        cd = tensor_correlation(norm(c1), norm(c2))\n\t        if self.cfg[\"zero_clamp\"]:\n\t            min_val = 0.0\n\t        else:\n\t            min_val = -9999.0\n\t        if self.cfg[\"stabilize\"]:\n\t            loss = - cd.clamp(min_val, .8) * (fd - shift)\n", "        else:\n\t            loss = - cd.clamp(min_val) * (fd - shift)\n\t        return loss, cd\n\t    def forward(self,\n\t                orig_feats: torch.Tensor,\n\t                orig_feats_pos: torch.Tensor,\n\t                orig_code: torch.Tensor,\n\t                orig_code_pos: torch.Tensor,\n\t                ):\n\t        coord_shape = [orig_feats.shape[0], self.cfg[\"feature_samples\"], self.cfg[\"feature_samples\"], 2]\n", "        coords1 = torch.rand(coord_shape, device=orig_feats.device) * 2 - 1\n\t        coords2 = torch.rand(coord_shape, device=orig_feats.device) * 2 - 1\n\t        feats = sample(orig_feats, coords1)\n\t        code = sample(orig_code, coords1)\n\t        feats_pos = sample(orig_feats_pos, coords2)\n\t        code_pos = sample(orig_code_pos, coords2)\n\t        pos_intra_loss, pos_intra_cd = self.helper(\n\t            feats, feats, code, code, self.cfg[\"corr_loss\"][\"pos_intra_shift\"])\n\t        pos_inter_loss, pos_inter_cd = self.helper(\n\t            feats, feats_pos, code, code_pos, self.cfg[\"corr_loss\"][\"pos_inter_shift\"])\n", "        neg_losses = []\n\t        neg_cds = []\n\t        for i in range(self.cfg[\"neg_samples\"]):\n\t            perm_neg = super_perm(orig_feats.shape[0], orig_feats.device)\n\t            feats_neg = sample(orig_feats[perm_neg], coords2)\n\t            code_neg = sample(orig_code[perm_neg], coords2)\n\t            neg_inter_loss, neg_inter_cd = self.helper(\n\t                feats, feats_neg, code, code_neg, self.cfg[\"corr_loss\"][\"neg_inter_shift\"])\n\t            neg_losses.append(neg_inter_loss)\n\t            neg_cds.append(neg_inter_cd)\n", "        neg_inter_loss = torch.cat(neg_losses, axis=0)\n\t        neg_inter_cd = torch.cat(neg_cds, axis=0)\n\t        return (self.cfg[\"corr_loss\"][\"pos_intra_weight\"] * pos_intra_loss.mean() +\n\t                self.cfg[\"corr_loss\"][\"pos_inter_weight\"] * pos_inter_loss.mean() +\n\t                self.cfg[\"corr_loss\"][\"neg_inter_weight\"] * neg_inter_loss.mean(),\n\t                {\"self_loss\": pos_intra_loss.mean().item(),\n\t                 \"knn_loss\": pos_inter_loss.mean().item(),\n\t                 \"rand_loss\": neg_inter_loss.mean().item()}\n\t                )\n\tclass LinearLoss(nn.Module):\n", "    def __init__(self, cfg: dict):\n\t        super(LinearLoss, self).__init__()\n\t        self.cfg = cfg\n\t        self.linear_loss = nn.CrossEntropyLoss()\n\t    def forward(self, linear_logits: torch.Tensor, label: torch.Tensor, n_classes: int):\n\t        flat_label = label.reshape(-1)\n\t        mask = (flat_label >= 0) & (flat_label < n_classes)\n\t        linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n\t        linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, n_classes)\n\t        linear_loss = self.linear_loss(linear_logits[mask], flat_label[mask]).mean()\n", "        return linear_loss\n\tclass SupConLoss(nn.Module):\n\t    def __init__(self, temperature=0.07, contrast_mode='one',\n\t                 base_temperature=0.07):\n\t        super(SupConLoss, self).__init__()\n\t        self.temperature = temperature\n\t        self.contrast_mode = contrast_mode\n\t        self.base_temperature = base_temperature\n\t    def forward(self, modeloutput_z, modeloutput_s_pr=None, modeloutput_f=None,\n\t                Pool_ag=None, Pool_sp=None, opt=None, lmbd=None, modeloutput_z_mix=None):\n", "        device = (torch.device('cuda')\n\t                  if modeloutput_z.is_cuda\n\t                  else torch.device('cpu'))\n\t        batch_size = modeloutput_z.shape[0]\n\t        spatial_size = opt[\"model\"][\"spatial_size\"]\n\t        split = int(spatial_size*spatial_size)\n\t        mini_iters = int(batch_size/split)\n\t        negative_mask_one = torch.scatter(torch.ones((split,batch_size), dtype=torch.float16), 1,\n\t                                        torch.arange(split).view(-1,1),0).to(device)\n\t        mask_neglect_base = torch.FloatTensor(split,batch_size).uniform_() < opt[\"rho\"]\n", "        mask_neglect_base = mask_neglect_base.type(torch.float16)\n\t        mask_neglect_base = mask_neglect_base.cuda()\n\t        loss = torch.tensor(0).to(device)\n\t        for mi in range(mini_iters):\n\t            modeloutput_f_one = modeloutput_f[mi*split : (mi+1)*split]\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                output_cossim_one = torch.matmul(modeloutput_f_one, modeloutput_f.transpose(0, 1))\n\t                Rpoint = torch.matmul(modeloutput_f, Pool_ag.transpose(0, 1))\n\t            Rpoint = torch.max(Rpoint, dim=1).values\n\t            Rpoint_T = Rpoint.unsqueeze(-1).repeat(1, split)\n", "            output_cossim_one_T = output_cossim_one.transpose(0, 1)\n\t            mask_one_T = (Rpoint_T < output_cossim_one_T)\n\t            mask_one_T = torch.tensor(mask_one_T.transpose(0, 1), dtype=torch.float16)\n\t            Rpoint_one = Rpoint[mi*split : (mi+1)*split]\n\t            Rpoint_one = Rpoint_one.unsqueeze(-1).repeat(1, batch_size)\n\t            mask_one = torch.tensor((Rpoint_one < output_cossim_one), dtype=torch.float16)\n\t            mask_one = torch.logical_or(mask_one, mask_one_T).type(torch.float16)\n\t            neglect_mask = torch.logical_or(mask_one, mask_neglect_base).type(torch.float16)\n\t            neglect_negative_mask_one = negative_mask_one * neglect_mask\n\t            mask_one = mask_one * negative_mask_one\n", "            modeloutput_s_pr_one = modeloutput_s_pr[mi*split : (mi+1)*split]\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                output_cossim_ema_one = torch.matmul(modeloutput_s_pr_one, modeloutput_s_pr.transpose(0, 1))\n\t                Rpoint_ema = torch.matmul(modeloutput_s_pr, Pool_sp.transpose(0, 1))\n\t            Rpoint_ema = torch.max(Rpoint_ema, dim=1).values\n\t            Rpoint_ema_T = Rpoint_ema.unsqueeze(-1).repeat(1, split)\n\t            output_cossim_ema_one_T = output_cossim_ema_one.transpose(0, 1)\n\t            mask_ema_one_T = (Rpoint_ema_T < output_cossim_ema_one_T)\n\t            mask_ema_one_T = torch.tensor(mask_ema_one_T.transpose(0, 1), dtype=torch.float16)\n\t            Rpoint_ema_one = Rpoint_ema[mi*split : (mi+1)*split]\n", "            Rpoint_ema_one = Rpoint_ema_one.unsqueeze(-1).repeat(1, batch_size)\n\t            mask_ema_one = torch.tensor((Rpoint_ema_one < output_cossim_ema_one), dtype=torch.float16)\n\t            mask_ema_one = torch.logical_or(mask_ema_one, mask_ema_one_T).type(torch.float16)\n\t            mask_ema_one = mask_ema_one * negative_mask_one\n\t            modeloutput_z_one = modeloutput_z[mi*split : (mi+1)*split]\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                anchor_dot_contrast_one = torch.div(\n\t                    torch.matmul(modeloutput_z_one, modeloutput_z.T),\n\t                    self.temperature)\n\t            logits_max_one, _ = torch.max(anchor_dot_contrast_one, dim=1, keepdim=True)\n", "            logits_one = anchor_dot_contrast_one - logits_max_one.detach()\n\t            exp_logits_one = torch.exp(logits_one) * neglect_negative_mask_one\n\t            log_prob_one = logits_one - torch.log(exp_logits_one.sum(1, keepdim=True))\n\t            if opt[\"loss_version\"] == 1:\n\t                nonzero_idx = torch.where(mask_one.sum(1) != 0.)\n\t                mask_one = mask_one[nonzero_idx]\n\t                log_prob_one = log_prob_one[nonzero_idx]\n\t                mask_ema_one = mask_ema_one[nonzero_idx]\n\t                weighted_mask = torch.tensor(mask_one.detach()) + torch.tensor(mask_ema_one).detach()*lmbd\n\t                if opt[\"reweighting\"] == 1:\n", "                    pnm = torch.tensor(torch.sum(weighted_mask, dim=1), dtype=torch.float32)\n\t                    pnm = (pnm / torch.sum(pnm))\n\t                    pnm = pnm / torch.mean(pnm)\n\t                else:\n\t                    pnm = 1\n\t                mean_log_prob_pos_one = (weighted_mask * log_prob_one).sum(1) / (weighted_mask.sum(1))\n\t                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one * pnm)\n\t            elif opt[\"loss_version\"] == 2:\n\t                nonzero_idx = torch.where(mask_one.sum(1) != 0.)\n\t                mask_one = mask_one[nonzero_idx]\n", "                nonzero_idx_ema = torch.where(mask_ema_one.sum(1) != 0.)\n\t                mask_ema_one = mask_ema_one[nonzero_idx_ema]\n\t                if opt[\"reweighting\"] == 1:\n\t                    pnm = torch.tensor(torch.sum(mask_one, dim=1), dtype=torch.float32)\n\t                    pnm = (pnm / torch.sum(pnm))\n\t                    pnm = pnm / torch.mean(pnm)\n\t                    pnm_ema = torch.tensor(torch.sum(mask_ema_one, dim=1), dtype=torch.float32)\n\t                    pnm_ema = (pnm_ema / torch.sum(pnm_ema))\n\t                    pnm_ema = pnm_ema / torch.mean(pnm_ema)\n\t                else:\n", "                    pnm = 1\n\t                    pnm_ema=1\n\t                mean_log_prob_pos_one = (mask_one * log_prob_one[nonzero_idx]).sum(1) / (mask_one.sum(1))\n\t                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one * pnm)\n\t                mean_log_prob_pos_one_ema = (mask_ema_one * log_prob_one).sum(1) / (mask_ema_one.sum(1))\n\t                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one_ema * pnm_ema) * lmbd\n\t            modeloutput_z_mix_one = modeloutput_z_mix[mi * split: (mi + 1) * split]\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                anchor_dot_contrast_one_lhp = torch.div(\n\t                    torch.matmul(modeloutput_z_mix_one, modeloutput_z_mix.T),\n", "                    self.temperature)\n\t            logits_max_one_lhp, _ = torch.max(anchor_dot_contrast_one_lhp, dim=1, keepdim=True)\n\t            logits_one_lhp = anchor_dot_contrast_one_lhp - logits_max_one_lhp.detach()\n\t            exp_logits_one_lhp = torch.exp(logits_one_lhp) * neglect_negative_mask_one\n\t            log_prob_one_lhp = logits_one_lhp - torch.log(exp_logits_one_lhp.sum(1, keepdim=True))\n\t            if opt[\"loss_version\"]==1:\n\t                log_prob_one_lhp = log_prob_one_lhp[nonzero_idx]\n\t                mean_log_prob_pos_one_lhp = (weighted_mask * log_prob_one_lhp).sum(1) / (weighted_mask.sum(1))\n\t                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one_lhp * pnm)\n\t            elif opt[\"loss_version\"]==2:\n", "                mean_log_prob_pos_one = (mask_one * log_prob_one[nonzero_idx]).sum(1) / (mask_one.sum(1))\n\t                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one * pnm)\n\t                mean_log_prob_pos_one_ema = (mask_ema_one * log_prob_one[nonzero_idx_ema]).sum(1) / (mask_ema_one.sum(1))\n\t                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one_ema * pnm_ema) * lmbd\n\t            negative_mask_one = torch.roll(negative_mask_one, split, dims=1)\n\t        loss = loss / mini_iters / 2\n\t        return loss\n"]}
{"filename": "run.py", "chunked_list": ["from typing import Dict, Tuple\n\timport argparse\n\tfrom functools import partial\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.backends import cudnn\n\timport torch.distributed as dist\n\tfrom torch.nn.parallel.distributed import DistributedDataParallel\n\tfrom torch.utils.data.dataloader import DataLoader\n", "import wandb\n\timport os\n\tfrom tqdm import tqdm\n\tfrom utils.common_utils import (save_checkpoint, parse, dprint, time_log, compute_param_norm,\n\t                                freeze_bn, zero_grad_bn, RunningAverage, Timer)\n\tfrom utils.dist_utils import all_reduce_dict\n\tfrom utils.wandb_utils import set_wandb\n\tfrom utils.seg_utils import UnsupervisedMetrics, batched_crf, get_metrics\n\tfrom build import (build_model, build_criterion, build_dataset, build_dataloader, build_optimizer)\n\tfrom pytorch_lightning.utilities.seed import seed_everything\n", "from torchvision import datasets, transforms\n\timport numpy as np\n\tfrom torch.optim import Adam, AdamW\n\tfrom loss import SupConLoss\n\tdef run(opt: dict, is_test: bool = False, is_debug: bool = False):\n\t    is_train = (not is_test)\n\t    seed_everything(seed=0)\n\t    scaler = torch.cuda.amp.GradScaler(init_scale=2048, growth_interval=1000, enabled=True)\n\t    # -------------------- Folder Setup (Task-Specific) --------------------------#\n\t    prefix = \"{}/{}_{}\".format(opt[\"output_dir\"], opt[\"dataset\"][\"data_type\"], opt[\"wandb\"][\"name\"])\n", "    opt[\"full_name\"] = prefix\n\t    cudnn.benchmark = True\n\t    world_size=1\n\t    local_rank = 0\n\t    wandb_save_dir = set_wandb(opt, local_rank, force_mode=\"disabled\" if (is_debug or is_test) else None)\n\t    train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n\t    train_loader_memory = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\t    # ------------------------ DataLoader ------------------------------#\n\t    if is_train:\n\t        train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n", "        train_loader = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\t    else:\n\t        train_loader = None\n\t    val_dataset = build_dataset(opt[\"dataset\"], mode=\"val\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n\t    val_loader = build_dataloader(val_dataset, opt[\"dataloader\"], shuffle=False,\n\t                                  batch_size=16)\n\t    # -------------------------- Define -------------------------------#\n\t    net_model, linear_model, cluster_model = build_model(opt=opt[\"model\"],\n\t                                                         n_classes=val_dataset.n_classes,\n\t                                                         is_direct=opt[\"eval\"][\"is_direct\"])\n", "    criterion = build_criterion(n_classes=val_dataset.n_classes,\n\t                                opt=opt[\"loss\"])\n\t    device = torch.device(\"cuda\", 0)\n\t    net_model = net_model.to(device)\n\t    linear_model = linear_model.to(device)\n\t    cluster_model = cluster_model.to(device)\n\t    project_head = nn.Linear(opt['model']['dim'], opt['model']['dim'])\n\t    project_head.cuda()\n\t    head_optimizer = Adam(project_head.parameters(), lr=opt[\"optimizer\"][\"net\"][\"lr\"])\n\t    criterion = criterion.to(device)\n", "    supcon_criterion = SupConLoss(temperature=opt[\"tau\"]).to(device)\n\t    pd = nn.PairwiseDistance()\n\t    model = net_model\n\t    model_m = model\n\t    print(\"Model:\")\n\t    print(model_m)\n\t    # ------------------- Optimizer  -----------------------#\n\t    if is_train:\n\t        net_optimizer, linear_probe_optimizer, cluster_probe_optimizer = build_optimizer(\n\t            main_params=model_m.parameters(),\n", "            linear_params=linear_model.parameters(),\n\t            cluster_params=cluster_model.parameters(),\n\t            opt=opt[\"optimizer\"],\n\t            model_type=opt[\"model\"][\"name\"])\n\t    else:\n\t        net_optimizer, linear_probe_optimizer, cluster_probe_optimizer = None, None, None\n\t    start_epoch, current_iter = 0, 0\n\t    best_metric, best_epoch, best_iter = 0, 0, 0\n\t    num_accum = 1\n\t    timer = Timer()\n", "    if opt[\"model\"][\"pretrained\"][\"model_type\"] == \"vit_small\":\n\t        feat_dim = 384\n\t    else:\n\t        feat_dim = 768\n\t    # ---------------------------- memory ---------------------------- #\n\t    with torch.no_grad():\n\t        Pool_ag = torch.zeros((opt[\"model\"][\"pool_size\"], feat_dim), dtype=torch.float16).cuda()\n\t        Pool_sp = torch.zeros((opt[\"model\"][\"pool_size\"], opt[\"model\"][\"dim\"]), dtype=torch.float16).cuda()\n\t        Pool_iter = iter(train_loader_memory)\n\t        for _iter in range(len(train_loader_memory)):\n", "            data = next(Pool_iter)\n\t            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n\t            if _iter >= opt[\"model\"][\"pool_size\"] / opt[\"dataloader\"][\"batch_size\"]:\n\t                break\n\t            img = img.cuda()\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                model_output = net_model(img)\n\t                modeloutput_f = model_output[0].clone().detach()\n\t                modeloutput_f = modeloutput_f.view(modeloutput_f.size(0), modeloutput_f.size(1), -1)\n\t                modeloutput_s_pr = model_output[2].clone().detach()\n", "                modeloutput_s_pr = modeloutput_s_pr.view(modeloutput_s_pr.size(0), modeloutput_s_pr.size(1), -1)\n\t            for _iter2 in range(modeloutput_f.size(0)):\n\t                randidx = np.random.randint(0, model_output[0].size(-1) * model_output[0].size(-2))\n\t                Pool_ag[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_f[_iter2][:,randidx]\n\t            for _iter2 in range(modeloutput_s_pr.size(0)):\n\t                randidx = np.random.randint(0, model_output[2].size(-1) * model_output[2].size(-2))\n\t                Pool_sp[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_s_pr[_iter2][:,randidx]\n\t            if _iter % 10 == 0:\n\t                print (\"Filling Pool Memory [{} / {}]\".format((_iter+1)*opt[\"dataloader\"][\"batch_size\"], opt[\"model\"][\"pool_size\"]))\n\t        Pool_ag = F.normalize(Pool_ag, dim=1)\n", "        Pool_sp = F.normalize(Pool_sp, dim=1)\n\t    # --------------------------- Train --------------------------------#\n\t    assert is_train\n\t    max_epoch = opt[\"train\"][\"epoch\"]\n\t    print_freq = opt[\"train\"][\"print_freq\"]\n\t    valid_freq = opt[\"train\"][\"valid_freq\"]\n\t    grad_norm = opt[\"train\"][\"grad_norm\"]\n\t    freeze_encoder_bn = opt[\"train\"][\"freeze_encoder_bn\"]\n\t    freeze_all_bn = opt[\"train\"][\"freeze_all_bn\"]\n\t    best_valid_metrics = dict(Cluster_mIoU=0, Cluster_Accuracy=0, Linear_mIoU=0, Linear_Accuracy=0)\n", "    train_stats = RunningAverage()\n\t    for current_epoch in range(start_epoch, max_epoch):\n\t        print(f\"-------- [{current_epoch}/{max_epoch} (iters: {current_iter})]--------\")\n\t        g_norm = torch.zeros(1, dtype=torch.float32, device=device)\n\t        net_model.train()\n\t        linear_model.train()\n\t        cluster_model.train()\n\t        project_head.train()\n\t        train_stats.reset()\n\t        _ = timer.update()\n", "        maxiter = len(train_loader) * opt[\"train\"][\"epoch\"]\n\t        for i, data in enumerate(train_loader):\n\t            trainingiter = current_epoch*len(train_loader) + i\n\t            if trainingiter <= opt[\"model\"][\"warmup\"]:\n\t                lmbd = 0\n\t            else:\n\t                lmbd = (trainingiter - opt[\"model\"][\"warmup\"]) / (maxiter - opt[\"model\"][\"warmup\"])\n\t            # newly initialize\n\t            if i % opt[\"renew_interval\"] == 0 and i!= 0:\n\t                with torch.no_grad():\n", "                    Pool_sp = torch.zeros((opt[\"model\"][\"pool_size\"], opt[\"model\"][\"dim\"]), dtype=torch.float16).cuda()\n\t                    for _iter, data in enumerate(train_loader_memory):\n\t                        if _iter >= opt[\"model\"][\"pool_size\"] / opt[\"dataloader\"][\"batch_size\"]:\n\t                            break\n\t                        img_net: torch.Tensor = data['img'].to(device, non_blocking=True)\n\t                        with torch.cuda.amp.autocast(enabled=True):\n\t                            model_output = net_model(img_net)\n\t                            modeloutput_s_pr = model_output[2].clone().detach()\n\t                            modeloutput_s_pr = modeloutput_s_pr.view(modeloutput_s_pr.size(0), modeloutput_s_pr.size(1), -1)\n\t                        for _iter2 in range(modeloutput_s_pr.size(0)):\n", "                            randidx = np.random.randint(0, model_output[2].size(-1) * model_output[2].size(-2))\n\t                            Pool_sp[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_s_pr[_iter2][:, randidx]\n\t                        if _iter == 0:\n\t                            print(\"Filling Pool Memory [{} / {}]\".format(\n\t                                (_iter + 1) * opt[\"dataloader\"][\"batch_size\"], opt[\"model\"][\"pool_size\"]))\n\t                    Pool_sp = F.normalize(Pool_sp, dim=1)\n\t            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n\t            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\t            img_aug = data['img_aug'].to(device, non_blocking=True)\n\t            data_time = timer.update()\n", "            if freeze_encoder_bn:\n\t                freeze_bn(model_m.model)\n\t            if 0 < freeze_all_bn <= current_epoch:\n\t                freeze_bn(net_model)\n\t            batch_size = img.shape[0]\n\t            net_optimizer.zero_grad(set_to_none=True)\n\t            linear_probe_optimizer.zero_grad(set_to_none=True)\n\t            cluster_probe_optimizer.zero_grad(set_to_none=True)\n\t            head_optimizer.zero_grad(set_to_none=True)\n\t            model_input = (img, label)\n", "            with torch.cuda.amp.autocast(enabled=True):\n\t                model_output = net_model(img, train=True)\n\t                model_output_aug = net_model(img_aug)\n\t            modeloutput_f = model_output[0].clone().detach().permute(0, 2, 3, 1).reshape(-1, feat_dim)\n\t            modeloutput_f = F.normalize(modeloutput_f, dim=1)\n\t            modeloutput_s = model_output[1].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\t            modeloutput_s_aug = model_output_aug[1].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                modeloutput_z = project_head(modeloutput_s)\n\t                modeloutput_z_aug = project_head(modeloutput_s_aug)\n", "            modeloutput_z = F.normalize(modeloutput_z, dim=1)\n\t            modeloutput_z_aug = F.normalize(modeloutput_z_aug, dim=1)\n\t            loss_consistency = torch.mean(pd(modeloutput_z, modeloutput_z_aug))\n\t            modeloutput_s_mix = model_output[3].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                modeloutput_z_mix = project_head(modeloutput_s_mix)\n\t            modeloutput_z_mix = F.normalize(modeloutput_z_mix, dim=1)\n\t            modeloutput_s_pr = model_output[2].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\t            modeloutput_s_pr = F.normalize(modeloutput_s_pr, dim=1)\n\t            loss_supcon = supcon_criterion(modeloutput_z, modeloutput_s_pr=modeloutput_s_pr, modeloutput_f=modeloutput_f,\n", "                                   Pool_ag=Pool_ag, Pool_sp=Pool_sp,\n\t                                   opt=opt, lmbd=lmbd, modeloutput_z_mix=modeloutput_z_mix)\n\t            detached_code = torch.clone(model_output[1].detach())\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                linear_output = linear_model(detached_code)\n\t                cluster_output = cluster_model(detached_code, None, is_direct=False)\n\t                loss, loss_dict, corr_dict = criterion(model_input=model_input,\n\t                                                       model_output=model_output,\n\t                                                       linear_output=linear_output,\n\t                                                       cluster_output=cluster_output\n", "                                                       )\n\t                loss = loss + loss_supcon + loss_consistency*opt[\"alpha\"]\n\t                # loss = loss / num_accum\n\t            forward_time = timer.update()\n\t            scaler.scale(loss).backward()\n\t            if freeze_encoder_bn:\n\t                zero_grad_bn(model_m)\n\t            if 0 < freeze_all_bn <= current_epoch:\n\t                zero_grad_bn(net_model)\n\t            scaler.unscale_(net_optimizer)\n", "            g_norm = nn.utils.clip_grad_norm_(net_model.parameters(), grad_norm)\n\t            scaler.step(net_optimizer)\n\t            scaler.step(linear_probe_optimizer)\n\t            scaler.step(cluster_probe_optimizer)\n\t            scaler.step(head_optimizer)\n\t            scaler.update()\n\t            current_iter += 1\n\t            backward_time = timer.update()\n\t            loss_dict = all_reduce_dict(loss_dict, op=\"mean\")\n\t            train_stats.append(loss_dict[\"loss\"])\n", "            if i % print_freq == 0:\n\t                lrs = [int(pg[\"lr\"] * 1e8) / 1e8 for pg in net_optimizer.param_groups]\n\t                p_norm = compute_param_norm(net_model.parameters())\n\t                s = time_log()\n\t                s += f\"epoch: {current_epoch}, iters: {current_iter} \" \\\n\t                     f\"({i} / {len(train_loader)} -th batch of loader)\\n\"\n\t                s += f\"loss(now/avg): {loss_dict['loss']:.6f}/{train_stats.avg:.6f}\\n\"\n\t                if len(loss_dict) > 2:\n\t                    for loss_k, loss_v in loss_dict.items():\n\t                        if loss_k != \"loss\":\n", "                            s += f\"-- {loss_k}(now): {loss_v:.6f}\\n\"\n\t                            if loss_k == \"corr\":\n\t                                for k, v in corr_dict.items():\n\t                                    s += f\"  -- {k}(now): {v:.6f}\\n\"\n\t                s += f\"time(data/fwd/bwd): {data_time:.3f}/{forward_time:.3f}/{backward_time:.3f}\\n\"\n\t                s += f\"LR: {lrs}\\n\"\n\t                s += f\"batch_size x world_size x num_accum: \" \\\n\t                     f\"{batch_size} x {world_size} x {num_accum} = {batch_size * world_size * num_accum}\\n\"\n\t                s += f\"norm(param/grad): {p_norm.item():.3f}/{g_norm.item():.3f}\"\n\t                print(s)\n", "            # --------------------------- Valid --------------------------------#\n\t            if ((i + 1) % valid_freq == 0) or ((i + 1) == len(train_loader)):\n\t                _ = timer.update()\n\t                valid_loss, valid_metrics = evaluate(net_model, linear_model,\n\t                                                    cluster_model, val_loader,\n\t                                                     device=device, opt=opt, n_classes=val_dataset.n_classes)\n\t                s = time_log()\n\t                s += f\"[VAL] -------- [{current_epoch}/{max_epoch} (iters: {current_iter})]--------\\n\"\n\t                s += f\"[VAL] epoch: {current_epoch}, iters: {current_iter}\\n\"\n\t                s += f\"[VAL] loss: {valid_loss:.6f}\\n\"\n", "                metric = \"All\"\n\t                prev_best_metric = best_metric\n\t                if best_metric <= (valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"]):\n\t                    best_metric = (valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"])\n\t                    best_epoch = current_epoch\n\t                    best_iter = current_iter\n\t                    s += f\"[VAL] -------- updated ({metric})! {prev_best_metric:.6f} -> {best_metric:.6f}\\n\"\n\t                    save_checkpoint(\n\t                        \"ckpt\", net_model, net_optimizer,\n\t                        linear_model, linear_probe_optimizer,\n", "                        cluster_model, cluster_probe_optimizer,\n\t                        current_epoch, current_iter, best_metric, wandb_save_dir, model_only=True)\n\t                    print (\"SAVED CHECKPOINT\")\n\t                    for metric_k, metric_v in valid_metrics.items():\n\t                        s += f\"[VAL] {metric_k} : {best_valid_metrics[metric_k]:.6f} -> {metric_v:.6f}\\n\"\n\t                    best_valid_metrics.update(valid_metrics)\n\t                else:\n\t                    now_metric = valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"]\n\t                    s += f\"[VAL] -------- not updated ({metric}).\" \\\n\t                         f\" (now) {now_metric:.6f} vs (best) {prev_best_metric:.6f}\\n\"\n", "                    s += f\"[VAL] previous best was at {best_epoch} epoch, {best_iter} iters\\n\"\n\t                    for metric_k, metric_v in valid_metrics.items():\n\t                        s += f\"[VAL] {metric_k} : {metric_v:.6f} vs {best_valid_metrics[metric_k]:.6f}\\n\"\n\t                print(s)\n\t                net_model.train()\n\t                linear_model.train()\n\t                cluster_model.train()\n\t                train_stats.reset()\n\t            _ = timer.update()\n\t    checkpoint_loaded = torch.load(f\"{wandb_save_dir}/ckpt.pth\", map_location=device)\n", "    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n\t    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n\t    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n\t    loss_out, metrics_out = evaluate(net_model, linear_model,\n\t        cluster_model, val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes)\n\t    s = time_log()\n\t    for metric_k, metric_v in metrics_out.items():\n\t        s += f\"[before CRF] {metric_k} : {metric_v:.2f}\\n\"\n\t    print(s)\n\t    checkpoint_loaded = torch.load(f\"{wandb_save_dir}/ckpt.pth\", map_location=device)\n", "    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n\t    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n\t    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n\t    loss_out, metrics_out = evaluate(net_model, linear_model, cluster_model,\n\t        val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes, is_crf=opt[\"eval\"][\"is_crf\"])\n\t    s = time_log()\n\t    for metric_k, metric_v in metrics_out.items():\n\t        s += f\"[after CRF] {metric_k} : {metric_v:.2f}\\n\"\n\t    print(s)\n\t    wandb.finish()\n", "    print(f\"-------- Train Finished --------\")\n\tdef evaluate(net_model: nn.Module,\n\t             linear_model: nn.Module,\n\t             cluster_model: nn.Module,\n\t             eval_loader: DataLoader,\n\t             device: torch.device,\n\t             opt: Dict,\n\t             n_classes: int,\n\t             is_crf: bool = False,\n\t             data_type: str = \"\",\n", "             ) -> Tuple[float, Dict[str, float]]:\n\t    net_model.eval()\n\t    cluster_metrics = UnsupervisedMetrics(\n\t        \"Cluster_\", n_classes, opt[\"eval\"][\"extra_clusters\"], True)\n\t    linear_metrics = UnsupervisedMetrics(\n\t        \"Linear_\", n_classes, 0, False)\n\t    with torch.no_grad():\n\t        eval_stats = RunningAverage()\n\t        for i, data in enumerate(tqdm(eval_loader)):\n\t            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n", "            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                output = net_model(img)\n\t            feats = output[0]\n\t            head_code = output[1]\n\t            head_code = F.interpolate(head_code, label.shape[-2:], mode='bilinear', align_corners=False)\n\t            if is_crf:\n\t                with torch.cuda.amp.autocast(enabled=True):\n\t                    linear_preds = torch.log_softmax(linear_model(head_code), dim=1)\n\t                with torch.cuda.amp.autocast(enabled=True):\n", "                    cluster_loss, cluster_preds = cluster_model(head_code, 2, log_probs=True, is_direct=opt[\"eval\"][\"is_direct\"])\n\t                linear_preds = batched_crf(img, linear_preds).argmax(1).cuda()\n\t                cluster_preds = batched_crf(img, cluster_preds).argmax(1).cuda()\n\t            else:\n\t                with torch.cuda.amp.autocast(enabled=True):\n\t                    linear_preds = linear_model(head_code).argmax(1)\n\t                with torch.cuda.amp.autocast(enabled=True):\n\t                    cluster_loss, cluster_preds = cluster_model(head_code, None, is_direct=opt[\"eval\"][\"is_direct\"])\n\t                cluster_preds = cluster_preds.argmax(1)\n\t            linear_metrics.update(linear_preds, label)\n", "            cluster_metrics.update(cluster_preds, label)\n\t            eval_stats.append(cluster_loss)\n\t        eval_metrics = get_metrics(cluster_metrics, linear_metrics)\n\t        return eval_stats.avg, eval_metrics\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--opt\", type=str, required=True, help=\"Path to option JSON file.\")\n\t    parser.add_argument(\"--test\", action=\"store_true\", help=\"Test mode, no WandB, highest priority.\")\n\t    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode, no WandB, second highest priority.\")\n\t    parser.add_argument(\"--checkpoint\", type=str, default=None, help=\"Checkpoint override\")\n", "    parser.add_argument(\"--data_path\", type=str, default=None, help=\"Data path override\")\n\t    parser_args = parser.parse_args()\n\t    parser_opt = parse(parser_args.opt)\n\t    if parser_args.checkpoint is not None:\n\t        parser_opt[\"checkpoint\"] = parser_args.checkpoint\n\t    if parser_args.data_path is not None:\n\t        parser_opt[\"dataset\"][\"data_path\"] = parser_args.data_path\n\t    run(parser_opt, is_test=parser_args.test, is_debug=parser_args.debug)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "eval.py", "chunked_list": ["from typing import Dict, Tuple\n\timport argparse\n\tfrom functools import partial\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F  # noqa\n\tfrom torch.backends import cudnn\n\timport torch.distributed as dist\n\tfrom torch.nn.parallel.distributed import DistributedDataParallel\n\tfrom torch.utils.data.dataloader import DataLoader\n", "import wandb\n\timport os\n\tfrom tqdm import tqdm\n\tfrom utils.common_utils import (save_checkpoint, parse, dprint, time_log, compute_param_norm,\n\t                                freeze_bn, zero_grad_bn, RunningAverage, Timer)\n\tfrom utils.dist_utils import all_reduce_dict\n\tfrom utils.wandb_utils import set_wandb\n\tfrom utils.seg_utils import UnsupervisedMetrics, batched_crf, get_metrics\n\tfrom build import (build_model, build_criterion, build_dataset, build_dataloader, build_optimizer)\n\tfrom pytorch_lightning.utilities.seed import seed_everything\n", "from torchvision import datasets, transforms\n\timport numpy as np\n\tfrom torch.optim import Adam, AdamW\n\tfrom loss import SupConLoss\n\tdef run(opt: dict, is_test: bool = False, is_debug: bool = False):\n\t    is_train = (not is_test)\n\t    seed_everything(seed=0)\n\t    scaler = torch.cuda.amp.GradScaler(init_scale=2048, growth_interval=1000, enabled=True)\n\t    # -------------------- Folder Setup (Task-Specific) --------------------------#\n\t    prefix = \"{}/{}_{}\".format(opt[\"output_dir\"], opt[\"dataset\"][\"data_type\"], opt[\"wandb\"][\"name\"])\n", "    opt[\"full_name\"] = prefix\n\t    # -------------------- Distributed Setup --------------------------#\n\t    if (opt[\"num_gpus\"] == 0) or (not torch.cuda.is_available()):\n\t        raise ValueError(\"Run requires at least 1 GPU.\")\n\t    if (opt[\"num_gpus\"] > 1) and (not dist.is_initialized()):\n\t        assert dist.is_available()\n\t        dist.init_process_group(backend=\"nccl\")  # nccl for NVIDIA GPUs\n\t        world_size = int(dist.get_world_size())\n\t        local_rank = int(dist.get_rank())\n\t        torch.cuda.set_device(local_rank)\n", "        print_fn = partial(dprint, local_rank=local_rank)  # only prints when local_rank == 0\n\t        is_distributed = True\n\t    else:\n\t        world_size = 1\n\t        local_rank = 0\n\t        print_fn = print\n\t        is_distributed = False\n\t    cudnn.benchmark = True\n\t    is_master = (local_rank == 0)\n\t    wandb_save_dir = set_wandb(opt, local_rank, force_mode=\"disabled\" if (is_debug or is_test) else None)\n", "    if not wandb_save_dir:\n\t        wandb_save_dir = os.path.join(opt[\"output_dir\"], opt[\"wandb\"][\"name\"])\n\t    if is_test:\n\t        wandb_save_dir = \"/\".join(opt[\"checkpoint\"].split(\"/\")[:-1])\n\t    train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n\t    train_loader_memory = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\t    # ------------------------ DataLoader ------------------------------#\n\t    if is_train:\n\t        train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n\t        train_loader = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n", "    else:\n\t        train_loader = None\n\t    val_dataset = build_dataset(opt[\"dataset\"], mode=\"val\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n\t    val_loader = build_dataloader(val_dataset, opt[\"dataloader\"], shuffle=False,\n\t                                  batch_size=world_size*32)\n\t    # -------------------------- Define -------------------------------#\n\t    net_model, linear_model, cluster_model = build_model(opt=opt[\"model\"],\n\t                                                         n_classes=val_dataset.n_classes,\n\t                                                         is_direct=opt[\"eval\"][\"is_direct\"])\n\t    device = torch.device(\"cuda\", local_rank)\n", "    net_model = net_model.to(device)\n\t    linear_model = linear_model.to(device)\n\t    cluster_model = cluster_model.to(device)\n\t    model = net_model\n\t    model_m = model\n\t    print_fn(\"Model:\")\n\t    print_fn(model_m)\n\t    # --------------------------- Evaluate with Best --------------------------------#\n\t    loading_dir = os.path.join(opt['output_dir'], opt['checkpoint'])\n\t    checkpoint_loaded = torch.load(f\"{loading_dir}/ckpt.pth\", map_location=device)\n", "    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n\t    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n\t    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n\t    loss_, metrics_ = evaluate(net_model, linear_model, cluster_model, val_loader, device=device,\n\t                                                                            opt=opt, n_classes=train_dataset.n_classes)\n\t    s = time_log()\n\t    s += f\" ------------------- before crf ---------------------\\n\"\n\t    for metric_k, metric_v in metrics_.items():\n\t        s += f\"before crf{metric_k} : {metric_v:.2f}\\n\"\n\t    print_fn(s)\n", "    loss_, metrics_ = evaluate(net_model, linear_model, cluster_model,\n\t        val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes, is_crf=opt[\"eval\"][\"is_crf\"])\n\t    s = time_log()\n\t    s += f\" -------------------after crf ---------------------\\n\"\n\t    for metric_k, metric_v in metrics_.items():\n\t        s += f\"[after crf] {metric_k} : {metric_v:.2f}\\n\"\n\t    print_fn(s)\n\tdef evaluate(net_model: nn.Module,\n\t             linear_model: nn.Module,\n\t             cluster_model: nn.Module,\n", "             eval_loader: DataLoader,\n\t             device: torch.device,\n\t             opt: Dict,\n\t             n_classes: int,\n\t             is_crf: bool = False,\n\t             data_type: str = \"\",\n\t             ) -> Tuple[float, Dict[str, float]]:\n\t    net_model.eval()\n\t    cluster_metrics = UnsupervisedMetrics(\n\t        \"Cluster_\", n_classes, opt[\"eval\"][\"extra_clusters\"], True)\n", "    linear_metrics = UnsupervisedMetrics(\n\t        \"Linear_\", n_classes, 0, False)\n\t    with torch.no_grad():\n\t        eval_stats = RunningAverage()\n\t        for i, data in enumerate(tqdm(eval_loader)):\n\t            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n\t            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\t            with torch.cuda.amp.autocast(enabled=True):\n\t                output = net_model(img)\n\t            feats = output[0]\n", "            head_code = output[1]\n\t            head_code = F.interpolate(head_code, label.shape[-2:], mode='bilinear', align_corners=False)\n\t            if is_crf:\n\t                with torch.cuda.amp.autocast(enabled=True):\n\t                    linear_preds = torch.log_softmax(linear_model(head_code), dim=1)\n\t                with torch.cuda.amp.autocast(enabled=True):\n\t                    cluster_loss, cluster_preds = cluster_model(head_code, 2, log_probs=True, is_direct=opt[\"eval\"][\"is_direct\"])\n\t                linear_preds = batched_crf(img, linear_preds).argmax(1).cuda()\n\t                cluster_preds = batched_crf(img, cluster_preds).argmax(1).cuda()\n\t            else:\n", "                with torch.cuda.amp.autocast(enabled=True):\n\t                    linear_preds = linear_model(head_code).argmax(1)\n\t                with torch.cuda.amp.autocast(enabled=True):\n\t                    cluster_loss, cluster_preds = cluster_model(head_code, None, is_direct=opt[\"eval\"][\"is_direct\"])\n\t                cluster_preds = cluster_preds.argmax(1)\n\t            linear_metrics.update(linear_preds, label)\n\t            cluster_metrics.update(cluster_preds, label)\n\t            eval_stats.append(cluster_loss)\n\t        eval_metrics = get_metrics(cluster_metrics, linear_metrics)\n\t        return eval_stats.avg, eval_metrics\n", "def main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--opt\", type=str, required=True, help=\"Path to option JSON file.\")\n\t    parser.add_argument(\"--test\", action=\"store_true\", help=\"Test mode, no WandB, highest priority.\")\n\t    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode, no WandB, second highest priority.\")\n\t    parser.add_argument(\"--checkpoint\", type=str, default=None, help=\"Checkpoint override\")\n\t    parser.add_argument(\"--data_path\", type=str, default=None, help=\"Data path override\")\n\t    parser_args = parser.parse_args()\n\t    parser_opt = parse(parser_args.opt)\n\t    # if parser_args.checkpoint is not None:\n", "    #     parser_opt[\"checkpoint\"] = parser_args.checkpoint\n\t    if parser_args.data_path is not None:\n\t        parser_opt[\"dataset\"][\"data_path\"] = parser_args.data_path\n\t    run(parser_opt, is_test=parser_args.test, is_debug=parser_args.debug)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "build.py", "chunked_list": ["from typing import Optional, Dict\n\tfrom torch.optim import Adam, AdamW\n\timport torch.distributed as dist\n\tfrom torch.utils.data.distributed import DistributedSampler\n\tfrom torch.utils.data.dataloader import DataLoader\n\tfrom model.STEGO import (STEGOmodel)\n\tfrom model.LambdaLayer import LambdaLayer\n\tfrom model.dino.DinoFeaturizer import DinoFeaturizer\n\tfrom dataset.data import ContrastiveSegDataset, get_transform\n\tfrom torchvision import transforms as T\n", "from loss import *\n\tdef build_model(opt: dict, n_classes: int = 27, is_direct: bool = False):\n\t    model_type = opt[\"name\"].lower()\n\t    if \"stego\" in model_type:\n\t        model = STEGOmodel.build(\n\t            opt=opt,\n\t            n_classes=n_classes\n\t        )\n\t        net_model = model.net\n\t        linear_model = model.linear_probe\n", "        cluster_model = model.cluster_probe\n\t    elif model_type == \"dino\":\n\t        model = nn.Sequential(\n\t            DinoFeaturizer(20, opt),\n\t            LambdaLayer(lambda p: p[0])\n\t        )\n\t    else:\n\t        raise ValueError(\"No model: {} found\".format(model_type))\n\t    bn_momentum = opt.get(\"bn_momentum\", None)\n\t    if bn_momentum is not None:\n", "        for module_name, module in model.named_modules():\n\t            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n\t                module.momentum = bn_momentum\n\t    bn_eps = opt.get(\"bn_eps\", None)\n\t    if bn_eps is not None:\n\t        for module_name, module in model.named_modules():\n\t            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n\t                module.eps = bn_eps\n\t    if \"stego\" in model_type:\n\t        return net_model, linear_model, cluster_model\n", "    elif model_type == \"dino\":\n\t        return model\n\tdef build_criterion(n_classes: int, opt: dict):\n\t    # opt = opt[\"loss\"]\n\t    loss_name = opt[\"name\"].lower()\n\t    if \"stego\" in loss_name:\n\t        loss = StegoLoss(n_classes=n_classes, cfg=opt, corr_weight=opt[\"correspondence_weight\"])\n\t    else:\n\t        raise ValueError(f\"Unsupported loss type {loss_name}\")\n\t    return loss\n", "def split_params_for_optimizer(model, opt):\n\t    # opt = opt[\"optimizer\"]\n\t    params_small_lr = []\n\t    params_small_lr_no_wd = []\n\t    params_base_lr = []\n\t    params_base_lr_no_wd = []\n\t    for param_name, param_value in model.named_parameters():\n\t        param_value: torch.Tensor\n\t        if not param_value.requires_grad:\n\t            continue\n", "        if \"encoder\" in param_name:\n\t            if (param_value.ndim > 1) and (\"position\" not in param_name):\n\t                params_small_lr.append(param_value)\n\t            else:\n\t                params_small_lr_no_wd.append(param_value)\n\t        else:  # decoder\n\t            if (param_value.ndim > 1) and (\"position\" not in param_name):\n\t                params_base_lr.append(param_value)\n\t            else:\n\t                params_base_lr_no_wd.append(param_value)\n", "    same_lr = opt.get(\"same_lr\", True)\n\t    encoder_weight = 1.0 if same_lr else 0.1\n\t    params_for_optimizer = [\n\t        {\"params\": params_base_lr},\n\t        {\"params\": params_base_lr_no_wd, \"weight_decay\": 0.0},\n\t        # {\"params\": params_small_lr, \"lr\": opt[\"lr\"] * encoder_weight, \"weight_decay\": opt[\"weight_decay\"] * 0.1},\n\t        {\"params\": params_small_lr, \"lr\": opt[\"lr\"] * encoder_weight},\n\t        {\"params\": params_small_lr_no_wd, \"lr\": opt[\"lr\"] * encoder_weight, \"weight_decay\": 0.0},\n\t    ]\n\t    return params_for_optimizer\n", "def build_optimizer(main_params, linear_params, cluster_params, opt: dict, model_type: str):\n\t    # opt = opt[\"optimizer\"]\n\t    model_type = model_type.lower()\n\t    if \"stego\" in model_type:\n\t        net_optimizer_type = opt[\"net\"][\"name\"].lower()\n\t        if net_optimizer_type == \"adam\":\n\t            net_optimizer = Adam(main_params, lr=opt[\"net\"][\"lr\"])\n\t        elif net_optimizer_type == \"adamw\":\n\t            net_optimizer = AdamW(main_params, lr=opt[\"net\"][\"lr\"], weight_decay=opt[\"net\"][\"weight_decay\"])\n\t        else:\n", "            raise ValueError(f\"Unsupported optimizer type {net_optimizer_type}.\")\n\t        linear_probe_optimizer_type = opt[\"linear\"][\"name\"].lower()\n\t        if linear_probe_optimizer_type == \"adam\":\n\t            linear_probe_optimizer = Adam(linear_params, lr=opt[\"linear\"][\"lr\"])\n\t        else:\n\t            raise ValueError(f\"Unsupported optimizer type {linear_probe_optimizer_type}.\")\n\t        cluster_probe_optimizer_type = opt[\"cluster\"][\"name\"].lower()\n\t        if cluster_probe_optimizer_type == \"adam\":\n\t            cluster_probe_optimizer = Adam(cluster_params, lr=opt[\"cluster\"][\"lr\"])\n\t        else:\n", "            raise ValueError(f\"Unsupported optimizer type {cluster_probe_optimizer_type}.\")\n\t        return net_optimizer, linear_probe_optimizer, cluster_probe_optimizer\n\t    else:\n\t        raise ValueError(\"No model: {} found\".format(model_type))\n\tdef build_scheduler(opt: dict, optimizer, loader, start_epoch):\n\t    # opt = opt BE CAREFUL!\n\t    scheduler_type = opt[\"scheduler\"]['name'].lower()\n\t    if scheduler_type == \"onecycle\":\n\t        max_lrs = [pg[\"lr\"] for pg in optimizer.param_groups]\n\t        scheduler = torch.optim.lr_scheduler.OneCycleLR(  # noqa\n", "            optimizer,\n\t            # max_lr=opt['optimizer']['lr'],\n\t            max_lr=max_lrs,\n\t            epochs=opt['train']['epoch'] + 1,\n\t            steps_per_epoch=len(loader) // opt[\"train\"][\"num_accum\"],\n\t            cycle_momentum=opt[\"scheduler\"].get(\"cycle_momentum\", True),\n\t            base_momentum=0.85,\n\t            max_momentum=0.95,\n\t            pct_start=opt[\"scheduler\"][\"pct_start\"],\n\t            last_epoch=start_epoch - 1,\n", "            div_factor=opt[\"scheduler\"]['div_factor'],\n\t            final_div_factor=opt[\"scheduler\"]['final_div_factor']\n\t        )\n\t    else:\n\t        raise ValueError(f\"Unsupported scheduler type {scheduler_type}.\")\n\t    return scheduler\n\tdef build_dataset(opt: dict, mode: str = \"train\", model_type: str = \"dino\") -> ContrastiveSegDataset:\n\t    # opt = opt[\"dataset\"]\n\t    data_type = opt[\"data_type\"].lower()\n\t    if mode == \"train\":\n", "        geometric_transforms = T.Compose([\n\t            T.RandomHorizontalFlip(),\n\t            T.RandomResizedCrop(size=opt[\"res\"], scale=(0.8, 1.0))\n\t        ])\n\t        photometric_transforms = T.Compose([\n\t            T.ColorJitter(brightness=.3, contrast=.3, saturation=.3, hue=.1),\n\t            T.RandomGrayscale(.2),\n\t            T.RandomApply([T.GaussianBlur((5, 5))])\n\t        ])\n\t        return ContrastiveSegDataset(\n", "            pytorch_data_dir=opt[\"data_path\"],\n\t            dataset_name=opt[\"data_type\"],\n\t            crop_type=opt[\"crop_type\"],\n\t            model_type=model_type,\n\t            image_set=mode,\n\t            transform=get_transform(opt[\"res\"], False, opt[\"loader_crop_type\"]),\n\t            target_transform=get_transform(opt[\"res\"], True, opt[\"loader_crop_type\"]),\n\t            cfg=opt,\n\t            aug_geometric_transform=geometric_transforms,\n\t            aug_photometric_transform=photometric_transforms,\n", "            num_neighbors=opt[\"num_neighbors\"],\n\t            mask=True,\n\t            pos_images=False,\n\t            pos_labels=False\n\t        )\n\t    elif mode == \"val\" or mode == \"test\":\n\t        if mode == \"test\":\n\t            loader_crop = \"center\"\n\t        elif data_type == \"voc\":\n\t            loader_crop = None\n", "        else:\n\t            loader_crop = \"center\"\n\t        return ContrastiveSegDataset(\n\t            pytorch_data_dir=opt[\"data_path\"],\n\t            dataset_name=opt[\"data_type\"],\n\t            crop_type=None,\n\t            model_type=model_type,\n\t            image_set=\"val\",\n\t            transform=get_transform(320, False, loader_crop),\n\t            target_transform=get_transform(320, True, loader_crop),\n", "            mask=True,\n\t            cfg=opt,\n\t        )\n\tdef build_dataloader(dataset,\n\t                     opt: dict, shuffle: bool = True, pin_memory: bool = True,\n\t                     batch_size: Optional[int] = None) -> DataLoader:\n\t    # opt = opt[\"dataloader\"]\n\t    if batch_size is None:  # override\n\t        batch_size = opt[\"batch_size\"]\n\t    if not dist.is_initialized():\n", "        return DataLoader(\n\t            dataset,\n\t            batch_size=max(batch_size, 1),\n\t            shuffle=shuffle,\n\t            num_workers=opt.get(\"num_workers\", 4),\n\t            pin_memory=pin_memory,\n\t            drop_last=shuffle,\n\t        )\n\t    else:\n\t        assert dist.is_available() and dist.is_initialized()\n", "        ddp_sampler = DistributedSampler(\n\t            dataset,\n\t            num_replicas=dist.get_world_size(),\n\t            rank=dist.get_rank(),\n\t            shuffle=shuffle,\n\t            drop_last=shuffle,\n\t        )\n\t        world_size = dist.get_world_size()\n\t        return DataLoader(\n\t            dataset,\n", "            batch_size=max(batch_size // world_size, 1),\n\t            num_workers=(opt.get(\"num_workers\", 4) + world_size - 1) // world_size,\n\t            pin_memory=pin_memory,\n\t            sampler=ddp_sampler,\n\t            prefetch_factor=opt.get(\"prefetch_factor\", 1)\n\t        )\n"]}
{"filename": "visualize.py", "chunked_list": ["import torch.nn.functional as F\n\timport numpy as np\n\timport os\n\tfrom collections import defaultdict\n\tfrom os.path import join\n\tfrom dataset.data import create_cityscapes_colormap, create_pascal_label_colormap\n\tfrom utils.seg_utils import unnorm\n\tfrom PIL import Image\n\tfrom utils.seg_utils import UnsupervisedMetrics\n\tdef prep_for_plot(img, rescale=True, resize=None):\n", "    if resize is not None:\n\t        img = F.interpolate(img.unsqueeze(0), resize, mode=\"bilinear\")\n\t    else:\n\t        img = img.unsqueeze(0)\n\t    plot_img = unnorm(img).squeeze(0).squeeze(0).cpu().permute(1, 2, 0)\n\t    if rescale:\n\t        plot_img = (plot_img - plot_img.min()) / (plot_img.max() - plot_img.min())\n\t    return plot_img\n\tdef visualization(save_dir: str, dataset_type: str, saved_data: defaultdict, cluster_metrics: UnsupervisedMetrics,\n\t                  is_label: bool = False):\n", "    if is_label:\n\t        os.makedirs(join(save_dir, \"label\"), exist_ok=True)\n\t    os.makedirs(join(save_dir, \"cluster\"), exist_ok=True)\n\t    os.makedirs(join(save_dir, \"linear\"), exist_ok=True)\n\t    if dataset_type.startswith(\"cityscapes\"):\n\t        label_cmap = create_cityscapes_colormap()\n\t    else:\n\t        label_cmap = create_pascal_label_colormap()\n\t    for index in range(len(saved_data[\"img_path\"])):\n\t        file_name = str(saved_data[\"img_path\"][index]).split(\"/\")[-1].split(\".\")[0]\n", "        if is_label:\n\t            plot_label = (label_cmap[saved_data[\"label\"][index]]).astype(np.uint8)\n\t            Image.fromarray(plot_label).save(join(join(save_dir, \"label\", file_name + \".png\")))\n\t        plot_cluster = (label_cmap[cluster_metrics.map_clusters(saved_data[\"cluster_preds\"][index])]).astype(np.uint8)\n\t        Image.fromarray(plot_cluster).save(join(join(save_dir, \"cluster\", file_name + \".png\")))\n\t        plot_linear = (label_cmap[saved_data[\"linear_preds\"][index]]).astype(np.uint8)\n\t        Image.fromarray(plot_linear).save(join(join(save_dir, \"linear\", file_name + \".png\")))\n\tdef visualization_label(save_dir: str, saved_data: defaultdict):\n\t    label_cmap = create_pascal_label_colormap()\n\t    for index in range(saved_data[\"label\"][0].size(0)):\n", "        os.makedirs(join('./visualize/attn', str(index)), exist_ok=True)\n\t        plot_label = (label_cmap[saved_data[\"label\"][0][index]]).astype(np.uint8)\n\t        imagename = str(index)+\"_classlabel.png\"\n\t        Image.fromarray(plot_label).save(join(save_dir, str(index), imagename))\n"]}
{"filename": "utils/augment_utils.py", "chunked_list": ["import torchvision.transforms as transforms\n\tfrom PIL import ImageFilter, ImageOps, Image\n\timport random\n\timport numpy as np\n\tclass GaussianBlur(object):\n\t    \"\"\"\n\t    Apply Gaussian Blur to the PIL image.\n\t    \"\"\"\n\t    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n\t        self.prob = p\n", "        self.radius_min = radius_min\n\t        self.radius_max = radius_max\n\t    def __call__(self, img):\n\t        do_it = random.random() <= self.prob\n\t        if not do_it:\n\t            return img\n\t        return img.filter(\n\t            ImageFilter.GaussianBlur(\n\t                radius=random.uniform(self.radius_min, self.radius_max)\n\t            )\n", "        )\n\tclass Solarization(object):\n\t    \"\"\"\n\t    Apply Solarization to the PIL image.\n\t    \"\"\"\n\t    def __init__(self, p):\n\t        self.p = p\n\t    def __call__(self, img):\n\t        if random.random() < self.p:\n\t            return ImageOps.solarize(img)\n", "        else:\n\t            return img\n\tclass DataAugmentationDINO(object):\n\t    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n\t        flip_and_color_jitter = transforms.Compose([\n\t            transforms.RandomHorizontalFlip(p=0.5),\n\t            transforms.RandomApply(\n\t                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n\t                p=0.8\n\t            ),\n", "            transforms.RandomGrayscale(p=0.2),\n\t        ])\n\t        normalize = transforms.Compose([\n\t            transforms.ToTensor(),\n\t            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n\t        ])\n\t        # first global crop\n\t        self.global_transfo1 = transforms.Compose([\n\t            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n\t            flip_and_color_jitter,\n", "            GaussianBlur(1.0),\n\t            normalize,\n\t        ])\n\t        # second global crop\n\t        self.global_transfo2 = transforms.Compose([\n\t            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n\t            flip_and_color_jitter,\n\t            GaussianBlur(0.1),\n\t            Solarization(0.2),\n\t            normalize,\n", "        ])\n\t        # transformation for the local small crops\n\t        self.local_crops_number = local_crops_number\n\t        self.local_transfo = transforms.Compose([\n\t            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n\t            flip_and_color_jitter,\n\t            GaussianBlur(p=0.5),\n\t            normalize,\n\t        ])\n\t    def __call__(self, image):\n", "        crops = []\n\t        crops.append(self.global_transfo1(image))\n\t        crops.append(self.global_transfo2(image))\n\t        for _ in range(self.local_crops_number):\n\t            crops.append(self.local_transfo(image))\n\t        return crops"]}
{"filename": "utils/dist_utils.py", "chunked_list": ["from typing import Any, Dict, List\n\tfrom numbers import Number\n\timport torch\n\timport torch.distributed as dist\n\t__all__ = [\n\t    \"all_reduce_scalar\",\n\t    \"all_reduce_tensor\",\n\t    \"all_reduce_dict\",\n\t    \"all_gather_tensor\",\n\t]\n", "def all_reduce_scalar(value: Number, op: str = \"sum\") -> Number:\n\t    \"\"\"All-reduce single scalar value. NOT torch tensor.\"\"\"\n\t    # https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/utils/distributed.py\n\t    if dist.is_initialized() and dist.is_available():\n\t        op = op.lower()\n\t        if (op == \"sum\") or (op == \"mean\"):\n\t            dist_op = dist.ReduceOp.SUM\n\t        elif op == \"min\":\n\t            dist_op = dist.ReduceOp.MIN\n\t        elif op == \"max\":\n", "            dist_op = dist.ReduceOp.MAX\n\t        elif op == \"product\":\n\t            dist_op = dist.ReduceOp.PRODUCT\n\t        else:\n\t            raise RuntimeError(f\"Invalid all_reduce op: {op}\")\n\t        backend = dist.get_backend()\n\t        if backend == torch.distributed.Backend.NCCL:\n\t            device = torch.device(\"cuda\")\n\t        elif backend == torch.distributed.Backend.GLOO:\n\t            device = torch.device(\"cpu\")\n", "        else:\n\t            raise RuntimeError(f\"Unsupported distributed backend: {backend}\")\n\t        tensor = torch.tensor(value, device=device, requires_grad=False)\n\t        dist.all_reduce(tensor, op=dist_op)\n\t        if op == \"mean\":\n\t            tensor /= dist.get_world_size()\n\t        ret = tensor.item()\n\t    else:\n\t        ret = value\n\t    return ret\n", "def all_reduce_tensor(tensor: torch.Tensor, op=\"sum\", detach: bool = True) -> torch.Tensor:\n\t    if dist.is_initialized() and dist.is_available():\n\t        ret = tensor.clone()\n\t        if detach:\n\t            ret = ret.detach()\n\t        if (op == \"sum\") or (op == \"mean\"):\n\t            dist_op = dist.ReduceOp.SUM\n\t        else:\n\t            raise RuntimeError(f\"Invalid all_reduce op: {op}\")\n\t        dist.all_reduce(ret, op=dist_op)\n", "        if op == \"mean\":\n\t            ret /= dist.get_world_size()\n\t    else:\n\t        ret = tensor\n\t    return ret\n\tdef all_reduce_dict(result: Dict[str, Any], op=\"sum\") -> Dict[str, Any]:\n\t    new_result = {}\n\t    for k, v in result.items():\n\t        if isinstance(v, torch.Tensor):\n\t            new_result[k] = all_reduce_tensor(v, op)\n", "        elif isinstance(v, Number):\n\t            new_result[k] = all_reduce_scalar(v, op)\n\t        else:\n\t            raise RuntimeError(f\"Dictionary all_reduce should only have either tensor or scalar, got: {type(v)}\")\n\t    return new_result\n\tdef all_gather_tensor(tensor: torch.Tensor) -> List[torch.Tensor]:\n\t    if dist.is_initialized() and dist.is_available():\n\t        world_size = dist.get_world_size()\n\t        local_rank = dist.get_rank()\n\t        output = [\n", "            tensor if (i == local_rank) else torch.empty_like(tensor) for i in range(world_size)\n\t        ]\n\t        dist.all_gather(output, tensor, async_op=False)\n\t        return output\n\t    else:\n\t        return [tensor]\n"]}
{"filename": "utils/layer_utils.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass ClusterLookup(nn.Module):\n\t    def __init__(self, dim: int, n_classes: int):\n\t        super(ClusterLookup, self).__init__()\n\t        self.n_classes = n_classes\n\t        self.dim = dim\n\t        self.clusters = torch.nn.Parameter(torch.randn(n_classes, dim))\n\t    def reset_parameters(self):\n", "        with torch.no_grad():\n\t            self.clusters.copy_(torch.randn(self.n_classes, self.dim))\n\t    def forward(self, x, alpha, log_probs=False, is_direct=False):\n\t        if is_direct:\n\t            inner_products = x\n\t        else:\n\t            normed_clusters = F.normalize(self.clusters, dim=1)\n\t            normed_features = F.normalize(x, dim=1)\n\t            inner_products = torch.einsum(\"bchw,nc->bnhw\", normed_features, normed_clusters)\n\t        if alpha is None:\n", "            cluster_probs = F.one_hot(torch.argmax(inner_products, dim=1), self.clusters.shape[0]) \\\n\t                .permute(0, 3, 1, 2).to(torch.float32)\n\t        else:\n\t            cluster_probs = nn.functional.softmax(inner_products * alpha, dim=1)\n\t        cluster_loss = -(cluster_probs * inner_products).sum(1).mean()\n\t        if log_probs:\n\t            return cluster_loss, nn.functional.log_softmax(inner_products * alpha, dim=1)\n\t        else:\n\t            return cluster_loss, cluster_probs\n"]}
{"filename": "utils/__init__.py", "chunked_list": []}
{"filename": "utils/seg_utils.py", "chunked_list": ["from typing import Dict, Any\n\tfrom torchmetrics import Metric\n\tfrom scipy.optimize import linear_sum_assignment\n\tfrom utils.dist_utils import all_reduce_dict\n\timport numpy as np\n\timport pydensecrf.densecrf as dcrf\n\timport pydensecrf.utils as utils\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision.transforms.functional as VF\n", "MAX_ITER = 10\n\tPOS_W = 3\n\tPOS_XY_STD = 1\n\tBi_W = 4\n\tBi_XY_STD = 67\n\tBi_RGB_STD = 3\n\tBGR_MEAN = np.array([104.008, 116.669, 122.675])\n\tclass UnNormalize(object):\n\t    def __init__(self, mean, std):\n\t        self.mean = mean\n", "        self.std = std\n\t    def __call__(self, image):\n\t        image2 = torch.clone(image)\n\t        for t, m, s in zip(image2, self.mean, self.std):\n\t            t.mul_(s).add_(m)\n\t        return image2\n\tunnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\tdef dense_crf(image_tensor: torch.FloatTensor, output_logits: torch.FloatTensor):\n\t    image = np.array(VF.to_pil_image(unnorm(image_tensor)))[:, :, ::-1]\n\t    H, W = image.shape[:2]\n", "    image = np.ascontiguousarray(image)\n\t    output_logits = F.interpolate(output_logits.unsqueeze(0), size=(H, W), mode=\"bilinear\",\n\t                                  align_corners=False).squeeze()\n\t    output_probs = F.softmax(output_logits, dim=0).cpu().numpy()\n\t    c = output_probs.shape[0]\n\t    h = output_probs.shape[1]\n\t    w = output_probs.shape[2]\n\t    U = utils.unary_from_softmax(output_probs)\n\t    U = np.ascontiguousarray(U)\n\t    d = dcrf.DenseCRF2D(w, h, c)\n", "    d.setUnaryEnergy(U)\n\t    d.addPairwiseGaussian(sxy=POS_XY_STD, compat=POS_W)\n\t    d.addPairwiseBilateral(sxy=Bi_XY_STD, srgb=Bi_RGB_STD, rgbim=image, compat=Bi_W)\n\t    Q = d.inference(MAX_ITER)\n\t    Q = np.array(Q).reshape((c, h, w))\n\t    return Q\n\tdef _apply_crf(tup):\n\t    return dense_crf(tup[0], tup[1])\n\tdef batched_crf(img_tensor, prob_tensor):\n\t    batch_size = list(img_tensor.size())[0]\n", "    img_tensor_cpu = img_tensor.detach().cpu()\n\t    prob_tensor_cpu = prob_tensor.detach().cpu()\n\t    out = []\n\t    for i in range(batch_size):\n\t        out_ = dense_crf(img_tensor_cpu[i], prob_tensor_cpu[i])\n\t        out.append(out_)\n\t    return torch.cat([torch.from_numpy(arr).unsqueeze(0) for arr in out], dim=0)\n\tclass UnsupervisedMetrics(Metric):\n\t    def __init__(self, prefix: str, n_classes: int, extra_clusters: int, compute_hungarian: bool,\n\t                 dist_sync_on_step=True):\n", "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n\t        self.n_classes = n_classes\n\t        self.extra_clusters = extra_clusters\n\t        self.compute_hungarian = compute_hungarian\n\t        self.prefix = prefix\n\t        self.stats = torch.zeros(n_classes + self.extra_clusters, n_classes,\n\t                                           dtype=torch.int64, device=\"cuda\")\n\t    def update(self, preds: torch.Tensor, target: torch.Tensor):\n\t        with torch.no_grad():\n\t            actual = target.reshape(-1) # 32*320*320\n", "            preds = preds.reshape(-1) # 32*320*320\n\t            mask = (actual >= 0) & (actual < self.n_classes) & (preds >= 0) & (preds < self.n_classes)\n\t            actual = actual[mask]\n\t            preds = preds[mask]\n\t            self.stats += torch.bincount(\n\t                (self.n_classes + self.extra_clusters) * actual + preds,\n\t                minlength=self.n_classes * (self.n_classes + self.extra_clusters)) \\\n\t                .reshape(self.n_classes, self.n_classes + self.extra_clusters).t().to(self.stats.device)\n\t    def map_clusters(self, clusters):\n\t        if self.extra_clusters == 0:\n", "            return torch.tensor(self.assignments[1])[clusters]\n\t        else:\n\t            missing = sorted(list(set(range(self.n_classes + self.extra_clusters)) - set(self.assignments[0])))\n\t            cluster_to_class = self.assignments[1]\n\t            for missing_entry in missing:\n\t                if missing_entry == cluster_to_class.shape[0]:\n\t                    cluster_to_class = np.append(cluster_to_class, -1)\n\t                else:\n\t                    cluster_to_class = np.insert(cluster_to_class, missing_entry + 1, -1)\n\t            cluster_to_class = torch.tensor(cluster_to_class)\n", "            return cluster_to_class[clusters]\n\t    def compute(self):\n\t        if self.compute_hungarian:  # cluster\n\t            self.assignments = linear_sum_assignment(self.stats.detach().cpu(), maximize=True)  # row, col\n\t            if self.extra_clusters == 0:\n\t                self.histogram = self.stats[np.argsort(self.assignments[1]), :]\n\t            if self.extra_clusters > 0:\n\t                self.assignments_t = linear_sum_assignment(self.stats.detach().cpu().t(), maximize=True)\n\t                histogram = self.stats[self.assignments_t[1], :]\n\t                missing = list(set(range(self.n_classes + self.extra_clusters)) - set(self.assignments[0]))\n", "                new_row = self.stats[missing, :].sum(0, keepdim=True)\n\t                histogram = torch.cat([histogram, new_row], axis=0)\n\t                new_col = torch.zeros(self.n_classes + 1, 1, device=histogram.device)\n\t                self.histogram = torch.cat([histogram, new_col], axis=1)\n\t        else:  # linear\n\t            self.assignments = (torch.arange(self.n_classes).unsqueeze(1),\n\t                                torch.arange(self.n_classes).unsqueeze(1))\n\t            self.histogram = self.stats\n\t        tp = torch.diag(self.histogram)\n\t        fp = torch.sum(self.histogram, dim=0) - tp\n", "        fn = torch.sum(self.histogram, dim=1) - tp\n\t        iou = tp / (tp + fp + fn)\n\t        prc = tp / (tp + fn)\n\t        opc = torch.sum(tp) / torch.sum(self.histogram)\n\t        metric_dict = {self.prefix + \"mIoU\": iou[~torch.isnan(iou)].mean().item(),\n\t                       self.prefix + \"Accuracy\": opc.item()}\n\t        return {k: 100 * v for k, v in metric_dict.items()}\n\tdef get_metrics(m1: UnsupervisedMetrics, m2: UnsupervisedMetrics) -> Dict[str, Any]:\n\t    metric_dict_1 = m1.compute()\n\t    metric_dict_2 = m2.compute()\n", "    metrics = all_reduce_dict(metric_dict_1, op=\"mean\")\n\t    tmp = all_reduce_dict(metric_dict_2, op=\"mean\")\n\t    metrics.update(tmp)\n\t    return metrics\n"]}
{"filename": "utils/wandb_utils.py", "chunked_list": ["from typing import Dict, Optional\n\timport os\n\timport wandb\n\t__all__ = [\"set_wandb\"]\n\tdef set_wandb(opt: Dict, local_rank: int = 0, force_mode: Optional[str] = None) -> str:\n\t    if local_rank != 0:\n\t        return \"\"\n\t    # opt = opt\n\t    save_dir = os.path.join(opt[\"output_dir\"], opt[\"wandb\"][\"name\"]) # root save dir\n\t    wandb_mode = opt[\"wandb\"][\"mode\"].lower()\n", "    if force_mode is not None:\n\t        wandb_mode = force_mode.lower()\n\t    if wandb_mode not in (\"online\", \"offline\", \"disabled\"):\n\t        raise ValueError(f\"WandB mode {wandb_mode} invalid.\")\n\t    os.makedirs(save_dir, exist_ok=True)\n\t    wandb_project = opt[\"wandb\"][\"project\"]\n\t    wandb_entity = opt[\"wandb\"][\"entity\"]\n\t    wandb_name = opt[\"wandb\"][\"name\"]\n\t    wandb_id = opt[\"wandb\"].get(\"id\", None)\n\t    wandb_notes = opt[\"wandb\"].get(\"notes\", None)\n", "    wandb_tags = opt[\"wandb\"].get(\"tags\", None)\n\t    if wandb_tags is None:\n\t        wandb_tags = [opt[\"dataset\"][\"data_type\"], ]\n\t    wandb.init(\n\t        project=wandb_project,\n\t        entity=wandb_entity,\n\t        name=wandb_name,\n\t        dir=save_dir,\n\t        resume=\"allow\",\n\t        mode=wandb_mode,\n", "        id=wandb_id,\n\t        notes=wandb_notes,\n\t        tags=wandb_tags,\n\t        config=opt,\n\t    )\n\t    wandb_path = wandb.run.dir if (wandb_mode != \"disabled\") else save_dir\n\t    return wandb_path\n"]}
{"filename": "utils/common_utils.py", "chunked_list": ["from datetime import datetime\n\tfrom typing import Dict\n\timport time\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn.parallel.distributed import DistributedDataParallel\n\timport json\n\timport os\n\tfrom collections import OrderedDict\n\tdef save_checkpoint(prefix: str,\n", "                    net_model, net_optimizer,\n\t                    linear_model, linear_optimizer,\n\t                    cluster_model, cluster_optimizer,\n\t                    current_epoch, current_iter,\n\t                    best_value, save_dir: str,\n\t                    best_epoch=None, best_iter=None,\n\t                    *, model_only: bool = False) -> None:\n\t    model_name = f\"{save_dir}/{prefix}.pth\"\n\t    if isinstance(net_model, DistributedDataParallel):\n\t        net_model = net_model.module\n", "    if isinstance(linear_model, DistributedDataParallel):\n\t        linear_model = linear_model.module\n\t    if isinstance(cluster_model, DistributedDataParallel):\n\t        cluster_model = cluster_model.module\n\t    torch.save(\n\t        {\n\t            'epoch': current_epoch,\n\t            'iter': current_iter,\n\t            'best_epoch': best_epoch if (best_epoch is not None) else current_epoch,\n\t            'best_iter': best_iter if (best_iter is not None) else current_iter,\n", "            'net_model_state_dict': net_model.state_dict(),\n\t            'net_optimizer_state_dict': net_optimizer.state_dict() if (not model_only) else None,\n\t            'linear_model_state_dict': linear_model.state_dict(),\n\t            'linear_optimizer_state_dict': linear_optimizer.state_dict() if (not model_only) else None,\n\t            'cluster_model_state_dict': cluster_model.state_dict(),\n\t            'cluster_optimizer_state_dict': cluster_optimizer.state_dict() if (not model_only) else None,\n\t            'best': best_value,\n\t        }, model_name)\n\tdef parse(json_path: str) -> dict:\n\t    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n", "        opt = json.load(f, object_pairs_hook=OrderedDict)  # noqa\n\t    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n\t    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n\t    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n\t    opt['num_gpus'] = len(opt['gpu_ids'])\n\t    print('export CUDA_VISIBLE_DEVICES=' + gpu_list)\n\t    print('number of GPUs=' + str(opt['num_gpus']))\n\t    os.makedirs(opt[\"output_dir\"], exist_ok=True)\n\t    with open(opt['output_dir'] + '/option.json', 'w', encoding='utf-8') as f:\n\t        json.dump(opt, f, indent=\"\\t\")\n", "    return opt\n\tdef dprint(*args, local_rank: int = 0, **kwargs) -> None:\n\t    if local_rank == 0:\n\t        print(*args, **kwargs)\n\tdef time_log() -> str:\n\t    a = datetime.now()\n\t    return f\"*\" * 48 + f\"  {a.year:>4}/{a.month:>2}/{a.day:>2} | {a.hour:>2}:{a.minute:>2}:{a.second:>2}\\n\"\n\t@torch.no_grad()\n\tdef compute_param_norm(parameters, norm_type: float = 2.0) -> torch.Tensor:\n\t    if isinstance(parameters, torch.Tensor):\n", "        parameters = [parameters]\n\t    parameters = [p for p in parameters if p.requires_grad]\n\t    if len(parameters) == 0:\n\t        return torch.as_tensor(0., dtype=torch.float32)\n\t    device = parameters[0].device\n\t    total_norm = torch.norm(torch.stack([torch.norm(p, norm_type).to(device) for p in parameters]), norm_type)\n\t    return total_norm\n\tdef freeze_bn(model: nn.Module) -> None:\n\t    for m in model.modules():\n\t        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n", "            m.eval()\n\tdef zero_grad_bn(model: nn.Module) -> None:\n\t    for m in model.modules():\n\t        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n\t            for p in m.parameters():\n\t                # p.grad.fill_(0.0)\n\t                p.grad = None\n\tclass RunningAverage:\n\t    def __init__(self):\n\t        self._avg = 0.0\n", "        self._count = 0\n\t    def append(self, value: float) -> None:\n\t        if isinstance(value, torch.Tensor):\n\t            value = value.item()\n\t        self._avg = (value + self._count * self._avg) / (self._count + 1)\n\t        self._count += 1\n\t    @property\n\t    def avg(self) -> float:\n\t        return self._avg\n\t    @property\n", "    def count(self) -> int:\n\t        return self._count\n\t    def reset(self) -> None:\n\t        self._avg = 0.0\n\t        self._count = 0\n\tclass RunningAverageDict:\n\t    def __init__(self):\n\t        self._dict = None\n\t    def update(self, new_dict):\n\t        if self._dict is None:\n", "            self._dict = dict()\n\t            for key, value in new_dict.items():\n\t                self._dict[key] = RunningAverage()\n\t        for key, value in new_dict.items():\n\t            self._dict[key].append(value)\n\t    def get_value(self) -> Dict[str, float]:\n\t        return {key: value.avg for key, value in self._dict.items()}\n\t    def reset(self) -> None:\n\t        if self._dict is None:\n\t            return\n", "        for k in self._dict.keys():\n\t            self._dict[k].reset()\n\tclass Timer:\n\t    def __init__(self):\n\t        self._now = time.process_time()\n\t        # self._now = time.process_time_ns()\n\t    def update(self) -> float:\n\t        current = time.process_time()\n\t        # current = time.process_time_ns()\n\t        duration = current - self._now\n", "        self._now = current\n\t        return duration / 1e6  # ms\n"]}
{"filename": "model/pretrained_download.py", "chunked_list": ["'''\n\t@article{hamilton2022unsupervised,\n\t  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},\n\t  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},\n\t  journal={arXiv preprint arXiv:2203.08414},\n\t  year={2022}\n\t}\n\t'''\n\tfrom os.path import join, exists\n\timport wget\n", "import os\n\tmodels_dir = join(\"./\", \"pretrained_models\")\n\tos.makedirs(models_dir, exist_ok=True)\n\tmodel_url_root = \"https://marhamilresearch4.blob.core.windows.net/stego-public/models/models/\"\n\tmodel_names = [\"moco_v2_800ep_pretrain.pth.tar\",\n\t               \"model_epoch_0720_iter_085000.pth\",\n\t               \"picie.pkl\"]\n\tsaved_models_dir = join(\"./\", \"saved_models\")\n\tos.makedirs(saved_models_dir, exist_ok=True)\n\tsaved_model_url_root = \"https://marhamilresearch4.blob.core.windows.net/stego-public/saved_models/\"\n", "saved_model_names = [\"cityscapes_vit_base_1.ckpt\",\n\t                     \"cocostuff27_vit_base_5.ckpt\",\n\t                     \"picie_and_probes.pth\",\n\t                     \"potsdam_test.ckpt\"]\n\ttarget_files = [join(models_dir, mn) for mn in model_names] + \\\n\t               [join(saved_models_dir, mn) for mn in saved_model_names]\n\ttarget_urls = [model_url_root + mn for mn in model_names] + \\\n\t              [saved_model_url_root + mn for mn in saved_model_names]\n\tfor target_file, target_url in zip(target_files, target_urls):\n\t    if not exists(target_file):\n", "        print(\"\\nDownloading file from {}\".format(target_url))\n\t        wget.download(target_url, target_file)\n\t    else:\n\t        print(\"\\nFound {}, skipping download\".format(target_file))\n"]}
{"filename": "model/STEGO.py", "chunked_list": ["from typing import Tuple\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F  # noqa\n\tfrom model.dino.DinoFeaturizer import DinoFeaturizer\n\tfrom utils.layer_utils import ClusterLookup\n\timport numpy as np\n\timport torch.distributed as dist\n\tfrom utils.dist_utils import all_reduce_tensor, all_gather_tensor\n\tclass STEGOmodel(nn.Module):\n", "    # opt[\"model\"]\n\t    def __init__(self,\n\t                 opt: dict,\n\t                 n_classes:int\n\t                 ):\n\t        super().__init__()\n\t        self.opt = opt\n\t        self.n_classes= n_classes\n\t        if not opt[\"continuous\"]:\n\t            dim = n_classes\n", "        else:\n\t            dim = opt[\"dim\"]\n\t        if opt[\"arch\"] == \"dino\":\n\t            self.net = DinoFeaturizer(dim, opt)\n\t        else:\n\t            raise ValueError(\"Unknown arch {}\".format(opt[\"arch\"]))\n\t        self.cluster_probe = ClusterLookup(dim, n_classes + opt[\"extra_clusters\"])\n\t        self.linear_probe = nn.Conv2d(dim, n_classes, (1, 1))\n\t        self.cluster_probe2 = ClusterLookup(dim, n_classes + opt[\"extra_clusters\"])\n\t        self.linear_probe2 = nn.Conv2d(dim, n_classes, (1, 1))\n", "        self.cluster_probe3 = ClusterLookup(dim, n_classes + opt[\"extra_clusters\"])\n\t        self.linear_probe3 = nn.Conv2d(dim, n_classes, (1, 1))\n\t    def forward(self, x: torch.Tensor):\n\t        return self.net(x)[1]\n\t    @classmethod\n\t    def build(cls, opt, n_classes):\n\t        # opt = opt[\"model\"]\n\t        m = cls(\n\t            opt = opt,\n\t            n_classes= n_classes\n", "        )\n\t        print(f\"Model built! #params {m.count_params()}\")\n\t        return m\n\t    def count_params(self) -> int:\n\t        count = 0\n\t        for p in self.parameters():\n\t            count += p.numel()\n\t        return count\n\tif __name__ == '__main__':\n\t    net = STEGOmodel()\n", "    dummy_input = torch.empty(2, 3, 352, 1216)\n\t    dummy_output = net(dummy_input)[0]\n\t    print(dummy_output.shape)  # (2, 1, 88, 304)\n"]}
{"filename": "model/__init__.py", "chunked_list": []}
{"filename": "model/LambdaLayer.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass LambdaLayer(nn.Module):\n\t    def __init__(self, lambd):\n\t        super(LambdaLayer, self).__init__()\n\t        self.lambd = lambd\n\t    def forward(self, x):\n\t        return self.lambd(x)\n"]}
{"filename": "model/dino/utils.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# \n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t# \n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# \n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\"\n\tMisc functions.\n\tMostly copy-paste from torchvision references or other public repos like DETR:\n\thttps://github.com/facebookresearch/detr/blob/master/util/misc.py\n\t\"\"\"\n\timport os\n\timport sys\n", "import time\n\timport math\n\timport random\n\timport datetime\n\timport subprocess\n\tfrom collections import defaultdict, deque\n\timport numpy as np\n\timport torch\n\tfrom torch import nn\n\timport torch.distributed as dist\n", "from PIL import ImageFilter, ImageOps\n\tclass GaussianBlur(object):\n\t    \"\"\"\n\t    Apply Gaussian Blur to the PIL image.\n\t    \"\"\"\n\t    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n\t        self.prob = p\n\t        self.radius_min = radius_min\n\t        self.radius_max = radius_max\n\t    def __call__(self, img):\n", "        do_it = random.random() <= self.prob\n\t        if not do_it:\n\t            return img\n\t        return img.filter(\n\t            ImageFilter.GaussianBlur(\n\t                radius=random.uniform(self.radius_min, self.radius_max)\n\t            )\n\t        )\n\tclass Solarization(object):\n\t    \"\"\"\n", "    Apply Solarization to the PIL image.\n\t    \"\"\"\n\t    def __init__(self, p):\n\t        self.p = p\n\t    def __call__(self, img):\n\t        if random.random() < self.p:\n\t            return ImageOps.solarize(img)\n\t        else:\n\t            return img\n\tdef load_pretrained_weights(model, pretrained_weights, checkpoint_key, model_name, patch_size):\n", "    if os.path.isfile(pretrained_weights):\n\t        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n\t        if checkpoint_key is not None and checkpoint_key in state_dict:\n\t            print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n\t            state_dict = state_dict[checkpoint_key]\n\t        # remove `module.` prefix\n\t        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n\t        # remove `backbone.` prefix induced by multicrop wrapper\n\t        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n\t        msg = model.load_state_dict(state_dict, strict=False)\n", "        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n\t    else:\n\t        print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n\t        url = None\n\t        if model_name == \"vit_small\" and patch_size == 16:\n\t            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n\t        elif model_name == \"vit_small\" and patch_size == 8:\n\t            url = \"dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\"\n\t        elif model_name == \"vit_base\" and patch_size == 16:\n\t            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n", "        elif model_name == \"vit_base\" and patch_size == 8:\n\t            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n\t        if url is not None:\n\t            print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n\t            state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n\t            model.load_state_dict(state_dict, strict=True)\n\t        else:\n\t            print(\"There is no reference weights available for this model => We use random weights.\")\n\tdef clip_gradients(model, clip):\n\t    norms = []\n", "    for name, p in model.named_parameters():\n\t        if p.grad is not None:\n\t            param_norm = p.grad.data.norm(2)\n\t            norms.append(param_norm.item())\n\t            clip_coef = clip / (param_norm + 1e-6)\n\t            if clip_coef < 1:\n\t                p.grad.data.mul_(clip_coef)\n\t    return norms\n\tdef cancel_gradients_last_layer(epoch, model, freeze_last_layer):\n\t    if epoch >= freeze_last_layer:\n", "        return\n\t    for n, p in model.named_parameters():\n\t        if \"last_layer\" in n:\n\t            p.grad = None\n\tdef restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):\n\t    \"\"\"\n\t    Re-start from checkpoint\n\t    \"\"\"\n\t    if not os.path.isfile(ckp_path):\n\t        return\n", "    print(\"Found checkpoint at {}\".format(ckp_path))\n\t    # open checkpoint file\n\t    checkpoint = torch.load(ckp_path, map_location=\"cpu\")\n\t    # key is what to look for in the checkpoint file\n\t    # value is the object to load\n\t    # example: {'state_dict': model}\n\t    for key, value in kwargs.items():\n\t        if key in checkpoint and value is not None:\n\t            try:\n\t                msg = value.load_state_dict(checkpoint[key], strict=False)\n", "                print(\"=> loaded {} from checkpoint '{}' with msg {}\".format(key, ckp_path, msg))\n\t            except TypeError:\n\t                try:\n\t                    msg = value.load_state_dict(checkpoint[key])\n\t                    print(\"=> loaded {} from checkpoint '{}'\".format(key, ckp_path))\n\t                except ValueError:\n\t                    print(\"=> failed to load {} from checkpoint '{}'\".format(key, ckp_path))\n\t        else:\n\t            print(\"=> failed to load {} from checkpoint '{}'\".format(key, ckp_path))\n\t    # re load variable important for the run\n", "    if run_variables is not None:\n\t        for var_name in run_variables:\n\t            if var_name in checkpoint:\n\t                run_variables[var_name] = checkpoint[var_name]\n\tdef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n\t    warmup_schedule = np.array([])\n\t    warmup_iters = warmup_epochs * niter_per_ep\n\t    if warmup_epochs > 0:\n\t        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n\t    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n", "    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n\t    schedule = np.concatenate((warmup_schedule, schedule))\n\t    assert len(schedule) == epochs * niter_per_ep\n\t    return schedule\n\tdef bool_flag(s):\n\t    \"\"\"\n\t    Parse boolean arguments from the command line.\n\t    \"\"\"\n\t    FALSY_STRINGS = {\"off\", \"false\", \"0\"}\n\t    TRUTHY_STRINGS = {\"on\", \"true\", \"1\"}\n", "    if s.lower() in FALSY_STRINGS:\n\t        return False\n\t    elif s.lower() in TRUTHY_STRINGS:\n\t        return True\n\t    else:\n\t        raise argparse.ArgumentTypeError(\"invalid value for a boolean flag\")\n\tdef fix_random_seeds(seed=31):\n\t    \"\"\"\n\t    Fix random seeds.\n\t    \"\"\"\n", "    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\tclass SmoothedValue(object):\n\t    \"\"\"Track a series of values and provide access to smoothed values over a\n\t    window or the global series average.\n\t    \"\"\"\n\t    def __init__(self, window_size=20, fmt=None):\n\t        if fmt is None:\n\t            fmt = \"{median:.6f} ({global_avg:.6f})\"\n", "        self.deque = deque(maxlen=window_size)\n\t        self.total = 0.0\n\t        self.count = 0\n\t        self.fmt = fmt\n\t    def update(self, value, n=1):\n\t        self.deque.append(value)\n\t        self.count += n\n\t        self.total += value * n\n\t    def synchronize_between_processes(self):\n\t        \"\"\"\n", "        Warning: does not synchronize the deque!\n\t        \"\"\"\n\t        if not is_dist_avail_and_initialized():\n\t            return\n\t        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n\t        dist.barrier()\n\t        dist.all_reduce(t)\n\t        t = t.tolist()\n\t        self.count = int(t[0])\n\t        self.total = t[1]\n", "    @property\n\t    def median(self):\n\t        d = torch.tensor(list(self.deque))\n\t        return d.median().item()\n\t    @property\n\t    def avg(self):\n\t        d = torch.tensor(list(self.deque), dtype=torch.float32)\n\t        return d.mean().item()\n\t    @property\n\t    def global_avg(self):\n", "        return self.total / self.count\n\t    @property\n\t    def max(self):\n\t        return max(self.deque)\n\t    @property\n\t    def value(self):\n\t        return self.deque[-1]\n\t    def __str__(self):\n\t        return self.fmt.format(\n\t            median=self.median,\n", "            avg=self.avg,\n\t            global_avg=self.global_avg,\n\t            max=self.max,\n\t            value=self.value)\n\tdef reduce_dict(input_dict, average=True):\n\t    \"\"\"\n\t    Args:\n\t        input_dict (dict): all the values will be reduced\n\t        average (bool): whether to do average or sum\n\t    Reduce the values in the dictionary from all processes so that all processes\n", "    have the averaged results. Returns a dict with the same fields as\n\t    input_dict, after reduction.\n\t    \"\"\"\n\t    world_size = get_world_size()\n\t    if world_size < 2:\n\t        return input_dict\n\t    with torch.no_grad():\n\t        names = []\n\t        values = []\n\t        # sort the keys so that they are consistent across processes\n", "        for k in sorted(input_dict.keys()):\n\t            names.append(k)\n\t            values.append(input_dict[k])\n\t        values = torch.stack(values, dim=0)\n\t        dist.all_reduce(values)\n\t        if average:\n\t            values /= world_size\n\t        reduced_dict = {k: v for k, v in zip(names, values)}\n\t    return reduced_dict\n\tclass MetricLogger(object):\n", "    def __init__(self, delimiter=\"\\t\"):\n\t        self.meters = defaultdict(SmoothedValue)\n\t        self.delimiter = delimiter\n\t    def update(self, **kwargs):\n\t        for k, v in kwargs.items():\n\t            if isinstance(v, torch.Tensor):\n\t                v = v.item()\n\t            assert isinstance(v, (float, int))\n\t            self.meters[k].update(v)\n\t    def __getattr__(self, attr):\n", "        if attr in self.meters:\n\t            return self.meters[attr]\n\t        if attr in self.__dict__:\n\t            return self.__dict__[attr]\n\t        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\t            type(self).__name__, attr))\n\t    def __str__(self):\n\t        loss_str = []\n\t        for name, meter in self.meters.items():\n\t            loss_str.append(\n", "                \"{}: {}\".format(name, str(meter))\n\t            )\n\t        return self.delimiter.join(loss_str)\n\t    def synchronize_between_processes(self):\n\t        for meter in self.meters.values():\n\t            meter.synchronize_between_processes()\n\t    def add_meter(self, name, meter):\n\t        self.meters[name] = meter\n\t    def log_every(self, iterable, print_freq, header=None):\n\t        i = 0\n", "        if not header:\n\t            header = ''\n\t        start_time = time.time()\n\t        end = time.time()\n\t        iter_time = SmoothedValue(fmt='{avg:.6f}')\n\t        data_time = SmoothedValue(fmt='{avg:.6f}')\n\t        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n\t        if torch.cuda.is_available():\n\t            log_msg = self.delimiter.join([\n\t                header,\n", "                '[{0' + space_fmt + '}/{1}]',\n\t                'eta: {eta}',\n\t                '{meters}',\n\t                'time: {time}',\n\t                'data: {data}',\n\t                'max mem: {memory:.0f}'\n\t            ])\n\t        else:\n\t            log_msg = self.delimiter.join([\n\t                header,\n", "                '[{0' + space_fmt + '}/{1}]',\n\t                'eta: {eta}',\n\t                '{meters}',\n\t                'time: {time}',\n\t                'data: {data}'\n\t            ])\n\t        MB = 1024.0 * 1024.0\n\t        for obj in iterable:\n\t            data_time.update(time.time() - end)\n\t            yield obj\n", "            iter_time.update(time.time() - end)\n\t            if i % print_freq == 0 or i == len(iterable) - 1:\n\t                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n\t                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\t                if torch.cuda.is_available():\n\t                    print(log_msg.format(\n\t                        i, len(iterable), eta=eta_string,\n\t                        meters=str(self),\n\t                        time=str(iter_time), data=str(data_time),\n\t                        memory=torch.cuda.max_memory_allocated() / MB))\n", "                else:\n\t                    print(log_msg.format(\n\t                        i, len(iterable), eta=eta_string,\n\t                        meters=str(self),\n\t                        time=str(iter_time), data=str(data_time)))\n\t            i += 1\n\t            end = time.time()\n\t        total_time = time.time() - start_time\n\t        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t        print('{} Total time: {} ({:.6f} s / it)'.format(\n", "            header, total_time_str, total_time / len(iterable)))\n\tdef get_sha():\n\t    cwd = os.path.dirname(os.path.abspath(__file__))\n\t    def _run(command):\n\t        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n\t    sha = 'N/A'\n\t    diff = \"clean\"\n\t    branch = 'N/A'\n\t    try:\n\t        sha = _run(['git', 'rev-parse', 'HEAD'])\n", "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n\t        diff = _run(['git', 'diff-index', 'HEAD'])\n\t        diff = \"has uncommited changes\" if diff else \"clean\"\n\t        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n\t    except Exception:\n\t        pass\n\t    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n\t    return message\n\tdef is_dist_avail_and_initialized():\n\t    if not dist.is_available():\n", "        return False\n\t    if not dist.is_initialized():\n\t        return False\n\t    return True\n\tdef get_world_size():\n\t    if not is_dist_avail_and_initialized():\n\t        return 1\n\t    return dist.get_world_size()\n\tdef get_rank():\n\t    if not is_dist_avail_and_initialized():\n", "        return 0\n\t    return dist.get_rank()\n\tdef is_main_process():\n\t    return get_rank() == 0\n\tdef save_on_master(*args, **kwargs):\n\t    if is_main_process():\n\t        torch.save(*args, **kwargs)\n\tdef setup_for_distributed(is_master):\n\t    \"\"\"\n\t    This function disables printing when not in master process\n", "    \"\"\"\n\t    import builtins as __builtin__\n\t    builtin_print = __builtin__.print\n\t    def print(*args, **kwargs):\n\t        force = kwargs.pop('force', False)\n\t        if is_master or force:\n\t            builtin_print(*args, **kwargs)\n\t    __builtin__.print = print\n\tdef init_distributed_mode(args):\n\t    # launched with torch.distributed.launch\n", "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n\t        args.rank = int(os.environ[\"RANK\"])\n\t        args.world_size = int(os.environ['WORLD_SIZE'])\n\t        args.gpu = int(os.environ['LOCAL_RANK'])\n\t    # launched with submitit on a slurm cluster\n\t    elif 'SLURM_PROCID' in os.environ:\n\t        args.rank = int(os.environ['SLURM_PROCID'])\n\t        args.gpu = args.rank % torch.cuda.device_count()\n\t    # launched naively with `python main_dino.py`\n\t    # we manually add MASTER_ADDR and MASTER_PORT to env variables\n", "    elif torch.cuda.is_available():\n\t        print('Will run the code on one GPU.')\n\t        args.rank, args.gpu, args.world_size = 0, 0, 1\n\t        os.environ['MASTER_ADDR'] = '127.0.0.1'\n\t        os.environ['MASTER_PORT'] = '29500'\n\t    else:\n\t        print('Does not support training without GPU.')\n\t        sys.exit(1)\n\t    dist.init_process_group(\n\t        backend=\"nccl\",\n", "        init_method=args.dist_url,\n\t        world_size=args.world_size,\n\t        rank=args.rank,\n\t    )\n\t    torch.cuda.set_device(args.gpu)\n\t    print('| distributed init (rank {}): {}'.format(\n\t        args.rank, args.dist_url), flush=True)\n\t    dist.barrier()\n\t    setup_for_distributed(args.rank == 0)\n\tdef accuracy(output, target, topk=(1,)):\n", "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n\t    maxk = max(topk)\n\t    batch_size = target.size(0)\n\t    _, pred = output.topk(maxk, 1, True, True)\n\t    pred = pred.t()\n\t    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n\t    return [correct[:k].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]\n\tdef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n\t    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n\t    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n", "    def norm_cdf(x):\n\t        # Computes standard normal cumulative distribution function\n\t        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\t    if (mean < a - 2 * std) or (mean > b + 2 * std):\n\t        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n\t                      \"The distribution of values may be incorrect.\",\n\t                      stacklevel=2)\n\t    with torch.no_grad():\n\t        # Values are generated by using a truncated uniform distribution and\n\t        # then using the inverse CDF for the normal distribution.\n", "        # Get upper and lower cdf values\n\t        l = norm_cdf((a - mean) / std)\n\t        u = norm_cdf((b - mean) / std)\n\t        # Uniformly fill tensor with values from [l, u], then translate to\n\t        # [2l-1, 2u-1].\n\t        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\t        # Use inverse cdf transform for normal distribution to get truncated\n\t        # standard normal\n\t        tensor.erfinv_()\n\t        # Transform to proper mean, std\n", "        tensor.mul_(std * math.sqrt(2.))\n\t        tensor.add_(mean)\n\t        # Clamp to ensure it's in the proper range\n\t        tensor.clamp_(min=a, max=b)\n\t        return tensor\n\tdef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # type: (Tensor, float, float, float, float) -> Tensor\n\t    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\tclass LARS(torch.optim.Optimizer):\n\t    \"\"\"\n", "    Almost copy-paste from https://github.com/facebookresearch/barlowtwins/blob/main/main.py\n\t    \"\"\"\n\t    def __init__(self, params, lr=0, weight_decay=0, momentum=0.9, eta=0.001,\n\t                 weight_decay_filter=None, lars_adaptation_filter=None):\n\t        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\n\t                        eta=eta, weight_decay_filter=weight_decay_filter,\n\t                        lars_adaptation_filter=lars_adaptation_filter)\n\t        super().__init__(params, defaults)\n\t    @torch.no_grad()\n\t    def step(self):\n", "        for g in self.param_groups:\n\t            for p in g['params']:\n\t                dp = p.grad\n\t                if dp is None:\n\t                    continue\n\t                if p.ndim != 1:\n\t                    dp = dp.add(p, alpha=g['weight_decay'])\n\t                if p.ndim != 1:\n\t                    param_norm = torch.norm(p)\n\t                    update_norm = torch.norm(dp)\n", "                    one = torch.ones_like(param_norm)\n\t                    q = torch.where(param_norm > 0.,\n\t                                    torch.where(update_norm > 0,\n\t                                                (g['eta'] * param_norm / update_norm), one), one)\n\t                    dp = dp.mul(q)\n\t                param_state = self.state[p]\n\t                if 'mu' not in param_state:\n\t                    param_state['mu'] = torch.zeros_like(p)\n\t                mu = param_state['mu']\n\t                mu.mul_(g['momentum']).add_(dp)\n", "                p.add_(mu, alpha=-g['lr'])\n\tclass MultiCropWrapper(nn.Module):\n\t    \"\"\"\n\t    Perform forward pass separately on each resolution input.\n\t    The inputs corresponding to a single resolution are clubbed and single\n\t    forward is run on the same resolution inputs. Hence we do several\n\t    forward passes = number of different resolutions used. We then\n\t    concatenate all the output features and run the head forward on these\n\t    concatenated features.\n\t    \"\"\"\n", "    def __init__(self, backbone, head):\n\t        super(MultiCropWrapper, self).__init__()\n\t        # disable layers dedicated to ImageNet labels classification\n\t        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n\t        self.backbone = backbone\n\t        self.head = head\n\t    def forward(self, x):\n\t        # convert to list\n\t        if not isinstance(x, list):\n\t            x = [x]\n", "        idx_crops = torch.cumsum(torch.unique_consecutive(\n\t            torch.tensor([inp.shape[-1] for inp in x]),\n\t            return_counts=True,\n\t        )[1], 0)\n\t        start_idx = 0\n\t        for end_idx in idx_crops:\n\t            _out = self.backbone(torch.cat(x[start_idx: end_idx]))\n\t            if start_idx == 0:\n\t                output = _out\n\t            else:\n", "                output = torch.cat((output, _out))\n\t            start_idx = end_idx\n\t        # Run the head forward on the concatenated features.\n\t        return self.head(output)\n\tdef get_params_groups(model):\n\t    regularized = []\n\t    not_regularized = []\n\t    for name, param in model.named_parameters():\n\t        if not param.requires_grad:\n\t            continue\n", "        # we do not regularize biases nor Norm parameters\n\t        if name.endswith(\".bias\") or len(param.shape) == 1:\n\t            not_regularized.append(param)\n\t        else:\n\t            regularized.append(param)\n\t    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]\n\tdef has_batchnorms(model):\n\t    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)\n\t    for name, module in model.named_modules():\n\t        if isinstance(module, bn_types):\n", "            return True\n\t    return False\n"]}
{"filename": "model/dino/DinoFeaturizer.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport model.dino.vision_transformer as vits\n\tclass DinoFeaturizer(nn.Module):\n\t    def __init__(self, dim, cfg):  # cfg[\"pretrained\"]\n\t        super().__init__()\n\t        self.cfg = cfg\n\t        self.dim = dim\n\t        patch_size = self.cfg[\"pretrained\"][\"dino_patch_size\"]\n\t        self.patch_size = patch_size\n", "        self.feat_type = self.cfg[\"pretrained\"][\"dino_feat_type\"]\n\t        arch = self.cfg[\"pretrained\"][\"model_type\"]\n\t        self.model = vits.__dict__[arch](\n\t            patch_size=patch_size,\n\t            num_classes=0)\n\t        self.n_feats = 384\n\t        self.const = 28\n\t        for p in self.model.parameters():\n\t            p.requires_grad = False\n\t        self.model.eval().cuda()\n", "        self.dropout = torch.nn.Dropout2d(p=.1)\n\t        if arch == \"vit_small\" and patch_size == 16:\n\t            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n\t        elif arch == \"vit_small\" and patch_size == 8:\n\t            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n\t        elif arch == \"vit_base\" and patch_size == 16:\n\t            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n\t        elif arch == \"vit_base\" and patch_size == 8:\n\t            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n\t        else:\n", "            raise ValueError(\"Unknown arch and patch size\")\n\t        if arch == \"vit_small\" and patch_size == 16:\n\t            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n\t        elif arch == \"vit_small\" and patch_size == 8:\n\t            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n\t        elif arch == \"vit_base\" and patch_size == 16:\n\t            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n\t        elif arch == \"vit_base\" and patch_size == 8:\n\t            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n\t        else:\n", "            raise ValueError(\"Unknown arch and patch size\")\n\t        if cfg[\"pretrained\"][\"pretrained_weights\"] is not None:\n\t            state_dict = torch.load(cfg[\"pretrained\"][\"pretrained_weights\"], map_location=\"cpu\")\n\t            state_dict = state_dict[\"teacher\"]\n\t            # remove `module.` prefix\n\t            state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n\t            # remove `backbone.` prefix induced by multicrop wrapper\n\t            state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n\t            msg = self.model.load_state_dict(state_dict, strict=False)\n\t            print('Pretrained weights found at {} and loaded with msg: {}'.format(\n", "                cfg[\"pretrained\"][\"pretrained_weights\"], msg))\n\t        else:\n\t            print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n\t            state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n\t            self.model.load_state_dict(state_dict, strict=True)\n\t        if arch == \"vit_small\":\n\t            self.n_feats = 384\n\t        else:\n\t            self.n_feats = 768\n\t        self.cluster1 = self.make_clusterer(self.n_feats)\n", "        self.proj_type = cfg[\"pretrained\"][\"projection_type\"]\n\t        if self.proj_type == \"nonlinear\":\n\t            self.cluster2 = self.make_nonlinear_clusterer(self.n_feats)\n\t        self.ema_model1 = self.make_clusterer(self.n_feats)\n\t        self.ema_model2 = self.make_nonlinear_clusterer(self.n_feats)\n\t        for param_q, param_k in zip(self.cluster1.parameters(), self.ema_model1.parameters()):\n\t            param_k.data.copy_(param_q.detach().data)  # initialize\n\t            param_k.requires_grad = False  # not update by gradient for eval_net\n\t        self.ema_model1.cuda()\n\t        self.ema_model1.eval()\n", "        for param_q, param_k in zip(self.cluster2.parameters(), self.ema_model2.parameters()):\n\t            param_k.data.copy_(param_q.detach().data)  # initialize\n\t            param_k.requires_grad = False  # not update by gradient for eval_net\n\t        self.ema_model2.cuda()\n\t        self.ema_model2.eval()\n\t        sz = cfg[\"spatial_size\"]\n\t        self.index_mask = torch.zeros((sz*sz, sz*sz), dtype=torch.float16)\n\t        self.divide_num = torch.zeros((sz*sz), dtype=torch.long)\n\t        for _im in range(sz*sz):\n\t            if _im == 0:\n", "                index_set = torch.tensor([_im, _im+1, _im+sz, _im+(sz+1)])\n\t            elif _im==(sz-1):\n\t                index_set = torch.tensor([_im-1, _im, _im+(sz-1), _im+sz])\n\t            elif _im==(sz*sz-sz):\n\t                index_set = torch.tensor([_im-sz, _im-(sz-1), _im, _im+1])\n\t            elif _im==(sz*sz-1):\n\t                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-1, _im])\n\t            elif ((1 <= _im) and (_im <= (sz-2))):\n\t                index_set = torch.tensor([_im-1, _im, _im+1, _im+(sz-1), _im+sz, _im+(sz+1)])\n\t            elif (((sz*sz-sz+1) <= _im) and (_im <= (sz*sz-2))):\n", "                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-(sz-1), _im-1, _im, _im+1])\n\t            elif (_im % sz == 0):\n\t                index_set = torch.tensor([_im-sz, _im-(sz-1), _im, _im+1, _im+sz, _im+(sz+1)])\n\t            elif ((_im+1) % sz == 0):\n\t                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-1, _im, _im+(sz-1), _im+sz])\n\t            else:\n\t                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-(sz-1), _im-1, _im, _im+1, _im+(sz-1), _im+sz, _im+(sz+1)])\n\t            self.index_mask[_im][index_set] = 1.\n\t            self.divide_num[_im] = index_set.size(0)\n\t        self.index_mask = self.index_mask.cuda()\n", "        self.divide_num = self.divide_num.unsqueeze(1)\n\t        self.divide_num = self.divide_num.cuda()\n\t    def make_clusterer(self, in_channels):\n\t        return torch.nn.Sequential(\n\t            torch.nn.Conv2d(in_channels, self.dim, (1, 1)))\n\t    def make_nonlinear_clusterer(self, in_channels):\n\t        return torch.nn.Sequential(\n\t            torch.nn.Conv2d(in_channels, in_channels, (1, 1)),\n\t            torch.nn.ReLU(),\n\t            torch.nn.Conv2d(in_channels, self.dim, (1, 1)))\n", "    def make_nonlinear_clusterer_layer3(self, in_channels):\n\t        return torch.nn.Sequential(\n\t            torch.nn.Conv2d(in_channels, in_channels, (1, 1)),\n\t            torch.nn.ReLU(),\n\t            torch.nn.Conv2d(in_channels, in_channels, (1, 1)),\n\t            torch.nn.ReLU(),\n\t            torch.nn.Conv2d(in_channels, self.dim, (1, 1)))\n\t    @torch.no_grad()\n\t    def ema_model_update(self, model, ema_model, ema_m):\n\t        \"\"\"\n", "        Momentum update of evaluation model (exponential moving average)\n\t        \"\"\"\n\t        for param_train, param_eval in zip(model.parameters(), ema_model.parameters()):\n\t            param_eval.copy_(param_eval * ema_m + param_train.detach() * (1 - ema_m))\n\t        for buffer_train, buffer_eval in zip(model.buffers(), ema_model.buffers()):\n\t            buffer_eval.copy_(buffer_train)\n\t    def forward(self, img, n=1, return_class_feat=False, train=False):\n\t        self.model.eval()\n\t        batch_size = img.shape[0]\n\t        with torch.no_grad():\n", "            assert (img.shape[2] % self.patch_size == 0)\n\t            assert (img.shape[3] % self.patch_size == 0)\n\t            # get selected layer activations\n\t            feat, attn, qkv = self.model.get_intermediate_feat(img, n=n)\n\t            feat, attn, qkv = feat[0], attn[0], qkv[0]\n\t            if train==True:\n\t                attn = attn[:, :, 1:, 1:]\n\t                attn = torch.mean(attn, dim=1)\n\t                attn = attn.type(torch.float32)\n\t                attn_max = torch.quantile(attn, 0.9, dim=2, keepdim=True)\n", "                attn_min = torch.quantile(attn, 0.1, dim=2, keepdim=True)\n\t                attn = torch.max(torch.min(attn, attn_max), attn_min)\n\t                attn = attn.softmax(dim=-1)\n\t                attn = attn*self.const\n\t                attn[attn < torch.mean(attn, dim=2, keepdim=True)] = 0.\n\t            feat_h = img.shape[2] // self.patch_size\n\t            feat_w = img.shape[3] // self.patch_size\n\t            if self.feat_type == \"feat\":\n\t                image_feat = feat[:, 1:, :].reshape(feat.shape[0], feat_h, feat_w, -1).permute(0, 3, 1, 2)\n\t            elif self.feat_type == \"KK\":\n", "                image_k = qkv[1, :, :, 1:, :].reshape(feat.shape[0], 6, feat_h, feat_w, -1)\n\t                B, H, I, J, D = image_k.shape\n\t                image_feat = image_k.permute(0, 1, 4, 2, 3).reshape(B, H * D, I, J)\n\t            else:\n\t                raise ValueError(\"Unknown feat type:{}\".format(self.feat_type))\n\t            if return_class_feat:\n\t                return feat[:, :1, :].reshape(feat.shape[0], 1, 1, -1).permute(0, 3, 1, 2)\n\t        if self.proj_type is not None:\n\t            code = self.cluster1(self.dropout(image_feat))\n\t            code_ema = self.ema_model1(self.dropout(image_feat))\n", "            if self.proj_type == \"nonlinear\":\n\t                code += self.cluster2(self.dropout(image_feat))\n\t                code_ema += self.ema_model2(self.dropout(image_feat))\n\t        else:\n\t            code = image_feat\n\t        if train==True:\n\t            attn = attn * self.index_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n\t            code_clone = code.clone()\n\t            code_clone = code_clone.view(code_clone.size(0), code_clone.size(1), -1)\n\t            code_clone = code_clone.permute(0,2,1)\n", "            code_3x3_all = []\n\t            for bs in range(batch_size):\n\t                code_3x3 = attn[bs].unsqueeze(-1) * code_clone[bs].unsqueeze(0)\n\t                code_3x3 = torch.sum(code_3x3, dim=1)\n\t                code_3x3 = code_3x3 / self.divide_num\n\t                code_3x3_all.append(code_3x3)\n\t            code_3x3_all = torch.stack(code_3x3_all)\n\t            code_3x3_all = code_3x3_all.permute(0,2,1).view(code.size(0), code.size(1), code.size(2), code.size(3))\n\t        if train==True:\n\t            with torch.no_grad():\n", "                self.ema_model_update(self.cluster1, self.ema_model1, self.cfg[\"ema_m\"])\n\t                self.ema_model_update(self.cluster2, self.ema_model2, self.cfg[\"ema_m\"])\n\t        if train==True:\n\t            if self.cfg[\"pretrained\"][\"dropout\"]:\n\t                return self.dropout(image_feat), code, self.dropout(code_ema), self.dropout(code_3x3_all)\n\t            else:\n\t                return image_feat, code, code_ema, code_3x3_all\n\t        else:\n\t            if self.cfg[\"pretrained\"][\"dropout\"]:\n\t                return self.dropout(image_feat), code, self.dropout(code_ema)\n", "            else:\n\t                return image_feat, code, code_ema\n"]}
{"filename": "model/dino/vision_transformer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# \n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t# \n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# \n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\"\n\tMostly copy-paste from timm library.\n\thttps://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n\t\"\"\"\n\timport math\n\timport warnings\n\tfrom functools import partial\n", "import torch\n\timport torch.nn as nn\n\tfrom model.dino.utils import trunc_normal_\n\tdef drop_path(x, drop_prob: float = 0., training: bool = False):\n\t    if drop_prob == 0. or not training:\n\t        return x\n\t    keep_prob = 1 - drop_prob\n\t    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n\t    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n\t    random_tensor.floor_()  # binarize\n", "    output = x.div(keep_prob) * random_tensor\n\t    return output\n\tclass DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\t    \"\"\"\n\t    def __init__(self, drop_prob=None):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n\t    def forward(self, x):\n\t        return drop_path(x, self.drop_prob, self.training)\n", "class Mlp(nn.Module):\n\t    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t    def forward(self, x):\n", "        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n\t        x = self.fc2(x)\n\t        x = self.drop(x)\n\t        return x\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n", "        head_dim = dim // num_heads\n\t        self.scale = qk_scale or head_dim ** -0.5\n\t        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t    def forward(self, x, return_qkv=False):\n\t        B, N, C = x.shape\n\t        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[2]\n", "        attn = (q @ k.transpose(-2, -1)) * self.scale\n\t        attn_hook = attn.clone()\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x, attn_hook, qkv\n\tclass Block(nn.Module):\n\t    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n", "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n\t        super().__init__()\n\t        self.norm1 = norm_layer(dim)\n\t        self.attn = Attention(\n\t            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\t        self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\t    def forward(self, x, return_attention=False, return_qkv=False):\n", "        y, attn, qkv = self.attn(self.norm1(x))\n\t        if return_attention:\n\t            return attn\n\t        x = x + self.drop_path(y)\n\t        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        if return_qkv:\n\t            return x, attn, qkv\n\t        return x\n\tclass PatchEmbed(nn.Module):\n\t    \"\"\" Image to Patch Embedding\n", "    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n\t        super().__init__()\n\t        num_patches = (img_size // patch_size) * (img_size // patch_size)\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n\t        self.num_patches = num_patches\n\t        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\t    def forward(self, x):\n\t        B, C, H, W = x.shape\n", "        x = self.proj(x).flatten(2).transpose(1, 2)\n\t        return x\n\tclass VisionTransformer(nn.Module):\n\t    \"\"\" Vision Transformer \"\"\"\n\t    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n\t                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n\t                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n\t        super().__init__()\n\t        self.num_features = self.embed_dim = embed_dim\n\t        self.patch_embed = PatchEmbed(\n", "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\t        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\t        self.blocks = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n", "            for i in range(depth)])\n\t        self.norm = norm_layer(embed_dim)\n\t        # Classifier head\n\t        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        trunc_normal_(self.pos_embed, std=.02)\n\t        trunc_normal_(self.cls_token, std=.02)\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n", "            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def interpolate_pos_encoding(self, x, w, h):\n\t        npatch = x.shape[1] - 1\n\t        N = self.pos_embed.shape[1] - 1\n\t        if npatch == N and w == h:\n\t            return self.pos_embed\n", "        class_pos_embed = self.pos_embed[:, 0]\n\t        patch_pos_embed = self.pos_embed[:, 1:]\n\t        dim = x.shape[-1]\n\t        w0 = w // self.patch_embed.patch_size\n\t        h0 = h // self.patch_embed.patch_size\n\t        # we add a small number to avoid floating point error in the interpolation\n\t        # see discussion at https://github.com/facebookresearch/dino/issues/8\n\t        w0, h0 = w0 + 0.1, h0 + 0.1\n\t        patch_pos_embed = nn.functional.interpolate(\n\t            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n", "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n\t            mode='bicubic',\n\t        )\n\t        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n\t        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n\t        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\t    def prepare_tokens(self, x):\n\t        B, nc, w, h = x.shape\n\t        x = self.patch_embed(x)  # patch linear embedding\n\t        # add the [CLS] token to the embed patch tokens\n", "        cls_tokens = self.cls_token.expand(B, -1, -1)\n\t        x = torch.cat((cls_tokens, x), dim=1)\n\t        # add positional encoding to each token\n\t        x = x + self.interpolate_pos_encoding(x, w, h)\n\t        return self.pos_drop(x)\n\t    def forward(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for blk in self.blocks:\n\t            x = blk(x)\n\t        x = self.norm(x)\n", "        return x[:, 0]\n\t    def forward_feats(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for blk in self.blocks:\n\t            x = blk(x)\n\t        x = self.norm(x)\n\t        return x\n\t    def get_intermediate_feat(self, x, n=1):\n\t        x = self.prepare_tokens(x)\n\t        # we return the output tokens from the `n` last blocks\n", "        feat = []\n\t        attns = []\n\t        qkvs = []\n\t        for i, blk in enumerate(self.blocks):\n\t            x, attn, qkv = blk(x, return_qkv=True)\n\t            if len(self.blocks) - i <= n:\n\t                feat.append(self.norm(x))\n\t                qkvs.append(qkv)\n\t                attns.append(attn)\n\t        return feat, attns, qkvs\n", "    def get_last_selfattention(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for i, blk in enumerate(self.blocks):\n\t            if i < len(self.blocks) - 1:\n\t                x = blk(x)\n\t            else:\n\t                # return attention of the last block\n\t                return blk(x, return_attention=True)\n\t    def get_intermediate_layers(self, x, n=1):\n\t        x = self.prepare_tokens(x)\n", "        # we return the output tokens from the `n` last blocks\n\t        output = []\n\t        for i, blk in enumerate(self.blocks):\n\t            x = blk(x)\n\t            if len(self.blocks) - i <= n:\n\t                output.append(self.norm(x))\n\t        return output\n\tdef vit_tiny(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n", "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tdef vit_small(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tdef vit_base(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n", "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tclass DINOHead(nn.Module):\n\t    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048,\n\t                 bottleneck_dim=256):\n\t        super().__init__()\n\t        nlayers = max(nlayers, 1)\n\t        if nlayers == 1:\n\t            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n\t        else:\n", "            layers = [nn.Linear(in_dim, hidden_dim)]\n\t            if use_bn:\n\t                layers.append(nn.BatchNorm1d(hidden_dim))\n\t            layers.append(nn.GELU())\n\t            for _ in range(nlayers - 2):\n\t                layers.append(nn.Linear(hidden_dim, hidden_dim))\n\t                if use_bn:\n\t                    layers.append(nn.BatchNorm1d(hidden_dim))\n\t                layers.append(nn.GELU())\n\t            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n", "            self.mlp = nn.Sequential(*layers)\n\t        self.apply(self._init_weights)\n\t        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n\t        self.last_layer.weight_g.data.fill_(1)\n\t        if norm_last_layer:\n\t            self.last_layer.weight_g.requires_grad = False\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n", "                nn.init.constant_(m.bias, 0)\n\t    def forward(self, x):\n\t        x = self.mlp(x)\n\t        x = nn.functional.normalize(x, dim=-1, p=2)\n\t        x = self.last_layer(x)\n\t        return x\n"]}
{"filename": "dataset/__init__.py", "chunked_list": []}
{"filename": "dataset/data.py", "chunked_list": ["import os\n\timport random\n\tfrom os.path import join\n\timport numpy as np\n\timport torch.multiprocessing\n\timport torchvision.transforms as T\n\tfrom PIL import Image\n\tfrom scipy.io import loadmat\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.data import Dataset\n", "from torchvision.datasets.cityscapes import Cityscapes\n\tfrom torchvision.transforms.functional import to_pil_image\n\tfrom tqdm import tqdm\n\timport torchvision.transforms as transforms\n\tfrom torchvision.utils import save_image\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\tdef get_transform(res, is_label, crop_type):\n\t    if crop_type == \"center\":\n\t        cropper = T.CenterCrop(res)\n", "    elif crop_type == \"random\":\n\t        cropper = T.RandomCrop(res)\n\t    elif crop_type is None:\n\t        cropper = T.Lambda(lambda x: x)\n\t        res = (res, res)\n\t    else:\n\t        raise ValueError(\"Unknown Cropper {}\".format(crop_type))\n\t    if is_label:\n\t        return T.Compose([T.Resize(res, Image.NEAREST),\n\t                          cropper,\n", "                          ToTargetTensor()])\n\t    else:\n\t        return T.Compose([T.Resize(res, Image.NEAREST),\n\t                          cropper,\n\t                          T.ToTensor(),\n\t                          normalize])\n\tnormalize = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\tclass ToTargetTensor(object):\n\t    def __call__(self, target):\n\t        return torch.as_tensor(np.array(target), dtype=torch.int64).unsqueeze(0)\n", "def bit_get(val, idx):\n\t    \"\"\"Gets the bit value.\n\t    Args:\n\t      val: Input value, int or numpy int array.\n\t      idx: Which bit of the input val.\n\t    Returns:\n\t      The \"idx\"-th bit of input val.\n\t    \"\"\"\n\t    return (val >> idx) & 1\n\tdef create_pascal_label_colormap():\n", "    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n\t    Returns:\n\t      A colormap for visualizing segmentation results.\n\t    \"\"\"\n\t    colormap = np.zeros((512, 3), dtype=int)\n\t    ind = np.arange(512, dtype=int)\n\t    for shift in reversed(list(range(8))):\n\t        for channel in range(3):\n\t            colormap[:, channel] |= bit_get(ind, channel) << shift\n\t        ind >>= 3\n", "    return colormap\n\tdef create_cityscapes_colormap():\n\t    colors = [(128, 64, 128),\n\t              (244, 35, 232),\n\t              (250, 170, 160),\n\t              (230, 150, 140),\n\t              (70, 70, 70),\n\t              (102, 102, 156),\n\t              (190, 153, 153),\n\t              (180, 165, 180),\n", "              (150, 100, 100),\n\t              (150, 120, 90),\n\t              (153, 153, 153),\n\t              (153, 153, 153),\n\t              (250, 170, 30),\n\t              (220, 220, 0),\n\t              (107, 142, 35),\n\t              (152, 251, 152),\n\t              (70, 130, 180),\n\t              (220, 20, 60),\n", "              (255, 0, 0),\n\t              (0, 0, 142),\n\t              (0, 0, 70),\n\t              (0, 60, 100),\n\t              (0, 0, 90),\n\t              (0, 0, 110),\n\t              (0, 80, 100),\n\t              (0, 0, 230),\n\t              (119, 11, 32),\n\t              (0, 0, 0)]\n", "    return np.array(colors)\n\tclass DirectoryDataset(Dataset):\n\t    def __init__(self, root, path, image_set, transform, target_transform):\n\t        super(DirectoryDataset, self).__init__()\n\t        self.split = image_set\n\t        self.dir = join(root, path)\n\t        self.img_dir = join(self.dir, \"imgs\", self.split)\n\t        self.label_dir = join(self.dir, \"labels\", self.split)\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n", "        self.img_files = np.array(sorted(os.listdir(self.img_dir)))\n\t        assert len(self.img_files) > 0\n\t        if os.path.exists(join(self.dir, \"labels\")):\n\t            self.label_files = np.array(sorted(os.listdir(self.label_dir)))\n\t            assert len(self.img_files) == len(self.label_files)\n\t        else:\n\t            self.label_files = None\n\t    def __getitem__(self, index):\n\t        image_fn = self.img_files[index]\n\t        img = Image.open(join(self.img_dir, image_fn))\n", "        if self.label_files is not None:\n\t            label_fn = self.label_files[index]\n\t            label = Image.open(join(self.label_dir, label_fn))\n\t        seed = random.randint(0, 2147483647)\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        img = self.transform(img)\n\t        if self.label_files is not None:\n\t            random.seed(seed)\n\t            torch.manual_seed(seed)\n", "            label = self.target_transform(label)\n\t        else:\n\t            label = torch.zeros(img.shape[1], img.shape[2], dtype=torch.int64) - 1\n\t        mask = (label > 0).to(torch.float32)\n\t        return img, label, mask\n\t    def __len__(self):\n\t        return len(self.img_files)\n\tclass Potsdam(Dataset):\n\t    def __init__(self, root, image_set, transform, target_transform, coarse_labels):\n\t        super(Potsdam, self).__init__()\n", "        self.split = image_set\n\t        self.root = root\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n\t        split_files = {\n\t            \"train\": [\"labelled_train.txt\"],\n\t            \"unlabelled_train\": [\"unlabelled_train.txt\"],\n\t            # \"train\": [\"unlabelled_train.txt\"],\n\t            \"val\": [\"labelled_test.txt\"],\n\t            \"train+val\": [\"labelled_train.txt\", \"labelled_test.txt\"],\n", "            \"all\": [\"all.txt\"]\n\t        }\n\t        assert self.split in split_files.keys()\n\t        self.files = []\n\t        for split_file in split_files[self.split]:\n\t            with open(join(self.root, split_file), \"r\") as f:\n\t                self.files.extend(fn.rstrip() for fn in f.readlines())\n\t        self.coarse_labels = coarse_labels\n\t        self.fine_to_coarse = {0: 0, 4: 0,  # roads and cars\n\t                               1: 1, 5: 1,  # buildings and clutter\n", "                               2: 2, 3: 2,  # vegetation and trees\n\t                               255: -1\n\t                               }\n\t    def __getitem__(self, index):\n\t        image_id = self.files[index]\n\t        img = loadmat(join(self.root, \"imgs\", image_id + \".mat\"))[\"img\"]\n\t        img = to_pil_image(torch.from_numpy(img).permute(2, 0, 1)[:3])\n\t        try:\n\t            label = loadmat(join(self.root, \"gt\", image_id + \".mat\"))[\"gt\"]\n\t            label = to_pil_image(torch.from_numpy(label).unsqueeze(-1).permute(2, 0, 1))\n", "        except FileNotFoundError:\n\t            label = to_pil_image(torch.ones(1, img.height, img.width))\n\t        seed = random.randint(0, 2147483647)\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        img = self.transform(img)\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        label = self.target_transform(label).squeeze(0)\n\t        if self.coarse_labels:\n", "            new_label_map = torch.zeros_like(label)\n\t            for fine, coarse in self.fine_to_coarse.items():\n\t                new_label_map[label == fine] = coarse\n\t            label = new_label_map\n\t        mask = (label > 0).to(torch.float32)\n\t        return img, label, mask\n\t    def __len__(self):\n\t        return len(self.files)\n\tclass PotsdamRaw(Dataset):\n\t    def __init__(self, root, image_set, transform, target_transform, coarse_labels):\n", "        super(PotsdamRaw, self).__init__()\n\t        self.split = image_set\n\t        self.root = os.path.join(root, \"processed\")\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n\t        self.files = []\n\t        for im_num in range(38):\n\t            for i_h in range(15):\n\t                for i_w in range(15):\n\t                    self.files.append(\"{}_{}_{}.mat\".format(im_num, i_h, i_w))\n", "        self.coarse_labels = coarse_labels\n\t        self.fine_to_coarse = {0: 0, 4: 0,  # roads and cars\n\t                               1: 1, 5: 1,  # buildings and clutter\n\t                               2: 2, 3: 2,  # vegetation and trees\n\t                               255: -1\n\t                               }\n\t    def __getitem__(self, index):\n\t        image_id = self.files[index]\n\t        img = loadmat(join(self.root, \"imgs\", image_id))[\"img\"]\n\t        img = to_pil_image(torch.from_numpy(img).permute(2, 0, 1)[:3])\n", "        try:\n\t            label = loadmat(join(self.root, \"gt\", image_id))[\"gt\"]\n\t            label = to_pil_image(torch.from_numpy(label).unsqueeze(-1).permute(2, 0, 1))\n\t        except FileNotFoundError:\n\t            label = to_pil_image(torch.ones(1, img.height, img.width))\n\t        seed = random.randint(0, 2147483647)\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        img = self.transform(img)\n\t        random.seed(seed)\n", "        torch.manual_seed(seed)\n\t        label = self.target_transform(label).squeeze(0)\n\t        if self.coarse_labels:\n\t            new_label_map = torch.zeros_like(label)\n\t            for fine, coarse in self.fine_to_coarse.items():\n\t                new_label_map[label == fine] = coarse\n\t            label = new_label_map\n\t        mask = (label > 0).to(torch.float32)\n\t        return img, label, mask\n\t    def __len__(self):\n", "        return len(self.files)\n\tclass Coco(Dataset):\n\t    def __init__(self, root, image_set, transform, target_transform,\n\t                 coarse_labels, exclude_things, subset=None):\n\t        super(Coco, self).__init__()\n\t        self.split = image_set\n\t        self.root = root\n\t        self.coarse_labels = coarse_labels\n\t        self.transform = transform\n\t        self.label_transform = target_transform\n", "        self.subset = subset\n\t        self.exclude_things = exclude_things\n\t        if self.subset is None:\n\t            self.image_list = \"Coco164kFull_Stuff_Coarse.txt\"\n\t        elif self.subset == 6:  # IIC Coarse\n\t            self.image_list = \"Coco164kFew_Stuff_6.txt\"\n\t        elif self.subset == 7:  # IIC Fine\n\t            self.image_list = \"Coco164kFull_Stuff_Coarse_7.txt\"\n\t        assert self.split in [\"train\", \"val\", \"train+val\"]\n\t        split_dirs = {\n", "            \"train\": [\"train2017\"],\n\t            \"val\": [\"val2017\"],\n\t            \"train+val\": [\"train2017\", \"val2017\"]\n\t        }\n\t        self.image_files = []\n\t        self.label_files = []\n\t        for split_dir in split_dirs[self.split]:\n\t            with open(join(self.root, \"curated\", split_dir, self.image_list), \"r\") as f:\n\t                img_ids = [fn.rstrip() for fn in f.readlines()]\n\t                for img_id in img_ids:\n", "                    self.image_files.append(join(self.root, \"images\", split_dir, img_id + \".jpg\"))\n\t                    self.label_files.append(join(self.root, \"annotations\", split_dir, img_id + \".png\"))\n\t        self.fine_to_coarse = {0: 9, 1: 11, 2: 11, 3: 11, 4: 11, 5: 11, 6: 11, 7: 11, 8: 11, 9: 8, 10: 8, 11: 8, 12: 8,\n\t                               13: 8, 14: 8, 15: 7, 16: 7, 17: 7, 18: 7, 19: 7, 20: 7, 21: 7, 22: 7, 23: 7, 24: 7,\n\t                               25: 6, 26: 6, 27: 6, 28: 6, 29: 6, 30: 6, 31: 6, 32: 6, 33: 10, 34: 10, 35: 10, 36: 10,\n\t                               37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 5, 44: 5, 45: 5, 46: 5, 47: 5, 48: 5,\n\t                               49: 5, 50: 5, 51: 2, 52: 2, 53: 2, 54: 2, 55: 2, 56: 2, 57: 2, 58: 2, 59: 2, 60: 2,\n\t                               61: 3, 62: 3, 63: 3, 64: 3, 65: 3, 66: 3, 67: 3, 68: 3, 69: 3, 70: 3, 71: 0, 72: 0,\n\t                               73: 0, 74: 0, 75: 0, 76: 0, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 4, 84: 4,\n\t                               85: 4, 86: 4, 87: 4, 88: 4, 89: 4, 90: 4, 91: 17, 92: 17, 93: 22, 94: 20, 95: 20, 96: 22,\n", "                               97: 15, 98: 25, 99: 16, 100: 13, 101: 12, 102: 12, 103: 17, 104: 17, 105: 23, 106: 15,\n\t                               107: 15, 108: 17, 109: 15, 110: 21, 111: 15, 112: 25, 113: 13, 114: 13, 115: 13, 116: 13,\n\t                               117: 13, 118: 22, 119: 26, 120: 14, 121: 14, 122: 15, 123: 22, 124: 21, 125: 21, 126: 24,\n\t                               127: 20, 128: 22, 129: 15, 130: 17, 131: 16, 132: 15, 133: 22, 134: 24, 135: 21, 136: 17,\n\t                               137: 25, 138: 16, 139: 21, 140: 17, 141: 22, 142: 16, 143: 21, 144: 21, 145: 25, 146: 21,\n\t                               147: 26, 148: 21, 149: 24, 150: 20, 151: 17, 152: 14, 153: 21, 154: 26, 155: 15, 156: 23,\n\t                               157: 20, 158: 21, 159: 24, 160: 15, 161: 24, 162: 22, 163: 25, 164: 15, 165: 20, 166: 17,\n\t                               167: 17, 168: 22, 169: 14, 170: 18, 171: 18, 172: 18, 173: 18, 174: 18, 175: 18, 176: 18,\n\t                               177: 26, 178: 26, 179: 19, 180: 19, 181: 24}\n\t        self._label_names = [\n", "            \"ground-stuff\",\n\t            \"plant-stuff\",\n\t            \"sky-stuff\",\n\t        ]\n\t        self.cocostuff3_coarse_classes = [23, 22, 21]\n\t        self.first_stuff_index = 12\n\t    def __getitem__(self, index):\n\t        image_path = self.image_files[index]\n\t        label_path = self.label_files[index]\n\t        seed = random.randint(0, 2147483647)\n", "        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        img = self.transform(Image.open(image_path).convert(\"RGB\"))\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        label = self.label_transform(Image.open(label_path)).squeeze(0)\n\t        label[label == 255] = -1  # to be consistent with 10k\n\t        coarse_label = torch.zeros_like(label)\n\t        for fine, coarse in self.fine_to_coarse.items():\n\t            coarse_label[label == fine] = coarse\n", "        coarse_label[label == -1] = -1\n\t        if self.coarse_labels:\n\t            coarser_labels = -torch.ones_like(label)\n\t            for i, c in enumerate(self.cocostuff3_coarse_classes):\n\t                coarser_labels[coarse_label == c] = i\n\t            return img, coarser_labels, coarser_labels >= 0\n\t        else:\n\t            if self.exclude_things:\n\t                return img, coarse_label - self.first_stuff_index, (coarse_label >= self.first_stuff_index), image_path\n\t            else:\n", "                return img, coarse_label, coarse_label >= 0, image_path\n\t    def __len__(self):\n\t        return len(self.image_files)\n\tclass CityscapesSeg(Dataset):\n\t    def __init__(self, root, image_set, transform, target_transform):\n\t        super(CityscapesSeg, self).__init__()\n\t        self.split = image_set\n\t        self.root = root\n\t        if image_set == \"train\":\n\t            # our_image_set = \"train_extra\"\n", "            # mode = \"coarse\"\n\t            our_image_set = \"train\"\n\t            mode = \"fine\"\n\t        else:\n\t            our_image_set = image_set\n\t            mode = \"fine\"\n\t        self.inner_loader = Cityscapes(self.root, our_image_set,\n\t                                       mode=mode,\n\t                                       target_type=\"semantic\",\n\t                                       transform=None,\n", "                                       target_transform=None)\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n\t        self.first_nonvoid = 7\n\t    def __getitem__(self, index):\n\t        if self.transform is not None:\n\t            image, target = self.inner_loader[index]\n\t            seed = random.randint(0, 2147483647)\n\t            random.seed(seed)\n\t            torch.manual_seed(seed)\n", "            image = self.transform(image)\n\t            random.seed(seed)\n\t            torch.manual_seed(seed)\n\t            target = self.target_transform(target)\n\t            target = target - self.first_nonvoid\n\t            target[target < 0] = -1\n\t            mask = target == -1\n\t            return image, target.squeeze(0), mask\n\t        else:\n\t            return self.inner_loader[index]\n", "    def __len__(self):\n\t        return len(self.inner_loader)\n\tclass CroppedDataset(Dataset):\n\t    def __init__(self, root, dataset_name, crop_type, crop_ratio, image_set, transform, target_transform):\n\t        super(CroppedDataset, self).__init__()\n\t        self.dataset_name = dataset_name\n\t        self.split = image_set\n\t        self.root = join(root, \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n", "        self.img_dir = join(self.root, \"img\", self.split)\n\t        self.label_dir = join(self.root, \"label\", self.split)\n\t        self.num_images = len(os.listdir(self.img_dir))\n\t        assert self.num_images == len(os.listdir(self.label_dir))\n\t    def __getitem__(self, index):\n\t        image = Image.open(join(self.img_dir, \"{}.jpg\".format(index))).convert('RGB')\n\t        target = Image.open(join(self.label_dir, \"{}.png\".format(index)))\n\t        seed = random.randint(0, 2147483647)\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n", "        image = self.transform(image)\n\t        random.seed(seed)\n\t        torch.manual_seed(seed)\n\t        target = self.target_transform(target)\n\t        target = target - 1\n\t        mask = target == -1\n\t        return image, target.squeeze(0), mask, index\n\t    def __len__(self):\n\t        return self.num_images\n\tclass MaterializedDataset(Dataset):\n", "    def __init__(self, ds):\n\t        self.ds = ds\n\t        self.materialized = []\n\t        loader = DataLoader(ds, num_workers=12, collate_fn=lambda l: l[0])\n\t        for batch in tqdm(loader):\n\t            self.materialized.append(batch)\n\t    def __len__(self):\n\t        return len(self.ds)\n\t    def __getitem__(self, ind):\n\t        return self.materialized[ind]\n", "class ContrastiveSegDataset(Dataset):\n\t    def __init__(self,\n\t                 pytorch_data_dir: str,\n\t                 dataset_name: str,\n\t                 crop_type: str,\n\t                 model_type: str,\n\t                 image_set: str,\n\t                 transform,\n\t                 target_transform,\n\t                 cfg,\n", "                 aug_geometric_transform=None,\n\t                 aug_photometric_transform=None,\n\t                 num_neighbors=5,\n\t                 compute_knns=False,\n\t                 mask=False,\n\t                 pos_labels=False,\n\t                 pos_images=False,\n\t                 extra_transform=None,\n\t                 model_type_override=None\n\t                 ):\n", "        super(ContrastiveSegDataset).__init__()\n\t        self.num_neighbors = num_neighbors\n\t        self.image_set = image_set\n\t        self.dataset_name = dataset_name\n\t        self.mask = mask\n\t        self.pos_labels = pos_labels\n\t        self.pos_images = pos_images\n\t        self.extra_transform = extra_transform\n\t        self.model_type = model_type\n\t        if dataset_name == \"potsdam\":\n", "            self.n_classes = 3\n\t            dataset_class = Potsdam\n\t            extra_args = dict(coarse_labels=True)\n\t        elif dataset_name == \"potsdamraw\":\n\t            self.n_classes = 3\n\t            dataset_class = PotsdamRaw\n\t            extra_args = dict(coarse_labels=True)\n\t        elif dataset_name == \"cityscapes\" and crop_type is None:\n\t            self.n_classes = 27\n\t            dataset_class = CityscapesSeg\n", "            extra_args = dict()\n\t        elif dataset_name == \"cityscapes\" and crop_type is not None:\n\t            self.n_classes = 27\n\t            dataset_class = CroppedDataset\n\t            extra_args = dict(dataset_name=\"cityscapes\", crop_type=crop_type, crop_ratio=cfg[\"crop_ratio\"])\n\t        elif dataset_name == \"cocostuff3\":\n\t            self.n_classes = 3\n\t            dataset_class = Coco\n\t            extra_args = dict(coarse_labels=True, subset=6, exclude_things=True)\n\t        elif dataset_name == \"cocostuff15\":\n", "            self.n_classes = 15\n\t            dataset_class = Coco\n\t            extra_args = dict(coarse_labels=False, subset=7, exclude_things=True)\n\t        elif dataset_name == \"cocostuff27\" and crop_type is not None:\n\t            self.n_classes = 27\n\t            dataset_class = CroppedDataset\n\t            extra_args = dict(dataset_name=\"cocostuff27\", crop_type=cfg[\"crop_type\"], crop_ratio=cfg[\"crop_ratio\"])\n\t        elif dataset_name == \"cocostuff27\" and crop_type is None:\n\t            self.n_classes = 27\n\t            dataset_class = Coco\n", "            extra_args = dict(coarse_labels=False, subset=None, exclude_things=False)\n\t            if image_set == \"val\":\n\t                extra_args[\"subset\"] = 7\n\t        else:\n\t            raise ValueError(\"Unknown dataset: {}\".format(dataset_name))\n\t        self.aug_geometric_transform = aug_geometric_transform\n\t        self.aug_photometric_transform = aug_photometric_transform\n\t        self.dataset = dataset_class(\n\t            root=pytorch_data_dir,\n\t            image_set=self.image_set,\n", "            transform=transform,\n\t            target_transform=target_transform, **extra_args)\n\t        if model_type_override is not None:\n\t            model_type = model_type_override\n\t        else:\n\t            model_type = self.model_type\n\t        nice_dataset_name = cfg.dir_dataset_name if dataset_name == \"directory\" else dataset_name\n\t        feature_cache_file = join(\"nns\", \"nns_{}_{}_{}_{}_{}.npz\".format(\n\t            model_type, nice_dataset_name, image_set, crop_type, cfg[\"res\"]))\n\t        if pos_labels or pos_images:\n", "            if not os.path.exists(feature_cache_file) or compute_knns:\n\t                raise ValueError(\"could not find nn file {} please run precompute_knns\".format(feature_cache_file))\n\t            else:\n\t                loaded = np.load(feature_cache_file)\n\t                self.nns = loaded[\"nns\"]\n\t            assert len(self.dataset) == self.nns.shape[0]\n\t    def __len__(self):\n\t        return len(self.dataset)\n\t    def _set_seed(self, seed):\n\t        random.seed(seed)  # apply this seed to img tranfsorms\n", "        torch.manual_seed(seed)  # needed for torchvision 0.7\n\t        # pass\n\t    def __getitem__(self, ind):\n\t        pack = self.dataset[ind]\n\t        if self.pos_images or self.pos_labels:\n\t            ind_pos = self.nns[ind][torch.randint(low=1, high=self.num_neighbors + 1, size=[]).item()]\n\t            pack_pos = self.dataset[ind_pos]\n\t        seed = random.randint(0, 2147483647)  # make a seed with numpy generator\n\t        self._set_seed(seed)\n\t        coord_entries = torch.meshgrid([torch.linspace(-1, 1, pack[0].shape[1]),\n", "                                        torch.linspace(-1, 1, pack[0].shape[2])], indexing=\"ij\")\n\t        coord = torch.cat([t.unsqueeze(0) for t in coord_entries], 0)\n\t        if self.extra_transform is not None:\n\t            extra_trans = self.extra_transform\n\t        else:\n\t            extra_trans = lambda i, x: x\n\t        ret = {\n\t            \"ind\": ind,\n\t            \"img\": extra_trans(ind, pack[0]),\n\t            \"label\": extra_trans(ind, pack[1]),\n", "            # \"img_path\": extra_trans(ind, pack[3])\n\t        }\n\t        if self.pos_images:\n\t            ret[\"img_pos\"] = extra_trans(ind, pack_pos[0])\n\t            ret[\"ind_pos\"] = ind_pos\n\t        if self.mask:\n\t            ret[\"mask\"] = pack[2]\n\t        if self.pos_labels:\n\t            ret[\"label_pos\"] = extra_trans(ind, pack_pos[1])\n\t            ret[\"mask_pos\"] = pack_pos[2]\n", "        if self.aug_photometric_transform is not None:\n\t            # img_aug = self.aug_photometric_transform(self.aug_geometric_transform(pack[0]))\n\t            img_aug = self.aug_photometric_transform(pack[0])\n\t            # img_pos_aug = self.aug_photometric_transform(pack_pos[0])\n\t            self._set_seed(seed)\n\t            coord_aug = self.aug_geometric_transform(coord)\n\t            ret[\"img_aug\"] = img_aug\n\t            ret[\"coord_aug\"] = coord_aug.permute(1, 2, 0)\n\t            #--- aug for img_pos(KNN) ---#\n\t            # ret[\"img_pos_aug\"] = img_pos_aug\n", "        return ret\n\tdef get_class_labels(dataset_name):\n\t    if dataset_name.startswith(\"cityscapes\"):\n\t        return [\n\t            'road', 'sidewalk', 'parking', 'rail track', 'building',\n\t            'wall', 'fence', 'guard rail', 'bridge', 'tunnel',\n\t            'pole', 'polegroup', 'traffic light', 'traffic sign', 'vegetation',\n\t            'terrain', 'sky', 'person', 'rider', 'car',\n\t            'truck', 'bus', 'caravan', 'trailer', 'train',\n\t            'motorcycle', 'bicycle']\n", "    elif dataset_name == \"cocostuff27\":\n\t        return [\n\t            \"electronic\", \"appliance\", \"food\", \"furniture\", \"indoor\",\n\t            \"kitchen\", \"accessory\", \"animal\", \"outdoor\", \"person\",\n\t            \"sports\", \"vehicle\", \"ceiling\", \"floor\", \"food\",\n\t            \"furniture\", \"rawmaterial\", \"textile\", \"wall\", \"window\",\n\t            \"building\", \"ground\", \"plant\", \"sky\", \"solid\",\n\t            \"structural\", \"water\"]\n\t    elif dataset_name == \"voc\":\n\t        return [\n", "            'background',\n\t            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n\t            'bus', 'car', 'cat', 'chair', 'cow',\n\t            'diningtable', 'dog', 'horse', 'motorbike', 'person',\n\t            'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\t    elif dataset_name == \"potsdam\":\n\t        return [\n\t            'roads and cars',\n\t            'buildings and clutter',\n\t            'trees and vegetation']\n", "    else:\n\t        raise ValueError(\"Unknown Dataset {}\".format(dataset_name))\n\tdef create_cityscapes_colormap():\n\t    colors = [(128, 64, 128),\n\t              (244, 35, 232),\n\t              (250, 170, 160),\n\t              (230, 150, 140),\n\t              (70, 70, 70),\n\t              (102, 102, 156),\n\t              (190, 153, 153),\n", "              (180, 165, 180),\n\t              (150, 100, 100),\n\t              (150, 120, 90),\n\t              (153, 153, 153),\n\t              (153, 153, 153),\n\t              (250, 170, 30),\n\t              (220, 220, 0),\n\t              (107, 142, 35),\n\t              (152, 251, 152),\n\t              (70, 130, 180),\n", "              (220, 20, 60),\n\t              (255, 0, 0),\n\t              (0, 0, 142),\n\t              (0, 0, 70),\n\t              (0, 60, 100),\n\t              (0, 0, 90),\n\t              (0, 0, 110),\n\t              (0, 80, 100),\n\t              (0, 0, 230),\n\t              (119, 11, 32),\n", "              (0, 0, 0)]\n\t    return np.array(colors)\n\tif __name__ == \"__main__\":\n\t    cmap = create_pascal_label_colormap()\n\t    x = np.arange(27)\n\t    y = np.ones(27)\n\t    colors = cmap[:27]/255\n\t    plt.bar(x, y, color=colors)\n\t    plt.show()\n"]}
{"filename": "dataset/dataset_download.py", "chunked_list": ["'''\n\t@article{hamilton2022unsupervised,\n\t  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},\n\t  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},\n\t  journal={arXiv preprint arXiv:2203.08414},\n\t  year={2022}\n\t}\n\t'''\n\timport wget\n\timport os\n", "def my_app() -> None:\n\t    pytorch_data_dir = \"/data/Datasets\"\n\t    dataset_names = [\n\t        \"potsdam\",\n\t        \"cityscapes\",\n\t        \"cocostuff\",\n\t        \"potsdamraw\"]\n\t    url_base = \"https://marhamilresearch4.blob.core.windows.net/stego-public/pytorch_data/\"\n\t    os.makedirs(pytorch_data_dir, exist_ok=True)\n\t    for dataset_name in dataset_names:\n", "        if (not os.path.exists(os.path.join(pytorch_data_dir, dataset_name))) or \\\n\t                (not os.path.exists(os.path.join(pytorch_data_dir, dataset_name + \".zip\"))):\n\t            print(\"\\n Downloading {}\".format(dataset_name))\n\t            wget.download(url_base + dataset_name + \".zip\", os.path.join(pytorch_data_dir, dataset_name + \".zip\"))\n\t        else:\n\t            print(\"\\n Found {}, skipping download\".format(dataset_name))\n\tif __name__ == \"__main__\":\n\t    my_app()\n"]}
{"filename": "dataset/crop_datasets.py", "chunked_list": ["import os\n\tfrom os.path import join\n\tfrom dataset.data import ContrastiveSegDataset, ToTargetTensor\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision.transforms.functional import five_crop, _get_image_size, crop\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import Dataset\n\tfrom PIL import Image\n\tfrom torchvision import transforms as T\n", "import argparse\n\tfrom utils.common_utils import parse\n\tdef _random_crops(img, size, seed, n):\n\t    \"\"\"Crop the given image into four corners and the central crop.\n\t    If the image is torch Tensor, it is expected\n\t    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n\t    .. Note::\n\t        This transform returns a tuple of images and there may be a\n\t        mismatch in the number of inputs and targets your ``Dataset`` returns.\n\t    Args:\n", "        img (PIL Image or Tensor): Image to be cropped.\n\t        size (sequence or int): Desired output size of the crop. If size is an\n\t            int instead of sequence like (h, w), a square crop (size, size) is\n\t            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n\t    Returns:\n\t       tuple: tuple (tl, tr, bl, br, center)\n\t                Corresponding top left, top right, bottom left, bottom right and center crop.\n\t    \"\"\"\n\t    if isinstance(size, int):\n\t        size = (int(size), int(size))\n", "    elif isinstance(size, (tuple, list)) and len(size) == 1:\n\t        size = (size[0], size[0])\n\t    if len(size) != 2:\n\t        raise ValueError(\"Please provide only two dimensions (h, w) for size.\")\n\t    image_width, image_height = _get_image_size(img)\n\t    crop_height, crop_width = size\n\t    if crop_width > image_width or crop_height > image_height:\n\t        msg = \"Requested crop size {} is bigger than input size {}\"\n\t        raise ValueError(msg.format(size, (image_height, image_width)))\n\t    images = []\n", "    for i in range(n):\n\t        seed1 = hash((seed, i, 0))\n\t        seed2 = hash((seed, i, 1))\n\t        crop_height, crop_width = int(crop_height), int(crop_width)\n\t        top = seed1 % (image_height - crop_height)\n\t        left = seed2 % (image_width - crop_width)\n\t        images.append(crop(img, top, left, crop_height, crop_width))\n\t    return images\n\tclass RandomCropComputer(Dataset):\n\t    def _get_size(self, img):\n", "        if len(img.shape) == 3:\n\t            return [int(img.shape[1] * self.crop_ratio), int(img.shape[2] * self.crop_ratio)]\n\t        elif len(img.shape) == 2:\n\t            return [int(img.shape[0] * self.crop_ratio), int(img.shape[1] * self.crop_ratio)]\n\t        else:\n\t            raise ValueError(\"Bad image shape {}\".format(img.shape))\n\t    def random_crops(self, i, img):\n\t        return _random_crops(img, self._get_size(img), i, 5)\n\t    def five_crops(self, i, img):\n\t        return five_crop(img, self._get_size(img))\n", "    def __init__(self, cfg, dataset_name, img_set, crop_type, crop_ratio):\n\t        self.pytorch_data_dir = cfg[\"dataset\"][\"data_path\"]\n\t        self.crop_ratio = crop_ratio\n\t        self.save_dir = join(\n\t            cfg[\"dataset\"][\"data_path\"], \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n\t        self.img_set = img_set\n\t        self.dataset_name = dataset_name\n\t        self.cfg = cfg\n\t        self.img_dir = join(self.save_dir, \"img\", img_set)\n\t        self.label_dir = join(self.save_dir, \"label\", img_set)\n", "        os.makedirs(self.img_dir, exist_ok=True)\n\t        os.makedirs(self.label_dir, exist_ok=True)\n\t        if crop_type == \"random\":\n\t            cropper = lambda i, x: self.random_crops(i, x)\n\t        elif crop_type == \"five\":\n\t            cropper = lambda i, x: self.five_crops(i, x)\n\t        else:\n\t            raise ValueError('Unknown crop type {}'.format(crop_type))\n\t        self.dataset = ContrastiveSegDataset(\n\t            cfg[\"dataset\"][\"data_path\"],\n", "            dataset_name,\n\t            None,\n\t            img_set,\n\t            T.ToTensor(),\n\t            ToTargetTensor(),\n\t            cfg=cfg,\n\t            num_neighbors=cfg[\"dataset\"][\"num_neighbors\"],\n\t            pos_labels=False,\n\t            pos_images=False,\n\t            mask=False,\n", "            aug_geometric_transform=None,\n\t            aug_photometric_transform=None,\n\t            extra_transform=cropper\n\t        )\n\t    def __getitem__(self, item):\n\t        batch = self.dataset[item]\n\t        imgs = batch['img']\n\t        labels = batch['label']\n\t        for crop_num, (img, label) in enumerate(zip(imgs, labels)):\n\t            img_num = item * 5 + crop_num\n", "            img_arr = img.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n\t            label_arr = (label + 1).unsqueeze(0).permute(1, 2, 0).to('cpu', torch.uint8).numpy().squeeze(-1)\n\t            Image.fromarray(img_arr).save(join(self.img_dir, \"{}.jpg\".format(img_num)), 'JPEG')\n\t            Image.fromarray(label_arr).save(join(self.label_dir, \"{}.png\".format(img_num)), 'PNG')\n\t        return True\n\t    def __len__(self):\n\t        return len(self.dataset)\n\tdef my_app(cfg) -> None:\n\t    # dataset_names = [\"cityscapes\", \"cocostuff27\"]\n\t    dataset_names = [\"cocostuff27\"]\n", "    img_sets = [\"train\", \"val\"]\n\t    crop_types = [\"five\"]\n\t    crop_ratios = [.5, .7]\n\t    # dataset_names = [\"cityscapes\"]\n\t    # img_sets = [\"train\", \"val\"]\n\t    # crop_types = [\"five\"]\n\t    # crop_ratios = [.5]\n\t    for crop_ratio in crop_ratios:\n\t        for crop_type in crop_types:\n\t            for dataset_name in dataset_names:\n", "                for img_set in img_sets:\n\t                    dataset = RandomCropComputer(cfg, dataset_name, img_set, crop_type, crop_ratio)\n\t                    loader = DataLoader(dataset, 1, shuffle=False, num_workers=8, collate_fn=lambda l: l)\n\t                    for _ in tqdm(loader):\n\t                        pass\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--opt\", type=str, required=True, help=\"Path to option JSON file.\")\n\t    parser_args = parser.parse_args()\n\t    parser_opt = parse(parser_args.opt)\n", "    my_app(parser_opt)\n"]}
