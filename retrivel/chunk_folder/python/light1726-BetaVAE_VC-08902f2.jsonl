{"filename": "inference-from-wav.py", "chunked_list": ["import os\n\timport argparse\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tqdm import tqdm\n\tfrom configs import CNENHPS\n\tfrom models import BetaVAEVC\n\tfrom audio import TestUtils, Audio\n\tdef read_mels(wav_list_f, audio_processor):\n\t    mels = []\n", "    mel_names = []\n\t    with open(wav_list_f, 'r', encoding='utf-8') as f:\n\t        for line in f:\n\t            mel = extract_mel(line.strip(), audio_processor).astype(np.float32)\n\t            mels.append(mel)\n\t            name = line.strip().split('/')[-1].split('.')[0]\n\t            mel_names.append(name)\n\t    return mels, mel_names\n\tdef extract_mel(wav_f, audio_processor):\n\t    wav_arr = audio_processor.load_wav(wav_f)\n", "    wav_arr = audio_processor.trim_silence_by_trial(wav_arr, top_db=20., lower_db=25.)\n\t    wav_arr = wav_arr / max(0.01, np.max(np.abs(wav_arr)))\n\t    wav_arr = audio_processor.preemphasize(wav_arr)\n\t    mel = audio_processor.melspectrogram(wav_arr).T\n\t    return mel\n\tdef synthesize_from_mel(args):\n\t    ckpt_path = args.ckpt_path\n\t    ckpt_step = ckpt_path.split('-')[-1]\n\t    assert os.path.isfile(args.src_wavs)\n\t    assert os.path.isfile(args.ref_wavs)\n", "    test_dir = args.test_dir\n\t    if not os.path.isdir(test_dir):\n\t        os.makedirs(test_dir)\n\t    hparams = CNENHPS()\n\t    tester = TestUtils(hparams, args.test_dir)\n\t    audio_processor = Audio(hparams.Audio)\n\t    # setup model\n\t    model = BetaVAEVC(hparams)\n\t    checkpoint = tf.train.Checkpoint(model=model)\n\t    checkpoint.restore(ckpt_path).expect_partial()\n", "    # set up tf function\n\t    @tf.function(input_signature=[\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n\t    def vc(mels, mel_ext, m_lengths):\n\t        out, _ = model.post_inference(mels, m_lengths, mel_ext)\n\t        return out\n\t    src_mels, src_names = read_mels(args.src_wavs, audio_processor)\n\t    ref_mels, ref_names = read_mels(args.ref_wavs, audio_processor)\n", "    for src_mel, src_name in tqdm(zip(src_mels, src_names)):\n\t        for ref_mel, ref_name in zip(ref_mels, ref_names):\n\t            while ref_mel.shape[0] < hparams.Dataset.chunk_size:\n\t                ref_mel = np.concatenate([ref_mel, ref_mel], axis=0)\n\t            ref_mel = ref_mel[:hparams.Dataset.chunk_size, :]\n\t            assert src_mel.shape[1] == hparams.Audio.num_mels\n\t            src_mel_batch = tf.constant(np.expand_dims(src_mel, axis=0))\n\t            ref_mel_batch = tf.constant(np.expand_dims(ref_mel, axis=0))\n\t            mel_len_batch = tf.constant([src_mel.shape[0]])\n\t            ids = ['{}_to_{}'.format(src_name, ref_name)]\n", "            prediction = vc(src_mel_batch, ref_mel_batch, mel_len_batch)\n\t            # tester.synthesize_and_save_wavs(\n\t            #     ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n\t            tester.write_mels(ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n\t    return\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser('')\n\t    parser.add_argument('--ckpt_path', type=str, help='path to the model ckpt')\n\t    parser.add_argument('--test_dir', type=str, help='directory to save test results')\n\t    parser.add_argument('--src_wavs', type=str, help='source wav file list')\n", "    parser.add_argument('--ref_wavs', type=str, help='reference wav npy file list')\n\t    main_args = parser.parse_args()\n\t    synthesize_from_mel(main_args)\n"]}
{"filename": "train.py", "chunked_list": ["import os\n\timport sys\n\timport random\n\timport argparse\n\timport datetime\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom time import time\n\tfrom models import BetaVAEVC\n\tfrom audio import TestUtils\n", "from datasets import TFRecordWriter\n\tfrom configs import CNENHPS, Logger\n\tdef set_seeds(seed):\n\t    os.environ['PYTHONHASHSEED'] = str(seed)\n\t    random.seed(seed)\n\t    tf.random.set_seed(seed)\n\t    np.random.seed(seed)\n\t    return\n\tdef set_global_determinism(seed):\n\t    set_seeds(seed=seed)\n", "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\t    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n\t    return\n\tdef main():\n\t    parser = argparse.ArgumentParser('Training parameters parser')\n\t    parser.add_argument('--data_dir', type=str, help='dataset tfrecord directory')\n\t    parser.add_argument('--out_dir', type=str, help='directory to save logs', default='outputs')\n\t    args = parser.parse_args()\n\t    hparams = CNENHPS()\n\t    # set random seed\n", "    set_global_determinism(hparams.Train.random_seed)\n\t    # validate log directories\n\t    out_dir = args.out_dir\n\t    if not os.path.isdir(out_dir):\n\t        os.makedirs(out_dir)\n\t    model_dir = os.path.join(out_dir, 'models')\n\t    if not os.path.isdir(model_dir):\n\t        os.makedirs(model_dir)\n\t    log_dir = os.path.join(out_dir, 'logs')\n\t    if not os.path.isdir(log_dir):\n", "        os.makedirs(log_dir)\n\t    test_dir = os.path.join(out_dir, 'tests')\n\t    if not os.path.isdir(test_dir):\n\t        os.makedirs(test_dir)\n\t    # set up test utils\n\t    tester = TestUtils(hparams, test_dir)\n\t    # set up logger\n\t    sys.stdout = Logger(log_dir)\n\t    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\t    train_dir = os.path.join(log_dir, current_time, 'train')\n", "    os.makedirs(train_dir)\n\t    val_dir = os.path.join(log_dir, current_time, 'val')\n\t    os.makedirs(val_dir)\n\t    # hyperparameters\n\t    data_records = TFRecordWriter(\n\t        save_dir=args.data_dir, chunk_size=hparams.Dataset.chunk_size)\n\t    train_set = data_records.create_dataset(\n\t        buffer_size=hparams.Dataset.buffer_size,\n\t        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n\t        batch_size=hparams.Train.train_batch_size,\n", "        num_mels=hparams.Audio.num_mels,\n\t        shuffle_buffer=hparams.Train.shuffle_buffer,\n\t        shuffle=hparams.Train.shuffle,\n\t        tfrecord_files=data_records.get_tfrecords_list('train'),\n\t        seed=hparams.Train.random_seed)\n\t    val_set = data_records.create_dataset(\n\t        buffer_size=hparams.Dataset.buffer_size,\n\t        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n\t        batch_size=hparams.Train.train_batch_size,\n\t        num_mels=hparams.Audio.num_mels,\n", "        shuffle_buffer=hparams.Train.shuffle_buffer,\n\t        shuffle=hparams.Train.shuffle,\n\t        tfrecord_files=data_records.get_tfrecords_list('val'),\n\t        seed=hparams.Train.random_seed)\n\t    test_set = data_records.create_dataset(\n\t        buffer_size=hparams.Dataset.buffer_size,\n\t        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n\t        batch_size=hparams.Train.test_batch_size,\n\t        num_mels=hparams.Audio.num_mels,\n\t        shuffle_buffer=hparams.Train.shuffle_buffer,\n", "        shuffle=hparams.Train.shuffle,\n\t        tfrecord_files=data_records.get_tfrecords_list('test'),\n\t        seed=hparams.Train.random_seed,\n\t        drop_remainder=True)\n\t    # 2. setup model\n\t    model = BetaVAEVC(hparams)\n\t    learning_rate = hparams.Train.learning_rate\n\t    optimizer = tf.keras.optimizers.Adam(\n\t        learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n\t    # 3. define training step\n", "    @tf.function(input_signature=[\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n\t    def train_step(mels, mel_ext, m_lengths):\n\t        print('Tracing back at train_step')\n\t        with tf.GradientTape() as tape:\n\t            predictions, mel_l2, content_kl, spk_kl = model(\n\t                inputs=mels, mel_lengths=m_lengths, inp_ext=mel_ext,\n\t                training=True, reduce_loss=True)\n", "            loss = (mel_l2 + hparams.Train.content_kl_weight * content_kl\n\t                    + hparams.Train.spk_kl_weight * spk_kl)\n\t        gradients = tape.gradient(loss, model.trainable_variables)\n\t        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\t        return loss, mel_l2, content_kl, spk_kl\n\t    # 4. define validate step\n\t    @tf.function(input_signature=[\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n", "    def val_step(mels, mel_ext, m_lengths):\n\t        print('Tracing back at val step')\n\t        predictions, mel_l2, content_kl, spk_kl = model(\n\t            inputs=mels, mel_lengths=m_lengths, inp_ext=mel_ext,\n\t            training=False, reduce_loss=True)\n\t        loss = (mel_l2 + hparams.Train.content_kl_weight * content_kl\n\t                + hparams.Train.spk_kl_weight * spk_kl)\n\t        return loss, mel_l2, content_kl, spk_kl\n\t    # @tf.function\n\t    def train_one_epoch(dataset):\n", "        # print('tracing back at train_one_epoch')\n\t        step = 0\n\t        total = 0.0\n\t        mel_l2 = 0.0\n\t        kl = 0.0\n\t        spk_kl = 0.\n\t        for _, train_mels, train_m_lengths, train_mel_ext in dataset:\n\t            step_start = time()\n\t            _total, _mel_l2, _kl, _spk_kl = train_step(\n\t                train_mels, train_mel_ext, train_m_lengths)\n", "            step_end = time()\n\t            print('Step {}: total {:.4f}, mel-l2 {:.4f}, content-kl {:.4f},'\n\t                  ' spk-kl: {:.4f}, time {:.4f}'.format(\n\t                step, _total.numpy(), _mel_l2.numpy(), _kl.numpy(),\n\t                _spk_kl.numpy(), step_end - step_start))\n\t            step += 1\n\t            total += _total.numpy()\n\t            mel_l2 += _mel_l2.numpy()\n\t            kl += _kl.numpy()\n\t            spk_kl += _spk_kl.numpy()\n", "        return total / step, mel_l2 / step, kl / step, spk_kl / step\n\t    # @tf.function\n\t    def val_one_epoch(dataset):\n\t        step = 0\n\t        total = 0.0\n\t        mel_l2 = 0.0\n\t        kl = 0.0\n\t        spk_kl = 0.\n\t        for _, val_mels, val_m_lengths, val_mel_ext in dataset:\n\t            _total, _mel_l2, _kl, _spk_kl = val_step(\n", "                val_mels, val_mel_ext, val_m_lengths)\n\t            step += 1\n\t            total += _total.numpy()\n\t            mel_l2 += _mel_l2.numpy()\n\t            kl += _kl.numpy()\n\t            spk_kl += _spk_kl.numpy()\n\t        return total / step, mel_l2 / step, kl / step, spk_kl / step\n\t    # 8. setup summary writer\n\t    train_summary_writer = tf.summary.create_file_writer(train_dir)\n\t    val_summary_writer = tf.summary.create_file_writer(val_dir)\n", "    # 9. setup checkpoint: all workers will need checkpoint manager to load checkpoint\n\t    checkpoint = tf.train.Checkpoint(step=tf.Variable(0, dtype=tf.int64, trainable=False),\n\t                                     optimizer=optimizer, model=model)\n\t    manager = tf.train.CheckpointManager(checkpoint, model_dir, max_to_keep=20)\n\t    checkpoint.restore(manager.latest_checkpoint)\n\t    if manager.latest_checkpoint:\n\t        print(\"Restored from {}\".format(manager.latest_checkpoint))\n\t        step = checkpoint.step.numpy()\n\t    else:\n\t        print(\"Initializing from scratch.\")\n", "        step = 0\n\t    # 8. start training\n\t    for epoch in range(step + 1, hparams.Train.epochs + 1):\n\t        print('Training Epoch {} ...'.format(epoch))\n\t        epoch_start = time()\n\t        train_total, train_mel_l2, train_kl, train_spk_kl = train_one_epoch(train_set)\n\t        epoch_dur = time() - epoch_start\n\t        print('\\nTraining Epoch {} finished in {:.3f} Secs'.format(epoch, epoch_dur))\n\t        # save summary and evaluate\n\t        with train_summary_writer.as_default():\n", "            tf.summary.scalar('total-loss', train_total, step=epoch)\n\t            tf.summary.scalar('recon-loss', train_mel_l2, step=epoch)\n\t            tf.summary.scalar('content-kl', train_kl, step=epoch)\n\t            tf.summary.scalar('speaker-kl', train_spk_kl, step=epoch)\n\t        # validation\n\t        print('Validation ...')\n\t        val_start = time()\n\t        val_total, val_mel_l2, val_kl, val_spk_kl = val_one_epoch(val_set)\n\t        print('Validation finished in {:.3f} Secs'.format(time() - val_start))\n\t        with val_summary_writer.as_default():\n", "            tf.summary.scalar('total-loss', val_total, step=epoch)\n\t            tf.summary.scalar('recon-loss', val_mel_l2, step=epoch)\n\t            tf.summary.scalar('content-kl', val_kl, step=epoch)\n\t            tf.summary.scalar('speaker-kl', val_spk_kl, step=epoch)\n\t        print('Epoch {}: l2 {:.4f} / {:.4f}, content-kl {:.4f} / {:.4f}, spk-kl: {:.4f} / {:.4f}'.format(\n\t            epoch, train_mel_l2, val_mel_l2, train_kl, val_kl, train_spk_kl, val_spk_kl))\n\t        if epoch % hparams.Train.ckpt_interval == 0:\n\t            # save checkpoint\n\t            save_path = manager.save(checkpoint_number=epoch)\n\t            print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n", "        # test\n\t        if epoch % hparams.Train.test_interval == 0:\n\t            print('Testing ...')\n\t            i = 0\n\t            ref_mels = None\n\t            ref_spk_ids = None\n\t            for test_ids, test_mels, test_m_lengths, test_mel_ext in test_set.take(2):\n\t                if i == 0:\n\t                    ref_mels = test_mel_ext\n\t                    ref_spk_ids = test_ids\n", "                    i += 1\n\t                    continue\n\t                post_mel, _ = model.post_inference(test_mels, test_m_lengths, ref_mels)\n\t                fids = [sid.decode('utf-8') + '-ref-' + rid.decode('utf-8')\n\t                        for sid, rid in zip(test_ids.numpy(), ref_spk_ids.numpy())]\n\t                try:\n\t                    tester.synthesize_and_save_wavs(\n\t                        epoch, post_mel.numpy(), test_m_lengths.numpy(), fids, 'post')\n\t                except:\n\t                    print('Something wrong with the generated waveform!')\n", "            print('test finished, check {} for the results'.format(test_dir))\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "inference-from-mel.py", "chunked_list": ["import os\n\timport argparse\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tqdm import tqdm\n\tfrom configs import CNENHPS\n\tfrom audio import TestUtils\n\tfrom models import BetaVAEVC\n\tdef read_mels(mel_list_f):\n\t    mels = []\n", "    mel_names = []\n\t    with open(mel_list_f, 'r', encoding='utf-8') as f:\n\t        for line in f:\n\t            mel = np.load(line.strip()).astype(np.float32)\n\t            mels.append(mel)\n\t            name = line.strip().split('/')[-1].split('.')[0]\n\t            mel_names.append(name)\n\t    return mels, mel_names\n\tdef synthesize_from_mel(args):\n\t    ckpt_path = args.ckpt_path\n", "    ckpt_step = ckpt_path.split('-')[-1]\n\t    assert os.path.isfile(args.src_mels)\n\t    assert os.path.isfile(args.ref_mels)\n\t    test_dir = args.test_dir\n\t    if not os.path.isdir(test_dir):\n\t        os.makedirs(test_dir)\n\t    hparams = CNENHPS()\n\t    tester = TestUtils(hparams, args.test_dir)\n\t    # setup model\n\t    model = BetaVAEVC(hparams)\n", "    checkpoint = tf.train.Checkpoint(model=model)\n\t    checkpoint.restore(ckpt_path).expect_partial()\n\t    # set up tf function\n\t    @tf.function(input_signature=[\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n\t    def vc(mels, mel_ext, m_lengths):\n\t        out, _ = model.post_inference(mels, m_lengths, mel_ext)\n\t        return out\n", "    src_mels, src_names = read_mels(args.src_mels)\n\t    ref_mels, ref_names = read_mels(args.ref_mels)\n\t    for src_mel, src_name in tqdm(zip(src_mels, src_names)):\n\t        for ref_mel, ref_name in zip(ref_mels, ref_names):\n\t            while ref_mel.shape[0] < hparams.Dataset.chunk_size:\n\t                ref_mel = np.concatenate([ref_mel, ref_mel], axis=0)\n\t            ref_mel = ref_mel[:hparams.Dataset.chunk_size, :]\n\t            assert src_mel.shape[1] == hparams.Audio.num_mels\n\t            src_mel_batch = tf.constant(np.expand_dims(src_mel, axis=0))\n\t            ref_mel_batch = tf.constant(np.expand_dims(ref_mel, axis=0))\n", "            mel_len_batch = tf.constant([src_mel.shape[0]])\n\t            ids = ['{}_to_{}'.format(src_name, ref_name)]\n\t            prediction = vc(src_mel_batch, ref_mel_batch, mel_len_batch)\n\t            tester.write_mels(ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n\t            # tester.synthesize_and_save_wavs(ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n\t    return\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser('')\n\t    parser.add_argument('--ckpt_path', type=str, help='path to the model ckpt')\n\t    parser.add_argument('--test_dir', type=str, help='directory to save test results')\n", "    parser.add_argument('--src_mels', type=str, help='source mel npy file list')\n\t    parser.add_argument('--ref_mels', type=str, help='reference mel npy file list')\n\t    main_args = parser.parse_args()\n\t    synthesize_from_mel(main_args)\n"]}
{"filename": "preprocess.py", "chunked_list": ["import os\n\timport random\n\timport numpy as np\n\timport warnings\n\tfrom configs import CNENHPS\n\tfrom datasets import VCTK, AiShell3, MelPreprocessor, TFRecordWriter\n\twarnings.filterwarnings(\"ignore\")\n\tdef main():\n\t    hps = CNENHPS()\n\t    random.seed(hps.Train.random_seed)\n", "    np.random.seed(hps.Train.random_seed)\n\t    vctk_writer = VCTK(\n\t        data_dir=hps.Dataset.VCTK.corpus_dir,\n\t        out_dir=hps.Dataset.dataset_dir,\n\t        val_spks=hps.Dataset.VCTK.val_spks,\n\t        test_spks=hps.Dataset.VCTK.test_spks)\n\t    vctk_writer.write_summary()\n\t    aishell_writer = AiShell3(\n\t        data_dir=hps.Dataset.AiShell3.corpus_dir,\n\t        out_dir=hps.Dataset.dataset_dir,\n", "        train_spk_file=None,\n\t        val_test_spk_file=hps.Dataset.AiShell3.val_test_spk_file)\n\t    aishell_writer.write_summary()\n\t    feats_extractor = MelPreprocessor(\n\t        [vctk_writer.summary_file, aishell_writer.summary_file],\n\t        save_dir=hps.Dataset.dataset_dir, hps=hps)\n\t    feats_extractor.feature_extraction()\n\t    tfrecord_save_dir = os.path.join(hps.Dataset.dataset_dir, 'tfrecords')\n\t    if not os.path.exists(tfrecord_save_dir):\n\t        os.makedirs(tfrecord_save_dir)\n", "    tfrecord_writer = TFRecordWriter(\n\t        train_split=hps.Dataset.n_record_split,\n\t        data_dir=hps.Dataset.dataset_dir,\n\t        save_dir=tfrecord_save_dir,\n\t        chunk_size=hps.Dataset.chunk_size)\n\t    tfrecord_writer.write_all()\n\t    # test\n\t    print('TFRecord test...')\n\t    tf_dataset = tfrecord_writer.create_dataset(\n\t        buffer_size=hps.Dataset.buffer_size,\n", "        num_parallel_reads=hps.Dataset.num_parallel_reads,\n\t        batch_size=hps.Train.test_batch_size,\n\t        num_mels=hps.Audio.num_mels,\n\t        shuffle_buffer=hps.Train.shuffle_buffer,\n\t        shuffle=hps.Train.shuffle,\n\t        tfrecord_files=tfrecord_writer.get_tfrecords_list('test'))\n\t    for epoch in range(2):\n\t        for i, data in enumerate(tf_dataset):\n\t            print('epoch {}, step: {}'.format(epoch, i))\n\t            fid, mel, mel_len, mel_ext = data\n", "            print(fid.numpy(), mel.shape, mel_len.numpy(), mel_ext.shape)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "latent_extraction.py", "chunked_list": ["import os\n\timport argparse\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tqdm import tqdm\n\tfrom configs import CNENHPS\n\tfrom models import BetaVAEVC\n\tfrom datasets import TFRecordWriter\n\tdef main(args):\n\t    hparams = CNENHPS()\n", "    data_records = TFRecordWriter(\n\t        save_dir=args.data_dir, chunk_size=hparams.Dataset.chunk_size)\n\t    val_set = data_records.create_dataset(\n\t        buffer_size=hparams.Dataset.buffer_size,\n\t        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n\t        batch_size=hparams.Train.test_batch_size,\n\t        num_mels=hparams.Audio.num_mels,\n\t        shuffle_buffer=hparams.Train.shuffle_buffer,\n\t        shuffle=hparams.Train.shuffle,\n\t        tfrecord_files=data_records.get_tfrecords_list('val'),\n", "        seed=hparams.Train.random_seed,\n\t        drop_remainder=False)\n\t    test_set = data_records.create_dataset(\n\t        buffer_size=hparams.Dataset.buffer_size,\n\t        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n\t        batch_size=hparams.Train.test_batch_size,\n\t        num_mels=hparams.Audio.num_mels,\n\t        shuffle_buffer=hparams.Train.shuffle_buffer,\n\t        shuffle=hparams.Train.shuffle,\n\t        tfrecord_files=data_records.get_tfrecords_list('test'),\n", "        seed=hparams.Train.random_seed,\n\t        drop_remainder=False)\n\t    ckpt_path = args.ckpt_path\n\t    save_dir = args.save_dir\n\t    if not os.path.isdir(save_dir):\n\t        os.makedirs(save_dir)\n\t    cn_dir = os.path.join(save_dir, 'CN')\n\t    en_dir = os.path.join(save_dir, 'EN')\n\t    os.makedirs(cn_dir, exist_ok=True)\n\t    os.makedirs(en_dir, exist_ok=True)\n", "    # setup model\n\t    model = BetaVAEVC(hparams)\n\t    checkpoint = tf.train.Checkpoint(model=model)\n\t    checkpoint.restore(ckpt_path).expect_partial()\n\t    def save_spk_npy(arrs, fids):\n\t        for a, n in zip(arrs.numpy(), fids.numpy()):\n\t            n = n.decode('utf-8') if type(n) is bytes else n\n\t            if n.startswith('SSB'):\n\t                save_name = os.path.join(cn_dir, '{}-spk.npy'.format(n))\n\t            else:\n", "                save_name = os.path.join(en_dir, '{}-spk.npy'.format(n))\n\t            np.save(save_name, a)\n\t        return\n\t    def save_content_npy(arrs, lens, fids):\n\t        for a, l, n in zip(arrs.numpy(), lens.numpy(), fids.numpy()):\n\t            n = n.decode('utf-8') if type(n) is bytes else n\n\t            if n.startswith('SSB'):\n\t                save_name = os.path.join(cn_dir, '{}-content.npy'.format(n))\n\t            else:\n\t                save_name = os.path.join(en_dir, '{}-content.npy'.format(n))\n", "            np.save(save_name, a[:l, :])\n\t        return\n\t    @tf.function(input_signature=[\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n\t        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n\t    def inference(mels, mels_ext, mel_lengths):\n\t        spk_mu, _ = model.spk_posterior(mels_ext)\n\t        reduced_mels = mels[:, ::model.reduction_factor, :]\n\t        reduced_lens = (mel_lengths + model.reduction_factor - 1) // model.reduction_factor\n", "        content_mu, _, _ = model.posterior(reduced_mels, lengths=reduced_lens, training=False)\n\t        return spk_mu, content_mu, reduced_lens\n\t    for dataset in [val_set, test_set]:\n\t        for _fids, _mels, _m_lengths, _mels_ext in tqdm(dataset):\n\t            spk_emb, content_emb, reduced_lengths = inference(_mels, _mels_ext, _m_lengths)\n\t            save_spk_npy(spk_emb, _fids)\n\t            save_content_npy(content_emb, reduced_lengths, _fids)\n\t    return\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser('')\n", "    parser.add_argument('--data_dir', type=str, help='Tf-Records directory')\n\t    parser.add_argument('--ckpt_path', type=str, help='path to the model ckpt')\n\t    parser.add_argument('--save_dir', type=str, help='directory to save test results')\n\t    main_args = parser.parse_args()\n\t    main(main_args)\n"]}
{"filename": "audio/audio.py", "chunked_list": ["import os\n\timport re\n\timport pysptk\n\timport librosa\n\timport warnings\n\timport soundfile\n\timport numpy as np\n\tfrom scipy import signal\n\tfrom scipy.io import wavfile\n\tfrom scipy import interpolate\n", "warnings.filterwarnings(\"ignore\")\n\tclass Audio:\n\t    def __init__(self, audio_hparams):\n\t        self.hps = audio_hparams\n\t    def load_wav(self, path):\n\t        return librosa.core.load(path, sr=self.hps.sample_rate)[0]\n\t    def get_duration(self, path):\n\t        dur = librosa.get_duration(filename=path)\n\t        return dur\n\t    def save_wav(self, wav, path):\n", "        wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n\t        wavfile.write(path, self.hps.sample_rate, wav.astype(np.int16))\n\t        return\n\t    def trim_silence_by_label(self, wav_arr, inds):\n\t        \"\"\"remove silence given the starting and ending time in secs\n\t        wav_arr: waveform samples array\n\t        inds: [start_time, end_time], float\n\t        \"\"\"\n\t        sample_inds = librosa.core.time_to_samples(inds, self.hps.sample_rate)\n\t        return wav_arr[sample_inds[0]: sample_inds[1]]\n", "    def trim_silence_by_trial(self, wav_arr, top_db=15., lower_db=40., time_threshold=0.3):\n\t        sample_len_th = int(time_threshold * self.hps.sample_rate)\n\t        trimed, old_inds = librosa.effects.trim(\n\t            wav_arr, top_db=top_db, frame_length=self.hps.frame_length_sample,\n\t            hop_length=self.hps.frame_shift_sample)\n\t        for top_db in np.arange(top_db, lower_db, 0.1):\n\t            trimed, inds = librosa.effects.trim(\n\t                wav_arr, top_db=top_db, frame_length=self.hps.frame_length_sample,\n\t                hop_length=self.hps.frame_shift_sample)\n\t            if old_inds[0] - inds[0] + inds[1] - old_inds[1] >= sample_len_th:\n", "                break\n\t            else:\n\t                old_inds = inds\n\t        trimed = wav_arr[old_inds[0]: old_inds[1]]\n\t        return trimed\n\t    def spectrogram(self, y, clip_norm=True):\n\t        D = self._stft(y)\n\t        S = self._amp_to_db(np.abs(D)) - self.hps.ref_level_db\n\t        if clip_norm:\n\t            S = self._normalize(S)\n", "        return S\n\t    def logf0(self, y):\n\t        lower_f0 = self.hps.lower_f0\n\t        upper_f0 = self.hps.upper_f0\n\t        sr = self.hps.sample_rate\n\t        hop_len = self.hps.frame_shift_sample\n\t        x = y / max(abs(y)) * 32768.0\n\t        f0 = pysptk.sptk.rapt(x, sr, hopsize=hop_len, min=lower_f0, max=upper_f0, otype='f0')\n\t        logf0 = np.log(np.where(f0 == 0., np.ones_like(f0), f0))\n\t        return logf0\n", "    def inv_spectrogram(self, spectrogram):\n\t        S = self._db_to_amp(self._denormalize(spectrogram) + self.hps.ref_level_db)\n\t        return self._griffin_lim(S ** self.hps.power)\n\t    def test(self, y, clip_norm=True):\n\t        D = self._stft(y)\n\t        src = np.abs(D)\n\t        print('linear: ', np.min(src), np.max(src))\n\t        mel_ = self._linear_to_mel(np.abs(D))\n\t        print('mel_linear: ', np.min(mel_), np.max(mel_))\n\t        mel_db = self._amp_to_db(mel_)\n", "        print('mel_db: ', np.min(mel_db), np.max(mel_db))\n\t        mel_db_ref = mel_db - self.hps.ref_level_db\n\t        print('mel_db_ref: ', np.min(mel_db_ref), np.max(mel_db_ref))\n\t        if clip_norm:\n\t            mel_db_ref = self._normalize(mel_db_ref)\n\t            print('mel_db_ref_norm: ', np.min(mel_db_ref), np.max(mel_db_ref))\n\t            mel_db_ref_denorm = self._denormalize(mel_db_ref)\n\t            print('mel_db_ref_denorm: ', np.min(mel_db_ref_denorm), np.max(mel_db_ref_denorm))\n\t        else:\n\t            mel_db_ref_denorm = mel_db_ref\n", "        mel_db_de_ref = mel_db_ref_denorm + self.hps.ref_level_db\n\t        print('mel_db_de_ref: ', np.min(mel_db_de_ref), np.max(mel_db_de_ref))\n\t        mel_linear = self._db_to_amp(mel_db_de_ref)\n\t        print('mel_linear_re: ', np.min(mel_linear), np.max(mel_linear))\n\t        linear_sp = self._mel_to_linear(mel_linear)\n\t        print('linear_re: ', np.min(linear_sp), np.max(linear_sp))\n\t        print(np.mean(np.abs(src - linear_sp)))\n\t    def melspectrogram(self, y, clip_norm=True):\n\t        D = self._stft(y)\n\t        S = self._amp_to_db(self._linear_to_mel(np.abs(D))) - self.hps.ref_level_db\n", "        if clip_norm:\n\t            S = self._normalize(S)\n\t        return S\n\t    def inv_mel_spectrogram(self, mel_spectrogram):\n\t        S = self._mel_to_linear(self._db_to_amp(\n\t            self._denormalize(mel_spectrogram) + self.hps.ref_level_db))\n\t        return self._griffin_lim(S ** self.hps.power)\n\t    def find_endpoint(self, wav, threshold_db=-40.0, min_silence_sec=0.8):\n\t        window_length = int(self.hps.sample_rate * min_silence_sec)\n\t        hop_length = int(window_length / 4)\n", "        threshold = self._db_to_amp(threshold_db)\n\t        for x in range(hop_length, len(wav) - window_length, hop_length):\n\t            if np.max(wav[x: x + window_length]) < threshold:\n\t                return x + hop_length\n\t        return len(wav)\n\t    def _griffin_lim(self, S):\n\t        angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n\t        S_complex = np.abs(S).astype(np.complex)\n\t        y = self._istft(S_complex * angles)\n\t        for i in range(self.hps.griffin_lim_iters):\n", "            angles = np.exp(1j * np.angle(self._stft(y)))\n\t            y = self._istft(S_complex * angles)\n\t        return y\n\t    def _stft(self, y):\n\t        n_fft, hop_length, win_length = self._stft_parameters()\n\t        if len(y.shape) == 1:  # [time_steps]\n\t            return librosa.stft(y=y, n_fft=n_fft,\n\t                                hop_length=hop_length,\n\t                                win_length=win_length,\n\t                                center=self.hps.center)\n", "        elif len(y.shape) == 2:  # [batch_size, time_steps]\n\t            if y.shape[0] == 1:  # batch_size=1\n\t                return np.expand_dims(librosa.stft(y=y[0], n_fft=n_fft,\n\t                                                   hop_length=hop_length,\n\t                                                   win_length=win_length,\n\t                                                   center=self.hps.center),\n\t                                      axis=0)\n\t            else:  # batch_size > 1\n\t                spec_list = list()\n\t                for wav in y:\n", "                    spec_list.append(librosa.stft(y=wav, n_fft=n_fft,\n\t                                                  hop_length=hop_length,\n\t                                                  win_length=win_length,\n\t                                                  center=self.hps.center))\n\t                return np.concatenate(spec_list, axis=0)\n\t        else:\n\t            raise Exception('Wav dimension error in stft function!')\n\t    def _istft(self, y):\n\t        _, hop_length, win_length = self._stft_parameters()\n\t        if len(y.shape) == 2:  # spectrogram shape: [n_frame, n_fft]\n", "            return librosa.istft(y, hop_length=hop_length,\n\t                                 win_length=win_length,\n\t                                 center=self.hps.center)\n\t        elif len(y.shape) == 3:  # spectrogram shape: [batch_size, n_frame, n_fft]\n\t            if y.shape[0] == 1:  # batch_size = 1\n\t                return np.expand_dims(librosa.istft(y[0],\n\t                                                    hop_length=hop_length,\n\t                                                    win_length=win_length,\n\t                                                    center=self.hps.center),\n\t                                      axis=0)\n", "            else:  # batch_size > 1\n\t                wav_list = list()\n\t                for spec in y:\n\t                    wav_list.append(librosa.istft(spec,\n\t                                                  hop_length=hop_length,\n\t                                                  win_length=win_length,\n\t                                                  center=self.hps.center))\n\t                    return np.concatenate(wav_list, axis=0)\n\t        else:\n\t            raise Exception('Spectrogram dimension error in istft function!')\n", "    def _stft_parameters(self):\n\t        n_fft = (self.hps.num_freq - 1) * 2\n\t        # hop_length = int(self.hps.frame_shift_ms / 1000 * self.hps.sample_rate)\n\t        # win_length = int(self.hps.frame_length_ms / 1000 * self.hps.sample_rate)\n\t        hop_length = self.hps.frame_shift_sample\n\t        win_length = self.hps.frame_length_sample\n\t        return n_fft, hop_length, win_length\n\t    def _linear_to_mel(self, spectrogram):\n\t        _mel_basis = self._build_mel_basis()\n\t        return np.dot(_mel_basis, spectrogram)\n", "    def _mel_to_linear(self, mel_spectrogram):\n\t        _inv_mel_basis = np.linalg.pinv(self._build_mel_basis())\n\t        linear_spectrogram = np.dot(_inv_mel_basis, mel_spectrogram)\n\t        if len(linear_spectrogram.shape) == 3:\n\t            # for 3-dimension mel, the shape of\n\t            # inverse linear spectrogram will be [num_freq, batch_size, n_frame]\n\t            linear_spectrogram = np.transpose(linear_spectrogram, [1, 0, 2])\n\t        return np.maximum(1e-10, linear_spectrogram)\n\t    def _build_mel_basis(self):\n\t        n_fft = (self.hps.num_freq - 1) * 2\n", "        return librosa.filters.mel(\n\t            self.hps.sample_rate,\n\t            n_fft=n_fft,\n\t            n_mels=self.hps.num_mels,\n\t            fmin=self.hps.min_mel_freq,\n\t            fmax=self.hps.max_mel_freq)\n\t    @staticmethod\n\t    def _amp_to_db(x):\n\t        return 20 * np.log10(np.maximum(1e-5, x))\n\t    @staticmethod\n", "    def _db_to_amp(x):\n\t        return np.power(10.0, x * 0.05)\n\t    def _normalize(self, S):\n\t        if self.hps.symmetric_specs:\n\t            return np.clip(\n\t                (2 * self.hps.max_abs_value) * (\n\t                        (S - self.hps.min_level_db) / (-self.hps.min_level_db)\n\t                ) - self.hps.max_abs_value,\n\t                -self.hps.max_abs_value, self.hps.max_abs_value)\n\t        else:\n", "            return np.clip(self.hps.max_abs_value * (\n\t                    (S - self.hps.min_level_db) / (-self.hps.min_level_db)),\n\t                           0, self.hps.max_abs_value)\n\t    def _denormalize(self, S):\n\t        if self.hps.symmetric_specs:\n\t            return ((np.clip(S, -self.hps.max_abs_value, self.hps.max_abs_value)\n\t                     + self.hps.max_abs_value) * (-self.hps.min_level_db)\n\t                    / (2 * self.hps.max_abs_value)\n\t                    + self.hps.min_level_db)\n\t        else:\n", "            return ((np.clip(S, 0, self.hps.max_abs_value) * (-self.hps.min_level_db)\n\t                     / self.hps.max_abs_value)\n\t                    + self.hps.min_level_db)\n\t    def preemphasize(self, x):\n\t        if len(x.shape) == 1:  # [time_steps]\n\t            return signal.lfilter([1, -self.hps.preemphasize], [1], x).astype(x.dtype)\n\t        elif len(x.shape) == 2:  # [batch_size, time_steps]\n\t            if x.shape[0] == 1:\n\t                return np.expand_dims(\n\t                    signal.lfilter([1, -self.hps.preemphasize], [1], x[0]), axis=0).astype(x.dtype)\n", "            wav_list = list()\n\t            for wav in x:\n\t                wav_list.append(signal.lfilter([1, -self.hps.preemphasize], [1], wav).astype(x.dtype))\n\t            return np.concatenate(wav_list, axis=0)\n\t        else:\n\t            raise Exception('Wave dimension error in pre-emphasis')\n\t    def inv_preemphasize(self, x):\n\t        if self.hps.preemphasize is None:\n\t            return x\n\t        if len(x.shape) == 1:  # [time_steps]\n", "            return signal.lfilter([1], [1, -self.hps.preemphasize], x)\n\t        elif len(x.shape) == 2:  # [batch_size, time_steps]\n\t            if x.shape[0] == 1:\n\t                return np.expand_dims(\n\t                    signal.lfilter([1], [1, -self.hps.preemphasize], x[0]), axis=0)\n\t            wav_list = list()\n\t            for wav in x:\n\t                wav_list.append(signal.lfilter([1], [1, -self.hps.preemphasize], wav))\n\t            return np.concatenate(wav_list, axis=0)\n\t        else:\n", "            raise Exception('Wave dimension error in inverse pre-emphasis')\n\t    def mfcc(self, y):\n\t        from scipy.fftpack import dct\n\t        preemphasized = self.preemphasize(y)\n\t        D = self._stft(preemphasized)\n\t        S = librosa.power_to_db(self._linear_to_mel(np.abs(D) ** 2))\n\t        mfcc = dct(x=S, axis=0, type=2, norm='ortho')[:self.hps.n_mfcc]\n\t        deltas = librosa.feature.delta(mfcc)\n\t        delta_deltas = librosa.feature.delta(mfcc, order=2)\n\t        mfcc_feature = np.concatenate((mfcc, deltas, delta_deltas), axis=0)\n", "        return mfcc_feature.T\n\t    def hyper_parameters_estimation(self, wav_dir):\n\t        from tqdm import tqdm\n\t        wavs = []\n\t        for root, dirs, files in os.walk(wav_dir):\n\t            for f in files:\n\t                if re.match(r'.+\\.wav', f):\n\t                    wavs.append(os.path.join(root, f))\n\t        mel_db_min = 100.0\n\t        mel_db_max = -100.0\n", "        for f in tqdm(wavs):\n\t            wav_arr = self.load_wav(f)\n\t            pre_emphasized = self.preemphasize(wav_arr)\n\t            D = self._stft(pre_emphasized)\n\t            S = self._amp_to_db(self._linear_to_mel(np.abs(D)))\n\t            mel_db_max = np.max(S) if np.max(S) > mel_db_max else mel_db_max\n\t            mel_db_min = np.min(S) if np.min(S) < mel_db_min else mel_db_min\n\t        return mel_db_min, mel_db_max\n\t    def _magnitude_spectrogram(self, audio, clip_norm):\n\t        preemp_audio = self.preemphasize(audio)\n", "        mel_spec = self.melspectrogram(preemp_audio, clip_norm)\n\t        linear_spec = self.spectrogram(preemp_audio, clip_norm)\n\t        return mel_spec.T, linear_spec.T\n\t    def _energy_spectrogram(self, audio):\n\t        preemp_audio = self.preemphasize(audio)\n\t        linear_spec = np.abs(self._stft(preemp_audio)) ** 2\n\t        mel_spec = self._linear_to_mel(linear_spec)\n\t        return mel_spec.T, linear_spec.T\n\t    def _extract_min_max(self, wav_path, mode, post_fn=lambda x: x):\n\t        num_mels = self.hps.num_mels\n", "        num_linears = self.hps.num_freq\n\t        wavs = []\n\t        for root, dirs, files in os.walk(wav_path):\n\t            for f in files:\n\t                if re.match(r'.+\\.wav', f):\n\t                    wavs.append(os.path.join(root, f))\n\t        num_wavs = len(wavs)\n\t        mel_mins_per_wave = np.zeros((num_wavs, num_mels))\n\t        mel_maxs_per_wave = np.zeros((num_wavs, num_mels))\n\t        linear_mins_per_wave = np.zeros((num_wavs, num_linears))\n", "        linear_maxs_per_wave = np.zeros((num_wavs, num_linears))\n\t        for i, wav in enumerate(post_fn(wavs)):\n\t            audio, sr = soundfile.read(wav)\n\t            if mode == 'magnitude':\n\t                mel, linear = self._magnitude_spectrogram(audio, clip_norm=False)\n\t            elif mode == 'energy':\n\t                mel, linear = self._energy_spectrogram(audio)\n\t            else:\n\t                raise Exception('Only magnitude or energy is supported!')\n\t            mel_mins_per_wave[i,] = np.amin(mel, axis=0)\n", "            mel_maxs_per_wave[i,] = np.amax(mel, axis=0)\n\t            linear_mins_per_wave[i,] = np.amin(linear, axis=0)\n\t            linear_maxs_per_wave[i,] = np.amax(linear, axis=0)\n\t        mel_mins = np.reshape(np.amin(mel_mins_per_wave, axis=0), (1, num_mels))\n\t        mel_maxs = np.reshape(np.amax(mel_maxs_per_wave, axis=0), (1, num_mels))\n\t        linear_mins = np.reshape(np.amin(linear_mins_per_wave, axis=0), (1, num_mels))\n\t        linear_maxs = np.reshape(np.amax(linear_mins_per_wave, axis=0), (1, num_mels))\n\t        min_max = {\n\t            'mel_min': mel_mins,\n\t            'mel_max': mel_maxs,\n", "            'linear_mins': linear_mins,\n\t            'linear_max': linear_maxs\n\t        }\n\t        return min_max\n\t    @staticmethod\n\t    def _normalize_min_max(spec, maxs, mins, max_value=1.0, min_value=0.0):\n\t        spec_dim = len(spec.T)\n\t        num_frame = len(spec)\n\t        max_min = maxs - mins\n\t        max_min = np.reshape(max_min, (1, spec_dim))\n", "        max_min[max_min <= 0.0] = 1.0\n\t        target_max_min = np.zeros((1, spec_dim))\n\t        target_max_min.fill(max_value - min_value)\n\t        target_max_min[max_min <= 0.0] = 1.0\n\t        spec_min = np.tile(mins, (num_frame, 1))\n\t        target_min = np.tile(min_value, (num_frame, spec_dim))\n\t        spec_range = np.tile(max_min, (num_frame, 1))\n\t        norm_spec = np.tile(target_max_min, (num_frame, 1)) / spec_range\n\t        norm_spec = norm_spec * (spec - spec_min) + target_min\n\t        return norm_spec\n", "    @staticmethod\n\t    def _denormalize_min_max(spec, maxs, mins, max_value=1.0, min_value=0.0):\n\t        spec_dim = len(spec.T)\n\t        num_frame = len(spec)\n\t        max_min = maxs - mins\n\t        max_min = np.reshape(max_min, (1, spec_dim))\n\t        max_min[max_min <= 0.0] = 1.0\n\t        target_max_min = np.zeros((1, spec_dim))\n\t        target_max_min.fill(max_value - min_value)\n\t        target_max_min[max_min <= 0.0] = 1.0\n", "        spec_min = np.tile(mins, (num_frame, 1))\n\t        target_min = np.tile(min_value, (num_frame, spec_dim))\n\t        spec_range = np.tile(max_min, (num_frame, 1))\n\t        denorm_spec = spec_range / np.tile(target_max_min, (num_frame, 1))\n\t        denorm_spec = denorm_spec * (spec - target_min) + spec_min\n\t        return denorm_spec\n\t    @staticmethod\n\t    def rescale(mel):\n\t        x = np.linspace(1, mel.shape[0], mel.shape[0])\n\t        xn = np.linspace(1, mel.shape[0], int(mel.shape[0] * 1.25))\n", "        f = interpolate.interp1d(x, mel, kind='cubic', axis=0)\n\t        rescaled_mel = f(xn)\n\t        return rescaled_mel\n"]}
{"filename": "audio/__init__.py", "chunked_list": ["from .audio import *\n\tfrom .utils import *\n"]}
{"filename": "audio/utils.py", "chunked_list": ["import os\n\timport threading\n\timport multiprocessing\n\timport numpy as np\n\timport matplotlib\n\timport matplotlib.pyplot as plt\n\tfrom audio import Audio\n\tclass TestUtils:\n\t    def __init__(self, hps, save_dir):\n\t        self.prcocessor = Audio(hps.Audio)\n", "        self.hps = hps\n\t        self.save_dir = save_dir\n\t    def write_mels(self, step, mel_batch, mel_lengths, ids, prefix=''):\n\t        for i in range(mel_batch.shape[0]):\n\t            mel = mel_batch[i][:mel_lengths[i], :]\n\t            idx = ids[i].decode('utf-8') if type(ids[i]) is bytes else ids[i]\n\t            mel_name = os.path.join(self.save_dir, '{}-{}-{}.npy'.format(prefix, idx, step))\n\t            np.save(mel_name, mel)\n\t        return\n\t    def synthesize_and_save_wavs(self, step, mel_batch, mel_lengths, ids, prefix=''):\n", "        def _synthesize(mel, fid):\n\t            wav_arr = self.prcocessor.inv_mel_spectrogram(mel.T)\n\t            wav_arr = self.prcocessor.inv_preemphasize(wav_arr)\n\t            self.prcocessor.save_wav(wav_arr, os.path.join(self.save_dir, '{}-{}-{}.wav'.format(prefix, fid, step)))\n\t            return\n\t        threads = []\n\t        for i in range(mel_batch.shape[0]):\n\t            mel = mel_batch[i][:mel_lengths[i], :]\n\t            idx = ids[i].decode('utf-8') if type(ids[i]) is bytes else ids[i]\n\t            t = threading.Thread(target=_synthesize, args=(mel, idx))\n", "            threads.append(t)\n\t            t.start()\n\t        for x in threads:\n\t            x.join()\n\t        return\n\t    @staticmethod\n\t    def draw_mel_process(args):\n\t        mel, ml, save_name = args\n\t        plt.imshow(mel[:ml, :].T, aspect='auto', origin='lower')\n\t        plt.tight_layout()\n", "        plt.savefig('{}'.format(save_name))\n\t        plt.close()\n\t    def draw_melspectrograms(self, step, mel_batch, mel_lengths, ids, prefix=''):\n\t        matplotlib.use('agg')\n\t        save_names = []\n\t        for idx in ids:\n\t            idx = idx.decode('utf-8') if type(idx) is bytes else idx\n\t            save_name = self.save_dir + '/{}-{}-{}.pdf'.format(prefix, idx, step)\n\t            save_names.append(save_name)\n\t        pool = multiprocessing.Pool()\n", "        data = zip(mel_batch, mel_lengths, save_names)\n\t        pool.map(TestUtils.draw_mel_process, data)\n\t        return\n\t    def draw_self_att_process(self, args):\n\t        ali, mlen, save_name = args\n\t        fig, ax = plt.subplots()\n\t        ax.imshow(ali[:mlen, : mlen], aspect='auto', origin='lower')\n\t        plt.tight_layout()\n\t        plt.savefig('{}.pdf'.format(save_name))\n\t        plt.close()\n", "        return\n\t    def draw_multi_head_self_att_process(self, args):\n\t        ali, mlen, save_name, num_heads = args\n\t        fig = plt.figure()\n\t        for j, head_ali in enumerate(ali):\n\t            ax = fig.add_subplot(2, num_heads // 2, j + 1)\n\t            ax.imshow(head_ali[:mlen, :mlen], aspect='auto', origin='lower')\n\t        plt.tight_layout()\n\t        plt.savefig('{}'.format(save_name))\n\t        plt.close()\n", "        return\n\t    def multi_draw_self_att_alignments(self, batch_ali, mel_lengths, step, ids, prefix='posterior'):\n\t        matplotlib.use('agg')\n\t        save_names = []\n\t        for idx in ids:\n\t            idx = idx.decode('utf-8') if type(idx) is bytes else idx\n\t            save_name = self.save_dir + '/{}-{}-{}.pdf'.format(prefix, idx, step)\n\t            save_names.append(save_name)\n\t        pool = multiprocessing.Pool()\n\t        if len(batch_ali.shape) == 3:\n", "            data = zip(batch_ali, mel_lengths, save_names)\n\t            pool.map(self.draw_self_att_process, data)\n\t        elif len(batch_ali.shape) == 4:\n\t            data = zip(batch_ali, mel_lengths, save_names,\n\t                       [batch_ali.shape[1]] * batch_ali.shape[0])\n\t            pool.map(self.draw_multi_head_self_att_process, data)\n\t        print('Attentions for {} are plotted'.format(prefix))\n\t        return\n"]}
{"filename": "configs/logger.py", "chunked_list": ["import os\n\timport sys\n\tclass Logger(object):\n\t    def __init__(self, log_dir, log_f='train.log'):\n\t        self.terminal = sys.stdout\n\t        log_fn = os.path.join(log_dir, log_f)\n\t        self.log = open(log_fn, \"a\")\n\t    def write(self, message):\n\t        self.terminal.write(message)\n\t        self.log.write(message)\n", "    def flush(self):\n\t        #this flush method is needed for python 3 compatibility.\n\t        #this handles the flush command by doing nothing.\n\t        #you might want to specify some extra behavior here.\n\t        pass\n"]}
{"filename": "configs/__init__.py", "chunked_list": ["from .hparams import *\n\tfrom .logger import Logger\n"]}
{"filename": "configs/hparams.py", "chunked_list": ["class CNENHPS:\n\t    class Train:\n\t        random_seed = 123\n\t        epochs = 1000\n\t        train_batch_size = 32\n\t        test_batch_size = 8\n\t        test_interval = 100\n\t        ckpt_interval = 50\n\t        shuffle_buffer = 128\n\t        shuffle = True\n", "        num_samples = 1\n\t        length_weight = 1.\n\t        content_kl_weight = 5e-3\n\t        spk_kl_weight = 1e-5\n\t        learning_rate = 1.25e-4\n\t    class Dataset:\n\t        buffer_size = 65536\n\t        num_parallel_reads = 64\n\t        dev_set_rate = 0.05\n\t        test_set_rate = 0.05\n", "        chunk_size = 256  # the length of the mel-sepctrogram that is required by speaker encoder\n\t        segment_size = 16  # the length of the smallest unit to segment and shuffle the chunk\n\t        n_record_split = 32\n\t        preprocess_n_jobs = 16\n\t        # define dataset specifics below\n\t        class VCTK:\n\t            corpus_dir = '/path/to/extracted/vctk'\n\t            val_spks = ['p225', 'p243', 'p231', 'p251', 'p258', 'p271', 'p284', 'p326', 'p374', 'p334']\n\t            test_spks = ['p274', 'p293', 'p360', 'p262', 'p314', 'p239',  'p273', 'p302', 'p270', 'p340']\n\t        class AiShell3:\n", "            corpus_dir = '/path/to/extracted/data_aishell3'\n\t            train_spk_file = './train-speakers.txt'\n\t            val_test_spk_file = './test-speakers.txt'\n\t        dataset_dir = '/path/to/save/features'\n\t    class Audio:\n\t        num_mels = 80\n\t        num_freq = 1025\n\t        min_mel_freq = 0.\n\t        max_mel_freq = 8000.\n\t        sample_rate = 16000\n", "        frame_length_sample = 800\n\t        frame_shift_sample = 200\n\t        preemphasize = 0.97\n\t        sil_trim_db = 20.\n\t        min_level_db = -100.0\n\t        ref_level_db = 20.0\n\t        max_abs_value = 1\n\t        symmetric_specs = False\n\t        griffin_lim_iters = 60\n\t        power = 1.5\n", "        center = True\n\t    class Common:\n\t        latent_dim = 128\n\t        output_dim = 80\n\t        reduction_factor = 2\n\t    class Decoder:\n\t        class Transformer:\n\t            pre_n_conv = 2\n\t            pre_conv_kernel = 3\n\t            pre_drop_rate = 0.2\n", "            nblk = 2\n\t            attention_dim = 256\n\t            attention_heads = 4\n\t            ffn_hidden = 1024\n\t            attention_temperature = 1.\n\t            attention_causality = False\n\t            attention_window = 16\n\t            post_n_conv = 5\n\t            post_conv_filters = 256\n\t            post_conv_kernel = 5\n", "            post_drop_rate = 0.2\n\t    class ContentPosterior:\n\t        class Transformer:\n\t            pre_n_conv = 2\n\t            pre_conv_kernel = 3\n\t            pre_hidden = 256\n\t            pre_drop_rate = 0.2\n\t            pos_drop_rate = 0.2\n\t            nblk = 2\n\t            attention_dim = 256\n", "            attention_heads = 4\n\t            temperature = 1.0\n\t            attention_causality = False\n\t            attention_window = 8\n\t            ffn_hidden = 1024\n\t    class SpkPosterior:\n\t        class ConvSpkEncoder:\n\t            hidden_channels = 256\n\t            conv_kernels = [3, 3, 5, 5]\n\t            activation = 'relu'\n"]}
{"filename": "tests/compute_eer.py", "chunked_list": ["import os\n\timport json\n\timport random\n\timport argparse\n\timport scipy.stats\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom scipy.optimize import brentq\n\tfrom sklearn.metrics import roc_curve\n\tfrom scipy.interpolate import interp1d\n", "def create_trials(data_dir, mode='spk', anchor_f=None):\n\t    \"\"\"\n\t    :param data_dir: under which there are spk_uid.npy\n\t    :param mode: suffix of the feature numpy filename\n\t    :return: trials: [(enrolled_vector, test_vector), ...]\n\t             labels: [0, 1, ...]\n\t             spk2vecs: {spk: {fid: [arr, ...]}}\n\t    \"\"\"\n\t    assert mode in ['spk', 'content']\n\t    def get_spk_fid(basename):\n", "        if basename.startswith('SSB'):\n\t            spk = basename[:7]\n\t            fid = basename.split('-')[0]\n\t        else:\n\t            spk = basename.split('_')[0]\n\t            fid = basename.split('-')[0]\n\t        return spk, fid\n\t    spk2vecs = {}\n\t    for f in os.listdir(data_dir):\n\t        if f.endswith('{}.npy'.format(mode)):\n", "            spk, fid = get_spk_fid(f)\n\t            path = os.path.join(data_dir, f)\n\t            feat = np.load(path)\n\t            if len(feat.shape) == 2:\n\t                feat = np.mean(feat, axis=0)\n\t            if spk in spk2vecs.keys():\n\t                spk2vecs[spk][fid] = feat\n\t            else:\n\t                spk2vecs[spk] = {fid: feat}\n\t    trials = []\n", "    labels = []\n\t    if anchor_f is not None:\n\t        anchor_dict = get_anchors(anchor_f)\n\t    else:\n\t        anchor_dict = {}\n\t        for spk in spk2vecs.keys():\n\t            if len(spk2vecs[spk].keys()) <= 100:\n\t                continue\n\t            anchors = random.sample(list(spk2vecs[spk].keys()), 4)\n\t            anchor_dict[spk] = anchors\n", "        lang = 'CN' if spk.startswith('SSB') else 'EN'\n\t        with open('SV_anchors-{}.json'.format(lang), 'w', encoding='utf-8') as f:\n\t            json.dump(anchor_dict, f)\n\t    for spk in spk2vecs.keys():\n\t        if len(spk2vecs[spk].keys()) <= 100:\n\t            continue\n\t        anchors = anchor_dict[spk]\n\t        aps = list(spk2vecs[spk].keys())\n\t        for anchor in anchors:\n\t            if anchor not in aps:\n", "                print(anchor)\n\t            else:\n\t                aps.remove(anchor)\n\t        ans = []\n\t        for s in spk2vecs.keys():\n\t            if s != spk:\n\t                ans += list(spk2vecs[s].keys())\n\t        anchor_key = '{}_anchor'.format(spk)\n\t        spk2vecs[spk][anchor_key] = np.mean(\n\t            np.stack([spk2vecs[spk][k] for k in anchors], axis=0), axis=0)\n", "        for ap in aps:\n\t            trials.append((anchor_key, ap))\n\t            labels.append(1)\n\t        for an in ans:\n\t            trials.append((anchor_key, an))\n\t            labels.append(0)\n\t    return trials, labels, spk2vecs\n\tdef get_anchors(anchor_f):\n\t    with open(anchor_f, 'r') as f:\n\t        anchor_dict = json.load(f)  # {spk: [fid1, fid2, ...]}\n", "    return anchor_dict\n\tdef mean_confidence_interval(data, confidence=0.95):\n\t    a = 1.0 * np.array(data)\n\t    n = len(a)\n\t    m, se = np.mean(a), scipy.stats.sem(a)\n\t    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n - 1)\n\t    return m, h\n\tdef cosine_score(trials, vecs_dict):\n\t    def get_spk(inp_str):\n\t        if inp_str.startswith('SSB'):\n", "            spk = inp_str[:7]\n\t        else:\n\t            spk = inp_str.split('_')[0]\n\t        return spk\n\t    scores = []\n\t    for item in tqdm(trials):\n\t        spk0 = get_spk(item[0])\n\t        enroll_vector = vecs_dict[spk0][item[0]]\n\t        spk1 = get_spk(item[1])\n\t        test_vector = vecs_dict[spk1][item[1]]\n", "        score = enroll_vector.dot(test_vector.T)\n\t        denom = np.linalg.norm(enroll_vector) * np.linalg.norm(test_vector)\n\t        score = score / denom\n\t        scores.append(score)\n\t    return scores\n\tdef compute_eer(labels, scores):\n\t    \"\"\"sklearn style compute eer\n\t    \"\"\"\n\t    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n\t    eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n", "    threshold = interp1d(fpr, thresholds)(eer)\n\t    return eer, threshold\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        '--data_dir', type=str,\n\t        help='directory of all test speaker embedding vectors', required=True)\n\t    parser.add_argument(\n\t        '--mode', type=str, choices=['spk', 'content'], required=True)\n\t    parser.add_argument('--n_runs', type=int, default=1)\n", "    parser.add_argument('--anchor', type=str, help='Anchor file')\n\t    args = parser.parse_args()\n\t    assert os.path.isdir(args.data_dir)\n\t    eers = []\n\t    labels = []\n\t    for _ in range(args.n_runs):\n\t        trials, labels, spk2vecs = create_trials(args.data_dir, args.mode, anchor_f=args.anchor)\n\t        scores = cosine_score(trials, spk2vecs)\n\t        eer, _ = compute_eer(labels, scores)\n\t        eers.append(eer)\n", "    eer, h = mean_confidence_interval(eers)\n\t    n_trials = len(labels)\n\t    n_pos = sum(labels)\n\t    n_neg = n_trials - n_pos\n\t    print('There are {} trials for {} representation, {} of them are positive trials '\n\t          'and {} are negative ones.'.format(n_trials, args.mode, n_pos, n_neg))\n\t    print('The EER is {:.4f} ± {:.4f}'.format(eer, h))\n\t    return\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "models/vc.py", "chunked_list": ["import tensorflow as tf\n\tfrom modules.decoder import TransformerDecoder\n\tfrom modules.posterior import TransformerPosterior, ConvSpkEncoder\n\tclass BetaVAEVC(tf.keras.Model):\n\t    def __init__(self, hps, name='BetaVAEVC', **kwargs):\n\t        super(BetaVAEVC, self).__init__(name=name, **kwargs)\n\t        self.hps = hps\n\t        self.n_sample = hps.Train.num_samples\n\t        self.reduction_factor = hps.Common.reduction_factor\n\t        self.chunk_size = hps.Dataset.chunk_size\n", "        self.segment_size = hps.Dataset.segment_size\n\t        assert self.chunk_size % self.segment_size == 0\n\t        self.spk_posterior = ConvSpkEncoder(\n\t            hidden_channels=hps.SpkPosterior.ConvSpkEncoder.hidden_channels,\n\t            conv_kernels=hps.SpkPosterior.ConvSpkEncoder.conv_kernels,\n\t            out_channels=hps.Common.latent_dim,\n\t            activation=hps.SpkPosterior.ConvSpkEncoder.activation)\n\t        self.decoder = TransformerDecoder(\n\t            pre_n_conv=hps.Decoder.Transformer.pre_n_conv,\n\t            pre_conv_kernel=hps.Decoder.Transformer.pre_conv_kernel,\n", "            pre_drop_rate=hps.Decoder.Transformer.pre_drop_rate,\n\t            nblk=hps.Decoder.Transformer.nblk,\n\t            attention_dim=hps.Decoder.Transformer.attention_dim,\n\t            attention_heads=hps.Decoder.Transformer.attention_heads,\n\t            temperature=hps.Decoder.Transformer.attention_temperature,\n\t            attention_causality=hps.Decoder.Transformer.attention_causality,\n\t            attention_window=hps.Decoder.Transformer.attention_window,\n\t            ffn_hidden=hps.Decoder.Transformer.ffn_hidden,\n\t            post_n_conv=hps.Decoder.Transformer.post_n_conv,\n\t            post_conv_filters=hps.Decoder.Transformer.post_conv_filters,\n", "            post_conv_kernel=hps.Decoder.Transformer.post_conv_kernel,\n\t            post_drop_rate=hps.Decoder.Transformer.post_drop_rate,\n\t            out_dim=hps.Common.output_dim,\n\t            reduction_factor=hps.Common.reduction_factor,\n\t            name='transformer_decoder')\n\t        self.posterior = TransformerPosterior(\n\t            pre_n_conv=hps.ContentPosterior.Transformer.pre_n_conv,\n\t            pre_conv_kernel=hps.ContentPosterior.Transformer.pre_conv_kernel,\n\t            pre_hidden=hps.ContentPosterior.Transformer.pre_hidden,\n\t            pre_drop_rate=hps.ContentPosterior.Transformer.pre_drop_rate,\n", "            pos_drop_rate=hps.ContentPosterior.Transformer.pos_drop_rate,\n\t            nblk=hps.ContentPosterior.Transformer.nblk,\n\t            attention_dim=hps.ContentPosterior.Transformer.attention_dim,\n\t            attention_heads=hps.ContentPosterior.Transformer.attention_heads,\n\t            temperature=hps.ContentPosterior.Transformer.temperature,\n\t            attention_causality=hps.ContentPosterior.Transformer.attention_causality,\n\t            attention_window=hps.ContentPosterior.Transformer.attention_window,\n\t            ffn_hidden=hps.ContentPosterior.Transformer.ffn_hidden,\n\t            latent_dim=hps.Common.latent_dim)\n\t    def _compute_l2_loss(self, reconstructed, targets, lengths=None, reduce=None):\n", "        max_time = tf.shape(reconstructed)[1]\n\t        dim = tf.shape(reconstructed)[2]\n\t        r = tf.reshape(reconstructed, [-1, self.n_sample, max_time, dim])\n\t        t = tf.reshape(targets, [-1, self.n_sample, max_time, dim])\n\t        if lengths is not None:\n\t            seq_mask = tf.sequence_mask(lengths, max_time, dtype=tf.float32)\n\t            seq_mask = tf.reshape(seq_mask, [-1, self.n_sample, max_time])\n\t            reshaped_lens = tf.reshape(lengths, [-1, self.n_sample])\n\t            l2_loss = tf.reduce_mean(\n\t                tf.reduce_sum(\n", "                    tf.reduce_mean(tf.square(r - t) + tf.abs(r - t), axis=-1) * seq_mask,\n\t                    axis=-1) / tf.cast(reshaped_lens, tf.float32),\n\t                axis=-1)\n\t        else:\n\t            l2_loss = tf.math.reduce_mean(tf.square(r - t) + tf.abs(r - t), axis=[1, 2, 3])\n\t        if reduce:\n\t            return tf.math.reduce_mean(l2_loss)\n\t        else:\n\t            return l2_loss\n\t    @staticmethod\n", "    def kl_with_normal(logvar, mu, reduce=None):\n\t        kl = -0.5 * tf.reduce_sum(1. + logvar - mu ** 2 - tf.exp(logvar), axis=1)\n\t        if reduce:\n\t            return tf.reduce_mean(kl)\n\t        else:\n\t            return kl\n\t    @staticmethod\n\t    def kl_between_normal2d(mu1, logvar1, mu2, logvar2, reduce=None):\n\t        var1 = tf.math.exp(logvar1)\n\t        var2 = tf.math.exp(logvar2)\n", "        kl = 0.5 * tf.reduce_sum(\n\t            logvar2 - logvar1 - 1. + var1 / var2 + (mu2 - mu1) ** 2. / var2, axis=1)\n\t        if reduce:\n\t            kl = tf.reduce_mean(kl)\n\t        return kl\n\t    @staticmethod\n\t    def kl_between_normal3d(mu1, logvar1, mu2, logvar2, lengths=None, reduce=None):\n\t        var1 = tf.math.exp(logvar1)\n\t        var2 = tf.math.exp(logvar2)\n\t        kl = 0.5 * tf.reduce_sum(\n", "            logvar2 - logvar1 - 1. + var1 / var2 + (mu2 - mu1) ** 2. / var2, axis=2)\n\t        if lengths is None:\n\t            kl = tf.reduce_mean(kl, axis=1)\n\t        else:\n\t            max_len = tf.shape(mu1)[1]\n\t            mask = tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\n\t            kl = tf.reduce_sum(kl * mask, axis=1) / tf.cast(lengths, tf.float32)\n\t        if reduce:\n\t            kl = tf.reduce_mean(kl)\n\t        return kl\n", "    def random_shuffle(self, inputs):\n\t        \"\"\"\n\t        :param inputs: [batch, chunk_size, dim]\n\t        :return:\n\t        \"\"\"\n\t        bs = tf.shape(inputs)[0]\n\t        mel_dim = self.hps.Audio.num_mels\n\t        reshaped = tf.reshape(\n\t            inputs, [bs, self.chunk_size // self.segment_size, self.segment_size * mel_dim])\n\t        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n", "        shuffled = tf.random.shuffle(transposed)\n\t        shuffled = tf.transpose(shuffled, perm=[1, 0, 2])\n\t        shuffled = tf.reshape(shuffled, [bs, self.chunk_size, mel_dim])\n\t        return shuffled\n\t    def call(self, inputs, mel_lengths, inp_ext, training=None, reduce_loss=None):\n\t        \"\"\"\n\t        :param inputs: [batch, mel_max_time, mel_dim]\n\t        :param mel_lengths: [batch, ]\n\t        :param inp_ext: [batch, chunk_size, mel_dim]\n\t        :param training: bool\n", "        :param reduce_loss: bool\n\t        :return: predicted mel: [batch, mel_max_time, mel_dim]\n\t                 loss: float32\n\t        \"\"\"\n\t        print('Tracing back at Model.call')\n\t        # shape info\n\t        mel_max_len = tf.shape(inputs)[1]\n\t        # reduce the mels\n\t        reduced_mels = inputs[:, ::self.reduction_factor, :]\n\t        reduced_mels.set_shape([None, None, self.hps.Audio.num_mels])\n", "        reduced_mel_lens = (mel_lengths + self.reduction_factor - 1) // self.reduction_factor\n\t        reduced_max_len = tf.shape(reduced_mels)[1]\n\t        # text encoding\n\t        content_mu, content_logvar, _ = self.posterior(\n\t            reduced_mels, lengths=reduced_mel_lens, training=training)\n\t        # samples, eps: [batch, n_sample, mel_max_time, dim]\n\t        samples, eps = self.posterior.reparameterize(\n\t            content_mu, content_logvar, self.n_sample)\n\t        posterior_samples = tf.squeeze(samples, axis=1)\n\t        # compute speaker identity prior\n", "        ref_mel = self.random_shuffle(inp_ext)\n\t        spk_posterior_mu, spk_posterior_logvar = self.spk_posterior(ref_mel)\n\t        spk_posterior_embd = self.spk_posterior.sample(spk_posterior_mu, spk_posterior_logvar)\n\t        spk_kl = self.kl_between_normal2d(\n\t            spk_posterior_mu, spk_posterior_logvar, tf.zeros_like(spk_posterior_mu),\n\t            tf.zeros_like(spk_posterior_logvar), reduce=reduce_loss)\n\t        content_kl = self.kl_between_normal3d(\n\t            content_mu, content_logvar, tf.zeros_like(content_mu),\n\t            tf.zeros_like(content_logvar), reduced_mel_lens, reduce_loss)\n\t        posterior_samples_spk = tf.concat(\n", "            [posterior_samples, tf.tile(tf.expand_dims(spk_posterior_embd, axis=1),\n\t                                        [1, reduced_max_len, 1])], axis=2)\n\t        decoded_initial, decoded_post, _ = self.decoder(\n\t            posterior_samples_spk, reduced_mel_lens, training=training)\n\t        decoded_initial = decoded_initial[:, :mel_max_len, :]\n\t        decoded_post = decoded_post[:, :mel_max_len, :]\n\t        initial_l2_loss = self._compute_l2_loss(\n\t            decoded_initial, inputs, mel_lengths, reduce_loss)\n\t        post_l2_loss = self._compute_l2_loss(\n\t            decoded_post, inputs, mel_lengths, reduce_loss)\n", "        l2_loss = initial_l2_loss + post_l2_loss\n\t        return decoded_post, l2_loss, content_kl, spk_kl\n\t    def post_inference(self, inputs, mel_lengths, ref_mels):\n\t        spk_mu, spk_logs = self.spk_posterior(ref_mels)\n\t        inputs = inputs[:, ::self.reduction_factor, :]\n\t        mel_lengths = (mel_lengths + self.reduction_factor - 1) // self.reduction_factor\n\t        mu, _, _ = self.posterior(\n\t            inputs, lengths=mel_lengths, training=False)\n\t        reduced_max_len = tf.shape(mu)[1]\n\t        mu = tf.concat(\n", "            [mu, tf.tile(tf.expand_dims(spk_mu, axis=1), [1, reduced_max_len, 1])], axis=2)\n\t        _, predicted_mel, dec_alignments = self.decoder(\n\t            inputs=mu, lengths=mel_lengths, training=False)\n\t        return predicted_mel, dec_alignments\n"]}
{"filename": "models/__init__.py", "chunked_list": ["from .vc import *\n"]}
{"filename": "modules/posterior.py", "chunked_list": ["import numpy as np\n\timport tensorflow as tf\n\tfrom modules.attention import SelfAttentionBLK\n\tfrom modules.utils import PositionalEncoding, ConvPreNet, EncBlk, get_activation\n\tclass TransformerPosterior(tf.keras.layers.Layer):\n\t    def __init__(self, pre_n_conv, pre_conv_kernel, pre_hidden, pre_drop_rate,\n\t                 pos_drop_rate, nblk, attention_dim, attention_heads,\n\t                 temperature, attention_causality, attention_window,\n\t                 ffn_hidden, latent_dim, name='TransformerPosterior'):\n\t        super(TransformerPosterior, self).__init__(name=name)\n", "        self.pos_weight = tf.Variable(1.0, trainable=True)\n\t        self.prenet = ConvPreNet(\n\t            nconv=pre_n_conv, hidden=pre_hidden, conv_kernel=pre_conv_kernel, drop_rate=pre_drop_rate)\n\t        self.pe = PositionalEncoding('EncoderPositionEncoding')\n\t        self.pe_dropout = tf.keras.layers.Dropout(rate=pos_drop_rate)\n\t        self.attentions = []\n\t        for i in range(nblk):\n\t            attention = SelfAttentionBLK(\n\t                input_dim=pre_hidden, attention_dim=attention_dim,\n\t                attention_heads=attention_heads, attention_temperature=temperature,\n", "                causality=attention_causality, attention_window=attention_window,\n\t                ffn_hidden=ffn_hidden)\n\t            self.attentions.append(attention)\n\t        self.mu_projection = tf.keras.layers.Dense(\n\t            latent_dim, kernel_initializer='zeros', name='mu_projection')\n\t        self.logvar_projection = tf.keras.layers.Dense(\n\t            latent_dim, kernel_initializer='zeros', name='logvar_projection')\n\t    def call(self, inputs, lengths=None, training=None):\n\t        print('Tracing back at posterior call')\n\t        prenet_outs = self.prenet(inputs, training=training)\n", "        max_time = tf.shape(prenet_outs)[1]\n\t        dim = tf.shape(prenet_outs)[2]\n\t        pos = self.pe.positional_encoding(max_time, dim)\n\t        pos_embs = prenet_outs + self.pos_weight * pos\n\t        pos_embs = self.pe_dropout(pos_embs, training=training)\n\t        att_outs = pos_embs\n\t        for att in self.attentions:\n\t            att_outs, alignments = att(\n\t                inputs=att_outs, memory=att_outs, query_lengths=lengths,\n\t                memory_lengths=lengths, training=training)\n", "        mu = self.mu_projection(att_outs)\n\t        logvar = self.logvar_projection(att_outs)\n\t        return mu, logvar, att_outs\n\t    def sample(self, inputs, lengths, nsamples=tf.constant(1),\n\t               random=tf.constant(True), training=None):\n\t        mu, logvar, _ = self.call(inputs, lengths, training=training)\n\t        samples, eps = self.reparameterize(mu, logvar, nsamples, random)\n\t        log_probs = self.log_probability(mu, logvar, eps, lengths)\n\t        return samples, log_probs\n\t    @staticmethod\n", "    def reparameterize(mu, logvar, nsamples=tf.constant(1), random=tf.constant(True)):\n\t        \"\"\"\n\t        :param mu: [batch, max_time, dim]\n\t        :param logvar: [batch, max_time, dim]\n\t        :param nsamples: int\n\t        :param random: whether sample from N(0, 1) or just use zeros\n\t        :return: samples, noises, [batch, nsamples, max_time, dim]\n\t        \"\"\"\n\t        print('Tracing back at posterior reparameterize')\n\t        batch = tf.shape(mu)[0]\n", "        max_time = tf.shape(mu)[1]\n\t        dim = tf.shape(mu)[2]\n\t        std = tf.math.exp(0.5 * logvar)\n\t        if random:\n\t            eps = tf.random.normal([batch, nsamples, max_time, dim])\n\t        else:\n\t            eps = tf.zeros([batch, nsamples, max_time, dim])\n\t        samples = eps * tf.expand_dims(std, axis=1) + tf.expand_dims(mu, axis=1)\n\t        return samples, eps\n\t    @staticmethod\n", "    def log_probability(mu, logvar, z=None, eps=None, seq_lengths=None, epsilon=tf.constant(1e-8)):\n\t        \"\"\"\n\t        :param mu: [batch, max_time, dim]\n\t        :param logvar: [batch, max_time, dim]\n\t        :param z: [batch, nsamples, max_time, dim]\n\t        :param eps: [batch, nsamples, max_time, dim]\n\t        :param seq_lengths: [batch, ]\n\t        :param epsilon: small float number to avoid overflow\n\t        :return: log probabilities, [batch, nsamples]\n\t        \"\"\"\n", "        print('Tracing back at posterior log-probability')\n\t        batch = tf.shape(mu)[0]\n\t        max_time = tf.shape(mu)[1]\n\t        dim = tf.shape(mu)[2]\n\t        std = tf.math.exp(0.5 * logvar)\n\t        normalized_samples = (eps if eps is not None\n\t                              else (z - tf.expand_dims(mu, axis=1))\n\t                                   / (tf.expand_dims(std, axis=1) + epsilon))\n\t        expanded_logvar = tf.expand_dims(logvar, axis=1)\n\t        # time_level_log_probs [batch, nsamples, max_time]\n", "        time_level_log_probs = -0.5 * (\n\t                tf.cast(dim, tf.float32) * tf.math.log(2 * np.pi)\n\t                + tf.reduce_sum(expanded_logvar + normalized_samples ** 2.,\n\t                                axis=3))\n\t        seq_mask = (tf.sequence_mask(seq_lengths, maxlen=max_time, dtype=tf.float32)\n\t                    if seq_lengths is not None\n\t                    else tf.ones([batch, max_time]))\n\t        seq_mask = tf.expand_dims(seq_mask, axis=1)  # [batch, 1, max_time]\n\t        sample_level_log_probs = tf.reduce_sum(seq_mask * time_level_log_probs,\n\t                                               axis=2)  # [batch, nsamples]\n", "        return sample_level_log_probs\n\tclass ConvSpkEncoder(tf.keras.layers.Layer):\n\t    \"\"\"\n\t    Encode time-invariant features, e.g., speaker identity\n\t    \"\"\"\n\t    def __init__(self, hidden_channels, conv_kernels, out_channels, activation, name='ConvSpkEncoder'):\n\t        super(ConvSpkEncoder, self).__init__(name=name)\n\t        self.out_channels = out_channels\n\t        self.activation = get_activation(activation)\n\t        self.conv_initial = tf.keras.layers.Conv1D(hidden_channels, kernel_size=1, padding='valid')\n", "        self.conv_layers = [\n\t            EncBlk(hidden_channels, kernel_size=k, activation=self.activation, pooling_kernel=2)\n\t            for k in conv_kernels]\n\t        self.mu_logs_linear = tf.keras.layers.Dense(self.out_channels * 2, kernel_initializer='zeros')\n\t        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n\t    def call(self, x):\n\t        \"\"\"\n\t        :param x: [batch, channels, time-length]\n\t        :return: mu and logs of shape [batch, out_channels]\n\t        \"\"\"\n", "        print('Tracing back at ConvSpkEncoder.call')\n\t        h = self.conv_initial(x)\n\t        for conv in self.conv_layers:\n\t            h = conv(h)\n\t        h = self.activation(h)\n\t        h_avg = self.global_avg_pooling(h)\n\t        mu_logs = self.mu_logs_linear(h_avg)\n\t        mu, logs = tf.split(mu_logs, 2, axis=1)\n\t        return mu, logs\n\t    def sample(self, spk_mu, spk_logvar, temperature=1.):\n", "        batch_size = tf.shape(spk_mu)[0]\n\t        eps = tf.random.normal(\n\t            [batch_size, self.out_channels], mean=0., stddev=temperature)\n\t        return spk_mu + eps * tf.math.exp(0.5 * spk_logvar)\n\t    @staticmethod\n\t    def log_probability(spk_mu, spk_logvar, samples, epsilon=tf.constant(1e-8)):\n\t        spk_std = tf.math.exp(0.5 * spk_logvar)\n\t        normalized_samples = (samples - spk_mu) / (spk_std + epsilon)\n\t        # [batch,]\n\t        log_probs = -0.5 * tf.reduce_sum(\n", "            tf.math.log(2 * np.pi) + spk_logvar + normalized_samples ** 2., axis=1)\n\t        return log_probs\n"]}
{"filename": "modules/decoder.py", "chunked_list": ["import tensorflow as tf\n\tfrom modules.utils import Conv1D, PostNet, ConvPreNet\n\tfrom modules.attention import SelfAttentionBLK\n\tclass TransformerDecoder(tf.keras.layers.Layer):\n\t    def __init__(self, pre_n_conv, pre_conv_kernel, pre_drop_rate, nblk, attention_dim,\n\t                 attention_heads, temperature, attention_causality, attention_window,\n\t                 ffn_hidden, post_n_conv, post_conv_filters, post_conv_kernel, post_drop_rate,\n\t                 out_dim, reduction_factor, name='TransformerDecoder'):\n\t        super(TransformerDecoder, self).__init__(name=name)\n\t        self.reduction_factor = reduction_factor\n", "        self.out_dim = out_dim\n\t        self.pre_projection = ConvPreNet(\n\t            nconv=pre_n_conv, hidden=attention_dim, conv_kernel=pre_conv_kernel, drop_rate=pre_drop_rate)\n\t        self.attentions = []\n\t        for i in range(nblk):\n\t            attention = SelfAttentionBLK(\n\t                input_dim=attention_dim,\n\t                attention_dim=attention_dim,\n\t                attention_heads=attention_heads,\n\t                attention_temperature=temperature,\n", "                causality=attention_causality,\n\t                attention_window=attention_window,\n\t                ffn_hidden=ffn_hidden, name='decoder-attention-{}'.format(i))\n\t            self.attentions.append(attention)\n\t        self.out_projection = tf.keras.layers.Dense(units=out_dim * self.reduction_factor,\n\t                                                    name='linear_outputs')\n\t        self.postnet = PostNet(n_conv=post_n_conv, conv_filters=post_conv_filters,\n\t                               conv_kernel=post_conv_kernel, drop_rate=post_drop_rate,\n\t                               name='postnet')\n\t        self.residual_projection = tf.keras.layers.Dense(units=out_dim, name='residual_outputs')\n", "    def call(self, inputs, lengths=None, training=None):\n\t        print('Tracing back at Self-attention decoder')\n\t        batch_size = tf.shape(inputs)[0]\n\t        max_len = tf.shape(inputs)[1]\n\t        att_outs = self.pre_projection(inputs, training=training)\n\t        alignments = {}\n\t        for att in self.attentions:\n\t            att_outs, ali = att(\n\t                inputs=att_outs, memory=att_outs, query_lengths=lengths,\n\t                memory_lengths=lengths, training=training)\n", "            alignments[att.name] = ali\n\t        initial_outs = self.out_projection(att_outs)\n\t        initial_outs = tf.reshape(\n\t            initial_outs, [batch_size, max_len * self.reduction_factor, self.out_dim])\n\t        residual = self.postnet(initial_outs, training=training)\n\t        residual = self.residual_projection(residual)\n\t        outputs = residual + initial_outs\n\t        return initial_outs, outputs, alignments\n\tclass PitchPredictor(tf.keras.layers.Layer):\n\t    def __init__(self, n_conv, conv_filters, conv_kernel, drop_rate, name='PitchPredictor'):\n", "        super(PitchPredictor, self).__init__(name=name)\n\t        self.conv_stack = [Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n\t                                  activation=tf.nn.relu, drop_rate=drop_rate)\n\t                           for _ in range(n_conv)]\n\t        self.out_proj = tf.keras.layers.Dense(units=4)\n\t    def call(self, inputs, training=None):\n\t        conv_outs = inputs\n\t        for conv in self.conv_stack:\n\t            conv_outs = conv(conv_outs, training=training)\n\t        outs = self.out_proj(conv_outs)\n", "        return outs\n"]}
{"filename": "modules/__init__.py", "chunked_list": ["from .decoder import TransformerDecoder\n\tfrom .posterior import TransformerPosterior\n"]}
{"filename": "modules/utils.py", "chunked_list": ["import tensorflow as tf\n\tclass PreNet(tf.keras.layers.Layer):\n\t    def __init__(self, units, drop_rate, activation, name='PreNet', **kwargs):\n\t        super(PreNet, self).__init__(name=name, **kwargs)\n\t        self.dense1 = tf.keras.layers.Dense(\n\t            units=units, activation=activation, name='dense_1')\n\t        self.dense2 = tf.keras.layers.Dense(\n\t            units=units, activation=activation, name='dense_2')\n\t        self.dropout_layer = tf.keras.layers.Dropout(rate=drop_rate)\n\t    def call(self, inputs, training=None):\n", "        dense1_out = self.dense1(inputs)\n\t        dense1_out = self.dropout_layer(dense1_out, training=training)\n\t        dense2_out = self.dense2(dense1_out)\n\t        dense2_out = self.dropout_layer(dense2_out, training=training)\n\t        return dense2_out\n\tclass ConvPreNet(tf.keras.layers.Layer):\n\t    def __init__(self, nconv, hidden, conv_kernel, drop_rate,\n\t                 activation=tf.nn.relu, bn_before_act=True, name='ConvPrenet', **kwargs):\n\t        super(ConvPreNet, self).__init__(name=name, **kwargs)\n\t        self.conv_stack = []\n", "        for i in range(nconv):\n\t            conv = Conv1D(filters=hidden, kernel_size=conv_kernel, activation=activation,\n\t                          drop_rate=drop_rate, bn_before_act=bn_before_act,\n\t                          name='PreNetConv{}'.format(i))\n\t            self.conv_stack.append(conv)\n\t        self.projection = tf.keras.layers.Dense(units=hidden)\n\t    def call(self, inputs, training=None):\n\t        conv_outs = inputs\n\t        for conv in self.conv_stack:\n\t            conv_outs = conv(conv_outs, training=training)\n", "        projections = self.projection(conv_outs)\n\t        return projections\n\tclass FFN(tf.keras.layers.Layer):\n\t    def __init__(self, hidden1, hidden2, name='PositionalFeedForward', **kwargs):\n\t        super(FFN, self).__init__(name=name, **kwargs)\n\t        self.dense1 = tf.keras.layers.Dense(units=hidden1, activation='relu')\n\t        self.dense2 = tf.keras.layers.Dense(units=hidden2, activation=None)\n\t        self.layer_norm = tf.keras.layers.LayerNormalization()\n\t    def call(self, inputs, training=None):\n\t        dense1_outs = self.dense1(inputs)\n", "        dense2_outs = self.dense2(dense1_outs)\n\t        outs = dense2_outs + inputs\n\t        outs = self.layer_norm(outs, training=training)\n\t        return outs\n\tclass Conv1DLayerNorm(tf.keras.layers.Layer):\n\t    def __init__(self, filters, kernel_size, activation, drop_rate,\n\t                 padding='SAME', strides=1, name='Conv1D_with_dropout_IN'):\n\t        super(Conv1DLayerNorm, self).__init__(name=name)\n\t        self.filters = filters\n\t        self.kernel_size = kernel_size\n", "        self.drop_rate = drop_rate\n\t        self.padding = padding\n\t        self.conv1d = tf.keras.layers.Conv1D(filters=filters,\n\t                                             kernel_size=kernel_size,\n\t                                             strides=strides,\n\t                                             padding=padding,\n\t                                             activation=None,\n\t                                             name='conv1d')\n\t        self.activation = activation if activation is not None else tf.identity\n\t        self.layer_norm = tf.keras.layers.LayerNormalization(name='layerNorm')\n", "        self.dropout = tf.keras.layers.Dropout(rate=drop_rate, name='dropout')\n\t    def call(self, inputs, training=None):\n\t        conv_outs = self.conv1d(inputs)\n\t        conv_outs = self.layer_norm(conv_outs, training=training)\n\t        conv_outs = self.activation(conv_outs)\n\t        dropouts = self.dropout(conv_outs, training=training)\n\t        return dropouts\n\tclass Conv1D(tf.keras.layers.Layer):\n\t    def __init__(self, filters, kernel_size, activation, drop_rate,\n\t                 bn_before_act=False, padding='SAME', strides=1,\n", "                 name='Conv1D_with_dropout_BN', **kwargs):\n\t        super(Conv1D, self).__init__(name=name, **kwargs)\n\t        self.filters = filters\n\t        self.kernel_size = kernel_size\n\t        self.drop_rate = drop_rate\n\t        self.padding = padding\n\t        self.conv1d = tf.keras.layers.Conv1D(filters=filters,\n\t                                             kernel_size=kernel_size,\n\t                                             strides=strides,\n\t                                             padding=padding,\n", "                                             activation=None,\n\t                                             name='conv1d')\n\t        self.activation = activation if activation is not None else tf.identity\n\t        self.bn = tf.keras.layers.BatchNormalization(name='batch_norm')\n\t        self.dropout = tf.keras.layers.Dropout(rate=drop_rate, name='dropout')\n\t        self.bn_before_act = bn_before_act\n\t    def call(self, inputs, training=None):\n\t        conv_outs = self.conv1d(inputs)\n\t        if self.bn_before_act:\n\t            conv_outs = self.bn(conv_outs, training=training)\n", "            conv_outs = self.activation(conv_outs)\n\t        else:\n\t            conv_outs = self.activation(conv_outs)\n\t            conv_outs = self.bn(conv_outs, training=training)\n\t        dropouts = self.dropout(conv_outs, training=training)\n\t        return dropouts\n\t    def get_config(self):\n\t        config = super(Conv1D, self).get_config()\n\t        config.update({'filters': self.filters,\n\t                       'kernel_size': self.kernel_size,\n", "                       'padding': self.padding,\n\t                       'activation': self.activation,\n\t                       'dropout_rate': self.drop_rate,\n\t                       'bn_before_act': self.bn_before_act})\n\t        return config\n\tclass PostNet(tf.keras.layers.Layer):\n\t    def __init__(self, n_conv, conv_filters, conv_kernel,\n\t                 drop_rate, name='PostNet', **kwargs):\n\t        super(PostNet, self).__init__(name=name, **kwargs)\n\t        self.conv_stack = []\n", "        self.batch_norm_stack = []\n\t        activations = [tf.math.tanh] * (n_conv - 1) + [tf.identity]\n\t        for i in range(n_conv):\n\t            conv = Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n\t                          padding='same', activation=activations[i],\n\t                          drop_rate=drop_rate, name='conv_{}'.format(i))\n\t            self.conv_stack.append(conv)\n\t    def call(self, inputs, training=None):\n\t        conv_out = inputs\n\t        for conv in self.conv_stack:\n", "            conv_out = conv(conv_out, training)\n\t        return conv_out\n\tclass PositionalEncoding(tf.keras.layers.Layer):\n\t    def __init__(self, name='PositionalEncoding'):\n\t        super(PositionalEncoding, self).__init__(name=name)\n\t    @staticmethod\n\t    def positional_encoding(len, dim, step=1.):\n\t        \"\"\"\n\t        :param len: int scalar\n\t        :param dim: int scalar\n", "        :param step:\n\t        :return: position embedding\n\t        \"\"\"\n\t        pos_mat = tf.tile(\n\t            tf.expand_dims(\n\t                tf.range(0, tf.cast(len, dtype=tf.float32), dtype=tf.float32) * step,\n\t                axis=-1),\n\t            [1, dim])\n\t        dim_mat = tf.tile(\n\t            tf.expand_dims(\n", "                tf.range(0, tf.cast(dim, dtype=tf.float32), dtype=tf.float32),\n\t                axis=0),\n\t            [len, 1])\n\t        dim_mat_int = tf.cast(dim_mat, dtype=tf.int32)\n\t        pos_encoding = tf.where(  # [time, dims]\n\t            tf.math.equal(tf.math.mod(dim_mat_int, 2), 0),\n\t            x=tf.math.sin(pos_mat / tf.pow(10000., dim_mat / tf.cast(dim, tf.float32))),\n\t            y=tf.math.cos(pos_mat / tf.pow(10000., (dim_mat - 1) / tf.cast(dim, tf.float32))))\n\t        return pos_encoding\n\tclass EncBlk(tf.keras.layers.Layer):\n", "    \"\"\"\n\t    1D convolutional block for time-invariant feature extraction\n\t    \"\"\"\n\t    def __init__(self, out_channels, kernel_size, activation, pooling_kernel):\n\t        super(EncBlk, self).__init__()\n\t        self.conv1 = tf.keras.layers.Conv1D(out_channels, kernel_size=kernel_size, padding='same')\n\t        self.conv2 = tf.keras.layers.Conv1D(out_channels, kernel_size=kernel_size, padding='same')\n\t        self.conv_sc = tf.keras.layers.Conv1D(out_channels, kernel_size=1, padding='valid')\n\t        self.downsample = tf.keras.layers.AveragePooling1D(pool_size=pooling_kernel)\n\t        self.activation = activation\n", "    def call(self, x):\n\t        \"\"\"\n\t        :param x: [batch, channels, time-length]\n\t        :return:\n\t        \"\"\"\n\t        h = self.activation(x)\n\t        h = self.activation(self.conv1(h))\n\t        h = self.conv2(h)\n\t        h = self.downsample(h)\n\t        sc = self.downsample(self.conv_sc(x))\n", "        out = h + sc\n\t        return out\n\tdef get_activation(act_str):\n\t    return {'relu': tf.nn.relu, 'leaky_relu': tf.nn.leaky_relu, 'tanh': tf.math.tanh}[act_str]\n\t@tf.custom_gradient\n\tdef grad_reverse(x):\n\t    y = tf.identity(x)\n\t    def custom_grad(dy):\n\t        return -dy\n\t    return y, custom_grad\n", "class GradReverse(tf.keras.layers.Layer):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def call(self, x):\n\t        return grad_reverse(x)\n"]}
{"filename": "modules/attention.py", "chunked_list": ["import tensorflow as tf\n\tfrom typing import Tuple\n\tfrom .utils import FFN\n\tclass BaseAttention(tf.keras.layers.Layer):\n\t    def __init__(self, attention_dim, name='BaseAttention', **kwargs):\n\t        super(BaseAttention, self).__init__(name=name, **kwargs)\n\t        self.attention_dim = attention_dim\n\t    def call(self, inputs, memory, memory_lengths, query_lengths) -> Tuple[tf.Tensor, tf.Tensor]:\n\t        \"\"\"\n\t        :param inputs: query, [batch, q_time, q_dim]\n", "        :param memory: [batch, m_time, m_dim]\n\t        :param memory_lengths: [batch,]\n\t        :param query_lengths: [batch,]\n\t        :return: (tensor1, tensor2)\n\t            tensor1: contexts, [batch, q_time, attention_dim]\n\t            tensor2: alignments, probabilities, [batch, q_time, m_time]\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @staticmethod\n\t    def _get_key_mask(batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths):\n", "        memory_lengths = (memory_lengths if memory_lengths is not None\n\t                          else tf.ones([batch_size], dtype=tf.int32) * memory_max_time)\n\t        memeory_mask = tf.sequence_mask(memory_lengths, maxlen=memory_max_time)\n\t        memeory_mask = tf.tile(tf.expand_dims(memeory_mask, axis=1),  # [batch, 1, m_max_time]\n\t                               [1, query_max_time, 1])  # [batch, q_max_time, m_max_time]\n\t        query_lengths = (query_lengths if query_lengths is not None\n\t                         else tf.ones([batch_size], dtype=tf.int32) * query_max_time)\n\t        query_mask = tf.sequence_mask(query_lengths, maxlen=query_max_time)  # [batch, q_max_time]\n\t        query_mask = tf.tile(tf.expand_dims(query_mask, axis=2),  # [batch, q_max_time, 1]\n\t                             [1, 1, memory_max_time])  # [batch, q_max_time, m_max_time]\n", "        length_mask = tf.logical_and(memeory_mask, query_mask)\n\t        return length_mask\n\t    def get_config(self):\n\t        config = super(BaseAttention, self).get_config()\n\t        config.update({'attention_dim': self.attention_dim})\n\t        return config\n\tclass BahdanauAttention(BaseAttention):\n\t    def __init__(self, attention_dim, temperature=1.0, name='attention', **kwargs):\n\t        \"\"\"\n\t        :param attention_dim:\n", "        :param name:\n\t        :param kwargs:\n\t        \"\"\"\n\t        super(BahdanauAttention, self).__init__(attention_dim=attention_dim,\n\t                                                name=name, **kwargs)\n\t        self.query_layer = tf.keras.layers.Dense(\n\t            units=attention_dim, use_bias=False, name='query_layer')\n\t        self.memory_layer = tf.keras.layers.Dense(\n\t            units=attention_dim, use_bias=False, name='memory_layer')\n\t        self.score_v = tf.Variable(tf.random.normal([attention_dim, ]),\n", "                                   name='attention_v', trainable=True,\n\t                                   dtype=tf.float32)\n\t        self.score_b = tf.Variable(tf.zeros([attention_dim, ]),\n\t                                   name='attention_b', trainable=True,\n\t                                   dtype=tf.float32)\n\t        self.temperature = temperature\n\t    def _bahdanau_score(self, w_queries, w_keys):\n\t        \"\"\"\n\t        :param w_queries: [batch, q_max_time, attention_dim]\n\t        :param w_keys: [batch, k_max_time, attention_dim]\n", "        :return: [batch, q_max_time, k_max_time], attention score (energy)\n\t        \"\"\"\n\t        # [batch, q_max_time, 1, attention_dim]\n\t        expanded_queries = tf.expand_dims(w_queries, axis=2)\n\t        # [batch, 1, k_max_time, attention_dim]\n\t        expanded_keys = tf.expand_dims(w_keys, axis=1)\n\t        # [batch, q_max_time, k_max_time]\n\t        return tf.math.reduce_sum(\n\t            self.score_v * tf.nn.tanh(\n\t                # [batch, q_max_time, k_max_time, attention_dim]\n", "                expanded_keys + expanded_queries + self.score_b),\n\t            axis=3)\n\t    def call(self, inputs, memory, memory_lengths=None, query_lengths=None):\n\t        # TODO: add query lengths and query mask\n\t        \"\"\"\n\t        :param inputs: query: [batch, q_max_time, query_depth]\n\t        :param memory: [batch, m_max_time, memory_depth]\n\t        :param memory_lengths: [batch, ]\n\t        :param query_lengths: [batch, ]\n\t        :return: contexts: [batch, q_max_time, attention_dim],\n", "                 alignments: [batch, q_max_time, m_max_time]\n\t        \"\"\"\n\t        processed_query = self.query_layer(inputs)\n\t        values = self.memory_layer(memory)\n\t        # energy: [batch, q_max_time, m_max_time]\n\t        energy = self._bahdanau_score(processed_query, values) / self.temperature\n\t        # apply mask\n\t        batch_size = tf.shape(memory)[0]\n\t        memory_max_time = tf.shape(memory)[1]\n\t        query_max_time = tf.shape(inputs)[1]\n", "        length_mask = self._get_key_mask(\n\t            batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths)\n\t        # [batch, q_max_time, m_max_time]\n\t        paddings = tf.ones_like(energy) * (-2. ** 32 + 1)\n\t        energy = tf.where(length_mask, energy, paddings)\n\t        # alignments shape = energy shape = [batch, q_max_time, m_max_time]\n\t        alignments = tf.math.softmax(energy, axis=2)\n\t        # compute context vector\n\t        # context: [batch, q_max_time, attention_dim]\n\t        contexts = tf.linalg.matmul(alignments, values)\n", "        return contexts, alignments\n\tclass ScaledDotProductAttention(BaseAttention):\n\t    def __init__(self, attention_dim, value_dim, temperature=1.0,\n\t                 name='ScaledDotProductAttention', **kwargs):\n\t        super(ScaledDotProductAttention, self).__init__(\n\t            attention_dim=attention_dim, name=name, **kwargs)\n\t        self.query_layer = tf.keras.layers.Dense(\n\t            units=attention_dim, use_bias=False, name='query_layer')\n\t        self.key_layer = tf.keras.layers.Dense(\n\t            units=attention_dim, use_bias=False, name='key_layer')\n", "        self.value_layer = tf.keras.layers.Dense(\n\t            units=value_dim, use_bias=False, name='value_layer')\n\t        self.temperature = temperature\n\t    def call(self, inputs, memory, memory_lengths=None, query_lengths=None):\n\t        queries = self.query_layer(inputs)  # [batch, Tq, D]\n\t        keys = self.key_layer(memory)  # [batch, Tk, D]\n\t        values = self.key_layer(memory)  # [batch, Tk, Dv]\n\t        logits = tf.linalg.matmul(queries, keys, transpose_b=True)  # [batch, Tq, Tk]\n\t        logits = logits / tf.math.sqrt(tf.cast(self.attention_dim, tf.float32))  # scale\n\t        logits = logits / self.temperature  # temperature\n", "        # apply mask\n\t        batch_size = tf.shape(memory)[0]\n\t        memory_max_time = tf.shape(memory)[1]\n\t        query_max_time = tf.shape(inputs)[1]\n\t        length_mask = self._get_key_mask(\n\t            batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths)\n\t        paddings = tf.ones_like(logits) * (-2. ** 32 + 1)\n\t        logits = tf.where(length_mask, logits, paddings)\n\t        alignments = tf.math.softmax(logits, axis=2)\n\t        contexts = tf.linalg.matmul(alignments, values)\n", "        return contexts, alignments\n\tclass MultiHeadScaledProductAttention(BaseAttention):\n\t    def __init__(self, attention_dim, num_head, temperature=1.0, causality=None,\n\t                 attention_window=-1, name='attention', **kwargs):\n\t        assert attention_dim % num_head == 0\n\t        super(MultiHeadScaledProductAttention, self).__init__(\n\t            attention_dim=attention_dim, name=name, **kwargs)\n\t        self.causality = causality\n\t        self.attention_window = attention_window\n\t        self.query_layer = tf.keras.layers.Dense(\n", "            units=attention_dim, use_bias=False, name='query_layer')\n\t        self.key_layer = tf.keras.layers.Dense(\n\t            units=attention_dim, use_bias=False, name='key_layer')\n\t        self.value_layer = tf.keras.layers.Dense(\n\t            units=attention_dim, use_bias=False, name='value_layer')\n\t        self.num_head = num_head\n\t        self.temperature = temperature\n\t    def _split_head(self, inputs):\n\t        \"\"\"\n\t        :param inputs: [batch, time, dim]\n", "        :return: [batch, num_head, time, dim // head]\n\t        \"\"\"\n\t        batch = tf.shape(inputs)[0]\n\t        max_time = tf.shape(inputs)[1]\n\t        dim = tf.shape(inputs)[2]\n\t        reshaped = tf.reshape(inputs,\n\t                              [batch, max_time, self.num_head,\n\t                               dim // self.num_head])\n\t        # [batch, time, num_head, dim // head]\n\t        transposed = tf.transpose(reshaped, [0, 2, 1, 3])\n", "        # [batch, num_head, time, dim // head]\n\t        return transposed\n\t    def _merge_head(self, inputs):\n\t        \"\"\"\n\t        :param inputs: [batch, num_head, time, dim]\n\t        :return: [batch, time, attention_dim]\n\t        \"\"\"\n\t        batch = tf.shape(inputs)[0]\n\t        time = tf.shape(inputs)[2]\n\t        head_dim = tf.shape(inputs)[3]\n", "        transposed = tf.transpose(inputs, [0, 2, 1, 3])\n\t        # [batch, time, num_head, dim]\n\t        reshaped = tf.reshape(transposed, [batch, time, self.num_head * head_dim])\n\t        return reshaped\n\t    def _get_key_mask(self, batch_size, memory_max_time, query_max_time,\n\t                      memory_lengths, query_lengths):\n\t        memory_lengths = (memory_lengths if memory_lengths is not None\n\t                          else tf.ones([batch_size], dtype=tf.int32) * memory_max_time)\n\t        memory_mask = tf.sequence_mask(memory_lengths, maxlen=memory_max_time,\n\t                                       name='length_mask')  # [batch, m_max_time]\n", "        memory_mask = tf.tile(tf.expand_dims(memory_mask, axis=1),  # [batch, 1, m_max_time]\n\t                              [1, query_max_time, 1])  # [batch, q_max_time, m_max_time]\n\t        query_lengths = (query_lengths if query_lengths is not None\n\t                         else tf.ones([batch_size], dtype=tf.int32) * query_max_time)\n\t        query_mask = tf.sequence_mask(query_lengths, maxlen=query_max_time)  # [batch, q_max_time]\n\t        query_mask = tf.tile(tf.expand_dims(query_mask, axis=2),  # [batch, q_max_time, 1]\n\t                             [1, 1, memory_max_time])  # [batch, q_max_time, m_max_time]\n\t        length_mask = tf.logical_and(memory_mask, query_mask)\n\t        length_mask = tf.tile(tf.expand_dims(length_mask, axis=1),\n\t                              [1, self.num_head, 1, 1])\n", "        # [batch, num_head, q_max_time, m_max_time]\n\t        return length_mask\n\t    def get_window_mask(self, logits):\n\t        win_mask = tf.ones(tf.shape(logits), dtype=tf.bool)\n\t        if self.causality:\n\t            win_mask = tf.linalg.band_part(win_mask, self.attention_window, 0, name='causal_mask')\n\t        else:\n\t            win_mask = tf.linalg.band_part(\n\t                win_mask, self.attention_window, self.attention_window, name='causal_mask')\n\t        return win_mask\n", "    def call(self, inputs, memory, memory_lengths=None, query_lengths=None):\n\t        queries = self.query_layer(inputs)  # [batch, Tq, D]\n\t        keys = self.key_layer(memory)  # [batch, Tk, D]\n\t        values = self.value_layer(memory)  # [batch, Tk, Dv]\n\t        headed_queries = self._split_head(queries)  # [batch, num_head, Tq, head_dim]\n\t        headed_keys = self._split_head(keys)  # [batch, num_head, Tk, head_dim]\n\t        headed_values = self._split_head(values)  # [batch, num_head, Tk, head_dim]\n\t        logits = tf.linalg.matmul(headed_queries,\n\t                                  headed_keys,\n\t                                  transpose_b=True)  # [batch, num_head, Tq, Tk]\n", "        logits = logits / tf.sqrt(\n\t            tf.cast(self.attention_dim // self.num_head, dtype=tf.float32))  # scale\n\t        logits = logits / self.temperature  # temperature\n\t        # apply mask\n\t        batch_size = tf.shape(memory)[0]\n\t        memory_max_time = tf.shape(memory)[1]\n\t        query_max_time = tf.shape(inputs)[1]\n\t        length_mask = self._get_key_mask(\n\t            batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths)\n\t        window_mask = self.get_window_mask(logits)\n", "        length_mask = tf.math.logical_and(length_mask, window_mask)\n\t        # [batch, num_head, q_max_time, m_max_time]\n\t        paddings = tf.ones_like(logits, dtype=tf.float32) * (-2. ** 32 + 1)\n\t        logits = tf.where(length_mask, logits, paddings)\n\t        alignments = tf.math.softmax(logits, axis=3)  # [batch, num_head, Tq, Tk]\n\t        contexts = tf.linalg.matmul(alignments, headed_values)\n\t        # [batch, num_head, Tq, head_dim]\n\t        contexts = self._merge_head(contexts)  # [batch, Tq, attention_dim]\n\t        return contexts, alignments\n\tclass SelfAttentionBLK(tf.keras.layers.Layer):\n", "    def __init__(self, input_dim, attention_dim, attention_heads, attention_temperature, ffn_hidden,\n\t                 causality=None, attention_window=-1, name='self_attention_blk', **kwargs):\n\t        super(SelfAttentionBLK, self).__init__(name=name, **kwargs)\n\t        self.input_dim = input_dim\n\t        self.attention_dim = attention_dim\n\t        self.attention = MultiHeadScaledProductAttention(attention_dim=attention_dim,\n\t                                                         num_head=attention_heads,\n\t                                                         temperature=attention_temperature,\n\t                                                         causality=causality,\n\t                                                         attention_window=attention_window)\n", "        self.att_proj = tf.keras.layers.Dense(units=input_dim)\n\t        self.layer_norm = tf.keras.layers.LayerNormalization()\n\t        self.ffn = FFN(hidden1=ffn_hidden, hidden2=input_dim)\n\t    def call(self, inputs, memory, query_lengths, memory_lengths, training=None):\n\t        att_outs, alignments = self.attention(inputs=inputs, memory=memory,\n\t                                              query_lengths=query_lengths,\n\t                                              memory_lengths=memory_lengths)\n\t        contexts = tf.concat([inputs, att_outs], axis=-1)\n\t        contexts.set_shape([None, None, self.attention_dim + self.input_dim])\n\t        att_outs = self.att_proj(contexts)\n", "        att_outs = self.layer_norm(inputs + att_outs, training=training)\n\t        ffn_outs = self.ffn(att_outs, training=training)\n\t        return ffn_outs, alignments\n\tclass CrossAttentionBLK(tf.keras.layers.Layer):\n\t    def __init__(self, input_dim, attention_dim, attention_heads, attention_temperature,\n\t                 ffn_hidden, causality=None, attention_window=-1, name='cross_attention_blk', **kwargs):\n\t        super(CrossAttentionBLK, self).__init__(name=name, **kwargs)\n\t        self.input_dim = input_dim\n\t        self.attention_dim = attention_dim\n\t        self.self_attention = MultiHeadScaledProductAttention(\n", "            attention_dim=attention_dim, num_head=attention_heads,\n\t            temperature=attention_temperature, name='self_attention',\n\t            causality=causality, attention_window=attention_window)\n\t        self.att_proj1 = tf.keras.layers.Dense(units=input_dim)\n\t        self.layer_norm1 = tf.keras.layers.LayerNormalization(name='layerNorm1')\n\t        self.cross_attention = MultiHeadScaledProductAttention(\n\t            attention_dim=attention_dim, num_head=attention_heads,\n\t            temperature=attention_temperature, causality=False,\n\t            attention_window=-1, name='cross_attention')\n\t        self.att_proj2 = tf.keras.layers.Dense(units=attention_dim)\n", "        self.layer_norm2 = tf.keras.layers.LayerNormalization(name='layerNorm2')\n\t        self.ffn = FFN(hidden1=ffn_hidden, hidden2=attention_dim)\n\t    def call(self, inputs, memory, query_lengths, memory_lengths, training=None):\n\t        self_att_outs, self_ali = self.self_attention(\n\t            inputs=inputs, memory=inputs, query_lengths=query_lengths,\n\t            memory_lengths=query_lengths)\n\t        contexts = tf.concat([inputs, self_att_outs], axis=-1)\n\t        contexts.set_shape([None, None, self.attention_dim + self.input_dim])\n\t        self_att_outs = self.att_proj1(contexts)\n\t        self_att_outs = self.layer_norm1(self_att_outs + inputs, training=training)\n", "        att_outs, cross_ali = self.cross_attention(\n\t            inputs=self_att_outs, memory=memory, query_lengths=query_lengths,\n\t            memory_lengths=memory_lengths)\n\t        contexts = tf.concat([self_att_outs, att_outs], axis=-1)\n\t        contexts.set_shape([None, None, self.attention_dim * 2])\n\t        att_outs = self.att_proj2(contexts)\n\t        att_outs = self.layer_norm2(att_outs + self_att_outs, training=training)\n\t        ffn_outs = self.ffn(att_outs, training=training)\n\t        return ffn_outs, cross_ali\n"]}
{"filename": "datasets/VCTK.py", "chunked_list": ["import os\n\timport json\n\timport random\n\tclass VCTK:\n\t    \"\"\"\n\t    Generate the json file obtained by dumping the dictionary\n\t    {'train': {fid: wav_path, ...}, 'validation': {fid: wav_path, ...}, 'test': {fid: wav_path, ...}}\n\t    \"\"\"\n\t    def __init__(self, data_dir, out_dir, val_spks=None, test_spks=None):\n\t        self.data_dir = data_dir\n", "        self.out_dir = out_dir\n\t        self.summary_file = os.path.join(out_dir, 'vctk-summary.json')\n\t        self.wav_ext = 'mic2.flac'\n\t        self.dataset_summary = {}\n\t        assert ((val_spks is None and test_spks is None) or\n\t                (val_spks is not None and test_spks is not None),\n\t                \"Please specify both val and test speakers!\")\n\t        self.val_spks = val_spks\n\t        self.test_spks = test_spks\n\t        self.validate_dir()\n", "    def validate_dir(self):\n\t        if not os.path.isdir(self.data_dir):\n\t            raise NotADirectoryError('{} is not a valid directory!'.format(self.data_dir))\n\t        if not os.path.isdir(self.out_dir):\n\t            os.makedirs(self.out_dir)\n\t        return\n\t    @staticmethod\n\t    def extract_spk_fid(filename):\n\t        \"\"\"\n\t        :param filename:\n", "        :return: spk_name, fid\n\t        \"\"\"\n\t        basename = filename.split('/')[-1].split('.')[0]\n\t        spk = basename.split('_')[0]\n\t        uid = basename.split('_')[1]\n\t        fid = '{}_{}'.format(spk, uid)\n\t        return spk, fid\n\t    def write_dataset_info(self):\n\t        with open(self.summary_file, 'w') as f:\n\t            json.dump(self.dataset_summary, f, sort_keys=True, indent=4)\n", "        return\n\t    def write_summary(self):\n\t        dataset_summary = {}\n\t        for root, dirs, files in os.walk(self.data_dir, followlinks=True):\n\t            for basename in files:\n\t                if basename.endswith(self.wav_ext):\n\t                    filename = os.path.join(root, basename)\n\t                    spk, fid = self.extract_spk_fid(filename)\n\t                    wav_path = os.path.join(root, basename)\n\t                    if spk not in dataset_summary.keys():\n", "                        dataset_summary[spk] = {}\n\t                        dataset_summary[spk][fid] = wav_path\n\t                    else:\n\t                        dataset_summary[spk][fid] = wav_path\n\t        if self.val_spks is None and self.test_spks is None:\n\t            all_spks = list(dataset_summary.keys())\n\t            random.shuffle(all_spks)\n\t            self.test_spks = all_spks[-10:]\n\t            self.val_spks = all_spks[-20: -10]\n\t        train_summary = {}\n", "        val_summary = {}\n\t        test_summary = {}\n\t        for spk in dataset_summary.keys():\n\t            if spk in self.val_spks:\n\t                val_summary.update(dataset_summary[spk])\n\t            elif spk in self.test_spks:\n\t                test_summary.update(dataset_summary[spk])\n\t            else:\n\t                train_summary.update(dataset_summary[spk])\n\t        self.dataset_summary = {'train': train_summary,\n", "                                'validation': val_summary,\n\t                                'test': test_summary}\n\t        self.write_dataset_info()\n\t        return\n"]}
{"filename": "datasets/AiShell3.py", "chunked_list": ["import os\n\timport json\n\timport random\n\tclass AiShell3:\n\t    \"\"\"\n\t    Generate the json file obtained by dumping the dictionary\n\t    {'train': {fid: wav_path, ...}, 'validation': {fid: wav_path, ...}, 'test': {fid: wav_path, ...}}\n\t    \"\"\"\n\t    def __init__(self, data_dir, out_dir, train_spk_file=None, val_test_spk_file=None):\n\t        self.data_dir = data_dir\n", "        self.out_dir = out_dir\n\t        self.train_spks = self.read_train_spks(train_spk_file)\n\t        val_test_spks = self.read_val_test_spks(val_test_spk_file)\n\t        random.shuffle(val_test_spks)\n\t        self.val_spks = val_test_spks[:len(val_test_spks) // 2]\n\t        self.test_spks = val_test_spks[len(val_test_spks) // 2:]\n\t        self.summary_file = os.path.join(out_dir, 'aishell3-summary.json')\n\t        self.wav_ext = '.wav'\n\t        self.dataset_summary = {}\n\t        self.validate_dir()\n", "    def read_train_spks(self, f):\n\t        train_spks = []\n\t        if f is not None:\n\t            with open(f, 'r', encoding='utf-8') as f:\n\t                for line in f:\n\t                    train_spks.append(line.strip())\n\t        else:\n\t            train_spks = os.listdir(os.path.join(self.data_dir, 'train/wav'))\n\t        assert len(train_spks) > 0\n\t        return train_spks\n", "    def read_val_test_spks(self, f):\n\t        vt_spks = []\n\t        if f is not None:\n\t            with open(f, 'r', encoding='utf-8') as f:\n\t                for line in f:\n\t                    vt_spks.append(line.strip())\n\t        else:\n\t            vt_spks = [spk for spk in os.listdir(os.path.join(self.data_dir, 'test/wav'))\n\t                       if spk not in self.train_spks]\n\t        assert len(vt_spks) > 0\n", "        return vt_spks\n\t    def validate_dir(self):\n\t        if not os.path.isdir(self.data_dir):\n\t            raise NotADirectoryError('{} is not a valid directory!'.format(self.data_dir))\n\t        if not os.path.isdir(self.out_dir):\n\t            os.makedirs(self.out_dir)\n\t        return\n\t    @staticmethod\n\t    def extract_spk_fid(filename):\n\t        \"\"\"\n", "        :param filename:\n\t        :return: spk_name, fid\n\t        \"\"\"\n\t        fid = filename.split('/')[-1].split('.')[0]\n\t        spk = filename.split('/')[-2]\n\t        return spk, fid\n\t    def write_dataset_info(self):\n\t        with open(self.summary_file, 'w') as f:\n\t            json.dump(self.dataset_summary, f, sort_keys=True, indent=4)\n\t        return\n", "    def write_summary(self):\n\t        train_summary = {}\n\t        val_summary = {}\n\t        test_summary = {}\n\t        for root, dirs, files in os.walk(self.data_dir, followlinks=True):\n\t            for basename in files:\n\t                if basename.endswith(self.wav_ext):\n\t                    wav_path = os.path.join(root, basename)\n\t                    spk, fid = self.extract_spk_fid(wav_path)\n\t                    if spk in self.train_spks:\n", "                        train_summary[fid] = wav_path\n\t                    elif spk in self.val_spks:\n\t                        val_summary[fid] = wav_path\n\t                    elif spk in self.test_spks:\n\t                        test_summary[fid] = wav_path\n\t                    else:\n\t                        continue\n\t        self.dataset_summary = {'train': train_summary,\n\t                                'validation': val_summary,\n\t                                'test': test_summary}\n", "        self.write_dataset_info()\n\t        return\n"]}
{"filename": "datasets/tfrecord_maker.py", "chunked_list": ["import os\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tqdm import tqdm\n\tclass TFRecordWriter:\n\t    def __init__(self, train_split=None, data_dir=None, save_dir=None, chunk_size=None):\n\t        self.train_split = train_split\n\t        self.data_dir = data_dir\n\t        self.save_dir = save_dir\n\t        self.chunk_size = chunk_size\n", "        self.train_ids_file = os.path.join(self.data_dir, 'train.txt') if data_dir is not None else None\n\t        self.val_ids_file = os.path.join(self.data_dir, 'val.txt') if data_dir is not None else None\n\t        self.test_ids_file = os.path.join(self.data_dir, 'test.txt') if data_dir is not None else None\n\t    @staticmethod\n\t    def _bytes_feature(value):\n\t        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n\t        if isinstance(value, type(tf.constant(0))):\n\t            value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n\t        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\t    @staticmethod\n", "    def _float_feature(value):\n\t        \"\"\"Returns a float_list from a float / double.\"\"\"\n\t        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\t    @staticmethod\n\t    def _int64_feature(value):\n\t        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n\t        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\t    @staticmethod\n\t    def serialize_example(fid, mel, mel_len):\n\t        \"\"\"\n", "        :param fid: string\n\t        :param mel: np array, [mel_len, num_mels]\n\t        :param mel_len: int32\n\t        :return: byte string\n\t        \"\"\"\n\t        feature = {\n\t            'fid': TFRecordWriter._bytes_feature(fid.encode('utf-8')),\n\t            'mel': TFRecordWriter._bytes_feature(tf.io.serialize_tensor(mel)),\n\t            'mel_len': TFRecordWriter._int64_feature(mel_len)}\n\t        # Create a Features message using tf.train.Example.\n", "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n\t        return example_proto.SerializeToString()\n\t    def _parse_fids(self, mode='train'):\n\t        fids_f = {'train': self.train_ids_file,\n\t                  'val': self.val_ids_file,\n\t                  'test': self.test_ids_file}[mode]\n\t        fids = []\n\t        with open(fids_f, 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                fids.append(line.strip())\n", "        return fids\n\t    def _get_features(self, fid):\n\t        mel = np.load(os.path.join(self.data_dir, 'mels', '{}.npy'.format(fid))).astype(np.float64)\n\t        mel_len = mel.shape[0]\n\t        return mel, mel_len\n\t    def mel_exist(self, fid):\n\t        mel_npy = os.path.join(self.data_dir, 'mels', '{}.npy'.format(fid))\n\t        return os.path.isfile(mel_npy)\n\t    def write(self, mode='train'):\n\t        fids = self._parse_fids(mode)\n", "        if mode == 'train':\n\t            splited_fids = [fids[i::self.train_split] for i in range(self.train_split)]\n\t        else:\n\t            splited_fids = [fids]\n\t        for i, ids in enumerate(splited_fids):\n\t            tfrecord_path = os.path.join(self.save_dir, '{}-{}.tfrecords'.format(mode, i))\n\t            with tf.io.TFRecordWriter(tfrecord_path) as writer:\n\t                for fid in tqdm(ids):\n\t                    if not self.mel_exist(fid):\n\t                        continue\n", "                    mel, mel_len = self._get_features(fid)\n\t                    serialized_example = self.serialize_example(fid, mel, mel_len)\n\t                    writer.write(serialized_example)\n\t        return\n\t    def write_all(self):\n\t        self.write('train')\n\t        self.write('val')\n\t        self.write('test')\n\t        return\n\t    def pad2chunk(self, inputs):\n", "        inp_exp = tf.tile(inputs, tf.constant([14, 1]))\n\t        inp_exp = inp_exp[:self.chunk_size, :]\n\t        return inp_exp\n\t    def parse_example(self, serialized_example):\n\t        feature_description = {\n\t            'fid': tf.io.FixedLenFeature((), tf.string),\n\t            'mel': tf.io.FixedLenFeature((), tf.string),\n\t            'mel_len': tf.io.FixedLenFeature((), tf.int64)}\n\t        example = tf.io.parse_single_example(serialized_example, feature_description)\n\t        fid = example['fid']\n", "        mel = tf.io.parse_tensor(example['mel'], out_type=tf.float64)\n\t        mel_ext = self.pad2chunk(mel)\n\t        mel_len = example['mel_len']\n\t        return (fid,\n\t                tf.cast(mel, tf.float32),\n\t                tf.cast(mel_len, tf.int32),\n\t                tf.cast(mel_ext, tf.float32))\n\t    def create_dataset(self, buffer_size, num_parallel_reads,\n\t                       batch_size, num_mels, shuffle_buffer, shuffle,\n\t                       tfrecord_files, seed=1, drop_remainder=False):\n", "        tfrecord_dataset = tf.data.TFRecordDataset(\n\t            tfrecord_files, buffer_size=buffer_size,\n\t            num_parallel_reads=num_parallel_reads)\n\t        tfdataset = tfrecord_dataset.map(\n\t            self.parse_example,\n\t            num_parallel_calls=num_parallel_reads)\n\t        tfdataset = tfdataset.padded_batch(\n\t            batch_size=batch_size,\n\t            drop_remainder=drop_remainder,\n\t            padded_shapes=([], [None, num_mels], [], [None, num_mels]))\n", "        tfdataset = (tfdataset.shuffle(buffer_size=shuffle_buffer, seed=seed)\n\t                     if shuffle else tfdataset)\n\t        tfdataset = tfdataset.prefetch(tf.data.experimental.AUTOTUNE)\n\t        return tfdataset\n\t    def get_tfrecords_list(self, mode='train'):\n\t        assert self.save_dir is not None\n\t        assert mode in ['train', 'val', 'test']\n\t        return [os.path.join(self.save_dir, f)\n\t                for f in os.listdir(self.save_dir) if f.startswith(mode)]\n"]}
{"filename": "datasets/preprocessor.py", "chunked_list": ["import os\n\timport json\n\timport joblib\n\timport random\n\timport numpy as np\n\tfrom audio import Audio\n\tclass MelPreprocessor:\n\t    \"\"\"\n\t    Extract mel-spectrograms given the multiple data summary json file\n\t    \"\"\"\n", "    def __init__(self, data_summary_paths, save_dir, hps):\n\t        \"\"\"\n\t        :param data_summary_paths: list of data summary json files containing paths of the waveform\n\t        :param save_dir: directory to save the feature\n\t        :param hps: hyper-parameters\n\t        \"\"\"\n\t        self.save_dir = save_dir\n\t        self.data_summary_paths = data_summary_paths\n\t        self.data_summary = self.load_dataset_info()\n\t        self.hps = hps\n", "        self.mel_dir = os.path.join(self.save_dir, 'mels')\n\t        self.train_list_f = os.path.join(self.save_dir, 'train.txt')\n\t        self.val_list_f = os.path.join(self.save_dir, 'val.txt')\n\t        self.test_list_f = os.path.join(self.save_dir, 'test.txt')\n\t        self.num_mels = hps.Audio.num_mels\n\t        self.audio_processor = Audio(hps.Audio)\n\t        self.n_jobs = hps.Dataset.preprocess_n_jobs\n\t        self.train_set_size = None\n\t        self.dev_set_size = None\n\t        self.test_set_size = None\n", "    def feature_extraction(self):\n\t        self._validate_dir()\n\t        print('Process text file...')\n\t        print('Split the data set into train, dev and test set...')\n\t        self.train_set_size, self.dev_set_size, self.test_set_size = self.write_splits()\n\t        print('Extracting Mel-Spectrograms...')\n\t        self.extract_mels()\n\t        return\n\t    def load_dataset_info(self):\n\t        train_summary = {}\n", "        val_summary = {}\n\t        test_summary = {}\n\t        for summary_f in self.data_summary_paths:\n\t            if not os.path.isfile(summary_f):\n\t                raise FileNotFoundError(\n\t                    '{} not exists! Please generate it first!'.format(summary_f))\n\t            with open(summary_f, 'r') as f:\n\t                summary = json.load(f)\n\t                train_summary.update(summary['train'])\n\t                val_summary.update(summary['validation'])\n", "                test_summary.update(summary['test'])\n\t        dataset_summary = {'train': train_summary, 'validation': val_summary, 'test': test_summary}\n\t        return dataset_summary\n\t    def _validate_dir(self):\n\t        if not os.path.isdir(self.save_dir):\n\t            os.makedirs(self.save_dir)\n\t        if not os.path.isdir(self.mel_dir):\n\t            os.makedirs(self.mel_dir)\n\t        return\n\t    def write_splits(self):\n", "        train_set = [fid for fid in self.data_summary['train'].keys()]\n\t        val_set = [fid for fid in self.data_summary['validation'].keys()]\n\t        test_set = [fid for fid in self.data_summary['test'].keys()]\n\t        random.shuffle(train_set)\n\t        random.shuffle(val_set)\n\t        random.shuffle(test_set)\n\t        with open(self.train_list_f, 'w') as f:\n\t            for idx in train_set:\n\t                f.write(\"{}\\n\".format(idx))\n\t        with open(self.val_list_f, 'w') as f:\n", "            for idx in val_set:\n\t                f.write(\"{}\\n\".format(idx))\n\t        with open(self.test_list_f, 'w') as f:\n\t            for idx in test_set:\n\t                f.write(\"{}\\n\".format(idx))\n\t        return len(train_set), len(val_set), len(test_set)\n\t    def single_mel_lf0_extraction(self, wav_f, fid):\n\t        mel_name = os.path.join(self.mel_dir, '{}.npy'.format(fid))\n\t        if os.path.isfile(mel_name):\n\t            return\n", "        else:\n\t            wav_arr = self.audio_processor.load_wav(wav_f)\n\t            wav_arr = self.audio_processor.trim_silence_by_trial(wav_arr, top_db=15., lower_db=25.)\n\t            wav_arr = wav_arr / max(0.01, np.max(np.abs(wav_arr)))\n\t            wav_arr = self.audio_processor.preemphasize(wav_arr)\n\t            mel = self.audio_processor.melspectrogram(wav_arr)\n\t            np.save(mel_name, mel.T)\n\t            return\n\t    def extract_mels(self):\n\t        wav_list = []\n", "        for split in self.data_summary.keys():\n\t            for fid in self.data_summary[split].keys():\n\t                wav_list.append((self.data_summary[split][fid], fid))\n\t        jobs = [joblib.delayed(self.single_mel_lf0_extraction)(wav_f, fid)\n\t                for wav_f, fid in wav_list]\n\t        _ = joblib.Parallel(n_jobs=self.n_jobs, verbose=True)(jobs)\n\t        return\n"]}
{"filename": "datasets/__init__.py", "chunked_list": ["from .VCTK import VCTK\n\tfrom .AiShell3 import AiShell3\n\tfrom .preprocessor import MelPreprocessor\n\tfrom .tfrecord_maker import TFRecordWriter\n"]}
