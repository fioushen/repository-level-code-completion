{"filename": "e2e_tests/src/e2e_common/indexer_db.py", "chunked_list": ["import psycopg2\n\tclass IndexerDB:\n\t    def __init__(self, host, port, user, password, dbname):\n\t        self.host = host\n\t        self.port = port\n\t        self.user = user\n\t        self.password = password\n\t        self.dbname = dbname\n\t    @classmethod\n\t    def from_connection_string(cls, connection_string):\n", "        init_args = {\n\t            keyval.split(\"=\")[0]: keyval.split(\"=\")[1]\n\t            for keyval in connection_string.split()\n\t        }\n\t        return cls(\n\t            host=init_args[\"host\"],\n\t            port=init_args[\"port\"],\n\t            user=init_args[\"user\"],\n\t            password=init_args[\"password\"],\n\t            dbname=init_args[\"dbname\"],\n", "        )\n\t    def select_one(self, query) -> tuple:\n\t        with psycopg2.connect(\n\t            host=self.host,\n\t            port=self.port,\n\t            user=self.user,\n\t            password=self.password,\n\t            dbname=self.dbname,\n\t        ) as connection:\n\t            with connection.cursor() as cursor:\n", "                cursor.execute(query)\n\t                return cursor.fetchone()  # type: ignore\n\t    def get_txn_min_max_round(self):\n\t        min_round, max_round = self.select_one(\"SELECT min(round), max(round) FROM txn\")\n\t        return min_round, max_round\n\t    def get_table_row_count(self, table_name):\n\t        return self.select_one(f\"SELECT count(*) FROM {table_name}\")[0]\n\t    def get_block_header_final_round(self):\n\t        return self.select_one(\"SELECT max(round) FROM block_header\")[0]\n"]}
{"filename": "e2e_tests/src/e2e_common/__init__.py", "chunked_list": []}
{"filename": "e2e_tests/src/e2e_common/util.py", "chunked_list": ["#!/usr/bin/env python3\n\timport atexit\n\timport logging\n\timport os\n\timport random\n\timport sqlite3\n\timport subprocess\n\timport sys\n\timport time\n\timport msgpack\n", "logger = logging.getLogger(__name__)\n\tdef maybedecode(x):\n\t    if hasattr(x, \"decode\"):\n\t        return x.decode()\n\t    return x\n\tdef mloads(x):\n\t    return msgpack.loads(x, strict_map_key=False, raw=True)\n\tdef unmsgpack(ob):\n\t    \"convert dict from msgpack.loads() with byte string keys to text string keys\"\n\t    if isinstance(ob, dict):\n", "        od = {}\n\t        for k, v in ob.items():\n\t            k = maybedecode(k)\n\t            okv = False\n\t            if (not okv) and (k == \"note\"):\n\t                try:\n\t                    v = unmsgpack(mloads(v))\n\t                    okv = True\n\t                except:\n\t                    pass\n", "            if (not okv) and k in (\"type\", \"note\"):\n\t                try:\n\t                    v = v.decode()\n\t                    okv = True\n\t                except:\n\t                    pass\n\t            if not okv:\n\t                v = unmsgpack(v)\n\t            od[k] = v\n\t        return od\n", "    if isinstance(ob, list):\n\t        return [unmsgpack(v) for v in ob]\n\t    # if isinstance(ob, bytes):\n\t    #    return base64.b64encode(ob).decode()\n\t    return ob\n\tdef _getio(p, od, ed):\n\t    if od is not None:\n\t        od = maybedecode(od)\n\t    elif p.stdout:\n\t        try:\n", "            od = maybedecode(p.stdout.read())\n\t        except:\n\t            logger.error(\"subcomand out\", exc_info=True)\n\t    if ed is not None:\n\t        ed = maybedecode(ed)\n\t    elif p.stderr:\n\t        try:\n\t            ed = maybedecode(p.stderr.read())\n\t        except:\n\t            logger.error(\"subcomand err\", exc_info=True)\n", "    return od, ed\n\tdef xrun(cmd, *args, **kwargs):\n\t    timeout = kwargs.pop(\"timeout\", None)\n\t    kwargs[\"stdout\"] = subprocess.PIPE\n\t    kwargs[\"stderr\"] = subprocess.STDOUT\n\t    cmdr = \" \".join(map(repr, cmd))\n\t    try:\n\t        p = subprocess.Popen(cmd, *args, **kwargs)\n\t    except Exception as e:\n\t        logger.error(\"subprocess failed {}\".format(cmdr), exc_info=True)\n", "        raise\n\t    stdout_data, stderr_data = None, None\n\t    try:\n\t        if timeout:\n\t            stdout_data, stderr_data = p.communicate(timeout=timeout)\n\t        else:\n\t            stdout_data, stderr_data = p.communicate()\n\t    except subprocess.TimeoutExpired as te:\n\t        logger.error(\"subprocess timed out {}\".format(cmdr), exc_info=True)\n\t        stdout_data, stderr_data = _getio(p, stdout_data, stderr_data)\n", "        if stdout_data:\n\t            sys.stderr.write(\"output from {}:\\n{}\\n\\n\".format(cmdr, stdout_data))\n\t        if stderr_data:\n\t            sys.stderr.write(\"stderr from {}:\\n{}\\n\\n\".format(cmdr, stderr_data))\n\t        raise\n\t    except Exception as e:\n\t        logger.error(\"subprocess exception {}\".format(cmdr), exc_info=True)\n\t        stdout_data, stderr_data = _getio(p, stdout_data, stderr_data)\n\t        if stdout_data:\n\t            sys.stderr.write(\"output from {}:\\n{}\\n\\n\".format(cmdr, stdout_data))\n", "        if stderr_data:\n\t            sys.stderr.write(\"stderr from {}:\\n{}\\n\\n\".format(cmdr, stderr_data))\n\t        raise\n\t    if p.returncode != 0:\n\t        logger.error(\"cmd failed ({}) {}\".format(p.returncode, cmdr))\n\t        stdout_data, stderr_data = _getio(p, stdout_data, stderr_data)\n\t        if stdout_data:\n\t            sys.stderr.write(\"output from {}:\\n{}\\n\\n\".format(cmdr, stdout_data))\n\t        if stderr_data:\n\t            sys.stderr.write(\"stderr from {}:\\n{}\\n\\n\".format(cmdr, stderr_data))\n", "        raise Exception(\"error: cmd failed: {}\".format(cmdr))\n\t    if logger.isEnabledFor(logging.DEBUG):\n\t        logger.debug(\n\t            \"cmd success: %s\\n%s\\n%s\\n\",\n\t            cmdr,\n\t            maybedecode(stdout_data),\n\t            maybedecode(stderr_data),\n\t        )\n\tdef atexitrun(cmd, *args, **kwargs):\n\t    cargs = [cmd] + list(args)\n", "    atexit.register(xrun, *cargs, **kwargs)\n\tdef find_binary(binary, exc=True, binary_name=\"algorand-indexer\"):\n\t    if binary:\n\t        return binary\n\t    # manually search local build and PATH for binary_name\n\t    path = [f\"cmd/{binary_name}\"] + os.getenv(\"PATH\").split(\":\")\n\t    for pd in path:\n\t        ib = os.path.join(pd, binary_name)\n\t        if os.path.exists(ib):\n\t            return ib\n", "    msg = f\"could not find {binary_name} at the provided location or PATH environment variable.\"\n\t    if exc:\n\t        raise Exception(msg)\n\t    logger.error(msg)\n\t    return None\n\tdef ensure_test_db(connection_string, keep_temps=False):\n\t    if connection_string:\n\t        # use the passed db\n\t        return connection_string\n\t    # create a temporary database\n", "    dbname = \"e2eindex_{}_{}\".format(int(time.time()), random.randrange(1000))\n\t    xrun([\"dropdb\", \"--if-exists\", dbname], timeout=5)\n\t    xrun([\"createdb\", dbname], timeout=5)\n\t    if not keep_temps:\n\t        atexitrun([\"dropdb\", \"--if-exists\", dbname], timeout=5)\n\t    else:\n\t        logger.info(\"leaving db %r\", dbname)\n\t    return \"dbname={} sslmode=disable\".format(dbname)\n\t# whoever calls this will need to import boto and get the s3 client\n\tdef firstFromS3Prefix(\n", "    s3, bucket, prefix, desired_filename, outdir=None, outpath=None\n\t) -> bool:\n\t    haystack = []\n\t    found_needle = False\n\t    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=100)\n\t    if (not response.get(\"KeyCount\")) or (\"Contents\" not in response):\n\t        raise Exception(\"nothing found in s3://{}/{}\".format(bucket, prefix))\n\t    for x in response[\"Contents\"]:\n\t        path = x[\"Key\"]\n\t        haystack.append(path)\n", "        _, fname = path.rsplit(\"/\", 1)\n\t        if fname == desired_filename:\n\t            if outpath is None:\n\t                if outdir is None:\n\t                    outdir = \".\"\n\t                outpath = os.path.join(outdir, desired_filename)\n\t            logger.info(\"s3://%s/%s -> %s\", bucket, x[\"Key\"], outpath)\n\t            s3.download_file(bucket, x[\"Key\"], outpath)\n\t            found_needle = True\n\t            break\n", "    if not found_needle:\n\t        logger.warning(\"file {} not found in s3://{}/{}\".format(desired_filename, bucket, prefix))\n\t    return found_needle\n\tdef countblocks(path):\n\t    db = sqlite3.connect(path)\n\t    cursor = db.cursor()\n\t    cursor.execute(\"SELECT max(rnd) FROM blocks\")\n\t    row = cursor.fetchone()\n\t    cursor.close()\n\t    db.close()\n", "    return row[0]\n\tdef hassuffix(x, *suffixes):\n\t    for s in suffixes:\n\t        if x.endswith(s):\n\t            return True\n\t    return False\n"]}
{"filename": "e2e_tests/src/e2e_conduit/runner.py", "chunked_list": ["import atexit\n\timport os\n\timport logging\n\timport shutil\n\timport subprocess\n\timport sys\n\timport tempfile\n\timport time\n\timport yaml\n\tfrom e2e_common.util import find_binary\n", "from e2e_conduit.subslurp import subslurp\n\tlogger = logging.getLogger(__name__)\n\tclass ConduitE2ETestRunner:\n\t    def __init__(self, conduit_bin, keep_temps=False):\n\t        self.conduit_bin = find_binary(conduit_bin, binary_name=\"conduit\")\n\t        self.keep_temps = keep_temps\n\t    def setup_scenario(self, scenario):\n\t        # Setup conduit_dir for conduit data dir\n\t        scenario.conduit_dir = tempfile.mkdtemp()\n\t        if not self.keep_temps:\n", "            atexit.register(shutil.rmtree, scenario.conduit_dir, onerror=logger.error)\n\t        else:\n\t            logger.info(f\"leaving temp dir {scenario.conduit_dir}\")\n\t        scenario.accumulated_config = {\n\t            \"conduit_dir\": scenario.conduit_dir,\n\t        }\n\t        for plugin in [scenario.importer, *scenario.processors, scenario.exporter]:\n\t            plugin.setup(scenario.accumulated_config)\n\t            plugin.resolve_config()\n\t            scenario.accumulated_config = {\n", "                **scenario.accumulated_config,\n\t                **plugin.config_output,\n\t            }\n\t        # Write conduit config to data directory\n\t        with open(\n\t            os.path.join(scenario.conduit_dir, \"conduit.yml\"), \"w\"\n\t        ) as conduit_cfg:\n\t            yaml.dump(\n\t                {\n\t                    \"log-level\": \"info\",\n", "                    \"importer\": {\n\t                        \"name\": scenario.importer.name,\n\t                        \"config\": scenario.importer.config_input,\n\t                    },\n\t                    \"processors\": [\n\t                        {\n\t                            \"name\": processor.name,\n\t                            \"config\": processor.config_input,\n\t                        }\n\t                        for processor in scenario.processors\n", "                    ],\n\t                    \"exporter\": {\n\t                        \"name\": scenario.exporter.name,\n\t                        \"config\": scenario.exporter.config_input,\n\t                    },\n\t                },\n\t                conduit_cfg,\n\t            )\n\t    def run_scenario(self, scenario):\n\t        # Run conduit\n", "        start = time.time()\n\t        cmd = [self.conduit_bin, \"-d\", scenario.conduit_dir]\n\t        logger.info(f\"running scenario {scenario.name}\")\n\t        logger.debug(\"%s\", \" \".join(map(repr, cmd)))\n\t        indexerdp = subprocess.Popen(\n\t            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n\t        )\n\t        atexit.register(indexerdp.kill)\n\t        indexerout = subslurp(indexerdp.stdout)\n\t        logger.info(f\"Waiting for conduit to reach round {scenario.importer.lastblock}\")\n", "        try:\n\t            indexerout.run(scenario.importer.lastblock)\n\t        except RuntimeError as exc:\n\t            logger.error(f\"{exc}\")\n\t            logger.error(\n\t                f\"conduit hit an error during execution: {indexerout.error_log}\"\n\t            )\n\t            sys.stderr.write(indexerout.dump())\n\t            return 1\n\t        if indexerout.round < scenario.importer.lastblock:\n", "            logger.error(\"conduit did not reach round={scenario.importer.lastblock}\")\n\t            sys.stderr.write(indexerout.dump())\n\t            return 1\n\t        # now indexer's round == the final network round\n\t        if errors := scenario.get_validation_errors():\n\t            logger.error(f\"conduit failed validation: {errors}\")\n\t            sys.stderr.write(indexerout.dump())\n\t            return 1\n\t        logger.info(\n\t            f\"reached expected round={scenario.importer.lastblock} and passed validation\"\n", "        )\n\t        dt = time.time() - start\n\t        sys.stdout.write(\"conduit e2etest OK ({:.1f}s)\\n\".format(dt))\n\t        return 0\n"]}
{"filename": "e2e_tests/src/e2e_conduit/subslurp.py", "chunked_list": ["from datetime import datetime, timedelta\n\timport gzip\n\timport io\n\timport logging\n\timport re\n\tlogger = logging.getLogger(__name__)\n\tclass subslurp:\n\t    \"\"\"accumulate stdout or stderr from a subprocess and hold it for debugging if something goes wrong\"\"\"\n\t    def __init__(self, f):\n\t        self.f = f\n", "        self.buf = io.BytesIO()\n\t        self.gz = gzip.open(self.buf, \"wb\")\n\t        self.timeout = timedelta(seconds=120)\n\t        # Matches conduit log output: \"Pipeline round: 110\"\n\t        self.round_re = re.compile(b'.*\"Pipeline round: ([0-9]+)\"')\n\t        self.round = 0\n\t        self.error_log = None\n\t    def logIsError(self, log_line):\n\t        if b\"error\" in log_line:\n\t            self.error_log = log_line\n", "            return True\n\t        return False\n\t    def tryParseRound(self, log_line):\n\t        m = self.round_re.match(log_line)\n\t        if m is not None and m.group(1) is not None:\n\t            self.round = int(m.group(1))\n\t    def run(self, lastround):\n\t        if len(self.f.peek().strip()) == 0:\n\t            logger.info(\"No Conduit output found\")\n\t            return\n", "        start = datetime.now()\n\t        lastlog = datetime.now()\n\t        while (\n\t            datetime.now() - start < self.timeout\n\t            and datetime.now() - lastlog < timedelta(seconds=15)\n\t        ):\n\t            for line in self.f:\n\t                lastlog = datetime.now()\n\t                if self.gz is not None:\n\t                    self.gz.write(line)\n", "                self.tryParseRound(line)\n\t                if self.round >= lastround:\n\t                    logger.info(f\"Conduit reached desired lastround: {lastround}\")\n\t                    return\n\t                if self.logIsError(line):\n\t                    raise RuntimeError(f\"E2E tests logged an error: {self.error_log}\")\n\t    def dump(self) -> str:\n\t        if self.gz is not None:\n\t            self.gz.close()\n\t        self.gz = None\n", "        self.buf.seek(0)\n\t        r = gzip.open(self.buf, \"rt\")\n\t        return r.read() # type: ignore\n"]}
{"filename": "e2e_tests/src/e2e_conduit/__init__.py", "chunked_list": []}
{"filename": "e2e_tests/src/e2e_conduit/e2econduit.py", "chunked_list": ["#!/usr/bin/env python3\n\timport argparse\n\timport logging\n\timport os\n\timport sys\n\tfrom e2e_conduit.runner import ConduitE2ETestRunner\n\tfrom e2e_conduit.scenarios import scenarios\n\tfrom e2e_conduit.scenarios.follower_indexer_scenario import (\n\t    FollowerIndexerScenario,\n\t    FollowerIndexerScenarioWithDeleteTask,\n", ")\n\tfrom e2e_conduit.scenarios.filter_scenario import (\n\t    app_filter_indexer_scenario,\n\t    pay_filter_indexer_scenario,\n\t)\n\tlogger = logging.getLogger(__name__)\n\tdef main():\n\t    ap = argparse.ArgumentParser()\n\t    # TODO FIXME convert keep_temps to debug mode which will leave all resources running/around\n\t    # So files will not be deleted and docker containers will be left running\n", "    ap.add_argument(\"--keep-temps\", default=False, action=\"store_true\")\n\t    ap.add_argument(\n\t        \"--conduit-bin\",\n\t        default=None,\n\t        help=\"path to conduit binary, otherwise search PATH\",\n\t    )\n\t    ap.add_argument(\n\t        \"--source-net\",\n\t        help=\"Path to test network directory containing Primary and other nodes. May be a tar file.\",\n\t    )\n", "    ap.add_argument(\n\t        \"--s3-source-net\",\n\t        help=\"AWS S3 key suffix to test network tarball containing Primary and other nodes. Must be a tar bz2 file.\",\n\t    )\n\t    ap.add_argument(\"--verbose\", default=False, action=\"store_true\")\n\t    args = ap.parse_args()\n\t    if args.verbose:\n\t        logging.basicConfig(level=logging.DEBUG)\n\t    else:\n\t        logging.basicConfig(level=logging.INFO)\n", "    sourcenet = args.source_net\n\t    if not sourcenet:\n\t        e2edata = os.getenv(\"E2EDATA\")\n\t        sourcenet = e2edata and os.path.join(e2edata, \"net\")\n\t    importer_source = sourcenet if sourcenet else args.s3_source_net\n\t    if importer_source:\n\t        scenarios.extend(\n\t            [\n\t                FollowerIndexerScenario(importer_source),\n\t                FollowerIndexerScenarioWithDeleteTask(importer_source),\n", "                app_filter_indexer_scenario(importer_source),\n\t                pay_filter_indexer_scenario(importer_source),\n\t            ]\n\t        )\n\t    runner = ConduitE2ETestRunner(args.conduit_bin, keep_temps=args.keep_temps)\n\t    success = True\n\t    for scenario in scenarios:\n\t        runner.setup_scenario(scenario)\n\t        if scenario.exporter.name == \"postgresql\":\n\t            print(\n", "                f\"postgresql exporter with connect info: {scenario.exporter.config_input['connection-string']}\"\n\t            )\n\t        if runner.run_scenario(scenario) != 0:\n\t            success = False\n\t    return 0 if success else 1\n\tif __name__ == \"__main__\":\n\t    sys.exit(main())\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/__init__.py", "chunked_list": []}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/plugin_fixture.py", "chunked_list": ["class PluginFixture:\n\t    def __init__(self):\n\t        self.config_input = {}\n\t        self.config_output = {}\n\t    @property\n\t    def name(self):\n\t        raise NotImplementedError\n\t    def resolve_config(self):\n\t        self.resolve_config_input()\n\t        self.resolve_config_output()\n", "    def resolve_config_input(self):\n\t        pass\n\t    def resolve_config_output(self):\n\t        pass\n\t    def setup(self, accumulated_config):\n\t        raise NotImplementedError\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/processors/filter.py", "chunked_list": ["from e2e_conduit.fixtures.plugin_fixture import PluginFixture\n\t''' FilterProcessor is a simple passthrough for the `filter_processor`\n\t    which sets the scenario's filters to the initialized value\n\t'''\n\tclass FilterProcessor(PluginFixture):\n\t    def __init__(self, filters, search_inner=True):\n\t        self.filters = filters\n\t        self.search_inner = search_inner\n\t        super().__init__()\n\t    @property\n", "    def name(self):\n\t        return \"filter_processor\"\n\t    def resolve_config_input(self):\n\t        self.config_input[\"filters\"] = self.filters\n\t        self.config_input[\"search-inner\"] = self.search_inner\n\t    def resolve_config_output(self):\n\t        pass\n\t    def setup(self, accumulated_config):\n\t        pass\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/processors/__init__.py", "chunked_list": ["from e2e_conduit.fixtures.processors.filter import FilterProcessor"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/exporters/file_exporter.py", "chunked_list": ["import logging\n\timport random\n\timport string\n\timport sys\n\timport time\n\tfrom e2e_conduit.fixtures.plugin_fixture import PluginFixture\n\tfrom e2e_common.util import atexitrun, xrun\n\tlogger = logging.getLogger(__name__)\n\tclass FileExporter(PluginFixture):\n\t    def __init__(self):\n", "        super().__init__()\n\t    @property\n\t    def name(self):\n\t        return \"filewriter\"\n\t    def setup(self, accumulated_config):\n\t        try:\n\t            conduit_dir = accumulated_config[\"conduit_dir\"]\n\t        except KeyError as exc:\n\t            logger.error(\n\t                f\"FileExporter needs to be provided with the proper config: {exc}\"\n", "            )\n\t            raise\n\t            atexitrun([\"docker\", \"kill\", self.container_name])\n\t    def resolve_config_input(self):\n\t        self.config_input = {\n\t            \"connection-string\": f\"host=localhost port={self.port} user={self.user} password={self.password} dbname={self.db_name} sslmode=disable\",\n\t            \"max-conn\": self.max_conn,\n\t        }\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/exporters/postgresql.py", "chunked_list": ["from dataclasses import dataclass, asdict\n\timport logging\n\timport random\n\timport string\n\timport sys\n\timport time\n\tfrom e2e_conduit.fixtures.plugin_fixture import PluginFixture\n\tfrom e2e_common.util import atexitrun, xrun\n\tlogger = logging.getLogger(__name__)\n\tCONFIG_DELETE_TASK = \"delete-task\"\n", "@dataclass\n\tclass DeleteTask:\n\t    interval: int\n\t    rounds: int\n\tclass PostgresqlExporter(PluginFixture):\n\t    def __init__(self, max_conn=0, delete_interval=0, delete_rounds=0):\n\t        self.user = \"algorand\"\n\t        self.password = \"algorand\"\n\t        self.db_name = \"e2e_db\"\n\t        # Should we have a random port here so that we can run multiple of these in parallel?\n", "        self.port = \"45432\"\n\t        self.container_name = \"\"\n\t        self.max_conn = max_conn\n\t        self.delete_task = DeleteTask(delete_interval, delete_rounds)\n\t        super().__init__()\n\t    @property\n\t    def name(self):\n\t        return \"postgresql\"\n\t    def setup(self, _):\n\t        self.container_name = \"\".join(\n", "            random.choice(string.ascii_lowercase) for _ in range(10)\n\t        )\n\t        self.port = f\"{random.randint(1150, 65535)}\"\n\t        try:\n\t            xrun(\n\t                [\n\t                    \"docker\",\n\t                    \"run\",\n\t                    \"-d\",\n\t                    \"--name\",\n", "                    self.container_name,\n\t                    \"-p\",\n\t                    f\"{self.port}:5432\",\n\t                    \"-e\",\n\t                    f\"POSTGRES_PASSWORD={self.password}\",\n\t                    \"-e\",\n\t                    f\"POSTGRES_USER={self.user}\",\n\t                    \"-e\",\n\t                    f\"POSTGRES_DB={self.db_name}\",\n\t                    \"postgres:13-alpine\",\n", "                ]\n\t            )\n\t            # Sleep 15 seconds to let postgresql start--otherwise conduit might fail on startup\n\t            time.sleep(15)\n\t            atexitrun([\"docker\", \"kill\", self.container_name])\n\t        except Exception as exc:\n\t            logger.error(f\"docker postgres container startup failed: {exc}\")\n\t            sys.exit(1)\n\t    def resolve_config_input(self):\n\t        self.config_input = {\n", "            \"connection-string\": f\"host=localhost port={self.port} user={self.user} password={self.password} dbname={self.db_name} sslmode=disable\",\n\t            \"max-conn\": self.max_conn,\n\t            CONFIG_DELETE_TASK: asdict(self.delete_task),\n\t        }\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/exporters/__init__.py", "chunked_list": ["from e2e_conduit.fixtures.exporters.postgresql import PostgresqlExporter\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/importers/follower_algod.py", "chunked_list": ["import glob\n\timport json\n\timport logging\n\timport os\n\timport boto3\n\tfrom botocore.config import Config\n\tfrom botocore import UNSIGNED\n\tfrom e2e_common.util import hassuffix, xrun, firstFromS3Prefix, countblocks, atexitrun\n\tfrom e2e_conduit.fixtures.importers.importer_plugin import ImporterPlugin\n\tlogger = logging.getLogger(__name__)\n", "class FollowerAlgodImporter(ImporterPlugin):\n\t    def __init__(self, sourcenet):\n\t        self.algoddir = None\n\t        self.sourcenet = sourcenet\n\t        self.last = None\n\t        super().__init__()\n\t    @property\n\t    def name(self):\n\t        return \"algod\"\n\t    @property\n", "    def lastblock(self):\n\t        if self.last is None:\n\t            raise RuntimeError(\"algod importer has no blockfiles configured\")\n\t        return self.last\n\t    def resolve_config_input(self):\n\t        self.config_input[\"mode\"] = \"follower\"\n\t        with open(os.path.join(self.algoddir, \"algod.net\"), \"r\") as algod_net:\n\t            self.config_input[\"netaddr\"] = \"http://\" + algod_net.read().strip()\n\t        with open(os.path.join(self.algoddir, \"algod.token\"), \"r\") as algod_token:\n\t            self.config_input[\"token\"] = algod_token.read().strip()\n", "    def resolve_config_output(self):\n\t        self.config_output[\"algod_token\"] = self.config_input[\"token\"]\n\t        self.config_output[\"algod_net\"] = self.config_input[\"netaddr\"]\n\t        self.config_output[\"algod_data_dir\"] = self.algoddir\n\t    def setup(self, accumulated_config):\n\t        try:\n\t            conduit_dir = accumulated_config[\"conduit_dir\"]\n\t        except KeyError as exc:\n\t            logger.error(\n\t                f\"AlgodImporter needs to be provided with the proper config: {exc}\"\n", "            )\n\t            raise\n\t        source_is_tar = hassuffix(\n\t            self.sourcenet, \".tar\", \".tar.gz\", \".tar.bz2\", \".tar.xz\"\n\t        )\n\t        if not (source_is_tar or (self.sourcenet and os.path.isdir(self.sourcenet))):\n\t            # Assuming self.sourcenet is an s3 source net\n\t            tarname = f\"{self.sourcenet}.tar.bz2\"\n\t            # fetch test data from S3\n\t            bucket = \"algorand-testdata\"\n", "            s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n\t            prefix = \"indexer/e2e4\"\n\t            if \"/\" in tarname:\n\t                cmhash_tarnme = tarname.split(\"/\")\n\t                cmhash = cmhash_tarnme[0]\n\t                tarname = cmhash_tarnme[1]\n\t                prefix += \"/\" + cmhash\n\t                tarpath = os.path.join(conduit_dir, tarname)\n\t            else:\n\t                tarpath = os.path.join(conduit_dir, tarname)\n", "            success = firstFromS3Prefix(s3, bucket, prefix, tarname, outpath=tarpath)\n\t            if not success:\n\t                raise Exception(\n\t                    f\"failed to locate tarname={tarname} from AWS S3 path {bucket}/{prefix}\"\n\t                )\n\t            source_is_tar = True\n\t            self.sourcenet = tarpath\n\t        tempnet = os.path.join(conduit_dir, \"net\")\n\t        if source_is_tar:\n\t            xrun([\"tar\", \"-C\", conduit_dir, \"-x\", \"-f\", self.sourcenet])\n", "        else:\n\t            xrun([\"rsync\", \"-a\", self.sourcenet + \"/\", tempnet + \"/\"])\n\t        blockfiles = glob.glob(\n\t            os.path.join(conduit_dir, \"net\", \"Primary\", \"*\", \"*.block.sqlite\")\n\t        )\n\t        self.last = countblocks(blockfiles[0])\n\t        # Reset the secondary node, and enable follow mode.\n\t        # This is what conduit will connect to for data access.\n\t        for root, _, files in os.walk(os.path.join(tempnet, \"Node\", \"tbd-v1\")):\n\t            for f in files:\n", "                if \".sqlite\" in f:\n\t                    os.remove(os.path.join(root, f))\n\t        cf = {}\n\t        with open(os.path.join(tempnet, \"Node\", \"config.json\"), \"r\") as config_file:\n\t            cf = json.load(config_file)\n\t            cf[\"EnableFollowMode\"] = True\n\t        with open(os.path.join(tempnet, \"Node\", \"config.json\"), \"w\") as config_file:\n\t            config_file.write(json.dumps(cf))\n\t        try:\n\t            xrun([\"goal\", \"network\", \"start\", \"-r\", tempnet])\n", "        except Exception:\n\t            logger.error(\"failed to start private network, looking for node.log\")\n\t            for root, dirs, files in os.walk(tempnet):\n\t                for f in files:\n\t                    if f == \"node.log\":\n\t                        p = os.path.join(root, f)\n\t                        logger.error(\"found node.log: {}\".format(p))\n\t                        with open(p) as nf:\n\t                            for line in nf:\n\t                                logger.error(\"   {}\".format(line))\n", "            raise\n\t        atexitrun([\"goal\", \"network\", \"stop\", \"-r\", tempnet])\n\t        self.algoddir = os.path.join(tempnet, \"Node\")\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/importers/algod.py", "chunked_list": ["import atexit\n\timport glob\n\timport logging\n\timport os\n\timport shutil\n\timport tempfile\n\timport boto3\n\tfrom botocore.config import Config\n\tfrom botocore import UNSIGNED\n\tfrom e2e_common.util import hassuffix, xrun, firstFromS3Prefix, countblocks, atexitrun\n", "from e2e_conduit.fixtures.importers.importer_plugin import ImporterPlugin\n\tlogger = logging.getLogger(__name__)\n\tclass AlgodImporter(ImporterPlugin):\n\t    def __init__(self, sourcenet):\n\t        self.algoddir = None\n\t        self.sourcenet = sourcenet\n\t        self.last = None\n\t        super().__init__()\n\t    @property\n\t    def name(self):\n", "        return \"algod\"\n\t    @property\n\t    def lastblock(self):\n\t        if self.last is None:\n\t            raise RuntimeError(\"algod importer has no blockfiles configured\")\n\t        return self.last\n\t    def resolve_config_input(self):\n\t        with open(os.path.join(self.algoddir, \"algod.net\"), \"r\") as algod_net:\n\t            self.config_input[\"netaddr\"] = \"http://\" + algod_net.read().strip()\n\t        with open(os.path.join(self.algoddir, \"algod.token\"), \"r\") as algod_token:\n", "            self.config_input[\"token\"] = algod_token.read().strip()\n\t    def resolve_config_output(self):\n\t        self.config_output[\"algod_token\"] = self.config_input[\"token\"]\n\t        self.config_output[\"algod_net\"] = self.config_input[\"netaddr\"]\n\t        self.config_output[\"algod_data_dir\"] = self.algoddir\n\t    def setup(self, accumulated_config):\n\t        try:\n\t            conduit_dir = accumulated_config[\"conduit_dir\"]\n\t        except KeyError as exc:\n\t            logger.error(\n", "                f\"AlgodImporter needs to be provided with the proper config: {exc}\"\n\t            )\n\t            raise\n\t        source_is_tar = hassuffix(\n\t            self.sourcenet, \".tar\", \".tar.gz\", \".tar.bz2\", \".tar.xz\"\n\t        )\n\t        if not (source_is_tar or (self.sourcenet and os.path.isdir(self.sourcenet))):\n\t            # Assuming self.sourcenet is an s3 source net\n\t            tarname = f\"{self.sourcenet}.tar.bz2\"\n\t            # fetch test data from S3\n", "            bucket = \"algorand-testdata\"\n\t            s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n\t            prefix = \"indexer/e2e4\"\n\t            if \"/\" in tarname:\n\t                cmhash_tarnme = tarname.split(\"/\")\n\t                cmhash = cmhash_tarnme[0]\n\t                tarname =cmhash_tarnme[1]\n\t                prefix+=\"/\"+cmhash\n\t                tarpath = os.path.join(conduit_dir, tarname)\n\t            else:\n", "                tarpath = os.path.join(conduit_dir, tarname)\n\t            success = firstFromS3Prefix(s3, bucket, prefix, tarname, outpath=tarpath)\n\t            if not success:\n\t                raise Exception(\n\t                    f\"failed to locate tarname={tarname} from AWS S3 path {bucket}/{prefix}\"\n\t                )\n\t            source_is_tar = True\n\t            self.sourcenet = tarpath\n\t        tempnet = os.path.join(conduit_dir, \"net\")\n\t        if source_is_tar:\n", "            xrun([\"tar\", \"-C\", conduit_dir, \"-x\", \"-f\", self.sourcenet])\n\t        else:\n\t            xrun([\"rsync\", \"-a\", self.sourcenet + \"/\", tempnet + \"/\"])\n\t        blockfiles = glob.glob(\n\t            os.path.join(conduit_dir, \"net\", \"Primary\", \"*\", \"*.block.sqlite\")\n\t        )\n\t        self.last = countblocks(blockfiles[0])\n\t        try:\n\t            xrun([\"goal\", \"network\", \"start\", \"-r\", tempnet])\n\t        except Exception:\n", "            logger.error(\"failed to start private network, looking for node.log\")\n\t            for root, dirs, files in os.walk(tempnet):\n\t                for f in files:\n\t                    if f == \"node.log\":\n\t                        p = os.path.join(root, f)\n\t                        logger.error(\"found node.log: {}\".format(p))\n\t                        with open(p) as nf:\n\t                            for line in nf:\n\t                                logger.error(\"   {}\".format(line))\n\t            raise\n", "        atexitrun([\"goal\", \"network\", \"stop\", \"-r\", tempnet])\n\t        self.algoddir = os.path.join(tempnet, \"Primary\")\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/importers/__init__.py", "chunked_list": ["from e2e_conduit.fixtures.importers.algod import AlgodImporter\n\tfrom e2e_conduit.fixtures.importers.follower_algod import FollowerAlgodImporter\n"]}
{"filename": "e2e_tests/src/e2e_conduit/fixtures/importers/importer_plugin.py", "chunked_list": ["from e2e_conduit.fixtures.plugin_fixture import PluginFixture\n\tclass ImporterPlugin(PluginFixture):\n\t    def __init__(self):\n\t        super().__init__()\n\t    @property\n\t    def lastblock(self):\n\t        raise NotImplementedError\n"]}
{"filename": "e2e_tests/src/e2e_conduit/scenarios/file_exporter_scenario.py", "chunked_list": ["import e2e_conduit.fixtures.importers as importers\n\timport e2e_conduit.fixtures.processors as processors\n\timport e2e_conduit.fixtures.exporters as exporters\n\tfrom e2e_conduit.scenarios import Scenario\n\tdef indexer_scenario(sourcenet):\n\t    return Scenario(\n\t        \"indexer_scenario\",\n\t        importer=importers.AlgodImporter(sourcenet),\n\t        processors=[\n\t            processors.BlockEvaluator(),\n", "        ],\n\t        exporter=exporters.PostgresqlExporter(),\n\t    )\n"]}
{"filename": "e2e_tests/src/e2e_conduit/scenarios/__init__.py", "chunked_list": ["from __future__ import annotations\n\tfrom dataclasses import dataclass, field\n\tfrom e2e_conduit.fixtures.importers.importer_plugin import ImporterPlugin\n\tfrom e2e_conduit.fixtures.plugin_fixture import PluginFixture\n\t@dataclass\n\tclass Scenario:\n\t    \"\"\"Data class for conduit E2E test pipelines\"\"\"\n\t    name: str\n\t    importer: ImporterPlugin\n\t    processors: list[PluginFixture]\n", "    exporter: PluginFixture\n\t    accumulated_config: dict = field(default_factory=dict)\n\t    conduit_dir: str = \"\"\n\t    def get_validation_errors(self) -> list[str]:\n\t        \"\"\"\n\t        Validate the scenario.\n\t        A non empty list of error messages signals a failed validation.\n\t        \"\"\"\n\t        return []\n\tscenarios: list[Scenario] = []\n"]}
{"filename": "e2e_tests/src/e2e_conduit/scenarios/follower_indexer_scenario.py", "chunked_list": ["import time\n\tfrom e2e_common.indexer_db import IndexerDB\n\tfrom e2e_conduit.fixtures import importers, exporters\n\tfrom e2e_conduit.fixtures.exporters.postgresql import CONFIG_DELETE_TASK\n\tfrom e2e_conduit.scenarios import Scenario\n\tclass _Errors:\n\t    # SQL errors:\n\t    def block_header_final_round_query_error(self, e):\n\t        return f\"Failed to get final round from indexer block_header table: {e}\"\n\t    def txn_min_max_query_error(self, e):\n", "        return f\"Failed to get min/max round from indexer txn table: {e}\"\n\t    def table_row_count_query_error(self, table_name, e):\n\t        return f\"Failed to get row count from indexer {table_name} table: {e}\"\n\t    # Logic errors:\n\t    def txn_table_biggest_round_too_big(self, last_txn_round):\n\t        return f\"Indexer table txn has round={last_txn_round} greater than network's final round={self.importer.lastblock}\"\n\t    def block_header_round_mismatch(self, indexer_final_round):\n\t        return f\"Indexer table block_header has final round={indexer_final_round} different from network's final round={self.importer.lastblock}\"\n\t    def delete_task_txn_rounds_different(self, first_txn_round, last_txn_round):\n\t        return f\"\"\"Indexer table txn has smallest round={first_txn_round} different from greatest round={last_txn_round}.\n", "This is problematic for the delete task because delete-task configuration is {self.exporter.config_input[CONFIG_DELETE_TASK]} so we should only keep transactions for the very last round.\"\"\"\n\tclass FollowerIndexerScenario(Scenario, _Errors):\n\t    def __init__(self, sourcenet):\n\t        super().__init__(\n\t            name=\"follower_indexer_scenario\",\n\t            importer=importers.FollowerAlgodImporter(sourcenet),\n\t            processors=[],\n\t            exporter=exporters.PostgresqlExporter(),\n\t        )\n\t    def get_validation_errors(self) -> list[str]:\n", "        \"\"\"\n\t        validation checks that indexer tables block_header and txn makes sense when\n\t        compared with the importer lastblock which is the network's last round in the\n\t        network's blocks table.\n\t        \"\"\"\n\t        time.sleep(1)\n\t        init_args = {\n\t            item.split(\"=\")[0]: item.split(\"=\")[1]\n\t            for item in self.exporter.config_input[\"connection-string\"].split()\n\t        }\n", "        idb = IndexerDB(\n\t            host=init_args[\"host\"],\n\t            port=init_args[\"port\"],\n\t            user=init_args[\"user\"],\n\t            password=init_args[\"password\"],\n\t            dbname=init_args[\"dbname\"],\n\t        )\n\t        errors = []\n\t        try:\n\t            _, last_txn_round = idb.get_txn_min_max_round()\n", "            if last_txn_round > self.importer.lastblock:\n\t                errors.append(self.txn_table_biggest_round_too_big(last_txn_round))\n\t        except Exception as e:\n\t            errors.append(self.txn_min_max_query_error(e))\n\t        try:\n\t            indexer_final_round = idb.get_block_header_final_round()\n\t            if indexer_final_round != self.importer.lastblock:\n\t                errors.append(self.block_header_round_mismatch(indexer_final_round))\n\t        except Exception as e:\n\t            errors.append(self.block_header_final_round_query_error(e))\n", "        return errors\n\tclass FollowerIndexerScenarioWithDeleteTask(Scenario, _Errors):\n\t    def __init__(self, sourcenet):\n\t        super().__init__(\n\t            name=\"follower_indexer_scenario_with_delete_task\",\n\t            importer=importers.FollowerAlgodImporter(sourcenet),\n\t            processors=[],\n\t            exporter=exporters.PostgresqlExporter(delete_interval=1, delete_rounds=1),\n\t        )\n\t    def get_validation_errors(self) -> list[str]:\n", "        \"\"\"\n\t        validation checks that txn either contains no rows or that\n\t        the max round contained is the same as the network's lastblock\n\t        \"\"\"\n\t        # sleep for 3 seconds to allow delete_task iteration to wake up after 2 seconds\n\t        # for its final pruning when Conduit is all caught up with the network\n\t        time.sleep(3)\n\t        idb = IndexerDB.from_connection_string(\n\t            self.exporter.config_input[\"connection-string\"]\n\t        )\n", "        errors = []\n\t        try:\n\t            num_txn_rows = idb.get_table_row_count(\"txn\")\n\t            if num_txn_rows > 0:\n\t                try:\n\t                    first_txn_round, last_txn_round = idb.get_txn_min_max_round()\n\t                    if first_txn_round != last_txn_round:\n\t                        errors.append(\n\t                            self.delete_task_txn_rounds_different(\n\t                                first_txn_round, last_txn_round\n", "                            )\n\t                        )\n\t                    if last_txn_round > self.importer.lastblock:\n\t                        errors.append(\n\t                            self.txn_table_biggest_round_too_big(last_txn_round)\n\t                        )\n\t                except Exception as e:\n\t                    errors.append(self.txn_min_max_query_error(e))\n\t        except Exception as e:\n\t            errors.append(self.table_row_count_query_error(\"txn\", e))\n", "        return errors\n"]}
{"filename": "e2e_tests/src/e2e_conduit/scenarios/filter_scenario.py", "chunked_list": ["import e2e_conduit.fixtures.importers as importers\n\timport e2e_conduit.fixtures.processors as processors\n\timport e2e_conduit.fixtures.exporters as exporters\n\tfrom e2e_conduit.scenarios import Scenario, scenarios\n\tdef app_filter_indexer_scenario(sourcenet):\n\t    return Scenario(\n\t        \"app_filter_indexer_scenario\",\n\t        importer=importers.FollowerAlgodImporter(sourcenet),\n\t        processors=[\n\t            processors.FilterProcessor([\n", "                {\"any\": [\n\t                    {\"tag\": \"txn.type\",\n\t                     \"expression-type\": \"equal\",\n\t                     \"expression\": \"appl\"\n\t                    }\n\t                ]}\n\t            ]),\n\t        ],\n\t        exporter=exporters.PostgresqlExporter(),\n\t    )\n", "def pay_filter_indexer_scenario(sourcenet):\n\t    return Scenario(\n\t        \"pay_filter_indexer_scenario\",\n\t        importer=importers.FollowerAlgodImporter(sourcenet),\n\t        processors=[\n\t            processors.FilterProcessor([\n\t                {\"any\": [\n\t                    {\"tag\": \"txn.type\",\n\t                     \"expression-type\": \"equal\",\n\t                     \"expression\": \"pay\"\n", "                    }\n\t                ]}\n\t            ]),\n\t        ],\n\t        exporter=exporters.PostgresqlExporter(),\n\t    )\n"]}
