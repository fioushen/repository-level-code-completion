{"filename": "tc_distill_edm.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport copy\n\timport functools\n\timport pickle\n\timport sys\n\tfrom typing import Dict, Optional\n\timport torch\n", "import torch.nn.functional\n\tfrom absl import app, flags\n\timport lib\n\tfrom lib.distributed import device, device_id\n\tfrom lib.util import FLAGS, int_str\n\t# Imports within edm/ are often relative to edm/ so we do this.\n\tsys.path.append('edm')\n\timport dnnlib\n\tfrom torch_utils import distributed as dist\n\tfrom torch_utils import misc\n", "class EluDDIM05TCMultiStepx0(lib.train.TrainModel):\n\t    SIGMA_DATA = 0.5\n\t    SIGMA_MIN: float = 0.002\n\t    SIGMA_MAX: float = 80.\n\t    RHO: float = 7.\n\t    def __init__(self, res: int, timesteps: int, **params):\n\t        super().__init__(\"EluUNet\", res, timesteps, **params)\n\t        self.use_imagenet = FLAGS.dataset == \"imagenet64\"\n\t        self.num_classes = 1000 if self.use_imagenet else 10\n\t        # Setup pretrained model\n", "        lib.distributed.barrier()\n\t        if FLAGS.dataset == \"imagenet64\":\n\t            pretrained_url = \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl\"\n\t        elif FLAGS.dataset == \"cifar10\":\n\t            pretrained_url = \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-uncond-ve.pkl\"\n\t        else:\n\t            raise ValueError(\"Only cifar10 or imagenet64 is supported for now.\")\n\t        with dnnlib.util.open_url(pretrained_url) as f:\n\t            pretrained = pickle.load(f)['ema']\n\t        lib.distributed.barrier()\n", "        network_kwargs = self.get_pretrained_cifar10_network_kwargs()\n\t        if self.use_imagenet:\n\t            network_kwargs = self.get_pretrained_imagenet_network_kwargs()\n\t        label_dim = self.num_classes if self.use_imagenet else 0\n\t        interface_kwargs = dict(img_resolution=res, img_channels=3, label_dim=label_dim)\n\t        model = dnnlib.util.construct_class_by_name(**network_kwargs, **interface_kwargs)\n\t        model.train().requires_grad_(True)\n\t        misc.copy_params_and_buffers(src_module=pretrained, dst_module=model, require_all=False)\n\t        del pretrained      # save memory\n\t        self.time_schedule = tuple(int(x) for x in self.params.time_schedule.split(','))\n", "        steps_per_phase = int_str(FLAGS.train_len) / (FLAGS.batch * (len(self.time_schedule) - 1))\n\t        ema = self.params.ema_residual ** (1 / steps_per_phase)\n\t        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - ema))\n\t        model.apply(functools.partial(lib.nn.functional.set_dropout, p=self.params.dropout))\n\t        self.model = lib.distributed.wrap(model)\n\t        self.model_eval = lib.optim.ModuleEMA(model, momentum=ema).eval().requires_grad_(False).to(device_id())\n\t        lib.distributed.barrier()\n\t        # Disable dropout noise for teacher\n\t        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n\t        self.self_teacher = lib.optim.ModuleEMA(model, momentum=self.params.sema).to(device_id())\n", "        self.self_teacher.eval().requires_grad_(False)\n\t        self.teacher = copy.deepcopy(model).to(device_id())\n\t        self.teacher.eval().requires_grad_(False)\n\t        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr, weight_decay=0.0)\n\t        # Setup noise schedule\n\t        sigma = torch.linspace(self.SIGMA_MIN ** (1 / self.RHO),\n\t                               self.SIGMA_MAX ** (1 / self.RHO), timesteps, dtype=torch.double).pow(self.RHO)\n\t        sigma = torch.cat([torch.zeros_like(sigma[:1]), sigma])\n\t        self.register_buffer('sigma', sigma.to(device()))\n\t        self.timesteps = timesteps\n", "    def get_pretrained_cifar10_network_kwargs(self):\n\t        network_kwargs = dnnlib.EasyDict()\n\t        network_kwargs.update(model_type='SongUNet', embedding_type='fourier', encoder_type='residual', decoder_type='standard')\n\t        network_kwargs.update(channel_mult_noise=2, resample_filter=[1,3,3,1], model_channels=128, channel_mult=[2,2,2])\n\t        network_kwargs.class_name = 'training.networks.EDMPrecond'\n\t        network_kwargs.augment_dim = 0\n\t        network_kwargs.update(dropout=0.0, use_fp16=False)\n\t        return network_kwargs\n\t    def get_pretrained_imagenet_network_kwargs(self):\n\t        network_kwargs = dnnlib.EasyDict()\n", "        network_kwargs.update(model_type='DhariwalUNet', model_channels=192, channel_mult=[1,2,3,4])\n\t        network_kwargs.class_name = 'training.networks.EDMPrecond'\n\t        network_kwargs.update(dropout=0.0, use_fp16=False)\n\t        return network_kwargs\n\t    @classmethod\n\t    def c_in(cls, sigma: torch.Tensor) -> torch.Tensor:\n\t        return (sigma ** 2 + cls.SIGMA_DATA ** 2) ** -0.5\n\t    @classmethod\n\t    def c_skip(cls, sigma: torch.Tensor) -> torch.Tensor:\n\t        return (cls.SIGMA_DATA ** 2) / (sigma ** 2 + cls.SIGMA_DATA ** 2)\n", "    @classmethod\n\t    def c_out(cls, sigma: torch.Tensor) -> torch.Tensor:\n\t        return sigma * cls.SIGMA_DATA * (cls.SIGMA_DATA ** 2 + sigma ** 2) ** -0.5\n\t    @staticmethod\n\t    def c_noise(sigma: torch.Tensor) -> torch.Tensor:\n\t        return 0.25 * sigma.clamp(1e-20).log()\n\t    def forward(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n\t        step = self.timesteps // self.time_schedule[1]\n\t        shape = n, self.COLORS, self.params.res, self.params.res\n\t        xt = self.sigma[-1] * torch.randn(shape, generator=generator, dtype=torch.double).to(device())\n", "        class_labels = (torch.eye(self.num_classes, device=device())[torch.randint(self.num_classes, size=[n], device=device())]) if self.use_imagenet else None\n\t        for t in reversed(range(0, self.timesteps, step)):\n\t            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n\t            g = tuple(self.sigma[i].view(-1, 1, 1, 1) for i in ix)\n\t            x0 = self.model_eval(xt, g[0], class_labels).to(torch.float64)\n\t            xt = self.post_xt_x0(xt, x0, g[0], g[1])\n\t        return xt.clamp(-1, 1).float()\n\t    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, sigma: torch.Tensor, sigma1: torch.Tensor) -> torch.Tensor:\n\t        x0 = torch.clip(out, -1., 1.)\n\t        eps = (xt - x0) / sigma\n", "        return torch.nan_to_num(x0 + eps * sigma1)\n\t    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n\t        if self.num_classes == 1000:    # imagenet\n\t            y = torch.nn.functional.one_hot(y, self.num_classes).to(y.device)\n\t        else:\n\t            y = None\n\t        with torch.no_grad():\n\t            step = self.timesteps // self.time_schedule[1]\n\t            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n\t            semi_index = torch.randint(step, index.shape, device=device())\n", "            ix = index - semi_index, (index - semi_index - 1).clamp(1), index - step\n\t            s = tuple(self.sigma[i].view(-1, 1, 1, 1) for i in ix)\n\t            noise = torch.randn_like(x).to(device())\n\t            # RK step from teacher\n\t            xt = x.double() + noise * s[0]\n\t            x0 = self.teacher(xt, s[0], y)\n\t            eps = (xt - x0) / s[0]\n\t            xt_ = xt + (s[1] - s[0]) * eps\n\t            x0_ = self.teacher(xt_, s[1], y)\n\t            eps = .5 * (eps + (xt_ - x0_) / s[1])\n", "            xt_ = xt + (s[1] - s[0]) * eps      # RK target from teacher; no RK needed for sigma_min\n\t            # self-teacher step\n\t            xt2 = self.post_xt_x0(xt_, self.self_teacher(xt_, s[1], y), s[1], s[2])\n\t            xt2 += ((semi_index + 1) == step).view(-1, 1, 1, 1) * (xt_ - xt2)   # Only propagate inside phase semi_range\n\t            xt2 = ((xt2 * s[0] - xt * s[2]) / (s[0] - s[2]))\n\t            # Boundary and terminal condition: last time step, no RK and self-teaching needed\n\t            target_without_precon = torch.where((index - semi_index - 1).view(-1, 1, 1, 1) == 0, x0.double(), xt2.double())\n\t            target = (target_without_precon - self.c_skip(s[0]) * xt) / self.c_out(s[0])\n\t        self.opt.zero_grad(set_to_none=True)\n\t        pred = self.model(xt.float(), s[0].float(), y).double()\n", "        pred = (pred - self.c_skip(s[0]) * xt) / self.c_out(s[0])\n\t        weight = (s[0] ** 2 + self.SIGMA_DATA ** 2) * (self.c_out(s[0]) ** 2) * (s[0] * self.SIGMA_DATA) ** -2\n\t        loss = (torch.nn.functional.mse_loss(pred.float(), target.float(), reduction='none')).mean((1, 2, 3))\n\t        loss = (weight.float() * loss).mean()\n\t        loss.backward()\n\t        # LR warmup and clip gradient like EDM paper\n\t        if self.params.lr_warmup is not None:\n\t            for g in self.opt.param_groups:\n\t                g['lr'] = self.params.lr * min(info.samples / max(int_str(self.params.lr_warmup), 1e-8), 1)\n\t        for param in self.model.parameters():\n", "            if param.grad is not None:\n\t                torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)\n\t        self.opt.step()\n\t        self.self_teacher.update(self.model)\n\t        self.model_eval.update(self.model)\n\t        return {'loss/global': loss}\n\tdef check_steps():\n\t    timesteps = [int(x) for x in FLAGS.time_schedule.split(',')]\n\t    assert len(timesteps) > 1\n\t    assert timesteps[0] == FLAGS.timesteps\n", "    for i in range(len(timesteps) - 1):\n\t        assert timesteps[i + 1] < timesteps[i]\n\t@lib.distributed.auto_distribute\n\tdef main(_):\n\t    check_steps()\n\t    data = lib.data.DATASETS[FLAGS.dataset]()\n\t    lib.distributed.barrier()\n\t    model = EluDDIM05TCMultiStepx0(data.res, FLAGS.timesteps, batch=FLAGS.batch, lr=FLAGS.lr,\n\t                                   ema_residual=FLAGS.ema_residual, sema=FLAGS.sema, lr_warmup=FLAGS.lr_warmup,\n\t                                   aug_prob=FLAGS.aug_prob, dropout=FLAGS.dropout, time_schedule=FLAGS.time_schedule)\n", "    lib.distributed.barrier()\n\t    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n\t    train, fid = data.make_dataloaders()\n\t    model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)\n\tif __name__ == '__main__':\n\t    flags.DEFINE_float('ema_residual', 1e-3, help='Residual for the Exponential Moving Average of model.')\n\t    flags.DEFINE_float('sema', 0.5, help='Exponential Moving Average of self-teacher.')\n\t    flags.DEFINE_float('lr', 1e-3, help='Learning rate.')\n\t    flags.DEFINE_string('lr_warmup', None, help='Warmup for LR in num samples, e.g. 4M')\n\t    flags.DEFINE_integer('fid_len', 50000, help='Number of samples for FID evaluation.')\n", "    flags.DEFINE_integer('timesteps', 40, help='Sampling timesteps.')\n\t    flags.DEFINE_string('time_schedule', None, required=True,\n\t                        help='Comma separated distillation timesteps, for example: 36,1.')\n\t    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset. Either cifar10 or imagenet64')\n\t    flags.DEFINE_string('report_len', '1M', help='Reporting interval in samples.')\n\t    flags.DEFINE_string('train_len', '64M', help='Training duration in samples per distillation logstep.')\n\t    flags.DEFINE_float('aug_prob', 0.0, help='Probability of applying data augmentation in training.')\n\t    flags.DEFINE_float('dropout', 0.0, help='Dropout probability for training.')\n\t    flags.FLAGS.set_default('report_img_len', '1M')\n\t    flags.FLAGS.set_default('report_fid_len', '4M')\n", "    app.run(lib.distributed.main(main))\n"]}
{"filename": "binary_distill.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport copy\n\timport functools\n\timport math\n\timport os\n\timport pathlib\n\timport shutil\n", "from typing import Callable, Dict, Optional\n\timport torch\n\timport torch.nn.functional\n\tfrom absl import app, flags\n\timport lib\n\tfrom lib.distributed import device, device_id, print\n\tfrom lib.util import FLAGS\n\tfrom lib.zoo.unet import UNet\n\tdef get_model(name: str):\n\t    if name == 'cifar10':\n", "        net = UNet(in_channel=3,\n\t                   channel=256,\n\t                   emb_channel=1024,\n\t                   channel_multiplier=[1, 1, 1],\n\t                   n_res_blocks=3,\n\t                   attn_rezs=[8, 16],\n\t                   attn_heads=1,\n\t                   head_dim=None,\n\t                   use_affine_time=True,\n\t                   dropout=0.2,\n", "                   num_output=1,\n\t                   resample=True,\n\t                   num_classes=1)\n\t    elif name == 'imagenet64':\n\t        # imagenet model is class conditional\n\t        net = UNet(in_channel=3,\n\t                   channel=192,\n\t                   emb_channel=768,\n\t                   channel_multiplier=[1, 2, 3, 4],\n\t                   n_res_blocks=3,\n", "                   init_rez=64,\n\t                   attn_rezs=[8, 16, 32],\n\t                   attn_heads=None,\n\t                   head_dim=64,\n\t                   use_affine_time=True,\n\t                   dropout=0.,\n\t                   num_output=2,  # predict signal and noise\n\t                   resample=True,\n\t                   num_classes=1000)\n\t    else:\n", "        raise NotImplementedError(name)\n\t    return net\n\tclass BinaryDistillGoogleModel(lib.train.TrainModel):\n\t    R_NONE, R_STEP, R_PHASE = 'none', 'step', 'phase'\n\t    R_ALL = R_NONE, R_STEP, R_PHASE\n\t    def __init__(self, name: str, res: int, timesteps: int, **params):\n\t        super().__init__(\"GoogleUNet\", res, timesteps, **params)\n\t        self.num_classes = 1\n\t        self.shape = 3, res, res\n\t        self.timesteps = timesteps\n", "        model = get_model(name)\n\t        if 'cifar' in name:\n\t            self.ckpt_path = 'ckpts/cifar_original.pt'\n\t            self.predict_both = False\n\t        elif 'imagenet' in name:\n\t            self.ckpt_path = 'ckpts/imagenet_original.pt'\n\t            self.predict_both = False\n\t        elif 'imagenet' in name:\n\t            self.ckpt_path = 'ckpts/imagenet_original.pt'\n\t            self.num_classes = 1000\n", "            self.predict_both = True\n\t            self.EVAL_COLUMNS = self.EVAL_ROWS = 8\n\t        else:\n\t            raise NotImplementedError(name)\n\t        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - self.params.ema))\n\t        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n\t        self.model = lib.distributed.wrap(model)\n\t        self.model_eval = lib.optim.ModuleEMA(model, momentum=self.params.ema).to(device_id())\n\t        self.teacher = copy.deepcopy(model).to(device_id())\n\t        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr)\n", "        self.register_buffer('phase', torch.zeros((), dtype=torch.long))\n\t        self.cur_step = self.timesteps // 2\n\t    def initialize_weights_from_teacher(self, logdir: pathlib.Path, teacher_ckpt: Optional[str] = None):\n\t        teacher_ckpt_path = logdir / 'ckpt/teacher.ckpt'\n\t        if device_id() == 0:\n\t            os.makedirs(logdir / 'ckpt', exist_ok=True)\n\t            shutil.copy2(self.ckpt_path, teacher_ckpt_path)\n\t        lib.distributed.barrier()\n\t        self.model.module.load_state_dict(torch.load(teacher_ckpt_path))\n\t        self.model_eval.module.load_state_dict(torch.load(teacher_ckpt_path))\n", "        self.self_teacher.module.load_state_dict(torch.load(teacher_ckpt_path))\n\t        self.teacher.load_state_dict(torch.load(teacher_ckpt_path))\n\t    def randn(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n\t        if generator is not None:\n\t            assert generator.device == torch.device('cpu')\n\t        return torch.randn((n, *self.shape), device='cpu', generator=generator, dtype=torch.double).to(self.device)\n\t    def call_model(self, model: Callable, xt: torch.Tensor, index: torch.Tensor,\n\t                   y: Optional[torch.Tensor] = None) -> torch.Tensor:\n\t        if y is None:\n\t            return model(xt.float(), index.float()).double()\n", "        else:\n\t            return model(xt.float(), index.float(), y.long()).double()\n\t    def forward(self, samples: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n\t        step = self.timesteps // self.cur_step\n\t        xt = self.randn(samples, generator).to(device_id())\n\t        if self.num_classes > 1:\n\t            y = torch.randint(0, self.num_classes, (samples,)).to(xt)\n\t        else:\n\t            y = None\n\t        for t in reversed(range(0, self.timesteps, step)):\n", "            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n\t            logsnr = tuple(self.logsnr_schedule_cosine(i / self.timesteps).to(xt.double()) for i in ix)\n\t            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n\t            x0 = self.call_model(self.model_eval, xt, logsnr[0].repeat(xt.shape[0]), y)\n\t            xt = self.post_xt_x0(xt, x0, g[0], g[1], clip_x=True)\n\t        return xt\n\t    @staticmethod\n\t    def logsnr_schedule_cosine(t, logsnr_min=torch.Tensor([-20.]), logsnr_max=torch.Tensor([20.])):\n\t        b = torch.arctan(torch.exp(-0.5 * logsnr_max)).to(t)\n\t        a = torch.arctan(torch.exp(-0.5 * logsnr_min)).to(t) - b\n", "        return -2. * torch.log(torch.tan(a * t + b))\n\t    @staticmethod\n\t    def predict_eps_from_x(z, x, logsnr):\n\t        \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n\t        assert logsnr.ndim == x.ndim\n\t        return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))\n\t    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, g: torch.Tensor, g1: torch.Tensor, clip_x=False) -> torch.Tensor:\n\t        if self.predict_both:\n\t            assert out.shape[1] == 6\n\t            model_x, model_eps = out[:, :3], out[:, 3:]\n", "            # reconcile the two predictions\n\t            model_x_eps = (xt - model_eps * (1 - g).sqrt()) * g.rsqrt()\n\t            wx = 1 - g\n\t            x0 = wx * model_x + (1. - wx) * model_x_eps\n\t        else:\n\t            x0 = out\n\t        if clip_x:\n\t            x0 = torch.clip(x0, -1., 1.)\n\t        eps = (xt - x0 * g.sqrt()) * (1 - g).rsqrt()\n\t        return torch.nan_to_num(x0 * g1.sqrt() + eps * (1 - g1).sqrt())\n", "    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n\t        if self.num_classes == 1:\n\t            y = None\n\t        else:\n\t            y = y[:, 0]\n\t        with torch.no_grad():\n\t            phase = int(info.progress * (1 - 1e-9) * math.log(self.timesteps, 2))\n\t            if phase != self.phase:\n\t                print(f'Refreshing teacher {phase}')\n\t                self.phase.add_(1)\n", "                self.teacher.load_state_dict(self.model_eval.module.state_dict())\n\t                if self.params.reset == self.R_PHASE:\n\t                    self.model_eval.step.mul_(0)\n\t                self.cur_step = self.cur_step // 2\n\t                assert self.cur_step >= 1\n\t            step = self.timesteps // self.cur_step\n\t            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n\t            ix = index, index - step // 2, index - step\n\t            logsnr = tuple(self.logsnr_schedule_cosine(i.double() / self.timesteps).to(x.double()) for i in ix)\n\t            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n", "            noise = torch.randn_like(x)\n\t            xt0 = x.double() * g[0].sqrt() + noise * (1 - g[0]).sqrt()\n\t            xt1 = self.post_xt_x0(xt0, self.call_model(self.teacher, xt0, logsnr[0], y), g[0], g[1])\n\t            x_hat = self.call_model(self.teacher, xt1, logsnr[1], y)\n\t            xt2 = self.post_xt_x0(xt1, x_hat, g[1], g[2])\n\t            # Find target such that self.post_xt_x0(xt0, target, g[0], g[2]) == xt2\n\t            target = ((xt0 * (1 - g[2]).sqrt() - xt2 * (1 - g[0]).sqrt()) /\n\t                      ((g[0] * (1 - g[2])).sqrt() - (g[2] * (1 - g[0])).sqrt()))\n\t            # use predicted x0 as target when t=0\n\t            target += (index == step).view(-1, 1, 1, 1) * (x_hat[:, :3] - target)\n", "        self.opt.zero_grad(set_to_none=True)\n\t        pred = self.call_model(self.model, xt0, logsnr[0], y)\n\t        if self.predict_both:\n\t            assert pred.shape[1] == 6\n\t            model_x, model_eps = pred[:, :3], pred[:, 3:]\n\t            # reconcile the two predictions\n\t            model_x_eps = (xt0 - model_eps * (1 - g[0]).sqrt()) * g[0].rsqrt()\n\t            wx = 1 - g[0]\n\t            pred_x = wx * model_x + (1. - wx) * model_x_eps\n\t        else:\n", "            pred_x = pred\n\t        loss = ((g[0] / (1 - g[0])).clamp(1) * (pred_x - target.detach()).square()).mean(0).sum()\n\t        loss.backward()\n\t        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n\t        self.opt.step()\n\t        self.model_eval.update(self.model)\n\t        return {'loss/global': loss, 'stat/timestep': self.cur_step}\n\t@ lib.distributed.auto_distribute\n\tdef main(_):\n\t    data = lib.data.DATASETS[FLAGS.dataset]()\n", "    model = BinaryDistillGoogleModel(FLAGS.dataset, data.res, FLAGS.timesteps, reset=FLAGS.reset,\n\t                                     batch=FLAGS.batch, lr=FLAGS.lr, ema=FLAGS.ema)\n\t    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n\t    # resume from previous run (ckpt will be loaded in train.py)\n\t    if FLAGS.restart_ckpt:\n\t        lib.distributed.barrier()\n\t    if FLAGS.eval:\n\t        if not FLAGS.restart_ckpt:\n\t            model.initialize_weights_from_teacher(logdir, FLAGS.teacher_ckpt)\n\t        model.eval()\n", "        with torch.no_grad():\n\t            generator = torch.Generator(device='cpu')\n\t            generator.manual_seed(123623113456)\n\t            model(4, generator)\n\t    else:\n\t        train, fid = data.make_dataloaders()\n\t        if not FLAGS.restart_ckpt:\n\t            model.initialize_weights_from_teacher(logdir, FLAGS.teacher_ckpt)\n\t        model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)\n\tif __name__ == '__main__':\n", "    flags.DEFINE_bool('eval', False, help='Whether to run model evaluation.')\n\t    flags.DEFINE_enum('reset', BinaryDistillGoogleModel.R_NONE, BinaryDistillGoogleModel.R_ALL, help='EMA reset mode.')\n\t    flags.DEFINE_float('ema', 0.9995, help='Exponential Moving Average of model.')\n\t    flags.DEFINE_float('lr', 2e-4, help='Learning rate.')\n\t    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n\t    flags.DEFINE_integer('timesteps', 1024, help='Sampling timesteps.')\n\t    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n\t    flags.DEFINE_string('train_len', '64M', help='Training duration in samples per distillation logstep.')\n\t    flags.DEFINE_string('report_len', '1M', help='Reporting interval in samples.')\n\t    flags.FLAGS.set_default('report_img_len', '1M')\n", "    flags.FLAGS.set_default('report_fid_len', '4M')\n\t    flags.DEFINE_string('restart_ckpt', None,\n\t                        help='Trainer checkpoint in the form <taskid>:<ckpt_path> with <ckpt_path> of the form \"ckpt/*.pth\" .')\n\t    flags.DEFINE_string('teacher_ckpt', None,\n\t                        help='Teacher checkpoint in the form <taskid>:<ckpt_path> with <ckpt_path> of the form \"ckpt/model_*.ckpt\".')\n\t    app.run(lib.distributed.main(main))\n"]}
{"filename": "tc_distill.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport copy\n\timport functools\n\timport os\n\timport pathlib\n\timport shutil\n\tfrom typing import Callable, Dict, Optional\n", "import torch\n\timport torch.nn.functional\n\tfrom absl import app, flags\n\timport lib\n\tfrom lib.distributed import device, device_id, print\n\tfrom lib.util import FLAGS, int_str\n\tfrom lib.zoo.unet import UNet\n\tdef get_model(name: str):\n\t    if name == 'cifar10':\n\t        net = UNet(in_channel=3,\n", "                   channel=256,\n\t                   emb_channel=1024,\n\t                   channel_multiplier=[1, 1, 1],\n\t                   n_res_blocks=3,\n\t                   attn_rezs=[8, 16],\n\t                   attn_heads=1,\n\t                   head_dim=None,\n\t                   use_affine_time=True,\n\t                   dropout=0.2,\n\t                   num_output=1,\n", "                   resample=True,\n\t                   num_classes=1)\n\t    elif name == 'imagenet64':\n\t        # imagenet model is class conditional\n\t        net = UNet(in_channel=3,\n\t                   channel=192,\n\t                   emb_channel=768,\n\t                   channel_multiplier=[1, 2, 3, 4],\n\t                   n_res_blocks=3,\n\t                   init_rez=64,\n", "                   attn_rezs=[8, 16, 32],\n\t                   attn_heads=None,\n\t                   head_dim=64,\n\t                   use_affine_time=True,\n\t                   dropout=0.,\n\t                   num_output=2,  # predict signal and noise\n\t                   resample=True,\n\t                   num_classes=1000)\n\t    else:\n\t        raise NotImplementedError(name)\n", "    return net\n\tclass TCDistillGoogleModel(lib.train.TrainModel):\n\t    R_NONE, R_STEP, R_PHASE = 'none', 'step', 'phase'\n\t    R_ALL = R_NONE, R_STEP, R_PHASE\n\t    def __init__(self, name: str, res: int, timesteps: int, **params):\n\t        super().__init__(\"GoogleUNet\", res, timesteps, **params)\n\t        self.num_classes = 1\n\t        self.shape = 3, res, res\n\t        self.timesteps = timesteps\n\t        model = get_model(name)\n", "        if 'cifar' in name:\n\t            self.ckpt_path = 'ckpts/cifar_original.pt'\n\t            self.predict_both = False\n\t        elif 'imagenet' in name:\n\t            self.ckpt_path = 'ckpts/imagenet_original.pt'\n\t            self.num_classes = 1000\n\t            self.predict_both = True\n\t            self.EVAL_COLUMNS = self.EVAL_ROWS = 8\n\t        else:\n\t            raise NotImplementedError(name)\n", "        self.time_schedule = tuple(int(x) for x in self.params.time_schedule.split(','))\n\t        steps_per_phase = int_str(FLAGS.train_len) / (FLAGS.batch * (len(self.time_schedule) - 1))\n\t        ema = self.params.ema_residual ** (1 / steps_per_phase)\n\t        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - ema))\n\t        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n\t        self.model = lib.distributed.wrap(model)\n\t        self.model_eval = lib.optim.ModuleEMA(model, momentum=ema).to(device_id())\n\t        self.self_teacher = lib.optim.ModuleEMA(model, momentum=self.params.sema).to(device_id())\n\t        self.teacher = copy.deepcopy(model).to(device_id())\n\t        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr)\n", "        self.register_buffer('phase', torch.zeros((), dtype=torch.long))\n\t    def initialize_weights_from_teacher(self, logdir: pathlib.Path):\n\t        teacher_ckpt_path = logdir / 'ckpt/teacher.ckpt'\n\t        if device_id() == 0:\n\t            os.makedirs(logdir / 'ckpt', exist_ok=True)\n\t            shutil.copy2(self.ckpt_path, teacher_ckpt_path)\n\t        lib.distributed.barrier()\n\t        self.model.module.load_state_dict(torch.load(teacher_ckpt_path))\n\t        self.model_eval.module.load_state_dict(torch.load(teacher_ckpt_path))\n\t        self.self_teacher.module.load_state_dict(torch.load(teacher_ckpt_path))\n", "        self.teacher.load_state_dict(torch.load(teacher_ckpt_path))\n\t    def randn(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n\t        if generator is not None:\n\t            assert generator.device == torch.device('cpu')\n\t        return torch.randn((n, *self.shape), device='cpu', generator=generator, dtype=torch.double).to(self.device)\n\t    def call_model(self, model: Callable, xt: torch.Tensor, index: torch.Tensor,\n\t                   y: Optional[torch.Tensor] = None) -> torch.Tensor:\n\t        if y is None:\n\t            return model(xt.float(), index.float()).double()\n\t        else:\n", "            return model(xt.float(), index.float(), y.long()).double()\n\t    def forward(self, samples: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n\t        step = self.timesteps // self.time_schedule[self.phase.item() + 1]\n\t        xt = self.randn(samples, generator).to(device_id())\n\t        if self.num_classes > 1:\n\t            y = torch.randint(0, self.num_classes, (samples,)).to(xt)\n\t        else:\n\t            y = None\n\t        for t in reversed(range(0, self.timesteps, step)):\n\t            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n", "            logsnr = tuple(self.logsnr_schedule_cosine(i / self.timesteps).to(xt.double()) for i in ix)\n\t            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n\t            x0 = self.call_model(self.model_eval, xt, logsnr[0].repeat(xt.shape[0]), y)\n\t            xt = self.post_xt_x0(xt, x0, g[0], g[1])\n\t        return xt\n\t    @staticmethod\n\t    def logsnr_schedule_cosine(t, logsnr_min=torch.Tensor([-20.]), logsnr_max=torch.Tensor([20.])):\n\t        b = torch.arctan(torch.exp(-0.5 * logsnr_max)).to(t)\n\t        a = torch.arctan(torch.exp(-0.5 * logsnr_min)).to(t) - b\n\t        return -2. * torch.log(torch.tan(a * t + b))\n", "    @staticmethod\n\t    def predict_eps_from_x(z, x, logsnr):\n\t        \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n\t        assert logsnr.ndim == x.ndim\n\t        return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))\n\t    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, g: torch.Tensor, g1: torch.Tensor) -> torch.Tensor:\n\t        if self.predict_both:\n\t            assert out.shape[1] == 6\n\t            model_x, model_eps = out[:, :3], out[:, 3:]\n\t            # reconcile the two predictions\n", "            model_x_eps = (xt - model_eps * (1 - g).sqrt()) * g.rsqrt()\n\t            wx = 1 - g\n\t            x0 = wx * model_x + (1. - wx) * model_x_eps\n\t        else:\n\t            x0 = out\n\t        x0 = torch.clip(x0, -1., 1.)\n\t        eps = (xt - x0 * g.sqrt()) * (1 - g).rsqrt()\n\t        return torch.nan_to_num(x0 * g1.sqrt() + eps * (1 - g1).sqrt())\n\t    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n\t        if self.num_classes == 1:\n", "            y = None\n\t        with torch.no_grad():\n\t            phase = int(info.progress * (1 - 1e-9) * (len(self.time_schedule) - 1))\n\t            if phase != self.phase:\n\t                print(f'Refreshing teacher {phase}')\n\t                self.phase.add_(1)\n\t                self.teacher.load_state_dict(self.model_eval.module.state_dict())\n\t                if self.params.reset == self.R_PHASE:\n\t                    self.model_eval.step.mul_(0)\n\t            semi_range = self.time_schedule[phase] // self.time_schedule[phase + 1]\n", "            semi = self.timesteps // self.time_schedule[phase]\n\t            step = self.timesteps // self.time_schedule[phase + 1]\n\t            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n\t            semi_index = torch.randint(semi_range, index.shape, device=device()) * semi\n\t            ix = index - semi_index, index - semi_index - semi, index - step\n\t            logsnr = tuple(self.logsnr_schedule_cosine(i.double() / self.timesteps).to(x.double()) for i in ix)\n\t            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n\t            noise = torch.randn_like(x)\n\t            xt0 = x.double() * g[0].sqrt() + noise * (1 - g[0]).sqrt()\n\t            xt1 = self.post_xt_x0(xt0, self.call_model(self.teacher, xt0, logsnr[0], y), g[0], g[1])\n", "            xt2 = self.post_xt_x0(xt1, self.call_model(self.self_teacher, xt1, logsnr[1], y), g[1], g[2])\n\t            xt2 += (semi_index + semi == step).view(-1, 1, 1, 1) * (xt1 - xt2)  # Only propagate inside phase semi_range\n\t            # Find target such that self.post_xt_x0(xt0, target, g[0], g[2]) == xt2\n\t            target = ((xt0 * (1 - g[2]).sqrt() - xt2 * (1 - g[0]).sqrt()) /\n\t                      ((g[0] * (1 - g[2])).sqrt() - (g[2] * (1 - g[0])).sqrt()))\n\t        self.opt.zero_grad(set_to_none=True)\n\t        pred = self.call_model(self.model, xt0, logsnr[0], y)\n\t        if self.predict_both:\n\t            assert pred.shape[1] == 6\n\t            model_x, model_eps = pred[:, :3], pred[:, 3:]\n", "            # reconcile the two predictions\n\t            model_x_eps = (xt0 - model_eps * (1 - g[0]).sqrt()) * g[0].rsqrt()\n\t            wx = 1 - g[0]\n\t            pred_x = wx * model_x + (1. - wx) * model_x_eps\n\t        else:\n\t            pred_x = pred\n\t        loss = ((g[0] / (1 - g[0])).clamp(1) * (pred_x - target.detach()).square()).mean(0).sum()\n\t        loss.backward()\n\t        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n\t        self.opt.step()\n", "        self.self_teacher.update(self.model)\n\t        self.model_eval.update(self.model)\n\t        return {'loss/global': loss, 'stat/timestep': self.time_schedule[phase + 1]}\n\tdef check_steps():\n\t    timesteps = [int(x) for x in FLAGS.time_schedule.split(',')]\n\t    assert len(timesteps) > 1\n\t    for i in range(len(timesteps) - 1):\n\t        assert timesteps[i + 1] < timesteps[i]\n\t@lib.distributed.auto_distribute\n\tdef main(_):\n", "    check_steps()\n\t    data = lib.data.DATASETS[FLAGS.dataset]()\n\t    model = TCDistillGoogleModel(FLAGS.dataset, data.res, FLAGS.timesteps, reset=FLAGS.reset,\n\t                                 batch=FLAGS.batch, lr=FLAGS.lr, ema_residual=FLAGS.ema_residual,\n\t                                 sema=FLAGS.sema, time_schedule=FLAGS.time_schedule)\n\t    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n\t    train, fid = data.make_dataloaders()\n\t    model.initialize_weights_from_teacher(logdir)\n\t    model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)\n\tif __name__ == '__main__':\n", "    flags.DEFINE_enum('reset', TCDistillGoogleModel.R_NONE, TCDistillGoogleModel.R_ALL, help='EMA reset mode.')\n\t    flags.DEFINE_float('ema_residual', 1e-3, help='Residual for the Exponential Moving Average of model.')\n\t    flags.DEFINE_float('sema', 0.5, help='Exponential Moving Average of self-teacher.')\n\t    flags.DEFINE_float('lr', 2e-4, help='Learning rate.')\n\t    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n\t    flags.DEFINE_integer('timesteps', 1024, help='Sampling timesteps.')\n\t    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n\t    flags.DEFINE_string('time_schedule', None, required=True,\n\t                        help='Comma separated distillation timesteps, for example: 1024,32,1.')\n\t    flags.DEFINE_string('train_len', '64M', help='Training duration in samples per distillation logstep.')\n", "    flags.DEFINE_string('report_len', '1M', help='Reporting interval in samples.')\n\t    flags.FLAGS.set_default('report_img_len', '1M')\n\t    flags.FLAGS.set_default('report_fid_len', '4M')\n\t    app.run(lib.distributed.main(main))\n"]}
{"filename": "teacher/download_and_convert_jax.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport os\n\timport pathlib\n\tfrom flax import serialization\n\tfrom tensorflow.compat.v2.io import gfile\n\tfrom lib.zoo.unet import UNet\n\timport numpy as np\n", "import torch\n\timport einops as ei\n\tfrom absl import app, flags\n\tdef to_torch(x):\n\t    return torch.nn.Parameter(torch.from_numpy(x.copy()))\n\tdef check_and_convert_gcs_filepath(filepath, raise_if_not_gcs=False):\n\t    \"\"\"Utility for loading model checkpoints from GCS.\"\"\"\n\t    local_filepath = filepath.split('/')[-1]\n\t    if os.path.exists(local_filepath):\n\t        print('loading from local copy of GCS file: ' + local_filepath)\n", "    else:\n\t        print('downloading file from GCS: ' + filepath)\n\t        os.system('gsutil cp ' + filepath + ' ' + local_filepath)\n\t    return local_filepath\n\tdef restore_from_path(ckpt_path, target):\n\t    ckpt_path = check_and_convert_gcs_filepath(ckpt_path)\n\t    with gfile.GFile(ckpt_path, 'rb') as fp:\n\t        return serialization.from_bytes(target, fp.read())\n\tdef convert_conv(module_from, module_to):\n\t    # PyTorch kernel has shape [outC, inC, kH, kW] and the Flax kernel has shape [kH, kW, inC, outC]\n", "    module_to.weight = to_torch(module_from['kernel'].transpose(3, 2, 0, 1))\n\t    module_to.bias = to_torch(module_from['bias'])\n\tdef convert_conv_after_qkv(module_from, module_to):\n\t    module_to.weight = to_torch(ei.rearrange(module_from['kernel'], \"nh h f -> f (nh h) 1 1\"))\n\t    module_to.bias = to_torch(module_from['bias'])\n\tdef convert_fc(module_from, module_to):\n\t    # PyTorch kernel has shape [outC, inC] and the Flax kernel has shape [inC, outC]\n\t    module_to.weight = to_torch(module_from['kernel'].transpose(1, 0))\n\t    module_to.bias = to_torch(module_from['bias'])\n\tdef convert_group_norm(module_from, module_to):\n", "    module_to.weight = to_torch(module_from['scale'])\n\t    module_to.bias = to_torch(module_from['bias'])\n\tdef convert_qkv(module_from_q, module_from_k, module_from_v, module_to):\n\t    weight = np.concatenate((module_from_q['kernel'], module_from_k['kernel'], module_from_v['kernel']), 2)\n\t    module_to.weight = to_torch(ei.rearrange(weight, 'f nh h -> (nh h) f 1 1'))\n\t    bias = np.concatenate((module_from_q['bias'], module_from_k['bias'], module_from_v['bias']), 1)\n\t    module_to.bias = to_torch(ei.rearrange(bias, 'nh h -> (nh h)'))\n\tdef convert1x1conv(module_from, module_to):\n\t    module_to.weight = to_torch(module_from['kernel'].transpose(1, 0)[:, :, None, None])\n\t    module_to.bias = to_torch(module_from['bias'])\n", "def convert_res_block(module_from, module_to):\n\t    convert_group_norm(module_from['norm1'], module_to.norm1)\n\t    convert_conv(module_from['conv1'], module_to.conv1)\n\t    convert_fc(module_from['temb_proj'], module_to.time[1])\n\t    convert_group_norm(module_from['norm2'], module_to.norm2)\n\t    convert_conv(module_from['conv2'], module_to.conv2)\n\t    if 'nin_shortcut' in module_from:\n\t        convert1x1conv(module_from['nin_shortcut'], module_to.skip)\n\tdef convert_attention(module_from, module_to):\n\t    convert_group_norm(module_from['norm'], module_to.norm)\n", "    convert_qkv(module_from['q'], module_from['k'], module_from['v'], module_to.qkv)\n\t    convert_conv_after_qkv(module_from['proj_out'], module_to.out)\n\tdef convert_down(module_from, module_to, n_down_blocks, n_res_blocks):\n\t    convert_conv(module_from['conv_in'], module_to[0])\n\t    module_to_idx = 1\n\t    for i in range(n_down_blocks):\n\t        for j in range(n_res_blocks):\n\t            convert_res_block(module_from[f'down_{i}.block_{j}'], module_to[module_to_idx].resblocks)\n\t            if f'down_{i}.attn_{j}' in module_from.keys():\n\t                convert_attention(module_from[f'down_{i}.attn_{j}'], module_to[module_to_idx].attention)\n", "            module_to_idx += 1\n\t        # downsample layer is a res block\n\t        if f'down_{i}.downsample' in module_from.keys():\n\t            convert_res_block(module_from[f'down_{i}.downsample'], module_to[module_to_idx])\n\t            module_to_idx += 1\n\t    assert module_to_idx == len(module_to)\n\tdef convert_mid(module_from, module_to):\n\t    convert_res_block(module_from['mid.block_1'], module_to[0].resblocks)\n\t    convert_attention(module_from['mid.attn_1'], module_to[0].attention)\n\t    convert_res_block(module_from['mid.block_2'], module_to[1].resblocks)\n", "def convert_up(module_from, module_to, num_up_blocks, n_res_blocks):\n\t    module_to_idx = 0\n\t    for i in reversed(range(num_up_blocks)):\n\t        for j in range(n_res_blocks + 1):\n\t            convert_res_block(module_from[f'up_{i}.block_{j}'], module_to[module_to_idx].resblocks)\n\t            if f'up_{i}.attn_{j}' in module_from.keys():\n\t                convert_attention(module_from[f'up_{i}.attn_{j}'], module_to[module_to_idx].attention)\n\t            module_to_idx += 1\n\t        # upsample layer is a res block\n\t        if f'up_{i}.upsample' in module_from.keys():\n", "            convert_res_block(module_from[f'up_{i}.upsample'], module_to[module_to_idx])\n\t            module_to_idx += 1\n\t    assert module_to_idx == len(module_to)\n\tdef convert_out(module_from, module_to):\n\t    convert_group_norm(module_from['norm_out'], module_to[0])\n\t    convert_conv(module_from['conv_out'], module_to[2])\n\tdef convert_time(module_from, module_to):\n\t    convert_fc(module_from['dense0'], module_to[0])\n\t    convert_fc(module_from['dense1'], module_to[2])\n\tdef convert_class(module_from, module_to):\n", "    convert_fc(module_from['class_emb'], module_to)\n\tdef convert(module_from, module_to, n_down_blocks, n_up_blocks, n_res_blocks, class_conditional=False):\n\t    # downsample\n\t    convert_down(module_from['ema_params'], module_to.down, n_down_blocks, n_res_blocks)\n\t    # mid\n\t    convert_mid(module_from['ema_params'], module_to.mid)\n\t    # up\n\t    convert_up(module_from['ema_params'], module_to.up, n_up_blocks, n_res_blocks)\n\t    # out\n\t    convert_out(module_from['ema_params'], module_to.out)\n", "    # time\n\t    convert_time(module_from['ema_params'], module_to.time)\n\t    # class\n\t    if class_conditional:\n\t        convert_class(module_from['ema_params'], module_to.class_emb)\n\tdef cifar10(path: pathlib.Path):\n\t    ckpt = restore_from_path('gs://gresearch/diffusion-distillation/cifar_original', None)\n\t    net = UNet(in_channel=3,\n\t               channel=256,\n\t               emb_channel=1024,\n", "               channel_multiplier=[1, 1, 1],\n\t               n_res_blocks=3,\n\t               attn_rezs=[8, 16],\n\t               attn_heads=1,\n\t               head_dim=None,\n\t               use_affine_time=True,\n\t               dropout=0.2,\n\t               num_output=1,\n\t               resample=True,\n\t               num_classes=1)\n", "    convert(ckpt, net, n_down_blocks=3, n_up_blocks=3, n_res_blocks=3)\n\t    # save torch checkpoint\n\t    torch.save(net.state_dict(), path / 'cifar_original.pt')\n\t    return net\n\tdef imagenet64_conditional(path: pathlib.Path):\n\t    ckpt = restore_from_path('gs://gresearch/diffusion-distillation/imagenet_original', None)\n\t    net = UNet(in_channel=3,\n\t               channel=192,\n\t               emb_channel=768,\n\t               channel_multiplier=[1, 2, 3, 4],\n", "               n_res_blocks=3,\n\t               init_rez=64,\n\t               attn_rezs=[8, 16, 32],\n\t               attn_heads=None,\n\t               head_dim=64,\n\t               use_affine_time=True,\n\t               dropout=0.,\n\t               num_output=2,  # predict signal and noise\n\t               resample=True,\n\t               num_classes=1000)\n", "    convert(ckpt, net, n_down_blocks=4, n_up_blocks=4, n_res_blocks=3, class_conditional=True)\n\t    # save torch checkpoint\n\t    torch.save(net.state_dict(), path / 'imagenet_original.pt')\n\t    return net\n\tdef main(_):\n\t    path = pathlib.Path(flags.FLAGS.path)\n\t    os.makedirs(path, exist_ok=True)\n\t    imagenet64_conditional(path)\n\t    cifar10(path)\n\tif __name__ == '__main__':\n", "    flags.DEFINE_string('path', './ckpts/', help='Path to save the checkpoints.')\n\t    app.run(main)\n"]}
{"filename": "lib/distributed.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t\"\"\"\n\tSingle machine, multi GPU training support\n\t\"\"\"\n\t__all__ = ['WrapModel', 'auto_distribute', 'barrier', 'device', 'device_id', 'gather_tensor', 'is_master', 'main',\n\t           'print', 'reduce_dict_mean', 'tqdm', 'tqdm_module', 'tqdm_with', 'trange', 'world_size', 'wrap']\n\timport builtins\n", "import contextlib\n\timport functools\n\timport os\n\timport time\n\tfrom types import SimpleNamespace\n\tfrom typing import Callable, Dict, Iterable, Optional\n\timport torch\n\timport torch.distributed\n\timport tqdm as tqdm_module\n\tfrom .util import FLAGS, setup\n", "class WrapModel(torch.nn.Module):\n\t    def __init__(self, m: torch.nn.Module):\n\t        super().__init__()\n\t        self.module = m\n\t    def forward(self, *args, **kwargs):\n\t        return self.module(*args, **kwargs)\n\tdef auto_distribute(f: Callable) -> Callable:\n\t    \"\"\"Automatically make a function distributed\"\"\"\n\t    @functools.wraps(f)\n\t    def wrapped(node_rank: Optional[int], world_size: Optional[int], flag_values: SimpleNamespace, *args):\n", "        if node_rank is None:\n\t            return f(*args)\n\t        setup(quiet=True, flags_values=flag_values)\n\t        os.environ['MASTER_ADDR'] = 'localhost'\n\t        os.environ['MASTER_PORT'] = '12359'\n\t        rank = node_rank\n\t        torch.distributed.init_process_group('nccl', rank=rank, world_size=torch.cuda.device_count())\n\t        time.sleep(1)\n\t        try:\n\t            return f(*args)\n", "        finally:\n\t            torch.distributed.destroy_process_group()\n\t    return wrapped\n\tdef barrier():\n\t    if torch.distributed.is_initialized():\n\t        torch.distributed.barrier()\n\tdef device() -> str:\n\t    return f'cuda:{device_id()}'\n\tdef device_id() -> int:\n\t    if not torch.distributed.is_initialized():\n", "        return 0\n\t    return torch.distributed.get_rank() % 8\n\tdef gather_tensor(x: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"Returns a concatenated tensor from all the devices.\"\"\"\n\t    if not torch.distributed.is_initialized():\n\t        return x\n\t    x_list = [torch.empty_like(x) for _ in range(world_size())]\n\t    torch.distributed.all_gather(x_list, x, async_op=False)\n\t    return torch.cat(x_list, dim=0)\n\tdef is_master() -> bool:\n", "    return device_id() == 0\n\tdef main(main_fn: Callable) -> Callable:\n\t    \"\"\"Main function that automatically handle multiprocessing\"\"\"\n\t    @functools.wraps(main_fn)\n\t    def wrapped(*args):\n\t        setup()\n\t        if torch.cuda.device_count() == 1:\n\t            return main_fn(None, None, FLAGS, *args)\n\t        num_gpus = torch.cuda.device_count()\n\t        torch.multiprocessing.spawn(main_fn, args=(num_gpus, FLAGS, *args), nprocs=num_gpus, join=True)\n", "    return wrapped\n\tdef print(*args, **kwargs):\n\t    if is_master():\n\t        builtins.print(*args, **kwargs)\n\tdef reduce_dict_mean(d: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n\t    \"\"\"Mean reduce the tensor in a dict.\"\"\"\n\t    if not torch.distributed.is_initialized():\n\t        return d\n\t    d = {k: (v if isinstance(v, torch.Tensor) else torch.tensor(v)).to(device_id()) for k, v in d.items()}\n\t    e = {k: [torch.empty_like(v) for _ in range(world_size())] for k, v in d.items()}\n", "    # Ideally we should be using all_reduce, but it mysteriously returns incorrect results for the loss\n\t    [v.wait() for v in [torch.distributed.all_gather(e[k], d[k], async_op=True) for k in d]]\n\t    return {k: sum(v) / len(v) for k, v in e.items()}\n\tdef tqdm(iterable: Iterable, **kwargs) -> Iterable:\n\t    return tqdm_module.tqdm(iterable, **kwargs)\n\tdef tqdm_with(**kwargs) -> Iterable:\n\t    class Noop:\n\t        def update(self, *args, **kwargs):\n\t            pass\n\t    @contextlib.contextmanager\n", "    def noop():\n\t        yield Noop()\n\t    return tqdm_module.tqdm(**kwargs)\n\tdef trange(*args, **kwargs):\n\t    return tqdm_module.trange(*args, **kwargs)\n\tdef rank() -> int:\n\t    if not torch.distributed.is_initialized():\n\t        return 1\n\t    return torch.distributed.get_rank()\n\tdef world_size() -> int:\n", "    if not torch.distributed.is_initialized():\n\t        return 1\n\t    return torch.distributed.get_world_size()\n\tdef wrap(m: torch.nn.Module):\n\t    if not torch.distributed.is_initialized():\n\t        return WrapModel(m.to(device()))\n\t    return torch.nn.parallel.DistributedDataParallel(m.to(device_id()), device_ids=[device_id()])\n"]}
{"filename": "lib/train.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['TrainInfo', 'TrainModel', 'DistillModel']\n\timport dataclasses\n\timport json\n\timport pathlib\n\timport time\n\tfrom types import SimpleNamespace\n", "from typing import Callable, Dict, Iterable, List, Optional\n\timport torch.distributed\n\timport torch.nn.functional\n\tfrom absl import flags\n\tfrom lib.eval.fid import FID\n\tfrom .distributed import (gather_tensor, is_master, print,\n\t                          rank, trange, world_size)\n\tfrom .io import Checkpoint, Summary, SummaryWriter, zip_batch_as_png\n\tfrom .util import (FLAGS, command_line, int_str, repeater,\n\t                   report_module_weights, time_format)\n", "flags.DEFINE_integer('logstart', 1, help='Logstep at which to start.')\n\tflags.DEFINE_string('report_fid_len', '16M', help='How often to compute the FID during evaluations.')\n\tflags.DEFINE_string('report_img_len', '4M', help='How often to sample images during evaluations.')\n\t@dataclasses.dataclass\n\tclass TrainInfo:\n\t    samples: int\n\t    progress: float\n\tclass TrainModel(torch.nn.Module):\n\t    COLORS = 3\n\t    EVAL_ROWS = 16\n", "    EVAL_COLUMNS = 16\n\t    model: torch.nn.Module\n\t    model_eval: torch.nn.Module\n\t    train_op: Callable[..., Dict[str, torch.Tensor]]\n\t    def __init__(self, arch: str, res: int, timesteps: int, **params):\n\t        super().__init__()\n\t        self.params = SimpleNamespace(arch=arch, res=res, timesteps=timesteps, **params)\n\t        self.register_buffer('logstep', torch.zeros((), dtype=torch.long))\n\t    @property\n\t    def device(self) -> str:\n", "        for x in self.model.parameters():\n\t            return x.device\n\t    @property\n\t    def logdir(self) -> str:\n\t        params = '_'.join(f'{k}@{v}' for k, v in sorted(vars(self.params).items()) if k not in ('arch',))\n\t        return f'{self.__class__.__name__}({self.params.arch})/{params}'\n\t    def __str__(self) -> str:\n\t        return '\\n'.join((\n\t            f'{\" Model \":-^80}', str(self.model),\n\t            f'{\" Parameters \":-^80}', report_module_weights(self.model),\n", "            f'{\" Config \":-^80}',\n\t            '\\n'.join(f'{k:20s}: {v}' for k, v in vars(self.params).items())\n\t        ))\n\t    def save_meta(self, logdir: pathlib.Path, data_logger: Optional[SummaryWriter] = None):\n\t        if not is_master():\n\t            return\n\t        if data_logger is not None:\n\t            summary = Summary()\n\t            summary.text('info', f'<pre>{self}</pre>')\n\t            data_logger.write(summary, 0)\n", "        (logdir / 'params.json').open('w').write(json.dumps(vars(self.params), indent=4))\n\t        (logdir / 'model.txt').open('w').write(str(self.model.module))\n\t        (logdir / 'cmd.txt').open('w').write(command_line())\n\t    def evaluate(self, summary: Summary,\n\t                 logdir: pathlib.Path,\n\t                 ckpt: Optional[Checkpoint] = None,\n\t                 data_fid: Optional[Iterable] = None,\n\t                 fid_len: int = 0, sample_imgs: bool = True):\n\t        assert (self.EVAL_ROWS * self.EVAL_COLUMNS) % world_size() == 0\n\t        self.eval()\n", "        with torch.no_grad():\n\t            if sample_imgs:\n\t                generator = torch.Generator(device='cpu')\n\t                generator.manual_seed(123623113456 + rank())\n\t                fixed = self((self.EVAL_ROWS * self.EVAL_COLUMNS) // world_size(), generator)\n\t                rand = self((self.EVAL_ROWS * self.EVAL_COLUMNS) // world_size())\n\t                fixed, rand = (gather_tensor(x) for x in (fixed, rand))\n\t                summary.png('eval/fixed', fixed.view(self.EVAL_ROWS, self.EVAL_COLUMNS, *fixed.shape[1:]))\n\t                summary.png('eval/random', rand.view(self.EVAL_ROWS, self.EVAL_COLUMNS, *rand.shape[1:]))\n\t            if fid_len and data_fid:\n", "                fid = FID(FLAGS.dataset, (self.COLORS, self.params.res, self.params.res))\n\t                fake_activations, fake_samples = fid.generate_activations_and_samples(self, FLAGS.fid_len)\n\t                timesteps = self.params.timesteps >> self.logstep.item()\n\t                zip_batch_as_png(fake_samples, logdir / f'samples_{fid_len}_timesteps_{timesteps}.zip')\n\t                fidn, fid50 = fid.approximate_fid(fake_activations)\n\t                summary.scalar(f'eval/fid({fid_len})', fidn)\n\t                summary.scalar('eval/fid(50000)', fid50)\n\t                if ckpt:\n\t                    ckpt.save_file(self.model_eval.module, f'model_{fid50:.5f}.ckpt')\n\t    def train_loop(self,\n", "                   data_train: Iterable,\n\t                   data_fid: Optional[Iterable],\n\t                   batch: int,\n\t                   train_len: str,\n\t                   report_len: str,\n\t                   logdir: pathlib.Path,\n\t                   *,\n\t                   fid_len: int = 4096,\n\t                   keep_ckpts: int = 2):\n\t        print(self)\n", "        print(f'logdir: {logdir}')\n\t        train_len, report_len, report_fid_len, report_img_len = (int_str(x) for x in (\n\t            train_len, report_len, FLAGS.report_fid_len, FLAGS.report_img_len))\n\t        assert report_len % batch == 0\n\t        assert train_len % report_len == 0\n\t        assert report_fid_len % report_len == 0\n\t        assert report_img_len % report_len == 0\n\t        data_train = repeater(data_train)\n\t        ckpt = Checkpoint(self, logdir, keep_ckpts)\n\t        start = ckpt.restore()[0]\n", "        if start:\n\t            print(f'Resuming training at {start} ({start / (1 << 20):.2f}M samples)')\n\t        with SummaryWriter.create(logdir) as data_logger:\n\t            if start == 0:\n\t                self.save_meta(logdir, data_logger)\n\t            for i in range(start, train_len, report_len):\n\t                self.train()\n\t                summary = Summary()\n\t                range_iter = trange(i, i + report_len, batch, leave=False, unit='samples',\n\t                                    unit_scale=batch,\n", "                                    desc=f'Training kimg {i >> 10}/{train_len >> 10}')\n\t                t0 = time.time()\n\t                for samples in range_iter:\n\t                    self.train_step(summary, TrainInfo(samples, samples / train_len), next(data_train))\n\t                samples += batch\n\t                t1 = time.time()\n\t                summary.scalar('sys/samples_per_sec_train', report_len / (t1 - t0))\n\t                compute_fid = (samples % report_fid_len == 0) or (samples >= train_len)\n\t                self.evaluate(summary, logdir, ckpt, data_fid, fid_len=fid_len if compute_fid else 0,\n\t                              sample_imgs=samples % report_img_len == 0)\n", "                t2 = time.time()\n\t                summary.scalar('sys/eval_time', t2 - t1)\n\t                data_logger.write(summary, samples)\n\t                ckpt.save(samples)\n\t                print(f'{samples / (1 << 20):.2f}M/{train_len / (1 << 20):.2f}M samples, '\n\t                      f'time left {time_format((t2 - t0) * (train_len - samples) / report_len)}\\n{summary}')\n\t        ckpt.save_file(self.model_eval.module, 'model.ckpt')\n\t    def train_step(self, summary: Summary, info: TrainInfo, batch: List[torch.Tensor]) -> None:\n\t        device = self.device\n\t        metrics = self.train_op(info, *[x.to(device, non_blocking=True) for x in batch])\n", "        summary.from_metrics(metrics)\n"]}
{"filename": "lib/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\tfrom . import data  # noqa\n\tfrom . import distributed  # noqa\n\tfrom . import eval  # noqa\n\tfrom . import io  # noqa\n\tfrom . import nn  # noqa\n\tfrom . import optim  # noqa\n", "from . import train  # noqa\n\tfrom . import util  # noqa\n"]}
{"filename": "lib/util.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['FLAGS', 'artifact_dir', 'command_line', 'convert_256_to_11', 'cpu_count',\n\t           'downcast', 'ilog2', 'int_str', 'local_kwargs', 'power_of_2', 'repeater', 'report_module_weights', 'setup',\n\t           'time_format', 'to_numpy', 'to_png', 'tqdm', 'tqdm_with', 'trange']\n\timport contextlib\n\timport dataclasses\n\timport inspect\n", "import io\n\timport multiprocessing\n\timport os\n\timport pathlib\n\timport random\n\timport re\n\timport sys\n\tfrom types import SimpleNamespace\n\tfrom typing import Callable, Iterable, Optional, Union\n\timport absl.flags\n", "import numpy as np\n\timport torch\n\timport torch.backends.cudnn\n\timport tqdm as tqdm_module\n\tfrom absl import flags\n\tfrom PIL import Image\n\tFLAGS = SimpleNamespace()\n\tflags.DEFINE_string('logdir', 'e', help='Directory whwer to save logs.')\n\tSYSTEM_FLAGS = {'?', 'alsologtostderr', 'help', 'helpfull', 'helpshort', 'helpxml', 'log_dir', 'logger_levels',\n\t                'logtostderr', 'only_check_args', 'pdb', 'pdb_post_mortem', 'profile_file', 'run_with_pdb',\n", "                'run_with_profiling', 'showprefixforinfo', 'stderrthreshold', 'use_cprofile_for_profiling', 'v',\n\t                'verbosity'}\n\t@dataclasses.dataclass\n\tclass MemInfo:\n\t    total: int  # KB\n\t    res: int  # KB\n\t    shared: int  # KB\n\t    @classmethod\n\t    def query(cls):\n\t        with open(f'/proc/{os.getpid()}/statm', 'r') as f:\n", "            return cls(*[int(x) for x in f.read().split(' ')[:3]])\n\t    def __str__(self):\n\t        gb = 1 << 20\n\t        return f'Total {self.total / gb:.4f} GB | Res {self.res / gb:.4f} GB | Shared {self.shared / gb:.4f} GB'\n\tdef artifact_dir(*args) -> pathlib.Path:\n\t    path = pathlib.Path(FLAGS.logdir)\n\t    return path.joinpath(*args)\n\tdef command_line() -> str:\n\t    argv = sys.argv[:]\n\t    rex = re.compile(r'([!|*$#?~&<>{}()\\[\\]\\\\ \"\\'])')\n", "    cmd = ' '.join(rex.sub(r'\\\\\\1', v) for v in argv)\n\t    return cmd\n\tdef convert_256_to_11(x: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"Lossless conversion of 0,255 interval to -1,1 interval.\"\"\"\n\t    return x / 128 - 255 / 256\n\tdef cpu_count() -> int:\n\t    return multiprocessing.cpu_count()\n\tdef downcast(x: Union[np.ndarray, np.dtype]) -> Union[np.ndarray, np.dtype]:\n\t    \"\"\"Downcast numpy float64 to float32.\"\"\"\n\t    if isinstance(x, np.dtype):\n", "        return np.float32 if x == np.float64 else x\n\t    if x.dtype == np.float64:\n\t        return x.astype('f')\n\t    return x\n\tdef ilog2(x: int) -> int:\n\t    y = x.bit_length() - 1\n\t    assert 1 << y == x\n\t    return y\n\tdef int_str(s: str) -> int:\n\t    p = 1\n", "    if s.endswith('K'):\n\t        s, p = s[:-1], 1 << 10\n\t    elif s.endswith('M'):\n\t        s, p = s[:-1], 1 << 20\n\t    elif s.endswith('G'):\n\t        s, p = s[:-1], 1 << 30\n\t    return int(float(eval(s)) * p)\n\tdef local_kwargs(kwargs: dict, f: Callable) -> dict:\n\t    \"\"\"Return the kwargs from dict that are inputs to function f.\"\"\"\n\t    s = inspect.signature(f)\n", "    p = s.parameters\n\t    if next(reversed(p.values())).kind == inspect.Parameter.VAR_KEYWORD:\n\t        return kwargs\n\t    if len(kwargs) < len(p):\n\t        return {k: v for k, v in kwargs.items() if k in p}\n\t    return {k: kwargs[k] for k in p.keys() if k in kwargs}\n\tdef power_of_2(x: int) -> int:\n\t    \"\"\"Return highest power of 2 <= x\"\"\"\n\t    return 1 << (x.bit_length() - 1)\n\tdef repeater(it: Iterable):\n", "    \"\"\"Helper function to repeat an iterator in a memory efficient way.\"\"\"\n\t    while True:\n\t        for x in it:\n\t            yield x\n\tdef report_module_weights(m: torch.nn.Module):\n\t    weights = [(k, tuple(v.shape)) for k, v in m.named_parameters()]\n\t    weights.append((f'Total ({len(weights)})', (sum(np.prod(x[1]) for x in weights),)))\n\t    width = max(len(x[0]) for x in weights)\n\t    return '\\n'.join(f'{k:<{width}} {np.prod(s):>10} {str(s):>16}' for k, s in weights)\n\tdef setup(seed: Optional[int] = None, quiet: bool = False, flags_values: Optional[SimpleNamespace] = None):\n", "    if flags_values:\n\t        for k, v in vars(flags_values).items():\n\t            setattr(FLAGS, k, v)\n\t    else:\n\t        for k in absl.flags.FLAGS:\n\t            if k not in SYSTEM_FLAGS:\n\t                setattr(FLAGS, k, getattr(absl.flags.FLAGS, k))\n\t    torch.backends.cudnn.benchmark = True\n\t    # os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\n\t    try:\n", "        torch.multiprocessing.set_start_method('spawn')\n\t    except RuntimeError:\n\t        pass\n\t    if seed is not None:\n\t        random.seed(seed)\n\t        np.random.seed(seed)\n\t        torch.manual_seed(seed)\n\t    if not quiet:\n\t        print(f'{\" Flags \":-^79s}')\n\t        for k in sorted(vars(FLAGS)):\n", "            print(f'{k:32s}: {getattr(FLAGS, k)}')\n\t        print(f'{\" System \":-^79s}')\n\t        for k, v in {'cpus(system)': multiprocessing.cpu_count(),\n\t                     'cpus(fixed)': cpu_count(),\n\t                     'multiprocessing.start_method': torch.multiprocessing.get_start_method()}.items():\n\t            print(f'{k:32s}: {v}')\n\tdef time_format(t: float) -> str:\n\t    t = int(t)\n\t    hours = t // 3600\n\t    mins = (t // 60) % 60\n", "    secs = t % 60\n\t    return f'{hours:02d}:{mins:02d}:{secs:02d}'\n\tdef to_numpy(x: Union[np.ndarray, torch.Tensor]):\n\t    if not isinstance(x, torch.Tensor):\n\t        return x\n\t    return x.detach().cpu().numpy()\n\tdef to_png(x: Union[np.ndarray, torch.Tensor]) -> bytes:\n\t    \"\"\"Converts numpy array in (C, H, W) or (Rows, Cols, C, H, W) format into PNG format.\"\"\"\n\t    assert x.ndim in (3, 5)\n\t    if isinstance(x, torch.Tensor):\n", "        x = to_numpy(x)\n\t    if x.ndim == 5:  # Image grid\n\t        x = np.transpose(x, (2, 0, 3, 1, 4))\n\t        x = x.reshape((x.shape[0], x.shape[1] * x.shape[2], x.shape[3] * x.shape[4]))  # (C, H, W)\n\t    if x.dtype in (np.float64, np.float32, np.float16):\n\t        x = np.transpose(np.round(127.5 * (x + 1)), (1, 2, 0)).clip(0, 255).astype('uint8')\n\t    elif x.dtype != np.uint8:\n\t        raise ValueError('Unsupported array type, expecting float or uint8', x.dtype)\n\t    if x.shape[2] == 1:\n\t        x = np.broadcast_to(x, x.shape[:2] + (3,))\n", "    with io.BytesIO() as f:\n\t        Image.fromarray(x).save(f, 'png')\n\t        return f.getvalue()\n\tdef tqdm(iterable: Iterable, **kwargs) -> Iterable:\n\t    return tqdm_module.tqdm(iterable, **kwargs)\n\tdef tqdm_with(**kwargs) -> Iterable:\n\t    class Noop:\n\t        def update(self, *args, **kwargs):\n\t            pass\n\t    @contextlib.contextmanager\n", "    def noop():\n\t        yield Noop()\n\t    return tqdm_module.tqdm(**kwargs)\n\tdef trange(*args, **kwargs):\n\t    return tqdm_module.trange(*args, **kwargs)\n"]}
{"filename": "lib/optim.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport copy\n\timport itertools\n\timport torch\n\timport torch.optim.swa_utils\n\tclass ModuleEMA(torch.nn.Module):  # Preferred to PyTorch's builtin because this is pickable\n\t    def __init__(self, m: torch.nn.Module, momentum: float):\n", "        super().__init__()\n\t        self.module = copy.deepcopy(m)\n\t        self.momentum = momentum\n\t        self.register_buffer('step', torch.zeros((), dtype=torch.long))\n\t    def update(self, source: torch.nn.Module):\n\t        self.step.add_(1)\n\t        decay = (1 - self.momentum) / (1 - self.momentum ** self.step)\n\t        with torch.no_grad():\n\t            for p_self, p_source in zip(self.module.parameters(), source.parameters()):\n\t                p_self.add_(p_source - p_self, alpha=decay)\n", "            for p_self, p_source in zip(self.module.buffers(), source.buffers()):\n\t                if torch.is_floating_point(p_source):\n\t                    assert torch.is_floating_point(p_self)\n\t                    p_self.add_(p_source - p_self, alpha=decay)\n\t                else:\n\t                    assert not torch.is_floating_point(p_self)\n\t                    p_self.add_(p_source - p_self)\n\t    def forward(self, *args, **kwargs):\n\t        return self.module.forward(*args, **kwargs)\n\tclass AveragedModel(torch.optim.swa_utils.AveragedModel):\n", "    def update_parameters(self, model):\n\t        self_param = itertools.chain(self.module.parameters(), self.module.buffers())\n\t        model_param = itertools.chain(model.parameters(), model.buffers())\n\t        for p_swa, p_model in zip(self_param, model_param):\n\t            device = p_swa.device\n\t            p_model_ = p_model.detach().to(device)\n\t            if self.n_averaged == 0:\n\t                p_swa.detach().copy_(p_model_)\n\t            else:\n\t                p_swa.detach().copy_(self.avg_fn(p_swa.detach(), p_model_, self.n_averaged.to(device)))\n", "        self.n_averaged += 1\n\tdef module_exponential_moving_average(model: torch.nn.Module,\n\t                                      momentum: float) -> torch.optim.swa_utils.AveragedModel:\n\t    \"\"\"Create an AverageModel using Stochastic Weight Averaging.\n\t    Args:\n\t        model: the torch Module to average.\n\t        momentum: the running average momentum coefficient. I found values in 0.9, 0.99, 0.999,\n\t          0.9999, ... to give good results. The closer to 1 the better, but the longer one needs\n\t          to train.\n\t    Returns:\n", "        torch.optim.swa_utils.AveragedModel module that replicates the model behavior with SWA\n\t          weights.\n\t    \"\"\"\n\t    def ema(target: torch.Tensor, source: torch.Tensor, count: int) -> torch.Tensor:\n\t        mu = 1 - (1 - momentum) / (1 - momentum ** (1 + count))\n\t        return mu * target + (1 - mu) * source\n\t    return AveragedModel(model, avg_fn=ema)\n\tclass HalfLifeEMA(torch.nn.Module):\n\t    def __init__(self, m: torch.nn.Module, half_life: int = 500000, batch_size: int = 512):\n\t        \"\"\"\n", "        EMA Module based of half life (units of samples/images).\n\t        Args:\n\t            half_life   :   Half life of EMA in units of samples.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.module = copy.deepcopy(m)\n\t        self.half_life = half_life\n\t        self.batch_size = batch_size\n\t    def update(self, source: torch.nn.Module):\n\t        ema_beta = 0.5 ** (self.batch_size / self.half_life)\n", "        with torch.no_grad():\n\t            for p_self, p_source in zip(self.module.parameters(), source.parameters()):\n\t                p_self.copy_(p_source.lerp(p_self, ema_beta))\n\t            for p_self, p_source in zip(self.module.buffers(), source.buffers()):\n\t                p_self.copy_(p_source)\n\t    def forward(self, *args, **kwargs):\n\t        return self.module.forward(*args, **kwargs)\n\tclass CopyModule(torch.nn.Module):\n\t    def __init__(self, m: torch.nn.Module):\n\t        \"\"\"Copy Module for self-conditioning.\"\"\"\n", "        super().__init__()\n\t        self.module = copy.deepcopy(m)\n\t    def update(self, source: torch.nn.Module):\n\t        with torch.no_grad():\n\t            for p_self, p_source in zip(self.module.parameters(), source.parameters()):\n\t                p_self.copy_(p_source)\n\t            for p_self, p_source in zip(self.module.buffers(), source.buffers()):\n\t                p_self.copy_(p_source)\n\t    def forward(self, *args, **kwargs):\n\t        return self.module.forward(*args, **kwargs)"]}
{"filename": "lib/io.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['Checkpoint', 'Summary', 'SummaryWriter', 'zip_batch_as_png']\n\timport enum\n\timport io\n\timport os\n\timport pathlib\n\timport zipfile\n", "from time import time\n\tfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\timport imageio\n\timport matplotlib.figure\n\timport numpy as np\n\timport torch\n\timport torch.nn\n\tfrom tensorboard.compat.proto import event_pb2, summary_pb2\n\tfrom tensorboard.summary.writer.event_file_writer import EventFileWriter\n\tfrom tensorboard.util.tensor_util import make_tensor_proto\n", "from .distributed import is_master, print, reduce_dict_mean\n\tfrom .util import to_numpy, to_png\n\tclass Checkpoint:\n\t    DIR_NAME: str = 'ckpt'\n\t    FILE_MATCH: str = '*.pth'\n\t    FILE_FORMAT: str = '%012d.pth'\n\t    def __init__(self,\n\t                 model: torch.nn.Module,\n\t                 logdir: pathlib.Path,\n\t                 keep_ckpts: int = 0):\n", "        self.model = model\n\t        self.logdir = logdir / self.DIR_NAME\n\t        self.keep_ckpts = keep_ckpts\n\t    @staticmethod\n\t    def checkpoint_idx(filename: str) -> int:\n\t        return int(os.path.basename(filename).split('.')[0])\n\t    def restore(self, idx: Optional[int] = None) -> Tuple[int, Optional[pathlib.Path]]:\n\t        if idx is None:\n\t            all_ckpts = self.logdir.glob(self.FILE_MATCH)\n\t            try:\n", "                idx = self.checkpoint_idx(max(str(x) for x in all_ckpts))\n\t            except ValueError:\n\t                return 0, None\n\t        ckpt = self.logdir / (self.FILE_FORMAT % idx)\n\t        print(f'Resuming from: {ckpt}')\n\t        with ckpt.open('rb') as f:\n\t            self.model.load_state_dict(torch.load(f, map_location='cpu'))\n\t        return idx, ckpt\n\t    def save(self, idx: int) -> None:\n\t        if not is_master():  # only save master's state\n", "            return\n\t        self.logdir.mkdir(exist_ok=True, parents=True)\n\t        ckpt = self.logdir / (self.FILE_FORMAT % idx)\n\t        with ckpt.open('wb') as f:\n\t            torch.save(self.model.state_dict(), f)\n\t        old_ckpts = sorted(self.logdir.glob(self.FILE_MATCH), key=str)\n\t        for ckpt in old_ckpts[:-self.keep_ckpts]:\n\t            ckpt.unlink()\n\t    def save_file(self, model: torch.nn.Module, filename: str) -> None:\n\t        if not is_master():  # only save master's state\n", "            return\n\t        self.logdir.mkdir(exist_ok=True, parents=True)\n\t        with (self.logdir / filename).open('wb') as f:\n\t            torch.save(model.state_dict(), f)\n\tclass Summary(dict):\n\t    \"\"\"Helper to generate summary_pb2.Summary protobufs.\"\"\"\n\t    # Inspired from https://github.com/google/objax/blob/master/objax/jaxboard.py\n\t    class ProtoMode(enum.Flag):\n\t        \"\"\"Enum describing what to export to a tensorboard proto.\"\"\"\n\t        IMAGES = enum.auto()\n", "        VIDEOS = enum.auto()\n\t        OTHERS = enum.auto()\n\t        ALL = IMAGES | VIDEOS | OTHERS\n\t    class Scalar:\n\t        \"\"\"Class for a Summary Scalar.\"\"\"\n\t        def __init__(self, reduce: Callable[[Sequence[float]], float] = np.mean):\n\t            self.values = []\n\t            self.reduce = reduce\n\t        def __call__(self):\n\t            return self.reduce(self.values)\n", "    class Text:\n\t        \"\"\"Class for a Summary Text.\"\"\"\n\t        def __init__(self, text: str):\n\t            self.text = text\n\t    class Image:\n\t        \"\"\"Class for a Summary Image.\"\"\"\n\t        def __init__(self, shape: Tuple[int, int, int], image_bytes: bytes):\n\t            self.shape = shape  # (C, H, W)\n\t            self.image_bytes = image_bytes\n\t    class Video:\n", "        \"\"\"Class for a Summary Video.\"\"\"\n\t        def __init__(self, shape: Tuple[int, int, int], image_bytes: bytes):\n\t            self.shape = shape  # (C, H, W)\n\t            self.image_bytes = image_bytes\n\t    def from_metrics(self, metrics: Dict[str, torch.Tensor]):\n\t        metrics = reduce_dict_mean(metrics)\n\t        for k, v in metrics.items():\n\t            v = to_numpy(v)\n\t            if np.isnan(v):\n\t                raise ValueError('NaN', k)\n", "            self.scalar(k, float(v))\n\t    def gif(self, tag: str, imgs: List[np.ndarray]):\n\t        assert imgs\n\t        try:\n\t            height, width, _ = imgs[0].shape\n\t            vid_save_path = '/tmp/video.gif'\n\t            imageio.mimsave(vid_save_path, [np.array(img) for i, img in enumerate(imgs) if i % 2 == 0], fps=30)\n\t            with open(vid_save_path, 'rb') as f:\n\t                encoded_image_string = f.read()\n\t            self[tag] = Summary.Video((3, height, width), encoded_image_string)\n", "        except AttributeError:\n\t            # the kitchen and hand manipulation envs do not support rendering.\n\t            return\n\t    def plot(self, tag: str, fig: matplotlib.figure.Figure):\n\t        byte_data = io.BytesIO()\n\t        fig.savefig(byte_data, format='png')\n\t        img_w, img_h = fig.canvas.get_width_height()\n\t        self[tag] = Summary.Image((4, img_h, img_w), byte_data.getvalue())\n\t    def png(self, tag: str, img: Union[np.ndarray, torch.Tensor]):\n\t        if img.ndim == 3:\n", "            shape = (img.shape[2], *img.shape[:2])\n\t        elif img.ndim == 5:\n\t            shape = (img.shape[2], img.shape[0] * img.shape[3], img.shape[1] * img.shape[4])\n\t        else:\n\t            raise ValueError(f'Unsupported image shape {img.shape}')\n\t        self[tag] = Summary.Image(shape, to_png(img))\n\t    def scalar(self, tag: str, value: float, reduce: Callable[[Sequence[float]], float] = np.mean):\n\t        if tag not in self:\n\t            self[tag] = Summary.Scalar(reduce)\n\t        self[tag].values.append(value)\n", "    def text(self, tag: str, text: str):\n\t        self[tag] = Summary.Text(text)\n\t    def proto(self, mode: ProtoMode = ProtoMode.ALL):\n\t        entries = []\n\t        for tag, value in self.items():\n\t            if isinstance(value, Summary.Scalar):\n\t                if mode & self.ProtoMode.OTHERS:\n\t                    entries.append(summary_pb2.Summary.Value(tag=tag, simple_value=value()))\n\t            elif isinstance(value, Summary.Text):\n\t                if mode & self.ProtoMode.OTHERS:\n", "                    metadata = summary_pb2.SummaryMetadata(\n\t                        plugin_data=summary_pb2.SummaryMetadata.PluginData(plugin_name='text'))\n\t                    entries.append(summary_pb2.Summary.Value(\n\t                        tag=tag, metadata=metadata,\n\t                        tensor=make_tensor_proto(values=value.text.encode('utf-8'), shape=(1,))))\n\t            elif isinstance(value, (Summary.Image, Summary.Video)):\n\t                if mode & (self.ProtoMode.IMAGES | self.ProtoMode.VIDEOS):\n\t                    image_summary = summary_pb2.Summary.Image(\n\t                        encoded_image_string=value.image_bytes,\n\t                        colorspace=value.shape[0],  # RGBA\n", "                        height=value.shape[1],\n\t                        width=value.shape[2])\n\t                    entries.append(summary_pb2.Summary.Value(tag=tag, image=image_summary))\n\t            else:\n\t                raise NotImplementedError(tag, value)\n\t        return summary_pb2.Summary(value=entries)\n\t    def to_dict(self) -> Dict[str, Any]:\n\t        entries = {}\n\t        for tag, value in self.items():\n\t            if isinstance(value, Summary.Scalar):\n", "                entries[tag] = float(value())\n\t            elif isinstance(value, (Summary.Text, Summary.Image, Summary.Video)):\n\t                pass\n\t            else:\n\t                raise NotImplementedError(tag, value)\n\t        return entries\n\t    def __str__(self) -> str:\n\t        return '\\n'.join(f'    {k:40s}: {v:.6f}' for k, v in self.to_dict().items())\n\tclass SummaryForgetter:\n\t    \"\"\"Used as placeholder for workers, it basically does nothing.\"\"\"\n", "    def __init__(self,\n\t                 logdir: pathlib.Path,\n\t                 queue_size: int = 5,\n\t                 write_interval: int = 5):\n\t        self.logdir = logdir\n\t    def write(self, summary: Summary, step: int):\n\t        pass\n\t    def close(self):\n\t        \"\"\"Flushes the event file to disk and close the file.\"\"\"\n\t        pass\n", "    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        self.close()\n\t# Inspired from https://github.com/google/objax/blob/master/objax/jaxboard.py\n\tclass SummaryWriter:\n\t    \"\"\"Writes entries to logdir to be consumed by TensorBoard and Weight & Biases.\"\"\"\n\t    def __init__(self,\n\t                 logdir: pathlib.Path,\n\t                 queue_size: int = 5,\n", "                 write_interval: int = 5):\n\t        (logdir / 'tb').mkdir(exist_ok=True, parents=True)\n\t        self.logdir = logdir\n\t        self.writer = EventFileWriter(logdir / 'tb', queue_size, write_interval)\n\t        self.writer_image = EventFileWriter(logdir / 'tb', queue_size, write_interval, filename_suffix='images')\n\t    def write(self, summary: Summary, step: int):\n\t        \"\"\"Add on event to the event file.\"\"\"\n\t        self.writer.add_event(\n\t            event_pb2.Event(step=step, summary=summary.proto(summary.ProtoMode.OTHERS),\n\t                            wall_time=time()))\n", "        self.writer_image.add_event(\n\t            event_pb2.Event(step=step, summary=summary.proto(summary.ProtoMode.IMAGES),\n\t                            wall_time=time()))\n\t    def close(self):\n\t        \"\"\"Flushes the event file to disk and close the file.\"\"\"\n\t        self.writer.close()\n\t        self.writer_image.close()\n\t    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_val, exc_tb):\n", "        self.close()\n\t    @classmethod\n\t    def create(cls, logdir: pathlib.Path,\n\t               queue_size: int = 5,\n\t               write_interval: int = 5) -> Union[SummaryForgetter, 'SummaryWriter']:\n\t        if is_master():\n\t            return cls(logdir, queue_size, write_interval)\n\t        return SummaryForgetter(logdir, queue_size, write_interval)\n\tdef zip_batch_as_png(x: Union[np.ndarray, torch.Tensor], filename: pathlib.Path):\n\t    if not is_master():\n", "        return\n\t    assert x.ndim == 4\n\t    with zipfile.ZipFile(filename, 'w') as fzip:\n\t        for i in range(x.shape[0]):\n\t            with fzip.open(f'{i:06d}.png', 'w') as f:\n\t                f.write(to_png(x[i]))\n"]}
{"filename": "lib/data/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport os\n\timport pathlib\n\tfrom .data import *\n\tfrom .data_torch import *\n\tML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n"]}
{"filename": "lib/data/data.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['Dataset']\n\tfrom typing import Callable\n\tfrom absl import flags\n\tflags.DEFINE_integer('batch', 256, help='Batch size.')\n\tclass Dataset:\n\t    def __init__(self, res: int, make_train: Callable, make_fid: Callable):\n", "        self.res = res\n\t        self.make_train = make_train\n\t        self.make_fid = make_fid\n"]}
{"filename": "lib/data/data_torch.py", "chunked_list": ["# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['make_cifar10', 'make_imagenet64', 'DATASETS']\n\timport os\n\timport pathlib\n\tfrom typing import Tuple\n\timport torch\n\timport torch.distributed\n\timport torch.nn.functional\n", "import torchvision.datasets\n\timport torchvision.transforms.functional\n\tfrom lib.util import FLAGS\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision.transforms import Compose\n\tfrom . import data\n\tML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n\tclass DatasetTorch(data.Dataset):\n\t    def make_dataloaders(self, **kwargs) -> Tuple[DataLoader, DataLoader]:\n\t        batch = FLAGS.batch\n", "        if torch.distributed.is_initialized():\n\t            assert batch % torch.distributed.get_world_size() == 0\n\t            batch //= torch.distributed.get_world_size()\n\t        return (DataLoader(self.make_train(), shuffle=True, drop_last=True, batch_size=batch,\n\t                           num_workers=4, prefetch_factor=8, persistent_workers=True, **kwargs),\n\t                DataLoader(self.make_fid(), shuffle=True, drop_last=True, batch_size=batch,\n\t                           num_workers=4, prefetch_factor=8, persistent_workers=True, **kwargs))\n\tdef normalize(x: torch.Tensor) -> torch.Tensor:\n\t    return 2 * x - 1\n\tdef make_cifar10() -> DatasetTorch:\n", "    transforms = [\n\t        torchvision.transforms.ToTensor(),\n\t        normalize,\n\t    ]\n\t    transforms_fid = Compose(transforms)\n\t    transforms_train = Compose(transforms + [torchvision.transforms.RandomHorizontalFlip()])\n\t    fid = lambda: torchvision.datasets.CIFAR10(str(ML_DATA), train=True, transform=transforms_fid, download=True)\n\t    train = lambda: torchvision.datasets.CIFAR10(str(ML_DATA), train=True, transform=transforms_train, download=True)\n\t    return DatasetTorch(32, train, fid)\n\tdef make_imagenet64() -> DatasetTorch:\n", "    transforms = [\n\t        torchvision.transforms.ToTensor(),\n\t        torchvision.transforms.CenterCrop(64),\n\t        normalize,\n\t    ]\n\t    transforms_fid = Compose(transforms)\n\t    transforms_train = Compose(transforms + [torchvision.transforms.RandomHorizontalFlip()])\n\t    fid = lambda: torchvision.datasets.ImageFolder(str(ML_DATA / \"imagenet\" / \"train\"), transform=transforms_fid)\n\t    train = lambda: torchvision.datasets.ImageFolder(str(ML_DATA / \"imagenet\" / \"train\"), transform=transforms_train)\n\t    return DatasetTorch(64, train, fid)\n", "DATASETS = {\n\t    'cifar10': make_cifar10,\n\t    'imagenet64': make_imagenet64,\n\t}\n"]}
{"filename": "lib/eval/inception_net.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport os\n\timport pathlib\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n", "import lib\n\ttry:\n\t    from torchvision.models.utils import load_state_dict_from_url\n\texcept ImportError:\n\t    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\t# Inception weights ported to Pytorch from\n\t# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n\tFID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'  # noqa: E501\n\tFID_WEIGHTS_FILE = 'pt_inception-2015-12-05-6726825d.pth'\n\tclass InceptionV3(nn.Module):\n", "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\t    # Index of default block of inception to return,\n\t    # corresponds to output of final average pooling\n\t    DEFAULT_BLOCK_INDEX = 3\n\t    # Maps feature dimensionality to their output blocks indices\n\t    BLOCK_INDEX_BY_DIM = {\n\t        64: 0,  # First max pooling features\n\t        192: 1,  # Second max pooling featurs\n\t        768: 2,  # Pre-aux classifier features\n\t        2048: 3  # Final average pooling features\n", "    }\n\t    def __init__(self,\n\t                 output_blocks=(DEFAULT_BLOCK_INDEX,),\n\t                 resize_input=True,\n\t                 normalize_input=False,\n\t                 requires_grad=False,\n\t                 use_fid_inception=True):\n\t        \"\"\"Build pretrained InceptionV3\n\t        Parameters\n\t        ----------\n", "        output_blocks : list of int\n\t            Indices of blocks to return features of. Possible values are:\n\t                - 0: corresponds to output of first max pooling\n\t                - 1: corresponds to output of second max pooling\n\t                - 2: corresponds to output which is fed to aux classifier\n\t                - 3: corresponds to output of final average pooling\n\t        resize_input : bool\n\t            If true, bilinearly resizes input to width and height 299 before\n\t            feeding input to model. As the network without fully connected\n\t            layers is fully convolutional, it should be able to handle inputs\n", "            of arbitrary size, so resizing might not be strictly needed\n\t        normalize_input : bool\n\t            If true, scales the input from range (0, 1) to the range the\n\t            pretrained Inception network expects, namely (-1, 1)\n\t        requires_grad : bool\n\t            If true, parameters of the model require gradients. Possibly useful\n\t            for finetuning the network\n\t        use_fid_inception : bool\n\t            If true, uses the pretrained Inception model used in Tensorflow's\n\t            FID implementation. If false, uses the pretrained Inception model\n", "            available in torchvision. The FID Inception model has different\n\t            weights and a slightly different structure from torchvision's\n\t            Inception model. If you want to compute FID scores, you are\n\t            strongly advised to set this parameter to true to get comparable\n\t            results.\n\t        \"\"\"\n\t        super(InceptionV3, self).__init__()\n\t        self.resize_input = resize_input\n\t        self.normalize_input = normalize_input\n\t        self.output_blocks = sorted(output_blocks)\n", "        self.last_needed_block = max(output_blocks)\n\t        assert self.last_needed_block <= 3, \\\n\t            'Last possible output block index is 3'\n\t        self.blocks = nn.ModuleList()\n\t        if use_fid_inception:\n\t            inception = fid_inception_v3()\n\t        else:\n\t            inception = _inception_v3(pretrained=True)\n\t        # Block 0: input to maxpool1\n\t        block0 = [\n", "            inception.Conv2d_1a_3x3,\n\t            inception.Conv2d_2a_3x3,\n\t            inception.Conv2d_2b_3x3,\n\t            nn.MaxPool2d(kernel_size=3, stride=2)\n\t        ]\n\t        self.blocks.append(nn.Sequential(*block0))\n\t        # Block 1: maxpool1 to maxpool2\n\t        if self.last_needed_block >= 1:\n\t            block1 = [\n\t                inception.Conv2d_3b_1x1,\n", "                inception.Conv2d_4a_3x3,\n\t                nn.MaxPool2d(kernel_size=3, stride=2)\n\t            ]\n\t            self.blocks.append(nn.Sequential(*block1))\n\t        # Block 2: maxpool2 to aux classifier\n\t        if self.last_needed_block >= 2:\n\t            block2 = [\n\t                inception.Mixed_5b,\n\t                inception.Mixed_5c,\n\t                inception.Mixed_5d,\n", "                inception.Mixed_6a,\n\t                inception.Mixed_6b,\n\t                inception.Mixed_6c,\n\t                inception.Mixed_6d,\n\t                inception.Mixed_6e,\n\t            ]\n\t            self.blocks.append(nn.Sequential(*block2))\n\t        # Block 3: aux classifier to final avgpool\n\t        if self.last_needed_block >= 3:\n\t            block3 = [\n", "                inception.Mixed_7a,\n\t                inception.Mixed_7b,\n\t                inception.Mixed_7c,\n\t                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\t            ]\n\t            self.blocks.append(nn.Sequential(*block3))\n\t        for param in self.parameters():\n\t            param.requires_grad = requires_grad\n\t    def forward(self, inp):\n\t        \"\"\"Get Inception feature maps\n", "        Parameters\n\t        ----------\n\t        inp : torch.autograd.Variable\n\t            Input tensor of shape Bx3xHxW. Values are expected to be in\n\t            range (0, 1)\n\t        Returns\n\t        -------\n\t        List of torch.autograd.Variable, corresponding to the selected output\n\t        block, sorted ascending by index\n\t        \"\"\"\n", "        outp = []\n\t        x = inp\n\t        if self.resize_input:\n\t            x = F.interpolate(x,\n\t                              size=(299, 299),\n\t                              mode='bilinear',\n\t                              align_corners=False)\n\t        if self.normalize_input:\n\t            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n\t        for idx, block in enumerate(self.blocks):\n", "            x = block(x)\n\t            if idx in self.output_blocks:\n\t                outp.append(x)\n\t            if idx == self.last_needed_block:\n\t                break\n\t        return outp\n\tdef _inception_v3(*args, **kwargs):\n\t    \"\"\"Wraps `torchvision.models.inception_v3`\n\t    Skips default weight inititialization if supported by torchvision version.\n\t    See https://github.com/mseitzer/pytorch-fid/issues/28.\n", "    \"\"\"\n\t    try:\n\t        version = tuple(map(int, torchvision.__version__.split('.')[:2]))\n\t    except ValueError:\n\t        # Just a caution against weird version strings\n\t        version = (0,)\n\t    if version >= (0, 6):\n\t        kwargs['init_weights'] = False\n\t    return torchvision.models.inception_v3(*args, **kwargs)\n\tdef fid_inception_v3():\n", "    \"\"\"Build pretrained Inception model for FID computation\n\t    The Inception model for FID computation uses a different set of weights\n\t    and has a slightly different structure than torchvision's Inception.\n\t    This method first constructs torchvision's Inception and then patches the\n\t    necessary parts that are different in the FID Inception model.\n\t    \"\"\"\n\t    inception = _inception_v3(num_classes=1008,\n\t                              aux_logits=False,\n\t                              pretrained=False)\n\t    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n", "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n\t    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n\t    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n\t    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n\t    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n\t    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n\t    inception.Mixed_7b = FIDInceptionE_1(1280)\n\t    inception.Mixed_7c = FIDInceptionE_2(2048)\n\t    local_fid_weights = pathlib.Path(lib.data.ML_DATA / os.path.basename(FID_WEIGHTS_URL))\n\t    if local_fid_weights.is_file():\n", "        state_dict = torch.load(local_fid_weights)\n\t    else:\n\t        state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n\t    inception.load_state_dict(state_dict)\n\t    return inception\n\tclass FIDInceptionA(torchvision.models.inception.InceptionA):\n\t    \"\"\"InceptionA block patched for FID computation\"\"\"\n\t    def __init__(self, in_channels, pool_features):\n\t        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n\t    def forward(self, x):\n", "        branch1x1 = self.branch1x1(x)\n\t        branch5x5 = self.branch5x5_1(x)\n\t        branch5x5 = self.branch5x5_2(branch5x5)\n\t        branch3x3dbl = self.branch3x3dbl_1(x)\n\t        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n\t        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\t        # Patch: Tensorflow's average pool does not use the padded zero's in\n\t        # its average calculation\n\t        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n\t                                   count_include_pad=False)\n", "        branch_pool = self.branch_pool(branch_pool)\n\t        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n\t        return torch.cat(outputs, 1)\n\tclass FIDInceptionC(torchvision.models.inception.InceptionC):\n\t    \"\"\"InceptionC block patched for FID computation\"\"\"\n\t    def __init__(self, in_channels, channels_7x7):\n\t        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n\t    def forward(self, x):\n\t        branch1x1 = self.branch1x1(x)\n\t        branch7x7 = self.branch7x7_1(x)\n", "        branch7x7 = self.branch7x7_2(branch7x7)\n\t        branch7x7 = self.branch7x7_3(branch7x7)\n\t        branch7x7dbl = self.branch7x7dbl_1(x)\n\t        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n\t        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n\t        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n\t        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\t        # Patch: Tensorflow's average pool does not use the padded zero's in\n\t        # its average calculation\n\t        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n", "                                   count_include_pad=False)\n\t        branch_pool = self.branch_pool(branch_pool)\n\t        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n\t        return torch.cat(outputs, 1)\n\tclass FIDInceptionE_1(torchvision.models.inception.InceptionE):\n\t    \"\"\"First InceptionE block patched for FID computation\"\"\"\n\t    def __init__(self, in_channels):\n\t        super(FIDInceptionE_1, self).__init__(in_channels)\n\t    def forward(self, x):\n\t        branch1x1 = self.branch1x1(x)\n", "        branch3x3 = self.branch3x3_1(x)\n\t        branch3x3 = [\n\t            self.branch3x3_2a(branch3x3),\n\t            self.branch3x3_2b(branch3x3),\n\t        ]\n\t        branch3x3 = torch.cat(branch3x3, 1)\n\t        branch3x3dbl = self.branch3x3dbl_1(x)\n\t        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n\t        branch3x3dbl = [\n\t            self.branch3x3dbl_3a(branch3x3dbl),\n", "            self.branch3x3dbl_3b(branch3x3dbl),\n\t        ]\n\t        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\t        # Patch: Tensorflow's average pool does not use the padded zero's in\n\t        # its average calculation\n\t        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n\t                                   count_include_pad=False)\n\t        branch_pool = self.branch_pool(branch_pool)\n\t        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n\t        return torch.cat(outputs, 1)\n", "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n\t    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n\t    def __init__(self, in_channels):\n\t        super(FIDInceptionE_2, self).__init__(in_channels)\n\t    def forward(self, x):\n\t        branch1x1 = self.branch1x1(x)\n\t        branch3x3 = self.branch3x3_1(x)\n\t        branch3x3 = [\n\t            self.branch3x3_2a(branch3x3),\n\t            self.branch3x3_2b(branch3x3),\n", "        ]\n\t        branch3x3 = torch.cat(branch3x3, 1)\n\t        branch3x3dbl = self.branch3x3dbl_1(x)\n\t        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n\t        branch3x3dbl = [\n\t            self.branch3x3dbl_3a(branch3x3dbl),\n\t            self.branch3x3dbl_3b(branch3x3dbl),\n\t        ]\n\t        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\t        # Patch: The FID Inception model uses max pooling instead of average\n", "        # pooling. This is likely an error in this specific Inception\n\t        # implementation, as other Inception models use average pooling here\n\t        # (which matches the description in the paper).\n\t        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n\t        branch_pool = self.branch_pool(branch_pool)\n\t        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n\t        return torch.cat(outputs, 1)\n"]}
{"filename": "lib/eval/fid.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport os\n\timport pathlib\n\tfrom typing import Iterable, Tuple\n\timport numpy as np\n\timport scipy\n\timport torch\n", "import torch.nn.functional\n\tfrom lib.distributed import (barrier, device_id, gather_tensor, is_master,\n\t                             trange, world_size)\n\tfrom lib.util import FLAGS, to_numpy\n\tfrom .inception_net import InceptionV3\n\tML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n\tclass FID:\n\t    def __init__(self, dataset: str, shape: Tuple[int, int, int], dims: int = 2048):\n\t        assert dataset in ('cifar10', 'imagenet64')\n\t        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n", "        self.dims = dims\n\t        self.shape = shape\n\t        self.model = InceptionV3([block_idx]).eval().to(device_id())\n\t        self.post = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(1), torch.nn.Flatten())\n\t        if pathlib.Path(f'{ML_DATA}/{dataset}_activation_mean.npy').exists():\n\t            self.real_activations_mean = torch.from_numpy(np.load(f'{ML_DATA}/{dataset}_activation_mean.npy'))\n\t            self.real_activations_std = torch.from_numpy(np.load(f'{ML_DATA}/{dataset}_activation_std.npy'))\n\t    def generate_activations_and_samples(self, model: torch.nn.Module, n: int) -> Tuple[torch.Tensor, torch.Tensor]:\n\t        barrier()\n\t        samples = torch.empty((n, *self.shape))\n", "        activations = torch.empty((n, self.dims), dtype=torch.double).to(device_id())\n\t        k = world_size()\n\t        assert FLAGS.batch % k == 0\n\t        for i in trange(0, n, FLAGS.batch, desc='Generating FID samples'):\n\t            p = min(n - i, FLAGS.batch)\n\t            x = model(FLAGS.batch // k).float()\n\t            # Discretize to {0,...,255} and project back to [-1,1]\n\t            x = torch.round(127.5 * (x + 1)).clamp(0, 255) / 127.5 - 1\n\t            y = self.post(self.model(x)[0])\n\t            samples[i: i + p] = gather_tensor(x)[:p]\n", "            activations[i: i + p] = gather_tensor(y)[:p]\n\t        return activations, samples\n\t    def data_activations(self, iterator: Iterable, n: int, cpu: bool = False) -> torch.Tensor:\n\t        activations = torch.empty((n, self.dims), dtype=torch.double)\n\t        if not cpu:\n\t            activations = activations.to(device_id())\n\t        k = world_size()\n\t        it = iter(iterator)\n\t        for i in trange(0, n, FLAGS.batch, desc='Calculating activations'):\n\t            x = next(it)[0]\n", "            p = min((n - i) // k, x.shape[0])\n\t            y = self.post(self.model(x.to(device_id()))[0])\n\t            activations[i: i + k * p] = gather_tensor(y[:p]).cpu() if cpu else gather_tensor(y[:p])\n\t        return activations\n\t    @staticmethod\n\t    def calculate_activation_statistics(activations: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n\t        return activations.mean(0), torch.cov(activations.T)\n\t    def calculate_fid(self, fake_activations: torch.Tensor) -> float:\n\t        m_fake, s_fake = self.calculate_activation_statistics(fake_activations)\n\t        m_real = self.real_activations_mean.to(m_fake)\n", "        s_real = self.real_activations_std.to(s_fake)\n\t        return self.calculate_frechet_distance(m_fake, s_fake, m_real, s_real)\n\t    def approximate_fid(self, fake_activations: torch.Tensor, n: int = 50_000) -> Tuple[float, float]:\n\t        k = fake_activations.shape[0]\n\t        fid = self.calculate_fid(fake_activations)\n\t        fid_half = []\n\t        for it in range(5):\n\t            sel_fake = np.random.choice(k, k // 2, replace=False)\n\t            fid_half.append(self.calculate_fid(fake_activations[sel_fake]))\n\t        fid_half = np.median(fid_half)\n", "        return fid, fid + (fid_half - fid) * (k / n - 1)\n\t    def calculate_frechet_distance(self, mu1: torch.Tensor, sigma1: torch.Tensor,\n\t                                   mu2: torch.Tensor, sigma2: torch.Tensor, eps: float = 1e-6) -> float:\n\t        \"\"\"Numpy implementation of the Frechet Distance.\n\t        The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n\t        and X_2 ~ N(mu_2, C_2) is\n\t                d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n\t        Stable version by Dougal J. Sutherland.\n\t        Params:\n\t        -- mu1   : Numpy array containing the activations of a layer of the\n", "                   inception net (like returned by the function 'get_predictions')\n\t                   for generated samples.\n\t        -- mu2   : The sample mean over activations, precalculated on an\n\t                   representative data set.\n\t        -- sigma1: The covariance matrix over activations for generated samples.\n\t        -- sigma2: The covariance matrix over activations, precalculated on an\n\t                   representative data set.\n\t        Returns:\n\t        --   : The Frechet Distance.\n\t        \"\"\"\n", "        if not is_master():\n\t            return 0\n\t        mu1, mu2, sigma1, sigma2 = (to_numpy(x) for x in (mu1, mu2, sigma1, sigma2))\n\t        mu1 = np.atleast_1d(mu1)\n\t        mu2 = np.atleast_1d(mu2)\n\t        sigma1 = np.atleast_2d(sigma1)\n\t        sigma2 = np.atleast_2d(sigma2)\n\t        assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n\t        assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n\t        diff = mu1 - mu2\n", "        # Product might be almost singular\n\t        covmean = scipy.linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n\t        if not np.isfinite(covmean).all():\n\t            print(f'fid calculation produces singular product;  adding {eps} to diagonal of cov estimates')\n\t            offset = np.eye(sigma1.shape[0]) * eps\n\t            covmean = scipy.linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\t        # Numerical error might give slight imaginary component\n\t        if np.iscomplexobj(covmean):\n\t            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n\t                m = np.max(np.abs(covmean.imag))\n", "                raise ValueError(f'Imaginary component {m}')\n\t            covmean = covmean.real\n\t        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n"]}
{"filename": "lib/eval/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\tfrom .fid import *  # noqa\n"]}
{"filename": "lib/nn/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\tfrom . import functional  # noqa\n\tfrom . import ncsnpp  # noqa\n\tfrom .nn import *  # noqa\n"]}
{"filename": "lib/nn/nn.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['AutoNorm', 'CondAffinePost', 'CondAffineScaleThenOffset', 'CondLinearlyCombine', 'EMA',\n\t           'EmbeddingTriangle', 'Residual']\n\tfrom typing import Callable, Optional, Tuple, Sequence\n\timport torch\n\timport torch.nn\n\timport torch.nn.functional\n", "from .functional import expand_to\n\tclass AutoNorm(torch.nn.Module):\n\t    def __init__(self, n: int, momentum: float):\n\t        super().__init__()\n\t        self.avg = EMA((n,), momentum)\n\t        self.var = EMA((n,), momentum)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        pad = [1] * (x.ndim - 2)\n\t        if self.training:\n\t            reduce = tuple(i for i in range(x.ndim) if i != 1)\n", "            avg = self.avg(x.mean(reduce))\n\t            var = self.var((x - x.mean(reduce, keepdims=True)).square().mean(reduce))\n\t        else:\n\t            avg, var = self.avg(), self.var()\n\t        return (x - avg.view(1, -1, *pad)) * var.clamp(1e-6).rsqrt().view(1, -1, *pad)\n\t    def denorm(self, x: torch.Tensor) -> torch.Tensor:\n\t        assert not self.training\n\t        pad = [1] * (x.ndim - 2)\n\t        avg, var = self.avg(), self.var()\n\t        return avg.view(1, -1, *pad) + x * var.clamp(1e-6).sqrt().view(1, -1, *pad)\n", "class CondAffinePost(torch.nn.Module):\n\t    def __init__(self, ncond: int, nout: int, op: torch.nn.Module, scale: bool = True):\n\t        super().__init__()\n\t        self.op = op\n\t        self.scale = scale\n\t        self.m = torch.nn.Linear(ncond, nout + (nout if scale else 0))\n\t        self.cond: Optional[torch.Tensor] = None\n\t    def set_cond(self, x: Optional[torch.Tensor]):\n\t        self.cond = x if x is None else self.m(x)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n", "        if self.scale:\n\t            w, b = expand_to(self.cond, x).chunk(2, dim=1)\n\t            return self.op(x) * w + b\n\t        return self.op(x) + expand_to(self.cond, x)\n\tclass CondAffineScaleThenOffset(torch.nn.Module):\n\t    def __init__(self, ncond: int, nin: int, nout: int, op: torch.nn.Module, scale: bool = True):\n\t        super().__init__()\n\t        self.op = op\n\t        self.nin = nin\n\t        self.scale = scale\n", "        self.m = torch.nn.Linear(ncond, nout + (nin if scale else 0))\n\t        self.cond: Optional[torch.Tensor] = None\n\t    def set_cond(self, x: Optional[torch.Tensor]):\n\t        self.cond = x if x is None else self.m(x)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        cond = expand_to(self.cond, x)\n\t        if self.scale:\n\t            w, b = cond[:, :self.nin], cond[:, self.nin:]\n\t            return self.op(x * w) + b\n\t        return self.op(x) + cond\n", "class CondLinearlyCombine(torch.nn.Module):\n\t    def __init__(self, ncond: int, n: int):\n\t        super().__init__()\n\t        self.n = n\n\t        self.mix = torch.nn.Linear(ncond, n)\n\t        self.cond: Optional[torch.Tensor] = None\n\t    def set_cond(self, x: Optional[torch.Tensor]):\n\t        self.cond = x if x is None else self.mix(x)\n\t    def forward(self, x: Sequence[torch.Tensor]) -> torch.Tensor:\n\t        cond = expand_to(self.cond, x[0])\n", "        return sum(x[i] * cond[:, i:i + 1] for i in range(self.n))\n\tclass EMA(torch.nn.Module):\n\t    def __init__(self, shape: Tuple[int, ...], momentum: float):\n\t        super().__init__()\n\t        self.momentum = momentum\n\t        self.register_buffer('step', torch.zeros((), dtype=torch.long))\n\t        self.register_buffer('ema', torch.zeros(shape))\n\t    def forward(self, x: Optional[torch.Tensor] = None) -> torch.Tensor:\n\t        if self.training:\n\t            self.step.add_(1)\n", "            mu = 1 - (1 - self.momentum) / (1 - self.momentum ** self.step)\n\t            self.ema.add_((1 - mu) * (x - self.ema))\n\t        return self.ema\n\tclass EmbeddingTriangle(torch.nn.Module):\n\t    def __init__(self, dim: int, delta: float):\n\t        \"\"\"dim number of dimensions for embedding, delta is minimum distance between two values.\"\"\"\n\t        super().__init__()\n\t        logres = -torch.tensor(max(2 ** -31, 2 * delta)).log2()\n\t        logfreqs = torch.nn.functional.pad(torch.linspace(0, logres, dim - 1), (1, 0), mode='constant', value=-1)\n\t        self.register_buffer('freq', torch.pow(2, logfreqs))\n", "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        y = 2 * (x.view(-1, 1) * self.freq).fmod(1)\n\t        return 2 * (y * (y < 1) + (2 - y) * (y >= 1)) - 1\n\tclass Residual(torch.nn.Module):\n\t    def __init__(self, residual: Callable, skip: Optional[Callable] = None):\n\t        super().__init__()\n\t        self.residual = residual\n\t        self.skip = torch.nn.Identity() if skip is None else skip\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        return self.skip(x) + self.residual(x)\n"]}
{"filename": "lib/nn/ncsnpp/layerspp.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t# This file is adapted and modified from https://github.com/yang-song/score_sde_pytorch.\n\t\"\"\"Layers for defining NCSN++.\"\"\"\n\tfrom . import layers\n\tfrom . import up_or_down_sampling\n\timport torch.nn as nn\n\timport torch\n", "import torch.nn.functional as F\n\timport numpy as np\n\tconv1x1 = layers.ddpm_conv1x1\n\tconv3x3 = layers.ddpm_conv3x3\n\tNIN = layers.NIN\n\tdefault_init = layers.default_init\n\tclass GaussianFourierProjection(nn.Module):\n\t    \"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"\n\t    def __init__(self, embedding_size=256, scale=1.0):\n\t        super().__init__()\n", "        self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n\t    def forward(self, x):\n\t        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n\t        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n\tclass Combine(nn.Module):\n\t    \"\"\"Combine information from skip connections.\"\"\"\n\t    def __init__(self, dim1, dim2, method='cat'):\n\t        super().__init__()\n\t        self.Conv_0 = conv1x1(dim1, dim2)\n\t        self.method = method\n", "    def forward(self, x, y):\n\t        h = self.Conv_0(x)\n\t        if self.method == 'cat':\n\t            return torch.cat([h, y], dim=1)\n\t        elif self.method == 'sum':\n\t            return h + y\n\t        else:\n\t            raise ValueError(f'Method {self.method} not recognized.')\n\tclass AttnBlockpp(nn.Module):\n\t    \"\"\"Channel-wise self-attention block. Modified from DDPM.\"\"\"\n", "    def __init__(self, channels, skip_rescale=False, init_scale=0.):\n\t        super().__init__()\n\t        self.GroupNorm_0 = nn.GroupNorm(num_groups=min(channels // 4, 32), num_channels=channels,\n\t                                        eps=1e-6)\n\t        self.NIN_0 = NIN(channels, channels)\n\t        self.NIN_1 = NIN(channels, channels)\n\t        self.NIN_2 = NIN(channels, channels)\n\t        self.NIN_3 = NIN(channels, channels, init_scale=init_scale)\n\t        self.skip_rescale = skip_rescale\n\t    def forward(self, x):\n", "        B, C, H, W = x.shape\n\t        h = self.GroupNorm_0(x)\n\t        q = self.NIN_0(h)\n\t        k = self.NIN_1(h)\n\t        v = self.NIN_2(h)\n\t        w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n\t        w = torch.reshape(w, (B, H, W, H * W))\n\t        w = F.softmax(w, dim=-1)\n\t        w = torch.reshape(w, (B, H, W, H, W))\n\t        h = torch.einsum('bhwij,bcij->bchw', w, v)\n", "        h = self.NIN_3(h)\n\t        if not self.skip_rescale:\n\t            return x + h\n\t        else:\n\t            return (x + h) / np.sqrt(2.)\n\tclass Upsample(nn.Module):\n\t    def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False, fir_kernel=(1, 3, 3, 1)):\n\t        super().__init__()\n\t        out_ch = out_ch if out_ch else in_ch\n\t        if not fir:\n", "            if with_conv:\n\t                self.Conv_0 = conv3x3(in_ch, out_ch)\n\t        else:\n\t            if with_conv:\n\t                self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch, kernel=3, up=True, resample_kernel=fir_kernel, use_bias=True, kernel_init=default_init())\n\t        self.fir = fir\n\t        self.with_conv = with_conv\n\t        self.fir_kernel = fir_kernel\n\t        self.out_ch = out_ch\n\t    def forward(self, x):\n", "        B, C, H, W = x.shape\n\t        if not self.fir:\n\t            h = F.interpolate(x, (H * 2, W * 2), 'nearest')\n\t            if self.with_conv:\n\t                h = self.Conv_0(h)\n\t        else:\n\t            if not self.with_conv:\n\t                h = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n\t            else:\n\t                h = self.Conv2d_0(x)\n", "        return h\n\tclass Downsample(nn.Module):\n\t    def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False,\n\t                 fir_kernel=(1, 3, 3, 1)):\n\t        super().__init__()\n\t        out_ch = out_ch if out_ch else in_ch\n\t        if not fir:\n\t            if with_conv:\n\t                self.Conv_0 = conv3x3(in_ch, out_ch, stride=2, padding=0)\n\t        else:\n", "            if with_conv:\n\t                self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch, kernel=3, down=True, resample_kernel=fir_kernel, use_bias=True, kernel_init=default_init())\n\t        self.fir = fir\n\t        self.fir_kernel = fir_kernel\n\t        self.with_conv = with_conv\n\t        self.out_ch = out_ch\n\t    def forward(self, x):\n\t        B, C, H, W = x.shape\n\t        if not self.fir:\n\t            if self.with_conv:\n", "                x = F.pad(x, (0, 1, 0, 1))\n\t                x = self.Conv_0(x)\n\t            else:\n\t                x = F.avg_pool2d(x, 2, stride=2)\n\t        else:\n\t            if not self.with_conv:\n\t                x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n\t            else:\n\t                x = self.Conv2d_0(x)\n\t        return x\n", "class ResnetBlockDDPMpp(nn.Module):\n\t    \"\"\"ResBlock adapted from DDPM.\"\"\"\n\t    def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1, skip_rescale=False, init_scale=0.):\n\t        super().__init__()\n\t        out_ch = out_ch if out_ch else in_ch\n\t        self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n\t        self.Conv_0 = conv3x3(in_ch, out_ch)\n\t        if temb_dim is not None:\n\t            self.Dense_0 = nn.Linear(temb_dim, out_ch)\n\t            self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n", "            nn.init.zeros_(self.Dense_0.bias)\n\t        self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n\t        self.Dropout_0 = nn.Dropout(dropout)\n\t        self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n\t        if in_ch != out_ch:\n\t            if conv_shortcut:\n\t                self.Conv_2 = conv3x3(in_ch, out_ch)\n\t            else:\n\t                self.NIN_0 = NIN(in_ch, out_ch)\n\t        self.skip_rescale = skip_rescale\n", "        self.act = act\n\t        self.out_ch = out_ch\n\t        self.conv_shortcut = conv_shortcut\n\t    def forward(self, x, temb=None):\n\t        h = self.act(self.GroupNorm_0(x))\n\t        h = self.Conv_0(h)\n\t        if temb is not None:\n\t            h += self.Dense_0(self.act(temb))[:, :, None, None]\n\t        h = self.act(self.GroupNorm_1(h))\n\t        h = self.Dropout_0(h)\n", "        h = self.Conv_1(h)\n\t        if x.shape[1] != self.out_ch:\n\t            if self.conv_shortcut:\n\t                x = self.Conv_2(x)\n\t            else:\n\t                x = self.NIN_0(x)\n\t        if not self.skip_rescale:\n\t            return x + h\n\t        else:\n\t            return (x + h) / np.sqrt(2.)\n", "class ResnetBlockBigGANpp(nn.Module):\n\t    def __init__(self, act, in_ch, out_ch=None, temb_dim=None, up=False, down=False,\n\t                 dropout=0.1, fir=False, fir_kernel=(1, 3, 3, 1),\n\t                 skip_rescale=True, init_scale=0.):\n\t        super().__init__()\n\t        out_ch = out_ch if out_ch else in_ch\n\t        self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n\t        self.up = up\n\t        self.down = down\n\t        self.fir = fir\n", "        self.fir_kernel = fir_kernel\n\t        self.Conv_0 = conv3x3(in_ch, out_ch)\n\t        if temb_dim is not None:\n\t            self.Dense_0 = nn.Linear(temb_dim, out_ch)\n\t            self.Dense_0.weight.data = default_init()(self.Dense_0.weight.shape)\n\t            nn.init.zeros_(self.Dense_0.bias)\n\t        self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n\t        self.Dropout_0 = nn.Dropout(dropout)\n\t        self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n\t        if in_ch != out_ch or up or down:\n", "            self.Conv_2 = conv1x1(in_ch, out_ch)\n\t        self.skip_rescale = skip_rescale\n\t        self.act = act\n\t        self.in_ch = in_ch\n\t        self.out_ch = out_ch\n\t    def forward(self, x, temb=None):\n\t        h = self.act(self.GroupNorm_0(x))\n\t        if self.up:\n\t            if self.fir:\n\t                h = up_or_down_sampling.upsample_2d(h, self.fir_kernel, factor=2)\n", "                x = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n\t            else:\n\t                h = up_or_down_sampling.naive_upsample_2d(h, factor=2)\n\t                x = up_or_down_sampling.naive_upsample_2d(x, factor=2)\n\t        elif self.down:\n\t            if self.fir:\n\t                h = up_or_down_sampling.downsample_2d(h, self.fir_kernel, factor=2)\n\t                x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n\t            else:\n\t                h = up_or_down_sampling.naive_downsample_2d(h, factor=2)\n", "                x = up_or_down_sampling.naive_downsample_2d(x, factor=2)\n\t        h = self.Conv_0(h)\n\t        # Add bias to each feature map conditioned on the time embedding\n\t        if temb is not None:\n\t            h += self.Dense_0(self.act(temb))[:, :, None, None]\n\t        h = self.act(self.GroupNorm_1(h))\n\t        h = self.Dropout_0(h)\n\t        h = self.Conv_1(h)\n\t        if self.in_ch != self.out_ch or self.up or self.down:\n\t            x = self.Conv_2(x)\n", "        if not self.skip_rescale:\n\t            return x + h\n\t        else:\n\t            return (x + h) / np.sqrt(2.)\n"]}
{"filename": "lib/nn/ncsnpp/layers.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t# This file is adapted and modified from https://github.com/yang-song/score_sde_pytorch.\n\timport math\n\timport string\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n", "import torch.nn.functional as F\n\tdef variance_scaling(scale, mode, distribution,\n\t                     in_axis=1, out_axis=0,\n\t                     dtype=torch.float32,\n\t                     device='cpu'):\n\t    def _compute_fans(shape, in_axis=1, out_axis=0):\n\t        receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n\t        fan_in = shape[in_axis] * receptive_field_size\n\t        fan_out = shape[out_axis] * receptive_field_size\n\t        return fan_in, fan_out\n", "    def init(shape, dtype=dtype, device=device):\n\t        fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n\t        if mode == \"fan_in\":\n\t            denominator = fan_in\n\t        elif mode == \"fan_out\":\n\t            denominator = fan_out\n\t        elif mode == \"fan_avg\":\n\t            denominator = (fan_in + fan_out) / 2\n\t        else:\n\t            raise ValueError(\n", "                \"invalid mode for variance scaling initializer: {}\".format(mode))\n\t        variance = scale / denominator\n\t        if distribution == \"normal\":\n\t            return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n\t        elif distribution == \"uniform\":\n\t            return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n\t        else:\n\t            raise ValueError(\"invalid distribution for variance scaling initializer\")\n\t    return init\n\tdef default_init(scale=1.):\n", "    \"\"\"Initialize the same way as per DDPM.\"\"\"\n\t    scale = 1e-10 if scale == 0 else scale\n\t    return variance_scaling(scale, 'fan_avg', 'uniform')\n\tdef get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n\t    \"\"\"Return timestep embedding for positional embeddings.\"\"\"\n\t    assert len(timesteps.shape) == 1\n\t    half_dim = embedding_dim // 2\n\t    # magic number 10000 is from transformers\n\t    emb = math.log(max_positions) / (half_dim - 1)\n\t    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n", "    emb = timesteps.float()[:, None] * emb[None, :]\n\t    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n\t    if embedding_dim % 2 == 1:  # zero pad\n\t        emb = F.pad(emb, (0, 1), mode='constant')\n\t    assert emb.shape == (timesteps.shape[0], embedding_dim)\n\t    return emb\n\tdef ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1., padding=0):\n\t    \"\"\"Return 1x1 convolution with DDPM initialization.\"\"\"\n\t    conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=padding, bias=bias)\n\t    conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n", "    if bias:\n\t        nn.init.zeros_(conv.bias)\n\t    return conv\n\tdef ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n\t    \"\"\"Return 3x3 convolution with DDPM initialization.\"\"\"\n\t    conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n\t                     dilation=dilation, bias=bias)\n\t    conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n\t    if bias:\n\t        nn.init.zeros_(conv.bias)\n", "    return conv\n\tdef _einsum(a, b, c, x, y):\n\t    einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n\t    return torch.einsum(einsum_str, x, y)\n\tdef contract_inner(x, y):\n\t    \"\"\"Return tensordot(x, y, 1).\"\"\"\n\t    x_chars = list(string.ascii_lowercase[:len(x.shape)])\n\t    y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n\t    y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n\t    out_chars = x_chars[:-1] + y_chars[1:]\n", "    return _einsum(x_chars, y_chars, out_chars, x, y)\n\tclass NIN(nn.Module):\n\t    def __init__(self, in_dim, num_units, init_scale=0.1):\n\t        super().__init__()\n\t        self.W = nn.Parameter(default_init(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n\t        self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n\t    def forward(self, x):\n\t        x = x.permute(0, 2, 3, 1)\n\t        y = contract_inner(x, self.W) + self.b\n\t        return y.permute(0, 3, 1, 2)\n"]}
{"filename": "lib/nn/ncsnpp/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#"]}
{"filename": "lib/nn/ncsnpp/up_or_down_sampling.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t# This file is adapted from https://github.com/yang-song/score_sde_pytorch.\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn import functional as F\n\tdef _setup_kernel(k):\n", "    k = np.asarray(k, dtype=np.float32)\n\t    if k.ndim == 1:\n\t        k = np.outer(k, k)\n\t    k /= np.sum(k)\n\t    assert k.ndim == 2\n\t    assert k.shape[0] == k.shape[1]\n\t    return k\n\tdef _shape(x, dim):\n\t    return x.shape[dim]\n\tdef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n", "    r\"\"\"Pad, upsample, filter, and downsample a batch of 2D images.\n\t    Performs the following sequence of operations for each channel:\n\t    1. Upsample the input by inserting N-1 zeros after each pixel (`up`).\n\t    2. Pad the image with the specified number of zeros on each side (`padding`).\n\t       Negative padding corresponds to cropping the image.\n\t    3. Convolve the image with the specified 2D FIR filter (`f`), shrinking it\n\t       so that the footprint of all output pixels lies within the input image.\n\t    4. Downsample the image by keeping every Nth pixel (`down`).\n\t    This sequence of operations bears close resemblance to scipy.signal.upfirdn().\n\t    It supports gradients of arbitrary order.\n", "    Args:\n\t        input:       Float32/float64/float16 input tensor of the shape\n\t                     `[batch_size, num_channels, in_height, in_width]`.\n\t        kernel:      Float32 FIR filter of the shape\n\t                     `[filter_height, filter_width]` called from _setup_kernel.\n\t        up:          Integer upsampling factor.\n\t        down:        Integer downsampling factor.\n\t        pad:         Padding with respect to the upsampled image. list/tuple `[x, y]`.\n\t    Returns:\n\t        Tensor of the shape `[batch_size, num_channels, out_height, out_width]`.\n", "    \"\"\"\n\t    def upfirdn2d_native(\n\t        input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n\t    ):\n\t        _, channel, in_h, in_w = input.shape\n\t        input = input.reshape(-1, in_h, in_w, 1)\n\t        _, in_h, in_w, minor = input.shape\n\t        kernel_h, kernel_w = kernel.shape\n\t        out = input.view(-1, in_h, 1, in_w, 1, minor)\n\t        out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n", "        out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\t        out = F.pad(\n\t            out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n\t        )\n\t        out = out[\n\t            :,\n\t            max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n\t            max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n\t            :,\n\t        ]\n", "        out = out.permute(0, 3, 1, 2)\n\t        out = out.reshape(\n\t            [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n\t        )\n\t        w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n\t        out = F.conv2d(out, w)\n\t        out = out.reshape(\n\t            -1,\n\t            minor,\n\t            in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n", "            in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n\t        )\n\t        out = out.permute(0, 2, 3, 1)\n\t        out = out[:, ::down_y, ::down_x, :]\n\t        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n\t        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n\t        return out.view(-1, channel, out_h, out_w)\n\t    out = upfirdn2d_native(\n\t        input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]\n\t    )\n", "    return out\n\t# Function ported from StyleGAN2\n\tdef get_weight(module,\n\t               shape,\n\t               weight_var='weight',\n\t               kernel_init=None):\n\t    \"\"\"Get/create weight tensor for a convolution or fully-connected layer.\"\"\"\n\t    return module.param(weight_var, kernel_init, shape)\n\tclass Conv2d(nn.Module):\n\t    \"\"\"Conv2d layer with optimal upsampling and downsampling (StyleGAN2).\"\"\"\n", "    def __init__(self, in_ch, out_ch, kernel, up=False, down=False,\n\t                 resample_kernel=(1, 3, 3, 1),\n\t                 use_bias=True,\n\t                 kernel_init=None):\n\t        super().__init__()\n\t        assert not (up and down)\n\t        assert kernel >= 1 and kernel % 2 == 1\n\t        self.weight = nn.Parameter(torch.zeros(out_ch, in_ch, kernel, kernel))\n\t        if kernel_init is not None:\n\t            self.weight.data = kernel_init(self.weight.data.shape)\n", "        if use_bias:\n\t            self.bias = nn.Parameter(torch.zeros(out_ch))\n\t        self.up = up\n\t        self.down = down\n\t        self.resample_kernel = resample_kernel\n\t        self.kernel = kernel\n\t        self.use_bias = use_bias\n\t    def forward(self, x):\n\t        if self.up:\n\t            x = upsample_conv_2d(x, self.weight, k=self.resample_kernel)\n", "        elif self.down:\n\t            x = conv_downsample_2d(x, self.weight, k=self.resample_kernel)\n\t        else:\n\t            x = F.conv2d(x, self.weight, stride=1, padding=self.kernel // 2)\n\t        if self.use_bias:\n\t            x = x + self.bias.reshape(1, -1, 1, 1)\n\t        return x\n\tdef naive_upsample_2d(x, factor=2):\n\t    _N, C, H, W = x.shape\n\t    x = torch.reshape(x, (-1, C, H, 1, W, 1))\n", "    x = x.repeat(1, 1, 1, factor, 1, factor)\n\t    return torch.reshape(x, (-1, C, H * factor, W * factor))\n\tdef naive_downsample_2d(x, factor=2):\n\t    _N, C, H, W = x.shape\n\t    x = torch.reshape(x, (-1, C, H // factor, factor, W // factor, factor))\n\t    return torch.mean(x, dim=(3, 5))\n\tdef upsample_conv_2d(x, w, k=None, factor=2, gain=1):\n\t    \"\"\"Fused `upsample_2d()` followed by `tf.nn.conv2d()`.\n\t    Padding is performed only once at the beginning, not between the\n\t    operations.\n", "    The fused op is considerably more efficient than performing the same\n\t    calculation\n\t    using standard TensorFlow ops. It supports gradients of arbitrary order.\n\t    Args:\n\t        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n\t            C]`.\n\t        w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n\t            outChannels]`. Grouped convolution can be performed by `inChannels =\n\t            x.shape[0] // numGroups`.\n\t        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n", "            (separable). The default is `[1] * factor`, which corresponds to\n\t            nearest-neighbor upsampling.\n\t        factor:       Integer upsampling factor (default: 2).\n\t        gain:         Scaling factor for signal magnitude (default: 1.0).\n\t    Returns:\n\t        Tensor of the shape `[N, C, H * factor, W * factor]` or\n\t        `[N, H * factor, W * factor, C]`, and same datatype as `x`.\n\t    \"\"\"\n\t    assert isinstance(factor, int) and factor >= 1\n\t    # Check weight shape.\n", "    assert len(w.shape) == 4\n\t    convH = w.shape[2]\n\t    convW = w.shape[3]\n\t    inC = w.shape[1]\n\t    w.shape[0]\n\t    assert convW == convH\n\t    # Setup filter kernel.\n\t    if k is None:\n\t        k = [1] * factor\n\t    k = _setup_kernel(k) * (gain * (factor ** 2))\n", "    p = (k.shape[0] - factor) - (convW - 1)\n\t    stride = (factor, factor)\n\t    # Determine data dimensions.\n\t    stride = [1, 1, factor, factor]\n\t    output_shape = ((_shape(x, 2) - 1) * factor + convH, (_shape(x, 3) - 1) * factor + convW)\n\t    output_padding = (output_shape[0] - (_shape(x, 2) - 1) * stride[0] - convH,\n\t                                        output_shape[1] - (_shape(x, 3) - 1) * stride[1] - convW)\n\t    assert output_padding[0] >= 0 and output_padding[1] >= 0\n\t    num_groups = _shape(x, 1) // inC\n\t    # Transpose weights.\n", "    w = torch.reshape(w, (num_groups, -1, inC, convH, convW))\n\t    w = w[..., ::-1, ::-1].permute(0, 2, 1, 3, 4)\n\t    w = torch.reshape(w, (num_groups * inC, -1, convH, convW))\n\t    x = F.conv_transpose2d(x, w, stride=stride, output_padding=output_padding, padding=0)\n\t    return upfirdn2d(x, torch.tensor(k, device=x.device),\n\t                     pad=((p + 1) // 2 + factor - 1, p // 2 + 1))\n\tdef conv_downsample_2d(x, w, k=None, factor=2, gain=1):\n\t    \"\"\"Fused `tf.nn.conv2d()` followed by `downsample_2d()`.\n\t    Padding is performed only once at the beginning, not between the operations.\n\t    The fused op is considerably more efficient than performing the same\n", "    calculation\n\t    using standard TensorFlow ops. It supports gradients of arbitrary order.\n\t    Args:\n\t            x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n\t                C]`.\n\t            w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n\t                outChannels]`. Grouped convolution can be performed by `inChannels =\n\t                x.shape[0] // numGroups`.\n\t            k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n\t                (separable). The default is `[1] * factor`, which corresponds to\n", "                average pooling.\n\t            factor:       Integer downsampling factor (default: 2).\n\t            gain:         Scaling factor for signal magnitude (default: 1.0).\n\t    Returns:\n\t            Tensor of the shape `[N, C, H // factor, W // factor]` or\n\t            `[N, H // factor, W // factor, C]`, and same datatype as `x`.\n\t    \"\"\"\n\t    assert isinstance(factor, int) and factor >= 1\n\t    _outC, _inC, convH, convW = w.shape\n\t    assert convW == convH\n", "    if k is None:\n\t        k = [1] * factor\n\t    k = _setup_kernel(k) * gain\n\t    p = (k.shape[0] - factor) + (convW - 1)\n\t    s = [factor, factor]\n\t    x = upfirdn2d(x, torch.tensor(k, device=x.device),\n\t                  pad=((p + 1) // 2, p // 2))\n\t    return F.conv2d(x, w, stride=s, padding=0)\n\tdef upsample_2d(x, k=None, factor=2, gain=1):\n\t    r\"\"\"Upsample a batch of 2D images with the given filter.\n", "    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n\t    and upsamples each image with the given filter. The filter is normalized so\n\t    that\n\t    if the input pixels are constant, they will be scaled by the specified\n\t    `gain`.\n\t    Pixels outside the image are assumed to be zero, and the filter is padded\n\t    with\n\t    zeros so that its shape is a multiple of the upsampling factor.\n\t    Args:\n\t            x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n", "                C]`.\n\t            k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n\t                (separable). The default is `[1] * factor`, which corresponds to\n\t                nearest-neighbor upsampling.\n\t            factor:       Integer upsampling factor (default: 2).\n\t            gain:         Scaling factor for signal magnitude (default: 1.0).\n\t    Returns:\n\t            Tensor of the shape `[N, C, H * factor, W * factor]`\n\t    \"\"\"\n\t    assert isinstance(factor, int) and factor >= 1\n", "    if k is None:\n\t        k = [1] * factor\n\t    k = _setup_kernel(k) * (gain * (factor ** 2))\n\t    p = k.shape[0] - factor\n\t    return upfirdn2d(x,torch.tensor(k, device=x.device),\n\t                     up=factor, pad=((p + 1) // 2 + factor - 1, p // 2))\n\tdef downsample_2d(x, k=None, factor=2, gain=1):\n\t    r\"\"\"Downsample a batch of 2D images with the given filter.\n\t    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n\t    and downsamples each image with the given filter. The filter is normalized\n", "    so that\n\t    if the input pixels are constant, they will be scaled by the specified\n\t    `gain`.\n\t    Pixels outside the image are assumed to be zero, and the filter is padded\n\t    with\n\t    zeros so that its shape is a multiple of the downsampling factor.\n\t    Args:\n\t            x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n\t                C]`.\n\t            k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n", "                (separable). The default is `[1] * factor`, which corresponds to\n\t                average pooling.\n\t            factor:       Integer downsampling factor (default: 2).\n\t            gain:         Scaling factor for signal magnitude (default: 1.0).\n\t    Returns:\n\t            Tensor of the shape `[N, C, H // factor, W // factor]`\n\t    \"\"\"\n\t    assert isinstance(factor, int) and factor >= 1\n\t    if k is None:\n\t        k = [1] * factor\n", "    k = _setup_kernel(k) * gain\n\t    p = k.shape[0] - factor\n\t    return upfirdn2d(x, torch.tensor(k, device=x.device),\n\t                     down=factor, pad=((p + 1) // 2, p // 2))\n"]}
{"filename": "lib/nn/functional/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\tfrom .functional import *  # noqa\n"]}
{"filename": "lib/nn/functional/functional.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t__all__ = ['expand_to', 'float_index', 'label_smoothing', 'set_bn_momentum', 'set_cond', 'set_dropout',\n\t           'default', 'log']\n\tfrom typing import Optional\n\timport torch\n\timport torch.nn.functional\n\tdef expand_to(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n", "    \"\"\"Expand x to the number of dimensions in y.\"\"\"\n\t    return x.view(x.shape + (1,) * (y.ndim - x.ndim))\n\tdef float_index(x: torch.Tensor, i: torch.Tensor) -> torch.Tensor:\n\t    a, b = x[i.long()], x[i.ceil().long()]\n\t    return a + i.frac() * (b - a)\n\tdef label_smoothing(x: torch.Tensor, q: float) -> torch.Tensor:\n\t    u = torch.zeros_like(x) + 1 / x.shape[-1]\n\t    return x + q * (u - x)\n\tdef set_bn_momentum(m: torch.nn.Module, momentum: float):\n\t    if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n", "        print('Set momentum for', m)\n\t        m.momentum = momentum\n\tdef set_cond(cond: Optional[torch.Tensor]):\n\t    def apply_op(m: torch.nn.Module):\n\t        if hasattr(m, 'set_cond'):\n\t            m.set_cond(cond)\n\t    return apply_op\n\tdef set_dropout(m: torch.nn.Module, p: float):\n\t    if isinstance(m, torch.nn.modules.dropout._DropoutNd):\n\t        print(f'Set dropout to {p} for', m)\n", "        m.p = p\n\tdef default(val, d):\n\t    if val is not None:\n\t        return val\n\t    return d() if callable(d) else d\n\tdef log(t, eps=1e-20):\n\t    return torch.log(t.clamp(min=eps))\n"]}
{"filename": "lib/zoo/unet.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t# Mostly copied from https://github.com/rosinality/denoising-diffusion-pytorch\n\t# modified to match https://github.com/google-research/google-research/blob/master/diffusion_distillation/diffusion_distillation/unet.py\n\timport math\n\tfrom typing import List, Tuple, Optional\n\timport torch\n\tfrom torch import nn\n", "from torch.nn import functional as F\n\tswish = F.silu\n\tdef get_timestep_embedding(timesteps, embedding_dim, max_time=1000.):\n\t    \"\"\"Build sinusoidal embeddings (from Fairseq).\n\t    This matches the implementation in tensor2tensor, but differs slightly\n\t    from the description in Section 3.5 of \"Attention Is All You Need\".\n\t    Args:\n\t      timesteps: jnp.ndarray: generate embedding vectors at these timesteps\n\t      embedding_dim: int: dimension of the embeddings to generate\n\t      max_time: float: largest time input\n", "      dtype: data type of the generated embeddings\n\t    Returns:\n\t      embedding vectors with shape `(len(timesteps), embedding_dim)`\n\t    \"\"\"\n\t    assert len(timesteps.shape) == 1\n\t    timesteps *= (1000. / max_time)\n\t    half_dim = embedding_dim // 2\n\t    emb = math.log(10000) / (half_dim - 1)\n\t    emb = torch.exp(torch.arange(half_dim) * -emb)\n\t    emb = timesteps.float()[:, None] * emb[None, :].to(timesteps)\n", "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n\t    assert emb.shape == (timesteps.shape[0], embedding_dim)\n\t    return emb\n\t@torch.no_grad()\n\tdef variance_scaling_init_(tensor, scale=1, mode=\"fan_avg\", distribution=\"uniform\"):\n\t    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n\t    if mode == \"fan_in\":\n\t        scale /= fan_in\n\t    elif mode == \"fan_out\":\n\t        scale /= fan_out\n", "    else:\n\t        scale /= (fan_in + fan_out) / 2\n\t    if distribution == \"normal\":\n\t        std = math.sqrt(scale)\n\t        return tensor.normal_(0, std)\n\t    else:\n\t        bound = math.sqrt(3 * scale)\n\t        return tensor.uniform_(-bound, bound)\n\tdef conv2d(\n\t        in_channel,\n", "        out_channel,\n\t        kernel_size,\n\t        stride=1,\n\t        padding=0,\n\t        bias=True,\n\t        scale=1,\n\t        mode=\"fan_avg\",\n\t):\n\t    conv = nn.Conv2d(\n\t        in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias\n", "    )\n\t    variance_scaling_init_(conv.weight, scale, mode=mode)\n\t    if bias:\n\t        nn.init.zeros_(conv.bias)\n\t    return conv\n\tdef linear(in_channel, out_channel, scale=1, mode=\"fan_avg\"):\n\t    lin = nn.Linear(in_channel, out_channel)\n\t    variance_scaling_init_(lin.weight, scale, mode=mode)\n\t    nn.init.zeros_(lin.bias)\n\t    return lin\n", "class Swish(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def forward(self, input):\n\t        return swish(input)\n\tclass Upsample(nn.Sequential):\n\t    def __init__(self, channel):\n\t        layers = [\n\t            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n\t            conv2d(channel, channel, 3, padding=1),\n", "        ]\n\t        super().__init__(*layers)\n\tclass Downsample(nn.Sequential):\n\t    def __init__(self, channel):\n\t        layers = [conv2d(channel, channel, 3, stride=2, padding=1)]\n\t        super().__init__(*layers)\n\tclass ResBlock(nn.Module):\n\t    def __init__(\n\t            self, in_channel, out_channel, time_dim, resample, use_affine_time=False, dropout=0, group_norm=32\n\t    ):\n", "        super().__init__()\n\t        self.use_affine_time = use_affine_time\n\t        self.resample = resample\n\t        time_out_dim = out_channel\n\t        time_scale = 1\n\t        if self.use_affine_time:\n\t            time_out_dim *= 2\n\t            time_scale = 1e-10\n\t        self.norm1 = nn.GroupNorm(group_norm, in_channel)\n\t        self.activation1 = Swish()\n", "        if self.resample:\n\t            self.updown = {\n\t                'up':  nn.Upsample(scale_factor=2, mode=\"nearest\"),\n\t                'down': nn.AvgPool2d(kernel_size=2, stride=2)\n\t            }[self.resample]\n\t        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n\t        self.time = nn.Sequential(\n\t            Swish(), linear(time_dim, time_out_dim, scale=time_scale)\n\t        )\n\t        self.norm2 = nn.GroupNorm(group_norm, out_channel)\n", "        self.activation2 = Swish()\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n\t        if in_channel != out_channel:\n\t            self.skip = conv2d(in_channel, out_channel, 1)\n\t        else:\n\t            self.skip = None\n\t    def forward(self, input, time):\n\t        batch = input.shape[0]\n\t        out = self.norm1(input)\n", "        out = self.activation1(out)\n\t        if self.resample:\n\t            out = self.updown(out)\n\t            input = self.updown(input)\n\t        out = self.conv1(out)\n\t        if self.use_affine_time:\n\t            gamma, beta = self.time(time).view(batch, -1, 1, 1).chunk(2, dim=1)\n\t            out = (1 + gamma) * self.norm2(out) + beta\n\t        else:\n\t            out = out + self.time(time).view(batch, -1, 1, 1)\n", "            out = self.norm2(out)\n\t        out = self.conv2(self.dropout(self.activation2(out)))\n\t        if self.skip is not None:\n\t            input = self.skip(input)\n\t        return out + input\n\tclass SelfAttention(nn.Module):\n\t    def __init__(self, in_channel, n_head=1, head_dim=None, group_norm=32):\n\t        super().__init__()\n\t        if head_dim is None:\n\t            assert n_head is not None\n", "            assert in_channel % n_head == 0\n\t            self.n_head = n_head\n\t            self.head_dim = in_channel // n_head\n\t        else:\n\t            assert n_head is None\n\t            assert in_channel % head_dim == 0\n\t            self.head_dim = head_dim\n\t            self.n_head = in_channel // head_dim\n\t        self.norm = nn.GroupNorm(group_norm, in_channel)\n\t        self.qkv = conv2d(in_channel, in_channel * 3, 1)\n", "        self.out = conv2d(in_channel, in_channel, 1, scale=1e-10)\n\t    def forward(self, input):\n\t        batch, channel, height, width = input.shape\n\t        norm = self.norm(input)\n\t        qkv = self.qkv(norm).view(batch, self.n_head, self.head_dim * 3, height, width)\n\t        query, key, value = qkv.chunk(3, dim=2)  # bhdyx\n\t        attn = torch.einsum(\n\t            \"bnchw, bncyx -> bnhwyx\", query, key\n\t        ).contiguous() / math.sqrt(self.head_dim)\n\t        attn = attn.view(batch, self.n_head, height, width, -1)\n", "        attn = torch.softmax(attn, -1)\n\t        attn = attn.view(batch, self.n_head, height, width, height, width)\n\t        out = torch.einsum(\"bnhwyx, bncyx -> bnchw\", attn, value).contiguous()\n\t        out = self.out(out.view(batch, channel, height, width))\n\t        return out + input\n\tclass ResBlockWithAttention(nn.Module):\n\t    def __init__(\n\t            self,\n\t            in_channel,\n\t            out_channel,\n", "            time_dim,\n\t            dropout,\n\t            resample,\n\t            use_attention=False,\n\t            attention_head: Optional[int] = 1,\n\t            head_dim: Optional[int] = None,\n\t            use_affine_time=False,\n\t            group_norm=32,\n\t    ):\n\t        super().__init__()\n", "        self.resblocks = ResBlock(\n\t            in_channel, out_channel, time_dim, resample, use_affine_time, dropout, group_norm=group_norm\n\t        )\n\t        if use_attention:\n\t            self.attention = SelfAttention(out_channel, n_head=attention_head, head_dim=head_dim, group_norm=group_norm)\n\t        else:\n\t            self.attention = None\n\t    def forward(self, input, time):\n\t        out = self.resblocks(input, time)\n\t        if self.attention is not None:\n", "            out = self.attention(out)\n\t        return out\n\tclass UNet(nn.Module):\n\t    def __init__(\n\t            self,\n\t            in_channel: int,\n\t            channel: int,\n\t            emb_channel: int,\n\t            channel_multiplier: List[int],\n\t            n_res_blocks: int,\n", "            attn_rezs: List[int],\n\t            attn_heads: Optional[int],\n\t            head_dim: Optional[int],\n\t            use_affine_time: bool = False,\n\t            dropout: float = 0,\n\t            num_output: int = 1,\n\t            resample: bool = False,\n\t            init_rez: int = 32,\n\t            logsnr_input_type: str = 'inv_cos',\n\t            logsnr_scale_range: Tuple[float, float] = (-10., 10.),\n", "            num_classes: int = 1\n\t    ):\n\t        super().__init__()\n\t        self.resample = resample\n\t        self.channel = channel\n\t        self.logsnr_input_type = logsnr_input_type\n\t        self.logsnr_scale_range = logsnr_scale_range\n\t        self.num_classes = num_classes\n\t        time_dim = emb_channel\n\t        group_norm = 32\n", "        n_block = len(channel_multiplier)\n\t        if self.num_classes > 1:\n\t            self.class_emb = nn.Linear(self.num_classes, time_dim)\n\t        self.time = nn.Sequential(\n\t            linear(channel, time_dim),\n\t            Swish(),\n\t            linear(time_dim, time_dim),\n\t        )\n\t        down_layers = [conv2d(in_channel, channel, 3, padding=1)]\n\t        feat_channels = [channel]\n", "        in_channel = channel\n\t        cur_rez = init_rez\n\t        for i in range(n_block):\n\t            for _ in range(n_res_blocks):\n\t                channel_mult = channel * channel_multiplier[i]\n\t                down_layers.append(\n\t                    ResBlockWithAttention(\n\t                        in_channel,\n\t                        channel_mult,\n\t                        time_dim,\n", "                        dropout,\n\t                        resample=None,\n\t                        use_attention=cur_rez in attn_rezs,\n\t                        attention_head=attn_heads,\n\t                        head_dim=head_dim,\n\t                        use_affine_time=use_affine_time,\n\t                        group_norm=group_norm\n\t                    )\n\t                )\n\t                feat_channels.append(channel_mult)\n", "                in_channel = channel_mult\n\t            if i != n_block - 1:\n\t                if self.resample:\n\t                    down_layers.append(ResBlock(\n\t                        in_channel,\n\t                        in_channel,\n\t                        time_dim,\n\t                        resample='down',\n\t                        use_affine_time=use_affine_time,\n\t                        dropout=dropout,\n", "                        group_norm=group_norm\n\t                    ))\n\t                else:\n\t                    down_layers.append(Downsample(in_channel))\n\t                cur_rez = cur_rez // 2\n\t                feat_channels.append(in_channel)\n\t        self.down = nn.ModuleList(down_layers)\n\t        self.mid = nn.ModuleList(\n\t            [\n\t                ResBlockWithAttention(\n", "                    in_channel,\n\t                    in_channel,\n\t                    time_dim,\n\t                    resample=None,\n\t                    dropout=dropout,\n\t                    use_attention=True,\n\t                    attention_head=attn_heads,\n\t                    head_dim=head_dim,\n\t                    use_affine_time=use_affine_time,\n\t                    group_norm=group_norm\n", "                ),\n\t                ResBlockWithAttention(\n\t                    in_channel,\n\t                    in_channel,\n\t                    time_dim,\n\t                    resample=None,\n\t                    dropout=dropout,\n\t                    use_affine_time=use_affine_time,\n\t                    group_norm=group_norm\n\t                ),\n", "            ]\n\t        )\n\t        up_layers = []\n\t        for i in reversed(range(n_block)):\n\t            for _ in range(n_res_blocks + 1):\n\t                channel_mult = channel * channel_multiplier[i]\n\t                up_layers.append(\n\t                    ResBlockWithAttention(\n\t                        in_channel + feat_channels.pop(),\n\t                        channel_mult,\n", "                        time_dim,\n\t                        resample=None,\n\t                        dropout=dropout,\n\t                        use_attention=cur_rez in attn_rezs,\n\t                        attention_head=attn_heads,\n\t                        head_dim=head_dim,\n\t                        use_affine_time=use_affine_time,\n\t                        group_norm=group_norm\n\t                    )\n\t                )\n", "                in_channel = channel_mult\n\t            if i != 0:\n\t                if self.resample:\n\t                    up_layers.append(ResBlock(\n\t                        in_channel,\n\t                        in_channel,\n\t                        time_dim,\n\t                        resample='up',\n\t                        use_affine_time=use_affine_time,\n\t                        dropout=dropout,\n", "                        group_norm=group_norm\n\t                    ))\n\t                else:\n\t                    up_layers.append(Upsample(in_channel))\n\t                cur_rez = cur_rez * 2\n\t        self.up = nn.ModuleList(up_layers)\n\t        self.out = nn.Sequential(\n\t            nn.GroupNorm(group_norm, in_channel),\n\t            Swish(),\n\t            conv2d(in_channel, 3 * num_output, 3, padding=1, scale=1e-10),\n", "        )\n\t    def get_time_embed(self, logsnr):\n\t        if self.logsnr_input_type == 'linear':\n\t            logsnr_input = (logsnr - self.logsnr_scale_range[0]) / (self.logsnr_scale_range[1] - self.logsnr_scale_range[0])\n\t        elif self.logsnr_input_type == 'sigmoid':\n\t            logsnr_input = torch.sigmoid(logsnr)\n\t        elif self.logsnr_input_type == 'inv_cos':\n\t            logsnr_input = (torch.arctan(torch.exp(-0.5 * torch.clip(logsnr, -20., 20.))) / (0.5 * torch.pi))\n\t        else:\n\t            raise NotImplementedError(self.logsnr_input_type)\n", "        time_emb = get_timestep_embedding(logsnr_input, embedding_dim=self.channel, max_time=1.)\n\t        time_embed = self.time(time_emb)\n\t        return time_embed\n\t    def forward(self, input, logsnr, y=None):\n\t        time_embed = self.get_time_embed(logsnr)\n\t        # Class embedding\n\t        assert self.num_classes >= 1\n\t        if self.num_classes > 1:\n\t            y_emb = nn.functional.one_hot(y, num_classes=self.num_classes).float()\n\t            y_emb = self.class_emb(y_emb)\n", "            time_embed += y_emb\n\t        del y\n\t        feats = []\n\t        out = input\n\t        for layer in self.down:\n\t            if isinstance(layer, ResBlockWithAttention):\n\t                out = layer(out, time_embed)\n\t            elif isinstance(layer, ResBlock):\n\t                out = layer(out, time_embed)\n\t            else:\n", "                out = layer(out)\n\t            feats.append(out)\n\t        for layer in self.mid:\n\t            out = layer(out, time_embed)\n\t        for layer in self.up:\n\t            if isinstance(layer, ResBlockWithAttention):\n\t                out = layer(torch.cat((out, feats.pop()), 1), time_embed)\n\t            elif isinstance(layer, ResBlock):\n\t                out = layer(out, time_embed)\n\t            else:\n", "                out = layer(out)\n\t        out = self.out(out)\n\t        return out\n"]}
{"filename": "lib/zoo/__init__.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\tfrom . import unet  # noqa"]}
{"filename": "data/image_datasets.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t# Adapted from OpenAI https://github.com/openai/guided-diffusion\n\timport math\n\timport random\n\tfrom PIL import Image\n\timport blobfile as bf\n\timport numpy as np\n", "from torch.utils.data import Dataset\n\tdef _list_image_files_recursively(data_dir):\n\t    results = []\n\t    for entry in sorted(bf.listdir(data_dir)):\n\t        full_path = bf.join(data_dir, entry)\n\t        ext = entry.split(\".\")[-1]\n\t        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n\t            results.append(full_path)\n\t        elif bf.isdir(full_path):\n\t            results.extend(_list_image_files_recursively(full_path))\n", "    return results\n\tclass ImageDataset(Dataset):\n\t    def __init__(\n\t        self,\n\t        resolution,\n\t        data_dir,\n\t        shard=0,\n\t        num_shards=1,\n\t        random_crop=False,\n\t    ):\n", "        super().__init__()\n\t        image_paths = _list_image_files_recursively(data_dir)\n\t        # Assume classes are the first part of the filename,\n\t        # before an underscore.\n\t        class_names = [bf.basename(path).split(\"_\")[0] for path in image_paths]\n\t        sorted_classes = {x: i for i, x in enumerate(sorted(set(class_names)))}\n\t        classes = [sorted_classes[x] for x in class_names]\n\t        self.resolution = resolution\n\t        self.local_images = image_paths[shard:][::num_shards]\n\t        self.local_classes = None if classes is None else classes[shard:][::num_shards]\n", "        self.random_crop = random_crop\n\t    def __len__(self):\n\t        return len(self.local_images)\n\t    def __getitem__(self, idx):\n\t        path = self.local_images[idx]\n\t        with bf.BlobFile(path, \"rb\") as f:\n\t            pil_image = Image.open(f)\n\t            pil_image.load()\n\t        pil_image = pil_image.convert(\"RGB\")\n\t        if self.random_crop:\n", "            arr = random_crop_arr(pil_image, self.resolution)\n\t        else:\n\t            arr = center_crop_arr(pil_image, self.resolution)\n\t        out_dict = {}\n\t        if self.local_classes is not None:\n\t            out_dict[\"y\"] = np.array(self.local_classes[idx], dtype=np.int64)\n\t        return arr, out_dict[\"y\"]\n\tdef center_crop_arr(pil_image, image_size):\n\t    # We are not on a new enough PIL to support the `reducing_gap`\n\t    # argument, which uses BOX downsampling at powers of two first.\n", "    # Thus, we do it by hand to improve downsample quality.\n\t    while min(*pil_image.size) >= 2 * image_size:\n\t        pil_image = pil_image.resize(\n\t            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n\t        )\n\t    scale = image_size / min(*pil_image.size)\n\t    pil_image = pil_image.resize(\n\t        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n\t    )\n\t    arr = np.array(pil_image)\n", "    crop_y = (arr.shape[0] - image_size) // 2\n\t    crop_x = (arr.shape[1] - image_size) // 2\n\t    return arr[crop_y:crop_y + image_size, crop_x:crop_x + image_size]\n\tdef random_crop_arr(pil_image, image_size, min_crop_frac=0.8, max_crop_frac=1.0):\n\t    min_smaller_dim_size = math.ceil(image_size / max_crop_frac)\n\t    max_smaller_dim_size = math.ceil(image_size / min_crop_frac)\n\t    smaller_dim_size = random.randrange(min_smaller_dim_size, max_smaller_dim_size + 1)\n\t    # We are not on a new enough PIL to support the `reducing_gap`\n\t    # argument, which uses BOX downsampling at powers of two first.\n\t    # Thus, we do it by hand to improve downsample quality.\n", "    while min(*pil_image.size) >= 2 * smaller_dim_size:\n\t        pil_image = pil_image.resize(\n\t            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n\t        )\n\t    scale = smaller_dim_size / min(*pil_image.size)\n\t    pil_image = pil_image.resize(\n\t        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n\t    )\n\t    arr = np.array(pil_image)\n\t    crop_y = random.randrange(arr.shape[0] - image_size + 1)\n", "    crop_x = random.randrange(arr.shape[1] - image_size + 1)\n\t    return arr[crop_y:crop_y + image_size, crop_x:crop_x + image_size]\n"]}
{"filename": "fid/fid_zip.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t\"\"\"Compute FID and approximation at 50,000 for zip file of samples.\"\"\"\n\timport time\n\timport zipfile\n\timport lib\n\timport torch\n\timport torchvision.transforms.functional\n", "from absl import app, flags\n\tfrom lib.distributed import auto_distribute, device_id, is_master, world_size\n\tfrom lib.util import FLAGS\n\tfrom PIL import Image\n\t@auto_distribute\n\tdef main(argv):\n\t    def zip_iterator(filename: str, batch: int):\n\t        with zipfile.ZipFile(filename, 'r') as fzip:\n\t            x = []\n\t            fn_list = [fn for fn in fzip.namelist() if fn.endswith('.png')]\n", "            assert len(fn_list) >= FLAGS.fid_len\n\t            for fn in fn_list[device_id()::world_size()]:\n\t                with fzip.open(fn, 'r') as f:\n\t                    y = torchvision.transforms.functional.to_tensor(Image.open(f))\n\t                x.append(2 * y - 1)\n\t                if len(x) == batch:\n\t                    yield torch.stack(x), None\n\t                    x = []\n\t    t0 = time.time()\n\t    data = lib.data.DATASETS[FLAGS.dataset]()\n", "    fake = (x for x in zip_iterator(argv[1], FLAGS.batch // world_size()))\n\t    with torch.no_grad():\n\t        fid = lib.eval.FID(FLAGS.dataset, (3, data.res, data.res))\n\t        fake_activations = fid.data_activations(fake, FLAGS.fid_len)\n\t        fid, fid50 = fid.approximate_fid(fake_activations)\n\t    if is_master():\n\t        print(f'dataset={FLAGS.dataset}')\n\t        print(f'fid{FLAGS.fid_len}={fid}')\n\t        print(f'fid(50000)={fid50}')\n\t        print(f'time={time.time() - t0}')\n", "if __name__ == '__main__':\n\t    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n\t    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n\t    app.run(lib.distributed.main(main))\n"]}
{"filename": "fid/fid_model.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\t\"\"\"Compute FID and approximation at 50,000 for zip file of samples.\"\"\"\n\timport pathlib\n\timport time\n\tfrom types import SimpleNamespace\n\tfrom typing import Optional\n\timport lib\n", "import torch\n\tfrom absl import app, flags\n\tfrom lib.distributed import auto_distribute, device_id\n\tfrom lib.eval.fid import FID\n\tfrom lib.io import Summary, SummaryWriter, zip_batch_as_png\n\tfrom lib.util import FLAGS\n\tfrom lib.zoo.unet import UNet\n\tdef logsnr_schedule_cosine(t, logsnr_min=torch.Tensor([-20.]), logsnr_max=torch.Tensor([20.])):\n\t    b = torch.arctan(torch.exp(-0.5 * logsnr_max))\n\t    a = torch.arctan(torch.exp(-0.5 * logsnr_min)) - b\n", "    return -2. * torch.log(torch.tan(a * t + b))\n\tdef predict_eps_from_x(z, x, logsnr):\n\t    \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n\t    assert logsnr.ndim == x.ndim\n\t    return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))\n\tdef predict_x_from_eps(z, eps, logsnr):\n\t    \"\"\"x = (z - sigma*eps)/alpha.\"\"\"\n\t    assert logsnr.ndim == eps.ndim\n\t    return torch.sqrt(1. + torch.exp(-logsnr)) * (z - eps * torch.rsqrt(1. + torch.exp(logsnr)))\n\tclass ModelFID(torch.nn.Module):\n", "    COLORS = 3\n\t    def __init__(self, name: str, res: int, timesteps: int, **params):\n\t        super().__init__()\n\t        self.name = name\n\t        if name == 'cifar10':\n\t            self.model = UNet(in_channel=3,\n\t                              channel=256,\n\t                              emb_channel=1024,\n\t                              channel_multiplier=[1, 1, 1],\n\t                              n_res_blocks=3,\n", "                              attn_rezs=[8, 16],\n\t                              attn_heads=1,\n\t                              head_dim=None,\n\t                              use_affine_time=True,\n\t                              dropout=0.2,\n\t                              num_output=1,\n\t                              resample=True,\n\t                              num_classes=1).to(device_id())\n\t            self.shape = 3, 32, 32\n\t            self.mean_type = 'x'\n", "            self.ckpt_name = 'cifar_original.pt'\n\t            self.num_classes = 1\n\t        elif name == 'imagenet64':\n\t            # imagenet model is class conditional\n\t            self.model = UNet(in_channel=3,\n\t                              channel=192,\n\t                              emb_channel=768,\n\t                              channel_multiplier=[1, 2, 3, 4],\n\t                              n_res_blocks=3,\n\t                              init_rez=64,\n", "                              attn_rezs=[8, 16, 32],\n\t                              attn_heads=None,\n\t                              head_dim=64,\n\t                              use_affine_time=True,\n\t                              dropout=0.,\n\t                              num_output=2,  # predict signal and noise\n\t                              resample=True,\n\t                              num_classes=1000).to(device_id())\n\t            self.shape = 3, 64, 64\n\t            self.mean_type = 'both'\n", "            self.ckpt_name = 'imagenet_original.pt'\n\t            self.num_classes = 1000\n\t        else:\n\t            raise NotImplementedError(name)\n\t        self.params = SimpleNamespace(res=res, timesteps=timesteps, **params)\n\t        self.timesteps = timesteps\n\t        self.logstep = 0\n\t        self.clip_x = True\n\t    @property\n\t    def logdir(self) -> str:\n", "        params = ','.join(f'{k}={v}' for k, v in sorted(vars(self.params).items()))\n\t        return f'{self.__class__.__name__}({params})'\n\t    def initialize_weights(self, logdir: pathlib.Path):\n\t        self.model.load_state_dict(torch.load(self.params.ckpt))\n\t    def run_model(self, z, logsnr, y=None):\n\t        if self.mean_type == 'x':\n\t            model_x = self.model(z.float(), logsnr.float(), y).double()\n\t            logsnr = logsnr[:, None, None, None]\n\t        elif self.mean_type == 'both':\n\t            output = self.model(z.float(), logsnr.float(), y).double()\n", "            model_x, model_eps = output[:, :3], output[:, 3:]\n\t            # reconcile the two predictions\n\t            logsnr = logsnr[:, None, None, None]\n\t            model_x_eps = predict_x_from_eps(z=z, eps=model_eps, logsnr=logsnr)\n\t            wx = torch.sigmoid(-logsnr)\n\t            model_x = wx * model_x + (1. - wx) * model_x_eps\n\t        else:\n\t            raise NotImplementedError(self.mean_type)\n\t        # clipping\n\t        if self.clip_x:\n", "            model_x = torch.clip(model_x, -1., 1.)\n\t        model_eps = predict_eps_from_x(z=z, x=model_x, logsnr=logsnr)\n\t        return {'model_x': model_x,\n\t                'model_eps': model_eps}\n\t    def ddim_step(self, t, z_t, y=None, step=1024):\n\t        logsnr_t = logsnr_schedule_cosine((t+step) / self.timesteps).to(z_t)\n\t        logsnr_s = logsnr_schedule_cosine(t / self.timesteps).to(z_t)\n\t        model_out = self.run_model(z=z_t, logsnr=logsnr_t.repeat(\n\t            z_t.shape[0]), y=y.to(z_t).long() if y is not None else None)\n\t        x_pred_t = model_out['model_x']\n", "        eps_pred_t = model_out['model_eps']\n\t        stdv_s = torch.sqrt(torch.sigmoid(-logsnr_s))\n\t        alpha_s = torch.sqrt(torch.sigmoid(logsnr_s))\n\t        z_s_pred = alpha_s * x_pred_t + stdv_s * eps_pred_t\n\t        return torch.where(torch.Tensor([t]).to(x_pred_t) == 0, x_pred_t, z_s_pred)\n\t    def sample_loop(self, init_x, y=None, step=1024):\n\t        # loop over t = num_steps-1, ..., 0\n\t        image = init_x\n\t        for t in reversed(range(self.timesteps // step)):\n\t            image = self.ddim_step(t * step, image, y, step=step)\n", "        return image\n\t    def forward(self, samples: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n\t        if generator is not None:\n\t            assert generator.device == torch.device('cpu')\n\t            init_x = torch.randn((samples, *self.shape), device='cpu', generator=generator, dtype=torch.double).to(device_id())\n\t        else:\n\t            init_x = torch.randn((samples, *self.shape), dtype=torch.double).to(device_id())\n\t        if self.name == 'imagenet64':\n\t            y = torch.randint(0, self.num_classes, (samples,)).to(device_id())\n\t        else:\n", "            y = None\n\t        return self.sample_loop(init_x, y, step=1 << self.logstep)\n\t@auto_distribute\n\tdef main(_):\n\t    data = lib.data.DATASETS[FLAGS.dataset]()\n\t    model = ModelFID(FLAGS.dataset, data.res, FLAGS.timesteps,\n\t                     batch=FLAGS.batch, fid_len=FLAGS.fid_len, ckpt=FLAGS.ckpt)\n\t    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n\t    model.initialize_weights(logdir)\n\t    model.eval()\n", "    if FLAGS.eval:\n\t        model.eval()\n\t        with torch.no_grad():\n\t            generator = torch.Generator(device='cpu')\n\t            generator.manual_seed(123623113456)\n\t            x = model(4, generator)\n\t        open('debug_fid_model.png', 'wb').write(lib.util.to_png(x.view(2, 2, *x.shape[1:])))\n\t        import numpy as np\n\t        np.save('debug_arr_fid_model.npy', x.detach().cpu().numpy())\n\t        return\n", "    def eval(logstep: int):\n\t        model.logstep = logstep\n\t        summary = Summary()\n\t        t0 = time.time()\n\t        with torch.no_grad():\n\t            fid = FID(FLAGS.dataset, (model.COLORS, model.params.res, model.params.res))\n\t            fake_activations, fake_samples = fid.generate_activations_and_samples(model, FLAGS.fid_len)\n\t            timesteps = model.params.timesteps >> model.logstep\n\t            zip_batch_as_png(fake_samples, logdir / f'samples_{FLAGS.fid_len}_timesteps_{timesteps}.zip')\n\t            fidn, fid50 = fid.approximate_fid(fake_activations)\n", "        summary.scalar('eval/logstep', logstep)\n\t        summary.scalar('eval/timesteps', timesteps)\n\t        summary.scalar(f'eval/fid({FLAGS.fid_len})', fidn)\n\t        summary.scalar('eval/fid(50000)', fid50)\n\t        summary.scalar('system/eval_time', time.time() - t0)\n\t        data_logger.write(summary, logstep)\n\t        if lib.distributed.is_master():\n\t            print(f'Logstep {logstep} Timesteps {timesteps}')\n\t            print(summary)\n\t    with SummaryWriter.create(logdir) as data_logger:\n", "        if FLAGS.denoise_steps:\n\t            logstep = lib.util.ilog2(FLAGS.timesteps // FLAGS.denoise_steps)\n\t            eval(logstep)\n\t        else:\n\t            for logstep in range(lib.util.ilog2(FLAGS.timesteps) + 1):\n\t                eval(logstep)\n\tif __name__ == '__main__':\n\t    flags.DEFINE_bool('eval', False, help='Whether to run model evaluation.')\n\t    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n\t    flags.DEFINE_integer('timesteps', 1024, help='Sampling timesteps.')\n", "    flags.DEFINE_string('dataset', 'cifar10', help='Dataset.')\n\t    flags.DEFINE_integer('denoise_steps', None, help='Denoising timesteps.')\n\t    flags.DEFINE_string('ckpt', None, help='Path to the model checkpoint.')\n\t    app.run(lib.distributed.main(main))\n"]}
{"filename": "fid/compute_fid_stats.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE file.\n\t# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n\t#\n\timport os\n\timport pathlib\n\timport lib\n\timport numpy as np\n\timport torch\n\tfrom absl import app, flags\n", "from lib.distributed import auto_distribute\n\tfrom lib.util import FLAGS, artifact_dir\n\tML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n\t@auto_distribute\n\tdef main(argv):\n\t    data = lib.data.DATASETS[FLAGS.dataset]()\n\t    real = data.make_dataloaders()[1]\n\t    num_samples = len(real) * FLAGS.batch\n\t    with torch.no_grad():\n\t        fid = lib.eval.FID(FLAGS.dataset, (3, data.res, data.res))\n", "        real_activations = fid.data_activations(real, num_samples, cpu=True)\n\t        m_real, s_real = fid.calculate_activation_statistics(real_activations)\n\t    np.save(f'{ML_DATA}/{FLAGS.dataset}_activation_mean.npy', m_real.numpy())\n\t    np.save(f'{ML_DATA}/{FLAGS.dataset}_activation_std.npy', s_real.numpy())\n\tif __name__ == '__main__':\n\t    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n\t    app.run(lib.distributed.main(main))\n"]}
