{"filename": "example.py", "chunked_list": ["from src.linkedin.api import API\n\tfrom src.linkedin.user import User\n\tfrom rich import print\n\timport os\n\tos.environ[\n\t    \"LINKEDIN_TOKEN\"\n\t] = \"AQUsFm3ywriXg7JvgFCXW3uSsnI5lmvhq1cmzbrN0FRQK1FRRbdkxIo7w9SpQ09fb4hxWoO22mJb8_6Llox5PuBxpE8GpweThQy0xqAwcD2Ni8LDFo4q0i_0RG3THsBVDn_WgXRryzvIW0vlCuM2aUQmsvPzQR2fHjqSYb-k5n_nOgA6Ak7SRQ1lcF92yoN-R22M_ovaVXxfhK0MpHIKEe89BJRaSIdA_TPIsBAjO3gQiY2KLz2cZyDV1JXSW5ukeEK-nD2eROv10rWFL-3J4gWiIBw886J-PdArTZquhEyEGUV056ggAtLnevO8gjveNasJ912Rw8Qmc0ZRvoTK5c0ros56ZQ\"\n\tapi = API.from_env()\n\tuser = User()\n\tres = user.create_post(\n", "    \"test from my custom python APIs with two images\",\n\t    images=[\n\t        (\"/home/zuppif/Documents/LinkedInGPT/grogu.jpg\", \"grogu\"),\n\t        (\"/home/zuppif/Documents/LinkedInGPT/grogu_2.png\", \"grogu2\"),\n\t    ],\n\t)\n"]}
{"filename": "gurus/linkedin_ai.py", "chunked_list": ["import sys\n\t# lazy to make a package\n\tsys.path.append(\".\")\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tfrom pathlib import Path\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.chains import LLMChain\n\tfrom langchain.prompts import PromptTemplate\n\tfrom src.guru import Guru\n", "from src.actions.linkedIn import PostOnLinkedInAction\n\tfrom src.storages import SQLiteStorage\n\tfrom src.content_providers import TrendingAIPapersProvider\n\tfrom src.confirmations.input_confirmation import input_confirmation\n\tprompt = PromptTemplate(\n\t    input_variables=[\"content\", \"bot_name\"],\n\t    template=Path(\"prompts/guru.prompt\").read_text(),\n\t)\n\tllm = ChatOpenAI(temperature=0)\n\tchain = LLMChain(llm=llm, prompt=prompt)\n", "guru = Guru(\n\t    name=\"Leonardo\",\n\t    content_provider=TrendingAIPapersProvider(),\n\t    storage=SQLiteStorage(),\n\t    action=PostOnLinkedInAction(),\n\t    confirmation=input_confirmation,\n\t    llm_chain=chain\n\t)\n\tguru.run()"]}
{"filename": "experiments/bot.py", "chunked_list": ["import json\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tfrom pathlib import Path\n\tfrom langchain.agents import Tool\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.memory import ConversationBufferWindowMemory\n\tfrom rich import print\n\tfrom db import insert_post\n\tfrom linkedin.user import User\n", "from logger import logger\n\tfrom tools.linkedIn import write_linkedin_post\n\tfrom tools.papers import get_a_trending_paper_for_a_post\n\tuser = User()\n\tmemory = ConversationBufferWindowMemory(\n\t    memory_key=\"chat_history\", k=5, return_messages=True\n\t)\n\tfrom langchain.chains import LLMChain\n\tfrom langchain.prompts import PromptTemplate\n\tprompt = PromptTemplate(\n", "    input_variables=[\"paper\", \"bot_name\"],\n\t    template=Path(\"prompts/bot.prompt\").read_text(),\n\t)\n\tllm = ChatOpenAI(temperature=0)\n\tpaper = get_a_trending_paper_for_a_post(only_from_db=True)\n\tchain = LLMChain(llm=llm, prompt=prompt)\n\tcontent = chain.run({\"paper\": json.dumps(paper), \"bot_name\": \"Leonardo\"})\n\tprint(paper)\n\tprint(content)\n\tconfirmation = input(\"Proceed? [y/n]:\")\n", "if confirmation == \"y\":\n\t    print(\"Writing...\")\n\t    write_linkedin_post(user, content, media_url=paper[\"media\"])\n"]}
{"filename": "experiments/agent.py", "chunked_list": ["import os\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\timport json\n\timport random\n\tfrom functools import partial\n\tfrom pathlib import Path\n\tfrom typing import List\n\timport requests\n\tfrom bs4 import BeautifulSoup\n", "from langchain.agents import Tool, initialize_agent, load_tools\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.experimental import AutoGPT\n\tfrom langchain.memory import (ConversationBufferMemory,\n\t                              ConversationBufferWindowMemory)\n\tfrom langchain.tools.file_management.read import ReadFileTool\n\tfrom langchain.tools.file_management.write import WriteFileTool\n\tfrom langchain.utilities import (ArxivAPIWrapper, GoogleSearchAPIWrapper,\n\t                                 TextRequestsWrapper)\n\tfrom rich import print\n", "from db import Paper\n\tfrom linkedin.user import User\n\tuser = User()\n\tdef get_paper_info(uid: str, *args, **kwargs):\n\t    response = requests.get(f\"https://paperswithcode.com{uid}\")\n\t    result = {}\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        paper_abstract_div = soup.select_one(\".paper-abstract\")\n\t        # Extract the abstract\n", "        abstract = paper_abstract_div.find(\"p\").text.strip()\n\t        # Extract the arXiv URL\n\t        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\t        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\t    else:\n\t        print(\n\t            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n\t        )\n\t    return result\n\tdef write_linkedin_post(content: str, media_url: str):\n", "    file_path = f\"media.{Path(media_url).suffix}\"\n\t    with open(file_path, \"wb\") as f:\n\t        f.write(requests.get(media_url).content)\n\t    user.create_post(\n\t        content\n\t        + \"\\n#opensource #llms #datascience #machinelearning #programming #ai #ds #python #deeplearning #nlp\",\n\t        [(file_path, \"media\")],\n\t    )\n\t    return \"Post created! Good job!\"\n\tdef get_latest_papers(*args, **kwargs) -> List[Paper]:\n", "    response = requests.get(\"https://paperswithcode.com/\")\n\t    results = []\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        for row in soup.select(\n\t            \".infinite-container .row.infinite-item.item.paper-card\"\n\t        ):\n\t            paper_dict = {\n\t                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n\t                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),\n", "                \"media\": row.select_one(\".item-image\")[\"style\"]\n\t                .split(\"('\")[1]\n\t                .split(\"')\")[0],\n\t                \"tags\": [\n\t                    a.get_text(strip=True) for a in row.select(\".badge-primary a\")\n\t                ],\n\t                \"stars\": int(\n\t                    row.select_one(\".entity-stars .badge\")\n\t                    .get_text(strip=True)\n\t                    .split(\" \")[0]\n", "                    .replace(\",\", \"\")\n\t                ),\n\t                \"github_link\": row.select_one(\".item-github-link a\")[\"href\"],\n\t                \"uid\": row.select_one(\"h1 a\")[\"href\"],\n\t            }\n\t            paper_dict = paper_dict\n\t            results.append({**paper_dict, **get_paper_info(paper_dict[\"uid\"])})\n\t        else:\n\t            print(\"No div element with the specified class was found\")\n\t    else:\n", "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n\t    return results\n\tdef get_a_trending_paper(*args, **kwargs) -> Paper:\n\t    papers = get_latest_papers()\n\t    print(papers)\n\t    paper = random.choice(papers)\n\t    return paper\n\t# write_linkedin_post(\n\t#     \"foo\",\n\t#     \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/3ef6f4ee-4b0b-4654-8bcb-9f0f1299261f.jpg\",\n", "# )\n\t# print(get_a_trending_paper())\n\ttools = [\n\t    Tool(\n\t        name=\"get_a_trending_paper\",\n\t        func=get_a_trending_paper,\n\t        description=\"Use this to find a new and trending AI paper, it returns a JSON with information about a paper.\",\n\t    ),\n\t    #     # Tool(\n\t    #     #     name=\"get_paper_info\",\n", "    #     #     func=get_paper_info,\n\t    #     #     description=\"Use this tool to get the abstract and the arxiv link from a `uid`. It will return a JSON. You need to pass the `uid`\",\n\t    #     # ),\n\t    # Tool(\n\t    #     name=\"write LinkedIn post\",\n\t    #     func=write_linkedin_post,\n\t    #     description=\"Use it to write a LinkedIn post, input is the post's content and an image url.\",\n\t    # ),\n\t]\n\tmemory = ConversationBufferWindowMemory(\n", "    memory_key=\"chat_history\", k=10, return_messages=True\n\t)\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.chains import LLMChain\n\tprompt = PromptTemplate(\n\t    input_variables=[\"paper\"],\n\t    template=Path(\"goal_agent.prompt\").read_text(),\n\t)\n\tllm = ChatOpenAI(temperature=0)\n\tpaper = get_a_trending_paper()\n", "chain = LLMChain(llm=llm, prompt=prompt)\n\tcontent = chain.run(json.dumps(paper))\n\tprint(paper)\n\tprint(content)\n\t# write_linkedin_post(content, paper['media'])\n\t# conversational_agent = initialize_agent(\n\t#     agent=\"chat-conversational-react-description\",\n\t#     tools=tools,\n\t#     llm=ChatOpenAI(temperature=0),\n\t#     verbose=True,\n", "#     max_iterations=10,\n\t#     early_stopping_method=\"generate\",\n\t#     memory=memory,\n\t# )\n\t# print(conversational_agent.run(Path(\"goal_agent.prompt\").read_text()))\n\t# {\n\t#     'title': 'Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond',\n\t#     'subtitle': 'This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.',\n\t#     'media': 'https://production-media.paperswithcode.com/thumbnails/papergithubrepo/16d0f450-e5e7-4471-9a8b-42727da19551.gif',\n\t#     'tags': [],\n", "#     'stars': 2156,\n\t#     'github_link': 'https://github.com/mooler0410/llmspracticalguide',\n\t#     'uid': '/paper/harnessing-the-power-of-llms-in-practice-a',\n\t#     'abstract': 'This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. \n\t# Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, \n\t# such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to \n\t# understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive\n\t# guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \n\t# \\\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.',\n\t#     'arxiv_link': 'https://arxiv.org/pdf/2304.13712v2.pdf'\n", "# }\n\tUnlock the Power of Large Language Models in NLP Tasks 🤖📝\n\tThis paper provides a practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. Key takeaways include: \n\t▪️ Understanding the influence of pre-training data, training data, and test data on LLMs \n\t▪️ Detailed discussion of use and non-use cases of LLMs for various NLP tasks \n\t▪️ Importance of data and specific challenges associated with each NLP task \n\t▪️ Impact of spurious biases on LLMs and other essential considerations \n\t🔗 GitHub: https://github.com/mooler0410/llmspracticalguide\n\t🔗 Arvix: https://arxiv.org/pdf/2304.13712v2.pdf"]}
{"filename": "experiments/db.py", "chunked_list": ["import sqlite3\n\tfrom typing import List, TypedDict\n\t\"\"\"\n\tBasically some poor man CRUD functions to store papers and post inside a sqlite db.\n\t\"\"\"\n\tclass Paper(TypedDict):\n\t    uid: str\n\t    title: str\n\t    subtitle: str\n\t    abstract: str\n", "    media: str\n\t    tags: List[str]\n\t    stars: int\n\t    github_link: str\n\t    arxiv_link: str\n\tclass Post(TypedDict):\n\t    paper_uid: str\n\t    text: str\n\t    media: str\n\tdef create_connection() -> sqlite3.Connection:\n", "    return sqlite3.connect(\"agent.db\")\n\tdef row_to_dict(cursor, row):\n\t    return {col[0]: row[idx] for idx, col in enumerate(cursor.description)}\n\tdef create_table_papers(conn: sqlite3.Connection):\n\t    cur = conn.cursor()\n\t    cur.execute(\n\t        \"\"\"CREATE TABLE IF NOT EXISTS papers (\n\t                                    uid TEXT PRIMARY KEY,\n\t                                    title TEXT NOT NULL,\n\t                                    subtitle TEXT,\n", "                                    abstract TEXT,\n\t                                    media TEXT,\n\t                                    tags TEXT,\n\t                                    stars INTEGER,\n\t                                    github_link TEXT,\n\t                                    arxiv_link TEXT\n\t                                );\"\"\"\n\t    )\n\tdef delete_paper(conn: sqlite3.Connection, uid: str):\n\t    sql = \"DELETE FROM papers WHERE uid=?\"\n", "    cur = conn.cursor()\n\t    cur.execute(sql, (uid,))\n\t    conn.commit()\n\tdef insert_paper(conn: sqlite3.Connection, paper: Paper):\n\t    sql = \"\"\"INSERT OR IGNORE INTO papers(uid, title, subtitle, abstract, media, tags, stars, github_link, arxiv_link)\n\t             VALUES(:uid, :title, :subtitle, :abstract, :media, :tags, :stars, :github_link, :arxiv_link)\"\"\"\n\t    cur = conn.cursor()\n\t    cur.execute(\n\t        sql,\n\t        {\n", "            \"uid\": paper[\"uid\"],\n\t            \"title\": paper[\"title\"],\n\t            \"subtitle\": paper[\"subtitle\"],\n\t            \"abstract\": paper[\"abstract\"],\n\t            \"media\": paper[\"media\"],\n\t            \"tags\": \",\".join(paper[\"tags\"]),\n\t            \"stars\": paper[\"stars\"],\n\t            \"github_link\": paper[\"github_link\"],\n\t            \"arxiv_link\": paper[\"arxiv_link\"],\n\t        },\n", "    )\n\t    conn.commit()\n\t    return cur.lastrowid\n\tdef get_all_papers(conn: sqlite3.Connection) -> List[Paper]:\n\t    cur = conn.cursor()\n\t    cur.execute(\"SELECT * FROM papers\")\n\t    rows = cur.fetchall()\n\t    return [row_to_dict(cur, row) for row in rows]\n\tdef get_paper_by_uid(conn: sqlite3.Connection, uid: str) -> Paper:\n\t    cur = conn.cursor()\n", "    cur.execute(\"SELECT * FROM papers WHERE uid=?\", (uid,))\n\t    row = cur.fetchone()\n\t    if row:\n\t        return row_to_dict(cur, row)\n\t    return None\n\tdef create_post_table(conn: sqlite3.Connection):\n\t    cursor = conn.cursor()\n\t    cursor.execute(\n\t        \"\"\"\n\t        CREATE TABLE IF NOT EXISTS posts (\n", "            paper_uid TEXT PRIMARY KEY,\n\t            text TEXT,\n\t            media TEXT\n\t        )\n\t    \"\"\"\n\t    )\n\t    conn.commit()\n\tdef insert_post(conn: sqlite3.Connection, post: Post):\n\t    cursor = conn.cursor()\n\t    cursor.execute(\n", "        \"\"\"\n\t        INSERT INTO posts (paper_uid, text, media) VALUES (?, ?, ?)\n\t    \"\"\",\n\t        (post[\"paper_uid\"], post[\"text\"], post[\"media\"]),\n\t    )\n\t    conn.commit()\n\tdef get_all_posts(conn: sqlite3.Connection) -> List[Post]:\n\t    cursor = conn.cursor()\n\t    cursor.execute(\"SELECT * FROM posts\")\n\t    rows = cursor.fetchall()\n", "    return [row_to_dict(cursor, row) for row in rows]\n\tdef get_all_papers_without_a_post(conn: sqlite3.Connection) -> List[Paper]:\n\t    cursor = conn.cursor()\n\t    cursor.execute(\n\t        \"\"\"\n\t        SELECT papers.* FROM papers\n\t        LEFT JOIN posts ON papers.uid = posts.paper_uid\n\t        WHERE posts.paper_uid IS NULL\n\t    \"\"\"\n\t    )\n", "    rows = cursor.fetchall()\n\t    return [row_to_dict(cursor, row) for row in rows]\n\tdef init_db() -> sqlite3.Connection:\n\t    conn = create_connection()\n\t    create_table_papers(conn)\n\t    create_post_table(conn)\n\t    return conn\n\tif __name__ == \"__main__\":\n\t    conn = create_connection()\n\t    create_table_papers(conn)\n", "    create_post_table(conn)\n\t    insert_paper(\n\t        conn,\n\t        Paper(\n\t            uid=\"foo1\",\n\t            title=\"FOO1\",\n\t            subtitle=\"baa\",\n\t            abstract=\"asddsa\",\n\t            media=\"/media.png\",\n\t            tags=[\"foo\"],\n", "            stars=10,\n\t            github_link=\"github/foo\",\n\t            arxiv_link=\"arxiv/foo\",\n\t        ),\n\t    )\n\t    insert_paper(\n\t        conn,\n\t        Paper(\n\t            uid=\"foo2\",\n\t            title=\"FOO2\",\n", "            subtitle=\"baa\",\n\t            abstract=\"asddsa\",\n\t            media=\"/media.png\",\n\t            tags=[\"foo\"],\n\t            stars=10,\n\t            github_link=\"github/foo\",\n\t            arxiv_link=\"arxiv/foo\",\n\t        ),\n\t    )\n\t    papers = get_all_papers(conn)\n", "    post = Post(paper_uid=papers[0][\"uid\"], text=\"a post\", media=\"example.com\")\n\t    insert_post(conn, post)\n\t    print(get_all_posts(conn))\n\t    print(get_all_papers_without_a_post(conn))\n"]}
{"filename": "experiments/auto_gpt.py", "chunked_list": ["import os\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\timport json\n\timport random\n\tfrom pathlib import Path\n\tfrom typing import List\n\timport faiss\n\timport requests\n\tfrom bs4 import BeautifulSoup\n", "from langchain.agents import Tool\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.docstore import InMemoryDocstore\n\tfrom langchain.embeddings import OpenAIEmbeddings\n\tfrom langchain.experimental import AutoGPT\n\tfrom langchain.memory import ConversationBufferWindowMemory\n\tfrom langchain.vectorstores import FAISS\n\tfrom rich import print\n\tfrom functools import cache\n\tfrom db import Paper\n", "from linkedin.user import User\n\tuser = User()\n\tdef get_paper_info(uid: str, *args, **kwargs):\n\t    response = requests.get(f\"https://paperswithcode.com{uid}\")\n\t    result = {}\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        paper_abstract_div = soup.select_one(\".paper-abstract\")\n\t        # Extract the abstract\n\t        abstract = paper_abstract_div.find(\"p\").text.strip()\n", "        # Extract the arXiv URL\n\t        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\t        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\t    else:\n\t        print(\n\t            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n\t        )\n\t    return result\n\tdef write_linkedin_post(content: str, media_url: str):\n\t    file_path = f\"media.{Path(media_url).suffix}\"\n", "    with open(file_path, \"wb\") as f:\n\t        f.write(requests.get(media_url).content)\n\t    user.create_post(\n\t        content\n\t        + \"\\n#opensource #llms #datascience #machinelearning #programming #ai #ds #python #deeplearning #nlp\",\n\t        [(file_path, \"media\")],\n\t    )\n\t    return \"Post created! Good job!\"\n\tdef get_latest_papers(*args, **kwargs) -> List[Paper]:\n\t    response = requests.get(\"https://paperswithcode.com/\")\n", "    results = []\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        for row in soup.select(\n\t            \".infinite-container .row.infinite-item.item.paper-card\"\n\t        ):\n\t            paper_dict = {\n\t                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n\t                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),\n\t                \"media\": row.select_one(\".item-image\")[\"style\"]\n", "                .split(\"('\")[1]\n\t                .split(\"')\")[0],\n\t                \"tags\": [\n\t                    a.get_text(strip=True) for a in row.select(\".badge-primary a\")\n\t                ],\n\t                \"stars\": int(\n\t                    row.select_one(\".entity-stars .badge\")\n\t                    .get_text(strip=True)\n\t                    .split(\" \")[0]\n\t                    .replace(\",\", \"\")\n", "                ),\n\t                \"github_link\": row.select_one(\".item-github-link a\")[\"href\"],\n\t                \"uid\": row.select_one(\"h1 a\")[\"href\"],\n\t            }\n\t            paper_dict = paper_dict\n\t            results.append({**paper_dict, **get_paper_info(paper_dict[\"uid\"])})\n\t        else:\n\t            print(\"No div element with the specified class was found\")\n\t    else:\n\t        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n", "    return results\n\t@cache\n\tdef get_a_trending_paper(*args, **kwargs):\n\t    papers = get_latest_papers()\n\t    print(papers)\n\t    paper = random.choice(papers)\n\t    return json.dumps(paper)\n\t# write_linkedin_post(\n\t#     \"foo\",\n\t#     \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/3ef6f4ee-4b0b-4654-8bcb-9f0f1299261f.jpg\",\n", "# )\n\t# print(get_a_trending_paper())\n\ttools = [\n\t    Tool(\n\t        name=\"get_a_trending_paper\",\n\t        func=get_a_trending_paper,\n\t        description=\"Use this to find a new and trending AI paper, it returns a JSON with information about a paper.\",\n\t    ),\n\t    #     # Tool(\n\t    #     #     name=\"get_paper_info\",\n", "    #     #     func=get_paper_info,\n\t    #     #     description=\"Use this tool to get the abstract and the arxiv link from a `uid`. It will return a JSON. You need to pass the `uid`\",\n\t    #     # ),\n\t    Tool(\n\t        name=\"write LinkedIn post\",\n\t        func=write_linkedin_post,\n\t        description=\"Use it to write a LinkedIn post, inputs are `content`, the post text and `media_url`, a link to a media from the paper. \",\n\t    ),\n\t]\n\tmemory = ConversationBufferWindowMemory(\n", "    memory_key=\"chat_history\", k=3, return_messages=True\n\t)\n\t# Define your embedding model\n\tembeddings_model = OpenAIEmbeddings()\n\t# Initialize the vectorstore as empty\n\tembedding_size = 1536\n\tindex = faiss.IndexFlatL2(embedding_size)\n\tvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n\tagent = AutoGPT.from_llm_and_tools(\n\t    ai_name=\"Jacob\",\n", "    ai_role=\"Assistant\",\n\t    tools=tools,\n\t    llm=ChatOpenAI(temperature=0),\n\t    memory=vectorstore.as_retriever(),\n\t)\n\t# Set verbose to be true\n\t# agent.chain.verbose = True\n\tagent.run([Path(\"goal.prompt\").read_text()])\n"]}
{"filename": "src/types.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import Any, Dict, Optional\n\t@dataclass\n\tclass Content:\n\t    uid: str\n\t    data: Dict[str, Any]\n\t    created: Optional[bool] = False\n\t@dataclass\n\tclass GeneratedContent:\n\t    text: str\n", "    media_url: Optional[str] = None\n"]}
{"filename": "src/logger.py", "chunked_list": ["import logging\n\timport os\n\tfrom rich.logging import RichHandler\n\tlogger = logging.getLogger(\"linkedInGPT\")\n\tlogger.setLevel(os.environ.get(\"LOG_LEVEL\", logging.INFO))\n\tlogger.addHandler(RichHandler())\n"]}
{"filename": "src/__init__.py", "chunked_list": []}
{"filename": "src/guru.py", "chunked_list": ["import random\n\tfrom typing import Callable, List, Optional\n\tfrom langchain import LLMChain\n\tfrom src.actions import Action\n\tfrom src.content_providers import ContentProvider\n\tfrom src.storages import Storage\n\tfrom src.types import Content, GeneratedContent\n\tfrom src.logger import logger\n\tdef random_content_selection_strategy(contents: List[Content]) -> Content:\n\t    return random.choice(contents)\n", "class Guru:\n\t    def __init__(\n\t        self,\n\t        name: str,\n\t        content_provider: ContentProvider,\n\t        storage: Storage,\n\t        action: Action,\n\t        confirmation: Callable[[Callable], bool],\n\t        llm_chain: LLMChain,\n\t        content_selection_strategy: Optional[Callable[[List[Content]], Content]] = None,\n", "    ) -> None:\n\t        self.name = name\n\t        self.content_provider = content_provider\n\t        self.storage = storage\n\t        self.action = action\n\t        self.confirmation = confirmation\n\t        self.llm_chain = llm_chain\n\t        self.content_selection_strategy = (\n\t            random_content_selection_strategy\n\t            if content_selection_strategy is None\n", "            else content_selection_strategy\n\t        )\n\t    def run(self):\n\t        contents = self.content_provider.get_contents()\n\t        list(map(lambda content: self.storage.store(content), contents))\n\t        contents = self.storage.get_all(created=False)\n\t        logger.info(contents)\n\t        content = self.content_selection_strategy(contents)\n\t        generated_text = self.llm_chain.run({\"content\": content.__dict__, \"bot_name\": self.name})\n\t        logger.info(f\"Generated text for content:\\n{generated_text}\")\n", "        if self.confirmation(self.run):\n\t            logger.info(f\"Running action {self.action}\")\n\t            # [TODO] here I know in my setup what 'media' will be inside content because I will get it from paperswithcode\n\t            generated_content = GeneratedContent(generated_text, media_url=content.data['media'])\n\t            self.action(generated_content)\n\t            content.created = True\n\t            self.storage.update(content)\n\t            logger.info(\"Done!\")\n"]}
{"filename": "src/actions/base.py", "chunked_list": ["from typing import Any\n\tfrom src.types import GeneratedContent\n\tclass Action:\n\t    def __call__(self, content: GeneratedContent) -> Any:\n\t        pass\n"]}
{"filename": "src/actions/__init__.py", "chunked_list": ["from .base import Action\n\tfrom .linkedIn import PostOnLinkedInAction\n"]}
{"filename": "src/actions/linkedIn.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Any\n\timport requests\n\tfrom linkedin_python import User\n\tfrom src.types import GeneratedContent\n\tfrom .base import Action\n\tclass PostOnLinkedInAction(Action):\n\t    def __init__(self) -> None:\n\t        self.user = User()\n\t    def __call__(self, content: GeneratedContent):\n", "        if content.media_url is not None:\n\t            media_url = content.media_url\n\t            file_path = f\"media.{Path(media_url).suffix}\"\n\t            with open(file_path, \"wb\") as f:\n\t                f.write(requests.get(media_url).content)\n\t            self.user.create_post(\n\t                content.text,\n\t                [(file_path, \"media\")],\n\t            )\n\t        else:\n", "            self.user.create_post(content.text)\n"]}
{"filename": "src/storages/sqlite.py", "chunked_list": ["import json\n\timport sqlite3\n\tfrom typing import Dict, List, Optional, TypedDict\n\tfrom src.types import Content\n\tfrom .base import Storage\n\tclass SQLiteStorage(Storage):\n\t    def __init__(self, name: str = \"content.db\"):\n\t        self.conn = sqlite3.connect(\"content.db\")\n\t        self._init_table()\n\t    def _init_table(self):\n", "        cursor = self.conn.cursor()\n\t        cursor.execute(\n\t            \"\"\"\n\t        CREATE TABLE IF NOT EXISTS content (\n\t            uid TEXT PRIMARY KEY,\n\t            data TEXT,\n\t            created INTEGER\n\t        )\n\t        \"\"\"\n\t        )\n", "        self.conn.commit()\n\t    def store(self, content: Content):\n\t        cursor = self.conn.cursor()\n\t        cursor.execute(\n\t            \"\"\"\n\t        INSERT OR IGNORE INTO content (uid, data, created) VALUES (?, ?, ?)\n\t        \"\"\",\n\t            (content.uid, json.dumps(content.data), int(content.created)),\n\t        )\n\t        self.conn.commit()\n", "    def update(self, updated_content: Content):\n\t        cursor = self.conn.cursor()\n\t        cursor.execute(\n\t            \"\"\"\n\t        UPDATE content\n\t        SET data = ?, created = ?\n\t        WHERE uid = ?\n\t        \"\"\",\n\t            (\n\t                json.dumps(updated_content.data),\n", "                int(updated_content.created),\n\t                updated_content.uid,\n\t            ),\n\t        )\n\t        self.conn.commit()\n\t    def get_all(self, created: Optional[bool] = None) -> List[Content]:\n\t        cursor = self.conn.cursor()\n\t        if created is None:\n\t            cursor.execute(\"SELECT uid, data, created FROM content\")\n\t        else:\n", "            cursor.execute(\n\t                \"SELECT uid, data, created FROM content WHERE created=?\",\n\t                (int(created),),\n\t            )\n\t        results = cursor.fetchall()\n\t        contents = [\n\t            Content(uid=row[0], data=json.loads(row[1]), created=bool(row[2]))\n\t            for row in results\n\t        ]\n\t        return contents\n", "    def close(self):\n\t        self.conn.close\n"]}
{"filename": "src/storages/base.py", "chunked_list": ["from typing import List\n\tfrom src.types import Content\n\tclass Storage:\n\t    def store(self, content: Content):\n\t        raise NotImplemented\n\t    def update(self, content: Content):\n\t        raise NotImplemented\n\t    def get_all(self) -> List[Content]:\n\t        raise NotImplemented\n\t    def close(self):\n", "        raise NotImplemented\n"]}
{"filename": "src/storages/__init__.py", "chunked_list": ["from .base import Storage\n\tfrom .sqlite import SQLiteStorage\n"]}
{"filename": "src/content_providers/base.py", "chunked_list": ["from typing import List\n\tfrom src.types import Content\n\tclass ContentProvider:\n\t    def get_contents(self) -> List[Content]:\n\t        raise NotImplemented\n"]}
{"filename": "src/content_providers/__init__.py", "chunked_list": ["from .ai_papers.provider import TrendingAIPapersProvider\n\tfrom .base import ContentProvider\n"]}
{"filename": "src/content_providers/ai_papers/__init__.py", "chunked_list": []}
{"filename": "src/content_providers/ai_papers/papers_with_code.py", "chunked_list": ["from random import choice\n\tfrom typing import Dict, List\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom src.logger import logger\n\tfrom src.types import Content\n\tdef get_paper_info_from_papers_with_code(uid: str, *args, **kwargs) -> Dict:\n\t    \"\"\"\n\t    Fetching the paper little summary page on papers with code, e.g.\n\t    https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos\n", "    This function returns a dictionary with `abtract` and `arxiv_link`.\n\t    \"\"\"\n\t    response = requests.get(f\"https://paperswithcode.com{uid}\")\n\t    result = {}\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        paper_abstract_div = soup.select_one(\".paper-abstract\")\n\t        # Extract the abstract\n\t        abstract = paper_abstract_div.find(\"p\").text.strip()\n\t        # Extract the arXiv URL\n", "        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\t        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\t    else:\n\t        logger.warning(\n\t            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n\t        )\n\t    return result\n\tdef get_latest_papers_from_papers_with_code(*args, **kwargs) -> List[Content]:\n\t    logger.info(\"Getting papers from https://paperswithcode.com/\")\n\t    response = requests.get(\"https://paperswithcode.com/\")\n", "    results: List[Content] = []\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        for row in soup.select(\n\t            \".infinite-container .row.infinite-item.item.paper-card\"\n\t        ):\n\t            paper_dict = {\n\t                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n\t                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),\n\t                \"media\": row.select_one(\".item-image\")[\"style\"]\n", "                .split(\"('\")[1]\n\t                .split(\"')\")[0],\n\t                \"tags\": [\n\t                    a.get_text(strip=True) for a in row.select(\".badge-primary a\")\n\t                ],\n\t                \"stars\": int(\n\t                    row.select_one(\".entity-stars .badge\")\n\t                    .get_text(strip=True)\n\t                    .split(\" \")[0]\n\t                    .replace(\",\", \"\")\n", "                ),\n\t                \"github_link\": row.select_one(\".item-github-link a\")[\"href\"],\n\t                \"uid\": row.select_one(\"h1 a\")[\"href\"],\n\t            }\n\t            paper_dict = paper_dict\n\t            results.append(\n\t                Content(\n\t                    paper_dict[\"uid\"],\n\t                    data={\n\t                        **paper_dict,\n", "                        **get_paper_info_from_papers_with_code(paper_dict[\"uid\"]),\n\t                    },\n\t                )\n\t            )\n\t        else:\n\t            logger.warning(\"Was not able to scrape the html for one row.\")\n\t    else:\n\t        logger.warning(\n\t            f\"Failed to fetch the webpage. Status code: {response.status_code}.\"\n\t        )\n", "    return results\n"]}
{"filename": "src/content_providers/ai_papers/provider.py", "chunked_list": ["from typing import List\n\tfrom src.types import Content\n\tfrom ..base import ContentProvider\n\tfrom .papers_with_code import get_latest_papers_from_papers_with_code\n\tclass TrendingAIPapersProvider(ContentProvider):\n\t    def get_contents(self) -> List[Content]:\n\t        return get_latest_papers_from_papers_with_code()\n"]}
{"filename": "src/linkedin/schemas.py", "chunked_list": ["from typing import Dict, List, Optional, Tuple, TypedDict\n\tclass ProfilePicture(TypedDict):\n\t    displayImage: str\n\tclass LocalizedName(TypedDict):\n\t    it_IT: str\n\tclass PreferredLocale(TypedDict):\n\t    country: str\n\t    language: str\n\tclass Name(TypedDict):\n\t    localized: LocalizedName\n", "    preferredLocale: PreferredLocale\n\tclass UserProfile(TypedDict):\n\t    localizedLastName: str\n\t    profilePicture: ProfilePicture\n\t    firstName: Name\n\t    lastName: Name\n\t    id: str\n\t    localizedFirstName: str\n\tclass ServiceRelationship(TypedDict):\n\t    relationshipType: str\n", "    identifier: str\n\tclass RegisterUploadRequest(TypedDict):\n\t    recipes: List[str]\n\t    owner: str\n\t    serviceRelationships: List[ServiceRelationship]\n\tclass RegisterUploadBody(TypedDict):\n\t    registerUploadRequest: RegisterUploadRequest\n\tclass UploadHttpRequest(TypedDict):\n\t    uploadUrl: str\n\t    headers: Dict[str, str]\n", "class UploadMechanism(TypedDict):\n\t    com_linkedin_digitalmedia_uploading_MediaUploadHttpRequest: UploadHttpRequest\n\tclass Value(TypedDict):\n\t    mediaArtifact: str\n\t    uploadMechanism: UploadMechanism\n\t    asset: str\n\t    assetRealTimeTopic: str\n\tclass RegisterUploadResponse(TypedDict):\n\t    value: Value\n\tclass Text(TypedDict):\n", "    text: str\n\tclass Title(TypedDict):\n\t    text: str\n\tclass Description(TypedDict):\n\t    text: str\n\tclass Media(TypedDict):\n\t    status: str\n\t    description: Description\n\t    media: str\n\t    title: Title\n", "class ShareCommentary(TypedDict):\n\t    text: str\n\tclass ShareContent(TypedDict):\n\t    shareCommentary: ShareCommentary\n\t    shareMediaCategory: str\n\t    media: Optional[List[Media]]\n\tclass SpecificContent(TypedDict):\n\t    com_linkedin_ugc_ShareContent: ShareContent\n\tclass MemberNetworkVisibility(TypedDict):\n\t    com_linkedin_ugc_MemberNetworkVisibility: str\n", "class CreatePostBody(TypedDict):\n\t    author: str\n\t    lifecycleState: str\n\t    specificContent: SpecificContent\n\t    visibility: MemberNetworkVisibility\n\tclass CreatePostResponse(TypedDict):\n\t    id: str\n"]}
{"filename": "src/linkedin/api.py", "chunked_list": ["import os\n\tfrom functools import cache\n\tfrom typing import Dict, TypedDict\n\timport requests\n\tfrom .schemas import *\n\tclass API:\n\t    def __init__(self, token: str):\n\t        self.token = token\n\t        self.session = requests.Session()\n\t        self._set_headers()\n", "        self._set_hooks()\n\t    def _set_headers(self):\n\t        headers = {\n\t            \"Authorization\": f\"Bearer {self.token}\",\n\t            \"Content-Type\": \"application/json\",\n\t        }\n\t        self.session.headers = headers\n\t    def _set_hooks(self):\n\t        self.session.hooks = {\n\t            \"response\": lambda r, *args, **kwargs: r.raise_for_status()\n", "        }\n\t    @cache\n\t    def get_me(self) -> UserProfile:\n\t        return self.session.get(\"https://api.linkedin.com/v2/me\").json()\n\t    def create_post(self, body: Dict) -> CreatePostResponse:\n\t        return self.session.post(\"https://api.linkedin.com/v2/ugcPosts\", json=body)\n\t    def register_upload(self, body: RegisterUploadBody) -> RegisterUploadResponse:\n\t        return self.session.post(\n\t            \"https://api.linkedin.com/v2/assets?action=registerUpload\", json=body\n\t        ).json()\n", "    def upload_image(self, file_path: str, upload_url: str) -> requests.Response:\n\t        with open(file_path, \"rb\") as file:\n\t            response = requests.put(\n\t                upload_url, data=file, headers={\"Authorization\": f\"Bearer {self.token}\"}\n\t            )\n\t        return response\n\t    @classmethod\n\t    def from_env(cls):\n\t        return cls(os.environ[\"LINKEDIN_TOKEN\"])\n"]}
{"filename": "src/linkedin/__init__.py", "chunked_list": []}
{"filename": "src/linkedin/user.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import List, Optional, Tuple\n\tfrom requests import Response\n\tfrom rich import print\n\tfrom .api import API\n\tfrom .schemas import *\n\tclass User:\n\t    def __init__(self, api: API = None):\n\t        self._api = API.from_env() if api is None else api\n\t        self._me = self._api.get_me()\n", "    def _register_and_upload_image(self, image_path: str) -> str:\n\t        author = f\"urn:li:person:{self._me['id']}\"\n\t        register_upload_body = RegisterUploadBody(\n\t            registerUploadRequest=RegisterUploadRequest(\n\t                recipes=[\"urn:li:digitalmediaRecipe:feedshare-image\"],\n\t                owner=author,\n\t                serviceRelationships=[\n\t                    ServiceRelationship(\n\t                        relationshipType=\"OWNER\",\n\t                        identifier=\"urn:li:userGeneratedContent\",\n", "                    )\n\t                ],\n\t            )\n\t        )\n\t        response = self._api.register_upload(register_upload_body)\n\t        # [NOTE] LinkedIn apis have '.' in the name making it impossible to use TypeDicts\n\t        upload_url = response[\"value\"][\"uploadMechanism\"][\n\t            \"com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest\"\n\t        ][\"uploadUrl\"]\n\t        asset_id = response[\"value\"][\"asset\"]\n", "        response = self._api.upload_image(image_path, upload_url)\n\t        return asset_id\n\t    def create_post(\n\t        self, text: str, images: Optional[List[Tuple[str, str]]] = None\n\t    ) -> bool:\n\t        \"\"\"To create a post with the LinkedIn Apis we need to\n\t         - (if we have an image): register the image upload, store the returned uploadUrl\n\t         - (if we have an image): upload the image\n\t         - send the post, (if we have an image) add the correct image to the `media` request body\n\t        Args:\n", "            text (str): _description_\n\t            image_path (Optional[str], optional): _description_. Defaults to None.\n\t        Returns:\n\t            bool: _description_\n\t        \"\"\"\n\t        shareMediaCategory = \"NONE\" if images is None else \"IMAGE\"\n\t        author = f\"urn:li:person:{self._me['id']}\"\n\t        media: List[Media] = []\n\t        if images is not None:\n\t            for image_path, image_description in images:\n", "                asset_id = self._register_and_upload_image(image_path)\n\t                media.append(\n\t                    Media(\n\t                        status=\"READY\",\n\t                        description=Description(text=image_description),\n\t                        media=asset_id,\n\t                        title=Title(text=\"Image Title\"),\n\t                    )\n\t                )\n\t        create_post_body = {\n", "            \"author\": author,\n\t            \"lifecycleState\": \"PUBLISHED\",\n\t            \"specificContent\": {\n\t                \"com.linkedin.ugc.ShareContent\": {\n\t                    \"shareCommentary\": {\"text\": text},\n\t                    \"shareMediaCategory\": shareMediaCategory,\n\t                    \"media\": media,\n\t                }\n\t            },\n\t            \"visibility\": {\"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"},\n", "        }\n\t        return self._api.create_post(create_post_body).json()\n"]}
{"filename": "src/confirmations/input_confirmation.py", "chunked_list": ["from typing import Callable\n\tfrom src.actions import Action\n\tfrom src.types import GeneratedContent\n\tdef input_confirmation(repeat_func: Callable) -> bool:\n\t    while True:\n\t        user_input = input(\"Proceed [yes/no/repeat]\").lower().strip()\n\t        match user_input:\n\t            case \"yes\":\n\t                return True\n\t            case \"no\":\n", "                return False\n\t            case \"repeat\":\n\t                return repeat_func()"]}
{"filename": "src/confirmations/__init__.py", "chunked_list": []}
