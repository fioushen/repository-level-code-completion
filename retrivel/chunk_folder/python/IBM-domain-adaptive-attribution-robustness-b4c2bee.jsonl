{"filename": "utils.py", "chunked_list": ["import os\n\timport torch as ch\n\tdef scale_11(data):\n\t    global_max = ch.max(ch.abs(data)) + 1e-9\n\t    return (data/global_max).detach()\n\tdef get_dirname(_file):\n\t    return os.path.dirname(os.path.realpath(_file))\n\tdef get_project_rootdir():\n\t    return get_dirname(__file__)\n\tdef get_available_gpus():\n", "    availables = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"\")\n\t    if availables in {\"\"}:\n\t        return tuple([\"cpu\"])\n\t    return tuple(ch.device(f\"cuda:{dev}\") for dev in availables.split(\",\"))\n\tdef batch_up(length, batch_size):\n\t    return list(range(0, length + batch_size, batch_size))\n\tdef argsort(seq):\n\t    # http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python\n\t    return sorted(range(len(seq)), key=seq.__getitem__)\n\tdef get_multilabel_predicted_labels(logits, threshold: float = 0.0):\n", "    return (logits > threshold).to(ch.int32)\n\tdef filter_data(data, fil):\n\t    if isinstance(data, tuple):\n\t        return tuple(e for i, e in enumerate(data) if fil[i])\n\t    if isinstance(data, list):\n\t        return [e for i, e in enumerate(data) if fil[i]]\n\t    if isinstance(data, ch.Tensor):\n\t        return data[fil, ...]\n\t    if isinstance(data, dict):\n\t        return {k: filter_data(v, fil) for k, v in data.items()}\n", "def flatten(list_of_lists):\n\t    return [e for l in list_of_lists for e in l]"]}
{"filename": "constants.py", "chunked_list": ["SEPARATOR = \"~\"\n\tCONCATENATOR = \"|\"\n\tNON_WORD_TOKENS = {SEPARATOR, \",\", \".\", \";\", \"-\", \"[PAD]\", \"<pad>\", \"[CLS]\", \"<cls>\", \"[SEP]\", \"<sep>\", \"<s>\", \"</s>\", \"(\", \")\", \"[\", \"]\", \"/\", \"?\", \"<\", \">\", \"@\", \"!\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"_\", \"+\", \"{\", \"}\", \":\", \"|\", \"/\", \"Â§\", \"Â±\", \"...\", \"<br\", \"/><br\", \"=\", \"'\", \"â–\", \"Ä \"}\n\tEPS = 1e-9\n\tREPLACED_CHARS = ((\"Â–\", \" \"), (\"Â—\", \" \"), (\"Â”\", \" \"), (\"Â \", \" \"), (\"Â’\", \"'\"), (\"Â˜\", \"'\"), (\" sÂ¸ \", \" \"),(\"Ã†\", \"ae\"), (\"Ã¦\", \"ae\"), (\"Ä±\", \"l\"), (\"Â½\", \"1/2\"), (\"Â¼\", \"1/4\"), (\"Â¾\", \"3/4\"), (\"â…›\", \"1/8\"), (\"Ã—\", \"x\"), (\"Ã·\", \":\"), (\"â„…\", \"c/o\"), (\".\", \".\"), (\" Ñ\", \" \"), (\"Â¹\", \"1\"), (\"Ã©\", \"e\"), (\"Ã¾\", \"r\"),(\"Ã¡\", \"a\"), (\"Äƒ\", \"a\"), (\"Ã \", \"a\"), (\"Ã¥\", \"a\"), (\"Ã¢\", \"a\"), (\"Ã¤\", \"ae\"), (\"aÌˆ\", \"ae\"), (\"haÌˆf\", \"haef\"),\n\t(\"Ã£\", \"a\"), (\"Ãª\", \"e\"), (\"Ã¨\", \"e\"), (\"Ã«\", \"e\"), (\"Ð·\", \"e\"), (\"Ã«\", \"e\"), (\"Ä‡\", \"c\"), (\"ï¿½\", \"e\"),\n\t(\"ï¿½\", \"e\"), (\"Ã¯\", \"i\"), (\"á»‹\", \"i\"), (\"Ã­\", \"i\"), (\"Ã¬\", \"i\"), (\"Ã®\", \"i\"), (\"ï¬\", \"fi\"), (\"Ã½\", \"y\"),\n\t(\"Ã§\", \"c\"), (\"Ã°\", \"d\"), (\"Ã´\", \"o\"), (\"Ã¸\", \"o\"), (\"Ã³\", \"o\"), (\"Ãµ\", \"o\"), (\"Ã¶\", \"oe\"), (\"Å“\", \"oe\"),\n\t(\"Ã¼\", \"ue\"), (\"Ãº\", \"u\"), (\"Ã¹\", \"u\"), (\"Ã±\", \"n\"), (\"ÃŸ\", \"ss\"), (\"ÅŸ\", \"s\"), (\"Å›\", \"s\"), (\"s\", \"s\"),\n\t(\"Å¡\", \"s\"), (\"Ñ•Ð½\", \"sh\"), (\"ÄŸ\", \"g\"), (\"iÌ‡\", \"i\"), (\"Â¡\", \"!\"), (\"Â¿\", \"?\"), (\"Â»\", \">>\"), (\"â€º\", \">\"),\n", "(\"Â«\", \"<<\"), (\"â€¹\", \"<\"), (\"Â©\", \" C\"), (\"â„¢\", \" TM\"), (\"Â±\", \"+-\"), (\"áµ›\", \"v\"), (\"áµ‰\", \"e\"), (\"Ê³\", \"r\"),\n\t(\"á¶¦\", \"i\"), (\"á¶ \", \"f\"), (\"áµˆ\", \"d\"), (\"Ð¾\", \"o\"), (\"Ðº\", \"k\"), (\"Ñ‚\", \"t\"), (\"Ð²\", \"b\"), (\"Ð°\", \"a\"),\n\t(\"Ð²\", \"b\"), (\"Ð³\", \"t\"), (\"á´‡\", \"e\"), (\"á´€\", \"a\"), (\"s\", \"s\"), (\"á´›\", \"t\"), (\"Ê€\", \"r\"), (\"s\", \"s\"),(\"á´œ\", \"u\"), (\"É´\", \"n\"), (\"á´…\", \"d\"), (\"Ê\", \"y\"), (\"mÂ²\", \"m2\"), (\"Â³\", \"3\"), (\" Ð­\", \" \"), (\"Å‚\", \"l\"),\n\t(\"Â´\", \"'\"), (\"â„\", \"/\"), (\"â€˜\", \"'\"), (\"â€²\", \"'\"), (\"â€“\", \"-\"), (\"â€”\", \"-\"), (\"â€’\", \"-\"), (\"â€•\", \"-\"),\n\t(\"â€\", \"-\"), (\"â€¦\", \"...\"), (\"Â¸\", \",\"), (\"  \", \" \"), (\"/>\", \"\"), (\"/><br\", \"\"), (\"\\\\\\\"\", \"-\"),\n\t(\"\\\\n\", \" \"), (\"\\n\", \" \"), (\"\\t\", \" \"), (\"\\r\\n\", \" \"), (\"  \", \" \"), (\"\\\\\", \"\"), (SEPARATOR, \"\"), (\"\\\"\", \"'\"),\n\t(\"â€œ\", \"'\"), (\"â€\", \"'\"), (\"â€³\", \"'\"), (\"â€ž\", \"'\"), (\"â€™\", \"'\"), (\"  \", \" \"), (\"&#039;\", \"'\"), (\"\\u2029\", \" \"),\n\t(\"\\u2028\", \" \"), (\"\\u0085\", \" \"), (\"%u2019\", \"'\")\n\t)\n\tDELETED_CHARS = (\"Â­\", \"Â‘\", \"Â„\", \"Â•\", \"Â“\", \"Â™\", \"Âš\", \"Âœ\", \"Â\", \"â€‹\", \"Â\", \"ï»¿\", \"ÂŸ\", \"â€‚\", \"Â\", \"Â…\", \"Ã¢Â€\", \"Ä‘Å¸â€¡ÅŸÄ‘Å¸â€¡Â¸\", \"sÂ¸\", \"â†’\", \"â†\", \"Å¥\", \"Â§\", \"Ä‘\", \"Ã¿\", \"â€¡\", \"Â®\", \"Î¾\", \"Â·\", \"Âº\", \"Âº\", \"\\x85\", \"\\xa0\", \"\\u200a\", \"\\u2009\", \"Ã¯Â¿Â½\", \"Ø¹Ø±Ø¨ÙŠ\", \"Ã¢Ä¢Ä¶\", \"Â°\", \"â†‘\", \"ðŸ‘½\", \"ðŸ˜ƒ\", \"ðŸ‘\", \"ðŸ‘‡\", \"ðŸ™‚\", \"ðŸ˜¬\", \"ðŸ¤”\", \"ðŸ˜‚\", \"ðŸ˜«\",\n", "\"ðŸ˜³\", \"ðŸ˜€\", \"ðŸ˜‰\", \"ðŸ˜›\", \"ðŸ‡ºðŸ‡¸\", \"ðŸ‡²ðŸ‡½\", \"ðŸ˜¡\", \"ðŸŽ™\", \"ðŸ¸\", \"â¤ï¸\", \"ðŸ˜ˆ\", \"ðŸ‘ŠðŸ¾\", \"ðŸ¾ï¸\", \"ðŸ‘¨\", \"â€¼ï¸\", \"ðŸ™ðŸ»\",\n\t\"ðŸŽ‰\", \"ÃƒÄ¥Ã„Â¦ÃƒÄ¦Ã‚Â¯\",\n\t\"ðŸŽˆ\", \"ðŸ¾\", \"âœ¨\", \"ðŸš½\", \"ðŸ’”\", \"ðŸ˜Ž\", \"ðŸ•¶\", \"âš¡ï¸\", \"â°ï¸\", \"â°ï¸\", \"ðŸŒ ï¸\", \"ðŸŒ \", \"ðŸŒž\", \"ðŸŒŠ\", \"ðŸ”¥\", \"ðŸ‘\", \"ðŸŒ¹\",\n\t\"ðŸ’¯\", \"ðŸ‡·\", \"ðŸ‡¹\", \"â™«\", \"â¤\", \"ãƒ„\", \"â˜¢\", \"ï’©\", \"ï‡ºï‡¸\", \"Â£\", \"Â¨\", \"â‚¬\", \"Â¥\", \"â– \", \"â—†\", \"â–²\", \"â–º\", \"â˜…\", \"â€¢\", \"âœ“\",\n\tSEPARATOR, \"  \", chr(195), chr(293), chr(196), chr(166), chr(195), chr(294), chr(194), chr(175))\n\tWRONG_ORDS = {195, 293, 196, 166, 195, 294, 194, 175}\n\twith open(\"constants_data/STOP_WORDS.txt\") as infile:\n\t    STOP_WORDS = set([w.replace(\"\\n\", \"\") for w in sorted(set(infile.readlines()))])\n\twith open(\"constants_data/SYNONYMS_EXCLUDED_SHORT_WORDS.txt\") as infile:\n\t    SYNONYMS_EXCLUDED_SHORT_WORDS = set([w.replace(\"\\n\", \"\") for w in sorted(set(infile.readlines()))])\n", "SYNONYMS_SENSELESS_WORDS = {\"ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\\\n\tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\\\n\tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\\\n\tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\"}\n"]}
{"filename": "attacks/a2t.py", "chunked_list": ["import random\n\tfrom copy import deepcopy\n\tfrom typing import Collection\n\timport torch as ch\n\timport torch.nn.functional as f\n\tfrom constants import STOP_WORDS, EPS\n\tfrom explainers import wordwise_embedding_attributions, wordwise_attributions\n\tfrom utils import batch_up\n\tfrom .base_attack import BaseAttack\n\tclass A2T(BaseAttack):\n", "    def __init__(self, candidate_extractor_name: str):\n\t        super().__init__(candidate_extractor_name=candidate_extractor_name)\n\t    def attack(self, sentences: Collection[str], labels: ch.Tensor, model, tokenizer_module: ch.nn.Module, embedding_module: ch.nn.Module,\n\t            device: ch.device, rho_max: float = 0.15, rho_batch: float = 0.05, num_candidates: int = 5, stop_word_filter: bool = True,\n\t            random_word_importance: bool = False, random_synonym: bool = False, min_embedding_sim: float = 0.0, multilabel: bool = False,\n\t            pos_weight: float = None, attack_recall: bool = False):\n\t        with ch.no_grad():\n\t            if pos_weight is not None:\n\t                pos_weight = pos_weight.to(labels.device)\n\t            tmp_train = model.training\n", "            model.eval()\n\t            BATCH_SIZE = len(sentences)\n\t            tokenized = tokenizer_module(sentences, max_length=model.input_shape[0], device=device)\n\t            orig_pos = self.pos_tag_batch(tokenized[\"input_words\"])\n\t            adv_tokenized = deepcopy(tokenized)\n\t            distances = [ch.zeros(size=(sum(tokenized[\"word_mask\"][s_idx]),), dtype=ch.float32, device=device) + EPS\n\t                         for s_idx in range(BATCH_SIZE)]\n\t            # Get original labels\n\t            model.zero_grad()\n\t            embeds = embedding_module(tokenized)\n", "            embeds[\"input_embeds\"] = embeds[\"input_embeds\"].detach()\n\t        with ch.enable_grad():\n\t            embeds[\"input_embeds\"].requires_grad = True\n\t            preds = model(**embeds)\n\t            if multilabel:\n\t                loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32), reduction=\"none\", pos_weight=pos_weight).sum(-1)\n\t                pos_logits = preds * labels\n\t            else:\n\t                loss = f.cross_entropy(preds, labels, weight=pos_weight, reduction=\"none\")\n\t            if random_word_importance:\n", "                distances = [d + ch.rand(size=d.size(), dtype=d.dtype, device=d.device) + EPS for d in distances]\n\t            else:\n\t                loss.backward(inputs=embeds[\"input_embeds\"], gradient=ch.ones_like(loss))\n\t                grads = embeds[\"input_embeds\"].grad.data.detach().clone()\n\t                grads = wordwise_attributions(grads)\n\t                grads = wordwise_embedding_attributions(grads, tokenized[\"word_ranges\"], tokenized[\"word_mask\"])\n\t                for s_idx, expl in enumerate(grads):\n\t                    if expl.size() != distances[s_idx].size():\n\t                        raise ValueError(\"Explanations and distances not of equal size\")\n\t                distances = grads\n", "        with ch.no_grad():\n\t            argsorted_distances = [ch.argsort(d, dim=0, descending=True) for d in distances]\n\t            distance_sorted_word_indices = [ch.zeros_like(sd) for sd in argsorted_distances]\n\t            for s_idx, word_mask in enumerate(adv_tokenized[\"word_mask\"]):\n\t                v_ctr = 0\n\t                idx_map = {}\n\t                for w_idx, v_word in enumerate(word_mask):\n\t                    if v_word:\n\t                        idx_map[v_ctr] = w_idx\n\t                        v_ctr += 1\n", "                if v_ctr != sum(word_mask):\n\t                    raise ValueError(\"Not all indices mapped to true 'input_words' index\")\n\t                for w_idx, _idx in enumerate(argsorted_distances[s_idx]):\n\t                    distance_sorted_word_indices[s_idx][w_idx] = idx_map[_idx.item()]\n\t            del distances, argsorted_distances\n\t            s_lengths = [sum(s) for s in adv_tokenized[\"word_mask\"]]\n\t            tmp_max_l = max([len(x) for x in distance_sorted_word_indices])\n\t            max_num_c = [max(0, int(l * rho_max)) for s_idx, l in enumerate(s_lengths)]\n\t            s_batch_sizes = [min(max_num_c[s_idx], max(1, int(l * rho_batch))) for s_idx, l in enumerate(s_lengths)]\n\t            candidate_index_batches = [\n", "                [(t_indices.tolist() + [None] * (tmp_max_l - len(t_indices)))[i: i + s_batch_sizes[s_idx]]\n\t                 for i in (range(0, len(t_indices), s_batch_sizes[s_idx]) if s_batch_sizes[s_idx] > 0 else [])]\n\t                for s_idx, t_indices in enumerate(distance_sorted_word_indices)]\n\t            tmp_max_l = max([len(x) for x in candidate_index_batches])\n\t            candidate_index_batches = [c + [[None] * s_batch_sizes[s_idx]] * (tmp_max_l - len(c)) for s_idx, c in\n\t                                       enumerate(candidate_index_batches)]\n\t            max_dist = loss.detach().clone()  # Keep track of maximal distance for each sample\n\t            if attack_recall:\n\t                max_dist = - pos_logits.sum(-1).detach().clone()\n\t            replacements = [{} for _ in range(BATCH_SIZE)]  # Create a dict of replacements for each sample\n", "            for b_top_i in range(model.input_shape[0]):\n\t                b_indices = [candidate_index_batches[s_idx][b_top_i] if len(candidate_index_batches[s_idx]) > b_top_i\n\t                             else None for s_idx in range(BATCH_SIZE)]\n\t                if all(b is None for b in b_indices):\n\t                    continue\n\t                if all(len(replacements[s_idx]) + 1 > sum(adv_tokenized[\"word_mask\"][s_idx]) * rho_max\n\t                       for s_idx in range(BATCH_SIZE)):\n\t                    continue\n\t                with ch.no_grad():\n\t                    syns = self.get_candidate_extractor_fn(self.candidate_extractor_name)(\n", "                        deepcopy(adv_tokenized[\"input_words\"]), deepcopy(adv_tokenized[\"word_mask\"]), model, tokenizer_module,\n\t                        n_candidates=num_candidates, top_indices=b_indices, min_embedding_sim=min_embedding_sim, device=device,\n\t                        orig_pos=orig_pos\n\t                    )\n\t                for tmp_b_idx in range(max(len(s) for s in syns if s is not None)):\n\t                    top_indices = [b_indices[s_idx][tmp_b_idx] if len(b_indices[s_idx]) > tmp_b_idx else None\n\t                                   for s_idx in range(BATCH_SIZE)]\n\t                    top_indices = [None if len(replacements[s_idx]) + 1 > sum(adv_tokenized[\"word_mask\"][s_idx])\n\t                                   * rho_max else _idx for s_idx, _idx in enumerate(top_indices)]\n\t                    # Get all current words\n", "                    curr_words = [adv_tokenized[\"input_words\"][s_idx][top_idx] if top_idx is not None else None\n\t                                  for s_idx, top_idx in enumerate(top_indices)]\n\t                    curr_words = [w if w not in STOP_WORDS or not stop_word_filter else None for w in curr_words]\n\t                    # If all current words are either None or special tokens, continue\n\t                    if all(w in {None}.union(tokenizer_module.special_tokens) for w in curr_words):\n\t                        continue\n\t                    curr_syns = [\n\t                        syns[s_idx][tmp_b_idx] if syns[s_idx] is not None and len(syns[s_idx]) > tmp_b_idx and\n\t                                                  syns[s_idx][tmp_b_idx] is not None and top_indices[s_idx]\n\t                                                  is not None else [] for s_idx in range(BATCH_SIZE)\n", "                    ]\n\t                    syn_lens = [len(curr_syn) for curr_syn in curr_syns]\n\t                    if sum(syn_lens) < 1:\n\t                        continue\n\t                    with ch.no_grad():\n\t                        adv_preds = model(**embedding_module(adv_tokenized))\n\t                        if multilabel:\n\t                            adv_correct = ch.ones_like(max_dist, dtype=ch.bool)\n\t                        else:\n\t                            adv_correct = ch.argmax(adv_preds, dim=1) == labels\n", "                    curr_syns = [syn_set if adv_correct[s_idx] else [] for s_idx, syn_set in enumerate(curr_syns)]\n\t                    \"\"\"\n\t                    Check prediction and choose the one that maximizes cross-entropy loss\n\t                    \"\"\"\n\t                    # If random synonym, choose a random one\n\t                    if random_synonym:\n\t                        curr_syns = [list(random.choices(syn_set, k=1)) if len(syn_set) > 0 else tuple() for syn_set in\n\t                                     curr_syns]\n\t                    syn_lens = [len(curr_syn) for curr_syn in curr_syns]\n\t                    if sum(syn_lens) < 1:\n", "                        continue\n\t                    adv_words_il = self.repeat(adv_tokenized[\"input_words\"], syn_lens)\n\t                    adv_word_mask_il = self.repeat(adv_tokenized[\"word_mask\"], syn_lens)\n\t                    adv_targets_il = self.tensor_repeat(labels, syn_lens)\n\t                    # Replace word with the synonyms\n\t                    s_ctr = 0\n\t                    for s_idx, syn_set in enumerate(curr_syns):\n\t                        for _, (_syn, s) in enumerate(syn_set):\n\t                            # Needed to make sure tokenizer tokenizes to same word\n\t                            special_case_op = getattr(tokenizer_module, \"add_token_rule\", None)\n", "                            if callable(special_case_op):\n\t                                special_case_op(_syn, _syn)\n\t                            adv_words_il[s_ctr][top_indices[s_idx]] = _syn\n\t                            s_ctr += 1\n\t                    if s_ctr != sum(syn_lens):\n\t                        raise ValueError(f\"Counter error while constructing synonym data: {s_ctr} != {sum(syn_lens)}\")\n\t                    indices = batch_up(len(adv_words_il), batch_size=BATCH_SIZE)\n\t                    s_idx, s_ctr = 0, 0\n\t                    for i in range(1, len(indices)):\n\t                        tmp_words = adv_words_il[indices[i - 1]: indices[i]]\n", "                        tmp_word_masks = adv_word_mask_il[indices[i - 1]: indices[i]]\n\t                        tmp_targets = adv_targets_il[indices[i - 1]: indices[i]]\n\t                        tmp_tokenized = tokenizer_module.tokenize_from_words(tmp_words, tmp_word_masks,\n\t                                                                             max_length=model.input_shape[0],\n\t                                                                             device=device)\n\t                        with ch.no_grad():\n\t                            tmp_embeds = embedding_module(tmp_tokenized)\n\t                            tmp_preds = model(**tmp_embeds)\n\t                        if multilabel:\n\t                            tmp_losses = f.binary_cross_entropy_with_logits(tmp_preds, tmp_targets.to(ch.float32), reduction=\"none\", pos_weight=pos_weight).sum(-1)\n", "                            positive_logits = tmp_preds * tmp_targets\n\t                            if attack_recall:\n\t                                tmp_losses = - positive_logits.sum(-1)\n\t                        else:\n\t                            tmp_losses = f.cross_entropy(tmp_preds, tmp_targets, reduction=\"none\", weight=pos_weight)\n\t                        for tmp_idx, tmp_pred in enumerate(tmp_preds):\n\t                            # If already all synonyms have been processed\n\t                            while s_idx < len(syn_lens) and s_ctr >= syn_lens[s_idx]:\n\t                                s_idx += 1\n\t                                s_ctr = 0\n", "                            dist = tmp_losses[tmp_idx].detach().clone()\n\t                            if dist > max_dist[s_idx] or random_synonym:\n\t                                max_dist[s_idx] = dist.detach().item()\n\t                                adv_tokenized[\"input_words\"][s_idx][top_indices[s_idx]] = deepcopy(curr_syns[s_idx][s_ctr][0])\n\t                                replacements[s_idx][top_indices[s_idx]] = {\"old\": tokenized[\"input_words\"][\n\t                                    s_idx][top_indices[s_idx]], \"new\": curr_syns[s_idx][s_ctr][0], \"sim\": curr_syns[s_idx][s_ctr][1]}\n\t                            s_ctr += 1\n\t        if tmp_train:\n\t            model.train()\n\t        adv_sentences = tokenizer_module.decode_from_words(adv_tokenized[\"input_words\"], adv_tokenized[\"word_mask\"])\n", "        return adv_sentences, replacements, max_dist\n"]}
{"filename": "attacks/base_attack.py", "chunked_list": ["from copy import deepcopy\n\timport nltk\n\timport torch as ch\n\tfrom candidate_extractors import candidate_extractor_map\n\tfrom constants import STOP_WORDS\n\tclass BaseAttack(ch.nn.Module):\n\t    # Adapted from https://huggingface.co/docs/transformers/master/en/task_summary\n\t    def __init__(self, candidate_extractor_name: str):\n\t        super().__init__()\n\t        self.candidate_extractor_name = candidate_extractor_name\n", "        self.candidate_extractor = candidate_extractor_map[candidate_extractor_name].init_for_dataset(dataset=None, weight_path=None)\n\t        # Set parameters non-trainable\n\t        for param in self.parameters():\n\t            param.requires_grad = False\n\t    def __call__(self, *args, **kwargs):\n\t        return self.attack(*args, **kwargs)\n\t    def get_candidate_extractor_fn(self, candidate_extractor_name):\n\t        if candidate_extractor_name == \"synonym\":\n\t            return self.get_candidates_syn\n\t        else:\n", "            return self.get_candidates_mlm\n\t    def get_candidates_syn(self, input_words, word_mask, model, tokenizer, top_indices, n_candidates, orig_pos,\n\t                           stop_word_filter=None, min_embedding_sim=0.0, device=None):\n\t        with ch.no_grad():\n\t            if stop_word_filter is None:\n\t                stop_word_filter = {}\n\t            candidates = [None for _ in range(len(input_words))]\n\t            for s_idx, t_indices in enumerate(top_indices):\n\t                if t_indices is not None:  # This is the batch of top indices for a given sample\n\t                    tmp_cand = [None for _ in range(len(t_indices))]  # Initialize candidates for the batch of top indices\n", "                    for m_idx, t_idx in enumerate(t_indices):  # Iterate over the batch of top indices for given sample\n\t                        if t_idx is not None:  # If the given top index is not None\n\t                            orig_word = input_words[s_idx][t_idx]\n\t                            syns = list(self.candidate_extractor(orig_word, n=5*n_candidates, min_sim=min_embedding_sim, stop_filter=STOP_WORDS if stop_word_filter else set()))\n\t                            syns = [(t.strip(\" \"), s) for (t, s) in syns if not t.startswith(\"##\") and t != orig_word]\n\t                            filtered_syns = []\n\t                            n_ctr = 0\n\t                            for syn, s in syns:\n\t                                new_tokens = [t if _i != t_idx else syn for _i, t in enumerate(input_words[s_idx])]\n\t                                if len(input_words[s_idx]) != len(new_tokens):\n", "                                    raise ValueError(\"Token length inconsistency in POS filter\")\n\t                                n_ctr += 1\n\t                                new_pos = self.pos_tag_batch([new_tokens])[0][t_idx]\n\t                                if new_pos == orig_pos[s_idx][t_idx]:\n\t                                    filtered_syns.append((syn, s))\n\t                                if len(filtered_syns) >= n_candidates:\n\t                                    break\n\t                            syns = filtered_syns\n\t                            tmp_cand[m_idx] = syns\n\t                    candidates[s_idx] = tmp_cand\n", "        return candidates\n\t    def get_candidates_mlm(self, input_words, word_mask, model, tokenizer, top_indices, n_candidates, orig_pos, stop_word_filter=None, min_embedding_sim=0.0, device=None):\n\t        with ch.no_grad():\n\t            if stop_word_filter is None:\n\t                stop_word_filter = {}\n\t            masked_input_words = deepcopy(input_words)\n\t            for s_idx, mask_indices in enumerate(top_indices):\n\t                if mask_indices is not None:\n\t                    for mask_idx in mask_indices:\n\t                        if mask_idx is not None:\n", "                            masked_input_words[s_idx][mask_idx] = self.candidate_extractor.tokenizer.mask_token\n\t            masked_sentences = tokenizer.decode_from_words(masked_input_words, word_mask)\n\t            masked_data = self.candidate_extractor.tokenizer(masked_sentences, max_length=model.input_shape[0],\n\t                                                             device=device)\n\t            # Extract mapping from top_indices to actual masked token index\n\t            top_word_idx_to_masked_token_idx = [{ti: None for ti in s_top_indices} for s_top_indices in top_indices]\n\t            for s_idx, s_top_indices in enumerate(top_indices):\n\t                if s_top_indices is None:\n\t                    continue\n\t                tmp_masked_idx = 0\n", "                # If index is None, nothing should be masked, therefore we can just filter those indices out\n\t                for ti in sorted([s for s in s_top_indices if s is not None]):  # Sort to have correct ordering\n\t                    if ti is None:\n\t                        continue\n\t                    if tmp_masked_idx >= len(masked_data[\"tokenized\"][\"input_tokens\"][s_idx]):\n\t                        break\n\t                    while masked_data[\"tokenized\"][\"input_tokens\"][s_idx][tmp_masked_idx] != self.candidate_extractor.tokenizer.mask_token:\n\t                        if tmp_masked_idx >= len(masked_data[\"tokenized\"][\"input_tokens\"][s_idx]) - 1:\n\t                            break\n\t                        tmp_masked_idx += 1\n", "                    if masked_data[\"tokenized\"][\"input_tokens\"][s_idx][tmp_masked_idx] == self.candidate_extractor.tokenizer.mask_token:\n\t                        top_word_idx_to_masked_token_idx[s_idx][ti] = tmp_masked_idx\n\t                        tmp_masked_idx += 1\n\t            # Initialize candidates\n\t            candidates = [[None] * len(top_indices[s_idx]) for s_idx in range(len(top_indices))]\n\t            token_logits = self.candidate_extractor(**masked_data[\"tokenized\"]).logits\n\t            for s_idx, s_top_indices in enumerate(top_indices):\n\t                for t_idx, ti in enumerate(s_top_indices):\n\t                    mask_token_idx = top_word_idx_to_masked_token_idx[s_idx][ti]\n\t                    if mask_token_idx is None:\n", "                        continue\n\t                    mask_token_logits = token_logits[s_idx, mask_token_idx, :]\n\t                    mask_token_probs = ch.nn.functional.softmax(mask_token_logits, dim=0)\n\t                    # Get tokens with larges logits, use twice the num_c to ~ get enough candidates that are not subtokens\n\t                    top_tokens = ch.topk(mask_token_logits, k=5*n_candidates).indices.tolist()\n\t                    top_logits = mask_token_probs[top_tokens]\n\t                    # Decode tok tokens\n\t                    decoded_top_tokens = [(self.candidate_extractor.tokenizer.tok.decode([t]), top_logits[_i]) for _i, t in enumerate(top_tokens)]\n\t                    # Filter for subtokens, they are not allowed, as they result in different word length\n\t                    if self.candidate_extractor.tokenizer.subtoken_prefix is not None:\n", "                        decoded_top_tokens = [t for t in decoded_top_tokens if not t[0].startswith(\n\t                            self.candidate_extractor.tokenizer.subtoken_prefix)]\n\t                    decoded_top_tokens = [(t[0].strip(\" \"), t[1]) for t in decoded_top_tokens]\n\t                    # Sometimes, it would fill in the same word, do not allow that\n\t                    decoded_top_tokens = [t for t in decoded_top_tokens if t[0] != input_words[s_idx][ti]]\n\t                    # Stop word filter\n\t                    decoded_top_tokens = [t for t in decoded_top_tokens if t[0] not in stop_word_filter]\n\t                    # Filter out special tokens\n\t                    decoded_top_tokens = [t for t in decoded_top_tokens if t[0] not in tokenizer.special_tokens and t[0] not in\n\t                                          self.candidate_extractor.tokenizer.special_tokens]\n", "                    filtered_syns = []\n\t                    for (syn, s) in decoded_top_tokens:\n\t                        if s < min_embedding_sim:\n\t                            continue\n\t                        filtered_syns.append((syn, s))\n\t                        if len(filtered_syns) >= n_candidates:\n\t                            break\n\t                    decoded_top_tokens = filtered_syns\n\t                    # Add the proper number of candidates to the final list\n\t                    candidates[s_idx][t_idx] = decoded_top_tokens[:n_candidates]\n", "        return candidates\n\t    @staticmethod\n\t    def repeat(data, seq_lens):\n\t        return [deepcopy(d) for s_idx, d in enumerate(data) for _ in range(seq_lens[s_idx])]\n\t    @staticmethod\n\t    def tensor_repeat(data, seq_lens):\n\t        return ch.repeat_interleave(data, ch.tensor(seq_lens, device=data.device), dim=0)\n\t    @staticmethod\n\t    def pos_tag_batch(input_words):\n\t        return [[t[1] for t in nltk.pos_tag(sent_words, tagset=\"universal\")] for sent_words in input_words]\n"]}
{"filename": "attacks/__init__.py", "chunked_list": ["from .explanationfooler import ExplanationFooler\n\tfrom .a2t import A2T\n\tattack_map = {\"ef\": ExplanationFooler, \"a2t\": A2T}\n"]}
{"filename": "attacks/explanationfooler.py", "chunked_list": ["import random\n\tfrom copy import deepcopy\n\tfrom inspect import signature\n\tfrom typing import Collection\n\timport torch as ch\n\timport torch.nn.functional as f\n\tfrom constants import STOP_WORDS, EPS\n\tfrom explainers import explainer_map\n\tfrom losses import loss_map\n\tfrom utils import batch_up, get_multilabel_predicted_labels\n", "from .base_attack import BaseAttack\n\tclass ExplanationFooler(BaseAttack):\n\t    # Adapted from https://huggingface.co/docs/transformers/master/en/task_summary\n\t    def __init__(self, candidate_extractor_name: str):\n\t        super().__init__(candidate_extractor_name=candidate_extractor_name)\n\t    def attack(\n\t            self, sentences: Collection[str], model, tokenizer_module: ch.nn.Module, embedding_module: ch.nn.Module,\n\t            explainer_module: ch.nn.Module, device: ch.device, rho_max: float = 0.15, rho_batch: float = 0.05,\n\t            num_candidates: int = 5, stop_word_filter: bool = True, random_word_importance: bool = False,\n\t            random_synonym: bool = False, min_embedding_sim: float = 0.0, attribution_loss_fn_name: str = \"pc\",\n", "            attribution_loss_fn_args: dict = None, gradient_word_importance: bool = True, multilabel: bool = False,\n\t            adversarial_preds: bool = False, delta: float = 1.0, sentence_similarity_module: ch.nn.Module = None,\n\t            min_sentence_sim: float = 0.0, pos_weight=None, detach: bool = False,\n\t    ):\n\t        if attribution_loss_fn_args is None:\n\t            attribution_loss_fn_args = {\"normalize_to_loss\": True}\n\t        attribution_loss_fn = loss_map[attribution_loss_fn_name]\n\t        tmp_train = model.training\n\t        model.eval()\n\t        BATCH_SIZE = len(sentences)\n", "        # Setup attack and data gathering\n\t        tokenized = tokenizer_module(sentences, max_length=model.input_shape[0], device=device)\n\t        embeds = embedding_module(tokenized)\n\t        with ch.no_grad():\n\t            if multilabel:\n\t                labels = get_multilabel_predicted_labels(model(**embeds))\n\t            else:\n\t                labels = ch.argmax(model(**embeds), dim=1)\n\t        with ch.enable_grad():\n\t            orig_explanations = self.get_explanations(sentences, model, tokenizer_module, embedding_module, explainer_module, labels, multilabel=multilabel, device=device, detach=detach)\n", "        with ch.no_grad():\n\t            orig_pos = self.pos_tag_batch(tokenized[\"input_words\"])\n\t            adv_tokenized = deepcopy(tokenized)\n\t            \"\"\"\n\t            Check most important words by setting them to unk_token\n\t            \"\"\"\n\t            distances = [ch.zeros(size=(sum(tokenized[\"word_mask\"][s_idx]),), dtype=ch.float32, device=device) + EPS for s_idx in range(BATCH_SIZE)]\n\t            if random_word_importance:\n\t                # Fill up tensors with random distances in [0,1)\n\t                distances = [d + ch.rand(size=d.size(), dtype=d.dtype, device=d.device) + EPS for d in distances]\n", "            else:\n\t                if gradient_word_importance:\n\t                    with ch.enable_grad():\n\t                        wi_explainer_module = explainer_map[\"s\"](model)\n\t                        expls = self.get_explanations(sentences, model, tokenizer_module, embedding_module, wi_explainer_module, labels, multilabel=multilabel,\n\t                                                      device=device, detach=detach)\n\t                    for s_idx, expl in enumerate(expls):\n\t                        if expl.size() != distances[s_idx].size():\n\t                            raise ValueError(\"Explanations and distances not of equal size\")\n\t                    distances = expls\n", "                    del wi_explainer_module\n\t                else:\n\t                    # TODO not adjusted to multilabel\n\t                    # Create list of input words as many times as there are words in each sentence\n\t                    adv_words_il = self.repeat(adv_tokenized[\"input_words\"], [sum(w) for w in adv_tokenized[\"word_mask\"]])\n\t                    adv_word_mask_il = self.repeat(adv_tokenized[\"word_mask\"], [sum(w) for w in adv_tokenized[\"word_mask\"]])\n\t                    # Replace each word once with unk_token\n\t                    int_idx = 0\n\t                    for s_idx, num_words in enumerate([sum(w) for w in adv_tokenized[\"word_mask\"]]):\n\t                        num_replaced = 0\n", "                        for w_idx, _word_mask in enumerate(adv_tokenized[\"word_mask\"][s_idx]):\n\t                            if _word_mask:\n\t                                adv_words_il[int_idx][w_idx] = tokenizer_module.unk_token\n\t                                num_replaced += 1\n\t                                int_idx += 1\n\t                        if num_replaced != num_words:\n\t                            raise ValueError(\"Not all valid words have been replaced by unk_token\")\n\t                    if int_idx != sum([sum(w) for w in adv_tokenized[\"word_mask\"]]):\n\t                        raise ValueError(\"Word importance ranking error with sequence lengths\")\n\t                    # Process temporary explanations in batches\n", "                    indices = batch_up(len(adv_words_il), batch_size=BATCH_SIZE)\n\t                    s_idx, w_ctr = 0, 0\n\t                    for i in range(1, len(indices)):\n\t                        tmp_words = adv_words_il[indices[i - 1]: indices[i]]\n\t                        tmp_word_masks = adv_word_mask_il[indices[i - 1]: indices[i]]\n\t                        tmp_data = tokenizer_module.tokenize_from_words(tmp_words, tmp_word_masks, model.input_shape[0],\n\t                                                                        device=device)\n\t                        tmp_embeds = embedding_module(tmp_data)\n\t                        tmp_labels = ch.argmax(model(**tmp_embeds), dim=1).detach()\n\t                        tmp_explanations = self.get_explanations(tokenizer_module.decode_from_words(tmp_words, tmp_word_masks),\n", "                                                                 model, tokenizer_module, embedding_module, explainer_module,\n\t                                                                 tmp_labels, device=device, multilabel=multilabel, detach=detach)\n\t                        for tmp_idx, tmp_expl in enumerate(tmp_explanations):\n\t                            s_offset = 0  # Calculate the number of non-word tokens in the beginning of the sentence\n\t                            while not tmp_word_masks[tmp_idx][s_offset]:\n\t                                s_offset += 1\n\t                            if orig_explanations[s_idx].size() != tmp_expl.size():\n\t                                dist = EPS\n\t                            # If word is a stop word, set distance to zero to deprioritize\n\t                            elif stop_word_filter and adv_tokenized[\"input_words\"][s_idx][w_ctr + s_offset] in STOP_WORDS:\n", "                                dist = EPS\n\t                            else:\n\t                                dist = attribution_loss_fn(ref_data=orig_explanations[s_idx], mod_data=tmp_expl, **attribution_loss_fn_args).detach()\n\t                            distances[s_idx][w_ctr] = dist\n\t                            w_ctr += 1\n\t                            if w_ctr >= sum(tmp_word_masks[tmp_idx]):\n\t                                s_idx += 1\n\t                                w_ctr = 0\n\t                    if w_ctr != 0:\n\t                        raise ValueError(\"Word counter did not reset to zero.\")\n", "                    if s_idx != BATCH_SIZE:\n\t                        raise ValueError(\"Not all samples are processed.\")\n\t                    # Delete data that is not needed anymore\n\t                    del adv_words_il, adv_word_mask_il, tmp_words, tmp_word_masks, tmp_explanations, tmp_data\n\t            # Argsort the distances. They need to be adjusted to reflect the actual words in \"input_words\"\n\t            # Depending on front/back padding and special tokens\n\t            argsorted_distances = [ch.argsort(d, dim=0, descending=True) for d in distances]\n\t            # This corresponds to the converted indices in the \"input_words\" field\n\t            distance_sorted_word_indices = [ch.zeros_like(sd) for sd in argsorted_distances]\n\t            for s_idx, word_mask in enumerate(adv_tokenized[\"word_mask\"]):\n", "                v_ctr, idx_map = 0, {}\n\t                for w_idx, v_word in enumerate(word_mask):\n\t                    if v_word:\n\t                        idx_map[v_ctr] = w_idx\n\t                        v_ctr += 1\n\t                if v_ctr != sum(word_mask):\n\t                    raise ValueError(\"Not all indices mapped to true 'input_words' index\")\n\t                for w_idx, _idx in enumerate(argsorted_distances[s_idx]):\n\t                    distance_sorted_word_indices[s_idx][w_idx] = idx_map[_idx.item()]\n\t            del distances, argsorted_distances\n", "            # Batch up top indices\n\t            s_lengths = [sum(s) for s in adv_tokenized[\"word_mask\"]]\n\t            tmp_max_l = max([len(x) for x in distance_sorted_word_indices])\n\t            max_num_c = [max(0, int(l * rho_max)) for s_idx, l in enumerate(s_lengths)]\n\t            s_batch_sizes = [min(max_num_c[s_idx], max(1, int(l * rho_batch))) for s_idx, l in enumerate(s_lengths)]\n\t            candidate_index_batches = [\n\t                [(t_indices.tolist() + [None] * (tmp_max_l - len(t_indices)))[i: i + s_batch_sizes[s_idx]]\n\t                 for i in (range(0, len(t_indices), s_batch_sizes[s_idx]) if s_batch_sizes[s_idx] > 0 else [])]\n\t                for s_idx, t_indices in enumerate(distance_sorted_word_indices)]\n\t            tmp_max_l = max([len(x) for x in candidate_index_batches])\n", "            candidate_index_batches = [c + [[None] * s_batch_sizes[s_idx]] * (tmp_max_l - len(c)) for s_idx, c in enumerate(candidate_index_batches)]\n\t            \"\"\"\n\t            Start replacing words with their synonyms\n\t            \"\"\"\n\t            max_dist = [0.0 for _ in range(BATCH_SIZE)]  # Keep track of maximal distance for each sample\n\t            replacements = [{} for _ in range(BATCH_SIZE)]  # Create a dict of replacements for each sample\n\t            for b_top_i in range(model.input_shape[0]):\n\t                b_indices = [candidate_index_batches[s_idx][b_top_i] if len(candidate_index_batches[s_idx]) > b_top_i else None for s_idx in range(BATCH_SIZE)]\n\t                if all(b is None for b in b_indices) or all(len(replacements[s_idx]) + 1 > sum(adv_tokenized[\"word_mask\"][s_idx]) * rho_max for s_idx in range(BATCH_SIZE)):\n\t                    continue\n", "                # Extract candidates for the masked words\n\t                syns = self.get_candidate_extractor_fn(self.candidate_extractor_name)(\n\t                    adv_tokenized[\"input_words\"], adv_tokenized[\"word_mask\"], model, tokenizer_module,\n\t                    n_candidates=num_candidates, top_indices=b_indices, min_embedding_sim=min_embedding_sim, device=device, orig_pos=orig_pos)\n\t                for tmp_b_idx in range(max(len(s) for s in syns if s is not None)):\n\t                    # Get current top index, if number of words is large enough in sample, else None\n\t                    top_indices = [b_indices[s_idx][tmp_b_idx] if len(b_indices[s_idx]) > tmp_b_idx else None for s_idx in range(BATCH_SIZE)]\n\t                    # Set current idx to None if another change would result in too high perturbation ratio\n\t                    top_indices = [None if len(replacements[s_idx]) + 1 > sum(adv_tokenized[\"word_mask\"][s_idx]) * rho_max else _idx for s_idx, _idx in enumerate(top_indices)]\n\t                    # Get all current words\n", "                    curr_words = [adv_tokenized[\"input_words\"][s_idx][top_idx] if top_idx is not None else None for s_idx, top_idx in enumerate(top_indices)]\n\t                    curr_words = [w if w not in STOP_WORDS or not stop_word_filter else None for w in curr_words]\n\t                    # If all current words are either None or special tokens, continue\n\t                    if all(w in {None}.union(tokenizer_module.special_tokens) for w in curr_words):\n\t                        continue\n\t                    curr_syns = [syns[s_idx][tmp_b_idx] if syns[s_idx] is not None and len(syns[s_idx]) > tmp_b_idx and syns[s_idx][tmp_b_idx]\n\t                                 is not None and top_indices[s_idx] is not None else [] for s_idx in range(BATCH_SIZE)]\n\t                    syn_lens = [len(curr_syn) for curr_syn in curr_syns]\n\t                    if sum(syn_lens) < 1:\n\t                        continue\n", "                    adv_words_il = self.repeat(adv_tokenized[\"input_words\"], syn_lens)\n\t                    adv_word_mask_il = self.repeat(adv_tokenized[\"word_mask\"], syn_lens)\n\t                    s_ctr = 0\n\t                    for s_idx, syn_set in enumerate(curr_syns):\n\t                        for _, (_syn, s) in enumerate(syn_set):\n\t                            adv_words_il[s_ctr][top_indices[s_idx]] = _syn\n\t                            s_ctr += 1\n\t                    if s_ctr != sum(syn_lens):\n\t                        raise ValueError(f\"Counter error while constructing synonym data INF: {s_ctr} != {sum(syn_lens)}\")\n\t                    adv_tokenized_il = tokenizer_module.tokenize_from_words(adv_words_il, adv_word_mask_il, model.input_shape[0], device=device)\n", "                    with ch.no_grad():\n\t                        adv_embeds_il = embedding_module(adv_tokenized_il)\n\t                        if multilabel:\n\t                            il_logits = model(**adv_embeds_il)\n\t                            il_c = get_multilabel_predicted_labels(il_logits) == self.tensor_repeat(labels, syn_lens)\n\t                            il_correct = ch.eq(il_c.sum(dim=1), ch.zeros_like(il_c.sum(dim=1)) + model.num_classes)\n\t                        else:\n\t                            il_output = model(**adv_embeds_il)\n\t                            il_correct = ch.argmax(il_output, dim=1) == self.tensor_repeat(labels, syn_lens)\n\t                        if adversarial_preds:\n", "                            il_correct = ch.ones_like(il_correct)\n\t                    ctr = 0\n\t                    for s_idx, syn_set in enumerate(list(curr_syns)):\n\t                        filtered_syns = []\n\t                        for _syn in syn_set:\n\t                            if il_correct[ctr]:\n\t                                filtered_syns.append(_syn)\n\t                            ctr += 1\n\t                        curr_syns[s_idx] = filtered_syns\n\t                    if ctr != sum(syn_lens):\n", "                        raise ValueError(f\"Counter error while constructing synonym data: {ctr} != {sum(syn_lens)}\")\n\t                    \"\"\"\n\t                    Explain and chose the best synonym for each s_id\n\t                    \"\"\"\n\t                    if random_synonym:\n\t                        curr_syns = [list(random.choices(syn_set, k=1)) if len(syn_set) > 0 else tuple() for syn_set in curr_syns]\n\t                    syn_lens = [len(curr_syn) for curr_syn in curr_syns]\n\t                    # If no synonyms anymore, continue\n\t                    if sum(syn_lens) < 1:\n\t                        continue\n", "                    # Repeat the data again with valid synonyms\n\t                    adv_words_il = self.repeat(adv_tokenized[\"input_words\"], syn_lens)\n\t                    adv_word_mask_il = self.repeat(adv_tokenized[\"word_mask\"], syn_lens)\n\t                    adv_labels_il = self.tensor_repeat(labels, syn_lens)\n\t                    s_ctr = 0\n\t                    for s_idx, syn_set in enumerate(curr_syns):\n\t                        for _, (_syn, s) in enumerate(syn_set):\n\t                            # Needed to make sure tokenizer tokenizes to same word\n\t                            special_case_op = getattr(tokenizer_module, \"add_token_rule\", None)\n\t                            if callable(special_case_op):\n", "                                special_case_op(_syn, _syn)\n\t                            adv_words_il[s_ctr][top_indices[s_idx]] = _syn\n\t                            s_ctr += 1\n\t                    if s_ctr != sum(syn_lens):\n\t                        raise ValueError(f\"Counter error while constructing synonym data: {s_ctr} != {sum(syn_lens)}\")\n\t                    # Process temporary explanations in batches\n\t                    indices = batch_up(len(adv_words_il), batch_size=BATCH_SIZE)\n\t                    s_idx, s_ctr = 0, 0\n\t                    for i in range(1, len(indices)):\n\t                        tmp_words = adv_words_il[indices[i - 1]: indices[i]]\n", "                        tmp_word_masks = adv_word_mask_il[indices[i - 1]: indices[i]]\n\t                        tmp_labels = adv_labels_il[indices[i - 1]: indices[i]].detach()\n\t                        if adversarial_preds:\n\t                            tmp_preds = model(**embedding_module(tokenizer_module( tokenizer_module.decode_from_words(tmp_words, tmp_word_masks),\n\t                                max_length=model.input_shape[0], device=device)))\n\t                            if multilabel:\n\t                                tmp_loss = f.binary_cross_entropy_with_logits(tmp_preds, tmp_labels.to(ch.float32), reduction=\"none\", pos_weight=pos_weight).sum(-1).detach()\n\t                            else:\n\t                                tmp_loss = f.cross_entropy(tmp_preds, tmp_labels, reduction=\"none\", weight=pos_weight).detach()\n\t                        with ch.enable_grad():\n", "                            tmp_explanations = self.get_explanations(tokenizer_module.decode_from_words(tmp_words, tmp_word_masks), model, tokenizer_module, embedding_module, explainer_module,\n\t                                tmp_labels, device=device, multilabel=multilabel, detach=detach)\n\t                        for tmp_idx, tmp_expl in enumerate(tmp_explanations):\n\t                            # If already all synonyms have been processed\n\t                            while s_idx < len(syn_lens) and s_ctr >= syn_lens[s_idx]:\n\t                                s_idx += 1\n\t                                s_ctr = 0\n\t                            if orig_explanations[s_idx].size() != tmp_expl.size():\n\t                                s_ctr += 1\n\t                                continue\n", "                            e_dist = attribution_loss_fn(ref_data=orig_explanations[s_idx], mod_data=tmp_expl, **attribution_loss_fn_args).detach()\n\t                            if adversarial_preds:\n\t                                dist = delta * e_dist + tmp_loss[tmp_idx]\n\t                            else:\n\t                                dist = e_dist\n\t                            if dist > max_dist[s_idx] or random_synonym:\n\t                                max_dist[s_idx] = dist.detach().item()\n\t                                adv_tokenized[\"input_words\"][s_idx] = deepcopy(tmp_words[tmp_idx])\n\t                                replacements[s_idx][top_indices[s_idx]] = {\"old\": tokenized[\"input_words\"][s_idx][top_indices[s_idx]], \"new\": curr_syns[s_idx][s_ctr][0], \"sim\": curr_syns[s_idx][s_ctr][1]}\n\t                            s_ctr += 1\n", "        if tmp_train:\n\t            model.train()\n\t        adv_sentences = tokenizer_module.decode_from_words(adv_tokenized[\"input_words\"], adv_tokenized[\"word_mask\"])\n\t        return adv_sentences, replacements, max_dist\n\t    @staticmethod\n\t    def get_explanations(sentences, model, tokenizer_module, embedding_module, explainer_module, labels: ch.Tensor = None, device: ch.device = ch.device(\"cpu\"),\n\t                         multilabel: bool = False, detach: bool = False) -> ch.Tensor:\n\t        from explainers import wordwise_embedding_attributions, wordwise_attributions\n\t        model.zero_grad()\n\t        with ch.no_grad():\n", "            tokenized = tokenizer_module(sentences, max_length=model.input_shape[0], device=device)\n\t            embeds = embedding_module(tokenized)\n\t            if labels is None:\n\t                if multilabel:\n\t                    labels = get_multilabel_predicted_labels(model(**embeds))\n\t                else:\n\t                    labels = ch.argmax(model(**embeds), dim=1)\n\t            tmp_cudnn_be = ch.backends.cudnn.enabled\n\t            ch.backends.cudnn.enabled = False\n\t            attribute_kwargs = {}\n", "            if \"baselines\" in signature(explainer_module.attribute).parameters:\n\t                attribute_kwargs[\"baselines\"] = ch.zeros_like(embeds[\"input_embeds\"], dtype=ch.float32, device=device)\n\t            if \"n_steps\" in signature(explainer_module.attribute).parameters:\n\t                attribute_kwargs[\"n_steps\"] = 5\n\t            if \"sliding_window_shapes\" in signature(explainer_module.attribute).parameters:\n\t                attribute_kwargs[\"sliding_window_shapes\"] = (1,) + embeds[\"input_embeds\"].size()[2:]\n\t            if \"internal_batch_size\" in signature(explainer_module.attribute).parameters:\n\t                attribute_kwargs[\"internal_batch_size\"] = embeds[\"input_embeds\"].size(0)\n\t            additional_forward_args = tuple()\n\t            if \"attention_mask\" in signature(model.forward).parameters:\n", "                additional_forward_args += (embeds[\"attention_mask\"],)\n\t            if \"token_type_ids\" in signature(model.forward).parameters:\n\t                additional_forward_args += (embeds[\"token_type_ids\"],)\n\t            if \"global_attention_mask\" in signature(model.forward).parameters:\n\t                additional_forward_args += (embeds[\"global_attention_mask\"],)\n\t            model.zero_grad()\n\t            ch.use_deterministic_algorithms(True, warn_only=True)\n\t        with ch.enable_grad():\n\t            if not multilabel:\n\t                attrs = explainer_module.attribute(inputs=embeds[\"input_embeds\"], target=labels, additional_forward_args=additional_forward_args, **attribute_kwargs)\n", "            elif isinstance(explainer_module, explainer_map[\"a\"]):\n\t                attrs = explainer_module.attribute(inputs=embeds[\"input_embeds\"], target=None, additional_forward_args=additional_forward_args, **attribute_kwargs)\n\t            else:\n\t                attrs = ch.zeros_like(embeds[\"input_embeds\"])\n\t                for c_idx in range(labels.size(1)):\n\t                    nonzero_s_indices = ch.nonzero(labels[:, c_idx]).squeeze(-1)\n\t                    if len(nonzero_s_indices) < 1:\n\t                        continue\n\t                    if \"baselines\" in signature(explainer_module.attribute).parameters:\n\t                        attribute_kwargs[\"baselines\"] = ch.zeros_like(embeds[\"input_embeds\"][nonzero_s_indices, ...], dtype=ch.float32, device=device)\n", "                    tmp_additional_args = tuple(arg[nonzero_s_indices, ...] for arg in additional_forward_args)\n\t                    tmp_attrs = explainer_module.attribute(\n\t                        inputs=embeds[\"input_embeds\"][nonzero_s_indices, ...], target=ch.zeros_like(nonzero_s_indices) + c_idx, additional_forward_args=tmp_additional_args, **attribute_kwargs)\n\t                    attrs[nonzero_s_indices] = attrs[nonzero_s_indices] + tmp_attrs.to(attrs.dtype)\n\t                    if detach:\n\t                        attrs = attrs.detach()\n\t            if detach:\n\t                attrs = attrs.detach()\n\t            ch.use_deterministic_algorithms(True, warn_only=False)\n\t            # Reduce last dimension to get one scalar for each word\n", "            attrs = wordwise_attributions(attrs)\n\t            # Here, add word ranges and process explanation\n\t            attrs = wordwise_embedding_attributions(attrs, tokenized[\"word_ranges\"], tokenized[\"word_mask\"])\n\t            ch.backends.cudnn.enabled = tmp_cudnn_be\n\t        return attrs\n"]}
{"filename": "tokenizer_wrappers/clinicallongformer.py", "chunked_list": ["import torch as ch\n\tfrom transformers import LongformerTokenizerFast, AutoTokenizer, AutoModel\n\tfrom tokenizer_wrappers.transformer_tokenizer import TransformerTokenizer\n\tclass ClinicalLongformerTokenizer(TransformerTokenizer):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.lang = \"yikuan8/Clinical-Longformer\"\n\t        self.unk_token = \"<unk>\"\n\t        self.zero_token = \"<unk>\"\n\t        self.sep_token = \"</s>\"\n", "        self.pad_token = \"<pad>\"\n\t        self.cls_token = \"<s>\"\n\t        self.mask_token = \"<mask>\"\n\t        self.bos_token = \"<s>\"\n\t        self.eos_token = \"</s>\"\n\t        # Special tokens\n\t        self.subtoken_prefix = None\n\t        self.token_space_prefix = \"Ä \"\n\t        self.tok = LongformerTokenizerFast.from_pretrained(self.lang, do_lower_case=True, unk_token=self.unk_token, sep_token=self.sep_token, pad_token=self.pad_token, cls_token=self.cls_token, mask_token=self.mask_token, clean_text=True)\n\t        self._update_special_tokens()\n", "    def tokenize(self, batch_of_sentences, max_length, device=None):\n\t        return_val = super().tokenize(batch_of_sentences, max_length, device)\n\t        return_val[\"tokenized\"][\"global_attention_mask\"] = ch.zeros_like(return_val[\"tokenized\"][\"attention_mask\"])\n\t        return_val[\"tokenized\"][\"global_attention_mask\"][:, 0] = 1\n\t        return return_val\n\t    def _tokenize(self, batch_of_sentences, max_length):\n\t        enc = super()._tokenize(batch_of_sentences, max_length)\n\t        enc[\"global_attention_mask\"] = ch.zeros_like(enc[\"attention_mask\"])\n\t        enc[\"global_attention_mask\"][:, 0] = 1\n\t        return enc\n", "    def _update_tokenized(self, data_dict, max_length):\n\t        enc = super()._update_tokenized(data_dict, max_length)\n\t        enc[\"global_attention_mask\"] = ch.zeros_like(enc[\"attention_mask\"])\n\t        enc[\"global_attention_mask\"][:, 0] = 1\n\t        return enc"]}
{"filename": "tokenizer_wrappers/__init__.py", "chunked_list": ["import os\n\tfrom .roberta import RoBERTaTokenizer\n\tfrom .distilroberta import DistilRoBERTaTokenizer\n\tfrom .pubmedbert import PubMedBERTTokenizer\n\tfrom .biolinkbert import BioLinkBERTTokenizer\n\tfrom .clinicallongformer import ClinicalLongformerTokenizer\n\tos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\ttokenizer_map = {\"roberta\": RoBERTaTokenizer, \"distilroberta\": DistilRoBERTaTokenizer, \"biolinkbert\": BioLinkBERTTokenizer,\n\t    \"pubmedbert\": PubMedBERTTokenizer, \"clinicallongformer\": ClinicalLongformerTokenizer}\n"]}
{"filename": "tokenizer_wrappers/transformer_tokenizer.py", "chunked_list": ["import torch as ch\n\tfrom tokenizer_wrappers.common import BaseTokenizer, update_input_data\n\tfrom constants import WRONG_ORDS\n\tclass TransformerTokenizer(BaseTokenizer):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t    \"\"\"\n\t    Encode methods\n\t    \"\"\"\n\t    def _tokenize(self, batch_of_sentences, max_length):\n", "        enc = self.tok(list(batch_of_sentences), max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n\t        enc[\"input_tokens\"] = list(list(self.tok.convert_ids_to_tokens(list(tok_sen))) for tok_sen in enc[\"input_ids\"])\n\t        # This can only create 0 as token_type_id\n\t        enc[\"token_type_ids\"] = ch.zeros_like(enc[\"attention_mask\"])\n\t        return enc\n\t    def _update_tokenized(self, data_dict, max_length):\n\t        batch_of_sentences = self.decode_from_words(data_dict[\"input_words\"], data_dict[\"word_mask\"])\n\t        enc = self._tokenize(batch_of_sentences, max_length)\n\t        updated_data = update_input_data(enc, seq_len=max_length, pad_token=self.pad_token, pad_id=self.tok.pad_token_id)\n\t        return {\"input_tokens\": updated_data[\"input_tokens\"], \"input_ids\": updated_data[\"input_ids\"], \"token_type_ids\": updated_data[\"token_type_ids\"], \"attention_mask\": updated_data[\"attention_mask\"]}\n", "    def tokenize_from_sentences(self, batch_of_sentences, max_length):\n\t        enc = self._tokenize(batch_of_sentences, max_length)\n\t        word_ranges = self.get_word_ranges(enc[\"input_tokens\"])\n\t        words, word_mask, token_word_mask = self.extract_words(enc[\"input_tokens\"], word_ranges)\n\t        sentences = self.decode_from_words(words, word_mask)\n\t        # Retokenize to make sure decoding and encoding leads to same data\n\t        num_tries = 0\n\t        while num_tries < 3:\n\t            enc = self._tokenize(self.decode_from_words(words, word_mask), max_length)\n\t            word_ranges = self.get_word_ranges(enc[\"input_tokens\"])\n", "            words, word_mask, token_word_mask = self.extract_words(enc[\"input_tokens\"], word_ranges)\n\t            sentences_new = self.decode_from_words(words, word_mask)\n\t            if sentences_new == sentences:\n\t                break\n\t            else:\n\t                num_tries += 1\n\t        data_dict = {\"sentences\": self.decode_from_words(words, word_mask), \"word_ranges\": word_ranges, \"input_words\": words, \"word_mask\": word_mask, \"token_mask\": token_word_mask}\n\t        data_dict[\"tokenized\"] = self._update_tokenized(data_dict, max_length)\n\t        return data_dict\n\t    \"\"\"\n", "    Decode methods\n\t    \"\"\"\n\t    def decode_from_tokenized(self, tokenized_dict: dict, remove_special_tokens=True, remove_pad_token=True) -> list:\n\t        to_remove = set()\n\t        if remove_pad_token:\n\t            to_remove.add(self.pad_token)\n\t        if remove_special_tokens:\n\t            to_remove.update(self.special_tokens)\n\t        sents = list(self.tok.batch_decode([[t for t in self.tok.convert_tokens_to_ids(\n\t            [w for w in sent if w not in to_remove])] for sent in tokenized_dict[\"input_tokens\"]], skip_special_tokens=False, clean_up_tokenization_spaces=True))\n", "        sents = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in sents]\n\t        return sents\n"]}
{"filename": "tokenizer_wrappers/pubmedbert.py", "chunked_list": ["from transformers import BertTokenizerFast\n\tfrom tokenizer_wrappers.transformer_tokenizer import TransformerTokenizer\n\tclass PubMedBERTTokenizer(TransformerTokenizer):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.lang = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n\t        self.unk_token = \"[UNK]\"\n\t        self.zero_token = \"[UNK]\"\n\t        self.sep_token = \"[SEP]\"\n\t        self.pad_token = \"[PAD]\"\n", "        self.cls_token = \"[CLS]\"\n\t        self.mask_token = \"[MASK]\"\n\t        self.bos_token = None\n\t        self.eos_token = None\n\t        # Special tokens\n\t        self.subtoken_prefix = \"##\"\n\t        self.token_space_prefix = None\n\t        self.tok = BertTokenizerFast.from_pretrained(self.lang, do_lower_case=True, unk_token=self.unk_token, sep_token=self.sep_token, pad_token=self.pad_token, cls_token=self.cls_token, mask_token=self.mask_token, clean_text=True)\n\t        self._update_special_tokens()\n"]}
{"filename": "tokenizer_wrappers/distilroberta.py", "chunked_list": ["from transformers import RobertaTokenizerFast\n\tfrom tokenizer_wrappers.transformer_tokenizer import TransformerTokenizer\n\tclass DistilRoBERTaTokenizer(TransformerTokenizer):\n\t    def __init__(self):\n\t        super(DistilRoBERTaTokenizer, self).__init__()\n\t        self.lang = \"distilroberta-base\"\n\t        self.bos_token = \"<s>\"\n\t        self.eos_token = \"</s>\"\n\t        self.sep_token = \"</s>\"\n\t        self.cls_token = \"<s>\"\n", "        self.unk_token = \"<unk>\"\n\t        self.zero_token = \"<unk>\"\n\t        self.pad_token = \"<pad>\"\n\t        self.mask_token = \"<mask>\"\n\t        self.subtoken_prefix = None  # No need, because if token_space_prefix not present, it's a subword\n\t        self.token_space_prefix = \"Ä \"\n\t        self.tok = RobertaTokenizerFast.from_pretrained(self.lang, do_lower_case=True, bos_token=self.bos_token, eos_token=self.eos_token, sep_token=self.sep_token, cls_token=self.cls_token, unk_token=self.unk_token, pad_token=self.pad_token, mask_token=self.mask_token, add_prefix_space=True)\n\t        self._update_special_tokens()\n\t    def get_word_ranges(self, batch_tokenized_sentences: list) -> list:\n\t        word_ranges = [[] for _ in batch_tokenized_sentences]\n", "        for s_idx, sentence_tokens in enumerate(batch_tokenized_sentences):\n\t            token_i = 0\n\t            while token_i < len(sentence_tokens):\n\t                next_token_i = token_i + 1\n\t                while next_token_i < len(sentence_tokens):\n\t                    if sentence_tokens[next_token_i].startswith(self.token_space_prefix):\n\t                        break\n\t                    if sentence_tokens[next_token_i] in self.special_tokens and sentence_tokens[token_i].replace(self.token_space_prefix, \"\") != \"\":\n\t                        break\n\t                    next_token_i += 1\n", "                word_ranges[s_idx].append((token_i, next_token_i))\n\t                token_i = next_token_i\n\t        return word_ranges\n"]}
{"filename": "tokenizer_wrappers/biolinkbert.py", "chunked_list": ["from transformers import BertTokenizerFast\n\tfrom tokenizer_wrappers.transformer_tokenizer import TransformerTokenizer\n\tclass BioLinkBERTTokenizer(TransformerTokenizer):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.lang = \"michiyasunaga/BioLinkBERT-base\"\n\t        self.unk_token = \"[UNK]\"\n\t        self.zero_token = \"[UNK]\"\n\t        self.sep_token = \"[SEP]\"\n\t        self.pad_token = \"[PAD]\"\n", "        self.cls_token = \"[CLS]\"\n\t        self.mask_token = \"[MASK]\"\n\t        self.bos_token = None\n\t        self.eos_token = None\n\t        # Special tokens\n\t        self.subtoken_prefix = \"##\"\n\t        self.token_space_prefix = None\n\t        self.tok = BertTokenizerFast.from_pretrained(self.lang, do_lower_case=True, unk_token=self.unk_token, sep_token=self.sep_token, pad_token=self.pad_token, cls_token=self.cls_token, mask_token=self.mask_token, clean_text=True)\n\t        self._update_special_tokens()\n"]}
{"filename": "tokenizer_wrappers/common.py", "chunked_list": ["import torch as ch\n\tclass BaseTokenizer(object):\n\t    def __call__(self, *args, **kwargs):\n\t        return self.tokenize(*args, **kwargs)\n\t    def tokenize(self, batch_of_sentences, max_length, device=None):\n\t        return_val = self.tokenize_from_sentences(batch_of_sentences, max_length)\n\t        # Move tensors to specific device\n\t        if device is not None:\n\t            return_val[\"tokenized\"][\"input_ids\"] = return_val[\"tokenized\"][\"input_ids\"].to(device)\n\t            return_val[\"tokenized\"][\"token_type_ids\"] = return_val[\"tokenized\"][\"token_type_ids\"].to(device)\n", "            return_val[\"tokenized\"][\"attention_mask\"] = return_val[\"tokenized\"][\"attention_mask\"].to(device)\n\t        return return_val\n\t    def tokenize_from_words(self, input_words, word_mask, max_length, device=None):\n\t        return self.tokenize(self.decode_from_words(input_words, word_mask), max_length, device=device)\n\t    def get_word_ranges(self, batch_tokenized_sentences: list) -> list:\n\t        word_ranges = [[] for _ in batch_tokenized_sentences]\n\t        # \"\"\"\n\t        # No subword tokenization\n\t        # \"\"\"\n\t        if self.subtoken_prefix is None and self.token_space_prefix is None:\n", "            for s_idx, sentence_tokens in enumerate(batch_tokenized_sentences):\n\t                word_ranges[s_idx] = [(i, i + 1) for i in range(len(sentence_tokens))]\n\t        # \"\"\"\n\t        # Subword tokenization\n\t        # \"\"\"\n\t        else:\n\t            for s_idx, sentence_tokens in enumerate(batch_tokenized_sentences):\n\t                token_i = 0\n\t                while token_i < len(sentence_tokens):\n\t                    if sentence_tokens[token_i] in self.special_tokens:\n", "                        word_ranges[s_idx].append((token_i, token_i + 1))\n\t                        token_i += 1\n\t                        continue\n\t                    if self.token_space_prefix is None and self.subtoken_prefix is not None:\n\t                        next_token_i = token_i + 1\n\t                        while next_token_i < len(sentence_tokens) and sentence_tokens[next_token_i].startswith(self.subtoken_prefix):\n\t                            next_token_i += 1\n\t                        word_ranges[s_idx].append((token_i, next_token_i))\n\t                        token_i = next_token_i\n\t                    elif self.subtoken_prefix is None and self.token_space_prefix is not None:\n", "                        next_token_i = token_i + 1\n\t                        while next_token_i < len(sentence_tokens) and not sentence_tokens[next_token_i].startswith(self.token_space_prefix) and sentence_tokens[next_token_i] not in self.special_tokens:\n\t                            next_token_i += 1\n\t                        word_ranges[s_idx].append((token_i, next_token_i))\n\t                        token_i = next_token_i\n\t                    else:\n\t                        raise NotImplementedError(\"Ranges with both subtoken prefix and space prefix not implemented\")\n\t        return word_ranges\n\t    def extract_words(self, batch_input_tokens: list, word_ranges: list) -> tuple:\n\t        batch_words = []\n", "        for s_idx, tokens in enumerate(batch_input_tokens):\n\t            stripped_tokens = [tok.replace(self.subtoken_prefix, \"\") if self.subtoken_prefix else tok for tok in tokens]\n\t            stripped_tokens = [tok.replace(self.token_space_prefix, \"\") if self.token_space_prefix else tok for tok in stripped_tokens]\n\t            words = [\"\".join(stripped_tokens[word_range[0]: word_range[1]]) for word_range in word_ranges[s_idx]]\n\t            batch_words.append(words)\n\t        word_mask = [[word not in self.special_tokens.difference({self.mask_token, self.unk_token}) for word in words] for words in batch_words]\n\t        token_word_mask = []\n\t        for s_idx, word_range in enumerate(word_ranges):\n\t            sample_token_mask = []\n\t            for w_idx, (w_start, w_end) in enumerate(word_range):\n", "                for t_idx in range(w_start, w_end):\n\t                    sample_token_mask.append(word_mask[s_idx][w_idx])\n\t            token_word_mask.append(sample_token_mask)\n\t        return batch_words, word_mask, token_word_mask\n\t    def _update_special_tokens(self):\n\t        self.special_tokens = {self.unk_token, self.sep_token, self.pad_token, self.cls_token, self.mask_token, self.bos_token, self.eos_token, self.zero_token}\n\t    @staticmethod\n\t    def decode_from_words(batch_of_word_lists, word_mask):\n\t        sentences = [\" \".join([w for w_idx, w in enumerate(word_list) if word_mask[s_idx][w_idx]]) for s_idx, word_list in enumerate(batch_of_word_lists)]\n\t        return sentences\n", "    @staticmethod\n\t    def get_valid_words(batch_of_word_lists, word_mask):\n\t        return [[w for w_idx, w in enumerate(word_list) if word_mask[s_idx][w_idx]] for s_idx, word_list in enumerate(batch_of_word_lists)]\n\t    def convert_to_lower(self, _str):\n\t        _str = _str.lower()\n\t        for _tok in self.special_tokens:\n\t            if _tok is not None and _tok.lower() in _str:\n\t                _str = _str.replace(_tok.lower(), _tok)\n\t        return _str\n\tdef update_input_data(data_dict, seq_len, pad_token, pad_id=0):\n", "    with ch.no_grad():\n\t        for _i in range(len(data_dict[\"input_tokens\"])):\n\t            data_dict[\"input_tokens\"][_i] = data_dict[\"input_tokens\"][_i] + (seq_len - len(data_dict[\"input_tokens\"][_i])) * [pad_token]\n\t        helper_in_ids = ch.zeros(size=(data_dict[\"input_ids\"].size(0), seq_len), dtype=data_dict[\"input_ids\"].dtype, device=data_dict[\"input_ids\"].device)\n\t        # Add the ID of pad token\n\t        helper_in_ids += pad_id\n\t        # Fill data with actual IDs\n\t        for _i, _m in enumerate(data_dict[\"input_ids\"].unbind()):\n\t            helper_in_ids[_i, :len(_m)] = _m\n\t        data_dict[\"input_ids\"] = helper_in_ids\n", "        helper_mask = ch.zeros(size=(data_dict[\"attention_mask\"].size(0), seq_len), dtype=data_dict[\"attention_mask\"].dtype, device=data_dict[\"attention_mask\"].device)\n\t        for _i, _m in enumerate(data_dict[\"attention_mask\"].unbind()):\n\t            helper_mask[_i, :len(_m)] = _m\n\t        data_dict[\"attention_mask\"] = helper_mask\n\t        helper_ids = ch.zeros(size=(data_dict[\"token_type_ids\"].size(0), seq_len), dtype=data_dict[\"token_type_ids\"].dtype, device=data_dict[\"token_type_ids\"].device)\n\t        for _i, _m in enumerate(data_dict[\"token_type_ids\"].unbind()):\n\t            helper_ids[_i, :len(_m)] = _m\n\t        data_dict[\"token_type_ids\"] = helper_ids\n\t    return data_dict"]}
{"filename": "tokenizer_wrappers/roberta.py", "chunked_list": ["from transformers import RobertaTokenizerFast\n\tfrom tokenizer_wrappers.transformer_tokenizer import TransformerTokenizer\n\tclass RoBERTaTokenizer(TransformerTokenizer):\n\t    def __init__(self):\n\t        super(RoBERTaTokenizer, self).__init__()\n\t        self.lang = \"roberta-base\"\n\t        self.bos_token = \"<s>\"\n\t        self.eos_token = \"</s>\"\n\t        self.sep_token = \"</s>\"\n\t        self.cls_token = \"<s>\"\n", "        self.unk_token = \"<unk>\"\n\t        self.zero_token = \"<unk>\"\n\t        self.pad_token = \"<pad>\"\n\t        self.mask_token = \"<mask>\"\n\t        self.subtoken_prefix = None  # No need, because if token_space_prefix not present, it's a subword\n\t        self.token_space_prefix = \"Ä \"\n\t        self.tok = RobertaTokenizerFast.from_pretrained(self.lang, do_lower_case=True, bos_token=self.bos_token, eos_token=self.eos_token, sep_token=self.sep_token, cls_token=self.cls_token, unk_token=self.unk_token, pad_token=self.pad_token, mask_token=self.mask_token, add_prefix_space=True)\n\t        self._update_special_tokens()\n\t    def get_word_ranges(self, batch_tokenized_sentences: list) -> list:\n\t        word_ranges = [[] for _ in batch_tokenized_sentences]\n", "        for s_idx, sentence_tokens in enumerate(batch_tokenized_sentences):\n\t            token_i = 0\n\t            while token_i < len(sentence_tokens):\n\t                next_token_i = token_i + 1\n\t                while next_token_i < len(sentence_tokens):\n\t                    if sentence_tokens[next_token_i].startswith(self.token_space_prefix):\n\t                        break\n\t                    if sentence_tokens[next_token_i] in self.special_tokens and sentence_tokens[token_i].replace(elf.token_space_prefix, \"\") != \"\":\n\t                        break\n\t                    next_token_i += 1\n", "                word_ranges[s_idx].append((token_i, next_token_i))\n\t                token_i = next_token_i\n\t        return word_ranges\n"]}
{"filename": "embeddings/clinicallongformer.py", "chunked_list": ["import torch as ch\n\tfrom transformers import LongformerModel\n\tclass ClinicalLongformerEmbedding(ch.nn.Module):\n\t    def __init__(self, word_embeddings=None):\n\t        super().__init__()\n\t        if word_embeddings is None:\n\t            self.word_embeddings = LongformerModel.from_pretrained(\"yikuan8/Clinical-Longformer\").embeddings.word_embeddings\n\t        else:\n\t            self.word_embeddings = word_embeddings\n\t    def __call__(self, tokenized):\n", "        embedding_output = self.word_embeddings(tokenized[\"tokenized\"][\"input_ids\"])\n\t        return {\"input_embeds\": embedding_output, \"token_type_ids\": tokenized[\"tokenized\"][\"token_type_ids\"], \"attention_mask\": tokenized[\"tokenized\"][\"attention_mask\"], \"global_attention_mask\": tokenized[\"tokenized\"][\"global_attention_mask\"]}\n"]}
{"filename": "embeddings/__init__.py", "chunked_list": ["from .roberta import RoBERTaEmbedding\n\tfrom .biolinkbert import BioLinkBERTEmbedding\n\tfrom .clinicallongformer import ClinicalLongformerEmbedding\n\tembedding_map = {\"roberta\": RoBERTaEmbedding, \"biolinkbert\": BioLinkBERTEmbedding, \"clinicallongformer\": ClinicalLongformerEmbedding}"]}
{"filename": "embeddings/biolinkbert.py", "chunked_list": ["import torch as ch\n\tfrom transformers import AutoModel\n\tclass BioLinkBERTEmbedding(ch.nn.Module):\n\t    def __init__(self, word_embeddings=None):\n\t        super().__init__()\n\t        if word_embeddings is None:\n\t            self.word_embeddings = AutoModel.from_pretrained(\"michiyasunaga/BioLinkBERT-base\").embeddings.word_embeddings\n\t        else:\n\t            self.word_embeddings = word_embeddings\n\t    def __call__(self, tokenized):\n", "        embedding_output = self.word_embeddings(tokenized[\"tokenized\"][\"input_ids\"])\n\t        return {\"input_embeds\": embedding_output, \"token_type_ids\": tokenized[\"tokenized\"][\"token_type_ids\"], \"attention_mask\": tokenized[\"tokenized\"][\"attention_mask\"]}\n"]}
{"filename": "embeddings/roberta.py", "chunked_list": ["import torch as ch\n\tfrom transformers import RobertaModel\n\tclass RoBERTaEmbedding(ch.nn.Module):\n\t    def __init__(self, word_embeddings=None):\n\t        super().__init__()\n\t        if word_embeddings is None:\n\t            self.word_embeddings = RobertaModel.from_pretrained(\"roberta-base\").embeddings.word_embeddings\n\t        else:\n\t            self.word_embeddings = word_embeddings\n\t    def __call__(self, tokenized):\n", "        embedding_output = self.word_embeddings(tokenized[\"tokenized\"][\"input_ids\"])\n\t        return {\"input_embeds\": embedding_output, \"token_type_ids\": tokenized[\"tokenized\"][\"token_type_ids\"], \"attention_mask\": tokenized[\"tokenized\"][\"attention_mask\"]}\n"]}
{"filename": "explainers/__init__.py", "chunked_list": ["import torch as ch\n\tfrom typing import AnyStr, List, Tuple\n\tfrom captum.attr import Saliency, IntegratedGradients, DeepLift  # noqa: F401\n\tclass Attention(object):\n\t    def __init__(self, model):\n\t        self.model = model\n\t    def attribute(self, inputs, additional_forward_args=None, **kwargs):\n\t        if additional_forward_args is None:\n\t            additional_forward_args = tuple()\n\t        _, att_weights = self.model(inputs, *additional_forward_args, return_att_weights=True)\n", "        return att_weights.unsqueeze(-1)\n\texplainer_map = {\"ig\": IntegratedGradients, \"s\": Saliency, \"dl\": DeepLift, \"a\": Attention}\n\tdef wordwise_embedding_attributions(attributions: ch.Tensor, word_ranges: List[Tuple[int]], word_mask: List[List[bool]], reduction: AnyStr = \"mean\") -> List[ch.Tensor]:\n\t    reduced_expl = []\n\t    for s_idx, expl_v in enumerate(attributions.unbind()):\n\t        sample_expl = ch.zeros(size=(sum(word_mask[s_idx]),) + expl_v.size()[1:], device=attributions.device,\n\t                               dtype=attributions.dtype)\n\t        w_ctr = 0\n\t        for w_idx, (w_start, w_end) in enumerate(word_ranges[s_idx]):\n\t            if word_mask[s_idx][w_idx]:\n", "                if reduction == \"mean\":\n\t                    sample_expl[w_ctr] = ch.mean(expl_v[w_start:w_end, ...], dim=0)\n\t                    w_ctr += 1\n\t                else:\n\t                    raise NotImplementedError(f\"Reduction {reduction} not supported\")\n\t        if w_ctr != sum(word_mask[s_idx]):\n\t            raise ValueError(\"Not all words accounted for in attribution reduction\")\n\t        reduced_expl.append(sample_expl.flatten())\n\t    return reduced_expl\n\tdef wordwise_attributions(_input):\n", "    return _input.sum(-1)"]}
{"filename": "losses/__init__.py", "chunked_list": ["import torch as ch\n\timport torch.nn.functional as f\n\tdef _cosine(ref_data, mod_data, normalize_to_loss=False):\n\t    v_data, v_ref_data = mod_data.view(-1), ref_data.view(-1)\n\t    _cos = f.cosine_similarity(v_data, v_ref_data, 0, 1e-8)\n\t    if normalize_to_loss:\n\t        return ch.ones_like(_cos) - (_cos + ch.ones_like(_cos)) / (2.0 * ch.ones_like(_cos))\n\t    return _cos\n\tloss_map = {\"cos\": _cosine}\n"]}
{"filename": "scripts/train.py", "chunked_list": ["import json, os, argparse\n\timport torch as ch\n\timport torch.nn.functional as f\n\timport pytorch_lightning as pl\n\tfrom scripts.base_trainer import BaseClassifier, get_trainer\n\tfrom models import model_map\n\tfrom data import data_map\n\tfrom constants import WRONG_ORDS\n\tfrom utils import get_dirname\n\tclass Classifier(BaseClassifier):\n", "    def __init__(self, model, datamodule, optimizer_name=\"adamw\", learning_rate=0.001, weight_decay=0.0):\n\t        super().__init__(model=model, optimizer_name=optimizer_name, learning_rate=learning_rate, weight_decay=weight_decay, datamodule=datamodule)\n\t    def training_step(self, batch, batch_idx):\n\t        data_sentences, labels = batch\n\t        data_sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in data_sentences]\n\t        tokenized = self.tokenizer(data_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t        embeds = self.embedding(tokenized)\n\t        preds = self.model(**embeds)\n\t        if not self.multilabel:\n\t            if self.datamodule.pos_weights is not None:\n", "                loss = f.cross_entropy(preds, labels, weight=self.datamodule.pos_weights.to(preds.device))\n\t            else:\n\t                loss = f.cross_entropy(preds, labels)\n\t        else:\n\t            if self.datamodule.pos_weights is not None:\n\t                loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32), pos_weight=self.datamodule.pos_weights.to(preds.device))\n\t            else:\n\t                loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32))\n\t        return loss\n\t    def validation_step(self, batch, batch_idx):\n", "        with ch.no_grad():\n\t            data_sentences, labels = batch\n\t            data_sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in data_sentences]\n\t            tokenized = self.tokenizer(data_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            embeds = self.embedding(tokenized)\n\t            preds = self.model(**embeds)\n\t        return preds, labels\n\t    def validation_epoch_end(self, validation_step_outputs, stage_suffix=\"v\"):\n\t        preds = ch.cat([t[0] for t in validation_step_outputs], dim=0)\n\t        labels = ch.cat([t[1] for t in validation_step_outputs], dim=0)\n", "        if self.trainer.world_size == 1:\n\t            all_preds = preds\n\t            all_labels = labels\n\t        else:\n\t            all_preds = ch.cat(self.all_gather(preds).unbind(), dim=0)\n\t            all_labels = ch.cat(self.all_gather(labels).unbind(), dim=0)\n\t        if not self.multilabel:\n\t            loss = f.cross_entropy(all_preds, all_labels)\n\t        else:\n\t            loss = f.binary_cross_entropy_with_logits(all_preds, all_labels.to(ch.float32))\n", "        log_dict = {f\"loss_{stage_suffix}\": loss}\n\t        for n, metric, _ in self.logged_metrics:\n\t            for avg in [\"macro\", \"micro\"]:\n\t                log_dict[f\"{n}_{stage_suffix}_{avg}\"] = self.metrics[f\"{n}_{avg}\"].to(self.device)(all_preds, all_labels.to(ch.int32))\n\t        self.log_dict(log_dict, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True, rank_zero_only=True)\n\tif __name__ == \"__main__\":\n\t    with open(os.path.join(get_dirname(__file__), \"van_config.json\")) as json_file:\n\t        config = json.load(json_file)\n\t    args = argparse.Namespace(**config)\n\t    pl.seed_everything(args.overall_seed, workers=True)\n", "    dataset = data_map[args.dataset](batch_size=args.batch_size, seed=args.data_seed)\n\t    model = model_map[args.model].init_for_dataset(args.dataset, args.model_path)\n\t    classifier = Classifier(model, optimizer_name=args.optimizer_name, learning_rate=args.learning_rate, weight_decay=args.weight_decay, datamodule=dataset)\n\t    classifier.print = args.print\n\t    trainer = get_trainer(args)\n\t    if not args.only_eval:\n\t        trainer.fit(model=classifier, datamodule=dataset, ckpt_path=args.checkpoint_path)\n\t    trainer.test(model=classifier, datamodule=dataset, verbose=True)\n"]}
{"filename": "scripts/far_train.py", "chunked_list": ["import os\n\timport argparse\n\timport json\n\timport numpy as np\n\tfrom time import time\n\timport torch as ch\n\timport torch.nn.functional as f\n\timport pytorch_lightning as pl\n\timport torchmetrics.functional as tmf\n\tfrom scripts.base_trainer import BaseClassifier, get_trainer\n", "from models import model_map\n\tfrom data import data_map\n\tfrom attacks import attack_map\n\tfrom explainers import explainer_map\n\tfrom losses import loss_map\n\tfrom sentence_encoders import sentence_encoder_map\n\tfrom constants import WRONG_ORDS\n\tfrom utils import filter_data, get_multilabel_predicted_labels, get_dirname\n\tclass FARClassifier(BaseClassifier):\n\t    def __init__(self, model, explainer, datamodule, optimizer_name: str = \"adam\", learning_rate: float = 0.00001, weight_decay: float = 0.0,\n", "            delta: float = 5.0, attack_name: str = \"ef\", candidate_extractor_name: str = \"distilroberta\", attack_ratio: float = 0.75, rho_max: float = 0.15,\n\t            rho_batch: float = 0.0, num_candidates: int = 15, adversarial_preds: bool = False, relax_multilabel_constraint: bool = False,\n\t            stop_word_filter: bool = True, random_word_importance: bool = False, random_synonym: bool = False, attribution_loss_name: str = \"cos\"):\n\t        super().__init__(model=model, optimizer_name=optimizer_name, learning_rate=learning_rate, weight_decay=weight_decay, datamodule=datamodule)\n\t        self.explainer = explainer\n\t        self.attack_algo = attack_map[attack_name](candidate_extractor_name=candidate_extractor_name)\n\t        self.dataset_name = datamodule.name\n\t        self.attack_ratio = attack_ratio\n\t        self.delta = delta\n\t        self.rho_max = rho_max\n", "        self.rho_batch = rho_batch\n\t        self.num_candidates = num_candidates\n\t        self.adversarial_preds = adversarial_preds\n\t        self.relax_multilabel_constraint = relax_multilabel_constraint\n\t        self.stop_word_filter = stop_word_filter\n\t        self.random_word_importance = random_word_importance\n\t        self.random_synonym = random_synonym\n\t        self.attribution_loss_name = attribution_loss_name\n\t        self.med_bce = sentence_encoder_map[\"med_bce\"]()\n\t    def training_step(self, batch, batch_idx):\n", "        with ch.no_grad():\n\t            data_sentences, labels = batch\n\t            tokenized = self.tokenizer(data_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            sentences = self.tokenizer.decode_from_words(tokenized[\"input_words\"], tokenized[\"word_mask\"])\n\t            sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in sentences]\n\t        attack_start = time()\n\t        is_attacked = np.random.random_sample(size=(len(sentences),)) < self.attack_ratio\n\t        if sum(is_attacked) > 0:\n\t            adv_sentences, _, _ = self.attack_algo.attack(sentences=[s for i, s in enumerate(sentences) if is_attacked[i]], model=self.model,\n\t                multilabel=self.datamodule.multilabel, tokenizer_module=self.tokenizer, embedding_module=self.embedding, explainer_module=self.explainer,\n", "                device=self.device, rho_max=self.rho_max, rho_batch=self.rho_batch, num_candidates=self.num_candidates, adversarial_preds=self.adversarial_preds,\n\t                delta=self.delta, stop_word_filter=self.stop_word_filter, random_word_importance=self.random_word_importance, random_synonym=self.random_synonym,\n\t                attribution_loss_fn_name=self.attribution_loss_name, attribution_loss_fn_args=None)\n\t        else:\n\t            adv_sentences = []\n\t        self.model.zero_grad()\n\t        attack_time = time() - attack_start\n\t        # Overwrite adversarial sentences\n\t        train_sentences = [adv_sentences.pop(0) if is_attacked[i] else s for i, s in enumerate(sentences)]\n\t        if len(adv_sentences) > 0:\n", "            raise ValueError(\"Not all adversarial sentences overwritten\")\n\t        with ch.no_grad():\n\t            tokenized = self.tokenizer(train_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            embeds = self.embedding(tokenized)\n\t        preds = self.model(**embeds)\n\t        orig_explanations = self.attack_algo.get_explanations(sentences, self.model, self.tokenizer, self.embedding, self.explainer, labels=labels, device=self.device, multilabel=self.multilabel)\n\t        adv_explanations = self.attack_algo.get_explanations(train_sentences, self.model, self.tokenizer, self.embedding, self.explainer, labels=labels, device=self.device, multilabel=self.multilabel)\n\t        if not self.multilabel:\n\t            if self.datamodule.pos_weights is not None:\n\t                p_loss = f.cross_entropy(preds, labels, weight=self.datamodule.pos_weights.to(preds.device))\n", "            else:\n\t                p_loss = f.cross_entropy(preds, labels)\n\t        else:\n\t            if self.datamodule.pos_weights is not None:\n\t                p_loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32), pos_weight=self.datamodule.pos_weights.to(preds.device))\n\t            else:\n\t                p_loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32))\n\t        for s_idx, (oe, ae) in enumerate(zip(orig_explanations, adv_explanations)):\n\t            if oe.size() != ae.size():\n\t                print(\"WARNING wrong explanation sizes\")\n", "        e_loss = ch.stack([loss_map[self.attribution_loss_name](oe, ae, normalize_to_loss=True) for oe, ae in zip(orig_explanations, adv_explanations) if oe.size() == ae.size()]).mean()\n\t        loss = p_loss + self.delta * e_loss\n\t        adv_preds, adv_labels = preds[is_attacked], labels[is_attacked]\n\t        clean_preds, clean_labels = preds[~is_attacked], labels[~is_attacked]\n\t        log_dict = {\"a_time\": float(attack_time / max(sum(is_attacked), 1)), \"num_a\": float(sum(is_attacked)), \"num_c\": float(sum(~is_attacked)),\n\t            \"loss_tr\": loss, \"e_loss_tr\": e_loss, \"p_loss_tr\": p_loss, \"acc_tr_a\": tmf.accuracy(adv_preds, adv_labels.to(ch.int32), num_classes=self.model.num_classes) if len(adv_preds) > 0 else 0.0,\n\t            \"acc_tr_c\": tmf.accuracy(clean_preds, clean_labels.to(ch.int32), num_classes=self.model.num_classes) if len(clean_preds) > 0 else 0.0,\n\t            \"f1_tr_a\": tmf.f1_score(adv_preds, adv_labels.to(ch.int32), num_classes=self.model.num_classes, average=\"macro\") if len(adv_preds) > 0 else 0.0,\n\t            \"f1_tr_c\": tmf.f1_score(clean_preds, clean_labels.to(ch.int32), num_classes=self.model.num_classes, average=\"macro\") if len(clean_preds) > 0 else 0.0}\n\t        if self.multilabel:\n", "            for top_k_r in [0.1, 0.16, 0.3]:\n\t                top_k = int(round(top_k_r * self.model.num_classes))\n\t                if top_k <= self.model.num_classes:\n\t                    log_dict[f\"p_{top_k}_tr_a\"] = tmf.precision(adv_preds, adv_labels.to(ch.int32), num_classes=self.model.num_classes, top_k=top_k) if len(adv_preds) > 0 else 0.0\n\t                    log_dict[f\"p_{top_k}_tr_c\"] = tmf.precision(clean_preds, clean_labels.to(ch.int32), num_classes=self.model.num_classes, top_k=top_k) if len(clean_preds) > 0 else 0.0\n\t        self.log_dict(log_dict, prog_bar=True, on_step=True, on_epoch=False, batch_size=len(data_sentences))\n\t        return loss\n\t    def validation_step(self, batch, batch_idx):\n\t        with ch.no_grad():\n\t            data_sentences, labels = batch\n", "            data_sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in data_sentences]\n\t            tokenized = self.tokenizer(data_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            sentences = self.tokenizer.decode_from_words(tokenized[\"input_words\"], tokenized[\"word_mask\"])\n\t            sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in sentences]\n\t            tokenized = self.tokenizer(sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            embeds = self.embedding(tokenized)\n\t            preds = self.model(**embeds)\n\t            # Filter correct preds\n\t            if self.multilabel:\n\t                fil = get_multilabel_predicted_labels(preds) == labels\n", "                _d_filter = ch.eq(fil.sum(dim=1), ch.zeros_like(fil.sum(dim=1)) + self.model.num_classes)\n\t                # Filter no labels\n\t                _d_filter = _d_filter * (labels.sum(dim=1) > 0)\n\t                if self.relax_multilabel_constraint:\n\t                    _d_filter = ch.ones_like(_d_filter, dtype=ch.bool)\n\t            else:\n\t                _d_filter = ch.argmax(preds, dim=1) == labels\n\t            # If no data is correctly classified\n\t            if _d_filter.sum() < 1:\n\t                print(\"No correctly predicted samples in batch\")\n", "                return (None,)*12\n\t            sentences, labels = filter_data(sentences, fil=_d_filter), filter_data(labels, fil=_d_filter)\n\t            from time import time\n\t            start = time()\n\t            adv_sentences, replacements, _ = self.attack_algo.attack(sentences=sentences, model=self.model, multilabel=self.datamodule.multilabel, tokenizer_module=self.tokenizer,\n\t                embedding_module=self.embedding, explainer_module=self.explainer, device=self.device, rho_max=self.rho_max,\n\t                rho_batch=self.rho_batch, num_candidates=self.num_candidates, adversarial_preds=False, delta=self.delta,\n\t                stop_word_filter=self.stop_word_filter, random_word_importance=self.random_word_importance, random_synonym=self.random_synonym,\n\t                attribution_loss_fn_name=self.attribution_loss_name, attribution_loss_fn_args=None, detach=True)\n\t        self.model.zero_grad()\n", "        avg_attack_time = (time() - start)/len(adv_sentences)\n\t        orig_tokenized = self.tokenizer(sentences, max_length=self.model.input_shape[0], device=self.device)\n\t        adv_tokenized = self.tokenizer(adv_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t        orig_embeds = self.embedding(orig_tokenized)\n\t        adv_embeds = self.embedding(adv_tokenized)\n\t        with ch.no_grad():\n\t            if self.multilabel:\n\t                orig_preds = self.model(**orig_embeds)\n\t                orig_labels = get_multilabel_predicted_labels(orig_preds)\n\t                adv_preds = self.model(**adv_embeds)\n", "                adv_labels = get_multilabel_predicted_labels(adv_preds)\n\t            else:\n\t                orig_preds = f.softmax(self.model(**orig_embeds), dim=1)\n\t                adv_preds = f.softmax(self.model(**adv_embeds), dim=1)\n\t                orig_labels = ch.argmax(orig_preds, dim=1)\n\t                adv_labels = ch.argmax(adv_preds, dim=1)\n\t        orig_explanations = self.attack_algo.get_explanations(self.tokenizer.decode_from_words(orig_tokenized[\"input_words\"], orig_tokenized[\"word_mask\"]),\n\t            self.model, self.tokenizer, self.embedding, self.explainer, labels=orig_labels, device=self.device,\n\t            multilabel=self.multilabel, detach=True)\n\t        self.model.zero_grad()\n", "        adv_explanations = self.attack_algo.get_explanations(self.tokenizer.decode_from_words(adv_tokenized[\"input_words\"], adv_tokenized[\"word_mask\"]),\n\t            self.model, self.tokenizer, self.embedding, self.explainer, labels=adv_labels, device=self.device,\n\t            multilabel=self.multilabel, detach=True)\n\t        e_sim = [loss_map[self.attribution_loss_name](ox, ax).detach().item() for ox, ax in zip(orig_explanations, adv_explanations) if ox.size() == ax.size()]\n\t        e_loss = ch.stack([loss_map[self.attribution_loss_name](oe, ae, normalize_to_loss=True) for oe, ae in zip(orig_explanations, adv_explanations) if oe.size() == ae.size()])\n\t        ss_use, ss_tse, ss_ce, ss_pp, ss_med_bce, ss_bertscore = [], [], [], [], [], []\n\t        for s_idx, s in enumerate(sentences):\n\t            for enc_name, enc, res_l in [(\"med_bce\", self.med_bce, ss_med_bce)]:\n\t                sim = enc.semantic_sim(sentence1=sentences[s_idx], sentence2=adv_sentences[s_idx], reduction=\"min\").detach().item()\n\t                res_l.append(sim)\n", "        med_ks = [(el / ((1.0 - medss) + 0.0000001)).item() for el, medss in zip(e_loss, ss_med_bce)]\n\t        return (orig_preds.detach(), adv_preds.detach(), labels.to(ch.int32).detach(), e_loss.detach(), len(e_sim), avg_attack_time, e_sim, replacements, ss_med_bce, med_ks)\n\t    def validation_epoch_end(self, validation_step_outputs, stage_suffix=\"v\"):\n\t        if all([all(e is None for e in t) for t in validation_step_outputs]):\n\t            self.log_dict({\"e_loss_v_a\": 1e6}, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True, rank_zero_only=True)\n\t            return\n\t        preds = ch.cat([t[0] for t in validation_step_outputs if t[0] is not None], dim=0)\n\t        adv_preds = ch.cat([t[1] for t in validation_step_outputs if t[1] is not None], dim=0)\n\t        labels = ch.cat([t[2] for t in validation_step_outputs if t[2] is not None], dim=0)\n\t        e_loss = ch.cat([t[3] for t in validation_step_outputs if t[3] is not None], dim=0)\n", "        if self.trainer.world_size == 1 or True:\n\t            all_e_loss = e_loss\n\t            all_preds = preds\n\t            all_adv_preds = adv_preds\n\t            all_labels = labels\n\t        else:\n\t            all_e_loss = ch.cat(self.all_gather(e_loss).unbind(), dim=0)\n\t            all_preds = ch.cat(self.all_gather(preds).unbind(), dim=0)\n\t            all_adv_preds = ch.cat(self.all_gather(adv_preds).unbind(), dim=0)\n\t            all_labels = ch.cat(self.all_gather(labels).unbind(), dim=0)\n", "        if not self.multilabel:\n\t            clean_loss = f.cross_entropy(all_preds, all_labels.to(ch.int64))\n\t            adv_loss = f.cross_entropy(all_adv_preds, all_labels.to(ch.int64))\n\t        else:\n\t            clean_loss = f.binary_cross_entropy_with_logits(all_preds, all_labels.to(ch.float32))\n\t            adv_loss = f.binary_cross_entropy_with_logits(all_adv_preds, all_labels.to(ch.float32))\n\t        log_dict = {f\"loss_{stage_suffix}_c\": clean_loss, f\"loss_{stage_suffix}_a\": adv_loss, f\"avg_e_loss_{stage_suffix}_a\": all_e_loss.mean()}\n\t        self.log_dict(log_dict, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True, rank_zero_only=True)\n\tif __name__ == \"__main__\":\n\t    with open(os.path.join(get_dirname(__file__), \"far_config.json\")) as json_file:\n", "        config = json.load(json_file)\n\t    args = argparse.Namespace(**config)\n\t    pl.seed_everything(args.overall_seed, workers=True)\n\t    dataset = data_map[args.dataset](batch_size=args.batch_size, seed=args.data_seed + 42)\n\t    model = model_map[args.model].init_for_dataset(args.dataset, args.model_path)\n\t    explainer = explainer_map[args.explainer](model)\n\t    classifier = FARClassifier(model, explainer, attack_name=\"ef\", candidate_extractor_name=args.candidate_extractor_name,\n\t        rho_max=args.rho_max, rho_batch=args.rho_batch, num_candidates=args.num_c, relax_multilabel_constraint=args.relax_multilabel_constraint,\n\t        stop_word_filter=args.stop_word_filter, random_word_importance=args.random_word_importance, random_synonym=args.random_synonym,\n\t        attribution_loss_name=args.attribution_loss_name, delta=args.expl_delta, optimizer_name=args.optimizer_name, learning_rate=args.learning_rate,\n", "        weight_decay=args.weight_decay, datamodule=dataset)\n\t    classifier.print = args.print\n\t    args.monitor_val = \"avg_e_loss_v_a\"\n\t    args.monitor_mode = \"min\"\n\t    trainer = get_trainer(args)\n\t    if not args.only_eval:\n\t        trainer.fit(model=classifier, datamodule=dataset, ckpt_path=args.checkpoint_path)\n\t    trainer.test(model=classifier, datamodule=dataset, verbose=True)\n"]}
{"filename": "scripts/adv_train.py", "chunked_list": ["import os, argparse, json\n\timport numpy as np\n\tfrom time import time\n\timport torch as ch\n\timport torch.nn.functional as f\n\timport pytorch_lightning as pl\n\tfrom torchmetrics import functional as tmf\n\tfrom scripts.base_trainer import BaseClassifier, get_trainer\n\tfrom models import model_map\n\tfrom data import data_map\n", "from attacks import attack_map\n\tfrom constants import WRONG_ORDS\n\tfrom utils import get_dirname\n\tclass AdvClassifier(BaseClassifier):\n\t    def __init__(self, model, datamodule, optimizer_name: str = \"adam\", learning_rate: float = 0.0001,\n\t            weight_decay: float = 0.0, attack_name: str = \"a2t\", candidate_extractor_name: str = \"distilroberta\",\n\t            attack_ratio: float = 0.75, rho_max: float = 0.15, rho_batch: float = 0.0, num_candidates: int = 15,\n\t            stop_word_filter: bool = True, random_word_importance: bool = False, random_synonym: bool = False,\n\t            attack_recall: bool = False):\n\t        super().__init__(model=model, optimizer_name=optimizer_name, learning_rate=learning_rate, weight_decay=weight_decay, datamodule=datamodule)\n", "        self.attack_algo = attack_map[attack_name](candidate_extractor_name=candidate_extractor_name)\n\t        self.dataset_name = datamodule.name\n\t        self.attack_ratio = attack_ratio\n\t        self.rho_max = rho_max\n\t        self.rho_batch = rho_batch\n\t        self.num_candidates = num_candidates\n\t        self.stop_word_filter = stop_word_filter\n\t        self.random_word_importance = random_word_importance\n\t        self.random_synonym = random_synonym\n\t        self.attack_recall = attack_recall\n", "    def training_step(self, batch, batch_idx):\n\t        with ch.no_grad():\n\t            data_sentences, labels = batch\n\t            tokenized = self.tokenizer(data_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            sentences = self.tokenizer.decode_from_words(tokenized[\"input_words\"], tokenized[\"word_mask\"])\n\t            sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in sentences]\n\t            is_attacked = np.random.random_sample(size=(len(sentences),)) < self.attack_ratio\n\t            attack_start = time()\n\t            if sum(is_attacked) > 0:\n\t                adv_sentences, _, _ = self.attack_algo.attack(sentences=[s for i, s in enumerate(sentences) if is_attacked[i]], model=self.model, labels=labels[is_attacked],\n", "                    multilabel=self.datamodule.multilabel, tokenizer_module=self.tokenizer, embedding_module=self.embedding,\n\t                    device=self.device, rho_max=self.rho_max, rho_batch=self.rho_batch, num_candidates=self.num_candidates,\n\t                    stop_word_filter=self.stop_word_filter, random_word_importance=self.random_word_importance, random_synonym=self.random_synonym,\n\t                    pos_weight=None, attack_recall=self.attack_recall)\n\t            else:\n\t                adv_sentences = []\n\t            attack_time = time() - attack_start\n\t            self.model.zero_grad()\n\t            train_sentences = [adv_sentences.pop(0) if is_attacked[i] else s for i, s in enumerate(sentences)]\n\t            if len(adv_sentences) > 0:\n", "                raise ValueError(\"Not all adversarial sentences overwritten\")\n\t            tokenized = self.tokenizer(train_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t        embeds = self.embedding(tokenized)\n\t        preds = self.model(**embeds)\n\t        if not self.multilabel:\n\t            if self.datamodule.pos_weights is not None:\n\t                loss = f.cross_entropy(preds, labels, weight=self.datamodule.pos_weights.to(preds.device))\n\t            else:\n\t                loss = f.cross_entropy(preds, labels)\n\t        else:\n", "            if self.datamodule.pos_weights is not None:\n\t                loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32), pos_weight=self.datamodule.pos_weights.to(preds.device))\n\t            else:\n\t                loss = f.binary_cross_entropy_with_logits(preds, labels.to(ch.float32))\n\t        # Extract log metrics\n\t        adv_preds, adv_labels = preds[is_attacked], labels[is_attacked]\n\t        clean_preds, clean_labels = preds[~is_attacked], labels[~is_attacked]\n\t        log_dict = {\"a_time\": float(attack_time / max(sum(is_attacked), 1)), \"num_a\": float(sum(is_attacked)), \"num_c\": float(sum(~is_attacked)),\n\t            \"loss_tr\": loss, \"acc_tr_a\": tmf.accuracy(adv_preds, adv_labels.to(ch.int32), num_classes=self.model.num_classes) if len(adv_preds) > 0 else 0.0,\n\t            \"acc_tr_c\": tmf.accuracy(clean_preds, clean_labels.to(ch.int32), num_classes=self.model.num_classes) if len(clean_preds) > 0 else 0.0,\n", "            \"f1_tr_a\": tmf.f1_score(adv_preds, adv_labels.to(ch.int32), num_classes=self.model.num_classes, average=\"macro\") if len(adv_preds) > 0 else 0.0,\n\t            \"f1_tr_c\": tmf.f1_score(clean_preds, clean_labels.to(ch.int32), num_classes=self.model.num_classes, average=\"macro\") if len(clean_preds) > 0 else 0.0,}\n\t        if self.multilabel:\n\t            for top_k_r in [0.1, 0.16, 0.3]:\n\t                top_k = int(round(top_k_r * self.model.num_classes))\n\t                if top_k <= self.model.num_classes:\n\t                    log_dict[f\"p_{top_k}_tr_a\"] = tmf.precision(adv_preds, adv_labels.to(ch.int32), num_classes=self.model.num_classes, top_k=top_k) if len(adv_preds) > 0 else 0.0\n\t                    log_dict[f\"p_{top_k}_tr_c\"] = tmf.precision(clean_preds, clean_labels.to(ch.int32), num_classes=self.model.num_classes, top_k=top_k) if len(clean_preds) > 0 else 0.0\n\t        # Log metrics\n\t        self.log_dict(log_dict, prog_bar=True, on_step=True, on_epoch=False, batch_size=len(data_sentences))\n", "        return loss\n\t    def validation_step(self, batch, batch_idx):\n\t        with ch.no_grad():\n\t            data_sentences, labels = batch\n\t            tokenized = self.tokenizer(data_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            sentences = self.tokenizer.decode_from_words(tokenized[\"input_words\"], tokenized[\"word_mask\"])\n\t            sentences = [\"\".join([c for c in s if ord(c) not in WRONG_ORDS]) for s in sentences]\n\t            tokenized = self.tokenizer(sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            embeds = self.embedding(tokenized)\n\t            clean_preds = self.model(**embeds)\n", "        attack_start = time()\n\t        adv_sentences, replacements, max_dist = self.attack_algo.attack(sentences=sentences, labels=labels, model=self.model, multilabel=self.datamodule.multilabel, tokenizer_module=self.tokenizer,\n\t            embedding_module=self.embedding, device=self.device, rho_max=self.rho_max, rho_batch=self.rho_batch, num_candidates=self.num_candidates,\n\t            stop_word_filter=self.stop_word_filter, random_word_importance=self.random_word_importance, random_synonym=self.random_synonym,\n\t            pos_weight=None, attack_recall=self.attack_recall)\n\t        with ch.no_grad():\n\t            adv_tokenized = self.tokenizer(adv_sentences, max_length=self.model.input_shape[0], device=self.device)\n\t            adv_embeds = self.embedding(adv_tokenized)\n\t            adv_preds = self.model(**adv_embeds)\n\t        return clean_preds, adv_preds, labels\n", "    def validation_epoch_end(self, validation_step_outputs, stage_suffix=\"v\"):\n\t        clean_preds = ch.cat([t[0] for t in validation_step_outputs], dim=0)\n\t        adv_preds = ch.cat([t[1] for t in validation_step_outputs], dim=0)\n\t        labels = ch.cat([t[2] for t in validation_step_outputs], dim=0)\n\t        if self.trainer.world_size == 1:\n\t            all_clean_preds = clean_preds\n\t            all_adv_preds = adv_preds\n\t            all_labels = labels\n\t        else:\n\t            all_clean_preds = ch.cat(self.all_gather(clean_preds).unbind(), dim=0)\n", "            all_adv_preds = ch.cat(self.all_gather(adv_preds).unbind(), dim=0)\n\t            all_labels = ch.cat(self.all_gather(labels).unbind(), dim=0)\n\t        if not self.multilabel:\n\t            clean_loss = f.cross_entropy(all_clean_preds, all_labels)\n\t            adv_loss = f.cross_entropy(all_adv_preds, all_labels)\n\t        else:\n\t            clean_loss = f.binary_cross_entropy_with_logits(all_clean_preds, all_labels.to(ch.float32))\n\t            adv_loss = f.binary_cross_entropy_with_logits(all_adv_preds, all_labels.to(ch.float32))\n\t        log_dict = {f\"loss_{stage_suffix}_c\": clean_loss, f\"loss_{stage_suffix}_a\": adv_loss }\n\t        for n, metric, _ in self.logged_metrics:\n", "            for avg in [\"macro\", \"micro\"]:\n\t                log_dict[f\"{n}_{stage_suffix}_{avg}_c\"] = self.metrics[f\"{n}_{avg}\"].to(self.device)(all_clean_preds, all_labels.to(ch.int32))\n\t                log_dict[f\"{n}_{stage_suffix}_{avg}_a\"] = self.metrics[f\"{n}_{avg}\"].to(self.device)(all_adv_preds, all_labels.to(ch.int32))\n\t        self.log_dict(\n\t            log_dict, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True, rank_zero_only=True\n\t        )\n\tif __name__ == \"__main__\":\n\t    with open(os.path.join(get_dirname(__file__), \"adv_config.json\")) as json_file:\n\t        config = json.load(json_file)\n\t    args = argparse.Namespace(**config)\n", "    pl.seed_everything(args.overall_seed, workers=True)\n\t    dataset = data_map[args.dataset](batch_size=args.batch_size, seed=args.data_seed)\n\t    model = model_map[args.model].init_for_dataset(args.dataset, args.model_path)\n\t    classifier = AdvClassifier(model, datamodule=dataset, attack_name=\"a2t\", candidate_extractor_name=args.candidate_extractor_name,\n\t        rho_max=args.rho_max, rho_batch=args.rho_batch, attack_ratio=args.attack_ratio, num_candidates=args.num_c,\n\t        stop_word_filter=args.stop_word_filter, random_word_importance=args.random_word_importance, random_synonym=args.random_synonym,\n\t        optimizer_name=args.optimizer_name, learning_rate=args.learning_rate, weight_decay=args.weight_decay, attack_recall=args.attack_recall)\n\t    classifier.print = args.print\n\t    if dataset.multilabel and args.attack_recall:\n\t        args.monitor_val = \"r_v_macro_a\"\n", "        args.monitor_mode = \"max\"\n\t    else:\n\t        args.monitor_val = \"loss_v_a\"\n\t        args.monitor_mode = \"min\"\n\t    trainer = get_trainer(args)\n\t    if not args.only_eval:\n\t        trainer.fit(model=classifier, datamodule=dataset, ckpt_path=args.checkpoint_path)\n\t    trainer.test(model=classifier, datamodule=dataset, verbose=True)\n"]}
{"filename": "scripts/base_trainer.py", "chunked_list": ["import torch as ch\n\timport pytorch_lightning as pl\n\tfrom collections import OrderedDict\n\tfrom torchmetrics import Accuracy, F1Score, Precision, Recall\n\tfrom tokenizer_wrappers import tokenizer_map\n\tfrom embeddings import embedding_map\n\tclass BaseClassifier(pl.LightningModule):\n\t    def __init__(self, model, optimizer_name: str = \"adamw\", learning_rate: float = 0.001, weight_decay: float = 0.0, datamodule=None):\n\t        super().__init__()\n\t        self.model = model\n", "        self.tokenizer = tokenizer_map[self.model.tokenizer_name]()\n\t        cls_embeds = None\n\t        if self.model.word_embeddings_var_name is not None:\n\t            cls_embeds = getattr(self.model, self.model.word_embeddings_var_name[0])\n\t            for attr in self.model.word_embeddings_var_name[1:]:\n\t                cls_embeds = getattr(cls_embeds, attr)\n\t        self.embedding = embedding_map[model.embedding_name](word_embeddings=cls_embeds)\n\t        self.datamodule = datamodule\n\t        self.optimizer_name = optimizer_name\n\t        self.learning_rate = learning_rate\n", "        self.weight_decay = weight_decay\n\t        self.multilabel = datamodule.multilabel\n\t        self.logged_metrics = [(\"acc\", Accuracy, {}), (\"f1\", F1Score, {}), (\"r\", Recall, {}), (f\"p1\", Precision, {\"top_k\": 1})]\n\t        if self.multilabel:\n\t            self.logged_metrics += [(f\"p{int(round(0.16 * self.model.num_classes))}\", Precision, {\"top_k\": int(round(0.16 * self.model.num_classes))}),\n\t                    (f\"p{int(round(0.3 * self.model.num_classes))}\", Precision, {\"top_k\": int(round(0.3 * self.model.num_classes))})]\n\t        self.metrics = {}\n\t        for n, metric, additional_init_args in self.logged_metrics:\n\t            for avg in [\"macro\", \"micro\"]:\n\t                if self.datamodule.multilabel:\n", "                    setattr(self, f\"{n}_{avg}\", metric(task=\"multilabel\", num_labels=self.model.num_classes, average=avg, num_classes=self.model.num_classes, **additional_init_args))\n\t                    self.metrics[f\"{n}_{avg}\"] = metric(task=\"multilabel\", num_labels=self.model.num_classes, average=avg, num_classes=self.model.num_classes, **additional_init_args)\n\t                else:\n\t                    setattr(self, f\"{n}_{avg}\", metric(num_classes=self.model.num_classes, average=avg, **additional_init_args))\n\t                    self.metrics[f\"{n}_{avg}\"] = metric(num_classes=self.model.num_classes, average=avg, **additional_init_args)\n\t    def configure_optimizers(self):\n\t        trained_params = self.model.parameters()\n\t        if self.optimizer_name == \"adam\":\n\t            optimizer = ch.optim.Adam(trained_params, lr=self.learning_rate, weight_decay=self.weight_decay)\n\t        elif self.optimizer_name == \"adamw\":\n", "            optimizer = ch.optim.AdamW(trained_params, lr=self.learning_rate, weight_decay=self.weight_decay)\n\t        elif self.optimizer_name == \"sgd\":\n\t            optimizer = ch.optim.SGD(trained_params, lr=self.learning_rate, weight_decay=self.weight_decay, momentum=0.9, dampening=0.99)\n\t        else:\n\t            raise NotImplementedError(f\"Optimizer {self.optimizer_name} not implemented.\")\n\t        return optimizer\n\t    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n\t        optimizer.zero_grad(set_to_none=True)\n\t    def on_save_checkpoint(self, checkpoint):\n\t        checkpoint[\"state_dict\"] = OrderedDict([(k.removeprefix(\"model.\"), v) for k, v in checkpoint[\"state_dict\"].items()])\n", "    def on_load_checkpoint(self, checkpoint):\n\t        checkpoint[\"state_dict\"] = OrderedDict([(f\"model.{k}\", v) if k in self.model.state_dict() else (k, v) for k, v in checkpoint[\"state_dict\"].items()])\n\t    def test_step(self, batch, batch_idx):\n\t        return self.validation_step(batch, batch_idx)\n\t    def test_epoch_end(self, test_step_outputs):\n\t        return self.validation_epoch_end(test_step_outputs, stage_suffix=\"te\")\n\tdef get_trainer(args):\n\t    callbacks = []\n\t    if args.swa_lr is not None:\n\t        callbacks.append(pl.callbacks.StochasticWeightAveraging(swa_lrs=args.swa_lr))\n", "    callbacks.append(\n\t        pl.callbacks.ModelCheckpoint(monitor=args.monitor_val, save_top_k=3, save_last=True, verbose=True, save_weights_only=False, mode=args.monitor_mode, every_n_epochs=1, auto_insert_metric_name=True)\n\t    )\n\t    callbacks.append(\n\t        pl.callbacks.RichProgressBar(\n\t            theme=pl.callbacks.progress.rich_progress.RichProgressBarTheme(description=\"green_yellow\",progress_bar=\"green1\",\n\t            progress_bar_finished=\"green1\", progress_bar_pulse=\"#6206E0\", batch_progress=\"green_yellow\", time=\"grey82\",\n\t            processing_speed=\"grey82\", metrics=\"grey82\")))\n\t    return pl.Trainer(accelerator='gpu', enable_progress_bar=True, accumulate_grad_batches=args.accumulate_grad_batches,\n\t        max_epochs=args.epochs, limit_test_batches=args.num_test_batches, log_every_n_steps=args.log_every_n_steps,\n", "        deterministic=True, gradient_clip_val=args.gradient_clip_val, profiler=args.profiler, default_root_dir=args.exp_folder,\n\t        callbacks=callbacks, fast_dev_run=args.fast_dev_run, precision=args.precision, amp_backend=args.amp_backend)\n"]}
{"filename": "candidate_extractors/clinicallongformer_mlm.py", "chunked_list": ["import torch as ch\n\tfrom collections import OrderedDict\n\tfrom transformers import LongformerForMaskedLM\n\tfrom data import data_length_map\n\tfrom tokenizer_wrappers import tokenizer_map\n\tclass ClinicalLongformerMLM(ch.nn.Module):\n\t    tokenizer_name = \"clinicallongformer\"\n\t    def __init__(self, input_shape, weight_path: str = None):\n\t        super().__init__()\n\t        self.input_shape = input_shape\n", "        self.name = \"yikuan8/Clinical-Longformer\"\n\t        self.model = LongformerForMaskedLM.from_pretrained(pretrained_model_name_or_path=self.name, hidden_dropout_prob=0.0, attention_probs_dropout_prob=0.0,)\n\t        self.model.eval()\n\t        self.tokenizer = tokenizer_map[self.tokenizer_name]()\n\t        if weight_path is not None:\n\t            self._load_weights(weight_path)\n\t    def forward(self, input_tokens, input_ids, token_type_ids, attention_mask, global_attention_mask=None, labels=None):\n\t        self.model.eval()\n\t        with ch.no_grad():\n\t            preds = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n", "            return preds\n\t    def _load_weights(self, path):\n\t        self.model.load_state_dict(OrderedDict([(k.replace(\"model.model.\", \"\"), v) for k, v in ch.load(path)[\"state_dict\"].items()]))\n\t    @staticmethod\n\t    def init_for_dataset(dataset: str, weight_path: str = None):\n\t        return ClinicalLongformerMLM(input_shape=(min(4096, data_length_map.get(dataset, 4096)),), weight_path=weight_path)\n"]}
{"filename": "candidate_extractors/__init__.py", "chunked_list": ["from .distilroberta_mlm import DistilRoBERTaMLM\n\tfrom .pubmedbert_mlm import PubMedBERTMLM\n\tfrom .clinicallongformer_mlm import ClinicalLongformerMLM\n\tcandidate_extractor_map = {\"distilroberta\": DistilRoBERTaMLM, \"pubmedbert\": PubMedBERTMLM, \"clinicallongformer\": ClinicalLongformerMLM}\n"]}
{"filename": "candidate_extractors/distilroberta_mlm.py", "chunked_list": ["import torch as ch\n\tfrom collections import OrderedDict\n\tfrom transformers import AutoModelForMaskedLM\n\tfrom data import data_length_map\n\tfrom tokenizer_wrappers import tokenizer_map\n\tclass DistilRoBERTaMLM(ch.nn.Module):\n\t    tokenizer_name = \"distilroberta\"\n\t    def __init__(self, input_shape, weight_path: str = None):\n\t        super().__init__()\n\t        self.input_shape = input_shape\n", "        self.name = \"distilroberta-base\"\n\t        self.model = AutoModelForMaskedLM.from_pretrained(pretrained_model_name_or_path=self.name, hidden_dropout_prob=0.0, attention_probs_dropout_prob=0.0)\n\t        self.model.eval()\n\t        self.tokenizer = tokenizer_map[self.tokenizer_name]()\n\t        if weight_path is not None:\n\t            self._load_weights(weight_path)\n\t    def forward(self, input_tokens, input_ids, token_type_ids, attention_mask, labels=None):\n\t        self.model.eval()\n\t        with ch.no_grad():\n\t            preds = self.model(input_ids, attention_mask, token_type_ids, labels=labels)\n", "            return preds\n\t    def _load_weights(self, path):\n\t        self.model.load_state_dict(OrderedDict([(k.replace(\"model.model.\", \"\"), v) for k, v in ch.load(path)[\"state_dict\"].items()]))\n\t    @staticmethod\n\t    def init_for_dataset(dataset: str, weight_path: str = None):\n\t        return DistilRoBERTaMLM(input_shape=(min(512, data_length_map.get(dataset, 512)),), weight_path=weight_path)\n"]}
{"filename": "candidate_extractors/pubmedbert_mlm.py", "chunked_list": ["import torch as ch\n\tfrom collections import OrderedDict\n\tfrom transformers import BertForMaskedLM\n\tfrom data import data_length_map\n\tfrom tokenizer_wrappers import tokenizer_map\n\tclass PubMedBERTMLM(ch.nn.Module):\n\t    tokenizer_name = \"pubmedbert\"\n\t    def __init__(self, input_shape, weight_path: str = None):\n\t        super().__init__()\n\t        self.input_shape = input_shape\n", "        self.name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n\t        self.model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path=self.name, hidden_dropout_prob=0.0, attention_probs_dropout_prob=0.0)\n\t        self.model.eval()\n\t        self.tokenizer = tokenizer_map[self.tokenizer_name]()\n\t        if weight_path is not None:\n\t            self._load_weights(weight_path)\n\t    def forward(self, input_tokens, input_ids, token_type_ids, attention_mask, labels=None):\n\t        self.model.eval()\n\t        with ch.no_grad():\n\t            preds = self.model(input_ids, attention_mask, token_type_ids, labels=labels)\n", "            return preds\n\t    def _load_weights(self, path):\n\t        self.model.load_state_dict(OrderedDict([(k.replace(\"model.model.\", \"\"), v) for k, v in ch.load(path)[\"state_dict\"].items()]))\n\t    @staticmethod\n\t    def init_for_dataset(dataset: str, weight_path: str = None):\n\t        return PubMedBERTMLM(input_shape=(min(512, data_length_map.get(dataset, 512)),), weight_path=weight_path)\n"]}
{"filename": "data/base_datamodule.py", "chunked_list": ["import os, math\n\timport pandas as pd\n\timport torch as ch\n\timport pytorch_lightning as pl\n\tfrom torch.utils.data import random_split, DataLoader\n\tfrom constants import SEPARATOR, DELETED_CHARS, REPLACED_CHARS, WRONG_ORDS\n\tclass BaseDataModule(pl.LightningDataModule):\n\t    multilabel = None\n\t    filters = ((\"text\", \"Ð¼\"), (\"text\", \"Ð¸\"), (\"text\", \"Ù…\"), (\"text\", \"Ùˆ\"), (\"text\", \"Ø¬\"), (\"text\", \"VOTE TRUMP\"),\n\t        (\"text\", \"ç‚¹\"), (\"text\", \"å‡»\"), (\"text\", \"æŸ¥\"), (\"text\", \"çœ‹\"), (\"text\", \"Ïƒ\"), (\"text\", \"Ï„\"), (\"text\", \"Îµ\"),\n", "        (\"text\", \"Ï\"), (\"text\", \"Ù†\"), (\"text\", \"ÑÐ¿a\"), (\"text\", \"Ð»\"), (\"text\", \"Ð½\"), (\"text\", \"Ñ\"), (\"text\", \"Ð±\"),\n\t        (\"text\", \"Ð¿\"))\n\t    def __init__(self, batch_size, seed: int = 0, train_ratio: float = 0.6, val_ratio: float = 0.2):\n\t        super().__init__()\n\t        self.batch_size = batch_size\n\t        self.train_ratio = train_ratio\n\t        self.val_ratio = val_ratio\n\t        self.seed = seed\n\t    def setup(self, stage: str = None):\n\t        data = pd.concat([pd.read_csv(os.path.join(self.data_dir, filename), on_bad_lines=\"error\", **self.load_args) for filename in self.filenames], ignore_index=True)\n", "        data = self.filter_data(data, self.filters)\n\t        data = data.reset_index(drop=True)\n\t        full_ds = self.extract_dataset_from_df(data)\n\t        del data\n\t        num_train = math.ceil(len(full_ds) * self.train_ratio)\n\t        num_val = math.ceil(len(full_ds) * self.val_ratio)\n\t        num_test = len(full_ds) - num_train - num_val\n\t        tmp_train_ds, tmp_val_ds, tmp_test_ds = random_split(full_ds, [num_train, num_val, num_test], generator=ch.Generator().manual_seed(self.seed))\n\t        del full_ds\n\t        self.train_ds, self.val_ds, self.test_ds = tmp_train_ds, tmp_val_ds, tmp_test_ds\n", "        self.calculate_pos_weight()\n\t    def calculate_pos_weight(self):\n\t        labels = ch.tensor([s[1] for s in self.train_ds])\n\t        label_freqs = ch.nn.functional.one_hot(labels, num_classes=len(set(self.class2idx.values()))).sum(0)\n\t        label_neg_freqs = len(self.train_ds) - label_freqs\n\t        self.pos_weights = label_neg_freqs / label_freqs\n\t    def train_dataloader(self):\n\t        num_workers = int(int(os.getenv(\"LSB_DJOB_NUMPROC\", default=ch.get_num_threads())) / (ch.distributed.get_world_size() if ch.distributed.is_initialized() else 1))\n\t        return DataLoader(self.train_ds, batch_size=self.batch_size, num_workers=num_workers, shuffle=False, drop_last=True, persistent_workers=True, pin_memory=True)\n\t    def val_dataloader(self):\n", "        num_workers = int(int(os.getenv(\"LSB_DJOB_NUMPROC\", default=ch.get_num_threads())) / (ch.distributed.get_world_size() if ch.distributed.is_initialized() else 1))\n\t        return DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=num_workers, shuffle=False, drop_last=True, generator=ch.Generator().manual_seed(self.seed) if not ch.distributed.is_initialized() else None,\n\t                          persistent_workers=True, pin_memory=True)\n\t    def test_dataloader(self):\n\t        num_workers = int(int(os.getenv(\"LSB_DJOB_NUMPROC\", default=ch.get_num_threads())) / (ch.distributed.get_world_size() if ch.distributed.is_initialized() else 1))\n\t        return DataLoader(self.test_ds, batch_size=self.batch_size, num_workers=num_workers, shuffle=True, drop_last=True, generator=ch.Generator().manual_seed(self.seed) if not ch.distributed.is_initialized() else None,\n\t                          persistent_workers=True, pin_memory=True)\n\t    def predict_dataloader(self):\n\t        return None\n\t    @staticmethod\n", "    def filter_data(df, filters=None):\n\t        if filters is None:\n\t            filters = []\n\t        for field, value in filters:\n\t            if field in df:\n\t                df = df[~df[field].str.contains(value)]\n\t        return df\n\t    @staticmethod\n\t    def preprocessing(_str, lowercase=True):\n\t        if lowercase:\n", "            # Lower-case all text\n\t            _str = _str.lower()\n\t        # Remove some chars\n\t        for char in DELETED_CHARS:\n\t            _str = _str.replace(char, \"\")\n\t        # Remove weird chars\n\t        _str = \"\".join([c for c in _str if ord(c) not in WRONG_ORDS])\n\t        # Replace some chars\n\t        for char, repl_char in REPLACED_CHARS:\n\t            _str = _str.replace(char, repl_char)\n", "        # Final removal of double whitespaces\n\t        while \"  \" in _str:\n\t            _str = _str.replace(\"  \", \" \")\n\t        # Remove leading and trailing whitespaces\n\t        _str = _str.strip()\n\t        # Final removal of separator\n\t        _str = _str.replace(SEPARATOR, \"\")\n\t        return _str\n\t    @property\n\t    def num_train_samples(self):\n", "        return len(self.train_ds)\n\t    @property\n\t    def num_val_samples(self):\n\t        return len(self.val_ds)\n\t    @property\n\t    def num_test_samples(self):\n\t        return len(self.test_ds)\n\t    @property\n\t    def num_total_samples(self):\n\t        return len(self.train_ds) + len(self.val_ds) + len(self.test_ds)\n"]}
{"filename": "data/datamodules.py", "chunked_list": ["import os, math, json\n\timport torch as ch\n\tfrom torch.utils.data import random_split\n\tfrom random import sample\n\timport pandas as pd\n\tfrom constants import CONCATENATOR, SEPARATOR\n\tfrom utils import get_dirname\n\tfrom data.base_datamodule import BaseDataModule\n\tclass DrugReviewsDataModule(BaseDataModule):\n\t    name = \"drugreviews\"\n", "    undersample = True\n\t    data_dir = os.path.join(get_dirname(__file__), \"raw\", \"drugreviews\")  # Download drugsComTest_raw.tsv and drugsComTrain_raw.tsv into this folder\n\t    filenames = (\"drugsComTrain_raw.tsv\", \"drugsComTest_raw.tsv\")\n\t    load_args = {\"sep\": \"\\t\", \"header\": 0}\n\t    mean_word_seq_length = 89\n\t    stdev_word_seq_length = 46\n\t    mean_non_word_seq_length = 19\n\t    stdev_non_word_seq_length = 8\n\t    class2idx = {1.0: 0, 2.0: 0, 3.0: 1, 4.0: 1, 5.0: 2, 6.0: 2, 7.0: 3, 8.0: 3, 9.0: 4, 10.0: 4}\n\t    def extract_dataset_from_df(self, df):\n", "        df[\"review\"] = df[\"review\"].apply(self.preprocessing)\n\t        df[\"rating\"] = df[\"rating\"].apply(lambda _label: self.class2idx[_label])\n\t        if self.undersample:\n\t            class_counts = {c: 0 for c in self.class2idx.values()}\n\t            for _, row in df.iterrows():\n\t                class_counts[row[\"rating\"]] += 1\n\t            min_class_count = min(class_counts.values())\n\t            df = pd.concat([df[df[\"rating\"] == c].reset_index(drop=True).iloc[\n\t                                sorted(set(sample(range(len(df[df[\"rating\"] == c])), min_class_count)))] for c in class_counts]).reset_index(drop=True)\n\t        class_counts = {c: 0 for c in self.class2idx.values()}\n", "        for _, row in df.iterrows():\n\t            class_counts[row[\"rating\"]] += 1\n\t        return [(row[\"review\"], row[\"rating\"]) for _, row in df.iterrows()]\n\tclass HOCDataModule(BaseDataModule):\n\t    name = \"hoc\"\n\t    multilabel = True\n\t    raw_path = os.path.join(get_dirname(__file__), \"raw\", \"hoc\")  # Download dev.json, test.json and train.json into this folder, or format it from raw\n\t    mean_word_seq_length = 224\n\t    stdev_word_seq_length = 62\n\t    mean_non_word_seq_length = 41\n", "    stdev_non_word_seq_length = 17\n\t    class2idx = {\"activating invasion and metastasis\": 0, \"avoiding immune destruction\": 1, \"cellular energetics\": 2,\n\t        \"enabling replicative immortality\": 3, \"evading growth suppressors\": 4, \"genomic instability and mutation\": 5,\n\t        \"inducingangiogenesis\": 6, \"resisting cell death\": 7, \"sustaining proliferative signaling\": 8, \"tumor promoting inflammation\": 9}\n\t    def setup(self, stage: str = None):\n\t        # Load datasets\n\t        data = {\"train\": {}, \"test\": {}, \"dev\": {}}\n\t        for _t in data:\n\t            data[_t] = {\"sentence\": [], \"label\": [], \"id\": []}\n\t            with open(os.path.join(self.raw_path, f\"{_t}.json\")) as infile:\n", "                for line in infile.readlines():\n\t                    line_dict = json.loads(line)\n\t                    data[_t][\"sentence\"].append(line_dict[\"sentence\"])\n\t                    data[_t][\"label\"].append(line_dict[\"label\"])\n\t                    data[_t][\"id\"].append(line_dict[\"id\"])\n\t        train_ds_data = pd.DataFrame(data[\"train\"]).reset_index(drop=True)\n\t        test_ds_data = pd.DataFrame(data[\"test\"]).reset_index(drop=True)\n\t        val_ds_data = pd.DataFrame(data[\"dev\"]).reset_index(drop=True)\n\t        # Experiment with random split here\n\t        full_ds = pd.concat([train_ds_data, test_ds_data, val_ds_data]).reset_index(drop=True)\n", "        num_train = math.ceil(len(full_ds) * self.train_ratio)\n\t        num_val = math.ceil(len(full_ds) * self.val_ratio)\n\t        num_test = len(full_ds) - num_train - num_val\n\t        indices = set(range(len(full_ds)))\n\t        train_indices = set(sample(indices, num_train))\n\t        indices = indices.difference(train_indices)\n\t        val_indices = set(sample(indices, num_val))\n\t        test_indices = indices.difference(val_indices)\n\t        if len(test_indices) != num_test:\n\t            raise ValueError(\"Error in test split\")\n", "        self.train_ds = [(row[\"sentence\"], ch.tensor(row[\"label\"])) for _, row in full_ds.iloc[sorted(train_indices)].iterrows()]\n\t        self.test_ds = [(row[\"sentence\"], ch.tensor(row[\"label\"])) for _, row in full_ds.iloc[sorted(test_indices)].iterrows()]\n\t        self.val_ds = [(row[\"sentence\"], ch.tensor(row[\"label\"])) for _, row in full_ds.iloc[sorted(val_indices)].iterrows()]\n\t        self.calculate_pos_weight()\n\t    def calculate_pos_weight(self):\n\t        labels = ch.stack([s[1] for s in self.train_ds], dim=0)\n\t        label_freqs = labels.sum(0)\n\t        label_neg_freqs = len(self.train_ds) - label_freqs\n\t        self.pos_weights = label_neg_freqs / label_freqs\n\tclass MIMICDataModule(BaseDataModule):\n", "    name = \"mimic\"\n\t    multilabel = True\n\t    data_dir = os.path.join(get_dirname(__file__), \"raw\", \"mimic\")  # Run the Python script provided in that folder\n\t    filenames = (\"mimic.csv\",)\n\t    load_args = {\"header\": 0, \"dtype\": str, \"sep\": SEPARATOR}\n\t    load_big_train = True\n\t    class2idx = {'4019': 0, '3893': 1, '4280': 2, '42731': 3, '41401': 4, '9604': 5, '966': 6, '5849': 7, '25000': 8, '9671': 9,\n\t        '2724': 10, '51881': 11, '9904': 12, '3961': 13, '5990': 14, '9672': 15, '53081': 16, '2720': 17, '311': 18,\n\t        '2859': 19, '8856': 20, '486': 21, '2449': 22, '3615': 23, '3891': 24, '9915': 25, '2851': 26, '496': 27,\n\t        '2762': 28, '5070': 29, '99592': 30, 'V5861': 31, '0389': 32, '8872': 33, '5859': 34, '40390': 35, '3722': 36,\n", "        '3995': 37, '412': 38, '3051': 39, '41071': 40, '2875': 41, '2761': 42, '4240': 43, 'V4581': 44, '5119': 45,\n\t        '3723': 46, '9390': 47, '3324': 48, '4241': 49}\n\t    def __init__(self, *args, **kwargs):\n\t        kwargs[\"train_ratio\"] = 0.75\n\t        kwargs[\"val_ratio\"] = 0.125\n\t        super().__init__(*args, **kwargs)\n\t    def extract_dataset_from_df(self, df):\n\t        def filter_no_labels(row):\n\t            labels = row[\"icd9_code\"].split(CONCATENATOR)\n\t            labels = [_l for _l in labels if _l in self.class2idx]\n", "            if len(labels) > 0:\n\t                return True\n\t            else:\n\t                return False\n\t        df[\"text\"] = df[\"text\"].apply(self.preprocessing)\n\t        df = df[~df[\"icd9_code\"].isna()]\n\t        df = df[df[\"icd9_code\"] != \"\"]\n\t        # Filter out samples with no labels\n\t        df = df[df.apply(filter_no_labels, axis=1)]\n\t        # Load train, test and val indices from files\n", "        if not self.load_big_train:\n\t            with open(os.path.join(self.data_dir, \"train_50_hadm_ids.csv\")) as trf, open(os.path.join(\n\t                    self.data_dir, \"test_50_hadm_ids.csv\")) as tef, open(os.path.join(self.data_dir, \"dev_50_hadm_ids.csv\")) as vf:\n\t                indices = {l.replace(\"\\n\", \"\") for _file in [trf, tef, vf] for l in _file.readlines()}\n\t            df = df[df[\"hadm_id\"].isin(indices)].reset_index(drop=True)\n\t        if not self.load_big_train:\n\t            return tuple((row[\"text\"], ch.nn.functional.one_hot(ch.tensor([self.class2idx[c] for c in row[\n\t                \"icd9_code\"].split(CONCATENATOR) if c in self.class2idx]), num_classes=len(self.class2idx)).sum(0)) for _, row in df.iterrows())\n\t        else:\n\t            return tuple((row[\"text\"], ch.nn.functional.one_hot(ch.tensor([self.class2idx[c] for c in row[\n", "                \"icd9_code\"].split(CONCATENATOR) if c in self.class2idx]), num_classes=len(self.class2idx)).sum(0), row[\"hadm_id\"]) for _, row in df.iterrows())\n\t    def setup(self, stage: str = None):\n\t        data = pd.concat([\n\t            pd.read_csv(os.path.join(self.data_dir, filename), on_bad_lines=\"error\", **self.load_args) for filename in self.filenames], ignore_index=True)\n\t        data = self.filter_data(data, self.filters)\n\t        data = data.reset_index(drop=True)\n\t        full_ds = self.extract_dataset_from_df(data)\n\t        del data\n\t        if self.load_big_train:\n\t            with open(os.path.join(self.data_dir, \"test_50_hadm_ids.csv\")) as tef:\n", "                test_indices = {l.replace(\"\\n\", \"\") for l in tef.readlines() if l.replace(\"\\n\", \"\") != \"\"}\n\t            tmp_test_ds = tuple((s[0], s[1]) for s in full_ds if s[2] in test_indices)\n\t            with open(os.path.join(self.data_dir, \"dev_50_hadm_ids.csv\")) as tef:\n\t                dev_indices = {l.replace(\"\\n\", \"\") for l in tef.readlines() if l.replace(\"\\n\", \"\") != \"\"}\n\t            tmp_val_ds = tuple((s[0], s[1]) for s in full_ds if s[2] in dev_indices)\n\t            tmp_train_ds = tuple((s[0], s[1]) for s in full_ds if s[2] not in dev_indices.union(test_indices))\n\t        else:\n\t            num_train = math.ceil(len(full_ds) * self.train_ratio)\n\t            num_val = math.ceil(len(full_ds) * self.val_ratio)\n\t            num_test = len(full_ds) - num_train - num_val\n", "            tmp_train_ds, tmp_val_ds, tmp_test_ds = random_split(full_ds, [num_train, num_val, num_test], generator=ch.Generator().manual_seed(self.seed))\n\t            del full_ds\n\t        self.train_ds, self.val_ds, self.test_ds = tmp_train_ds, tmp_val_ds, tmp_test_ds\n\t        self.calculate_pos_weight()\n\t    def calculate_pos_weight(self):\n\t        labels = ch.stack([s[1] for s in self.train_ds], dim=0)\n\t        label_freqs = labels.sum(0)\n\t        label_neg_freqs = len(self.train_ds) - label_freqs\n\t        self.pos_weights = label_neg_freqs / label_freqs\n"]}
{"filename": "data/__init__.py", "chunked_list": ["from .datamodules import HOCDataModule\n\tfrom .datamodules import DrugReviewsDataModule\n\tfrom .datamodules import MIMICDataModule\n\tdata_map = {HOCDataModule.name: HOCDataModule, DrugReviewsDataModule.name: DrugReviewsDataModule, MIMICDataModule.name: MIMICDataModule}\n\tdata_length_map = {HOCDataModule.name: 256, DrugReviewsDataModule.name: 128, MIMICDataModule.name: 4096}\n\tdata_num_classes = {HOCDataModule.name: 10, DrugReviewsDataModule.name: 5, MIMICDataModule.name: 50}\n"]}
{"filename": "data/raw/mimic/create_raw_data.py", "chunked_list": ["import os\n\timport pandas as pd\n\tfrom utils import get_dirname\n\tfrom constants import CONCATENATOR, SEPARATOR\n\tdef construct_raw_data():\n\t    data_dir = None  # Fill in path to raw data containing NOTEEVENTS, DIAGNOSES_ICD and PROCEDURES_ICD\n\t    out_dir = get_dirname(__file__)\n\t    load_args = {\"header\": 0, \"compression\": \"gzip\", \"dtype\": str, \"on_bad_lines\": \"error\"}\n\t    # Load event data\n\t    data = pd.read_csv(os.path.join(data_dir, \"NOTEEVENTS.csv.gz\"), **load_args)\n", "    data.columns = [col.lower() for col in data.columns]\n\t    # Filter out discharge summaries\n\t    data = data[data[\"category\"] == \"Discharge summary\"]\n\t    # Load ICD9 diagnosis codes\n\t    icd9_d_data = pd.read_csv(os.path.join(data_dir, \"DIAGNOSES_ICD.csv.gz\"), **load_args)\n\t    icd9_d_data.columns = [col.lower() for col in icd9_d_data.columns]\n\t    # Load ICD9 procedure codes\n\t    icd9_p_data = pd.read_csv(os.path.join(data_dir, \"PROCEDURES_ICD.csv.gz\"), **load_args)\n\t    icd9_p_data.columns = [col.lower() for col in icd9_p_data.columns]\n\t    icd_code_dict = {}\n", "    icd9_codes = pd.concat([icd9_d_data, icd9_p_data], ignore_index=True)\n\t    for (subject_id, hadm_id), icd_df in icd9_codes.groupby([\"subject_id\", \"hadm_id\"]):\n\t        codes = CONCATENATOR.join(sorted(icd_df[\"icd9_code\"].astype(str).unique()))\n\t        icd_code_dict[CONCATENATOR.join([subject_id, hadm_id])] = codes\n\t    data[\"icd9_code\"] = data.apply(lambda row: icd_code_dict.get(CONCATENATOR.join([row[\"subject_id\"], row[\"hadm_id\"]]), \"\"), axis=1)\n\t    # Filter out empty ICD codes\n\t    data = data[data[\"icd9_code\"] != \"\"]\n\t    data = data[data[\"icd9_code\"] != \"nan\"]\n\t    data[\"text\"] = data[\"text\"].apply(lambda text: text.replace(\"\\n\", \"\\t\"))\n\t    data[\"text\"] = data[\"text\"].apply(lambda text: text.replace(SEPARATOR, \"<|>\"))\n", "    data.to_csv(os.path.join(out_dir, f\"mimic.csv\"), sep=SEPARATOR, index_label=\"idx\", na_rep=\"nan\")\n\tif __name__ == \"__main__\":\n\t    construct_raw_data()"]}
{"filename": "models/clinicallongformer.py", "chunked_list": ["import torch as ch\n\tfrom transformers import LongformerModel\n\tfrom . import BaseModel\n\tfrom data import data_length_map, data_num_classes\n\t# Training should be inspired by: https://aclanthology.org/2021.emnlp-main.481.pdf\n\tclass ClinicalLongformer(BaseModel):\n\t    attention_layer_idx = None\n\t    attention_head_idx = None\n\t    attention_out_idx = None\n\t    tokenizer_name = \"clinicallongformer\"\n", "    embedding_name = \"clinicallongformer\"\n\t    def __init__(self, input_shape, num_classes, lin_dims, attention_layer_idx, attention_head_idx, attention_out_idx, weight_path=None):\n\t        super().__init__(input_shape=input_shape, num_classes=num_classes)\n\t        self.lin_dims = lin_dims\n\t        self.attention_layer_idx = attention_layer_idx\n\t        self.attention_head_idx = attention_head_idx\n\t        self.attention_out_idx = attention_out_idx\n\t        self.cls_model = LongformerModel.from_pretrained(\"yikuan8/Clinical-Longformer\", output_attentions=True)\n\t        self.word_embeddings_var_name = [\"cls_model\", \"embeddings\", \"word_embeddings\"]\n\t        tmp_lin_dims = [768] + self.lin_dims + [self.num_classes]\n", "        self.fcs = ch.nn.ModuleList([ch.nn.Linear(tmp_lin_dims[i - 1], tmp_lin_dims[i]) for i in range(1, len(tmp_lin_dims))])\n\t        if weight_path is not None:\n\t            self._load_weights(weight_path)\n\t    def _forward_preprocessing(self, word_embeds, token_type_ids):\n\t        embeds = self.cls_model.embeddings(input_ids=None, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=word_embeds)\n\t        return embeds\n\t    def _forward_features(self, input_embeds, attention_mask, token_type_ids, global_attention_mask=None):\n\t        tmp_det = ch.backends.cudnn.deterministic\n\t        ch.use_deterministic_algorithms(False, warn_only=True)\n\t        outs = self.cls_model(inputs_embeds=input_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids, global_attention_mask=global_attention_mask)\n", "        ch.use_deterministic_algorithms(tmp_det, warn_only=True)\n\t        return outs\n\t    def _forward_classifier(self, x):\n\t        for lin_layer in self.fcs[:-1]:\n\t            x = lin_layer(x)\n\t        return self.fcs[-1](x)\n\t    def forward(self, input_embeds, attention_mask, token_type_ids, global_attention_mask=None, return_att_weights=False):\n\t        full_embeds = self._forward_preprocessing(input_embeds, token_type_ids)\n\t        outs = self._forward_features(input_embeds=full_embeds,attention_mask=attention_mask, token_type_ids=token_type_ids, global_attention_mask=global_attention_mask)\n\t        if return_att_weights:\n", "            return self._forward_classifier(outs.pooler_output), outs.global_attentions[self.attention_layer_idx][:, self.attention_head_idx, :, 0]\n\t        return self._forward_classifier(outs.pooler_output)\n\t    @staticmethod\n\t    def init_for_dataset(dataset: str, weight_path: str = None):\n\t        return ClinicalLongformer(input_shape=(min(4096, data_length_map.get(dataset, 4096)),), num_classes=data_num_classes[dataset], lin_dims=[], attention_layer_idx=-1, attention_head_idx=-1, attention_out_idx=0, weight_path=weight_path)\n"]}
{"filename": "models/__init__.py", "chunked_list": ["import torch as ch\n\tfrom collections import OrderedDict\n\tclass BaseModel(ch.nn.Module):\n\t    input_shape = None\n\t    num_classes = None\n\t    def __init__(self, input_shape, num_classes):\n\t        super().__init__()\n\t        self.input_shape = input_shape\n\t        self.num_classes = num_classes\n\t    def _load_weights(self, path):\n", "        loaded_state_dict = ch.load(path, map_location=\"cpu\")[\"state_dict\"]\n\t        self.load_state_dict(OrderedDict([(k, v) for k, v in loaded_state_dict.items() if k in self.state_dict()]))\n\tfrom .biolinkbert import BioLinkBERT\n\tfrom .roberta import RoBERTa\n\tfrom .clinicallongformer import ClinicalLongformer\n\tmodel_map = {\"roberta\": RoBERTa, \"biolinkbert\": BioLinkBERT, \"clinicallongformer\": ClinicalLongformer}\n"]}
{"filename": "models/biolinkbert.py", "chunked_list": ["# https://github.com/michiyasunaga/LinkBERT\n\timport torch as ch\n\tfrom transformers import AutoModel\n\tfrom . import BaseModel\n\tfrom data import data_length_map, data_num_classes\n\tclass BioLinkBERT(BaseModel):\n\t    attention_layer_idx = -1\n\t    attention_head_idx = -1\n\t    attention_out_idx = 0\n\t    tokenizer_name = \"biolinkbert\"\n", "    embedding_name = \"biolinkbert\"\n\t    def __init__(self, input_shape, num_classes, lin_dims, attention_layer_idx, attention_head_idx, attention_out_idx,\n\t                 weight_path=None):\n\t        super().__init__(input_shape=input_shape, num_classes=num_classes)\n\t        self.lin_dims = lin_dims\n\t        self.attention_layer_idx = attention_layer_idx\n\t        self.attention_head_idx = attention_head_idx\n\t        self.attention_out_idx = attention_out_idx\n\t        self.cls_model = AutoModel.from_pretrained(\"michiyasunaga/BioLinkBERT-base\", output_attentions=True)\n\t        self.word_embeddings_var_name = [\"cls_model\", \"embeddings\", \"word_embeddings\"]\n", "        tmp_lin_dims = [768] + self.lin_dims + [self.num_classes]\n\t        self.fcs = ch.nn.ModuleList([ch.nn.Linear(tmp_lin_dims[i - 1], tmp_lin_dims[i]) for i in range(1, len(tmp_lin_dims))])\n\t        if weight_path is not None:\n\t            self._load_weights(weight_path)\n\t    def _forward_preprocessing(self, word_embeds, token_type_ids):\n\t        embeds = self.cls_model.embeddings(input_ids=None, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=word_embeds, past_key_values_length=0)\n\t        return embeds\n\t    def _forward_features(self, input_embeds, attention_mask, token_type_ids):\n\t        outs = self.cls_model(inputs_embeds=input_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\t        return outs\n", "    def _forward_classifier(self, x):\n\t        for lin_layer in self.fcs[:-1]:\n\t            x = lin_layer(x)\n\t        return self.fcs[-1](x)\n\t    def forward(self, input_embeds, attention_mask, token_type_ids, return_att_weights=False):\n\t        full_embeds = self._forward_preprocessing(input_embeds, token_type_ids)\n\t        outs = self._forward_features(full_embeds, attention_mask, token_type_ids)\n\t        if return_att_weights:\n\t            return self._forward_classifier(outs.pooler_output), outs.attentions[self.attention_layer_idx][:, self.attention_head_idx, self.attention_out_idx]\n\t        return self._forward_classifier(outs.pooler_output)\n", "    @staticmethod\n\t    def init_for_dataset(dataset: str, weight_path: str = None):\n\t        return BioLinkBERT(input_shape=(min(512, data_length_map.get(dataset, 512)),), num_classes=data_num_classes[dataset], lin_dims=[], attention_layer_idx=-1, attention_head_idx=-1, attention_out_idx=0, weight_path=weight_path)\n"]}
{"filename": "models/roberta.py", "chunked_list": ["import torch as ch\n\tfrom transformers import RobertaModel\n\tfrom . import BaseModel\n\tfrom data import data_length_map, data_num_classes\n\tclass RoBERTa(BaseModel):\n\t    attention_layer_idx = -1\n\t    attention_head_idx = -1\n\t    attention_out_idx = 0\n\t    tokenizer_name = \"roberta\"\n\t    embedding_name = \"roberta\"\n", "    def __init__(self, input_shape, num_classes, lin_dims, attention_layer_idx, attention_head_idx, attention_out_idx,\n\t                 weight_path=None):\n\t        super().__init__(input_shape=input_shape, num_classes=num_classes)\n\t        self.lin_dims = lin_dims\n\t        self.attention_layer_idx = attention_layer_idx\n\t        self.attention_head_idx = attention_head_idx\n\t        self.attention_out_idx = attention_out_idx\n\t        self.cls_model = RobertaModel.from_pretrained(\"roberta-base\", output_attentions=True)\n\t        self.word_embeddings_var_name = [\"cls_model\", \"embeddings\", \"word_embeddings\"]\n\t        tmp_lin_dims = [768] + self.lin_dims + [self.num_classes]\n", "        self.fcs = ch.nn.ModuleList([ch.nn.Linear(tmp_lin_dims[i - 1], tmp_lin_dims[i]) for i in range(\n\t            1, len(tmp_lin_dims))])\n\t        if weight_path is not None:\n\t            self._load_weights(weight_path)\n\t    def _forward_preprocessing(self, word_embeds, token_type_ids):\n\t        embeds = self.cls_model.embeddings(input_ids=None, position_ids=None, token_type_ids=token_type_ids,\n\t                                           inputs_embeds=word_embeds, past_key_values_length=0)\n\t        return embeds\n\t    def _forward_features(self, input_embeds, attention_mask, token_type_ids):\n\t        outs = self.cls_model(inputs_embeds=input_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids)\n", "        return outs\n\t    def _forward_classifier(self, x):\n\t        for lin_layer in self.fcs[:-1]:\n\t            x = lin_layer(x)\n\t        return self.fcs[-1](x)\n\t    def forward(self, input_embeds, attention_mask, token_type_ids, return_att_weights=False):\n\t        full_embeds = self._forward_preprocessing(input_embeds, token_type_ids)\n\t        outs = self._forward_features(full_embeds, attention_mask, token_type_ids)\n\t        if return_att_weights:\n\t            return self._forward_classifier(outs.pooler_output), outs.attentions[self.attention_layer_idx][\n", "                :, self.attention_head_idx, self.attention_out_idx]\n\t        return self._forward_classifier(outs.pooler_output)\n\t    @staticmethod\n\t    def init_for_dataset(dataset: str, weight_path: str = None):\n\t        return RoBERTa(input_shape=(min(512, data_length_map.get(dataset, 512)),), num_classes=data_num_classes[dataset],\n\t                       lin_dims=[], attention_layer_idx=-1, attention_head_idx=-1, attention_out_idx=0, weight_path=weight_path)"]}
{"filename": "sentence_encoders/med_bce.py", "chunked_list": ["# Download model from https://github.com/uf-hobi-informatics-lab/2019_N2C2_Track1_ClinicalSTS/tree/c00c5b822105a9a4c36ae7fcc268f8f25b01741c\n\timport os\n\timport transformers\n\tfrom utils import get_project_rootdir\n\tfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\timport torch as ch\n\tclass MedBCE(ch.nn.Module):\n\t    def __init__(self, **kwargs):\n\t        super(MedBCE, self).__init__()\n\t        self.max_length = 4096  # 4096 is the longest input to any model\n", "        self.chunk_size = 128  # MedSTS has a mean word count of ~48, therefore 128 seems a good chunk size\n\t        self.name_tag = os.path.join(get_project_rootdir(), \"sentence_encoders\", \"models\",\n\t                                     \"2019n2c2_tack1_roberta_stsc_6b_16b_3c_8c\")\n\t        self.tokenizer = RobertaTokenizer.from_pretrained(self.name_tag)\n\t        self.model = RobertaForSequenceClassification.from_pretrained(self.name_tag)\n\t    def semantic_sim(self, sentence1, sentence2, reduction=\"min\", **kwargs):\n\t        transformers.logging.set_verbosity_error()\n\t        sent1_tok = self.tokenizer([sentence1], max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\t        sent2_tok = self.tokenizer([sentence2], max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\t        sentences1 = [self.tokenizer.decode(sent1_tok[\"input_ids\"][0][self.chunk_size*_i: self.chunk_size*(_i+1)], skip_special_tokens=True, clean_up_tokenization_spaces=True) for _i in range(int((self.max_length + self.chunk_size - 1)/self.chunk_size))]\n", "        sentences1 = [s for s in sentences1 if s != \"\"]\n\t        sentences2 = [self.tokenizer.decode(sent2_tok[\"input_ids\"][0][self.chunk_size*_i: self.chunk_size*(_i+1)], skip_special_tokens=True, clean_up_tokenization_spaces=True) for _i in range(int((self.max_length + self.chunk_size - 1)/self.chunk_size))]\n\t        sentences2 = [s for s in sentences2 if s != \"\"]\n\t        sims = []\n\t        for i in range(min(len(sentences1), len(sentences2))):\n\t            tokenized = self.tokenizer([(sentences1[i], sentences2[i])], max_length=self.chunk_size, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\t            with ch.no_grad():\n\t                sims.append(self.model(**(tokenized.to(list(self.model.parameters())[0].device))).logits.clamp(min=0.0, max=5.0).div(5.0))\n\t        transformers.logging.set_verbosity_warning()\n\t        if reduction == \"mean\" or reduction is None:\n", "            return ch.mean(ch.cat(sims))\n\t        elif reduction == \"min\":\n\t            return ch.min(ch.cat(sims))\n\t        else:\n\t            raise ValueError(f\"Reduction '{reduction}' not supported\")\n"]}
{"filename": "sentence_encoders/__init__.py", "chunked_list": ["from .med_bce import MedBCE\n\tsentence_encoder_map = {\"med_bce\": MedBCE}\n"]}
