{"filename": "operators/appsflyer.py", "chunked_list": ["from airflow.models import BaseOperator\n\tfrom api.appsflyer.report_api import AppsflyerReportApi\n\tfrom utils.common import timing\n\tclass AppsflyerDownloadOperator(BaseOperator):\n\t    template_fields = (\"run_date\", \"download_dir\")\n\t    def __init__(self,\n\t                run_date: str,\n\t                download_dir: str,\n\t                app_id: str = None,\n\t                report_type: str = None,\n", "                **kwargs):\n\t        super(AppsflyerDownloadOperator, self).__init__(**kwargs)\n\t        self.run_date = run_date\n\t        self.download_dir = download_dir\n\t        self.app_id = app_id\n\t        self.report_type = report_type\n\t    @timing\n\t    def execute(self, context):\n\t        api = AppsflyerReportApi()\n\t        api.download_report_type(self.run_date, \n", "            app_id=self.app_id,\n\t            report_type=self.report_type,\n\t            download_dir=self.download_dir)\n"]}
{"filename": "operators/pg_transform.py", "chunked_list": ["import pandas.io.sql as psql\n\timport pendulum\n\tfrom airflow.models.baseoperator import BaseOperator\n\tfrom hooks.postgres_custom import PostgresCustomHook\n\tlocal_tz = pendulum.timezone('Asia/Ho_Chi_Minh')\n\tclass PostgresTransformOperator(BaseOperator):\n\t    template_fields = ('run_date', 'delete_sql',)\n\t    ui_color = '#33CBFE'\n\t    def __init__(\n\t            self,\n", "            replace: bool,\n\t            transferred_table: str,\n\t            schema: str = '',\n\t            conflict_sql: str = '',\n\t            sql_builder: callable = None,\n\t            postgres_conn_id='postgres_default',\n\t            database=None,\n\t            run_date: any = None,\n\t            commit_every: int = 10000,\n\t            delete_sql=None,\n", "            *args, **kwargs):\n\t        super(PostgresTransformOperator, self).__init__(*args, **kwargs)\n\t        self.sql_builder = sql_builder\n\t        self.postgres_conn_id = postgres_conn_id\n\t        self.database = database\n\t        self.transferred_table = transferred_table\n\t        self.schema = schema\n\t        self.run_date = run_date\n\t        self.replace = replace\n\t        self.commit_every = commit_every\n", "        self.conflict_sql = conflict_sql\n\t        self.delete_sql = delete_sql\n\t        self.hook = PostgresCustomHook(postgres_conn_id=self.postgres_conn_id, schema=self.database)\n\t    def execute(self, context):\n\t        target_fields, records = self.get_records()\n\t        self.log.info(\"Executing on %s\", self.run_date)\n\t        if self.delete_sql is not None:\n\t            self.do_delete()\n\t        self.insert_rows(target_fields=target_fields, records=records)\n\t        for output in self.hook.conn.notices:\n", "            self.log.info(output)\n\t    def get_records(self):\n\t        if self.run_date is None:\n\t            raise ValueError(\"Missing run_date argument!!\")\n\t        sql = self.sql_builder(self.run_date)\n\t        print(f\"Source SQL: {sql}\")\n\t        self.log.info('Executing: %s', sql)\n\t        df_records = psql.read_sql(sql, con=self.hook.get_conn())\n\t        target_fields = df_records.columns.tolist()\n\t        self.log.info(\"Target Fields: {}\".format(target_fields))\n", "        records = list(df_records.itertuples(index=False, name=None))\n\t        self.log.info(\"Get first {}\".format(records[0]))\n\t        self.log.info(\"Start loading %s records to %s ...\" % (len(records), self.transferred_table))\n\t        if len(records) < 1:\n\t            raise ValueError(\n\t                \"Data Quality validation FAILED on {} in table {}.{}.\".format(self.run_date,\n\t                                                                            self.schema,\n\t                                                                            self.transferred_table))\n\t        return target_fields, records\n\t    def insert_rows(self, target_fields, records):\n", "        self.hook.insert_rows(table=self.transferred_table,\n\t                                rows=records,\n\t                                target_fields=target_fields,\n\t                                replace=self.replace,\n\t                                conflict_sql=self.conflict_sql,\n\t                                commit_every=self.commit_every)\n\t    def do_delete(self):\n\t        self.hook.run(self.delete_sql, autocommit=True)\n\t        self.log.info(\"Executing delete statement:\\n %s\", self.delete_sql)\n"]}
{"filename": "operators/ms_teams_webhook.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t#\n\t# Licensed to the Apache Software Foundation (ASF) under one\n\t# or more contributor license agreements.  See the NOTICE file\n\t# distributed with this work for additional information\n\t# regarding copyright ownership.  The ASF licenses this file\n\t# to you under the Apache License, Version 2.0 (the\n\t# \"License\"); you may not use this file except in compliance\n\t# with the License.  You may obtain a copy of the License at\n\t#\n", "#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing,\n\t# software distributed under the License is distributed on an\n\t# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\t# KIND, either express or implied.  See the License for the\n\t# specific language governing permissions and limitations\n\t# under the License.\n\t#\n\tfrom airflow.operators.http_operator import SimpleHttpOperator\n", "from airflow.models import Variable\n\tfrom hooks.ms_teams_webhook import MSTeamsWebhookHook\n\timport logging\n\tclass MSTeamsWebhookOperator(SimpleHttpOperator):\n\t    \"\"\"\n\t    This operator allows you to post messages to MS Teams using the Incoming Webhooks connector.\n\t    Takes both MS Teams webhook token directly and connection that has MS Teams webhook token.\n\t    If both supplied, the webhook token will be appended to the host in the connection.\n\t    :param http_conn_id: connection that has MS Teams webhook URL\n\t    :type http_conn_id: str\n", "    :param webhook_token: MS Teams webhook token\n\t    :type webhook_token: str\n\t    :param message: The message you want to send on MS Teams\n\t    :type message: str\n\t    :param title: The subtitle of the message to send\n\t    :type title: str\n\t    :param button_text: The text of the action button\n\t    :type button_text: str\n\t    :param button_url: The URL for the action button click\n\t    :type button_url : str\n", "    :param theme_color: Hex code of the card theme, without the #\n\t    :type message: str\n\t    :param proxy: Proxy to use when making the webhook request\n\t    :type proxy: str\n\t    \"\"\"\n\t    template_fields = ('message',)\n\t    def __init__(self,\n\t                http_conn_id=None,\n\t                webhook_token=None,\n\t                owner=\"\",\n", "                message=\"\",\n\t                status=\"\",\n\t                button_text=\"\",\n\t                theme_color=\"00FF00\",\n\t                proxy=None,\n\t                *args,\n\t                **kwargs):\n\t        super(MSTeamsWebhookOperator, self).__init__(endpoint=webhook_token, *args, **kwargs)\n\t        self.http_conn_id = http_conn_id\n\t        self.webhook_token = webhook_token\n", "        self.message = message\n\t        self.status = status\n\t        self.owner = owner\n\t        self.button_text = button_text\n\t        self.theme_color = theme_color\n\t        self.proxy = proxy\n\t        self.hook = None\n\t    def execute(self, context):\n\t        dag_id = context['dag_run'].dag_id\n\t        task_id = context['ti'].task_id\n", "        airflow_url = Variable.get(\"airflow_url\")\n\t        logs_url = airflow_url + \"/admin/airflow/log?dag_id={}&task_id={}&execution_date={}\".format(\n\t            dag_id, task_id, context['ts'])\n\t        self.hook = MSTeamsWebhookHook(\n\t            http_conn_id=self.http_conn_id,\n\t            webhook_token=self.webhook_token,\n\t            message=self.message,\n\t            title=dag_id + \"-\" + self.status + f\"-@{self.owner}\",\n\t            button_text=self.button_text,\n\t            button_url=logs_url,\n", "            theme_color=self.theme_color,\n\t            proxy=self.proxy,\n\t            exec_time=context['ts'],\n\t            dag=dag_id,\n\t            task=task_id\n\t        )\n\t        self.hook.execute()\n\t        logging.info(\"Webhook request sent to MS Teams\")\n"]}
{"filename": "operators/postgres.py", "chunked_list": ["import os\n\timport time\n\timport pandas as pd\n\tfrom typing import Callable, Dict, Optional\n\tfrom airflow.models import BaseOperator\n\tfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\tfrom airflow.exceptions import AirflowException\n\tfrom hooks.postgres import PostgresPandasHook\n\tfrom utils.os_helper import make_new_folder\n\tclass PostgresExecOperator(BaseOperator):\n", "    \"\"\"\n\t    Executes sql code in a specific Postgres database\n\t    :param sql: the sql code to be executed. (templated)\n\t    :type sql: Can receive a str representing a sql statement,\n\t        a list of str (sql statements), or reference to a template file.\n\t        Template reference are recognized by str ending in '.sql'\n\t    :param sql_generator: the function that return sql code\n\t    :type sql_generator: callable\n\t    :param sql_generator_kwargs: a dict of parameters that passed into sql_generator function (templated)\n\t    :type sql_generator_kwargs: dict\n", "    :param hook_cls: reference to a name of sqoop_hook class\n\t    :type hook_cls: cls\n\t    :param postgres_conn_id: reference to a specific postgres database\n\t    :type postgres_conn_id: str\n\t    :param autocommit: if True, each command is automatically committed.\n\t        (default value: False)\n\t    :type autocommit: bool\n\t    :param parameters: (optional) the parameters to render the SQL query with.\n\t    :type parameters: mapping or iterable\n\t    :param schema: schema's name which overwrite one defined in connection\n", "    :type schema: str\n\t    \"\"\"\n\t    template_fields = ('sql', 'sql_generator_kwargs',)\n\t    template_ext = ('.sql',)\n\t    ui_color = '#3da3da'\n\t    def __init__(\n\t            self,\n\t            sql = None,\n\t            sql_generator: callable = None,\n\t            sql_generator_kwargs: dict = {},\n", "            hook_cls = PostgresHook, \n\t            postgres_conn_id='postgres_default',\n\t            autocommit = False,\n\t            parameters = None,\n\t            schema = None,\n\t            *args, **kwargs):\n\t        super(PostgresExecOperator, self).__init__(*args, **kwargs)\n\t        self.sql = sql\n\t        self.sql_generator = sql_generator\n\t        self.sql_generator_kwargs = sql_generator_kwargs\n", "        self.hook_cls = hook_cls\n\t        self.postgres_conn_id = postgres_conn_id\n\t        self.autocommit = autocommit\n\t        self.parameters = parameters\n\t        self.schema = schema\n\t    def execute(self, context):\n\t        if self.sql is not None:\n\t            sql = self.sql\n\t        elif self.sql_generator is not None:\n\t            sql = self.sql_generator(**self.sql_generator_kwargs)\n", "        else:\n\t            raise AirflowException(\"Require sql or sql_generator to execute PostgresExtraOperator\")\n\t        self.hook = self.hook_cls(\n\t            postgres_conn_id=self.postgres_conn_id,\n\t            schema=self.schema\n\t        )\n\t        start_time = time.time()\n\t        self.hook.run(sql, self.autocommit, parameters=self.parameters)\n\t        for output in self.hook.conn.notices:\n\t            self.log.info(output)\n", "        self.log.info(f\"Took {time.time() - start_time} s to execute query\")\n\tclass PostgresPandasTransformOperator(BaseOperator):\n\t    \"\"\"\n\t    Executes sql code from a specific Postgres database, load to pandas Dataframe and transform, and save to local file\n\t    :param sql: the sql code to be executed. (templated)\n\t    :type sql: Can receive a str representing a sql statement,\n\t        a list of str (sql statements), or reference to a template file.\n\t        Template reference are recognized by str ending in '.sql'\n\t    :param sql_generator: the function that return sql code\n\t    :type sql_generator: callable\n", "    :param sql_generator_kwargs: a dict of parameters that passed into sql_generator function (templated)\n\t    :type sql_generator_kwargs: dict\n\t    :param postgres_conn_id: reference to a specific postgres database\n\t    :type postgres_conn_id: str\n\t    :param schema: schema's name which will overwrite one defined in connection\n\t    :type schema: str\n\t    :param table: postgres's table name for selecting all from\n\t    :type table: str. Default: None\n\t    :param pd_transformer:  the function take a dataframe, transform and return a dataframe or list of dataframe\n\t        this function must take dataframe argument named 'dataframe' to pass a dataframe into function to transform\n", "    :type pd_transformer: callable\n\t    :param pd_transformer_kwargs: a dict of parameters that passed into pd_transformer function (templated)\n\t    :type pd_transformer_kwargs: dict\n\t    :param storage_type: type of file. Currently only support 'csv' or 'parquet' (default: 'parquet')\n\t    :type storage_type: str\n\t    :param storage_config: key value pair config for storing file (templated)\n\t        that will pass into pandas.to_csv or pandas.to_parquet function as kwargs\n\t    :type storage_config: dict\n\t    :param file_name_prefix: file name prefix when save file (default: 'file')\n\t    :type file_name_prefix: str\n", "    :param local_destination: directory to save file (default: '/tmp')\n\t    :type local_destination: str\n\t    \"\"\"\n\t    template_fields = ('sql', 'sql_generator_kwargs', 'pd_transformer_kwargs', 'storage_config', 'file_name_prefix', 'local_destination',)\n\t    template_ext = ('.sql',)\n\t    ui_color = '#c27878'\n\t    def __init__(\n\t            self,\n\t            sql: Optional[str] = None,\n\t            sql_generator: Optional[Callable] = None,\n", "            sql_generator_kwargs: Dict = {},\n\t            postgres_conn_id: str = \"postgres_default\",\n\t            schema: Optional[str] = None,\n\t            table: str = None,\n\t            pd_transformer: Optional[Callable] = None,\n\t            pd_transformer_kwargs: Dict = {},\n\t            column_map: Dict = {},\n\t            storage_type: str = \"parquet\",\n\t            storage_config: Dict = {},\n\t            file_name_prefix: str = \"file\",\n", "            local_destination: str = \"/tmp/airflow\",\n\t            *args, **kwargs):\n\t        super(PostgresPandasTransformOperator, self).__init__(*args, **kwargs)\n\t        self.sql = sql\n\t        self.sql_generator = sql_generator\n\t        self.sql_generator_kwargs = sql_generator_kwargs\n\t        self.postgres_conn_id = postgres_conn_id\n\t        self.schema = schema\n\t        self.table = table\n\t        self.pd_transformer = pd_transformer\n", "        self.pd_transformer_kwargs = pd_transformer_kwargs\n\t        self.column_map = column_map\n\t        self.storage_type = storage_type\n\t        self.storage_config = storage_config\n\t        self.local_destination = local_destination\n\t        self.file_name_prefix = file_name_prefix\n\t    def _pull_postgres_to_pandas(self):\n\t        if self.table is not None:\n\t            sql = f\"SELECT * FROM {self.schema}.{self.table}\"\n\t        elif self.sql is not None:\n", "            sql = self.sql\n\t        elif self.sql_generator is not None:\n\t            sql = self.sql_generator(**self.sql_generator_kwargs)\n\t        else:\n\t            raise AirflowException(\"Require table, sql or sql_generator to execute PostgresTransformOperator\")\n\t        start_time = time.time()\n\t        hook = PostgresPandasHook(\n\t            postgres_conn_id=self.postgres_conn_id,\n\t            # schema=self.schema\n\t        )\n", "        self.log.info(f\"SQL: {sql}\")\n\t        df = hook.query_from_postgres(sql)\n\t        self.log.info(f\"Took {time.time() - start_time}s to pull SQL\")\n\t        return df\n\t    def _transform_pandas(self, df: pd.DataFrame):\n\t        start_time = time.time()\n\t        if not self.pd_transformer:\n\t            return df\n\t        transformer_kwargs = self.pd_transformer_kwargs.copy()\n\t        transformer_kwargs[\"dataframe\"] = df\n", "        transformed_df = self.pd_transformer(**transformer_kwargs)\n\t        self.log.info(f\"Took {time.time() - start_time} s to transform pandas dataframe\")\n\t        return transformed_df\n\t    def _save_dataframe(self, df: pd.DataFrame, surfix=None):\n\t        if self.storage_type not in [\"csv\", \"parquet\"]:\n\t            raise AirflowException(f\"Storage type: {self.storage_type} currently not supported !!!\" + \\\n\t                \"Please use storage_type = 'csv' or 'parquet'\")\n\t        else:\n\t            location = os.path.join(self.local_destination, self.file_name_prefix)\n\t            if surfix is not None: location = location + \"_\" + str(surfix)\n", "            file_name = f\"{location}.{self.storage_type}\"\n\t            if self.storage_type == \"csv\":\n\t                df.to_csv(file_name, **self.storage_config)\n\t            else:\n\t                df.to_parquet(file_name, **self.storage_config)\n\t            self.log.info(f\"Saved file: {file_name}\")\n\t    def execute(self, context):\n\t        df = self._pull_postgres_to_pandas()\n\t        result = self._transform_pandas(df)\n\t        if self.column_map:\n", "            df.rename(columns=self.column_map, inplace=True)\n\t        make_new_folder(self.local_destination)\n\t        if isinstance(result, pd.DataFrame):\n\t            self._save_dataframe(result)\n\t        elif isinstance(result, list):\n\t            for i, _df in enumerate(result):\n\t                self._save_dataframe(_df, surfix=i)\n\t        else:\n\t            raise AirflowException(\"Transformer must return a dataframe or list of dataframe\")\n"]}
{"filename": "operators/pg_to_parquet.py", "chunked_list": ["import os\n\timport pandas.io.sql as psql\n\tfrom operators.pg_transform import PostgresTransformOperator\n\tclass PostgresToParquetOperator(PostgresTransformOperator):\n\t    template_fields = ('path_dir', 'file_name')\n\t    ui_color = '#33CBCE'\n\t    def __init__(self,\n\t                path_dir: str,\n\t                file_name: str,\n\t                compress: str = \"snappy\",\n", "                *args, **kwargs):\n\t        super(PostgresToParquetOperator, self).__init__(*args, **kwargs)\n\t        self.path_dir = path_dir\n\t        self.file_name = file_name\n\t        self.compress = compress\n\t    def execute(self, context):\n\t        if not os.path.exists(self.path_dir):\n\t            os.makedirs(self.path_dir)\n\t        full_path = \"{}/{}.parquet\".format(self.path_dir,self.file_name)\n\t        target_fields, df_records = self.get_records()\n", "        df_records.to_parquet(full_path, index=False, engine='pyarrow', compression=self.compress)\n\t        self.log.info(\"Store data in tmp file: {}\".format(full_path))\n\t    def get_records(self):\n\t        if self.run_date is None:\n\t            raise ValueError(\"Missing run_date argument!!\")\n\t        sql = self.sql_builder(self.run_date)\n\t        self.log.info('Executing: %s', sql)\n\t        df_records = psql.read_sql(sql, con=self.hook.get_conn())\n\t        target_fields = df_records.columns.tolist()\n\t        self.log.info(\"Target Fields: \", target_fields)\n", "        self.log.info(\"Get number of records {}\".format(df_records.shape[0]))\n\t        if df_records.shape[0] < 1:\n\t            raise ValueError(\n\t                \"Data Quality validation FAILED, get ZERO records!!\")\n\t        return target_fields, df_records\n"]}
{"filename": "operators/hive.py", "chunked_list": ["import time\n\timport shutil\n\timport contextlib\n\timport pandas as pd\n\tfrom datetime import timedelta\n\tfrom typing import Callable, Dict, Optional, Union, List\n\tfrom airflow.models import BaseOperator\n\tfrom airflow.exceptions import AirflowException\n\tfrom airflow.providers.apache.hive.hooks.hive import HiveServer2Hook, HiveMetastoreHook\n\tfrom hooks.hdfs import HDFSHook\n", "from operators.postgres import PostgresPandasTransformOperator\n\tfrom operators.hdfs import PutHDFSOperator, RmHDFSOperator\n\tfrom utils.os_helper import make_new_folder\n\tclass HiveServer2ExecOperator(BaseOperator):\n\t    template_fields = ('hql', 'hql_generator_kwargs')\n\t    def __init__(self,\n\t                hql: Union[str, List[str]] = None,\n\t                hql_generator: Optional[Callable] = None,\n\t                hql_generator_kwargs: Dict = {},\n\t                hiveserver2_conn_id: str = \"hive_server2_default\",\n", "                hive_schema: Optional[str] = None,\n\t                validator: Optional[str] = None,\n\t                **kwargs):\n\t        super(HiveServer2ExecOperator, self).__init__(**kwargs)\n\t        self.hql = hql\n\t        self.hql_generator = hql_generator\n\t        self.hql_generator_kwargs = hql_generator_kwargs\n\t        self.hiveserver2_conn_id = hiveserver2_conn_id\n\t        self.hive_schema = hive_schema\n\t        self.validator = validator\n", "    def _execute_queries(self, hqls: List[str]):\n\t        hook = HiveServer2Hook(hiveserver2_conn_id=self.hiveserver2_conn_id)\n\t        with contextlib.closing(hook.get_conn(self.hive_schema)) as conn, contextlib.closing(conn.cursor()) as cur:\n\t            for hql in hqls:\n\t                # self.log.info(f\"Executing HQL: {hql}\")\n\t                ret = cur.execute(hql)\n\t                self.log.info(ret)\n\t    def _validate_hqls(hql: Union[str, List[str]]):\n\t        if type(hql) is list: return hql\n\t        else: return [hql]\n", "    def execute(self, context):\n\t        if self.hql is not None:\n\t            hqls = self._validate_hqls(self.hql)\n\t        elif self.hql_generator:\n\t            hqls = self._validate_hqls(self.hql_generator(**self.hql_generator_kwargs))\n\t        else:\n\t            raise AirflowException(\"Require hql or hql_generator to execute HiveServer2ExecOperator\")\n\t        if self.validator:\n\t            self.log.info(\"Use validator hql\")\n\t            hqls.append(self.validator)\n", "        self._execute_queries(hqls)\n\tclass Postgres2HiveOperator(PostgresPandasTransformOperator, HiveServer2ExecOperator):\n\t    template_fields = ('hive_table', 'hive_partitions', 'hive_partitions_generator_kwargs', 'local_temporary_dir', 'hdfs_temporary_dir', \\\n\t                        'sql', 'sql_generator_kwargs', 'pd_transformer_kwargs', 'storage_config', 'file_name_prefix', 'local_destination',\n\t                        'hql', 'hql_generator_kwargs')\n\t    ui_color = '#3da3da'\n\t    \"\"\"\n\t    Migrate data from postgres to hive through hiveserver2\n\t    :param hive_table: the destination table on hive\n\t    :type hive_table: str\n", "    :param hive_overwrite: weather overwrite or not existed data on hive\n\t    :type hive_overwrite: bool\n\t    :param hive_partitions: hive specific partition using when insert into table, it'type may be List or Dict or None\n\t        Ex: if hive_partitions = {'date': '2022-01-01'}, partitioned statement will be \"PARTITION(date='2022-01-01')\"\n\t        else if hive_partitions = ['date'], the column 'date' must be contained in selected sql from postgres and partitioned statement\n\t        will be \"PARTITION(date)\"\n\t    :type hive_partitions: Union[Dict, List]\n\t    :param hive_partitions_generator: the callable that return hive_partitions\n\t    :type hive_partitions_generator: callable\n\t    :param hive_partitions_generator_kwargs: the dict contained parameters that will pass into hive_partitions_generator\n", "    :type hive_partitions_generator_kwargs: dict\n\t    :param local_temporary_dir: local temporary directory to store intermediary data from postgres\n\t    :type local_temporary_dir: str\n\t    :param hdfs_temporary_dir: hdfs temporary directory to store intermediary data before loading to hive table\n\t    :type hdfs_temporary_dir: str\n\t    \"\"\"\n\t    def __init__(self,\n\t                hive_table: str,\n\t                hive_overwrite: bool = False,\n\t                hive_partitions: Union[Dict, List] = None,\n", "                hive_partitions_generator: Optional[Callable] = None,\n\t                hive_partitions_generator_kwargs: Dict = {},\n\t                local_temporary_dir: Optional[str] = None,\n\t                hdfs_temporary_dir: Optional[str] = None,\n\t                metastore_conn_id: str = \"hive_metastore\",\n\t                hdfs_conn_id: str = \"hdfs_default\",\n\t                hdfs_user: str = \"hive\",\n\t                 **kwargs):\n\t        super(Postgres2HiveOperator, self).__init__(**kwargs)\n\t        self.hivehive_overwrite = hive_overwrite\n", "        self.hive_table = hive_table\n\t        self.hive_partitions = hive_partitions\n\t        self.hive_partitions_generator = hive_partitions_generator\n\t        self.hive_partitions_generator_kwargs = hive_partitions_generator_kwargs\n\t        self.local_temporary_dir = local_temporary_dir\n\t        self.hdfs_temporary_dir = hdfs_temporary_dir\n\t        self.metastore_conn_id = metastore_conn_id\n\t        self.hdfs_conn_id = hdfs_conn_id\n\t        self.hdfs_user = hdfs_user\n\t        self.is_partition_explicit = True\n", "    def _get_table_description(self):\n\t        hms_hook = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n\t        return hms_hook.get_table(self.hive_table, self.hive_schema)\n\t    def _normalize_pandas(self, df: pd.DataFrame):\n\t        t = self._get_table_description()\n\t        cols = t.sd.cols if self.is_partition_explicit else t.sd.cols + t.partitionKeys\n\t        for col in cols:\n\t            if col.type == \"tinyint\":\n\t                df[col.name] = df[col.name].astype('Int8')\n\t            elif col.type == \"smallint\":\n", "                df[col.name] = df[col.name].astype('Int16')\n\t            elif col.type == \"int\":\n\t                df[col.name] = df[col.name].astype('Int32')\n\t            elif col.type == \"bigint\":\n\t                df[col.name] = df[col.name].astype('Int64')\n\t            elif col.type == \"float\":\n\t                df[col.name] = df[col.name].astype('float32')\n\t            elif col.type == \"double\":\n\t                df[col.name] = df[col.name].astype('float64')\n\t            elif col.type == \"timestamp\":\n", "                df[col.name] = pd.to_datetime(df[col.name])\n\t            elif col.type == \"date\":\n\t                df[col.name] = df[col.name].astype('str')\n\t            elif col.type == \"boolean\":\n\t                pass\n\t            else:\n\t                df[col.name] = df[col.name].astype('str')\n\t        return df\n\t    def _generate_create_hive_temporay_table(self):\n\t        t = self._get_table_description()\n", "        cols = t.sd.cols if self.is_partition_explicit else t.sd.cols + t.partitionKeys\n\t        normalized_cols = list(map(lambda c: (c.name, 'string') if c.type == \"date\" else (c.name, c.type), cols))\n\t        defined_cols = \",\".join([f\"`{col[0]}` {col[1]}\" for col in normalized_cols])\n\t        return [\n\t            f\"DROP TABLE IF EXISTS {self.hive_temporary_table}\",\n\t            f\"\"\"\n\t                CREATE EXTERNAL TABLE IF NOT EXISTS {self.hive_schema}.{self.hive_temporary_table} ({defined_cols})\n\t                COMMENT 'temporary for transfer data from postgres to hive'\n\t                STORED AS PARQUET\n\t                LOCATION '{self.hdfs_temporary_dir}'\n", "                TBLPROPERTIES ('external.table.purge'='true')\n\t            \"\"\",\n\t        ]\n\t    def _generate_insert_data_from_temporary(self):\n\t        def _resolve_partition(kv):\n\t            if type(kv[1]) is str: return f\"{kv[0]}='{kv[1]}'\"\n\t            else: return f\"{kv[0]}={kv[1]}\"\n\t        partition_clause = \"\"\n\t        if self.hive_partitions:\n\t            if self.is_partition_explicit:\n", "                partition_cols = \", \".join(list(map(lambda kv: _resolve_partition(kv), self.hive_partitions.items())))\n\t            else:\n\t                partition_cols = \", \".join(self.hive_partitions)\n\t            partition_clause = f\"PARTITION({partition_cols})\"\n\t        overwrite_clause = \"OVERWRITE\" if self.hivehive_overwrite else \"INTO\"\n\t        return [\n\t            \"SET hive.execution.engine = mr\",\n\t            f\"\"\"\n\t                INSERT {overwrite_clause} TABLE {self.hive_table}\n\t                {partition_clause}\n", "                SELECT * FROM {self.hive_temporary_table}\n\t            \"\"\",\n\t        ]\n\t    def _generate_drop_hive_temporary_table(self):\n\t        return [f\"\"\"\n\t            DROP TABLE {self.hive_temporary_table}\n\t        \"\"\"]\n\t    def _preprocess_partition(self):\n\t        if self.hive_partitions_generator:\n\t            self.hive_partitions = self.hive_partitions_generator(**self.hive_partitions_generator_kwargs)\n", "        if self.hive_partitions:\n\t            if type(self.hive_partitions) is dict:\n\t                self.is_partition_explicit = True\n\t            elif type(self.hive_partitions) is list:\n\t                self.is_partition_explicit = False \n\t            else:\n\t                raise AirflowException(\"Type of hive_partitions must be List or Dict\")\n\t    def execute(self, context):\n\t        execution_date = (context['dag_run'].execution_date + timedelta(hours=7)).strftime('%Y%m%d')\n\t        self.local_temporary_dir = self.local_temporary_dir or f'/tmp/airflow/{self.dag_id}/{self.task_id}/{execution_date}'\n", "        self.hdfs_temporary_dir = self.hdfs_temporary_dir or f'/tmp/airflow/{self.dag_id}/{self.task_id}/{execution_date}'\n\t        self.hive_temporary_table = self.hive_table + \"_\" + execution_date\n\t        start_time = time.time()\n\t        df = self._pull_postgres_to_pandas()\n\t        if self.column_map: df.rename(columns=self.column_map, inplace=True)\n\t        df = self._transform_pandas(df)\n\t        df = self._normalize_pandas(df)\n\t        make_new_folder(self.local_temporary_dir)\n\t        df.to_parquet(f\"{self.local_temporary_dir}/{self.hive_table}.parquet\", index=False,  engine=\"pyarrow\", compression=None, allow_truncated_timestamps=True, use_deprecated_int96_timestamps=True)\n\t        self.log.info(\"STEP 1: took {}s to pull and transform data from postgres\".format(time.time() - start_time))\n", "        start_time = time.time()\n\t        hook = HDFSHook(hdfs_conn_id=self.hdfs_conn_id, hdfs_user=self.hdfs_user)\n\t        client = hook.get_conn()\n\t        file_conf = hook.get_file_conf()\n\t        PutHDFSOperator._copyObjToDir(self.local_temporary_dir, self.hdfs_temporary_dir, client, file_conf, file_filter=None)\n\t        self.log.info(\"STEP 2: took {}s to push data to hdfs\".format(time.time() - start_time))\n\t        start_time = time.time()\n\t        hqls = []\n\t        self._preprocess_partition()\n\t        hqls.extend(self._generate_create_hive_temporay_table())\n", "        hqls.extend(self._generate_insert_data_from_temporary())\n\t        hqls.extend(self._generate_drop_hive_temporary_table())\n\t        self._execute_queries(hqls)\n\t        self.log.info(\"STEP 3: took {}s to load data from hdfs to hive\".format(time.time() - start_time))\n\t        shutil.rmtree(self.local_temporary_dir)\n\t        self.log.info(f\"STEP 4: clean local temporary dir: {self.local_temporary_dir}\")\n\t        RmHDFSOperator._remove(client, self.hdfs_temporary_dir)\n\t        self.log.info(f\"STEP 5: clean hdfs temporary dir: {self.hdfs_temporary_dir}\")\n"]}
{"filename": "operators/airflow_monitors.py", "chunked_list": ["import os\n\tfrom typing import Optional\n\tfrom datetime import datetime, timedelta\n\tfrom airflow.models.baseoperator import BaseOperator\n\tfrom airflow.configuration import conf\n\tfrom utils.os_helper import clear_fs_obj\n\tfrom utils.common import println, timing\n\tclass AirflowCleanLogsOperator(BaseOperator):\n\t    \"\"\"\n\t    A operator to clean logs by retain days\n", "    :param run_date: the execute_date of dagrun with format '%Y-%m-%d' (templated)\n\t    :type run_date: str\n\t    :param system_retain_days: the number of days until the run_date should be keep system logs (templated)\n\t    :type system_retain_days: int\n\t    :param user_retain_days: the number of days until the run_date should be keep user defined dag logs (templated)\n\t    :type user_retain_days: int\n\t    :param ssh_conn_id: the ssh connection information for ssh to all airflow component node\n\t    :type ssh_conn_id: str\n\t    :param base_log_folder: the root folder to store logs\n\t    :type base_log_folder: Optional[str]\n", "    \"\"\"\n\t    template_fields = ('system_retain_days', 'user_retain_days', 'run_date', 'base_log_folder')\n\t    ui_color = '#64d97d'\n\t    def __init__(self,\n\t            run_date: str,\n\t            system_retain_days: int = 3,\n\t            user_retain_days: int = 15,\n\t            base_log_folder: Optional[str] = None,\n\t            ssh_conn_id: str = 'ssh_airflow_default',\n\t            *args, **kwargs):\n", "        super(AirflowCleanLogsOperator, self).__init__(*args, **kwargs)\n\t        self.run_date = run_date\n\t        self.system_retain_days = system_retain_days\n\t        self.user_retain_days = user_retain_days\n\t        self.ssh_conn_id = ssh_conn_id\n\t        self.base_log_folder = base_log_folder\n\t    @timing\n\t    def execute(self, context):\n\t        base_log_folder = self.base_log_folder or conf.get('logging', 'base_log_folder')\n\t        system_dag_names = ['dag_processor_manager', 'scheduler']\n", "        try:\n\t            run_date = datetime.strptime(self.run_date, '%Y-%m-%d').date()\n\t        except:\n\t            run_date = datetime.utcnow().date()\n\t        dag_names = os.listdir(base_log_folder)\n\t        self.log.info(f\"Dag run_date: {run_date}\")\n\t        println(f\"Clear log inside base_log_folder: {base_log_folder}\")\n\t        for dag in dag_names:\n\t            dag_folder = os.path.join(base_log_folder, dag)\n\t            println(f\"++ Clear log for sub folder: {dag_folder}\")\n", "            if dag not in system_dag_names:\n\t                dag_runs = os.listdir(dag_folder)\n\t                for run_id in dag_runs:\n\t                    scheduled_at = run_id.replace(\"run_id=scheduled__\", \"\")\n\t                    try:\n\t                        dt = datetime.strptime(scheduled_at, '%Y-%m-%dT%H:%M:%S%z').date()\n\t                    except:\n\t                        try:\n\t                            dt = datetime.strptime(scheduled_at, '%Y-%m-%d').date()\n\t                        except:\n", "                            dt = run_date\n\t                    if dag not in system_dag_names:\n\t                        if dt + timedelta(days=self.user_retain_days) < run_date:\n\t                            clear_fs_obj(os.path.join(dag_folder, run_id))\n\t                    elif dag == 'scheduler':\n\t                        if dt + timedelta(days=self.system_retain_days) < run_date:\n\t                            clear_fs_obj(os.path.join(dag_folder, run_id))\n"]}
{"filename": "operators/druid.py", "chunked_list": ["import os\n\timport json\n\timport pandas as pd\n\tfrom typing import Dict, Optional, Callable\n\tfrom airflow.models import BaseOperator\n\tfrom airflow.providers.apache.druid.hooks.druid import DruidDbApiHook\n\tfrom airflow.exceptions import AirflowException\n\tfrom hooks.postgres import PostgresPandasHook\n\tfrom hooks.druid_custom import DruidCustomHook\n\tfrom operators.hdfs import HDFSException, PutHDFSOperator, RmHDFSOperator\n", "from hooks.hdfs import HDFSHook\n\tfrom utils.common import timing\n\tclass DruidIngestOperator(BaseOperator):\n\t    \"\"\"\n\t    Ingest data into specific Druid database with input payload\n\t    :param payload: the ingest payload to be executed. (templated)\n\t    :type payload: str\n\t    :param payload_generator: the function that return payload\n\t    :type payload_generator: callable\n\t    :param payload_generator_kwargs: a dict of parameters that passed into payload_generator function\n", "    :type payload_generator_kwargs: dict\n\t    :param druid_ingest_conn_id: reference to a specific druid ingest api\n\t    :type druid_ingest_conn_id: str\n\t    :param timeout: period time to check task status\n\t    :type timeout: int\n\t    :param max_ingestion_time: max ingestion time to wait before shutdown task\n\t    :type max_ingestion_time: int\n\t    \"\"\"\n\t    template_fields = ('payload', 'payload_generator_kwargs',)\n\t    template_ext = ('.sql',)\n", "    ui_color = '#64d97d'\n\t    def __init__(self,\n\t            payload = None,\n\t            payload_generator: callable = None,\n\t            payload_generator_kwargs: dict = {},\n\t            druid_ingest_conn_id = 'druid_ingest_default',\n\t            timeout = 30,\n\t            max_ingestion_time = None,\n\t            wait_to_finish: bool = True,\n\t            *args, **kwargs):\n", "        super(DruidIngestOperator, self).__init__(*args, **kwargs)\n\t        self.payload = payload\n\t        self.payload_generator = payload_generator\n\t        self.payload_generator_kwargs = payload_generator_kwargs\n\t        self.druid_ingest_conn_id = druid_ingest_conn_id\n\t        self.timeout = timeout\n\t        self.max_ingestion_time = max_ingestion_time\n\t        self.wait_to_finish = wait_to_finish\n\t    @timing\n\t    def execute(self, context):\n", "        if self.payload is not None:\n\t            payload = self.payload\n\t        elif self.payload_generator is not None:\n\t            payload = self.payload_generator(**self.payload_generator_kwargs)\n\t        else:\n\t            raise AirflowException(\"Require payload or payload_generator to execute DruidIngestOperator\")\n\t        if not payload:\n\t            self.log.info(\"Return with no payload to ingest\")\n\t            return\n\t        if type(payload) is dict:\n", "            payload = json.dumps(payload, indent=2)\n\t        self.hook = DruidCustomHook(\n\t            druid_ingest_conn_id=self.druid_ingest_conn_id,\n\t            timeout=self.timeout,\n\t            max_ingestion_time=self.max_ingestion_time\n\t        )\n\t        self.hook.submit_indexing_job(payload)\n\tclass Druid2PostgresOperator(BaseOperator):\n\t    \"\"\"\n\t    Executes sql code in a specific Druid database api and transfer it to postgresql\n", "    :param sql: the sql code to be executed. (templated)\n\t    :type sql: Can receive a str representing a sql statement,\n\t        a list of str (sql statements), or reference to a template file.\n\t        Template reference are recognized by str ending in '.sql'\n\t    :param sql_generator: the function that return sql code\n\t    :type sql_generator: callable\n\t    :param sql_generator_kwargs: a dict of parameters that passed into sql_generator function (templated)\n\t    :type sql_generator_kwargs: dict\n\t    :param druid_query_conn_id: reference to a specific postgres database\n\t    :type druid_query_conn_id: str\n", "    :param postgres_conn_id: reference to a specific postgres database\n\t    :type postgres_conn_id: str\n\t    :param pg_database: name of postgres database which overwrite defined one in connection\n\t    :type pg_database: str\n\t    :param pg_insert_kwargs: key words arguments that pass into insert method\n\t        (see more: PostgresInsertHook.insert_pandas_2_postgres)\n\t    :type pg_insert_kwargs: dict\n\t    \"\"\"\n\t    template_fields = ('sql', 'sql_generator_kwargs',)\n\t    template_ext = ('.sql',)\n", "    ui_color = '#00eaff'\n\t    def __init__(\n\t            self,\n\t            sql = None,\n\t            sql_generator: callable = None,\n\t            sql_generator_kwargs: dict = {},\n\t            druid_query_conn_id = \"druid_query_default\",\n\t            postgres_conn_id='postgres_default',\n\t            pg_database=None,\n\t            pg_insert_kwargs={},\n", "            *args, **kwargs):\n\t        super(Druid2PostgresOperator, self).__init__(*args, **kwargs)\n\t        self.sql = sql\n\t        self.sql_generator = sql_generator\n\t        self.sql_generator_kwargs = sql_generator_kwargs\n\t        self.druid_query_conn_id = druid_query_conn_id\n\t        self.postgres_conn_id = postgres_conn_id\n\t        self.pg_database = pg_database\n\t        self.pg_insert_kwargs = pg_insert_kwargs\n\t    def get_druid_hook(self):\n", "        \"\"\"\n\t        Return the druid db api sqoop_hook.\n\t        \"\"\"\n\t        return DruidDbApiHook(druid_broker_conn_id=self.druid_query_conn_id)\n\t    def get_postgres_hook(self):\n\t        return PostgresPandasHook(\n\t            postgres_conn_id=self.postgres_conn_id,\n\t            schema=self.pg_database\n\t        )\n\t    @timing\n", "    def execute(self, context):\n\t        if self.sql is not None:\n\t            sql = self.sql\n\t        elif self.sql_generator is not None:\n\t            sql = self.sql_generator(**self.sql_generator_kwargs)\n\t        else:\n\t            raise AirflowException(\"Require sql or sql_generator to execute PostgresExtraOperator\")\n\t        engine = self.get_druid_hook().get_sqlalchemy_engine()\n\t        dataframe = pd.read_sql_query(sql, con=engine)\n\t        dataframe.info()\n", "        self.get_postgres_hook().insert_pandas_2_postgres(\n\t            dataframe=dataframe,\n\t            **self.pg_insert_kwargs\n\t        )\n\tclass DruidMarkSegmentAsUnusedOperator(BaseOperator):\n\t    \"\"\"\n\t    Mard unused segments by intervals or segment ids\n\t    :param datasource: name of datasource\n\t    :type datasource: str\n\t    :param since: the start timestamp in UTC of interval to mark segments as unused (ex: 2021-01-01T12:00:00.000Z)\n", "    :type since: str\n\t    :param until: the end timestamp in UTC of interval to mark segments as unused (ex: 2021-01-02T12:00:00.000Z)\n\t    :type until: str\n\t    :param interval_generator: a callable that return tuple (since, until) or list[(since1, until1), (since2, until2)]\n\t        Note that since and until parameters with be ignored when this is not None\n\t    :type interval_generator: callable\n\t    :param interval_generator_kwargs: a dict of parameters that passed into interval_generator\n\t    :type interval_generator_kwargs: dict\n\t    :param segment_ids: list of segment id\n\t    :type segment_ids: list(str)\n", "    :param segment_ids_generator: a callable that return segment_ids\n\t    :type segment_ids_generator: callable\n\t    :param segment_ids_generator_kwargs: a dict of parameters that passed into segment_ids_generator\n\t    :type segment_ids_generator_kwargs: dict\n\t    :param druid_ingest_conn_id: reference to a specific druid ingest api\n\t    :type druid_ingest_conn_id: str\n\t    \"\"\"\n\t    template_fields = ('datasource', 'since', 'until', 'interval_generator_kwargs', 'segment_ids_generator_kwargs')\n\t    ui_color = '#ff6924'\n\t    def __init__(self,\n", "            datasource,\n\t            since = None,\n\t            until = None,\n\t            interval_generator: callable = None,\n\t            interval_generator_kwargs: dict = {},\n\t            segment_ids = None,\n\t            segment_ids_generator: callable = None,\n\t            segment_ids_generator_kwargs: dict = {},\n\t            druid_ingest_conn_id = 'druid_ingest_default',\n\t            *args, **kwargs):\n", "        super(DruidMarkSegmentAsUnusedOperator, self).__init__(*args, **kwargs)\n\t        self.datasource = datasource\n\t        self.since = since\n\t        self.until = until\n\t        self.interval_generator = interval_generator\n\t        self.interval_generator_kwargs = interval_generator_kwargs\n\t        self.segment_ids = segment_ids\n\t        self.segment_ids_generator = segment_ids_generator\n\t        self.segment_ids_generator_kwargs = segment_ids_generator_kwargs\n\t        self.druid_ingest_conn_id = druid_ingest_conn_id\n", "    @timing\n\t    def execute(self, context):\n\t        hook = DruidCustomHook(\n\t            druid_ingest_conn_id=self.druid_ingest_conn_id,\n\t        )\n\t        if (self.since and self.until) is not None or self.interval_generator:\n\t            intervals = self.interval_generator(**self.interval_generator_kwargs) \\\n\t                if self.interval_generator else (self.since, self.until)\n\t            if isinstance(intervals, list):\n\t                for since, until in intervals:\n", "                    hook.mask_unused_by_intervals(self.datasource, since, until)\n\t            elif isinstance(intervals, tuple):\n\t                hook.mask_unused_by_intervals(self.datasource, intervals[0], intervals[1])\n\t            else:\n\t                raise AirflowException(\"intervals return from interval_generator must be a list of tuple (since, until) or (since, until)\")\n\t        else:\n\t            if self.segment_ids is None and self.segment_ids_generator is None:\n\t                raise AirflowException(\"Require both since and until aren't None or interval_generator isn't None\" + \\\n\t                    \"or segment_ids isn't None or segment_ids_generator isn't None\")\n\t            segment_ids = self.segment_ids or self.segment_ids_generator(**self.segment_ids_generator_kwargs)\n", "            hook.mark_unused_by_segment_ids(self.datasource, segment_ids)\n\tclass DruidTruncateDatasourceOperator(BaseOperator):\n\t    \"\"\"\n\t    Mard unused all segments of datasource\n\t    :param datasource: name of datasource\n\t    :type datasource: str\n\t    :param druid_ingest_conn_id: reference to a specific druid ingest api\n\t    :type druid_ingest_conn_id: str\n\t    \"\"\"\n\t    template_fields = ('datasource', 'druid_ingest_conn_id',)\n", "    ui_color = '#ff6924'\n\t    def __init__(self,\n\t        datasource,\n\t        druid_ingest_conn_id=\"druid_ingest_default\",\n\t        *args, **kwargs):\n\t        super(DruidTruncateDatasourceOperator, self).__init__(*args, **kwargs)\n\t        self.datasource = datasource\n\t        self.druid_ingest_conn_id = druid_ingest_conn_id\n\t    @timing\n\t    def execute(self, context):\n", "        hook = DruidCustomHook(\n\t            druid_ingest_conn_id=self.druid_ingest_conn_id,\n\t        )\n\t        hook.truncate_datasource(self.datasource)\n\tclass Local2DruidOperator(BaseOperator):\n\t    \"\"\"\n\t    This operator using to put local file or folder of data into a HDFS folder\n\t    :param local_source: local absolute path of file or folder \n\t    :param dest_dir: HDFS destination directory\n\t    :param file_conf: the configuration about storage file on HDFS include number of replication and blocksize (unit: byte)\n", "    :type file_conf: dict (default: {'replication': 2, 'blocksize': 134217728})\n\t    :param file_filter: a callcable that receive a file_name and return False if want to filter out this file\n\t    :type file_conf: Callable (default: None)\n\t    :param hook: Hook class that this operator based on\n\t    :type hook: cls\n\t    :param hdfs_conn_id: the connection ID of HDFS\n\t    :type hdfs_conn_id: str\n\t    :param hdfs_user: the user do this operator \n\t    :type hdfs_user : str (default: hadoop)\n\t    \"\"\"\n", "    template_fields = ('local_source', 'hdfs_dir', 'file_conf', 'file_filter', )\n\t    def __init__(self,\n\t        local_source: str,\n\t        hdfs_dir: str,\n\t        file_conf: Optional[Dict] = None,\n\t        file_filter: Optional[Callable] = None,\n\t        hdfs_hook = HDFSHook,\n\t        hdfs_conn_id: str = \"hdfs_default\",\n\t        hdfs_user: str = \"hadoop\",\n\t        remove_after = False,\n", "        payload = None,\n\t        payload_generator: callable = None,\n\t        payload_generator_kwargs: dict = {},\n\t        druid_ingest_conn_id = 'druid_ingest_default',\n\t        timeout = 30,\n\t        max_ingestion_time = None,\n\t        wait_to_finish: bool = True,\n\t        **kwargs):\n\t        super(Local2DruidOperator, self).__init__(**kwargs)\n\t        self.local_source = local_source\n", "        self.hdfs_dir = hdfs_dir\n\t        self.file_conf = file_conf\n\t        self.file_filter = file_filter\n\t        self.hdfs_hook = hdfs_hook\n\t        self.hdfs_conn_id = hdfs_conn_id\n\t        self.hdfs_user = hdfs_user\n\t        self.remove_after = remove_after\n\t        self.druid_hook = DruidCustomHook(\n\t                druid_ingest_conn_id=druid_ingest_conn_id,\n\t                timeout=timeout,\n", "                max_ingestion_time=max_ingestion_time\n\t            )\n\t        self.wait_to_finish = wait_to_finish\n\t        self.payload = payload\n\t        self.payload_generator = payload_generator\n\t        self.payload_generator_kwargs = payload_generator_kwargs\n\t    @timing\n\t    def execute(self, context):\n\t        self.log.info(\"Local files: {}\".format(self.local_source))\n\t        if not self.local_source:\n", "            raise HDFSException('Source must be provided !!!')\n\t        if not os.path.exists(self.local_source):\n\t            raise HDFSException(\n\t                f\"Source {self.local_source} isn't existed !!!\")\n\t        if not self.hdfs_dir:\n\t            raise HDFSException('HDFS middle folder must be provided !!!')\n\t        if self.payload is not None:\n\t            payload = self.payload\n\t        elif self.payload_generator is not None:\n\t            payload = self.payload_generator(**self.payload_generator_kwargs)\n", "        else:\n\t            raise AirflowException(\"Require payload or payload_generator to execute DruidIngestOperator\")\n\t        if not payload:\n\t            self.log.info(\"Return with no payload to ingest\")\n\t            return\n\t        hdfs_hook = self.hdfs_hook(hdfs_conn_id=self.hdfs_conn_id, hdfs_user=self.hdfs_user)\n\t        self.client = hdfs_hook.get_conn()\n\t        self.file_conf = self.file_conf if self.file_conf is not None else hdfs_hook.get_file_conf()\n\t        PutHDFSOperator._copyObjToDir(self.local_source, self.hdfs_dir, self.client, self.file_conf, self.file_filter)\n\t        if type(payload) is dict:\n", "            payload = json.dumps(payload, indent=2) \n\t        self.druid_hook.submit_indexing_job(payload)\n\t        if self.remove_after:\n\t            RmHDFSOperator._remove(self.client, self.hdfs_dir)\n"]}
{"filename": "operators/amplitude.py", "chunked_list": ["import os\n\tfrom datetime import timedelta\n\tfrom typing import Tuple\n\tfrom airflow.models.baseoperator import BaseOperator\n\tfrom hooks.amplitude import AmplitudeHook\n\tfrom utils.parse_date import convert_to_datetime_from_utc, covert_format_date_from_utc, create_path_from_date_time\n\tclass AmplitudeDownloadOperator(BaseOperator):\n\t    template_fields = ('op_kwargs',)\n\t    ui_color = '#33BCFE'\n\t    def __init__(self,\n", "                conn_id: str,\n\t                compress: str,\n\t                op_kwargs: dict,\n\t                time_range: int,\n\t                *args,\n\t                **kwargs):\n\t        super(AmplitudeDownloadOperator, self).__init__(*args, **kwargs)\n\t        self.conn_id = conn_id\n\t        self.compress = compress\n\t        self.op_kwargs = op_kwargs\n", "        self.time_range = time_range\n\t    def _download(self,\n\t                api_endpoint,\n\t                compress,\n\t                base_url,\n\t                start,\n\t                end,\n\t                file_path,\n\t                auth: Tuple[str, str]):\n\t        request_uri = \"\"\"curl -u {login}:{password} '{base}{endpoint}?start={start}&end={end}' >> {file_path}.{type}\"\"\".format(\n", "            login=auth[0],\n\t            password=auth[1],\n\t            base=base_url,\n\t            endpoint=api_endpoint,\n\t            start=start,\n\t            end=end,\n\t            type=compress,\n\t            file_path=file_path)\n\t        # print(request_uri)\n\t        self.log.info(\" *** Start downloading Amplitude data and put to dir %s.%s\" % (file_path, compress))\n", "        os.system(request_uri)\n\t        return\n\t    def execute(self, context):\n\t        run_date = self.op_kwargs.get('run_date')\n\t        amplitude_project_id = self.op_kwargs.get('amplitude_project_id')\n\t        start_datetime = convert_to_datetime_from_utc(run_date)\n\t        end_datetime = start_datetime + timedelta(hours=self.time_range)\n\t        amplitude_hook = AmplitudeHook(amplitude_conn=self.conn_id)\n\t        file_path, file_name = create_path_from_date_time(run_date)\n\t        download_dir = os.path.join(self.op_kwargs.get(\"local_dir\"), amplitude_project_id, file_path)\n", "        print(download_dir)\n\t        if not os.path.exists(download_dir):\n\t            os.makedirs(download_dir)\n\t        base_url, auth = amplitude_hook.get_conn()\n\t        self._download(api_endpoint=self.op_kwargs.get(\"api_endpoint\"),\n\t                            compress=self.compress,\n\t                            base_url=base_url,\n\t                            auth=auth,\n\t                            start=covert_format_date_from_utc(start_datetime, new_format='%Y%m%dT%H'),\n\t                            end=covert_format_date_from_utc(end_datetime, new_format='%Y%m%dT%H'),\n", "                            file_path=os.path.join(download_dir, file_name))\n"]}
{"filename": "operators/ssh_sqoop.py", "chunked_list": ["from airflow import AirflowException\n\tfrom airflow.models import BaseOperator\n\tfrom airflow.providers.apache.sqoop.hooks.sqoop import SqoopHook\n\tfrom airflow.providers.ssh.operators.ssh import SSHOperator\n\tfrom airflow.providers.ssh.hooks.ssh import SSHHook\n\tfrom hooks.sqoop import SqoopCustomHook\n\tfrom airflow.providers.ssh.operators.ssh import SSHOperator\n\tfrom select import select\n\tfrom airflow.configuration import conf\n\tfrom base64 import b64encode\n", "from select import select\n\tfrom typing import Optional, Union\n\tclass SSHSqoopImportOperator(BaseOperator):\n\t    template_fields = ('query', 'table', 'warehouse_dir', 'extra_import_options',)\n\t    ui_color = '#33CBFE'\n\t    def __init__(\n\t            self,\n\t            ssh_conn_id='ssh-default',\n\t            sqoop_conn_id='sqoop-default',\n\t            table=None,\n", "            query=None,\n\t            target_dir=None,\n\t            append: bool = False,\n\t            split_by=None,\n\t            file_type=None,\n\t            columns=None,\n\t            where=None,\n\t            direct=None,\n\t            driver=None,\n\t            schema=None,\n", "            ssh_timeout: int = 10,\n\t            keepalive_interval: int = 10,\n\t            extra_import_options=None,\n\t            verbose: bool = False,\n\t            num_mappers: int = None,\n\t            *args, **kwargs):\n\t        super(SSHSqoopImportOperator, self).__init__(*args, **kwargs)\n\t        self.ssh_conn_id = ssh_conn_id\n\t        self.sqoop_conn_id = sqoop_conn_id\n\t        self.table = table\n", "        self.query = query\n\t        self.target_dir = target_dir\n\t        self.append = append\n\t        self.file_type = file_type\n\t        self.columns = columns\n\t        self.split_by = split_by\n\t        self.where = where\n\t        self.direct = direct\n\t        self.driver = driver\n\t        self.extra_import_options = extra_import_options\n", "        self.verbose = verbose\n\t        self.ssh_timeout = ssh_timeout\n\t        self.sqoop_hook = None\n\t        self.ssh_hook = None\n\t        self.schema = schema\n\t        self.num_mappers = num_mappers\n\t        self.keepalive_interval = keepalive_interval\n\t    def execute(self, context):\n\t        self.sqoop_hook = SqoopCustomHook(schema=self.schema,\n\t                                          num_mappers=self.num_mappers,\n", "                                          conn_id=self.sqoop_conn_id,\n\t                                          verbose=self.verbose)\n\t        if self.query:\n\t            cmd = self.sqoop_hook.import_query_cmd(self.query,\n\t                                                    self.target_dir,\n\t                                                    self.append,\n\t                                                    self.file_type,\n\t                                                    self.split_by,\n\t                                                    self.direct,\n\t                                                    self.driver,\n", "                                                    self.extra_import_options)\n\t        elif self.table:\n\t            cmd = self.sqoop_hook.import_table_cmd(self.table,\n\t                                                    self.target_dir,\n\t                                                    self.append,\n\t                                                    self.file_type,\n\t                                                    self.split_by,\n\t                                                    self.direct,\n\t                                                    self.driver,\n\t                                                    self.extra_import_options)\n", "        else:\n\t            raise AirflowException(\"Provide query or table parameter to import using Sqoop\")\n\t        cmd = \" \".join(cmd)\n\t        try:\n\t            if self.ssh_conn_id:\n\t                if self.ssh_hook and isinstance(self.ssh_hook, SSHHook):\n\t                    self.log.info(\"ssh_conn_id is ignored when ssh_hook is provided.\")\n\t                else:\n\t                    self.log.info(\n\t                        \"ssh_hook is not provided or invalid. Trying ssh_conn_id to create SSHHook.\"\n", "                    )\n\t                    self.ssh_hook = SSHHook(ssh_conn_id=self.ssh_conn_id,\n\t                                            timeout=self.ssh_timeout,\n\t                                            keepalive_interval=self.keepalive_interval)\n\t            if not self.ssh_hook:\n\t                raise AirflowException(\"Cannot operate without ssh_hook or ssh_conn_id.\")\n\t            with self.ssh_hook.get_conn() as ssh_client:\n\t                self.log.info(\"Running command: %s\", cmd)\n\t                # set timeout taken as params\n\t                stdin, stdout, stderr = ssh_client.exec_command(\n", "                    command=cmd,\n\t                    timeout=self.ssh_timeout\n\t                )\n\t                # get channels\n\t                channel = stdout.channel\n\t                # closing stdin\n\t                stdin.close()\n\t                channel.shutdown_write()\n\t                agg_stdout = b''\n\t                agg_stderr = b''\n", "                # capture any initial output in case channel is closed already\n\t                stdout_buffer_length = len(stdout.channel.in_buffer)\n\t                if stdout_buffer_length > 0:\n\t                    agg_stdout += stdout.channel.recv(stdout_buffer_length)\n\t                # read from both stdout and stderr\n\t                while not channel.closed or channel.recv_ready() or channel.recv_stderr_ready():\n\t                    readq, _, _ = select([channel], [], [], self.ssh_timeout)\n\t                    for recv in readq:\n\t                        if recv.recv_ready():\n\t                            line = stdout.channel.recv(len(recv.in_buffer))\n", "                            agg_stdout += line\n\t                            self.log.info(line.decode('utf-8', 'replace').strip('\\n'))\n\t                        if recv.recv_stderr_ready():\n\t                            line = stderr.channel.recv_stderr(len(recv.in_stderr_buffer))\n\t                            agg_stderr += line\n\t                            self.log.warning(line.decode('utf-8', 'replace').strip('\\n'))\n\t                    if (\n\t                            stdout.channel.exit_status_ready()\n\t                            and not stderr.channel.recv_stderr_ready()\n\t                            and not stdout.channel.recv_ready()\n", "                    ):\n\t                        stdout.channel.shutdown_read()\n\t                        stdout.channel.close()\n\t                        break\n\t                stdout.close()\n\t                stderr.close()\n\t                exit_status = stdout.channel.recv_exit_status()\n\t                if exit_status == 0:\n\t                    enable_pickling = conf.getboolean('core', 'enable_xcom_pickling')\n\t                    if enable_pickling:\n", "                        return agg_stdout\n\t                    else:\n\t                        return b64encode(agg_stdout).decode('utf-8')\n\t                else:\n\t                    error_msg = agg_stderr.decode('utf-8')\n\t                    raise AirflowException(f\"error running cmd: {cmd}, error: {error_msg}\")\n\t        except Exception as e:\n\t            raise AirflowException(f\"SSH operator error: {str(e)}\")\n"]}
{"filename": "operators/__init__.py", "chunked_list": ["\"\"\"\n\tThis module store any custom operators\n\t\"\"\""]}
{"filename": "operators/pg_to_cassandra.py", "chunked_list": ["import pandas as pd\n\tfrom typing import Dict\n\tfrom airflow.exceptions import AirflowException\n\tfrom operators.postgres import PostgresPandasTransformOperator\n\tfrom hooks.cassandra_custom import CassandraCustomHook\n\tclass PostgresToCassandraOperator(PostgresPandasTransformOperator):\n\t    \"\"\"\n\t    Transfer data from postgres to cassandra.\n\t    :param cassandra_conn_id: the connection id of cassandra\n\t    :type cassandra_conn_id: str\n", "    :param cassandra_keyspace: the cassandra keyspace destination\n\t    :type cassandra_keyspace: str\n\t    :param cassandra_table: the cassandra table destination\n\t    :type cassandra_table: str\n\t    :param cassandra_hook_kwargs: the additional parameters of hooks.cassandra.CassandraCustomHook\n\t    :type cassandra_hook_kwargs: dict\n\t    \"\"\"\n\t    ui_color = '#ff007b'\n\t    def __init__(\n\t            self,\n", "            cassandra_conn_id=\"cassandra_default\",\n\t            cassandra_keyspace: str = \"\",\n\t            cassandra_table: str = \"\",\n\t            cassandra_hook_kwargs: Dict = {},\n\t            *args, **kwargs):\n\t        super(PostgresToCassandraOperator, self).__init__(*args, **kwargs)\n\t        self.cassandra_conn_id = cassandra_conn_id\n\t        if not cassandra_keyspace or not cassandra_table:\n\t            raise AirflowException(\"Require cassandra_keyspace and cassandra_table to write data\")\n\t        self.cassandra_keyspace = cassandra_keyspace\n", "        self.cassandra_table = cassandra_table\n\t        self.cassandra_hook_kwargs = cassandra_hook_kwargs\n\t    def _write_dataframe_to_cassandra(self, df: pd.DataFrame, index = None):\n\t        cass_hook = CassandraCustomHook(\n\t            cassandra_conn_id=self.cassandra_conn_id,\n\t            keyspace=self.cassandra_keyspace,\n\t            **self.cassandra_hook_kwargs\n\t            )\n\t        if not cass_hook.table_exists(self.cassandra_table):\n\t            raise AirflowException(f\"Cassandra table {self.cassandra_table} is not exists\")\n", "        if index:\n\t            self.log.info(f\"Writing dataframe {index} to cassandra\")\n\t        cass_hook.insert_dataframe(df, self.cassandra_table, batch_insert_records=200)\n\t    def execute(self, context):\n\t        df = self._pull_postgres_to_pandas()\n\t        result = self._transform_pandas(df)\n\t        # result.replace({np.nan: None}, inplace=True)\n\t        if self.column_map:\n\t            result.rename(columns=self.column_map, inplace=True)\n\t        if isinstance(result, pd.DataFrame):\n", "            self._write_dataframe_to_cassandra(result)\n\t        elif isinstance(result, list):\n\t            for i, _df in enumerate(result):\n\t                self._write_dataframe_to_cassandra(_df, index=i)\n\t        else:\n\t            raise AirflowException(\"Transformer must return a dataframe or list of dataframe\")\n"]}
{"filename": "operators/pandas_to_report.py", "chunked_list": ["import os\n\timport pandas as pd\n\timport pandas.io.sql as psql\n\tfrom operators.pandas_sql import PandasSqlOperator\n\tclass PandasToReportOperator(PandasSqlOperator):\n\t    # template_fields = ('dir_path', 'excel_name', 'run_date', 'delete_sql', )\n\t    def __init__(self,\n\t                dir_path: str,\n\t                excel_name: str,\n\t                # graph_name: str,\n", "                *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.dir_path = dir_path\n\t        self.excel_name = excel_name\n\t        # self.graph_name = graph_name\n\t    def execute(self, context):\n\t        if not os.path.exists(self.dir_path):\n\t            os.makedirs(self.dir_path)\n\t        full_path = \"{}/{}.xlsx\".format(self.dir_path, self.excel_name)\n\t        target_fields, records, df_records = self.get_records()\n", "        self.log.info(\"Executing on %s\", self.run_date)\n\t        if self.delete_sql is not None:\n\t            self.do_delete()\n\t        self.insert_rows(target_fields=target_fields, records=records)\n\t        for output in self.hook.conn.notices:\n\t            self.log.info(output)\n\t        writer = pd.ExcelWriter(full_path, engine='xlsxwriter')\n\t        df_records.to_excel(writer, sheet_name='Sheet1', columns=target_fields, index=False)\n\t        self.log.info(\"Store data in tmp file: {}\".format(full_path))\n\t    def get_records(self):\n", "        if self.run_date is None:\n\t            raise ValueError(\"Missing run_date argument!!\")\n\t        self.log.info(\"**** Run query with execution_date: {}\".format(self.run_date))\n\t        conn = self.hook.get_conn()\n\t        df_records = self.df_transform_function(run_date=self.run_date, conn=conn, **self.op_kwargs)\n\t        target_fields = df_records.columns.tolist()\n\t        self.log.info(\"Target Fields: {}\".format(target_fields))\n\t        records = list(df_records.itertuples(index=False, name=None))\n\t        self.log.info(\"Get first {}\".format(records[0]))\n\t        self.log.info(\"Start loading %s records to %s ...\" % (len(records), self.transferred_table))\n", "        if len(records) < 1:\n\t            raise ValueError(\n\t                \"Data Quality validation FAILED on {} in table {}.{}.\".format(self.run_date,\n\t                                                                            self.schema,\n\t                                                                            self.transferred_table))\n\t        return target_fields, records, df_records\n"]}
{"filename": "operators/hdfs.py", "chunked_list": ["import os\n\tfrom typing import Dict, Optional, Callable\n\tfrom airflow.models import BaseOperator\n\tfrom airflow.exceptions import AirflowException\n\tfrom hooks.hdfs import HDFSHook\n\tclass HDFSException(AirflowException):\n\t    pass\n\tclass HDFSOperator(BaseOperator):\n\t    def __init__(self, command, op_kwargs, hook=HDFSHook, **kwargs):\n\t        super(HDFSOperator, self).__init__(**kwargs)\n", "        self.command = command\n\t        self.op_kwargs = op_kwargs\n\t        self.hook = hook\n\t    def execute(self, context):\n\t        hook = self.hook()\n\t        client = hook.get_conn()\n\t        if self.command == 'put':\n\t            source = self.op_kwargs['source']\n\t            dest_dir = self.op_kwargs['dest_dir']\n\t            if not os.path.exists(source) or not dest_dir:\n", "                print(\"Source: {} isn't existed, error!!!\")\n\t                exit(1)\n\t            else:\n\t                client.makedirs(dest_dir, mode=0o755)\n\t                if os.path.isdir(source):\n\t                    files = os.listdir(source)\n\t                    for file in files:\n\t                        source_file = os.path.join(source, file)\n\t                        client.put(filename=source_file,\n\t                                    path='{}/{}'.format(dest_dir, file))\n", "                else:\n\t                    file = source.split('/')[-1]\n\t                    client.put(filename=source,\n\t                               path='{}/{}'.format(dest_dir, file))\n\t        elif self.command == 'rm':\n\t            dest = self.op_kwargs['dest']\n\t            recursive = self.op_kwargs.get('recursive', True)\n\t            client.rm(path=dest, recursive=recursive)\n\t        elif self.command == 'mv':\n\t            source = self.op_kwargs['source']\n", "            dest = self.op_kwargs['dest']\n\t            client.mv(source, dest)\n\t        else:\n\t            raise HDFSException(\n\t                \"Unsupported HDFS command {}\".format(self.command))\n\tclass PutHDFSOperator(BaseOperator):\n\t    \"\"\"\n\t    This operator using to put local file or folder of data into a HDFS folder\n\t    :param local_source: local absolute path of file or folder \n\t    :type local_source: str\n", "    :param dest_dir: HDFS destination directory\n\t    :type dest_dir: str\n\t    :param file_conf: the configuration about storage file on HDFS include number of replication and blocksize (unit: byte)\n\t    :type file_conf: dict (default: {'replication': 2, 'blocksize': 134217728})\n\t    :param file_filter: a callcable that receive a file_name and return False if want to filter out this file\n\t    :type file_conf: Callable (default: None)\n\t    :param hook: Hook class that this operator based on\n\t    :type hook: cls\n\t    :param hdfs_conn_id: the connection ID of HDFS\n\t    :type hdfs_conn_id: str\n", "    :param hdfs_user: the user do this operator \n\t    :type hdfs_user : str (default: hadoop)\n\t    \"\"\"\n\t    template_fields = ('local_source', 'dest_dir', 'file_conf', 'file_filter')\n\t    def __init__(self,\n\t        local_source: str,\n\t        dest_dir: str,\n\t        file_conf: Optional[Dict] = None,\n\t        file_filter: Optional[Callable] = None,\n\t        hook = HDFSHook,\n", "        hdfs_conn_id: str = \"hdfs_default\",\n\t        hdfs_user: str = \"hadoop\",\n\t        **kwargs):\n\t        super(PutHDFSOperator, self).__init__(**kwargs)\n\t        self.hook = hook\n\t        self.local_source = local_source\n\t        self.dest_dir = dest_dir\n\t        self.file_conf = file_conf\n\t        self.file_filter = file_filter\n\t        self.hdfs_conn_id = hdfs_conn_id\n", "        self.hdfs_user = hdfs_user\n\t    def execute(self, context):\n\t        self.log.info(\"Local files: {}\".format(self.local_source))\n\t        if not self.local_source:\n\t            raise HDFSException('Source must be provided !!!')\n\t        if not os.path.exists(self.local_source):\n\t            raise HDFSException(\n\t                f\"Source {self.local_source} isn't existed !!!\")\n\t        if not self.dest_dir:\n\t            raise HDFSException('Dest dir must be provided !!!')\n", "        hook = self.hook(hdfs_conn_id=self.hdfs_conn_id, hdfs_user=self.hdfs_user)\n\t        self.client = hook.get_conn()\n\t        self.file_conf = self.file_conf if self.file_conf is not None else hook.get_file_conf()\n\t        PutHDFSOperator._copyObjToDir(self.local_source, self.dest_dir, self.client, self.file_conf, self.file_filter)\n\t    @staticmethod\n\t    def _copyObjToDir(local_obj, hdfs_dir, client, file_conf, file_filter):\n\t        if os.path.isdir(local_obj):\n\t            PutHDFSOperator._copyDirToDir(local_obj, hdfs_dir, client, file_conf, file_filter)\n\t        else:\n\t            PutHDFSOperator._copyFileIntoDir(local_obj, hdfs_dir, client, file_conf, file_filter)\n", "    @staticmethod\n\t    def _copyDirToDir(local_dir, hdfs_dir, client, file_conf, file_filter):\n\t        for o in os.listdir(local_dir):\n\t            sub_local_obj = os.path.join(local_dir, o)\n\t            if os.path.isdir(sub_local_obj):\n\t                sub_hdfs_dir = os.path.join(hdfs_dir, o)\n\t                PutHDFSOperator._copyDirToDir(sub_local_obj, sub_hdfs_dir, client, file_conf, file_filter)\n\t            else:\n\t                PutHDFSOperator._copyFileIntoDir(sub_local_obj, hdfs_dir, client, file_conf, file_filter)\n\t    @staticmethod\n", "    def _copyFileIntoDir(local_file, hdfs_dir, client, file_conf, file_filter):\n\t        file_name = local_file.split('/')[-1]\n\t        replication = file_conf.get('replication', 2)\n\t        block_size = file_conf.get('blocksize', 134217728)\n\t        hdfs_path = os.path.join(hdfs_dir, file_name)\n\t        if file_filter is None or file_filter(file_name):\n\t            client.makedirs(hdfs_dir, mode=0o755)\n\t            client.put(filename=local_file,\n\t                            path=hdfs_path,\n\t                            replication=replication,\n", "                            block_size=block_size)\n\t            print(f'Put {local_file} to {hdfs_path}')\n\tclass RmHDFSOperator(BaseOperator):\n\t    \"\"\"\n\t    This operator using to remove a HDFS file or folder\n\t    :param path: HDFS file or folder path you want to remove\n\t    :type path: str\n\t    \"\"\"\n\t    template_fields = ('path',)\n\t    def __init__(self, path: str, hook=HDFSHook, hdfs_conn_id: str = 'hdfs_default', **kwargs):\n", "        super(RmHDFSOperator, self).__init__(**kwargs)\n\t        self.hook = hook\n\t        self.path = path\n\t        self.hdfs_conn_id = hdfs_conn_id\n\t    @staticmethod\n\t    def _remove(client, path):\n\t        client.rm(path, recursive=True)\n\t    def execute(self, context):\n\t        if not self.path:\n\t            raise HDFSException('Path to remove must be provided !!!')\n", "        hook = self.hook(hdfs_conn_id=self.hdfs_conn_id)\n\t        client = hook.get_conn()\n\t        try:\n\t            RmHDFSOperator._remove(client, self.path)\n\t            self.log.info(\" ***** Removed hdfs files {}\".format(self.path))\n\t        except:\n\t            self.log.warning(\"File or folder not found: {}\".format(self.path))\n\tclass GetHDFSOperator(BaseOperator):\n\t    \"\"\"\n\t    This operator using to get a HDFS file or folder to local \n", "    :param hdfs_path: HDFS path for copying from\n\t    :type hdfs_path: str\n\t    :param local_path: local path for moving to\n\t    :type local_path: str\n\t    \"\"\"\n\t    template_fields = ('hdfs_source', 'dest_dir')\n\t    def __init__(self,\n\t        hdfs_source: str,\n\t        dest_dir: str,\n\t        file_filter: Optional[Callable] = None,\n", "        hook=HDFSHook,\n\t        hdfs_conn_id: str = 'hdfs_default',\n\t        **kwargs):\n\t        super(GetHDFSOperator, self).__init__(**kwargs)\n\t        self.hook = hook\n\t        self.hdfs_source = hdfs_source\n\t        self.dest_dir = dest_dir\n\t        self.file_filter = file_filter\n\t        self.hdfs_conn_id = hdfs_conn_id\n\t    def execute(self, context):\n", "        hook = self.hook(hdfs_conn_id=self.hdfs_conn_id)\n\t        self.client = hook.get_conn()\n\t        self.log.info(\"HDFS source: {}\".format(self.hdfs_source))\n\t        if not self.hdfs_source:\n\t            raise HDFSException('Source must be provided !!!')\n\t        if not self.client.exists(self.hdfs_source):\n\t            raise HDFSException(\n\t                f\"Source {self.hdfs_source} isn't existed !!!\")\n\t        if not self.dest_dir:\n\t            raise HDFSException('Dest dir must be provided !!!')\n", "        GetHDFSOperator._copyObjToDir(self.hdfs_source, self.dest_dir, self.client, self.file_filter)\n\t    @staticmethod\n\t    def _copyObjToDir(hdfs_obj, local_dir, client, file_filter):\n\t        if client.isdir(hdfs_obj):\n\t            GetHDFSOperator._copyDirToDir(hdfs_obj, local_dir, client, file_filter)\n\t        else:\n\t            GetHDFSOperator._copyFileIntoDir(hdfs_obj, local_dir, client, file_filter)\n\t    @staticmethod\n\t    def _copyDirToDir(hdfs_dir, local_dir, client, file_filter):\n\t        for sub_hdfs_obj in client.ls(hdfs_dir):\n", "            if client.isdir(sub_hdfs_obj):\n\t                sub_local_dir = os.path.join(local_dir, sub_hdfs_obj.split(\"/\")[-1])\n\t                GetHDFSOperator._copyDirToDir(sub_hdfs_obj, sub_local_dir, client, file_filter)\n\t            else:\n\t                GetHDFSOperator._copyFileIntoDir(sub_hdfs_obj, local_dir, client, file_filter)\n\t    @staticmethod\n\t    def _copyFileIntoDir(hdfs_file, local_dir, client, file_filter):\n\t        file_name = hdfs_file.split('/')[-1]\n\t        local_path = os.path.join(local_dir, file_name)\n\t        if file_filter is None or file_filter(file_name):\n", "            if not os.path.exists(local_dir):\n\t                os.makedirs(local_dir, mode=0o755)\n\t            client.get(hdfs_path=hdfs_file,\n\t                        local_path=local_path)\n\t            print(f'Get {hdfs_file} to {local_path}')\n"]}
{"filename": "operators/data_quality.py", "chunked_list": ["from airflow.providers.postgres.hooks.postgres import PostgresHook\n\tfrom airflow.models import BaseOperator\n\tclass DataQualityOperator(BaseOperator):\n\t    template_fields = ('run_date',)\n\t    ui_color = '#89DA59'\n\t    def __init__(self,\n\t                sql_builder: callable,\n\t                schema='public',\n\t                conn_id=\"\",\n\t                tables=[],\n", "                run_date: str = '',\n\t                *args, **kwargs):\n\t        super(DataQualityOperator, self).__init__(*args, **kwargs)\n\t        self.conn_id = conn_id\n\t        self.tables = tables\n\t        self.run_date = run_date\n\t        self.sql_builder = sql_builder\n\t        self.schema = schema\n\t    def execute(self, context):\n\t        redshift_hook = PostgresHook(postgres_conn_id=self.conn_id)\n", "        for table in self.tables:\n\t            self.log.info(\"Starting data quality validation on table : {}\".format(table))\n\t            sql = self.sql_builder(self.run_date, self.schema, table)\n\t            records = redshift_hook.get_records(sql)\n\t            if len(records) < 1 or len(records[0]) < 1 or records[0][0] < 1:\n\t                self.log.error(\n\t                    \"Data Quality validation failed on {} in table {}.{}.\".format(self.run_date, self.schema, table))\n\t                raise ValueError(\n\t                    \"Data Quality validation failed on {} in table {}.{}.\".format(self.run_date, self.schema, table))\n\t            self.log.info(\n", "                \"Data Quality validation failed on {} in table {}.{}.\".format(self.run_date, self.schema, table))\n"]}
{"filename": "operators/pandas_sql.py", "chunked_list": ["import pendulum\n\tfrom operators.pg_transform import PostgresTransformOperator\n\tlocal_tz = pendulum.timezone('Asia/Ho_Chi_Minh')\n\tclass PandasSqlOperator(PostgresTransformOperator):\n\t    ui_color = '#12CCFE'\n\t    def __init__(\n\t            self,\n\t            df_transform_function: callable,\n\t            op_kwargs: dict = {},\n\t            *args, **kwargs):\n", "        super(PandasSqlOperator, self).__init__(*args, **kwargs)\n\t        self.df_transform_function = df_transform_function\n\t        self.op_kwargs = op_kwargs\n\t    def get_records(self):\n\t        if self.run_date is None:\n\t            raise ValueError(\"Missing run_date argument!!\")\n\t        self.log.info(\"**** Run query with execution_date: {}\".format(self.run_date))\n\t        conn = self.hook.get_conn()\n\t        df_records = self.df_transform_function(run_date=self.run_date, conn=conn, **self.op_kwargs)\n\t        target_fields = df_records.columns.tolist()\n", "        self.log.info(\"Target Fields: {}\".format(target_fields))\n\t        records = list(df_records.itertuples(index=False, name=None))\n\t        del df_records\n\t        if len(records) < 1:\n\t            raise ValueError(\n\t                \"Data Quality validation FAILED on {} in table {}.{}.\".format(self.run_date,\n\t                                                                            self.schema,\n\t                                                                            self.transferred_table))\n\t        else:\n\t            self.log.info(\"Get first {}\".format(records[0]))\n", "            self.log.info(\"Start loading %s records to %s ...\" % (len(records), self.transferred_table))\n\t        return target_fields, records\n"]}
{"filename": "operators/gg_bigquery.py", "chunked_list": ["import os\n\tfrom datetime import timedelta\n\tfrom google.cloud import bigquery\n\tfrom airflow.models import BaseOperator\n\tfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\n\tfrom hooks.postgres_custom import PostgresCustomHook\n\tfrom operators.pg_transform import PostgresTransformOperator\n\tclass BigQueryPandasToPostgresOperator(PostgresTransformOperator):\n\t    ui_color = '#11BCFE'\n\t    def __init__(\n", "            self,\n\t            bigquery_conn_id='bigquery_default',\n\t            *args, **kwargs):\n\t        super(BigQueryPandasToPostgresOperator, self).__init__(*args, **kwargs)\n\t        self.bigquery_conn_id = bigquery_conn_id\n\t    def get_records(self):\n\t        self.bq_hook = BigQueryHook(gcp_conn_id=self.bigquery_conn_id, use_legacy_sql=False)\n\t        bq_client = bigquery.Client(project=self.bq_hook._get_field(\"project\"),\n\t                                    credentials=self.bq_hook._get_credentials())\n\t        if self.run_date is None:\n", "            raise ValueError(\"Missing run_date argument!!\")\n\t        sql = self.sql_builder(self.run_date)\n\t        self.log.info('Executing: %s', sql)\n\t        df_records = bq_client.query(sql).to_dataframe()\n\t        target_fields = df_records.columns.tolist()\n\t        self.log.info(\"Target Fields: {}\".format(target_fields))\n\t        records = list(df_records.itertuples(index=False, name=None))\n\t        self.log.info(\"Get first {}\".format(records[0]))\n\t        self.log.info(\"Start loading %s records to %s ...\" % (len(records), self.transferred_table))\n\t        if len(records) < 1:\n", "            raise ValueError(\n\t                \"Data Quality validation FAILED on {} in table {}.{}.\".format(self.run_date,\n\t                                                                                self.schema,\n\t                                                                                self.transferred_table))\n\t        return target_fields, records\n\tclass BigQueryPandas2FileOperator(BaseOperator):\n\t    template_fields = ('run_date',)\n\t    ui_color = '#21BFFE'\n\t    def __init__(self, bigquery_conn_id,\n\t                store_dir,\n", "                run_date,\n\t                sql_builder,\n\t                file_format=\"csv\",\n\t                 *args, **kwargs):\n\t        super(BigQueryPandas2FileOperator, self).__init__(*args, **kwargs)\n\t        self.store_dir = store_dir\n\t        self.bigquery_conn_id = bigquery_conn_id\n\t        self.run_date = run_date\n\t        self.sql_builder = sql_builder\n\t        self.file_format = file_format\n", "    def get_records(self):\n\t        self.bq_hook = BigQueryHook(gcp_conn_id=self.bigquery_conn_id, use_legacy_sql=False)\n\t        bq_client = bigquery.Client(project=self.bq_hook._get_field(\"project\"),\n\t                                    credentials=self.bq_hook._get_credentials())\n\t        if self.run_date is None:\n\t            raise ValueError(\"Missing run_date argument!!\")\n\t        sql = self.sql_builder(self.run_date)\n\t        self.log.info('Executing: %s', sql)\n\t        df_records = bq_client.query(sql).to_dataframe()\n\t        self.log.info(\"df with shape: {}\".format(df_records.shape))\n", "        target_fields = df_records.columns.tolist()\n\t        self.log.info(\"Target Fields: {}\".format(target_fields))\n\t        if df_records.shape[0] < 1:\n\t            raise ValueError(\"Data Quality validation FAILED!!\")\n\t        return target_fields, df_records\n\t    def execute(self, context):\n\t        target_fields, df_records = self.get_records()\n\t        self.log.info(\"Executing on %s\", self.run_date)\n\t        if not os.path.exists(self.store_dir):\n\t            os.makedirs(self.store_dir)\n", "        save_path = f\"{self.store_dir}/{self.run_date}.{self.file_format}\"\n\t        self.log.info(\" *** Saved file to local path: {}\".format(save_path))\n\t        if self.file_format == \"csv\":\n\t            df_records.to_csv(save_path, index=False)\n\t        elif self.file_format == \"parquet\":\n\t            df_records.to_parquet(save_path, index=False, engine='pyarrow')\n\t        else:\n\t            raise ValueError(\"Please specify file format either csv or parquet\")\n\tclass BigqueryTransformPostgreOperator(BaseOperator):\n\t    \"\"\"\n", "    author: phuong.nguyen.huu@vn\n\t    \"\"\"\n\t    template_fields = ('run_date',)\n\t    ui_color = '#33CBFE'\n\t    def __init__(\n\t            self,\n\t            target_fields: list,\n\t            transferred_table: str,\n\t            replace: bool,\n\t            is_update: bool,\n", "            # schema: str = '',\n\t            # self.validator = validator,\n\t            database: str = None,\n\t            run_date: str = None,\n\t            sql_builder: callable = None,\n\t            postgres_conn_id: str = 'postgres_default',\n\t            bigquery_conn_id: str = 'bigquery_default',\n\t            use_legacy_sql: bool = False,\n\t            commit_every: int = 5000,\n\t            replace_index: list = None,\n", "            sum_index: list = [],\n\t            coalesces=None,\n\t            *args, **kwargs):\n\t        super(BigqueryTransformPostgreOperator, self).__init__(*args, **kwargs)\n\t        self.target_fields = target_fields\n\t        self.sql_builder = sql_builder\n\t        self.transferred_table = transferred_table\n\t        self.replace = replace\n\t        self.replace_index = replace_index\n\t        self.is_update = is_update\n", "        self.sum_index = sum_index\n\t        self.commit_every = commit_every\n\t        self.database = database\n\t        self.run_date = run_date\n\t        self.coalesces = coalesces\n\t        self.postgres_conn_id = postgres_conn_id\n\t        self.bigquery_conn_id = bigquery_conn_id\n\t        self.use_legacy_sql = use_legacy_sql\n\t        self.postgres_hook = PostgresCustomHook(postgres_conn_id=self.postgres_conn_id, schema=self.database)\n\t        self.bigquery_hook = BigQueryHook(bigquery_conn_id=self.bigquery_conn_id, use_legacy_sql=self.use_legacy_sql)\n", "    def execute(self, context):\n\t        if context['dag_run'].conf is not None:\n\t            if context['dag_run'].conf.get('run_date') is not None:\n\t                self.run_date = context['dag_run'].conf.get('run_date')\n\t                self.log.info(\"**** trigger by externally dags with run_date: {}\".format(self.run_date))\n\t        if self.run_date is None or self.run_date == 'None':\n\t            self.run_date = (context['dag_run'].execution_date + timedelta(hours=7)).strftime('%Y-%m-%d')\n\t        self.log.info('**** Run query with execution date: {}'.format(self.run_date))\n\t        self.log.dedug('**** Executing {}'.format(self.sql_builder))\n\t        sql = self.sql_builder(self.run_date)\n", "        df_records = self.bigquery_hook.get_pandas_df(sql)\n\t        records = list(df_records.itertuples(index=False, name=None))\n\t        if len(records) > 0:\n\t            self.log.info(\"Start loading {} records to {} ...\".format(len(records), self.transferred_table))\n\t            self.insert_data_to_postgres(records)\n\t        else:\n\t            self.log.warn(\"Zero records insert to table {}\".format(self.transferred_table))\n\t    def insert_data_to_postgres(self, records):\n\t        self.hook.insert_rows(table=self.transferred_table,\n\t                                rows=records,\n", "                                target_fields=self.target_fields,\n\t                                replace=self.replace,\n\t                                replace_index=self.replace_index,\n\t                                is_update=self.is_update,\n\t                                sum_index=self.sum_index,\n\t                                commit_every=self.commit_every,\n\t                                coalesces=self.coalesces)\n"]}
{"filename": "operators/elasticsearch_parquet.py", "chunked_list": ["import json\n\timport os\n\timport time\n\timport pandas as pd\n\tfrom airflow.models.baseoperator import BaseOperator\n\tfrom airflow.utils.operator_helpers import context_to_airflow_vars\n\tfrom elasticsearch.helpers import ScanError\n\tfrom hooks.es_hook import ElasticSearchHook\n\tfrom utils.parse_date import create_path_from_date_time\n\tclass ES2ParquetOperator(BaseOperator):\n", "    template_fields = ('op_kwargs',)\n\t    ui_color = '#33BBFE'\n\t    def __init__(self,\n\t                parse_function: callable,\n\t                query_builder: callable,\n\t                es_conn_id: str = \"\",\n\t                es_index: str = \"\",\n\t                source: list = [],\n\t                scroll: str = \"1m\",\n\t                chunk_size: int = 30,\n", "                size: int = 10000,\n\t                path_to_store: str = \"\",\n\t                op_kwargs: dict = {},\n\t                extension: str = \"gzip\",\n\t                force_str_cols=[],\n\t                 *args, **kwargs):\n\t        super(ES2ParquetOperator, self).__init__(*args, **kwargs)\n\t        self.es_conn_id = es_conn_id\n\t        self.index = es_index\n\t        self.source = source\n", "        self.parse_function = parse_function\n\t        self.query_builder = query_builder\n\t        self.scroll = scroll\n\t        self.local_storage = path_to_store\n\t        self.chunk_size = chunk_size\n\t        self.size = size\n\t        self.op_kwargs = op_kwargs\n\t        self.extension = extension\n\t        self.force_str_cols = force_str_cols\n\t        if not os.path.exists(self.local_storage):\n", "            os.makedirs(self.local_storage)\n\t    def execute(self, context):\n\t        airflow_context_vars = context_to_airflow_vars(context, in_env_var_format=True)\n\t        self.log.debug(\"Exporting the following env vars:\\n%s\",\n\t                        '\\n'.join([\"{}={}\".format(k, v)\n\t                                for k, v in airflow_context_vars.items()]))\n\t        os.environ.update(airflow_context_vars)\n\t        es_hook = ElasticSearchHook(conn_id=self.es_conn_id)\n\t        es_client = es_hook.get_conn()\n\t        lucene_query = self.query_builder(**self.op_kwargs)\n", "        self.log.info(\n\t            \"Run elastic search query of index {}: \\n{}\".format(self.index, json.dumps(lucene_query, indent=2)))\n\t        start_time = time.time()\n\t        self.scan(client=es_client,\n\t                    scroll=self.scroll,\n\t                    query=lucene_query,\n\t                    index=self.index,\n\t                    size=self.size,\n\t                    raise_on_error=False)\n\t        self.log.info(\"Process after {}s\".format(time.time() - start_time))\n", "    def _to_parquet(self, store_dir, file_name, result_list):\n\t        self.log.info(\" *** Saving data to parquet file ...\")\n\t        df = pd.DataFrame(result_list)\n\t        if not os.path.exists(store_dir):\n\t            os.makedirs(store_dir)\n\t        save_path = \"{}/{}.parquet.{}\".format(store_dir, file_name, self.extension)\n\t        self.log.info(\" *** Saved file to local path: {}\".format(save_path))\n\t        for f in self.force_str_cols:\n\t            df[f] = df[f].astype(str)\n\t        df.to_parquet(save_path, index=False, engine='pyarrow', compression=self.extension)\n", "    def scan(self,\n\t            client,\n\t            query=None,\n\t            scroll=\"5m\",\n\t            raise_on_error=True,\n\t            preserve_order=False,\n\t            size=10000,\n\t            request_timeout=None,\n\t            clear_scroll=True,\n\t            scroll_kwargs=None,\n", "            **kwargs):\n\t        \"\"\"\n\t        Simple abstraction on top of the\n\t        :meth:`~elasticsearch.Elasticsearch.scroll` api - a simple iterator that\n\t        yields all hits as returned by underlining scroll requests.\n\t        By default scan does not return results in any pre-determined order. To\n\t        have a standard order in the returned documents (either by score or\n\t        explicit sort definition) when scrolling, use ``preserve_order=True``. This\n\t        may be an expensive operation and will negate the performance benefits of\n\t        using ``scan``.\n", "        :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n\t        :arg query: body for the :meth:`~elasticsearch.Elasticsearch.search` api\n\t        :arg scroll: Specify how long a consistent view of the index should be\n\t            maintained for scrolled search\n\t        :arg raise_on_error: raises an exception (``ScanError``) if an error is\n\t            encountered (some shards fail to execute). By default we raise.\n\t        :arg preserve_order: don't set the ``search_type`` to ``scan`` - this will\n\t            cause the scroll to paginate with preserving the order. Note that this\n\t            can be an extremely expensive operation and can easily lead to\n\t            unpredictable results, use with caution.\n", "        :arg size: size (per shard) of the batch send at each iteration.\n\t        :arg request_timeout: explicit timeout for each call to ``scan``\n\t        :arg clear_scroll: explicitly calls delete on the scroll id via the clear\n\t            scroll API at the end of the method on completion or error, defaults\n\t            to true.\n\t        :arg scroll_kwargs: additional kwargs to be passed to\n\t            :meth:`~elasticsearch.Elasticsearch.scroll`\n\t        Any additional keyword arguments will be passed to the initial\n\t        :meth:`~elasticsearch.Elasticsearch.search` call::\n\t            scan(es,\n", "                query={\"query\": {\"match\": {\"title\": \"python\"}}},\n\t                index=\"orders-*\",\n\t                doc_type=\"books\"\n\t            )\n\t        \"\"\"\n\t        scroll_kwargs = scroll_kwargs or {}\n\t        if not preserve_order:\n\t            query = query.copy() if query else {}\n\t            query[\"sort\"] = \"_doc\"\n\t        # initial search\n", "        resp = client.search(\n\t            body=query, scroll=scroll, size=size, request_timeout=request_timeout, **kwargs\n\t        )\n\t        scroll_id = resp.get(\"_scroll_id\")\n\t        no_returned_res = resp[\"hits\"][\"total\"][\"value\"]\n\t        self.log.info(\"Got {} total documents\".format(no_returned_res))\n\t        no_loops = no_returned_res // self.size + 1\n\t        self.log.info(\"Number of scroll loops: {}\".format(no_loops))\n\t        scroll_times = 0\n\t        results = list()\n", "        number = 0\n\t        no_actual_res = 0\n\t        try:\n\t            path_dir, file_name = create_path_from_date_time(self.op_kwargs.get('run_date'))\n\t            store_dir = os.path.join(self.local_storage, path_dir)\n\t            while scroll_id and resp[\"hits\"][\"hits\"]:\n\t                scroll_times += 1\n\t                for hit in resp[\"hits\"][\"hits\"]:\n\t                    r = self.parse_function(hit)\n\t                    if r is not None:\n", "                        results.append(r)\n\t                if scroll_times % self.chunk_size == 0:\n\t                    no_actual_res += len(results)\n\t                    self._to_parquet(store_dir, file_name + \"_\" + str(number), results)\n\t                    number += 1\n\t                    del results\n\t                    results = list()\n\t                if (resp[\"_shards\"][\"successful\"] + resp[\"_shards\"][\"skipped\"]) < resp[\n\t                    \"_shards\"\n\t                ][\"total\"]:\n", "                    self.log.warning(\n\t                        \"Scroll request has only succeeded on %d (+%d skipped) shards out of %d.\",\n\t                        resp[\"_shards\"][\"successful\"],\n\t                        resp[\"_shards\"][\"skipped\"],\n\t                        resp[\"_shards\"][\"total\"],\n\t                    )\n\t                    if raise_on_error:\n\t                        raise ScanError(\n\t                            scroll_id,\n\t                            \"Scroll request has only succeeded on %d (+%d skiped) shards out of %d.\"\n", "                            % (\n\t                                resp[\"_shards\"][\"successful\"],\n\t                                resp[\"_shards\"][\"skipped\"],\n\t                                resp[\"_shards\"][\"total\"],\n\t                            ),\n\t                        )\n\t                resp = client.scroll(\n\t                    body={\"scroll_id\": scroll_id, \"scroll\": scroll}, **scroll_kwargs\n\t                )\n\t                scroll_id = resp.get(\"_scroll_id\")\n", "            if len(results) > 0:\n\t                no_actual_res += len(results)\n\t                self._to_parquet(store_dir,\n\t                                    file_name + \"_\" + str(number),\n\t                                    results)\n\t            self.log.info(\"Number of scroll times: {}\".format(scroll_times))\n\t            self.log.info(\"Get number of {}/{} results\".format(no_actual_res, no_returned_res))\n\t        finally:\n\t            if scroll_id and clear_scroll:\n\t                client.clear_scroll(body={\"scroll_id\": [scroll_id]}, ignore=(404,))\n"]}
{"filename": "utils/os_helper.py", "chunked_list": ["import os\n\timport shutil\n\tfrom glob import glob\n\tdef listdir(path, recursive=False):\n\t    return glob(path, recursive)\n\tdef make_new_folder(folder):\n\t    if os.path.exists(folder):\n\t        shutil.rmtree(folder)\n\t    os.makedirs(folder)\n\tdef clear_fs_obj(obj):\n", "    if os.path.isdir(obj):\n\t        shutil.rmtree(obj)\n\t    else:\n\t        os.remove(obj)\n"]}
{"filename": "utils/jsonutil.py", "chunked_list": ["import re\n\timport json\n\tfrom typing import Callable, Dict, Union\n\tconvert_to_snake_case = lambda name: re.sub(\n\t    '_+', '_', re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)).lower()\n\tdef json_flatten(\n\t        data: Union[Dict, str],\n\t        col_normalize: Callable = lambda c: c,\n\t        sep: str = \"_\"):\n\t    js = json.loads(data) if type(data) is str else data\n", "    record = {}\n\t    def flatten_rec(k, v):\n\t        if type(v) is not dict:\n\t            record[k] = v\n\t        else:\n\t            for sk, sv in v.items():\n\t                flatten_rec(sep.join([col_normalize(k), col_normalize(sk)]), sv)\n\t    for k, v in js.items():\n\t        flatten_rec(k, v)\n\t    return record\n"]}
{"filename": "utils/parse_date.py", "chunked_list": ["from datetime import datetime\n\tutc_iso_format = '%Y-%m-%dT%H:%M:%S%z'\n\tdef parse_hour_date_from_utc_format(date_time: str) -> (str, int):\n\t    converted = convert_to_datetime_from_utc(date_time)\n\t    date_str = converted.date().strftime('%Y-%m-%d')\n\t    return date_str\n\tdef parse_year_month_day_hour_from_utc(date_time: str) -> (int, int, int):\n\t    converted = convert_to_datetime_from_utc(date_time)\n\t    return converted.year, converted.month, converted.day, converted.hour\n\tdef change_format_date(date_time: str) -> str:\n", "    date, hour = parse_hour_date_from_utc_format(date_time)\n\t    return \"{}T{:0>2}-00-00\".format(date, hour)\n\tdef covert_format_date_from_utc(date_time: datetime, new_format='%Y%m%dT%H%M%S'):\n\t    date_str = date_time.strftime(new_format)\n\t    return date_str\n\tdef convert_to_datetime_from_utc(date_time: str, date_format='%Y-%m-%dT%H:%M:%S%z'):\n\t    try:\n\t        converted = datetime.strptime(date_time, date_format)\n\t    except:\n\t        converted = datetime.strptime(date_time, '%Y-%m-%dT%H:%M:%S.%f%z')\n", "    return converted\n\tdef create_path_from_date_time(date_time: str):\n\t    year, month, day, hour = parse_year_month_day_hour_from_utc(date_time)\n\t    return \"{}/{:0>2}/{:0>2}\".format(year, month, day), \"{:0>2}-00-00\".format(hour)\n"]}
{"filename": "utils/common.py", "chunked_list": ["import time\n\timport string\n\timport random\n\timport importlib\n\tfrom typing import Dict, List\n\tfrom itertools import islice\n\tdef sub_dict(d: Dict, included_keys: List[str] = []) -> Dict:\n\t    return {k:v for k,v in d.items() if k in included_keys}\n\tdef println(msg, new_line=1):\n\t    for _ in range(new_line):\n", "        print()\n\t    print(msg)\n\tdef chunk(it, size):\n\t    it = iter(it)\n\t    return iter(lambda: list(islice(it, size)), [])\n\tdef timing(func):\n\t    def wrapper(*args, **kwargs):\n\t        start_time = time.time()\n\t        returned_value = func(*args, **kwargs)\n\t        print(f\"Took {time.time() - start_time} seconds\")\n", "        return returned_value\n\t    return wrapper\n\tdef rand_str(length=10):\n\t    return ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(length))\n\tdef get_func_from_name(function_string: str):\n\t    mod_name, func_name = function_string.rsplit('.',1)\n\t    mod = importlib.import_module(mod_name)\n\t    func = getattr(mod, func_name)\n\t    return func\n"]}
{"filename": "utils/python_callable.py", "chunked_list": ["from operators.ms_teams_webhook import MSTeamsWebhookOperator\n\tfrom datetime import datetime, timedelta\n\tdef on_failure(context, **kwargs):\n\t    owner = context['dag'].default_args['owner']\n\t    message = f\"\"\"&#x1F4A9; &#x1F4A9; &#x1F4A9; &#x1F4A9; <strong>{owner}</strong>\"\"\"\n\t    teams_notification = MSTeamsWebhookOperator(\n\t        status=\"FAILED\",\n\t        task_id=\"msteams_notify_failure\",\n\t        owner=f'{owner}',\n\t        trigger_rule=\"all_done\",\n", "        message=message,\n\t        button_text=\"View log\",\n\t        theme_color=\"FF0000\",\n\t        http_conn_id='ms_team_conn_failure')\n\t    teams_notification.execute(context)\n\tdef on_success(context, **kwargs):\n\t    owner = context['dag'].default_args['owner']\n\t    message = f\"\"\"A may ding, gut chop &#x1F49E; &#x1F49E; <strong>{owner}</strong>\"\"\"\n\t    teams_notification = MSTeamsWebhookOperator(\n\t        status=\"SUCCESS\",\n", "        task_id=\"msteams_notify_success\",\n\t        owner=f'{owner}',\n\t        trigger_rule=\"all_done\",\n\t        message=message,\n\t        button_text=\"View log\",\n\t        theme_color=\"0072C6\",\n\t        http_conn_id='ms_team_conn_success')\n\t    teams_notification.execute(context)\n\tdef conditionally_trigger(context, dag_run_obj):\n\t    \"\"\"This function decides whether or not to Trigger the remote DAG\"\"\"\n", "    c_p = context['params']['condition_param']\n\t    print(\"Controller DAG : conditionally_trigger = {}\".format(c_p))\n\t    if c_p:\n\t        consistent = context['params'].get('consistent_run_date', True)  # set default as True\n\t        if consistent:\n\t            run_date = context['dag_run'].conf.get('run_date')\n\t        else:\n\t            run_date = (context['dag_run'].execution_date + timedelta(hours=7)).strftime(\"%Y-%m-%d\")\n\t        dag_run_obj.payload = {'message': context['params']['message'],\n\t                               'run_date': run_date,\n", "                               \"dag_controller_id\": context['dag_run'].dag_id,\n\t                               \"task_controller_id\": context['ti'].task_id}\n\t        print(dag_run_obj.payload)\n\t        return dag_run_obj\n\tdef receive_trigger_payload(ds, **kwargs):\n\t    \"\"\"\n\t    Print the payload \"message\" passed to the DagRun conf attribute.\n\t    :param context: The execution context\n\t    :type context: dict\n\t    \"\"\"\n", "    print(\"Received trigger from task {task} in dag {dag} on {run_date}\".format(\n\t        dag=kwargs[\"dag_run\"].conf.get('dag_controller_id', ''),\n\t        task=kwargs[\"dag_run\"].conf.get('task_controller_id', ''),\n\t        run_date=kwargs[\"dag_run\"].conf.get(\"run_date\", None)\n\t    ))\n"]}
{"filename": "sensors/__init__.py", "chunked_list": []}
{"filename": "sensors/sftp.py", "chunked_list": ["from typing import Optional\n\tfrom paramiko import SFTP_NO_SUCH_FILE\n\tfrom airflow.providers.sftp.hooks.sftp import SFTPHook\n\tfrom airflow.sensors.base import BaseSensorOperator\n\tclass SFTPMultipleFilesSensor(BaseSensorOperator):\n\t    \"\"\"\n\t    Waits for a file or directory to be present on SFTP.\n\t    :param path: Remote file or directory path\n\t    :type path: str\n\t    :param sftp_conn_id: The connection to run the sensor against\n", "    :type sftp_conn_id: str\n\t    \"\"\"\n\t    template_fields = (\"paths\",)\n\t    def __init__(self, *, paths: list, sftp_conn_id: str = \"sftp_default\", **kwargs) -> None:\n\t        super().__init__(**kwargs)\n\t        self.paths = paths\n\t        self.hook: Optional[SFTPHook] = None\n\t        self.sftp_conn_id = sftp_conn_id\n\t    def poke(self, context: dict) -> bool:\n\t        self.hook = SFTPHook(self.sftp_conn_id)\n", "        self.log.info(\"Poking for %s\", self.paths)\n\t        missed_files = []\n\t        existing_file = []\n\t        for p in self.paths:\n\t            try:\n\t                mod_time = self.hook.get_mod_time(p)\n\t                self.log.info(\"Found File %s last modified: %s\", str(p), str(mod_time))\n\t                existing_file.append(p)\n\t            except OSError as e:\n\t                if e.errno != SFTP_NO_SUCH_FILE:\n", "                    raise e\n\t                missed_files.append(p)\n\t        files_info = {\"missing_files\": missed_files, \"existing_files\": existing_file}\n\t        self.xcom_push(context, key=\"files_info\", value=files_info)\n\t        if len(missed_files) > 0:\n\t            self.log.info(\"Missing files: {}\".format(missed_files))\n\t            return False\n\t        self.hook.close_conn()\n\t        return True"]}
{"filename": "hooks/appsflyer.py", "chunked_list": ["from typing import Any, Dict\n\tfrom airflow import AirflowException\n\tfrom airflow.hooks.base import BaseHook\n\tclass AppsflyerHook(BaseHook):\n\t    conn_name_attr = 'appsflyer_conn_id'\n\t    default_conn_name = 'appsflyer_report_conn'\n\t    conn_type = 'appsflyer'\n\t    hook_name = 'Appsflyer'\n\t    @staticmethod\n\t    def get_ui_field_behaviour() -> Dict[str, Any]:\n", "        \"\"\"Returns custom field behaviour\"\"\"\n\t        return {\n\t            \"hidden_fields\": [\"host\", \"schema\", \"port\"],\n\t            \"relabeling\": {\n\t                \"login\": 'App IDs',\n\t                \"password\": \"Api Token\"\n\t            },\n\t        }\n\t    def __init__(self, appsflyer_conn_id: str = default_conn_name):\n\t        self.appsflyer_conn_id = appsflyer_conn_id\n", "        conn = self.get_connection(self.appsflyer_conn_id)\n\t        if not conn.password:\n\t            raise AirflowException(\"Must provide api token\")\n\t        self.__app_ids = conn.login.split(\",\")\n\t        self.__api_token = conn.password\n\t        self.__timezone = conn.extra_dejson.get(\"timezone\", \"Asia/Ho_Chi_Minh\")\n\t    def get_conn(self) -> Dict[str, Any]:\n\t        return {\n\t            \"app_ids\": self.__app_ids,\n\t            \"api_token\": self.__api_token,\n", "            \"timezone\": self.__timezone\n\t        }\n"]}
{"filename": "hooks/druid_custom.py", "chunked_list": ["import json\n\timport requests\n\tfrom furl import furl\n\tfrom airflow.providers.apache.druid.hooks.druid import DruidHook\n\tfrom airflow.exceptions import AirflowException\n\tclass DruidCustomHook(DruidHook):\n\t    \"\"\"\n\t    Connection to Druid overlord (or Coordinator) for ingestion\n\t    To connect to a Druid cluster that is secured with the druid-basic-security\n\t    extension, add the username and password to the druid ingestion connection.\n", "    :param druid_ingest_conn_id: The connection id to the Druid overlord machine which accepts index jobs\n\t    :type druid_ingest_conn_id: str\n\t    :param timeout: The interval between polling the Druid job for the status of the ingestion job. Must be greater than or equal to 1\n\t    :type timeout: int\n\t    :param max_ingestion_time: The maximum ingestion time before assuming the job failed\n\t    :type max_ingestion_time: int\n\t    :param endpoint: The endpoint that overwrite endpoint in connection\n\t    :type endpoint: int\n\t    \"\"\"\n\t    def __init__(\n", "            self,\n\t            endpoint=None,\n\t            **kwargs):\n\t        super(DruidCustomHook, self).__init__(**kwargs)\n\t        self.endpoint = endpoint\n\t    def _get_conn_url(self, endpoint=None):\n\t        conn = self.get_connection(self.druid_ingest_conn_id)\n\t        host = conn.host\n\t        if conn.port is not None:\n\t            host += ':{port}'.format(port=conn.port)\n", "        conn_type = 'http' if not conn.conn_type else conn.conn_type\n\t        _endpoint = endpoint if endpoint is not None else conn.extra_dejson.get('endpoint', 'druid/indexer/v1/task')\n\t        return furl(f\"{conn_type}://{host}\").add(path=_endpoint).url\n\t    def get_base_url(self):\n\t        return self._get_conn_url('')\n\t    def get_conn_url(self):\n\t        return self._get_conn_url(self.endpoint)\n\t    def _mark_unused(self, payload, datasource):\n\t        self.log.info(payload)\n\t        url = furl(self.get_base_url()).add(path=f\"/druid/coordinator/v1/datasources/{datasource}/markUnused\").url\n", "        req_index = requests.post(url, data=payload, headers=self.header, auth=self.get_auth())\n\t        if req_index.status_code != 200:\n\t            raise AirflowException('Did not get 200 when '\n\t                                    'submitting the Druid job to {}'.format(url))\n\t        self.log.info(\"Mark as unused return: {}\".format(req_index.json()))\n\t    def mask_unused_by_intervals(self, datasource, since, until):\n\t        payload = json.dumps({\n\t            \"interval\": f\"{since}/{until}\"\n\t        }, indent=2)\n\t        self._mark_unused(payload, datasource)\n", "    def mark_unused_by_segment_ids(self, datasource, ids):\n\t        payload = json.dumps({\n\t            \"segmentIds\": ids\n\t        }, indent=2)\n\t        self._mark_unused(payload, datasource)\n\t    def get_all_segments(self, datasource):\n\t        self.log.info(f\"Getting all segments of datasource: {datasource}\")\n\t        url = furl(self.get_base_url()).add(path=f\"/druid/coordinator/v1/metadata/datasources/{datasource}/segments\").url\n\t        req_index = requests.get(url, headers=self.header, auth=self.get_auth())\n\t        if req_index.status_code != 200:\n", "            raise AirflowException('Did not get 200 when '\n\t                                    'submitting the Druid job to {}'.format(url))\n\t        self.log.info(req_index.json())\n\t        return req_index.json()\n\t    def truncate_datasource(self, datasource):\n\t        self.log.info(f\"Deleting datasource: {datasource}\")\n\t        url = furl(self.get_base_url()).add(path=f\"/druid/coordinator/v1/datasources/{datasource}\").url\n\t        req_index = requests.delete(url, headers=self.header, auth=self.get_auth())\n\t        if req_index.status_code != 200:\n\t            raise AirflowException('Did not get 200 when '\n", "                                    'submitting the Druid job to {}'.format(url))\n\t        self.log.info(req_index.json())\n\t        return req_index.json()\n"]}
{"filename": "hooks/ms_teams_webhook.py", "chunked_list": ["from airflow.hooks.http_hook import HttpHook\n\tfrom airflow.exceptions import AirflowException\n\tclass MSTeamsWebhookHook(HttpHook):\n\t    \"\"\"\n\t    This sqoop_hook allows you to post messages to MS Teams using the Incoming Webhook connector.\n\t    Takes both MS Teams webhook token directly and connection that has MS Teams webhook token.\n\t    If both supplied, the webhook token will be appended to the host in the connection.\n\t    :param http_conn_id: connection that has MS Teams webhook URL\n\t    :type http_conn_id: str\n\t    :param webhook_token: MS Teams webhook token\n", "    :type webhook_token: str\n\t    :param message: The message you want to send on MS Teams\n\t    :type message: str\n\t    :param title: The subtitle of the message to send\n\t    :type title: str\n\t    :param button_text: The text of the action button\n\t    :type button_text: str\n\t    :param button_url: The URL for the action button click\n\t    :type button_url : str\n\t    :param theme_color: Hex code of the card theme, without the #\n", "    :type message: str\n\t    :param proxy: Proxy to use when making the webhook request\n\t    :type proxy: str\n\t    \"\"\"\n\t    def __init__(self,\n\t                dag=None,\n\t                task=None,\n\t                http_conn_id=None,\n\t                webhook_token=None,\n\t                message=\"\",\n", "                title=\"\",\n\t                exec_time=None,\n\t                button_text=\"\",\n\t                button_url=\"\",\n\t                theme_color=\"00FF00\",\n\t                proxy=None,\n\t                *args,\n\t                **kwargs):\n\t        super(MSTeamsWebhookHook, self).__init__(*args, **kwargs)\n\t        self.http_conn_id = http_conn_id\n", "        self.webhook_token = self.get_token(webhook_token, http_conn_id)\n\t        self.message = message\n\t        self.title = title\n\t        self.button_text = button_text\n\t        self.button_url = button_url\n\t        self.theme_color = theme_color\n\t        self.proxy = proxy\n\t        self.dag = dag,\n\t        self.task = task,\n\t        self.exec_time = exec_time\n", "    def get_proxy(self, http_conn_id):\n\t        conn = self.get_connection(http_conn_id)\n\t        extra = conn.extra_dejson\n\t        print(extra)\n\t        return extra.get(\"proxy\", '')\n\t    def get_token(self, token, http_conn_id):\n\t        \"\"\"\n\t        Given either a manually set token or a conn_id, return the webhook_token to use\n\t        :param token: The manually provided token\n\t        :param conn_id: The conn_id provided\n", "        :return: webhook_token (str) to use\n\t        \"\"\"\n\t        if token:\n\t            return token\n\t        elif http_conn_id:\n\t            conn = self.get_connection(http_conn_id)\n\t            extra = conn.extra_dejson\n\t            return extra.get('webhook_token', '')\n\t        else:\n\t            raise AirflowException('Cannot get URL: No valid MS Teams '\n", "                                    'webhook URL nor conn_id supplied')\n\t    def build_message(self):\n\t        print(self.dag, self.task)\n\t        card_json = \"\"\"\n\t        {{\n\t            \"@type\": \"MessageCard\",\n\t            \"@context\": \"http://schema.org/extensions\",\n\t            \"themeColor\": \"{theme}\",\n\t            \"title\": \"{title}\",\n\t            \"summary\": \"{title}\",\n", "            \"potentialAction\": [\n\t                {{\n\t                    \"@type\": \"OpenUri\",\n\t                    \"name\": \"{button_text}\",\n\t                    \"targets\": [\n\t                        {{ \"os\": \"default\", \"uri\": \"{button_url}\" }}\n\t                    ]\n\t                }}\n\t            ],\n\t            \"sections\": [{{\n", "                \"markdown\": false,\n\t                \"facts\": [\n\t                    {{\n\t                        \"name\": \"Dag ID:\",\n\t                        \"value\": \"{dag}\"\n\t                    }},\n\t                    {{\n\t                        \"name\": \"Task ID:\",\n\t                        \"value\": \"{task}\"\n\t                    }},\n", "                    {{\n\t                        \"name\": \"Ex Time:\",\n\t                        \"value\": \"{ex_time}\"\n\t                    }}\n\t                ],\n\t                \"text\": \"{message}\"\n\t            }}]\n\t        }}\n\t        \"\"\"\n\t        return card_json.format(title=self.title, message=self.message, theme=self.theme_color,\n", "                                button_text=self.button_text, button_url=self.button_url,\n\t                                dag=self.dag[0], task=self.task[0], ex_time=self.exec_time)\n\t    def execute(self):\n\t        \"\"\"\n\t        Remote Popen (actually execute the webhook call)\n\t        :param cmd: command to remotely execute\n\t        :param kwargs: extra arguments to Popen (see subprocess.Popen)\n\t        \"\"\"\n\t        proxies = {}\n\t        proxy_url = self.get_proxy(self.http_conn_id)\n", "        print(\"Proxy is : \" + proxy_url)\n\t        if len(proxy_url) > 5:\n\t            proxies = {'https': proxy_url}\n\t        self.run(endpoint=self.webhook_token,\n\t                    data=self.build_message(),\n\t                    headers={'Content-type': 'application/json'},\n\t                    extra_options={'proxies': proxies})\n"]}
{"filename": "hooks/postgres.py", "chunked_list": ["import psycopg2\n\timport pandas as pd\n\timport numpy as np\n\tfrom contextlib import closing\n\tfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\tdef nan_to_null(f,\n\t                _NULL=psycopg2.extensions.AsIs('NULL'),\n\t                _Float=psycopg2.extensions.Float):\n\t    if not np.isnan(f):\n\t        return _Float(f)\n", "    return _NULL\n\tclass PostgresPandasHook(PostgresHook):\n\t    \"\"\"\n\t    Add Insert a dataframe to postgres method\n\t    \"\"\"\n\t    def __init__(self, *args, **kwargs):\n\t        super(PostgresPandasHook, self).__init__(*args, **kwargs)\n\t    def insert_pandas_2_postgres(self,\n\t        dataframe,\n\t        table,\n", "        conflict_columns = [],\n\t        on_conflict_update_columns = [],\n\t        schema=\"public\"):\n\t        psycopg2.extensions.register_adapter(float, nan_to_null)\n\t        values = [tuple(x) for x in dataframe.to_numpy()]\n\t        columns = ','.join(list(dataframe.columns))\n\t        on_conflict = \"\" if not conflict_columns else \"ON CONFLICT (%s)\" % (','.join(conflict_columns))\n\t        conflict_do = \" DO NOTHING;\" if not on_conflict_update_columns else \"DO UPDATE SET %s;\" \\\n\t            % (','.join(list(map(lambda x: f\"{x} = EXCLUDED.{x}\", on_conflict_update_columns))))\n\t        conflict_resolve = on_conflict if not on_conflict else on_conflict + conflict_do\n", "        query = \"\"\"\n\t            INSERT INTO %s.%s as f (%s)\n\t            VALUES %%s\n\t            %s\n\t            \"\"\" % (schema, table, columns, conflict_resolve)\n\t        psycopg2.extensions.register_adapter(float, nan_to_null)\n\t        conn = self.get_conn()\n\t        cursor = conn.cursor()\n\t        try:\n\t            psycopg2.extras.execute_values(cursor, query, values)\n", "            conn.commit()\n\t            self.log.info(f\"Upsert data to {table} successful!!!\")\n\t        except (Exception, psycopg2.DatabaseError) as error:\n\t            print(\"Error: %s\" % error)\n\t            conn.rollback()\n\t        finally:\n\t            if cursor is not None:\n\t                cursor.close()\n\t            if conn is not None:\n\t                conn.close()\n", "    def query_from_postgres(self, sql: str) -> pd.DataFrame:\n\t        engine = self.get_sqlalchemy_engine()\n\t        dataframe = pd.read_sql_query(sql, con=engine)\n\t        return dataframe\n"]}
{"filename": "hooks/es_hook.py", "chunked_list": ["import json\n\tfrom ssl import CERT_NONE\n\tfrom airflow.hooks.base import BaseHook\n\tfrom elasticsearch import Elasticsearch, exceptions\n\tclass ElasticSearchHook(BaseHook):\n\t    def __init__(self, conn_id='es_dev_conn', *args, **kwargs):\n\t        super(ElasticSearchHook, self).__init__()\n\t        self.es_conn_id = conn_id\n\t        self._connection = self.get_connection(conn_id)\n\t        self.client = None\n", "        self.extras = self._connection.extra_dejson.copy()\n\t        self._uri = '{content_type}://{creds}{host}{port}'.format(\n\t            content_type=self._connection.conn_type,\n\t            creds='{}:{}@'.format(\n\t                self._connection.login, self._connection.password\n\t            ) if self._connection.login else '',\n\t            host=self._connection.host,\n\t            port='' if self._connection.port is None else ':{}'.format(self._connection.port),\n\t        )\n\t    def check_valid_client(self) -> None:\n", "        try:\n\t            # get information on client\n\t            client_info = Elasticsearch.info(self.client)\n\t            self.log.info('Elasticsearch client info:\\n {} '.format(json.dumps(client_info, indent=4)))\n\t        except exceptions.ConnectionError as err:\n\t            self.log.debug('Elasticsearch client error:', err)\n\t    def get_conn(self) -> Elasticsearch:\n\t        \"\"\"\n\t        Fetches ElasticSearch Client\n\t        \"\"\"\n", "        if self.client is not None:\n\t            return self.client\n\t        # Elastic search Connection Options dict that is unpacked when passed to Elasticsearch client\n\t        options = self.extras\n\t        # If we are using SSL disable requiring certs from specific hostname\n\t        if options.get('ssl', False):\n\t            options.update({'ssl_cert_reqs': CERT_NONE})\n\t        self.client = Elasticsearch(self._uri, **options)\n\t        # self.check_valid_client()\n\t        return self.client\n", "    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        if self.client is not None:\n\t            self.close_conn()\n\t    def close_conn(self):\n\t        client = self.client\n\t        if client is not None:\n\t            client.transport.close()\n\t            self.client = None\n"]}
{"filename": "hooks/cassandra_custom.py", "chunked_list": ["import pandas as pd\n\tfrom typing import List, Union, Optional\n\tfrom cassandra import ConsistencyLevel\n\tfrom cassandra.cluster import Session\n\tfrom cassandra.query import BatchStatement, Statement\n\tfrom airflow.providers.apache.cassandra.hooks.cassandra import CassandraHook\n\tfrom airflow.exceptions import AirflowException\n\tdefault_write_consistency_level = ConsistencyLevel.LOCAL_QUORUM\n\tclass CassandraCustomHook(CassandraHook):\n\t    \"\"\"\n", "    Cassandra connect interaction wrapper\n\t    :param keyspace: The keyspace that overwrite keyspace in connection\n\t    :type keyspace: str\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n\t            keyspace=None,\n\t            **kwargs):\n\t        super(CassandraCustomHook, self).__init__(**kwargs)\n\t        if keyspace is not None:\n", "            self.keyspace = keyspace\n\t    def _resolve_consistency_level(self, consistency_level) -> ConsistencyLevel:\n\t        if type(consistency_level) is str:\n\t            if consistency_level == \"ALL\":\n\t                return ConsistencyLevel.ALL\n\t            elif consistency_level == \"EACH_QUORUM\":\n\t                return ConsistencyLevel.EACH_QUORUM\n\t            elif consistency_level == \"QUORUM\":\n\t                return ConsistencyLevel.QUORUM\n\t            elif consistency_level == \"LOCAL_QUORUM\":\n", "                return ConsistencyLevel.LOCAL_QUORUM\n\t            elif consistency_level == \"ONE\":\n\t                return ConsistencyLevel.ONE\n\t            elif consistency_level == \"TWO\":\n\t                return ConsistencyLevel.TWO\n\t            elif consistency_level == \"THREE\":\n\t                return ConsistencyLevel.THREE\n\t            elif consistency_level == \"LOCAL_ONE\":\n\t                return ConsistencyLevel.LOCAL_ONE\n\t            elif consistency_level == \"ANY\":\n", "                return ConsistencyLevel.ANY\n\t            else:\n\t                self.log.warning(f\"Consistency level {consistency_level} not supperted\" +\n\t                    \"\\nPlease read https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlConfigConsistency.html to more details\" +\n\t                    f\"\\nUse default consistency level: {default_write_consistency_level}\")\n\t                return default_write_consistency_level\n\t        elif type(consistency_level) is ConsistencyLevel:\n\t            return consistency_level\n\t        else:\n\t            self.log.warning(f\"Consistency level type {type(consistency_level)} is invalid.\" +\n", "                \"\\nPlease use correct type [str, cassandra.ConsistencyLevel].\" +\n\t                f\"\\nUse default consistency level: {default_write_consistency_level}\")\n\t            return default_write_consistency_level\n\t    def insert_dataframe(self,\n\t        df: pd.DataFrame,\n\t        table: str,\n\t        batch_insert_records = 500,\n\t        async_timeout = 300000,\n\t        write_consistency_level: Union[str, ConsistencyLevel] = default_write_consistency_level) -> bool:\n\t        \"\"\"\n", "        Write pandas.DataFrame to cassandra\n\t        :param batch_insert_records: the number of row with insert into a BatchStatement\n\t        :type batch_insert_records: int\n\t        :param async_timeout: the timeout in miliseconds to wait a async query\n\t        :type async_timeout: long\n\t        :param write_consistency_level: the consistency level\n\t        :type write_consistency_level: Union[str, cassandra.ConsistencyLevel]\n\t        \"\"\"\n\t        _write_consistency_level = self._resolve_consistency_level(write_consistency_level)\n\t        _is_failed = False\n", "        def when_success(results):\n\t            self.log.info(f\"Insert rows successful\")\n\t        def when_failed(err):\n\t            self.log.error(\"Insert failed: %s\", err)\n\t            _is_failed = True\n\t        cols = df.columns.tolist()\n\t        session = self.get_conn()\n\t        query = \"INSERT INTO %s(%s) VALUES (%s)\" % (table, ','.join(cols), ','.join(['?']*len(cols)))\n\t        prepared_query = session.prepare(query)\n\t        for partition in self._split_to_partitions(df, batch_insert_records):\n", "            batch = BatchStatement(consistency_level=_write_consistency_level)\n\t            for _, item in partition.iterrows():\n\t                values = tuple(item.to_list())\n\t                batch.add(prepared_query, values)\n\t            f = session.execute_async(batch, async_timeout)\n\t            f.add_callbacks(when_success, when_failed)\n\t        if _is_failed:\n\t            raise AirflowException(\"Error when insert data to cassandra\")\n\t    def _split_to_partitions(self, df: pd.DataFrame, batch_insert_records=500) -> List[pd.DataFrame]:\n\t        partitions = []\n", "        i = 0\n\t        while i < df.shape[0]:\n\t            partitions.append(df.loc[i:i+batch_insert_records - 1])\n\t            i = i + batch_insert_records\n\t        return partitions\n\t    def select_dataframe(self, cql: Union[str, Statement], fetch_size = None, timeout = None, existed_session: Optional[Session] = None) -> pd.DataFrame:\n\t        def pandas_factory(colnames, rows) -> pd.DataFrame:\n\t            return pd.DataFrame(rows, columns=colnames)\n\t        session = existed_session if existed_session is not None else self.get_conn()\n\t        session.row_factory = pandas_factory\n", "        session.default_fetch_size = fetch_size\n\t        result = session.execute(cql, timeout=timeout)\n\t        df = result._current_rows\n\t        return df\n"]}
{"filename": "hooks/amplitude.py", "chunked_list": ["from typing import Tuple\n\tfrom airflow.hooks.base import BaseHook\n\tclass AmplitudeHook(BaseHook):\n\t    def __init__(self, amplitude_conn='amplitude_conn', **kwargs):\n\t        super(AmplitudeHook, self).__init__(source='amplitude')\n\t        self.http_conn_id = amplitude_conn\n\t        self.base_url = None\n\t        self._auth = None\n\t    def get_conn(self) -> Tuple[str, Tuple[str, str]]:\n\t        conn = self.get_connection(self.http_conn_id)\n", "        if conn.host and \"://\" in conn.host:\n\t            self.base_url = conn.host\n\t        else:\n\t            # schema defaults to HTTP\n\t            schema = conn.schema if conn.schema else \"http\"\n\t            host = conn.host if conn.host else \"\"\n\t            self.base_url = schema + \"://\" + host\n\t        if conn.port:\n\t            self.base_url = self.base_url + \":\" + str(conn.port)\n\t        if conn.login:\n", "            self._auth = (conn.login, conn.password)\n\t        return self.base_url, self._auth\n"]}
{"filename": "hooks/appstoreconnect.py", "chunked_list": ["from airflow.exceptions import AirflowException\n\tfrom airflow.hooks.base import BaseHook\n\tfrom airflow.utils.log.logging_mixin import LoggingMixin\n\tfrom api.appstoreconnect import Api as AppStoreConnectApi\n\tclass AppStoreConnectHook(BaseHook, LoggingMixin):\n\t    conn_name_attr = 'appstore_connect_conn_id'\n\t    default_conn_name = 'appstore_connect_default'\n\t    conn_type = 'appstore_connect'\n\t    hook_name = 'AppStore Connect'\n\t    def __init__(self, conn_id: str = default_conn_name) -> None:\n", "        self.conn_id = conn_id\n\t        conn = self.get_connection(self.conn_id)\n\t        config = conn.extra_dejson\n\t        if not config.get('key_id', None):\n\t            raise AirflowException('No Key ID provided')\n\t        if not config.get('key_file', None):\n\t            raise AirflowException('No Key File provided')\n\t        if not config.get('issuer_id', None):\n\t            raise AirflowException('No Issuer ID provided')\n\t        self.key_id = config['key_id']\n", "        self.key_file = config['key_file']\n\t        self.issuer_id = config['issuer_id']\n\t        self._api: AppStoreConnectApi = None\n\t    def get_api(self) -> AppStoreConnectApi:\n\t        if not self._api:\n\t            self._api = AppStoreConnectApi(key_id=self.key_id, key_file=self.key_file, issuer_id=self.issuer_id)\n\t        return self._api\n"]}
{"filename": "hooks/__init__.py", "chunked_list": ["\"\"\"\n\tThis module store any custom hooks\n\t\"\"\""]}
{"filename": "hooks/postgres_custom.py", "chunked_list": ["import psycopg2.extensions\n\timport psycopg2\n\tfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\timport numpy as np\n\tdef nan_to_null(f,\n\t                _NULL=psycopg2.extensions.AsIs('NULL'),\n\t                _Float=psycopg2.extensions.Float):\n\t    if not np.isnan(f):\n\t        return _Float(f)\n\t    return _NULL\n", "psycopg2.extensions.register_adapter(float, nan_to_null)\n\tclass PostgresCustomHook(PostgresHook):\n\t    conn_name_attr = 'postgres_conn_id'\n\t    default_conn_name = 'postgres_default'\n\t    supports_autocommit = True\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t    @staticmethod\n\t    def _generate_insert_sql(table, values, target_fields, replace, **kwargs):\n\t        \"\"\"\n", "        Static helper method that generate the INSERT SQL statement.\n\t        The REPLACE variant is specific to MySQL syntax.\n\t        :param table: Name of the target table\n\t        :type table: str\n\t        :param values: The row to insert into the table\n\t        :type values: tuple of cell values\n\t        :param target_fields: The names of the columns to fill in the table\n\t        :type target_fields: iterable of strings\n\t        :param replace: Whether to replace instead of insert\n\t        :type replace: bool\n", "        :param replace_index: the column or list of column names to act as\n\t            index for the ON CONFLICT clause\n\t        :type replace_index: str or list\n\t        :return: The generated INSERT or REPLACE SQL statement\n\t        :rtype: str\n\t        \"\"\"\n\t        placeholders = [\"%s\", ] * len(values)\n\t        if target_fields:\n\t            target_fields_fragment = \", \".join(target_fields)\n\t            target_fields_fragment = \"({})\".format(target_fields_fragment)\n", "        else:\n\t            target_fields_fragment = ''\n\t        sql = \"INSERT INTO {0} as {0} {1} VALUES ({2})\".format(\n\t            table,\n\t            target_fields_fragment,\n\t            \",\".join(placeholders))\n\t        if replace:\n\t            conflict_sql = kwargs.get(\"conflict_sql\", None)\n\t            if conflict_sql is None:\n\t                raise ValueError(\"PostgreSQL ON CONFLICT statement must be specify!\")\n", "            # ex: conflict_sql = \"ON CONFLICT (p1, p2) DO UPDATE SET f1 = EXCLUDE.f1, f2 = EXCLUDE.f2\"\n\t            sql += \" \" + conflict_sql\n\t        return sql\n"]}
{"filename": "hooks/moengage.py", "chunked_list": ["import sys\n\timport base64\n\tif sys.version_info >= (3, 8):\n\t    from functools import cached_property\n\telse:\n\t    from cached_property import cached_property\n\tfrom airflow.exceptions import AirflowException\n\tfrom airflow.hooks.base import BaseHook\n\tclass MoEngageDataApiHook(BaseHook):\n\t    \"\"\"\n", "        A Hook to load moengage secret information from airflow config\n\t        and prepare authorization's encrytion\n\t    \"\"\"\n\t    conn_name_attr = 'data_api_conn'\n\t    default_conn_name = 'moengage_data_api'\n\t    conn_type = 'moengage_data_api'\n\t    hook_name = 'MoEngage Data Api'\n\t    def __init__(self, data_api_conn=default_conn_name):\n\t        self.data_api_conn = data_api_conn\n\t        self.parse_connection()\n", "    def parse_connection(self):\n\t        self.log.info(\"Fetching MoEngage connection: %s\", self.data_api_conn)\n\t        conn = self.get_connection(self.data_api_conn)\n\t        self.endpoint = conn.host\n\t        self.app_id = conn.schema\n\t        self.data_api_id = conn.login\n\t        self.data_api_key = conn.password\n\t    def get_b64_authorization(self) -> str:\n\t        auth = f\"{self.data_api_id}:{self.data_api_key}\"\n\t        auth_b64 = base64.b64encode(auth.encode(\"ascii\"))\n", "        return auth_b64\n"]}
{"filename": "hooks/hdfs.py", "chunked_list": ["from airflow.exceptions import AirflowException\n\tfrom airflow.hooks.base import BaseHook\n\ttry:\n\t    from hdfs3 import HDFileSystem\n\t    hdfs3_loaded = True\n\texcept ImportError:\n\t    hdfs3_loaded = False\n\tclass HDFSHookException(AirflowException):\n\t    pass\n\tclass HDFSHook(BaseHook):\n", "    \"\"\"\n\t        Interact with HDFS. This class is a wrapper around the hdfs3 library.\n\t        :param hdfs_conn_id: Connection id to fetch connection info\n\t        :type hdfs_conn_id: str\n\t        :param proxy_user: effective user for HDFS operations\n\t        :type proxy_user: str\n\t    \"\"\"\n\t    def __init__(self, hdfs_conn_id='hdfs_default', hdfs_user='hadoop'):\n\t        if not hdfs3_loaded:\n\t            raise ImportError('This HDFSHook implementation requires hdfs3 and libhdfs3')\n", "        self.hdfs_conn_id = hdfs_conn_id\n\t        self.hdfs_user = hdfs_user\n\t        self.client: HDFileSystem = None\n\t        self.uri = None\n\t    def get_conn(self) -> HDFileSystem:\n\t        \"\"\"\n\t        Returns a hdfs3.HDFileSystem object.\n\t        \"\"\"\n\t        if self.client is not None:\n\t            return self.client\n", "        try:\n\t            conn = self.get_connection(self.hdfs_conn_id)\n\t            config_extra = conn.extra_dejson\n\t            if not self.hdfs_user:\n\t                self.hdfs_user = conn.login\n\t        except AirflowException:\n\t            raise(\"Can't get HDFS connection. Please add HDFS connection on WebUI\")\n\t        try:\n\t            self.client = HDFileSystem(\n\t                host=conn.host,\n", "                port=conn.port,\n\t                autoconf=config_extra.get('autoconf', True),\n\t                pars=config_extra.get(\"pars\", None),\n\t                user=self.hdfs_user\n\t            )\n\t            return self.client\n\t        except:\n\t            raise HDFSHookException(\"conn_id doesn't exist in the repository \"\n\t                                    \"and autoconfig is not specified\")\n\t    def get_conn_uri(self) -> str:\n", "        if self.uri is not None:\n\t            return self.uri\n\t        try:\n\t            conn = self.get_connection(self.hdfs_conn_id)\n\t            if not self.hdfs_user:\n\t                self.hdfs_user = conn.login\n\t            uri = f\"hdfs://{conn.host}\"\n\t            if conn.port:\n\t                uri += f\":{conn.port}\"\n\t            return uri\n", "        except:\n\t            return None\n\t    def get_file_conf(self):\n\t        conn = self.get_connection(self.hdfs_conn_id)\n\t        config_extra = conn.extra_dejson\n\t        return config_extra.get('file_conf', None)\n\t    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        self.disconnect()\n", "    def disconnect(self):\n\t        \"\"\"Closes fs connection (if applicable).\"\"\"\n\t        self.client.disconnect()\n"]}
{"filename": "hooks/sqoop.py", "chunked_list": ["from typing import List, Any, Optional, Dict\n\tfrom airflow.providers.apache.sqoop.hooks.sqoop import SqoopHook\n\tclass SqoopCustomHook(SqoopHook):\n\t    def __init__(self, schema, conn_id, verbose, num_mappers):\n\t        super(SqoopCustomHook, self).__init__(num_mappers=num_mappers, conn_id=conn_id, verbose=verbose)\n\t        self.schema: str = schema\n\t    def import_query_cmd(\n\t            self,\n\t            query: str,\n\t            target_dir: Optional[str] = None,\n", "            append: bool = False,\n\t            file_type: str = \"text\",\n\t            split_by: Optional[str] = None,\n\t            direct: Optional[bool] = None,\n\t            driver: Optional[Any] = None,\n\t            extra_import_options: Optional[Dict[str, Any]] = None,\n\t    ) -> Any:\n\t        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)\n\t        cmd += [\"--query\", query]\n\t        return cmd\n", "    def import_table_cmd(\n\t            self,\n\t            table: str,\n\t            target_dir: Optional[str] = None,\n\t            append: bool = False,\n\t            file_type: str = \"text\",\n\t            columns: Optional[str] = None,\n\t            split_by: Optional[str] = None,\n\t            where: Optional[str] = None,\n\t            direct: bool = False,\n", "            driver: Any = None,\n\t            extra_import_options: Optional[Dict[str, Any]] = None,\n\t    ) -> Any:\n\t        \"\"\"\n\t        Imports table from remote location to target dir. Arguments are\n\t        copies of direct sqoop command line arguments\n\t        :param table: Table to read\n\t        :param target_dir: HDFS destination dir\n\t        :param append: Append data to an existing dataset in HDFS\n\t        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".\n", "            Imports data to into the specified format. Defaults to text.\n\t        :param columns: <col,col,col…> Columns to import from table\n\t        :param split_by: Column of the table used to split work units\n\t        :param where: WHERE clause to use during import\n\t        :param direct: Use direct connector if exists for the database\n\t        :param driver: Manually specify JDBC driver class to use\n\t        :param extra_import_options: Extra import options to pass as dict.\n\t            If a key doesn't have a value, just pass an empty string to it.\n\t            Don't include prefix of -- for sqoop options.\n\t        \"\"\"\n", "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)\n\t        cmd += [\"--table\", table]\n\t        if columns:\n\t            cmd += [\"--columns\", columns]\n\t        if where:\n\t            cmd += [\"--where\", where]\n\t        return cmd\n"]}
