{"filename": "acad_bot.py", "chunked_list": ["\"\"\"\n\tadd deps\n\tTODO: pip install discord requests validators bs4 elasticsearch==7.0.0\n\t# TODO:\n\t- add timestamps for:\n\t    1. when bookmark webpage itself was actually created (optional)\n\t    2. when bookmark was last updated\n\t- annotated file `upload` command\n\tCollaboration:\n\t- add clear roadmap for how to add new commands based on other modalities\n", "- target for mid may to release to public\n\t\"\"\"\n\tfrom typing import List, Optional\n\timport discord\n\timport validators\n\tfrom discord import app_commands\n\tfrom acad_bot_utils import get_url_type, process_url\n\tfrom acad_gpt.datastore import ElasticSearchDataStore, ElasticSearchStoreConfig, RedisDataStore, RedisDataStoreConfig\n\tfrom acad_gpt.environment import (\n\t    DISCORD_BOOKMARK_BOT_GUILD_ID,\n", "    DISCORD_BOOKMARK_BOT_TOKEN,\n\t    ES_INDEX,\n\t    ES_PASSWORD,\n\t    ES_PORT,\n\t    ES_URL,\n\t    ES_USERNAME,\n\t    OPENAI_API_KEY,\n\t    REDIS_HOST,\n\t    REDIS_PASSWORD,\n\t    REDIS_PORT,\n", ")\n\tfrom acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\n\tfrom acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient\n\tfrom acad_gpt.llm_client.openai.conversation.config import ChatGPTConfig\n\tfrom acad_gpt.memory.manager import MemoryManager\n\t# Instantiate an EmbeddingClient object with the EmbeddingConfig object\n\tembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\tembed_client = EmbeddingClient(config=embedding_config)\n\t# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n\tredis_datastore_config = RedisDataStoreConfig(\n", "    host=REDIS_HOST,\n\t    port=REDIS_PORT,\n\t    password=REDIS_PASSWORD,\n\t)\n\tredis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\t# Instantiate an ElasticSearchDataStore object with the ElasticSearchStoreConfig object\n\tes_datastore_config = ElasticSearchStoreConfig(\n\t    host=ES_URL,\n\t    port=ES_PORT,\n\t    username=ES_USERNAME,\n", "    password=ES_PASSWORD,\n\t    index_name=ES_INDEX,\n\t)\n\tes_datastore = ElasticSearchDataStore(config=es_datastore_config)\n\t# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\n\tmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\tchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\tchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\tintents = discord.Intents.default()\n\tintents.message_content = True\n", "intents.members = True\n\tclient = discord.Client(intents=intents)\n\ttree = app_commands.CommandTree(client)\n\t@client.event\n\tasync def on_ready():\n\t    await tree.sync(guild=discord.Object(id=DISCORD_BOOKMARK_BOT_GUILD_ID))\n\t    print(\"Logged in as {0.user}\".format(client))\n\t@client.event\n\tasync def on_message(message):\n\t    if message.author == client.user:\n", "        return\n\t    print(\"{0.author}: {0.content}\".format(message))\n\t    if validators.url(message.content):\n\t        type, url = get_url_type(message.content)\n\t        document = process_url(url, type, embed_client)\n\t        redis_datastore.index_documents(documents=[document])\n\t        es_datastore.index_documents(documents=[document])\n\t        await message.channel.send(f\"`{type}` Bookmark saved!\")\n\t    else:\n\t        await message.channel.send(\"Something went wrong!\")\n", "@tree.command(\n\t    name=\"search\", description=\"To search indexed content\", guild=discord.Object(id=DISCORD_BOOKMARK_BOT_GUILD_ID)\n\t)\n\tasync def search(\n\t    interaction, query: str, status: Optional[str] = None, topk: Optional[int] = 5, ask_gpt: Optional[bool] = False\n\t):\n\t    if ask_gpt:\n\t        await interaction.response.defer()\n\t        chat_result = (\n\t            f\"`{client.user}`: {query}\\n`ChatGPT`: {chat_gpt_client.converse(query, topk=topk).chat_gpt_answer}\"\n", "        )\n\t        await interaction.followup.send(chat_result)\n\t    else:\n\t        topk_hits = es_datastore.search_documents(query, status=status, topk=topk)\n\t        if len(topk_hits) > 0:\n\t            response = f\"Here are the top {len(topk_hits)} results for your query: `{query}`\\n\"\n\t            response += \"\\n\".join([f\"{idx+1}. {hit['title']}\\n<{hit['url']}>\" for idx, hit in enumerate(topk_hits)])\n\t            await interaction.response.send_message(response)\n\t        else:\n\t            await interaction.response.send_message(f\"Sorry, no results found for your query: {query}\")\n", "@tree.command(\n\t    name=\"update\", description=\"To update indexed content\", guild=discord.Object(id=DISCORD_BOOKMARK_BOT_GUILD_ID)\n\t)\n\tasync def update(interaction, url: str, status: str):\n\t    if validators.url(url):\n\t        type, url = get_url_type(url)\n\t        document = process_url(url, type, embed_client, status=status)\n\t        redis_datastore.index_documents(documents=[document])\n\t        document.pop(\"embedding\")\n\t        es_datastore.index_documents(documents=[document])\n", "        await interaction.response.send_message(f\"Bookmark for <{url}> is set to {status} status!\")\n\t    else:\n\t        await interaction.response.send_message(\"Something went wrong!\")\n\t@update.autocomplete(\"status\")\n\t@search.autocomplete(\"status\")\n\tasync def status_autocomplete(\n\t    interaction: discord.Interaction,\n\t    current: str,\n\t) -> List[app_commands.Choice[str]]:\n\t    statuses = [\"todo\", \"done\"]\n", "    return [app_commands.Choice(name=status, value=status) for status in statuses if current.lower() in status.lower()]\n\tclient.run(DISCORD_BOOKMARK_BOT_TOKEN)\n"]}
{"filename": "rest_api.py", "chunked_list": ["# import enum\n\t# import os\n\t# import shutil\n\t# import uuid\n\t# from pathlib import Path\n\t# from typing import Any, Dict, List, Union\n\t# import numpy as np\n\t# from fastapi import FastAPI, File, UploadFile\n\t# from fastapi.middleware.cors import CORSMiddleware\n\t# from pydantic import BaseModel\n", "# from acad_gpt.datastore import RedisDataStore, RedisDataStoreConfig\n\t# from acad_gpt.docstore.hf_file_system_storage import HfFSDocStore, HfFSDocStoreConfig\n\t# from acad_gpt.environment import (\n\t#     FILE_UPLOAD_PATH,\n\t#     HF_ENDPOINT,\n\t#     HF_REPO,\n\t#     HF_TOKEN,\n\t#     OPENAI_API_KEY,\n\t#     REDIS_HOST,\n\t#     REDIS_PASSWORD,\n", "#     REDIS_PORT,\n\t# )\n\t# from acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\n\t# from acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient\n\t# from acad_gpt.llm_client.openai.conversation.config import ChatGPTConfig\n\t# from acad_gpt.memory.manager import MemoryManager\n\t# from acad_gpt.parsers import ParserConfig, PDFParser\n\t# class DocumentType(str, enum.Enum):\n\t#     Table = \"Table\"\n\t#     Figure = \"Figure\"\n", "#     Highlight = \"Highlight\"\n\t#     Section = \"Section\"\n\t# class SearchPayload(BaseModel):\n\t#     query: str\n\t#     k: int = 5\n\t#     document_type: DocumentType = DocumentType.Section\n\t# class Response(BaseModel):\n\t#     file_id: str\n\t#     message: str\n\t# origins = [\n", "#     \"http://localhost:3000\",\n\t# ]\n\t# app = FastAPI()\n\t# app.add_middleware(\n\t#     CORSMiddleware,\n\t#     allow_origins=origins,\n\t#     allow_credentials=True,\n\t#     allow_methods=[\"*\"],\n\t#     allow_headers=[\"*\"],\n\t# )\n", "# # Instantiate document store for storing PDFs on cloud\n\t# hf_docstore_config = HfFSDocStoreConfig(repo=HF_REPO, token=HF_TOKEN, endpoint=HF_ENDPOINT)\n\t# hf_docstore = HfFSDocStore(config=hf_docstore_config)\n\t# # Instantiate an EmbeddingConfig object with the OpenAI API key\n\t# embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t# # Instantiate an EmbeddingClient object with the EmbeddingConfig object\n\t# embed_client = EmbeddingClient(config=embedding_config)\n\t# # Instantiate a RedisDataStoreConfig object with the Redis connection details\n\t# redis_datastore_config = RedisDataStoreConfig(\n\t#     host=REDIS_HOST,\n", "#     port=REDIS_PORT,\n\t#     password=REDIS_PASSWORD,\n\t# )\n\t# # Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n\t# redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\t# # Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\n\t# memory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\t# # Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\n\t# chat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\t# # Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\n", "# chat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\t# @app.post(\"/file-upload\")\n\t# def upload_file(\n\t#     files: Union[UploadFile, List[UploadFile]] = File(...),\n\t# ):\n\t#     response = []\n\t#     for file in files:\n\t#         try:\n\t#             file_name = f\"{uuid.uuid4().hex}_{file.filename}\"\n\t#             file_path = Path(FILE_UPLOAD_PATH) / file_name\n", "#             isExist = os.path.exists(FILE_UPLOAD_PATH)\n\t#             if not isExist:\n\t#                 # create the metadata directory because it does not exist\n\t#                 os.makedirs(FILE_UPLOAD_PATH)\n\t#             with file_path.open(\"wb\") as buffer:\n\t#                 shutil.copyfileobj(file.file, buffer)\n\t#                 hf_docstore.upload_from_filename(file_path=f\"{FILE_UPLOAD_PATH}/{file_name}\", file_name=file_name)\n\t#                 parser = PDFParser()\n\t#                 parser_config = ParserConfig(file_path_or_url=str(file_path), file_type=\"PDF\", extract_figures=False)\n\t#                 results = parser.parse(config=parser_config)\n", "#                 documents = parser.pdf_to_documents(\n\t#                     pdf_contents=results, embed_client=embed_client, file_name=file_name.replace(\".pdf\", \"\")\n\t#                 )\n\t#                 redis_datastore.index_documents(documents)\n\t#                 response.append(\n\t#                     Response(\n\t#                         file_id=file_name,\n\t#                         message=f\"File `{file.filename}` has been indexed with {len(documents)} passages.\",\n\t#                     )\n\t#                 )\n", "#         except Exception as e:\n\t#             response.append(\n\t#                 Response(\n\t#                     file_id=file_name,\n\t#                     message=f\"Something went wrong!\\n Detail: {e}\",\n\t#                 )\n\t#             )\n\t#         finally:\n\t#             file.file.close()\n\t#     return response\n", "# @app.post(\"/search/\")\n\t# async def search(search_payload: SearchPayload) -> List[Any]:\n\t#     query_vector = embed_client.embed_queries(queries=[search_payload.query])[0].astype(np.float32).tobytes()\n\t#     search_result = redis_datastore.search_documents(query_vector=query_vector, topk=search_payload.k)\n\t#     return search_result\n\t# @app.post(\"/chat/\")\n\t# async def chat(search_payload: SearchPayload) -> Dict:\n\t#     chat_result = chat_gpt_client.converse(message=search_payload.query).dict()\n\t#     return chat_result\n"]}
{"filename": "acad_bot_utils.py", "chunked_list": ["import os\n\timport requests\n\tfrom acad_gpt.environment import FILE_UPLOAD_PATH\n\tfrom acad_gpt.parsers import DocumentType\n\tfrom acad_gpt.parsers.base_parser import DocumentStatus\n\tfrom acad_gpt.parsers.config import ParserConfig\n\tfrom acad_gpt.parsers.pdf_parser import PDFParser\n\tfrom acad_gpt.parsers.webpage_parser import WebPageParser\n\tdef get_url_type(url):\n\t    if url.split(\".\")[-1].strip() == \"pdf\":\n", "        return DocumentType.pdf, url\n\t    else:\n\t        return DocumentType.webpage, url\n\tdef download(url: str):\n\t    if not os.path.exists(FILE_UPLOAD_PATH):\n\t        os.makedirs(FILE_UPLOAD_PATH)  # create folder if it does not exist\n\t    filename = url.split(\"/\")[-1].replace(\" \", \"_\")  # be careful with file names\n\t    file_path = os.path.join(FILE_UPLOAD_PATH, filename)\n\t    r = requests.get(url, stream=True)\n\t    is_saved = False\n", "    if r.ok:\n\t        print(\"saving to\", os.path.abspath(file_path))\n\t        with open(file_path, \"wb\") as f:\n\t            for chunk in r.iter_content(chunk_size=1024 * 8):\n\t                if chunk:\n\t                    f.write(chunk)\n\t                    f.flush()\n\t                    os.fsync(f.fileno())\n\t            is_saved = True\n\t    else:  # HTTP status code 4XX/5XX\n", "        print(\"Download failed: status code {}\\n{}\".format(r.status_code, r.text))\n\t    return is_saved, file_path\n\tdef process_url(\n\t    url,\n\t    type,\n\t    embed_client,\n\t    status=DocumentStatus.todo,\n\t):\n\t    document = None\n\t    if type in [DocumentType.pdf, DocumentType.paper]:\n", "        is_saved, file_path = download(url)\n\t        if is_saved:\n\t            parser = PDFParser()\n\t            parser_config = ParserConfig(file_path=file_path, file_url=url)\n\t            results = parser.parse(config=parser_config)\n\t            document = parser.to_documents(\n\t                pdf_contents=results,\n\t                embed_client=embed_client,\n\t                type=type,\n\t                status=status,\n", "            )\n\t    else:\n\t        parser = WebPageParser()\n\t        parser_config = ParserConfig(file_path=url, file_url=url)\n\t        results = parser.parse(config=parser_config)\n\t        document = parser.to_documents(\n\t            web_contents=results,\n\t            embed_client=embed_client,\n\t            type=type,\n\t            status=status,\n", "        )\n\t    return document\n"]}
{"filename": "tests/test_memory_manager.py", "chunked_list": ["from acad_gpt.datastore.config import RedisDataStoreConfig\n\tfrom acad_gpt.datastore.redis import RedisDataStore\n\tfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tfrom acad_gpt.memory.manager import MemoryManager\n\tfrom acad_gpt.memory.memory import Memory\n\tclass TestMemoryManager:\n\t    def setup(self):\n\t        # create a redis datastore\n", "        redis_datastore_config = RedisDataStoreConfig(\n\t            host=REDIS_HOST,\n\t            port=REDIS_PORT,\n\t            password=REDIS_PASSWORD,\n\t        )\n\t        self.datastore = RedisDataStore(redis_datastore_config, do_flush_data=True)\n\t        # create an openai embedding client\n\t        embedding_client_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t        self.embedding_client = EmbeddingClient(embedding_client_config)\n\t    def test_conversation_insertion_and_deletion(self):\n", "        # create a memory manager\n\t        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\t        # assert that the memory manager is initially empty\n\t        assert len(memory_manager.conversations) == 0\n\t        # add a conversation to the memory manager\n\t        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\t        # assert that the memory manager has 1 conversation\n\t        assert len(memory_manager.conversations) == 1\n\t        # remove the conversation from the memory manager\n\t        memory_manager.remove_conversation(Memory(conversation_id=\"1\"))\n", "        # assert that the memory manager is empty\n\t        assert len(memory_manager.conversations) == 0\n\t    def test_adding_messages_to_conversation(self):\n\t        # create a memory manager\n\t        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\t        # add a conversation to the memory manager\n\t        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\t        # assert that the memory manager has 1 conversation\n\t        assert len(memory_manager.conversations) == 1\n\t        # add a message to the conversation\n", "        memory_manager.add_message(conversation_id=\"1\", human=\"Hello\", assistant=\"Hello. How are you?\")\n\t        # get messages for that conversation\n\t        messages = memory_manager.get_messages(conversation_id=\"1\", query=\"Hello\")\n\t        # assert that the message was added\n\t        assert len(messages) == 1\n\t        # assert that the message is correct\n\t        assert messages[0].text == \"Human: Hello\\nAssistant: Hello. How are you?\"\n\t        assert messages[0].conversation_id == \"1\"\n"]}
{"filename": "tests/test_redis_datastore.py", "chunked_list": ["import numpy as np\n\tfrom acad_gpt.datastore.redis import RedisDataStore\n\tfrom acad_gpt.environment import OPENAI_API_KEY\n\tfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tSAMPLE_QUERIES = [\"Where is Berlin?\"]\n\tSAMPLE_DOCUMENTS = [\n\t    {\"text\": \"Berlin is located in Germany.\", \"conversation_id\": \"1\"},\n\t    {\"text\": \"Vienna is in Austria.\", \"conversation_id\": \"1\"},\n\t    {\"text\": \"Salzburg is in Austria.\", \"conversation_id\": \"2\"},\n", "]\n\tdef test_redis_datastore(redis_datastore: RedisDataStore):\n\t    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t    openai_embedding_client = EmbeddingClient(config=embedding_config)\n\t    assert (\n\t        redis_datastore.redis_connection.ping()\n\t    ), \"Redis connection failed,\\\n\t          double check your connection parameters\"\n\t    document_embeddings: np.ndarray = openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS)\n\t    for idx, embedding in enumerate(document_embeddings):\n", "        SAMPLE_DOCUMENTS[idx][\"embedding\"] = embedding.astype(np.float32).tobytes()\n\t    redis_datastore.index_documents(documents=SAMPLE_DOCUMENTS)\n\t    query_embeddings: np.ndarray = openai_embedding_client.embed_queries(SAMPLE_QUERIES)\n\t    query_vector = query_embeddings[0].astype(np.float32).tobytes()\n\t    search_results = redis_datastore.search_documents(query_vector=query_vector, conversation_id=\"1\", topk=1)\n\t    assert len(search_results), \"No documents returned, expected 1 document.\"\n\t    assert search_results[0].text == \"Berlin is located in Germany.\", \"Incorrect document returned as search result.\"\n\t    redis_datastore.delete_documents(conversation_id=\"1\")\n\t    assert redis_datastore.get_all_conversation_ids() == [\n\t        \"2\"\n", "    ], \"Document deletion failed, inconsistent documents in redis index\"\n"]}
{"filename": "tests/test_docstore.py", "chunked_list": ["import pytest\n\tfrom acad_gpt.docstore.base import DocStore\n\tfrom acad_gpt.docstore.in_memory_storage import InMemoryStorage, InMemoryStorageConfig\n\tclass TestDocStore:\n\t    @pytest.fixture\n\t    def storage(self) -> DocStore:\n\t        config = InMemoryStorageConfig()\n\t        return InMemoryStorage(config)\n\t    def test_upload_from_filename_and_exists(self, tmp_path, storage: DocStore):\n\t        file_path = tmp_path / \"test.txt\"\n", "        file_path.write_text(\"test\")\n\t        assert not storage.exists(\"test.txt\")\n\t        storage.upload_from_filename(str(file_path), \"test.txt\")\n\t        assert storage.exists(\"test.txt\")\n\t    def test_upload_from_file_and_exists(self, tmp_path, storage: DocStore):\n\t        file_path = tmp_path / \"test.txt\"\n\t        file_path.write_text(\"test\")\n\t        assert not storage.exists(\"test.txt\")\n\t        with open(str(file_path), \"rb\") as f:\n\t            storage.upload_from_file(f, \"test.txt\")\n", "        assert storage.exists(\"test.txt\")\n\t    def test_download_to_filename(self, tmp_path, storage: DocStore):\n\t        file_path = tmp_path / \"test.txt\"\n\t        file_path.write_text(\"test\")\n\t        storage.upload_from_filename(str(file_path), \"test.txt\")\n\t        other_file_path = tmp_path / \"other_test.txt\"\n\t        assert not other_file_path.exists(), \"other_test.txt should not exist\"\n\t        storage.download_to_filename(\"test.txt\", str(other_file_path))\n\t        assert other_file_path.read_text() == \"test\"\n\t    def test_download_to_file(self, tmp_path, storage: DocStore):\n", "        file_path = tmp_path / \"test.txt\"\n\t        file_path.write_text(\"test\")\n\t        storage.upload_from_filename(str(file_path), \"test.txt\")\n\t        other_file_path = tmp_path / \"other_test.txt\"\n\t        assert not other_file_path.exists(), \"other_test.txt should not exist\"\n\t        with open(str(other_file_path), \"wb\") as f:\n\t            storage.download_to_file(\"test.txt\", f)\n\t        assert other_file_path.read_text() == \"test\"\n\t    def test_list(self, tmp_path, storage: DocStore):\n\t        file_path = tmp_path / \"test.txt\"\n", "        file_path.write_text(\"test\")\n\t        storage.upload_from_filename(str(file_path), \"test.txt\")\n\t        storage.upload_from_filename(str(file_path), \"test2.txt\")\n\t        assert storage.list() == [\"test.txt\", \"test2.txt\"]\n\t    def test_delete(self, tmp_path, storage: DocStore):\n\t        file_path = tmp_path / \"test.txt\"\n\t        file_path.write_text(\"test\")\n\t        storage.upload_from_filename(str(file_path), \"test.txt\")\n\t        assert storage.exists(\"test.txt\")\n\t        storage.delete(\"test.txt\")\n", "        assert not storage.exists(\"test.txt\")\n"]}
{"filename": "tests/test_llm_embedding_client.py", "chunked_list": ["from acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tSAMPLE_QUERIES = [\"Where is Berlin?\"]\n\tSAMPLE_DOCUMENTS = [{\"text\": \"Berlin is located in Germany.\"}]\n\tEXPECTED_EMBEDDING_DIMENSIONS = (1, 1024)\n\tdef test_openai_embedding_client(openai_embedding_client: EmbeddingClient):\n\t    assert (\n\t        openai_embedding_client.embed_queries(SAMPLE_QUERIES).shape == EXPECTED_EMBEDDING_DIMENSIONS\n\t    ), \"Generated query embedding is of inconsistent dimension\"\n\t    assert (\n\t        openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS).shape == EXPECTED_EMBEDDING_DIMENSIONS\n", "    ), \"Generated document(s) embedding is of inconsistent dimension\"\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import pytest\n\tfrom acad_gpt.datastore.config import RedisDataStoreConfig\n\tfrom acad_gpt.datastore.redis import RedisDataStore\n\tfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\t@pytest.fixture(scope=\"session\")\n\tdef openai_embedding_client():\n\t    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t    return EmbeddingClient(config=embedding_config)\n", "@pytest.fixture(scope=\"session\")\n\tdef redis_datastore():\n\t    redis_datastore_config = RedisDataStoreConfig(\n\t        host=REDIS_HOST,\n\t        port=REDIS_PORT,\n\t        password=REDIS_PASSWORD,\n\t    )\n\t    redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\t    return redis_datastore\n"]}
{"filename": "examples/simple_usage.py", "chunked_list": ["#!/bin/env python3\n\t\"\"\"\n\tThis script describes a simple usage of the library.\n\tYou can see a breakdown of the individual steps in the README.md file.\n\t\"\"\"\n\tfrom acad_gpt.datastore import RedisDataStore, RedisDataStoreConfig\n\t## set the following ENVIRONMENT Variables before running this script\n\t# Import necessary modules\n\tfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom acad_gpt.llm_client import ChatGPTClient, ChatGPTConfig, EmbeddingClient, EmbeddingConfig\n", "from acad_gpt.memory import MemoryManager\n\t# Instantiate an EmbeddingConfig object with the OpenAI API key\n\tembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t# Instantiate an EmbeddingClient object with the EmbeddingConfig object\n\tembed_client = EmbeddingClient(config=embedding_config)\n\t# Instantiate a RedisDataStoreConfig object with the Redis connection details\n\tredis_datastore_config = RedisDataStoreConfig(\n\t    host=REDIS_HOST,\n\t    port=REDIS_PORT,\n\t    password=REDIS_PASSWORD,\n", ")\n\t# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n\tredis_datastore = RedisDataStore(config=redis_datastore_config)\n\t# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\n\tmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\t# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\n\tchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\t# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\n\tchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\t# Initialize conversation_id to None\n", "conversation_id = None\n\t# Start the chatbot loop\n\twhile True:\n\t    # Prompt the user for input\n\t    user_message = input(\"\\n Please enter your message: \")\n\t    # Use the ChatGPTClient object to generate a response\n\t    response = chat_gpt_client.converse(message=user_message, conversation_id=conversation_id)\n\t    # Update the conversation_id with the conversation_id from the response\n\t    conversation_id = response.conversation_id\n\t    # Print the response generated by the chatbot\n", "    print(response.chat_gpt_answer)\n"]}
{"filename": "examples/paper_highlights/paper_highlights_bullets.py", "chunked_list": ["import numpy as np\n\tfrom acad_gpt.datastore import RedisDataStore, RedisDataStoreConfig\n\t## set the following ENVIRONMENT Variables before running this script\n\t# Import necessary modules\n\tfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\n\tfrom acad_gpt.parsers import ParserConfig, PDFParser\n\tif __name__ == \"__main__\":\n\t    # Instantiate an EmbeddingConfig object with the OpenAI API key\n\t    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n", "    # Instantiate an EmbeddingClient object with the EmbeddingConfig object\n\t    embed_client = EmbeddingClient(config=embedding_config)\n\t    # Instantiate a RedisDataStoreConfig object with the Redis connection details\n\t    redis_datastore_config = RedisDataStoreConfig(\n\t        host=REDIS_HOST,\n\t        port=REDIS_PORT,\n\t        password=REDIS_PASSWORD,\n\t    )\n\t    # Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n\t    redis_datastore = RedisDataStore(config=redis_datastore_config)\n", "    parser = PDFParser()\n\t    parser_config = ParserConfig(\n\t        file_path_or_url=\"examples/paper_highlights/pdf/paper.pdf\", file_type=\"PDF\", extract_figures=True\n\t    )\n\t    results = parser.parse(config=parser_config)\n\t    documents = parser.pdf_to_documents(pdf_contents=results, embed_client=embed_client, file_name=\"paper.pdf\")\n\t    redis_datastore.index_documents(documents)\n\t    while True:\n\t        query = input(\"Enter your query: \")\n\t        query_vector = embed_client.embed_queries(queries=[query])[0].astype(np.float32).tobytes()\n", "        print(redis_datastore.search_documents(query_vector=query_vector))\n"]}
{"filename": "acad_gpt/environment.py", "chunked_list": ["import os\n\timport dotenv\n\t# Load environment variables from .env file\n\t_TESTING = os.getenv(\"CHATGPT_MEMORY_TESTING\", False)\n\tif _TESTING:\n\t    # for testing we use the .env.example file instead\n\t    dotenv.load_dotenv(dotenv.find_dotenv(\".env.example\"))\n\telse:\n\t    dotenv.load_dotenv()\n\t# Any remote API (OpenAI, Cohere etc.)\n", "OPENAI_TIMEOUT = float(os.getenv(\"REMOTE_API_TIMEOUT_SEC\", 30))\n\tOPENAI_BACKOFF = float(os.getenv(\"REMOTE_API_BACKOFF_SEC\", 10))\n\tOPENAI_MAX_RETRIES = int(os.getenv(\"REMOTE_API_MAX_RETRIES\", 5))\n\tOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\t# Cloud data store (Redis, Pinecone etc.)\n\tREDIS_HOST = os.getenv(\"REDIS_HOST\")\n\tREDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))\n\tREDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n\t# configure discord bot\n\tDISCORD_BOOKMARK_BOT_TOKEN = os.getenv(\"DISCORD_BOOKMARK_TOKEN\")\n", "DISCORD_BOOKMARK_BOT_GUILD_ID = int(os.getenv(\"DISCORD_BOOKMARK_BOT_GUILD_ID\"))\n\t# configure elastic search\n\tES_URL = os.getenv(\"ES_URL\")\n\tES_USERNAME = os.getenv(\"ES_USERNAME\")\n\tES_PASSWORD = os.getenv(\"ES_PASSWORD\")\n\tES_INDEX = os.getenv(\"ES_INDEX\", \"discord-bookmark\")\n\tES_PORT = int(os.getenv(\"ES_PORT\", 443))\n\t# HF Token\n\t# HF_TOKEN = os.getenv(\"HF_TOKEN\")\n\t# assert (\n", "#     os.getenv(\"HF_REPO\", None) is not None\n\t# ), \"\"\"\n\t# Environment variable `HF_REPO` should be set prior to running acad-gpt.\n\t# You'd need to create a new model on the huggingface hub using the url:\n\t# `https://huggingface.co/new`\n\t# then set `HF_REPO` variables value to: HF_REPO={hf_username}/{hf_model_name}\n\t# \"\"\"\n\t# HF_REPO = os.getenv(\"HF_REPO\")\n\t# HF_ENDPOINT = os.getenv(\"HF_ENDPOINT\", \"https://huggingface.co\")\n\t# API Config\n", "DEFAULT_PATH = str(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"examples/paper_highlights/pdf\")))\n\tFILE_UPLOAD_PATH = os.getenv(\"FILE_UPLOAD_PATH\", DEFAULT_PATH)\n"]}
{"filename": "acad_gpt/errors.py", "chunked_list": ["\"\"\"Custom Errors for ChatGptMemory\"\"\"\n\tfrom typing import Optional\n\tclass ChatGPTMemoryError(Exception):\n\t    \"\"\"\n\t    Any error generated by ChatGptMemory.\n\t    This error wraps its source transparently in such a way that its attributes\n\t    can be accessed directly: for example, if the original error has a `message`\n\t    attribute.\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        message: Optional[str] = None,\n\t    ):\n\t        super().__init__()\n\t        if message:\n\t            self.message = message\n\t    def __getattr__(self, attr):\n\t        # If self.__cause__ is None, it will raise the expected AttributeError\n\t        getattr(self.__cause__, attr)\n\t    def __repr__(self):\n", "        return str(self)\n\tclass OpenAIError(ChatGPTMemoryError):\n\t    \"\"\"Exception for issues that occur in the OpenAI APIs\"\"\"\n\t    def __init__(\n\t        self,\n\t        message: Optional[str] = None,\n\t        status_code: Optional[int] = None,\n\t    ):\n\t        super().__init__(message=message)\n\t        self.status_code = status_code\n", "class OpenAIRateLimitError(OpenAIError):\n\t    \"\"\"\n\t    Rate limit error for OpenAI API (status code 429), See below:\n\t    https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors\n\t    https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n\t    \"\"\"\n\t    def __init__(self, message: Optional[str] = None):\n\t        super().__init__(message=message, status_code=429)\n\t    def __repr__(self):\n\t        return f\"message= {self.message}, status_code={self.status_code}\"\n"]}
{"filename": "acad_gpt/__init__.py", "chunked_list": []}
{"filename": "acad_gpt/constants.py", "chunked_list": ["# LLM Config related\n\t\"\"\"\n\tif OpenAI embedding model type is \"*-001\" the set max sequence length to `2046`,\n\totherwise for type \"*-002\" set `8191`\n\t\"\"\"\n\tMAX_ALLOWED_SEQ_LEN_001 = 2046\n\tMAX_ALLOWED_SEQ_LEN_002 = 8191\n"]}
{"filename": "acad_gpt/utils/reflection.py", "chunked_list": ["import inspect\n\timport logging\n\timport time\n\tfrom random import random\n\tfrom typing import Any, Callable, Dict, Tuple\n\tfrom acad_gpt.errors import OpenAIRateLimitError\n\tlogger = logging.getLogger(__name__)\n\tdef args_to_kwargs(args: Tuple, func: Callable) -> Dict[str, Any]:\n\t    sig = inspect.signature(func)\n\t    arg_names = list(sig.parameters.keys())\n", "    # skip self and cls args for instance and class methods\n\t    if any(arg_names) and arg_names[0] in [\"self\", \"cls\"]:\n\t        arg_names = arg_names[1 : 1 + len(args)]\n\t    args_as_kwargs = {arg_name: arg for arg, arg_name in zip(args, arg_names)}\n\t    return args_as_kwargs\n\tdef retry_with_exponential_backoff(\n\t    backoff_in_seconds: float = 1,\n\t    max_retries: int = 10,\n\t    errors: tuple = (OpenAIRateLimitError,),\n\t):\n", "    \"\"\"\n\t    Decorator to retry a function with exponential backoff.\n\t    :param backoff_in_seconds: The initial backoff in seconds.\n\t    :param max_retries: The maximum number of retries.\n\t    :param errors: The errors to catch retry on.\n\t    \"\"\"\n\t    def decorator(function):\n\t        def wrapper(*args, **kwargs):\n\t            # Initialize variables\n\t            num_retries = 0\n", "            # Loop until a successful response or max_retries is hit or an\n\t            # exception is raised\n\t            while True:\n\t                try:\n\t                    return function(*args, **kwargs)\n\t                # Retry on specified errors\n\t                except errors as e:\n\t                    # Check if max retries has been reached\n\t                    if num_retries > max_retries:\n\t                        raise Exception(f\"Maximum number of retries ({max_retries}) exceeded.\")\n", "                    # Increment the delay\n\t                    sleep_time = backoff_in_seconds * 2**num_retries + random()\n\t                    # Sleep for the delay\n\t                    logger.warning(\n\t                        \"%s - %s, retry %s in %s seconds...\",\n\t                        e.__class__.__name__,\n\t                        e,\n\t                        function.__name__,\n\t                        \"{0:.2f}\".format(sleep_time),\n\t                    )\n", "                    time.sleep(sleep_time)\n\t                    # Increment retries\n\t                    num_retries += 1\n\t        return wrapper\n\t    return decorator\n"]}
{"filename": "acad_gpt/utils/pdf_column_classifier.py", "chunked_list": ["\"\"\"\n\tThis module contains a function for classifying a PDF as either single-column or two-column.\n\tThe classification is based on simple heuristics, and is not guaranteed to be accurate.\n\tIt is done as follows:\n\t1. Convert the PDF to a list of images.\n\t2. For each image, calculate the percentage of whitish pixels in the middle of the image.\n\t3. If the percentage of whitish pixels in the middle of the image is greater than `VOTE_THRESHOLD`,\n\t    then the image is two-column.\n\t4. If the majority of the images in the PDF are classified as two-column, then the PDF is two-column.\n\t\"\"\"\n", "from pathlib import Path\n\tfrom typing import List, Union\n\timport numpy as np\n\timport pdf2image\n\tfrom PIL import Image, ImageFilter\n\t__all__ = [\"classify_pdf_from_path\", \"classify_pdf_from_bytes\"]\n\tDEFAULT_DPI = 50\n\tDEFAULT_MIDDLE_COLUMN_WIDTH = 2\n\tEROSION_KERNEL_SIZE = 5\n\tVOTE_THRESHOLD = 0.7\n", "PDF_CONVERSION_OPTIONS = dict(\n\t    grayscale=True,\n\t    hide_annotations=True,\n\t    paths_only=True,\n\t)\n\tdef classify_pdf_from_path(pdf_path: Union[str, Path], dpi: int = DEFAULT_DPI) -> bool:\n\t    \"\"\"\n\t    Classify a PDF as either single-column or two-column.\n\t    Args:\n\t        pdf_path: The PDF to classify.\n", "        dpi: The DPI to use when converting the PDF to images. Defaults to DEFAULT_DPI (50).\n\t    Returns:\n\t        True if the PDF is two-column, False otherwise.\n\t    \"\"\"\n\t    # first, convert the PDF to a list of images.\n\t    imgs = pdf2image.convert_from_path(pdf_path, dpi=dpi, **PDF_CONVERSION_OPTIONS)\n\t    return _classify_imgs(imgs)\n\tdef classify_pdf_from_bytes(pdf_bytes: bytes, dpi: int = DEFAULT_DPI) -> bool:\n\t    \"\"\"\n\t    Classify a PDF as either single-column or two-column.\n", "    Args:\n\t        pdf_bytes: The PDF as bytes to classify.\n\t        dpi: The DPI to use when converting the PDF to images. Defaults to DEFAULT_DPI (50).\n\t    Returns:\n\t        True if the PDF is two-column, False otherwise.\n\t    \"\"\"\n\t    # first, convert the PDF to a list of images.\n\t    imgs = pdf2image.convert_from_bytes(pdf_bytes, dpi=dpi, **PDF_CONVERSION_OPTIONS)\n\t    return _classify_imgs(imgs)\n\tdef _classify_imgs(imgs: List[Image]) -> bool:\n", "    \"\"\"\n\t    Classify a list of images as either single-column or two-column.\n\t    Args:\n\t        imgs: The images to classify.\n\t    Returns:\n\t        True if the images are two-column, False otherwise.\n\t    \"\"\"\n\t    # classify each image in the PDF as either single-column or two-column.\n\t    votes = []\n\t    for img in imgs:\n", "        # TODO: should we consider image pdfs? (we're not considering doing OCR anyways)\n\t        # apply erosion to make the text more prominent\n\t        if EROSION_KERNEL_SIZE > 1:\n\t            img = img.filter(ImageFilter.MinFilter(EROSION_KERNEL_SIZE))\n\t        # calculate the percentage of whitish pixels in the middle of the image\n\t        percentage_whitish_in_middle = _calc_percentage_whitish_in_middle(img)\n\t        # if the percentage of whitish pixels in the middle of the\n\t        # image is greater than VOTE_THRESHOLD, then the image is two-column.\n\t        vote = percentage_whitish_in_middle > VOTE_THRESHOLD\n\t        # add the vote to list\n", "        votes.append(vote)\n\t    # if the majority of the images in the PDF are classified as two-column, then the PDF is two-column.\n\t    return sum(votes) > len(votes) // 2\n\tdef _calc_percentage_whitish_in_middle(img: Image, middle_column_width: int = DEFAULT_MIDDLE_COLUMN_WIDTH) -> float:\n\t    \"\"\"\n\t    Calculate the percentage of whitish pixels in the middle of the image.\n\t    Args:\n\t        img: The image to calculate the percentage of whitish pixels in the middle of.\n\t        middle_column_width: The width of the middle column to calculate the percentage of whitish pixels in.\n\t            Defaults to DEFAULT_MIDDLE_COLUMN_WIDTH (2).\n", "    Returns:\n\t        The percentage of whitish pixels in the middle of the image.\n\t    \"\"\"\n\t    img = img.convert(\"L\")\n\t    middle_column = img.crop(\n\t        (img.width // 2 - middle_column_width // 2, 0, img.width // 2 + middle_column_width // 2, img.height)\n\t    )\n\t    middle_column = middle_column.point(lambda x: 0 if x < 150 else 255, mode=\"1\")\n\t    return np.array(middle_column).mean()\n"]}
{"filename": "acad_gpt/utils/openai_utils.py", "chunked_list": ["\"\"\"Utils for using OpenAI API\"\"\"\n\timport json\n\timport logging\n\tfrom typing import Any, Dict, Tuple, Union\n\timport requests\n\tfrom transformers import GPT2TokenizerFast\n\tfrom acad_gpt.environment import OPENAI_BACKOFF, OPENAI_MAX_RETRIES, OPENAI_TIMEOUT\n\tfrom acad_gpt.errors import OpenAIError, OpenAIRateLimitError\n\tfrom acad_gpt.utils.reflection import retry_with_exponential_backoff\n\tlogger = logging.getLogger(__name__)\n", "def load_openai_tokenizer(tokenizer_name: str, use_tiktoken: bool) -> Any:\n\t    \"\"\"\n\t    Load either the tokenizer from tiktoken (if the library is available) or\n\t    fallback to the GPT2TokenizerFast from the transformers library.\n\t    Args:\n\t        tokenizer_name (str): The name of the tokenizer to load.\n\t        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\t    Raises:\n\t        ImportError: When `tiktoken` package is missing.\n\t        To use tiktoken tokenizer install it as follows:\n", "        `pip install tiktoken`\n\t    Returns:\n\t        tokenizer: Tokenizer of either GPT2 kind or tiktoken based.\n\t    \"\"\"\n\t    tokenizer = None\n\t    if use_tiktoken:\n\t        try:\n\t            import tiktoken  # pylint: disable=import-error\n\t            logger.debug(\"Using tiktoken %s tokenizer\", tokenizer_name)\n\t            tokenizer = tiktoken.get_encoding(tokenizer_name)\n", "        except ImportError:\n\t            raise ImportError(\n\t                \"The `tiktoken` package not found.\",\n\t                \"To install it use the following:\",\n\t                \"`pip install tiktoken`\",\n\t            )\n\t    else:\n\t        logger.warning(\n\t            \"OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and \"\n\t            \"AARCH64. Falling back to GPT2TokenizerFast.\"\n", "        )\n\t        logger.debug(\"Using GPT2TokenizerFast tokenizer\")\n\t        tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)\n\t    return tokenizer\n\tdef count_openai_tokens(text: str, tokenizer: Any, use_tiktoken: bool) -> int:\n\t    \"\"\"\n\t    Count the number of tokens in `text` based on the provided OpenAI `tokenizer`.\n\t    Args:\n\t        text (str):  A string to be tokenized.\n\t        tokenizer (Any): An OpenAI tokenizer.\n", "        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\t    Returns:\n\t        int: Number of tokens in the text.\n\t    \"\"\"\n\t    if use_tiktoken:\n\t        return len(tokenizer.encode(text))\n\t    else:\n\t        return len(tokenizer.tokenize(text))\n\t@retry_with_exponential_backoff(\n\t    backoff_in_seconds=OPENAI_BACKOFF,\n", "    max_retries=OPENAI_MAX_RETRIES,\n\t    errors=(OpenAIRateLimitError, OpenAIError),\n\t)\n\tdef openai_request(\n\t    url: str,\n\t    headers: Dict,\n\t    payload: Dict,\n\t    timeout: Union[float, Tuple[float, float]] = OPENAI_TIMEOUT,\n\t) -> Dict:\n\t    \"\"\"\n", "    Make a request to the OpenAI API given a `url`, `headers`, `payload`, and\n\t    `timeout`.\n\t    Args:\n\t        url (str): The URL of the OpenAI API.\n\t        headers (Dict): Dictionary of HTTP Headers to send with the :class:`Request`.\n\t        payload (Dict): The payload to send with the request.\n\t        timeout (Union[float, Tuple[float, float]], optional): The timeout length of the request. The default is 30s.\n\t        Defaults to OPENAI_TIMEOUT.\n\t    Raises:\n\t        openai_error: If the request fails.\n", "    Returns:\n\t        Dict: OpenAI Embedding API response.\n\t    \"\"\"\n\t    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload), timeout=timeout)\n\t    res = json.loads(response.text)\n\t    # if request is unsucessful and `status_code = 429` then,\n\t    # raise rate limiting error else the OpenAIError\n\t    if response.status_code != 200:\n\t        openai_error: OpenAIError\n\t        if response.status_code == 429:\n", "            openai_error = OpenAIRateLimitError(f\"API rate limit exceeded: {response.text}\")\n\t        else:\n\t            openai_error = OpenAIError(\n\t                f\"OpenAI returned an error.\\n\"\n\t                f\"Status code: {response.status_code}\\n\"\n\t                f\"Response body: {response.text}\",\n\t                status_code=response.status_code,\n\t            )\n\t        raise openai_error\n\t    return res\n", "def get_prompt(message: str, history: str) -> str:\n\t    \"\"\"\n\t    Generates the prompt based on the current history and message.\n\t    Args:\n\t        message (str): Current message from user.\n\t        history (str): Retrieved history for the current message.\n\t        History follows the following format for example:\n\t        ```\n\t        Human: hello\n\t        Assistant: hello, how are you?\n", "        Human: good, you?\n\t        Assistant: I am doing good as well. How may I help you?\n\t        ```\n\t    Returns:\n\t        prompt: Curated prompt for the ChatGPT API based on current params.\n\t    \"\"\"\n\t    prompt = f\"\"\"Assistant is a large language model trained by OpenAI.\n\t    Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\t    Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\t    Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n", "    {history}\n\t    Human: {message}\n\t    Assistant:\"\"\"\n\t    return prompt\n"]}
{"filename": "acad_gpt/datastore/config.py", "chunked_list": ["from enum import Enum\n\tfrom pydantic import BaseModel\n\tclass RedisIndexType(str, Enum):\n\t    hnsw = \"HNSW\"\n\t    flat = \"FLAT\"\n\tclass DataStoreConfig(BaseModel):\n\t    host: str\n\t    port: int\n\t    password: str\n\tclass RedisDataStoreConfig(DataStoreConfig):\n", "    index_type: str = RedisIndexType.hnsw\n\t    vector_field_name: str = \"embedding\"\n\t    vector_dimensions: int = 1024\n\t    distance_metric: str = \"L2\"\n\t    number_of_vectors: int = 686\n\t    M: int = 40\n\t    EF: int = 200\n\tclass ElasticSearchStoreConfig(DataStoreConfig):\n\t    username: str\n\t    index_name: str\n"]}
{"filename": "acad_gpt/datastore/elasticsearch.py", "chunked_list": ["import hashlib\n\timport logging\n\tfrom typing import Any, Dict, List\n\tfrom elasticsearch import Elasticsearch, RequestsHttpConnection\n\tfrom acad_gpt.datastore.config import ElasticSearchStoreConfig\n\tfrom acad_gpt.datastore.datastore import DataStore\n\tlogger = logging.getLogger(__name__)\n\tclass ElasticSearchDataStore(DataStore):\n\t    def __init__(self, config: ElasticSearchStoreConfig, **kwargs):\n\t        super().__init__(config=config)\n", "        self.config = config\n\t        self.connect()\n\t        self.create_index()\n\t    def connect(self):\n\t        \"\"\"\n\t        Connect to the ES server.\n\t        \"\"\"\n\t        self.es_connection = Elasticsearch(\n\t            self.config.host,\n\t            port=self.config.port,\n", "            connection_class=RequestsHttpConnection,\n\t            http_auth=(self.config.username, self.config.password),\n\t            use_ssl=True,\n\t            verify_certs=False,\n\t        )\n\t    def create_index(self):\n\t        \"\"\"\n\t        Creates a ES index.\n\t        \"\"\"\n\t        try:\n", "            ES_INDEX_MAPPING = {\n\t                \"properties\": {\n\t                    \"title\": {\"type\": \"text\"},\n\t                    \"text\": {\"type\": \"text\"},\n\t                    \"url\": {\"type\": \"text\"},\n\t                    \"status\": {\"type\": \"keyword\"},\n\t                    \"type\": {\"type\": \"keyword\"},\n\t                }\n\t            }\n\t            if not self.es_connection.indices.exists(self.config.index_name):\n", "                self.es_client.indices.create(index=self.config.index_name, body={\"mappings\": ES_INDEX_MAPPING})\n\t            logger.info(\"Created a new ES index\")\n\t        except Exception as es_error:\n\t            logger.info(f\"Working with existing ES index.\\nDetails: {es_error}\")\n\t    def index_documents(self, documents: List[Dict]):\n\t        \"\"\"\n\t        Indexes the set of documents.\n\t        Args:\n\t            documents (List[Dict]): List of documents to be indexed.\n\t        \"\"\"\n", "        for document in documents:\n\t            try:\n\t                assert \"text\" in document and \"url\" in document, \"Document must include the fields `text` and `url`\"\n\t                sha = hashlib.sha256()\n\t                sha.update(document.get(\"url\").encode())\n\t                self.es_connection.update(\n\t                    index=self.config.index_name, id=sha.hexdigest(), body={\"doc\": document, \"doc_as_upsert\": True}\n\t                )\n\t            except Exception as es_error:\n\t                logger.info(f\"Error in indexing document.\\nDetails: {es_error}\")\n", "    def search_documents(self, query: str, topk: int = 5, **kwargs) -> List[Any]:\n\t        \"\"\"\n\t        Searches the redis index using the query vector.\n\t        Args:\n\t            query_vector (np.ndarray): Embedded query vector.\n\t            topk (int, optional): Number of results. Defaults to 5.\n\t        Returns:\n\t            List[Any]: Search result documents.\n\t        \"\"\"\n\t        filter = [{\"match\": {\"text\": query}}]\n", "        status = kwargs.get(\"status\")\n\t        if status:\n\t            filter.append({\"match\": {\"status\": status}})\n\t        es_query = {\n\t            \"query\": {\"bool\": {\"must\": filter}},\n\t            \"size\": topk,\n\t        }\n\t        topk_hits = []\n\t        es_response = self.es_connection.search(index=self.config.index_name, body=es_query)\n\t        for inter_res in es_response[\"hits\"][\"hits\"]:\n", "            topk_hits.append(inter_res[\"_source\"])\n\t        return topk_hits\n\t    def delete_document(self, url: str):\n\t        \"\"\"\n\t        Deletes all documents for a given conversation id.\n\t        Args:\n\t            document_id (str): Id of the conversation to be deleted.\n\t        \"\"\"\n\t        sha = hashlib.sha256()\n\t        sha.update(url.encode())\n", "        try:\n\t            self.es_connection.delete(index=self.es_connection, id=sha.hexdigest())\n\t        except Exception as es_error:\n\t            logger.info(f\"Error in deleting document.\\nDetails: {es_error}\")\n"]}
{"filename": "acad_gpt/datastore/__init__.py", "chunked_list": ["from acad_gpt.datastore.config import (  # noqa: F401\n\t    DataStoreConfig,\n\t    ElasticSearchStoreConfig,\n\t    RedisDataStoreConfig,\n\t    RedisIndexType,\n\t)\n\tfrom acad_gpt.datastore.elasticsearch import ElasticSearchDataStore  # noqa: F401\n\tfrom acad_gpt.datastore.redis import RedisDataStore  # noqa: F401\n"]}
{"filename": "acad_gpt/datastore/redis.py", "chunked_list": ["import hashlib\n\timport logging\n\tfrom typing import Any, Dict, List\n\timport redis\n\tfrom redis.commands.search.field import TagField, TextField, VectorField\n\tfrom redis.commands.search.query import Query\n\tfrom acad_gpt.datastore.config import RedisDataStoreConfig\n\tfrom acad_gpt.datastore.datastore import DataStore\n\tlogger = logging.getLogger(__name__)\n\tclass RedisDataStore(DataStore):\n", "    def __init__(self, config: RedisDataStoreConfig, do_flush_data: bool = False, **kwargs):\n\t        super().__init__(config=config)\n\t        self.config = config\n\t        self.do_flush_data = do_flush_data\n\t        self.connect()\n\t        self.create_index()\n\t    def connect(self):\n\t        \"\"\"\n\t        Connect to the Redis server.\n\t        \"\"\"\n", "        connection_pool = redis.ConnectionPool(\n\t            host=self.config.host, port=self.config.port, password=self.config.password\n\t        )\n\t        self.redis_connection = redis.Redis(connection_pool=connection_pool)\n\t        # flush data only once after establishing connection\n\t        if self.do_flush_data:\n\t            self.flush_all_documents()\n\t            self.do_flush_data = False\n\t    def flush_all_documents(self):\n\t        \"\"\"\n", "        Removes all documents from the redis index.\n\t        \"\"\"\n\t        self.redis_connection.flushall()\n\t    def create_index(self):\n\t        \"\"\"\n\t        Creates a Redis index with a dense vector field.\n\t        \"\"\"\n\t        try:\n\t            self.redis_connection.ft().create_index(\n\t                [\n", "                    VectorField(\n\t                        self.config.vector_field_name,\n\t                        self.config.index_type,\n\t                        {\n\t                            \"TYPE\": \"FLOAT32\",\n\t                            \"DIM\": self.config.vector_dimensions,\n\t                            \"DISTANCE_METRIC\": self.config.distance_metric,\n\t                            \"INITIAL_CAP\": self.config.number_of_vectors,\n\t                            \"M\": self.config.M,\n\t                            \"EF_CONSTRUCTION\": self.config.EF,\n", "                        },\n\t                    ),\n\t                    TextField(\"text\"),  # contains the original message\n\t                    TextField(\"title\"),\n\t                    TagField(\"type\"),\n\t                    TagField(\"status\"),\n\t                    TextField(\"url\"),\n\t                ]\n\t            )\n\t            logger.info(\"Created a new Redis index for storing chat history\")\n", "        except redis.exceptions.ResponseError as redis_error:\n\t            logger.info(f\"Working with existing Redis index.\\nDetails: {redis_error}\")\n\t    def index_documents(self, documents: List[Dict]):\n\t        \"\"\"\n\t        Indexes the set of documents.\n\t        Args:\n\t            documents (List[Dict]): List of documents to be indexed.\n\t        \"\"\"\n\t        redis_pipeline = self.redis_connection.pipeline(transaction=False)\n\t        for document in documents:\n", "            assert \"text\" in document and \"url\" in document, \"Document must include the fields `text`, and `url`\"\n\t            sha = hashlib.sha256()\n\t            sha.update(document.get(\"url\").encode())\n\t            redis_pipeline.hset(sha.hexdigest(), mapping=document)\n\t        redis_pipeline.execute()\n\t    def search_documents(self, query: bytes, topk: int = 5, **kwargs) -> List[Any]:\n\t        \"\"\"\n\t        Searches the redis index using the query vector.\n\t        Args:\n\t            query_vector (np.ndarray): Embedded query vector.\n", "            topk (int, optional): Number of results. Defaults to 5.\n\t        Returns:\n\t            List[Any]: Search result documents.\n\t        \"\"\"\n\t        status = kwargs.get(\"status\", None)\n\t        type = kwargs.get(\"type\", None)\n\t        tag = \"(\"\n\t        if status:\n\t            tag += f\"@status:{{{status}}}\"\n\t        if type:\n", "            tag += f\"@type:{{{type}}}\"\n\t        tag += \")\"\n\t        # if no tags are selected\n\t        if len(tag) < 3:\n\t            tag = \"*\"\n\t        query_obj = (\n\t            Query(\n\t                f\"\"\"{tag}=>[KNN {topk} \\\n\t                    @{self.config.vector_field_name} $vec_param AS vector_score]\"\"\"\n\t            )\n", "            .sort_by(\"vector_score\")\n\t            .paging(0, topk)\n\t            .return_fields(\n\t                \"vector_score\",\n\t                \"text\",\n\t                \"title\",\n\t                \"type\",\n\t                \"status\",\n\t                \"url\",\n\t            )\n", "            .dialect(2)\n\t        )\n\t        params_dict = {\"vec_param\": query}\n\t        try:\n\t            result_documents = self.redis_connection.ft().search(query_obj, query_params=params_dict).docs\n\t        except redis.exceptions.ResponseError as redis_error:\n\t            logger.info(f\"Details: {redis_error}\")\n\t        return result_documents\n\t    def get_all_document_ids(self) -> List[str]:\n\t        \"\"\"\n", "        Returns document titles of all documents.\n\t        Returns:\n\t            List[str]: List of document titles stored in redis.\n\t        \"\"\"\n\t        query = Query(\"*\").return_fields(\"title\")\n\t        result_documents = self.redis_connection.ft().search(query).docs\n\t        document_ids: List[str] = []\n\t        document_ids = list(set([getattr(result_document, \"document_id\") for result_document in result_documents]))\n\t        return document_ids\n\t    def delete_documents(self, document_id: str):\n", "        \"\"\"\n\t        Deletes all documents for a given conversation id.\n\t        Args:\n\t            document_id (str): Id of the conversation to be deleted.\n\t        \"\"\"\n\t        query = (\n\t            Query(f\"\"\"(@document_id:{{{document_id}}})\"\"\")\n\t            .return_fields(\n\t                \"id\",\n\t            )\n", "            .dialect(2)\n\t        )\n\t        for document in self.redis_connection.ft().search(query).docs:\n\t            document_id = getattr(document, \"id\")\n\t            deletion_status = self.redis_connection.ft().delete_document(document_id, delete_actual_document=True)\n\t            assert deletion_status, f\"Deletion of the document with id {document_id} failed!\"\n"]}
{"filename": "acad_gpt/datastore/datastore.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Any, Dict, List\n\tfrom acad_gpt.datastore.config import DataStoreConfig\n\tclass DataStore(ABC):\n\t    \"\"\"\n\t    Abstract class for datastores.\n\t    \"\"\"\n\t    def __init__(self, config: DataStoreConfig):\n\t        self.config = config\n\t    @abstractmethod\n", "    def connect(self):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def create_index(self):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def index_documents(self, documents: List[Dict]):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def search_documents(self, query: Any, topk: int, **kwargs) -> List[Any]:\n", "        raise NotImplementedError\n"]}
{"filename": "acad_gpt/parsers/config.py", "chunked_list": ["from pydantic import BaseModel\n\tclass ParserConfig(BaseModel):\n\t    file_path: str\n\t    file_url: str\n\tclass PDF2ImageConversionOptions(BaseModel):\n\t    grayscale: bool = True\n\t    hide_annotations: bool = True\n\t    paths_only: bool = True\n\t    last_page: int = 5\n\tclass PDFColumnClassifierConfig(BaseModel):\n", "    dpi: int = 50\n\t    middle_column_width: int = 2\n\t    erosion_kernel_size: int = 5\n\t    vote_threshold: float = 0.7\n\t    pdf_conversion_options: PDF2ImageConversionOptions = PDF2ImageConversionOptions()\n"]}
{"filename": "acad_gpt/parsers/__init__.py", "chunked_list": ["from acad_gpt.parsers.base_parser import BaseParser, Document, DocumentType  # noqa: 401\n\tfrom acad_gpt.parsers.config import ParserConfig  # noqa: 401\n\tfrom acad_gpt.parsers.pdf_parser import PDFParser  # noqa: 401\n\tfrom acad_gpt.parsers.webpage_parser import WebPageParser  # noqa: 401\n"]}
{"filename": "acad_gpt/parsers/pdf_parser_deprecated.py", "chunked_list": ["# \"\"\"\n\t# pdfannots = \"^0.4\"\n\t# PyMuPDF = \"^1.18.13\"\n\t# pillow = \"^9.5.0\"\n\t# pdf2image = \"^1.16.3\"\n\t# \"\"\"\n\t# import json\n\t# import logging\n\t# import os\n\t# import shutil\n", "# from pathlib import Path\n\t# from typing import Any, Dict, List, Union\n\t# from uuid import uuid4\n\t# import numpy as np\n\t# import pdf2image\n\t# import pdfannots\n\t# import pdfminer\n\t# import scipdf\n\t# from PIL import Image, ImageFilter\n\t# from acad_gpt.environment import FILE_UPLOAD_PATH\n", "# from acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\t# from acad_gpt.parsers.base_parser import BaseParser, Document, DocumentType\n\t# from acad_gpt.parsers.config import ParserConfig, PDFColumnClassifierConfig\n\t# logger = logging.getLogger(__name__)\n\t# class PDFParser(BaseParser):\n\t#     def parse(self, config: Union[ParserConfig, List[ParserConfig]]) -> Dict:\n\t#         if not isinstance(config, List):\n\t#             config = [config]\n\t#         parsed_content: Dict[str, Any] = {}\n\t#         for config_item in config:\n", "#             file_path = getattr(config_item, \"file_path_or_url\")\n\t#             extract_highlights = getattr(config_item, \"extract_highlights\")\n\t#             pdf_metadata, metadata_path = PDFParser.get_pdf_metadata(\n\t#                 file_path=file_path, extract_figures=config_item.extract_figures\n\t#             )\n\t#             pdf_metadata[\"file_name\"] = os.path.basename(file_path)\n\t#             pdf_metadata[\"url\"] = file_path\n\t#             parsed_content = {\n\t#                 \"metadata\": pdf_metadata,\n\t#                 \"metadata_path\": metadata_path if config_item.extract_figures else None,\n", "#             }\n\t#             if extract_highlights:\n\t#                 pdf_layout = 2 if PDFParser.classify_pdf_from_path(pdf_path=file_path) else 1\n\t#                 parsed_content[\"highlights\"] = PDFParser.get_pdf_highlights(file_path=file_path, pdf_layout=pdf_layout)\n\t#         return parsed_content\n\t#     @staticmethod\n\t#     def get_pdf_highlights(file_path: str, pdf_layout: int) -> List[Dict]:\n\t#         highlights: List[Dict] = []\n\t#         path = Path(file_path)\n\t#         laparams = pdfminer.layout.LAParams()\n", "#         with path.open(\"rb\") as f:\n\t#             doc = pdfannots.process_file(f, columns_per_page=pdf_layout, laparams=laparams)\n\t#         for page in doc.pages:\n\t#             for annotation in page.annots:\n\t#                 try:\n\t#                     highlights.append(\n\t#                         {\n\t#                             \"text\": annotation.gettext(remove_hyphens=True),\n\t#                             \"page\": annotation.pos.page.pageno + 1,\n\t#                             \"regionBoundary\": [annotation.pos.x, annotation.pos.y],\n", "#                         }\n\t#                     )\n\t#                 except Exception:\n\t#                     continue\n\t#         return highlights\n\t#     @staticmethod\n\t#     def get_pdf_metadata(file_path: str, extract_figures: bool = False, figures_directory: str = FILE_UPLOAD_PATH):\n\t#         pdf_metadata = None\n\t#         try:\n\t#             pdf_metadata = scipdf.parse_pdf_to_dict(file_path)\n", "#             if extract_figures:\n\t#                 isExist = os.path.exists(figures_directory)\n\t#                 if not isExist:\n\t#                     # create the metadata directory because it does not exist\n\t#                     os.makedirs(figures_directory)\n\t#                 # folder should contain only PDF files\n\t#                 scipdf.parse_figures(file_path, output_folder=figures_directory)\n\t#                 # TODO: clean up extracted images after storing them in object storage\n\t#         except Exception as error:\n\t#             logger.error(error)\n", "#         return pdf_metadata, figures_directory\n\t#     def pdf_to_documents(\n\t#         self,\n\t#         pdf_contents: Dict,\n\t#         embed_client: EmbeddingClient,\n\t#         file_name: str,\n\t#         clean_up: bool = True,\n\t#         extract_highlights: bool = False,\n\t#     ) -> List[Dict]:\n\t#         metadata_path = pdf_contents.pop(\"metadata_path\")\n", "#         documents = []\n\t#         metadata = dict(pdf_contents.pop(\"metadata\")) if pdf_contents.get(\"metadata\") else None\n\t#         document_id = metadata.get(\"file_name\") if metadata else None\n\t#         url = metadata.get(\"file_name\", \"\")\n\t#         title = metadata.pop(\"title\", \"\")\n\t#         if metadata and metadata_path:\n\t#             abstract = metadata.pop(\"abstract\")\n\t#             sections = \"\\n\".join([section.get(\"text\", \"\") for section in metadata.pop(\"sections\", [])])\n\t#             embedding = (\n\t#                 embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  Abstract: \\n  {abstract}\"}])[0]\n", "#                 .astype(np.float32)\n\t#                 .tobytes()\n\t#             )\n\t#             abstract_doc = Document(\n\t#                 text=f\"{abstract} {sections}\",\n\t#                 title=title,\n\t#                 type=DocumentType.paper,\n\t#                 url=url,\n\t#                 embedding=embedding,\n\t#             ).dict()\n", "#             documents = [abstract_doc]\n\t#         if \"highlights\" in pdf_contents:\n\t#             for section in sections:\n\t#                 section_name = section.get(\"heading\", \"\")\n\t#                 text = section.get(\"text\", \"\")\n\t#                 embedding = (\n\t#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n {section_name}: \\n  {text}\"}])[0]\n\t#                     .astype(np.float32)\n\t#                     .tobytes()\n\t#                 )\n", "#                 document = Document(\n\t#                     document_id=document_id,\n\t#                     section=section_name,\n\t#                     text=text,\n\t#                     title=title,\n\t#                     type=\"Section\",\n\t#                     page=0,\n\t#                     embedding=embedding,\n\t#                     regionBoundary=\"\",\n\t#                 )\n", "#                 documents.append(document.dict())\n\t#             with open(f\"{metadata_path}/data/{file_name}.json\") as user_file:\n\t#                 file_contents = user_file.read()\n\t#             for doc in json.loads(file_contents):\n\t#                 text = doc.get(\"caption\", \"\")\n\t#                 embedding = (\n\t#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n {text}\"}])[0].astype(np.float32).tobytes()\n\t#                 )\n\t#                 document = Document(\n\t#                     document_id=uuid4().hex,\n", "#                     section=\"\",\n\t#                     text=text,\n\t#                     title=title,\n\t#                     type=doc.get(\"figType\", \"\"),\n\t#                     page=int(doc.get(\"page\", \"\")),\n\t#                     embedding=embedding,\n\t#                     regionBoundary=str(doc.get(\"regionBoundary\", \"\")),\n\t#                 )\n\t#                 documents.append(document.dict())\n\t#             for doc in pdf_contents[\"highlights\"]:\n", "#                 text = doc.get(\"text\", \"\")\n\t#                 embedding = (\n\t#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  {text}\"}])[0].astype(np.float32).tobytes()\n\t#                 )\n\t#                 document = Document(\n\t#                     document_id=uuid4().hex,\n\t#                     section=\"\",\n\t#                     text=text,\n\t#                     title=title,\n\t#                     type=\"Highlight\",\n", "#                     page=int(doc.get(\"page\", -1)),\n\t#                     embedding=embedding,\n\t#                     regionBoundary=str(doc.get(\"regionBoundary\", \"\")),\n\t#                 )\n\t#                 documents.append(document.dict())\n\t#         if clean_up and os.path.exists(f\"{FILE_UPLOAD_PATH}\"):\n\t#             shutil.rmtree(FILE_UPLOAD_PATH)\n\t#         return documents\n\t#     @staticmethod\n\t#     def classify_pdf_from_path(\n", "#         pdf_path: Union[str, Path], config: PDFColumnClassifierConfig = PDFColumnClassifierConfig()\n\t#     ) -> bool:\n\t#         \"\"\"\n\t#         Classify a PDF as either single-column or two-column.\n\t#         Args:\n\t#             pdf_path: The PDF to classify.\n\t#             config: The configuration to use when classifying the PDF. Uses the default configuration if not specified.\n\t#         Returns:\n\t#             True if the PDF is two-column, False otherwise.\n\t#         \"\"\"\n", "#         # first, convert the PDF to a list of images.\n\t#         imgs = pdf2image.convert_from_path(pdf_path, dpi=config.dpi, **config.pdf_conversion_options.dict())\n\t#         return PDFParser._classify_imgs(imgs, config)\n\t#     @staticmethod\n\t#     def classify_pdf_from_bytes(\n\t#         pdf_bytes: bytes, config: PDFColumnClassifierConfig = PDFColumnClassifierConfig()\n\t#     ) -> bool:\n\t#         \"\"\"\n\t#         Classify a PDF as either single-column or two-column.\n\t#         Args:\n", "#             pdf_bytes: The PDF as bytes to classify.\n\t#             config: The configuration to use when classifying the PDF. Uses the default configuration if not specified.\n\t#         Returns:\n\t#             True if the PDF is two-column, False otherwise.\n\t#         \"\"\"\n\t#         # first, convert the PDF to a list of images.\n\t#         imgs = pdf2image.convert_from_bytes(pdf_bytes, dpi=config.dpi, **config.pdf_conversion_options.dict())\n\t#         return PDFParser._classify_imgs(imgs, config)\n\t#     @staticmethod\n\t#     def _classify_imgs(imgs: List[Any], config: PDFColumnClassifierConfig) -> bool:\n", "#         \"\"\"\n\t#         Classify a list of images as either single-column or two-column.\n\t#         Args:\n\t#             imgs: The images to classify.\n\t#             config: The configuration to use when classifying the images.\n\t#         Returns:\n\t#             True if the images are two-column, False otherwise.\n\t#         \"\"\"\n\t#         # classify each image in the PDF as either single-column or two-column.\n\t#         votes = []\n", "#         for img in imgs:\n\t#             # apply erosion to make the text more prominent\n\t#             if config.erosion_kernel_size > 1:\n\t#                 img = img.filter(ImageFilter.MinFilter(config.erosion_kernel_size))\n\t#             # calculate the percentage of whitish pixels in the middle of the image\n\t#             percentage_whitish_in_middle = PDFParser._calc_percentage_whitish_in_middle(img, config.middle_column_width)\n\t#             # if the percentage of whitish pixels in the middle of the\n\t#             # image is greater than VOTE_THRESHOLD, then the image is two-column.\n\t#             vote = percentage_whitish_in_middle > config.vote_threshold\n\t#             # add the vote to list\n", "#             votes.append(vote)\n\t#         # if the majority of the images in the PDF are classified as two-column, then the PDF is two-column.\n\t#         return sum(votes) > len(votes) // 2\n\t#     @staticmethod\n\t#     def _calc_percentage_whitish_in_middle(img: Image, middle_column_width: int) -> float:\n\t#         \"\"\"\n\t#         Calculate the percentage of whitish pixels in the middle of the image.\n\t#         Args:\n\t#             img: The image to calculate the percentage of whitish pixels in the middle of.\n\t#             middle_column_width: The width of the middle column to calculate the percentage of whitish pixels in.\n", "#         Returns:\n\t#             The percentage of whitish pixels in the middle of the image.\n\t#         \"\"\"\n\t#         img = img.convert(\"L\")\n\t#         middle_column = img.crop(\n\t#             (img.width // 2 - middle_column_width // 2, 0, img.width // 2 + middle_column_width // 2, img.height)\n\t#         )\n\t#         middle_column = middle_column.point(lambda x: 0 if x < 150 else 255, mode=\"1\")\n\t#         return np.array(middle_column).mean()\n"]}
{"filename": "acad_gpt/parsers/base_parser.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom enum import Enum\n\tfrom typing import Any\n\tfrom pydantic import BaseModel\n\tfrom acad_gpt.parsers.config import ParserConfig\n\tclass DocumentType(str, Enum):\n\t    paper = \"paper\"\n\t    code = \"code\"\n\t    webpage = \"webpage\"\n\t    pdf = \"pdf\"\n", "class DocumentStatus(str, Enum):\n\t    todo = \"todo\"\n\t    done = \"done\"\n\tclass Document(BaseModel):\n\t    text: str\n\t    title: str\n\t    type: str\n\t    url: str\n\t    embedding: Any\n\t    status: str = DocumentStatus.todo\n", "class BaseParser(ABC):\n\t    \"\"\"\n\t    Abstract class for datastores.\n\t    \"\"\"\n\t    @abstractmethod\n\t    def parse(self, config: ParserConfig):\n\t        raise NotImplementedError\n"]}
{"filename": "acad_gpt/parsers/pdf_parser.py", "chunked_list": ["import logging\n\timport os\n\timport shutil\n\tfrom typing import Any, Dict, List, Union\n\timport numpy as np\n\timport scipdf\n\tfrom acad_gpt.environment import FILE_UPLOAD_PATH\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tfrom acad_gpt.parsers.base_parser import BaseParser, Document, DocumentStatus, DocumentType\n\tfrom acad_gpt.parsers.config import ParserConfig\n", "logger = logging.getLogger(__name__)\n\tclass PDFParser(BaseParser):\n\t    def parse(self, config: Union[ParserConfig, List[ParserConfig]]) -> Dict:\n\t        if not isinstance(config, List):\n\t            config = [config]\n\t        parsed_content: Dict[str, Any] = {}\n\t        for config_item in config:\n\t            file_path = getattr(config_item, \"file_path\")\n\t            pdf_metadata = PDFParser.get_pdf_metadata(file_path=file_path)\n\t            pdf_metadata[\"file_name\"] = os.path.basename(file_path)\n", "            pdf_metadata[\"url\"] = getattr(config_item, \"file_url\")\n\t            parsed_content = {\n\t                \"metadata\": pdf_metadata,\n\t            }\n\t        return parsed_content\n\t    @staticmethod\n\t    def get_pdf_metadata(file_path: str, figures_directory: str = FILE_UPLOAD_PATH):\n\t        pdf_metadata = None\n\t        try:\n\t            pdf_metadata = scipdf.parse_pdf_to_dict(file_path)\n", "        except Exception as error:\n\t            logger.error(error)\n\t        return pdf_metadata\n\t    def to_documents(\n\t        self,\n\t        pdf_contents: Dict,\n\t        embed_client: EmbeddingClient,\n\t        type: str = DocumentType.pdf,\n\t        status: str = DocumentStatus.todo,\n\t        **kwargs,\n", "    ) -> Dict:\n\t        document = {}\n\t        clean_up = bool(kwargs.get(\"clean_up\", True))\n\t        metadata = dict(pdf_contents.pop(\"metadata\")) if pdf_contents.get(\"metadata\") else None\n\t        url = metadata.get(\"url\", \"\")\n\t        title = metadata.pop(\"title\", \"\")\n\t        if metadata:\n\t            abstract = metadata.pop(\"abstract\")\n\t            sections = \"\\n\".join([section.get(\"text\", \"\") for section in metadata.pop(\"sections\", [])])\n\t            embedding = (\n", "                embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  {abstract} \\n  {sections}\"}])[0]\n\t                .astype(np.float32)\n\t                .tobytes()\n\t            )\n\t            document = Document(\n\t                text=abstract,\n\t                title=title,\n\t                type=type,\n\t                url=url,\n\t                status=status,\n", "                embedding=embedding,\n\t            ).dict()\n\t        if clean_up and os.path.exists(f\"{FILE_UPLOAD_PATH}\"):\n\t            shutil.rmtree(FILE_UPLOAD_PATH)\n\t        return document\n"]}
{"filename": "acad_gpt/parsers/webpage_parser.py", "chunked_list": ["import logging\n\tfrom typing import Dict\n\timport numpy as np\n\timport requests\n\tfrom bs4 import BeautifulSoup, Comment\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tfrom acad_gpt.parsers.base_parser import BaseParser, Document, DocumentStatus, DocumentType\n\tfrom acad_gpt.parsers.config import ParserConfig\n\tlogger = logging.getLogger(__name__)\n\tclass WebPageParser(BaseParser):\n", "    def parse(self, config: ParserConfig) -> Dict:\n\t        self.config = config\n\t        # parse the webpage\n\t        title = self.get_title()\n\t        text = self.get_text()\n\t        return {\"title\": title, \"text\": text, \"url\": self.config.file_url}\n\t    def get_title(self):\n\t        reqs = requests.get(self.config.file_url)\n\t        soup = BeautifulSoup(reqs.text, \"html.parser\")\n\t        for title in soup.find_all(\"title\"):\n", "            if title.get_text():\n\t                return title.get_text()\n\t            else:\n\t                return \"\"\n\t    def get_text(self):\n\t        reqs = requests.get(self.config.file_url)\n\t        soup = BeautifulSoup(reqs.text, \"html.parser\")\n\t        texts = soup.findAll(text=True)\n\t        visible_texts = filter(WebPageParser.tag_visible, texts)\n\t        return \" \".join(t.strip() for t in visible_texts)\n", "    @staticmethod\n\t    def tag_visible(element):\n\t        if element.parent.name in [\"style\", \"script\", \"head\", \"title\", \"meta\", \"[document]\"]:\n\t            return False\n\t        if isinstance(element, Comment):\n\t            return False\n\t        return True\n\t    def to_documents(\n\t        self,\n\t        web_contents: Dict,\n", "        embed_client: EmbeddingClient,\n\t        type: str = DocumentType.webpage,\n\t        status: str = DocumentStatus.todo,\n\t        **kwargs,\n\t    ) -> Dict:\n\t        document = {}\n\t        url = web_contents.get(\"url\", \"\")\n\t        title = web_contents.pop(\"title\", \"\")\n\t        text = web_contents.pop(\"text\", \"\")[:4500]\n\t        embedding = embed_client.embed_documents(docs=[{\"text\": text}])[0].astype(np.float32).tobytes()\n", "        document = Document(\n\t            text=text,\n\t            title=title,\n\t            url=url,\n\t            type=type,\n\t            embedding=embedding,\n\t            status=status,\n\t        ).dict()\n\t        return document\n"]}
{"filename": "acad_gpt/llm_client/config.py", "chunked_list": ["from pydantic import BaseModel\n\tclass LLMClientConfig(BaseModel):\n\t    api_key: str\n\t    time_out: float = 30\n"]}
{"filename": "acad_gpt/llm_client/__init__.py", "chunked_list": ["from acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient, ChatGPTConfig  # noqa: F401\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient  # noqa: F401\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingConfig  # noqa: F401\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingModels  # noqa: F401\n"]}
{"filename": "acad_gpt/llm_client/llm_client.py", "chunked_list": ["from abc import ABC\n\tfrom acad_gpt.llm_client.config import LLMClientConfig\n\tclass LLMClient(ABC):\n\t    \"\"\"\n\t    Wrapper for the HTTP APIs for LLMs acting as data container for API configurations.\n\t    \"\"\"\n\t    def __init__(self, config: LLMClientConfig):\n\t        self._api_key = config.api_key\n\t        self._time_out = config.time_out\n\t    @property\n", "    def api_key(self):\n\t        return self._api_key\n\t    @property\n\t    def time_out(self):\n\t        return self._time_out\n"]}
{"filename": "acad_gpt/llm_client/openai/__init__.py", "chunked_list": []}
{"filename": "acad_gpt/llm_client/openai/embedding/embedding_client.py", "chunked_list": ["import logging\n\tfrom typing import Any, Dict, List, Union\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom acad_gpt.constants import MAX_ALLOWED_SEQ_LEN_001, MAX_ALLOWED_SEQ_LEN_002\n\tfrom acad_gpt.llm_client.llm_client import LLMClient\n\tfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig, EmbeddingModels\n\tfrom acad_gpt.utils.openai_utils import count_openai_tokens, load_openai_tokenizer, openai_request\n\tlogger = logging.getLogger(__name__)\n\tclass EmbeddingClient(LLMClient):\n", "    def __init__(self, config: EmbeddingConfig):\n\t        super().__init__(config=config)\n\t        self.openai_embedding_config = config\n\t        model_class: str = EmbeddingModels(self.openai_embedding_config.model).name\n\t        tokenizer = self._setup_encoding_models(\n\t            model_class,\n\t            self.openai_embedding_config.model,\n\t            self.openai_embedding_config.max_seq_len,\n\t        )\n\t        self._tokenizer = load_openai_tokenizer(\n", "            tokenizer_name=tokenizer,\n\t            use_tiktoken=self.openai_embedding_config.use_tiktoken,\n\t        )\n\t    def _setup_encoding_models(self, model_class: str, model_name: str, max_seq_len: int):\n\t        \"\"\"\n\t        Setup the encoding models for the retriever.\n\t        Raises:\n\t            ImportError: When `tiktoken` package is missing.\n\t            To use tiktoken tokenizer install it as follows:\n\t            `pip install tiktoken`\n", "        \"\"\"\n\t        tokenizer_name = \"gpt2\"\n\t        # new generation of embedding models (December 2022), specify the full name\n\t        if model_name.endswith(\"-002\"):\n\t            self.query_encoder_model = model_name\n\t            self.doc_encoder_model = model_name\n\t            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_002, max_seq_len)\n\t            if self.openai_embedding_config.use_tiktoken:\n\t                try:\n\t                    from tiktoken.model import MODEL_TO_ENCODING\n", "                    tokenizer_name = MODEL_TO_ENCODING.get(model_name, \"cl100k_base\")\n\t                except ImportError:\n\t                    raise ImportError(\n\t                        \"The `tiktoken` package not found.\",\n\t                        \"To install it use the following:\",\n\t                        \"`pip install tiktoken`\",\n\t                    )\n\t        else:\n\t            self.query_encoder_model = f\"text-search-{model_class}-query-001\"\n\t            self.doc_encoder_model = f\"text-search-{model_class}-doc-001\"\n", "            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_001, max_seq_len)\n\t        return tokenizer_name\n\t    def _ensure_text_limit(self, text: str) -> str:\n\t        \"\"\"\n\t         Ensure that length of the text is within the maximum length of the model.\n\t        OpenAI v1 embedding models have a limit of 2046 tokens, and v2 models have\n\t        a limit of 8191 tokens.\n\t        Args:\n\t            text (str):  Text to be checked if it exceeds the max token limit\n\t        Returns:\n", "            text (str): Trimmed text if exceeds the max token limit\n\t        \"\"\"\n\t        n_tokens = count_openai_tokens(text, self._tokenizer, self.openai_embedding_config.use_tiktoken)\n\t        if n_tokens <= self.max_seq_len:\n\t            return text\n\t        logger.warning(\n\t            \"The prompt has been truncated from %s tokens to %s tokens to fit\" \"within the max token limit.\",\n\t            \"Reduce the length of the prompt to prevent it from being cut off.\",\n\t            n_tokens,\n\t            self.max_seq_len,\n", "        )\n\t        if self.openai_embedding_config.use_tiktoken:\n\t            tokenized_payload = self._tokenizer.encode(text)\n\t            decoded_string = self._tokenizer.decode(tokenized_payload[: self.max_seq_len])\n\t        else:\n\t            tokenized_payload = self._tokenizer.tokenize(text)\n\t            decoded_string = self._tokenizer.convert_tokens_to_string(tokenized_payload[: self.max_seq_len])\n\t        return decoded_string\n\t    def embed(self, model: str, text: List[str]) -> np.ndarray:\n\t        \"\"\"\n", "        Embeds the batch of texts using the specified LLM.\n\t        Args:\n\t            model (str): LLM model name for embeddings.\n\t            text (List[str]): List of documents to be embedded.\n\t        Raises:\n\t            ValueError: When the OpenAI API key is missing.\n\t        Returns:\n\t            np.ndarray: embeddings for the input documents.\n\t        \"\"\"\n\t        if self.api_key is None:\n", "            raise ValueError(\n\t                \"OpenAI API key is not set. You can set it via the \" \"`api_key` parameter of the `LLMClient`.\"\n\t            )\n\t        generated_embeddings: List[Any] = []\n\t        headers: Dict[str, str] = {\"Content-Type\": \"application/json\"}\n\t        payload: Dict[str, Union[List[str], str]] = {\"model\": model, \"input\": text}\n\t        headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\t        res = openai_request(\n\t            url=self.openai_embedding_config.url,\n\t            headers=headers,\n", "            payload=payload,\n\t            timeout=self.time_out,\n\t        )\n\t        unordered_embeddings = [(ans[\"index\"], ans[\"embedding\"]) for ans in res[\"data\"]]\n\t        ordered_embeddings = sorted(unordered_embeddings, key=lambda x: x[0])\n\t        generated_embeddings = [emb[1] for emb in ordered_embeddings]\n\t        return np.array(generated_embeddings)\n\t    def embed_batch(self, model: str, text: List[str]) -> np.ndarray:\n\t        all_embeddings = []\n\t        for i in tqdm(\n", "            range(0, len(text), self.openai_embedding_config.batch_size),\n\t            disable=not self.openai_embedding_config.progress_bar,\n\t            desc=\"Calculating embeddings\",\n\t        ):\n\t            batch = text[i : i + self.openai_embedding_config.batch_size]\n\t            batch_limited = [self._ensure_text_limit(content) for content in batch]\n\t            generated_embeddings = self.embed(model, batch_limited)\n\t            all_embeddings.append(generated_embeddings)\n\t        return np.concatenate(all_embeddings)\n\t    def embed_queries(self, queries: List[str]) -> np.ndarray:\n", "        return self.embed_batch(self.query_encoder_model, queries)\n\t    def embed_documents(self, docs: List[Dict]) -> np.ndarray:\n\t        return self.embed_batch(self.doc_encoder_model, [d[\"text\"] for d in docs])\n"]}
{"filename": "acad_gpt/llm_client/openai/embedding/config.py", "chunked_list": ["from enum import Enum\n\tfrom acad_gpt.llm_client.config import LLMClientConfig\n\tclass EmbeddingModels(Enum):\n\t    ada = \"*-ada-*-001\"\n\t    babbage = \"*-babbage-*-001\"\n\t    curie = \"*-curie-*-001\"\n\t    davinci = \"*-davinci-*-001\"\n\tclass EmbeddingConfig(LLMClientConfig):\n\t    url: str = \"https://api.openai.com/v1/embeddings\"\n\t    batch_size: int = 64\n", "    progress_bar: bool = False\n\t    model: str = EmbeddingModels.ada.value\n\t    max_seq_len: int = 8191\n\t    use_tiktoken: bool = False\n"]}
{"filename": "acad_gpt/llm_client/openai/embedding/__init__.py", "chunked_list": []}
{"filename": "acad_gpt/llm_client/openai/conversation/config.py", "chunked_list": ["from acad_gpt.llm_client.config import LLMClientConfig\n\tclass ChatGPTConfig(LLMClientConfig):\n\t    temperature: float = 0\n\t    model_name: str = \"gpt-3.5-turbo\"\n\t    max_retries: int = 6\n\t    max_tokens: int = 256\n\t    verbose: bool = False\n"]}
{"filename": "acad_gpt/llm_client/openai/conversation/__init__.py", "chunked_list": []}
{"filename": "acad_gpt/llm_client/openai/conversation/chatgpt_client.py", "chunked_list": ["import logging\n\tfrom langchain import LLMChain, OpenAI, PromptTemplate\n\tfrom pydantic import BaseModel\n\tfrom acad_gpt.llm_client.llm_client import LLMClient\n\tfrom acad_gpt.llm_client.openai.conversation.config import ChatGPTConfig\n\tfrom acad_gpt.memory.manager import MemoryManager\n\tfrom acad_gpt.utils.openai_utils import get_prompt\n\tlogger = logging.getLogger(__name__)\n\tclass ChatGPTResponse(BaseModel):\n\t    message: str\n", "    chat_gpt_answer: str\n\tclass ChatGPTClient(LLMClient):\n\t    \"\"\"\n\t    ChatGPT client allows to interact with the ChatGPT model alonside having infinite contextual and adaptive memory.\n\t    \"\"\"\n\t    def __init__(self, config: ChatGPTConfig, memory_manager: MemoryManager):\n\t        super().__init__(config=config)\n\t        prompt = PromptTemplate(input_variables=[\"prompt\"], template=\"{prompt}\")\n\t        self.chatgpt_chain = LLMChain(\n\t            llm=OpenAI(\n", "                temperature=config.temperature,\n\t                openai_api_key=self.api_key,\n\t                model_name=config.model_name,\n\t                max_retries=config.max_retries,\n\t                max_tokens=config.max_tokens,\n\t            ),\n\t            prompt=prompt,\n\t            verbose=config.verbose,\n\t        )\n\t        self.memory_manager = memory_manager\n", "    def converse(self, message: str, topk: int = 5, **kwargs) -> ChatGPTResponse:\n\t        \"\"\"\n\t        Allows user to chat with user by leveraging the infinite contextual memory for fetching and\n\t        adding historical messages to the prompt to the ChatGPT model.\n\t        Args:\n\t            message (str): Message by the human user.\n\t        Returns:\n\t            ChatGPTResponse: Response includes answer from th ChatGPT, conversation_id, and human message.\n\t        \"\"\"\n\t        history = \"\"\n", "        try:\n\t            past_messages = self.memory_manager.get_messages(query=message, topk=topk, kwargs=kwargs)\n\t            history = \"\\n\".join([past_message.text for past_message in past_messages if getattr(past_message, \"text\")])\n\t        except ValueError as history_not_found_error:\n\t            logger.warning(f\"Details: {history_not_found_error}\")\n\t        prompt = get_prompt(message=message, history=history)\n\t        chat_gpt_answer = self.chatgpt_chain.predict(prompt=prompt)\n\t        return ChatGPTResponse(message=message, chat_gpt_answer=chat_gpt_answer)\n"]}
{"filename": "acad_gpt/docstore/base.py", "chunked_list": ["import abc\n\tfrom typing import List\n\tclass DocStore(abc.ABC):\n\t    @abc.abstractmethod\n\t    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n\t        \"\"\"\n\t        Uploads a file to the docstore from a local file path.\n\t        Args:\n\t            file_path: The path to the file to upload.\n\t            file_name: The name to give the file in the docstore.\n", "        \"\"\"\n\t        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def download_to_filename(self, file_name: str, file_path: str) -> None:\n\t        \"\"\"\n\t        Downloads a file from the docstore to a local file path.\n\t        Args:\n\t            file_name: The name of the file in the docstore.\n\t            file_path: The path to save the file to.\n\t        \"\"\"\n", "        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def upload_from_file(self, file, file_name: str) -> None:\n\t        \"\"\"\n\t        Uploads a file to the docstore from a file object.\n\t        Args:\n\t            file: The file object to upload.\n\t            file_name: The name to give the file in the docstore.\n\t        \"\"\"\n\t        raise NotImplementedError\n", "    @abc.abstractmethod\n\t    def download_to_file(self, file_name: str, file) -> None:\n\t        \"\"\"\n\t        Downloads a file from the docstore to a byte array.\n\t        Args:\n\t            file_name: The name of the file in the docstore.\n\t            file: The file object to save the file to.\n\t        Returns:\n\t            The bytes of the file.\n\t        \"\"\"\n", "        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def delete(self, file_name: str) -> None:\n\t        \"\"\"\n\t        Deletes a file from the docstore.\n\t        Args:\n\t            file_name: The name of the file in the docstore.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abc.abstractmethod\n", "    def list(self) -> List[str]:\n\t        \"\"\"\n\t        Lists all files in the docstore.\n\t        Returns:\n\t            A list of file names.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def exists(self, file_name: str) -> bool:\n\t        \"\"\"\n", "        Checks if a file exists in the docstore.\n\t        Args:\n\t            file_name: The name of the file in the docstore.\n\t        Returns:\n\t            True if the file exists, False otherwise.\n\t        \"\"\"\n\t        raise NotImplementedError\n"]}
{"filename": "acad_gpt/docstore/hf_file_system_storage.py", "chunked_list": ["\"\"\"\n\tBased on HuggingFace's File System API.\n\tSee https://huggingface.co/docs/huggingface_hub/v0.14.1/en/package_reference/hf_file_system#huggingface_hub.HfFileSystem\n\t\"\"\"\n\timport logging\n\timport os\n\tfrom typing import List, Optional\n\tfrom huggingface_hub import HfFileSystem, login\n\tfrom pydantic import BaseSettings, Field\n\tfrom acad_gpt.docstore.base import DocStore\n", "logger = logging.getLogger(__name__)\n\tclass HfFSDocStoreConfig(BaseSettings):\n\t    repo: str = Field(..., env=\"HF_FS_REPO\")\n\t    \"\"\" The repository to use as a file system docstore. \"\"\"\n\t    endpoint: Optional[str] = Field(None, env=\"HF_ENDPOINT\")\n\t    \"\"\" The endpoint to use. If None, the default endpoint is used. \"\"\"\n\t    token: Optional[str] = Field(None, env=\"HF_TOKEN\")\n\t    \"\"\" The token to use. If None, the default stored token is used. \"\"\"\n\tclass HfFSDocStore(DocStore):\n\t    \"\"\"\n", "    A docstore that uses the HuggingFace File System API as the backend.\n\t    Args:\n\t        config (HfFSDocStoreConfig): The configuration to use.\n\t    \"\"\"\n\t    def __init__(self, config: HfFSDocStoreConfig):\n\t        self.config = config\n\t        \"\"\" The config to use.\"\"\"\n\t        # fail early incase no token is set\n\t        try:\n\t            login(self.config.token)\n", "        except ValueError as hf_value_error:\n\t            logger.error(hf_value_error)\n\t        self._hf_fs = HfFileSystem(**self.config.dict())\n\t    @property\n\t    def fs(self) -> HfFileSystem:\n\t        return self._hf_fs\n\t    def path_in_repo(self, file_name: str) -> str:\n\t        return os.path.join(self.config.repo, file_name)\n\t    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n\t        self.fs.put(lpath=file_path, rpath=self.path_in_repo(file_name))\n", "    def download_to_filename(self, file_name: str, file_path: str) -> None:\n\t        self.fs.get(rpath=self.path_in_repo(file_name), lpath=file_path)\n\t    def upload_from_file(self, file, file_name: str) -> None:\n\t        with self.fs.open(self.path_in_repo(file_name), \"wb\") as f:\n\t            f.write(file.read())\n\t    def download_to_file(self, file_name: str, file) -> None:\n\t        with self.fs.open(self.path_in_repo(file_name), \"rb\") as f:\n\t            file.write(f.read())\n\t    def delete(self, file_name: str) -> None:\n\t        self.fs.delete(self.path_in_repo(file_name))\n", "    def list(self) -> List[str]:\n\t        # 1. get files\n\t        files = self.fs.ls(self.config.repo, detail=False)\n\t        # 2. remove repo prefix for consistency with other API\n\t        files = [file_name[len(self.config.repo) + 1 :] for file_name in files]\n\t        return files\n\t    def exists(self, file_name: str) -> bool:\n\t        return self.fs.exists(self.path_in_repo(file_name))\n"]}
{"filename": "acad_gpt/docstore/__init__.py", "chunked_list": []}
{"filename": "acad_gpt/docstore/in_memory_storage.py", "chunked_list": ["from typing import List, Optional\n\tfrom pydantic import BaseModel\n\tfrom acad_gpt.docstore.base import DocStore\n\tclass InMemoryStorageConfig(BaseModel):\n\t    mutable: bool = True\n\t    \"\"\" Whether the storage is mutable. If mutable, the storage can be modified. If not, the storage is read-only. \"\"\"\n\t    max_size: Optional[int] = None\n\t    \"\"\" The maximum size of the storage. If None, the storage has no maximum size. \"\"\"\n\tclass InMemoryStorage(DocStore):\n\t    \"\"\"\n", "    A docstore that uses an in-memory dictionary as the backend.\n\t    Note:\n\t        Primarily intended for testing purposes.\n\t    Args:\n\t        config (InMemoryStorageConfig): The configuration to use.\n\t        storage (Optional[dict]): The storage to use. If None, an empty storage is initialized.\n\t    \"\"\"\n\t    def __init__(self, config: InMemoryStorageConfig, storage: Optional[dict] = None):\n\t        self.config = config\n\t        self.mutable = config.mutable\n", "        self.max_size = config.max_size\n\t        if storage is None:\n\t            storage = dict()\n\t        self.storage = storage\n\t        \"\"\" The in-memory storage. \"\"\"\n\t    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n\t        if not self.mutable:\n\t            raise ValueError(\"The storage is not mutable.\")\n\t        if self.max_size is not None and len(self.storage) >= self.max_size:\n\t            raise ValueError(\"The storage is full.\")\n", "        with open(file_path, \"rb\") as f:\n\t            self.storage[file_name] = f.read()\n\t    def download_to_filename(self, file_name: str, file_path: str) -> None:\n\t        with open(file_path, \"wb\") as f:\n\t            f.write(self.storage[file_name])\n\t    def upload_from_file(self, file, file_name: str) -> None:\n\t        if not self.mutable:\n\t            raise ValueError(\"The storage is not mutable.\")\n\t        if self.max_size is not None and len(self.storage) >= self.max_size:\n\t            raise ValueError(\"The storage is full.\")\n", "        self.storage[file_name] = file.read()\n\t    def download_to_file(self, file_name: str, file) -> None:\n\t        file.write(self.storage[file_name])\n\t    def exists(self, file_name: str) -> bool:\n\t        return file_name in self.storage\n\t    def delete(self, file_name: str) -> None:\n\t        if not self.mutable:\n\t            raise ValueError(\"The storage is not mutable.\")\n\t        del self.storage[file_name]\n\t    def list(self) -> List[str]:\n", "        return list(self.storage.keys())\n"]}
{"filename": "acad_gpt/memory/manager.py", "chunked_list": ["from typing import Any, Dict, List\n\timport numpy as np\n\tfrom acad_gpt.datastore.redis import RedisDataStore\n\tfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tfrom .memory import Memory\n\tclass MemoryManager:\n\t    \"\"\"\n\t    Manages the memory of conversations.\n\t    Attributes:\n\t        datastore (DataStore): Datastore to use for storing and retrieving memories.\n", "        embed_client (EmbeddingClient): Embedding client to call for embedding conversations.\n\t        conversations (List[Memory]): List of conversation IDs to memories to be managed.\n\t    \"\"\"\n\t    def __init__(self, datastore: RedisDataStore, embed_client: EmbeddingClient, topk: int = 5) -> None:\n\t        \"\"\"\n\t        Initializes the memory manager.\n\t        Args:\n\t            datastore (DataStore): Datastore to be used. Assumed to be connected.\n\t            embed_client (EmbeddingClient): Embedding client to be used.\n\t            topk (int): Number of past message to be retrieved as context for current message.\n", "        \"\"\"\n\t        self.datastore = datastore\n\t        self.embed_client = embed_client\n\t        self.topk = topk\n\t        self.conversations: List[Memory] = [\n\t            Memory(conversation_id=conversation_id) for conversation_id in datastore.get_all_document_ids()\n\t        ]\n\t    def __del__(self) -> None:\n\t        \"\"\"Clear the memory manager when manager is deleted.\"\"\"\n\t        self.clear()\n", "    def add_conversation(self, conversation: Memory) -> None:\n\t        \"\"\"\n\t        Adds a conversation to the memory manager to be stored and manage.\n\t        Args:\n\t            conversation (Memory): Conversation to be added.\n\t        \"\"\"\n\t        if conversation not in self.conversations:\n\t            self.conversations.append(conversation)\n\t    def remove_conversation(self, conversation: Memory) -> None:\n\t        \"\"\"\n", "        Removes a conversation from the memory manager.\n\t        Args:\n\t            conversation (Memory): Conversation to be removed containing `conversation_id`.\n\t        \"\"\"\n\t        if conversation not in self.conversations:\n\t            return\n\t        conversation_idx = self.conversations.index(conversation)\n\t        if conversation_idx >= 0:\n\t            del self.conversations[conversation_idx]\n\t            self.datastore.delete_documents(document_id=conversation.conversation_id)\n", "    def clear(self) -> None:\n\t        \"\"\"\n\t        Clears the memory manager.\n\t        \"\"\"\n\t        self.datastore.flush_all_documents()\n\t        self.conversations = []\n\t    def add_message(self, conversation_id: str, human: str, assistant: str) -> None:\n\t        \"\"\"\n\t        Adds a message to a conversation.\n\t        Args:\n", "            conversation_id (str): ID of the conversation to add the message to.\n\t            human (str): User message.\n\t            assistant (str): Assistant message.\n\t        \"\"\"\n\t        document: Dict = {\"text\": f\"Human: {human}\\nAssistant: {assistant}\", \"conversation_id\": conversation_id}\n\t        document[\"embedding\"] = self.embed_client.embed_documents(docs=[document])[0].astype(np.float32).tobytes()\n\t        self.datastore.index_documents(documents=[document])\n\t        # optionally check if it is a new conversation\n\t        self.add_conversation(Memory(conversation_id=conversation_id))\n\t    def get_messages(self, query: str, topk: int = 5, **kwargs) -> List[Any]:\n", "        \"\"\"\n\t        Gets the messages of a conversation using the query message.\n\t        Args:\n\t            query (str): Current user message you want to pull history for to use in the prompt.\n\t            topk (int): Number of messages to be returned. Defaults to 5.\n\t        Returns:\n\t            List[Any]: List of messages of the conversation.\n\t        \"\"\"\n\t        query_vector = self.embed_client.embed_queries([query])[0].astype(np.float32).tobytes()\n\t        messages = self.datastore.search_documents(query=query_vector, topk=topk, kwargs=kwargs)\n", "        return messages\n"]}
{"filename": "acad_gpt/memory/__init__.py", "chunked_list": ["from acad_gpt.memory.manager import MemoryManager  # noqa: F401\n\tfrom acad_gpt.memory.memory import Memory  # noqa: F401\n"]}
{"filename": "acad_gpt/memory/memory.py", "chunked_list": ["\"\"\"\n\tContains a memory dataclass.\n\t\"\"\"\n\tfrom pydantic import BaseModel\n\tclass Memory(BaseModel):\n\t    \"\"\"\n\t    A memory dataclass.\n\t    \"\"\"\n\t    conversation_id: str\n\t    \"\"\"ID of the conversation.\"\"\"\n"]}
