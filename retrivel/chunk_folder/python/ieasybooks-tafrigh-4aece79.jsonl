{"filename": "tafrigh/writer.py", "chunked_list": ["import os\n\tfrom pathlib import Path\n\tfrom typing import Union\n\tfrom tafrigh.config import Config\n\tfrom tafrigh.types.transcript_type import TranscriptType\n\tfrom tafrigh.utils import time_utils\n\tclass Writer:\n\t    def write_all(\n\t        self,\n\t        file_name: str,\n", "        segments: list[dict[str, Union[str, float]]],\n\t        output_config: Config.Output,\n\t    ) -> None:\n\t        if output_config.save_files_before_compact:\n\t            for output_format in output_config.output_formats:\n\t                self.write(\n\t                    output_format,\n\t                    os.path.join(output_config.output_dir, f'{file_name}-original.{output_format}'),\n\t                    segments,\n\t                )\n", "        if not output_config.save_files_before_compact or output_config.min_words_per_segment != 0:\n\t            compacted_segments = self.compact_segments(segments, output_config.min_words_per_segment)\n\t            for output_format in output_config.output_formats:\n\t                self.write(\n\t                    output_format,\n\t                    os.path.join(output_config.output_dir, f'{file_name}.{output_format}'),\n\t                    compacted_segments,\n\t                )\n\t    def write(\n\t        self,\n", "        format: TranscriptType,\n\t        file_path: str,\n\t        segments: list[dict[str, Union[str, float]]],\n\t    ) -> None:\n\t        if format == TranscriptType.TXT:\n\t            self.write_txt(file_path, segments)\n\t        elif format == TranscriptType.SRT:\n\t            self.write_srt(file_path, segments)\n\t        elif format == TranscriptType.VTT:\n\t            self.write_vtt(file_path, segments)\n", "    def write_txt(\n\t        self,\n\t        file_path: str,\n\t        segments: list[dict[str, Union[str, float]]],\n\t    ) -> None:\n\t        self._write_to_file(file_path, self.generate_txt(segments))\n\t    def write_srt(\n\t        self,\n\t        file_path: str,\n\t        segments: list[dict[str, Union[str, float]]],\n", "    ) -> None:\n\t        self._write_to_file(file_path, self.generate_srt(segments))\n\t    def write_vtt(\n\t        self,\n\t        file_path: str,\n\t        segments: list[dict[str, Union[str, float]]],\n\t    ) -> None:\n\t        self._write_to_file(file_path, self.generate_vtt(segments))\n\t    def generate_txt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n\t        return '\\n'.join(list(map(lambda segment: segment['text'].strip(), segments))) + '\\n'\n", "    def generate_srt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n\t        return ''.join(\n\t            f\"{i}\\n\"\n\t            f\"{time_utils.format_timestamp(segment['start'], include_hours=True, decimal_marker=',')} --> \"\n\t            f\"{time_utils.format_timestamp(segment['end'], include_hours=True, decimal_marker=',')}\\n\"\n\t            f\"{segment['text'].strip()}\\n\\n\"\n\t            for i, segment in enumerate(segments, start=1)\n\t        )\n\t    def generate_vtt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n\t        return 'WEBVTT\\n\\n' + ''.join(\n", "            f\"{time_utils.format_timestamp(segment['start'])} --> {time_utils.format_timestamp(segment['end'])}\\n\"\n\t            f\"{segment['text'].strip()}\\n\\n\"\n\t            for segment in segments\n\t        )\n\t    def compact_segments(\n\t        self,\n\t        segments: list[dict[str, Union[str, float]]],\n\t        min_words_per_segment: int,\n\t    ) -> list[dict[str, Union[str, float]]]:\n\t        if min_words_per_segment == 0:\n", "            return segments\n\t        compacted_segments = []\n\t        tmp_segment = None\n\t        for segment in segments:\n\t            if tmp_segment:\n\t                tmp_segment['text'] += f\" {segment['text'].strip()}\"\n\t                tmp_segment['end'] = segment['end']\n\t                if len(tmp_segment['text'].split()) >= min_words_per_segment:\n\t                    compacted_segments.append(tmp_segment)\n\t                    tmp_segment = None\n", "            elif len(segment['text'].split()) < min_words_per_segment:\n\t                tmp_segment = dict(segment)\n\t            elif len(segment['text'].split()) >= min_words_per_segment:\n\t                compacted_segments.append(dict(segment))\n\t        if tmp_segment:\n\t            compacted_segments.append(tmp_segment)\n\t        return compacted_segments\n\t    def is_output_exist(self, file_name: str, output_config: Config.Output):\n\t        if output_config.save_files_before_compact and not all(\n\t            Path(output_config.output_dir, f'{file_name}-original.{output_format}').is_file()\n", "            for output_format in output_config.output_formats\n\t        ):\n\t            return False\n\t        if (not output_config.save_files_before_compact or output_config.min_words_per_segment != 0) and not all(\n\t            Path(output_config.output_dir, f'{file_name}.{output_format}').is_file()\n\t            for output_format in output_config.output_formats\n\t        ):\n\t            return False\n\t        return True\n\t    def _write_to_file(self, file_path: str, content: str) -> None:\n", "        with open(file_path, 'w', encoding='utf-8') as fp:\n\t            fp.write(content)\n"]}
{"filename": "tafrigh/config.py", "chunked_list": ["import logging\n\tfrom tafrigh.types.transcript_type import TranscriptType\n\tclass Config:\n\t    def __init__(\n\t        self,\n\t        urls_or_paths: list[str],\n\t        skip_if_output_exist: bool,\n\t        playlist_items: str,\n\t        verbose: bool,\n\t        model_name_or_path: str,\n", "        task: str,\n\t        language: str,\n\t        use_faster_whisper: bool,\n\t        use_whisper_jax: bool,\n\t        beam_size: int,\n\t        ct2_compute_type: str,\n\t        wit_client_access_token: str,\n\t        max_cutting_duration: int,\n\t        min_words_per_segment: int,\n\t        save_files_before_compact: bool,\n", "        save_yt_dlp_responses: bool,\n\t        output_sample: int,\n\t        output_formats: list[str],\n\t        output_dir: str,\n\t    ):\n\t        self.input = self.Input(urls_or_paths, skip_if_output_exist, playlist_items, verbose)\n\t        self.whisper = self.Whisper(\n\t            model_name_or_path,\n\t            task,\n\t            language,\n", "            use_faster_whisper,\n\t            use_whisper_jax,\n\t            beam_size,\n\t            ct2_compute_type,\n\t        )\n\t        self.wit = self.Wit(wit_client_access_token, max_cutting_duration)\n\t        self.output = self.Output(\n\t            min_words_per_segment,\n\t            save_files_before_compact,\n\t            save_yt_dlp_responses,\n", "            output_sample,\n\t            output_formats,\n\t            output_dir,\n\t        )\n\t    def use_wit(self) -> bool:\n\t        return self.wit.wit_client_access_token != ''\n\t    class Input:\n\t        def __init__(self, urls_or_paths: list[str], skip_if_output_exist: bool, playlist_items: str, verbose: bool):\n\t            self.urls_or_paths = urls_or_paths\n\t            self.skip_if_output_exist = skip_if_output_exist\n", "            self.playlist_items = playlist_items\n\t            self.verbose = verbose\n\t    class Whisper:\n\t        def __init__(\n\t            self,\n\t            model_name_or_path: str,\n\t            task: str,\n\t            language: str,\n\t            use_faster_whisper: bool,\n\t            use_whisper_jax: bool,\n", "            beam_size: int,\n\t            ct2_compute_type: str,\n\t        ):\n\t            if model_name_or_path.endswith('.en'):\n\t                logging.warn(f'{model_name_or_path} is an English-only model, setting language to English.')\n\t                language = 'en'\n\t            self.model_name_or_path = model_name_or_path\n\t            self.task = task\n\t            self.language = language\n\t            self.use_faster_whisper = use_faster_whisper\n", "            self.use_whisper_jax = use_whisper_jax\n\t            self.beam_size = beam_size\n\t            self.ct2_compute_type = ct2_compute_type\n\t    class Wit:\n\t        def __init__(self, wit_client_access_token: str, max_cutting_duration: int):\n\t            self.wit_client_access_token = wit_client_access_token\n\t            self.max_cutting_duration = max_cutting_duration\n\t    class Output:\n\t        def __init__(\n\t            self,\n", "            min_words_per_segment: int,\n\t            save_files_before_compact: bool,\n\t            save_yt_dlp_responses: bool,\n\t            output_sample: int,\n\t            output_formats: list[str],\n\t            output_dir: str,\n\t        ):\n\t            if 'all' in output_formats:\n\t                output_formats = list(TranscriptType)\n\t            else:\n", "                output_formats = [TranscriptType(output_format) for output_format in output_formats]\n\t            if TranscriptType.ALL in output_formats:\n\t                output_formats.remove(TranscriptType.ALL)\n\t            if TranscriptType.NONE in output_formats:\n\t                output_formats.remove(TranscriptType.NONE)\n\t            self.min_words_per_segment = min_words_per_segment\n\t            self.save_files_before_compact = save_files_before_compact\n\t            self.save_yt_dlp_responses = save_yt_dlp_responses\n\t            self.output_sample = output_sample\n\t            self.output_formats = output_formats\n", "            self.output_dir = output_dir\n"]}
{"filename": "tafrigh/__init__.py", "chunked_list": ["from tafrigh.cli import farrigh\n\tfrom tafrigh.config import Config\n\tfrom tafrigh.downloader import Downloader\n\tfrom tafrigh.types.transcript_type import TranscriptType\n\tfrom tafrigh.writer import Writer\n\ttry:\n\t    from tafrigh.recognizers.whisper_recognizer import WhisperRecognizer\n\texcept ModuleNotFoundError:\n\t    pass\n\ttry:\n", "    from tafrigh.audio_splitter import AudioSplitter\n\t    from tafrigh.recognizers.wit_recognizer import WitRecognizer\n\texcept ModuleNotFoundError:\n\t    pass\n"]}
{"filename": "tafrigh/audio_splitter.py", "chunked_list": ["import os\n\timport tempfile\n\timport numpy as np\n\tfrom auditok.core import split\n\tfrom scipy.io import wavfile\n\tclass AudioSplitter:\n\t    def split(\n\t        self,\n\t        file_path: str,\n\t        output_dir: str,\n", "        min_dur: float = 0.5,\n\t        max_dur: float = 15,\n\t        max_silence: float = 0.5,\n\t        energy_threshold: float = 50,\n\t        expand_segments_with_noise: bool = False,\n\t        noise_seconds: int = 1,\n\t        noise_amplitude: int = 10,\n\t    ) -> list[tuple[str, float, float]]:\n\t        sampling_rate, data = self._read_audio(file_path)\n\t        temp_file_name = self._write_temp_audio(sampling_rate, data)\n", "        segments = self._split_audio(temp_file_name, min_dur, max_dur, max_silence, energy_threshold)\n\t        os.remove(temp_file_name)\n\t        if expand_segments_with_noise:\n\t            expanded_segments = self._expand_segments_with_noise(\n\t                segments,\n\t                noise_seconds,\n\t                noise_amplitude,\n\t                sampling_rate,\n\t                data.dtype,\n\t            )\n", "        else:\n\t            expanded_segments = [(segment, segment.meta.start, segment.meta.end) for segment in segments]\n\t        return self._save_segments(output_dir, sampling_rate, expanded_segments)\n\t    def _read_audio(self, file_path: str) -> tuple[int, np.ndarray]:\n\t        sampling_rate, data = wavfile.read(file_path)\n\t        if len(data.shape) > 1 and data.shape[1] > 1:\n\t            data = np.mean(data, axis=1)\n\t        return sampling_rate, data\n\t    def _write_audio(self, file_path: str, sampling_rate: int, data: np.ndarray) -> None:\n\t        wavfile.write(file_path, sampling_rate, data.astype(np.int16))\n", "    def _write_temp_audio(self, sampling_rate: int, data: np.ndarray) -> str:\n\t        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n\t            temp_file_name = temp_file.name\n\t            self._write_audio(temp_file_name, sampling_rate, data)\n\t        return temp_file_name\n\t    def _split_audio(\n\t        self,\n\t        temp_file_name: str,\n\t        min_dur: float,\n\t        max_dur: float,\n", "        max_silence: float,\n\t        energy_threshold: float,\n\t    ):\n\t        return split(\n\t            temp_file_name,\n\t            min_dur=min_dur,\n\t            max_dur=max_dur,\n\t            max_silence=max_silence,\n\t            energy_threshold=energy_threshold,\n\t        )\n", "    def _expand_segments_with_noise(\n\t        self,\n\t        segments: list,\n\t        noise_seconds: int,\n\t        noise_amplitude: int,\n\t        sampling_rate: int,\n\t        dtype: np.dtype,\n\t    ) -> list[tuple[np.ndarray, float, float]]:\n\t        expanded_segments = []\n\t        for segment in segments:\n", "            # Have different noise in the beginning and the end gave us better results :).\n\t            prepend_noise = np.random.normal(0, noise_amplitude, int(noise_seconds * sampling_rate)).astype(dtype)\n\t            append_noise = np.random.normal(0, noise_amplitude, int(noise_seconds * sampling_rate)).astype(dtype)\n\t            expanded_segment = np.concatenate((prepend_noise, segment, append_noise))\n\t            expanded_segments.append((expanded_segment, segment.meta.start, segment.meta.end))\n\t        return expanded_segments\n\t    def _save_segments(\n\t        self,\n\t        output_dir: str,\n\t        sampling_rate: int,\n", "        expanded_segments: list[tuple[np.ndarray, float, float]],\n\t    ) -> list[tuple[str, float, float]]:\n\t        segments = []\n\t        for i, (expanded_segment, start, end) in enumerate(expanded_segments):\n\t            output_file = os.path.join(output_dir, f\"segment_{i + 1}.wav\")\n\t            self._write_audio(output_file, sampling_rate, expanded_segment)\n\t            segments.append((output_file, start, end))\n\t        return segments\n"]}
{"filename": "tafrigh/cli.py", "chunked_list": ["import csv\n\timport logging\n\timport os\n\timport random\n\timport re\n\timport sys\n\tfrom collections import deque\n\tfrom pathlib import Path\n\tfrom typing import Any, Generator, Union\n\tfrom tqdm import tqdm\n", "from tafrigh.config import Config\n\tfrom tafrigh.downloader import Downloader\n\tfrom tafrigh.utils import cli_utils, file_utils, time_utils\n\tfrom tafrigh.writer import Writer\n\ttry:\n\t    import requests\n\t    from tafrigh.recognizers.wit_recognizer import WitRecognizer\n\t    from tafrigh.utils.wit import file_utils as wit_file_utils\n\texcept ModuleNotFoundError:\n\t    pass\n", "try:\n\t    from tafrigh.recognizers.whisper_recognizer import WhisperRecognizer\n\t    from tafrigh.types.whisper.type_hints import WhisperModel\n\t    from tafrigh.utils.whisper import whisper_utils\n\texcept ModuleNotFoundError:\n\t    pass\n\tdef main():\n\t    args = cli_utils.parse_args(sys.argv[1:])\n\t    config = Config(\n\t        urls_or_paths=args.urls_or_paths,\n", "        skip_if_output_exist=args.skip_if_output_exist,\n\t        playlist_items=args.playlist_items,\n\t        verbose=args.verbose,\n\t        #\n\t        model_name_or_path=args.model_name_or_path,\n\t        task=args.task,\n\t        language=args.language,\n\t        use_faster_whisper=args.use_faster_whisper,\n\t        use_whisper_jax=args.use_whisper_jax,\n\t        beam_size=args.beam_size,\n", "        ct2_compute_type=args.ct2_compute_type,\n\t        #\n\t        wit_client_access_token=args.wit_client_access_token,\n\t        max_cutting_duration=args.max_cutting_duration,\n\t        min_words_per_segment=args.min_words_per_segment,\n\t        #\n\t        save_files_before_compact=args.save_files_before_compact,\n\t        save_yt_dlp_responses=args.save_yt_dlp_responses,\n\t        output_sample=args.output_sample,\n\t        output_formats=args.output_formats,\n", "        output_dir=args.output_dir,\n\t    )\n\t    if config.use_wit() and config.input.skip_if_output_exist:\n\t        retries = 3\n\t        while retries > 0:\n\t            try:\n\t                deque(farrigh(config), maxlen=0)\n\t                break\n\t            except requests.exceptions.RetryError:\n\t                retries -= 1\n", "    else:\n\t        deque(farrigh(config), maxlen=0)\n\tdef farrigh(config: Config) -> Generator[dict[str, int], None, None]:\n\t    prepare_output_dir(config.output.output_dir)\n\t    model = None\n\t    if not config.use_wit():\n\t        model = whisper_utils.load_model(config.whisper)\n\t    segments = []\n\t    for idx, item in enumerate(tqdm(config.input.urls_or_paths, desc='URLs or local paths')):\n\t        progress_info = {\n", "            'outer_total': len(config.input.urls_or_paths),\n\t            'outer_current': idx + 1,\n\t            'outer_status': 'processing',\n\t        }\n\t        if Path(item).exists():\n\t            file_or_folder = Path(item)\n\t            for progress_info, local_elements_segments in process_local(file_or_folder, model, config, progress_info):\n\t                segments.extend(local_elements_segments)\n\t                yield progress_info\n\t        elif re.match('(https?://)', item):\n", "            for progress_info, url_elements_segments in process_url(item, model, config, progress_info):\n\t                segments.extend(url_elements_segments)\n\t                yield progress_info\n\t        else:\n\t            logging.error(f'Path {item} does not exist and is not a URL either.')\n\t            progress_info['outer_status'] = 'completed'\n\t            yield progress_info\n\t            continue\n\t        progress_info['outer_status'] = 'completed'\n\t        yield progress_info\n", "    write_output_sample(segments, config.output)\n\tdef prepare_output_dir(output_dir: str) -> None:\n\t    os.makedirs(output_dir, exist_ok=True)\n\tdef process_local(\n\t    path: Path,\n\t    model: 'WhisperModel',\n\t    config: Config,\n\t    progress_info: dict,\n\t) -> Generator[tuple[dict[str, int], list[list[dict[str, Union[str, float]]]]], None, None]:\n\t    filtered_media_files: list[Path] = file_utils.filter_media_files([path] if path.is_file() else path.iterdir())\n", "    files: list[dict[str, Any]] = [{'file_name': file.name, 'file_path': file} for file in filtered_media_files]\n\t    for idx, file in enumerate(tqdm(files, desc='Local files')):\n\t        new_progress_info = progress_info.copy()\n\t        new_progress_info.update(\n\t            {\n\t                'inner_total': len(files),\n\t                'inner_current': idx + 1,\n\t                'inner_status': 'processing',\n\t                'progress': 0.0,\n\t                'remaining_time': None,\n", "            }\n\t        )\n\t        yield new_progress_info, []\n\t        writer = Writer()\n\t        if config.input.skip_if_output_exist and writer.is_output_exist(Path(file['file_name']).stem, config.output):\n\t            new_progress_info['inner_status'] = 'completed'\n\t            yield new_progress_info, []\n\t            continue\n\t        file_path = str(file['file_path'].absolute())\n\t        if config.use_wit():\n", "            wav_file_path = str(wit_file_utils.convert_to_wav(file['file_path']).absolute())\n\t            recognize_generator = WitRecognizer(verbose=config.input.verbose).recognize(wav_file_path, config.wit)\n\t        else:\n\t            recognize_generator = WhisperRecognizer(verbose=config.input.verbose).recognize(\n\t                file_path,\n\t                model,\n\t                config.whisper,\n\t            )\n\t        while True:\n\t            try:\n", "                new_progress_info.update(next(recognize_generator))\n\t                yield new_progress_info, []\n\t            except StopIteration as exception:\n\t                segments = exception.value\n\t                break\n\t        if config.use_wit() and file['file_path'].suffix != '.wav':\n\t            Path(wav_file_path).unlink(missing_ok=True)\n\t        writer.write_all(Path(file['file_name']).stem, segments, config.output)\n\t        for segment in segments:\n\t            segment['url'] = f\"file://{file_path}&t={int(segment['start'])}\"\n", "            segment['file_path'] = file_path\n\t        new_progress_info['inner_status'] = 'completed'\n\t        new_progress_info['progress'] = 100.0\n\t        yield new_progress_info, writer.compact_segments(segments, config.output.min_words_per_segment)\n\tdef process_url(\n\t    url: str,\n\t    model: 'WhisperModel',\n\t    config: Config,\n\t    progress_info: dict,\n\t) -> Generator[tuple[dict[str, int], list[list[dict[str, Union[str, float]]]]], None, None]:\n", "    url_data = Downloader(playlist_items=config.input.playlist_items, output_dir=config.output.output_dir).download(\n\t        url,\n\t        save_response=config.output.save_yt_dlp_responses,\n\t    )\n\t    if '_type' in url_data and url_data['_type'] == 'playlist':\n\t        url_data = url_data['entries']\n\t    else:\n\t        url_data = [url_data]\n\t    for idx, element in enumerate(tqdm(url_data, desc='URL elements')):\n\t        if not element:\n", "            continue\n\t        new_progress_info = progress_info.copy()\n\t        new_progress_info.update(\n\t            {\n\t                'inner_total': len(url_data),\n\t                'inner_current': idx + 1,\n\t                'inner_status': 'processing',\n\t                'progress': 0.0,\n\t                'remaining_time': None,\n\t            }\n", "        )\n\t        yield new_progress_info, []\n\t        writer = Writer()\n\t        if config.input.skip_if_output_exist and writer.is_output_exist(element['id'], config.output):\n\t            new_progress_info['inner_status'] = 'completed'\n\t            yield new_progress_info, []\n\t            continue\n\t        file_path = os.path.join(config.output.output_dir, f\"{element['id']}.wav\")\n\t        if config.use_wit():\n\t            recognize_generator = WitRecognizer(verbose=config.input.verbose).recognize(file_path, config.wit)\n", "        else:\n\t            recognize_generator = WhisperRecognizer(verbose=config.input.verbose).recognize(\n\t                file_path,\n\t                model,\n\t                config.whisper,\n\t            )\n\t        while True:\n\t            try:\n\t                new_progress_info.update(next(recognize_generator))\n\t                yield new_progress_info, []\n", "            except StopIteration as exception:\n\t                segments = exception.value\n\t                break\n\t        writer.write_all(element['id'], segments, config.output)\n\t        for segment in segments:\n\t            segment['url'] = f\"https://youtube.com/watch?v={element['id']}&t={int(segment['start'])}\"\n\t            segment['file_path'] = file_path\n\t        new_progress_info['inner_status'] = 'completed'\n\t        new_progress_info['progress'] = 100.0\n\t        yield new_progress_info, writer.compact_segments(segments, config.output.min_words_per_segment)\n", "def write_output_sample(segments: list[dict[str, Union[str, float]]], output: Config.Output) -> None:\n\t    if output.output_sample == 0:\n\t        return\n\t    random.shuffle(segments)\n\t    with open(os.path.join(output.output_dir, 'sample.csv'), 'w') as fp:\n\t        writer = csv.DictWriter(fp, fieldnames=['start', 'end', 'text', 'url', 'file_path'])\n\t        writer.writeheader()\n\t        for segment in segments[: output.output_sample]:\n\t            segment['start'] = time_utils.format_timestamp(segment['start'], include_hours=True, decimal_marker=',')\n\t            segment['end'] = time_utils.format_timestamp(segment['end'], include_hours=True, decimal_marker=',')\n", "            writer.writerow(segment)\n"]}
{"filename": "tafrigh/downloader.py", "chunked_list": ["import json\n\timport os\n\tfrom typing import Any, Union\n\timport yt_dlp\n\tclass Downloader:\n\t    def __init__(self, playlist_items: str, output_dir: str):\n\t        self.playlist_items = playlist_items\n\t        self.output_dir = output_dir\n\t        self.youtube_dl_with_archive = yt_dlp.YoutubeDL(self._config(os.path.join(self.output_dir, 'archive.txt')))\n\t        self.youtube_dl_without_archive = yt_dlp.YoutubeDL(self._config(False))\n", "    def _config(self, download_archive: Union[str, bool]) -> dict[str, Any]:\n\t        return {\n\t            'quiet': True,\n\t            'verbose': False,\n\t            'format': 'wav/bestaudio/best',\n\t            'outtmpl': os.path.join(self.output_dir, '%(id)s.%(ext)s'),\n\t            'ignoreerrors': True,\n\t            'download_archive': download_archive,\n\t            'playlist_items': self.playlist_items,\n\t            'postprocessors': [\n", "                {\n\t                    'key': 'FFmpegExtractAudio',\n\t                    'preferredcodec': 'wav',\n\t                },\n\t            ],\n\t        }\n\t    def download(self, url: str, save_response: bool = False) -> dict[str, Any]:\n\t        self.youtube_dl_with_archive.download(url)\n\t        url_data = self.youtube_dl_without_archive.extract_info(url, download=False)\n\t        if save_response:\n", "            self._save_response(url_data)\n\t        return url_data\n\t    def _save_response(self, url_data: dict[str, Any]) -> None:\n\t        if '_type' in url_data and url_data['_type'] == 'playlist':\n\t            for entry in url_data['entries']:\n\t                if entry and 'requested_downloads' in entry:\n\t                    self._remove_postprocessors(entry['requested_downloads'])\n\t        elif 'requested_downloads' in url_data:\n\t            self._remove_postprocessors(url_data['requested_downloads'])\n\t        file_path = os.path.join(self.output_dir, f\"{url_data['id']}.json\")\n", "        with open(file_path, 'w', encoding='utf-8') as fp:\n\t            json.dump(url_data, fp, indent=2, ensure_ascii=False)\n\t    def _remove_postprocessors(self, requested_downloads: list[dict[str, Any]]) -> None:\n\t        for requested_download in requested_downloads:\n\t            requested_download.pop('__postprocessors')\n"]}
{"filename": "tafrigh/recognizers/__init__.py", "chunked_list": []}
{"filename": "tafrigh/recognizers/wit_recognizer.py", "chunked_list": ["import json\n\timport logging\n\timport multiprocessing\n\timport os\n\timport shutil\n\timport tempfile\n\timport time\n\tfrom typing import Generator, Union\n\timport requests\n\tfrom requests.adapters import HTTPAdapter\n", "from tqdm import tqdm\n\tfrom urllib3.util.retry import Retry\n\tfrom tafrigh.audio_splitter import AudioSplitter\n\tfrom tafrigh.config import Config\n\tfrom tafrigh.utils.decorators import minimum_execution_time\n\tclass WitRecognizer:\n\t    def __init__(self, verbose: bool):\n\t        self.verbose = verbose\n\t    def recognize(\n\t        self,\n", "        file_path: str,\n\t        wit_config: Config.Wit,\n\t    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n\t        temp_directory = tempfile.mkdtemp()\n\t        segments = AudioSplitter().split(\n\t            file_path,\n\t            temp_directory,\n\t            max_dur=wit_config.max_cutting_duration,\n\t            expand_segments_with_noise=True,\n\t        )\n", "        retry_strategy = Retry(\n\t            total=5,\n\t            status_forcelist=[429, 500, 502, 503, 504],\n\t            allowed_methods=['POST'],\n\t            backoff_factor=1,\n\t        )\n\t        adapter = HTTPAdapter(max_retries=retry_strategy)\n\t        session = requests.Session()\n\t        session.mount('https://', adapter)\n\t        with multiprocessing.Pool(processes=min(4, multiprocessing.cpu_count() - 1)) as pool:\n", "            async_results = [\n\t                pool.apply_async(self._process_segment, (segment, file_path, wit_config, session))\n\t                for segment in segments\n\t            ]\n\t            transcriptions = []\n\t            with tqdm(total=len(segments), disable=self.verbose is not False) as pbar:\n\t                while async_results:\n\t                    if async_results[0].ready():\n\t                        transcriptions.append(async_results.pop(0).get())\n\t                        pbar.update(1)\n", "                    yield {\n\t                        'progress': round(len(transcriptions) / len(segments) * 100, 2),\n\t                        'remaining_time': (pbar.total - pbar.n) / pbar.format_dict['rate']\n\t                        if pbar.format_dict['rate'] and pbar.total\n\t                        else None,\n\t                    }\n\t                    time.sleep(0.5)\n\t        shutil.rmtree(temp_directory)\n\t        return transcriptions\n\t    @minimum_execution_time(min(4, multiprocessing.cpu_count() - 1) + 1)\n", "    def _process_segment(\n\t        self,\n\t        segment: tuple[str, float, float],\n\t        file_path: str,\n\t        wit_config: Config.Wit,\n\t        session: requests.Session,\n\t    ) -> dict[str, Union[str, float]]:\n\t        segment_file_path, start, end = segment\n\t        with open(segment_file_path, 'rb') as wav_file:\n\t            audio_content = wav_file.read()\n", "        retries = 5\n\t        text = ''\n\t        while retries > 0:\n\t            response = session.post(\n\t                'https://api.wit.ai/speech',\n\t                headers={\n\t                    'Accept': 'application/vnd.wit.20200513+json',\n\t                    'Content-Type': 'audio/wav',\n\t                    'Authorization': f'Bearer {wit_config.wit_client_access_token}',\n\t                },\n", "                data=audio_content,\n\t            )\n\t            if response.status_code == 200:\n\t                try:\n\t                    text = json.loads(response.text)['text']\n\t                    break\n\t                except KeyError:\n\t                    retries -= 1\n\t            else:\n\t                retries -= 1\n", "                time.sleep(min(4, multiprocessing.cpu_count() - 1) + 1)\n\t        if retries == 0:\n\t            logging.warn(\n\t                f\"The segment from `{file_path}` file that starts at {start} and ends at {end}\"\n\t                \" didn't transcribed successfully.\"\n\t            )\n\t        os.remove(segment_file_path)\n\t        return {\n\t            'start': start,\n\t            'end': end,\n", "            'text': text.strip(),\n\t        }\n"]}
{"filename": "tafrigh/recognizers/whisper_recognizer.py", "chunked_list": ["import warnings\n\tfrom typing import Generator, Union\n\timport faster_whisper\n\timport whisper\n\timport whisper_jax\n\tfrom tqdm import tqdm\n\tfrom tafrigh.config import Config\n\tfrom tafrigh.types.whisper.type_hints import WhisperModel\n\tclass WhisperRecognizer:\n\t    def __init__(self, verbose: bool):\n", "        self.verbose = verbose\n\t    def recognize(\n\t        self,\n\t        file_path: str,\n\t        model: WhisperModel,\n\t        whisper_config: Config.Whisper,\n\t    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n\t        with warnings.catch_warnings():\n\t            warnings.simplefilter('ignore')\n\t            if isinstance(model, whisper.Whisper):\n", "                whisper_generator = self._recognize_stable_whisper(file_path, model, whisper_config)\n\t            elif isinstance(model, faster_whisper.WhisperModel):\n\t                whisper_generator = self._recognize_faster_whisper(file_path, model, whisper_config)\n\t            elif isinstance(model, whisper_jax.FlaxWhisperPipline):\n\t                whisper_generator = self._recognize_jax_whisper(file_path, model, whisper_config)\n\t            while True:\n\t                try:\n\t                    yield next(whisper_generator)\n\t                except StopIteration as e:\n\t                    return e.value\n", "    def _recognize_stable_whisper(\n\t        self,\n\t        audio_file_path: str,\n\t        model: whisper.Whisper,\n\t        whisper_config: Config.Whisper,\n\t    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n\t        yield {'progress': 0.0, 'remaining_time': None}\n\t        segments = model.transcribe(\n\t            audio=audio_file_path,\n\t            verbose=self.verbose,\n", "            task=whisper_config.task,\n\t            language=whisper_config.language,\n\t            beam_size=whisper_config.beam_size,\n\t        ).segments\n\t        return [\n\t            {\n\t                'start': segment.start,\n\t                'end': segment.end,\n\t                'text': segment.text.strip(),\n\t            }\n", "            for segment in segments\n\t        ]\n\t    def _recognize_faster_whisper(\n\t        self,\n\t        audio_file_path: str,\n\t        model: faster_whisper.WhisperModel,\n\t        whisper_config: Config.Whisper,\n\t    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n\t        segments, info = model.transcribe(\n\t            audio=audio_file_path,\n", "            task=whisper_config.task,\n\t            language=whisper_config.language,\n\t            beam_size=whisper_config.beam_size,\n\t        )\n\t        converted_segments = []\n\t        last_end = 0\n\t        with tqdm(\n\t            total=round(info.duration, 2),\n\t            unit='sec',\n\t            bar_format='{desc}: {percentage:.2f}%|{bar}| {n:.2f}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]',\n", "            disable=self.verbose is not False,\n\t        ) as pbar:\n\t            for segment in segments:\n\t                converted_segments.append(\n\t                    {\n\t                        'start': segment.start,\n\t                        'end': segment.end,\n\t                        'text': segment.text.strip(),\n\t                    }\n\t                )\n", "                pbar_update = min(segment.end - last_end, info.duration - pbar.n)\n\t                pbar.update(pbar_update)\n\t                last_end = segment.end\n\t                yield {\n\t                    'progress': round(pbar.n / pbar.total * 100, 2),\n\t                    'remaining_time': (pbar.total - pbar.n) / pbar.format_dict['rate']\n\t                    if pbar.format_dict['rate'] and pbar.total\n\t                    else None,\n\t                }\n\t        return converted_segments\n", "    def _recognize_jax_whisper(\n\t        self,\n\t        audio_file_path: str,\n\t        model: whisper_jax.FlaxWhisperPipline,\n\t        whisper_config: Config.Whisper,\n\t    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n\t        yield {'progress': 0.0, 'remaining_time': None}\n\t        segments = model(\n\t            audio_file_path,\n\t            task=whisper_config.task,\n", "            language=whisper_config.language,\n\t            return_timestamps=True,\n\t        )['chunks']\n\t        return [\n\t            {\n\t                'start': segment['timestamp'][0],\n\t                'end': segment['timestamp'][1],\n\t                'text': segment['text'].strip(),\n\t            }\n\t            for segment in segments\n", "        ]\n"]}
{"filename": "tafrigh/utils/decorators.py", "chunked_list": ["import time\n\tfrom functools import wraps\n\tfrom typing import Callable, TypeVar\n\tT = TypeVar(\"T\", bound=Callable)\n\tdef minimum_execution_time(minimum_time: float) -> Callable[[T], T]:\n\t    def decorator(func: T) -> T:\n\t        @wraps(func)\n\t        def wrapper(*args, **kwargs):\n\t            start_time = time.time()\n\t            result = func(*args, **kwargs)\n", "            end_time = time.time()\n\t            elapsed_time = end_time - start_time\n\t            if elapsed_time < minimum_time:\n\t                time.sleep(minimum_time - elapsed_time)\n\t            return result\n\t        return wrapper\n\t    return decorator\n"]}
{"filename": "tafrigh/utils/time_utils.py", "chunked_list": ["def format_timestamp(seconds: float, include_hours: bool = False, decimal_marker: str = '.') -> str:\n\t    assert seconds >= 0, 'Non-negative timestamp expected'\n\t    total_milliseconds = int(round(seconds * 1_000))\n\t    hours, total_milliseconds = divmod(total_milliseconds, 3_600_000)\n\t    minutes, total_milliseconds = divmod(total_milliseconds, 60_000)\n\t    seconds, milliseconds = divmod(total_milliseconds, 1_000)\n\t    if include_hours or hours > 0:\n\t        time_str = f\"{hours:02d}:{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n\t    else:\n\t        time_str = f\"{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n", "    return time_str\n"]}
{"filename": "tafrigh/utils/file_utils.py", "chunked_list": ["import mimetypes\n\tfrom pathlib import Path\n\tmimetypes.init()\n\tdef filter_media_files(paths: list[Path]) -> list[Path]:\n\t    # Filter out non audio or video files\n\t    filtered_media_files: list[str] = []\n\t    for path in paths:\n\t        mime = mimetypes.guess_type(path)[0]\n\t        if mime is None:\n\t            continue\n", "        mime_type = mime.split('/')[0]\n\t        if mime_type not in ('audio', 'video'):\n\t            continue\n\t        filtered_media_files.append(path)\n\t    return filtered_media_files\n"]}
{"filename": "tafrigh/utils/cli_utils.py", "chunked_list": ["import argparse\n\timport re\n\tfrom tafrigh.types.transcript_type import TranscriptType\n\tPLAYLIST_ITEMS_RE = re.compile(\n\t    r'''(?x)\n\t        (?P<start>[+-]?\\d+)?\n\t        (?P<range>[:-]\n\t            (?P<end>[+-]?\\d+|inf(?:inite)?)?\n\t            (?::(?P<step>[+-]?\\d+))?\n\t        )?'''\n", ")\n\tdef parse_args(argv: list[str]) -> argparse.Namespace:\n\t    parser = argparse.ArgumentParser()\n\t    input_group = parser.add_argument_group('Input')\n\t    input_group.add_argument(\n\t        'urls_or_paths',\n\t        nargs='+',\n\t        help='Video/Playlist URLs or local folder/file(s) to transcribe.',\n\t    )\n\t    input_group.add_argument(\n", "        '--skip_if_output_exist',\n\t        action=argparse.BooleanOptionalAction,\n\t        default=False,\n\t        help='Whether to skip generating the output if the output file already exists.',\n\t    )\n\t    input_group.add_argument(\n\t        '--playlist_items',\n\t        type=parse_playlist_items,\n\t        help='Comma separated playlist_index of the items to download. You can specify a range using \"[START]:[STOP][:STEP]\".',\n\t    )\n", "    input_group.add_argument(\n\t        '--verbose',\n\t        action=argparse.BooleanOptionalAction,\n\t        default=False,\n\t        help='Whether to print out the progress and debug messages.',\n\t    )\n\t    whisper_group = parser.add_argument_group('Whisper')\n\t    whisper_group.add_argument(\n\t        '-m',\n\t        '--model_name_or_path',\n", "        default='small',\n\t        help='Name or path of the Whisper model to use.',\n\t    )\n\t    whisper_group.add_argument(\n\t        '-t',\n\t        '--task',\n\t        default='transcribe',\n\t        choices=[\n\t            'transcribe',\n\t            'translate',\n", "        ],\n\t        help=\"Whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate').\",\n\t    )\n\t    whisper_group.add_argument(\n\t        '-l',\n\t        '--language',\n\t        default=None,\n\t        choices=['af', 'am', 'ar', 'as', 'az', 'ba', 'be', 'bg', 'bn', 'bo', 'br', 'bs', 'ca', 'cs', 'cy', 'da', 'de']\n\t        + ['el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fo', 'fr', 'gl', 'gu', 'ha', 'haw', 'he', 'hi', 'hr', 'ht', 'hu']\n\t        + ['hy', 'id', 'is', 'it', 'ja', 'jw', 'ka', 'kk', 'km', 'kn', 'ko', 'la', 'lb', 'ln', 'lo', 'lt', 'lv', 'mg']\n", "        + ['mi', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'ne', 'nl', 'nn', 'no', 'oc', 'pa', 'pl', 'ps', 'pt', 'ro']\n\t        + ['ru', 'sa', 'sd', 'si', 'sk', 'sl', 'sn', 'so', 'sq', 'sr', 'su', 'sv', 'sw', 'ta', 'te', 'tg', 'th', 'tk']\n\t        + ['tl', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'yi', 'yo', 'zh'],\n\t        help='Language spoken in the audio, skip to perform language detection.',\n\t    )\n\t    whisper_group.add_argument(\n\t        '--use_faster_whisper',\n\t        action=argparse.BooleanOptionalAction,\n\t        default=False,\n\t        help='Whether to use Faster Whisper implementation.',\n", "    )\n\t    whisper_group.add_argument(\n\t        '--use_whisper_jax',\n\t        action=argparse.BooleanOptionalAction,\n\t        default=False,\n\t        help='Whether to use Whisper JAX implementation. Make sure to have JAX installed before using this option.',\n\t    )\n\t    whisper_group.add_argument(\n\t        '--beam_size',\n\t        type=int,\n", "        default=5,\n\t        help='Number of beams in beam search, only applicable when temperature is zero.',\n\t    )\n\t    whisper_group.add_argument(\n\t        '--ct2_compute_type',\n\t        default='default',\n\t        choices=[\n\t            'default',\n\t            'int8',\n\t            'int8_float16',\n", "            'int16',\n\t            'float16',\n\t        ],\n\t        help='Quantization type applied while converting the model to CTranslate2 format.',\n\t    )\n\t    wit_group = parser.add_argument_group('Wit')\n\t    wit_group.add_argument(\n\t        '-w',\n\t        '--wit_client_access_token',\n\t        default='',\n", "        help='wit.ai client access token. If provided, wit.ai APIs will be used to do the transcription, otherwise whisper will be used.',\n\t    )\n\t    wit_group.add_argument(\n\t        '--max_cutting_duration',\n\t        type=int,\n\t        default=15,\n\t        choices=range(1, 17),\n\t        metavar='[1-17]',\n\t        help='The maximum allowed cutting duration. It should be between 1 and 17.',\n\t    )\n", "    output_group = parser.add_argument_group('Output')\n\t    output_group.add_argument(\n\t        '--min_words_per_segment',\n\t        type=int,\n\t        default=1,\n\t        help='The minimum number of words should appear in each transcript segment. Any segment have words count less than this threshold will be merged with the next one. Pass 0 to disable this behavior.',\n\t    )\n\t    output_group.add_argument(\n\t        '--save_files_before_compact',\n\t        action=argparse.BooleanOptionalAction,\n", "        default=False,\n\t        help='Saves the output files before applying the compact logic that is based on --min_words_per_segment.',\n\t    )\n\t    output_group.add_argument(\n\t        '--save_yt_dlp_responses',\n\t        action=argparse.BooleanOptionalAction,\n\t        default=False,\n\t        help='Whether to save the yt-dlp library JSON responses or not.',\n\t    )\n\t    output_group.add_argument(\n", "        '--output_sample',\n\t        type=int,\n\t        default=0,\n\t        help='Samples random compacted segments from the output and generates a CSV file contains the sampled data. Pass 0 to disable this behavior.',\n\t    )\n\t    output_group.add_argument(\n\t        '-f',\n\t        '--output_formats',\n\t        nargs='+',\n\t        default='all',\n", "        choices=[transcript_type.value for transcript_type in TranscriptType],\n\t        help='Format of the output file; if not specified, all available formats will be produced.',\n\t    )\n\t    output_group.add_argument('-o', '--output_dir', default='.', help='Directory to save the outputs.')\n\t    return parser.parse_args(argv)\n\tdef parse_playlist_items(arg_value: str) -> str:\n\t    for segment in arg_value.split(','):\n\t        if not segment:\n\t            raise ValueError('There is two or more consecutive commas.')\n\t        mobj = PLAYLIST_ITEMS_RE.fullmatch(segment)\n", "        if not mobj:\n\t            raise ValueError(f'{segment!r} is not a valid specification.')\n\t        _, _, step, _ = mobj.group('start', 'end', 'step', 'range')\n\t        if int_or_none(step) == 0:\n\t            raise ValueError(f'Step in {segment!r} cannot be zero.')\n\t    return arg_value\n\tdef int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n\t    if get_attr and v is not None:\n\t        v = getattr(v, get_attr, None)\n\t    try:\n", "        return int(v) * invscale // scale\n\t    except (ValueError, TypeError, OverflowError):\n\t        return default\n"]}
{"filename": "tafrigh/utils/__init__.py", "chunked_list": []}
{"filename": "tafrigh/utils/wit/file_utils.py", "chunked_list": ["from pathlib import Path\n\tfrom pydub import AudioSegment\n\tdef convert_to_wav(file: Path) -> Path:\n\t    audio_file = AudioSegment.from_file(str(file))\n\t    converted_file_path = file.with_suffix('.wav')\n\t    audio_file.export(str(converted_file_path), format='wav')\n\t    return converted_file_path\n"]}
{"filename": "tafrigh/utils/wit/__init__.py", "chunked_list": []}
{"filename": "tafrigh/utils/whisper/whisper_utils.py", "chunked_list": ["import faster_whisper\n\timport stable_whisper\n\timport whisper_jax\n\tfrom tafrigh.config import Config\n\tfrom tafrigh.types.whisper.type_hints import WhisperModel\n\tdef load_model(whisper_config: Config.Whisper) -> WhisperModel:\n\t    if whisper_config.use_whisper_jax:\n\t        return whisper_jax.FlaxWhisperPipline(f'openai/whisper-{whisper_config.model_name_or_path}')\n\t    elif whisper_config.use_faster_whisper:\n\t        return faster_whisper.WhisperModel(\n", "            whisper_config.model_name_or_path,\n\t            compute_type=whisper_config.ct2_compute_type,\n\t        )\n\t    else:\n\t        return stable_whisper.load_model(whisper_config.model_name_or_path)\n"]}
{"filename": "tafrigh/utils/whisper/__init__.py", "chunked_list": []}
{"filename": "tafrigh/types/transcript_type.py", "chunked_list": ["from enum import Enum\n\tclass TranscriptType(Enum):\n\t    ALL = 'all'\n\t    TXT = 'txt'\n\t    SRT = 'srt'\n\t    VTT = 'vtt'\n\t    NONE = 'none'\n\t    def __str__(self):\n\t        return self.value\n"]}
{"filename": "tafrigh/types/__init__.py", "chunked_list": []}
{"filename": "tafrigh/types/whisper/type_hints.py", "chunked_list": ["from typing import TypeVar\n\timport faster_whisper\n\timport whisper\n\timport whisper_jax\n\tWhisperModel = TypeVar(\n\t    'WhisperModel',\n\t    whisper.Whisper,\n\t    faster_whisper.WhisperModel,\n\t    whisper_jax.FlaxWhisperPipline,\n\t)\n"]}
{"filename": "tafrigh/types/whisper/__init__.py", "chunked_list": []}
