{"filename": "setup.py", "chunked_list": ["from skbuild import setup  # This line replaces 'from setuptools import setup'\n\tsetup(\n\t    name=\"llamppl\",\n\t    version=\"0.0.1\",\n\t    description=\"LLaMPPL: probabilistic programming with language models\",\n\t    author='Alex Lew',\n\t    packages=['llamppl'],\n\t    package_dir={'llamppl': 'llamppl'},\n\t    python_requires=\">=3.8\",\n\t    setup_requires=['numpy'],\n", "    install_requires=['numpy'],\n\t)\n"]}
{"filename": "llamppl/model.py", "chunked_list": ["from .context import ActiveLLaMA, LLaMAContext\n\tclass Model:\n\t    def __init__(self):\n\t        self.weight = 0.0\n\t        self.finished = False\n\t        self.llama = ActiveLLaMA()\n\t        self.mode = \"sample\"\n\t        self.beam_idx = 0\n\t        self.force_eos = False\n\t        self.s = \"\"\n", "    def reset(self):\n\t        self.weight = 0.0\n\t        self.finished = False\n\t        self.llama.reset()\n\t        self.mode = \"sample\"\n\t        self.beam_idx = 0\n\t        self.force_eos = False\n\t        self.s = \"\"\n\t    def new_context(self, prompt=None):\n\t        ctx = LLaMAContext(self.llama)\n", "        if prompt is not None:\n\t            ctx.prompt(prompt)\n\t        return ctx\n\t    def finish(self):\n\t        self.finished = True\n\t    def done_stepping(self):\n\t        return self.finished\n\t    def step(self):\n\t        if not self.done_stepping():\n\t            raise NotImplementedError(\"Model.step() must be implemented by subclasses\")\n", "    def __str__(self):\n\t        return self.s\n\t    def start(self):\n\t        pass\n\t    def score(self, score):\n\t        self.weight += score\n\t    def condition(self, b):\n\t        if not b:\n\t            self.score(float('-inf'))\n\t            self.finish()\n", "    def observe(self, dist, x):\n\t        self.score(dist.log_prob(x))\n\t        return x\n\t    def sample(self, dist, proposal=None):\n\t        # Special logic for beam search\n\t        if self.mode == \"beam\":\n\t            d = dist if proposal is None else proposal\n\t            x, w = d.argmax(self.beam_idx)\n\t            if proposal is not None:\n\t                self.score(dist.log_prob(x))\n", "            else:\n\t                self.score(w)\n\t            return x\n\t        # If no proposal, sample from the distribution\n\t        if proposal is None:\n\t            x, _ = dist.sample() # TODO: update context for dist\n\t            return x\n\t        # Otherwise, sample from the proposal\n\t        else:\n\t            x, q = proposal.sample()\n", "            self.score(dist.log_prob(x) - q)\n\t            return x\n\t    def vocab(self):\n\t        return self.llama.vocab"]}
{"filename": "llamppl/llama_cpp.py", "chunked_list": ["import sys\n\timport os\n\timport ctypes\n\tfrom ctypes import (\n\t    c_int,\n\t    c_float,\n\t    c_char_p,\n\t    c_void_p,\n\t    c_bool,\n\t    POINTER,\n", "    Structure,\n\t    c_uint8,\n\t    c_size_t,\n\t)\n\timport pathlib\n\t# Load the LLaMA library -- code copied from https://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama_cpp.py\n\tdef _load_shared_library(lib_base_name):\n\t    # Determine the file extension based on the platform\n\t    if sys.platform.startswith(\"linux\"):\n\t        lib_ext = \".so\"\n", "    elif sys.platform == \"darwin\":\n\t        lib_ext = \".so\"\n\t    elif sys.platform == \"win32\":\n\t        lib_ext = \".dll\"\n\t    else:\n\t        raise RuntimeError(\"Unsupported platform\")\n\t    # Construct the paths to the possible shared library names\n\t    _base_path = pathlib.Path(__file__).parent.resolve()\n\t    # Searching for the library in the current directory under the name \"libllama\" (default name\n\t    # for llamacpp) and \"llama\" (default name for this repo)\n", "    _lib_paths = [\n\t        _base_path / f\"lib{lib_base_name}{lib_ext}\",\n\t        _base_path / f\"{lib_base_name}{lib_ext}\",\n\t    ]\n\t    if \"LLAMA_CPP_LIB\" in os.environ:\n\t        lib_base_name = os.environ[\"LLAMA_CPP_LIB\"]\n\t        _lib = pathlib.Path(lib_base_name)\n\t        _base_path = _lib.parent.resolve()\n\t        _lib_paths = [_lib.resolve()]\n\t    cdll_args = dict() # type: ignore\n", "    # Add the library directory to the DLL search path on Windows (if needed)\n\t    if sys.platform == \"win32\" and sys.version_info >= (3, 8):\n\t        os.add_dll_directory(str(_base_path))\n\t        cdll_args[\"winmode\"] = 0\n\t    # Try to load the shared library, handling potential errors\n\t    for _lib_path in _lib_paths:\n\t        if _lib_path.exists():\n\t            try:\n\t                return ctypes.CDLL(str(_lib_path), **cdll_args)\n\t            except Exception as e:\n", "                raise RuntimeError(f\"Failed to load shared library '{_lib_path}': {e}\")\n\t    raise FileNotFoundError(\n\t        f\"Shared library with base name '{lib_base_name}' not found\"\n\t    )\n\t# Specify the base name of the shared library to load\n\t_lib_base_name = \"llama\"\n\t# Load the library\n\t_lib = _load_shared_library(_lib_base_name)\n\t# C types\n\tLLAMA_FILE_VERSION = c_int(2)\n", "LLAMA_FILE_MAGIC = b\"ggjt\"\n\tLLAMA_FILE_MAGIC_UNVERSIONED = b\"ggml\"\n\tLLAMA_SESSION_MAGIC = b\"ggsn\"\n\tLLAMA_SESSION_VERSION = c_int(1)\n\tllama_context_p = c_void_p\n\tllama_token = c_int\n\tllama_token_p = POINTER(llama_token)\n\tllama_progress_callback = ctypes.CFUNCTYPE(None, c_float, c_void_p)\n\tclass llama_context_params(Structure):\n\t    _fields_ = [\n", "        (\"n_ctx\", c_int),  # text context\n\t        (\"n_parts\", c_int),  # -1 for default\n\t        (\"n_gpu_layers\", c_int),  # number of layers to store in VRAM\n\t        (\"seed\", c_int),  # RNG seed, 0 for random\n\t        (\"f16_kv\", c_bool),  # use fp16 for KV cache\n\t        (\n\t            \"logits_all\",\n\t            c_bool,\n\t        ),  # the llama_eval() call computes all logits, not just the last one\n\t        (\"vocab_only\", c_bool),  # only load the vocabulary, no weights\n", "        (\"use_mmap\", c_bool),  # use mmap if possible\n\t        (\"use_mlock\", c_bool),  # force system to keep model in RAM\n\t        (\"embedding\", c_bool),  # embedding mode only\n\t        # called with a progress value between 0 and 1, pass NULL to disable\n\t        (\"progress_callback\", llama_progress_callback),\n\t        # context pointer passed to the progress callback\n\t        (\"progress_callback_user_data\", c_void_p),\n\t    ]\n\tllama_context_params_p = POINTER(llama_context_params)\n\tLLAMA_FTYPE_ALL_F32 = c_int(0)\n", "LLAMA_FTYPE_MOSTLY_F16 = c_int(1)  # except 1d tensors\n\tLLAMA_FTYPE_MOSTLY_Q4_0 = c_int(2)  # except 1d tensors\n\tLLAMA_FTYPE_MOSTLY_Q4_1 = c_int(3)  # except 1d tensors\n\tLLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = c_int(\n\t    4\n\t)  # tok_embeddings.weight and output.weight are F16\n\t# LLAMA_FTYPE_MOSTLY_Q4_2 = c_int(5)  # except 1d tensors\n\t# LLAMA_FTYPE_MOSTYL_Q4_3 = c_int(6)  # except 1d tensors\n\tLLAMA_FTYPE_MOSTLY_Q8_0 = c_int(7)  # except 1d tensors\n\tLLAMA_FTYPE_MOSTLY_Q5_0 = c_int(8)  # except 1d tensors\n", "LLAMA_FTYPE_MOSTLY_Q5_1 = c_int(9)  # except 1d tensors\n\t# Misc\n\tc_float_p = POINTER(c_float)\n\tc_uint8_p = POINTER(c_uint8)\n\tc_size_t_p = POINTER(c_size_t)\n\tdef llama_context_default_params():\n\t    return _lib.llama_context_default_params()\n\t_lib.llama_context_default_params.argtypes = []\n\t_lib.llama_context_default_params.restype = llama_context_params\n\tdef llama_init_from_file(path_model, params):\n", "    return _lib.llama_init_from_file(path_model, params)\n\t_lib.llama_init_from_file.argtypes = [c_char_p, llama_context_params]\n\t_lib.llama_init_from_file.restype = llama_context_p\n\t# Frees all allocated memory\n\tdef llama_free(ctx):\n\t    _lib.llama_free(ctx)\n\t_lib.llama_free.argtypes = [llama_context_p]\n\t_lib.llama_free.restype = None\n\t# Returns the number of tokens in the KV cache\n\tdef llama_get_kv_cache_token_count(ctx):\n", "    return _lib.llama_get_kv_cache_token_count(ctx)\n\t_lib.llama_get_kv_cache_token_count.argtypes = [llama_context_p]\n\t_lib.llama_get_kv_cache_token_count.restype = c_int\n\t# Sets the current rng seed.\n\tdef llama_set_rng_seed(ctx, seed):\n\t    return _lib.llama_set_rng_seed(ctx, seed)\n\t_lib.llama_set_rng_seed.argtypes = [llama_context_p, c_int]\n\t_lib.llama_set_rng_seed.restype = None\n\t# Convert the provided text into tokens.\n\t# The tokens pointer must be large enough to hold the resulting tokens.\n", "# Returns the number of tokens on success, no more than n_max_tokens\n\t# Returns a negative number on failure - the number of tokens that would have been returned\n\t# TODO: not sure if correct\n\tdef llama_tokenize(\n\t    ctx,\n\t    text,\n\t    tokens,\n\t    n_max_tokens,\n\t    add_bos):\n\t    return _lib.llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)\n", "_lib.llama_tokenize.argtypes = [llama_context_p, c_char_p, llama_token_p, c_int, c_bool]\n\t_lib.llama_tokenize.restype = c_int\n\tdef llama_n_vocab(ctx: llama_context_p):\n\t    return _lib.llama_n_vocab(ctx)\n\t_lib.llama_n_vocab.argtypes = [llama_context_p]\n\t_lib.llama_n_vocab.restype = c_int\n\tdef llama_n_ctx(ctx):\n\t    return _lib.llama_n_vocab(ctx)\n\t_lib.llama_n_ctx.argtypes = [llama_context_p]\n\t_lib.llama_n_ctx.restype = c_int\n", "# Token logits obtained from the last call to llama_eval()\n\t# The logits for the last token are stored in the last row\n\t# Can be mutated in order to change the probabilities of the next token\n\t# Rows: n_tokens\n\t# Cols: n_vocab\n\tdef llama_get_logits(ctx):\n\t    return _lib.llama_get_logits(ctx)\n\t_lib.llama_get_logits.argtypes = [llama_context_p]\n\t_lib.llama_get_logits.restype = c_float_p\n\t# Token Id -> String. Uses the vocabulary in the provided context\n", "def llama_token_to_str(ctx, token):\n\t    return _lib.llama_token_to_str(ctx, token)\n\t_lib.llama_token_to_str.argtypes = [llama_context_p, llama_token]\n\t_lib.llama_token_to_str.restype = c_char_p\n\t# Special tokens\n\tdef llama_token_bos():\n\t    return _lib.llama_token_bos()\n\t_lib.llama_token_bos.argtypes = []\n\t_lib.llama_token_bos.restype = llama_token\n\tdef llama_token_eos():\n", "    return _lib.llama_token_eos()\n\t_lib.llama_token_eos.argtypes = []\n\t_lib.llama_token_eos.restype = llama_token\n\tdef llama_token_nl():\n\t    return _lib.llama_token_nl()\n\t_lib.llama_token_nl.argtypes = []\n\t_lib.llama_token_nl.restype = llama_token\n\t# Run the llama inference to obtain the logits and probabilities for the next token.\n\t# tokens + n_tokens is the provided batch of new tokens to process\n\t# n_past is the number of tokens to use from previous eval calls\n", "# Returns 0 on success\n\tdef llama_eval_multi(\n\t    ctx,\n\t    tokens,\n\t    token_indices,\n\t    attn_mask,\n\t    n_tokens,\n\t    n_past,\n\t    n_threads\n\t):\n", "    return _lib.llama_eval_multi(ctx, tokens, token_indices, attn_mask, n_tokens, n_past, n_threads)\n\t_lib.llama_eval_multi.argtypes = [llama_context_p, llama_token_p, POINTER(c_int), c_float_p, c_int, c_int, c_int]\n\t_lib.llama_eval_multi.restype = c_int"]}
{"filename": "llamppl/context.py", "chunked_list": ["import numpy as np\n\tfrom . import llama_cpp\n\timport multiprocessing\n\tfrom .util import *\n\tfrom .token import Token\n\tfrom .constants import BOS, EOS\n\tfrom .distributions.tokendist import TokenCategorical\n\tN_THREADS = multiprocessing.cpu_count()\n\tclass TokenTrie:\n\t    # Trie of tokens. At each node, we store the token and its absolute position in the KV cache.\n", "    # We also may store the logprob or logits of the token, if they have been evaluated.\n\t    # A *particle* points to a node in the Trie, and maintains various statistics for\n\t    # querying the language model.\n\t    def __init__(self, kv_index, parent=None, logprob=None, logits=None):\n\t        self.kv_index = kv_index\n\t        self.children = {} # maps token ID to child\n\t        self.logprob = logprob # of this token, given previous\n\t        self.logits = logits # for next token\n\t        if parent is None:\n\t            self.mask_fragment = [0.0] # BOS token attends to itself\n", "        else:\n\t            num_intervening_tokens = kv_index - parent.kv_index - 1\n\t            self.mask_fragment = [-float('inf')] * num_intervening_tokens + [0.0]\n\t    def has_token(self, token_id):\n\t        return token_id in self.children\n\t    def get_token(self, token_id):\n\t        return self.children[token_id]\n\t    def add_token(self, token_id, kv_index, logprob = None, logits = None):\n\t        self.children[token_id] = TokenTrie(kv_index, self, logprob, logits)\n\t        return self.children[token_id]\n", "class LLaMAConfig:\n\t    model_path = None\n\t    @classmethod\n\t    def set_model_path(cls, path):\n\t        if not isinstance(path, str):\n\t            raise ValueError(\"Model path must be a string.\")\n\t        cls.model_path = path.encode('utf-8')\n\tclass ActiveLLaMA:\n\t    def __init__(self):\n\t        self.ctx = llama_cpp.llama_init_from_file(LLaMAConfig.model_path, llama_cpp.llama_context_default_params())\n", "        self.kv_index = -1 # Index of last token in KV cache\n\t        self.vocab = [Token(token, llama_cpp.llama_token_to_str(self.ctx, token).decode('utf-8', errors='ignore')) \n\t                      for token in range(llama_cpp.llama_n_vocab(self.ctx))]\n\t        TokenCategorical.set_vocab(self.vocab)\n\t        # We store the root node of a TokenTrie, but the LlamaContext object is not\n\t        # itself responsible for maintaining the cache.\n\t        self.trie = TokenTrie(0)\n\t        # Evaluate beginning-of-sequence token\n\t        self.eval([BOS], [0], [0.0])\n\t    def reset(self):\n", "        # Free context\n\t        llama_cpp.llama_free(self.ctx)\n\t        # Reinitialize\n\t        self.ctx = llama_cpp.llama_init_from_file(LLaMAConfig.model_path, llama_cpp.llama_context_default_params())\n\t        self.kv_index = -1\n\t        self.trie = TokenTrie(0)\n\t        self.eval([BOS], [0], [0.0])\n\t    def __deepcopy__(self, memo):\n\t        return self\n\t    def eval(self, tokens, indices, attention_mask):\n", "        n_new = len(tokens)\n\t        # TODO: make this number configurable from within Python library\n\t        if self.kv_index + n_new >= 512:\n\t            assert False, \"Cache has more than 512 tokens. Please configure with larger context and try again.\"\n\t        tokens = (llama_cpp.llama_token * len(tokens))(*tokens)\n\t        indices = (llama_cpp.c_int * len(tokens))(*indices)\n\t        attention_mask = (llama_cpp.c_float * (len(attention_mask)))(*attention_mask)\n\t        llama_cpp.llama_eval_multi(self.ctx, tokens, indices, attention_mask, n_new, self.kv_index+1, N_THREADS)\n\t        self.kv_index += n_new\n\t    def get_last_token_logits(self):\n", "        return np.array(llama_cpp.llama_get_logits(self.ctx)[0:llama_cpp.llama_n_vocab(self.ctx)]) # [-1] -- currently we do not have logits_all = True\n\t    def __str__(self):\n\t        # Using recursion, render a Trie in a visually natural way\n\t        def render(node, indent):\n\t            s = \"\"\n\t            for token_id, child in node.children.items():\n\t                s += \" \" * indent + f\"{llama_cpp.llama_token_to_str(self.ctx, token_id).decode('utf-8', errors='ignore')} ({child.kv_index})\\n\"\n\t                s += render(child, indent + 2)\n\t            return s\n\t        return render(self.trie, 0)\n", "    def tokenize(self, prompt):\n\t        prompt = prompt.encode('utf-8')\n\t        tokens = (llama_cpp.llama_token * (len(prompt) + 1))()\n\t        num_tokens = llama_cpp.llama_tokenize(self.ctx, prompt, tokens, len(tokens), False)\n\t        return [self.vocab[i] for i in tokens[:num_tokens]]\n\tdef autoregressive_mask(n_tokens):\n\t    return [[llama_cpp.c_float(0.0)] * (i + 1) + [llama_cpp.c_float(float('-inf'))] * (n_tokens - i - 1) for i in range(n_tokens)]\n\t# LLaMA interface used by a particular particle during inference.\n\t# The particle holds a reference to a Trie of tokens, which is shared\n\t# among many particles. Where possible, it reuses results from this\n", "# Trie. When it needs to evaluate a new token, it adds it to the Trie.\n\tclass LLaMAContext:\n\t    def __init__(self, llama, trie=None, index=1, mask=None, kv_index=0):\n\t        self.llama = llama\n\t        self.vocab = self.llama.vocab\n\t        self.trie = trie if trie is not None else llama.trie\n\t        self.current_index = index # BOS is token 0, already included in context\n\t        self.current_mask = mask if mask is not None else [0.0] # BOS token is attended to\n\t        self.kv_index = kv_index # Equal to self.trie.kv_index... so maybe can delete?\n\t    def reset(self):\n", "        self.llama.reset()\n\t        self.trie = self.llama.trie\n\t        self.current_index = 1\n\t        self.current_mask = [0.0]\n\t        self.kv_index = 0\n\t    def extend_mask(self):\n\t        if self.kv_index < self.llama.kv_index:\n\t            self.current_mask.extend([-float('inf')] * (self.llama.kv_index - self.kv_index))\n\t            self.kv_index = self.llama.kv_index\n\t    def prompt(self, prompt):\n", "        # Tokenize the prompt\n\t        tokens = self.llama.tokenize(prompt)\n\t        num_tokens = len(tokens)\n\t        # Advance in the trie as far as possible\n\t        consumed = 0\n\t        while consumed < num_tokens:\n\t            token = tokens[consumed]\n\t            if self.trie.has_token(token.token_id):\n\t                self.trie           = self.trie.get_token(token.token_id)\n\t                consumed           += 1\n", "                self.current_index += 1\n\t                self.current_mask  += self.trie.mask_fragment\n\t                self.kv_index       = self.trie.kv_index\n\t            else:\n\t                break\n\t        num_tokens -= consumed\n\t        tokens      = tokens[consumed:]\n\t        if num_tokens == 0:\n\t            return self\n\t        # Update mask and kv_index\n", "        self.extend_mask()\n\t        # Compute indices and attention mask\n\t        indices = range(self.current_index, self.current_index + num_tokens)\n\t        attention_mask = sum([self.current_mask + m for m in autoregressive_mask(num_tokens)], [])\n\t        # Evaluate\n\t        self.llama.eval([t.token_id for t in tokens], indices, attention_mask)\n\t        # Update stats\n\t        for token in tokens:\n\t            self.kv_index += 1\n\t            self.current_index += 1\n", "            self.current_mask.append(0.0)\n\t            self.trie = self.trie.add_token(token.token_id, self.kv_index)\n\t        # Save logits for end of prompt\n\t        self.trie.logits = self.llama.get_last_token_logits()\n\t        return self\n\t    def logits(self):\n\t        if self.trie.logits is None and self.trie.kv_index == self.llama.kv_index:\n\t            self.trie.logits = self.llama.get_last_token_logits()\n\t        # TODO: error if still None?\n\t        return self.trie.logits\n", "    def observe_token(self, token_id):\n\t        # Check if token is in trie\n\t        if self.trie.has_token(token_id):\n\t            # If so, update trie and return logprob\n\t            self.trie = self.trie.get_token(token_id)\n\t            self.current_mask += self.trie.mask_fragment\n\t            self.current_index += 1\n\t            self.kv_index = self.trie.kv_index\n\t            logprob = self.trie.logprob\n\t            if logprob is None and self.trie.parent.logits is not None:\n", "                logprob = np.log(softmax(self.trie.parent.logits)[token_id])\n\t                self.trie.logprob = logprob\n\t            # TODO: error if still None?\n\t            return logprob\n\t        # If not, extend mask and evaluate\n\t        logits = self.logits()\n\t        logprob = (logits - logsumexp(logits))[token_id]\n\t        self.extend_mask()\n\t        self.current_mask.append(0.0)\n\t        self.llama.eval([token_id], [self.current_index], self.current_mask)\n", "        self.current_index += 1\n\t        self.kv_index += 1\n\t        self.trie = self.trie.add_token(token_id, self.kv_index, logprob, self.llama.get_last_token_logits())\n\t        return logprob\n\t    def observe_tokens(self, tokens):\n\t        score = 0.0\n\t        for token in tokens:\n\t            score += self.observe_token(token)\n\t        return score\n\t    def observe_text(self, s):\n", "        # Tokenize string\n\t        tokens = (llama_cpp.llama_token * (len(s) + 1))()\n\t        num_tokens = llama_cpp.llama_tokenize(self.llama.ctx, s, tokens, len(tokens), False)\n\t        tokens = tokens[:num_tokens]\n\t        # Observe tokens\n\t        return self.observe_tokens(tokens)\n\t    def __deepcopy__(self, memo):\n\t        # Does not copy the context or trie, which are shared across copies.\n\t        # The mask is just a list of floats and can be copied shallowly.\n\t        return LLaMAContext(self.llama, self.trie, self.current_index, self.current_mask.copy(), self.kv_index)\n"]}
{"filename": "llamppl/__init__.py", "chunked_list": ["from .llama_cpp import *\n\tfrom .context import *\n\tfrom .inference.smc_standard import *\n\tfrom .util import *\n\tfrom .model import *\n\tfrom .inference.smc_steer import smc_steer\n\tfrom .inference.beam import beam\n\tfrom .constants import *\n\tfrom .distributions.transformer import *\n\tfrom .distributions.tokendist import *\n", "from .distributions.geometric import *"]}
{"filename": "llamppl/constants.py", "chunked_list": ["from .llama_cpp import llama_token_eos, llama_token_bos\n\tEOS = llama_token_eos()\n\tBOS = llama_token_bos()"]}
{"filename": "llamppl/util.py", "chunked_list": ["import numpy as np\n\tdef logsumexp(arr):\n\t    # Numerically stable implementation\n\t    m = np.max(arr)\n\t    return m + np.log(np.sum(np.exp(arr - m)))\n\tdef lognormalize(arr):\n\t    # Numerically stable implementation\n\t    return arr - logsumexp(arr)\n\tdef softmax(arr):\n\t    # Numerically stable implementation\n", "    return np.exp(arr - logsumexp(arr))"]}
{"filename": "llamppl/token.py", "chunked_list": ["class Token:\n\t    def __init__(self, token_id, token_str):\n\t        self.token_id = token_id\n\t        self.token_str = token_str\n\t    # Support adding tokens to strings\n\t    def __add__(self, other):\n\t        if isinstance(other, Token):\n\t            return self.token_str + other.token_str\n\t        else:\n\t            return self.token_str + other\n", "    # Support adding strings to tokens\n\t    def __radd__(self, other):\n\t        return other + self.token_str\n\t    # Support checking for EOS\n\t    def __eq__(self, other):\n\t        if isinstance(other, Token):\n\t            return self.token_id == other.token_id\n\t        elif isinstance(other, int):\n\t            return self.token_id == other\n\t        else:\n", "            return self.token_str == other\n\t    def __str__(self):\n\t        return self.token_str"]}
{"filename": "llamppl/inference/smc_steer.py", "chunked_list": ["# SMC steering algorithm from https://arxiv.org/pdf/2306.03081.pdf\n\t# Optimal resampling from https://academic.oup.com/jrsssb/article/65/4/887/7092877\n\timport numpy as np\n\timport copy\n\tfrom ..util import logsumexp, softmax\n\tdef find_c(weights, N):\n\t    # Sort the weights\n\t    sorted_weights = np.sort(weights)\n\t    # Find the smallest chi\n\t    B_val = 0.0\n", "    A_val = len(weights)\n\t    for i in range(len(sorted_weights)):\n\t        chi = sorted_weights[i]\n\t        # Calculate A_val -- number of weights larger than chi\n\t        A_val -= 1\n\t        # Update B_val -- add the sum of weights smaller than or equal to chi\n\t        B_val += chi\n\t        if B_val / chi + A_val - N <= 1e-12:\n\t            return (N - A_val) / B_val\n\t    return N\n", "def resample_optimal(weights, N):\n\t    c = find_c(weights, N)\n\t    # Weights for which c * w >= 1 are deterministically resampled\n\t    deterministic = np.where(c * weights >= 1)[0]\n\t    # Weights for which c * w <= 1 are stochastically resampled\n\t    stochastic = np.where(c * weights < 1)[0]\n\t    # Stratified sampling to generate N-len(deterministic) indices\n\t    # from the stochastic weights\n\t    n_stochastic = len(stochastic)\n\t    n_resample = N - len(deterministic)\n", "    if n_resample == 0:\n\t        return deterministic, np.array([], dtype=int), c\n\t    K = np.sum(weights[stochastic]) / (n_resample)\n\t    u = np.random.uniform(0, K)\n\t    i = 0\n\t    stoch_resampled = np.array([], dtype=int)\n\t    while i < n_stochastic:\n\t        u = u - weights[stochastic[i]]\n\t        if u <= 0:\n\t            # Add stochastic[i] to resampled indices\n", "            stoch_resampled = np.append(stoch_resampled, stochastic[i])\n\t            # Update u\n\t            u = u + K\n\t            i = i + 1\n\t        else:\n\t            i += 1\n\t    # Concatenate the deterministic and stochastic resampled indices\n\t    #resampled = np.concatenate((deterministic, stoch_resampled))\n\t    #return resampled\n\t    return deterministic, stoch_resampled, c\n", "def smc_steer(model, n_particles, n_beam):\n\t    # Create n_particles copies of the model\n\t    particles = [copy.deepcopy(model) for _ in range(n_particles)]\n\t    for particle in particles:\n\t        particle.start()\n\t    for particle in particles:\n\t        for _ in range(n_beam):\n\t            print(\"\\n\")\n\t    while any(map(lambda p: not p.done_stepping(), particles)):\n\t        # Count the number of finished particles\n", "        n_finished = sum(map(lambda p: p.done_stepping(), particles))\n\t        n_total = n_finished + (n_particles - n_finished) * n_beam\n\t        # Create a super-list of particles that has n_beam copies of each\n\t        super_particles = []\n\t        for p in particles:\n\t            super_particles.append(p)\n\t            if p.done_stepping():\n\t                p.weight += np.log(n_total) - np.log(n_particles)\n\t            else:\n\t                p.weight += np.log(n_total) - np.log(n_particles) - np.log(n_beam)\n", "                super_particles.extend([copy.deepcopy(p) for _ in range(n_beam-1)])\n\t        # Step each super-particle\n\t        for i, p in enumerate(super_particles):\n\t            # Step\n\t            if not p.done_stepping():\n\t                p.step()\n\t            print(f\"Particle {i}: {p} (weight {p.weight})\")\n\t        # Use optimal resampling to resample\n\t        W = np.array([p.weight for p in super_particles])\n\t        W_tot = logsumexp(W)\n", "        W_normalized = softmax(W)\n\t        det_indices, stoch_indices, c = resample_optimal(W_normalized, n_particles)\n\t        particles = [super_particles[i] for i in np.concatenate((det_indices, stoch_indices))]\n\t        # For deterministic particles: w = w * N/N'\n\t        for i in det_indices:\n\t            super_particles[i].weight += np.log(n_particles) - np.log(n_total)\n\t        # For stochastic particles: w = 1/c * total       sum(stoch weights) / num_stoch = sum(stoch weights / total) / num_stoch * total * N/M\n\t        for i in stoch_indices:\n\t            super_particles[i].weight = W_tot - np.log(c) + np.log(n_particles) - np.log(n_total)\n\t    # Return the particles\n", "    return particles\n"]}
{"filename": "llamppl/inference/smc_standard.py", "chunked_list": ["import copy\n\timport numpy as np\n\tfrom ..util import logsumexp\n\tdef smc_standard(model, n_particles, ess_threshold=0.5):\n\t    # Create n_particles copies of the model\n\t    particles = [copy.deepcopy(model) for _ in range(n_particles)]\n\t    weights = [0.0 for _ in range(n_particles)]\n\t    while any(map(lambda p: not p.done_stepping(), particles)):\n\t        # Step each particle\n\t        for i, p in enumerate(particles):\n", "            if not p.done_stepping():\n\t                p.step()\n\t            print(f\"Particle {i}: {p} (weight {p.weight})\")\n\t        # Normalize weights\n\t        W = np.array([p.weight for p in particles])\n\t        w_sum = logsumexp(W)\n\t        normalized_weights = W - w_sum\n\t        # Resample if necessary\n\t        if -logsumexp(normalized_weights * 2) < np.log(ess_threshold) + np.log(n_particles):\n\t            # Alternative implementation uses a multinomial distribution and only makes n-1 copies, reusing existing one, but fine for now\n", "            probs = np.exp(normalized_weights)\n\t            particles = [copy.deepcopy(particles[np.random.choice(range(len(particles)), p=probs)]) for _ in range(n_particles)]\n\t            avg_weight = w_sum - np.log(n_particles)\n\t            for p in particles:\n\t                p.weight = avg_weight\n\t    # Return the particles\n\t    return particles\n"]}
{"filename": "llamppl/inference/beam.py", "chunked_list": ["import numpy as np\n\timport copy\n\tfrom ..util import logsumexp, softmax\n\tdef copy_with_index(particle, index):\n\t    new_particle = copy.deepcopy(particle)\n\t    new_particle.beam_idx = index\n\t    new_particle.force_eos = False\n\t    return new_particle\n\tdef beam(model, n_beam, n_explore=None, force_eos = False, mode = \"beam\"):\n\t    # Create n_beam copies of the model\n", "    particles = [copy.deepcopy(model) for _ in range(n_beam)]\n\t    if n_explore is None:\n\t        n_explore = n_beam\n\t    for (i, particle) in enumerate(particles):\n\t        particle.mode = mode\n\t        particle.start()\n\t        particle.beam_idx = i+1\n\t        particle.step()\n\t    while any(map(lambda p: not p.done_stepping(), particles)):\n\t        # Create a super-list of particles that has n_beam copies of each\n", "        super_particles = []\n\t        for p in particles:\n\t            p.beam_idx = 1\n\t            super_particles.append(p)\n\t            super_particles.extend([copy_with_index(p,i+2) for i in range(n_explore-1)])\n\t            if force_eos:\n\t                super_particles.append(copy.deepcopy(p))\n\t                super_particles[-1].force_eos = True\n\t        # Step each super-particle\n\t        for i, p in enumerate(super_particles):\n", "            # Step\n\t            if not p.done_stepping():\n\t                p.step()\n\t            print(f\"Particle {i}: {p} (weight {p.weight})\")\n\t        # Take the best n_beam particles\n\t        super_particles.sort(key=lambda p: p.weight, reverse=True)\n\t        particles = super_particles[:n_beam]\n\t    # Return the particles\n\t    return particles\n"]}
{"filename": "llamppl/distributions/categorical.py", "chunked_list": ["from .distribution import Distribution\n\timport numpy as np\n\tclass Categorical(Distribution):\n\t    def __init__(self, probs):\n\t        self.probs = probs\n\t    def sample(self):\n\t        x = np.random.choice(len(self.probs), p=self.probs)\n\t        return x, self.log_prob(x)\n\t    def argmax(self, idx):\n\t        return np.argsort(self.probs)[-idx]\n", "    def log_prob(self, value):\n\t        return np.log(self.probs[value])"]}
{"filename": "llamppl/distributions/logcategorical.py", "chunked_list": ["from .distribution import Distribution\n\timport numpy as np\n\tfrom ..util import softmax\n\tclass LogCategorical(Distribution):\n\t    def __init__(self, logits):\n\t        self.probs = softmax(logits)\n\t    def sample(self):\n\t        n = np.random.choice(len(self.probs), p=(self.probs))\n\t        return n, self.log_prob(n)\n\t    def log_prob(self, value):\n", "        return np.log(self.probs[value])\n\t    def argmax(self, idx):\n\t        return np.argsort(self.probs)[-idx]"]}
{"filename": "llamppl/distributions/transformer.py", "chunked_list": ["from .distribution import Distribution\n\tfrom ..util import softmax\n\tfrom ..token import Token\n\timport numpy as np\n\tclass Transformer(Distribution):\n\t    # TODO: support custom temperatures\n\t    def __init__(self, ctx):\n\t        self.ctx = ctx\n\t    def log_prob(self, x):\n\t        # Check if x is a token or an int\n", "        if isinstance(x, Token):\n\t            return self.ctx.observe_token(x.token_id)\n\t        else:\n\t            return self.ctx.observe_token(x)\n\t    def sample(self):\n\t        probs = softmax(self.ctx.logits())\n\t        token_id = np.random.choice(len(probs), p=(probs))\n\t        logprob = self.ctx.observe_token(token_id)\n\t        return self.ctx.vocab[token_id], logprob\n\t    def argmax(self, idx):\n", "        token_id = np.argsort(self.ctx.logits())[-idx]\n\t        logprob = self.ctx.observe_token(token_id)\n\t        return self.ctx.vocab[token_id], logprob"]}
{"filename": "llamppl/distributions/geometric.py", "chunked_list": ["from .distribution import Distribution\n\timport numpy as np\n\tclass Geometric(Distribution):\n\t    def __init__(self, p):\n\t        self.p = p\n\t    def sample(self):\n\t        n = np.random.geometric(self.p)\n\t        return n, self.log_prob(n)\n\t    def log_prob(self, value):\n\t        return np.log(self.p) + np.log(1 - self.p)*(value - 1)\n", "    def argmax(self, idx):\n\t        return idx - 1 # Most likely outcome is 0, then 1, etc."]}
{"filename": "llamppl/distributions/tokendist.py", "chunked_list": ["from .distribution import Distribution\n\timport numpy as np\n\tfrom ..util import softmax\n\tclass TokenCategorical(Distribution):\n\t    vocab = None\n\t    @classmethod\n\t    def set_vocab(cls, vocab):\n\t        cls.vocab = vocab\n\t    def __init__(self, logits):\n\t        # Error if class variable vocab is not set\n", "        if TokenCategorical.vocab is None:\n\t            raise Exception(\"TokenCategorical.vocab is not set\")\n\t        self.probs = softmax(logits)\n\t    def sample(self):\n\t        n = np.random.choice(len(self.probs), p=(self.probs))\n\t        return TokenCategorical.vocab[n], np.log(self.probs[n])\n\t    def log_prob(self, value):\n\t        return np.log(self.probs[value.token_id])\n\t    def argmax(self, idx):\n\t        tok = np.argsort(self.probs)[-idx]\n", "        return TokenCategorical.vocab[tok], np.log(self.probs[tok])"]}
{"filename": "llamppl/distributions/distribution.py", "chunked_list": ["class Distribution:\n\t    def log_prob(self, x):\n\t        raise NotImplementedError()\n\t    def sample(self):\n\t        raise NotImplementedError()\n\t    def argmax(self, idx):\n\t        raise NotImplementedError()"]}
{"filename": "examples/infilling.py", "chunked_list": ["import llamppl as llp\n\timport numpy as np\n\tclass Infilling(llp.Model):\n\t    def __init__(self, words):\n\t        super().__init__()\n\t        self.s = words.pop(0)\n\t        self.ctx = self.new_context(self.s)\n\t        self.remaining_segments = [self.llama.tokenize(w) for w in words]\n\t    def start(self):\n\t        self.step()\n", "    def step(self):\n\t        # Generate a token\n\t        n = self.sample(llp.Geometric(0.5)) + 1\n\t        for _ in range(n):\n\t            self.s += self.sample(llp.Transformer(self.ctx))\n\t        # Observe the next tokens\n\t        for token in self.remaining_segments.pop(0):\n\t            self.s += self.observe(llp.Transformer(self.ctx), token)\n\t        # Check if done\n\t        if len(self.remaining_segments) == 0:\n", "            self.observe(llp.Transformer(self.ctx), llp.EOS)\n\t            self.finish()\n\t# Create the model\n\tllp.LLaMAConfig.set_model_path(input(\"Path to GGML LLaMA model weights: \"))\n\tmodel = Infilling([\"Well, you see, every\", \" he\", \" to\", \" another\", \"!\"])\n\t# Run SMC\n\tfor i,p in enumerate(llp.smc_steer(model, 4,4)):\n\t    print(f\"Particle {i}: {p} (weight {p.weight})\")\n"]}
{"filename": "examples/prompt_intersection.py", "chunked_list": ["import llamppl as llp\n\tclass PromptIntersection(llp.Model):\n\t    # Constructs the model. Should not make any\n\t    # random choices, as it will only be executed\n\t    # one time, before inference begins.\n\t    def __init__(self, prompts):\n\t        super().__init__()\n\t        self.contexts = [self.new_context(prompt) for prompt in prompts]\n\t    def step(self):\n\t        # Generate proposed token\n", "        token = self.sample(llp.Transformer(self.contexts[0]), \n\t                            proposal=self.locally_optimal_proposal())\n\t            # Observe from other LLMs\n\t        for context in self.contexts[1:]:\n\t            self.observe(llp.Transformer(context), token)\n\t        # Check for eos \n\t        if token == llp.EOS:\n\t            self.finish()\n\t            return\n\t        # Update generated string\n", "        self.s += token\n\t    # Greedy / locally optimal proposal for next token\n\t    def locally_optimal_proposal(self):\n\t        logits = [context.logits() for context in self.contexts]\n\t        logprobs = [l - llp.logsumexp(l) for l in logits]\n\t        p_scores = sum(logprobs)\n\t        q_logprobs = p_scores - llp.logsumexp(p_scores)\n\t        return llp.TokenCategorical(q_logprobs)\n\t# Create the model\n\tllp.LLaMAConfig.set_model_path(input(\"Path to GGML LLaMA model weights: \"))\n", "prompts = [\" My favorite writer is probably\", \" My favorite physicist is probably\"]\n\tmodel = PromptIntersection(prompts)\n\t# Run SMC\n\tfor i, p in enumerate(llp.smc_steer(model, 5, 3)):\n\t    print(f\"Particle {i}: {p} (weight {p.weight})\")"]}
{"filename": "examples/constraints.py", "chunked_list": ["import llamppl as llp\n\timport numpy as np\n\tdef can_follow(str_so_far, s):\n\t    if isinstance(s, llp.Token):\n\t        s = str(s)\n\t    if len(s.strip()) > 5:\n\t        return False\n\t    if len(s.strip()) == 0:\n\t        return True\n\t    if not s[0].isalpha():\n", "        return True\n\t    if len(str_so_far) == 0:\n\t        return True # First token, can be alphanumeric\n\t    words = str_so_far.split()\n\t    if len(words) >= 1 and len(words[-1]) + len(s) <= 5:\n\t        return True\n\t    else:\n\t        return False\n\tclass ConstraintModel(llp.Model):\n\t    # Constructs the model. Should not make any\n", "    # random choices, as it will only be executed\n\t    # one time, before inference begins.\n\t    def __init__(self, prompt, can_follow):\n\t        super().__init__()\n\t        self.context = self.new_context(prompt)\n\t        self.can_follow = can_follow\n\t    def step(self):\n\t        # Generate proposed token.\n\t        token = self.sample(llp.Transformer(self.context),\n\t                            proposal=self.locally_optimal_proposal())\n", "        # Condition on constraint\n\t        self.condition(self.can_follow(self.s, token))\n\t        # Check if done\n\t        if token == llp.EOS:\n\t            self.finish()\n\t            return\n\t        # Update generated string\n\t        self.s += token\n\t    def locally_optimal_proposal(self):\n\t        # Get next token logits\n", "        logits = self.context.logits()\n\t        # Compute locally optimal proposal\n\t        mask = np.array([0.0 if self.can_follow(self.s, v) else float('-inf') for v in self.vocab()])\n\t        q_logprobs = llp.lognormalize(logits + mask)\n\t        return llp.TokenCategorical(q_logprobs)\n\t# Create the model\n\tllp.LLaMAConfig.set_model_path(input(\"Path to GGML LLaMA model weights: \"))\n\tprompt = \" The Fed says\"\n\tmodel = ConstraintModel(prompt, can_follow)\n\tfor i, p in enumerate(llp.smc_steer(model, 8, 3)):\n", "    print(f\"Particle {i}: {p} (weight {p.weight})\")\n"]}
