{"filename": "gradio_tools/__init__.py", "chunked_list": ["from gradio_tools.tools import (BarkTextToSpeechTool, ClipInterrogatorTool,\n\t                                DocQueryDocumentAnsweringTool, GradioTool,\n\t                                ImageCaptioningTool, ImageToMusicTool,\n\t                                SAMImageSegmentationTool,\n\t                                StableDiffusionPromptGeneratorTool,\n\t                                StableDiffusionTool, TextToVideoTool,\n\t                                WhisperAudioTranscriptionTool)\n\t__all__ = [\n\t    \"GradioTool\",\n\t    \"StableDiffusionTool\",\n", "    \"ClipInterrogatorTool\",\n\t    \"ImageCaptioningTool\",\n\t    \"ImageToMusicTool\",\n\t    \"WhisperAudioTranscriptionTool\",\n\t    \"StableDiffusionPromptGeneratorTool\",\n\t    \"TextToVideoTool\",\n\t    \"DocQueryDocumentAnsweringTool\",\n\t    \"BarkTextToSpeechTool\",\n\t    \"SAMImageSegmentationTool\",\n\t]\n"]}
{"filename": "gradio_tools/tools/sam_with_clip.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass SAMImageSegmentationTool(GradioTool):\n\t    \"\"\"Tool for segmenting images based on natural language queries.\"\"\"\n\t    def __init__(\n\t        self,\n", "        name=\"SAMImageSegmentation\",\n\t        description=(\n\t            \"A tool for identifying objects in images. \"\n\t            \"Input will be a five strings separated by a |: \"\n\t            \"the first will be the full path or URL to an image file. \"\n\t            \"The second will be the string query describing the objects to identify in the image. \"\n\t            \"The query string should be as detailed as possible. \"\n\t            \"The third will be the predicted_iou_threshold, if not specified by the user set it to 0.9. \"\n\t            \"The fourth will be the stability_score_threshold, if not specified by the user set it to 0.8. \"\n\t            \"The fifth is the clip_threshold, if not specified by the user set it to 0.85. \"\n", "            \"The output will the a path with an image file with the identified objects overlayed in the image.\"\n\t        ),\n\t        src=\"curt-park/segment-anything-with-clip\",\n\t        hf_token=None,\n\t        duplicate=False,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n\t        try:\n\t            (\n", "                image,\n\t                query,\n\t                predicted_iou_threshold,\n\t                stability_score_threshold,\n\t                clip_threshold,\n\t            ) = query.split(\"|\")\n\t        except ValueError as e:\n\t            raise ValueError(\n\t                \"Not enough arguments passed to the SAMImageSegmentationTool! \"\n\t                \"Expected 5 (image, query, predicted_iou_threshold, stability_score_threshold, clip_threshold)\"\n", "            ) from e\n\t        return self.client.submit(\n\t            float(predicted_iou_threshold),\n\t            float(stability_score_threshold),\n\t            float(clip_threshold),\n\t            image,\n\t            query.strip(),\n\t            api_name=\"/predict\",\n\t        )\n\t    def postprocess(self, output: str) -> str:\n", "        return output\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Number(), gr.Number(), gr.Number(), gr.Image(), gr.Textbox()]\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Image()]\n"]}
{"filename": "gradio_tools/tools/image_captioning.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass ImageCaptioningTool(GradioTool):\n\t    \"\"\"Tool for captioning images.\"\"\"\n\t    def __init__(\n\t        self,\n", "        name=\"ImageCaptioner\",\n\t        description=(\n\t            \"An image captioner. Use this to create a caption for an image. \"\n\t            \"Input will be a path to an image file. \"\n\t            \"The output will be a caption of that image.\"\n\t        ),\n\t        src=\"gradio-client-demos/BLIP-2\",\n\t        hf_token=None,\n\t        duplicate=True,\n\t    ) -> None:\n", "        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n\t        return self.client.submit(query.strip(\"'\"), \"Beam Search\", fn_index=0)\n\t    def postprocess(self, output: str) -> str:\n\t        return output  # type: ignore\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Image()]\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Textbox()]\n"]}
{"filename": "gradio_tools/tools/document_qa.py", "chunked_list": ["from typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass DocQueryDocumentAnsweringTool(GradioTool):\n\t    def __init__(\n\t        self,\n\t        name=\"DocQuery\",\n\t        description=(\n", "            \"A tool for answering questions about a document from the from the image of the document. Input will be a two strings separated by a |: the first will be the path or URL to an image of a document. The second will be your question about the document.\"\n\t            \"The output will the text answer to your question.\"\n\t        ),\n\t        src=\"abidlabs/docquery\",\n\t        hf_token=None,\n\t        duplicate=True,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n\t        img, question = query.split(\"|\")\n", "        return self.client.submit(img.strip(), question.strip(), api_name=\"/predict\")\n\t    def postprocess(self, output: str) -> str:\n\t        return output\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Image(), gr.Textbox()]\n"]}
{"filename": "gradio_tools/tools/prompt_generator.py", "chunked_list": ["from __future__ import annotations\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tclass StableDiffusionPromptGeneratorTool(GradioTool):\n\t    def __init__(\n\t        self,\n\t        name=\"StableDiffusionPromptGenerator\",\n\t        description=(\n\t            \"Use this tool to improve a prompt for stable diffusion and other image and video generators. \"\n\t            \"This tool will refine your prompt to include key words and phrases that make \"\n", "            \"stable diffusion and other art generation algorithms perform better. The input is a prompt text string \"\n\t            \"and the output is a prompt text string\"\n\t        ),\n\t        src=\"microsoft/Promptist\",\n\t        hf_token=None,\n\t        duplicate=False,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n\t        return self.client.submit(query, api_name=\"/predict\")\n", "    def postprocess(self, output: str) -> str:\n\t        return output\n"]}
{"filename": "gradio_tools/tools/whisper.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass WhisperAudioTranscriptionTool(GradioTool):\n\t    def __init__(\n\t        self,\n\t        name=\"WhisperAudioTranscription\",\n", "        description=(\n\t            \"A tool for transcribing audio. Use this tool to transcribe an audio file. \"\n\t            \"track from an image. Input will be a path to an audio file. \"\n\t            \"The output will the text transcript of that file.\"\n\t        ),\n\t        src=\"abidlabs/whisper\",\n\t        hf_token=None,\n\t        duplicate=False,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n", "    def create_job(self, query: str) -> Job:\n\t        return self.client.submit(query, api_name=\"/predict\")\n\t    def postprocess(self, output: str) -> str:\n\t        return output\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Audio()]\n"]}
{"filename": "gradio_tools/tools/gradio_tool.py", "chunked_list": ["from __future__ import annotations\n\timport time\n\tfrom abc import abstractmethod\n\tfrom typing import Any, List, Tuple, Union\n\timport gradio_client as grc\n\timport huggingface_hub\n\tfrom gradio_client.client import Job\n\tfrom gradio_client.utils import QueueError\n\ttry:\n\t    import langchain as lc\n", "    LANGCHAIN_INSTALLED = True\n\texcept (ModuleNotFoundError, ImportError):\n\t    LANGCHAIN_INSTALLED = False\n\tclass GradioTool:\n\t    def __init__(\n\t        self,\n\t        name: str,\n\t        description: str,\n\t        src: str,\n\t        hf_token: str | None = None,\n", "        duplicate: bool = True,\n\t    ) -> None:\n\t        self.name = name\n\t        self.description = description\n\t        if hf_token and self._is_space(src) and duplicate:\n\t            self.client = grc.Client.duplicate(from_id=src, hf_token=hf_token)\n\t            self.src = self.client.space_id\n\t        else:\n\t            self.src = src\n\t            self.client = grc.Client(self.src, hf_token=hf_token)\n", "        self.description += (\n\t            \"If the output is the string QUEUE_FULL you \"\n\t            \"should try this action again and never stop until you \"\n\t            \"don't see this message.\"\n\t        )\n\t        self._block = None\n\t    @staticmethod\n\t    def _is_space(src: str) -> bool:\n\t        try:\n\t            huggingface_hub.get_space_runtime(src)\n", "            return True\n\t        except huggingface_hub.hf_api.RepositoryNotFoundError:\n\t            return False\n\t    @abstractmethod\n\t    def create_job(self, query: str) -> Job:\n\t        pass\n\t    @abstractmethod\n\t    def postprocess(self, output: Union[Tuple[Any], Any]) -> str:\n\t        pass\n\t    def run(self, query: str):\n", "        job = self.create_job(query)\n\t        while not job.done():\n\t            status = job.status()\n\t            print(f\"\\nJob Status: {str(status.code)} eta: {status.eta}\")\n\t            time.sleep(30)\n\t        try:\n\t            output = self.postprocess(job.result())\n\t        except QueueError:\n\t            output = \"QUEUE_FULL\"\n\t        return output\n", "    # Optional gradio functionalities\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Textbox()]\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Textbox()]\n\t    def block_input(self) -> List[\"gr.components.Component\"]:\n\t        try:\n\t            import gradio as gr\n\t            GRADIO_INSTALLED = True\n\t        except (ModuleNotFoundError, ImportError):\n", "            GRADIO_INSTALLED = False\n\t        if not GRADIO_INSTALLED:\n\t            raise ModuleNotFoundError(\"gradio must be installed to call block_input\")\n\t        else:\n\t            return self._block_input(gr)\n\t    def block_output(self) -> List[\"gr.components.Component\"]:\n\t        try:\n\t            import gradio as gr\n\t            GRADIO_INSTALLED = True\n\t        except (ModuleNotFoundError, ImportError):\n", "            GRADIO_INSTALLED = False\n\t        if not GRADIO_INSTALLED:\n\t            raise ModuleNotFoundError(\"gradio must be installed to call block_output\")\n\t        else:\n\t            return self._block_output(gr)\n\t    def block(self):\n\t        \"\"\"Get the gradio Blocks of this tool for visualization.\"\"\"\n\t        try:\n\t            import gradio as gr\n\t        except (ModuleNotFoundError, ImportError):\n", "            raise ModuleNotFoundError(\"gradio must be installed to call block\")\n\t        if not self._block:\n\t            self._block = gr.load(name=self.src, src=\"spaces\")\n\t        return self._block\n\t    # Optional langchain functionalities\n\t    @property\n\t    def langchain(self) -> \"langchain.agents.Tool\":  # type: ignore\n\t        if not LANGCHAIN_INSTALLED:\n\t            raise ModuleNotFoundError(\n\t                \"langchain must be installed to access langchain tool\"\n", "            )\n\t        return lc.agents.Tool(  # type: ignore\n\t            name=self.name, func=self.run, description=self.description\n\t        )\n\t    def __repr__(self) -> str:\n\t        return f\"GradioTool(name={self.name}, src={self.src})\"\n"]}
{"filename": "gradio_tools/tools/text_to_video.py", "chunked_list": ["from typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass TextToVideoTool(GradioTool):\n\t    def __init__(\n\t        self,\n\t        name=\"TextToVideo\",\n\t        description=(\n", "            \"A tool for creating videos from text.\"\n\t            \"Use this tool to create videos from text prompts. \"\n\t            \"Input will be a text prompt describing a video scene. \"\n\t            \"The output will be a path to a video file.\"\n\t        ),\n\t        src=\"damo-vilab/modelscope-text-to-video-synthesis\",\n\t        hf_token=None,\n\t        duplicate=False,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n", "    def create_job(self, query: str) -> Job:\n\t        return self.client.submit(query, -1, 16, 25, fn_index=1)\n\t    def postprocess(self, output: str) -> str:\n\t        return output\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Video()]\n"]}
{"filename": "gradio_tools/tools/__init__.py", "chunked_list": ["from gradio_tools.tools.bark import BarkTextToSpeechTool\n\tfrom gradio_tools.tools.clip_interrogator import ClipInterrogatorTool\n\tfrom gradio_tools.tools.document_qa import DocQueryDocumentAnsweringTool\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tfrom gradio_tools.tools.image_captioning import ImageCaptioningTool\n\tfrom gradio_tools.tools.image_to_music import ImageToMusicTool\n\tfrom gradio_tools.tools.prompt_generator import \\\n\t    StableDiffusionPromptGeneratorTool\n\tfrom gradio_tools.tools.sam_with_clip import SAMImageSegmentationTool\n\tfrom gradio_tools.tools.stable_diffusion import StableDiffusionTool\n", "from gradio_tools.tools.text_to_video import TextToVideoTool\n\tfrom gradio_tools.tools.whisper import WhisperAudioTranscriptionTool\n\t__all__ = [\n\t    \"GradioTool\",\n\t    \"StableDiffusionTool\",\n\t    \"ClipInterrogatorTool\",\n\t    \"ImageCaptioningTool\",\n\t    \"ImageToMusicTool\",\n\t    \"WhisperAudioTranscriptionTool\",\n\t    \"StableDiffusionPromptGeneratorTool\",\n", "    \"TextToVideoTool\",\n\t    \"DocQueryDocumentAnsweringTool\",\n\t    \"BarkTextToSpeechTool\",\n\t    \"SAMImageSegmentationTool\",\n\t]\n"]}
{"filename": "gradio_tools/tools/bark.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tSUPPORTED_LANGS = [\n\t    (\"English\", \"en\"),\n\t    (\"German\", \"de\"),\n\t    (\"Spanish\", \"es\"),\n", "    (\"French\", \"fr\"),\n\t    (\"Hindi\", \"hi\"),\n\t    (\"Italian\", \"it\"),\n\t    (\"Japanese\", \"ja\"),\n\t    (\"Korean\", \"ko\"),\n\t    (\"Polish\", \"pl\"),\n\t    (\"Portuguese\", \"pt\"),\n\t    (\"Russian\", \"ru\"),\n\t    (\"Turkish\", \"tr\"),\n\t    (\"Chinese\", \"zh\"),\n", "]\n\tSUPPORTED_LANGS = {lang: code for lang, code in SUPPORTED_LANGS}\n\tVOICES = [\"Unconditional\", \"Announcer\"]\n\tSUPPORTED_SPEAKERS = VOICES + [p for p in SUPPORTED_LANGS]\n\tNON_SPEECH_TOKENS = [\n\t    \"[laughter]\",\n\t    \"[laughs]\",\n\t    \"[sighs]\",\n\t    \"[music]\",\n\t    \"[gasps]\",\n", "    \"[clears throat]\",\n\t    \"'♪' for song lyrics. Put ♪ on either side of the the text\",\n\t    \"'…' for hesitations\",\n\t]\n\tclass BarkTextToSpeechTool(GradioTool):\n\t    \"\"\"Tool for calling bark text-to-speech llm.\"\"\"\n\t    def __init__(\n\t        self,\n\t        name=\"BarkTextToSpeech\",\n\t        description=(\n", "            \"A tool for text-to-speech. Use this tool to convert text \"\n\t            \"into sounds that sound like a human read it. Input will be a two strings separated by a |: \"\n\t            \"the first will be the text to read. The second will be the desired speaking language. \"\n\t            f\"It MUST be one of the following choices {','.join(SUPPORTED_SPEAKERS)}. \"\n\t            f\"Additionally, you can include the following non speech tokens: {NON_SPEECH_TOKENS}\"\n\t            \"The output will the text transcript of that file.\"\n\t        ),\n\t        src=\"suno/bark\",\n\t        hf_token=None,\n\t        duplicate=False,\n", "    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n\t        try:\n\t            text, speaker = (\n\t                query[: query.rindex(\"|\")],\n\t                query[(query.rindex(\"|\") + 1) :].strip(),\n\t            )\n\t        except ValueError:\n\t            text, speaker = query, \"Unconditional\"\n", "        if speaker in VOICES:\n\t            pass\n\t        elif speaker in SUPPORTED_LANGS:\n\t            speaker = f\"Speaker 0 ({SUPPORTED_LANGS[speaker]})\"\n\t        else:\n\t            speaker = \"Unconditional\"\n\t        return self.client.submit(text, speaker, fn_index=3)\n\t    def postprocess(self, output: str) -> str:\n\t        return output\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n", "        return [gr.Textbox()]\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Audio()]\n"]}
{"filename": "gradio_tools/tools/clip_interrogator.py", "chunked_list": ["from typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass ClipInterrogatorTool(GradioTool):\n\t    def __init__(\n\t        self,\n\t        name=\"ClipInterrogator\",\n\t        description=(\n", "            \"A tool for reverse engineering a prompt from a source image. \"\n\t            \"Use this tool to create a prompt for StableDiffusion that matches the \"\n\t            \"input image. The imput is a path to an image. The output is a text string.\"\n\t        ),\n\t        src=\"pharma/CLIP-Interrogator\",\n\t        hf_token=None,\n\t        duplicate=True,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n", "        return self.client.submit(\n\t            query, \"ViT-L (best for Stable Diffusion 1.*)\", \"best\", fn_index=3\n\t        )\n\t    def postprocess(self, output: str) -> str:\n\t        return output\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Image()]\n"]}
{"filename": "gradio_tools/tools/stable_diffusion.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING, List\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass StableDiffusionTool(GradioTool):\n\t    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\t    def __init__(\n\t        self,\n", "        name=\"StableDiffusion\",\n\t        description=(\n\t            \"An image generator. Use this to generate images based on \"\n\t            \"text input. Input should be a description of what the image should \"\n\t            \"look like. The output will be a path to an image file.\"\n\t        ),\n\t        src=\"gradio-client-demos/text-to-image\",\n\t        hf_token=None,\n\t        duplicate=False,\n\t    ) -> None:\n", "        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n\t        return self.client.submit(query, api_name=\"/predict\")\n\t    def postprocess(self, output: str) -> str:\n\t        return output\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Textbox()]\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Image()]\n"]}
{"filename": "gradio_tools/tools/image_to_music.py", "chunked_list": ["from typing import TYPE_CHECKING, Any, List, Tuple, Union\n\tfrom gradio_client.client import Job\n\tfrom gradio_tools.tools.gradio_tool import GradioTool\n\tif TYPE_CHECKING:\n\t    import gradio as gr\n\tclass ImageToMusicTool(GradioTool):\n\t    def __init__(\n\t        self,\n\t        name=\"ImagetoMusic\",\n\t        description=(\n", "            \"A tool for creating music from images. Use this tool to create a musical \"\n\t            \"track from an image. Input will be a path to an image file. \"\n\t            \"The output will be an audio file generated from that image.\"\n\t        ),\n\t        src=\"fffiloni/img-to-music\",\n\t        hf_token=None,\n\t        duplicate=False,\n\t    ) -> None:\n\t        super().__init__(name, description, src, hf_token, duplicate)\n\t    def create_job(self, query: str) -> Job:\n", "        return self.client.submit(\n\t            query.strip(\"'\"), 15, \"medium\", \"loop\", None, fn_index=0\n\t        )\n\t    def postprocess(self, output: Union[Tuple[Any], Any]) -> str:\n\t        return output[1]  # type: ignore\n\t    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Image()]\n\t    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n\t        return [gr.Audio()]\n"]}
{"filename": "tests/test_tools.py", "chunked_list": ["import pytest\n\tfrom unittest.mock import patch\n\timport gradio_tools\n\tfrom gradio_tools import GradioTool\n\ttry:\n\t    import gradio as gr\n\t    GRADIO_INSTALLED = True\n\texcept:\n\t    GRADIO_INSTALLED = False\n\t@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\n", "@patch(\"gradio_client.Client.duplicate\")\n\tdef test_duplicate(mock_duplicate, tool_class):\n\t    tool_class(duplicate=True, hf_token=\"dafsdf\")\n\t    mock_duplicate.assert_called_once()\n\t@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\n\t@patch(\"gradio_client.Client.duplicate\")\n\tdef test_dont_duplicate(mock_duplicate, tool_class):\n\t    tool_class(duplicate=False)\n\t    mock_duplicate.assert_not_called()\n\t@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\n", "def test_all_listed_in_init(tool_class):\n\t    assert tool_class.__name__ in gradio_tools.__all__\n\t@pytest.mark.skipif(not GRADIO_INSTALLED, reason=\"Gradio not installed\")\n\t@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\n\tdef test_input_output(tool_class):\n\t    if tool_class.__name__ == \"BarkTextToSpeechTool\":\n\t        inp = [gr.Textbox()]\n\t        output = [gr.Audio()]\n\t    elif tool_class.__name__ == \"ClipInterrogatorTool\":\n\t        inp = [gr.Image()]\n", "        output = [gr.Textbox()]\n\t    elif tool_class.__name__ == \"DocQueryDocumentAnsweringTool\":\n\t        inp = [gr.Image(), gr.Textbox()]\n\t        output = [gr.Textbox()]\n\t    elif tool_class.__name__ == \"ImageCaptioningTool\":\n\t        inp = [gr.Image()]\n\t        output = [gr.Textbox()]\n\t    elif tool_class.__name__ == \"ImageToMusicTool\":\n\t        inp = [gr.Image()]\n\t        output = [gr.Audio()]\n", "    elif tool_class.__name__ == \"StableDiffusionPromptGeneratorTool\":\n\t        inp = [gr.Textbox()]\n\t        output = [gr.Textbox()]\n\t    elif tool_class.__name__ == \"SAMImageSegmentationTool\":\n\t        inp = [gr.Number(), gr.Number(), gr.Number(), gr.Image(), gr.Textbox()]\n\t        output = [gr.Image()]\n\t    elif tool_class.__name__ == \"StableDiffusionTool\":\n\t        inp = [gr.Textbox()]\n\t        output = [gr.Image()]\n\t    elif tool_class.__name__ == \"TextToVideoTool\":\n", "        inp = [gr.Textbox()]\n\t        output = [gr.Video()]\n\t    elif tool_class.__name__ == \"WhisperAudioTranscriptionTool\":\n\t        inp = [gr.Audio()]\n\t        output = [gr.Textbox()]\n\t    else:\n\t        raise ValueError(f\"Test does not have a case for: {tool_class.__name__}\")\n\t    tool = tool_class()\n\t    assert [t.__class__ for t in tool.block_input()] == [t.__class__ for t in inp]\n\t    assert [t.__class__ for t in tool.block_output()] == [t.__class__ for t in output]\n"]}
{"filename": "tests/test_sam.py", "chunked_list": ["import pytest\n\tfrom unittest.mock import patch\n\tfrom gradio_tools import SAMImageSegmentationTool\n\t@patch(\"gradio_client.Client.submit\")\n\tdef test_input_parsing(mock_submit):\n\t    tool = SAMImageSegmentationTool()\n\t    tool.create_job(\"my_image.png| a red horse|0.9|0.8|0.9\")\n\t    mock_submit.assert_called_with(0.9,0.8,0.9,\"my_image.png\", \"a red horse\", api_name=\"/predict\")\n\t@patch(\"gradio_client.Client.submit\")\n\tdef test_raise_error(mock_submit):\n", "    tool = SAMImageSegmentationTool()\n\t    with pytest.raises(ValueError,\n\t                       match=\"Not enough arguments passed to the SAMImageSegmentationTool!\"):\n\t        tool.create_job(\"my_image.png| a red horse|\")"]}
{"filename": "examples/langchain/document_qa.py", "chunked_list": ["import os\n\timport pathlib\n\tif not os.getenv(\"OPENAI_API_KEY\"):\n\t    raise ValueError(\"OPENAI_API_KEY must be set\")\n\tfrom langchain.agents import initialize_agent, load_tools\n\tfrom langchain.llms import OpenAI\n\tfrom gradio_tools import DocQueryDocumentAnsweringTool\n\tfrom langchain.memory import ConversationBufferMemory\n\tllm = OpenAI(temperature=0)\n\tmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n", "tools = [DocQueryDocumentAnsweringTool().langchain]\n\tIMG_PATH = pathlib.Path(__file__).parent / \"florida-drivers-license.jpeg\"\n\tagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\n\toutput = agent.run(input=f\"What is the date of birth the driver in {IMG_PATH}?\")\n\toutput = agent.run(input=f\"What is the current date?\")\n\toutput = agent.run(input=f\"Using the current date, what is the age of the driver? Explain your reasoning.\")\n\toutput = agent.run(input=f\"What is the driver's license number?\")"]}
{"filename": "examples/langchain/example.py", "chunked_list": ["import os\n\tif not os.getenv(\"OPENAI_API_KEY\"):\n\t    raise ValueError(\"OPENAI_API_KEY must be set\")\n\tfrom langchain.agents import initialize_agent\n\tfrom langchain.llms import OpenAI\n\tfrom gradio_tools.tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,\n\t                                TextToVideoTool)\n\tfrom langchain.memory import ConversationBufferMemory\n\tllm = OpenAI(temperature=0)\n\tmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n", "tools = [StableDiffusionTool().langchain, ImageCaptioningTool().langchain,\n\t         StableDiffusionPromptGeneratorTool().langchain, TextToVideoTool().langchain]\n\tagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\n\toutput = agent.run(input=(\"Please create a photo of a dog riding a skateboard \"\n\t                          \"but improve my prompt prior to using an image generator.\"\n\t                          \"Please caption the generated image and create a video for it using the improved prompt.\"))\n"]}
{"filename": "examples/langchain/bark_example.py", "chunked_list": ["import os\n\tif not os.getenv(\"OPENAI_API_KEY\"):\n\t    raise ValueError(\"OPENAI_API_KEY must be set\")\n\tfrom langchain.agents import initialize_agent\n\tfrom langchain.llms import OpenAI\n\tfrom gradio_tools.tools import BarkTextToSpeechTool, StableDiffusionTool, StableDiffusionPromptGeneratorTool\n\tfrom langchain.memory import ConversationBufferMemory\n\tllm = OpenAI(temperature=0)\n\tmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n\ttools = [BarkTextToSpeechTool().langchain,\n", "         StableDiffusionTool().langchain,\n\t         StableDiffusionPromptGeneratorTool().langchain]\n\tagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\n\toutput = agent.run(input=(\"Please create a jingle for a spanish company called 'Chipi Chups' that makes lollipops. \"\n\t                          \"The jingle should be catchy and playful and meant to appeal to all ages.\"))\n\tprint(output)\n\toutput = agent.run(input=(\"Now create a logo for this company. Please improve\"))\n\tprint(output)\n"]}
{"filename": "examples/langchain/sam_example.py", "chunked_list": ["import os\n\timport pathlib\n\tif not os.getenv(\"OPENAI_API_KEY\"):\n\t    raise ValueError(\"OPENAI_API_KEY must be set\")\n\tfrom langchain.agents import initialize_agent\n\tfrom langchain.llms import OpenAI\n\tfrom gradio_tools.tools import SAMImageSegmentationTool\n\tfrom langchain.memory import ConversationBufferMemory\n\tllm = OpenAI(temperature=0)\n\tmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n", "tools = [SAMImageSegmentationTool().langchain]\n\twaldo_1 = pathlib.Path(__file__).parent / \"waldo.jpeg\"\n\twaldo_2 = pathlib.Path(__file__).parent / \"waldo_3.webp\"\n\tagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\n\toutput = agent.run(input=(f\"Please find Waldo in this image: {waldo_1}. \"\n\t                          \"Waldo is a man with glasses wearing sweater with red and white stripes\"))\n\tprint(output)\n\toutput = agent.run(input=(f\"Great job! Now find Waldo in this image: {waldo_2}.\"))\n\tprint(output)\n"]}
{"filename": "examples/minichain/agent.py", "chunked_list": ["from minichain import Id, prompt, OpenAI, show, transform, Mock, Break\n\tfrom gradio_tools.tools import StableDiffusionTool, ImageCaptioningTool, ImageToMusicTool\n\ttools = [StableDiffusionTool(), ImageCaptioningTool(), ImageToMusicTool()]\n\t@prompt(OpenAI(stop=[\"Observation:\"]),\n\t        template_file=\"agent.pmpt.tpl\")\n\tdef agent(model, query, history):\n\t    return model(dict(tools=[(str(tool.__class__.__name__), tool.description)\n\t                             for tool in tools],\n\t                      input=query,\n\t                      agent_scratchpad=history\n", "                      ))\n\t@transform()\n\tdef tool_parse(out):\n\t    lines = out.split(\"\\n\")\n\t    if lines[0].split(\"?\")[-1].strip() == \"Yes\":\n\t        tool = lines[1].split(\":\", 1)[-1].strip()\n\t        command = lines[2].split(\":\", 1)[-1].strip()\n\t        return tool, command\n\t    else:\n\t        return Break()\n", "@prompt(tools)\n\tdef tool_use(model, usage):\n\t    selector, command = usage\n\t    for i, tool in enumerate(tools):\n\t        if selector == tool.__class__.__name__:\n\t            return model(command, tool_num=i)\n\t    return (\"\",)\n\t@transform()\n\tdef append(history, new, observation):\n\t    return history + \"\\n\" + new + \"Observation: \" + observation\n", "def run(query):\n\t    history = \"\"\n\t    observations = []\n\t    for i in range(3):\n\t        select_input = agent(query, history)\n\t        observations.append(tool_use(tool_parse(select_input)))\n\t        history = append(history, select_input, observations[i])\n\t    return observations[-1]\n\tdesc = \"\"\"\n\t### Agent\n", "Chain that executes different tools based on model decisions. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/MiniChain/blob/master/examples/bash.ipynb)\n\t(Adapted from LangChain )\n\t\"\"\"\n\tgradio = show(run,\n\t              subprompts=[agent, tool_use] * 3,\n\t              examples=[\n\t                  \"I would please like a photo of a dog riding a skateboard. \"\n\t                  \"Please caption this image and create a song for it.\",\n\t                  'Use an image generator tool to draw a cat.',\n\t                  'Caption the image https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png from the internet'],\n", "              out_type=\"markdown\",\n\t              description=desc,\n\t              show_advanced=False\n\t              )\n\tif __name__ == \"__main__\":\n\t    gradio.queue().launch()\n"]}
{"filename": "examples/minichain/example.py", "chunked_list": ["from minichain import show, prompt, OpenAI, GradioConf\n\timport gradio as gr\n\tfrom gradio_tools.tools import StableDiffusionTool, ImageCaptioningTool\n\t@prompt(OpenAI())\n\tdef picture(model, query):\n\t    return model(query)\n\t@prompt(StableDiffusionTool(),\n\t        gradio_conf=GradioConf(\n\t            block_output= lambda: gr.Image(),\n\t            block_input= lambda: gr.Textbox(show_label=False)))\n", "def gen(model, query):\n\t    return model(query)\n\t@prompt(ImageCaptioningTool(),\n\t        gradio_conf=GradioConf(\n\t            block_input= lambda: gr.Image(),\n\t            block_output=lambda: gr.Textbox(show_label=False)))\n\tdef caption(model, img_src):\n\t    return model(img_src)\n\tdef gradio_example(query):\n\t    return caption(gen(picture(query)))\n", "desc = \"\"\"\n\t### Gradio Tool\n\tExamples using the gradio tool [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/MiniChain/blob/master/examples/gradio_example.ipynb)\n\t\"\"\"\n\tgradio = show(gradio_example,\n\t              subprompts=[picture, gen, caption],\n\t              examples=['Describe a one-sentence fantasy scene.',\n\t                        'Describe a one-sentence scene happening on the moon.'],\n\t              out_type=\"markdown\",\n\t              description=desc,\n", "              show_advanced=False\n\t              )\n\tif __name__ == \"__main__\":\n\t    gradio.queue().launch()\n"]}
