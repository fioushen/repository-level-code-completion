{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\tfrom setuptools import find_packages, setup\n\tif __name__ == '__main__':\n\t    setup(\n\t        name='cma',\n\t        version='1.0',\n\t        description='',\n\t        author='brdavid',\n\t        author_email='',\n\t        url='',\n", "        install_requires=['pytorch-lightning'],\n\t        packages=find_packages(),\n\t    )\n"]}
{"filename": "tools/run.py", "chunked_list": ["import sys\n\timport pytorch_lightning as pl\n\tfrom helpers.pseudo_labels import generate_pseudo_labels\n\tfrom pytorch_lightning.cli import LightningCLI\n\tclass MyLightningCLI(LightningCLI):\n\t    def add_arguments_to_parser(self, parser):\n\t        parser.add_optimizer_args(\n\t            nested_key=\"optimizer\", link_to=\"model.init_args.optimizer_init\")\n\t        parser.add_lr_scheduler_args(\n\t            nested_key=\"lr_scheduler\", link_to=\"model.init_args.lr_scheduler_init\")\n", "def cli_main():\n\t    if sys.argv[1] == \"generate_pl\":\n\t        del sys.argv[1]\n\t        sys.argv.append('--data.init_args.generate_pseudo_labels')\n\t        sys.argv.append('True')\n\t        cli = MyLightningCLI(pl.LightningModule,\n\t                             pl.LightningDataModule,\n\t                             subclass_mode_model=True,\n\t                             subclass_mode_data=True,\n\t                             save_config_kwargs={'overwrite': True},\n", "                             seed_everything_default=2770466080,\n\t                             run=False)\n\t        generate_pseudo_labels(cli.model, cli.datamodule)\n\t    else:\n\t        cli = MyLightningCLI(pl.LightningModule,\n\t                             pl.LightningDataModule,\n\t                             subclass_mode_model=True,\n\t                             subclass_mode_data=True,\n\t                             save_config_kwargs={'overwrite': True},\n\t                             seed_everything_default=2770466080)\n", "if __name__ == '__main__':\n\t    cli_main()\n"]}
{"filename": "data_modules/transforms.py", "chunked_list": ["import random\n\timport torch\n\timport torch.nn as nn\n\timport torchvision\n\tIMNET_MEAN = (0.485, 0.456, 0.406)\n\tIMNET_STD = (0.229, 0.224, 0.225)\n\tclass ToTensor:\n\t    def __init__(self, apply_keys='all'):\n\t        self.apply_keys = apply_keys\n\t    def __call__(self, sample):\n", "        if self.apply_keys == 'all':\n\t            apply_keys = list(sample)\n\t        elif self.apply_keys == 'none':\n\t            apply_keys = []\n\t        else:\n\t            apply_keys = self.apply_keys\n\t        for key in apply_keys:\n\t            val = sample[key]\n\t            if key in ['image', 'image_ref']:\n\t                sample[key] = torchvision.transforms.functional.pil_to_tensor(\n", "                    val)\n\t            elif key in ['semantic']:\n\t                sample[key] = torchvision.transforms.functional.pil_to_tensor(\n\t                    val).squeeze(0)\n\t            elif key in ['filename']:\n\t                pass\n\t            else:\n\t                raise ValueError\n\t        return sample\n\tclass RandomCrop(nn.Module):\n", "    def __init__(self, apply_keys='all', size=None, ignore_index=255, cat_max_ratio=1.0):\n\t        super().__init__()\n\t        self.apply_keys = apply_keys\n\t        self.size = size\n\t        self.ignore_index = ignore_index\n\t        self.cat_max_ratio = cat_max_ratio\n\t    def forward(self, sample):\n\t        if self.apply_keys == 'all':\n\t            apply_keys = list(sample)\n\t        else:\n", "            apply_keys = self.apply_keys\n\t        for k in ['image', 'image_ref', 'semantic']:\n\t            if k in sample.keys():\n\t                h, w = sample[k].shape[-2:]\n\t        crop_params = self.get_params([h, w], self.size)\n\t        if self.cat_max_ratio < 1.:\n\t            # Repeat 10 times\n\t            for _ in range(10):\n\t                seg_tmp = self.crop(sample['semantic'], *crop_params)\n\t                labels, cnt = torch.unique(seg_tmp, return_counts=True)\n", "                cnt = cnt[labels != self.ignore_index]\n\t                if len(cnt) > 1 and cnt.max() / torch.sum(cnt).float() < self.cat_max_ratio:\n\t                    break\n\t                crop_params = self.get_params([h, w], self.size)\n\t        for key in apply_keys:\n\t            val = sample[key]\n\t            if key in ['image',\n\t                       'image_ref',\n\t                       'semantic']:\n\t                sample[key] = self.crop(val, *crop_params)\n", "            elif key in ['filename']:\n\t                pass\n\t            else:\n\t                raise ValueError('unknown key: {}'.format(key))\n\t        return sample\n\t    @staticmethod\n\t    def get_params(img_size, output_size):\n\t        \"\"\"Get parameters for ``crop`` for a random crop.\n\t        Args:\n\t            img (PIL Image or Tensor): Image to be cropped.\n", "            output_size (tuple): Expected output size of the crop.\n\t        Returns:\n\t            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n\t        \"\"\"\n\t        h, w = img_size\n\t        th, tw = output_size\n\t        if w == tw and h == th:\n\t            return 0, 0, h, w\n\t        i = random.randint(0, max(h - th, 0))\n\t        j = random.randint(0, max(w - tw, 0))\n", "        return i, j, min(th, h), min(tw, w)\n\t    def crop(self, img, top, left, height, width):\n\t        h, w = img.shape[-2:]\n\t        right = left + width\n\t        bottom = top + height\n\t        if left < 0 or top < 0 or right > w or bottom > h:\n\t            raise ValueError(\"Invalid crop parameters: {}, img size: {}\".format(\n\t                (top, left, height, width), (h, w)))\n\t        return img[..., top:bottom, left:right]\n\tclass RandomHorizontalFlip(nn.Module):\n", "    def __init__(self, apply_keys='all', p=0.5):\n\t        super().__init__()\n\t        self.apply_keys = apply_keys\n\t        self.p = p\n\t    def forward(self, sample):\n\t        if self.apply_keys == 'all':\n\t            apply_keys = list(sample)\n\t        else:\n\t            apply_keys = self.apply_keys\n\t        if random.random() < self.p:\n", "            for key in apply_keys:\n\t                val = sample[key]\n\t                if key in ['image',\n\t                           'image_ref',\n\t                           'semantic']:\n\t                    sample[key] = torchvision.transforms.functional.hflip(val)\n\t                elif key in ['filename']:\n\t                    pass\n\t                else:\n\t                    raise ValueError\n", "        return sample\n\tclass ConvertImageDtype(torchvision.transforms.ConvertImageDtype):\n\t    def __init__(self, apply_keys='all', **kwargs):\n\t        dtype = kwargs.pop('dtype', torch.float)\n\t        super().__init__(dtype=dtype, **kwargs)\n\t        self.apply_keys = apply_keys\n\t    def forward(self, sample):\n\t        if self.apply_keys == 'all':\n\t            apply_keys = list(sample)\n\t        else:\n", "            apply_keys = self.apply_keys\n\t        for key in apply_keys:\n\t            val = sample[key]\n\t            if key in ['image', 'image_ref']:\n\t                sample[key] = super().forward(val)\n\t            elif key in ['semantic']:\n\t                sample[key] = val.to(torch.long)  # from byte to long\n\t            elif key in ['filename']:\n\t                pass\n\t            else:\n", "                raise ValueError\n\t        return sample\n\tclass Normalize(torchvision.transforms.Normalize):\n\t    def __init__(self, apply_keys='all', **kwargs):\n\t        # set imagenet statistics as default\n\t        mean = kwargs.pop('mean', IMNET_MEAN)\n\t        std = kwargs.pop('std', IMNET_STD)\n\t        super().__init__(mean=mean, std=std, **kwargs)\n\t        self.apply_keys = apply_keys\n\t    def forward(self, sample):\n", "        if self.apply_keys == 'all':\n\t            apply_keys = list(sample)\n\t        else:\n\t            apply_keys = self.apply_keys\n\t        for key in apply_keys:\n\t            val = sample[key]\n\t            if key in ['image', 'image_ref']:\n\t                sample[key] = super().forward(val)\n\t            elif key in ['semantic', 'filename']:\n\t                pass\n", "            else:\n\t                raise ValueError\n\t        return sample\n"]}
{"filename": "data_modules/combined_data_module.py", "chunked_list": ["import os\n\tfrom itertools import chain\n\tfrom operator import itemgetter\n\tfrom typing import Optional\n\timport torch\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom pytorch_lightning.cli import instantiate_class\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision.transforms import Compose\n\tfrom . import transforms as transform_lib\n", "from .datasets import *\n\tDATA_DIR = os.environ['DATA_DIR']\n\tclass CombinedDataModule(LightningDataModule):\n\t    def __init__(\n\t        self,\n\t        load_config: dict,\n\t        num_workers: int = 0,\n\t        batch_size: int = 8,\n\t        batch_size_divisor: int = 1,\n\t        generate_pseudo_labels: bool = False,\n", "        pin_memory: bool = True,\n\t    ) -> None:\n\t        super().__init__()\n\t        self.data_dirs = {\n\t            'ACDC': os.path.join(DATA_DIR, 'ACDC'),\n\t            'DarkZurich': os.path.join(DATA_DIR, 'DarkZurich'),\n\t            'RobotCar': os.path.join(DATA_DIR, 'RobotCar'),\n\t            'ACG': DATA_DIR,\n\t        }\n\t        self.num_workers = num_workers\n", "        assert batch_size % batch_size_divisor == 0\n\t        self.batch_size_divisor = batch_size_divisor\n\t        self.batch_size = batch_size // batch_size_divisor\n\t        self.pin_memory = pin_memory\n\t        self.generate_pseudo_labels = generate_pseudo_labels\n\t        self.train_on = []\n\t        self.train_config = []\n\t        self.val_on = []\n\t        self.val_config = []\n\t        self.test_on = []\n", "        self.test_config = []\n\t        self.predict_on = []\n\t        self.predict_config = []\n\t        # parse load_config\n\t        if 'train' in load_config:\n\t            for ds, conf in load_config['train'].items():\n\t                if isinstance(conf, dict):\n\t                    self.train_on.append(ds)\n\t                    self.train_config.append(conf)\n\t                elif isinstance(conf, list):\n", "                    for el in conf:\n\t                        self.train_on.append(ds)\n\t                        self.train_config.append(el)\n\t        if 'val' in load_config:\n\t            for ds, conf in load_config['val'].items():\n\t                if isinstance(conf, dict):\n\t                    self.val_on.append(ds)\n\t                    self.val_config.append(conf)\n\t                elif isinstance(conf, list):\n\t                    for el in conf:\n", "                        self.val_on.append(ds)\n\t                        self.val_config.append(el)\n\t        if 'test' in load_config:\n\t            for ds, conf in load_config['test'].items():\n\t                if isinstance(conf, dict):\n\t                    self.test_on.append(ds)\n\t                    self.test_config.append(conf)\n\t                elif isinstance(conf, list):\n\t                    for el in conf:\n\t                        self.test_on.append(ds)\n", "                        self.test_config.append(el)\n\t        if 'predict' in load_config:\n\t            for ds, conf in load_config['predict'].items():\n\t                if isinstance(conf, dict):\n\t                    if self.generate_pseudo_labels:\n\t                        conf['predict_on'] = 'train'\n\t                    self.predict_on.append(ds)\n\t                    self.predict_config.append(conf)\n\t                elif isinstance(conf, list):\n\t                    for el in conf:\n", "                        if self.generate_pseudo_labels:\n\t                            el['predict_on'] = 'train'\n\t                        self.predict_on.append(ds)\n\t                        self.predict_config.append(el)\n\t        self.idx_to_name = {'train': {}, 'val': {}, 'test': {}, 'predict': {}}\n\t        for idx, ds in enumerate(self.train_on):\n\t            self.idx_to_name['train'][idx] = ds\n\t        for idx, ds in enumerate(self.val_on):\n\t            self.idx_to_name['val'][idx] = ds\n\t        for idx, ds in enumerate(self.test_on):\n", "            self.idx_to_name['test'][idx] = ds\n\t        for idx, ds in enumerate(self.predict_on):\n\t            self.idx_to_name['predict'][idx] = ds\n\t        if len(self.train_on) > 0:\n\t            assert self.batch_size % len(\n\t                self.train_on) == 0, 'batch size should be divisible by number of train datasets'\n\t        # handle transformations\n\t        for idx, (ds, cfg) in enumerate(zip(self.train_on, self.train_config)):\n\t            trafos = cfg.pop('transforms', None)\n\t            if trafos:\n", "                self.train_config[idx]['transforms'] = Compose(\n\t                    [instantiate_class(tuple(), t) for t in trafos])\n\t            else:\n\t                self.train_config[idx]['transforms'] = transform_lib.ToTensor()\n\t        for idx, (ds, cfg) in enumerate(zip(self.val_on, self.val_config)):\n\t            trafos = cfg.pop('transforms', None)\n\t            if trafos:\n\t                self.val_config[idx]['transforms'] = Compose(\n\t                    [instantiate_class(tuple(), t) for t in trafos])\n\t            else:\n", "                self.val_config[idx]['transforms'] = transform_lib.ToTensor()\n\t        for idx, (ds, cfg) in enumerate(zip(self.test_on, self.test_config)):\n\t            trafos = cfg.pop('transforms', None)\n\t            if trafos:\n\t                self.test_config[idx]['transforms'] = Compose(\n\t                    [instantiate_class(tuple(), t) for t in trafos])\n\t            else:\n\t                self.test_config[idx]['transforms'] = transform_lib.ToTensor()\n\t        for idx, (ds, cfg) in enumerate(zip(self.predict_on, self.predict_config)):\n\t            trafos = cfg.pop('transforms', None)\n", "            if trafos:\n\t                self.predict_config[idx]['transforms'] = Compose(\n\t                    [instantiate_class(tuple(), t) for t in trafos])\n\t            else:\n\t                self.predict_config[idx]['transforms'] = transform_lib.ToTensor(\n\t                )\n\t        self.val_batch_size = 1\n\t        self.test_batch_size = 1\n\t    def setup(self, stage: Optional[str] = None):\n\t        if stage in (None, \"fit\"):\n", "            self.train_ds = []\n\t            for ds, cfg in zip(self.train_on, self.train_config):\n\t                self.train_ds.append(globals()[ds](\n\t                    self.data_dirs[ds],\n\t                    stage=\"train\",\n\t                    **cfg\n\t                ))\n\t        if stage in (None, \"fit\", \"validate\"):\n\t            self.val_ds = []\n\t            for ds, cfg in zip(self.val_on, self.val_config):\n", "                self.val_ds.append(globals()[ds](\n\t                    self.data_dirs[ds],\n\t                    stage=\"val\",\n\t                    **cfg\n\t                ))\n\t        if stage in (None, \"test\"):\n\t            self.test_ds = []\n\t            for ds, cfg in zip(self.test_on, self.test_config):\n\t                self.test_ds.append(globals()[ds](\n\t                    self.data_dirs[ds],\n", "                    stage=\"test\",\n\t                    **cfg\n\t                ))\n\t        if stage in (None, \"predict\"):\n\t            self.predict_ds = []\n\t            for ds, cfg in zip(self.predict_on, self.predict_config):\n\t                self.predict_ds.append(globals()[ds](\n\t                    self.data_dirs[ds],\n\t                    stage=\"predict\",\n\t                    **cfg\n", "                ))\n\t    def train_dataloader(self):\n\t        loader_list = []\n\t        for ds in self.train_ds:\n\t            loader = DataLoader(\n\t                dataset=ds,\n\t                batch_size=self.batch_size // len(self.train_on),\n\t                shuffle=True,\n\t                num_workers=self.num_workers,\n\t                drop_last=True,\n", "                pin_memory=self.pin_memory,\n\t            )\n\t            loader_list.append(loader)\n\t        return loader_list\n\t    def val_dataloader(self):\n\t        loader_list = []\n\t        for ds in self.val_ds:\n\t            loader = DataLoader(\n\t                dataset=ds,\n\t                batch_size=self.val_batch_size,\n", "                shuffle=False,\n\t                num_workers=self.num_workers,\n\t                pin_memory=self.pin_memory,\n\t                drop_last=False,\n\t            )\n\t            loader_list.append(loader)\n\t        return loader_list\n\t    def test_dataloader(self):\n\t        loader_list = []\n\t        for ds in self.test_ds:\n", "            loader = DataLoader(\n\t                dataset=ds,\n\t                batch_size=self.test_batch_size,\n\t                shuffle=False,\n\t                num_workers=self.num_workers,\n\t                pin_memory=self.pin_memory,\n\t                drop_last=False,\n\t            )\n\t            loader_list.append(loader)\n\t        return loader_list\n", "    def predict_dataloader(self, shuffle=False):\n\t        loader_list = []\n\t        for ds in self.predict_ds:\n\t            loader = DataLoader(\n\t                dataset=ds,\n\t                batch_size=self.test_batch_size,\n\t                shuffle=shuffle,\n\t                num_workers=self.num_workers,\n\t                pin_memory=self.pin_memory,\n\t                drop_last=False,\n", "            )\n\t            loader_list.append(loader)\n\t        return loader_list\n\t    def on_before_batch_transfer(self, batch, dataloader_idx):\n\t        if self.trainer.training:\n\t            tmp_batch = {k: list(map(itemgetter(k), batch)) for k in set(chain.from_iterable(batch))}\n\t            return {k: torch.cat(v, dim=0) if k != 'filename' else [\n\t                item for sublist in v for item in sublist] for k, v in tmp_batch.items()}\n\t        else:\n\t            return batch\n"]}
{"filename": "data_modules/__init__.py", "chunked_list": ["from .combined_data_module import CombinedDataModule\n"]}
{"filename": "data_modules/datasets/darkzurich.py", "chunked_list": ["import io\n\timport os\n\timport zipfile\n\tfrom typing import Any, Callable, List, Optional, Union\n\timport requests\n\timport torch\n\tfrom PIL import Image\n\tURLS = {\n\t    \"pseudo_labels_train_DarkZurich_cma_segformer\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_DarkZurich_cma_segformer.zip\",\n\t}\n", "class DarkZurich(torch.utils.data.Dataset):\n\t    orig_dims = (1080, 1920)\n\t    def __init__(\n\t            self,\n\t            root: str,\n\t            stage: str = \"train\",\n\t            load_keys: Union[List[str], str] = [\"image_ref\", \"image\"],\n\t            transforms: Optional[Callable] = None,\n\t            predict_on: str = \"test\",\n\t            load_pseudo_labels: bool = False,\n", "            pseudo_label_dir: Optional[str] = None,\n\t            **kwargs\n\t    ) -> None:\n\t        super().__init__()\n\t        self.root = root\n\t        self.transforms = transforms\n\t        self.load_pseudo_labels = load_pseudo_labels\n\t        if self.load_pseudo_labels:\n\t            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n\t            os.makedirs(_dpath, exist_ok=True)\n", "            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n\t                print('Downloading and extracting pseudo-labels...')\n\t                r = requests.get(URLS[pseudo_label_dir])\n\t                z = zipfile.ZipFile(io.BytesIO(r.content))\n\t                z.extractall(_dpath)\n\t            self.pseudo_label_dir = os.path.join(\n\t                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\t        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n\t        self.stage = stage\n\t        # mapping from stage to splits\n", "        if self.stage == 'train':\n\t            self.split = 'train'\n\t        elif self.stage == 'val':\n\t            self.split = 'val'\n\t        elif self.stage == 'test':\n\t            self.split = 'val'  # test on val split\n\t        elif self.stage == 'predict':\n\t            self.split = predict_on\n\t        if isinstance(load_keys, str):\n\t            self.load_keys = [load_keys]\n", "        else:\n\t            self.load_keys = load_keys\n\t        self.paths = {k: []\n\t                      for k in ['image', 'image_ref', 'semantic']}\n\t        self.images_dir = os.path.join(self.root, 'rgb_anon')\n\t        self.semantic_dir = os.path.join(self.root, 'gt')\n\t        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.semantic_dir):\n\t            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n\t                               ' specified \"split\" and \"condition\" are inside the \"root\" directory')\n\t        if self.split == 'train':\n", "            img_ids = [i_id.strip() for i_id in open(os.path.join(\n\t                os.path.dirname(__file__), 'lists/zurich_dn_pair_train.csv'))]\n\t            for pair in img_ids:\n\t                night, day = pair.split(\",\")\n\t                for k in ['image', 'image_ref']:\n\t                    if k == 'image':\n\t                        file_path = os.path.join(\n\t                            self.root, 'rgb_anon', night + \"_rgb_anon.png\")\n\t                    elif k == 'image_ref':\n\t                        file_path = os.path.join(\n", "                            self.root, 'rgb_anon', day + \"_rgb_anon.png\")\n\t                    self.paths[k].append(file_path)\n\t        else:\n\t            img_parent_dir = os.path.join(self.images_dir, self.split, 'night')\n\t            semantic_parent_dir = os.path.join(\n\t                self.semantic_dir, self.split, 'night')\n\t            for recording in os.listdir(img_parent_dir):\n\t                img_dir = os.path.join(img_parent_dir, recording)\n\t                semantic_dir = os.path.join(semantic_parent_dir, recording)\n\t                for file_name in os.listdir(img_dir):\n", "                    for k in ['image', 'image_ref', 'semantic']:\n\t                        if k == 'image':\n\t                            file_path = os.path.join(img_dir, file_name)\n\t                        elif k == 'image_ref':\n\t                            if self.split == 'val':\n\t                                ref_img_dir = img_dir.replace(self.split, self.split + '_ref').replace(\n\t                                    'night', 'day').replace(recording, recording + '_ref')\n\t                                ref_file_name = file_name.replace(\n\t                                    'rgb_anon.png', 'ref_rgb_anon.png')\n\t                                file_path = os.path.join(\n", "                                    ref_img_dir, ref_file_name)\n\t                            elif self.split == 'test':\n\t                                ref_img_dir = img_dir.replace(self.split, self.split + '_ref').replace(\n\t                                    'night', 'day').replace(recording, recording + '_ref')\n\t                                ref_file_name_start = file_name.split('rgb_anon.png')[\n\t                                    0]\n\t                                for f in os.listdir(ref_img_dir):\n\t                                    if f.startswith(ref_file_name_start):\n\t                                        ref_file_name = f\n\t                                        break\n", "                                file_path = os.path.join(\n\t                                    ref_img_dir, ref_file_name)\n\t                        elif k == 'semantic':\n\t                            semantic_file_name = file_name.replace(\n\t                                'rgb_anon.png', 'gt_labelTrainIds.png')\n\t                            file_path = os.path.join(\n\t                                semantic_dir, semantic_file_name)\n\t                        self.paths[k].append(file_path)\n\t    def __getitem__(self, index: int):\n\t        sample: Any = {}\n", "        filename = self.paths['image'][index].split('/')[-1]\n\t        sample['filename'] = filename\n\t        for k in self.load_keys:\n\t            if k in ['image', 'image_ref']:\n\t                data = Image.open(self.paths[k][index]).convert('RGB')\n\t            elif k == 'semantic':\n\t                if self.load_pseudo_labels:\n\t                    path = os.path.join(self.pseudo_label_dir, filename)\n\t                    data = Image.open(path)\n\t                else:\n", "                    data = Image.open(self.paths[k][index])\n\t            else:\n\t                raise ValueError('invalid load_key')\n\t            sample[k] = data\n\t        if self.transforms is not None:\n\t            sample = self.transforms(sample)\n\t        return sample\n\t    def __len__(self) -> int:\n\t        return len(next(iter(self.paths.values())))\n"]}
{"filename": "data_modules/datasets/robotcar.py", "chunked_list": ["import io\n\timport os\n\timport zipfile\n\tfrom typing import Any, Callable, List, Optional, Union\n\timport h5py\n\timport numpy as np\n\timport requests\n\timport torch\n\tfrom PIL import Image\n\tURLS = {\n", "    \"pseudo_labels_train_RobotCar_cma_segformer\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_RobotCar_cma_segformer.zip\",\n\t}\n\tclass RobotCar(torch.utils.data.Dataset):\n\t    ignore_index = 255\n\t    id_to_trainid = {-1: ignore_index, 0: ignore_index, 1: ignore_index, 2: ignore_index,\n\t                     3: ignore_index, 4: ignore_index, 5: ignore_index, 6: ignore_index,\n\t                     7: 0, 8: 1, 9: ignore_index, 10: ignore_index, 11: 2, 12: 3, 13: 4,\n\t                     14: ignore_index, 15: ignore_index, 16: ignore_index, 17: 5,\n\t                     18: ignore_index, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n\t                     28: 15, 29: ignore_index, 30: ignore_index, 31: 16, 32: 17, 33: 18}\n", "    orig_dims = (1024, 1024)\n\t    def __init__(\n\t            self,\n\t            root: str,\n\t            stage: str = \"train\",\n\t            load_keys: Union[List[str], str] = [\n\t                \"image_ref\", \"image\", \"semantic\"],\n\t            transforms: Optional[Callable] = None,\n\t            predict_on: str = \"test\",\n\t            load_pseudo_labels: bool = False,\n", "            pseudo_label_dir: Optional[str] = None,\n\t            **kwargs\n\t    ) -> None:\n\t        super().__init__()\n\t        self.root = root\n\t        self.transforms = transforms\n\t        self.load_pseudo_labels = load_pseudo_labels\n\t        if self.load_pseudo_labels:\n\t            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n\t            os.makedirs(_dpath, exist_ok=True)\n", "            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n\t                print('Downloading and extracting pseudo-labels...')\n\t                r = requests.get(URLS[pseudo_label_dir])\n\t                z = zipfile.ZipFile(io.BytesIO(r.content))\n\t                z.extractall(_dpath)\n\t            self.pseudo_label_dir = os.path.join(\n\t                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\t        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n\t        self.stage = stage\n\t        # mapping from stage to splits\n", "        if self.stage == 'train':\n\t            self.split = 'train'\n\t        elif self.stage == 'val':\n\t            self.split = 'val'\n\t        elif self.stage == 'test':\n\t            self.split = 'test'\n\t        elif self.stage == 'predict':\n\t            self.split = predict_on\n\t        if isinstance(load_keys, str):\n\t            self.load_keys = [load_keys]\n", "        else:\n\t            self.load_keys = load_keys\n\t        if not os.path.isdir(self.root):\n\t            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n\t                               ' specified \"split\" are inside the \"root\" directory')\n\t        if self.split == 'train':\n\t            self.images_dir = os.path.join(self.root, 'images')\n\t            corr_dir = os.path.join(self.root, 'correspondence_data')\n\t            self.paths = {'corr_files': []}\n\t            f_name_list = [fn for fn in os.listdir(\n", "                corr_dir) if fn.endswith('mat')]\n\t            for f_name in f_name_list:\n\t                self.paths['corr_files'].append(\n\t                    os.path.join(corr_dir, f_name))\n\t        else:\n\t            assert not \"image_ref\" in self.load_keys\n\t            self.paths = {k: [] for k in self.load_keys}\n\t            splitdir_map = {'val': 'validation', 'test': 'testing'}\n\t            images_dir = os.path.join(\n\t                self.root, 'segmented_images', splitdir_map[self.split], 'imgs')\n", "            semantic_dir = os.path.join(\n\t                self.root, 'segmented_images', splitdir_map[self.split], 'annos')\n\t            for img_name in os.listdir(images_dir):\n\t                for k in self.load_keys:\n\t                    if k == 'image':\n\t                        file_path = os.path.join(images_dir, img_name)\n\t                    elif k == 'semantic':\n\t                        file_path = os.path.join(semantic_dir, img_name)\n\t                    self.paths[k].append(file_path)\n\t    def __getitem__(self, index: int):\n", "        sample: Any = {}\n\t        if self.split == 'train':\n\t            # Load correspondence data\n\t            mat_content = {}\n\t            f = h5py.File(self.paths['corr_files'][index], 'r')\n\t            for k, v in f.items():\n\t                mat_content[k] = np.array(v)\n\t            im1name = ''.join(chr(a[0])\n\t                              for a in mat_content['im_i_path'])  # convert to string\n\t            im2name = ''.join(chr(a[0])\n", "                              for a in mat_content['im_j_path'])  # convert to string\n\t            filename = im2name.split('/')[-1]\n\t            sample['filename'] = filename\n\t            for k in self.load_keys:\n\t                if k == 'image_ref':\n\t                    data = Image.open(os.path.join(\n\t                        self.images_dir, im1name)).convert('RGB')\n\t                elif k == 'image':\n\t                    data = Image.open(os.path.join(\n\t                        self.images_dir, im2name)).convert('RGB')\n", "                elif k == 'semantic':\n\t                    assert self.load_pseudo_labels\n\t                    name = '.'.join([*filename.split('.')[:-1], 'png'])\n\t                    path = os.path.join(self.pseudo_label_dir, name)\n\t                    data = Image.open(path)\n\t                sample[k] = data\n\t        else:\n\t            sample['filename'] = self.paths['image'][index].split('/')[-1]\n\t            for k in self.load_keys:\n\t                if k == 'image':\n", "                    data = Image.open(self.paths[k][index]).convert('RGB')\n\t                elif k == 'semantic':\n\t                    data = Image.open(self.paths[k][index])\n\t                    data = self.encode_semantic_map(data)\n\t                sample[k] = data\n\t        if self.transforms is not None:\n\t            sample = self.transforms(sample)\n\t        return sample\n\t    def __len__(self) -> int:\n\t        return len(next(iter(self.paths.values())))\n", "    @classmethod\n\t    def encode_semantic_map(cls, semseg):\n\t        semseg_arr = np.array(semseg)\n\t        semseg_arr_copy = semseg_arr.copy()\n\t        for k, v in cls.id_to_trainid.items():\n\t            semseg_arr_copy[semseg_arr == k] = v\n\t        return Image.fromarray(semseg_arr_copy.astype(np.uint8))\n"]}
{"filename": "data_modules/datasets/__init__.py", "chunked_list": ["from .acdc import ACDC\n\tfrom .acg import ACG\n\tfrom .darkzurich import DarkZurich\n\tfrom .robotcar import RobotCar\n"]}
{"filename": "data_modules/datasets/acg.py", "chunked_list": ["import json\n\timport os\n\tfrom collections import namedtuple\n\tfrom typing import Any, Callable, List, Optional, Union\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n\tclass ACG(torch.utils.data.Dataset):\n\t    \"\"\"ACG benchmark from the paper:\n\t        Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation\n", "    \"\"\"\n\t    WildDash2Class = namedtuple(\"WildDash2Class\", [\"name\", \"id\", \"train_id\"])\n\t    labels = [\n\t        #       name                             id    trainId  \n\t        WildDash2Class(  'unlabeled'            ,  0 ,      255  ),\n\t        WildDash2Class(  'ego vehicle'          ,  1 ,      255  ),\n\t        WildDash2Class(  'overlay'              ,  2 ,      255  ),\n\t        WildDash2Class(  'out of roi'           ,  3 ,      255  ),\n\t        WildDash2Class(  'static'               ,  4 ,      255  ),\n\t        WildDash2Class(  'dynamic'              ,  5 ,      255  ),\n", "        WildDash2Class(  'ground'               ,  6 ,      255  ),\n\t        WildDash2Class(  'road'                 ,  7 ,        0  ),\n\t        WildDash2Class(  'sidewalk'             ,  8 ,        1  ),\n\t        WildDash2Class(  'parking'              ,  9 ,      255  ),\n\t        WildDash2Class(  'rail track'           , 10 ,      255  ),\n\t        WildDash2Class(  'building'             , 11 ,        2  ),\n\t        WildDash2Class(  'wall'                 , 12 ,        3  ),\n\t        WildDash2Class(  'fence'                , 13 ,        4  ),\n\t        WildDash2Class(  'guard rail'           , 14 ,      255  ),\n\t        WildDash2Class(  'bridge'               , 15 ,      255  ),\n", "        WildDash2Class(  'tunnel'               , 16 ,      255  ),\n\t        WildDash2Class(  'pole'                 , 17 ,        5  ),\n\t        WildDash2Class(  'polegroup'            , 18 ,      255  ),\n\t        WildDash2Class(  'traffic light'        , 19 ,        6  ),\n\t        WildDash2Class(  'traffic sign front'   , 20 ,        7  ),\n\t        WildDash2Class(  'vegetation'           , 21 ,        8  ),\n\t        WildDash2Class(  'terrain'              , 22 ,        9  ),\n\t        WildDash2Class(  'sky'                  , 23 ,       10  ),\n\t        WildDash2Class(  'person'               , 24 ,       11  ),\n\t        WildDash2Class(  'rider'                , 25 ,       12  ),\n", "        WildDash2Class(  'car'                  , 26 ,       13  ),\n\t        WildDash2Class(  'truck'                , 27 ,       14  ),\n\t        WildDash2Class(  'bus'                  , 28 ,       15  ),\n\t        WildDash2Class(  'caravan'              , 29 ,      255  ),\n\t        WildDash2Class(  'trailer'              , 30 ,      255  ),\n\t        WildDash2Class(  'on rails'             , 31 ,       16  ),\n\t        WildDash2Class(  'motorcycle'           , 32 ,       17  ),\n\t        WildDash2Class(  'bicycle'              , 33 ,       18  ),\n\t        WildDash2Class(  'pickup'               , 34 ,       14  ),\n\t        WildDash2Class(  'van'                  , 35 ,       13  ),\n", "        WildDash2Class(  'billboard'            , 36 ,      255  ),\n\t        WildDash2Class(  'street light'         , 37 ,      255  ),\n\t        WildDash2Class(  'road marking'         , 38 ,        0  ),\n\t        WildDash2Class(  'junctionbox'          , 39 ,      255  ),\n\t        WildDash2Class(  'mailbox'              , 40 ,      255  ),\n\t        WildDash2Class(  'manhole'              , 41 ,        0  ),\n\t        WildDash2Class(  'phonebooth'           , 42 ,      255  ),\n\t        WildDash2Class(  'pothole'              , 43 ,        0  ),\n\t        WildDash2Class(  'bikerack'             , 44 ,      255  ),\n\t        WildDash2Class(  'traffic sign frame'   , 45 ,        5  ),\n", "        WildDash2Class(  'utility pole'         , 46 ,        5  ),\n\t        WildDash2Class(  'motorcyclist'         , 47 ,       12  ),\n\t        WildDash2Class(  'bicyclist'            , 48 ,       12  ),\n\t        WildDash2Class(  'other rider'          , 49 ,       12  ),\n\t        WildDash2Class(  'bird'                 , 50 ,      255  ),\n\t        WildDash2Class(  'ground animal'        , 51 ,      255  ),\n\t        WildDash2Class(  'curb'                 , 52 ,        1  ),\n\t        WildDash2Class(  'traffic sign any'     , 53 ,      255  ),\n\t        WildDash2Class(  'traffic sign back'    , 54 ,      255  ),\n\t        WildDash2Class(  'trashcan'             , 55 ,      255  ),\n", "        WildDash2Class(  'other barrier'        , 56 ,        3  ),\n\t        WildDash2Class(  'other vehicle'        , 57 ,      255  ),\n\t        WildDash2Class(  'auto rickshaw'        , 58 ,       17  ),\n\t        WildDash2Class(  'bench'                , 59 ,      255  ),\n\t        WildDash2Class(  'mountain'             , 60 ,      255  ),\n\t        WildDash2Class(  'tram track'           , 61 ,        0  ),\n\t        WildDash2Class(  'wheeled slow'         , 62 ,      255  ),\n\t        WildDash2Class(  'boat'                 , 63 ,      255  ),\n\t        WildDash2Class(  'bikelane'             , 64 ,        0  ),\n\t        WildDash2Class(  'bikelane sidewalk'    , 65 ,        1  ),\n", "        WildDash2Class(  'banner'               , 66 ,      255  ),\n\t        WildDash2Class(  'dashcam mount'        , 67 ,      255  ),\n\t        WildDash2Class(  'water'                , 68 ,      255  ),\n\t        WildDash2Class(  'sand'                 , 69 ,      255  ),\n\t        WildDash2Class(  'pedestrian area'      , 70 ,        0  ),\n\t        WildDash2Class(  'fire hydrant'         , 71 ,      255  ),\n\t        WildDash2Class(  'cctv camera'          , 72 ,      255  ),\n\t        WildDash2Class(  'snow'                 , 73 ,      255  ),\n\t        WildDash2Class(  'catch basin'          , 74 ,        0  ),\n\t        WildDash2Class(  'crosswalk plain'      , 75 ,        0  ),\n", "        WildDash2Class(  'crosswalk zebra'      , 76 ,        0  ),\n\t        WildDash2Class(  'manhole sidewalk'     , 77 ,        1  ),\n\t        WildDash2Class(  'curb terrain'         , 78 ,        9  ),\n\t        WildDash2Class(  'service lane'         , 79 ,        0  ),\n\t        WildDash2Class(  'curb cut'             , 80 ,        1  ),\n\t        WildDash2Class(  'license plate'        , -1 ,      255  ),\n\t    ]\n\t    id_to_train_id = np.array([c.train_id for c in labels], dtype=int)\n\t    def __init__(\n\t            self,\n", "            root: str,\n\t            stage: str = \"test\",\n\t            load_keys: Union[List[str], str] = [\"image\", \"semantic\"],\n\t            conditions: Union[List[str], str] = [\"fog\", \"night\", \"rain\", \"snow\"],\n\t            transforms: Optional[Callable] = None,\n\t    ) -> None:\n\t        super().__init__()\n\t        self.root = root\n\t        self.conditions = [conditions] if isinstance(conditions, str) else conditions\n\t        self.transforms = transforms\n", "        assert stage == 'test'\n\t        self.stage = stage\n\t        self.split = 'test'\n\t        if isinstance(load_keys, str):\n\t            self.load_keys = [load_keys]\n\t        else:\n\t            self.load_keys = load_keys\n\t        # load WildDash2 encodings\n\t        pan = json.load(open(os.path.join(self.root, 'WildDash2', 'panoptic.json')))\n\t        self.img_to_segments = {img_dict['file_name']: img_dict['segments_info'] for img_dict in pan['annotations']}\n", "        self.paths = {k: [] for k in ['image', 'semantic', 'dataset']}\n\t        paths_night = {k: [] for k in ['image', 'semantic', 'dataset']}\n\t        paths_rain = {k: [] for k in ['image', 'semantic', 'dataset']}\n\t        paths_snow = {k: [] for k in ['image', 'semantic', 'dataset']}\n\t        paths_fog = {k: [] for k in ['image', 'semantic', 'dataset']}\n\t        wilddash_img_ids_fog = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_fog'))]\n\t        for file_name in wilddash_img_ids_fog:\n\t            file_path = os.path.join(self.root, 'WildDash2', file_name)\n\t            semantic_name = file_name.replace('images/', 'panoptic/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n", "            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n\t            paths_fog['image'].append(file_path)\n\t            paths_fog['semantic'].append(semantic_path)\n\t            paths_fog['dataset'].append('wilddash')\n\t        wilddash_img_ids_night = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_night'))]\n\t        for file_name in wilddash_img_ids_night:\n\t            file_path = os.path.join(self.root, 'WildDash2', file_name)\n\t            semantic_name = file_name.replace('images/', 'panoptic/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n", "            paths_night['image'].append(file_path)\n\t            paths_night['semantic'].append(semantic_path)\n\t            paths_night['dataset'].append('wilddash')\n\t        wilddash_img_ids_rain = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_rain'))]\n\t        for file_name in wilddash_img_ids_rain:\n\t            file_path = os.path.join(self.root, 'WildDash2', file_name)\n\t            semantic_name = file_name.replace('images/', 'panoptic/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n\t            paths_rain['image'].append(file_path)\n", "            paths_rain['semantic'].append(semantic_path)\n\t            paths_rain['dataset'].append('wilddash')\n\t        wilddash_img_ids_snow = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_snow'))]\n\t        for file_name in wilddash_img_ids_snow:\n\t            file_path = os.path.join(self.root, 'WildDash2', file_name)\n\t            semantic_name = file_name.replace('images/', 'panoptic/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n\t            paths_snow['image'].append(file_path)\n\t            paths_snow['semantic'].append(semantic_path)\n", "            paths_snow['dataset'].append('wilddash')\n\t        bdd100k_img_ids_fog = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_fog'))]\n\t        for file_name in bdd100k_img_ids_fog:\n\t            file_path = os.path.join(self.root, 'bdd100k', file_name)\n\t            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n\t            paths_fog['image'].append(file_path)\n\t            paths_fog['semantic'].append(semantic_path)\n\t            paths_fog['dataset'].append('bdd100k')\n", "        bdd100k_img_ids_night = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_night'))]\n\t        for file_name in bdd100k_img_ids_night:\n\t            file_path = os.path.join(self.root, 'bdd100k', file_name)\n\t            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n\t            paths_night['image'].append(file_path)\n\t            paths_night['semantic'].append(semantic_path)\n\t            paths_night['dataset'].append('bdd100k')\n\t        bdd100k_img_ids_rain = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_rain'))]\n", "        for file_name in bdd100k_img_ids_rain:\n\t            file_path = os.path.join(self.root, 'bdd100k', file_name)\n\t            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n\t            paths_rain['image'].append(file_path)\n\t            paths_rain['semantic'].append(semantic_path)\n\t            paths_rain['dataset'].append('bdd100k')\n\t        bdd100k_img_ids_snow = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_snow'))]\n\t        for file_name in bdd100k_img_ids_snow:\n", "            file_path = os.path.join(self.root, 'bdd100k', file_name)\n\t            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n\t            semantic_name = semantic_name.replace('.jpg', '.png')\n\t            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n\t            paths_snow['image'].append(file_path)\n\t            paths_snow['semantic'].append(semantic_path)\n\t            paths_snow['dataset'].append('bdd100k')\n\t        foggydriving_img_ids = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_FoggyDriving_fog'))]\n\t        for file_name in foggydriving_img_ids:\n\t            file_path = os.path.join(self.root, 'Foggy_Driving', file_name)\n", "            if 'test_extra' in file_name:\n\t                semantic_name = file_name.replace('leftImg8bit/test_extra/', 'gtCoarse/test_extra/')\n\t                semantic_name = semantic_name.replace('_leftImg8bit.png', '_gtCoarse_labelTrainIds.png')\n\t            else:\n\t                semantic_name = file_name.replace('leftImg8bit/test/', 'gtFine/test/')\n\t                semantic_name = semantic_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n\t            semantic_path = os.path.join(self.root, 'Foggy_Driving', semantic_name)\n\t            paths_fog['image'].append(file_path)\n\t            paths_fog['semantic'].append(semantic_path)\n\t            paths_fog['dataset'].append('foggydriving')\n", "        foggyzurich_img_ids = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_FoggyZurich_fog'))]\n\t        for file_name in foggyzurich_img_ids:\n\t            file_path = os.path.join(self.root, 'Foggy_Zurich', file_name)\n\t            semantic_path = file_path.replace('/RGB/', '/gt_labelTrainIds/')\n\t            paths_fog['image'].append(file_path)\n\t            paths_fog['semantic'].append(semantic_path)\n\t            paths_fog['dataset'].append('foggyzurich')\n\t        custom_img_ids = [\n\t            'train_000001.jpg',\n\t            'train_000002.jpg',\n", "            'train_000003.jpg',\n\t        ]\n\t        for file_name in custom_img_ids:\n\t            file_path = os.path.join(self.root, 'ACG', file_name)\n\t            semantic_path = file_path.replace('.jpg', '_mask.png')\n\t            paths_rain['image'].append(file_path)\n\t            paths_rain['semantic'].append(semantic_path)\n\t            paths_rain['dataset'].append('custom')\n\t        self.len_fog = len(paths_fog['image'])\n\t        self.len_night = len(paths_night['image'])\n", "        self.len_rain = len(paths_rain['image'])\n\t        self.len_snow = len(paths_snow['image'])\n\t        for c in self.conditions:\n\t            if c == \"fog\":\n\t                self.paths['image'].extend(paths_fog['image'])\n\t                self.paths['semantic'].extend(paths_fog['semantic'])\n\t                self.paths['dataset'].extend(paths_fog['dataset'])\n\t            elif c == \"night\":\n\t                self.paths['image'].extend(paths_night['image'])\n\t                self.paths['semantic'].extend(paths_night['semantic'])\n", "                self.paths['dataset'].extend(paths_night['dataset'])\n\t            elif c == \"rain\":\n\t                self.paths['image'].extend(paths_rain['image'])\n\t                self.paths['semantic'].extend(paths_rain['semantic'])\n\t                self.paths['dataset'].extend(paths_rain['dataset'])\n\t            elif c == \"snow\":\n\t                self.paths['image'].extend(paths_snow['image'])\n\t                self.paths['semantic'].extend(paths_snow['semantic'])\n\t                self.paths['dataset'].extend(paths_snow['dataset'])\n\t    def __getitem__(self, index: int):\n", "        sample: Any = {}\n\t        filename = self.paths['image'][index].split('/')[-1]\n\t        sample['filename'] = filename\n\t        dataset = self.paths['dataset'][index]\n\t        for k in self.load_keys:\n\t            if k == 'image':\n\t                data = Image.open(self.paths[k][index]).convert('RGB')\n\t            elif k == 'semantic':\n\t                data = Image.open(self.paths[k][index])\n\t                if dataset == 'wilddash':\n", "                    data = self.encode_semantic_map(\n\t                        data, filename.replace('.jpg', '.png'))\n\t            else:\n\t                raise ValueError('invalid load_key')\n\t            sample[k] = data\n\t        if self.transforms is not None:\n\t            sample = self.transforms(sample)\n\t        return sample\n\t    def __len__(self) -> int:\n\t        return len(self.paths['image'])\n", "    def encode_semantic_map(self, semseg, filename):\n\t        pan_format = np.array(semseg, dtype=np.uint32)\n\t        pan = self.rgb2id(pan_format)\n\t        semantic = np.zeros(pan.shape, dtype=np.uint8)\n\t        for segm_info in self.img_to_segments[filename]:\n\t            semantic[pan == segm_info['id']] = segm_info['category_id']\n\t        semantic = self.id_to_train_id[semantic.astype(int)]\n\t        return Image.fromarray(semantic.astype(np.uint8))\n\t    @staticmethod\n\t    def rgb2id(color):\n", "        if color.dtype == np.uint8:\n\t            color = color.astype(np.uint32)\n\t        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n"]}
{"filename": "data_modules/datasets/acdc.py", "chunked_list": ["import io\n\timport os\n\timport zipfile\n\tfrom typing import Any, Callable, List, Optional, Union\n\timport requests\n\timport torch\n\tfrom PIL import Image\n\tURLS = {\n\t    \"pseudo_labels_train_ACDC_cma_segformer\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_ACDC_cma_segformer.zip\",\n\t    \"pseudo_labels_train_ACDC_cma_deeplabv2\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_ACDC_cma_deeplabv2.zip\",\n", "}\n\tclass ACDC(torch.utils.data.Dataset):\n\t    orig_dims = (1080, 1920)\n\t    def __init__(\n\t            self,\n\t            root: str,\n\t            stage: str = \"train\",\n\t            condition: Union[List[str], str] = [\n\t                \"fog\", \"night\", \"rain\", \"snow\"],\n\t            load_keys: Union[List[str], str] = [\n", "                \"image_ref\", \"image\", \"semantic\"],\n\t            transforms: Optional[Callable] = None,\n\t            predict_on: str = \"test\",\n\t            load_pseudo_labels: bool = False,\n\t            pseudo_label_dir: Optional[str] = None,\n\t            **kwargs\n\t    ) -> None:\n\t        super().__init__()\n\t        self.root = root\n\t        self.transforms = transforms\n", "        self.load_pseudo_labels = load_pseudo_labels\n\t        if self.load_pseudo_labels:\n\t            assert pseudo_label_dir is not None\n\t            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n\t            os.makedirs(_dpath, exist_ok=True)\n\t            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n\t                print('Downloading and extracting pseudo-labels...')\n\t                r = requests.get(URLS[pseudo_label_dir])\n\t                z = zipfile.ZipFile(io.BytesIO(r.content))\n\t                z.extractall(_dpath)\n", "            self.pseudo_label_dir = os.path.join(\n\t                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\t        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n\t        self.stage = stage\n\t        # mapping from stage to splits\n\t        if self.stage == 'train':\n\t            self.split = 'train'\n\t        elif self.stage == 'val':\n\t            self.split = 'val'\n\t        elif self.stage == 'test':\n", "            self.split = 'val'  # test on val split\n\t        elif self.stage == 'predict':\n\t            self.split = predict_on\n\t        self.condition = [condition] if isinstance(\n\t            condition, str) else condition\n\t        self.load_keys = [load_keys] if isinstance(\n\t            load_keys, str) else load_keys\n\t        self.paths = {k: []\n\t                      for k in ['image', 'image_ref', 'semantic']}\n\t        self.images_dir = os.path.join(self.root, 'rgb_anon')\n", "        self.semantic_dir = os.path.join(self.root, 'gt')\n\t        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.semantic_dir):\n\t            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n\t                               ' specified \"split\" and \"condition\" are inside the \"root\" directory')\n\t        for cond in self.condition:\n\t            img_parent_dir = os.path.join(self.images_dir, cond, self.split)\n\t            semantic_parent_dir = os.path.join(\n\t                self.semantic_dir, cond, self.split)\n\t            for recording in os.listdir(img_parent_dir):\n\t                img_dir = os.path.join(img_parent_dir, recording)\n", "                semantic_dir = os.path.join(semantic_parent_dir, recording)\n\t                for file_name in os.listdir(img_dir):\n\t                    for k in ['image', 'image_ref', 'semantic']:\n\t                        if k == 'image':\n\t                            file_path = os.path.join(img_dir, file_name)\n\t                        elif k == 'image_ref':\n\t                            ref_img_dir = img_dir.replace(\n\t                                self.split, self.split + '_ref')\n\t                            ref_file_name = file_name.replace(\n\t                                'rgb_anon', 'rgb_ref_anon')\n", "                            file_path = os.path.join(\n\t                                ref_img_dir, ref_file_name)\n\t                        elif k == 'semantic':\n\t                            semantic_file_name = file_name.replace(\n\t                                'rgb_anon.png', 'gt_labelTrainIds.png')\n\t                            file_path = os.path.join(\n\t                                semantic_dir, semantic_file_name)\n\t                        self.paths[k].append(file_path)\n\t    def __getitem__(self, index: int):\n\t        sample: Any = {}\n", "        if (not 'image' in self.load_keys) and ('image_ref' in self.load_keys):\n\t            filename = self.paths['image_ref'][index].split('/')[-1]\n\t        else:\n\t            filename = self.paths['image'][index].split('/')[-1]\n\t        sample['filename'] = filename\n\t        for k in self.load_keys:\n\t            if k in ['image', 'image_ref']:\n\t                data = Image.open(self.paths[k][index]).convert('RGB')\n\t            elif k == 'semantic':\n\t                if self.load_pseudo_labels:\n", "                    path = os.path.join(self.pseudo_label_dir, filename)\n\t                    data = Image.open(path)\n\t                else:\n\t                    data = Image.open(self.paths[k][index])\n\t            else:\n\t                raise ValueError('invalid load_key')\n\t            sample[k] = data\n\t        if self.transforms is not None:\n\t            sample = self.transforms(sample)\n\t        return sample\n", "    def __len__(self) -> int:\n\t        return len(next(iter(self.paths.values())))\n"]}
{"filename": "helpers/metrics.py", "chunked_list": ["from typing import Any, Literal, Optional\n\timport torch\n\tfrom torch import Tensor\n\tfrom torchmetrics.classification import MulticlassJaccardIndex\n\tfrom torchmetrics.utilities.compute import _safe_divide\n\tdef _my_jaccard_index_reduce(\n\t    confmat: Tensor,\n\t    average: Optional[Literal[\"macro\", \"none\"]],\n\t    over_present_classes: bool = False,\n\t) -> Tensor:\n", "    allowed_average = [\"macro\", \"none\", None]\n\t    if average not in allowed_average:\n\t        raise ValueError(f\"The `average` has to be one of {allowed_average}, got {average}.\")\n\t    confmat = confmat.float()\n\t    if confmat.ndim == 3:  # multilabel\n\t        raise NotImplementedError\n\t    else:  # multiclass\n\t        num = torch.diag(confmat)\n\t        denom = confmat.sum(0) + confmat.sum(1) - num\n\t        if over_present_classes:\n", "            present_classes = confmat.sum(1) != 0\n\t            num = torch.masked_select(num, present_classes)\n\t            denom = torch.masked_select(denom, present_classes)\n\t    jaccard = _safe_divide(num, denom)\n\t    if average is None or average == \"none\":\n\t        return jaccard\n\t    return (jaccard / jaccard.numel()).sum()\n\tclass IoU(MulticlassJaccardIndex):\n\t    def __init__(\n\t        self,\n", "        over_present_classes: bool = False,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        self.over_present_classes = over_present_classes\n\t        super().__init__(**kwargs)\n\t    def compute(self) -> Tensor:\n\t        return _my_jaccard_index_reduce(self.confmat, average=self.average, over_present_classes=self.over_present_classes)\n"]}
{"filename": "helpers/pseudo_labels.py", "chunked_list": ["import os\n\tfrom collections import defaultdict\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom models.backbones import MixVisionTransformer, ResNet\n\tfrom models.heads import DeepLabV2Head, SegFormerHead\n\tfrom PIL import Image\n\tfrom tqdm import tqdm\n\t@torch.inference_mode()\n", "def generate_pseudo_labels(model, datamodule):\n\t    print(\"Generating pseudo labels...\")\n\t    datamodule.prepare_data()\n\t    datamodule.setup('predict')\n\t    assert len(datamodule.predict_ds) == 1\n\t    # iterate through all samples to collect statistics\n\t    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\t    model = model.to(device)\n\t    model.eval()\n\t    model.use_ensemble_inference = True\n", "    model.inference_ensemble_scale = [\n\t        0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n\t    print(\"Collecting statistics...\")\n\t    conf_dict = defaultdict(list)\n\t    for batch in tqdm(datamodule.predict_dataloader()[0]):\n\t        batch = model.transfer_batch_to_device(\n\t            batch, device, dataloader_idx=0)\n\t        orig_size = datamodule.predict_ds[0].orig_dims\n\t        x = batch['image'] if 'image' in batch.keys() else batch['image_ref']\n\t        y_hat = model.forward(x, orig_size)\n", "        probs = y_hat if model.use_ensemble_inference else nn.functional.softmax(y_hat, dim=1)\n\t        pred_prob, pred_labels = probs.max(dim=1)\n\t        for idx_cls in range(model.head.num_classes):\n\t            idx_temp = pred_labels == idx_cls\n\t            if idx_temp.any():\n\t                conf_cls_temp = pred_prob[idx_temp][::10].cpu(\n\t                ).tolist()\n\t                conf_dict[idx_cls].extend(conf_cls_temp)\n\t    trg_portion = 0.2\n\t    cls_thresh = torch.ones(\n", "        model.head.num_classes, dtype=torch.float, device=device)\n\t    for idx_cls in range(model.head.num_classes):\n\t        cls_probs = sorted(conf_dict[idx_cls], reverse=True)\n\t        len_cls_sel = int(len(cls_probs) * trg_portion)\n\t        if len_cls_sel > 0:\n\t            cls_thresh[idx_cls] = cls_probs[len_cls_sel - 1]\n\t    print(\"Extract PL...\")\n\t    dataset_name = datamodule.predict_on[0]\n\t    if isinstance(model.backbone, MixVisionTransformer) and isinstance(model.head, SegFormerHead):\n\t        model_name = 'segformer'\n", "    elif isinstance(model.backbone, ResNet) and isinstance(model.head, DeepLabV2Head):\n\t        model_name = 'deeplabv2'\n\t    else:\n\t        raise RuntimeError('unknown model configuration')\n\t    save_dir = os.path.join(\n\t        os.environ['$DATA_DIR'], 'pseudo_labels', 'pseudo_labels_train_{}_cma_{}'.format(dataset_name, model_name))\n\t    os.makedirs(save_dir, exist_ok=True)\n\t    # iterate through all samples to generate pseudo label\n\t    for batch in tqdm(datamodule.predict_dataloader()[0]):\n\t        filename = batch['filename']\n", "        if all(os.path.isfile(os.path.join(\n\t                save_dir, '.'.join([*im_name.split('.')[:-1], 'png']))) for im_name in filename):\n\t            continue\n\t        batch = model.transfer_batch_to_device(\n\t            batch, device, dataloader_idx=0)\n\t        orig_size = datamodule.predict_ds[0].orig_dims\n\t        x = batch['image'] if 'image' in batch.keys() else batch['image_ref']\n\t        y_hat = model.forward(x, orig_size)\n\t        probs = y_hat if model.use_ensemble_inference else nn.functional.softmax(y_hat, dim=1)\n\t        weighted_prob = probs / cls_thresh.view(1, -1, 1, 1)\n", "        weighted_conf, preds = weighted_prob.max(dim=1)\n\t        preds[weighted_conf < 1] = 255\n\t        for pred, im_name in zip(preds, filename):\n\t            arr = pred.cpu().numpy()\n\t            image = Image.fromarray(arr.astype(np.uint8))\n\t            image.save(os.path.join(\n\t                save_dir, '.'.join([*im_name.split('.')[:-1], 'png'])))\n"]}
{"filename": "helpers/__init__.py", "chunked_list": []}
{"filename": "helpers/lr_scheduler.py", "chunked_list": ["import torch\n\tfrom torch.optim import Optimizer\n\tclass LinearWarmupLinearLR(torch.optim.lr_scheduler._LRScheduler):\n\t    def __init__(self,\n\t                 optimizer: Optimizer,\n\t                 max_steps: int = None,\n\t                 warmup_iters: int = 1500,\n\t                 warmup_ratio: float = 1e-6,\n\t                 min_lr=0.,\n\t                 last_epoch=-1):\n", "        self.max_updates = max_steps\n\t        self.warmup_iters = warmup_iters\n\t        self.warmup_ratio = warmup_ratio\n\t        self.min_lr = min_lr\n\t        super().__init__(optimizer, last_epoch)\n\t    def get_lr(self):\n\t        # warmup phase\n\t        if self.last_epoch < self.warmup_iters:\n\t            k = (1 - self.last_epoch / self.warmup_iters) * \\\n\t                (1 - self.warmup_ratio)\n", "            return [_lr * (1 - k) for _lr in self.base_lrs]\n\t        # poly phase\n\t        else:\n\t            coeff = 1 - (self.last_epoch - self.warmup_iters) / \\\n\t                     float(self.max_updates - self.warmup_iters)\n\t            return [(base_lr - self.min_lr) * coeff + self.min_lr for base_lr in self.base_lrs]\n"]}
{"filename": "models/model.py", "chunked_list": ["import copy\n\timport itertools\n\timport math\n\timport os\n\tfrom typing import List, Optional\n\timport numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\timport torch.nn as nn\n\timport torchmetrics\n", "from PIL import Image\n\tfrom pytorch_lightning.callbacks import Checkpoint\n\tfrom pytorch_lightning.cli import instantiate_class\n\tfrom .backbones.mix_transformer import DropPath\n\tfrom .heads.base import BaseHead\n\tfrom .utils import (colorize_mask, estimate_probability_of_confidence_interval,\n\t                    warp)\n\tclass CMAModel(pl.LightningModule):\n\t    def __init__(self,\n\t                 optimizer_init: dict,\n", "                 lr_scheduler_init: dict,\n\t                 backbone: nn.Module,\n\t                 head: BaseHead,\n\t                 contrastive_head: Optional[nn.Module] = None,\n\t                 alignment_backbone: Optional[nn.Module] = None,\n\t                 alignment_head: Optional[BaseHead] = None,\n\t                 self_training_loss: Optional[nn.Module] = None,\n\t                 self_training_loss_weight: float = 1.0,\n\t                 entropy_loss: Optional[nn.Module] = None,\n\t                 entropy_loss_weight: float = 1.0,\n", "                 contrastive_loss: Optional[nn.Module] = None,\n\t                 contrastive_loss_weight: float = 1.0,\n\t                 use_slide_inference: bool = True,\n\t                 use_ensemble_inference: bool = False,\n\t                 inference_ensemble_scale: List[float] = [1.0],\n\t                 inference_ensemble_scale_divisor: Optional[int] = None,\n\t                 projection_head_lr_factor: float = 10.0,\n\t                 ema_momentum: float = 0.9999,\n\t                 freeze_decoder: bool = True,\n\t                 metrics: dict = {},\n", "                 ):\n\t        super().__init__()\n\t        self.save_hyperparameters(ignore=[\n\t            'backbone',\n\t            'head',\n\t            'contrastive_head',\n\t            'alignment_backbone',\n\t            'alignment_head',\n\t            'self_training_loss',\n\t            'entropy_loss',\n", "            'contrastive_loss'\n\t        ])\n\t        #### MODEL ####\n\t        self.backbone = backbone\n\t        self.head = head\n\t        self.contrastive_head = contrastive_head\n\t        self.m_backbone = copy.deepcopy(self.backbone)\n\t        self.m_contrastive_head = copy.deepcopy(self.contrastive_head)\n\t        for ema_m in filter(None, [self.m_backbone, self.m_contrastive_head]):\n\t            for param in ema_m.parameters():\n", "                param.requires_grad = False\n\t        self.alignment_backbone = alignment_backbone\n\t        self.alignment_head = alignment_head\n\t        for alignment_m in filter(None, [self.alignment_backbone, self.alignment_head]):\n\t            for param in alignment_m.parameters():\n\t                param.requires_grad = False\n\t        #### LOSSES ####\n\t        self.self_training_loss = self_training_loss\n\t        self.self_training_loss_weight = self_training_loss_weight\n\t        self.entropy_loss = entropy_loss\n", "        self.entropy_loss_weight = entropy_loss_weight\n\t        self.contrastive_loss = contrastive_loss\n\t        self.contrastive_loss_weight = contrastive_loss_weight\n\t        #### INFERENCE ####\n\t        self.use_slide_inference = use_slide_inference\n\t        self.use_ensemble_inference = use_ensemble_inference\n\t        self.inference_ensemble_scale = inference_ensemble_scale\n\t        self.inference_ensemble_scale_divisor = inference_ensemble_scale_divisor\n\t        #### METRICS ####\n\t        val_metrics = {'val_{}_{}'.format(ds, el['class_path'].split(\n", "            '.')[-1]): instantiate_class(tuple(), el) for ds, m in metrics.get('val', {}).items() for el in m}\n\t        test_metrics = {'test_{}_{}'.format(ds, el['class_path'].split(\n\t            '.')[-1]): instantiate_class(tuple(), el) for ds, m in metrics.get('test', {}).items() for el in m}\n\t        self.valid_metrics = torchmetrics.MetricCollection(val_metrics)\n\t        self.test_metrics = torchmetrics.MetricCollection(test_metrics)\n\t        #### OPTIMIZATION ####\n\t        self.optimizer_init = optimizer_init\n\t        self.lr_scheduler_init = lr_scheduler_init\n\t        #### OTHER STUFF ####\n\t        self.projection_head_lr_factor = projection_head_lr_factor\n", "        self.ema_momentum = ema_momentum\n\t        self.freeze_decoder = freeze_decoder\n\t        if self.freeze_decoder:\n\t            for param in self.head.parameters():\n\t                param.requires_grad = False\n\t    def training_step(self, batch, batch_idx):\n\t        #\n\t        # MODEL FORWARD\n\t        #\n\t        images_trg = batch['image']\n", "        pseudo_label_trg = batch['semantic']\n\t        images_ref = batch['image_ref']\n\t        feats_trg = self.backbone(images_trg)\n\t        logits_trg = self.head(feats_trg)\n\t        logits_trg = nn.functional.interpolate(\n\t            logits_trg, images_trg.shape[-2:], mode='bilinear', align_corners=False)\n\t        if self.contrastive_loss is not None:\n\t            if self.global_step < self.contrastive_loss.warm_up_steps:\n\t                if isinstance(feats_trg, (list, tuple)):\n\t                    contrastive_inp = [el.detach() for el in feats_trg]\n", "                else:\n\t                    contrastive_inp = feats_trg.detach()\n\t            else:\n\t                contrastive_inp = feats_trg\n\t            emb_anc = self.contrastive_head(contrastive_inp)\n\t            with torch.no_grad():\n\t                self.update_momentum_encoder()\n\t                m_input = torch.cat((images_trg, images_ref))\n\t                m_output = self.m_contrastive_head(self.m_backbone(m_input))\n\t                emb_neg, emb_pos = torch.tensor_split(m_output, 2)\n", "                # warp the reference embeddings\n\t                emb_pos, confidence = self.align(emb_pos, images_ref, images_trg)\n\t        #\n\t        # LOSSES\n\t        #\n\t        loss = torch.tensor(0.0, device=self.device)\n\t        # SELF-TRAINING / CONSISTENCY\n\t        if self.self_training_loss is not None:\n\t            self_training_loss = self.self_training_loss(logits_trg, pseudo_label_trg)\n\t            self_training_loss *= self.self_training_loss_weight\n", "            self.log(\"train_loss_selftraining\", self_training_loss)\n\t            loss += self_training_loss\n\t        # ENTROPY MINIMIZATION\n\t        if self.entropy_loss is not None:\n\t            entropy_loss = self.entropy_loss(logits_trg)\n\t            entropy_loss *= self.entropy_loss_weight\n\t            self.log(\"train_loss_entropy\", entropy_loss)\n\t            loss += entropy_loss\n\t        # CONTRASTIVE\n\t        if self.contrastive_loss is not None:\n", "            contrastive_loss = self.contrastive_loss(emb_anc, emb_pos, confidence)\n\t            contrastive_loss *= self.contrastive_loss_weight\n\t            self.log(\"train_loss_contrastive\", contrastive_loss)\n\t            loss += contrastive_loss\n\t            # update the queue\n\t            with torch.no_grad():\n\t                emb_neg = self.all_gather(emb_neg)\n\t                if emb_neg.dim() == 5:\n\t                    emb_neg = torch.flatten(emb_neg, start_dim=0, end_dim=1)\n\t                self.contrastive_loss.update_queue(emb_neg)\n", "        return loss\n\t    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n\t        x, y = batch['image'], batch['semantic']\n\t        y_hat = self.forward(x, out_size=y.shape[-2:])\n\t        src_name = self.trainer.datamodule.idx_to_name['val'][dataloader_idx]\n\t        if not self.trainer.sanity_checking:\n\t            for k, m in self.valid_metrics.items():\n\t                if src_name in k:\n\t                    m.update(y_hat, y)\n\t    def validation_epoch_end(self, outs):\n", "        if not self.trainer.sanity_checking:\n\t            out_dict = self.valid_metrics.compute()\n\t            for k, v in out_dict.items():\n\t                self.log(k, v)\n\t        self.valid_metrics.reset()\n\t    def test_step(self, batch, batch_idx, dataloader_idx=0):\n\t        x, y = batch['image'], batch['semantic']\n\t        y_hat = self.forward(x, out_size=y.shape[-2:])\n\t        src_name = self.trainer.datamodule.idx_to_name['test'][dataloader_idx]\n\t        for k, m in self.test_metrics.items():\n", "            if src_name in k:\n\t                m.update(y_hat, y)\n\t    def test_epoch_end(self, outs):\n\t        out_dict = self.test_metrics.compute()\n\t        for k, v in out_dict.items():\n\t            self.log(k, v)\n\t        self.test_metrics.reset()\n\t    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n\t        dataset_name = self.trainer.datamodule.predict_on[dataloader_idx]\n\t        save_dir = os.path.join(os.path.dirname(\n", "            self.ckpt_dir), 'preds', dataset_name)\n\t        col_save_dir = os.path.join(os.path.dirname(\n\t            self.ckpt_dir), 'color_preds', dataset_name)\n\t        if self.trainer.is_global_zero:\n\t            os.makedirs(save_dir, exist_ok=True)\n\t            os.makedirs(col_save_dir, exist_ok=True)\n\t        img_names = batch['filename']\n\t        x = batch['image'] if 'image' in batch.keys() else batch['image_ref']\n\t        orig_size = self.trainer.datamodule.predict_ds[dataloader_idx].orig_dims\n\t        y_hat = self.forward(x, orig_size)\n", "        preds = torch.argmax(y_hat, dim=1)\n\t        for pred, im_name in zip(preds, img_names):\n\t            arr = pred.cpu().numpy()\n\t            image = Image.fromarray(arr.astype(np.uint8))\n\t            image.save(os.path.join(\n\t                save_dir, '.'.join([*im_name.split('.')[:-1], 'png'])))\n\t            col_image = colorize_mask(image)\n\t            col_image.save(os.path.join(\n\t                col_save_dir, '.'.join([*im_name.split('.')[:-1], 'png'])))\n\t    def forward(self, x, out_size=None):\n", "        if self.use_ensemble_inference:\n\t            out = self.forward_ensemble(x, out_size)\n\t        else:\n\t            if self.use_slide_inference:\n\t                out = self.slide_inference(x)\n\t            else:\n\t                out = self.whole_inference(x)\n\t            if out_size is not None:\n\t                out = nn.functional.interpolate(\n\t                    out, size=out_size, mode='bilinear', align_corners=False)\n", "        return out\n\t    def forward_ensemble(self, x, out_size=None):\n\t        assert out_size is not None\n\t        h, w = x.shape[-2:]\n\t        # RETURNS SUM OF PROBABILITIES\n\t        out_stack = 0\n\t        cnt = 0\n\t        for flip, scale in itertools.product([False, True], self.inference_ensemble_scale):\n\t            new_h, new_w = int(h * scale + 0.5), int(w * scale + 0.5)\n\t            if self.inference_ensemble_scale_divisor is not None:\n", "                size_divisor = self.inference_ensemble_scale_divisor\n\t                new_h = int(math.ceil(new_h / size_divisor)) * size_divisor\n\t                new_w = int(math.ceil(new_w / size_divisor)) * size_divisor\n\t            # this resize should mimic PIL\n\t            x_inp = nn.functional.interpolate(\n\t                x, size=(new_h, new_w), mode='bilinear', align_corners=False, antialias=True)\n\t            x_inp = x_inp.flip(-1) if flip else x_inp\n\t            if self.use_slide_inference:\n\t                out = self.slide_inference(x_inp)\n\t            else:\n", "                out = self.whole_inference(x_inp)\n\t            out = nn.functional.interpolate(\n\t                out, size=out_size, mode='bilinear', align_corners=False)\n\t            out = out.flip(-1) if flip else out\n\t            out = nn.functional.softmax(out, dim=1)\n\t            out_stack += out\n\t            cnt += 1\n\t        return out_stack / cnt\n\t    def whole_inference(self, x):\n\t        logits = self.head(self.backbone(x))\n", "        logits = nn.functional.interpolate(\n\t            logits, x.shape[-2:], mode='bilinear', align_corners=False)\n\t        return logits\n\t    def slide_inference(self, img):\n\t        \"\"\"\n\t        ---------------------------------------------------------------------------\n\t        Copyright (c) OpenMMLab. All rights reserved.\n\t        This source code is licensed under the license found in the\n\t        LICENSE file in https://github.com/open-mmlab/mmsegmentation.\n\t        ---------------------------------------------------------------------------\n", "        \"\"\"\n\t        batch_size, _, h_img, w_img = img.size()\n\t        h_stride, w_stride, h_crop, w_crop = self.get_inference_slide_params(\n\t            h_img, w_img)\n\t        num_classes = self.head.num_classes\n\t        h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n\t        w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n\t        preds = img.new_zeros((batch_size, num_classes, h_img, w_img))\n\t        count_mat = img.new_zeros((batch_size, 1, h_img, w_img))\n\t        for h_idx in range(h_grids):\n", "            for w_idx in range(w_grids):\n\t                y1 = h_idx * h_stride\n\t                x1 = w_idx * w_stride\n\t                y2 = min(y1 + h_crop, h_img)\n\t                x2 = min(x1 + w_crop, w_img)\n\t                y1 = max(y2 - h_crop, 0)\n\t                x1 = max(x2 - w_crop, 0)\n\t                crop_img = img[:, :, y1:y2, x1:x2]\n\t                crop_seg_logit = self.whole_inference(crop_img)\n\t                preds += nn.functional.pad(crop_seg_logit,\n", "                                            (int(x1), int(preds.shape[3] - x2), int(y1),\n\t                                            int(preds.shape[2] - y2)))\n\t                count_mat[:, :, y1:y2, x1:x2] += 1\n\t        assert (count_mat == 0).sum() == 0\n\t        preds = preds / count_mat\n\t        return preds\n\t    def configure_optimizers(self):\n\t        optimizer = instantiate_class(\n\t            self.optimizer_parameters(), self.optimizer_init)\n\t        lr_scheduler = {\n", "            \"scheduler\": instantiate_class(optimizer, self.lr_scheduler_init),\n\t            \"interval\": \"step\"\n\t        }\n\t        return [optimizer], [lr_scheduler]\n\t    def optimizer_parameters(self):\n\t        proj_weight_params = []\n\t        proj_bias_params = []\n\t        other_weight_params = []\n\t        other_bias_params = []\n\t        for name, p in self.named_parameters():\n", "            if not p.requires_grad: \n\t                continue\n\t            if name.startswith('contrastive_head'):\n\t                proj_bias_params.append(p) if len(p.shape) == 1 else proj_weight_params.append(p)\n\t            else:\n\t                other_bias_params.append(p) if len(p.shape) == 1 else other_weight_params.append(p)\n\t        lr = self.optimizer_init['init_args']['lr']\n\t        weight_decay = self.optimizer_init['init_args']['weight_decay']\n\t        return [\n\t            {'name': 'other_weight', 'params': other_weight_params,\n", "                'lr': lr, 'weight_decay': weight_decay},\n\t            {'name': 'other_bias', 'params': other_bias_params,\n\t                'lr': lr, 'weight_decay': 0},\n\t            {'name': 'proj_weight', 'params': proj_weight_params,\n\t                'lr': self.projection_head_lr_factor * lr, 'weight_decay': weight_decay},\n\t            {'name': 'proj_bias', 'params': proj_bias_params,\n\t                'lr': self.projection_head_lr_factor * lr, 'weight_decay': 0}\n\t        ]\n\t    @torch.no_grad()\n\t    def align(self, logits_ref, images_ref, images_trg):\n", "        assert self.alignment_head is not None\n\t        h, w = logits_ref.shape[-2:]\n\t        images_trg = nn.functional.interpolate(images_trg, size=(h, w), mode='bilinear', align_corners=False, antialias=True)\n\t        images_ref = nn.functional.interpolate(images_ref, size=(h, w), mode='bilinear', align_corners=False, antialias=True)\n\t        images_trg_256 = nn.functional.interpolate(\n\t            images_trg, size=(256, 256), mode='area')\n\t        images_ref_256 = nn.functional.interpolate(\n\t            images_ref, size=(256, 256), mode='area')\n\t        x_backbone = self.alignment_backbone(\n\t            torch.cat([images_ref, images_trg]), extract_only_indices=[-3, -2])\n", "        unpacked_x = [torch.tensor_split(l, 2) for l in x_backbone]\n\t        pyr_ref, pyr_trg = zip(*unpacked_x)\n\t        x_backbone_256 = self.alignment_backbone(\n\t            torch.cat([images_ref_256, images_trg_256]), extract_only_indices=[-2, -1])\n\t        unpacked_x_256 = [torch.tensor_split(l, 2) for l in x_backbone_256]\n\t        pyr_ref_256, pyr_trg_256 = zip(*unpacked_x_256)\n\t        trg_ref_flow, trg_ref_uncert = self.alignment_head(\n\t            pyr_trg, pyr_ref, pyr_trg_256, pyr_ref_256, (h, w))[-1]\n\t        trg_ref_flow = nn.functional.interpolate(\n\t            trg_ref_flow, size=(h, w), mode='bilinear', align_corners=False)\n", "        trg_ref_uncert = nn.functional.interpolate(\n\t            trg_ref_uncert, size=(h, w), mode='bilinear', align_corners=False)\n\t        trg_ref_cert = estimate_probability_of_confidence_interval(\n\t            trg_ref_uncert, R=1.0)\n\t        warped_ref_logits, trg_ref_mask = warp(\n\t            logits_ref, trg_ref_flow, return_mask=True)\n\t        warp_confidence = trg_ref_mask.unsqueeze(1) * trg_ref_cert\n\t        return warped_ref_logits, warp_confidence\n\t    @staticmethod\n\t    def get_inference_slide_params(h, w):\n", "        if h == w:\n\t            return 1, 1, h, w\n\t        if min(h, w) == h:  # wide image\n\t            assert w <= 2 * h\n\t            h_crop, w_crop, h_stride = h, h, 1\n\t            w_stride = w // 2 - h // 2 if w > 1.5 * h else w - h\n\t        else:  # tall image\n\t            assert h <= 2 * w\n\t            h_crop, w_crop, w_stride = w, w, 1\n\t            h_stride = h // 2 - w // 2 if h > 1.5 * w else h - w\n", "        return h_stride, w_stride, h_crop, w_crop\n\t    @torch.no_grad()\n\t    def update_momentum_encoder(self):\n\t        m = min(1.0 - 1 / (float(self.global_step) + 1.0),\n\t                self.ema_momentum)  # limit momentum in the beginning\n\t        for param, param_m in zip(\n\t                itertools.chain(self.backbone.parameters(), self.contrastive_head.parameters()),\n\t                itertools.chain(self.m_backbone.parameters(), self.m_contrastive_head.parameters())):\n\t            if not param.data.shape:\n\t                param_m.data = param_m.data * m + param.data * (1. - m)\n", "            else:\n\t                param_m.data[:] = param_m[:].data[:] * \\\n\t                    m + param[:].data[:] * (1. - m)\n\t    def train(self, mode=True):\n\t        super().train(mode=mode)\n\t        for m in filter(None, [self.alignment_backbone, self.alignment_head]):\n\t            m.eval()\n\t        for m in filter(None, [self.m_backbone, self.m_contrastive_head]):\n\t            if isinstance(m, (nn.modules.dropout._DropoutNd, DropPath)):\n\t                m.training = False\n", "        if self.freeze_decoder:\n\t            self.head.eval()\n\t    @property\n\t    def ckpt_dir(self):\n\t        # mirroring https://github.com/Lightning-AI/lightning/blob/3bee81960a6f8979c8e1b5e747a17124feee652d/src/pytorch_lightning/callbacks/model_checkpoint.py#L571\n\t        for cb in self.trainer.callbacks:\n\t            if isinstance(cb, Checkpoint):\n\t                if cb.dirpath is not None:\n\t                    return cb.dirpath\n\t        if len(self.trainer.loggers) > 0:\n", "            if self.trainer.loggers[0].save_dir is not None:\n\t                save_dir = self.trainer.loggers[0].save_dir\n\t            else:\n\t                save_dir = self.trainer.default_root_dir\n\t            name = self.trainer.loggers[0].name\n\t            version = self.trainer.loggers[0].version\n\t            version = version if isinstance(version, str) else f\"version_{version}\"\n\t            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\n\t        else:\n\t            # if no loggers, use default_root_dir\n", "            ckpt_path = os.path.join(self.trainer.default_root_dir, \"checkpoints\")\n\t        return ckpt_path\n"]}
{"filename": "models/losses.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn as nn\n\tfrom torch import Tensor\n\tclass NormalizedEntropyLoss(nn.Module):\n\t    def __init__(self, reduction: str = 'mean'):\n\t        super().__init__()\n\t        assert reduction in ['none', 'mean', 'sum'], 'invalid reduction'\n\t        self.reduction = reduction\n\t    def forward(self, logits: Tensor):\n", "        assert logits.dim() in [3, 4]\n\t        dim = logits.shape[1]\n\t        p_log_p = nn.functional.softmax(\n\t            logits, dim=1) * nn.functional.log_softmax(logits, dim=1)\n\t        ent = -1.0 * p_log_p.sum(dim=1)  # b x h x w OR b x n\n\t        loss = ent / math.log(dim)\n\t        if self.reduction == 'none':\n\t            return loss\n\t        elif self.reduction == 'mean':\n\t            return loss.mean()\n", "        elif self.reduction == 'sum':\n\t            return loss.sum()\n\tclass CDCLoss(nn.Module):\n\t    def __init__(self,\n\t                 feat_dim: int = 128,\n\t                 temperature: float = 0.3,\n\t                 num_grid: int = 7,\n\t                 queue_len: int = 65536,\n\t                 warm_up_steps: int = 2500,\n\t                 confidence_threshold: float = 0.2):\n", "        super().__init__()\n\t        self.feat_dim = feat_dim\n\t        self.temperature = temperature\n\t        self.num_grid = num_grid\n\t        self.queue_len = queue_len\n\t        self.warm_up_steps = warm_up_steps\n\t        self.confidence_threshold = confidence_threshold\n\t        self.register_buffer(\"queue\", torch.randn(feat_dim, queue_len))\n\t        self.queue = nn.functional.normalize(self.queue, p=2, dim=0)\n\t        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n", "    def forward(self, emb_anc, emb_pos, confidence):\n\t        # since the features are normalized afterwards, we don't need to worry about the\n\t        # normalization factor during pooling (for a weighted average)\n\t        emb_anc = emb_anc * confidence\n\t        emb_anc = nn.functional.adaptive_avg_pool2d(emb_anc, self.num_grid).permute(0, 2, 3, 1).contiguous().view(-1, self.feat_dim)\n\t        emb_anc_zero_mask = torch.linalg.norm(emb_anc, dim=1) != 0\n\t        emb_pos = emb_pos * confidence\n\t        emb_pos = nn.functional.adaptive_avg_pool2d(emb_pos, self.num_grid).permute(0, 2, 3, 1).contiguous().view(-1, self.feat_dim)\n\t        emb_pos_zero_mask = torch.linalg.norm(emb_pos, dim=1) != 0\n\t        avg_confidence = nn.functional.adaptive_avg_pool2d(confidence, self.num_grid).view(-1)\n", "        avg_confidence_mask = avg_confidence >= self.confidence_threshold\n\t        zero_mask = emb_anc_zero_mask & emb_pos_zero_mask & avg_confidence_mask\n\t        emb_anc = emb_anc[zero_mask]\n\t        emb_pos = emb_pos[zero_mask]\n\t        emb_anc = nn.functional.normalize(emb_anc, p=2, dim=1)\n\t        emb_pos = nn.functional.normalize(emb_pos, p=2, dim=1)\n\t        l_pos_dense = torch.einsum('nc,nc->n', [emb_anc, emb_pos]).unsqueeze(-1)\n\t        l_neg_dense = torch.einsum('nc,ck->nk', [emb_anc, self.queue.clone().detach()])\n\t        logits = torch.cat((l_pos_dense, l_neg_dense), dim=1) / self.temperature\n\t        labels = torch.zeros((logits.size(0), ), dtype=torch.long, device=logits.device)\n", "        loss = nn.functional.cross_entropy(logits, labels, reduction='mean')\n\t        return loss\n\t    @torch.no_grad()\n\t    def update_queue(self, emb_neg):\n\t        emb_neg = nn.functional.adaptive_avg_pool2d(emb_neg, self.num_grid).permute(0, 2, 3, 1).contiguous().view(-1, self.feat_dim)\n\t        emb_neg = nn.functional.normalize(emb_neg, p=2, dim=1)\n\t        batch_size = emb_neg.shape[0]\n\t        ptr = int(self.queue_ptr)\n\t        if ptr + batch_size > self.queue_len:\n\t            sec1, sec2 = self.queue_len - \\\n", "                ptr, ptr + batch_size - self.queue_len\n\t            emb_neg1, emb_neg2 = torch.split(emb_neg, [sec1, sec2], dim=0)\n\t            self.queue[:, -sec1:] = emb_neg1.transpose(0, 1)\n\t            self.queue[:, :sec2] = emb_neg2.transpose(0, 1)\n\t        else:\n\t            self.queue[:, ptr:ptr + batch_size] = emb_neg.transpose(0, 1)\n\t        ptr = (ptr + batch_size) % self.queue_len  # move pointer\n\t        self.queue_ptr[0] = ptr\n"]}
{"filename": "models/__init__.py", "chunked_list": ["from .model import CMAModel\n"]}
{"filename": "models/utils.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom PIL import Image\n\tPALETTE = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n\t           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n\t           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n\tfor i in range(256 * 3 - len(PALETTE)):\n\t    PALETTE.append(0)\n\tdef colorize_mask(mask):\n\t    assert isinstance(mask, Image.Image)\n", "    new_mask = mask.convert('P')\n\t    new_mask.putpalette(PALETTE)\n\t    return new_mask\n\tdef warp(x, flo, padding_mode='zeros', return_mask=False):\n\t    \"\"\"\n\t    ---------------------------------------------------------------------------\n\t    Copyright (c) Prune Truong. All rights reserved.\n\t    This source code is licensed under the license found in the\n\t    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n\t    ---------------------------------------------------------------------------\n", "    warp an image/tensor (im2) back to im1, according to the optical flow\n\t    Args:\n\t        x: [B, C, H, W] (im2)\n\t        flo: [B, 2, H, W] flow\n\t    \"\"\"\n\t    B, C, H, W = x.size()\n\t    if torch.all(flo == 0):\n\t        if return_mask:\n\t            return x, torch.ones((B, H, W), dtype=torch.bool, device=x.device)\n\t        return x\n", "    # mesh grid\n\t    xx = torch.arange(0, W, dtype=flo.dtype,\n\t                      device=flo.device).view(1, -1).repeat(H, 1)\n\t    yy = torch.arange(0, H, dtype=flo.dtype,\n\t                      device=flo.device).view(-1, 1).repeat(1, W)\n\t    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n\t    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n\t    grid = torch.cat((xx, yy), 1)\n\t    vgrid = grid + flo\n\t    # scale grid to [-1,1]\n", "    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :] / float(max(W-1, 1)) - 1.0\n\t    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :] / float(max(H-1, 1)) - 1.0\n\t    vgrid = vgrid.permute(0, 2, 3, 1)\n\t    output = nn.functional.grid_sample(\n\t        x, vgrid, align_corners=True, padding_mode=padding_mode)\n\t    if return_mask:\n\t        vgrid = vgrid.detach().clone().permute(0, 3, 1, 2)\n\t        mask = (vgrid[:, 0] > -1) & (vgrid[:, 1] > -\n\t                                     1) & (vgrid[:, 0] < 1) & (vgrid[:, 1] < 1)\n\t        return output, mask\n", "    return output\n\tdef estimate_probability_of_confidence_interval(uncert_output, R=1.0):\n\t    assert uncert_output.shape[1] == 1\n\t    var = torch.exp(uncert_output)\n\t    p_r = 1.0 - torch.exp(-R ** 2 / (2 * var))\n\t    return p_r\n"]}
{"filename": "models/backbones/mix_transformer.py", "chunked_list": ["# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in https://github.com/NVlabs/SegFormer.\n\t#\n\timport collections.abc\n\timport math\n\timport os\n\timport warnings\n\tfrom functools import partial\n", "from itertools import repeat\n\tfrom typing import Optional\n\timport torch\n\timport torch.nn as nn\n\tmodel_urls = {\n\t    \"imagenet\": {\n\t        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n\t        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/mit_b5.pth\",\n\t    },\n\t    \"cityscapes\": {\n", "        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n\t        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/segformer.b5.1024x1024.city.160k.pth\",\n\t    }\n\t}\n\tdef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n\t    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n\t    def norm_cdf(x):\n\t        # Computes standard normal cumulative distribution function\n\t        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n", "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n\t        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n\t                      \"The distribution of values may be incorrect.\",\n\t                      stacklevel=2)\n\t    with torch.no_grad():\n\t        # Values are generated by using a truncated uniform distribution and\n\t        # then using the inverse CDF for the normal distribution.\n\t        # Get upper and lower cdf values\n\t        l = norm_cdf((a - mean) / std)\n\t        u = norm_cdf((b - mean) / std)\n", "        # Uniformly fill tensor with values from [l, u], then translate to\n\t        # [2l-1, 2u-1].\n\t        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\t        # Use inverse cdf transform for normal distribution to get truncated\n\t        # standard normal\n\t        tensor.erfinv_()\n\t        # Transform to proper mean, std\n\t        tensor.mul_(std * math.sqrt(2.))\n\t        tensor.add_(mean)\n\t        # Clamp to ensure it's in the proper range\n", "        tensor.clamp_(min=a, max=b)\n\t        return tensor\n\t# From PyTorch internals\n\tdef _ntuple(n):\n\t    def parse(x):\n\t        if isinstance(x, collections.abc.Iterable):\n\t            return x\n\t        return tuple(repeat(x, n))\n\t    return parse\n\tto_2tuple = _ntuple(2)\n", "class Mlp(nn.Module):\n\t    def __init__(self,\n\t                 in_features,\n\t                 hidden_features=None,\n\t                 out_features=None,\n\t                 act_layer=nn.GELU,\n\t                 drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n", "        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.dwconv = DWConv(hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t    def forward(self, x, H, W):\n\t        x = self.fc1(x)\n\t        x = self.dwconv(x, H, W)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n", "        x = self.fc2(x)\n\t        x = self.drop(x)\n\t        return x\n\tclass Attention(nn.Module):\n\t    def __init__(self,\n\t                 dim,\n\t                 num_heads=8,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n\t                 attn_drop=0.,\n", "                 proj_drop=0.,\n\t                 sr_ratio=1):\n\t        super().__init__()\n\t        assert dim % num_heads == 0, f'dim {dim} should be divided by ' \\\n\t                                     f'num_heads {num_heads}.'\n\t        self.dim = dim\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n\t        self.scale = qk_scale or head_dim**-0.5\n\t        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n", "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t        self.sr_ratio = sr_ratio\n\t        if sr_ratio > 1:\n\t            self.sr = nn.Conv2d(\n\t                dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n\t            self.norm = nn.LayerNorm(dim)\n\t    def forward(self, x, H, W):\n", "        B, N, C = x.shape\n\t        q = self.q(x).reshape(B, N, self.num_heads,\n\t                              C // self.num_heads).permute(0, 2, 1,\n\t                                                           3).contiguous()\n\t        if self.sr_ratio > 1:\n\t            x_ = x.permute(0, 2, 1).contiguous().reshape(B, C, H, W)\n\t            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1).contiguous()\n\t            x_ = self.norm(x_)\n\t            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads,\n\t                                     C // self.num_heads).permute(\n", "                                         2, 0, 3, 1, 4).contiguous()\n\t        else:\n\t            kv = self.kv(x).reshape(B, -1, 2, self.num_heads,\n\t                                    C // self.num_heads).permute(\n\t                                        2, 0, 3, 1, 4).contiguous()\n\t        k, v = kv[0], kv[1]\n\t        attn = (q @ k.transpose(-2, -1).contiguous()) * self.scale\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).contiguous().reshape(B, N, C)\n", "        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n\tclass Block(nn.Module):\n\t    def __init__(self,\n\t                 dim,\n\t                 num_heads,\n\t                 mlp_ratio=4.,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n", "                 drop=0.,\n\t                 attn_drop=0.,\n\t                 drop_path=0.,\n\t                 act_layer=nn.GELU,\n\t                 norm_layer=nn.LayerNorm,\n\t                 sr_ratio=1):\n\t        super().__init__()\n\t        self.norm1 = norm_layer(dim)\n\t        self.attn = Attention(\n\t            dim,\n", "            num_heads=num_heads,\n\t            qkv_bias=qkv_bias,\n\t            qk_scale=qk_scale,\n\t            attn_drop=attn_drop,\n\t            proj_drop=drop,\n\t            sr_ratio=sr_ratio)\n\t        # NOTE: drop path for stochastic depth, we shall see if this is better\n\t        # than dropout here\n\t        self.drop_path = DropPath(\n\t            drop_path) if drop_path > 0. else nn.Identity()\n", "        self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(\n\t            in_features=dim,\n\t            hidden_features=mlp_hidden_dim,\n\t            act_layer=act_layer,\n\t            drop=drop)\n\t    def forward(self, x, H, W):\n\t        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n\t        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n", "        return x\n\tclass OverlapPatchEmbed(nn.Module):\n\t    \"\"\"Image to Patch Embedding.\"\"\"\n\t    def __init__(self,\n\t                 img_size=224,\n\t                 patch_size=7,\n\t                 stride=4,\n\t                 in_chans=3,\n\t                 embed_dim=768):\n\t        super().__init__()\n", "        img_size = to_2tuple(img_size)\n\t        patch_size = to_2tuple(patch_size)\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n\t        self.H, self.W = img_size[0] // patch_size[0], img_size[\n\t            1] // patch_size[1]\n\t        self.num_patches = self.H * self.W\n\t        self.proj = nn.Conv2d(\n\t            in_chans,\n\t            embed_dim,\n", "            kernel_size=patch_size,\n\t            stride=stride,\n\t            padding=(patch_size[0] // 2, patch_size[1] // 2))\n\t        self.norm = nn.LayerNorm(embed_dim)\n\t    def forward(self, x):\n\t        x = self.proj(x)\n\t        _, _, H, W = x.shape\n\t        x = x.flatten(2).transpose(1, 2).contiguous()\n\t        x = self.norm(x)\n\t        return x, H, W\n", "class DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\t    \"\"\"\n\t    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n\t        self.scale_by_keep = scale_by_keep\n\t    def forward(self, x):\n\t        return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\t    @staticmethod\n", "    def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n\t        \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\t        This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n\t        the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n\t        See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n\t        changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n\t        'survival rate' as the argument.\n\t        \"\"\"\n\t        if drop_prob == 0. or not training:\n\t            return x\n", "        keep_prob = 1 - drop_prob\n\t        # work with diff dim tensors, not just 2D ConvNets\n\t        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n\t        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n\t        if keep_prob > 0.0 and scale_by_keep:\n\t            random_tensor.div_(keep_prob)\n\t        return x * random_tensor\n\tclass MixVisionTransformer(nn.Module):\n\t    arch_settings = {\n\t        'mit_b0': {\n", "            'patch_size': 4,\n\t            'embed_dims': [32, 64, 160, 256],\n\t            'num_heads': [1, 2, 5, 8],\n\t            'mlp_ratios': [4, 4, 4, 4],\n\t            'qkv_bias': True,\n\t            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n\t            'depths': [2, 2, 2, 2],\n\t            'sr_ratios': [8, 4, 2, 1],\n\t        },\n\t        'mit_b1': {\n", "            'patch_size': 4,\n\t            'embed_dims': [64, 128, 320, 512],\n\t            'num_heads': [1, 2, 5, 8],\n\t            'mlp_ratios': [4, 4, 4, 4],\n\t            'qkv_bias': True,\n\t            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n\t            'depths': [2, 2, 2, 2],\n\t            'sr_ratios': [8, 4, 2, 1],\n\t        },\n\t        'mit_b2': {\n", "            'patch_size': 4,\n\t            'embed_dims': [64, 128, 320, 512],\n\t            'num_heads': [1, 2, 5, 8],\n\t            'mlp_ratios': [4, 4, 4, 4],\n\t            'qkv_bias': True,\n\t            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n\t            'depths': [3, 4, 6, 3],\n\t            'sr_ratios': [8, 4, 2, 1],\n\t        },\n\t        'mit_b3': {\n", "            'patch_size': 4,\n\t            'embed_dims': [64, 128, 320, 512],\n\t            'num_heads': [1, 2, 5, 8],\n\t            'mlp_ratios': [4, 4, 4, 4],\n\t            'qkv_bias': True,\n\t            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n\t            'depths': [3, 4, 18, 3],\n\t            'sr_ratios': [8, 4, 2, 1],\n\t        },\n\t        'mit_b4': {\n", "            'patch_size': 4,\n\t            'embed_dims': [64, 128, 320, 512],\n\t            'num_heads': [1, 2, 5, 8],\n\t            'mlp_ratios': [4, 4, 4, 4],\n\t            'qkv_bias': True,\n\t            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n\t            'depths': [3, 8, 27, 3],\n\t            'sr_ratios': [8, 4, 2, 1],\n\t        },\n\t        'mit_b5': {\n", "            'patch_size': 4,\n\t            'embed_dims': [64, 128, 320, 512],\n\t            'num_heads': [1, 2, 5, 8],\n\t            'mlp_ratios': [4, 4, 4, 4],\n\t            'qkv_bias': True,\n\t            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n\t            'depths': [3, 6, 40, 3],\n\t            'sr_ratios': [8, 4, 2, 1],\n\t        }\n\t    }\n", "    def __init__(self,\n\t                 model_type: str,\n\t                 pretrained: Optional[str] = None,\n\t                 img_size: int = 224,\n\t                 in_chans: int = 3,\n\t                 qk_scale: Optional[float] = None,\n\t                 drop_rate: float = 0.,\n\t                 attn_drop_rate: float = 0.,\n\t                 drop_path_rate: float = 0.1,\n\t                 freeze_patch_embed: bool = False):\n", "        super().__init__()\n\t        embed_dims = self.arch_settings[model_type]['embed_dims']\n\t        num_heads = self.arch_settings[model_type]['num_heads']\n\t        mlp_ratios = self.arch_settings[model_type]['mlp_ratios']\n\t        qkv_bias = self.arch_settings[model_type]['qkv_bias']\n\t        norm_layer = self.arch_settings[model_type]['norm_layer']\n\t        depths = self.arch_settings[model_type]['depths']\n\t        sr_ratios = self.arch_settings[model_type]['sr_ratios']\n\t        self.model_type = model_type\n\t        self.depths = depths\n", "        # patch_embed\n\t        self.patch_embed1 = OverlapPatchEmbed(\n\t            img_size=img_size,\n\t            patch_size=7,\n\t            stride=4,\n\t            in_chans=in_chans,\n\t            embed_dim=embed_dims[0])\n\t        self.patch_embed2 = OverlapPatchEmbed(\n\t            img_size=img_size // 4,\n\t            patch_size=3,\n", "            stride=2,\n\t            in_chans=embed_dims[0],\n\t            embed_dim=embed_dims[1])\n\t        self.patch_embed3 = OverlapPatchEmbed(\n\t            img_size=img_size // 8,\n\t            patch_size=3,\n\t            stride=2,\n\t            in_chans=embed_dims[1],\n\t            embed_dim=embed_dims[2])\n\t        self.patch_embed4 = OverlapPatchEmbed(\n", "            img_size=img_size // 16,\n\t            patch_size=3,\n\t            stride=2,\n\t            in_chans=embed_dims[2],\n\t            embed_dim=embed_dims[3])\n\t        if freeze_patch_embed:\n\t            self.freeze_patch_emb()\n\t        # transformer encoder\n\t        dpr = [\n\t            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n", "        ]  # stochastic depth decay rule\n\t        cur = 0\n\t        self.block1 = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dims[0],\n\t                num_heads=num_heads[0],\n\t                mlp_ratio=mlp_ratios[0],\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n", "                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[cur + i],\n\t                norm_layer=norm_layer,\n\t                sr_ratio=sr_ratios[0]) for i in range(depths[0])\n\t        ])\n\t        self.norm1 = norm_layer(embed_dims[0])\n\t        cur += depths[0]\n\t        self.block2 = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dims[1],\n", "                num_heads=num_heads[1],\n\t                mlp_ratio=mlp_ratios[1],\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n\t                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[cur + i],\n\t                norm_layer=norm_layer,\n\t                sr_ratio=sr_ratios[1]) for i in range(depths[1])\n\t        ])\n", "        self.norm2 = norm_layer(embed_dims[1])\n\t        cur += depths[1]\n\t        self.block3 = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dims[2],\n\t                num_heads=num_heads[2],\n\t                mlp_ratio=mlp_ratios[2],\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n", "                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[cur + i],\n\t                norm_layer=norm_layer,\n\t                sr_ratio=sr_ratios[2]) for i in range(depths[2])\n\t        ])\n\t        self.norm3 = norm_layer(embed_dims[2])\n\t        cur += depths[2]\n\t        self.block4 = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dims[3],\n", "                num_heads=num_heads[3],\n\t                mlp_ratio=mlp_ratios[3],\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n\t                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[cur + i],\n\t                norm_layer=norm_layer,\n\t                sr_ratio=sr_ratios[3]) for i in range(depths[3])\n\t        ])\n", "        self.norm4 = norm_layer(embed_dims[3])\n\t        self.init_weights(pretrained=pretrained)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n", "        elif isinstance(m, nn.Conv2d):\n\t            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n\t            fan_out //= m.groups\n\t            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n\t            if m.bias is not None:\n\t                m.bias.data.zero_()\n\t    def init_weights(self, pretrained=None):\n\t        if pretrained is None:\n\t            for m in self.modules():\n\t                self._init_weights(m)\n", "        else:\n\t            if pretrained == 'imagenet':\n\t                pretrained = model_urls['imagenet'][self.model_type]\n\t            elif pretrained == 'cityscapes':\n\t                pretrained = model_urls['cityscapes'][self.model_type]\n\t            if os.path.exists(pretrained):\n\t                checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n\t            elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n\t                checkpoint = torch.load(os.path.join(os.environ.get(\n\t                    'TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n", "            else:\n\t                checkpoint = torch.hub.load_state_dict_from_url(\n\t                    pretrained, progress=True, map_location=lambda storage, loc: storage)\n\t            if 'state_dict' in checkpoint:\n\t                state_dict = checkpoint['state_dict']\n\t            else:\n\t                state_dict = checkpoint\n\t            if any(el.startswith('backbone.') for el in state_dict.keys()):\n\t                new_state_dict = {}\n\t                for k, v in state_dict.items():\n", "                    if k.startswith('backbone.'):\n\t                        new_k = k.replace('backbone.', '')\n\t                        new_state_dict[new_k] = v\n\t                state_dict = new_state_dict\n\t            state_dict = {k: v for k, v in state_dict.items()\n\t                          if not k.startswith('head.')}\n\t            self.load_state_dict(state_dict, strict=True)\n\t    def reset_drop_path(self, drop_path_rate):\n\t        dpr = [\n\t            x.item()\n", "            for x in torch.linspace(0, drop_path_rate, sum(self.depths))\n\t        ]\n\t        cur = 0\n\t        for i in range(self.depths[0]):\n\t            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n\t        cur += self.depths[0]\n\t        for i in range(self.depths[1]):\n\t            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n\t        cur += self.depths[1]\n\t        for i in range(self.depths[2]):\n", "            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n\t        cur += self.depths[2]\n\t        for i in range(self.depths[3]):\n\t            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n\t    def freeze_patch_emb(self):\n\t        self.patch_embed1.requires_grad = False\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {\n\t            'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'\n", "        }  # has pos_embed may be better\n\t    def forward_features(self, x):\n\t        B = x.shape[0]\n\t        outs = []\n\t        # stage 1\n\t        x, H, W = self.patch_embed1(x)\n\t        for i, blk in enumerate(self.block1):\n\t            x = blk(x, H, W)\n\t        x = self.norm1(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n", "        outs.append(x)\n\t        # stage 2\n\t        x, H, W = self.patch_embed2(x)\n\t        for i, blk in enumerate(self.block2):\n\t            x = blk(x, H, W)\n\t        x = self.norm2(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n\t        # stage 3\n\t        x, H, W = self.patch_embed3(x)\n", "        for i, blk in enumerate(self.block3):\n\t            x = blk(x, H, W)\n\t        x = self.norm3(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n\t        # stage 4\n\t        x, H, W = self.patch_embed4(x)\n\t        for i, blk in enumerate(self.block4):\n\t            x = blk(x, H, W)\n\t        x = self.norm4(x)\n", "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n\t        return outs\n\t    def forward(self, x):\n\t        x = self.forward_features(x)\n\t        # x = self.head(x)\n\t        return x\n\tclass DWConv(nn.Module):\n\t    def __init__(self, dim=768):\n\t        super(DWConv, self).__init__()\n", "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\t    def forward(self, x, H, W):\n\t        B, N, C = x.shape\n\t        x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n\t        x = self.dwconv(x)\n\t        x = x.flatten(2).transpose(1, 2).contiguous()\n\t        return x\n"]}
{"filename": "models/backbones/__init__.py", "chunked_list": ["from .mix_transformer import MixVisionTransformer\n\tfrom .resnet import ResNet\n\tfrom .vgg import VGG\n"]}
{"filename": "models/backbones/vgg.py", "chunked_list": ["# Copyright (c) Prune Truong. All rights reserved.\n\t# \n\t# This source code is licensed under the license found in the\n\t# LICENSE file in https://github.com/PruneTruong/DenseMatching.\n\t#\n\timport os\n\tfrom typing import Dict, List, Optional, Union, cast\n\timport torch\n\timport torch.nn as nn\n\tmodel_urls = {\n", "    \"vgg11\": \"https://download.pytorch.org/models/vgg11-8a719046.pth\",\n\t    \"vgg13\": \"https://download.pytorch.org/models/vgg13-19584684.pth\",\n\t    \"vgg16\": \"https://download.pytorch.org/models/vgg16-397923af.pth\",\n\t    \"vgg19\": \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\",\n\t    \"vgg11_bn\": \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\",\n\t    \"vgg13_bn\": \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\",\n\t    \"vgg16_bn\": \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\",\n\t    \"vgg19_bn\": \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\",\n\t}\n\tcfgs: Dict[str, List[Union[str, int]]] = {\n", "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n\t    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n\t    \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n\t    \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n\t}\n\tclass VGG(nn.Module):\n\t    arch_settings = {\n\t        'vgg11': {\n\t            'cfg': 'A',\n\t            'batch_norm': False,\n", "        },\n\t        'vgg11_bn': {\n\t            'cfg': 'A',\n\t            'batch_norm': True,\n\t        },\n\t        'vgg13': {\n\t            'cfg': 'B',\n\t            'batch_norm': False,\n\t        },\n\t        'vgg13_bn': {\n", "            'cfg': 'B',\n\t            'batch_norm': True,\n\t        },\n\t        'vgg16': {\n\t            'cfg': 'D',\n\t            'batch_norm': False,\n\t        },\n\t        'vgg16_bn': {\n\t            'cfg': 'D',\n\t            'batch_norm': True,\n", "        },\n\t        'vgg19': {\n\t            'cfg': 'E',\n\t            'batch_norm': False,\n\t        },\n\t        'vgg19_bn': {\n\t            'cfg': 'E',\n\t            'batch_norm': True,\n\t        }\n\t    }\n", "    def __init__(\n\t        self, model_type: str, out_indices: list = [0, 1, 2, 3, 4, 5], pretrained: Optional[str] = None\n\t    ) -> None:\n\t        super().__init__()\n\t        self.model_type = model_type\n\t        cfg = self.arch_settings[model_type]['cfg']\n\t        batch_norm = self.arch_settings[model_type]['batch_norm']\n\t        self.features, layer_indices = self._make_layers(\n\t            cfgs[cfg], batch_norm=batch_norm)\n\t        self.layer_indices = [layer_indices[i] for i in out_indices]\n", "        self.init_weights(pretrained=pretrained)\n\t    def init_weights(self, pretrained=None):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(\n\t                    m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n\t                if m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n", "                nn.init.constant_(m.bias, 0)\n\t        if pretrained is not None:\n\t            if pretrained == 'imagenet':\n\t                pretrained = model_urls[self.model_type]\n\t            if os.path.exists(pretrained):\n\t                checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n\t            else:\n\t                checkpoint = torch.hub.load_state_dict_from_url(\n\t                    pretrained, progress=True, map_location=lambda storage, loc: storage)\n\t            if 'state_dict' in checkpoint.keys():\n", "                state_dict = checkpoint['state_dict']\n\t            else:\n\t                state_dict = checkpoint\n\t            state_dict = {k: v for k, v in state_dict.items(\n\t            ) if not k.startswith('classifier.')}\n\t            self.load_state_dict(state_dict, strict=True)\n\t    def forward(self, x: torch.Tensor, extract_only_indices=None) -> torch.Tensor:\n\t        if extract_only_indices:\n\t            layer_indices = [self.layer_indices[i]\n\t                             for i in extract_only_indices]\n", "        else:\n\t            layer_indices = self.layer_indices\n\t        prev_i = 0\n\t        outs = []\n\t        for i in layer_indices:\n\t            x = self.features[prev_i:i](x)\n\t            outs.append(x)\n\t            prev_i = i\n\t        return outs\n\t    @staticmethod\n", "    def _make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n\t        layers: List[nn.Module] = []\n\t        in_channels = 3\n\t        current_idx = 0\n\t        layer_indices = []\n\t        first_relu = True\n\t        for v in cfg:\n\t            if v == \"M\":\n\t                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n\t                current_idx += 1\n", "                layer_indices.append(current_idx)\n\t            else:\n\t                v = cast(int, v)\n\t                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n\t                if batch_norm:\n\t                    layers += [conv2d,\n\t                               nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n\t                    current_idx += 3\n\t                else:\n\t                    layers += [conv2d, nn.ReLU(inplace=True)]\n", "                    current_idx += 2\n\t                in_channels = v\n\t                if first_relu:\n\t                    first_relu = False\n\t                    layer_indices.append(current_idx)\n\t        return nn.Sequential(*layers), layer_indices\n"]}
{"filename": "models/backbones/resnet.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\n\t# \n\t# This source code is licensed under the license found in the\n\t# LICENSE file in https://github.com/open-mmlab/mmsegmentation.\n\t#\n\timport os\n\tfrom typing import Optional\n\timport torch\n\timport torch.nn as nn\n\timport torch.utils.checkpoint as cp\n", "model_urls = {\n\t    \"imagenet\": {\n\t        \"resnet18_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet18_v1c-b5776b93.pth\",\n\t        \"resnet50_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet50_v1c-2cccc1ad.pth\",\n\t        \"resnet101_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet101_v1c-e67eebb6.pth\",\n\t    },\n\t    \"cityscapes\": {\n\t        \"resnet101_v1c\": \"https://data.vision.ee.ethz.ch/brdavid/cma/deeplabv2_cityscapes.pth\"\n\t    }\n\t}\n", "class ResLayer(nn.Sequential):\n\t    \"\"\"ResLayer to build ResNet style backbone.\n\t    Args:\n\t        block (nn.Module): Residual block used to build ResLayer.\n\t        num_blocks (int): Number of blocks.\n\t        in_channels (int): Input channels of this block.\n\t        out_channels (int): Output channels of this block.\n\t        expansion (int, optional): The expansion for BasicBlock/Bottleneck.\n\t            If not specified, it will firstly be obtained via\n\t            ``block.expansion``. If the block has no attribute \"expansion\",\n", "            the following default values will be used: 1 for BasicBlock and\n\t            4 for Bottleneck. Default: None.\n\t        stride (int): stride of the first block. Default: 1.\n\t        avg_down (bool): Use AvgPool instead of stride conv when\n\t            downsampling in the bottleneck. Default: False\n\t        conv_cfg (dict, optional): dictionary to construct and config conv\n\t            layer. Default: None\n\t        norm_cfg (dict): dictionary to construct and config norm layer.\n\t            Default: dict(type='BN')\n\t    \"\"\"\n", "    def __init__(self,\n\t                 block,\n\t                 num_blocks,\n\t                 in_channels,\n\t                 out_channels,\n\t                 stride=1,\n\t                 dilation=1,\n\t                 avg_down=False,\n\t                 norm_layer=nn.BatchNorm2d,\n\t                 contract_dilation=False,\n", "                 **kwargs):\n\t        self.block = block\n\t        downsample = None\n\t        if stride != 1 or in_channels != out_channels * block.expansion:\n\t            downsample = []\n\t            conv_stride = stride\n\t            if avg_down and stride != 1:\n\t                conv_stride = 1\n\t                downsample.append(\n\t                    nn.AvgPool2d(\n", "                        kernel_size=stride,\n\t                        stride=stride,\n\t                        ceil_mode=True,\n\t                        count_include_pad=False))\n\t            downsample.extend([\n\t                nn.Conv2d(\n\t                    in_channels,\n\t                    out_channels * block.expansion,\n\t                    kernel_size=1,\n\t                    stride=conv_stride,\n", "                    bias=False),\n\t                norm_layer(out_channels * block.expansion)\n\t            ])\n\t            downsample = nn.Sequential(*downsample)\n\t        layers = []\n\t        if dilation > 1 and contract_dilation:\n\t            first_dilation = dilation // 2\n\t        else:\n\t            first_dilation = dilation\n\t        layers.append(\n", "            block(\n\t                inplanes=in_channels,\n\t                planes=out_channels,\n\t                stride=stride,\n\t                dilation=first_dilation,\n\t                downsample=downsample,\n\t                norm_layer=norm_layer,\n\t                **kwargs))\n\t        in_channels = out_channels * block.expansion\n\t        for _ in range(1, num_blocks):\n", "            layers.append(\n\t                block(\n\t                    inplanes=in_channels,\n\t                    planes=out_channels,\n\t                    stride=1,\n\t                    dilation=dilation,\n\t                    norm_layer=norm_layer,\n\t                    **kwargs))\n\t        super(ResLayer, self).__init__(*layers)\n\tclass BasicBlock(nn.Module):\n", "    \"\"\"BasicBlock for ResNet.\n\t    Args:\n\t        in_channels (int): Input channels of this block.\n\t        out_channels (int): Output channels of this block.\n\t        expansion (int): The ratio of ``out_channels/mid_channels`` where\n\t            ``mid_channels`` is the output channels of conv1. This is a\n\t            reserved argument in BasicBlock and should always be 1. Default: 1.\n\t        stride (int): stride of the block. Default: 1\n\t        dilation (int): dilation of convolution. Default: 1\n\t        downsample (nn.Module, optional): downsample operation on identity\n", "            branch. Default: None.\n\t        style (str): `pytorch` or `caffe`. It is unused and reserved for\n\t            unified API with Bottleneck.\n\t        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n\t            memory while slowing down the training speed.\n\t        conv_cfg (dict, optional): dictionary to construct and config conv\n\t            layer. Default: None\n\t        norm_cfg (dict): dictionary to construct and config norm layer.\n\t            Default: dict(type='BN')\n\t    \"\"\"\n", "    expansion = 1\n\t    def __init__(self,\n\t                 inplanes,\n\t                 planes,\n\t                 stride=1,\n\t                 dilation=1,\n\t                 downsample=None,\n\t                 style='pytorch',\n\t                 with_cp=False,\n\t                 norm_layer=nn.BatchNorm2d,\n", "                 **kwargs):\n\t        super(BasicBlock, self).__init__()\n\t        self.conv1 = nn.Conv2d(\n\t            inplanes,\n\t            planes,\n\t            3,\n\t            stride=stride,\n\t            padding=dilation,\n\t            dilation=dilation,\n\t            bias=False)\n", "        self.bn1 = norm_layer(planes)\n\t        self.conv2 = nn.Conv2d(\n\t            planes,\n\t            planes,\n\t            3,\n\t            padding=1,\n\t            bias=False)\n\t        self.bn2 = norm_layer(planes)\n\t        self.relu = nn.ReLU(inplace=True)\n\t        self.out_channels = planes\n", "        self.downsample = downsample\n\t        self.stride = stride\n\t        self.dilation = dilation\n\t        self.with_cp = with_cp\n\t    def forward(self, x):\n\t        def _inner_forward(x):\n\t            identity = x\n\t            out = self.conv1(x)\n\t            out = self.bn1(out)\n\t            out = self.relu(out)\n", "            out = self.conv2(out)\n\t            out = self.bn2(out)\n\t            if self.downsample is not None:\n\t                identity = self.downsample(x)\n\t            out += identity\n\t            return out\n\t        if self.with_cp and x.requires_grad:\n\t            out = cp.checkpoint(_inner_forward, x)\n\t        else:\n\t            out = _inner_forward(x)\n", "        out = self.relu(out)\n\t        return out\n\tclass Bottleneck(nn.Module):\n\t    \"\"\"Bottleneck block for ResNet.\n\t    Args:\n\t        in_channels (int): Input channels of this block.\n\t        out_channels (int): Output channels of this block.\n\t        expansion (int): The ratio of ``out_channels/mid_channels`` where\n\t            ``mid_channels`` is the input/output channels of conv2. Default: 4.\n\t        stride (int): stride of the block. Default: 1\n", "        dilation (int): dilation of convolution. Default: 1\n\t        downsample (nn.Module, optional): downsample operation on identity\n\t            branch. Default: None.\n\t        style (str): ``\"pytorch\"`` or ``\"caffe\"``. If set to \"pytorch\", the\n\t            stride-two layer is the 3x3 conv layer, otherwise the stride-two\n\t            layer is the first 1x1 conv layer. Default: \"pytorch\".\n\t        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n\t            memory while slowing down the training speed.\n\t        conv_cfg (dict, optional): dictionary to construct and config conv\n\t            layer. Default: None\n", "        norm_cfg (dict): dictionary to construct and config norm layer.\n\t            Default: dict(type='BN')\n\t    \"\"\"\n\t    expansion = 4\n\t    def __init__(self,\n\t                 inplanes,\n\t                 planes,\n\t                 stride=1,\n\t                 dilation=1,\n\t                 downsample=None,\n", "                 style='pytorch',\n\t                 with_cp=False,\n\t                 norm_layer=nn.BatchNorm2d,\n\t                 **kwargs):\n\t        super(Bottleneck, self).__init__()\n\t        assert style in ['pytorch', 'caffe']\n\t        self.stride = stride\n\t        self.dilation = dilation\n\t        self.style = style\n\t        self.with_cp = with_cp\n", "        self.out_channels = planes * self.expansion\n\t        if self.style == 'pytorch':\n\t            self.conv1_stride = 1\n\t            self.conv2_stride = stride\n\t        else:\n\t            self.conv1_stride = stride\n\t            self.conv2_stride = 1\n\t        self.bn1 = norm_layer(planes)\n\t        self.bn2 = norm_layer(planes)\n\t        self.bn3 = norm_layer(planes * self.expansion)\n", "        self.conv1 = nn.Conv2d(\n\t            inplanes,\n\t            planes,\n\t            kernel_size=1,\n\t            stride=self.conv1_stride,\n\t            bias=False)\n\t        self.conv2 = nn.Conv2d(\n\t            planes,\n\t            planes,\n\t            kernel_size=3,\n", "            stride=self.conv2_stride,\n\t            padding=dilation,\n\t            dilation=dilation,\n\t            bias=False)\n\t        self.conv3 = nn.Conv2d(\n\t            planes,\n\t            planes * self.expansion,\n\t            kernel_size=1,\n\t            bias=False)\n\t        self.relu = nn.ReLU(inplace=True)\n", "        self.downsample = downsample\n\t    def forward(self, x):\n\t        def _inner_forward(x):\n\t            identity = x\n\t            out = self.conv1(x)\n\t            out = self.bn1(out)\n\t            out = self.relu(out)\n\t            out = self.conv2(out)\n\t            out = self.bn2(out)\n\t            out = self.relu(out)\n", "            out = self.conv3(out)\n\t            out = self.bn3(out)\n\t            if self.downsample is not None:\n\t                identity = self.downsample(x)\n\t            out += identity\n\t            return out\n\t        if self.with_cp and x.requires_grad:\n\t            out = cp.checkpoint(_inner_forward, x)\n\t        else:\n\t            out = _inner_forward(x)\n", "        out = self.relu(out)\n\t        return out\n\tclass ResNet(nn.Module):\n\t    \"\"\"ResNet backbone.\n\t    Please refer to the `paper <https://arxiv.org/abs/1512.03385>`__ for\n\t    details.\n\t    Args:\n\t        depth (int): Network depth, from {18, 34, 50, 101, 152}.\n\t        in_channels (int): Number of input image channels. Default: 3.\n\t        stem_channels (int): Output channels of the stem layer. Default: 64.\n", "        base_channels (int): Middle channels of the first stage. Default: 64.\n\t        num_stages (int): Stages of the network. Default: 4.\n\t        strides (Sequence[int]): Strides of the first block of each stage.\n\t            Default: ``(1, 2, 2, 2)``.\n\t        dilations (Sequence[int]): Dilation of each stage.\n\t            Default: ``(1, 1, 1, 1)``.\n\t        out_indices (Sequence[int]): Output from which stages. If only one\n\t            stage is specified, a single tensor (feature map) is returned,\n\t            otherwise multiple stages are specified, a tuple of tensors will\n\t            be returned. Default: ``(3, )``.\n", "        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n\t            layer is the 3x3 conv layer, otherwise the stride-two layer is\n\t            the first 1x1 conv layer.\n\t        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv.\n\t            Default: False.\n\t        avg_down (bool): Use AvgPool instead of stride conv when\n\t            downsampling in the bottleneck. Default: False.\n\t        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n\t            -1 means not freezing any parameters. Default: -1.\n\t        conv_cfg (dict | None): The config dict for conv layers. Default: None.\n", "        norm_cfg (dict): The config dict for norm layers.\n\t        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n\t            freeze running stats (mean and var). Note: Effect on Batch Norm\n\t            and its variants only. Default: False.\n\t        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n\t            memory while slowing down the training speed. Default: False.\n\t        zero_init_residual (bool): Whether to use zero init for last norm layer\n\t            in resblocks to let them behave as identity. Default: True.\n\t    Example:\n\t        >>> from mmcls.models import ResNet\n", "        >>> import torch\n\t        >>> self = ResNet(depth=18)\n\t        >>> self.eval()\n\t        >>> inputs = torch.rand(1, 3, 32, 32)\n\t        >>> level_outputs = self.forward(inputs)\n\t        >>> for level_out in level_outputs:\n\t        ...     print(tuple(level_out.shape))\n\t        (1, 64, 8, 8)\n\t        (1, 128, 4, 4)\n\t        (1, 256, 2, 2)\n", "        (1, 512, 1, 1)\n\t    \"\"\"\n\t    arch_settings = {\n\t        'resnet18_v1c': {\n\t            'depth': 18,\n\t            'block': BasicBlock,\n\t            'stage_blocks': (2, 2, 2, 2),\n\t            'deep_stem': True,\n\t            'avg_down': False,\n\t            'cifar_version': False\n", "        },\n\t        'resnet50_v1c': {\n\t            'depth': 50,\n\t            'block': Bottleneck,\n\t            'stage_blocks': (3, 4, 6, 3),\n\t            'deep_stem': True,\n\t            'avg_down': False,\n\t            'cifar_version': False\n\t        },\n\t        'resnet101_v1': {\n", "            'depth': 101,\n\t            'block': Bottleneck,\n\t            'stage_blocks': (3, 4, 23, 3),\n\t            'deep_stem': False,\n\t            'avg_down': False,\n\t            'cifar_version': False\n\t        },\n\t        'resnet101_v1c': {\n\t            'depth': 101,\n\t            'block': Bottleneck,\n", "            'stage_blocks': (3, 4, 23, 3),\n\t            'deep_stem': True,\n\t            'avg_down': False,\n\t            'cifar_version': False\n\t        },\n\t    }\n\t    def __init__(self,\n\t                 model_type: str,\n\t                 pretrained: Optional[str] = None,\n\t                 in_channels=3,\n", "                 stem_channels=64,\n\t                 base_channels=64,\n\t                 num_stages=4,\n\t                 strides=(1, 2, 2, 2),\n\t                 dilations=(1, 1, 1, 1),\n\t                 out_indices=(0, 1, 2, 3),\n\t                 style='pytorch',\n\t                 frozen_stages=-1,\n\t                 norm_eval=False,\n\t                 with_cp=False,\n", "                 zero_init_residual=True,\n\t                 contract_dilation=False,\n\t                 max_pool_ceil_mode=False):\n\t        norm_layer = nn.BatchNorm2d\n\t        super(ResNet, self).__init__()\n\t        self.model_type = model_type\n\t        self.depth = self.arch_settings[model_type]['depth']\n\t        self.stem_channels = stem_channels\n\t        self.base_channels = base_channels\n\t        self.num_stages = num_stages\n", "        assert num_stages >= 1 and num_stages <= 4\n\t        self.strides = strides\n\t        self.dilations = dilations\n\t        assert len(strides) == len(dilations) == num_stages\n\t        self.out_indices = out_indices\n\t        assert max(out_indices) < num_stages\n\t        self.style = style\n\t        self.deep_stem = self.arch_settings[model_type]['deep_stem']\n\t        self.avg_down = self.arch_settings[model_type]['avg_down']\n\t        self.frozen_stages = frozen_stages\n", "        self.with_cp = with_cp\n\t        self.norm_eval = norm_eval\n\t        self.zero_init_residual = zero_init_residual\n\t        self.block = self.arch_settings[model_type]['block']\n\t        stage_blocks = self.arch_settings[model_type]['stage_blocks']\n\t        self.stage_blocks = stage_blocks[:num_stages]\n\t        self.contract_dilation = contract_dilation\n\t        self.max_pool_ceil_mode = max_pool_ceil_mode\n\t        self._make_stem_layer(in_channels, stem_channels, norm_layer)\n\t        self.res_layers = []\n", "        _in_channels = stem_channels\n\t        for i, num_blocks in enumerate(self.stage_blocks):\n\t            stride = strides[i]\n\t            dilation = dilations[i]\n\t            _out_channels = base_channels * 2**i\n\t            res_layer = self.make_res_layer(\n\t                block=self.block,\n\t                num_blocks=num_blocks,\n\t                in_channels=_in_channels,\n\t                out_channels=_out_channels,\n", "                stride=stride,\n\t                dilation=dilation,\n\t                style=self.style,\n\t                avg_down=self.avg_down,\n\t                with_cp=with_cp,\n\t                norm_layer=norm_layer,\n\t                contract_dilation=contract_dilation)\n\t            _in_channels = _out_channels * self.block.expansion\n\t            layer_name = f'layer{i + 1}'\n\t            self.add_module(layer_name, res_layer)\n", "            self.res_layers.append(layer_name)\n\t        self._freeze_stages()\n\t        self.init_weights(pretrained=pretrained)\n\t    def make_res_layer(self, **kwargs):\n\t        return ResLayer(**kwargs)\n\t    def _make_stem_layer(self, in_channels, stem_channels, norm_layer):\n\t        if self.deep_stem:\n\t            self.stem = nn.Sequential(\n\t                nn.Conv2d(\n\t                    in_channels,\n", "                    stem_channels // 2,\n\t                    kernel_size=3,\n\t                    stride=2,\n\t                    padding=1,\n\t                    bias=False),\n\t                norm_layer(stem_channels // 2),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(\n\t                    stem_channels // 2,\n\t                    stem_channels // 2,\n", "                    kernel_size=3,\n\t                    stride=1,\n\t                    padding=1,\n\t                    bias=False),\n\t                norm_layer(stem_channels // 2),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(\n\t                    stem_channels // 2,\n\t                    stem_channels,\n\t                    kernel_size=3,\n", "                    stride=1,\n\t                    padding=1,\n\t                    bias=False),\n\t                norm_layer(stem_channels),\n\t                nn.ReLU(inplace=True))\n\t        else:\n\t            self.conv1 = nn.Conv2d(\n\t                in_channels,\n\t                stem_channels,\n\t                kernel_size=7,\n", "                stride=2,\n\t                padding=3,\n\t                bias=False)\n\t            self.norm1 = norm_layer(stem_channels)\n\t            self.relu = nn.ReLU(inplace=True)\n\t        self.maxpool = nn.MaxPool2d(\n\t            kernel_size=3, stride=2, padding=1, ceil_mode=self.max_pool_ceil_mode)\n\t    def _freeze_stages(self):\n\t        if self.frozen_stages >= 0:\n\t            if self.deep_stem:\n", "                self.stem.eval()\n\t                for param in self.stem.parameters():\n\t                    param.requires_grad = False\n\t            else:\n\t                self.norm1.eval()\n\t                for m in [self.conv1, self.norm1]:\n\t                    for param in m.parameters():\n\t                        param.requires_grad = False\n\t        for i in range(1, self.frozen_stages + 1):\n\t            m = getattr(self, f'layer{i}')\n", "            m.eval()\n\t            for param in m.parameters():\n\t                param.requires_grad = False\n\t    def init_weights(self, pretrained=None):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(\n\t                    m.weight, mode='fan_out', nonlinearity='relu')\n\t            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n\t                nn.init.constant_(m.weight, 1)\n", "                nn.init.constant_(m.bias, 0)\n\t        # Zero-initialize the last BN in each residual branch,\n\t        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n\t        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n\t        if self.zero_init_residual:\n\t            for m in self.modules():\n\t                if isinstance(m, (Bottleneck)):\n\t                    # type: ignore[arg-type]\n\t                    nn.init.constant_(m.bn3.weight, 0)\n\t                elif isinstance(m, (BasicBlock)):\n", "                    # type: ignore[arg-type]\n\t                    nn.init.constant_(m.bn2.weight, 0)\n\t        if pretrained is not None:\n\t            if pretrained == 'imagenet':\n\t                pretrained = model_urls['imagenet'][self.model_type]\n\t            elif pretrained == 'cityscapes':\n\t                pretrained = model_urls['cityscapes'][self.model_type]\n\t            if os.path.exists(pretrained):\n\t                checkpoint = torch.load(\n\t                    pretrained, map_location=lambda storage, loc: storage)\n", "            elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n\t                checkpoint = torch.load(os.path.join(\n\t                    os.environ.get('TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n\t            else:\n\t                checkpoint = torch.hub.load_state_dict_from_url(\n\t                    pretrained, progress=True, map_location=lambda storage, loc: storage)\n\t            if 'state_dict' in checkpoint.keys():\n\t                state_dict = checkpoint['state_dict']\n\t            else:\n\t                state_dict = checkpoint\n", "            if any(el.startswith('backbone.') for el in state_dict.keys()):\n\t                new_state_dict = {}\n\t                for k, v in state_dict.items():\n\t                    if k.startswith('backbone.'):\n\t                        new_k = k.replace('backbone.', '')\n\t                        new_state_dict[new_k] = v\n\t                state_dict = new_state_dict\n\t            state_dict = {k: v for k, v in state_dict.items()\n\t                          if not k.startswith('fc.')}\n\t            self.load_state_dict(state_dict, strict=True)\n", "    def forward(self, x):\n\t        if self.deep_stem:\n\t            x = self.stem(x)\n\t        else:\n\t            x = self.conv1(x)\n\t            x = self.norm1(x)\n\t            x = self.relu(x)\n\t        x = self.maxpool(x)\n\t        outs = []\n\t        for i, layer_name in enumerate(self.res_layers):\n", "            res_layer = getattr(self, layer_name)\n\t            x = res_layer(x)\n\t            if i in self.out_indices:\n\t                outs.append(x)\n\t        return tuple(outs)\n\t    def train(self, mode=True):\n\t        super(ResNet, self).train(mode)\n\t        self._freeze_stages()\n\t        if mode and self.norm_eval:\n\t            for m in self.modules():\n", "                # trick: eval have effect on BatchNorm only\n\t                if isinstance(m, nn.BatchNorm2d):\n\t                    m.eval()\n"]}
{"filename": "models/correlation_ops/__init__.py", "chunked_list": ["import os\n\timport torch\n\tfrom torch.utils import cpp_extension\n\tcwd = os.path.dirname(os.path.realpath(__file__))\n\tsources = []\n\tsources.append(os.path.join(cwd, 'correlation.cpp'))\n\tif torch.cuda.is_available():\n\t    sources.append(os.path.join(cwd, 'correlation_sampler.cpp'))\n\t    sources.append(os.path.join(cwd, 'correlation_cuda_kernel.cu'))\n\t    correlation = cpp_extension.load('correlation',\n", "                                        sources=sources,\n\t                                        build_directory=cwd,\n\t                                        extra_cflags=['-fopenmp'],\n\t                                        extra_ldflags=['-lgomp'],\n\t                                        with_cuda=True,\n\t                                        verbose=False)\n\telse:\n\t    # CPU only version\n\t    sources.append(os.path.join(cwd, 'correlation_sampler_cpu.cpp'))\n\t    correlation = cpp_extension.load('correlation',\n", "                                        sources=sources,\n\t                                        build_directory=cwd,\n\t                                        extra_cflags=['-fopenmp'],\n\t                                        extra_ldflags=['-lgomp'],\n\t                                        with_cuda=False,\n\t                                        verbose=False)\n"]}
{"filename": "models/correlation_ops/correlation_function.py", "chunked_list": ["# Copyright (c) Clemend Pinard. All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in https://github.com/ClementPinard/Pytorch-Correlation-extension.\n\t#\n\timport torch\n\tfrom torch.autograd.function import once_differentiable\n\tfrom torch.cuda.amp import custom_bwd, custom_fwd\n\tfrom torch.nn.modules.utils import _pair\n\tfrom .. import correlation_ops\n", "def spatial_correlation_sample(input1,\n\t                               input2,\n\t                               kernel_size=1,\n\t                               patch_size=1,\n\t                               stride=1,\n\t                               padding=0,\n\t                               dilation=1,\n\t                               dilation_patch=1):\n\t    \"\"\"Apply spatial correlation sampling on from input1 to input2,\n\t    Every parameter except input1 and input2 can be either single int\n", "    or a pair of int. For more information about Spatial Correlation\n\t    Sampling, see this page.\n\t    https://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/\n\t    Args:\n\t        input1 : The first parameter.\n\t        input2 : The second parameter.\n\t        kernel_size : total size of your correlation kernel, in pixels\n\t        patch_size : total size of your patch, determining how many\n\t            different shifts will be applied\n\t        stride : stride of the spatial sampler, will modify output\n", "            height and width\n\t        padding : padding applied to input1 and input2 before applying\n\t            the correlation sampling, will modify output height and width\n\t        dilation_patch : step for every shift in patch\n\t    Returns:\n\t        Tensor: Result of correlation sampling\n\t    \"\"\"\n\t    return SpatialCorrelationSamplerFunction.apply(input1, input2,\n\t                                                   kernel_size, patch_size,\n\t                                                   stride, padding, dilation, dilation_patch)\n", "class SpatialCorrelationSamplerFunction(torch.autograd.Function):\n\t    ''' For AMP, we need to cast to float32\n\t    '''\n\t    @staticmethod\n\t    @custom_fwd(cast_inputs=torch.float32)\n\t    def forward(ctx,\n\t                input1,\n\t                input2,\n\t                kernel_size=1,\n\t                patch_size=1,\n", "                stride=1,\n\t                padding=0,\n\t                dilation=1,\n\t                dilation_patch=1):\n\t        ctx.save_for_backward(input1, input2)\n\t        kH, kW = ctx.kernel_size = _pair(kernel_size)\n\t        patchH, patchW = ctx.patch_size = _pair(patch_size)\n\t        padH, padW = ctx.padding = _pair(padding)\n\t        dilationH, dilationW = ctx.dilation = _pair(dilation)\n\t        dilation_patchH, dilation_patchW = ctx.dilation_patch = _pair(\n", "            dilation_patch)\n\t        dH, dW = ctx.stride = _pair(stride)\n\t        output = correlation_ops.correlation.forward(input1, input2,\n\t                                                     kH, kW, patchH, patchW,\n\t                                                     padH, padW, dilationH, dilationW,\n\t                                                     dilation_patchH, dilation_patchW,\n\t                                                     dH, dW)\n\t        return output\n\t    @staticmethod\n\t    @once_differentiable\n", "    @custom_bwd\n\t    def backward(ctx, grad_output):\n\t        input1, input2 = ctx.saved_tensors\n\t        kH, kW = ctx.kernel_size\n\t        patchH, patchW = ctx.patch_size\n\t        padH, padW = ctx.padding\n\t        dilationH, dilationW = ctx.dilation\n\t        dilation_patchH, dilation_patchW = ctx.dilation_patch\n\t        dH, dW = ctx.stride\n\t        grad_input1, grad_input2 = correlation_ops.correlation.backward(input1, input2, grad_output,\n", "                                                                        kH, kW, patchH, patchW,\n\t                                                                        padH, padW, dilationH, dilationW,\n\t                                                                        dilation_patchH, dilation_patchW,\n\t                                                                        dH, dW)\n\t        return grad_input1, grad_input2, None, None, None, None, None, None\n"]}
{"filename": "models/heads/projection.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\n\t# \n\t# This source code is licensed under the license found in the\n\t# LICENSE file in https://github.com/open-mmlab/mmsegmentation.\n\t#\n\tfrom typing import List, Optional, Sequence, Union\n\timport torch.nn as nn\n\tfrom .base import BaseHead\n\tfrom .modules import ConvBNReLU\n\tclass ProjectionHead(BaseHead):\n", "    \"\"\"Projection Head for feature dimension reduction in contrastive loss.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 in_channels: Union[int, List[int]],\n\t                 in_index: Union[List[int], int],\n\t                 input_transform: Optional[str] = None,\n\t                 channels: int = 128,\n\t                 kernel_size: int = 1):\n\t        super().__init__(-1, in_index, input_transform)\n\t        self.in_channels = in_channels\n", "        self.channels = channels\n\t        self.kernel_size = kernel_size\n\t        if self.input_transform == 'multiple_select':\n\t            assert isinstance(self.in_channels, Sequence)\n\t            self.convs = nn.ModuleList([])\n\t            for i in range(len(self.in_channels)):\n\t                self.convs.append(nn.Sequential(\n\t                    ConvBNReLU(\n\t                        self.in_channels[i],\n\t                        self.in_channels[i],\n", "                        kernel_size=kernel_size,\n\t                        norm_layer=None,\n\t                        activation_layer=nn.ReLU),\n\t                    ConvBNReLU(\n\t                        self.in_channels[i],\n\t                        self.channels,\n\t                        kernel_size=kernel_size,\n\t                        norm_layer=None,\n\t                        activation_layer=None)))\n\t        else:\n", "            if self.input_transform == 'resize_concat':\n\t                if isinstance(self.in_channels, Sequence):\n\t                    in_channels = sum(self.in_channels)\n\t                else:\n\t                    in_channels = self.in_channels\n\t            else:\n\t                in_channels = self.in_channels\n\t            self.convs = nn.Sequential(\n\t                ConvBNReLU(\n\t                    in_channels,\n", "                    in_channels,\n\t                    kernel_size=kernel_size,\n\t                    norm_layer=None,\n\t                    activation_layer=nn.ReLU),\n\t                ConvBNReLU(\n\t                    in_channels,\n\t                    self.channels,\n\t                    kernel_size=kernel_size,\n\t                    norm_layer=None,\n\t                    activation_layer=None))\n", "        # mmseg init\n\t        for m in self.modules():\n\t            # initialize ConvBNReLU as in mmsegmentation\n\t            if isinstance(m, ConvBNReLU) and not m.depthwise_separable:\n\t                nn.init.kaiming_normal_(\n\t                    m.conv.weight, a=0, mode='fan_out', nonlinearity='relu')\n\t                if hasattr(m.conv, 'bias') and m.conv.bias is not None:\n\t                    nn.init.constant_(m.conv.bias, 0)\n\t                if m.use_norm:\n\t                    if hasattr(m.bn, 'weight') and m.bn.weight is not None:\n", "                        nn.init.constant_(m.bn.weight, 1)\n\t                    if hasattr(m.bn, 'bias') and m.bn.bias is not None:\n\t                        nn.init.constant_(m.bn.bias, 0)\n\t        nn.init.normal_(\n\t            self.convs[-1].conv.weight, mean=0, std=0.01)\n\t        nn.init.constant_(self.convs[-1].conv.bias, 0)\n\t    def forward(self, inputs):\n\t        \"\"\"Forward function.\"\"\"\n\t        x = self._transform_inputs(inputs)\n\t        if isinstance(x, list):\n", "            # multiple_select\n\t            output = [self.convs[i](x[i]) for i in range(len(x))]\n\t        else:\n\t            # resize_concat or single_select\n\t            output = self.convs(x)\n\t        return output\n"]}
{"filename": "models/heads/segformer.py", "chunked_list": ["# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in https://github.com/NVlabs/SegFormer.\n\t#\n\timport os\n\tfrom typing import List, Optional, Union\n\timport torch\n\timport torch.nn as nn\n\tfrom .base import BaseHead\n", "from .modules import ConvBNReLU\n\tmodel_urls = {\n\t    \"cityscapes\": {\n\t        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n\t        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/segformer.b5.1024x1024.city.160k.pth\",\n\t    }\n\t}\n\tclass MLP(nn.Module):\n\t    \"\"\"\n\t    Linear Embedding\n", "    \"\"\"\n\t    def __init__(self, input_dim=2048, embed_dim=768):\n\t        super().__init__()\n\t        self.proj = nn.Linear(input_dim, embed_dim)\n\t    def forward(self, x):\n\t        x = x.flatten(2).transpose(1, 2)\n\t        x = self.proj(x)\n\t        return x\n\tclass SegFormerHead(BaseHead):\n\t    \"\"\"\n", "    SegFormer: Simple and Efficient Design for Semantic Segmentation with\n\t    Transformers\n\t    \"\"\"\n\t    def __init__(self,\n\t                 in_channels: List[int],\n\t                 in_index: Union[List[int], int],\n\t                 num_classes: int,\n\t                 input_transform: Optional[str] = None,\n\t                 channels: int = 256,\n\t                 dropout_ratio: float = 0.1,\n", "                 pretrained: Optional[str] = None,\n\t                 ):\n\t        super().__init__(num_classes, in_index, input_transform)\n\t        self.in_channels = in_channels\n\t        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n\t        embedding_dim = channels\n\t        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n\t        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n\t        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n\t        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n", "        self.linear_fuse = ConvBNReLU(\n\t            in_channels=embedding_dim*4,\n\t            out_channels=embedding_dim,\n\t            kernel_size=1,\n\t            norm_layer=nn.BatchNorm2d,\n\t        )\n\t        self.linear_pred = nn.Conv2d(\n\t            embedding_dim, self.num_classes, kernel_size=1)\n\t        if dropout_ratio > 0:\n\t            self.dropout = nn.Dropout2d(dropout_ratio)\n", "        else:\n\t            self.dropout = None\n\t        self.init_weights()\n\t        self.load_pretrained(pretrained)\n\t    def init_weights(self):\n\t        # mmseg init\n\t        nn.init.normal_(self.linear_pred.weight, mean=0, std=0.01)\n\t        nn.init.constant_(self.linear_pred.bias, 0)\n\t        for m in self.modules():\n\t            # initialize ConvBNReLU as in mmsegmentation\n", "            if isinstance(m, ConvBNReLU) and not m.depthwise_separable:\n\t                nn.init.kaiming_normal_(\n\t                    m.conv.weight, a=0, mode='fan_out', nonlinearity='relu')\n\t                if hasattr(m.conv, 'bias') and m.conv.bias is not None:\n\t                    nn.init.constant_(m.conv.bias, 0)\n\t                if m.use_norm:\n\t                    if hasattr(m.bn, 'weight') and m.bn.weight is not None:\n\t                        nn.init.constant_(m.bn.weight, 1)\n\t                    if hasattr(m.bn, 'bias') and m.bn.bias is not None:\n\t                        nn.init.constant_(m.bn.bias, 0)\n", "    def forward(self, inputs):\n\t        # Receive 4 stage backbone feature map: 1/4, 1/8, 1/16, 1/32\n\t        c1, c2, c3, c4 = inputs\n\t        ############## MLP decoder on C1-C4 ###########\n\t        n, _, h, w = c4.shape\n\t        _c4 = self.linear_c4(c4).permute(0, 2, 1).reshape(\n\t            n, -1, c4.shape[2], c4.shape[3])\n\t        _c4 = nn.functional.interpolate(\n\t            _c4, size=c1.size()[2:], mode='bilinear', align_corners=False)\n\t        _c3 = self.linear_c3(c3).permute(0, 2, 1).reshape(\n", "            n, -1, c3.shape[2], c3.shape[3])\n\t        _c3 = nn.functional.interpolate(\n\t            _c3, size=c1.size()[2:], mode='bilinear', align_corners=False)\n\t        _c2 = self.linear_c2(c2).permute(0, 2, 1).reshape(\n\t            n, -1, c2.shape[2], c2.shape[3])\n\t        _c2 = nn.functional.interpolate(\n\t            _c2, size=c1.size()[2:], mode='bilinear', align_corners=False)\n\t        _c1 = self.linear_c1(c1).permute(0, 2, 1).reshape(\n\t            n, -1, c1.shape[2], c1.shape[3])\n\t        x = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n", "        if self.dropout is not None:\n\t            x = self.dropout(x)\n\t        x = self.linear_pred(x)\n\t        return x\n\t    def load_pretrained(self, pretrained):\n\t        if pretrained is None:\n\t            return\n\t        if pretrained == 'cityscapes':\n\t            pretrained = model_urls['cityscapes']['mit_b5']\n\t        if os.path.exists(pretrained):\n", "            checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n\t        elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n\t            checkpoint = torch.load(os.path.join(os.environ.get(\n\t                'TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n\t        else:\n\t            checkpoint = torch.hub.load_state_dict_from_url(\n\t                pretrained, progress=True, map_location=lambda storage, loc: storage)\n\t        if 'state_dict' in checkpoint:\n\t            state_dict = checkpoint['state_dict']\n\t        else:\n", "            state_dict = checkpoint\n\t        new_state_dict = {}\n\t        for k, v in state_dict.items():\n\t            if k.startswith('decode_head.'):\n\t                new_k = k.replace('decode_head.', '')\n\t                new_state_dict[new_k] = v\n\t            elif k.startswith('head.'):\n\t                new_k = k.replace('head.', '')\n\t                new_state_dict[new_k] = v\n\t            else:\n", "                pass\n\t        new_state_dict = {k: v for k, v in new_state_dict.items()\n\t                      if not k.startswith('conv_seg.')}\n\t        self.load_state_dict(new_state_dict, strict=True)\n"]}
{"filename": "models/heads/base.py", "chunked_list": ["from typing import Iterable, List, Optional, Union\n\timport torch\n\timport torch.nn as nn\n\tclass BaseHead(nn.Module):\n\t    def __init__(self,\n\t                 num_classes: int,\n\t                 in_index: Union[List[int], int],\n\t                 input_transform: Optional[str] = None):\n\t        super().__init__()\n\t        self.input_transform = input_transform\n", "        if isinstance(in_index, Iterable) and len(in_index) == 1:\n\t            self.in_index = in_index[0]\n\t        else:\n\t            self.in_index = in_index\n\t        self.num_classes = num_classes\n\t    def forward(self, inp):\n\t        raise NotImplementedError\n\t    def _transform_inputs(self, inputs):\n\t        \"\"\"Transform inputs for decoder.\n\t        Args:\n", "            inputs (list[Tensor]): List of multi-level img features.\n\t        Returns:\n\t            Tensor: The transformed inputs\n\t        \"\"\"\n\t        if self.input_transform == 'resize_concat':\n\t            inputs = [inputs[i] for i in self.in_index]\n\t            upsampled_inputs = [\n\t                nn.functional.interpolate(\n\t                    input=x,\n\t                    size=inputs[0].shape[2:],\n", "                    mode='bilinear',\n\t                    align_corners=False) for x in inputs\n\t            ]\n\t            inputs = torch.cat(upsampled_inputs, dim=1)\n\t        elif self.input_transform == 'multiple_select':\n\t            inputs = [inputs[i] for i in self.in_index]\n\t        else:\n\t            inputs = inputs[self.in_index]\n\t        return inputs\n"]}
{"filename": "models/heads/__init__.py", "chunked_list": ["from .deeplabv2 import DeepLabV2Head\n\tfrom .projection import ProjectionHead\n\tfrom .segformer import SegFormerHead\n\tfrom .uawarpc import UAWarpCHead\n"]}
{"filename": "models/heads/modules.py", "chunked_list": ["import torch.nn as nn\n\tclass ConvBNReLU(nn.Module):\n\t    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, groups=1, padding=None,\n\t                 norm_layer=nn.BatchNorm2d, activation_layer=nn.ReLU, bias='auto',\n\t                 depthwise_separable=False, inplace=True, affine=True):\n\t        super().__init__()\n\t        padding = dilation * \\\n\t            (kernel_size - 1) // 2 if padding is None else padding\n\t        self.use_norm = norm_layer is not None\n\t        self.use_activation = activation_layer is not None\n", "        self.depthwise_separable = depthwise_separable\n\t        if bias == 'auto':\n\t            bias = not self.use_norm\n\t        if depthwise_separable:\n\t            assert kernel_size > 1\n\t            assert groups == 1  # not sure how to handle this\n\t            self.depthwise_conv = ConvBNReLU(in_channels, in_channels, kernel_size, stride=stride,\n\t                                             padding=padding, dilation=dilation, groups=in_channels,\n\t                                             norm_layer=norm_layer, activation_layer=activation_layer)\n\t            self.pointwise_conv = ConvBNReLU(in_channels, out_channels, 1,\n", "                                             norm_layer=norm_layer, activation_layer=activation_layer)\n\t        else:\n\t            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n\t                                  dilation=dilation, groups=groups, bias=bias)\n\t            if self.use_norm:\n\t                self.bn = norm_layer(out_channels, affine=affine)\n\t        if self.use_activation:\n\t            self.activation = activation_layer(inplace=inplace)\n\t    def forward(self, x):\n\t        if self.depthwise_separable:\n", "            x = self.depthwise_conv(x)\n\t            x = self.pointwise_conv(x)\n\t        else:\n\t            x = self.conv(x)\n\t            if self.use_norm:\n\t                x = self.bn(x)\n\t            if self.use_activation:\n\t                x = self.activation(x)\n\t        return x\n"]}
{"filename": "models/heads/uawarpc.py", "chunked_list": ["import math\n\timport os\n\tfrom functools import partial\n\tfrom typing import List, Optional, Union\n\timport torch\n\timport torch.nn as nn\n\tfrom ..utils import warp\n\tfrom .base import BaseHead\n\tfrom .modules import ConvBNReLU\n\tmodel_urls = {\n", "    # checkpoint from Refign: https://github.com/brdav/refign\n\t    \"megadepth\": \"https://data.vision.ee.ethz.ch/brdavid/refign/uawarpc_megadepth.ckpt\"\n\t}\n\tclass LocalFeatureCorrelationLayer(nn.Module):\n\t    def __init__(self, patch_size=9):\n\t        super().__init__()\n\t        try:\n\t            # If the PyTorch correlation module is installed,\n\t            # we can avoid JIT compilation of the extension:\n\t            # https://github.com/ClementPinard/Pytorch-Correlation-extension\n", "            from spatial_correlation_sampler import spatial_correlation_sample\n\t        except ModuleNotFoundError:\n\t            from models.correlation_ops.correlation_function import \\\n\t                spatial_correlation_sample\n\t        self.local_correlation = spatial_correlation_sample\n\t        self.patch_size = patch_size\n\t    def forward(self, feature_source, feature_target):\n\t        b = feature_target.shape[0]\n\t        corr = self.local_correlation(feature_target,\n\t                                      feature_source,\n", "                                      patch_size=self.patch_size)\n\t        corr = corr.view(b, self.patch_size * self.patch_size,\n\t                         feature_target.shape[2], feature_target.shape[3])\n\t        corr = nn.functional.normalize(nn.functional.relu(corr), p=2, dim=1)\n\t        return corr\n\tclass GlobalFeatureCorrelationLayer(nn.Module):\n\t    \"\"\"\n\t    ---------------------------------------------------------------------------\n\t    Copyright (c) Prune Truong. All rights reserved.\n\t    This source code is licensed under the license found in the\n", "    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n\t    ---------------------------------------------------------------------------\n\t    Implementation of the global feature correlation layer\n\t    Source and query, as well as target and reference refer to the same images.\n\t    \"\"\"\n\t    def __init__(self, cyclic_consistency=True):\n\t        super().__init__()\n\t        self.cyclic_consistency = cyclic_consistency\n\t    def forward(self, feature_source, feature_target):\n\t        b = feature_target.shape[0]\n", "        # directly obtain the 3D correlation\n\t        corr = self.compute_global_correlation(feature_source,\n\t                                               feature_target)\n\t        if self.cyclic_consistency:\n\t            # to add on top of the correlation ! (already included in NC-Net)\n\t            corr4d = self.mutual_matching(corr.view(\n\t                b, feature_source.shape[2], feature_source.shape[3], feature_target.shape[2], feature_target.shape[3]).unsqueeze(1))\n\t            corr = corr4d.squeeze(1).view(\n\t                b, feature_source.shape[2] * feature_source.shape[3], feature_target.shape[2], feature_target.shape[3])\n\t        corr = nn.functional.normalize(nn.functional.relu(corr), p=2, dim=1)\n", "        return corr\n\t    @staticmethod\n\t    def mutual_matching(corr4d):\n\t        # mutual matching\n\t        batch_size, ch, fs1, fs2, fs3, fs4 = corr4d.size()\n\t        # [batch_idx,k_A,i_B,j_B] #correlation target\n\t        corr4d_B = corr4d.view(batch_size, fs1 * fs2, fs3, fs4)\n\t        corr4d_A = corr4d.view(batch_size, fs1, fs2, fs3 * fs4)\n\t        # get max\n\t        corr4d_B_max, _ = torch.max(corr4d_B, dim=1, keepdim=True)\n", "        corr4d_A_max, _ = torch.max(corr4d_A, dim=3, keepdim=True)\n\t        eps = 1e-5\n\t        corr4d_B = corr4d_B / (corr4d_B_max + eps)\n\t        corr4d_A = corr4d_A / (corr4d_A_max + eps)\n\t        corr4d_B = corr4d_B.view(batch_size, 1, fs1, fs2, fs3, fs4)\n\t        corr4d_A = corr4d_A.view(batch_size, 1, fs1, fs2, fs3, fs4)\n\t        # parenthesis are important for symmetric output\n\t        corr4d = corr4d * (corr4d_A * corr4d_B)\n\t        return corr4d\n\t    @staticmethod\n", "    def compute_global_correlation(feature_source, feature_target, shape='3D', put_W_first_in_channel_dimension=False):\n\t        if shape == '3D':\n\t            b, c, h_source, w_source = feature_source.size()\n\t            b, c, h_target, w_target = feature_target.size()\n\t            if put_W_first_in_channel_dimension:\n\t                # FOR SOME REASON, THIS IS THE DEFAULT\n\t                feature_source = feature_source.transpose(\n\t                    2, 3).contiguous().view(b, c, w_source * h_source)\n\t                # ATTENTION, here the w and h of the source features are inverted !!!\n\t                # shape (b,c, w_source * h_source)\n", "                feature_target = feature_target.view(\n\t                    b, c, h_target * w_target).transpose(1, 2)\n\t                # shape (b,h_target*w_target,c)\n\t                # perform matrix mult.\n\t                feature_mul = torch.bmm(feature_target, feature_source)\n\t                # shape (b,h_target*w_target, w_source*h_source)\n\t                # indexed [batch,idx_A=row_A+h*col_A,row_B,col_B]\n\t                correlation_tensor = feature_mul.view(b, h_target, w_target, w_source * h_source).transpose(2, 3) \\\n\t                    .transpose(1, 2)\n\t                # shape (b, w_source*h_source, h_target, w_target)\n", "                # ATTENTION, here in source dimension, W is first !! (channel dimension is W,H)\n\t            else:\n\t                feature_source = feature_source.contiguous().view(b, c, h_source * w_source)\n\t                # shape (b,c, h_source * w_source)\n\t                feature_target = feature_target.view(\n\t                    b, c, h_target * w_target).transpose(1, 2)\n\t                # shape (b,h_target*w_target,c)\n\t                # perform matrix mult.\n\t                feature_mul = torch.bmm(feature_target,\n\t                                        feature_source)  # shape (b,h_target*w_target, h_source*w_source)\n", "                correlation_tensor = feature_mul.view(b, h_target, w_target, h_source * w_source).transpose(2, 3) \\\n\t                    .transpose(1, 2)\n\t                # shape (b, h_source*w_source, h_target, w_target)\n\t                # ATTENTION, here H is first in channel dimension !\n\t        elif shape == '4D':\n\t            b, c, hsource, wsource = feature_source.size()\n\t            b, c, htarget, wtarget = feature_target.size()\n\t            # reshape features for matrix multiplication\n\t            feature_source = feature_source.view(\n\t                b, c, hsource * wsource).transpose(1, 2)  # size [b,hsource*wsource,c]\n", "            feature_target = feature_target.view(\n\t                b, c, htarget * wtarget)  # size [b,c,htarget*wtarget]\n\t            # perform matrix mult.\n\t            # size [b, hsource*wsource, htarget*wtarget]\n\t            feature_mul = torch.bmm(feature_source, feature_target)\n\t            correlation_tensor = feature_mul.view(\n\t                b, hsource, wsource, htarget, wtarget).unsqueeze(1)\n\t            # size is [b, 1, hsource, wsource, htarget, wtarget]\n\t        else:\n\t            raise ValueError('tensor should be 3D or 4D')\n", "        return correlation_tensor\n\tclass OpticalFlowEstimatorResidualConnection(nn.Module):\n\t    \"\"\"\n\t    ---------------------------------------------------------------------------\n\t    Copyright (c) Prune Truong. All rights reserved.\n\t    This source code is licensed under the license found in the\n\t    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n\t    ---------------------------------------------------------------------------\n\t    \"\"\"\n\t    def __init__(self, in_channels, out_channels=2, batch_norm=True, output_x=False, extra_bias='auto'):\n", "        super().__init__()\n\t        self.output_x = output_x\n\t        norm_layer = nn.BatchNorm2d if batch_norm else None\n\t        activation_layer = partial(nn.LeakyReLU, negative_slope=0.1)\n\t        self.leaky_relu = activation_layer()\n\t        self.conv_0 = ConvBNReLU(in_channels, 128, kernel_size=3,\n\t                                 norm_layer=norm_layer, activation_layer=None, bias=extra_bias)\n\t        self.conv0_skip = ConvBNReLU(\n\t            128, 96, kernel_size=1, norm_layer=norm_layer, activation_layer=None)\n\t        self.conv_1 = ConvBNReLU(128, 128, kernel_size=3, norm_layer=norm_layer,\n", "                                 activation_layer=activation_layer, bias=extra_bias)\n\t        self.conv_2 = ConvBNReLU(\n\t            128, 96, kernel_size=3, norm_layer=norm_layer, activation_layer=None, bias=extra_bias)\n\t        self.conv2_skip = ConvBNReLU(\n\t            96, 32, kernel_size=1, norm_layer=norm_layer, activation_layer=None)\n\t        self.conv_3 = ConvBNReLU(96, 64, kernel_size=3, norm_layer=norm_layer,\n\t                                 activation_layer=activation_layer, bias=extra_bias)\n\t        self.conv_4 = ConvBNReLU(\n\t            64, 32, kernel_size=3, norm_layer=norm_layer, activation_layer=None, bias=extra_bias)\n\t        self.predict_mapping = nn.Conv2d(\n", "            32, out_channels, kernel_size=3, padding=1, bias=True)\n\t    def forward(self, x):\n\t        x0 = self.conv_0(x)\n\t        x0_relu = self.leaky_relu(x0)\n\t        x2 = self.conv_2(self.conv_1(x0_relu))\n\t        x2_skip = x2 + self.conv0_skip(x0)\n\t        x2_skip_relu = self.leaky_relu(x2_skip)\n\t        x4 = self.conv_4(self.conv_3(x2_skip_relu))\n\t        x4_skip = x4 + self.conv2_skip(x2_skip)\n\t        x = self.leaky_relu(x4_skip)\n", "        mapping = self.predict_mapping(x)\n\t        if self.output_x:\n\t            return mapping, x\n\t        else:\n\t            return mapping\n\tclass RefinementModule(nn.Module):\n\t    \"\"\"\n\t    ---------------------------------------------------------------------------\n\t    Copyright (c) Prune Truong. All rights reserved.\n\t    This source code is licensed under the license found in the\n", "    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n\t    ---------------------------------------------------------------------------\n\t    \"\"\"\n\t    def __init__(self, in_channels, out_channels=2, batch_norm=True, extra_bias='auto'):\n\t        super().__init__()\n\t        norm_layer = nn.BatchNorm2d if batch_norm else None\n\t        activation_layer = partial(nn.LeakyReLU, negative_slope=0.1)\n\t        self.dc_convs = nn.Sequential(\n\t            ConvBNReLU(in_channels, 128, kernel_size=3, dilation=1, norm_layer=norm_layer,\n\t                       activation_layer=activation_layer, bias=extra_bias),\n", "            ConvBNReLU(128, 128, kernel_size=3, dilation=2, norm_layer=norm_layer,\n\t                       activation_layer=activation_layer, bias=extra_bias),\n\t            ConvBNReLU(128, 128, kernel_size=3, dilation=4, norm_layer=norm_layer,\n\t                       activation_layer=activation_layer, bias=extra_bias),\n\t            ConvBNReLU(128, 96, kernel_size=3, dilation=8, norm_layer=norm_layer,\n\t                       activation_layer=activation_layer, bias=extra_bias),\n\t            ConvBNReLU(96, 64, kernel_size=3, dilation=16, norm_layer=norm_layer,\n\t                       activation_layer=activation_layer, bias=extra_bias),\n\t            ConvBNReLU(64, 32, kernel_size=3, dilation=1, norm_layer=norm_layer,\n\t                       activation_layer=activation_layer, bias=extra_bias),\n", "            nn.Conv2d(32, out_channels, kernel_size=3, padding=1, bias=True)\n\t        )\n\t    def forward(self, x):\n\t        return self.dc_convs(x)\n\tclass UncertaintyModule(nn.Module):\n\t    \"\"\"\n\t    ---------------------------------------------------------------------------\n\t    Copyright (c) Prune Truong. All rights reserved.\n\t    This source code is licensed under the license found in the\n\t    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n", "    ---------------------------------------------------------------------------\n\t    \"\"\"\n\t    def __init__(self,\n\t                 in_channels,\n\t                 feed_in_previous=False,\n\t                 out_channels=1,\n\t                 search_size=9,\n\t                 batch_norm=True,\n\t                 depthwise_separable=False):\n\t        super().__init__()\n", "        norm_layer = nn.BatchNorm2d if batch_norm else None\n\t        activation_layer = partial(nn.LeakyReLU, negative_slope=0.1)\n\t        self.search_size = search_size\n\t        self.feed_in_previous = feed_in_previous\n\t        if self.feed_in_previous:\n\t            add_channels = 2 + 1  # 2 flow channels and 1 sigma channel\n\t        else:\n\t            add_channels = 0\n\t        out_channels = 1\n\t        if self.search_size == 9:\n", "            self.conv_0 = ConvBNReLU(in_channels, 32, kernel_size=3, stride=1, padding=0,\n\t                                     norm_layer=norm_layer, activation_layer=activation_layer, depthwise_separable=False)\n\t            self.conv_1 = ConvBNReLU(32, 32, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n\t                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n\t            self.conv_2 = ConvBNReLU(32, 16, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n\t                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n\t            self.predict_uncertainty = nn.Conv2d(\n\t                16, 6, kernel_size=3, stride=1, padding=0, bias=True)\n\t        elif search_size == 16:\n\t            self.conv_0 = ConvBNReLU(in_channels, 32, kernel_size=3, stride=1, padding=0,\n", "                                     norm_layer=norm_layer, activation_layer=activation_layer, depthwise_separable=False)\n\t            self.maxpool = nn.MaxPool2d((2, 2))\n\t            self.conv_1 = ConvBNReLU(32, 32, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n\t                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n\t            self.conv_2 = ConvBNReLU(32, 16, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n\t                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n\t            self.predict_uncertainty = nn.Conv2d(\n\t                16, 6, kernel_size=3, stride=1, padding=0, bias=True)\n\t        self.pred_conv_0 = ConvBNReLU(6 + 32 + add_channels, 32, kernel_size=3, norm_layer=norm_layer,\n\t                                      activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n", "        self.pred_conv_1 = ConvBNReLU(32, 16, kernel_size=3, norm_layer=norm_layer,\n\t                                      activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n\t        self.predict_uncertainty_final = nn.Conv2d(\n\t            16, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n\t    def forward(self, corr, feat, up_previous_uncertainty=None, up_previous_flow=None, logits_corr=None):\n\t        # x shape is b, s*s, h, w\n\t        b, _, h, w = corr.size()\n\t        x = corr.permute(0, 2, 3, 1).contiguous().view(\n\t            b * h * w, self.search_size, self.search_size).unsqueeze(1)\n\t        # x is now shape b*h*w, 1, s, s\n", "        if logits_corr is not None:\n\t            x_logits = logits_corr.permute(0, 2, 3, 1).contiguous().view(\n\t                b * h * w, self.search_size, self.search_size).unsqueeze(1)\n\t            x = torch.cat((x, x_logits), dim=1)\n\t        if self.search_size == 9:\n\t            x = self.conv_2(self.conv_1(self.conv_0(x)))\n\t            uncertainty_corr = self.predict_uncertainty(x)\n\t        elif self.search_size == 16:\n\t            x = self.conv_0(x)\n\t            x = self.maxpool(x)\n", "            x = self.conv_2(self.conv_1(x))\n\t            uncertainty_corr = self.predict_uncertainty(x)\n\t        uncertainty_corr = uncertainty_corr.squeeze().view(\n\t            b, h, w, -1).permute(0, 3, 1, 2)\n\t        if self.feed_in_previous:\n\t            uncertainty = torch.cat(\n\t                (uncertainty_corr, feat, up_previous_uncertainty, up_previous_flow), 1)\n\t        else:\n\t            uncertainty = torch.cat((uncertainty_corr, feat), 1)\n\t        uncertainty = self.pred_conv_1(self.pred_conv_0(uncertainty))\n", "        uncertainty = self.predict_uncertainty_final(uncertainty)\n\t        return uncertainty\n\tclass UAWarpCHead(BaseHead):\n\t    def __init__(self,\n\t                 in_index: Union[List[int], int],\n\t                 input_transform: Optional[str] = None,\n\t                 pretrained: Optional[str] = None,\n\t                 batch_norm: bool = True,\n\t                 refinement_at_adaptive_res: bool = True,\n\t                 refinement_at_finest_level: bool = True,\n", "                 estimate_uncertainty: bool = False,\n\t                 uncertainty_mixture: bool = False,\n\t                 iterative_refinement: bool = False):\n\t        super().__init__(None, in_index, input_transform)\n\t        self.estimate_uncertainty = estimate_uncertainty\n\t        self.uncertainty_mixture = uncertainty_mixture\n\t        self.iterative_refinement = iterative_refinement\n\t        self.global_corr = GlobalFeatureCorrelationLayer(\n\t            cyclic_consistency=True)\n\t        self.local_corr = LocalFeatureCorrelationLayer(\n", "            patch_size=9)\n\t        # level 4, 16x16, global correlation\n\t        nd = 16 * 16\n\t        od = nd\n\t        self.decoder4 = OpticalFlowEstimatorResidualConnection(\n\t            in_channels=od, batch_norm=batch_norm, output_x=True)\n\t        # level 3, 32x32, constrained correlation, patchsize 9\n\t        nd = 9 * 9\n\t        od = nd + 2\n\t        if self.estimate_uncertainty:\n", "            od += 1  # input also the upsampled log_var of previous resolution\n\t        self.decoder3 = OpticalFlowEstimatorResidualConnection(\n\t            in_channels=od, batch_norm=batch_norm, output_x=True)\n\t        self.refinement_at_adaptive_res = refinement_at_adaptive_res\n\t        if self.refinement_at_adaptive_res:\n\t            self.refinement_module_adaptive = RefinementModule(\n\t                32, batch_norm=batch_norm)\n\t        nbr_upfeat_channels = 2\n\t        od = nd + 2\n\t        if self.estimate_uncertainty:\n", "            od += 1  # input also the upsampled log_var of previous resolution\n\t        self.decoder2 = OpticalFlowEstimatorResidualConnection(\n\t            in_channels=od, batch_norm=batch_norm, output_x=True)\n\t        self.reduce = nn.Conv2d(32, nbr_upfeat_channels,\n\t                                kernel_size=1, bias=True)\n\t        od = nd + 2 + nbr_upfeat_channels\n\t        if self.estimate_uncertainty:\n\t            od += 1  # input also the upsampled log_var of previous resolution\n\t        self.decoder1 = OpticalFlowEstimatorResidualConnection(\n\t            in_channels=od, batch_norm=batch_norm, output_x=True)\n", "        self.refinement_at_finest_level = refinement_at_finest_level\n\t        if self.refinement_at_finest_level:\n\t            self.refinement_module_finest = RefinementModule(\n\t                32, batch_norm=batch_norm)\n\t        if self.estimate_uncertainty:\n\t            self.estimate_uncertainty_components4 = UncertaintyModule(in_channels=1,\n\t                                                                      search_size=16)\n\t            self.estimate_uncertainty_components3 = UncertaintyModule(in_channels=1,\n\t                                                                      search_size=9,\n\t                                                                      feed_in_previous=True)\n", "            self.estimate_uncertainty_components2 = UncertaintyModule(in_channels=1,\n\t                                                                      search_size=9,\n\t                                                                      feed_in_previous=True)\n\t            self.estimate_uncertainty_components1 = UncertaintyModule(in_channels=1,\n\t                                                                      search_size=9,\n\t                                                                      feed_in_previous=True)\n\t        if pretrained is not None:\n\t            self.load_weights(pretrained)\n\t    def forward(self, trg, src, trg_256, src_256, out_size, **kwargs):\n\t        c11, c12 = self._transform_inputs(trg)\n", "        c13, c14 = self._transform_inputs(trg_256)\n\t        c21, c22 = self._transform_inputs(src)\n\t        c23, c24 = self._transform_inputs(src_256)\n\t        c11 = nn.functional.normalize(c11, p=2, dim=1)\n\t        c12 = nn.functional.normalize(c12, p=2, dim=1)\n\t        c13 = nn.functional.normalize(c13, p=2, dim=1)\n\t        c14 = nn.functional.normalize(c14, p=2, dim=1)\n\t        c21 = nn.functional.normalize(c21, p=2, dim=1)\n\t        c22 = nn.functional.normalize(c22, p=2, dim=1)\n\t        c23 = nn.functional.normalize(c23, p=2, dim=1)\n", "        c24 = nn.functional.normalize(c24, p=2, dim=1)\n\t        h_256, w_256 = (256, 256)\n\t        # level 4: 16x16\n\t        h_4, w_4 = c14.shape[-2:]\n\t        assert (h_4, w_4) == (16, 16), (h_4, w_4)\n\t        corr4 = self.global_corr(c24, c14)\n\t        # init_map = corr4.new_zeros(size=(b, 2, h_4, w_4))\n\t        est_map4, x4 = self.decoder4(corr4)\n\t        flow4_256 = self.unnormalise_and_convert_mapping_to_flow(est_map4)\n\t        # scale flow values to 256x256\n", "        flow4_256[:, 0, :, :] *= w_256 / float(w_4)\n\t        flow4_256[:, 1, :, :] *= h_256 / float(h_4)\n\t        if self.estimate_uncertainty:\n\t            uncert_components4_256 = self.estimate_uncertainty_components4(\n\t                corr4, x4)\n\t            # scale uncert values to 256\n\t            assert w_256 / float(w_4) == h_256 / float(h_4)\n\t            uncert_components4_256[:, 0, :, :] += 2 * \\\n\t                math.log(w_256 / float(w_4))\n\t        # level 3: 32x32\n", "        h_3, w_3 = c13.shape[-2:]\n\t        assert (h_3, w_3) == (32, 32), (h_3, w_3)\n\t        up_flow4 = nn.functional.interpolate(input=flow4_256, size=(\n\t            h_3, w_3), mode='bilinear', align_corners=False)\n\t        if self.estimate_uncertainty:\n\t            up_uncert_components4 = nn.functional.interpolate(\n\t                input=uncert_components4_256, size=(h_3, w_3), mode='bilinear', align_corners=False)\n\t        # for warping, we need flow values at 32x32 scale\n\t        up_flow_4_warping = up_flow4.clone()\n\t        up_flow_4_warping[:, 0, :, :] *= w_3 / float(w_256)\n", "        up_flow_4_warping[:, 1, :, :] *= h_3 / float(h_256)\n\t        warp3 = warp(c23, up_flow_4_warping)\n\t        # constrained correlation\n\t        corr3 = self.local_corr(warp3, c13)\n\t        if self.estimate_uncertainty:\n\t            inp_flow_dec3 = torch.cat(\n\t                (corr3, up_flow4, up_uncert_components4), 1)\n\t        else:\n\t            inp_flow_dec3 = torch.cat((corr3, up_flow4), 1)\n\t        res_flow3, x3 = self.decoder3(inp_flow_dec3)\n", "        if self.refinement_at_adaptive_res:\n\t            res_flow3 = res_flow3 + self.refinement_module_adaptive(x3)\n\t        flow3 = res_flow3 + up_flow4\n\t        if self.estimate_uncertainty:\n\t            uncert_components3 = self.estimate_uncertainty_components3(\n\t                corr3, x3, up_uncert_components4, up_flow4)\n\t        # change from absolute resolutions to relative resolutions\n\t        # scale flow4 and flow3 magnitude to original resolution\n\t        h_original, w_original = out_size\n\t        flow3[:, 0, :, :] *= w_original / float(w_256)\n", "        flow3[:, 1, :, :] *= h_original / float(h_256)\n\t        if self.estimate_uncertainty:\n\t            # APPROXIMATION FOR NON-SQUARE IMAGES --> use the diagonal\n\t            diag_original = math.sqrt(h_original ** 2 + w_original ** 2)\n\t            diag_256 = math.sqrt(h_256 ** 2 + w_256 ** 2)\n\t            uncert_components3[:, 0, :, :] += 2 * \\\n\t                math.log(diag_original / float(diag_256))\n\t        if self.iterative_refinement and not self.training:\n\t            # from 32x32 resolution, if upsampling to 1/8*original resolution is too big,\n\t            # do iterative upsampling so that gap is always smaller than 2.\n", "            R = float(max(h_original, w_original)) / 8.0 / 32.0\n\t            minimum_ratio = 3.0  # if the image is >= 1086 in one dimension, do refinement\n\t            nbr_extra_layers = max(\n\t                0, int(round(math.log(R / minimum_ratio) / math.log(2))))\n\t            if nbr_extra_layers > 0:\n\t                for n in range(nbr_extra_layers):\n\t                    ratio = 1.0 / (8.0 * 2 ** (nbr_extra_layers - n))\n\t                    h_this = int(h_original * ratio)\n\t                    w_this = int(w_original * ratio)\n\t                    up_flow3 = nn.functional.interpolate(input=flow3, size=(\n", "                        h_this, w_this), mode='bilinear', align_corners=False)\n\t                    if self.estimate_uncertainty:\n\t                        up_uncert_components3 = nn.functional.interpolate(input=uncert_components3, size=(\n\t                            h_this, w_this), mode='bilinear', align_corners=False)\n\t                    c23_bis = nn.functional.interpolate(\n\t                        c22, size=(h_this, w_this), mode='area')\n\t                    c13_bis = nn.functional.interpolate(\n\t                        c12, size=(h_this, w_this), mode='area')\n\t                    warp3 = warp(c23_bis, up_flow3 * ratio)\n\t                    corr3 = self.local_corr(warp3, c13_bis)\n", "                    if self.estimate_uncertainty:\n\t                        inp_flow_dec3 = torch.cat(\n\t                            (corr3, up_flow3, up_uncert_components3), 1)\n\t                    else:\n\t                        inp_flow_dec3 = torch.cat((corr3, up_flow3), 1)\n\t                    res_flow3, x3 = self.decoder2(inp_flow_dec3)\n\t                    flow3 = res_flow3 + up_flow3\n\t                    if self.estimate_uncertainty:\n\t                        uncert_components3 = self.estimate_uncertainty_components2(\n\t                            corr3, x3, up_uncert_components3, up_flow3)\n", "        # level 2: 1/8 of original resolution\n\t        h_2, w_2 = c12.shape[-2:]\n\t        up_flow3 = nn.functional.interpolate(input=flow3, size=(\n\t            h_2, w_2), mode='bilinear', align_corners=False)\n\t        if self.estimate_uncertainty:\n\t            up_uncert_components3 = nn.functional.interpolate(\n\t                input=uncert_components3, size=(h_2, w_2), mode='bilinear', align_corners=False)\n\t        up_flow_3_warping = up_flow3.clone()\n\t        up_flow_3_warping[:, 0, :, :] *= w_2 / float(w_original)\n\t        up_flow_3_warping[:, 1, :, :] *= h_2 / float(h_original)\n", "        warp2 = warp(c22, up_flow_3_warping)\n\t        # constrained correlation\n\t        corr2 = self.local_corr(warp2, c12)\n\t        if self.estimate_uncertainty:\n\t            inp_flow_dec2 = torch.cat(\n\t                (corr2, up_flow3, up_uncert_components3), 1)\n\t        else:\n\t            inp_flow_dec2 = torch.cat((corr2, up_flow3), 1)\n\t        res_flow2, x2 = self.decoder2(inp_flow_dec2)\n\t        flow2 = res_flow2 + up_flow3\n", "        if self.estimate_uncertainty:\n\t            uncert_components2 = self.estimate_uncertainty_components2(\n\t                corr2, x2, up_uncert_components3, up_flow3)\n\t        # level 1: 1/4 of original resolution\n\t        h_1, w_1 = c11.shape[-2:]\n\t        up_flow2 = nn.functional.interpolate(input=flow2, size=(\n\t            h_1, w_1), mode='bilinear', align_corners=False)\n\t        if self.estimate_uncertainty:\n\t            up_uncert_components2 = nn.functional.interpolate(\n\t                input=uncert_components2, size=(h_1, w_1), mode='bilinear', align_corners=False)\n", "        up_feat2 = nn.functional.interpolate(input=x2, size=(\n\t            h_1, w_1), mode='bilinear', align_corners=False)\n\t        up_feat2 = self.reduce(up_feat2)\n\t        up_flow_2_warping = up_flow2.clone()\n\t        up_flow_2_warping[:, 0, :, :] *= w_1 / float(w_original)\n\t        up_flow_2_warping[:, 1, :, :] *= h_1 / float(h_original)\n\t        warp1 = warp(c21, up_flow_2_warping)\n\t        corr1 = self.local_corr(warp1, c11)\n\t        if self.estimate_uncertainty:\n\t            inp_flow_dec1 = torch.cat(\n", "                (corr1, up_flow2, up_feat2, up_uncert_components2), 1)\n\t        else:\n\t            inp_flow_dec1 = torch.cat((corr1, up_flow2, up_feat2), 1)\n\t        res_flow1, x = self.decoder1(inp_flow_dec1)\n\t        if self.refinement_at_finest_level:\n\t            res_flow1 = res_flow1 + self.refinement_module_finest(x)\n\t        flow1 = res_flow1 + up_flow2\n\t        # upscale also flow4\n\t        flow4 = flow4_256.clone()\n\t        flow4[:, 0, :, :] *= w_original / float(w_256)\n", "        flow4[:, 1, :, :] *= h_original / float(h_256)\n\t        if self.estimate_uncertainty:\n\t            uncert_components1 = self.estimate_uncertainty_components1(\n\t                corr1, x, up_uncert_components2, up_flow2)\n\t            # APPROXIMATION FOR NON-SQUARE IMAGES --> use the diagonal\n\t            uncert_components4 = uncert_components4_256.clone()\n\t            uncert_components4[:, 0, :, :] += 2 * \\\n\t                math.log(diag_original / float(diag_256))\n\t            return (flow4, uncert_components4), (flow3, uncert_components3), (flow2, uncert_components2), (flow1, uncert_components1)\n\t        return flow4, flow3, flow2, flow1\n", "    @staticmethod\n\t    def unnormalise_and_convert_mapping_to_flow(map, output_channel_first=True):\n\t        \"\"\"\n\t        ---------------------------------------------------------------------------\n\t        Copyright (c) Prune Truong. All rights reserved.\n\t        This source code is licensed under the license found in the\n\t        LICENSE file in https://github.com/PruneTruong/DenseMatching.\n\t        ---------------------------------------------------------------------------\n\t        \"\"\"\n\t        if map.shape[1] != 2:\n", "            # load_size is BxHxWx2\n\t            map = map.permute(0, 3, 1, 2)\n\t        # channel first, here map is normalised to -1;1\n\t        # we put it back to 0,W-1, then convert it to flow\n\t        B, C, H, W = map.size()\n\t        mapping = torch.zeros_like(map)\n\t        # mesh grid\n\t        mapping[:, 0, :, :] = (map[:, 0, :, :] + 1) * \\\n\t            (W - 1) / 2.0  # unormalise\n\t        mapping[:, 1, :, :] = (map[:, 1, :, :] + 1) * \\\n", "            (H - 1) / 2.0  # unormalise\n\t        xx = torch.arange(0, W, dtype=mapping.dtype,\n\t                        device=mapping.device).view(1, -1).repeat(H, 1)\n\t        yy = torch.arange(0, H, dtype=mapping.dtype,\n\t                        device=mapping.device).view(-1, 1).repeat(1, W)\n\t        xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n\t        yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n\t        grid = torch.cat((xx, yy), 1)\n\t        flow = mapping - grid  # here also channel first\n\t        if not output_channel_first:\n", "            flow = flow.permute(0, 2, 3, 1)\n\t        return flow\n\t    def load_weights(self, pretrain_path):\n\t        if pretrain_path is None:\n\t            return\n\t        if pretrain_path == 'megadepth':\n\t            pretrain_path = model_urls['megadepth']\n\t        if os.path.exists(pretrain_path):\n\t            checkpoint = torch.load(\n\t                pretrain_path, map_location=lambda storage, loc: storage)\n", "        elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrain_path)):\n\t            checkpoint = torch.load(os.path.join(os.environ.get(\n\t                'TORCH_HOME', ''), 'hub', pretrain_path), map_location=lambda storage, loc: storage)\n\t        else:\n\t            checkpoint = torch.hub.load_state_dict_from_url(\n\t                pretrain_path, progress=True, map_location=lambda storage, loc: storage)\n\t        if 'state_dict' in checkpoint.keys():\n\t            state_dict = checkpoint['state_dict']\n\t        else:\n\t            state_dict = checkpoint\n", "        new_state_dict = {}\n\t        for k, v in state_dict.items():\n\t            if k.startswith('alignment_head.'):\n\t                new_k = k.replace('alignment_head.', '')\n\t            else:\n\t                continue  # ignore the rest\n\t            new_state_dict[new_k] = v\n\t        self.load_state_dict(new_state_dict, strict=True)\n"]}
{"filename": "models/heads/deeplabv2.py", "chunked_list": ["import os\n\tfrom typing import List, Optional, Union\n\timport torch\n\timport torch.nn as nn\n\tfrom .base import BaseHead\n\tmodel_urls = {\n\t    # DeepLabv2 trained on Cityscapes\n\t    \"cityscapes\": \"https://data.vision.ee.ethz.ch/brdavid/coma/deeplabv2_cityscapes.pth\"\n\t}\n\tclass DeepLabV2Head(BaseHead):\n", "    def __init__(self,\n\t                 in_channels: int,\n\t                 in_index: Union[List[int], int],\n\t                 num_classes: int,\n\t                 input_transform: Optional[str] = None,\n\t                 dilation_series: List[int] = [6, 12, 18, 24],\n\t                 padding_series: List[int] = [6, 12, 18, 24],\n\t                 pretrained: Optional[str] = None):\n\t        super().__init__(num_classes, in_index, input_transform)\n\t        self.conv2d_list = nn.ModuleList()\n", "        for dilation, padding in zip(dilation_series, padding_series):\n\t            self.conv2d_list.append(\n\t                nn.Conv2d(in_channels, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias=True))\n\t        self.init_weights()\n\t        self.load_pretrained(pretrained)\n\t    def init_weights(self):\n\t        for m in self.conv2d_list:\n\t            m.weight.data.normal_(0, 0.01)\n\t            m.bias.data.zero_()\n\t    def forward(self, x):\n", "        x = self._transform_inputs(x)\n\t        return sum(stage(x) for stage in self.conv2d_list)\n\t    def load_pretrained(self, pretrained):\n\t        if pretrained is None:\n\t            return\n\t        if pretrained == 'cityscapes':\n\t            pretrained = model_urls['cityscapes']\n\t        if os.path.exists(pretrained):\n\t            checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n\t        elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n", "            checkpoint = torch.load(os.path.join(os.environ.get(\n\t                'TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n\t        else:\n\t            checkpoint = torch.hub.load_state_dict_from_url(\n\t                pretrained, progress=True, map_location=lambda storage, loc: storage)\n\t        if 'state_dict' in checkpoint:\n\t            state_dict = checkpoint['state_dict']\n\t        else:\n\t            state_dict = checkpoint\n\t        new_state_dict = {}\n", "        for k, v in state_dict.items():\n\t            if k.startswith('head.'):\n\t                new_k = k.replace('head.', '')\n\t                new_state_dict[new_k] = v\n\t            else:\n\t                pass\n\t        self.load_state_dict(new_state_dict, strict=True)\n"]}
