{"filename": "hub.py", "chunked_list": ["# Copyleft (c), Speech Lab, NTU, Taiwan\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# This code only add three upstream models based on the following code:\n\t# https://github.com/s3prl/s3prl/blob/v0.4.10/s3prl/hub.py\n\tfrom s3prl.downstream.timit_phone.hubconf import timit_posteriorgram\n\tfrom s3prl.upstream.apc.hubconf import *\n\tfrom s3prl.upstream.ast.hubconf import *\n\tfrom s3prl.upstream.audio_albert.hubconf import *\n\tfrom s3prl.upstream.baseline.hubconf import *\n\tfrom s3prl.upstream.byol_a.hubconf import *\n", "from s3prl.upstream.byol_s.hubconf import *\n\tfrom s3prl.upstream.cpc.hubconf import *\n\tfrom s3prl.upstream.data2vec.hubconf import *\n\tfrom s3prl.upstream.decoar2.hubconf import *\n\tfrom s3prl.upstream.decoar.hubconf import *\n\tfrom s3prl.upstream.decoar_layers.hubconf import *\n\tfrom s3prl.upstream.distiller.hubconf import *\n\tfrom s3prl.upstream.example.hubconf import *\n\tfrom s3prl.upstream.hf_hubert.hubconf import *\n\tfrom s3prl.upstream.hf_wav2vec2.hubconf import *\n", "from s3prl.upstream.hubert.hubconf import *\n\tfrom s3prl.upstream.lighthubert.hubconf import *\n\tfrom s3prl.upstream.log_stft.hubconf import *\n\tfrom s3prl.upstream.mae_ast.hubconf import *\n\tfrom s3prl.upstream.mockingjay.hubconf import *\n\tfrom s3prl.upstream.mos_prediction.hubconf import *\n\tfrom s3prl.upstream.npc.hubconf import *\n\tfrom s3prl.upstream.pase.hubconf import *\n\tfrom s3prl.upstream.passt.hubconf import *\n\tfrom s3prl.upstream.roberta.hubconf import *\n", "from s3prl.upstream.ssast.hubconf import *\n\tfrom s3prl.upstream.tera.hubconf import *\n\tfrom s3prl.upstream.unispeech_sat.hubconf import *\n\tfrom s3prl.upstream.vggish.hubconf import *\n\tfrom s3prl.upstream.vq_apc.hubconf import *\n\tfrom s3prl.upstream.vq_wav2vec.hubconf import *\n\tfrom s3prl.upstream.wav2vec2.hubconf import *\n\tfrom s3prl.upstream.wav2vec.hubconf import *\n\tfrom s3prl.upstream.wavlm.hubconf import *\n\tfrom s3prl.upstream.hf_nlp_ssl.hubconf import *\n", "from s3prl.upstream.hf_speechssl_no_pretrained_weights.hubconf import *\n\tfrom s3prl.upstream.embedding.hubconf import *\n\tdef options(only_registered_ckpt: bool = False):\n\t    all_options = []\n\t    for name, value in globals().items():\n\t        torch_hubconf_policy = not name.startswith(\"_\") and callable(value)\n\t        if torch_hubconf_policy and name != \"options\":\n\t            if only_registered_ckpt and (\n\t                name.endswith(\"_local\")\n\t                or name.endswith(\"_url\")\n", "                or name.endswith(\"_gdriveid\")\n\t                or name.endswith(\"_custom\")\n\t            ):\n\t                continue\n\t            all_options.append(name)\n\t    return all_options\n"]}
{"filename": "downstream/glue/model.py", "chunked_list": ["import torch.nn as nn\n\tclass SequenceClassifierWithCLSPooling(nn.Module):\n\t    def __init__(self, input_dim, output_dim, pooling_dim=None, dropout=0.1, **kwargs):\n\t        super(SequenceClassifierWithCLSPooling, self).__init__()\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.pooling_dim = pooling_dim\n\t        if pooling_dim is None:\n\t            pooling_dim = input_dim\n\t        else:\n\t            self.pooling_linear = nn.Linear(input_dim, pooling_dim)\n", "            self.pooling_activation = nn.Tanh()\n\t        self.classifier = nn.Linear(pooling_dim, output_dim)\n\t    def forward(self, features, features_len=None):\n\t        # features: BxTxF\n\t        # The first token output corresponding to [CLS] token for the BERT model\n\t        cls_tensor = features[:, 0, :]\n\t        if self.pooling_dim is not None:\n\t            cls_tensor = self.pooling_linear(cls_tensor)\n\t            cls_tensor = self.pooling_activation(cls_tensor)\n\t        cls_tensor = self.dropout(cls_tensor)\n", "        logits = self.classifier(cls_tensor)\n\t        return logits, None\n"]}
{"filename": "downstream/glue/expert.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\t# This code follows the downstream interface of S3PRL\n\t# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\tfrom pathlib import Path\n\timport evaluate\n\timport torch\n\timport torch.nn as nn\n", "from torch.distributed import is_initialized\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tfrom torch.utils.data import DataLoader, DistributedSampler\n\tfrom ..model import *\n\tfrom .dataset import GLUEDataset\n\tfrom .model import *\n\ttask_to_metrics = {\n\t    \"cola\": (\"matthews_correlation\", None),\n\t    \"mnli\": (\"accuracy\", None),\n\t    \"mrpc\": (\"accuracy\", \"f1\"),\n", "    \"qnli\": (\"accuracy\", None),\n\t    \"qqp\": (\"accuracy\", \"f1\"),\n\t    \"rte\": (\"accuracy\", None),\n\t    \"sst2\": (\"accuracy\", None),\n\t    \"stsb\": (\"pearson\", \"spearmanr\"),\n\t    \"wnli\": (\"accuracy\", None),\n\t}\n\tclass DownstreamExpert(nn.Module):\n\t    def __init__(self, upstream_dim, downstream_expert, expdir, **kwargs):\n\t        super(DownstreamExpert, self).__init__()\n", "        self.upstream_dim = upstream_dim\n\t        self.datarc = downstream_expert[\"datarc\"]\n\t        self.modelrc = downstream_expert[\"modelrc\"]\n\t        self.expdir = expdir\n\t        self.upstream_ckpt = kwargs[\"upstream_ckpt\"]\n\t        # define a task\n\t        self.glue_task = self.datarc[\"glue_task\"]\n\t        self.is_regression = self.glue_task == \"stsb\"\n\t        if not self.is_regression:\n\t            self.objective = nn.CrossEntropyLoss()\n", "            self.num_class = GLUEDataset(\n\t                \"train\", upstream_ckpt=self.upstream_ckpt, **self.datarc\n\t            ).num_class\n\t        else:\n\t            self.objective = nn.MSELoss()\n\t            self.num_class = 1\n\t            print(f\"{self.glue_task} will be executed as a regression task\")\n\t        model_cls = eval(self.modelrc[\"select\"])\n\t        model_conf = self.modelrc.get(self.modelrc[\"select\"], {})\n\t        projector_dim = self.modelrc.get(\"projector_dim\", None)\n", "        if projector_dim is not None:\n\t            self.projector = nn.Linear(upstream_dim, self.modelrc[\"projector_dim\"])\n\t            model_input_dim = projector_dim\n\t        else:\n\t            self.projector = None\n\t            model_input_dim = upstream_dim\n\t        self.model = model_cls(\n\t            input_dim=model_input_dim,\n\t            output_dim=self.num_class,\n\t            **model_conf,\n", "        )\n\t        self.normalize = self.modelrc.get(\"tanh_normalization\", False)\n\t        if self.normalize:\n\t            print(\"Use Tanh normalization\")\n\t            self.norm_act_fn = nn.Tanh()\n\t        self.dropout = self.modelrc.get(\"dropout\", None)\n\t        if self.dropout is not None:\n\t            print(\"Use dropout after projection\")\n\t            self.dropout = nn.Dropout(self.dropout)\n\t        self.metric = evaluate.load(\"glue\", self.glue_task)\n", "        self.metric_keys1, self.metric_keys2 = task_to_metrics[self.glue_task]\n\t        self.expdir = expdir\n\t        self.register_buffer(\"best_metric1_score\", torch.zeros(1))\n\t        self.register_buffer(\"best_metric2_score\", torch.zeros(1))\n\t    def _get_train_dataloader(self, dataset):\n\t        sampler = DistributedSampler(dataset) if is_initialized() else None\n\t        return DataLoader(\n\t            dataset,\n\t            batch_size=self.datarc[\"train_batch_size\"],\n\t            shuffle=(sampler is None),\n", "            sampler=sampler,\n\t            num_workers=self.datarc[\"num_workers\"],\n\t            collate_fn=dataset.collate_fn,\n\t        )\n\t    def _get_eval_dataloader(self, dataset):\n\t        return DataLoader(\n\t            dataset,\n\t            batch_size=self.datarc[\"eval_batch_size\"],\n\t            shuffle=False,\n\t            num_workers=self.datarc[\"num_workers\"],\n", "            collate_fn=dataset.collate_fn,\n\t        )\n\t    # Interface\n\t    def get_dataloader(self, split):\n\t        if not hasattr(self, f\"{split}_dataset\"):\n\t            setattr(\n\t                self,\n\t                f\"{split}_dataset\",\n\t                GLUEDataset(split, upstream_ckpt=self.upstream_ckpt, **self.datarc),\n\t            )\n", "        if split == \"train\":\n\t            return self._get_train_dataloader(self.train_dataset)\n\t        else:\n\t            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\t    # Interface\n\t    def forward(self, mode, features, labels, filenames, records, **kwargs):\n\t        device = features[0].device\n\t        features_len = torch.IntTensor([len(feat) for feat in features]).to(\n\t            device=device\n\t        )\n", "        features = pad_sequence(features, batch_first=True)\n\t        if self.projector is not None:\n\t            features = self.projector(features)\n\t        if self.normalize:\n\t            features = self.norm_act_fn(features)\n\t        if self.dropout is not None:\n\t            features = self.dropout(features)\n\t        predicted, _ = self.model(features, features_len)\n\t        if not self.is_regression:\n\t            labels = torch.LongTensor(labels).to(features.device).view(-1)\n", "            predicted = predicted.view(-1, self.num_class)\n\t            predicted_id_value = torch.argmax(predicted, dim=-1)\n\t        else:\n\t            labels = torch.FloatTensor(labels).to(features.device).squeeze()\n\t            predicted = predicted.squeeze()\n\t            predicted_id_value = predicted\n\t        loss = self.objective(predicted, labels)\n\t        records[\"loss\"].append(loss.item())\n\t        records[\"filename\"] += filenames\n\t        records[\"predict\"] += predicted_id_value.cpu().flatten().tolist()\n", "        records[\"truth\"] += labels.cpu().flatten().tolist()\n\t        return loss\n\t    def dump_prediction(self, outpath, filename, pred, label, step=0):\n\t        with open(outpath, \"w\") as file:\n\t            line = [f\"{step},{f},{p},{l}\\n\" for f, p, l in zip(filename, pred, label)]\n\t            file.writelines(line)\n\t    # interface\n\t    def log_records(self, mode, records, logger, global_step, **kwargs):\n\t        dev_update1 = False\n\t        dev_update2 = False\n", "        save_names = []\n\t        # loss related\n\t        values = records[\"loss\"]\n\t        loss_average = torch.FloatTensor(values).mean().item()\n\t        logger.add_scalar(\n\t            f\"glue-{self.glue_task}/{mode}-loss\",\n\t            loss_average,\n\t            global_step=global_step,\n\t        )\n\t        # score related\n", "        results = self.metric.compute(\n\t            predictions=records[\"predict\"], references=records[\"truth\"]\n\t        )\n\t        print(f\"{mode}: {results}\")\n\t        for k in [self.metric_keys1, self.metric_keys2]:\n\t            if k is None:\n\t                continue\n\t            result = results[k]\n\t            logger.add_scalar(\n\t                f\"glue-{self.glue_task}/{mode}-{k}\",\n", "                result,\n\t                global_step=global_step,\n\t            )\n\t            with open(Path(self.expdir) / \"train.csv\", \"a\") as f:\n\t                f.write(f\"{mode},{global_step},{loss_average},{k},{result}\\n\")\n\t                if mode == \"dev\":\n\t                    if result > self.best_metric1_score and k == self.metric_keys1:\n\t                        dev_update1 = True\n\t                        self.best_metric1_score = torch.ones(1) * result\n\t                        f.write(\n", "                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n\t                        )\n\t                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\t                    if result > self.best_metric2_score and k == self.metric_keys2:\n\t                        dev_update2 = True\n\t                        self.best_metric2_score = torch.ones(1) * result\n\t                        f.write(\n\t                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n\t                        )\n\t                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n", "        if mode == \"test\":\n\t            self.dump_prediction(\n\t                Path(self.expdir) / f\"dump_{mode}.csv\",\n\t                records[\"filename\"],\n\t                records[\"predict\"],\n\t                records[\"truth\"],\n\t            )\n\t        elif mode == \"dev\" and dev_update1:\n\t            self.dump_prediction(\n\t                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys1}_best.csv\",\n", "                records[\"filename\"],\n\t                records[\"predict\"],\n\t                records[\"truth\"],\n\t                global_step,\n\t            )\n\t        elif mode == \"dev\" and dev_update2:\n\t            self.dump_prediction(\n\t                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys2}_best.csv\",\n\t                records[\"filename\"],\n\t                records[\"predict\"],\n", "                records[\"truth\"],\n\t                global_step,\n\t            )\n\t        return save_names\n"]}
{"filename": "downstream/glue/dataset.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\t# This code follows the downstream interface of S3PRL\n\t# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\timport os\n\timport numpy as np\n\timport pandas as pd\n\tfrom espnet2.bin.tts_inference import Text2Speech\n", "from torch.utils.data.dataset import Dataset\n\tfrom transformers import AutoTokenizer\n\ttask_to_keys = {\n\t    \"cola\": (\"sentence\", None),\n\t    \"mnli\": (\"premise\", \"hypothesis\"),\n\t    \"mrpc\": (\"sentence1\", \"sentence2\"),\n\t    \"qnli\": (\"question\", \"sentence\"),\n\t    \"qqp\": (\"question1\", \"question2\"),\n\t    \"rte\": (\"sentence1\", \"sentence2\"),\n\t    \"sst2\": (\"sentence\", None),\n", "    \"stsb\": (\"sentence1\", \"sentence2\"),\n\t    \"wnli\": (\"sentence1\", \"sentence2\"),\n\t}\n\tclass GLUEDataset(Dataset):\n\t    def __init__(self, split, glue_task, glue_root, upstream_ckpt, **kwargs):\n\t        super(GLUEDataset, self).__init__()\n\t        self.glue_task = glue_task\n\t        self.glue_root = glue_root\n\t        self.split_sets = kwargs[split]\n\t        self.glue_dir = os.path.join(glue_root, glue_task)\n", "        self.upstream_ckpt = upstream_ckpt\n\t        assert os.path.isdir(\n\t            self.glue_dir\n\t        ), \"Please first run `python downstream/glue_asr/data_prep.py -h` to get TTS version text file.\"\n\t        table_list = []\n\t        for item in self.split_sets:\n\t            file_path = os.path.join(self.glue_dir, item, \"data.csv\")\n\t            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n\t            table_list.append(pd.read_csv(file_path))\n\t        self.sentence1_key, self.sentence2_key = task_to_keys[self.glue_task]\n", "        self.df_dataset = pd.concat(table_list)\n\t        assert len(self.df_dataset) != 0, f\"0 data found for {split}\"\n\t        self.num_class = len(set(list(self.df_dataset[\"label\"])))\n\t        # tokenizer\n\t        self.use_phoneme = kwargs.get(\"use_phoneme\", False)\n\t        if self.use_phoneme:\n\t            print(\"Use phoneme tokenizer\")\n\t            # ckpt is the ESPnet TTS model name such as \"kan-bayashi/ljspeech_vits\"\n\t            text2speech = Text2Speech.from_pretrained(self.upstream_ckpt, device=\"cuda\")\n\t            self.proc_fn = text2speech.preprocess_fn\n", "            self.text_name = self.proc_fn.text_name\n\t        else:\n\t            use_fast_tokenizer = kwargs.get(\"use_fast_tokenizer\", True)\n\t            self.tokenizer = AutoTokenizer.from_pretrained(\n\t                self.upstream_ckpt,\n\t                cache_dir=\"data\",\n\t                use_fast=use_fast_tokenizer,\n\t            )\n\t            self.max_seq_length = self.tokenizer.model_max_length\n\t            # whether to distinguish the first and second sentence\n", "            self.use_segment_emb = kwargs.get(\"use_segment_emb\", True)\n\t            # whether to use only text token embedding (if True, not use [CLS], [SEP]...)\n\t            self.use_only_text_token = kwargs.get(\"use_only_text_token\", False)\n\t    def _x_name(self, index):\n\t        return self.glue_task + \"_\" + str(index)\n\t    def _load_text(self, index):\n\t        texts = (\n\t            (str(self.df_dataset[self.sentence1_key][index]),)\n\t            if self.sentence2_key is None\n\t            else (\n", "                str(self.df_dataset[self.sentence1_key][index]),\n\t                str(self.df_dataset[self.sentence2_key][index]),\n\t            )\n\t        )\n\t        return texts\n\t    def _load_bpe_token(self, args):\n\t        if self.use_only_text_token:\n\t            token_id_seq = self.tokenizer.tokenize(\n\t                *args, max_length=self.max_seq_length, truncation=True\n\t            )\n", "            token_id_seq = np.array(\n\t                self.tokenizer.convert_tokens_to_ids(token_id_seq)\n\t            ).reshape(1, 1, -1)\n\t        else:\n\t            if self.use_segment_emb:\n\t                token_id_seq = self.tokenizer(\n\t                    *args,\n\t                    max_length=self.max_seq_length,\n\t                    truncation=True,\n\t                    return_tensors=\"np\",\n", "                )\n\t                # not use attention_mask in order to pad later\n\t                # keys: ['input_ids', 'token_type_ids', 'attention_mask']\n\t                if set(token_id_seq.keys()) != set(\n\t                    [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n\t                ):\n\t                    raise ValueError(\n\t                        f\"Invalid tokenize output keys: {token_id_seq.keys()}\"\n\t                    )\n\t                token_id_seq = np.array(\n", "                    [\n\t                        token_id_seq[k]\n\t                        for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n\t                    ]\n\t                )  # KxBxT (K=3, B=1)\n\t            else:\n\t                token_id_seq = self.tokenizer(\n\t                    *args,\n\t                    max_length=self.max_seq_length,\n\t                    truncation=True,\n", "                    return_tensors=\"np\",\n\t                )[\n\t                    \"input_ids\"\n\t                ]  # BxT\n\t                token_id_seq = np.expand_dims(token_id_seq, 0)  # KxBxT (K=1, B=1)\n\t        if token_id_seq.shape[1] != 1:\n\t            raise ValueError(f\"Invalid batch size ({token_id_seq.shape[1]})\")\n\t        return token_id_seq[:, 0, :].transpose(1, 0)  # KxT -> TxK\n\t    def _load_phoneme_token(self, args):\n\t        if self.sentence2_key is None:\n", "            token_id_seq = self.proc_fn._text_process({self.text_name: args[0]})[\"text\"]\n\t        else:\n\t            token_id_seq = np.concatenate(\n\t                [\n\t                    self.proc_fn._text_process({self.text_name: args[0]})[\"text\"],\n\t                    np.array([-1]),\n\t                    self.proc_fn._text_process({self.text_name: args[1]})[\"text\"],\n\t                ]\n\t            )\n\t        # adding two value for [PAD] and [SEP] token (define [PAD] and [SEP] token as 0 and 1)\n", "        token_id_seq += 2\n\t        return token_id_seq  # T\n\t    def __len__(self):\n\t        return len(self.df_dataset)\n\t    def __getitem__(self, index):\n\t        label = self.df_dataset[\"label\"][index]\n\t        filename = self._x_name(index)\n\t        texts = self._load_text(index)\n\t        if self.use_phoneme:\n\t            token_seq = self._load_phoneme_token(texts)\n", "        else:\n\t            token_seq = self._load_bpe_token(texts)\n\t        return token_seq, label, filename\n\t    def collate_fn(self, samples):\n\t        return zip(*samples)\n"]}
{"filename": "downstream/glue/__init__.py", "chunked_list": []}
{"filename": "downstream/speechglue/mk_white_noise.py", "chunked_list": ["import os\n\timport sys\n\timport numpy as np\n\timport soundfile\n\tSAMPLE_RATE = 16000\n\tnp.random.seed(seed=0)\n\tif len(sys.argv) == 1:\n\t    duration_msec = 50\n\t    use_50ms = True\n\telse:\n", "    duration_msec = int(sys.argv[1])\n\t    use_50ms = False\n\tduration = int(SAMPLE_RATE * duration_msec / 1000)\n\tsep_sig = np.random.randn(duration)\n\t# prevent from a saturation\n\tsep_sig = sep_sig / np.max(np.abs(sep_sig)) * 0.99\n\tif use_50ms:\n\t    out_path = os.path.join(\"dump\", \"white_noise.wav\")\n\telse:\n\t    out_path = os.path.join(\"dump\", f\"white_noise_{duration_msec}ms.wav\")\n", "soundfile.write(out_path, sep_sig, SAMPLE_RATE, \"PCM_16\")\n"]}
{"filename": "downstream/speechglue/model.py", "chunked_list": ["import torch.nn as nn\n\t# import the various pooling layers from:\n\t# https://github.com/s3prl/s3prl/blob/main/s3prl/downstream/model.py\n\tfrom ..model import *\n\tclass SequenceClassifierWithDropout(nn.Module):\n\t    def __init__(\n\t        self,\n\t        input_dim,\n\t        output_dim,\n\t        pooling=\"MeanPooling\",\n", "        activation=\"ReLU\",\n\t        dropout=0.1,\n\t        pooling_dim=None,\n\t        **kwargs,\n\t    ):\n\t        super().__init__()\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.pooling_dim = pooling_dim\n\t        if pooling_dim is None:\n\t            pooling_dim = input_dim\n", "        else:\n\t            self.pooling_linear = nn.Linear(input_dim, pooling_dim)\n\t            self.pooling_activation = nn.Tanh()\n\t        self.pooler = eval(pooling)(input_dim=input_dim, activation=activation)\n\t        self.classifier = nn.Linear(pooling_dim, output_dim)\n\t    def forward(self, hidden_state, features_len=None):\n\t        pooled_tensor, features_len = self.pooler(hidden_state, features_len)\n\t        if self.pooling_dim is not None:\n\t            pooled_tensor = self.pooling_linear(pooled_tensor)\n\t            pooled_tensor = self.pooling_activation(pooled_tensor)\n", "        pooled_tensor = self.dropout(pooled_tensor)\n\t        logit = self.classifier(pooled_tensor)\n\t        return logit, None\n"]}
{"filename": "downstream/speechglue/expert.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\t# This code follows the downstream interface of S3PRL\n\t# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\tfrom pathlib import Path\n\timport evaluate\n\timport torch\n\timport torch.nn as nn\n", "from torch.distributed import is_initialized\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tfrom torch.utils.data import DataLoader, DistributedSampler\n\tfrom ..model import *\n\tfrom .dataset import SpeechGLUEDataset\n\tfrom .model import *\n\ttask_to_metrics = {\n\t    \"cola\": (\"matthews_correlation\", None),\n\t    \"mnli\": (\"accuracy\", None),\n\t    \"mrpc\": (\"accuracy\", \"f1\"),\n", "    \"qnli\": (\"accuracy\", None),\n\t    \"qqp\": (\"accuracy\", \"f1\"),\n\t    \"rte\": (\"accuracy\", None),\n\t    \"sst2\": (\"accuracy\", None),\n\t    \"stsb\": (\"pearson\", \"spearmanr\"),\n\t    \"wnli\": (\"accuracy\", None),\n\t}\n\tclass DownstreamExpert(nn.Module):\n\t    \"\"\"\n\t    Used to handle downstream-specific operations\n", "    eg. downstream forward, metric computation, contents to log\n\t    \"\"\"\n\t    def __init__(self, upstream_dim, downstream_expert, expdir, **kwargs):\n\t        super(DownstreamExpert, self).__init__()\n\t        self.upstream_dim = upstream_dim\n\t        self.datarc = downstream_expert[\"datarc\"]\n\t        self.modelrc = downstream_expert[\"modelrc\"]\n\t        self.expdir = expdir\n\t        # define a task\n\t        self.speechglue_task = self.datarc[\"speechglue_task\"]\n", "        self.is_regression = self.speechglue_task == \"stsb\"\n\t        if not self.is_regression:\n\t            self.objective = nn.CrossEntropyLoss()\n\t            self.num_class = SpeechGLUEDataset(\"train\", **self.datarc).num_class\n\t        else:\n\t            self.objective = nn.MSELoss()\n\t            self.num_class = 1\n\t            print(f\"{self.speechglue_task} will be executed as a regression task\")\n\t        model_cls = eval(self.modelrc[\"select\"])\n\t        model_conf = self.modelrc.get(self.modelrc[\"select\"], {})\n", "        projector_dim = self.modelrc.get(\"projector_dim\", None)\n\t        if projector_dim is not None:\n\t            self.projector = nn.Linear(upstream_dim, self.modelrc[\"projector_dim\"])\n\t            model_input_dim = projector_dim\n\t        else:\n\t            self.projector = None\n\t            model_input_dim = upstream_dim\n\t        self.model = model_cls(\n\t            input_dim=model_input_dim,\n\t            output_dim=self.num_class,\n", "            **model_conf,\n\t        )\n\t        self.normalize = self.modelrc.get(\"tanh_normalization\", False)\n\t        if self.normalize:\n\t            print(\"Use Tanh normalization\")\n\t            self.norm_act_fn = nn.Tanh()\n\t        self.dropout = self.modelrc.get(\"dropout\", None)\n\t        if self.dropout is not None:\n\t            print(\"Use dropout after projection\")\n\t            self.dropout = nn.Dropout(self.dropout)\n", "        self.late_concat = self.datarc.get(\"late_concat\", False)\n\t        self.metric = evaluate.load(\"glue\", self.speechglue_task)\n\t        self.metric_keys1, self.metric_keys2 = task_to_metrics[self.speechglue_task]\n\t        self.expdir = expdir\n\t        self.register_buffer(\"best_metric1_score\", torch.zeros(1))\n\t        self.register_buffer(\"best_metric2_score\", torch.zeros(1))\n\t    def _get_train_dataloader(self, dataset):\n\t        sampler = DistributedSampler(dataset) if is_initialized() else None\n\t        return DataLoader(\n\t            dataset,\n", "            batch_size=self.datarc[\"train_batch_size\"],\n\t            shuffle=(sampler is None),\n\t            sampler=sampler,\n\t            num_workers=self.datarc[\"num_workers\"],\n\t            collate_fn=dataset.collate_fn,\n\t        )\n\t    def _get_eval_dataloader(self, dataset):\n\t        return DataLoader(\n\t            dataset,\n\t            batch_size=self.datarc[\"eval_batch_size\"],\n", "            shuffle=False,\n\t            num_workers=self.datarc[\"num_workers\"],\n\t            collate_fn=dataset.collate_fn,\n\t        )\n\t    # Interface\n\t    def get_dataloader(self, split):\n\t        if not hasattr(self, f\"{split}_dataset\"):\n\t            setattr(self, f\"{split}_dataset\", SpeechGLUEDataset(split, **self.datarc))\n\t        if split == \"train\":\n\t            return self._get_train_dataloader(self.train_dataset)\n", "        else:\n\t            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\t    # Interface\n\t    def forward(self, mode, features, labels, filenames, records, **kwargs):\n\t        # add zeros corresponding to [SEP] token\n\t        if self.late_concat:\n\t            features = self.separate_and_concate_data(features)\n\t        device = features[0].device\n\t        features_len = torch.IntTensor([len(feat) for feat in features]).to(\n\t            device=device\n", "        )\n\t        features = pad_sequence(features, batch_first=True)\n\t        if self.projector is not None:\n\t            features = self.projector(features)\n\t        if self.normalize:\n\t            features = self.norm_act_fn(features)\n\t        if self.dropout is not None:\n\t            features = self.dropout(features)\n\t        predicted, _ = self.model(features, features_len)\n\t        if not self.is_regression:\n", "            labels = torch.LongTensor(labels).to(features.device).view(-1)\n\t            predicted = predicted.view(-1, self.num_class)\n\t            predicted_id_value = torch.argmax(predicted, dim=-1)\n\t        else:\n\t            labels = torch.FloatTensor(labels).to(features.device).squeeze()\n\t            predicted = predicted.squeeze()\n\t            predicted_id_value = predicted\n\t        loss = self.objective(predicted, labels)\n\t        # records[\"acc\"] += (predicted_id_value == labels).view(-1).cpu().float().tolist()\n\t        records[\"loss\"].append(loss.item())\n", "        records[\"filename\"] += filenames\n\t        records[\"predict\"] += predicted_id_value.cpu().flatten().tolist()\n\t        records[\"truth\"] += labels.cpu().flatten().tolist()\n\t        return loss\n\t    def separate_and_concate_data(self, feats):\n\t        # feats is list of tensors(TxF)\n\t        assert len(feats) % 2 == 0\n\t        total_num = len(feats) // 2\n\t        sep_vec = torch.zeros(\n\t            [1, feats[0].shape[1]], dtype=feats[0].dtype, device=feats[0].device\n", "        )\n\t        return [\n\t            torch.cat((feats[i], sep_vec, feats[i + total_num]), dim=0)\n\t            for i in range(total_num)\n\t        ]\n\t    def dump_prediction(self, outpath, filename, pred, label, step=0):\n\t        with open(outpath, \"w\") as file:\n\t            line = [f\"{step},{f},{p},{l}\\n\" for f, p, l in zip(filename, pred, label)]\n\t            file.writelines(line)\n\t    # interface\n", "    def log_records(self, mode, records, logger, global_step, **kwargs):\n\t        dev_update1 = False\n\t        dev_update2 = False\n\t        save_names = []\n\t        # loss related\n\t        values = records[\"loss\"]\n\t        loss_average = torch.FloatTensor(values).mean().item()\n\t        logger.add_scalar(\n\t            f\"speechglue-{self.speechglue_task}/{mode}-loss\",\n\t            loss_average,\n", "            global_step=global_step,\n\t        )\n\t        # score related\n\t        results = self.metric.compute(\n\t            predictions=records[\"predict\"], references=records[\"truth\"]\n\t        )\n\t        print(f\"{mode}: {results}\")\n\t        for k in [self.metric_keys1, self.metric_keys2]:\n\t            if k is None:\n\t                continue\n", "            result = results[k]\n\t            logger.add_scalar(\n\t                f\"speechglue-{self.speechglue_task}/{mode}-{k}\",\n\t                result,\n\t                global_step=global_step,\n\t            )\n\t            with open(Path(self.expdir) / \"train.csv\", \"a\") as f:\n\t                f.write(f\"{mode},{global_step},{loss_average},{k},{result}\\n\")\n\t                if mode == \"dev\":\n\t                    if result > self.best_metric1_score and k == self.metric_keys1:\n", "                        dev_update1 = True\n\t                        self.best_metric1_score = torch.ones(1) * result\n\t                        f.write(\n\t                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n\t                        )\n\t                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\t                    if result > self.best_metric2_score and k == self.metric_keys2:\n\t                        dev_update2 = True\n\t                        self.best_metric2_score = torch.ones(1) * result\n\t                        f.write(\n", "                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n\t                        )\n\t                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\t        if mode == \"test\":\n\t            self.dump_prediction(\n\t                Path(self.expdir) / f\"dump_{mode}.csv\",\n\t                records[\"filename\"],\n\t                records[\"predict\"],\n\t                records[\"truth\"],\n\t            )\n", "        elif mode == \"dev\" and dev_update1:\n\t            self.dump_prediction(\n\t                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys1}_best.csv\",\n\t                records[\"filename\"],\n\t                records[\"predict\"],\n\t                records[\"truth\"],\n\t                global_step,\n\t            )\n\t        elif mode == \"dev\" and dev_update2:\n\t            self.dump_prediction(\n", "                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys2}_best.csv\",\n\t                records[\"filename\"],\n\t                records[\"predict\"],\n\t                records[\"truth\"],\n\t                global_step,\n\t            )\n\t        return save_names\n"]}
{"filename": "downstream/speechglue/dataset.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\t# This code follows the downstream interface of S3PRL\n\t# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\timport os\n\timport pandas as pd\n\timport torch\n\timport torchaudio\n", "from torch.utils.data.dataset import Dataset\n\tSAMPLE_RATE = 16000\n\tSEP_DURATION = int(0.05 * 16000)  # 50 ms\n\ttask_to_keys = {\n\t    \"cola\": (\"sentence\", None),\n\t    \"mnli\": (\"premise\", \"hypothesis\"),\n\t    \"mrpc\": (\"sentence1\", \"sentence2\"),\n\t    \"qnli\": (\"question\", \"sentence\"),\n\t    \"qqp\": (\"question1\", \"question2\"),\n\t    \"rte\": (\"sentence1\", \"sentence2\"),\n", "    \"sst2\": (\"sentence\", None),\n\t    \"stsb\": (\"sentence1\", \"sentence2\"),\n\t    \"wnli\": (\"sentence1\", \"sentence2\"),\n\t}\n\tclass SpeechGLUEDataset(Dataset):\n\t    def __init__(self, split, speechglue_task, speechglue_root, **kwargs):\n\t        super(SpeechGLUEDataset, self).__init__()\n\t        self.speechglue_task = speechglue_task\n\t        self.speechglue_root = speechglue_root\n\t        self.sample_rate = SAMPLE_RATE\n", "        self.split_sets = kwargs[split]\n\t        self.speechglue_dir = os.path.join(speechglue_root, speechglue_task)\n\t        # use a fixed random signal\n\t        sep_sig_length = kwargs.get(\"sep_sig_length\", 50)\n\t        if sep_sig_length == 50:\n\t            self.sep_sig_path = os.path.join(\n\t                \"downstream\", \"speechglue\", \"white_noise.wav\"\n\t            )\n\t        else:\n\t            print(f\"Use {sep_sig_length}ms SEP signal\")\n", "            self.sep_sig_path = os.path.join(\n\t                \"dump\", f\"white_noise_{sep_sig_length}ms.wav\"\n\t            )\n\t        self.late_concat = kwargs.get(\"late_concat\", False)\n\t        assert os.path.isdir(\n\t            self.speechglue_dir\n\t        ), \"Please first run `python downstream/speechglue_asr/data_prep.py -h` to get TTS file.\"\n\t        table_list = []\n\t        for item in self.split_sets:\n\t            file_path = os.path.join(self.speechglue_dir, item, \"data.csv\")\n", "            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n\t            table_list.append(pd.read_csv(file_path))\n\t        self.sentence1_key, self.sentence2_key = task_to_keys[self.speechglue_task]\n\t        self.df_dataset = pd.concat(table_list)\n\t        assert len(self.df_dataset) != 0, f\"0 data found for {split}\"\n\t        self.num_class = len(set(list(self.df_dataset[\"label\"])))\n\t        if not self.late_concat:\n\t            self.sep_sig, sr = torchaudio.load(self.sep_sig_path)\n\t    def _x_name(self, index):\n\t        return self.speechglue_task + \"_\" + str(index)\n", "    def _load_wav(self, index):\n\t        wav1, sr = torchaudio.load(self.df_dataset[\"file_\" + self.sentence1_key][index])\n\t        assert (\n\t            sr == self.sample_rate\n\t        ), f\"Sample rate mismatch: real {sr}, config {self.sample_rate}\"\n\t        if self.sentence2_key is None:\n\t            return wav1.view(-1)\n\t        else:\n\t            wav2, sr = torchaudio.load(\n\t                self.df_dataset[\"file_\" + self.sentence2_key][index]\n", "            )\n\t            assert (\n\t                sr == self.sample_rate\n\t            ), f\"Sample rate mismatch: real {sr}, config {self.sample_rate}\"\n\t            if not self.late_concat:\n\t                sep_sig = self.sep_sig.to(device=wav1.device, dtype=wav1.dtype)\n\t                return torch.cat((wav1.view(-1), sep_sig.view(-1), wav2.view(-1)))\n\t            else:\n\t                return wav1.view(-1), wav2.view(-1)\n\t    def __len__(self):\n", "        return len(self.df_dataset)\n\t    def __getitem__(self, index):\n\t        label = self.df_dataset[\"label\"][index]\n\t        filename = self._x_name(index)\n\t        if not self.late_concat:\n\t            wav = self._load_wav(index).numpy()\n\t            return wav, label, filename\n\t        else:\n\t            wav1, wav2 = self._load_wav(index)\n\t            return wav1.numpy(), wav2.numpy(), label, filename\n", "    def collate_fn(self, samples):\n\t        if not self.late_concat:\n\t            return zip(*samples)\n\t        else:\n\t            wavs1, wavs2, labels, filenames = zip(*samples)\n\t            all_wavs = wavs1 + wavs2\n\t            return all_wavs, labels, filenames\n"]}
{"filename": "downstream/speechglue/__init__.py", "chunked_list": []}
{"filename": "downstream/speechglue/data_prep.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\timport argparse\n\timport logging\n\timport math\n\timport os\n\timport re\n\timport unicodedata\n", "from builtins import str as unicode\n\timport numpy as np\n\timport soundfile\n\timport tacotron_cleaner.cleaners\n\tfrom datasets import load_dataset\n\tfrom espnet2.bin.tts_inference import Text2Speech\n\tfrom inflect import NumOutOfRangeError\n\tfrom librosa import resample\n\ttask_to_keys = {\n\t    \"cola\": (\"sentence\", None),\n", "    \"mnli\": (\"premise\", \"hypothesis\"),\n\t    \"mrpc\": (\"sentence1\", \"sentence2\"),\n\t    \"qnli\": (\"question\", \"sentence\"),\n\t    \"qqp\": (\"question1\", \"question2\"),\n\t    \"rte\": (\"sentence1\", \"sentence2\"),\n\t    \"sst2\": (\"sentence\", None),\n\t    \"stsb\": (\"sentence1\", \"sentence2\"),\n\t    \"wnli\": (\"sentence1\", \"sentence2\"),\n\t}\n\tdef get_parser():\n", "    parser = argparse.ArgumentParser(\n\t        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t        description=\"Data preparation for SpeechGLUE\",\n\t    )\n\t    parser.add_argument(\"--verbose\", \"-V\", default=1, type=int, help=\"Verbose option\")\n\t    parser.add_argument(\n\t        \"--data-dir\",\n\t        type=str,\n\t        default=\"data\",\n\t        help=\"Path to storing the original GLUE dataset downloaded from huggingface.co\",\n", "    )\n\t    parser.add_argument(\n\t        \"--dump-dir\",\n\t        type=str,\n\t        default=\"dump\",\n\t        help=\"Path to storing the SpeechGLUE dataset\",\n\t    )\n\t    parser.add_argument(\n\t        \"--device\",\n\t        type=str,\n", "        default=\"cuda\",\n\t        choices=[\"cuda\", \"cpu\"],\n\t        help=\"Pytorch device\",\n\t    )\n\t    parser.add_argument(\n\t        \"--num-workers\",\n\t        type=int,\n\t        default=1,\n\t        help=\"Number of workers for map() of TTS\",\n\t    )\n", "    parser.add_argument(\n\t        \"--glue-task\",\n\t        type=str,\n\t        default=\"all\",\n\t        choices=[\"all\"] + list(task_to_keys.keys()),\n\t        help=\"Name of the GLUE task for synthesizing\",\n\t    )\n\t    parser.add_argument(\n\t        \"--max-tts-sample\",\n\t        type=int,\n", "        default=None,\n\t        help=\"Number of limited examples for synthesizing (for testing purposes only)\",\n\t    )\n\t    parser.add_argument(\n\t        \"--tts-model\",\n\t        type=str,\n\t        default=\"kan-bayashi/ljspeech_vits\",\n\t        help=\"Name of ESPnet TTS model (listed in https://github.com/espnet/espnet_model_zoo)\",\n\t    )\n\t    return parser\n", "def text_normalization(text, idx=0):\n\t    # espnet-TTS uses the preprocessing sequence of text-cleaner & g2p\n\t    # e.g. kan-bayashi/ljspeech_vits configure with tacotron cleaner & g2p_en_no_space\n\t    # therefore, this code also uses same text-cleaner & text-processing in a g2p\n\t    # https://github.com/espnet/espnet_tts_frontend/blob/master/tacotron_cleaner/cleaners.py\n\t    # https://github.com/Kyubyong/g2p/blob/master/g2p_en/g2p.py\n\t    def _space_normalization(text_with_space):\n\t        # text normalization related with a space\n\t        t_list = text_with_space.split()\n\t        norm_list = []\n", "        i = 0\n\t        while i < len(t_list):\n\t            if i < len(t_list) - 1:\n\t                # merge two words with an apostrophe (e.g., \"can' t\" -> \"can't\")\n\t                if t_list[i + 1][0] == \"'\":\n\t                    norm_list.append(t_list[i] + t_list[i + 1])\n\t                    i += 1\n\t                # add space after comma (e.g., \",2000\" -> \", 2000\")\n\t                elif t_list[i + 1][0] == \",\":\n\t                    if t_list[i + 1] == \",\":\n", "                        norm_list.extend([t_list[i] + \",\"])\n\t                    else:\n\t                        norm_list.extend([t_list[i] + \",\", t_list[i + 1][1:].strip()])\n\t                    i += 1\n\t                # add space after period (e.g., \".2000\" -> \". 2000\")\n\t                elif t_list[i + 1][0] == \".\":\n\t                    if t_list[i + 1] == \".\":\n\t                        norm_list.extend([t_list[i] + \".\"])\n\t                    else:\n\t                        norm_list.extend([t_list[i] + \".\", t_list[i + 1][1:].strip()])\n", "                    i += 1\n\t                else:\n\t                    norm_list.append(t_list[i])\n\t            else:\n\t                norm_list.append(t_list[i])\n\t            i += 1\n\t        return \" \".join(norm_list)\n\t    norm_text = _space_normalization(\n\t        text.replace(\". . .\", \".\").replace(\"...\", \".\").replace(\"$,\", \"$\")\n\t    )\n", "    try:\n\t        norm_text = tacotron_cleaner.cleaners.custom_english_cleaners(norm_text)\n\t        # from https://github.com/Kyubyong/g2p/blob/master/g2p_en/g2p.py#L148\n\t        # but normalize_numbers() has already been applied in the custom_english_cleaners()\n\t        norm_text = unicode(norm_text)\n\t        norm_text = \"\".join(\n\t            char\n\t            for char in unicodedata.normalize(\"NFD\", norm_text)\n\t            if unicodedata.category(char) != \"Mn\"\n\t        )\n", "        norm_text = norm_text.lower()\n\t        norm_text = re.sub(\"[^ a-z'.,?!\\-]\", \"\", norm_text)\n\t        norm_text = norm_text.replace(\"i.e.\", \"that is\")\n\t        norm_text = norm_text.replace(\"e.g.\", \"for example\")\n\t        # space-related normalization again after removing some punctuations\n\t        norm_text = _space_normalization(norm_text)\n\t    except (RuntimeError, NumOutOfRangeError) as e:\n\t        # Some sentences can't be tokenized to vocode.\n\t        # E.g., contain only symbols such as \"(\" and \")\"\n\t        # E.g., contain a number out of range (i.e., NumOutOfRangeError of https://github.com/jaraco/inflect)\n", "        logging.warning(\n\t            f\"{e}\\n\"\n\t            + \"Invalid sentence may be inputted and this column will be deleted.\"\n\t            + f\" (sentence: {text}, idx: {idx})\"\n\t        )\n\t        norm_text = None\n\t    if norm_text == \"\":\n\t        norm_text = None\n\t        logging.warning(\n\t            \"Invalid sentence may be inputted and this column will be deleted.\"\n", "            + f\" (sentence: {text}, idx: {idx})\"\n\t        )\n\t    return norm_text\n\tdef main():\n\t    parser = get_parser()\n\t    args = parser.parse_args()\n\t    # Setup logging\n\t    if args.verbose > 1:\n\t        logging.basicConfig(\n\t            level=logging.DEBUG,\n", "            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n\t        )\n\t    elif args.verbose > 0:\n\t        logging.basicConfig(\n\t            level=logging.INFO,\n\t            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n\t        )\n\t    else:\n\t        logging.basicConfig(\n\t            level=logging.WARN,\n", "            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n\t        )\n\t        logging.warning(\"Skip DEBUG/INFO messages\")\n\t    # check args\n\t    if args.num_workers > 1 and args.device == \"cuda\":\n\t        logging.warning(\"only single GPU decoding is supported\")\n\t        args.num_workers = 1\n\t    # instantiate the text-to-speech model\n\t    text2speech = Text2Speech.from_pretrained(args.tts_model, device=args.device)\n\t    if args.glue_task == \"all\":\n", "        task_names = task_to_keys.keys()\n\t    else:\n\t        task_names = [args.glue_task]\n\t    for task_name in task_names:\n\t        logging.info(\"[\" + task_name + \"] Start dataset preparation\")\n\t        # set a data split\n\t        valid_column = \"validation_matched\" if task_name == \"mnli\" else \"validation\"\n\t        eval_column = \"test_matched\" if task_name == \"mnli\" else \"test\"\n\t        extend_column = (\n\t            [\"validation_mismatched\", \"test_mismatched\"] if task_name == \"mnli\" else []\n", "        )\n\t        data_splits = [\"train\", valid_column, eval_column] + extend_column\n\t        logging.info(\"[\" + task_name + \"] Splits of dataset = \" + str(data_splits))\n\t        sentence1_key, sentence2_key = task_to_keys[task_name]\n\t        # TTS function applied by a map()\n\t        def tts(examples):\n\t            # first sentence\n\t            dirname = str(math.floor(examples[\"idx\"] / 10000) * 10000)\n\t            outdir_base = os.path.abspath(\n\t                os.path.join(args.dump_dir, task_name, data_split)\n", "            )\n\t            wav_name = str(examples[\"idx\"]) + \".wav\"\n\t            out_path1 = os.path.join(outdir_base, sentence1_key, dirname, wav_name)\n\t            text1 = text_normalization(examples[sentence1_key], examples[\"idx\"])\n\t            if text1 is None:\n\t                out_path1 = None\n\t                length1 = None\n\t            else:\n\t                speech = text2speech(text1)[\"wav\"]\n\t                speech = resample(\n", "                    speech.cpu().numpy(),\n\t                    orig_sr=text2speech.fs,\n\t                    target_sr=16000,\n\t                    res_type=\"kaiser_best\",\n\t                )\n\t                length1 = speech.shape[0]\n\t                soundfile.write(out_path1, speech, 16000, \"PCM_16\")\n\t            if sentence2_key is None:\n\t                return {\n\t                    sentence1_key: text1,\n", "                    \"file_\" + sentence1_key: out_path1,\n\t                    \"length_\" + sentence1_key: length1,\n\t                }\n\t            # second sentence\n\t            out_path2 = os.path.join(outdir_base, sentence2_key, dirname, wav_name)\n\t            text2 = text_normalization(examples[sentence2_key], examples[\"idx\"])\n\t            if text2 is None:\n\t                out_path1 = None  # for filtering\n\t                out_path2 = None\n\t                length1 = None\n", "                length2 = None\n\t            else:\n\t                speech = text2speech(text2)[\"wav\"]\n\t                speech = resample(\n\t                    speech.cpu().numpy(),\n\t                    orig_sr=text2speech.fs,\n\t                    target_sr=16000,\n\t                    res_type=\"kaiser_best\",\n\t                )\n\t                length2 = speech.shape[0]\n", "                soundfile.write(out_path2, speech, 16000, \"PCM_16\")\n\t            return {\n\t                sentence1_key: text1,\n\t                sentence2_key: text2,\n\t                \"file_\" + sentence1_key: out_path1,\n\t                \"file_\" + sentence2_key: out_path2,\n\t                \"length_\" + sentence1_key: length1,\n\t                \"length_\" + sentence2_key: length2,\n\t            }\n\t        # initialize a dataset and generate synthesized speech data\n", "        logging.info(\"[\" + task_name + \"] Generating TTS data\")\n\t        for data_split in data_splits:\n\t            # take a dataset from HuggingFace's GLUE\n\t            raw_datasets = load_dataset(\n\t                \"glue\",\n\t                task_name,\n\t                split=data_split,\n\t                cache_dir=args.data_dir,\n\t                use_auth_token=None,\n\t            )\n", "            num_utt = len(raw_datasets)\n\t            logging.info(\n\t                f\"The number of rows of the original data of {data_split} split: {num_utt}\"\n\t            )\n\t            # make output directories\n\t            dirnames = np.arange(0, math.floor(num_utt / 10000) * 10000 + 1, 10000)\n\t            for dirname in dirnames:\n\t                os.makedirs(\n\t                    os.path.join(\n\t                        args.dump_dir,\n", "                        task_name,\n\t                        data_split,\n\t                        sentence1_key,\n\t                        str(dirname),\n\t                    ),\n\t                    exist_ok=True,\n\t                )\n\t                if sentence2_key is not None:\n\t                    os.makedirs(\n\t                        os.path.join(\n", "                            args.dump_dir,\n\t                            task_name,\n\t                            data_split,\n\t                            sentence2_key,\n\t                            str(dirname),\n\t                        ),\n\t                        exist_ok=True,\n\t                    )\n\t            # limit the number of examples for testing\n\t            if args.max_tts_sample is not None:\n", "                raw_datasets = raw_datasets.select(range(args.max_tts_sample))\n\t            # run a text-to-speech\n\t            tts_datasets = raw_datasets.map(\n\t                tts,\n\t                num_proc=args.num_workers,\n\t                desc=\"Running TTS on the \"\n\t                + data_split\n\t                + \" set of \"\n\t                + task_name\n\t                + \" dataset\",\n", "            )\n\t            # filter rows that could not TTS\n\t            tts_datasets = tts_datasets.filter(\n\t                lambda example: example[\"file_\" + sentence1_key] is not None\n\t            )\n\t            logging.info(\n\t                f\"The number of rows of the synthesized data of {data_split} split: {len(tts_datasets)}\\n\"\n\t                + \"-----------------------\"\n\t            )\n\t            # save the audio files with CSV format\n", "            tts_datasets.to_csv(\n\t                os.path.join(args.dump_dir, task_name, data_split, \"data.csv\"),\n\t                index=False,\n\t            )\n\t        logging.info(\"[\" + task_name + \"] Successfully finished dataset preparation\")\n\t    logging.info(\"All dataset preparation finished successfully\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "downstream/speechglue_asr/select_sample.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\timport argparse\n\timport os\n\timport random\n\timport pandas as pd\n\ttask_to_keys = {\n\t    \"cola\": (\"sentence\", None),\n", "    \"mnli\": (\"premise\", \"hypothesis\"),\n\t    \"mrpc\": (\"sentence1\", \"sentence2\"),\n\t    \"qnli\": (\"question\", \"sentence\"),\n\t    \"qqp\": (\"question1\", \"question2\"),\n\t    \"rte\": (\"sentence1\", \"sentence2\"),\n\t    \"sst2\": (\"sentence\", None),\n\t    \"stsb\": (\"sentence1\", \"sentence2\"),\n\t    \"wnli\": (\"sentence1\", \"sentence2\"),\n\t}\n\tdef get_parser():\n", "    parser = argparse.ArgumentParser(\n\t        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t        description=\"Data preparation for SpeechGLUE\",\n\t    )\n\t    parser.add_argument(\n\t        \"--dump-dir\",\n\t        type=str,\n\t        default=\"dump\",\n\t        help=\"Path to storing the SpeechGLUE dataset\",\n\t    )\n", "    parser.add_argument(\n\t        \"--glue-task\",\n\t        type=str,\n\t        default=\"all\",\n\t        choices=[\"all\"] + list(task_to_keys.keys()),\n\t        help=\"Name of the GLUE task\",\n\t    )\n\t    parser.add_argument(\n\t        \"--split\",\n\t        type=str,\n", "        default=\"train\",\n\t        choices=[\"train\", \"validation\", \"test\"],\n\t        help=\"Split of dataset\",\n\t    )\n\t    parser.add_argument(\n\t        \"--max-hours\",\n\t        type=int,\n\t        default=None,\n\t        help=\"Upper limit of time in hours\",\n\t    )\n", "    parser.add_argument(\n\t        \"--no-use-predefined-sampling\",\n\t        action=\"store_true\",\n\t        help=\"No predefined sampling\",\n\t    )\n\t    parser.add_argument(\n\t        \"--seed\",\n\t        type=int,\n\t        default=1,\n\t        help=\"Random seed value\",\n", "    )\n\t    return parser\n\tdef main():\n\t    parser = get_parser()\n\t    args = parser.parse_args()\n\t    random.seed(args.seed)\n\t    max_samples = int(args.max_hours * 60 * 60 * 16000)\n\t    if args.glue_task == \"all\":\n\t        task_names = task_to_keys.keys()\n\t    else:\n", "        task_names = [args.glue_task]\n\t    for task_name in task_names:\n\t        # set a data split\n\t        if args.split == \"train\":\n\t            data_splits = [\"train\"]\n\t        elif args.split == \"validation\":\n\t            data_splits = (\n\t                [\"validation_matched\", \"validation_mismatched\"]\n\t                if task_name == \"mnli\"\n\t                else [\"validation\"]\n", "            )\n\t        elif args.split == \"test\":\n\t            data_splits = (\n\t                [\"test_matched\", \"test_mismatched\"] if task_name == \"mnli\" else [\"test\"]\n\t            )\n\t        else:\n\t            raise ValueError(\n\t                \"args.split must be one of the ['train', 'validation', 'test']\"\n\t            )\n\t        for data_split in data_splits:\n", "            file_path = os.path.join(args.dump_dir, task_name, data_split, \"data.csv\")\n\t            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n\t            table = pd.read_csv(file_path)\n\t            sentence1_key, sentence2_key = task_to_keys[task_name]\n\t            file_paths = table[\"file_\" + sentence1_key].tolist()\n\t            labels = table[sentence1_key].tolist()\n\t            lengths = table[\"length_\" + sentence1_key].tolist()\n\t            if sentence2_key is not None:\n\t                file_paths.extend(table[\"file_\" + sentence2_key].tolist())\n\t                labels.extend(table[sentence2_key].tolist())\n", "                lengths.extend(table[\"length_\" + sentence2_key].tolist())\n\t            if sum(lengths) < max_samples:\n\t                current_hours = round(sum(lengths) / 16000 / 60 / 60, 1)\n\t                print(\n\t                    f\"The {data_split} set of {task_name} task ({current_hours}) is already less than {args.max_hours} hours.\"\n\t                )\n\t                continue\n\t            select_outdir = os.path.join(\n\t                \"downstream\", \"speechglue_asr\", \"selected_uttids\"\n\t            )\n", "            select_filename = task_name + \"_\" + data_split + \".list\"\n\t            if args.no_use_predefined_sampling:\n\t                uttids = list(range(len(file_paths)))\n\t                random.shuffle(uttids)\n\t            else:\n\t                with open(os.path.join(select_outdir, select_filename), \"r\") as f:\n\t                    uttids = [int(i) for i in f.read().splitlines()]\n\t            num_sample = 0\n\t            file_paths_lim = []\n\t            labels_lim = []\n", "            lengths_lim = []\n\t            uttids_lim = []\n\t            for uttid in uttids:\n\t                num_sample += lengths[uttid]\n\t                if num_sample < max_samples:\n\t                    file_paths_lim.append(file_paths[uttid])\n\t                    labels_lim.append(labels[uttid])\n\t                    lengths_lim.append(lengths[uttid])\n\t                    uttids_lim.append(uttid)\n\t                else:\n", "                    break\n\t            df_dataset = pd.DataFrame(\n\t                data={\n\t                    \"file_path\": file_paths_lim,\n\t                    \"length\": lengths_lim,\n\t                    \"label\": labels_lim,\n\t                },\n\t                columns=[\"file_path\", \"length\", \"label\"],\n\t            )\n\t            df_outdir = os.path.join(\n", "                args.dump_dir, task_name, data_split + \"-\" + str(args.max_hours)\n\t            )\n\t            os.makedirs(df_outdir, exist_ok=True)\n\t            df_dataset.to_csv(os.path.join(df_outdir, \"data.csv\"), index=False)\n\t            if args.no_use_predefined_sampling:\n\t                os.makedirs(select_outdir, exist_ok=True)\n\t                with open(os.path.join(select_outdir, select_filename), \"w\") as f:\n\t                    f.writelines([str(l) + os.linesep for l in uttids_lim])\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "downstream/speechglue_asr/expert.py", "chunked_list": ["# Copyright (c) Facebook, Inc. All Rights Reserved\n\t# Copyleft (c), Speech Lab, NTU, Taiwan\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# This code only changes lines 82 and 90 of the following code (with some code formatting):\n\t# https://github.com/s3prl/s3prl/blob/v0.4.10/s3prl/downstream/asr/expert.py\n\t# L82: in order to load a dictionary of each task on SpeechGLUE dataset\n\t# L350: in order to use 'dev' set (not 'dev-clean' set of LibriSpeech)\n\timport math  # noqa\n\timport os\n\tfrom argparse import Namespace\n", "from pathlib import Path\n\timport editdistance\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.distributed import is_initialized\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tfrom torch.utils.data import DataLoader, DistributedSampler\n\tfrom ..model import *\n\tfrom .dataset import SequenceDataset\n\tfrom .dictionary import Dictionary\n", "from .model import *\n\tdef token_to_word(text):\n\t    # Hard coding but it is only used here for now.\n\t    # Assumption that units are characters. Doesn't handle BPE.\n\t    # Inter-character separator is \" \" and inter-word separator is \"|\".\n\t    return text.replace(\" \", \"\").replace(\"|\", \" \").strip()\n\tdef get_decoder(decoder_args_dict, dictionary):\n\t    decoder_args = Namespace(**decoder_args_dict)\n\t    if decoder_args.decoder_type == \"kenlm\":\n\t        from .w2l_decoder import W2lKenLMDecoder\n", "        decoder_args.beam_size_token = len(dictionary)\n\t        if isinstance(decoder_args.unk_weight, str):\n\t            decoder_args.unk_weight = eval(decoder_args.unk_weight)\n\t        return W2lKenLMDecoder(decoder_args, dictionary)\n\t    return None\n\tclass DownstreamExpert(nn.Module):\n\t    \"\"\"\n\t    Used to handle downstream-specific operations\n\t    eg. downstream forward, metric computation, contents to log\n\t    \"\"\"\n", "    def __init__(\n\t        self, upstream_dim, upstream_rate, downstream_expert, expdir, **kwargs\n\t    ):\n\t        \"\"\"\n\t        Args:\n\t            upstream_dim: int\n\t                Different upstream will give different representation dimension\n\t                You might want to first project them to the same dimension\n\t            upstream_rate: int\n\t                160: for upstream with 10 ms per frame\n", "                320: for upstream with 20 ms per frame\n\t            downstream_expert: dict\n\t                The 'downstream_expert' field specified in your downstream config file\n\t                eg. downstream/example/config.yaml\n\t            expdir: string\n\t                The expdir from command-line argument, you should save all results into\n\t                this directory, like some logging files.\n\t            **kwargs: dict\n\t                All the arguments specified by the argparser in run_downstream.py\n\t                and all the other fields in config.yaml, in case you need it.\n", "                Note1. Feel free to add new argument for __init__ as long as it is\n\t                a command-line argument or a config field. You can check the constructor\n\t                code in downstream/runner.py\n\t        \"\"\"\n\t        super(DownstreamExpert, self).__init__()\n\t        self.upstream_dim = upstream_dim\n\t        self.upstream_rate = upstream_rate\n\t        self.datarc = downstream_expert[\"datarc\"]\n\t        self.modelrc = downstream_expert[\"modelrc\"]\n\t        self.expdir = expdir\n", "        self.dictionary = Dictionary.load(\n\t            os.path.join(\n\t                self.datarc[\"speechglue_root\"],\n\t                self.datarc[\"speechglue_task\"],\n\t                \"char.dict\",\n\t            )\n\t        )\n\t        self.projector = nn.Linear(upstream_dim, self.modelrc[\"project_dim\"])\n\t        model_cls = eval(self.modelrc[\"select\"])\n\t        model_conf = self.modelrc[self.modelrc[\"select\"]]\n", "        self.model = model_cls(\n\t            self.modelrc[\"project_dim\"],\n\t            len(self.dictionary.symbols),\n\t            upstream_rate,\n\t            **model_conf,\n\t        )\n\t        self.blank = self.dictionary.bos()\n\t        self.objective = nn.CTCLoss(\n\t            blank=self.blank, zero_infinity=self.datarc[\"zero_infinity\"]\n\t        )\n", "        decoder_args = self.datarc.get(\"decoder_args\")\n\t        self.decoder = get_decoder(decoder_args, self.dictionary)\n\t        self.register_buffer(\"best_score\", torch.ones(1) * 100)\n\t    # Interface\n\t    def get_dataloader(self, split):\n\t        \"\"\"\n\t        Args:\n\t            split: string\n\t                The name of the dataloader, can be train/dev/test-clean/test-other for asr\n\t        Return:\n", "            a torch.utils.data.DataLoader returning each batch in the format of:\n\t            [wav1, wav2, ...], your_other_contents1, your_other_contents2, ...\n\t            where wav1, wav2 ... are in variable length\n\t            each wav is torch.FloatTensor in cpu with:\n\t                1. dim() == 1\n\t                2. sample_rate == 16000\n\t                3. directly loaded by torchaudio\n\t        \"\"\"\n\t        if not hasattr(self, f\"{split}_dataset\"):\n\t            batch_size = (\n", "                self.datarc[\"batch_size\"]\n\t                if split == \"train\"\n\t                else self.datarc[\"eval_batch_size\"]\n\t            )\n\t            setattr(\n\t                self,\n\t                f\"{split}_dataset\",\n\t                SequenceDataset(split, batch_size, self.dictionary, **self.datarc),\n\t            )\n\t        if split == \"train\":\n", "            return self._get_train_dataloader(self.train_dataset)\n\t        else:\n\t            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\t    def _get_train_dataloader(self, dataset):\n\t        sampler = DistributedSampler(dataset) if is_initialized() else None\n\t        return DataLoader(\n\t            dataset,\n\t            batch_size=1,\n\t            shuffle=(sampler is None),\n\t            sampler=sampler,\n", "            num_workers=self.datarc[\"num_workers\"],\n\t            collate_fn=dataset.collate_fn,\n\t        )\n\t    def _get_eval_dataloader(self, dataset):\n\t        return DataLoader(\n\t            dataset,\n\t            batch_size=1,\n\t            shuffle=False,\n\t            num_workers=self.datarc[\"num_workers\"],\n\t            collate_fn=dataset.collate_fn,\n", "        )\n\t    def _compute_metrics(\n\t        self, pred_tokens_all, pred_words_all, target_tokens_all, target_words_all\n\t    ):\n\t        \"\"\"Computes WER and UER given the prediction and true transcriptions\"\"\"\n\t        unit_error_sum = 0.0\n\t        word_error_sum = 0.0\n\t        unit_length_sum = 0\n\t        word_length_sum = 0\n\t        for pred_tokens, pred_words, target_tokens, target_words in zip(\n", "            pred_tokens_all, pred_words_all, target_tokens_all, target_words_all\n\t        ):\n\t            pred_tokens = pred_tokens.split()\n\t            target_tokens = target_tokens.split()\n\t            unit_error_sum += editdistance.eval(pred_tokens, target_tokens)\n\t            unit_length_sum += len(target_tokens)\n\t            word_error_sum += editdistance.eval(pred_words, target_words)\n\t            word_length_sum += len(target_words)\n\t        uer, wer = 100.0, 100.0\n\t        if unit_length_sum > 0:\n", "            uer = 100.0 * unit_error_sum / unit_length_sum\n\t        if word_length_sum > 0:\n\t            wer = 100.0 * word_error_sum / word_length_sum\n\t        return uer, wer\n\t    def _decode(self, log_probs, input_lens):\n\t        \"\"\"Decoder that take log probabilities as input and outputs decoded seq\"\"\"\n\t        pred_tokens_batch = []\n\t        pred_words_batch = []\n\t        for log_prob, in_len in zip(log_probs, input_lens):\n\t            log_prob = log_prob[:in_len].unsqueeze(0)\n", "            decoded = None\n\t            if self.decoder is not None and not self.training:\n\t                decoded = self.decoder.decode(log_prob)\n\t                if len(decoded) >= 1:\n\t                    decoded = decoded[0]\n\t                    decoded = None if len(decoded) < 1 else decoded[0]\n\t            pred_token_ids = log_prob.argmax(dim=-1).unique_consecutive()\n\t            pred_token_ids = pred_token_ids[pred_token_ids != self.blank].tolist()\n\t            pred_tokens = self.dictionary.string(pred_token_ids)\n\t            if decoded is not None and \"words\" in decoded:\n", "                pred_words = decoded[\"words\"]\n\t            else:\n\t                pred_words = token_to_word(pred_tokens).split()\n\t            pred_tokens_batch.append(pred_tokens)\n\t            pred_words_batch.append(pred_words)\n\t        return pred_tokens_batch, pred_words_batch\n\t    def _get_log_probs(self, features):\n\t        device = features[0].device\n\t        features_len = torch.IntTensor([len(feat) for feat in features])\n\t        features = pad_sequence(features, batch_first=True).to(device=device)\n", "        features = self.projector(features)\n\t        logits, log_probs_len = self.model(features, features_len)\n\t        log_probs = nn.functional.log_softmax(logits, dim=-1)\n\t        return log_probs, log_probs_len\n\t    def inference(self, features, filenames):\n\t        log_probs, log_probs_len = self._get_log_probs(features)\n\t        _, pred_words_batch = self._decode(\n\t            log_probs.float().contiguous().cpu(), log_probs_len\n\t        )\n\t        hyps = [\" \".join(hyp) for hyp in pred_words_batch]\n", "        if filenames != []:\n\t            with open(Path(self.expdir) / \"inference.ark\", \"w\") as file:\n\t                for hyp, filename in zip(hyps, filenames):\n\t                    file.write(f\"{filename} {hyp}\\n\")\n\t        return hyps\n\t    # Interface\n\t    def forward(self, split, features, labels, filenames, records, **kwargs):\n\t        \"\"\"\n\t        Args:\n\t            split: string\n", "                The name of the dataloader, can be train/dev/test-clean/test-other for asr\n\t            features:\n\t                list of unpadded features [feat1, feat2, ...]\n\t                each feat is in torch.FloatTensor and already\n\t                put in the device assigned by command-line args\n\t            your_other_contents1, ... :\n\t                in the order defined by your dataloader (dataset + collate_fn)\n\t                these are all in cpu, and you can move them to the same device\n\t                as features\n\t            records:\n", "                defaultdict(list), by appending contents into records,\n\t                these contents can be averaged and logged on Tensorboard\n\t                later by self.log_records (also customized by you)\n\t                Note1. downstream/runner.py will call self.log_records\n\t                    1. every `log_step` during training\n\t                    2. once after evalute the whole dev/test dataloader\n\t                Note2. `log_step` is defined in your downstream config\n\t                eg. downstream/example/config.yaml\n\t        Return:\n\t            loss:\n", "                the loss to be optimized, should not be detached\n\t                a single scalar in torch.FloatTensor\n\t        \"\"\"\n\t        log_probs, log_probs_len = self._get_log_probs(features)\n\t        device = features[0].device\n\t        labels = [torch.IntTensor(l) for l in labels]\n\t        labels_len = torch.IntTensor([len(label) for label in labels]).to(device=device)\n\t        labels = pad_sequence(\n\t            labels,\n\t            batch_first=True,\n", "            padding_value=self.dictionary.pad(),\n\t        ).to(device=device)\n\t        loss = self.objective(\n\t            log_probs.transpose(0, 1),  # (N, T, C) -> (T, N, C)\n\t            labels,\n\t            log_probs_len,\n\t            labels_len,\n\t        )\n\t        records[\"loss\"].append(loss.item())\n\t        target_tokens_batch = []\n", "        target_words_batch = []\n\t        for label in labels:\n\t            label_idx = (label != self.dictionary.pad()) & (\n\t                label != self.dictionary.eos()\n\t            )\n\t            target_token_ids = label[label_idx].tolist()\n\t            target_tokens = self.dictionary.string(target_token_ids)\n\t            target_words = token_to_word(target_tokens).split()\n\t            target_tokens_batch.append(target_tokens)\n\t            target_words_batch.append(target_words)\n", "        with torch.no_grad():\n\t            pred_tokens_batch, pred_words_batch = self._decode(\n\t                log_probs.float().contiguous().cpu(), log_probs_len\n\t            )\n\t        records[\"target_tokens\"] += target_tokens_batch\n\t        records[\"target_words\"] += target_words_batch\n\t        records[\"pred_tokens\"] += pred_tokens_batch\n\t        records[\"pred_words\"] += pred_words_batch\n\t        records[\"filenames\"] += filenames\n\t        return loss\n", "    # interface\n\t    def log_records(\n\t        self, split, records, logger, global_step, batch_ids, total_batch_num, **kwargs\n\t    ):\n\t        \"\"\"\n\t        Args:\n\t            split: string\n\t                'train':\n\t                    records and batchids contain contents for `log_step` batches\n\t                    `log_step` is defined in your downstream config\n", "                    eg. downstream/example/config.yaml\n\t                'dev' or 'test-clean' or 'test-other' :\n\t                    records and batchids contain contents for the entire evaluation dataset\n\t            records:\n\t                defaultdict(list), contents already prepared by self.forward\n\t            logger:\n\t                Tensorboard SummaryWriter\n\t                please use f'{your_task_name}/{split}-{key}' as key name to log your contents,\n\t                preventing conflict with the logging of other tasks\n\t            global_step:\n", "                The global_step when training, which is helpful for Tensorboard logging\n\t            batch_ids:\n\t                The batches contained in records when enumerating over the dataloader\n\t            total_batch_num:\n\t                The total amount of batches in the dataloader\n\t        Return:\n\t            a list of string\n\t                Each string is a filename we wish to use to save the current model\n\t                according to the evaluation result, like the best.ckpt on the dev set\n\t                You can return nothing or an empty list when no need to save the checkpoint\n", "        \"\"\"\n\t        loss = torch.FloatTensor(records[\"loss\"]).mean().item()\n\t        print(f\"{split} loss: {loss}\")\n\t        uer, wer = self._compute_metrics(\n\t            records[\"pred_tokens\"],\n\t            records[\"pred_words\"],\n\t            records[\"target_tokens\"],\n\t            records[\"target_words\"],\n\t        )\n\t        logger.add_scalar(f\"asr/{split}-loss\", loss, global_step=global_step)\n", "        logger.add_scalar(f\"asr/{split}-uer\", uer, global_step=global_step)\n\t        logger.add_scalar(f\"asr/{split}-wer\", wer, global_step=global_step)\n\t        print(f\"{split} uer: {uer}\")\n\t        print(f\"{split} wer: {wer}\")\n\t        save_names = []\n\t        if split == \"dev\" and wer < self.best_score:\n\t            self.best_score = torch.ones(1) * wer\n\t            save_names.append(f\"{split}-best.ckpt\")\n\t        if \"test\" in split or \"dev\" in split:\n\t            lm = \"noLM\" if self.decoder is None else \"LM\"\n", "            hyp_ark = open(os.path.join(self.expdir, f\"{split}-{lm}-hyp.ark\"), \"w\")\n\t            ref_ark = open(os.path.join(self.expdir, f\"{split}-{lm}-ref.ark\"), \"w\")\n\t            for filename, hyp, ref in zip(\n\t                records[\"filenames\"], records[\"pred_words\"], records[\"target_words\"]\n\t            ):\n\t                hyp = \" \".join(hyp)\n\t                ref = \" \".join(ref)\n\t                hyp_ark.write(f\"{filename} {hyp}\\n\")\n\t                ref_ark.write(f\"{filename} {ref}\\n\")\n\t            hyp_ark.close()\n", "            ref_ark.close()\n\t        return save_names\n"]}
{"filename": "downstream/speechglue_asr/dataset.py", "chunked_list": ["# Copyleft (c), Speech Lab, NTU, Taiwan\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# This code changes to load speechGLUE data based on the following code (and some code formatting).\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\t# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n\timport os\n\timport re\n", "import pandas as pd\n\timport torchaudio\n\tfrom torch.utils.data.dataset import Dataset\n\tfrom tqdm import tqdm\n\tfrom .dictionary import Dictionary\n\tSAMPLE_RATE = 16000\n\tHALF_BATCHSIZE_TIME = 2000\n\ttask_to_keys = {\n\t    \"cola\": (\"sentence\", None),\n\t    \"mnli\": (\"premise\", \"hypothesis\"),\n", "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n\t    \"qnli\": (\"question\", \"sentence\"),\n\t    \"qqp\": (\"question1\", \"question2\"),\n\t    \"rte\": (\"sentence1\", \"sentence2\"),\n\t    \"sst2\": (\"sentence\", None),\n\t    \"stsb\": (\"sentence1\", \"sentence2\"),\n\t    \"wnli\": (\"sentence1\", \"sentence2\"),\n\t}\n\t####################\n\t# Sequence Dataset #\n", "####################\n\tclass SequenceDataset(Dataset):\n\t    def __init__(\n\t        self, split, bucket_size, dictionary, speechglue_task, speechglue_root, **kwargs\n\t    ):\n\t        super(SequenceDataset, self).__init__()\n\t        self.dictionary = dictionary\n\t        self.speechglue_task = speechglue_task\n\t        self.speechglue_root = speechglue_root\n\t        self.sample_rate = SAMPLE_RATE\n", "        self.split_sets = kwargs[split]\n\t        self.speechglue_dir = os.path.join(speechglue_root, speechglue_task)\n\t        # Read table for bucketing\n\t        assert os.path.isdir(\n\t            self.speechglue_dir\n\t        ), \"Please first run `python downstream/speechglue_asr/data_prep.py -h` to get TTS file.\"\n\t        # Wavs\n\t        table_list = []\n\t        for item in self.split_sets:\n\t            file_path = os.path.join(self.speechglue_dir, item, \"data.csv\")\n", "            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n\t            table_list.append(pd.read_csv(file_path))\n\t        table_list = pd.concat(table_list)\n\t        dataset_columns = [\"file_path\", \"length\", \"label\"]\n\t        # the case of a dataset with a limited amount of samples in advance\n\t        if set(table_list.columns) == set(dataset_columns):\n\t            df_dataset = table_list\n\t        else:\n\t            sentence1_key, sentence2_key = task_to_keys[self.speechglue_task]\n\t            file_paths = table_list[\"file_\" + sentence1_key].tolist()\n", "            labels = table_list[sentence1_key].tolist()\n\t            lengths = table_list[\"length_\" + sentence1_key].tolist()\n\t            if sentence2_key is not None:\n\t                file_paths.extend(table_list[\"file_\" + sentence2_key].tolist())\n\t                labels.extend(table_list[sentence2_key].tolist())\n\t                lengths.extend(table_list[\"length_\" + sentence2_key].tolist())\n\t            df_dataset = pd.DataFrame(\n\t                data={\"file_path\": file_paths, \"length\": lengths, \"label\": labels},\n\t                columns=dataset_columns,\n\t            )\n", "        df_dataset = df_dataset.sort_values(by=[\"length\"], ascending=False)\n\t        X = df_dataset[\"file_path\"].tolist()\n\t        X_lens = df_dataset[\"length\"].tolist()\n\t        Y = self._load_transcript(df_dataset[\"label\"].tolist())\n\t        Y = [\n\t            self.dictionary.encode_line(y, line_tokenizer=lambda x: x.split()).long()\n\t            for y in Y\n\t        ]\n\t        assert len(X) != 0, f\"0 data found for {split}\"\n\t        # Use bucketing to allow different batch sizes at run time\n", "        self.X = []\n\t        self.Y = []\n\t        batch_x, batch_len, batch_y = [], [], []\n\t        for x, x_len, y in tqdm(\n\t            zip(X, X_lens, Y),\n\t            total=len(X),\n\t            desc=f\"ASR dataset {split}\",\n\t            dynamic_ncols=True,\n\t        ):\n\t            batch_x.append(x)\n", "            batch_len.append(x_len)\n\t            batch_y.append(y)\n\t            # Fill in batch_x until batch is full\n\t            if len(batch_x) == bucket_size:\n\t                # Half the batch size if seq too long\n\t                if (bucket_size >= 2) and (max(batch_len) > HALF_BATCHSIZE_TIME):\n\t                    self.X.append(batch_x[: bucket_size // 2])\n\t                    self.X.append(batch_x[bucket_size // 2 :])\n\t                    self.Y.append(batch_y[: bucket_size // 2])\n\t                    self.Y.append(batch_y[bucket_size // 2 :])\n", "                else:\n\t                    self.X.append(batch_x)\n\t                    self.Y.append(batch_y)\n\t                batch_x, batch_len, batch_y = [], [], []\n\t        # Gather the last batch\n\t        if len(batch_x) > 1:\n\t            self.X.append(batch_x)\n\t            self.Y.append(batch_y)\n\t    def _parse_x_name(self, x):\n\t        return \"-\".join(x.split(\"/\")[-4:]).split(\".\")[0]\n", "    def _load_wav(self, wav_path):\n\t        wav, sr = torchaudio.load(wav_path)\n\t        assert (\n\t            sr == self.sample_rate\n\t        ), f\"Sample rate mismatch: real {sr}, config {self.sample_rate}\"\n\t        return wav.view(-1)\n\t    def _load_transcript(self, x_list):\n\t        def process_trans(transcript):\n\t            transcript = re.sub(\"[.,?!]\", \"\", transcript).replace(\" \", \"|\")\n\t            # word to char\n", "            return \" \".join(list(transcript)) + \" |\"\n\t        return [process_trans(x) for x in x_list]\n\t    def _build_dictionary(\n\t        self, transcripts, workers=1, threshold=-1, nwords=-1, padding_factor=8\n\t    ):\n\t        d = Dictionary()\n\t        transcript_list = list(transcripts.values())\n\t        Dictionary.add_transcripts_to_dictionary(transcript_list, d, workers)\n\t        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n\t        return d\n", "    def __len__(self):\n\t        return len(self.X)\n\t    def __getitem__(self, index):\n\t        # Load acoustic feature and pad\n\t        wav_batch = [self._load_wav(x_file).numpy() for x_file in self.X[index]]\n\t        label_batch = [y.numpy() for y in self.Y[index]]\n\t        filename_batch = [self._parse_x_name(x_file) for x_file in self.X[index]]\n\t        return (\n\t            wav_batch,\n\t            label_batch,\n", "            filename_batch,\n\t        )  # bucketing, return ((wavs, labels))\n\t    def collate_fn(self, items):\n\t        assert len(items) == 1\n\t        return (\n\t            items[0][0],\n\t            items[0][1],\n\t            items[0][2],\n\t        )  # hack bucketing, return (wavs, labels, filenames)\n"]}
{"filename": "downstream/speechglue_asr/mk_char_dict.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n\t#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\t# we utilize the GLUE tasks listed in the below code\n\t# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\timport argparse\n\timport os\n\timport re\n\timport pandas as pd\n\ttask_to_keys = {\n\t    \"cola\": (\"sentence\", None),\n", "    \"mnli\": (\"premise\", \"hypothesis\"),\n\t    \"mrpc\": (\"sentence1\", \"sentence2\"),\n\t    \"qnli\": (\"question\", \"sentence\"),\n\t    \"qqp\": (\"question1\", \"question2\"),\n\t    \"rte\": (\"sentence1\", \"sentence2\"),\n\t    \"sst2\": (\"sentence\", None),\n\t    \"stsb\": (\"sentence1\", \"sentence2\"),\n\t    \"wnli\": (\"sentence1\", \"sentence2\"),\n\t}\n\tdef get_parser():\n", "    parser = argparse.ArgumentParser(\n\t        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t        description=\"Dictionary preparation for SpeechGLUE\",\n\t    )\n\t    parser.add_argument(\n\t        \"--dump-dir\",\n\t        type=str,\n\t        default=\"dump\",\n\t        help=\"Path to storing the SpeechGLUE dataset\",\n\t    )\n", "    parser.add_argument(\n\t        \"--glue-task\",\n\t        type=str,\n\t        default=\"all\",\n\t        choices=[\"all\"] + list(task_to_keys.keys()),\n\t        help=\"Name of the GLUE task\",\n\t    )\n\t    return parser\n\tdef main():\n\t    parser = get_parser()\n", "    args = parser.parse_args()\n\t    if args.glue_task == \"all\":\n\t        task_names = task_to_keys.keys()\n\t    else:\n\t        task_names = [args.glue_task]\n\t    for task_name in task_names:\n\t        sentence1_key, sentence2_key = task_to_keys[task_name]\n\t        csv_path = os.path.join(args.dump_dir, task_name, \"train\", \"data.csv\")\n\t        # some sentences include only \"null\"\n\t        # therefore, keep_default_na is added to interpret as is\n", "        csv = pd.read_csv(csv_path, keep_default_na=False)\n\t        sentences = list(csv[sentence1_key])\n\t        if sentence2_key is not None:\n\t            sentences.extend(list(csv[sentence2_key]))\n\t        sentences = \"|\".join(sentences)\n\t        sentences = re.sub(\"[.,?!]\", \"\", sentences).replace(\" \", \"|\") + \"|\"\n\t        char_counts = {c: sentences.count(c) for c in set(sentences)}\n\t        outdic = os.path.join(args.dump_dir, task_name, \"char.dict\")\n\t        with open(outdic, \"w\") as f:\n\t            for x in sorted(\n", "                char_counts.items(), key=lambda char: char[1], reverse=True\n\t            ):\n\t                f.write(x[0] + \" \" + str(x[1]) + \"\\n\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "downstream/speechglue_asr/__init__.py", "chunked_list": []}
{"filename": "upstream/hf_nlp_ssl/expert.py", "chunked_list": ["import logging\n\timport torch\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tfrom transformers import AutoModel, AutoTokenizer\n\tSAMPLE_RATE = 16000\n\tHF_INPUT_KEYS = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n\tlogger = logging.getLogger(__name__)\n\tclass UpstreamExpert(torch.nn.Module):\n\t    def __init__(self, ckpt, **kwds):\n\t        super().__init__()\n", "        self.model = AutoModel.from_pretrained(ckpt)\n\t        tokenizer = AutoTokenizer.from_pretrained(\n\t            ckpt,\n\t            cache_dir=\"data\",\n\t        )\n\t        self.pad_values = [tokenizer.pad_token_id, tokenizer.pad_token_type_id, 0]\n\t        self.vocab_size = tokenizer.vocab_size\n\t    def pad_token(self, tokens):\n\t        # https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py\n\t        device = tokens[0].device\n", "        key_size = tokens[0].shape[1]\n\t        output_dict = {}\n\t        for key_id in range(key_size):\n\t            key_name = HF_INPUT_KEYS[key_id]\n\t            padded_token = pad_sequence(\n\t                [token[:, key_id] for token in tokens],\n\t                batch_first=True,\n\t                padding_value=self.pad_values[key_id],\n\t            )\n\t            output_dict[key_name] = padded_token.to(dtype=torch.int64, device=device)\n", "        return output_dict\n\t    def get_downsample_rates(self, key: str = None) -> int:\n\t        return 1\n\t    def forward(self, tokens):\n\t        # tokens: List of FloatTensor(TxK)\n\t        # when Featurizer instantiation, tokens is List of FloatTensor(T)\n\t        # https://github.com/s3prl/s3prl/blob/main/s3prl/upstream/interfaces.py\n\t        if tokens[0].dim() == 1 and tokens[0].shape[0] == SAMPLE_RATE:\n\t            print(\"Featurizer instantiation related forward\")\n\t            tokens[0] = torch.randint(\n", "                0, self.vocab_size, (20, 1), device=tokens[0].device\n\t            )\n\t        input_dict = self.pad_token(tokens)\n\t        output_values = self.model(\n\t            **input_dict, output_hidden_states=True\n\t        )  # Tuple of BxTxF\n\t        return {\"hidden_states\": output_values.hidden_states}\n"]}
{"filename": "upstream/hf_nlp_ssl/__init__.py", "chunked_list": []}
{"filename": "upstream/hf_nlp_ssl/hubconf.py", "chunked_list": ["from .expert import UpstreamExpert as _UpstreamExpert\n\tdef hf_nlp_ssl(ckpt, *args, **kwargs):\n\t    return _UpstreamExpert(ckpt, *args, **kwargs)\n"]}
{"filename": "upstream/hf_speechssl_no_pretrained_weights/expert.py", "chunked_list": ["import logging\n\timport torch\n\tfrom transformers import AutoConfig, AutoFeatureExtractor, AutoModel\n\tSAMPLE_RATE = 16000\n\tEXAMPLE_SEC = 5\n\tlogger = logging.getLogger(__name__)\n\tclass UpstreamExpert(torch.nn.Module):\n\t    def __init__(self, ckpt, **kwds):\n\t        super().__init__()\n\t        config = AutoConfig.from_pretrained(ckpt)\n", "        self.extracter = AutoFeatureExtractor.from_pretrained(ckpt)\n\t        self.model = AutoModel.from_config(config)\n\t    def get_downsample_rates(self, key: str = None) -> int:\n\t        return 320\n\t    def forward(self, wavs):\n\t        device = wavs[0].device\n\t        wavs = [wav.detach().cpu().numpy() for wav in wavs]\n\t        input_values = self.extracter(\n\t            wavs,\n\t            return_tensors=\"pt\",\n", "            padding=True,\n\t            return_attention_mask=True,\n\t            sampling_rate=SAMPLE_RATE,\n\t        ).to(device)\n\t        output_values = self.model(**input_values, output_hidden_states=True)\n\t        return {\"hidden_states\": output_values.hidden_states}\n"]}
{"filename": "upstream/hf_speechssl_no_pretrained_weights/__init__.py", "chunked_list": []}
{"filename": "upstream/hf_speechssl_no_pretrained_weights/hubconf.py", "chunked_list": ["from .expert import UpstreamExpert as _UpstreamExpert\n\tdef hf_speechssl_no_pretrained_weights(ckpt, *args, **kwargs):\n\t    return _UpstreamExpert(ckpt, *args, **kwargs)\n"]}
{"filename": "upstream/embedding/expert.py", "chunked_list": ["import logging\n\timport torch\n\timport torch.nn as nn\n\timport yaml\n\tfrom espnet2.bin.tts_inference import Text2Speech\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tSAMPLE_RATE = 16000\n\tlogger = logging.getLogger(__name__)\n\tclass UpstreamExpert(torch.nn.Module):\n\t    def __init__(self, ckpt, model_config=None, **kwds):\n", "        super().__init__()\n\t        # ckpt is the ESPnet TTS model name such as \"kan-bayashi/ljspeech_vits\"\n\t        text2speech = Text2Speech.from_pretrained(ckpt, device=\"cuda\")\n\t        # adding two value for [PAD] and [SEP] token (define [PAD] and [SEP] token as 0 and 1)\n\t        self.vocab_size = (\n\t            len(text2speech.preprocess_fn.token_id_converter.token_list) + 2\n\t        )\n\t        print(f\"Phoneme vocabulary size is {self.vocab_size}\")\n\t        if model_config is not None:\n\t            print(\n", "                \"[UpstreamExpert] - Using upstream expert config file from:\",\n\t                model_config,\n\t            )\n\t            with open(model_config, \"r\") as file:\n\t                options = yaml.load(file, Loader=yaml.FullLoader)\n\t        else:\n\t            print(\"[UpstreamExpert] - Using the default upstream expert config\")\n\t            options = {\n\t                \"embedding_size\": 256,\n\t            }\n", "        self.model = nn.Embedding(\n\t            self.vocab_size, options[\"embedding_size\"], padding_idx=0\n\t        )\n\t        print(f\"Embedding size is {options['embedding_size']}\")\n\t    def get_downsample_rates(self, key: str = None) -> int:\n\t        return 1\n\t    def forward(self, tokens):\n\t        # tokens: List of FloatTensor(T)\n\t        # when Featurizer instantiation, tokens is List of FloatTensor(T)\n\t        # https://github.com/s3prl/s3prl/blob/main/s3prl/upstream/interfaces.py\n", "        if tokens[0].dim() == 1 and tokens[0].shape[0] == SAMPLE_RATE:\n\t            print(\"Featurizer instantiation related forward\")\n\t            tokens[0] = torch.randint(\n\t                0, self.vocab_size, (20, 1), device=tokens[0].device\n\t            )\n\t        padded_token = pad_sequence(tokens, batch_first=True, padding_value=0).to(\n\t            dtype=torch.int64\n\t        )  # BxT\n\t        output_values = self.model(padded_token)  # BxTxF\n\t        return {\"hidden_states\": output_values}\n"]}
{"filename": "upstream/embedding/__init__.py", "chunked_list": []}
{"filename": "upstream/embedding/hubconf.py", "chunked_list": ["from .expert import UpstreamExpert as _UpstreamExpert\n\tdef embedding(ckpt, *args, **kwargs):\n\t    return _UpstreamExpert(ckpt, *args, **kwargs)\n"]}
