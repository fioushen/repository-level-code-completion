{"filename": "onnx_helper.py", "chunked_list": ["#\n\t# SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t# SPDX-License-Identifier: Apache-2.0\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n", "# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\timport numpy as np\n\t# import tensorflow as tf\n\timport tensorrt as trt\n\timport pycuda.driver as cuda\n", "import pycuda.autoinit\n\t# For ONNX:\n\tclass ONNXClassifierWrapper():\n\t    def __init__(self, file, num_classes, target_dtype = np.float32):\n\t        self.target_dtype = target_dtype\n\t        self.num_classes = num_classes\n\t        self.load(file)\n\t        self.stream = None\n\t    def load(self, file):\n\t        f = open(file, \"rb\")\n", "        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n\t        engine = runtime.deserialize_cuda_engine(f.read())\n\t        self.context = engine.create_execution_context()\n\t    def allocate_memory(self, batch):\n\t        self.output = np.empty(self.num_classes, dtype = self.target_dtype) # Need to set both input and output precisions to FP16 to fully enable FP16\n\t        # Allocate device memory\n\t        self.d_input = cuda.mem_alloc(1 * batch.nbytes)\n\t        self.d_output = cuda.mem_alloc(1 * self.output.nbytes)\n\t        self.bindings = [int(self.d_input), int(self.d_output)]\n\t        self.stream = cuda.Stream()\n", "    def predict(self, batch): # result gets copied into output\n\t        if self.stream is None:\n\t            self.allocate_memory(batch)\n\t        # Transfer input data to device\n\t        cuda.memcpy_htod_async(self.d_input, batch, self.stream)\n\t        # Execute model\n\t        self.context.execute_async_v2(self.bindings, self.stream.handle, None)\n\t        # Transfer predictions back\n\t        cuda.memcpy_dtoh_async(self.output, self.d_output, self.stream)\n\t        # Syncronize threads\n", "        self.stream.synchronize()\n\t        return self.output\n\tdef convert_onnx_to_engine(onnx_filename, engine_filename = None, max_batch_size = 32, max_workspace_size = 1 << 30, fp16_mode = True):\n\t    logger = trt.Logger(trt.Logger.WARNING)\n\t    with trt.Builder(logger) as builder, builder.create_network() as network, trt.OnnxParser(network, logger) as parser:\n\t        builder.max_workspace_size = max_workspace_size\n\t        builder.fp16_mode = fp16_mode\n\t        builder.max_batch_size = max_batch_size\n\t        print(\"Parsing ONNX file.\")\n\t        with open(onnx_filename, 'rb') as model:\n", "            if not parser.parse(model.read()):\n\t                for error in range(parser.num_errors):\n\t                    print(parser.get_error(error))\n\t        print(\"Building TensorRT engine. This may take a few minutes.\")\n\t        engine = builder.build_cuda_engine(network)\n\t        if engine_filename:\n\t            with open(engine_filename, 'wb') as f:\n\t                f.write(engine.serialize())\n\t        return engine, logger"]}
{"filename": "train.py", "chunked_list": ["import math\n\timport argparse\n\tfrom pathlib import Path\n\tfrom tqdm import tqdm\n\timport time\n\timport os\n\timport sys\n\timport shutil\n\tfrom omegaconf import OmegaConf\n\timport numpy as np\n", "import string\n\tfrom typing import List\n\timport torch\n\tfrom torch.optim import Optimizer\n\tfrom torch.optim.lr_scheduler import OneCycleLR\n\timport torch.distributed as dist\n\tfrom torch.utils.data.distributed import DistributedSampler\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom torch.utils.tensorboard import SummaryWriter \n\tfrom data.module import SceneTextDataModule\n", "from model.parseq import PARSeq\n\tfrom torch.utils.data import DataLoader\n\tfrom glob import glob\n\tif 'LOCAL_RANK' in os.environ:\n\t    # Environment variables set by torch.distributed.launch or torchrun\n\t    LOCAL_RANK = int(os.environ['LOCAL_RANK'])\n\t    WORLD_SIZE = int(os.environ['WORLD_SIZE'])\n\t    WORLD_RANK = int(os.environ['RANK'])\n\t    print('LOCAL_RANK, WORLD_SIZE, WORLD_RANK: ', LOCAL_RANK, WORLD_SIZE, WORLD_RANK)\n\telse:\n", "    sys.exit(\"Can't find the evironment variables for local rank\")\n\tdef set_random_seeds(random_seed=0):\n\t    torch.manual_seed(random_seed)\n\t    torch.backends.cudnn.deterministic = True\n\t    torch.backends.cudnn.benchmark = False\n\t    np.random.seed(random_seed)\n\t    random.seed(random_seed)\n\tdef sync_across_gpus(t):\n\t    gather_t_tensor = [torch.ones_like(t) for _ in range(WORLD_SIZE)]\n\t    torch.distributed.all_gather(gather_t_tensor, t)\n", "    return torch.stack(gather_t_tensor)\n\tdef prepare(dataset, rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n\t    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, \\\n\t                                 shuffle=True, drop_last=False)\n\t    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, \\\n\t                            num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n\t    return dataloader\n\tclass Balanced_Batch(SceneTextDataModule):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n", "        self.dataloader_list = []\n\t        self.dataloader_iter_list = []\n\t        self.batch_size_list = []\n\t        self.root_dir = self.train_dir\n\t    def prepare(self, rank, world_size, pin_memory=False, num_workers=0):\n\t        sub_folders = glob(os.path.join(self.train_dir, '*'))\n\t        for folder, folder_weight in self.data_weights:\n\t            self.train_dir = os.path.join(self.root_dir, folder)\n\t            self._train_dataset = None\n\t            print(\"FOLDER/WEIGHT/DIRECTORY: \", folder, folder_weight, self.train_dir)\n", "            assert os.path.exists(self.train_dir), f'directory {self.train_dir} does not exist. check the folders and weights argument' \n\t            folder_batch_size = max(round(self.batch_size * folder_weight), 1)\n\t            folder_dataset = self.train_dataset\n\t            sampler = DistributedSampler(folder_dataset, num_replicas=world_size, rank=rank, \\\n\t                                         shuffle=True, drop_last=False)\n\t            folder_dataloader = DataLoader(folder_dataset, batch_size=folder_batch_size, pin_memory=pin_memory, \n\t                                    num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n\t#             print('folder data len: ', len(folder_dataset), len(folder_dataloader))\n\t            self.batch_size_list.append(folder_batch_size)\n\t            self.dataloader_list.append(folder_dataloader)\n", "            self.dataloader_iter_list.append(iter(folder_dataloader))\n\t        self.batch_size = sum(self.batch_size_list)\n\t    def get_batch(self):\n\t        balanced_batch_images = []\n\t        balanced_batch_texts = []\n\t        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n\t            try:\n\t                image, text = data_loader_iter.next()\n\t                balanced_batch_images.append(image)\n\t                balanced_batch_texts += text\n", "            except StopIteration:\n\t                self.dataloader_iter_list[i] = iter(self.dataloader_list[i])\n\t                image, text = self.dataloader_iter_list[i].next()\n\t                balanced_batch_images.append(image)\n\t                balanced_batch_texts += text\n\t            except ValueError:\n\t                pass\n\t        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n\t        return balanced_batch_images, balanced_batch_texts\n\tdef main(args):\n", "    config = OmegaConf.load(args.config)\n\t    # updating korean charset\n\t    chars = \"\"\n\t    with open(\n\t        os.path.join(config.data_loader.character.dict_dir, \"charset.txt\"), \"r\"\n\t    ) as f:\n\t        curr_char = f.read()\n\t    curr_char = curr_char.strip()\n\t    chars += curr_char\n\t    chars = \"\".join(sorted(set(chars)))\n", "    config.model.charset_train = chars\n\t    config.model.charset_test = config.model.charset_train\n\t    # Special handling for PARseq\n\t    if config.model.get('perm_mirrored', False):\n\t        assert config.model.perm_num % 2 == 0, 'perm_num should be even if perm_mirrored = True'\n\t    if ~os.path.exists(config.log_dir):\n\t        os.makedirs(config.log_dir, exist_ok=True)\n\t    # config save\n\t    OmegaConf.save(config, os.path.join(config.log_dir, \"config.yaml\"))\n\t    # Tensorboard logs\n", "    exp_log = SummaryWriter(os.path.join(config.log_dir, 'tf_runs'))\n\t    dist.init_process_group(\"nccl\", rank=WORLD_RANK, world_size=WORLD_SIZE)\n\t    # Balanced data setting\n\t    datamodule = Balanced_Batch(**config.data)\n\t    datamodule.prepare(WORLD_RANK, WORLD_SIZE, num_workers=config.data.num_workers)\n\t    val_dataset = datamodule.val_dataset\n\t    config.data.batch_size = datamodule.batch_size\n\t    config.model.batch_size = config.data.batch_size\n\t    print('Updated Batch_Size: ', config.data.batch_size)\n\t    val_loader = prepare(val_dataset, WORLD_RANK, WORLD_SIZE, batch_size=config.data.batch_size, \\\n", "                           num_workers=config.data.num_workers)\n\t    print('val-loader length: ', len(val_loader))\n\t    if LOCAL_RANK == 0:\n\t        print('Baseline Model Training ... \\n')\n\t    model = PARSeq(**config.model)\n\t    save_name = os.path.join(config.log_dir, 'Baseline')\n\t    ckpt_savepath = f'{save_name}_parseq_ckpt.pth'\n\t    best_ckpt_savepath = f'{save_name}_best_parseq_ckpt.pth'\n\t    if not config.get('resume', None) and config.get('pretrained', None):\n\t        pretrained_ckpt = torch.load(config.pretrained_ckpt, map_location='cuda:{}'.format(LOCAL_RANK))\n", "        model.load_state_dict(pretrained_ckpt['model'])\n\t    if config.get('resume', None):  \n\t        ckpt = torch.load(config.resume, map_location='cuda:{}'.format(LOCAL_RANK))\n\t        model.load_state_dict(ckpt['model'])\n\t    model = model.to(LOCAL_RANK)\n\t    device = torch.device('cuda:{}'.format(LOCAL_RANK))\n\t    model._device = device\n\t    model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK, find_unused_parameters=True)\n\t    estimated_stepping_batches = config.trainer.num_iters\n\t    # setting optimizers and schedular\n", "    filtered_parameters = []\n\t    params_num = []\n\t    for p in filter(lambda p: p.requires_grad, model.parameters()):\n\t        filtered_parameters.append(p)\n\t        params_num.append(np.prod(p.size()))\n\t    print('Trainable params num : ', sum(params_num))\n\t    config.model.lr = config.model.lr * math.sqrt(WORLD_SIZE) * config.model.batch_size / 256\n\t    optimizer = torch.optim.AdamW(filtered_parameters, lr=config.model.lr, \\\n\t                                weight_decay=config.model.weight_decay)\n\t    sched = OneCycleLR(optimizer, config.model.lr, estimated_stepping_batches, \\\n", "                       pct_start=config.model.warmup_pct,\n\t                           cycle_momentum=False)\n\t    iter_start = 0\n\t    if config.get('resume', None):\n\t        optimizer.load_state_dict(ckpt['optimizer'])\n\t        iter_start = ckpt['iter']\n\t    validate_iter = config.trainer.validate_iter\n\t    num_iters = config.trainer.num_iters\n\t    best_acc = 0\n\t    with tqdm(range(iter_start, num_iters)) as t_iter:\n", "        for iterr in t_iter:\n\t            if iterr % validate_iter == 0:\n\t                start_timer = time.time()\n\t            img, label = datamodule.get_batch()\n\t            img = img.to(device)\n\t            loss = model.module.training_step(img, label)                \n\t            log_loss = (torch.sum(sync_across_gpus(loss))/WORLD_SIZE)\n\t            optimizer.zero_grad()\n\t            loss.backward()\n\t            optimizer.step()\n", "            sched.step()\n\t            t_iter.set_postfix(Loss=loss.item())\n\t            exp_log.add_scalars('train/loss', {f'process_{LOCAL_RANK}':loss.item()}, iterr)\n\t            if LOCAL_RANK == 0:\n\t                exp_log.add_scalars('train/loss', {f'average':log_loss.item()}, iterr)\n\t                exp_log.add_scalars('train/lr', {'lr_change':optimizer.param_groups[0]['lr']}, iterr)\n\t            if iterr > 0 and iterr % validate_iter == 0:\n\t                torch.cuda.synchronize()\n\t                outputs = []\n\t                for batch_idx, val_data in tqdm(enumerate(val_loader)):\n", "                    with torch.no_grad():\n\t                        img, label = val_data\n\t                        img = img.to(device)\n\t                        outputs.append(model.module.validation_step((img, label), batch_idx))\n\t                acc, ned, loss = model.module._aggregate_results(outputs)\n\t                exp_log.add_scalars('val/loss', {f'process_{LOCAL_RANK}':loss}, iterr)\n\t                exp_log.add_scalars('val/acc', {f'process_{LOCAL_RANK}':acc}, iterr)\n\t                exp_log.add_scalars('val/ned', {f'process_{LOCAL_RANK}':ned}, iterr)\n\t                acc_avg = (torch.sum(sync_across_gpus(torch.tensor(acc).to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n\t                ned_avg = (torch.sum(sync_across_gpus(torch.tensor(ned).to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n", "                loss_avg = (torch.sum(sync_across_gpus(loss.to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n\t                end_timer = time.time()\n\t                elapsed = end_timer - start_timer\n\t                time_avg = (torch.sum(sync_across_gpus(torch.tensor(elapsed).to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n\t                exp_log.add_scalars('train/time', {f'time_per_{validate_iter}iter_{LOCAL_RANK}':elapsed}, iterr)\n\t                if LOCAL_RANK == 0:\n\t                    print(\"Validation Accuracy: \", acc_avg.item()*100)\n\t                    print(\"Validation NED: \", ned_avg.item()*100)\n\t                    print(\"Validation Loss: \", loss_avg.item())\n\t                    exp_log.add_scalars('val/loss', {f'avg_loss':loss_avg.item()}, iterr)\n", "                    exp_log.add_scalars('val/acc', {f'avg_acc':acc_avg.item()}, iterr)\n\t                    exp_log.add_scalars('val/ned', {f'avg_ned':ned_avg.item()}, iterr)\n\t                    exp_log.add_scalars('train/time', {f'avg_time_per_{validate_iter}iter':time_avg.item()}, iterr)\n\t                    torch.save({'model':model.module.state_dict(), 'optimizer':optimizer.state_dict(), 'cfg':config}, ckpt_savepath)\n\t                    if best_acc < acc_avg:\n\t                        best_acc = acc_avg\n\t                        shutil.copyfile(ckpt_savepath, best_ckpt_savepath)\n\t    exp_log.close()\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument('config', help='path of baseline or CR-based self-supervised config')\n\t    #parser.add_argument('--korean_chars', type=str, default='./configs/korean_charset.txt', help='korean characters list text file')\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "pytorch2onnx.py", "chunked_list": ["import torch._C\n\timport torch\n\timport torch.serialization\n\timport onnx\n\timport onnxruntime as rt\n\timport numpy as np\n\tfrom model.parseq_test import PARSeq\n\tfrom omegaconf import DictConfig, open_dict, OmegaConf\n\t# args\n\tOUTPUT_FILE = 'parseq_ar_r1.onnx'\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\tckpt_path = '/outputs/baseline_all/best_parseq_ckpt.pth'\n\tconfig_path = '/outputs/baseline_all/config.yaml'\n\t# config load\n\tconfig = OmegaConf.load(config_path)\n\t# config.model.decode_ar = False\n\t# config.model.decode_ar = False\n\t# model load \n\tmodel = PARSeq(**config.model)\n\t# h, w = config.model.img_size\n", "# model = torch.jit.script(model, example_inputs={model: torch.randn(1, 3, h, w, requires_grad=True)})\n\tmodel.load_state_dict(torch.load(ckpt_path)['model'])\n\tmodel._device = torch.device(device)\n\tmodel = model.eval().to(device)\n\t# print('Model parameters: ', config.model, sep='\\n')\n\t# input define\n\th, w = config.model.img_size\n\tx = torch.randn(1, 3, h, w)\n\tx = x.to(device)\n\t# input test results\n", "out = model(x)\n\t#Onnx export details\n\tdynamic_axes = None\n\tdynamic_export = False\n\topset_version = 14\n\tshow = True\n\t#dynamic_axes = {'input' : {0 : 'batch_size'},\n\t#                     'output' : {0 : 'batch_size'}}\n\tif dynamic_export:\n\t    dynamic_axes = {\n", "        'input': {\n\t            0: 'batch',\n\t#             1: 'channel',\n\t#             2: 'height',\n\t#             3: 'widht'\n\t        },\n\t        'output': {\n\t            0: 'batch',\n\t            1: 'max_len',\n\t#             2: 'charset_len'\n", "        }\n\t    }\n\twith torch.no_grad():\n\t    torch.onnx.export(\n\t        model, x,\n\t        OUTPUT_FILE,\n\t        training=None,\n\t        input_names=['input'],\n\t        output_names=['output'],\n\t        export_params=True,\n", "        verbose=show,\n\t        opset_version=opset_version,\n\t        dynamic_axes=dynamic_axes)\n\t    print(f'Successfully exported ONNX model: {OUTPUT_FILE}')\n\t# helper function\n\tdef to_numpy(tensor):\n\t    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\t## Onnx and torch model output similarity check\n\t# Onnx model loading\n\tonnx_model = onnx.load(OUTPUT_FILE)\n", "onnx.checker.check_model(onnx_model)\n\tort_session = rt.InferenceSession(OUTPUT_FILE)\n\t# compute ONNX Runtime output prediction\n\tort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n\tort_outs = ort_session.run(None, ort_inputs)\n\t# compare ONNX Runtime and PyTorch results\n\tnp.testing.assert_allclose(to_numpy(out), ort_outs[0], rtol=1e-05, atol=1e-01)\n\tprint(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"]}
{"filename": "test_onnx.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Scene Text Recognition Model Hub\n\t# Copyright 2022 Darwin Bautista\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     https://www.apache.org/licenses/LICENSE-2.0\n\t#\n", "# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom glob import glob\n\timport argparse\n\timport string\n\timport sys\n\timport os\n", "from dataclasses import dataclass\n\tfrom typing import Sequence, Any, Optional, Tuple, List\n\tfrom omegaconf import DictConfig, open_dict, OmegaConf\n\tfrom tqdm import tqdm\n\timport torch\n\timport onnx\n\timport onnxruntime as rt\n\tfrom data.module import SceneTextDataModule\n\tfrom model.parseq_test import PARSeq\n\tfrom model.tokenizer_utils import Tokenizer\n", "import numpy as np\n\tfrom nltk import edit_distance\n\tfrom torch import Tensor\n\t@dataclass\n\tclass BatchResult:\n\t    num_samples: int\n\t    correct: int\n\t    ned: float\n\t    confidence: float\n\t    label_length: int\n", "#     loss: Tensor\n\t#     loss_numel: int\n\t@dataclass\n\tclass Result:\n\t    dataset: str\n\t    num_samples: int\n\t    accuracy: float\n\t    ned: float\n\t    confidence: float\n\t    label_length: float\n", "def to_numpy(tensor):\n\t    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\tdef print_results_table(results: List[Result], file=None):\n\t    w = max(map(len, map(getattr, results, ['dataset'] * len(results))))\n\t    w = max(w, len('Dataset'), len('Combined'))\n\t    print('| {:<{w}} | # samples | Accuracy | 1 - NED | Confidence | Label Length |'.format('Dataset', w=w), file=file)\n\t    print('|:{:-<{w}}:|----------:|---------:|--------:|-----------:|-------------:|'.format('----', w=w), file=file)\n\t    c = Result('Combined', 0, 0, 0, 0, 0)\n\t    for res in results:\n\t        c.num_samples += res.num_samples\n", "        c.accuracy += res.num_samples * res.accuracy\n\t        c.ned += res.num_samples * res.ned\n\t        c.confidence += res.num_samples * res.confidence\n\t        c.label_length += res.num_samples * res.label_length\n\t        print(f'| {res.dataset:<{w}} | {res.num_samples:>9} | {res.accuracy:>8.2f} | {res.ned:>7.2f} '\n\t              f'| {res.confidence:>10.2f} | {res.label_length:>12.2f} |', file=file)\n\t    c.accuracy /= c.num_samples\n\t    c.ned /= c.num_samples\n\t    c.confidence /= c.num_samples\n\t    c.label_length /= c.num_samples\n", "    print('|-{:-<{w}}-|-----------|----------|---------|------------|--------------|'.format('----', w=w), file=file)\n\t    print(f'| {c.dataset:<{w}} | {c.num_samples:>9} | {c.accuracy:>8.2f} | {c.ned:>7.2f} '\n\t          f'| {c.confidence:>10.2f} | {c.label_length:>12.2f} |', file=file)\n\t@torch.inference_mode()\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--config', help='path of baseline or CR-based self-supervised config')\n\t    parser.add_argument('--data_root', default='data')\n\t    parser.add_argument('--batch_size', type=int, default=126)\n\t    parser.add_argument('--num_workers', type=int, default=4)\n", "    parser.add_argument('--cased', action='store_true', default=False, help='Cased comparison')\n\t    parser.add_argument('--punctuation', action='store_true', default=False, help='Check punctuation')\n\t    parser.add_argument('--new', action='store_true', default=False, help='Evaluate on new benchmark datasets')\n\t    parser.add_argument('--rotation', type=int, default=0, help='Angle of rotation (counter clockwise) in degrees.')\n\t    parser.add_argument('--device', default='cuda:2')\n\t    args = parser.parse_args()\n\t    args.config = '/outputs/baseline_all/config.yaml'\n\t    OUTPUT_FILE = 'parseq_ar_r1.onnx'\n\t    config = OmegaConf.load(args.config)\n\t    print(config.data.charset_train)\n", "    tokenizer = Tokenizer(config.data.charset_train)\n\t    # Onnx model loading\n\t    onnx_model = onnx.load(OUTPUT_FILE)\n\t    onnx.checker.check_model(onnx_model)\n\t    ort_session = rt.InferenceSession(OUTPUT_FILE)\n\t    args.data_root = 'data/'\n\t    train_dir = os.path.join(args.data_root, 'train')\n\t    val_dir = os.path.join(args.data_root, 'valid')\n\t    test_dir = os.path.join(args.data_root, 'valid')\n\t    datamodule = SceneTextDataModule(root_dir = args.data_root, \n", "                                      train_dir = train_dir, \n\t                                      val_dir = val_dir,\n\t                                      test_dir = test_dir,\n\t                                      img_size = config.data.img_size,\n\t                                      max_label_length = config.data.max_label_length,\n\t                                      charset_train = config.data.charset_test, # hp.charset_train,\n\t                                      charset_test = config.data.charset_test, # hp.charset_test,\n\t                                      batch_size = args.batch_size,\n\t                                      num_workers = args.num_workers,\n\t                                      remove_whitespace = False, \n", "                                      normalize_unicode = False,\n\t                                      augment = False,\n\t                                      rotation = args.rotation\n\t                                      )\n\t    test_folders = glob(os.path.join(test_dir, '*'))\n\t    test_set= sorted(set([t.split('/')[-1] for t in test_folders]))[3:5]\n\t    results = {}\n\t    max_width = max(map(len, test_set))\n\t    for name, dataloader in datamodule.test_dataloaders(test_set).items():\n\t        total = 0\n", "        correct = 0\n\t        ned = 0\n\t        confidence = 0\n\t        label_length = 0\n\t        for imgs, labels in tqdm(iter(dataloader), desc=f'{name:>{max_width}}'): # f'{name:>{max_width}}'\n\t            res = _eval_step(ort_session, (imgs.to(args.device), labels), tokenizer)['output']\n\t            total += res.num_samples\n\t            correct += res.correct\n\t            ned += res.ned\n\t            confidence += res.confidence\n", "            label_length += res.label_length\n\t        accuracy = 100 * correct / total\n\t        mean_ned = 100 * (1 - ned / total)\n\t        mean_conf = 100 * confidence / total\n\t        mean_label_length = label_length / total\n\t        results[name] = Result(name, total, accuracy, mean_ned, mean_conf, mean_label_length)\n\t#         break\n\t    result_groups = {\n\t        t : [t] for t in test_set\n\t    }\n", "    if args.new:\n\t        result_groups.update({'New': SceneTextDataModule.TEST_NEW})\n\t    save_folder = os.path.join('/'.join(args.config.split('/')[:-1]), 'test_logs')\n\t    if not os.path.exists(save_folder):\n\t        os.makedirs(save_folder)\n\t    with open(os.path.join(save_folder, os.path.basename(args.config)[:-4] + '.log.txt'), 'w') as f:\n\t        for out in [f, sys.stdout]:\n\t            for group, subset in result_groups.items():\n\t                print(f'{group} set:', file=out)\n\t                print_results_table([results[s] for s in subset], out)  \n", "                print('\\n', file=out)\n\tdef _eval_step(ort_session, batch, tokenizer): #-> Optional[STEP_OUTPUT]:\n\t    images, labels = batch\n\t    correct = 0\n\t    total = 0\n\t    ned = 0\n\t    confidence = 0\n\t    label_length = 0\n\t    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(images)}\n\t    logits = ort_session.run(None, ort_inputs)[0]\n", "    probs = torch.tensor(logits).softmax(-1)\n\t    preds, probs = tokenizer.decode(probs)\n\t    for pred, prob, gt in zip(preds, probs, labels):\n\t        confidence += prob.prod().item()\n\t#             pred = charset_adapter(pred)\n\t        # Follow ICDAR 2019 definition of N.E.D.\n\t        ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n\t        if pred == gt:\n\t            correct += 1\n\t        total += 1\n", "        label_length += len(pred)\n\t    return dict(output=BatchResult(total, correct, ned, confidence, label_length))\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "test_trt.py", "chunked_list": ["from glob import glob\n\timport argparse\n\timport string\n\timport sys\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom typing import Sequence, Any, Optional, Tuple, List\n\tfrom omegaconf import DictConfig, open_dict, OmegaConf\n\tfrom tqdm import tqdm\n\timport torch\n", "import onnx\n\timport onnxruntime as rt\n\tfrom data.module import SceneTextDataModule\n\tfrom model.parseq import PARSeq\n\tfrom model.tokenizer_utils import Tokenizer\n\timport numpy as np\n\tfrom nltk import edit_distance\n\tfrom torch import Tensor\n\timport tensorrt as trt\n\timport pycuda.driver as cuda\n", "try:\n\t    import pycuda.autoprimaryctx\n\texcept ModuleNotFoundError:\n\t    import pycuda.autoinit\n\timport common\n\tTRT_LOGGER = trt.Logger()\n\t@dataclass\n\tclass BatchResult:\n\t    num_samples: int\n\t    correct: int\n", "    ned: float\n\t    confidence: float\n\t    label_length: int\n\t#     loss: Tensor\n\t#     loss_numel: int\n\t@dataclass\n\tclass Result:\n\t    dataset: str\n\t    num_samples: int\n\t    accuracy: float\n", "    ned: float\n\t    confidence: float\n\t    label_length: float\n\tdef to_numpy(tensor):\n\t    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\tdef print_results_table(results: List[Result], file=None):\n\t    w = max(map(len, map(getattr, results, ['dataset'] * len(results))))\n\t    w = max(w, len('Dataset'), len('Combined'))\n\t    print('| {:<{w}} | # samples | Accuracy | 1 - NED | Confidence | Label Length |'.format('Dataset', w=w), file=file)\n\t    print('|:{:-<{w}}:|----------:|---------:|--------:|-----------:|-------------:|'.format('----', w=w), file=file)\n", "    c = Result('Combined', 0, 0, 0, 0, 0)\n\t    for res in results:\n\t        c.num_samples += res.num_samples\n\t        c.accuracy += res.num_samples * res.accuracy\n\t        c.ned += res.num_samples * res.ned\n\t        c.confidence += res.num_samples * res.confidence\n\t        c.label_length += res.num_samples * res.label_length\n\t        print(f'| {res.dataset:<{w}} | {res.num_samples:>9} | {res.accuracy:>8.2f} | {res.ned:>7.2f} '\n\t              f'| {res.confidence:>10.2f} | {res.label_length:>12.2f} |', file=file)\n\t    c.accuracy /= c.num_samples\n", "    c.ned /= c.num_samples\n\t    c.confidence /= c.num_samples\n\t    c.label_length /= c.num_samples\n\t    print('|-{:-<{w}}-|-----------|----------|---------|------------|--------------|'.format('----', w=w), file=file)\n\t    print(f'| {c.dataset:<{w}} | {c.num_samples:>9} | {c.accuracy:>8.2f} | {c.ned:>7.2f} '\n\t          f'| {c.confidence:>10.2f} | {c.label_length:>12.2f} |', file=file)\n\tdef get_engine(engine_file_path):\n\t    if os.path.exists(engine_file_path):\n\t        # If a serialized engine exists, use it instead of building an engine.\n\t        print(\"Reading engine from file {}\".format(engine_file_path))\n", "        with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n\t            return runtime.deserialize_cuda_engine(f.read())\n\t    else:\n\t        raise Exception(f\"File not found: {engine_file_path}\")\n\t@torch.inference_mode()\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--config', help='path of baseline or CR-based self-supervised config')\n\t    parser.add_argument('--data_root', default='data')\n\t    parser.add_argument('--batch_size', type=int, default=1)\n", "    parser.add_argument('--num_workers', type=int, default=4)\n\t    parser.add_argument('--cased', action='store_true', default=False, help='Cased comparison')\n\t    parser.add_argument('--punctuation', action='store_true', default=False, help='Check punctuation')\n\t    parser.add_argument('--new', action='store_true', default=False, help='Evaluate on new benchmark datasets')\n\t    parser.add_argument('--rotation', type=int, default=0, help='Angle of rotation (counter clockwise) in degrees.')\n\t    parser.add_argument('--device', default='cuda:2')\n\t    args = parser.parse_args()\n\t    args.config = 'outputs/exp_logs_baseline_all/config.yaml'\n\t    engine_file_path = 'parseq_ar_r1.trt'\n\t    config = OmegaConf.load(args.config)\n", "    tokenizer = Tokenizer(config.data.charset_train)\n\t    args.data_root = 'dataset'\n\t    train_dir = os.path.join(args.data_root, 'train')\n\t    val_dir = os.path.join(args.data_root, 'valid')\n\t    test_dir = os.path.join(args.data_root, 'valid')\n\t    datamodule = SceneTextDataModule(root_dir = args.data_root, \n\t                                      train_dir = train_dir, \n\t                                      val_dir = val_dir,\n\t                                      test_dir = test_dir,\n\t                                      img_size = config.data.img_size,\n", "                                      max_label_length = config.data.max_label_length,\n\t                                      charset_train = config.data.charset_test, # hp.charset_train,\n\t                                      charset_test = config.data.charset_test, # hp.charset_test,\n\t                                      batch_size = args.batch_size,\n\t                                      num_workers = args.num_workers,\n\t                                      remove_whitespace = False, \n\t                                      normalize_unicode = False,\n\t                                      augment = False,\n\t                                      rotation = args.rotation\n\t                                      )\n", "    test_folders = glob(os.path.join(test_dir, '*'))\n\t    test_set= sorted(set([t.split('/')[-1] for t in test_folders]))[:]\n\t    results = {}\n\t    max_width = max(map(len, test_set))\n\t    # trt engine \n\t    with get_engine(engine_file_path) as engine, engine.create_execution_context() as context:\n\t        inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n\t        # Do inference\n\t        for name, dataloader in datamodule.test_dataloaders(test_set).items():\n\t            print(\"Running inference on {}...\".format(name))\n", "            total = 0\n\t            correct = 0\n\t            ned = 0\n\t            confidence = 0\n\t            label_length = 0\n\t            for imgs, labels in tqdm(iter(dataloader), desc=f'{name:>{max_width}}'): # f'{name:>{max_width}}'\n\t                inputs[0].host = np.array(imgs, dtype=np.float32)\n\t                trt_outputs = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n\t                trt_outputs = [out.reshape(1, 26, 1794) for out in trt_outputs]\n\t                res = _eval_step(trt_outputs, labels, tokenizer)['output']\n", "                total += res.num_samples\n\t                correct += res.correct\n\t                ned += res.ned\n\t                confidence += res.confidence\n\t                label_length += res.label_length\n\t            accuracy = 100 * correct / total\n\t            mean_ned = 100 * (1 - ned / total)\n\t            mean_conf = 100 * confidence / total\n\t            mean_label_length = label_length / total\n\t            results[name] = Result(name, total, accuracy, mean_ned, mean_conf, mean_label_length)\n", "#         break\n\t    result_groups = {\n\t        t : [t] for t in test_set\n\t    }\n\t    if args.new:\n\t        result_groups.update({'New': SceneTextDataModule.TEST_NEW})\n\t    save_folder = os.path.join('/'.join(args.config.split('/')[:-1]), 'trt_test_logs')\n\t    if not os.path.exists(save_folder):\n\t        os.makedirs(save_folder)\n\t    with open(os.path.join(save_folder, os.path.basename(args.config)[:-4] + '.log.txt'), 'w') as f:\n", "        for out in [f, sys.stdout]:\n\t            for group, subset in result_groups.items():\n\t                print(f'{group} set:', file=out)\n\t                print_results_table([results[s] for s in subset], out)  \n\t                print('\\n', file=out)\n\tdef _eval_step(trt_outputs, labels, tokenizer): #-> Optional[STEP_OUTPUT]:\n\t    correct = 0\n\t    total = 0\n\t    ned = 0\n\t    confidence = 0\n", "    label_length = 0\n\t    logits = trt_outputs[0]\n\t    probs = torch.tensor(logits).softmax(-1)\n\t    preds, probs = tokenizer.decode(probs)\n\t    for pred, prob, gt in zip(preds, probs, labels):\n\t        confidence += prob.prod().item()\n\t        # Follow ICDAR 2019 definition of N.E.D.\n\t        ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n\t        if pred == gt:\n\t            correct += 1\n", "        total += 1\n\t        label_length += len(pred)\n\t    return dict(output=BatchResult(total, correct, ned, confidence, label_length))\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "common.py", "chunked_list": ["#\n\t# SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t# SPDX-License-Identifier: Apache-2.0\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n", "# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\timport argparse\n\timport os\n\timport numpy as np\n\t# Use autoprimaryctx if available (pycuda >= 2021.1) to\n", "# prevent issues with other modules that rely on the primary\n\t# device context.\n\ttry:\n\t    import pycuda.autoprimaryctx\n\texcept ModuleNotFoundError:\n\t    import pycuda.autoinit\n\timport pycuda.driver as cuda\n\timport tensorrt as trt\n\ttry:\n\t    # Sometimes python does not understand FileNotFoundError\n", "    FileNotFoundError\n\texcept NameError:\n\t    FileNotFoundError = IOError\n\tEXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n\tdef GiB(val):\n\t    return val * 1 << 30\n\tdef add_help(description):\n\t    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t    args, _ = parser.parse_known_args()\n\tdef find_sample_data(description=\"Runs a TensorRT Python sample\", subfolder=\"\", find_files=[], err_msg=\"\"):\n", "    \"\"\"\n\t    Parses sample arguments.\n\t    Args:\n\t        description (str): Description of the sample.\n\t        subfolder (str): The subfolder containing data relevant to this sample\n\t        find_files (str): A list of filenames to find. Each filename will be replaced with an absolute path.\n\t    Returns:\n\t        str: Path of data directory.\n\t    \"\"\"\n\t    # Standard command-line arguments for all samples.\n", "    kDEFAULT_DATA_ROOT = os.path.join(os.sep, \"usr\", \"src\", \"tensorrt\", \"data\")\n\t    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t    parser.add_argument(\n\t        \"-d\",\n\t        \"--datadir\",\n\t        help=\"Location of the TensorRT sample data directory, and any additional data directories.\",\n\t        action=\"append\",\n\t        default=[kDEFAULT_DATA_ROOT],\n\t    )\n\t    args, _ = parser.parse_known_args()\n", "    def get_data_path(data_dir):\n\t        # If the subfolder exists, append it to the path, otherwise use the provided path as-is.\n\t        data_path = os.path.join(data_dir, subfolder)\n\t        if not os.path.exists(data_path):\n\t            if data_dir != kDEFAULT_DATA_ROOT:\n\t                print(\"WARNING: \" + data_path + \" does not exist. Trying \" + data_dir + \" instead.\")\n\t            data_path = data_dir\n\t        # Make sure data directory exists.\n\t        if not (os.path.exists(data_path)) and data_dir != kDEFAULT_DATA_ROOT:\n\t            print(\n", "                \"WARNING: {:} does not exist. Please provide the correct data path with the -d option.\".format(\n\t                    data_path\n\t                )\n\t            )\n\t        return data_path\n\t    data_paths = [get_data_path(data_dir) for data_dir in args.datadir]\n\t    return data_paths, locate_files(data_paths, find_files, err_msg)\n\tdef locate_files(data_paths, filenames, err_msg=\"\"):\n\t    \"\"\"\n\t    Locates the specified files in the specified data directories.\n", "    If a file exists in multiple data directories, the first directory is used.\n\t    Args:\n\t        data_paths (List[str]): The data directories.\n\t        filename (List[str]): The names of the files to find.\n\t    Returns:\n\t        List[str]: The absolute paths of the files.\n\t    Raises:\n\t        FileNotFoundError if a file could not be located.\n\t    \"\"\"\n\t    found_files = [None] * len(filenames)\n", "    for data_path in data_paths:\n\t        # Find all requested files.\n\t        for index, (found, filename) in enumerate(zip(found_files, filenames)):\n\t            if not found:\n\t                file_path = os.path.abspath(os.path.join(data_path, filename))\n\t                if os.path.exists(file_path):\n\t                    found_files[index] = file_path\n\t    # Check that all files were found\n\t    for f, filename in zip(found_files, filenames):\n\t        if not f or not os.path.exists(f):\n", "            raise FileNotFoundError(\n\t                \"Could not find {:}. Searched in data paths: {:}\\n{:}\".format(filename, data_paths, err_msg)\n\t            )\n\t    return found_files\n\t# Simple helper data class that's a little nicer to use than a 2-tuple.\n\tclass HostDeviceMem(object):\n\t    def __init__(self, host_mem, device_mem):\n\t        self.host = host_mem\n\t        self.device = device_mem\n\t    def __str__(self):\n", "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n\t    def __repr__(self):\n\t        return self.__str__()\n\t# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n\tdef allocate_buffers(engine):\n\t    inputs = []\n\t    outputs = []\n\t    bindings = []\n\t    stream = cuda.Stream()\n\t    for binding in engine:\n", "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n\t        dtype = trt.nptype(engine.get_binding_dtype(binding))\n\t        # Allocate host and device buffers\n\t        host_mem = cuda.pagelocked_empty(size, dtype)\n\t        device_mem = cuda.mem_alloc(host_mem.nbytes)\n\t        # Append the device buffer to device bindings.\n\t        bindings.append(int(device_mem))\n\t        # Append to the appropriate list.\n\t        if engine.binding_is_input(binding):\n\t            inputs.append(HostDeviceMem(host_mem, device_mem))\n", "        else:\n\t            outputs.append(HostDeviceMem(host_mem, device_mem))\n\t    return inputs, outputs, bindings, stream\n\t# This function is generalized for multiple inputs/outputs.\n\t# inputs and outputs are expected to be lists of HostDeviceMem objects.\n\tdef do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n\t    # Transfer input data to the GPU.\n\t    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n\t    # Run inference.\n\t    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n", "    # Transfer predictions back from the GPU.\n\t    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n\t    # Synchronize the stream\n\t    stream.synchronize()\n\t    # Return only the host outputs.\n\t    return [out.host for out in outputs]\n\t# This function is generalized for multiple inputs/outputs for full dimension networks.\n\t# inputs and outputs are expected to be lists of HostDeviceMem objects.\n\tdef do_inference_v2(context, bindings, inputs, outputs, stream):\n\t    # Transfer input data to the GPU.\n", "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n\t    # Run inference.\n\t    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n\t    # Transfer predictions back from the GPU.\n\t    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n\t    # Synchronize the stream\n\t    stream.synchronize()\n\t    # Return only the host outputs.\n\t    return [out.host for out in outputs]"]}
{"filename": "test.py", "chunked_list": ["from glob import glob\n\timport argparse\n\timport string\n\timport sys\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom typing import List\n\tfrom omegaconf import DictConfig, open_dict, OmegaConf\n\tfrom tqdm import tqdm\n\timport torch\n", "import string\n\tfrom data.module import SceneTextDataModule\n\tfrom model.parseq import PARSeq\n\t@dataclass\n\tclass Result:\n\t    dataset: str\n\t    num_samples: int\n\t    accuracy: float\n\t    ned: float\n\t    confidence: float\n", "    label_length: float\n\tdef print_results_table(results: List[Result], file=None):\n\t    w = max(map(len, map(getattr, results, ['dataset'] * len(results))))\n\t    w = max(w, len('Dataset'), len('Combined'))\n\t    print('| {:<{w}} | # samples | Accuracy | 1 - NED | Confidence | Label Length |'.format('Dataset', w=w), file=file)\n\t    print('|:{:-<{w}}:|----------:|---------:|--------:|-----------:|-------------:|'.format('----', w=w), file=file)\n\t    c = Result('Combined', 0, 0, 0, 0, 0)\n\t    for res in results:\n\t        c.num_samples += res.num_samples\n\t        c.accuracy += res.num_samples * res.accuracy\n", "        c.ned += res.num_samples * res.ned\n\t        c.confidence += res.num_samples * res.confidence\n\t        c.label_length += res.num_samples * res.label_length\n\t        print(f'| {res.dataset:<{w}} | {res.num_samples:>9} | {res.accuracy:>8.2f} | {res.ned:>7.2f} '\n\t              f'| {res.confidence:>10.2f} | {res.label_length:>12.2f} |', file=file)\n\t    c.accuracy /= c.num_samples\n\t    c.ned /= c.num_samples\n\t    c.confidence /= c.num_samples\n\t    c.label_length /= c.num_samples\n\t    print('|-{:-<{w}}-|-----------|----------|---------|------------|--------------|'.format('----', w=w), file=file)\n", "    print(f'| {c.dataset:<{w}} | {c.num_samples:>9} | {c.accuracy:>8.2f} | {c.ned:>7.2f} '\n\t          f'| {c.confidence:>10.2f} | {c.label_length:>12.2f} |', file=file)\n\t@torch.inference_mode()\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('config', help='path of baseline or CR-based self-supervised config')\n\t    parser.add_argument('--checkpoint', default= '', help=\"Model checkpoint (or 'pretrained=<model_id>')\")\n\t    #parser.add_argument('--korean_chars', type=str, default='./configs/korean_charset.txt', help='korean characters list text file')\n\t    parser.add_argument('--data_root', default='data')\n\t    parser.add_argument('--batch_size', type=int, default=512)\n", "    parser.add_argument('--num_workers', type=int, default=4)\n\t    parser.add_argument('--cased', action='store_true', default=False, help='Cased comparison')\n\t    parser.add_argument('--punctuation', action='store_true', default=False, help='Check punctuation')\n\t    parser.add_argument('--new', action='store_true', default=False, help='Evaluate on new benchmark datasets')\n\t    parser.add_argument('--rotation', type=int, default=0, help='Angle of rotation (counter clockwise) in degrees.')\n\t    parser.add_argument('--device', default='cuda:0')\n\t    args = parser.parse_args()\n\t    config = OmegaConf.load(args.config)\n\t    if not args.checkpoint:\n\t        print('\\nDEFAULT CHECKPOINT PATH SET TO CONFIG LOG_DIR.\\n')\n", "        if config.data.consistency_regularization:\n\t            args.checkpoint = os.path.join(config.log_dir, 'CR_best_parseq_ckpt')\n\t        else:\n\t            args.checkpoint = os.path.join(config.log_dir, 'Baseline_best_parseq_ckpt')\n\t    print('CHECKPOINT PATH: ', args.checkpoint)\n\t    # updating korean charset\n\t    chars = \"\"\n\t    with open(\n\t        os.path.join(config.data_loader.character.dict_dir, \"charset.txt\"), \"r\"\n\t    ) as f:\n", "        curr_char = f.read()\n\t    curr_char = curr_char.strip()\n\t    chars += curr_char\n\t    chars = \"\".join(sorted(set(chars)))\n\t    config.model.charset_train = chars\n\t    config.model.charset_test = config.model.charset_train\n\t    # Special handling for PARseq\n\t    if config.model.get('perm_mirrored', False):\n\t        assert config.model.perm_num % 2 == 0, 'perm_num should be even if perm_mirrored = True'\n\t    args.data_root = 'dataset'\n", "    model = PARSeq(**config.model)\n\t    model.load_state_dict(torch.load(args.checkpoint)['model'])\n\t    model._device = args.device\n\t    model = model.eval().to(args.device)\n\t    print('Model parameters: ', config.model, sep='\\n')\n\t    train_dir = os.path.join(args.data_root, 'train')\n\t    val_dir = os.path.join(args.data_root, 'valid')\n\t    test_dir = os.path.join(args.data_root, 'valid')\n\t    datamodule = SceneTextDataModule(root_dir = args.data_root, \n\t                                      train_dir = train_dir, \n", "                                      val_dir = val_dir,\n\t                                      test_dir = test_dir,\n\t                                      img_size = config.data.img_size,\n\t                                      max_label_length = config.data.max_label_length,\n\t                                      charset_train = config.data.charset_test, # hp.charset_train,\n\t                                      charset_test = config.data.charset_test, # hp.charset_test,\n\t                                      batch_size = args.batch_size,\n\t                                      num_workers = args.num_workers,\n\t                                      remove_whitespace = False, \n\t                                      normalize_unicode = False,\n", "                                      augment = False,\n\t                                      rotation = args.rotation\n\t                                      )\n\t    test_folders = glob(os.path.join(test_dir, '*'))\n\t    test_set= sorted(set([t.split('/')[-1] for t in test_folders]))\n\t    results = {}\n\t    max_width = max(map(len, test_set))\n\t    for name, dataloader in datamodule.test_dataloaders(test_set).items():\n\t        total = 0\n\t        correct = 0\n", "        ned = 0\n\t        confidence = 0\n\t        label_length = 0\n\t        for imgs, labels in tqdm(iter(dataloader), desc=f'{name:>{max_width}}'): # f'{name:>{max_width}}'\n\t            res = model.test_step((imgs.to(args.device), labels), -1)['output']\n\t            total += res.num_samples\n\t            correct += res.correct\n\t            ned += res.ned\n\t            confidence += res.confidence\n\t            label_length += res.label_length\n", "        accuracy = 100 * correct / total\n\t        mean_ned = 100 * (1 - ned / total)\n\t        mean_conf = 100 * confidence / total\n\t        mean_label_length = label_length / total\n\t        results[name] = Result(name, total, accuracy, mean_ned, mean_conf, mean_label_length)\n\t    result_groups = {\n\t        t : [t] for t in test_set\n\t    }\n\t    if args.new:\n\t        result_groups.update({'New': SceneTextDataModule.TEST_NEW})\n", "    save_folder = os.path.join('/'.join(args.checkpoint.split('/')[:-1]), 'test_logs')\n\t    if not os.path.exists(save_folder):\n\t        os.makedirs(save_folder)\n\t    with open(os.path.join(save_folder, os.path.basename(args.checkpoint)[:-4] + '.log.txt'), 'w') as f:\n\t        for out in [f, sys.stdout]:\n\t            for group, subset in result_groups.items():\n\t                print(f'{group} set:', file=out)\n\t                print_results_table([results[s] for s in subset], out)  \n\t                print('\\n', file=out)\n\tif __name__ == '__main__':\n", "    main()\n"]}
{"filename": "onnx2trt.py", "chunked_list": ["from __future__ import print_function\n\timport numpy as np\n\timport tensorrt as trt\n\timport pycuda.driver as cuda\n\t# Use autoprimaryctx if available (pycuda >= 2021.1) to\n\t# prevent issues with other modules that rely on the primary\n\t# device context.\n\ttry:\n\t    import pycuda.autoprimaryctx\n\texcept ModuleNotFoundError:\n", "    import pycuda.autoinit\n\timport sys, os\n\tsys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n\timport common\n\t# from downloader import getFilePath\n\tTRT_LOGGER = trt.Logger()\n\timport torch\n\tfrom torchvision.transforms import Normalize\n\tfrom skimage import io\n\tfrom skimage.transform import resize\n", "from skimage.color import gray2rgb\n\timport matplotlib.pyplot as plt\n\tfrom model.tokenizer_utils import Tokenizer\n\tdef preprocess_image(img):\n\t    norm = Normalize(0.5, 0.5)\n\t    result = norm(torch.from_numpy(img).transpose(0,2).transpose(1,2))\n\t    return np.array(result, dtype=np.float16)\n\tdef get_engine(onnx_file_path, engine_file_path=\"\"):\n\t    \"\"\"Attempts to load a serialized engine if available, otherwise builds a new TensorRT engine and saves it.\"\"\"\n\t    def build_engine():\n", "        \"\"\"Takes an ONNX file and creates a TensorRT engine to run inference with\"\"\"\n\t        with trt.Builder(TRT_LOGGER) as builder, builder.create_network(\n\t            common.EXPLICIT_BATCH\n\t        ) as network, builder.create_builder_config() as config, trt.OnnxParser(\n\t            network, TRT_LOGGER\n\t        ) as parser, trt.Runtime(\n\t            TRT_LOGGER\n\t        ) as runtime:\n\t            config.max_workspace_size = 1 << 28  # 256MiB\n\t            builder.max_batch_size = 1\n", "            # Parse model file\n\t            if not os.path.exists(onnx_file_path):\n\t                print(\n\t                    \"ONNX file {} not found, please run yolov3_to_onnx.py first to generate it.\".format(onnx_file_path)\n\t                )\n\t                exit(0)\n\t            print(\"Loading ONNX file from path {}...\".format(onnx_file_path))\n\t            with open(onnx_file_path, \"rb\") as model:\n\t                print(\"Beginning ONNX file parsing\")\n\t                if not parser.parse(model.read()):\n", "                    print(\"ERROR: Failed to parse the ONNX file.\")\n\t                    for error in range(parser.num_errors):\n\t                        print(parser.get_error(error))\n\t                    return None\n\t            # The actual yolov3.onnx is generated with batch size 64. Reshape input to batch size 1\n\t            network.get_input(0).shape = [1, 3, 32, 128]\n\t            print(\"Completed parsing of ONNX file\")\n\t            print(\"Building an engine from file {}; this may take a while...\".format(onnx_file_path))\n\t            plan = builder.build_serialized_network(network, config)\n\t            engine = runtime.deserialize_cuda_engine(plan)\n", "            print(\"Completed creating Engine\")\n\t            with open(engine_file_path, \"wb\") as f:\n\t                f.write(plan)\n\t            return engine\n\t    if os.path.exists(engine_file_path):\n\t        # If a serialized engine exists, use it instead of building an engine.\n\t        print(\"Reading engine from file {}\".format(engine_file_path))\n\t        with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n\t            return runtime.deserialize_cuda_engine(f.read())\n\t    else:\n", "        return build_engine()\n\tdef main():\n\t    \"\"\"Create a TensorRT engine for ONNX-based YOLOv3-608 and run inference.\"\"\"\n\t    # Try to load a previously generated YOLOv3-608 network graph in ONNX format:\n\t    onnx_file_path = \"parseq_ar_r1.onnx\"\n\t    engine_file_path = \"parseq_ar_r1.trt\"\n\t#     test_folder_path = ''\n\t    # Download a dog image and save it to the following file path:\n\t#     input_image_path = getFilePath(\"samples/python/yolov3_onnx/dog.jpg\")\n\t    img_path = 'sample.png'\n", "    img = resize(gray2rgb(io.imread(img_path)), (32, 128))\n\t    input_batch = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), 1, axis=0), dtype=np.float32)\n\t    print('input shape: ', input_batch.shape)\n\t    preprocessed_images = np.array([preprocess_image(image) for image in input_batch])\n\t    # Do inference with TensorRT\n\t    trt_outputs = []\n\t    with get_engine(onnx_file_path, engine_file_path) as engine, engine.create_execution_context() as context:\n\t        inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n\t        # Do inference\n\t        print(\"Running inference on image {}...\".format(img_path))\n", "        # Set host input to the image. The common.do_inference function will copy the input to the GPU before executing.\n\t        inputs[0].host = preprocessed_images\n\t        trt_outputs = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n\t    trt_outputs = [out.reshape(1, 26, 1794) for out in trt_outputs]\n\t    print('Type of Generated outputs: ', type(trt_outputs))\n\t    print('Len of Generated outputs: ', (trt_outputs[0].shape))\n\t    print('Generated outputs: ', trt_outputs)\n\t    pred_labels = post_process(trt_outputs)\n\t    print('Final results: ', pred_labels)\n\tdef post_process(trt_outputs):\n", "    with open('./char_dicts/charset.txt', 'r') as f:\n\t        charset = f.read()\n\t    tokenizer = Tokenizer(charset)\n\t    trt_logits = [torch.nn.functional.softmax(torch.from_numpy(out), dim=-1) for out in trt_outputs]\n\t    final_outputs = []\n\t    for logit in trt_logits:\n\t        preds, probs = tokenizer.decode(logit)\n\t        final_outputs.append([preds[0], probs])\n\t    return final_outputs\n\tif __name__ == \"__main__\":\n", "    main()"]}
{"filename": "data/transforms.py", "chunked_list": ["import math\n\timport numbers\n\timport random\n\timport cv2\n\timport numpy as np\n\tfrom PIL import Image\n\tfrom torchvision import transforms\n\tfrom torchvision.transforms import Compose\n\tclass ImageToArray(object):\n\t    def __call__(self, img):\n", "        return np.array(img)\n\tclass ImageToPIL(object):\n\t    def __call__(self, img):\n\t        return Image.fromarray(img)\n\tdef sample_asym(magnitude, size=None):\n\t    return np.random.beta(1, 4, size) * magnitude\n\tdef sample_sym(magnitude, size=None):\n\t    return (np.random.beta(4, 4, size=size) - 0.5) * 2 * magnitude\n\tdef sample_uniform(low, high, size=None):\n\t    return np.random.uniform(low, high, size=size)\n", "def get_interpolation(type='random'):\n\t    if type == 'random':\n\t        choice = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA]\n\t        interpolation = choice[random.randint(0, len(choice) - 1)]\n\t    elif type == 'nearest':\n\t        interpolation = cv2.INTER_NEAREST\n\t    elif type == 'linear':\n\t        interpolation = cv2.INTER_LINEAR\n\t    elif type == 'cubic':\n\t        interpolation = cv2.INTER_CUBIC\n", "    elif type == 'area':\n\t        interpolation = cv2.INTER_AREA\n\t    else:\n\t        raise TypeError('Interpolation types only nearest, linear, cubic, area are supported!')\n\t    return interpolation\n\tclass CVRandomRotation(object):\n\t    def __init__(self, degrees=15):\n\t        assert isinstance(degrees, numbers.Number), \"degree should be a single number.\"\n\t        assert degrees >= 0, \"degree must be positive.\"\n\t        self.degrees = degrees\n", "    @staticmethod\n\t    def get_params(degrees):\n\t        return sample_sym(degrees)\n\t    def __call__(self, img):\n\t        angle = self.get_params(self.degrees)\n\t        src_h, src_w = img.shape[:2]\n\t        M = cv2.getRotationMatrix2D(center=(src_w / 2, src_h / 2), angle=angle, scale=1.0)\n\t        abs_cos, abs_sin = abs(M[0, 0]), abs(M[0, 1])\n\t        dst_w = int(src_h * abs_sin + src_w * abs_cos)\n\t        dst_h = int(src_h * abs_cos + src_w * abs_sin)\n", "        M[0, 2] += (dst_w - src_w) / 2\n\t        M[1, 2] += (dst_h - src_h) / 2\n\t        flags = get_interpolation()\n\t        return cv2.warpAffine(img, M, (dst_w, dst_h), flags=flags, borderMode=cv2.BORDER_REPLICATE)\n\tclass CVRandomAffine(object):\n\t    def __init__(self, degrees, translate=None, scale=None, shear=None):\n\t        assert isinstance(degrees, numbers.Number), \"degree should be a single number.\"\n\t        assert degrees >= 0, \"degree must be positive.\"\n\t        self.degrees = degrees\n\t        if translate is not None:\n", "            assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n\t                \"translate should be a list or tuple and it must be of length 2.\"\n\t            for t in translate:\n\t                if not (0.0 <= t <= 1.0):\n\t                    raise ValueError(\"translation values should be between 0 and 1\")\n\t        self.translate = translate\n\t        if scale is not None:\n\t            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n\t                \"scale should be a list or tuple and it must be of length 2.\"\n\t            for s in scale:\n", "                if s <= 0:\n\t                    raise ValueError(\"scale values should be positive\")\n\t        self.scale = scale\n\t        if shear is not None:\n\t            if isinstance(shear, numbers.Number):\n\t                if shear < 0:\n\t                    raise ValueError(\"If shear is a single number, it must be positive.\")\n\t                self.shear = [shear]\n\t            else:\n\t                assert isinstance(shear, (tuple, list)) and (len(shear) == 2), \\\n", "                    \"shear should be a list or tuple and it must be of length 2.\"\n\t                self.shear = shear\n\t        else:\n\t            self.shear = shear\n\t    def _get_inverse_affine_matrix(self, center, angle, translate, scale, shear):\n\t        # https://github.com/pytorch/vision/blob/v0.4.0/torchvision/transforms/functional.py#L717\n\t        from numpy import sin, cos, tan\n\t        if isinstance(shear, numbers.Number):\n\t            shear = [shear, 0]\n\t        if not isinstance(shear, (tuple, list)) and len(shear) == 2:\n", "            raise ValueError(\n\t                \"Shear should be a single value or a tuple/list containing \" +\n\t                \"two values. Got {}\".format(shear))\n\t        rot = math.radians(angle)\n\t        sx, sy = [math.radians(s) for s in shear]\n\t        cx, cy = center\n\t        tx, ty = translate\n\t        # RSS without scaling\n\t        a = cos(rot - sy) / cos(sy)\n\t        b = -cos(rot - sy) * tan(sx) / cos(sy) - sin(rot)\n", "        c = sin(rot - sy) / cos(sy)\n\t        d = -sin(rot - sy) * tan(sx) / cos(sy) + cos(rot)\n\t        # Inverted rotation matrix with scale and shear\n\t        # det([[a, b], [c, d]]) == 1, since det(rotation) = 1 and det(shear) = 1\n\t        M = [d, -b, 0,\n\t             -c, a, 0]\n\t        M = [x / scale for x in M]\n\t        # Apply inverse of translation and of center translation: RSS^-1 * C^-1 * T^-1\n\t        M[2] += M[0] * (-cx - tx) + M[1] * (-cy - ty)\n\t        M[5] += M[3] * (-cx - tx) + M[4] * (-cy - ty)\n", "        # Apply center translation: C * RSS^-1 * C^-1 * T^-1\n\t        M[2] += cx\n\t        M[5] += cy\n\t        return M\n\t    @staticmethod\n\t    def get_params(degrees, translate, scale_ranges, shears, height):\n\t        angle = sample_sym(degrees)\n\t        if translate is not None:\n\t            max_dx = translate[0] * height\n\t            max_dy = translate[1] * height\n", "            translations = (np.round(sample_sym(max_dx)), np.round(sample_sym(max_dy)))\n\t        else:\n\t            translations = (0, 0)\n\t        if scale_ranges is not None:\n\t            scale = sample_uniform(scale_ranges[0], scale_ranges[1])\n\t        else:\n\t            scale = 1.0\n\t        if shears is not None:\n\t            if len(shears) == 1:\n\t                shear = [sample_sym(shears[0]), 0.]\n", "            elif len(shears) == 2:\n\t                shear = [sample_sym(shears[0]), sample_sym(shears[1])]\n\t        else:\n\t            shear = 0.0\n\t        return angle, translations, scale, shear\n\t    def __call__(self, img):\n\t        src_h, src_w = img.shape[:2]\n\t        angle, translate, scale, shear = self.get_params(\n\t            self.degrees, self.translate, self.scale, self.shear, src_h)\n\t        M = self._get_inverse_affine_matrix((src_w / 2, src_h / 2), angle, (0, 0), scale, shear)\n", "        M = np.array(M).reshape(2, 3)\n\t        startpoints = [(0, 0), (src_w - 1, 0), (src_w - 1, src_h - 1), (0, src_h - 1)]\n\t        project = lambda x, y, a, b, c: int(a * x + b * y + c)\n\t        endpoints = [(project(x, y, *M[0]), project(x, y, *M[1])) for x, y in startpoints]\n\t        rect = cv2.minAreaRect(np.array(endpoints))\n\t        bbox = cv2.boxPoints(rect).astype(dtype=np.int)\n\t        max_x, max_y = bbox[:, 0].max(), bbox[:, 1].max()\n\t        min_x, min_y = bbox[:, 0].min(), bbox[:, 1].min()\n\t        dst_w = int(max_x - min_x)\n\t        dst_h = int(max_y - min_y)\n", "        M[0, 2] += (dst_w - src_w) / 2\n\t        M[1, 2] += (dst_h - src_h) / 2\n\t        # add translate\n\t        dst_w += int(abs(translate[0]))\n\t        dst_h += int(abs(translate[1]))\n\t        if translate[0] < 0: M[0, 2] += abs(translate[0])\n\t        if translate[1] < 0: M[1, 2] += abs(translate[1])\n\t        flags = get_interpolation()\n\t        return cv2.warpAffine(img, M, (dst_w, dst_h), flags=flags, borderMode=cv2.BORDER_REPLICATE)\n\tclass CVRandomPerspective(object):\n", "    def __init__(self, distortion=0.5):\n\t        self.distortion = distortion\n\t    def get_params(self, width, height, distortion):\n\t        offset_h = sample_asym(distortion * height / 2, size=4).astype(dtype=np.int)\n\t        offset_w = sample_asym(distortion * width / 2, size=4).astype(dtype=np.int)\n\t        topleft = (offset_w[0], offset_h[0])\n\t        topright = (width - 1 - offset_w[1], offset_h[1])\n\t        botright = (width - 1 - offset_w[2], height - 1 - offset_h[2])\n\t        botleft = (offset_w[3], height - 1 - offset_h[3])\n\t        startpoints = [(0, 0), (width - 1, 0), (width - 1, height - 1), (0, height - 1)]\n", "        endpoints = [topleft, topright, botright, botleft]\n\t        return np.array(startpoints, dtype=np.float32), np.array(endpoints, dtype=np.float32)\n\t    def __call__(self, img):\n\t        height, width = img.shape[:2]\n\t        startpoints, endpoints = self.get_params(width, height, self.distortion)\n\t        M = cv2.getPerspectiveTransform(startpoints, endpoints)\n\t        # TODO: more robust way to crop image\n\t        rect = cv2.minAreaRect(endpoints)\n\t        bbox = cv2.boxPoints(rect).astype(dtype=np.int)\n\t        max_x, max_y = bbox[:, 0].max(), bbox[:, 1].max()\n", "        min_x, min_y = bbox[:, 0].min(), bbox[:, 1].min()\n\t        min_x, min_y = max(min_x, 0), max(min_y, 0)\n\t        flags = get_interpolation()\n\t        img = cv2.warpPerspective(img, M, (max_x, max_y), flags=flags, borderMode=cv2.BORDER_REPLICATE)\n\t        img = img[min_y:, min_x:]\n\t        return img\n\tclass CVRescale(object):\n\t    def __init__(self, factor=4, base_size=(128, 512)):\n\t        \"\"\" Define image scales using gaussian pyramid and rescale image to target scale.\n\t        Args:\n", "            factor: the decayed factor from base size, factor=4 keeps target scale by default.\n\t            base_size: base size the build the bottom layer of pyramid\n\t        \"\"\"\n\t        if isinstance(factor, numbers.Number):\n\t            self.factor = round(sample_uniform(0, factor))\n\t        elif isinstance(factor, (tuple, list)) and len(factor) == 2:\n\t            self.factor = round(sample_uniform(factor[0], factor[1]))\n\t        else:\n\t            raise Exception('factor must be number or list with length 2')\n\t        # assert factor is valid\n", "        self.base_h, self.base_w = base_size[:2]\n\t    def __call__(self, img):\n\t        if self.factor == 0: return img\n\t        src_h, src_w = img.shape[:2]\n\t        cur_w, cur_h = self.base_w, self.base_h\n\t        scale_img = cv2.resize(img, (cur_w, cur_h), interpolation=get_interpolation())\n\t        for _ in range(self.factor):\n\t            scale_img = cv2.pyrDown(scale_img)\n\t        scale_img = cv2.resize(scale_img, (src_w, src_h), interpolation=get_interpolation())\n\t        return scale_img\n", "class CVGaussianNoise(object):\n\t    def __init__(self, mean=0, var=20):\n\t        self.mean = mean\n\t        if isinstance(var, numbers.Number):\n\t            self.var = max(int(sample_asym(var)), 1)\n\t        elif isinstance(var, (tuple, list)) and len(var) == 2:\n\t            self.var = int(sample_uniform(var[0], var[1]))\n\t        else:\n\t            raise Exception('degree must be number or list with length 2')\n\t    def __call__(self, img):\n", "        noise = np.random.normal(self.mean, self.var ** 0.5, img.shape)\n\t        img = np.clip(img + noise, 0, 255).astype(np.uint8)\n\t        return img\n\tclass CVMotionBlur(object):\n\t    def __init__(self, degrees=12, angle=90):\n\t        if isinstance(degrees, numbers.Number):\n\t            self.degree = max(int(sample_asym(degrees)), 1)\n\t        elif isinstance(degrees, (tuple, list)) and len(degrees) == 2:\n\t            self.degree = int(sample_uniform(degrees[0], degrees[1]))\n\t        else:\n", "            raise Exception('degree must be number or list with length 2')\n\t        self.angle = sample_uniform(-angle, angle)\n\t    def __call__(self, img):\n\t        M = cv2.getRotationMatrix2D((self.degree // 2, self.degree // 2), self.angle, 1)\n\t        motion_blur_kernel = np.zeros((self.degree, self.degree))\n\t        motion_blur_kernel[self.degree // 2, :] = 1\n\t        motion_blur_kernel = cv2.warpAffine(motion_blur_kernel, M, (self.degree, self.degree))\n\t        motion_blur_kernel = motion_blur_kernel / self.degree\n\t        img = cv2.filter2D(img, -1, motion_blur_kernel)\n\t        img = np.clip(img, 0, 255).astype(np.uint8)\n", "        return img\n\tclass CVGeometry(object):\n\t    def __init__(self, degrees=15, translate=(0.3, 0.3), scale=(0.5, 2.),\n\t                 shear=(45, 15), distortion=0.5, p=0.5):\n\t        self.p = p\n\t        type_p = random.random()\n\t        if type_p < 0.33:\n\t            self.transforms = CVRandomRotation(degrees=degrees)\n\t        elif type_p < 0.66:\n\t            self.transforms = CVRandomAffine(degrees=degrees, translate=translate, scale=scale, shear=shear)\n", "        else:\n\t            self.transforms = CVRandomPerspective(distortion=distortion)\n\t    def __call__(self, img):\n\t        if random.random() < self.p:\n\t            img = np.array(img)\n\t            return Image.fromarray(self.transforms(img))\n\t        else:\n\t            return img\n\tclass CVDeterioration(object):\n\t    def __init__(self, var, degrees, factor, p=0.5):\n", "        self.p = p\n\t        transforms = []\n\t        if var is not None:\n\t            transforms.append(CVGaussianNoise(var=var))\n\t        if degrees is not None:\n\t            transforms.append(CVMotionBlur(degrees=degrees))\n\t        if factor is not None:\n\t            transforms.append(CVRescale(factor=factor))\n\t        random.shuffle(transforms)\n\t        transforms = Compose(transforms)\n", "        self.transforms = transforms\n\t    def __call__(self, img):\n\t        if random.random() < self.p:\n\t            img = np.array(img)\n\t            return Image.fromarray(self.transforms(img))\n\t        else:\n\t            return img\n\tclass CVColorJitter(object):\n\t    def __init__(self, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1, p=0.5):\n\t        self.p = p\n", "        self.transforms = transforms.ColorJitter(brightness=brightness, contrast=contrast,\n\t                                                 saturation=saturation, hue=hue)\n\t    def __call__(self, img):\n\t        if random.random() < self.p:\n\t            return self.transforms(img)\n\t        else:\n\t            return img\n\tclass ImageToArray(object):\n\t    def __call__(self, img):\n\t        return np.array(img)\n", "class ImageToPIL(object):\n\t    def __call__(self, img):\n\t        return Image.fromarray(img)\n"]}
{"filename": "data/dataset.py", "chunked_list": ["import glob\n\timport io\n\timport logging\n\timport unicodedata\n\tfrom pathlib import Path, PurePath\n\tfrom typing import Callable, Optional, Union, Tuple, List\n\timport random\n\timport numpy as np\n\timport re\n\timport torch\n", "import lmdb\n\tfrom PIL import Image\n\timport cv2\n\tfrom torch.utils.data import Dataset, ConcatDataset\n\tfrom torchvision import transforms\n\tfrom data.utils import CharsetAdapter\n\tfrom data.transforms import ImageToArray, ImageToPIL, CVColorJitter, CVDeterioration, CVGeometry\n\tlog = logging.getLogger(__name__)\n\tdef build_tree_dataset(root: Union[PurePath, str], *args, **kwargs):\n\t    try:\n", "        kwargs.pop('root')  # prevent 'root' from being passed via kwargs\n\t    except KeyError:\n\t        pass\n\t    root = Path(root).absolute()\n\t    log.info(f'dataset root:\\t{root}')\n\t    consistency_regularization = kwargs.get('consistency_regularization', None)\n\t    exclude_folder = kwargs.get('exclude_folder', [])\n\t    try :\n\t        kwargs.pop('exclude_folder')\n\t    except KeyError : # prevent 'exclude_folder' passed to Dataset\n", "        pass \n\t    datasets = []\n\t    for mdb in glob.glob(str(root / '**/data.mdb'), recursive=True):\n\t        mdb = Path(mdb)\n\t        ds_name = str(mdb.parent.relative_to(root))\n\t        ds_root = str(mdb.parent.absolute())\n\t        if str(mdb.parts[-2]) in exclude_folder : # child folder name in exclude_folder e.g) Vertical\n\t            continue\n\t        if consistency_regularization :\n\t            dataset = ConsistencyRegulariationLmdbDataset(ds_root, *args, **kwargs)\n", "        else : \n\t            dataset = LmdbDataset(ds_root, *args, **kwargs)\n\t        log.info(f'\\tlmdb:\\t{ds_name}\\tnum samples: {len(dataset)}')\n\t        print(f'\\tlmdb:\\t{ds_name}\\tnum samples: {len(dataset)}')\n\t        datasets.append(dataset)\n\t    return ConcatDataset(datasets)\n\tclass LmdbDataset(Dataset):\n\t    \"\"\"Dataset interface to an LMDB database.\n\t    It supports both labelled and unlabelled datasets. For unlabelled datasets, the image index itself is returned\n\t    as the label. Unicode characters are normalized by default. Case-sensitivity is inferred from the charset.\n", "    Labels are transformed according to the charset.\n\t    \"\"\"\n\t    def __init__(self, root: str, charset: str, max_label_len: int, min_image_dim: int = 0,\n\t                 remove_whitespace: bool = True, normalize_unicode: bool = True,\n\t                 unlabelled: bool = False, transform: Optional[Callable] = None,\n\t                 limit_size: bool = False, size_of_limit: Optional[int] = None, img_size: Optional[Tuple] = (32,128), \n\t                 consistency_regularization = False, is_training: bool = False, twinreader_folders: Optional[List] = []):\n\t        self._env = None\n\t        self.root = root\n\t        self.unlabelled = unlabelled\n", "        self.transform = transform\n\t        self.labels = []\n\t        self.filtered_index_list = []\n\t        self.index_list = []\n\t        self.img_size = img_size\n\t        self.limit_size = limit_size\n\t        self.size_of_limit = size_of_limit\n\t        self.min_image_dim = min_image_dim\n\t        self.normalize_unicode = normalize_unicode\n\t        self.remove_whitespace = remove_whitespace\n", "        self.max_label_len = max_label_len\n\t        self.is_training = is_training\n\t        self.charset_adapter = CharsetAdapter(charset)\n\t        if self.limit_size and self.root.split('/')[-1] in twinreader_folders:\n\t            self.limit_size = False\n\t        self.consistency_regularization = consistency_regularization\n\t        if not self.is_training:\n\t            self.num_samples = self._preprocess_labels(charset, remove_whitespace, normalize_unicode,\n\t                                                   max_label_len, min_image_dim)\n\t        else:\n", "            self.num_samples = self.train_num_samples()\n\t    def train_num_samples(self):\n\t        with self._create_env() as env, env.begin() as txn:\n\t            num_samples = int(txn.get('num-samples'.encode()))\n\t            if self.unlabelled:\n\t                return num_samples\n\t            self.index_list = range(num_samples)\n\t            if self.limit_size :\n\t                self.index_list = np.random.permutation(self.index_list)[:self.size_of_limit]\n\t            return len(self.index_list)\n", "    def __del__(self):\n\t        if self._env is not None:\n\t            self._env.close()\n\t            self._env = None\n\t    def _create_env(self):\n\t        return lmdb.open(self.root, max_readers=1, readonly=True, create=False,\n\t                         readahead=False, meminit=False, lock=False)\n\t    @property\n\t    def env(self):\n\t        if self._env is None:\n", "            self._env = self._create_env()\n\t        return self._env\n\t    def _preprocess_labels(self, charset, remove_whitespace, normalize_unicode, max_label_len, min_image_dim):\n\t        charset_adapter = CharsetAdapter(charset)\n\t        with self._create_env() as env, env.begin() as txn:\n\t            num_samples = int(txn.get('num-samples'.encode()))\n\t            if self.unlabelled:\n\t                return num_samples\n\t            index_list = range(num_samples)\n\t            if self.limit_size :\n", "                index_list = np.random.permutation(index_list)[:self.limit_size]\n\t            for index in index_list : #range(num_samples):\n\t                index += 1  # lmdb starts with 1\n\t                label_key = f'label-{index:09d}'.encode()\n\t                label = txn.get(label_key).decode()\n\t                #\n\t                label = edit_label(label, charset_adapter.charset) # edit self.charset in CharsetAdapter\n\t                if len(label) > max_label_len:\n\t                    continue\n\t                label = charset_adapter(label)\n", "                # We filter out samples which don't contain any supported characters\n\t                if not label:\n\t                    continue\n\t                # Filter images that are too small.\n\t                if self.min_image_dim > 0:\n\t                    img_key = f'image-{index:09d}'.encode()\n\t                    buf = io.BytesIO(txn.get(img_key))\n\t                    w, h = Image.open(buf).size\n\t                    if w < self.min_image_dim or h < self.min_image_dim:\n\t                        continue\n", "                self.labels.append(label)\n\t                self.filtered_index_list.append(index)\n\t        return len(self.labels)\n\t    def __len__(self):\n\t        return self.num_samples\n\t    def next_sample(self):\n\t        next_index = random.randint(0, len(self) - 1)\n\t        if self.limit_size:\n\t            next_index = self.index_list[next_index]\n\t        return self.get(next_index)\n", "    def get(self, index):\n\t        index += 1  # lmdb starts with 1\n\t        with self._create_env() as env, env.begin() as txn:\n\t            label_key = f'label-{index:09d}'.encode()\n\t            label = txn.get(label_key).decode()\n\t            #\n\t            label = edit_label(label, self.charset_adapter.charset) # edit self.charset in CharsetAdapter\n\t            if len(label) > self.max_label_len:\n\t                return self.next_sample()\n\t            label = self.charset_adapter(label)\n", "            # We filter out samples which don't contain any supported characters\n\t            if not label:\n\t                return self.next_sample()\n\t            # Filter images that are too small.\n\t            img_key = f'image-{index:09d}'.encode()\n\t            buf = io.BytesIO(txn.get(img_key))\n\t            img = Image.open(buf).convert('RGB')\n\t            if self.min_image_dim > 0:\n\t                w, h = img.size\n\t    #             w, h = Image.open(buf).size\n", "                if w < self.min_image_dim or h < self.min_image_dim:\n\t                    return self.next_sample()\n\t            img=np.array(img)\n\t            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\t            img = np.dstack([img,img,img])\n\t            img=Image.fromarray(img)        \n\t            w,h=img.size\n\t            # vertical data handling\n\t            if h/w>2.5:\n\t                img=img.rotate(90, expand=True)\n", "            if self.transform is not None:\n\t                img = self.transform(img)\n\t        return img, label\n\t    def __getitem__(self, index):\n\t        if self.is_training:\n\t            if self.limit_size:\n\t                index = self.index_list[index]\n\t            return self.get(index)\n\t        if self.unlabelled:\n\t            label = index\n", "        else:\n\t            label = self.labels[index]\n\t            index = self.filtered_index_list[index]\n\t        img_key = f'image-{index:09d}'.encode()\n\t        with self.env.begin() as txn:\n\t            imgbuf = txn.get(img_key)\n\t        buf = io.BytesIO(imgbuf)\n\t        img = Image.open(buf).convert('RGB')\n\t        img=np.array(img)\n\t        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n", "        img = np.dstack([img,img,img])\n\t        img=Image.fromarray(img)        \n\t        w,h=img.size\n\t        # vertical data handling\n\t        if h/w>2.5:\n\t            img=img.rotate(90, expand=True)\n\t        if self.transform is not None:\n\t            img = self.transform(img)\n\t        return img, label\n\tdef edit_label(label, char_list) :\n", "################################## New editions\n\t#     match=re.findall(r'\\[UNK[0-9]*\\]',label)\n\t#     if match:\n\t#         for mat in match: label=label.replace(mat,'[UNK]')\n\t    out_of_char = r'[^{}]'.format(re.escape(''.join(char_list)))\n\t    label = label.replace('###','[UNK]')\n\t    label = re.sub(out_of_char, \"[UNK]\", label)\n\t    return label\n"]}
{"filename": "data/__init__.py", "chunked_list": []}
{"filename": "data/aa_overrides.py", "chunked_list": ["\"\"\"Extends default ops to accept optional parameters.\"\"\"\n\tfrom functools import partial\n\tfrom timm.data.auto_augment import _LEVEL_DENOM, _randomly_negate, LEVEL_TO_ARG, NAME_TO_OP, rotate\n\tdef rotate_expand(img, degrees, **kwargs):\n\t    \"\"\"Rotate operation with expand=True to avoid cutting off the characters\"\"\"\n\t    kwargs['expand'] = True\n\t    return rotate(img, degrees, **kwargs)\n\tdef _level_to_arg(level, hparams, key, default):\n\t    magnitude = hparams.get(key, default)\n\t    level = (level / _LEVEL_DENOM) * magnitude\n", "    level = _randomly_negate(level)\n\t    return level,\n\tdef apply():\n\t    # Overrides\n\t    NAME_TO_OP.update({\n\t        'Rotate': rotate_expand\n\t    })\n\t    LEVEL_TO_ARG.update({\n\t        'Rotate': partial(_level_to_arg, key='rotate_deg', default=30.),\n\t        'ShearX': partial(_level_to_arg, key='shear_x_pct', default=0.3),\n", "        'ShearY': partial(_level_to_arg, key='shear_y_pct', default=0.3),\n\t        'TranslateXRel': partial(_level_to_arg, key='translate_x_pct', default=0.45),\n\t        'TranslateYRel': partial(_level_to_arg, key='translate_y_pct', default=0.45),\n\t    })\n"]}
{"filename": "data/utils.py", "chunked_list": ["import re\n\tfrom abc import ABC, abstractmethod\n\tfrom itertools import groupby\n\tfrom typing import List, Optional, Tuple\n\timport torch\n\tfrom torch import Tensor\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tclass CharsetAdapter:\n\t    \"\"\"Transforms labels according to the target charset.\"\"\"\n\t    def __init__(self, target_charset) -> None:\n", "        super().__init__()\n\t        self.charset = target_charset ###\n\t        self.lowercase_only = target_charset == target_charset.lower()\n\t        self.uppercase_only = target_charset == target_charset.upper()\n\t#         self.unsupported = f'[^{re.escape(target_charset)}]'\n\t    def __call__(self, label):\n\t        if self.lowercase_only:\n\t            label = label.lower()\n\t        elif self.uppercase_only:\n\t            label = label.upper()\n", "        # Remove unsupported characters\n\t#         label = re.sub(self.unsupported, '', label)\n\t        return label\n\tclass BaseTokenizer(ABC):\n\t    def __init__(self, charset: str, specials_first: tuple = (), specials_last: tuple = ()) -> None:\n\t        self._itos = specials_first + tuple(charset+'ल') + specials_last\n\t        self._stoi = {s: i for i, s in enumerate(self._itos)}\n\t    def __len__(self):\n\t        return len(self._itos)\n\t    def _tok2ids(self, tokens: str) -> List[int]:\n", "        return [self._stoi[s] for s in tokens]\n\t    def _ids2tok(self, token_ids: List[int], join: bool = True) -> str:\n\t        tokens = [self._itos[i] for i in token_ids]\n\t        return ''.join(tokens) if join else tokens\n\t    @abstractmethod\n\t    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n\t        \"\"\"Encode a batch of labels to a representation suitable for the model.\n\t        Args:\n\t            labels: List of labels. Each can be of arbitrary length.\n\t            device: Create tensor on this device.\n", "        Returns:\n\t            Batched tensor representation padded to the max label length. Shape: N, L\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n\t        \"\"\"Internal method which performs the necessary filtering prior to decoding.\"\"\"\n\t        raise NotImplementedError\n\t    def decode(self, token_dists: Tensor, raw: bool = False) -> Tuple[List[str], List[Tensor]]:\n\t        \"\"\"Decode a batch of token distributions.\n", "        Args:\n\t            token_dists: softmax probabilities over the token distribution. Shape: N, L, C\n\t            raw: return unprocessed labels (will return list of list of strings)\n\t        Returns:\n\t            list of string labels (arbitrary length) and\n\t            their corresponding sequence probabilities as a list of Tensors\n\t        \"\"\"\n\t        batch_tokens = []\n\t        batch_probs = []\n\t        for dist in token_dists:\n", "            probs, ids = dist.max(-1)  # greedy selection\n\t            if not raw:\n\t                probs, ids = self._filter(probs, ids)\n\t            tokens = self._ids2tok(ids, not raw)\n\t            batch_tokens.append(tokens)\n\t            batch_probs.append(probs)\n\t        return batch_tokens, batch_probs\n\tclass Tokenizer(BaseTokenizer):\n\t    BOS = '[B]'\n\t    EOS = '[E]'\n", "    PAD = '[P]'\n\t    def __init__(self, charset: str) -> None:\n\t        specials_first = (self.EOS,)\n\t        specials_last = (self.BOS, self.PAD)\n\t        super().__init__(charset, specials_first, specials_last)\n\t        self.eos_id, self.bos_id, self.pad_id = [self._stoi[s] for s in specials_first + specials_last]\n\t    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n\t        batch = [torch.as_tensor([self.bos_id] + self._tok2ids(y) + [self.eos_id], dtype=torch.long, device=device)\n\t                 for y in labels]\n\t        return pad_sequence(batch, batch_first=True, padding_value=self.pad_id)\n", "    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n\t        ids = ids.tolist()\n\t        try:\n\t            eos_idx = ids.index(self.eos_id)\n\t        except ValueError:\n\t            eos_idx = len(ids)  # Nothing to truncate.\n\t        # Truncate after EOS\n\t        ids = ids[:eos_idx]\n\t        probs = probs[:eos_idx + 1]  # but include prob. for EOS (if it exists)\n\t        return probs, ids\n", "class CTCTokenizer(BaseTokenizer):\n\t    BLANK = '[B]'\n\t    def __init__(self, charset: str) -> None:\n\t        # BLANK uses index == 0 by default\n\t        super().__init__(charset, specials_first=(self.BLANK,))\n\t        self.blank_id = self._stoi[self.BLANK]\n\t    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n\t        # We use a padded representation since we don't want to use CUDNN's CTC implementation\n\t        batch = [torch.as_tensor(self._tok2ids(y), dtype=torch.long, device=device) for y in labels]\n\t        return pad_sequence(batch, batch_first=True, padding_value=self.blank_id)\n", "    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n\t        # Best path decoding:\n\t        ids = list(zip(*groupby(ids.tolist())))[0]  # Remove duplicate tokens\n\t        ids = [x for x in ids if x != self.blank_id]  # Remove BLANKs\n\t        # `probs` is just pass-through since all positions are considered part of the path\n\t        return probs, ids\n"]}
{"filename": "data/module.py", "chunked_list": ["from pathlib import PurePath\n\tfrom typing import Optional, Callable, Sequence, Tuple, List\n\t# import pytorch_lightning as pl\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms as T\n\tfrom .dataset import build_tree_dataset, LmdbDataset\n\tclass SceneTextDataModule():\n\t    def __init__(self, root_dir: str, train_dir: str, val_dir : str, test_dir : str, \n\t                img_size: Sequence[int],\n\t                max_label_length: int, \n", "                charset_train: str, \n\t                charset_test: str, \n\t                batch_size: int, \n\t                num_workers: int, \n\t                augment: bool, \n\t                remove_whitespace: bool = True, \n\t                normalize_unicode: bool = True,\n\t                min_image_dim: int = 0, \n\t                rotation: int = 0, \n\t                collate_fn: Optional[Callable] = None, \n", "                limit_size : bool = False , \n\t                size_of_limit : int = None,\n\t                consistency_regularization: Optional[bool] = False, \n\t                exclude_folder: Optional[List] = [],\n\t                data_weights: Optional[List] = []):\n\t        super().__init__()\n\t        self.root_dir = root_dir\n\t        self.train_dir = train_dir\n\t        self.val_dir = val_dir\n\t        self.test_dir = test_dir\n", "        self.img_size = tuple(img_size)\n\t        self.max_label_length = max_label_length\n\t        self.charset_train = charset_train\n\t        self.charset_test = charset_test\n\t        self.batch_size = batch_size\n\t        self.num_workers = num_workers\n\t        self.augment = augment\n\t        self.remove_whitespace = remove_whitespace\n\t        self.normalize_unicode = normalize_unicode\n\t        self.min_image_dim = min_image_dim\n", "        self.rotation = rotation\n\t        self.collate_fn = collate_fn\n\t        self.limit_size = limit_size\n\t        self.size_of_limit = size_of_limit\n\t        self.consistency_regularization = consistency_regularization\n\t        self.exclude_folder = exclude_folder\n\t        self.data_weights = data_weights\n\t        self._train_dataset = None\n\t        self._val_dataset = None\n\t    @staticmethod\n", "    def get_transform(img_size: Tuple[int], augment: bool = False, rotation: int = 0, consistency_regularization: Optional[bool] = False):\n\t        if consistency_regularization :\n\t            from .augmentation_pipelines import get_augmentation_pipeline\n\t            augmentation_severity = 2 # 2 suits to document image\n\t            pipeline = get_augmentation_pipeline(augmentation_severity)\n\t            # pipeline.append(iaa.Resize(img_size))\n\t            return pipeline.augment_image\n\t        else :\n\t            transforms = []\n\t            if augment:\n", "                from .augment import rand_augment_transform\n\t                transforms.append(rand_augment_transform())\n\t            if rotation:\n\t                transforms.append(lambda img: img.rotate(rotation, expand=True))\n\t            transforms.extend([\n\t                T.Resize(img_size, T.InterpolationMode.BICUBIC),\n\t                T.ToTensor(),\n\t                T.Normalize(0.5, 0.5)\n\t            ])\n\t            return T.Compose(transforms)        \n", "    @property\n\t    def train_dataset(self):\n\t        if self._train_dataset is None:\n\t            transform = self.get_transform(self.img_size, self.augment, consistency_regularization = self.consistency_regularization)\n\t            root = PurePath(self.train_dir)\n\t            self._train_dataset = build_tree_dataset(root, self.charset_train, self.max_label_length,\n\t                                                     self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n\t                                                     transform=transform, limit_size = self.limit_size, size_of_limit = self.size_of_limit,\n\t                                                     consistency_regularization = self.consistency_regularization,\n\t                                                     img_size = self.img_size, twinreader_folders = self.exclude_folder, is_training = True\n", "                                                     )\n\t        return self._train_dataset\n\t    @property\n\t    def val_dataset(self):\n\t        if self._val_dataset is None:\n\t            transform = self.get_transform(self.img_size)\n\t            root = PurePath(self.val_dir)\n\t            self._val_dataset = build_tree_dataset(root, self.charset_test, self.max_label_length, \n\t                                                   self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n\t                                                   img_size = self.img_size,\n", "                                                   transform=transform, limit_size = False, is_training = False)\n\t        return self._val_dataset\n\t    def train_dataloader(self):\n\t        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n\t                          num_workers=self.num_workers, persistent_workers=self.num_workers > 0,\n\t                          pin_memory=True, collate_fn=self.collate_fn)\n\t    def val_dataloader(self):\n\t        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n\t                          num_workers=self.num_workers, persistent_workers=self.num_workers > 0,\n\t                          pin_memory=True, collate_fn=self.collate_fn)\n", "    def test_dataloaders(self, subset):\n\t        transform = self.get_transform(self.img_size, rotation=self.rotation)\n\t        root = PurePath(self.test_dir)\n\t        datasets = {s: LmdbDataset(str(root / s), self.charset_test, self.max_label_length,\n\t                                   self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n\t                                   transform=transform, is_training=False) for s in subset}\n\t        return {k: DataLoader(v, batch_size=self.batch_size, num_workers=self.num_workers,\n\t                              pin_memory=True, collate_fn=self.collate_fn)\n\t                for k, v in datasets.items()}\n"]}
{"filename": "data/augment.py", "chunked_list": ["from functools import partial\n\timport imgaug.augmenters as iaa\n\timport numpy as np\n\tfrom PIL import ImageFilter, Image\n\tfrom timm.data import auto_augment\n\tfrom . import aa_overrides\n\taa_overrides.apply()\n\t_OP_CACHE = {}\n\tdef _get_op(key, factory):\n\t    try:\n", "        op = _OP_CACHE[key]\n\t    except KeyError:\n\t        op = factory()\n\t        _OP_CACHE[key] = op\n\t    return op\n\tdef _get_param(level, img, max_dim_factor, min_level=1):\n\t    max_level = max(min_level, max_dim_factor * max(img.size))\n\t    return round(min(level, max_level))\n\tdef gaussian_blur(img, radius, **__):\n\t    radius = _get_param(radius, img, 0.02)\n", "    key = 'gaussian_blur_' + str(radius)\n\t    op = _get_op(key, lambda: ImageFilter.GaussianBlur(radius))\n\t    return img.filter(op)\n\tdef motion_blur(img, k, **__):\n\t    k = _get_param(k, img, 0.08, 3) | 1  # bin to odd values\n\t    key = 'motion_blur_' + str(k)\n\t    op = _get_op(key, lambda: iaa.MotionBlur(k))\n\t    return Image.fromarray(op(image=np.asarray(img)))\n\tdef gaussian_noise(img, scale, **_):\n\t    scale = _get_param(scale, img, 0.25) | 1  # bin to odd values\n", "    key = 'gaussian_noise_' + str(scale)\n\t    op = _get_op(key, lambda: iaa.AdditiveGaussianNoise(scale=scale))\n\t    return Image.fromarray(op(image=np.asarray(img)))\n\tdef poisson_noise(img, lam, **_):\n\t    lam = _get_param(lam, img, 0.2) | 1  # bin to odd values\n\t    key = 'poisson_noise_' + str(lam)\n\t    op = _get_op(key, lambda: iaa.AdditivePoissonNoise(lam))\n\t    return Image.fromarray(op(image=np.asarray(img)))\n\tdef _level_to_arg(level, _hparams, max):\n\t    level = max * level / auto_augment._LEVEL_DENOM\n", "    return level,\n\t_RAND_TRANSFORMS = auto_augment._RAND_INCREASING_TRANSFORMS.copy()\n\t_RAND_TRANSFORMS.remove('SharpnessIncreasing')  # remove, interferes with *blur ops\n\t_RAND_TRANSFORMS.extend([\n\t    'GaussianBlur',\n\t    # 'MotionBlur',\n\t    # 'GaussianNoise',\n\t    'PoissonNoise'\n\t])\n\tauto_augment.LEVEL_TO_ARG.update({\n", "    'GaussianBlur': partial(_level_to_arg, max=4),\n\t    'MotionBlur': partial(_level_to_arg, max=20),\n\t    'GaussianNoise': partial(_level_to_arg, max=0.1 * 255),\n\t    'PoissonNoise': partial(_level_to_arg, max=40)\n\t})\n\tauto_augment.NAME_TO_OP.update({\n\t    'GaussianBlur': gaussian_blur,\n\t    'MotionBlur': motion_blur,\n\t    'GaussianNoise': gaussian_noise,\n\t    'PoissonNoise': poisson_noise\n", "})\n\tdef rand_augment_transform(magnitude=5, num_layers=3):\n\t    # These are tuned for magnitude=5, which means that effective magnitudes are half of these values.\n\t    hparams = {\n\t        'rotate_deg': 30,\n\t        'shear_x_pct': 0.9,\n\t        'shear_y_pct': 0.2,\n\t        'translate_x_pct': 0.10,\n\t        'translate_y_pct': 0.30\n\t    }\n", "    ra_ops = auto_augment.rand_augment_ops(magnitude, hparams, transforms=_RAND_TRANSFORMS)\n\t    # Supply weights to disable replacement in random selection (i.e. avoid applying the same op twice)\n\t    choice_weights = [1. / len(ra_ops) for _ in range(len(ra_ops))]\n\t    return auto_augment.RandAugment(ra_ops, num_layers, choice_weights)\n"]}
{"filename": "model/parseq_test.py", "chunked_list": ["# Scene Text Recognition Model Hub\n\t# Copyright 2022 Darwin Bautista\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     https://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\tfrom functools import partial\n\tfrom itertools import permutations\n\tfrom typing import Sequence, Any, Optional, Tuple, List\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import Tensor\n\tfrom timm.models.helpers import named_apply\n\tfrom dataclasses import dataclass\n\tfrom nltk import edit_distance\n\tfrom .utils import init_weights\n\tfrom .modules import DecoderLayer, Decoder, Encoder, TokenEmbedding\n\tfrom .tokenizer_utils import Tokenizer, BaseTokenizer\n\t@dataclass\n", "class BatchResult:\n\t    num_samples: int\n\t    correct: int\n\t    ned: float\n\t    confidence: float\n\t    label_length: int\n\t    loss: Tensor\n\t    loss_numel: int\n\tclass PARSeq(nn.Module):\n\t    def __init__(self, charset_train: str, charset_test: str, max_label_length: int,\n", "                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float,\n\t                 img_size: Sequence[int], patch_size: Sequence[int], embed_dim: int,\n\t                 enc_num_heads: int, enc_mlp_ratio: int, enc_depth: int,\n\t                 dec_num_heads: int, dec_mlp_ratio: int, dec_depth: int,\n\t                 perm_num: int, perm_forward: bool, perm_mirrored: bool,\n\t                 decode_ar: bool, refine_iters: int, dropout: float, **kwargs: Any) -> None:\n\t        tokenizer = Tokenizer(charset_train)\n\t        super().__init__()\n\t        self.tokenizer = tokenizer\n\t        self.bos_id = self.tokenizer.bos_id\n", "        self.eos_id = self.tokenizer.eos_id\n\t        self.pad_id = self.tokenizer.pad_id\n\t        self.batch_size = batch_size\n\t        self.lr = lr\n\t        self.warmup_pct = warmup_pct\n\t        self.weight_decay = weight_decay\n\t        self._device = None\n\t        self.max_label_length = max_label_length\n\t        self.decode_ar = decode_ar\n\t        self.refine_iters = refine_iters\n", "        self.encoder = Encoder(img_size, patch_size, embed_dim=embed_dim, depth=enc_depth, num_heads=enc_num_heads,\n\t                               mlp_ratio=enc_mlp_ratio)\n\t        decoder_layer = DecoderLayer(embed_dim, dec_num_heads, embed_dim * dec_mlp_ratio, dropout)\n\t        self.decoder = Decoder(decoder_layer, num_layers=dec_depth, norm=nn.LayerNorm(embed_dim))\n\t        # Perm/attn mask stuff\n\t        self.rng = np.random.default_rng()\n\t        self.max_gen_perms = perm_num // 2 if perm_mirrored else perm_num\n\t        self.perm_forward = perm_forward\n\t        self.perm_mirrored = perm_mirrored\n\t        # We don't predict <bos> nor <pad>\n", "        self.head = nn.Linear(embed_dim, len(self.tokenizer) - 2)\n\t        self.text_embed = TokenEmbedding(len(self.tokenizer), embed_dim)\n\t        # +1 for <eos>\n\t        self.pos_queries = nn.Parameter(torch.Tensor(1, max_label_length + 1, embed_dim))\n\t        self.dropout = nn.Dropout(p=dropout)\n\t        # Encoder has its own init.\n\t        named_apply(partial(init_weights, exclude=['encoder']), self)\n\t        nn.init.trunc_normal_(self.pos_queries, std=.02)\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n", "        param_names = {'text_embed.embedding.weight', 'pos_queries'}\n\t        enc_param_names = {'encoder.' + n for n in self.encoder.no_weight_decay()}\n\t        return param_names.union(enc_param_names)\n\t    def encode(self, img: torch.Tensor):\n\t        return self.encoder(img)\n\t    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[Tensor] = None,\n\t               tgt_padding_mask: Optional[Tensor] = None, tgt_query: Optional[Tensor] = None,\n\t               tgt_query_mask: Optional[Tensor] = None):\n\t        N, L = tgt.shape\n\t        # <bos> stands for the null context. We only supply position information for characters after <bos>.\n", "        null_ctx = self.text_embed(tgt[:, :1])\n\t        tgt_emb = self.pos_queries[:, :L - 1] + self.text_embed(tgt[:, 1:])\n\t        tgt_emb = self.dropout(torch.cat([null_ctx, tgt_emb], dim=1))\n\t        if tgt_query is None:\n\t            tgt_query = self.pos_queries[:, :L].expand(N, -1, -1)\n\t        tgt_query = self.dropout(tgt_query)\n\t        return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask, tgt_mask, tgt_padding_mask)\n\t    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n\t        targets = self.tokenizer.encode(labels, self._device)\n\t        targets = targets[:, 1:]  # Discard <bos>\n", "        max_len = targets.shape[1] - 1  # exclude <eos> from count\n\t        logits = self.forward(images, max_len)\n\t        loss = F.cross_entropy(logits.flatten(end_dim=1), targets.flatten(), ignore_index=self.pad_id)\n\t        loss_numel = (targets != self.pad_id).sum()\n\t        return logits, loss, loss_numel\n\t    def forward(self, images: Tensor, max_length: Optional[int] = None) -> Tensor:\n\t        testing = max_length is None\n\t        max_length = self.max_label_length if max_length is None else min(max_length, self.max_label_length)\n\t        bs = images.shape[0]\n\t        # +1 for <eos> at end of sequence.\n", "        num_steps = max_length + 1\n\t        memory = self.encode(images)\n\t        # Query positions up to `num_steps`\n\t        pos_queries = self.pos_queries[:, :num_steps].expand(bs, -1, -1)\n\t        # Special case for the forward permutation. Faster than using `generate_attn_masks()`\n\t        tgt_mask = query_mask = torch.triu(torch.full((num_steps, num_steps), float('-inf'), device=self._device), 1)\n\t        if self.decode_ar:\n\t            tgt_in = torch.full((bs, num_steps), self.pad_id, dtype=torch.long, device=self._device)\n\t            tgt_in[:, 0] = self.bos_id\n\t            logits = []\n", "            for i in range(num_steps):\n\t                j = i + 1  # next token index\n\t                # Efficient decoding:\n\t                # Input the context up to the ith token. We use only one query (at position = i) at a time.\n\t                # This works because of the lookahead masking effect of the canonical (forward) AR context.\n\t                # Past tokens have no access to future tokens, hence are fixed once computed.\n\t                tgt_out = self.decode(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n\t                                      tgt_query_mask=query_mask[i:j, :j])\n\t                # the next token probability is in the output's ith token position\n\t                p_i = self.head(tgt_out)\n", "                logits.append(p_i)\n\t                if j < num_steps:\n\t                    # greedy decode. add the next token index to the target input\n\t                    tgt_in[:, j] = p_i.squeeze().argmax(-1)\n\t#                     tgt_in[:, j] = p_i.argmax(-1).squeeze()\n\t                    # Efficient batch decoding: If all output words have at least one EOS token, end decoding.\n\t#                     if torch.onnx.is_in_onnx_export() and torch.where(torch.where(tgt_in == torch.tensor(self.eos_id), 1, 0).sum(dim=-1).sum() == bs, 1, 0):\n\t#                         break\n\t#                     el\n\t#                     if testing and (tgt_in == self.eos_id).any(dim=-1).all():\n", "#                         break\n\t            logits = torch.cat(logits, dim=1)\n\t        else:\n\t            # No prior context, so input is just <bos>. We query all positions.\n\t            tgt_in = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n\t            tgt_out = self.decode(tgt_in, memory, tgt_query=pos_queries)\n\t            logits = self.head(tgt_out)\n\t        if self.refine_iters:\n\t            # For iterative refinement, we always use a 'cloze' mask.\n\t            # We can derive it from the AR forward mask by unmasking the token context to the right.\n", "            query_mask[torch.triu(torch.ones(num_steps, num_steps, dtype=torch.bool, device=self._device), 2)] = 0\n\t            bos = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n\t            for i in range(self.refine_iters):\n\t                # Prior context is the previous output.\n\t                tgt_in = torch.cat([bos, logits[:, :-1].argmax(-1)], dim=1)\n\t                tgt_padding_mask = ((tgt_in == self.eos_id).int().cumsum(-1) > 0)  # mask tokens beyond the first EOS token.\n\t                tgt_out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask,\n\t                                      tgt_query=pos_queries, tgt_query_mask=query_mask[:, :tgt_in.shape[1]])\n\t                logits = self.head(tgt_out)\n\t        return logits\n", "    def gen_tgt_perms(self, tgt):\n\t        \"\"\"Generate shared permutations for the whole batch.\n\t           This works because the same attention mask can be used for the shorter sequences\n\t           because of the padding mask.\n\t        \"\"\"\n\t        # We don't permute the position of BOS, we permute EOS separately\n\t        max_num_chars = tgt.shape[1] - 2\n\t        # Special handling for 1-character sequences\n\t        if max_num_chars == 1:\n\t            return torch.arange(3, device=self._device).unsqueeze(0)\n", "        perms = [torch.arange(max_num_chars, device=self._device)] if self.perm_forward else []\n\t        # Additional permutations if needed\n\t        max_perms = math.factorial(max_num_chars)\n\t        if self.perm_mirrored:\n\t            max_perms //= 2\n\t        num_gen_perms = min(self.max_gen_perms, max_perms)\n\t        # For 4-char sequences and shorter, we generate all permutations and sample from the pool to avoid collisions\n\t        # Note that this code path might NEVER get executed since the labels in a mini-batch typically exceed 4 chars.\n\t        if max_num_chars < 5:\n\t            # Pool of permutations to sample from. We only need the first half (if complementary option is selected)\n", "            # Special handling for max_num_chars == 4 which correctly divides the pool into the flipped halves\n\t            if max_num_chars == 4 and self.perm_mirrored:\n\t                selector = [0, 3, 4, 6, 9, 10, 12, 16, 17, 18, 19, 21]\n\t            else:\n\t                selector = list(range(max_perms))\n\t            perm_pool = torch.as_tensor(list(permutations(range(max_num_chars), max_num_chars)), device=self._device)[selector]\n\t            # If the forward permutation is always selected, no need to add it to the pool for sampling\n\t            if self.perm_forward:\n\t                perm_pool = perm_pool[1:]\n\t            perms = torch.stack(perms)\n", "            if len(perm_pool):\n\t                i = self.rng.choice(len(perm_pool), size=num_gen_perms - len(perms), replace=False)\n\t                perms = torch.cat([perms, perm_pool[i]])\n\t        else:\n\t            perms.extend([torch.randperm(max_num_chars, device=self._device) for _ in range(num_gen_perms - len(perms))])\n\t            perms = torch.stack(perms)\n\t        if self.perm_mirrored:\n\t            # Add complementary pairs\n\t            comp = perms.flip(-1)\n\t            # Stack in such a way that the pairs are next to each other.\n", "            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n\t        # NOTE:\n\t        # The only meaningful way of permuting the EOS position is by moving it one character position at a time.\n\t        # However, since the number of permutations = T! and number of EOS positions = T + 1, the number of possible EOS\n\t        # positions will always be much less than the number of permutations (unless a low perm_num is set).\n\t        # Thus, it would be simpler to just train EOS using the full and null contexts rather than trying to evenly\n\t        # distribute it across the chosen number of permutations.\n\t        # Add position indices of BOS and EOS\n\t        bos_idx = perms.new_zeros((len(perms), 1))\n\t        eos_idx = perms.new_full((len(perms), 1), max_num_chars + 1)\n", "        perms = torch.cat([bos_idx, perms + 1, eos_idx], dim=1)\n\t        # Special handling for the reverse direction. This does two things:\n\t        # 1. Reverse context for the characters\n\t        # 2. Null context for [EOS] (required for learning to predict [EOS] in NAR mode)\n\t        if len(perms) > 1:\n\t            perms[1, 1:] = max_num_chars + 1 - torch.arange(max_num_chars + 1, device=self._device)\n\t        return perms\n\t    def generate_attn_masks(self, perm):\n\t        \"\"\"Generate attention masks given a sequence permutation (includes pos. for bos and eos tokens)\n\t        :param perm: the permutation sequence. i = 0 is always the BOS\n", "        :return: lookahead attention masks\n\t        \"\"\"\n\t        sz = perm.shape[0]\n\t        mask = torch.zeros((sz, sz), device=self._device)\n\t        for i in range(sz):\n\t            query_idx = perm[i]\n\t            masked_keys = perm[i + 1:]\n\t            mask[query_idx, masked_keys] = float('-inf')\n\t        content_mask = mask[:-1, :-1].clone()\n\t        mask[torch.eye(sz, dtype=torch.bool, device=self._device)] = float('-inf')  # mask \"self\"\n", "        query_mask = mask[1:, :-1]\n\t        return content_mask, query_mask\n\t    def training_step(self, images, labels): #-> STEP_OUTPUT:\n\t        tgt = self.tokenizer.encode(labels, self._device)\n\t        # Encode the source sequence (i.e. the image codes)\n\t        memory = self.encode(images)\n\t        # Prepare the target sequences (input and output)\n\t        tgt_perms = self.gen_tgt_perms(tgt)\n\t        tgt_in = tgt[:, :-1]\n\t        tgt_out = tgt[:, 1:]\n", "        # The [EOS] token is not depended upon by any other token in any permutation ordering\n\t        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n\t        loss = 0\n\t        loss_numel = 0\n\t        n = (tgt_out != self.pad_id).sum().item()\n\t        for i, perm in enumerate(tgt_perms):\n\t            tgt_mask, query_mask = self.generate_attn_masks(perm)\n\t            out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n\t            logits = self.head(out).flatten(end_dim=1)\n\t            loss += n * F.cross_entropy(logits, tgt_out.flatten(), ignore_index=self.pad_id)\n", "            loss_numel += n\n\t            # After the second iteration (i.e. done with canonical and reverse orderings),\n\t            # remove the [EOS] tokens for the succeeding perms\n\t            if i == 1:\n\t                tgt_out = torch.where(tgt_out == self.eos_id, self.pad_id, tgt_out)\n\t                n = (tgt_out != self.pad_id).sum().item()\n\t        loss /= loss_numel\n\t        return loss\n\t    def _eval_step(self, batch, validation: bool): #-> Optional[STEP_OUTPUT]:\n\t        images, labels = batch\n", "        correct = 0\n\t        total = 0\n\t        ned = 0\n\t        confidence = 0\n\t        label_length = 0\n\t        if validation:\n\t            logits, loss, loss_numel = self.forward_logits_loss(images, labels)\n\t        else:\n\t            # At test-time, we shouldn't specify a max_label_length because the test-time charset used\n\t            # might be different from the train-time charset. max_label_length in eval_logits_loss() is computed\n", "            # based on the transformed label, which could be wrong if the actual gt label contains characters existing\n\t            # in the train-time charset but not in the test-time charset. For example, \"aishahaleyes.blogspot.com\"\n\t            # is exactly 25 characters, but if processed by CharsetAdapter for the 36-char set, it becomes 23 characters\n\t            # long only, which sets max_label_length = 23. This will cause the model prediction to be truncated.\n\t            logits = self.forward(images)\n\t            loss = loss_numel = None  # Only used for validation; not needed at test-time.\n\t        probs = logits.softmax(-1)\n\t        preds, probs = self.tokenizer.decode(probs)\n\t        for pred, prob, gt in zip(preds, probs, labels):\n\t            confidence += prob.prod().item()\n", "#             pred = self.charset_adapter(pred)\n\t            # Follow ICDAR 2019 definition of N.E.D.\n\t            pred=pred.replace('ल','[UNK]')\n\t            ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n\t            if pred == gt:\n\t                correct += 1\n\t            total += 1\n\t            label_length += len(pred)\n\t        return dict(output=BatchResult(total, correct, ned, confidence, label_length, loss, loss_numel))\n\t    @staticmethod\n", "    def _aggregate_results(outputs) -> Tuple[float, float, float]:\n\t        if not outputs:\n\t            return 0., 0., 0.\n\t        total_loss = 0\n\t        total_loss_numel = 0\n\t        total_n_correct = 0\n\t        total_norm_ED = 0\n\t        total_size = 0\n\t        for result in outputs:\n\t            result = result['output']\n", "            total_loss += result.loss_numel * result.loss\n\t            total_loss_numel += result.loss_numel\n\t            total_n_correct += result.correct\n\t            total_norm_ED += result.ned\n\t            total_size += result.num_samples\n\t        acc = total_n_correct / total_size\n\t        ned = (1 - total_norm_ED / total_size)\n\t        loss = total_loss / total_loss_numel\n\t        return acc, ned, loss\n\t    def validation_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n", "        return self._eval_step(batch, True)\n\t    def test_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n\t        return self._eval_step(batch, False)\n\tclass PARSeqWithConsistencyRegularization(PARSeq):\n\t    def __init__(self, *args, cr_loss_weight, supervised_flag, teacher_one_hot, use_threshold, kl_div, ema, ema_decay, **kwargs: Any) -> None :\n\t        super().__init__(*args, **kwargs)\n\t        # consistency regularization\n\t        self.cr_loss_weight = cr_loss_weight\n\t        self.supervised_flag = supervised_flag\n\t        self.teacher_one_hot_labels = teacher_one_hot\n", "        self.use_threshold = use_threshold\n\t        self.kl_div = kl_div\n\t        self.ema = ema\n\t        self.ema_decay = ema_decay\n\t    def get_supervised_loss(self, images, labels):\n\t        tgt = self.tokenizer.encode(labels, self._device)\n\t        teacher_img, student_img = images[:,0], images[:,1]\n\t        # Encode the source sequence (i.e. the image codes)\n\t        teacher_memory = self.encode(teacher_img)\n\t        student_memory = self.encode(student_img)\n", "        # Prepare the target sequences (input and output)\n\t        tgt_perms = self.gen_tgt_perms(tgt)\n\t        tgt_in = tgt[:, :-1]\n\t        tgt_out = tgt[:, 1:]\n\t        # The [EOS] token is not depended upon by any other token in any permutation ordering\n\t        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n\t        teacher_loss = 0\n\t        student_loss = 0\n\t        loss_numel = 0\n\t        n = (tgt_out != self.pad_id).sum().item()\n", "        for i, perm in enumerate(tgt_perms):\n\t            tgt_mask, query_mask = self.generate_attn_masks(perm)\n\t            teacher_out = self.decode(tgt_in, teacher_memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n\t            teacher_logits = self.head(teacher_out)\n\t            teacher_loss += n * F.cross_entropy(teacher_logits.flatten(end_dim=1), tgt_out.flatten(), ignore_index=self.pad_id)\n\t            student_out = self.decode(tgt_in, student_memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n\t            student_logits = self.head(student_out)\n\t            student_loss += n * F.cross_entropy(student_logits.flatten(end_dim=1), tgt_out.flatten(), ignore_index=self.pad_id)\n\t            loss_numel += n\n\t            # After the second iteration (i.e. done with canonical and reverse orderings),\n", "            # remove the [EOS] tokens for the succeeding perms\n\t            if i == 1:\n\t                tgt_out = torch.where(tgt_out == self.eos_id, self.pad_id, tgt_out)\n\t                n = (tgt_out != self.pad_id).sum().item()\n\t        teacher_loss /= loss_numel\n\t        student_loss /= loss_numel\n\t        loss = teacher_loss + student_loss\n\t        return loss, teacher_logits, student_logits, tgt_out\n\t    def training_step(self, images, labels): #-> STEP_OUTPUT:\n\t        ## semimtr.modules.model_fusion_consistency_regularization.py\n", "        # Decoder returns query        \n\t        # Encoder visiontransformer(transformer encoder with image embedding) returns forward_features \n\t        loss = 0\n\t        if self.supervised_flag : \n\t            ce_loss_ts, teacher_logits, student_logits, labels_encoded = self.get_supervised_loss(images, labels)\n\t            loss += ce_loss_ts\n\t        mask = torch.sum(labels_encoded != self.pad_id, dim=1)\n\t        teacher_logits, threshold_mask = self.create_teacher_labels(teacher_logits)\n\t        masked_student_logits = self._flatten(student_logits, mask)\n\t        masked_teacher_logits = self._flatten(teacher_logits, mask)\n", "        ce_loss_student_teacher = F.cross_entropy(masked_student_logits.view(-1, masked_student_logits.shape[-1]), \\\n\t                                                  masked_teacher_logits.view(-1, masked_teacher_logits.shape[-1]), reduction='none')\n\t        if threshold_mask:\n\t            ce_loss_student_teacher = ce_loss_student_teacher * threshold_mask.view(-1, threshold_mask.shape[-1])\n\t        ce_loss_student_teacher = ce_loss_student_teacher.mean()\n\t        loss += ce_loss_student_teacher * self.cr_loss_weight\n\t        return loss\n\t    def create_teacher_labels(self, teacher_predictions):\n\t        self.threshold_value = 0.9\n\t        self.teacher_stop_gradients=True\n", "        pt_logits_teacher = teacher_predictions\n\t        if self.teacher_stop_gradients:\n\t            pt_logits_teacher = pt_logits_teacher.detach()\n\t        pt_labels_teacher = F.softmax(pt_logits_teacher, dim=-1)\n\t        max_values, max_indices = torch.max(pt_labels_teacher, dim=-1, keepdim=True)\n\t        if self.teacher_one_hot_labels:\n\t            pt_labels_teacher = torch.zeros_like(pt_logits_teacher).scatter_(-1, max_indices, 1)\n\t        threshold_mask = (max_values.squeeze() > self.threshold_value).float() if self.use_threshold else None\n\t        return pt_labels_teacher, threshold_mask   \n\t    def _flatten(self, sources, lengths):\n", "        return torch.cat([t[:l] for t, l in zip(sources, lengths)])"]}
{"filename": "model/base.py", "chunked_list": ["import math\n\tfrom abc import ABC, abstractmethod\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional, Tuple, List\n\timport torch\n\timport torch.nn.functional as F\n\tfrom nltk import edit_distance\n\tfrom torch import Tensor\n\tfrom torch.optim import Optimizer\n\tfrom torch.optim.lr_scheduler import OneCycleLR\n", "from model.tokenizer_utils import CTCTokenizer, Tokenizer, BaseTokenizer\n\t@dataclass\n\tclass BatchResult:\n\t    num_samples: int\n\t    correct: int\n\t    ned: float\n\t    confidence: float\n\t    label_length: int\n\t    loss: Tensor\n\t    loss_numel: int\n", "class BaseSystem(torch.nn.Module):\n\t    def __init__(self, tokenizer: BaseTokenizer, charset_test: str,\n\t                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float) -> None:\n\t        super().__init__()\n\t        self.tokenizer = tokenizer\n\t        self.batch_size = batch_size\n\t        self.lr = lr\n\t        self.warmup_pct = warmup_pct\n\t        self.weight_decay = weight_decay\n\t    @abstractmethod\n", "    def forward(self, images: Tensor, max_length: Optional[int] = None) -> Tensor:\n\t        \"\"\"Inference\n\t        Args:\n\t            images: Batch of images. Shape: N, Ch, H, W\n\t            max_length: Max sequence length of the output. If None, will use default.\n\t        Returns:\n\t            logits: N, L, C (L = sequence length, C = number of classes, typically len(charset_train) + num specials)\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n", "    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n\t        \"\"\"Like forward(), but also computes the loss (calls forward() internally).\n\t        Args:\n\t            images: Batch of images. Shape: N, Ch, H, W\n\t            labels: Text labels of the images\n\t        Returns:\n\t            logits: N, L, C (L = sequence length, C = number of classes, typically len(charset_train) + num specials)\n\t            loss: mean loss for the batch\n\t            loss_numel: number of elements the loss was calculated from\n\t        \"\"\"\n", "        raise NotImplementedError\n\t    def _eval_step(self, batch, validation: bool): #-> Optional[STEP_OUTPUT]:\n\t        images, labels = batch\n\t        correct = 0\n\t        total = 0\n\t        ned = 0\n\t        confidence = 0\n\t        label_length = 0\n\t        if validation:\n\t            logits, loss, loss_numel = self.forward_logits_loss(images, labels)\n", "        else:\n\t            logits = self.forward(images)\n\t            loss = loss_numel = None  # Only used for validation; not needed at test-time.\n\t        probs = logits.softmax(-1)\n\t        preds, probs = self.tokenizer.decode(probs)\n\t        for pred, prob, gt in zip(preds, probs, labels):\n\t            confidence += prob.prod().item()\n\t            ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n\t            if pred == gt:\n\t                correct += 1\n", "            total += 1\n\t            label_length += len(pred)\n\t        return dict(output=BatchResult(total, correct, ned, confidence, label_length, loss, loss_numel))\n\t    @staticmethod\n\t    def _aggregate_results(outputs) -> Tuple[float, float, float]:\n\t        if not outputs:\n\t            return 0., 0., 0.\n\t        total_loss = 0\n\t        total_loss_numel = 0\n\t        total_n_correct = 0\n", "        total_norm_ED = 0\n\t        total_size = 0\n\t        for result in outputs:\n\t            result = result['output']\n\t            total_loss += result.loss_numel * result.loss\n\t            total_loss_numel += result.loss_numel\n\t            total_n_correct += result.correct\n\t            total_norm_ED += result.ned\n\t            total_size += result.num_samples\n\t        acc = total_n_correct / total_size\n", "        ned = (1 - total_norm_ED / total_size)\n\t        loss = total_loss / total_loss_numel\n\t        return acc, ned, loss\n\t    def validation_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n\t        return self._eval_step(batch, True)\n\t    def test_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n\t        return self._eval_step(batch, False)\n\tclass CrossEntropySystem(BaseSystem):\n\t    def __init__(self, charset_train: str, charset_test: str,\n\t                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float) -> None:\n", "        tokenizer = Tokenizer(charset_train)\n\t        super().__init__(tokenizer, charset_test, batch_size, lr, warmup_pct, weight_decay)\n\t        self.bos_id = tokenizer.bos_id\n\t        self.eos_id = tokenizer.eos_id\n\t        self.pad_id = tokenizer.pad_id\n\t    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n\t        targets = self.tokenizer.encode(labels, self._device)\n\t        targets = targets[:, 1:]  # Discard <bos>\n\t        max_len = targets.shape[1] - 1  # exclude <eos> from count\n\t        logits = self.forward(images, max_len)\n", "        loss = F.cross_entropy(logits.flatten(end_dim=1), targets.flatten(), ignore_index=self.pad_id)\n\t        loss_numel = (targets != self.pad_id).sum()\n\t        return logits, loss, loss_numel\n\tclass CTCSystem(BaseSystem):\n\t    def __init__(self, charset_train: str, charset_test: str,\n\t                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float) -> None:\n\t        tokenizer = CTCTokenizer(charset_train)\n\t        super().__init__(tokenizer, charset_test, batch_size, lr, warmup_pct, weight_decay)\n\t        self.blank_id = tokenizer.blank_id\n\t    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n", "        targets = self.tokenizer.encode(labels, self._device)\n\t        logits = self.forward(images)\n\t        log_probs = logits.log_softmax(-1).transpose(0, 1)  # swap batch and seq. dims\n\t        T, N, _ = log_probs.shape\n\t        input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long, device=self._device)\n\t        target_lengths = torch.as_tensor(list(map(len, labels)), dtype=torch.long, device=self._device)\n\t        loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=self.blank_id, zero_infinity=True)\n\t        return logits, loss, N\n"]}
{"filename": "model/tokenizer_utils.py", "chunked_list": ["import re\n\tfrom abc import ABC, abstractmethod\n\tfrom itertools import groupby\n\tfrom typing import List, Optional, Tuple\n\timport torch\n\tfrom torch import Tensor\n\tfrom torch.nn.utils.rnn import pad_sequence\n\tclass CharsetAdapter:\n\t    \"\"\"Transforms labels according to the target charset.\"\"\"\n\t    def __init__(self, target_charset) -> None:\n", "        super().__init__()\n\t        self.charset = target_charset ###\n\t        self.lowercase_only = target_charset == target_charset.lower()\n\t        self.uppercase_only = target_charset == target_charset.upper()\n\t#         self.unsupported = f'[^{re.escape(target_charset)}]'\n\t    def __call__(self, label):\n\t        if self.lowercase_only:\n\t            label = label.lower()\n\t        elif self.uppercase_only:\n\t            label = label.upper()\n", "        return label\n\tclass BaseTokenizer(ABC):\n\t    def __init__(self, charset: str, specials_first: tuple = (), specials_last: tuple = ()) -> None:\n\t        self._itos = specials_first + tuple(charset+'[UNK]') + specials_last\n\t        self._stoi = {s: i for i, s in enumerate(self._itos)}\n\t    def __len__(self):\n\t        return len(self._itos)\n\t    def _tok2ids(self, tokens: str) -> List[int]:\n\t        return [self._stoi[s] for s in tokens]\n\t    def _ids2tok(self, token_ids: List[int], join: bool = True) -> str:\n", "        tokens = [self._itos[i] for i in token_ids]\n\t        return ''.join(tokens) if join else tokens\n\t    @abstractmethod\n\t    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n\t        \"\"\"Encode a batch of labels to a representation suitable for the model.\n\t        Args:\n\t            labels: List of labels. Each can be of arbitrary length.\n\t            device: Create tensor on this device.\n\t        Returns:\n\t            Batched tensor representation padded to the max label length. Shape: N, L\n", "        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n\t        \"\"\"Internal method which performs the necessary filtering prior to decoding.\"\"\"\n\t        raise NotImplementedError\n\t    def decode(self, token_dists: Tensor, raw: bool = False) -> Tuple[List[str], List[Tensor]]:\n\t        \"\"\"Decode a batch of token distributions.\n\t        Args:\n\t            token_dists: softmax probabilities over the token distribution. Shape: N, L, C\n", "            raw: return unprocessed labels (will return list of list of strings)\n\t        Returns:\n\t            list of string labels (arbitrary length) and\n\t            their corresponding sequence probabilities as a list of Tensors\n\t        \"\"\"\n\t        batch_tokens = []\n\t        batch_probs = []\n\t        for dist in token_dists:\n\t            probs, ids = dist.max(-1)  # greedy selection\n\t            if not raw:\n", "                probs, ids = self._filter(probs, ids)\n\t            tokens = self._ids2tok(ids, not raw)\n\t            batch_tokens.append(tokens)\n\t            batch_probs.append(probs)\n\t        return batch_tokens, batch_probs\n\tclass Tokenizer(BaseTokenizer):\n\t    BOS = '[B]'\n\t    EOS = '[E]'\n\t    PAD = '[P]'\n\t    def __init__(self, charset: str) -> None:\n", "        specials_first = (self.EOS,)\n\t        specials_last = (self.BOS, self.PAD)\n\t        super().__init__(charset, specials_first, specials_last)\n\t        self.eos_id, self.bos_id, self.pad_id = [self._stoi[s] for s in specials_first + specials_last]\n\t    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n\t        batch = [torch.as_tensor([self.bos_id] + self._tok2ids(y) + [self.eos_id], dtype=torch.long, device=device)\n\t                 for y in labels]\n\t        return pad_sequence(batch, batch_first=True, padding_value=self.pad_id)\n\t    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n\t        ids = ids.tolist()\n", "        try:\n\t            eos_idx = ids.index(self.eos_id)\n\t        except ValueError:\n\t            eos_idx = len(ids)  # Nothing to truncate.\n\t        # Truncate after EOS\n\t        ids = ids[:eos_idx]\n\t        probs = probs[:eos_idx + 1]  # but include prob. for EOS (if it exists)\n\t        return probs, ids\n\tclass CTCTokenizer(BaseTokenizer):\n\t    BLANK = '[B]'\n", "    def __init__(self, charset: str) -> None:\n\t        # BLANK uses index == 0 by default\n\t        super().__init__(charset, specials_first=(self.BLANK,))\n\t        self.blank_id = self._stoi[self.BLANK]\n\t    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n\t        # We use a padded representation since we don't want to use CUDNN's CTC implementation\n\t        batch = [torch.as_tensor(self._tok2ids(y), dtype=torch.long, device=device) for y in labels]\n\t        return pad_sequence(batch, batch_first=True, padding_value=self.blank_id)\n\t    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n\t        # Best path decoding:\n", "        ids = list(zip(*groupby(ids.tolist())))[0]  # Remove duplicate tokens\n\t        ids = [x for x in ids if x != self.blank_id]  # Remove BLANKs\n\t        # `probs` is just pass-through since all positions are considered part of the path\n\t        return probs, ids\n"]}
{"filename": "model/utils.py", "chunked_list": ["from pathlib import PurePath\n\tfrom typing import Sequence\n\timport torch\n\tfrom torch import nn\n\timport yaml\n\tclass InvalidModelError(RuntimeError):\n\t    \"\"\"Exception raised for any model-related error (creation, loading)\"\"\"\n\t_WEIGHTS_URL = {\n\t    'parseq-tiny': 'https://github.com/baudm/parseq/releases/download/v1.0.0/parseq_tiny-e7a21b54.pt',\n\t    'parseq': 'https://github.com/baudm/parseq/releases/download/v1.0.0/parseq-bb5792a6.pt',\n", "    'abinet': 'https://github.com/baudm/parseq/releases/download/v1.0.0/abinet-1d1e373e.pt',\n\t    'trba': 'https://github.com/baudm/parseq/releases/download/v1.0.0/trba-cfaed284.pt',\n\t    'vitstr': 'https://github.com/baudm/parseq/releases/download/v1.0.0/vitstr-26d0fcf4.pt',\n\t    'crnn': 'https://github.com/baudm/parseq/releases/download/v1.0.0/crnn-679d0e31.pt',\n\t}\n\tdef _get_config(experiment: str, **kwargs):\n\t    \"\"\"Emulates hydra config resolution\"\"\"\n\t    root = PurePath(__file__).parents[2]\n\t    with open(root / 'configs/main.yaml', 'r') as f:\n\t        config = yaml.load(f, yaml.Loader)['model']\n", "    with open(root / f'configs/charset/94_full.yaml', 'r') as f:\n\t        config.update(yaml.load(f, yaml.Loader)['model'])\n\t    with open(root / f'configs/experiment/{experiment}.yaml', 'r') as f:\n\t        exp = yaml.load(f, yaml.Loader)\n\t    # Apply base model config\n\t    model = exp['defaults'][0]['override /model']\n\t    with open(root / f'configs/model/{model}.yaml', 'r') as f:\n\t        config.update(yaml.load(f, yaml.Loader))\n\t    # Apply experiment config\n\t    if 'model' in exp:\n", "        config.update(exp['model'])\n\t    config.update(kwargs)\n\t    # Workaround for now: manually cast the lr to the correct type.\n\t    config['lr'] = float(config['lr'])\n\t    return config\n\tdef _get_model_class(key):\n\t    if 'abinet' in key:\n\t        from .abinet.system import ABINet as ModelClass\n\t    elif 'crnn' in key:\n\t        from .crnn.system import CRNN as ModelClass\n", "    elif 'parseq' in key:\n\t        from .parseq.system import PARSeq as ModelClass\n\t    elif 'trba' in key:\n\t        from .trba.system import TRBA as ModelClass\n\t    elif 'trbc' in key:\n\t        from .trba.system import TRBC as ModelClass\n\t    elif 'vitstr' in key:\n\t        from .vitstr.system import ViTSTR as ModelClass\n\t    else:\n\t        raise InvalidModelError(\"Unable to find model class for '{}'\".format(key))\n", "    return ModelClass\n\tdef get_pretrained_weights(experiment):\n\t    try:\n\t        url = _WEIGHTS_URL[experiment]\n\t    except KeyError:\n\t        raise InvalidModelError(\"No pretrained weights found for '{}'\".format(experiment)) from None\n\t    return torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)\n\tdef create_model(experiment: str, pretrained: bool = False, **kwargs):\n\t    try:\n\t        config = _get_config(experiment, **kwargs)\n", "    except FileNotFoundError:\n\t        raise InvalidModelError(\"No configuration found for '{}'\".format(experiment)) from None\n\t    ModelClass = _get_model_class(experiment)\n\t    model = ModelClass(**config)\n\t    if pretrained:\n\t        model.load_state_dict(get_pretrained_weights(experiment))\n\t    return model\n\tdef load_from_checkpoint(checkpoint_path: str, **kwargs):\n\t    if checkpoint_path.startswith('pretrained='):\n\t        model_id = checkpoint_path.split('=', maxsplit=1)[1]\n", "        model = create_model(model_id, True, **kwargs)\n\t    else:\n\t        ModelClass = _get_model_class(checkpoint_path)\n\t        model = ModelClass.load_from_checkpoint(checkpoint_path, **kwargs)\n\t    return model\n\tdef parse_model_args(args):\n\t    kwargs = {}\n\t    arg_types = {t.__name__: t for t in [int, float, str]}\n\t    arg_types['bool'] = lambda v: v.lower() == 'true'  # special handling for bool\n\t    for arg in args:\n", "        name, value = arg.split('=', maxsplit=1)\n\t        name, arg_type = name.split(':', maxsplit=1)\n\t        kwargs[name] = arg_types[arg_type](value)\n\t    return kwargs\n\tdef init_weights(module: nn.Module, name: str = '', exclude: Sequence[str] = ()):\n\t    \"\"\"Initialize the weights using the typical initialization schemes used in SOTA models.\"\"\"\n\t    if any(map(name.startswith, exclude)):\n\t        return\n\t    if isinstance(module, nn.Linear):\n\t        nn.init.trunc_normal_(module.weight, std=.02)\n", "        if module.bias is not None:\n\t            nn.init.zeros_(module.bias)\n\t    elif isinstance(module, nn.Embedding):\n\t        nn.init.trunc_normal_(module.weight, std=.02)\n\t        if module.padding_idx is not None:\n\t            module.weight.data[module.padding_idx].zero_()\n\t    elif isinstance(module, nn.Conv2d):\n\t        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n\t        if module.bias is not None:\n\t            nn.init.zeros_(module.bias)\n", "    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n\t        nn.init.ones_(module.weight)\n\t        nn.init.zeros_(module.bias)\n"]}
{"filename": "model/modules.py", "chunked_list": ["import math\n\tfrom typing import Optional\n\timport torch\n\tfrom torch import nn as nn, Tensor\n\tfrom torch.nn import functional as F\n\tfrom torch.nn.modules import transformer\n\tfrom timm.models.vision_transformer import VisionTransformer, PatchEmbed\n\tclass DecoderLayer(nn.Module):\n\t    \"\"\"A Transformer decoder layer supporting two-stream attention (XLNet)\n\t       This implements a pre-LN decoder, as opposed to the post-LN default in PyTorch.\"\"\"\n", "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='gelu',\n\t                 layer_norm_eps=1e-5):\n\t        super().__init__()\n\t        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n\t        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n\t        # Implementation of Feedforward model\n\t        self.linear1 = nn.Linear(d_model, dim_feedforward)\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\t        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n", "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n\t        self.norm_q = nn.LayerNorm(d_model, eps=layer_norm_eps)\n\t        self.norm_c = nn.LayerNorm(d_model, eps=layer_norm_eps)\n\t        self.dropout1 = nn.Dropout(dropout)\n\t        self.dropout2 = nn.Dropout(dropout)\n\t        self.dropout3 = nn.Dropout(dropout)\n\t        self.activation = transformer._get_activation_fn(activation)\n\t    def __setstate__(self, state):\n\t        if 'activation' not in state:\n\t            state['activation'] = F.gelu\n", "        super().__setstate__(state)\n\t    def forward_stream(self, tgt: Tensor, tgt_norm: Tensor, tgt_kv: Tensor, memory: Tensor, tgt_mask: Optional[Tensor],\n\t                       tgt_key_padding_mask: Optional[Tensor]):\n\t        tgt2, sa_weights = self.self_attn(tgt_norm, tgt_kv, tgt_kv, attn_mask=tgt_mask,\n\t                                          key_padding_mask=tgt_key_padding_mask)\n\t        tgt = tgt + self.dropout1(tgt2)\n\t        tgt2, ca_weights = self.cross_attn(self.norm1(tgt), memory, memory)\n\t        tgt = tgt + self.dropout2(tgt2)\n\t        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(self.norm2(tgt)))))\n\t        tgt = tgt + self.dropout3(tgt2)\n", "        return tgt, sa_weights, ca_weights\n\t    def forward(self, query, content, memory, query_mask: Optional[Tensor] = None, content_mask: Optional[Tensor] = None,\n\t                content_key_padding_mask: Optional[Tensor] = None, update_content: bool = True):\n\t        query_norm = self.norm_q(query)\n\t        content_norm = self.norm_c(content)\n\t        query = self.forward_stream(query, query_norm, content_norm, memory, query_mask, content_key_padding_mask)[0]\n\t        if update_content:\n\t            content = self.forward_stream(content, content_norm, content_norm, memory, content_mask,\n\t                                          content_key_padding_mask)[0]\n\t        return query, content\n", "class Decoder(nn.Module):\n\t    __constants__ = ['norm']\n\t    def __init__(self, decoder_layer, num_layers, norm):\n\t        super().__init__()\n\t        self.layers = transformer._get_clones(decoder_layer, num_layers)\n\t        self.num_layers = num_layers\n\t        self.norm = norm\n\t    def forward(self, query, content, memory, query_mask: Optional[Tensor] = None, content_mask: Optional[Tensor] = None,\n\t                content_key_padding_mask: Optional[Tensor] = None):\n\t        for i, mod in enumerate(self.layers):\n", "            last = i == len(self.layers) - 1\n\t            query, content = mod(query, content, memory, query_mask, content_mask, content_key_padding_mask,\n\t                                 update_content=not last)\n\t        query = self.norm(query)\n\t        return query\n\tclass Encoder(VisionTransformer):\n\t    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n\t                 qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed):\n\t        super().__init__(img_size, patch_size, in_chans, embed_dim=embed_dim, depth=depth, num_heads=num_heads,\n\t                         mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n", "                         drop_path_rate=drop_path_rate, embed_layer=embed_layer,\n\t                         num_classes=0, global_pool='', class_token=False)  # these disable the classifier head\n\t    def forward(self, x):\n\t        # Return all tokens\n\t        return self.forward_features(x)\n\tclass TokenEmbedding(nn.Module):\n\t    def __init__(self, charset_size: int, embed_dim: int):\n\t        super().__init__()\n\t        self.embedding = nn.Embedding(charset_size, embed_dim)\n\t        self.embed_dim = embed_dim\n", "    def forward(self, tokens: torch.Tensor):\n\t        return math.sqrt(self.embed_dim) * self.embedding(tokens)\n"]}
{"filename": "model/parseq.py", "chunked_list": ["import math\n\tfrom functools import partial\n\tfrom itertools import permutations\n\tfrom typing import Sequence, Any, Optional, Tuple, List\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import Tensor\n\tfrom timm.models.helpers import named_apply\n", "from dataclasses import dataclass\n\tfrom nltk import edit_distance\n\tfrom .utils import init_weights\n\tfrom .modules import DecoderLayer, Decoder, Encoder, TokenEmbedding\n\tfrom .tokenizer_utils import Tokenizer, BaseTokenizer\n\t@dataclass\n\tclass BatchResult:\n\t    num_samples: int\n\t    correct: int\n\t    ned: float\n", "    confidence: float\n\t    label_length: int\n\t    loss: Tensor\n\t    loss_numel: int\n\tclass PARSeq(nn.Module):\n\t    def __init__(self, charset_train: str, charset_test: str, max_label_length: int,\n\t                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float,\n\t                 img_size: Sequence[int], patch_size: Sequence[int], embed_dim: int,\n\t                 enc_num_heads: int, enc_mlp_ratio: int, enc_depth: int,\n\t                 dec_num_heads: int, dec_mlp_ratio: int, dec_depth: int,\n", "                 perm_num: int, perm_forward: bool, perm_mirrored: bool,\n\t                 decode_ar: bool, refine_iters: int, dropout: float, **kwargs: Any) -> None:\n\t        tokenizer = Tokenizer(charset_train)\n\t        super().__init__()\n\t        self.tokenizer = tokenizer\n\t        self.bos_id = self.tokenizer.bos_id\n\t        self.eos_id = self.tokenizer.eos_id\n\t        self.pad_id = self.tokenizer.pad_id\n\t        self.batch_size = batch_size\n\t        self.lr = lr\n", "        self.warmup_pct = warmup_pct\n\t        self.weight_decay = weight_decay\n\t        self._device = None\n\t        self.max_label_length = max_label_length\n\t        self.decode_ar = decode_ar\n\t        self.refine_iters = refine_iters\n\t        self.encoder = Encoder(img_size, patch_size, embed_dim=embed_dim, depth=enc_depth, num_heads=enc_num_heads,\n\t                               mlp_ratio=enc_mlp_ratio)\n\t        decoder_layer = DecoderLayer(embed_dim, dec_num_heads, embed_dim * dec_mlp_ratio, dropout)\n\t        self.decoder = Decoder(decoder_layer, num_layers=dec_depth, norm=nn.LayerNorm(embed_dim))\n", "        # Perm/attn mask stuff\n\t        self.rng = np.random.default_rng()\n\t        self.max_gen_perms = perm_num // 2 if perm_mirrored else perm_num\n\t        self.perm_forward = perm_forward\n\t        self.perm_mirrored = perm_mirrored\n\t        # We don't predict <bos> nor <pad>\n\t        self.head = nn.Linear(embed_dim, len(self.tokenizer) - 2)\n\t        self.text_embed = TokenEmbedding(len(self.tokenizer), embed_dim)\n\t        # +1 for <eos>\n\t        self.pos_queries = nn.Parameter(torch.Tensor(1, max_label_length + 1, embed_dim))\n", "        self.dropout = nn.Dropout(p=dropout)\n\t        # Encoder has its own init.\n\t        named_apply(partial(init_weights, exclude=['encoder']), self)\n\t        nn.init.trunc_normal_(self.pos_queries, std=.02)\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        param_names = {'text_embed.embedding.weight', 'pos_queries'}\n\t        enc_param_names = {'encoder.' + n for n in self.encoder.no_weight_decay()}\n\t        return param_names.union(enc_param_names)\n\t    def encode(self, img: torch.Tensor):\n", "        return self.encoder(img)\n\t    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[Tensor] = None,\n\t               tgt_padding_mask: Optional[Tensor] = None, tgt_query: Optional[Tensor] = None,\n\t               tgt_query_mask: Optional[Tensor] = None):\n\t        N, L = tgt.shape\n\t        # <bos> stands for the null context. We only supply position information for characters after <bos>.\n\t        null_ctx = self.text_embed(tgt[:, :1])\n\t        tgt_emb = self.pos_queries[:, :L - 1] + self.text_embed(tgt[:, 1:])\n\t        tgt_emb = self.dropout(torch.cat([null_ctx, tgt_emb], dim=1))\n\t        if tgt_query is None:\n", "            tgt_query = self.pos_queries[:, :L].expand(N, -1, -1)\n\t        tgt_query = self.dropout(tgt_query)\n\t        return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask, tgt_mask, tgt_padding_mask)\n\t    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n\t        targets = self.tokenizer.encode(labels, self._device)\n\t        targets = targets[:, 1:]  # Discard <bos>\n\t        max_len = targets.shape[1] - 1  # exclude <eos> from count\n\t        logits = self.forward(images, max_len)\n\t        loss = F.cross_entropy(logits.flatten(end_dim=1), targets.flatten(), ignore_index=self.pad_id)\n\t        loss_numel = (targets != self.pad_id).sum()\n", "        return logits, loss, loss_numel\n\t    def forward(self, images: Tensor, max_length: Optional[int] = None) -> Tensor:\n\t        testing = max_length is None\n\t        max_length = self.max_label_length if max_length is None else min(max_length, self.max_label_length)\n\t        bs = images.shape[0]\n\t        # +1 for <eos> at end of sequence.\n\t        num_steps = max_length + 1\n\t        memory = self.encode(images)\n\t        # Query positions up to `num_steps`\n\t        pos_queries = self.pos_queries[:, :num_steps].expand(bs, -1, -1)\n", "        # Special case for the forward permutation. Faster than using `generate_attn_masks()`\n\t        tgt_mask = query_mask = torch.triu(torch.full((num_steps, num_steps), float('-inf'), device=self._device), 1)\n\t        if self.decode_ar:\n\t            tgt_in = torch.full((bs, num_steps), self.pad_id, dtype=torch.long, device=self._device)\n\t            tgt_in[:, 0] = self.bos_id\n\t            logits = []\n\t            for i in range(num_steps):\n\t                j = i + 1  # next token index\n\t                tgt_out = self.decode(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n\t                                      tgt_query_mask=query_mask[i:j, :j])\n", "                # the next token probability is in the output's ith token position\n\t                p_i = self.head(tgt_out)\n\t                logits.append(p_i)\n\t                if j < num_steps:\n\t                    # greedy decode. add the next token index to the target input\n\t                    tgt_in[:, j] = p_i.squeeze().argmax(-1)\n\t                    if testing and (tgt_in == self.eos_id).any(dim=-1).all():\n\t                        break\n\t            logits = torch.cat(logits, dim=1)\n\t        else:\n", "            # No prior context, so input is just <bos>. We query all positions.\n\t            tgt_in = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n\t            tgt_out = self.decode(tgt_in, memory, tgt_query=pos_queries)\n\t            logits = self.head(tgt_out)\n\t        if self.refine_iters:\n\t            # For iterative refinement, we always use a 'cloze' mask.\n\t            # We can derive it from the AR forward mask by unmasking the token context to the right.\n\t            query_mask[torch.triu(torch.ones(num_steps, num_steps, dtype=torch.bool, device=self._device), 2)] = 0\n\t            bos = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n\t            for i in range(self.refine_iters):\n", "                # Prior context is the previous output.\n\t                tgt_in = torch.cat([bos, logits[:, :-1].argmax(-1)], dim=1)\n\t                tgt_padding_mask = ((tgt_in == self.eos_id).int().cumsum(-1) > 0)  # mask tokens beyond the first EOS token.\n\t                tgt_out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask,\n\t                                      tgt_query=pos_queries, tgt_query_mask=query_mask[:, :tgt_in.shape[1]])\n\t                logits = self.head(tgt_out)\n\t        return logits\n\t    def gen_tgt_perms(self, tgt):\n\t        \"\"\"Generate shared permutations for the whole batch.\n\t           This works because the same attention mask can be used for the shorter sequences\n", "           because of the padding mask.\n\t        \"\"\"\n\t        # We don't permute the position of BOS, we permute EOS separately\n\t        max_num_chars = tgt.shape[1] - 2\n\t        # Special handling for 1-character sequences\n\t        if max_num_chars == 1:\n\t            return torch.arange(3, device=self._device).unsqueeze(0)\n\t        perms = [torch.arange(max_num_chars, device=self._device)] if self.perm_forward else []\n\t        # Additional permutations if needed\n\t        max_perms = math.factorial(max_num_chars)\n", "        if self.perm_mirrored:\n\t            max_perms //= 2\n\t        num_gen_perms = min(self.max_gen_perms, max_perms)\n\t        if max_num_chars < 5:\n\t            if max_num_chars == 4 and self.perm_mirrored:\n\t                selector = [0, 3, 4, 6, 9, 10, 12, 16, 17, 18, 19, 21]\n\t            else:\n\t                selector = list(range(max_perms))\n\t            perm_pool = torch.as_tensor(list(permutations(range(max_num_chars), max_num_chars)), device=self._device)[selector]\n\t            # If the forward permutation is always selected, no need to add it to the pool for sampling\n", "            if self.perm_forward:\n\t                perm_pool = perm_pool[1:]\n\t            perms = torch.stack(perms)\n\t            if len(perm_pool):\n\t                i = self.rng.choice(len(perm_pool), size=num_gen_perms - len(perms), replace=False)\n\t                perms = torch.cat([perms, perm_pool[i]])\n\t        else:\n\t            perms.extend([torch.randperm(max_num_chars, device=self._device) for _ in range(num_gen_perms - len(perms))])\n\t            perms = torch.stack(perms)\n\t        if self.perm_mirrored:\n", "            # Add complementary pairs\n\t            comp = perms.flip(-1)\n\t            # Stack in such a way that the pairs are next to each other.\n\t            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n\t        bos_idx = perms.new_zeros((len(perms), 1))\n\t        eos_idx = perms.new_full((len(perms), 1), max_num_chars + 1)\n\t        perms = torch.cat([bos_idx, perms + 1, eos_idx], dim=1)\n\t        if len(perms) > 1:\n\t            perms[1, 1:] = max_num_chars + 1 - torch.arange(max_num_chars + 1, device=self._device)\n\t        return perms\n", "    def generate_attn_masks(self, perm):\n\t        \"\"\"Generate attention masks given a sequence permutation (includes pos. for bos and eos tokens)\n\t        :param perm: the permutation sequence. i = 0 is always the BOS\n\t        :return: lookahead attention masks\n\t        \"\"\"\n\t        sz = perm.shape[0]\n\t        mask = torch.zeros((sz, sz), device=self._device)\n\t        for i in range(sz):\n\t            query_idx = perm[i]\n\t            masked_keys = perm[i + 1:]\n", "            mask[query_idx, masked_keys] = float('-inf')\n\t        content_mask = mask[:-1, :-1].clone()\n\t        mask[torch.eye(sz, dtype=torch.bool, device=self._device)] = float('-inf')  # mask \"self\"\n\t        query_mask = mask[1:, :-1]\n\t        return content_mask, query_mask\n\t    def training_step(self, images, labels): #-> STEP_OUTPUT:\n\t        tgt = self.tokenizer.encode(labels, self._device)\n\t        # Encode the source sequence (i.e. the image codes)\n\t        memory = self.encode(images)\n\t        # Prepare the target sequences (input and output)\n", "        tgt_perms = self.gen_tgt_perms(tgt)\n\t        tgt_in = tgt[:, :-1]\n\t        tgt_out = tgt[:, 1:]\n\t        # The [EOS] token is not depended upon by any other token in any permutation ordering\n\t        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n\t        loss = 0\n\t        loss_numel = 0\n\t        n = (tgt_out != self.pad_id).sum().item()\n\t        for i, perm in enumerate(tgt_perms):\n\t            tgt_mask, query_mask = self.generate_attn_masks(perm)\n", "            out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n\t            logits = self.head(out).flatten(end_dim=1)\n\t            loss += n * F.cross_entropy(logits, tgt_out.flatten(), ignore_index=self.pad_id)\n\t            loss_numel += n\n\t            if i == 1:\n\t                tgt_out = torch.where(tgt_out == self.eos_id, self.pad_id, tgt_out)\n\t                n = (tgt_out != self.pad_id).sum().item()\n\t        loss /= loss_numel\n\t        return loss\n\t    def _eval_step(self, batch, validation: bool): #-> Optional[STEP_OUTPUT]:\n", "        images, labels = batch\n\t        correct = 0\n\t        total = 0\n\t        ned = 0\n\t        confidence = 0\n\t        label_length = 0\n\t        if validation:\n\t            logits, loss, loss_numel = self.forward_logits_loss(images, labels)\n\t        else:\n\t            logits = self.forward(images)\n", "            loss = loss_numel = None  # Only used for validation; not needed at test-time.\n\t        probs = logits.softmax(-1)\n\t        preds, probs = self.tokenizer.decode(probs)\n\t        for pred, prob, gt in zip(preds, probs, labels):\n\t            confidence += prob.prod().item()\n\t            ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n\t            pred = pred.replace('ल','[UNK]').replace('###','[UNK]').replace(' ','').replace('[U]','[UNK]')\n\t            gt = gt.replace('###','[UNK]').replace(' ','')\n\t            if pred == gt:\n\t                correct += 1\n", "            total += 1\n\t            label_length += len(pred)\n\t        return dict(output=BatchResult(total, correct, ned, confidence, label_length, loss, loss_numel))\n\t    @staticmethod\n\t    def _aggregate_results(outputs) -> Tuple[float, float, float]:\n\t        if not outputs:\n\t            return 0., 0., 0.\n\t        total_loss = 0\n\t        total_loss_numel = 0\n\t        total_n_correct = 0\n", "        total_norm_ED = 0\n\t        total_size = 0\n\t        for result in outputs:\n\t            result = result['output']\n\t            total_loss += result.loss_numel * result.loss\n\t            total_loss_numel += result.loss_numel\n\t            total_n_correct += result.correct\n\t            total_norm_ED += result.ned\n\t            total_size += result.num_samples\n\t        acc = total_n_correct / total_size\n", "        ned = (1 - total_norm_ED / total_size)\n\t        loss = total_loss / total_loss_numel\n\t        return acc, ned, loss\n\t    def validation_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n\t        return self._eval_step(batch, True)\n\t    def test_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n\t        return self._eval_step(batch, False)\n"]}
