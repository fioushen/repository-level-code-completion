{"filename": "tests/test_duckdb_pandas_compare.py", "chunked_list": ["import numpy as np\n\timport teehr.queries.pandas as tqk\n\timport teehr.queries.duckdb as tqu\n\tfrom pathlib import Path\n\tTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\n\tPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\n\tSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\n\tCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\n\tGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\tdef test_metric_compare_1():\n", "    include_metrics = [\n\t        \"primary_count\",\n\t        \"secondary_count\",\n\t        \"primary_minimum\",\n\t        \"secondary_minimum\",\n\t        \"primary_maximum\",\n\t        \"secondary_maximum\",\n\t        \"primary_average\",\n\t        \"secondary_average\",\n\t        \"primary_sum\",\n", "        \"secondary_sum\",\n\t        \"primary_variance\",\n\t        \"secondary_variance\",\n\t        \"max_value_delta\",\n\t        \"bias\",\n\t        \"nash_sutcliffe_efficiency\",\n\t        \"kling_gupta_efficiency\",\n\t        \"mean_error\",\n\t        \"mean_squared_error\",\n\t        \"root_mean_squared_error\",\n", "    ]\n\t    group_by = [\n\t        \"primary_location_id\",\n\t        \"reference_time\"\n\t    ]\n\t    args = {\n\t        \"primary_filepath\": PRIMARY_FILEPATH,\n\t        \"secondary_filepath\": SECONDARY_FILEPATH,\n\t        \"crosswalk_filepath\": CROSSWALK_FILEPATH,\n\t        \"geometry_filepath\": GEOMETRY_FILEPATH,\n", "        \"group_by\": group_by,\n\t        \"order_by\": [\"primary_location_id\"],\n\t        \"include_metrics\": include_metrics,\n\t        \"return_query\": False\n\t    }\n\t    pandas_df = tqk.get_metrics(**args)\n\t    duckdb_df = tqu.get_metrics(**args)\n\t    for m in include_metrics:\n\t        # print(m)\n\t        duckdb_np = duckdb_df[m].to_numpy()\n", "        pandas_np = pandas_df[m].to_numpy()\n\t        assert np.allclose(duckdb_np, pandas_np)\n\tdef test_metric_compare_time_metrics():\n\t    include_metrics = [\n\t        \"primary_max_value_time\",\n\t        \"secondary_max_value_time\",\n\t        \"max_value_timedelta\",\n\t    ]\n\t    group_by = [\n\t        \"primary_location_id\",\n", "        \"reference_time\"\n\t    ]\n\t    args = {\n\t        \"primary_filepath\": PRIMARY_FILEPATH,\n\t        \"secondary_filepath\": SECONDARY_FILEPATH,\n\t        \"crosswalk_filepath\": CROSSWALK_FILEPATH,\n\t        \"geometry_filepath\": GEOMETRY_FILEPATH,\n\t        \"group_by\": group_by,\n\t        \"order_by\": [\"primary_location_id\"],\n\t        \"include_metrics\": include_metrics,\n", "        \"return_query\": False\n\t    }\n\t    pandas_df = tqk.get_metrics(**args)\n\t    duckdb_df = tqu.get_metrics(**args)\n\t    for m in include_metrics:\n\t        duckdb_np = duckdb_df[m].astype('int64').to_numpy()\n\t        pandas_np = pandas_df[m].astype('int64').to_numpy()\n\t        assert np.allclose(duckdb_np, pandas_np)\n\tif __name__ == \"__main__\":\n\t    test_metric_compare_1()\n", "    test_metric_compare_time_metrics()\n\t    pass\n"]}
{"filename": "tests/test_remote_nwm_filelist_generation.py", "chunked_list": ["import pandas as pd\n\tfrom pathlib import Path\n\tfrom teehr.loading.utils_nwm import build_remote_nwm_filelist\n\tdef test_remote_filelist():\n\t    run = \"analysis_assim\"\n\t    output_type = \"channel_rt\"\n\t    t_minus_hours = [2]\n\t    start_date = \"2023-03-18\"\n\t    ingest_days = 1\n\t    component_paths = build_remote_nwm_filelist(\n", "        run,\n\t        output_type,\n\t        start_date,\n\t        ingest_days,\n\t        t_minus_hours,\n\t    )\n\t    test_list_path = Path(\"tests\", \"data\", \"test_remote_list.csv\")\n\t    test_df = pd.read_csv(test_list_path)\n\t    test_list = test_df[\"filename\"].to_list()\n\t    test_list.sort()\n", "    component_paths.sort()\n\t    assert test_list == component_paths\n\tif __name__ == \"__main__\":\n\t    test_remote_filelist()\n\t    pass\n"]}
{"filename": "tests/test_duckdb_filter_formatting.py", "chunked_list": ["from datetime import datetime\n\timport pytest\n\timport teehr.models.queries as tmq\n\tfrom pydantic import ValidationError\n\timport teehr.queries.utils as tqu\n\tdef test_filter_string():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\"=\",\n\t        value=\"123456\"\n", "    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id = '123456'\"\n\tdef test_filter_int():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\"=\",\n\t        value=123456\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n", "    assert filter_str == \"secondary_location_id = 123456\"\n\tdef test_filter_int_gte():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\">=\",\n\t        value=123456\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id >= 123456\"\n\tdef test_filter_int_lt():\n", "    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\"<\",\n\t        value=123456\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id < 123456\"\n\tdef test_filter_float():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n", "        operator=\"=\",\n\t        value=123.456\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id = 123.456\"\n\tdef test_filter_datetime():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"reference_time\",\n\t        operator=\"=\",\n\t        value=datetime(2023, 4, 1, 23, 30)\n", "    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"sf.reference_time = '2023-04-01 23:30:00'\"\n\tdef test_in_filter_string_wrong_operator():\n\t    with pytest.raises(ValidationError):\n\t        filter = tmq.JoinedFilter(\n\t            column=\"secondary_location_id\",\n\t            operator=\"=\",\n\t            value=[\"123456\", \"9876\"]\n\t        )\n", "        tqu._format_filter_item(filter)\n\tdef test_in_filter_string_wrong_value_type():\n\t    with pytest.raises(ValidationError):\n\t        filter = tmq.JoinedFilter(\n\t            column=\"secondary_location_id\",\n\t            operator=\"in\",\n\t            value=\"9876\"\n\t        )\n\t        tqu._format_filter_item(filter)\n\tdef test_in_filter_string():\n", "    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\"in\",\n\t        value=[\"123456\", \"9876\"]\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id in ('123456','9876')\"\n\tdef test_in_filter_int():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n", "        operator=\"in\",\n\t        value=[123456, 9876]\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id in (123456,9876)\"\n\tdef test_in_filter_float():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\"in\",\n\t        value=[123.456, 98.76]\n", "    )\n\t    filter_str = tqu._format_filter_item(filter)\n\t    assert filter_str == \"secondary_location_id in (123.456,98.76)\"\n\tdef test_in_filter_datetime():\n\t    filter = tmq.JoinedFilter(\n\t        column=\"reference_time\",\n\t        operator=\"in\",\n\t        value=[datetime(2023, 4, 1, 23, 30), datetime(2023, 4, 2, 23, 30)]\n\t    )\n\t    filter_str = tqu._format_filter_item(filter)\n", "    assert filter_str == \"sf.reference_time in ('2023-04-01 23:30:00','2023-04-02 23:30:00')\"  # noqa\n\tif __name__ == \"__main__\":\n\t    test_filter_string()\n\t    test_filter_int()\n\t    test_filter_int_gte()\n\t    test_filter_int_lt()\n\t    test_filter_float()\n\t    test_filter_datetime()\n\t    test_in_filter_string_wrong_operator()\n\t    test_in_filter_string_wrong_value_type()\n", "    test_in_filter_string()\n\t    test_in_filter_int()\n\t    test_in_filter_float()\n\t    test_in_filter_datetime()\n\t    pass\n"]}
{"filename": "tests/test_pandas_metric_queries.py", "chunked_list": ["import pandas as pd\n\timport geopandas as gpd\n\t# import pytest\n\t# from pydantic import ValidationError\n\timport teehr.queries.pandas as tqk\n\tfrom pathlib import Path\n\tTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\n\tPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\n\tSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\n\tCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\n", "GEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\tdef test_metric_query_df():\n\t    query_df = tqk.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        group_by=[\"primary_location_id\"],\n\t        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n", "    )\n\t    # print(query_df)\n\t    assert len(query_df) == 3\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_df2():\n\t    query_df = tqk.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        group_by=[\"primary_location_id\", \"reference_time\"],\n", "        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 9\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_filter_df():\n\t    query_df = tqk.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n", "        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        group_by=[\"primary_location_id\", \"reference_time\"],\n\t        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n\t        filters=[\n\t            {\n\t                \"column\": \"primary_location_id\",\n\t                \"operator\": \"=\",\n", "                \"value\": \"gage-A\"\n\t            },\n\t        ]\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 3\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_gdf():\n\t    query_df = tqk.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n", "        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=[\"primary_location_id\"],\n\t        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n\t        include_geometry=True,\n\t    )\n\t    # print(query_df)\n", "    assert len(query_df) == 3\n\t    assert isinstance(query_df, gpd.GeoDataFrame)\n\tdef test_metric_query_df_limit_metrics():\n\t    include_metrics = [\n\t        \"bias\",\n\t        \"root_mean_squared_error\",\n\t        \"nash_sutcliffe_efficiency\",\n\t        \"kling_gupta_efficiency\",\n\t        \"mean_error\",\n\t        \"mean_squared_error\",\n", "        ]\n\t    group_by = [\"primary_location_id\"]\n\t    query_df = tqk.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=group_by,\n\t        order_by=[\"primary_location_id\"],\n\t        include_metrics=include_metrics,\n", "        return_query=False,\n\t        include_geometry=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 3\n\t    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_df_time_metrics():\n\t    include_metrics = [\n\t        \"primary_max_value_time\",\n", "        \"secondary_max_value_time\",\n\t        \"max_value_timedelta\"\n\t    ]\n\t    group_by = [\"primary_location_id\", \"reference_time\"]\n\t    query_df = tqk.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=group_by,\n", "        order_by=[\"primary_location_id\", \"reference_time\"],\n\t        include_metrics=include_metrics,\n\t        return_query=False,\n\t        include_geometry=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 9\n\t    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n\t    assert isinstance(query_df, pd.DataFrame)\n\tif __name__ == \"__main__\":\n", "    # test_metric_query_df()\n\t    # test_metric_query_df2()\n\t    # test_metric_query_filter_df()\n\t    # test_metric_query_gdf()\n\t    # test_metric_query_df_limit_metrics()\n\t    test_metric_query_df_time_metrics()\n\t    pass\n"]}
{"filename": "tests/test_duckdb_metric_filter_formatting.py", "chunked_list": ["from datetime import datetime\n\timport teehr.models.queries as tmq\n\timport teehr.queries.utils as tqu\n\tdef test_multiple_filters():\n\t    filter_1 = tmq.JoinedFilter(\n\t        column=\"secondary_location_id\",\n\t        operator=\"in\",\n\t        value=[\"123456\", \"9876543\"]\n\t    )\n\t    filter_2 = tmq.JoinedFilter(\n", "        column=\"reference_time\",\n\t        operator=\"=\",\n\t        value=datetime(2023, 1, 1, 0, 0, 0)\n\t    )\n\t    filter_str = tqu.filters_to_sql([filter_1, filter_2])\n\t    assert filter_str == \"WHERE secondary_location_id in ('123456','9876543') AND sf.reference_time = '2023-01-01 00:00:00'\"  # noqa\n\tdef test_no_filters():\n\t    filter_str = tqu.filters_to_sql([])\n\t    assert filter_str == \"--no where clause\"\n\tif __name__ == \"__main__\":\n", "    test_multiple_filters()\n\t    test_no_filters()\n\t    pass\n"]}
{"filename": "tests/test_duckdb_timeseries_queries.py", "chunked_list": ["import pandas as pd\n\timport geopandas as gpd\n\timport teehr.queries.duckdb as tqu\n\tfrom pathlib import Path\n\tfrom datetime import datetime\n\tTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\n\tPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\n\tSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\n\tCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\n\tGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n", "def test_joined_timeseries_query_df():\n\t    query_df = tqu.get_joined_timeseries(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        order_by=[\"primary_location_id\", \"lead_time\"],\n\t        return_query=False\n\t    )\n\t    # print(query_df.info())\n", "    assert len(query_df) == 3 * 3 * 24\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_joined_timeseries_query_gdf():\n\t    query_df = tqu.get_joined_timeseries(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        order_by=[\"primary_location_id\", \"lead_time\"],\n\t        return_query=False,\n", "        include_geometry=True,\n\t    )\n\t    # print(query_df.info())\n\t    assert len(query_df) == 3 * 3 * 24\n\t    assert isinstance(query_df, gpd.GeoDataFrame)\n\tdef test_joined_timeseries_query_df_filter():\n\t    query_df = tqu.get_joined_timeseries(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n", "        geometry_filepath=GEOMETRY_FILEPATH,\n\t        order_by=[\"primary_location_id\", \"lead_time\"],\n\t        return_query=False,\n\t        filters=[\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"2022-01-01 00:00:00\"\n\t            },\n\t            {\n", "                \"column\": \"primary_location_id\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"gage-A\"\n\t            },\n\t        ]\n\t    )\n\t    # print(query_df.info())\n\t    assert len(query_df) == 24\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_timeseries_query_df():\n", "    query_df = tqu.get_timeseries(\n\t        timeseries_filepath=PRIMARY_FILEPATH,\n\t        order_by=[\"location_id\"],\n\t        return_query=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 26*3\n\tdef test_timeseries_query_df2():\n\t    query_df = tqu.get_timeseries(\n\t        timeseries_filepath=SECONDARY_FILEPATH,\n", "        order_by=[\"location_id\"],\n\t        return_query=False,\n\t    )\n\t    assert len(query_df) == 24*3*3\n\tdef test_timeseries_query_one_site_df():\n\t    query_df = tqu.get_timeseries(\n\t        timeseries_filepath=PRIMARY_FILEPATH,\n\t        order_by=[\"location_id\"],\n\t        filters=[{\n\t            \"column\": \"location_id\",\n", "            \"operator\": \"=\",\n\t            \"value\": \"gage-C\"\n\t        }],\n\t        return_query=False,\n\t    )\n\t    assert len(query_df) == 26\n\tdef test_timeseries_query_one_site_one_ref_df():\n\t    query_df = tqu.get_timeseries(\n\t        timeseries_filepath=SECONDARY_FILEPATH,\n\t        order_by=[\"value_time\"],\n", "        filters=[\n\t            {\n\t                \"column\": \"location_id\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"fcst-1\"\n\t            },\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": datetime(2022, 1, 1)\n", "            },\n\t        ],\n\t        return_query=False,\n\t    )\n\t    assert len(query_df) == 24\n\tdef test_timeseries_char_query_df():\n\t    query_df = tqu.get_timeseries_chars(\n\t        timeseries_filepath=PRIMARY_FILEPATH,\n\t        group_by=[\"location_id\"],\n\t        order_by=[\"location_id\"],\n", "        return_query=False,\n\t    )\n\t    df = pd.DataFrame(\n\t        {\n\t            'location_id': {0: 'gage-A', 1: 'gage-B', 2: 'gage-C'},\n\t            'count': {0: 26, 1: 26, 2: 26},\n\t            'min': {0: 0.1, 1: 10.1, 2: 0.0},\n\t            'max': {0: 5.0, 1: 15.0, 2: 180.0},\n\t            'average': {\n\t                0: 1.2038461538461542,\n", "                1: 11.203846153846156,\n\t                2: 100.38461538461539\n\t            },\n\t            'sum': {\n\t                0: 31.300000000000008,\n\t                1: 291.30000000000007,\n\t                2: 2610.0\n\t            },\n\t            'variance': {\n\t                0: 1.9788313609467447,\n", "                1: 1.9788313609467456,\n\t                2: 2726.7751479289923\n\t            },\n\t            'max_value_time': {\n\t                0: pd.Timestamp('2022-01-01 15:00:00'),\n\t                1: pd.Timestamp('2022-01-01 15:00:00'),\n\t                2: pd.Timestamp('2022-01-01 06:00:00')\n\t            }\n\t        }\n\t    )\n", "    assert df.equals(query_df)\n\tdef test_timeseries_char_query_df2():\n\t    query_df = tqu.get_timeseries_chars(\n\t        timeseries_filepath=SECONDARY_FILEPATH,\n\t        group_by=[\"location_id\", \"reference_time\"],\n\t        order_by=[\"location_id\"],\n\t        return_query=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 9\n", "def test_timeseries_char_query_filter_df():\n\t    query_df = tqu.get_timeseries_chars(\n\t        timeseries_filepath=SECONDARY_FILEPATH,\n\t        group_by=[\"location_id\"],\n\t        order_by=[\"location_id\"],\n\t        return_query=False,\n\t        filters=[\n\t            {\n\t                \"column\": \"location_id\",\n\t                \"operator\": \"=\",\n", "                \"value\": \"fcst-1\"\n\t            },\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": datetime(2022, 1, 1)\n\t            },\n\t        ],\n\t    )\n\t    assert len(query_df) == 1\n", "if __name__ == \"__main__\":\n\t    test_joined_timeseries_query_df()\n\t    test_joined_timeseries_query_gdf()\n\t    test_joined_timeseries_query_df_filter()\n\t    test_timeseries_query_df()\n\t    test_timeseries_query_df2()\n\t    test_timeseries_query_one_site_one_ref_df()\n\t    test_timeseries_char_query_df()\n\t    test_timeseries_char_query_df2()\n\t    test_timeseries_char_query_filter_df()\n", "    pass\n"]}
{"filename": "tests/test_weight_generation.py", "chunked_list": ["import pandas as pd\n\tfrom pathlib import Path\n\tfrom teehr.loading.generate_weights import generate_weights_file\n\tTEST_DIR = Path(\"tests\", \"data\")\n\tTEMPLATE_FILEPATH = Path(TEST_DIR, \"test_template_grid.nc\")\n\tZONES_FILEPATH = Path(TEST_DIR, \"test_ngen_divides.parquet\")\n\tWEIGHTS_FILEPATH = Path(TEST_DIR, \"test_weights_results.parquet\")\n\tdef test_weights():\n\t    df = generate_weights_file(\n\t        zone_polygon_filepath=ZONES_FILEPATH,\n", "        template_dataset=TEMPLATE_FILEPATH,\n\t        variable_name=\"RAINRATE\",\n\t        output_weights_filepath=None,\n\t        unique_zone_id=\"id\",\n\t    )\n\t    df_test = pd.read_parquet(WEIGHTS_FILEPATH)\n\t    assert df.equals(df_test)\n\tif __name__ == \"__main__\":\n\t    test_weights()\n\t    pass\n"]}
{"filename": "tests/test_duckdb_metric_queries.py", "chunked_list": ["import pandas as pd\n\timport geopandas as gpd\n\timport pytest\n\tfrom pydantic import ValidationError\n\timport teehr.queries.duckdb as tqu\n\tfrom pathlib import Path\n\tTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\n\tPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\n\tSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\n\tCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\n", "GEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\tdef test_metric_query_str():\n\t    query_str = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=[\"primary_location_id\"],\n\t        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n", "        return_query=True\n\t    )\n\t    # print(query_str)\n\t    assert type(query_str) == str\n\tdef test_metric_query_df():\n\t    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        group_by=[\"primary_location_id\"],\n", "        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 3\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_gdf():\n\t    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n", "        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=[\"primary_location_id\"],\n\t        order_by=[\"primary_location_id\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n\t        include_geometry=True,\n\t    )\n\t    # print(query_df)\n", "    assert len(query_df) == 3\n\t    assert isinstance(query_df, gpd.GeoDataFrame)\n\tdef test_metric_query_gdf_2():\n\t    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=[\"primary_location_id\", \"reference_time\"],\n\t        order_by=[\"primary_location_id\"],\n", "        include_metrics=\"all\",\n\t        return_query=False,\n\t        include_geometry=True,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 9\n\t    assert isinstance(query_df, gpd.GeoDataFrame)\n\tdef test_metric_query_gdf_no_geom():\n\t    with pytest.raises(ValidationError):\n\t        tqu.get_metrics(\n", "            primary_filepath=PRIMARY_FILEPATH,\n\t            secondary_filepath=SECONDARY_FILEPATH,\n\t            crosswalk_filepath=CROSSWALK_FILEPATH,\n\t            group_by=[\"primary_location_id\", \"reference_time\"],\n\t            order_by=[\"primary_location_id\"],\n\t            include_metrics=\"all\",\n\t            return_query=False,\n\t            include_geometry=True,\n\t        )\n\tdef test_metric_query_gdf_missing_group_by():\n", "    with pytest.raises(ValidationError):\n\t        tqu.get_metrics(\n\t            primary_filepath=PRIMARY_FILEPATH,\n\t            secondary_filepath=SECONDARY_FILEPATH,\n\t            crosswalk_filepath=CROSSWALK_FILEPATH,\n\t            geometry_filepath=GEOMETRY_FILEPATH,\n\t            group_by=[\"reference_time\"],\n\t            order_by=[\"primary_location_id\"],\n\t            include_metrics=\"all\",\n\t            return_query=False,\n", "            include_geometry=True,\n\t        )\n\tdef test_metric_query_df_2():\n\t    include_metrics = [\n\t        \"primary_count\",\n\t        \"secondary_count\",\n\t        \"primary_minimum\",\n\t        \"secondary_minimum\",\n\t        \"primary_maximum\",\n\t        \"secondary_maximum\",\n", "        \"primary_average\",\n\t        \"secondary_average\",\n\t        \"primary_sum\",\n\t        \"secondary_sum\",\n\t        \"primary_variance\",\n\t        \"secondary_variance\",\n\t        \"max_value_delta\",\n\t        \"bias\",\n\t        \"nash_sutcliffe_efficiency\",\n\t        \"kling_gupta_efficiency\",\n", "        \"mean_error\",\n\t        \"mean_squared_error\",\n\t        \"root_mean_squared_error\",\n\t    ]\n\t    group_by = [\"primary_location_id\"]\n\t    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        group_by=group_by,\n", "        order_by=[\"primary_location_id\"],\n\t        include_metrics=include_metrics,\n\t        return_query=False,\n\t        include_geometry=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 3\n\t    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_df_time_metrics():\n", "    include_metrics = [\n\t        \"primary_max_value_time\",\n\t        \"secondary_max_value_time\",\n\t        \"max_value_timedelta\"\n\t    ]\n\t    group_by = [\"primary_location_id\", \"reference_time\"]\n\t    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n", "        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=group_by,\n\t        order_by=[\"primary_location_id\", \"reference_time\"],\n\t        include_metrics=include_metrics,\n\t        return_query=False,\n\t        include_geometry=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 9\n\t    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n", "    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_df_all():\n\t    group_by = [\"primary_location_id\", \"reference_time\"]\n\t    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=group_by,\n\t        order_by=[\"primary_location_id\", \"reference_time\"],\n", "        include_metrics=\"all\",\n\t        return_query=False,\n\t        include_geometry=False,\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 9\n\t    assert len(query_df.columns) == len(group_by) + 22\n\t    assert isinstance(query_df, pd.DataFrame)\n\tdef test_metric_query_value_time_filter():\n\t    group_by = [\"primary_location_id\", \"reference_time\"]\n", "    query_df = tqu.get_metrics(\n\t        primary_filepath=PRIMARY_FILEPATH,\n\t        secondary_filepath=SECONDARY_FILEPATH,\n\t        crosswalk_filepath=CROSSWALK_FILEPATH,\n\t        geometry_filepath=GEOMETRY_FILEPATH,\n\t        group_by=group_by,\n\t        order_by=[\"primary_location_id\", \"reference_time\"],\n\t        include_metrics=\"all\",\n\t        return_query=False,\n\t        include_geometry=False,\n", "        filters=[\n\t            {\n\t                \"column\": \"value_time\",\n\t                \"operator\": \">=\",\n\t                \"value\": f\"{'2022-01-01 13:00:00'}\"\n\t            },\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \">=\",\n\t                \"value\": f\"{'2022-01-01 02:00:00'}\"\n", "            }\n\t        ],\n\t    )\n\t    # print(query_df)\n\t    assert len(query_df) == 3\n\t    assert query_df[\"primary_count\"].iloc[0] == 13\n\t    assert isinstance(query_df, pd.DataFrame)\n\tif __name__ == \"__main__\":\n\t    test_metric_query_str()\n\t    test_metric_query_df()\n", "    test_metric_query_gdf()\n\t    test_metric_query_gdf_2()\n\t    test_metric_query_gdf_no_geom()\n\t    test_metric_query_gdf_missing_group_by()\n\t    test_metric_query_df_2()\n\t    test_metric_query_df_time_metrics()\n\t    test_metric_query_df_all()\n\t    test_metric_query_value_time_filter()\n\t    pass\n"]}
{"filename": "tests/data/test_study/geo/convert.py", "chunked_list": ["\"\"\"\n\tThis simple script converts `crosswalk.csv` and `gages.geojson` \n\tto parquet files.\n\tTo run this:\n\t```bash\n\t$ cd cd tests/data/test_study/geo/\n\t$ python convert.py\n\t```\n\t\"\"\"\n\timport pandas as pd\n", "import geopandas as gpd\n\tprint(f\"crosswalk.csv\")\n\tdf = pd.read_csv(\"crosswalk.csv\")\n\tdf.to_parquet(\"crosswalk.parquet\")\n\tprint(f\"gages.geojson\")\n\tgdf = gpd.read_file(\"gages.geojson\")\n\tgdf.to_parquet(\"gages.parquet\")"]}
{"filename": "tests/data/test_study/timeseries/convert.py", "chunked_list": ["\"\"\"\n\tThis simple script converts `test_short_fcast.csv` and `test_short_obs.csv` \n\tto parquet files.\n\tTo run this:\n\t```bash\n\t$ cd cd tests/data/test_study/timeseries/\n\t$ python convert.py\n\t```\n\t\"\"\"\n\timport pandas as pd\n", "print(f\"test_short_fcast.csv\")\n\tdf = pd.read_csv(\"test_short_fcast.csv\", parse_dates=['reference_time', 'value_time'])\n\tdf.to_parquet(\"test_short_fcast.parquet\")\n\tprint(df.info())\n\tprint(f\"test_short_obs.csv\")\n\tdf = pd.read_csv(\"test_short_obs.csv\", parse_dates=['reference_time', 'value_time'])\n\tdf.to_parquet(\"test_short_obs.parquet\")\n\tprint(df.info())"]}
{"filename": "study_template/study_1/scripts/script.py", "chunked_list": []}
{"filename": "study_template/study_1/dashboards/utils.py", "chunked_list": []}
{"filename": "src/teehr/__init__.py", "chunked_list": []}
{"filename": "src/teehr/queries/pandas.py", "chunked_list": ["import numpy as np\n\timport pandas as pd\n\timport geopandas as gpd\n\t# import dask.dataframe as dd\n\tfrom hydrotools.metrics import metrics as hm\n\tfrom typing import List, Union\n\timport teehr.models.queries as tmq\n\timport teehr.queries.duckdb as tqu\n\tSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\tdef get_metrics(\n", "    primary_filepath: str,\n\t    secondary_filepath: str,\n\t    crosswalk_filepath: str,\n\t    group_by: List[str],\n\t    order_by: List[str],\n\t    include_metrics: Union[List[tmq.MetricEnum], \"all\"],\n\t    filters: Union[List[dict], None] = None,\n\t    return_query: bool = False,\n\t    geometry_filepath: Union[str, None] = None,\n\t    include_geometry: bool = False,\n", ") -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n\t    \"\"\"Calculate performance metrics using a Pandas or Dask DataFrame.\n\t    Parameters\n\t    ----------\n\t    primary_filepath : str\n\t        File path to the \"observed\" data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    secondary_filepath : str\n\t        File path to the \"forecast\" data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n", "    crosswalk_filepath : str\n\t        File path to single crosswalk file.\n\t    group_by : List[str]\n\t        List of column/field names to group timeseries data by.\n\t        Must provide at least one.\n\t    order_by : List[str]\n\t        List of column/field names to order results by.\n\t        Must provide at least one.\n\t    include_metrics = List[str]\n\t        List of metrics (see below) for allowable list, or \"all\" to return all\n", "    filters : Union[List[dict], None] = None\n\t        List of dictionaries describing the \"where\" clause to limit data that\n\t        is included in metrics.\n\t    return_query: bool = False\n\t        True returns the query string instead of the data\n\t    include_geometry: bool = True\n\t        True joins the geometry to the query results.\n\t        Only works if `primary_location_id`\n\t        is included as a group_by field.\n\t    Returns\n", "    -------\n\t    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\t    Available Metrics\n\t    -----------------------\n\t    Basic\n\t    * primary_count\n\t    * secondary_count\n\t    * primary_minimum\n\t    * secondary_minimum\n\t    * primary_maximum\n", "    * secondary_maximum\n\t    * primary_average\n\t    * secondary_average\n\t    * primary_sum\n\t    * secondary_sum\n\t    * primary_variance\n\t    * secondary_variance\n\t    * max_value_delta\n\t        max(secondary_value) - max(primary_value)\n\t    * bias\n", "        sum(primary_value - secondary_value)/count(*)\n\t    HydroTools Metrics\n\t    * nash_sutcliffe_efficiency\n\t    * kling_gupta_efficiency\n\t    * coefficient_of_extrapolation\n\t    * coefficient_of_persistence\n\t    * mean_error\n\t    * mean_squared_error\n\t    * root_mean_squared_error\n\t    Time-based Metrics\n", "    * primary_max_value_time\n\t    * secondary_max_value_time\n\t    * max_value_timedelta\n\t    Examples\n\t    --------\n\t        group_by = [\"lead_time\", \"primary_location_id\"]\n\t        order_by = [\"lead_time\", \"primary_location_id\"]\n\t        filters = [\n\t            {\n\t                \"column\": \"primary_location_id\",\n", "                \"operator\": \"=\",\n\t                \"value\": \"'123456'\"\n\t            },\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"'2022-01-01 00:00'\"\n\t            },\n\t            {\n\t                \"column\": \"lead_time\",\n", "                \"operator\": \"<=\",\n\t                \"value\": \"'10 days'\"\n\t            }\n\t        ]\n\t        include_metrics=[\"nash_sutcliffe_efficiency\"]\n\t    \"\"\"\n\t    mq = tmq.MetricQuery.parse_obj(\n\t        {\n\t            \"primary_filepath\": primary_filepath,\n\t            \"secondary_filepath\": secondary_filepath,\n", "            \"crosswalk_filepath\": crosswalk_filepath,\n\t            \"group_by\": group_by,\n\t            \"order_by\": order_by,\n\t            \"include_metrics\": include_metrics,\n\t            \"filters\": filters,\n\t            \"return_query\": return_query,\n\t            \"include_geometry\": include_geometry,\n\t            \"geometry_filepath\": geometry_filepath\n\t        }\n\t    )\n", "    if mq.return_query:\n\t        raise ValueError(\n\t            \"`return query` is not a valid option \"\n\t            \"for `dataframe.get_metrics()`.\"\n\t        )\n\t    # This loads all the timeseries in memory\n\t    df = tqu.get_joined_timeseries(\n\t        primary_filepath=mq.primary_filepath,\n\t        secondary_filepath=mq.secondary_filepath,\n\t        crosswalk_filepath=mq.crosswalk_filepath,\n", "        order_by=mq.order_by,\n\t        filters=mq.filters,\n\t        return_query=False,\n\t    )\n\t    # Pandas DataFrame GroupBy approach (works).\n\t    grouped = df.groupby(mq.group_by, as_index=False)\n\t    calculated_metrics = grouped.apply(\n\t        calculate_group_metrics,\n\t        include_metrics=include_metrics\n\t    )\n", "    # Dask DataFrame GroupBy approach (does not work).\n\t    # ddf = dd.from_pandas(df, npartitions=4)\n\t    # calculated_metrics = ddf.groupby(mq.group_by).apply(\n\t    #     calculate_metrics_on_groups,\n\t    #     metrics=[\"primary_count\"],\n\t    #     meta={\"primary_count\": \"int\"}\n\t    # ).compute()\n\t    if mq.include_geometry:\n\t        gdf = gpd.read_parquet(mq.geometry_filepath)\n\t        merged_gdf = gdf.merge(\n", "            calculated_metrics,\n\t            left_on=\"id\",\n\t            right_on=\"primary_location_id\"\n\t        )\n\t        return merged_gdf\n\t    return calculated_metrics\n\tdef calculate_group_metrics(\n\t        group: pd.DataFrame,\n\t        include_metrics: Union[List[str], str]\n\t):\n", "    \"\"\"Calculate metrics on a pd.DataFrame.\n\t    Note this approach to calculating metrics is not as fast as\n\t    `teehr.queries.duckdb.get_metrics()` but is easier to update\n\t    and contains more metrics.  It also serves as the reference\n\t    implementation for the duckdb queries.\n\t    Parameters\n\t    ----------\n\t    group : pd.DataFrame\n\t        Represents a population group to calculate the metrics on\n\t    include_metrics = List[str]\n", "        List of metrics (see below) for allowable list, or \"all\" to\n\t        return all\n\t    Returns\n\t    -------\n\t    calculated_metrics : pd.DataFrame\n\t    Available Metrics\n\t    -----------------------\n\t    Basic\n\t    * primary_count\n\t    * secondary_count\n", "    * primary_minimum\n\t    * secondary_minimum\n\t    * primary_maximum\n\t    * secondary_maximum\n\t    * primary_average\n\t    * secondary_average\n\t    * primary_sum\n\t    * secondary_sum\n\t    * primary_variance\n\t    * secondary_variance\n", "    * max_value_delta\n\t        max(secondary_value) - max(primary_value)\n\t    * bias\n\t        sum(primary_value - secondary_value)/count(*)\n\t    HydroTools Metrics\n\t    * nash_sutcliffe_efficiency\n\t    * kling_gupta_efficiency\n\t    * coefficient_of_extrapolation\n\t    * coefficient_of_persistence\n\t    * mean_error\n", "    * mean_squared_error\n\t    * root_mean_squared_error\n\t    Time-based Metrics\n\t    * primary_max_value_time\n\t    * secondary_max_value_time\n\t    * max_value_timedelta\n\t    \"\"\"\n\t    data = {}\n\t    # Simple Metrics\n\t    if include_metrics == \"all\" or \"primary_count\" in include_metrics:\n", "        data[\"primary_count\"] = len(group[\"primary_value\"])\n\t    if include_metrics == \"all\" or \"secondary_count\" in include_metrics:\n\t        data[\"secondary_count\"] = len(group[\"secondary_value\"])\n\t    if include_metrics == \"all\" or \"primary_minimum\" in include_metrics:\n\t        data[\"primary_minimum\"] = np.min(group[\"primary_value\"])\n\t    if include_metrics == \"all\" or \"secondary_minimum\" in include_metrics:\n\t        data[\"secondary_minimum\"] = np.min(group[\"secondary_value\"])\n\t    if include_metrics == \"all\" or \"primary_maximum\" in include_metrics:\n\t        data[\"primary_maximum\"] = np.max(group[\"primary_value\"])\n\t    if include_metrics == \"all\" or \"secondary_maximum\" in include_metrics:\n", "        data[\"secondary_maximum\"] = np.max(group[\"secondary_value\"])\n\t    if include_metrics == \"all\" or \"primary_average\" in include_metrics:\n\t        data[\"primary_average\"] = np.mean(group[\"primary_value\"])\n\t    if include_metrics == \"all\" or \"secondary_average\" in include_metrics:\n\t        data[\"secondary_average\"] = np.mean(group[\"secondary_value\"])\n\t    if include_metrics == \"all\" or \"primary_sum\" in include_metrics:\n\t        data[\"primary_sum\"] = np.sum(group[\"primary_value\"])\n\t    if include_metrics == \"all\" or \"secondary_sum\" in include_metrics:\n\t        data[\"secondary_sum\"] = np.sum(group[\"secondary_value\"])\n\t    if include_metrics == \"all\" or \"primary_variance\" in include_metrics:\n", "        data[\"primary_variance\"] = np.var(group[\"primary_value\"])\n\t    if include_metrics == \"all\" or \"secondary_variance\" in include_metrics:\n\t        data[\"secondary_variance\"] = np.var(group[\"secondary_value\"])\n\t    if include_metrics == \"all\" or \"bias\" in include_metrics:\n\t        group[\"difference\"] = group[\"primary_value\"] - group[\"secondary_value\"]\n\t        data[\"bias\"] = np.sum(group[\"difference\"])/len(group)\n\t    if include_metrics == \"all\" or \"max_value_delta\" in include_metrics:\n\t        data[\"max_value_delta\"] = (\n\t            np.max(group[\"secondary_value\"])\n\t            - np.max(group[\"primary_value\"])\n", "        )\n\t    # HydroTools Forecast Metrics\n\t    if (\n\t        include_metrics == \"all\"\n\t        or \"nash_sutcliffe_efficiency\" in include_metrics\n\t    ):\n\t        nse = hm.nash_sutcliffe_efficiency(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n", "        data[\"nash_sutcliffe_efficiency\"] = nse\n\t    if include_metrics == \"all\" or \"kling_gupta_efficiency\" in include_metrics:\n\t        kge = hm.kling_gupta_efficiency(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n\t        data[\"kling_gupta_efficiency\"] = kge\n\t    if (\n\t        include_metrics == \"all\"\n\t        or \"coefficient_of_extrapolation\" in include_metrics\n", "    ):\n\t        coe = hm.coefficient_of_extrapolation(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n\t        data[\"coefficient_of_extrapolation\"] = coe\n\t    if (\n\t        include_metrics == \"all\"\n\t        or \"coefficient_of_persistence\" in include_metrics\n\t    ):\n", "        cop = hm.coefficient_of_persistence(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n\t        data[\"coefficient_of_persistence\"] = cop\n\t    if include_metrics == \"all\" or \"mean_error\" in include_metrics:\n\t        me = hm.mean_error(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n", "        data[\"mean_error\"] = me\n\t    if include_metrics == \"all\" or \"mean_squared_error\" in include_metrics:\n\t        mse = hm.mean_squared_error(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n\t        data[\"mean_squared_error\"] = mse\n\t    if (\n\t        include_metrics == \"all\"\n\t        or \"root_mean_squared_error\" in include_metrics\n", "    ):\n\t        rmse = hm.root_mean_squared_error(\n\t            group[\"primary_value\"],\n\t            group[\"secondary_value\"]\n\t        )\n\t        data[\"root_mean_squared_error\"] = rmse\n\t    # Time-based Metrics\n\t    time_indexed_df = group.set_index(\"value_time\")\n\t    if (\n\t        include_metrics == \"all\"\n", "        or \"primary_max_value_time\" in include_metrics\n\t    ):\n\t        pmvt = time_indexed_df[\"primary_value\"].idxmax()\n\t        data[\"primary_max_value_time\"] = pmvt\n\t    if (\n\t        include_metrics == \"all\"\n\t        or \"secondary_max_value_time\" in include_metrics\n\t    ):\n\t        smvt = time_indexed_df[\"secondary_value\"].idxmax()\n\t        data[\"secondary_max_value_time\"] = smvt\n", "    if (\n\t        include_metrics == \"all\"\n\t        or \"max_value_timedelta\" in include_metrics\n\t    ):\n\t        pmvt = time_indexed_df[\"primary_value\"].idxmax()\n\t        smvt = time_indexed_df[\"secondary_value\"].idxmax()\n\t        data[\"max_value_timedelta\"] = smvt - pmvt\n\t    return pd.Series(data)\n"]}
{"filename": "src/teehr/queries/__init__.py", "chunked_list": ["import duckdb"]}
{"filename": "src/teehr/queries/utils.py", "chunked_list": ["import pandas as pd\n\timport geopandas as gpd\n\timport warnings\n\tfrom collections.abc import Iterable\n\tfrom datetime import datetime\n\tfrom typing import List, Union\n\tfrom teehr.models.queries import (\n\t    JoinedFilter,\n\t    MetricQuery,\n\t    JoinedTimeseriesQuery,\n", "    TimeseriesFilter,\n\t)\n\tSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\tdef _get_datetime_list_string(values):\n\t    return [f\"'{v.strftime(SQL_DATETIME_STR_FORMAT)}'\" for v in values]\n\tdef _format_iterable_value(\n\t        values: Iterable[Union[str, int, float, datetime]]\n\t) -> str:\n\t    \"\"\"Returns an SQL formatted string from list of values.\n\t    Parameters\n", "    ----------\n\t    values : Iterable\n\t        Contains values to be formatted as a string for SQL. Only one type of\n\t        value (str, int, float, datetime) should be used. First value in list\n\t        is used to determine value type. Values are not checked for type\n\t        consistency.\n\t    Returns\n\t    -------\n\t    formatted_string : str\n\t    \"\"\"\n", "    # string\n\t    if isinstance(values[0], str):\n\t        return f\"\"\"({\",\".join([f\"'{v}'\" for v in values])})\"\"\"\n\t    # int or float\n\t    elif (\n\t        isinstance(values[0], int)\n\t        or isinstance(values[0], float)\n\t    ):\n\t        return f\"\"\"({\",\".join([f\"{v}\" for v in values])})\"\"\"\n\t    # datetime\n", "    elif isinstance(values[0], datetime):\n\t        return f\"\"\"({\",\".join(_get_datetime_list_string(values))})\"\"\"\n\t    else:\n\t        warnings.warn(\n\t            \"treating value as string because didn't know what else to do.\"\n\t        )\n\t        return f\"\"\"({\",\".join([f\"'{str(v)}'\" for v in values])})\"\"\"\n\tdef _format_filter_item(filter: Union[JoinedFilter, TimeseriesFilter]) -> str:\n\t    \"\"\"Returns an SQL formatted string for single filter object.\n\t    Parameters\n", "    ----------\n\t    filter: models.*Filter\n\t        A single *Filter object.\n\t    Returns\n\t    -------\n\t    formatted_string : str\n\t    \"\"\"\n\t    column = filter.column\n\t    if column == \"value_time\":\n\t        column = \"sf.value_time\"\n", "    if column == \"reference_time\":\n\t        column = \"sf.reference_time\"\n\t    if isinstance(filter.value, str):\n\t        return f\"\"\"{column} {filter.operator} '{filter.value}'\"\"\"\n\t    elif (\n\t        isinstance(filter.value, int)\n\t        or isinstance(filter.value, float)\n\t    ):\n\t        return f\"\"\"{column} {filter.operator} {filter.value}\"\"\"\n\t    elif isinstance(filter.value, datetime):\n", "        dt_str = filter.value.strftime(SQL_DATETIME_STR_FORMAT)\n\t        return f\"\"\"{column} {filter.operator} '{dt_str}'\"\"\"\n\t    elif (\n\t        isinstance(filter.value, Iterable)\n\t        and not isinstance(filter.value, str)\n\t    ):\n\t        value = _format_iterable_value(filter.value)\n\t        return f\"\"\"{column} {filter.operator} {value}\"\"\"\n\t    else:\n\t        warnings.warn(\n", "            \"treating value as string because didn't know what else to do.\"\n\t        )\n\t        return f\"\"\"{column} {filter.operator} '{str(filter.value)}'\"\"\"\n\tdef filters_to_sql(filters: List[JoinedFilter]) -> List[str]:\n\t    \"\"\"Generate SQL where clause string from filters.\n\t    Parameters\n\t    ----------\n\t    filters : List[MetricFilter]\n\t        A list of MetricFilter objects describing the filters.\n\t    Returns\n", "    -------\n\t    where_clause : str\n\t        A where clause formatted string\n\t    \"\"\"\n\t    if len(filters) > 0:\n\t        filter_strs = []\n\t        for f in filters:\n\t            filter_strs.append(_format_filter_item(f))\n\t        qry = f\"\"\"WHERE {f\" AND \".join(filter_strs)}\"\"\"\n\t        return qry\n", "    return \"--no where clause\"\n\tdef geometry_join_clause(\n\t        q: Union[MetricQuery, JoinedTimeseriesQuery]\n\t) -> str:\n\t    \"\"\"Generate the join clause for\"\"\"\n\t    if q.include_geometry:\n\t        return f\"\"\"JOIN read_parquet('{str(q.geometry_filepath)}') gf\n\t            on pf.location_id = gf.id\n\t        \"\"\"\n\t    return \"\"\n", "def geometry_select_clause(\n\t        q: Union[MetricQuery, JoinedTimeseriesQuery]\n\t) -> str:\n\t    if q.include_geometry:\n\t        return \",gf.geometry as geometry\"\n\t    return \"\"\n\tdef metric_geometry_join_clause(\n\t        q: Union[MetricQuery, JoinedTimeseriesQuery]\n\t) -> str:\n\t    \"\"\"Generate the join clause for\"\"\"\n", "    if q.include_geometry:\n\t        return f\"\"\"JOIN read_parquet('{str(q.geometry_filepath)}') gf\n\t            on primary_location_id = gf.id\n\t        \"\"\"\n\t    return \"\"\n\tdef _join_time_on(join: str, join_to: str, join_on: List[str]):\n\t    qry = f\"\"\"\n\t        INNER JOIN {join}\n\t        ON {f\" AND \".join([f\"{join}.{jo} = {join_to}.{jo}\" for jo in join_on])}\n\t        AND {join}.n = 1\n", "    \"\"\"\n\t    return qry\n\tdef _join_on(join: str, join_to: str, join_on: List[str]) -> str:\n\t    qry = f\"\"\"\n\t        INNER JOIN {join}\n\t        ON {f\" AND \".join([f\"{join}.{jo} = {join_to}.{jo}\" for jo in join_on])}\n\t    \"\"\"\n\t    return qry\n\tdef _nse_cte(mq: MetricQuery) -> str:\n\t    if (\n", "        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return f\"\"\"\n\t        ,nse AS (\n\t            SELECT\n\t                {\",\".join(mq.group_by)}\n\t                , value_time\n\t                , pow(\n\t                    primary_value - secondary_value, 2\n", "                ) as primary_minus_secondary_squared\n\t                , pow(\n\t                    primary_value\n\t                    - avg(primary_value)\n\t                    OVER(PARTITION BY {\",\".join(mq.group_by)}), 2\n\t                ) as primary_minus_primary_mean_squared\n\t            FROM joined\n\t        )\n\t        \"\"\"\n\t    return \"\"\n", "def _pmxt_cte(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_max_value_time\" in mq.include_metrics\n\t        or \"max_value_timedelta\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return f\"\"\"\n\t            , pmxt AS (\n\t                SELECT\n\t                    {\",\".join(mq.group_by)}\n", "                    , primary_value as value\n\t                    , value_time\n\t                    , ROW_NUMBER() OVER(\n\t                        PARTITION BY {\",\".join(mq.group_by)}\n\t                        ORDER BY value DESC, value_time\n\t                    ) as n\n\t                FROM joined\n\t            )\n\t        \"\"\"\n\t    return \"\"\n", "def _smxt_cte(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_max_value_time\" in mq.include_metrics\n\t        or \"max_value_timedelta\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return f\"\"\"\n\t            , smxt AS (\n\t            SELECT\n\t                {\",\".join(mq.group_by)}\n", "                , secondary_value as value\n\t                , value_time\n\t                , ROW_NUMBER() OVER(\n\t                    PARTITION BY {\",\".join(mq.group_by)}\n\t                    ORDER BY value DESC, value_time\n\t                ) as n\n\t            FROM joined\n\t        )\n\t        \"\"\"\n\t    return \"\"\n", "def _join_nse_cte(mq: MetricQuery) -> str:\n\t    if (\n\t        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return f\"\"\"\n\t            {_join_on(join=\"nse\", join_to=\"joined\", join_on=mq.group_by)}\n\t            AND nse.value_time = joined.value_time\n\t        \"\"\"\n\t    return \"\"\n", "def _select_max_value_timedelta(mq: MetricQuery) -> str:\n\t    if (\n\t        \"max_value_timedelta\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", smxt.value_time - pmxt.value_time as max_value_timedelta\"\"\"\n\t    return \"\"\n\tdef _select_secondary_max_value_time(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_max_value_time\" in mq.include_metrics\n", "        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", smxt.value_time as secondary_max_value_time\"\"\"\n\t    return \"\"\n\tdef _select_primary_max_value_time(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_max_value_time\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", pmxt.value_time as primary_max_value_time\"\"\"\n", "    return \"\"\n\tdef _select_root_mean_squared_error(mq: MetricQuery) -> str:\n\t    if (\n\t        \"root_mean_squared_error\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", sqrt(sum(power(absolute_difference, 2))/count(*))\n\t            as root_mean_squared_error\n\t        \"\"\"\n\t    return \"\"\n", "def _select_mean_squared_error(mq: MetricQuery) -> str:\n\t    if (\n\t        \"mean_squared_error\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", sum(power(absolute_difference, 2))/count(*)\n\t            as mean_squared_error\n\t        \"\"\"\n\t    return \"\"\n\tdef _select_mean_error(mq: MetricQuery) -> str:\n", "    if (\n\t        \"mean_error\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", sum(absolute_difference)/count(*) as mean_error\"\"\"\n\t    return \"\"\n\tdef _select_kling_gupta_efficiency(mq: MetricQuery) -> str:\n\t    if (\n\t        \"kling_gupta_efficiency\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n", "    ):\n\t        return \"\"\", 1 - sqrt(\n\t            pow(corr(secondary_value, primary_value) - 1, 2)\n\t            + pow(stddev(secondary_value)\n\t                / stddev(primary_value) - 1, 2)\n\t            + pow(avg(secondary_value) / avg(primary_value) - 1, 2)\n\t        ) AS kling_gupta_efficiency\n\t        \"\"\"\n\t    return \"\"\n\tdef _select_nash_sutcliffe_efficiency(mq: MetricQuery) -> str:\n", "    if (\n\t        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", 1 - (\n\t                sum(nse.primary_minus_secondary_squared)\n\t                /sum(nse.primary_minus_primary_mean_squared)\n\t            ) as nash_sutcliffe_efficiency\n\t        \"\"\"\n\t    return \"\"\n", "def _select_bias(mq: MetricQuery) -> str:\n\t    if (\n\t        \"bias\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", sum(primary_value - secondary_value)/count(*) as bias\"\"\"\n\t    return \"\"\n\tdef _select_max_value_delta(mq: MetricQuery) -> str:\n\t    if (\n\t        \"max_value_delta\" in mq.include_metrics\n", "        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", max(secondary_value) - max(primary_value)\n\t            as max_value_delta\n\t        \"\"\"\n\t    return \"\"\n\tdef _select_primary_count(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_count\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n", "    ):\n\t        return \"\"\", count(primary_value) as primary_count\"\"\"\n\t    return \"\"\n\tdef _select_secondary_count(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_count\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", count(secondary_value) as secondary_count\"\"\"\n\t    return \"\"\n", "def _select_primary_minimum(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_minimum\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", min(primary_value) as primary_minimum\"\"\"\n\t    return \"\"\n\tdef _select_secondary_minimum(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_minimum\" in mq.include_metrics\n", "        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", min(secondary_value) as secondary_minimum\"\"\"\n\t    return \"\"\n\tdef _select_primary_maximum(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_maximum\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", max(primary_value) as primary_maximum\"\"\"\n", "    return \"\"\n\tdef _select_secondary_maximum(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_maximum\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", max(secondary_value) as secondary_maximum\"\"\"\n\t    return \"\"\n\tdef _select_primary_average(mq: MetricQuery) -> str:\n\t    if (\n", "        \"primary_average\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", avg(primary_value) as primary_average\"\"\"\n\t    return \"\"\n\tdef _select_secondary_average(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_average\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n", "        return \"\"\", avg(secondary_value) as secondary_average\"\"\"\n\t    return \"\"\n\tdef _select_primary_sum(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_sum\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", sum(primary_value) as primary_sum\"\"\"\n\t    return \"\"\n\tdef _select_secondary_sum(mq: MetricQuery) -> str:\n", "    if (\n\t        \"secondary_sum\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", sum(secondary_value) as secondary_sum\"\"\"\n\t    return \"\"\n\tdef _select_primary_variance(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_variance\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n", "    ):\n\t        return \"\"\", var_pop(primary_value) as primary_variance\"\"\"\n\t    return \"\"\n\tdef _select_secondary_variance(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_variance\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return \"\"\", var_pop(secondary_value) as secondary_variance\"\"\"\n\t    return \"\"\n", "def _join_primary_join_max_time(mq: MetricQuery) -> str:\n\t    if (\n\t        \"primary_max_value_time\" in mq.include_metrics\n\t        or \"max_value_timedelta\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return _join_time_on(\n\t            join=\"pmxt\",\n\t            join_to=\"metrics\",\n\t            join_on=mq.group_by\n", "        )\n\t    return \"\"\n\tdef _join_secondary_join_max_time(mq: MetricQuery) -> str:\n\t    if (\n\t        \"secondary_max_value_time\" in mq.include_metrics\n\t        or \"max_value_timedelta\" in mq.include_metrics\n\t        or mq.include_metrics == \"all\"\n\t    ):\n\t        return _join_time_on(\n\t            join=\"smxt\",\n", "            join_to=\"metrics\",\n\t            join_on=mq.group_by\n\t        )\n\t    return \"\"\n\tdef df_to_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n\t    \"\"\"Convert pd.DataFrame to gpd.GeoDataFrame.\n\t    When the `geometry` column is read from a parquet file using DuckBD\n\t    it is a bytearray in the resulting pd.DataFrame.  The `geometry` needs\n\t    to be convert to bytes before GeoPandas can work with it.  This function\n\t    does that.\n", "    Parameters\n\t    ----------\n\t    df : pd.DataFrame\n\t        DataFrame with a `geometry` column that has geometry stored as\n\t        a bytearray.\n\t    Returns\n\t    -------\n\t    gdf : gpd.GeoDataFrame\n\t        GeoDataFrame with a valid `geometry` column.\n\t    \"\"\"\n", "    df[\"geometry\"] = gpd.GeoSeries.from_wkb(\n\t        df[\"geometry\"].apply(lambda x: bytes(x))\n\t        )\n\t    return gpd.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=\"geometry\")\n\tdef remove_empty_lines(text: str) -> str:\n\t    \"\"\"Remove empty lines from string.\"\"\"\n\t    return \"\".join([s for s in text.splitlines(True) if s.strip()])"]}
{"filename": "src/teehr/queries/duckdb.py", "chunked_list": ["import duckdb\n\timport pandas as pd\n\timport geopandas as gpd\n\tfrom typing import List, Union\n\tfrom teehr.models.queries import (\n\t    MetricQuery,\n\t    JoinedTimeseriesQuery,\n\t    TimeseriesQuery,\n\t    TimeseriesCharQuery,\n\t)\n", "import teehr.queries.utils as tqu\n\timport teehr.models.queries as tmq\n\tSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\tdef get_metrics(\n\t    primary_filepath: str,\n\t    secondary_filepath: str,\n\t    crosswalk_filepath: str,\n\t    group_by: List[str],\n\t    order_by: List[str],\n\t    include_metrics: Union[List[tmq.MetricEnum], \"all\"],\n", "    filters: Union[List[dict], None] = None,\n\t    return_query: bool = False,\n\t    geometry_filepath: Union[str, None] = None,\n\t    include_geometry: bool = False,\n\t) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n\t    \"\"\"Calculate performance metrics using database queries.\n\t    Parameters\n\t    ----------\n\t    primary_filepath : str\n\t        File path to the \"observed\" data.  String must include path to file(s)\n", "        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    secondary_filepath : str\n\t        File path to the \"forecast\" data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    crosswalk_filepath : str\n\t        File path to single crosswalk file.\n\t    group_by : List[str]\n\t        List of column/field names to group timeseries data by.\n\t        Must provide at least one.\n\t    order_by : List[str]\n", "        List of column/field names to order results by.\n\t        Must provide at least one.\n\t    include_metrics = List[str]\n\t        List of metrics (see below) for allowable list, or \"all\" to return all\n\t        Placeholder, currently ignored -> returns \"all\"\n\t    filters : Union[List[dict], None] = None\n\t        List of dictionaries describing the \"where\" clause to limit data that\n\t        is included in metrics.\n\t    return_query: bool = False\n\t        True returns the query string instead of the data\n", "    include_geometry: bool = True\n\t        True joins the geometry to the query results.\n\t        Only works if `primary_location_id`\n\t        is included as a group_by field.\n\t    Returns\n\t    -------\n\t    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\t    Filter, Order By and Group By Fields\n\t    -----------------------------------\n\t    * reference_time\n", "    * primary_location_id\n\t    * secondary_location_id\n\t    * primary_value\n\t    * secondary_value\n\t    * value_time\n\t    * configuration\n\t    * measurement_unit\n\t    * variable_name\n\t    * lead_time\n\t    Available Metrics\n", "    -----------------------\n\t    Basic\n\t    * primary_count\n\t    * secondary_count\n\t    * primary_minimum\n\t    * secondary_minimum\n\t    * primary_maximum\n\t    * secondary_maximum\n\t    * primary_average\n\t    * secondary_average\n", "    * primary_sum\n\t    * secondary_sum\n\t    * primary_variance\n\t    * secondary_variance\n\t    * max_value_delta\n\t        max(secondary_value) - max(primary_value)\n\t    * bias\n\t        sum(primary_value - secondary_value)/count(*)\n\t    HydroTools Metrics\n\t    * nash_sutcliffe_efficiency\n", "    * kling_gupta_efficiency\n\t    * coefficient_of_extrapolation\n\t    * coefficient_of_persistence\n\t    * mean_error\n\t    * mean_squared_error\n\t    * root_mean_squared_error\n\t    Time-based Metrics\n\t    * primary_max_value_time\n\t    * secondary_max_value_time\n\t    * max_value_timedelta\n", "    Examples:\n\t        group_by = [\"lead_time\", \"primary_location_id\"]\n\t        order_by = [\"lead_time\", \"primary_location_id\"]\n\t        filters = [\n\t            {\n\t                \"column\": \"primary_location_id\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"'123456'\"\n\t            },\n\t            {\n", "                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"'2022-01-01 00:00'\"\n\t            },\n\t            {\n\t                \"column\": \"lead_time\",\n\t                \"operator\": \"<=\",\n\t                \"value\": \"'10 days'\"\n\t            }\n\t        ]\n", "    \"\"\"\n\t    mq = MetricQuery.parse_obj(\n\t        {\n\t            \"primary_filepath\": primary_filepath,\n\t            \"secondary_filepath\": secondary_filepath,\n\t            \"crosswalk_filepath\": crosswalk_filepath,\n\t            \"group_by\": group_by,\n\t            \"order_by\": order_by,\n\t            \"include_metrics\": include_metrics,\n\t            \"filters\": filters,\n", "            \"return_query\": return_query,\n\t            \"include_geometry\": include_geometry,\n\t            \"geometry_filepath\": geometry_filepath\n\t        }\n\t    )\n\t    query = f\"\"\"\n\t        WITH joined as (\n\t            SELECT\n\t                sf.reference_time\n\t                , sf.value_time as value_time\n", "                , sf.location_id as secondary_location_id\n\t                , sf.value as secondary_value\n\t                , sf.configuration\n\t                , sf.measurement_unit\n\t                , sf.variable_name\n\t                , pf.value as primary_value\n\t                , pf.location_id as primary_location_id\n\t                , sf.value_time - sf.reference_time as lead_time\n\t                , abs(pf.value - sf.value) as absolute_difference\n\t            FROM read_parquet('{str(mq.secondary_filepath)}') sf\n", "            JOIN read_parquet('{str(mq.crosswalk_filepath)}') cf\n\t                on cf.secondary_location_id = sf.location_id\n\t            JOIN read_parquet('{str(mq.primary_filepath)}') pf\n\t                on cf.primary_location_id = pf.location_id\n\t                and sf.value_time = pf.value_time\n\t                and sf.measurement_unit = pf.measurement_unit\n\t                and sf.variable_name = pf.variable_name\n\t            {tqu.filters_to_sql(mq.filters)}\n\t        )\n\t        {tqu._nse_cte(mq)}\n", "        {tqu._pmxt_cte(mq)}\n\t        {tqu._smxt_cte(mq)}\n\t        , metrics AS (\n\t            SELECT\n\t                {\",\".join([f\"joined.{gb}\" for gb in mq.group_by])}\n\t                {tqu._select_primary_count(mq)}\n\t                {tqu._select_secondary_count(mq)}\n\t                {tqu._select_primary_minimum(mq)}\n\t                {tqu._select_secondary_minimum(mq)}\n\t                {tqu._select_primary_maximum(mq)}\n", "                {tqu._select_secondary_maximum(mq)}\n\t                {tqu._select_primary_average(mq)}\n\t                {tqu._select_secondary_average(mq)}\n\t                {tqu._select_primary_sum(mq)}\n\t                {tqu._select_secondary_sum(mq)}\n\t                {tqu._select_primary_variance(mq)}\n\t                {tqu._select_secondary_variance(mq)}\n\t                {tqu._select_max_value_delta(mq)}\n\t                {tqu._select_bias(mq)}\n\t                {tqu._select_nash_sutcliffe_efficiency(mq)}\n", "                {tqu._select_kling_gupta_efficiency(mq)}\n\t                {tqu._select_mean_error(mq)}\n\t                {tqu._select_mean_squared_error(mq)}\n\t                {tqu._select_root_mean_squared_error(mq)}\n\t            FROM\n\t                joined\n\t            {tqu._join_nse_cte(mq)}\n\t            GROUP BY\n\t                {\",\".join([f\"joined.{gb}\" for gb in mq.group_by])}\n\t        )\n", "        SELECT\n\t            metrics.*\n\t            {tqu._select_primary_max_value_time(mq)}\n\t            {tqu._select_secondary_max_value_time(mq)}\n\t            {tqu._select_max_value_timedelta(mq)}\n\t            {tqu.geometry_select_clause(mq)}\n\t        FROM metrics\n\t        {tqu.metric_geometry_join_clause(mq)}\n\t        {tqu._join_primary_join_max_time(mq)}\n\t        {tqu._join_secondary_join_max_time(mq)}\n", "        ORDER BY\n\t            {\",\".join([f\"metrics.{gb}\" for gb in mq.group_by])}\n\t    ;\"\"\"\n\t    if mq.return_query:\n\t        return tqu.remove_empty_lines(query)\n\t    df = duckdb.query(query).to_df()\n\t    if mq.include_geometry:\n\t        return tqu.df_to_gdf(df)\n\t    return df\n\tdef get_joined_timeseries(\n", "    primary_filepath: str,\n\t    secondary_filepath: str,\n\t    crosswalk_filepath: str,\n\t    order_by: List[str],\n\t    filters: Union[List[dict], None] = None,\n\t    return_query: bool = False,\n\t    geometry_filepath: Union[str, None] = None,\n\t    include_geometry: bool = False,\n\t) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n\t    \"\"\"Retrieve joined timeseries using database query.\n", "    Parameters\n\t    ----------\n\t    primary_filepath : str\n\t        File path to the \"observed\" data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    secondary_filepath : str\n\t        File path to the \"forecast\" data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    crosswalk_filepath : str\n\t        File path to single crosswalk file.\n", "    order_by : List[str]\n\t        List of column/field names to order results by.\n\t        Must provide at least one.\n\t    filters : Union[List[dict], None] = None\n\t        List of dictionaries describing the \"where\" clause to limit data that\n\t        is included in metrics.\n\t    return_query: bool = False\n\t        True returns the query string instead of the data\n\t    include_geometry: bool = True\n\t        True joins the geometry to the query results.\n", "        Only works if `primary_location_id`.\n\t        is included as a group_by field.\n\t    Returns\n\t    -------\n\t    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\t    Filter and Order By Fields\n\t    --------------------------\n\t    * reference_time\n\t    * primary_location_id\n\t    * secondary_location_id\n", "    * primary_value\n\t    * secondary_value\n\t    * value_time\n\t    * configuration\n\t    * measurement_unit\n\t    * variable_name\n\t    * lead_time\n\t    Examples:\n\t        order_by = [\"lead_time\", \"primary_location_id\"]\n\t        filters = [\n", "            {\n\t                \"column\": \"primary_location_id\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"'123456'\"\n\t            },\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"'2022-01-01 00:00'\"\n\t            },\n", "            {\n\t                \"column\": \"lead_time\",\n\t                \"operator\": \"<=\",\n\t                \"value\": \"'10 days'\"\n\t            }\n\t        ]\n\t    \"\"\"\n\t    jtq = JoinedTimeseriesQuery.parse_obj(\n\t        {\n\t            \"primary_filepath\": primary_filepath,\n", "            \"secondary_filepath\": secondary_filepath,\n\t            \"crosswalk_filepath\": crosswalk_filepath,\n\t            \"order_by\": order_by,\n\t            \"filters\": filters,\n\t            \"return_query\": return_query,\n\t            \"include_geometry\": include_geometry,\n\t            \"geometry_filepath\": geometry_filepath\n\t        }\n\t    )\n\t    query = f\"\"\"\n", "        WITH joined as (\n\t            SELECT\n\t                sf.reference_time,\n\t                sf.value_time,\n\t                sf.location_id as secondary_location_id,\n\t                sf.value as secondary_value,\n\t                sf.configuration,\n\t                sf.measurement_unit,\n\t                sf.variable_name,\n\t                pf.value as primary_value,\n", "                pf.location_id as primary_location_id,\n\t                sf.value_time - sf.reference_time as lead_time\n\t                {tqu.geometry_select_clause(jtq)}\n\t            FROM read_parquet('{str(jtq.secondary_filepath)}') sf\n\t            JOIN read_parquet('{str(jtq.crosswalk_filepath)}') cf\n\t                on cf.secondary_location_id = sf.location_id\n\t            JOIN read_parquet('{str(jtq.primary_filepath)}') pf\n\t                on cf.primary_location_id = pf.location_id\n\t                and sf.value_time = pf.value_time\n\t                and sf.measurement_unit = pf.measurement_unit\n", "                and sf.variable_name = pf.variable_name\n\t            {tqu.geometry_join_clause(jtq)}\n\t            {tqu.filters_to_sql(jtq.filters)}\n\t        )\n\t        SELECT\n\t            *\n\t        FROM\n\t            joined\n\t        ORDER BY\n\t            {\",\".join(jtq.order_by)}\n", "    ;\"\"\"\n\t    if jtq.return_query:\n\t        return tqu.remove_empty_lines(query)\n\t    df = duckdb.query(query).to_df()\n\t    df[\"primary_location_id\"] = df[\"primary_location_id\"].astype(\"category\")\n\t    df[\"secondary_location_id\"] = df[\"secondary_location_id\"].astype(\"category\")  # noqa\n\t    df[\"configuration\"] = df[\"configuration\"].astype(\"category\")\n\t    df[\"measurement_unit\"] = df[\"measurement_unit\"].astype(\"category\")\n\t    df[\"variable_name\"] = df[\"variable_name\"].astype(\"category\")\n\t    if jtq.include_geometry:\n", "        return tqu.df_to_gdf(df)\n\t    return df\n\tdef get_timeseries(\n\t    timeseries_filepath: str,\n\t    order_by: List[str],\n\t    filters: Union[List[dict], None] = None,\n\t    return_query: bool = False,\n\t) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n\t    \"\"\"Retrieve joined timeseries using database query.\n\t    Parameters\n", "    ----------\n\t    timeseries_filepath : str\n\t        File path to the timeseries data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    order_by : List[str]\n\t        List of column/field names to order results by.\n\t        Must provide at least one.\n\t    filters : Union[List[dict], None] = None\n\t        List of dictionaries describing the \"where\" clause to limit data that\n\t        is included in metrics.\n", "    return_query: bool = False\n\t        True returns the query string instead of the data\n\t    Returns\n\t    -------\n\t    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\t    Filter and Order By Fields\n\t    --------------------------\n\t    * value_time\n\t    * location_id\n\t    * value\n", "    * measurement_unit\n\t    * reference_time\n\t    * configuration\n\t    * variable_name\n\t    Examples:\n\t        order_by = [\"lead_time\", \"primary_location_id\"]\n\t        filters = [\n\t            {\n\t                \"column\": \"location_id\",\n\t                \"operator\": \"in\",\n", "                \"value\": [12345, 54321]\n\t            },\n\t        ]\n\t    \"\"\"\n\t    tq = TimeseriesQuery.parse_obj(\n\t        {\n\t            \"timeseries_filepath\": timeseries_filepath,\n\t            \"order_by\": order_by,\n\t            \"filters\": filters,\n\t            \"return_query\": return_query\n", "        }\n\t    )\n\t    query = f\"\"\"\n\t        WITH joined as (\n\t            SELECT\n\t                sf.reference_time,\n\t                sf.value_time,\n\t                sf.location_id,\n\t                sf.value,\n\t                sf.configuration,\n", "                sf.measurement_unit,\n\t                sf.variable_name\n\t            FROM\n\t                read_parquet('{str(tq.timeseries_filepath)}') sf\n\t            {tqu.filters_to_sql(tq.filters)}\n\t        )\n\t        SELECT * FROM\n\t            joined\n\t        ORDER BY\n\t            {\",\".join(tq.order_by)}\n", "    ;\"\"\"\n\t    if tq.return_query:\n\t        return tqu.remove_empty_lines(query)\n\t    df = duckdb.query(query).to_df()\n\t    df[\"location_id\"] = df[\"location_id\"].astype(\"category\")\n\t    df[\"configuration\"] = df[\"configuration\"].astype(\"category\")\n\t    df[\"measurement_unit\"] = df[\"measurement_unit\"].astype(\"category\")\n\t    df[\"variable_name\"] = df[\"variable_name\"].astype(\"category\")\n\t    return df\n\tdef get_timeseries_chars(\n", "    timeseries_filepath: str,\n\t    group_by: list[str],\n\t    order_by: List[str],\n\t    filters: Union[List[dict], None] = None,\n\t    return_query: bool = False,\n\t) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n\t    \"\"\"Retrieve joined timeseries using database query.\n\t    Parameters\n\t    ----------\n\t    timeseries_filepath : str\n", "        File path to the \"observed\" data.  String must include path to file(s)\n\t        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n\t    order_by : List[str]\n\t        List of column/field names to order results by.\n\t        Must provide at least one.\n\t    group_by : List[str]\n\t        List of column/field names to group timeseries data by.\n\t        Must provide at least one.\n\t    filters : Union[List[dict], None] = None\n\t        List of dictionaries describing the \"where\" clause to limit data that\n", "        is included in metrics.\n\t    return_query: bool = False\n\t        True returns the query string instead of the data\n\t    Returns\n\t    -------\n\t    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\t    Filter, Group By and Order By Fields\n\t    ------------------------------------\n\t    * value_time\n\t    * location_id\n", "    * value\n\t    * measurement_unit\n\t    * reference_time\n\t    * configuration\n\t    * variable_name\n\t    Examples:\n\t        order_by = [\"lead_time\", \"primary_location_id\"]\n\t        filters = [\n\t            {\n\t                \"column\": \"primary_location_id\",\n", "                \"operator\": \"=\",\n\t                \"value\": \"'123456'\"\n\t            },\n\t            {\n\t                \"column\": \"reference_time\",\n\t                \"operator\": \"=\",\n\t                \"value\": \"'2022-01-01 00:00'\"\n\t            },\n\t            {\n\t                \"column\": \"lead_time\",\n", "                \"operator\": \"<=\",\n\t                \"value\": \"'10 days'\"\n\t            }\n\t        ]\n\t    \"\"\"\n\t    tcq = TimeseriesCharQuery.parse_obj(\n\t        {\n\t            \"timeseries_filepath\": timeseries_filepath,\n\t            \"order_by\": order_by,\n\t            \"group_by\": group_by,\n", "            \"filters\": filters,\n\t            \"return_query\": return_query\n\t        }\n\t    )\n\t    join_max_time_on = tqu._join_time_on(\n\t        join=\"mxt\",\n\t        join_to=\"chars\",\n\t        join_on=tcq.group_by\n\t    )\n\t    query = f\"\"\"\n", "        WITH fts AS (\n\t            SELECT sf.* FROM\n\t            read_parquet('{str(tcq.timeseries_filepath)}') sf\n\t            {tqu.filters_to_sql(tcq.filters)}\n\t        ),\n\t        mxt AS (\n\t            SELECT\n\t                {\",\".join(tcq.group_by)}\n\t                , value\n\t                , value_time\n", "                , ROW_NUMBER() OVER(\n\t                    PARTITION BY {\",\".join(tcq.group_by)}\n\t                    ORDER BY value DESC, value_time\n\t                ) as n\n\t            FROM fts\n\t        ),\n\t        chars AS (\n\t            SELECT\n\t                {\",\".join(tcq.group_by)}\n\t                ,count(fts.value) as count\n", "                ,min(fts.value) as min\n\t                ,max(fts.value) as max\n\t                ,avg(fts.value) as average\n\t                ,sum(fts.value) as sum\n\t                ,var_pop(fts.value) as variance\n\t            FROM\n\t                fts\n\t            GROUP BY\n\t                {\",\".join(tcq.group_by)}\n\t            ORDER BY\n", "                {\",\".join(tcq.order_by)}\n\t        )\n\t        SELECT\n\t            chars.*\n\t            ,mxt.value_time as max_value_time\n\t        FROM chars\n\t        {join_max_time_on}\n\t    ;\"\"\"\n\t    if tcq.return_query:\n\t        return tqu.remove_empty_lines(query)\n", "    df = duckdb.query(query).to_df()\n\t    return df\n"]}
{"filename": "src/teehr/models/__init__.py", "chunked_list": []}
{"filename": "src/teehr/models/loading.py", "chunked_list": ["from enum import Enum\n\tclass ChunkByEnum(str, Enum):\n\t    day = \"day\"\n\t    location_id = \"location_id\"\n"]}
{"filename": "src/teehr/models/queries.py", "chunked_list": ["from collections.abc import Iterable\n\tfrom datetime import datetime\n\tfrom enum import Enum\n\tfrom typing import List, Optional, Union\n\tfrom pydantic import BaseModel as PydanticBaseModel\n\tfrom pydantic import validator\n\tfrom pathlib import Path\n\tclass BaseModel(PydanticBaseModel):\n\t    class Config:\n\t        arbitrary_types_allowed = True\n", "        smart_union = True\n\tclass FilterOperatorEnum(str, Enum):\n\t    eq = \"=\"\n\t    gt = \">\"\n\t    lt = \"<\"\n\t    gte = \">=\"\n\t    lte = \"<=\"\n\t    islike = \"like\"\n\t    isin = \"in\"\n\tclass MetricEnum(str, Enum):\n", "    primary_count = \"primary_count\"\n\t    secondary_count = \"secondary_count\"\n\t    primary_minimum = \"primary_minimum\"\n\t    secondary_minimum = \"secondary_minimum\"\n\t    primary_maximum = \"primary_maximum\"\n\t    secondary_maximum = \"secondary_maximum\"\n\t    primary_average = \"primary_average\"\n\t    secondary_average = \"secondary_average\"\n\t    primary_sum = \"primary_sum\"\n\t    secondary_sum = \"secondary_sum\"\n", "    primary_variance = \"primary_variance\"\n\t    secondary_variance = \"secondary_variance\"\n\t    max_value_delta = \"max_value_delta\"\n\t    bias = \"bias\"\n\t    nash_sutcliffe_efficiency = \"nash_sutcliffe_efficiency\"\n\t    kling_gupta_efficiency = \"kling_gupta_efficiency\"\n\t    mean_error = \"mean_error\"\n\t    mean_squared_error = \"mean_squared_error\"\n\t    root_mean_squared_error = \"root_mean_squared_error\"\n\t    primary_max_value_time = \"primary_max_value_time\"\n", "    secondary_max_value_time = \"secondary_max_value_time\"\n\t    max_value_timedelta = \"max_value_timedelta\"\n\tclass JoinedFilterFieldEnum(str, Enum):\n\t    value_time = \"value_time\"\n\t    reference_time = \"reference_time\"\n\t    secondary_location_id = \"secondary_location_id\"\n\t    secondary_value = \"secondary_value\"\n\t    configuration = \"configuration\"\n\t    measurement_unit = \"measurement_unit\"\n\t    variable_name = \"variable_name\"\n", "    primary_value = \"primary_value\"\n\t    primary_location_id = \"primary_location_id\"\n\t    lead_time = \"lead_time\"\n\t    geometry = \"geometry\"\n\tclass TimeseriesFilterFieldEnum(str, Enum):\n\t    value_time = \"value_time\"\n\t    reference_time = \"reference_time\"\n\t    location_id = \"location_id\"\n\t    value = \"value\"\n\t    configuration = \"configuration\"\n", "    measurement_unit = \"measurement_unit\"\n\t    variable_name = \"variable_name\"\n\t    lead_time = \"lead_time\"\n\t    geometry = \"geometry\"\n\tclass ChunkByEnum(str, Enum):\n\t    day = \"day\"\n\t    site = \"site\"\n\tclass JoinedFilter(BaseModel):\n\t    column: JoinedFilterFieldEnum\n\t    operator: FilterOperatorEnum\n", "    value: Union[\n\t        str, int, float, datetime,\n\t        List[Union[str, int, float, datetime]]\n\t    ]\n\t    def is_iterable_not_str(obj):\n\t        if isinstance(obj, Iterable) and not isinstance(obj, str):\n\t            return True\n\t        return False\n\t    @validator('value')\n\t    def in_operator_must_have_iterable(cls, v, values):\n", "        if cls.is_iterable_not_str(v) and values[\"operator\"] != \"in\":\n\t            raise ValueError(\"iterable value must be used with 'in' operator\")\n\t        if values[\"operator\"] == \"in\" and not cls.is_iterable_not_str(v):\n\t            raise ValueError(\n\t                \"'in' operator can only be used with iterable value\"\n\t            )\n\t        return v\n\tclass TimeseriesFilter(BaseModel):\n\t    column: TimeseriesFilterFieldEnum\n\t    operator: FilterOperatorEnum\n", "    value: Union[\n\t        str, int, float, datetime,\n\t        List[Union[str, int, float, datetime]]\n\t    ]\n\t    def is_iterable_not_str(obj):\n\t        if isinstance(obj, Iterable) and not isinstance(obj, str):\n\t            return True\n\t        return False\n\t    @validator('value')\n\t    def in_operator_must_have_iterable(cls, v, values):\n", "        if cls.is_iterable_not_str(v) and values[\"operator\"] != \"in\":\n\t            raise ValueError(\"iterable value must be used with 'in' operator\")\n\t        if values[\"operator\"] == \"in\" and not cls.is_iterable_not_str(v):\n\t            raise ValueError(\n\t                \"'in' operator can only be used with iterable value\"\n\t            )\n\t        return v\n\tclass MetricQuery(BaseModel):\n\t    primary_filepath: Union[str, Path]\n\t    secondary_filepath: Union[str, Path]\n", "    crosswalk_filepath: Union[str, Path]\n\t    group_by: List[JoinedFilterFieldEnum]\n\t    order_by: List[JoinedFilterFieldEnum]\n\t    include_metrics: Union[List[str], str]\n\t    filters: Optional[List[JoinedFilter]] = []\n\t    return_query: bool\n\t    geometry_filepath: Optional[Union[str, Path]]\n\t    include_geometry: bool\n\t    @validator('include_geometry')\n\t    def include_geometry_must_group_by_primary_location_id(cls, v, values):\n", "        if (\n\t            v is True\n\t            and JoinedFilterFieldEnum.primary_location_id not in values[\"group_by\"]  # noqa\n\t        ):\n\t            raise ValueError(\n\t                \"`group_by` must contain `primary_location_id` \"\n\t                \"to include geometry in returned data\"\n\t            )\n\t        if v is True and not values[\"geometry_filepath\"]:\n\t            raise ValueError(\n", "                \"`geometry_filepath` must be provided to include geometry \"\n\t                \"in returned data\"\n\t            )\n\t        if JoinedFilterFieldEnum.geometry in values[\"group_by\"] and v is False:\n\t            raise ValueError(\n\t                \"group_by contains `geometry` field but `include_geometry` \"\n\t                \"is False, must be True\"\n\t            )\n\t        return v\n\t    @validator('filters')\n", "    def filter_must_be_list(cls, v):\n\t        if v is None:\n\t            return []\n\t        return v\n\tclass JoinedTimeseriesQuery(BaseModel):\n\t    primary_filepath: Union[str, Path]\n\t    secondary_filepath: Union[str, Path]\n\t    crosswalk_filepath: Union[str, Path]\n\t    order_by: List[JoinedFilterFieldEnum]\n\t    filters: Optional[List[JoinedFilter]] = []\n", "    return_query: bool\n\t    geometry_filepath: Optional[Union[str, Path]]\n\t    include_geometry: bool\n\t    @validator('include_geometry')\n\t    def include_geometry_must_group_by_primary_location_id(cls, v, values):\n\t        if v is True and not values[\"geometry_filepath\"]:\n\t            raise ValueError(\n\t                \"`geometry_filepath` must be provided to include geometry \"\n\t                \"in returned data\"\n\t            )\n", "        return v\n\t    @validator('filters')\n\t    def filter_must_be_list(cls, v):\n\t        if v is None:\n\t            return []\n\t        return v\n\tclass TimeseriesQuery(BaseModel):\n\t    timeseries_filepath: Union[str, Path]\n\t    order_by: List[TimeseriesFilterFieldEnum]\n\t    filters: Optional[List[TimeseriesFilter]] = []\n", "    return_query: bool\n\t    @validator('filters')\n\t    def filter_must_be_list(cls, v):\n\t        if v is None:\n\t            return []\n\t        return v\n\tclass TimeseriesCharQuery(BaseModel):\n\t    timeseries_filepath: Union[str, Path]\n\t    order_by: List[TimeseriesFilterFieldEnum]\n\t    group_by: List[TimeseriesFilterFieldEnum]\n", "    filters: Optional[List[TimeseriesFilter]] = []\n\t    return_query: bool\n\t    @validator('filters')\n\t    def filter_must_be_list(cls, v):\n\t        if v is None:\n\t            return []\n\t        return v\n"]}
{"filename": "src/teehr/loading/nwm_grid_data.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Union, Iterable, Optional, List\n\tfrom datetime import datetime\n\timport xarray as xr\n\timport numpy as np\n\timport pandas as pd\n\timport dask\n\tfrom teehr.loading.utils_nwm import (\n\t    validate_run_args,\n\t    build_remote_nwm_filelist,\n", "    build_zarr_references,\n\t    get_dataset,\n\t)\n\tfrom teehr.loading.const_nwm import (\n\t    NWM22_UNIT_LOOKUP,\n\t)\n\tdef compute_zonal_mean(\n\t    da: xr.DataArray, weights_filepath: str\n\t) -> pd.DataFrame:\n\t    \"\"\"Compute zonal mean for given zones and weights\"\"\"\n", "    # Read weights file\n\t    weights_df = pd.read_parquet(\n\t        weights_filepath, columns=[\"row\", \"col\", \"weight\", \"zone\"]\n\t    )\n\t    # Get variable data\n\t    arr_2d = da.values[0]\n\t    arr_2d[arr_2d == da.rio.nodata] = np.nan\n\t    # Get row/col indices\n\t    rows = weights_df.row.values\n\t    cols = weights_df.col.values\n", "    # Get the values and apply weights\n\t    var_values = arr_2d[rows, cols]\n\t    weights_df[\"value\"] = var_values * weights_df.weight.values\n\t    # Compute mean\n\t    df = weights_df.groupby(by=\"zone\")[\"value\"].mean().to_frame()\n\t    df.reset_index(inplace=True)\n\t    return df\n\t@dask.delayed\n\tdef process_single_file(\n\t    singlefile: str, run: str, variable_name: str, weights_filepath: str\n", "):\n\t    \"\"\"Compute zonal mean for a single json reference file and format\n\t    to a dataframe using the TEEHR data model\"\"\"\n\t    ds = get_dataset(singlefile)\n\t    ref_time = ds.reference_time.values[0]\n\t    nwm22_units = ds[variable_name].attrs[\"units\"]\n\t    teehr_units = NWM22_UNIT_LOOKUP.get(nwm22_units, nwm22_units)\n\t    value_time = ds.time.values[0]\n\t    da = ds[variable_name]\n\t    # Calculate mean areal of selected variable\n", "    df = compute_zonal_mean(da, weights_filepath)\n\t    df[\"value_time\"] = value_time\n\t    df[\"reference_time\"] = ref_time\n\t    df[\"measurement_unit\"] = teehr_units\n\t    df[\"configuration\"] = run\n\t    df[\"variable_name\"] = variable_name\n\t    return df\n\tdef fetch_and_format_nwm_grids(\n\t    json_paths: List[str],\n\t    run: str,\n", "    variable_name: str,\n\t    output_parquet_dir: str,\n\t    zonal_weights_filepath: str,\n\t) -> None:\n\t    \"\"\"\n\t    Reads in the single reference jsons, subsets the NWM data based on\n\t    provided IDs and formats and saves the data as a parquet files\n\t    \"\"\"\n\t    output_parquet_dir = Path(output_parquet_dir)\n\t    if not output_parquet_dir.exists():\n", "        output_parquet_dir.mkdir(parents=True)\n\t    # Format file list into a dataframe and group by reference time\n\t    days = []\n\t    z_hours = []\n\t    for path in json_paths:\n\t        filename = Path(path).name\n\t        days.append(filename.split(\".\")[1])\n\t        z_hours.append(filename.split(\".\")[3])\n\t    df_refs = pd.DataFrame(\n\t        {\"day\": days, \"z_hour\": z_hours, \"filepath\": json_paths}\n", "    )\n\t    gps = df_refs.groupby([\"day\", \"z_hour\"])\n\t    for gp in gps:\n\t        _, df = gp\n\t        results = []\n\t        for singlefile in df.filepath.tolist():\n\t            results.append(\n\t                process_single_file(\n\t                    singlefile,\n\t                    run,\n", "                    variable_name,\n\t                    zonal_weights_filepath,\n\t                )\n\t            )\n\t        z_hour_df = pd.concat(dask.compute(results)[0])\n\t        # Save to parquet\n\t        yrmoday = df.day.iloc[0]\n\t        z_hour = df.z_hour.iloc[0][1:3]\n\t        ref_time_str = f\"{yrmoday}T{z_hour}Z\"\n\t        parquet_filepath = Path(\n", "            Path(output_parquet_dir), f\"{ref_time_str}.parquet\"\n\t        )\n\t        z_hour_df.sort_values([\"zone\", \"value_time\"], inplace=True)\n\t        z_hour_df.to_parquet(parquet_filepath)\n\tdef nwm_grids_to_parquet(\n\t    run: str,\n\t    output_type: str,\n\t    variable_name: str,\n\t    start_date: Union[str, datetime],\n\t    ingest_days: int,\n", "    zonal_weights_filepath: str,\n\t    json_dir: str,\n\t    output_parquet_dir: str,\n\t    t_minus_hours: Optional[Iterable[int]] = None,\n\t):\n\t    \"\"\"\n\t    Fetches NWM gridded data, calculates zonal statistics (mean) of selected\n\t    variable for given zones, converts and saves to TEEHR tabular format\n\t    Parameters\n\t    ----------\n", "    run : str\n\t        NWM forecast category.\n\t        (e.g., \"analysis_assim\", \"short_range\", ...)\n\t    output_type : str\n\t        Output component of the configuration.\n\t        (e.g., \"channel_rt\", \"reservoir\", ...)\n\t    variable_name : str\n\t        Name of the NWM data variable to download.\n\t        (e.g., \"streamflow\", \"velocity\", ...)\n\t    start_date : str or datetime\n", "        Date to begin data ingest.\n\t        Str formats can include YYYY-MM-DD or MM/DD/YYYY\n\t    ingest_days : int\n\t        Number of days to ingest data after start date\n\t    zonal_weights_filepath: str\n\t        Path to the array containing fraction of pixel overlap\n\t        for each zone\n\t    json_dir : str\n\t        Directory path for saving json reference files\n\t    output_parquet_dir : str\n", "        Path to the directory for the final parquet files\n\t    t_minus_hours: Optional[Iterable[int]]\n\t        Specifies the look-back hours to include if an assimilation\n\t        run is specified.\n\t    The NWM configuration variables, including run, output_type, and\n\t    variable_name are stored in the NWM22_RUN_CONFIG dictionary in\n\t    const_nwm.py.\n\t    Forecast and assimilation data is grouped and saved one file per reference\n\t    time, using the file name convention \"YYYYMMDDTHHZ\".  The tabular output\n\t    parquet files follow the timeseries data model described here:\n", "    https://github.com/RTIInternational/teehr/blob/main/docs/data_models.md#timeseries  # noqa\n\t    \"\"\"\n\t    validate_run_args(run, output_type, variable_name)\n\t    component_paths = build_remote_nwm_filelist(\n\t        run,\n\t        output_type,\n\t        start_date,\n\t        ingest_days,\n\t        t_minus_hours,\n\t    )\n", "    json_paths = build_zarr_references(\n\t        component_paths,\n\t        json_dir\n\t    )\n\t    fetch_and_format_nwm_grids(\n\t        json_paths,\n\t        run,\n\t        variable_name,\n\t        output_parquet_dir,\n\t        zonal_weights_filepath,\n", "    )\n\tif __name__ == \"__main__\":\n\t    # Local testing\n\t    single_filepath = \"/mnt/sf_shared/data/ciroh/nwm.20201218_forcing_short_range_nwm.t00z.short_range.forcing.f001.conus.nc\"  # noqa\n\t    weights_parquet = \"/mnt/sf_shared/data/ciroh/wbdhuc10_weights.parquet\"\n\t    nwm_grids_to_parquet(\n\t        \"forcing_analysis_assim\",\n\t        \"forcing\",\n\t        \"RAINRATE\",\n\t        \"2020-12-18\",\n", "        1,\n\t        weights_parquet,\n\t        \"/home/sam/forcing_jsons\",\n\t        \"/home/sam/forcing_parquet\",\n\t        [0, 1, 2],\n\t    )\n"]}
{"filename": "src/teehr/loading/usgs.py", "chunked_list": ["import pandas as pd\n\tfrom typing import List, Union\n\tfrom pathlib import Path\n\tfrom datetime import datetime, timedelta\n\tfrom hydrotools.nwis_client.iv import IVDataService\n\tfrom teehr.models.loading import ChunkByEnum\n\tfrom pydantic import validate_arguments\n\tDATETIME_STR_FMT = \"%Y-%m-%dT%H:%M:00+0000\"\n\tdef _filter_to_hourly(df: pd.DataFrame) -> pd.DataFrame:\n\t    \"\"\"Filter out data not reported on the hour.\"\"\"\n", "    df.set_index(\"value_time\", inplace=True)\n\t    df2 = df[\n\t        df.index.hour.isin(range(0, 24))\n\t        & (df.index.minute == 0)\n\t        & (df.index.second == 0)\n\t    ]\n\t    df2.reset_index(level=0, allow_duplicates=True, inplace=True)\n\t    return df2\n\tdef _filter_no_data(df: pd.DataFrame, no_data_value=-999) -> pd.DataFrame:\n\t    \"\"\"Filter out no data values.\"\"\"\n", "    df2 = df[df[\"value\"] != no_data_value]\n\t    return df2\n\tdef _convert_to_si_units(df: pd.DataFrame) -> pd.DataFrame:\n\t    \"\"\"Convert streamflow values from english to metric.\"\"\"\n\t    df[\"value\"] = df[\"value\"] * 0.3048**3\n\t    df[\"measurement_unit\"] = \"m3/s\"\n\t    return df\n\tdef _datetime_to_date(dt: datetime) -> datetime:\n\t    \"\"\"Convert datetime to date only\"\"\"\n\t    dt.replace(\n", "        hour=0,\n\t        minute=0,\n\t        second=0,\n\t        microsecond=0\n\t    )\n\t    return dt\n\tdef _format_df(df: pd.DataFrame) -> pd.DataFrame:\n\t    \"\"\"Format HydroTools dataframe columns to TEEHR data model.\"\"\"\n\t    df.rename(columns={\"usgs_site_code\": \"location_id\"}, inplace=True)\n\t    df[\"location_id\"] = \"usgs-\" + df[\"location_id\"].astype(str)\n", "    df[\"configuration\"] = \"usgs_gage_data\"\n\t    df[\"reference_time\"] = df[\"value_time\"]\n\t    return df[[\n\t        \"location_id\",\n\t        \"reference_time\",\n\t        \"value_time\",\n\t        \"value\",\n\t        \"variable_name\",\n\t        \"measurement_unit\",\n\t        \"configuration\"\n", "    ]]\n\tdef _fetch_usgs(\n\t    sites: List[str],\n\t    start_date: datetime,\n\t    end_date: datetime,\n\t    filter_to_hourly: bool = True,\n\t    filter_no_data: bool = True,\n\t    convert_to_si: bool = True\n\t) -> pd.DataFrame:\n\t    \"\"\"Fetch USGS gage data and format to TEEHR format.\"\"\"\n", "    start_dt_str = start_date.strftime(DATETIME_STR_FMT)\n\t    end_dt_str = (\n\t        end_date\n\t        - timedelta(minutes=1)\n\t    ).strftime(DATETIME_STR_FMT)\n\t    # Retrieve data\n\t    service = IVDataService(\n\t        value_time_label=\"value_time\",\n\t        enable_cache=False\n\t    )\n", "    usgs_df = service.get(\n\t        sites=sites,\n\t        startDT=start_dt_str,\n\t        endDT=end_dt_str\n\t    )\n\t    if filter_to_hourly is True:\n\t        usgs_df = _filter_to_hourly(usgs_df)\n\t    if filter_no_data is True:\n\t        usgs_df = _filter_no_data(usgs_df)\n\t    if convert_to_si is True:\n", "        usgs_df = _convert_to_si_units(usgs_df)\n\t    usgs_df = _format_df(usgs_df)\n\t    # Return the data\n\t    return usgs_df\n\t@validate_arguments()\n\tdef usgs_to_parquet(\n\t    sites: List[str],\n\t    start_date: Union[str, datetime, pd.Timestamp],\n\t    end_date: Union[str, datetime, pd.Timestamp],\n\t    output_parquet_dir: Union[str, Path],\n", "    chunk_by: Union[ChunkByEnum, None] = None,\n\t    filter_to_hourly: bool = True,\n\t    filter_no_data: bool = True,\n\t    convert_to_si: bool = True\n\t):\n\t    \"\"\"Fetch USGS gage data and save as a Parquet file.\n\t    Parameters\n\t    ----------\n\t    sites : List[str]\n\t        List of USGS gages sites to fetch.\n", "        Must be string to preserve the leading 0.\n\t    start_date : datetime\n\t        Start time of data to fetch.\n\t    end_date : datetime\n\t        End time of data to fetch. Note, since startDt is inclusive for the\n\t        USGS service, we subtract 1 minute from this time so we don't get\n\t        overlap between consecutive calls.\n\t    output_parquet_dir : Union[str, Path]\n\t        Path of directory where parquet files will be saved.\n\t    chunk_by : Union[str, None], default = None\n", "        How to \"chunk\" the fetching and storing of the data.\n\t        Valid options = [\"day\", \"site\", None]\n\t    filter_to_hourly: bool = True\n\t        Return only values that fall on the hour (i.e. drop 15 minute data).\n\t    filter_no_data: bool = True\n\t        Filter out -999 values\n\t    convert_to_si: bool = True\n\t        Multiplies values by 0.3048**3 and sets `measurement_units` to `m3/s`\n\t    \"\"\"\n\t    start_date = pd.Timestamp(start_date)\n", "    end_date = pd.Timestamp(end_date)\n\t    # Check if output_parquet_dir is an existing dir\n\t    output_dir = Path(output_parquet_dir)\n\t    if not output_dir.exists():\n\t        output_dir.mkdir(parents=True)\n\t    # Fetch all at once\n\t    if chunk_by is None:\n\t        usgs_df = _fetch_usgs(\n\t            sites=sites,\n\t            start_date=start_date,\n", "            end_date=end_date,\n\t            filter_to_hourly=filter_to_hourly,\n\t            filter_no_data=filter_no_data,\n\t            convert_to_si=convert_to_si\n\t        )\n\t        if len(usgs_df) > 0:\n\t            output_filepath = Path(output_parquet_dir, \"usgs.parquet\")\n\t            usgs_df.to_parquet(output_filepath)\n\t            # output_filepath = Path(usgs.output_parquet_dir, \"usgs.csv\")\n\t            # usgs_df.to_csv(output_filepath)\n", "            # print(usgs_df)\n\t    if chunk_by == \"day\":\n\t        # Determine number of days to fetch\n\t        period_length = timedelta(days=1)\n\t        start_date = _datetime_to_date(start_date)\n\t        end_date = _datetime_to_date(end_date)\n\t        period = end_date - start_date\n\t        if period < period_length:\n\t            period = period_length\n\t        number_of_days = period.days\n", "        # Fetch data in daily batches\n\t        for day in range(number_of_days):\n\t            # Setup start and end date for fetch\n\t            start_dt = (start_date + period_length * day)\n\t            end_dt = (\n\t                start_date\n\t                + period_length * (day + 1)\n\t                - timedelta(minutes=1)\n\t            )\n\t            usgs_df = _fetch_usgs(\n", "                sites=sites,\n\t                start_date=start_dt,\n\t                end_date=end_dt,\n\t                filter_to_hourly=filter_to_hourly,\n\t                filter_no_data=filter_no_data,\n\t                convert_to_si=convert_to_si\n\t            )\n\t            if len(usgs_df) > 0:\n\t                output_filepath = Path(\n\t                    output_parquet_dir,\n", "                    f\"{start_dt.strftime('%Y-%m-%d')}.parquet\"\n\t                )\n\t                usgs_df.to_parquet(output_filepath)\n\t                # output_filepath = Path(\n\t                #     usgs.output_parquet_dir,\n\t                #     f\"{start_dt.strftime('%Y-%m-%d')}.csv\"\n\t                # )\n\t                # usgs_df.to_csv(output_filepath)\n\t                # print(usgs_df)\n\t    if chunk_by == \"location_id\":\n", "        for site in sites:\n\t            usgs_df = _fetch_usgs(\n\t                sites=[site],\n\t                start_date=start_date,\n\t                end_date=end_date,\n\t                filter_to_hourly=filter_to_hourly,\n\t                filter_no_data=filter_no_data,\n\t                convert_to_si=convert_to_si\n\t            )\n\t            if len(usgs_df) > 0:\n", "                output_filepath = Path(\n\t                    output_parquet_dir,\n\t                    f\"{site}.parquet\"\n\t                )\n\t                usgs_df.to_parquet(output_filepath)\n\t                # output_filepath = Path(\n\t                #     usgs.output_parquet_dir,\n\t                #     f\"{site}.csv\"\n\t                # )\n\t                # usgs_df.to_csv(output_filepath)\n", "                # print(usgs_df)\n\tif __name__ == \"__main__\":\n\t    # Examples\n\t    usgs_to_parquet(\n\t        sites=[\n\t            \"02449838\",\n\t            \"02450825\"\n\t        ],\n\t        start_date=datetime(2023, 2, 20),\n\t        end_date=datetime(2023, 2, 25),\n", "        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n\t        chunk_by=\"location_id\"\n\t    )\n\t    usgs_to_parquet(\n\t        sites=[\n\t            \"02449838\",\n\t            \"02450825\"\n\t        ],\n\t        start_date=datetime(2023, 2, 20),\n\t        end_date=datetime(2023, 2, 25),\n", "        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n\t        chunk_by=\"day\"\n\t    )\n\t    usgs_to_parquet(\n\t        sites=[\n\t            \"02449838\",\n\t            \"02450825\"\n\t        ],\n\t        start_date=datetime(2023, 2, 20),\n\t        end_date=datetime(2023, 2, 25),\n", "        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n\t    )\n\t    pass\n"]}
{"filename": "src/teehr/loading/nwm_point_data.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Union, Iterable, Tuple, Optional, List\n\tfrom datetime import datetime\n\timport fsspec\n\timport xarray as xr\n\timport pandas as pd\n\tfrom kerchunk.combine import MultiZarrToZarr\n\timport dask\n\timport ujson\n\tfrom teehr.loading.utils_nwm import (\n", "    validate_run_args,\n\t    build_remote_nwm_filelist,\n\t    build_zarr_references,\n\t)\n\tfrom teehr.loading.const_nwm import (\n\t    NWM22_UNIT_LOOKUP\n\t)\n\tdef fetch_and_format_nwm_points(\n\t    json_paths: List[str],\n\t    location_ids: Iterable[int],\n", "    run: str,\n\t    variable_name: str,\n\t    output_parquet_dir: str,\n\t    concat_dims=[\"time\"],\n\t):\n\t    \"\"\"Reads in the single reference jsons, subsets the\n\t        NWM data based on provided IDs and formats and saves\n\t        the data as a parquet files using Dask.\n\t    Parameters\n\t    ----------\n", "    json_paths: list\n\t        List of the single json reference filepaths\n\t    location_ids : Iterable[int]\n\t        Array specifying NWM IDs of interest\n\t    run : str\n\t        NWM forecast category\n\t    variable_name : str\n\t        Name of the NWM data variable to download\n\t    output_parquet_dir : str\n\t        Path to the directory for the final parquet files\n", "    \"\"\"\n\t    output_parquet_dir = Path(output_parquet_dir)\n\t    if not output_parquet_dir.exists():\n\t        output_parquet_dir.mkdir(parents=True)\n\t    # Format file list into a dataframe and group by reference time\n\t    days = []\n\t    z_hours = []\n\t    for path in json_paths:\n\t        filename = Path(path).name\n\t        days.append(filename.split(\".\")[1])\n", "        z_hours.append(filename.split(\".\")[3])\n\t    df_refs = pd.DataFrame(\n\t        {\n\t            \"day\": days,\n\t            \"z_hour\": z_hours,\n\t            \"filepath\": json_paths\n\t        }\n\t    )\n\t    gps = df_refs.groupby([\"day\", \"z_hour\"])\n\t    results = []\n", "    for gp in gps:\n\t        results.append(\n\t            fetch_and_format(\n\t                gp,\n\t                location_ids,\n\t                run,\n\t                variable_name,\n\t                output_parquet_dir,\n\t                concat_dims\n\t            )\n", "        )\n\t    dask.compute(results)\n\t@dask.delayed\n\tdef fetch_and_format(\n\t    gp: Tuple[Tuple[str, str], pd.DataFrame],\n\t    location_ids: Iterable[int],\n\t    run: str,\n\t    variable_name: str,\n\t    output_parquet_dir: str,\n\t    concat_dims: List[str],\n", ") -> None:\n\t    \"\"\"Helper function to fetch and format the NWM data using Dask.\n\t    Parameters\n\t    ----------\n\t    gp : Tuple[Tuple[str, str], pd.Dataframe],\n\t        A tuple containing a tuple of (day, z_hour) and a dataframe of\n\t        reference json filepaths for a specific z_hour.  Results from\n\t        pandas dataframe.groupby([\"day\", \"z_hour\"])\n\t    location_ids : Iterable[int]\n\t        Array specifying NWM IDs of interest\n", "    run : str\n\t        NWM forecast category\n\t    variable_name : str\n\t        Name of the NWM data variable to download\n\t    output_parquet_dir : str\n\t        Path to the directory for the final parquet files\n\t    concat_dims : list of strings\n\t        List of dimensions to use when concatenating single file\n\t        jsons to multifile\n\t    \"\"\"\n", "    _, df = gp\n\t    # Only combine if there is more than one file for this group,\n\t    # otherwise a warning is thrown\n\t    if len(df.index) > 1:\n\t        mzz = MultiZarrToZarr(\n\t            df.filepath.tolist(),\n\t            remote_protocol=\"gcs\",\n\t            remote_options={\"anon\": True},\n\t            concat_dims=concat_dims,\n\t        )\n", "        json = mzz.translate()\n\t    else:\n\t        json = ujson.load(open(df.filepath.iloc[0]))\n\t    fs = fsspec.filesystem(\"reference\", fo=json)\n\t    m = fs.get_mapper(\"\")\n\t    ds_nwm_subset = xr.open_zarr(m, consolidated=False, chunks={}).sel(\n\t        feature_id=location_ids\n\t    )\n\t    # Convert to dataframe and do some reformatting\n\t    df_temp = ds_nwm_subset[variable_name].to_dataframe()\n", "    df_temp.reset_index(inplace=True)\n\t    if len(df.index) == 1:\n\t        df_temp[\"time\"] = ds_nwm_subset.time.values[0]\n\t    df_temp.rename(\n\t        columns={\n\t            variable_name: \"value\",\n\t            \"time\": \"value_time\",\n\t            \"feature_id\": \"location_id\",\n\t        },\n\t        inplace=True,\n", "    )\n\t    df_temp.dropna(subset=[\"value\"], inplace=True)\n\t    nwm22_units = ds_nwm_subset[variable_name].units\n\t    teehr_units = NWM22_UNIT_LOOKUP.get(nwm22_units, nwm22_units)\n\t    df_temp[\"measurement_unit\"] = teehr_units\n\t    ref_time = ds_nwm_subset.reference_time.values[0]\n\t    df_temp[\"reference_time\"] = ref_time\n\t    df_temp[\"configuration\"] = run\n\t    df_temp[\"variable_name\"] = variable_name\n\t    df_temp[\"location_id\"] = \"nwm22-\" + df_temp.location_id.astype(int).astype(str)\n", "    # Save to parquet\n\t    ref_time_str = pd.to_datetime(ref_time).strftime(\"%Y%m%dT%HZ\")\n\t    parquet_filepath = Path(output_parquet_dir, f\"{ref_time_str}.parquet\")\n\t    df_temp.to_parquet(parquet_filepath)\n\tdef nwm_to_parquet(\n\t    run: str,\n\t    output_type: str,\n\t    variable_name: str,\n\t    start_date: Union[str, datetime],\n\t    ingest_days: int,\n", "    location_ids: Iterable[int],\n\t    json_dir: str,\n\t    output_parquet_dir: str,\n\t    t_minus_hours: Optional[Iterable[int]] = None,\n\t):\n\t    \"\"\"Fetches NWM point data, formats to tabular, and saves to parquet\n\t    Parameters\n\t    ----------\n\t    run : str\n\t        NWM forecast category.\n", "        (e.g., \"analysis_assim\", \"short_range\", ...)\n\t    output_type : str\n\t        Output component of the configuration.\n\t        (e.g., \"channel_rt\", \"reservoir\", ...)\n\t    variable_name : str\n\t        Name of the NWM data variable to download.\n\t        (e.g., \"streamflow\", \"velocity\", ...)\n\t    start_date : str or datetime\n\t        Date to begin data ingest.\n\t        Str formats can include YYYY-MM-DD or MM/DD/YYYY\n", "    ingest_days : int\n\t        Number of days to ingest data after start date\n\t    location_ids : Iterable[int]\n\t        Array specifying NWM IDs of interest\n\t    json_dir : str\n\t        Directory path for saving json reference files\n\t    output_parquet_dir : str\n\t        Path to the directory for the final parquet files\n\t    t_minus_hours: Optional[Iterable[int]]\n\t        Specifies the look-back hours to include if an assimilation\n", "        run is specified.\n\t    The NWM configuration variables, including run, output_type, and\n\t    variable_name are stored in the NWM22_RUN_CONFIG dictionary in\n\t    const_nwm.py.\n\t    Forecast and assimilation data is grouped and saved one file per reference\n\t    time, using the file name convention \"YYYYMMDDTHHZ\".  The tabular output\n\t    parquet files follow the timeseries data model described here:\n\t    https://github.com/RTIInternational/teehr/blob/main/docs/data_models.md#timeseries  # noqa\n\t    \"\"\"\n\t    validate_run_args(\n", "        run,\n\t        output_type,\n\t        variable_name\n\t    )\n\t    component_paths = build_remote_nwm_filelist(\n\t        run,\n\t        output_type,\n\t        start_date,\n\t        ingest_days,\n\t        t_minus_hours,\n", "    )\n\t    json_paths = build_zarr_references(\n\t        component_paths,\n\t        json_dir\n\t    )\n\t    fetch_and_format_nwm_points(\n\t        json_paths,\n\t        location_ids,\n\t        run,\n\t        variable_name,\n", "        output_parquet_dir,\n\t    )\n"]}
{"filename": "src/teehr/loading/generate_weights.py", "chunked_list": ["from typing import Union\n\tfrom pathlib import Path\n\timport warnings\n\timport geopandas as gpd\n\timport numpy as np\n\timport xarray as xr\n\tfrom rasterio.transform import rowcol\n\timport rasterio\n\timport pandas as pd\n\timport dask\n", "import shapely\n\tfrom teehr.loading.utils_nwm import load_gdf\n\timport teehr.loading.const_nwm as const_nwm\n\t@dask.delayed\n\tdef vectorize(data_array: xr.DataArray) -> gpd.GeoDataFrame:\n\t    \"\"\"\n\t    Convert 2D xarray.DataArray into a geopandas.GeoDataFrame\n\t    Heavily borrowed from GeoCube, see:\n\t    https://github.com/corteva/geocube/blob/master/geocube/vector.py#L12\n\t    \"\"\"\n", "    # nodata mask\n\t    mask = None\n\t    if np.isnan(data_array.rio.nodata):\n\t        mask = ~data_array.isnull()\n\t    elif data_array.rio.nodata is not None:\n\t        mask = data_array != data_array.rio.nodata\n\t    # Give all pixels a unique value\n\t    data_array.values[:, :] = np.arange(0, data_array.values.size).reshape(\n\t        data_array.shape\n\t    )\n", "    # vectorize generator\n\t    vectorized_data = (\n\t        (value, shapely.geometry.shape(polygon))\n\t        for polygon, value in rasterio.features.shapes(\n\t            data_array,\n\t            transform=data_array.rio.transform(),\n\t            mask=mask,\n\t        )\n\t    )\n\t    gdf = gpd.GeoDataFrame(\n", "        vectorized_data,\n\t        columns=[data_array.name, \"geometry\"],\n\t        crs=data_array.rio.crs,\n\t    )\n\t    xx, yy = np.meshgrid(data_array.x.values, data_array.y.values)\n\t    gdf[\"x\"] = xx.ravel()\n\t    gdf[\"y\"] = yy.ravel()\n\t    return gdf\n\t@dask.delayed\n\tdef overlay_zones(\n", "    grid: gpd.GeoDataFrame, zones: gpd.GeoDataFrame\n\t) -> gpd.GeoDataFrame:\n\t    with pd.option_context(\n\t        \"mode.chained_assignment\", None\n\t    ):  # to ignore setwithcopywarning\n\t        grid.loc[:, \"pixel_area\"] = grid.geometry.area\n\t        overlay_gdf = grid.overlay(zones, keep_geom_type=True)\n\t        overlay_gdf.loc[:, \"overlay_area\"] = overlay_gdf.geometry.area\n\t        overlay_gdf.loc[:, \"weight\"] = (\n\t            overlay_gdf.overlay_area / overlay_gdf.pixel_area\n", "        )\n\t    return overlay_gdf\n\tdef vectorize_grid(\n\t    src_da: xr.DataArray,\n\t    nodata_val: float,\n\t    vectorize_chunk: float = 40,\n\t) -> gpd.GeoDataFrame:\n\t    \"\"\"Vectorize pixels in the template array in chunks using dask\n\t    Note: Parameter vectorize_chunk determines how many pixels will\n\t    be vectorized at one time\n", "    (thousands of pixels)\n\t    \"\"\"\n\t    src_da = src_da.persist()\n\t    max_pixels = vectorize_chunk * 1000\n\t    num_splits = np.ceil(src_da.values.size / max_pixels).astype(int)\n\t    # Prepare each data array\n\t    if num_splits > 0:\n\t        da_list = np.array_split(src_da, num_splits)\n\t        [da.rio.write_nodata(nodata_val, inplace=True) for da in da_list]\n\t    else:\n", "        src_da.rio.write_nodata(nodata_val, inplace=True)\n\t        da_list = [src_da]\n\t    results = []\n\t    for da_subset in da_list:\n\t        results.append(vectorize(da_subset))\n\t    grid_gdf = pd.concat(dask.compute(results)[0])\n\t    grid_gdf.crs = const_nwm.CONUS_NWM_WKT\n\t    # Reindex to remove duplicates\n\t    grid_gdf[\"index\"] = np.arange(len(grid_gdf.index))\n\t    grid_gdf.set_index(\"index\", inplace=True)\n", "    return grid_gdf\n\tdef calculate_weights(\n\t    grid_gdf: gpd.GeoDataFrame,\n\t    zone_gdf: gpd.GeoDataFrame,\n\t    overlay_chunk: float = 250,\n\t) -> gpd.GeoDataFrame:\n\t    \"\"\"Overlay vectorized pixels and zone polygons, and calculate\n\t    areal weights, returning a geodataframe\n\t    Note: Parameter overlay_chunk determines the size of the rectangular\n\t    window that spatially subsets datasets for the operation\n", "    (thousands of pixels)\n\t    \"\"\"\n\t    # Make sure geometries are valid\n\t    grid_gdf[\"geometry\"] = grid_gdf.geometry.make_valid()\n\t    zone_gdf[\"geometry\"] = zone_gdf.geometry.make_valid()\n\t    xmin, ymin, xmax, ymax = zone_gdf.total_bounds\n\t    x_steps = np.arange(xmin, xmax, overlay_chunk * 1000)\n\t    y_steps = np.arange(ymin, ymax, overlay_chunk * 1000)\n\t    x_steps = np.append(x_steps, xmax)\n\t    y_steps = np.append(y_steps, ymax)\n", "    results = []\n\t    for i in range(x_steps.size - 1):\n\t        for j in range(y_steps.size - 1):\n\t            xmin = x_steps[i]\n\t            xmax = x_steps[i + 1]\n\t            ymin = y_steps[j]\n\t            ymax = y_steps[j + 1]\n\t            zone = zone_gdf.cx[xmin:xmax, ymin:ymax]\n\t            grid = grid_gdf.cx[xmin:xmax, ymin:ymax]\n\t            if len(zone.index) == 0 or len(grid.index) == 0:\n", "                continue\n\t            results.append(overlay_zones(grid, zone))\n\t    overlay_gdf = pd.concat(dask.compute(results)[0])\n\t    return overlay_gdf\n\tdef generate_weights_file(\n\t    zone_polygon_filepath: Union[Path, str],\n\t    template_dataset: Union[str, Path],\n\t    variable_name: str,\n\t    output_weights_filepath: Union[str, Path],\n\t    unique_zone_id: str = None,\n", "    **read_args: str,\n\t) -> None:\n\t    \"\"\"Generate a file of row/col indices and weights for pixels intersecting\n\t       given zone polyons\n\t    Parameters\n\t    ----------\n\t    zone_polygon_filepath : str\n\t        Path to the polygons geoparquet file\n\t    template_dataset : str\n\t        Path to the grid dataset to use as a template\n", "    variable_name : str\n\t        Name of the variable within the dataset\n\t    output_weights_filepath : str\n\t        Path to the resultant weights file\n\t    unique_zone_id: str\n\t        Name of the field in the zone polygon file containing unique IDs\n\t    save_to_disk: boolean\n\t        Flag to indicate whether or not to save results to disk\n\t    read_args: dict, optional\n\t        Keyword arguments to be passed to GeoPandas read_file(),\n", "        read_parquet(), and read_feather() methods\n\t    \"\"\"\n\t    zone_gdf = load_gdf(zone_polygon_filepath, **read_args)\n\t    zone_gdf = zone_gdf.to_crs(const_nwm.CONUS_NWM_WKT)\n\t    ds = xr.open_dataset(template_dataset)\n\t    src_da = ds[variable_name]\n\t    src_da = src_da.rio.write_crs(const_nwm.CONUS_NWM_WKT, inplace=True)\n\t    grid_transform = src_da.rio.transform()\n\t    nodata_val = src_da.rio.nodata\n\t    # Get the subset of the grid that intersects the total zone bounds\n", "    bbox = tuple(zone_gdf.total_bounds)\n\t    src_da = src_da.sel(x=slice(bbox[0], bbox[2]), y=slice(bbox[1], bbox[3]))[\n\t        0\n\t    ]\n\t    src_da = src_da.astype(\"float32\")\n\t    src_da[\"x\"] = np.float32(src_da.x.values)\n\t    src_da[\"y\"] = np.float32(src_da.y.values)\n\t    # Vectorize source grid pixels\n\t    grid_gdf = vectorize_grid(src_da, nodata_val)\n\t    # Overlay and calculate areal weights of pixels within each zone\n", "    # Note: Temporarily suppress the dask UserWarning: \"Large object detected\n\t    #  in task graph\" until a better approach is found\n\t    with warnings.catch_warnings():\n\t        warnings.filterwarnings(\"ignore\", category=UserWarning)\n\t        weights_gdf = calculate_weights(grid_gdf, zone_gdf)\n\t    weights_gdf = weights_gdf.drop_duplicates(\n\t        subset=[\"x\", \"y\", unique_zone_id]\n\t    )\n\t    # Convert x-y to row-col using original transform\n\t    rows, cols = rowcol(\n", "        grid_transform, weights_gdf.x.values, weights_gdf.y.values\n\t    )\n\t    weights_gdf[\"row\"] = rows\n\t    weights_gdf[\"col\"] = cols\n\t    if unique_zone_id:\n\t        df = weights_gdf[[\"row\", \"col\", \"weight\", unique_zone_id]].copy()\n\t        df.rename(columns={unique_zone_id: \"zone\"}, inplace=True)\n\t    else:\n\t        df = weights_gdf[[\"row\", \"col\", \"weight\"]]\n\t        df[\"zone\"] = weights_gdf.index.values\n", "    if output_weights_filepath:\n\t        df.to_parquet(output_weights_filepath)\n\t        df = None\n\t    return df\n\tif __name__ == \"__main__\":\n\t    # Local testing\n\t    zone_polygon_filepath = \"/mnt/sf_shared/data/ciroh/nextgen_03S.gpkg\"\n\t    template_dataset = \"/mnt/sf_shared/data/ciroh/nwm.20201218_forcing_short_range_nwm.t00z.short_range.forcing.f001.conus.nc\"  # noqa\n\t    variable_name = \"RAINRATE\"\n\t    unique_zone_id = \"id\"\n", "    output_weights_filepath = (\n\t        \"/mnt/sf_shared/data/ciroh/wbdhu10_medium_range_weights.parquet\"\n\t    )\n\t    zone_polygon_filepath = (\n\t        \"/mnt/sf_shared/data/ciroh/test_ngen_divides.parquet\"\n\t    )\n\t    generate_weights_file(\n\t        zone_polygon_filepath,\n\t        template_dataset,\n\t        variable_name,\n", "        output_weights_filepath,\n\t        unique_zone_id,\n\t    )\n"]}
{"filename": "src/teehr/loading/__init__.py", "chunked_list": []}
{"filename": "src/teehr/loading/utils_nwm.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Union, Optional, Iterable, List\n\tfrom datetime import datetime\n\timport dask\n\timport fsspec\n\timport ujson  # fast json\n\tfrom kerchunk.hdf import SingleHdf5ToZarr\n\timport pandas as pd\n\timport numpy as np\n\timport xarray as xr\n", "import geopandas as gpd\n\tfrom teehr.loading.const_nwm import (\n\t    NWM22_RUN_CONFIG,\n\t    NWM22_ANALYSIS_CONFIG,\n\t    NWM_BUCKET,\n\t)\n\tdef load_gdf(filepath: Union[str, Path], **kwargs: str) -> gpd.GeoDataFrame:\n\t    \"\"\"Load any supported geospatial file type into a gdf using GeoPandas.\"\"\"\n\t    try:\n\t        gdf = gpd.read_file(filepath, **kwargs)\n", "        return gdf\n\t    except Exception:\n\t        pass\n\t    try:\n\t        gdf = gpd.read_parquet(filepath, **kwargs)\n\t        return gdf\n\t    except Exception:\n\t        pass\n\t    try:\n\t        gdf = gpd.read_feather(filepath, **kwargs)\n", "        return gdf\n\t    except Exception:\n\t        raise Exception(\"Unsupported zone polygon file type\")\n\tdef parquet_to_gdf(parquet_filepath: str) -> gpd.GeoDataFrame:\n\t    gdf = gpd.read_parquet(parquet_filepath)\n\t    return gdf\n\tdef np_to_list(t):\n\t    return [a.tolist() for a in t]\n\tdef get_dataset(zarr_json: str) -> xr.Dataset:\n\t    \"\"\"Retrieve a blob from the data service as xarray.Dataset.\n", "    Parameters\n\t    ----------\n\t    blob_name: str, required\n\t        Name of blob to retrieve.\n\t    Returns\n\t    -------\n\t    ds : xarray.Dataset\n\t        The data stored in the blob.\n\t    \"\"\"\n\t    backend_args = {\n", "        \"consolidated\": False,\n\t        \"storage_options\": {\n\t            \"fo\": zarr_json,\n\t            \"remote_protocol\": \"gcs\",\n\t            \"remote_options\": {\"anon\": True},\n\t        },\n\t    }\n\t    ds = xr.open_dataset(\n\t        \"reference://\",\n\t        engine=\"zarr\",\n", "        backend_kwargs=backend_args,\n\t    )\n\t    return ds\n\tdef list_to_np(lst):\n\t    return tuple([np.array(a) for a in lst])\n\t@dask.delayed\n\tdef gen_json(\n\t    remote_path: str, fs: fsspec.filesystem, json_dir: Union[str, Path]\n\t) -> str:\n\t    \"\"\"Helper function for creating single-file kerchunk reference JSONs.\n", "    Parameters\n\t    ----------\n\t    remote_path : str\n\t        Path to the file in the remote location (ie, GCS bucket)\n\t    fs : fsspec.filesystem\n\t        fsspec filesystem mapped to GCS\n\t    json_dir : str\n\t        Directory for saving zarr reference json files\n\t    Returns\n\t    -------\n", "    str\n\t        Path to the local zarr reference json file\n\t    \"\"\"\n\t    so = dict(\n\t        mode=\"rb\",\n\t        anon=True,\n\t        default_fill_cache=False,\n\t        default_cache_type=\"first\",  # noqa\n\t    )\n\t    with fs.open(remote_path, **so) as infile:\n", "        h5chunks = SingleHdf5ToZarr(infile, remote_path, inline_threshold=300)\n\t        p = remote_path.split(\"/\")\n\t        date = p[3]\n\t        fname = p[5]\n\t        outf = str(Path(json_dir, f\"{date}.{fname}.json\"))\n\t        with open(outf, \"wb\") as f:\n\t            f.write(ujson.dumps(h5chunks.translate()).encode())\n\t    return outf\n\tdef build_zarr_references(\n\t    remote_paths: List[str], json_dir: Union[str, Path]\n", ") -> list[str]:\n\t    \"\"\"Builds the single file zarr json reference files using kerchunk.\n\t    Parameters\n\t    ----------\n\t    remote_paths : List[str]\n\t        List of remote filepaths\n\t    json_dir : str or Path\n\t        Local directory for caching json files\n\t    Returns\n\t    -------\n", "    list[str]\n\t        List of paths to the zarr reference json files\n\t    \"\"\"\n\t    json_dir_path = Path(json_dir)\n\t    if not json_dir_path.exists():\n\t        json_dir_path.mkdir(parents=True)\n\t    fs = fsspec.filesystem(\"gcs\", anon=True)\n\t    results = []\n\t    for path in remote_paths:\n\t        results.append(gen_json(path, fs, json_dir))\n", "    json_paths = dask.compute(results)[0]\n\t    return sorted(json_paths)\n\tdef validate_run_args(run: str, output_type: str, variable: str):\n\t    \"\"\"Validates user-provided NWMv22 run arguments.\n\t    Parameters\n\t    ----------\n\t    run : str\n\t        Run type/configuration\n\t    output_type : str\n\t        Output component of the configuration\n", "    variable : str\n\t        Name of the variable to fetch within the output_type\n\t    Raises\n\t    ------\n\t    KeyError\n\t        Invalid key error\n\t    \"\"\"\n\t    try:\n\t        NWM22_RUN_CONFIG[run]\n\t    except Exception as e:\n", "        raise ValueError(f\"Invalid RUN entry: {str(e)}\")\n\t    try:\n\t        NWM22_RUN_CONFIG[run][output_type]\n\t    except Exception as e:\n\t        raise ValueError(f\"Invalid RUN entry: {str(e)}\")\n\t    if variable not in NWM22_RUN_CONFIG[run][output_type]:\n\t        raise KeyError(f\"Invalid VARIABLE_NAME entry: {variable}\")\n\tdef construct_assim_paths(\n\t    gcs_dir: str,\n\t    run: str,\n", "    output_type: str,\n\t    dates: pd.DatetimeIndex,\n\t    t_minus: Iterable[int],\n\t    run_name_in_filepath: str,\n\t    cycle_z_hours: Iterable[int],\n\t    domain: str,\n\t) -> list[str]:\n\t    \"\"\"Constructs paths to NWM point assimilation data based on specified\n\t        parameters.\n\t    This function prioritizes value time over reference time so that only\n", "    files with value times falling within the specified date range are included\n\t    in the resulting file list.\n\t    Parameters\n\t    ----------\n\t    gcs_dir : str\n\t        Path to the NWM data on GCS\n\t    run : str\n\t        Run type/configuration\n\t    output_type : str\n\t        Output component of the configuration\n", "    dates : pd.DatetimeIndex\n\t        Range of days to fetch data\n\t    t_minus : Iterable[int]\n\t        Collection of lookback hours to include when fetching assimilation data\n\t    run_name_in_filepath : str\n\t        Name of the assimilation run as represented in the GCS file.\n\t        Defined in const_nwm.py\n\t    cycle_z_hours : Iterable[int]\n\t        The z-hour of the assimilation run per day. Defined in const_nwm.py\n\t    domain : str\n", "        Geographic region covered by the assimilation run.\n\t        Defined in const_nwm.py\n\t    Returns\n\t    -------\n\t    list[str]\n\t        List of remote filepaths\n\t    \"\"\"\n\t    component_paths = []\n\t    for dt in dates:\n\t        dt_str = dt.strftime(\"%Y%m%d\")\n", "        # Add the values starting from day 1,\n\t        # skipping value times in the previous day\n\t        if \"hawaii\" in run:\n\t            for cycle_hr in cycle_z_hours:\n\t                for tm in t_minus:\n\t                    for tm2 in [0, 15, 30, 45]:\n\t                        if (tm * 100 + tm2) > cycle_hr * 100:\n\t                            continue\n\t                        file_path = f\"{gcs_dir}/nwm.{dt_str}/{run}/nwm.t{cycle_hr:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}{tm2:02d}.{domain}.nc\"  # noqa\n\t                        component_paths.append(file_path)\n", "        else:\n\t            for cycle_hr in cycle_z_hours:\n\t                for tm in t_minus:\n\t                    if tm > cycle_hr:\n\t                        continue\n\t                    file_path = f\"{gcs_dir}/nwm.{dt_str}/{run}/nwm.t{cycle_hr:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}.{domain}.nc\"  # noqa\n\t                    component_paths.append(file_path)\n\t        # Now add the values from the day following the end day,\n\t        # whose value times that fall within the end day\n\t        if \"extend\" in run:\n", "            for tm in t_minus:\n\t                dt_add = dt + pd.Timedelta(cycle_hr + 24, unit=\"hours\")\n\t                hr_add = dt_add.hour\n\t                if tm > hr_add:\n\t                    dt_add_str = dt_add.strftime(\"%Y%m%d\")\n\t                    file_path = f\"{gcs_dir}/nwm.{dt_add_str}/{run}/nwm.t{hr_add:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}.{domain}.nc\"  # noqa\n\t                    component_paths.append(file_path)\n\t        elif \"hawaii\" in run:\n\t            for cycle_hr2 in cycle_z_hours:\n\t                for tm in t_minus:\n", "                    for tm2 in [0, 15, 30, 45]:\n\t                        if cycle_hr2 > 0:\n\t                            dt_add = dt + pd.Timedelta(\n\t                                cycle_hr + cycle_hr2, unit=\"hours\"\n\t                            )\n\t                            hr_add = dt_add.hour\n\t                            if (tm * 100 + tm2) > hr_add * 100:\n\t                                dt_add_str = dt_add.strftime(\"%Y%m%d\")\n\t                                file_path = f\"{gcs_dir}/nwm.{dt_add_str}/{run}/nwm.t{hr_add:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}{tm2:02d}.{domain}.nc\"  # noqa\n\t                                component_paths.append(file_path)\n", "        else:\n\t            for cycle_hr2 in cycle_z_hours:\n\t                for tm in t_minus:\n\t                    if cycle_hr2 > 0:\n\t                        dt_add = dt + pd.Timedelta(\n\t                            cycle_hr + cycle_hr2, unit=\"hours\"\n\t                        )\n\t                        hr_add = dt_add.hour\n\t                        if tm > hr_add:\n\t                            dt_add_str = dt_add.strftime(\"%Y%m%d\")\n", "                            file_path = f\"{gcs_dir}/nwm.{dt_add_str}/{run}/nwm.t{hr_add:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}.{domain}.nc\"  # noqa\n\t                            component_paths.append(file_path)\n\t    return sorted(component_paths)\n\tdef build_remote_nwm_filelist(\n\t    run: str,\n\t    output_type: str,\n\t    start_dt: Union[str, datetime],\n\t    ingest_days: int,\n\t    t_minus_hours: Optional[Iterable[int]],\n\t) -> List[str]:\n", "    \"\"\"Assembles a list of remote NWM files in GCS based on specified user\n\t        parameters.\n\t    Parameters\n\t    ----------\n\t    run : str\n\t        Run type/configuration\n\t    output_type : str\n\t        Output component of the configuration\n\t    start_dt : str YYYY-MM-DD or datetime\n\t        Date to begin data ingest\n", "    ingest_days : int\n\t        Number of days to ingest data after start date\n\t    t_minus_hours: Iterable[int]\n\t        Only necessary if assimilation data is requested.\n\t        Collection of lookback hours to include when fetching assimilation data\n\t    Returns\n\t    -------\n\t    list\n\t        List of remote filepaths (strings)\n\t    \"\"\"\n", "    gcs_dir = f\"gcs://{NWM_BUCKET}\"\n\t    fs = fsspec.filesystem(\"gcs\", anon=True)\n\t    dates = pd.date_range(start=start_dt, periods=ingest_days, freq=\"1d\")\n\t    if \"assim\" in run:\n\t        cycle_z_hours = NWM22_ANALYSIS_CONFIG[run][\"cycle_z_hours\"]\n\t        domain = NWM22_ANALYSIS_CONFIG[run][\"domain\"]\n\t        run_name_in_filepath = NWM22_ANALYSIS_CONFIG[run][\n\t            \"run_name_in_filepath\"\n\t        ]\n\t        max_lookback = NWM22_ANALYSIS_CONFIG[run][\"num_lookback_hrs\"]\n", "        if max(t_minus_hours) > max_lookback - 1:\n\t            raise ValueError(\n\t                f\"The maximum specified t-minus hour exceeds the lookback \"\n\t                f\"period for this configuration: {run}; max t-minus: \"\n\t                f\"{max(t_minus_hours)} hrs; \"\n\t                f\"look-back period: {max_lookback} hrs\"\n\t            )\n\t        component_paths = construct_assim_paths(\n\t            gcs_dir,\n\t            run,\n", "            output_type,\n\t            dates,\n\t            t_minus_hours,\n\t            run_name_in_filepath,\n\t            cycle_z_hours,\n\t            domain,\n\t        )\n\t    else:\n\t        component_paths = []\n\t        for dt in dates:\n", "            dt_str = dt.strftime(\"%Y%m%d\")\n\t            file_path = f\"{gcs_dir}/nwm.{dt_str}/{run}/nwm.*.{output_type}*\"\n\t            component_paths.extend(fs.glob(file_path))\n\t        component_paths = sorted([f\"gcs://{path}\" for path in component_paths])\n\t    return component_paths\n"]}
{"filename": "src/teehr/loading/ngen.py", "chunked_list": ["\"\"\"\n\tLibrary of code to ingest NGEN outputs to the TEEHR parquet data model format.\n\tThis code would basically just be a *.csv parser the write to parquet files.\n\tGiven how variable the ngen output can be at this point, it is not worth\n\twriting robust converter code.  A few helper functions below.\n\tSee also examples/loading/ngen_to_parquet.ipynb\n\t\"\"\"\n\t# import json\n\t# import re\n\t# def get_forcing_file_pattern(realization_filepath):\n", "#     with open(realization_filepath) as f:\n\t#         j = json.load(f)\n\t#     file_pattern = j[\"global\"][\"forcing\"].get(\"file_pattern\", None)\n\t#     path = j[\"global\"][\"forcing\"].get(\"path\", None)\n\t# def glob_re(pattern, strings):\n\t#     return filter(re.compile(pattern).match, strings)\n\tpass\n"]}
{"filename": "src/teehr/loading/nwm21_retrospective.py", "chunked_list": ["import pandas as pd\n\timport xarray as xr\n\timport fsspec\n\t# import numpy as np\n\tfrom datetime import datetime, timedelta\n\tfrom pathlib import Path\n\tfrom typing import Union, Iterable\n\tfrom teehr.loading.const_nwm import (\n\t    NWM22_UNIT_LOOKUP\n\t)\n", "from teehr.models.loading import (\n\t    ChunkByEnum\n\t)\n\t# URL = 's3://noaa-nwm-retro-v2-zarr-pds'\n\t# MIN_DATE = datetime(1993, 1, 1)\n\t# MAX_DATE = datetime(2018, 12, 31, 23)\n\tURL = 's3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr/'\n\tMIN_DATE = pd.Timestamp(1979, 1, 1)\n\tMAX_DATE = pd.Timestamp(2020, 12, 31, 23)\n\tdef _datetime_to_date(dt: datetime) -> datetime:\n", "    \"\"\"Convert datetime to date only\"\"\"\n\t    dt.replace(\n\t        hour=0,\n\t        minute=0,\n\t        second=0,\n\t        microsecond=0\n\t    )\n\t    return dt\n\tdef _da_to_df(da: xr.DataArray) -> pd.DataFrame:\n\t    \"\"\"Format NWM retrospective data to TEEHR format.\"\"\"\n", "    df = da.to_dataframe()\n\t    df.reset_index(inplace=True)\n\t    df[\"measurement_unit\"] = NWM22_UNIT_LOOKUP.get(da.units, da.units)\n\t    df[\"variable_name\"] = da.name\n\t    df[\"configuration\"] = \"nwm22_retrospective\"\n\t    df[\"reference_time\"] = df[\"time\"]\n\t    df.rename(\n\t        columns={\n\t            \"time\": \"value_time\",\n\t            \"feature_id\": \"location_id\",\n", "            da.name: \"value\"\n\t        },\n\t        inplace=True\n\t    )\n\t    df.drop(columns=[\"latitude\", \"longitude\"], inplace=True)\n\t    df[\"location_id\"] = \"nwm22-\" + df[\"location_id\"].astype(str)\n\t    df[\"location_id\"] = df[\"location_id\"].astype(str).astype(\"category\")\n\t    df[\"measurement_unit\"] = df[\"measurement_unit\"].astype(\"category\")\n\t    df[\"variable_name\"] = df[\"variable_name\"].astype(\"category\")\n\t    df[\"configuration\"] = df[\"configuration\"].astype(\"category\")\n", "    return df\n\tdef validate_start_end_date(\n\t    start_date: Union[str, datetime],\n\t    end_date: Union[str, datetime],\n\t):\n\t    if end_date <= start_date:\n\t        raise ValueError(\"start_date must be before end_date\")\n\t    if start_date < MIN_DATE:\n\t        raise ValueError(f\"start_date must be on or after {MIN_DATE}\")\n\t    if end_date > MAX_DATE:\n", "        raise ValueError(f\"end_date must be on or before {MAX_DATE}\")\n\tdef nwm_retro_to_parquet(\n\t    variable_name: str,\n\t    location_ids: Iterable[int],\n\t    start_date: Union[str, datetime, pd.Timestamp],\n\t    end_date: Union[str, datetime, pd.Timestamp],\n\t    output_parquet_dir: Union[str, Path],\n\t    chunk_by: Union[ChunkByEnum, None] = None,\n\t):\n\t    \"\"\"Fetch NWM retrospective at NWM COMIDs and store as Parquet.\n", "    Parameters\n\t    ----------\n\t    Returns\n\t    -------\n\t    \"\"\"\n\t    start_date = pd.Timestamp(start_date)\n\t    end_date = pd.Timestamp(end_date)\n\t    validate_start_end_date(start_date, end_date)\n\t    output_dir = Path(output_parquet_dir)\n\t    if not output_dir.exists():\n", "        output_dir.mkdir(parents=True)\n\t    ds = xr.open_zarr(fsspec.get_mapper(URL, anon=True), consolidated=True)\n\t    # Fetch all at once\n\t    if chunk_by is None:\n\t        da = ds[variable_name].sel(\n\t            feature_id=location_ids,\n\t            time=slice(start_date, end_date)\n\t        )\n\t        df = _da_to_df(da)\n\t        output_filepath = Path(\n", "            output_parquet_dir,\n\t            \"nwm22_retrospective.parquet\"\n\t        )\n\t        df.to_parquet(output_filepath)\n\t        # output_filepath = Path(\n\t        #     output_parquet_dir,\n\t        #     \"nwm22_retrospective.csv\"\n\t        # )\n\t        # df.to_csv(output_filepath)\n\t        # print(df)\n", "    if chunk_by == \"day\":\n\t        # Determine number of days to fetch\n\t        period_length = timedelta(days=1)\n\t        start_date = _datetime_to_date(start_date)\n\t        end_date = _datetime_to_date(end_date)\n\t        period = end_date - start_date\n\t        if period < period_length:\n\t            period = period_length\n\t        number_of_days = period.days\n\t        # Fetch data in daily batches\n", "        for day in range(number_of_days):\n\t            # Setup start and end date for fetch\n\t            start_dt = (start_date + period_length * day)\n\t            end_dt = (\n\t                start_date\n\t                + period_length * (day + 1)\n\t                - timedelta(minutes=1)\n\t            )\n\t            da = ds[variable_name].sel(\n\t                feature_id=location_ids,\n", "                time=slice(start_dt, end_dt)\n\t            )\n\t            df = _da_to_df(da)\n\t            output_filepath = Path(\n\t                output_parquet_dir,\n\t                f\"{start_dt.strftime('%Y-%m-%d')}.parquet\"\n\t            )\n\t            df.to_parquet(output_filepath)\n\t            # output_filepath = Path(\n\t            #     output_parquet_dir,\n", "            #     f\"{start_dt.strftime('%Y-%m-%d')}.csv\"\n\t            # )\n\t            # df.to_csv(output_filepath)\n\t            # print(df)\n\t    # fetch data by site\n\t    if chunk_by == \"location_id\":\n\t        for location_id in location_ids:\n\t            da = ds[variable_name].sel(\n\t                feature_id=location_id,\n\t                time=slice(start_date, end_date)\n", "            )\n\t            df = _da_to_df(da)\n\t            output_filepath = Path(\n\t                output_parquet_dir,\n\t                f\"{location_id}.parquet\"\n\t            )\n\t            df.to_parquet(output_filepath)\n\t            # output_filepath = Path(\n\t            #     output_parquet_dir,\n\t            #     f\"{location_id}.csv\"\n", "            # )\n\t            # df.to_csv(output_filepath)\n\t            # print(df)\n\tif __name__ == \"__main__\":\n\t    # Examples\n\t    LOCATION_IDS = [7086109, 7040481]\n\t    nwm_retro_to_parquet(\n\t        variable_name='streamflow',\n\t        start_date=\"2000-01-01\",\n\t        end_date=\"2000-01-02 23:00\",\n", "        location_ids=LOCATION_IDS,\n\t        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\")\n\t    )\n\t    nwm_retro_to_parquet(\n\t        variable_name='streamflow',\n\t        start_date=datetime(2000, 1, 1),\n\t        end_date=datetime(2000, 1, 3),\n\t        location_ids=LOCATION_IDS,\n\t        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\"),\n\t        chunk_by=\"day\",\n", "    )\n\t    nwm_retro_to_parquet(\n\t        variable_name='streamflow',\n\t        start_date=datetime(2000, 1, 1),\n\t        end_date=datetime(2000, 1, 2, 23),\n\t        location_ids=LOCATION_IDS,\n\t        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\"),\n\t        chunk_by=\"location_id\",\n\t    )\n\t    pass\n"]}
{"filename": "src/teehr/loading/const_nwm.py", "chunked_list": ["import numpy as np\n\tNWM_BUCKET = \"national-water-model\"\n\tNWM22_CHANNEL_RT_VARS = [\n\t    \"nudge\",\n\t    \"qBtmVertRunoff\",\n\t    \"qBucket\",\n\t    \"qSfcLatRunoff\",\n\t    \"streamflow\",\n\t    \"velocity\",\n\t]\n", "NWM22_CHANNEL_RT_VARS_NO_DA = [\n\t    \"nudge\",\n\t    \"qBucket\",\n\t    \"qSfcLatRunoff\",\n\t    \"streamflow\",\n\t    \"velocity\",\n\t]\n\tNWM22_CHANNEL_RT_VARS_LONG = [\"nudge\", \"streamflow\", \"velocity\"]\n\tNWM22_TERRAIN_VARS = [\"sfcheadsubrt\", \"zwattablrt\"]\n\tNWM22_RESERVOIR_VARS = [\n", "    \"inflow\",\n\t    \"outflow\",\n\t    \"reservoir_assimiated_value\",\n\t    \"water_sfc_elev\",\n\t]\n\tNWM22_LAND_VARS_ASSIM = [\n\t    \"ACCET\",\n\t    \"ACSNOM\",\n\t    \"EDIR\",\n\t    \"FSNO\",\n", "    \"ISNOW\",\n\t    \"QRAIN\",\n\t    \"QSNOW\",\n\t    \"SNEQV\",\n\t    \"SNLIQ\",\n\t    \"SNOWH\",\n\t    \"SNOWT_AVG\",\n\t    \"SOILICE\",\n\t    \"SOILSAT_TOP\",\n\t    \"SOIL_M\",\n", "    \"SOIL_T\",\n\t]\n\tNWM22_LAND_VARS_SHORT = [\n\t    \"ACCET\",\n\t    \"SNOWT_AVG\",\n\t    \"SOILSAT_TOP\",\n\t    \"FSNO\",\n\t    \"SNOWH\",\n\t    \"SNEQV\",\n\t]\n", "NWM22_LAND_VARS_MEDIUM = [\n\t    \"FSA\",\n\t    \"FIRA\",\n\t    \"GRDFLX\",\n\t    \"HFX\",\n\t    \"LH\",\n\t    \"UGDRNOFF\",\n\t    \"ACCECAN\",\n\t    \"ACCEDIR\",\n\t    \"ACCETRAN\",\n", "    \"TRAD\",\n\t    \"SNLIQ\",\n\t    \"SOIL_T\",\n\t    \"SOIL_M\",\n\t    \"SNOWH\",\n\t    \"SNEQV\",\n\t    \"ISNOW\",\n\t    \"FSNO\",\n\t    \"ACSNOM\",\n\t    \"ACCET\",\n", "    \"CANWAT\",\n\t    \"SOILICE\",\n\t    \"SOILSAT_TOP\",\n\t    \"SNOWT_AVG\",\n\t]\n\tNWM22_LAND_VARS_LONG = [\n\t    \"UGDRNOFF\",\n\t    \"SFCRNOFF\",\n\t    \"SNEQV\",\n\t    \"ACSNOM\",\n", "    \"ACCET\",\n\t    \"CANWAT\",\n\t    \"SOILSAT_TOP\",\n\t    \"SOILSAT\",\n\t]\n\tNWM22_FORCING_VARS = [\n\t    \"U2D\",\n\t    \"V2D\",\n\t    \"T2D\",\n\t    \"Q2D\",\n", "    \"LWDOWN\",\n\t    \"SWDOWN\",\n\t    \"RAINRATE\",\n\t    \"PSFC\",\n\t]\n\tNWM22_RUN_CONFIG = {\n\t    \"analysis_assim\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_extend\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n", "        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_extend_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n", "    \"analysis_assim_long\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_long_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_hawaii\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_hawaii_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n", "        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"analysis_assim_puertorico\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n", "    \"analysis_assim_puertorico_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_ASSIM,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"short_range\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_SHORT,\n", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"short_range_hawaii\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_SHORT,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"short_range_puertorico\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n", "        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_SHORT,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"short_range_hawaii_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_SHORT,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n", "    \"short_range_puertorico_no_da\": {\n\t        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n\t        \"terrain_rt\": NWM22_TERRAIN_VARS,\n\t        \"land\": NWM22_LAND_VARS_SHORT,\n\t        \"reservoir\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_mem1\": {\n\t        \"channel_rt_1\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt_1\": NWM22_TERRAIN_VARS,\n\t        \"land_1\": NWM22_LAND_VARS_MEDIUM,\n", "        \"reservoir_1\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_mem2\": {\n\t        \"channel_rt_2\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt_2\": NWM22_TERRAIN_VARS,\n\t        \"land_2\": NWM22_LAND_VARS_MEDIUM,\n\t        \"reservoir_2\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_mem3\": {\n\t        \"channel_rt_3\": NWM22_CHANNEL_RT_VARS,\n", "        \"terrain_rt_3\": NWM22_TERRAIN_VARS,\n\t        \"land_3\": NWM22_LAND_VARS_MEDIUM,\n\t        \"reservoir_3\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_mem4\": {\n\t        \"channel_rt_4\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt_4\": NWM22_TERRAIN_VARS,\n\t        \"land_4\": NWM22_LAND_VARS_MEDIUM,\n\t        \"reservoir_4\": NWM22_RESERVOIR_VARS,\n\t    },\n", "    \"medium_range_mem5\": {\n\t        \"channel_rt_5\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt_5\": NWM22_TERRAIN_VARS,\n\t        \"land_5\": NWM22_LAND_VARS_MEDIUM,\n\t        \"reservoir_5\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_mem6\": {\n\t        \"channel_rt_6\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt_6\": NWM22_TERRAIN_VARS,\n\t        \"land_6\": NWM22_LAND_VARS_MEDIUM,\n", "        \"reservoir_6\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_mem7\": {\n\t        \"channel_rt_7\": NWM22_CHANNEL_RT_VARS,\n\t        \"terrain_rt_7\": NWM22_TERRAIN_VARS,\n\t        \"land_7\": NWM22_LAND_VARS_MEDIUM,\n\t        \"reservoir_7\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"medium_range_no_da\": {\"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA},\n\t    \"long_range_mem1\": {\n", "        \"channel_rt_1\": NWM22_CHANNEL_RT_VARS_LONG,\n\t        \"land_1\": NWM22_LAND_VARS_LONG,\n\t        \"reservoir_1\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"long_range_mem2\": {\n\t        \"channel_rt_2\": NWM22_CHANNEL_RT_VARS_LONG,\n\t        \"land_2\": NWM22_LAND_VARS_LONG,\n\t        \"reservoir_2\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"long_range_mem3\": {\n", "        \"channel_rt_3\": NWM22_CHANNEL_RT_VARS_LONG,\n\t        \"land_3\": NWM22_LAND_VARS_LONG,\n\t        \"reservoir_3\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"long_range_mem4\": {\n\t        \"channel_rt_4\": NWM22_CHANNEL_RT_VARS_LONG,\n\t        \"land_4\": NWM22_LAND_VARS_LONG,\n\t        \"reservoir_4\": NWM22_RESERVOIR_VARS,\n\t    },\n\t    \"forcing_medium_range\": {\"forcing\": NWM22_FORCING_VARS},\n", "    \"forcing_short_range\": {\"forcing\": NWM22_FORCING_VARS},\n\t    \"forcing_short_range_hawaii\": {\"forcing\": NWM22_FORCING_VARS},\n\t    \"forcing_short_range_puertorico\": {\"forcing\": NWM22_FORCING_VARS},\n\t    \"forcing_analysis_assim\": {\"forcing\": NWM22_FORCING_VARS},\n\t    \"forcing_analysis_assim_extend\": {\"forcing\": NWM22_FORCING_VARS},\n\t    \"forcing_analysis_assim_hawaii\": {\"forcing\": NWM22_FORCING_VARS},\n\t    \"forcing_analysis_assim_puertorico\": {\"forcing\": NWM22_FORCING_VARS},\n\t}\n\tNWM22_ANALYSIS_CONFIG = {\n\t    \"analysis_assim\": {\n", "        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim\",\n\t    },\n\t    \"analysis_assim_no_da\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim_no_da\",\n", "    },\n\t    \"analysis_assim_extend\": {\n\t        \"num_lookback_hrs\": 28,\n\t        \"cycle_z_hours\": [16],\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim_extend\",\n\t    },\n\t    \"analysis_assim_extend_no_da\": {\n\t        \"num_lookback_hrs\": 28,\n\t        \"cycle_z_hours\": [16],\n", "        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim_extend_no_da\",\n\t    },\n\t    \"analysis_assim_long\": {\n\t        \"num_lookback_hrs\": 12,\n\t        \"cycle_z_hours\": np.arange(0, 24, 6),\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim_long\",\n\t    },\n\t    \"analysis_assim_long_no_da\": {\n", "        \"num_lookback_hrs\": 12,\n\t        \"cycle_z_hours\": np.arange(0, 24, 6),\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim_long_no_da\",\n\t    },\n\t    \"analysis_assim_hawaii\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n\t        \"domain\": \"hawaii\",\n\t        \"run_name_in_filepath\": \"analysis_assim\",\n", "    },\n\t    \"analysis_assim_hawaii_no_da\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_cycle_z_hourstimes\": np.arange(0, 24, 1),\n\t        \"domain\": \"hawaii\",\n\t        \"run_name_in_filepath\": \"analysis_assim_no_da\",\n\t    },\n\t    \"analysis_assim_puertorico\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n", "        \"domain\": \"puertorico\",\n\t        \"run_name_in_filepath\": \"analysis_assim\",\n\t    },\n\t    \"analysis_assim_puertorico_no_da\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n\t        \"domain\": \"puertorico\",\n\t        \"run_name_in_filepath\": \"analysis_assim_no_da\",\n\t    },\n\t    \"forcing_analysis_assim\": {\n", "        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim\",\n\t    },\n\t    \"forcing_analysis_assim_extend\": {\n\t        \"num_lookback_hrs\": 28,\n\t        \"cycle_z_hours\": [16],\n\t        \"domain\": \"conus\",\n\t        \"run_name_in_filepath\": \"analysis_assim_extend\",\n", "    },\n\t    \"forcing_analysis_assim_hawaii\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n\t        \"domain\": \"hawaii\",\n\t        \"run_name_in_filepath\": \"analysis_assim\",\n\t    },\n\t    \"forcing_analysis_assim_puertorico\": {\n\t        \"num_lookback_hrs\": 3,\n\t        \"cycle_z_hours\": np.arange(0, 24, 1),\n", "        \"domain\": \"puertorico\",\n\t        \"run_name_in_filepath\": \"analysis_assim\",\n\t    },\n\t}\n\tNWM22_UNIT_LOOKUP = {\"m3 s-1\": \"m3/s\"}\n\t# WKT strings extracted from NWM grids\n\tCONUS_NWM_WKT = 'PROJCS[\"Lambert_Conformal_Conic\",GEOGCS[\"GCS_Sphere\",DATUM[\"D_Sphere\",SPHEROID[\"Sphere\",6370000.0,0.0]], \\\n\tPRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\n\tPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-97.0],PARAMETER[\"standard_parallel_1\",30.0],\\\n\tPARAMETER[\"standard_parallel_2\",60.0],PARAMETER[\"latitude_of_origin\",40.0],UNIT[\"Meter\",1.0]]'\n", "HI_NWM_WKT = 'PROJCS[\"Lambert_Conformal_Conic\",GEOGCS[\"GCS_Sphere\",DATUM[\"D_Sphere\",SPHEROID[\"Sphere\",6370000.0,0.0]],\\\n\tPRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\n\tPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-157.42],PARAMETER[\"standard_parallel_1\",10.0],\\\n\tPARAMETER[\"standard_parallel_2\",30.0],PARAMETER[\"latitude_of_origin\",20.6],UNIT[\"Meter\",1.0]]'\n\tPR_NWM_WKT = 'PROJCS[\"Sphere_Lambert_Conformal_Conic\",GEOGCS[\"GCS_Sphere\",DATUM[\"D_Sphere\",SPHEROID[\"Sphere\",6370000.0,0.0]],\\\n\tPRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\n\tPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-65.91],PARAMETER[\"standard_parallel_1\",18.1],\\\n\tPARAMETER[\"standard_parallel_2\",18.1],PARAMETER[\"latitude_of_origin\",18.1],UNIT[\"Meter\",1.0]]'\n"]}
