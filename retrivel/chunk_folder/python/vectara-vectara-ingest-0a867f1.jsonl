{"filename": "ingest.py", "chunked_list": ["import logging\n\timport json\n\timport requests\n\timport time\n\tfrom omegaconf import OmegaConf, DictConfig\n\timport toml     # type: ignore\n\timport sys\n\timport os\n\tfrom typing import Any\n\timport importlib\n", "from core.crawler import Crawler\n\tfrom authlib.integrations.requests_client import OAuth2Session\n\tdef instantiate_crawler(base_class, folder_name: str, class_name: str, *args, **kwargs) -> Any:   # type: ignore\n\t    sys.path.insert(0, os.path.abspath(folder_name))\n\t    crawler_name = class_name.split('Crawler')[0]\n\t    module_name = f\"{folder_name}.{crawler_name.lower()}_crawler\"  # Construct the full module path\n\t    module = importlib.import_module(module_name)\n\t    class_ = getattr(module, class_name)\n\t    # Ensure the class is a subclass of the base class\n\t    if not issubclass(class_, base_class):\n", "        raise TypeError(f\"{class_name} is not a subclass of {base_class.__name__}\")\n\t    # Instantiate the class and return the instance\n\t    return class_(*args, **kwargs)\n\tdef get_jwt_token(auth_url: str, auth_id: str, auth_secret: str, customer_id: str) -> Any:\n\t    \"\"\"Connect to the server and get a JWT token.\"\"\"\n\t    token_endpoint = f'{auth_url}/oauth2/token'\n\t    session = OAuth2Session(auth_id, auth_secret, scope=\"\")\n\t    token = session.fetch_token(token_endpoint, grant_type=\"client_credentials\")\n\t    return token[\"access_token\"]\n\tdef reset_corpus(endpoint: str, customer_id: str, corpus_id: int, auth_url: str, auth_id: str, auth_secret: str) -> None:\n", "    \"\"\"\n\t    Reset the corpus by deleting all documents and metadata.\n\t    Args:\n\t        endpoint (str): Endpoint for the Vectara API.\n\t        customer_id (str): ID of the Vectara customer.\n\t        appclient_id (str): ID of the Vectara app client.\n\t        appclient_secret (str): Secret key for the Vectara app client.\n\t        corpus_id (int): ID of the Vectara corpus to index to.\n\t    \"\"\"\n\t    url = f\"https://{endpoint}/v1/reset-corpus\"\n", "    payload = json.dumps({\n\t        \"customerId\": customer_id,\n\t        \"corpusId\": corpus_id\n\t    })\n\t    token = get_jwt_token(auth_url, auth_id, auth_secret, customer_id)\n\t    headers = {\n\t        'Content-Type': 'application/json',\n\t        'Accept': 'application/json',\n\t        'customer-id': str(customer_id),\n\t        'Authorization': f'Bearer {token}'\n", "    }\n\t    response = requests.request(\"POST\", url, headers=headers, data=payload)\n\t    if response.status_code == 200:\n\t        logging.info(f\"Reset corpus {corpus_id}\")\n\t    else:\n\t        logging.error(f\"Error resetting corpus: {response.status_code} {response.text}\")\n\tdef main() -> None:\n\t    \"\"\"\n\t    Main function that runs the web crawler based on environment variables.\n\t    Reads the necessary environment variables and sets up the web crawler\n", "    accordingly. Starts the crawl loop and logs the progress and errors.\n\t    \"\"\"\n\t    if len(sys.argv) != 3:\n\t        logging.info(\"Usage: python ingest.py <config_file> <secrets-profile>\")\n\t        return\n\t    config_name = sys.argv[1]\n\t    profile_name = sys.argv[2]\n\t    # process arguments \n\t    cfg: DictConfig = DictConfig(OmegaConf.load(config_name))\n\t    # add .env params, by profile\n", "    volume = '/home/vectara/env'\n\t    with open(f\"{volume}/secrets.toml\", 'r') as f:\n\t        env_dict = toml.load(f)\n\t    if profile_name not in env_dict:\n\t        logging.info(f'Profile \"{profile_name}\" not found in secrets.toml')\n\t        return\n\t    env_dict = env_dict[profile_name]\n\t    for k,v in env_dict.items():\n\t        if k=='HUBSPOT_API_KEY':\n\t            OmegaConf.update(cfg, f'hubspot_crawler.{k.lower()}', v)\n", "            continue\n\t        if k=='NOTION_API_KEY':\n\t            OmegaConf.update(cfg, f'notion_crawler.{k.lower()}', v)\n\t            continue\n\t        if k=='DISCOURSE_API_KEY':\n\t            OmegaConf.update(cfg, f'discourse_crawler.{k.lower()}', v)\n\t            continue\n\t        if k=='FMP_API_KEY':\n\t            OmegaConf.update(cfg, f'fmp_crawler.{k.lower()}', v)\n\t            continue\n", "        if k=='JIRA_PASSWORD':\n\t            OmegaConf.update(cfg, f'jira_crawler.{k.lower()}', v)\n\t            continue\n\t        if k=='GITHUB_TOKEN':\n\t            OmegaConf.update(cfg, f'github_crawler.{k.lower()}', v)\n\t            continue\n\t        if k.startswith('aws_'):\n\t            OmegaConf.update(cfg, f's3_crawler.{k.lower()}', v)\n\t            continue\n\t        # default (otherwise) - add to vectara config\n", "        OmegaConf.update(cfg['vectara'], k, v)\n\t    endpoint = 'api.vectara.io'\n\t    customer_id = cfg.vectara.customer_id\n\t    corpus_id = cfg.vectara.corpus_id\n\t    api_key = cfg.vectara.api_key\n\t    crawler_type = cfg.crawling.crawler_type\n\t    # instantiate the crawler\n\t    crawler = instantiate_crawler(Crawler, 'crawlers', f'{crawler_type.capitalize()}Crawler', cfg, endpoint, customer_id, corpus_id, api_key)\n\t    # When debugging a crawler, it is sometimes useful to reset the corpus (remove all documents)\n\t    # To do that you would have to set this to True and also include <auth_url> and <auth_id> in the secrets.toml file\n", "    # NOTE: use with caution; this will delete all documents in the corpus and is irreversible\n\t    reset_corpus_flag = False\n\t    if reset_corpus_flag:\n\t        logging.info(\"Resetting corpus\")\n\t        reset_corpus(endpoint, customer_id, corpus_id, cfg.vectara.auth_url, cfg.vectara.auth_id, cfg.vectara.auth_secret)\n\t        time.sleep(5)   # wait 5 seconds to allow reset_corpus enough time to complete on the backend\n\t    logging.info(f\"Starting crawl of type {crawler_type}...\")\n\t    crawler.crawl()\n\t    logging.info(f\"Finished crawl of type {crawler_type}...\")\n\tif __name__ == '__main__':\n", "    logging.basicConfig(format=\"%(asctime)s %(levelname)-8s %(message)s\", level=logging.INFO)\n\t    main()"]}
{"filename": "core/indexer.py", "chunked_list": ["import logging\n\timport json\n\timport os\n\tfrom typing import Tuple, Dict, Any, List, Optional\n\tfrom slugify import slugify         \n\timport time\n\tfrom core.utils import create_session_with_retries\n\tfrom goose3 import Goose\n\tfrom goose3.text import StopWordsArabic, StopWordsKorean, StopWordsChinese\n\timport justext\n", "from bs4 import BeautifulSoup\n\tfrom omegaconf import OmegaConf\n\tfrom nbconvert import HTMLExporter      # type: ignore\n\timport nbformat\n\timport markdown\n\timport docutils.core\n\tfrom core.utils import html_to_text, detect_language, get_file_size_in_MB\n\tfrom playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError\n\tfrom unstructured.partition.auto import partition\n\timport unstructured as us\n", "language_stopwords_Goose = {\n\t    'en': None,  # English stopwords are the default\n\t    'ar': StopWordsArabic,  # Use Arabic stopwords\n\t    'zh-cn': StopWordsChinese,  # Use Chinese stopwords\n\t    'zh-tw': StopWordsChinese,  # Use Chinese stopwords\n\t    'ko': StopWordsKorean  # Use Korean stopwords\n\t        }\n\tlanguage_stopwords_JusText = {\n\t    'en': None,  # English stopwords are the default\n\t    'ar': 'Arabic',  # Use Arabic stopwords\n", "    'ko': 'Korean' , # Use Korean stopwords\n\t    'ur': 'Urdu' ,  # Use urdu stopwords\n\t    'hi': 'Hindi' ,  # Use Hindi stopwords \n\t    'fa': 'Persian',  # Use Persian stopwords\n\t    'ja': 'Japanese',  # Use Japanese stopwords\n\t    'bg': 'Bulgarian',  # Use Bulgarian stopwords\n\t    'sv': 'Swedish',  # Use Swedish stopwords\n\t    'lv': 'Latvian',  # Use Latvian stopwords\n\t    'sr': 'Serbian',  # Use Serbian stopwords\n\t    'sq': 'Albanian',  # Use Albanian stopwords\n", "    'es': 'Spanish',  # Use Spanish stopwords\n\t    'ka': 'Gerogian',  # Use Georgian stopwords\n\t    'de': 'German',  # Use German stopwords\n\t    'el': 'Greek',  # Use Greek stopwords\n\t    'ga': 'Irish',  # Use Irish stopwords\n\t    'vi': 'Vietnamese',  # Use Vietnamese stopwords\n\t    'hu': 'Hungarian',  # Use Hungarian stopwords\n\t    'pt': 'Portuguese',  # Use Portuguese stopwords\n\t    'pl': 'Polish',  # Use Polish stopwords\n\t    'it': 'Italian',  # Use Italian stopwords\n", "    'la': 'Latin',  # Use Latin stopwords\n\t    'tr': 'Turkish',  # Use Turkish stopwords\n\t    'id': 'Indonesian',  # Use Indonesian stopwords\n\t    'hr': 'Croatian',  # Use Croatian stopwords\n\t    'be': 'Belarusian',  # Use Belarusian stopwords\n\t    'ru': 'Russian',  # Use Russian stopwords\n\t    'et': 'Estonian',  # Use Estonian stopwords\n\t    'uk': 'Ukranian',  # Use Ukranian stopwords\n\t    'ro': 'Romanian',  # Use Romanian stowords\n\t    'cs': 'Czech',  # Use Czech stopwords\n", "    'ml': 'Malyalam',  # Use Malyalam stopwords\n\t    'sk': 'Slovak',  # Use Slovak stopwords\n\t    'fi': 'Finnish',  # Use Finnish stopwords\n\t    'da': 'Danish',  # Use Danish stopwords\n\t    'ms': 'Malay',  # Use Malay stopwords\n\t    'ca': 'Catalan',  # Use Catalan stopwords\n\t    'eo': 'Esperanto',  # Use Esperanto stopwords\n\t    'nb': 'Norwegian_Bokmal',  # Use Norwegian_Bokmal stopwords\n\t    'nn': 'Norwegian_Nynorsk'  # Use Norwegian_Nynorsk stopwords\n\t    # Add more languages and their stopwords keywords here\n", "}\n\tdef get_content_with_justext(html_content: str, detected_language: str) -> Tuple[str, str]:\n\t    if detected_language == 'en':\n\t        paragraphs = justext.justext(html_content, justext.get_stoplist(\"English\")) \n\t    else:\n\t        stopwords_keyword = language_stopwords_JusText.get(detected_language, 'English')\n\t    # Extract paragraphs using the selected stoplist\n\t        paragraphs = justext.justext(html_content, justext.get_stoplist(stopwords_keyword))\n\t    text = '\\n'.join([p.text for p in paragraphs if not p.is_boilerplate])\n\t    soup = BeautifulSoup(html_content, 'html.parser')\n", "    stitle = soup.find('title')\n\t    if stitle:\n\t        title = stitle.text\n\t    else:\n\t        title = 'No title'\n\t    return text, title\n\tdef get_content_with_goose3(html_content: str, url: str, detected_language: str) -> Tuple[str, str]:\n\t    if detected_language in language_stopwords_Goose:\n\t        stopwords_class = language_stopwords_Goose.get(detected_language, None)\n\t        if stopwords_class is not None:\n", "            g = Goose({'stopwords_class': stopwords_class})\n\t        else:\n\t            g = Goose()  # Use the default stopwords for languages that don't have a configured StopWords class\n\t        article = g.extract(url=url, raw_html=html_content)\n\t        title = article.title\n\t        text = article.cleaned_text\n\t        return text, title\n\t    else:\n\t        title = \"\"\n\t        text = \"\"\n", "        logging.info(f\"{detected_language} is not supported by Goose\")\n\t        return text, title\n\tdef get_content_and_title(html_content: str, url: str, detected_language: str) -> Tuple[str, str]:\n\t    text1, title1 = get_content_with_goose3(html_content, url, detected_language)\n\t    text2, title2 = get_content_with_justext(html_content, detected_language)\n\t    # both Goose and Justext do extraction without boilerplate; return the one that produces the longest text, trying to optimize for content\n\t    if len(text1)>len(text2):\n\t        return text1, title1\n\t    else:\n\t        return text2, title2\n", "class Indexer(object):\n\t    \"\"\"\n\t    Vectara API class.\n\t    Args:\n\t        endpoint (str): Endpoint for the Vectara API.\n\t        customer_id (str): ID of the Vectara customer.\n\t        corpus_id (int): ID of the Vectara corpus to index to.\n\t        api_key (str): API key for the Vectara API.\n\t    \"\"\"\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str, reindex: bool = True) -> None:\n", "        self.cfg = cfg\n\t        self.endpoint = endpoint\n\t        self.customer_id = customer_id\n\t        self.corpus_id = corpus_id\n\t        self.api_key = api_key\n\t        self.reindex = reindex\n\t        self.timeout = 30\n\t        self.detected_language: Optional[str] = None\n\t        # setup requests session and mount adapter to retry requests\n\t        self.session = create_session_with_retries()\n", "        # create playwright browser so we can reuse it across all Indexer operations\n\t        self.p = sync_playwright().start()\n\t        self.browser = self.p.firefox.launch(headless=True)\n\t    def fetch_content_with_timeout(self, url: str, timeout: int = 30) -> Tuple[str, str] :\n\t        '''\n\t        Fetch content from a URL with a timeout.\n\t        Args:\n\t            url (str): URL to fetch.\n\t            timeout (int, optional): Timeout in seconds. Defaults to 30.\n\t        Returns:\n", "            str: Content from the URL.\n\t        '''\n\t        page = context = None\n\t        try:\n\t            context = self.browser.new_context()\n\t            page = context.new_page()\n\t            page.route(\"**/*\", lambda route: route.abort()  # do not load images as they are unnecessary for our purpose\n\t                if route.request.resource_type == \"image\" \n\t                else route.continue_() \n\t            ) \n", "            page.goto(url, timeout=timeout*1000)\n\t            content = page.content()\n\t            out_url = page.url\n\t        except PlaywrightTimeoutError:\n\t            logging.info(f\"Page loading timed out for {url}\")\n\t            return '', ''\n\t        except Exception as e:\n\t            logging.info(f\"Page loading failed for {url} with exception '{e}'\")\n\t            content = ''\n\t            out_url = ''\n", "            if not self.browser.is_connected():\n\t                self.browser = self.p.firefox.launch(headless=True)\n\t        finally:\n\t            if context:\n\t                context.close()\n\t            if page:\n\t                page.close()\n\t        return out_url, content\n\t    # delete document; returns True if successful, False otherwise\n\t    def delete_doc(self, doc_id: str) -> bool:\n", "        \"\"\"\n\t        Delete a document from the Vectara corpus.\n\t        Args:\n\t            url (str): URL of the page to delete.\n\t            doc_id (str): ID of the document to delete.\n\t        Returns:\n\t            bool: True if the delete was successful, False otherwise.\n\t        \"\"\"\n\t        body = {'customer_id': self.customer_id, 'corpus_id': self.corpus_id, 'document_id': doc_id}\n\t        post_headers = { 'x-api-key': self.api_key, 'customer-id': str(self.customer_id) }\n", "        response = self.session.post(\n\t            f\"https://{self.endpoint}/v1/delete-doc\", data=json.dumps(body),\n\t            verify=True, headers=post_headers)\n\t        if response.status_code != 200:\n\t            logging.error(f\"Delete request failed for doc_id = {doc_id} with status code {response.status_code}, reason {response.reason}, text {response.text}\")\n\t            return False\n\t        return True\n\t    def index_file(self, filename: str, uri: str, metadata: Dict[str, Any]) -> bool:\n\t        \"\"\"\n\t        Index a file on local file system by uploading it to the Vectara corpus.\n", "        Args:\n\t            filename (str): Name of the PDF file to create.\n\t            uri (str): URI for where the document originated. In some cases the local file name is not the same, and we want to include this in the index.\n\t            metadata (dict): Metadata for the document.\n\t        Returns:\n\t            bool: True if the upload was successful, False otherwise.\n\t        \"\"\"\n\t        if os.path.exists(filename) == False:\n\t            logging.error(f\"File {filename} does not exist\")\n\t            return False\n", "        # if file size is more than 50MB, then we extract the text locally and send over with standard API\n\t        if filename.endswith(\".pdf\") and get_file_size_in_MB(filename) > 50:\n\t            elements = partition(filename)\n\t            parts = [str(t) for t in elements if type(t)!=us.documents.elements.Title]\n\t            titles = [str(x) for x in elements if type(x)==us.documents.elements.Title and len(str(x))>20]\n\t            title = titles[0] if len(titles)>0 else 'unknown'\n\t            succeeded = self.index_segments(doc_id=slugify(filename), parts=parts, metadatas=[{}]*len(parts), \n\t                                            doc_metadata=metadata, title=title)\n\t            logging.info(f\"For file {filename}, indexing text only since file size is larger than 50MB\")\n\t            return succeeded\n", "        post_headers = { \n\t            'x-api-key': self.api_key,\n\t            'customer-id': str(self.customer_id),\n\t        }\n\t        files: Any = {\n\t            \"file\": (uri, open(filename, 'rb')),\n\t            \"doc_metadata\": (None, json.dumps(metadata)),\n\t        }  \n\t        response = self.session.post(\n\t            f\"https://{self.endpoint}/upload?c={self.customer_id}&o={self.corpus_id}&d=True\",\n", "            files=files, verify=True, headers=post_headers)\n\t        if response.status_code == 409:\n\t            if self.reindex:\n\t                doc_id = response.json()['details'].split('document id')[1].split(\"'\")[1]\n\t                self.delete_doc(doc_id)\n\t                response = self.session.post(\n\t                    f\"https://{self.endpoint}/upload?c={self.customer_id}&o={self.corpus_id}\",\n\t                    files=files, verify=True, headers=post_headers)\n\t                if response.status_code == 200:\n\t                    logging.info(f\"REST upload for {uri} successful (reindex)\")\n", "                    return True\n\t                else:\n\t                    logging.info(f\"REST upload for {uri} failed with code = {response.status_code}, text = {response.text}\")\n\t                    return True\n\t            return False\n\t        elif response.status_code != 200:\n\t            logging.error(f\"REST upload for {uri} failed with code {response.status_code}, text = {response.text}\")\n\t            return False\n\t        logging.info(f\"REST upload for {uri} succeesful\")\n\t        return True\n", "    def index_url(self, url: str, metadata: Dict[str, Any]) -> bool:\n\t        \"\"\"\n\t        Index a url by rendering it with scrapy-playwright, extracting paragraphs, then uploading to the Vectara corpus.\n\t        Args:\n\t            url (str): URL for where the document originated. \n\t            metadata (dict): Metadata for the document.\n\t        Returns:\n\t            bool: True if the upload was successful, False otherwise.\n\t        \"\"\"\n\t        st = time.time()\n", "        try:\n\t            headers = {\n\t                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n\t                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\"\n\t            }\n\t            response = self.session.get(url, headers=headers, timeout=self.timeout)\n\t            if response.status_code != 200:\n\t                logging.info(f\"Page {url} is unavailable ({response.status_code})\")\n\t                return False\n\t            else:\n", "                content_type = str(response.headers[\"Content-Type\"])\n\t        except Exception as e:\n\t            logging.info(f\"Failed to crawl {url} to get content_type, skipping...\")\n\t            return False\n\t        # read page content: everything is translated into various segments (variable \"elements\") so that we can use index_segment()\n\t        # If PDF then use partition from  \"unstructured.io\" to extract the content\n\t        if content_type == 'application/pdf' or url.endswith(\".pdf\"):\n\t            try:\n\t                response = self.session.get(url, timeout=self.timeout)\n\t                response.raise_for_status()\n", "                fname = 'tmp.pdf'\n\t                with open(fname, 'wb') as f:\n\t                    f.write(response.content)\n\t                elements = partition(fname)\n\t                parts = [str(t) for t in elements if type(t)!=us.documents.elements.Title]\n\t                titles = [str(x) for x in elements if type(x)==us.documents.elements.Title and len(str(x))>20]\n\t                extracted_title = titles[0] if len(titles)>0 else 'unknown'\n\t            except Exception as e:\n\t                logging.info(f\"Failed to crawl {url} to get PDF content with error {e}, skipping...\")\n\t                return False\n", "        # If MD, RST of IPYNB file, then we don't need playwright - can just download content directly and convert to text\n\t        elif url.endswith(\".md\") or url.endswith(\".rst\") or url.lower().endswith(\".ipynb\"):\n\t            response = self.session.get(url, timeout=self.timeout)\n\t            response.raise_for_status()\n\t            dl_content = response.content.decode('utf-8')\n\t            if url.endswith('rst'):\n\t                html_content = docutils.core.publish_string(dl_content, writer_name='html')\n\t            elif url.endswith('md'):\n\t                html_content = markdown.markdown(dl_content)\n\t            elif url.lower().endswith('ipynb'):\n", "                nb = nbformat.reads(dl_content, nbformat.NO_CONVERT)    # type: ignore\n\t                exporter = HTMLExporter()\n\t                html_content, _ = exporter.from_notebook_node(nb)\n\t            extracted_title = url.split('/')[-1]      # no title in these files, so using file name\n\t            text = html_to_text(html_content)\n\t            parts = [text]\n\t        # for everything else, use PlayWright as we may want it to render JS on the page before reading the content\n\t        else:\n\t            try:\n\t                actual_url, html_content = self.fetch_content_with_timeout(url)\n", "                if html_content is None or len(html_content)<3:\n\t                    return False\n\t                if self.detected_language is None:\n\t                    soup = BeautifulSoup(html_content, 'html.parser')\n\t                    body_text = soup.body.get_text()\n\t                    self.detected_language = detect_language(body_text)\n\t                    logging.info(f\"The detected language is {self.detected_language}\")\n\t                url = actual_url\n\t                text, extracted_title = get_content_and_title(html_content, url, self.detected_language)\n\t                parts = [text]\n", "                logging.info(f\"retrieving content took {time.time()-st:.2f} seconds\")\n\t            except Exception as e:\n\t                import traceback\n\t                logging.info(f\"Failed to crawl {url}, skipping due to error {e}, traceback={traceback.format_exc()}\")\n\t                return False\n\t        succeeded = self.index_segments(doc_id=slugify(url), parts=parts, metadatas=[{}]*len(parts), \n\t                                        doc_metadata=metadata, title=extracted_title)\n\t        if succeeded:\n\t            return True\n\t        else:\n", "            return False\n\t    def index_segments(self, doc_id: str, parts: List[str], metadatas: List[Dict[str, Any]], doc_metadata: Dict[str, Any] = {}, title: str = \"\") -> bool:\n\t        \"\"\"\n\t        Index a document (by uploading it to the Vectara corpus) from the set of segments (parts) that make up the document.\n\t        \"\"\"\n\t        document = {}\n\t        document[\"documentId\"] = doc_id\n\t        if title:\n\t            document[\"title\"] = title\n\t        document[\"section\"] = [{\"text\": part, \"metadataJson\": json.dumps(md)} for part,md in zip(parts,metadatas)]  # type: ignore\n", "        if doc_metadata:\n\t            document[\"metadataJson\"] = json.dumps(doc_metadata)\n\t        return self.index_document(document)\n\t    def index_document(self, document: Dict[str, Any]) -> bool:\n\t        \"\"\"\n\t        Index a document (by uploading it to the Vectara corpus) from the document dictionary\n\t        \"\"\"\n\t        api_endpoint = f\"https://{self.endpoint}/v1/index\"\n\t        request = {\n\t            'customer_id': self.customer_id,\n", "            'corpus_id': self.corpus_id,\n\t            'document': document,\n\t        }\n\t        post_headers = { \n\t            'x-api-key': self.api_key,\n\t            'customer-id': str(self.customer_id),\n\t        }\n\t        try:\n\t            data = json.dumps(request)\n\t        except Exception as e:\n", "            logging.info(f\"Can't serialize request {request}, skipping\")   \n\t            return False\n\t        try:\n\t            response = self.session.post(api_endpoint, data=data, verify=True, headers=post_headers)\n\t        except Exception as e:\n\t            logging.info(f\"Exception {e} while indexing document {document['documentId']}\")\n\t            return False\n\t        if response.status_code != 200:\n\t            logging.error(\"REST upload failed with code %d, reason %s, text %s\",\n\t                          response.status_code,\n", "                          response.reason,\n\t                          response.text)\n\t            return False\n\t        result = response.json()\n\t        if \"status\" in result and result[\"status\"] and \\\n\t           (\"ALREADY_EXISTS\" in result[\"status\"][\"code\"] or \\\n\t            (\"CONFLICT\" in result[\"status\"][\"code\"] and \"Indexing doesn't support updating documents\" in result[\"status\"][\"statusDetail\"])):\n\t            if self.reindex:\n\t                logging.info(f\"Document {document['documentId']} already exists, re-indexing\")\n\t                self.delete_doc(document['documentId'])\n", "                response = self.session.post(api_endpoint, data=json.dumps(request), verify=True, headers=post_headers)\n\t                return True\n\t            else:\n\t                logging.info(f\"Document {document['documentId']} already exists, skipping\")\n\t                return False\n\t        if \"status\" in result and result[\"status\"] and \"OK\" in result[\"status\"][\"code\"]:\n\t            return True\n\t        logging.info(f\"Indexing document {document['documentId']} failed, response = {result}\")\n\t        return False\n"]}
{"filename": "core/crawler.py", "chunked_list": ["from omegaconf import OmegaConf, DictConfig\n\tfrom slugify import slugify\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom urllib.parse import urljoin\n\timport logging\n\tfrom typing import Set, Optional, List, Any\n\tfrom core.indexer import Indexer\n\tfrom core.pdf_convert import PDFConverter\n\tfrom core.utils import binary_extensions, doc_extensions\n", "def recursive_crawl(url: str, depth: int, url_regex: List[Any], visited: Optional[Set[str]]=None, session: Optional[requests.Session]=None) -> Set[str]:\n\t    if depth <= 0:\n\t        return set() if visited is None else set(visited)\n\t    if visited is None:\n\t        visited = set()\n\t    if session is None:\n\t        session = requests.Session()\n\t    # For binary files - we don't extract links from them, nor are they included in the crawled URLs list\n\t    # for document files (like PPT, DOCX, etc) we don't extract links from the, but they ARE included in the crawled URLs list\n\t    url_without_fragment = url.split(\"#\")[0]\n", "    if any([url_without_fragment.endswith(ext) for ext in binary_extensions]):\n\t        return visited\n\t    visited.add(url)\n\t    if any([url_without_fragment.endswith(ext) for ext in doc_extensions]):\n\t        return visited\n\t    try:\n\t        response = session.get(url)\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n\t        # Find all anchor tags and their href attributes\n\t        new_urls = [urljoin(url, link[\"href\"]) for link in soup.find_all(\"a\") if \"href\" in link.attrs]\n", "        new_urls = [u for u in new_urls if u not in visited and u.startswith('http') and any([r.match(u) for r in url_regex])]\n\t        new_urls = list(set(new_urls))\n\t        visited.update(new_urls)\n\t        for new_url in new_urls:\n\t            visited = recursive_crawl(new_url, depth-1, url_regex, visited, session)\n\t    except Exception as e:\n\t        logging.info(f\"Error {e} in recursive_crawl for {url}\")\n\t        pass\n\t    return set(visited)\n\tclass Crawler(object):\n", "    \"\"\"\n\t    Base class for a crawler that indexes documents to a Vectara corpus.\n\t    Args:\n\t        endpoint (str): Endpoint for the Vectara API.\n\t        customer_id (str): ID of the Vectara customer.\n\t        token (str): Bearer JWT token\n\t        corpus_id (int): ID of the Vectara corpus to index to.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        cfg: OmegaConf,\n\t        endpoint: str,\n\t        customer_id: str,\n\t        corpus_id: int,\n\t        api_key: str,\n\t    ) -> None:\n\t        self.cfg: DictConfig = DictConfig(cfg)\n\t        reindex = self.cfg.vectara.get(\"reindex\", False)\n\t        self.indexer = Indexer(cfg, endpoint, customer_id, corpus_id, api_key, reindex)\n\t    def url_to_file(self, url: str, title: str) -> str:\n", "        \"\"\"\n\t        Crawl a single webpage and create a PDF file to reflect its rendered content.\n\t        Args:\n\t            url (str): URL of the page to crawl.\n\t            title (str): Title to use in case HTML does not have its own title.\n\t        Returns:\n\t            str: Name of the PDF file created.\n\t        \"\"\"\n\t        # first verify the URL is valid\n\t        headers = {\n", "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n\t            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n\t            \"Accept-Language\": \"en-US,en;q=0.5\",\n\t            \"Accept-Encoding\": \"gzip, deflate\",\n\t            \"Connection\": \"keep-alive\",\n\t            \"Upgrade-Insecure-Requests\": \"1\",\n\t            \"Sec-Fetch-Dest\": \"document\",\n\t            \"Sec-Fetch-Mode\": \"navigate\",\n\t            \"Sec-Fetch-Site\": \"none\",\n\t            \"Sec-Fetch-User\": \"?1\",\n", "            \"Cache-Control\": \"max-age=0\",\n\t        }\n\t        response = requests.get(url, headers=headers)\n\t        if response.status_code != 200:\n\t            if response.status_code == 404:\n\t                raise Exception(f\"Error 404 - URL not found: {url}\")\n\t            elif response.status_code == 401:\n\t                raise Exception(f\"Error 403 - Unauthorized: {url}\")\n\t            elif response.status_code == 403:\n\t                raise Exception(f\"Error 403 - Access forbidden: {url}\")\n", "            elif response.status_code == 405:\n\t                raise Exception(f\"Error 405 - Method not allowed: {url}\")\n\t            else:\n\t                raise Exception(\n\t                    f\"Invalid URL: {url} (status code={response.status_code}, reason={response.reason})\"\n\t                )\n\t        if title is None:\n\t            soup = BeautifulSoup(response.text, \"html.parser\")\n\t            title = soup.title.string\n\t        # convert to local file (PDF)\n", "        filename = slugify(url) + \".pdf\"\n\t        if not PDFConverter(use_pdfkit=False).from_url(url, filename, title=title):\n\t            raise Exception(f\"Failed to convert {url} to PDF\")\n\t        return filename\n\t    def crawl(self) -> None:\n\t        raise Exception(\"Not implemented\")\n"]}
{"filename": "core/__init__.py", "chunked_list": []}
{"filename": "core/utils.py", "chunked_list": ["from bs4 import BeautifulSoup\n\timport requests\n\tfrom urllib.parse import urlparse, urlunparse, ParseResult\n\timport re\n\tfrom langdetect import detect\n\tfrom typing import List, Set\n\timport os\n\timg_extensions = [\"gif\", \"jpeg\", \"jpg\", \"mp3\", \"mp4\", \"png\", \"svg\", \"bmp\", \"eps\", \"ico\"]\n\tdoc_extensions = [\"doc\", \"docx\", \"ppt\", \"pptx\", \"xls\", \"xlsx\", \"pdf\", \"ps\"]\n\tarchive_extensions = [\"zip\", \"gz\", \"tar\", \"bz2\", \"7z\", \"rar\"]\n", "binary_extensions = archive_extensions + img_extensions + doc_extensions\n\tdef html_to_text(html: str) -> str:\n\t    soup = BeautifulSoup(html, features='html.parser')\n\t    return soup.get_text()\n\tdef create_session_with_retries(retries: int = 3) -> requests.Session:\n\t    session = requests.Session()\n\t    adapter = requests.adapters.HTTPAdapter(max_retries=retries)\n\t    session.mount('http://', adapter)\n\t    session.mount('https://', adapter)\n\t    return session\n", "def remove_anchor(url: str) -> str:\n\t    parsed = urlparse(url)\n\t    url_without_anchor = urlunparse(parsed._replace(fragment=\"\"))\n\t    return url_without_anchor\n\tdef normalize_url(url: str) -> str:\n\t    \"\"\"Normalize a URL by removing 'www', and query parameters.\"\"\"    \n\t    # Prepend with 'http://' if URL has no scheme\n\t    if '://' not in url:\n\t        url = 'http://' + url\n\t    p = urlparse(url)\n", "    # Remove 'www.'\n\t    netloc = p.netloc.replace('www.', '')\n\t    # Remove query parameters\n\t    path = p.path.split('?', 1)[0]\n\t    # Reconstruct URL with scheme, without 'www', and query parameters\n\t    return ParseResult(p.scheme, netloc, path, '', '', '').geturl()\n\tdef clean_urls(urls: Set[str]) -> List[str]:\n\t    return list(set(normalize_url(url) for url in urls))\n\tdef clean_email_text(text: str) -> str:\n\t    \"\"\"\n", "    Clean the text email by removing any unnecessary characters and indentation.\n\t    This function can be extended to clean emails in other ways.\n\t    \"\"\"    \n\t    cleaned_text = text.strip()\n\t    cleaned_text = re.sub(r\"[<>]+\", \"\", cleaned_text, flags=re.MULTILINE)\n\t    return cleaned_text\n\tdef detect_language(text: str) -> str:\n\t    try:\n\t        lang = detect(text)\n\t        return str(lang)\n", "    except Exception as e:\n\t        print(f\"Language detection failed with error: {e}\")\n\t        return \"en\"  # Default to English in case of errors\n\tdef get_file_size_in_MB(file_path: str) -> float:\n\t    file_size_bytes = os.path.getsize(file_path)\n\t    file_size_MB = file_size_bytes / (1024 * 1024)    \n\t    return file_size_MB\n"]}
{"filename": "core/pdf_convert.py", "chunked_list": ["import logging\n\timport subprocess\n\timport pdfkit\n\tclass PDFConverter:\n\t    \"\"\"\n\t    Helper class for converting web pages to PDF.\n\t    \"\"\"\n\t    def __init__(self, use_pdfkit: bool = False):\n\t        self.use_pdfkit = use_pdfkit\n\t    def from_url(self, url: str, filename: str, title: str = \"No Title\") -> bool:\n", "        \"\"\"\n\t        Convert a webpage to PDF and save it to a file.\n\t        Args:\n\t            url (str): The URL of the webpage to convert.\n\t            filename (str): The name of the file to save the PDF to.\n\t        Returns:\n\t            name of file\n\t        \"\"\"\n\t        try:\n\t            if self.use_pdfkit:\n", "                pdfkit.from_url(\n\t                    url=url,\n\t                    output_path=filename,\n\t                    verbose=False,\n\t                    options={'load-error-handling': 'ignore'}\n\t                )\n\t            else:\n\t                cmd = [\"wkhtmltopdf\", \"--quiet\", \"--load-error-handling\", \"ignore\", '--title', title, url, filename]\n\t                try:\n\t                    subprocess.call(cmd, timeout=120)\n", "                except subprocess.TimeoutExpired:\n\t                    logging.warning(f\"Timeout converting {url} to PDF\")\n\t                    return False\n\t            return True\n\t        except Exception as e:\n\t            logging.error(f\"Error {e} converting {url} to PDF\")\n\t            return False\n"]}
{"filename": "crawlers/s3_crawler.py", "chunked_list": ["import logging\n\timport pathlib\n\tfrom slugify import slugify\n\timport boto3\n\timport os\n\tfrom typing import List, Tuple\n\tfrom core.crawler import Crawler\n\tdef list_files_in_s3_bucket(bucket_name: str, prefix: str) -> List[str]:\n\t    \"\"\"\n\t    List all files in an S3 bucket.\n", "    args:\n\t        bucket_name: name of the S3 bucket\n\t        prefix: the \"folder\" on S3 to list files from\n\t    \"\"\"\n\t    s3 = boto3.client('s3')\n\t    result = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n\t    files = []\n\t    for content in result.get('Contents', []):\n\t        files.append(content['Key'])\n\t    while result.get('IsTruncated', False):\n", "        continuation_key = result.get('NextContinuationToken')\n\t        result = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, ContinuationToken=continuation_key)\n\t        for content in result.get('Contents', []):\n\t            files.append(content['Key'])\n\t    return files\n\tdef split_s3_uri(s3_uri: str) -> Tuple[str, str]:\n\t    \"\"\"\n\t    Split an S3 URI into bucket and object key.\n\t    \"\"\"\n\t    bucket_and_object = s3_uri[len(\"s3://\"):].split(\"/\", 1)\n", "    bucket = bucket_and_object[0]\n\t    object_key = bucket_and_object[1] if len(bucket_and_object) > 1 else \"\"\n\t    return bucket, object_key\n\tclass S3Crawler(Crawler):\n\t    \"\"\"\n\t    Crawler for S3 files.\n\t    \"\"\"\n\t    def crawl(self) -> None:\n\t        folder = self.cfg.s3_crawler.s3_path\n\t        extensions = self.cfg.s3_crawler.extensions\n", "        os.environ['AWS_ACCESS_KEY_ID'] = self.cfg.s3_crawler.aws_access_key_id\n\t        os.environ['AWS_SECRET_ACCESS_KEY'] = self.cfg.s3_crawler.aws_secret_access_key\n\t        bucket, key = split_s3_uri(folder)\n\t        s3_files = list_files_in_s3_bucket(bucket, key)\n\t        # process all files\n\t        s3 = boto3.client('s3')\n\t        for s3_file in s3_files:\n\t            file_extension = pathlib.Path(s3_file).suffix\n\t            if file_extension in extensions or \"*\" in extensions:\n\t                extension = s3_file.split('.')[-1]\n", "                local_fname = slugify(s3_file.replace(extension, ''), separator='_') + '.' + extension\n\t                s3.download_file(bucket, s3_file, local_fname)\n\t                url = f's3://{bucket}/{s3_file}'\n\t                metadata = {\n\t                    'source': 's3',\n\t                    'title': s3_file,\n\t                    'url': url\n\t                }\n\t                self.indexer.index_file(filename=local_fname, uri=url, metadata=metadata)\n"]}
{"filename": "crawlers/notion_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\tfrom omegaconf import OmegaConf\n\tfrom notion_client import Client\n\tfrom typing import Any, List, Dict\n\tdef get_text_from_block(block: Any) -> str:\n\t    \"\"\"\n\t    Recursively extract all text from a block.\n\t    \"\"\"\n\t    if block[\"type\"] == \"paragraph\":\n", "        text = \" \".join([text[\"plain_text\"] for text in block[\"paragraph\"][\"rich_text\"]])\n\t    else:\n\t        text = \"\"\n\t    if \"children\" in block:\n\t        for child_block in block[\"children\"]:\n\t            text += \"\\n\" + get_text_from_block(child_block)\n\t    return text\n\tdef list_all_pages(notion: Any) -> List[Dict[str, Any]]:\n\t    \"\"\"\n\t    List all pages in a Notion workspace.\n", "    \"\"\"\n\t    pages = []\n\t    has_more = True\n\t    start_cursor = None\n\t    while has_more:\n\t        list_pages_response = notion.search(filter={\"property\": \"object\", \"value\": \"page\"}, start_cursor=start_cursor)\n\t        for page in list_pages_response[\"results\"]:\n\t            pages.append(page)\n\t        has_more = list_pages_response[\"has_more\"]\n\t        start_cursor = list_pages_response[\"next_cursor\"]\n", "    return pages\n\tclass NotionCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n\t        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n\t        self.notion_api_key = self.cfg.notion_crawler.notion_api_key\n\t    def crawl(self) -> None:\n\t        notion = Client(auth=self.notion_api_key)\n\t        pages = list_all_pages(notion)\n\t        logging.info(f\"Found {len(pages)} pages in Notion.\")\n\t        for page in pages:\n", "            page_id = page[\"id\"]\n\t            title_obj = page.get('properties', {}).get('title', {}).get('title', [])\n\t            if len(title_obj)>0:\n\t                title = title_obj[0][\"plain_text\"]\n\t            else:\n\t                title = None\n\t            # Extract all text blocks from the page\n\t            try:\n\t                blocks = notion.blocks.children.list(page_id).get(\"results\")        # type: ignore\n\t            except Exception as e:\n", "                logging.error(f\"Failed to get blocks for page {page['url']}: {e}\")\n\t                continue\n\t            segments = []\n\t            metadatas = []\n\t            for block in blocks:\n\t                text = get_text_from_block(block)\n\t                if len(text)>2:\n\t                    segments.append(text)\n\t                    metadatas.append({'block_id': block['id'], 'block_type': block['type']})\n\t            doc_id = page['url']\n", "            if len(segments)>0:\n\t                logging.info(f\"Indexing {len(segments)} segments in page {doc_id}\")\n\t                succeeded = self.indexer.index_segments(doc_id, segments, metadatas, doc_metadata={'source': 'notion', 'title': title, 'url': page['url']})\n\t                if succeeded:\n\t                    logging.info(f\"Indexed notion page {doc_id}\")\n\t                else:\n\t                    logging.info(f\"Indexing failed for notion page {doc_id}\")\n\t            else:\n\t                logging.info(f\"No text found in notion page {doc_id}\")\n"]}
{"filename": "crawlers/mediawiki_crawler.py", "chunked_list": ["import logging\n\timport json\n\timport urllib.parse\n\timport time\n\tfrom datetime import datetime, timedelta  \n\tfrom mwviews.api import PageviewsClient\n\tfrom core.crawler import Crawler\n\tfrom core.utils import create_session_with_retries\n\tclass MediawikiCrawler(Crawler):\n\t    def crawl(self) -> None:\n", "        api_url = self.cfg.mediawiki_crawler.api_url\n\t        project = self.cfg.mediawiki_crawler.project\n\t        n_pages = self.cfg.mediawiki_crawler.n_pages\n\t        session = create_session_with_retries()\n\t        if n_pages > 1000:\n\t            n_pages = 1000\n\t            logging.info(f\"n_pages is too large, setting to 1000\")\n\t        metrics_date = datetime.now() - timedelta(days=7)\n\t        # Get most viewed pages\n\t        p = PageviewsClient(user_agent=\"crawler@example.com\")\n", "        year, month, day = metrics_date.strftime(\"%Y-%m-%d\").split(\"-\")\n\t        titles = [v['article'] for v in p.top_articles(project, limit=n_pages, year=year, month=month, day=day)]\n\t        logging.info(f\"indexing {len(titles)} top titles from {project}\")\n\t        # Process the pages in batches\n\t        for title in titles:\n\t            time.sleep(1)\n\t            params = {'action': 'query', 'prop': 'info|revisions', 'titles': title, 'inprop': 'url', 'rvprop': 'timestamp', 'format': 'json'}\n\t            response = session.get(api_url, params=params).json()\n\t            page_id = list(response['query']['pages'].keys())[0]\n\t            if int(page_id) <= 0:\n", "                continue\n\t            page_url = response['query']['pages'][page_id]['fullurl']\n\t            last_revision = response['query']['pages'][page_id]['revisions'][0]\n\t            last_editor = last_revision.get('user', 'unknown')\n\t            last_edited_at = last_revision['timestamp']\n\t            params = {'action': 'query', 'prop': 'extracts', 'titles': title, 'format': 'json', 'explaintext': 1}\n\t            response = session.get(api_url, params=params).json()\n\t            page_id = list(response[\"query\"][\"pages\"].keys())[0]\n\t            page_content = response[\"query\"][\"pages\"][page_id][\"extract\"]\n\t            if page_content is None or len(page_content) < 3:\n", "                continue                    # skip pages without content\n\t            logging.info(f\"Indexing page with {title}: url={page_url}\")\n\t            # Index the page into Vectara\n\t            document = {\n\t                \"documentId\": title,\n\t                \"title\": title,\n\t                \"description\": \"\",\n\t                \"metadataJson\": json.dumps({\n\t                    \"url\": f\"https://en.wikipedia.org/wiki/{urllib.parse.quote(title)}\",\n\t                    \"last_edit_time\": last_edited_at,\n", "                    \"last_edit_user\": last_editor,\n\t                    \"source\": \"mediawiki\",\n\t                }),\n\t                \"section\": [\n\t                    {\n\t                        \"text\": page_content,\n\t                    }\n\t                ]\n\t            }\n\t            succeeded = self.indexer.index_document(document)\n", "            if not succeeded:\n\t                logging.info(f\"Failed to index page {page_id}: url={page_url}, title={title}\")\n"]}
{"filename": "crawlers/jira_crawler.py", "chunked_list": ["import logging\n\timport requests\n\timport json\n\tfrom core.crawler import Crawler\n\tfrom core.utils import create_session_with_retries\n\tclass JiraCrawler(Crawler):\n\t    def crawl(self) -> None:\n\t        self.jira_headers = { \"Accept\": \"application/json\" }\n\t        self.jira_auth = (self.cfg.jira_crawler.jira_username, self.cfg.jira_crawler.jira_password)\n\t        session = create_session_with_retries()\n", "        issue_count = 0\n\t        startAt = 0\n\t        res_cnt = 100\n\t        while True:\n\t            jira_query_url = f\"{self.cfg.jira_crawler.jira_base_url}/rest/api/3/search?jql={self.cfg.jira_crawler.jira_jql}&fields=*all&maxResults={res_cnt}&startAt={startAt}\"\n\t            jira_response = session.get(jira_query_url, headers=self.jira_headers, auth=self.jira_auth)\n\t            jira_response.raise_for_status()\n\t            jira_data = jira_response.json()\n\t            actual_cnt = len(jira_data[\"issues\"])\n\t            if actual_cnt > 0:\n", "                for issue in jira_data[\"issues\"]:\n\t                    # Collect as much metadata as possible\n\t                    metadata = {}\n\t                    metadata[\"project\"] = issue[\"fields\"][\"project\"][\"name\"]\n\t                    metadata[\"issueType\"] = issue[\"fields\"][\"issuetype\"][\"name\"]\n\t                    metadata[\"status\"] = issue[\"fields\"][\"status\"][\"name\"]\n\t                    metadata[\"priority\"] = issue[\"fields\"][\"priority\"][\"name\"]\n\t                    metadata[\"reporter\"] = issue[\"fields\"][\"reporter\"][\"displayName\"]\n\t                    metadata[\"assignee\"] = issue[\"fields\"][\"assignee\"][\"displayName\"] if issue[\"fields\"][\"assignee\"] else None\n\t                    metadata[\"created\"] = issue[\"fields\"][\"created\"]\n", "                    metadata[\"updated\"] = issue[\"fields\"][\"updated\"]\n\t                    metadata[\"resolved\"] = issue[\"fields\"][\"resolutiondate\"] if \"resolutiondate\" in issue[\"fields\"] else None\n\t                    metadata[\"labels\"] = issue[\"fields\"][\"labels\"]\n\t                    metadata[\"source\"] = \"jira\"\n\t                    metadata[\"url\"] = f\"{self.cfg.jira_crawler.jira_base_url}/browse/{issue['key']}\"\n\t                    # Create a Vectara document with the metadata and the issue fields\n\t                    title = issue[\"fields\"][\"summary\"]\n\t                    document = {\n\t                        \"documentId\": issue[\"key\"],\n\t                        \"title\": title,\n", "                        \"metadataJson\": json.dumps(metadata),\n\t                        \"section\": []\n\t                    }\n\t                    comments_data = issue[\"fields\"][\"comment\"][\"comments\"]\n\t                    comments = []\n\t                    for comment in comments_data:\n\t                        author = comment[\"author\"][\"displayName\"]\n\t                        try:\n\t                            comment_body = comment[\"body\"][\"content\"][0][\"content\"][0][\"text\"]\n\t                            comments.append(f'{author}: {comment_body}')\n", "                        except Exception as e:\n\t                            continue\n\t                    try:\n\t                        description = issue[\"fields\"][\"description\"][\"content\"][0][\"content\"][0][\"text\"]\n\t                    except Exception as e:\n\t                        description = str(issue['key'])\n\t                    document[\"section\"] = [\n\t                        {\n\t                            \"title\": \"Comments\",\n\t                            \"text\": \"\\n\\n\".join(comments)\n", "                        },\n\t                        {\n\t                            \"title\": \"Description\",\n\t                            \"text\": description\n\t                        },\n\t                        {\n\t                            \"title\": \"Status\",\n\t                            \"text\": f'Issue {title} is {issue[\"fields\"][\"status\"][\"name\"]}'\n\t                        }\n\t                    ]\n", "                    succeeded = self.indexer.index_document(document)\n\t                    if succeeded:\n\t                        logging.info(f\"Indexed issue {document['documentId']}\")\n\t                        issue_count += 1\n\t                    else:\n\t                        logging.info(f\"Error indexing issue {document['documentId']}\")\n\t                startAt = startAt + actual_cnt\n\t            else:\n\t                break\n\t        logging.info(f\"Finished indexing all issues (total={issue_count})\")\n"]}
{"filename": "crawlers/database_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\timport sqlalchemy\n\timport pandas as pd\n\timport unicodedata\n\tclass DatabaseCrawler(Crawler):\n\t    def crawl(self) -> None: \n\t        db_url = self.cfg.database_crawler.db_url\n\t        db_table = self.cfg.database_crawler.db_table\n\t        text_columns = list(self.cfg.database_crawler.text_columns)\n", "        metadata_columns = list(self.cfg.database_crawler.metadata_columns)\n\t        select_condition = self.cfg.database_crawler.get(\"select_condition\", None)\n\t        doc_id_columns = list(self.cfg.database_crawler.get(\"doc_id_columns\", None))\n\t        all_columns = text_columns + metadata_columns\n\t        if select_condition:\n\t            query = f'SELECT {\",\".join(all_columns)} FROM {db_table} WHERE {select_condition}'\n\t        else:\n\t            query = f'SELECT {\",\".join(all_columns)} FROM {db_table}'\n\t        conn = sqlalchemy.create_engine(db_url).connect()\n\t        df = pd.read_sql_query(sqlalchemy.text(query), conn)\n", "        logging.info(f\"indexing {len(df)} rows from the database using query: '{query}'\")\n\t        def index_df(doc_id: str, title: str, df: pd.DataFrame) -> None:\n\t            parts = []\n\t            metadatas = []\n\t            for _, row in df.iterrows():\n\t                text = ' - '.join(str(x) for x in row[text_columns].tolist() if x)\n\t                parts.append(unicodedata.normalize('NFD', text))\n\t                metadatas.append({column: row[column] for column in metadata_columns})\n\t            logging.info(f\"Indexing df for '{doc_id}' with ({len(df)}) rows\")\n\t            self.indexer.index_segments(doc_id, parts, metadatas, title=title, doc_metadata = {'source': 'database'})\n", "        if doc_id_columns:\n\t            grouped = df.groupby(doc_id_columns)\n\t            for name, group in grouped:\n\t                gr_str = name if type(name)==str else ' - '.join(str(x) for x in name)\n\t                index_df(doc_id=gr_str, title=gr_str, df=group)\n\t        else:\n\t            rows_per_chunk = self.cfg.database_crawler.get(\"rows_per_chunk\", 500)\n\t            for inx in range(0, df.shape[0], rows_per_chunk):\n\t                sub_df = df[inx: inx+rows_per_chunk]\n\t                name = f'rows {inx}-{inx+rows_per_chunk-1}'\n", "                index_df(doc_id=name, title=name, df=sub_df)\n"]}
{"filename": "crawlers/folder_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\timport os\n\timport pathlib\n\timport time\n\tclass FolderCrawler(Crawler):\n\t    def crawl(self) -> None:\n\t        folder = \"/home/vectara/data\"\n\t        extensions = self.cfg.folder_crawler.extensions\n\t        # Walk the directory and upload files with the specified extension to Vectara\n", "        logging.info(f\"indexing files in {self.cfg.folder_crawler.path} with extensions {extensions}\")\n\t        source = self.cfg.folder_crawler.source\n\t        for root, _, files in os.walk(folder):\n\t            for file in files:\n\t                file_extension = pathlib.Path(file).suffix\n\t                if file_extension in extensions or \"*\" in extensions:\n\t                    file_path = os.path.join(root, file)\n\t                    file_name = os.path.relpath(file_path, folder)\n\t                    file_metadata = {\n\t                        'created_at': time.strftime('%Y-%m-%dT%H:%M:%S', time.gmtime(os.path.getctime(file_path))),\n", "                        'modified_at': time.strftime('%Y-%m-%dT%H:%M:%S', time.gmtime(os.path.getmtime(file_path))),\n\t                        'file_size': os.path.getsize(file_path),\n\t                        'source': source,\n\t                        'title': file_name,\n\t#                        'url': file_name\n\t                    }\n\t                    self.indexer.index_file(filename=file_path, uri=file_name, metadata=file_metadata)\n"]}
{"filename": "crawlers/__init__.py", "chunked_list": []}
{"filename": "crawlers/docs_crawler.py", "chunked_list": ["from core.crawler import Crawler\n\tfrom bs4 import BeautifulSoup\n\timport logging\n\tfrom urllib.parse import urljoin, urlparse\n\timport re\n\tfrom collections import deque\n\tfrom ratelimiter import RateLimiter\n\tfrom core.utils import create_session_with_retries, binary_extensions\n\tfrom typing import Tuple, Set\n\tclass DocsCrawler(Crawler):\n", "    def concat_url_and_href(self, url: str, href: str) -> str:\n\t        if href.startswith('http'):\n\t            return href\n\t        else:\n\t            if 'index.html?' in href:\n\t                href = href.replace('index.html?', '/')     # special fix for Spark docs\n\t            joined = urljoin(url, href)\n\t            return joined\n\t    def get_url_content(self, url: str) -> Tuple[str, BeautifulSoup]:\n\t        headers = {\n", "            'User-Agent': 'Mozilla/5.0',\n\t            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n\t        }\n\t        with self.rate_limiter:\n\t            response = self.session.get(url, headers=headers)\n\t        if response.status_code != 200:\n\t            logging.info(f\"Failed to crawl {url}, response code is {response.status_code}\")\n\t            return None, None\n\t        # check for refresh redirect        \n\t        soup = BeautifulSoup(response.content, 'html.parser')\n", "        meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})\n\t        if meta_refresh:\n\t            href = meta_refresh['content'].split('url=')[-1]            # type: ignore\n\t            url = self.concat_url_and_href(url, href)\n\t            response = self.session.get(url, headers=headers)\n\t            if response.status_code != 200:\n\t                logging.info(f\"Failed to crawl redirect {url}, response code is {response.status_code}\")\n\t                return None, None\n\t        page_content = BeautifulSoup(response.content, 'lxml')\n\t        return url, page_content\n", "    def collect_urls(self, base_url: str) -> None:\n\t        new_urls = deque([base_url])\n\t        # Crawl each URL in the queue\n\t        while len(new_urls):\n\t            n_urls = len(self.crawled_urls)\n\t            if n_urls>0 and n_urls%100==0:\n\t                logging.info(f\"Currently have {n_urls} crawled urls identified\")\n\t            # pop the left-most URL from new_urls\n\t            url = new_urls.popleft()\n\t            try:\n", "                url, page_content = self.get_url_content(url)\n\t                self.crawled_urls.add(url)\n\t                # Find all the new URLs in the page's content and add them into the queue\n\t                if page_content:\n\t                    for link in page_content.find_all('a'):\n\t                        href = link.get('href')\n\t                        if href is None:\n\t                            continue\n\t                        abs_url = self.concat_url_and_href(url, href)\n\t                        if ((any([r.match(abs_url) for r in self.pos_regex])) and                           # match any of the positive regexes\n", "                            (not any([r.match(abs_url) for r in self.neg_regex])) and                       # don't match any of the negative regexes\n\t                            (abs_url.startswith(\"http\")) and                                                # starts with http/https\n\t                            (abs_url not in self.ignored_urls) and                                          # not previously ignored    \n\t                            (len(urlparse(abs_url).fragment)==0) and                                        # does not have fragment\n\t                            (any([abs_url.endswith(ext) for ext in self.extensions_to_ignore])==False)):    # not any of the specified extensions to ignore\n\t                                # add URL if needed\n\t                                if abs_url not in self.crawled_urls and abs_url not in new_urls:\n\t                                    new_urls.append(abs_url)\n\t                        else:\n\t                            self.ignored_urls.add(abs_url)\n", "            except Exception as e:\n\t                import traceback\n\t                logging.info(f\"Error crawling {url}: {e}, traceback={traceback.format_exc()}\")\n\t                continue\n\t    def crawl(self) -> None:\n\t        self.crawled_urls: Set[str] = set()\n\t        self.ignored_urls: Set[str] = set()\n\t        self.extensions_to_ignore = list(set(self.cfg.docs_crawler.extensions_to_ignore + binary_extensions))\n\t        self.pos_regex = [re.compile(r) for r in self.cfg.docs_crawler.pos_regex] if self.cfg.docs_crawler.pos_regex else []\n\t        self.neg_regex = [re.compile(r) for r in self.cfg.docs_crawler.neg_regex] if self.cfg.docs_crawler.neg_regex else []\n", "        self.session = create_session_with_retries()\n\t        self.rate_limiter = RateLimiter(max_calls=2, period=1)\n\t        for base_url in self.cfg.docs_crawler.base_urls:\n\t            self.collect_urls(base_url)\n\t        logging.info(f\"Found {len(self.crawled_urls)} urls in {self.cfg.docs_crawler.base_urls}\")\n\t        source = self.cfg.docs_crawler.docs_system\n\t        for url in self.crawled_urls:\n\t            self.indexer.index_url(url, metadata={'url': url, 'source': source})\n\t            logging.info(f\"{source.capitalize()} Crawler: finished indexing {url}\")\n"]}
{"filename": "crawlers/hackernews_crawler.py", "chunked_list": ["import requests\n\timport logging\n\tfrom core.crawler import Crawler\n\timport os\n\tfrom slugify import slugify\n\tfrom core.utils import html_to_text, create_session_with_retries\n\tfrom typing import List\n\tdef get_comments(kids: List[str], entrypoint: str) -> List[str]:\n\t    comments = []\n\t    for kid in kids:\n", "        try:\n\t            response = requests.get(entrypoint + 'item/{}.json'.format(kid))\n\t            comment = response.json()\n\t        except Exception as e:\n\t            logging.info(f\"Error retrieving comment {kid}, e={e}\")\n\t            comment = None\n\t        if comment is not None and comment.get('type', '') == 'comment':\n\t            comments.append(html_to_text(comment.get('text', '')))\n\t            sub_kids = comment.get('kids', [])\n\t            if len(sub_kids)>0:\n", "                comments += get_comments(sub_kids, entrypoint)\n\t    return comments\n\tclass HackernewsCrawler(Crawler):\n\t    def crawl(self) -> None:\n\t        N_ARTICLES = self.cfg.hackernews_crawler.max_articles\n\t        # URL for the Hacker News API\n\t        entrypoint = 'https://hacker-news.firebaseio.com/v0/'\n\t        # Retrieve the IDs of the top N_ARTICLES stories\n\t        session = create_session_with_retries()\n\t        resp1= session.get(entrypoint + 'topstories.json')\n", "        resp2 = session.get(entrypoint + 'newstories.json')\n\t        resp3 = session.get(entrypoint + 'beststories.json')\n\t        top_ids = list(set(list(resp1.json()) + list(resp2.json()) + list(resp3.json())))[:N_ARTICLES]\n\t        num_ids = len(top_ids)\n\t        logging.info(f\"Crawling {num_ids} stories\")\n\t        # Retrieve the details of each story\n\t        for n_id, id in enumerate(top_ids):\n\t            if n_id % 20 == 0:\n\t                logging.info(f\"Crawled {n_id} stories so far\")\n\t            try:\n", "                response = session.get(entrypoint + 'item/{}.json'.format(id))\n\t                story = response.json()\n\t                url = story.get('url', None)\n\t                if url is None:\n\t                    continue\n\t                title = html_to_text(story.get('title', ''))\n\t                text = story.get('text', None)\n\t                if text:\n\t                    fname = slugify(url) + \".html\"\n\t                    with open(fname, 'w') as f:\n", "                        f.write(text)\n\t                    self.indexer.index_file(fname, uri=url, metadata={'title': title})\n\t                    os.remove(fname)\n\t                else:\n\t                    metadata = {'source': 'hackernews', 'title': title}\n\t                    self.indexer.index_url(url, metadata=metadata)\n\t            except Exception as e:\n\t                import traceback\n\t                logging.error(f\"Error crawling story {url}, error={e}, traceback={traceback.format_exc()}\")\n"]}
{"filename": "crawlers/arxiv_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\timport arxiv\n\tfrom core.utils import create_session_with_retries\n\tdef validate_category(category: str) -> bool:\n\t    valid_categories = [\n\t        \"cs\", \"econ\", \"q-fin\",\"stat\",\n\t        \"math\", \"math-ph\", \"q-bio\", \"stat-mech\",\n\t        \"physics\", \"astro-ph\", \"cond-mat\", \"gr-qc\", \"hep-ex\", \"hep-lat\", \"hep-ph\", \n\t        \"hep-th\", \"nucl-ex\", \"nucl-th\", \"physics-ao-ph\", \"physics-ao-pl\", \"physics-ao-po\",\n", "        \"physics-ao-ps\", \"physics-app-ph\",\n\t        \"quant-ph\"\n\t    ]\n\t    return category in valid_categories\n\tclass ArxivCrawler(Crawler):\n\t    def get_citations(self, arxiv_id: str) -> int:\n\t        \"\"\"\n\t        Retrieves the number of citations for a given paper from Semantic Scholar API based on its arXiv ID.\n\t        Parameters:\n\t        arxiv_id (str): The arXiv ID of the paper.\n", "        Returns:\n\t        int: Number of citations if the paper exists and the request was successful, otherwise -1.\n\t        If an exception occurs during the request, it also returns -1 and logs the exception.\n\t        \"\"\"\n\t        base_url = \"https://api.semanticscholar.org/v1/paper/arXiv:\"\n\t        arxiv_id = arxiv_id.split('v')[0]   # remove any 'v1' or 'v2', etc from the ending, if it exists\n\t        try:\n\t            response = self.session.get(base_url + arxiv_id)\n\t            if response.status_code != 200:\n\t                return -1\n", "            paper_info = response.json()\n\t            paper_id = paper_info.get('paperId')\n\t            base_url = \"https://api.semanticscholar.org/v1/paper/\"\n\t            response = self.session.get(base_url + paper_id)\n\t            if response.status_code == 200:\n\t                paper_info = response.json()\n\t                n_citations = len(paper_info.get(\"citations\"))\n\t                return n_citations\n\t            else:\n\t                return -1\n", "        except Exception as e:\n\t            logging.info(f\"Error parsing response from arxiv API: {e}, response={response.text}\")\n\t            return -1\n\t    def crawl(self) -> None:\n\t        n_papers = self.cfg.arxiv_crawler.n_papers\n\t        query_terms = self.cfg.arxiv_crawler.query_terms\n\t        year = self.cfg.arxiv_crawler.start_year\n\t        category = self.cfg.arxiv_crawler.arxiv_category\n\t        if not validate_category(category):\n\t            logging.info(f\"Invalid arxiv category: {category}, please check the config file\")\n", "            exit(1)\n\t        # setup requests session and mount adapter to retry requests\n\t        self.session = create_session_with_retries()\n\t        # define query for arxiv: search for the query in the \"computer science\" (cs.*) category\n\t        query = f\"cat:{category}.* AND \" + ' AND '.join([f'all:{q}' for q in query_terms])\n\t        if self.cfg.arxiv_crawler.sort_by == 'citations':\n\t            # for sort by n_citations We pull 100x papers so that we can get citations and enough highly cited paper.\n\t            search = arxiv.Search(\n\t                query = query,\n\t                max_results = n_papers*100,\n", "                sort_by = arxiv.SortCriterion.Relevance,\n\t                sort_order = arxiv.SortOrder.Descending\n\t            )\n\t        else:\n\t            search = arxiv.Search(\n\t                query = query,\n\t                max_results = n_papers,\n\t                sort_by = arxiv.SortCriterion.submittedDate,\n\t                sort_order = arxiv.SortOrder.Descending\n\t            )\n", "        # filter by publication year\n\t        papers = []\n\t        try:\n\t            for result in search.results():\n\t                date = result.published.date()\n\t                if date.year < year:\n\t                    continue\n\t                id = result.entry_id.split('/')[-1]\n\t                papers.append({\n\t                    'id': result.entry_id,\n", "                    'citation_count': self.get_citations(id),\n\t                    'url': result.pdf_url,\n\t                    'title': result.title,\n\t                    'authors': result.authors,\n\t                    'abstract': result.summary,\n\t                    'published': str(date)\n\t                })\n\t        except Exception as e:\n\t            logging.info(f\"Exception {e}, we have {len(papers)} papers already, so will continue with indexing\")\n\t        if len(papers) == 0:\n", "            logging.info(f\"Found 0 papers for query: {query}, ignore crawl\")\n\t            return\n\t        # sort by citation count and get top n papers\n\t        if self.cfg.arxiv_crawler.sort_by == 'citations':\n\t            sorted_papers = sorted(papers, key=lambda x: x['citation_count'], reverse=True) \n\t            top_n = sorted_papers[:n_papers]\n\t        else:\n\t            top_n = papers\n\t        # Index top papers selected in Vectara\n\t        for paper in top_n:\n", "            url = paper['url'] + \".pdf\"\n\t            metadata = {'source': 'arxiv', 'title': paper['title'], 'abstract': paper['abstract'], 'url': paper['url'],\n\t                        'published': str(paper['published']), 'citations': str(paper['citation_count'])}\n\t            self.indexer.index_url(url, metadata=metadata)\n"]}
{"filename": "crawlers/website_crawler.py", "chunked_list": ["import logging\n\timport os\n\tfrom usp.tree import sitemap_tree_for_homepage\n\tfrom ratelimiter import RateLimiter\n\tfrom core.crawler import Crawler, recursive_crawl\n\tfrom core.utils import clean_urls, archive_extensions, img_extensions\n\timport re\n\tfrom typing import List, Set\n\t# disable USP annoying logging\n\tlogging.getLogger(\"usp.fetch_parse\").setLevel(logging.ERROR)\n", "logging.getLogger(\"usp.helpers\").setLevel(logging.ERROR)\n\tclass WebsiteCrawler(Crawler):\n\t    def crawl(self) -> None:\n\t        base_urls = self.cfg.website_crawler.urls\n\t        crawled_urls = set()\n\t        if \"url_regex\" in self.cfg.website_crawler:\n\t            url_regex = [re.compile(r) for r in self.cfg.website_crawler.url_regex]\n\t            logging.info(\n\t                f\"Filtering URLs by these regular expressions: {self.cfg.website_crawler.url_regex}\"\n\t            )\n", "        else:\n\t            url_regex = []\n\t        for homepage in base_urls:\n\t            if self.cfg.website_crawler.pages_source == \"sitemap\":\n\t                tree = sitemap_tree_for_homepage(homepage)\n\t                urls = [page.url for page in tree.all_pages()]\n\t            elif self.cfg.website_crawler.pages_source == \"crawl\":\n\t                urls_set = recursive_crawl(homepage, self.cfg.website_crawler.max_depth, url_regex=url_regex)\n\t                urls = clean_urls(urls_set)\n\t            else:\n", "                logging.info(f\"Unknown pages_source: {self.cfg.website_crawler.pages_source}\")\n\t                return\n\t            # remove URLS that are out of our regex regime or are archives or images\n\t            if url_regex:\n\t                urls = [u for u in urls if any([r.match(u) for r in url_regex])]\n\t            urls = [u for u in urls if not any([u.endswith(ext) for ext in archive_extensions + img_extensions])]\n\t            urls = list(set(urls))\n\t            logging.info(f\"Finished crawling using {homepage}, found {len(urls)} URLs to index\")\n\t            file_types = list(set([u[-10:].split('.')[-1] for u in urls if '.' in u[-10:]]))\n\t            logging.info(f\"File types = {file_types}\")\n", "            delay = max(self.cfg.website_crawler.get(\"delay\", 0.1), 0.1)    # seconds between requests\n\t            rate_limiter = RateLimiter(\n\t                max_calls=1, period=delay                                   # at most 1 call every `delay` seconds\n\t            )\n\t            extraction = self.cfg.website_crawler.extraction\n\t            for inx, url in enumerate(urls):\n\t                if url in crawled_urls:\n\t                    logging.info(\n\t                        f\"Skipping {url} since it was already crawled in this round\"\n\t                    )\n", "                    continue\n\t                if inx % 100 == 0:\n\t                    logging.info(f\"Crawling URL number {inx} out of {len(urls)}\")\n\t                metadata = {\"source\": \"website\", \"url\": url}\n\t                if extraction == \"pdf\":\n\t                    try:\n\t                        with rate_limiter:\n\t                            filename = self.url_to_file(url, title=\"\")\n\t                    except Exception as e:\n\t                        logging.error(f\"Error while processing {url}: {e}\")\n", "                        continue\n\t                    try:\n\t                        succeeded = self.indexer.index_file(filename, uri=url, metadata=metadata)\n\t                        if not succeeded:\n\t                            logging.info(f\"Indexing failed for {url}\")\n\t                        else:\n\t                            if os.path.exists(filename):\n\t                                os.remove(filename)\n\t                            crawled_urls.add(url)\n\t                            logging.info(f\"Indexing {url} was successfully\")\n", "                    except Exception as e:\n\t                        import traceback\n\t                        logging.error(\n\t                            f\"Error while indexing {url}: {e}, traceback={traceback.format_exc()}\"\n\t                        )\n\t                else:  # use index_url which uses PlayWright\n\t                    logging.info(f\"Crawling and indexing {url}\")\n\t                    try:\n\t                        with rate_limiter:\n\t                            succeeded = self.indexer.index_url(url, metadata=metadata)\n", "                        if not succeeded:\n\t                            logging.info(f\"Indexing failed for {url}\")\n\t                        else:\n\t                            crawled_urls.add(url)\n\t                            logging.info(f\"Indexing {url} was successfully\")\n\t                    except Exception as e:\n\t                        import traceback\n\t                        logging.error(\n\t                            f\"Error while indexing {url}: {e}, traceback={traceback.format_exc()}\"\n\t                        )\n"]}
{"filename": "crawlers/github_crawler.py", "chunked_list": ["import json\n\tfrom core.crawler import Crawler\n\tfrom omegaconf import OmegaConf\n\timport requests\n\tfrom attrdict import AttrDict\n\timport logging\n\timport base64\n\tfrom ratelimiter import RateLimiter\n\tfrom core.utils import create_session_with_retries\n\tfrom typing import List, Any\n", "class Github(object):\n\t    def __init__(self, repo: str, owner: str, token: str) -> None:\n\t        self.repo = repo\n\t        self.owner = owner\n\t        self.token = token\n\t        self.session = create_session_with_retries()\n\t    def get_issues(self, state: str) -> List[Any]:\n\t        # state can be \"open\", \"closed\", or \"all\"\n\t        api_url = f\"https://api.github.com/repos/{self.owner}/{self.repo}/issues?state={state}\"\n\t        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Accept\": \"application/vnd.github+json\"}\n", "        response = self.session.get(api_url, headers=headers)\n\t        if response.status_code == 200:\n\t            return list(response.json())\n\t        else:\n\t            logging.info(f\"Error retrieving issues: {response.status_code}, {response.text}\")\n\t            return []\n\t    def get_comments(self, issue_number: str) -> List[Any]:\n\t        api_url = f\"https://api.github.com/repos/{self.owner}/{self.repo}/issues/{issue_number}/comments\"\n\t        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Accept\": \"application/vnd.github+json\"}\n\t        response = self.session.get(api_url, headers=headers)\n", "        if response.status_code == 200:\n\t            return list(response.json())\n\t        else:\n\t            logging.info(f\"Error retrieving comments: {response.status_code}, {response.text}\")\n\t            return []\n\tclass GithubCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n\t        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n\t        self.github_token = self.cfg.github_crawler.get(\"github_token\", None)\n\t        self.owner = self.cfg.github_crawler.owner\n", "        self.repos = self.cfg.github_crawler.repos\n\t        self.crawl_code = self.cfg.github_crawler.crawl_code\n\t        self.rate_limiter = RateLimiter(max_calls=1, period=1)\n\t        self.session = create_session_with_retries()\n\t        adapter = requests.adapters.HTTPAdapter(max_retries=3)\n\t        self.session.mount('http://', adapter)\n\t        self.session.mount('https://', adapter)\n\t    def crawl_code_folder(self, base_url: str, path: str = \"\") -> None:\n\t        headers = { \"Accept\": \"application/vnd.github+json\"}\n\t        if self.github_token:\n", "            headers[\"Authorization\"] = f\"token {self.github_token}\"\n\t        with self.rate_limiter:\n\t            response = self.session.get( f\"{base_url}/contents/{path}\", headers=headers)\n\t        if response.status_code != 200:\n\t            logging.info(f\"Error fetching {base_url}/contents/{path}: {response.text}\")\n\t            return\n\t        for item in response.json():\n\t            if item[\"type\"] == \"file\":\n\t                fname = item[\"path\"]\n\t                url = item[\"html_url\"]\n", "                if url.lower().endswith(\".md\") or url.lower().endswith(\".mdx\"):     # Only index markdown files from the code, not the code itself\n\t                    try:\n\t                        file_response = self.session.get(item[\"url\"], headers={\"Authorization\": f\"token {self.github_token}\"})\n\t                        file_content = base64.b64decode(file_response.json()[\"content\"]).decode(\"utf-8\")\n\t                    except Exception as e:\n\t                        logging.info(f\"Failed to retrieve content for {fname} with url {url}: {e}\")\n\t                        continue\n\t                    metadata = {'file': fname, 'source': 'github', 'url': url}\n\t                    code_doc = {\n\t                        'documentId': f'github-{item[\"path\"]}',\n", "                        'title': item[\"name\"],\n\t                        'description': f'Markdown of {fname}',\n\t                        'metadataJson': json.dumps(metadata),\n\t                        'section': [{\n\t                            'title': 'markdown',\n\t                            'text': file_content,\n\t                        }]\n\t                    }\n\t                    logging.info(f\"Indexing codebase markdown: {item['path']}\")\n\t                    self.indexer.index_document(code_doc)\n", "            elif item[\"type\"] == \"dir\":\n\t                self.crawl_code_folder(base_url, path=item[\"path\"])\n\t    def crawl_repo(self, repo: str, owner: str, token: str) -> None:\n\t        g = Github(repo, owner, token)\n\t        issues = g.get_issues(\"all\")\n\t        for d_issue in issues:\n\t            # Extract issue metadata\n\t            issue = AttrDict(d_issue)\n\t            issue_id = f'github-issue-{issue.id}'\n\t            title = issue.title\n", "            description = issue.body\n\t            created_at = str(issue.created_at)\n\t            updated_at = str(issue.updated_at)\n\t            labels = [label.name for label in issue.labels]\n\t            author = issue.user.login\n\t            metadata = {'issue_number': issue.number, 'labels': labels, 'source': 'github', 'url': issue.html_url, 'state': issue.state}\n\t            issue_doc = {\n\t                'documentId': f'github-issue-{issue_id}',\n\t                'title': title,\n\t                'description': description,\n", "                'metadataJson': json.dumps(metadata),\n\t                'section': [{\n\t                    'title': 'issue',\n\t                    'text': description,\n\t                    'metadataJson': json.dumps({\n\t                        'author': author,\n\t                        'created_at': created_at,\n\t                        'updated_at': updated_at\n\t                    })\n\t                }]\n", "            }\n\t            logging.info(f\"Indexing issue: {issue.id}\")\n\t            self.indexer.index_document(issue_doc)\n\t            # Extract and index comments\n\t            comments = g.get_comments(issue.number)\n\t            if len(comments)>0:\n\t                logging.info(f\"Indexing {len(comments)} comments for issue {issue.number}\")\n\t            else:\n\t                logging.info(f\"No comments for issue {issue.number}\")\n\t            for d_comment in comments:\n", "                comment = AttrDict(d_comment)\n\t                comment_id = comment.id\n\t                comment_text = comment.body\n\t                comment_author = comment.user.login\n\t                comment_created_at = str(comment.created_at)\n\t                metadata = {'comment_id': comment.id, 'url': comment.html_url, 'source': 'github'}\n\t                comment_doc = {\n\t                    'documentId': f'github-comment-{comment_id}',\n\t                    'title': title,\n\t                    'description': comment_text,\n", "                    'metadataJson': json.dumps(metadata),\n\t                    'section': [{\n\t                        'title': 'comment',\n\t                        'text': comment_text,\n\t                        'metadataJson': json.dumps({\n\t                            'author': comment_author,\n\t                            'created_at': comment_created_at,\n\t                            'updated_at': updated_at\n\t                        })\n\t                    }]\n", "                }\n\t                try:\n\t                    self.indexer.index_document(comment_doc)\n\t                except Exception as e:\n\t                    logging.info(f\"Error {e} indexing comment document {comment_doc}\")\n\t                    continue\n\t        if self.crawl_code:\n\t            base_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n\t            self.crawl_code_folder(base_url)\n\t    def crawl(self) -> None:\n", "        for repo in self.repos:\n\t            logging.info(f\"Crawling repo {repo}\")\n\t            self.crawl_repo(repo, self.owner, self.github_token)\n"]}
{"filename": "crawlers/pmc_crawler.py", "chunked_list": ["import logging\n\timport os\n\tfrom Bio import Entrez\n\timport json\n\tfrom bs4 import BeautifulSoup\n\tfrom ratelimiter import RateLimiter\n\timport xmltodict\n\tfrom datetime import datetime, timedelta\n\tfrom typing import Set, List, Dict, Any\n\tfrom core.utils import html_to_text, create_session_with_retries\n", "from core.crawler import Crawler\n\tfrom omegaconf import OmegaConf\n\tdef get_top_n_papers(topic: str, n: int, email: str) -> Any:\n\t    \"\"\"\n\t    Get the top n papers for a given topic from PMC\n\t    \"\"\"\n\t    Entrez.email = email\n\t    search_results = Entrez.read(\n\t        Entrez.esearch(\n\t            db=\"pmc\",\n", "            term=topic,\n\t            retmax=n,\n\t            usehistory=\"y\",\n\t        )\n\t    )\n\t    id_list = search_results[\"IdList\"]    \n\t    return id_list\n\tclass PmcCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n\t        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n", "        self.site_urls: Set[str] = set()\n\t        self.crawled_pmc_ids: Set[str] = set()\n\t        self.session = create_session_with_retries()\n\t    def index_papers_by_topic(self, topic: str, n_papers: int) -> None:\n\t        \"\"\"\n\t        Index the top n papers for a given topic\n\t        \"\"\"\n\t        email = \"crawler@vectara.com\"\n\t        papers = list(set(get_top_n_papers(topic, n_papers, email)))\n\t        logging.info(f\"Found {len(papers)} papers for topic {topic}, now indexing...\")\n", "        # index the papers\n\t        rate_limiter = RateLimiter(max_calls=3, period=1)\n\t        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n\t        for i, pmc_id in enumerate(papers):\n\t            if i%100 == 0:\n\t                logging.info(f\"Indexed {i} papers so far for topic {topic}\")\n\t            if pmc_id in self.crawled_pmc_ids:\n\t                continue\n\t            params = {\"db\": \"pmc\", \"id\": pmc_id, \"retmode\": \"xml\", \"tool\": \"python_script\", \"email\": email}\n\t            try:\n", "                with rate_limiter:\n\t                    response = self.session.get(base_url, params=params)\n\t            except Exception as e:\n\t                logging.info(f\"Failed to download paper {pmc_id} due to error {e}, skipping\")\n\t                continue\n\t            if response.status_code != 200:\n\t                logging.info(f\"Failed to download paper {pmc_id}, skipping\")\n\t                continue\n\t            soup = BeautifulSoup(response.text, \"xml\")\n\t            # Extract the title\n", "            title_element = soup.find(\"article-title\")\n\t            if title_element:\n\t                title = title_element.get_text(strip=True)\n\t            else:\n\t                title = \"Title not found\"\n\t            # Extract the publication date\n\t            pub_date_soup = soup.find(\"pub-date\")\n\t            if pub_date_soup is not None:\n\t                year = pub_date_soup.find(\"year\")\n\t                if year is None:\n", "                    year_text = '1970'\n\t                else:\n\t                    year_text = str(year)\n\t                month = pub_date_soup.find(\"month\")\n\t                if month is None:\n\t                    month_text = '1'\n\t                else:\n\t                    month_text = str(month)\n\t                day = pub_date_soup.find(\"day\")\n\t                if day is None:\n", "                    day_text = '1'\n\t                else:\n\t                    day_text = str(day)\n\t                try:\n\t                    pub_date = f\"{year_text}-{month_text}-{day_text}\"\n\t                except Exception as e:\n\t                    pub_date = 'unknown'\n\t            else:\n\t                pub_date = \"Publication date not found\"\n\t            self.crawled_pmc_ids.add(pmc_id)\n", "            logging.info(f\"Indexing paper {pmc_id} with publication date {pub_date} and title '{title}'\")\n\t            # Index the page into Vectara\n\t            document = {\n\t                \"documentId\": pmc_id,\n\t                \"title\": title,\n\t                \"description\": \"\",\n\t                \"metadataJson\": json.dumps({\n\t                    \"url\": f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/\",\n\t                    \"publicationDate\": pub_date,\n\t                    \"source\": \"pmc\",\n", "                }),\n\t                \"section\": []\n\t            }\n\t            for paragraph in soup.findall('body p'):\n\t                document['section'].append({\n\t                    \"text\": paragraph.text,\n\t                })\n\t            succeeded = self.indexer.index_document(document)\n\t            if not succeeded:\n\t                logging.info(f\"Failed to index document {pmc_id}\")\n", "    def _get_xml_dict(self) -> Any:\n\t        days_back = 1\n\t        max_days = 30\n\t        while (days_back <= max_days):\n\t            xml_date = (datetime.now() - timedelta(days = days_back)).strftime(\"%Y-%m-%d\")\n\t            url = f'https://medlineplus.gov/xml/mplus_topics_{xml_date}.xml'\n\t            response = self.session.get(url)\n\t            if response.status_code == 200:\n\t                break\n\t            days_back += 1\n", "        if days_back == max_days:\n\t            logging.info(f\"Could not find medline plus topics after checkint last {max_days} days\")\n\t            return {}\n\t        logging.info(f\"Using MedlinePlus topics from {xml_date}\")        \n\t        url = f'https://medlineplus.gov/xml/mplus_topics_{xml_date}.xml'\n\t        response = self.session.get(url)\n\t        response.raise_for_status()\n\t        xml_dict = xmltodict.parse(response.text)\n\t        return xml_dict\n\t    def index_medline_plus(self, topics: List[str]) -> None:\n", "        xml_dict = self._get_xml_dict()\n\t        logging.info(f\"Indexing {xml_dict['health-topics']['@total']} health topics from MedlinePlus\")    \n\t        rate_limiter = RateLimiter(max_calls=3, period=1)\n\t        for ht in xml_dict['health-topics']['health-topic']:\n\t            title = ht['@title']\n\t            all_names = [title.lower()]\n\t            if 'also-called' in ht:\n\t                synonyms = ht['also-called']\n\t                if type(synonyms)==list:\n\t                    all_names += [x.lower() for x in synonyms]\n", "                else:\n\t                    all_names += [synonyms.lower()]\n\t            if not any([t.lower() in all_names for t in topics]):\n\t                logging.info(f\"Skipping {title} because it is not in our list of topics to crawl\")\n\t                continue\n\t            medline_id = ht['@id']\n\t            topic_url = ht['@url']\n\t            date_created = ht['@date-created']\n\t            summary = html_to_text(ht['full-summary'])\n\t            meta_desc = ht['@meta-desc']\n", "            document = {\n\t                \"documentId\": f'medline-plus-{medline_id}',\n\t                \"title\": title,\n\t                \"description\": f'medline information for {title}',\n\t                \"metadataJson\": json.dumps({\n\t                    \"url\": topic_url,\n\t                    \"publicationDate\": date_created,\n\t                    \"source\": \"pmc\",\n\t                }),\n\t                \"section\": [\n", "                    {\n\t                        'text': meta_desc\n\t                    },\n\t                    {\n\t                        'text': summary\n\t                    }\n\t                ]\n\t            }\n\t            logging.info(f\"Indexing data about {title}\")\n\t            succeeded = self.indexer.index_document(document)\n", "            if not succeeded:\n\t                logging.info(f\"Failed to index document with title {title}\")\n\t                continue\n\t            for site in ht['site']:\n\t                site_title = site['@title']\n\t                site_url = site['@url']\n\t                if site_url in self.site_urls:\n\t                    continue\n\t                else:\n\t                    self.site_urls.add(site_url)\n", "                with rate_limiter:\n\t                    succeeded = self.indexer.index_url(site_url, metadata={'url': site_url, 'source': 'medline_plus', 'title': site_title})\n\t    def crawl(self) -> None:\n\t        folder = 'papers'\n\t        os.makedirs(folder, exist_ok=True)\n\t        topics = self.cfg.pmc_crawler.topics\n\t        n_papers = self.cfg.pmc_crawler.n_papers\n\t        self.index_medline_plus(topics)\n\t        for topic in topics:\n\t            self.index_papers_by_topic(topic, n_papers)\n"]}
{"filename": "crawlers/hubspot_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\tfrom omegaconf import OmegaConf\n\timport requests\n\tfrom core.utils import clean_email_text\n\tfrom slugify import slugify\n\tfrom presidio_analyzer import AnalyzerEngine\n\tfrom presidio_anonymizer import AnonymizerEngine\n\timport datetime\n\tfrom typing import Any, Dict, List, Tuple\n", "# Initialize Presidio Analyzer and Anonymizer\n\tanalyzer = AnalyzerEngine()\n\tanonymizer = AnonymizerEngine()\n\tclass HubspotCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n\t        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n\t        self.hubspot_api_key = self.cfg.hubspot_crawler.hubspot_api_key\n\t    def mask_pii(self, text: str) -> str:\n\t        # Analyze and anonymize PII data\n\t        results = analyzer.analyze(text=text,\n", "                    entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\", \"EMAIL_ADDRESS\", \"IBAN_CODE\", \"PERSON\", \"US_BANK_NUMBER\", \"US_PASSPORT\", \"US_SSN\", \"LOCATION\"],\n\t                    language='en')    \n\t        anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n\t        return str(anonymized_text.text)\n\t    def crawl(self) -> None:\n\t        logging.info(\"Starting HubSpot Crawler.\")\n\t        # API endpoint for fetching contacts\n\t        api_endpoint_contacts = \"https://api.hubapi.com/crm/v3/objects/contacts\"\n\t        headers = {\n\t            \"Authorization\": f\"Bearer {self.hubspot_api_key}\",\n", "            \"Content-Type\": \"application/json\"\n\t        }\n\t        query_params_contacts = {\n\t            \"limit\": 100\n\t        }\n\t        after_contact = 1  # This is to use for pagination of contacts. The loop breaks when after_contact is None\n\t        email_count = 0\n\t        while after_contact:\n\t            if after_contact:\n\t                query_params_contacts[\"after\"] = after_contact\n", "            response_contacts = requests.get(api_endpoint_contacts, headers=headers, params=query_params_contacts)\n\t            if response_contacts.status_code == 200:\n\t                contacts_data = response_contacts.json()\n\t                contacts = contacts_data[\"results\"]\n\t                if not contacts:\n\t                    break\n\t                for contact in contacts:\n\t                    contact_id = contact[\"id\"]\n\t                    engagements, engagements_per_contact = self.get_contact_engagements(contact_id)\n\t                    logging.info(f\"NUMBER OF ENGAGEMENTS: {engagements_per_contact} FOR CONTACT ID: {contact_id}\")\n", "                    for engagement in engagements:\n\t                        engagement_type = engagement[\"engagement\"].get(\"type\", \"UNKNOWN\")\n\t                        if engagement_type == \"EMAIL\" and \"text\" in engagement[\"metadata\"] and \"subject\" in engagement[\"metadata\"]:\n\t                            email_subject = engagement[\"metadata\"][\"subject\"]\n\t                            email_text = engagement[\"metadata\"][\"text\"]\n\t                            email_url = self.get_email_url(contact_id, engagement[\"engagement\"][\"id\"])\n\t                        else:\n\t                            continue\n\t                        # Skip indexing if email text is empty or None\n\t                        if email_text is None or email_text.strip() == \"\":\n", "                            logging.info(f\"Email '{email_subject}' has no text. Skipping indexing.\")\n\t                            continue\n\t                        masked_email_text = self.mask_pii(email_text)\n\t                        cleaned_email_text = clean_email_text(masked_email_text)\n\t                        metadata = {\n\t                            \"source\": engagement['engagement']['source'],\n\t                            \"createdAt\": datetime.datetime.utcfromtimestamp(int(engagement['engagement']['createdAt'])/1000).strftime(\"%Y-%m-%d\"),\n\t                        }\n\t                        # Generate a unique doc_id for indexing\n\t                        doc_id = str(contact_id) + \"_\" + str(engagement['engagement']['id'])\n", "                        logging.info(f\"Indexing email with doc_id '{doc_id}' and subject '{email_subject}'\")\n\t                        succeeded = self.indexer.index_segments(\n\t                            doc_id=doc_id,\n\t                            parts=[cleaned_email_text],\n\t                            metadatas=[metadata],\n\t                            doc_metadata={'source': 'hubspot', 'title': email_subject, 'url': email_url}\n\t                        )\n\t                        if succeeded:\n\t                            logging.info(f\"Email with doc_id '{doc_id}' and subject '{email_subject}' indexed successfully.\")\n\t                            email_count += 1\n", "                        else:\n\t                            logging.error(f\"Failed to index email '{email_subject}'.\")\n\t                paging_info = contacts_data.get(\"paging\", {})\n\t                after_contact = paging_info.get(\"next\", {}).get(\"after\")\n\t                logging.info(f\"Crawled and indexed {email_count} emails successfully\")\n\t            else:\n\t                logging.error(f\"Error: {response_contacts.status_code} - {response_contacts.text}\")\n\t    def get_contact_engagements(self, contact_id: str) -> Tuple[List[Dict[str, Any]], int]:\n\t        api_endpoint_engagements = f\"https://api.hubapi.com/engagements/v1/engagements/associated/contact/{contact_id}/paged\"\n\t        headers = {\n", "            \"Authorization\": f\"Bearer {self.hubspot_api_key}\",\n\t            \"Content-Type\": \"application/json\"\n\t        }\n\t        all_engagements = []\n\t        while True:\n\t            response_engagements = requests.get(api_endpoint_engagements, headers=headers)\n\t            if response_engagements.status_code == 200:\n\t                engagements_data = response_engagements.json()\n\t                engagements = engagements_data.get(\"results\", [])\n\t                all_engagements.extend(engagements)\n", "                # Check if there are more engagements to fetch\n\t                if engagements_data.get(\"hasMore\"):\n\t                    offset = engagements_data.get(\"offset\")\n\t                    api_endpoint_engagements = f\"https://api.hubapi.com/engagements/v1/engagements/associated/contact/{contact_id}/paged?offset={offset}\"\n\t                else:\n\t                    break\n\t            else:\n\t                logging.error(f\"Error: {response_engagements.status_code} - {response_engagements.text}\")\n\t                break\n\t        return all_engagements, len(all_engagements)\n", "    def get_email_url(self, contact_id: str, engagement_id: str) -> str:\n\t        email_url = f\"https://app.hubspot.com/contacts/{self.cfg.hubspot_crawler.hubspot_customer_id}/contact/{contact_id}/?engagement={engagement_id}\"\n\t        return email_url\n"]}
{"filename": "crawlers/rss_crawler.py", "chunked_list": ["import logging\n\timport time\n\tfrom core.crawler import Crawler\n\timport feedparser\n\tfrom datetime import datetime, timedelta\n\tfrom time import mktime\n\tclass RssCrawler(Crawler):\n\t    def crawl(self) -> None:\n\t        \"\"\"\n\t        Crawl RSS feeds and upload to Vectara.\n", "        \"\"\"\n\t        rss_pages = self.cfg.rss_crawler.rss_pages\n\t        source = self.cfg.rss_crawler.source\n\t        if type(rss_pages) == str:\n\t            rss_pages = [rss_pages]\n\t        delay_in_secs = self.cfg.rss_crawler.delay\n\t        today = datetime.now().replace(microsecond=0)\n\t        days_ago = today - timedelta(days=self.cfg.rss_crawler.days_past)\n\t        # collect all URLs from the RSS feeds\n\t        urls = []\n", "        for rss_page in rss_pages:\n\t            feed = feedparser.parse(rss_page)\n\t            for entry in feed.entries:\n\t                if \"published_parsed\" not in entry:\n\t                    urls.append([entry.link, entry.title, None])\n\t                    continue\n\t                entry_date = datetime.fromtimestamp(mktime(entry.published_parsed))\n\t                if entry_date >= days_ago and entry_date <= today:\n\t                    urls.append([entry.link, entry.title, entry_date])\n\t        logging.info(f\"Found {len(urls)} URLs to index from the last {self.cfg.rss_crawler.days_past} days ({source})\")\n", "        crawled_urls = set()        # sometimes same url (with diff title) may appear twice, so we keep track of crawled pages to avoid duplication.\n\t        for url,title,pub_date in urls:\n\t            if url in crawled_urls:\n\t                logging.info(f\"Skipping {url} since it was already crawled in this round\")\n\t                continue\n\t            # index document into Vectara\n\t            try:\n\t                if pub_date:\n\t                    pub_date_int = int(str(pub_date.timestamp()).split('.')[0])\n\t                else:\n", "                    pub_date_int = 0        # unknown published date\n\t                    pub_date = 'unknown'\n\t                crawl_date_int = int(str(today.timestamp()).split('.')[0])\n\t                metadata = {\n\t                    'source': source, 'url': url, 'title': title, \n\t                    'pub_date': str(pub_date), 'pub_date_int': pub_date_int,\n\t                    'crawl_date': str(today),\n\t                    'crawl_date_int': crawl_date_int\n\t                }\n\t                succeeded = self.indexer.index_url(url, metadata=metadata)\n", "                if succeeded:\n\t                    logging.info(f\"Successfully indexed {url}\")\n\t                    crawled_urls.add(url)\n\t                else:\n\t                    logging.info(f\"Indexing failed for {url}\")\n\t            except Exception as e:\n\t                logging.error(f\"Error while indexing {url}: {e}\")\n\t            time.sleep(delay_in_secs)\n\t        return\n"]}
{"filename": "crawlers/fmp_crawler.py", "chunked_list": ["import logging\n\timport json\n\tfrom typing import Dict, Any\n\tfrom omegaconf import OmegaConf, DictConfig\n\tfrom core.crawler import Crawler\n\tfrom core.utils import create_session_with_retries\n\t# Crawler for financial information using the financialmodelingprep.com service\n\t# To use this crawler you have to have an fmp API_key in your secrets.toml profile\n\tclass FmpCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n", "        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n\t        cfg_dict: DictConfig = DictConfig(cfg)\n\t        self.tickers = cfg_dict.fmp_crawler.tickers\n\t        self.start_year = int(cfg_dict.fmp_crawler.start_year)\n\t        self.end_year = int(cfg_dict.fmp_crawler.end_year)\n\t        self.api_key = cfg_dict.fmp_crawler.fmp_api_key\n\t        self.session = create_session_with_retries()\n\t    def index_doc(self, document: Dict[str, Any]) -> bool:\n\t        try:\n\t            succeeded = self.indexer.index_document(document)\n", "            if succeeded:\n\t                logging.info(f\"Indexed {document['documentId']}\")\n\t            else:\n\t                logging.info(f\"Error indexing issue {document['documentId']}\")\n\t            return succeeded\n\t        except Exception as e:\n\t            logging.info(f\"Error during indexing of {document['documentId']}: {e}\")\n\t            return False\n\t    def crawl(self) -> None:\n\t        base_url = 'https://financialmodelingprep.com'\n", "        for ticker in self.tickers:\n\t            # get profile\n\t            url = f'{base_url}/api/v3/profile/{ticker}?apikey={self.api_key}'\n\t            try:\n\t                response = self.session.get(url)\n\t            except Exception as e:\n\t                logging.info(f\"Error getting transcript for {ticker}: {e}\")\n\t                continue\n\t            if response.status_code == 200:\n\t                data = response.json()\n", "                company_name = data[0]['companyName']\n\t                logging.info(f\"Processing {company_name}\")\n\t            else:\n\t                logging.info(f\"Can't get company profile for {ticker} - skipping\")\n\t                continue\n\t            # index 10-K for ticker in date range\n\t            url = f'{base_url}/api/v3/sec_filings/{ticker}?type=10-K&page=0&apikey={self.api_key}'\n\t            filings = self.session.get(url).json()\n\t            for year in range(self.start_year, self.end_year+1):\n\t                url = f'{base_url}/api/v4/financial-reports-json?symbol={ticker}&year={year}&period=FY&apikey={self.api_key}'\n", "                try:\n\t                    response = self.session.get(url)\n\t                except Exception as e:\n\t                    logging.info(f\"Error getting transcript for {ticker}: {e}\")\n\t                    continue\n\t                if response.status_code == 200:\n\t                    data = response.json()\n\t                    doc_title = f\"10-K for {company_name} from {year}\"\n\t                    rel_filings = [f for f in filings if f['acceptedDate'][:4] == str(year)]\n\t                    url = rel_filings[0]['finalLink'] if len(rel_filings)>0 else None\n", "                    metadata = {'source': 'finsearch', 'title': doc_title, 'ticker': ticker, 'company name': company_name, 'year': year, 'type': '10-K', 'url': url}\n\t                    document: Dict[str, Any] = {\n\t                        \"documentId\": f\"10-K-{company_name}-{year}\",\n\t                        \"title\": doc_title,\n\t                        \"metadataJson\": json.dumps(metadata),\n\t                        \"section\": []\n\t                    }\n\t                    for key in data.keys():\n\t                        if type(data[key])==str:\n\t                            continue\n", "                        for item_dict in data[key]:\n\t                            for title, values in item_dict.items():\n\t                                values = [v for v in values if v and type(v)==str and len(v)>=10]\n\t                                if len(values)>0 and len(' '.join(values))>100:\n\t                                    document['section'].append({'title': f'{key} - {title}', 'text': '\\n'.join(values)})\n\t                    self.index_doc(document)\n\t            # Index earnings call transcript\n\t            logging.info(f\"Getting transcripts\")\n\t            for year in range(self.start_year, self.end_year+1):\n\t                for quarter in range(1, 5):\n", "                    url = f'{base_url}/api/v3/earning_call_transcript/{ticker}?quarter={quarter}&year={year}&apikey={self.api_key}'\n\t                    try:\n\t                        response = self.session.get(url)\n\t                    except Exception as e:\n\t                        logging.info(f\"Error getting transcript for {company_name} quarter {quarter} of {year}: {e}\")\n\t                        continue\n\t                    if response.status_code == 200:\n\t                        for transcript in response.json():\n\t                            title = f\"Earnings call transcript for {company_name}, quarter {quarter} of {year}\"\n\t                            metadata = {'source': 'finsearch', 'title': title, 'ticker': ticker, 'company name': company_name, 'year': year, 'quarter': quarter, 'type': 'transcript'}\n", "                            document = {\n\t                                \"documentId\": f\"transcript-{company_name}-{year}-{quarter}\",\n\t                                \"title\": title,\n\t                                \"metadataJson\": json.dumps(metadata),\n\t                                \"section\": [\n\t                                    {\n\t                                        'text': transcript['content']\n\t                                    }\n\t                                ]\n\t                            }\n", "                            self.index_doc(document)\n"]}
{"filename": "crawlers/discourse_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\tfrom omegaconf import OmegaConf\n\timport json\n\tfrom html.parser import HTMLParser\n\tfrom io import StringIO\n\tfrom core.utils import create_session_with_retries\n\tfrom typing import List, Dict, Any\n\tclass MLStripper(HTMLParser):\n\t    def __init__(self) -> None:\n", "        super().__init__()\n\t        self.reset()\n\t        self.strict = False\n\t        self.convert_charrefs= True\n\t        self.text = StringIO()\n\t    def get_data(self) -> str:\n\t        return self.text.getvalue()\n\tdef strip_html(text: str) -> str:\n\t    \"\"\"\n\t    Strip HTML tags from text\n", "    \"\"\"\n\t    s = MLStripper()\n\t    s.feed(text)\n\t    return s.get_data()\n\tclass DiscourseCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n\t        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n\t        self.discourse_base_url = self.cfg.discourse_crawler.base_url\n\t        self.discourse_api_key = self.cfg.discourse_crawler.discourse_api_key\n\t        self.session = create_session_with_retries()\n", "    # function to fetch the topics from the Discourse API\n\t    def index_topics(self) -> List[Dict[str, Any]]:\n\t        url = self.discourse_base_url + '/latest.json'\n\t        params = { 'api_key': self.discourse_api_key, 'api_username': 'ofer@vectara.com', 'page': '0'}\n\t        response = self.session.get(url, params=params)\n\t        if response.status_code != 200:\n\t            raise Exception(f'Failed to fetch topics from Discourse, exception = {response.status_code}, {response.text}')\n\t        # index all topics\n\t        topics = list(json.loads(response.text)['topic_list']['topics'])\n\t        for topic in topics:\n", "            topic_id = topic['id']\n\t            logging.info(f\"Indexing topic {topic_id}\")\n\t            url = self.discourse_base_url + '/t/' + str(topic_id)\n\t            document = {\n\t                'documentId': 'topic-' + str(topic_id),\n\t                'title': topic['title'],\n\t                'metadataJson': json.dumps({\n\t                    'created_at': topic['created_at'],\n\t                    'views': topic['views'],\n\t                    'like_count': topic['like_count'],\n", "                    'last_poster': topic['last_poster_username'],\n\t                    'source': 'discourse',\n\t                    'url': url\n\t                }),\n\t                'section': [\n\t                    {\n\t                        'text': topic['fancy_title'],\n\t                    }\n\t                ]\n\t            }\n", "            self.indexer.index_document(document)\n\t        return topics\n\t    # function to fetch the posts for a topic from the Discourse API\n\t    def index_posts(self, topic: Dict[str, Any]) -> List[Any]:\n\t        topic_id = topic[\"id\"]\n\t        url_json = self.discourse_base_url + '/t/' + str(topic_id) + '.json'\n\t        params = { 'api_key': self.discourse_api_key, 'api_username': 'ofer@vectara.com'}\n\t        response = self.session.get(url_json, params=params)\n\t        if response.status_code != 200:\n\t            raise Exception('Failed to fetch posts for topic ' + str(topic_id) + ' from Discourse')\n", "        # parse the response JSON\n\t        posts = list(json.loads(response.text)['post_stream']['posts'])\n\t        for post in posts:\n\t            post_id = post['id']\n\t            logging.info(f\"Indexing post {post_id}\")\n\t            document = {\n\t                'documentId': 'post-' + str(post_id),\n\t                'title': topic['title'],\n\t                'metadataJson': json.dumps({\n\t                    'created_at': post['created_at'],\n", "                    'updated_at': post['updated_at'],\n\t                    'poster': post['username'],\n\t                    'poster_name': post['name'],\n\t                    'source': 'discourse',\n\t                    'url': self.discourse_base_url + '/p/' + str(post_id)\n\t                }),\n\t                'section': [\n\t                    {\n\t                        'text': strip_html(post['cooked'])\n\t                    }\n", "                ]\n\t            }\n\t            self.indexer.index_document(document)\n\t        return posts\n\t    def crawl(self) -> None:\n\t        topics = self.index_topics()\n\t        logging.info(f\"Indexed {len(topics)} topics from Discourse\")\n\t        for topic in topics:\n\t            posts = self.index_posts(topic)\n\t            logging.info(f\"Indexed {len(posts)} posts for topic {topic['id']} from Discourse\")\n"]}
{"filename": "crawlers/edgar_crawler.py", "chunked_list": ["import logging\n\tfrom omegaconf import OmegaConf\n\timport time\n\tfrom bs4 import BeautifulSoup \n\timport pandas as pd\n\timport datetime\n\tfrom ratelimiter import RateLimiter\n\tfrom core.crawler import Crawler\n\tfrom core.utils import create_session_with_retries\n\tfrom typing import Dict, List\n", "# build mapping of ticker to cik\n\tdf = pd.read_csv('https://www.sec.gov/include/ticker.txt', sep='\\t', names=['ticker', 'cik'], dtype=str)\n\tticker_dict = dict(zip(df.ticker.map(lambda x: str(x).upper()), df.cik))\n\tdef get_headers() -> Dict[str, str]:\n\t    \"\"\"\n\t    Get a set of headers to use for HTTP requests.\n\t    \"\"\"\n\t    headers = {\n\t        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n\t        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\" \n", "    }\n\t    return headers\n\tdef get_filings(cik: str, start_date_str: str, end_date_str: str, filing_type: str = \"10-K\") -> List[Dict[str, str]]:\n\t    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n\t    params = {\n\t        \"action\": \"getcompany\", \"CIK\": cik, \"type\": filing_type, \"dateb\": \"\", \"owner\": \"exclude\", \n\t        \"start\": \"\", \"output\": \"atom\", \"count\": \"100\"\n\t    }\n\t    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d')\n\t    end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d')\n", "    filings: List[Dict[str, str]] = []\n\t    current_start = 0\n\t    rate_limiter = RateLimiter(max_calls=1, period=1)\n\t    session = create_session_with_retries()\n\t    while True:\n\t        params[\"start\"] = str(current_start)\n\t        with rate_limiter:\n\t            response = session.get(base_url, params=params, headers=get_headers())\n\t        if response.status_code != 200:\n\t            logging.warning(f\"Error: status code {response.status_code} for {cik}\")\n", "            return filings\n\t        soup = BeautifulSoup(response.content, 'lxml-xml')\n\t        entries = soup.find_all(\"entry\")\n\t        if len(entries) == 0:\n\t            break\n\t        for entry in entries:\n\t            filing_date_str = entry.find(\"filing-date\").text\n\t            filing_date = datetime.datetime.strptime(filing_date_str, '%Y-%m-%d')\n\t            if start_date <= filing_date <= end_date:\n\t                try:\n", "                    url = entry.link[\"href\"]\n\t                    with rate_limiter:\n\t                        soup = BeautifulSoup(session.get(url, headers=get_headers()).content, \"html.parser\")\n\t                    l = soup.select_one('td:-soup-contains(\"10-K\") + td a')\n\t                    html_url = \"https://www.sec.gov\" + str(l[\"href\"])\n\t                    l = soup.select_one('td:-soup-contains(\"Complete submission text file\") + td a')\n\t                    submission_url = \"https://www.sec.gov\" + str(l[\"href\"])\n\t                    filings.append({\"date\": filing_date_str, \"submission_url\": submission_url, \"html_url\": html_url})\n\t                except Exception as e:\n\t                    pass\n", "            elif filing_date < start_date:\n\t                logging.info(f\"Error: filing date {filing_date_str} is before start date {start_date}\")\n\t                return filings\n\t        current_start += len(entries)\n\t    return filings\n\tclass EdgarCrawler(Crawler):\n\t    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n\t        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n\t        self.tickers = self.cfg.edgar_crawler.tickers\n\t        self.start_date = self.cfg.edgar_crawler.start_date\n", "        self.end_date = self.cfg.edgar_crawler.end_date\n\t    def crawl(self) -> None:\n\t        rate_limiter = RateLimiter(max_calls=1, period=1)\n\t        for ticker in self.tickers:\n\t            logging.info(f\"downloading 10-Ks for {ticker}\")\n\t            cik = ticker_dict[ticker]\n\t            filings = get_filings(cik, self.start_date, self.end_date, '10-K')\n\t            # no more filings in search universe\n\t            if len(filings) == 0:\n\t                logging.info(f\"For {ticker}, no filings found in search universe\")\n", "                continue\n\t            for filing in filings:\n\t                url = filing['html_url']\n\t                title = ticker + '-' + filing['date'] + '-' + filing['html_url'].split(\"/\")[-1].split(\".\")[0]\n\t                logging.info(f\"indexing document {url}\")\n\t                metadata = {'source': 'edgar', 'url': url, 'title': title}\n\t                with rate_limiter:\n\t                    succeeded = self.indexer.index_url(url, metadata=metadata)\n\t                if not succeeded:\n\t                    logging.info(f\"Indexing failed for url {url}\")\n", "                time.sleep(1)\n"]}
{"filename": "crawlers/csv_crawler.py", "chunked_list": ["import logging\n\tfrom core.crawler import Crawler\n\timport pandas as pd\n\timport unicodedata\n\tclass CsvCrawler(Crawler):\n\t    def crawl(self) -> None:\n\t        text_columns = list(self.cfg.csv_crawler.text_columns)\n\t        metadata_columns = list(self.cfg.csv_crawler.metadata_columns)\n\t        csv_path = self.cfg.csv_crawler.csv_path\n\t        csv_file = '/home/vectara/data/file.csv'\n", "        doc_id_columns = list(self.cfg.csv_crawler.get(\"doc_id_columns\", None))\n\t        all_columns = text_columns + metadata_columns\n\t        df = pd.read_csv(csv_file, usecols=all_columns)\n\t        logging.info(f\"indexing {len(df)} rows from the CSV file {csv_path}\")\n\t        def index_df(doc_id: str, title: str, df: pd.DataFrame) -> None:\n\t            parts = []\n\t            metadatas = []\n\t            for _, row in df.iterrows():\n\t                text = ' - '.join(str(x) for x in row[text_columns].tolist() if x)\n\t                parts.append(unicodedata.normalize('NFD', text))\n", "                metadatas.append({column: row[column] for column in metadata_columns})\n\t            logging.info(f\"Indexing df for '{doc_id}' with ({len(df)}) rows\")\n\t            self.indexer.index_segments(doc_id, parts, metadatas, title=title, doc_metadata = {'source': 'csv'})\n\t        if doc_id_columns:\n\t            grouped = df.groupby(doc_id_columns)\n\t            for name, group in grouped:\n\t                gr_str = name if type(name)==str else ' - '.join(str(x) for x in name)\n\t                index_df(doc_id=gr_str, title=gr_str, df=group)\n\t        else:\n\t            rows_per_chunk = self.cfg.csv_crawler.get(\"rows_per_chunk\", 500)\n", "            for inx in range(0, df.shape[0], rows_per_chunk):\n\t                sub_df = df[inx: inx+rows_per_chunk]\n\t                name = f'rows {inx}-{inx+rows_per_chunk-1}'\n\t                index_df(doc_id=name, title=name, df=sub_df)\n"]}
