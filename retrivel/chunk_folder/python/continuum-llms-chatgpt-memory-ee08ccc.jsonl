{"filename": "ui.py", "chunked_list": ["\"\"\"\n\tAdapted from https://github.com/avrabyt/MemoryBot\n\t\"\"\"\n\timport requests\n\t# Import necessary libraries\n\timport streamlit as st\n\tfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\t# Set Streamlit page configuration\n\tst.set_page_config(page_title=\"🧠MemoryBot🤖\", layout=\"wide\")\n\t# Initialize session states\n", "if \"generated\" not in st.session_state:\n\t    st.session_state[\"generated\"] = []\n\tif \"past\" not in st.session_state:\n\t    st.session_state[\"past\"] = []\n\tif \"input\" not in st.session_state:\n\t    st.session_state[\"input\"] = \"\"\n\tif \"stored_session\" not in st.session_state:\n\t    st.session_state[\"stored_session\"] = []\n\tif \"conversation_id\" not in st.session_state:\n\t    st.session_state[\"conversation_id\"] = None\n", "# Define function to get user input\n\tdef get_text():\n\t    \"\"\"\n\t    Get the user input text.\n\t    Returns:\n\t        (str): The text entered by the user\n\t    \"\"\"\n\t    input_text = st.text_input(\n\t        \"You: \",\n\t        st.session_state[\"input\"],\n", "        key=\"input\",\n\t        placeholder=\"Your AI assistant here! Ask me anything ...\",\n\t        label_visibility=\"hidden\",\n\t        on_change=send_text,\n\t    )\n\t    return input_text\n\tdef send_text():\n\t    user_input = st.session_state[\"input\"]\n\t    if user_input:\n\t        # Use the ChatGPTClient object to generate a response\n", "        url = \"http://localhost:8000/converse\"\n\t        payload = {\"message\": user_input, \"conversation_id\": st.session_state.conversation_id}\n\t        response = requests.post(url, json=payload).json()\n\t        # Update the conversation_id with the conversation_id from the response\n\t        if not st.session_state.conversation_id:\n\t            st.session_state.conversation_id = response[\"conversation_id\"]\n\t        st.session_state.past.insert(0, user_input)\n\t        st.session_state.generated.insert(0, response[\"chat_gpt_answer\"])\n\t        st.session_state[\"input\"] = \"\"\n\t# Define function to start a new chat\n", "def new_chat():\n\t    \"\"\"\n\t    Clears session state and starts a new chat.\n\t    \"\"\"\n\t    save = []\n\t    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n\t        save.append(\"Human:\" + st.session_state[\"past\"][i])\n\t        save.append(\"Assistant:\" + st.session_state[\"generated\"][i])\n\t    st.session_state[\"stored_session\"].append(save)\n\t    st.session_state[\"generated\"] = []\n", "    st.session_state[\"past\"] = []\n\t    st.session_state[\"input\"] = \"\"\n\t    st.session_state[\"conversation_id\"] = None\n\t# Set up the Streamlit app layout\n\tst.title(\"🤖 Chat Bot with 🧠\")\n\tst.subheader(\" Powered by ChatGPT Memory + Redis Search\")\n\t# Session state storage would be ideal\n\tif not OPENAI_API_KEY:\n\t    st.sidebar.warning(\"API key required to try this app. The API key is not stored in any form.\")\n\telif not (REDIS_HOST and REDIS_PASSWORD and REDIS_PORT):\n", "    st.sidebar.warning(\n\t        \"Redis `REDIS_HOST`, `REDIS_PASSWORD`, `REDIS_PORT` are required to try this app. Please set them as env variables properly.\"\n\t    )\n\t# Add a button to start a new chat\n\tst.sidebar.button(\"New Chat\", on_click=new_chat, type=\"primary\")\n\t# Get the user input\n\tuser_input = get_text()\n\t# Allow to download as well\n\tdownload_str = []\n\t# Display the conversation history using an expander, and allow the user to download it\n", "with st.expander(\"Conversation\", expanded=True):\n\t    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n\t        st.info(st.session_state[\"past\"][i], icon=\"🧐\")\n\t        st.success(st.session_state[\"generated\"][i], icon=\"🤖\")\n\t        download_str.append(st.session_state[\"past\"][i])\n\t        download_str.append(st.session_state[\"generated\"][i])\n\t    # Can throw error - requires fix\n\t    download_str = [\"\\n\".join(download_str)]\n\t    if download_str:\n\t        st.download_button(\"Download\", download_str[0])\n", "# Display stored conversation sessions in the sidebar\n\tfor i, sublist in enumerate(st.session_state.stored_session):\n\t    with st.sidebar.expander(label=f\"Conversation-Session:{i}\"):\n\t        st.write(sublist)\n\t# Allow the user to clear all stored conversation sessions\n\tif st.session_state.stored_session:\n\t    if st.sidebar.checkbox(\"Clear-all\"):\n\t        del st.session_state.stored_session\n"]}
{"filename": "rest_api.py", "chunked_list": ["from typing import Optional\n\tfrom fastapi import FastAPI\n\tfrom pydantic import BaseModel\n\tfrom chatgpt_memory.datastore import RedisDataStore, RedisDataStoreConfig\n\tfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom chatgpt_memory.llm_client import ChatGPTClient, ChatGPTConfig, ChatGPTResponse, EmbeddingClient, EmbeddingConfig\n\tfrom chatgpt_memory.memory import MemoryManager\n\t# Instantiate an EmbeddingConfig object with the OpenAI API key\n\tembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t# Instantiate an EmbeddingClient object with the EmbeddingConfig object\n", "embed_client = EmbeddingClient(config=embedding_config)\n\t# Instantiate a RedisDataStoreConfig object with the Redis connection details\n\tredis_datastore_config = RedisDataStoreConfig(\n\t    host=REDIS_HOST,\n\t    port=REDIS_PORT,\n\t    password=REDIS_PASSWORD,\n\t)\n\t# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n\tredis_datastore = RedisDataStore(config=redis_datastore_config)\n\t# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\n", "memory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\t# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\n\tchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\t# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\n\tchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\tclass MessagePayload(BaseModel):\n\t    conversation_id: Optional[str]\n\t    message: str\n\tapp = FastAPI()\n\t@app.post(\"/converse/\")\n", "async def converse(message_payload: MessagePayload) -> ChatGPTResponse:\n\t    response = chat_gpt_client.converse(**message_payload.dict())\n\t    return response\n"]}
{"filename": "tests/test_memory_manager.py", "chunked_list": ["from chatgpt_memory.datastore.config import RedisDataStoreConfig\n\tfrom chatgpt_memory.datastore.redis import RedisDataStore\n\tfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tfrom chatgpt_memory.memory.manager import MemoryManager\n\tfrom chatgpt_memory.memory.memory import Memory\n\tclass TestMemoryManager:\n\t    def setup(self):\n\t        # create a redis datastore\n", "        redis_datastore_config = RedisDataStoreConfig(\n\t            host=REDIS_HOST,\n\t            port=REDIS_PORT,\n\t            password=REDIS_PASSWORD,\n\t        )\n\t        self.datastore = RedisDataStore(redis_datastore_config, do_flush_data=True)\n\t        # create an openai embedding client\n\t        embedding_client_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t        self.embedding_client = EmbeddingClient(embedding_client_config)\n\t    def test_conversation_insertion_and_deletion(self):\n", "        # create a memory manager\n\t        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\t        # assert that the memory manager is initially empty\n\t        assert len(memory_manager.conversations) == 0\n\t        # add a conversation to the memory manager\n\t        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\t        # assert that the memory manager has 1 conversation\n\t        assert len(memory_manager.conversations) == 1\n\t        # remove the conversation from the memory manager\n\t        memory_manager.remove_conversation(Memory(conversation_id=\"1\"))\n", "        # assert that the memory manager is empty\n\t        assert len(memory_manager.conversations) == 0\n\t    def test_adding_messages_to_conversation(self):\n\t        # create a memory manager\n\t        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\t        # add a conversation to the memory manager\n\t        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\t        # assert that the memory manager has 1 conversation\n\t        assert len(memory_manager.conversations) == 1\n\t        # add a message to the conversation\n", "        memory_manager.add_message(conversation_id=\"1\", human=\"Hello\", assistant=\"Hello. How are you?\")\n\t        # get messages for that conversation\n\t        messages = memory_manager.get_messages(conversation_id=\"1\", query=\"Hello\")\n\t        # assert that the message was added\n\t        assert len(messages) == 1\n\t        # assert that the message is correct\n\t        assert messages[0].text == \"Human: Hello\\nAssistant: Hello. How are you?\"\n\t        assert messages[0].conversation_id == \"1\"\n"]}
{"filename": "tests/test_redis_datastore.py", "chunked_list": ["import numpy as np\n\tfrom chatgpt_memory.datastore.redis import RedisDataStore\n\tfrom chatgpt_memory.environment import OPENAI_API_KEY\n\tfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tSAMPLE_QUERIES = [\"Where is Berlin?\"]\n\tSAMPLE_DOCUMENTS = [\n\t    {\"text\": \"Berlin is located in Germany.\", \"conversation_id\": \"1\"},\n\t    {\"text\": \"Vienna is in Austria.\", \"conversation_id\": \"1\"},\n\t    {\"text\": \"Salzburg is in Austria.\", \"conversation_id\": \"2\"},\n", "]\n\tdef test_redis_datastore(redis_datastore: RedisDataStore):\n\t    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t    openai_embedding_client = EmbeddingClient(config=embedding_config)\n\t    assert (\n\t        redis_datastore.redis_connection.ping()\n\t    ), \"Redis connection failed,\\\n\t          double check your connection parameters\"\n\t    document_embeddings: np.ndarray = openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS)\n\t    for idx, embedding in enumerate(document_embeddings):\n", "        SAMPLE_DOCUMENTS[idx][\"embedding\"] = embedding.astype(np.float32).tobytes()\n\t    redis_datastore.index_documents(documents=SAMPLE_DOCUMENTS)\n\t    query_embeddings: np.ndarray = openai_embedding_client.embed_queries(SAMPLE_QUERIES)\n\t    query_vector = query_embeddings[0].astype(np.float32).tobytes()\n\t    search_results = redis_datastore.search_documents(query_vector=query_vector, conversation_id=\"1\", topk=1)\n\t    assert len(search_results), \"No documents returned, expected 1 document.\"\n\t    assert search_results[0].text == \"Berlin is located in Germany.\", \"Incorrect document returned as search result.\"\n\t    redis_datastore.delete_documents(conversation_id=\"1\")\n\t    assert redis_datastore.get_all_conversation_ids() == [\n\t        \"2\"\n", "    ], \"Document deletion failed, inconsistent documents in redis index\"\n"]}
{"filename": "tests/test_llm_embedding_client.py", "chunked_list": ["from chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tSAMPLE_QUERIES = [\"Where is Berlin?\"]\n\tSAMPLE_DOCUMENTS = [{\"text\": \"Berlin is located in Germany.\"}]\n\tEXPECTED_EMBEDDING_DIMENSIONS = (1, 1024)\n\tdef test_openai_embedding_client(openai_embedding_client: EmbeddingClient):\n\t    assert (\n\t        openai_embedding_client.embed_queries(SAMPLE_QUERIES).shape == EXPECTED_EMBEDDING_DIMENSIONS\n\t    ), \"Generated query embedding is of inconsistent dimension\"\n\t    assert (\n\t        openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS).shape == EXPECTED_EMBEDDING_DIMENSIONS\n", "    ), \"Generated document(s) embedding is of inconsistent dimension\"\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import pytest\n\tfrom chatgpt_memory.datastore.config import RedisDataStoreConfig\n\tfrom chatgpt_memory.datastore.redis import RedisDataStore\n\tfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\t@pytest.fixture(scope=\"session\")\n\tdef openai_embedding_client():\n\t    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t    return EmbeddingClient(config=embedding_config)\n", "@pytest.fixture(scope=\"session\")\n\tdef redis_datastore():\n\t    redis_datastore_config = RedisDataStoreConfig(\n\t        host=REDIS_HOST,\n\t        port=REDIS_PORT,\n\t        password=REDIS_PASSWORD,\n\t    )\n\t    redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\t    return redis_datastore\n"]}
{"filename": "chatgpt_memory/environment.py", "chunked_list": ["import os\n\timport dotenv\n\t# Load environment variables from .env file\n\t_TESTING = os.getenv(\"CHATGPT_MEMORY_TESTING\", False)\n\tif _TESTING:\n\t    # for testing we use the .env.example file instead\n\t    dotenv.load_dotenv(dotenv.find_dotenv(\".env.example\"))\n\telse:\n\t    dotenv.load_dotenv()\n\t# Any remote API (OpenAI, Cohere etc.)\n", "OPENAI_TIMEOUT = float(os.getenv(\"REMOTE_API_TIMEOUT_SEC\", 30))\n\tOPENAI_BACKOFF = float(os.getenv(\"REMOTE_API_BACKOFF_SEC\", 10))\n\tOPENAI_MAX_RETRIES = int(os.getenv(\"REMOTE_API_MAX_RETRIES\", 5))\n\tOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\t# Cloud data store (Redis, Pinecone etc.)\n\tREDIS_HOST = os.getenv(\"REDIS_HOST\")\n\tREDIS_PORT = int(os.getenv(\"REDIS_PORT\"))\n\tREDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n"]}
{"filename": "chatgpt_memory/errors.py", "chunked_list": ["\"\"\"Custom Errors for ChatGptMemory\"\"\"\n\tfrom typing import Optional\n\tclass ChatGPTMemoryError(Exception):\n\t    \"\"\"\n\t    Any error generated by ChatGptMemory.\n\t    This error wraps its source transparently in such a way that its attributes\n\t    can be accessed directly: for example, if the original error has a `message`\n\t    attribute.\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        message: Optional[str] = None,\n\t    ):\n\t        super().__init__()\n\t        if message:\n\t            self.message = message\n\t    def __getattr__(self, attr):\n\t        # If self.__cause__ is None, it will raise the expected AttributeError\n\t        getattr(self.__cause__, attr)\n\t    def __repr__(self):\n", "        return str(self)\n\tclass OpenAIError(ChatGPTMemoryError):\n\t    \"\"\"Exception for issues that occur in the OpenAI APIs\"\"\"\n\t    def __init__(\n\t        self,\n\t        message: Optional[str] = None,\n\t        status_code: Optional[int] = None,\n\t    ):\n\t        super().__init__(message=message)\n\t        self.status_code = status_code\n", "class OpenAIRateLimitError(OpenAIError):\n\t    \"\"\"\n\t    Rate limit error for OpenAI API (status code 429), See below:\n\t    https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors\n\t    https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n\t    \"\"\"\n\t    def __init__(self, message: Optional[str] = None):\n\t        super().__init__(message=message, status_code=429)\n\t    def __repr__(self):\n\t        return f\"message= {self.message}, status_code={self.status_code}\"\n"]}
{"filename": "chatgpt_memory/__init__.py", "chunked_list": []}
{"filename": "chatgpt_memory/constants.py", "chunked_list": ["# LLM Config related\n\t\"\"\"\n\tif OpenAI embedding model type is \"*-001\" the set max sequence length to `2046`,\n\totherwise for type \"*-002\" set `8191`\n\t\"\"\"\n\tMAX_ALLOWED_SEQ_LEN_001 = 2046\n\tMAX_ALLOWED_SEQ_LEN_002 = 8191\n"]}
{"filename": "chatgpt_memory/utils/reflection.py", "chunked_list": ["import inspect\n\timport logging\n\timport time\n\tfrom random import random\n\tfrom typing import Any, Callable, Dict, Tuple\n\tfrom chatgpt_memory.errors import OpenAIRateLimitError\n\tlogger = logging.getLogger(__name__)\n\tdef args_to_kwargs(args: Tuple, func: Callable) -> Dict[str, Any]:\n\t    sig = inspect.signature(func)\n\t    arg_names = list(sig.parameters.keys())\n", "    # skip self and cls args for instance and class methods\n\t    if any(arg_names) and arg_names[0] in [\"self\", \"cls\"]:\n\t        arg_names = arg_names[1 : 1 + len(args)]\n\t    args_as_kwargs = {arg_name: arg for arg, arg_name in zip(args, arg_names)}\n\t    return args_as_kwargs\n\tdef retry_with_exponential_backoff(\n\t    backoff_in_seconds: float = 1,\n\t    max_retries: int = 10,\n\t    errors: tuple = (OpenAIRateLimitError,),\n\t):\n", "    \"\"\"\n\t    Decorator to retry a function with exponential backoff.\n\t    :param backoff_in_seconds: The initial backoff in seconds.\n\t    :param max_retries: The maximum number of retries.\n\t    :param errors: The errors to catch retry on.\n\t    \"\"\"\n\t    def decorator(function):\n\t        def wrapper(*args, **kwargs):\n\t            # Initialize variables\n\t            num_retries = 0\n", "            # Loop until a successful response or max_retries is hit or an\n\t            # exception is raised\n\t            while True:\n\t                try:\n\t                    return function(*args, **kwargs)\n\t                # Retry on specified errors\n\t                except errors as e:\n\t                    # Check if max retries has been reached\n\t                    if num_retries > max_retries:\n\t                        raise Exception(f\"Maximum number of retries ({max_retries}) exceeded.\")\n", "                    # Increment the delay\n\t                    sleep_time = backoff_in_seconds * 2**num_retries + random()\n\t                    # Sleep for the delay\n\t                    logger.warning(\n\t                        \"%s - %s, retry %s in %s seconds...\",\n\t                        e.__class__.__name__,\n\t                        e,\n\t                        function.__name__,\n\t                        \"{0:.2f}\".format(sleep_time),\n\t                    )\n", "                    time.sleep(sleep_time)\n\t                    # Increment retries\n\t                    num_retries += 1\n\t        return wrapper\n\t    return decorator\n"]}
{"filename": "chatgpt_memory/utils/openai_utils.py", "chunked_list": ["\"\"\"Utils for using OpenAI API\"\"\"\n\timport json\n\timport logging\n\tfrom typing import Any, Dict, Tuple, Union\n\timport requests\n\tfrom transformers import GPT2TokenizerFast\n\tfrom chatgpt_memory.environment import OPENAI_BACKOFF, OPENAI_MAX_RETRIES, OPENAI_TIMEOUT\n\tfrom chatgpt_memory.errors import OpenAIError, OpenAIRateLimitError\n\tfrom chatgpt_memory.utils.reflection import retry_with_exponential_backoff\n\tlogger = logging.getLogger(__name__)\n", "def load_openai_tokenizer(tokenizer_name: str, use_tiktoken: bool) -> Any:\n\t    \"\"\"\n\t    Load either the tokenizer from tiktoken (if the library is available) or\n\t    fallback to the GPT2TokenizerFast from the transformers library.\n\t    Args:\n\t        tokenizer_name (str): The name of the tokenizer to load.\n\t        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\t    Raises:\n\t        ImportError: When `tiktoken` package is missing.\n\t        To use tiktoken tokenizer install it as follows:\n", "        `pip install tiktoken`\n\t    Returns:\n\t        tokenizer: Tokenizer of either GPT2 kind or tiktoken based.\n\t    \"\"\"\n\t    tokenizer = None\n\t    if use_tiktoken:\n\t        try:\n\t            import tiktoken  # pylint: disable=import-error\n\t            logger.debug(\"Using tiktoken %s tokenizer\", tokenizer_name)\n\t            tokenizer = tiktoken.get_encoding(tokenizer_name)\n", "        except ImportError:\n\t            raise ImportError(\n\t                \"The `tiktoken` package not found.\",\n\t                \"To install it use the following:\",\n\t                \"`pip install tiktoken`\",\n\t            )\n\t    else:\n\t        logger.warning(\n\t            \"OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and \"\n\t            \"AARCH64. Falling back to GPT2TokenizerFast.\"\n", "        )\n\t        logger.debug(\"Using GPT2TokenizerFast tokenizer\")\n\t        tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)\n\t    return tokenizer\n\tdef count_openai_tokens(text: str, tokenizer: Any, use_tiktoken: bool) -> int:\n\t    \"\"\"\n\t    Count the number of tokens in `text` based on the provided OpenAI `tokenizer`.\n\t    Args:\n\t        text (str):  A string to be tokenized.\n\t        tokenizer (Any): An OpenAI tokenizer.\n", "        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\t    Returns:\n\t        int: Number of tokens in the text.\n\t    \"\"\"\n\t    if use_tiktoken:\n\t        return len(tokenizer.encode(text))\n\t    else:\n\t        return len(tokenizer.tokenize(text))\n\t@retry_with_exponential_backoff(\n\t    backoff_in_seconds=OPENAI_BACKOFF,\n", "    max_retries=OPENAI_MAX_RETRIES,\n\t    errors=(OpenAIRateLimitError, OpenAIError),\n\t)\n\tdef openai_request(\n\t    url: str,\n\t    headers: Dict,\n\t    payload: Dict,\n\t    timeout: Union[float, Tuple[float, float]] = OPENAI_TIMEOUT,\n\t) -> Dict:\n\t    \"\"\"\n", "    Make a request to the OpenAI API given a `url`, `headers`, `payload`, and\n\t    `timeout`.\n\t    Args:\n\t        url (str): The URL of the OpenAI API.\n\t        headers (Dict): Dictionary of HTTP Headers to send with the :class:`Request`.\n\t        payload (Dict): The payload to send with the request.\n\t        timeout (Union[float, Tuple[float, float]], optional): The timeout length of the request. The default is 30s.\n\t        Defaults to OPENAI_TIMEOUT.\n\t    Raises:\n\t        openai_error: If the request fails.\n", "    Returns:\n\t        Dict: OpenAI Embedding API response.\n\t    \"\"\"\n\t    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload), timeout=timeout)\n\t    res = json.loads(response.text)\n\t    # if request is unsucessful and `status_code = 429` then,\n\t    # raise rate limiting error else the OpenAIError\n\t    if response.status_code != 200:\n\t        openai_error: OpenAIError\n\t        if response.status_code == 429:\n", "            openai_error = OpenAIRateLimitError(f\"API rate limit exceeded: {response.text}\")\n\t        else:\n\t            openai_error = OpenAIError(\n\t                f\"OpenAI returned an error.\\n\"\n\t                f\"Status code: {response.status_code}\\n\"\n\t                f\"Response body: {response.text}\",\n\t                status_code=response.status_code,\n\t            )\n\t        raise openai_error\n\t    return res\n", "def get_prompt(message: str, history: str) -> str:\n\t    \"\"\"\n\t    Generates the prompt based on the current history and message.\n\t    Args:\n\t        message (str): Current message from user.\n\t        history (str): Retrieved history for the current message.\n\t        History follows the following format for example:\n\t        ```\n\t        Human: hello\n\t        Assistant: hello, how are you?\n", "        Human: good, you?\n\t        Assistant: I am doing good as well. How may I help you?\n\t        ```\n\t    Returns:\n\t        prompt: Curated prompt for the ChatGPT API based on current params.\n\t    \"\"\"\n\t    prompt = f\"\"\"Assistant is a large language model trained by OpenAI.\n\t    Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\t    Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\t    Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n", "    {history}\n\t    Human: {message}\n\t    Assistant:\"\"\"\n\t    return prompt\n"]}
{"filename": "chatgpt_memory/datastore/config.py", "chunked_list": ["from enum import Enum\n\tfrom pydantic import BaseModel\n\tclass RedisIndexType(Enum):\n\t    hnsw = \"HNSW\"\n\t    flat = \"FLAT\"\n\tclass DataStoreConfig(BaseModel):\n\t    host: str\n\t    port: int\n\t    password: str\n\tclass RedisDataStoreConfig(DataStoreConfig):\n", "    index_type: str = RedisIndexType.hnsw.value\n\t    vector_field_name: str = \"embedding\"\n\t    vector_dimensions: int = 1024\n\t    distance_metric: str = \"L2\"\n\t    number_of_vectors: int = 686\n\t    M: int = 40\n\t    EF: int = 200\n"]}
{"filename": "chatgpt_memory/datastore/__init__.py", "chunked_list": ["from chatgpt_memory.datastore.config import DataStoreConfig, RedisDataStoreConfig, RedisIndexType  # noqa: F401\n\tfrom chatgpt_memory.datastore.redis import RedisDataStore  # noqa: F401\n"]}
{"filename": "chatgpt_memory/datastore/redis.py", "chunked_list": ["import logging\n\tfrom typing import Any, Dict, List\n\tfrom uuid import uuid4\n\timport redis\n\tfrom redis.commands.search.field import TagField, TextField, VectorField\n\tfrom redis.commands.search.query import Query\n\tfrom chatgpt_memory.datastore.config import RedisDataStoreConfig\n\tfrom chatgpt_memory.datastore.datastore import DataStore\n\tlogger = logging.getLogger(__name__)\n\tclass RedisDataStore(DataStore):\n", "    def __init__(self, config: RedisDataStoreConfig, do_flush_data: bool = False):\n\t        super().__init__(config=config)\n\t        self.config = config\n\t        self.do_flush_data = do_flush_data\n\t        self.connect()\n\t        self.create_index()\n\t    def connect(self):\n\t        \"\"\"\n\t        Connect to the Redis server.\n\t        \"\"\"\n", "        connection_pool = redis.ConnectionPool(\n\t            host=self.config.host, port=self.config.port, password=self.config.password\n\t        )\n\t        self.redis_connection = redis.Redis(connection_pool=connection_pool)\n\t        # flush data only once after establishing connection\n\t        if self.do_flush_data:\n\t            self.flush_all_documents()\n\t            self.do_flush_data = False\n\t    def flush_all_documents(self):\n\t        \"\"\"\n", "        Removes all documents from the redis index.\n\t        \"\"\"\n\t        self.redis_connection.flushall()\n\t    def create_index(self):\n\t        \"\"\"\n\t        Creates a Redis index with a dense vector field.\n\t        \"\"\"\n\t        try:\n\t            self.redis_connection.ft().create_index(\n\t                [\n", "                    VectorField(\n\t                        self.config.vector_field_name,\n\t                        self.config.index_type,\n\t                        {\n\t                            \"TYPE\": \"FLOAT32\",\n\t                            \"DIM\": self.config.vector_dimensions,\n\t                            \"DISTANCE_METRIC\": self.config.distance_metric,\n\t                            \"INITIAL_CAP\": self.config.number_of_vectors,\n\t                            \"M\": self.config.M,\n\t                            \"EF_CONSTRUCTION\": self.config.EF,\n", "                        },\n\t                    ),\n\t                    TextField(\"text\"),  # contains the original message\n\t                    TagField(\"conversation_id\"),  # `conversation_id` for each session\n\t                ]\n\t            )\n\t            logger.info(\"Created a new Redis index for storing chat history\")\n\t        except redis.exceptions.ResponseError as redis_error:\n\t            logger.info(f\"Working with existing Redis index.\\nDetails: {redis_error}\")\n\t    def index_documents(self, documents: List[Dict]):\n", "        \"\"\"\n\t        Indexes the set of documents.\n\t        Args:\n\t            documents (List[Dict]): List of documents to be indexed.\n\t        \"\"\"\n\t        redis_pipeline = self.redis_connection.pipeline(transaction=False)\n\t        for document in documents:\n\t            assert (\n\t                \"text\" in document and \"conversation_id\" in document\n\t            ), \"Document must include the fields `text`, and `conversation_id`\"\n", "            redis_pipeline.hset(uuid4().hex, mapping=document)\n\t        redis_pipeline.execute()\n\t    def search_documents(\n\t        self,\n\t        query_vector: bytes,\n\t        conversation_id: str,\n\t        topk: int = 5,\n\t    ) -> List[Any]:\n\t        \"\"\"\n\t        Searches the redis index using the query vector.\n", "        Args:\n\t            query_vector (np.ndarray): Embedded query vector.\n\t            topk (int, optional): Number of results. Defaults to 5.\n\t            result_fields (int, optional): Name of the fields that you want to be\n\t                                           returned from the search result documents\n\t        Returns:\n\t            List[Any]: Search result documents.\n\t        \"\"\"\n\t        query = (\n\t            Query(\n", "                f\"\"\"(@conversation_id:{{{conversation_id}}})=>[KNN {topk} \\\n\t                    @{self.config.vector_field_name} $vec_param AS vector_score]\"\"\"\n\t            )\n\t            .sort_by(\"vector_score\")\n\t            .paging(0, topk)\n\t            .return_fields(\n\t                # parse `result_fields` as strings separated by comma to pass as params\n\t                \"conversation_id\",\n\t                \"vector_score\",\n\t                \"text\",\n", "            )\n\t            .dialect(2)\n\t        )\n\t        params_dict = {\"vec_param\": query_vector}\n\t        result_documents = self.redis_connection.ft().search(query, query_params=params_dict).docs\n\t        return result_documents\n\t    def get_all_conversation_ids(self) -> List[str]:\n\t        \"\"\"\n\t        Returns conversation ids of all conversations.\n\t        Returns:\n", "            List[str]: List of conversation ids stored in redis.\n\t        \"\"\"\n\t        query = Query(\"*\").return_fields(\"conversation_id\")\n\t        result_documents = self.redis_connection.ft().search(query).docs\n\t        conversation_ids: List[str] = []\n\t        conversation_ids = list(\n\t            set([getattr(result_document, \"conversation_id\") for result_document in result_documents])\n\t        )\n\t        return conversation_ids\n\t    def delete_documents(self, conversation_id: str):\n", "        \"\"\"\n\t        Deletes all documents for a given conversation id.\n\t        Args:\n\t            conversation_id (str): Id of the conversation to be deleted.\n\t        \"\"\"\n\t        query = (\n\t            Query(f\"\"\"(@conversation_id:{{{conversation_id}}})\"\"\")\n\t            .return_fields(\n\t                \"id\",\n\t            )\n", "            .dialect(2)\n\t        )\n\t        for document in self.redis_connection.ft().search(query).docs:\n\t            document_id = getattr(document, \"id\")\n\t            deletion_status = self.redis_connection.ft().delete_document(document_id, delete_actual_document=True)\n\t            assert deletion_status, f\"Deletion of the document with id {document_id} failed!\"\n"]}
{"filename": "chatgpt_memory/datastore/datastore.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Any, Dict, List\n\tfrom chatgpt_memory.datastore.config import DataStoreConfig\n\tclass DataStore(ABC):\n\t    \"\"\"\n\t    Abstract class for datastores.\n\t    \"\"\"\n\t    def __init__(self, config: DataStoreConfig):\n\t        self.config = config\n\t    @abstractmethod\n", "    def connect(self):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def create_index(self):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def index_documents(self, documents: List[Dict]):\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def search_documents(self, query_vector: Any, conversation_id: str, topk: int) -> List[Any]:\n", "        raise NotImplementedError\n"]}
{"filename": "chatgpt_memory/llm_client/config.py", "chunked_list": ["from pydantic import BaseModel\n\tclass LLMClientConfig(BaseModel):\n\t    api_key: str\n\t    time_out: float = 30\n"]}
{"filename": "chatgpt_memory/llm_client/__init__.py", "chunked_list": ["from chatgpt_memory.llm_client.openai.conversation.chatgpt_client import (  # noqa: F401\n\t    ChatGPTClient,\n\t    ChatGPTConfig,\n\t    ChatGPTResponse,\n\t)\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient  # noqa: F401\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingConfig  # noqa: F401\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingModels  # noqa: F401\n"]}
{"filename": "chatgpt_memory/llm_client/llm_client.py", "chunked_list": ["from abc import ABC\n\tfrom chatgpt_memory.llm_client.config import LLMClientConfig\n\tclass LLMClient(ABC):\n\t    \"\"\"\n\t    Wrapper for the HTTP APIs for LLMs acting as data container for API configurations.\n\t    \"\"\"\n\t    def __init__(self, config: LLMClientConfig):\n\t        self._api_key = config.api_key\n\t        self._time_out = config.time_out\n\t    @property\n", "    def api_key(self):\n\t        return self._api_key\n\t    @property\n\t    def time_out(self):\n\t        return self._time_out\n"]}
{"filename": "chatgpt_memory/llm_client/openai/__init__.py", "chunked_list": []}
{"filename": "chatgpt_memory/llm_client/openai/embedding/embedding_client.py", "chunked_list": ["import logging\n\tfrom typing import Any, Dict, List, Union\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom chatgpt_memory.constants import MAX_ALLOWED_SEQ_LEN_001, MAX_ALLOWED_SEQ_LEN_002\n\tfrom chatgpt_memory.llm_client.llm_client import LLMClient\n\tfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig, EmbeddingModels\n\tfrom chatgpt_memory.utils.openai_utils import count_openai_tokens, load_openai_tokenizer, openai_request\n\tlogger = logging.getLogger(__name__)\n\tclass EmbeddingClient(LLMClient):\n", "    def __init__(self, config: EmbeddingConfig):\n\t        super().__init__(config=config)\n\t        self.openai_embedding_config = config\n\t        model_class: str = EmbeddingModels(self.openai_embedding_config.model).name\n\t        tokenizer = self._setup_encoding_models(\n\t            model_class,\n\t            self.openai_embedding_config.model,\n\t            self.openai_embedding_config.max_seq_len,\n\t        )\n\t        self._tokenizer = load_openai_tokenizer(\n", "            tokenizer_name=tokenizer,\n\t            use_tiktoken=self.openai_embedding_config.use_tiktoken,\n\t        )\n\t    def _setup_encoding_models(self, model_class: str, model_name: str, max_seq_len: int):\n\t        \"\"\"\n\t        Setup the encoding models for the retriever.\n\t        Raises:\n\t            ImportError: When `tiktoken` package is missing.\n\t            To use tiktoken tokenizer install it as follows:\n\t            `pip install tiktoken`\n", "        \"\"\"\n\t        tokenizer_name = \"gpt2\"\n\t        # new generation of embedding models (December 2022), specify the full name\n\t        if model_name.endswith(\"-002\"):\n\t            self.query_encoder_model = model_name\n\t            self.doc_encoder_model = model_name\n\t            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_002, max_seq_len)\n\t            if self.openai_embedding_config.use_tiktoken:\n\t                try:\n\t                    from tiktoken.model import MODEL_TO_ENCODING\n", "                    tokenizer_name = MODEL_TO_ENCODING.get(model_name, \"cl100k_base\")\n\t                except ImportError:\n\t                    raise ImportError(\n\t                        \"The `tiktoken` package not found.\",\n\t                        \"To install it use the following:\",\n\t                        \"`pip install tiktoken`\",\n\t                    )\n\t        else:\n\t            self.query_encoder_model = f\"text-search-{model_class}-query-001\"\n\t            self.doc_encoder_model = f\"text-search-{model_class}-doc-001\"\n", "            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_001, max_seq_len)\n\t        return tokenizer_name\n\t    def _ensure_text_limit(self, text: str) -> str:\n\t        \"\"\"\n\t         Ensure that length of the text is within the maximum length of the model.\n\t        OpenAI v1 embedding models have a limit of 2046 tokens, and v2 models have\n\t        a limit of 8191 tokens.\n\t        Args:\n\t            text (str):  Text to be checked if it exceeds the max token limit\n\t        Returns:\n", "            text (str): Trimmed text if exceeds the max token limit\n\t        \"\"\"\n\t        n_tokens = count_openai_tokens(text, self._tokenizer, self.openai_embedding_config.use_tiktoken)\n\t        if n_tokens <= self.max_seq_len:\n\t            return text\n\t        logger.warning(\n\t            \"The prompt has been truncated from %s tokens to %s tokens to fit\" \"within the max token limit.\",\n\t            \"Reduce the length of the prompt to prevent it from being cut off.\",\n\t            n_tokens,\n\t            self.max_seq_len,\n", "        )\n\t        if self.openai_embedding_config.use_tiktoken:\n\t            tokenized_payload = self._tokenizer.encode(text)\n\t            decoded_string = self._tokenizer.decode(tokenized_payload[: self.max_seq_len])\n\t        else:\n\t            tokenized_payload = self._tokenizer.tokenize(text)\n\t            decoded_string = self._tokenizer.convert_tokens_to_string(tokenized_payload[: self.max_seq_len])\n\t        return decoded_string\n\t    def embed(self, model: str, text: List[str]) -> np.ndarray:\n\t        \"\"\"\n", "        Embeds the batch of texts using the specified LLM.\n\t        Args:\n\t            model (str): LLM model name for embeddings.\n\t            text (List[str]): List of documents to be embedded.\n\t        Raises:\n\t            ValueError: When the OpenAI API key is missing.\n\t        Returns:\n\t            np.ndarray: embeddings for the input documents.\n\t        \"\"\"\n\t        if self.api_key is None:\n", "            raise ValueError(\n\t                \"OpenAI API key is not set. You can set it via the \" \"`api_key` parameter of the `LLMClient`.\"\n\t            )\n\t        generated_embeddings: List[Any] = []\n\t        headers: Dict[str, str] = {\"Content-Type\": \"application/json\"}\n\t        payload: Dict[str, Union[List[str], str]] = {\"model\": model, \"input\": text}\n\t        headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\t        res = openai_request(\n\t            url=self.openai_embedding_config.url,\n\t            headers=headers,\n", "            payload=payload,\n\t            timeout=self.time_out,\n\t        )\n\t        unordered_embeddings = [(ans[\"index\"], ans[\"embedding\"]) for ans in res[\"data\"]]\n\t        ordered_embeddings = sorted(unordered_embeddings, key=lambda x: x[0])\n\t        generated_embeddings = [emb[1] for emb in ordered_embeddings]\n\t        return np.array(generated_embeddings)\n\t    def embed_batch(self, model: str, text: List[str]) -> np.ndarray:\n\t        all_embeddings = []\n\t        for i in tqdm(\n", "            range(0, len(text), self.openai_embedding_config.batch_size),\n\t            disable=not self.openai_embedding_config.progress_bar,\n\t            desc=\"Calculating embeddings\",\n\t        ):\n\t            batch = text[i : i + self.openai_embedding_config.batch_size]\n\t            batch_limited = [self._ensure_text_limit(content) for content in batch]\n\t            generated_embeddings = self.embed(model, batch_limited)\n\t            all_embeddings.append(generated_embeddings)\n\t        return np.concatenate(all_embeddings)\n\t    def embed_queries(self, queries: List[str]) -> np.ndarray:\n", "        return self.embed_batch(self.query_encoder_model, queries)\n\t    def embed_documents(self, docs: List[Dict]) -> np.ndarray:\n\t        return self.embed_batch(self.doc_encoder_model, [d[\"text\"] for d in docs])\n"]}
{"filename": "chatgpt_memory/llm_client/openai/embedding/config.py", "chunked_list": ["from enum import Enum\n\tfrom chatgpt_memory.llm_client.config import LLMClientConfig\n\tclass EmbeddingModels(Enum):\n\t    ada = \"*-ada-*-001\"\n\t    babbage = \"*-babbage-*-001\"\n\t    curie = \"*-curie-*-001\"\n\t    davinci = \"*-davinci-*-001\"\n\tclass EmbeddingConfig(LLMClientConfig):\n\t    url: str = \"https://api.openai.com/v1/embeddings\"\n\t    batch_size: int = 64\n", "    progress_bar: bool = False\n\t    model: str = EmbeddingModels.ada.value\n\t    max_seq_len: int = 8191\n\t    use_tiktoken: bool = False\n"]}
{"filename": "chatgpt_memory/llm_client/openai/embedding/__init__.py", "chunked_list": []}
{"filename": "chatgpt_memory/llm_client/openai/conversation/config.py", "chunked_list": ["from chatgpt_memory.llm_client.config import LLMClientConfig\n\tclass ChatGPTConfig(LLMClientConfig):\n\t    temperature: float = 0\n\t    model_name: str = \"gpt-3.5-turbo\"\n\t    max_retries: int = 6\n\t    max_tokens: int = 256\n\t    verbose: bool = False\n"]}
{"filename": "chatgpt_memory/llm_client/openai/conversation/__init__.py", "chunked_list": []}
{"filename": "chatgpt_memory/llm_client/openai/conversation/chatgpt_client.py", "chunked_list": ["import logging\n\timport uuid\n\tfrom langchain import LLMChain, OpenAI, PromptTemplate\n\tfrom pydantic import BaseModel\n\tfrom chatgpt_memory.llm_client.llm_client import LLMClient\n\tfrom chatgpt_memory.llm_client.openai.conversation.config import ChatGPTConfig\n\tfrom chatgpt_memory.memory.manager import MemoryManager\n\tfrom chatgpt_memory.utils.openai_utils import get_prompt\n\tlogger = logging.getLogger(__name__)\n\tclass ChatGPTResponse(BaseModel):\n", "    conversation_id: str\n\t    message: str\n\t    chat_gpt_answer: str\n\tclass ChatGPTClient(LLMClient):\n\t    \"\"\"\n\t    ChatGPT client allows to interact with the ChatGPT model alonside having infinite contextual and adaptive memory.\n\t    \"\"\"\n\t    def __init__(self, config: ChatGPTConfig, memory_manager: MemoryManager):\n\t        super().__init__(config=config)\n\t        prompt = PromptTemplate(input_variables=[\"prompt\"], template=\"{prompt}\")\n", "        self.chatgpt_chain = LLMChain(\n\t            llm=OpenAI(\n\t                temperature=config.temperature,\n\t                openai_api_key=self.api_key,\n\t                model_name=config.model_name,\n\t                max_retries=config.max_retries,\n\t                max_tokens=config.max_tokens,\n\t            ),\n\t            prompt=prompt,\n\t            verbose=config.verbose,\n", "        )\n\t        self.memory_manager = memory_manager\n\t    def converse(self, message: str, conversation_id: str = None) -> ChatGPTResponse:\n\t        \"\"\"\n\t        Allows user to chat with user by leveraging the infinite contextual memor for fetching and\n\t        adding historical messages to the prompt to the ChatGPT model.\n\t        Args:\n\t            message (str): Message by the human user.\n\t            conversation_id (str, optional): Id of the conversation, if session already exists. Defaults to None.\n\t        Returns:\n", "            ChatGPTResponse: Response includes answer from th ChatGPT, conversation_id, and human message.\n\t        \"\"\"\n\t        if not conversation_id:\n\t            conversation_id = uuid.uuid4().hex\n\t        history = \"\"\n\t        try:\n\t            past_messages = self.memory_manager.get_messages(conversation_id=conversation_id, query=message)\n\t            history = \"\\n\".join([past_message.text for past_message in past_messages if getattr(past_message, \"text\")])\n\t        except ValueError as history_not_found_error:\n\t            logger.warning(\n", "                f\"No previous chat history found for conversation_id: {conversation_id}.\\nDetails: {history_not_found_error}\"\n\t            )\n\t        prompt = get_prompt(message=message, history=history)\n\t        chat_gpt_answer = self.chatgpt_chain.predict(prompt=prompt)\n\t        if len(message.strip()) and len(chat_gpt_answer.strip()):\n\t            self.memory_manager.add_message(conversation_id=conversation_id, human=message, assistant=chat_gpt_answer)\n\t        return ChatGPTResponse(message=message, chat_gpt_answer=chat_gpt_answer, conversation_id=conversation_id)\n"]}
{"filename": "chatgpt_memory/memory/manager.py", "chunked_list": ["from typing import Any, Dict, List\n\timport numpy as np\n\tfrom chatgpt_memory.datastore.redis import RedisDataStore\n\tfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\tfrom .memory import Memory\n\tclass MemoryManager:\n\t    \"\"\"\n\t    Manages the memory of conversations.\n\t    Attributes:\n\t        datastore (DataStore): Datastore to use for storing and retrieving memories.\n", "        embed_client (EmbeddingClient): Embedding client to call for embedding conversations.\n\t        conversations (List[Memory]): List of conversation IDs to memories to be managed.\n\t    \"\"\"\n\t    def __init__(self, datastore: RedisDataStore, embed_client: EmbeddingClient, topk: int = 5) -> None:\n\t        \"\"\"\n\t        Initializes the memory manager.\n\t        Args:\n\t            datastore (DataStore): Datastore to be used. Assumed to be connected.\n\t            embed_client (EmbeddingClient): Embedding client to be used.\n\t            topk (int): Number of past message to be retrieved as context for current message.\n", "        \"\"\"\n\t        self.datastore = datastore\n\t        self.embed_client = embed_client\n\t        self.topk = topk\n\t        self.conversations: List[Memory] = [\n\t            Memory(conversation_id=conversation_id) for conversation_id in datastore.get_all_conversation_ids()\n\t        ]\n\t    def __del__(self) -> None:\n\t        \"\"\"Clear the memory manager when manager is deleted.\"\"\"\n\t        self.clear()\n", "    def add_conversation(self, conversation: Memory) -> None:\n\t        \"\"\"\n\t        Adds a conversation to the memory manager to be stored and manage.\n\t        Args:\n\t            conversation (Memory): Conversation to be added.\n\t        \"\"\"\n\t        if conversation not in self.conversations:\n\t            self.conversations.append(conversation)\n\t    def remove_conversation(self, conversation: Memory) -> None:\n\t        \"\"\"\n", "        Removes a conversation from the memory manager.\n\t        Args:\n\t            conversation (Memory): Conversation to be removed containing `conversation_id`.\n\t        \"\"\"\n\t        if conversation not in self.conversations:\n\t            return\n\t        conversation_idx = self.conversations.index(conversation)\n\t        if conversation_idx >= 0:\n\t            del self.conversations[conversation_idx]\n\t            self.datastore.delete_documents(conversation_id=conversation.conversation_id)\n", "    def clear(self) -> None:\n\t        \"\"\"\n\t        Clears the memory manager.\n\t        \"\"\"\n\t        self.datastore.flush_all_documents()\n\t        self.conversations = []\n\t    def add_message(self, conversation_id: str, human: str, assistant: str) -> None:\n\t        \"\"\"\n\t        Adds a message to a conversation.\n\t        Args:\n", "            conversation_id (str): ID of the conversation to add the message to.\n\t            human (str): User message.\n\t            assistant (str): Assistant message.\n\t        \"\"\"\n\t        document: Dict = {\"text\": f\"Human: {human}\\nAssistant: {assistant}\", \"conversation_id\": conversation_id}\n\t        document[\"embedding\"] = self.embed_client.embed_documents(docs=[document])[0].astype(np.float32).tobytes()\n\t        self.datastore.index_documents(documents=[document])\n\t        # optionally check if it is a new conversation\n\t        self.add_conversation(Memory(conversation_id=conversation_id))\n\t    def get_messages(self, conversation_id: str, query: str) -> List[Any]:\n", "        \"\"\"\n\t        Gets the messages of a conversation using the query message.\n\t        Args:\n\t            conversation_id (str): ID of the conversation to get the messages of.\n\t            query (str): Current user message you want to pull history for to use in the prompt.\n\t            topk (int): Number of messages to be returned. Defaults to 5.\n\t        Returns:\n\t            List[Any]: List of messages of the conversation.\n\t        \"\"\"\n\t        if Memory(conversation_id=conversation_id) not in self.conversations:\n", "            raise ValueError(f\"Conversation id: {conversation_id} is not present in past conversations.\")\n\t        query_vector = self.embed_client.embed_queries([query])[0].astype(np.float32).tobytes()\n\t        messages = self.datastore.search_documents(\n\t            query_vector=query_vector, conversation_id=conversation_id, topk=self.topk\n\t        )\n\t        return messages\n"]}
{"filename": "chatgpt_memory/memory/__init__.py", "chunked_list": ["from chatgpt_memory.memory.manager import MemoryManager  # noqa: F401\n\tfrom chatgpt_memory.memory.memory import Memory  # noqa: F401\n"]}
{"filename": "chatgpt_memory/memory/memory.py", "chunked_list": ["\"\"\"\n\tContains a memory dataclass.\n\t\"\"\"\n\tfrom pydantic import BaseModel\n\tclass Memory(BaseModel):\n\t    \"\"\"\n\t    A memory dataclass.\n\t    \"\"\"\n\t    conversation_id: str\n\t    \"\"\"ID of the conversation.\"\"\"\n"]}
{"filename": "examples/simple_usage.py", "chunked_list": ["#!/bin/env python3\n\t\"\"\"\n\tThis script describes a simple usage of the library.\n\tYou can see a breakdown of the individual steps in the README.md file.\n\t\"\"\"\n\tfrom chatgpt_memory.datastore import RedisDataStore, RedisDataStoreConfig\n\t## set the following ENVIRONMENT Variables before running this script\n\t# Import necessary modules\n\tfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\tfrom chatgpt_memory.llm_client import ChatGPTClient, ChatGPTConfig, EmbeddingClient, EmbeddingConfig\n", "from chatgpt_memory.memory import MemoryManager\n\t# Instantiate an EmbeddingConfig object with the OpenAI API key\n\tembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\t# Instantiate an EmbeddingClient object with the EmbeddingConfig object\n\tembed_client = EmbeddingClient(config=embedding_config)\n\t# Instantiate a RedisDataStoreConfig object with the Redis connection details\n\tredis_datastore_config = RedisDataStoreConfig(\n\t    host=REDIS_HOST,\n\t    port=REDIS_PORT,\n\t    password=REDIS_PASSWORD,\n", ")\n\t# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n\tredis_datastore = RedisDataStore(config=redis_datastore_config)\n\t# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\n\tmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\t# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\n\tchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\t# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\n\tchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\t# Initialize conversation_id to None\n", "conversation_id = None\n\t# Start the chatbot loop\n\twhile True:\n\t    # Prompt the user for input\n\t    user_message = input(\"\\n \\033[92m Please enter your message: \")\n\t    # Use the ChatGPTClient object to generate a response\n\t    response = chat_gpt_client.converse(message=user_message, conversation_id=None)\n\t    # Update the conversation_id with the conversation_id from the response\n\t    conversation_id = response.conversation_id\n\t    print(\"\\n \\033[96m Assisstant: \" + response.chat_gpt_answer)\n", "    # Print the response generated by the chatbot\n\t    # print(response.chat_gpt_answer)\n"]}
