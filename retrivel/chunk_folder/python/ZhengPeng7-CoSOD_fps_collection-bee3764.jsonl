{"filename": "test_fps.py", "chunked_list": ["from time import time\n\timport torch\n\tdef test_fps(model, size=256, batch_size=2):\n\t    # Init model\n\t    # print('111!!!!')\n\t    model.cuda()\n\t    model.eval()\n\t    N = 2\n\t    time_total = 0.\n\t    buf_iter = -500\n", "    # print('!!!!')\n\t    with torch.no_grad():\n\t        for i in range(1+buf_iter, 1000+1):\n\t            # print(i, end=', ')\n\t            inputs = torch.randn(batch_size, 3, size, size).float().cuda()\n\t            time_st = time()\n\t            _ = model(inputs)\n\t            if i > 0:\n\t                time_latest = time() - time_st\n\t                time_total += time_latest\n", "                if i % 300 == 0:\n\t                    print(i, 'time_avg: {:.4f}s, time_curr: {:.4f}s.'.format(time_total / i / N, time_latest / N))\n\t    return time_total / i / N\n"]}
{"filename": "CoSOD_CoADNet/code/backbone.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\tfrom CoSOD_CoADNet.code.modules import CAM\n\tfrom CoSOD_CoADNet.code.ops import CU\n\timport torch.utils.model_zoo as model_zoo\n\tclass Bottleneck(nn.Module):\n\t    expansion = 4\n\t    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=(1, 1), residual=True):\n\t        super(Bottleneck, self).__init__()\n\t        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n\t        self.bn1 = nn.BatchNorm2d(planes)\n", "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=dilation[1], bias=False, dilation=dilation[1])\n\t        self.bn2 = nn.BatchNorm2d(planes)\n\t        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n\t        self.bn3 = nn.BatchNorm2d(planes * 4)\n\t        self.relu = nn.ReLU(inplace=True)\n\t        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x):\n\t        residual = x\n\t        out = self.conv1(x)\n", "        out = self.bn1(out)\n\t        out = self.relu(out)\n\t        out = self.conv2(out)\n\t        out = self.bn2(out)\n\t        out = self.relu(out)\n\t        out = self.conv3(out)\n\t        out = self.bn3(out)\n\t        if self.downsample is not None:\n\t            residual = self.downsample(x)\n\t        out += residual\n", "        out = self.relu(out)\n\t        return out\n\tclass DRN_A(nn.Module):\n\t    def __init__(self, block, layers, num_classes=1000):\n\t        self.inplanes = 64\n\t        super(DRN_A, self).__init__()\n\t        self.out_dim = 512 * block.expansion\n\t        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\t        self.bn1 = nn.BatchNorm2d(64)\n\t        self.relu = nn.ReLU(inplace=True)\n", "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\t        self.layer1 = self._make_layer(block, 64, layers[0])\n\t        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n\t        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n\t        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n\t        self.avgpool = nn.AvgPool2d(28, stride=1)\n\t        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n", "                m.weight.data.normal_(0, math.sqrt(2. / n))\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                m.weight.data.fill_(1)\n\t                m.bias.data.zero_()\n\t    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n\t        downsample = None\n\t        if stride != 1 or self.inplanes != planes * block.expansion:\n\t            downsample = nn.Sequential(\n\t                     nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n\t                     nn.BatchNorm2d(planes * block.expansion))\n", "        layers = []\n\t        layers.append(block(self.inplanes, planes, stride, downsample))\n\t        self.inplanes = planes * block.expansion\n\t        for i in range(1, blocks):\n\t            layers.append(block(self.inplanes, planes, dilation=(dilation, dilation)))\n\t        return nn.Sequential(*layers)\n\t    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n", "        x = self.maxpool(x)\n\t        x = self.layer1(x)\n\t        x = self.layer2(x)\n\t        x = self.layer3(x)\n\t        x = self.layer4(x)\n\t        x = self.avgpool(x)\n\t        x = x.view(x.size(0), -1)\n\t        x = self.fc(x)\n\t        return x\n\tdef Build_DRN_A_50(pretrained=False, **kwargs):\n", "    model = DRN_A(Bottleneck, [3, 4, 6, 3], **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'))\n\t    return model\n\tclass Backbone_Wrapper_VGG16(nn.Module):\n\t    def __init__(self):\n\t        super(Backbone_Wrapper_VGG16, self).__init__()\n\t        bb = models.vgg16_bn(pretrained=False).features\n\t        self.C1 = nn.Sequential()\n\t        self.C2 = nn.Sequential()\n", "        self.C3 = nn.Sequential()\n\t        self.C4 = nn.Sequential()\n\t        self.C5 = nn.Sequential()\n\t        for layer_index, (name, sub_module) in enumerate(bb.named_children()):\n\t            if layer_index>=0 and layer_index<=5:\n\t                self.C1.add_module(name, sub_module)           \n\t            if layer_index>=7 and layer_index <= 12:\n\t                self.C2.add_module(name, sub_module)\n\t            if layer_index>=14 and layer_index <= 22:\n\t                self.C3.add_module(name, sub_module)\n", "            if layer_index>=24 and layer_index <= 32:\n\t                self.C4.add_module(name, sub_module)\n\t            if layer_index>=34 and layer_index <= 42:\n\t                self.C5.add_module(name, sub_module)\n\t        # channels = [64, 128, 256, 512, 512]\n\t        self.C4_Att = CAM(512, 8)\n\t        self.C5_Att = CAM(512, 8)\n\t        self.pool = nn.MaxPool2d(kernel_size=2)\n\t    def forward(self, image):\n\t        # image: [B, 3, 128, 128]\n", "        ftr_s = self.C3(self.pool(self.C2(self.C1(image)))) # [B, 256, 64, 64]\n\t        ftr_d = self.C5_Att(self.C5(self.pool(self.C4_Att(self.C4(self.pool(ftr_s)))))) # [B, 512, 16, 16]\n\t        return ftr_d\n\tclass Backbone_Wrapper_ResNet50(nn.Module):\n\t    def __init__(self):\n\t        super(Backbone_Wrapper_ResNet50, self).__init__()\n\t        bb = models.resnet50(pretrained=False)\n\t        bb.conv1.stride = (1, 1)\n\t        self.C1 = nn.Sequential(bb.conv1, bb.bn1, bb.relu)\n\t        self.C2 = bb.layer1\n", "        self.C3 = bb.layer2\n\t        self.C4 = bb.layer3\n\t        self.C5 = bb.layer4\n\t        # channels = [64, 256, 512, 1024, 2048]\n\t        self.C4_Att = CAM(1024, 8)\n\t        self.C5_Att = CAM(2048, 16)\n\t        self.squeeze_channels = CU(2048, 1024, 1, True, 'relu')\n\t    def forward(self, image):\n\t        # image: [B, 3, 128, 128]\n\t        ftr_s = self.C3(self.C2(self.C1(image))) # [B, 512, 64, 64]\n", "        ftr_d = self.C5_Att(self.C5(self.C4_Att(self.C4(ftr_s)))) # [B, 2048, 16, 16]\n\t        ftr_d = self.squeeze_channels(ftr_d) # [B, 1024, 16, 16]\n\t        return ftr_d\n\tclass Backbone_Wrapper_Dilated_ResNet50(nn.Module):\n\t    def __init__(self):\n\t        super(Backbone_Wrapper_Dilated_ResNet50, self).__init__()\n\t        bb = Build_DRN_A_50(pretrained=False)\n\t        bb.conv1.stride = (1, 1)\n\t        self.C1 = nn.Sequential(bb.conv1, bb.bn1, bb.relu, bb.maxpool)\n\t        self.C2 = bb.layer1\n", "        self.C3 = bb.layer2\n\t        self.C4 = bb.layer3\n\t        self.C5 = bb.layer4\n\t        # channels = [64, 256, 512, 1024, 2048]\n\t        self.C4_Att = CAM(1024, 8)\n\t        self.C5_Att = CAM(2048, 16)\n\t        self.pool = nn.MaxPool2d(kernel_size=2)\n\t        self.squeeze_channels = CU(2048, 1024, 1, True, 'relu')\n\t    def forward(self, image):\n\t        # image: [B, 3, 128, 128]\n", "        ftr_s = self.C2(self.C1(image)) # [B, 256, 64, 64]\n\t        ftr_d = self.C5_Att(self.C5(self.C4_Att(self.C4(self.pool(self.C3(ftr_s)))))) # [B, 2048, 16, 16]\n\t        ftr_d = self.squeeze_channels(ftr_d) # [B, 1024, 16, 16]\n\t        return ftr_d\n"]}
{"filename": "CoSOD_CoADNet/code/network.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\tfrom CoSOD_CoADNet.code.ops import *\n\tfrom CoSOD_CoADNet.code.misc import *\n\tfrom CoSOD_CoADNet.code.modules import *\n\tfrom CoSOD_CoADNet.code.backbone import Backbone_Wrapper_VGG16, Backbone_Wrapper_ResNet50, Backbone_Wrapper_Dilated_ResNet50\n\tclass CoADNet_VGG16(nn.Module):\n\t    def __init__(self, mode, compute_loss):\n\t        super(CoADNet_VGG16, self).__init__()\n\t        assert mode in ['train', 'test']\n\t        self.mode = mode\n", "        self.compute_loss = compute_loss\n\t        self.M = 5\n\t        self.D = 512\n\t        self.S = 4\n\t        assert np.mod(self.D, self.S) == 0\n\t        self.backbone = Backbone_Wrapper_VGG16()\n\t        self.IaSH = IaSH('VGG16')\n\t        self.online_intra_saliency_guidance = OIaSG()\n\t        self.lca = nn.ModuleList([LCA(self.D//self.S, 1, [1, 3, 5]) for s in range(self.S)])\n\t        self.gca = nn.ModuleList([GCA(self.D//self.S, 1) for s in range(self.S)])\n", "        self.block_fusion = BlockFusion(self.D, 2)\n\t        self.ggd = GGD(self.D, 1)\n\t        decode_dims = [self.D, 512, 256, 128]\n\t        self.gcpd_1 = GCPD(decode_dims[0], decode_dims[1], 4)\n\t        self.gcpd_2 = GCPD(decode_dims[1], decode_dims[2], 1)\n\t        self.gcpd_3 = GCPD(decode_dims[2], decode_dims[3], 1)\n\t        self.cosal_head = CoSH(decode_dims[3])\n\t    def forward(self, gi, si=None, gl=None, sl=None):\n\t        # gi: [Bg, M, 3, 128, 128]\n\t        # si: [Bs, 3, 128, 128]\n", "        # gl: [Bg, M, 1, 128, 128]\n\t        # sl: [Bs, 1, 128, 128]\n\t        M = self.M # number of groups\n\t        D = self.D # backbone feature dimension\n\t        S = self.S # number of blocks\n\t        H, W = gi.size(3), gi.size(4)\n\t        if self.mode == 'train':\n\t            assert si is not None\n\t            assert gl is not None\n\t            assert sl is not None\n", "            Bg = gi.size(0)\n\t            Bs = si.size(0)\n\t            si_ftr = self.backbone(si) # [Bs, 512, 16, 16]\n\t            si_sm = self.IaSH(si_ftr) # [Bs, 1, 64, 64]\n\t        if self.mode == 'test':\n\t            assert si is None\n\t            assert gl is None\n\t            assert sl is None\n\t            Bg = gi.size(0)\n\t        # feature extrcation on gi\n", "        gi_ftr = self.backbone(gi.view(-1, 3, H, W)) # [Bg*M, D, 16, 16]\n\t        gi_sm = self.IaSH(gi_ftr) # [Bg*M, 1, 64, 64]\n\t        gi_ftr_g = self.online_intra_saliency_guidance(DS(gi_sm, 4), gi_ftr).view(Bg, M, D, H//8, W//8) # [Bg, M, D, 16, 16]\n\t        # block-wise group shuffling\n\t        # re-organize a tensor \"gi_ftr_g\" into a tuple \"shuffled_blocks\"\n\t        # \"shuffled_blocks[sid] (sid=1,...,S)\" is a tensor with dimension of [Bg, M*D/S, 16, 16]\n\t        shuffled_blocks = torch.chunk(self.block_wise_shuffle(gi_ftr_g, S), S, dim=1) \n\t        # local & global aggregation\n\t        bag = []\n\t        for sid in range(S):\n", "            blk = self.attentional_aggregation(shuffled_blocks[sid], M) # [Bg, D/S, 16, 16]\n\t            blk = self.lca[sid](blk) # [Bg, D/S, 16, 16]\n\t            blk = self.gca[sid](blk) # [Bg, D/S, 16, 16]\n\t            bag.append(blk)\n\t        concat_blk = torch.cat(bag, dim=1) # [Bg, D, 16, 16]\n\t        gs = self.block_fusion(concat_blk) # [Bg, D, 16, 16]\n\t        # gated group distribution\n\t        X_0 = []\n\t        for mid in range(M):\n\t            gi_ftr_cosal = self.ggd(gs, gi_ftr_g[:, mid, :, :, :]) # [Bg, D, 16, 16]\n", "            X_0.append(gi_ftr_cosal.unsqueeze(1)) # [Bg, 1, D, 16, 16]\n\t        X_0 = torch.cat(X_0, dim=1) # [Bg, M, D, 16, 16]\n\t        # feature decoding\n\t        X_1 = self.gcpd_1(X_0) # [Bg, M, 512, 32, 32]\n\t        X_2 = self.gcpd_2(X_1) # [Bg, M, 256, 64, 64]\n\t        X_3 = self.gcpd_3(X_2) # [Bg, M, 256, 128, 128]\n\t        csm = self.cosal_head(X_3) # [Bg, M, 1, 128, 128]\n\t        if self.mode=='train' and self.compute_loss==True:\n\t            cosod_loss = self.compute_cosod_loss(csm, gl)\n\t            sod_loss = self.compute_sod_loss(si_sm, sl)\n", "            return csm, cosod_loss, sod_loss\n\t        else:\n\t            return csm\n\t    def block_wise_shuffle(self, gi_ftr_g, S):\n\t        # gi_ftr_g: [Bg, M, D, fh, fw]\n\t        Bg, M, D, fh, fw = gi_ftr_g.size()\n\t        assert np.mod(D, S) == 0\n\t        return gi_ftr_g.view(Bg, M, S, D//S, fh, fw).transpose(1, 2).contiguous().view(Bg, M, D, fh, fw).view(Bg, M*D, fh, fw)\n\t    def attentional_aggregation(self, blk, M):\n\t        # blk: [B, M*D/S, fh, fw]\n", "        # blk_agg: # [B, D/S, fh, fw]\n\t        blk = F.softmax(blk, dim=1) * blk # [B, M*D/S, fh, fw]\n\t        blk_bag = torch.chunk(blk, M, dim=1) # blk_bag[mid]: [B, D/S, H, W], mid=1,...,M\n\t        blk_agg = blk_bag[0] # [B, D/S, fh, fw]\n\t        for mid in range(1, M):\n\t            blk_agg = blk_agg + blk_bag[mid] # [B, D/S, fh, fw]\n\t        return blk_agg\n\t    def compute_cosod_loss(self, csm, gl):\n\t        # csm: [Bg, M, 1, 128, 128]\n\t        # gl: [Bg, M, 1, 128, 128]\n", "        Bg, M, _, H, W = gl.size()\n\t        cm = csm.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n\t        gt = gl.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n\t        return F.binary_cross_entropy(cm, gt)\n\t    def compute_sod_loss(self, si_sm, sl):\n\t        # si_sm: [Bs, 1, 64, 64]\n\t        # sl: [Bs, 1, 128, 128]\n\t        return F.binary_cross_entropy(si_sm, DS(sl, 2, 'max'))\n\tclass CoADNet_ResNet50(nn.Module):\n\t    def __init__(self, mode, compute_loss):\n", "        super(CoADNet_ResNet50, self).__init__()\n\t        assert mode in ['train', 'test']\n\t        self.mode = mode\n\t        self.compute_loss = compute_loss\n\t        self.M = 5\n\t        self.D = 1024\n\t        self.S = 8\n\t        assert np.mod(self.D, self.S) == 0\n\t        self.backbone = Backbone_Wrapper_Dilated_ResNet50()\n\t        self.IaSH = IaSH('ResNet50')\n", "        self.online_intra_saliency_guidance = OIaSG()\n\t        self.lca = nn.ModuleList([LCA(self.D//self.S, 4, [1, 3, 5]) for s in range(self.S)])\n\t        self.gca = nn.ModuleList([GCA(self.D//self.S, 2) for s in range(self.S)])\n\t        self.block_fusion = BlockFusion(self.D, 8)\n\t        self.ggd = GGD(self.D, 8)\n\t        decode_dims = [self.D, 512, 256, 128]\n\t        self.gcpd_1 = GCPD(decode_dims[0], decode_dims[1], 8)\n\t        self.gcpd_2 = GCPD(decode_dims[1], decode_dims[2], 2)\n\t        self.gcpd_3 = GCPD(decode_dims[2], decode_dims[3], 2)\n\t        self.cosal_head = CoSH(decode_dims[3])\n", "    def forward(self, gi, si=None, gl=None, sl=None):\n\t        # gi: [Bg, M, 3, 128, 128]\n\t        # si: [Bs, 3, 128, 128]\n\t        # gl: [Bg, M, 1, 128, 128]\n\t        # sl: [Bs, 1, 128, 128]\n\t        M = self.M # number of groups\n\t        D = self.D # backbone feature dimension\n\t        S = self.S # number of blocks\n\t        H, W = gi.size(3), gi.size(4)\n\t        if self.mode == 'train':\n", "            assert si is not None\n\t            assert gl is not None\n\t            assert sl is not None\n\t            Bg = gi.size(0)\n\t            Bs = si.size(0)\n\t            si_ftr = self.backbone(si) # [Bs, 1024, 16, 16]\n\t            si_sm = self.IaSH(si_ftr) # [Bs, 1, 64, 64]\n\t        if self.mode == 'test':\n\t            assert si is None\n\t            assert gl is None\n", "            assert sl is None\n\t            Bg = gi.size(0)\n\t        # feature extrcation on gi\n\t        gi_ftr = self.backbone(gi.view(-1, 3, H, W)) # [Bg*M, D, 16, 16]\n\t        gi_sm = self.IaSH(gi_ftr) # [Bg*M, 1, 64, 64]\n\t        gi_ftr_g = self.online_intra_saliency_guidance(DS(gi_sm, 4), gi_ftr).view(Bg, M, D, H//8, W//8) # [Bg, M, D, 16, 16]\n\t        # block-wise group shuffling\n\t        # re-organize a tensor \"gi_ftr_g\" into a tuple \"shuffled_blocks\"\n\t        # \"shuffled_blocks[sid] (sid=1,...,S)\" is a tensor with dimension of [Bg, M*D/S, 16, 16]\n\t        shuffled_blocks = torch.chunk(self.block_wise_shuffle(gi_ftr_g, S), S, dim=1) \n", "        # local & global aggregation\n\t        bag = []\n\t        for sid in range(S):\n\t            blk = self.attentional_aggregation(shuffled_blocks[sid], M) # [Bg, D/S, 16, 16]\n\t            blk = self.lca[sid](blk) # [Bg, D/S, 16, 16]\n\t            blk = self.gca[sid](blk) # [Bg, D/S, 16, 16]\n\t            bag.append(blk)\n\t        concat_blk = torch.cat(bag, dim=1) # [Bg, D, 16, 16]\n\t        gs = self.block_fusion(concat_blk) # [Bg, D, 16, 16]\n\t        # gated group distribution\n", "        X_0 = []\n\t        for mid in range(M):\n\t            gi_ftr_cosal = self.ggd(gs, gi_ftr_g[:, mid, :, :, :]) # [Bg, D, 16, 16]\n\t            X_0.append(gi_ftr_cosal.unsqueeze(1)) # [Bg, 1, D, 16, 16]\n\t        X_0 = torch.cat(X_0, dim=1) # [Bg, M, D, 16, 16]\n\t        # feature decoding\n\t        X_1 = self.gcpd_1(X_0) # [Bg, M, 512, 32, 32]\n\t        X_2 = self.gcpd_2(X_1) # [Bg, M, 256, 64, 64]\n\t        X_3 = self.gcpd_3(X_2) # [Bg, M, 256, 128, 128]\n\t        csm = self.cosal_head(X_3) # [Bg, M, 1, 128, 128]\n", "        if self.mode=='train' and self.compute_loss==True:\n\t            cosod_loss = self.compute_cosod_loss(csm, gl)\n\t            sod_loss = self.compute_sod_loss(si_sm, sl)\n\t            return csm, cosod_loss, sod_loss\n\t        else:\n\t            return csm\n\t    def block_wise_shuffle(self, gi_ftr_g, S):\n\t        # gi_ftr_g: [Bg, M, D, fh, fw]\n\t        Bg, M, D, fh, fw = gi_ftr_g.size()\n\t        assert np.mod(D, S) == 0\n", "        return gi_ftr_g.view(Bg, M, S, D//S, fh, fw).transpose(1, 2).contiguous().view(Bg, M, D, fh, fw).view(Bg, M*D, fh, fw)\n\t    def attentional_aggregation(self, blk, M):\n\t        # blk: [B, M*D/S, fh, fw]\n\t        # blk_agg: # [B, D/S, fh, fw]\n\t        blk = F.softmax(blk, dim=1) * blk # [B, M*D/S, fh, fw]\n\t        blk_bag = torch.chunk(blk, M, dim=1) # blk_bag[mid]: [B, D/S, H, W], mid=1,...,M\n\t        blk_agg = blk_bag[0] # [B, D/S, fh, fw]\n\t        for mid in range(1, M):\n\t            blk_agg = blk_agg + blk_bag[mid] # [B, D/S, fh, fw]\n\t        return blk_agg\n", "    def compute_cosod_loss(self, csm, gl):\n\t        # csm: [Bg, M, 1, 128, 128]\n\t        # gl: [Bg, M, 1, 128, 128]\n\t        Bg, M, _, H, W = gl.size()\n\t        cm = csm.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n\t        gt = gl.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n\t        return F.binary_cross_entropy(cm, gt)\n\t    def compute_sod_loss(self, si_sm, sl):\n\t        # si_sm: [Bs, 1, 64, 64]\n\t        # sl: [Bs, 1, 128, 128]\n", "        return F.binary_cross_entropy(si_sm, DS(sl, 2, 'max'))\n\tclass CoADNet_Dilated_ResNet50(nn.Module):\n\t    def __init__(self, mode='test', compute_loss=False):\n\t        super(CoADNet_Dilated_ResNet50, self).__init__()\n\t        assert mode in ['train', 'test']\n\t        self.mode = mode\n\t        self.compute_loss = compute_loss\n\t        self.M = 2\n\t        self.D = 1024\n\t        self.S = 8\n", "        assert np.mod(self.D, self.S) == 0\n\t        self.backbone = Backbone_Wrapper_Dilated_ResNet50()\n\t        self.IaSH = IaSH('Dilated_ResNet50')\n\t        self.online_intra_saliency_guidance = OIaSG()\n\t        self.lca = nn.ModuleList([LCA(self.D//self.S, 4, [1, 3, 5]) for s in range(self.S)])\n\t        self.gca = nn.ModuleList([GCA(self.D//self.S, 2) for s in range(self.S)])\n\t        self.block_fusion = BlockFusion(self.D, 8)\n\t        self.ggd = GGD(self.D, 8)\n\t        decode_dims = [self.D, 512, 256, 128]\n\t        self.gcpd_1 = GCPD(decode_dims[0], decode_dims[1], 8)\n", "        self.gcpd_2 = GCPD(decode_dims[1], decode_dims[2], 2)\n\t        self.gcpd_3 = GCPD(decode_dims[2], decode_dims[3], 2)\n\t        self.cosal_head = CoSH(decode_dims[3])\n\t    def forward(self, gi, si=None, gl=None, sl=None):\n\t        # gi: [Bg, M, 3, 128, 128]\n\t        # si: [Bs, 3, 128, 128]\n\t        # gl: [Bg, M, 1, 128, 128]\n\t        # sl: [Bs, 1, 128, 128]\n\t        M = self.M # number of groups\n\t        D = self.D # backbone feature dimension\n", "        S = self.S # number of blocks\n\t        gi = gi.unsqueeze(0)\n\t        H, W = gi.size(3), gi.size(4)\n\t        if self.mode == 'train':\n\t            assert si is not None\n\t            assert gl is not None\n\t            assert sl is not None\n\t            Bg = gi.size(0)\n\t            Bs = si.size(0)\n\t            si_ftr = self.backbone(si) # [Bs, 1024, 16, 16]\n", "            si_sm = self.IaSH(si_ftr) # [Bs, 1, 64, 64]\n\t        if self.mode == 'test':\n\t            assert si is None\n\t            assert gl is None\n\t            assert sl is None\n\t            Bg = gi.size(0)\n\t        # feature extrcation on gi\n\t        gi_ftr = self.backbone(gi.view(-1, 3, H, W)) # [Bg*M, D, 16, 16]\n\t        gi_sm = self.IaSH(gi_ftr) # [Bg*M, 1, 64, 64]\n\t        gi_ftr_g = self.online_intra_saliency_guidance(DS(gi_sm, 4), gi_ftr).view(Bg, M, D, H//8, W//8) # [Bg, M, D, 16, 16]\n", "        # block-wise group shuffling\n\t        # re-organize a tensor \"gi_ftr_g\" into a tuple \"shuffled_blocks\"\n\t        # \"shuffled_blocks[sid] (sid=1,...,S)\" is a tensor with dimension of [Bg, M*D/S, 16, 16]\n\t        shuffled_blocks = torch.chunk(self.block_wise_shuffle(gi_ftr_g, S), S, dim=1) \n\t        # local & global aggregation\n\t        bag = []\n\t        for sid in range(S):\n\t            blk = self.attentional_aggregation(shuffled_blocks[sid], M) # [Bg, D/S, 16, 16]\n\t            blk = self.lca[sid](blk) # [Bg, D/S, 16, 16]\n\t            blk = self.gca[sid](blk) # [Bg, D/S, 16, 16]\n", "            bag.append(blk)\n\t        concat_blk = torch.cat(bag, dim=1) # [Bg, D, 16, 16]\n\t        gs = self.block_fusion(concat_blk) # [Bg, D, 16, 16]\n\t        # gated group distribution\n\t        X_0 = []\n\t        for mid in range(M):\n\t            gi_ftr_cosal = self.ggd(gs, gi_ftr_g[:, mid, :, :, :]) # [Bg, D, 16, 16]\n\t            X_0.append(gi_ftr_cosal.unsqueeze(1)) # [Bg, 1, D, 16, 16]\n\t        X_0 = torch.cat(X_0, dim=1) # [Bg, M, D, 16, 16]\n\t        # feature decoding\n", "        X_1 = self.gcpd_1(X_0) # [Bg, M, 512, 32, 32]\n\t        X_2 = self.gcpd_2(X_1) # [Bg, M, 256, 64, 64]\n\t        X_3 = self.gcpd_3(X_2) # [Bg, M, 256, 128, 128]\n\t        csm = self.cosal_head(X_3) # [Bg, M, 1, 128, 128]\n\t        if self.mode=='train' and self.compute_loss==True:\n\t            cosod_loss = self.compute_cosod_loss(csm, gl)\n\t            sod_loss = self.compute_sod_loss(si_sm, sl)\n\t            return csm, cosod_loss, sod_loss\n\t        else:\n\t            return csm\n", "    def block_wise_shuffle(self, gi_ftr_g, S):\n\t        # gi_ftr_g: [Bg, M, D, fh, fw]\n\t        Bg, M, D, fh, fw = gi_ftr_g.size()\n\t        assert np.mod(D, S) == 0\n\t        return gi_ftr_g.view(Bg, M, S, D//S, fh, fw).transpose(1, 2).contiguous().view(Bg, M, D, fh, fw).view(Bg, M*D, fh, fw)\n\t    def attentional_aggregation(self, blk, M):\n\t        # blk: [B, M*D/S, fh, fw]\n\t        # blk_agg: # [B, D/S, fh, fw]\n\t        blk = F.softmax(blk, dim=1) * blk # [B, M*D/S, fh, fw]\n\t        blk_bag = torch.chunk(blk, M, dim=1) # blk_bag[mid]: [B, D/S, H, W], mid=1,...,M\n", "        blk_agg = blk_bag[0] # [B, D/S, fh, fw]\n\t        for mid in range(1, M):\n\t            blk_agg = blk_agg + blk_bag[mid] # [B, D/S, fh, fw]\n\t        return blk_agg\n\t    def compute_cosod_loss(self, csm, gl):\n\t        # csm: [Bg, M, 1, 128, 128]\n\t        # gl: [Bg, M, 1, 128, 128]\n\t        Bg, M, _, H, W = gl.size()\n\t        cm = csm.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n\t        gt = gl.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n", "        return F.binary_cross_entropy(cm, gt)\n\t    def compute_sod_loss(self, si_sm, sl):\n\t        # si_sm: [Bs, 1, 64, 64]\n\t        # sl: [Bs, 1, 128, 128]\n\t        return F.binary_cross_entropy(si_sm, DS(sl, 2, 'max'))\n"]}
{"filename": "CoSOD_CoADNet/code/ops.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\tdef US(x, up_scale_factor):\n\t    # up-scale features\n\t    # x: [B, C, H, W]\n\t    # y: [B, C, H_us, W_us]\n\t    isinstance(up_scale_factor, int)\n\t    y = F.interpolate(x, scale_factor=2, mode='bilinear')\n\t    return y\n\tdef DS(x, down_scale_factor, ds_mode='max'):\n\t    # down-scale features\n", "    # x: [B, C, H, W]\n\t    # y: [B, C, H_ds, W_ds]\n\t    isinstance(down_scale_factor, int)\n\t    assert ds_mode in ['max', 'avg']\n\t    if ds_mode == 'max':\n\t        y = F.max_pool2d(x, down_scale_factor)\n\t    if ds_mode == 'avg':\n\t        y = F.avg_pool2d(x, down_scale_factor)\n\t    return y\n\tdef GAP(x):\n", "    # global average pooling\n\t    # x: [B, C, H, W]\n\t    # y: [B, C, 1, 1]\n\t    y = F.adaptive_avg_pool2d(x, (1, 1))\n\t    return y\n\tclass CU(nn.Module):\n\t    # Convolution Unit\n\t    def __init__(self, ic, oc, ks, is_bn, na):\n\t        # ic: input channels\n\t        # oc: output channels\n", "        # ks: kernel size\n\t        # is_bn: True/False\n\t        # na: non-linear activation\n\t        super(CU, self).__init__()\n\t        assert isinstance(ic, int)\n\t        assert isinstance(oc, int)\n\t        assert isinstance(ks, int)\n\t        assert isinstance(is_bn, bool)\n\t        assert isinstance(na, str)\n\t        assert np.mod(ks + 1, 2) == 0\n", "        assert na in ['none', 'relu', 'sigmoid']\n\t        self.is_bn = is_bn\n\t        self.na = na\n\t        self.convolution = nn.Conv2d(ic, oc, ks, padding=(ks-1)//2, bias=False)\n\t        if self.is_bn:\n\t            self.batch_normalization = nn.BatchNorm2d(oc)\n\t        if self.na == 'relu':\n\t            self.activation = nn.ReLU(inplace=True)\n\t        if self.na == 'sigmoid':\n\t            self.activation = nn.Sigmoid()\n", "    def forward(self, x):\n\t        # x: [B, ic, H, W]\n\t        # y: [B, oc, H, W]\n\t        y = self.convolution(x)\n\t        if self.is_bn:\n\t            y = self.batch_normalization(y)\n\t        if self.na != 'none':\n\t            y = self.activation(y)\n\t        return y\n\tclass FC(nn.Module):\n", "    # Fully-Connected Layer\n\t    def __init__(self, ic, oc, is_bn, na):\n\t        super(FC, self).__init__()\n\t        # ic: input channels\n\t        # oc: output channels\n\t        # is_bn: True/False\n\t        # na: non-linear activation\n\t        assert isinstance(ic, int)\n\t        assert isinstance(oc, int)\n\t        assert isinstance(is_bn, bool)\n", "        assert isinstance(na, str)\n\t        assert na in ['none', 'relu', 'sigmoid']\n\t        self.is_bn = is_bn\n\t        self.na = na\n\t        self.linear = nn.Linear(ic, oc, bias=False)\n\t        if self.is_bn:\n\t            self.batch_normalization = nn.BatchNorm1d(oc)\n\t        if self.na == 'relu':\n\t            self.activation = nn.ReLU(inplace=True)\n\t        if self.na == 'sigmoid':\n", "            self.activation = nn.Sigmoid()\n\t    def forward(self, x):\n\t        # x: [B, ic]\n\t        # y: [B, oc]\n\t        y = self.linear(x)\n\t        if self.is_bn:\n\t            y = self.batch_normalization(y)\n\t        if self.na != 'none':\n\t            y = self.activation(y)\n\t        return y\n", "class DilConv3(nn.Module):\n\t    #  Dilated Convolution with 3*3 kernel size\n\t    def __init__(self, ic, oc, is_bn, na, dr):\n\t        super(DilConv3, self).__init__()\n\t        # ic: input channels\n\t        # oc: output channels\n\t        # is_bn: True/False\n\t        # na: non-linear activation\n\t        # dr: dilation rate\n\t        assert isinstance(ic, int)\n", "        assert isinstance(oc, int)\n\t        assert isinstance(is_bn, bool)\n\t        assert isinstance(na, str)\n\t        assert isinstance(dr, int)\n\t        assert na in ['none', 'relu', 'sigmoid']\n\t        self.is_bn = is_bn\n\t        self.na = na\n\t        self.dil_conv = nn.Conv2d(ic, oc, kernel_size=3, padding=dr, dilation=dr, bias=False)\n\t        if self.is_bn:\n\t            self.batch_normalization = nn.BatchNorm2d(oc)\n", "        if self.na == 'relu':\n\t            self.activation = nn.ReLU(inplace=True)\n\t        if self.na == 'sigmoid':\n\t            self.activation = nn.Sigmoid()\n\t    def forward(self, x):\n\t        # x: [B, ic, H, W]\n\t        # y: [B, oc, H, W]\n\t        y = self.dil_conv(x)\n\t        if self.is_bn:\n\t            y = self.batch_normalization(y)\n", "        if self.na != 'none':\n\t            y = self.activation(y) \n\t        return y\n"]}
{"filename": "CoSOD_CoADNet/code/misc.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\tdef align_number(number, N):\n\t    assert type(number) == int\n\t    num_str = str(number)\n\t    assert len(num_str) <= N\n\t    return (N - len(num_str)) * '0' + num_str\n\tdef min_max_normalize(x):\n\t    x_normed = (x - np.min(x)) / (np.max(x)-np.min(x))\n\t    return x_normed\n\tdef visualize_image_tensor(image_tensor):\n", "    # image_tensor: [3, H, W]\n\t    # image_pil: PIL.Image object\n\t    unloader = transforms.ToPILImage()\n\t    image_tensor = image_tensor.cpu()\n\t    image_pil = unloader(image_tensor)\n\t    return image_pil\n\tdef visualize_label_tensor(label_tensor):\n\t    # label_tensor: [1, H, W]\n\t    # label_pil: PIL.Image object\n\t    label_numpy = label_tensor.squeeze(0).cpu().data.numpy() # (H, W)\n", "    label_numpy = min_max_normalize(label_numpy) * 255\n\t    label_pil = Image.fromarray(label_numpy).convert('L')\n\t    return label_pil\n\tdef binarize_label(label_tensor, threshold):\n\t    assert label_tensor.min()>=0 and label_tensor.max()<=1\n\t    assert threshold>=0 and threshold<=1\n\t    label_tensor[label_tensor < threshold] = 0\n\t    label_tensor[label_tensor >= threshold] = 1\n\t    return label_tensor\n\tdef random_crop_tensor(x, cropped_h, cropped_w):\n", "    # x: [in_channels, input_h, input_w]\n\t    # y: [in_channels, cropped_h, cropped_w]\n\t    in_channels, input_h, input_w = x.size()\n\t    assert input_h>cropped_h and input_w>cropped_w\n\t    h_start = np.random.randint(0, (input_h-cropped_h))\n\t    w_start = np.random.randint(0, (input_w-cropped_w))\n\t    y = x[:, h_start:(h_start+cropped_h), w_start:(w_start+cropped_w)] # [in_channels, cropped_h, cropped_w]\n\t    return y\n\tdef random_crop_image_label(image, label, expansion_ratio):\n\t    # image & label: PIL.Image\n", "    # image_cropped, label_cropped: torch.tensor, [3, height, width] & [1, height, width]\n\t    assert image.height==label.height and image.width==label.width\n\t    assert expansion_ratio > 0\n\t    height, width = image.height, image.width\n\t    exp_height, exp_width = int(height*(1+expansion_ratio)), int(width*(1+expansion_ratio))\n\t    to_tensor = transforms.ToTensor()\n\t    image = to_tensor(image.resize((exp_height, exp_width))) # [3, exp_height, exp_width]\n\t    label = to_tensor(label.resize((exp_height, exp_width))) # [1, exp_height, exp_width]\n\t    image_concat_label = torch.cat((image, label), dim=0) # [4, exp_height, exp_width]\n\t    image_concat_label_cropped = random_crop_tensor(image_concat_label, height, width) # [4, height, width]\n", "    image_cropped = image_concat_label_cropped[0:3, :, :] # [3, height, width]\n\t    label_cropped = image_concat_label_cropped[3:4, :, :] # [1, height, width]\n\t    label_cropped = binarize_label(label_cropped, 0.50) # [1, height, width]\n\t    return image_cropped, label_cropped\n\t# def read_image(path_list_every, return_list):\n\t#     for load_path in path_list_every:\n\t#         return_list.append(Image.open(load_path))\n\t# def image_loader_multi_processing(path_list, num_cores):\n\t#     num_total = len(path_list)\n\t#     num_every = int(num_total / num_cores) + 1\n", "#     path_list_every = []\n\t#     for index in range(num_cores):\n\t#         index_start = index * num_every\n\t#         index_end = (index + 1) * num_every\n\t#         path_list_every.append(path_list[index_start:index_end])\n\t#     manager = Manager()\n\t#     return_list = manager.list()\n\t#     jobs = []\n\t#     for i in range(num_cores):\n\t#         prcs = Process(target=read_image, args=(path_list_every[i], return_list))\n", "#         jobs.append(prcs)\n\t#         prcs.start()\n\t#     for jb in jobs:\n\t#         jb.join()\n\t#     return return_list\n\tdef load_image_as_tensor(image_full_path):\n\t    loader = transforms.ToTensor()\n\t    image_tensor = loader(Image.open(image_full_path)) # [C, H, W], [0, 1]\n\t    image_tensor_batch = image_tensor.unsqueeze(0) # add batch dimension, [1, C, H, W]\n\t    return image_tensor_batch\n", "def unload(x):\n\t    y = x.squeeze().cpu().data.numpy()\n\t    return y\n\tdef convert2img(x):\n\t    return Image.fromarray(x*255).convert('L')\n\tdef save_smap(smap, path, negative_threshold=0.0):\n\t    # smap: [1, H, W]\n\t    if torch.max(smap) <= negative_threshold:\n\t        smap[smap<negative_threshold] = 0\n\t        smap = convert2img(unload(smap))\n", "    else:\n\t        smap = convert2img(min_max_normalize(unload(smap)))\n\t    smap.save(path)\n\tdef cache_model(model, path, multi_gpu):\n\t    if multi_gpu:\n\t        torch.save(model.module.state_dict(), path)\n\t    else:\n\t        torch.save(model.state_dict(), path)\n"]}
{"filename": "CoSOD_CoADNet/code/modules.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\tfrom CoSOD_CoADNet.code.ops import *\n\tfrom CoSOD_CoADNet.code.misc import *\n\tclass CAM(nn.Module): \n\t    # Channel Attention Module\n\t    def __init__(self, in_channels, squeeze_ratio):\n\t        super(CAM, self).__init__()\n\t        inter_channels = in_channels // squeeze_ratio\n\t        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\t        self.max_pool = nn.AdaptiveMaxPool2d(1)\n", "        fc_1 = FC(in_channels, inter_channels, False, 'relu')\n\t        fc_2 = FC(inter_channels, in_channels, False, 'none')\n\t        self.fc = nn.Sequential(fc_1, fc_2)\n\t    def forward(self, x):\n\t        # x: [B, in_channels, fh, fw]\n\t        # y: [B, in_channels, fh, fw]\n\t        avg_pooled = self.avg_pool(x).squeeze(-1).squeeze(-1) # [B, in_channels]\n\t        max_pooled = self.max_pool(x).squeeze(-1).squeeze(-1) # [B, in_channels]\n\t        ap_weights = self.fc(avg_pooled) # [B, in_channels]\n\t        mp_weights = self.fc(max_pooled) # [B, in_channels]\n", "        weights = F.sigmoid(ap_weights + mp_weights) # [B, in_channels]\n\t        y = x * weights.unsqueeze(-1).unsqueeze(-1) + x\n\t        return y\n\tclass SAM(nn.Module): \n\t    # Spatial Attention Module\n\t    def __init__(self, conv_ks):\n\t        super(SAM, self).__init__()\n\t        assert conv_ks>=3 and np.mod(conv_ks+1, 2)==0\n\t        self.conv = CU(ic=2, oc=1, ks=conv_ks, is_bn=False, na='sigmoid')\n\t    def forward(self, x):\n", "        # x: [B, ic, fh, fw]\n\t        # y: [B, ic, fh, fw]\n\t        avg_pooled = torch.mean(x, dim=1, keepdim=True) # [B, 1, fh, fw]\n\t        max_pooled = torch.max(x, dim=1, keepdim=True)[0] # [B, 1, fh, fw]\n\t        cat_pooled = torch.cat((avg_pooled, max_pooled), dim=1) # [B, 2, fh, fw]\n\t        weights = self.conv(cat_pooled) # [B, 1, fh, fw]\n\t        y = x * weights + x\n\t        return y\n\tclass LCA(nn.Module): \n\t    # Local Context Aggregation\n", "    def __init__(self, in_channels, squeeze_ratio, dr_list):\n\t        super(LCA, self).__init__()\n\t        inter_channels = in_channels // squeeze_ratio\n\t        self.conv_1 = CU(in_channels, inter_channels, 1, True, 'relu')\n\t        self.conv_2 = DilConv3(in_channels, inter_channels, True, 'relu', dr_list[0])\n\t        self.conv_3 = DilConv3(in_channels, inter_channels, True, 'relu', dr_list[1])\n\t        self.conv_4 = DilConv3(in_channels, inter_channels, True, 'relu', dr_list[2])\n\t        self.fusion = CU(inter_channels*4, in_channels, 1, True, 'relu')\n\t    def forward(self, x):\n\t        # x: [B, in_channels, fh, fw]\n", "        # y: [B, in_channels, fh, fw]\n\t        x_1 = self.conv_1(x) # [B, inter_channels, fh, fw]\n\t        x_2 = self.conv_2(x) # [B, inter_channels, fh, fw]\n\t        x_3 = self.conv_3(x) # [B, inter_channels, fh, fw]\n\t        x_4 = self.conv_4(x) # [B, inter_channels, fh, fw]\n\t        x_f = self.fusion(torch.cat((x_1, x_2, x_3, x_4), dim=1)) # [B, in_channels, fh, fw]\n\t        y = x_f + x\n\t        return y\n\tclass GCA(nn.Module):\n\t    # Global Context Aggregation\n", "    def __init__(self, in_channels, squeeze_ratio):\n\t        super(GCA, self).__init__()\n\t        self.map_q = CU(in_channels, in_channels//squeeze_ratio, 1, False, 'none')\n\t        self.map_k = CU(in_channels, in_channels//squeeze_ratio, 1, False, 'none')\n\t        self.map_v = CU(in_channels, in_channels, 1, False, 'none')\n\t        self.gamma = nn.Parameter(torch.zeros(1))\n\t    def forward(self, ftr):\n\t        # ftr: [B, C, H, W]\n\t        # ftr_fusion: # [B, C, H, W]\n\t        B, C, H, W = ftr.size()\n", "        N = H * W\n\t        ftr_q = self.map_q(ftr).view(B, -1, N).transpose(1, 2).contiguous() # [B, N, C']\n\t        ftr_k = self.map_k(ftr).view(B, -1, N) # [B, C', N]\n\t        aff_mat = F.softmax(torch.bmm(ftr_q, ftr_k), dim=1) # [B, N, N]\n\t        ftr_v = self.map_v(ftr).view(B, -1, N) # [B, C, N]\n\t        ftr_gca = torch.bmm(ftr_v, aff_mat).view(B, C, H, W) # [B, C, H, W]\n\t        ftr_fusion = ftr + ftr_gca * self.gamma # [B, C, H, W]\n\t        return ftr_fusion\n\tclass IaSH(nn.Module):\n\t    # Intra-Saliency Head\n", "    def __init__(self, bb_type):\n\t        super(IaSH, self).__init__()\n\t        assert bb_type in ['VGG16', 'ResNet50', 'Dilated_ResNet50']\n\t        self.bb_type = bb_type\n\t        upsample_2x = nn.Upsample(scale_factor=2, mode='bilinear')\n\t        if bb_type == 'VGG16':\n\t            self.conv_1 = nn.Sequential(upsample_2x, CU(512, 256, 3, True, 'relu'))\n\t            self.conv_2 = nn.Sequential(upsample_2x, CU(256, 128, 3, True, 'relu'))\n\t            self.output = nn.Sequential(CU(128, 128, 3, True, 'relu'), CU(128, 64, 3, True, 'relu'), CU(64, 1, 3, False, 'sigmoid'))\n\t        if bb_type == 'ResNet50':\n", "            self.conv_1 = nn.Sequential(CU(1024, 512, 1, True, 'relu'), upsample_2x, CU(512, 256, 3, True, 'relu'))\n\t            self.conv_2 = nn.Sequential(upsample_2x, CU(256, 128, 3, True, 'relu'))\n\t            self.output = nn.Sequential(CU(128, 128, 3, True, 'relu'), CU(128, 64, 3, True, 'relu'), CU(64, 1, 3, False, 'sigmoid'))\n\t        if bb_type == 'Dilated_ResNet50':          \n\t            self.conv_1 = nn.Sequential(CU(1024, 512, 1, True, 'relu'), upsample_2x, CU(512, 256, 3, True, 'relu'))\n\t            self.conv_2 = nn.Sequential(upsample_2x, CU(256, 128, 3, True, 'relu'))\n\t            self.output = nn.Sequential(CU(128, 128, 3, True, 'relu'), CU(128, 64, 3, True, 'relu'), CU(64, 1, 3, False, 'sigmoid'))\n\t    def forward(self, si_ftr):\n\t        # si_ftr: [Bs, D, 16, 16]\n\t        sm = self.output(self.conv_2(self.conv_1(si_ftr))) # [Bs, 1, 64, 64]\n", "        return sm\n\tclass OIaSG(nn.Module):\n\t    # Online-Intra Saliency Guidance\n\t    def __init__(self):\n\t        super(OIaSG, self).__init__()\n\t        self.fusion_1 = CU(ic=2, oc=1, ks=3, is_bn=False, na='sigmoid')\n\t        self.fusion_2 = CU(ic=2, oc=1, ks=3, is_bn=False, na='sigmoid')\n\t    def forward(self, gi_sm, gi_ftr_d):\n\t        # gi_sm: [Bg*M, 1, 16, 16]\n\t        # gi_ftr_d: [Bg*M, Cd, 16, 16]\n", "        ftr_avg = torch.mean(gi_ftr_d, dim=1, keepdim=True) # [Bg*M, 1, 16, 16]\n\t        ftr_max = torch.max(gi_ftr_d, dim=1, keepdim=True)[0] # [Bg*M, 1, 16, 16]\n\t        ftr_concat = torch.cat((ftr_avg, ftr_max), dim=1) # [Bg*M, 2, 16, 16]\n\t        ftr_fusion = self.fusion_1(ftr_concat) # [Bg*M, 1, 16, 16]\n\t        gm = self.fusion_2(torch.cat((ftr_fusion, gi_sm), dim=1)) # [Bg*M, 1, 16, 16]\n\t        intra_sal_ftr = gi_ftr_d + gi_ftr_d * gm # [Bg*M, Cd, 16, 16]\n\t        return intra_sal_ftr\n\tclass BlockFusion(nn.Module):\n\t    def __init__(self, in_channels, squeeze_ratio):\n\t        super(BlockFusion, self).__init__()\n", "        inter_channels = in_channels // squeeze_ratio\n\t        conv_1 = CU(in_channels, in_channels, 1, True, 'relu')\n\t        conv_2 = CU(in_channels, inter_channels, 3, True, 'relu')\n\t        conv_3 = CU(inter_channels, in_channels, 3, True, 'relu')\n\t        self.conv = nn.Sequential(conv_1, conv_2, conv_3)\n\t    def forward(self, ftr):\n\t        # ftr: [B, C, H, W]\n\t        # ftr_fusion: [B, C, H, W]\n\t        ftr_fusion = self.conv(ftr) # [B, C, H, W]\n\t        return ftr_fusion + ftr\n", "class GGD(nn.Module):\n\t    # Gated Group Distribution\n\t    def __init__(self, in_channels, squeeze_ratio):\n\t        super(GGD, self).__init__()\n\t        inter_channels = in_channels // squeeze_ratio\n\t        self.squeeze_channel = nn.Sequential(CU(in_channels*2, in_channels//2, 1, True, 'relu'), CAM(in_channels//2, 8))\n\t        conv_1 = CU(in_channels//2, inter_channels, 3, True, 'relu')\n\t        conv_2 = CU(inter_channels, in_channels, 3, False, 'sigmoid')\n\t        self.gie = nn.Sequential(conv_1, conv_2)\n\t    def forward(self, G, U):\n", "        # G: [Bg, D, H, W]\n\t        # U: [Bg, D, H, W] \n\t        P = self.gie(self.squeeze_channel(torch.cat((G, U), dim=1))) # [B, D, H, W]\n\t        ftr_cosal = (G - U) * P + U # [B, D, H, W], \"G*P+U*(1-P)\"\n\t        return ftr_cosal\n\tclass GCPD(nn.Module):\n\t    # Group Consistency Preserving Decoder\n\t    def __init__(self, in_channels, out_channels, squeeze_ratio):\n\t        super(GCPD, self).__init__()\n\t        tf_1 = CU(in_channels, in_channels//2, 1, True, 'relu')\n", "        tf_2 = nn.Upsample(scale_factor=2, mode='bilinear')\n\t        tf_3 = CU(in_channels//2, in_channels//2, 3, True, 'relu')\n\t        self.transform = nn.Sequential(tf_1, tf_2, tf_3)\n\t        inter_channels = in_channels // squeeze_ratio\n\t        smlp_1 = FC(in_channels, inter_channels, False, 'relu')\n\t        smlp_2 = FC(inter_channels, in_channels//2, False, 'sigmoid')\n\t        self.smlp = nn.Sequential(smlp_1, smlp_2)\n\t        self.conv = CU(in_channels//2, out_channels, 3, True, 'relu')\n\t    def forward(self, X):\n\t        # X: [B, M, ic, fh, fw]\n", "        # X_d: [B, M, oc, fh*2, fw*2]\n\t        B, M, ic, fh, fw = X.size()\n\t        X_t = self.transform(X.view(-1, ic, fh, fw)) # [B*M, ic//2, fh*2, fw*2]\n\t        V = GAP(X_t).squeeze(-1).squeeze(-1).view(B, M, ic//2) # [B, M, ic//2]\n\t        y = torch.sum(V * F.softmax(V, dim=1), dim=1) # [B, ic//2]\n\t        V_cat_y = torch.cat((V, y.unsqueeze(1).repeat(1, M, 1)), dim=-1) # [B, M, ic]\n\t        A = self.smlp(V_cat_y.view(-1, ic)) # [B, M, ic//2]\n\t        X_t = X_t * A.unsqueeze(-1).unsqueeze(-1) + X_t # [B*M, ic//2, fh*2, fw*2]\n\t        X_d = self.conv(X_t).view(B, M, -1, fh*2, fw*2) # [B, M, oc, fh*2, fw*2]\n\t        return X_d\n", "class CoSH(nn.Module):\n\t    # Co-Saliency Head\n\t    def __init__(self, in_channels):\n\t        super(CoSH, self).__init__()\n\t        dims = [in_channels, in_channels, 64, 1]\n\t        head_1 = CU(dims[0], dims[1], 3, True, 'relu')\n\t        head_2 = CU(dims[1], dims[2], 3, True, 'relu')\n\t        head_3 = CU(dims[2], dims[3], 3, False, 'sigmoid')\n\t        self.head = nn.Sequential(head_1, head_2, head_3)\n\t    def forward(self, x):\n", "        # x: [B, M, in_channels, H, W]\n\t        # y: [B, M, 1, H, W]\n\t        B, M, C, H, W = x.size()\n\t        y = self.head(x.view(-1, C, H, W)).view(B, M, 1, H, W)\n\t        return y\n"]}
{"filename": "CoSOD_CoADNet/code/common_packages.py", "chunked_list": ["import os\n\timport sys\n\timport math\n\timport time\n\timport numpy as np\n\tfrom PIL import Image\n\tfrom numpy import random\n\tfrom multiprocessing import Process, Queue, Pool, Pipe, Manager\n\timport torch\n\timport torch.nn as nn\n", "from torch import optim\n\tfrom torch.utils import data\n\tfrom torch.nn import Parameter\n\timport torch.nn.functional as F\n\tfrom torchvision import models, transforms\n\timport warnings\n\twarnings.filterwarnings('ignore')\n"]}
{"filename": "GCAGC_CVPR2020/model3/model2_graph4_hrnet_agcm.py", "chunked_list": ["from torch.optim import Adam\n\timport math\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n\t    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n\t#from resnext import ResNeXt101\n\tfrom torch.autograd import Variable\n\tfrom .default import _C as config\n", "from .cls_hrnet import get_cls_net\n\tmodel5 = get_cls_net(config)\n\tmodel5.cuda()\n\t###############\n\tclass Model(nn.Module):\n\t    def __init__(self):\n\t        super(Model, self).__init__()\n\t        self.prnet = model5\n\t        self.gc4_1 = GraphConvolution(384,  192)\n\t        self.gc4_2 = GraphConvolution(192,  192)\n", "        self.gc3_1 = GraphConvolution(192, 96)\n\t        self.gc3_2 = GraphConvolution(96,  96)\n\t        self.gc2_1 = GraphConvolution(96,  48)\n\t        self.gc2_2 = GraphConvolution(48,  48)\n\t        self.group_size=5\n\t        # decoder\n\t        de_in_channels=int(48+96+192)\n\t        de_layers = make_decoder_layers(decoder_archs['d16'], de_in_channels, batch_norm=True)\n\t        self.decoder = DOCSDecoderNet(de_layers)\n\t#        self.initialize_weights()\n", "    def forward(self, img):\n\t        with torch.no_grad():\n\t           fea = self.prnet(img)           #### layer4=(B*N, C, H, W)  \n\t        out4 = unsqz_fea(fea[3])        #### fea5=(B,N,392,H,W)\n\t        out3 = unsqz_fea(fea[2])        #### fea5=(B,N,192,H,W)\n\t        out2 = unsqz_fea(fea[1])\n\t        for a in range(out4.shape[0]):\n\t            ######################## layer4\n\t            feat4 = out4[a].permute(0, 2, 3, 1).reshape(-1, out4.shape[2]) # (N*H*W,C)           \n\t            adj4 = torch.mm(feat4, torch.t(feat4)) # (N*H*W, N*H*W)\n", "            adj4 = row_normalize(adj4)\n\t            gc4 = F.relu(self.gc4_1(feat4, adj4))\n\t            gc4 = F.relu(self.gc4_2(gc4, adj4))    # (N*H*W,192)\n\t            gc4 = gc4.reshape(out4[a].shape[0], out4[a].shape[2], out4[a].shape[3], 192)\n\t            gc44 = gc4.permute(0, 3, 1, 2)       # (N,192,H,W)\n\t            gc4 = F.upsample(gc44, scale_factor=4, mode='bilinear')\n\t            ######################### layer3\n\t            feat3=out3[a].permute(0,2,3,1).reshape(-1, out3.shape[2])\n\t            adj3=torch.mm(feat3, torch.t(feat3))\n\t            adj3=row_normalize(adj3)\n", "            gc3=F.relu(self.gc3_1(feat3, adj3))\n\t            gc3=F.relu(self.gc3_2(gc3, adj3))   # (5*14*14,96)\n\t            gc3 = gc3.reshape(out3[a].shape[0], out3[a].shape[2], out3[a].shape[3], 96)\n\t            gc3 = gc3.permute(0, 3, 1, 2)       # (N,96,H,W) \n\t            gc3 = F.upsample(gc3, scale_factor=2, mode='bilinear')\n\t            ######################### layer2\n\t            feat2=out2[a].permute(0,2,3,1).reshape(-1, out2.shape[2])\n\t            adj2=torch.mm(feat2, torch.t(feat2))            \n\t            adj2=row_normalize(adj2)\n\t            gc2=F.relu(self.gc2_1(feat2, adj2))\n", "            gc2=F.relu(self.gc2_2(gc2, adj2))\n\t            gc2 = gc2.reshape(out2[a].shape[0], out2[a].shape[2], out2[a].shape[3], 48)\n\t            gc2 = gc2.permute(0, 3, 1, 2)\n\t            #########################\n\t            gc_fuse = torch.cat((gc4, gc3, gc2), dim=1)\n\t            if a == 0:\n\t                gcx_out = gc_fuse\n\t                gc4_out = gc44\n\t            else:\n\t                gcx_out = torch.cat((gcx_out, gc_fuse), dim=0)\n", "                gc4_out = torch.cat((gc4_out, gc44), dim=0)\n\t        spa_masks = spatial_optimize(gc4_out, self.group_size).cuda()\n\t        spa_masks = F.upsample(spa_masks, scale_factor=4, mode='bilinear')\n\t        spa_masks = spa_masks.expand_as(gcx_out)\n\t        ######################\n\t        out_final=self.decoder(spa_masks + gcx_out)\n\t        out_final=F.upsample(out_final, size=img.size()[2:], mode='bilinear')\n\t        out_final=out_final.sigmoid().squeeze()\n\t        return out_final,fea[3], fea[2], fea[1]\n\t#    def initialize_weights(self):\n", "#        pretrained_dict=torch.load('/home/litengpeng/CODE/semantic-segmentation/CPD-HR2/hrnetv2_w48_imagenet_pretrained.pth')\n\t#        net_dict=self.prnet.state_dict()\n\t#        for key,value in pretrained_dict.items():\n\t#          if key in net_dict.keys():\n\t#             net_dict[key]=value\n\t#        net_dict.update(net_dict)\n\t#        self.prnet.load_state_dict(net_dict)    \n\t############## unsupervised masks\n\t############## unsupervised masks\n\tdef norm(x,dim):\n", "    squared_norm=(x**2).sum(dim=dim, keepdim=True)\n\t    normed=x/torch.sqrt(squared_norm)\n\t    return normed\n\tdef spatial_optimize(fmap, group_size):\n\t    N, H, W = 2, 8, 8\n\t    fmap_split = torch.split(fmap, group_size, dim=0)\n\t    for i in range(len(fmap_split)):\n\t        cur_fmap = fmap_split[i]\n\t        with torch.no_grad():\n\t            spatial_x = cur_fmap.permute(0, 2, 3, 1).contiguous().view(-1, cur_fmap.size(1)).transpose(1, 0)\n", "            spatial_x = norm(spatial_x, dim=0)\n\t            spatial_x_t = spatial_x.transpose(1, 0)\n\t            G = torch.mm(spatial_x_t , spatial_x) - 1\n\t            G = G.detach().cpu()\n\t        with torch.enable_grad():\n\t            # N*H*W\n\t            # M = 5 * 7 * 7\n\t            M = N * H * W\n\t            spatial_s = nn.Parameter(torch.sqrt(M * torch.ones((M, 1))) / M, requires_grad=True)\n\t            spatial_s_t = spatial_s.transpose(1, 0)\n", "            spatial_s_optimizer = Adam([spatial_s], 0.01)\n\t            for iter in range(200):\n\t                f_spa_loss = -1 * torch.sum(torch.mm(torch.mm(spatial_s_t, G), spatial_s))\n\t                spatial_s_d = torch.sqrt(torch.sum(spatial_s ** 2))\n\t                if spatial_s_d >= 1:\n\t                    d_loss = -1 * torch.log(2 - spatial_s_d)\n\t                else:\n\t                    d_loss = -1 * torch.log(spatial_s_d)\n\t                all_loss = 50 * d_loss + f_spa_loss\n\t#                if iter%20==0:\n", "#                   print('iter: [%.4f], loss: [%.4f], dloss:[%.4f], floss: [%.4f]' %(iter, all_loss, d_loss, f_spa_loss))\n\t                spatial_s_optimizer.zero_grad()\n\t                all_loss.backward()\n\t                spatial_s_optimizer.step()\n\t        result_map = spatial_s.data.view(N, 1, H, W)\n\t        if i == 0:\n\t            spa_mask = result_map\n\t        else:\n\t            spa_mask = torch.cat(([spa_mask, result_map]), dim=0)\n\t    return spa_mask\n", "##################### unsupervised masks\n\t##################### unsupervised masks\n\tdef row_normalize(mx):\n\t    \"\"\"Row-normalize sparse matrix\"\"\"\n\t    rowsum = torch.sum(mx, dim=1)\n\t    r_inv = 1 / (rowsum + 1e-10)\n\t    r_mat_inv = torch.diag(r_inv)\n\t    mx = torch.mm(r_mat_inv, mx)\n\t    return mx\n\tdef unsqz_fea(dim4_data):\n", "    split_data = torch.split(dim4_data, 5, dim=0)\n\t    for i in range(len(split_data)):\n\t        if i == 0:\n\t            dim5_data = split_data[i].unsqueeze(dim=0)\n\t        else:\n\t            dim5_data = torch.cat((dim5_data, split_data[i].unsqueeze(dim=0)), dim=0)\n\t    return dim5_data\n\tdef sqz_fea(dim5_data):\n\t    if dim5_data.size(1) == 1:\n\t        return dim5_data.squeeze()\n", "    else:\n\t        b = dim5_data.size(0)\n\t        for i in range(b):\n\t            if i == 0:\n\t                new_dim4_data = dim5_data[i, :, :, :, :]\n\t            else:\n\t                new_dim4_data = torch.cat((new_dim4_data, dim5_data[i, :, :, :, :]), dim=0)\n\t    return new_dim4_data    \n\t################################\n\tclass GraphConvolution(Module):\n", "    \"\"\"\n\t    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n\t    \"\"\"\n\t    def __init__(self, in_features, out_features, bias=True):\n\t        super(GraphConvolution, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n\t        if bias:\n\t            self.bias = Parameter(torch.FloatTensor(out_features))\n", "        else:\n\t            self.register_parameter('bias', None)\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n\t        stdv = 1. / math.sqrt(self.weight.size(1))\n\t        self.weight.data.uniform_(-stdv, stdv)\n\t        if self.bias is not None:\n\t            self.bias.data.uniform_(-stdv, stdv)\n\t    def forward(self, input, adj):\n\t        support = torch.mm(input, self.weight)\n", "        output = torch.mm(adj, support)\n\t        if self.bias is not None:\n\t            return output + self.bias\n\t        else:\n\t            return output\n\t#################\n\tdecoder_archs = {\n\t    'd16': [336, 'd128', 128, 128, 'd64', 64, 64, 'c1']\n\t}\n\tdef make_decoder_layers(cfg, in_channels, batch_norm=True):\n", "    layers = []\n\t    for v in cfg:\n\t        if type(v) is str:\n\t            if v[0] == 'd':\n\t                v = int(v[1:])\n\t                convtrans2d = nn.ConvTranspose2d(in_channels, v, kernel_size=4, stride=2, padding=1)\n\t                if batch_norm:\n\t                    layers += [convtrans2d, nn.BatchNorm2d(v), nn.ReLU()]\n\t                else:\n\t                    layers += [convtrans2d, nn.ReLU()]\n", "                in_channels = v\n\t            elif v[0] == 'c':\n\t                v = int(v[1:])\n\t                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1)]\n\t        else:\n\t            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n\t            if batch_norm:\n\t                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n\t            else:\n\t                layers += [conv2d, nn.ReLU()]\n", "            in_channels = v\n\t    return nn.Sequential(*layers)\n\tclass DOCSDecoderNet(nn.Module):\n\t    def __init__(self, features):\n\t        super(DOCSDecoderNet, self).__init__()\n\t        self.features = features\n\t    def forward(self, x):\n\t        return self.features(x)"]}
{"filename": "GCAGC_CVPR2020/model3/model2_graph4_hrnet.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n\t    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n\t#from resnext import ResNeXt101\n\tfrom torch.autograd import Variable\n\tfrom default import _C as config\n\tfrom cls_hrnet import get_cls_net\n", "model5 = get_cls_net(config)\n\tmodel5.cuda()\n\t###############\n\tclass Model(nn.Module):\n\t    def __init__(self):\n\t        super(Model, self).__init__()\n\t        self.prnet = model5\n\t        self.gc4_1 = GraphConvolution(384,  192)\n\t        self.gc4_2 = GraphConvolution(192,  192)\n\t        self.gc3_1 = GraphConvolution(192, 96)\n", "        self.gc3_2 = GraphConvolution(96,  96)\n\t        self.gc2_1 = GraphConvolution(96,  48)\n\t        self.gc2_2 = GraphConvolution(48,  48)\n\t        # \n\t        #self.pro3=nn.Conv2d(192, 192, 1)\n\t        # decoder\n\t        de_in_channels=int(48+96+192)\n\t        de_layers = make_decoder_layers(decoder_archs['d16'], de_in_channels, batch_norm=True)\n\t        self.decoder = DOCSDecoderNet(de_layers)\n\t#        self.decoder=nn.Sequential(nn.Conv2d(64, 64, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.ConvTranspose2d(64, 1, 64, 32, 16))\n", "#        self.initialize_weights()\n\t    def forward(self, img):\n\t        with torch.no_grad():\n\t            fea = self.prnet(img)       #### layer4=(B*N, C, H, W)  \n\t        out4 = unsqz_fea(fea[3])        #### fea5=(B,N,392,H,W)\n\t        out3 = unsqz_fea(fea[2])        #### fea5=(B,N,192,H,W)\n\t        out2 = unsqz_fea(fea[1])\n\t        for a in range(out4.shape[0]):\n\t            ######################## layer4\n\t            feat4 = out4[a].permute(0, 2, 3, 1).reshape(-1, out4.shape[2]) # (N*H*W,C)           \n", "            adj4 = torch.mm(feat4, torch.t(feat4)) # (N*H*W, N*H*W)\n\t            adj4 = row_normalize(adj4)\n\t            gc4 = F.relu(self.gc4_1(feat4, adj4))\n\t            gc4 = F.relu(self.gc4_2(gc4, adj4))    # (N*H*W,192)\n\t            gc4 = gc4.reshape(out4[a].shape[0], out4[a].shape[2], out4[a].shape[3], 192)\n\t            gc4 = gc4.permute(0, 3, 1, 2)       # (N,192,H,W)\n\t            gc4 = F.upsample(gc4, scale_factor=4, mode='bilinear')\n\t            ######################### layer3\n\t            feat3=out3[a].permute(0,2,3,1).reshape(-1, out3.shape[2])\n\t            adj3=torch.mm(feat3, torch.t(feat3))\n", "            adj3=row_normalize(adj3)\n\t            gc3=F.relu(self.gc3_1(feat3, adj3))\n\t            gc3=F.relu(self.gc3_2(gc3, adj3))   # (5*14*14,96)\n\t            gc3 = gc3.reshape(out3[a].shape[0], out3[a].shape[2], out3[a].shape[3], 96)\n\t            gc3 = gc3.permute(0, 3, 1, 2)       # (N,96,H,W) \n\t            gc3 = F.upsample(gc3, scale_factor=2, mode='bilinear')\n\t            ######################### layer2\n\t            feat2=out2[a].permute(0,2,3,1).reshape(-1, out2.shape[2])\n\t            adj2=torch.mm(feat2, torch.t(feat2))            \n\t            adj2=row_normalize(adj2)\n", "            gc2=F.relu(self.gc2_1(feat2, adj2))\n\t            gc2=F.relu(self.gc2_2(gc2, adj2))\n\t            gc2 = gc2.reshape(out2[a].shape[0], out2[a].shape[2], out2[a].shape[3], 48)\n\t            gc2 = gc2.permute(0, 3, 1, 2)\n\t            #########################\n\t            gc_fuse = torch.cat((gc4, gc3, gc2), dim=1)\n\t            if a == 0:\n\t                gcx_out = gc_fuse\n\t            else:\n\t                gcx_out = torch.cat((gcx_out, gc_fuse), dim=0)\n", "        ######################\n\t        out_final=self.decoder(gcx_out)\n\t        out_final=F.upsample(out_final, size=img.size()[2:], mode='bilinear')\n\t        out_final=out_final.sigmoid().squeeze()\n\t        return out_final, fea[3], fea[2], fea[1]\n\t#    def initialize_weights(self):\n\t#        pretrained_dict=torch.load('/home/litengpeng/CODE/semantic-segmentation/CPD-HR2/hrnetv2_w48_imagenet_pretrained.pth')\n\t#        net_dict=self.prnet.state_dict()\n\t#        for key,value in pretrained_dict.items():\n\t#          if key in net_dict.keys():\n", "#             net_dict[key]=value\n\t#        net_dict.update(net_dict)\n\t#        self.prnet.load_state_dict(net_dict)    \n\tdef row_normalize(mx):\n\t    \"\"\"Row-normalize sparse matrix\"\"\"\n\t    rowsum = torch.sum(mx, dim=1)\n\t    r_inv = 1 / (rowsum + 1e-10)\n\t    r_mat_inv = torch.diag(r_inv)\n\t    mx = torch.mm(r_mat_inv, mx)\n\t    return mx\n", "def unsqz_fea(dim4_data):\n\t    split_data = torch.split(dim4_data, 5, dim=0)\n\t    for i in range(len(split_data)):\n\t        if i == 0:\n\t            dim5_data = split_data[i].unsqueeze(dim=0)\n\t        else:\n\t            dim5_data = torch.cat((dim5_data, split_data[i].unsqueeze(dim=0)), dim=0)\n\t    return dim5_data\n\tdef sqz_fea(dim5_data):\n\t    if dim5_data.size(1) == 1:\n", "        return dim5_data.squeeze()\n\t    else:\n\t        b = dim5_data.size(0)\n\t        for i in range(b):\n\t            if i == 0:\n\t                new_dim4_data = dim5_data[i, :, :, :, :]\n\t            else:\n\t                new_dim4_data = torch.cat((new_dim4_data, dim5_data[i, :, :, :, :]), dim=0)\n\t    return new_dim4_data    \n\t################################\n", "class GraphConvolution(Module):\n\t    \"\"\"\n\t    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n\t    \"\"\"\n\t    def __init__(self, in_features, out_features, bias=True):\n\t        super(GraphConvolution, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n\t        if bias:\n", "            self.bias = Parameter(torch.FloatTensor(out_features))\n\t        else:\n\t            self.register_parameter('bias', None)\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n\t        stdv = 1. / math.sqrt(self.weight.size(1))\n\t        self.weight.data.uniform_(-stdv, stdv)\n\t        if self.bias is not None:\n\t            self.bias.data.uniform_(-stdv, stdv)\n\t    def forward(self, input, adj):\n", "        support = torch.mm(input, self.weight)\n\t        output = torch.mm(adj, support)\n\t        if self.bias is not None:\n\t            return output + self.bias\n\t        else:\n\t            return output\n\t#################\n\tdecoder_archs = {\n\t    'd16': [336, 'd128', 128, 128, 'd64', 64, 64, 'c1']\n\t}\n", "def make_decoder_layers(cfg, in_channels, batch_norm=True):\n\t    layers = []\n\t    for v in cfg:\n\t        if type(v) is str:\n\t            if v[0] == 'd':\n\t                v = int(v[1:])\n\t                convtrans2d = nn.ConvTranspose2d(in_channels, v, kernel_size=4, stride=2, padding=1)\n\t                if batch_norm:\n\t                    layers += [convtrans2d, nn.BatchNorm2d(v), nn.ReLU()]\n\t                else:\n", "                    layers += [convtrans2d, nn.ReLU()]\n\t                in_channels = v\n\t            elif v[0] == 'c':\n\t                v = int(v[1:])\n\t                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1)]\n\t        else:\n\t            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n\t            if batch_norm:\n\t                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n\t            else:\n", "                layers += [conv2d, nn.ReLU()]\n\t            in_channels = v\n\t    return nn.Sequential(*layers)\n\tclass DOCSDecoderNet(nn.Module):\n\t    def __init__(self, features):\n\t        super(DOCSDecoderNet, self).__init__()\n\t        self.features = features\n\t    def forward(self, x):\n\t        return self.features(x)\n\t#####################\n", "# def pca_group(fmap, group_size):\n\t#     fmap_split = torch.split(fmap, group_size, dim=0)\n\t#     for i in range(len(fmap_split)):\n\t#         cur_fmap = fmap_split[i]\n\t#         p4pca = cur_fmap.permute(0, 2, 3, 1).contiguous().view(-1, cur_fmap.size()[1])\n\t#         p4pca_mean = torch.mean(p4pca, dim=0)\n\t#         p4pca = p4pca - p4pca_mean.expand_as(p4pca)\n\t#         U, S, V = torch.svd(torch.t(p4pca))\n\t#         C = torch.mm(p4pca, U[:, 0].view(-1, 1))\n\t#         a4pca = C.view(cur_fmap.size()[0], cur_fmap.size()[2], cur_fmap.size()[3], 1)\n", "#         cur_mask = a4pca.permute(0, 3, 1, 2)\n\t#         # ddd = cur_mask.detach().cpu().numpy()\n\t#         if i == 0:\n\t#             pca_mask = cur_mask\n\t#         else:\n\t#             pca_mask = torch.cat(([pca_mask, cur_mask]), dim=0)\n\t#     # ddd = pca_mask.detach().cpu().numpy()\n\t#     return pca_mask"]}
{"filename": "GCAGC_CVPR2020/model3/model2_graph4_hrnet_sal.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n\t    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n\t#from resnext import ResNeXt101\n\tfrom torch.autograd import Variable\n\tfrom .model2_graph4_hrnet_agcm import Model\n\tmodel6 = Model()\n", "model6.cuda()\n\tclass Model2(nn.Module):\n\t    def __init__(self):\n\t        super(Model2, self).__init__()\n\t        self.cosalnet = model6\n\t        self.jpu = JPU([96, 96, 96, 192, 384], width=96, norm_layer=nn.BatchNorm2d)\n\t        de_in_channels=int(96*4)\n\t        de_layers = make_decoder_layers(decoder_archs['d16'], de_in_channels, batch_norm=True)\n\t        self.decodersal = DOCSDecoderNet(de_layers)\n\t#        self.initialize_weights()\n", "    def forward(self, img):  \n\t        cosalmap, fea3, fea2, fea1 = self.cosalnet(img)\n\t        with torch.no_grad():\n\t          feat= self.jpu(fea1, fea2, fea3)\n\t          pred=self.decodersal(feat)\n\t          pred=F.upsample(pred, size=img.size()[2:], mode='bilinear')\n\t        #cosalmap=F.upsample(cosalmap, img.size()[2:], mode='bilinear')\n\t        return pred, cosalmap\n\t#    def initialize_weights(self):\n\t#        pretrained_dict=torch.load('/home/litengpeng/CODE/cosal/aaai19/mine1-cosal/model_path/graph4_decoder_hrnet/hrnet_iter67500.pth')\n", "#        net_dict=self.cosalnet.state_dict()\n\t#        for key,value in pretrained_dict.items():\n\t#          if key in net_dict.keys():\n\t#             net_dict[key]=value\n\t#        net_dict.update(net_dict)\n\t#        self.cosalnet.load_state_dict(net_dict)    \n\tclass JPU(nn.Module):\n\t    def __init__(self, in_channels, width=96, norm_layer=nn.BatchNorm2d):\n\t        super(JPU, self).__init__()\n\t        self.conv5 = nn.Sequential(\n", "            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n\t            norm_layer(width),\n\t            nn.ReLU(inplace=True))\n\t        self.conv4 = nn.Sequential(\n\t            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n\t            norm_layer(width),\n\t            nn.ReLU(inplace=True))\n\t        self.conv3 = nn.Sequential(\n\t            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n\t            norm_layer(width),\n", "            nn.ReLU(inplace=True))\n\t        self.dilation1 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n\t                                       norm_layer(width),\n\t                                       nn.ReLU(inplace=True))\n\t        self.dilation2 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n\t                                       norm_layer(width),\n\t                                       nn.ReLU(inplace=True))\n\t        self.dilation3 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=4, dilation=4, bias=False),\n\t                                       norm_layer(width),\n\t                                       nn.ReLU(inplace=True))\n", "        self.dilation4 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=8, dilation=8, bias=False),\n\t                                       norm_layer(width),\n\t                                       nn.ReLU(inplace=True))\n\t    def forward(self, *inputs):\n\t        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n\t        _, _, h, w = feats[-1].size()\n\t        feats[-2] = F.upsample(feats[-2], (h, w), mode='bilinear')\n\t        feats[-3] = F.upsample(feats[-3], (h, w), mode='bilinear')\n\t        feat = torch.cat(feats, dim=1)   #### channel 128*3=284\n\t        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)], dim=1)\n", "        return feat  ##### channel 128*4=512\n\tclass SeparableConv2d(nn.Module):\n\t    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=False, BatchNorm=nn.BatchNorm2d):\n\t        super(SeparableConv2d, self).__init__()\n\t        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation, groups=inplanes, bias=bias)\n\t        self.bn = BatchNorm(inplanes)\n\t        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\t    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = self.bn(x)\n", "        x = self.pointwise(x)\n\t        return x\n\t#################\n\tdecoder_archs = {\n\t    'd16': [96*4, 'd256', 256, 256, 'd128', 128, 128, 'd64', 64, 64, 'c1']\n\t}\n\tdef make_decoder_layers(cfg, in_channels, batch_norm=True):\n\t    layers = []\n\t    for v in cfg:\n\t        if type(v) is str:\n", "            if v[0] == 'd':\n\t                v = int(v[1:])\n\t                convtrans2d = nn.ConvTranspose2d(in_channels, v, kernel_size=4, stride=2, padding=1)\n\t                if batch_norm:\n\t                    layers += [convtrans2d, nn.BatchNorm2d(v), nn.ReLU()]\n\t                else:\n\t                    layers += [convtrans2d, nn.ReLU()]\n\t                in_channels = v\n\t            elif v[0] == 'c':\n\t                v = int(v[1:])\n", "                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1)]\n\t        else:\n\t            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n\t            if batch_norm:\n\t                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n\t            else:\n\t                layers += [conv2d, nn.ReLU()]\n\t            in_channels = v\n\t    return nn.Sequential(*layers)\n\tclass DOCSDecoderNet(nn.Module):\n", "    def __init__(self, features):\n\t        super(DOCSDecoderNet, self).__init__()\n\t        self.features = features\n\t    def forward(self, x):\n\t        return self.features(x)"]}
{"filename": "GCAGC_CVPR2020/model3/cls_hrnet.py", "chunked_list": ["# ------------------------------------------------------------------------------\n\t# Copyright (c) Microsoft\n\t# Licensed under the MIT License.\n\t# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n\t# Modified by Ke Sun (sunk@mail.ustc.edu.cn)\n\t# ------------------------------------------------------------------------------\n\tfrom __future__ import absolute_import\n\tfrom __future__ import division\n\tfrom __future__ import print_function\n\timport os\n", "import logging\n\timport functools\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch._utils\n\timport torch.nn.functional as F\n\tBN_MOMENTUM = 0.1\n\tlogger = logging.getLogger(__name__)\n\tdef conv3x3(in_planes, out_planes, stride=1):\n", "    \"\"\"3x3 convolution with padding\"\"\"\n\t    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n\t                     padding=1, bias=False)\n\tclass BasicBlock(nn.Module):\n\t    expansion = 1\n\t    def __init__(self, inplanes, planes, stride=1, downsample=None):\n\t        super(BasicBlock, self).__init__()\n\t        self.conv1 = conv3x3(inplanes, planes, stride)\n\t        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n\t        self.relu = nn.ReLU(inplace=True)\n", "        self.conv2 = conv3x3(planes, planes)\n\t        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n\t        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x):\n\t        residual = x\n\t        out = self.conv1(x)\n\t        out = self.bn1(out)\n\t        out = self.relu(out)\n\t        out = self.conv2(out)\n", "        out = self.bn2(out)\n\t        if self.downsample is not None:\n\t            residual = self.downsample(x)\n\t        out += residual\n\t        out = self.relu(out)\n\t        return out\n\tclass Bottleneck(nn.Module):\n\t    expansion = 4\n\t    def __init__(self, inplanes, planes, stride=1, downsample=None):\n\t        super(Bottleneck, self).__init__()\n", "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n\t        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n\t        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n\t                               padding=1, bias=False)\n\t        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n\t        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n\t                               bias=False)\n\t        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n\t                               momentum=BN_MOMENTUM)\n\t        self.relu = nn.ReLU(inplace=True)\n", "        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x):\n\t        residual = x\n\t        out = self.conv1(x)\n\t        out = self.bn1(out)\n\t        out = self.relu(out)\n\t        out = self.conv2(out)\n\t        out = self.bn2(out)\n\t        out = self.relu(out)\n", "        out = self.conv3(out)\n\t        out = self.bn3(out)\n\t        if self.downsample is not None:\n\t            residual = self.downsample(x)\n\t        out += residual\n\t        out = self.relu(out)\n\t        return out\n\tclass HighResolutionModule(nn.Module):\n\t    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n\t                 num_channels, fuse_method, multi_scale_output=True):\n", "        super(HighResolutionModule, self).__init__()\n\t        self._check_branches(\n\t            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\t        self.num_inchannels = num_inchannels\n\t        self.fuse_method = fuse_method\n\t        self.num_branches = num_branches\n\t        self.multi_scale_output = multi_scale_output\n\t        self.branches = self._make_branches(\n\t            num_branches, blocks, num_blocks, num_channels)\n\t        self.fuse_layers = self._make_fuse_layers()\n", "        self.relu = nn.ReLU(False)\n\t    def _check_branches(self, num_branches, blocks, num_blocks,\n\t                        num_inchannels, num_channels):\n\t        if num_branches != len(num_blocks):\n\t            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n\t                num_branches, len(num_blocks))\n\t            logger.error(error_msg)\n\t            raise ValueError(error_msg)\n\t        if num_branches != len(num_channels):\n\t            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n", "                num_branches, len(num_channels))\n\t            logger.error(error_msg)\n\t            raise ValueError(error_msg)\n\t        if num_branches != len(num_inchannels):\n\t            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n\t                num_branches, len(num_inchannels))\n\t            logger.error(error_msg)\n\t            raise ValueError(error_msg)\n\t    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n\t                         stride=1):\n", "        downsample = None\n\t        if stride != 1 or \\\n\t           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n\t            downsample = nn.Sequential(\n\t                nn.Conv2d(self.num_inchannels[branch_index],\n\t                          num_channels[branch_index] * block.expansion,\n\t                          kernel_size=1, stride=stride, bias=False),\n\t                nn.BatchNorm2d(num_channels[branch_index] * block.expansion,\n\t                            momentum=BN_MOMENTUM),\n\t            )\n", "        layers = []\n\t        layers.append(block(self.num_inchannels[branch_index],\n\t                            num_channels[branch_index], stride, downsample))\n\t        self.num_inchannels[branch_index] = \\\n\t            num_channels[branch_index] * block.expansion\n\t        for i in range(1, num_blocks[branch_index]):\n\t            layers.append(block(self.num_inchannels[branch_index],\n\t                                num_channels[branch_index]))\n\t        return nn.Sequential(*layers)\n\t    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n", "        branches = []\n\t        for i in range(num_branches):\n\t            branches.append(\n\t                self._make_one_branch(i, block, num_blocks, num_channels))\n\t        return nn.ModuleList(branches)\n\t    def _make_fuse_layers(self):\n\t        if self.num_branches == 1:\n\t            return None\n\t        num_branches = self.num_branches\n\t        num_inchannels = self.num_inchannels\n", "        fuse_layers = []\n\t        for i in range(num_branches if self.multi_scale_output else 1):\n\t            fuse_layer = []\n\t            for j in range(num_branches):\n\t                if j > i:\n\t                    fuse_layer.append(nn.Sequential(\n\t                        nn.Conv2d(num_inchannels[j],\n\t                                  num_inchannels[i],\n\t                                  1,\n\t                                  1,\n", "                                  0,\n\t                                  bias=False),\n\t                        nn.BatchNorm2d(num_inchannels[i], \n\t                                       momentum=BN_MOMENTUM),\n\t                        nn.Upsample(scale_factor=2**(j-i), mode='nearest')))\n\t                elif j == i:\n\t                    fuse_layer.append(None)\n\t                else:\n\t                    conv3x3s = []\n\t                    for k in range(i-j):\n", "                        if k == i - j - 1:\n\t                            num_outchannels_conv3x3 = num_inchannels[i]\n\t                            conv3x3s.append(nn.Sequential(\n\t                                nn.Conv2d(num_inchannels[j],\n\t                                          num_outchannels_conv3x3,\n\t                                          3, 2, 1, bias=False),\n\t                                nn.BatchNorm2d(num_outchannels_conv3x3, \n\t                                            momentum=BN_MOMENTUM)))\n\t                        else:\n\t                            num_outchannels_conv3x3 = num_inchannels[j]\n", "                            conv3x3s.append(nn.Sequential(\n\t                                nn.Conv2d(num_inchannels[j],\n\t                                          num_outchannels_conv3x3,\n\t                                          3, 2, 1, bias=False),\n\t                                nn.BatchNorm2d(num_outchannels_conv3x3,\n\t                                            momentum=BN_MOMENTUM),\n\t                                nn.ReLU(False)))\n\t                    fuse_layer.append(nn.Sequential(*conv3x3s))\n\t            fuse_layers.append(nn.ModuleList(fuse_layer))\n\t        return nn.ModuleList(fuse_layers)\n", "    def get_num_inchannels(self):\n\t        return self.num_inchannels\n\t    def forward(self, x):\n\t        if self.num_branches == 1:\n\t            return [self.branches[0](x[0])]\n\t        for i in range(self.num_branches):\n\t            x[i] = self.branches[i](x[i])\n\t        x_fuse = []\n\t        for i in range(len(self.fuse_layers)):\n\t            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n", "            for j in range(1, self.num_branches):\n\t                if i == j:\n\t                    y = y + x[j]\n\t                else:\n\t                    y = y + self.fuse_layers[i][j](x[j])\n\t            x_fuse.append(self.relu(y))\n\t        return x_fuse\n\tblocks_dict = {\n\t    'BASIC': BasicBlock,\n\t    'BOTTLENECK': Bottleneck\n", "}\n\tclass HighResolutionNet(nn.Module):\n\t    def __init__(self, cfg, **kwargs):\n\t        super(HighResolutionNet, self).__init__()\n\t        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n\t                               bias=False)\n\t        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n\t        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n\t                               bias=False)\n\t        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n", "        self.relu = nn.ReLU(inplace=True)\n\t        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4)\n\t        self.stage2_cfg = cfg['MODEL']['EXTRA']['STAGE2']\n\t        num_channels = self.stage2_cfg['NUM_CHANNELS']\n\t        block = blocks_dict[self.stage2_cfg['BLOCK']]\n\t        num_channels = [\n\t            num_channels[i] * block.expansion for i in range(len(num_channels))]\n\t        self.transition1 = self._make_transition_layer(\n\t            [256], num_channels)\n\t        self.stage2, pre_stage_channels = self._make_stage(\n", "            self.stage2_cfg, num_channels)\n\t        self.stage3_cfg = cfg['MODEL']['EXTRA']['STAGE3']\n\t        num_channels = self.stage3_cfg['NUM_CHANNELS']\n\t        block = blocks_dict[self.stage3_cfg['BLOCK']]\n\t        num_channels = [\n\t            num_channels[i] * block.expansion for i in range(len(num_channels))]\n\t        self.transition2 = self._make_transition_layer(\n\t            pre_stage_channels, num_channels)\n\t        self.stage3, pre_stage_channels = self._make_stage(\n\t            self.stage3_cfg, num_channels)\n", "        self.stage4_cfg = cfg['MODEL']['EXTRA']['STAGE4']\n\t        num_channels = self.stage4_cfg['NUM_CHANNELS']\n\t        block = blocks_dict[self.stage4_cfg['BLOCK']]\n\t        num_channels = [\n\t            num_channels[i] * block.expansion for i in range(len(num_channels))]\n\t        self.transition3 = self._make_transition_layer(\n\t            pre_stage_channels, num_channels)\n\t        self.stage4, pre_stage_channels = self._make_stage(\n\t            self.stage4_cfg, num_channels, multi_scale_output=True)\n\t        # Classification Head\n", "#        self.incre_modules, self.downsamp_modules, \\\n\t#            self.final_layer = self._make_head(pre_stage_channels)\n\t#\n\t#        self.classifier = nn.Linear(2048, 1000)\n\t#    def _make_head(self, pre_stage_channels):\n\t#        head_block = Bottleneck\n\t#        head_channels = [32, 64, 128, 256]\n\t#\n\t#        # Increasing the #channels on each resolution \n\t#        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n", "#        incre_modules = []\n\t#        for i, channels  in enumerate(pre_stage_channels):\n\t#            incre_module = self._make_layer(head_block,\n\t#                                            channels,\n\t#                                            head_channels[i],\n\t#                                            1,\n\t#                                            stride=1)\n\t#            incre_modules.append(incre_module)\n\t#        incre_modules = nn.ModuleList(incre_modules)\n\t#            \n", "#        # downsampling modules\n\t#        downsamp_modules = []\n\t#        for i in range(len(pre_stage_channels)-1):\n\t#            in_channels = head_channels[i] * head_block.expansion\n\t#            out_channels = head_channels[i+1] * head_block.expansion\n\t#\n\t#            downsamp_module = nn.Sequential(\n\t#                nn.Conv2d(in_channels=in_channels,\n\t#                          out_channels=out_channels,\n\t#                          kernel_size=3,\n", "#                          stride=2,\n\t#                          padding=1),\n\t#                nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM),\n\t#                nn.ReLU(inplace=True)\n\t#            )\n\t#\n\t#            downsamp_modules.append(downsamp_module)\n\t#        downsamp_modules = nn.ModuleList(downsamp_modules)\n\t#\n\t#        final_layer = nn.Sequential(\n", "#            nn.Conv2d(\n\t#                in_channels=head_channels[3] * head_block.expansion,\n\t#                out_channels=2048,\n\t#                kernel_size=1,\n\t#                stride=1,\n\t#                padding=0\n\t#            ),\n\t#            nn.BatchNorm2d(2048, momentum=BN_MOMENTUM),\n\t#            nn.ReLU(inplace=True)\n\t#        )\n", "#\n\t#        return incre_modules, downsamp_modules, final_layer\n\t    def _make_transition_layer(\n\t            self, num_channels_pre_layer, num_channels_cur_layer):\n\t        num_branches_cur = len(num_channels_cur_layer)\n\t        num_branches_pre = len(num_channels_pre_layer)\n\t        transition_layers = []\n\t        for i in range(num_branches_cur):\n\t            if i < num_branches_pre:\n\t                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n", "                    transition_layers.append(nn.Sequential(\n\t                        nn.Conv2d(num_channels_pre_layer[i],\n\t                                  num_channels_cur_layer[i],\n\t                                  3,\n\t                                  1,\n\t                                  1,\n\t                                  bias=False),\n\t                        nn.BatchNorm2d(\n\t                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n\t                        nn.ReLU(inplace=True)))\n", "                else:\n\t                    transition_layers.append(None)\n\t            else:\n\t                conv3x3s = []\n\t                for j in range(i+1-num_branches_pre):\n\t                    inchannels = num_channels_pre_layer[-1]\n\t                    outchannels = num_channels_cur_layer[i] \\\n\t                        if j == i-num_branches_pre else inchannels\n\t                    conv3x3s.append(nn.Sequential(\n\t                        nn.Conv2d(\n", "                            inchannels, outchannels, 3, 2, 1, bias=False),\n\t                        nn.BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n\t                        nn.ReLU(inplace=True)))\n\t                transition_layers.append(nn.Sequential(*conv3x3s))\n\t        return nn.ModuleList(transition_layers)\n\t    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n\t        downsample = None\n\t        if stride != 1 or inplanes != planes * block.expansion:\n\t            downsample = nn.Sequential(\n\t                nn.Conv2d(inplanes, planes * block.expansion,\n", "                          kernel_size=1, stride=stride, bias=False),\n\t                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n\t            )\n\t        layers = []\n\t        layers.append(block(inplanes, planes, stride, downsample))\n\t        inplanes = planes * block.expansion\n\t        for i in range(1, blocks):\n\t            layers.append(block(inplanes, planes))\n\t        return nn.Sequential(*layers)\n\t    def _make_stage(self, layer_config, num_inchannels,\n", "                    multi_scale_output=True):\n\t        num_modules = layer_config['NUM_MODULES']\n\t        num_branches = layer_config['NUM_BRANCHES']\n\t        num_blocks = layer_config['NUM_BLOCKS']\n\t        num_channels = layer_config['NUM_CHANNELS']\n\t        block = blocks_dict[layer_config['BLOCK']]\n\t        fuse_method = layer_config['FUSE_METHOD']\n\t        modules = []\n\t        for i in range(num_modules):\n\t            # multi_scale_output is only used last module\n", "            if not multi_scale_output and i == num_modules - 1:\n\t                reset_multi_scale_output = False\n\t            else:\n\t                reset_multi_scale_output = True\n\t            modules.append(\n\t                HighResolutionModule(num_branches,\n\t                                      block,\n\t                                      num_blocks,\n\t                                      num_inchannels,\n\t                                      num_channels,\n", "                                      fuse_method,\n\t                                      reset_multi_scale_output)\n\t            )\n\t            num_inchannels = modules[-1].get_num_inchannels()\n\t        return nn.Sequential(*modules), num_inchannels\n\t    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n\t        x = self.conv2(x)\n", "        x = self.bn2(x)\n\t        x = self.relu(x)\n\t        x = self.layer1(x)\n\t        x_list = []\n\t        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n\t            if self.transition1[i] is not None:\n\t                x_list.append(self.transition1[i](x))\n\t            else:\n\t                x_list.append(x)\n\t        y_list = self.stage2(x_list)\n", "        x_list = []\n\t        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n\t            if self.transition2[i] is not None:\n\t                x_list.append(self.transition2[i](y_list[-1]))\n\t            else:\n\t                x_list.append(y_list[i])\n\t        y_list = self.stage3(x_list)\n\t        x_list = []\n\t        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n\t            if self.transition3[i] is not None:\n", "                x_list.append(self.transition3[i](y_list[-1]))\n\t            else:\n\t                x_list.append(y_list[i])\n\t        y_list = self.stage4(x_list)\n\t        # Classification Head\n\t#        y = self.incre_modules[0](y_list[0])\n\t#        for i in range(len(self.downsamp_modules)):\n\t#            y = self.incre_modules[i+1](y_list[i+1]) + \\\n\t#                        self.downsamp_modules[i](y)\n\t#\n", "#        y = self.final_layer(y)\n\t#\n\t#        y = F.avg_pool2d(y, kernel_size=y.size()\n\t#                             [2:]).view(y.size(0), -1)\n\t#            \n\t#        y = self.classifier(y)\n\t        return y_list\n\t    def init_weights(self, pretrained='',):\n\t        logger.info('=> init weights from normal distribution')\n\t        for m in self.modules():\n", "            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(\n\t                    m.weight, mode='fan_out', nonlinearity='relu')\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t        if os.path.isfile(pretrained):\n\t            pretrained_dict = torch.load(pretrained)\n\t            logger.info('=> loading pretrained model {}'.format(pretrained))\n\t            model_dict = self.state_dict()\n", "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n\t                               if k in model_dict.keys()}\n\t            for k, _ in pretrained_dict.items():\n\t                logger.info(\n\t                    '=> loading {} pretrained model {}'.format(k, pretrained))\n\t            model_dict.update(pretrained_dict)\n\t            self.load_state_dict(model_dict)\n\tdef get_cls_net(config, **kwargs):\n\t    model = HighResolutionNet(config, **kwargs)\n\t    model.init_weights()\n", "    return model\n"]}
{"filename": "GCAGC_CVPR2020/model3/__init__.py", "chunked_list": []}
{"filename": "GCAGC_CVPR2020/model3/default.py", "chunked_list": ["# ------------------------------------------------------------------------------\n\t# Copyright (c) Microsoft\n\t# Licensed under the MIT License.\n\t# Written by Ke Sun (sunk@mail.ustc.edu.cn)\n\t# ------------------------------------------------------------------------------\n\tfrom __future__ import absolute_import\n\tfrom __future__ import division\n\tfrom __future__ import print_function\n\timport os\n\tfrom yacs.config import CfgNode as CN\n", "_C = CN()\n\t#\n\t# # common params for NETWORK\n\t# _C.MODEL = CN()\n\t# _C.MODEL.NAME = 'seg_hrnet'\n\t# _C.MODEL.PRETRAINED = ''\n\t# _C.MODEL.EXTRA = CN(new_allowed=True)\n\t#\n\t# # stage2\n\t# _C.MODEL.EXTRA.STAGE2 = CN()\n", "# _C.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\n\t# _C.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\n\t# _C.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\n\t# _C.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\n\t# _C.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [48, 96]\n\t# _C.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n\t#\n\t# # stage3\n\t# _C.MODEL.EXTRA.STAGE3 = CN()\n\t# _C.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\n", "# _C.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\n\t# _C.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\n\t# _C.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\n\t# _C.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [48, 96, 192]\n\t# _C.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n\t#\n\t# # stage4\n\t# _C.MODEL.EXTRA.STAGE4 = CN()\n\t# _C.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\n\t# _C.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\n", "# _C.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\n\t# _C.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\n\t# _C.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [48, 96, 192, 384]\n\t# _C.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n\t_C.MODEL = CN()\n\t_C.MODEL.EXTRA = CN(new_allowed=True)\n\t_C.MODEL.PRETRAINED_LAYERS = ['*']\n\t_C.MODEL.STEM_INPLANES = 64\n\t_C.MODEL.FINAL_CONV_KERNEL = 1\n\t_C.MODEL.WITH_HEAD = True\n", "_C.MODEL.EXTRA.STAGE2 = CN()\n\t_C.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\n\t_C.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\n\t_C.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\n\t_C.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [48, 96]\n\t_C.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\n\t_C.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n\t_C.MODEL.EXTRA.STAGE3 = CN()\n\t_C.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\n\t_C.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\n", "_C.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\n\t_C.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [48, 96, 192]\n\t_C.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\n\t_C.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n\t_C.MODEL.EXTRA.STAGE4 = CN()\n\t_C.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\n\t_C.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\n\t_C.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\n\t_C.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [48, 96, 192, 384]\n\t_C.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\n", "_C.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n"]}
{"filename": "MCCL/config.py", "chunked_list": ["import os\n\tclass Config():\n\t    def __init__(self) -> None:\n\t        # Backbone\n\t        self.bb = ['cnn-vgg16', 'cnn-vgg16bn', 'cnn-resnet50', 'trans-pvt'][3]\n\t        self.pvt_weights = ['../bb_weights/pvt_v2_b2.pth', ''][0]\n\t        # BN\n\t        self.use_bn = self.bb not in ['cnn-vgg16']\n\t        # Augmentation\n\t        self.preproc_methods = ['flip', 'enhance', 'rotate', 'crop', 'pepper'][:3]\n", "        # Components\n\t        self.consensus = ['', 'GCAM', 'GWM', 'SGS'][1]\n\t        self.dec_blk = ['ResBlk'][0]\n\t        self.GCAM_metric = ['online', 'offline', ''][0] if self.consensus else ''\n\t        # Training\n\t        self.batch_size = 48\n\t        self.loadN = 2\n\t        self.dec_att = ['', 'ASPP'][0]\n\t        self.auto_pad = ['', 'fixed', 'adaptive'][0]\n\t        self.optimizer = ['Adam', 'AdamW'][1]\n", "        self.lr = 1e-4\n\t        self.freeze = True\n\t        self.lr_decay_epochs = [-20]    # Set to negative N to decay the lr in the last N-th epoch.\n\t        self.forward_per_dataset = True\n\t        # Adv\n\t        self.lambda_adv_g = 10. * 1        # turn to 0 to avoid adv training\n\t        self.lambda_adv_d = 3. * (self.lambda_adv_g > 0)\n\t        # Loss\n\t        losses = ['sal']\n\t        self.loss = losses[:]\n", "        self.cls_mask_operation = ['x', '+', 'c'][0]\n\t        # Loss + Triplet Loss\n\t        self.lambdas_sal_last = {\n\t            # not 0 means opening this loss\n\t            # original rate -- 1 : 30 : 1.5 : 0.2, bce x 30\n\t            'bce': 30 * 1,          # high performance\n\t            'iou': 0.5 * 1,         # 0 / 255\n\t            'ssim': 1 * 0,          # help contours\n\t            'mse': 150 * 0,         # can smooth the saliency map\n\t            'reg': 100 * 0,\n", "            'triplet': 3 * 1,\n\t        }\n\t        self.db_output_decoder = False\n\t        self.refine = False\n\t        self.db_output_refiner = False\n\t        # Triplet Loss\n\t        self.triplet = ['_x5', 'mask'][:1]\n\t        self.triplet_loss_margin = 0.1\n\t        # Intermediate Layers\n\t        self.lambdas_sal_others = {\n", "            'bce': 0,\n\t            'iou': 0.,\n\t            'ssim': 0,\n\t            'mse': 0,\n\t            'reg': 0,\n\t            'triplet': 0,\n\t        }\n\t        self.output_number = 1\n\t        self.loss_sal_layers = 4              # used to be last 4 layers\n\t        self.loss_cls_mask_last_layers = 1         # used to be last 4 layers\n", "        if 'keep in range':\n\t            self.loss_sal_layers = min(self.output_number, self.loss_sal_layers)\n\t            self.loss_cls_mask_last_layers = min(self.output_number, self.loss_cls_mask_last_layers)\n\t            self.output_number = min(self.output_number, max(self.loss_sal_layers, self.loss_cls_mask_last_layers))\n\t            if self.output_number == 1:\n\t                for cri in self.lambdas_sal_others:\n\t                    self.lambdas_sal_others[cri] = 0\n\t        self.conv_after_itp = False\n\t        self.complex_lateral_connection = False\n\t        # to control the quantitive level of each single loss by number of output branches.\n", "        self.loss_cls_mask_ratio_by_last_layers = 4 / self.loss_cls_mask_last_layers\n\t        for loss_sal in self.lambdas_sal_last.keys():\n\t            loss_sal_ratio_by_last_layers = 4 / (int(bool(self.lambdas_sal_others[loss_sal])) * (self.loss_sal_layers - 1) + 1)\n\t            self.lambdas_sal_last[loss_sal] *= loss_sal_ratio_by_last_layers\n\t            self.lambdas_sal_others[loss_sal] *= loss_sal_ratio_by_last_layers\n\t        self.lambda_cls_mask = 2.5 * self.loss_cls_mask_ratio_by_last_layers\n\t        self.lambda_cls = 3.\n\t        self.lambda_contrast = 250.\n\t        # others\n\t        self.self_supervision = False\n", "        self.label_smoothing = False\n\t        self.validation = False\n\t        self.rand_seed = 7\n\t        # run_sh_file = [f for f in os.listdir('.') if 'go' in f and '.sh' in f] + [os.path.join('..', f) for f in os.listdir('..') if 'gco' in f and '.sh' in f]\n\t        # with open(run_sh_file[0], 'r') as f:\n\t        #     lines = f.readlines()\n\t        #     self.val_last = int([l.strip() for l in lines if 'val_last=' in l][0].split('=')[-1])\n\t        #     self.save_step = int([l.strip() for l in lines if 'step=' in l][0].split('=')[-1])\n"]}
{"filename": "MCCL/models/pvt.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom functools import partial\n\tfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\tfrom timm.models.registry import register_model\n\tfrom timm.models.vision_transformer import _cfg\n\tfrom timm.models.registry import register_model\n\timport math\n\tclass Mlp(nn.Module):\n", "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.dwconv = DWConv(hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t        self.apply(self._init_weights)\n", "    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t        elif isinstance(m, nn.Conv2d):\n\t            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n", "            fan_out //= m.groups\n\t            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n\t            if m.bias is not None:\n\t                m.bias.data.zero_()\n\t    def forward(self, x, H, W):\n\t        x = self.fc1(x)\n\t        x = self.dwconv(x, H, W)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n\t        x = self.fc2(x)\n", "        x = self.drop(x)\n\t        return x\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n\t        super().__init__()\n\t        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\t        self.dim = dim\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n\t        self.scale = qk_scale or head_dim ** -0.5\n", "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n\t        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t        self.sr_ratio = sr_ratio\n\t        if sr_ratio > 1:\n\t            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n\t            self.norm = nn.LayerNorm(dim)\n\t        self.apply(self._init_weights)\n", "    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t        elif isinstance(m, nn.Conv2d):\n\t            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n", "            fan_out //= m.groups\n\t            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n\t            if m.bias is not None:\n\t                m.bias.data.zero_()\n\t    def forward(self, x, H, W):\n\t        B, N, C = x.shape\n\t        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\t        if self.sr_ratio > 1:\n\t            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n\t            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n", "            x_ = self.norm(x_)\n\t            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        else:\n\t            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        k, v = kv[0], kv[1]\n\t        attn = (q @ k.transpose(-2, -1)) * self.scale\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n\t        x = self.proj(x)\n", "        x = self.proj_drop(x)\n\t        return x\n\tclass Block(nn.Module):\n\t    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n\t                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n\t        super().__init__()\n\t        self.norm1 = norm_layer(dim)\n\t        self.attn = Attention(\n\t            dim,\n\t            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n", "            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n\t        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\t        self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n", "            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t        elif isinstance(m, nn.Conv2d):\n\t            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n\t            fan_out //= m.groups\n\t            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n\t            if m.bias is not None:\n", "                m.bias.data.zero_()\n\t    def forward(self, x, H, W):\n\t        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n\t        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n\t        return x\n\tclass OverlapPatchEmbed(nn.Module):\n\t    \"\"\" Image to Patch Embedding\n\t    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n\t        super().__init__()\n", "        img_size = to_2tuple(img_size)\n\t        patch_size = to_2tuple(patch_size)\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n\t        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n\t        self.num_patches = self.H * self.W\n\t        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n\t                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n\t        self.norm = nn.LayerNorm(embed_dim)\n\t        self.apply(self._init_weights)\n", "    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t        elif isinstance(m, nn.Conv2d):\n\t            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n", "            fan_out //= m.groups\n\t            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n\t            if m.bias is not None:\n\t                m.bias.data.zero_()\n\t    def forward(self, x):\n\t        x = self.proj(x)\n\t        _, _, H, W = x.shape\n\t        x = x.flatten(2).transpose(1, 2)\n\t        x = self.norm(x)\n\t        return x, H, W\n", "class PyramidVisionTransformerImpr(nn.Module):\n\t    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n\t                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n\t                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n\t                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n\t        super().__init__()\n\t        self.num_classes = num_classes\n\t        self.depths = depths\n\t        # patch_embed\n\t        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n", "                                              embed_dim=embed_dims[0])\n\t        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n\t                                              embed_dim=embed_dims[1])\n\t        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n\t                                              embed_dim=embed_dims[2])\n\t        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n\t                                              embed_dim=embed_dims[3])\n\t        # transformer encoder\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\t        cur = 0\n", "        self.block1 = nn.ModuleList([Block(\n\t            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n\t            sr_ratio=sr_ratios[0])\n\t            for i in range(depths[0])])\n\t        self.norm1 = norm_layer(embed_dims[0])\n\t        cur += depths[0]\n\t        self.block2 = nn.ModuleList([Block(\n\t            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n", "            sr_ratio=sr_ratios[1])\n\t            for i in range(depths[1])])\n\t        self.norm2 = norm_layer(embed_dims[1])\n\t        cur += depths[1]\n\t        self.block3 = nn.ModuleList([Block(\n\t            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n\t            sr_ratio=sr_ratios[2])\n\t            for i in range(depths[2])])\n\t        self.norm3 = norm_layer(embed_dims[2])\n", "        cur += depths[2]\n\t        self.block4 = nn.ModuleList([Block(\n\t            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n\t            sr_ratio=sr_ratios[3])\n\t            for i in range(depths[3])])\n\t        self.norm4 = norm_layer(embed_dims[3])\n\t        # classification head\n\t        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n\t        self.apply(self._init_weights)\n", "    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t        elif isinstance(m, nn.Conv2d):\n\t            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n", "            fan_out //= m.groups\n\t            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n\t            if m.bias is not None:\n\t                m.bias.data.zero_()\n\t    def init_weights(self, pretrained=None):\n\t        if isinstance(pretrained, str):\n\t            logger = 1\n\t            #load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n\t    def reset_drop_path(self, drop_path_rate):\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n", "        cur = 0\n\t        for i in range(self.depths[0]):\n\t            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n\t        cur += self.depths[0]\n\t        for i in range(self.depths[1]):\n\t            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n\t        cur += self.depths[1]\n\t        for i in range(self.depths[2]):\n\t            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n\t        cur += self.depths[2]\n", "        for i in range(self.depths[3]):\n\t            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n\t    def freeze_patch_emb(self):\n\t        self.patch_embed1.requires_grad = False\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n\t    def get_classifier(self):\n\t        return self.head\n\t    def reset_classifier(self, num_classes, global_pool=''):\n", "        self.num_classes = num_classes\n\t        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t    # def _get_pos_embed(self, pos_embed, patch_embed, H, W):\n\t    #     if H * W == self.patch_embed1.num_patches:\n\t    #         return pos_embed\n\t    #     else:\n\t    #         return F.interpolate(\n\t    #             pos_embed.reshape(1, patch_embed.H, patch_embed.W, -1).permute(0, 3, 1, 2),\n\t    #             size=(H, W), mode=\"bilinear\").reshape(1, -1, H * W).permute(0, 2, 1)\n\t    def forward_features(self, x):\n", "        B = x.shape[0]\n\t        outs = []\n\t        # stage 1\n\t        x, H, W = self.patch_embed1(x)\n\t        for i, blk in enumerate(self.block1):\n\t            x = blk(x, H, W)\n\t        x = self.norm1(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n\t        # stage 2\n", "        x, H, W = self.patch_embed2(x)\n\t        for i, blk in enumerate(self.block2):\n\t            x = blk(x, H, W)\n\t        x = self.norm2(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n\t        # stage 3\n\t        x, H, W = self.patch_embed3(x)\n\t        for i, blk in enumerate(self.block3):\n\t            x = blk(x, H, W)\n", "        x = self.norm3(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n\t        # stage 4\n\t        x, H, W = self.patch_embed4(x)\n\t        for i, blk in enumerate(self.block4):\n\t            x = blk(x, H, W)\n\t        x = self.norm4(x)\n\t        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\t        outs.append(x)\n", "        return outs\n\t        # return x.mean(dim=1)\n\t    def forward(self, x):\n\t        x = self.forward_features(x)\n\t        # x = self.head(x)\n\t        return x\n\tclass DWConv(nn.Module):\n\t    def __init__(self, dim=768):\n\t        super(DWConv, self).__init__()\n\t        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n", "    def forward(self, x, H, W):\n\t        B, N, C = x.shape\n\t        x = x.transpose(1, 2).view(B, C, H, W)\n\t        x = self.dwconv(x)\n\t        x = x.flatten(2).transpose(1, 2)\n\t        return x\n\tdef _conv_filter(state_dict, patch_size=16):\n\t    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n\t    out_dict = {}\n\t    for k, v in state_dict.items():\n", "        if 'patch_embed.proj.weight' in k:\n\t            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n\t        out_dict[k] = v\n\t    return out_dict\n\t@register_model\n\tclass pvt_v2_b0(PyramidVisionTransformerImpr):\n\t    def __init__(self, **kwargs):\n\t        super(pvt_v2_b0, self).__init__(\n\t            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n\t            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n", "            drop_rate=0.0, drop_path_rate=0.1)\n\t@register_model\n\tclass pvt_v2_b1(PyramidVisionTransformerImpr):\n\t    def __init__(self, **kwargs):\n\t        super(pvt_v2_b1, self).__init__(\n\t            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n\t            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n\t            drop_rate=0.0, drop_path_rate=0.1)\n\t@register_model\n\tclass pvt_v2_b2(PyramidVisionTransformerImpr):\n", "    def __init__(self, **kwargs):\n\t        super(pvt_v2_b2, self).__init__(\n\t            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n\t            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n\t            drop_rate=0.0, drop_path_rate=0.1)\n\t@register_model\n\tclass pvt_v2_b3(PyramidVisionTransformerImpr):\n\t    def __init__(self, **kwargs):\n\t        super(pvt_v2_b3, self).__init__(\n\t            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n", "            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n\t            drop_rate=0.0, drop_path_rate=0.1)\n\t@register_model\n\tclass pvt_v2_b4(PyramidVisionTransformerImpr):\n\t    def __init__(self, **kwargs):\n\t        super(pvt_v2_b4, self).__init__(\n\t            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n\t            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n\t            drop_rate=0.0, drop_path_rate=0.1)\n\t@register_model\n", "class pvt_v2_b5(PyramidVisionTransformerImpr):\n\t    def __init__(self, **kwargs):\n\t        super(pvt_v2_b5, self).__init__(\n\t            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n\t            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n\t            drop_rate=0.0, drop_path_rate=0.1)\n"]}
{"filename": "MCCL/models/modules.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\t# import fvcore.nn.weight_init as weight_init\n\tfrom functools import partial\n\tfrom MCCL.config import Config\n\tconfig = Config()\n\tclass ResBlk(nn.Module):\n\t    def __init__(self, channel_in=64, channel_out=64, groups=0):\n", "        super(ResBlk, self).__init__()\n\t        self.conv_in = nn.Conv2d(channel_in, 64, 3, 1, 1)\n\t        self.relu_in = nn.ReLU(inplace=True)\n\t        if config.dec_att == 'ASPP':\n\t            self.dec_att = ASPP(channel_in=64)\n\t        self.conv_out = nn.Conv2d(64, channel_out, 3, 1, 1)\n\t        if config.use_bn:\n\t            self.bn_in = nn.BatchNorm2d(64)\n\t            self.bn_out = nn.BatchNorm2d(channel_out)\n\t    def forward(self, x):\n", "        x = self.conv_in(x)\n\t        if config.use_bn:\n\t            x = self.bn_in(x)\n\t        x = self.relu_in(x)\n\t        if config.dec_att:\n\t            x = self.dec_att(x)\n\t        x = self.conv_out(x)\n\t        if config.use_bn:\n\t            x = self.bn_out(x)\n\t        return x\n", "class _ASPPModule(nn.Module):\n\t    def __init__(self, channel_in, planes, kernel_size, padding, dilation):\n\t        super(_ASPPModule, self).__init__()\n\t        self.atrous_conv = nn.Conv2d(channel_in, planes, kernel_size=kernel_size,\n\t                                            stride=1, padding=padding, dilation=dilation, bias=False)\n\t        self.bn = nn.BatchNorm2d(planes)\n\t        self.relu = nn.ReLU(inplace=True)\n\t    def forward(self, x):\n\t        x = self.atrous_conv(x)\n\t        x = self.bn(x)\n", "        return self.relu(x)\n\tclass ASPP(nn.Module):\n\t    def __init__(self, channel_in=64, output_stride=16):\n\t        super(ASPP, self).__init__()\n\t        self.down_scale = 1\n\t        self.channel_inter = 256 // self.down_scale\n\t        if output_stride == 16:\n\t            dilations = [1, 6, 12, 18]\n\t        elif output_stride == 8:\n\t            dilations = [1, 12, 24, 36]\n", "        else:\n\t            raise NotImplementedError\n\t        self.aspp1 = _ASPPModule(channel_in, self.channel_inter, 1, padding=0, dilation=dilations[0])\n\t        self.aspp2 = _ASPPModule(channel_in, self.channel_inter, 3, padding=dilations[1], dilation=dilations[1])\n\t        self.aspp3 = _ASPPModule(channel_in, self.channel_inter, 3, padding=dilations[2], dilation=dilations[2])\n\t        self.aspp4 = _ASPPModule(channel_in, self.channel_inter, 3, padding=dilations[3], dilation=dilations[3])\n\t        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n\t                                             nn.Conv2d(channel_in, self.channel_inter, 1, stride=1, bias=False),\n\t                                             nn.BatchNorm2d(self.channel_inter),\n\t                                             nn.ReLU(inplace=True))\n", "        self.conv1 = nn.Conv2d(self.channel_inter * 5, channel_in, 1, bias=False)\n\t        self.bn1 = nn.BatchNorm2d(channel_in)\n\t        self.relu = nn.ReLU(inplace=True)\n\t        self.dropout = nn.Dropout(0.5)\n\t    def forward(self, x):\n\t        x1 = self.aspp1(x)\n\t        x2 = self.aspp2(x)\n\t        x3 = self.aspp3(x)\n\t        x4 = self.aspp4(x)\n\t        x5 = self.global_avg_pool(x)\n", "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n\t        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\t        x = self.conv1(x)\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n\t        return self.dropout(x)\n\tclass CoAttLayer(nn.Module):\n\t    def __init__(self, channel_in=512):\n\t        super(CoAttLayer, self).__init__()\n\t        self.all_attention = GCAM(channel_in)\n", "    def forward(self, x):\n\t        if self.training:\n\t            if config.loadN > 1:\n\t                channel_per_class = x.shape[0] // config.loadN\n\t                x_per_class_corr_list = []\n\t                for idx in range(0, x.shape[0], channel_per_class):\n\t                    x_per_class = x[idx:idx+channel_per_class]\n\t                    x_new_per_class = self.all_attention(x_per_class)\n\t                    x_per_class_proto = torch.mean(x_new_per_class, (0, 2, 3), True).view(1, -1)\n\t                    x_per_class_proto = x_per_class_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n", "                    x_per_class_corr = x_per_class * x_per_class_proto\n\t                    x_per_class_corr_list.append(x_per_class_corr)\n\t                weighted_x = torch.cat(x_per_class_corr_list, dim=0)\n\t            else:\n\t                x_new = self.all_attention(x)\n\t                x_proto = torch.mean(x_new, (0, 2, 3), True).view(1, -1)\n\t                x_proto = x_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t                weighted_x = x * x_proto\n\t        else:\n\t            x_new = self.all_attention(x)\n", "            x_proto = torch.mean(x_new, (0, 2, 3), True).view(1, -1)\n\t            x_proto = x_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            weighted_x = x * x_proto\n\t        return weighted_x\n\tclass GCAM(nn.Module):\n\t    def __init__(self, channel_in=512):\n\t        super(GCAM, self).__init__()\n\t        self.query_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        self.key_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        self.scale = 1.0 / (channel_in ** 0.5)\n", "        self.conv6 = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        # for layer in [self.query_transform, self.key_transform, self.conv6]:\n\t        #     weight_init.c2_msra_fill(layer)\n\t    def forward(self, x):\n\t        # x: B,C,H,W\n\t        # x_query: B,C,HW\n\t        B, C, H5, W5 = x.size()\n\t        x_query = self.query_transform(x).view(B, C, -1)\n\t        # x_query: B,HW,C\n\t        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C) # BHW, C\n", "        # x_key: B,C,HW\n\t        x_key = self.key_transform(x).view(B, C, -1)\n\t        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1) # C, BHW\n\t        # W = Q^T K: B,HW,HW\n\t        x_w = torch.matmul(x_query, x_key) #* self.scale # BHW, BHW\n\t        x_w = x_w.view(B*H5*W5, B, H5*W5)\n\t        x_w = torch.max(x_w, -1).values # BHW, B\n\t        x_w = x_w.mean(-1)\n\t        #x_w = torch.mean(x_w, -1).values # BHW\n\t        x_w = x_w.view(B, -1) * self.scale # B, HW\n", "        x_w = F.softmax(x_w, dim=-1) # B, HW\n\t        x_w = x_w.view(B, H5, W5).unsqueeze(1) # B, 1, H, W\n\t        x = x * x_w\n\t        x = self.conv6(x)\n\t        return x\n"]}
{"filename": "MCCL/models/GCoNet.py", "chunked_list": ["from collections import OrderedDict\n\timport torch\n\tfrom torch.functional import norm\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torchvision.models import vgg16, vgg16_bn\n\tfrom torchvision.models import resnet50\n\tfrom MCCL.models.modules import ResBlk, CoAttLayer\n\tfrom MCCL.models.pvt import pvt_v2_b2\n\tfrom MCCL.config import Config\n", "class MCCL(nn.Module):\n\t    def __init__(self):\n\t        super(MCCL, self).__init__()\n\t        self.config = Config()\n\t        bb = self.config.bb\n\t        if bb == 'cnn-vgg16':\n\t            bb_net = list(vgg16(pretrained=False).children())[0]\n\t            bb_convs = OrderedDict({\n\t                'conv1': bb_net[:4],\n\t                'conv2': bb_net[4:9],\n", "                'conv3': bb_net[9:16],\n\t                'conv4': bb_net[16:23]\n\t            })\n\t        elif bb == 'cnn-vgg16bn':\n\t            bb_net = list(vgg16_bn(pretrained=False).children())[0]\n\t            bb_convs = OrderedDict({\n\t                'conv1': bb_net[:6],\n\t                'conv2': bb_net[6:13],\n\t                'conv3': bb_net[13:23],\n\t                'conv4': bb_net[23:33]\n", "            })\n\t        elif bb == 'cnn-resnet50':\n\t            bb_net = list(resnet50(pretrained=False).children())\n\t            bb_convs = OrderedDict({\n\t                'conv1': nn.Sequential(*bb_net[0:3]),\n\t                'conv2': bb_net[4],\n\t                'conv3': bb_net[5],\n\t                'conv4': bb_net[6]\n\t            })\n\t        elif bb == 'trans-pvt':\n", "            self.bb = pvt_v2_b2()\n\t            # if self.config.pvt_weights:\n\t            #     save_model = torch.load(self.config.pvt_weights)\n\t            #     model_dict = self.bb.state_dict()\n\t            #     state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n\t            #     model_dict.update(state_dict)\n\t            #     self.bb.load_state_dict(model_dict)\n\t        if 'cnn-' in bb:\n\t            self.bb = nn.Sequential(bb_convs)\n\t        lateral_channels_in = {\n", "            'cnn-vgg16': [512, 256, 128, 64],\n\t            'cnn-vgg16bn': [512, 256, 128, 64],\n\t            'cnn-resnet50': [1024, 512, 256, 64],\n\t            'trans-pvt': [512, 320, 128, 64],\n\t        }\n\t        if self.config.consensus == 'GCAM':\n\t            self.co_x4 = CoAttLayer(channel_in=lateral_channels_in[bb][0])\n\t        elif self.config.consensus == 'SGS':\n\t            self.co_x4 = SGS(channel_in=lateral_channels_in[bb][0])\n\t        elif self.config.consensus == 'GWM':\n", "            self.co_x4 = GWM(channel_in=lateral_channels_in[bb][0])\n\t        if self.config.dec_blk == 'ResBlk':\n\t            DecBlk = ResBlk\n\t        self.top_layer = DecBlk(lateral_channels_in[bb][0], lateral_channels_in[bb][1])\n\t        self.dec_layer4 = DecBlk(lateral_channels_in[bb][1], lateral_channels_in[bb][1])\n\t        self.lat_layer4 = nn.Conv2d(lateral_channels_in[bb][1], lateral_channels_in[bb][1], 1, 1, 0)\n\t        self.dec_layer3 = DecBlk(lateral_channels_in[bb][1], lateral_channels_in[bb][2])\n\t        self.lat_layer3 = nn.Conv2d(lateral_channels_in[bb][2], lateral_channels_in[bb][2], 1, 1, 0)\n\t        self.dec_layer2 = DecBlk(lateral_channels_in[bb][2], lateral_channels_in[bb][3])\n\t        self.lat_layer2 = nn.Conv2d(lateral_channels_in[bb][3], lateral_channels_in[bb][3], 1, 1, 0)\n", "        self.dec_layer1 = DecBlk(lateral_channels_in[bb][3], lateral_channels_in[bb][3]//2)\n\t        self.conv_out1 = nn.Sequential(nn.Conv2d(lateral_channels_in[bb][3]//2, 1, 1, 1, 0))\n\t    def forward(self, x):\n\t        ########## Encoder ##########\n\t        if 'trans' in self.config.bb:\n\t            x1, x2, x3, x4 = self.bb(x)\n\t        else:\n\t            x1 = self.bb.conv1(x)\n\t            x2 = self.bb.conv2(x1)\n\t            x3 = self.bb.conv3(x2)\n", "            x4 = self.bb.conv4(x3)\n\t        if self.config.consensus:\n\t            x4 = self.co_x4(x4)\n\t        p4 = self.top_layer(x4)\n\t        ########## Decoder ##########\n\t        scaled_preds = []\n\t        p4 = self.dec_layer4(p4)\n\t        p4 = F.interpolate(p4, size=x3.shape[2:], mode='bilinear', align_corners=True)\n\t        p3 = p4 + self.lat_layer4(x3)\n\t        p3 = self.dec_layer3(p3)\n", "        p3 = F.interpolate(p3, size=x2.shape[2:], mode='bilinear', align_corners=True)\n\t        p2 = p3 + self.lat_layer3(x2)\n\t        p2 = self.dec_layer2(p2)\n\t        p2 = F.interpolate(p2, size=x1.shape[2:], mode='bilinear', align_corners=True)\n\t        p1 = p2 + self.lat_layer2(x1)\n\t        p1 = self.dec_layer1(p1)\n\t        p1 = F.interpolate(p1, size=x.shape[2:], mode='bilinear', align_corners=True)\n\t        if self.config.db_output_decoder:\n\t            p1_out = self.db_output_decoder(p1)\n\t        else:\n", "            p1_out = self.conv_out1(p1)\n\t        scaled_preds.append(p1_out)\n\t        if self.training:\n\t            return_values = [scaled_preds, x4]\n\t            return return_values\n\t        else:\n\t            return scaled_preds\n"]}
{"filename": "CADC/parameter.py", "chunked_list": ["import os\n\t################# Training #################\n\t# Your path for pretrained vgg model\n\tload_model = './pretrained_models/vgg16_20M.caffemodel.pth'\n\t# Your path for COCO9213\n\timg_root_coco = './Data/COCO9213-os/img/'\n\tgt_root_coco = './Data/COCO9213-os/gt/'\n\t# Your path for DUTS Class\n\timg_root = './Data/DUTS_class/img/'\n\tgt_root = './Data/DUTS_class/gt/'\n", "# Your path for our synethsis data\n\timg_syn_root = './Data/DUTS_class_syn/img_png_seamless_cloning_add_naive/img/'\n\timg_ReverseSyn_root = './Data/DUTS_class_syn/img_png_seamless_cloning_add_naive_reverse/img/'\n\tgt_ReverseSyn_root = './Data/DUTS_class_syn/img_png_seamless_cloning_add_naive_reverse/gt/'\n\t# save model path\n\tsave_model_dir = './checkpoint/'\n\t# if not os.path.exists(save_model_dir):\n\t#     os.makedirs(save_model_dir)\n\t# settings\n\tgpu_id = \"0\"\n", "max_num = 14\n\tdec_channels = 64\n\timg_size = 256\n\tscale_size = 288\n\tbatch_size = 1\n\tlr = 0.01\n\tepochs = 300\n\ttrain_steps = 40000\n\tlr_decay_gamma = 0.1\n\tstepvalue1 = 20000\n", "stepvalue2 = 30000\n\tloss_weights = [1, 0.8, 0.8, 0.5, 0.5, 0.5]\n\tbn_momentum = 0.001\n\t################# Testing #################\n\t# save output path\n\t# save_test_path_root = './Preds/'\n\t# if not os.path.exists(save_test_path_root):\n\t#     os.makedirs(save_test_path_root)\n\t# testing your own trained model\n\ttest_model = 'iterations40000.pth'\n", "test_model_dir = save_model_dir + test_model\n\t# testing our pretrained CADC model\n\t# test_model_dir = 'checkpoint/CADC.pth'\n\t# test dataset path\n\ttest_dir_img = ['./Data/CoCA/image',\n\t                './Data/CoSal2015/Image',\n\t                './Data/CoSOD3k/Image',\n\t                './Data/MSRC/Image',\n\t                ]\n"]}
{"filename": "CADC/ImageBranchEncoder/__init__.py", "chunked_list": ["from .ImageBranchEncoder import ImageBranchEncoder"]}
{"filename": "CADC/ImageBranchEncoder/ImageBranchEncoder.py", "chunked_list": ["import torch.nn as nn\n\tclass ImageBranchEncoder(nn.Module):\n\t    def __init__(self, n_channels):\n\t        super(ImageBranchEncoder, self).__init__()\n\t        self.conv1 = nn.Sequential(\n\t            nn.Conv2d(n_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t        self.conv2 = nn.Sequential(\n", "            nn.ReLU(),\n\t            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True),\n\t            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n\t        )\n\t        self.conv3 = nn.Sequential(\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True),\n\t            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n", "            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n\t        )\n\t        self.conv4 = nn.Sequential(\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True),\n\t            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n", "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n\t        )\n\t        self.conv5 = nn.Sequential(\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n\t            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, dilation=2, stride=1, padding=2),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, dilation=2, stride=1, padding=2),\n", "            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, dilation=2, stride=1, padding=2),\n\t        )\n\t        self.fc6 = nn.Sequential(\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n\t            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, dilation=12, padding=12),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.dropout = nn.Dropout(0.5)\n", "        self.fc7 = nn.Sequential(\n\t            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t    def forward(self, x):\n\t        out_conv1 = self.conv1(x)\n\t        out_conv2 = self.conv2(out_conv1)\n\t        out_conv3 = self.conv3(out_conv2)\n\t        out_conv4 = self.conv4(out_conv3)\n\t        out_conv5 = self.conv5(out_conv4)\n", "        x6 = self.fc6(out_conv5)\n\t        x7 = self.fc7(self.dropout(x6))\n\t        return out_conv1, out_conv2, out_conv3, out_conv4, out_conv5, x7\n"]}
{"filename": "CADC/ImageBranchDecoder/ImageBranchDecoder.py", "chunked_list": ["import torch.nn as nn\n\timport torch\n\timport torch.nn.functional as F\n\tfrom CADC.parameter import *\n\tfrom CADC.Modules import KernelModule\n\tfrom CADC.Modules import Transformer\n\tfrom CADC.Modules import poolingModule\n\tclass spatialAttDecoder_module(nn.Module):\n\t    def __init__(self, in_channels, out_channels, fusing=True):\n\t        super(spatialAttDecoder_module, self).__init__()\n", "        if fusing:\n\t            self.enc_fea_proc = nn.Sequential(\n\t                nn.BatchNorm2d(in_channels, momentum=bn_momentum),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t            in_channels = in_channels + dec_channels\n\t        self.decoding1 = nn.Sequential(\n\t            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n\t            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n\t            nn.ReLU(inplace=True),\n", "        )\n\t        self.decoding2 = nn.Sequential(\n\t            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n\t            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.SpatialAtt = nn.Sequential(\n\t            nn.Conv2d(out_channels, 1, kernel_size=3, stride=1, padding=1),\n\t            nn.Sigmoid()\n\t        )\n", "    def forward(self, enc_fea, dec_fea=None):\n\t        if dec_fea is not None:\n\t            enc_fea = self.enc_fea_proc(enc_fea)\n\t            if dec_fea.size(2) != enc_fea.size(2):\n\t                dec_fea = F.upsample(dec_fea, size=[enc_fea.size(2), enc_fea.size(3)], mode='bilinear', align_corners=True)\n\t            spatial_att = self.SpatialAtt(dec_fea)\n\t            enc_fea = enc_fea*spatial_att\n\t            enc_fea = torch.cat([enc_fea, dec_fea], dim=1)\n\t        output = self.decoding1(enc_fea)\n\t        output = self.decoding2(output)\n", "        return output\n\tclass decoder_module(nn.Module):\n\t    def __init__(self, in_channels, out_channels, fusing=True):\n\t        super(decoder_module, self).__init__()\n\t        if fusing:\n\t            self.enc_fea_proc = nn.Sequential(\n\t                nn.BatchNorm2d(in_channels, momentum=bn_momentum),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t            self.poolingModule = poolingModule()\n", "            self.Transformer = Transformer(in_channels)\n\t            self.bn_relu = nn.Sequential(\n\t                nn.BatchNorm1d(in_channels),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t            self.KernelModule = KernelModule(in_channels)\n\t            in_channels = 3*dec_channels\n\t        self.decoding1 = nn.Sequential(\n\t            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n\t            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n", "            nn.ReLU(inplace=True),\n\t        )\n\t        self.decoding2 = nn.Sequential(\n\t            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n\t            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t    def forward(self, enc_fea, dec_fea=None):\n\t        if dec_fea is not None:\n\t            # [1] Consensus Feature Aggregation\n", "            output = self.poolingModule(enc_fea) # [N, C, 46]\n\t            Transformer_output, affinity = self.Transformer(output)\n\t            # Transformer_output [N, 46, C]   affinity (Nx46)x(Nx46)\n\t            Transformer_output = self.bn_relu(Transformer_output.permute(0, 2, 1)) # [N, C, 46]\n\t            # [2] Consensus-aware Kernel Construction and Search\n\t            enc_fea = self.enc_fea_proc(enc_fea)\n\t            enc_fea = self.KernelModule(Transformer_output, enc_fea, affinity)\n\t            if dec_fea.size(2) != enc_fea.size(2):\n\t                dec_fea = F.upsample(dec_fea, size=[enc_fea.size(2), enc_fea.size(3)], mode='bilinear', align_corners=True)\n\t            enc_fea = torch.cat([enc_fea, dec_fea], dim=1)\n", "        output = self.decoding1(enc_fea)\n\t        output = self.decoding2(output)\n\t        return output\n\tclass ImageBranchDecoder(nn.Module):\n\t    def __init__(self):\n\t        super(ImageBranchDecoder, self).__init__()\n\t        channels = [64, 128, 256, 512, 512, 512]\n\t        self.decoder6 = decoder_module(dec_channels*2, dec_channels, False)\n\t        self.decoder5 = decoder_module(channels[4], dec_channels)\n\t        self.decoder4 = decoder_module(channels[3], dec_channels)\n", "        self.decoder3 = decoder_module(channels[2], dec_channels)\n\t        self.decoder2 = spatialAttDecoder_module(channels[1], dec_channels)\n\t        self.decoder1 = spatialAttDecoder_module(channels[0], dec_channels)\n\t        self.conv_loss6 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\t        self.conv_loss5 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\t        self.conv_loss4 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\t        self.conv_loss3 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\t        self.conv_loss2 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\t        self.conv_loss1 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\t    def forward(self, enc_fea, kenerled_afteraspp, Transformer_output, affinity):\n", "        encoder_conv1, encoder_conv2, encoder_conv3, encoder_conv4, encoder_conv5, x7 = enc_fea\n\t        dec_fea_6 = self.decoder6(kenerled_afteraspp)\n\t        mask6 = self.conv_loss6(dec_fea_6)\n\t        dec_fea_5 = self.decoder5(encoder_conv5, dec_fea_6)\n\t        mask5 = self.conv_loss5(dec_fea_5)\n\t        dec_fea_4 = self.decoder4(encoder_conv4, dec_fea_5)\n\t        mask4 = self.conv_loss4(dec_fea_4)\n\t        dec_fea_3 = self.decoder3(encoder_conv3, dec_fea_4)\n\t        mask3 = self.conv_loss3(dec_fea_3)\n\t        dec_fea_2 = self.decoder2(encoder_conv2, dec_fea_3)\n", "        mask2 = self.conv_loss2(dec_fea_2)\n\t        dec_fea_1 = self.decoder1(encoder_conv1, dec_fea_2)\n\t        mask1 = self.conv_loss1(dec_fea_1)\n\t        return mask6, mask5, mask4, mask3, mask2, mask1\n"]}
{"filename": "CADC/ImageBranchDecoder/__init__.py", "chunked_list": ["from .ImageBranchDecoder import ImageBranchDecoder"]}
{"filename": "CADC/CoSODNet/__init__.py", "chunked_list": ["from .CoSODNet import CoSODNet\n"]}
{"filename": "CADC/CoSODNet/CoSODNet.py", "chunked_list": ["from CADC.ImageBranchEncoder import ImageBranchEncoder\n\tfrom CADC.ImageBranchDecoder import ImageBranchDecoder\n\timport torch.nn as nn\n\timport torch\n\tfrom torch.nn import BatchNorm2d as bn\n\tfrom CADC.Modules import KernelModule\n\tfrom CADC.Modules import Transformer\n\tfrom CADC.Modules import poolingModule\n\tclass _DenseAsppBlock(nn.Sequential):\n\t    \"\"\" ConvNet block for building DenseASPP. \"\"\"\n", "    def __init__(self, input_num, num1, num2, dilation_rate):\n\t        super(_DenseAsppBlock, self).__init__()\n\t        self.conv1 = nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)\n\t        self.bn1 = bn(num1, momentum=0.0003)\n\t        self.relu1 = nn.ReLU(inplace=True)\n\t        self.conv2 = nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n\t                               dilation=dilation_rate, padding=dilation_rate)\n\t        self.bn2 = bn(num2, momentum=0.0003)\n\t        self.relu2 = nn.ReLU(inplace=True)\n\t    def forward(self, input):\n", "        feature = self.relu1(self.bn1(self.conv1(input)))\n\t        feature = self.relu2(self.bn2(self.conv2(feature)))\n\t        return feature\n\tclass DASPPmodule(nn.Module):\n\t    def __init__(self):\n\t        super(DASPPmodule, self).__init__()\n\t        num_features = 512\n\t        d_feature1 = 176\n\t        d_feature0 = num_features//2\n\t        self.AvgPool = nn.Sequential(\n", "            nn.AvgPool2d([32, 32], [32, 32]),\n\t            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1),\n\t            nn.BatchNorm2d(512),\n\t            nn.ReLU(inplace=True),\n\t            nn.Upsample(size=32, mode='nearest'),\n\t        )\n\t        self.ASPP_2 = _DenseAsppBlock(input_num=num_features, num1=d_feature0, num2=d_feature1,\n\t                                      dilation_rate=2)\n\t        self.ASPP_4 = _DenseAsppBlock(input_num=num_features + d_feature1 * 1, num1=d_feature0, num2=d_feature1,\n\t                                      dilation_rate=4)\n", "        self.ASPP_8 = _DenseAsppBlock(input_num=num_features + d_feature1 * 2, num1=d_feature0, num2=d_feature1,\n\t                                       dilation_rate=8)\n\t        self.afterASPP = nn.Sequential(\n\t            nn.Conv2d(in_channels=512*2 + 176*3, out_channels=512, kernel_size=1),)\n\t    def forward(self, encoder_fea):\n\t        imgAvgPool = self.AvgPool(encoder_fea)\n\t        aspp2 = self.ASPP_2(encoder_fea)\n\t        feature = torch.cat([aspp2, encoder_fea], dim=1)\n\t        aspp4 = self.ASPP_4(feature)\n\t        feature = torch.cat([aspp4, feature], dim=1)\n", "        aspp8 = self.ASPP_8(feature)\n\t        feature = torch.cat([aspp8, feature], dim=1)\n\t        asppFea = torch.cat([feature, imgAvgPool], dim=1)\n\t        AfterASPP = self.afterASPP(asppFea)\n\t        return AfterASPP\n\tclass CoSODNet(nn.Module):\n\t    def __init__(self, n_channels=3, mode='train'):\n\t        super(CoSODNet, self).__init__()\n\t        self.mode = mode\n\t        self.ImageBranchEncoder = ImageBranchEncoder(n_channels)\n", "        self.ImageBranch_fc7_1 = nn.Sequential(\n\t            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),\n\t            nn.BatchNorm2d(512),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.ImageBranch_DASPP = DASPPmodule()\n\t        self.poolingModule = poolingModule()\n\t        self.Transformer = Transformer(512)\n\t        self.bn_relu = nn.Sequential(\n\t            nn.BatchNorm1d(512),\n", "            nn.ReLU(inplace=True),\n\t        )\n\t        self.KernelModule = KernelModule()\n\t        self.ImageBranchDecoder = ImageBranchDecoder()\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_uniform_(m.weight),\n\t                nn.init.constant_(m.bias, 0),\n\t    def forward(self, image_Input):\n\t        if self.mode == 'train':\n", "            preds = self._train_forward(image_Input)\n\t        else:\n\t            preds = self._test_forward(image_Input)\n\t        return preds\n\t    def _train_forward(self, image_Input):\n\t        outputs_image = self._forward(image_Input)\n\t        return outputs_image\n\t    def _test_forward(self, image_Input):\n\t        with torch.no_grad():\n\t            outputs_image = self._forward(image_Input)\n", "            return outputs_image\n\t    def _forward(self, image_Input):\n\t        N, _, _, _ = image_Input.size()\n\t        image_feas = self.ImageBranchEncoder(image_Input)\n\t        afteraspp = self.ImageBranch_DASPP(self.ImageBranch_fc7_1(image_feas[-1]))\n\t        ####################### Consensus Feature Aggregation #######################\n\t        output = self.poolingModule(afteraspp)  # [N, C, 46]\n\t        Transformer_output, affinity = self.Transformer(output)\n\t        # Transformer_output [N, 46, C]   affinity (Nx46)x(Nx46)\n\t        Transformer_output = self.bn_relu(Transformer_output.permute(0, 2, 1))  # [N, C, 46]\n", "        _, cha, _ = output.size()\n\t        ####################### Consensus-aware Kernel Construction and Search #######################\n\t        kenerled_afteraspp = self.KernelModule(Transformer_output, afteraspp, affinity)\n\t        outputs_image = self.ImageBranchDecoder(image_feas, kenerled_afteraspp, Transformer_output, affinity)\n\t        return outputs_image\n\t    def init_parameters(self, pretrain_vgg16_1024):\n\t        conv_blocks = [self.ImageBranchEncoder.conv1,\n\t                       self.ImageBranchEncoder.conv2,\n\t                       self.ImageBranchEncoder.conv3,\n\t                       self.ImageBranchEncoder.conv4,\n", "                       self.ImageBranchEncoder.conv5,\n\t                       self.ImageBranchEncoder.fc6,\n\t                       self.ImageBranchEncoder.fc7]\n\t        listkey = [['conv1_1', 'conv1_2'], ['conv2_1', 'conv2_2'], ['conv3_1', 'conv3_2', 'conv3_3'],\n\t                   ['conv4_1', 'conv4_2', 'conv4_3'], ['conv5_1', 'conv5_2', 'conv5_3'], ['fc6'], ['fc7']]\n\t        for idx, conv_block in enumerate(conv_blocks):\n\t            num_conv = 0\n\t            for l2 in conv_block:\n\t                if isinstance(l2, nn.Conv2d):\n\t                    num_conv += 1\n", "                    l2.weight.data = pretrain_vgg16_1024[str(listkey[idx][num_conv - 1]) + '.weight']\n\t                    l2.bias.data = pretrain_vgg16_1024[str(listkey[idx][num_conv - 1]) + '.bias'].squeeze(0).squeeze(0).squeeze(0).squeeze(0)\n\t        return self\n"]}
{"filename": "CADC/Modules/Modules.py", "chunked_list": ["import torch.nn as nn\n\timport torch\n\timport torch.nn.functional as F\n\tclass KernelModule(nn.Module):\n\t    def __init__(self, channel=512):\n\t        super(KernelModule, self).__init__()\n\t        self.encoder_fea_channel = channel\n\t        self.self_att = nn.Sequential(\n\t            nn.Linear(46 * self.encoder_fea_channel, 1024),\n\t            nn.BatchNorm1d(1024),\n", "            nn.ReLU(inplace=True),\n\t            nn.Linear(1024, 46)\n\t        )\n\t        self.generate_depthwise_Adakernel = nn.Sequential(\n\t            nn.Linear(46, 46),\n\t            nn.BatchNorm1d(46),\n\t            nn.PReLU(num_parameters=1, init=0.1),\n\t            nn.Linear(46, 9)\n\t        )\n\t        self.generate_pointwise_Adakernel = nn.Sequential(\n", "            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel),\n\t            nn.BatchNorm1d(self.encoder_fea_channel),\n\t            nn.PReLU(num_parameters=1, init=0.1),\n\t            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel * 64)\n\t        )\n\t        self.att_depthwise_part1 = nn.Sequential(\n\t            nn.Linear(46 * self.encoder_fea_channel, 1024),\n\t            nn.BatchNorm1d(1024),\n\t            nn.ReLU(inplace=True),\n\t            nn.Linear(1024, self.encoder_fea_channel)\n", "        )\n\t        self.att_depthwise_part2 = nn.Sequential(\n\t            nn.Linear(46 * self.encoder_fea_channel, 1024),\n\t            nn.BatchNorm1d(1024),\n\t            nn.ReLU(inplace=True),\n\t            nn.Linear(1024, 46)\n\t        )\n\t        self.generate_depthwise_Cokernel = nn.Sequential(\n\t            nn.Linear(46, 46),\n\t            nn.BatchNorm1d(46),\n", "            nn.PReLU(num_parameters=1, init=0.1),\n\t            nn.Linear(46, 9)\n\t        )\n\t        self.generate_pointwise_Cokernel = nn.Sequential(\n\t            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel),\n\t            nn.PReLU(num_parameters=1, init=0.1),\n\t            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel * 64)\n\t        )\n\t    def forward(self, output, enc_fea, affinity):\n\t        N, cha, _ = output.size()\n", "        ##################### Adaptive Kernel Construction #####################\n\t        # [1] depthwise adaptive kernels\n\t        depthwise_Adakernel = self.generate_depthwise_Adakernel(output.view(N * cha, 46))\n\t        depthwise_Adakernel = depthwise_Adakernel.reshape(N, cha, 3, 3)\n\t        depthwise_Adakernel = depthwise_Adakernel.unsqueeze(2)\n\t        # [2] pointwise adaptive kernels\n\t        att = F.softmax(self.self_att(output.view(-1, 46 * cha)), dim=1)  # N x 46\n\t        att = att.unsqueeze(2) # N x 46 x 1\n\t        fea = ((output.permute(0, 2, 1)) * att).sum(dim=1).reshape(N, cha) # N x 512\n\t        pointwise_Adakernel = self.generate_pointwise_Adakernel(fea)\n", "        pointwise_Adakernel = pointwise_Adakernel.reshape(N, 64, cha, 1, 1)\n\t        ##################### Common Kernel Construction #####################\n\t        # [1] pointwise common kernel\n\t        affinity_att = affinity.mean(dim=0)  # [N*46]\n\t        fea_for_pointwise = torch.matmul(affinity_att.unsqueeze(0),\n\t                                         output.permute(0, 2, 1).contiguous().view(N * 46, cha))  # [1, C]\n\t        pointwise_Cokernel = self.generate_pointwise_Cokernel(fea_for_pointwise)\n\t        pointwise_Cokernel = pointwise_Cokernel.reshape(64, cha, 1, 1)\n\t        # [2] depthwise common kernel\n\t        att_part1_depthwise = self.att_depthwise_part1(output.permute(0, 2, 1).contiguous().view(N, 46 * cha))\n", "        att_part1_depthwise = F.softmax(att_part1_depthwise, dim=1) # alpha1 [N, C]\n\t        att_part2_depthwise = self.att_depthwise_part2(output.permute(0, 2, 1).contiguous().view(N, 46 * cha))\n\t        att_part2_depthwise = F.softmax(att_part2_depthwise, dim=1)  # alpha2 [N, 46]\n\t        att_for_depthwise = (output * att_part1_depthwise.unsqueeze(2)).sum(dim=1) # [N, 46]\n\t        att_for_depthwise = (att_for_depthwise * att_part2_depthwise).sum(dim=1) # [N]\n\t        att_for_depthwise = F.softmax(att_for_depthwise).unsqueeze(0)  # alpha3 [1 x N]\n\t        # att_for_depthwise [1 x N]   output [N, C, 46]\n\t        fea_for_depthwise = torch.matmul(att_for_depthwise, output.view(N, cha * 46))  # [1, C*46]\n\t        depthwise_Cokernel = self.generate_depthwise_Cokernel(fea_for_depthwise.reshape(cha, 46))\n\t        depthwise_Cokernel = depthwise_Cokernel.reshape(cha, 3, 3)\n", "        depthwise_Cokernel = depthwise_Cokernel.unsqueeze(1)\n\t        ##################### Searching via Adaptive Kernel #####################\n\t        _, _, H, W = enc_fea.size()\n\t        Adpkenerled_enc_fea = torch.cuda.FloatTensor(N, 64, H, W)\n\t        for num in range(N):\n\t            tmp_fea = F.conv2d(enc_fea[num, :, :, :].unsqueeze(0), depthwise_Adakernel[num, :, :, :, :], stride=1,\n\t                               padding=1, groups=cha)\n\t            Adpkenerled_enc_fea[num, :, :, :] = F.conv2d(tmp_fea, pointwise_Adakernel[num, :, :, :, :], stride=1,\n\t                                                           padding=0)\n\t        ##################### Searching via Common Kernel #####################\n", "        Cokenerled_enc_fea = F.conv2d(enc_fea, depthwise_Cokernel, stride=1, padding=1, groups=cha)\n\t        Cokenerled_enc_fea = F.conv2d(Cokenerled_enc_fea, pointwise_Cokernel, stride=1, padding=0)\n\t        kenerled_afteraspp = torch.cat([Adpkenerled_enc_fea, Cokenerled_enc_fea], dim=1)\n\t        return kenerled_afteraspp\n\tclass Transformer(nn.Module):\n\t    def __init__(self, in_channels):\n\t        super(Transformer, self).__init__()\n\t        self.in_channels = in_channels\n\t        self.inter_channels = self.in_channels // 2\n\t        self.bn_relu = nn.Sequential(\n", "            nn.BatchNorm1d(self.in_channels),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.theta = nn.Linear(self.in_channels, self.inter_channels)\n\t        self.phi = nn.Linear(self.in_channels, self.inter_channels)\n\t        self.g = nn.Linear(self.in_channels, self.inter_channels)\n\t        self.W = nn.Linear(self.inter_channels, self.in_channels)\n\t    def forward(self, ori_feature):\n\t        # ori_feature N x C x 46\n\t        feature = self.bn_relu(ori_feature)\n", "        feature = feature.permute(0, 2, 1)\n\t        # feature N x 46 x C\n\t        N, num, c = feature.size()\n\t        x_theta = self.theta(feature.contiguous().view(-1, c))\n\t        x_phi = self.phi(feature.contiguous().view(-1, c))\n\t        x_phi = x_phi.permute(1, 0)\n\t        attention = torch.matmul(x_theta, x_phi)\n\t        for k in range(N):\n\t            attention[k*46:k*46+46, k*46:k*46+46] = -1000\n\t        # (Nx46)x(Nx46)\n", "        f_div_C = F.softmax(attention, dim=-1)\n\t        g_x = self.g((feature.contiguous().view(-1, c)))\n\t        y = torch.matmul(f_div_C, g_x)\n\t        # (Nx46)xc/2\n\t        W_y = self.W(y).contiguous().view(N, num, c)\n\t        att_fea = ori_feature.permute(0, 2, 1) + W_y\n\t        return att_fea, f_div_C\n\tclass poolingModule(nn.Module):\n\t    def __init__(self):\n\t        super(poolingModule, self).__init__()\n", "        self.maxpool1 = torch.nn.AdaptiveMaxPool2d((1,1))\n\t        self.maxpool2 = torch.nn.AdaptiveMaxPool2d((3,3))\n\t        self.maxpool3 = torch.nn.AdaptiveMaxPool2d((6,6))\n\t    def forward(self, feature):\n\t        batch_size, cha, _, _ = feature.size()\n\t        maxpool_fea1 = self.maxpool1(feature).view(batch_size, cha, -1)\n\t        maxpool_fea2 = self.maxpool2(feature).view(batch_size, cha, -1)\n\t        maxpool_fea3 = self.maxpool3(feature).view(batch_size, cha, -1)\n\t        maxpool_fea = torch.cat([maxpool_fea1, maxpool_fea2, maxpool_fea3], dim=2)\n\t        return maxpool_fea\n"]}
{"filename": "CADC/Modules/__init__.py", "chunked_list": ["from .Modules import KernelModule\n\tfrom .Modules import Transformer\n\tfrom .Modules import poolingModule"]}
{"filename": "DCFM/util.py", "chunked_list": ["import logging\n\timport os\n\timport torch\n\timport shutil\n\tfrom torchvision import transforms\n\timport numpy as np\n\timport random\n\timport cv2\n\tclass Logger():\n\t    def __init__(self, path=\"log.txt\"):\n", "        self.logger = logging.getLogger('DCFM')\n\t        self.file_handler = logging.FileHandler(path, \"w\")\n\t        self.stdout_handler = logging.StreamHandler()\n\t        self.stdout_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n\t        self.file_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n\t        self.logger.addHandler(self.file_handler)\n\t        self.logger.addHandler(self.stdout_handler)\n\t        self.logger.setLevel(logging.INFO)\n\t        self.logger.propagate = False\n\t    def info(self, txt):\n", "        self.logger.info(txt)\n\t    def close(self):\n\t        self.file_handler.close()\n\t        self.stdout_handler.close()\n\tclass AverageMeter(object):\n\t    \"\"\"Computes and stores the average and current value\"\"\"\n\t    def __init__(self):\n\t        self.reset()\n\t    def reset(self):\n\t        self.val = 0.0\n", "        self.avg = 0.0\n\t        self.sum = 0.0\n\t        self.count = 0.0\n\t    def update(self, val, n=1):\n\t        self.val = val\n\t        self.sum += val * n\n\t        self.count += n\n\t        self.avg = self.sum / self.count\n\tdef save_checkpoint(state, path, filename=\"checkpoint.pth\"):\n\t    torch.save(state, os.path.join(path, filename))\n", "def save_tensor_img(tenor_im, path):\n\t    im = tenor_im.cpu().clone()\n\t    im = im.squeeze(0)\n\t    tensor2pil = transforms.ToPILImage()\n\t    im = tensor2pil(im)\n\t    im.save(path)\n\tdef save_tensor_merge(tenor_im, tensor_mask, path, colormap='HOT'):\n\t    im = tenor_im.cpu().detach().clone()\n\t    im = im.squeeze(0).numpy()\n\t    im = ((im - np.min(im)) / (np.max(im) - np.min(im) + 1e-20)) * 255\n", "    im = np.array(im,np.uint8)\n\t    mask = tensor_mask.cpu().detach().clone()\n\t    mask = mask.squeeze(0).numpy()\n\t    mask = ((mask - np.min(mask)) / (np.max(mask) - np.min(mask) + 1e-20)) * 255\n\t    mask = np.clip(mask, 0, 255)\n\t    mask = np.array(mask, np.uint8)\n\t    if colormap == 'HOT':\n\t        mask = cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_HOT)\n\t    elif colormap == 'PINK':\n\t        mask = cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_PINK)\n", "    elif colormap == 'BONE':\n\t        mask = cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_BONE)\n\t    # exec('cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_' + colormap+')')\n\t    im = im.transpose((1, 2, 0))\n\t    im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n\t    mix = cv2.addWeighted(im, 0.3, mask, 0.7, 0)\n\t    cv2.imwrite(path, mix)\n\tdef set_seed(seed):\n\t     torch.manual_seed(seed)\n\t     torch.cuda.manual_seed(seed)\n", "     torch.cuda.manual_seed_all(seed)\n\t     np.random.seed(seed)\n\t     random.seed(seed)\n\t     torch.backends.cudnn.deterministic = True\n\t     torch.backends.cudnn.benchmark = False\n"]}
{"filename": "DCFM/models/main.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom DCFM.models.vgg import VGG_Backbone\n\tfrom DCFM.util import *\n\tdef weights_init(module):\n\t    if isinstance(module, nn.Conv2d):\n\t        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n\t        if module.bias is not None:\n\t            nn.init.zeros_(module.bias)\n", "    elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n\t        nn.init.ones_(module.weight)\n\t        if module.bias is not None:\n\t            nn.init.zeros_(module.bias)\n\t    elif isinstance(module, nn.Linear):\n\t        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n\t        if module.bias is not None:\n\t            nn.init.zeros_(module.bias)\n\tclass EnLayer(nn.Module):\n\t    def __init__(self, in_channel=64):\n", "        super(EnLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        return x\n\tclass LatLayer(nn.Module):\n", "    def __init__(self, in_channel):\n\t        super(LatLayer, self).__init__()\n\t        self.convlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t    def forward(self, x):\n\t        x = self.convlayer(x)\n\t        return x\n", "class DSLayer(nn.Module):\n\t    def __init__(self, in_channel=64):\n\t        super(DSLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.predlayer = nn.Sequential(\n", "            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0))#, nn.Sigmoid())\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        x = self.predlayer(x)\n\t        return x\n\tclass half_DSLayer(nn.Module):\n\t    def __init__(self, in_channel=512):\n\t        super(half_DSLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, int(in_channel/4), kernel_size=3, stride=1, padding=1),\n", "            nn.ReLU(inplace=True),\n\t        )\n\t        self.predlayer = nn.Sequential(\n\t            nn.Conv2d(int(in_channel/4), 1, kernel_size=1, stride=1, padding=0)) #, nn.Sigmoid())\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        x = self.predlayer(x)\n\t        return x\n\tclass AugAttentionModule(nn.Module):\n\t    def __init__(self, input_channels=512):\n", "        super(AugAttentionModule, self).__init__()\n\t        self.query_transform = nn.Sequential(\n\t            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t        )\n\t        self.key_transform = nn.Sequential(\n\t            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t        )\n\t        self.value_transform = nn.Sequential(\n", "            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t        )\n\t        self.scale = 1.0 / (input_channels ** 0.5)\n\t        self.conv = nn.Sequential(\n\t            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t    def forward(self, x):\n\t        B, C, H, W = x.size()\n", "        x = self.conv(x)\n\t        x_query = self.query_transform(x).view(B, C, -1).permute(0, 2, 1)  # B,HW,C\n\t        # x_key: C,BHW\n\t        x_key = self.key_transform(x).view(B, C, -1)  # B, C,HW\n\t        # x_value: BHW, C\n\t        x_value = self.value_transform(x).view(B, C, -1).permute(0, 2, 1)  # B,HW,C\n\t        attention_bmm = torch.bmm(x_query, x_key)*self.scale # B, HW, HW\n\t        attention = F.softmax(attention_bmm, dim=-1)\n\t        attention_sort = torch.sort(attention_bmm, dim=-1, descending=True)[1]\n\t        attention_sort = torch.sort(attention_sort, dim=-1)[1]\n", "        #####\n\t        attention_positive_num = torch.ones_like(attention).cuda()\n\t        attention_positive_num[attention_bmm < 0] = 0\n\t        att_pos_mask = attention_positive_num.clone()\n\t        attention_positive_num = torch.sum(attention_positive_num, dim=-1, keepdim=True).expand_as(attention_sort)\n\t        attention_sort_pos = attention_sort.float().clone()\n\t        apn = attention_positive_num-1\n\t        attention_sort_pos[attention_sort > apn] = 0\n\t        attention_mask = ((attention_sort_pos+1)**3)*att_pos_mask + (1-att_pos_mask)\n\t        out = torch.bmm(attention*attention_mask, x_value)\n", "        out = out.view(B, H, W, C).permute(0, 3, 1, 2)\n\t        return out+x\n\tclass AttLayer(nn.Module):\n\t    def __init__(self, input_channels=512):\n\t        super(AttLayer, self).__init__()\n\t        self.query_transform = nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0)\n\t        self.key_transform = nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0)\n\t        self.scale = 1.0 / (input_channels ** 0.5)\n\t        self.conv = nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0)\n\t    def correlation(self, x5, seeds):\n", "        B, C, H5, W5 = x5.size()\n\t        if self.training:\n\t            correlation_maps = F.conv2d(x5, weight=seeds)  # B,B,H,W\n\t        else:\n\t            correlation_maps = torch.relu(F.conv2d(x5, weight=seeds))  # B,B,H,W\n\t        correlation_maps = correlation_maps.mean(1).view(B, -1)\n\t        min_value = torch.min(correlation_maps, dim=1, keepdim=True)[0]\n\t        max_value = torch.max(correlation_maps, dim=1, keepdim=True)[0]\n\t        correlation_maps = (correlation_maps - min_value) / (max_value - min_value + 1e-12)  # shape=[B, HW]\n\t        correlation_maps = correlation_maps.view(B, 1, H5, W5)  # shape=[B, 1, H, W]\n", "        return correlation_maps\n\t    def forward(self, x5):\n\t        # x: B,C,H,W\n\t        x5 = self.conv(x5)+x5\n\t        B, C, H5, W5 = x5.size()\n\t        x_query = self.query_transform(x5).view(B, C, -1)\n\t        # x_query: B,HW,C\n\t        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C)  # BHW, C\n\t        # x_key: B,C,HW\n\t        x_key = self.key_transform(x5).view(B, C, -1)\n", "        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1)  # C, BHW\n\t        # W = Q^T K: B,HW,HW\n\t        x_w1 = torch.matmul(x_query, x_key) * self.scale # BHW, BHW\n\t        x_w = x_w1.view(B * H5 * W5, B, H5 * W5)\n\t        x_w = torch.max(x_w, -1).values  # BHW, B\n\t        x_w = x_w.mean(-1)\n\t        x_w = x_w.view(B, -1)   # B, HW\n\t        x_w = F.softmax(x_w, dim=-1)  # B, HW\n\t        #####  mine ######\n\t        # x_w_max = torch.max(x_w, -1)\n", "        # max_indices0 = x_w_max.indices.unsqueeze(-1).unsqueeze(-1)\n\t        norm0 = F.normalize(x5, dim=1)\n\t        # norm = norm0.view(B, C, -1)\n\t        # max_indices = max_indices0.expand(B, C, -1)\n\t        # seeds = torch.gather(norm, 2, max_indices).unsqueeze(-1)\n\t        x_w = x_w.unsqueeze(1)\n\t        x_w_max = torch.max(x_w, -1).values.unsqueeze(2).expand_as(x_w)\n\t        mask = torch.zeros_like(x_w).cuda()\n\t        mask[x_w == x_w_max] = 1\n\t        mask = mask.view(B, 1, H5, W5)\n", "        seeds = norm0 * mask\n\t        seeds = seeds.sum(3).sum(2).unsqueeze(2).unsqueeze(3)\n\t        cormap = self.correlation(norm0, seeds)\n\t        x51 = x5 * cormap\n\t        proto1 = torch.mean(x51, (0, 2, 3), True)\n\t        return x5, proto1, x5*proto1+x51, mask\n\tclass Decoder(nn.Module):\n\t    def __init__(self):\n\t        super(Decoder, self).__init__()\n\t        self.toplayer = nn.Sequential(\n", "            nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0))\n\t        self.latlayer4 = LatLayer(in_channel=512)\n\t        self.latlayer3 = LatLayer(in_channel=256)\n\t        self.latlayer2 = LatLayer(in_channel=128)\n\t        self.latlayer1 = LatLayer(in_channel=64)\n\t        self.enlayer4 = EnLayer()\n\t        self.enlayer3 = EnLayer()\n\t        self.enlayer2 = EnLayer()\n", "        self.enlayer1 = EnLayer()\n\t        self.dslayer4 = DSLayer()\n\t        self.dslayer3 = DSLayer()\n\t        self.dslayer2 = DSLayer()\n\t        self.dslayer1 = DSLayer()\n\t    def _upsample_add(self, x, y):\n\t        [_, _, H, W] = y.size()\n\t        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)\n\t        return x + y\n\t    def forward(self, weighted_x5, x4, x3, x2, x1, H, W):\n", "        preds = []\n\t        p5 = self.toplayer(weighted_x5)\n\t        p4 = self._upsample_add(p5, self.latlayer4(x4))\n\t        p4 = self.enlayer4(p4)\n\t        _pred = self.dslayer4(p4)\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear', align_corners=False))\n\t        p3 = self._upsample_add(p4, self.latlayer3(x3))\n", "        p3 = self.enlayer3(p3)\n\t        _pred = self.dslayer3(p3)\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear', align_corners=False))\n\t        p2 = self._upsample_add(p3, self.latlayer2(x2))\n\t        p2 = self.enlayer2(p2)\n\t        _pred = self.dslayer2(p2)\n\t        preds.append(\n", "            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear', align_corners=False))\n\t        p1 = self._upsample_add(p2, self.latlayer1(x1))\n\t        p1 = self.enlayer1(p1)\n\t        _pred = self.dslayer1(p1)\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear', align_corners=False))\n", "        return preds\n\tclass DCFMNet(nn.Module):\n\t    \"\"\" Class for extracting activations and\n\t    registering gradients from targetted intermediate layers \"\"\"\n\t    def __init__(self, mode='train'):\n\t        super(DCFMNet, self).__init__()\n\t        self.gradients = None\n\t        self.backbone = VGG_Backbone()\n\t        self.mode = mode\n\t        self.aug = AugAttentionModule()\n", "        self.fusion = AttLayer(512)\n\t        self.decoder = Decoder()\n\t    def set_mode(self, mode):\n\t        self.mode = mode\n\t    def forward(self, x, gt):\n\t        if self.mode == 'train':\n\t            preds = self._forward(x, gt)\n\t        else:\n\t            with torch.no_grad():\n\t                preds = self._forward(x, gt)\n", "        return preds\n\t    def featextract(self, x):\n\t        x1 = self.backbone.conv1(x)\n\t        x2 = self.backbone.conv2(x1)\n\t        x3 = self.backbone.conv3(x2)\n\t        x4 = self.backbone.conv4(x3)\n\t        x5 = self.backbone.conv5(x4)\n\t        return x5, x4, x3, x2, x1\n\t    def _forward(self, x, gt):\n\t        [B, _, H, W] = x.size()\n", "        x5, x4, x3, x2, x1 = self.featextract(x)\n\t        feat, proto, weighted_x5, cormap = self.fusion(x5)\n\t        feataug = self.aug(weighted_x5)\n\t        preds = self.decoder(feataug, x4, x3, x2, x1, H, W)\n\t        if self.training:\n\t            gt = F.interpolate(gt, size=weighted_x5.size()[2:], mode='bilinear', align_corners=False)\n\t            feat_pos, proto_pos, weighted_x5_pos, cormap_pos = self.fusion(x5 * gt)\n\t            feat_neg, proto_neg, weighted_x5_neg, cormap_neg = self.fusion(x5*(1-gt))\n\t            return preds, proto, proto_pos, proto_neg\n\t        return preds\n", "class DCFM(nn.Module):\n\t    def __init__(self, mode='train'):\n\t        super(DCFM, self).__init__()\n\t        set_seed(123)\n\t        self.dcfmnet = DCFMNet()\n\t        self.mode = mode\n\t    def set_mode(self, mode):\n\t        self.mode = mode\n\t        self.dcfmnet.set_mode(self.mode)\n\t    def forward(self, x, gt=None):\n", "        ########## Co-SOD ############\n\t        preds = self.dcfmnet(x, gt)\n\t        return preds\n"]}
{"filename": "DCFM/models/vgg.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport os\n\tclass VGG_Backbone(nn.Module):\n\t    # VGG16 with two branches\n\t    # pooling layer at the front of block\n\t    def __init__(self):\n\t        super(VGG_Backbone, self).__init__()\n\t        conv1 = nn.Sequential()\n\t        conv1.add_module('conv1_1', nn.Conv2d(3, 64, 3, 1, 1))\n", "        conv1.add_module('relu1_1', nn.ReLU(inplace=True))\n\t        conv1.add_module('conv1_2', nn.Conv2d(64, 64, 3, 1, 1))\n\t        conv1.add_module('relu1_2', nn.ReLU(inplace=True))\n\t        self.conv1 = conv1\n\t        conv2 = nn.Sequential()\n\t        conv2.add_module('pool1', nn.MaxPool2d(2, stride=2))\n\t        conv2.add_module('conv2_1', nn.Conv2d(64, 128, 3, 1, 1))\n\t        conv2.add_module('relu2_1', nn.ReLU())\n\t        conv2.add_module('conv2_2', nn.Conv2d(128, 128, 3, 1, 1))\n\t        conv2.add_module('relu2_2', nn.ReLU())\n", "        self.conv2 = conv2\n\t        conv3 = nn.Sequential()\n\t        conv3.add_module('pool2', nn.MaxPool2d(2, stride=2))\n\t        conv3.add_module('conv3_1', nn.Conv2d(128, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_1', nn.ReLU())\n\t        conv3.add_module('conv3_2', nn.Conv2d(256, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_2', nn.ReLU())\n\t        conv3.add_module('conv3_3', nn.Conv2d(256, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_3', nn.ReLU())\n\t        self.conv3 = conv3\n", "        conv4 = nn.Sequential()\n\t        conv4.add_module('pool3', nn.MaxPool2d(2, stride=2))\n\t        conv4.add_module('conv4_1', nn.Conv2d(256, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_1', nn.ReLU())\n\t        conv4.add_module('conv4_2', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_2', nn.ReLU())\n\t        conv4.add_module('conv4_3', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_3', nn.ReLU())\n\t        self.conv4 = conv4\n\t        conv5 = nn.Sequential()\n", "        conv5.add_module('pool4', nn.MaxPool2d(2, stride=2))\n\t        conv5.add_module('conv5_1', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_1', nn.ReLU())\n\t        conv5.add_module('conv5_2', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_2', nn.ReLU())\n\t        conv5.add_module('conv5_3', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_3', nn.ReLU())\n\t        self.conv5 = conv5\n\t        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n\t        self.classifier = nn.Sequential(\n", "            nn.Linear(512 * 7 * 7, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 1000),\n\t        )\n\t        # pre_train = torch.load(os.path.dirname(__file__) + '/vgg16-397923af.pth')\n\t        # self._initialize_weights(pre_train)\n", "    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = self.conv2(x)\n\t        x = self.conv3(x)\n\t        x1 = self.conv4_1(x)\n\t        x1 = self.conv5_1(x1)\n\t        x1 = self.avgpool(x1)\n\t        _x1 = x1.view(x1.size(0), -1)\n\t        pred_vector = self.classifier(_x1)\n\t        x2 = self.conv4_2(x)\n", "        x2 = self.conv5_2(x2)\n\t        return x1, pred_vector, x2\n\t    def _initialize_weights(self, pre_train):\n\t        keys = list(pre_train.keys())\n\t        self.conv1.conv1_1.weight.data.copy_(pre_train[keys[0]])\n\t        self.conv1.conv1_2.weight.data.copy_(pre_train[keys[2]])\n\t        self.conv2.conv2_1.weight.data.copy_(pre_train[keys[4]])\n\t        self.conv2.conv2_2.weight.data.copy_(pre_train[keys[6]])\n\t        self.conv3.conv3_1.weight.data.copy_(pre_train[keys[8]])\n\t        self.conv3.conv3_2.weight.data.copy_(pre_train[keys[10]])\n", "        self.conv3.conv3_3.weight.data.copy_(pre_train[keys[12]])\n\t        self.conv4.conv4_1.weight.data.copy_(pre_train[keys[14]])\n\t        self.conv4.conv4_2.weight.data.copy_(pre_train[keys[16]])\n\t        self.conv4.conv4_3.weight.data.copy_(pre_train[keys[18]])\n\t        self.conv5.conv5_1.weight.data.copy_(pre_train[keys[20]])\n\t        self.conv5.conv5_2.weight.data.copy_(pre_train[keys[22]])\n\t        self.conv5.conv5_3.weight.data.copy_(pre_train[keys[24]])\n\t        self.conv1.conv1_1.bias.data.copy_(pre_train[keys[1]])\n\t        self.conv1.conv1_2.bias.data.copy_(pre_train[keys[3]])\n\t        self.conv2.conv2_1.bias.data.copy_(pre_train[keys[5]])\n", "        self.conv2.conv2_2.bias.data.copy_(pre_train[keys[7]])\n\t        self.conv3.conv3_1.bias.data.copy_(pre_train[keys[9]])\n\t        self.conv3.conv3_2.bias.data.copy_(pre_train[keys[11]])\n\t        self.conv3.conv3_3.bias.data.copy_(pre_train[keys[13]])\n\t        self.conv4.conv4_1.bias.data.copy_(pre_train[keys[15]])\n\t        self.conv4.conv4_2.bias.data.copy_(pre_train[keys[17]])\n\t        self.conv4.conv4_3.bias.data.copy_(pre_train[keys[19]])\n\t        self.conv5.conv5_1.bias.data.copy_(pre_train[keys[21]])\n\t        self.conv5.conv5_2.bias.data.copy_(pre_train[keys[23]])\n\t        self.conv5.conv5_3.bias.data.copy_(pre_train[keys[25]])\n", "        self.classifier[0].weight.data.copy_(pre_train[keys[26]])\n\t        self.classifier[0].bias.data.copy_(pre_train[keys[27]])\n\t        self.classifier[3].weight.data.copy_(pre_train[keys[28]])\n\t        self.classifier[3].bias.data.copy_(pre_train[keys[29]])\n\t        self.classifier[6].weight.data.copy_(pre_train[keys[30]])\n\t        self.classifier[6].bias.data.copy_(pre_train[keys[31]])\n"]}
{"filename": "GCoNet/models/vgg.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport os\n\tclass VGG_Backbone(nn.Module):\n\t    # VGG16 with two branches\n\t    # pooling layer at the front of block\n\t    def __init__(self):\n\t        super(VGG_Backbone, self).__init__()\n\t        conv1 = nn.Sequential()\n\t        conv1.add_module('conv1_1', nn.Conv2d(3, 64, 3, 1, 1))\n", "        conv1.add_module('relu1_1', nn.ReLU(inplace=True))\n\t        conv1.add_module('conv1_2', nn.Conv2d(64, 64, 3, 1, 1))\n\t        conv1.add_module('relu1_2', nn.ReLU(inplace=True))\n\t        self.conv1 = conv1\n\t        conv2 = nn.Sequential()\n\t        conv2.add_module('pool1', nn.MaxPool2d(2, stride=2))\n\t        conv2.add_module('conv2_1', nn.Conv2d(64, 128, 3, 1, 1))\n\t        conv2.add_module('relu2_1', nn.ReLU())\n\t        conv2.add_module('conv2_2', nn.Conv2d(128, 128, 3, 1, 1))\n\t        conv2.add_module('relu2_2', nn.ReLU())\n", "        self.conv2 = conv2\n\t        conv3 = nn.Sequential()\n\t        conv3.add_module('pool2', nn.MaxPool2d(2, stride=2))\n\t        conv3.add_module('conv3_1', nn.Conv2d(128, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_1', nn.ReLU())\n\t        conv3.add_module('conv3_2', nn.Conv2d(256, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_2', nn.ReLU())\n\t        conv3.add_module('conv3_3', nn.Conv2d(256, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_3', nn.ReLU())\n\t        self.conv3 = conv3\n", "        conv4 = nn.Sequential()\n\t        conv4.add_module('pool3', nn.MaxPool2d(2, stride=2))\n\t        conv4.add_module('conv4_1', nn.Conv2d(256, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_1', nn.ReLU())\n\t        conv4.add_module('conv4_2', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_2', nn.ReLU())\n\t        conv4.add_module('conv4_3', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_3', nn.ReLU())\n\t        self.conv4 = conv4\n\t        conv5 = nn.Sequential()\n", "        conv5.add_module('pool4', nn.MaxPool2d(2, stride=2))\n\t        conv5.add_module('conv5_1', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_1', nn.ReLU())\n\t        conv5.add_module('conv5_2', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_2', nn.ReLU())\n\t        conv5.add_module('conv5_3', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_3', nn.ReLU())\n\t        self.conv5 = conv5\n\t        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n\t        self.classifier = nn.Sequential(\n", "            nn.Linear(512 * 7 * 7, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 1000),\n\t        )\n\t        # pre_train = torch.load(os.path.dirname(__file__) + '/vgg16-397923af.pth')\n\t        # self._initialize_weights(pre_train)\n", "    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = self.conv2(x)\n\t        x = self.conv3(x)\n\t        x1 = self.conv4_1(x)\n\t        x1 = self.conv5_1(x1)\n\t        x1 = self.avgpool(x1)\n\t        _x1 = x1.view(x1.size(0), -1)\n\t        pred_vector = self.classifier(_x1)\n\t        x2 = self.conv4_2(x)\n", "        x2 = self.conv5_2(x2)\n\t        return x1, pred_vector, x2\n\t    def _initialize_weights(self, pre_train):\n\t        keys = list(pre_train.keys())\n\t        self.conv1.conv1_1.weight.data.copy_(pre_train[keys[0]])\n\t        self.conv1.conv1_2.weight.data.copy_(pre_train[keys[2]])\n\t        self.conv2.conv2_1.weight.data.copy_(pre_train[keys[4]])\n\t        self.conv2.conv2_2.weight.data.copy_(pre_train[keys[6]])\n\t        self.conv3.conv3_1.weight.data.copy_(pre_train[keys[8]])\n\t        self.conv3.conv3_2.weight.data.copy_(pre_train[keys[10]])\n", "        self.conv3.conv3_3.weight.data.copy_(pre_train[keys[12]])\n\t        self.conv4.conv4_1.weight.data.copy_(pre_train[keys[14]])\n\t        self.conv4.conv4_2.weight.data.copy_(pre_train[keys[16]])\n\t        self.conv4.conv4_3.weight.data.copy_(pre_train[keys[18]])\n\t        self.conv5.conv5_1.weight.data.copy_(pre_train[keys[20]])\n\t        self.conv5.conv5_2.weight.data.copy_(pre_train[keys[22]])\n\t        self.conv5.conv5_3.weight.data.copy_(pre_train[keys[24]])\n\t        self.conv1.conv1_1.bias.data.copy_(pre_train[keys[1]])\n\t        self.conv1.conv1_2.bias.data.copy_(pre_train[keys[3]])\n\t        self.conv2.conv2_1.bias.data.copy_(pre_train[keys[5]])\n", "        self.conv2.conv2_2.bias.data.copy_(pre_train[keys[7]])\n\t        self.conv3.conv3_1.bias.data.copy_(pre_train[keys[9]])\n\t        self.conv3.conv3_2.bias.data.copy_(pre_train[keys[11]])\n\t        self.conv3.conv3_3.bias.data.copy_(pre_train[keys[13]])\n\t        self.conv4.conv4_1.bias.data.copy_(pre_train[keys[15]])\n\t        self.conv4.conv4_2.bias.data.copy_(pre_train[keys[17]])\n\t        self.conv4.conv4_3.bias.data.copy_(pre_train[keys[19]])\n\t        self.conv5.conv5_1.bias.data.copy_(pre_train[keys[21]])\n\t        self.conv5.conv5_2.bias.data.copy_(pre_train[keys[23]])\n\t        self.conv5.conv5_3.bias.data.copy_(pre_train[keys[25]])\n", "        self.classifier[0].weight.data.copy_(pre_train[keys[26]])\n\t        self.classifier[0].bias.data.copy_(pre_train[keys[27]])\n\t        self.classifier[3].weight.data.copy_(pre_train[keys[28]])\n\t        self.classifier[3].bias.data.copy_(pre_train[keys[29]])\n\t        self.classifier[6].weight.data.copy_(pre_train[keys[30]])\n\t        self.classifier[6].bias.data.copy_(pre_train[keys[31]])\n"]}
{"filename": "GCoNet/models/GCoNet.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom GCoNet.models.vgg import VGG_Backbone\n\timport numpy as np\n\timport torch.optim as optim\n\tfrom torchvision.models import vgg16\n\t# import fvcore.nn.weight_init as weight_init\n\tclass EnLayer(nn.Module):\n\t    def __init__(self, in_channel=64):\n", "        super(EnLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        return x\n\tclass LatLayer(nn.Module):\n", "    def __init__(self, in_channel):\n\t        super(LatLayer, self).__init__()\n\t        self.convlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t    def forward(self, x):\n\t        x = self.convlayer(x)\n\t        return x\n", "class DSLayer(nn.Module):\n\t    def __init__(self, in_channel=64):\n\t        super(DSLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.predlayer = nn.Sequential(\n", "            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        x = self.predlayer(x)\n\t        return x\n\tclass half_DSLayer(nn.Module):\n\t    def __init__(self, in_channel=512):\n\t        super(half_DSLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, int(in_channel/4), kernel_size=3, stride=1, padding=1),\n", "            nn.ReLU(inplace=True),\n\t            #nn.Conv2d(int(in_channel/2), int(in_channel/4), kernel_size=3, stride=1, padding=1),\n\t            #nn.ReLU(inplace=True),\n\t        )\n\t        self.predlayer = nn.Sequential(\n\t            nn.Conv2d(int(in_channel/4), 1, kernel_size=1, stride=1, padding=0)) #, nn.Sigmoid())\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        x = self.predlayer(x)\n\t        return x\n", "class AllAttLayer(nn.Module):\n\t    def __init__(self, input_channels=512):\n\t        super(AllAttLayer, self).__init__()\n\t        self.query_transform = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\t        self.key_transform = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\t        self.scale = 1.0 / (input_channels ** 0.5)\n\t        self.conv6 = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\t        # for layer in [self.query_transform, self.key_transform, self.conv6]:\n\t        #     weight_init.c2_msra_fill(layer)\n\t    def forward(self, x5):\n", "        # x: B,C,H,W\n\t        # x_query: B,C,HW\n\t        B, C, H5, W5 = x5.size()\n\t        x_query = self.query_transform(x5).view(B, C, -1)\n\t        # x_query: B,HW,C\n\t        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C) # BHW, C\n\t        # x_key: B,C,HW\n\t        x_key = self.key_transform(x5).view(B, C, -1)\n\t        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1) # C, BHW\n\t        # W = Q^T K: B,HW,HW\n", "        x_w = torch.matmul(x_query, x_key) #* self.scale # BHW, BHW\n\t        x_w = x_w.view(B*H5*W5, B, H5*W5)\n\t        x_w = torch.max(x_w, -1).values # BHW, B\n\t        x_w = x_w.mean(-1)\n\t        #x_w = torch.mean(x_w, -1).values # BHW\n\t        x_w = x_w.view(B, -1) * self.scale # B, HW\n\t        x_w = F.softmax(x_w, dim=-1) # B, HW\n\t        x_w = x_w.view(B, H5, W5).unsqueeze(1) # B, 1, H, W\n\t        x5 = x5 * x_w\n\t        x5 = self.conv6(x5)\n", "        return x5\n\tclass CoAttLayer(nn.Module):\n\t    def __init__(self, input_channels=512):\n\t        super(CoAttLayer, self).__init__()\n\t        self.all_attention = AllAttLayer(input_channels)\n\t        self.conv_output = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\t        self.conv_transform = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\t        self.fc_transform = nn.Linear(input_channels, input_channels)\n\t        # for layer in [self.conv_output, self.conv_transform, self.fc_transform]:\n\t        #     weight_init.c2_msra_fill(layer)\n", "    def forward(self, x5):\n\t        if self.training:\n\t            f_begin = 0\n\t            f_end = int(x5.shape[0] / 2)\n\t            s_begin = f_end\n\t            s_end = int(x5.shape[0])\n\t            x5_1 = x5[f_begin: f_end]\n\t            x5_2 = x5[s_begin: s_end]\n\t            x5_new_1 = self.all_attention(x5_1)\n\t            x5_new_2 = self.all_attention(x5_2)\n", "            x5_1_proto = torch.mean(x5_new_1, (0, 2, 3), True).view(1, -1)\n\t            x5_1_proto = x5_1_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            x5_2_proto = torch.mean(x5_new_2, (0, 2, 3), True).view(1, -1)\n\t            x5_2_proto = x5_2_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            x5_11 = x5_1 * x5_1_proto\n\t            x5_22 = x5_2 * x5_2_proto\n\t            weighted_x5 = torch.cat([x5_11, x5_22], dim=0)\n\t            x5_12 = x5_1 * x5_2_proto\n\t            x5_21 = x5_2 * x5_1_proto\n\t            neg_x5 = torch.cat([x5_12, x5_21], dim=0)\n", "        else:\n\t            x5_new = self.all_attention(x5)\n\t            x5_proto = torch.mean(x5_new, (0, 2, 3), True).view(1, -1)\n\t            x5_proto = x5_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            weighted_x5 = x5 * x5_proto #* cweight\n\t            neg_x5 = None\n\t        return weighted_x5, neg_x5\n\tclass Conv2d(torch.nn.Conv2d):\n\t    \"\"\"\n\t    A wrapper around :class:`torch.nn.Conv2d` to support more features.\n", "    \"\"\"\n\t    def __init__(self, *args, **kwargs):\n\t        \"\"\"\n\t        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:\n\t        Args:\n\t            norm (nn.Module, optional): a normalization layer\n\t            activation (callable(Tensor) -> Tensor): a callable activation function\n\t        It assumes that norm layer is used before activation.\n\t        \"\"\"\n\t        norm = kwargs.pop(\"norm\", None)\n", "        activation = kwargs.pop(\"activation\", None)\n\t        super().__init__(*args, **kwargs)\n\t        self.norm = norm\n\t        self.activation = activation\n\t    def forward(self, x):\n\t        if x.numel() == 0 and self.training:\n\t            # https://github.com/pytorch/pytorch/issues/12013\n\t            assert not isinstance(\n\t                self.norm, torch.nn.SyncBatchNorm\n\t            ), \"SyncBatchNorm does not support empty inputs!\"\n", "        x = super().forward(x)\n\t        if self.norm is not None:\n\t            x = self.norm(x)\n\t        if self.activation is not None:\n\t            x = self.activation(x)\n\t        return x\n\tclass GINet(nn.Module):\n\t    \"\"\" Class for extracting activations and \n\t    registering gradients from targetted intermediate layers \"\"\"\n\t    def __init__(self, mode='train'):\n", "        super(GINet, self).__init__()\n\t        self.gradients = None\n\t        self.backbone = VGG_Backbone()\n\t        self.mode = mode\n\t        self.toplayer = nn.Sequential(\n\t            nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0))\n\t        self.latlayer4 = LatLayer(in_channel=512)\n\t        self.latlayer3 = LatLayer(in_channel=256)\n", "        self.latlayer2 = LatLayer(in_channel=128)\n\t        self.latlayer1 = LatLayer(in_channel=64)\n\t        self.enlayer4 = EnLayer()\n\t        self.enlayer3 = EnLayer()\n\t        self.enlayer2 = EnLayer()\n\t        self.enlayer1 = EnLayer()\n\t        self.dslayer4 = DSLayer()\n\t        self.dslayer3 = DSLayer()\n\t        self.dslayer2 = DSLayer()\n\t        self.dslayer1 = DSLayer()\n", "        self.pred_layer = half_DSLayer(512)\n\t        self.co_x5 = CoAttLayer()\n\t        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\t        self.classifier = nn.Linear(512, 291)\n\t        # for layer in [self.classifier]:\n\t        #     weight_init.c2_msra_fill(layer)\n\t    def set_mode(self, mode):\n\t        self.mode = mode\n\t    def _upsample_add(self, x, y):\n\t        [_, _, H, W] = y.size()\n", "        return F.interpolate(\n\t            x, size=(H, W), mode='bilinear', align_corners=True) + y\n\t    def _fg_att(self, feat, pred):\n\t        [_, _, H, W] = feat.size()\n\t        pred = F.interpolate(pred,\n\t                             size=(H, W),\n\t                             mode='bilinear',\n\t                             align_corners=True)\n\t        return feat * pred\n\t    def forward(self, x):\n", "        if self.mode == 'train':\n\t            preds = self._forward(x)\n\t        else:\n\t            with torch.no_grad():\n\t                preds = self._forward(x)\n\t        return preds\n\t    def _forward(self, x):\n\t        [_, _, H, W] = x.size()\n\t        x1 = self.backbone.conv1(x)\n\t        x2 = self.backbone.conv2(x1)\n", "        x3 = self.backbone.conv3(x2)\n\t        x4 = self.backbone.conv4(x3)\n\t        x5 = self.backbone.conv5(x4)\n\t        _x5 = self.avgpool(x5)\n\t        _x5 = _x5.view(_x5.size(0), -1)\n\t        pred_cls = self.classifier(_x5)\n\t        weighted_x5, neg_x5 = self.co_x5(x5)\n\t        cam = torch.mean(weighted_x5, dim=1).unsqueeze(1)\n\t        cam = cam.sigmoid()\n\t        if self.training:\n", "            ########## contrastive branch #########\n\t            cat_x5 = torch.cat([weighted_x5, neg_x5], dim=0)\n\t            pred_x5 = self.pred_layer(cat_x5)\n\t            pred_x5 = F.interpolate(pred_x5,\n\t                              size=(H, W),\n\t                              mode='bilinear',\n\t                              align_corners=True)\n\t        ########## Up-Sample ##########\n\t        preds = []\n\t        p5 = self.toplayer(weighted_x5)\n", "        _pred = cam\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear',\n\t                          align_corners=True))\n\t        p4 = self._upsample_add(p5, self.latlayer4(x4)) \n\t        p4 = self.enlayer4(p4)\n\t        _pred = self.dslayer4(p4)\n\t        preds.append(\n", "            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear',\n\t                          align_corners=True))\n\t        p3 = self._upsample_add(p4, self.latlayer3(x3)) \n\t        p3 = self.enlayer3(p3)\n\t        _pred = self.dslayer3(p3)\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n", "                          mode='bilinear',\n\t                          align_corners=True))\n\t        p2 = self._upsample_add(p3, self.latlayer2(x2)) \n\t        p2 = self.enlayer2(p2)\n\t        _pred = self.dslayer2(p2)\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear',\n\t                          align_corners=True))\n", "        p1 = self._upsample_add(p2, self.latlayer1(x1)) \n\t        p1 = self.enlayer1(p1)\n\t        _pred = self.dslayer1(p1)\n\t        preds.append(\n\t            F.interpolate(_pred,\n\t                          size=(H, W),\n\t                          mode='bilinear',\n\t                          align_corners=True))\n\t        if self.training:\n\t            return preds, pred_cls, pred_x5\n", "        else:\n\t            return preds\n\tclass GCoNet(nn.Module):\n\t    def __init__(self, mode='train'):\n\t        super(GCoNet, self).__init__()\n\t        self.co_classifier = vgg16(pretrained=False).eval()\n\t        self.ginet = GINet()\n\t        self.mode = mode\n\t    def set_mode(self, mode):\n\t        self.mode = mode\n", "        self.ginet.set_mode(self.mode)\n\t    def forward(self, x):\n\t        ########## Co-SOD ############\n\t        preds = self.ginet(x)\n\t        return preds\n"]}
{"filename": "GCoNet_plus/config.py", "chunked_list": ["import os\n\tclass Config():\n\t    def __init__(self) -> None:\n\t        # Backbone\n\t        self.bb = ['vgg16', 'vgg16bn', 'resnet50'][1]\n\t        # BN\n\t        self.use_bn = 'bn' in self.bb or 'resnet' in self.bb\n\t        # Augmentation\n\t        self.preproc_methods = ['flip', 'enhance', 'rotate', 'crop', 'pepper'][:3]\n\t        # Mask\n", "        losses = ['sal', 'cls', 'contrast', 'cls_mask']\n\t        self.loss = losses[:]\n\t        self.cls_mask_operation = ['x', '+', 'c'][0]\n\t        # Loss + Triplet Loss\n\t        self.lambdas_sal_last = {\n\t            # not 0 means opening this loss\n\t            # original rate -- 1 : 30 : 1.5 : 0.2, bce x 30\n\t            'bce': 30 * 1,          # high performance\n\t            'iou': 0.5 * 1,         # 0 / 255\n\t            'ssim': 1 * 0,          # help contours\n", "            'mse': 150 * 0,         # can smooth the saliency map\n\t            'reg': 100 * 0,\n\t            'triplet': 3 * 1 * ('cls' in self.loss),\n\t        }\n\t        # DB\n\t        self.db_output_decoder = True\n\t        self.db_k = 300\n\t        self.db_k_alpha = 1\n\t        self.split_mask = True and 'cls_mask' in self.loss\n\t        self.db_mask = False and self.split_mask\n", "        # Triplet Loss\n\t        self.triplet = ['_x5', 'mask'][:1]\n\t        self.triplet_loss_margin = 0.1\n\t        # Adv\n\t        self.lambda_adv = 0.        # turn to 0 to avoid adv training\n\t        # Refiner\n\t        self.refine = [0, 1, 4][0]         # 0 -- no refinement, 1 -- only output mask for refinement, 4 -- but also raw input.\n\t        if self.refine:\n\t            self.batch_size = 16\n\t        else:\n", "            if self.bb != 'vgg16':\n\t                self.batch_size = 26\n\t            else:\n\t                self.batch_size = 48\n\t        self.db_output_refiner = False and self.refine\n\t        # Intermediate Layers\n\t        self.lambdas_sal_others = {\n\t            'bce': 0,\n\t            'iou': 0.,\n\t            'ssim': 0,\n", "            'mse': 0,\n\t            'reg': 0,\n\t            'triplet': 0,\n\t        }\n\t        self.output_number = 1\n\t        self.loss_sal_layers = 4              # used to be last 4 layers\n\t        self.loss_cls_mask_last_layers = 1         # used to be last 4 layers\n\t        if 'keep in range':\n\t            self.loss_sal_layers = min(self.output_number, self.loss_sal_layers)\n\t            self.loss_cls_mask_last_layers = min(self.output_number, self.loss_cls_mask_last_layers)\n", "            self.output_number = min(self.output_number, max(self.loss_sal_layers, self.loss_cls_mask_last_layers))\n\t            if self.output_number == 1:\n\t                for cri in self.lambdas_sal_others:\n\t                    self.lambdas_sal_others[cri] = 0\n\t        self.conv_after_itp = False\n\t        self.complex_lateral_connection = False\n\t        # to control the quantitive level of each single loss by number of output branches.\n\t        self.loss_cls_mask_ratio_by_last_layers = 4 / self.loss_cls_mask_last_layers\n\t        for loss_sal in self.lambdas_sal_last.keys():\n\t            loss_sal_ratio_by_last_layers = 4 / (int(bool(self.lambdas_sal_others[loss_sal])) * (self.loss_sal_layers - 1) + 1)\n", "            self.lambdas_sal_last[loss_sal] *= loss_sal_ratio_by_last_layers\n\t            self.lambdas_sal_others[loss_sal] *= loss_sal_ratio_by_last_layers\n\t        self.lambda_cls_mask = 2.5 * self.loss_cls_mask_ratio_by_last_layers\n\t        self.lambda_cls = 3.\n\t        self.lambda_contrast = 250.\n\t        # Performance of GCoNet\n\t        self.val_measures = {\n\t            'Emax': {'CoCA': 0.760, 'CoSOD3k': 0.860, 'CoSal2015': 0.887},\n\t            'Smeasure': {'CoCA': 0.673, 'CoSOD3k': 0.802, 'CoSal2015': 0.845},\n\t            'Fmax': {'CoCA': 0.544, 'CoSOD3k': 0.777, 'CoSal2015': 0.847},\n", "        }\n\t        # others\n\t        self.GAM = True\n\t        if not self.GAM and 'contrast' in self.loss:\n\t            self.loss.remove('contrast')\n\t        self.lr = 1e-4 * (self.batch_size / 16)\n\t        self.relation_module = ['GAM', 'ICE', 'NonLocal', 'MHA'][0]\n\t        self.self_supervision = False\n\t        self.label_smoothing = False\n\t        self.freeze = True\n", "        self.validation = False\n\t        self.decay_step_size = 3000\n\t        self.rand_seed = 7\n\t        # run_sh_file = [f for f in os.listdir('.') if 'gco' in f and '.sh' in f] + [os.path.join('..', f) for f in os.listdir('..') if 'gco' in f and '.sh' in f]\n\t        # with open(run_sh_file[0], 'r') as f:\n\t        #     self.val_last = int([l.strip() for l in f.readlines() if 'val_last=' in l][0].split('=')[-1])"]}
{"filename": "GCoNet_plus/models/modules.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\t# import fvcore.nn.weight_init as weight_init\n\tfrom GCoNet_plus.config import Config\n\tconfig = Config()\n\tclass ResBlk(nn.Module):\n\t    def __init__(self, channel_in=64, channel_out=64):\n\t        super(ResBlk, self).__init__()\n", "        self.conv_in = nn.Conv2d(channel_in, 64, 3, 1, 1)\n\t        self.relu_in = nn.ReLU(inplace=True)\n\t        self.conv_out = nn.Conv2d(64, channel_out, 3, 1, 1)\n\t        if config.use_bn:\n\t            self.bn_in = nn.BatchNorm2d(64)\n\t            self.bn_out = nn.BatchNorm2d(channel_out)\n\t    def forward(self, x):\n\t        x = self.conv_in(x)\n\t        if config.use_bn:\n\t            x = self.bn_in(x)\n", "        x = self.relu_in(x)\n\t        x = self.conv_out(x)\n\t        if config.use_bn:\n\t            x = self.bn_out(x)\n\t        return x\n\tclass DSLayer(nn.Module):\n\t    def __init__(self, channel_in=64, channel_out=1, activation_out='relu'):\n\t        super(DSLayer, self).__init__()\n\t        self.activation_out = activation_out\n\t        self.conv1 = nn.Conv2d(channel_in, 64, kernel_size=3, stride=1, padding=1)\n", "        self.relu1 = nn.ReLU(inplace=True)\n\t        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n\t        self.relu2 = nn.ReLU(inplace=True)\n\t        if activation_out:\n\t            self.pred_conv = nn.Conv2d(64, channel_out, kernel_size=1, stride=1, padding=0)\n\t            self.pred_relu = nn.ReLU(inplace=True)\n\t        else:\n\t            self.pred_conv = nn.Conv2d(64, channel_out, kernel_size=1, stride=1, padding=0)\n\t        if config.use_bn:\n\t            self.bn1 = nn.BatchNorm2d(64)\n", "            self.bn2 = nn.BatchNorm2d(64)\n\t            self.pred_bn = nn.BatchNorm2d(channel_out)\n\t    def forward(self, x):\n\t        x = self.conv1(x)\n\t        if config.use_bn:\n\t            x = self.bn1(x)\n\t        x = self.relu1(x)\n\t        x = self.conv2(x)\n\t        if config.use_bn:\n\t            x = self.bn2(x)\n", "        x = self.relu2(x)\n\t        x = self.pred_conv(x)\n\t        if config.use_bn:\n\t            x = self.pred_bn(x)\n\t        if self.activation_out:\n\t            x = self.pred_relu(x)\n\t        return x\n\tclass half_DSLayer(nn.Module):\n\t    def __init__(self, channel_in=512):\n\t        super(half_DSLayer, self).__init__()\n", "        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(channel_in, int(channel_in//4), kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True)\n\t        )\n\t        self.predlayer = nn.Sequential(\n\t            nn.Conv2d(int(channel_in//4), 1, kernel_size=1, stride=1, padding=0),\n\t        )\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        x = self.predlayer(x)\n", "        return x\n\tclass CoAttLayer(nn.Module):\n\t    def __init__(self, channel_in=512):\n\t        super(CoAttLayer, self).__init__()\n\t        self.all_attention = eval(Config().relation_module + '(channel_in)')\n\t        self.conv_output = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        self.conv_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        self.fc_transform = nn.Linear(channel_in, channel_in)\n\t        # for layer in [self.conv_output, self.conv_transform, self.fc_transform]:\n\t        #     weight_init.c2_msra_fill(layer)\n", "    def forward(self, x5):\n\t        if self.training:\n\t            f_begin = 0\n\t            f_end = int(x5.shape[0] / 2)\n\t            s_begin = f_end\n\t            s_end = int(x5.shape[0])\n\t            x5_1 = x5[f_begin: f_end]\n\t            x5_2 = x5[s_begin: s_end]\n\t            x5_new_1 = self.all_attention(x5_1)\n\t            x5_new_2 = self.all_attention(x5_2)\n", "            x5_1_proto = torch.mean(x5_new_1, (0, 2, 3), True).view(1, -1)\n\t            x5_1_proto = x5_1_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            x5_2_proto = torch.mean(x5_new_2, (0, 2, 3), True).view(1, -1)\n\t            x5_2_proto = x5_2_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            x5_11 = x5_1 * x5_1_proto\n\t            x5_22 = x5_2 * x5_2_proto\n\t            weighted_x5 = torch.cat([x5_11, x5_22], dim=0)\n\t            x5_12 = x5_1 * x5_2_proto\n\t            x5_21 = x5_2 * x5_1_proto\n\t            neg_x5 = torch.cat([x5_12, x5_21], dim=0)\n", "        else:\n\t            x5_new = self.all_attention(x5)\n\t            x5_proto = torch.mean(x5_new, (0, 2, 3), True).view(1, -1)\n\t            x5_proto = x5_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\t            weighted_x5 = x5 * x5_proto #* cweight\n\t            neg_x5 = None\n\t        return weighted_x5, neg_x5\n\tclass ICE(nn.Module):\n\t    # The Integrity Channel Enhancement (ICE) module\n\t    # _X means in X-th column\n", "    def __init__(self, channel_in=512):\n\t        super(ICE, self).__init__()\n\t        self.conv_1 = nn.Conv2d(channel_in, channel_in, 3, 1, 1)\n\t        self.conv_2 = nn.Conv1d(channel_in, channel_in, 3, 1, 1)\n\t        self.conv_3 = nn.Conv2d(channel_in*3, channel_in, 3, 1, 1)\n\t        self.fc_2 = nn.Linear(channel_in, channel_in)\n\t        self.fc_3 = nn.Linear(channel_in, channel_in)\n\t    def forward(self, x):\n\t        x_1, x_2, x_3 = x, x, x\n\t        x_1 = x_1 * x_2 * x_3\n", "        x_2 = x_1 + x_2 + x_3\n\t        x_3 = torch.cat((x_1, x_2, x_3), dim=1)\n\t        V = self.conv_1(x_1)\n\t        bs, c, h, w = x_2.shape\n\t        K = self.conv_2(x_2.view(bs, c, h*w))\n\t        Q_prime = self.conv_3(x_3)\n\t        Q_prime = torch.norm(Q_prime, dim=(-2, -1)).view(bs, c, 1, 1)\n\t        Q_prime = Q_prime.view(bs, -1)\n\t        Q_prime = self.fc_3(Q_prime)\n\t        Q_prime = torch.softmax(Q_prime, dim=-1)\n", "        Q_prime = Q_prime.unsqueeze(1)\n\t        Q = torch.matmul(Q_prime, K)\n\t        x_2 = torch.nn.functional.cosine_similarity(K, Q, dim=-1)\n\t        x_2 = torch.sigmoid(x_2)\n\t        x_2 = self.fc_2(x_2)\n\t        x_2 = x_2.unsqueeze(-1).unsqueeze(-1)\n\t        x_1 = V * x_2 + V\n\t        return x_1\n\tclass GAM(nn.Module):\n\t    def __init__(self, channel_in=512):\n", "        super(GAM, self).__init__()\n\t        self.query_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        self.key_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        self.scale = 1.0 / (channel_in ** 0.5)\n\t        self.conv6 = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\t        # for layer in [self.query_transform, self.key_transform, self.conv6]:\n\t        #     weight_init.c2_msra_fill(layer)\n\t    def forward(self, x5):\n\t        # x: B,C,H,W\n\t        # x_query: B,C,HW\n", "        B, C, H5, W5 = x5.size()\n\t        x_query = self.query_transform(x5).view(B, C, -1)\n\t        # x_query: B,HW,C\n\t        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C) # BHW, C\n\t        # x_key: B,C,HW\n\t        x_key = self.key_transform(x5).view(B, C, -1)\n\t        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1) # C, BHW\n\t        # W = Q^T K: B,HW,HW\n\t        x_w = torch.matmul(x_query, x_key) #* self.scale # BHW, BHW\n\t        x_w = x_w.view(B*H5*W5, B, H5*W5)\n", "        x_w = torch.max(x_w, -1).values # BHW, B\n\t        x_w = x_w.mean(-1)\n\t        #x_w = torch.mean(x_w, -1).values # BHW\n\t        x_w = x_w.view(B, -1) * self.scale # B, HW\n\t        x_w = F.softmax(x_w, dim=-1) # B, HW\n\t        x_w = x_w.view(B, H5, W5).unsqueeze(1) # B, 1, H, W\n\t        x5 = x5 * x_w\n\t        x5 = self.conv6(x5)\n\t        return x5\n\tclass MHA(nn.Module):\n", "    '''\n\t    Scaled dot-product attention\n\t    '''\n\t    def __init__(self, d_model=512, d_k=512, d_v=512, h=8, dropout=.1, channel_in=512):\n\t        '''\n\t        :param d_model: Output dimensionality of the model\n\t        :param d_k: Dimensionality of queries and keys\n\t        :param d_v: Dimensionality of values\n\t        :param h: Number of heads\n\t        '''\n", "        super(MHA, self).__init__()\n\t        self.query_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0)\n\t        self.key_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0)\n\t        self.value_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0)\n\t        self.fc_q = nn.Linear(d_model, h * d_k)\n\t        self.fc_k = nn.Linear(d_model, h * d_k)\n\t        self.fc_v = nn.Linear(d_model, h * d_v)\n\t        self.fc_o = nn.Linear(h * d_v, d_model)\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.d_model = d_model\n", "        self.d_k = d_k\n\t        self.d_v = d_v\n\t        self.h = h\n\t        self.init_weights()\n\t    def init_weights(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n\t                if m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n", "            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.normal_(m.weight, std=0.001)\n\t                if m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n\t    def forward(self, x, attention_mask=None, attention_weights=None):\n\t        '''\n\t        Computes\n", "        :param queries: Queries (b_s, nq, d_model)\n\t        :param keys: Keys (b_s, nk, d_model)\n\t        :param values: Values (b_s, nk, d_model)\n\t        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n\t        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n\t        :return:\n\t        '''\n\t        B, C, H, W = x.size()\n\t        queries = self.query_transform(x).view(B, -1, C)\n\t        keys = self.query_transform(x).view(B, -1, C)\n", "        values = self.query_transform(x).view(B, -1, C)\n\t        b_s, nq = queries.shape[:2]\n\t        nk = keys.shape[1]\n\t        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n\t        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n\t        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n\t        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n\t        if attention_weights is not None:\n\t            att = att * attention_weights\n\t        if attention_mask is not None:\n", "            att = att.masked_fill(attention_mask, -np.inf)\n\t        att = torch.softmax(att, -1)\n\t        att = self.dropout(att)\n\t        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n\t        out = self.fc_o(out).view(B, C, H, W)  # (b_s, nq, d_model)\n\t        return out\n\tclass NonLocal(nn.Module):\n\t    def __init__(self, channel_in=512, inter_channels=None, dimension=2, sub_sample=True, bn_layer=True):\n\t        super(NonLocal, self).__init__()\n\t        assert dimension in [1, 2, 3]\n", "        self.dimension = dimension\n\t        self.sub_sample = sub_sample\n\t        self.channel_in = channel_in\n\t        self.inter_channels = inter_channels\n\t        if self.inter_channels is None:\n\t            self.inter_channels = channel_in // 2\n\t            if self.inter_channels == 0:\n\t                self.inter_channels = 1\n\t        self.g = nn.Conv2d(self.channel_in, self.inter_channels, 1, 1, 0)\n\t        if bn_layer:\n", "            self.W = nn.Sequential(\n\t                nn.Conv2d(self.inter_channels, self.channel_in, kernel_size=1, stride=1, padding=0),\n\t                nn.BatchNorm2d(self.channel_in)\n\t            )\n\t            nn.init.constant_(self.W[1].weight, 0)\n\t            nn.init.constant_(self.W[1].bias, 0)\n\t        else:\n\t            self.W = nn.Conv2d(self.inter_channels, self.channel_in, kernel_size=1, stride=1, padding=0)\n\t            nn.init.constant_(self.W.weight, 0)\n\t            nn.init.constant_(self.W.bias, 0)\n", "        self.theta = nn.Conv2d(self.channel_in, self.inter_channels, kernel_size=1, stride=1, padding=0)\n\t        self.phi = nn.Conv2d(self.channel_in, self.inter_channels, kernel_size=1, stride=1, padding=0)\n\t        if sub_sample:\n\t            self.g = nn.Sequential(self.g, nn.MaxPool2d(kernel_size=(2, 2)))\n\t            self.phi = nn.Sequential(self.phi, nn.MaxPool2d(kernel_size=(2, 2)))\n\t    def forward(self, x, return_nl_map=False):\n\t        \"\"\"\n\t        :param x: (b, c, t, h, w)\n\t        :param return_nl_map: if True return z, nl_map, else only return z.\n\t        :return:\n", "        \"\"\"\n\t        batch_size = x.size(0)\n\t        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n\t        g_x = g_x.permute(0, 2, 1)\n\t        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n\t        theta_x = theta_x.permute(0, 2, 1)\n\t        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n\t        f = torch.matmul(theta_x, phi_x)\n\t        f_div_C = F.softmax(f, dim=-1)\n\t        y = torch.matmul(f_div_C, g_x)\n", "        y = y.permute(0, 2, 1).contiguous()\n\t        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n\t        W_y = self.W(y)\n\t        z = W_y + x\n\t        if return_nl_map:\n\t            return z, f_div_C\n\t        return z\n\tclass DBHead(nn.Module):\n\t    def __init__(self, channel_in=32, channel_out=1, k=config.db_k):\n\t        super().__init__()\n", "        self.k = k\n\t        self.binarize = nn.Sequential(\n\t            nn.Conv2d(channel_in, channel_in, 3, 1, 1),\n\t            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n\t            nn.Conv2d(channel_in, channel_in, 3, 1, 1),\n\t            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n\t            nn.Conv2d(channel_in, channel_out, 1, 1, 0),\n\t            nn.Sigmoid()\n\t        )\n\t        self.thresh = nn.Sequential(\n", "            nn.Conv2d(channel_in, channel_in, 3, padding=1),\n\t            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n\t            nn.Conv2d(channel_in, channel_in, 3, 1, 1),\n\t            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n\t            nn.Conv2d(channel_in, channel_out, 1, 1, 0),\n\t            nn.Sigmoid()\n\t        )\n\t    def forward(self, x):\n\t        shrink_maps = self.binarize(x)\n\t        threshold_maps = self.thresh(x)\n", "        binary_maps = self.step_function(shrink_maps, threshold_maps)\n\t        return binary_maps\n\t    def step_function(self, x, y):\n\t        if config.db_k_alpha != 1:\n\t            z = x - y\n\t            mask_neg_inv = 1 - 2 * (z < 0)\n\t            a = torch.exp(-self.k * (torch.pow(z * mask_neg_inv + 1e-16, 1/config.k_alpha) * mask_neg_inv))\n\t        else:\n\t            a = torch.exp(-self.k * (x - y))\n\t        if torch.isinf(a).any():\n", "            a = torch.exp(-50 * (x - y))\n\t        return torch.reciprocal(1 + a)\n\tclass RefUnet(nn.Module):\n\t    # Refinement\n\t    def __init__(self, in_ch, inc_ch):\n\t        super(RefUnet, self).__init__()\n\t        self.conv0 = nn.Conv2d(in_ch, inc_ch, 3, padding=1)\n\t        self.conv1 = nn.Conv2d(inc_ch, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn1 = nn.BatchNorm2d(64)\n", "        self.relu1 = nn.ReLU(inplace=True)\n\t        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)\n\t        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn2 = nn.BatchNorm2d(64)\n\t        self.relu2 = nn.ReLU(inplace=True)\n\t        self.pool2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n\t        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn3 = nn.BatchNorm2d(64)\n", "        self.relu3 = nn.ReLU(inplace=True)\n\t        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n\t        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn4 = nn.BatchNorm2d(64)\n\t        self.relu4 = nn.ReLU(inplace=True)\n\t        self.pool4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n\t        #####\n\t        self.conv5 = nn.Conv2d(64, 64, 3, padding=1)\n\t        if config.use_bn:\n", "            self.bn5 = nn.BatchNorm2d(64)\n\t        self.relu5 = nn.ReLU(inplace=True)\n\t        #####\n\t        self.conv_d4 = nn.Conv2d(128, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn_d4 = nn.BatchNorm2d(64)\n\t        self.relu_d4 = nn.ReLU(inplace=True)\n\t        self.conv_d3 = nn.Conv2d(128, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn_d3 = nn.BatchNorm2d(64)\n", "        self.relu_d3 = nn.ReLU(inplace=True)\n\t        self.conv_d2 = nn.Conv2d(128, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn_d2 = nn.BatchNorm2d(64)\n\t        self.relu_d2 = nn.ReLU(inplace=True)\n\t        self.conv_d1 = nn.Conv2d(128, 64, 3, padding=1)\n\t        if config.use_bn:\n\t            self.bn_d1 = nn.BatchNorm2d(64)\n\t        self.relu_d1 = nn.ReLU(inplace=True)\n\t        self.conv_d0 = nn.Conv2d(64, 1, 3, padding=1)\n", "        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\t        if config.db_output_refiner:\n\t            self.db_output_refiner = DBHead(64)\n\t    def forward(self, x):\n\t        hx = x\n\t        hx = self.conv1(self.conv0(hx))\n\t        if config.use_bn:\n\t            hx = self.bn1(hx)\n\t        hx1 = self.relu1(hx)\n\t        hx = self.conv2(self.pool1(hx1))\n", "        if config.use_bn:\n\t            hx = self.bn2(hx)\n\t        hx2 = self.relu2(hx)\n\t        hx = self.conv3(self.pool2(hx2))\n\t        if config.use_bn:\n\t            hx = self.bn3(hx)\n\t        hx3 = self.relu3(hx)\n\t        hx = self.conv4(self.pool3(hx3))\n\t        if config.use_bn:\n\t            hx = self.bn4(hx)\n", "        hx4 = self.relu4(hx)\n\t        hx = self.conv5(self.pool4(hx4))\n\t        if config.use_bn:\n\t            hx = self.bn5(hx)\n\t        hx5 = self.relu5(hx)\n\t        hx = self.upscore2(hx5)\n\t        d4 = self.conv_d4(torch.cat((hx, hx4), 1))\n\t        if config.use_bn:\n\t            d4 = self.bn_d4(d4)\n\t        d4 = self.relu_d4(d4)\n", "        hx = self.upscore2(d4)\n\t        d3 = self.conv_d3(torch.cat((hx, hx3), 1))\n\t        if config.use_bn:\n\t            d3 = self.bn_d3(d3)\n\t        d3 = self.relu_d3(d3)\n\t        hx = self.upscore2(d3)\n\t        d2 = self.conv_d2(torch.cat((hx, hx2), 1))\n\t        if config.use_bn:\n\t            d2 = self.bn_d2(d2)\n\t        d2 = self.relu_d2(d2)\n", "        hx = self.upscore2(d2)\n\t        d1 = self.conv_d1(torch.cat((hx, hx1), 1))\n\t        if config.use_bn:\n\t            d1 = self.bn_d1(d1)\n\t        d1 = self.relu_d1(d1)\n\t        if config.db_output_refiner:\n\t            x = self.db_output_refiner(d1)\n\t        else:\n\t            residual = self.conv_d0(d1)\n\t            x = x + residual\n", "        return x\n"]}
{"filename": "GCoNet_plus/models/GCoNet.py", "chunked_list": ["from collections import OrderedDict\n\timport torch\n\tfrom torch.functional import norm\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torchvision.models import vgg16, vgg16_bn\n\t# import fvcore.nn.weight_init as weight_init\n\tfrom torchvision.models import resnet50\n\tfrom GCoNet_plus.models.modules import ResBlk, DSLayer, half_DSLayer, CoAttLayer, RefUnet, DBHead\n\tfrom GCoNet_plus.config import Config\n", "class GCoNet_plus(nn.Module):\n\t    def __init__(self):\n\t        super(GCoNet_plus, self).__init__()\n\t        self.config = Config()\n\t        bb = self.config.bb\n\t        if bb == 'vgg16':\n\t            bb_net = list(vgg16(pretrained=False).children())[0]\n\t            bb_convs = OrderedDict({\n\t                'conv1': bb_net[:4],\n\t                'conv2': bb_net[4:9],\n", "                'conv3': bb_net[9:16],\n\t                'conv4': bb_net[16:23],\n\t                'conv5': bb_net[23:30]\n\t            })\n\t            channel_scale = 1\n\t        elif bb == 'resnet50':\n\t            bb_net = list(resnet50(pretrained=False).children())\n\t            bb_convs = OrderedDict({\n\t                'conv1': nn.Sequential(*bb_net[0:3]),\n\t                'conv2': bb_net[4],\n", "                'conv3': bb_net[5],\n\t                'conv4': bb_net[6],\n\t                'conv5': bb_net[7]\n\t            })\n\t            channel_scale = 4\n\t        elif bb == 'vgg16bn':\n\t            bb_net = list(vgg16_bn(pretrained=False).children())[0]\n\t            bb_convs = OrderedDict({\n\t                'conv1': bb_net[:6],\n\t                'conv2': bb_net[6:13],\n", "                'conv3': bb_net[13:23],\n\t                'conv4': bb_net[23:33],\n\t                'conv5': bb_net[33:43]\n\t            })\n\t            channel_scale = 1\n\t        self.bb = nn.Sequential(bb_convs)\n\t        lateral_channels_in = [512, 512, 256, 128, 64] if 'vgg16' in bb else [2048, 1024, 512, 256, 64]\n\t        # channel_scale_latlayer = channel_scale // 2 if bb == 'resnet50' else 1\n\t        # channel_last = 32\n\t        ch_decoder = lateral_channels_in[0]//2//channel_scale\n", "        self.top_layer = ResBlk(lateral_channels_in[0], ch_decoder)\n\t        self.enlayer5 = ResBlk(ch_decoder, ch_decoder)\n\t        if self.config.conv_after_itp:\n\t            self.dslayer5 = DSLayer(ch_decoder, ch_decoder)\n\t        self.latlayer5 = ResBlk(lateral_channels_in[1], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[1], ch_decoder, 1, 1, 0)\n\t        ch_decoder //= 2\n\t        self.enlayer4 = ResBlk(ch_decoder*2, ch_decoder)\n\t        if self.config.conv_after_itp:\n\t            self.dslayer4 = DSLayer(ch_decoder, ch_decoder)\n\t        self.latlayer4 = ResBlk(lateral_channels_in[2], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[2], ch_decoder, 1, 1, 0)\n", "        if self.config.output_number >= 4:\n\t            self.conv_out4 = nn.Sequential(nn.Conv2d(ch_decoder, 32, 1, 1, 0), nn.ReLU(inplace=True), nn.Conv2d(32, 1, 1, 1, 0))\n\t        ch_decoder //= 2\n\t        self.enlayer3 = ResBlk(ch_decoder*2, ch_decoder)\n\t        if self.config.conv_after_itp:\n\t            self.dslayer3 = DSLayer(ch_decoder, ch_decoder)\n\t        self.latlayer3 = ResBlk(lateral_channels_in[3], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[3], ch_decoder, 1, 1, 0)\n\t        if self.config.output_number >= 3:\n\t            self.conv_out3 = nn.Sequential(nn.Conv2d(ch_decoder, 32, 1, 1, 0), nn.ReLU(inplace=True), nn.Conv2d(32, 1, 1, 1, 0))\n\t        ch_decoder //= 2\n", "        self.enlayer2 = ResBlk(ch_decoder*2, ch_decoder)\n\t        if self.config.conv_after_itp:\n\t            self.dslayer2 = DSLayer(ch_decoder, ch_decoder)\n\t        self.latlayer2 = ResBlk(lateral_channels_in[4], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[4], ch_decoder, 1, 1, 0)\n\t        if self.config.output_number >= 2:\n\t            self.conv_out2 = nn.Sequential(nn.Conv2d(ch_decoder, 32, 1, 1, 0), nn.ReLU(inplace=True), nn.Conv2d(32, 1, 1, 1, 0))\n\t        self.enlayer1 = ResBlk(ch_decoder, ch_decoder)\n\t        self.conv_out1 = nn.Sequential(nn.Conv2d(ch_decoder, 1, 1, 1, 0))\n\t        if self.config.GAM:\n\t            self.co_x5 = CoAttLayer(channel_in=lateral_channels_in[0])\n", "        if 'contrast' in self.config.loss:\n\t            self.pred_layer = half_DSLayer(lateral_channels_in[0])\n\t        if {'cls', 'cls_mask'} & set(self.config.loss):\n\t            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\t            self.classifier = nn.Linear(lateral_channels_in[0], 291)       # DUTS_class has 291 classes\n\t            # for layer in [self.classifier]:\n\t            #     weight_init.c2_msra_fill(layer)\n\t        if self.config.split_mask:\n\t            self.sgm = nn.Sigmoid()\n\t        if self.config.refine:\n", "            self.refiner = nn.Sequential(RefUnet(self.config.refine, 64))\n\t        if self.config.split_mask:\n\t            self.conv_out_mask = nn.Sequential(nn.Conv2d(ch_decoder, 1, 1, 1, 0))\n\t        if self.config.db_mask:\n\t            self.db_mask = DBHead(32)\n\t        if self.config.db_output_decoder:\n\t            self.db_output_decoder = DBHead(32)\n\t        if self.config.cls_mask_operation == 'c':\n\t            self.conv_cat_mask = nn.Conv2d(4, 3, 1, 1, 0)\n\t    def forward(self, x):\n", "        ########## Encoder ##########\n\t        [N, _, H, W] = x.size()\n\t        x1 = self.bb.conv1(x)\n\t        x2 = self.bb.conv2(x1)\n\t        x3 = self.bb.conv3(x2)\n\t        x4 = self.bb.conv4(x3)\n\t        x5 = self.bb.conv5(x4)\n\t        if 'cls' in self.config.loss:\n\t            _x5 = self.avgpool(x5)\n\t            _x5 = _x5.view(_x5.size(0), -1)\n", "            pred_cls = self.classifier(_x5)\n\t        if self.config.GAM:\n\t            weighted_x5, neg_x5 = self.co_x5(x5)\n\t            if 'contrast' in self.config.loss:\n\t                if self.training:\n\t                    ########## contrastive branch #########\n\t                    cat_x5 = torch.cat([weighted_x5, neg_x5], dim=0)\n\t                    pred_contrast = self.pred_layer(cat_x5)\n\t                    pred_contrast = F.interpolate(pred_contrast, size=(H, W), mode='bilinear', align_corners=True)\n\t            p5 = self.top_layer(weighted_x5)\n", "        else:\n\t            p5 = self.top_layer(x5)\n\t        ########## Decoder ##########\n\t        scaled_preds = []\n\t        p5 = self.enlayer5(p5)\n\t        p5 = F.interpolate(p5, size=x4.shape[2:], mode='bilinear', align_corners=True)\n\t        if self.config.conv_after_itp:\n\t            p5 = self.dslayer5(p5)\n\t        p4 = p5 + self.latlayer5(x4)\n\t        p4 = self.enlayer4(p4)\n", "        p4 = F.interpolate(p4, size=x3.shape[2:], mode='bilinear', align_corners=True)\n\t        if self.config.conv_after_itp:\n\t            p4 = self.dslayer4(p4)\n\t        if self.config.output_number >= 4:\n\t            p4_out = self.conv_out4(p4)\n\t            scaled_preds.append(p4_out)\n\t        p3 = p4 + self.latlayer4(x3)\n\t        p3 = self.enlayer3(p3)\n\t        p3 = F.interpolate(p3, size=x2.shape[2:], mode='bilinear', align_corners=True)\n\t        if self.config.conv_after_itp:\n", "            p3 = self.dslayer3(p3)\n\t        if self.config.output_number >= 3:\n\t            p3_out = self.conv_out3(p3)\n\t            scaled_preds.append(p3_out)\n\t        p2 = p3 + self.latlayer3(x2)\n\t        p2 = self.enlayer2(p2)\n\t        p2 = F.interpolate(p2, size=x1.shape[2:], mode='bilinear', align_corners=True)\n\t        if self.config.conv_after_itp:\n\t            p2 = self.dslayer2(p2)\n\t        if self.config.output_number >= 2:\n", "            p2_out = self.conv_out2(p2)\n\t            scaled_preds.append(p2_out)\n\t        p1 = p2 + self.latlayer2(x1)\n\t        p1 = self.enlayer1(p1)\n\t        p1 = F.interpolate(p1, size=x.shape[2:], mode='bilinear', align_corners=True)\n\t        if self.config.db_output_decoder:\n\t            p1_out = self.db_output_decoder(p1)\n\t        else:\n\t            p1_out = self.conv_out1(p1)\n\t        scaled_preds.append(p1_out)\n", "        if self.config.refine == 1:\n\t            scaled_preds.append(self.refiner(p1_out))\n\t        elif self.config.refine == 4:\n\t            scaled_preds.append(self.refiner(torch.cat([x, p1_out], dim=1)))\n\t        if 'cls_mask' in self.config.loss:\n\t            pred_cls_masks = []\n\t            norm_features_mask = []\n\t            input_features = [x, x1, x2, x3][:self.config.loss_cls_mask_last_layers]\n\t            bb_lst = [self.bb.conv1, self.bb.conv2, self.bb.conv3, self.bb.conv4, self.bb.conv5]\n\t            for idx_out in range(self.config.loss_cls_mask_last_layers):\n", "                if idx_out:\n\t                    mask_output = scaled_preds[-(idx_out+1+int(bool(self.config.refine)))]\n\t                else:\n\t                    if self.config.split_mask:\n\t                        if self.config.db_mask:\n\t                            mask_output = self.db_mask(p1)\n\t                        else:\n\t                            mask_output = self.sgm(self.conv_out_mask(p1))\n\t                if self.config.cls_mask_operation == 'x':\n\t                    masked_features = input_features[idx_out] * mask_output\n", "                elif self.config.cls_mask_operation == '+':\n\t                    masked_features = input_features[idx_out] + mask_output\n\t                elif self.config.cls_mask_operation == 'c':\n\t                    masked_features = self.conv_cat_mask(torch.cat((input_features[idx_out], mask_output), dim=1))\n\t                norm_feature_mask = self.avgpool(\n\t                    nn.Sequential(*bb_lst[idx_out:])(\n\t                        masked_features\n\t                    )\n\t                ).view(N, -1)\n\t                norm_features_mask.append(norm_feature_mask)\n", "                pred_cls_masks.append(\n\t                    self.classifier(\n\t                        norm_feature_mask\n\t                    )\n\t                )\n\t        if self.training:\n\t            return_values = []\n\t            if {'sal', 'cls', 'contrast', 'cls_mask'} == set(self.config.loss):\n\t                return_values = [scaled_preds, pred_cls, pred_contrast, pred_cls_masks]\n\t            elif {'sal', 'cls', 'contrast'} == set(self.config.loss):\n", "                return_values = [scaled_preds, pred_cls, pred_contrast]\n\t            elif {'sal', 'cls', 'cls_mask'} == set(self.config.loss):\n\t                return_values = [scaled_preds, pred_cls, pred_cls_masks]\n\t            elif {'sal', 'cls'} == set(self.config.loss):\n\t                return_values = [scaled_preds, pred_cls]\n\t            elif {'sal', 'contrast'} == set(self.config.loss):\n\t                return_values = [scaled_preds, pred_contrast]\n\t            elif {'sal', 'cls_mask'} == set(self.config.loss):\n\t                return_values = [scaled_preds, pred_cls_masks]\n\t            else:\n", "                return_values = [scaled_preds]\n\t            if self.config.lambdas_sal_last['triplet']:\n\t                norm_features = []\n\t                if '_x5' in self.config.triplet:\n\t                    norm_features.append(_x5)\n\t                if 'mask' in self.config.triplet:\n\t                    norm_features.append(norm_features_mask[0])\n\t                return_values.append(norm_features)\n\t            return return_values\n\t        else:\n", "            return scaled_preds\n"]}
{"filename": "gicd/models/__init__.py", "chunked_list": ["from .GICD import GICD"]}
{"filename": "gicd/models/vgg.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass VGG_Backbone(nn.Module):\n\t    def __init__(self):\n\t        super(VGG_Backbone, self).__init__()\n\t        conv1 = nn.Sequential()\n\t        conv1.add_module('conv1_1', nn.Conv2d(3, 64, 3, 1, 1))\n\t        conv1.add_module('relu1_1', nn.ReLU(inplace=True))\n\t        conv1.add_module('conv1_2', nn.Conv2d(64, 64, 3, 1, 1))\n\t        conv1.add_module('relu1_2', nn.ReLU(inplace=True))\n", "        self.conv1 = conv1\n\t        conv2 = nn.Sequential()\n\t        conv2.add_module('pool1', nn.MaxPool2d(2, stride=2))\n\t        conv2.add_module('conv2_1', nn.Conv2d(64, 128, 3, 1, 1))\n\t        conv2.add_module('relu2_1', nn.ReLU())\n\t        conv2.add_module('conv2_2', nn.Conv2d(128, 128, 3, 1, 1))\n\t        conv2.add_module('relu2_2', nn.ReLU())\n\t        self.conv2 = conv2\n\t        conv3 = nn.Sequential()\n\t        conv3.add_module('pool2', nn.MaxPool2d(2, stride=2))\n", "        conv3.add_module('conv3_1', nn.Conv2d(128, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_1', nn.ReLU())\n\t        conv3.add_module('conv3_2', nn.Conv2d(256, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_2', nn.ReLU())\n\t        conv3.add_module('conv3_3', nn.Conv2d(256, 256, 3, 1, 1))\n\t        conv3.add_module('relu3_3', nn.ReLU())\n\t        self.conv3 = conv3\n\t        conv4 = nn.Sequential()\n\t        conv4.add_module('pool3', nn.MaxPool2d(2, stride=2))\n\t        conv4.add_module('conv4_1', nn.Conv2d(256, 512, 3, 1, 1))\n", "        conv4.add_module('relu4_1', nn.ReLU())\n\t        conv4.add_module('conv4_2', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_2', nn.ReLU())\n\t        conv4.add_module('conv4_3', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv4.add_module('relu4_3', nn.ReLU())\n\t        self.conv4 = conv4\n\t        conv5 = nn.Sequential()\n\t        conv5.add_module('pool4', nn.MaxPool2d(2, stride=2))\n\t        conv5.add_module('conv5_1', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_1', nn.ReLU())\n", "        conv5.add_module('conv5_2', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_2', nn.ReLU())\n\t        conv5.add_module('conv5_3', nn.Conv2d(512, 512, 3, 1, 1))\n\t        conv5.add_module('relu5_3', nn.ReLU())\n\t        self.conv5 = conv5\n\t        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n\t        self.classifier = nn.Sequential(\n\t            nn.Linear(512 * 7 * 7, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n", "            nn.Linear(4096, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 1000),\n\t        )\n\t    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = self.conv2(x)\n\t        x = self.conv3(x)\n\t        x1 = self.conv4_1(x)\n", "        x1 = self.conv5_1(x1)\n\t        x1 = self.avgpool(x1)\n\t        _x1 = x1.view(x1.size(0), -1)\n\t        pred_vector = self.classifier(_x1)\n\t        x2 = self.conv4_2(x)\n\t        x2 = self.conv5_2(x2)\n\t        return x1, pred_vector, x2\n"]}
{"filename": "gicd/models/GICD.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom gicd.models.vgg import VGG_Backbone\n\timport numpy as np\n\timport torch.optim as optim\n\tfrom torchvision.models import vgg16\n\tclass EnLayer(nn.Module):\n\t    def __init__(self, in_channel=64):\n\t        super(EnLayer, self).__init__()\n", "        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        return x\n\tclass LatLayer(nn.Module):\n\t    def __init__(self, in_channel):\n", "        super(LatLayer, self).__init__()\n\t        self.convlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t        )\n\t    def forward(self, x):\n\t        x = self.convlayer(x)\n\t        return x\n\tclass DSLayer(nn.Module):\n", "    def __init__(self, in_channel=64):\n\t        super(DSLayer, self).__init__()\n\t        self.enlayer = nn.Sequential(\n\t            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n\t            nn.ReLU(inplace=True),\n\t        )\n\t        self.predlayer = nn.Sequential(\n\t            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n", "    def forward(self, x):\n\t        x = self.enlayer(x)\n\t        x = self.predlayer(x)\n\t        return x\n\tclass GINet(nn.Module):\n\t    def __init__(self, mode='train'):\n\t        super(GINet, self).__init__()\n\t        self.gradients = None\n\t        self.backbone = VGG_Backbone()\n\t        self.toplayer = nn.Sequential(\n", "            nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0))\n\t        self.latlayer4 = LatLayer(in_channel=512)\n\t        self.latlayer3 = LatLayer(in_channel=256)\n\t        self.latlayer2 = LatLayer(in_channel=128)\n\t        self.latlayer1 = LatLayer(in_channel=64)\n\t        self.enlayer4 = EnLayer()\n\t        self.enlayer3 = EnLayer()\n\t        self.enlayer2 = EnLayer()\n", "        self.enlayer1 = EnLayer()\n\t        self.dslayer4 = DSLayer()\n\t        self.dslayer3 = DSLayer()\n\t        self.dslayer2 = DSLayer()\n\t        self.dslayer1 = DSLayer()\n\t    def save_gradient(self, grad):\n\t        self.gradients = grad\n\t    def _upsample_add(self, x, y):\n\t        [_, _, H, W] = y.size()\n\t        return F.interpolate(\n", "            x, size=(H, W), mode='bilinear', align_corners=True) + y\n\t    def _fg_att(self, feat, pred):\n\t        [_, _, H, W] = feat.size()\n\t        pred = F.interpolate(pred,\n\t                             size=(H, W),\n\t                             mode='bilinear',\n\t                             align_corners=True)\n\t        return feat * pred\n\t    def forward(self, x, co_coding):\n\t        [_, _, H, W] = x.size()\n", "        with torch.no_grad():\n\t            x1 = self.backbone.conv1(x)\n\t            x2 = self.backbone.conv2(x1)\n\t            x3 = self.backbone.conv3(x2)\n\t            x4 = self.backbone.conv4(x3)\n\t        x5 = self.backbone.conv5(x4)\n\t        x5.requires_grad_()\n\t        x5.register_hook(self.save_gradient)\n\t        x5_p = self.backbone.avgpool(x5)\n\t        _x5_p = x5_p.view(x5_p.size(0), -1)\n", "        pred_vector = self.backbone.classifier(_x5_p)\n\t        co_coding = co_coding.requires_grad_()\n\t        similarity = torch.sum(co_coding.cuda() * pred_vector)\n\t        similarity.backward(retain_graph=True)\n\t        cweight = F.adaptive_avg_pool2d(self.gradients, (1, 1))\n\t        cweight = F.relu(cweight)\n\t        cweight = (cweight - torch.min(cweight)) / (torch.max(cweight) -\n\t                                                    torch.min(cweight) + 1e-20)\n\t        weighted_x5 = x5 * cweight\n\t        cam = torch.mean(weighted_x5, dim=1).unsqueeze(1)\n", "        cam = torch.relu(cam)\n\t        cam = cam - torch.min(cam)\n\t        cam = cam / (torch.max(cam) + 1e-6)\n\t        cam = torch.clamp(cam, 0, 1)\n\t        with torch.no_grad():\n\t            ########## Up-Sample ##########\n\t            preds = []\n\t            p5 = self.toplayer(weighted_x5)\n\t            _pred = cam\n\t            preds.append(\n", "                F.interpolate(_pred,\n\t                              size=(H, W),\n\t                              mode='bilinear',\n\t                              align_corners=True))\n\t            p4 = self._upsample_add(p5, self.latlayer4(self._fg_att(x4,\n\t                                                                    _pred)))\n\t            p4 = self.enlayer4(p4)\n\t            _pred = self.dslayer4(p4)\n\t            preds.append(\n\t                F.interpolate(_pred,\n", "                              size=(H, W),\n\t                              mode='bilinear',\n\t                              align_corners=True))\n\t            p3 = self._upsample_add(p4, self.latlayer3(self._fg_att(x3,\n\t                                                                    _pred)))\n\t            p3 = self.enlayer3(p3)\n\t            _pred = self.dslayer3(p3)\n\t            preds.append(\n\t                F.interpolate(_pred,\n\t                              size=(H, W),\n", "                              mode='bilinear',\n\t                              align_corners=True))\n\t            p2 = self._upsample_add(p3, self.latlayer2(self._fg_att(x2,\n\t                                                                    _pred)))\n\t            p2 = self.enlayer2(p2)\n\t            _pred = self.dslayer2(p2)\n\t            preds.append(\n\t                F.interpolate(_pred,\n\t                              size=(H, W),\n\t                              mode='bilinear',\n", "                              align_corners=True))\n\t            p1 = self._upsample_add(p2, self.latlayer1(self._fg_att(x1,\n\t                                                                    _pred)))\n\t            p1 = self.enlayer1(p1)\n\t            _pred = self.dslayer1(p1)\n\t            preds.append(\n\t                F.interpolate(_pred,\n\t                              size=(H, W),\n\t                              mode='bilinear',\n\t                              align_corners=True))\n", "        return preds\n\tclass GICD(nn.Module):\n\t    def __init__(self, mode='test'):\n\t        super(GICD, self).__init__()\n\t        self.co_classifier = vgg16(pretrained=0).eval()\n\t        self.ginet = GINet()\n\t    def forward(self, x):\n\t        x = x.unsqueeze(0)\n\t        [_, N, _, _, _] = x.size()\n\t        with torch.no_grad():\n", "            ######### Co-Classify ########\n\t            co_coding = 0\n\t            for inum in range(N):\n\t                co_coding += self.co_classifier(\n\t                    x[:, inum, :, :, :]).cpu().data.numpy()\n\t            co_coding = torch.from_numpy(co_coding)\n\t            co_coding = F.softmax(co_coding, dim=1)  # dim\n\t        ########## Co-SOD ############\n\t        preds = []\n\t        for inum in range(N):\n", "            ipreds = self.ginet(x[:, inum, :, :, :], co_coding)\n\t            preds.append(ipreds)\n\t        return preds\n"]}
{"filename": "ICNet/ICNet/network.py", "chunked_list": ["import torch\n\timport time\n\timport numpy as np\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom torch.nn import init\n\tfrom os.path import join\n\tnp.set_printoptions(suppress=True, threshold=1e5)\n\t\"\"\"\n\tresize:\n", "    Resize tensor (shape=[N, C, H, W]) to the target size (default: 224*224).\n\t\"\"\"\n\tdef resize(input, target_size=(224, 224)):\n\t    return F.interpolate(input, (target_size[0], target_size[1]), mode='bilinear', align_corners=True)\n\t\"\"\"\n\tweights_init:\n\t    Weights initialization.\n\t\"\"\"\n\tdef weights_init(module):\n\t    if isinstance(module, nn.Conv2d):\n", "        init.normal_(module.weight, 0, 0.01)\n\t        if module.bias is not None:\n\t            init.constant_(module.bias, 0)\n\t    elif isinstance(module, nn.BatchNorm2d):\n\t        init.constant_(module.weight, 1)\n\t        init.constant_(module.bias, 0)\n\t\"\"\"\"\n\tVGG16:\n\t    VGG16 backbone.\n\t\"\"\" \n", "class VGG16(nn.Module):\n\t    def __init__(self):\n\t        super(VGG16, self).__init__()\n\t        layers = []\n\t        in_channel = 3\n\t        vgg_out_channels = (64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M')\n\t        for out_channel in vgg_out_channels:\n\t            if out_channel == 'M':\n\t                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n\t            else:\n", "                conv2d = nn.Conv2d(in_channel, out_channel, 3, 1, 1)\n\t                layers += [conv2d, nn.ReLU(inplace=True)]\n\t                in_channel = out_channel\n\t        self.vgg = nn.ModuleList(layers)\n\t        self.table = {'conv1_1': 0, 'conv1_2': 2, 'conv1_2_mp': 4,\n\t                      'conv2_1': 5, 'conv2_2': 7, 'conv2_2_mp': 9,\n\t                      'conv3_1': 10, 'conv3_2': 12, 'conv3_3': 14, 'conv3_3_mp': 16,\n\t                      'conv4_1': 17, 'conv4_2': 19, 'conv4_3': 21, 'conv4_3_mp': 23,\n\t                      'conv5_1': 24, 'conv5_2': 26, 'conv5_3': 28, 'conv5_3_mp': 30, 'final': 31}\n\t    def forward(self, feats, start_layer_name, end_layer_name):\n", "        start_idx = self.table[start_layer_name]\n\t        end_idx = self.table[end_layer_name]\n\t        for idx in range(start_idx, end_idx):\n\t            feats = self.vgg[idx](feats)\n\t        return feats\n\t\"\"\"\n\tPrediction:\n\t    Compress the channel of input features to 1, then predict maps with sigmoid function.\n\t\"\"\"\n\tclass Prediction(nn.Module):\n", "    def __init__(self, in_channel):\n\t        super(Prediction, self).__init__()\n\t        self.pred = nn.Sequential(nn.Conv2d(in_channel, 1, 1), nn.Sigmoid())\n\t    def forward(self, feats):\n\t        pred = self.pred(feats)\n\t        return pred\n\t\"\"\"\n\tRes:\n\t    Two convolutional layers with residual structure.\n\t\"\"\"\n", "class Res(nn.Module):\n\t    def __init__(self, in_channel):\n\t        super(Res, self).__init__()\n\t        self.conv = nn.Sequential(nn.Conv2d(in_channel, in_channel, 3, 1, 1), \n\t                                  nn.BatchNorm2d(in_channel), nn.ReLU(inplace=True),\n\t                                  nn.Conv2d(in_channel, in_channel, 3, 1, 1))\n\t    def forward(self, feats):\n\t        feats = feats + self.conv(feats)\n\t        feats = F.relu(feats, inplace=True)\n\t        return feats\n", "\"\"\"\n\tCosal_Module:\n\t    Given features extracted from the VGG16 backbone,\n\t    exploit SISMs to build intra cues and inter cues.\n\t\"\"\"\n\tclass Cosal_Module(nn.Module):\n\t    def __init__(self, H, W):\n\t        super(Cosal_Module, self).__init__()\n\t        self.cosal_feat = Cosal_Sub_Module(H, W)\n\t        self.conv = nn.Sequential(nn.Conv2d(256, 128, 1), Res(128))\n", "    def forward(self, feats, SISMs):\n\t        # Get foreground co-saliency features.\n\t        fore_cosal_feats = self.cosal_feat(feats, SISMs)\n\t        # Get background co-saliency features.\n\t        back_cosal_feats = self.cosal_feat(feats, 1.0 - SISMs)\n\t        # Fuse foreground and background co-saliency features\n\t        # to generate co-saliency enhanced features.\n\t        cosal_enhanced_feats = self.conv(torch.cat([fore_cosal_feats, back_cosal_feats], dim=1))\n\t        return cosal_enhanced_feats\n\t\"\"\"\n", "Cosal_Sub_Module:\n\t  * The kernel module of ICNet.\n\t    Generate foreground/background co-salient features by using SISMs.\n\t\"\"\"\n\tclass Cosal_Sub_Module(nn.Module):\n\t    def __init__(self, H, W):\n\t        super(Cosal_Sub_Module, self).__init__()\n\t        channel = H * W\n\t        self.conv = nn.Sequential(nn.Conv2d(channel, 128, 1), Res(128))\n\t    def forward(self, feats, SISMs):\n", "        N, C, H, W = feats.shape\n\t        HW = H * W\n\t        # Resize SISMs to the same size as the input feats.\n\t        SISMs = resize(SISMs, [H, W])  # shape=[N, 1, H, W]\n\t        # NFs: L2-normalized features.\n\t        NFs = F.normalize(feats, dim=1)  # shape=[N, C, H, W]\n\t        def CFM(SIVs, NFs):\n\t            # Compute correlation maps [Figure 4] between SIVs and pixel-wise feature vectors in NFs by inner product.\n\t            # We implement this process by ``F.conv2d()'', which takes SIVs as 1*1 kernels to convolve NFs.\n\t            correlation_maps = F.conv2d(NFs, weight=SIVs)  # shape=[N, N, H, W]\n", "            # Vectorize and normalize correlation maps.\n\t            correlation_maps = F.normalize(correlation_maps.reshape(N, N, HW), dim=2)  # shape=[N, N, HW]\n\t            # Compute the weight vectors [Equation 2].\n\t            correlation_matrix = torch.matmul(correlation_maps, correlation_maps.permute(0, 2, 1))  # shape=[N, N, N]\n\t            weight_vectors = correlation_matrix.sum(dim=2).softmax(dim=1)  # shape=[N, N]\n\t            # Fuse correlation maps with the weight vectors to build co-salient attention (CSA) maps.\n\t            CSA_maps = torch.sum(correlation_maps * weight_vectors.view(N, N, 1), dim=1)  # shape=[N, HW]\n\t            # Max-min normalize CSA maps.\n\t            min_value = torch.min(CSA_maps, dim=1, keepdim=True)[0]\n\t            max_value = torch.max(CSA_maps, dim=1, keepdim=True)[0]\n", "            CSA_maps = (CSA_maps - min_value) / (max_value - min_value + 1e-12)  # shape=[N, HW]\n\t            CSA_maps = CSA_maps.view(N, 1, H, W)  # shape=[N, 1, H, W]\n\t            return CSA_maps\n\t        def get_SCFs(NFs):\n\t            NFs = NFs.view(N, C, HW)  # shape=[N, C, HW]\n\t            SCFs = torch.matmul(NFs.permute(0, 2, 1), NFs).view(N, -1, H, W)  # shape=[N, HW, H, W]\n\t            return SCFs\n\t        # Compute SIVs [Section 3.2, Equation 1].\n\t        SIVs = F.normalize((NFs * SISMs).mean(dim=3).mean(dim=2), dim=1).view(N, C, 1, 1)  # shape=[N, C, 1, 1]\n\t        # Compute co-salient attention (CSA) maps [Section 3.3].\n", "        CSA_maps = CFM(SIVs, NFs)  # shape=[N, 1, H, W]\n\t        # Compute self-correlation features (SCFs) [Section 3.4].\n\t        SCFs = get_SCFs(NFs)  # shape=[N, HW, H, W]\n\t        # Rearrange the channel order of SCFs to obtain RSCFs [Section 3.4].\n\t        evidence = CSA_maps.view(N, HW)  # shape=[N, HW]\n\t        indices = torch.argsort(evidence, dim=1, descending=True).view(N, HW, 1, 1).repeat(1, 1, H, W)  # shape=[N, HW, H, W]\n\t        RSCFs = torch.gather(SCFs, dim=1, index=indices)  # shape=[N, HW, H, W]\n\t        cosal_feat = self.conv(RSCFs * CSA_maps)  # shape=[N, 128, H, W]\n\t        return cosal_feat\n\t\"\"\"\n", "Decoder_Block:\n\t    U-net like decoder block that fuses co-saliency features and low-level features for upsampling. \n\t\"\"\"\n\tclass Decoder_Block(nn.Module):\n\t    def __init__(self, in_channel):\n\t        super(Decoder_Block, self).__init__()\n\t        self.cmprs = nn.Conv2d(in_channel, 32, 1)\n\t        self.merge_conv = nn.Sequential(nn.Conv2d(96, 96, 3, 1, 1), nn.BatchNorm2d(96), nn.ReLU(inplace=True),\n\t                                        nn.Conv2d(96, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True))\n\t        self.pred = Prediction(32)\n", "    def forward(self, low_level_feats, cosal_map, SISMs, old_feats):\n\t        _, _, H, W = low_level_feats.shape\n\t        # Adjust cosal_map, SISMs and old_feats to the same spatial size as low_level_feats.\n\t        cosal_map = resize(cosal_map, [H, W])\n\t        SISMs = resize(SISMs, [H, W])\n\t        old_feats = resize(old_feats, [H, W])\n\t        # Predict co-saliency maps with the size of H*W.\n\t        cmprs = self.cmprs(low_level_feats)\n\t        new_feats = self.merge_conv(torch.cat([cmprs * cosal_map, \n\t                                               cmprs * SISMs, \n", "                                               old_feats], dim=1))\n\t        new_cosal_map = self.pred(new_feats)\n\t        return new_feats, new_cosal_map\n\t\"\"\"\n\tICNet:\n\t    The entire ICNet.\n\t    Given a group of images and corresponding SISMs, ICNet outputs a group of co-saliency maps (predictions) at once.\n\t\"\"\"\n\tclass ICNet(nn.Module):\n\t    def __init__(self):\n", "        super(ICNet, self).__init__()\n\t        self.vgg = VGG16()\n\t        self.s = 8 # Change size from 224x224 to 256x256, s: 7 -> 8\n\t        self.Co6 = Cosal_Module(self.s*(2**0), self.s*(2**0))\n\t        self.Co5 = Cosal_Module(self.s*(2**1), self.s*(2**1))\n\t        self.Co4 = Cosal_Module(self.s*(2**2), self.s*(2**2))\n\t        self.conv6_cmprs = nn.Sequential(nn.MaxPool2d(2, 2), nn.Conv2d(512, 128, 1),\n\t                                         nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n\t                                         nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n\t                                         nn.Conv2d(128, 128, 3, 1, 1))\n", "        self.conv5_cmprs = nn.Conv2d(512, 256, 1)\n\t        self.conv4_cmprs = nn.Conv2d(512, 256, 1)\n\t        self.merge_co_56 = Res(128)\n\t        self.merge_co_45 = nn.Sequential(Res(128), nn.Conv2d(128, 32, 1))\n\t        self.get_pred_4 = Prediction(32)\n\t        self.refine_3 = Decoder_Block(256)\n\t        self.refine_2 = Decoder_Block(128)\n\t        self.refine_1 = Decoder_Block(64)\n\t    def forward(self, image_group, SISMs=None, is_training=False):\n\t        if SISMs is None:\n", "            # May cost a little extra time.\n\t            SISMs = image_group[:, :1, ...]\n\t        # Extract features from the VGG16 backbone.\n\t        conv1_2 = self.vgg(image_group, 'conv1_1', 'conv1_2_mp') # shape=[N, 64, 224, 224]\n\t        conv2_2 = self.vgg(conv1_2, 'conv1_2_mp', 'conv2_2_mp')  # shape=[N, 128, 112, 112]\n\t        conv3_3 = self.vgg(conv2_2, 'conv2_2_mp', 'conv3_3_mp')  # shape=[N, 256, 56, 56]\n\t        conv4_3 = self.vgg(conv3_3, 'conv3_3_mp', 'conv4_3_mp')  # shape=[N, 512, 28, 28]\n\t        conv5_3 = self.vgg(conv4_3, 'conv4_3_mp', 'conv5_3_mp')  # shape=[N, 512, 14, 14]\n\t        # Compress the channels of high-level features.\n\t        conv6_cmprs = self.conv6_cmprs(conv5_3)  # shape=[N, 128, 7, 7]\n", "        conv5_cmprs = self.conv5_cmprs(conv5_3)  # shape=[N, 256, 14, 14]\n\t        conv4_cmprs = self.conv4_cmprs(conv4_3)  # shape=[N, 256, 28, 28]\n\t        # Obtain co-saliancy features.\n\t        cosal_feat_6 = self.Co6(conv6_cmprs, SISMs) # shape=[N, 128, 7, 7]\n\t        cosal_feat_5 = self.Co5(conv5_cmprs, SISMs) # shape=[N, 128, 14, 14]\n\t        cosal_feat_4 = self.Co4(conv4_cmprs, SISMs) # shape=[N, 128, 28, 28]\n\t        # Merge co-saliancy features and predict co-saliency maps with size of 28*28 (i.e., \"cosal_map_4\").\n\t        feat_56 = self.merge_co_56(cosal_feat_5 + resize(cosal_feat_6, [self.s*(2**1), self.s*(2**1)])) # shape=[N, 128, 14, 14]\n\t        feat_45 = self.merge_co_45(cosal_feat_4 + resize(feat_56, [self.s*(2**2), self.s*(2**2)]))      # shape=[N, 128, 28, 28]\n\t        cosal_map_4 = self.get_pred_4(feat_45)                                    # shape=[N, 1, 28, 28]\n", "        # Obtain co-saliency maps with size of 224*224 (i.e., \"cosal_map_1\") by progressively upsampling.\n\t        feat_34, cosal_map_3 = self.refine_3(conv3_3, cosal_map_4, SISMs, feat_45)\n\t        feat_23, cosal_map_2 = self.refine_2(conv2_2, cosal_map_4, SISMs, feat_34)\n\t        _, cosal_map_1 = self.refine_1(conv1_2, cosal_map_4, SISMs, feat_23)      # shape=[N, 1, 224, 224]\n\t        # Return predicted co-saliency maps.\n\t        if is_training:\n\t            preds_list = [resize(cosal_map_4), resize(cosal_map_3), resize(cosal_map_2), cosal_map_1]\n\t            return preds_list\n\t        else:\n\t            preds = cosal_map_1\n", "            return preds\n"]}
