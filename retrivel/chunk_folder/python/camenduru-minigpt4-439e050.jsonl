{"filename": "app.py", "chunked_list": ["import argparse\n\timport os\n\timport random\n\timport numpy as np\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\timport gradio as gr\n\tfrom minigpt4.common.config import Config\n\tfrom minigpt4.common.dist_utils import get_rank\n\tfrom minigpt4.common.registry import registry\n", "from minigpt4.conversation.conversation import Chat, CONV_VISION\n\t# imports modules for registration\n\tfrom minigpt4.datasets.builders import *\n\tfrom minigpt4.models import *\n\tfrom minigpt4.processors import *\n\tfrom minigpt4.runners import *\n\tfrom minigpt4.tasks import *\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser(description=\"Demo\")\n\t    parser.add_argument(\"--cfg-path\", type=str, default='eval_configs/minigpt4.yaml', help=\"path to configuration file.\")\n", "    parser.add_argument(\n\t        \"--options\",\n\t        nargs=\"+\",\n\t        help=\"override some settings in the used config, the key-value pair \"\n\t        \"in xxx=yyy format will be merged into config file (deprecate), \"\n\t        \"change to --cfg-options instead.\",\n\t    )\n\t    args = parser.parse_args()\n\t    return args\n\tdef setup_seeds(config):\n", "    seed = config.run_cfg.seed + get_rank()\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\t    torch.manual_seed(seed)\n\t    cudnn.benchmark = False\n\t    cudnn.deterministic = True\n\t# ========================================\n\t#             Model Initialization\n\t# ========================================\n\tSHARED_UI_WARNING = f'''### [NOTE] It is possible that you are waiting in a lengthy queue.\n", "You can duplicate and use it with a paid private GPU.\n\t<a class=\"duplicate-button\" style=\"display:inline-block\" target=\"_blank\" href=\"https://huggingface.co/spaces/Vision-CAIR/minigpt4?duplicate=true\"><img style=\"margin-top:0;margin-bottom:0\" src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/duplicate-this-space-xl-dark.svg\" alt=\"Duplicate Space\"></a>\n\tAlternatively, you can also use the demo on our [project page](https://minigpt-4.github.io).\n\t'''\n\tprint('Initializing Chat')\n\tcfg = Config(parse_args())\n\tmodel_config = cfg.model_cfg\n\tmodel_cls = registry.get_model_class(model_config.arch)\n\tmodel = model_cls.from_config(model_config).to('cuda:0')\n\tvis_processor_cfg = cfg.datasets_cfg.cc_align.vis_processor.train\n", "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n\tchat = Chat(model, vis_processor)\n\tprint('Initialization Finished')\n\t# ========================================\n\t#             Gradio Setting\n\t# ========================================\n\tdef gradio_reset(chat_state, img_list):\n\t    if chat_state is not None:\n\t        chat_state.messages = []\n\t    if img_list is not None:\n", "        img_list = []\n\t    return None, gr.update(value=None, interactive=True), gr.update(placeholder='Please upload your image first', interactive=False), gr.update(value=\"Upload & Start Chat\", interactive=True), chat_state, img_list\n\tdef upload_img(gr_img, text_input, chat_state):\n\t    if gr_img is None:\n\t        return None, None, gr.update(interactive=True), chat_state, None\n\t    chat_state = CONV_VISION.copy()\n\t    img_list = []\n\t    llm_message = chat.upload_img(gr_img, chat_state, img_list)\n\t    return gr.update(interactive=False), gr.update(interactive=True, placeholder='Type and press Enter'), gr.update(value=\"Start Chatting\", interactive=False), chat_state, img_list\n\tdef gradio_ask(user_message, chatbot, chat_state):\n", "    if len(user_message) == 0:\n\t        return gr.update(interactive=True, placeholder='Input should not be empty!'), chatbot, chat_state\n\t    chat.ask(user_message, chat_state)\n\t    chatbot = chatbot + [[user_message, None]]\n\t    return '', chatbot, chat_state\n\tdef gradio_answer(chatbot, chat_state, img_list, num_beams, temperature):\n\t    llm_message = chat.answer(conv=chat_state, img_list=img_list, max_new_tokens=300, num_beams=1, temperature=temperature, max_length=2000)[0]\n\t    chatbot[-1][1] = llm_message\n\t    return chatbot, chat_state, img_list\n\ttitle = \"\"\"<h1 align=\"center\">Demo of MiniGPT-4</h1>\"\"\"\n", "description = \"\"\"<h3>This is the demo of MiniGPT-4. Upload your images and start chatting!</h3>\"\"\"\n\tarticle = \"\"\"<p><a href='https://minigpt-4.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/Github-Code-blue'></a></p><p><a href='https://github.com/TsuTikgiau/blip2-llm/blob/release_prepare/MiniGPT_4.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a></p>\n\t\"\"\"\n\t#TODO show examples below\n\twith gr.Blocks() as demo:\n\t    gr.Markdown(title)\n\t    gr.Markdown(SHARED_UI_WARNING)\n\t    gr.Markdown(description)\n\t    gr.Markdown(article)\n\t    with gr.Row():\n", "        with gr.Column(scale=0.5):\n\t            image = gr.Image(type=\"pil\")\n\t            upload_button = gr.Button(value=\"Upload & Start Chat\", interactive=True, variant=\"primary\")\n\t            clear = gr.Button(\"Restart\")\n\t            num_beams = gr.Slider(\n\t                minimum=1,\n\t                maximum=5,\n\t                value=1,\n\t                step=1,\n\t                interactive=True,\n", "                label=\"beam search numbers)\",\n\t            )\n\t            temperature = gr.Slider(\n\t                minimum=0.1,\n\t                maximum=2.0,\n\t                value=1.0,\n\t                step=0.1,\n\t                interactive=True,\n\t                label=\"Temperature\",\n\t            )\n", "        with gr.Column():\n\t            chat_state = gr.State()\n\t            img_list = gr.State()\n\t            chatbot = gr.Chatbot(label='MiniGPT-4')\n\t            text_input = gr.Textbox(label='User', placeholder='Please upload your image first', interactive=False)\n\t    upload_button.click(upload_img, [image, text_input, chat_state], [image, text_input, upload_button, chat_state, img_list])\n\t    text_input.submit(gradio_ask, [text_input, chatbot, chat_state], [text_input, chatbot, chat_state]).then(\n\t        gradio_answer, [chatbot, chat_state, img_list, num_beams, temperature], [chatbot, chat_state, img_list]\n\t    )\n\t    clear.click(gradio_reset, [chat_state, img_list], [chatbot, image, text_input, upload_button, chat_state, img_list], queue=False)\n", "demo.launch(enable_queue=True)"]}
{"filename": "minigpt4/__init__.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport os\n\timport sys\n\tfrom omegaconf import OmegaConf\n\tfrom minigpt4.common.registry import registry\n", "from minigpt4.datasets.builders import *\n\tfrom minigpt4.models import *\n\tfrom minigpt4.processors import *\n\tfrom minigpt4.tasks import *\n\troot_dir = os.path.dirname(os.path.abspath(__file__))\n\tdefault_cfg = OmegaConf.load(os.path.join(root_dir, \"configs/default.yaml\"))\n\tregistry.register_path(\"library_root\", root_dir)\n\trepo_root = os.path.join(root_dir, \"..\")\n\tregistry.register_path(\"repo_root\", repo_root)\n\tcache_root = os.path.join(repo_root, default_cfg.env.cache_root)\n", "registry.register_path(\"cache_root\", cache_root)\n\tregistry.register(\"MAX_INT\", sys.maxsize)\n\tregistry.register(\"SPLIT_NAMES\", [\"train\", \"val\", \"test\"])\n"]}
{"filename": "minigpt4/processors/blip_processors.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport re\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.processors.base_processor import BaseProcessor\n\tfrom minigpt4.processors.randaugment import RandomAugment\n", "from omegaconf import OmegaConf\n\tfrom torchvision import transforms\n\tfrom torchvision.transforms.functional import InterpolationMode\n\tclass BlipImageBaseProcessor(BaseProcessor):\n\t    def __init__(self, mean=None, std=None):\n\t        if mean is None:\n\t            mean = (0.48145466, 0.4578275, 0.40821073)\n\t        if std is None:\n\t            std = (0.26862954, 0.26130258, 0.27577711)\n\t        self.normalize = transforms.Normalize(mean, std)\n", "@registry.register_processor(\"blip_caption\")\n\tclass BlipCaptionProcessor(BaseProcessor):\n\t    def __init__(self, prompt=\"\", max_words=50):\n\t        self.prompt = prompt\n\t        self.max_words = max_words\n\t    def __call__(self, caption):\n\t        caption = self.prompt + self.pre_caption(caption)\n\t        return caption\n\t    @classmethod\n\t    def from_config(cls, cfg=None):\n", "        if cfg is None:\n\t            cfg = OmegaConf.create()\n\t        prompt = cfg.get(\"prompt\", \"\")\n\t        max_words = cfg.get(\"max_words\", 50)\n\t        return cls(prompt=prompt, max_words=max_words)\n\t    def pre_caption(self, caption):\n\t        caption = re.sub(\n\t            r\"([.!\\\"()*#:;~])\",\n\t            \" \",\n\t            caption.lower(),\n", "        )\n\t        caption = re.sub(\n\t            r\"\\s{2,}\",\n\t            \" \",\n\t            caption,\n\t        )\n\t        caption = caption.rstrip(\"\\n\")\n\t        caption = caption.strip(\" \")\n\t        # truncate caption\n\t        caption_words = caption.split(\" \")\n", "        if len(caption_words) > self.max_words:\n\t            caption = \" \".join(caption_words[: self.max_words])\n\t        return caption\n\t@registry.register_processor(\"blip2_image_train\")\n\tclass Blip2ImageTrainProcessor(BlipImageBaseProcessor):\n\t    def __init__(self, image_size=224, mean=None, std=None, min_scale=0.5, max_scale=1.0):\n\t        super().__init__(mean=mean, std=std)\n\t        self.transform = transforms.Compose(\n\t            [\n\t                transforms.RandomResizedCrop(\n", "                    image_size,\n\t                    scale=(min_scale, max_scale),\n\t                    interpolation=InterpolationMode.BICUBIC,\n\t                ),\n\t                transforms.ToTensor(),\n\t                self.normalize,\n\t            ]\n\t        )\n\t    def __call__(self, item):\n\t        return self.transform(item)\n", "    @classmethod\n\t    def from_config(cls, cfg=None):\n\t        if cfg is None:\n\t            cfg = OmegaConf.create()\n\t        image_size = cfg.get(\"image_size\", 224)\n\t        mean = cfg.get(\"mean\", None)\n\t        std = cfg.get(\"std\", None)\n\t        min_scale = cfg.get(\"min_scale\", 0.5)\n\t        max_scale = cfg.get(\"max_scale\", 1.0)\n\t        return cls(\n", "            image_size=image_size,\n\t            mean=mean,\n\t            std=std,\n\t            min_scale=min_scale,\n\t            max_scale=max_scale,\n\t        )\n\t@registry.register_processor(\"blip2_image_eval\")\n\tclass Blip2ImageEvalProcessor(BlipImageBaseProcessor):\n\t    def __init__(self, image_size=224, mean=None, std=None):\n\t        super().__init__(mean=mean, std=std)\n", "        self.transform = transforms.Compose(\n\t            [\n\t                transforms.Resize(\n\t                    (image_size, image_size), interpolation=InterpolationMode.BICUBIC\n\t                ),\n\t                transforms.ToTensor(),\n\t                self.normalize,\n\t            ]\n\t        )\n\t    def __call__(self, item):\n", "        return self.transform(item)\n\t    @classmethod\n\t    def from_config(cls, cfg=None):\n\t        if cfg is None:\n\t            cfg = OmegaConf.create()\n\t        image_size = cfg.get(\"image_size\", 224)\n\t        mean = cfg.get(\"mean\", None)\n\t        std = cfg.get(\"std\", None)\n\t        return cls(image_size=image_size, mean=mean, std=std)"]}
{"filename": "minigpt4/processors/__init__.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom minigpt4.processors.base_processor import BaseProcessor\n\tfrom minigpt4.processors.blip_processors import (\n\t    Blip2ImageTrainProcessor,\n\t    Blip2ImageEvalProcessor,\n", "    BlipCaptionProcessor,\n\t)\n\tfrom minigpt4.common.registry import registry\n\t__all__ = [\n\t    \"BaseProcessor\",\n\t    \"Blip2ImageTrainProcessor\",\n\t    \"Blip2ImageEvalProcessor\",\n\t    \"BlipCaptionProcessor\",\n\t]\n\tdef load_processor(name, cfg=None):\n", "    \"\"\"\n\t    Example\n\t    >>> processor = load_processor(\"alpro_video_train\", cfg=None)\n\t    \"\"\"\n\t    processor = registry.get_processor_class(name).from_config(cfg)\n\t    return processor\n"]}
{"filename": "minigpt4/processors/randaugment.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport cv2\n\timport numpy as np\n\timport torch\n\t## aug functions\n", "def identity_func(img):\n\t    return img\n\tdef autocontrast_func(img, cutoff=0):\n\t    \"\"\"\n\t    same output as PIL.ImageOps.autocontrast\n\t    \"\"\"\n\t    n_bins = 256\n\t    def tune_channel(ch):\n\t        n = ch.size\n\t        cut = cutoff * n // 100\n", "        if cut == 0:\n\t            high, low = ch.max(), ch.min()\n\t        else:\n\t            hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n\t            low = np.argwhere(np.cumsum(hist) > cut)\n\t            low = 0 if low.shape[0] == 0 else low[0]\n\t            high = np.argwhere(np.cumsum(hist[::-1]) > cut)\n\t            high = n_bins - 1 if high.shape[0] == 0 else n_bins - 1 - high[0]\n\t        if high <= low:\n\t            table = np.arange(n_bins)\n", "        else:\n\t            scale = (n_bins - 1) / (high - low)\n\t            offset = -low * scale\n\t            table = np.arange(n_bins) * scale + offset\n\t            table[table < 0] = 0\n\t            table[table > n_bins - 1] = n_bins - 1\n\t        table = table.clip(0, 255).astype(np.uint8)\n\t        return table[ch]\n\t    channels = [tune_channel(ch) for ch in cv2.split(img)]\n\t    out = cv2.merge(channels)\n", "    return out\n\tdef equalize_func(img):\n\t    \"\"\"\n\t    same output as PIL.ImageOps.equalize\n\t    PIL's implementation is different from cv2.equalize\n\t    \"\"\"\n\t    n_bins = 256\n\t    def tune_channel(ch):\n\t        hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n\t        non_zero_hist = hist[hist != 0].reshape(-1)\n", "        step = np.sum(non_zero_hist[:-1]) // (n_bins - 1)\n\t        if step == 0:\n\t            return ch\n\t        n = np.empty_like(hist)\n\t        n[0] = step // 2\n\t        n[1:] = hist[:-1]\n\t        table = (np.cumsum(n) // step).clip(0, 255).astype(np.uint8)\n\t        return table[ch]\n\t    channels = [tune_channel(ch) for ch in cv2.split(img)]\n\t    out = cv2.merge(channels)\n", "    return out\n\tdef rotate_func(img, degree, fill=(0, 0, 0)):\n\t    \"\"\"\n\t    like PIL, rotate by degree, not radians\n\t    \"\"\"\n\t    H, W = img.shape[0], img.shape[1]\n\t    center = W / 2, H / 2\n\t    M = cv2.getRotationMatrix2D(center, degree, 1)\n\t    out = cv2.warpAffine(img, M, (W, H), borderValue=fill)\n\t    return out\n", "def solarize_func(img, thresh=128):\n\t    \"\"\"\n\t    same output as PIL.ImageOps.posterize\n\t    \"\"\"\n\t    table = np.array([el if el < thresh else 255 - el for el in range(256)])\n\t    table = table.clip(0, 255).astype(np.uint8)\n\t    out = table[img]\n\t    return out\n\tdef color_func(img, factor):\n\t    \"\"\"\n", "    same output as PIL.ImageEnhance.Color\n\t    \"\"\"\n\t    ## implementation according to PIL definition, quite slow\n\t    #  degenerate = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis]\n\t    #  out = blend(degenerate, img, factor)\n\t    #  M = (\n\t    #      np.eye(3) * factor\n\t    #      + np.float32([0.114, 0.587, 0.299]).reshape(3, 1) * (1. - factor)\n\t    #  )[np.newaxis, np.newaxis, :]\n\t    M = np.float32(\n", "        [[0.886, -0.114, -0.114], [-0.587, 0.413, -0.587], [-0.299, -0.299, 0.701]]\n\t    ) * factor + np.float32([[0.114], [0.587], [0.299]])\n\t    out = np.matmul(img, M).clip(0, 255).astype(np.uint8)\n\t    return out\n\tdef contrast_func(img, factor):\n\t    \"\"\"\n\t    same output as PIL.ImageEnhance.Contrast\n\t    \"\"\"\n\t    mean = np.sum(np.mean(img, axis=(0, 1)) * np.array([0.114, 0.587, 0.299]))\n\t    table = (\n", "        np.array([(el - mean) * factor + mean for el in range(256)])\n\t        .clip(0, 255)\n\t        .astype(np.uint8)\n\t    )\n\t    out = table[img]\n\t    return out\n\tdef brightness_func(img, factor):\n\t    \"\"\"\n\t    same output as PIL.ImageEnhance.Contrast\n\t    \"\"\"\n", "    table = (np.arange(256, dtype=np.float32) * factor).clip(0, 255).astype(np.uint8)\n\t    out = table[img]\n\t    return out\n\tdef sharpness_func(img, factor):\n\t    \"\"\"\n\t    The differences the this result and PIL are all on the 4 boundaries, the center\n\t    areas are same\n\t    \"\"\"\n\t    kernel = np.ones((3, 3), dtype=np.float32)\n\t    kernel[1][1] = 5\n", "    kernel /= 13\n\t    degenerate = cv2.filter2D(img, -1, kernel)\n\t    if factor == 0.0:\n\t        out = degenerate\n\t    elif factor == 1.0:\n\t        out = img\n\t    else:\n\t        out = img.astype(np.float32)\n\t        degenerate = degenerate.astype(np.float32)[1:-1, 1:-1, :]\n\t        out[1:-1, 1:-1, :] = degenerate + factor * (out[1:-1, 1:-1, :] - degenerate)\n", "        out = out.astype(np.uint8)\n\t    return out\n\tdef shear_x_func(img, factor, fill=(0, 0, 0)):\n\t    H, W = img.shape[0], img.shape[1]\n\t    M = np.float32([[1, factor, 0], [0, 1, 0]])\n\t    out = cv2.warpAffine(\n\t        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n\t    ).astype(np.uint8)\n\t    return out\n\tdef translate_x_func(img, offset, fill=(0, 0, 0)):\n", "    \"\"\"\n\t    same output as PIL.Image.transform\n\t    \"\"\"\n\t    H, W = img.shape[0], img.shape[1]\n\t    M = np.float32([[1, 0, -offset], [0, 1, 0]])\n\t    out = cv2.warpAffine(\n\t        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n\t    ).astype(np.uint8)\n\t    return out\n\tdef translate_y_func(img, offset, fill=(0, 0, 0)):\n", "    \"\"\"\n\t    same output as PIL.Image.transform\n\t    \"\"\"\n\t    H, W = img.shape[0], img.shape[1]\n\t    M = np.float32([[1, 0, 0], [0, 1, -offset]])\n\t    out = cv2.warpAffine(\n\t        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n\t    ).astype(np.uint8)\n\t    return out\n\tdef posterize_func(img, bits):\n", "    \"\"\"\n\t    same output as PIL.ImageOps.posterize\n\t    \"\"\"\n\t    out = np.bitwise_and(img, np.uint8(255 << (8 - bits)))\n\t    return out\n\tdef shear_y_func(img, factor, fill=(0, 0, 0)):\n\t    H, W = img.shape[0], img.shape[1]\n\t    M = np.float32([[1, 0, 0], [factor, 1, 0]])\n\t    out = cv2.warpAffine(\n\t        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n", "    ).astype(np.uint8)\n\t    return out\n\tdef cutout_func(img, pad_size, replace=(0, 0, 0)):\n\t    replace = np.array(replace, dtype=np.uint8)\n\t    H, W = img.shape[0], img.shape[1]\n\t    rh, rw = np.random.random(2)\n\t    pad_size = pad_size // 2\n\t    ch, cw = int(rh * H), int(rw * W)\n\t    x1, x2 = max(ch - pad_size, 0), min(ch + pad_size, H)\n\t    y1, y2 = max(cw - pad_size, 0), min(cw + pad_size, W)\n", "    out = img.copy()\n\t    out[x1:x2, y1:y2, :] = replace\n\t    return out\n\t### level to args\n\tdef enhance_level_to_args(MAX_LEVEL):\n\t    def level_to_args(level):\n\t        return ((level / MAX_LEVEL) * 1.8 + 0.1,)\n\t    return level_to_args\n\tdef shear_level_to_args(MAX_LEVEL, replace_value):\n\t    def level_to_args(level):\n", "        level = (level / MAX_LEVEL) * 0.3\n\t        if np.random.random() > 0.5:\n\t            level = -level\n\t        return (level, replace_value)\n\t    return level_to_args\n\tdef translate_level_to_args(translate_const, MAX_LEVEL, replace_value):\n\t    def level_to_args(level):\n\t        level = (level / MAX_LEVEL) * float(translate_const)\n\t        if np.random.random() > 0.5:\n\t            level = -level\n", "        return (level, replace_value)\n\t    return level_to_args\n\tdef cutout_level_to_args(cutout_const, MAX_LEVEL, replace_value):\n\t    def level_to_args(level):\n\t        level = int((level / MAX_LEVEL) * cutout_const)\n\t        return (level, replace_value)\n\t    return level_to_args\n\tdef solarize_level_to_args(MAX_LEVEL):\n\t    def level_to_args(level):\n\t        level = int((level / MAX_LEVEL) * 256)\n", "        return (level,)\n\t    return level_to_args\n\tdef none_level_to_args(level):\n\t    return ()\n\tdef posterize_level_to_args(MAX_LEVEL):\n\t    def level_to_args(level):\n\t        level = int((level / MAX_LEVEL) * 4)\n\t        return (level,)\n\t    return level_to_args\n\tdef rotate_level_to_args(MAX_LEVEL, replace_value):\n", "    def level_to_args(level):\n\t        level = (level / MAX_LEVEL) * 30\n\t        if np.random.random() < 0.5:\n\t            level = -level\n\t        return (level, replace_value)\n\t    return level_to_args\n\tfunc_dict = {\n\t    \"Identity\": identity_func,\n\t    \"AutoContrast\": autocontrast_func,\n\t    \"Equalize\": equalize_func,\n", "    \"Rotate\": rotate_func,\n\t    \"Solarize\": solarize_func,\n\t    \"Color\": color_func,\n\t    \"Contrast\": contrast_func,\n\t    \"Brightness\": brightness_func,\n\t    \"Sharpness\": sharpness_func,\n\t    \"ShearX\": shear_x_func,\n\t    \"TranslateX\": translate_x_func,\n\t    \"TranslateY\": translate_y_func,\n\t    \"Posterize\": posterize_func,\n", "    \"ShearY\": shear_y_func,\n\t}\n\ttranslate_const = 10\n\tMAX_LEVEL = 10\n\treplace_value = (128, 128, 128)\n\targ_dict = {\n\t    \"Identity\": none_level_to_args,\n\t    \"AutoContrast\": none_level_to_args,\n\t    \"Equalize\": none_level_to_args,\n\t    \"Rotate\": rotate_level_to_args(MAX_LEVEL, replace_value),\n", "    \"Solarize\": solarize_level_to_args(MAX_LEVEL),\n\t    \"Color\": enhance_level_to_args(MAX_LEVEL),\n\t    \"Contrast\": enhance_level_to_args(MAX_LEVEL),\n\t    \"Brightness\": enhance_level_to_args(MAX_LEVEL),\n\t    \"Sharpness\": enhance_level_to_args(MAX_LEVEL),\n\t    \"ShearX\": shear_level_to_args(MAX_LEVEL, replace_value),\n\t    \"TranslateX\": translate_level_to_args(translate_const, MAX_LEVEL, replace_value),\n\t    \"TranslateY\": translate_level_to_args(translate_const, MAX_LEVEL, replace_value),\n\t    \"Posterize\": posterize_level_to_args(MAX_LEVEL),\n\t    \"ShearY\": shear_level_to_args(MAX_LEVEL, replace_value),\n", "}\n\tclass RandomAugment(object):\n\t    def __init__(self, N=2, M=10, isPIL=False, augs=[]):\n\t        self.N = N\n\t        self.M = M\n\t        self.isPIL = isPIL\n\t        if augs:\n\t            self.augs = augs\n\t        else:\n\t            self.augs = list(arg_dict.keys())\n", "    def get_random_ops(self):\n\t        sampled_ops = np.random.choice(self.augs, self.N)\n\t        return [(op, 0.5, self.M) for op in sampled_ops]\n\t    def __call__(self, img):\n\t        if self.isPIL:\n\t            img = np.array(img)\n\t        ops = self.get_random_ops()\n\t        for name, prob, level in ops:\n\t            if np.random.random() > prob:\n\t                continue\n", "            args = arg_dict[name](level)\n\t            img = func_dict[name](img, *args)\n\t        return img\n\tclass VideoRandomAugment(object):\n\t    def __init__(self, N=2, M=10, p=0.0, tensor_in_tensor_out=True, augs=[]):\n\t        self.N = N\n\t        self.M = M\n\t        self.p = p\n\t        self.tensor_in_tensor_out = tensor_in_tensor_out\n\t        if augs:\n", "            self.augs = augs\n\t        else:\n\t            self.augs = list(arg_dict.keys())\n\t    def get_random_ops(self):\n\t        sampled_ops = np.random.choice(self.augs, self.N, replace=False)\n\t        return [(op, self.M) for op in sampled_ops]\n\t    def __call__(self, frames):\n\t        assert (\n\t            frames.shape[-1] == 3\n\t        ), \"Expecting last dimension for 3-channels RGB (b, h, w, c).\"\n", "        if self.tensor_in_tensor_out:\n\t            frames = frames.numpy().astype(np.uint8)\n\t        num_frames = frames.shape[0]\n\t        ops = num_frames * [self.get_random_ops()]\n\t        apply_or_not = num_frames * [np.random.random(size=self.N) > self.p]\n\t        frames = torch.stack(\n\t            list(map(self._aug, frames, ops, apply_or_not)), dim=0\n\t        ).float()\n\t        return frames\n\t    def _aug(self, img, ops, apply_or_not):\n", "        for i, (name, level) in enumerate(ops):\n\t            if not apply_or_not[i]:\n\t                continue\n\t            args = arg_dict[name](level)\n\t            img = func_dict[name](img, *args)\n\t        return torch.from_numpy(img)\n\tif __name__ == \"__main__\":\n\t    a = RandomAugment()\n\t    img = np.random.randn(32, 32, 3)\n\t    a(img)\n"]}
{"filename": "minigpt4/processors/base_processor.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom omegaconf import OmegaConf\n\tclass BaseProcessor:\n\t    def __init__(self):\n\t        self.transform = lambda x: x\n", "        return\n\t    def __call__(self, item):\n\t        return self.transform(item)\n\t    @classmethod\n\t    def from_config(cls, cfg=None):\n\t        return cls()\n\t    def build(self, **kwargs):\n\t        cfg = OmegaConf.create(kwargs)\n\t        return self.from_config(cfg)\n"]}
{"filename": "minigpt4/common/optims.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport math\n\tfrom minigpt4.common.registry import registry\n\t@registry.register_lr_scheduler(\"linear_warmup_step_lr\")\n\tclass LinearWarmupStepLRScheduler:\n", "    def __init__(\n\t        self,\n\t        optimizer,\n\t        max_epoch,\n\t        min_lr,\n\t        init_lr,\n\t        decay_rate=1,\n\t        warmup_start_lr=-1,\n\t        warmup_steps=0,\n\t        **kwargs\n", "    ):\n\t        self.optimizer = optimizer\n\t        self.max_epoch = max_epoch\n\t        self.min_lr = min_lr\n\t        self.decay_rate = decay_rate\n\t        self.init_lr = init_lr\n\t        self.warmup_steps = warmup_steps\n\t        self.warmup_start_lr = warmup_start_lr if warmup_start_lr >= 0 else init_lr\n\t    def step(self, cur_epoch, cur_step):\n\t        if cur_epoch == 0:\n", "            warmup_lr_schedule(\n\t                step=cur_step,\n\t                optimizer=self.optimizer,\n\t                max_step=self.warmup_steps,\n\t                init_lr=self.warmup_start_lr,\n\t                max_lr=self.init_lr,\n\t            )\n\t        else:\n\t            step_lr_schedule(\n\t                epoch=cur_epoch,\n", "                optimizer=self.optimizer,\n\t                init_lr=self.init_lr,\n\t                min_lr=self.min_lr,\n\t                decay_rate=self.decay_rate,\n\t            )\n\t@registry.register_lr_scheduler(\"linear_warmup_cosine_lr\")\n\tclass LinearWarmupCosineLRScheduler:\n\t    def __init__(\n\t        self,\n\t        optimizer,\n", "        max_epoch,\n\t        iters_per_epoch,\n\t        min_lr,\n\t        init_lr,\n\t        warmup_steps=0,\n\t        warmup_start_lr=-1,\n\t        **kwargs\n\t    ):\n\t        self.optimizer = optimizer\n\t        self.max_epoch = max_epoch\n", "        self.iters_per_epoch = iters_per_epoch\n\t        self.min_lr = min_lr\n\t        self.init_lr = init_lr\n\t        self.warmup_steps = warmup_steps\n\t        self.warmup_start_lr = warmup_start_lr if warmup_start_lr >= 0 else init_lr\n\t    def step(self, cur_epoch, cur_step):\n\t        total_cur_step = cur_epoch * self.iters_per_epoch + cur_step\n\t        if total_cur_step < self.warmup_steps:\n\t            warmup_lr_schedule(\n\t                step=cur_step,\n", "                optimizer=self.optimizer,\n\t                max_step=self.warmup_steps,\n\t                init_lr=self.warmup_start_lr,\n\t                max_lr=self.init_lr,\n\t            )\n\t        else:\n\t            cosine_lr_schedule(\n\t                epoch=total_cur_step,\n\t                optimizer=self.optimizer,\n\t                max_epoch=self.max_epoch * self.iters_per_epoch,\n", "                init_lr=self.init_lr,\n\t                min_lr=self.min_lr,\n\t            )\n\tdef cosine_lr_schedule(optimizer, epoch, max_epoch, init_lr, min_lr):\n\t    \"\"\"Decay the learning rate\"\"\"\n\t    lr = (init_lr - min_lr) * 0.5 * (\n\t        1.0 + math.cos(math.pi * epoch / max_epoch)\n\t    ) + min_lr\n\t    for param_group in optimizer.param_groups:\n\t        param_group[\"lr\"] = lr\n", "def warmup_lr_schedule(optimizer, step, max_step, init_lr, max_lr):\n\t    \"\"\"Warmup the learning rate\"\"\"\n\t    lr = min(max_lr, init_lr + (max_lr - init_lr) * step / max(max_step, 1))\n\t    for param_group in optimizer.param_groups:\n\t        param_group[\"lr\"] = lr\n\tdef step_lr_schedule(optimizer, epoch, init_lr, min_lr, decay_rate):\n\t    \"\"\"Decay the learning rate\"\"\"\n\t    lr = max(min_lr, init_lr * (decay_rate**epoch))\n\t    for param_group in optimizer.param_groups:\n\t        param_group[\"lr\"] = lr\n"]}
{"filename": "minigpt4/common/registry.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tclass Registry:\n\t    mapping = {\n\t        \"builder_name_mapping\": {},\n\t        \"task_name_mapping\": {},\n", "        \"processor_name_mapping\": {},\n\t        \"model_name_mapping\": {},\n\t        \"lr_scheduler_name_mapping\": {},\n\t        \"runner_name_mapping\": {},\n\t        \"state\": {},\n\t        \"paths\": {},\n\t    }\n\t    @classmethod\n\t    def register_builder(cls, name):\n\t        r\"\"\"Register a dataset builder to registry with key 'name'\n", "        Args:\n\t            name: Key with which the builder will be registered.\n\t        Usage:\n\t            from minigpt4.common.registry import registry\n\t            from minigpt4.datasets.base_dataset_builder import BaseDatasetBuilder\n\t        \"\"\"\n\t        def wrap(builder_cls):\n\t            from minigpt4.datasets.builders.base_dataset_builder import BaseDatasetBuilder\n\t            assert issubclass(\n\t                builder_cls, BaseDatasetBuilder\n", "            ), \"All builders must inherit BaseDatasetBuilder class, found {}\".format(\n\t                builder_cls\n\t            )\n\t            if name in cls.mapping[\"builder_name_mapping\"]:\n\t                raise KeyError(\n\t                    \"Name '{}' already registered for {}.\".format(\n\t                        name, cls.mapping[\"builder_name_mapping\"][name]\n\t                    )\n\t                )\n\t            cls.mapping[\"builder_name_mapping\"][name] = builder_cls\n", "            return builder_cls\n\t        return wrap\n\t    @classmethod\n\t    def register_task(cls, name):\n\t        r\"\"\"Register a task to registry with key 'name'\n\t        Args:\n\t            name: Key with which the task will be registered.\n\t        Usage:\n\t            from minigpt4.common.registry import registry\n\t        \"\"\"\n", "        def wrap(task_cls):\n\t            from minigpt4.tasks.base_task import BaseTask\n\t            assert issubclass(\n\t                task_cls, BaseTask\n\t            ), \"All tasks must inherit BaseTask class\"\n\t            if name in cls.mapping[\"task_name_mapping\"]:\n\t                raise KeyError(\n\t                    \"Name '{}' already registered for {}.\".format(\n\t                        name, cls.mapping[\"task_name_mapping\"][name]\n\t                    )\n", "                )\n\t            cls.mapping[\"task_name_mapping\"][name] = task_cls\n\t            return task_cls\n\t        return wrap\n\t    @classmethod\n\t    def register_model(cls, name):\n\t        r\"\"\"Register a task to registry with key 'name'\n\t        Args:\n\t            name: Key with which the task will be registered.\n\t        Usage:\n", "            from minigpt4.common.registry import registry\n\t        \"\"\"\n\t        def wrap(model_cls):\n\t            from minigpt4.models import BaseModel\n\t            assert issubclass(\n\t                model_cls, BaseModel\n\t            ), \"All models must inherit BaseModel class\"\n\t            if name in cls.mapping[\"model_name_mapping\"]:\n\t                raise KeyError(\n\t                    \"Name '{}' already registered for {}.\".format(\n", "                        name, cls.mapping[\"model_name_mapping\"][name]\n\t                    )\n\t                )\n\t            cls.mapping[\"model_name_mapping\"][name] = model_cls\n\t            return model_cls\n\t        return wrap\n\t    @classmethod\n\t    def register_processor(cls, name):\n\t        r\"\"\"Register a processor to registry with key 'name'\n\t        Args:\n", "            name: Key with which the task will be registered.\n\t        Usage:\n\t            from minigpt4.common.registry import registry\n\t        \"\"\"\n\t        def wrap(processor_cls):\n\t            from minigpt4.processors import BaseProcessor\n\t            assert issubclass(\n\t                processor_cls, BaseProcessor\n\t            ), \"All processors must inherit BaseProcessor class\"\n\t            if name in cls.mapping[\"processor_name_mapping\"]:\n", "                raise KeyError(\n\t                    \"Name '{}' already registered for {}.\".format(\n\t                        name, cls.mapping[\"processor_name_mapping\"][name]\n\t                    )\n\t                )\n\t            cls.mapping[\"processor_name_mapping\"][name] = processor_cls\n\t            return processor_cls\n\t        return wrap\n\t    @classmethod\n\t    def register_lr_scheduler(cls, name):\n", "        r\"\"\"Register a model to registry with key 'name'\n\t        Args:\n\t            name: Key with which the task will be registered.\n\t        Usage:\n\t            from minigpt4.common.registry import registry\n\t        \"\"\"\n\t        def wrap(lr_sched_cls):\n\t            if name in cls.mapping[\"lr_scheduler_name_mapping\"]:\n\t                raise KeyError(\n\t                    \"Name '{}' already registered for {}.\".format(\n", "                        name, cls.mapping[\"lr_scheduler_name_mapping\"][name]\n\t                    )\n\t                )\n\t            cls.mapping[\"lr_scheduler_name_mapping\"][name] = lr_sched_cls\n\t            return lr_sched_cls\n\t        return wrap\n\t    @classmethod\n\t    def register_runner(cls, name):\n\t        r\"\"\"Register a model to registry with key 'name'\n\t        Args:\n", "            name: Key with which the task will be registered.\n\t        Usage:\n\t            from minigpt4.common.registry import registry\n\t        \"\"\"\n\t        def wrap(runner_cls):\n\t            if name in cls.mapping[\"runner_name_mapping\"]:\n\t                raise KeyError(\n\t                    \"Name '{}' already registered for {}.\".format(\n\t                        name, cls.mapping[\"runner_name_mapping\"][name]\n\t                    )\n", "                )\n\t            cls.mapping[\"runner_name_mapping\"][name] = runner_cls\n\t            return runner_cls\n\t        return wrap\n\t    @classmethod\n\t    def register_path(cls, name, path):\n\t        r\"\"\"Register a path to registry with key 'name'\n\t        Args:\n\t            name: Key with which the path will be registered.\n\t        Usage:\n", "            from minigpt4.common.registry import registry\n\t        \"\"\"\n\t        assert isinstance(path, str), \"All path must be str.\"\n\t        if name in cls.mapping[\"paths\"]:\n\t            raise KeyError(\"Name '{}' already registered.\".format(name))\n\t        cls.mapping[\"paths\"][name] = path\n\t    @classmethod\n\t    def register(cls, name, obj):\n\t        r\"\"\"Register an item to registry with key 'name'\n\t        Args:\n", "            name: Key with which the item will be registered.\n\t        Usage::\n\t            from minigpt4.common.registry import registry\n\t            registry.register(\"config\", {})\n\t        \"\"\"\n\t        path = name.split(\".\")\n\t        current = cls.mapping[\"state\"]\n\t        for part in path[:-1]:\n\t            if part not in current:\n\t                current[part] = {}\n", "            current = current[part]\n\t        current[path[-1]] = obj\n\t    # @classmethod\n\t    # def get_trainer_class(cls, name):\n\t    #     return cls.mapping[\"trainer_name_mapping\"].get(name, None)\n\t    @classmethod\n\t    def get_builder_class(cls, name):\n\t        return cls.mapping[\"builder_name_mapping\"].get(name, None)\n\t    @classmethod\n\t    def get_model_class(cls, name):\n", "        return cls.mapping[\"model_name_mapping\"].get(name, None)\n\t    @classmethod\n\t    def get_task_class(cls, name):\n\t        return cls.mapping[\"task_name_mapping\"].get(name, None)\n\t    @classmethod\n\t    def get_processor_class(cls, name):\n\t        return cls.mapping[\"processor_name_mapping\"].get(name, None)\n\t    @classmethod\n\t    def get_lr_scheduler_class(cls, name):\n\t        return cls.mapping[\"lr_scheduler_name_mapping\"].get(name, None)\n", "    @classmethod\n\t    def get_runner_class(cls, name):\n\t        return cls.mapping[\"runner_name_mapping\"].get(name, None)\n\t    @classmethod\n\t    def list_runners(cls):\n\t        return sorted(cls.mapping[\"runner_name_mapping\"].keys())\n\t    @classmethod\n\t    def list_models(cls):\n\t        return sorted(cls.mapping[\"model_name_mapping\"].keys())\n\t    @classmethod\n", "    def list_tasks(cls):\n\t        return sorted(cls.mapping[\"task_name_mapping\"].keys())\n\t    @classmethod\n\t    def list_processors(cls):\n\t        return sorted(cls.mapping[\"processor_name_mapping\"].keys())\n\t    @classmethod\n\t    def list_lr_schedulers(cls):\n\t        return sorted(cls.mapping[\"lr_scheduler_name_mapping\"].keys())\n\t    @classmethod\n\t    def list_datasets(cls):\n", "        return sorted(cls.mapping[\"builder_name_mapping\"].keys())\n\t    @classmethod\n\t    def get_path(cls, name):\n\t        return cls.mapping[\"paths\"].get(name, None)\n\t    @classmethod\n\t    def get(cls, name, default=None, no_warning=False):\n\t        r\"\"\"Get an item from registry with key 'name'\n\t        Args:\n\t            name (string): Key whose value needs to be retrieved.\n\t            default: If passed and key is not in registry, default value will\n", "                     be returned with a warning. Default: None\n\t            no_warning (bool): If passed as True, warning when key doesn't exist\n\t                               will not be generated. Useful for MMF's\n\t                               internal operations. Default: False\n\t        \"\"\"\n\t        original_name = name\n\t        name = name.split(\".\")\n\t        value = cls.mapping[\"state\"]\n\t        for subname in name:\n\t            value = value.get(subname, default)\n", "            if value is default:\n\t                break\n\t        if (\n\t            \"writer\" in cls.mapping[\"state\"]\n\t            and value == default\n\t            and no_warning is False\n\t        ):\n\t            cls.mapping[\"state\"][\"writer\"].warning(\n\t                \"Key {} is not present in registry, returning default value \"\n\t                \"of {}\".format(original_name, default)\n", "            )\n\t        return value\n\t    @classmethod\n\t    def unregister(cls, name):\n\t        r\"\"\"Remove an item from registry with key 'name'\n\t        Args:\n\t            name: Key which needs to be removed.\n\t        Usage::\n\t            from mmf.common.registry import registry\n\t            config = registry.unregister(\"config\")\n", "        \"\"\"\n\t        return cls.mapping[\"state\"].pop(name, None)\n\tregistry = Registry()\n"]}
{"filename": "minigpt4/common/config.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport logging\n\timport json\n\tfrom typing import Dict\n\tfrom omegaconf import OmegaConf\n", "from minigpt4.common.registry import registry\n\tclass Config:\n\t    def __init__(self, args):\n\t        self.config = {}\n\t        self.args = args\n\t        # Register the config and configuration for setup\n\t        registry.register(\"configuration\", self)\n\t        user_config = self._build_opt_list(self.args.options)\n\t        config = OmegaConf.load(self.args.cfg_path)\n\t        runner_config = self.build_runner_config(config)\n", "        model_config = self.build_model_config(config, **user_config)\n\t        dataset_config = self.build_dataset_config(config)\n\t        # Validate the user-provided runner configuration\n\t        # model and dataset configuration are supposed to be validated by the respective classes\n\t        # [TODO] validate the model/dataset configuration\n\t        # self._validate_runner_config(runner_config)\n\t        # Override the default configuration with user options.\n\t        self.config = OmegaConf.merge(\n\t            runner_config, model_config, dataset_config, user_config\n\t        )\n", "    def _validate_runner_config(self, runner_config):\n\t        \"\"\"\n\t        This method validates the configuration, such that\n\t            1) all the user specified options are valid;\n\t            2) no type mismatches between the user specified options and the config.\n\t        \"\"\"\n\t        runner_config_validator = create_runner_config_validator()\n\t        runner_config_validator.validate(runner_config)\n\t    def _build_opt_list(self, opts):\n\t        opts_dot_list = self._convert_to_dot_list(opts)\n", "        return OmegaConf.from_dotlist(opts_dot_list)\n\t    @staticmethod\n\t    def build_model_config(config, **kwargs):\n\t        model = config.get(\"model\", None)\n\t        assert model is not None, \"Missing model configuration file.\"\n\t        model_cls = registry.get_model_class(model.arch)\n\t        assert model_cls is not None, f\"Model '{model.arch}' has not been registered.\"\n\t        model_type = kwargs.get(\"model.model_type\", None)\n\t        if not model_type:\n\t            model_type = model.get(\"model_type\", None)\n", "        # else use the model type selected by user.\n\t        assert model_type is not None, \"Missing model_type.\"\n\t        model_config_path = model_cls.default_config_path(model_type=model_type)\n\t        model_config = OmegaConf.create()\n\t        # hiararchy override, customized config > default config\n\t        model_config = OmegaConf.merge(\n\t            model_config,\n\t            OmegaConf.load(model_config_path),\n\t            {\"model\": config[\"model\"]},\n\t        )\n", "        return model_config\n\t    @staticmethod\n\t    def build_runner_config(config):\n\t        return {\"run\": config.run}\n\t    @staticmethod\n\t    def build_dataset_config(config):\n\t        datasets = config.get(\"datasets\", None)\n\t        if datasets is None:\n\t            raise KeyError(\n\t                \"Expecting 'datasets' as the root key for dataset configuration.\"\n", "            )\n\t        dataset_config = OmegaConf.create()\n\t        for dataset_name in datasets:\n\t            builder_cls = registry.get_builder_class(dataset_name)\n\t            dataset_config_type = datasets[dataset_name].get(\"type\", \"default\")\n\t            dataset_config_path = builder_cls.default_config_path(\n\t                type=dataset_config_type\n\t            )\n\t            # hiararchy override, customized config > default config\n\t            dataset_config = OmegaConf.merge(\n", "                dataset_config,\n\t                OmegaConf.load(dataset_config_path),\n\t                {\"datasets\": {dataset_name: config[\"datasets\"][dataset_name]}},\n\t            )\n\t        return dataset_config\n\t    def _convert_to_dot_list(self, opts):\n\t        if opts is None:\n\t            opts = []\n\t        if len(opts) == 0:\n\t            return opts\n", "        has_equal = opts[0].find(\"=\") != -1\n\t        if has_equal:\n\t            return opts\n\t        return [(opt + \"=\" + value) for opt, value in zip(opts[0::2], opts[1::2])]\n\t    def get_config(self):\n\t        return self.config\n\t    @property\n\t    def run_cfg(self):\n\t        return self.config.run\n\t    @property\n", "    def datasets_cfg(self):\n\t        return self.config.datasets\n\t    @property\n\t    def model_cfg(self):\n\t        return self.config.model\n\t    def pretty_print(self):\n\t        logging.info(\"\\n=====  Running Parameters    =====\")\n\t        logging.info(self._convert_node_to_json(self.config.run))\n\t        logging.info(\"\\n======  Dataset Attributes  ======\")\n\t        datasets = self.config.datasets\n", "        for dataset in datasets:\n\t            if dataset in self.config.datasets:\n\t                logging.info(f\"\\n======== {dataset} =======\")\n\t                dataset_config = self.config.datasets[dataset]\n\t                logging.info(self._convert_node_to_json(dataset_config))\n\t            else:\n\t                logging.warning(f\"No dataset named '{dataset}' in config. Skipping\")\n\t        logging.info(f\"\\n======  Model Attributes  ======\")\n\t        logging.info(self._convert_node_to_json(self.config.model))\n\t    def _convert_node_to_json(self, node):\n", "        container = OmegaConf.to_container(node, resolve=True)\n\t        return json.dumps(container, indent=4, sort_keys=True)\n\t    def to_dict(self):\n\t        return OmegaConf.to_container(self.config)\n\tdef node_to_dict(node):\n\t    return OmegaConf.to_container(node)\n\tclass ConfigValidator:\n\t    \"\"\"\n\t    This is a preliminary implementation to centralize and validate the configuration.\n\t    May be altered in the future.\n", "    A helper class to validate configurations from yaml file.\n\t    This serves the following purposes:\n\t        1. Ensure all the options in the yaml are defined, raise error if not.\n\t        2. when type mismatches are found, the validator will raise an error.\n\t        3. a central place to store and display helpful messages for supported configurations.\n\t    \"\"\"\n\t    class _Argument:\n\t        def __init__(self, name, choices=None, type=None, help=None):\n\t            self.name = name\n\t            self.val = None\n", "            self.choices = choices\n\t            self.type = type\n\t            self.help = help\n\t        def __str__(self):\n\t            s = f\"{self.name}={self.val}\"\n\t            if self.type is not None:\n\t                s += f\", ({self.type})\"\n\t            if self.choices is not None:\n\t                s += f\", choices: {self.choices}\"\n\t            if self.help is not None:\n", "                s += f\", ({self.help})\"\n\t            return s\n\t    def __init__(self, description):\n\t        self.description = description\n\t        self.arguments = dict()\n\t        self.parsed_args = None\n\t    def __getitem__(self, key):\n\t        assert self.parsed_args is not None, \"No arguments parsed yet.\"\n\t        return self.parsed_args[key]\n\t    def __str__(self) -> str:\n", "        return self.format_help()\n\t    def add_argument(self, *args, **kwargs):\n\t        \"\"\"\n\t        Assume the first argument is the name of the argument.\n\t        \"\"\"\n\t        self.arguments[args[0]] = self._Argument(*args, **kwargs)\n\t    def validate(self, config=None):\n\t        \"\"\"\n\t        Convert yaml config (dict-like) to list, required by argparse.\n\t        \"\"\"\n", "        for k, v in config.items():\n\t            assert (\n\t                k in self.arguments\n\t            ), f\"\"\"{k} is not a valid argument. Support arguments are {self.format_arguments()}.\"\"\"\n\t            if self.arguments[k].type is not None:\n\t                try:\n\t                    self.arguments[k].val = self.arguments[k].type(v)\n\t                except ValueError:\n\t                    raise ValueError(f\"{k} is not a valid {self.arguments[k].type}.\")\n\t            if self.arguments[k].choices is not None:\n", "                assert (\n\t                    v in self.arguments[k].choices\n\t                ), f\"\"\"{k} must be one of {self.arguments[k].choices}.\"\"\"\n\t        return config\n\t    def format_arguments(self):\n\t        return str([f\"{k}\" for k in sorted(self.arguments.keys())])\n\t    def format_help(self):\n\t        # description + key-value pair string for each argument\n\t        help_msg = str(self.description)\n\t        return help_msg + \", available arguments: \" + self.format_arguments()\n", "    def print_help(self):\n\t        # display help message\n\t        print(self.format_help())\n\tdef create_runner_config_validator():\n\t    validator = ConfigValidator(description=\"Runner configurations\")\n\t    validator.add_argument(\n\t        \"runner\",\n\t        type=str,\n\t        choices=[\"runner_base\", \"runner_iter\"],\n\t        help=\"\"\"Runner to use. The \"runner_base\" uses epoch-based training while iter-based\n", "            runner runs based on iters. Default: runner_base\"\"\",\n\t    )\n\t    # add argumetns for training dataset ratios\n\t    validator.add_argument(\n\t        \"train_dataset_ratios\",\n\t        type=Dict[str, float],\n\t        help=\"\"\"Ratios of training dataset. This is used in iteration-based runner.\n\t        Do not support for epoch-based runner because how to define an epoch becomes tricky.\n\t        Default: None\"\"\",\n\t    )\n", "    validator.add_argument(\n\t        \"max_iters\",\n\t        type=float,\n\t        help=\"Maximum number of iterations to run.\",\n\t    )\n\t    validator.add_argument(\n\t        \"max_epoch\",\n\t        type=int,\n\t        help=\"Maximum number of epochs to run.\",\n\t    )\n", "    # add arguments for iters_per_inner_epoch\n\t    validator.add_argument(\n\t        \"iters_per_inner_epoch\",\n\t        type=float,\n\t        help=\"Number of iterations per inner epoch. This is required when runner is runner_iter.\",\n\t    )\n\t    lr_scheds_choices = registry.list_lr_schedulers()\n\t    validator.add_argument(\n\t        \"lr_sched\",\n\t        type=str,\n", "        choices=lr_scheds_choices,\n\t        help=\"Learning rate scheduler to use, from {}\".format(lr_scheds_choices),\n\t    )\n\t    task_choices = registry.list_tasks()\n\t    validator.add_argument(\n\t        \"task\",\n\t        type=str,\n\t        choices=task_choices,\n\t        help=\"Task to use, from {}\".format(task_choices),\n\t    )\n", "    # add arguments for init_lr\n\t    validator.add_argument(\n\t        \"init_lr\",\n\t        type=float,\n\t        help=\"Initial learning rate. This will be the learning rate after warmup and before decay.\",\n\t    )\n\t    # add arguments for min_lr\n\t    validator.add_argument(\n\t        \"min_lr\",\n\t        type=float,\n", "        help=\"Minimum learning rate (after decay).\",\n\t    )\n\t    # add arguments for warmup_lr\n\t    validator.add_argument(\n\t        \"warmup_lr\",\n\t        type=float,\n\t        help=\"Starting learning rate for warmup.\",\n\t    )\n\t    # add arguments for learning rate decay rate\n\t    validator.add_argument(\n", "        \"lr_decay_rate\",\n\t        type=float,\n\t        help=\"Learning rate decay rate. Required if using a decaying learning rate scheduler.\",\n\t    )\n\t    # add arguments for weight decay\n\t    validator.add_argument(\n\t        \"weight_decay\",\n\t        type=float,\n\t        help=\"Weight decay rate.\",\n\t    )\n", "    # add arguments for training batch size\n\t    validator.add_argument(\n\t        \"batch_size_train\",\n\t        type=int,\n\t        help=\"Training batch size.\",\n\t    )\n\t    # add arguments for evaluation batch size\n\t    validator.add_argument(\n\t        \"batch_size_eval\",\n\t        type=int,\n", "        help=\"Evaluation batch size, including validation and testing.\",\n\t    )\n\t    # add arguments for number of workers for data loading\n\t    validator.add_argument(\n\t        \"num_workers\",\n\t        help=\"Number of workers for data loading.\",\n\t    )\n\t    # add arguments for warm up steps\n\t    validator.add_argument(\n\t        \"warmup_steps\",\n", "        type=int,\n\t        help=\"Number of warmup steps. Required if a warmup schedule is used.\",\n\t    )\n\t    # add arguments for random seed\n\t    validator.add_argument(\n\t        \"seed\",\n\t        type=int,\n\t        help=\"Random seed.\",\n\t    )\n\t    # add arguments for output directory\n", "    validator.add_argument(\n\t        \"output_dir\",\n\t        type=str,\n\t        help=\"Output directory to save checkpoints and logs.\",\n\t    )\n\t    # add arguments for whether only use evaluation\n\t    validator.add_argument(\n\t        \"evaluate\",\n\t        help=\"Whether to only evaluate the model. If true, training will not be performed.\",\n\t    )\n", "    # add arguments for splits used for training, e.g. [\"train\", \"val\"]\n\t    validator.add_argument(\n\t        \"train_splits\",\n\t        type=list,\n\t        help=\"Splits to use for training.\",\n\t    )\n\t    # add arguments for splits used for validation, e.g. [\"val\"]\n\t    validator.add_argument(\n\t        \"valid_splits\",\n\t        type=list,\n", "        help=\"Splits to use for validation. If not provided, will skip the validation.\",\n\t    )\n\t    # add arguments for splits used for testing, e.g. [\"test\"]\n\t    validator.add_argument(\n\t        \"test_splits\",\n\t        type=list,\n\t        help=\"Splits to use for testing. If not provided, will skip the testing.\",\n\t    )\n\t    # add arguments for accumulating gradient for iterations\n\t    validator.add_argument(\n", "        \"accum_grad_iters\",\n\t        type=int,\n\t        help=\"Number of iterations to accumulate gradient for.\",\n\t    )\n\t    # ====== distributed training ======\n\t    validator.add_argument(\n\t        \"device\",\n\t        type=str,\n\t        choices=[\"cpu\", \"cuda\"],\n\t        help=\"Device to use. Support 'cuda' or 'cpu' as for now.\",\n", "    )\n\t    validator.add_argument(\n\t        \"world_size\",\n\t        type=int,\n\t        help=\"Number of processes participating in the job.\",\n\t    )\n\t    validator.add_argument(\"dist_url\", type=str)\n\t    validator.add_argument(\"distributed\", type=bool)\n\t    # add arguments to opt using distributed sampler during evaluation or not\n\t    validator.add_argument(\n", "        \"use_dist_eval_sampler\",\n\t        type=bool,\n\t        help=\"Whether to use distributed sampler during evaluation or not.\",\n\t    )\n\t    # ====== task specific ======\n\t    # generation task specific arguments\n\t    # add arguments for maximal length of text output\n\t    validator.add_argument(\n\t        \"max_len\",\n\t        type=int,\n", "        help=\"Maximal length of text output.\",\n\t    )\n\t    # add arguments for minimal length of text output\n\t    validator.add_argument(\n\t        \"min_len\",\n\t        type=int,\n\t        help=\"Minimal length of text output.\",\n\t    )\n\t    # add arguments number of beams\n\t    validator.add_argument(\n", "        \"num_beams\",\n\t        type=int,\n\t        help=\"Number of beams used for beam search.\",\n\t    )\n\t    # vqa task specific arguments\n\t    # add arguments for number of answer candidates\n\t    validator.add_argument(\n\t        \"num_ans_candidates\",\n\t        type=int,\n\t        help=\"\"\"For ALBEF and BLIP, these models first rank answers according to likelihood to select answer candidates.\"\"\",\n", "    )\n\t    # add arguments for inference method\n\t    validator.add_argument(\n\t        \"inference_method\",\n\t        type=str,\n\t        choices=[\"genearte\", \"rank\"],\n\t        help=\"\"\"Inference method to use for question answering. If rank, requires a answer list.\"\"\",\n\t    )\n\t    # ====== model specific ======\n\t    validator.add_argument(\n", "        \"k_test\",\n\t        type=int,\n\t        help=\"Number of top k most similar samples from ITC/VTC selection to be tested.\",\n\t    )\n\t    return validator\n"]}
{"filename": "minigpt4/common/dist_utils.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport datetime\n\timport functools\n\timport os\n\timport torch\n", "import torch.distributed as dist\n\timport timm.models.hub as timm_hub\n\tdef setup_for_distributed(is_master):\n\t    \"\"\"\n\t    This function disables printing when not in master process\n\t    \"\"\"\n\t    import builtins as __builtin__\n\t    builtin_print = __builtin__.print\n\t    def print(*args, **kwargs):\n\t        force = kwargs.pop(\"force\", False)\n", "        if is_master or force:\n\t            builtin_print(*args, **kwargs)\n\t    __builtin__.print = print\n\tdef is_dist_avail_and_initialized():\n\t    if not dist.is_available():\n\t        return False\n\t    if not dist.is_initialized():\n\t        return False\n\t    return True\n\tdef get_world_size():\n", "    if not is_dist_avail_and_initialized():\n\t        return 1\n\t    return dist.get_world_size()\n\tdef get_rank():\n\t    if not is_dist_avail_and_initialized():\n\t        return 0\n\t    return dist.get_rank()\n\tdef is_main_process():\n\t    return get_rank() == 0\n\tdef init_distributed_mode(args):\n", "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n\t        args.rank = int(os.environ[\"RANK\"])\n\t        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n\t        args.gpu = int(os.environ[\"LOCAL_RANK\"])\n\t    elif \"SLURM_PROCID\" in os.environ:\n\t        args.rank = int(os.environ[\"SLURM_PROCID\"])\n\t        args.gpu = args.rank % torch.cuda.device_count()\n\t    else:\n\t        print(\"Not using distributed mode\")\n\t        args.distributed = False\n", "        return\n\t    args.distributed = True\n\t    torch.cuda.set_device(args.gpu)\n\t    args.dist_backend = \"nccl\"\n\t    print(\n\t        \"| distributed init (rank {}, world {}): {}\".format(\n\t            args.rank, args.world_size, args.dist_url\n\t        ),\n\t        flush=True,\n\t    )\n", "    torch.distributed.init_process_group(\n\t        backend=args.dist_backend,\n\t        init_method=args.dist_url,\n\t        world_size=args.world_size,\n\t        rank=args.rank,\n\t        timeout=datetime.timedelta(\n\t            days=365\n\t        ),  # allow auto-downloading and de-compressing\n\t    )\n\t    torch.distributed.barrier()\n", "    setup_for_distributed(args.rank == 0)\n\tdef get_dist_info():\n\t    if torch.__version__ < \"1.0\":\n\t        initialized = dist._initialized\n\t    else:\n\t        initialized = dist.is_initialized()\n\t    if initialized:\n\t        rank = dist.get_rank()\n\t        world_size = dist.get_world_size()\n\t    else:  # non-distributed training\n", "        rank = 0\n\t        world_size = 1\n\t    return rank, world_size\n\tdef main_process(func):\n\t    @functools.wraps(func)\n\t    def wrapper(*args, **kwargs):\n\t        rank, _ = get_dist_info()\n\t        if rank == 0:\n\t            return func(*args, **kwargs)\n\t    return wrapper\n", "def download_cached_file(url, check_hash=True, progress=False):\n\t    \"\"\"\n\t    Download a file from a URL and cache it locally. If the file already exists, it is not downloaded again.\n\t    If distributed, only the main process downloads the file, and the other processes wait for the file to be downloaded.\n\t    \"\"\"\n\t    def get_cached_file_path():\n\t        # a hack to sync the file path across processes\n\t        parts = torch.hub.urlparse(url)\n\t        filename = os.path.basename(parts.path)\n\t        cached_file = os.path.join(timm_hub.get_cache_dir(), filename)\n", "        return cached_file\n\t    if is_main_process():\n\t        timm_hub.download_cached_file(url, check_hash, progress)\n\t    if is_dist_avail_and_initialized():\n\t        dist.barrier()\n\t    return get_cached_file_path()\n"]}
{"filename": "minigpt4/common/gradcam.py", "chunked_list": ["import numpy as np\n\tfrom matplotlib import pyplot as plt\n\tfrom scipy.ndimage import filters\n\tfrom skimage import transform as skimage_transform\n\tdef getAttMap(img, attMap, blur=True, overlap=True):\n\t    attMap -= attMap.min()\n\t    if attMap.max() > 0:\n\t        attMap /= attMap.max()\n\t    attMap = skimage_transform.resize(attMap, (img.shape[:2]), order=3, mode=\"constant\")\n\t    if blur:\n", "        attMap = filters.gaussian_filter(attMap, 0.02 * max(img.shape[:2]))\n\t        attMap -= attMap.min()\n\t        attMap /= attMap.max()\n\t    cmap = plt.get_cmap(\"jet\")\n\t    attMapV = cmap(attMap)\n\t    attMapV = np.delete(attMapV, 3, 2)\n\t    if overlap:\n\t        attMap = (\n\t            1 * (1 - attMap**0.7).reshape(attMap.shape + (1,)) * img\n\t            + (attMap**0.7).reshape(attMap.shape + (1,)) * attMapV\n", "        )\n\t    return attMap\n"]}
{"filename": "minigpt4/common/logger.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport datetime\n\timport logging\n\timport time\n\tfrom collections import defaultdict, deque\n", "import torch\n\timport torch.distributed as dist\n\tfrom minigpt4.common import dist_utils\n\tclass SmoothedValue(object):\n\t    \"\"\"Track a series of values and provide access to smoothed values over a\n\t    window or the global series average.\n\t    \"\"\"\n\t    def __init__(self, window_size=20, fmt=None):\n\t        if fmt is None:\n\t            fmt = \"{median:.4f} ({global_avg:.4f})\"\n", "        self.deque = deque(maxlen=window_size)\n\t        self.total = 0.0\n\t        self.count = 0\n\t        self.fmt = fmt\n\t    def update(self, value, n=1):\n\t        self.deque.append(value)\n\t        self.count += n\n\t        self.total += value * n\n\t    def synchronize_between_processes(self):\n\t        \"\"\"\n", "        Warning: does not synchronize the deque!\n\t        \"\"\"\n\t        if not dist_utils.is_dist_avail_and_initialized():\n\t            return\n\t        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n\t        dist.barrier()\n\t        dist.all_reduce(t)\n\t        t = t.tolist()\n\t        self.count = int(t[0])\n\t        self.total = t[1]\n", "    @property\n\t    def median(self):\n\t        d = torch.tensor(list(self.deque))\n\t        return d.median().item()\n\t    @property\n\t    def avg(self):\n\t        d = torch.tensor(list(self.deque), dtype=torch.float32)\n\t        return d.mean().item()\n\t    @property\n\t    def global_avg(self):\n", "        return self.total / self.count\n\t    @property\n\t    def max(self):\n\t        return max(self.deque)\n\t    @property\n\t    def value(self):\n\t        return self.deque[-1]\n\t    def __str__(self):\n\t        return self.fmt.format(\n\t            median=self.median,\n", "            avg=self.avg,\n\t            global_avg=self.global_avg,\n\t            max=self.max,\n\t            value=self.value,\n\t        )\n\tclass MetricLogger(object):\n\t    def __init__(self, delimiter=\"\\t\"):\n\t        self.meters = defaultdict(SmoothedValue)\n\t        self.delimiter = delimiter\n\t    def update(self, **kwargs):\n", "        for k, v in kwargs.items():\n\t            if isinstance(v, torch.Tensor):\n\t                v = v.item()\n\t            assert isinstance(v, (float, int))\n\t            self.meters[k].update(v)\n\t    def __getattr__(self, attr):\n\t        if attr in self.meters:\n\t            return self.meters[attr]\n\t        if attr in self.__dict__:\n\t            return self.__dict__[attr]\n", "        raise AttributeError(\n\t            \"'{}' object has no attribute '{}'\".format(type(self).__name__, attr)\n\t        )\n\t    def __str__(self):\n\t        loss_str = []\n\t        for name, meter in self.meters.items():\n\t            loss_str.append(\"{}: {}\".format(name, str(meter)))\n\t        return self.delimiter.join(loss_str)\n\t    def global_avg(self):\n\t        loss_str = []\n", "        for name, meter in self.meters.items():\n\t            loss_str.append(\"{}: {:.4f}\".format(name, meter.global_avg))\n\t        return self.delimiter.join(loss_str)\n\t    def synchronize_between_processes(self):\n\t        for meter in self.meters.values():\n\t            meter.synchronize_between_processes()\n\t    def add_meter(self, name, meter):\n\t        self.meters[name] = meter\n\t    def log_every(self, iterable, print_freq, header=None):\n\t        i = 0\n", "        if not header:\n\t            header = \"\"\n\t        start_time = time.time()\n\t        end = time.time()\n\t        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n\t        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n\t        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n\t        log_msg = [\n\t            header,\n\t            \"[{0\" + space_fmt + \"}/{1}]\",\n", "            \"eta: {eta}\",\n\t            \"{meters}\",\n\t            \"time: {time}\",\n\t            \"data: {data}\",\n\t        ]\n\t        if torch.cuda.is_available():\n\t            log_msg.append(\"max mem: {memory:.0f}\")\n\t        log_msg = self.delimiter.join(log_msg)\n\t        MB = 1024.0 * 1024.0\n\t        for obj in iterable:\n", "            data_time.update(time.time() - end)\n\t            yield obj\n\t            iter_time.update(time.time() - end)\n\t            if i % print_freq == 0 or i == len(iterable) - 1:\n\t                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n\t                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\t                if torch.cuda.is_available():\n\t                    print(\n\t                        log_msg.format(\n\t                            i,\n", "                            len(iterable),\n\t                            eta=eta_string,\n\t                            meters=str(self),\n\t                            time=str(iter_time),\n\t                            data=str(data_time),\n\t                            memory=torch.cuda.max_memory_allocated() / MB,\n\t                        )\n\t                    )\n\t                else:\n\t                    print(\n", "                        log_msg.format(\n\t                            i,\n\t                            len(iterable),\n\t                            eta=eta_string,\n\t                            meters=str(self),\n\t                            time=str(iter_time),\n\t                            data=str(data_time),\n\t                        )\n\t                    )\n\t            i += 1\n", "            end = time.time()\n\t        total_time = time.time() - start_time\n\t        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t        print(\n\t            \"{} Total time: {} ({:.4f} s / it)\".format(\n\t                header, total_time_str, total_time / len(iterable)\n\t            )\n\t        )\n\tclass AttrDict(dict):\n\t    def __init__(self, *args, **kwargs):\n", "        super(AttrDict, self).__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\tdef setup_logger():\n\t    logging.basicConfig(\n\t        level=logging.INFO if dist_utils.is_main_process() else logging.WARN,\n\t        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n\t        handlers=[logging.StreamHandler()],\n\t    )\n"]}
{"filename": "minigpt4/common/__init__.py", "chunked_list": []}
{"filename": "minigpt4/common/utils.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport io\n\timport json\n\timport logging\n\timport os\n", "import pickle\n\timport re\n\timport shutil\n\timport urllib\n\timport urllib.error\n\timport urllib.request\n\tfrom typing import Optional\n\tfrom urllib.parse import urlparse\n\timport numpy as np\n\timport pandas as pd\n", "import yaml\n\tfrom iopath.common.download import download\n\tfrom iopath.common.file_io import file_lock, g_pathmgr\n\tfrom minigpt4.common.registry import registry\n\tfrom torch.utils.model_zoo import tqdm\n\tfrom torchvision.datasets.utils import (\n\t    check_integrity,\n\t    download_file_from_google_drive,\n\t    extract_archive,\n\t)\n", "def now():\n\t    from datetime import datetime\n\t    return datetime.now().strftime(\"%Y%m%d%H%M\")[:-1]\n\tdef is_url(url_or_filename):\n\t    parsed = urlparse(url_or_filename)\n\t    return parsed.scheme in (\"http\", \"https\")\n\tdef get_cache_path(rel_path):\n\t    return os.path.expanduser(os.path.join(registry.get_path(\"cache_root\"), rel_path))\n\tdef get_abs_path(rel_path):\n\t    return os.path.join(registry.get_path(\"library_root\"), rel_path)\n", "def load_json(filename):\n\t    with open(filename, \"r\") as f:\n\t        return json.load(f)\n\t# The following are adapted from torchvision and vissl\n\t# torchvision: https://github.com/pytorch/vision\n\t# vissl: https://github.com/facebookresearch/vissl/blob/main/vissl/utils/download.py\n\tdef makedir(dir_path):\n\t    \"\"\"\n\t    Create the directory if it does not exist.\n\t    \"\"\"\n", "    is_success = False\n\t    try:\n\t        if not g_pathmgr.exists(dir_path):\n\t            g_pathmgr.mkdirs(dir_path)\n\t        is_success = True\n\t    except BaseException:\n\t        print(f\"Error creating directory: {dir_path}\")\n\t    return is_success\n\tdef get_redirected_url(url: str):\n\t    \"\"\"\n", "    Given a URL, returns the URL it redirects to or the\n\t    original URL in case of no indirection\n\t    \"\"\"\n\t    import requests\n\t    with requests.Session() as session:\n\t        with session.get(url, stream=True, allow_redirects=True) as response:\n\t            if response.history:\n\t                return response.url\n\t            else:\n\t                return url\n", "def to_google_drive_download_url(view_url: str) -> str:\n\t    \"\"\"\n\t    Utility function to transform a view URL of google drive\n\t    to a download URL for google drive\n\t    Example input:\n\t        https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\n\t    Example output:\n\t        https://drive.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n\t    \"\"\"\n\t    splits = view_url.split(\"/\")\n", "    assert splits[-1] == \"view\"\n\t    file_id = splits[-2]\n\t    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\tdef download_google_drive_url(url: str, output_path: str, output_file_name: str):\n\t    \"\"\"\n\t    Download a file from google drive\n\t    Downloading an URL from google drive requires confirmation when\n\t    the file of the size is too big (google drive notifies that\n\t    anti-viral checks cannot be performed on such files)\n\t    \"\"\"\n", "    import requests\n\t    with requests.Session() as session:\n\t        # First get the confirmation token and append it to the URL\n\t        with session.get(url, stream=True, allow_redirects=True) as response:\n\t            for k, v in response.cookies.items():\n\t                if k.startswith(\"download_warning\"):\n\t                    url = url + \"&confirm=\" + v\n\t        # Then download the content of the file\n\t        with session.get(url, stream=True, verify=True) as response:\n\t            makedir(output_path)\n", "            path = os.path.join(output_path, output_file_name)\n\t            total_size = int(response.headers.get(\"Content-length\", 0))\n\t            with open(path, \"wb\") as file:\n\t                from tqdm import tqdm\n\t                with tqdm(total=total_size) as progress_bar:\n\t                    for block in response.iter_content(\n\t                        chunk_size=io.DEFAULT_BUFFER_SIZE\n\t                    ):\n\t                        file.write(block)\n\t                        progress_bar.update(len(block))\n", "def _get_google_drive_file_id(url: str) -> Optional[str]:\n\t    parts = urlparse(url)\n\t    if re.match(r\"(drive|docs)[.]google[.]com\", parts.netloc) is None:\n\t        return None\n\t    match = re.match(r\"/file/d/(?P<id>[^/]*)\", parts.path)\n\t    if match is None:\n\t        return None\n\t    return match.group(\"id\")\n\tdef _urlretrieve(url: str, filename: str, chunk_size: int = 1024) -> None:\n\t    with open(filename, \"wb\") as fh:\n", "        with urllib.request.urlopen(\n\t            urllib.request.Request(url, headers={\"User-Agent\": \"vissl\"})\n\t        ) as response:\n\t            with tqdm(total=response.length) as pbar:\n\t                for chunk in iter(lambda: response.read(chunk_size), \"\"):\n\t                    if not chunk:\n\t                        break\n\t                    pbar.update(chunk_size)\n\t                    fh.write(chunk)\n\tdef download_url(\n", "    url: str,\n\t    root: str,\n\t    filename: Optional[str] = None,\n\t    md5: Optional[str] = None,\n\t) -> None:\n\t    \"\"\"Download a file from a url and place it in root.\n\t    Args:\n\t        url (str): URL to download file from\n\t        root (str): Directory to place downloaded file in\n\t        filename (str, optional): Name to save the file under.\n", "                                  If None, use the basename of the URL.\n\t        md5 (str, optional): MD5 checksum of the download. If None, do not check\n\t    \"\"\"\n\t    root = os.path.expanduser(root)\n\t    if not filename:\n\t        filename = os.path.basename(url)\n\t    fpath = os.path.join(root, filename)\n\t    makedir(root)\n\t    # check if file is already present locally\n\t    if check_integrity(fpath, md5):\n", "        print(\"Using downloaded and verified file: \" + fpath)\n\t        return\n\t    # expand redirect chain if needed\n\t    url = get_redirected_url(url)\n\t    # check if file is located on Google Drive\n\t    file_id = _get_google_drive_file_id(url)\n\t    if file_id is not None:\n\t        return download_file_from_google_drive(file_id, root, filename, md5)\n\t    # download the file\n\t    try:\n", "        print(\"Downloading \" + url + \" to \" + fpath)\n\t        _urlretrieve(url, fpath)\n\t    except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n\t        if url[:5] == \"https\":\n\t            url = url.replace(\"https:\", \"http:\")\n\t            print(\n\t                \"Failed download. Trying https -> http instead.\"\n\t                \" Downloading \" + url + \" to \" + fpath\n\t            )\n\t            _urlretrieve(url, fpath)\n", "        else:\n\t            raise e\n\t    # check integrity of downloaded file\n\t    if not check_integrity(fpath, md5):\n\t        raise RuntimeError(\"File not found or corrupted.\")\n\tdef download_and_extract_archive(\n\t    url: str,\n\t    download_root: str,\n\t    extract_root: Optional[str] = None,\n\t    filename: Optional[str] = None,\n", "    md5: Optional[str] = None,\n\t    remove_finished: bool = False,\n\t) -> None:\n\t    download_root = os.path.expanduser(download_root)\n\t    if extract_root is None:\n\t        extract_root = download_root\n\t    if not filename:\n\t        filename = os.path.basename(url)\n\t    download_url(url, download_root, filename, md5)\n\t    archive = os.path.join(download_root, filename)\n", "    print(\"Extracting {} to {}\".format(archive, extract_root))\n\t    extract_archive(archive, extract_root, remove_finished)\n\tdef cache_url(url: str, cache_dir: str) -> str:\n\t    \"\"\"\n\t    This implementation downloads the remote resource and caches it locally.\n\t    The resource will only be downloaded if not previously requested.\n\t    \"\"\"\n\t    parsed_url = urlparse(url)\n\t    dirname = os.path.join(cache_dir, os.path.dirname(parsed_url.path.lstrip(\"/\")))\n\t    makedir(dirname)\n", "    filename = url.split(\"/\")[-1]\n\t    cached = os.path.join(dirname, filename)\n\t    with file_lock(cached):\n\t        if not os.path.isfile(cached):\n\t            logging.info(f\"Downloading {url} to {cached} ...\")\n\t            cached = download(url, dirname, filename=filename)\n\t    logging.info(f\"URL {url} cached in {cached}\")\n\t    return cached\n\t# TODO (prigoyal): convert this into RAII-style API\n\tdef create_file_symlink(file1, file2):\n", "    \"\"\"\n\t    Simply create the symlinks for a given file1 to file2.\n\t    Useful during model checkpointing to symlinks to the\n\t    latest successful checkpoint.\n\t    \"\"\"\n\t    try:\n\t        if g_pathmgr.exists(file2):\n\t            g_pathmgr.rm(file2)\n\t        g_pathmgr.symlink(file1, file2)\n\t    except Exception as e:\n", "        logging.info(f\"Could NOT create symlink. Error: {e}\")\n\tdef save_file(data, filename, append_to_json=True, verbose=True):\n\t    \"\"\"\n\t    Common i/o utility to handle saving data to various file formats.\n\t    Supported:\n\t        .pkl, .pickle, .npy, .json\n\t    Specifically for .json, users have the option to either append (default)\n\t    or rewrite by passing in Boolean value to append_to_json.\n\t    \"\"\"\n\t    if verbose:\n", "        logging.info(f\"Saving data to file: {filename}\")\n\t    file_ext = os.path.splitext(filename)[1]\n\t    if file_ext in [\".pkl\", \".pickle\"]:\n\t        with g_pathmgr.open(filename, \"wb\") as fopen:\n\t            pickle.dump(data, fopen, pickle.HIGHEST_PROTOCOL)\n\t    elif file_ext == \".npy\":\n\t        with g_pathmgr.open(filename, \"wb\") as fopen:\n\t            np.save(fopen, data)\n\t    elif file_ext == \".json\":\n\t        if append_to_json:\n", "            with g_pathmgr.open(filename, \"a\") as fopen:\n\t                fopen.write(json.dumps(data, sort_keys=True) + \"\\n\")\n\t                fopen.flush()\n\t        else:\n\t            with g_pathmgr.open(filename, \"w\") as fopen:\n\t                fopen.write(json.dumps(data, sort_keys=True) + \"\\n\")\n\t                fopen.flush()\n\t    elif file_ext == \".yaml\":\n\t        with g_pathmgr.open(filename, \"w\") as fopen:\n\t            dump = yaml.dump(data)\n", "            fopen.write(dump)\n\t            fopen.flush()\n\t    else:\n\t        raise Exception(f\"Saving {file_ext} is not supported yet\")\n\t    if verbose:\n\t        logging.info(f\"Saved data to file: {filename}\")\n\tdef load_file(filename, mmap_mode=None, verbose=True, allow_pickle=False):\n\t    \"\"\"\n\t    Common i/o utility to handle loading data from various file formats.\n\t    Supported:\n", "        .pkl, .pickle, .npy, .json\n\t    For the npy files, we support reading the files in mmap_mode.\n\t    If the mmap_mode of reading is not successful, we load data without the\n\t    mmap_mode.\n\t    \"\"\"\n\t    if verbose:\n\t        logging.info(f\"Loading data from file: {filename}\")\n\t    file_ext = os.path.splitext(filename)[1]\n\t    if file_ext == \".txt\":\n\t        with g_pathmgr.open(filename, \"r\") as fopen:\n", "            data = fopen.readlines()\n\t    elif file_ext in [\".pkl\", \".pickle\"]:\n\t        with g_pathmgr.open(filename, \"rb\") as fopen:\n\t            data = pickle.load(fopen, encoding=\"latin1\")\n\t    elif file_ext == \".npy\":\n\t        if mmap_mode:\n\t            try:\n\t                with g_pathmgr.open(filename, \"rb\") as fopen:\n\t                    data = np.load(\n\t                        fopen,\n", "                        allow_pickle=allow_pickle,\n\t                        encoding=\"latin1\",\n\t                        mmap_mode=mmap_mode,\n\t                    )\n\t            except ValueError as e:\n\t                logging.info(\n\t                    f\"Could not mmap {filename}: {e}. Trying without g_pathmgr\"\n\t                )\n\t                data = np.load(\n\t                    filename,\n", "                    allow_pickle=allow_pickle,\n\t                    encoding=\"latin1\",\n\t                    mmap_mode=mmap_mode,\n\t                )\n\t                logging.info(\"Successfully loaded without g_pathmgr\")\n\t            except Exception:\n\t                logging.info(\"Could not mmap without g_pathmgr. Trying without mmap\")\n\t                with g_pathmgr.open(filename, \"rb\") as fopen:\n\t                    data = np.load(fopen, allow_pickle=allow_pickle, encoding=\"latin1\")\n\t        else:\n", "            with g_pathmgr.open(filename, \"rb\") as fopen:\n\t                data = np.load(fopen, allow_pickle=allow_pickle, encoding=\"latin1\")\n\t    elif file_ext == \".json\":\n\t        with g_pathmgr.open(filename, \"r\") as fopen:\n\t            data = json.load(fopen)\n\t    elif file_ext == \".yaml\":\n\t        with g_pathmgr.open(filename, \"r\") as fopen:\n\t            data = yaml.load(fopen, Loader=yaml.FullLoader)\n\t    elif file_ext == \".csv\":\n\t        with g_pathmgr.open(filename, \"r\") as fopen:\n", "            data = pd.read_csv(fopen)\n\t    else:\n\t        raise Exception(f\"Reading from {file_ext} is not supported yet\")\n\t    return data\n\tdef abspath(resource_path: str):\n\t    \"\"\"\n\t    Make a path absolute, but take into account prefixes like\n\t    \"http://\" or \"manifold://\"\n\t    \"\"\"\n\t    regex = re.compile(r\"^\\w+://\")\n", "    if regex.match(resource_path) is None:\n\t        return os.path.abspath(resource_path)\n\t    else:\n\t        return resource_path\n\tdef makedir(dir_path):\n\t    \"\"\"\n\t    Create the directory if it does not exist.\n\t    \"\"\"\n\t    is_success = False\n\t    try:\n", "        if not g_pathmgr.exists(dir_path):\n\t            g_pathmgr.mkdirs(dir_path)\n\t        is_success = True\n\t    except BaseException:\n\t        logging.info(f\"Error creating directory: {dir_path}\")\n\t    return is_success\n\tdef is_url(input_url):\n\t    \"\"\"\n\t    Check if an input string is a url. look for http(s):// and ignoring the case\n\t    \"\"\"\n", "    is_url = re.match(r\"^(?:http)s?://\", input_url, re.IGNORECASE) is not None\n\t    return is_url\n\tdef cleanup_dir(dir):\n\t    \"\"\"\n\t    Utility for deleting a directory. Useful for cleaning the storage space\n\t    that contains various training artifacts like checkpoints, data etc.\n\t    \"\"\"\n\t    if os.path.exists(dir):\n\t        logging.info(f\"Deleting directory: {dir}\")\n\t        shutil.rmtree(dir)\n", "    logging.info(f\"Deleted contents of directory: {dir}\")\n\tdef get_file_size(filename):\n\t    \"\"\"\n\t    Given a file, get the size of file in MB\n\t    \"\"\"\n\t    size_in_mb = os.path.getsize(filename) / float(1024**2)\n\t    return size_in_mb\n"]}
{"filename": "minigpt4/models/blip2.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2023, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport contextlib\n\timport logging\n\timport os\n\timport time\n", "import datetime\n\timport torch\n\timport torch.nn as nn\n\timport torch.distributed as dist\n\timport torch.nn.functional as F\n\timport minigpt4.common.dist_utils as dist_utils\n\tfrom minigpt4.common.dist_utils import download_cached_file\n\tfrom minigpt4.common.utils import is_url\n\tfrom minigpt4.common.logger import MetricLogger\n\tfrom minigpt4.models.base_model import BaseModel\n", "from minigpt4.models.Qformer import BertConfig, BertLMHeadModel\n\tfrom minigpt4.models.eva_vit import create_eva_vit_g\n\tfrom transformers import BertTokenizer\n\tclass Blip2Base(BaseModel):\n\t    @classmethod\n\t    def init_tokenizer(cls):\n\t        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\t        tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n\t        return tokenizer\n\t    def maybe_autocast(self, dtype=torch.float16):\n", "        # if on cpu, don't use autocast\n\t        # if on gpu, use autocast with dtype if provided, otherwise use torch.float16\n\t        enable_autocast = self.device != torch.device(\"cpu\")\n\t        if enable_autocast:\n\t            return torch.cuda.amp.autocast(dtype=dtype)\n\t        else:\n\t            return contextlib.nullcontext()\n\t    @classmethod\n\t    def init_Qformer(cls, num_query_token, vision_width, cross_attention_freq=2):\n\t        encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n", "        encoder_config.encoder_width = vision_width\n\t        # insert cross-attention layer every other block\n\t        encoder_config.add_cross_attention = True\n\t        encoder_config.cross_attention_freq = cross_attention_freq\n\t        encoder_config.query_length = num_query_token\n\t        Qformer = BertLMHeadModel(config=encoder_config)\n\t        query_tokens = nn.Parameter(\n\t            torch.zeros(1, num_query_token, encoder_config.hidden_size)\n\t        )\n\t        query_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\n", "        return Qformer, query_tokens\n\t    @classmethod\n\t    def init_vision_encoder(\n\t        cls, model_name, img_size, drop_path_rate, use_grad_checkpoint, precision\n\t    ):\n\t        assert model_name == \"eva_clip_g\", \"vit model must be eva_clip_g for current version of MiniGPT-4\"\n\t        visual_encoder = create_eva_vit_g(\n\t            img_size, drop_path_rate, use_grad_checkpoint, precision\n\t        )\n\t        ln_vision = LayerNorm(visual_encoder.num_features)\n", "        return visual_encoder, ln_vision\n\t    def load_from_pretrained(self, url_or_filename):\n\t        if is_url(url_or_filename):\n\t            cached_file = download_cached_file(\n\t                url_or_filename, check_hash=False, progress=True\n\t            )\n\t            checkpoint = torch.load(cached_file, map_location=\"cpu\")\n\t        elif os.path.isfile(url_or_filename):\n\t            checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n\t        else:\n", "            raise RuntimeError(\"checkpoint url or path is invalid\")\n\t        state_dict = checkpoint[\"model\"]\n\t        msg = self.load_state_dict(state_dict, strict=False)\n\t        # logging.info(\"Missing keys {}\".format(msg.missing_keys))\n\t        logging.info(\"load checkpoint from %s\" % url_or_filename)\n\t        return msg\n\tdef disabled_train(self, mode=True):\n\t    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n\t    does not change anymore.\"\"\"\n\t    return self\n", "class LayerNorm(nn.LayerNorm):\n\t    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\t    def forward(self, x: torch.Tensor):\n\t        orig_type = x.dtype\n\t        ret = super().forward(x.type(torch.float32))\n\t        return ret.type(orig_type)\n\tdef compute_sim_matrix(model, data_loader, **kwargs):\n\t    k_test = kwargs.pop(\"k_test\")\n\t    metric_logger = MetricLogger(delimiter=\"  \")\n\t    header = \"Evaluation:\"\n", "    logging.info(\"Computing features for evaluation...\")\n\t    start_time = time.time()\n\t    texts = data_loader.dataset.text\n\t    num_text = len(texts)\n\t    text_bs = 256\n\t    text_ids = []\n\t    text_embeds = []\n\t    text_atts = []\n\t    for i in range(0, num_text, text_bs):\n\t        text = texts[i : min(num_text, i + text_bs)]\n", "        text_input = model.tokenizer(\n\t            text,\n\t            padding=\"max_length\",\n\t            truncation=True,\n\t            max_length=35,\n\t            return_tensors=\"pt\",\n\t        ).to(model.device)\n\t        text_feat = model.forward_text(text_input)\n\t        text_embed = F.normalize(model.text_proj(text_feat))\n\t        text_embeds.append(text_embed)\n", "        text_ids.append(text_input.input_ids)\n\t        text_atts.append(text_input.attention_mask)\n\t    text_embeds = torch.cat(text_embeds, dim=0)\n\t    text_ids = torch.cat(text_ids, dim=0)\n\t    text_atts = torch.cat(text_atts, dim=0)\n\t    vit_feats = []\n\t    image_embeds = []\n\t    for samples in data_loader:\n\t        image = samples[\"image\"]\n\t        image = image.to(model.device)\n", "        image_feat, vit_feat = model.forward_image(image)\n\t        image_embed = model.vision_proj(image_feat)\n\t        image_embed = F.normalize(image_embed, dim=-1)\n\t        vit_feats.append(vit_feat.cpu())\n\t        image_embeds.append(image_embed)\n\t    vit_feats = torch.cat(vit_feats, dim=0)\n\t    image_embeds = torch.cat(image_embeds, dim=0)\n\t    sims_matrix = []\n\t    for image_embed in image_embeds:\n\t        sim_q2t = image_embed @ text_embeds.t()\n", "        sim_i2t, _ = sim_q2t.max(0)\n\t        sims_matrix.append(sim_i2t)\n\t    sims_matrix = torch.stack(sims_matrix, dim=0)\n\t    score_matrix_i2t = torch.full(\n\t        (len(data_loader.dataset.image), len(texts)), -100.0\n\t    ).to(model.device)\n\t    num_tasks = dist_utils.get_world_size()\n\t    rank = dist_utils.get_rank()\n\t    step = sims_matrix.size(0) // num_tasks + 1\n\t    start = rank * step\n", "    end = min(sims_matrix.size(0), start + step)\n\t    for i, sims in enumerate(\n\t        metric_logger.log_every(sims_matrix[start:end], 50, header)\n\t    ):\n\t        topk_sim, topk_idx = sims.topk(k=k_test, dim=0)\n\t        image_inputs = vit_feats[start + i].repeat(k_test, 1, 1).to(model.device)\n\t        score = model.compute_itm(\n\t            image_inputs=image_inputs,\n\t            text_ids=text_ids[topk_idx],\n\t            text_atts=text_atts[topk_idx],\n", "        ).float()\n\t        score_matrix_i2t[start + i, topk_idx] = score + topk_sim\n\t    sims_matrix = sims_matrix.t()\n\t    score_matrix_t2i = torch.full(\n\t        (len(texts), len(data_loader.dataset.image)), -100.0\n\t    ).to(model.device)\n\t    step = sims_matrix.size(0) // num_tasks + 1\n\t    start = rank * step\n\t    end = min(sims_matrix.size(0), start + step)\n\t    for i, sims in enumerate(\n", "        metric_logger.log_every(sims_matrix[start:end], 50, header)\n\t    ):\n\t        topk_sim, topk_idx = sims.topk(k=k_test, dim=0)\n\t        image_inputs = vit_feats[topk_idx.cpu()].to(model.device)\n\t        score = model.compute_itm(\n\t            image_inputs=image_inputs,\n\t            text_ids=text_ids[start + i].repeat(k_test, 1),\n\t            text_atts=text_atts[start + i].repeat(k_test, 1),\n\t        ).float()\n\t        score_matrix_t2i[start + i, topk_idx] = score + topk_sim\n", "    if dist_utils.is_dist_avail_and_initialized():\n\t        dist.barrier()\n\t        torch.distributed.all_reduce(\n\t            score_matrix_i2t, op=torch.distributed.ReduceOp.SUM\n\t        )\n\t        torch.distributed.all_reduce(\n\t            score_matrix_t2i, op=torch.distributed.ReduceOp.SUM\n\t        )\n\t    total_time = time.time() - start_time\n\t    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n", "    logging.info(\"Evaluation time {}\".format(total_time_str))\n\t    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()\n"]}
{"filename": "minigpt4/models/eva_vit.py", "chunked_list": ["# Based on EVA, BEIT, timm and DeiT code bases\n\t# https://github.com/baaivision/EVA\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/facebookresearch/deit/\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport math\n\tfrom functools import partial\n\timport torch\n", "import torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.utils.checkpoint as checkpoint\n\tfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\n\tfrom timm.models.registry import register_model\n\tfrom minigpt4.common.dist_utils import download_cached_file\n\tdef _cfg(url='', **kwargs):\n\t    return {\n\t        'url': url,\n\t        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n", "        'crop_pct': .9, 'interpolation': 'bicubic',\n\t        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n\t        **kwargs\n\t    }\n\tclass DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\t    \"\"\"\n\t    def __init__(self, drop_prob=None):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n", "    def forward(self, x):\n\t        return drop_path(x, self.drop_prob, self.training)\n\t    def extra_repr(self) -> str:\n\t        return 'p={}'.format(self.drop_prob)\n\tclass Mlp(nn.Module):\n\t    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n", "        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        # x = self.drop(x)\n\t        # commit this for the orignal BERT implement \n\t        x = self.fc2(x)\n\t        x = self.drop(x)\n", "        return x\n\tclass Attention(nn.Module):\n\t    def __init__(\n\t            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n\t            proj_drop=0., window_size=None, attn_head_dim=None):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n\t        if attn_head_dim is not None:\n\t            head_dim = attn_head_dim\n", "        all_head_dim = head_dim * self.num_heads\n\t        self.scale = qk_scale or head_dim ** -0.5\n\t        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\t        if qkv_bias:\n\t            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n\t            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n\t        else:\n\t            self.q_bias = None\n\t            self.v_bias = None\n\t        if window_size:\n", "            self.window_size = window_size\n\t            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n\t            self.relative_position_bias_table = nn.Parameter(\n\t                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\t            # cls to token & token 2 cls & cls to cls\n\t            # get pair-wise relative position index for each token inside the window\n\t            coords_h = torch.arange(window_size[0])\n\t            coords_w = torch.arange(window_size[1])\n\t            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n\t            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n", "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n\t            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n\t            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n\t            relative_coords[:, :, 1] += window_size[1] - 1\n\t            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n\t            relative_position_index = \\\n\t                torch.zeros(size=(window_size[0] * window_size[1] + 1, ) * 2, dtype=relative_coords.dtype)\n\t            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n\t            relative_position_index[0, 0:] = self.num_relative_distance - 3\n\t            relative_position_index[0:, 0] = self.num_relative_distance - 2\n", "            relative_position_index[0, 0] = self.num_relative_distance - 1\n\t            self.register_buffer(\"relative_position_index\", relative_position_index)\n\t        else:\n\t            self.window_size = None\n\t            self.relative_position_bias_table = None\n\t            self.relative_position_index = None\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(all_head_dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t    def forward(self, x, rel_pos_bias=None):\n", "        B, N, C = x.shape\n\t        qkv_bias = None\n\t        if self.q_bias is not None:\n\t            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\t        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n\t        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n\t        q = q * self.scale\n\t        attn = (q @ k.transpose(-2, -1))\n", "        if self.relative_position_bias_table is not None:\n\t            relative_position_bias = \\\n\t                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n\t                    self.window_size[0] * self.window_size[1] + 1,\n\t                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n\t            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\t            attn = attn + relative_position_bias.unsqueeze(0)\n\t        if rel_pos_bias is not None:\n\t            attn = attn + rel_pos_bias\n\t        attn = attn.softmax(dim=-1)\n", "        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n\tclass Block(nn.Module):\n\t    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n\t                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n\t                 window_size=None, attn_head_dim=None):\n\t        super().__init__()\n", "        self.norm1 = norm_layer(dim)\n\t        self.attn = Attention(\n\t            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n\t        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\t        self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\t        if init_values is not None and init_values > 0:\n", "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\t            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\t        else:\n\t            self.gamma_1, self.gamma_2 = None, None\n\t    def forward(self, x, rel_pos_bias=None):\n\t        if self.gamma_1 is None:\n\t            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n\t            x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        else:\n\t            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n", "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n\t        return x\n\tclass PatchEmbed(nn.Module):\n\t    \"\"\" Image to Patch Embedding\n\t    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n\t        super().__init__()\n\t        img_size = to_2tuple(img_size)\n\t        patch_size = to_2tuple(patch_size)\n\t        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n", "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n\t        self.num_patches = num_patches\n\t        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\t    def forward(self, x, **kwargs):\n\t        B, C, H, W = x.shape\n\t        # FIXME look at relaxing size constraints\n\t        assert H == self.img_size[0] and W == self.img_size[1], \\\n\t            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n", "        x = self.proj(x).flatten(2).transpose(1, 2)\n\t        return x\n\tclass RelativePositionBias(nn.Module):\n\t    def __init__(self, window_size, num_heads):\n\t        super().__init__()\n\t        self.window_size = window_size\n\t        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n\t        self.relative_position_bias_table = nn.Parameter(\n\t            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\t        # cls to token & token 2 cls & cls to cls\n", "        # get pair-wise relative position index for each token inside the window\n\t        coords_h = torch.arange(window_size[0])\n\t        coords_w = torch.arange(window_size[1])\n\t        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n\t        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n\t        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n\t        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n\t        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n\t        relative_coords[:, :, 1] += window_size[1] - 1\n\t        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n", "        relative_position_index = \\\n\t            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n\t        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n\t        relative_position_index[0, 0:] = self.num_relative_distance - 3\n\t        relative_position_index[0:, 0] = self.num_relative_distance - 2\n\t        relative_position_index[0, 0] = self.num_relative_distance - 1\n\t        self.register_buffer(\"relative_position_index\", relative_position_index)\n\t        # trunc_normal_(self.relative_position_bias_table, std=.02)\n\t    def forward(self):\n\t        relative_position_bias = \\\n", "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n\t                self.window_size[0] * self.window_size[1] + 1,\n\t                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n\t        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\tclass VisionTransformer(nn.Module):\n\t    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n\t                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n\t                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n", "                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n\t                 use_mean_pooling=True, init_scale=0.001, use_checkpoint=False):\n\t        super().__init__()\n\t        self.image_size = img_size\n\t        self.num_classes = num_classes\n\t        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\t        self.patch_embed = PatchEmbed(\n\t            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n", "        if use_abs_pos_emb:\n\t            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n\t        else:\n\t            self.pos_embed = None\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        if use_shared_rel_pos_bias:\n\t            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n\t        else:\n\t            self.rel_pos_bias = None\n\t        self.use_checkpoint = use_checkpoint\n", "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\t        self.use_rel_pos_bias = use_rel_pos_bias\n\t        self.blocks = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n\t                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n\t            for i in range(depth)])\n\t#         self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n\t#         self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n", "#         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        if self.pos_embed is not None:\n\t            trunc_normal_(self.pos_embed, std=.02)\n\t        trunc_normal_(self.cls_token, std=.02)\n\t        # trunc_normal_(self.mask_token, std=.02)\n\t#         if isinstance(self.head, nn.Linear):\n\t#             trunc_normal_(self.head.weight, std=.02)\n\t        self.apply(self._init_weights)\n\t        self.fix_init_weight()\n\t#         if isinstance(self.head, nn.Linear):\n", "#             self.head.weight.data.mul_(init_scale)\n\t#             self.head.bias.data.mul_(init_scale)\n\t    def fix_init_weight(self):\n\t        def rescale(param, layer_id):\n\t            param.div_(math.sqrt(2.0 * layer_id))\n\t        for layer_id, layer in enumerate(self.blocks):\n\t            rescale(layer.attn.proj.weight.data, layer_id + 1)\n\t            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n", "            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def get_classifier(self):\n\t        return self.head\n\t    def reset_classifier(self, num_classes, global_pool=''):\n\t        self.num_classes = num_classes\n", "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t    def forward_features(self, x):\n\t        x = self.patch_embed(x)\n\t        batch_size, seq_len, _ = x.size()\n\t        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n\t        x = torch.cat((cls_tokens, x), dim=1)\n\t        if self.pos_embed is not None:\n\t            x = x + self.pos_embed\n\t        x = self.pos_drop(x)\n\t        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n", "        for blk in self.blocks:\n\t            if self.use_checkpoint:\n\t                x = checkpoint.checkpoint(blk, x, rel_pos_bias)\n\t            else:\n\t                x = blk(x, rel_pos_bias)\n\t        return x\n\t#         x = self.norm(x)\n\t#         if self.fc_norm is not None:\n\t#             t = x[:, 1:, :]\n\t#             return self.fc_norm(t.mean(1))\n", "#         else:\n\t#             return x[:, 0]\n\t    def forward(self, x):\n\t        x = self.forward_features(x)\n\t#         x = self.head(x)\n\t        return x\n\t    def get_intermediate_layers(self, x):\n\t        x = self.patch_embed(x)\n\t        batch_size, seq_len, _ = x.size()\n\t        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n", "        x = torch.cat((cls_tokens, x), dim=1)\n\t        if self.pos_embed is not None:\n\t            x = x + self.pos_embed\n\t        x = self.pos_drop(x)\n\t        features = []\n\t        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n\t        for blk in self.blocks:\n\t            x = blk(x, rel_pos_bias)\n\t            features.append(x)\n\t        return features\n", "def interpolate_pos_embed(model, checkpoint_model):\n\t    if 'pos_embed' in checkpoint_model:\n\t        pos_embed_checkpoint = checkpoint_model['pos_embed'].float()\n\t        embedding_size = pos_embed_checkpoint.shape[-1]\n\t        num_patches = model.patch_embed.num_patches\n\t        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n\t        # height (== width) for the checkpoint position embedding\n\t        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n\t        # height (== width) for the new position embedding\n\t        new_size = int(num_patches ** 0.5)\n", "        # class_token and dist_token are kept unchanged\n\t        if orig_size != new_size:\n\t            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n\t            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n\t            # only the position tokens are interpolated\n\t            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n\t            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n\t            pos_tokens = torch.nn.functional.interpolate(\n\t                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n\t            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n", "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n\t            checkpoint_model['pos_embed'] = new_pos_embed\n\tdef convert_weights_to_fp16(model: nn.Module):\n\t    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\t    def _convert_weights_to_fp16(l):\n\t        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n\t            l.weight.data = l.weight.data.half()\n\t            if l.bias is not None:\n\t                l.bias.data = l.bias.data.half()\n\t#         if isinstance(l, (nn.MultiheadAttention, Attention)):\n", "#             for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n\t#                 tensor = getattr(l, attr)\n\t#                 if tensor is not None:\n\t#                     tensor.data = tensor.data.half()\n\t    model.apply(_convert_weights_to_fp16)\n\tdef create_eva_vit_g(img_size=224,drop_path_rate=0.4,use_checkpoint=False,precision=\"fp16\"):\n\t    model = VisionTransformer(\n\t        img_size=img_size,\n\t        patch_size=14,\n\t        use_mean_pooling=False,\n", "        embed_dim=1408,\n\t        depth=39,\n\t        num_heads=1408//88,\n\t        mlp_ratio=4.3637,\n\t        qkv_bias=True,\n\t        drop_path_rate=drop_path_rate,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        use_checkpoint=use_checkpoint,\n\t    )  \n\t    url = \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth\"\n", "    cached_file = download_cached_file(\n\t        url, check_hash=False, progress=True\n\t    )\n\t    state_dict = torch.load(cached_file, map_location=\"cpu\")    \n\t    interpolate_pos_embed(model,state_dict)\n\t    incompatible_keys = model.load_state_dict(state_dict, strict=False)\n\t#     print(incompatible_keys)\n\t    if precision == \"fp16\":\n\t#         model.to(\"cuda\") \n\t        convert_weights_to_fp16(model)\n", "    return model"]}
{"filename": "minigpt4/models/blip2_outputs.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional\n\timport torch\n\tfrom transformers.modeling_outputs import (\n", "    ModelOutput,\n\t    BaseModelOutputWithPoolingAndCrossAttentions,\n\t    CausalLMOutputWithCrossAttentions,\n\t)\n\t@dataclass\n\tclass BlipSimilarity(ModelOutput):\n\t    sim_i2t: torch.FloatTensor = None\n\t    sim_t2i: torch.FloatTensor = None\n\t    sim_i2t_m: Optional[torch.FloatTensor] = None\n\t    sim_t2i_m: Optional[torch.FloatTensor] = None\n", "    sim_i2t_targets: Optional[torch.FloatTensor] = None\n\t    sim_t2i_targets: Optional[torch.FloatTensor] = None\n\t@dataclass\n\tclass BlipIntermediateOutput(ModelOutput):\n\t    \"\"\"\n\t    Data class for intermediate outputs of BLIP models.\n\t    image_embeds (torch.FloatTensor): Image embeddings, shape (batch_size, num_patches, embed_dim).\n\t    text_embeds (torch.FloatTensor): Text embeddings, shape (batch_size, seq_len, embed_dim).\n\t    image_embeds_m (torch.FloatTensor): Image embeddings from momentum visual encoder, shape (batch_size, num_patches, embed_dim).\n\t    text_embeds_m (torch.FloatTensor): Text embeddings from momentum text encoder, shape (batch_size, seq_len, embed_dim).\n", "    encoder_output (BaseModelOutputWithPoolingAndCrossAttentions): output from the image-grounded text encoder.\n\t    encoder_output_neg (BaseModelOutputWithPoolingAndCrossAttentions): output from the image-grounded text encoder for negative pairs.\n\t    decoder_output (CausalLMOutputWithCrossAttentions): output from the image-grounded text decoder.\n\t    decoder_labels (torch.LongTensor): labels for the captioning loss.\n\t    itm_logits (torch.FloatTensor): logits for the image-text matching loss, shape (batch_size * 3, 2).\n\t    itm_labels (torch.LongTensor): labels for the image-text matching loss, shape (batch_size * 3,)\n\t    \"\"\"\n\t    # uni-modal features\n\t    image_embeds: torch.FloatTensor = None\n\t    text_embeds: Optional[torch.FloatTensor] = None\n", "    image_embeds_m: Optional[torch.FloatTensor] = None\n\t    text_embeds_m: Optional[torch.FloatTensor] = None\n\t    # intermediate outputs of multimodal encoder\n\t    encoder_output: Optional[BaseModelOutputWithPoolingAndCrossAttentions] = None\n\t    encoder_output_neg: Optional[BaseModelOutputWithPoolingAndCrossAttentions] = None\n\t    itm_logits: Optional[torch.FloatTensor] = None\n\t    itm_labels: Optional[torch.LongTensor] = None\n\t    # intermediate outputs of multimodal decoder\n\t    decoder_output: Optional[CausalLMOutputWithCrossAttentions] = None\n\t    decoder_labels: Optional[torch.LongTensor] = None\n", "@dataclass\n\tclass BlipOutput(ModelOutput):\n\t    # some finetuned models (e.g. BlipVQA) do not compute similarity, thus optional.\n\t    sims: Optional[BlipSimilarity] = None\n\t    intermediate_output: BlipIntermediateOutput = None\n\t    loss: Optional[torch.FloatTensor] = None\n\t    loss_itc: Optional[torch.FloatTensor] = None\n\t    loss_itm: Optional[torch.FloatTensor] = None\n\t    loss_lm: Optional[torch.FloatTensor] = None\n\t@dataclass\n", "class BlipOutputFeatures(ModelOutput):\n\t    \"\"\"\n\t    Data class of features from BlipFeatureExtractor.\n\t    Args:\n\t        image_embeds: (torch.FloatTensor) of shape (batch_size, num_patches+1, embed_dim), optional\n\t        image_features: (torch.FloatTensor) of shape (batch_size, num_patches+1, feature_dim), optional\n\t        text_embeds: (torch.FloatTensor) of shape (batch_size, sequence_length+1, embed_dim), optional\n\t        text_features: (torch.FloatTensor) of shape (batch_size, sequence_length+1, feature_dim), optional\n\t        The first embedding or feature is for the [CLS] token.\n\t        Features are obtained by projecting the corresponding embedding into a normalized low-dimensional space.\n", "    \"\"\"\n\t    image_embeds: Optional[torch.FloatTensor] = None\n\t    image_embeds_proj: Optional[torch.FloatTensor] = None\n\t    text_embeds: Optional[torch.FloatTensor] = None\n\t    text_embeds_proj: Optional[torch.FloatTensor] = None\n\t    multimodal_embeds: Optional[torch.FloatTensor] = None\n"]}
{"filename": "minigpt4/models/modeling_llama.py", "chunked_list": ["# coding=utf-8\n\t# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n\t#\n\t# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n\t# and OPT implementations in this library. It has been modified from its\n\t# original forms to accommodate minor architectural differences compared\n\t# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n", "# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\" PyTorch LLaMA model.\"\"\"\n", "import math\n\tfrom typing import List, Optional, Tuple, Union\n\timport torch\n\timport torch.utils.checkpoint\n\tfrom torch import nn\n\tfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\tfrom transformers.activations import ACT2FN\n\tfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n\tfrom transformers.modeling_utils import PreTrainedModel\n\tfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n", "from transformers.models.llama.configuration_llama import LlamaConfig\n\tlogger = logging.get_logger(__name__)\n\t_CONFIG_FOR_DOC = \"LlamaConfig\"\n\t# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n\tdef _make_causal_mask(\n\t    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n\t):\n\t    \"\"\"\n\t    Make causal mask used for bi-directional self-attention.\n\t    \"\"\"\n", "    bsz, tgt_len = input_ids_shape\n\t    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n\t    mask_cond = torch.arange(mask.size(-1), device=device)\n\t    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n\t    mask = mask.to(dtype)\n\t    if past_key_values_length > 0:\n\t        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n\t    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\t# Copied from transformers.models.bart.modeling_bart._expand_mask\n\tdef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n", "    \"\"\"\n\t    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n\t    \"\"\"\n\t    bsz, src_len = mask.size()\n\t    tgt_len = tgt_len if tgt_len is not None else src_len\n\t    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\t    inverted_mask = 1.0 - expanded_mask\n\t    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\tclass LlamaRMSNorm(nn.Module):\n\t    def __init__(self, hidden_size, eps=1e-6):\n", "        \"\"\"\n\t        LlamaRMSNorm is equivalent to T5LayerNorm\n\t        \"\"\"\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(hidden_size))\n\t        self.variance_epsilon = eps\n\t    def forward(self, hidden_states):\n\t        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n\t        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\t        # convert into half-precision if necessary\n", "        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n\t            hidden_states = hidden_states.to(self.weight.dtype)\n\t        return self.weight * hidden_states\n\tclass LlamaRotaryEmbedding(torch.nn.Module):\n\t    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n\t        super().__init__()\n\t        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n\t        self.register_buffer(\"inv_freq\", inv_freq)\n\t        # Build here to make `torch.jit.trace` work.\n\t        self.max_seq_len_cached = max_position_embeddings\n", "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n\t        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n\t        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n\t        emb = torch.cat((freqs, freqs), dim=-1)\n\t        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n\t        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\t    def forward(self, x, seq_len=None):\n\t        # x: [bs, num_attention_heads, seq_len, head_size]\n\t        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n\t        if seq_len > self.max_seq_len_cached:\n", "            self.max_seq_len_cached = seq_len\n\t            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n\t            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n\t            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n\t            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n\t            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n\t            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\t        return (\n\t            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n\t            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n", "        )\n\tdef rotate_half(x):\n\t    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n\t    x1 = x[..., : x.shape[-1] // 2]\n\t    x2 = x[..., x.shape[-1] // 2 :]\n\t    return torch.cat((-x2, x1), dim=-1)\n\tdef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n\t    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n\t    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n\t    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n", "    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n\t    q_embed = (q * cos) + (rotate_half(q) * sin)\n\t    k_embed = (k * cos) + (rotate_half(k) * sin)\n\t    return q_embed, k_embed\n\tclass LlamaMLP(nn.Module):\n\t    def __init__(\n\t        self,\n\t        hidden_size: int,\n\t        intermediate_size: int,\n\t        hidden_act: str,\n", "    ):\n\t        super().__init__()\n\t        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n\t        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n\t        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n\t        self.act_fn = ACT2FN[hidden_act]\n\t    def forward(self, x):\n\t        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\tclass LlamaAttention(nn.Module):\n\t    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n", "    def __init__(self, config: LlamaConfig):\n\t        super().__init__()\n\t        self.config = config\n\t        self.hidden_size = config.hidden_size\n\t        self.num_heads = config.num_attention_heads\n\t        self.head_dim = self.hidden_size // self.num_heads\n\t        self.max_position_embeddings = config.max_position_embeddings\n\t        if (self.head_dim * self.num_heads) != self.hidden_size:\n\t            raise ValueError(\n\t                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n", "                f\" and `num_heads`: {self.num_heads}).\"\n\t            )\n\t        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n\t        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n\t        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n\t        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\t        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\t    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n\t        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\t    def forward(\n", "        self,\n\t        hidden_states: torch.Tensor,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t        output_attentions: bool = False,\n\t        use_cache: bool = False,\n\t    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\t        bsz, q_len, _ = hidden_states.size()\n\t        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n", "        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t        kv_seq_len = key_states.shape[-2]\n\t        if past_key_value is not None:\n\t            kv_seq_len += past_key_value[0].shape[-2]\n\t        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\t        # [bsz, nh, t, hd]\n\t        if past_key_value is not None:\n\t            # reuse k, v, self_attention\n", "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n\t            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\t        past_key_value = (key_states, value_states) if use_cache else None\n\t        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\t        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n\t            raise ValueError(\n\t                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n\t                f\" {attn_weights.size()}\"\n\t            )\n\t        if attention_mask is not None:\n", "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n\t                raise ValueError(\n\t                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n\t                )\n\t            attn_weights = attn_weights + attention_mask\n\t            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\t        # upcast attention to fp32\n\t        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\t        attn_output = torch.matmul(attn_weights, value_states)\n\t        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n", "            raise ValueError(\n\t                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n\t                f\" {attn_output.size()}\"\n\t            )\n\t        attn_output = attn_output.transpose(1, 2)\n\t        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\t        attn_output = self.o_proj(attn_output)\n\t        if not output_attentions:\n\t            attn_weights = None\n\t        return attn_output, attn_weights, past_key_value\n", "class LlamaDecoderLayer(nn.Module):\n\t    def __init__(self, config: LlamaConfig):\n\t        super().__init__()\n\t        self.hidden_size = config.hidden_size\n\t        self.self_attn = LlamaAttention(config=config)\n\t        self.mlp = LlamaMLP(\n\t            hidden_size=self.hidden_size,\n\t            intermediate_size=config.intermediate_size,\n\t            hidden_act=config.hidden_act,\n\t        )\n", "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\t        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\t    def forward(\n\t        self,\n\t        hidden_states: torch.Tensor,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t        output_attentions: Optional[bool] = False,\n\t        use_cache: Optional[bool] = False,\n", "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\t        \"\"\"\n\t        Args:\n\t            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n\t            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n\t                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n\t            output_attentions (`bool`, *optional*):\n\t                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n\t                returned tensors for more detail.\n\t            use_cache (`bool`, *optional*):\n", "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n\t                (see `past_key_values`).\n\t            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n\t        \"\"\"\n\t        residual = hidden_states\n\t        hidden_states = self.input_layernorm(hidden_states)\n\t        # Self Attention\n\t        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\t            hidden_states=hidden_states,\n\t            attention_mask=attention_mask,\n", "            position_ids=position_ids,\n\t            past_key_value=past_key_value,\n\t            output_attentions=output_attentions,\n\t            use_cache=use_cache,\n\t        )\n\t        hidden_states = residual + hidden_states\n\t        # Fully Connected\n\t        residual = hidden_states\n\t        hidden_states = self.post_attention_layernorm(hidden_states)\n\t        hidden_states = self.mlp(hidden_states)\n", "        hidden_states = residual + hidden_states\n\t        outputs = (hidden_states,)\n\t        if output_attentions:\n\t            outputs += (self_attn_weights,)\n\t        if use_cache:\n\t            outputs += (present_key_value,)\n\t        return outputs\n\tLLAMA_START_DOCSTRING = r\"\"\"\n\t    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n\t    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n", "    etc.)\n\t    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n\t    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n\t    and behavior.\n\t    Parameters:\n\t        config ([`LlamaConfig`]):\n\t            Model configuration class with all the parameters of the model. Initializing with a config file does not\n\t            load the weights associated with the model, only the configuration. Check out the\n\t            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\t\"\"\"\n", "@add_start_docstrings(\n\t    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n\t    LLAMA_START_DOCSTRING,\n\t)\n\tclass LlamaPreTrainedModel(PreTrainedModel):\n\t    config_class = LlamaConfig\n\t    base_model_prefix = \"model\"\n\t    supports_gradient_checkpointing = True\n\t    _no_split_modules = [\"LlamaDecoderLayer\"]\n\t    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n", "    def _init_weights(self, module):\n\t        std = self.config.initializer_range\n\t        if isinstance(module, nn.Linear):\n\t            module.weight.data.normal_(mean=0.0, std=std)\n\t            if module.bias is not None:\n\t                module.bias.data.zero_()\n\t        elif isinstance(module, nn.Embedding):\n\t            module.weight.data.normal_(mean=0.0, std=std)\n\t            if module.padding_idx is not None:\n\t                module.weight.data[module.padding_idx].zero_()\n", "    def _set_gradient_checkpointing(self, module, value=False):\n\t        if isinstance(module, LlamaModel):\n\t            module.gradient_checkpointing = value\n\tLLAMA_INPUTS_DOCSTRING = r\"\"\"\n\t    Args:\n\t        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n\t            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n\t            it.\n\t            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n\t            [`PreTrainedTokenizer.__call__`] for details.\n", "            [What are input IDs?](../glossary#input-ids)\n\t        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n\t            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\t            - 1 for tokens that are **not masked**,\n\t            - 0 for tokens that are **masked**.\n\t            [What are attention masks?](../glossary#attention-mask)\n\t            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n\t            [`PreTrainedTokenizer.__call__`] for details.\n\t            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n\t            `past_key_values`).\n", "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n\t            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n\t            information on the default strategy.\n\t            - 1 indicates the head is **not masked**,\n\t            - 0 indicates the head is **masked**.\n\t        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n\t            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n\t            config.n_positions - 1]`.\n\t            [What are position IDs?](../glossary#position-ids)\n\t        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n", "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n\t            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n\t            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\t            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n\t            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\t            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n\t            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n\t            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n\t        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n\t            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n", "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n\t            model's internal embedding lookup matrix.\n\t        use_cache (`bool`, *optional*):\n\t            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n\t            `past_key_values`).\n\t        output_attentions (`bool`, *optional*):\n\t            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n\t            tensors for more detail.\n\t        output_hidden_states (`bool`, *optional*):\n\t            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n", "            more detail.\n\t        return_dict (`bool`, *optional*):\n\t            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\t\"\"\"\n\t@add_start_docstrings(\n\t    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n\t    LLAMA_START_DOCSTRING,\n\t)\n\tclass LlamaModel(LlamaPreTrainedModel):\n\t    \"\"\"\n", "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\t    Args:\n\t        config: LlamaConfig\n\t    \"\"\"\n\t    def __init__(self, config: LlamaConfig):\n\t        super().__init__(config)\n\t        self.padding_idx = config.pad_token_id\n\t        self.vocab_size = config.vocab_size\n\t        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n\t        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n", "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\t        self.gradient_checkpointing = False\n\t        # Initialize weights and apply final processing\n\t        self.post_init()\n\t    def get_input_embeddings(self):\n\t        return self.embed_tokens\n\t    def set_input_embeddings(self, value):\n\t        self.embed_tokens = value\n\t    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n\t    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n", "        # create causal mask\n\t        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n\t        combined_attention_mask = None\n\t        if input_shape[-1] > 1:\n\t            combined_attention_mask = _make_causal_mask(\n\t                input_shape,\n\t                inputs_embeds.dtype,\n\t                device=inputs_embeds.device,\n\t                past_key_values_length=past_key_values_length,\n\t            )\n", "        if attention_mask is not None:\n\t            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n\t            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n\t                inputs_embeds.device\n\t            )\n\t            combined_attention_mask = (\n\t                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n\t            )\n\t        return combined_attention_mask\n\t    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n", "    def forward(\n\t        self,\n\t        input_ids: torch.LongTensor = None,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_values: Optional[List[torch.FloatTensor]] = None,\n\t        inputs_embeds: Optional[torch.FloatTensor] = None,\n\t        query_embeds: Optional[torch.FloatTensor] = None,\n\t        use_cache: Optional[bool] = None,\n\t        output_attentions: Optional[bool] = None,\n", "        output_hidden_states: Optional[bool] = None,\n\t        return_dict: Optional[bool] = None,\n\t    ) -> Union[Tuple, BaseModelOutputWithPast]:\n\t        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n\t        output_hidden_states = (\n\t            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n\t        )\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\t        # retrieve input_ids and inputs_embeds\n", "        if input_ids is not None and inputs_embeds is not None:\n\t            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n\t        elif input_ids is not None:\n\t            batch_size, seq_length = input_ids.shape\n\t        elif inputs_embeds is not None:\n\t            batch_size, seq_length, _ = inputs_embeds.shape\n\t        else:\n\t            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\t        if inputs_embeds is None:\n\t            inputs_embeds = self.embed_tokens(input_ids)\n", "        if query_embeds is not None:\n\t            inputs_embeds = torch.cat([query_embeds, inputs_embeds], dim=1)\n\t            batch_size, seq_length, _ = inputs_embeds.shape\n\t        seq_length_with_past = seq_length\n\t        past_key_values_length = 0\n\t        if past_key_values is not None:\n\t            past_key_values_length = past_key_values[0][0].shape[2]\n\t            seq_length_with_past = seq_length_with_past + past_key_values_length\n\t        if position_ids is None:\n\t            device = input_ids.device if input_ids is not None else inputs_embeds.device\n", "            position_ids = torch.arange(\n\t                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n\t            )\n\t            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n\t        else:\n\t            position_ids = position_ids.view(-1, seq_length).long()\n\t        # embed positions\n\t        if attention_mask is None:\n\t            attention_mask = torch.ones(\n\t                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n", "            )\n\t        attention_mask = self._prepare_decoder_attention_mask(\n\t            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n\t        )\n\t        hidden_states = inputs_embeds\n\t        if self.gradient_checkpointing and self.training:\n\t            if use_cache:\n\t                logger.warning_once(\n\t                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n\t                )\n", "                use_cache = False\n\t        # decoder layers\n\t        all_hidden_states = () if output_hidden_states else None\n\t        all_self_attns = () if output_attentions else None\n\t        next_decoder_cache = () if use_cache else None\n\t        for idx, decoder_layer in enumerate(self.layers):\n\t            if output_hidden_states:\n\t                all_hidden_states += (hidden_states,)\n\t            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\t            if self.gradient_checkpointing and self.training:\n", "                def create_custom_forward(module):\n\t                    def custom_forward(*inputs):\n\t                        # None for past_key_value\n\t                        return module(*inputs, output_attentions, None)\n\t                    return custom_forward\n\t                layer_outputs = torch.utils.checkpoint.checkpoint(\n\t                    create_custom_forward(decoder_layer),\n\t                    hidden_states,\n\t                    attention_mask,\n\t                    position_ids,\n", "                    None,\n\t                )\n\t            else:\n\t                layer_outputs = decoder_layer(\n\t                    hidden_states,\n\t                    attention_mask=attention_mask,\n\t                    position_ids=position_ids,\n\t                    past_key_value=past_key_value,\n\t                    output_attentions=output_attentions,\n\t                    use_cache=use_cache,\n", "                )\n\t            hidden_states = layer_outputs[0]\n\t            if use_cache:\n\t                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\t            if output_attentions:\n\t                all_self_attns += (layer_outputs[1],)\n\t        hidden_states = self.norm(hidden_states)\n\t        # add hidden states from the last decoder layer\n\t        if output_hidden_states:\n\t            all_hidden_states += (hidden_states,)\n", "        next_cache = next_decoder_cache if use_cache else None\n\t        if not return_dict:\n\t            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n\t        return BaseModelOutputWithPast(\n\t            last_hidden_state=hidden_states,\n\t            past_key_values=next_cache,\n\t            hidden_states=all_hidden_states,\n\t            attentions=all_self_attns,\n\t        )\n\tclass LlamaForCausalLM(LlamaPreTrainedModel):\n", "    def __init__(self, config):\n\t        super().__init__(config)\n\t        self.model = LlamaModel(config)\n\t        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\t        # Initialize weights and apply final processing\n\t        self.post_init()\n\t    def get_input_embeddings(self):\n\t        return self.model.embed_tokens\n\t    def set_input_embeddings(self, value):\n\t        self.model.embed_tokens = value\n", "    def get_output_embeddings(self):\n\t        return self.lm_head\n\t    def set_output_embeddings(self, new_embeddings):\n\t        self.lm_head = new_embeddings\n\t    def set_decoder(self, decoder):\n\t        self.model = decoder\n\t    def get_decoder(self):\n\t        return self.model\n\t    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n\t    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n", "    def forward(\n\t        self,\n\t        input_ids: torch.LongTensor = None,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_values: Optional[List[torch.FloatTensor]] = None,\n\t        inputs_embeds: Optional[torch.FloatTensor] = None,\n\t        query_embeds: Optional[torch.FloatTensor] = None,\n\t        labels: Optional[torch.LongTensor] = None,\n\t        use_cache: Optional[bool] = None,\n", "        output_attentions: Optional[bool] = None,\n\t        output_hidden_states: Optional[bool] = None,\n\t        return_dict: Optional[bool] = None,\n\t    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\t        r\"\"\"\n\t        Args:\n\t            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n\t                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n\t                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n\t                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n", "        Returns:\n\t        Example:\n\t        ```python\n\t        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\t        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n\t        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\t        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n\t        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\t        >>> # Generate\n\t        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n", "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\t        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n\t        ```\"\"\"\n\t        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n\t        output_hidden_states = (\n\t            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n\t        )\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\t        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n\t        outputs = self.model(\n", "            input_ids=input_ids,\n\t            attention_mask=attention_mask,\n\t            position_ids=position_ids,\n\t            past_key_values=past_key_values,\n\t            inputs_embeds=inputs_embeds,\n\t            query_embeds=query_embeds,\n\t            use_cache=use_cache,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n", "        )\n\t        hidden_states = outputs[0]\n\t        logits = self.lm_head(hidden_states)\n\t        loss = None\n\t        if labels is not None:\n\t            # Shift so that tokens < n predict n\n\t            shift_logits = logits[..., :-1, :].contiguous()\n\t            shift_labels = labels[..., 1:].contiguous()\n\t            # Flatten the tokens\n\t            loss_fct = CrossEntropyLoss()\n", "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n\t            shift_labels = shift_labels.view(-1)\n\t            # Enable model parallelism\n\t            shift_labels = shift_labels.to(shift_logits.device)\n\t            loss = loss_fct(shift_logits, shift_labels)\n\t        if not return_dict:\n\t            output = (logits,) + outputs[1:]\n\t            return (loss,) + output if loss is not None else output\n\t        return CausalLMOutputWithPast(\n\t            loss=loss,\n", "            logits=logits,\n\t            past_key_values=outputs.past_key_values,\n\t            hidden_states=outputs.hidden_states,\n\t            attentions=outputs.attentions,\n\t        )\n\t    def prepare_inputs_for_generation(\n\t        self, input_ids, query_embeds=None, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n\t    ):\n\t        if past_key_values:\n\t            input_ids = input_ids[:, -1:]\n", "        position_ids = kwargs.get(\"position_ids\", None)\n\t        if attention_mask is not None and position_ids is None:\n\t            # create position_ids on the fly for batch generation\n\t            position_ids = attention_mask.long().cumsum(-1) - 1\n\t            position_ids.masked_fill_(attention_mask == 0, 1)\n\t            if past_key_values:\n\t                position_ids = position_ids[:, -1].unsqueeze(-1)\n\t                query_embeds = None\n\t        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n\t        if inputs_embeds is not None and past_key_values is None:\n", "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n\t        else:\n\t            model_inputs = {\"input_ids\": input_ids}\n\t        model_inputs.update(\n\t            {\n\t                \"position_ids\": position_ids,\n\t                \"query_embeds\": query_embeds,\n\t                \"past_key_values\": past_key_values,\n\t                \"use_cache\": kwargs.get(\"use_cache\"),\n\t                \"attention_mask\": attention_mask,\n", "            }\n\t        )\n\t        return model_inputs\n\t    @staticmethod\n\t    def _reorder_cache(past_key_values, beam_idx):\n\t        reordered_past = ()\n\t        for layer_past in past_key_values:\n\t            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n\t        return reordered_past\n"]}
{"filename": "minigpt4/models/__init__.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport logging\n\timport torch\n\tfrom omegaconf import OmegaConf\n\tfrom minigpt4.common.registry import registry\n", "from minigpt4.models.base_model import BaseModel\n\tfrom minigpt4.models.blip2 import Blip2Base\n\tfrom minigpt4.models.mini_gpt4 import MiniGPT4\n\tfrom minigpt4.processors.base_processor import BaseProcessor\n\t__all__ = [\n\t    \"load_model\",\n\t    \"BaseModel\",\n\t    \"Blip2Base\",\n\t    \"MiniGPT4\",\n\t]\n", "def load_model(name, model_type, is_eval=False, device=\"cpu\", checkpoint=None):\n\t    \"\"\"\n\t    Load supported models.\n\t    To list all available models and types in registry:\n\t    >>> from minigpt4.models import model_zoo\n\t    >>> print(model_zoo)\n\t    Args:\n\t        name (str): name of the model.\n\t        model_type (str): type of the model.\n\t        is_eval (bool): whether the model is in eval mode. Default: False.\n", "        device (str): device to use. Default: \"cpu\".\n\t        checkpoint (str): path or to checkpoint. Default: None.\n\t            Note that expecting the checkpoint to have the same keys in state_dict as the model.\n\t    Returns:\n\t        model (torch.nn.Module): model.\n\t    \"\"\"\n\t    model = registry.get_model_class(name).from_pretrained(model_type=model_type)\n\t    if checkpoint is not None:\n\t        model.load_checkpoint(checkpoint)\n\t    if is_eval:\n", "        model.eval()\n\t    if device == \"cpu\":\n\t        model = model.float()\n\t    return model.to(device)\n\tdef load_preprocess(config):\n\t    \"\"\"\n\t    Load preprocessor configs and construct preprocessors.\n\t    If no preprocessor is specified, return BaseProcessor, which does not do any preprocessing.\n\t    Args:\n\t        config (dict): preprocessor configs.\n", "    Returns:\n\t        vis_processors (dict): preprocessors for visual inputs.\n\t        txt_processors (dict): preprocessors for text inputs.\n\t        Key is \"train\" or \"eval\" for processors used in training and evaluation respectively.\n\t    \"\"\"\n\t    def _build_proc_from_cfg(cfg):\n\t        return (\n\t            registry.get_processor_class(cfg.name).from_config(cfg)\n\t            if cfg is not None\n\t            else BaseProcessor()\n", "        )\n\t    vis_processors = dict()\n\t    txt_processors = dict()\n\t    vis_proc_cfg = config.get(\"vis_processor\")\n\t    txt_proc_cfg = config.get(\"text_processor\")\n\t    if vis_proc_cfg is not None:\n\t        vis_train_cfg = vis_proc_cfg.get(\"train\")\n\t        vis_eval_cfg = vis_proc_cfg.get(\"eval\")\n\t    else:\n\t        vis_train_cfg = None\n", "        vis_eval_cfg = None\n\t    vis_processors[\"train\"] = _build_proc_from_cfg(vis_train_cfg)\n\t    vis_processors[\"eval\"] = _build_proc_from_cfg(vis_eval_cfg)\n\t    if txt_proc_cfg is not None:\n\t        txt_train_cfg = txt_proc_cfg.get(\"train\")\n\t        txt_eval_cfg = txt_proc_cfg.get(\"eval\")\n\t    else:\n\t        txt_train_cfg = None\n\t        txt_eval_cfg = None\n\t    txt_processors[\"train\"] = _build_proc_from_cfg(txt_train_cfg)\n", "    txt_processors[\"eval\"] = _build_proc_from_cfg(txt_eval_cfg)\n\t    return vis_processors, txt_processors\n\tdef load_model_and_preprocess(name, model_type, is_eval=False, device=\"cpu\"):\n\t    \"\"\"\n\t    Load model and its related preprocessors.\n\t    List all available models and types in registry:\n\t    >>> from minigpt4.models import model_zoo\n\t    >>> print(model_zoo)\n\t    Args:\n\t        name (str): name of the model.\n", "        model_type (str): type of the model.\n\t        is_eval (bool): whether the model is in eval mode. Default: False.\n\t        device (str): device to use. Default: \"cpu\".\n\t    Returns:\n\t        model (torch.nn.Module): model.\n\t        vis_processors (dict): preprocessors for visual inputs.\n\t        txt_processors (dict): preprocessors for text inputs.\n\t    \"\"\"\n\t    model_cls = registry.get_model_class(name)\n\t    # load model\n", "    model = model_cls.from_pretrained(model_type=model_type)\n\t    if is_eval:\n\t        model.eval()\n\t    # load preprocess\n\t    cfg = OmegaConf.load(model_cls.default_config_path(model_type))\n\t    if cfg is not None:\n\t        preprocess_cfg = cfg.preprocess\n\t        vis_processors, txt_processors = load_preprocess(preprocess_cfg)\n\t    else:\n\t        vis_processors, txt_processors = None, None\n", "        logging.info(\n\t            f\"\"\"No default preprocess for model {name} ({model_type}).\n\t                This can happen if the model is not finetuned on downstream datasets,\n\t                or it is not intended for direct use without finetuning.\n\t            \"\"\"\n\t        )\n\t    if device == \"cpu\" or device == torch.device(\"cpu\"):\n\t        model = model.float()\n\t    return model.to(device), vis_processors, txt_processors\n\tclass ModelZoo:\n", "    \"\"\"\n\t    A utility class to create string representation of available model architectures and types.\n\t    >>> from minigpt4.models import model_zoo\n\t    >>> # list all available models\n\t    >>> print(model_zoo)\n\t    >>> # show total number of models\n\t    >>> print(len(model_zoo))\n\t    \"\"\"\n\t    def __init__(self) -> None:\n\t        self.model_zoo = {\n", "            k: list(v.PRETRAINED_MODEL_CONFIG_DICT.keys())\n\t            for k, v in registry.mapping[\"model_name_mapping\"].items()\n\t        }\n\t    def __str__(self) -> str:\n\t        return (\n\t            \"=\" * 50\n\t            + \"\\n\"\n\t            + f\"{'Architectures':<30} {'Types'}\\n\"\n\t            + \"=\" * 50\n\t            + \"\\n\"\n", "            + \"\\n\".join(\n\t                [\n\t                    f\"{name:<30} {', '.join(types)}\"\n\t                    for name, types in self.model_zoo.items()\n\t                ]\n\t            )\n\t        )\n\t    def __iter__(self):\n\t        return iter(self.model_zoo.items())\n\t    def __len__(self):\n", "        return sum([len(v) for v in self.model_zoo.values()])\n\tmodel_zoo = ModelZoo()\n"]}
{"filename": "minigpt4/models/base_model.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\tfrom minigpt4.common.dist_utils import download_cached_file, is_dist_avail_and_initialized\n\tfrom minigpt4.common.utils import get_abs_path, is_url\n\tfrom omegaconf import OmegaConf\n\tclass BaseModel(nn.Module):\n\t    \"\"\"Base class for models.\"\"\"\n\t    def __init__(self):\n\t        super().__init__()\n\t    @property\n\t    def device(self):\n", "        return list(self.parameters())[0].device\n\t    def load_checkpoint(self, url_or_filename):\n\t        \"\"\"\n\t        Load from a finetuned checkpoint.\n\t        This should expect no mismatch in the model keys and the checkpoint keys.\n\t        \"\"\"\n\t        if is_url(url_or_filename):\n\t            cached_file = download_cached_file(\n\t                url_or_filename, check_hash=False, progress=True\n\t            )\n", "            checkpoint = torch.load(cached_file, map_location=\"cpu\")\n\t        elif os.path.isfile(url_or_filename):\n\t            checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n\t        else:\n\t            raise RuntimeError(\"checkpoint url or path is invalid\")\n\t        if \"model\" in checkpoint.keys():\n\t            state_dict = checkpoint[\"model\"]\n\t        else:\n\t            state_dict = checkpoint\n\t        msg = self.load_state_dict(state_dict, strict=False)\n", "        logging.info(\"Missing keys {}\".format(msg.missing_keys))\n\t        logging.info(\"load checkpoint from %s\" % url_or_filename)\n\t        return msg\n\t    @classmethod\n\t    def from_pretrained(cls, model_type):\n\t        \"\"\"\n\t        Build a pretrained model from default configuration file, specified by model_type.\n\t        Args:\n\t            - model_type (str): model type, specifying architecture and checkpoints.\n\t        Returns:\n", "            - model (nn.Module): pretrained or finetuned model, depending on the configuration.\n\t        \"\"\"\n\t        model_cfg = OmegaConf.load(cls.default_config_path(model_type)).model\n\t        model = cls.from_config(model_cfg)\n\t        return model\n\t    @classmethod\n\t    def default_config_path(cls, model_type):\n\t        assert (\n\t            model_type in cls.PRETRAINED_MODEL_CONFIG_DICT\n\t        ), \"Unknown model type {}\".format(model_type)\n", "        return get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])\n\t    def load_checkpoint_from_config(self, cfg, **kwargs):\n\t        \"\"\"\n\t        Load checkpoint as specified in the config file.\n\t        If load_finetuned is True, load the finetuned model; otherwise, load the pretrained model.\n\t        When loading the pretrained model, each task-specific architecture may define their\n\t        own load_from_pretrained() method.\n\t        \"\"\"\n\t        load_finetuned = cfg.get(\"load_finetuned\", True)\n\t        if load_finetuned:\n", "            finetune_path = cfg.get(\"finetuned\", None)\n\t            assert (\n\t                finetune_path is not None\n\t            ), \"Found load_finetuned is True, but finetune_path is None.\"\n\t            self.load_checkpoint(url_or_filename=finetune_path)\n\t        else:\n\t            # load pre-trained weights\n\t            pretrain_path = cfg.get(\"pretrained\", None)\n\t            assert \"Found load_finetuned is False, but pretrain_path is None.\"\n\t            self.load_from_pretrained(url_or_filename=pretrain_path, **kwargs)\n", "    def before_evaluation(self, **kwargs):\n\t        pass\n\t    def show_n_params(self, return_str=True):\n\t        tot = 0\n\t        for p in self.parameters():\n\t            w = 1\n\t            for x in p.shape:\n\t                w *= x\n\t            tot += w\n\t        if return_str:\n", "            if tot >= 1e6:\n\t                return \"{:.1f}M\".format(tot / 1e6)\n\t            else:\n\t                return \"{:.1f}K\".format(tot / 1e3)\n\t        else:\n\t            return tot\n\tclass BaseEncoder(nn.Module):\n\t    \"\"\"\n\t    Base class for primitive encoders, such as ViT, TimeSformer, etc.\n\t    \"\"\"\n", "    def __init__(self):\n\t        super().__init__()\n\t    def forward_features(self, samples, **kwargs):\n\t        raise NotImplementedError\n\t    @property\n\t    def device(self):\n\t        return list(self.parameters())[0].device\n\tclass SharedQueueMixin:\n\t    @torch.no_grad()\n\t    def _dequeue_and_enqueue(self, image_feat, text_feat, idxs=None):\n", "        # gather keys before updating queue\n\t        image_feats = concat_all_gather(image_feat)\n\t        text_feats = concat_all_gather(text_feat)\n\t        batch_size = image_feats.shape[0]\n\t        ptr = int(self.queue_ptr)\n\t        assert self.queue_size % batch_size == 0  # for simplicity\n\t        # replace the keys at ptr (dequeue and enqueue)\n\t        self.image_queue[:, ptr : ptr + batch_size] = image_feats.T\n\t        self.text_queue[:, ptr : ptr + batch_size] = text_feats.T\n\t        if idxs is not None:\n", "            idxs = concat_all_gather(idxs)\n\t            self.idx_queue[:, ptr : ptr + batch_size] = idxs.T\n\t        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n\t        self.queue_ptr[0] = ptr\n\tclass MomentumDistilationMixin:\n\t    @torch.no_grad()\n\t    def copy_params(self):\n\t        for model_pair in self.model_pairs:\n\t            for param, param_m in zip(\n\t                model_pair[0].parameters(), model_pair[1].parameters()\n", "            ):\n\t                param_m.data.copy_(param.data)  # initialize\n\t                param_m.requires_grad = False  # not update by gradient\n\t    @torch.no_grad()\n\t    def _momentum_update(self):\n\t        for model_pair in self.model_pairs:\n\t            for param, param_m in zip(\n\t                model_pair[0].parameters(), model_pair[1].parameters()\n\t            ):\n\t                param_m.data = param_m.data * self.momentum + param.data * (\n", "                    1.0 - self.momentum\n\t                )\n\tclass GatherLayer(torch.autograd.Function):\n\t    \"\"\"\n\t    Gather tensors from all workers with support for backward propagation:\n\t    This implementation does not cut the gradients as torch.distributed.all_gather does.\n\t    \"\"\"\n\t    @staticmethod\n\t    def forward(ctx, x):\n\t        output = [\n", "            torch.zeros_like(x) for _ in range(torch.distributed.get_world_size())\n\t        ]\n\t        torch.distributed.all_gather(output, x)\n\t        return tuple(output)\n\t    @staticmethod\n\t    def backward(ctx, *grads):\n\t        all_gradients = torch.stack(grads)\n\t        torch.distributed.all_reduce(all_gradients)\n\t        return all_gradients[torch.distributed.get_rank()]\n\tdef all_gather_with_grad(tensors):\n", "    \"\"\"\n\t    Performs all_gather operation on the provided tensors.\n\t    Graph remains connected for backward grad computation.\n\t    \"\"\"\n\t    # Queue the gathered tensors\n\t    world_size = torch.distributed.get_world_size()\n\t    # There is no need for reduction in the single-proc case\n\t    if world_size == 1:\n\t        return tensors\n\t    # tensor_all = GatherLayer.apply(tensors)\n", "    tensor_all = GatherLayer.apply(tensors)\n\t    return torch.cat(tensor_all, dim=0)\n\t@torch.no_grad()\n\tdef concat_all_gather(tensor):\n\t    \"\"\"\n\t    Performs all_gather operation on the provided tensors.\n\t    *** Warning ***: torch.distributed.all_gather has no gradient.\n\t    \"\"\"\n\t    # if use distributed training\n\t    if not is_dist_avail_and_initialized():\n", "        return tensor\n\t    tensors_gather = [\n\t        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())\n\t    ]\n\t    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\t    output = torch.cat(tensors_gather, dim=0)\n\t    return output\n\tdef tile(x, dim, n_tile):\n\t    init_dim = x.size(dim)\n\t    repeat_idx = [1] * x.dim()\n", "    repeat_idx[dim] = n_tile\n\t    x = x.repeat(*(repeat_idx))\n\t    order_index = torch.LongTensor(\n\t        np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])\n\t    )\n\t    return torch.index_select(x, dim, order_index.to(x.device))\n"]}
{"filename": "minigpt4/models/mini_gpt4.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2023, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport logging\n\timport random\n\timport os\n\timport torch\n", "from torch.cuda.amp import autocast as autocast\n\timport torch.nn as nn\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.models.blip2 import Blip2Base, disabled_train\n\tfrom minigpt4.models.modeling_llama import LlamaForCausalLM\n\tfrom transformers import LlamaTokenizer\n\t@registry.register_model(\"mini_gpt4\")\n\tclass MiniGPT4(Blip2Base):\n\t    \"\"\"\n\t    BLIP2 GPT-LLAMA model.\n", "    \"\"\"\n\t    PRETRAINED_MODEL_CONFIG_DICT = {\n\t        \"pretrain_vicuna\": \"configs/models/minigpt4.yaml\",\n\t    }\n\t    def __init__(\n\t        self,\n\t        vit_model=\"eva_clip_g\",\n\t        q_former_model=\"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth\",\n\t        img_size=224,\n\t        drop_path_rate=0,\n", "        use_grad_checkpoint=False,\n\t        vit_precision=\"fp16\",\n\t        freeze_vit=True,\n\t        freeze_qformer=True,\n\t        num_query_token=32,\n\t        llama_model=\"\",\n\t        llama_cache_dir='',\n\t        prompt_path=\"\",\n\t        prompt_template=\"\",\n\t        max_txt_len=32,\n", "        end_sym='\\n',\n\t    ):\n\t        super().__init__()\n\t        self.tokenizer = self.init_tokenizer()\n\t        print('Loading VIT')\n\t        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n\t            vit_model, img_size, drop_path_rate, use_grad_checkpoint, vit_precision\n\t        )\n\t        if freeze_vit:\n\t            for name, param in self.visual_encoder.named_parameters():\n", "                param.requires_grad = False\n\t            self.visual_encoder = self.visual_encoder.eval()\n\t            self.visual_encoder.train = disabled_train\n\t            for name, param in self.ln_vision.named_parameters():\n\t                param.requires_grad = False\n\t            self.ln_vision = self.ln_vision.eval()\n\t            self.ln_vision.train = disabled_train\n\t            logging.info(\"freeze vision encoder\")\n\t        print('Loading VIT Done')\n\t        print('Loading Q-Former')\n", "        self.Qformer, self.query_tokens = self.init_Qformer(\n\t            num_query_token, self.visual_encoder.num_features\n\t        )\n\t        self.Qformer.cls = None\n\t        self.Qformer.bert.embeddings.word_embeddings = None\n\t        self.Qformer.bert.embeddings.position_embeddings = None\n\t        for layer in self.Qformer.bert.encoder.layer:\n\t            layer.output = None\n\t            layer.intermediate = None\n\t        self.load_from_pretrained(url_or_filename=q_former_model)\n", "        if freeze_qformer:\n\t            for name, param in self.Qformer.named_parameters():\n\t                param.requires_grad = False\n\t            self.Qformer = self.Qformer.eval()\n\t            self.Qformer.train = disabled_train\n\t            self.query_tokens.requires_grad = False\n\t            logging.info(\"freeze Qformer\")\n\t        print('Loading Q-Former Done')\n\t        print('Loading LLAMA')\n\t        self.llama_tokenizer = LlamaTokenizer.from_pretrained('Vision-CAIR/vicuna', use_fast=False, use_auth_token=os.environ[\"API_TOKEN\"])\n", "        self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token\n\t        if llama_cache_dir:\n\t            self.llama_model = LlamaForCausalLM.from_pretrained(\n\t                'Vision-CAIR/vicuna', load_in_8bit=True, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=os.environ[\"API_TOKEN\"]\n\t            )\n\t        else:\n\t            self.llama_model = LlamaForCausalLM.from_pretrained(\n\t                'Vision-CAIR/vicuna', load_in_8bit=True, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=os.environ[\"API_TOKEN\"]\n\t            )\n\t        for name, param in self.llama_model.named_parameters():\n", "            param.requires_grad = False\n\t        print('Loading LLAMA Done')\n\t        self.llama_proj = nn.Linear(\n\t            self.Qformer.config.hidden_size, self.llama_model.config.hidden_size\n\t        )\n\t        self.max_txt_len = max_txt_len\n\t        self.end_sym = end_sym\n\t        if prompt_path:\n\t            with open(prompt_path, 'r') as f:\n\t                raw_prompts = f.read().splitlines()\n", "            filted_prompts = [raw_prompt for raw_prompt in raw_prompts if \"<ImageHere>\" in raw_prompt]\n\t            self.prompt_list = [prompt_template.format(p) for p in filted_prompts]\n\t            print('Load {} training prompts'.format(len(self.prompt_list)))\n\t            print('Prompt Example \\n{}'.format(random.choice(self.prompt_list)))\n\t        else:\n\t            self.prompt_list = []\n\t    def vit_to_cpu(self):\n\t        self.ln_vision.to(\"cpu\")\n\t        self.ln_vision.float()\n\t        self.visual_encoder.to(\"cpu\")\n", "        self.visual_encoder.float()\n\t    def encode_img(self, image):\n\t        device = image.device\n\t        self.vit_to_cpu()\n\t        image = image.to(\"cpu\")\n\t        with self.maybe_autocast():\n\t            image_embeds = self.ln_vision(self.visual_encoder(image)).to(device)\n\t            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)\n\t            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n\t            query_output = self.Qformer.bert(\n", "                query_embeds=query_tokens,\n\t                encoder_hidden_states=image_embeds,\n\t                encoder_attention_mask=image_atts,\n\t                return_dict=True,\n\t            )\n\t            inputs_llama = self.llama_proj(query_output.last_hidden_state)\n\t            atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)\n\t        return inputs_llama, atts_llama\n\t    def prompt_wrap(self, img_embeds, atts_img, prompt):\n\t        if prompt:\n", "            batch_size = img_embeds.shape[0]\n\t            p_before, p_after = prompt.split('<ImageHere>')\n\t            p_before_tokens = self.llama_tokenizer(\n\t                p_before, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n\t            p_after_tokens = self.llama_tokenizer(\n\t                p_after, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n\t            p_before_embeds = self.llama_model.model.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)\n\t            p_after_embeds = self.llama_model.model.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)\n\t            wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)\n\t            wrapped_atts_img = atts_img[:, :1].expand(-1, wrapped_img_embeds.shape[1])\n", "            return wrapped_img_embeds, wrapped_atts_img\n\t        else:\n\t            return img_embeds, atts_img\n\t    def forward(self, samples):\n\t        image = samples[\"image\"]\n\t        img_embeds, atts_img = self.encode_img(image)\n\t        if hasattr(samples, 'question_split'):  # VQA dataset\n\t            print('VQA Batch')\n\t            vqa_prompt = '###Human: <Img><ImageHere></Img> '\n\t            img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, vqa_prompt)\n", "        elif self.prompt_list:\n\t            prompt = random.choice(self.prompt_list)\n\t            img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, prompt)\n\t        self.llama_tokenizer.padding_side = \"right\"\n\t        text = [t + self.end_sym for t in samples[\"text_input\"]]\n\t        to_regress_tokens = self.llama_tokenizer(\n\t            text,\n\t            return_tensors=\"pt\",\n\t            padding=\"longest\",\n\t            truncation=True,\n", "            max_length=self.max_txt_len,\n\t            add_special_tokens=False\n\t        ).to(image.device)\n\t        targets = to_regress_tokens.input_ids.masked_fill(\n\t            to_regress_tokens.input_ids == self.llama_tokenizer.pad_token_id, -100\n\t        )\n\t        empty_targets = (\n\t            torch.ones([atts_img.shape[0], atts_img.shape[1]+1],\n\t                       dtype=torch.long).to(image.device).fill_(-100)  # plus one for bos\n\t        )\n", "        targets = torch.cat([empty_targets, targets], dim=1)\n\t        batch_size = img_embeds.shape[0]\n\t        bos = torch.ones([batch_size, 1],\n\t                         dtype=to_regress_tokens.input_ids.dtype,\n\t                         device=to_regress_tokens.input_ids.device) * self.llama_tokenizer.bos_token_id\n\t        bos_embeds = self.llama_model.model.embed_tokens(bos)\n\t        atts_bos = atts_img[:, :1]\n\t        to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids)\n\t        inputs_embeds = torch.cat([bos_embeds, img_embeds, to_regress_embeds], dim=1)\n\t        attention_mask = torch.cat([atts_bos, atts_img, to_regress_tokens.attention_mask], dim=1)\n", "        with self.maybe_autocast():\n\t            outputs = self.llama_model(\n\t                inputs_embeds=inputs_embeds,\n\t                attention_mask=attention_mask,\n\t                return_dict=True,\n\t                labels=targets,\n\t            )\n\t        loss = outputs.loss\n\t        return {\"loss\": loss}\n\t    @classmethod\n", "    def from_config(cls, cfg):\n\t        vit_model = cfg.get(\"vit_model\", \"eva_clip_g\")\n\t        q_former_model = cfg.get(\"q_former_model\", \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth\")\n\t        img_size = cfg.get(\"image_size\")\n\t        num_query_token = cfg.get(\"num_query_token\")\n\t        llama_model = cfg.get(\"llama_model\")\n\t        drop_path_rate = cfg.get(\"drop_path_rate\", 0)\n\t        use_grad_checkpoint = cfg.get(\"use_grad_checkpoint\", False)\n\t        vit_precision = cfg.get(\"vit_precision\", \"fp16\")\n\t        freeze_vit = cfg.get(\"freeze_vit\", True)\n", "        freeze_qformer = cfg.get(\"freeze_qformer\", True)\n\t        llama_cache_dir = cfg.get(\"llama_cache_dir\", \"\")\n\t        prompt_path = cfg.get(\"prompt_path\", \"\")\n\t        prompt_template = cfg.get(\"prompt_template\", \"\")\n\t        max_txt_len = cfg.get(\"max_txt_len\", 32)\n\t        end_sym = cfg.get(\"end_sym\", '\\n')\n\t        model = cls(\n\t            vit_model=vit_model,\n\t            q_former_model=q_former_model,\n\t            img_size=img_size,\n", "            drop_path_rate=drop_path_rate,\n\t            use_grad_checkpoint=use_grad_checkpoint,\n\t            vit_precision=vit_precision,\n\t            freeze_vit=freeze_vit,\n\t            freeze_qformer=freeze_qformer,\n\t            llama_cache_dir=llama_cache_dir,\n\t            num_query_token=num_query_token,\n\t            llama_model=llama_model,\n\t            prompt_path=prompt_path,\n\t            prompt_template=prompt_template,\n", "            max_txt_len=max_txt_len,\n\t            end_sym=end_sym\n\t        )\n\t        ckpt_path = cfg.get(\"ckpt\", \"\")  # load weights of MiniGPT-4\n\t        if ckpt_path:\n\t            print(\"Load BLIP2-LLM Checkpoint: {}\".format(ckpt_path))\n\t            ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\t            msg = model.load_state_dict(ckpt['model'], strict=False)\n\t        return model\n"]}
{"filename": "minigpt4/models/Qformer.py", "chunked_list": ["\"\"\"\n\t * Copyright (c) 2023, salesforce.com, inc.\n\t * All rights reserved.\n\t * SPDX-License-Identifier: BSD-3-Clause\n\t * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t * By Junnan Li\n\t * Based on huggingface code base\n\t * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert\n\t\"\"\"\n\timport math\n", "import os\n\timport warnings\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional, Tuple, Dict, Any\n\timport torch\n\tfrom torch import Tensor, device, dtype, nn\n\timport torch.utils.checkpoint\n\tfrom torch import nn\n\tfrom torch.nn import CrossEntropyLoss\n\timport torch.nn.functional as F\n", "from transformers.activations import ACT2FN\n\tfrom transformers.file_utils import (\n\t    ModelOutput,\n\t)\n\tfrom transformers.modeling_outputs import (\n\t    BaseModelOutputWithPastAndCrossAttentions,\n\t    BaseModelOutputWithPoolingAndCrossAttentions,\n\t    CausalLMOutputWithCrossAttentions,\n\t    MaskedLMOutput,\n\t    MultipleChoiceModelOutput,\n", "    NextSentencePredictorOutput,\n\t    QuestionAnsweringModelOutput,\n\t    SequenceClassifierOutput,\n\t    TokenClassifierOutput,\n\t)\n\tfrom transformers.modeling_utils import (\n\t    PreTrainedModel,\n\t    apply_chunking_to_forward,\n\t    find_pruneable_heads_and_indices,\n\t    prune_linear_layer,\n", ")\n\tfrom transformers.utils import logging\n\tfrom transformers.models.bert.configuration_bert import BertConfig\n\tlogger = logging.get_logger(__name__)\n\tclass BertEmbeddings(nn.Module):\n\t    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.word_embeddings = nn.Embedding(\n\t            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id\n", "        )\n\t        self.position_embeddings = nn.Embedding(\n\t            config.max_position_embeddings, config.hidden_size\n\t        )\n\t        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n\t        # any TensorFlow checkpoint file\n\t        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\t        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\t        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n\t        self.register_buffer(\n", "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1))\n\t        )\n\t        self.position_embedding_type = getattr(\n\t            config, \"position_embedding_type\", \"absolute\"\n\t        )\n\t        self.config = config\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        position_ids=None,\n", "        query_embeds=None,\n\t        past_key_values_length=0,\n\t    ):\n\t        if input_ids is not None:\n\t            seq_length = input_ids.size()[1]\n\t        else:\n\t            seq_length = 0\n\t        if position_ids is None:\n\t            position_ids = self.position_ids[\n\t                :, past_key_values_length : seq_length + past_key_values_length\n", "            ].clone()\n\t        if input_ids is not None:\n\t            embeddings = self.word_embeddings(input_ids)\n\t            if self.position_embedding_type == \"absolute\":\n\t                position_embeddings = self.position_embeddings(position_ids)\n\t                embeddings = embeddings + position_embeddings\n\t            if query_embeds is not None:\n\t                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n\t        else:\n\t            embeddings = query_embeds\n", "        embeddings = self.LayerNorm(embeddings)\n\t        embeddings = self.dropout(embeddings)\n\t        return embeddings\n\tclass BertSelfAttention(nn.Module):\n\t    def __init__(self, config, is_cross_attention):\n\t        super().__init__()\n\t        self.config = config\n\t        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n\t            config, \"embedding_size\"\n\t        ):\n", "            raise ValueError(\n\t                \"The hidden size (%d) is not a multiple of the number of attention \"\n\t                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n\t            )\n\t        self.num_attention_heads = config.num_attention_heads\n\t        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n\t        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\t        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n\t        if is_cross_attention:\n\t            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n", "            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n\t        else:\n\t            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n\t            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\t        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\t        self.position_embedding_type = getattr(\n\t            config, \"position_embedding_type\", \"absolute\"\n\t        )\n\t        if (\n\t            self.position_embedding_type == \"relative_key\"\n", "            or self.position_embedding_type == \"relative_key_query\"\n\t        ):\n\t            self.max_position_embeddings = config.max_position_embeddings\n\t            self.distance_embedding = nn.Embedding(\n\t                2 * config.max_position_embeddings - 1, self.attention_head_size\n\t            )\n\t        self.save_attention = False\n\t    def save_attn_gradients(self, attn_gradients):\n\t        self.attn_gradients = attn_gradients\n\t    def get_attn_gradients(self):\n", "        return self.attn_gradients\n\t    def save_attention_map(self, attention_map):\n\t        self.attention_map = attention_map\n\t    def get_attention_map(self):\n\t        return self.attention_map\n\t    def transpose_for_scores(self, x):\n\t        new_x_shape = x.size()[:-1] + (\n\t            self.num_attention_heads,\n\t            self.attention_head_size,\n\t        )\n", "        x = x.view(*new_x_shape)\n\t        return x.permute(0, 2, 1, 3)\n\t    def forward(\n\t        self,\n\t        hidden_states,\n\t        attention_mask=None,\n\t        head_mask=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n\t        past_key_value=None,\n", "        output_attentions=False,\n\t    ):\n\t        # If this is instantiated as a cross-attention module, the keys\n\t        # and values come from an encoder; the attention mask needs to be\n\t        # such that the encoder's padding tokens are not attended to.\n\t        is_cross_attention = encoder_hidden_states is not None\n\t        if is_cross_attention:\n\t            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n\t            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n\t            attention_mask = encoder_attention_mask\n", "        elif past_key_value is not None:\n\t            key_layer = self.transpose_for_scores(self.key(hidden_states))\n\t            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\t            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n\t            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n\t        else:\n\t            key_layer = self.transpose_for_scores(self.key(hidden_states))\n\t            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\t        mixed_query_layer = self.query(hidden_states)\n\t        query_layer = self.transpose_for_scores(mixed_query_layer)\n", "        past_key_value = (key_layer, value_layer)\n\t        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n\t        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\t        if (\n\t            self.position_embedding_type == \"relative_key\"\n\t            or self.position_embedding_type == \"relative_key_query\"\n\t        ):\n\t            seq_length = hidden_states.size()[1]\n\t            position_ids_l = torch.arange(\n\t                seq_length, dtype=torch.long, device=hidden_states.device\n", "            ).view(-1, 1)\n\t            position_ids_r = torch.arange(\n\t                seq_length, dtype=torch.long, device=hidden_states.device\n\t            ).view(1, -1)\n\t            distance = position_ids_l - position_ids_r\n\t            positional_embedding = self.distance_embedding(\n\t                distance + self.max_position_embeddings - 1\n\t            )\n\t            positional_embedding = positional_embedding.to(\n\t                dtype=query_layer.dtype\n", "            )  # fp16 compatibility\n\t            if self.position_embedding_type == \"relative_key\":\n\t                relative_position_scores = torch.einsum(\n\t                    \"bhld,lrd->bhlr\", query_layer, positional_embedding\n\t                )\n\t                attention_scores = attention_scores + relative_position_scores\n\t            elif self.position_embedding_type == \"relative_key_query\":\n\t                relative_position_scores_query = torch.einsum(\n\t                    \"bhld,lrd->bhlr\", query_layer, positional_embedding\n\t                )\n", "                relative_position_scores_key = torch.einsum(\n\t                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding\n\t                )\n\t                attention_scores = (\n\t                    attention_scores\n\t                    + relative_position_scores_query\n\t                    + relative_position_scores_key\n\t                )\n\t        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n\t        if attention_mask is not None:\n", "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n\t            attention_scores = attention_scores + attention_mask\n\t        # Normalize the attention scores to probabilities.\n\t        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\t        if is_cross_attention and self.save_attention:\n\t            self.save_attention_map(attention_probs)\n\t            attention_probs.register_hook(self.save_attn_gradients)\n\t        # This is actually dropping out entire tokens to attend to, which might\n\t        # seem a bit unusual, but is taken from the original Transformer paper.\n\t        attention_probs_dropped = self.dropout(attention_probs)\n", "        # Mask heads if we want to\n\t        if head_mask is not None:\n\t            attention_probs_dropped = attention_probs_dropped * head_mask\n\t        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\t        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n\t        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n\t        context_layer = context_layer.view(*new_context_layer_shape)\n\t        outputs = (\n\t            (context_layer, attention_probs) if output_attentions else (context_layer,)\n\t        )\n", "        outputs = outputs + (past_key_value,)\n\t        return outputs\n\tclass BertSelfOutput(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n\t        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\t        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\t    def forward(self, hidden_states, input_tensor):\n\t        hidden_states = self.dense(hidden_states)\n", "        hidden_states = self.dropout(hidden_states)\n\t        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n\t        return hidden_states\n\tclass BertAttention(nn.Module):\n\t    def __init__(self, config, is_cross_attention=False):\n\t        super().__init__()\n\t        self.self = BertSelfAttention(config, is_cross_attention)\n\t        self.output = BertSelfOutput(config)\n\t        self.pruned_heads = set()\n\t    def prune_heads(self, heads):\n", "        if len(heads) == 0:\n\t            return\n\t        heads, index = find_pruneable_heads_and_indices(\n\t            heads,\n\t            self.self.num_attention_heads,\n\t            self.self.attention_head_size,\n\t            self.pruned_heads,\n\t        )\n\t        # Prune linear layers\n\t        self.self.query = prune_linear_layer(self.self.query, index)\n", "        self.self.key = prune_linear_layer(self.self.key, index)\n\t        self.self.value = prune_linear_layer(self.self.value, index)\n\t        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\t        # Update hyper params and store pruned heads\n\t        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n\t        self.self.all_head_size = (\n\t            self.self.attention_head_size * self.self.num_attention_heads\n\t        )\n\t        self.pruned_heads = self.pruned_heads.union(heads)\n\t    def forward(\n", "        self,\n\t        hidden_states,\n\t        attention_mask=None,\n\t        head_mask=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n\t        past_key_value=None,\n\t        output_attentions=False,\n\t    ):\n\t        self_outputs = self.self(\n", "            hidden_states,\n\t            attention_mask,\n\t            head_mask,\n\t            encoder_hidden_states,\n\t            encoder_attention_mask,\n\t            past_key_value,\n\t            output_attentions,\n\t        )\n\t        attention_output = self.output(self_outputs[0], hidden_states)\n\t        outputs = (attention_output,) + self_outputs[\n", "            1:\n\t        ]  # add attentions if we output them\n\t        return outputs\n\tclass BertIntermediate(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n\t        if isinstance(config.hidden_act, str):\n\t            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n\t        else:\n", "            self.intermediate_act_fn = config.hidden_act\n\t    def forward(self, hidden_states):\n\t        hidden_states = self.dense(hidden_states)\n\t        hidden_states = self.intermediate_act_fn(hidden_states)\n\t        return hidden_states\n\tclass BertOutput(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n\t        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n", "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\t    def forward(self, hidden_states, input_tensor):\n\t        hidden_states = self.dense(hidden_states)\n\t        hidden_states = self.dropout(hidden_states)\n\t        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n\t        return hidden_states\n\tclass BertLayer(nn.Module):\n\t    def __init__(self, config, layer_num):\n\t        super().__init__()\n\t        self.config = config\n", "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n\t        self.seq_len_dim = 1\n\t        self.attention = BertAttention(config)\n\t        self.layer_num = layer_num\n\t        if (\n\t            self.config.add_cross_attention\n\t            and layer_num % self.config.cross_attention_freq == 0\n\t        ):\n\t            self.crossattention = BertAttention(\n\t                config, is_cross_attention=self.config.add_cross_attention\n", "            )\n\t            self.has_cross_attention = True\n\t        else:\n\t            self.has_cross_attention = False\n\t        self.intermediate = BertIntermediate(config)\n\t        self.output = BertOutput(config)\n\t        self.intermediate_query = BertIntermediate(config)\n\t        self.output_query = BertOutput(config)\n\t    def forward(\n\t        self,\n", "        hidden_states,\n\t        attention_mask=None,\n\t        head_mask=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n\t        past_key_value=None,\n\t        output_attentions=False,\n\t        query_length=0,\n\t    ):\n\t        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n", "        self_attn_past_key_value = (\n\t            past_key_value[:2] if past_key_value is not None else None\n\t        )\n\t        self_attention_outputs = self.attention(\n\t            hidden_states,\n\t            attention_mask,\n\t            head_mask,\n\t            output_attentions=output_attentions,\n\t            past_key_value=self_attn_past_key_value,\n\t        )\n", "        attention_output = self_attention_outputs[0]\n\t        outputs = self_attention_outputs[1:-1]\n\t        present_key_value = self_attention_outputs[-1]\n\t        if query_length > 0:\n\t            query_attention_output = attention_output[:, :query_length, :]\n\t            if self.has_cross_attention:\n\t                assert (\n\t                    encoder_hidden_states is not None\n\t                ), \"encoder_hidden_states must be given for cross-attention layers\"\n\t                cross_attention_outputs = self.crossattention(\n", "                    query_attention_output,\n\t                    attention_mask,\n\t                    head_mask,\n\t                    encoder_hidden_states,\n\t                    encoder_attention_mask,\n\t                    output_attentions=output_attentions,\n\t                )\n\t                query_attention_output = cross_attention_outputs[0]\n\t                outputs = (\n\t                    outputs + cross_attention_outputs[1:-1]\n", "                )  # add cross attentions if we output attention weights\n\t            layer_output = apply_chunking_to_forward(\n\t                self.feed_forward_chunk_query,\n\t                self.chunk_size_feed_forward,\n\t                self.seq_len_dim,\n\t                query_attention_output,\n\t            )\n\t            if attention_output.shape[1] > query_length:\n\t                layer_output_text = apply_chunking_to_forward(\n\t                    self.feed_forward_chunk,\n", "                    self.chunk_size_feed_forward,\n\t                    self.seq_len_dim,\n\t                    attention_output[:, query_length:, :],\n\t                )\n\t                layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n\t        else:\n\t            layer_output = apply_chunking_to_forward(\n\t                self.feed_forward_chunk,\n\t                self.chunk_size_feed_forward,\n\t                self.seq_len_dim,\n", "                attention_output,\n\t            )\n\t        outputs = (layer_output,) + outputs\n\t        outputs = outputs + (present_key_value,)\n\t        return outputs\n\t    def feed_forward_chunk(self, attention_output):\n\t        intermediate_output = self.intermediate(attention_output)\n\t        layer_output = self.output(intermediate_output, attention_output)\n\t        return layer_output\n\t    def feed_forward_chunk_query(self, attention_output):\n", "        intermediate_output = self.intermediate_query(attention_output)\n\t        layer_output = self.output_query(intermediate_output, attention_output)\n\t        return layer_output\n\tclass BertEncoder(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.config = config\n\t        self.layer = nn.ModuleList(\n\t            [BertLayer(config, i) for i in range(config.num_hidden_layers)]\n\t        )\n", "    def forward(\n\t        self,\n\t        hidden_states,\n\t        attention_mask=None,\n\t        head_mask=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n\t        past_key_values=None,\n\t        use_cache=None,\n\t        output_attentions=False,\n", "        output_hidden_states=False,\n\t        return_dict=True,\n\t        query_length=0,\n\t    ):\n\t        all_hidden_states = () if output_hidden_states else None\n\t        all_self_attentions = () if output_attentions else None\n\t        all_cross_attentions = (\n\t            () if output_attentions and self.config.add_cross_attention else None\n\t        )\n\t        next_decoder_cache = () if use_cache else None\n", "        for i in range(self.config.num_hidden_layers):\n\t            layer_module = self.layer[i]\n\t            if output_hidden_states:\n\t                all_hidden_states = all_hidden_states + (hidden_states,)\n\t            layer_head_mask = head_mask[i] if head_mask is not None else None\n\t            past_key_value = past_key_values[i] if past_key_values is not None else None\n\t            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\t                if use_cache:\n\t                    logger.warn(\n\t                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n", "                    )\n\t                    use_cache = False\n\t                def create_custom_forward(module):\n\t                    def custom_forward(*inputs):\n\t                        return module(\n\t                            *inputs, past_key_value, output_attentions, query_length\n\t                        )\n\t                    return custom_forward\n\t                layer_outputs = torch.utils.checkpoint.checkpoint(\n\t                    create_custom_forward(layer_module),\n", "                    hidden_states,\n\t                    attention_mask,\n\t                    layer_head_mask,\n\t                    encoder_hidden_states,\n\t                    encoder_attention_mask,\n\t                )\n\t            else:\n\t                layer_outputs = layer_module(\n\t                    hidden_states,\n\t                    attention_mask,\n", "                    layer_head_mask,\n\t                    encoder_hidden_states,\n\t                    encoder_attention_mask,\n\t                    past_key_value,\n\t                    output_attentions,\n\t                    query_length,\n\t                )\n\t            hidden_states = layer_outputs[0]\n\t            if use_cache:\n\t                next_decoder_cache += (layer_outputs[-1],)\n", "            if output_attentions:\n\t                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n\t                all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n\t        if output_hidden_states:\n\t            all_hidden_states = all_hidden_states + (hidden_states,)\n\t        if not return_dict:\n\t            return tuple(\n\t                v\n\t                for v in [\n\t                    hidden_states,\n", "                    next_decoder_cache,\n\t                    all_hidden_states,\n\t                    all_self_attentions,\n\t                    all_cross_attentions,\n\t                ]\n\t                if v is not None\n\t            )\n\t        return BaseModelOutputWithPastAndCrossAttentions(\n\t            last_hidden_state=hidden_states,\n\t            past_key_values=next_decoder_cache,\n", "            hidden_states=all_hidden_states,\n\t            attentions=all_self_attentions,\n\t            cross_attentions=all_cross_attentions,\n\t        )\n\tclass BertPooler(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n\t        self.activation = nn.Tanh()\n\t    def forward(self, hidden_states):\n", "        # We \"pool\" the model by simply taking the hidden state corresponding\n\t        # to the first token.\n\t        first_token_tensor = hidden_states[:, 0]\n\t        pooled_output = self.dense(first_token_tensor)\n\t        pooled_output = self.activation(pooled_output)\n\t        return pooled_output\n\tclass BertPredictionHeadTransform(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n", "        if isinstance(config.hidden_act, str):\n\t            self.transform_act_fn = ACT2FN[config.hidden_act]\n\t        else:\n\t            self.transform_act_fn = config.hidden_act\n\t        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\t    def forward(self, hidden_states):\n\t        hidden_states = self.dense(hidden_states)\n\t        hidden_states = self.transform_act_fn(hidden_states)\n\t        hidden_states = self.LayerNorm(hidden_states)\n\t        return hidden_states\n", "class BertLMPredictionHead(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.transform = BertPredictionHeadTransform(config)\n\t        # The output weights are the same as the input embeddings, but there is\n\t        # an output-only bias for each token.\n\t        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\t        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\t        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n\t        self.decoder.bias = self.bias\n", "    def forward(self, hidden_states):\n\t        hidden_states = self.transform(hidden_states)\n\t        hidden_states = self.decoder(hidden_states)\n\t        return hidden_states\n\tclass BertOnlyMLMHead(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.predictions = BertLMPredictionHead(config)\n\t    def forward(self, sequence_output):\n\t        prediction_scores = self.predictions(sequence_output)\n", "        return prediction_scores\n\tclass BertPreTrainedModel(PreTrainedModel):\n\t    \"\"\"\n\t    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n\t    models.\n\t    \"\"\"\n\t    config_class = BertConfig\n\t    base_model_prefix = \"bert\"\n\t    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\t    def _init_weights(self, module):\n", "        \"\"\"Initialize the weights\"\"\"\n\t        if isinstance(module, (nn.Linear, nn.Embedding)):\n\t            # Slightly different from the TF version which uses truncated_normal for initialization\n\t            # cf https://github.com/pytorch/pytorch/pull/5617\n\t            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n\t        elif isinstance(module, nn.LayerNorm):\n\t            module.bias.data.zero_()\n\t            module.weight.data.fill_(1.0)\n\t        if isinstance(module, nn.Linear) and module.bias is not None:\n\t            module.bias.data.zero_()\n", "class BertModel(BertPreTrainedModel):\n\t    \"\"\"\n\t    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n\t    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n\t    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n\t    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n\t    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n\t    input to the forward pass.\n\t    \"\"\"\n\t    def __init__(self, config, add_pooling_layer=False):\n", "        super().__init__(config)\n\t        self.config = config\n\t        self.embeddings = BertEmbeddings(config)\n\t        self.encoder = BertEncoder(config)\n\t        self.pooler = BertPooler(config) if add_pooling_layer else None\n\t        self.init_weights()\n\t    def get_input_embeddings(self):\n\t        return self.embeddings.word_embeddings\n\t    def set_input_embeddings(self, value):\n\t        self.embeddings.word_embeddings = value\n", "    def _prune_heads(self, heads_to_prune):\n\t        \"\"\"\n\t        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n\t        class PreTrainedModel\n\t        \"\"\"\n\t        for layer, heads in heads_to_prune.items():\n\t            self.encoder.layer[layer].attention.prune_heads(heads)\n\t    def get_extended_attention_mask(\n\t        self,\n\t        attention_mask: Tensor,\n", "        input_shape: Tuple[int],\n\t        device: device,\n\t        is_decoder: bool,\n\t        has_query: bool = False,\n\t    ) -> Tensor:\n\t        \"\"\"\n\t        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\t        Arguments:\n\t            attention_mask (:obj:`torch.Tensor`):\n\t                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n", "            input_shape (:obj:`Tuple[int]`):\n\t                The shape of the input to the model.\n\t            device: (:obj:`torch.device`):\n\t                The device of the input to the model.\n\t        Returns:\n\t            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n\t        \"\"\"\n\t        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n\t        # ourselves in which case we just need to make it broadcastable to all heads.\n\t        if attention_mask.dim() == 3:\n", "            extended_attention_mask = attention_mask[:, None, :, :]\n\t        elif attention_mask.dim() == 2:\n\t            # Provided a padding mask of dimensions [batch_size, seq_length]\n\t            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n\t            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n\t            if is_decoder:\n\t                batch_size, seq_length = input_shape\n\t                seq_ids = torch.arange(seq_length, device=device)\n\t                causal_mask = (\n\t                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n", "                    <= seq_ids[None, :, None]\n\t                )\n\t                # add a prefix ones mask to the causal mask\n\t                # causal and attention masks must have same type with pytorch version < 1.3\n\t                causal_mask = causal_mask.to(attention_mask.dtype)\n\t                if causal_mask.shape[1] < attention_mask.shape[1]:\n\t                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n\t                    if has_query:  # UniLM style attention mask\n\t                        causal_mask = torch.cat(\n\t                            [\n", "                                torch.zeros(\n\t                                    (batch_size, prefix_seq_len, seq_length),\n\t                                    device=device,\n\t                                    dtype=causal_mask.dtype,\n\t                                ),\n\t                                causal_mask,\n\t                            ],\n\t                            axis=1,\n\t                        )\n\t                    causal_mask = torch.cat(\n", "                        [\n\t                            torch.ones(\n\t                                (batch_size, causal_mask.shape[1], prefix_seq_len),\n\t                                device=device,\n\t                                dtype=causal_mask.dtype,\n\t                            ),\n\t                            causal_mask,\n\t                        ],\n\t                        axis=-1,\n\t                    )\n", "                extended_attention_mask = (\n\t                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n\t                )\n\t            else:\n\t                extended_attention_mask = attention_mask[:, None, None, :]\n\t        else:\n\t            raise ValueError(\n\t                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n\t                    input_shape, attention_mask.shape\n\t                )\n", "            )\n\t        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n\t        # masked positions, this operation will create a tensor which is 0.0 for\n\t        # positions we want to attend and -10000.0 for masked positions.\n\t        # Since we are adding it to the raw scores before the softmax, this is\n\t        # effectively the same as removing these entirely.\n\t        extended_attention_mask = extended_attention_mask.to(\n\t            dtype=self.dtype\n\t        )  # fp16 compatibility\n\t        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n", "        return extended_attention_mask\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        attention_mask=None,\n\t        position_ids=None,\n\t        head_mask=None,\n\t        query_embeds=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n", "        past_key_values=None,\n\t        use_cache=None,\n\t        output_attentions=None,\n\t        output_hidden_states=None,\n\t        return_dict=None,\n\t        is_decoder=False,\n\t    ):\n\t        r\"\"\"\n\t        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n\t            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n", "            the model is configured as a decoder.\n\t        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n\t            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n\t            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\t            - 1 for tokens that are **not masked**,\n\t            - 0 for tokens that are **masked**.\n\t        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n\t            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\t            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n\t            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n", "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n\t        use_cache (:obj:`bool`, `optional`):\n\t            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n\t            decoding (see :obj:`past_key_values`).\n\t        \"\"\"\n\t        output_attentions = (\n\t            output_attentions\n\t            if output_attentions is not None\n\t            else self.config.output_attentions\n\t        )\n", "        output_hidden_states = (\n\t            output_hidden_states\n\t            if output_hidden_states is not None\n\t            else self.config.output_hidden_states\n\t        )\n\t        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        # use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        if input_ids is None:\n", "            assert (\n\t                query_embeds is not None\n\t            ), \"You have to specify query_embeds when input_ids is None\"\n\t        # past_key_values_length\n\t        past_key_values_length = (\n\t            past_key_values[0][0].shape[2] - self.config.query_length\n\t            if past_key_values is not None\n\t            else 0\n\t        )\n\t        query_length = query_embeds.shape[1] if query_embeds is not None else 0\n", "        embedding_output = self.embeddings(\n\t            input_ids=input_ids,\n\t            position_ids=position_ids,\n\t            query_embeds=query_embeds,\n\t            past_key_values_length=past_key_values_length,\n\t        )\n\t        input_shape = embedding_output.size()[:-1]\n\t        batch_size, seq_length = input_shape\n\t        device = embedding_output.device\n\t        if attention_mask is None:\n", "            attention_mask = torch.ones(\n\t                ((batch_size, seq_length + past_key_values_length)), device=device\n\t            )\n\t        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n\t        # ourselves in which case we just need to make it broadcastable to all heads.\n\t        if is_decoder:\n\t            extended_attention_mask = self.get_extended_attention_mask(\n\t                attention_mask,\n\t                input_ids.shape,\n\t                device,\n", "                is_decoder,\n\t                has_query=(query_embeds is not None),\n\t            )\n\t        else:\n\t            extended_attention_mask = self.get_extended_attention_mask(\n\t                attention_mask, input_shape, device, is_decoder\n\t            )\n\t        # If a 2D or 3D attention mask is provided for the cross-attention\n\t        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n\t        if encoder_hidden_states is not None:\n", "            if type(encoder_hidden_states) == list:\n\t                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[\n\t                    0\n\t                ].size()\n\t            else:\n\t                (\n\t                    encoder_batch_size,\n\t                    encoder_sequence_length,\n\t                    _,\n\t                ) = encoder_hidden_states.size()\n", "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n\t            if type(encoder_attention_mask) == list:\n\t                encoder_extended_attention_mask = [\n\t                    self.invert_attention_mask(mask) for mask in encoder_attention_mask\n\t                ]\n\t            elif encoder_attention_mask is None:\n\t                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\t                encoder_extended_attention_mask = self.invert_attention_mask(\n\t                    encoder_attention_mask\n\t                )\n", "            else:\n\t                encoder_extended_attention_mask = self.invert_attention_mask(\n\t                    encoder_attention_mask\n\t                )\n\t        else:\n\t            encoder_extended_attention_mask = None\n\t        # Prepare head mask if needed\n\t        # 1.0 in head_mask indicate we keep the head\n\t        # attention_probs has shape bsz x n_heads x N x N\n\t        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n", "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n\t        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\t        encoder_outputs = self.encoder(\n\t            embedding_output,\n\t            attention_mask=extended_attention_mask,\n\t            head_mask=head_mask,\n\t            encoder_hidden_states=encoder_hidden_states,\n\t            encoder_attention_mask=encoder_extended_attention_mask,\n\t            past_key_values=past_key_values,\n\t            use_cache=use_cache,\n", "            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t            query_length=query_length,\n\t        )\n\t        sequence_output = encoder_outputs[0]\n\t        pooled_output = (\n\t            self.pooler(sequence_output) if self.pooler is not None else None\n\t        )\n\t        if not return_dict:\n", "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\t        return BaseModelOutputWithPoolingAndCrossAttentions(\n\t            last_hidden_state=sequence_output,\n\t            pooler_output=pooled_output,\n\t            past_key_values=encoder_outputs.past_key_values,\n\t            hidden_states=encoder_outputs.hidden_states,\n\t            attentions=encoder_outputs.attentions,\n\t            cross_attentions=encoder_outputs.cross_attentions,\n\t        )\n\tclass BertLMHeadModel(BertPreTrainedModel):\n", "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\t    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t        self.bert = BertModel(config, add_pooling_layer=False)\n\t        self.cls = BertOnlyMLMHead(config)\n\t        self.init_weights()\n\t    def get_output_embeddings(self):\n\t        return self.cls.predictions.decoder\n\t    def set_output_embeddings(self, new_embeddings):\n", "        self.cls.predictions.decoder = new_embeddings\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        attention_mask=None,\n\t        position_ids=None,\n\t        head_mask=None,\n\t        query_embeds=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n", "        labels=None,\n\t        past_key_values=None,\n\t        use_cache=True,\n\t        output_attentions=None,\n\t        output_hidden_states=None,\n\t        return_dict=None,\n\t        return_logits=False,\n\t        is_decoder=True,\n\t        reduction=\"mean\",\n\t    ):\n", "        r\"\"\"\n\t        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n\t            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n\t            the model is configured as a decoder.\n\t        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n\t            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n\t            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\t            - 1 for tokens that are **not masked**,\n\t            - 0 for tokens that are **masked**.\n\t        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n", "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n\t            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n\t            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n\t        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n\t            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\t            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n\t            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n\t            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n\t        use_cache (:obj:`bool`, `optional`):\n\t            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n", "            decoding (see :obj:`past_key_values`).\n\t        Returns:\n\t        Example::\n\t            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n\t            >>> import torch\n\t            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\t            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n\t            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n\t            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\t            >>> outputs = model(**inputs)\n", "            >>> prediction_logits = outputs.logits\n\t        \"\"\"\n\t        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        if labels is not None:\n\t            use_cache = False\n\t        if past_key_values is not None:\n\t            query_embeds = None\n\t        outputs = self.bert(\n", "            input_ids,\n\t            attention_mask=attention_mask,\n\t            position_ids=position_ids,\n\t            head_mask=head_mask,\n\t            query_embeds=query_embeds,\n\t            encoder_hidden_states=encoder_hidden_states,\n\t            encoder_attention_mask=encoder_attention_mask,\n\t            past_key_values=past_key_values,\n\t            use_cache=use_cache,\n\t            output_attentions=output_attentions,\n", "            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t            is_decoder=is_decoder,\n\t        )\n\t        sequence_output = outputs[0]\n\t        if query_embeds is not None:\n\t            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]\n\t        prediction_scores = self.cls(sequence_output)\n\t        if return_logits:\n\t            return prediction_scores[:, :-1, :].contiguous()\n", "        lm_loss = None\n\t        if labels is not None:\n\t            # we are doing next-token prediction; shift prediction scores and input ids by one\n\t            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n\t            labels = labels[:, 1:].contiguous()\n\t            loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1)\n\t            lm_loss = loss_fct(\n\t                shifted_prediction_scores.view(-1, self.config.vocab_size),\n\t                labels.view(-1),\n\t            )\n", "            if reduction == \"none\":\n\t                lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)\n\t        if not return_dict:\n\t            output = (prediction_scores,) + outputs[2:]\n\t            return ((lm_loss,) + output) if lm_loss is not None else output\n\t        return CausalLMOutputWithCrossAttentions(\n\t            loss=lm_loss,\n\t            logits=prediction_scores,\n\t            past_key_values=outputs.past_key_values,\n\t            hidden_states=outputs.hidden_states,\n", "            attentions=outputs.attentions,\n\t            cross_attentions=outputs.cross_attentions,\n\t        )\n\t    def prepare_inputs_for_generation(\n\t        self, input_ids, query_embeds, past=None, attention_mask=None, **model_kwargs\n\t    ):\n\t        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n\t        if attention_mask is None:\n\t            attention_mask = input_ids.new_ones(input_ids.shape)\n\t        query_mask = input_ids.new_ones(query_embeds.shape[:-1])\n", "        attention_mask = torch.cat([query_mask, attention_mask], dim=-1)\n\t        # cut decoder_input_ids if past is used\n\t        if past is not None:\n\t            input_ids = input_ids[:, -1:]\n\t        return {\n\t            \"input_ids\": input_ids,\n\t            \"query_embeds\": query_embeds,\n\t            \"attention_mask\": attention_mask,\n\t            \"past_key_values\": past,\n\t            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n", "            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n\t            \"is_decoder\": True,\n\t        }\n\t    def _reorder_cache(self, past, beam_idx):\n\t        reordered_past = ()\n\t        for layer_past in past:\n\t            reordered_past += (\n\t                tuple(\n\t                    past_state.index_select(0, beam_idx) for past_state in layer_past\n\t                ),\n", "            )\n\t        return reordered_past\n\tclass BertForMaskedLM(BertPreTrainedModel):\n\t    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\t    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t        self.bert = BertModel(config, add_pooling_layer=False)\n\t        self.cls = BertOnlyMLMHead(config)\n\t        self.init_weights()\n", "    def get_output_embeddings(self):\n\t        return self.cls.predictions.decoder\n\t    def set_output_embeddings(self, new_embeddings):\n\t        self.cls.predictions.decoder = new_embeddings\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        attention_mask=None,\n\t        position_ids=None,\n\t        head_mask=None,\n", "        query_embeds=None,\n\t        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n\t        labels=None,\n\t        output_attentions=None,\n\t        output_hidden_states=None,\n\t        return_dict=None,\n\t        return_logits=False,\n\t        is_decoder=False,\n\t    ):\n", "        r\"\"\"\n\t        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n\t            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n\t            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n\t            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n\t        \"\"\"\n\t        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        outputs = self.bert(\n", "            input_ids,\n\t            attention_mask=attention_mask,\n\t            position_ids=position_ids,\n\t            head_mask=head_mask,\n\t            query_embeds=query_embeds,\n\t            encoder_hidden_states=encoder_hidden_states,\n\t            encoder_attention_mask=encoder_attention_mask,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n", "            is_decoder=is_decoder,\n\t        )\n\t        if query_embeds is not None:\n\t            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]\n\t        prediction_scores = self.cls(sequence_output)\n\t        if return_logits:\n\t            return prediction_scores\n\t        masked_lm_loss = None\n\t        if labels is not None:\n\t            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n", "            masked_lm_loss = loss_fct(\n\t                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)\n\t            )\n\t        if not return_dict:\n\t            output = (prediction_scores,) + outputs[2:]\n\t            return (\n\t                ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\t            )\n\t        return MaskedLMOutput(\n\t            loss=masked_lm_loss,\n", "            logits=prediction_scores,\n\t            hidden_states=outputs.hidden_states,\n\t            attentions=outputs.attentions,\n\t        )\n"]}
{"filename": "minigpt4/datasets/data_utils.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport gzip\n\timport logging\n\timport os\n\timport random as rnd\n", "import tarfile\n\timport zipfile\n\timport random\n\tfrom typing import List\n\tfrom tqdm import tqdm\n\timport decord\n\tfrom decord import VideoReader\n\timport webdataset as wds\n\timport numpy as np\n\timport torch\n", "from torch.utils.data.dataset import IterableDataset\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.datasets.datasets.base_dataset import ConcatDataset\n\tdecord.bridge.set_bridge(\"torch\")\n\tMAX_INT = registry.get(\"MAX_INT\")\n\tclass ChainDataset(wds.DataPipeline):\n\t    r\"\"\"Dataset for chaining multiple :class:`DataPipeline` s.\n\t    This class is useful to assemble different existing dataset streams. The\n\t    chaining operation is done on-the-fly, so concatenating large-scale\n\t    datasets with this class will be efficient.\n", "    Args:\n\t        datasets (iterable of IterableDataset): datasets to be chained together\n\t    \"\"\"\n\t    def __init__(self, datasets: List[wds.DataPipeline]) -> None:\n\t        super().__init__()\n\t        self.datasets = datasets\n\t        self.prob = []\n\t        self.names = []\n\t        for dataset in self.datasets:\n\t            if hasattr(dataset, 'name'):\n", "                self.names.append(dataset.name)\n\t            else:\n\t                self.names.append('Unknown')\n\t            if hasattr(dataset, 'sample_ratio'):\n\t                self.prob.append(dataset.sample_ratio)\n\t            else:\n\t                self.prob.append(1)\n\t                logging.info(\"One of the datapipeline doesn't define ratio and set to 1 automatically.\")\n\t    def __iter__(self):\n\t        datastreams = [iter(dataset) for dataset in self.datasets]\n", "        while True:\n\t            select_datastream = random.choices(datastreams, weights=self.prob, k=1)[0]\n\t            yield next(select_datastream)\n\tdef apply_to_sample(f, sample):\n\t    if len(sample) == 0:\n\t        return {}\n\t    def _apply(x):\n\t        if torch.is_tensor(x):\n\t            return f(x)\n\t        elif isinstance(x, dict):\n", "            return {key: _apply(value) for key, value in x.items()}\n\t        elif isinstance(x, list):\n\t            return [_apply(x) for x in x]\n\t        else:\n\t            return x\n\t    return _apply(sample)\n\tdef move_to_cuda(sample):\n\t    def _move_to_cuda(tensor):\n\t        return tensor.cuda()\n\t    return apply_to_sample(_move_to_cuda, sample)\n", "def prepare_sample(samples, cuda_enabled=True):\n\t    if cuda_enabled:\n\t        samples = move_to_cuda(samples)\n\t    # TODO fp16 support\n\t    return samples\n\tdef reorg_datasets_by_split(datasets):\n\t    \"\"\"\n\t    Organizes datasets by split.\n\t    Args:\n\t        datasets: dict of torch.utils.data.Dataset objects by name.\n", "    Returns:\n\t        Dict of datasets by split {split_name: List[Datasets]}.\n\t    \"\"\"\n\t    # if len(datasets) == 1:\n\t    #     return datasets[list(datasets.keys())[0]]\n\t    # else:\n\t    reorg_datasets = dict()\n\t    # reorganize by split\n\t    for _, dataset in datasets.items():\n\t        for split_name, dataset_split in dataset.items():\n", "            if split_name not in reorg_datasets:\n\t                reorg_datasets[split_name] = [dataset_split]\n\t            else:\n\t                reorg_datasets[split_name].append(dataset_split)\n\t    return reorg_datasets\n\tdef concat_datasets(datasets):\n\t    \"\"\"\n\t    Concatenates multiple datasets into a single dataset.\n\t    It supports may-style datasets and DataPipeline from WebDataset. Currently, does not support\n\t    generic IterableDataset because it requires creating separate samplers.\n", "    Now only supports conctenating training datasets and assuming validation and testing\n\t    have only a single dataset. This is because metrics should not be computed on the concatenated\n\t    datasets.\n\t    Args:\n\t        datasets: dict of torch.utils.data.Dataset objects by split.\n\t    Returns:\n\t        Dict of concatenated datasets by split, \"train\" is the concatenation of multiple datasets,\n\t        \"val\" and \"test\" remain the same.\n\t        If the input training datasets contain both map-style and DataPipeline datasets, returns\n\t        a tuple, where the first element is a concatenated map-style dataset and the second\n", "        element is a chained DataPipeline dataset.\n\t    \"\"\"\n\t    # concatenate datasets in the same split\n\t    for split_name in datasets:\n\t        if split_name != \"train\":\n\t            assert (\n\t                len(datasets[split_name]) == 1\n\t            ), \"Do not support multiple {} datasets.\".format(split_name)\n\t            datasets[split_name] = datasets[split_name][0]\n\t        else:\n", "            iterable_datasets, map_datasets = [], []\n\t            for dataset in datasets[split_name]:\n\t                if isinstance(dataset, wds.DataPipeline):\n\t                    logging.info(\n\t                        \"Dataset {} is IterableDataset, can't be concatenated.\".format(\n\t                            dataset\n\t                        )\n\t                    )\n\t                    iterable_datasets.append(dataset)\n\t                elif isinstance(dataset, IterableDataset):\n", "                    raise NotImplementedError(\n\t                        \"Do not support concatenation of generic IterableDataset.\"\n\t                    )\n\t                else:\n\t                    map_datasets.append(dataset)\n\t            # if len(iterable_datasets) > 0:\n\t            # concatenate map-style datasets and iterable-style datasets separately\n\t            if len(iterable_datasets) > 1:\n\t                chained_datasets = (\n\t                    ChainDataset(iterable_datasets)\n", "                )\n\t            elif len(iterable_datasets) == 1:\n\t                chained_datasets = iterable_datasets[0]\n\t            else:\n\t                chained_datasets = None\n\t            concat_datasets = (\n\t                ConcatDataset(map_datasets) if len(map_datasets) > 0 else None\n\t            )\n\t            train_datasets = concat_datasets, chained_datasets\n\t            train_datasets = tuple([x for x in train_datasets if x is not None])\n", "            train_datasets = (\n\t                train_datasets[0] if len(train_datasets) == 1 else train_datasets\n\t            )\n\t            datasets[split_name] = train_datasets\n\t    return datasets\n"]}
{"filename": "minigpt4/datasets/__init__.py", "chunked_list": []}
{"filename": "minigpt4/datasets/builders/base_dataset_builder.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport logging\n\timport os\n\timport shutil\n\timport warnings\n", "from omegaconf import OmegaConf\n\timport torch.distributed as dist\n\tfrom torchvision.datasets.utils import download_url\n\timport minigpt4.common.utils as utils\n\tfrom minigpt4.common.dist_utils import is_dist_avail_and_initialized, is_main_process\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.processors.base_processor import BaseProcessor\n\tclass BaseDatasetBuilder:\n\t    train_dataset_cls, eval_dataset_cls = None, None\n\t    def __init__(self, cfg=None):\n", "        super().__init__()\n\t        if cfg is None:\n\t            # help to create datasets from default config.\n\t            self.config = load_dataset_config(self.default_config_path())\n\t        elif isinstance(cfg, str):\n\t            self.config = load_dataset_config(cfg)\n\t        else:\n\t            # when called from task.build_dataset()\n\t            self.config = cfg\n\t        self.data_type = self.config.data_type\n", "        self.vis_processors = {\"train\": BaseProcessor(), \"eval\": BaseProcessor()}\n\t        self.text_processors = {\"train\": BaseProcessor(), \"eval\": BaseProcessor()}\n\t    def build_datasets(self):\n\t        # download, split, etc...\n\t        # only called on 1 GPU/TPU in distributed\n\t        if is_main_process():\n\t            self._download_data()\n\t        if is_dist_avail_and_initialized():\n\t            dist.barrier()\n\t        # at this point, all the annotations and image/videos should be all downloaded to the specified locations.\n", "        logging.info(\"Building datasets...\")\n\t        datasets = self.build()  # dataset['train'/'val'/'test']\n\t        return datasets\n\t    def build_processors(self):\n\t        vis_proc_cfg = self.config.get(\"vis_processor\")\n\t        txt_proc_cfg = self.config.get(\"text_processor\")\n\t        if vis_proc_cfg is not None:\n\t            vis_train_cfg = vis_proc_cfg.get(\"train\")\n\t            vis_eval_cfg = vis_proc_cfg.get(\"eval\")\n\t            self.vis_processors[\"train\"] = self._build_proc_from_cfg(vis_train_cfg)\n", "            self.vis_processors[\"eval\"] = self._build_proc_from_cfg(vis_eval_cfg)\n\t        if txt_proc_cfg is not None:\n\t            txt_train_cfg = txt_proc_cfg.get(\"train\")\n\t            txt_eval_cfg = txt_proc_cfg.get(\"eval\")\n\t            self.text_processors[\"train\"] = self._build_proc_from_cfg(txt_train_cfg)\n\t            self.text_processors[\"eval\"] = self._build_proc_from_cfg(txt_eval_cfg)\n\t    @staticmethod\n\t    def _build_proc_from_cfg(cfg):\n\t        return (\n\t            registry.get_processor_class(cfg.name).from_config(cfg)\n", "            if cfg is not None\n\t            else None\n\t        )\n\t    @classmethod\n\t    def default_config_path(cls, type=\"default\"):\n\t        return utils.get_abs_path(cls.DATASET_CONFIG_DICT[type])\n\t    def _download_data(self):\n\t        self._download_ann()\n\t        self._download_vis()\n\t    def _download_ann(self):\n", "        \"\"\"\n\t        Download annotation files if necessary.\n\t        All the vision-language datasets should have annotations of unified format.\n\t        storage_path can be:\n\t          (1) relative/absolute: will be prefixed with env.cache_root to make full path if relative.\n\t          (2) basename/dirname: will be suffixed with base name of URL if dirname is provided.\n\t        Local annotation paths should be relative.\n\t        \"\"\"\n\t        anns = self.config.build_info.annotations\n\t        splits = anns.keys()\n", "        cache_root = registry.get_path(\"cache_root\")\n\t        for split in splits:\n\t            info = anns[split]\n\t            urls, storage_paths = info.get(\"url\", None), info.storage\n\t            if isinstance(urls, str):\n\t                urls = [urls]\n\t            if isinstance(storage_paths, str):\n\t                storage_paths = [storage_paths]\n\t            assert len(urls) == len(storage_paths)\n\t            for url_or_filename, storage_path in zip(urls, storage_paths):\n", "                # if storage_path is relative, make it full by prefixing with cache_root.\n\t                if not os.path.isabs(storage_path):\n\t                    storage_path = os.path.join(cache_root, storage_path)\n\t                dirname = os.path.dirname(storage_path)\n\t                if not os.path.exists(dirname):\n\t                    os.makedirs(dirname)\n\t                if os.path.isfile(url_or_filename):\n\t                    src, dst = url_or_filename, storage_path\n\t                    if not os.path.exists(dst):\n\t                        shutil.copyfile(src=src, dst=dst)\n", "                    else:\n\t                        logging.info(\"Using existing file {}.\".format(dst))\n\t                else:\n\t                    if os.path.isdir(storage_path):\n\t                        # if only dirname is provided, suffix with basename of URL.\n\t                        raise ValueError(\n\t                            \"Expecting storage_path to be a file path, got directory {}\".format(\n\t                                storage_path\n\t                            )\n\t                        )\n", "                    else:\n\t                        filename = os.path.basename(storage_path)\n\t                    download_url(url=url_or_filename, root=dirname, filename=filename)\n\t    def _download_vis(self):\n\t        storage_path = self.config.build_info.get(self.data_type).storage\n\t        storage_path = utils.get_cache_path(storage_path)\n\t        if not os.path.exists(storage_path):\n\t            warnings.warn(\n\t                f\"\"\"\n\t                The specified path {storage_path} for visual inputs does not exist.\n", "                Please provide a correct path to the visual inputs or\n\t                refer to datasets/download_scripts/README.md for downloading instructions.\n\t                \"\"\"\n\t            )\n\t    def build(self):\n\t        \"\"\"\n\t        Create by split datasets inheriting torch.utils.data.Datasets.\n\t        # build() can be dataset-specific. Overwrite to customize.\n\t        \"\"\"\n\t        self.build_processors()\n", "        build_info = self.config.build_info\n\t        ann_info = build_info.annotations\n\t        vis_info = build_info.get(self.data_type)\n\t        datasets = dict()\n\t        for split in ann_info.keys():\n\t            if split not in [\"train\", \"val\", \"test\"]:\n\t                continue\n\t            is_train = split == \"train\"\n\t            # processors\n\t            vis_processor = (\n", "                self.vis_processors[\"train\"]\n\t                if is_train\n\t                else self.vis_processors[\"eval\"]\n\t            )\n\t            text_processor = (\n\t                self.text_processors[\"train\"]\n\t                if is_train\n\t                else self.text_processors[\"eval\"]\n\t            )\n\t            # annotation path\n", "            ann_paths = ann_info.get(split).storage\n\t            if isinstance(ann_paths, str):\n\t                ann_paths = [ann_paths]\n\t            abs_ann_paths = []\n\t            for ann_path in ann_paths:\n\t                if not os.path.isabs(ann_path):\n\t                    ann_path = utils.get_cache_path(ann_path)\n\t                abs_ann_paths.append(ann_path)\n\t            ann_paths = abs_ann_paths\n\t            # visual data storage path\n", "            vis_path = os.path.join(vis_info.storage, split)\n\t            if not os.path.isabs(vis_path):\n\t                # vis_path = os.path.join(utils.get_cache_path(), vis_path)\n\t                vis_path = utils.get_cache_path(vis_path)\n\t            if not os.path.exists(vis_path):\n\t                warnings.warn(\"storage path {} does not exist.\".format(vis_path))\n\t            # create datasets\n\t            dataset_cls = self.train_dataset_cls if is_train else self.eval_dataset_cls\n\t            datasets[split] = dataset_cls(\n\t                vis_processor=vis_processor,\n", "                text_processor=text_processor,\n\t                ann_paths=ann_paths,\n\t                vis_root=vis_path,\n\t            )\n\t        return datasets\n\tdef load_dataset_config(cfg_path):\n\t    cfg = OmegaConf.load(cfg_path).datasets\n\t    cfg = cfg[list(cfg.keys())[0]]\n\t    return cfg\n"]}
{"filename": "minigpt4/datasets/builders/__init__.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom minigpt4.datasets.builders.base_dataset_builder import load_dataset_config\n\tfrom minigpt4.datasets.builders.image_text_pair_builder import (\n\t    CCCombineBuilder,\n\t    LaionBuilder,\n", "    CCAlignBuilder\n\t)\n\tfrom minigpt4.common.registry import registry\n\t__all__ = [\n\t    \"CCCombineBuilder\",\n\t    \"LaionBuilder\",\n\t    \"CCAlignBuilder\"\n\t]\n\tdef load_dataset(name, cfg_path=None, vis_path=None, data_type=None):\n\t    \"\"\"\n", "    Example\n\t    >>> dataset = load_dataset(\"coco_caption\", cfg=None)\n\t    >>> splits = dataset.keys()\n\t    >>> print([len(dataset[split]) for split in splits])\n\t    \"\"\"\n\t    if cfg_path is None:\n\t        cfg = None\n\t    else:\n\t        cfg = load_dataset_config(cfg_path)\n\t    try:\n", "        builder = registry.get_builder_class(name)(cfg)\n\t    except TypeError:\n\t        print(\n\t            f\"Dataset {name} not found. Available datasets:\\n\"\n\t            + \", \".join([str(k) for k in dataset_zoo.get_names()])\n\t        )\n\t        exit(1)\n\t    if vis_path is not None:\n\t        if data_type is None:\n\t            # use default data type in the config\n", "            data_type = builder.config.data_type\n\t        assert (\n\t            data_type in builder.config.build_info\n\t        ), f\"Invalid data_type {data_type} for {name}.\"\n\t        builder.config.build_info.get(data_type).storage = vis_path\n\t    dataset = builder.build_datasets()\n\t    return dataset\n\tclass DatasetZoo:\n\t    def __init__(self) -> None:\n\t        self.dataset_zoo = {\n", "            k: list(v.DATASET_CONFIG_DICT.keys())\n\t            for k, v in sorted(registry.mapping[\"builder_name_mapping\"].items())\n\t        }\n\t    def get_names(self):\n\t        return list(self.dataset_zoo.keys())\n\tdataset_zoo = DatasetZoo()\n"]}
{"filename": "minigpt4/datasets/builders/image_text_pair_builder.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport os\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.datasets.builders.base_dataset_builder import BaseDatasetBuilder\n\tfrom minigpt4.datasets.datasets.laion_dataset import LaionDataset\n", "from minigpt4.datasets.datasets.cc_combine_dataset import CCCombineDataset, CCAlignDataset\n\t@registry.register_builder(\"cc_combine\")\n\tclass CCCombineBuilder(BaseDatasetBuilder):\n\t    train_dataset_cls = CCCombineDataset\n\t    DATASET_CONFIG_DICT = {\"default\": \"configs/datasets/cc_combine/defaults.yaml\"}\n\t    def _download_ann(self):\n\t        pass\n\t    def _download_vis(self):\n\t        pass\n\t    def build(self):\n", "        self.build_processors()\n\t        build_info = self.config.build_info\n\t        datasets = dict()\n\t        split = \"train\"\n\t        # create datasets\n\t        # [NOTE] return inner_datasets (wds.DataPipeline)\n\t        dataset_cls = self.train_dataset_cls\n\t        datasets[split] = dataset_cls(\n\t            vis_processor=self.vis_processors[split],\n\t            text_processor=self.text_processors[split],\n", "            location=build_info.storage,\n\t        ).inner_dataset\n\t        return datasets\n\t@registry.register_builder(\"laion\")\n\tclass LaionBuilder(BaseDatasetBuilder):\n\t    train_dataset_cls = LaionDataset\n\t    DATASET_CONFIG_DICT = {\"default\": \"configs/datasets/laion/defaults.yaml\"}\n\t    def _download_ann(self):\n\t        pass\n\t    def _download_vis(self):\n", "        pass\n\t    def build(self):\n\t        self.build_processors()\n\t        build_info = self.config.build_info\n\t        datasets = dict()\n\t        split = \"train\"\n\t        # create datasets\n\t        # [NOTE] return inner_datasets (wds.DataPipeline)\n\t        dataset_cls = self.train_dataset_cls\n\t        datasets[split] = dataset_cls(\n", "            vis_processor=self.vis_processors[split],\n\t            text_processor=self.text_processors[split],\n\t            location=build_info.storage,\n\t        ).inner_dataset\n\t        return datasets\n\t@registry.register_builder(\"cc_align\")\n\tclass CCAlignBuilder(BaseDatasetBuilder):\n\t    train_dataset_cls = CCAlignDataset\n\t    DATASET_CONFIG_DICT = {\n\t        \"default\": \"configs/datasets/cc_combine/align.yaml\",\n", "    }"]}
{"filename": "minigpt4/datasets/datasets/__init__.py", "chunked_list": []}
{"filename": "minigpt4/datasets/datasets/base_dataset.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport json\n\tfrom typing import Iterable\n\tfrom torch.utils.data import Dataset, ConcatDataset\n\tfrom torch.utils.data.dataloader import default_collate\n", "class BaseDataset(Dataset):\n\t    def __init__(\n\t        self, vis_processor=None, text_processor=None, vis_root=None, ann_paths=[]\n\t    ):\n\t        \"\"\"\n\t        vis_root (string): Root directory of images (e.g. coco/images/)\n\t        ann_root (string): directory to store the annotation file\n\t        \"\"\"\n\t        self.vis_root = vis_root\n\t        self.annotation = []\n", "        for ann_path in ann_paths:\n\t            self.annotation.extend(json.load(open(ann_path, \"r\"))['annotations'])\n\t        self.vis_processor = vis_processor\n\t        self.text_processor = text_processor\n\t        self._add_instance_ids()\n\t    def __len__(self):\n\t        return len(self.annotation)\n\t    def collater(self, samples):\n\t        return default_collate(samples)\n\t    def set_processors(self, vis_processor, text_processor):\n", "        self.vis_processor = vis_processor\n\t        self.text_processor = text_processor\n\t    def _add_instance_ids(self, key=\"instance_id\"):\n\t        for idx, ann in enumerate(self.annotation):\n\t            ann[key] = str(idx)\n\tclass ConcatDataset(ConcatDataset):\n\t    def __init__(self, datasets: Iterable[Dataset]) -> None:\n\t        super().__init__(datasets)\n\t    def collater(self, samples):\n\t        # TODO For now only supports datasets with same underlying collater implementations\n", "        all_keys = set()\n\t        for s in samples:\n\t            all_keys.update(s)\n\t        shared_keys = all_keys\n\t        for s in samples:\n\t            shared_keys = shared_keys & set(s.keys())\n\t        samples_shared_keys = []\n\t        for s in samples:\n\t            samples_shared_keys.append({k: s[k] for k in s.keys() if k in shared_keys})\n\t        return self.datasets[0].collater(samples_shared_keys)\n"]}
{"filename": "minigpt4/datasets/datasets/laion_dataset.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport webdataset as wds\n\tfrom minigpt4.datasets.datasets.base_dataset import BaseDataset\n\tclass LaionDataset(BaseDataset):\n\t    def __init__(self, vis_processor, text_processor, location):\n", "        super().__init__(vis_processor=vis_processor, text_processor=text_processor)\n\t        self.inner_dataset = wds.DataPipeline(\n\t            wds.ResampledShards(location),\n\t            wds.tarfile_to_samples(handler=wds.warn_and_continue),\n\t            wds.shuffle(1000, handler=wds.warn_and_continue),\n\t            wds.decode(\"pilrgb\", handler=wds.warn_and_continue),\n\t            wds.to_tuple(\"jpg\", \"json\", handler=wds.warn_and_continue),\n\t            wds.map_tuple(self.vis_processor, handler=wds.warn_and_continue),\n\t            wds.map(self.to_dict, handler=wds.warn_and_continue),\n\t        )\n", "    def to_dict(self, sample):\n\t        return {\n\t            \"image\": sample[0],\n\t            \"text_input\": self.text_processor(sample[1][\"caption\"]),\n\t        }\n"]}
{"filename": "minigpt4/datasets/datasets/caption_datasets.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport os\n\tfrom collections import OrderedDict\n\tfrom minigpt4.datasets.datasets.base_dataset import BaseDataset\n\tfrom PIL import Image\n", "class __DisplMixin:\n\t    def displ_item(self, index):\n\t        sample, ann = self.__getitem__(index), self.annotation[index]\n\t        return OrderedDict(\n\t            {\n\t                \"file\": ann[\"image\"],\n\t                \"caption\": ann[\"caption\"],\n\t                \"image\": sample[\"image\"],\n\t            }\n\t        )\n", "class CaptionDataset(BaseDataset, __DisplMixin):\n\t    def __init__(self, vis_processor, text_processor, vis_root, ann_paths):\n\t        \"\"\"\n\t        vis_root (string): Root directory of images (e.g. coco/images/)\n\t        ann_root (string): directory to store the annotation file\n\t        \"\"\"\n\t        super().__init__(vis_processor, text_processor, vis_root, ann_paths)\n\t        self.img_ids = {}\n\t        n = 0\n\t        for ann in self.annotation:\n", "            img_id = ann[\"image_id\"]\n\t            if img_id not in self.img_ids.keys():\n\t                self.img_ids[img_id] = n\n\t                n += 1\n\t    def __getitem__(self, index):\n\t        # TODO this assumes image input, not general enough\n\t        ann = self.annotation[index]\n\t        img_file = '{:0>12}.jpg'.format(ann[\"image_id\"])\n\t        image_path = os.path.join(self.vis_root, img_file)\n\t        image = Image.open(image_path).convert(\"RGB\")\n", "        image = self.vis_processor(image)\n\t        caption = self.text_processor(ann[\"caption\"])\n\t        return {\n\t            \"image\": image,\n\t            \"text_input\": caption,\n\t            \"image_id\": self.img_ids[ann[\"image_id\"]],\n\t        }\n\tclass CaptionEvalDataset(BaseDataset, __DisplMixin):\n\t    def __init__(self, vis_processor, text_processor, vis_root, ann_paths):\n\t        \"\"\"\n", "        vis_root (string): Root directory of images (e.g. coco/images/)\n\t        ann_root (string): directory to store the annotation file\n\t        split (string): val or test\n\t        \"\"\"\n\t        super().__init__(vis_processor, text_processor, vis_root, ann_paths)\n\t    def __getitem__(self, index):\n\t        ann = self.annotation[index]\n\t        image_path = os.path.join(self.vis_root, ann[\"image\"])\n\t        image = Image.open(image_path).convert(\"RGB\")\n\t        image = self.vis_processor(image)\n", "        return {\n\t            \"image\": image,\n\t            \"image_id\": ann[\"image_id\"],\n\t            \"instance_id\": ann[\"instance_id\"],\n\t        }\n"]}
{"filename": "minigpt4/datasets/datasets/cc_combine_dataset.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport os\n\tfrom PIL import Image\n\timport webdataset as wds\n\tfrom minigpt4.datasets.datasets.base_dataset import BaseDataset\n", "from minigpt4.datasets.datasets.caption_datasets import CaptionDataset\n\tclass CCCombineDataset(BaseDataset):\n\t    def __init__(self, vis_processor, text_processor, location):\n\t        super().__init__(vis_processor=vis_processor, text_processor=text_processor)\n\t        self.inner_dataset = wds.DataPipeline(\n\t            wds.ResampledShards(location),\n\t            wds.tarfile_to_samples(handler=wds.warn_and_continue),\n\t            wds.shuffle(1000, handler=wds.warn_and_continue),\n\t            wds.decode(\"pilrgb\", handler=wds.warn_and_continue),\n\t            wds.to_tuple(\"jpg\", \"json\", handler=wds.warn_and_continue),\n", "            wds.map_tuple(self.vis_processor, handler=wds.warn_and_continue),\n\t            wds.map(self.to_dict, handler=wds.warn_and_continue),\n\t        )\n\t    def to_dict(self, sample):\n\t        return {\n\t            \"image\": sample[0],\n\t            \"text_input\": self.text_processor(sample[1][\"caption\"]),\n\t        }\n\tclass CCAlignDataset(CaptionDataset):\n\t    def __getitem__(self, index):\n", "        # TODO this assumes image input, not general enough\n\t        ann = self.annotation[index]\n\t        img_file = '{}.jpg'.format(ann[\"image_id\"])\n\t        image_path = os.path.join(self.vis_root, img_file)\n\t        image = Image.open(image_path).convert(\"RGB\")\n\t        image = self.vis_processor(image)\n\t        caption = ann[\"caption\"]\n\t        return {\n\t            \"image\": image,\n\t            \"text_input\": caption,\n", "            \"image_id\": self.img_ids[ann[\"image_id\"]],\n\t        }"]}
{"filename": "minigpt4/datasets/datasets/dataloader_utils.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport time\n\timport random\n\timport torch\n\tfrom minigpt4.datasets.data_utils import move_to_cuda\n", "from torch.utils.data import DataLoader\n\tclass MultiIterLoader:\n\t    \"\"\"\n\t    A simple wrapper for iterating over multiple iterators.\n\t    Args:\n\t        loaders (List[Loader]): List of Iterator loaders.\n\t        ratios (List[float]): List of ratios to sample from each loader. If None, all loaders are sampled uniformly.\n\t    \"\"\"\n\t    def __init__(self, loaders, ratios=None):\n\t        # assert all loaders has __next__ method\n", "        for loader in loaders:\n\t            assert hasattr(\n\t                loader, \"__next__\"\n\t            ), \"Loader {} has no __next__ method.\".format(loader)\n\t        if ratios is None:\n\t            ratios = [1.0] * len(loaders)\n\t        else:\n\t            assert len(ratios) == len(loaders)\n\t            ratios = [float(ratio) / sum(ratios) for ratio in ratios]\n\t        self.loaders = loaders\n", "        self.ratios = ratios\n\t    def __next__(self):\n\t        # random sample from each loader by ratio\n\t        loader_idx = random.choices(range(len(self.loaders)), self.ratios, k=1)[0]\n\t        return next(self.loaders[loader_idx])\n\tclass PrefetchLoader(object):\n\t    \"\"\"\n\t    Modified from https://github.com/ChenRocks/UNITER.\n\t    overlap compute and cuda data transfer\n\t    (copied and then modified from nvidia apex)\n", "    \"\"\"\n\t    def __init__(self, loader):\n\t        self.loader = loader\n\t        self.stream = torch.cuda.Stream()\n\t    def __iter__(self):\n\t        loader_it = iter(self.loader)\n\t        self.preload(loader_it)\n\t        batch = self.next(loader_it)\n\t        while batch is not None:\n\t            is_tuple = isinstance(batch, tuple)\n", "            if is_tuple:\n\t                task, batch = batch\n\t            if is_tuple:\n\t                yield task, batch\n\t            else:\n\t                yield batch\n\t            batch = self.next(loader_it)\n\t    def __len__(self):\n\t        return len(self.loader)\n\t    def preload(self, it):\n", "        try:\n\t            self.batch = next(it)\n\t        except StopIteration:\n\t            self.batch = None\n\t            return\n\t        # if record_stream() doesn't work, another option is to make sure\n\t        # device inputs are created on the main stream.\n\t        # self.next_input_gpu = torch.empty_like(self.next_input,\n\t        #                                        device='cuda')\n\t        # self.next_target_gpu = torch.empty_like(self.next_target,\n", "        #                                         device='cuda')\n\t        # Need to make sure the memory allocated for next_* is not still in use\n\t        # by the main stream at the time we start copying to next_*:\n\t        # self.stream.wait_stream(torch.cuda.current_stream())\n\t        with torch.cuda.stream(self.stream):\n\t            self.batch = move_to_cuda(self.batch)\n\t            # more code for the alternative if record_stream() doesn't work:\n\t            # copy_ will record the use of the pinned source tensor in this\n\t            # side stream.\n\t            # self.next_input_gpu.copy_(self.next_input, non_blocking=True)\n", "            # self.next_target_gpu.copy_(self.next_target, non_blocking=True)\n\t            # self.next_input = self.next_input_gpu\n\t            # self.next_target = self.next_target_gpu\n\t    def next(self, it):\n\t        torch.cuda.current_stream().wait_stream(self.stream)\n\t        batch = self.batch\n\t        if batch is not None:\n\t            record_cuda_stream(batch)\n\t        self.preload(it)\n\t        return batch\n", "    def __getattr__(self, name):\n\t        method = self.loader.__getattribute__(name)\n\t        return method\n\tdef record_cuda_stream(batch):\n\t    if isinstance(batch, torch.Tensor):\n\t        batch.record_stream(torch.cuda.current_stream())\n\t    elif isinstance(batch, list) or isinstance(batch, tuple):\n\t        for t in batch:\n\t            record_cuda_stream(t)\n\t    elif isinstance(batch, dict):\n", "        for t in batch.values():\n\t            record_cuda_stream(t)\n\t    else:\n\t        pass\n\tclass IterLoader:\n\t    \"\"\"\n\t    A wrapper to convert DataLoader as an infinite iterator.\n\t    Modified from:\n\t        https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/iter_based_runner.py\n\t    \"\"\"\n", "    def __init__(self, dataloader: DataLoader, use_distributed: bool = False):\n\t        self._dataloader = dataloader\n\t        self.iter_loader = iter(self._dataloader)\n\t        self._use_distributed = use_distributed\n\t        self._epoch = 0\n\t    @property\n\t    def epoch(self) -> int:\n\t        return self._epoch\n\t    def __next__(self):\n\t        try:\n", "            data = next(self.iter_loader)\n\t        except StopIteration:\n\t            self._epoch += 1\n\t            if hasattr(self._dataloader.sampler, \"set_epoch\") and self._use_distributed:\n\t                self._dataloader.sampler.set_epoch(self._epoch)\n\t            time.sleep(2)  # Prevent possible deadlock during epoch transition\n\t            self.iter_loader = iter(self._dataloader)\n\t            data = next(self.iter_loader)\n\t        return data\n\t    def __iter__(self):\n", "        return self\n\t    def __len__(self):\n\t        return len(self._dataloader)\n"]}
{"filename": "minigpt4/tasks/image_text_pretrain.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.tasks.base_task import BaseTask\n\t@registry.register_task(\"image_text_pretrain\")\n\tclass ImageTextPretrainTask(BaseTask):\n", "    def __init__(self):\n\t        super().__init__()\n\t    def evaluation(self, model, data_loader, cuda_enabled=True):\n\t        pass\n"]}
{"filename": "minigpt4/tasks/base_task.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport logging\n\timport os\n\timport torch\n\timport torch.distributed as dist\n", "from minigpt4.common.dist_utils import get_rank, get_world_size, is_main_process, is_dist_avail_and_initialized\n\tfrom minigpt4.common.logger import MetricLogger, SmoothedValue\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.datasets.data_utils import prepare_sample\n\tclass BaseTask:\n\t    def __init__(self, **kwargs):\n\t        super().__init__()\n\t        self.inst_id_key = \"instance_id\"\n\t    @classmethod\n\t    def setup_task(cls, **kwargs):\n", "        return cls()\n\t    def build_model(self, cfg):\n\t        model_config = cfg.model_cfg\n\t        model_cls = registry.get_model_class(model_config.arch)\n\t        return model_cls.from_config(model_config)\n\t    def build_datasets(self, cfg):\n\t        \"\"\"\n\t        Build a dictionary of datasets, keyed by split 'train', 'valid', 'test'.\n\t        Download dataset and annotations automatically if not exist.\n\t        Args:\n", "            cfg (common.config.Config): _description_\n\t        Returns:\n\t            dict: Dictionary of torch.utils.data.Dataset objects by split.\n\t        \"\"\"\n\t        datasets = dict()\n\t        datasets_config = cfg.datasets_cfg\n\t        assert len(datasets_config) > 0, \"At least one dataset has to be specified.\"\n\t        for name in datasets_config:\n\t            dataset_config = datasets_config[name]\n\t            builder = registry.get_builder_class(name)(dataset_config)\n", "            dataset = builder.build_datasets()\n\t            dataset['train'].name = name\n\t            if 'sample_ratio' in dataset_config:\n\t                dataset['train'].sample_ratio = dataset_config.sample_ratio\n\t            datasets[name] = dataset\n\t        return datasets\n\t    def train_step(self, model, samples):\n\t        loss = model(samples)[\"loss\"]\n\t        return loss\n\t    def valid_step(self, model, samples):\n", "        raise NotImplementedError\n\t    def before_evaluation(self, model, dataset, **kwargs):\n\t        model.before_evaluation(dataset=dataset, task_type=type(self))\n\t    def after_evaluation(self, **kwargs):\n\t        pass\n\t    def inference_step(self):\n\t        raise NotImplementedError\n\t    def evaluation(self, model, data_loader, cuda_enabled=True):\n\t        metric_logger = MetricLogger(delimiter=\"  \")\n\t        header = \"Evaluation\"\n", "        # TODO make it configurable\n\t        print_freq = 10\n\t        results = []\n\t        for samples in metric_logger.log_every(data_loader, print_freq, header):\n\t            samples = prepare_sample(samples, cuda_enabled=cuda_enabled)\n\t            eval_output = self.valid_step(model=model, samples=samples)\n\t            results.extend(eval_output)\n\t        if is_dist_avail_and_initialized():\n\t            dist.barrier()\n\t        return results\n", "    def train_epoch(\n\t        self,\n\t        epoch,\n\t        model,\n\t        data_loader,\n\t        optimizer,\n\t        lr_scheduler,\n\t        scaler=None,\n\t        cuda_enabled=False,\n\t        log_freq=50,\n", "        accum_grad_iters=1,\n\t    ):\n\t        return self._train_inner_loop(\n\t            epoch=epoch,\n\t            iters_per_epoch=lr_scheduler.iters_per_epoch,\n\t            model=model,\n\t            data_loader=data_loader,\n\t            optimizer=optimizer,\n\t            scaler=scaler,\n\t            lr_scheduler=lr_scheduler,\n", "            log_freq=log_freq,\n\t            cuda_enabled=cuda_enabled,\n\t            accum_grad_iters=accum_grad_iters,\n\t        )\n\t    def train_iters(\n\t        self,\n\t        epoch,\n\t        start_iters,\n\t        iters_per_inner_epoch,\n\t        model,\n", "        data_loader,\n\t        optimizer,\n\t        lr_scheduler,\n\t        scaler=None,\n\t        cuda_enabled=False,\n\t        log_freq=50,\n\t        accum_grad_iters=1,\n\t    ):\n\t        return self._train_inner_loop(\n\t            epoch=epoch,\n", "            start_iters=start_iters,\n\t            iters_per_epoch=iters_per_inner_epoch,\n\t            model=model,\n\t            data_loader=data_loader,\n\t            optimizer=optimizer,\n\t            scaler=scaler,\n\t            lr_scheduler=lr_scheduler,\n\t            log_freq=log_freq,\n\t            cuda_enabled=cuda_enabled,\n\t            accum_grad_iters=accum_grad_iters,\n", "        )\n\t    def _train_inner_loop(\n\t        self,\n\t        epoch,\n\t        iters_per_epoch,\n\t        model,\n\t        data_loader,\n\t        optimizer,\n\t        lr_scheduler,\n\t        scaler=None,\n", "        start_iters=None,\n\t        log_freq=50,\n\t        cuda_enabled=False,\n\t        accum_grad_iters=1,\n\t    ):\n\t        \"\"\"\n\t        An inner training loop compatible with both epoch-based and iter-based training.\n\t        When using epoch-based, training stops after one epoch; when using iter-based,\n\t        training stops after #iters_per_epoch iterations.\n\t        \"\"\"\n", "        use_amp = scaler is not None\n\t        if not hasattr(data_loader, \"__next__\"):\n\t            # convert to iterator if not already\n\t            data_loader = iter(data_loader)\n\t        metric_logger = MetricLogger(delimiter=\"  \")\n\t        metric_logger.add_meter(\"lr\", SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n\t        metric_logger.add_meter(\"loss\", SmoothedValue(window_size=1, fmt=\"{value:.4f}\"))\n\t        # if iter-based runner, schedule lr based on inner epoch.\n\t        logging.info(\n\t            \"Start training epoch {}, {} iters per inner epoch.\".format(\n", "                epoch, iters_per_epoch\n\t            )\n\t        )\n\t        header = \"Train: data epoch: [{}]\".format(epoch)\n\t        if start_iters is None:\n\t            # epoch-based runner\n\t            inner_epoch = epoch\n\t        else:\n\t            # In iter-based runner, we schedule the learning rate based on iterations.\n\t            inner_epoch = start_iters // iters_per_epoch\n", "            header = header + \"; inner epoch [{}]\".format(inner_epoch)\n\t        for i in metric_logger.log_every(range(iters_per_epoch), log_freq, header):\n\t            # if using iter-based runner, we stop after iters_per_epoch iterations.\n\t            if i >= iters_per_epoch:\n\t                break\n\t            samples = next(data_loader)\n\t            samples = prepare_sample(samples, cuda_enabled=cuda_enabled)\n\t            samples.update(\n\t                {\n\t                    \"epoch\": inner_epoch,\n", "                    \"num_iters_per_epoch\": iters_per_epoch,\n\t                    \"iters\": i,\n\t                }\n\t            )\n\t            lr_scheduler.step(cur_epoch=inner_epoch, cur_step=i)\n\t            with torch.cuda.amp.autocast(enabled=use_amp):\n\t                loss = self.train_step(model=model, samples=samples)\n\t            # after_train_step()\n\t            if use_amp:\n\t                scaler.scale(loss).backward()\n", "            else:\n\t                loss.backward()\n\t            # update gradients every accum_grad_iters iterations\n\t            if (i + 1) % accum_grad_iters == 0:\n\t                if use_amp:\n\t                    scaler.step(optimizer)\n\t                    scaler.update()                     \n\t                else:    \n\t                    optimizer.step()\n\t                optimizer.zero_grad()\n", "            metric_logger.update(loss=loss.item())\n\t            metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\t        # after train_epoch()\n\t        # gather the stats from all processes\n\t        metric_logger.synchronize_between_processes()\n\t        logging.info(\"Averaged stats: \" + str(metric_logger.global_avg()))\n\t        return {\n\t            k: \"{:.3f}\".format(meter.global_avg)\n\t            for k, meter in metric_logger.meters.items()\n\t        }\n", "    @staticmethod\n\t    def save_result(result, result_dir, filename, remove_duplicate=\"\"):\n\t        import json\n\t        result_file = os.path.join(\n\t            result_dir, \"%s_rank%d.json\" % (filename, get_rank())\n\t        )\n\t        final_result_file = os.path.join(result_dir, \"%s.json\" % filename)\n\t        json.dump(result, open(result_file, \"w\"))\n\t        if is_dist_avail_and_initialized():\n\t            dist.barrier()\n", "        if is_main_process():\n\t            logging.warning(\"rank %d starts merging results.\" % get_rank())\n\t            # combine results from all processes\n\t            result = []\n\t            for rank in range(get_world_size()):\n\t                result_file = os.path.join(\n\t                    result_dir, \"%s_rank%d.json\" % (filename, rank)\n\t                )\n\t                res = json.load(open(result_file, \"r\"))\n\t                result += res\n", "            if remove_duplicate:\n\t                result_new = []\n\t                id_list = []\n\t                for res in result:\n\t                    if res[remove_duplicate] not in id_list:\n\t                        id_list.append(res[remove_duplicate])\n\t                        result_new.append(res)\n\t                result = result_new\n\t            json.dump(result, open(final_result_file, \"w\"))\n\t            print(\"result file saved to %s\" % final_result_file)\n", "        return final_result_file\n"]}
{"filename": "minigpt4/tasks/__init__.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.tasks.base_task import BaseTask\n\tfrom minigpt4.tasks.image_text_pretrain import ImageTextPretrainTask\n\tdef setup_task(cfg):\n", "    assert \"task\" in cfg.run_cfg, \"Task name must be provided.\"\n\t    task_name = cfg.run_cfg.task\n\t    task = registry.get_task_class(task_name).setup_task(cfg=cfg)\n\t    assert task is not None, \"Task {} not properly registered.\".format(task_name)\n\t    return task\n\t__all__ = [\n\t    \"BaseTask\",\n\t    \"ImageTextPretrainTask\",\n\t]\n"]}
{"filename": "minigpt4/conversation/__init__.py", "chunked_list": []}
{"filename": "minigpt4/conversation/conversation.py", "chunked_list": ["import argparse\n\timport time\n\tfrom PIL import Image\n\timport torch\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n\tfrom transformers import StoppingCriteria, StoppingCriteriaList\n\timport dataclasses\n\tfrom enum import auto, Enum\n\tfrom typing import List, Tuple, Any\n\tfrom minigpt4.common.registry import registry\n", "class SeparatorStyle(Enum):\n\t    \"\"\"Different separator style.\"\"\"\n\t    SINGLE = auto()\n\t    TWO = auto()\n\t@dataclasses.dataclass\n\tclass Conversation:\n\t    \"\"\"A class that keeps all conversation history.\"\"\"\n\t    system: str\n\t    roles: List[str]\n\t    messages: List[List[str]]\n", "    offset: int\n\t    # system_img: List[Image.Image] = []\n\t    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n\t    sep: str = \"###\"\n\t    sep2: str = None\n\t    skip_next: bool = False\n\t    conv_id: Any = None\n\t    def get_prompt(self):\n\t        if self.sep_style == SeparatorStyle.SINGLE:\n\t            ret = self.system + self.sep\n", "            for role, message in self.messages:\n\t                if message:\n\t                    ret += role + \": \" + message + self.sep\n\t                else:\n\t                    ret += role + \":\"\n\t            return ret\n\t        elif self.sep_style == SeparatorStyle.TWO:\n\t            seps = [self.sep, self.sep2]\n\t            ret = self.system + seps[0]\n\t            for i, (role, message) in enumerate(self.messages):\n", "                if message:\n\t                    ret += role + \": \" + message + seps[i % 2]\n\t                else:\n\t                    ret += role + \":\"\n\t            return ret\n\t        else:\n\t            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\t    def append_message(self, role, message):\n\t        self.messages.append([role, message])\n\t    def to_gradio_chatbot(self):\n", "        ret = []\n\t        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n\t            if i % 2 == 0:\n\t                ret.append([msg, None])\n\t            else:\n\t                ret[-1][-1] = msg\n\t        return ret\n\t    def copy(self):\n\t        return Conversation(\n\t            system=self.system,\n", "            # system_img=self.system_img,\n\t            roles=self.roles,\n\t            messages=[[x, y] for x, y in self.messages],\n\t            offset=self.offset,\n\t            sep_style=self.sep_style,\n\t            sep=self.sep,\n\t            sep2=self.sep2,\n\t            conv_id=self.conv_id)\n\t    def dict(self):\n\t        return {\n", "            \"system\": self.system,\n\t            # \"system_img\": self.system_img,\n\t            \"roles\": self.roles,\n\t            \"messages\": self.messages,\n\t            \"offset\": self.offset,\n\t            \"sep\": self.sep,\n\t            \"sep2\": self.sep2,\n\t            \"conv_id\": self.conv_id,\n\t        }\n\tclass StoppingCriteriaSub(StoppingCriteria):\n", "    def __init__(self, stops=[], encounters=1):\n\t        super().__init__()\n\t        self.stops = stops\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n\t        for stop in self.stops:\n\t            if torch.all((stop == input_ids[0][-len(stop):])).item():\n\t                return True\n\t        return False\n\tCONV_VISION = Conversation(\n\t    system=\"Give the following image: <Img>ImageContent</Img>. \"\n", "           \"You will be able to see the image once I provide it to you. Please answer my questions.\",\n\t    roles=(\"Human\", \"Assistant\"),\n\t    messages=[],\n\t    offset=2,\n\t    sep_style=SeparatorStyle.SINGLE,\n\t    sep=\"###\",\n\t)\n\tclass Chat:\n\t    def __init__(self, model, vis_processor, device='cuda:0'):\n\t        self.device = device\n", "        self.model = model\n\t        self.vis_processor = vis_processor\n\t        stop_words_ids = [torch.tensor([835]).to(self.device),\n\t                          torch.tensor([2277, 29937]).to(self.device)]  # '###' can be encoded in two different ways.\n\t        self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n\t    def ask(self, text, conv):\n\t        if len(conv.messages) > 0 and conv.messages[-1][0] == conv.roles[0] \\\n\t                and conv.messages[-1][1][-6:] == '</Img>':  # last message is image.\n\t            conv.messages[-1][1] = ' '.join([conv.messages[-1][1], text])\n\t        else:\n", "            conv.append_message(conv.roles[0], text)\n\t    def answer(self, conv, img_list, max_new_tokens=200, num_beams=5, min_length=1, top_p=0.9,\n\t               repetition_penalty=1.0, length_penalty=1, temperature=1, max_length=2000):\n\t        conv.append_message(conv.roles[1], None)\n\t        embs = self.get_context_emb(conv, img_list)\n\t        # current_max_len = embs.shape[1] + max_new_tokens + 100\n\t        # begin_idx = max(0, current_max_len - max_length)\n\t        # embs = embs[:, begin_idx:]\n\t        outputs = self.model.llama_model.generate(\n\t            inputs_embeds=embs,\n", "            max_new_tokens=max_new_tokens,\n\t            stopping_criteria=self.stopping_criteria,\n\t            num_beams=num_beams,\n\t            min_length=min_length,\n\t            top_p=top_p,\n\t            repetition_penalty=repetition_penalty,\n\t            length_penalty=length_penalty,\n\t            temperature=temperature,\n\t        )\n\t        output_token = outputs[0]\n", "        if output_token[0] == 0:\n\t            output_token = output_token[1:]\n\t        output_text = self.model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n\t        output_text = output_text.split('###')[0]  # remove the stop sign '###'\n\t        output_text = output_text.split('Assistant:')[-1].strip()\n\t        conv.messages[-1][1] = output_text\n\t        return output_text, output_token.cpu().numpy()\n\t    def upload_img(self, image, conv, img_list):\n\t        if isinstance(image, str):  # is a image path\n\t            raw_image = Image.open(image).convert('RGB')\n", "            image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\n\t        elif isinstance(image, Image.Image):\n\t            raw_image = image\n\t            image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\n\t        elif isinstance(image, torch.Tensor):\n\t            if len(image.shape) == 3:\n\t                image = image.unsqueeze(0)\n\t            image = image.to(self.device)\n\t        image_emb, _ = self.model.encode_img(image)\n\t        img_list.append(image_emb)\n", "        conv.append_message(conv.roles[0], \"<Img><ImageHere></Img>\")\n\t        msg = \"Received.\"\n\t        # self.conv.append_message(self.conv.roles[1], msg)\n\t        return msg\n\t    def get_context_emb(self, conv, img_list):\n\t        prompt = conv.get_prompt()\n\t        prompt_segs = prompt.split('<ImageHere>')\n\t        assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n\t        seg_tokens = [\n\t            self.model.llama_tokenizer(\n", "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(self.device).input_ids\n\t            # only add bos to the first seg\n\t            for i, seg in enumerate(prompt_segs)\n\t        ]\n\t        seg_embs = [self.model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n\t        mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n\t        mixed_embs = torch.cat(mixed_embs, dim=1)\n\t        return mixed_embs\n"]}
{"filename": "minigpt4/runners/runner_base.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\timport datetime\n\timport json\n\timport logging\n\timport os\n", "import time\n\tfrom pathlib import Path\n\timport torch\n\timport torch.distributed as dist\n\timport webdataset as wds\n\tfrom minigpt4.common.dist_utils import (\n\t    download_cached_file,\n\t    get_rank,\n\t    get_world_size,\n\t    is_main_process,\n", "    main_process,\n\t)\n\tfrom minigpt4.common.registry import registry\n\tfrom minigpt4.common.utils import is_url\n\tfrom minigpt4.datasets.data_utils import concat_datasets, reorg_datasets_by_split, ChainDataset\n\tfrom minigpt4.datasets.datasets.dataloader_utils import (\n\t    IterLoader,\n\t    MultiIterLoader,\n\t    PrefetchLoader,\n\t)\n", "from torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom torch.utils.data import DataLoader, DistributedSampler\n\t@registry.register_runner(\"runner_base\")\n\tclass RunnerBase:\n\t    \"\"\"\n\t    A runner class to train and evaluate a model given a task and datasets.\n\t    The runner uses pytorch distributed data parallel by default. Future release\n\t    will support other distributed frameworks.\n\t    \"\"\"\n\t    def __init__(self, cfg, task, model, datasets, job_id):\n", "        self.config = cfg\n\t        self.job_id = job_id\n\t        self.task = task\n\t        self.datasets = datasets\n\t        self._model = model\n\t        self._wrapped_model = None\n\t        self._device = None\n\t        self._optimizer = None\n\t        self._scaler = None\n\t        self._dataloaders = None\n", "        self._lr_sched = None\n\t        self.start_epoch = 0\n\t        # self.setup_seeds()\n\t        self.setup_output_dir()\n\t    @property\n\t    def device(self):\n\t        if self._device is None:\n\t            self._device = torch.device(self.config.run_cfg.device)\n\t        return self._device\n\t    @property\n", "    def use_distributed(self):\n\t        return self.config.run_cfg.distributed\n\t    @property\n\t    def model(self):\n\t        \"\"\"\n\t        A property to get the DDP-wrapped model on the device.\n\t        \"\"\"\n\t        # move model to device\n\t        if self._model.device != self.device:\n\t            self._model = self._model.to(self.device)\n", "            # distributed training wrapper\n\t            if self.use_distributed:\n\t                if self._wrapped_model is None:\n\t                    self._wrapped_model = DDP(\n\t                        self._model, device_ids=[self.config.run_cfg.gpu]\n\t                    )\n\t            else:\n\t                self._wrapped_model = self._model\n\t        return self._wrapped_model\n\t    @property\n", "    def optimizer(self):\n\t        # TODO make optimizer class and configurations\n\t        if self._optimizer is None:\n\t            num_parameters = 0\n\t            p_wd, p_non_wd = [], []\n\t            for n, p in self.model.named_parameters():\n\t                if not p.requires_grad:\n\t                    continue  # frozen weights\n\t                print(n)\n\t                if p.ndim < 2 or \"bias\" in n or \"ln\" in n or \"bn\" in n:\n", "                    p_non_wd.append(p)\n\t                else:\n\t                    p_wd.append(p)\n\t                num_parameters += p.data.nelement()\n\t            logging.info(\"number of trainable parameters: %d\" % num_parameters)\n\t            optim_params = [\n\t                {\n\t                    \"params\": p_wd,\n\t                    \"weight_decay\": float(self.config.run_cfg.weight_decay),\n\t                },\n", "                {\"params\": p_non_wd, \"weight_decay\": 0},\n\t            ]\n\t            beta2 = self.config.run_cfg.get(\"beta2\", 0.999)\n\t            self._optimizer = torch.optim.AdamW(\n\t                optim_params,\n\t                lr=float(self.config.run_cfg.init_lr),\n\t                weight_decay=float(self.config.run_cfg.weight_decay),\n\t                betas=(0.9, beta2),\n\t            )\n\t        return self._optimizer\n", "    @property\n\t    def scaler(self):\n\t        amp = self.config.run_cfg.get(\"amp\", False)\n\t        if amp:\n\t            if self._scaler is None:\n\t                self._scaler = torch.cuda.amp.GradScaler()\n\t        return self._scaler\n\t    @property\n\t    def lr_scheduler(self):\n\t        \"\"\"\n", "        A property to get and create learning rate scheduler by split just in need.\n\t        \"\"\"\n\t        if self._lr_sched is None:\n\t            lr_sched_cls = registry.get_lr_scheduler_class(self.config.run_cfg.lr_sched)\n\t            # max_epoch = self.config.run_cfg.max_epoch\n\t            max_epoch = self.max_epoch\n\t            # min_lr = self.config.run_cfg.min_lr\n\t            min_lr = self.min_lr\n\t            # init_lr = self.config.run_cfg.init_lr\n\t            init_lr = self.init_lr\n", "            # optional parameters\n\t            decay_rate = self.config.run_cfg.get(\"lr_decay_rate\", None)\n\t            warmup_start_lr = self.config.run_cfg.get(\"warmup_lr\", -1)\n\t            warmup_steps = self.config.run_cfg.get(\"warmup_steps\", 0)\n\t            iters_per_epoch = self.config.run_cfg.get(\"iters_per_epoch\", None)\n\t            if iters_per_epoch is None:\n\t                try:\n\t                    iters_per_epoch = len(self.dataloaders['train'])\n\t                except (AttributeError, TypeError):\n\t                    iters_per_epoch = 10000\n", "            self._lr_sched = lr_sched_cls(\n\t                optimizer=self.optimizer,\n\t                max_epoch=max_epoch,\n\t                iters_per_epoch=iters_per_epoch,\n\t                min_lr=min_lr,\n\t                init_lr=init_lr,\n\t                decay_rate=decay_rate,\n\t                warmup_start_lr=warmup_start_lr,\n\t                warmup_steps=warmup_steps,\n\t            )\n", "        return self._lr_sched\n\t    @property\n\t    def dataloaders(self) -> dict:\n\t        \"\"\"\n\t        A property to get and create dataloaders by split just in need.\n\t        If no train_dataset_ratio is provided, concatenate map-style datasets and\n\t        chain wds.DataPipe datasets separately. Training set becomes a tuple\n\t        (ConcatDataset, ChainDataset), both are optional but at least one of them is\n\t        required. The resultant ConcatDataset and ChainDataset will be sampled evenly.\n\t        If train_dataset_ratio is provided, create a MultiIterLoader to sample\n", "        each dataset by ratios during training.\n\t        Currently do not support multiple datasets for validation and test.\n\t        Returns:\n\t            dict: {split_name: (tuples of) dataloader}\n\t        \"\"\"\n\t        if self._dataloaders is None:\n\t            # concatenate map-style datasets and chain wds.DataPipe datasets separately\n\t            # training set becomes a tuple (ConcatDataset, ChainDataset), both are\n\t            # optional but at least one of them is required. The resultant ConcatDataset\n\t            # and ChainDataset will be sampled evenly.\n", "            logging.info(\n\t                \"dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).\"\n\t            )\n\t            datasets = reorg_datasets_by_split(self.datasets)\n\t            self.datasets = datasets\n\t            # self.datasets = concat_datasets(datasets)\n\t            # print dataset statistics after concatenation/chaining\n\t            for split_name in self.datasets:\n\t                if isinstance(self.datasets[split_name], tuple) or isinstance(\n\t                    self.datasets[split_name], list\n", "                ):\n\t                    # mixed wds.DataPipeline and torch.utils.data.Dataset\n\t                    num_records = sum(\n\t                        [\n\t                            len(d)\n\t                            if not type(d) in [wds.DataPipeline, ChainDataset]\n\t                            else 0\n\t                            for d in self.datasets[split_name]\n\t                        ]\n\t                    )\n", "                else:\n\t                    if hasattr(self.datasets[split_name], \"__len__\"):\n\t                        # a single map-style dataset\n\t                        num_records = len(self.datasets[split_name])\n\t                    else:\n\t                        # a single wds.DataPipeline\n\t                        num_records = -1\n\t                        logging.info(\n\t                            \"Only a single wds.DataPipeline dataset, no __len__ attribute.\"\n\t                        )\n", "                if num_records >= 0:\n\t                    logging.info(\n\t                        \"Loaded {} records for {} split from the dataset.\".format(\n\t                            num_records, split_name\n\t                        )\n\t                    )\n\t            # create dataloaders\n\t            split_names = sorted(self.datasets.keys())\n\t            datasets = [self.datasets[split] for split in split_names]\n\t            is_trains = [split in self.train_splits for split in split_names]\n", "            batch_sizes = [\n\t                self.config.run_cfg.batch_size_train\n\t                if split == \"train\"\n\t                else self.config.run_cfg.batch_size_eval\n\t                for split in split_names\n\t            ]\n\t            collate_fns = []\n\t            for dataset in datasets:\n\t                if isinstance(dataset, tuple) or isinstance(dataset, list):\n\t                    collate_fns.append([getattr(d, \"collater\", None) for d in dataset])\n", "                else:\n\t                    collate_fns.append(getattr(dataset, \"collater\", None))\n\t            dataloaders = self.create_loaders(\n\t                datasets=datasets,\n\t                num_workers=self.config.run_cfg.num_workers,\n\t                batch_sizes=batch_sizes,\n\t                is_trains=is_trains,\n\t                collate_fns=collate_fns,\n\t            )\n\t            self._dataloaders = {k: v for k, v in zip(split_names, dataloaders)}\n", "        return self._dataloaders\n\t    @property\n\t    def cuda_enabled(self):\n\t        return self.device.type == \"cuda\"\n\t    @property\n\t    def max_epoch(self):\n\t        return int(self.config.run_cfg.max_epoch)\n\t    @property\n\t    def log_freq(self):\n\t        log_freq = self.config.run_cfg.get(\"log_freq\", 50)\n", "        return int(log_freq)\n\t    @property\n\t    def init_lr(self):\n\t        return float(self.config.run_cfg.init_lr)\n\t    @property\n\t    def min_lr(self):\n\t        return float(self.config.run_cfg.min_lr)\n\t    @property\n\t    def accum_grad_iters(self):\n\t        return int(self.config.run_cfg.get(\"accum_grad_iters\", 1))\n", "    @property\n\t    def valid_splits(self):\n\t        valid_splits = self.config.run_cfg.get(\"valid_splits\", [])\n\t        if len(valid_splits) == 0:\n\t            logging.info(\"No validation splits found.\")\n\t        return valid_splits\n\t    @property\n\t    def test_splits(self):\n\t        test_splits = self.config.run_cfg.get(\"test_splits\", [])\n\t        return test_splits\n", "    @property\n\t    def train_splits(self):\n\t        train_splits = self.config.run_cfg.get(\"train_splits\", [])\n\t        if len(train_splits) == 0:\n\t            logging.info(\"Empty train splits.\")\n\t        return train_splits\n\t    @property\n\t    def evaluate_only(self):\n\t        \"\"\"\n\t        Set to True to skip training.\n", "        \"\"\"\n\t        return self.config.run_cfg.evaluate\n\t    @property\n\t    def use_dist_eval_sampler(self):\n\t        return self.config.run_cfg.get(\"use_dist_eval_sampler\", True)\n\t    @property\n\t    def resume_ckpt_path(self):\n\t        return self.config.run_cfg.get(\"resume_ckpt_path\", None)\n\t    @property\n\t    def train_loader(self):\n", "        train_dataloader = self.dataloaders[\"train\"]\n\t        return train_dataloader\n\t    def setup_output_dir(self):\n\t        lib_root = Path(registry.get_path(\"library_root\"))\n\t        output_dir = lib_root / self.config.run_cfg.output_dir / self.job_id\n\t        result_dir = output_dir / \"result\"\n\t        output_dir.mkdir(parents=True, exist_ok=True)\n\t        result_dir.mkdir(parents=True, exist_ok=True)\n\t        registry.register_path(\"result_dir\", str(result_dir))\n\t        registry.register_path(\"output_dir\", str(output_dir))\n", "        self.result_dir = result_dir\n\t        self.output_dir = output_dir\n\t    def train(self):\n\t        start_time = time.time()\n\t        best_agg_metric = 0\n\t        best_epoch = 0\n\t        self.log_config()\n\t        # resume from checkpoint if specified\n\t        if not self.evaluate_only and self.resume_ckpt_path is not None:\n\t            self._load_checkpoint(self.resume_ckpt_path)\n", "        for cur_epoch in range(self.start_epoch, self.max_epoch):\n\t            # training phase\n\t            if not self.evaluate_only:\n\t                logging.info(\"Start training\")\n\t                train_stats = self.train_epoch(cur_epoch)\n\t                self.log_stats(split_name=\"train\", stats=train_stats)\n\t            # evaluation phase\n\t            if len(self.valid_splits) > 0:\n\t                for split_name in self.valid_splits:\n\t                    logging.info(\"Evaluating on {}.\".format(split_name))\n", "                    val_log = self.eval_epoch(\n\t                        split_name=split_name, cur_epoch=cur_epoch\n\t                    )\n\t                    if val_log is not None:\n\t                        if is_main_process():\n\t                            assert (\n\t                                \"agg_metrics\" in val_log\n\t                            ), \"No agg_metrics found in validation log.\"\n\t                            agg_metrics = val_log[\"agg_metrics\"]\n\t                            if agg_metrics > best_agg_metric and split_name == \"val\":\n", "                                best_epoch, best_agg_metric = cur_epoch, agg_metrics\n\t                                self._save_checkpoint(cur_epoch, is_best=True)\n\t                            val_log.update({\"best_epoch\": best_epoch})\n\t                            self.log_stats(val_log, split_name)\n\t            else:\n\t                # if no validation split is provided, we just save the checkpoint at the end of each epoch.\n\t                if not self.evaluate_only:\n\t                    self._save_checkpoint(cur_epoch, is_best=False)\n\t            if self.evaluate_only:\n\t                break\n", "            if self.config.run_cfg.distributed:\n\t                dist.barrier()\n\t        # testing phase\n\t        test_epoch = \"best\" if len(self.valid_splits) > 0 else cur_epoch\n\t        self.evaluate(cur_epoch=test_epoch, skip_reload=self.evaluate_only)\n\t        total_time = time.time() - start_time\n\t        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t        logging.info(\"Training time {}\".format(total_time_str))\n\t    def evaluate(self, cur_epoch=\"best\", skip_reload=False):\n\t        test_logs = dict()\n", "        if len(self.test_splits) > 0:\n\t            for split_name in self.test_splits:\n\t                test_logs[split_name] = self.eval_epoch(\n\t                    split_name=split_name, cur_epoch=cur_epoch, skip_reload=skip_reload\n\t                )\n\t            return test_logs\n\t    def train_epoch(self, epoch):\n\t        # train\n\t        self.model.train()\n\t        return self.task.train_epoch(\n", "            epoch=epoch,\n\t            model=self.model,\n\t            data_loader=self.train_loader,\n\t            optimizer=self.optimizer,\n\t            scaler=self.scaler,\n\t            lr_scheduler=self.lr_scheduler,\n\t            cuda_enabled=self.cuda_enabled,\n\t            log_freq=self.log_freq,\n\t            accum_grad_iters=self.accum_grad_iters,\n\t        )\n", "    @torch.no_grad()\n\t    def eval_epoch(self, split_name, cur_epoch, skip_reload=False):\n\t        \"\"\"\n\t        Evaluate the model on a given split.\n\t        Args:\n\t            split_name (str): name of the split to evaluate on.\n\t            cur_epoch (int): current epoch.\n\t            skip_reload_best (bool): whether to skip reloading the best checkpoint.\n\t                During training, we will reload the best checkpoint for validation.\n\t                During testing, we will use provided weights and skip reloading the best checkpoint .\n", "        \"\"\"\n\t        data_loader = self.dataloaders.get(split_name, None)\n\t        assert data_loader, \"data_loader for split {} is None.\".format(split_name)\n\t        # TODO In validation, you need to compute loss as well as metrics\n\t        # TODO consider moving to model.before_evaluation()\n\t        model = self.unwrap_dist_model(self.model)\n\t        if not skip_reload and cur_epoch == \"best\":\n\t            model = self._reload_best_model(model)\n\t        model.eval()\n\t        self.task.before_evaluation(\n", "            model=model,\n\t            dataset=self.datasets[split_name],\n\t        )\n\t        results = self.task.evaluation(model, data_loader)\n\t        if results is not None:\n\t            return self.task.after_evaluation(\n\t                val_result=results,\n\t                split_name=split_name,\n\t                epoch=cur_epoch,\n\t            )\n", "    def unwrap_dist_model(self, model):\n\t        if self.use_distributed:\n\t            return model.module\n\t        else:\n\t            return model\n\t    def create_loaders(\n\t        self,\n\t        datasets,\n\t        num_workers,\n\t        batch_sizes,\n", "        is_trains,\n\t        collate_fns,\n\t        dataset_ratios=None,\n\t    ):\n\t        \"\"\"\n\t        Create dataloaders for training and validation.\n\t        \"\"\"\n\t        def _create_loader(dataset, num_workers, bsz, is_train, collate_fn):\n\t            # create a single dataloader for each split\n\t            if isinstance(dataset, ChainDataset) or isinstance(\n", "                dataset, wds.DataPipeline\n\t            ):\n\t                # wds.WebdDataset instance are chained together\n\t                # webdataset.DataPipeline has its own sampler and collate_fn\n\t                loader = iter(\n\t                    DataLoader(\n\t                        dataset,\n\t                        batch_size=bsz,\n\t                        num_workers=num_workers,\n\t                        pin_memory=True,\n", "                    )\n\t                )\n\t            else:\n\t                # map-style dataset are concatenated together\n\t                # setup distributed sampler\n\t                if self.use_distributed:\n\t                    sampler = DistributedSampler(\n\t                        dataset,\n\t                        shuffle=is_train,\n\t                        num_replicas=get_world_size(),\n", "                        rank=get_rank(),\n\t                    )\n\t                    if not self.use_dist_eval_sampler:\n\t                        # e.g. retrieval evaluation\n\t                        sampler = sampler if is_train else None\n\t                else:\n\t                    sampler = None\n\t                loader = DataLoader(\n\t                    dataset,\n\t                    batch_size=bsz,\n", "                    num_workers=num_workers,\n\t                    pin_memory=True,\n\t                    sampler=sampler,\n\t                    shuffle=sampler is None and is_train,\n\t                    collate_fn=collate_fn,\n\t                    drop_last=True if is_train else False,\n\t                )\n\t                loader = PrefetchLoader(loader)\n\t                if is_train:\n\t                    loader = IterLoader(loader, use_distributed=self.use_distributed)\n", "            return loader\n\t        loaders = []\n\t        for dataset, bsz, is_train, collate_fn in zip(\n\t            datasets, batch_sizes, is_trains, collate_fns\n\t        ):\n\t            if isinstance(dataset, list) or isinstance(dataset, tuple):\n\t                if hasattr(dataset[0], 'sample_ratio') and dataset_ratios is None:\n\t                    dataset_ratios = [d.sample_ratio for d in dataset]\n\t                loader = MultiIterLoader(\n\t                    loaders=[\n", "                        _create_loader(d, num_workers, bsz, is_train, collate_fn[i])\n\t                        for i, d in enumerate(dataset)\n\t                    ],\n\t                    ratios=dataset_ratios,\n\t                )\n\t            else:\n\t                loader = _create_loader(dataset, num_workers, bsz, is_train, collate_fn)\n\t            loaders.append(loader)\n\t        return loaders\n\t    @main_process\n", "    def _save_checkpoint(self, cur_epoch, is_best=False):\n\t        \"\"\"\n\t        Save the checkpoint at the current epoch.\n\t        \"\"\"\n\t        model_no_ddp = self.unwrap_dist_model(self.model)\n\t        param_grad_dic = {\n\t            k: v.requires_grad for (k, v) in model_no_ddp.named_parameters()\n\t        }\n\t        state_dict = model_no_ddp.state_dict()\n\t        for k in list(state_dict.keys()):\n", "            if k in param_grad_dic.keys() and not param_grad_dic[k]:\n\t                # delete parameters that do not require gradient\n\t                del state_dict[k]\n\t        save_obj = {\n\t            \"model\": state_dict,\n\t            \"optimizer\": self.optimizer.state_dict(),\n\t            \"config\": self.config.to_dict(),\n\t            \"scaler\": self.scaler.state_dict() if self.scaler else None,\n\t            \"epoch\": cur_epoch,\n\t        }\n", "        save_to = os.path.join(\n\t            self.output_dir,\n\t            \"checkpoint_{}.pth\".format(\"best\" if is_best else cur_epoch),\n\t        )\n\t        logging.info(\"Saving checkpoint at epoch {} to {}.\".format(cur_epoch, save_to))\n\t        torch.save(save_obj, save_to)\n\t    def _reload_best_model(self, model):\n\t        \"\"\"\n\t        Load the best checkpoint for evaluation.\n\t        \"\"\"\n", "        checkpoint_path = os.path.join(self.output_dir, \"checkpoint_best.pth\")\n\t        logging.info(\"Loading checkpoint from {}.\".format(checkpoint_path))\n\t        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n\t        try:\n\t            model.load_state_dict(checkpoint[\"model\"])\n\t        except RuntimeError as e:\n\t            logging.warning(\n\t                \"\"\"\n\t                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.\n\t                Trying to load the model with strict=False.\n", "                \"\"\"\n\t            )\n\t            model.load_state_dict(checkpoint[\"model\"], strict=False)\n\t        return model\n\t    def _load_checkpoint(self, url_or_filename):\n\t        \"\"\"\n\t        Resume from a checkpoint.\n\t        \"\"\"\n\t        if is_url(url_or_filename):\n\t            cached_file = download_cached_file(\n", "                url_or_filename, check_hash=False, progress=True\n\t            )\n\t            checkpoint = torch.load(cached_file, map_location=self.device, strict=False)\n\t        elif os.path.isfile(url_or_filename):\n\t            checkpoint = torch.load(url_or_filename, map_location=self.device, strict=False)\n\t        else:\n\t            raise RuntimeError(\"checkpoint url or path is invalid\")\n\t        state_dict = checkpoint[\"model\"]\n\t        self.unwrap_dist_model(self.model).load_state_dict(state_dict)\n\t        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n", "        if self.scaler and \"scaler\" in checkpoint:\n\t            self.scaler.load_state_dict(checkpoint[\"scaler\"])\n\t        self.start_epoch = checkpoint[\"epoch\"] + 1\n\t        logging.info(\"Resume checkpoint from {}\".format(url_or_filename))\n\t    @main_process\n\t    def log_stats(self, stats, split_name):\n\t        if isinstance(stats, dict):\n\t            log_stats = {**{f\"{split_name}_{k}\": v for k, v in stats.items()}}\n\t            with open(os.path.join(self.output_dir, \"log.txt\"), \"a\") as f:\n\t                f.write(json.dumps(log_stats) + \"\\n\")\n", "        elif isinstance(stats, list):\n\t            pass\n\t    @main_process\n\t    def log_config(self):\n\t        with open(os.path.join(self.output_dir, \"log.txt\"), \"a\") as f:\n\t            f.write(json.dumps(self.config.to_dict(), indent=4) + \"\\n\")\n"]}
{"filename": "minigpt4/runners/__init__.py", "chunked_list": ["\"\"\"\n\t Copyright (c) 2022, salesforce.com, inc.\n\t All rights reserved.\n\t SPDX-License-Identifier: BSD-3-Clause\n\t For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\t\"\"\"\n\tfrom minigpt4.runners.runner_base import RunnerBase\n\t__all__ = [\"RunnerBase\"]\n"]}
