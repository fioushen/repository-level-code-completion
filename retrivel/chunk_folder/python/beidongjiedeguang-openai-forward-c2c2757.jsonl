{"filename": "openai_forward/__main__.py", "chunked_list": ["import os\n\timport fire\n\timport uvicorn\n\tclass Cli:\n\t    @staticmethod\n\t    def run(port=8000, workers=1, log_chat=None):\n\t        \"\"\"\n\t        Runs the application using the Uvicorn server.\n\t        Args:\n\t            port (int): The port number on which to run the server. Default is 8000.\n", "            workers (int): The number of worker processes to run. Default is 1.\n\t            log_chat (str): whether to log llm chat. Default is None.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        if log_chat:\n\t            os.environ[\"LOG_CHAT\"] = log_chat\n\t        ssl_keyfile = os.environ.get(\"ssl_keyfile\", None) or None\n\t        ssl_certfile = os.environ.get(\"ssl_certfile\", None) or None\n\t        uvicorn.run(\n", "            app=\"openai_forward.app:app\",\n\t            host=\"0.0.0.0\",\n\t            port=port,\n\t            workers=workers,\n\t            app_dir=\"..\",\n\t            ssl_keyfile=ssl_keyfile,\n\t            ssl_certfile=ssl_certfile,\n\t        )\n\t    @staticmethod\n\t    def convert(log_folder: str = None, target_path: str = None):\n", "        \"\"\"\n\t        Converts log files in a folder to a JSONL file.\n\t        Args:\n\t            log_folder (str, optional): The path to the folder containing the log files. Default is None.\n\t            target_path (str, optional): The path to the target JSONL file. Default is None.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        from openai_forward.forwarding.settings import OPENAI_ROUTE_PREFIX\n\t        from openai_forward.helper import convert_folder_to_jsonl\n", "        print(60 * '-')\n\t        if log_folder is None:\n\t            if target_path is not None:\n\t                raise ValueError(\"target_path must be None when log_folder is None\")\n\t            _prefix_list = [i.replace(\"/\", \"_\") for i in OPENAI_ROUTE_PREFIX]\n\t            for _prefix in _prefix_list:\n\t                log_folder = f\"./Log/chat/{_prefix}\"\n\t                target_path = f\"./Log/chat{_prefix}.json\"\n\t                print(f\"Convert {log_folder}/*.log to {target_path}\")\n\t                convert_folder_to_jsonl(log_folder, target_path)\n", "                print(60 * '-')\n\t        else:\n\t            print(f\"Convert {log_folder}/*.log to {target_path}\")\n\t            convert_folder_to_jsonl(log_folder, target_path)\n\t            print(60 * '-')\n\tdef main():\n\t    fire.Fire(Cli)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "openai_forward/app.py", "chunked_list": ["from fastapi import FastAPI, Request, status\n\tfrom slowapi import Limiter, _rate_limit_exceeded_handler\n\tfrom slowapi.errors import RateLimitExceeded\n\tfrom .forwarding import get_fwd_anything_objs, get_fwd_openai_style_objs\n\tfrom .forwarding.settings import (\n\t    RATE_LIMIT_STRATEGY,\n\t    dynamic_rate_limit,\n\t    get_limiter_key,\n\t)\n\tlimiter = Limiter(key_func=get_limiter_key, strategy=RATE_LIMIT_STRATEGY)\n", "app = FastAPI(title=\"openai_forward\", version=\"0.5\")\n\tapp.state.limiter = limiter\n\tapp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\t@app.get(\n\t    \"/healthz\",\n\t    summary=\"Perform a Health Check\",\n\t    response_description=\"Return HTTP Status Code 200 (OK)\",\n\t    status_code=status.HTTP_200_OK,\n\t)\n\t@limiter.limit(dynamic_rate_limit)\n", "def healthz(request: Request):\n\t    print(request.scope.get(\"client\"))\n\t    return \"OK\"\n\tadd_route = lambda obj: app.add_route(\n\t    obj.ROUTE_PREFIX + \"{api_path:path}\",\n\t    limiter.limit(dynamic_rate_limit)(obj.reverse_proxy),\n\t    methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\", \"HEAD\", \"PATCH\", \"TRACE\"],\n\t)\n\t[add_route(obj) for obj in get_fwd_openai_style_objs()]\n\t[add_route(obj) for obj in get_fwd_anything_objs()]\n"]}
{"filename": "openai_forward/config.py", "chunked_list": ["import functools\n\timport logging\n\timport os\n\timport sys\n\timport time\n\tfrom loguru import logger\n\tfrom rich import print\n\tfrom rich.panel import Panel\n\tfrom rich.table import Table\n\tdef print_startup_info(base_url, route_prefix, api_key, fwd_key, log_chat, style):\n", "    \"\"\"\n\t    Prints the startup information of the application.\n\t    \"\"\"\n\t    try:\n\t        from dotenv import load_dotenv\n\t        load_dotenv(\".env\")\n\t    except Exception:\n\t        ...\n\t    route_prefix = route_prefix or \"/\"\n\t    if not isinstance(api_key, str):\n", "        api_key = True if len(api_key) else False\n\t    if not isinstance(fwd_key, str):\n\t        fwd_key = True if len(fwd_key) else False\n\t    table = Table(title=\"\", box=None, width=50)\n\t    matrcs = {\n\t        \"base url\": {\n\t            'value': base_url,\n\t        },\n\t        \"route prefix\": {\n\t            'value': route_prefix,\n", "        },\n\t        \"api keys\": {\n\t            'value': str(api_key),\n\t        },\n\t        \"forward keys\": {\n\t            'value': str(fwd_key),\n\t            'style': \"#62E883\" if fwd_key or not api_key else \"red\",\n\t        },\n\t        \"Log chat\": {\n\t            'value': str(log_chat),\n", "        },\n\t    }\n\t    table.add_column(\"\", justify='left', width=10)\n\t    table.add_column(\"\", justify='left')\n\t    for key, value in matrcs.items():\n\t        table.add_row(key, value['value'], style=value.get('style', style))\n\t    print(Panel(table, title=\"ğŸ¤— openai-forward is ready to serve! \", expand=False))\n\tdef print_rate_limit_info(rate_limit: dict, strategy: str, **kwargs):\n\t    \"\"\"\n\t    Print rate limit information.\n", "    Args:\n\t        rate_limit (dict): A dictionary containing route rate limit.\n\t        strategy (str): The strategy used for rate limiting.\n\t        **kwargs: Other limits info.\n\t    Returns:\n\t        None\n\t    \"\"\"\n\t    table = Table(title=\"\", box=None, width=50)\n\t    table.add_column(\"\")\n\t    table.add_column(\"\", justify='left')\n", "    table.add_row(\"strategy\", strategy, style='#7CD9FF')\n\t    for key, value in rate_limit.items():\n\t        table.add_row(key, str(value), style='#C5FF95')\n\t    for key, value in kwargs.items():\n\t        table.add_row(key, str(value), style='#C5FF95')\n\t    print(Panel(table, title=\"â±ï¸ Rate Limit configuration\", expand=False))\n\tclass InterceptHandler(logging.Handler):\n\t    def emit(self, record):\n\t        # Get corresponding Loguru level if it exists\n\t        try:\n", "            level = logger.level(record.levelname).name\n\t        except ValueError:\n\t            level = record.levelno\n\t        # Find caller from where originated the logged message\n\t        frame, depth = logging.currentframe(), 6\n\t        while frame.f_code.co_filename == logging.__file__:\n\t            frame = frame.f_back\n\t            depth += 1\n\t        logger.opt(depth=depth, exception=record.exc_info).log(\n\t            level, record.getMessage()\n", "        )\n\tdef setting_log(\n\t    save_file=False,\n\t    openai_route_prefix=None,\n\t    log_name=\"openai_forward\",\n\t    multi_process=True,\n\t):\n\t    \"\"\"\n\t    Configures the logging settings for the application.\n\t    \"\"\"\n", "    # TODO ä¿®å¤æ—¶åŒºé…ç½®\n\t    if os.environ.get(\"TZ\") == \"Asia/Shanghai\":\n\t        os.environ[\"TZ\"] = \"UTC-8\"\n\t        if hasattr(time, \"tzset\"):\n\t            time.tzset()\n\t    logging.root.handlers = [InterceptHandler()]\n\t    for name in logging.root.manager.loggerDict.keys():\n\t        logging.getLogger(name).handlers = []\n\t        logging.getLogger(name).propagate = True\n\t    config_handlers = [\n", "        {\"sink\": sys.stdout, \"level\": \"DEBUG\"},\n\t    ]\n\t    def filter_func(_prefix, _postfix, record):\n\t        chat_key = f\"{_prefix}{_postfix}\"\n\t        return chat_key in record[\"extra\"]\n\t    for prefix in openai_route_prefix or []:\n\t        _prefix = prefix.replace('/', '_')\n\t        config_handlers.extend(\n\t            [\n\t                {\n", "                    \"sink\": f\"./Log/chat/{_prefix}/chat.log\",\n\t                    \"enqueue\": multi_process,\n\t                    \"rotation\": \"50 MB\",\n\t                    \"filter\": functools.partial(filter_func, _prefix, \"_chat\"),\n\t                    \"format\": \"{message}\",\n\t                },\n\t                {\n\t                    \"sink\": f\"./Log/whisper/{_prefix}/whisper.log\",\n\t                    \"enqueue\": multi_process,\n\t                    \"rotation\": \"30 MB\",\n", "                    \"filter\": functools.partial(filter_func, _prefix, \"_whisper\"),\n\t                    \"format\": \"{message}\",\n\t                },\n\t            ]\n\t        )\n\t    if save_file:\n\t        config_handlers += [\n\t            {\n\t                \"sink\": f\"./Log/{log_name}.log\",\n\t                \"enqueue\": multi_process,\n", "                \"rotation\": \"100 MB\",\n\t                \"level\": \"INFO\",\n\t            }\n\t        ]\n\t    logger_config = {\"handlers\": config_handlers}\n\t    logger.configure(**logger_config)\n"]}
{"filename": "openai_forward/__init__.py", "chunked_list": ["__version__ = \"0.5.0\"\n\tfrom dotenv import load_dotenv\n\tload_dotenv(override=False)\n"]}
{"filename": "openai_forward/helper.py", "chunked_list": ["import ast\n\timport asyncio\n\timport inspect\n\timport os\n\timport time\n\tfrom functools import wraps\n\tfrom pathlib import Path\n\tfrom typing import Dict, List, Union\n\timport orjson\n\tfrom fastapi import Request\n", "from rich import print\n\tdef get_client_ip(request: Request):\n\t    if \"x-forwarded-for\" in request.headers:\n\t        return request.headers[\"x-forwarded-for\"]\n\t    elif not request.client or not request.client.host:\n\t        return \"127.0.0.1\"\n\t    return request.client.host\n\tdef relp(rel_path: Union[str, Path], parents=0, return_str=True, strict=False):\n\t    currentframe = inspect.currentframe()\n\t    f = currentframe.f_back\n", "    for _ in range(parents):\n\t        f = f.f_back\n\t    current_path = Path(f.f_code.co_filename).parent\n\t    pathlib_path = current_path / rel_path\n\t    pathlib_path = pathlib_path.resolve(strict=strict)\n\t    if return_str:\n\t        return str(pathlib_path)\n\t    else:\n\t        return pathlib_path\n\tdef ls(_dir, *patterns, concat='extend', recursive=False):\n", "    from glob import glob\n\t    path_list = []\n\t    for pattern in patterns:\n\t        if concat == 'extend':\n\t            path_list.extend(glob(os.path.join(_dir, pattern), recursive=recursive))\n\t        else:\n\t            path_list.append(glob(os.path.join(_dir, pattern), recursive=recursive))\n\t    return path_list\n\tdef retry(max_retries=3, delay=1, backoff=2, exceptions=(Exception,)):\n\t    \"\"\"\n", "    Retry decorator.\n\t    Parameters:\n\t    - max_retries: Maximum number of retries.\n\t    - delay: Initial delay in seconds.\n\t    - backoff: Multiplier for delay after each retry.\n\t    - exceptions: Exceptions to catch and retry on, as a tuple.\n\t    \"\"\"\n\t    def decorator(func):\n\t        @wraps(func)\n\t        def wrapper(*args, **kwargs):\n", "            retries = 0\n\t            current_delay = delay\n\t            while retries < max_retries:\n\t                try:\n\t                    return func(*args, **kwargs)\n\t                except exceptions as e:\n\t                    retries += 1\n\t                    if retries >= max_retries:\n\t                        raise\n\t                    time.sleep(current_delay)\n", "                    current_delay *= backoff\n\t        return wrapper\n\t    return decorator\n\tdef async_retry(max_retries=3, delay=1, backoff=2, exceptions=(Exception,)):\n\t    \"\"\"\n\t    Retry decorator for asynchronous functions.\n\t    Parameters:\n\t    - max_retries: Maximum number of retries.\n\t    - delay: Initial delay in seconds.\n\t    - backoff: Multiplier for delay after each retry.\n", "    - exceptions: Exceptions to catch and retry on, as a tuple.\n\t    \"\"\"\n\t    def decorator(func):\n\t        @wraps(func)\n\t        async def wrapper(*args, **kwargs):\n\t            retries = 0\n\t            current_delay = delay\n\t            while retries < max_retries:\n\t                try:\n\t                    return await func(*args, **kwargs)\n", "                except exceptions as e:\n\t                    retries += 1\n\t                    if retries >= max_retries:\n\t                        raise\n\t                    await asyncio.sleep(current_delay)\n\t                    current_delay *= backoff\n\t        return wrapper\n\t    return decorator\n\tdef json_load(filepath: str, rel=False, mode=\"rb\"):\n\t    abs_path = relp(filepath, parents=1) if rel else filepath\n", "    with open(abs_path, mode=mode) as f:\n\t        return orjson.loads(f.read())\n\tdef json_dump(\n\t    data: Union[List, Dict], filepath: str, rel=False, indent_2=False, mode=\"wb\"\n\t):\n\t    orjson_option = 0\n\t    if indent_2:\n\t        orjson_option = orjson.OPT_INDENT_2\n\t    abs_path = relp(filepath, parents=1) if rel else filepath\n\t    with open(abs_path, mode=mode) as f:\n", "        f.write(orjson.dumps(data, option=orjson_option))\n\tdef toml_load(filepath: str, rel=False):\n\t    import toml\n\t    abs_path = relp(filepath, parents=1) if rel else filepath\n\t    return toml.load(abs_path)\n\tdef str2list(s: str, sep):\n\t    if s:\n\t        return [i.strip() for i in s.split(sep) if i.strip()]\n\t    else:\n\t        return []\n", "def env2list(env_name: str, sep=\",\"):\n\t    return str2list(os.environ.get(env_name, \"\").strip(), sep=sep)\n\tdef env2dict(env_name: str) -> Dict:\n\t    import json\n\t    env_str = os.environ.get(env_name, \"\").strip()\n\t    if not env_str:\n\t        return {}\n\t    return json.loads(env_str)\n\tdef format_route_prefix(route_prefix: str):\n\t    if route_prefix:\n", "        if route_prefix.endswith(\"/\"):\n\t            route_prefix = route_prefix[:-1]\n\t        if not route_prefix.startswith(\"/\"):\n\t            route_prefix = \"/\" + route_prefix\n\t    return route_prefix\n\tdef get_matches(messages: List[Dict], assistants: List[Dict]):\n\t    msg_len, ass_len = len(messages), len(assistants)\n\t    if msg_len != ass_len:\n\t        print(f\"Length mismatch between message({msg_len}) and assistant({ass_len}) \")\n\t    cvt = lambda msg, ass: {\n", "        \"datetime\": msg.get('datetime'),\n\t        \"forwarded-for\": msg.get(\"forwarded-for\"),\n\t        \"model\": msg.get(\"model\"),\n\t        \"messages\": msg.get(\"messages\"),\n\t        \"assistant\": ass.get(\"assistant\"),\n\t    }\n\t    msg_uid_dict = {m.pop(\"uid\"): m for m in messages}\n\t    ass_uid_dict = {a.pop(\"uid\"): a for a in assistants}\n\t    matches = [\n\t        cvt(msg_uid_dict[uid], ass_uid_dict[uid])\n", "        for uid in msg_uid_dict\n\t        if uid in ass_uid_dict\n\t    ]\n\t    ref_len = max(msg_len, ass_len)\n\t    if len(matches) != ref_len:\n\t        print(f\"There are {ref_len - len(matches)} mismatched items\")\n\t    return matches\n\tdef parse_log_to_list(log_path: str):\n\t    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n\t        messages, assistant = [], []\n", "        for line in f.readlines():\n\t            content: dict = ast.literal_eval(line)\n\t            if content.get(\"messages\"):\n\t                messages.append(content)\n\t            else:\n\t                assistant.append(content)\n\t    return messages, assistant\n\tdef convert_chatlog_to_jsonl(log_path: str, target_path: str):\n\t    \"\"\"Convert single chatlog to jsonl\"\"\"\n\t    message_list, assistant_list = parse_log_to_list(log_path)\n", "    content_list = get_matches(messages=message_list, assistants=assistant_list)\n\t    json_dump(content_list, target_path, indent_2=True)\n\tdef get_log_files_from_folder(log_path: str):\n\t    return ls(log_path, \"*.log\")\n\tdef convert_folder_to_jsonl(folder_path: str, target_path: str):\n\t    \"\"\"Convert chatlog folder to jsonl\"\"\"\n\t    log_files = get_log_files_from_folder(folder_path)\n\t    messages = []\n\t    assistants = []\n\t    for log_path in log_files:\n", "        msg, ass = parse_log_to_list(log_path)\n\t        msg_len, ass_len = len(msg), len(ass)\n\t        if msg_len != ass_len:\n\t            print(\n\t                f\"{log_path}: Length mismatch between message({msg_len}) and assistant({ass_len}) \"\n\t            )\n\t        messages.extend(msg)\n\t        assistants.extend(ass)\n\t    content_list = get_matches(messages=messages, assistants=assistants)\n\t    json_dump(content_list, target_path, indent_2=True)\n", "    print(\"Converted successfully\")\n\t    print(f\"File saved to {target_path}\")\n"]}
{"filename": "openai_forward/forwarding/extra.py", "chunked_list": ["from .base import ForwardingBase\n\tclass AnyForwarding(ForwardingBase):\n\t    def __init__(self, base_url: str, route_prefix: str, proxy=None):\n\t        import httpx\n\t        self.BASE_URL = base_url\n\t        self.ROUTE_PREFIX = route_prefix\n\t        self.client = httpx.AsyncClient(\n\t            base_url=self.BASE_URL, proxies=proxy, http1=True, http2=False\n\t        )\n\tdef get_fwd_anything_objs():\n", "    \"\"\"\n\t    Generate extra forwarding objects.\n\t    Returns:\n\t        list: A list of AnyForwarding objects.\n\t    \"\"\"\n\t    from .settings import EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX, PROXY\n\t    extra_fwd_objs = []\n\t    for base_url, route_prefix in zip(EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX):\n\t        extra_fwd_objs.append(AnyForwarding(base_url, route_prefix, PROXY))\n\t    return extra_fwd_objs\n"]}
{"filename": "openai_forward/forwarding/base.py", "chunked_list": ["import asyncio\n\timport time\n\timport traceback\n\timport uuid\n\tfrom itertools import cycle\n\tfrom typing import Any, AsyncGenerator, List\n\timport anyio\n\timport httpx\n\tfrom fastapi import HTTPException, Request, status\n\tfrom fastapi.responses import StreamingResponse\n", "from loguru import logger\n\tfrom ..content import ChatSaver, WhisperSaver\n\tfrom ..helper import async_retry\n\tfrom .settings import *\n\tclass ForwardingBase:\n\t    BASE_URL = None\n\t    ROUTE_PREFIX = None\n\t    client: httpx.AsyncClient = None\n\t    if IP_BLACKLIST or IP_WHITELIST:\n\t        validate_host = True\n", "    else:\n\t        validate_host = False\n\t    timeout = TIMEOUT\n\t    @staticmethod\n\t    def validate_request_host(ip):\n\t        \"\"\"\n\t        Validates the request host IP address against the IP whitelist and blacklist.\n\t        Args:\n\t            ip (str): The IP address to be validated.\n\t        Raises:\n", "            HTTPException: If the IP address is not in the whitelist or if it is in the blacklist.\n\t        \"\"\"\n\t        if IP_WHITELIST and ip not in IP_WHITELIST:\n\t            raise HTTPException(\n\t                status_code=status.HTTP_403_FORBIDDEN,\n\t                detail=f\"Forbidden, ip={ip} not in whitelist!\",\n\t            )\n\t        if IP_BLACKLIST and ip in IP_BLACKLIST:\n\t            raise HTTPException(\n\t                status_code=status.HTTP_403_FORBIDDEN,\n", "                detail=f\"Forbidden, ip={ip} in blacklist!\",\n\t            )\n\t    @staticmethod\n\t    async def aiter_bytes(r: httpx.Response) -> AsyncGenerator[bytes, Any]:\n\t        async for chunk in r.aiter_bytes():\n\t            yield chunk\n\t        await r.aclose()\n\t    @async_retry(\n\t        max_retries=3,\n\t        delay=0.5,\n", "        backoff=2,\n\t        exceptions=(HTTPException, anyio.EndOfStream),\n\t    )\n\t    async def try_send(self, client_config: dict, request: Request):\n\t        \"\"\"\n\t        Try to send the request.\n\t        Args:\n\t            client_config (dict): The configuration for the client.\n\t            request (Request): The request to be sent.\n\t        Returns:\n", "            Response: The response from the client.\n\t        Raises:\n\t            HTTPException: If there is a connection error or any other exception occurs.\n\t        \"\"\"\n\t        try:\n\t            req = self.client.build_request(\n\t                method=request.method,\n\t                url=client_config['url'],\n\t                headers=client_config[\"headers\"],\n\t                content=request.stream(),\n", "                timeout=self.timeout,\n\t            )\n\t            return await self.client.send(req, stream=True)\n\t        except (httpx.ConnectError, httpx.ConnectTimeout) as e:\n\t            error_info = (\n\t                f\"{type(e)}: {e} | \"\n\t                f\"Please check if host={request.client.host} can access [{self.BASE_URL}] successfully?\"\n\t            )\n\t            traceback_info = traceback.format_exc()\n\t            logger.error(f\"{error_info} traceback={traceback_info}\")\n", "            raise HTTPException(\n\t                status_code=status.HTTP_504_GATEWAY_TIMEOUT, detail=error_info\n\t            )\n\t        except anyio.EndOfStream:\n\t            error_info = \"EndOfStream Error: trying to read from a stream that has been closed from the other end.\"\n\t            traceback_info = traceback.format_exc()\n\t            logger.error(f\"{error_info} traceback={traceback_info}\")\n\t            raise HTTPException(\n\t                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_info\n\t            )\n", "        except Exception as e:\n\t            error_info = f\"{type(e)}: {e}\"\n\t            traceback_info = traceback.format_exc()\n\t            logger.error(f\"{error_info} traceback={traceback_info}\")\n\t            raise HTTPException(\n\t                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_info\n\t            )\n\t    def prepare_client(self, request: Request):\n\t        \"\"\"\n\t        Prepares the client configuration based on the given request.\n", "        Args:\n\t            request (Request): The request object containing the necessary information.\n\t        Returns:\n\t            dict: The client configuration dictionary with the necessary parameters set.\n\t                  The dictionary has the following keys:\n\t                  - 'auth': The authorization header value.\n\t                  - 'headers': The dictionary of headers.\n\t                  - 'url': The URL object.\n\t                  - 'url_path': The URL path.\n\t        Raises:\n", "            AssertionError: If the `BASE_URL` or `ROUTE_PREFIX` is not set.\n\t        \"\"\"\n\t        assert self.BASE_URL is not None\n\t        assert self.ROUTE_PREFIX is not None\n\t        if self.validate_host:\n\t            ip = request.headers.get(\"x-forwarded-for\") or \"\"\n\t            self.validate_request_host(ip)\n\t        _url_path = request.url.path\n\t        prefix_index = 0 if self.ROUTE_PREFIX == '/' else len(self.ROUTE_PREFIX)\n\t        url_path = _url_path[prefix_index:]\n", "        url = httpx.URL(path=url_path, query=request.url.query.encode(\"utf-8\"))\n\t        headers = dict(request.headers)\n\t        auth = headers.pop(\"authorization\", \"\")\n\t        content_type = headers.pop(\"content-type\", \"application/json\")\n\t        auth_headers_dict = {\"Content-Type\": content_type, \"Authorization\": auth}\n\t        client_config = {\n\t            'auth': auth,\n\t            'headers': auth_headers_dict,\n\t            'url': url,\n\t            'url_path': url_path,\n", "        }\n\t        return client_config\n\t    async def reverse_proxy(self, request: Request):\n\t        \"\"\"\n\t        Reverse proxies the given request.\n\t        Args:\n\t            request (Request): The request to be reverse proxied.\n\t        Returns:\n\t            StreamingResponse: The response from the reverse proxied server, as a streaming response.\n\t        \"\"\"\n", "        assert self.client is not None\n\t        client_config = self.prepare_client(request)\n\t        r = await self.try_send(client_config, request)\n\t        return StreamingResponse(\n\t            self.aiter_bytes(r),\n\t            status_code=r.status_code,\n\t            media_type=r.headers.get(\"content-type\"),\n\t        )\n\tclass OpenaiBase(ForwardingBase):\n\t    _cycle_api_key = cycle(OPENAI_API_KEY)\n", "    _no_auth_mode = OPENAI_API_KEY != [] and FWD_KEY == set()\n\t    chatsaver: ChatSaver = None\n\t    whispersaver: WhisperSaver = None\n\t    def _add_result_log(\n\t        self, byte_list: List[bytes], uid: str, route_path: str, request_method: str\n\t    ):\n\t        \"\"\"\n\t        Adds a result log for the given byte list, uid, route path, and request method.\n\t        Args:\n\t            byte_list (List[bytes]): The list of bytes to be processed.\n", "            uid (str): The unique identifier.\n\t            route_path (str): The route path.\n\t            request_method (str): The request method.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        try:\n\t            if LOG_CHAT and request_method == \"POST\":\n\t                if route_path == \"/v1/chat/completions\":\n\t                    target_info = self.chatsaver.parse_iter_bytes(byte_list)\n", "                    self.chatsaver.log_chat(\n\t                        {target_info[\"role\"]: target_info[\"content\"], \"uid\": uid}\n\t                    )\n\t                elif route_path.startswith(\"/v1/audio/\"):\n\t                    self.whispersaver.add_log(b\"\".join([_ for _ in byte_list]))\n\t                else:\n\t                    ...\n\t        except Exception as e:\n\t            logger.warning(f\"log chat (not) error:\\n{traceback.format_exc()}\")\n\t    async def _add_payload_log(self, request: Request, url_path: str):\n", "        \"\"\"\n\t        Adds a payload log for the given request.\n\t        Args:\n\t            request (Request): The request object.\n\t            url_path (str): The URL path of the request.\n\t        Returns:\n\t            str: The unique identifier (UID) of the payload log, which is used to match the chat result log.\n\t        Raises:\n\t            Suppress all errors.\n\t        Notes:\n", "            - If `LOG_CHAT` is True and the request method is \"POST\", the chat payload will be logged.\n\t            - If the `url_path` is \"/v1/chat/completions\", the chat payload will be parsed and logged.\n\t            - If the `url_path` starts with \"/v1/audio/\", a new UID will be generated.\n\t        \"\"\"\n\t        uid = None\n\t        if LOG_CHAT and request.method == \"POST\":\n\t            try:\n\t                if url_path == \"/v1/chat/completions\":\n\t                    chat_info = await self.chatsaver.parse_payload(request)\n\t                    uid = chat_info.get(\"uid\")\n", "                    if chat_info:\n\t                        self.chatsaver.log_chat(chat_info)\n\t                elif url_path.startswith(\"/v1/audio/\"):\n\t                    uid = uuid.uuid4().__str__()\n\t                else:\n\t                    ...\n\t            except Exception as e:\n\t                logger.warning(\n\t                    f\"log chat error:\\nhost:{request.client.host} method:{request.method}: {traceback.format_exc()}\"\n\t                )\n", "        return uid\n\t    async def openai_aiter_bytes(\n\t        self, r: httpx.Response, request: Request, route_path: str, uid: str\n\t    ):\n\t        \"\"\"\n\t        Asynchronously iterates over the bytes of the response and yields each chunk.\n\t        Args:\n\t            r (httpx.Response): The HTTP response object.\n\t            request (Request): The original request object.\n\t            route_path (str): The route path.\n", "            uid (str): The unique identifier.\n\t        Returns:\n\t            A generator that yields each chunk of bytes from the response.\n\t        \"\"\"\n\t        byte_list = []\n\t        start_time = time.perf_counter()\n\t        idx = 0\n\t        async for chunk in r.aiter_bytes():\n\t            idx += 1\n\t            byte_list.append(chunk)\n", "            if TOKEN_INTERVAL > 0:\n\t                current_time = time.perf_counter()\n\t                delta = current_time - start_time\n\t                delay = TOKEN_INTERVAL - delta\n\t                if delay > 0:\n\t                    await asyncio.sleep(delay)\n\t                start_time = time.perf_counter()\n\t            yield chunk\n\t        await r.aclose()\n\t        if uid:\n", "            if r.is_success:\n\t                self._add_result_log(byte_list, uid, route_path, request.method)\n\t            else:\n\t                response_info = b\"\".join([_ for _ in byte_list])\n\t                logger.warning(f'uid: {uid}\\n' f'{response_info}')\n\t    async def reverse_proxy(self, request: Request):\n\t        \"\"\"\n\t        Reverse proxies the given requests.\n\t        Args:\n\t            request (Request): The incoming request object.\n", "        Returns:\n\t            StreamingResponse: The response from the reverse proxied server, as a streaming response.\n\t        \"\"\"\n\t        client_config = self.prepare_client(request)\n\t        url_path = client_config[\"url_path\"]\n\t        def set_apikey_from_preset():\n\t            nonlocal client_config\n\t            auth_prefix = \"Bearer \"\n\t            auth = client_config[\"auth\"]\n\t            if self._no_auth_mode or auth and auth[len(auth_prefix) :] in FWD_KEY:\n", "                auth = auth_prefix + next(self._cycle_api_key)\n\t                client_config[\"headers\"][\"Authorization\"] = auth\n\t        set_apikey_from_preset()\n\t        uid = await self._add_payload_log(request, url_path)\n\t        r = await self.try_send(client_config, request)\n\t        return StreamingResponse(\n\t            self.openai_aiter_bytes(r, request, url_path, uid),\n\t            status_code=r.status_code,\n\t            media_type=r.headers.get(\"content-type\"),\n\t        )\n"]}
{"filename": "openai_forward/forwarding/settings.py", "chunked_list": ["import itertools\n\timport os\n\timport limits\n\tfrom fastapi import Request\n\tfrom ..config import print_rate_limit_info, print_startup_info, setting_log\n\tfrom ..helper import env2dict, env2list, format_route_prefix\n\tTIMEOUT = 600\n\tENV_VAR_SEP = \",\"\n\tOPENAI_BASE_URL = env2list(\"OPENAI_BASE_URL\", sep=ENV_VAR_SEP) or [\n\t    \"https://api.openai.com\"\n", "]\n\tOPENAI_ROUTE_PREFIX = [\n\t    format_route_prefix(i) for i in env2list(\"OPENAI_ROUTE_PREFIX\", sep=ENV_VAR_SEP)\n\t] or ['/']\n\tEXTRA_BASE_URL = env2list(\"EXTRA_BASE_URL\", sep=ENV_VAR_SEP)\n\tEXTRA_ROUTE_PREFIX = [\n\t    format_route_prefix(i) for i in env2list(\"EXTRA_ROUTE_PREFIX\", sep=ENV_VAR_SEP)\n\t]\n\tLOG_CHAT = os.environ.get(\"LOG_CHAT\", \"False\").strip().lower() == \"true\"\n\tif LOG_CHAT:\n", "    setting_log(openai_route_prefix=OPENAI_ROUTE_PREFIX)\n\tIP_WHITELIST = env2list(\"IP_WHITELIST\", sep=ENV_VAR_SEP)\n\tIP_BLACKLIST = env2list(\"IP_BLACKLIST\", sep=ENV_VAR_SEP)\n\tOPENAI_API_KEY = env2list(\"OPENAI_API_KEY\", sep=ENV_VAR_SEP)\n\tFWD_KEY = env2list(\"FORWARD_KEY\", sep=ENV_VAR_SEP)\n\tPROXY = os.environ.get(\"PROXY\", \"\").strip()\n\tPROXY = PROXY if PROXY else None\n\tGLOBAL_RATE_LIMIT = os.environ.get(\"GLOBAL_RATE_LIMIT\", \"fixed-window\").strip() or None\n\tRATE_LIMIT_STRATEGY = os.environ.get(\"RATE_LIMIT_STRATEGY\", \"\").strip() or None\n\troute_rate_limit_conf = env2dict('ROUTE_RATE_LIMIT')\n", "def get_limiter_key(request: Request):\n\t    limiter_prefix = f\"{request.scope.get('root_path')}{request.scope.get('path')}\"\n\t    key = f\"{limiter_prefix}\"  # -{get_client_ip(request)}\"\n\t    return key\n\tdef dynamic_rate_limit(key: str):\n\t    for route in route_rate_limit_conf:\n\t        if key.startswith(route):\n\t            return route_rate_limit_conf[route]\n\t    return GLOBAL_RATE_LIMIT\n\tTOKEN_RATE_LIMIT = os.environ.get(\"TOKEN_RATE_LIMIT\", \"\").strip()\n", "if TOKEN_RATE_LIMIT:\n\t    rate_limit_item = limits.parse(TOKEN_RATE_LIMIT)\n\t    TOKEN_INTERVAL = (\n\t        rate_limit_item.multiples * rate_limit_item.GRANULARITY.seconds\n\t    ) / rate_limit_item.amount\n\telse:\n\t    TOKEN_INTERVAL = 0\n\tstyles = itertools.cycle(\n\t    [\"#7CD9FF\", \"#BDADFF\", \"#9EFFE3\", \"#f1b8e4\", \"#F5A88E\", \"#BBCA89\"]\n\t)\n", "for base_url, route_prefix in zip(OPENAI_BASE_URL, OPENAI_ROUTE_PREFIX):\n\t    print_startup_info(\n\t        base_url, route_prefix, OPENAI_API_KEY, FWD_KEY, LOG_CHAT, style=next(styles)\n\t    )\n\tfor base_url, route_prefix in zip(EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX):\n\t    print_startup_info(base_url, route_prefix, \"\\\\\", \"\\\\\", LOG_CHAT, style=next(styles))\n\tprint_rate_limit_info(\n\t    route_rate_limit_conf,\n\t    strategy=RATE_LIMIT_STRATEGY,\n\t    global_rate_limit=GLOBAL_RATE_LIMIT if GLOBAL_RATE_LIMIT else 'inf',\n", "    token_rate_limit=TOKEN_RATE_LIMIT if TOKEN_RATE_LIMIT else 'inf',\n\t    token_interval_time=f\"{TOKEN_INTERVAL:.4f}s\",\n\t)\n"]}
{"filename": "openai_forward/forwarding/openai.py", "chunked_list": ["import time\n\tfrom .base import ChatSaver, OpenaiBase, WhisperSaver\n\tfrom .settings import LOG_CHAT\n\tclass OpenaiForwarding(OpenaiBase):\n\t    def __init__(self, base_url: str, route_prefix: str, proxy=None):\n\t        import httpx\n\t        self.BASE_URL = base_url\n\t        self.ROUTE_PREFIX = route_prefix\n\t        if LOG_CHAT:\n\t            self.chatsaver = ChatSaver(route_prefix)\n", "            self.whispersaver = WhisperSaver(route_prefix)\n\t        self.client = httpx.AsyncClient(\n\t            base_url=self.BASE_URL, proxies=proxy, http1=True, http2=False\n\t        )\n\t        self.token_counts = 0\n\t        self.token_limit_dict = {'time': time.time(), 'count': 0}\n\tdef get_fwd_openai_style_objs():\n\t    \"\"\"\n\t    Generate OPENAI route style forwarding objects.\n\t    Returns:\n", "        fwd_objs (list): A list of OpenaiForwarding objects.\n\t    \"\"\"\n\t    from .settings import OPENAI_BASE_URL, OPENAI_ROUTE_PREFIX, PROXY\n\t    fwd_objs = []\n\t    for base_url, route_prefix in zip(OPENAI_BASE_URL, OPENAI_ROUTE_PREFIX):\n\t        fwd_objs.append(OpenaiForwarding(base_url, route_prefix, PROXY))\n\t    return fwd_objs\n"]}
{"filename": "openai_forward/forwarding/__init__.py", "chunked_list": ["from .extra import AnyForwarding, get_fwd_anything_objs\n\tfrom .openai import OpenaiForwarding, get_fwd_openai_style_objs\n"]}
{"filename": "openai_forward/content/decode.py", "chunked_list": ["from httpx._decoders import LineDecoder, TextChunker, TextDecoder\n\tdef iter_text(iter_tytes: list):\n\t    decoder = TextDecoder(\"utf-8\")\n\t    chunker = TextChunker()\n\t    for byte_content in iter_tytes:\n\t        text_content = decoder.decode(byte_content)\n\t        for chunk in chunker.decode(text_content):\n\t            yield chunk\n\t    text_content = decoder.flush()\n\t    for chunk in chunker.decode(text_content):\n", "        yield chunk\n\t    for chunk in chunker.flush():\n\t        yield chunk\n\tdef parse_to_lines(iter_bytes: list) -> list:\n\t    decoder = LineDecoder()\n\t    lines = []\n\t    for text in iter_text(iter_bytes):\n\t        lines.extend(decoder.decode(text))\n\t    lines.extend(decoder.flush())\n\t    return lines\n"]}
{"filename": "openai_forward/content/whisper.py", "chunked_list": ["from loguru import logger\n\tclass WhisperSaver:\n\t    def __init__(self, route_prefix: str):\n\t        _prefix = route_prefix.replace('/', '_')\n\t        self.logger = logger.bind(**{f\"{_prefix}_whisper\": True})\n\t    def add_log(self, bytes_: bytes):\n\t        text_content = bytes_.decode(\"utf-8\")\n\t        self.logger.debug(text_content)\n"]}
{"filename": "openai_forward/content/__init__.py", "chunked_list": ["from .chat import ChatSaver\n\tfrom .whisper import WhisperSaver\n"]}
{"filename": "openai_forward/content/chat.py", "chunked_list": ["import time\n\timport uuid\n\tfrom typing import List\n\timport orjson\n\tfrom fastapi import Request\n\tfrom loguru import logger\n\tfrom orjson import JSONDecodeError\n\tfrom ..helper import get_client_ip\n\tfrom .decode import parse_to_lines\n\tclass ChatSaver:\n", "    def __init__(self, route_prefix: str):\n\t        _prefix = route_prefix.replace('/', '_')\n\t        kwargs = {_prefix + \"_chat\": True}\n\t        self.logger = logger.bind(**kwargs)\n\t    @staticmethod\n\t    async def parse_payload(request: Request):\n\t        uid = uuid.uuid4().__str__()\n\t        payload = await request.json()\n\t        msgs = payload[\"messages\"]\n\t        model = payload[\"model\"]\n", "        content = {\n\t            \"messages\": [{msg[\"role\"]: msg[\"content\"]} for msg in msgs],\n\t            \"model\": model,\n\t            \"forwarded-for\": get_client_ip(request) or \"\",\n\t            \"uid\": uid,\n\t            \"datetime\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n\t        }\n\t        return content\n\t    def parse_iter_bytes(self, byte_list: List[bytes]):\n\t        \"\"\"\n", "        Parses a list of bytes and returns a dictionary.\n\t        Args:\n\t            byte_list (List[bytes]): A list of bytes to parse.\n\t        Returns:\n\t            Dict[str, Any]: A dictionary containing information about the target. The dictionary has the following keys:\n\t                - \"created\" (str)\n\t                - \"id\" (str)\n\t                - \"model\" (str)\n\t                - \"role\" (str)\n\t                - \"content\" (str)\n", "        \"\"\"\n\t        txt_lines = parse_to_lines(byte_list)\n\t        start_line = txt_lines[0]\n\t        target_info = dict()\n\t        start_token = \"data: \"\n\t        start_token_len = len(start_token)\n\t        if start_line.startswith(start_token):\n\t            stream = True\n\t            start_line = orjson.loads(start_line[start_token_len:])\n\t            msg = start_line[\"choices\"][0][\"delta\"]\n", "        else:\n\t            stream = False\n\t            start_line = orjson.loads(\"\".join(txt_lines))\n\t            msg = start_line[\"choices\"][0][\"message\"]\n\t        target_info[\"created\"] = start_line[\"created\"]\n\t        target_info[\"id\"] = start_line[\"id\"]\n\t        target_info[\"model\"] = start_line[\"model\"]\n\t        target_info[\"role\"] = msg[\"role\"]\n\t        target_info[\"content\"] = msg.get(\"content\", \"\")\n\t        if not stream:\n", "            return target_info\n\t        # loop for stream\n\t        for line in txt_lines[1:]:\n\t            if line.startswith(start_token):\n\t                target_info[\"content\"] += self._parse_one_line(line[start_token_len:])\n\t        return target_info\n\t    @staticmethod\n\t    def _parse_one_line(line: str):\n\t        try:\n\t            line_dict = orjson.loads(line)\n", "            return line_dict[\"choices\"][0][\"delta\"][\"content\"]\n\t        except JSONDecodeError:\n\t            return \"\"\n\t        except KeyError:\n\t            return \"\"\n\t    def log_chat(self, chat_info: dict):\n\t        self.logger.debug(f\"{chat_info}\")\n"]}
{"filename": "Examples/embedding.py", "chunked_list": ["import openai\n\tfrom sparrow import yaml_load\n\tconfig = yaml_load(\"config.yaml\")\n\topenai.api_base = config[\"api_base\"]\n\topenai.api_key = config[\"api_key\"]\n\tresponse = openai.Embedding.create(\n\t    input=\"Your text string goes here\", model=\"text-embedding-ada-002\"\n\t)\n\tembeddings = response['data'][0]['embedding']\n\tprint(embeddings)\n"]}
{"filename": "Examples/whisper.py", "chunked_list": ["# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n\timport openai\n\tfrom sparrow import relp, yaml_load\n\tconfig = yaml_load(\"config.yaml\")\n\topenai.api_base = config[\"api_base\"]\n\topenai.api_key = config[\"api_key\"]\n\taudio_file = open(relp(\"../.github/data/whisper.m4a\"), \"rb\")\n\ttranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n\tprint(transcript)\n"]}
{"filename": "Examples/chat.py", "chunked_list": ["import time\n\timport openai\n\tfrom rich import print\n\tfrom sparrow import yaml_load\n\tconfig = yaml_load(\"config.yaml\", rel_path=True)\n\tprint(f\"{config=}\")\n\topenai.api_base = config[\"api_base\"]\n\topenai.api_key = config[\"api_key\"]\n\tstream = True\n\tuser_content = \"\"\"\n", "ç”¨cå®ç°ç›®å‰å·²çŸ¥æœ€å¿«å¹³æ–¹æ ¹ç®—æ³•\n\t\"\"\"\n\tresp = openai.ChatCompletion.create(\n\t    model=\"gpt-3.5-turbo\",\n\t    # model=\"gpt-4\",\n\t    messages=[\n\t        {\"role\": \"user\", \"content\": user_content},\n\t    ],\n\t    stream=stream,\n\t)\n", "if stream:\n\t    chunk_message = next(resp)['choices'][0]['delta']\n\t    print(f\"{chunk_message['role']}: \")\n\t    for chunk in resp:\n\t        chunk_message = chunk['choices'][0]['delta']\n\t        content = chunk_message.get(\"content\", \"\")\n\t        print(content, end=\"\")\n\t    print()\n\telse:\n\t    print(resp.choices)\n", "\"\"\"\n\tgpt-4:\n\tä»¥ä¸‹æ˜¯ç”¨Cè¯­è¨€å®ç°çš„æœ€å¿«å·²çŸ¥çš„ä¸€ç§å¹³æ–¹æ ¹ç®—æ³•ï¼Œä¹Ÿå«åš \"Fast Inverse Square Root\"ã€‚è¿™ç§ç®—æ³•é¦–æ¬¡å‡ºç°åœ¨é›·ç¥ä¹‹é”¤3çš„æºä»£ç ä¸­ï¼Œè¢«å¤§é‡çš„ç°ä»£3Då›¾å½¢è®¡ç®—æ‰€ä½¿ç”¨ã€‚\n\t```c\n\t#include <stdint.h>\n\tfloat Q_rsqrt(float number){\n\t    long i;\n\t    float x2, y;\n\t    const float threehalfs = 1.5F;\n\t    x2 = number * 0.5F;\n", "    y  = number;\n\t    i  = * ( long * ) &y;\n\t    i  = 0x5f3759df - ( i >> 1 );\n\t    y  = * ( float * ) &i;\n\t    y  = y * ( threehalfs - ( x2 * y * y ) );\n\t    return y;\n\t}\n\t```\n\tè¿™ç§ç®—æ³•çš„ç²¾ç¡®åº¦å¹¶ä¸æ˜¯å¾ˆé«˜ï¼Œä½†å®ƒçš„é€Ÿåº¦å¿«åˆ°è¶³ä»¥åšå®æ—¶å›¾å½¢è®¡ç®—ã€‚ä¸Šè¿°ä»£ç çš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å¯¹IEEE 754æµ®ç‚¹æ•°è¡¨ç¤ºæ³•çš„ç†è§£å’Œåˆ©ç”¨ï¼Œå€ŸåŠ©æ•´æ•°å’Œæµ®ç‚¹æ•°çš„äºŒè¿›åˆ¶è¡¨ç¤ºåœ¨ç•¥æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œå®ç°äº†å¿«é€Ÿé€¼è¿‘æ±‚è§£å¹³æ–¹æ ¹å€’æ•°çš„æ–¹æ³•ã€‚\n\tæ³¨æ„ï¼Œè¿™ä¸ªå‡½æ•°å®é™…ä¸Šæ±‚è§£çš„æ˜¯å¹³æ–¹æ ¹çš„å€’æ•°ï¼Œä¹Ÿå°±æ˜¯1/sqrt(x)ï¼Œå¦‚æœéœ€è¦å¾—åˆ°sqrt(x)çš„ç»“æœï¼Œåªéœ€è¦å°†å‡½æ•°è¿”å›å€¼å–å€’æ•°å³å¯ã€‚è¿™æ˜¯å› ä¸ºåœ¨3Då›¾å½¢è®¡ç®—ä¸­ï¼Œå¾€å¾€æ›´é¢‘ç¹åœ°éœ€è¦æ±‚è§£å¹³æ–¹æ ¹å€’æ•°ï¼Œè€Œç›´æ¥æ±‚è§£å¹³æ–¹æ ¹åè€Œè¾ƒä¸ºç½•è§ã€‚\n", "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tgpt-3.5-turbo:\n\tç›®å‰å·²ç»å‘ç°çš„æœ€å¿«å¹³æ–¹æ ¹ç®—æ³•æ˜¯ç‰›é¡¿è¿­ä»£æ³•ï¼Œå¯ä»¥ç”¨Cè¯­è¨€å®ç°å¦‚ä¸‹ï¼š\n\t```c\n\t#include <stdio.h>\n\tdouble sqrt_newton(double x) {\n\t    if (x == 0) {\n\t        return 0;\n\t    }\n\t    double guess = x / 2;  // åˆå§‹çŒœæµ‹å€¼ä¸ºxçš„ä¸€åŠ\n", "    while (1) {\n\t        double new_guess = (guess + x / guess) / 2;  // æ ¹æ®ç‰›é¡¿è¿­ä»£æ³•è®¡ç®—æ–°çš„çŒœæµ‹å€¼\n\t        if (new_guess == guess) {  // å¦‚æœæ–°çš„çŒœæµ‹å€¼ä¸ä¸Šä¸€æ¬¡çš„çŒœæµ‹å€¼ç›¸åŒï¼Œè¿­ä»£ç»“æŸ\n\t            break;\n\t        }\n\t        guess = new_guess;\n\t    }\n\t    return guess;\n\t}\n\tint main() {\n", "    double x = 16;  // ä»¥16ä¸ºä¾‹è¿›è¡Œæµ‹è¯•\n\t    double result = sqrt_newton(x);\n\t    printf(\"The square root of %lf is %lf\\n\", x, result);\n\t    return 0;\n\t}\n\t```\n\tè¯¥ç¨‹åºä½¿ç”¨ç‰›é¡¿è¿­ä»£æ³•æ¥è®¡ç®—å¹³æ–¹æ ¹ï¼Œåˆå§‹çŒœæµ‹å€¼ä¸ºå¾…å¼€æ–¹æ•°çš„ä¸€åŠã€‚ç„¶åé€šè¿‡è¿­ä»£è®¡ç®—æ–°çš„çŒœæµ‹å€¼ï¼Œç›´åˆ°æ–°çš„çŒœæµ‹å€¼ä¸ä¸Šä¸€æ¬¡çš„çŒœæµ‹å€¼ç›¸åŒï¼Œè¿­ä»£ç»“æŸã€‚æœ€åè¾“å‡ºè®¡ç®—å¾—åˆ°çš„å¹³æ–¹æ ¹ç»“æœã€‚\n\t\"\"\"\n"]}
{"filename": "scripts/keep_render_alive.py", "chunked_list": ["import time\n\tfrom urllib.parse import urljoin\n\timport httpx\n\timport schedule\n\tdef job(url: str = \"https://render.openai-forward.com\"):\n\t    health_url = urljoin(url, \"/healthz\")\n\t    try:\n\t        r = httpx.get(health_url, timeout=5)\n\t        result = r.json()\n\t        print(result)\n", "        assert result == \"OK\"\n\t    except Exception as e:\n\t        print(e)\n\tif __name__ == \"__main__\":\n\t    job()\n\t    schedule.every(10).minutes.do(job)\n\t    while True:\n\t        schedule.run_pending()\n\t        time.sleep(60)\n"]}
{"filename": "tests/test_http.py", "chunked_list": ["import subprocess\n\timport time\n\timport httpx\n\tfrom sparrow.multiprocess import kill\n\tfrom utils import rm\n\tclass TestRun:\n\t    @classmethod\n\t    def setup_class(cls):\n\t        kill(8000)\n\t        base_url = \"https://api.openai.com\"\n", "        subprocess.Popen([\"nohup\", \"openai-forward\", \"run\", \"--base_url\", base_url])\n\t        time.sleep(3)\n\t    @classmethod\n\t    def teardown_class(cls):\n\t        kill(8000)\n\t        rm(\"nohup.out\")\n\t    def test_get_doc(self):\n\t        resp = httpx.get(\"http://localhost:8000/healthz\")\n\t        assert resp.is_success\n\t    def test_get_chat_completions(self):\n", "        resp = httpx.get(\"http://localhost:8000/v1/chat/completions\")\n\t        assert resp.status_code == 401\n"]}
{"filename": "tests/utils.py", "chunked_list": ["import os\n\timport shutil\n\tfrom sparrow import ls\n\tdef rm(*file_pattern: str, rel=False):\n\t    \"\"\"Remove files or directories.\n\t    Example:\n\t    --------\n\t        >>> rm(\"*.jpg\", \"*.png\")\n\t        >>> rm(\"*.jpg\", \"*.png\", rel=True)\n\t    \"\"\"\n", "    path_list = ls(\".\", *file_pattern, relp=rel, concat=\"extend\")\n\t    for file in path_list:\n\t        if os.path.isfile(file):\n\t            print(\"remove \", file)\n\t            os.remove(file)\n\t            # os.system(\"rm -f \" + file)\n\t        elif os.path.isdir(file):\n\t            shutil.rmtree(file, ignore_errors=True)\n\t            print(\"rm tree \", file)\n"]}
{"filename": "tests/test_api.py", "chunked_list": ["from itertools import cycle\n\timport pytest\n\tfrom fastapi import HTTPException\n\tfrom openai_forward.forwarding.openai import OpenaiForwarding\n\t@pytest.fixture(scope=\"module\")\n\tdef openai() -> OpenaiForwarding:\n\t    return OpenaiForwarding(\"https://api.openai-forward.com\", \"/\")\n\tclass TestOpenai:\n\t    @staticmethod\n\t    def teardown_method():\n", "        OpenaiForwarding.IP_BLACKLIST = []\n\t        OpenaiForwarding.IP_WHITELIST = []\n\t        OpenaiForwarding._default_api_key_list = []\n\t    def test_env(self, openai: OpenaiForwarding):\n\t        from openai_forward.forwarding.settings import (\n\t            LOG_CHAT,\n\t            OPENAI_BASE_URL,\n\t            OPENAI_ROUTE_PREFIX,\n\t        )\n\t        assert LOG_CHAT is False\n", "        assert OPENAI_BASE_URL == [\"https://api.openai.com\"]\n\t        assert OPENAI_ROUTE_PREFIX == [\"/\"]\n\t    def test_api_keys(self, openai: OpenaiForwarding):\n\t        assert openai._default_api_key_list == []\n\t        openai._default_api_key_list = [\"a\", \"b\"]\n\t        openai._cycle_api_key = cycle(openai._default_api_key_list)\n\t        assert next(openai._cycle_api_key) == \"a\"\n\t        assert next(openai._cycle_api_key) == \"b\"\n\t        assert next(openai._cycle_api_key) == \"a\"\n\t        assert next(openai._cycle_api_key) == \"b\"\n", "        assert next(openai._cycle_api_key) == \"a\"\n\t    def test_validate_ip(self, openai: OpenaiForwarding):\n\t        from openai_forward.forwarding.settings import IP_BLACKLIST, IP_WHITELIST\n\t        ip1 = \"1.1.1.1\"\n\t        ip2 = \"2.2.2.2\"\n\t        IP_WHITELIST.append(ip1)\n\t        with pytest.raises(HTTPException):\n\t            openai.validate_request_host(ip2)\n\t        IP_WHITELIST.clear()\n\t        IP_BLACKLIST.append(ip1)\n", "        with pytest.raises(HTTPException):\n\t            openai.validate_request_host(ip1)\n"]}
{"filename": "tests/test_env.py", "chunked_list": ["import importlib\n\timport os\n\timport time\n\timport pytest\n\tfrom dotenv import load_dotenv\n\timport openai_forward\n\tclass TestEnv:\n\t    with open(\".env\", \"r\", encoding=\"utf-8\") as f:\n\t        defualt_env = f.read()\n\t    @classmethod\n", "    def setup_class(cls):\n\t        env = \"\"\"\\\n\tLOG_CHAT=true\n\tOPENAI_BASE_URL=https://api.openai.com\n\tOPENAI_API_KEY=key1,key2\n\tOPENAI_ROUTE_PREFIX=\n\tFORWARD_KEY=ps1,ps2,ps3\n\tIP_WHITELIST=\n\tIP_BLACKLIST=\n\t\"\"\"\n", "        with open(\".env\", \"w\", encoding=\"utf-8\") as f:\n\t            f.write(env)\n\t            time.sleep(0.1)\n\t        load_dotenv(override=True)\n\t        importlib.reload(openai_forward.forwarding.openai)\n\t        importlib.reload(openai_forward.forwarding.settings)\n\t        cls.aibase = openai_forward.forwarding.openai.OpenaiForwarding(\n\t            'https://api.openai.com', '/'\n\t        )\n\t    @classmethod\n", "    def teardown_class(cls):\n\t        with open(\".env\", \"w\", encoding=\"utf-8\") as f:\n\t            f.write(cls.defualt_env)\n\t    def test_env1(self):\n\t        from openai_forward.forwarding.settings import FWD_KEY, OPENAI_API_KEY\n\t        assert OPENAI_API_KEY == [\"key1\", \"key2\"]\n\t        assert FWD_KEY == [\"ps1\", \"ps2\", \"ps3\"]\n\t        assert self.aibase._no_auth_mode is False\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n"]}
