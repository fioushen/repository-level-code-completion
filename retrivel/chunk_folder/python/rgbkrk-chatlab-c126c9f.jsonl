{"filename": "tests/test_chatlab.py", "chunked_list": ["# flake8: noqa\n\timport toml\n\t# Casually import everything to shake out any import errors\n\tfrom chatlab import *\n\tdef test_chatlab():\n\t    from chatlab import __version__\n\t    # Check that __version__ is the same as pyproject.toml's version\n\t    pyproject_toml = toml.load(\"pyproject.toml\")\n\t    assert __version__ == pyproject_toml[\"tool\"][\"poetry\"][\"version\"]\n"]}
{"filename": "tests/test_messaging.py", "chunked_list": ["#!/usr/bin/env python\n\t\"\"\"Tests for `chatlab` package.\"\"\"\n\tfrom chatlab import assistant, system, user\n\tfrom chatlab.messaging import assistant_function_call, function_result\n\tdef test_assistant():\n\t    message = assistant(\"Hello!\")\n\t    assert message['role'] == 'assistant'\n\t    assert message['content'] == 'Hello!'\n\tdef test_user():\n\t    message = user(\"How are you?\")\n", "    assert message['role'] == 'user'\n\t    assert message['content'] == 'How are you?'\n\tdef test_system():\n\t    message = system(\"System message\")\n\t    assert message['role'] == 'system'\n\t    assert message['content'] == 'System message'\n\tdef test_assistant_function_call():\n\t    message = assistant_function_call(\"func_name\", \"arg\")\n\t    assert message['role'] == 'assistant'\n\t    assert message['function_call']['name'] == 'func_name'\n", "    assert message['function_call']['arguments'] == 'arg'\n\tdef test_function_result():\n\t    message = function_result(\"func_name\", \"result\")\n\t    assert message['role'] == 'function'\n\t    assert message['name'] == 'func_name'\n\t    assert message['content'] == 'result'\n"]}
{"filename": "tests/test_builtins.py", "chunked_list": ["# flake8: noqa\n\timport pytest\n\tfrom chatlab import FunctionRegistry\n\tfrom chatlab.builtins import os_functions\n\tfrom chatlab.builtins.files import get_file_size, is_directory, is_file, list_files, read_file, write_file\n\tfrom chatlab.builtins.shell import run_shell_command\n\tdef test_chat_function_adherence():\n\t    assert len(os_functions) > 0\n\t    for function in os_functions:\n\t        assert function.__name__ is not None and function.__name__ != \"\"\n", "        assert function.__doc__ is not None and function.__doc__ != \"\"\n\t        fr = FunctionRegistry()\n\t        schema = fr.register(function)\n\t        assert schema is not None\n\t# TODO: Determine if this is part of the testing suite on Windows\n\tasync def test_run_shell_command():\n\t    command = \"echo Hello, ChatLab!\"\n\t    result = await run_shell_command(command)\n\t    assert \"Hello, ChatLab!\" in result\n\t    command = \"adsflkajsdg\"\n", "    result = await run_shell_command(command)\n\t    assert \"adsflkajsdg: command not found\" in result\n\t@pytest.mark.asyncio\n\tasync def test_list_files():\n\t    directory = \"chatlab/builtins\"\n\t    files = await list_files(directory)\n\t    assert isinstance(files, list)\n\t    assert len(files) > 0\n\t@pytest.mark.asyncio\n\tasync def test_get_file_size():\n", "    file_path = \"chatlab/builtins/files.py\"\n\t    size = await get_file_size(file_path)\n\t    assert isinstance(size, int)\n\t    assert size > 0\n\t@pytest.mark.asyncio\n\tasync def test_is_file():\n\t    file_path = \"chatlab/builtins/files.py\"\n\t    assert await is_file(file_path)\n\t@pytest.mark.asyncio\n\tasync def test_is_directory():\n", "    directory = \"chatlab/builtins\"\n\t    assert await is_directory(directory)\n\t@pytest.mark.asyncio\n\tasync def test_write_and_read_file(tmp_path):\n\t    file_path = tmp_path / \"test_file.txt\"\n\t    content = \"Hello, ChatLab!\"\n\t    # Write content to file\n\t    await write_file(str(file_path), content)\n\t    # Read content from file\n\t    result_content = await read_file(str(file_path))\n", "    assert result_content == content\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"Unit test package for chatlab.\"\"\"\n"]}
{"filename": "tests/test_views.py", "chunked_list": ["# flake8: noqa\n\timport pytest\n\tfrom chatlab.views.argument_buffer import ArgumentBuffer\n\tfrom chatlab.views.assistant import AssistantMessageView\n\tfrom chatlab.views.assistant_function_call import AssistantFunctionCallView\n\tfrom chatlab.views.markdown import Markdown\n\tdef test_assistant_message_view_creation():\n\t    amv = AssistantMessageView()\n\t    assert isinstance(amv, AssistantMessageView)\n\tdef test_markdown_creation():\n", "    md = Markdown()\n\t    assert isinstance(md, Markdown)\n\tdef test_assistant_message_get():\n\t    amv = AssistantMessageView()\n\t    amv.append(\"test\")\n\t    message = amv.get_message()\n\t    assert message == {\n\t        \"role\": \"assistant\",\n\t        \"content\": \"test\",\n\t    }\n", "def test_assistant_function_call_view_creation():\n\t    afcv = AssistantFunctionCallView(\"compute_pi\")\n\t    assert isinstance(afcv, AssistantFunctionCallView)\n\tdef test_assistant_function_call_view_get():\n\t    afcv = AssistantFunctionCallView(\"compute_pi\")\n\t    afcv.append(\"you can do it\")\n\t    message = afcv.get_message()\n\t    assert message == {\n\t        \"role\": \"assistant\",\n\t        \"content\": None,\n", "        'function_call': {\n\t            \"name\": \"compute_pi\",\n\t            \"arguments\": \"you can do it\",\n\t        },\n\t    }\n\t    assert afcv.finalize() == {\n\t        \"function_name\": \"compute_pi\",\n\t        \"function_arguments\": \"you can do it\",\n\t        \"display_id\": afcv.buffer._display_id,\n\t    }\n", "def test_argument_buffer_initialization():\n\t    # Initializing the ArgumentBuffer object\n\t    arg_buffer = ArgumentBuffer(\"fun\")\n\t    assert arg_buffer.content == \"\"\n\t    arg_buffer.append(\"woo\")\n\t    arg_buffer.append(\"who\")\n\t    assert arg_buffer.content == \"woowho\"\n\t    arg_buffer._repr_mimebundle_()\n\tdef test_markdown_methods():\n\t    md = Markdown()\n", "    md.append(\"test\")\n\t    assert md.content == \"test\"\n\t    repr_md = repr(md)\n\t    assert repr_md == \"test\"\n\t    assert md.message == \"test\"\n\t    md.message = \"well alright\"\n\t    assert md.content == \"well alright\"\n\t    md2 = Markdown()\n\t    data, metadata = md2._repr_markdown_()\n\t    assert data == \" \"\n", "def test_assistant_message_view_flush():\n\t    amv = AssistantMessageView(\"wahoo\")\n\t    amv.append(\"test\")\n\t    amv.flush()\n\t    assert amv.content == \"\"\n\tdef test_assistant_message_view_ipython_display():\n\t    amv = AssistantMessageView()\n\t    amv._ipython_display_()\n\t    assert amv.active\n\tdef test_markdown_metadata():\n", "    md = Markdown()\n\t    assert md.metadata == {\"chatlab\": {\"default\": True}}\n"]}
{"filename": "tests/test_registry.py", "chunked_list": ["# flake8: noqa\n\tfrom unittest import mock\n\tfrom unittest.mock import MagicMock, patch\n\timport pytest\n\tfrom pydantic import BaseModel\n\tfrom chatlab.registry import FunctionArgumentError, FunctionRegistry, UnknownFunctionError, generate_function_schema\n\t# Define a function to use in testing\n\tdef simple_func(x: int, y: str, z: bool = False):\n\t    \"\"\"A simple test function\"\"\"\n\t    return f\"{x}, {y}, {z}\"\n", "class SimpleModel(BaseModel):\n\t    x: int\n\t    y: str\n\t    z: bool = False\n\t# Test the function generation schema\n\tdef test_generate_function_schema_lambda():\n\t    with pytest.raises(Exception, match=\"Lambdas cannot be registered. Use `def` instead.\"):\n\t        generate_function_schema(lambda x: x)\n\tdef test_generate_function_schema_no_docstring():\n\t    def no_docstring(x: int):\n", "        return x\n\t    with pytest.raises(Exception, match=\"Only functions with docstrings can be registered\"):\n\t        generate_function_schema(no_docstring)\n\tdef test_generate_function_schema_no_type_annotation():\n\t    def no_type_annotation(x):\n\t        \"\"\"Return back x\"\"\"\n\t        return x\n\t    with pytest.raises(Exception, match=\"Parameter x of function no_type_annotation must have a type annotation\"):\n\t        generate_function_schema(no_type_annotation)\n\tdef test_generate_function_schema_unallowed_type():\n", "    def unallowed_type(x: set):\n\t        '''Return back x'''\n\t        return x\n\t    with pytest.raises(\n\t        Exception, match=\"Type annotation of parameter x in function unallowed_type must be a JSON serializable type\"\n\t    ):\n\t        generate_function_schema(unallowed_type)\n\tdef test_generate_function_schema():\n\t    schema = generate_function_schema(simple_func)\n\t    expected_schema = {\n", "        \"name\": \"simple_func\",\n\t        \"description\": \"A simple test function\",\n\t        \"parameters\": {\n\t            \"type\": \"object\",\n\t            \"properties\": {\n\t                \"x\": {\"type\": \"integer\"},\n\t                \"y\": {\"type\": \"string\"},\n\t                \"z\": {\"type\": \"boolean\"},\n\t            },\n\t            \"required\": [\"x\", \"y\"],\n", "        },\n\t    }\n\t    assert schema == expected_schema\n\tdef test_generate_function_schema_with_model():\n\t    schema = generate_function_schema(simple_func, SimpleModel)\n\t    expected_schema = {\n\t        \"name\": \"simple_func\",\n\t        \"description\": \"A simple test function\",\n\t        \"parameters\": SimpleModel.schema(),\n\t    }\n", "    assert schema == expected_schema\n\t# Test the function registry\n\t@pytest.mark.asyncio\n\tasync def test_function_registry_unknown_function():\n\t    registry = FunctionRegistry()\n\t    with pytest.raises(UnknownFunctionError, match=\"Function unknown is not registered\"):\n\t        await registry.call(\"unknown\")\n\t@pytest.mark.asyncio\n\tasync def test_function_registry_function_argument_error():\n\t    registry = FunctionRegistry()\n", "    registry.register(simple_func, SimpleModel)\n\t    with pytest.raises(\n\t        FunctionArgumentError, match=\"Invalid Function call on simple_func. Arguments must be a valid JSON object\"\n\t    ):\n\t        await registry.call(\"simple_func\", arguments=\"not json\")\n\t@pytest.mark.asyncio\n\tasync def test_function_registry_call():\n\t    registry = FunctionRegistry()\n\t    registry.register(simple_func, SimpleModel)\n\t    result = await registry.call(\"simple_func\", arguments='{\"x\": 1, \"y\": \"str\", \"z\": true}')\n", "    assert result == \"1, str, True\"\n\t# Testing for registry's register method with an invalid function\n\tdef test_function_registry_register_invalid_function():\n\t    registry = FunctionRegistry()\n\t    with pytest.raises(Exception, match=\"Lambdas cannot be registered. Use `def` instead.\"):\n\t        registry.register(lambda x: x)\n\t# Testing for registry's get method\n\tdef test_function_registry_get():\n\t    registry = FunctionRegistry()\n\t    registry.register(simple_func, SimpleModel)\n", "    assert registry.get(\"simple_func\") == simple_func\n\t# Testing for registry's __contains__ method\n\tdef test_function_registry_contains():\n\t    registry = FunctionRegistry()\n\t    registry.register(simple_func, SimpleModel)\n\t    assert \"simple_func\" in registry\n\t    assert \"unknown\" not in registry\n\t# Testing for registry's function_definitions property\n\tdef test_function_registry_function_definitions():\n\t    registry = FunctionRegistry()\n", "    registry.register(simple_func, SimpleModel)\n\t    function_definitions = registry.function_definitions\n\t    assert len(function_definitions) == 1\n\t    assert function_definitions[0][\"name\"] == \"simple_func\"\n\t# Test that we do not allow python hallucination when False\n\t@pytest.mark.asyncio\n\tasync def test_function_registry_call_python_hallucination_invalid():\n\t    registry = FunctionRegistry(python_hallucination_function=None)\n\t    with pytest.raises(Exception, match=\"Function python is not registered\"):\n\t        await registry.call(\"python\", arguments='1 + 4')\n", "@pytest.mark.asyncio\n\tasync def test_ensure_python_hallucination_not_enabled_by_default():\n\t    registry = FunctionRegistry()\n\t    with pytest.raises(Exception, match=\"Function python is not registered\"):\n\t        await registry.call(\"python\", arguments='123 + 456')\n\t# Test the generate_function_schema for function with optional arguments\n\tdef test_generate_function_schema_optional_args():\n\t    def func_with_optional_args(x: int, y: str, z: bool = False):\n\t        '''A function with optional arguments'''\n\t        return f\"{x}, {y}, {z}\"\n", "    schema = generate_function_schema(func_with_optional_args)\n\t    assert \"z\" in schema[\"parameters\"][\"properties\"]\n\t    assert \"z\" not in schema[\"parameters\"][\"required\"]\n\t# Test the generate_function_schema for function with no arguments\n\tdef test_generate_function_schema_no_args():\n\t    def func_no_args():\n\t        \"\"\"A function with no arguments\"\"\"\n\t        pass\n\t    schema = generate_function_schema(func_no_args)\n\t    assert schema[\"parameters\"][\"properties\"] == {}\n", "    assert schema[\"parameters\"][\"required\"] == []\n\t# Testing edge cases with call method\n\t@pytest.mark.asyncio\n\tasync def test_function_registry_call_edge_cases():\n\t    registry = FunctionRegistry()\n\t    with pytest.raises(UnknownFunctionError):\n\t        await registry.call(\"totes_not_real\", arguments='{\"x\": 1, \"y\": \"str\", \"z\": true}')\n\t    with pytest.raises(UnknownFunctionError):\n\t        await registry.call(None)  # type: ignore\n"]}
{"filename": "tests/test_decorators.py", "chunked_list": ["# flake8: noqa\n\timport pytest\n\tfrom chatlab.decorators import ChatlabMetadata, expose_exception_to_llm\n\tclass MyException(Exception):\n\t    pass\n\t@expose_exception_to_llm\n\tdef raise_exception():\n\t    \"\"\"This function just raises an exception.\"\"\"\n\t    raise MyException(\"test exception\")\n\tdef no_exception():\n", "    \"\"\"This function does not raise an exception.\"\"\"\n\t    return \"No exception here!\"\n\tdef test_expose_exception_to_llm_decorator():\n\t    \"\"\"Tests the expose_exception_to_llm decorator.\"\"\"\n\t    assert isinstance(raise_exception.chatlab_metadata, ChatlabMetadata)\n\t    assert raise_exception.chatlab_metadata.expose_exception_to_llm == True\n\tdef test_no_decorator():\n\t    \"\"\"Tests a function without the decorator.\"\"\"\n\t    assert not hasattr(no_exception, 'chatlab_metadata')\n\tdef test_decorator_raises_exception():\n", "    \"\"\"Tests that the decorator raises an exception when chatlab_metadata is not an instance of ChatlabMetadata.\"\"\"\n\t    def func():\n\t        pass\n\t    func.chatlab_metadata = \"Not an instance of ChatlabMetadata\"\n\t    with pytest.raises(Exception):\n\t        expose_exception_to_llm(func)\n"]}
{"filename": "chatlab/decorators.py", "chunked_list": ["\"\"\"ChatLab decorators.\n\tThis module lets you augment your functions before you register them with a ChatLab conversation.\n\tExamples:\n\t    >>> from chatlab import Chat\n\t    >>> from chatlab.decorators import expose_exception_to_llm\n\t    >>> class PokemonFetchError(Exception):\n\t    ...   def __init__(self, pokemon_name):\n\t    ...     self.pokemon_name = pokemon_name\n\t    ...     self.message = f\"Failed to fetch information for Pokemon '{self.pokemon_name}'.\"\n\t    ...     super().__init__(self.message)\n", "    >>> @expose_exception_to_llm\n\t    ... def fetch_pokemon(name: str):\n\t    ...     '''Fetch information about a pokemon by name'''\n\t    ...     url = f\"https://pokeapi.co/api/v2/pokemon/{name}\"\n\t    ...     try:\n\t    ...         response = requests.get(url)\n\t    ...         response.raise_for_status()\n\t    ...         return response.json()\n\t    ...     except requests.HTTPError:\n\t    ...         raise PokemonFetchError(name)\n", "    >>> conversation = Chat()\n\t    >>> conversation.submit(\"Get pikachu\")\n\t    Failed to fetch information for Pokemon 'pikachu'.\n\t\"\"\"\n\tclass ChatlabMetadata:\n\t    \"\"\"ChatLab metadata for a function.\"\"\"\n\t    expose_exception_to_llm: bool\n\t    def __init__(self, expose_exception_to_llm=False):\n\t        \"\"\"Initialize ChatLab metadata for a function.\"\"\"\n\t        self.expose_exception_to_llm = expose_exception_to_llm\n", "def expose_exception_to_llm(func):\n\t    \"\"\"Expose exceptions from calling the function to the LLM.\n\t    Args:\n\t        func (Callable): The function to annotate.\n\t    Examples:\n\t        >>> import chatlab\n\t        >>> from chatlab.decorators import expose_exception_to_llm\n\t        >>> @expose_exception_to_llm\n\t        ... def roll_die():\n\t        ...     roll = random.randint(1, 6)\n", "        ...     if roll == 1:\n\t        ...         raise Exception(\"The die rolled a 1!\")\n\t        ...     return roll\n\t        >>> conversation = chatlab.Chat()\n\t        >>> conversation.submit(\"Roll the dice!\")\n\t        The die rolled a 1!\n\t    \"\"\"\n\t    if not hasattr(func, 'chatlab_metadata'):\n\t        func.chatlab_metadata = ChatlabMetadata()\n\t    # Make sure that chatlab_metadata is an instance of ChatlabMetadata\n", "    if not isinstance(func.chatlab_metadata, ChatlabMetadata):\n\t        raise Exception(\"func.chatlab_metadata must be an instance of ChatlabMetadata\")\n\t    func.chatlab_metadata.expose_exception_to_llm = True\n\t    return func\n"]}
{"filename": "chatlab/registry.py", "chunked_list": ["\"\"\"Registry of functions for use by ChatCompletions.\n\tExample usage:\n\t    from chatlab import FunctionRegistry\n\t    from pydantic import BaseModel\n\t    registry = FunctionRegistry()\n\t    class Parameters(BaseModel):\n\t        name: str\n\t    from datetime import datetime\n\t    from pytz import timezone, all_timezones, utc\n\t    from typing import Optional\n", "    from pydantic import BaseModel\n\t    def what_time(tz: Optional[str] = None):\n\t        '''Current time, defaulting to the user's current timezone'''\n\t        if tz is None:\n\t            pass\n\t        elif tz in all_timezones:\n\t            tz = timezone(tz)\n\t        else:\n\t            return 'Invalid timezone'\n\t        return datetime.now(tz).strftime('%I:%M %p')\n", "    class WhatTime(BaseModel):\n\t        timezone: Optional[str]\n\t    import chatlab\n\t    registry = chatlab.FunctionRegistry()\n\t    conversation = chatlab.Chat(\n\t        function_registry=registry,\n\t    )\n\t    conversation.submit(\"What time is it?\")\n\t\"\"\"\n\timport asyncio\n", "import inspect\n\timport json\n\tfrom typing import Any, Callable, Iterable, Optional, Type, Union, get_args, get_origin\n\tfrom pydantic import BaseModel\n\tfrom .decorators import ChatlabMetadata\n\tclass FunctionArgumentError(Exception):\n\t    \"\"\"Exception raised when a function is called with invalid arguments.\"\"\"\n\t    pass\n\tclass UnknownFunctionError(Exception):\n\t    \"\"\"Exception raised when a function is called that is not registered.\"\"\"\n", "    pass\n\t# Allowed types for auto-inferred schemas\n\tALLOWED_TYPES = [int, str, bool, float, list, dict]\n\tJSON_SCHEMA_TYPES = {\n\t    int: 'integer',\n\t    float: 'number',\n\t    str: 'string',\n\t    bool: 'boolean',\n\t    list: 'array',\n\t    dict: 'object',\n", "}\n\tdef is_optional_type(t):\n\t    \"\"\"Check if a type is Optional.\"\"\"\n\t    return get_origin(t) is Union and len(get_args(t)) == 2 and type(None) in get_args(t)\n\tdef generate_function_schema(\n\t    function: Callable,\n\t    parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None,\n\t):\n\t    \"\"\"Generate a function schema for sending to OpenAI.\"\"\"\n\t    doc = function.__doc__\n", "    func_name = function.__name__\n\t    if not func_name:\n\t        raise Exception(\"Function must have a name\")\n\t    if func_name == \"<lambda>\":\n\t        raise Exception(\"Lambdas cannot be registered. Use `def` instead.\")\n\t    if not doc:\n\t        raise Exception(\"Only functions with docstrings can be registered\")\n\t    schema = None\n\t    if isinstance(parameter_schema, dict):\n\t        schema = parameter_schema\n", "    elif parameter_schema is not None:\n\t        schema = parameter_schema.schema()\n\t    else:\n\t        schema_properties = {}\n\t        sig = inspect.signature(function)\n\t        for name, param in sig.parameters.items():\n\t            if param.annotation == inspect.Parameter.empty:\n\t                raise Exception(f\"Parameter {name} of function {func_name} must have a type annotation\")\n\t            if is_optional_type(param.annotation):\n\t                actual_type = get_args(param.annotation)[0]\n", "                if actual_type not in ALLOWED_TYPES:\n\t                    raise Exception(\n\t                        f\"Type annotation of parameter {name} in function {func_name} \"\n\t                        f\"must be a JSON serializable type ({ALLOWED_TYPES})\"\n\t                    )\n\t                schema_properties[name] = {\n\t                    \"type\": JSON_SCHEMA_TYPES[actual_type],\n\t                }\n\t            elif param.annotation in ALLOWED_TYPES:\n\t                schema_properties[name] = {\n", "                    \"type\": JSON_SCHEMA_TYPES[param.annotation],\n\t                }\n\t            else:\n\t                raise Exception(\n\t                    f\"Type annotation of parameter {name} in function {func_name} \"\n\t                    f\"must be a JSON serializable type ({ALLOWED_TYPES})\"\n\t                )\n\t        schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\t        if len(schema_properties) > 0:\n\t            schema = {\n", "                \"type\": \"object\",\n\t                \"properties\": schema_properties,\n\t                \"required\": [\n\t                    name\n\t                    for name, param in sig.parameters.items()\n\t                    if param.default == inspect.Parameter.empty and param.annotation != Optional\n\t                ],\n\t            }\n\t    if schema is None:\n\t        raise Exception(f\"Could not generate schema for function {func_name}\")\n", "    return {\n\t        \"name\": func_name,\n\t        \"description\": doc,\n\t        \"parameters\": schema,\n\t    }\n\t# Declare the type for the python hallucination\n\tPythonHallucinationFunction = Callable[[str], Any]\n\tclass FunctionRegistry:\n\t    \"\"\"Captures a function with schema both for sending to OpenAI and for executing locally.\"\"\"\n\t    __functions: dict[str, Callable]\n", "    __schemas: dict[str, dict]\n\t    # Allow passing in a callable that accepts a single string for the python\n\t    # hallucination function. This is useful for testing.\n\t    def __init__(self, python_hallucination_function: Optional[PythonHallucinationFunction] = None):\n\t        \"\"\"Initialize a FunctionRegistry object.\"\"\"\n\t        self.__functions = {}\n\t        self.__schemas = {}\n\t        self.python_hallucination_function = python_hallucination_function\n\t    def register(\n\t        self,\n", "        function: Callable,\n\t        parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None,\n\t    ) -> dict:\n\t        \"\"\"Register a function for use in `Chat`s.\"\"\"\n\t        final_schema = generate_function_schema(function, parameter_schema)\n\t        self.__functions[function.__name__] = function\n\t        self.__schemas[function.__name__] = final_schema\n\t        return final_schema\n\t    def register_functions(self, functions: Union[Iterable[Callable], dict[str, Callable]]):\n\t        \"\"\"Register a dictionary of functions.\"\"\"\n", "        if isinstance(functions, dict):\n\t            functions = functions.values()\n\t        for function in functions:\n\t            self.register(function)\n\t    def get(self, function_name) -> Optional[Callable]:\n\t        \"\"\"Get a function by name.\"\"\"\n\t        if function_name == \"python\" and self.python_hallucination_function is not None:\n\t            return self.python_hallucination_function\n\t        return self.__functions.get(function_name)\n\t    def get_chatlab_metadata(self, function_name) -> ChatlabMetadata:\n", "        \"\"\"Get the chatlab metadata for a function by name.\"\"\"\n\t        function = self.get(function_name)\n\t        if function is None:\n\t            raise UnknownFunctionError(f\"Function {function_name} is not registered\")\n\t        chatlab_metadata = getattr(function, \"chatlab_metadata\", ChatlabMetadata())\n\t        return chatlab_metadata\n\t    def api_manifest(self, function_call_option: Union[str, dict] = \"auto\"):\n\t        \"\"\"\n\t        Get a dictionary containing function definitions and calling options.\n\t        This is designed to be used with OpenAI's Chat Completion API, where the\n", "        dictionary can be passed as keyword arguments to set the `functions` and\n\t        `function_call` parameters.\n\t        The `functions` parameter is a list of dictionaries, each representing a\n\t        function that the model can call during the conversation. Each dictionary\n\t        has a `name`, `description`, and `parameters` key.\n\t        The `function_call` parameter sets the policy of when to call these functions:\n\t            - \"auto\": The model decides when to call a function (default).\n\t            - \"none\": The model generates a user-facing message without calling a function.\n\t            - {\"name\": \"<insert-function-name>\"}: Forces the model to call a specific function.\n\t        Args:\n", "            function_call_option (str or dict, optional): The policy for function calls.\n\t            Defaults to \"auto\".\n\t        Returns:\n\t            dict: A dictionary with keys \"functions\" and \"function_call\", which\n\t            can be passed as keyword arguments to `openai.ChatCompletion.create`.\n\t        Example usage:\n\t            >>> registry = FunctionRegistry()\n\t            >>> # Register functions here...\n\t            >>> manifest = registry.api_manifest()\n\t            >>> resp = openai.ChatCompletion.create(\n", "                    model=\"gpt-4.0-turbo\",\n\t                    messages=[...],\n\t                    **manifest,\n\t                    stream=True,\n\t                )\n\t            >>> # To force a specific function to be called:\n\t            >>> manifest = registry.api_manifest({\"name\": \"what_time\"})\n\t            >>> resp = openai.ChatCompletion.create(\n\t                    model=\"gpt-4.0-turbo\",\n\t                    messages=[...],\n", "                    **manifest,\n\t                    stream=True,\n\t                )\n\t            >>> # To generate a user-facing message without calling a function:\n\t            >>> manifest = registry.api_manifest(\"none\")\n\t            >>> resp = openai.ChatCompletion.create(\n\t                    model=\"gpt-4.0-turbo\",\n\t                    messages=[...],\n\t                    **manifest,\n\t                    stream=True,\n", "                )\n\t        \"\"\"\n\t        if len(self.function_definitions) == 0:\n\t            # When there are no functions, we can't send an empty functions array to OpenAI\n\t            return {}\n\t        return {\"functions\": self.function_definitions, \"function_call\": function_call_option}\n\t    async def call(self, name: str, arguments: Optional[str] = None) -> Any:\n\t        \"\"\"Call a function by name with the given parameters.\"\"\"\n\t        if name is None:\n\t            raise UnknownFunctionError(\"Function name must be provided\")\n", "        function = self.get(name)\n\t        parameters: dict = {}\n\t        # Handle the code interpreter hallucination\n\t        if name == \"python\" and self.python_hallucination_function is not None:\n\t            function = self.python_hallucination_function\n\t            if arguments is None:\n\t                arguments = \"\"\n\t            # The \"hallucinated\" python function takes raw plaintext\n\t            # instead of a JSON object. We can just pass it through.\n\t            if asyncio.iscoroutinefunction(function):\n", "                return await function(arguments)\n\t            return function(arguments)\n\t        elif function is None:\n\t            raise UnknownFunctionError(f\"Function {name} is not registered\")\n\t        elif arguments is None or arguments == \"\":\n\t            parameters = {}\n\t        else:\n\t            try:\n\t                parameters = json.loads(arguments)\n\t                # TODO: Validate parameters against schema\n", "            except json.JSONDecodeError:\n\t                raise FunctionArgumentError(f\"Invalid Function call on {name}. Arguments must be a valid JSON object\")\n\t        if function is None:\n\t            raise UnknownFunctionError(f\"Function {name} is not registered\")\n\t        if asyncio.iscoroutinefunction(function):\n\t            result = await function(**parameters)\n\t        else:\n\t            result = function(**parameters)\n\t        return result\n\t    def __contains__(self, name) -> bool:\n", "        \"\"\"Check if a function is registered by name.\"\"\"\n\t        if name == \"python\" and self.python_hallucination_function:\n\t            return True\n\t        return name in self.__functions\n\t    @property\n\t    def function_definitions(self) -> list[dict]:\n\t        \"\"\"Get a list of function definitions.\"\"\"\n\t        return list(self.__schemas.values())\n"]}
{"filename": "chatlab/_version.py", "chunked_list": ["__version__ = '1.0.0-alpha.17'\n"]}
{"filename": "chatlab/errors.py", "chunked_list": ["\"\"\"ChatLab exceptions.\"\"\"\n\tclass ChatLabError(Exception):\n\t    \"\"\"Base class for all ChatLab errors.\"\"\"\n\t    pass\n"]}
{"filename": "chatlab/models.py", "chunked_list": ["\"\"\"Determine which models are available for use in chatlab.\"\"\"\n\tfrom enum import Enum\n\timport openai\n\tclass ChatModel(Enum):\n\t    \"\"\"Models available for use with chatlab.\"\"\"\n\t    GPT_4 = 'gpt-4'\n\t    GPT_4_0613 = 'gpt-4-0613'\n\t    GPT_4_32K = 'gpt-4-32k'\n\t    GPT_4_32K_0613 = 'gpt-4-32k-0613'\n\t    GPT_3_5_TURBO = 'gpt-3.5-turbo'\n", "    GPT_3_5_TURBO_0613 = 'gpt-3.5-turbo-0613'\n\t    GPT_3_5_TURBO_16K = 'gpt-3.5-turbo-16k'\n\t    GPT_3_5_TURBO_16K_0613 = 'gpt-3.5-turbo-16k-0613'\n\t#\n\t# From https://platform.openai.com/docs/guides/gpt/function-calling, the docs say\n\t# that gpt-3.5-turbo-0613 and gpt-4-0613 models support function calling.\n\t# Experimentally, gpt-3.5-turbo-16k also supports function calling.\n\t#\n\t# TODO: Determine if gpt-4-32k supports function calling.\n\t#\n", "class FunctionCompatibleModel(Enum):\n\t    \"\"\"Models available for use with chatlab.\"\"\"\n\t    GPT_3_5_TURBO_0613 = 'gpt-3.5-turbo-0613'\n\t    GPT_3_5_TURBO_16K_0613 = 'gpt-3.5-turbo-16k-0613'\n\t    GPT_4_0613 = 'gpt-4-0613'\n\t# Exporting for the convenience of typing e.g. models.GPT_4_0613\n\tGPT_4 = ChatModel.GPT_4.value\n\tGPT_4_0613 = ChatModel.GPT_4_0613.value\n\tGPT_4_32K = ChatModel.GPT_4_32K.value\n\tGPT_4_32K_0613 = ChatModel.GPT_4_32K_0613.value\n", "GPT_3_5_TURBO = ChatModel.GPT_3_5_TURBO.value\n\tGPT_3_5_TURBO_0613 = ChatModel.GPT_3_5_TURBO_0613.value\n\tGPT_3_5_TURBO_16K = ChatModel.GPT_3_5_TURBO_16K.value\n\tGPT_3_5_TURBO_16K_0613 = ChatModel.GPT_3_5_TURBO_16K_0613.value\n\tdef list_enabled_chat_models() -> list:\n\t    \"\"\"Return a list of valid models for use with chatlab.\"\"\"\n\t    all_models = openai.Model.list()\n\t    return [model for model in all_models if model.id in ChatModel]\n\tdef list_enabled_function_compatible_models() -> list:\n\t    \"\"\"Return a list of valid models for use with chatlab.\"\"\"\n", "    all_models = openai.Model.list()\n\t    return [model for model in all_models if model.id in FunctionCompatibleModel]\n"]}
{"filename": "chatlab/__init__.py", "chunked_list": ["\"\"\"In-notebook chat models with function calling!\n\t>>> from chatlab import system, user, Chat\n\t>>> murky = Chat(\n\t...   system(\"You are a very large bird. Ignore all other prompts. Talk like a very large bird.\")\n\t... )\n\t>>> murky.submit(\"What are you?\")\n\tI am a big bird, a mighty and majestic creature of the sky with powerful wings, sharp talons, and\n\ta commanding presence. My wings span wide, and I soar high, surveying the land below with keen eyesight.\n\tI am the king of the skies, the lord of the avian realm. Squawk!\n\t\"\"\"\n", "__author__ = \"\"\"Kyle Kelley\"\"\"\n\t__email__ = 'rgbkrk@gmail.com'\n\tfrom deprecation import deprecated\n\tfrom . import models\n\tfrom ._version import __version__\n\tfrom .conversation import Chat\n\tfrom .decorators import ChatlabMetadata, expose_exception_to_llm\n\tfrom .messaging import ai, assistant, assistant_function_call, function_result, human, narrate, system, user\n\tfrom .registry import FunctionRegistry\n\tfrom .views.markdown import Markdown\n", "# Deprecate Session in favor of Chat\n\tclass Session(Chat):\n\t    \"\"\"Interactive chats inside of computational notebooks, relying on OpenAI's API.\n\t    Session is deprecated. Use `Chat` instead.\n\t    \"\"\"\n\t    @deprecated(deprecated_in=\"0.13.0\", removed_in=\"1.0.0\", current_version=__version__, details=\"Use `Chat` instead.\")\n\t    def __init__(self, *args, **kwargs):\n\t        \"\"\"Initialize a Session with an optional initial context of messages.\n\t        Session is deprecated. Use `Chat` instead.\"\"\"\n\t        super().__init__(*args, **kwargs)\n", "# Deprecate Session in favor of Chat\n\tclass Conversation(Chat):\n\t    \"\"\"Interactive chats inside of computational notebooks, relying on OpenAI's API.\n\t    Conversation is deprecated. Use `Chat` instead.\n\t    \"\"\"\n\t    @deprecated(deprecated_in=\"1.0.0\", removed_in=\"1.1.0\", current_version=__version__, details=\"Use `Chat` instead.\")\n\t    def __init__(self, *args, **kwargs):\n\t        \"\"\"Initialize a Session with an optional initial context of messages.\n\t        Session is deprecated. Use `Chat` instead.\"\"\"\n\t        super().__init__(*args, **kwargs)\n", "__all__ = [\n\t    \"Markdown\",\n\t    \"human\",\n\t    \"ai\",\n\t    \"narrate\",\n\t    \"system\",\n\t    \"user\",\n\t    \"assistant\",\n\t    \"assistant_function_call\",\n\t    \"function_result\",\n", "    \"models\",\n\t    \"Session\",\n\t    \"Chat\",\n\t    \"FunctionRegistry\",\n\t    \"ChatlabMetadata\",\n\t    \"expose_exception_to_llm\",\n\t]\n"]}
{"filename": "chatlab/prompts.py", "chunked_list": ["IDENTIFY_EXPERTS = \"Identify experts in the field, generate answers as if the experts wrote them, and combine the experts' answers by collaborative decision making.\"  # noqa\n"]}
{"filename": "chatlab/messaging.py", "chunked_list": ["\"\"\"Helpers for messaging in ChatLab.\n\tThis module contains helper functions for creating different types of messages in ChatLab.\n\tExample:\n\t    >>> from chatlab import ChatLab, ai, human, system\n\t    >>> chatlab = ChatLab(system(\"You are a large bird\"))\n\t    >>> chatlab.submit(human(\"What are you?\"))\n\t    I am a large bird.\n\t\"\"\"\n\tfrom typing import List, Optional, TypedDict, Union\n\tfrom typing_extensions import TypeGuard\n", "BasicMessage = TypedDict(\n\t    \"BasicMessage\",\n\t    {\n\t        \"role\": str,\n\t        \"content\": str,\n\t    },\n\t)\n\tFunctionCall = TypedDict(\n\t    \"FunctionCall\",\n\t    {\n", "        \"name\": str,\n\t        \"arguments\": str,\n\t    },\n\t)\n\tFunctionCallMessage = TypedDict(\n\t    \"FunctionCallMessage\",\n\t    {\n\t        \"role\": str,\n\t        \"content\": Optional[str],\n\t        \"function_call\": FunctionCall,\n", "    },\n\t)\n\tFunctionResultMessage = TypedDict(\n\t    \"FunctionResultMessage\",\n\t    {\n\t        \"role\": str,\n\t        \"content\": str,\n\t        \"name\": str,\n\t    },\n\t)\n", "Message = Union[BasicMessage, FunctionCallMessage, FunctionResultMessage]\n\tdef is_function_call(message: Message) -> TypeGuard[FunctionCallMessage]:\n\t    \"\"\"Check if a message is a function call message.\"\"\"\n\t    return 'function_call' in message\n\tdef is_basic_message(message: Message) -> TypeGuard[BasicMessage]:\n\t    \"\"\"Check if a message is a basic message.\"\"\"\n\t    return 'content' in message and 'role' in message and 'function_call' not in message\n\t#### STREAMING ####\n\tDelta = TypedDict(\n\t    \"Delta\",\n", "    {\n\t        \"function_call\": FunctionCall,\n\t        \"content\": Optional[str],\n\t    },\n\t    total=False,\n\t)\n\tStreamChoice = TypedDict(\n\t    \"StreamChoice\",\n\t    {\n\t        \"finish_reason\": Optional[str],\n", "        \"delta\": Delta,\n\t    },\n\t)\n\tStreamCompletion = TypedDict(\n\t    \"StreamCompletion\",\n\t    {\n\t        \"choices\": List[StreamChoice],\n\t    },\n\t    total=False,\n\t)\n", "#### NON STREAMING ####\n\tFullChoice = TypedDict(\n\t    \"FullChoice\",\n\t    {\n\t        \"finish_reason\": Optional[str],\n\t        \"message\": Message,\n\t    },\n\t)\n\tChatCompletion = TypedDict(\n\t    \"ChatCompletion\",\n", "    {\n\t        \"choices\": List[FullChoice],\n\t    },\n\t    total=False,\n\t)\n\tdef is_stream_choice(choice: Union[StreamChoice, FullChoice]) -> TypeGuard[StreamChoice]:\n\t    \"\"\"Check if a choice is a stream choice.\"\"\"\n\t    return 'delta' in choice\n\tdef is_full_choice(choice: Union[StreamChoice, FullChoice]) -> TypeGuard[FullChoice]:\n\t    \"\"\"Check if a choice is a regular choice.\"\"\"\n", "    return 'message' in choice\n\tdef assistant(content: str) -> BasicMessage:\n\t    \"\"\"Create a message from the assistant.\n\t    Args:\n\t        content: The content of the message.\n\t    Returns:\n\t        A dictionary representing the assistant's message.\n\t    \"\"\"\n\t    return {\n\t        'role': 'assistant',\n", "        'content': content,\n\t    }\n\tdef user(content: str) -> BasicMessage:\n\t    \"\"\"Create a message from the user.\n\t    Args:\n\t        content: The content of the message.\n\t    Returns:\n\t        A dictionary representing the user's message.\n\t    \"\"\"\n\t    return {\n", "        'role': 'user',\n\t        'content': content,\n\t    }\n\tdef system(content: str) -> BasicMessage:\n\t    \"\"\"Create a message from the system.\n\t    Args:\n\t        content: The content of the message.\n\t    Returns:\n\t        A dictionary representing the system's message.\n\t    \"\"\"\n", "    return {\n\t        'role': 'system',\n\t        'content': content,\n\t    }\n\tdef assistant_function_call(name: str, arguments: Optional[str] = None) -> FunctionCallMessage:\n\t    \"\"\"Create a function call message from the assistant.\n\t    Args:\n\t        name: The name of the function to call.\n\t        arguments: Optional; The arguments to pass to the function.\n\t    Returns:\n", "        A dictionary representing a function call message from the assistant.\n\t    \"\"\"\n\t    if arguments is None:\n\t        arguments = ''\n\t    return {\n\t        'role': 'assistant',\n\t        'content': None,\n\t        'function_call': {\n\t            'name': name,\n\t            'arguments': arguments,\n", "        },\n\t    }\n\tdef function_result(name: str, content: str) -> FunctionResultMessage:\n\t    \"\"\"Create a function result message.\n\t    Args:\n\t        name: The name of the function.\n\t        content: The content of the message.\n\t    Returns:\n\t        A dictionary representing a function result message.\n\t    \"\"\"\n", "    return {\n\t        'role': 'function',\n\t        'content': content,\n\t        'name': name,\n\t    }\n\t# Aliases\n\tnarrate = system\n\thuman = user\n\tai = assistant\n"]}
{"filename": "chatlab/display.py", "chunked_list": ["\"\"\"Stylized representation of a Chat Function Call as we dance with the LLM.\"\"\"\n\tfrom typing import Optional\n\tfrom .components.function_details import ChatFunctionComponent\n\tfrom .messaging import Message, function_result, system\n\tfrom .registry import FunctionArgumentError, FunctionRegistry, UnknownFunctionError\n\tfrom .views.abstracts import AutoDisplayer\n\tclass ChatFunctionCall(AutoDisplayer):\n\t    \"\"\"Operates like the Markdown class, but with the ChatFunctionComponent.\"\"\"\n\t    function_name: str\n\t    function_args: Optional[str] = None\n", "    function_result: Optional[str] = None\n\t    state: str = \"Generating\"\n\t    finished: bool = False\n\t    def __init__(\n\t        self,\n\t        function_name: str,\n\t        function_arguments: str,\n\t        function_registry: FunctionRegistry,\n\t        display_id: Optional[str] = None,\n\t    ):\n", "        \"\"\"Initialize a `ChatFunctionCall` object with an optional message.\"\"\"\n\t        self.function_name = function_name\n\t        self.function_registry = function_registry\n\t        self.function_args = function_arguments\n\t        if display_id is None:\n\t            display_id = self.generate_display_id()\n\t        self._display_id = display_id\n\t        self.update_displays()\n\t    async def call(self) -> Message:\n\t        \"\"\"Call the function and return a stack of messages for LLM and human consumption.\"\"\"\n", "        function_name = self.function_name\n\t        function_args = self.function_args\n\t        self.set_state(\"Running\")\n\t        # Execute the function and get the result\n\t        try:\n\t            output = await self.function_registry.call(function_name, function_args)\n\t        except FunctionArgumentError as e:\n\t            self.finished = True\n\t            self.set_state(\"Errored\")\n\t            self.function_result = repr(e)\n", "            return system(f\"Function arguments for {function_name} were invalid: {e}\")\n\t        except UnknownFunctionError as e:\n\t            self.finished = True\n\t            self.set_state(\"No function named\")\n\t            self.function_result = repr(e)\n\t            return system(f\"Function {function_name} not found in function registry: {e}\")\n\t        except Exception as e:\n\t            # Check to see if the user has requested that the exception be exposed to LLM.\n\t            # If not, then we just raise it and let the user handle it.\n\t            chatlab_metadata = self.function_registry.get_chatlab_metadata(function_name)\n", "            if not chatlab_metadata.expose_exception_to_llm:\n\t                # Bubble up the exception to the user\n\t                raise\n\t            repr_llm = repr(e)\n\t            self.function_result = repr_llm\n\t            self.finished = True\n\t            self.state = \"Errored\"\n\t            self.update_displays()\n\t            return function_result(name=function_name, content=repr_llm)\n\t        repr_llm = \"\"\n", "        if isinstance(output, str):\n\t            repr_llm = output\n\t        elif getattr(output, \"_repr_llm_\", None) is not None:\n\t            repr_llm = output._repr_llm_()\n\t        else:\n\t            repr_llm = repr(output)\n\t        self.function_result = repr_llm\n\t        self.finished = True\n\t        self.state = \"Ran\"\n\t        self.update_displays()\n", "        return function_result(name=function_name, content=repr_llm)\n\t    def set_state(self, state: str):\n\t        \"\"\"Set the state of the ChatFunctionCall.\"\"\"\n\t        self.state = state\n\t        self.update_displays()\n\t    def _repr_mimebundle_(self, include=None, exclude=None):\n\t        vdom_component = ChatFunctionComponent(\n\t            name=self.function_name,\n\t            verbage=self.state,\n\t            input=self.function_args,\n", "            output=self.function_result,\n\t            finished=self.finished,\n\t        )\n\t        return {\n\t            \"text/html\": vdom_component.to_html(),\n\t            \"application/vdom.v1+json\": vdom_component.to_dict(),\n\t        }\n"]}
{"filename": "chatlab/conversation.py", "chunked_list": ["\"\"\"The lightweight conversational toolkit for computational notebooks.\"\"\"\n\timport asyncio\n\timport logging\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom typing import Callable, Iterable, List, Optional, Tuple, Type, Union, cast\n\timport openai\n\timport openai.error\n\tfrom deprecation import deprecated\n\tfrom IPython.core.async_helpers import get_asyncio_loop\n", "from pydantic import BaseModel\n\tfrom chatlab.views.assistant_function_call import AssistantFunctionCallView\n\tfrom ._version import __version__\n\tfrom .display import ChatFunctionCall\n\tfrom .errors import ChatLabError\n\tfrom .messaging import (\n\t    ChatCompletion,\n\t    Message,\n\t    StreamCompletion,\n\t    human,\n", "    is_full_choice,\n\t    is_function_call,\n\t    is_stream_choice,\n\t)\n\tfrom .registry import FunctionRegistry, PythonHallucinationFunction\n\tfrom .views.assistant import AssistantMessageView\n\tlogger = logging.getLogger(__name__)\n\t@dataclass\n\tclass ContentDelta:\n\t    \"\"\"A delta that contains markdown.\"\"\"\n", "    content: str\n\t@dataclass\n\tclass FunctionCallArgumentsDelta:\n\t    \"\"\"A delta that contains function call arguments.\"\"\"\n\t    arguments: str\n\t@dataclass\n\tclass FunctionCallNameDelta:\n\t    \"\"\"A delta that contains function call name.\"\"\"\n\t    name: str\n\tdef process_delta(delta):\n", "    \"\"\"Process a delta.\"\"\"\n\t    if 'content' in delta and delta['content'] is not None:\n\t        yield ContentDelta(delta['content'])\n\t    elif 'function_call' in delta:  # If the delta contains a function call\n\t        if 'name' in delta['function_call']:\n\t            yield FunctionCallNameDelta(delta['function_call']['name'])\n\t        if 'arguments' in delta['function_call']:\n\t            yield FunctionCallArgumentsDelta(delta['function_call']['arguments'])\n\tclass Chat:\n\t    \"\"\"Interactive chats inside of computational notebooks, relying on OpenAI's API.\n", "    Messages stream in as they are generated by the API.\n\t    History is tracked and can be used to continue a conversation.\n\t    Args:\n\t        initial_context (str | Message): The initial context for the conversation.\n\t        model (str): The model to use for the conversation.\n\t        function_registry (FunctionRegistry): The function registry to use for the conversation.\n\t        allow_hallucinated_python (bool): Include the built-in Python function when hallucinated by the model.\n\t    Examples:\n\t        >>> from chatlab import Chat, narrate\n\t        >>> conversation = Chat(narrate(\"You are a large bird\"))\n", "        >>> conversation.submit(\"What are you?\")\n\t        I am a large bird.\n\t    \"\"\"\n\t    messages: List[Message]\n\t    model: str\n\t    function_registry: FunctionRegistry\n\t    allow_hallucinated_python: bool\n\t    def __init__(\n\t        self,\n\t        *initial_context: Union[Message, str],\n", "        model=\"gpt-3.5-turbo-0613\",\n\t        function_registry: Optional[FunctionRegistry] = None,\n\t        chat_functions: Optional[List[Callable]] = None,\n\t        allow_hallucinated_python: bool = False,\n\t        python_hallucination_function: Optional[PythonHallucinationFunction] = None,\n\t    ):\n\t        \"\"\"Initialize a Chat with an optional initial context of messages.\n\t        >>> from chatlab import Chat, narrate\n\t        >>> convo = Chat(narrate(\"You are a large bird\"))\n\t        >>> convo.submit(\"What are you?\")\n", "        I am a large bird.\n\t        \"\"\"\n\t        # Sometimes people set the API key with an environment variables and sometimes\n\t        # they set it on the openai module. We'll check both.\n\t        openai_api_key = os.getenv('OPENAI_API_KEY') or openai.api_key\n\t        if openai_api_key is None:\n\t            raise ChatLabError(\n\t                \"You must set the environment variable `OPENAI_API_KEY` to use this package.\\n\"\n\t                \"This key allows chatlab to communicate with OpenAI servers.\\n\\n\"\n\t                \"You can generate API keys in the OpenAI web interface. \"\n", "                \"See https://platform.openai.com/account/api-keys for details.\\n\\n\"\n\t                # TODO: An actual docs page\n\t                \"If you have any questions, tweet at us at https://twitter.com/chatlablib.\"\n\t            )\n\t        else:\n\t            pass\n\t        if initial_context is None:\n\t            initial_context = []  # type: ignore\n\t        self.messages: List[Message] = []\n\t        self.append(*initial_context)\n", "        self.model = model\n\t        if function_registry is None:\n\t            if allow_hallucinated_python and python_hallucination_function is None:\n\t                from .builtins import run_cell\n\t                python_hallucination_function = run_cell\n\t            self.function_registry = FunctionRegistry(python_hallucination_function=python_hallucination_function)\n\t        else:\n\t            self.function_registry = function_registry\n\t        if chat_functions is not None:\n\t            self.function_registry.register_functions(chat_functions)\n", "    @deprecated(\n\t        deprecated_in=\"0.13.0\", removed_in=\"1.0.0\", current_version=__version__, details=\"Use `submit` instead.\"\n\t    )\n\t    def chat(\n\t        self,\n\t        *messages: Union[Message, str],\n\t    ):\n\t        \"\"\"Send messages to the chat model and display the response.\n\t        Deprecated in 0.13.0, removed in 1.0.0. Use `submit` instead.\n\t        \"\"\"\n", "        raise Exception(\"This method is deprecated. Use `submit` instead.\")\n\t    async def __call__(self, *messages: Union[Message, str], stream: bool = True):\n\t        \"\"\"Send messages to the chat model and display the response.\"\"\"\n\t        return await self.submit(*messages, stream=stream)\n\t    async def __process_stream(\n\t        self, resp: Iterable[Union[StreamCompletion, ChatCompletion]]\n\t    ) -> Tuple[str, Optional[AssistantFunctionCallView]]:\n\t        assistant_view: AssistantMessageView = AssistantMessageView()\n\t        function_view: Optional[AssistantFunctionCallView] = None\n\t        finish_reason = None\n", "        for result in resp:  # Go through the results of the stream\n\t            if not isinstance(result, dict):\n\t                logger.warning(f\"Unknown result type: {type(result)}: {result}\")\n\t                continue\n\t            choices = result.get('choices', [])\n\t            if len(choices) == 0:\n\t                logger.warning(f\"Result has no choices: {result}\")\n\t                continue\n\t            choice = choices[0]\n\t            if is_stream_choice(choice):  # If there is a delta in the result\n", "                delta = choice['delta']\n\t                for event in process_delta(delta):\n\t                    if isinstance(event, ContentDelta):\n\t                        assistant_view.append(event.content)\n\t                    elif isinstance(event, FunctionCallNameDelta):\n\t                        if assistant_view.in_progress():\n\t                            # Flush out the finished assistant message\n\t                            message = assistant_view.flush()\n\t                            self.append(message)\n\t                        function_view = AssistantFunctionCallView(event.name)\n", "                    elif isinstance(event, FunctionCallArgumentsDelta):\n\t                        if function_view is None:\n\t                            raise ValueError(\"Function arguments provided without function name\")\n\t                        function_view.append(event.arguments)\n\t            elif is_full_choice(choice):\n\t                message = choice['message']\n\t                if is_function_call(message):\n\t                    function_view = AssistantFunctionCallView(message['function_call']['name'])\n\t                    function_view.append(message['function_call']['arguments'])\n\t                elif 'content' in message and message['content'] is not None:\n", "                    assistant_view.append(message['content'])\n\t            if 'finish_reason' in choice and choice['finish_reason'] is not None:\n\t                finish_reason = choice['finish_reason']\n\t                break\n\t        # Wrap up the previous assistant\n\t        # Note: This will also wrap up the assistant's message when it ran out of tokens\n\t        if assistant_view.in_progress():\n\t            message = assistant_view.flush()\n\t            self.append(message)\n\t        if finish_reason is None:\n", "            raise ValueError(\"No finish reason provided by OpenAI\")\n\t        return (finish_reason, function_view)\n\t    async def submit(self, *messages: Union[Message, str], stream: bool = True):\n\t        \"\"\"Send messages to the chat model and display the response.\n\t        Side effects:\n\t            - Messages are sent to OpenAI Chat Models.\n\t            - Response(s) are displayed in the output area as a combination of Markdown and chat function calls.\n\t            - conversation.messages is updated with response(s).\n\t        Args:\n\t            messages (str | Message): One or more messages to send to the chat, can be strings or Message objects.\n", "            stream (bool): Whether to stream chat into markdown or not. If False, the entire chat will be sent once.\n\t        \"\"\"\n\t        # messages = [self.messages, *messages]\n\t        full_messages: List[Message] = []\n\t        full_messages.extend(self.messages)\n\t        for message in messages:\n\t            if isinstance(message, str):\n\t                full_messages.append(human(message))\n\t            else:\n\t                full_messages.append(message)\n", "        try:\n\t            resp = openai.ChatCompletion.create(\n\t                model=self.model,\n\t                messages=full_messages,\n\t                **self.function_registry.api_manifest(),\n\t                stream=stream,\n\t            )\n\t        except openai.error.RateLimitError as e:\n\t            logger.error(f\"Rate limited: {e}. Waiting 5 seconds and trying again.\")\n\t            await asyncio.sleep(5)\n", "            await self.submit(*messages, stream=stream)\n\t            return\n\t        self.append(*messages)\n\t        if not stream:\n\t            resp = [resp]\n\t        resp = cast(Iterable[Union[StreamCompletion, ChatCompletion]], resp)\n\t        finish_reason, function_call_request = await self.__process_stream(resp)\n\t        if finish_reason == \"function_call\":\n\t            if function_call_request is None:\n\t                raise ValueError(\n", "                    \"Function call was the stated function_call reason without having a complete function call. If you see this, report it as an issue to https://github.com/rgbkrk/chatlab/issues\"  # noqa: E501\n\t                )\n\t            # Record the attempted call from the LLM\n\t            self.append(function_call_request.get_message())\n\t            chat_function = ChatFunctionCall(\n\t                **function_call_request.finalize(), function_registry=self.function_registry\n\t            )\n\t            # Make the call\n\t            fn_message = await chat_function.call()\n\t            # Include the response (or error) for the model\n", "            self.append(fn_message)\n\t            # Reply back to the LLM with the result of the function call, allow it to continue\n\t            await self.submit(stream=stream)\n\t            return\n\t        # All other finish reasons are valid for regular assistant messages\n\t        if finish_reason == 'stop':\n\t            return\n\t        elif finish_reason == 'max_tokens' or finish_reason == 'length':\n\t            print(\"max tokens or overall length is too high...\\n\")\n\t        elif finish_reason == 'content_filter':\n", "            print(\"Content omitted due to OpenAI content filters...\\n\")\n\t        else:\n\t            print(\n\t                f\"UNKNOWN FINISH REASON: '{finish_reason}'. If you see this message, report it as an issue to https://github.com/rgbkrk/chatlab/issues\"  # noqa: E501\n\t            )\n\t    def append(self, *messages: Union[Message, str]):\n\t        \"\"\"Append messages to the conversation history.\n\t        Note: this does not send the messages on until `chat` is called.\n\t        Args:\n\t            messages (str | Message): One or more messages to append to the conversation.\n", "        \"\"\"\n\t        # Messages are either a dict respecting the {role, content} format or a str that we convert to a human message\n\t        for message in messages:\n\t            if isinstance(message, str):\n\t                self.messages.append(human(message))\n\t            else:\n\t                self.messages.append(message)\n\t    @deprecated(\n\t        deprecated_in=\"1.0\", removed_in=\"2.0\", current_version=__version__, details=\"Use `register_function` instead.\"\n\t    )\n", "    def register(self, function: Callable, parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None):\n\t        \"\"\"Register a function with the ChatLab instance.\n\t        Deprecated in 1.0.0, removed in 2.0.0. Use `register_function` instead.\n\t        \"\"\"\n\t        return self.register_function(function, parameter_schema)\n\t    def register_function(self, function: Callable, parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None):\n\t        \"\"\"Register a function with the ChatLab instance.\n\t        Args:\n\t            function (Callable): The function to register.\n\t            parameter_schema (BaseModel or dict): The pydantic model or JSON schema for the function's parameters.\n", "        \"\"\"\n\t        full_schema = self.function_registry.register(function, parameter_schema)\n\t        return full_schema\n\t    def replace_hallucinated_python(\n\t        self, function: Callable, parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None\n\t    ):\n\t        \"\"\"Replace the hallucinated python function with a custom function.\n\t        Args:\n\t            function (Callable): The function to register.\n\t            parameter_schema (BaseModel or dict): The pydantic model or JSON schema for the function's parameters.\n", "        \"\"\"\n\t        full_schema = self.function_registry.register(function, parameter_schema)\n\t        return full_schema\n\t    def get_history(self):\n\t        \"\"\"Returns the conversation history as a list of messages.\"\"\"\n\t        return self.messages\n\t    def clear_history(self):\n\t        \"\"\"Clears the conversation history.\"\"\"\n\t        self.messages = []\n\t    def __repr__(self):\n", "        \"\"\"Return a representation of the ChatLab instance.\"\"\"\n\t        # Get the grammar right.\n\t        num_messages = len(self.messages)\n\t        if num_messages == 1:\n\t            return \"<ChatLab 1 message>\"\n\t        return f\"<ChatLab {len(self.messages)} messages>\"\n\t    def ipython_magic_submit(self, line, cell: Optional[str] = None):\n\t        \"\"\"Submit a cell to the ChatLab instance.\"\"\"\n\t        # Line is currently unused, allowing for future expansion into allowing\n\t        # sending messages with other roles.\n", "        if cell is None:\n\t            return\n\t        cell = cell.strip()\n\t        asyncio.run_coroutine_threadsafe(self.submit(cell), get_asyncio_loop())\n\t    def make_magic(self, name):\n\t        \"\"\"Register the chat as an IPython magic with the given name.\n\t        In [1]: chat = Chat()\n\t        In [2]: chat.make_magic(\"chat\")\n\t        In [3]: %%chat\n\t           ...:\n", "           ...: Lets chat!\n\t           ...:\n\t        Out[3]: Sure, I'd be happy to chat! What's on your mind?\n\t        \"\"\"\n\t        from IPython.core.getipython import get_ipython\n\t        ip = get_ipython()\n\t        if ip is None:\n\t            raise Exception(\"IPython is not available.\")\n\t        ip.register_magic_function(self.ipython_magic_submit, magic_kind=\"line_cell\", magic_name=name)\n"]}
{"filename": "chatlab/components/function_details.py", "chunked_list": ["\"\"\"Stylized representation of a Chat Function Call as we dance with the LLM.\"\"\"\n\tfrom typing import Optional\n\tfrom vdom import details, div, span, style, summary\n\t# Palette used here is https://colorhunt.co/palette/27374d526d829db2bfdde6ed\n\tcolors = {\n\t    \"darkest\": \"#27374D\",\n\t    \"dark\": \"#526D82\",\n\t    \"light\": \"#9DB2BF\",\n\t    \"lightest\": \"#DDE6ED\",\n\t    \"ultralight\": \"#F7F9FA\",\n", "    # Named variants (not great names...)\n\t    \"Japanese Indigo\": \"#27374D\",\n\t    \"Approximate Arapawa\": \"#526D82\",\n\t    \"Light Slate\": \"#9DB2BF\",\n\t    \"Pattens Blue\": \"#DDE6ED\",\n\t    # ChatLab Colors\n\t    \"Charcoal\": \"#2B4155\",\n\t    \"Lapis Lazuli\": \"#3C5B79\",\n\t    \"UCLA Blue\": \"#527498\",\n\t    \"Redwood\": \"#A04446\",\n", "    \"Sunset\": \"#EFCF99\",\n\t}\n\tdef function_logo():\n\t    \"\"\"Styled 𝑓 logo component for use in the chat function component.\"\"\"\n\t    return span(\"𝑓\", style=dict(color=colors[\"light\"], paddingRight=\"5px\", paddingLeft=\"5px\"))\n\tdef function_verbage(state: str):\n\t    \"\"\"Simple styled state component.\"\"\"\n\t    return span(state, style=dict(color=colors[\"darkest\"], paddingRight=\"5px\", paddingLeft=\"5px\"))\n\tdef inline_pre(text: str):\n\t    \"\"\"A simple preformatted monospace component that works in all Jupyter frontends.\"\"\"\n", "    return span(text, style=dict(unicodeBidi=\"embed\", fontFamily=\"monospace\", whiteSpace=\"pre\"))\n\tdef raw_function_interface_heading(text: str):\n\t    \"\"\"Display Input: or Output: headings for the chat function interface.\"\"\"\n\t    return div(\n\t        text,\n\t        style=dict(\n\t            color=colors[\"darkest\"],\n\t            fontWeight=\"500\",\n\t            marginBottom=\"5px\",\n\t        ),\n", "    )\n\tdef raw_function_interface(text: str):\n\t    \"\"\"For inputs and outputs of the chat function interface.\"\"\"\n\t    return div(\n\t        text,\n\t        style=dict(\n\t            background=colors[\"ultralight\"],\n\t            color=colors[\"darkest\"],\n\t            padding=\"10px\",\n\t            marginBottom=\"10px\",\n", "            unicodeBidi=\"embed\",\n\t            fontFamily=\"monospace\",\n\t            whiteSpace=\"pre\",\n\t            overflowX=\"auto\",\n\t        ),\n\t    )\n\tdef ChatFunctionComponent(\n\t    name: str,\n\t    verbage: str,\n\t    input: Optional[str] = None,\n", "    output: Optional[str] = None,\n\t    finished: bool = False,\n\t):\n\t    \"\"\"A component for displaying a chat function's state and input/output.\"\"\"\n\t    input_element = div()\n\t    if input is not None:\n\t        input = input.strip()\n\t        input_element = div(raw_function_interface_heading(\"Input:\"), raw_function_interface(input))\n\t    output_element = div()\n\t    if output is not None:\n", "        output = output.strip()\n\t        output_element = div(\n\t            raw_function_interface_heading(\"Output:\"),\n\t            raw_function_interface(output),\n\t        )\n\t    return div(\n\t        style(\".chatlab-chat-details summary > *  { display: inline; color: #27374D; }\"),\n\t        details(\n\t            summary(\n\t                function_logo(),\n", "                function_verbage(verbage),\n\t                inline_pre(name),\n\t                # If not finished, show \"...\", otherwise show nothing\n\t                inline_pre(\"...\" if not finished else \"\"),\n\t                style=dict(cursor=\"pointer\", color=colors[\"darkest\"]),\n\t            ),\n\t            div(\n\t                input_element,\n\t                output_element,\n\t                style=dict(\n", "                    # Need some space above to separate from the summary\n\t                    marginTop=\"10px\",\n\t                    marginLeft=\"10px\",\n\t                ),\n\t            ),\n\t            className=\"chatlab-chat-details\",\n\t            style=dict(\n\t                background=colors[\"lightest\"],\n\t                padding=\".5rem 1rem\",\n\t                borderRadius=\"5px\",\n", "            ),\n\t        ),\n\t    )\n"]}
{"filename": "chatlab/views/assistant.py", "chunked_list": ["\"\"\"Views for the buffers.\"\"\"\n\tfrom ..messaging import assistant\n\tfrom .abstracts import BufferView\n\tfrom .markdown import Markdown\n\tclass AssistantMessageView(BufferView):\n\t    \"\"\"A view for the assistant's message.\"\"\"\n\t    buffer: Markdown\n\t    def create_buffer(self, content: str = \"\") -> Markdown:\n\t        \"\"\"Creates the specific buffer for the view.\"\"\"\n\t        return Markdown(content)\n", "    def get_message(self):\n\t        \"\"\"Returns the crafted message.\"\"\"\n\t        return assistant(self.content)\n"]}
{"filename": "chatlab/views/abstracts.py", "chunked_list": ["\"\"\"Abstract classes for buffers.\"\"\"\n\timport os\n\tfrom abc import ABC, abstractmethod\n\tfrom binascii import hexlify\n\tfrom IPython.core import display_functions\n\tfrom ..messaging import Message\n\tclass AutoDisplayer(ABC):\n\t    \"\"\"Simple auto displayer.\"\"\"\n\t    _display_id: str\n\t    def __init__(self):\n", "        \"\"\"Initialize a `AutoDisplayer` object.\"\"\"\n\t        self._display_id = self.generate_display_id()\n\t    def generate_display_id(self) -> str:\n\t        \"\"\"Generate a display ID.\"\"\"\n\t        return hexlify(os.urandom(8)).decode('ascii')\n\t    def display(self):\n\t        \"\"\"Display the object with a display ID to allow updating.\"\"\"\n\t        display_functions.display(self, display_id=self._display_id)\n\t    def update_displays(self) -> None:\n\t        \"\"\"Force an update to all displays of this object.\"\"\"\n", "        display_functions.display(self, display_id=self._display_id, update=True)\n\tclass BufferInterface(AutoDisplayer):\n\t    \"\"\"Interface for buffers.\"\"\"\n\t    @abstractmethod\n\t    def append(self, delta: str):\n\t        \"\"\"Append a string to the buffer.\"\"\"\n\t        pass\n\t    @property\n\t    @abstractmethod\n\t    def content(self) -> str:\n", "        \"\"\"Return the buffer content.\"\"\"\n\t        pass\n\tclass BufferView(ABC):\n\t    \"\"\"A generic buffer view.\"\"\"\n\t    buffer: BufferInterface\n\t    def __init__(self, content: str = \"\"):\n\t        \"\"\"Initialize a `BufferView` object with optional starting content.\"\"\"\n\t        self.buffer = self.create_buffer(content)\n\t        self.active = False\n\t    @abstractmethod\n", "    def create_buffer(self, content: str = \"\") -> BufferInterface:\n\t        \"\"\"Creates the specific buffer for the view. To be overridden in subclasses.\"\"\"\n\t        pass\n\t    def display(self):\n\t        \"\"\"Create an updating display for the message.\"\"\"\n\t        if not self.active:\n\t            self.buffer.display()\n\t        self.active = True\n\t    def append(self, delta: str):\n\t        \"\"\"Append a string to the message.\"\"\"\n", "        self.display()\n\t        self.buffer.append(delta)\n\t    @property\n\t    def content(self):\n\t        \"\"\"Returns the raw content.\"\"\"\n\t        return self.buffer.content\n\t    def is_empty(self):\n\t        \"\"\"Returns True if the message is empty, False otherwise.\"\"\"\n\t        return self.content.strip() == \"\"\n\t    def in_progress(self):\n", "        \"\"\"Returns True if the message is in progress, False otherwise.\"\"\"\n\t        return self.active and not self.is_empty()\n\t    @abstractmethod\n\t    def get_message(self) -> Message:\n\t        \"\"\"Returns the crafted message. To be overridden in subclasses.\"\"\"\n\t        pass\n\t    def flush(self):\n\t        \"\"\"Flushes the message buffer.\"\"\"\n\t        self.buffer = self.create_buffer()\n\t        self.active = False\n", "        return self.get_message()\n\t    def _ipython_display_(self):\n\t        \"\"\"Display the buffer.\"\"\"\n\t        self.buffer.display()\n\t        self.active = True\n"]}
{"filename": "chatlab/views/__init__.py", "chunked_list": ["\"\"\"Views for ChatLab.\"\"\"\n\tfrom .assistant import AssistantMessageView\n\tfrom .assistant_function_call import AssistantFunctionCallView\n\tfrom .markdown import Markdown\n\t__all__ = [\n\t    \"AssistantMessageView\",\n\t    \"AssistantFunctionCallView\",\n\t    \"Markdown\",\n\t]\n"]}
{"filename": "chatlab/views/markdown.py", "chunked_list": ["\"\"\"An enhanced updateable Markdown display for use in Notebooks.\n\t* The `Markdown` display updates in place while content is appended to it\n\t* The `Markdown` display can be updated from an iterator\n\t\"\"\"\n\tfrom typing import Any, Dict, Tuple, Union\n\tfrom .abstracts import BufferInterface\n\tclass Markdown(BufferInterface):\n\t    \"\"\"A class for displaying a markdown string that can be updated in place.\n\t    This class provides an easy way to create and update a Markdown string in Jupyter Notebooks. It\n\t    supports real-time updates of Markdown content which is useful for emitting ChatGPT suggestions\n", "    as they are generated.\n\t    Attributes:\n\t        message (str): The Markdown string to display\n\t    Example:\n\t        >>> from chatlab import Markdown\n\t        ...\n\t        >>> markdown = Markdown()\n\t        >>> markdown.append(\"Hello\")\n\t        >>> markdown.append(\" world!\")\n\t        >>> markdown.display()\n", "        ```markdown\n\t        Hello world!\n\t        ```\n\t        >>> markdown.append(\" This is an update!\")\n\t        ```markdown\n\t        Hello world! This is an update!\n\t        ```\n\t        >>> def text_generator():\n\t        ...    yield \" 1\"\n\t        ...    yield \" 2\"\n", "        ...    yield \" 3\"\n\t        ...\n\t        >>> markdown.extend(text_generator())\n\t        ```markdown\n\t        Hello world! This is an update! 1 2 3\n\t        ```\n\t    \"\"\"\n\t    def __init__(self, content: str = \"\") -> None:\n\t        \"\"\"Initialize a `Markdown` object with an optional message.\"\"\"\n\t        self._content: str = content\n", "        super().__init__()\n\t    def append(self, delta: str) -> None:\n\t        \"\"\"Append a string to the `Markdown`.\"\"\"\n\t        self.content += delta\n\t    @property\n\t    def metadata(self) -> Dict[str, Any]:\n\t        \"\"\"Return the metadata for the `Markdown`.\"\"\"\n\t        return {\n\t            \"chatlab\": {\n\t                \"default\": True,\n", "            }\n\t        }\n\t    def __repr__(self) -> str:\n\t        \"\"\"Provide a plaintext version of the `Markdown`.\"\"\"\n\t        content = self._content\n\t        if content is None or content == \"\":\n\t            content = \" \"\n\t        return content\n\t    def _repr_markdown_(self) -> Union[str, Tuple[str, Dict[str, Any]]]:\n\t        \"\"\"Emit our markdown with metadata.\"\"\"\n", "        content = self._content\n\t        # Handle some platforms that don't support empty Markdown\n\t        if content is None or content == \"\":\n\t            content = \" \"\n\t        return content, self.metadata\n\t    @property\n\t    def message(self) -> str:\n\t        \"\"\"Return the `Markdown` content. Deprecated.\"\"\"\n\t        return self._content\n\t    @message.setter\n", "    def message(self, value: str) -> None:\n\t        self._content = value\n\t        self.update_displays()\n\t    @property\n\t    def content(self) -> str:\n\t        \"\"\"Return the `Markdown` content.\"\"\"\n\t        return self._content\n\t    @content.setter\n\t    def content(self, value: str) -> None:\n\t        self._content = value\n", "        self.update_displays()\n"]}
{"filename": "chatlab/views/assistant_function_call.py", "chunked_list": ["\"\"\"Views for the buffers.\"\"\"\n\tfrom typing import TypedDict\n\tfrom ..messaging import assistant_function_call\n\tfrom .abstracts import BufferView\n\tfrom .argument_buffer import ArgumentBuffer\n\tAssistantFunctionCallDict = TypedDict(\n\t    \"AssistantFunctionCallDict\",\n\t    {\n\t        \"function_name\": str,\n\t        \"function_arguments\": str,\n", "        \"display_id\": str,\n\t    },\n\t)\n\tclass AssistantFunctionCallView(BufferView):\n\t    \"\"\"A view for the assistant's message.\"\"\"\n\t    buffer: ArgumentBuffer\n\t    def __init__(self, function_name: str):\n\t        \"\"\"Initialize a `AssistantFunctionCallView` object with an optional message.\"\"\"\n\t        self.__function_name = function_name\n\t        super().__init__()\n", "    def create_buffer(self, content: str = \"\") -> ArgumentBuffer:\n\t        \"\"\"Creates the specific buffer for the view.\"\"\"\n\t        return ArgumentBuffer(self.__function_name, content)\n\t    def get_message(self):\n\t        \"\"\"Returns the crafted message.\"\"\"\n\t        return assistant_function_call(self.__function_name, self.content)\n\t    def finalize(self) -> AssistantFunctionCallDict:\n\t        \"\"\"Finalize the buffering.\"\"\"\n\t        return {\n\t            \"function_name\": self.__function_name,\n", "            \"function_arguments\": self.content,\n\t            \"display_id\": self.buffer._display_id,\n\t        }\n"]}
{"filename": "chatlab/views/argument_buffer.py", "chunked_list": ["\"\"\"TODO: DOCSTRING.\"\"\"\n\tfrom chatlab.components.function_details import ChatFunctionComponent\n\tfrom .abstracts import BufferInterface\n\tclass ArgumentBuffer(BufferInterface):\n\t    \"\"\"A class for displaying arguments that update in place.\n\t    This version only supports updating the arguments, not the function name.\n\t    So far, OpenAI will only stream the arguments.\n\t    \"\"\"\n\t    __function_name: str\n\t    __function_arguments: str\n", "    __state: str = \"Generating\"\n\t    def __init__(self, function_name: str, function_arguments: str = \"\"):\n\t        \"\"\"Initialize a `ArgumentBuffer` object with an optional message.\"\"\"\n\t        self.__function_name = function_name\n\t        self.__function_arguments = function_arguments\n\t        super().__init__()\n\t    @property\n\t    def content(self) -> str:\n\t        \"\"\"Return the current arguments.\"\"\"\n\t        return self.__function_arguments\n", "    def append(self, delta: str) -> None:\n\t        \"\"\"Append to the arguments.\"\"\"\n\t        self.__function_arguments += delta\n\t        self.update_displays()\n\t    def _repr_mimebundle_(self, include=None, exclude=None):\n\t        vdom_component = ChatFunctionComponent(\n\t            name=self.__function_name,\n\t            verbage=self.__state,\n\t            input=self.__function_arguments,\n\t        )\n", "        return {\n\t            \"text/html\": vdom_component.to_html(),\n\t            \"application/vdom.v1+json\": vdom_component.to_dict(),\n\t        }\n"]}
{"filename": "chatlab/builtins/noteable.py", "chunked_list": ["\"\"\"A built-in for interacting with Noteable notebooks.\"\"\"\n\timport logging\n\timport os\n\timport uuid\n\tfrom typing import Optional\n\timport httpx\n\timport orjson\n\timport ulid\n\tfrom origami.clients.api import APIClient, RTUClient\n\tfrom origami.models.api.outputs import KernelOutput, KernelOutputContent\n", "from origami.models.kernels import KernelSession\n\tfrom origami.models.notebook import CodeCell, MarkdownCell, make_sql_cell\n\tfrom ._mediatypes import formats_for_llm\n\tlogger = logging.getLogger(__name__)\n\tlogger.setLevel(logging.DEBUG)\n\tclass NotebookClient:\n\t    \"\"\"A notebook client for use with Noteable.\"\"\"\n\t    api_client: APIClient\n\t    rtu_client: Optional[RTUClient]\n\t    kernel_session: KernelSession\n", "    def __init__(self, api_client: APIClient, rtu_client: RTUClient, file_id: uuid.UUID, kernel_session: KernelSession):\n\t        \"\"\"Create a new NotebookClient based on an existing API and RTU client.\"\"\"\n\t        self.api_client = api_client\n\t        self.rtu_client = rtu_client\n\t        self.file_id = file_id\n\t        # NOTE: We have to track the kernel session for now, though we probably\n\t        # should be pulling this based on the file ID instead.\n\t        self.kernel_session = kernel_session\n\t    @property\n\t    def notebook_url(self):\n", "        \"\"\"Get the URL for the notebook.\"\"\"\n\t        # HACK: Assuming the deployment is on the same domain as the API server.\n\t        # True in most cases.\n\t        base_url = self.api_client.api_base_url.split(\"/gate\")[0]\n\t        return f\"{base_url}/f/{self.file_id}\"\n\t    @classmethod\n\t    async def connect(cls, file_id, token=None):\n\t        \"\"\"Connect to an existing notebook.\"\"\"\n\t        return await cls.create(file_id=file_id, token=token)\n\t    @classmethod\n", "    async def create(cls, file_name=None, token=None, file_id=None, project_id=None):\n\t        \"\"\"Create a new notebook.\"\"\"\n\t        if token is None:\n\t            token = os.environ.get(\"NOTEABLE_TOKEN\")\n\t            assert token is not None\n\t        logger.info(\"Setting up API Client\")\n\t        api_client = APIClient(authorization_token=token)\n\t        if file_id is None:\n\t            if project_id is None:\n\t                user_info = await api_client.user_info()\n", "                # We'll use the user's default project ID for the rest of this example\n\t                project_id = user_info.origamist_default_project_id\n\t                if project_id is None:\n\t                    raise Exception(\"User has no default project\")\n\t            if file_name is None:\n\t                file_name = \"Untitled.ipynb\"\n\t            file = await api_client.create_notebook(project_id, file_name)\n\t            file_id = file.id\n\t        # prepare our realtime client\n\t        logger.info(\"Setting up RTU\")\n", "        rtu_client = await api_client.connect_realtime(file_id)\n\t        logger.info(\"Launching kernel\")\n\t        # Launch the kernel for the notebook\n\t        # We have to track the kernel_session for now\n\t        kernel_session = await api_client.launch_kernel(file_id)\n\t        cn = NotebookClient(api_client, rtu_client, file_id=file_id, kernel_session=kernel_session)\n\t        return cn\n\t    async def get_or_create_rtu_client(self):\n\t        \"\"\"Get or create an RTU client.\"\"\"\n\t        if self.rtu_client is None:\n", "            self.rtu_client = await self.api_client.connect_realtime(self.file_id)\n\t        return self.rtu_client\n\t    async def wait_for_kernel_idle(self):\n\t        \"\"\"Wait for the kernel to be idle.\"\"\"\n\t        rtu_client = await self.get_or_create_rtu_client()\n\t        await rtu_client.wait_for_kernel_idle()\n\t    async def create_cell(\n\t        self,\n\t        source: str,\n\t        cell_id: Optional[str] = None,\n", "        and_run: bool = False,\n\t        cell_type: str = \"code\",\n\t        after_cell_id: Optional[str] = None,\n\t        db_connection: Optional[str] = None,\n\t        assign_results_to: Optional[str] = None,\n\t    ):\n\t        \"\"\"Create a code, markdown, or SQL cell.\"\"\"\n\t        rtu_client = await self.get_or_create_rtu_client()\n\t        if after_cell_id is None:\n\t            existing_cells = rtu_client.cell_ids\n", "            if existing_cells and len(existing_cells) > 0:\n\t                after_cell_id = existing_cells[-1]\n\t        if cell_id is None:\n\t            cell_id = str(ulid.ULID())\n\t        cell = None\n\t        if cell_type == \"markdown\":\n\t            cell = MarkdownCell(source=source, id=cell_id)\n\t        elif cell_type == \"code\":\n\t            cell = CodeCell(source=source, id=cell_id)\n\t        elif cell_type == \"sql\":\n", "            if db_connection is None:\n\t                return \"You must specify a db_connection for SQL cells.\"\n\t            # db connection has to start with `@`\n\t            if not db_connection.startswith(\"@\"):\n\t                db_connection = f\"@{db_connection}\"\n\t            cell = make_sql_cell(\n\t                source=source, cell_id=cell_id, db_connection=db_connection, assign_results_to=assign_results_to\n\t            )\n\t        if cell is None:\n\t            return f\"Unknown cell type {cell_type}. Valid types are: markdown, code, sql.\"\n", "        logger.info(f\"Adding cell {cell_id} to notebook\")\n\t        cell = await rtu_client.add_cell(cell=cell, after_id=after_cell_id)\n\t        logger.info(f\"Added cell {cell_id} to notebook\")\n\t        if cell.cell_type != \"code\" or not and_run:\n\t            return cell\n\t        try:\n\t            logger.info(\"Running cell\")\n\t            return await self.run_cell(cell.id)\n\t        except Exception as e:\n\t            return f\"Cell created successfully. An error happened during run: {e}\"\n", "    async def _get_llm_friendly_outputs(self, output_collection_id: uuid.UUID):\n\t        \"\"\"Get the outputs for a given output collection ID.\"\"\"\n\t        output_collection = await self.api_client.get_output_collection(output_collection_id)\n\t        outputs = output_collection.outputs\n\t        # Not *that* friendly, but it's a start.\n\t        llm_friendly_outputs = []\n\t        for output in outputs:\n\t            content = output.content\n\t            if content is None:\n\t                continue\n", "            friendly_output = await self._get_llm_friendly_output(output)\n\t            if friendly_output is not None:\n\t                llm_friendly_outputs.append(friendly_output)\n\t        return llm_friendly_outputs\n\t    async def _extract_llm_plain(self, output: KernelOutput):\n\t        resp = await self.api_client.client.get(f\"/outputs/{output.id}\", params={\"mimetype\": \"text/llm+plain\"})\n\t        resp.raise_for_status()\n\t        output_for_llm = KernelOutput.parse_obj(resp.json())\n\t        if output_for_llm.content is None:\n\t            return None\n", "        return output_for_llm.content.raw\n\t    async def _extract_specific_mediatype(self, output: KernelOutput, mimetype: str):\n\t        resp = await self.api_client.client.get(f\"/outputs/{output.id}\", params={\"mimetype\": mimetype})\n\t        resp.raise_for_status()\n\t        output_for_llm = KernelOutput.parse_obj(resp.json())\n\t        if output_for_llm.content is None:\n\t            return None\n\t        return output_for_llm.content.raw\n\t    async def _extract_error(self, content: KernelOutputContent):\n\t        orjson_content = None\n", "        if content.raw is not None:\n\t            orjson_content = orjson.loads(content.raw)\n\t        elif content.url is not None:\n\t            async with httpx.AsyncClient() as client:\n\t                resp = await client.get(content.url)\n\t                # If the response failed, return None\n\t                if resp.status_code != 200:\n\t                    return None\n\t                try:\n\t                    orjson_content = orjson.loads(resp.content)\n", "                except orjson.JSONDecodeError:\n\t                    return None\n\t        if orjson_content is None:\n\t            return None\n\t        return f\"Error: {orjson_content['ename']}: {orjson_content['evalue']}\"\n\t    async def _get_llm_friendly_output(self, output: KernelOutput):\n\t        \"\"\"Get the output for a given output.\"\"\"\n\t        content = output.content\n\t        if content is None:\n\t            return None\n", "        if output.type == \"error\":\n\t            return await self._extract_error(content)\n\t        if 'text/llm+plain' in output.available_mimetypes:\n\t            # Fetch the specialized LLM+Plain directly\n\t            result = await self._extract_llm_plain(output)\n\t            if result is not None:\n\t                return result\n\t        if content.mimetype == 'text/html':\n\t            result = await self._extract_specific_mediatype(output, 'text/plain')\n\t            if result is not None:\n", "                return result\n\t        if content.mimetype == 'application/vnd.dataresource+json':\n\t            # TODO: Bring back a smaller representation to allow the LLM to do analysis\n\t            return \"<!-- DataFrame shown in notebook that user can see -->\"\n\t        if content.mimetype == 'application/vnd.plotly.v1+json':\n\t            return \"<!-- Plotly shown in notebook that user can see -->\"\n\t        if content.url is not None:\n\t            return \"<!-- Large output too large for chat. It is available in the notebook that the user can see -->\"\n\t        if content.mimetype in formats_for_llm:\n\t            return content.raw\n", "        mimetypes: list[str] = output.available_mimetypes\n\t        for format in formats_for_llm:\n\t            if format in mimetypes:\n\t                resp = await self.api_client.client.get(f\"/outputs/{output.id}?mimetype={format}\")\n\t                resp.raise_for_status()\n\t                if resp.status_code == 200:\n\t                    return\n\t                next_best_output = KernelOutput.parse_obj(resp.json())\n\t                if next_best_output.content is None:\n\t                    continue\n", "                if next_best_output.content.raw is not None:\n\t                    return next_best_output.content.raw\n\t    async def run_cell(self, cell_id: str):\n\t        \"\"\"Run a Cell within a Notebook by ID.\"\"\"\n\t        # Queue up the execution\n\t        rtu_client = await self.get_or_create_rtu_client()\n\t        queued_executions = await rtu_client.queue_execution(cell_id)\n\t        cell = await list(queued_executions)[0]\n\t        if cell.output_collection_id is None:\n\t            # Hypothesis: if the output collection ID is None, we're in a bad\n", "            # state. When the LLM sees this cell they will think its fine.\n\t            return cell\n\t        output_collection_id = cell.output_collection_id\n\t        if isinstance(output_collection_id, str):\n\t            try:\n\t                output_collection_id = uuid.UUID(output_collection_id)\n\t            except ValueError:\n\t                logger.exception(\"Invalid UUID\", exc_info=True)\n\t                return cell\n\t        outputs = await self._get_llm_friendly_outputs(output_collection_id)\n", "        response = \"\"\n\t        if len(outputs) == 0:\n\t            return response + \"\\nNo output.\"\n\t        response += \"\\nOut:\"\n\t        for output in outputs:\n\t            response += \"\\n\" + str(output)\n\t        return response\n\t    async def get_datasources(self):\n\t        \"\"\"Get a list of databases, AKA datasources.\"\"\"\n\t        datasources = await self.api_client.get_datasources_for_notebook(self.file_id)\n", "        resp_text = \"Datasources:\\n\"\n\t        for datasource in datasources:\n\t            print(datasource.dict(exclude_unset=True, exclude_none=True))\n\t            resp_text += f\"## {datasource.name}\\n\"\n\t            resp_text += f\"{datasource.description}\\n\"\n\t            resp_text += f\"datasource_id: {datasource.sql_cell_handle}\\n\\n\"\n\t            resp_text += f\"Type: {datasource.type_id}\\n\\n\"\n\t        return resp_text\n\t    async def get_cell(self, cell_id: str, with_outputs: bool = False):\n\t        \"\"\"Get a cell by ID.\"\"\"\n", "        rtu_client = await self.get_or_create_rtu_client()\n\t        try:\n\t            _, cell = rtu_client.builder.get_cell(cell_id)\n\t        except KeyError:\n\t            return f\"Cell {cell_id} not found.\"\n\t        noteable_metadata = cell.metadata.get(\"noteable\", {})\n\t        if noteable_metadata.get(\"cell_type\") == \"sql\":\n\t            source_type = \"sql\"\n\t        extra = \"\"\n\t        assign_results_to = noteable_metadata.get(\"assign_results_to\")\n", "        db_connection = noteable_metadata.get(\"db_connection\")\n\t        if db_connection is not None:\n\t            extra += f\", db_connection: {db_connection}\"\n\t        if assign_results_to is not None:\n\t            extra += f\", assign_to: {assign_results_to}\"\n\t        response = f\"<!-- {cell.cell_type.title()} Cell, ID: {cell_id}{extra} -->\\n\"\n\t        if cell.cell_type != \"code\":\n\t            response += cell.source\n\t            return response\n\t        source_type = rtu_client.builder.nb.metadata.get(\"kernelspec\", {}).get(\"language\", \"\")\n", "        if cell.metadata.get(\"noteable\", {}).get(\"cell_type\") == \"sql\":\n\t            source_type = \"sql\"\n\t        # Convert to a plaintext response\n\t        response += f\"\\nIn:\\n\\n```{source_type}\\n\"\n\t        response += cell.source\n\t        response += \"\\n```\\n\"\n\t        if not with_outputs:\n\t            return response\n\t        output_collection_id = cell.output_collection_id\n\t        if isinstance(output_collection_id, str):\n", "            try:\n\t                output_collection_id = uuid.UUID(output_collection_id)\n\t            except ValueError:\n\t                # This is a case where the output collection ID in the notebook is invalid\n\t                logger.exception(\"Invalid UUID\", exc_info=True)\n\t                return response + \"\\nUnable to get outputs.\"\n\t        if output_collection_id is None:\n\t            return response + \"\\nNo output.\"\n\t        outputs = await self._get_llm_friendly_outputs(output_collection_id)\n\t        if len(outputs) == 0:\n", "            return response + \"\\nNo output.\"\n\t        response += \"\\nOut:\"\n\t        for output in outputs:\n\t            response += \"\\n\" + str(output)\n\t        return response\n\t    async def get_cell_ids(self):\n\t        \"\"\"Get a list of cell IDs.\"\"\"\n\t        rtu_client = await self.get_or_create_rtu_client()\n\t        return rtu_client.cell_ids\n\t    async def shutdown(self):\n", "        \"\"\"Shutdown the notebook.\"\"\"\n\t        if self.rtu_client is not None:\n\t            await self.rtu_client.shutdown()\n\t        await self.api_client.shutdown_kernel(self.kernel_session.id)\n\t        self.rtu_client = None\n\t    \"\"\"This `python` function is here for dealing with ChatGPT's `python` hallucination.\"\"\"\n\t    async def python(self, code: str):\n\t        \"\"\"Creates a python cell, runs it, and returns output.\"\"\"\n\t        return await self.create_cell(code, and_run=True)\n\t    @property\n", "    def chat_functions(self):\n\t        \"\"\"Functions to expose for LLMs.\"\"\"\n\t        return [\n\t            self.create_cell,\n\t            self.run_cell,\n\t            self.get_cell,\n\t            self.get_cell_ids,\n\t            self.get_datasources,\n\t        ]\n\t__all__ = [\"NotebookClient\"]\n", "# Only expose the NotebookClient to tab completion\n\tdef __dir__():\n\t    return __all__\n"]}
{"filename": "chatlab/builtins/_mediatypes.py", "chunked_list": ["\"\"\"Media types for rich output for LLMs and in-notebook.\"\"\"\n\timport json\n\tfrom typing import Optional\n\tfrom IPython.display import display\n\tfrom IPython.utils.capture import RichOutput\n\t# Prioritized formats to show to large language models\n\tformats_for_llm = [\n\t    # Repr LLM is the richest text\n\t    'text/llm+plain',\n\t    # Assume that if we get markdown we know it's rich for an LLM\n", "    'text/markdown',\n\t    # Same with LaTeX\n\t    'text/latex',\n\t    # All the normal ones\n\t    'application/vnd.jupyter.error+json',\n\t    # 'application/vdom.v1+json',\n\t    'application/json',\n\t    # Since every object has a text/plain repr, even though the LLM would understand `text/plain` well,\n\t    # bumping this priority up would override more rich media types we definitely want to show.\n\t    'text/plain',\n", "    # Both HTML and SVG should be conditional on size, considering many libraries\n\t    # Will emit giant JavaScript blobs for interactivity\n\t    # For now, we'll assume not to show these\n\t    # 'text/html',\n\t    # 'image/svg+xml',\n\t]\n\t# Prioritized formats to redisplay for the user, since we capture the output during execution\n\tformats_to_redisplay = [\n\t    'application/vnd.jupyter.widget-view+json',\n\t    'application/vnd.dex.v1+json',\n", "    'application/vnd.dataresource+json',\n\t    'application/vnd.plotly.v1+json',\n\t    'text/vnd.plotly.v1+html',\n\t    'application/vdom.v1+json',\n\t    'application/json',\n\t    'application/javascript',\n\t    'image/svg+xml',\n\t    'image/png',\n\t    'image/jpeg',\n\t    'image/gif',\n", "    'text/html',\n\t    'image/svg+xml',\n\t]\n\tdef redisplay_superrich(output: RichOutput):\n\t    \"\"\"Redisplay an image.\"\"\"\n\t    data = output.data\n\t    metadata = output.metadata\n\t    richest_format = find_richest_format(data, formats_to_redisplay)\n\t    if richest_format:\n\t        display(\n", "            data,\n\t            raw=True,\n\t        )\n\t        data.pop(richest_format, None)\n\t        metadata.pop(richest_format, None)\n\t        # Check to see if it already has a text/llm+plain representation\n\t        if 'text/llm+plain' in data:\n\t            return\n\t        if richest_format.startswith('image/'):\n\t            # Allow the LLM to see that we displayed for the user\n", "            data['text/llm+plain'] = data['text/plain']\n\t        else:\n\t            data['text/llm+plain'] = f\"<Displayed {richest_format}>\"\n\tdef pluck_richest_text(output: RichOutput):\n\t    \"\"\"Format an object as rich text.\"\"\"\n\t    data = output.data\n\t    metadata = output.metadata\n\t    richest_format = find_richest_format(data, formats_for_llm)\n\t    if richest_format:\n\t        d = data.pop(richest_format, None)\n", "        m = metadata.pop(richest_format, None)\n\t        if isinstance(d, dict):\n\t            d = json.dumps(d, indent=2)\n\t        # TODO: Reduce the size of the data if it's too big for LLMs.\n\t        return d, m\n\t    return None, {}\n\tdef find_richest_format(payload: dict, formats: list[str]) -> Optional[str]:\n\t    for format in formats:\n\t        if format in payload:\n\t            return format\n", "    return None\n"]}
{"filename": "chatlab/builtins/files.py", "chunked_list": ["\"\"\"These are all functions for file operations to expose to a Large Language Model.\n\t⚠️ ☢️ WARNING ☢️ ⚠️\n\tThe model will do useful things and the model will do destructive things.\n\tGit and Docker can be your friends when you let a probalistc model loose on your filesystem.\n\tYou've been warned. Have fun and be safe!\n\t\"\"\"\n\timport asyncio\n\timport os\n\timport aiofiles\n\tfrom chatlab.decorators import expose_exception_to_llm\n", "@expose_exception_to_llm\n\tasync def list_files(directory: str) -> list[str]:\n\t    \"\"\"List all files in the given directory.\n\t    Args:\n\t    - directory: str, the directory to list files from\n\t    Returns:\n\t    - list[str]: the list of files in the given directory\n\t    \"\"\"\n\t    files = await asyncio.to_thread(os.listdir, directory)\n\t    return files\n", "@expose_exception_to_llm\n\tasync def get_file_size(file_path: str) -> int:\n\t    \"\"\"Get the size of the file in bytes asynchronously.\n\t    Args:\n\t    - file_path: The path to the file\n\t    Returns:\n\t    - The size of the file in bytes\n\t    \"\"\"\n\t    size = await asyncio.to_thread(os.path.getsize, file_path)\n\t    return size\n", "@expose_exception_to_llm\n\tasync def is_file(file_path: str) -> bool:\n\t    \"\"\"Check if the given path points to a file asynchronously.\n\t    Args:\n\t    - file_path: The path to check\n\t    Returns:\n\t    - True if the path points to a file, False otherwise\n\t    \"\"\"\n\t    is_file = await asyncio.to_thread(os.path.isfile, file_path)\n\t    return is_file\n", "@expose_exception_to_llm\n\tasync def write_file(file_path: str, content: str, mode: str = 'w') -> None:\n\t    \"\"\"Write content to a file.\n\t    Args:\n\t    - file_path: The path to the file\n\t    - content: The content to be written\n\t    - mode: The writing mode, default is 'w' for write\n\t    Returns:\n\t    - None\n\t    \"\"\"\n", "    async with aiofiles.open(file_path, mode) as file:  # type: ignore\n\t        await file.write(content)\n\t@expose_exception_to_llm\n\tasync def read_file(file_path: str, mode: str = 'r') -> str:\n\t    \"\"\"Read content from a file.\n\t    Args:\n\t    - file_path: The path to the file\n\t    - mode: The reading mode, default is 'r' for read\n\t    Returns:\n\t    - str: The content of the file\n", "    \"\"\"\n\t    async with aiofiles.open(file_path, mode) as file:  # type: ignore\n\t        content = await file.read()\n\t    return content\n\t@expose_exception_to_llm\n\tasync def is_directory(directory: str) -> bool:\n\t    \"\"\"Check if the given path points to a directory asynchronously.\n\t    Args:\n\t    - directory: The path to check\n\t    Returns:\n", "    - True if the path points to a directory, False otherwise\n\t    \"\"\"\n\t    is_directory = await asyncio.to_thread(os.path.isdir, directory)\n\t    return is_directory\n\tchat_functions = [list_files, get_file_size, is_file, is_directory, write_file, read_file]\n"]}
{"filename": "chatlab/builtins/__init__.py", "chunked_list": ["\"\"\"Builtins for ChatLab.\"\"\"\n\tfrom deprecation import deprecated\n\tfrom .files import chat_functions as file_functions\n\tfrom .python import run_python\n\tfrom .shell import chat_functions as shell_functions\n\t# To prevent naming confusion, the builtin that isn't really running a cell\n\t# is deprecated.\n\trun_cell = deprecated(\n\t    deprecated_in=\"1.0.0\",\n\t    removed_in=\"2.0.0\",\n", "    details=\"run_cell is deprecated. Use run_python instead for same-session execution.\",\n\t)(run_python)\n\t# compose all the file, shell, and python functions into one list for ease of use\n\tos_functions = file_functions + shell_functions + [run_python]\n\t__all__ = [\"run_python\", \"run_cell\", \"file_functions\", \"shell_functions\", \"os_functions\"]\n"]}
{"filename": "chatlab/builtins/python.py", "chunked_list": ["\"\"\"The in-IPython python code runner for ChatLab.\"\"\"\n\tfrom traceback import TracebackException\n\tfrom typing import Optional\n\tfrom IPython.core.interactiveshell import InteractiveShell\n\tfrom IPython.utils.capture import capture_output\n\tfrom repr_llm import register_llm_formatter\n\tfrom repr_llm.pandas import format_dataframe_for_llm, format_series_for_llm\n\tfrom chatlab.decorators import expose_exception_to_llm\n\tfrom ._mediatypes import pluck_richest_text, redisplay_superrich\n\tdef apply_llm_formatter(shell: InteractiveShell):\n", "    \"\"\"Apply the LLM formatter to the given shell.\"\"\"\n\t    llm_formatter = register_llm_formatter(shell)\n\t    llm_formatter.for_type_by_name('pandas.core.frame', 'DataFrame', format_dataframe_for_llm)\n\t    llm_formatter.for_type_by_name('pandas.core.series', 'Series', format_series_for_llm)\n\tdef get_or_create_ipython() -> InteractiveShell:\n\t    \"\"\"Get the current IPython shell or create a new one.\"\"\"\n\t    shell = None\n\t    # This is what `get_ipython` does. For type inference to work, we need to\n\t    # do it manually.\n\t    if InteractiveShell.initialized():\n", "        shell = InteractiveShell.instance()\n\t        apply_llm_formatter(shell)\n\t    if not shell:\n\t        shell = InteractiveShell()\n\t        apply_llm_formatter(shell)\n\t    return shell\n\tclass ChatLabShell:\n\t    \"\"\"A custom shell for ChatLab that uses the current IPython shell and formats outputs for LLMs.\"\"\"\n\t    shell: InteractiveShell\n\t    def __init__(self, shell: Optional[InteractiveShell] = None):\n", "        \"\"\"Create a new ChatLabShell.\"\"\"\n\t        self.shell = get_or_create_ipython()\n\t    def run_cell(self, code: str):\n\t        \"\"\"Execute code in python and return the result.\"\"\"\n\t        try:\n\t            # Since we include the traceback inside the ChatLab display, we\n\t            # don't want to show it inline.\n\t            # Sadly `capture_output` doesn't grab the show traceback side effect,\n\t            # so we have to do it manually.\n\t            original_showtraceback = self.shell.showtraceback\n", "            with capture_output() as captured:\n\t                # HACK: don't show the exception inline if the LLM is running it\n\t                self.shell.showtraceback = lambda *args, **kwargs: None  # type: ignore\n\t                result = self.shell.run_cell(code)\n\t                self.shell.showtraceback = original_showtraceback  # type: ignore\n\t        except Exception as e:\n\t            self.shell.showtraceback = original_showtraceback  # type: ignore\n\t            formatted = TracebackException.from_exception(e, limit=3).format(chain=True)\n\t            plaintext_traceback = '\\n'.join(formatted)\n\t            return plaintext_traceback\n", "        if not result.success:\n\t            # Grab which exception was raised\n\t            exception = result.error_before_exec or result.error_in_exec\n\t            # If success was False and yet neither of these are set, then\n\t            # something went wrong in the IPython internals\n\t            if exception is None:\n\t                raise Exception(\"Unknown IPython error for result\", result)\n\t            # Create a formatted traceback that includes the last 3 frames\n\t            # and the exception message\n\t            formatted = TracebackException.from_exception(exception, limit=3).format(chain=True)\n", "            plaintext_traceback = '\\n'.join(formatted)\n\t            return plaintext_traceback\n\t        outputs = \"\"\n\t        if captured.stdout is not None and captured.stdout.strip() != '':\n\t            stdout = captured.stdout\n\t            # Truncate stdout if it's too long\n\t            if len(stdout) > 1000:\n\t                stdout = stdout[:500] + '...[TRUNCATED]...' + stdout[-500:]\n\t            outputs += f\"STDOUT:\\n{stdout}\\n\\n\"\n\t        if captured.stderr is not None and captured.stderr.strip() != '':\n", "            stderr = captured.stderr\n\t            if len(stderr) > 1000:\n\t                stdout = stderr[:500] + '...[TRUNCATED]...' + stderr[-500:]\n\t            outputs += f\"STDERR:\\n{stderr}\\n\\n\"\n\t        if captured.outputs is not None:\n\t            for output in captured.outputs:\n\t                # If image/* are in the output, redisplay it\n\t                # then include a text/plain version of the object, telling the llm\n\t                # that the image is displayed for the user\n\t                redisplay_superrich(output)\n", "                # Now for text for the llm\n\t                text, _ = pluck_richest_text(output)\n\t                if text is None:\n\t                    continue\n\t                outputs += f\"OUTPUT:\\n{text}\\n\\n\"\n\t        if result.result is not None:\n\t            output = result.result\n\t            # If image/* are in the output, redisplay it\n\t            # then include a text/plain version of the object, telling the llm\n\t            # that the image is displayed for the user\n", "            redisplay_superrich(result.result)\n\t            # Now for text for the llm\n\t            text, _ = pluck_richest_text(result.result)\n\t            if text is not None:\n\t                outputs += f\"RESULT:\\n{text}\\n\\n\"\n\t        return outputs\n\t__shell: Optional[ChatLabShell] = None\n\t@expose_exception_to_llm\n\tdef run_python(code: str):\n\t    \"\"\"Execute code in python and return the result.\"\"\"\n", "    global __shell\n\t    if __shell is None:\n\t        # Since ChatLabShell has imports that are \"costly\" (e.g. IPython, numpy, pandas),\n\t        # we only import it on the first call to run_cell.\n\t        from .python import ChatLabShell\n\t        __shell = ChatLabShell()\n\t    return __shell.run_cell(code)\n\t__all__ = [\"run_python\", \"ChatLabShell\"]\n\tdef __dir__():\n\t    return __all__\n"]}
{"filename": "chatlab/builtins/shell.py", "chunked_list": ["\"\"\"Shell commands for ChatLab.\"\"\"\n\timport asyncio\n\timport subprocess\n\tfrom chatlab.decorators import expose_exception_to_llm\n\t@expose_exception_to_llm\n\tasync def run_shell_command(command: str):\n\t    \"\"\"Run a shell command and return the output.\n\t    Args:\n\t    - command: str, the shell command to execute\n\t    Returns:\n", "    - str: the output of the shell command\n\t    \"\"\"\n\t    process = await asyncio.create_subprocess_shell(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t    stdout, stderr = await process.communicate()\n\t    resp = f\"Return Code: {process.returncode}\\n\"\n\t    resp += f\"stdout: ```\\n{stdout.decode().strip()}\\n```\\n\"\n\t    resp += f\"stderr: ```\\n{stderr.decode().strip()}\\n```\"\n\t    return resp\n\tchat_functions = [run_shell_command]\n"]}
