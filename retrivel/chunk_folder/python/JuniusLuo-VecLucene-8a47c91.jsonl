{"filename": "main.py", "chunked_list": ["import argparse\n\tfrom server.server import start\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n\t    parser.add_argument(\"--port\", type=int, default=8080)\n\t    args = parser.parse_args()\n\t    start(host=args.host, port=args.port)\n"]}
{"filename": "index/docs.py", "chunked_list": ["from pydantic import BaseModel\n\tfrom typing import List, Optional\n\t# TODO support more attributes, such as tokenize, DocValuesType, etc.\n\tclass DocField(BaseModel):\n\t    name: str # field name\n\t    string_value: Optional[str] = None\n\t    numeric_value: Optional[int] = None\n\t    float_value: Optional[float] = None\n\t# All doc fields except the doc text. There are two reserved fields:\n\t# 1. The reserved field name for the doc text, \"doc_text\", defined by\n", "# index.FIELD_DOC_TEXT. This field name should not be used by application.\n\t# 2. The reserved field name for the doc id, \"doc_id\", defined by\n\t# index.FIELD_DOC_ID. If doc_id is not specified in the request, server will\n\t# automatically generate a unique id for the doc.\n\t#\n\t# For now, all fields are stored and not indexed. Only the doc contents are\n\t# indexed and also stored. TODO allow indexing more fields, such as title.\n\tclass DocFields(BaseModel):\n\t    fields: List[DocField]\n\tclass DocChunkScore(BaseModel):\n", "    doc_id: str\n\t    offset: int\n\t    length: int\n\t    score: float\n"]}
{"filename": "index/vector_index.py", "chunked_list": ["import os\n\timport logging\n\timport pickle\n\tfrom pydantic import BaseModel\n\tfrom typing import List, Dict\n\timport tiktoken\n\tfrom index.docs import DocChunkScore\n\tfrom model.model import Model\n\tfrom model.factory import get_model\n\tfrom vectorstore.vectorstore import Space, VectorStore\n", "from vectorstore.factory import get_vector_store\n\tDEFAULT_SPACE = Space.l2 # The default space\n\tDEFAULT_VECTOR_FILE_MAX_ELEMENTS = 5000 # The max elements in one vector file\n\tMIN_CHUNK_SIZE_CHARS = 350  # The minimum size of each text chunk in characters\n\tMIN_CHUNK_LENGTH_TO_EMBED = 5  # Discard chunks shorter than this\n\tEMBEDDINGS_BATCH_SIZE = 128 # The number of embeddings to request at a time\n\t# Global tokenizer\n\tdefault_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\t# The metadata for a document chunk\n\tclass ChunkMetadata(BaseModel):\n", "    offset: int # the chunk's start offset in the doc\n\t    length: int # the length of the chunk text\n\t    label: int # the label of the chunk embedding in the vector store\n\t# The id to uniquely define a document chunk in the vector index\n\tclass ChunkId(BaseModel):\n\t    doc_id: str\n\t    offset: int\n\t    length: int\n\tclass VectorIndexMetadata(BaseModel):\n\t    elements: int\n", "    last_label: int\n\tclass VectorIndex:\n\t    store_dir: str # the dir where the index files will be stored\n\t    # use openai cl100k_base as tokenizer.\n\t    tokenizer: tiktoken.core.Encoding\n\t    model: Model # the model to get the embeddings for text\n\t    space: Space\n\t    store: VectorStore # the vector store to store the embeddings\n\t    # the underline vector store usually supports int as label. maintain the\n\t    # mapping between the doc ids and labels. These metadatas are directly\n", "    # persisted using pickle.\n\t    # TODO support the metadata definition change, maybe use other\n\t    # serialization format, such as json, protobuf, etc.\n\t    # key: doc id, value: a list of chunk metadata\n\t    doc_id_to_metas: Dict[str, List[ChunkMetadata]]\n\t    # key: label, value: chunk id\n\t    label_to_chunk_id: Dict[int, ChunkId]\n\t    # the vector index metadata\n\t    metadata: VectorIndexMetadata\n\t    def __init__(\n", "        self,\n\t        store_dir: str,\n\t        model_provider:str,\n\t        vector_store: str,\n\t    ):\n\t        self.store_dir = store_dir\n\t        self.tokenizer = default_tokenizer\n\t        self.model = get_model(model_provider)\n\t        dim = self.model.get_dim()\n\t        # default max elements. TODO support more elements\n", "        max_elements = DEFAULT_VECTOR_FILE_MAX_ELEMENTS\n\t        self.space = DEFAULT_SPACE\n\t        self.store = get_vector_store(\n\t            vector_store, dim, self.space, max_elements)\n\t        self.doc_id_to_metas = {}\n\t        self.label_to_chunk_id ={}\n\t        self.metadata = VectorIndexMetadata(elements=0, last_label=0)\n\t    def load(self, version: int):\n\t        \"\"\"\n\t        Load the vectors from file.\n", "        \"\"\"\n\t        file_path = self._get_index_file(version)\n\t        self.store.load(file_path)\n\t        # load the mapping between doc ids and labels\n\t        id_file = self._get_id_to_label_file(version)\n\t        with open(id_file, \"rb\") as f:\n\t            self.doc_id_to_metas = pickle.load(f)\n\t        label_file = self._get_label_to_id_file(version)\n\t        with open(label_file, \"rb\") as f:\n\t            self.label_to_chunk_id = pickle.load(f)\n", "        metadata_file = self._get_metadata_file(version)\n\t        with open(metadata_file, \"rb\") as f:\n\t            self.metadata = pickle.load(f)\n\t    def save(self, version: int):\n\t        \"\"\"\n\t        Save the vectors to the file.\n\t        \"\"\"\n\t        file_path = self._get_index_file(version)\n\t        self.store.save(file_path)\n\t        # save the mapping between doc ids and labels\n", "        id_file = self._get_id_to_label_file(version)\n\t        with open(id_file, \"wb\") as f:\n\t            pickle.dump(self.doc_id_to_metas, f, pickle.HIGHEST_PROTOCOL)\n\t        label_file = self._get_label_to_id_file(version)\n\t        with open(label_file, \"wb\") as f:\n\t            pickle.dump(self.label_to_chunk_id, f, pickle.HIGHEST_PROTOCOL)\n\t        metadata_file = self._get_metadata_file(version)\n\t        with open(metadata_file, \"wb\") as f:\n\t            pickle.dump(self.metadata, f, pickle.HIGHEST_PROTOCOL)\n\t    def _get_index_file(self, version: int) -> str:\n", "        return os.path.join(self.store_dir, f\"{version}.index\")\n\t    def _get_id_to_label_file(self, version: int) -> str:\n\t        return os.path.join(self.store_dir, f\"id_to_label_{version}.pkl\")\n\t    def _get_label_to_id_file(self, version: int) -> str:\n\t        return os.path.join(self.store_dir, f\"label_to_id_{version}.pkl\")\n\t    def _get_metadata_file(self, version: int) -> str:\n\t        return os.path.join(self.store_dir, f\"metadata_{version}.pkl\")\n\t    def _get_chunk_id(self, doc_id: str, offset: int, length: int) -> str:\n\t        return f\"{doc_id}_{offset}_{length}\"\n\t    def add(self, doc_path: str, doc_id: str):\n", "        \"\"\"\n\t        Add a doc to the vector index. This function reads the doc text, splits\n\t        the doc to chunk if the doc is large, generates the embeddings for\n\t        chunks and adds the embeddings to the vector store.\n\t        TODO support multi-threads.\n\t        \"\"\"\n\t        # get embeddings for the doc text\n\t        chunk_embeddings, chunk_metas = self._get_embeddings(doc_path)\n\t        logging.info(\n\t            f\"get {len(chunk_embeddings)} embeddings for doc path={doc_path} \"\n", "            f\"id={doc_id}, last_label={self.metadata.last_label}\")\n\t        if len(chunk_embeddings) == 0:\n\t            # doc has no content, return\n\t            return\n\t        # assign the labels to the doc chunks\n\t        label = self.metadata.last_label\n\t        # update index metadata\n\t        self.metadata.last_label += len(chunk_metas)\n\t        self.metadata.elements += len(chunk_metas)\n\t        labels: List[int] = []\n", "        for i, chunk_meta in enumerate(chunk_metas):\n\t            label += 1\n\t            chunk_meta.label = label\n\t            labels.append(label)\n\t            # update the label_to_chunk_id Dict\n\t            self.label_to_chunk_id[label] = ChunkId(\n\t                doc_id=doc_id,\n\t                offset=chunk_meta.offset,\n\t                length=chunk_meta.length,\n\t            )\n", "        # update the doc_id_to_metas\n\t        self.doc_id_to_metas[doc_id] = chunk_metas\n\t        # add embeddings to the store\n\t        self.store.add(chunk_embeddings, labels)\n\t    def _get_embeddings(\n\t        self, doc_path: str, batch_size: int = EMBEDDINGS_BATCH_SIZE,\n\t    ) -> (List[List[float]], List[ChunkMetadata]):\n\t        \"\"\"\n\t        Split the doc's text into chunks, generate one embedding and metadata\n\t        for each chunk.\n", "        Returns:\n\t            A list of embeddings and metadatas for all chunks in the doc.\n\t        \"\"\"\n\t        # the embeddings for all chunks in the doc\n\t        chunk_embeddings: List[List[float]] = []\n\t        # the metadata for all chunks in the doc\n\t        chunk_metas: List[ChunkMetadata] = []\n\t        # read the whole file. TODO support pagination for large files.\n\t        with open(doc_path, mode=\"r\", encoding=\"utf-8\") as f:\n\t            text = f.read()\n", "        # return an empty list if the text is empty or whitespace\n\t        if not text or text.isspace():\n\t            return chunk_embeddings, chunk_metas\n\t        # split the doc text to chunks\n\t        chunk_token_size = self.model.get_max_token_size()\n\t        chunk_texts, chunk_metas = self._get_text_chunks(\n\t            doc_path, text, chunk_token_size, MIN_CHUNK_SIZE_CHARS)\n\t        # get embeddings for all chunks\n\t        for i in range(0, len(chunk_texts), EMBEDDINGS_BATCH_SIZE):\n\t            batch_texts = chunk_texts[i:i+EMBEDDINGS_BATCH_SIZE]\n", "            embeddings = self.model.get_embeddings(batch_texts)\n\t            chunk_embeddings.extend(embeddings)\n\t        return chunk_embeddings, chunk_metas\n\t    def _get_text_chunks(\n\t        self,\n\t        doc_path: str, # the doc path, for logging\n\t        text: str, # the doc text\n\t        chunk_token_size: int, # the number of tokens in one chunk\n\t        min_chunk_chars: int, # the minimum size of each text chunk in chars\n\t    ) -> (List[str], List[ChunkMetadata]):\n", "        \"\"\"\n\t        Split the text into chunks.\n\t        Return a list of texts and metadadatas for all chunks in the text.\n\t        \"\"\"\n\t        chunk_texts: List[str] = []\n\t        chunk_metas: List[ChunkMetadata] = []\n\t        # tokenize the text\n\t        # according to tiktoken/core.py, \"encode_ordinary is equivalent to\n\t        # `encode(text, disallowed_special=())` (but slightly faster).\"\n\t        tokens = self.tokenizer.encode_ordinary(text)\n", "        # loop until all tokens are consumed or the max elements are reached\n\t        offset = 0\n\t        while tokens:\n\t            # take the next chunk\n\t            chunk = tokens[:chunk_token_size]\n\t            # decode to text to check whitespace and sentence boundary\n\t            chunk_text = self.tokenizer.decode(chunk)\n\t            # skip the chunk if it is empty or whitespace\n\t            if not chunk_text or chunk_text.isspace():\n\t                # remove from the remaining tokens\n", "                tokens = tokens[len(chunk):]\n\t                # increase the offset\n\t                offset += len(chunk_text)\n\t                continue\n\t            # truncate chunk_text to the last complete sentence (punctation).\n\t            # TODO support other languages, maybe consider such as NLTK.\n\t            last_punc = max(\n\t                chunk_text.rfind(\".\"),\n\t                chunk_text.rfind(\"?\"),\n\t                chunk_text.rfind(\"!\"),\n", "                chunk_text.rfind(\"\\n\"),\n\t            )\n\t            if last_punc != -1 and last_punc > min_chunk_chars:\n\t                chunk_text = chunk_text[:last_punc+1]\n\t            chunk_text_len = len(chunk_text)\n\t            # adjust the chunk_text_len if needed.\n\t            # check if some text in the last token is skipped. For example,\n\t            # cl100k_base takes '.\"[' as one token. If two sentences have this\n\t            # string, 'This sentence.\"[1] Next sentence.', and \"This sentence.\"\n\t            # is the last sentence, the next offset will not align with tokens,\n", "            # e.g. the next offset will point to the first char in '\"[1',\n\t            # while, the decoded text of the next token is '1'.\n\t            chunk_tokens = self.tokenizer.encode_ordinary(chunk_text)\n\t            last_chunk_token = len(chunk_tokens) - 1\n\t            if chunk_tokens[last_chunk_token] != tokens[last_chunk_token]:\n\t                # align chunk_text_len with the last token\n\t                last_token_text = self.tokenizer.decode(\n\t                    chunk_tokens[last_chunk_token:])\n\t                token_text = self.tokenizer.decode(\n\t                    tokens[last_chunk_token:last_chunk_token+1])\n", "                chunk_text_len += len(token_text) - len(last_token_text)\n\t                logging.debug(f\"align last_token_text={last_token_text} \"\n\t                              f\"token_text={token_text}\")\n\t            logging.debug(f\"offset={offset} chunk_text_len={chunk_text_len}\")\n\t            # sanity check\n\t            if text[offset:offset+10] != chunk_text[:10]:\n\t                logging.warning(f\"doc_path={doc_path} offset={offset},\"\n\t                                f\"text chars={text[offset:offset+10]}\"\n\t                                f\"chunk chars={chunk_text[:20]}\")\n\t                raise Exception(\n", "                    f\"text and chunk not aligned, {doc_path} offset={offset}\")\n\t            # remove any newline characters and strip any leading or trailing\n\t            # whitespaces. Not needed if use NLTK.\n\t            chunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\n\t            if len(chunk_text_to_append) > MIN_CHUNK_LENGTH_TO_EMBED:\n\t                # add the chunk text\n\t                chunk_texts.append(chunk_text_to_append)\n\t                # add the chunk meta\n\t                chunk_metas.append(ChunkMetadata(\n\t                    offset=offset,\n", "                    length=chunk_text_len,\n\t                    label=0, # initial 0 label, will be assigned later\n\t                ))\n\t            # increase the offset\n\t            offset += chunk_text_len\n\t            # remove the chunk text tokens from the remaining tokens.\n\t            tokens = tokens[last_chunk_token+1:]\n\t        return chunk_texts, chunk_metas\n\t    def delete(self, doc_id: str):\n\t        \"\"\"\n", "        Delete a doc from the vector index.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    def search(self, query_string: str, top_k: int) -> List[DocChunkScore]:\n\t        \"\"\"\n\t        Take a query string, get embedding for the query string, find the\n\t        similar doc chunks in the store, calculate the scores and return the\n\t        top_k doc chunks.\n\t        The score for a doc chunk is calculated based on the distance to the\n\t        query string embedding.\n", "        Return the top-k doc chunks, sorted in descending order based on score.\n\t        \"\"\"\n\t        texts: List[str] = []\n\t        texts.append(query_string)\n\t        embeddings = self.model.get_embeddings(texts)\n\t        # check k with the current number of elements. Some store, such as\n\t        # hnswlib, throws RuntimeError if k > elements.\n\t        if top_k > self.metadata.elements:\n\t            top_k = self.metadata.elements\n\t        # query the vector store\n", "        labels, distances = self.store.query(embeddings, top_k)\n\t        # convert distances to scores\n\t        return self._distance_to_scores(labels[0], distances[0])\n\t    def _distance_to_scores(\n\t        self, labels: List[int], distances: List[float],\n\t    ) -> List[DocChunkScore]:\n\t        # Convert the distances to the scores in range (0, 1),\n\t        # higher score means closer.\n\t        chunk_scores: List[DocChunkScore] = []\n\t        for i, label in enumerate(labels):\n", "            if self.space == Space.l2:\n\t                # l2 distance, lower distance means closer\n\t                score = 1 / (1 + distances[i])\n\t            else:\n\t                # ip or cosine distance, higher distance means closer\n\t                score = (1 + distances[i]) / 2\n\t            # get the doc id for the chunk\n\t            chunk_id = self.label_to_chunk_id[label]\n\t            chunk_score = DocChunkScore(\n\t                doc_id=chunk_id.doc_id, offset=chunk_id.offset,\n", "                length=chunk_id.length, score=score)\n\t            chunk_scores.append(chunk_score)\n\t        return chunk_scores\n"]}
{"filename": "index/__init__.py", "chunked_list": []}
{"filename": "index/index.py", "chunked_list": ["import os\n\timport logging\n\timport lucene\n\tfrom typing import List\n\timport uuid\n\tfrom java.nio.file import Files, Path\n\tfrom org.apache.lucene.analysis.standard import StandardAnalyzer\n\tfrom org.apache.lucene.document import \\\n\t    Document, Field, StringField, TextField, StoredField\n\tfrom org.apache.lucene.index import \\\n", "    DirectoryReader, IndexWriter, IndexWriterConfig, Term\n\tfrom org.apache.lucene.queryparser.classic import QueryParser\n\tfrom org.apache.lucene.search import IndexSearcher, ScoreDoc, TermQuery\n\tfrom org.apache.lucene.store import FSDirectory\n\tfrom index.docs import DocField, DocFields, DocChunkScore\n\tfrom index.vector_index import VectorIndex\n\t# the reserved field names for the doc\n\tFIELD_DOC_ID = \"doc_id\"\n\tFIELD_DOC_TEXT = \"doc_text\"\n\tFIELD_VECTOR_INDEX_VERSION = \"vector_index_version\"\n", "# the reserved doc ids for the internal usage\n\t# the reserved doc id for the vector index metadata\n\tSYS_DOC_ID_VECTOR_INDEX = \"$sys_doc_id_vector_index\"\n\t# the subdir for Lucene\n\tSUBDIR_LUCENE = \"lucene\"\n\tSUBDIR_VECTOR = \"vector\"\n\t\"\"\"\n\tThe Index class combines Lucene index with the vector index. It accepts a\n\tdocument, splits the document content to chunks, generates embeddings for each\n\tchunk using the specified model, persists the embeddings in the vector index\n", "and persists Lucene fields in the Lucene index. Search could search both Lucene\n\tand vector index, and merge the results.\n\tThe Index class guarantees the consistency between Lucene index and vector\n\tindex, and manages the lifecycle of the documents.\n\tTODO this class is not thread safe for concurrent write and read. The underline\n\tvector store, such as Hnswlib, does not support concurrent write and read.\n\t\"\"\"\n\tclass Index:\n\t    index_dir: str\n\t    writer: IndexWriter\n", "    searcher: IndexSearcher\n\t    vector_index: VectorIndex\n\t    vector_index_version: int\n\t    def __init__(\n\t        self,\n\t        index_dir: str,\n\t        model_provider: str,\n\t        vector_store: str,\n\t    ):\n\t        if not os.path.exists(index_dir):\n", "            os.mkdir(index_dir)\n\t        lucene_dir = os.path.join(index_dir, SUBDIR_LUCENE)\n\t        if not os.path.exists(lucene_dir):\n\t            os.mkdir(lucene_dir)\n\t        vector_dir = os.path.join(index_dir, SUBDIR_VECTOR)\n\t        if not os.path.exists(vector_dir):\n\t            os.mkdir(vector_dir)\n\t        analyzer = StandardAnalyzer()\n\t        # initialize the IndexWriter for Lucene\n\t        fs_dir = FSDirectory.open(Path.of(lucene_dir))\n", "        config = IndexWriterConfig(analyzer)\n\t        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND)\n\t        self.writer = IndexWriter(fs_dir, config)\n\t        self.index_dir = index_dir\n\t        # initialize the IndexSearcher from the writer\n\t        reader = DirectoryReader.open(self.writer)\n\t        self.searcher = IndexSearcher(reader)\n\t        # initialize the vector index\n\t        self.vector_index = VectorIndex(\n\t            vector_dir, model_provider, vector_store)\n", "        # get the latest vector index version from Lucene\n\t        self.vector_index_version = self._get_vector_index_version()\n\t        if self.vector_index_version > 0:\n\t            # load the existing vectors\n\t            self.vector_index.load(self.vector_index_version)\n\t        logging.info(f\"Initialize the index index_dir={index_dir} \"\n\t                     f\"model={model_provider} vector_store={vector_store} \"\n\t                     f\"vector_index_version={self.vector_index_version}\")\n\t    def _get_vector_index_version(self) -> int:\n\t        reader = DirectoryReader.openIfChanged(self.searcher.getIndexReader())\n", "        if reader:\n\t            self.searcher.getIndexReader().close()\n\t            self.searcher = IndexSearcher(reader)\n\t        # doc may not exist if no doc is added to the index\n\t        vector_index_version = 0\n\t        term = Term(FIELD_DOC_ID, SYS_DOC_ID_VECTOR_INDEX)\n\t        q = TermQuery(term)\n\t        docs = self.searcher.search(q, 1).scoreDocs\n\t        if len(docs) > 0:\n\t            # get the latest vector index version\n", "            doc = self.searcher.doc(docs[0].doc)\n\t            field = doc.getField(FIELD_VECTOR_INDEX_VERSION)\n\t            vector_index_version = field.numericValue().longValue()\n\t        return vector_index_version\n\t    def close(self):\n\t        \"\"\"\n\t        Close the index. The user must call commit before close, to make sure\n\t        the possible in-memory changes are committed.\n\t        \"\"\"\n\t        self.writer.close()\n", "        self.searcher.getIndexReader().close()\n\t        logging.info(\"Close the index\")\n\t    # TODO not support async. The underline vector lib, such as hnswlib,\n\t    # does not support concurrent writes. Lucene supports concurrent writes\n\t    # using multiple writers, and merge the segments in the background.\n\t    def add(self, doc_path: str, doc_fields: DocFields) -> str:\n\t        \"\"\"\n\t        Add a doc to the index. The doc file must be a plain text file.\n\t        This function automatically generates the embeddings for the doc text.\n\t        Return the document id.\n", "        \"\"\"\n\t        # convert DocFields to Lucene fields\n\t        fields = self._convert_to_lucene_fields(doc_fields)\n\t        return self._add(doc_path, fields)\n\t    def _convert_to_lucene_fields(self, doc_fields: DocFields) -> List[Field]:\n\t        fields: List[Field] = []\n\t        if doc_fields is None:\n\t            return fields\n\t        for doc_field in doc_fields.fields:\n\t            field: Field = None\n", "            if doc_field.string_value is not None:\n\t                field = StringField(\n\t                    doc_field.name, doc_field.string_value, Field.Store.YES)\n\t            if doc_field.numeric_value is not None:\n\t                field = StoredField(doc_field.name, doc_field.numeric_value)\n\t            if doc_field.float_value is not None:\n\t                field = StoredField(doc_field.name, doc_field.float_value)\n\t            fields.append(field)\n\t        return fields\n\t    def _add(self, doc_path: str, fields: List[Field]) -> str:\n", "        # TODO support only a limited number of docs, e.g. less than\n\t        # vector_index.DEFAULT_VECTOR_FILE_MAX_ELEMENTS. One vector index\n\t        # element is one doc chunk.\n\t        # TODO support embeddings for other fields, such as title, etc.\n\t        # TODO support other type files, such as pdf, etc, e.g. extract text\n\t        # from file, write to a temporary text file, and then pass the\n\t        # temporary text file to this function.\n\t        # TODO support small files, such as 10KB. no need to persist the file\n\t        # to a temporary file, when running as http server.\n\t        # get doc_id from fields, assign a unique id to doc if doc_id is None\n", "        doc_id = \"\"\n\t        for field in fields:\n\t            if field.name() == FIELD_DOC_ID:\n\t                doc_id = field.stringValue()\n\t                break\n\t        # TODO if doc_id is passed in, check doc_id does not exist\n\t        if doc_id == \"\":\n\t            doc_id = str(uuid.uuid4())\n\t            fields.append(StringField(FIELD_DOC_ID, doc_id, Field.Store.YES))\n\t        # add the doc to vector writer\n", "        self.vector_index.add(doc_path, doc_id)\n\t        # add the doc to Lucene\n\t        self._add_to_lucene(doc_path, fields)\n\t        logging.debug(f\"add doc id={doc_id} to index\")\n\t        return doc_id\n\t    def _add_to_lucene(self, doc_path: str, fields: List[Field]):\n\t        file_path = Path.of(doc_path)\n\t        br = Files.newBufferedReader(file_path)\n\t        try:\n\t            doc = Document()\n", "            for field in fields:\n\t                doc.add(field)\n\t            text_field = TextField(FIELD_DOC_TEXT, br)\n\t            doc.add(text_field)\n\t            self.writer.addDocument(doc)\n\t        finally:\n\t            br.close()\n\t    def commit(self):\n\t        # flush the vector index. TODO delete the older vector index files.\n\t        self.vector_index.save(self.vector_index_version + 1)\n", "        # update the latest vector index version as the special doc0 in Lucene\n\t        doc = Document()\n\t        doc_id_field = StringField(\n\t            FIELD_DOC_ID, SYS_DOC_ID_VECTOR_INDEX, Field.Store.YES)\n\t        doc.add(doc_id_field)\n\t        vector_version_field = StoredField(\n\t            FIELD_VECTOR_INDEX_VERSION, self.vector_index_version + 1)\n\t        doc.add(vector_version_field)\n\t        if self.vector_index_version == 0:\n\t            # create the vector doc\n", "            self.writer.addDocument(doc)\n\t        else:\n\t            # update the vector doc\n\t            term = Term(FIELD_DOC_ID, SYS_DOC_ID_VECTOR_INDEX)\n\t            self.writer.updateDocument(term, doc)\n\t        # commit Lucene\n\t        self.writer.commit()\n\t        # successfully commit both vector and lucene indexes\n\t        self.vector_index_version += 1\n\t        logging.info(f\"Commit the index {self.index_dir}, \"\n", "                     f\"vector_index_version={self.vector_index_version}\")\n\t    def vector_search(\n\t        self, query_string: str, top_k: int,\n\t    ) -> List[DocChunkScore]:\n\t        \"\"\"\n\t        Take the query string, search over the doc content (text) and return\n\t        the top docs. The search will include both the traditional inverted\n\t        search and vector search.\n\t        \"\"\"\n\t        # TODO\n", "        # - support index and search other fields, such as title.\n\t        # - support more Lucene query abilities vs natural language search\n\t        #   like gmail. For example, user inputs \"a query string. field:value\",\n\t        #   automatically search the query string over all invert/vector\n\t        #   indexed fields, and search the specified field.\n\t        # - support retrieving the specified fields.\n\t        # - etc.\n\t        doc_chunk_scores = self.vector_index.search(query_string, top_k)\n\t        logging.debug(\n\t            f\"vector search query=\\'{query_string}\\' docs={doc_chunk_scores}\")\n", "        return doc_chunk_scores\n\t    def lucene_search(\n\t        self, query_string: str, top_k: int,\n\t    ) -> List[DocChunkScore]:\n\t        # TODO support concurrent reads\n\t        reader = DirectoryReader.openIfChanged(self.searcher.getIndexReader())\n\t        if reader:\n\t            self.searcher.getIndexReader().close()\n\t            self.searcher = IndexSearcher(reader)\n\t        analyzer = self.writer.getConfig().getAnalyzer()\n", "        parser = QueryParser(FIELD_DOC_TEXT, analyzer)\n\t        query = parser.parse(query_string)\n\t        logging.debug(f\"parse query string: {query_string}, to {query}\")\n\t        lucene_score_docs = self.searcher.search(query, top_k).scoreDocs\n\t        doc_chunk_scores: List[DocChunkScore] = []\n\t        for score_doc in lucene_score_docs:\n\t            # get doc id\n\t            doc = self.searcher.doc(score_doc.doc)\n\t            doc_id = doc.get(FIELD_DOC_ID)\n\t            # TODO get the offset and length via TermVector or Highlighter\n", "            doc_chunk_score = DocChunkScore(\n\t                doc_id=doc_id, offset=0, length=0, score=score_doc.score)\n\t            doc_chunk_scores.append(doc_chunk_score)\n\t        logging.debug(\n\t            f\"lucene search query=\\'{query_string}\\' docs={doc_chunk_scores}\")\n\t        return doc_chunk_scores\n"]}
{"filename": "tests/index/test_index.py", "chunked_list": ["import os\n\timport logging\n\timport lucene\n\timport pytest\n\timport time\n\tfrom typing import List\n\timport shutil\n\tfrom index.docs import DocField, DocFields\n\tfrom index.index import Index\n\tclass TestSentenceTransformerWithIndex:\n", "    def test_index(self):\n\t        t = IndexAndSearchTest()\n\t        t.index_docs_and_search(\n\t            \"./tests/index/\", \"sentence_transformer\", \"hnswlib\")\n\tclass IndexAndSearchTest:\n\t    def index_docs_and_search(\n\t        self, base_dir: str, model_name: str, vector_store: str,\n\t    ):\n\t        ut_dir = os.path.join(base_dir, \"utdir-index\")\n\t        if os.path.exists(ut_dir):\n", "            # remove the possible garbage by previous failed test\n\t            shutil.rmtree(ut_dir)\n\t        os.mkdir(ut_dir)\n\t        lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n\t        index = Index(ut_dir, model_name, vector_store)\n\t        try:\n\t            # step1: add the first file\n\t            doc_path1 = \"./tests/testfiles/single_sentence.txt\"\n\t            fields: List[DocField] = []\n\t            pathField = DocField(name=\"path\", string_value=doc_path1)\n", "            fields.append(pathField)\n\t            doc_fields = DocFields(fields=fields)\n\t            doc_id1 = index.add(doc_path1, doc_fields)\n\t            # search lucene\n\t            query_string = \"A person is eating food.\"\n\t            top_k = 3\n\t            start = time.monotonic()\n\t            lucene_score_docs = index.lucene_search(query_string, top_k)\n\t            dur = time.monotonic() - start\n\t            logging.info(f\"1 doc, lucene search time: {dur}s\")\n", "            assert 1 == len(lucene_score_docs)\n\t            assert doc_id1 == lucene_score_docs[0].doc_id\n\t            # search vector index\n\t            start = time.monotonic()\n\t            vector_score_docs = index.vector_search(query_string, top_k)\n\t            dur = time.monotonic() - start\n\t            logging.info(f\"1 doc, vector search time: {dur}s\")\n\t            assert 1 == len(vector_score_docs)\n\t            assert doc_id1 == vector_score_docs[0].doc_id\n\t            assert vector_score_docs[0].score > 0.9\n", "            # commit and verify the vector index version\n\t            index.commit()\n\t            vector_index_version = index._get_vector_index_version()\n\t            assert 1 == vector_index_version\n\t            # step2: add the second file\n\t            doc_path2 = \"./tests/testfiles/chatgpt.txt\"\n\t            fields.clear()\n\t            pathField = DocField(name=\"path\", string_value=doc_path2)\n\t            fields.append(pathField)\n\t            doc_fields = DocFields(fields=fields)\n", "            doc_id2 = index.add(doc_path2, doc_fields)\n\t            # search lucene only\n\t            query_string = \"A person is eating food.\"\n\t            top_k = 3\n\t            start = time.monotonic()\n\t            lucene_score_docs = index.lucene_search(query_string, top_k)\n\t            dur = time.monotonic() - start\n\t            logging.info(f\"2 docs, lucene search time: {dur}s\")\n\t            assert 2 == len(lucene_score_docs)\n\t            # search vector index\n", "            start = time.monotonic()\n\t            vector_score_docs = index.vector_search(query_string, top_k)\n\t            dur = time.monotonic() - start\n\t            logging.info(f\"2 docs, vector search time: {dur}s\")\n\t            # sentence_transformer returns:\n\t            # [DocChunkScore(doc_id1, offset=0, length=25, score=1.0),\n\t            #  DocChunkScore(doc_id2, offset=15234, length=1172, score=0.34),\n\t            #  DocChunkScore(doc_id2, offset=2219, length=1182, score=0.34)]\n\t            # openai returns, open file, seek and read, the text looks not\n\t            # related to the query_string, not sure why openai scores 0.63\n", "            # [DocChunkScore(doc_id1, offset=0, length=25, score=1.0),\n\t            #  DocChunkScore(doc_id2, offset=15234, length=1172, score=0.63),\n\t            #  DocChunkScore(doc_id2, offset=16406, length=1272, score=0.63)]\n\t            #logging.info(f\"=== {vector_score_docs}\")\n\t            assert 3 == len(vector_score_docs)\n\t            assert doc_id1 == vector_score_docs[0].doc_id\n\t            assert doc_id2 == vector_score_docs[1].doc_id\n\t            assert doc_id2 == vector_score_docs[2].doc_id\n\t            assert vector_score_docs[0].score > 0.9\n\t            if model_name == \"sentence_transformer\":\n", "                assert vector_score_docs[1].score < 0.5 # doc2 has low score\n\t                assert vector_score_docs[2].score < 0.5 # doc2 has low score\n\t            if vector_score_docs[1].score > 0.5:\n\t                score = vector_score_docs[1].score\n\t                logging.info(f\"{model_name} scores high {score}\")\n\t            # commit and verify the vector index version\n\t            index.commit()\n\t            vector_index_version = index._get_vector_index_version()\n\t            assert 2 == vector_index_version\n\t            index.close()\n", "            # step3: reload index\n\t            index = Index(ut_dir, model_name, vector_store)\n\t            assert 2 == index.vector_index_version\n\t            # search lucene only\n\t            query_string = \"A person is eating food.\"\n\t            top_k = 3\n\t            start = time.monotonic()\n\t            lucene_score_docs = index.lucene_search(query_string, top_k)\n\t            dur = time.monotonic() - start\n\t            logging.info(f\"2 docs, reload, lucene search time: {dur}s\")\n", "            assert 2 == len(lucene_score_docs)\n\t            # search vector index\n\t            start = time.monotonic()\n\t            vector_score_docs = index.vector_search(query_string, top_k)\n\t            dur = time.monotonic() - start\n\t            logging.info(f\"2 docs, reload, vector search time: {dur}s\")\n\t            assert 3 == len(vector_score_docs)\n\t            assert doc_id1 == vector_score_docs[0].doc_id\n\t            assert doc_id2 == vector_score_docs[1].doc_id\n\t            assert doc_id2 == vector_score_docs[2].doc_id\n", "            assert vector_score_docs[0].score > 0.9\n\t            if model_name == \"sentence_transformer\":\n\t                assert vector_score_docs[1].score < 0.5 # doc2 has low score\n\t                assert vector_score_docs[2].score < 0.5 # doc2 has low score\n\t        finally:\n\t            index.close()\n\t        # cleanup\n\t        shutil.rmtree(ut_dir)\n"]}
{"filename": "tests/index/test_vector_index.py", "chunked_list": ["import os\n\timport pytest\n\tfrom typing import List\n\timport shutil\n\tfrom index.vector_index import VectorIndex\n\tclass TestVectorIndex:\n\t    def test_single_sentence(self):\n\t        index = VectorIndex(\"./\", \"sentence_transformer\", \"hnswlib\")\n\t        text = \"A person is eating food.\"\n\t        doc_path = \"./tests/testfiles/single_sentence.txt\"\n", "        chunk_embeddings, chunk_metas = index._get_embeddings(doc_path)\n\t        assert 1 == len(chunk_embeddings)\n\t        assert 1 == len(chunk_metas)\n\t        assert 0 == chunk_metas[0].offset\n\t        assert len(text)+1 == chunk_metas[0].length\n\t        assert 0 == chunk_metas[0].label\n\t        texts = []\n\t        texts.append(text)\n\t        embeddings = index.model.get_embeddings(texts)\n\t        assert 1 == len(embeddings)\n", "        assert index.model.get_dim() == len(embeddings[0])\n\t        assert all([a == b for a, b in zip(chunk_embeddings[0], embeddings[0])])\n\t    def test_small_file(self):\n\t        index = VectorIndex(\"./\", \"sentence_transformer\", \"hnswlib\")\n\t        doc_path = \"./tests/testfiles/chatgpt.txt\"\n\t        chunk_embeddings, chunk_metas = index._get_embeddings(doc_path)\n\t        assert 28 == len(chunk_embeddings)\n\t        assert 28 == len(chunk_metas)\n\t        # the first meta has offset == 0\n\t        assert 0 == chunk_metas[0].offset\n", "        # _get_embeddings does not assign label\n\t        assert 0 == chunk_metas[0].label\n\t        # test small embedding batch\n\t        chunk_embeddings, chunk_metas = index._get_embeddings(doc_path, 5)\n\t        assert 28 == len(chunk_embeddings)\n\t        assert 28 == len(chunk_metas)\n\t        # the first meta has offset == 0\n\t        assert 0 == chunk_metas[0].offset\n\t        # _get_embeddings does not assign label\n\t        assert 0 == chunk_metas[0].label\n", "    def test_special_files(self):\n\t        index = VectorIndex(\"./\", \"sentence_transformer\", \"hnswlib\")\n\t        # test empty file\n\t        doc_path = \"./tests/testfiles/empty.txt\"\n\t        chunk_embeddings, chunk_metas = index._get_embeddings(doc_path)\n\t        assert 0 == len(chunk_embeddings)\n\t        assert 0 == len(chunk_metas)\n\t        # test file with only whitespaces\n\t        doc_path = \"./tests/testfiles/whitespaces.txt\"\n\t        chunk_embeddings, chunk_metas = index._get_embeddings(doc_path)\n", "        assert 0 == len(chunk_embeddings)\n\t        assert 0 == len(chunk_metas)\n\t        # test file with only 3 chars\n\t        doc_path = \"./tests/testfiles/3chars.txt\"\n\t        chunk_embeddings, chunk_metas = index._get_embeddings(doc_path)\n\t        assert 0 == len(chunk_embeddings)\n\t        assert 0 == len(chunk_metas)\n\t    def test_index(self):\n\t        # test index with 2 files, cover the mapping of doc ids and labels\n\t        index = VectorIndex(\"./\", \"sentence_transformer\", \"hnswlib\")\n", "        # add the first file\n\t        text = \"A person is eating food.\"\n\t        doc_path1 = \"./tests/testfiles/single_sentence.txt\"\n\t        doc_id1 = \"doc_id1\"\n\t        doc1_chunks = 1\n\t        label1 = 1\n\t        index.add(doc_path1, doc_id1)\n\t        assert doc1_chunks == index.metadata.elements\n\t        assert label1 == index.metadata.last_label\n\t        assert 1 == len(index.doc_id_to_metas)\n", "        assert doc1_chunks == len(index.doc_id_to_metas[doc_id1])\n\t        assert 0 == index.doc_id_to_metas[doc_id1][0].offset\n\t        assert len(text)+1 == index.doc_id_to_metas[doc_id1][0].length\n\t        assert label1 == index.doc_id_to_metas[doc_id1][0].label\n\t        assert 1 == len(index.label_to_chunk_id)\n\t        assert doc_id1 == index.label_to_chunk_id[label1].doc_id\n\t        assert 0 == index.label_to_chunk_id[label1].offset\n\t        assert len(text)+1 == index.label_to_chunk_id[label1].length\n\t        # search\n\t        query_string = \"A person is eating food.\"\n", "        top_k = 3\n\t        doc_chunk_scores = index.search(query_string, top_k)\n\t        assert 1 == len(doc_chunk_scores)\n\t        assert doc_id1 == doc_chunk_scores[0].doc_id\n\t        assert doc_chunk_scores[0].score > 0.9 # very high score\n\t        # add the second file\n\t        doc_path2 = \"./tests/testfiles/chatgpt.txt\"\n\t        doc_id2 = \"doc_id2\"\n\t        doc2_chunks = 28\n\t        index.add(doc_path2, doc_id2)\n", "        assert doc1_chunks+doc2_chunks == index.metadata.elements\n\t        assert label1+doc2_chunks == index.metadata.last_label\n\t        # make sure the offsets are continuous\n\t        offset = 0\n\t        for chunk_meta in index.doc_id_to_metas[doc_id2]:\n\t            assert offset == chunk_meta.offset\n\t            offset += chunk_meta.length\n\t        assert 2 == len(index.doc_id_to_metas)\n\t        # verify doc1 metas\n\t        assert 1 == len(index.doc_id_to_metas[doc_id1])\n", "        assert 0 == index.doc_id_to_metas[doc_id1][0].offset\n\t        assert len(text)+1 == index.doc_id_to_metas[doc_id1][0].length\n\t        assert label1 == index.doc_id_to_metas[doc_id1][0].label\n\t        # verify doc2 metas\n\t        assert doc2_chunks == len(index.doc_id_to_metas[doc_id2])\n\t        assert 0 == index.doc_id_to_metas[doc_id2][0].offset\n\t        for i, chunk_meta in enumerate(index.doc_id_to_metas[doc_id2]):\n\t            assert label1+i+1 == chunk_meta.label\n\t        assert doc1_chunks+doc2_chunks == len(index.label_to_chunk_id)\n\t        # verify doc1 chunk ids\n", "        assert doc_id1 == index.label_to_chunk_id[label1].doc_id\n\t        assert 0 == index.label_to_chunk_id[label1].offset\n\t        assert len(text)+1 == index.label_to_chunk_id[label1].length\n\t        # verify doc2 chunk ids\n\t        for label in range(label1+1, len(index.label_to_chunk_id)):\n\t            assert doc_id2 == index.label_to_chunk_id[label].doc_id\n\t        # search\n\t        query_string = \"A person is eating food.\"\n\t        top_k = 3\n\t        doc_chunk_scores = index.search(query_string, top_k)\n", "        assert top_k == len(doc_chunk_scores)\n\t        assert doc_id1 == doc_chunk_scores[0].doc_id\n\t        assert doc_id2 == doc_chunk_scores[1].doc_id\n\t        assert doc_id2 == doc_chunk_scores[2].doc_id\n\t        assert doc_chunk_scores[0].score > 0.9 # doc1 has high score\n\t        assert doc_chunk_scores[1].score < 0.5 # doc2 has low score\n\t        assert doc_chunk_scores[1].score < 0.5 # doc2 has low score\n\t        # search a unrelated string\n\t        query_string = \"a beautiful sky\"\n\t        top_k = 3\n", "        doc_chunk_scores = index.search(query_string, top_k)\n\t        assert 3 == len(doc_chunk_scores)\n\t        # all doc chunks have low score\n\t        assert doc_chunk_scores[0].score < 0.5\n\t        assert doc_chunk_scores[1].score < 0.5\n\t        assert doc_chunk_scores[2].score < 0.5\n\t    def test_save_load_index(self):\n\t        # test load index with 2 files\n\t        ut_dir = \"./tests/index/utdir-vectorindex\"\n\t        if os.path.exists(ut_dir):\n", "            # remove the possible garbage by previous failed test\n\t            shutil.rmtree(ut_dir)\n\t        os.mkdir(ut_dir)\n\t        # the first file\n\t        text = \"A person is eating food.\"\n\t        doc_path1 = \"./tests/testfiles/single_sentence.txt\"\n\t        doc_id1 = \"doc_id1\"\n\t        doc1_chunks = 1\n\t        label1 = 1\n\t        # the second file\n", "        doc_path2 = \"./tests/testfiles/chatgpt.txt\"\n\t        doc_id2 = \"doc_id2\"\n\t        doc2_chunks = 28\n\t        # vector file version\n\t        version = 1\n\t        # create the vector file inside try, so VectorIndex is destructed,\n\t        # but hnswlib still complains, \"Warning: Calling load_index for an\n\t        # already inited index.\". Check it later.\n\t        try:\n\t            index = VectorIndex(ut_dir, \"sentence_transformer\", \"hnswlib\")\n", "            # add the first file\n\t            index.add(doc_path1, doc_id1)\n\t            # add the second file\n\t            index.add(doc_path2, doc_id2)\n\t            # save the vectors to file\n\t            index.save(version)\n\t        except:\n\t            assert False\n\t        # load from file\n\t        index1 = VectorIndex(ut_dir, \"sentence_transformer\", \"hnswlib\")\n", "        assert 0 == index1.metadata.elements\n\t        assert 0 == index1.metadata.last_label\n\t        index1.load(version)\n\t        assert doc1_chunks+doc2_chunks == index1.metadata.elements\n\t        assert label1+doc2_chunks == index1.metadata.last_label\n\t        assert 2 == len(index1.doc_id_to_metas)\n\t        # verify doc1 metas\n\t        assert 1 == len(index1.doc_id_to_metas[doc_id1])\n\t        assert 0 == index1.doc_id_to_metas[doc_id1][0].offset\n\t        assert len(text)+1 == index1.doc_id_to_metas[doc_id1][0].length\n", "        assert label1 == index1.doc_id_to_metas[doc_id1][0].label\n\t        # verify doc2 metas\n\t        assert doc2_chunks == len(index1.doc_id_to_metas[doc_id2])\n\t        assert 0 == index1.doc_id_to_metas[doc_id2][0].offset\n\t        for i, chunk_meta in enumerate(index1.doc_id_to_metas[doc_id2]):\n\t            assert label1+i+1 == chunk_meta.label\n\t        assert doc1_chunks+doc2_chunks == len(index1.label_to_chunk_id)\n\t        # verify doc1 chunk ids\n\t        assert doc_id1 == index1.label_to_chunk_id[label1].doc_id\n\t        assert 0 == index1.label_to_chunk_id[label1].offset\n", "        assert len(text)+1 == index1.label_to_chunk_id[label1].length\n\t        # verify doc2 chunk ids\n\t        for label in range(label1+1, len(index1.label_to_chunk_id)):\n\t            assert doc_id2 == index1.label_to_chunk_id[label].doc_id\n\t        # search\n\t        query_string = \"A person is eating food.\"\n\t        top_k = 3\n\t        doc_chunk_scores = index.search(query_string, top_k)\n\t        assert 3 == len(doc_chunk_scores)\n\t        assert doc_id1 == doc_chunk_scores[0].doc_id\n", "        assert doc_id2 == doc_chunk_scores[1].doc_id\n\t        assert doc_id2 == doc_chunk_scores[2].doc_id\n\t        assert doc_chunk_scores[0].score > 0.9 # doc1 has high score\n\t        assert doc_chunk_scores[1].score < 0.5 # doc2 has low score\n\t        assert doc_chunk_scores[2].score < 0.5 # doc2 has low score\n\t        # search a unrelated string\n\t        query_string = \"a beautiful sky\"\n\t        top_k = 3\n\t        doc_chunk_scores = index.search(query_string, top_k)\n\t        assert 3 == len(doc_chunk_scores)\n", "        # all doc chunks have low score\n\t        assert doc_chunk_scores[0].score < 0.5\n\t        assert doc_chunk_scores[1].score < 0.5\n\t        assert doc_chunk_scores[2].score < 0.5\n\t        # cleanup\n\t        shutil.rmtree(ut_dir)\n"]}
{"filename": "tests/testfiles/download.py", "chunked_list": ["import requests\n\tfrom bs4 import BeautifulSoup\n\turl='https://en.wikipedia.org/wiki/ChatGPT'\n\t# save text from the url to a txt file\n\twith open(url.split('/')[-1]+\".txt\", \"w\", encoding=\"UTF-8\") as f:\n\t    # get the text from the URL using BeautifulSoup\n\t    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n\t    # save paragraphs\n\t    for i in soup.select('p'):\n\t        f.write(i.get_text())\n"]}
{"filename": "tests/model/test_sentence_transformer.py", "chunked_list": ["import os\n\timport logging\n\timport pytest\n\timport time\n\tfrom typing import List\n\timport numpy as np\n\tfrom model.factory import get_model\n\tclass TestSentTransformerModel():\n\t    def test_embeddings(self):\n\t        \"\"\"\n", "        simple measure the latency of different models on a MacBook M1Pro.\n\t        python3 -m pytest -s tests/model/test_sentence_transformer.py\n\t        default model load time: 1.4939462076872587s\n\t        get embeddings time: 0.05871379096060991s\n\t        all-mpnet-base-v2 model load time: 1.011457541026175s\n\t        get embeddings time: 0.17692300025373697s\n\t        \"\"\"\n\t        start = time.monotonic()\n\t        stmodel = get_model(\"sentence_transformer\")\n\t        assert 256 == stmodel.get_max_token_size()\n", "        assert 384 == stmodel.get_dim()\n\t        dur = time.monotonic() - start\n\t        logging.info(f\"\\ndefault model load time: {dur}s\")\n\t        sentences = ['A person is eating food.',\n\t                     'A person is eating a piece of bread.',\n\t                     'A person is riding a horse.',\n\t                     'A person is riding a white horse on an enclosed ground.']\n\t        start = time.monotonic()\n\t        embeddings = stmodel.get_embeddings(sentences)\n\t        assert len(sentences) == len(embeddings)\n", "        assert stmodel.get_dim() == len(embeddings[0])\n\t        dur = time.monotonic() - start\n\t        logging.info(f\"get embeddings time: {dur}s\")\n\t        # https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n\t        start = time.monotonic()\n\t        stmodel.set_model(\"all-mpnet-base-v2\", 384, 768)\n\t        assert 384 == stmodel.get_max_token_size()\n\t        assert 768 == stmodel.get_dim()\n\t        dur = time.monotonic() - start\n\t        logging.info(f\"all-mpnet-base-v2 model load time: {dur}s\")\n", "        start = time.monotonic()\n\t        embeddings = stmodel.get_embeddings(sentences)\n\t        assert len(sentences) == len(embeddings)\n\t        assert stmodel.get_dim() == len(embeddings[0])\n\t        dur = time.monotonic() - start\n\t        logging.info(f\"get embeddings time: {dur}s\")\n\t    def test_unsupported_model(self):\n\t        with pytest.raises(ValueError):\n\t            get_model(\"unknown_model\")\n"]}
{"filename": "tests/vectorstore/test_hnswlib.py", "chunked_list": ["import os\n\timport pytest\n\timport random\n\tfrom typing import List\n\timport numpy as np\n\tfrom vectorstore.factory import get_vector_store\n\tclass TestHnswlib():\n\t    def test_save_empty_index(self):\n\t        dim = 384\n\t        max_elements = 1\n", "        space = \"cosine\"\n\t        store = get_vector_store(\"hnswlib\", dim, space, max_elements)\n\t        index_path = f\"ut_empty_index.bin\"\n\t        store.save(index_path)\n\t        assert os.path.exists(index_path)\n\t        os.remove(index_path)\n\t    def test_index_cosines_space(self):\n\t        self.verify_index_spaces(\"cosine\")\n\t    def test_index_ip_space(self):\n\t        self.verify_index_spaces(\"ip\")\n", "    def verify_index_spaces(self, space: str):\n\t        dim = 16\n\t        max_elements = 5\n\t        store = get_vector_store(\"hnswlib\", dim, space, max_elements)\n\t        embeddings = np.float32(np.random.random((max_elements, dim)))\n\t        labels = np.arange(max_elements)\n\t        store.add(embeddings, labels)\n\t        query_embeddings: List[List[float]] = []\n\t        query_embeddings.append(embeddings[0])\n\t        qlabels, distances = store.query(\n", "            embeddings=query_embeddings, top_k=max_elements)\n\t        assert 1 == len(qlabels)\n\t        assert 1 == len(distances)\n\t        # verify all elements are returned\n\t        assert max_elements == len(qlabels[0])\n\t        assert max_elements == len(distances[0])\n\t        if space != \"ip\":\n\t            # inner product is not an actual metric. An element can be closer\n\t            # to some other element than to itself\n\t            assert labels[0] == qlabels[0][0]\n", "        qlabels[0].sort()\n\t        assert all([a == b for a, b in zip(qlabels[0], labels)])\n\t    def test_save_load_index_l2_space(self):\n\t        dim = 16\n\t        max_elements = 5\n\t        space = \"l2\"\n\t        store = get_vector_store(\"hnswlib\", dim, space, max_elements)\n\t        embeddings = np.float32(np.random.random((max_elements, dim)))\n\t        labels = np.arange(max_elements)\n\t        store.add(embeddings, labels)\n", "        qlabels, distances = store.query(embeddings=embeddings[0], top_k=1)\n\t        assert 1 == len(qlabels)\n\t        assert 1 == len(distances)\n\t        assert 1 == len(qlabels[0])\n\t        assert 1 == len(distances[0])\n\t        assert labels[0] == qlabels[0][0]\n\t        assert 0.0 == distances[0][0]\n\t        query_embeddings: List[List[float]] = []\n\t        query_embeddings.append(embeddings[0])\n\t        qlabels, distances = store.query(\n", "            embeddings=query_embeddings, top_k=max_elements)\n\t        assert 1 == len(qlabels)\n\t        assert 1 == len(distances)\n\t        assert max_elements == len(qlabels[0])\n\t        assert max_elements == len(distances[0])\n\t        assert labels[0] == qlabels[0][0]\n\t        # l2 equation, d = sum((Ai-Bi)^2), the distance of exact match is 0\n\t        assert 0.0 == distances[0][0]\n\t        qlabels[0].sort()\n\t        assert all([a == b for a, b in zip(qlabels[0], labels)])\n", "        index_path = \"ut_index.bin\"\n\t        store.save(index_path)\n\t        store1 = get_vector_store(\"hnswlib\", dim, space, max_elements)\n\t        store1.load(index_path)\n\t        qlabels, distances = store1.query(embeddings=embeddings[0], top_k=1)\n\t        assert 1 == len(qlabels)\n\t        assert 1 == len(distances)\n\t        assert 1 == len(qlabels[0])\n\t        assert 1 == len(distances[0])\n\t        assert labels[0] == qlabels[0][0]\n", "        assert 0.0 == distances[0][0]\n\t        qlabels, distances = store1.query(\n\t            embeddings=embeddings[0], top_k=max_elements)\n\t        assert 1 == len(qlabels)\n\t        assert 1 == len(distances)\n\t        assert max_elements == len(qlabels[0])\n\t        assert max_elements == len(distances[0])\n\t        assert labels[0] == qlabels[0][0]\n\t        assert 0.0 == distances[0][0]\n\t        qlabels[0].sort()\n", "        assert all([a == b for a, b in zip(qlabels[0], labels)])\n\t        os.remove(index_path)\n\t    def test_negative_cases(self):\n\t        dim = 384\n\t        max_elements = 5\n\t        space = \"cosine\"\n\t        store = get_vector_store(\"hnswlib\", dim, space, max_elements)\n\t        # negative test: num_elements > max_elements\n\t        num_elements = max_elements + 1\n\t        embeddings = np.float32(np.random.random((num_elements, dim)))\n", "        labels = np.arange(num_elements)\n\t        with pytest.raises(RuntimeError):\n\t            store.add(embeddings, labels)\n"]}
{"filename": "tests/openai/test_model_embedding.py", "chunked_list": ["import os\n\timport logging\n\timport pytest\n\timport time\n\tfrom model.factory import get_model\n\tclass TestOpenAIEmbeddingModel():\n\t    def test_embeddings(self):\n\t        m = get_model(\"openai_embedding\")\n\t        assert 256 == m.get_max_token_size()\n\t        assert 1536 == m.get_dim()\n", "        sentences = ['A person is eating food.',\n\t                     'A person is eating a piece of bread.',\n\t                     'A person is riding a horse.',\n\t                     'A person is riding a white horse on an enclosed ground.']\n\t        # example run time on a MacBook.\n\t        # run the test first time, get embeddings time: 0.48015683237463236s\n\t        # run the second time, get embeddings time: 0.25255241710692644s\n\t        start = time.monotonic()\n\t        embeddings = m.get_embeddings(sentences)\n\t        assert len(sentences) == len(embeddings)\n", "        assert m.get_dim() == len(embeddings[0])\n\t        dur = time.monotonic() - start\n\t        logging.info(f\"openai_embedding, get embeddings time: {dur}s\")\n\t        with pytest.raises(NotImplementedError):\n\t            m.set_model(\"model\", 1, 1)\n"]}
{"filename": "tests/openai/test_index.py", "chunked_list": ["import os\n\timport lucene\n\timport pytest\n\tfrom tests.index.test_index import IndexAndSearchTest\n\tclass TestIndexWithOpenAIAdaModel:\n\t    def test_index(self):\n\t        t = IndexAndSearchTest()\n\t        t.index_docs_and_search(\"./tests/openai/\", \"openai_embedding\", \"hnswlib\")\n"]}
{"filename": "server/api.py", "chunked_list": ["from enum import Enum\n\tfrom pydantic import BaseModel\n\tfrom typing import List, Optional\n\tfrom index.docs import DocChunkScore\n\tclass QueryType(str, Enum):\n\t    vector = \"vector\"\n\t    lucene = \"lucene\"\n\tclass QueryRequest(BaseModel):\n\t    # the query string. \n\t    # for lucene search, input the string supported by Lucene QueryParser.\n", "    # for vector search, simply input a string. TODO support QueryParser.\n\t    query: str\n\t    query_type: Optional[QueryType] = QueryType.vector\n\t    top_k: Optional[int] = 3\n\t# for now, simply return DocChunkScore.\n\t# TODO include the matched text, for such as highlight, etc.\n\t# TODO add auto QA ability. For a question, the server automatically sends the\n\t# top_k chunk texts as context to the QA model, such as ChatGPT, and includes\n\t# the answer in the response.\n\tclass QueryResponse(BaseModel):\n", "    doc_scores: List[DocChunkScore]\n"]}
{"filename": "server/__init__.py", "chunked_list": []}
{"filename": "server/server.py", "chunked_list": ["from contextlib import asynccontextmanager\n\tfrom fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile\n\timport logging\n\timport lucene\n\timport mimetypes\n\timport os\n\tfrom typing import Optional\n\timport sys\n\timport uvicorn\n\tfrom index.index import Index\n", "from index.docs import DocField, DocFields, DocChunkScore\n\tfrom server.api import QueryType, QueryRequest, QueryResponse\n\tlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\t# The embedding model provider: openai_embedding, sentence_transformer.\n\t# If model is set to openai_embedding, please remember to set OPENAI_API_KEY.\n\tENV_EMBEDDING_MODEL_PROVIDER = os.environ.get(\"ENV_EMBEDDING_MODEL_PROVIDER\")\n\t# The directory to store the lucene and vector index\n\tENV_INDEX_DIR = os.environ.get(\"ENV_INDEX_DIR\")\n\tDEFAULT_EMBEDDING_MODEL_PROVIDER = \"sentence_transformer\"\n\tDEFAULT_INDEX_DIR = \"./server_index_dir\"\n", "embedding_model = DEFAULT_EMBEDDING_MODEL_PROVIDER\n\tindex_dir = DEFAULT_INDEX_DIR\n\tif ENV_EMBEDDING_MODEL_PROVIDER is not None \\\n\t    and ENV_EMBEDDING_MODEL_PROVIDER != \"\":\n\t    embedding_model = ENV_EMBEDDING_MODEL_PROVIDER\n\tif ENV_INDEX_DIR is not None and ENV_INDEX_DIR != \"\":\n\t    index_dir = ENV_INDEX_DIR\n\t# the sub directory under index_dir to store the doc content\n\tindex_doc_dir = os.path.join(index_dir, \"docs\")\n\tdef start(host: str, port: int):\n", "    uvicorn.run(\"server.server:app\", host=host, port=port, reload=False)\n\t@asynccontextmanager\n\tasync def lifespan(app: FastAPI):\n\t    # init Index\n\t    global index\n\t    lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n\t    index = Index(index_dir=index_dir,\n\t                  model_provider=embedding_model,\n\t                  vector_store=\"hnswlib\")\n\t    logging.info(\"start the index\")\n", "    yield\n\t    # close the index\n\t    # TODO when use Ctrl+C to stop, close is not called. using the old shutdown\n\t    # does not work as well.\n\t    logging.info(\"close the index\")\n\t    index.close()\n\tapp = FastAPI(lifespan=lifespan)\n\t# TODO support creating the index\n\t@app.post(\"/add_doc\")\n\tasync def add_doc(\n", "    file: UploadFile = File(...),\n\t    fields: Optional[str] = Form(None),\n\t):\n\t    filename = file.filename\n\t    try:\n\t        # parse the fields\n\t        doc_fields: DocFields = None\n\t        if fields is not None:\n\t            doc_fields = DocFields.parse_raw(fields)\n\t        # save the file text\n", "        doc_path = await save_file_text(file)\n\t        # add file to index\n\t        doc_id = index.add(doc_path, doc_fields)\n\t        return doc_id\n\t    except Exception as e:\n\t        logging.error(f\"add doc {filename} error: {e}\")\n\t        raise HTTPException(status_code=500, detail=f\"str({e})\")\n\t@app.post(\"/commit\")\n\tasync def commit():\n\t    index.commit()\n", "@app.get(\n\t    \"/query\",\n\t    response_model=QueryResponse,\n\t)\n\tasync def query(\n\t    request: QueryRequest = Body(...),\n\t):\n\t    try:\n\t        docs: List[DocChunkScore] = None\n\t        if request.query_type == QueryType.vector:\n", "            docs = index.vector_search(request.query, request.top_k)\n\t        else:\n\t            docs = index.lucene_search(request.query, request.top_k)\n\t        return QueryResponse(doc_scores=docs)\n\t    except Exception as e:\n\t        logging.error(f\"query {request.query} error: {e}\")\n\t        raise HTTPException(status_code=500, detail=f\"str({e})\")\n\tasync def save_file_text(file: UploadFile) -> str:\n\t    \"\"\"\n\t    Extract text from file and save under index_doc_dir.\n", "    Return the absolute file path saved under index_doc_dir.\n\t    \"\"\"\n\t    # check file type. only support text file now.\n\t    mimetype = file.content_type\n\t    if mimetype is None:\n\t        mimetype, _ = mimetypes.guess_type(file.filename)\n\t    if mimetype != \"text/plain\" and mimetype != \"text/markdown\":\n\t        raise ValueError(f\"Unsupported file type: {mimetype}\")\n\t    # store the file text under index_doc_dir.\n\t    # TODO support other type file, extract the text from the file.\n", "    # TODO for small files, directly store in Lucene.\n\t    doc_path = os.path.join(index_doc_dir, file.filename)\n\t    os.makedirs(os.path.dirname(doc_path), exist_ok=True)\n\t    file_stream = await file.read()\n\t    # TODO if file exists, update doc\n\t    with open(doc_path, \"wb\") as f:\n\t        f.write(file_stream)\n\t    return doc_path\n"]}
{"filename": "model/model.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import List\n\tclass Model(ABC):\n\t    @abstractmethod\n\t    def get_max_token_size(self) -> int:\n\t        \"\"\"\n\t        Get the max number of tokens for the text. The input text longer than\n\t        the max token may be truncated.\n\t        \"\"\"\n\t        raise NotImplementedError\n", "    @abstractmethod\n\t    def get_dim(self) -> int:\n\t        \"\"\"\n\t        Return the embedding dimension\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def get_embeddings(self, texts: List[str]) -> List[List[float]]:\n\t        \"\"\"\n\t        Takes in a list of texts and returns a list of embeddings for each text.\n", "        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def set_model(self, model_name: str, max_token_size: int, dim: int):\n\t        \"\"\"\n\t        Set to use the specified model.\n\t        \"\"\"\n\t        raise NotImplementedError\n"]}
{"filename": "model/__init__.py", "chunked_list": []}
{"filename": "model/factory.py", "chunked_list": ["from model.model import Model\n\tdef get_model(provider: str) -> Model:\n\t    match provider:\n\t        case \"openai_embedding\":\n\t            from model.providers.openai_embedding import OpenAIEmbeddingModel\n\t            return OpenAIEmbeddingModel()\n\t        case \"sentence_transformer\":\n\t            from model.providers.sentence_transformer_model import \\\n\t                SentenceTransformerModel\n\t            return SentenceTransformerModel()\n", "        case _:\n\t            raise ValueError(f\"Unsupported model provider: {provider}\")\n"]}
{"filename": "model/providers/sentence_transformer_model.py", "chunked_list": ["from typing import List\n\tfrom sentence_transformers import SentenceTransformer\n\tfrom model.model import Model\n\tclass SentenceTransformerModel(Model):\n\t    model: SentenceTransformer\n\t    max_token_size: int\n\t    dim: int\n\t    def __init__(self):\n\t        \"\"\"\n\t        https://huggingface.co/blog/mteb, all-mpnet-base-v2 or all-MiniLM-L6-v2\n", "        provide a good balance between speed and performance.\n\t        https://www.sbert.net/docs/pretrained_models.html, test on a V100 GPU.\n\t        all-mpnet-base-v2, model size 420MB, encoding speed 2800 sentence/s.\n\t        all-MiniLM-L6-v2,  model size 80MB,  encoding speed 14200 sentence/s.\n\t        \"\"\"\n\t        # initialize with the default model\n\t        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n\t        # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\t        # By default, input text longer than 256 word pieces is truncated.\n\t        self.max_token_size = 256\n", "        self.dim = 384\n\t    def get_max_token_size(self) -> int:\n\t        \"\"\"\n\t        Return the max token for the text.\n\t        \"\"\"\n\t        # TODO depending on the tokenizer, 256 word pieces may not equal to\n\t        # 256 tokens.\n\t        return self.max_token_size\n\t    def get_dim(self) -> int:\n\t        \"\"\"\n", "        Return the embedding dimension\n\t        \"\"\"\n\t        return self.dim\n\t    def get_embeddings(self, texts: List[str]) -> List[List[float]]:\n\t        \"\"\"\n\t        Takes in a list of texts and returns a list of embeddings for each text.\n\t        \"\"\"\n\t        return self.model.encode(texts)\n\t    def set_model(self, model_name: str, max_token_size: int, dim: int):\n\t        \"\"\"\n", "        Set to use the specified model.\n\t        \"\"\"\n\t        self.model = SentenceTransformer(model_name)\n\t        self.max_token_size = max_token_size\n\t        self.dim = dim\n"]}
{"filename": "model/providers/openai_embedding.py", "chunked_list": ["from typing import List\n\tfrom openai import Embedding\n\tfrom tenacity import retry, wait_random_exponential, stop_after_attempt\n\tfrom model.model import Model\n\tclass OpenAIEmbeddingModel(Model):\n\t    # https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n\t    model_name: str\n\t    max_token_size: int\n\t    dim: int\n\t    def __init__(self):\n", "        self.model_name = \"text-embedding-ada-002\"\n\t        # what is the best token size? chatgpt-retrieval-plugin uses 200\n\t        self.max_token_size = 256\n\t        self.dim = 1536\n\t    def get_max_token_size(self) -> int:\n\t        \"\"\"\n\t        Return the max token for the text.\n\t        \"\"\"\n\t        return self.max_token_size\n\t    def get_dim(self) -> int:\n", "        \"\"\"\n\t        Return the embedding dimension\n\t        \"\"\"\n\t        return self.dim\n\t    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n\t    def get_embeddings(self, texts: List[str]) -> List[List[float]]:\n\t        \"\"\"\n\t        Takes in a list of texts and returns a list of embeddings for each text.\n\t        \"\"\"\n\t        # Call the OpenAI API to get the embeddings\n", "        response = Embedding.create(input=texts, model=self.model_name)\n\t        # Extract the embedding data from the response\n\t        data = response[\"data\"]  # type: ignore\n\t        # Return the embeddings as a list of lists of floats\n\t        return [result[\"embedding\"] for result in data]\n\t    def set_model(self, model_name: str, max_token_size: int, dim: int):\n\t        \"\"\"\n\t        Set to use the specified model.\n\t        \"\"\"\n\t        raise NotImplementedError\n"]}
{"filename": "example/cli.py", "chunked_list": ["import argparse\n\timport requests\n\timport sys\n\timport time\n\tdef upload_file(url: str, file_path: str):\n\t    with open(file_path, 'rb') as f:\n\t        resp = requests.post(\n\t            url=url, files={'file': (f.name, f, \"text/plain\")})\n\t        print(resp.json())\n\tdef upload_file_with_fields(url: str, file_path: str):\n", "    with open(file_path, 'rb') as f:\n\t        field1 = '{\"name\": \"field1\", \"string_value\": \"str1\"}'\n\t        field2 = '{\"name\": \"field2\", \"numeric_value\": 2}'\n\t        doc_fields = '{\"fields\": ' + f'[{field1}, {field2}]' + '}'\n\t        fields = {\"fields\": f'{doc_fields}'}\n\t        resp = requests.post(\n\t            url=url, files={'file': (f.name, f, \"text/plain\")}, data=fields)\n\t        print(resp.json())\n\tdef commit(url: str):\n\t    resp = requests.post(url=url)\n", "    print(resp.json())\n\tdef query(url: str, query_string: str, query_type: str):\n\t    query_request = '{' + f'\"query\": \"{query_string}\", ' + \\\n\t                    f'\"query_type\": \"{query_type}\"' + '}'\n\t    print(query_request)\n\t    resp = requests.get(url=url, data=query_request)\n\t    print(resp.json())\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--op\", type=str, required=True)\n", "    parser.add_argument(\"--host\", type=str, default=\"127.0.0.1\")\n\t    parser.add_argument(\"--port\", type=int, default=8080)\n\t    parser.add_argument(\"--file\", type=str)\n\t    parser.add_argument(\"--query_string\", type=str)\n\t    parser.add_argument(\"--query_type\", type=str, default=\"vector\",\n\t                        choices=[\"vector\", \"lucene\"])\n\t    args = parser.parse_args()\n\t    url = f\"http://{args.host}:{args.port}\"\n\t    match args.op:\n\t        case \"upload\":\n", "            if args.file is None:\n\t                print(\"please input the text file path\")\n\t            url += \"/add_doc\"\n\t            upload_file(url, args.file)\n\t        case \"commit\":\n\t            url += \"/commit\"\n\t            commit(url)\n\t        case \"query\":\n\t            if args.query_string is None:\n\t                print(\"please input the query string\")\n", "            url += \"/query\"\n\t            start = time.monotonic()\n\t            query(url, args.query_string, args.query_type)\n\t            dur = time.monotonic() - start\n\t            print(f\"{args.query_type} query time: {dur}s\")\n\t        case _:\n\t            print(\"supported op: upload, commit, query\")\n"]}
{"filename": "vectorstore/__init__.py", "chunked_list": []}
{"filename": "vectorstore/vectorstore.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom enum import Enum\n\tfrom typing import List\n\tclass Space(str, Enum):\n\t    l2 = \"l2\" # L2/Euclidean\n\t    ip = \"ip\" # inner/dot product\n\t    # The embedding model usually generates the normalized vectors. The cosine\n\t    # similarity similarity is a dot product on normalized vectors. Usually\n\t    # would not need to use cosine.\n\t    cosine = \"cosine\"\n", "class VectorStore(ABC):\n\t    @abstractmethod\n\t    def save(self, index_path: str):\n\t        \"\"\"\n\t        Save the vectors to the file specified by index_path.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def load(self, index_path: str):\n\t        \"\"\"\n", "        Load the vectors from the file specified by index_path.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def add(self, embeddings: List[List[float]], labels: List[int]):\n\t        \"\"\"\n\t        Add the embeddings and the corresponding labels.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n", "    def query(\n\t        self,\n\t        embeddings: List[List[float]],\n\t        top_k: int = 1,\n\t    ) -> (List[List[int]], List[List[float]]):\n\t        \"\"\"\n\t        Take one or more embeddings and return the top_k embedding ids and\n\t        distances for each embedding.\n\t        The distances are the original distances defined by the space, such as\n\t        L2, inner/dot product, etc. The vector store provider should return the\n", "        original distances.\n\t        \"\"\"\n\t        raise NotImplementedError\n"]}
{"filename": "vectorstore/factory.py", "chunked_list": ["from vectorstore.vectorstore import VectorStore\n\tdef get_vector_store(\n\t    store_name: str, dim: int, space: str, max_elements: int,\n\t) -> VectorStore:\n\t    match store_name:\n\t        case \"hnswlib\":\n\t            from vectorstore.providers.hnswlib_store import HnswlibStore\n\t            return HnswlibStore(dim, space, max_elements)\n\t        case _:\n\t            return ValueError(f\"Unsupported vector store: {store_name}\")\n"]}
{"filename": "vectorstore/providers/hnswlib_store.py", "chunked_list": ["import os\n\tfrom typing import List\n\timport hnswlib\n\tfrom vectorstore.vectorstore import Space, VectorStore\n\tclass HnswlibStore(VectorStore):\n\t    index: hnswlib.Index\n\t    # hnswlib params\n\t    dim: int\n\t    space: Space # ip, l2, or cosine\n\t    max_elements: int\n", "    # M: int # max number of connections on upper layers\n\t    # ef_construction: int # number of the nearest neighbors at index time\n\t    # ef_search: int # number of the nearest neighbors to search\n\t    def __init__(self, dim: int, space: Space, max_elements: int):\n\t        self.index = hnswlib.Index(space, dim)\n\t        self.index.init_index(max_elements)\n\t        self.dim = dim\n\t        self.max_elements = max_elements\n\t        self.space = space\n\t    def save(self, index_path: str):\n", "        self.index.save_index(index_path)\n\t    def load(self, index_path: str):\n\t        self.index.load_index(index_path, self.max_elements)\n\t    def add(self, embeddings: List[List[float]], labels: List[int]):\n\t        self.index.add_items(embeddings, labels)\n\t    def query(\n\t        self,\n\t        embeddings: List[List[float]],\n\t        top_k: int = 1,\n\t    ) -> (List[List[int]], List[List[float]]):\n", "        \"\"\"\n\t        Take one or more embeddings and return the top_k embedding labels and\n\t        the original distances, defined by space, for each embedding.\n\t        \"\"\"\n\t        labels, distances = self.index.knn_query(embeddings, top_k)\n\t        if self.space == Space.ip or self.space == Space.cosine:\n\t            # https://github.com/nmslib/hnswlib returns a slightly different\n\t            # distances, change back to the original distances.\n\t            distances = 1.0 - distances\n\t        return labels, distances\n"]}
