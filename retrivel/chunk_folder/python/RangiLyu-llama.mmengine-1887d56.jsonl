{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\t# Copyright (c) OpenMMLab. All rights reserved.\n\timport os\n\timport os.path as osp\n\timport platform\n\timport shutil\n\timport sys\n\timport warnings\n\tfrom setuptools import find_packages, setup\n\timport torch\n", "from torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension\n\tdef readme():\n\t    with open('README.md', encoding='utf-8') as f:\n\t        content = f.read()\n\t    return content\n\tversion_file = 'mmllama/version.py'\n\tdef get_version():\n\t    with open(version_file, 'r') as f:\n\t        exec(compile(f.read(), version_file, 'exec'))\n\t    return locals()['__version__']\n", "def make_cuda_ext(name, module, sources, sources_cuda=[]):\n\t    define_macros = []\n\t    extra_compile_args = {'cxx': []}\n\t    if torch.cuda.is_available() or os.getenv('FORCE_CUDA', '0') == '1':\n\t        define_macros += [('WITH_CUDA', None)]\n\t        extension = CUDAExtension\n\t        extra_compile_args['nvcc'] = [\n\t            '-D__CUDA_NO_HALF_OPERATORS__',\n\t            '-D__CUDA_NO_HALF_CONVERSIONS__',\n\t            '-D__CUDA_NO_HALF2_OPERATORS__',\n", "        ]\n\t        sources += sources_cuda\n\t    else:\n\t        print(f'Compiling {name} without CUDA')\n\t        extension = CppExtension\n\t    return extension(\n\t        name=f'{module}.{name}',\n\t        sources=[os.path.join(*module.split('.'), p) for p in sources],\n\t        define_macros=define_macros,\n\t        extra_compile_args=extra_compile_args)\n", "def parse_requirements(fname='requirements.txt', with_version=True):\n\t    \"\"\"Parse the package dependencies listed in a requirements file but strips\n\t    specific versioning information.\n\t    Args:\n\t        fname (str): path to requirements file\n\t        with_version (bool, default=False): if True include version specs\n\t    Returns:\n\t        List[str]: list of requirements items\n\t    CommandLine:\n\t        python -c \"import setup; print(setup.parse_requirements())\"\n", "    \"\"\"\n\t    import re\n\t    import sys\n\t    from os.path import exists\n\t    require_fpath = fname\n\t    def parse_line(line):\n\t        \"\"\"Parse information from a line in a requirements text file.\"\"\"\n\t        if line.startswith('-r '):\n\t            # Allow specifying requirements in other files\n\t            target = line.split(' ')[1]\n", "            for info in parse_require_file(target):\n\t                yield info\n\t        else:\n\t            info = {'line': line}\n\t            if line.startswith('-e '):\n\t                info['package'] = line.split('#egg=')[1]\n\t            elif '@git+' in line:\n\t                info['package'] = line\n\t            else:\n\t                # Remove versioning from the package\n", "                pat = '(' + '|'.join(['>=', '==', '>']) + ')'\n\t                parts = re.split(pat, line, maxsplit=1)\n\t                parts = [p.strip() for p in parts]\n\t                info['package'] = parts[0]\n\t                if len(parts) > 1:\n\t                    op, rest = parts[1:]\n\t                    if ';' in rest:\n\t                        # Handle platform specific dependencies\n\t                        # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\n\t                        version, platform_deps = map(str.strip,\n", "                                                     rest.split(';'))\n\t                        info['platform_deps'] = platform_deps\n\t                    else:\n\t                        version = rest  # NOQA\n\t                    info['version'] = (op, version)\n\t            yield info\n\t    def parse_require_file(fpath):\n\t        with open(fpath, 'r') as f:\n\t            for line in f.readlines():\n\t                line = line.strip()\n", "                if line and not line.startswith('#'):\n\t                    for info in parse_line(line):\n\t                        yield info\n\t    def gen_packages_items():\n\t        if exists(require_fpath):\n\t            for info in parse_require_file(require_fpath):\n\t                parts = [info['package']]\n\t                if with_version and 'version' in info:\n\t                    parts.extend(info['version'])\n\t                if not sys.version.startswith('3.4'):\n", "                    # apparently package_deps are broken in 3.4\n\t                    platform_deps = info.get('platform_deps')\n\t                    if platform_deps is not None:\n\t                        parts.append(';' + platform_deps)\n\t                item = ''.join(parts)\n\t                yield item\n\t    packages = list(gen_packages_items())\n\t    return packages\n\tdef add_mim_extension():\n\t    \"\"\"Add extra files that are required to support MIM into the package.\n", "    These files will be added by creating a symlink to the originals if the\n\t    package is installed in `editable` mode (e.g. pip install -e .), or by\n\t    copying from the originals otherwise.\n\t    \"\"\"\n\t    # parse installment mode\n\t    if 'develop' in sys.argv:\n\t        # installed by `pip install -e .`\n\t        if platform.system() == 'Windows':\n\t            # set `copy` mode here since symlink fails on Windows.\n\t            mode = 'copy'\n", "        else:\n\t            mode = 'symlink'\n\t    elif 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:\n\t        # installed by `pip install .`\n\t        # or create source distribution by `python setup.py sdist`\n\t        mode = 'copy'\n\t    else:\n\t        return\n\t    filenames = ['tools', 'configs', 'demo', 'model-index.yml']\n\t    repo_path = osp.dirname(__file__)\n", "    mim_path = osp.join(repo_path, 'mmllama', '.mim')\n\t    os.makedirs(mim_path, exist_ok=True)\n\t    for filename in filenames:\n\t        if osp.exists(filename):\n\t            src_path = osp.join(repo_path, filename)\n\t            tar_path = osp.join(mim_path, filename)\n\t            if osp.isfile(tar_path) or osp.islink(tar_path):\n\t                os.remove(tar_path)\n\t            elif osp.isdir(tar_path):\n\t                shutil.rmtree(tar_path)\n", "            if mode == 'symlink':\n\t                src_relpath = osp.relpath(src_path, osp.dirname(tar_path))\n\t                os.symlink(src_relpath, tar_path)\n\t            elif mode == 'copy':\n\t                if osp.isfile(src_path):\n\t                    shutil.copyfile(src_path, tar_path)\n\t                elif osp.isdir(src_path):\n\t                    shutil.copytree(src_path, tar_path)\n\t                else:\n\t                    warnings.warn(f'Cannot copy file {src_path}.')\n", "            else:\n\t                raise ValueError(f'Invalid mode {mode}')\n\tif __name__ == '__main__':\n\t    add_mim_extension()\n\t    setup(\n\t        name='mmllama',\n\t        version=get_version(),\n\t        description='llama with mmengine',\n\t        long_description=readme(),\n\t        long_description_content_type='text/markdown',\n", "        author='RangiLyu',\n\t        author_email='lyuchqi@gmail.com',\n\t        keywords='computer vision, object detection',\n\t        url='https://github.com/RangiLyu/llama.mmengine',\n\t        packages=find_packages(exclude=('configs', 'tools', 'demo')),\n\t        include_package_data=True,\n\t        classifiers=[\n\t            'Development Status :: 5 - Production/Stable',\n\t            'License :: OSI Approved :: Apache Software License',\n\t            'Operating System :: OS Independent',\n", "            'Programming Language :: Python :: 3',\n\t            'Programming Language :: Python :: 3.10',\n\t        ],\n\t        license='Apache License 2.0',\n\t        install_requires=parse_requirements('requirements.txt'),\n\t        extras_require={\n\t            'all': parse_requirements('requirements.txt'),\n\t        },\n\t        ext_modules=[],\n\t        cmdclass={'build_ext': BuildExtension},\n", "        zip_safe=False)\n"]}
{"filename": "tools/train.py", "chunked_list": ["import argparse\n\timport os\n\timport os.path as osp\n\tfrom datasets import load_dataset\n\tfrom mmengine.config import Config, DictAction\n\tfrom transformers import DataCollatorForSeq2Seq, LlamaTokenizer\n\tfrom mmllama.datasets import Prompter, seq2seq_collate\n\tfrom mmllama.engine import Runner\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser(description='Train a detector')\n", "    parser.add_argument('config', help='train config file path')\n\t    parser.add_argument('--work-dir', help='the dir to save logs and models')\n\t    parser.add_argument(\n\t        '--resume',\n\t        nargs='?',\n\t        type=str,\n\t        const='auto',\n\t        help='If specify checkpoint path, resume from it, while if not '\n\t        'specify, try to auto resume from the latest checkpoint '\n\t        'in the work directory.')\n", "    parser.add_argument(\n\t        '--cfg-options',\n\t        nargs='+',\n\t        action=DictAction,\n\t        help='override some settings in the used config, the key-value pair '\n\t        'in xxx=yyy format will be merged into config file. If the value to '\n\t        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n\t        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n\t        'Note that the quotation marks are necessary and that no white space '\n\t        'is allowed.')\n", "    parser.add_argument(\n\t        '--launcher',\n\t        choices=['none', 'pytorch', 'slurm', 'mpi'],\n\t        default='none',\n\t        help='job launcher')\n\t    parser.add_argument('--local_rank', type=int, default=0)\n\t    args = parser.parse_args()\n\t    if 'LOCAL_RANK' not in os.environ:\n\t        os.environ['LOCAL_RANK'] = str(args.local_rank)\n\t    return args\n", "def main():\n\t    args = parse_args()\n\t    # load config\n\t    cfg = Config.fromfile(args.config)\n\t    cfg.launcher = args.launcher\n\t    if args.cfg_options is not None:\n\t        cfg.merge_from_dict(args.cfg_options)\n\t    # work_dir is determined in this priority: CLI > segment in file > filename\n\t    if args.work_dir is not None:\n\t        # update configs according to CLI args if args.work_dir is not None\n", "        cfg.work_dir = args.work_dir\n\t    elif cfg.get('work_dir', None) is None:\n\t        # use config filename as default work_dir if cfg.work_dir is None\n\t        cfg.work_dir = osp.join('./work_dirs',\n\t                                osp.splitext(osp.basename(args.config))[0])\n\t    if cfg.data_path.endswith('.json') or cfg.data_path.endswith('.jsonl'):\n\t        data = load_dataset('json', data_files=cfg.data_path)\n\t    else:\n\t        data = load_dataset(cfg.data_path)\n\t    tokenizer = LlamaTokenizer(cfg.tokenizer_path)\n", "    tokenizer.pad_token_id = (\n\t        0  # unk. we want this to be different from the eos token\n\t    )\n\t    tokenizer.padding_side = 'left'  # Allow batched inference\n\t    # TODO: move hyps to cfg\n\t    cutoff_len: int = 256\n\t    prompt_template_name: str = 'alpaca'\n\t    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n\t    prompter = Prompter(prompt_template_name)\n\t    def tokenize(prompt, add_eos_token=True):\n", "        # there's probably a way to do this with the tokenizer settings\n\t        # but again, gotta move fast\n\t        result = tokenizer(\n\t            prompt,\n\t            truncation=True,\n\t            max_length=cutoff_len,\n\t            padding=False,\n\t            return_tensors=None,\n\t        )\n\t        if (\n", "            result['input_ids'][-1] != tokenizer.eos_token_id\n\t            and len(result['input_ids']) < cutoff_len\n\t            and add_eos_token\n\t        ):\n\t            result['input_ids'].append(tokenizer.eos_token_id)\n\t            result['attention_mask'].append(1)\n\t        result['labels'] = result['input_ids'].copy()\n\t        return result\n\t    def generate_and_tokenize_prompt(data_point):\n\t        full_prompt = prompter.generate_prompt(\n", "            data_point['instruction'],\n\t            data_point['input'],\n\t            data_point['output'],\n\t        )\n\t        tokenized_full_prompt = tokenize(full_prompt)\n\t        if not train_on_inputs:\n\t            user_prompt = prompter.generate_prompt(\n\t                data_point['instruction'], data_point['input']\n\t            )\n\t            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n", "            user_prompt_len = len(tokenized_user_prompt['input_ids'])\n\t            tokenized_full_prompt['labels'] = [\n\t                -100\n\t            ] * user_prompt_len + tokenized_full_prompt['labels'][\n\t                user_prompt_len:\n\t            ]  # could be sped up, probably\n\t        return tokenized_full_prompt\n\t    train_val = data['train'].train_test_split(\n\t        test_size=cfg.val_set_size, shuffle=True, seed=42\n\t    )\n", "    train_data = train_val['train'].shuffle().map(generate_and_tokenize_prompt)\n\t    val_data = train_val['test'].shuffle().map(generate_and_tokenize_prompt)\n\t    # collator = DataCollatorForSeq2Seq(\n\t    #         tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n\t    #     )\n\t    from mmengine.registry import FUNCTIONS\n\t    FUNCTIONS.register_module(name='seq2seq_collate', module=seq2seq_collate)\n\t    cfg.train_dataloader.dataset = train_data.remove_columns(('instruction', 'input', 'output'))\n\t    cfg.train_dataloader.collate_fn.tokenizer = tokenizer\n\t    cfg.val_dataloader.dataset = val_data.remove_columns(('instruction', 'input', 'output'))\n", "    cfg.val_dataloader.collate_fn.tokenizer = tokenizer\n\t    # resume is determined in this priority: resume from > auto_resume\n\t    if args.resume == 'auto':\n\t        cfg.resume = True\n\t        cfg.load_from = None\n\t    elif args.resume is not None:\n\t        cfg.resume = True\n\t        cfg.load_from = args.resume\n\t    runner = Runner.from_cfg(cfg)\n\t    # start training\n", "    runner.train()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "tools/convert_ckeckpoint.py", "chunked_list": ["# from https://github.com/Lightning-AI/lit-llama/blob/main/scripts/convert_checkpoint.py\n\timport os\n\timport shutil\n\tfrom pathlib import Path\n\tfrom typing import Dict\n\timport torch\n\tfrom tqdm import tqdm\n\t\"\"\"\n\tSample usage:\n\t```bash\n", "python -m scripts.convert_checkpoint -h\n\tpython -m scripts.convert_checkpoint converted\n\t```\n\t\"\"\"\n\tdef convert_state_dict(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n\t    converted = {}\n\t    converted['transformer.wte.weight'] = state_dict['tok_embeddings.weight']\n\t    converted['lm_head.weight'] = state_dict['output.weight']\n\t    converted['transformer.ln_f.scale'] = state_dict['norm.weight']\n\t    for key in [k for k in state_dict if k.startswith('layers')]:\n", "        layer_idx = key.split('.')[1]\n\t        # attention\n\t        # the wq, wk, wv from the FB model are stacked in our model as c_attn\n\t        converted[f'transformer.h.{layer_idx}.attn.c_attn.weight'] = torch.cat(\n\t            (\n\t                state_dict[f'layers.{layer_idx}.attention.wq.weight'],\n\t                state_dict[f'layers.{layer_idx}.attention.wk.weight'],\n\t                state_dict[f'layers.{layer_idx}.attention.wv.weight'],\n\t            )\n\t        )\n", "        converted[f'transformer.h.{layer_idx}.attn.c_proj.weight'] = state_dict[\n\t            f'layers.{layer_idx}.attention.wo.weight'\n\t        ]\n\t        # mlp\n\t        converted[f'transformer.h.{layer_idx}.mlp.c_fc1.weight'] = state_dict[\n\t            f'layers.{layer_idx}.feed_forward.w1.weight'\n\t        ]\n\t        converted[f'transformer.h.{layer_idx}.mlp.c_proj.weight'] = state_dict[\n\t            f'layers.{layer_idx}.feed_forward.w2.weight'\n\t        ]\n", "        converted[f'transformer.h.{layer_idx}.mlp.c_fc2.weight'] = state_dict[\n\t            f'layers.{layer_idx}.feed_forward.w3.weight'\n\t        ]\n\t        # rms norm\n\t        converted[f'transformer.h.{layer_idx}.rms_1.scale'] = state_dict[f'layers.{layer_idx}.attention_norm.weight']\n\t        converted[f'transformer.h.{layer_idx}.rms_2.scale'] = state_dict[f'layers.{layer_idx}.ffn_norm.weight']\n\t    return converted\n\tdef meta_weights_for_nano_model(\n\t    *,\n\t    output_dir: Path = Path('checkpoints/mm-llama'),\n", "    ckpt_dir: Path = Path('checkpoints/llama/'),\n\t    tokenizer_path: Path = Path('checkpoints/llama/tokenizer.model'),\n\t    model_size: str = '7B',\n\t) -> None:\n\t    output_dir = output_dir / model_size\n\t    ckpt_dir = ckpt_dir / model_size\n\t    os.makedirs(output_dir, exist_ok=True)\n\t    # the tokenizer is the same for all model sizes, so we store it in the parent dir\n\t    if 'tokenizer.model' not in os.listdir(output_dir.parent):\n\t        shutil.copy(tokenizer_path, output_dir.parent)\n", "    checkpoint_files = sorted(ckpt_dir.glob('*.pth'))\n\t    # for the bigger models, there are multiple model-parallel checkpoints\n\t    # and we combine them into one single file\n\t    combined = {}\n\t    for file in tqdm(checkpoint_files, total=len(checkpoint_files)):\n\t        checkpoint = torch.load(file, map_location='cpu')\n\t        converted = convert_state_dict(checkpoint)\n\t        combined.update(converted)\n\t    torch.save(combined, Path(output_dir, 'state_dict.pth'))\n\tif __name__ == '__main__':\n", "    from jsonargparse import CLI\n\t    CLI(meta_weights_for_nano_model)\n"]}
{"filename": "tools/generate.py", "chunked_list": ["import argparse\n\timport torch\n\tfrom mmengine.config import Config, DictAction\n\tfrom mmengine.logging import print_log\n\tfrom transformers import LlamaTokenizer\n\tfrom mmllama.datasets import Prompter\n\tfrom mmllama.registry import MODELS\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser(\n\t        description='MMDet test (and eval) a model')\n", "    parser.add_argument('config', help='test config file path')\n\t    parser.add_argument('checkpoint', help='checkpoint file')\n\t    parser.add_argument(\n\t        '--cfg-options',\n\t        nargs='+',\n\t        action=DictAction,\n\t        help='override some settings in the used config, the key-value pair '\n\t        'in xxx=yyy format will be merged into config file. If the value to '\n\t        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n\t        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n", "        'Note that the quotation marks are necessary and that no white space '\n\t        'is allowed.')\n\t    parser.add_argument(\n\t        '--instructions',\n\t        nargs='+',\n\t        help='instructions to generate responses')\n\t    args = parser.parse_args()\n\t    return args\n\tdef main():\n\t    args = parse_args()\n", "    # load config\n\t    cfg = Config.fromfile(args.config)\n\t    if args.cfg_options is not None:\n\t        cfg.merge_from_dict(args.cfg_options)\n\t    print_log('Building model', logger='current')\n\t    model = MODELS.build(cfg.model)\n\t    model.init_weights()\n\t    model.load_state_dict(torch.load(args.checkpoint)['state_dict'], strict=False)\n\t    model.half()\n\t    model.cuda()\n", "    model.eval()\n\t    print_log('Finished building model', logger='current')\n\t    tokenizer = LlamaTokenizer(cfg.tokenizer_path)\n\t    tokenizer.pad_token_id = (\n\t        0  # unk. we want this to be different from the eos token\n\t    )\n\t    tokenizer.padding_side = 'left'  # Allow batched inference\n\t    prompter = Prompter('alpaca')\n\t    def evaluate(\n\t        instruction,\n", "        input=None,\n\t        temperature=0.1,\n\t        top_p=0.75,\n\t        top_k=40,\n\t        num_beams=4,\n\t        max_new_tokens=128,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"Generate a response to an instruction.\n\t        Modified from https://github.com/tloen/alpaca-lora\n", "        \"\"\"\n\t        prompt = prompter.generate_prompt(instruction, input)\n\t        inputs = tokenizer(prompt, return_tensors='pt')\n\t        input_ids = inputs['input_ids'].to('cuda')\n\t        model.model.test_cfg.temperature=temperature\n\t        model.model.test_cfg.top_k=top_k\n\t        model.model.test_cfg.max_new_tokens=max_new_tokens\n\t        # TODO: beam search\n\t        with torch.no_grad():\n\t            generation_output = model(input_ids, mode='predict')\n", "        s = generation_output[0]\n\t        output = tokenizer.decode(s)\n\t        return prompter.get_response(output)\n\t    if args.instructions is not None:\n\t        instructions = args.instructions\n\t    else:\n\t        instructions = [\n\t        'Tell me about alpacas.',\n\t        'Tell me about the president of Mexico in 2019.',\n\t        'Tell me about the king of France in 2019.',\n", "        'List all Canadian provinces in alphabetical order.',\n\t        'Write a Python program that prints the first 10 Fibonacci numbers.',\n\t        \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n\t        \"Tell me five words that rhyme with 'shock'.\",\n\t        \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n\t        'Count up from 1 to 500.',\n\t    ]\n\t    for instruction in instructions:\n\t        print('Instruction:', instruction)\n\t        print('Response:', evaluate(instruction))\n", "        print()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "mmllama/version.py", "chunked_list": ["__version__ = '0.0.1'\n\tshort_version = __version__\n\tdef parse_version_info(version_str):\n\t    \"\"\"Parse a version string into a tuple.\n\t    Args:\n\t        version_str (str): The version string.\n\t    Returns:\n\t        tuple[int | str]: The version info, e.g., \"1.3.0\" is parsed into\n\t            (1, 3, 0), and \"2.0.0rc1\" is parsed into (2, 0, 0, 'rc1').\n\t    \"\"\"\n", "    version_info = []\n\t    for x in version_str.split('.'):\n\t        if x.isdigit():\n\t            version_info.append(int(x))\n\t        elif x.find('rc') != -1:\n\t            patch_version = x.split('rc')\n\t            version_info.append(int(patch_version[0]))\n\t            version_info.append(f'rc{patch_version[1]}')\n\t    return tuple(version_info)\n\tversion_info = parse_version_info(__version__)\n"]}
{"filename": "mmllama/registry.py", "chunked_list": ["\"\"\"MMEngine provides 17 registry nodes to support using modules across\n\tprojects. Each node is a child of the root registry in MMEngine.\n\tMore details can be found at\n\thttps://mmengine.readthedocs.io/en/latest/tutorials/registry.html.\n\t\"\"\"\n\tfrom mmengine.registry import DATA_SAMPLERS as MMENGINE_DATA_SAMPLERS\n\tfrom mmengine.registry import DATASETS as MMENGINE_DATASETS\n\tfrom mmengine.registry import EVALUATOR as MMENGINE_EVALUATOR\n\tfrom mmengine.registry import HOOKS as MMENGINE_HOOKS\n\tfrom mmengine.registry import LOG_PROCESSORS as MMENGINE_LOG_PROCESSORS\n", "from mmengine.registry import LOOPS as MMENGINE_LOOPS\n\tfrom mmengine.registry import METRICS as MMENGINE_METRICS\n\tfrom mmengine.registry import MODEL_WRAPPERS as MMENGINE_MODEL_WRAPPERS\n\tfrom mmengine.registry import MODELS as MMENGINE_MODELS\n\tfrom mmengine.registry import \\\n\t    OPTIM_WRAPPER_CONSTRUCTORS as MMENGINE_OPTIM_WRAPPER_CONSTRUCTORS\n\tfrom mmengine.registry import OPTIM_WRAPPERS as MMENGINE_OPTIM_WRAPPERS\n\tfrom mmengine.registry import OPTIMIZERS as MMENGINE_OPTIMIZERS\n\tfrom mmengine.registry import PARAM_SCHEDULERS as MMENGINE_PARAM_SCHEDULERS\n\tfrom mmengine.registry import RUNNER_CONSTRUCTORS as MMENGINE_RUNNER_CONSTRUCTORS\n", "from mmengine.registry import RUNNERS as MMENGINE_RUNNERS\n\tfrom mmengine.registry import TASK_UTILS as MMENGINE_TASK_UTILS\n\tfrom mmengine.registry import TRANSFORMS as MMENGINE_TRANSFORMS\n\tfrom mmengine.registry import VISBACKENDS as MMENGINE_VISBACKENDS\n\tfrom mmengine.registry import VISUALIZERS as MMENGINE_VISUALIZERS\n\tfrom mmengine.registry import WEIGHT_INITIALIZERS as MMENGINE_WEIGHT_INITIALIZERS\n\tfrom mmengine.registry import Registry\n\t# manage all kinds of runners like `EpochBasedRunner` and `IterBasedRunner`\n\tRUNNERS = Registry(\n\t    'runner', parent=MMENGINE_RUNNERS, locations=['mmllama.engine.runner'])\n", "# manage runner constructors that define how to initialize runners\n\tRUNNER_CONSTRUCTORS = Registry(\n\t    'runner constructor',\n\t    parent=MMENGINE_RUNNER_CONSTRUCTORS,\n\t    locations=['mmllama.engine.runner'])\n\t# manage all kinds of loops like `EpochBasedTrainLoop`\n\tLOOPS = Registry(\n\t    'loop', parent=MMENGINE_LOOPS, locations=['mmllama.engine.runner'])\n\t# manage all kinds of hooks like `CheckpointHook`\n\tHOOKS = Registry(\n", "    'hook', parent=MMENGINE_HOOKS, locations=['mmllama.engine.hooks'])\n\t# manage data-related modules\n\tDATASETS = Registry(\n\t    'dataset', parent=MMENGINE_DATASETS, locations=['mmllama.datasets'])\n\tDATA_SAMPLERS = Registry(\n\t    'data sampler',\n\t    parent=MMENGINE_DATA_SAMPLERS,\n\t    locations=['mmllama.datasets.samplers'])\n\tTRANSFORMS = Registry(\n\t    'transform',\n", "    parent=MMENGINE_TRANSFORMS,\n\t    locations=['mmllama.datasets.transforms'])\n\t# manage all kinds of modules inheriting `nn.Module`\n\tMODELS = Registry(\n\t    'model', parent=MMENGINE_MODELS, locations=['mmllama.models'])\n\t# manage all kinds of model wrappers like 'MMDistributedDataParallel'\n\tMODEL_WRAPPERS = Registry(\n\t    'model_wrapper',\n\t    parent=MMENGINE_MODEL_WRAPPERS,\n\t    locations=['mmllama.models'])\n", "# manage all kinds of weight initialization modules like `Uniform`\n\tWEIGHT_INITIALIZERS = Registry(\n\t    'weight initializer',\n\t    parent=MMENGINE_WEIGHT_INITIALIZERS,\n\t    locations=['mmllama.models'])\n\t# manage all kinds of optimizers like `SGD` and `Adam`\n\tOPTIMIZERS = Registry(\n\t    'optimizer',\n\t    parent=MMENGINE_OPTIMIZERS,\n\t    locations=['mmllama.engine.optimizers'])\n", "# manage optimizer wrapper\n\tOPTIM_WRAPPERS = Registry(\n\t    'optim_wrapper',\n\t    parent=MMENGINE_OPTIM_WRAPPERS,\n\t    locations=['mmllama.engine.optimizers'])\n\t# manage constructors that customize the optimization hyperparameters.\n\tOPTIM_WRAPPER_CONSTRUCTORS = Registry(\n\t    'optimizer constructor',\n\t    parent=MMENGINE_OPTIM_WRAPPER_CONSTRUCTORS,\n\t    locations=['mmllama.engine.optimizers'])\n", "# manage all kinds of parameter schedulers like `MultiStepLR`\n\tPARAM_SCHEDULERS = Registry(\n\t    'parameter scheduler',\n\t    parent=MMENGINE_PARAM_SCHEDULERS,\n\t    locations=['mmllama.engine.schedulers'])\n\t# manage all kinds of metrics\n\tMETRICS = Registry(\n\t    'metric', parent=MMENGINE_METRICS, locations=['mmllama.evaluation'])\n\t# manage evaluator\n\tEVALUATOR = Registry(\n", "    'evaluator', parent=MMENGINE_EVALUATOR, locations=['mmllama.evaluation'])\n\t# manage task-specific modules like anchor generators and box coders\n\tTASK_UTILS = Registry(\n\t    'task util', parent=MMENGINE_TASK_UTILS, locations=['mmllama.models'])\n\t# manage visualizer\n\tVISUALIZERS = Registry(\n\t    'visualizer',\n\t    parent=MMENGINE_VISUALIZERS,\n\t    locations=['mmllama.visualization'])\n\t# manage visualizer backend\n", "VISBACKENDS = Registry(\n\t    'vis_backend',\n\t    parent=MMENGINE_VISBACKENDS,\n\t    locations=['mmllama.visualization'])\n\t# manage logprocessor\n\tLOG_PROCESSORS = Registry(\n\t    'log_processor',\n\t    parent=MMENGINE_LOG_PROCESSORS,\n\t    # TODO: update the location when mmllama has its own log processor\n\t    locations=['mmllama.engine'])\n"]}
{"filename": "mmllama/__init__.py", "chunked_list": ["import mmengine\n\tfrom mmengine.utils import digit_version\n\tfrom .version import __version__, version_info\n\tmmengine_minimum_version = '0.7.0'\n\tmmengine_maximum_version = '1.0.0'\n\tmmengine_version = digit_version(mmengine.__version__)\n\tassert (mmengine_version >= digit_version(mmengine_minimum_version)\n\t        and mmengine_version < digit_version(mmengine_maximum_version)), \\\n\t    f'MMEngine=={mmengine.__version__} is used but incompatible. ' \\\n\t    f'Please install mmengine>={mmengine_minimum_version}, ' \\\n", "    f'<{mmengine_maximum_version}.'\n\t__all__ = ['__version__', 'version_info', 'digit_version']\n"]}
{"filename": "mmllama/evaluation/metrics.py", "chunked_list": ["from typing import Dict, List, Optional, Sequence, Union\n\tfrom mmengine.evaluator import BaseMetric\n\tfrom mmllama.registry import METRICS\n\t@METRICS.register_module()\n\tclass DummyMetric(BaseMetric):\n\t    \"\"\"\n\t    TODO: implement a metric\n\t    \"\"\"\n\t    default_prefix: Optional[str] = 'metric'\n\t    def __init__(self,\n", "                 collect_device: str = 'cpu',\n\t                 prefix: Optional[str] = None) -> None:\n\t        super().__init__(collect_device=collect_device, prefix=prefix)\n\t    # TODO: data_batch is no longer needed, consider adjusting the\n\t    #  parameter position\n\t    def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:\n\t        \"\"\"Process one batch of data samples and predictions. The processed\n\t        results should be stored in ``self.results``, which will be used to\n\t        compute the metrics when all batches have been processed.\n\t        Args:\n", "            data_batch (dict): A batch of data from the dataloader.\n\t            data_samples (Sequence[dict]): A batch of data samples that\n\t                contain annotations and predictions.\n\t        \"\"\"\n\t        for data_sample in data_samples:\n\t            result = dict()\n\t            self.results.append(result)\n\t    def compute_metrics(self, results: list) -> Dict[str, float]:\n\t        return {}\n"]}
{"filename": "mmllama/evaluation/__init__.py", "chunked_list": ["from .metrics import DummyMetric\n"]}
{"filename": "mmllama/engine/runner.py", "chunked_list": ["import logging\n\timport time\n\timport warnings\n\tfrom typing import Dict, List, Optional, Union\n\timport mmengine\n\tfrom mmengine.config import Config, ConfigDict\n\tfrom mmengine.dist import master_only\n\tfrom mmengine.fileio import FileClient, join_path\n\tfrom mmengine.logging import print_log\n\tfrom mmengine.model import is_model_wrapper\n", "from mmengine.optim import OptimWrapper, OptimWrapperDict, _ParamScheduler\n\tfrom mmengine.runner.checkpoint import save_checkpoint, weights_to_cpu\n\tfrom mmengine.utils import get_git_hash\n\tConfigType = Union[Dict, Config, ConfigDict]\n\tParamSchedulerType = Union[List[_ParamScheduler], Dict[str,\n\t                                                       List[_ParamScheduler]]]\n\tOptimWrapperType = Union[OptimWrapper, OptimWrapperDict]\n\tfrom mmengine.runner import Runner as _Runner\n\tclass Runner(_Runner):\n\t    @master_only\n", "    def save_checkpoint(\n\t        self,\n\t        out_dir: str,\n\t        filename: str,\n\t        file_client_args: Optional[dict] = None,\n\t        save_optimizer: bool = True,\n\t        save_param_scheduler: bool = True,\n\t        meta: dict = None,\n\t        by_epoch: bool = True,\n\t        backend_args: Optional[dict] = None,\n", "    ):\n\t        \"\"\"Save checkpoints.\n\t        ``CheckpointHook`` invokes this method to save checkpoints\n\t        periodically.\n\t        Args:\n\t            out_dir (str): The directory that checkpoints are saved.\n\t            filename (str): The checkpoint filename.\n\t            file_client_args (dict, optional): Arguments to instantiate a\n\t                FileClient. See :class:`mmengine.fileio.FileClient` for\n\t                details. Defaults to None. It will be deprecated in future.\n", "                Please use `backend_args` instead.\n\t            save_optimizer (bool): Whether to save the optimizer to\n\t                the checkpoint. Defaults to True.\n\t            save_param_scheduler (bool): Whether to save the param_scheduler\n\t                to the checkpoint. Defaults to True.\n\t            meta (dict, optional): The meta information to be saved in the\n\t                checkpoint. Defaults to None.\n\t            by_epoch (bool): Whether the scheduled momentum is updated by\n\t                epochs. Defaults to True.\n\t            backend_args (dict, optional): Arguments to instantiate the\n", "                prefix of uri corresponding backend. Defaults to None.\n\t                New in v0.2.0.\n\t        \"\"\"\n\t        if meta is None:\n\t            meta = {}\n\t        elif not isinstance(meta, dict):\n\t            raise TypeError(\n\t                f'meta should be a dict or None, but got {type(meta)}')\n\t        if by_epoch:\n\t            # self.epoch increments 1 after\n", "            # `self.call_hook('after_train_epoch)` but `save_checkpoint` is\n\t            # called by `after_train_epoch`` method of `CheckpointHook` so\n\t            # `epoch` should be `self.epoch + 1`\n\t            meta.update(epoch=self.epoch + 1, iter=self.iter)\n\t        else:\n\t            meta.update(epoch=self.epoch, iter=self.iter + 1)\n\t        if file_client_args is not None:\n\t            warnings.warn(\n\t                '\"file_client_args\" will be deprecated in future. '\n\t                'Please use \"backend_args\" instead', DeprecationWarning)\n", "            if backend_args is not None:\n\t                raise ValueError(\n\t                    '\"file_client_args\" and \"backend_args\" cannot be set at '\n\t                    'the same time.')\n\t            file_client = FileClient.infer_client(file_client_args, out_dir)\n\t            filepath = file_client.join_path(out_dir, filename)\n\t        else:\n\t            filepath = join_path(  # type: ignore\n\t                out_dir, filename, backend_args=backend_args)\n\t        meta.update(\n", "            cfg=self.cfg.pretty_text,\n\t            seed=self.seed,\n\t            experiment_name=self.experiment_name,\n\t            time=time.strftime('%Y%m%d_%H%M%S', time.localtime()),\n\t            mmengine_version=mmengine.__version__ + get_git_hash())\n\t        if hasattr(self.train_dataloader.dataset, 'metainfo'):\n\t            meta.update(dataset_meta=self.train_dataloader.dataset.metainfo)\n\t        if is_model_wrapper(self.model):\n\t            model = self.model.module\n\t        else:\n", "            model = self.model\n\t        checkpoint = {\n\t            'meta': meta,\n\t            'state_dict': weights_to_cpu(model.state_dict()),\n\t            'message_hub': self.message_hub.state_dict()\n\t        }\n\t        # save optimizer state dict to checkpoint\n\t        if save_optimizer:\n\t            if isinstance(self.optim_wrapper, OptimWrapper):\n\t                checkpoint['optimizer'] = self.optim_wrapper.state_dict()\n", "            else:\n\t                raise TypeError(\n\t                    'self.optim_wrapper should be an `OptimWrapper` '\n\t                    'or `OptimWrapperDict` instance, but got '\n\t                    f'{self.optim_wrapper}')\n\t        # save param scheduler state dict\n\t        if save_param_scheduler and self.param_schedulers is None:\n\t            print_log(\n\t                '`save_param_scheduler` is True but `self.param_schedulers` '\n\t                'is None, so skip saving parameter schedulers',\n", "                logger='current',\n\t                level=logging.WARNING)\n\t            save_param_scheduler = False\n\t        if save_param_scheduler:\n\t            if isinstance(self.param_schedulers, dict):\n\t                checkpoint['param_schedulers'] = dict()\n\t                for name, schedulers in self.param_schedulers.items():\n\t                    checkpoint['param_schedulers'][name] = []\n\t                    for scheduler in schedulers:\n\t                        state_dict = scheduler.state_dict()\n", "                        checkpoint['param_schedulers'][name].append(state_dict)\n\t            else:\n\t                checkpoint['param_schedulers'] = []\n\t                for scheduler in self.param_schedulers:  # type: ignore\n\t                    state_dict = scheduler.state_dict()  # type: ignore\n\t                    checkpoint['param_schedulers'].append(state_dict)\n\t        self.call_hook('before_save_checkpoint', checkpoint=checkpoint)\n\t        save_checkpoint(checkpoint, filepath)\n"]}
{"filename": "mmllama/engine/__init__.py", "chunked_list": ["from .runner import Runner\n\t__all__ = ['Runner']\n"]}
{"filename": "mmllama/models/llama.py", "chunked_list": ["\"\"\"Full definition of a LLaMA Language Model, all of it in this single file.\n\tModified from https://github.com/Lightning-AI/lit-llama/blob/main/lit_llama/model.py\n\t\"\"\"\n\timport math\n\tfrom functools import partial\n\timport torch\n\timport torch.nn as nn\n\tfrom mmengine import Config\n\tfrom mmengine.logging import print_log\n\tfrom mmengine.model import BaseModel\n", "from torch.nn import functional as F\n\tfrom mmllama.registry import MODELS\n\tdef build_rope_cache(seq_len: int,\n\t                     n_elem: int,\n\t                     dtype: torch.dtype,\n\t                     base: int = 10000) -> torch.Tensor:\n\t    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n\t    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n\t    transformers/rope/__init__.py. MIT License:\n\t    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n", "    \"\"\"\n\t    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n\t    theta = 1.0 / (base**(torch.arange(0, n_elem, 2, dtype=dtype) / n_elem))\n\t    # Create position indexes `[0, 1, ..., seq_len - 1]`\n\t    seq_idx = torch.arange(seq_len, dtype=dtype)\n\t    # Calculate the product of position index and $\\theta_i$\n\t    idx_theta = torch.outer(seq_idx, theta)\n\t    # Cache them\n\t    cache = torch.polar(torch.ones_like(idx_theta), idx_theta)  # complex64\n\t    return cache\n", "def apply_rope(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n\t    x = x.transpose(1, 2)\n\t    # truncate to support variable sizes\n\t    T = x.size(1)\n\t    rope_cache = rope_cache[:T]\n\t    # cast because `view_as_complex` does not support 16 bit tensors\n\t    xc = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n\t    rope_cache = rope_cache.view(1, xc.size(1), 1, xc.size(3))\n\t    x_out = torch.view_as_real(xc * rope_cache).flatten(3)\n\t    return x_out.transpose(1, 2).type_as(x)\n", "class RMSNorm(nn.Module):\n\t    \"\"\"Root Mean Square Layer Normalization.\n\t    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n\t    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n\t    \"\"\"\n\t    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n\t        super().__init__()\n\t        self.scale = nn.Parameter(torch.ones(size))\n\t        self.eps = eps\n\t        self.dim = dim\n", "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        # NOTE: the original RMSNorm paper implementation is not equivalent\n\t        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n\t        # rms_x = norm_x * d_x ** (-1. / 2)\n\t        # x_normed = x / (rms_x + self.eps)\n\t        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n\t        x_normed = x * torch.rsqrt(norm_x + self.eps)\n\t        return self.scale * x_normed\n\tclass CausalSelfAttention(nn.Module):\n\t    def __init__(self, n_embd, n_head, rope_cache: torch.Tensor) -> None:\n", "        super().__init__()\n\t        assert n_embd % n_head == 0\n\t        # key, query, value projections for all heads, but in a batch\n\t        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n\t        # output projection\n\t        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n\t        # regularization\n\t        self.n_head = n_head\n\t        self.n_embd = n_embd\n\t        self.register_buffer('rope_cache', rope_cache, persistent=False)\n", "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n\t        B, T, C = x.size(\n\t        )  # batch size, sequence length, embedding dimensionality (n_embd)\n\t        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n\t        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n\t        head_size = C // self.n_head\n\t        k = k.view(B, T, self.n_head, head_size).transpose(1,\n\t                                                           2)  # (B, nh, T, hs)\n\t        q = q.view(B, T, self.n_head, head_size).transpose(1,\n\t                                                           2)  # (B, nh, T, hs)\n", "        v = v.view(B, T, self.n_head, head_size).transpose(1,\n\t                                                           2)  # (B, nh, T, hs)\n\t        q = apply_rope(q, self.rope_cache)\n\t        k = apply_rope(k, self.rope_cache)\n\t        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n\t        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t        #  att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n\t        #  att = F.softmax(att, dim=-1)\n\t        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\t        # efficient attention using Flash Attention CUDA kernels\n", "        y = F.scaled_dot_product_attention(\n\t            q, k, v, attn_mask=attention_mask, dropout_p=0.0, is_causal=True)\n\t        y = y.transpose(1, 2).contiguous().view(\n\t            B, T, C)  # re-assemble all head outputs side by side\n\t        # output projection\n\t        y = self.c_proj(y)\n\t        return y\n\tclass MLP(nn.Module):\n\t    def __init__(self, n_embd, expand=4) -> None:\n\t        super().__init__()\n", "        hidden_dim = expand * n_embd\n\t        n_hidden = int(2 * hidden_dim / 3)\n\t        N = 256\n\t        # ensure n_hidden is multiple of N\n\t        n_hidden = ((n_hidden - 1) // N) * N + N\n\t        self.c_fc1 = nn.Linear(n_embd, n_hidden, bias=False)\n\t        self.c_fc2 = nn.Linear(n_embd, n_hidden, bias=False)\n\t        self.c_proj = nn.Linear(n_hidden, n_embd, bias=False)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n", "        x = self.c_proj(x)\n\t        return x\n\tclass Block(nn.Module):\n\t    def __init__(self, n_embd, n_head, rope_cache: torch.Tensor) -> None:\n\t        super().__init__()\n\t        self.rms_1 = RMSNorm(n_embd)\n\t        self.attn = CausalSelfAttention(\n\t            n_embd=n_embd, n_head=n_head, rope_cache=rope_cache)\n\t        self.rms_2 = RMSNorm(n_embd)\n\t        self.mlp = MLP(n_embd)\n", "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n\t        x = x + self.attn(self.rms_1(x), attention_mask)\n\t        x = x + self.mlp(self.rms_2(x))\n\t        return x\n\t@MODELS.register_module()\n\tclass LLaMA(BaseModel):\n\t    def __init__(self,\n\t                 block_size: int = 4096,\n\t                 vocab_size: int = 32000,\n\t                 n_layer: int = 32,\n", "                 n_head: int = 32,\n\t                 n_embd: int = 4096,\n\t                 pretrained=None,\n\t                 test_cfg=dict(\n\t                    max_new_tokens=50,\n\t                    temperature=1.0,\n\t                    top_k=200,)) -> None:\n\t        super().__init__()\n\t        assert vocab_size is not None\n\t        assert block_size is not None\n", "        self.block_size = block_size\n\t        self.vocab_size = vocab_size\n\t        self.n_layer = n_layer\n\t        self.pretrained = pretrained\n\t        self.test_cfg = Config(test_cfg)\n\t        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n\t        rope_cache = build_rope_cache(\n\t            seq_len=block_size,\n\t            n_elem=n_embd // n_head,\n\t            dtype=self.lm_head.weight.dtype)\n", "        self.transformer = nn.ModuleDict(\n\t            dict(\n\t                wte=nn.Embedding(vocab_size, n_embd),\n\t                h=nn.ModuleList([\n\t                    Block(n_embd, n_head, rope_cache) for _ in range(n_layer)\n\t                ]),\n\t                ln_f=RMSNorm(n_embd),\n\t            ))\n\t    def init_weights(self):\n\t        if self.pretrained is not None:\n", "            checkpoint = torch.load(self.pretrained)\n\t            self.load_state_dict(checkpoint, strict=False)\n\t            print_log(f'Load pretrained model from {self.pretrained}')\n\t    # def _init_weights(self, module: nn.Module) -> None:\n\t    #     if isinstance(module, nn.Linear):\n\t    #         torch.nn.init.normal_(\n\t    #             module.weight,\n\t    #             mean=0.0,\n\t    #             std=0.02 / math.sqrt(2 * self.n_layer))\n\t    #     elif isinstance(module, nn.Embedding):\n", "    #         torch.nn.init.normal_(\n\t    #             module.weight,\n\t    #             mean=0.0,\n\t    #             std=0.02 / math.sqrt(2 * self.n_layer))\n\t    def forward(self,\n\t                input_ids: torch.Tensor,\n\t                attention_mask=None,\n\t                labels=None,\n\t                mode='tensor') -> torch.Tensor:\n\t        if mode == 'tensor':\n", "            return self._forward(input_ids, attention_mask)\n\t        elif mode == 'loss':\n\t            return self.loss(input_ids, attention_mask, labels)\n\t        elif mode == 'predict':\n\t            return self.predict(input_ids)\n\t    def _forward(self, input_ids, attention_mask=None):\n\t        _, t = input_ids.size()\n\t        assert (\n\t            t <= self.block_size\n\t        ), f'Cannot forward sequence of length {t}, block size is only {self.block_size}'\n", "        # forward the LLaMA model itself\n\t        x = self.transformer.wte(\n\t            input_ids)  # token embeddings of shape (b, t, n_embd)\n\t        # TODO: prepare attn mask\n\t        if attention_mask is not None:\n\t            attention_mask = None\n\t        for block in self.transformer.h:\n\t            x = block(x, attention_mask)\n\t        x = self.transformer.ln_f(x)\n\t        logits = self.lm_head(x)  # (b, t, vocab_size)\n", "        return logits\n\t    def loss(self, input_ids, attention_mask, labels):\n\t        logits = self._forward(input_ids, attention_mask)\n\t        # Shift so that tokens < n predict n\n\t        shift_logits = logits[..., :-1, :].contiguous()\n\t        shift_labels = labels[..., 1:].contiguous()\n\t        # Flatten the tokens\n\t        loss_fct = nn.CrossEntropyLoss()\n\t        shift_logits = shift_logits.view(-1, self.vocab_size)\n\t        shift_labels = shift_labels.view(-1)\n", "        # Enable model parallelism\n\t        shift_labels = shift_labels.to(shift_logits.device)\n\t        loss = loss_fct(shift_logits, shift_labels)\n\t        return dict(loss=loss)\n\t    @torch.no_grad()\n\t    def predict(self, input_ids):\n\t        logits = self._forward(input_ids)\n\t        # create an empty tensor of the expected final shape and fill in the current tokens\n\t        B, T = input_ids.shape\n\t        T_new = T + self.test_cfg.max_new_tokens\n", "        empty = torch.empty(B, T_new, dtype=input_ids.dtype, device=input_ids.device)\n\t        empty[:, :T] = input_ids\n\t        input_ids = empty\n\t        max_seq_length = self.block_size\n\t        # generate max_new_tokens tokens\n\t        for t in range(T, T_new):\n\t            # ignore the not-filled-yet tokens\n\t            idx_cond = input_ids[:, :t]\n\t            # if the sequence context is growing too long we must crop it at max_seq_length\n\t            idx_cond = idx_cond if T <= max_seq_length else idx_cond[:, -max_seq_length:]\n", "            # forward\n\t            logits = self._forward(idx_cond)\n\t            logits = logits[:, -1] / self.test_cfg.temperature\n\t            # optionally crop the logits to only the top k options\n\t            if self.test_cfg.get('top_k', None) is not None:\n\t                v, _ = torch.topk(logits, min(self.test_cfg.top_k, logits.size(-1)))\n\t                logits[logits < v[:, [-1]]] = -float('Inf')\n\t            probs = torch.nn.functional.softmax(logits, dim=-1)\n\t            idx_next = torch.multinomial(probs, num_samples=1)\n\t            # concatenate the new column\n", "            input_ids[:, t:] = idx_next\n\t        return input_ids\n\tllama_configs = {\n\t    'toy': dict(n_layer=1, n_head=1,\n\t                n_embd=128, block_size=1024,\n\t                vocab_size=32000, pretrained=None),  # for debug\n\t    '7B': dict(n_layer=32, n_head=32,\n\t               n_embd=4096, block_size=4096,\n\t               vocab_size=32000, pretrained='checkpoints/mm-llama/7B/state_dict.pth'),\n\t    '13B': dict(n_layer=40, n_head=40,\n", "                n_embd=5120, block_size=4096,\n\t                vocab_size=32000, pretrained='checkpoints/mm-llama/13B/state_dict.pth'),\n\t    '30B': dict(n_layer=60, n_head=52,\n\t                n_embd=6656, block_size=4096,\n\t                vocab_size=32000, pretrained='checkpoints/mm-llama/30B/state_dict.pth'),\n\t    '65B': dict(n_layer=80, n_head=64,\n\t                n_embd=8192, block_size=4096,\n\t                vocab_size=32000, pretrained='checkpoints/mm-llama/65B/state_dict.pth'),\n\t}\n\t# TODO: mmengine support for partial\n", "# MODELS.register_module('LLaMA-toy', module=partial(LLaMA, **llama_configs['toy']))  # for debug\n\t# MODELS.register_module('LLaMA-7B', module=partial(LLaMA, **llama_configs['7B']))\n\t# MODELS.register_module('LLaMA-13B', module=partial(LLaMA, **llama_configs['13B']))\n\t# MODELS.register_module('LLaMA-30B', module=partial(LLaMA, **llama_configs['30B']))\n\t# MODELS.register_module('LLaMA-65B', module=partial(LLaMA, **llama_configs['65B']))\n\t@MODELS.register_module()\n\tclass LLaMAToy(LLaMA):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**llama_configs['toy'], **kwargs)\n\t@MODELS.register_module()\n", "class LLaMA7B(LLaMA):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**llama_configs['7B'], **kwargs)\n\t@MODELS.register_module()\n\tclass LLaMA13B(LLaMA):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**llama_configs['13B'], **kwargs)\n\t@MODELS.register_module()\n\tclass LLaMA30B(LLaMA):\n\t    def __init__(self, **kwargs):\n", "        super().__init__(**llama_configs['30B'], **kwargs)\n\t@MODELS.register_module()\n\tclass LLaMA65B(LLaMA):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**llama_configs['65B'], **kwargs)\n"]}
{"filename": "mmllama/models/__init__.py", "chunked_list": ["from .llama import LLaMA, llama_configs\n\tfrom .lora import LoRAModel\n\t__all__ = ['LLaMA', 'llama_configs', 'LoRAModel']\n"]}
{"filename": "mmllama/models/lora.py", "chunked_list": ["# Derived from https://github.com/microsoft/LoRA\n\t#  ------------------------------------------------------------------------------------------\n\t#  Copyright (c) Microsoft Corporation. All rights reserved.\n\t#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n\t#  ------------------------------------------------------------------------------------------\n\timport math\n\tfrom contextlib import contextmanager\n\tfrom dataclasses import dataclass\n\tfrom typing import Dict, List\n\timport torch\n", "import torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom mmengine.logging import print_log\n\tfrom mmengine.model import BaseModel\n\timport mmllama.models.llama as llama\n\tfrom mmllama.registry import MODELS\n\tclass LoRALayer():\n\t    def __init__(\n\t        self,\n\t        r: int,\n", "        lora_alpha: int,\n\t        lora_dropout: float,\n\t        merge_weights: bool,\n\t    ):\n\t        self.r = r\n\t        self.lora_alpha = lora_alpha\n\t        # Optional dropout\n\t        if lora_dropout > 0.:\n\t            self.lora_dropout = nn.Dropout(p=lora_dropout)\n\t        else:\n", "            self.lora_dropout = lambda x: x\n\t        # Mark the weight as unmerged\n\t        self.merged = False\n\t        self.merge_weights = merge_weights\n\tclass MergedLinear(nn.Linear, LoRALayer):\n\t    # LoRA implemented in a dense layer\n\t    def __init__(\n\t        self,\n\t        in_features: int,\n\t        out_features: int,\n", "        r: int = 0,\n\t        lora_alpha: int = 1,\n\t        lora_dropout: float = 0.,\n\t        enable_lora: List[bool] = [False],\n\t        fan_in_fan_out: bool = False,\n\t        merge_weights: bool = True,\n\t        **kwargs\n\t    ):\n\t        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n\t        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n", "                           merge_weights=merge_weights)\n\t        assert out_features % len(enable_lora) == 0, \\\n\t            'The length of enable_lora must divide out_features'\n\t        self.enable_lora = enable_lora\n\t        self.fan_in_fan_out = fan_in_fan_out\n\t        # Actual trainable parameters\n\t        if r > 0 and any(enable_lora):\n\t            self.lora_A = nn.Parameter(\n\t                self.weight.new_zeros((r * sum(enable_lora), in_features)))\n\t            self.lora_B = nn.Parameter(\n", "                self.weight.new_zeros((out_features // len(enable_lora) * sum(enable_lora), r))\n\t            ) # weights for Conv1D with groups=sum(enable_lora)\n\t            self.scaling = self.lora_alpha / self.r\n\t            # Freezing the pre-trained weight matrix\n\t            self.weight.requires_grad = False\n\t            # Compute the indices\n\t            self.lora_ind = self.weight.new_zeros(\n\t                (out_features, ), dtype=torch.bool\n\t            ).view(len(enable_lora), -1)\n\t            self.lora_ind[enable_lora, :] = True\n", "            self.lora_ind = self.lora_ind.view(-1)\n\t        self.reset_parameters()\n\t        if fan_in_fan_out:\n\t            self.weight.data = self.weight.data.T\n\t    def reset_parameters(self):\n\t        nn.Linear.reset_parameters(self)\n\t        if hasattr(self, 'lora_A'):\n\t            # initialize A the same way as the default for nn.Linear and B to zero\n\t            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n\t            nn.init.zeros_(self.lora_B)\n", "    def zero_pad(self, x):\n\t        result = x.new_zeros((*x.shape[:-1], self.out_features))\n\t        result = result.view(-1, self.out_features)\n\t        result[:, self.lora_ind] = x.reshape(\n\t            -1, self.out_features // len(self.enable_lora) * sum(self.enable_lora)\n\t        )\n\t        return result.view((*x.shape[:-1], self.out_features))\n\t    def train(self, mode: bool = True):\n\t        def T(w):\n\t            return w.T if self.fan_in_fan_out else w\n", "        nn.Linear.train(self, mode)\n\t        if self.merge_weights and self.merged:\n\t            # Make sure that the weights are not merged\n\t            if self.r > 0 and any(self.enable_lora):\n\t                delta_w = F.conv1d(\n\t                    self.lora_A.data.unsqueeze(0),\n\t                    self.lora_B.data.unsqueeze(-1),\n\t                    groups=sum(self.enable_lora)\n\t                ).squeeze(0)\n\t                self.weight.data -= self.zero_pad(T(delta_w * self.scaling))\n", "            self.merged = False\n\t    def eval(self):\n\t        def T(w):\n\t            return w.T if self.fan_in_fan_out else w\n\t        nn.Linear.eval(self)\n\t        if self.merge_weights and not self.merged:\n\t            # Merge the weights and mark it\n\t            if self.r > 0 and any(self.enable_lora):\n\t                delta_w = F.conv1d(\n\t                    self.lora_A.data.unsqueeze(0),\n", "                    self.lora_B.data.unsqueeze(-1),\n\t                    groups=sum(self.enable_lora)\n\t                ).squeeze(0)\n\t                self.weight.data += self.zero_pad(T(delta_w * self.scaling))\n\t            self.merged = True\n\t    def forward(self, x: torch.Tensor):\n\t        def T(w):\n\t            return w.T if self.fan_in_fan_out else w\n\t        if self.merged:\n\t            return F.linear(x, T(self.weight), bias=self.bias)\n", "        else:\n\t            result = F.linear(x, T(self.weight), bias=self.bias)\n\t            if self.r > 0:\n\t                after_A = F.linear(self.lora_dropout(x), self.lora_A)\n\t                after_B = F.conv1d(\n\t                    after_A.transpose(-2, -1),\n\t                    self.lora_B.unsqueeze(-1),\n\t                    groups=sum(self.enable_lora)\n\t                ).transpose(-2, -1)\n\t                result += self.zero_pad(after_B) * self.scaling\n", "            return result\n\tdef mark_only_lora_as_trainable(model: nn.Module, bias: str = 'none') -> None:\n\t    for n, p in model.named_parameters():\n\t        if 'lora_' not in n:\n\t            p.requires_grad = False\n\t    if bias == 'none':\n\t        return\n\t    elif bias == 'all':\n\t        for n, p in model.named_parameters():\n\t            if 'bias' in n:\n", "                p.requires_grad = True\n\t    elif bias == 'lora_only':\n\t        for m in model.modules():\n\t            if isinstance(m, LoRALayer) and \\\n\t                hasattr(m, 'bias') and \\\n\t                m.bias is not None:\n\t                    m.bias.requires_grad = True\n\t    else:\n\t        raise NotImplementedError\n\tdef lora_state_dict(model: nn.Module, bias: str = 'none') -> Dict[str, torch.Tensor]:\n", "    my_state_dict = model.state_dict()\n\t    if bias == 'none':\n\t        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k}\n\t    elif bias == 'all':\n\t        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k or 'bias' in k}\n\t    elif bias == 'lora_only':\n\t        to_return = {}\n\t        for k in my_state_dict:\n\t            if 'lora_' in k:\n\t                to_return[k] = my_state_dict[k]\n", "                bias_name = k.split('lora_')[0]+'bias'\n\t                if bias_name in my_state_dict:\n\t                    to_return[bias_name] = my_state_dict[bias_name]\n\t        return to_return\n\t    else:\n\t        raise NotImplementedError\n\t@dataclass\n\tclass LoRAConfig:\n\t    r: float = 0.0\n\t    alpha: float = 1.0\n", "    dropout: float = 0.0\n\tclass CausalSelfAttention(llama.CausalSelfAttention):\n\t    lora_config = None\n\t    def __init__(self,  n_embd, n_head, rope_cache: torch.Tensor) -> None:\n\t        # Skip the parent class __init__ altogether and replace it to avoid\n\t        # useless allocations\n\t        nn.Module.__init__(self)\n\t        assert n_embd % n_head == 0\n\t        # key, query, value projections for all heads, but in a batch\n\t        self.c_attn = MergedLinear(\n", "            in_features=n_embd,\n\t            out_features=3 * n_embd,\n\t            r=self.lora_config.r,\n\t            lora_alpha=self.lora_config.alpha,\n\t            lora_dropout=self.lora_config.dropout,\n\t            enable_lora=[True, False, True],\n\t            fan_in_fan_out = False,\n\t            merge_weights=True,\n\t            bias=False)\n\t        # output projection\n", "        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n\t        # regularization\n\t        self.n_head = n_head\n\t        self.n_embd = n_embd\n\t        self.register_buffer('rope_cache', rope_cache, persistent=False)\n\t@contextmanager\n\tdef lora(r, alpha, dropout, enabled: bool = True):\n\t    \"\"\"A context manager under which you can instantiate the model with\n\t    LoRA.\"\"\"\n\t    if not enabled:\n", "        yield\n\t        return\n\t    CausalSelfAttention.lora_config = LoRAConfig(r=r, alpha=alpha, dropout=dropout)\n\t    causal_self_attention = llama.CausalSelfAttention\n\t    llama.CausalSelfAttention = CausalSelfAttention\n\t    yield\n\t    llama.CausalSelfAttention = causal_self_attention\n\t    CausalSelfAttention.lora_config = None\n\t@MODELS.register_module()\n\tclass LoRAModel(BaseModel):\n", "    def __init__(self,\n\t                 model: dict,\n\t                 r: float,\n\t                 alpha: float,\n\t                 dropout: float):\n\t        super().__init__()\n\t        with lora(r=r, alpha=alpha, dropout=dropout, enabled=True):\n\t            print_log('Building model with LoRA', logger='current')\n\t            self.model = MODELS.build(model)\n\t            mark_only_lora_as_trainable(self.model)\n", "        self.data_preprocessor = self.model.data_preprocessor\n\t    def forward(self, *args, **kwargs):\n\t        return self.model(*args, **kwargs)\n\t    def state_dict(self, *args, **kwargs):\n\t        return lora_state_dict(self.model)\n\t    def load_state_dict(self, *args, **kwargs):\n\t        return self.model.load_state_dict(*args, **kwargs)\n"]}
{"filename": "mmllama/datasets/prompter.py", "chunked_list": ["\"\"\"A dedicated helper to manage templates and prompt building.\n\tModified from https://github.com/tloen/alpaca-lora/tree/main/utils\n\t\"\"\"\n\timport json\n\timport os.path as osp\n\tfrom typing import Union\n\tclass Prompter(object):\n\t    __slots__ = ('template', '_verbose')\n\t    def __init__(self, template_name: str = '', verbose: bool = False):\n\t        self._verbose = verbose\n", "        if not template_name:\n\t            # Enforce the default here, so the constructor can be called with '' and will not break.\n\t            template_name = 'alpaca'\n\t        file_name = osp.join('templates', f'{template_name}.json')\n\t        if not osp.exists(file_name):\n\t            raise ValueError(f\"Can't read {file_name}\")\n\t        with open(file_name) as fp:\n\t            self.template = json.load(fp)\n\t        if self._verbose:\n\t            print(\n", "                f\"Using prompt template {template_name}: {self.template['description']}\"\n\t            )\n\t    def generate_prompt(\n\t        self,\n\t        instruction: str,\n\t        input: Union[None, str] = None,\n\t        label: Union[None, str] = None,\n\t    ) -> str:\n\t        # returns the full prompt from instruction and optional input\n\t        # if a label (=response, =output) is provided, it's also appended.\n", "        if input:\n\t            res = self.template['prompt_input'].format(\n\t                instruction=instruction, input=input\n\t            )\n\t        else:\n\t            res = self.template['prompt_no_input'].format(\n\t                instruction=instruction\n\t            )\n\t        if label:\n\t            res = f'{res}{label}'\n", "        if self._verbose:\n\t            print(res)\n\t        return res\n\t    def get_response(self, output: str) -> str:\n\t        return output.split(self.template['response_split'])[1].strip()\n"]}
{"filename": "mmllama/datasets/collate_fn.py", "chunked_list": ["import numpy as np\n\tfrom mmengine.registry import FUNCTIONS\n\tFUNCTIONS.register_module()\n\tdef seq2seq_collate(features, tokenizer):\n\t    # import ipdb; ipdb.set_trace()\n\t    return_tensors = 'pt'\n\t    label_pad_token_id: int = -100\n\t    pad_to_multiple_of = 8\n\t    labels = [feature['labels'] for feature in features] if 'labels' in features[0].keys() else None\n\t    # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n", "    # same length to return tensors.\n\t    if labels is not None:\n\t        max_label_length = max(len(l) for l in labels)\n\t        if pad_to_multiple_of is not None:\n\t            max_label_length = (\n\t                (max_label_length + pad_to_multiple_of - 1)\n\t                // pad_to_multiple_of\n\t                * pad_to_multiple_of\n\t            )\n\t        padding_side = tokenizer.padding_side\n", "        for feature in features:\n\t            remainder = [label_pad_token_id] * (max_label_length - len(feature['labels']))\n\t            if isinstance(feature['labels'], list):\n\t                feature['labels'] = (\n\t                    feature['labels'] + remainder if padding_side == 'right' else remainder + feature['labels']\n\t                )\n\t            elif padding_side == 'right':\n\t                feature['labels'] = np.concatenate([feature['labels'], remainder]).astype(np.int64)\n\t            else:\n\t                feature['labels'] = np.concatenate([remainder, feature['labels']]).astype(np.int64)\n", "    features = tokenizer.pad(\n\t        features,\n\t        padding=True,\n\t        max_length=None,\n\t        pad_to_multiple_of=pad_to_multiple_of,\n\t        return_tensors=return_tensors,\n\t    )\n\t    return features\n"]}
{"filename": "mmllama/datasets/__init__.py", "chunked_list": ["from .collate_fn import seq2seq_collate\n\tfrom .prompter import Prompter\n\t__all__ = ['Prompter', 'seq2seq_collate']\n"]}
{"filename": "configs/llama-7B_finetune_3e.py", "chunked_list": ["_base_ = [\n\t    './_base_/schedules/finetune-3e.py', './_base_/default_runtime.py'\n\t]\n\tmodel = dict(type='LoRAModel',\n\t             model=dict(type='LLaMA7B'),\n\t             r=8,\n\t             alpha=16,\n\t             dropout=0.05)\n\tdata_path = 'yahma/alpaca-cleaned'\n\tval_set_size = 2000\n", "tokenizer_path = 'checkpoints/mm-llama/tokenizer.model'\n\ttrain_dataloader = dict(\n\t    batch_size=4,\n\t    num_workers=4,\n\t    persistent_workers=True,\n\t    sampler=dict(type='DefaultSampler', shuffle=True),\n\t    collate_fn=dict(type='seq2seq_collate'))\n\tval_dataloader = dict(\n\t    batch_size=4,\n\t    num_workers=4,\n", "    persistent_workers=True,\n\t    drop_last=False,\n\t    sampler=dict(type='DefaultSampler', shuffle=False),\n\t    collate_fn=dict(type='seq2seq_collate'))\n\tval_evaluator = dict(type='DummyMetric')\n\toptim_wrapper = dict(\n\t    type='AmpOptimWrapper',\n\t    optimizer=dict(type='AdamW', lr=3e-4),\n\t    accumulative_counts=128//4  # TODO minibatch=4\n\t    )\n", "# Default setting for scaling LR automatically\n\t#   - `enable` means enable scaling LR automatically\n\t#       or not by default.\n\t#   - `base_batch_size` = 4 samples per GPU).\n\tauto_scale_lr = dict(enable=False, base_batch_size=4)\n"]}
{"filename": "configs/_base_/default_runtime.py", "chunked_list": ["default_scope = 'mmllama'\n\tdefault_hooks = dict(\n\t    timer=dict(type='IterTimerHook'),\n\t    logger=dict(type='LoggerHook', interval=10),\n\t    param_scheduler=dict(type='ParamSchedulerHook'),\n\t    checkpoint=dict(type='CheckpointHook', interval=1),  # TODO\n\t    sampler_seed=dict(type='DistSamplerSeedHook'),\n\t    visualization=dict(type='NaiveVisualizationHook'))\n\tenv_cfg = dict(\n\t    cudnn_benchmark=False,\n", "    mp_cfg=dict(mp_start_method='fork'),\n\t    dist_cfg=dict(backend='nccl'),\n\t)\n\tvis_backends = [dict(type='LocalVisBackend'), dict(type='TensorboardVisBackend')]\n\tvisualizer = dict(\n\t    type='Visualizer', vis_backends=vis_backends, name='visualizer')\n\tlog_processor = dict(type='LogProcessor', window_size=10, by_epoch=True)\n\tlog_level = 'INFO'\n\tload_from = None\n\tresume = False\n"]}
{"filename": "configs/_base_/schedules/finetune-3e.py", "chunked_list": ["# training schedule for 1x\n\ttrain_cfg = dict(type='EpochBasedTrainLoop', max_epochs=3, val_interval=10)\n\tval_cfg = dict(type='ValLoop')\n\t# learning rate\n\tparam_scheduler = [\n\t    dict(\n\t        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=100),\n\t    dict(\n\t        type='LinearLR',\n\t        begin=0,\n", "        end=3,\n\t        by_epoch=True,\n\t        start_factor=1.0,\n\t        end_factor=0.0,\n\t        convert_to_iter_based=True\n\t        )\n\t]\n\t# optimizer\n\toptim_wrapper = dict(\n\t    type='AmpOptimWrapper',\n", "    optimizer=dict(type='AdamW', lr=3e-4),\n\t    clip_grad=dict(max_norm=1.0, norm_type=2),\n\t    )\n"]}
