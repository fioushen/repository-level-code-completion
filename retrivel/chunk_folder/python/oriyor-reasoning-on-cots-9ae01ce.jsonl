{"filename": "src/consts.py", "chunked_list": ["\"\"\"\n\tconsts used throughout the project\n\t\"\"\"\n\tCONTEXT_ANSWER_SEP = \"|||\"\n\tCONTEXT_PREFIX = \"Context: \"\n\tNUMBERED_CONTEXT_PREFIX = \"Context\"\n\tFOLLOW_UP_PREFIX = \"Follow up: \"\n\tKNOWLEDGE_FOLLOW_UP_PREFIX = \"Knowledge Follow up: \"\n\tREASONING_FOLLOW_UP_PREFIX = \"Reasoning Follow up: \"\n\tFINAL_ANSWER_PREFIX = \"So the final answer is: \"\n", "INTERMEDIATE_ANS_PREFIX = \"Intermediate answer: \"\n\tQUESTION_PREFIX = \"Question: \"\n\tCALL_API = True\n\tPOSITIVE_PREFIX = \"Positive: \"\n\tNEGATIVE_PREFIX = \"Negative: \"\n\tSTOP_TOKEN = \"\\n#\"\n\tSELF_ASK_ANSWER = \"So the final answer is: \"\n\tFACTOID_ANSWER_PREFIX = \"A: \"\n\tLOGGING_NAME = \"logger\"\n\t# mte\n", "FULL_MTE_FIELD = \"multi_trace_entailment_full\"\n\tPRED_MTE_FIELD = \"multi_trace_entailment_prediction\"\n\tACC_MTE_FIELD = \"acc@mte\"\n\tNUM_EXAMPLES_FIELD = \"num_examples\"\n\tNUM_ABSTAINS_FIELD = \"num_abstains\"\n\t# accuracy feilds\n\tACC_AT_1_FIELD = \"acc@1\"\n\tACC_AT_3_FIELD = \"acc@3\"\n\tACC_AT_MAJORITY_FIELD = \"acc@majority\"\n"]}
{"filename": "src/dataclasses.py", "chunked_list": ["from enum import Enum\n\tfrom typing import List, Dict\n\tfrom dataclasses import dataclass\n\tfrom src.common.config import Config\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\n\tfrom src.consts import (\n\t    POSITIVE_PREFIX,\n\t    NEGATIVE_PREFIX,\n\t    INTERMEDIATE_ANS_PREFIX,\n\t    KNOWLEDGE_FOLLOW_UP_PREFIX,\n", "    REASONING_FOLLOW_UP_PREFIX,\n\t    FINAL_ANSWER_PREFIX,\n\t    FOLLOW_UP_PREFIX,\n\t    CONTEXT_PREFIX,\n\t    CONTEXT_ANSWER_SEP,\n\t    NUMBERED_CONTEXT_PREFIX,\n\t)\n\t# Todo: TW - Commented out because of DeBerta in background\n\t# from src.entailment.entailment import run_entailment\n\t@dataclass\n", "class Example:\n\t    qid: str\n\t    question: str\n\t    gold_answer: str\n\t    prev_model_answer: str\n\t    metadata: Dict\n\t@dataclass\n\tclass BreakDecomposition:\n\t    decomposition: str\n\t    intermediate_questions: List[str]\n", "    intermediate_answers: List[str]\n\t    final_answer: str\n\t    intermediate_contexts: List[str]\n\t@dataclass\n\tclass IntermediateQuestionWithAnswer:\n\t    \"\"\"\n\t    an intermediate decompotision in the tree\n\t    \"\"\"\n\t    intermediate_question: str\n\t    answer: Dict = None  # from gpt-3\n", "@dataclass\n\tclass Question:\n\t    \"\"\"\n\t    a question object\n\t    \"\"\"\n\t    question: str  # question\n\t    decompositions: List[str]  # decompositions from gpt\n\t    intermediate_questions_with_answers: List[\n\t        IntermediateQuestionWithAnswer\n\t    ] = None  # intermediate steps with their answers\n", "@dataclass\n\tclass EntailmentResult:\n\t    entailment: float\n\t    neutral: float\n\t    contradiction: float\n\t@dataclass\n\tclass EntailmentResultWithInput:\n\t    premise: str\n\t    hypothesis: str\n\t    entailment_result: EntailmentResult\n", "@dataclass\n\tclass DecompositionStepV1:\n\t    question: str\n\t    answer: str\n\t    google_answer: str\n\t    google_answer_long: Dict\n\t    entailment_result_with_input: EntailmentResultWithInput\n\t    gpt_3_ans: str\n\t@dataclass\n\tclass TraceResult:\n", "    positive: EntailmentResultWithInput\n\t    negative: EntailmentResultWithInput\n\t@dataclass\n\tclass StatementResult:\n\t    original_statement: str\n\t    positive: str\n\t    negative: str\n\tdef format_ir_decomposition(\n\t    decomposition: str, entail_facts=False, contexts_first=None\n\t) -> List[DecompositionStepV1]:\n", "    \"\"\"format decomposition and store retrieved context(s) for each step\"\"\"\n\t    if contexts_first is True:\n\t        # decomposition is in SA format where contexts are appended at beginning: 'Context1:...Context2:...'\n\t        for i in range(1, 5):\n\t            decomposition = decomposition.replace(f\"Context{i}:\\n\\n\", f\"Context{i}: \")\n\t    decomposition_steps = []\n\t    lines = decomposition.split(\"\\n\")\n\t    question, answer, gpt_3_ans = None, None, None\n\t    context = None\n\t    context_counter = 1 if Config().get(\"decomposition.retrieve_orig_question\") else 0\n", "    all_contexts = []\n\t    for line in lines:\n\t        line = (\n\t            line + \"  \"\n\t        )  # add \" \" to avoid discarding traces when the answer is empty\n\t        if line.startswith(NUMBERED_CONTEXT_PREFIX):\n\t            if contexts_first is True:\n\t                context = \": \".join(\n\t                    line.split(NUMBERED_CONTEXT_PREFIX)[1].split(\":\")[1:]\n\t                ).strip()\n", "            else:\n\t                context = line.split(CONTEXT_PREFIX)[1].strip()\n\t            all_contexts += [context]\n\t        if line.startswith(FOLLOW_UP_PREFIX):\n\t            question = line.split(FOLLOW_UP_PREFIX)[1].strip()\n\t        if line.startswith(FINAL_ANSWER_PREFIX):\n\t            decomposition_steps.append(\n\t                DecompositionStepV1(\n\t                    question=None,\n\t                    answer=None,\n", "                    google_answer=None,\n\t                    google_answer_long=None,\n\t                    entailment_result_with_input=None,\n\t                    gpt_3_ans=line.split(FINAL_ANSWER_PREFIX)[1].strip(),\n\t                )\n\t            )\n\t        if line.startswith(INTERMEDIATE_ANS_PREFIX) and question is not None:\n\t            answer = line.split(INTERMEDIATE_ANS_PREFIX)[1].strip()\n\t            answer = \"\" if answer is None else answer\n\t            context = \"\" if context is None else context\n", "            if contexts_first is True and context_counter < len(all_contexts):\n\t                # for contexts first, we need to align preceding contexts with intermediate Qs\n\t                context = all_contexts[context_counter]\n\t                context_counter += 1\n\t            context_and_answer = context + CONTEXT_ANSWER_SEP + answer\n\t            decomposition_steps.append(\n\t                DecompositionStepV1(\n\t                    question=question,\n\t                    answer=context_and_answer,\n\t                    google_answer=None,\n", "                    google_answer_long=None,\n\t                    entailment_result_with_input=None,\n\t                    gpt_3_ans=None,\n\t                )\n\t            )\n\t            question, answer = None, None\n\t    return decomposition_steps\n\tdef format_statement(statement: str) -> StatementResult:\n\t    \"\"\" \"\"\"\n\t    positive, negative = None, None\n", "    lines = statement.split(\"\\n\")\n\t    for line in lines:\n\t        if line.startswith(POSITIVE_PREFIX):\n\t            positive = line.split(POSITIVE_PREFIX)[1]\n\t        if line.startswith(NEGATIVE_PREFIX):\n\t            negative = line.split(NEGATIVE_PREFIX)[1]\n\t    return StatementResult(\n\t        original_statement=statement, positive=positive, negative=negative\n\t    )\n\tdef format_decompsition_break(\n", "    decomposition: BreakDecomposition,\n\t) -> List[DecompositionStepV1]:\n\t    decomposition_steps = []\n\t    for i in range(len(decomposition.intermediate_questions)):\n\t        decomposition_steps.append(\n\t            DecompositionStepV1(\n\t                question=decomposition.intermediate_questions[i],\n\t                answer=decomposition.intermediate_answers[i + 1],\n\t                google_answer=None,\n\t                google_answer_long=None,\n", "                entailment_result_with_input=None,\n\t                gpt_3_ans=None,\n\t            )\n\t        )\n\t    decomposition_steps.append(\n\t        DecompositionStepV1(\n\t            question=None,\n\t            answer=None,\n\t            google_answer=None,\n\t            google_answer_long=None,\n", "            entailment_result_with_input=None,\n\t            gpt_3_ans=decomposition.final_answer,\n\t        )\n\t    )\n\t    return decomposition_steps\n\tdef format_decompsition_string(\n\t    decomposition: str, entail_facts=False\n\t) -> List[DecompositionStepV1]:\n\t    \"\"\" \"\"\"\n\t    decomposition_steps = []\n", "    lines = decomposition.split(\"\\n\")\n\t    question, answer, gpt_3_ans = None, None, None\n\t    for line in lines:\n\t        if line.startswith(KNOWLEDGE_FOLLOW_UP_PREFIX):\n\t            question = line.split(KNOWLEDGE_FOLLOW_UP_PREFIX)[1]\n\t        elif line.startswith(REASONING_FOLLOW_UP_PREFIX):\n\t            question = line.split(REASONING_FOLLOW_UP_PREFIX)[1]\n\t        elif line.startswith(FOLLOW_UP_PREFIX):\n\t            question = line.split(FOLLOW_UP_PREFIX)[1]\n\t        if line.startswith(FINAL_ANSWER_PREFIX):\n", "            decomposition_steps.append(\n\t                DecompositionStepV1(\n\t                    question=None,\n\t                    answer=None,\n\t                    google_answer=None,\n\t                    google_answer_long=None,\n\t                    entailment_result_with_input=None,\n\t                    gpt_3_ans=line.split(FINAL_ANSWER_PREFIX)[1],\n\t                )\n\t            )\n", "        if line.startswith(INTERMEDIATE_ANS_PREFIX) and question is not None:\n\t            answer = line.split(INTERMEDIATE_ANS_PREFIX)[1]\n\t            # calc answer\n\t            gooogle_ans_short, entailment_result_with_input, google_ans_long = (\n\t                None,\n\t                None,\n\t                None,\n\t            )\n\t            # Todo: TW - Commented out because of DeBerta in background\n\t            # if entail_facts:\n", "            #     gooogle_ans_short, google_ans_long = google(question)\n\t            #     premise = question + \" \" + gooogle_ans_short\n\t            #     hypothesis = question + \" \" + answer\n\t            #     entailment = run_entailment(premise, hypothesis)\n\t            #     entailment_result_with_input = EntailmentResultWithInput(\n\t            #         premise=premise, hypothesis=hypothesis, entailment_result=entailment\n\t            #     )\n\t            decomposition_steps.append(\n\t                DecompositionStepV1(\n\t                    question=question,\n", "                    answer=answer,\n\t                    google_answer=gooogle_ans_short,\n\t                    google_answer_long=google_ans_long,\n\t                    entailment_result_with_input=entailment_result_with_input,\n\t                    gpt_3_ans=None,\n\t                )\n\t            )\n\t            question, answer = None, None\n\t    return decomposition_steps\n\tdef linearize_decompositions(decompositions: List[DecompositionStepV1]) -> str:\n", "    \"\"\" \"\"\"\n\t    res = \"\"\n\t    valid_decompositions = [d for d in decompositions if d.question]\n\t    for decomp in valid_decompositions:\n\t        res += decomp.question + \" \" + decomp.answer + \"\\n\"\n\t    print(res)\n\t    return res\n\tdef linearize_decompositions_google(decompositions: List[DecompositionStepV1]) -> str:\n\t    \"\"\" \"\"\"\n\t    res = \"\"\n", "    valid_decompositions = [d for d in decompositions if d.question]\n\t    for decomp in valid_decompositions:\n\t        if decomp.google_answer is not None:\n\t            res += decomp.question + \" \" + decomp.google_answer + \"\\n\"\n\t    print(res)\n\t    return res\n\t@dataclass\n\tclass QuestionV1:\n\t    \"\"\"\n\t    a question object\n", "    \"\"\"\n\t    question: str  # original question\n\t    statement: StatementResult  # the parsed statement after calling gpt-3\n\t    decompositions: List[str]  # decompositions from gpt-3 self-ask\n\t    decompsition_steps: List[List[DecompositionStepV1]]  # reasoning traces\n\t    traces_entailments: List[List[TraceResult]]\n\t    def __init__(\n\t        self,\n\t        question: str,\n\t        prompt: str,\n", "        gpt3_accessor: GptAccessor,\n\t        model: str,\n\t        num_decompositions: int = 5,\n\t    ):\n\t        self.question = question\n\t        self.send_question_separately = Config().get(\n\t            \"decomposition.send_question_separately\"\n\t        )\n\t        self.gpt_accessor_indices_with_temperature_0 = Config().get(\n\t            \"decomposition.gpt_accessor_indices_with_temperature_0\"\n", "        )\n\t        self.model = model\n\t        self.curr_prompt = (\n\t            prompt if self.send_question_separately else prompt + self.question\n\t        )\n\t        self.num_decompositions = num_decompositions\n\t        self.gpt3_accessor = gpt3_accessor\n\t        self.statement = None\n\t        self.decompositions = None\n\t        self.decompsition_steps = None\n", "        self.traces_entailments = None\n\t        self.traces_entailments_google = None\n\t    # Todo: TW - Commented out because of DeBerta in background\n\t    # def _get_entailment_res(\n\t    #     self, decomposition_linearized: str, statement: str\n\t    # ) -> EntailmentResultWithInput:\n\t    #     entailment = run_entailment(\n\t    #         premise=decomposition_linearized, hypothesis=statement\n\t    #     )\n\t    #     entailment_result_with_input = EntailmentResultWithInput(\n", "    #         premise=decomposition_linearized,\n\t    #         hypothesis=statement,\n\t    #         entailment_result=entailment,\n\t    #     )\n\t    #     return entailment_result_with_input\n\t    def populate_statement(self):\n\t        # gpt_statement_res = call_gpt_statement_generatior(\n\t        #     statement_generation_prompt + self.question, \"\"\n\t        # )\n\t        gpt_statement_res = \"\"\n", "        self.statement = format_statement(gpt_statement_res)\n\t    def populate_decompositions(self):\n\t        def get_temp_at_index(index: int) -> float:\n\t            # check if the index should get temp 0\n\t            if (\n\t                self.gpt_accessor_indices_with_temperature_0 is not None\n\t                and index in self.gpt_accessor_indices_with_temperature_0\n\t            ):\n\t                return 0\n\t            return Config().get(\"decomposition.gpt3_accessor_temperature\")\n", "        print(\"\\nRunning decomposition + retrieval models\")\n\t        self.decompositions = [\n\t            self.gpt3_accessor.call_gpt(\n\t                self.curr_prompt, \"\", get_temp_at_index(i), self.model, i\n\t            )\n\t            if not self.send_question_separately\n\t            else self.gpt3_accessor.call_gpt(\n\t                self.curr_prompt, \"\", get_temp_at_index(i), self.question, self.model, i\n\t            )\n\t            for i in range(self.num_decompositions)\n", "        ]\n\t    def populate_decomposition_steps(self):\n\t        # print(\"\\npopulating_decompositions_steps\")\n\t        self.decompsition_steps = [\n\t            format_decompsition_break(decomposition)\n\t            if type(decomposition) == BreakDecomposition\n\t            else format_decompsition_string(decomposition)\n\t            for decomposition in self.decompositions\n\t        ]\n\t    def populate_trace_entailments(self):\n", "        print(\"\\npopulate_trace_entailments\")\n\t        trace_results = [\n\t            [\n\t                TraceResult(\n\t                    positive=self._get_entailment_res(\n\t                        linearize_decompositions(decompsition[: i + 1]),\n\t                        self.statement.positive,\n\t                    ),\n\t                    negative=self._get_entailment_res(\n\t                        linearize_decompositions(decompsition[: i + 1]),\n", "                        self.statement.negative,\n\t                    ),\n\t                )\n\t                for i in range(len(decompsition))\n\t            ]\n\t            for decompsition in self.decompsition_steps\n\t        ]\n\t        self.traces_entailments = trace_results\n\t        print(\"\\npopulate_trace_entailments_google\")\n\t        self.traces_entailments_google = []\n", "    def populate(self):\n\t        \"\"\" \"\"\"\n\t        self.populate_statement()\n\t        self.populate_decompositions()\n\t        self.populate_decomposition_steps()\n\t        # Todo: TW - Commented out because of DeBerta in background\n\t        # self.populate_trace_entailments()\n\tclass Answer(str, Enum):\n\t    Yes = \"YES\"\n\t    No = \"NO\"\n", "    MaybeYes = \"MaybeYes\"\n\t    MaybeNo = \"MaybeNo\"\n\t    Donno = \"Donno\"\n\t@dataclass\n\tclass QuestionWithAnswer:\n\t    question: QuestionV1\n\t    answers: Dict[str, Answer]\n\t    gpt_answers: List[str]\n"]}
{"filename": "src/dataset_readers/dataset_readers_factory.py", "chunked_list": ["from typing import Dict, Type\n\tfrom src.common.abstract_factory import AbstractFactory\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tfrom src.dataset_readers.readers.fermi_reader import FermiReader\n\t# from src.dataset_readers.readers.feverous_reader import FeverousDataReader\n\t# from src.dataset_readers.readers.hotpotqa_reader import HotpotQADataReader\n\t# from src.dataset_readers.readers.hover_reader import HoverDataReader\n\t# from src.dataset_readers.readers.quartz_reader import QuartzDataReader\n\t# from src.dataset_readers.readers.bamboogle_reader import BamboogleDataReader\n\tfrom src.dataset_readers.readers.strategy_qa import StrategyQADataReader\n", "from src.dataset_readers.readers.wikihop_reader import WikiHopDataReader\n\t# from src.info_seeking_questions.dataset_reader import DatasetReader\n\tclass DatasetReadersFactory(AbstractFactory):\n\t    \"\"\" \"\"\"\n\t    def get_instance_name_to_class_dict(self) -> Dict[str, Type[DatasetReader]]:\n\t        return {\n\t            \"strategy_qa\": StrategyQADataReader,\n\t            # \"bamboogle\": BamboogleDataReader,\n\t            # \"quartz\": QuartzDataReader,\n\t            \"wikihop\": WikiHopDataReader,\n", "            # \"feverous\": FeverousDataReader,\n\t            # \"hover\": HoverDataReader,\n\t            # \"hotpotqa\": HotpotQADataReader,\n\t            # \"fermi\": FermiReader,\n\t        }\n"]}
{"filename": "src/dataset_readers/readers/hotpotqa_reader.py", "chunked_list": ["from typing import Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tclass HotpotQADataReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def __init__(\n\t        self,\n", "        dataset_path=\"../../../data/full_datasets/hotpotqa/hotpot_dev_fullwiki_v1.json\",\n\t    ):\n\t        super().__init__(dataset_path=dataset_path)\n\t        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.examples = dataset_utils.load_json(self.dataset_path)\n\t    def get_examples(self):\n\t        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        return Example(\n", "            qid=example[\"_id\"],\n\t            question=example[\"question\"],\n\t            gold_answer=example[\"answer\"],\n\t            prev_model_answer=None,\n\t            metadata=example,\n\t        )\n\t# reader = HotpotQADataReader()\n\t# reader.read()\n\t# for ex in reader.get_examples()[:10]:\n\t#     x = reader.parse_example(ex)\n", "#     print(x)\n\t#     print(\"*\"*20)\n\t# print(len(reader.get_examples()))\n"]}
{"filename": "src/dataset_readers/readers/wikihop_reader.py", "chunked_list": ["from typing import Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tclass WikiHopDataReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def __init__(\n\t        self,\n", "        dataset_path=\"data/test_datasets/2wikihop/dev.json\",\n\t    ):\n\t        super().__init__(dataset_path=dataset_path)\n\t        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.examples = dataset_utils.load_json(self.dataset_path)\n\t    def get_examples(self):\n\t        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        # Manually corrected wrong answers in Bamboogle\n", "        return Example(\n\t            qid=example[\"_id\"],\n\t            question=example[\"question\"],\n\t            gold_answer=example[\"answer\"],\n\t            prev_model_answer=None,\n\t            metadata=example,\n\t        )\n"]}
{"filename": "src/dataset_readers/readers/dataset_reader.py", "chunked_list": ["class DatasetReader:\n\t    \"\"\"\n\t    info-seeking dataset reader\n\t    \"\"\"\n\t    def __init__(self, dataset_path):\n\t        self.dataset_path = dataset_path\n\t    def read(self, rand_sample=None):\n\t        raise NotImplementedError(\"Please Implement this method\")\n\t    def parse_example(self, example):\n\t        raise NotImplementedError(\"Please Implement this method\")\n"]}
{"filename": "src/dataset_readers/readers/strategy_qa.py", "chunked_list": ["from typing import Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tclass StrategyQADataReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def __init__(self, dataset_path=\"../../../data/full_datasets/strategyqa/sqa.csv\"):\n\t        super().__init__(dataset_path=dataset_path)\n", "        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.examples = dataset_utils.read_csv_to_dict(self.dataset_path)\n\t    def get_examples(self):\n\t        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        return Example(\n\t            qid=example[\"qid\"],\n\t            question=example[\"question\"],\n\t            gold_answer=example[\"gold_answer\"],\n", "            prev_model_answer=example[\"model_answer\"],\n\t            metadata=example,\n\t        )\n"]}
{"filename": "src/dataset_readers/readers/quartz_reader.py", "chunked_list": ["from typing import List, Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.info_seeking_questions.dataset_reader import DatasetReader\n\tclass QuartzDataReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def __init__(\n\t        self, dataset_path=\"data/test_datasets/quartz-dataset-v1-aug2019/dev.jsonl\"\n", "    ):\n\t        super().__init__(dataset_path=dataset_path)\n\t        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.examples = dataset_utils.load_jsonl(self.dataset_path)\n\t    def get_examples(self):\n\t        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        def _format_choices(choices: List[Dict[str, str]]) -> str:\n\t            return \" \".join(\n", "                [\n\t                    f'{choice[\"label\"]}. {choice[\"text\"]}'\n\t                    for choice in example[\"question\"][\"choices\"]\n\t                ]\n\t            )\n\t        return Example(\n\t            qid=example[\"id\"],\n\t            question=example[\"question\"][\"stem\"]\n\t            + \" \"\n\t            + _format_choices(example[\"question\"][\"choices\"]),\n", "            gold_answer=example[\"answerKey\"],\n\t            prev_model_answer=None,\n\t            metadata=example,\n\t        )\n"]}
{"filename": "src/dataset_readers/readers/fermi_reader.py", "chunked_list": ["from typing import List, Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tfrom src.serpapi.serpapi import get_string_hash\n\tclass FermiReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def __init__(self, dataset_path=\"data/test_datasets/fermi/train_realfp.json\"):\n", "        super().__init__(dataset_path=dataset_path)\n\t        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.train_examples = dataset_utils.load_json(self.dataset_path)\n\t        self.dev_examples = dataset_utils.load_json(\n\t            self.dataset_path.replace(\"train\", \"val\")\n\t        )\n\t        unique_questions, self.examples = set(), []\n\t        for x in self.train_examples + self.dev_examples:\n\t            if x[\"question\"] not in unique_questions:\n", "                unique_questions.add(x[\"question\"])\n\t                self.examples.append(x)\n\t    def get_examples(self):\n\t        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        def _format_question(example: Dict) -> str:\n\t            # check if the there's a $\n\t            if \"$\" in example[\"answer\"]:\n\t                return f'{example[\"question\"]} Unit: $.'\n\t            # else check the unit after the space\n", "            unit_split = example[\"answer\"].split(\" \")\n\t            if len(unit_split) == 1:\n\t                return example[\"question\"]\n\t            else:\n\t                return f'{example[\"question\"]} Unit: {unit_split[-1]}.'\n\t        return Example(\n\t            qid=get_string_hash(example[\"question\"]),\n\t            question=_format_question(example),\n\t            gold_answer=example[\"answer\"],\n\t            prev_model_answer=None,\n", "            metadata=example,\n\t        )\n"]}
{"filename": "src/dataset_readers/readers/bamboogle_reader.py", "chunked_list": ["from typing import Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tclass BamboogleDataReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def __init__(\n\t        self,\n", "        dataset_path=\"data/test_datasets/bamboogle_prerelease_random.csv\",\n\t    ):\n\t        super().__init__(dataset_path=dataset_path)\n\t        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.examples = dataset_utils.read_csv_to_dict(self.dataset_path)\n\t    def get_examples(self):\n\t        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        # Manually corrected wrong answers in Bamboogle\n", "        corrected_ans = (\n\t            example[\"Answer_corrected\"]\n\t            if example[\"Answer_corrected\"] != \"\"\n\t            else example[\"Answer\"]\n\t        )\n\t        return Example(\n\t            qid=[i for i, x in enumerate(self.examples) if x == example][0],\n\t            question=example[\"Question\"],\n\t            gold_answer=corrected_ans,\n\t            prev_model_answer=None,\n", "            metadata=example,\n\t        )\n"]}
{"filename": "src/dataset_readers/readers/feverous_reader.py", "chunked_list": ["from typing import Dict\n\tfrom src.common import dataset_utils\n\tfrom src.dataclasses import Example\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tPROMPT_CLAIMS = [\n\t    {\n\t        \"id\": 1,\n\t        \"label\": \"Yes\",\n\t        \"claim\": \"Albert Foday, an attacking midfielder and free kick expert (a method of restarting play after an infringement of the laws by the opponent) plays on Kallon FC, which team he played on in 2002-2003 before going to the Mighty Barolle for two years before his return.\",\n\t    },\n", "    {\n\t        \"id\": 2,\n\t        \"label\": \"Yes\",\n\t        \"claim\": \"Shattered Angels (a Japanese Manga) had five Absolute Angels, one of which was ClaÃƒ\\xadomh Solais.\",\n\t    },\n\t    {\n\t        \"id\": 3,\n\t        \"label\": \"No\",\n\t        \"claim\": \"KLLB was a radio station owned by Ford Motor Corporation and licensed in West Jordan, Utah until 2017.\",\n\t    },\n", "    {\n\t        \"id\": 4,\n\t        \"label\": \"Yes\",\n\t        \"claim\": \"Mikro globulus belongs to Kingdom: Animalia, Phylum: Gastropoda, Class: Mollusca, and Subclass: Vetigastropoda.\",\n\t    },\n\t    {\n\t        \"id\": 5,\n\t        \"label\": \"No\",\n\t        \"claim\": \"Valeri Karpin played for CSKA Moscow in the 1986 Soviet Second League season.\",\n\t    },\n", "    {\n\t        \"id\": 6,\n\t        \"label\": \"Yes\",\n\t        \"claim\": \"2016 BBC Sports Personality of the Year Award was won by Andy Murray of Tennis (who ranked world No. 1 by the Association of Tennis Professionals (ATP) for 41 weeks, and finished as the year-end No. 1 in 2016), garnering 33.1% of the votes ahead of other sports personalities such as Alistair Brownlee, Nick Skelton, and Mo Farah.\",\n\t    },\n\t    {\n\t        \"id\": 7,\n\t        \"label\": \"No\",\n\t        \"claim\": \"Ecstasea is a 585 tonnes yacht built in the Cayman Islands, an autonomous British Overseas Territory in the western Red Sea.\",\n\t    },\n", "    {\n\t        \"id\": 8,\n\t        \"label\": \"Yes\",\n\t        \"claim\": \"The DOHC Genesis engine has five valves per cylinder as Yamaha adopted the 5-valve concept because it allowed both excellent volumetric efficiency and high rpm.\",\n\t    },\n\t    {\n\t        \"id\": 9,\n\t        \"label\": \"No\",\n\t        \"claim\": \"Two episodes of Das unsichtbare Visier are Das Nest im Urwald, which is 89 minutes long, and Depot im Skagerrak, which is 117 minutes long.\",\n\t    },\n", "    {\n\t        \"id\": 10,\n\t        \"label\": \"No\",\n\t        \"claim\": \"Robert Craig Kent, born on November 28, 1828 in Wythe County, Virginia, was the 16th Lieutenant Governor of Virginia.\",\n\t    },\n\t]\n\tclass FeverousDataReader(DatasetReader):\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n", "    def __init__(\n\t        self,\n\t        dataset_path=\"../../../data/full_datasets/feverous/feverous_dev_challenges.jsonl\",\n\t    ):\n\t        super().__init__(dataset_path=dataset_path)\n\t        self.examples = None\n\t    def read(self, rand_sample=None):\n\t        self.examples = dataset_utils.load_jsonl(self.dataset_path)\n\t        # self.examples = PROMPT_CLAIMS  # todo: delete\n\t    def get_examples(self):\n", "        return self.examples\n\t    def parse_example(self, example: Dict) -> Example:\n\t        claim_to_question = f\"Is it true that {example['claim']}?\".replace(\".?\", \"?\")\n\t        gold_answer = \"Yes\" if example[\"label\"] == \"SUPPORTS\" else \"No\"\n\t        return Example(\n\t            qid=example[\"id\"],\n\t            question=claim_to_question,\n\t            gold_answer=gold_answer,\n\t            prev_model_answer=None,\n\t            metadata=example,\n", "        )\n\t#\n\t# reader = FeverousDataReader()\n\t# reader.read()\n\t# for ex in reader.get_examples()[:10]:\n\t#     x = reader.parse_example(ex)\n\t#     print(x)\n\t#     print(\"*\"*20)\n\t# print(len(reader.get_examples()))\n"]}
{"filename": "src/serpapi/serpapi.py", "chunked_list": ["import hashlib\n\timport os\n\timport json\n\tfrom typing import Dict\n\tfrom IPython.utils import io\n\tfrom serpapi import GoogleSearch\n\tfrom src.common.config import Config\n\tfrom src.serpapi.wikipedia import get_wikipedia_text\n\tREAD_CACHE = True\n\tdef google(question):\n", "    print(f\"Asking google: {question}\")\n\t    params = {\n\t        \"api_key\": os.getenv(\"SERP_API_KEY\"),\n\t        \"engine\": \"google\",\n\t        \"q\": question,\n\t        \"google_domain\": \"google.com\",\n\t        \"gl\": \"us\",\n\t        \"hl\": \"en\",\n\t    }\n\t    with io.capture_output() as captured:  # disables prints from GoogleSearch\n", "        print(\"hi man what's up?\")\n\t        search = GoogleSearch(params)\n\t        res = search.get_dict()\n\t    answer = None\n\t    snippet = None\n\t    title = None\n\t    if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\n\t        answer = res[\"answer_box\"][\"answer\"]\n\t    if \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n\t        snippet = res[\"answer_box\"][\"snippet\"]\n", "        title = res[\"answer_box\"][\"title\"]\n\t    # elif 'answer_box' in res.keys() and 'snippet_highlighted_words' in res['answer_box'].keys():\n\t    #     toret = res['answer_box'][\"snippet_highlighted_words\"][0]\n\t    elif (\n\t        \"answer_box\" in res.keys()\n\t        and \"contents\" in res[\"answer_box\"].keys()\n\t        and \"table\" in res[\"answer_box\"][\"contents\"].keys()\n\t    ):\n\t        snippet = res[\"answer_box\"][\"contents\"][\"table\"]\n\t        title = res[\"answer_box\"][\"title\"]\n", "    elif \"answer_box\" in res.keys() and \"list\" in res[\"answer_box\"].keys():\n\t        snippet = res[\"answer_box\"][\"list\"]\n\t        title = res[\"answer_box\"][\"title\"]\n\t    elif \"organic_results\" in res and \"snippet\" in res[\"organic_results\"][0].keys():\n\t        snippet = res[\"organic_results\"][0][\"snippet\"]\n\t        title = res[\"organic_results\"][0][\"title\"]\n\t    elif (\n\t        \"organic_results\" in res\n\t        and \"rich_snippet_table\" in res[\"organic_results\"][0].keys()\n\t    ):\n", "        snippet = res[\"organic_results\"][0][\"rich_snippet_table\"]\n\t        title = res[\"organic_results\"][0][\"title\"]\n\t    else:\n\t        snippet = None\n\t    if snippet is not None:\n\t        title = title.replace(\"- Wikipedia\", \"\").strip()\n\t        toret = f\"{title}: {snippet}\"\n\t        toret = f\"{toret} So the answer is {answer}.\" if answer is not None else toret\n\t    else:\n\t        toret = \"\"\n", "    return [toret, res]\n\tdef get_sentences(text, max_num_sentences, reverse=None):\n\t    if text == \"\":\n\t        return text\n\t    sentences = text.split(\". \")\n\t    ret_sentences = \"\"\n\t    actual_num_sentences = min(max_num_sentences, len(sentences))\n\t    if reverse:\n\t        for i in reversed(range(actual_num_sentences)):\n\t            ret_sentences += f\"{sentences[i]}. \"\n", "    else:\n\t        for i in range(actual_num_sentences):\n\t            ret_sentences += f\"{sentences[i]}. \"\n\t    return ret_sentences.strip()\n\tdef get_first_sentences(text, max_num_sentences):\n\t    return get_sentences(text, max_num_sentences)\n\tdef get_last_sentences(text, max_num_sentences):\n\t    return get_sentences(text, max_num_sentences, reverse=True)\n\tdef get_snippet_wiki_paragraph(wikipage_title, snippet):\n\t    full_wikipage_text = get_wikipedia_text(wikipage_title)\n", "    print(f\"Wikipedia title: {wikipage_title}\")\n\t    print(f\"Google snippet: {snippet}\")\n\t    try:\n\t        assert snippet in full_wikipage_text\n\t        text_before_snippet, text_after_snippet = full_wikipage_text.split(snippet)\n\t        prev_sentences = get_last_sentences(text_before_snippet, 5).strip()\n\t        next_sentences = get_first_sentences(text_after_snippet, 5).strip()\n\t        return f\"{prev_sentences} {snippet} {next_sentences}\"\n\t    except AssertionError:\n\t        print(\"* Unable to find snippet in Wikipedia text, return original snippet.\")\n", "    return snippet\n\tdef get_question_wiki_snippet(question, cache=None):\n\t    # google_wikipedia_query = f\"site:en.wikipedia.org '{question}'\"\n\t    try:\n\t        cached_query_results = read_google_res_from_cache(query=question)\n\t        snippet = cached_query_results[\"snippet\"]\n\t        print(f\"Read from cache for query: {question}, cached snippet: {snippet}\")\n\t    except IOError:\n\t        google_wikipedia_query = (\n\t            f\"en.wikipedia.org {question}\"  # same as Ori's query format\n", "        )\n\t        snippet, full_results = google(google_wikipedia_query)\n\t        # print(f\"full_results: {full_results}\")\n\t        # print(f\"snippet: {snippet}\")\n\t        if cache:\n\t            print(f\"Caching snippet: {snippet}\")\n\t            cache_google_res(\n\t                question, {\"snippet\": snippet, \"full_results\": full_results}\n\t            )\n\t    clean_snippet = snippet.replace(\"...\", \"\").strip()\n", "    return clean_snippet\n\tdef get_question_google_snippet(question, cache=None):\n\t    # google_wikipedia_query = f\"site:en.wikipedia.org '{question}'\"\n\t    try:\n\t        cached_query_results = read_google_res_from_cache(query=question)\n\t        snippet = cached_query_results[\"snippet\"]\n\t        print(f\"Read from cache for query: {question}, cached snippet: {snippet}\")\n\t    except IOError:\n\t        google_wikipedia_query = f\"{question}\"  # same as Ori's query format\n\t        snippet, full_results = google(google_wikipedia_query)\n", "        # print(f\"full_results: {full_results}\")\n\t        # print(f\"snippet: {snippet}\")\n\t        if cache:\n\t            print(f\"Caching snippet: {snippet}\")\n\t            cache_google_res(\n\t                question, {\"snippet\": snippet, \"full_results\": full_results}\n\t            )\n\t    clean_snippet = snippet.replace(\"...\", \"\").strip()\n\t    return clean_snippet\n\tdef get_string_hash(query: str) -> str:\n", "    return hashlib.md5(query.encode()).hexdigest()\n\tdef cache_google_res(query: str, res: Dict) -> None:\n\t    \"\"\"\"\"\"\n\t    filename = get_string_hash(query)\n\t    retriever_cache_dir = Config().get(\"decomposition.retriever_cache_dir\")\n\t    with open(f\"{retriever_cache_dir}/{filename}.json\", \"w\") as json_file:\n\t        json.dump(res, json_file)\n\t    # with open(f\"strategy_qa/google_results_2/{filename}.json\", \"w\") as json_file:\n\t    #     json.dump(res, json_file)\n\tdef read_google_res_from_cache(query: str) -> Dict:\n", "    filename = get_string_hash(query)\n\t    retriever_cache_dir = Config().get(\"decomposition.retriever_cache_dir\")\n\t    with open(f\"{retriever_cache_dir}/{filename}.json\", \"r\") as f:\n\t        data = json.load(f)\n\t    # with open(f\"strategy_qa/google_results_2/{filename}.json\", \"r\") as f:\n\t    #     data = json.load(f)\n\t    return data\n"]}
{"filename": "src/serpapi/wikipedia.py", "chunked_list": ["import wikipedia\n\timport re\n\tdef get_wikipedia_text(page_name):\n\t    def clean(text):\n\t        text = re.sub(r\"==.*?==+\", \"\", text)\n\t        text = text.replace(\"\\n\", \"\")\n\t        return text\n\t    def remove_parentheses_from_first_sent(text):\n\t        \"\"\"remove first sentence parentheses as the info contains redundant spelling info\"\"\"\n\t    # Specify the title of the Wikipedia page\n", "    wiki = wikipedia.page(page_name)\n\t    # Extract the plain text content of the page\n\t    page_text = wiki.content\n\t    return clean(page_text)\n"]}
{"filename": "src/common/dataset_utils.py", "chunked_list": ["import csv\n\tfrom tqdm import tqdm\n\timport pandas as pd\n\timport json\n\tfrom datetime import datetime\n\tdef file_as_string(file_path):\n\t    with open(file_path, \"r\") as file:\n\t        file_str = file.read()\n\t    return file_str.strip()\n\tdef timestamp_string():\n", "    now = datetime.now()\n\t    now = str(now).split(\".\")[0].strip()\n\t    return str(now).replace(\" \", \"_\").replace(\":\", \"-\")\n\tdef chunk_list_to_sublists(original_list, chunk_size):\n\t    chunked_list = list()\n\t    for i in range(0, len(original_list), chunk_size):\n\t        chunked_list.append(original_list[i : i + chunk_size])\n\t    return chunked_list\n\tdef read_csv_to_dict(csv_file, encoding=None):\n\t    # replace NaN values (empty cell) with \"\"\n", "    dict_from_csv = (\n\t        pd.read_csv(csv_file, encoding=encoding).fillna(\"\").to_dict(\"records\")\n\t    )\n\t    return dict_from_csv\n\tdef write_to_json(data, json_file, encoding=None):\n\t    encoding = \"utf-8\" if encoding is None else encoding\n\t    with open(json_file, mode=\"w+\", encoding=encoding) as file:\n\t        json.dump(data, file, indent=4)\n\t    return True\n\tdef load_json(filepath, encoding=None):\n", "    encoding = \"utf-8\" if encoding is None else encoding\n\t    with open(filepath, mode=\"r\", encoding=encoding) as reader:\n\t        text = reader.read()\n\t    return json.loads(text)\n\tdef load_jsonl(filepath, encoding=None):\n\t    encoding = \"utf-8\" if encoding is None else encoding\n\t    with open(filepath, \"r\", encoding=encoding) as reader:\n\t        data = [json.loads(line) for line in reader]\n\t    return data\n\tdef jaccard_similarity(list1, list2):\n", "    intersection = len(list(set(list1).intersection(list2)))\n\t    union = (len(list1) + len(list2)) - intersection\n\t    return float(intersection) / union\n\tdef remove_duplicates_from_list(items_list):\n\t    return list(dict.fromkeys(items_list))\n\tdef write_dict_list_to_csv(dict_list, dict_keys_list, output_csv):\n\t    csv_columns = dict_keys_list\n\t    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n\t        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n\t        writer.writeheader()\n", "        for i in tqdm(range(len(dict_list))):\n\t            data = dict_list[i]\n\t            writer.writerow(data)\n\t    return True\n"]}
{"filename": "src/common/config.py", "chunked_list": ["global logger\n\timport datetime\n\timport json\n\timport logging\n\ttracer = logging.getLogger(\"config\")\n\ttracer.setLevel(logging.CRITICAL)  # or desired level\n\ttracer.addHandler(logging.FileHandler(\"indexer.log\"))\n\tclass Config:\n\t    \"\"\"A python singleton\"\"\"\n\t    # Usage example\n", "    # Config().write_log('INFO', 'train_metric', context_dict=elastic_train_metrics)\n\t    class __impl:\n\t        \"\"\"Implementation of the singleton interface\"\"\"\n\t        def __init__(self):\n\t            self._start_time = datetime.datetime.utcnow().strftime(\"%d-%m-%y_%H:%M\")\n\t            self._config = {\"config_path\": \"\", \"use_wandb\": False, \"use_elastic\": False}\n\t        def load(self, config_path):\n\t            \"\"\"\n\t            Loads a full config jsonnet\n\t            \"\"\"\n", "            with open(config_path) as f:\n\t                self._config.update(json.load(f))\n\t        def override_dict(self, new_config):\n\t            \"\"\"\n\t            Adds a value to the config if missing, or overrides if already in config\n\t            key: string path in config separated by '.' e.g. training_arguments.num_train_epochs\n\t            \"\"\"\n\t            for key, val in new_config.items():\n\t                tokens = key.split(\".\")\n\t                config_internal = self._config\n", "                for token in tokens[:-1]:\n\t                    if token in config_internal:\n\t                        config_internal = config_internal[token]\n\t                    else:\n\t                        raise (ValueError(f\"could not find {key} in config!\"))\n\t                if tokens[-1] in config_internal:\n\t                    if val is not None:\n\t                        if val == \"False\":\n\t                            config_internal[tokens[-1]] = False\n\t                        elif val == \"True\":\n", "                            config_internal[tokens[-1]] = True\n\t                        else:\n\t                            config_internal[tokens[-1]] = val\n\t                else:\n\t                    raise (ValueError(f\"could not find {key} in config!\"))\n\t        def add_value(self, key, value):\n\t            \"\"\"\n\t            Adds a value to the config if missing, or overrides if already in config\n\t            key: string path in config separated by '.' e.g. training_arguments.num_train_epochs\n\t            \"\"\"\n", "            pass\n\t        def iter_on_config(self, d, key, full_key=\"\"):\n\t            entries = []\n\t            vals = []\n\t            val = None\n\t            for k, v in d.items():\n\t                if isinstance(v, dict):\n\t                    if key in v:\n\t                        vals.append(v[key])\n\t                        keys.append()\n", "                        return keys, vals\n\t                    else:\n\t                        keys, vals = self.iter_on_config(v, full_key)\n\t                        for i_k, i_v in zip(key, val):\n\t                            vals.append(val)\n\t            return full_key + \".\" + key, val\n\t        def get(self, key):\n\t            \"\"\"\n\t            Adds a value to the config if missing, or overrides if already in config\n\t            \"\"\"\n", "            tokens = key.split(\".\")\n\t            result = self._config\n\t            for token in tokens[:-1]:\n\t                if token in result:\n\t                    result = result[token]\n\t            if tokens[-1] in result:\n\t                return result[tokens[-1]]\n\t            else:\n\t                # return self.iter_on_config(self._config, key)\n\t                tracer.info(f\"could not find {key} in config, return none!\")\n", "                return None\n\t            return result\n\t    # storage for the instance reference\n\t    __instance = None\n\t    def __init__(self):\n\t        \"\"\"Create singleton instance\"\"\"\n\t        # Check whether we already have an instance\n\t        if Config.__instance is None:\n\t            # Create and remember instance\n\t            Config.__instance = Config.__impl()\n", "        # Store instance reference as the only member in the handle\n\t        self.__dict__[\"_Singleton__instance\"] = Config.__instance\n\t    def __getattr__(self, attr):\n\t        \"\"\"Delegate access to implementation\"\"\"\n\t        return getattr(self.__instance, attr)\n\t    def __setattr__(self, attr, value):\n\t        \"\"\"Delegate access to implementation\"\"\"\n\t        return setattr(self.__instance, attr, value)\n"]}
{"filename": "src/common/logger.py", "chunked_list": ["import logging\n\tlogging.basicConfig(level=logging.DEBUG)\n\tdef get_logger(LOGGING_NAME=None):\n\t    return logging.getLogger(LOGGING_NAME)\n"]}
{"filename": "src/common/abstract_factory.py", "chunked_list": ["from abc import ABC\n\tfrom threading import Lock\n\tfrom typing import Dict, TypeVar, Generic\n\tfrom src.common.logger import get_logger\n\tlogger = get_logger()\n\tT = TypeVar(\"T\")\n\tclass AbstractFactory(ABC, Generic[T]):\n\t    \"\"\"\n\t    Abstract class to represent a factory.\n\t    The concrete class factory should only implement the method `get_instance_name_to_class_dict`.\n", "    By default, Each instance is kept one time (\\singleton), unless one is passing `use_cache=False`.\n\t    \"\"\"\n\t    def __init__(self):\n\t        self._instances_dict: Dict[str, T] = {}\n\t        self._lock = Lock()\n\t    def get_instance_name_to_class_dict(self) -> Dict[str, T]:\n\t        raise NotImplementedError()\n\t    def get_instance(self, instance_name: str, use_cache: bool = True, *args, **kwargs):\n\t        cleaned_instance_name = instance_name.lower()\n\t        # check if we already created that instance from before and use it\n", "        if cleaned_instance_name in self._instances_dict and use_cache:\n\t            return self._instances_dict[cleaned_instance_name]\n\t        # map from instance name to instance type\n\t        instance_class = self.get_instance_name_to_class_dict().get(\n\t            cleaned_instance_name\n\t        )\n\t        if not instance_class:\n\t            raise ValueError(\n\t                f\"{cleaned_instance_name} does not exist. \"\n\t                f\"Please choose from {self.get_instance_name_to_class_dict().keys()}\"\n", "            )\n\t        with self._lock:\n\t            # check again that the instance was not created from other threads\n\t            if cleaned_instance_name in self._instances_dict and use_cache:\n\t                return self._instances_dict[cleaned_instance_name]\n\t            # instantiate the class\n\t            instance = instance_class.create(*args, **kwargs)\n\t            # save for later use\n\t            self._instances_dict[cleaned_instance_name] = instance\n\t            return instance\n"]}
{"filename": "src/pred_evaluators/evaluation.py", "chunked_list": ["# these functions are heavily influenced by the HF squad_metrics.py script\n\tdef normalize_text(s):\n\t    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n\t    import string, re\n\t    def remove_articles(text):\n\t        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n\t        return re.sub(regex, \" \", text)\n\t    def white_space_fix(text):\n\t        return \" \".join(text.split())\n\t    def remove_punc(text):\n", "        exclude = set(string.punctuation)\n\t        return \"\".join(ch for ch in text if ch not in exclude)\n\t    def lower(text):\n\t        return text.lower()\n\t    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\tdef compute_exact_match(prediction, truth):\n\t    return int(normalize_text(prediction) == normalize_text(truth))\n\tdef compute_f1(prediction, truth):\n\t    pred_tokens = normalize_text(prediction).split()\n\t    truth_tokens = normalize_text(truth).split()\n", "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n\t    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n\t        return int(pred_tokens == truth_tokens)\n\t    common_tokens = set(pred_tokens) & set(truth_tokens)\n\t    # if there are no common tokens then f1 = 0\n\t    if len(common_tokens) == 0:\n\t        return 0\n\t    prec = len(common_tokens) / len(pred_tokens)\n\t    rec = len(common_tokens) / len(truth_tokens)\n\t    return 2 * (prec * rec) / (prec + rec)\n"]}
{"filename": "src/pred_evaluators/evaluators_factory.py", "chunked_list": ["from typing import Dict, Type\n\tfrom src.common.abstract_factory import AbstractFactory\n\tfrom src.pred_evaluators.pred_evaluators.bamboogle_evaluator import BamboogleEvaluator\n\tfrom src.pred_evaluators.pred_evaluators.base_evaluator import Evaluator\n\tfrom src.pred_evaluators.pred_evaluators.em_evaluator import EMEvaluator\n\tfrom src.pred_evaluators.pred_evaluators.hover_evaluator import HoverEvaluator\n\tfrom src.pred_evaluators.pred_evaluators.fermi_evaluator import FermiEvaluator\n\tfrom src.pred_evaluators.pred_evaluators.quartz_em_evaluator import QuartzEMEvaluator\n\tfrom src.pred_evaluators.pred_evaluators.wikihop_evaluator import WikiHopEvaluator\n\tclass EvaluatorsFactory(AbstractFactory):\n", "    \"\"\" \"\"\"\n\t    def get_instance_name_to_class_dict(self) -> Dict[str, Type[Evaluator]]:\n\t        return {\n\t            \"em\": EMEvaluator,\n\t            \"quartz\": QuartzEMEvaluator,\n\t            \"bamboogle\": BamboogleEvaluator,\n\t            \"hover\": HoverEvaluator,\n\t            \"fermi\": FermiEvaluator,\n\t            \"wikihop\": WikiHopEvaluator,\n\t        }\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/quartz_em_evaluator.py", "chunked_list": ["class QuartzEMEvaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def evaluate(self, pred: str, gold: str) -> str:\n\t        label_split = pred.split(\".\")[0].replace(\"\\n\", \"\")[:1]\n\t        if len(label_split) == 0:\n\t            return\n\t        return label_split[0] == gold\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/bamboogle_evaluator.py", "chunked_list": ["from src.pred_evaluators.evaluation import compute_f1\n\tclass BamboogleEvaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def evaluate(self, pred: str, gold: str) -> str:\n\t        return compute_f1(pred, gold)\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/hover_evaluator.py", "chunked_list": ["class HoverEvaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def evaluate(self, pred: str, gold: str) -> str:\n\t        pred = pred.lower().strip()\n\t        if not (pred.startswith(\"yes\") or pred.startswith(\"no\")):\n\t            return 0\n\t        else:\n\t            pred = \"yes\" if pred.startswith(\"yes\") else \"no\"\n", "        return 1 if pred.lower() == gold.lower() else 0\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/wikihop_evaluator.py", "chunked_list": ["import re\n\timport string\n\tfrom collections import Counter\n\tdef normalize_answer(s):\n\t    def remove_articles(text):\n\t        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\t    def white_space_fix(text):\n\t        return \" \".join(text.split())\n\t    def remove_punc(text):\n\t        exclude = set(string.punctuation)\n", "        return \"\".join(ch for ch in text if ch not in exclude)\n\t    def lower(text):\n\t        return text.lower()\n\t    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\tclass WikiHopEvaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def evaluate(self, pred: str, gold: str) -> str:\n\t        normalized_prediction = normalize_answer(pred)\n", "        normalized_ground_truth = normalize_answer(gold)\n\t        ZERO_METRIC = 0\n\t        if (\n\t            normalized_prediction in [\"yes\", \"no\", \"noanswer\"]\n\t            and normalized_prediction != normalized_ground_truth\n\t        ):\n\t            return ZERO_METRIC\n\t        if (\n\t            normalized_ground_truth in [\"yes\", \"no\", \"noanswer\"]\n\t            and normalized_prediction != normalized_ground_truth\n", "        ):\n\t            return ZERO_METRIC\n\t        prediction_tokens = normalized_prediction.split()\n\t        ground_truth_tokens = normalized_ground_truth.split()\n\t        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n\t        num_same = sum(common.values())\n\t        if num_same == 0:\n\t            return ZERO_METRIC\n\t        precision = 1.0 * num_same / len(prediction_tokens)\n\t        recall = 1.0 * num_same / len(ground_truth_tokens)\n", "        f1 = (2 * precision * recall) / (precision + recall)\n\t        return f1\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/fermi_evaluator.py", "chunked_list": ["from copy import deepcopy\n\timport numpy as np\n\timport pint\n\tureg = pint.UnitRegistry(system=\"mks\", autoconvert_offset_to_baseunit=True)\n\t# ureg.load_definitions(\"data/test_datasets/fermi/units.txt\")\n\tclass FermiEvaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def convert_units(self, answer):\n", "        if type(answer) == str:\n\t            try:\n\t                # try converting units\n\t                original_pint = ureg(answer)\n\t            except:\n\t                # keelp as is, can be num or string\n\t                original_pint = answer\n\t        else:\n\t            original_pint = answer\n\t        # if it's still string, try parsing\n", "        if type(original_pint) == str:\n\t            try:\n\t                original_pint = float(original_pint.replace(\"$\", \"\").split(\" \")[0])\n\t            except:\n\t                original_pint = None\n\t        if original_pint is None:\n\t            return None, None\n\t        # this means it's ureg\n\t        if type(original_pint) not in [float, int]:\n\t            try:\n", "                converted_pint = original_pint.to_base_units()\n\t            except:\n\t                converted_pint = deepcopy(original_pint)\n\t            return converted_pint.magnitude, converted_pint.units\n\t        else:\n\t            return original_pint, None\n\t    def evaluate(self, pred, gold) -> str:\n\t        pred = pred.split(\"=\")[-1]\n\t        gold_split, pred_split = gold.split(\" \"), pred.split(\" \")\n\t        if len(gold_split) > 1 and len(pred_split) == 1:\n", "            gold_split_measurement_value = gold_split[-1]\n\t            if not pred_split[-1].endswith(gold_split_measurement_value):\n\t                pred += \" \" + gold_split_measurement_value\n\t        y, y_hat = self.convert_units(pred)[0], self.convert_units(gold)[0]\n\t        conversion_dict = {\"million\": 1e6, \"trillion\": 1e9}\n\t        for k, v in conversion_dict.items():\n\t            if k in pred:\n\t                converted_pred = self.convert_units(pred.replace(k, \"\".strip()))[0]\n\t                if converted_pred is not None:\n\t                    y = converted_pred * v\n", "        if type(y) not in [int, float, np.float64] or type(y_hat) not in [\n\t            int,\n\t            float,\n\t            np.float64,\n\t        ]:\n\t            return 0\n\t        if y is None or y_hat is None:\n\t            return 0\n\t        if y < 0 or y_hat < 0:\n\t            return 0\n", "        if y == 0 and y_hat == 0:\n\t            return 1\n\t        elif y == 0 or y_hat == 0:\n\t            return max(0, 1 - np.abs(np.log10(np.abs(y - y_hat))))\n\t        # elif y/y_hat == 0:\n\t        #     return 0\n\t        try:\n\t            return max(0, 3 - np.abs(np.log10(y / y_hat))) / 3\n\t        except:\n\t            return 0\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/base_evaluator.py", "chunked_list": ["class Evaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def evaluate(self, pred, gold) -> str:\n\t        raise NotImplementedError(\"Please Implement this method\")\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/em_evaluator.py", "chunked_list": ["class EMEvaluator:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def evaluate(self, pred: str, gold: str) -> str:\n\t        if type(pred) == str and type(gold) == str:\n\t            return 1 if pred.lower() == gold.lower() else 0\n\t        return 1 if pred == gold else 0\n"]}
{"filename": "src/opeanai/utils.py", "chunked_list": ["import time\n\timport openai\n\timport os\n\timport datetime\n\tfrom dotenv import load_dotenv\n\tfrom openai.error import RateLimitError, InvalidRequestError\n\timport json\n\tfrom src.common.logger import get_logger\n\tfrom src.serpapi.serpapi import google, get_question_wiki_snippet\n\tlogger = get_logger()\n", "# load openai keys from env, and set a random key at random\n\tload_dotenv()\n\topenai.api_key = os.getenv(\"OPENAI_KEY\")\n\topen_ai_keys = [openai.api_key]\n\tlast_time_out_for_keys = {k: datetime.datetime.min for k in open_ai_keys}\n\tsleep_time_per_key = 30\n\tprint()\n\tdef call_gpt(cur_prompt, stop=\"\\n\"):\n\t    \"\"\"\n\t    call the gpt-3 api\n", "    \"\"\"\n\t    print(\"calling gpt\")\n\t    ans = openai.Completion.create(\n\t        model=\"code-davinci-002\",\n\t        max_tokens=1,\n\t        stop=stop,\n\t        prompt=cur_prompt,\n\t        temperature=0,\n\t        logprobs=5,\n\t    )\n", "    returned = ans[\"choices\"][0][\"text\"]\n\t    return ans\n\tdef greenify(input):\n\t    return \"\\x1b[102m\" + input + \"\\x1b[0m\"\n\tdef yellowfy(input):\n\t    return \"\\x1b[106m\" + input + \"\\x1b[0m\"\n\t#\n\t# def format_question(question: str, show_question=True) -> str:\n\t#     \"\"\"\n\t#     format a question that wil be presented to gpt-3 validator\n", "#     \"\"\"\n\t#     # init\n\t#     res = \"Provide a yes or no answer to the question given the following facts.\\n\"\n\t#     intermediate_res = []\n\t#\n\t#     # get a list of facts and intermediate answers\n\t#     question_liines = question.split(\"\\n\")\n\t#     facts, intermediate_answers = [], []\n\t#     for line in question_liines:\n\t#         if line.startswith(QUESTION_PREFIX):\n", "#             question = line.split(QUESTION_PREFIX)[1]\n\t#             res += f\"{question}\\n\"\n\t#         if line.startswith(FOLLOW_UP_PREFIX):\n\t#             facts.append(line.split(FOLLOW_UP_PREFIX)[1])\n\t#         if line.startswith(INTERMEDIATE_ANS_PREFIX):\n\t#             intermediate_answers.append(line.split(INTERMEDIATE_ANS_PREFIX)[1])\n\t#\n\t#     for i, (fact, ans) in enumerate(zip(facts, intermediate_answers)):\n\t#         if show_question:\n\t#             res += f\"{i + 1}. {fact} {ans}\\n\"\n", "#             intermediate_res.append(f\"{res}Answer:\\n\")\n\t#         else:\n\t#             res += f\"{i + 1}. {ans}\\n\"\n\t#             intermediate_res.append(f\"{res}Answer:\\n\")\n\t#\n\t#     res += \"Answer:\\n\"\n\t#\n\t#     return intermediate_res\n\t#\n\t#\n", "# def call_gpt3_for_question(curr_question: Question) -> Question:\n\t#     \"\"\"\n\t#     calls gpt-3 an populates\n\t#     \"\"\"\n\t#     # create the set of questions\n\t#     intermediate_questions = []\n\t#     for decmop in curr_question.decompositions:\n\t#         question_with_decomp = f\"{QUESTION_PREFIX}{curr_question.question}\\n{decmop}\"\n\t#         intermediate_questions += format_question(question_with_decomp)\n\t#     qusetion_intermediate_questions = set(intermediate_questions)\n", "#     qusetion_intermediate_questions_with_answers = []\n\t#\n\t#     # print\n\t#     for intermediate_question in qusetion_intermediate_questions:\n\t#         gpt_res = call_gpt(intermediate_question[:-1], \"\\n\")\n\t#         gpt3_probs = {\n\t#             k: math.exp(v)\n\t#             for k, v in gpt_res[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n\t#             .to_dict()\n\t#             .items()\n", "#         }\n\t#         yes_probs = sum([v for k, v in gpt3_probs.items() if \"yes\" in k.lower()])\n\t#         no_probs = sum([v for k, v in gpt3_probs.items() if \"no\" in k.lower()])\n\t#         probs = {\"yes\": yes_probs, \"no\": no_probs, \"other\": 1 - yes_probs - no_probs}\n\t#         probs = {\n\t#             **probs,\n\t#             **{\n\t#                 \"yes_normalized\": probs[\"yes\"] / (probs[\"yes\"] + probs[\"no\"]),\n\t#                 \"no_normalized\": probs[\"no\"] / (probs[\"yes\"] + probs[\"no\"]),\n\t#             },\n", "#         }\n\t#         probs\n\t#         print(probs)\n\t#         qusetion_intermediate_questions_with_answers.append(\n\t#             IntermediateQuestionWithAnswer(\n\t#                 intermediate_question=intermediate_question, answer=probs\n\t#             )\n\t#         )\n\t#\n\t#     # set var\n", "#     curr_question.intermediate_questions_with_answers = (\n\t#         qusetion_intermediate_questions_with_answers\n\t#     )\n\t#     return curr_question\n\tdef change_openaikey_and_sleep():\n\t    \"\"\"\n\t    if we encountered a time-out, change the key and sleep if necessary\n\t    \"\"\"\n\t    # set the date for current time out\n\t    last_time_out_for_keys[openai.api_key] = datetime.datetime.now()\n", "    # get first time out and calc the time that passed\n\t    first_timed_out_key = min(last_time_out_for_keys, key=last_time_out_for_keys.get)\n\t    time_since_first_time_out = (\n\t        datetime.datetime.now() - last_time_out_for_keys[first_timed_out_key]\n\t    )\n\t    # change the key to be the one that was last timed out\n\t    openai.api_key = first_timed_out_key\n\t    logger.info(f\"switched to openaikey: {openai.api_key}\")\n\t    # sleep if the time that passed between now and when we last used the key is smaller than a threshold, sleep\n\t    if time_since_first_time_out.seconds < sleep_time_per_key:\n", "        sleeping_time = sleep_time_per_key - time_since_first_time_out.seconds\n\t        print(f\"sleeping for {sleeping_time} seconds\")\n\t        print(last_time_out_for_keys)\n\t        time.sleep(sleep_time_per_key - time_since_first_time_out.seconds)\n\t        print(\"finished sleeping\")\n\tdef gpt_simple_generator(\n\t    prompt, model=\"code-davinci-002\", stop_condition=\"\\n\", temperature=0, max_tokens=256\n\t):\n\t    retries = 6\n\t    for i in range(retries):\n", "        try:\n\t            print(f\"Using: {openai.api_key}\")\n\t            ans = openai.Completion.create(\n\t                model=model,\n\t                max_tokens=max_tokens,\n\t                stop=stop_condition,\n\t                prompt=prompt,\n\t                temperature=temperature,\n\t                # logprobs=5,\n\t            )\n", "            returned = [res[\"text\"] for res in ans[\"choices\"]]\n\t            # if the answer is of size 1, we were prompted with 1 prompt so print it and return\n\t            if len(returned) == 1:\n\t                print(greenify(returned[0]), end=\"\")\n\t                return returned[0], {}\n\t            # else iterate all results and return a list\n\t            else:\n\t                for res in returned:\n\t                    print(greenify(res), end=\"\")\n\t                return returned, {}\n", "        except RateLimitError as e:\n\t            print(f\"exception thrown, sleeping...\")\n\t            print(e)\n\t            change_openaikey_and_sleep()\n\t        except InvalidRequestError as e:\n\t            print(\n\t                \"Invalid request caught, maybe the prompt is too long? Sleeping, plz take a look!\"\n\t            )\n\t            time.sleep(30)\n\t            # return \"\" if type(prompt) == str else [\"\" for _ in range(len(prompt))], {}\n", "        except Exception as e:\n\t            print(e)\n\t            time.sleep(90)\n\tdef call_gpt_self_ask(cur_prompt, stop):\n\t    res = \"\"\n\t    retries = 3\n\t    # iterate decomposition for 5 steps\n\t    for i in range(5):\n\t        # get gpt ans with retries\n\t        for i in range(retries):\n", "            try:\n\t                ans = openai.Completion.create(\n\t                    model=\"code-davinci-002\",\n\t                    max_tokens=512,\n\t                    stop=[\"Context:\", \"#\"],\n\t                    prompt=cur_prompt,\n\t                    temperature=0.7,\n\t                )\n\t                break\n\t            except Exception as e:\n", "                print(\"exception thrown, sleeping...\", e)\n\t                time.sleep(60)\n\t                print(\"finished sleeping\")\n\t        # add context\n\t        returned = ans[\"choices\"][0][\"text\"]\n\t        res += returned\n\t        cur_prompt += returned\n\t        if \"Follow up: \" in returned:\n\t            question = returned.split(\"Follow up: \")[-1].replace(\"\\n\", \"\")\n\t            retrieval = get_question_wiki_snippet(question, cache=True)\n", "            cur_prompt += f\"Context: {retrieval}\\n\"\n\t            res += f\"Context: {retrieval}\\n\"\n\t        if \"So the final answer is: \" in returned:\n\t            print(greenify(res), end=\"\")\n\t            return res\n\t        print(greenify(res), end=\"\")\n\t    return res\n"]}
{"filename": "src/experiments/demo.py", "chunked_list": ["import dataclasses\n\tfrom typing import Dict\n\tfrom src.common.config import Config\n\tfrom src.dataclasses import QuestionV1\n\tfrom src.experiments.e2e.run import (\n\t    _set_entailment_input,\n\t    _call_gpt_for_entailment_with_batching,\n\t)\n\tfrom src.gpt3_accessors.gpt_accessor_factory import GptAccessorFactory\n\tfrom src.pred_evaluators.evaluators_factory import EvaluatorsFactory\n", "from src.prompting.prompt_factory import PromptFactoryDict\n\tdef run_question(\n\t    question_text: str,\n\t    dataset: str,\n\t    answer: str,\n\t    model: str = \"code-davinci-002\",\n\t    num_decomps: int = 3,\n\t) -> Dict:\n\t    \"\"\"\n\t    runs the question, first by populating decomps and then by running mcr\n", "    \"\"\"\n\t    # load config\n\t    Config().load(f\"src/config/{dataset}/config_with_retrieval_contexts_first.json\")\n\t    # set up object\n\t    question = QuestionV1(\n\t        question=question_text,\n\t        prompt=PromptFactoryDict[f\"{dataset}_decomposition\"],\n\t        gpt3_accessor=GptAccessorFactory().get_instance(\n\t            \"gpt_accessor_with_retrieval_context_first\"\n\t        ),\n", "        model=model,\n\t        num_decompositions=num_decomps,\n\t    )\n\t    question.populate()\n\t    # set mcr input\n\t    question_with_mcr = _set_entailment_input(\n\t        {\"question\": dataclasses.asdict(question), \"metadata_gold_answer\": answer},\n\t        PromptFactoryDict[f\"{dataset}_mcr\"],\n\t        entailment_values={\n\t            \"shuffle_context\": False,\n", "            \"use_ir_contexts\": False,\n\t            \"use_qa_pairs\": True,\n\t            \"question_prompt_prefix\": \"\",\n\t            \"stop_condition\": \"#\",\n\t            \"model\": model,\n\t        },\n\t        sa_contexts_first=True,\n\t        prefix=f\"mcr\",\n\t    )\n\t    print(\"\\n====================== Running meta-reasoner ======================\")\n", "    res = _call_gpt_for_entailment_with_batching(\n\t        [question_with_mcr],\n\t        EvaluatorsFactory().get_instance(\"bamboogle\"),\n\t        entailment_values={\n\t            \"shuffle_context\": False,\n\t            \"use_ir_contexts\": False,\n\t            \"use_qa_pairs\": True,\n\t            \"question_prompt_prefix\": \"\",\n\t            \"stop_condition\": \"#\",\n\t            \"model\": model,\n", "        },\n\t        prefix=f\"mcr\",\n\t    )[0]\n\t    print(f\"F1: {res['mcr_acc@mte']}\")\n\t#\n\t# dataset = \"strategyqa\"  # 2wikihop/strategyqa\n\t# question_text = \"Did Brad Peyton need to know about seismology?\"\n\t# answer = \"yes\"  # gold answer\n\t#\n\t# run_question(\n", "#     question_text=question_text,\n\t#     dataset=dataset,\n\t#     answer=answer,\n\t#     model=\"code-davinci-002\",\n\t# )\n"]}
{"filename": "src/experiments/e2e/run.py", "chunked_list": ["\"\"\"\n\tmain script to run experiments.\n\t\"\"\"\n\timport ast\n\timport numpy as np\n\tfrom tqdm import tqdm\n\timport argparse\n\timport dataclasses\n\timport json\n\timport os\n", "import random\n\tfrom collections import Counter\n\tfrom datetime import datetime\n\tfrom typing import Dict, List\n\timport pandas as pd\n\timport wandb\n\tfrom src.common.config import Config\n\tfrom src.common.logger import get_logger\n\tfrom src.consts import (\n\t    FULL_MTE_FIELD,\n", "    PRED_MTE_FIELD,\n\t    ACC_MTE_FIELD,\n\t    ACC_AT_1_FIELD,\n\t    ACC_AT_3_FIELD,\n\t    ACC_AT_MAJORITY_FIELD,\n\t    NUM_EXAMPLES_FIELD,\n\t    NUM_ABSTAINS_FIELD,\n\t    CONTEXT_ANSWER_SEP,\n\t)\n\tfrom src.dataclasses import (\n", "    QuestionV1,\n\t    QuestionWithAnswer,\n\t    format_decompsition_string,\n\t    format_ir_decomposition,\n\t)\n\tfrom src.dataset_readers.dataset_readers_factory import DatasetReadersFactory\n\tfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\n\tfrom src.gpt3_accessors.gpt_accessor_factory import GptAccessorFactory\n\tfrom src.opeanai.utils import gpt_simple_generator\n", "from src.pred_evaluators.evaluators_factory import EvaluatorsFactory\n\tfrom src.pred_evaluators.pred_evaluators.base_evaluator import Evaluator\n\tfrom src.prompting.prompt_factory import PromptFactoryDict\n\tfrom src.serpapi.serpapi import get_string_hash, google\n\tfrom src.serpapi.serpapi import get_question_wiki_snippet\n\tlogger = get_logger()\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--config_path\",\n", "        type=str,\n\t        default=\"src/config/2wikihop/config_with_retrieval_contexts_first.json\",\n\t        help=\"Config file path\",\n\t    )\n\t    return parser.parse_args()\n\tdef _call_gpt_for_entailment_with_batching(\n\t    questions: List[Dict],\n\t    evaluator: Evaluator,\n\t    entailment_values: Dict,\n\t    prefix=\"mte\",\n", "    batch_size: int = 10,\n\t) -> List[Dict]:\n\t    \"\"\"\n\t    call gpt for entailment with batching.\n\t    will sort the input to batches, call gpt for each batch and return results as a list of questions\n\t    \"\"\"\n\t    def batch_iterable(iterable, n=1):\n\t        l = len(iterable)\n\t        for ndx in range(0, l, n):\n\t            yield iterable[ndx : min(ndx + n, l)]\n", "    batches = [x for x in batch_iterable(list(questions), batch_size)]\n\t    logger.info(f\"Running entailment with batching.\")\n\t    for batch in tqdm(batches):\n\t        input_prompts = [question[f\"{prefix}_entailment_input\"] for question in batch]\n\t        # call gpt-3\n\t        gpt_trace_entailment, _ = gpt_simple_generator(\n\t            prompt=input_prompts,\n\t            model=entailment_values.get(\"model\"),\n\t            stop_condition=entailment_values.get(\"stop_condition\"),\n\t            temperature=0,\n", "        )\n\t        # change to a list if a string was returned\n\t        gpt_trace_entailment = (\n\t            [gpt_trace_entailment]\n\t            if type(gpt_trace_entailment) == str\n\t            else gpt_trace_entailment\n\t        )\n\t        # iterate questions in batch\n\t        for i, question in enumerate(batch):\n\t            # add prediction\n", "            gpt_res = gpt_trace_entailment[i]\n\t            question[f\"{prefix}_{FULL_MTE_FIELD}\"] = gpt_res\n\t            if \"the answer is:\" in question[f\"{prefix}_{FULL_MTE_FIELD}\"]:\n\t                # take the final answer\n\t                final_answer = gpt_res.split(\"the answer is:\")[-1].strip()\n\t                # remove the dot if it's the last character\n\t                if len(final_answer) and final_answer[-1] == \".\":\n\t                    final_answer = final_answer[:-1]\n\t                question[f\"{prefix}_{PRED_MTE_FIELD}\"] = final_answer.replace(\n\t                    \"\\n\", \"\"\n", "                ).strip()\n\t                if \"unknown\" in question[f\"{prefix}_{PRED_MTE_FIELD}\"].lower():\n\t                    mte_acc = None\n\t                else:\n\t                    mte_acc = evaluator.evaluate(\n\t                        question[f\"{prefix}_{PRED_MTE_FIELD}\"]\n\t                        .strip()\n\t                        .replace(\"\\n\", \"\"),\n\t                        question[\"metadata_gold_answer\"]\n\t                        if \"metadata\" not in question\n", "                        else question[\"metadata\"][\"gold_answer\"],\n\t                    )\n\t            else:\n\t                mte_acc = None\n\t            question[f\"{prefix}_{ACC_MTE_FIELD}\"] = mte_acc\n\t    return list(questions)\n\tdef _set_entailment_input(\n\t    question: Dict,\n\t    question_prompt: str,\n\t    entailment_values: Dict,\n", "    prefix=\"mte\",\n\t    seed=0,\n\t    sa_contexts_first=None,\n\t) -> Dict:\n\t    \"\"\"\n\t    adds gpt input field in place\n\t    \"\"\"\n\t    def _get_decomps(use_ir: bool) -> List[str]:\n\t        \"\"\"\n\t        get the list of facts from decompositions, either the contexts (use_ir) or the QA facts\n", "        \"\"\"\n\t        decompositions = []\n\t        # add question ir context if configged\n\t        if entailment_values.get(\"retrieve_orig_question\"):\n\t            decompositions.append(\n\t                [\n\t                    question[\"question\"][\"question\"]\n\t                    + \" | \"\n\t                    + get_question_wiki_snippet(question[\"question\"][\"question\"])\n\t                ]\n", "            )\n\t        # limit decomps, for example for ste\n\t        max_decompositions = entailment_values.get(\"max_decompositions\")\n\t        if max_decompositions is None:\n\t            max_decompositions = 100\n\t        for i, decomposition in enumerate(\n\t            question[\"question\"][\"decompositions\"][:max_decompositions]\n\t        ):\n\t            # format decompositions for the context\n\t            decomp = format_decompsition_string(\n", "                decomposition.replace(\"Follow up:\\n\\n\", \"Follow up: \")\n\t                .replace(\"Intermediate answer:\\n\\n\", \"Intermediate answer: \")\n\t                .replace(\"So the final answer is:\\n\\n\", \"So the final answer is: \")\n\t            )\n\t            if use_ir is not None:\n\t                decomp = format_ir_decomposition(\n\t                    decomposition.replace(\"Follow up:\\n\\n\", \"Follow up: \")\n\t                    .replace(\"Intermediate answer:\\n\\n\", \"Intermediate answer: \")\n\t                    .replace(\"So the final answer is:\\n\\n\", \"So the final answer is: \")\n\t                    .replace(\"Context:\\n\\n\", \"Context: \"),\n", "                    contexts_first=sa_contexts_first,\n\t                )\n\t            decomp_facts = []\n\t            for j, decomp_step in enumerate(decomp):\n\t                decomp_step = dataclasses.asdict(decomp_step)\n\t                # this is when the model decides to give a direct answer (no intermediate steps).\n\t                if j == 0 and decomp_step[\"gpt_3_ans\"]:\n\t                    decomp_facts.append(\n\t                        (\n\t                            question[\"question\"][\"question\"]\n", "                            + \" \"\n\t                            + decomp_step[\"gpt_3_ans\"]\n\t                        )\n\t                    )\n\t                # intermediate steps\n\t                if not decomp_step[\"gpt_3_ans\"]:\n\t                    if (\n\t                        decomp_step[\"question\"] is not None\n\t                        and decomp_step[\"answer\"] is not None\n\t                        and use_ir is None\n", "                    ):\n\t                        decomp_facts.append(\n\t                            decomp_step[\"question\"] + \" \" + decomp_step[\"answer\"]\n\t                        )\n\t                    # print(\"decomp_step: \", decomp_step)  # todo: tw - delete\n\t                    # print(\"decomposition: \", decomposition)  # todo: tw - delete\n\t                    # TW - added in case intermediate answer is None\n\t                    questn = (\n\t                        \"\"\n\t                        if decomp_step[\"question\"] is None\n", "                        else decomp_step[\"question\"]\n\t                    )\n\t                    if use_ir is not None:\n\t                        # when doing entailment on retrieved contexts, answer is comprised of [context][sep][ans]\n\t                        #   we split it using [sep] to extract just the context (retrieved snippet)\n\t                        if decomp_step[\"answer\"] is None:\n\t                            ans = \"\"\n\t                        else:\n\t                            retrieved_context, ans = decomp_step[\"answer\"].split(\n\t                                CONTEXT_ANSWER_SEP\n", "                            )\n\t                            ans = \"| \" + retrieved_context\n\t                        decomp_facts.append(questn + \" \" + ans)\n\t            decompositions.append(decomp_facts)\n\t        return decompositions\n\t    retrieved_contexts = _get_decomps(use_ir=True)\n\t    facts = _get_decomps(use_ir=None)\n\t    context_list = [d for f in retrieved_contexts for d in f]\n\t    fact_list = [d for f in facts for d in f]\n\t    # add the context to the suffix\n", "    prompt_suffix = \"\\n\"\n\t    if entailment_values.get(\"shuffle_context\"):\n\t        seed = random.randint(1, 1000)\n\t        random.seed(seed)\n\t        random.shuffle(context_list)\n\t        random.shuffle(fact_list)\n\t        question[f\"{prefix}_shuffle_seed\"] = seed\n\t    # add contexts\n\t    if entailment_values.get(\"use_ir_contexts\"):\n\t        prompt_suffix += \"\\n\\n\".join(context_list) + \"\\n\"\n", "        if entailment_values.get(\"use_qa_pairs\"):\n\t            prompt_suffix += \"\\nDerived facts:\\n\"\n\t    # add qa pairs\n\t    if entailment_values.get(\"use_qa_pairs\"):\n\t        prompt_suffix += \"\\n\".join(fact_list) + \"\\n\"\n\t    # add to prompt suffix\n\t    prompt_suffix += \"\\nQuestion:\" + \"\\n\" + question[\"question\"][\"question\"] + \"\\n\"\n\t    prompt_suffix += \"\\nAnswer:\"\n\t    question[f\"{prefix}_entailment_input\"] = question_prompt + prompt_suffix\n\t    return question\n", "def _populate_decompositions(\n\t    example: Dict,\n\t    prompt: str,\n\t    dataset_reader: DatasetReader,\n\t    gpt_accessor: GptAccessor,\n\t    evaluator: Evaluator,\n\t    decomposition_cache_dir: str,\n\t    num_decompositions: int,\n\t) -> Dict:\n\t    \"\"\" \"\"\"\n", "    example = dataset_reader.parse_example(example)\n\t    question = QuestionV1(\n\t        question=example.question,\n\t        prompt=prompt,\n\t        gpt3_accessor=gpt_accessor,\n\t        num_decompositions=num_decompositions,\n\t    )\n\t    question.populate()\n\t    gpt_answers = [\n\t        s[-1].gpt_3_ans[:-1] if s[-1].gpt_3_ans[-1] == \".\" else s[-1].gpt_3_ans\n", "        for s in question.decompsition_steps\n\t        if len(s) > 0 and s[-1].gpt_3_ans is not None and len(s[-1].gpt_3_ans) > 0\n\t    ]\n\t    question_with_answer = QuestionWithAnswer(\n\t        question=question, answers=None, gpt_answers=gpt_answers\n\t    )\n\t    question_with_answer_dict = dataclasses.asdict(question_with_answer)\n\t    results = {}\n\t    if len(question_with_answer_dict[\"gpt_answers\"]) > 0:\n\t        results[\"acc@1\"] = evaluator.evaluate(\n", "            question_with_answer_dict[\"gpt_answers\"][0], example.gold_answer\n\t        )\n\t        results[\"acc@3\"] = max(\n\t            [\n\t                evaluator.evaluate(ans, example.gold_answer)\n\t                for ans in question_with_answer_dict[\"gpt_answers\"]\n\t            ]\n\t        )\n\t        majority_prediction = Counter(\n\t            [y.lower() for y in question_with_answer_dict[\"gpt_answers\"]]\n", "        ).most_common(n=1)[0][0]\n\t        majority_prediction_at_three = Counter(\n\t            [y.lower() for y in question_with_answer_dict[\"gpt_answers\"][:3]]\n\t        ).most_common(n=1)[0][0]\n\t        results[\"acc@majority\"] = evaluator.evaluate(\n\t            majority_prediction, example.gold_answer\n\t        )\n\t        results[\"acc@majority_3\"] = evaluator.evaluate(\n\t            majority_prediction_at_three, example.gold_answer\n\t        )\n", "    else:\n\t        results[\"acc@1\"] = False\n\t        results[\"acc@3\"] = False\n\t        results[\"acc@majority\"] = False\n\t        results[\"acc@majority_3\"] = False\n\t    dataset_metadata_fields = Config().get(\"dataset.metadata_fields\")\n\t    results[\"metadata\"] = {\n\t        **{k: v for k, v in example.metadata.items() if k in dataset_metadata_fields},\n\t        **{k: v for k, v in dataclasses.asdict(example).items() if k != \"metadata\"},\n\t    }\n", "    logger.info(results)\n\t    question_with_answer_dict.update(results)\n\t    # save example\n\t    filename = get_string_hash(example.question)\n\t    with open(f\"{decomposition_cache_dir}/{filename}.json\", \"w\") as json_file:\n\t        json.dump(question_with_answer_dict, json_file)\n\t    return question_with_answer_dict\n\tdef _report_results(suffix: str, examples: List[Dict]):\n\t    \"\"\" \"\"\"\n\t    questions_with_entailment_df = pd.DataFrame(examples)\n", "    res = {\n\t        f\"{NUM_EXAMPLES_FIELD}_{suffix}\": questions_with_entailment_df.shape[0],\n\t        f\"{NUM_ABSTAINS_FIELD}_{suffix}\": questions_with_entailment_df[\n\t            f\"mte_{ACC_MTE_FIELD}\"\n\t        ]\n\t        .isna()\n\t        .sum(),\n\t        f\"{ACC_AT_1_FIELD}_{suffix}\": questions_with_entailment_df[\n\t            ACC_AT_1_FIELD\n\t        ].mean(),\n", "        f\"{ACC_AT_3_FIELD}_{suffix}\": questions_with_entailment_df[\n\t            ACC_AT_3_FIELD\n\t        ].mean(),\n\t        f\"{ACC_AT_MAJORITY_FIELD}_{suffix}\": questions_with_entailment_df[\n\t            ACC_AT_MAJORITY_FIELD\n\t        ].mean(),\n\t        f\"mte_{ACC_MTE_FIELD}_{suffix}\": questions_with_entailment_df[\n\t            f\"mte_{ACC_MTE_FIELD}\"\n\t        ].mean(),\n\t    }\n", "    for report_func in [logger.info]:\n\t        report_func(res)\n\tdef _save_examples(examples: List[Dict], output_path: str):\n\t    \"\"\" \"\"\"\n\t    # format\n\t    for ex in examples:\n\t        # flatten metadata\n\t        if \"metadata\" in ex:\n\t            for k, v in ex[\"metadata\"].items():\n\t                ex[f\"metadata_{k}\"] = v\n", "            del ex[\"metadata\"]\n\t        # flatten decompositions\n\t        num_decompositions = len(ex[\"question\"][\"decompositions\"])\n\t        for i in range(num_decompositions):\n\t            ex[f\"decomposition_{i}\"] = ex[\"question\"][\"decompositions\"][i]\n\t    # save\n\t    pd.DataFrame(examples).to_csv(output_path)\n\tdef _read_csv(file_path: str) -> List[Dict]:\n\t    \"\"\"\n\t    read decomps from a csv file\n", "    \"\"\"\n\t    logger.info(f\"Reading input data form csv file at path: {file_path}\")\n\t    df = pd.read_csv(file_path)\n\t    data = df.to_dict(\"rows\")\n\t    for x in data:\n\t        x[\"question\"] = ast.literal_eval(x[\"question\"])\n\t    num_rows = len(data)\n\t    logger.info(f\"Read {num_rows} from file\")\n\t    return data\n\tdef _run_decompositions(\n", "    examples: List[Dict],\n\t    cache_dir: str,\n\t    output_dir: str,\n\t    experiment_unique_name: str,\n\t    dataset: DatasetReader,\n\t    prompt: str,\n\t    gpt_accessor: GptAccessor,\n\t    evaluator: Evaluator,\n\t    num_decompositions: int,\n\t) -> List[Dict]:\n", "    if cache_dir is None:\n\t        decomposition_cache_dir = (\n\t            f\"{output_dir}/{experiment_unique_name}/decompositions\"\n\t        )\n\t        os.makedirs(decomposition_cache_dir, exist_ok=True)\n\t        logger.info(f\"Saving decompositions in: {decomposition_cache_dir}\")\n\t        questions_with_decompositions = [\n\t            _populate_decompositions(\n\t                example,\n\t                prompt,\n", "                dataset,\n\t                gpt_accessor,\n\t                evaluator,\n\t                decomposition_cache_dir,\n\t                num_decompositions,\n\t            )\n\t            for example in tqdm(examples)\n\t        ]\n\t    else:\n\t        logger.info(f\"Reading decompositions from: {cache_dir}\")\n", "        cached_files = [f for f in os.listdir(cache_dir)]\n\t        cached_decompositions = {}\n\t        for filename in cached_files:\n\t            with open(f\"{cache_dir}/{filename}\", \"r\") as f:\n\t                data = json.load(f)\n\t                cached_decompositions[filename] = data\n\t        questions_with_decompositions = cached_decompositions.values()\n\t    return questions_with_decompositions\n\tdef run_experiment(config_path: str):\n\t    \"\"\" \"\"\"\n", "    # read config\n\t    Config().load(config_path)\n\t    wandb.init(project=\"GRE\", config=Config()._config)\n\t    # start experiment\n\t    experiment_name = Config().get(\"experiment_name\")\n\t    logger.info(f\"Starting experiment: {experiment_name}\")\n\t    output_dir = Config().get(\"output_dir\")\n\t    # datetime\n\t    timestamp = datetime.now().timestamp()\n\t    date_time = datetime.fromtimestamp(timestamp)\n", "    str_date_time = date_time.strftime(\"%d_%m_%Y_%H_%M_%S\")\n\t    experiment_unique_name = f\"{experiment_name}_{str_date_time}\"\n\t    # read dataset\n\t    dataset_name = Config().get(\"dataset.name\")\n\t    logger.info(f\"Reading dataset: {dataset_name}.\")\n\t    dataset = DatasetReadersFactory().get_instance(dataset_name)\n\t    dataset.read()\n\t    examples = dataset.examples\n\t    # get evaluator\n\t    evaluator_name = Config().get(\"evaluator\")\n", "    logger.info(f\"Using evaluator: {evaluator_name} to report metrics\")\n\t    evaluator = EvaluatorsFactory().get_instance(evaluator_name)\n\t    # decomposition settings\n\t    gpt_accessor_name = Config().get(\"decomposition.gpt3_accessor\")\n\t    prompt_name = Config().get(\"decomposition.prompt\")\n\t    logger.info(\n\t        f\"GPT3 accessor for decompositions: {gpt_accessor_name}, with prompt: {prompt_name}\"\n\t    )\n\t    gpt_accessor = GptAccessorFactory().get_instance(gpt_accessor_name)\n\t    prompt = PromptFactoryDict[prompt_name]\n", "    num_decompositions = Config().get(\"decomposition.num_decompositions\")\n\t    # abstain settings\n\t    entailment_prompt_name = list(Config().get(\"entailment\").values())[0].get(\"prompt\")\n\t    logger.info(f\"Using entailment engine with prompt: {entailment_prompt_name}\")\n\t    entailment_prompt = PromptFactoryDict[entailment_prompt_name]\n\t    abstain_prompt_name = Config().get(\"abstain.prompt\")\n\t    logger.info(f\"Using entailment engine with prompt: {entailment_prompt_name}\")\n\t    abstain_prompt = PromptFactoryDict[abstain_prompt_name]\n\t    # filter examples in prompts\n\t    examples_not_in_prompts = [\n", "        e\n\t        for e in dataset.examples\n\t        if not (\n\t            dataset.parse_example(e).question.lower() in prompt.lower()\n\t            or dataset.parse_example(e).question.lower() in entailment_prompt.lower()\n\t        )\n\t    ]\n\t    num_examples, num_examples_not_in_prompts = len(examples), len(\n\t        examples_not_in_prompts\n\t    )\n", "    num_examples_in_prompt = num_examples - num_examples_not_in_prompts\n\t    logger.info(\n\t        f\"Removing {num_examples_in_prompt}/{num_examples} examples. Left with {num_examples_not_in_prompts} examples.\"\n\t    )\n\t    examples = examples_not_in_prompts\n\t    # sample examples\n\t    num_examples = Config().get(\"sampling.num_examples\")\n\t    if num_examples is not None:\n\t        sampling_seed = Config().get(\"sampling.seed\")\n\t        logger.info(f\"Down-sampling to {num_examples} examples.\")\n", "        random.seed(sampling_seed)\n\t        examples = random.sample(examples, num_examples)\n\t    # sample from csv\n\t    examples_csv = Config().get(\"sampling.examples_csv\")\n\t    if examples_csv is not None:\n\t        prev_examples = pd.read_csv(examples_csv).to_dict(\"rows\")\n\t        previous_qids = {x[\"metadata_qid\"] for x in prev_examples}\n\t        # create a dataset with the previous examples\n\t        prev_examples_dataset, prev_examples_dataset_qids = [], set()\n\t        for x in examples:\n", "            if x[\"qid\"] in previous_qids and x[\"qid\"] not in prev_examples_dataset_qids:\n\t                prev_examples_dataset.append(x)\n\t                prev_examples_dataset_qids.add(x[\"qid\"])\n\t        # assign the new dataset for the examples\n\t        examples = prev_examples_dataset\n\t    # retries\n\t    num_retries = Config().get(\"num_retries\")\n\t    logger.info(f\"Running with {num_retries} retries.\")\n\t    examples_with_answers: List[Dict] = []\n\t    examples_to_answer: List[Dict] = examples  # [207:]  # todo: TW - delete\n", "    # use contexts first format in Self-ask decomposition\n\t    sa_contexts_first = (\n\t        Config().get(\"decomposition.gpt3_accessor\")\n\t        == \"gpt_accessor_with_retrieval_context_first\"\n\t    )\n\t    for i in range(num_retries + 1):\n\t        # populate question with decomposition\n\t        cache_dir = Config().get(\"decomposition.cache_dir\")\n\t        csv_input_file = Config().get(\"decomposition.csv_input_file\")\n\t        questions_with_decompositions = (\n", "            _run_decompositions(\n\t                examples=examples_to_answer,\n\t                cache_dir=cache_dir if i == 0 else None,\n\t                output_dir=output_dir,\n\t                experiment_unique_name=experiment_unique_name,\n\t                dataset=dataset,\n\t                prompt=prompt,\n\t                gpt_accessor=gpt_accessor,\n\t                evaluator=evaluator,\n\t                num_decompositions=num_decompositions,\n", "            )\n\t            if csv_input_file is None\n\t            else _read_csv(csv_input_file)\n\t        )\n\t        # add input for entailment (for batching)\n\t        for entailment_prefix, entailment_values in Config().get(\"entailment\").items():\n\t            # set num iterations\n\t            iterations = entailment_values.get(\"iterations\")\n\t            if iterations is None:\n\t                iterations = 1\n", "            for k in range(iterations):\n\t                # entailment settings\n\t                entailment_prompt_name = entailment_values.get(\"prompt\")\n\t                logger.info(\n\t                    f\"Using entailment engine with prompt: {entailment_prompt_name}\"\n\t                )\n\t                entailment_prompt = PromptFactoryDict[entailment_prompt_name]\n\t                for question in questions_with_decompositions:\n\t                    _set_entailment_input(\n\t                        question,\n", "                        entailment_prompt,\n\t                        entailment_values,\n\t                        sa_contexts_first=sa_contexts_first,\n\t                        prefix=f\"{k}_{entailment_prefix}\",\n\t                    )\n\t                # call gpt with batching, adds results in place\n\t                questions_with_entailment = _call_gpt_for_entailment_with_batching(\n\t                    questions_with_decompositions,\n\t                    evaluator,\n\t                    entailment_values,\n", "                    prefix=f\"{k}_{entailment_prefix}\",\n\t                )\n\t        # questions_with_abstain = [\n\t        #     _populate_entailment(question, abstain_prompt, evaluator, 0, \"abstain\")\n\t        #     for question in tqdm(list(questions_with_decompositions))\n\t        # ]\n\t        # save results\n\t        if output_dir is not None:\n\t            output_path = f\"{output_dir}/{experiment_unique_name}_{i}.csv\"\n\t            logger.info(f\"Saving output path to: {output_path}\")\n", "            _save_examples(questions_with_entailment, output_path)\n\t        # report results\n\t        _report_results(\n\t            suffix=f\"_{experiment_name}_iteration={i}\",\n\t            examples=questions_with_decompositions,\n\t        )\n\t        # aggregate for next iteration\n\t        examples_with_answers += [\n\t            q\n\t            for q in questions_with_entailment\n", "            if\n\t            # 'unknown' not in q['abstain_multi_trace_entailment_prediction'].lower()\n\t            # and\n\t            q[f\"mte_{ACC_MTE_FIELD}\"] is not None\n\t        ]\n\t        # get abstain ids and keep only abstain answers in examples_to_answer\n\t        abstain_ids = {\n\t            q[\"metadata_qid\"]\n\t            for q in questions_with_entailment\n\t            if (\n", "                q[f\"mte_{ACC_MTE_FIELD}\"]\n\t                is None\n\t                # or 'unknown' in q['abstain_multi_trace_entailment_prediction'].lower()\n\t            )\n\t        }\n\t        examples_to_answer = [\n\t            e for e in examples if dataset.parse_example(e).qid in abstain_ids\n\t        ]\n\t        if len(examples_to_answer) == 0:\n\t            break\n", "    # aggregate after retries loop\n\t    unanswered_examples = [\n\t        q for q in questions_with_entailment if q[f\"mte_{ACC_MTE_FIELD}\"] is None\n\t    ]\n\t    all_examples = examples_with_answers + unanswered_examples\n\t    _report_results(suffix=f\"_{experiment_name}_final\", examples=all_examples)\n\t    # save results\n\t    if output_dir is not None:\n\t        output_path = f\"{output_dir}/{experiment_unique_name}_final.csv\"\n\t        logger.info(f\"Saving output path to: {output_path}\")\n", "        _save_examples(all_examples, output_path)\n\tif __name__ == \"__main__\":\n\t    \"\"\" \"\"\"\n\t    args = parse_args()\n\t    run_experiment(args.config_path)\n"]}
{"filename": "src/prompting/prompt_factory.py", "chunked_list": ["from typing import Dict\n\tfrom src.prompting.prompts_to_keep import (\n\t    wikihop_decompositions_with_retrieval_context_first,\n\t    wikihop_entailment,\n\t    strategyqa_decomposition,\n\t    strategy_mcr,\n\t)\n\tPromptFactoryDict: Dict[str, str] = {\n\t    \"2wikihop_decomposition\": wikihop_decompositions_with_retrieval_context_first,\n\t    \"2wikihop_mcr\": wikihop_entailment,\n", "    \"strategyqa_decomposition\": strategyqa_decomposition,\n\t    \"strategyqa_mcr\": strategy_mcr,\n\t}\n"]}
{"filename": "src/prompting/prompts_to_keep.py", "chunked_list": ["strategyqa_decomposition = \"\"\"Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question, you are given a context which is the top returned google snippet for the question from Wikipedia. If no follow up questions are necessary, answer the question directly.\n\t#\n\tContext1: Frost: Frost is a thin layer of ice on a solid surface, which forms from water vapor in an above-freezing atmosphere coming in contact with a solid surface whose ...\n\tContext2: Graduation: Graduation is the awarding of a diploma to a student by an educational institution. It may also refer to the ceremony that is associated with it.\n\tContext3: Winter: Winter ; Astronomical season, 22 December â€“ 21 March ; Meteorological season, 1 December â€“ 28/29 February ; Solar (Celtic) season, 1 November â€“ 31 January.\n\tQuestion: Is it common to see frost during some college commencements?\n\tAre follow up questions needed here: Yes.\n\tFollow up: What seasons can you expect to see frost?\n\tIntermediate answer: Frost is common during the winter.\n\tFollow up: When is college commencement?\n", "Intermediate answer: College commencement ceremonies often happen during the months of December, May, June.\n\tFollow up: Do any of the months December, May, June occur during the Winter?\n\tIntermediate answer: December is in the winter.\n\tSo the final answer is: Yes.\n\t#\n\tContext1: War in Vietnam (1945â€“1946): [[\\'Date\\', \\'September 13, 1945 â€“ March 30, 1946 (6 months, 2 weeks and 3 days)\\'], [\\'Location\\', \\'Southern Vietnam below the 16th parallel\\']]\n\tContext2: Llama: The gestation period of a llama is 11.5 months (350 days). Dams (female llamas) do not lick off their babies, as they have an attached tongue that does not reach outside of the mouth more than 13 millimetres (1â„2 inch). Rather, they will nuzzle and hum to their newborns.\n\tContext3: Logarithm: In mathematics, the logarithm is the inverse function to exponentiation. That means the logarithm of a number x to the base b is the exponent to ... The binary logarithm uses base 2 and is frequently used in computer ...\n\tQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\n\tAre follow up questions needed here: Yes.\n", "Follow up: How long did the vietnam war (1945-1946) last?\n\tIntermediate answer: The War in Vietnam (1945-46) lasted around 6 months.\n\tFollow up: How long is the llama gestation period?\n\tIntermediate answer: The gestation period for a llama is 11.5 months.\n\tFollow up: What is 2 times 11.5?\n\tIntermediate answer: 23, which is longer than 6.\n\tSo the final answer is: No.\n\t#\n\tQuestion: Do truck drivers generally also teach math?\n\tAre follow up questions needed here: No.\n", "So the final answer is: No.\n\t#\n\tContext1: Princeton University: She graduated from the Dwight-Englewood School in Englewood, New Jersey, in 1983. She went to Princeton University to pursue her bachelor's degree in French literature, where she graduated in 1987. So the answer is Princeton University.\n\tContext2: Princeton University: Princeton University is a private research university in Princeton, New Jersey. Founded in 1746 in Elizabeth as the College of New Jersey, Princeton is ... It is one of the highest-ranked universities in the world.\n\tContext3: University of Pennsylvania: It is the fourth-oldest institution of higher education in the United States and is ranked among the highest-regarded universities by numerous organizations and ...\n\tQuestion: Could Brooke Shields succeed at University of Pennsylvania?\n\tAre follow up questions needed here: Yes.\n\tFollow up: What college did Brooke Shields go to?\n\tIntermediate answer: Brooke Shields graduated from Princeton University.\n\tFollow up: Out of all colleges in the US, how is Princeton University ranked?\n", "Intermediate answer: Princeton is ranked as the number 1 national college by US news.\n\tFollow up: Is the ranking of University of Pennsylvania similar to Princeton University?\n\tIntermediate answer: University of Pennsylvania is also ranked at the top 10 national colleges by US news.\n\tSo the final answer is: Yes.\n\t#\n\tContext1: 23 April 1616: William Shakespeare ( bapt. 26 April 1564 â€“ 23 April 1616) was an English playwright, poet and actor. So the answer is 23 April 1616.\n\tContext2: Email: The original usage in June 1979 occurred in the journal Electronics in reference to the United States Postal Service initiative called E-COM, which was developed in the late 1970s and operated in the early 1980s.\n\tContext3: List of years: This page indexes the individual years pages. Contents. 1 3rd millennium. 1.1 21st century. 2 2nd millennium. 2.1 20th century; 2.2 19th century ...\n\tQuestion: Did William Shakespeare ever email his friends?\n\tAre follow up questions needed here: Yes.\n", "Follow up: When did William Shakespeare die?\n\tIntermediate answer: William Shakespeare died on 23 April 1616.\n\tFollow up: When was the email invented?\n\tIntermediate answer: The email was invented in 1971.\n\tFollow up: Is 1616 before 1971?\n\tIntermediate answer: Yes, 1616 is before 1971.\n\tSo the final answer is: No.\n\t#\n\tContext1: Hamster: Hamsters are rodents (order Rodentia) belonging to the subfamily Cricetinae, which contains 19 species classified in seven genera. They have become established as popular small pets. The best-known species of hamster is the golden or Syrian hamster (Mesocricetus auratus), which is the type most commonly kept as pets.\n\tContext2: Predation: Predation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding ...\n", "Question: Do hamsters provide food for any animals?\n\tAre follow up questions needed here: Yes.\n\tFollow up: What types of animals are hamsters?\n\tIntermediate answer: Hamsters are prey animals.\n\tFollow up: Do prey animals provide food for any other animals?\n\tIntermediate answer: Prey animals provide food for predators.\n\tSo the final answer is: Yes.\n\t#\n\tQuestion: Do the Druze believe in reincarnation?\n\tAre follow up questions needed here: No.\n", "So the final answer is: Yes.\n\t#\n\tContext1: about 1 gram per cubic centimetre: The density of water is about 1 gram per cubic centimetre (62 lb/cu ft): this relationship was originally used to define the gram. So the answer is about 1 gram per cubic centimetre.\n\tContext2: Gram per cubic centimetre: The density of water is about 1 g/cm3, since the gram was originally defined as the mass of one cubic centimetre of water at its maximum density at 4 Â°C.\n\tQuestion: Would a pear sink in water?\n\tAre follow up questions needed here: Yes.\n\tFollow up: What is the density of a pear?\n\tIntermediate answer: The density of a raw pear is about 0.59 g/cm^3.\n\tFollow up: What is the density of water?\n\tIntermediate answer: The density of water is about 1 g/cm^3.\n", "Follow up: Is 0.59 g/cm^3 greater than 1 g/cm^3?\n\tIntermediate answer: 0.59 g/cm^3 is less than 1 g/cm^3.\n\tSo the final answer is: No.\n\t#\n\tContext1: Last rites: The last rites, also known as the Commendation of the Dying, are the last prayers and ministrations given to an individual of Christian faith, when possible, shortly before death. They may be administered to those awaiting execution, mortally injured, or terminally ill.\n\tContext2: Richard Dawkins: Dawkins is an outspoken atheist and a supporter of various atheist, secular, and humanist organisations, including Humanists UK and the Brights movement. Dawkins suggests that atheists should be proud, not apologetic, stressing that atheism is evidence of a healthy, independent mind.\n\tContext3: Prayer in the Catholic Church: In the Catholic Church, prayer is \"the raising of one\\'s mind and heart to God or the requesting of good things from God.\" It is an act of the moral virtue ...\n\tQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\n\tAre follow up questions needed here: Yes.\n\tFollow up: What are the last Rites?\n", "Intermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death.\n\tFollow up: What are Richard Dawkins religious beliefs?\n\tIntermediate answer:  Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design.\n\tFollow up: Would an atheist participate in Catholics prayers?\n\tIntermediate answer:  It is unlikely that an atheist would participate in Catholics prayers.\n\tSo the final answer is: Yes.\n\t#\n\tContext1: number 1: Hydrogen is the chemical element with the symbol H and atomic number 1. Hydrogen is the lightest element. So the answer is number 1.\n\tContext2: Spice Girls - Simple English Wikipedia, the free encyclopedia: The group has five members. Each member uses a nickname initially given to them: Melanie Chisholm (\"Sporty Spice\"), Emma Bunton (\"Baby Spice\"), Melanie Brown (\"Scary Spice\"), Victoria Beckham (nÃ©e Adams) (\"Posh Spice\"), and Geri Halliwell (\"Ginger Spice\") .\n\tContext3: Square number: In mathematics, a square number or perfect square is an integer that is the square of an integer; in other words, it is the product of some integer with ...\n", "Question: Hydrogen's atomic number squared exceeds number of Spice Girls?\n\tAre follow up questions needed here: Yes.\n\tFollow up: What is the atomic number of hydrogen?\n\tIntermediate answer: Hydrogen has an atomic number of 1.\n\tFollow up: How many people are in the Spice Girls band?\n\tIntermediate answer: The Spice Girls has 5 members.\n\tFollow up: Is the square of 1 greater than 5?\n\tIntermediate answer: The square of 1 is 1 which is less than 5.\n\tSo the final answer is: No.\n\t#\"\"\"\n", "strategy_mcr = \"\"\"Given a question and a context, provide a Yes or No answer and explain why. If you are unsure, answer Unknown.\n\t#\n\tContext:\n\tWhat type of animal is a jellyfish? A jellyfish is a type of invertebrate.\n\tDo jellyfish have arteries? No, jellyfish do not have arteries. They have a type of nervous system called a nerve net.\n\tWhat is atherosclerosis? Atherosclerosis is a disease where plaque builds up in the arteries.\n\tDo jellyfish have arteries? Jellyfish do not have a circulatory system and therefore do not have arteries.\n\tSo jellyfish don't have atherosclerosis because they don't have arteries? Yes, that is correct.\n\tWhat is atherosclerosis? Atherosclerosis is a type of heart disease.\n\tIs an invertebrate susceptible to atherosclerosis? No, invertebrates are not susceptible to atherosclerosis.\n", "What is atherosclerosis? Atherosclerosis is a disease in which plaque builds up on the walls of the arteries.\n\tQuestion:\n\tIs a jellyfish safe from atherosclerosis?\n\tAnswer:\n\tJellyfish do not have a circulatory system and therefore do not have arteries. Atherosclerosis is a disease in which plaque builds up on the walls of the arteries.\n\tSo the answer is: Yes.\n\t#\n\tContext:\n\tIs 242 mph greater than 130 mph? Yes, 242 mph is greater than 130 mph.\n\tIs 130 mph greater than 200 mph? No, 130 mph is less than 200 mph.\n", "What is the top wind speed of Hurricane Harvey? Hurricane Harvey had top wind speeds of 130 mph.\n\tWhat is the top wind speed of a Peregrine falcon? A Peregrine falcon can fly at speeds of up to 200 mph.\n\tIs 240 mph greater than 130 mph? 240 mph is greater than 130 mph.\n\tHow fast can Hurricane Harvey winds go? Hurricane Harvey can have wind speeds up to 130 mph.\n\tHow fast can a Peregrine falcon fly? The Peregrine falcon is the fastest bird, and can fly up to 242 mph.\n\tHow fast can a Peregrine falcon fly? The Peregrine falcon can fly up to 240 mph.\n\tHow fast can Hurricane Harvey winds travel? Hurricane Harvey winds can travel up to 130 mph.\n\tQuestion:\n\tCould Hurricane Harvey catch a Peregrine falcon?\n\tAnswer:\n", "Hurricane Harvey had top wind speeds of 130 mph. A Peregrine falcon can fly at speeds of up to 200 mph. 130 mph is less than 200 mph.\n\tSo the answer is: No.\n\t#\n\tContext:\n\tWho is Donald Trump? Donald Trump is the 45th President of the United States.\n\tWhat has Donald Trump said about Muslims? Donald Trump has said many things that can be perceived as Islamophobic such as \"Islam hates us\" and \"I think Islam is a problem in the world.\n\tIs Donald Trump a Muslim? No, Donald Trump is not a Muslim.\n\tWhat is Islamophobia? Islamophobia is the fear, hatred of, or prejudice against, the Islamic religion or Muslims.\n\tWhat is the definition of Islamophobia? Islamophobia is the fear or hatred of Islam.\n\tDid Donald Trump show any fear or hatred of Islam? Yes, Donald Trump has made multiple statements that are critical of Islam.\n", "What is Islamophobia? Islamophobia is the fear or hatred of Muslims.\n\tDo Donald Trump's comments fit the definition of Islamophobia? Yes, Donald Trump's comments about Islam fit the definition of Islamophobia.\n\tQuestion:\n\tWas Donald Trump the target of Islamophobia?\n\tAnswer:\n\tIslamophobia is the fear or hatred of Islam. Donald Trump is not a Muslim.\n\tSo the answer is: No.\n\t#\n\tContext:\n\tWhen was Stone Cold Steve Austin born? Stone Cold Steve Austin was born on December 18, 1964.\n", "What is the latest year Stone Cold Steve Austin could have wrestled in? The latest year Stone Cold Steve Austin could have wrestled in is 1999.\n\tWhat is Stone Cold Steve Austin's birthdate? Stone Cold Steve Austin was born on December 18, 1964.\n\tWhat centuries do those years belong to? 1964 and 1989 belong to the 20th century, and 2003 belongs to the 21st century.\n\tWhat centuries did December 18, 1964 fall in? December 18, 1964 falls in the 20th century.\n\tDid Stone Cold Steve Austin wrestle in the 20th century? Yes.\n\tWhat years was Stone Cold Steve Austin born and active as a wrestler? Stone Cold Steve Austin was born in 1964 and was an active wrestler from 1989-2003.\n\tWhat is the earliest year Stone Cold Steve Austin could have wrestled in? The earliest year Stone Cold Steve Austin could have wrestled in is 1900.\n\tQuestion:\n\tDid Stone Cold Steve Austin wrestle in two different centuries?\n\tAnswer:\n", "Stone Cold Steve Austin was born in 1964 and was an active wrestler from 1989-2003. 1964 and 1989 belong to the 20th century, and 2003 belongs to the 21st century.\n\tSo the answer is: Yes.\n\t#\n\tContext:\n\tWhat is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility.\n\tWhat is Christopher Hitchens' religious affiliation? Christopher Hitchens was an atheist.\n\tWhat is tonsure? Tonsure is a practice of cutting or shaving the hair from the scalp.\n\tWould Christopher Hitchens be likely to do something as a sign of religious devotion? No.\n\tWould Christopher Hitchens be religious? Christopher Hitchens was an atheist.\n\tIs an atheist likely to engage in religious practices? No, an atheist is not likely to engage in religious practices.\n", "What is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility.\n\tWhat is Christopher Hitchens' religious affiliation? Christopher Hitchens was an atheist.\n\tDo atheists usually engage in tonsure? No, atheists do not usually engage in tonsure as it is a religious practice.\n\tQuestion:\n\tWould Christopher Hitchens be very unlikely to engage in tonsure?\n\tAnswer:\n\tTonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility. Christopher Hitchens was an atheist. An atheist is not likely to engage in religious practices.\n\tSo the answer is: Yes.\n\t#\n\tContext:\n", "What is the evidence that Neanderthals used arithmetic? There is no direct evidence that Neanderthals used arithmetic, but indirect evidence suggests that they may have had some understanding of basic mathematical concepts.\n\tWhat is the earliest evidence of arithmetic? The earliest evidence of arithmetic dates back to the ancient Sumerians, who lived in what is now modern-day Iraq.\n\tDid Neanderthals live before the ancient Sumerians? Yes.\n\tWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers.\n\tIs there any reason to believe that Neanderthals could not have used arithmetic? There is no reason to believe that Neanderthals could not have used arithmetic.\n\tDo we have any evidence that Neanderthals could manipulate numbers? We have no evidence that Neanderthals could manipulate numbers.\n\tWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers.\n\tWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the addition, subtraction, multiplication, and division of whole numbers and with the manipulation of fractions and decimals.\n\tQuestion:\n\tDid Neanderthals use arithmetic?\n", "Answer:\n\tThere is no evidence that Neanderthals used arithmetic. The earliest evidence of arithmetic dates back to the ancient Sumerians who lived after the Neanderthals.\n\tSo the answer is: No.\n\t#\n\tContext:\"\"\"\n\twikihop_entailment = \"\"\"Given a question and a context, answer the question and explain why. If you are unsure, answer Unknown.\n\t#\n\tContext:\n\tWho is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox.\n\tWhere was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba.\n", "Who is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox.\n\tWhere was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba.\n\tWho is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Mary.\n\tWhen and where was Mary born? Mary was born in c. 18 BC or September 8 (21), 16 BC Herodian Kingdom of Judea.\n\tQuestion:\n\tWhere was the wife of Douglas Leiterman born?\n\tAnswer:\n\tThe wife of Douglas Leiterman is Beryl Fox. Beryl Fox was born in Winnipeg, Manitoba.\n\tSo the answer is: Winnipeg, Manitoba.\n\t#\n", "Context:\n\tWho is the director of El extraÃ±o viaje? The director of El extraÃ±o viaje is Fernando FernÃ¡n GÃ³mez.\n\tWho is the director of Love in Pawn? The director of Love in Pawn is Charles Saunders.\n\tWhen was Fernando FernÃ¡n GÃ³mez born? Fernando FernÃ¡n GÃ³mez was born on 28 August 1921.\n\tWhen was Charles Saunders born? Charles Saunders was born on July 12, 1946.\n\tWho is the director of El extraÃ±o viaje? The director of El extraÃ±o viaje is Fernando FernÃ¡n GÃ³mez.\n\tWhen was Fernando FernÃ¡n GÃ³mez born? Fernando FernÃ¡n GÃ³mez was born on 28 August 1921.\n\tWho is the director of Love in Pawn? The director of Love in Pawn is Charles Saunders.\n\tWhen was Charles Saunders born? Charles Saunders was born on July 12, 1946.\n\tWho is the director of El extraÃ±o viaje? The director of El extraÃ±o viaje is Fernando FernÃ¡n GÃ³mez\n", "Who is the director of Love in Pawn? The director of Love in Pawn is Charles Saunders.\n\tWhen was Fernando FernÃ¡n GÃ³mez born? Fernando FernÃ¡n GÃ³mez was born on 28 August 1921.\n\tWhen was Charles Saunders (director) born? Charles Saunders (director) was born on 8 April 1904.\n\tQuestion:\n\tWhich film has the director who was born later, El ExtraÃ±o Viaje or Love In Pawn?\n\tAnswer:\n\tThe director of El extraÃ±o viaje is Fernando FernÃ¡n GÃ³mez. The director of Love in Pawn is Charles Saunders. Fernando FernÃ¡n GÃ³mez was born on 28 August 1921. Charles Saunders (director) was born on 8 April 1904.\n\tSo the answer is: El extraÃ±o viaje.\n\t#\n\tContext:\n", "What year did Blind Shaft come out? Blind Shaft came out in 2003.\n\tWhat year did The Mask of Fu Manchu come out? The Mask of Fu Manchu came out in 1932.\n\tWhen did Blind Shaft come out? Blind Shaft came out in 2003.\n\tWhen did The Mask of Fu Manchu come out? The Mask of Fu Manchu came out on December 2, 1932.\n\tWhen was the film Blind Shaft released? Blind Shaft was released on 4 February 2004.\n\tWhen was the film The Mask of Fu Manchu released? The Mask of Fu Manchu was released on December 2, 1932.\n\tQuestion:\n\tWhich film came out first, Blind Shaft or The Mask Of Fu Manchu?\n\tAnswer:\n\tBlind Shaft came out in 2003. The Mask of Fu Manchu came out in 1932. \n", "So the answer is: The Mask of Fu Manchu.\n\t#\n\tContext:\n\tWhen did John V, Prince Of Anhalt-Zerbst's father die? The father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\n\tWhen did Ernest I, Prince of Anhalt-Dessau die? Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\n\tWho is the father of John V, Prince of Anhalt-Zerbst? The father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\n\tWhen did Ernest I, Prince of Anhalt-Dessau die? Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\n\tWho is the father of John V, Prince of Anhalt-Zerbst? The father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\n\tWhen did Ernest I, Prince of Anhalt-Dessau die? Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\n\tQuestion:\n", "When did John V, Prince Of Anhalt-Zerbst's father die?\n\tAnswer:\n\tThe father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau. Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\n\tSo the answer is: 12 June 1516.\n\t#\n\tContext:\n\tWho is the husband of Catherine of Pomerania? The husband of Catherine of Pomerania is John II, Count Palatine of Neumarkt.\n\tWho is the father of John II, Count Palatine of Neumarkt? The father of John II, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\n\tWho are the parents of Rupert III, Elector Palatine? The parents of Rupert III, Elector Palatine are Rupert II, Elector Palatine and Beatrice of Aragon.\n\tWho is Beatrice of Aragon's father? The father of Beatrice of Aragon is King Ferdinand I of Naples.\n", "Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt's husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John I, Count Palatine of Neumarkt.\n\tWho is the father of John I, Count Palatine of Neumarkt? The father of John I, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\n\tWho is the father of Rupert III, Elector Palatine? The father of Rupert III, Elector Palatine is Rupert II, Elector Palatine.\n\tWho is Catherine Of Pomerania, Countess Palatine Of Neumarkt's husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John II, Count of Holstein-Rendsburg.\n\tWho is the father of John II, Count of Holstein-Rendsburg? The father of John II, Count of Holstein-Rendsburg is Henry II, Count of Holstein-Rendsburg.\n\tQuestion:\n\tWho is Catherine Of Pomerania, Countess Palatine Of Neumarkt's father-in-law?\n\tAnswer:\n\tThe husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John I, Count Palatine of Neumarkt. The father of John I, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\n\tSo the answer is: Rupert III, Elector Palatine.\n", "#\n\tContext:\n\tWho is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky.\n\tWho is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\n\tWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.\n\tWhen did Elio Petri die? Elio Petri died on 10 November 1982.\n\tWho is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky.\n\tWho is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\n\tWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.\n\tWhen did Elio Petri die? Elio Petri died on 10 November 1982.\n", "Who is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky.\n\tWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.\n\tWho is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\n\tWhen did Elio Petri die? Elio Petri died on 10 November 1982.\n\tQuestion:\n\tWhich film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\n\tAnswer:\n\tThe director of Crimen A Las Tres is Luis Saslavsky. The director of The Working Class Goes to Heaven is Elio Petri. Luis Saslavsky died on March 20, 1995. Elio Petri died on 10 November 1982.\n\tSo the answer is: The Working Class Goes To Heaven.\n\t#\n", "Context:\"\"\"\n\twikihop_decompositions_with_retrieval_context_first = \"\"\"Given the following question, answer it by providing follow up questions and intermediate answers. If no follow up questions are necessary, answer the question directly. You are also provided with the most relevant google snippet for each intermediate question.\n\t#\n\tContext1: Xawery Å»uÅ‚awski: Polish-Russian War (Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Å»uÅ‚awski based on the novel Polish-Russian War under the white-red flag by Dorota MasÅ‚owska. So the answer is Xawery Å»uÅ‚awski.\n\tContext2: Xawery Å»uÅ‚awski: Xawery Å»uÅ‚awski ; National Film School in ÅÃ³dÅº Â· 1995â€“present Â· Maria Strzelecka Â· 2.\n\tQuestion: Who is the mother of the director of film Polish-Russian War (Film)?\n\tAre follow up questions needed here: Yes.\n\tFollow up: Who is the director of the film Polish-Russian War (Film)?\n\tIntermediate answer: The director of the film Polish-Russian War is Xawery Å»uÅ‚awski.\n\tFollow up: Who is the mother of Xawery Å»uÅ‚awski?\n", "Intermediate answer: The mother of Xawery Å»uÅ‚awski is MaÅ‚gorzata Braunek.\n\tSo the final answer is: Rick Scott MaÅ‚gorzata Braunek.\n\t#\n\tContext1: 2003: Blind Shaft (Chinese: ç›²äº•; pinyin: MÃ¡ngjÇng) is a 2003 film about a pair of brutal con artists operating in the illegal coal mines of present-day northern China. So the answer is 2003.\n\tContext2: December 2, 1932: Release and reception. The Mask of Fu Manchu opened in New York on December 2, 1932. The film cost a total of $338,000 and had worldwide rentals of $625,000. It had a profit of $62,000. So the answer is December 2, 1932.\n\tQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\n\tAre follow up questions needed here: Yes.\n\tFollow up: When did Blind Shaft come out?\n\tIntermediate answer: Blind Shaft came out in 2003.\n\tFollow up: When did The Mask Of Fu Manchu come out?\n", "Intermediate answer: The Mask Of Fu Manchu came out in 1932.\n\tSo the final answer is: The Mask Of Fu Manchu.\n\t#\n\tContext1: John V, Prince of Anhalt-Zerbst: John was the second (but eldest surviving) son of Ernest I, Prince of Anhalt-Dessau, by his wife Margarete, daughter of Henry I, Duke of MÃ¼nsterberg-Oels, and granddaughter of George of PodÄ›brady, King of Bohemia.\n\tContext2: 12 June 1516: Ernest I, Prince of Anhalt-Dessau (died Dessau, 12 June 1516), was a German prince of the House of Ascania and ruler of the principality of Anhalt-Dessau. So the answer is 12 June 1516.\n\tQuestion: When did John V, Prince Of Anhalt-Zerbst's father die?\n\tAre follow up questions needed here: Yes.\n\tFollow up: Who is the father of John V, Prince Of Anhalt-Zerbst?\n\tIntermediate answer: The father of John V, Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\n\tFollow up: When did Ernest I, Prince of Anhalt-Dessau die?\n", "Intermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\n\tSo the final answer is: 12 June 1516\n\t#\n\tContext1: El extraÃ±o viaje: El extraÃ±o viaje (English: The Strange Voyage) is a 1964 Spanish black drama film directed by Fernando FernÃ¡n GÃ³mez.\n\tContext2: Love in Pawn: Love in Pawn is a 1953 British comedy film directed by Charles Saunders and starring Bernard Braden, Barbara Kelly and Jeannie Carson.\n\tContext3: 28 August 1921: Fernando FernÃ¡ndez GÃ³mez (28 August 1921 â€“ 21 November 2007) better known as Fernando FernÃ¡n GÃ³mez was a Spanish actor, screenwriter, film director, theater director and member of the Royal Spanish Academy for seven years. So the answer is 28 August 1921.\n\tContext4: Charles Saunders (director): Charles Joel Saunders (8 April 1904 â€“ 20 April 1997) was an English film director and screenwriter who began in the industry as a film editor, and who also contributed to television.\n\tQuestion: Which film has the director who was born later, El ExtraÃ±o Viaje or Love In Pawn?\n\tAre follow up questions needed here: Yes.\n\tFollow up: Who is the director of El ExtraÃ±o Viaje?\n", "Intermediate answer: The director of El ExtraÃ±o Viaje is Fernando FernÃ¡n GÃ³mez.\n\tFollow up: Who is the director of Love in Pawn?\n\tIntermediate answer: The director of Love in Pawn is Charles Saunders.\n\tFollow up: When was Fernando FernÃ¡n GÃ³mez born?\n\tIntermediate answer: Fernando FernÃ¡n GÃ³mez was born on 28 August 1921.\n\tFollow up: When was Charles Saunders (director) born?\n\tIntermediate answer: Charles Saunders was born on 8 April 1904.\n\tSo the final answer is: El ExtraÃ±o Viaje.\n\t#\n\tContext1: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 â€“ 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.\n", "Context2: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 â€“ 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.\n\tQuestion: Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt's father-in-law?\n\tAre follow up questions needed here: Yes.\n\tFollow up: Who is the husband of Catherine of Pomerania, Countess Palatine of Neumarkt?\n\tIntermediate answer: The husband of Catherine of Pomerania, Countess Palatine of Neumarkt is John, Count Palatine of Neumarkt.\n\tFollow up: Who is the father of John, Count Palatine of Neumarkt?\n\tIntermediate answer: The father of John, Count Palatine of Neumarkt is Rupert III of the Palatinate.\n\tSo the final answer is: Rupert III of the Palatinate.\n\t#\n\tContext1: Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. Crimen a las tres. Directed by, Luis Saslavsky.\n", "Context2: Elio Petri: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. So the answer is Elio Petri.\n\tContext3: March 20, 1995: Luis Saslavsky (April 21, 1903 â€“ March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. So the answer is March 20, 1995.\n\tContext4: Elio Petri: Final years. In 1981, Petri visited Geneva to direct Arthur Miller\\'s new play The American Clock, with Marcello Mastroianni playing the lead role. Petri died of cancer on 10 November 1982. He was 53 years old.\n\tQuestion: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\n\tAre follow up questions needed here: Yes.\n\tFollow up: Who is the director of Crimen a las tres?\n\tIntermediate answer: The director of Crimen a las tres is Luis Saslavsky.\n\tFollow up: Who is the director of The Working Class Goes to Heaven?\n\tIntermediate answer: The director of The Working Class Goes to Heaven is Elio Petri.\n\tFollow up: When did Luis Saslavsky die?\n", "Intermediate answer: Luis Saslavsky died on March 20, 1995.\n\tFollow up: When did Elio Petri die?\n\tIntermediate answer: Elio Petri died on 10 November 1982.\n\tSo the final answer is: The Working Class Goes to Heaven\n\t#\"\"\"\n"]}
{"filename": "src/gpt3_accessors/gpt_accessor_factory.py", "chunked_list": ["from typing import Dict, Type\n\tfrom src.common.abstract_factory import AbstractFactory\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_simple import GptAccessorSimple\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_with_retrieval import (\n\t    GptAccessorWithRetrieval,\n\t)\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_with_retrieval_context_first import (\n\t    GptAccessorWithRetrievalContextFirst,\n\t)\n", "class GptAccessorFactory(AbstractFactory):\n\t    \"\"\" \"\"\"\n\t    def get_instance_name_to_class_dict(self) -> Dict[str, Type[GptAccessor]]:\n\t        return {\n\t            \"gpt_accessor_simple\": GptAccessorSimple,\n\t            \"gpt_accessor_with_retrieval\": GptAccessorWithRetrieval,\n\t            \"gpt_accessor_with_retrieval_context_first\": GptAccessorWithRetrievalContextFirst,\n\t        }\n"]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_base.py", "chunked_list": ["class GptAccessor:\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return cls()\n\t    def call_gpt(\n\t        self, prompt: str, stop: str, temperature: float, *args, **kwargs\n\t    ) -> str:\n\t        raise NotImplementedError(\"Please Implement this method\")\n"]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_with_retrieval.py", "chunked_list": ["import time\n\timport openai\n\timport os\n\tfrom src.common.config import Config\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\n\tfrom src.opeanai.utils import greenify\n\tfrom src.serpapi.serpapi import (\n\t    google,\n\t    get_question_wiki_snippet,\n\t    get_question_google_snippet,\n", ")\n\topenai.api_key = os.getenv(\n\t    \"OPENAI_KEY\"\n\t)  # get one from https://openai.com , first few requests are free!\n\tclass GptAccessorWithRetrieval(GptAccessor):\n\t    def call_gpt(self, cur_prompt, stop, temperature):\n\t        res = \"\"\n\t        retries = 3\n\t        # iterate decomposition for 5 steps\n\t        for i in range(5):\n", "            # get gpt ans with retries\n\t            for i in range(retries):\n\t                try:\n\t                    ans = openai.Completion.create(\n\t                        model=\"code-davinci-002\",\n\t                        max_tokens=512,\n\t                        stop=[\"Context:\", \"#\"],\n\t                        prompt=cur_prompt,\n\t                        temperature=temperature,\n\t                    )\n", "                    break\n\t                except Exception as e:\n\t                    print(\"exception thrown, sleeping...\", e)\n\t                    time.sleep(60)\n\t                    print(\"finished sleeping\")\n\t            # add context\n\t            returned = ans[\"choices\"][0][\"text\"]\n\t            res += returned\n\t            cur_prompt += returned\n\t            if \"Follow up: \" in returned:\n", "                question = returned.split(\"Follow up: \")[-1].replace(\"\\n\", \"\")\n\t                retrieval = get_question_wiki_snippet(question, cache=True)\n\t                cur_prompt += f\"Context: {retrieval}\\n\"\n\t                res += f\"Context: {retrieval}\\n\"\n\t            if \"So the final answer is: \" in returned:\n\t                print(greenify(res), end=\"\")\n\t                return res\n\t            print(greenify(res), end=\"\")\n\t        return res\n"]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_simple.py", "chunked_list": ["import time\n\timport openai\n\timport os\n\tfrom src.common.config import Config\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\n\tfrom src.opeanai.utils import greenify\n\topenai.api_key = os.getenv(\n\t    \"OPENAI_KEY\"\n\t)  # get one from https://openai.com , first few requests are free!\n\tclass GptAccessorSimple(GptAccessor):\n", "    def call_gpt(self, cur_prompt, stop, temperature):\n\t        retries = 3\n\t        # get gpt ans with retries\n\t        for i in range(retries):\n\t            try:\n\t                ans = openai.Completion.create(\n\t                    model=\"code-davinci-002\",\n\t                    max_tokens=512,\n\t                    stop=\"#\",\n\t                    prompt=cur_prompt,\n", "                    temperature=temperature,\n\t                )\n\t                break\n\t            except Exception as e:\n\t                print(\"exception thrown, sleeping...\", e)\n\t                time.sleep(30)\n\t                print(\"finished sleeping\")\n\t        # add context\n\t        returned = ans[\"choices\"][0][\"text\"]\n\t        print(greenify(returned), end=\"\")\n", "        return returned\n"]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_with_retrieval_context_first.py", "chunked_list": ["import time\n\timport openai\n\timport os\n\tfrom src.common.config import Config\n\tfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\n\tfrom src.opeanai.utils import greenify, change_openaikey_and_sleep, gpt_simple_generator\n\tfrom src.serpapi.serpapi import (\n\t    google,\n\t    get_question_wiki_snippet,\n\t    get_question_google_snippet,\n", ")\n\timport re\n\topenai.api_key = os.getenv(\n\t    \"OPENAI_KEY\"\n\t)  # get one from https://openai.com , first few requests are free!\n\tFOLLOW_UP_REGEX_PATTERN = r\"Follow up:.*\\n\"\n\tclass GptAccessorWithRetrievalContextFirst(GptAccessor):\n\t    def call_gpt(\n\t        self, orig_prompt, stop, temperature, orig_question, model, decomposition_index\n\t    ):\n", "        print(\n\t            f\"\\n====================== Decomposition {decomposition_index} ======================\"\n\t        )\n\t        contexts = (\n\t            [get_question_wiki_snippet(orig_question, cache=True)]\n\t            if Config().get(\"decomposition.retrieve_orig_question\")\n\t            else []\n\t        )\n\t        stop_condition = \"Intermediate answer:\"\n\t        res = \"\"\n", "        retries = 6\n\t        # iterate decomposition for 5 steps\n\t        for i in range(5):\n\t            # format input, the input should be:\n\t            # contexts, followed by intermediate QAs\n\t            context_formatted = \"\\n\".join(\n\t                [f\"Context{k+1}: {context}\" for k, context in enumerate(contexts)]\n\t            )\n\t            context_formatted = (\n\t                \"\\n\" + context_formatted\n", "                if len(context_formatted)\n\t                else context_formatted\n\t            )\n\t            cur_prompt = (\n\t                orig_prompt\n\t                + context_formatted\n\t                + \"\\nQuestion: \"\n\t                + orig_question\n\t                + \"\\nAre follow up questions needed here: Yes.\"\n\t                + res\n", "            )\n\t            # get gpt ans with retries\n\t            for i in range(retries):\n\t                try:\n\t                    ans = openai.Completion.create(\n\t                        model=model,\n\t                        max_tokens=512,\n\t                        stop=[\"Context:\", \"#\"],\n\t                        prompt=cur_prompt,\n\t                        temperature=temperature,\n", "                    )\n\t                    break\n\t                except Exception as e:\n\t                    print(f\"exception thrown, sleeping...\")\n\t                    print(e)\n\t                    change_openaikey_and_sleep()\n\t            # add context\n\t            returned = ans[\"choices\"][0][\"text\"]\n\t            # check the stop condition, it's different if we generate the first follow up or if we are in an intermediate step\n\t            # if this is the first follow up, break on the first intermediate question\n", "            if stop_condition == \"Intermediate answer:\":\n\t                # add the follow-up question to the prompt and change the stop condition\n\t                res += returned.split(\"Intermediate answer:\")[0]\n\t                stop_condition = \"Follow up:\"\n\t            # else, we are in an intermediate step\n\t            elif stop_condition == \"Follow up:\":\n\t                # continue until we find the next follow up question\n\t                # since we want to add retrieved contexts given this question\n\t                followup_split = re.split(FOLLOW_UP_REGEX_PATTERN, returned)\n\t                # add everything until the next follow up\n", "                res += followup_split[0]\n\t                # add the next follow up\n\t                if len(followup_split) > 1:\n\t                    res += re.findall(FOLLOW_UP_REGEX_PATTERN, returned)[0]\n\t            # make sure the result does not end in a new line\n\t            if res[-1] == \"\\n\":\n\t                res = res[:-1]\n\t            # if we are in an intermediate step, add the retrieved context to the list of contexts\n\t            if \"Follow up: \" in returned:\n\t                # get the first follow up\n", "                question = [l for l in returned.split(\"\\n\") if \"Follow up: \" in l][\n\t                    0\n\t                ].split(\"Follow up: \")[-1]\n\t                retrieval = get_question_wiki_snippet(question, cache=True)\n\t                contexts.append(retrieval)\n\t            # end when the final answer is generated, this means that no follow up questions were asked\n\t            elif \"So the final answer is: \" in returned:\n\t                final_res = (\n\t                    context_formatted\n\t                    + \"\\nQuestion: \"\n", "                    + orig_question\n\t                    + \"\\nAre follow up questions needed here: Yes.\"\n\t                    + res\n\t                )\n\t                print(f\"\\nDecomposition {decomposition_index} result:\")\n\t                print(greenify(final_res), end=\"\")\n\t                # return the contexts followed by the original question, intermediate QAs and final answer\n\t                return final_res\n\t            print(greenify(res), end=\"\")\n\t            print(\"\\n\")\n", "        return res\n"]}
