{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tdef my_test_suite():\n\t    import unittest\n\t    test_loader = unittest.TestLoader()\n\t    test_suite = test_loader.discover('tests', pattern='test_*.py')\n\t    return test_suite\n\tsetup(\n\t    name=\"chatglm-q\",\n\t    version=\"0.0.2-alpha0\",\n\t    author=\"K024\",\n", "    description=\"Another ChatGLM implementation for optimized quantization\",\n\t    url=\"https://github.com/K024/chatglm-q\", \n\t    packages=find_packages(),\n\t    test_suite=\"setup.my_test_suite\",\n\t    install_requires=[\n\t        \"tqdm\",\n\t        \"safetensors\",\n\t        \"sentencepiece\",\n\t        \"huggingface_hub\",\n\t    ],\n", ")\n"]}
{"filename": "chatglm_q/decoder.py", "chunked_list": ["import re\n\timport time\n\timport torch\n\tfrom pathlib import Path\n\tfrom typing import Union\n\tfrom huggingface_hub import snapshot_download\n\tfrom .model import ChatGLM2Model\n\tfrom .tokenizer import ChatGLM2Tokenizer\n\tfrom .loader import ChatGLMLoadConfig, load_model_and_tokenizer, save_model_and_tokenizer\n\tdef top_p_sampling(logits: torch.Tensor, top_k=100, top_p=0.8, temperature=1.0):\n", "    # top_k\n\t    probs = torch.softmax(logits.float() / temperature, dim=-1)\n\t    probs, indices = torch.sort(probs, dim=-1, descending=True)\n\t    probs = probs[..., :top_k]\n\t    indices = indices[..., :top_k]\n\t    # top_p\n\t    cumsum = torch.cumsum(probs, dim=-1)\n\t    probs[(cumsum - probs) > top_p] = 0.0\n\t    probs = probs / torch.sum(probs, dim=-1, keepdim=True)\n\t    # sample\n", "    next_token = torch.multinomial(probs, num_samples=1)\n\t    output = torch.gather(indices, dim=-1, index=next_token)\n\t    return output[..., 0]\n\tclass ChatGLMDecoder():\n\t    def __init__(\n\t        self,\n\t        config: ChatGLMLoadConfig,\n\t        model: ChatGLM2Model,\n\t        tokenizer: ChatGLM2Tokenizer,\n\t        eos_token = \"</s>\",\n", "        device = None,\n\t        max_sequence_length: int = None,\n\t        time_log = False,\n\t    ):\n\t        self.config = config\n\t        self.model = model\n\t        self.tokenizer = tokenizer\n\t        self.device = device\n\t        self.eos_token_id = tokenizer[eos_token]\n\t        self.max_sequence_length = max_sequence_length or config.model_config.max_sequence_length\n", "        self.time_log = time_log\n\t    @staticmethod\n\t    def from_pretrained(path_or_repo_id: Union[Path, str], device=None, torch_dtype=None, cache_dir=None, token=None):\n\t        path = Path(path_or_repo_id)\n\t        if not path.exists() or not path.is_dir():\n\t            assert isinstance(path_or_repo_id, str)\n\t            path = snapshot_download(path_or_repo_id, cache_dir=cache_dir, token=token)\n\t        config, model, tokenizer = load_model_and_tokenizer(path, torch_dtype)\n\t        model.to(device=device)\n\t        return ChatGLMDecoder(config, model, tokenizer, device=device)\n", "    def save_pretrained(self, path: Union[Path, str], shard=True):\n\t        save_model_and_tokenizer(path, self.config, self.model, self.tokenizer, shard=shard)\n\t    def generate(self, prefix_text: str, max_generated_tokens=400, top_k=100, top_p=0.8, temperature=1.0):\n\t        model, tokenizer = self.model, self.tokenizer\n\t        eos_token_id = self.eos_token_id\n\t        prefix_ids = tokenizer.encode(prefix_text)\n\t        input_ids = torch.LongTensor([prefix_ids])\n\t        past_key_values = None\n\t        generated_tokens = []\n\t        generate_time = []\n", "        while len(generated_tokens) < max_generated_tokens \\\n\t            and len(generated_tokens) + len(prefix_ids) < self.max_sequence_length:\n\t            with torch.no_grad():\n\t                start_time = time.perf_counter()\n\t                _, logits, past_key_values = model(\n\t                    input_ids=input_ids.to(self.device),\n\t                    past_key_values=past_key_values,\n\t                )\n\t                next_token = top_p_sampling(logits[0, -1], top_k, top_p, temperature).item()\n\t                end_time = time.perf_counter()\n", "                generate_time.append(end_time - start_time)\n\t            generated_tokens += [next_token]\n\t            if next_token == eos_token_id:\n\t                break\n\t            response_text = process_response(tokenizer.decode(generated_tokens))\n\t            if response_text and response_text[-1] != \"�\":\n\t                yield response_text\n\t            input_ids = torch.tensor([[next_token]]).long()\n\t        if self.time_log:\n\t            init_time, *rest_time = generate_time\n", "            print(f\"Decoder:\")\n\t            print(f\"  len: {len(prefix_ids)}(prefix) + {len(generated_tokens)}(gen)\")\n\t            print(f\" init: {init_time:.6f} s\")\n\t            print(f\"  sum: {sum(generate_time):.6f} s\")\n\t            print(f\"  gen: {len(rest_time) / sum(rest_time):.6f} tok/s\")\n\t            print(f\"  avg: {len(generate_time) / sum(generate_time):.6f} tok/s\")\n\t        return process_response(tokenizer.decode(generated_tokens))\n\tdef chat_template(history: list[tuple[str, str]], current: str):\n\t    prompt = \"\"\n\t    chat_round = 1\n", "    for question, answer in history:\n\t        prompt += f\"[Round {chat_round}]\\n\\n问：{question}\\n\\n答：{answer}\\n\\n\"\n\t        chat_round += 1\n\t    prompt += f\"[Round {chat_round}]\\n\\n问：{current}\\n\\n答：\"\n\t    return prompt\n\tdef process_response(response: str):\n\t    response = response.strip()\n\t    response = response.replace(\"[[训练时间]]\", \"2023年\")\n\t    punkts = [\n\t        [\",\", \"，\"],\n", "        [\"!\", \"！\"],\n\t        [\":\", \"：\"],\n\t        [\";\", \"；\"],\n\t        [\"\\?\", \"？\"],\n\t    ]\n\t    for item in punkts:\n\t        response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n\t        response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n\t    return response\n"]}
{"filename": "chatglm_q/model.py", "chunked_list": ["import math\n\timport torch\n\tfrom typing import Optional\n\tfrom torch import nn, Tensor\n\timport torch.nn.functional as F\n\tfrom dataclasses import dataclass\n\t@dataclass\n\tclass ChatGLM2Config():\n\t    hidden_size: int = 4096\n\t    inner_hidden_size: int = 13696\n", "    head_hidden_size: int = 128\n\t    num_multi_query_groups: int = 2\n\t    num_attention_heads: int = 32\n\t    num_layers: int = 28\n\t    vocab_size: int = 65024\n\t    dropout_rate: float = 0.0\n\t    layernorm_epsilon: float = 1e-05\n\t    max_sequence_length: int = 8192\n\t# not used\n\tdef precompute_sinusoids(dim: int, length: int, scale = 10000.0):\n", "    assert dim % 2 == 0\n\t    log_timescale_increment = torch.log(scale) / (dim // 2 - 1)\n\t    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(dim // 2))\n\t    scaled_time = torch.outer(torch.arange(length).float(), inv_timescales)\n\t    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n\t# changed from v1: [r, r, ..., i, i, ...] => [[r, i], [r, i], ...]\n\tdef precompute_freqs_cis(dim: int, length: int, theta = 10000.0):\n\t    assert dim % 4 == 0\n\t    # half of the head_dim bypassed\n\t    dim = dim // 2\n", "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n\t    freqs = torch.outer(torch.arange(length).float(), freqs)\n\t    freqs_cis = torch.stack([torch.cos(freqs), torch.sin(freqs)], dim=-1)\n\t    freqs_bypass = torch.stack([torch.ones_like(freqs), torch.zeros_like(freqs)], dim=-1)\n\t    return torch.cat([freqs_cis, freqs_bypass], dim=-2)\n\t# changed from v1\n\tROTARY_VIEW_AS_COMPLEX = True\n\tdef apply_rotary_emb(\n\t    x: Tensor,          # (n_batch, n_seq, n_groups, n_head, d_head // 2, 2)\n\t    freqs_cis: Tensor,  # (n_batch, n_seq, 1, 1, d_head // 2, 2)\n", ") -> Tensor:\n\t    if ROTARY_VIEW_AS_COMPLEX and x.dtype in [torch.float32, torch.float16]:\n\t        x = torch.view_as_complex(x)\n\t        freqs_cis = torch.view_as_complex(freqs_cis)\n\t        return torch.view_as_real(x * freqs_cis).flatten(-2)\n\t    else:\n\t        o_r = x[..., 0] * freqs_cis[..., 0] - x[..., 1] * freqs_cis[..., 1]\n\t        o_i = x[..., 0] * freqs_cis[..., 1] + x[..., 1] * freqs_cis[..., 0]\n\t        return torch.stack([o_r, o_i], dim=-1).flatten(-2)\n\tclass RMSNorm(nn.Module):\n", "    def __init__(self, normalized_shape: tuple[int, ...], eps=1e-5, device=None, dtype=None) -> None:\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(normalized_shape, device=device, dtype=dtype))\n\t        self.eps = eps\n\t    def _norm(self, x: Tensor):\n\t        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\t    def forward(self, x: Tensor):\n\t        output = self._norm(x.float()).type_as(x)\n\t        return output * self.weight\n\tclass Linear(nn.Linear):\n", "    def forward(self, x: Tensor) -> Tensor:\n\t        return F.linear(x, self.weight.type_as(x),\n\t                        None if self.bias is None else self.bias.type_as(x))\n\t    def reset_parameters(self):\n\t        pass\n\tclass Embedding(nn.Embedding):\n\t    def reset_parameters(self):\n\t        pass\n\tclass ChatGLM2Attention(nn.Module):\n\t    def __init__(\n", "        self,\n\t        n_state: int,\n\t        n_head: int,\n\t        d_head: int,\n\t        n_groups: int,\n\t        layer_idx: int,\n\t        dropout_rate = 0.0,\n\t        qkv_bias = True,\n\t        o_bias = False,\n\t        dtype = None,\n", "    ):\n\t        super().__init__()\n\t        self.n_head = n_head\n\t        self.d_head = d_head\n\t        self.n_groups = n_groups\n\t        assert n_state % (n_head * 4) == 0\n\t        assert n_head % n_groups == 0\n\t        self.layer_idx = layer_idx\n\t        # multi-query attention\n\t        self.qkv_proj = Linear(n_state, d_head * (n_head + 2 * n_groups), bias=qkv_bias, dtype=dtype)\n", "        self.o_proj = Linear(d_head * n_head, n_state, bias=o_bias, dtype=dtype)\n\t        self.dropout = nn.Dropout(dropout_rate)\n\t    def forward(\n\t        self,\n\t        x: Tensor,\n\t        freqs_cis: Tensor,\n\t        attention_mask: Optional[Tensor] = None,\n\t        kv_cache: Optional[tuple[Tensor, ...]] = None,\n\t    ):\n\t        '''\n", "        x:\n\t            Shape: (n_batch, n_seq or n_seq_new (using cache), n_state)\n\t        freqs_cis:\n\t            Shape: (n_batch, n_seq or n_seq_new, 1, 1, d_head // 2, 2)\n\t        attention_mask:\n\t            0 for no mask, -inf for masked\n\t            Shape: (n_batch, n_seq_new, n_seq)\n\t        kv_cache:\n\t            Tuple of (k_cache, v_cache)\n\t        '''\n", "        n_batch, n_seq, _ = x.shape\n\t        d_head, n_head, n_groups = self.d_head, self.n_head, self.n_groups\n\t        fused_qkv = self.qkv_proj(x)\n\t        split_size = [d_head * n_head, d_head * n_groups, d_head * n_groups]\n\t        q, k, v = torch.split(fused_qkv, split_size, dim=-1)\n\t        # allow broadcast along groups\n\t        q = q.view(n_batch, n_seq, n_groups, n_head // n_groups, d_head // 2, 2)\n\t        k = k.view(n_batch, n_seq, n_groups, 1, d_head // 2, 2)\n\t        v = v.view(n_batch, n_seq, n_groups, 1, d_head)\n\t        q = apply_rotary_emb(q, freqs_cis)\n", "        k = apply_rotary_emb(k, freqs_cis)\n\t        if kv_cache is not None:\n\t            k_cache, v_cache = kv_cache\n\t            k = torch.cat([k_cache, k], dim=1)\n\t            v = torch.cat([v_cache, v], dim=1)\n\t        kv_cache = (k.detach(), v.detach())\n\t        q = q.permute(0, 2, 3, 1, 4)\n\t        k = k.permute(0, 2, 3, 4, 1)\n\t        v = v.permute(0, 2, 3, 1, 4)\n\t        # maybe useless, test needed\n", "        # scaling_coeff = float(self.layer_idx + 1)\n\t        q = q / (math.sqrt(d_head)) #  * scaling_coeff)\n\t        # (n_batch, n_group, n_heads, n_seq, n_seq_past)\n\t        qk = torch.matmul(q, k) # / math.sqrt(d_head) # no need to scale again\n\t        if attention_mask is not None:\n\t            qk = qk + attention_mask[:, None, None, :, :]\n\t        scores = F.softmax(qk.float(), dim=-1).type_as(x) # qk / scaling_coeff\n\t        scores = self.dropout(scores)\n\t        output = torch.matmul(scores, v)\n\t        output = output.permute(0, 3, 1, 2, 4).reshape(n_batch, n_seq, -1)\n", "        output = self.o_proj(output)\n\t        return output, kv_cache\n\tclass GatedFeedForward(nn.Module):\n\t    def __init__(\n\t        self,\n\t        dim: int,\n\t        hidden_dim: Optional[int] = None,\n\t        dropout_rate = 0.0,\n\t        bias = False,\n\t        dtype = None,\n", "        act_fn = F.silu,\n\t    ):\n\t        super().__init__()\n\t        hidden_dim = hidden_dim or dim * 4\n\t        self.hidden_dim = hidden_dim\n\t        # fused gate act\n\t        self.w_in = Linear(dim, hidden_dim * 2, bias=bias, dtype=dtype)\n\t        self.w_out = Linear(hidden_dim, dim, bias=bias, dtype=dtype)\n\t        self.dropout = nn.Dropout(dropout_rate)\n\t        self.act_fn = act_fn\n", "    def forward(self, x: Tensor):\n\t        h, gate = torch.split(self.w_in(x), self.hidden_dim, dim=-1)\n\t        return self.w_out(self.dropout(self.act_fn(h) * gate))\n\tclass ChatGLM2Block(nn.Module):\n\t    def __init__(self, layer_idx: int, config: ChatGLM2Config, dtype=None):\n\t        super().__init__()\n\t        self.layer_idx = layer_idx\n\t        self.attn_ln = RMSNorm(\n\t            config.hidden_size,\n\t            eps=config.layernorm_epsilon,\n", "            dtype=dtype)\n\t        self.attn = ChatGLM2Attention(\n\t            config.hidden_size,\n\t            config.num_attention_heads,\n\t            config.head_hidden_size,\n\t            config.num_multi_query_groups,\n\t            layer_idx,\n\t            dropout_rate=config.dropout_rate,\n\t            dtype=dtype)\n\t        self.ffn_ln = RMSNorm(\n", "            config.hidden_size,\n\t            eps=config.layernorm_epsilon,\n\t            dtype=dtype)\n\t        self.ffn = GatedFeedForward(\n\t            config.hidden_size,\n\t            config.inner_hidden_size,\n\t            config.dropout_rate,\n\t            dtype=dtype)\n\t    def forward(\n\t        self,\n", "        x: Tensor,\n\t        freqs_cis: Tensor,\n\t        attention_mask: Optional[Tensor] = None,\n\t        kv_cache: Optional[tuple[Tensor, ...]] = None,\n\t    ):\n\t        h, kv_cache = self.attn(\n\t            x=self.attn_ln(x),\n\t            freqs_cis=freqs_cis,\n\t            attention_mask=attention_mask,\n\t            kv_cache=kv_cache,\n", "        )\n\t        x = x + h\n\t        h = self.ffn(self.ffn_ln(x))\n\t        output = x + h\n\t        return output, kv_cache\n\tclass ChatGLM2Model(nn.Module):\n\t    def __init__(self, config: ChatGLM2Config, dtype=None):\n\t        super().__init__()\n\t        self.config = config\n\t        self.word_embedding = Embedding(\n", "            num_embeddings=config.vocab_size, embedding_dim=config.hidden_size, dtype=dtype\n\t        )\n\t        self.dropout = nn.Dropout(config.dropout_rate)\n\t        self.layers = nn.ModuleList([\n\t            ChatGLM2Block(layer_idx, config, dtype=dtype) for layer_idx in range(config.num_layers)\n\t        ])\n\t        self.final_ln = RMSNorm(\n\t            config.hidden_size, eps=config.layernorm_epsilon, dtype=dtype)\n\t        self.lm_head = Linear(\n\t            config.hidden_size, config.vocab_size, bias=False, dtype=dtype)\n", "        # half of head_dim bypassed\n\t        d_freqs_cis = config.head_hidden_size\n\t        self.d_freqs_cis = d_freqs_cis\n\t        freqs_cis_cache = precompute_freqs_cis(d_freqs_cis, config.max_sequence_length) \\\n\t            .view(config.max_sequence_length, -1).to(dtype=dtype)\n\t        self.register_buffer(\"freqs_cis_cache\", freqs_cis_cache, persistent=False)\n\t    def prepare_input(\n\t        self,\n\t        input_ids: Optional[torch.LongTensor] = None,\n\t        input_embeddings: Optional[torch.FloatTensor] = None,\n", "        attention_mask: Optional[torch.LongTensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_values: Optional[tuple[tuple[Tensor, ...], ...]] = None,\n\t    ) -> tuple[Tensor, Tensor, Tensor]:\n\t        \"\"\"\n\t        returns: (\n\t            input_embeddings,\n\t            attention_mask,\n\t            freqs_cis,\n\t        )\n", "        \"\"\"\n\t        if input_embeddings is None:\n\t            assert input_ids is not None, \"No input\"\n\t            device = input_ids.device\n\t            input_embeddings = self.word_embedding(input_ids)\n\t            n_batch, n_seq_new = input_ids.shape\n\t        else:\n\t            assert input_ids is None, \"Specify either 'input_ids' or 'input_embeddings'\"\n\t            device = input_embeddings.device\n\t            n_batch, n_seq_new, _ = input_embeddings.shape\n", "        if past_key_values is not None:\n\t            n_seq_past = past_key_values[0][0].shape[1]\n\t            n_seq = n_seq_new + n_seq_past\n\t        else:\n\t            n_seq = n_seq_new\n\t        if attention_mask is None:\n\t            attention_mask = torch.ones(n_batch, n_seq, dtype=torch.long, device=device)\n\t        if position_ids is None:\n\t            position_ids = torch.cumsum(attention_mask, dim=1)\n\t        # causal mask with full prefix attention\n", "        # trilu is not supported in onnxruntime\n\t        seq = torch.arange(n_seq, device=device)\n\t        causal_mask = (seq[:, None] < seq[None, :])\n\t        # make attention_mask to a float causal mask\n\t        attention_mask = (causal_mask[None, ...] | ~attention_mask[:, None, :].bool()).float() * -1e10\n\t        # align to input_ids\n\t        attention_mask = attention_mask[:, -n_seq_new:]\n\t        position_ids = position_ids[:, -n_seq_new:]\n\t        freqs_cis = F.embedding(position_ids, self.freqs_cis_cache) \\\n\t            .view(n_batch, n_seq_new, 1, 1, self.d_freqs_cis // 2, 2)\n", "        return (\n\t            input_embeddings,\n\t            attention_mask,\n\t            freqs_cis,\n\t        )\n\t    def forward(\n\t        self,\n\t        input_ids: Optional[torch.LongTensor] = None,\n\t        input_embeddings: Optional[torch.FloatTensor] = None,\n\t        attention_mask: Optional[torch.LongTensor] = None,\n", "        position_ids: Optional[torch.LongTensor] = None,\n\t        labels: Optional[torch.LongTensor] = None,\n\t        past_key_values: Optional[tuple[tuple[Tensor, ...], ...]] = None,\n\t    ):\n\t        '''\n\t        input_ids:\n\t            Shape: (n_batch, n_seq or n_new)\n\t        attention_mask:\n\t            Shape: (n_batch, n_seq) with 1 for token and 0 for pad\n\t        position_ids:\n", "            Shape: (n_batch, n_seq or n_new) same as input_ids\n\t        labels:\n\t            Same as input_ids (no shift required) with -100 for prefix and pad tokens\n\t        past_key_values:\n\t            Tuple[Tuple[Tensor, Tensor], ...] where each:\n\t            Shape: (n_batch, n_past, num_multi_query_groups, 1, head_hidden_size)\n\t                    n_seq = n_past + n_new\n\t        '''\n\t        (\n\t            input_embeddings,\n", "            attention_mask,\n\t            freqs_cis,\n\t        ) = self.prepare_input(\n\t            input_ids,\n\t            input_embeddings,\n\t            attention_mask,\n\t            position_ids,\n\t            past_key_values,\n\t        )\n\t        # forward layers\n", "        h = self.dropout(input_embeddings)\n\t        current_key_values = tuple()\n\t        for i, layer in enumerate(self.layers):\n\t            kv_cache = past_key_values[i] if past_key_values is not None else None\n\t            h, kv_cache = layer(\n\t                h,\n\t                attention_mask=attention_mask,\n\t                freqs_cis=freqs_cis,\n\t                kv_cache=kv_cache,\n\t            )\n", "            current_key_values += (kv_cache, )\n\t        h = self.final_ln(h)\n\t        output: Tensor = self.lm_head(h)\n\t        if labels is not None:\n\t            n_classes = self.config.vocab_size\n\t            shift_logits = output[..., :-1, :].contiguous().float()\n\t            shift_labels = labels[..., 1:].contiguous()\n\t            loss = F.cross_entropy(shift_logits.view(-1, n_classes), shift_labels.view(-1))\n\t        else:\n\t            loss = None\n", "        return loss, output, current_key_values\n"]}
{"filename": "chatglm_q/loader.py", "chunked_list": ["import json\n\timport shutil\n\tfrom pathlib import Path\n\tfrom collections import OrderedDict\n\tfrom dataclasses import dataclass, asdict, field\n\tfrom typing import Literal, Union\n\tfrom pathlib import Path\n\timport torch\n\tfrom tqdm.auto import tqdm\n\tfrom safetensors.torch import save_file, safe_open\n", "from .model import ChatGLM2Model, ChatGLM2Config\n\tfrom .tokenizer import ChatGLM2Tokenizer\n\t@dataclass\n\tclass ChatGLMLoadConfig():\n\t    model_type: Literal[\"ChatGLM2Model\"] = \"ChatGLM2Model\"\n\t    model_config: ChatGLM2Config = field(default_factory=ChatGLM2Config)\n\t    quant_type: Literal[\"none\", \"int8\", \"int4g32\"] = \"none\"\n\t    weight_files: list[str] = field(default_factory=list)\n\t    tokenizer_file: str = \"sentencepiece.model\"\n\t    torch_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = \"float32\"\n", "    def __post_init__(self):\n\t        assert self.model_type == \"ChatGLM2Model\", \"Only 'ChatGLM2Model' is supported\"\n\t        if not isinstance(self.model_config, ChatGLM2Config):\n\t            self.model_config = ChatGLM2Config(**self.model_config)\n\t    def get_torch_dtype(self):\n\t        return getattr(torch, self.torch_dtype)\n\t    @staticmethod\n\t    def from_json(json_str):\n\t        return ChatGLMLoadConfig(**json.loads(json_str))\n\t    def to_json(self):\n", "        return json.dumps(asdict(self), ensure_ascii=False, indent=2)\n\tdef create_quant_int8_model(config = ChatGLM2Config(), dtype=None):\n\t    try:\n\t        from . import model as modeling\n\t        from .int8.qlinear import DynamicQuantizeLinear, QEmbedding\n\t        prev_linear, prev_embedding = modeling.Linear, modeling.Embedding\n\t        modeling.Linear, modeling.Embedding = DynamicQuantizeLinear, QEmbedding\n\t        return ChatGLM2Model(config, dtype)\n\t    finally:\n\t        modeling.Linear, modeling.Embedding = prev_linear, prev_embedding\n", "def create_quant_int4_model(config=ChatGLM2Config(), group_size=32, dtype=None):\n\t    try:\n\t        from . import model as modeling\n\t        from .int4 import qlinear\n\t        from .int4.qlinear import DynamicQuantizeLinear, QEmbedding\n\t        prev_group_size = qlinear.DEFAULT_GROUP_SIZE\n\t        prev_linear, prev_embedding = modeling.Linear, modeling.Embedding\n\t        qlinear.DEFAULT_GROUP_SIZE = group_size\n\t        modeling.Linear, modeling.Embedding = DynamicQuantizeLinear, QEmbedding\n\t        return ChatGLM2Model(config, dtype)\n", "    finally:\n\t        qlinear.DEFAULT_GROUP_SIZE = prev_group_size\n\t        modeling.Linear, modeling.Embedding = prev_linear, prev_embedding\n\t@torch.no_grad()\n\tdef load_model_and_tokenizer(\n\t    model_path: Union[str, Path], torch_dtype=None, load_model=True, load_tokenizer=True,\n\t) -> tuple[ChatGLM2Config, ChatGLM2Model, ChatGLM2Tokenizer]:\n\t    model_path = Path(model_path)\n\t    config_path = model_path / \"config.json\"\n\t    config = ChatGLMLoadConfig.from_json(config_path.read_bytes())\n", "    torch_dtype = torch_dtype or config.get_torch_dtype()\n\t    model = None\n\t    if load_model:\n\t        if config.quant_type == \"none\":\n\t            model = ChatGLM2Model(config.model_config, torch_dtype)\n\t        elif config.quant_type == \"int8\":\n\t            model = create_quant_int8_model(config.model_config, torch_dtype)\n\t        elif config.quant_type == \"int4g32\":\n\t            model = create_quant_int4_model(config.model_config, 32, torch_dtype)\n\t        else:\n", "            raise NotImplementedError(f\"No quant_type named '{config.quant_type}'\")\n\t        state_dict = dict(**model.state_dict())\n\t        files = config.weight_files if len(config.weight_files) == 1 else tqdm(config.weight_files)\n\t        for file in files:\n\t            with safe_open(model_path / file, framework=\"pt\") as f:\n\t                for k in f.keys():\n\t                    try:\n\t                        if k not in state_dict:\n\t                            print(f'\"{k}\" is ignored')\n\t                            continue\n", "                        v = f.get_tensor(k)\n\t                        if state_dict[k].is_floating_point():\n\t                            v = v.type_as(state_dict[k])\n\t                        state_dict[k].copy_(v.to(state_dict[k].device))\n\t                        state_dict.pop(k)\n\t                    except:\n\t                        print(f\"error handling weight '{k}'\")\n\t                        raise\n\t        if len(state_dict):\n\t            print(f'model weights \"{\", \".join(state_dict.keys())}\" are not initialized')\n", "    tokenizer = None\n\t    if load_tokenizer:\n\t        tokenizer = ChatGLM2Tokenizer(model_path / config.tokenizer_file)\n\t    return config, model, tokenizer\n\tdef save_model_and_tokenizer(\n\t    path: Union[str, Path],\n\t    config: ChatGLMLoadConfig,\n\t    model: ChatGLM2Model,\n\t    tokenizer: ChatGLM2Tokenizer,\n\t    shard=True,\n", "    max_shard_bytes=2 * 1024 ** 3\n\t):\n\t    path = Path(path)\n\t    if not path.exists():\n\t        path.mkdir(parents=True)\n\t    else:\n\t        assert path.is_dir()\n\t    tokenizer_path = path / config.tokenizer_file\n\t    shutil.copy(tokenizer.vocab_file, tokenizer_path)\n\t    if not shard:\n", "        config.weight_files = [\"model_weights.safetensors\"]\n\t        save_file(model.state_dict(), path / config.weight_files[0])\n\t    else:\n\t        weight_mapping = {}\n\t        current_index = 0\n\t        current_size = 0\n\t        state_dict = model.state_dict()\n\t        for name, weight in state_dict.items():\n\t            size = weight.element_size() * weight.numel()\n\t            if current_size + size > max_shard_bytes:\n", "                current_index += 1\n\t                current_size = 0\n\t            current_size += size\n\t            weight_mapping[name] = f\"model_weights_{current_index}.safetensors\"\n\t        config.weight_files = sorted(set(weight_mapping.values()))\n\t        for file in tqdm(config.weight_files):\n\t            weights = { name: state_dict[name] for name, f in weight_mapping.items() if file == f }\n\t            save_file(weights, path / file)\n\t    config_path = path / \"config.json\"\n\t    config_path.write_text(config.to_json())\n"]}
{"filename": "chatglm_q/tokenizer.py", "chunked_list": ["import re\n\timport numpy\n\timport torch\n\tfrom typing import Any, Union, Literal\n\tfrom sentencepiece import SentencePieceProcessor\n\tclass BatchEncoding(dict[str, torch.Tensor]):\n\t    def to(self, device):\n\t        for key in list(self.keys()):\n\t            if isinstance(self[key], torch.Tensor):\n\t                self[key] = self[key].to(device)\n", "        return self\n\t    def __getattr__(self, item: str):\n\t        try:\n\t            return self[item]\n\t        except KeyError:\n\t            raise AttributeError\n\t    def __setattr__(self, item: str, value: Any):\n\t        self[item] = value\n\tclass ChatGLM2Tokenizer:\n\t    def __init__(self, vocab_file):\n", "        assert vocab_file is not None\n\t        self.vocab_file = vocab_file\n\t        self.special_tokens = [\"[MASK]\", \"[gMASK]\", \"[sMASK]\", \"<sop>\", \"<eop>\"]\n\t        self.text_tokenizer = SentencePieceProcessor(str(vocab_file))\n\t        self.vocab_size = len(self.text_tokenizer) + len(self.special_tokens)\n\t        self.true_vocab_size = len(self.text_tokenizer)\n\t        self.bos_id: int = self.text_tokenizer.bos_id()\n\t        self.eos_id: int = self.text_tokenizer.eos_id()\n\t        self.pad_id: int = self.text_tokenizer.unk_id()\n\t    def __len__(self):\n", "        return self.vocab_size\n\t    def __getitem__(self, key: str):\n\t        if key in self.special_tokens:\n\t            return len(self.text_tokenizer) + self.special_tokens.index(key)\n\t        return self.text_tokenizer[key]\n\t    def encode(\n\t        self, text: str, text_pair: str = None, add_special_tokens=True,\n\t    ) -> list[int]:\n\t        \"\"\"\n\t        text: Text to encode.\n", "        text_pair: Expected answer to encode.\n\t        add_special_tokens: Add \"[gMASK]\" \"<sop>\" before `text` and \"</s>\" after `text_pair`\n\t        \"\"\"\n\t        tokens = self.text_tokenizer.encode(text)\n\t        if add_special_tokens:\n\t            tokens = [self[\"[gMASK]\"], self[\"<sop>\"]] + tokens\n\t        if text_pair is not None:\n\t            pair_tokens = self.text_tokenizer.encode(text_pair)\n\t            tokens += pair_tokens\n\t            if add_special_tokens:\n", "                tokens += [self.eos_id]\n\t        return tokens\n\t    def decode(self, text_ids: list[int]) -> str:\n\t        text_ids = list(filter(lambda x: x < self.true_vocab_size, text_ids))\n\t        text = self.text_tokenizer.decode(text_ids)\n\t        return text\n\t    def __call__(\n\t        self,\n\t        text: Union[str, list[str]],\n\t        text_pair: Union[str, list[str]] = None,\n", "        add_special_tokens = True,\n\t        padding: Literal[True, False, \"left\", \"right\"] = False, # default pad to left\n\t        max_length: int = None,\n\t        return_tensors: Literal[False, \"pt\", \"np\"] = False,\n\t        return_labels = False,\n\t    ) -> BatchEncoding:\n\t        if isinstance(text, str):\n\t            text = [text]\n\t        if isinstance(text_pair, str):\n\t            text_pair = [text_pair]\n", "        if text_pair is None:\n\t            text_pair = [None] * len(text) \n\t        assert len(text) == len(text_pair)\n\t        input_ids = []\n\t        for t, tp in zip(text, text_pair):\n\t            input_ids.append(self.encode(t, tp, add_special_tokens))\n\t        attention_mask = []\n\t        for inputs in input_ids:\n\t            attention_mask.append([1] * len(inputs))\n\t        position_ids = []\n", "        for inputs in input_ids:\n\t            position_ids.append(list(range(len(inputs))))\n\t        if max_length:\n\t            for i in range(len(input_ids)):\n\t                input_ids[i] = input_ids[i][:max_length]\n\t                attention_mask[i] = attention_mask[i][:max_length]\n\t                position_ids[i] = position_ids[i][:max_length]\n\t        max_seq_length = max(map(lambda x: len(x), input_ids))\n\t        if padding == \"right\":\n\t            for i in range(len(input_ids)):\n", "                pad_length = max_seq_length - len(input_ids[i])\n\t                input_ids[i] = input_ids[i] + pad_length * [self.pad_id]\n\t                attention_mask[i] = attention_mask[i] + pad_length * [0]\n\t                position_ids[i] = position_ids[i] + pad_length * [0]\n\t        elif padding == \"left\" or padding == True:\n\t            for i in range(len(input_ids)):\n\t                pad_length = max_seq_length - len(input_ids[i])\n\t                input_ids[i] = pad_length * [self.pad_id] + input_ids[i]\n\t                attention_mask[i] = pad_length * [0] + attention_mask[i]\n\t                position_ids[i] = pad_length * [0] + position_ids[i]\n", "        else:\n\t            assert not return_tensors, \"set padding=True when return_tensors\"\n\t        if return_tensors == \"np\":\n\t            input_ids = numpy.array(input_ids, dtype=numpy.int64)\n\t            attention_mask = numpy.array(attention_mask, dtype=numpy.int64)\n\t            position_ids = numpy.array(position_ids, dtype=numpy.int64)\n\t        elif return_tensors == \"pt\":\n\t            input_ids = torch.tensor(input_ids, dtype=torch.long)\n\t            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n\t            position_ids = torch.tensor(position_ids, dtype=torch.long)\n", "        inputs = BatchEncoding(\n\t            input_ids=input_ids,\n\t            attention_mask=attention_mask,\n\t            position_ids=position_ids,\n\t        )\n\t        if return_labels:\n\t            assert return_tensors == \"pt\", \"'return_labels' should be used with return_tensors='pt'\"\n\t            # -100: CrossEntropyLoss ignore_index\n\t            labels = input_ids.masked_fill(~attention_mask.bool(), -100)\n\t            inputs[\"labels\"] = labels\n", "        return inputs\n"]}
{"filename": "chatglm_q/__init__.py", "chunked_list": []}
{"filename": "chatglm_q/int8/qlinear.py", "chunked_list": ["import torch\n\tfrom torch import nn, Tensor\n\tfrom torch.autograd.function import FunctionCtx\n\ttry:\n\t    from .triton_ops import (\n\t        check_input,\n\t        dynamic_quant_matmul as _dynamic_quant_matmul_impl,\n\t        dynamic_quant_matmul_transposed as _dynamic_quant_matmul_transposed_impl,\n\t    )\n\t    KERNEL_IMPL = \"triton\"\n", "except ImportError as e:\n\t    print(\"Import triton ops failed. Using slower torch fallback.\")\n\t    check_input = None\n\t    KERNEL_IMPL = \"none\"\n\tclass DynamicQuantizeMatMul(torch.autograd.Function):\n\t    '''\n\t    ONNXRuntime custom op\n\t    com.microsoft::DynamicQuantizeMatMul\n\t    A: tensor(float) m × k\n\t    B: tensor(int8) k × n\n", "    b_scale: tensor(float) n\n\t    In PyTorch, the weigth is dequantized first.\n\t    '''\n\t    @staticmethod\n\t    def forward(ctx: FunctionCtx, A: Tensor, B: Tensor, b_scale: Tensor):\n\t        # 'A' must be saved to get grad\n\t        ctx.save_for_backward(A, B, b_scale)\n\t        if check_input and check_input(A):\n\t            out = _dynamic_quant_matmul_impl(A, B, b_scale)\n\t        else:\n", "            out = A.matmul(B * b_scale)\n\t        return out\n\t    @staticmethod\n\t    def backward(ctx: FunctionCtx, grad_out: Tensor):\n\t        A, B, b_scale = ctx.saved_tensors\n\t        grad_A = None\n\t        if ctx.needs_input_grad[0]:\n\t            if check_input and check_input(A):\n\t                grad_A = _dynamic_quant_matmul_transposed_impl(grad_out, B, b_scale)\n\t            else:\n", "                grad_A = grad_out.matmul(B.t() * b_scale[:, None])\n\t        return grad_A, None, None\n\t    ONNX_CPU_ONLY = True\n\t    @staticmethod\n\t    def symbolic(g: torch.Graph, A, B, b_scale) -> torch.Value:\n\t        if DynamicQuantizeMatMul.ONNX_CPU_ONLY:\n\t            # return g.op(\"com.microsoft::DynamicQuantizeMatMul\", A, B, b_scale)\n\t            A_quant, A_scale, A_zero = g.op(\"DynamicQuantizeLinear\", A, outputs=3)\n\t            C = g.op(\"MatMulInteger\", A_quant, B, A_zero, torch.tensor(0, dtype=torch.int8))\n\t            C = g.op(\"Cast\", C, to_i=1) # TensorProto.DataType.FLOAT=1\n", "            return g.op(\"Mul\", C, g.op(\"Mul\", A_scale, b_scale))\n\t        else:\n\t            # is unstable on CUDA and produces NaN\n\t            A_scale = g.op(\"Div\", g.op(\"ReduceMax\", g.op(\"Abs\", A), keepdims_i=0), torch.tensor(127, dtype=torch.float32))\n\t            A_quant = g.op(\"QuantizeLinear\", A, A_scale, torch.tensor(0, dtype=torch.int8))\n\t            C = g.op(\"MatMulInteger\", A_quant, B, torch.tensor(0, dtype=torch.int8), torch.tensor(0, dtype=torch.int8))\n\t            C = g.op(\"Cast\", C, to_i=1) # TensorProto.DataType.FLOAT=1\n\t            return g.op(\"Mul\", C, g.op(\"Mul\", A_scale, b_scale))\n\tdef dynamic_quant_matmul(A: Tensor, B: torch.CharTensor, b_scale: Tensor) -> Tensor:\n\t    return DynamicQuantizeMatMul.apply(A, B, b_scale)\n", "class DynamicQuantizeLinear(nn.Module):\n\t    def __init__(self, in_features: int, out_features: int, bias: bool=True, device=None, dtype=None):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.register_buffer(\"weight\", torch.empty((out_features, in_features), device=device, dtype=torch.int8))\n\t        self.register_buffer(\"weight_scale\", torch.empty(out_features, device=device, dtype=dtype))\n\t        if bias:\n\t            self.register_buffer(\"bias\", torch.empty(out_features, device=device, dtype=dtype))\n\t        else:\n", "            self.register_buffer('bias', None)\n\t    def forward(self, input: Tensor):\n\t        out = dynamic_quant_matmul(input, self.weight.t(), self.weight_scale)\n\t        if self.bias is not None:\n\t            out += self.bias\n\t        return out\n\t    @torch.no_grad()\n\t    def apply_weights_(self, q_weight: Tensor, scale: Tensor, bias: Tensor = None):\n\t        self.weight.copy_(q_weight)\n\t        self.weight_scale.copy_(scale)\n", "        if bias is not None:\n\t            self.bias.copy_(bias)\n\t    def extra_repr(self) -> str:\n\t        return 'in_features={}, out_features={}, bias={}'.format(\n\t            self.in_features, self.out_features, self.bias is not None)\n\t    def reset_parameters(self):\n\t        pass\n\tclass QEmbedding(nn.Module):\n\t    def __init__(self, num_embeddings: int, embedding_dim: int, device=None, dtype=None):\n\t        super().__init__()\n", "        self.num_embeddings = num_embeddings\n\t        self.embedding_dim = embedding_dim\n\t        self.register_buffer(\"weight\", torch.empty((num_embeddings, embedding_dim), device=device, dtype=torch.int8))\n\t        self.register_buffer(\"weight_scale\", torch.empty(embedding_dim, device=device, dtype=dtype))\n\t    def forward(self, input: Tensor):\n\t        embeddings = nn.functional.embedding(input, self.weight)\n\t        return embeddings * self.weight_scale\n\t    @torch.no_grad()\n\t    def apply_weights_(self, q_weight: Tensor, scale: Tensor):\n\t        self.weight.copy_(q_weight)\n", "        self.weight_scale.copy_(scale)\n\t    def extra_repr(self) -> str:\n\t        return 'num_embeddings={}, embedding_dim={}'.format(\n\t            self.num_embeddings, self.embedding_dim)\n\t    def reset_parameters(self):\n\t        pass\n"]}
{"filename": "chatglm_q/int8/__init__.py", "chunked_list": []}
{"filename": "chatglm_q/int8/triton_ops.py", "chunked_list": ["import torch\n\tfrom torch import Tensor\n\timport triton\n\timport triton.language as tl\n\t# from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time\n\tdef check_input(a: torch.Tensor):\n\t    return a.get_device() >= 0\n\t@triton.autotune(\n\t    configs=[\n\t        # multiple configs not working for triton==2.0.0.post1\n", "        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_M': 8, 'SPLIT_K': 1}, num_stages=2, num_warps=8),\n\t    ],\n\t    key=['M', 'N', 'K'],\n\t)\n\t@triton.heuristics({\n\t    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n\t})\n\t@triton.jit\n\tdef _dynamic_quant_matmul_kernel(\n\t    A, B, B_scale, C, M, N, K,\n", "    stride_am, stride_ak,\n\t    stride_bk, stride_bn, stride_bscale,\n\t    stride_cm, stride_cn,\n\t    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n\t    GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n\t    allow_tf32: tl.constexpr,\n\t):\n\t    '''\n\t    A:        (M, K)  *float\n\t    B:        (K, N)  *int8\n", "    B_scale:  (N)     *float\n\t    C:        (M, N)  *float\n\t    '''\n\t    # matrix multiplication\n\t    pid = tl.program_id(0)\n\t    pid_z = tl.program_id(1)\n\t    grid_m = tl.cdiv(M, BLOCK_M)\n\t    grid_n = tl.cdiv(N, BLOCK_N)\n\t    # re-order program ID for better L2 performance\n\t    width = GROUP_M * grid_n\n", "    group_id = pid // width\n\t    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n\t    pid_m = group_id * GROUP_M + (pid % group_size)\n\t    pid_n = (pid % width) // (group_size)\n\t    # do matrix multiplication\n\t    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n\t    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n\t    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n", "    # pointers\n\t    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n\t    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n\t    B_scale = B_scale + (rbn[None, :] * stride_bscale)\n\t    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\t    scale = tl.load(B_scale)\n\t    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n\t        if EVEN_K:\n\t            a = tl.load(A)\n\t            b = tl.load(B)\n", "        else:\n\t            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n\t            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n\t            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n\t        b = b * scale\n\t        acc += tl.dot(a, b, allow_tf32=allow_tf32)\n\t        A += BLOCK_K * SPLIT_K * stride_ak\n\t        B += BLOCK_K * SPLIT_K * stride_bk\n\t    acc = acc.to(C.dtype.element_ty)\n\t    # rematerialize rm and rn to save registers\n", "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n\t    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\t    # handles write-back with reduction-splitting\n\t    if SPLIT_K == 1:\n\t        tl.store(C, acc, mask=mask)\n\t    else:\n\t        tl.atomic_add(C, acc, mask=mask)\n\tdef dynamic_quant_matmul(a: Tensor, b: Tensor, b_scale: Tensor, allow_tf32: bool=None):\n", "    '''\n\t    a:        (M, K)  *float\n\t    b:        (K, N)  *int8\n\t    b_scale:  (N)     *float\n\t    returns:  (M, N)  *float\n\t    '''\n\t    # checks constraints\n\t    output_shape = (*a.shape[:-1], b.shape[1])\n\t    a = a.flatten(0, -2)\n\t    assert len(b.shape) == 2\n", "    assert len(b_scale.shape) == 1\n\t    assert a.shape[1] == b.shape[0]\n\t    assert b.shape[1] == b_scale.shape[0]\n\t    assert b.dtype == torch.int8\n\t    assert a.dtype == b_scale.dtype\n\t    assert a.get_device() >= 0\n\t    assert b.get_device() == a.get_device(), f\"{b.device=}, {a.device=}\"\n\t    assert b_scale.get_device() == a.get_device(), f\"{b_scale.device=}, {a.device=}\"\n\t    # handle non-contiguous inputs if necessary\n\t    if a.stride(0) > 1 and a.stride(1) > 1:\n", "        a = a.contiguous()\n\t    if b.stride(0) > 1 and b.stride(1) > 1:\n\t        b = b.contiguous()\n\t    # allocates output\n\t    M, K = a.shape\n\t    _, N = b.shape\n\t    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\t    # launch kernel\n\t    if allow_tf32 is None:\n\t        allow_tf32 = bool(torch.backends.cudnn.allow_tf32)\n", "    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n\t    with torch.cuda.device(a.device):\n\t        _dynamic_quant_matmul_kernel[grid](\n\t            a, b, b_scale, c, M, N, K,\n\t            a.stride(0), a.stride(1),\n\t            b.stride(0), b.stride(1), b_scale.stride(0),\n\t            c.stride(0), c.stride(1),\n\t            allow_tf32=allow_tf32,\n\t        )\n\t        return c.reshape(output_shape)\n", "@triton.autotune(\n\t    configs=[\n\t        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64, 'BLOCK_K': 128, 'GROUP_M': 8, 'SPLIT_K': 1}, num_stages=2, num_warps=8),\n\t    ],\n\t    key=['M', 'N', 'K'],\n\t)\n\t@triton.heuristics({\n\t    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n\t})\n\t@triton.jit\n", "def _dynamic_quant_matmul_transposed_kernel(\n\t    A, B_T, B_scale, C, M, N, K,\n\t    stride_am, stride_ak,\n\t    stride_bn, stride_bk, stride_bscale,\n\t    stride_cm, stride_cn,\n\t    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n\t    GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n\t    allow_tf32: tl.constexpr,\n\t):\n\t    '''\n", "    A:        (M, K)  *float\n\t    B_T:      (N, K)  *int8  (transposed)\n\t    B_scale:  (K)     *float (transposed scale)\n\t    C:        (M, N)  *float\n\t    '''\n\t    # matrix multiplication\n\t    pid = tl.program_id(0)\n\t    pid_z = tl.program_id(1)\n\t    grid_m = tl.cdiv(M, BLOCK_M)\n\t    grid_n = tl.cdiv(N, BLOCK_N)\n", "    # re-order program ID for better L2 performance\n\t    width = GROUP_M * grid_n\n\t    group_id = pid // width\n\t    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n\t    pid_m = group_id * GROUP_M + (pid % group_size)\n\t    pid_n = (pid % width) // (group_size)\n\t    # do matrix multiplication\n\t    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n", "    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n\t    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n\t    # pointers\n\t    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n\t    B_T = B_T + (rbn[:, None] * stride_bn + rk[None, :] * stride_bk)\n\t    B_scale = B_scale + (rk[None, :] * stride_bscale)\n\t    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\t    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n\t        if EVEN_K:\n\t            a = tl.load(A)\n", "            b = tl.load(B_T)\n\t            scale = tl.load(B_scale)\n\t        else:\n\t            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n\t            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n\t            b = tl.load(B_T, mask=rk[None, :] < k_remaining, other=0.)\n\t            scale = tl.load(B_scale, mask=rk[None, :] < k_remaining, other=1.)\n\t        b = tl.trans(b * scale)\n\t        acc += tl.dot(a, b, allow_tf32=allow_tf32)\n\t        A += BLOCK_K * SPLIT_K * stride_ak\n", "        B_T += BLOCK_K * SPLIT_K * stride_bk\n\t        B_scale += BLOCK_K * SPLIT_K * stride_bscale\n\t    acc = acc.to(C.dtype.element_ty)\n\t    # rematerialize rm and rn to save registers\n\t    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n\t    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\t    # handles write-back with reduction-splitting\n\t    if SPLIT_K == 1:\n", "        tl.store(C, acc, mask=mask)\n\t    else:\n\t        tl.atomic_add(C, acc, mask=mask)\n\tdef dynamic_quant_matmul_transposed(a: Tensor, b_T: Tensor, b_scale: Tensor, allow_tf32: bool=None):\n\t    '''\n\t    a:        (M, K)  float\n\t    b_T:      (N, K)  int8  (transposed)\n\t    b_scale:  (K)     float (transposed scale)\n\t    returns:  (M, N)  float\n\t    '''\n", "    # checks constraints\n\t    output_shape = (*a.shape[:-1], b_T.shape[0])\n\t    a = a.flatten(0, -2)\n\t    assert len(b_T.shape) == 2\n\t    assert len(b_scale.shape) == 1\n\t    assert a.shape[1] == b_T.shape[1]\n\t    assert b_T.shape[1] == b_scale.shape[0]\n\t    assert b_T.dtype == torch.int8\n\t    assert a.dtype == b_scale.dtype\n\t    assert a.get_device() >= 0\n", "    assert b_T.get_device() == a.get_device(), f\"{b_T.device=}, {a.device=}\"\n\t    assert b_scale.get_device() == a.get_device(), f\"{b_scale.device=}, {a.device=}\"\n\t    # handle non-contiguous inputs if necessary\n\t    if a.stride(0) > 1 and a.stride(1) > 1:\n\t        a = a.contiguous()\n\t    if b_T.stride(0) > 1 and b_T.stride(1) > 1:\n\t        b_T = b_T.contiguous()\n\t    # allocates output\n\t    M, K = a.shape\n\t    N, _ = b_T.shape\n", "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\t    # launch kernel\n\t    if allow_tf32 is None:\n\t        allow_tf32 = bool(torch.backends.cudnn.allow_tf32)\n\t    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n\t    with torch.cuda.device(a.device):\n\t        _dynamic_quant_matmul_transposed_kernel[grid](\n\t            a, b_T, b_scale, c, M, N, K,\n\t            a.stride(0), a.stride(1),\n\t            b_T.stride(0), b_T.stride(1), b_scale.stride(0),\n", "            c.stride(0), c.stride(1),\n\t            allow_tf32=allow_tf32,\n\t        )\n\t        return c.reshape(output_shape)\n"]}
{"filename": "chatglm_q/int8/quantizer.py", "chunked_list": ["import math\n\timport torch\n\tfrom torch import nn, Tensor\n\tfrom .qlinear import DynamicQuantizeLinear, QEmbedding\n\tmax_q_int8 = 2 ** (8 - 1) - 1\n\tassert 127 == max_q_int8\n\t@torch.no_grad()\n\tdef quantize_int8(inputs: Tensor) -> tuple[torch.CharTensor, Tensor]:\n\t    '''\n\t    inputs: for weight (out_dim, in_dim), for activation (...channels, features)\n", "    '''\n\t    w_max, _ = torch.max(inputs.abs(), dim=1, keepdim=True)\n\t    scale = torch.clamp(w_max / max_q_int8, min=1e-10) # safe for float16\n\t    inputs = torch.clamp(torch.round(inputs / scale), -max_q_int8, max_q_int8)\n\t    return inputs.to(torch.int8), scale.squeeze(dim=-1)\n\t@torch.no_grad()\n\tdef clamp_to_quantize_grid(x: Tensor, scale: Tensor) -> Tensor:\n\t    q = torch.clamp(torch.round(x / scale), -max_q_int8, max_q_int8)\n\t    return scale * q\n\t@torch.no_grad()\n", "def quantize_with_scale(x: Tensor, scale: Tensor) -> Tensor:\n\t    return torch.clamp(torch.round(x / scale), -max_q_int8, max_q_int8).to(torch.int8)\n\t@torch.no_grad()\n\tdef get_quant_int8_linear(layer: nn.Linear):\n\t    assert isinstance(layer, nn.Linear)\n\t    q_weight, scale = quantize_int8(layer.weight)\n\t    qlinear = DynamicQuantizeLinear(layer.in_features, layer.out_features, layer.bias is not None)\n\t    qlinear.apply_weights_(q_weight, scale, layer.bias)\n\t    return qlinear\n\t@torch.no_grad()\n", "def get_quant_embedding(layer: nn.Embedding):\n\t    assert isinstance(layer, nn.Embedding)\n\t    q_weight, scale = quantize_int8(layer.weight.t())\n\t    qembedding = QEmbedding(layer.num_embeddings, layer.embedding_dim)\n\t    qembedding.apply_weights_(q_weight.t(), scale)\n\t    return qembedding\n\tclass GPTQLinearQuantizer():\n\t    '''\n\t    GPTQ quantization, see:\n\t        paper: https://arxiv.org/abs/2210.17323\n", "        code: https://github.com/IST-DASLab/gptq\n\t        license: Apache License 2.0 https://github.com/IST-DASLab/gptq/blob/main/LICENSE\n\t    '''\n\t    def __init__(self, layer: nn.Module):\n\t        assert isinstance(layer, nn.Linear)\n\t        self.layer = layer\n\t        self.hook = layer.register_forward_hook(self.forward_hook)\n\t        # output_dim, input_dim\n\t        self.n_rows, self.n_columns = layer.weight.shape\n\t        self.hessian = torch.zeros((self.n_columns, self.n_columns), device=layer.weight.device)\n", "        self.n_samples = 0\n\t        self.debug_input = None\n\t    @torch.no_grad()\n\t    def forward_hook(self, module: nn.Module, inputs: tuple[Tensor], output: Tensor):\n\t        input, = inputs\n\t        if len(input.shape) > 2:\n\t            input = input.flatten(0, -2)\n\t        self.debug_input = input.detach()\n\t        new_samples, d_hidden = input.shape\n\t        assert d_hidden == self.n_columns\n", "        input = input.t()\n\t        self.hessian *= self.n_samples / (self.n_samples + new_samples)\n\t        self.n_samples += new_samples\n\t        input = math.sqrt(2 / self.n_samples) * input.float()\n\t        self.hessian += input.matmul(input.t())\n\t    def remove_hook(self):\n\t        self.hook.remove()\n\t    @torch.no_grad()\n\t    def quantize_weight(self, blocksize=128, percdamp=.01):\n\t        assert self.n_samples > 0\n", "        hessian = self.hessian\n\t        weight = self.layer.weight.clone()\n\t        _, scale = quantize_int8(weight)\n\t        dead_rows = torch.diag(hessian) == 0\n\t        hessian[dead_rows, dead_rows] = 1\n\t        weight[:, dead_rows] = 0\n\t        quant_losses = torch.zeros_like(weight)\n\t        grid_weight = torch.zeros_like(weight)\n\t        damp = percdamp * torch.mean(torch.diag(hessian))\n\t        diag = torch.arange(self.n_columns, device=weight.device)\n", "        hessian[diag, diag] += damp\n\t        hessian_inv = torch.cholesky_inverse(torch.linalg.cholesky(hessian))\n\t        hessian_inv = torch.linalg.cholesky(hessian_inv, upper=True)\n\t        assert not hessian_inv.isnan().any()\n\t        for i1 in range(0, self.n_columns, blocksize):\n\t            i2 = min(i1 + blocksize, self.n_columns)\n\t            weight_block = weight[:, i1:i2].clone()\n\t            quant_block = torch.zeros_like(weight_block)\n\t            err_block = torch.zeros_like(weight_block)\n\t            losses_block = torch.zeros_like(weight_block)\n", "            h_inv_block = hessian_inv[i1:i2, i1:i2]\n\t            for j in range(i2 - i1):\n\t                w = weight_block[:, j]\n\t                d = h_inv_block[j, j]\n\t                q = clamp_to_quantize_grid(w, scale)\n\t                quant_block[:, j] = q\n\t                losses_block[:, j] = (w - q) ** 2 / d ** 2\n\t                err = (w - q) / d\n\t                weight_block[:, j:] -= err.unsqueeze(1).matmul(h_inv_block[j, j:].unsqueeze(0))\n\t                err_block[:, j] = err\n", "            grid_weight[:, i1:i2] = quant_block\n\t            quant_losses[:, i1:i2] = losses_block / 2\n\t            weight[:, i2:] -= err_block.matmul(hessian_inv[i1:i2, i2:])\n\t        debug_output = nn.functional.linear(self.debug_input, self.layer.weight)\n\t        quant_out = nn.functional.linear(self.debug_input, grid_weight)\n\t        debug_loss = torch.mean((quant_out - debug_output) ** 2).item()\n\t        quant_losses = quant_losses.mean().item()\n\t        return grid_weight, scale, quant_losses, debug_loss\n\t    @torch.no_grad()\n\t    def get_quantized_linear(self, blocksize=128, percdamp=.01, pring_loss=False):\n", "        grid_weight, scale, quant_losses, debug_loss = self.quantize_weight(blocksize, percdamp)\n\t        if pring_loss:\n\t            print(f\"{quant_losses=:.8f} {debug_loss=:.8f}\")\n\t        original = self.layer\n\t        modified = DynamicQuantizeLinear(original.in_features, original.out_features, original.bias is not None)\n\t        q_weight = quantize_with_scale(grid_weight, scale[:, None])\n\t        modified.apply_weights_(q_weight, scale, original.bias)\n\t        return modified\n"]}
{"filename": "chatglm_q/int4/qlinear.py", "chunked_list": ["import torch\n\tfrom torch import nn, Tensor\n\tfrom torch.autograd.function import FunctionCtx\n\tDEFAULT_GROUP_SIZE = 32\n\ttry:\n\t    from .triton_ops import (\n\t        check_input,\n\t        dynamic_quant_matmul_s4 as _dynamic_quant_matmul_impl,\n\t        dynamic_quant_matmul_transposed_s4 as _dynamic_quant_matmul_transposed_impl,\n\t    )\n", "    KERNEL_IMPL = \"triton\"\n\texcept ImportError as e:\n\t    print(\"Import triton ops failed. Using slower torch fallback.\")\n\t    check_input = None\n\t    KERNEL_IMPL = \"none\"\n\t@torch.no_grad()\n\tdef unpack_int4(x: torch.Tensor, x_scale: torch.Tensor):\n\t    # shape\n\t    K = x.shape[0] * 2\n\t    G, N = x_scale.shape\n", "    assert x.shape[1] == N\n\t    assert K % G == 0, f\"{K=}, {G=}\"\n\t    GROUP_K = K // G\n\t    # unpack\n\t    shifts = torch.tensor([0, 4]).reshape((1, 2, 1)).type_as(x)\n\t    x = x.reshape((K // 2, 1, N)).repeat((1, 2, 1))\n\t    x = ((x >> shifts) & 0xF).to(torch.int8) - 0x8\n\t    x = x.reshape((G, GROUP_K, N)) * x_scale[:, None, :]\n\t    return x.reshape((K, N))\n\tclass DynamicQuantizeMatMul(torch.autograd.Function):\n", "    '''\n\t    A: tensor(float) m × k\n\t    B: tensor(int8) k//2 × n\n\t    b_scale: tensor(float) g × n\n\t    '''\n\t    @staticmethod\n\t    def forward(ctx: FunctionCtx, A: Tensor, B: Tensor, b_scale: Tensor):\n\t        # 'A' must be saved to get grad\n\t        ctx.save_for_backward(A, B, b_scale)\n\t        if check_input and check_input(A):\n", "            out = _dynamic_quant_matmul_impl(A, B, b_scale)\n\t        else:\n\t            out = A.matmul(unpack_int4(B, b_scale))\n\t        return out\n\t    @staticmethod\n\t    def backward(ctx: FunctionCtx, grad_out: Tensor):\n\t        A, B, b_scale = ctx.saved_tensors\n\t        grad_A = None\n\t        if ctx.needs_input_grad[0]:\n\t            if check_input and check_input(A):\n", "                grad_A = _dynamic_quant_matmul_transposed_impl(grad_out, B, b_scale)\n\t            else:\n\t                grad_A = grad_out.matmul(unpack_int4(B, b_scale).t())\n\t        return grad_A, None, None\n\t    @staticmethod\n\t    def symbolic(g: torch.Graph, A, B, b_scale) -> torch.Value:\n\t        raise NotImplementedError()\n\tdef dynamic_quant_matmul(A: Tensor, B: torch.CharTensor, b_scale: Tensor) -> Tensor:\n\t    return DynamicQuantizeMatMul.apply(A, B, b_scale)\n\tclass DynamicQuantizeLinear(nn.Module):\n", "    def __init__(self, in_features: int, out_features: int, bias=True, group_size=DEFAULT_GROUP_SIZE, device=None, dtype=None):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        assert in_features % group_size == 0, f\"{in_features=}, {group_size=}\"\n\t        self.group_size = group_size\n\t        self.groups = in_features // group_size\n\t        self.register_buffer(\"weight\", torch.empty((in_features//2, out_features), device=device, dtype=torch.uint8))\n\t        self.register_buffer(\"weight_scale\", torch.empty((self.groups, out_features), device=device, dtype=dtype))\n\t        if bias:\n", "            self.register_buffer(\"bias\", torch.empty(out_features, device=device, dtype=dtype))\n\t        else:\n\t            self.register_buffer('bias', None)\n\t    def forward(self, input: Tensor):\n\t        out = dynamic_quant_matmul(input, self.weight, self.weight_scale)\n\t        if self.bias is not None:\n\t            out += self.bias\n\t        return out\n\t    @torch.no_grad()\n\t    def apply_weights_(self, q_weight: Tensor, scale: Tensor, bias: Tensor = None):\n", "        self.weight.copy_(q_weight)\n\t        self.weight_scale.copy_(scale)\n\t        if bias is not None:\n\t            self.bias.copy_(bias)\n\t    def extra_repr(self) -> str:\n\t        return 'in_features={}, out_features={}, group_size={}, bias={}'.format(\n\t            self.in_features, self.out_features, self.group_size, self.bias is not None)\n\t    def reset_parameters(self):\n\t        pass\n\tclass QEmbedding(nn.Module):\n", "    def __init__(self, num_embeddings: int, embedding_dim: int, group_size=DEFAULT_GROUP_SIZE, device=None, dtype=None):\n\t        super().__init__()\n\t        self.num_embeddings = num_embeddings\n\t        self.embedding_dim = embedding_dim\n\t        assert num_embeddings % group_size == 0, f\"{num_embeddings=}, {group_size=}\"\n\t        self.group_size = group_size\n\t        self.groups = num_embeddings // group_size\n\t        self.register_buffer(\"weight\", torch.empty((num_embeddings//2, embedding_dim), device=device, dtype=torch.uint8))\n\t        self.register_buffer(\"weight_scale\", torch.empty((self.groups, embedding_dim), device=device, dtype=dtype))\n\t    def forward(self, input: Tensor):\n", "        group_idx = input // self.group_size\n\t        embed_idx = input // 2\n\t        scales = nn.functional.embedding(group_idx, self.weight_scale)\n\t        embeddings = nn.functional.embedding(embed_idx, self.weight)\n\t        shifts = ((input % 2) * 4)[..., None].type_as(embeddings)\n\t        embeddings = ((embeddings >> shifts) & 0xF).to(torch.int8)\n\t        embeddings = (embeddings - 0x8) * scales\n\t        return embeddings\n\t    @torch.no_grad()\n\t    def apply_weights_(self, q_weight: Tensor, scale: Tensor):\n", "        self.weight.copy_(q_weight)\n\t        self.weight_scale.copy_(scale)\n\t    def extra_repr(self) -> str:\n\t        return 'num_embeddings={}, embedding_dim={}, group_size={}'.format(\n\t            self.num_embeddings, self.embedding_dim, self.group_size)\n\t    def reset_parameters(self):\n\t        pass\n"]}
{"filename": "chatglm_q/int4/__init__.py", "chunked_list": []}
{"filename": "chatglm_q/int4/triton_ops.py", "chunked_list": ["import torch\n\tfrom torch import Tensor\n\timport triton\n\timport triton.language as tl\n\t# from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time\n\tdef check_input(a: torch.Tensor):\n\t    return a.get_device() >= 0\n\tdef is_power_of_two(n: int):\n\t    return (n & (n-1) == 0) and n != 0\n\t@triton.autotune(\n", "    configs=[\n\t        # multiple configs not working for triton==2.0.0.post1\n\t        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128, 'GROUP_M': 8}, num_stages=2, num_warps=8),\n\t    ],\n\t    key=['M', 'N', 'K'],\n\t)\n\t@triton.jit\n\tdef _dynamic_quant_matmul_s4_kernel(\n\t    A, B, B_scale, C, M, N, K,\n\t    stride_am, stride_ak,\n", "    stride_bk, stride_bn,\n\t    stride_bscale_g, stride_bscale_n,\n\t    stride_cm, stride_cn,\n\t    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n\t    GROUP_M: tl.constexpr, GROUP_K: tl.constexpr,\n\t    allow_tf32: tl.constexpr,\n\t):\n\t    '''\n\t    A:        (M, K)     *float\n\t    B:        (K//2, N)  *uint8\n", "    B_scale:  (G, N)     *float\n\t    C:        (M, N)     *float\n\t    requirements: K // G == GROUP_K, GROUP_K % BLOCK_K == 0\n\t    '''\n\t    # matrix multiplication\n\t    pid = tl.program_id(0)\n\t    grid_m = tl.cdiv(M, BLOCK_M)\n\t    grid_n = tl.cdiv(N, BLOCK_N)\n\t    # re-order program ID for better L2 performance\n\t    width = GROUP_M * grid_n\n", "    group_id = pid // width\n\t    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n\t    pid_m = group_id * GROUP_M + (pid % group_size)\n\t    pid_n = (pid % width) // (group_size)\n\t    # do matrix multiplication\n\t    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n\t    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n\t    rk = tl.arange(0, BLOCK_K)\n", "    # pointers\n\t    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n\t    B = B + ((rk[:, None] // 2) * stride_bk + rbn[None, :] * stride_bn)\n\t    B_scale = B_scale + (0 * stride_bscale_g + rbn[None, :] * stride_bscale_n)\n\t    quant_group_by_k = (GROUP_K // BLOCK_K)\n\t    B_shift = ((rk % 2) * 4)[:, None]\n\t    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\t    for k in range(0, tl.cdiv(K, BLOCK_K)):\n\t        a = tl.load(A)\n\t        b = tl.load(B)\n", "        scale = tl.load(B_scale)\n\t        b = ((b >> B_shift) & 0xF).to(tl.int8)\n\t        b = (b - 0x8) * scale\n\t        acc += tl.dot(a, b, allow_tf32=allow_tf32)\n\t        A += BLOCK_K * stride_ak\n\t        B += (BLOCK_K // 2) * stride_bk\n\t        if (k + 1) % quant_group_by_k == 0:\n\t            B_scale += stride_bscale_g\n\t    acc = acc.to(C.dtype.element_ty)\n\t    # rematerialize rm and rn to save registers\n", "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n\t    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\t    # handles write-back with reduction-splitting\n\t    tl.store(C, acc, mask=mask)\n\tdef dynamic_quant_matmul_s4(a: Tensor, b: Tensor, b_scale: Tensor, allow_tf32: bool=None):\n\t    '''\n\t    A:        (M, K)     *float\n\t    B:        (K//2, N)  *uint8\n", "    B_scale:  (G, N)     *float\n\t    C:        (M, N)     *float\n\t    requirements: K // G == GROUP_K, GROUP_K % BLOCK_K == 0\n\t    '''\n\t    # checks constraints\n\t    output_shape = (*a.shape[:-1], b.shape[1])\n\t    a = a.flatten(0, -2)\n\t    assert len(b.shape) == 2\n\t    assert len(b_scale.shape) == 2\n\t    assert a.shape[1] == b.shape[0] * 2\n", "    assert b.shape[1] == b_scale.shape[1]\n\t    assert b.dtype == torch.uint8\n\t    assert a.dtype == b_scale.dtype\n\t    assert b.shape[0] % b_scale.shape[0] == 0\n\t    assert a.get_device() >= 0\n\t    assert b.get_device() == a.get_device(), f\"{b.device=}, {a.device=}\"\n\t    assert b_scale.get_device() == a.get_device(), f\"{b_scale.device=}, {a.device=}\"\n\t    # handle non-contiguous inputs if necessary\n\t    if a.stride(0) > 1 and a.stride(1) > 1:\n\t        a = a.contiguous()\n", "    if b.stride(0) > 1 and b.stride(1) > 1:\n\t        b = b.contiguous()\n\t    # allocates output\n\t    M, K = a.shape\n\t    G, N = b_scale.shape\n\t    GROUP_K = K // G\n\t    BLOCK_K = min(64, GROUP_K)\n\t    assert is_power_of_two(BLOCK_K)\n\t    assert is_power_of_two(GROUP_K)\n\t    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n", "    # launch kernel\n\t    if allow_tf32 is None:\n\t        allow_tf32 = bool(torch.backends.cudnn.allow_tf32)\n\t    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), )\n\t    with torch.cuda.device(a.device):\n\t        _dynamic_quant_matmul_s4_kernel[grid](\n\t            a, b, b_scale, c, M, N, K,\n\t            a.stride(0), a.stride(1),\n\t            b.stride(0), b.stride(1),\n\t            b_scale.stride(0), b_scale.stride(1),\n", "            c.stride(0), c.stride(1),\n\t            BLOCK_K=BLOCK_K, GROUP_K=GROUP_K,\n\t            allow_tf32=allow_tf32,\n\t        )\n\t        return c.reshape(output_shape)\n\t@triton.autotune(\n\t    configs=[\n\t        # multiple configs not working for triton==2.0.0.post1\n\t        triton.Config({'BLOCK_M': 16, 'BLOCK_K': 128, 'GROUP_M': 8}, num_stages=2, num_warps=8),\n\t    ],\n", "    key=['M', 'N', 'K'],\n\t)\n\t@triton.jit\n\tdef _dynamic_quant_matmul_transposed_s4_kernel(\n\t    A, B, B_scale, C, M, N, K,\n\t    stride_am, stride_ak,\n\t    stride_bn, stride_bk,\n\t    stride_bscale_g, stride_bscale_k,\n\t    stride_cm, stride_cn,\n\t    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n", "    GROUP_M: tl.constexpr, GROUP_N: tl.constexpr,\n\t    allow_tf32: tl.constexpr,\n\t):\n\t    '''\n\t    A:        (M, K)     *float\n\t    B:        (N//2, K)  *uint8\n\t    B_scale:  (G, K)     *float\n\t    C:        (M, N)     *float\n\t    requirements: N // G == GROUP_N, GROUP_N % BLOCK_N == 0, K % BLOCK_K == 0\n\t    '''\n", "    # matrix multiplication\n\t    pid = tl.program_id(0)\n\t    grid_m = tl.cdiv(M, BLOCK_M)\n\t    grid_n = tl.cdiv(N, BLOCK_N)\n\t    # re-order program ID for better L2 performance\n\t    width = GROUP_M * grid_n\n\t    group_id = pid // width\n\t    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n\t    pid_m = group_id * GROUP_M + (pid % group_size)\n\t    pid_n = (pid % width) // (group_size)\n", "    # do matrix multiplication\n\t    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n\t    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n\t    rk = tl.arange(0, BLOCK_K)\n\t    # pointers\n\t    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n\t    B = B + ((rbn[:, None] // 2) * stride_bn + rk[None, :] * stride_bk)\n\t    quant_group = pid_n // (GROUP_N // BLOCK_N)\n", "    B_scale = B_scale + (quant_group * stride_bscale_g + rk[None, :] * stride_bscale_k)\n\t    B_shift = ((rbn % 2) * 4)[:, None]\n\t    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\t    for k in range(0, tl.cdiv(K, BLOCK_K)):\n\t        a = tl.load(A)\n\t        b = tl.load(B)\n\t        scale = tl.load(B_scale)\n\t        b = ((b >> B_shift) & 0xF).to(tl.int8)\n\t        b = tl.trans((b - 0x8) * scale)\n\t        acc += tl.dot(a, b, allow_tf32=allow_tf32)\n", "        A += BLOCK_K * stride_ak\n\t        B += BLOCK_K * stride_bk\n\t        B_scale += BLOCK_K * stride_bscale_k\n\t    acc = acc.to(C.dtype.element_ty)\n\t    # rematerialize rm and rn to save registers\n\t    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\t    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\t    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n\t    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\t    # handles write-back with reduction-splitting\n", "    tl.store(C, acc, mask=mask)\n\tdef dynamic_quant_matmul_transposed_s4(a: Tensor, b: Tensor, b_scale: Tensor, allow_tf32: bool=None):\n\t    '''\n\t    A:        (M, K)     *float\n\t    B:        (N//2, K)  *uint8\n\t    B_scale:  (G, K)     *float\n\t    C:        (M, N)     *float\n\t    requirements: N // G == GROUP_N, GROUP_N % BLOCK_N == 0, K % BLOCK_K == 0\n\t    '''\n\t    # checks constraints\n", "    output_shape = (*a.shape[:-1], b.shape[0] * 2)\n\t    a = a.flatten(0, -2)\n\t    assert len(b.shape) == 2\n\t    assert len(b_scale.shape) == 2\n\t    assert a.shape[1] == b.shape[1]\n\t    assert b.shape[1] == b_scale.shape[1]\n\t    assert b.dtype == torch.uint8\n\t    assert a.dtype == b_scale.dtype\n\t    assert b.shape[0] % b_scale.shape[0] == 0\n\t    assert a.get_device() >= 0\n", "    assert b.get_device() == a.get_device(), f\"{b.device=}, {a.device=}\"\n\t    assert b_scale.get_device() == a.get_device(), f\"{b_scale.device=}, {a.device=}\"\n\t    # handle non-contiguous inputs if necessary\n\t    if a.stride(0) > 1 and a.stride(1) > 1:\n\t        a = a.contiguous()\n\t    if b.stride(0) > 1 and b.stride(1) > 1:\n\t        b = b.contiguous()\n\t    # allocates output\n\t    M, K = a.shape\n\t    G, _ = b_scale.shape\n", "    N = b.shape[0] * 2\n\t    GROUP_N = N // G\n\t    BLOCK_N = min(64, GROUP_N)\n\t    assert is_power_of_two(K)\n\t    assert is_power_of_two(BLOCK_N)\n\t    assert is_power_of_two(GROUP_N)\n\t    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\t    # launch kernel\n\t    if allow_tf32 is None:\n\t        allow_tf32 = bool(torch.backends.cudnn.allow_tf32)\n", "    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), )\n\t    with torch.cuda.device(a.device):\n\t        _dynamic_quant_matmul_transposed_s4_kernel[grid](\n\t            a, b, b_scale, c, M, N, K,\n\t            a.stride(0), a.stride(1),\n\t            b.stride(0), b.stride(1),\n\t            b_scale.stride(0), b_scale.stride(1),\n\t            c.stride(0), c.stride(1),\n\t            BLOCK_N=BLOCK_N, GROUP_N=GROUP_N,\n\t            allow_tf32=allow_tf32,\n", "        )\n\t        return c.reshape(output_shape)\n"]}
{"filename": "chatglm_q/int4/quantizer.py", "chunked_list": ["import math\n\timport torch\n\tfrom torch import nn, Tensor\n\tfrom . import qlinear\n\tfrom .qlinear import DynamicQuantizeLinear, QEmbedding\n\tmax_q_int4 = 2 ** (4 - 1) - 1\n\tassert 7 == max_q_int4\n\t@torch.no_grad()\n\tdef quantize_int4(x: torch.Tensor, GROUP_K = qlinear.DEFAULT_GROUP_SIZE):\n\t    '''\n", "    inputs: for weight (in_dim, out_dim)\n\t    '''\n\t    assert len(x.shape) == 2\n\t    K, N = x.shape\n\t    assert K % GROUP_K == 0\n\t    G = K // GROUP_K\n\t    x = x.reshape((G, GROUP_K, N))\n\t    w_max, _ = torch.max(x.abs(), dim=1, keepdim=True)\n\t    scale = torch.clamp(w_max / max_q_int4, min=1e-10)\n\t    x = torch.clamp(torch.round(x / scale), -max_q_int4, max_q_int4)\n", "    x = (x + 0x8).to(torch.uint8)\n\t    # pack\n\t    x = x.reshape((K, N))\n\t    x = (x[::2, :] & 0xF) | ((x[1::2, :] & 0xF) << 4)\n\t    return x.contiguous(), scale.reshape((G, N))\n\t@torch.no_grad()\n\tdef clamp_to_quantize_grid(x: Tensor, scale: Tensor) -> Tensor:\n\t    q = torch.clamp(torch.round(x / scale), -max_q_int4, max_q_int4)\n\t    return scale * q\n\t@torch.no_grad()\n", "def quantize_with_scale(x: Tensor, scale: Tensor) -> Tensor:\n\t    assert len(x.shape) == 2\n\t    assert len(scale.shape) == 2\n\t    K, N = x.shape\n\t    G, _ = scale.shape\n\t    assert K % G == 0\n\t    GROUP_K = K // G\n\t    x = x.reshape((G, GROUP_K, N))\n\t    scale = scale.reshape((G, 1, N))\n\t    x = torch.clamp(torch.round(x / scale), -max_q_int4, max_q_int4)\n", "    x = (x + 0x8).to(torch.uint8)\n\t    # pack\n\t    x = x.reshape((K, N))\n\t    x = (x[::2, :] & 0xF) | ((x[1::2, :] & 0xF) << 4)\n\t    return x\n\t@torch.no_grad()\n\tdef get_quant_int4_linear(layer: nn.Linear, group_size=qlinear.DEFAULT_GROUP_SIZE):\n\t    assert isinstance(layer, nn.Linear)\n\t    q_weight, scale = quantize_int4(layer.weight.t(), group_size)\n\t    qlinear = DynamicQuantizeLinear(layer.in_features, layer.out_features, layer.bias is not None, group_size)\n", "    qlinear.apply_weights_(q_weight, scale, layer.bias)\n\t    return qlinear\n\t@torch.no_grad()\n\tdef get_quant_embedding(layer: nn.Embedding, group_size=qlinear.DEFAULT_GROUP_SIZE):\n\t    assert isinstance(layer, nn.Embedding)\n\t    q_weight, scale = quantize_int4(layer.weight, group_size)\n\t    qembedding = QEmbedding(layer.num_embeddings, layer.embedding_dim)\n\t    qembedding.apply_weights_(q_weight, scale)\n\t    return qembedding\n\tclass GPTQLinearQuantizer():\n", "    '''\n\t    GPTQ quantization, see:\n\t        paper: https://arxiv.org/abs/2210.17323\n\t        code: https://github.com/IST-DASLab/gptq\n\t        license: Apache License 2.0 https://github.com/IST-DASLab/gptq/blob/main/LICENSE\n\t    '''\n\t    def __init__(self, layer: nn.Module):\n\t        assert isinstance(layer, nn.Linear)\n\t        self.layer = layer\n\t        self.hook = layer.register_forward_hook(self.forward_hook)\n", "        # output_dim, input_dim\n\t        self.n_rows, self.n_columns = layer.weight.shape\n\t        self.hessian = torch.zeros((self.n_columns, self.n_columns), device=layer.weight.device)\n\t        self.n_samples = 0\n\t        self.debug_input = None\n\t    @torch.no_grad()\n\t    def forward_hook(self, module: nn.Module, inputs: tuple[Tensor], output: Tensor):\n\t        input, = inputs\n\t        if len(input.shape) > 2:\n\t            input = input.flatten(0, -2)\n", "        self.debug_input = input.detach()\n\t        new_samples, d_hidden = input.shape\n\t        assert d_hidden == self.n_columns\n\t        input = input.t()\n\t        self.hessian *= self.n_samples / (self.n_samples + new_samples)\n\t        self.n_samples += new_samples\n\t        input = math.sqrt(2 / self.n_samples) * input.float()\n\t        self.hessian += input.matmul(input.t())\n\t    def remove_hook(self):\n\t        self.hook.remove()\n", "    @torch.no_grad()\n\t    def quantize_weight(self, blocksize=128, groupsize=qlinear.DEFAULT_GROUP_SIZE, percdamp=.01):\n\t        assert self.n_samples > 0\n\t        assert blocksize % groupsize == 0, f\"{blocksize=}, {groupsize=}\"\n\t        assert self.n_columns % groupsize == 0, f\"{self.n_columns=}, {groupsize=}\"\n\t        hessian = self.hessian\n\t        weight = self.layer.weight.clone()\n\t        scales = []\n\t        dead_rows = torch.diag(hessian) == 0\n\t        hessian[dead_rows, dead_rows] = 1\n", "        weight[:, dead_rows] = 0\n\t        quant_losses = torch.zeros_like(weight)\n\t        grid_weight = torch.zeros_like(weight)\n\t        damp = percdamp * torch.mean(torch.diag(hessian))\n\t        diag = torch.arange(self.n_columns, device=weight.device)\n\t        hessian[diag, diag] += damp\n\t        hessian_inv = torch.cholesky_inverse(torch.linalg.cholesky(hessian))\n\t        hessian_inv = torch.linalg.cholesky(hessian_inv, upper=True)\n\t        assert not hessian_inv.isnan().any()\n\t        for i1 in range(0, self.n_columns, blocksize):\n", "            i2 = min(i1 + blocksize, self.n_columns)\n\t            weight_block = weight[:, i1:i2].clone()\n\t            quant_block = torch.zeros_like(weight_block)\n\t            err_block = torch.zeros_like(weight_block)\n\t            losses_block = torch.zeros_like(weight_block)\n\t            h_inv_block = hessian_inv[i1:i2, i1:i2]\n\t            for j in range(i2 - i1):\n\t                w = weight_block[:, j]\n\t                d = h_inv_block[j, j]\n\t                if j % groupsize == 0:\n", "                    _, scale = quantize_int4(weight_block[:, j:j + groupsize].t(), groupsize)\n\t                    assert scale.shape == (1, self.n_rows)\n\t                    scales.append(scale)\n\t                q = clamp_to_quantize_grid(w, scale[0])\n\t                quant_block[:, j] = q\n\t                losses_block[:, j] = (w - q) ** 2 / d ** 2\n\t                err = (w - q) / d\n\t                weight_block[:, j:] -= err.unsqueeze(1).matmul(h_inv_block[j, j:].unsqueeze(0))\n\t                err_block[:, j] = err\n\t            grid_weight[:, i1:i2] = quant_block\n", "            quant_losses[:, i1:i2] = losses_block / 2\n\t            weight[:, i2:] -= err_block.matmul(hessian_inv[i1:i2, i2:])\n\t        debug_output = nn.functional.linear(self.debug_input, self.layer.weight)\n\t        quant_out = nn.functional.linear(self.debug_input, grid_weight)\n\t        debug_loss = torch.mean((quant_out - debug_output) ** 2).item()\n\t        quant_losses = quant_losses.mean().item()\n\t        scale = torch.cat(scales, dim=0)\n\t        return grid_weight, scale, quant_losses, debug_loss\n\t    @torch.no_grad()\n\t    def get_quantized_linear(self, blocksize=128, groupsize=qlinear.DEFAULT_GROUP_SIZE, percdamp=.01, pring_loss=False):\n", "        grid_weight, scale, quant_losses, debug_loss = self.quantize_weight(blocksize, groupsize, percdamp)\n\t        if pring_loss:\n\t            print(f\"{quant_losses=:.8f} {debug_loss=:.8f}\")\n\t        original = self.layer\n\t        modified = DynamicQuantizeLinear(original.in_features, original.out_features, original.bias is not None, groupsize)\n\t        q_weight = quantize_with_scale(grid_weight.t(), scale)\n\t        modified.apply_weights_(q_weight, scale, original.bias)\n\t        return modified\n"]}
{"filename": "tests/test_triton_ops.py", "chunked_list": ["import torch\n\timport unittest\n\tfrom chatglm_q.int8.triton_ops import dynamic_quant_matmul, dynamic_quant_matmul_transposed\n\tfrom chatglm_q.int8.qlinear import dynamic_quant_matmul as auto_grad_dynamic_quant_matmul\n\tclass TritonOpsTests(unittest.TestCase):\n\t    def test_dynamic_quant_matmul(self):\n\t        A = torch.randn((10, 128)).cuda()\n\t        B = torch.randint(-127, 127, (128, 256), dtype=torch.int8).cuda()\n\t        B_scale = torch.randn((256, )).cuda() / 256\n\t        result = dynamic_quant_matmul(A, B, B_scale, allow_tf32=False)\n", "        expected = A @ (B * B_scale)\n\t        self.assertTrue(torch.allclose(result, expected, atol=1e-4, rtol=1e-4))\n\t    def test_dynamic_quant_matmul_transposed(self):\n\t        A = torch.randn((10, 128)).cuda()\n\t        B = torch.randint(-127, 127, (256, 128), dtype=torch.int8).cuda()\n\t        B_scale = torch.randn((128, )).cuda() / 256\n\t        result = dynamic_quant_matmul_transposed(A, B, B_scale, allow_tf32=False)\n\t        expected = A @ (B * B_scale).t()\n\t        self.assertTrue(torch.allclose(result, expected, atol=1e-4, rtol=1e-4))\n\t    def test_auto_grad_dynamic_quant_matmul(self):\n", "        torch.backends.cudnn.allow_tf32 = False\n\t        A = torch.randn((10, 128)).cuda().requires_grad_()\n\t        B = torch.randint(-127, 127, (128, 256), dtype=torch.int8).cuda()\n\t        B_scale = (torch.randn((256, )) / 256).cuda()\n\t        result = auto_grad_dynamic_quant_matmul(A, B, B_scale)\n\t        result.sum().backward()\n\t        grad = A.grad.clone()\n\t        torch.zero_(A.grad)\n\t        result = A @ (B * B_scale)\n\t        result.sum().backward()\n", "        expected = A.grad.clone()\n\t        self.assertTrue(torch.allclose(grad, expected, atol=1e-4, rtol=1e-4))\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/test_tokenizer.py", "chunked_list": ["# %%\n\timport unittest\n\tfrom pathlib import Path\n\tfrom chatglm_q.tokenizer import ChatGLM2Tokenizer\n\tfile_dir = Path(__file__).parent\n\tmodel_path = file_dir / \"../models/chatglm2-6b-safe\"\n\ttokenizer_path = model_path / \"sentencepiece.model\"\n\t# %%\n\tclass ChatGLMTokenizerTests(unittest.TestCase):\n\t    def test_tokenize(self):\n", "        tokenizer = ChatGLM2Tokenizer(tokenizer_path)\n\t        input_ids = tokenizer.encode(\"[Round 0]\\n问：\\t\", \"问题1\")\n\t        self.assertEqual(\n\t            input_ids,\n\t            [64790, 64792, 790, 30951, 517, 30910, 30940, 30996, 13, 54761, 31211, 12, 30910, 31639, 30939, 2],\n\t        )\n\t        self.assertEqual(\n\t            [tokenizer[\"[gMASK]\"], tokenizer[\"<sop>\"], tokenizer['<eop>'], tokenizer['</s>']],\n\t            [64790, 64792, 64793, 2],\n\t        )\n", "    def test_blank_and_tab(self):\n\t        tokenizer = ChatGLM2Tokenizer(tokenizer_path)\n\t        input_ids = tokenizer.encode(\"\\t   \\t     \\n \\n\", add_special_tokens=False)\n\t        self.assertEqual(\n\t            input_ids,\n\t            [30910, 12, 2951, 12, 3729, 13, 30910, 13],\n\t        )\n\t        decoded = tokenizer.decode(input_ids)\n\t        self.assertEqual(\n\t            decoded,\n", "            \"\\t   \\t     \\n \\n\",\n\t        )\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/test_triton_ops_int4.py", "chunked_list": ["import math\n\timport torch\n\timport unittest\n\tfrom chatglm_q.int4.triton_ops import dynamic_quant_matmul_s4, dynamic_quant_matmul_transposed_s4\n\tfrom chatglm_q.int4.quantizer import quantize_int4\n\tfrom chatglm_q.int4.qlinear import unpack_int4, dynamic_quant_matmul as auto_grad_dynamic_quant_matmul\n\tclass TritonOpsTests(unittest.TestCase):\n\t    def test_dynamic_quant_matmul(self):\n\t        a = torch.randn((32, 512))\n\t        b = torch.randn((512, 256)) / math.sqrt(512)\n", "        ab = a @ b\n\t        b_quant, b_scale = quantize_int4(b)\n\t        ab_q = a @ unpack_int4(b_quant, b_scale)\n\t        self.assertLess(((ab - ab_q) ** 2).mean(), 0.1)\n\t        result = dynamic_quant_matmul_s4(a.cuda(), b_quant.cuda(), b_scale.cuda(), allow_tf32=False)\n\t        self.assertTrue(torch.allclose(result.cpu(), ab_q, atol=1e-4, rtol=1e-4))\n\t    def test_dynamic_quant_matmul_transposed(self):\n\t        a = torch.randn((32, 256))\n\t        b = torch.randn((512, 256)) / math.sqrt(512)\n\t        ab = a @ b.t()\n", "        b_quant, b_scale = quantize_int4(b)\n\t        ab_q = a @ unpack_int4(b_quant, b_scale).t()\n\t        self.assertLess(((ab - ab_q) ** 2).mean(), 0.1)\n\t        result = dynamic_quant_matmul_transposed_s4(a.cuda(), b_quant.cuda(), b_scale.cuda(), allow_tf32=False)\n\t        self.assertTrue(torch.allclose(result.cpu(), ab_q, atol=1e-4, rtol=1e-4))\n\t    def test_auto_grad_dynamic_quant_matmul(self):\n\t        torch.backends.cudnn.allow_tf32 = False\n\t        a = torch.randn((32, 512)).cuda().requires_grad_()\n\t        b = (torch.randn((512, 256)) / math.sqrt(512)).cuda()\n\t        b_quant, b_scale = quantize_int4(b)\n", "        result = auto_grad_dynamic_quant_matmul(a, b_quant, b_scale)\n\t        result.sum().backward()\n\t        grad = a.grad.clone()\n\t        torch.zero_(a.grad)\n\t        result = a @ unpack_int4(b_quant, b_scale)\n\t        result.sum().backward()\n\t        expected = a.grad.clone()\n\t        self.assertTrue(torch.allclose(grad, expected, atol=1e-4, rtol=1e-4))\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "examples/web-ui.py", "chunked_list": ["import torch\n\timport streamlit as st\n\tfrom chatglm_q.decoder import ChatGLMDecoder, chat_template\n\t# page state\n\t@st.cache_resource\n\tdef create_model():\n\t    device = torch.device(\"cuda\")\n\t    torch_dtype = torch.float16\n\t    decoder = ChatGLMDecoder.from_pretrained(\"K024/chatglm2-6b-int4g32\", device, torch_dtype)\n\t    # decoder.time_log = True # log generation performance\n", "    return decoder\n\twith st.spinner(\"加载模型中...\"):\n\t    model = create_model()\n\tif \"history\" not in st.session_state:\n\t    st.session_state[\"history\"] = []\n\t# parameters\n\twith st.sidebar:\n\t    st.markdown(\"## 采样参数\")\n\t    max_tokens = st.number_input(\"max_tokens\", min_value=1, max_value=2000, value=800)\n\t    temperature = st.number_input(\"temperature\", min_value=0.1, max_value=4.0, value=1.0)\n", "    top_p = st.number_input(\"top_p\", min_value=0.1, max_value=1.0, value=0.8)\n\t    top_k = st.number_input(\"top_k\", min_value=1, max_value=100, value=50)\n\t    if st.button(\"清空上下文\"):\n\t        st.session_state.history = []\n\t    st.markdown(\"\"\"\n\t    [ChatGLM2](https://huggingface.co/THUDM/chatglm2-6b)\n\t    [chatglm-q](https://github.com/K024/chatglm-q)\n\t    \"\"\")\n\t# main body\n\tst.markdown(\"## ChatGLM2\")\n", "history: list[tuple[str, str]] = st.session_state.history\n\tif len(history) == 0:\n\t    st.caption(\"请在下方输入消息开始会话\")\n\tfor idx, (question, answer) in enumerate(history):\n\t    with st.chat_message(\"user\"):\n\t        st.write(question)\n\t    with st.chat_message(\"assistant\"):\n\t        st.write(answer)\n\tquestion = st.chat_input(\"消息\", key=\"message\")\n\tif question:\n", "    with st.chat_message(\"user\"):\n\t        st.write(question)\n\t    with st.chat_message(\"assistant\"):\n\t        empty = st.empty()\n\t        with st.spinner(\"正在回复中\"):\n\t            prompt = chat_template(history, question)\n\t            for answer in model.generate(\n\t                prompt,\n\t                max_generated_tokens=max_tokens,\n\t                top_k=top_k,\n", "                top_p=top_p,\n\t                temperature=temperature,\n\t            ):\n\t                empty.write(answer)\n\t    st.session_state.history = history + [(question, answer)]\n"]}
{"filename": "examples/convert_weight.py", "chunked_list": ["# %%\n\tfrom huggingface_hub import snapshot_download\n\ttarget_path = \"../models/chatglm2-6b-safe\"\n\tpath_or_repo_id = \"https://huggingface.co/THUDM/chatglm2-6b\"\n\tcache_dir = None\n\ttoken = None\n\tmodel_path = snapshot_download(path_or_repo_id, cache_dir=cache_dir, token=token)\n\t# %%\n\tfrom pathlib import Path\n\tmodel_path = Path(model_path)\n", "target_path = Path(target_path)\n\ttarget_path.mkdir(parents=True)\n\t# %%\n\tname_mapping = {\n\t    'transformer.embedding.word_embeddings.weight': 'word_embedding.weight',\n\t    'transformer.encoder.final_layernorm.weight': 'final_ln.weight',\n\t    'transformer.output_layer.weight': 'lm_head.weight'\n\t}\n\tfor i in range(28):\n\t    name_mapping.update({\n", "        f'transformer.encoder.layers.{i}.input_layernorm.weight': f'layers.{i}.attn_ln.weight',\n\t        f'transformer.encoder.layers.{i}.self_attention.query_key_value.weight': f'layers.{i}.attn.qkv_proj.weight',\n\t        f'transformer.encoder.layers.{i}.self_attention.query_key_value.bias': f'layers.{i}.attn.qkv_proj.bias',\n\t        f'transformer.encoder.layers.{i}.self_attention.dense.weight': f'layers.{i}.attn.o_proj.weight',\n\t        f'transformer.encoder.layers.{i}.post_attention_layernorm.weight': f'layers.{i}.ffn_ln.weight',\n\t        f'transformer.encoder.layers.{i}.mlp.dense_h_to_4h.weight': f'layers.{i}.ffn.w_in.weight',\n\t        f'transformer.encoder.layers.{i}.mlp.dense_4h_to_h.weight': f'layers.{i}.ffn.w_out.weight',\n\t    })\n\t# %%\n\timport json\n", "import shutil\n\timport torch\n\tfrom tqdm.auto import tqdm\n\tfrom collections import OrderedDict\n\tfrom safetensors.torch import save_file\n\tfrom chatglm_q.loader import ChatGLMLoadConfig\n\tindices = json.loads((model_path / \"pytorch_model.bin.index.json\").read_bytes())\n\tbin_files = set(indices[\"weight_map\"].values())\n\tfor bin_file in tqdm(bin_files):\n\t    state_dict = torch.load(model_path / bin_file)\n", "    new_state_dict = OrderedDict()\n\t    for k, v in state_dict.items():\n\t        if k not in name_mapping:\n\t            print(f\"Unused weight '{k}'\")\n\t            continue\n\t        new_state_dict[name_mapping[k]] = v\n\t    save_file(new_state_dict, target_path / bin_file.replace(\".bin\", \".safetensors\"))\n\tconfig = ChatGLMLoadConfig(\n\t    weight_files = [bin_file.replace(\".bin\", \".safetensors\") for bin_file in bin_files],\n\t    torch_dtype=\"bfloat16\",\n", ")\n\tshutil.copy(model_path / \"tokenizer.model\", target_path / config.tokenizer_file)\n\tconfig_path = target_path / \"config.json\"\n\tconfig_path.write_text(config.to_json())\n\t# %%\n"]}
{"filename": "examples/onnx/merge_data.py", "chunked_list": ["# %%\n\tfrom tqdm.auto import tqdm\n\tfrom pathlib import Path\n\texport_path = Path(\"../../models/chatglm2-6b-int8-onnx/chatglm2-6b-int8-opt.onnx\")\n\tassert export_path.exists()\n\texport_path = str(export_path.absolute())\n\t# %%\n\timport onnx\n\timport onnx.external_data_helper\n\tmodel = onnx.load(export_path)\n", "onnx.external_data_helper.convert_model_from_external_data(model)\n\t# %%\n\ttensors = list(onnx.external_data_helper._get_initializer_tensors(model))\n\t# %%\n\tsize_threshold = 128\n\tcurrent_sum = 0\n\tbar = tqdm(tensors)\n\tfor tensor in bar:\n\t  size = onnx.external_data_helper.sys.getsizeof(tensor.raw_data)\n\t  if (tensor.HasField(\"raw_data\") and size >= size_threshold):\n", "    current_sum += size\n\t    file_idx = current_sum // (2 * 1024 ** 3) + 1\n\t    file_name = f\"model_weights_{file_idx}.bin\"\n\t    bar.set_postfix_str(f\"{file_idx=}\")\n\t    onnx.external_data_helper.set_external_data(tensor, file_name)\n\t# %%\n\tsave_path = Path(\"../../models/chatglm2-6b-int8-onnx-merged/chatglm2-6b-int8.onnx\")\n\tsave_path.parent.mkdir()\n\tsave_path = str(save_path.absolute())\n\tonnx.save_model(model, save_path)\n", "# %%\n\timport shutil\n\ttokenizer = \"../../models/chatglm2-6b-int8/sentencepiece.model\"\n\ttarget = \"../../models/chatglm2-6b-int8-onnx-merged/sentencepiece.model\"\n\tshutil.copy(tokenizer, target)\n\tshutil.rmtree(\"../../models/chatglm2-6b-int8-onnx\")\n\tshutil.move(\"../../models/chatglm2-6b-int8-onnx-merged\", \"../../models/chatglm2-6b-int8-onnx\")\n"]}
{"filename": "examples/onnx/export.py", "chunked_list": ["# %%\n\tfrom pathlib import Path\n\tmodel_path = Path(\"../../models/chatglm2-6b-int8/\")\n\texport_path = Path(\"../../models/chatglm2-6b-int8-onnx/chatglm2-6b-int8.onnx\")\n\texport_path.parent.mkdir()\n\texport_path = str(export_path.absolute())\n\t# %%\n\timport torch\n\tfrom chatglm_q import model as modeling\n\tfrom chatglm_q.loader import load_model_and_tokenizer\n", "modeling.ROTARY_VIEW_AS_COMPLEX = False\n\t_, model, tokenizer = load_model_and_tokenizer(model_path, torch_dtype=torch.float32)\n\tinputs = tokenizer(\"[Round 0]\\n\\n问：\", padding=True, return_tensors=\"pt\")\n\t_, _, past_key_values = model(**inputs)\n\t# %%\n\tinputs.input_ids = torch.LongTensor([[tokenizer.pad_id]])\n\tinputs.attention_mask = torch.cat([\n\t    inputs.attention_mask,\n\t    torch.LongTensor([[1]]),\n\t], dim=1)\n", "inputs.position_ids = inputs.position_ids[:, -1:] + 1\n\tinput_args = (\n\t    inputs.input_ids,\n\t    None, # input_embeddings\n\t    inputs.attention_mask,\n\t    inputs.position_ids,\n\t    None, # labels\n\t    past_key_values,\n\t)\n\tinput_names = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n", "output_names = [\"logits\"]\n\tdynamic_axes = { \n\t    \"input_ids\": { 0: \"batch_size\", 1: \"new_seq_length\" },\n\t    \"attention_mask\": { 0: \"batch_size\", 1: \"all_seq_length\" },\n\t    \"position_ids\": { 0: \"batch_size\", 1: \"new_seq_length\" },\n\t}\n\tfor layer_idx in range(model.config.num_layers):\n\t    input_names += [f\"past_key_{layer_idx}\", f\"past_value_{layer_idx}\"]\n\t    output_names += [f\"present_key_{layer_idx}\", f\"present_value_{layer_idx}\"]\n\t    dynamic_axes.update({\n", "        f\"past_key_{layer_idx}\": { 0: \"batch_size\", 1: \"past_seq_length\" },\n\t        f\"past_value_{layer_idx}\": { 0: \"batch_size\", 1: \"past_seq_length\" },\n\t    })\n\t# %%\n\ttorch.onnx.export(\n\t    model,\n\t    f=export_path,\n\t    args=input_args,\n\t    input_names=input_names,\n\t    output_names=output_names,\n", "    dynamic_axes=dynamic_axes,\n\t    opset_version=17,\n\t)\n\t# %%\n\tfrom onnxruntime.tools.optimize_onnx_model import optimize_model\n\toutput_path = \"../../models/chatglm2-6b-int8-onnx/chatglm2-6b-int8-opt.onnx\"\n\toptimize_model(Path(export_path), Path(output_path))\n\t# %%\n"]}
{"filename": "examples/quantize_gptq/mnist.py", "chunked_list": ["# %%\n\timport torch\n\tfrom torch import nn\n\t# PyTorch tutorial/quick start\n\t# torch.save(next(iter(test_data_loader)), \"test_data.pth\")\n\tdata, labels = torch.load(\"test_data.pth\")\n\tclass NeuralNetwork(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.flatten = nn.Flatten()\n", "        self.linear_relu_stack = nn.Sequential(\n\t            nn.Linear(28*28, 512),\n\t            nn.ReLU(),\n\t            nn.Linear(512, 512),\n\t            nn.ReLU(),\n\t            nn.Linear(512, 10)\n\t        )\n\t    def forward(self, x):\n\t        x = self.flatten(x)\n\t        logits = self.linear_relu_stack(x)\n", "        return logits\n\tmodel = NeuralNetwork()\n\tmodel.load_state_dict(torch.load(\"model.pth\"))\n\t# %%\n\tfrom chatglm_q.int8.quantizer import GPTQLinearQuantizer, get_quant_int8_linear\n\tqlayers: dict[str, GPTQLinearQuantizer] = {}\n\tqmodel = NeuralNetwork()\n\tqmodel.load_state_dict(torch.load(\"model.pth\"))\n\tfor name, module in model.named_modules():\n\t    if isinstance(module, nn.Linear):\n", "        qlayers[name] = GPTQLinearQuantizer(module)\n\t# %%\n\tmodel(data) # calibrate with data\n\tfor module in qlayers.values():\n\t   module.remove_hook()\n\t# %%\n\tfor name, module in qlayers.items():\n\t    parent_path, module_name = name.rsplit(\".\")\n\t    parent = qmodel.get_submodule(parent_path)\n\t    # Compare with naive quantization\n", "    # setattr(parent, module_name, get_quant_int8_linear(module.layer))\n\t    setattr(parent, module_name, module.get_quantized_linear(pring_loss=True))\n\t# %%\n\tprint(\"mean error:\", ((qmodel(data) - model(data)) ** 2).mean())\n\tprint(\"different predictions:\", (qmodel(data).argmax(-1) - model(data).argmax(-1)).bool().sum())\n\t# %%\n\ttorch.onnx.export(\n\t   model,\n\t   f=\"model.onnx\",\n\t   args=(data,),\n", "   input_names=[\"input\"],\n\t   output_names=[\"output\"],\n\t   dynamic_axes={ \"input\": { 0: \"batch_size\" } },\n\t)\n\ttorch.onnx.export(\n\t   qmodel,\n\t   f=\"qmodel.onnx\",\n\t   args=(data,),\n\t   input_names=[\"input\"],\n\t   output_names=[\"output\"],\n", "   dynamic_axes={ \"input\": { 0: \"batch_size\" } },\n\t)\n\t# %%\n\timport torch\n\timport onnxruntime\n\tdata, labels = torch.load(\"test_data.pth\")\n\tdata = data.numpy()\n\tqmodel = onnxruntime.InferenceSession(\"qmodel.onnx\", providers=[\"CPUExecutionProvider\"])\n\tmodel = onnxruntime.InferenceSession(\"model.onnx\", providers=[\"CPUExecutionProvider\"])\n\t# %%\n", "q_out, = qmodel.run([\"output\"], { \"input\": data })\n\tout, = model.run([\"output\"], { \"input\": data })\n\t# %%\n\tprint(\"mean error:\", ((q_out - out) ** 2).mean())\n\tprint(\"different predictions:\", (q_out.argmax(-1) - out.argmax(-1)).astype(bool).sum())\n\t# %%\n"]}
{"filename": "examples/quantize_gptq/int8.py", "chunked_list": ["# %%\n\timport json\n\timport torch\n\tfrom pathlib import Path\n\tfrom chatglm_q.loader import ChatGLMLoadConfig, load_model_and_tokenizer, save_model_and_tokenizer\n\ttorch.manual_seed(42)\n\t_, model, tokenizer = load_model_and_tokenizer(\"../../models/chatglm2-6b-safe\", torch.float32)\n\t# CEval data from https://github.com/THUDM/ChatGLM2-6B/tree/main/evaluation\n\tall_data = [\n\t    json.loads(line)\n", "    for file in Path(\"../../data/CEval/val\").rglob(\"*.jsonl\")\n\t    for line in file.read_text().splitlines()\n\t    if len(line)\n\t]\n\tbatch_size = 20\n\tcalibrate_data_size = 200\n\tcalibrate_data_idx = torch.randperm(len(all_data))[:calibrate_data_size] \\\n\t    .view(-1, batch_size).tolist()\n\tdata = [\n\t    tokenizer([\n", "        f\"问：{all_data[idx]['inputs_pretokenized']}\\n\\n\"\n\t        f\"答：{all_data[idx]['targets_pretokenized'][0]}\"\n\t        for idx in batch\n\t    ], padding=True, return_tensors=\"pt\")\n\t    for batch in calibrate_data_idx\n\t]\n\t# %%\n\tfrom torch import nn\n\tfrom chatglm_q.int8.quantizer import get_quant_embedding, GPTQLinearQuantizer\n\tdevice = torch.device(\"cuda\")\n", "# later move to device layer by layer\n\t# model.to(device)\n\tmodel.word_embedding = get_quant_embedding(model.word_embedding)\n\t# %%\n\tnum_layers = model.config.num_layers\n\twith torch.no_grad():\n\t    prepared_input = [\n\t        model.prepare_input(**batch)\n\t        for batch in data\n\t    ]\n", "    current_h = [batch[0] for batch in prepared_input]\n\t# %%\n\tfrom tqdm.auto import tqdm\n\tfor layer_idx in tqdm(range(num_layers)):\n\t    layer = model.layers[layer_idx]\n\t    layer.to(device)\n\t    qlayers: dict[str, GPTQLinearQuantizer] = {}\n\t    for name, module in layer.named_modules():\n\t        if isinstance(module, nn.Linear):\n\t            qlayers[name] = GPTQLinearQuantizer(module)\n", "    next_h = tuple()\n\t    for h, (_, mask, pe) in zip(current_h, prepared_input):\n\t        with torch.no_grad():\n\t            h, _ = layer(\n\t                x=h.to(device),\n\t                attention_mask=mask.to(device),\n\t                freqs_cis=pe.to(device),\n\t                kv_cache=None,\n\t            )\n\t        next_h += (h,)\n", "    current_h = next_h\n\t    for name, module in qlayers.items():\n\t        module.remove_hook()\n\t        parent_path, module_name = name.rsplit(\".\", 1)\n\t        parent = layer.get_submodule(parent_path)\n\t        setattr(parent, module_name, module.get_quantized_linear(pring_loss=True))\n\t    model.to(\"cpu\")\n\t    layer.to(\"cpu\")\n\tdel qlayers\n\t# %%\n", "model.final_ln.to(device)\n\tmodel.lm_head.to(device)\n\tlm_head_q = GPTQLinearQuantizer(model.lm_head)\n\twith torch.no_grad():\n\t    for h in current_h:\n\t        model.lm_head(model.final_ln(h))\n\tlm_head_q.remove_hook()\n\tsetattr(model, \"lm_head\", lm_head_q.get_quantized_linear(pring_loss=True))\n\tmodel.to(\"cpu\")\n\tdel lm_head_q\n", "del current_h\n\t# %%\n\t# set torch_dtype (activation type) as needed\n\tconfig = ChatGLMLoadConfig(model_config=model.config, quant_type=\"int8\", torch_dtype=\"float16\")\n\tsave_model_and_tokenizer(\"../../models/chatglm2-6b-int8\", config, model, tokenizer)\n\t# %%\n"]}
{"filename": "examples/quantize_gptq/int4g32.py", "chunked_list": ["# %%\n\timport json\n\timport torch\n\tfrom pathlib import Path\n\tfrom chatglm_q.loader import ChatGLMLoadConfig, load_model_and_tokenizer, save_model_and_tokenizer\n\ttorch.manual_seed(42)\n\t_, model, tokenizer = load_model_and_tokenizer(\"../../models/chatglm2-6b-safe\", torch.float32)\n\t# CEval data from https://github.com/THUDM/ChatGLM2-6B/tree/main/evaluation\n\tall_data = [\n\t    json.loads(line)\n", "    for file in Path(\"../../data/CEval/val\").rglob(\"*.jsonl\")\n\t    for line in file.read_text().splitlines()\n\t    if len(line)\n\t]\n\tbatch_size = 20\n\tcalibrate_data_size = 200\n\tcalibrate_data_idx = torch.randperm(len(all_data))[:calibrate_data_size] \\\n\t    .view(-1, batch_size).tolist()\n\tdata = [\n\t    tokenizer([\n", "        f\"问：{all_data[idx]['inputs_pretokenized']}\\n\\n\"\n\t        f\"答：{all_data[idx]['targets_pretokenized'][0]}\"\n\t        for idx in batch\n\t    ], padding=True, return_tensors=\"pt\")\n\t    for batch in calibrate_data_idx\n\t]\n\t# %%\n\tfrom torch import nn\n\tfrom chatglm_q.int4.quantizer import get_quant_embedding, GPTQLinearQuantizer\n\tdevice = torch.device(\"cuda\")\n", "# later move to device layer by layer\n\t# model.to(device)\n\tmodel.word_embedding = get_quant_embedding(model.word_embedding)\n\t# %%\n\tnum_layers = model.config.num_layers\n\twith torch.no_grad():\n\t    prepared_input = [\n\t        model.prepare_input(**batch)\n\t        for batch in data\n\t    ]\n", "    current_h = [batch[0] for batch in prepared_input]\n\t# %%\n\tfrom tqdm.auto import tqdm\n\tfor layer_idx in tqdm(range(num_layers)):\n\t    layer = model.layers[layer_idx]\n\t    layer.to(device)\n\t    qlayers: dict[str, GPTQLinearQuantizer] = {}\n\t    for name, module in layer.named_modules():\n\t        if isinstance(module, nn.Linear):\n\t            qlayers[name] = GPTQLinearQuantizer(module)\n", "    next_h = tuple()\n\t    for h, (_, mask, pe) in zip(current_h, prepared_input):\n\t        with torch.no_grad():\n\t            h, _ = layer(\n\t                x=h.to(device),\n\t                attention_mask=mask.to(device),\n\t                freqs_cis=pe.to(device),\n\t                kv_cache=None,\n\t            )\n\t        next_h += (h,)\n", "    current_h = next_h\n\t    for name, module in qlayers.items():\n\t        module.remove_hook()\n\t        parent_path, module_name = name.rsplit(\".\", 1)\n\t        parent = layer.get_submodule(parent_path)\n\t        setattr(parent, module_name, module.get_quantized_linear(pring_loss=True))\n\t    model.to(\"cpu\")\n\t    layer.to(\"cpu\")\n\tdel qlayers\n\t# %%\n", "model.final_ln.to(device)\n\tmodel.lm_head.to(device)\n\tlm_head_q = GPTQLinearQuantizer(model.lm_head)\n\twith torch.no_grad():\n\t    for h in current_h:\n\t        model.lm_head(model.final_ln(h))\n\tlm_head_q.remove_hook()\n\tsetattr(model, \"lm_head\", lm_head_q.get_quantized_linear(pring_loss=True))\n\tmodel.to(\"cpu\")\n\tdel lm_head_q\n", "del current_h\n\t# %%\n\t# set torch_dtype (activation type) as needed\n\tconfig = ChatGLMLoadConfig(model_config=model.config, quant_type=\"int4g32\", torch_dtype=\"float16\")\n\tsave_model_and_tokenizer(\"../../models/chatglm2-6b-int4g32\", config, model, tokenizer)\n\t# %%\n"]}
{"filename": "examples/quantize_naive/int8.py", "chunked_list": ["# %%\n\timport torch\n\tfrom torch import nn\n\tfrom tqdm.auto import tqdm\n\tfrom chatglm_q.loader import ChatGLMLoadConfig, load_model_and_tokenizer, save_model_and_tokenizer\n\t_, model, tokenizer = load_model_and_tokenizer(\"../../models/chatglm2-6b-safe\", torch.float32)\n\t# %%\n\tfrom chatglm_q.int8.quantizer import get_quant_int8_linear, get_quant_embedding\n\tmodel.word_embedding = get_quant_embedding(model.word_embedding)\n\t# %%\n", "linear_layers: dict[str, nn.Linear] = {}\n\tfor name, module in model.named_modules():\n\t    if isinstance(module, nn.Linear): # and \"lm_head\" not in name:\n\t        linear_layers[name] = module\n\tfor name, module in tqdm(linear_layers.items()):\n\t    if \".\" in name:\n\t        parent_path, module_name = name.rsplit(\".\", 1)\n\t        parent = model.get_submodule(parent_path)\n\t    else:\n\t        module_name = name\n", "        parent = model\n\t    module = get_quant_int8_linear(module)\n\t    setattr(parent, module_name, module)\n\t# %%\n\t# set torch_dtype (activation type) as needed\n\tconfig = ChatGLMLoadConfig(model_config=model.config, quant_type=\"int8\", torch_dtype=\"float16\")\n\tsave_model_and_tokenizer(\"../../models/chatglm2-6b-int8-naive\", config, model, tokenizer)\n\t# %%\n"]}
{"filename": "examples/quantize_naive/int4g32.py", "chunked_list": ["# %%\n\timport torch\n\tfrom torch import nn\n\tfrom tqdm.auto import tqdm\n\tfrom chatglm_q.loader import ChatGLMLoadConfig, load_model_and_tokenizer, save_model_and_tokenizer\n\t_, model, tokenizer = load_model_and_tokenizer(\"../../models/chatglm2-6b-safe\", torch.float32)\n\t# %%\n\tfrom chatglm_q.int4.quantizer import get_quant_int4_linear, get_quant_embedding\n\tmodel.word_embedding = get_quant_embedding(model.word_embedding)\n\t# %%\n", "linear_layers: dict[str, nn.Linear] = {}\n\tfor name, module in model.named_modules():\n\t    if isinstance(module, nn.Linear): # and \"lm_head\" not in name:\n\t        linear_layers[name] = module\n\tfor name, module in tqdm(linear_layers.items()):\n\t    if \".\" in name:\n\t        parent_path, module_name = name.rsplit(\".\", 1)\n\t        parent = model.get_submodule(parent_path)\n\t    else:\n\t        module_name = name\n", "        parent = model\n\t    module = get_quant_int4_linear(module)\n\t    setattr(parent, module_name, module)\n\t# %%\n\t# set torch_dtype (activation type) as needed\n\tconfig = ChatGLMLoadConfig(model_config=model.config, quant_type=\"int4g32\", torch_dtype=\"float16\")\n\tsave_model_and_tokenizer(\"../../models/chatglm2-6b-int4g32-naive\", config, model, tokenizer)\n\t# %%\n"]}
{"filename": "examples/evaluations/ppl.py", "chunked_list": ["# %%\n\timport json\n\tfrom pathlib import Path\n\t# CEval data from https://github.com/THUDM/ChatGLM2-6B/tree/main/evaluation\n\tall_data = [\n\t    json.loads(line)[\"inputs_pretokenized\"]\n\t    for file in Path(\"../../data/CEval/val\").rglob(\"*.jsonl\")\n\t    for line in file.read_text().splitlines()\n\t    if len(line)\n\t]\n", "batch_size = 20\n\tall_data = [\n\t    all_data[idx:idx + batch_size]\n\t    for idx in range(0, len(all_data), batch_size)\n\t]\n\t# %%\n\timport torch\n\tfrom chatglm_q.loader import load_model_and_tokenizer\n\tdevice = torch.device(\"cuda\")\n\ttorch_dtype = torch.float16\n", "_, model, tokenizer = load_model_and_tokenizer(\"../../models/chatglm2-6b-safe\", torch_dtype)\n\tmodel = model.to(device)\n\t# %%\n\tfrom tqdm.auto import tqdm\n\tlosses = []\n\tprogress_bar = tqdm(all_data)\n\tfor texts in progress_bar:\n\t    inputs = tokenizer(texts, padding=True, return_tensors=\"pt\", return_labels=True)\n\t    with torch.no_grad():\n\t        loss, _, _ = model(**inputs.to(device))\n", "        losses.append(loss.item())\n\t# %%\n\timport math\n\tavg = sum(losses) / len(losses)\n\tprint(f\"ppl: {math.exp(avg):.6f}\")\n\t# %%\n"]}
{"filename": "examples/evaluations/ceval.py", "chunked_list": ["# %%\n\timport json\n\tfrom pathlib import Path\n\t# CEval data from https://github.com/THUDM/ChatGLM2-6B/tree/main/evaluation\n\tall_data = [\n\t    (file.parent.name, file.stem, json.loads(line))\n\t    for file in Path(\"../../data/CEval/val\").rglob(\"*.jsonl\")\n\t    for line in file.read_text().splitlines()\n\t    if len(line)\n\t]\n", "# %%\n\timport torch\n\tfrom chatglm_q.decoder import ChatGLMDecoder\n\tactivation = torch.float16\n\tdevice = torch.device(\"cuda\")\n\tdecoder = ChatGLMDecoder.from_pretrained(\"../../models/chatglm2-6b-safe\", device, torch_dtype=activation)\n\t# %%\n\tchoice_tokens = [decoder.tokenizer[choice] for choice in \"ABCD\"]\n\tthink_template = \"[Round 1]\\n\\n问：{}\\n\\n答：\"\n\tfinal_template = \"[Round 1]\\n\\n问：{}\\n\\n答：{}\\n综上所述，正确的选项是：\"\n", "direct_template = \"[Round 1]\\n\\n问：{}\\n\\n答：正确的选项是：\"\n\tchain_of_thoughts = False\n\t# %%\n\tfrom tqdm.auto import tqdm\n\ttotal = 0\n\tcorrects = 0\n\tevaluations = []\n\tprogress_bar = tqdm(all_data)\n\tfor category, test_name, data in progress_bar:\n\t    question = data[\"inputs_pretokenized\"]\n", "    if chain_of_thoughts:\n\t        thoughts = list(decoder.generate(\n\t            think_template.format(question),\n\t            temperature=0.5\n\t        ))[-1]\n\t        prompt = final_template.format(question, thoughts),\n\t    else:\n\t        prompt = direct_template.format(question)\n\t    with torch.no_grad():\n\t      _, model_output, _ = decoder.model(\n", "          **decoder.tokenizer(\n\t              prompt,\n\t              padding=True,\n\t              return_tensors=\"pt\",\n\t          ).to(device),\n\t      )\n\t      model_choices = model_output[0, -1, choice_tokens]\n\t      model_predict = torch.argmax(model_choices).item()\n\t      correct = int(model_predict == data['label'])\n\t    evaluations.append((category, correct))\n", "    total += 1\n\t    corrects += correct\n\t    progress_bar.set_postfix_str(f\"{category} {corrects}/{total} {corrects/total:.2%}\")\n\t# %%\n\tprint(f\"{'total': <16}: {corrects}/{total} {corrects/total:.2%}\")\n\tprint(f\"-------\")\n\tcategories = { cat: [] for cat in sorted(set(data[0] for data in evaluations)) }\n\tfor cat, correct in evaluations:\n\t    categories[cat].append(int(correct))\n\tfor cat_name, cat_list in categories.items():\n", "    t = len(cat_list)\n\t    c = sum(cat_list)\n\t    print(f\"{cat_name: <16}: {c}/{t} {c/t:.2%}\")\n\t# %%\n"]}
