{"filename": "run.py", "chunked_list": ["import clrs\n\timport os\n\timport torch\n\timport typer\n\tfrom config.hyperparameters import HP_SPACE\n\tfrom functools import partial\n\tfrom nn.models import EncodeProcessDecode, MF_Net, MF_NetPipeline\n\tfrom pathlib import Path\n\tfrom statistics import mean, stdev\n\tfrom utils.data import load_dataset\n", "from utils.experiments import evaluate\n\tfrom utils.types import Algorithm\n\tfrom norm import set_seed\n\tfrom norm.experiments import Experiment, init_runs, run_exp\n\tfrom norm.io import dump, load\n\tapp = typer.Typer(add_completion=False)\n\tdef choose_default_type(name: str):\n\t    assert name in ['float16', 'float32']\n\t    if name == 'float16':\n\t        return torch.HalfTensor\n", "    return torch.FloatTensor\n\tdef choose_model(name: str):\n\t    assert name in [\"epd\", \"mf_net\", \"mf_net_pipe\", \"mf_net_res\"]\n\t    if name == \"epd\":\n\t        model_class = EncodeProcessDecode\n\t    elif name == \"mf_net\":\n\t        model_class = MF_Net\n\t    else:\n\t        model_class = MF_NetPipeline\n\t    return model_class\n", "def choose_hint_mode(mode: str):\n\t    assert mode in [\"io\", \"o\", \"none\"]\n\t    if mode == \"io\":\n\t        encode_hints, decode_hints = True, True\n\t    elif mode == \"o\":\n\t        encode_hints, decode_hints = False, True\n\t    elif mode == \"none\":\n\t        encode_hints, decode_hints = False, False\n\t    return encode_hints, decode_hints\n\tdef split_probes(feedback):\n", "    _, source, tail, weights, adj = feedback.features.inputs\n\t    return (\n\t        source.data.squeeze().numpy().argmax(-1),\n\t        tail.data.squeeze().numpy().argmax(-1),\n\t        weights.data.squeeze().numpy(),\n\t        adj.data.squeeze().numpy()\n\t    )\n\tdef _preprocess_yaml(config):\n\t    assert 'algorithm' in config.keys()\n\t    assert 'runs' in config.keys()\n", "    assert 'experiment' in config.keys()\n\t    for key in config['runs'].keys():\n\t        if key == 'hp_space':\n\t            config['runs'][key] = HP_SPACE[config['runs'][key]]\n\t            continue\n\t    return config\n\t@app.command()\n\tdef valid(exp_path: Path,\n\t          data_path: Path,\n\t          model: str = \"epd\",\n", "          hint_mode: str = \"io\",\n\t          max_steps: int = None,\n\t          num_cpus: int = None,\n\t          num_gpus: int = 1,\n\t          nw: int = 5,\n\t          no_feats: str = 'adj',\n\t          noise: bool = False,\n\t          processor: str = 'pgn',\n\t          aggregator: str = 'max',\n\t          save_path: Path = './runs',\n", "          dtype: str = 'float32',\n\t          num_test_trials: int = 5,\n\t          seed: int = None,):\n\t    torch.set_default_tensor_type(choose_default_type(dtype))\n\t    assert aggregator in ['max', 'sum', 'mean']\n\t    assert processor in ['mpnn', 'pgn']\n\t    if seed is None:\n\t        seed = int.from_bytes(os.urandom(2), byteorder=\"big\")\n\t    encode_hints, decode_hints = choose_hint_mode(hint_mode)\n\t    model_class = choose_model(model)\n", "    configs = _preprocess_yaml(load(exp_path))\n\t    alg = configs['algorithm']\n\t    set_seed(seed)\n\t    print(\"loading val...\")\n\t    vl_sampler, _ = load_dataset('val', alg, folder=data_path)\n\t    print(\"loading test...\")\n\t    ts_sampler, _ = load_dataset('test', alg, folder=data_path)\n\t    print(\"loading tr...\")\n\t    tr_sampler, spec = load_dataset('train', alg, folder=data_path)\n\t    print(\"loading done\")\n", "    model_fn = partial(model_class,\n\t                       spec=spec,\n\t                       dummy_trajectory=tr_sampler.next(1),\n\t                       decode_hints=decode_hints,\n\t                       encode_hints=encode_hints,\n\t                       add_noise=noise,\n\t                       no_feats=no_feats.split(','),\n\t                       max_steps=max_steps,\n\t                       processor=processor,\n\t                       aggregator=aggregator)\n", "    runs = init_runs(seed=seed,\n\t                     model_fn=model_fn,\n\t                     optim_fn=torch.optim.SGD,\n\t                     **configs['runs'])\n\t    experiment = Experiment(runs=runs,\n\t                            evaluate_fn=evaluate,\n\t                            save_path=save_path,\n\t                            num_cpus=num_cpus if num_cpus else num_gpus * nw,\n\t                            num_gpus=num_gpus,\n\t                            nw=nw,\n", "                            num_test_trials=num_test_trials,\n\t                            **configs['experiment'])\n\t    dump(dict(\n\t        alg=alg,\n\t        data_path=str(data_path),\n\t        hint_mode=hint_mode,\n\t        model=model,\n\t        aggregator=aggregator,\n\t        processor=processor,\n\t        no_feats=no_feats.split(','),\n", "        seed=seed,\n\t    ), save_path / experiment.name / 'config.json')\n\t    print(f\"Experiment name: {experiment.name}\")\n\t    run_exp(experiment=experiment,\n\t            tr_set=tr_sampler,\n\t            vl_set=vl_sampler,\n\t            ts_set=ts_sampler,\n\t            save_path=save_path)\n\t@app.command()\n\tdef test(alg: Algorithm,\n", "         test_path: Path,\n\t         data_path: Path,\n\t         max_steps: int = None,\n\t         test_set: str = 'test',):\n\t    from utils.metrics import eval_categorical, masked_mae\n\t    ts_sampler, spec = load_dataset(test_set, alg.value, folder=data_path)\n\t    best_run = load(test_path / 'best_run.json')['config']\n\t    config = load(test_path / 'config.json')\n\t    hint_mode = config['hint_mode']\n\t    encode_hints, decode_hints = choose_hint_mode(hint_mode)\n", "    model_class = choose_model(config['model'])\n\t    feedback = ts_sampler.next()\n\t    runs = []\n\t    adj = feedback.features.inputs[-2].data.numpy()\n\t    def predict(features, outputs, i):\n\t        model = model_class(spec=spec,\n\t                            dummy_trajectory=ts_sampler.next(1),\n\t                            num_hidden=best_run['num_hidden'],\n\t                            alpha=best_run['alpha'],\n\t                            aggregator=config['aggregator'],\n", "                            processor=config['processor'],\n\t                            max_steps=max_steps,\n\t                            no_feats=config['no_feats'],\n\t                            decode_hints=decode_hints,\n\t                            encode_hints=encode_hints,\n\t                            optim_fn=torch.optim.Adam)\n\t        model.restore_model(test_path / f'trial_{i}' / 'model_0.pth', 'cuda')\n\t        preds, aux = model.predict(features)\n\t        for key in preds:\n\t            preds[key].data = preds[key].data.cpu()\n", "        metrics = {}\n\t        for truth in feedback.outputs:\n\t            type_ = preds[truth.name].type_\n\t            y_pred = preds[truth.name].data.numpy()\n\t            y_true = truth.data.numpy()\n\t            if type_ == clrs.Type.SCALAR:\n\t                metrics[truth.name] = masked_mae(y_pred, y_true * adj).item()\n\t            elif type_ == clrs.Type.CATEGORICAL:\n\t                metrics[truth.name] = eval_categorical(y_pred, y_true).item()\n\t        dump(preds, test_path / f'trial_{i}' / f'preds_{i}.{test_set}.pkl')\n", "        dump(model.net_.flow_net.h_t.cpu(), test_path / f'trial_{i}' / f'H{i}.{test_set}.pth')\n\t        dump(model.net_.flow_net.edge_attr.cpu(), test_path / f'trial_{i}' / f'E{i}.{test_set}.pth')\n\t        return metrics\n\t    for i in range(5):\n\t        if not (test_path / f'trial_{i}' / 'model_0.pth').exists():\n\t            continue\n\t        runs.append(predict(feedback.features, feedback.outputs, i))\n\t        torch.cuda.empty_cache()\n\t    dump(runs, test_path / f'scores.{test_set}.json')\n\t    for key in runs[0]:\n", "        out = [evals[key] for evals in runs]\n\t        print(key, mean(out), \"pm\", stdev(out) if len(out) > 1 else 0)\n\tif __name__ == '__main__':\n\t    app()\n"]}
{"filename": "build_data.py", "chunked_list": ["import numpy as np\n\timport typer\n\timport os\n\tfrom config.data import DATA_SETTINGS\n\tfrom functools import partial\n\tfrom math import log\n\tfrom norm.io import dump\n\tfrom numpy.random import default_rng\n\tfrom numpy.typing import NDArray\n\tfrom pathlib import Path\n", "from utils.data import algorithms\n\tfrom utils.data.graphs import erdos_renyi_full, two_community, bipartite\n\tfrom utils.types import Algorithm\n\tdef bfs_init(adj, rng, **kwargs):\n\t    num_nodes = adj.shape[0]\n\t    source = rng.choice(num_nodes)\n\t    return adj, source\n\tdef ford_fulkerson_init(adj, rng, **kwargs):\n\t    num_nodes = adj.shape[0]\n\t    if kwargs['random_st']:\n", "        source = rng.choice(num_nodes // 2)\n\t        target = rng.choice(range(num_nodes // 2 + 1, num_nodes))\n\t        if source == target:\n\t            target = (source + 1) % num_nodes\n\t    else:\n\t        source, target = 0, num_nodes - 1\n\t    if kwargs['capacity']:\n\t        high = 10\n\t        capacity: NDArray = rng.integers(low=1, high=high, size=(num_nodes, num_nodes)) / high\n\t    else:\n", "        capacity: NDArray = np.ones((num_nodes, num_nodes))\n\t    capacity = np.maximum(capacity, capacity.T) * adj\n\t    capacity = capacity * np.abs((np.eye(num_nodes) - 1))\n\t    return capacity, source, target\n\t_INITS = {\n\t    Algorithm.ff: ford_fulkerson_init,\n\t    Algorithm.ffmc: ford_fulkerson_init\n\t}\n\t_GRAPH_DISTRIB = {\n\t    'two_community': two_community,\n", "    'erdos_renyi': erdos_renyi_full,\n\t    'bipartite': bipartite\n\t}\n\tdef accum_samples(alg, params, data):\n\t    algorithm_fn = getattr(algorithms, alg)\n\t    if alg == Algorithm.ek_bfs:\n\t        for _, probes in algorithm_fn(*params):\n\t            data.append(probes)\n\t    else:\n\t        _, probes = algorithm_fn(*params)\n", "        data.append(probes)\n\t    return data\n\tdef main(alg: Algorithm,\n\t         dataset_name: str = 'default',\n\t         graph_density: float = 0.35,\n\t         outer_prob: float = 0.05,\n\t         save_path: Path = './data/clrs',\n\t         graph_distrib: str = 'two_community',\n\t         weighted: bool = False,\n\t         directed: bool = False,\n", "         seed: int = None):\n\t    if seed is None:\n\t        seed = int.from_bytes(os.urandom(2), byteorder=\"big\")\n\t    assert graph_distrib in ['two_community', 'erdos_renyi', 'bipartite']\n\t    distrib = _GRAPH_DISTRIB[graph_distrib]\n\t    rng = default_rng(seed)\n\t    init_fn = _INITS[alg]\n\t    save_path = save_path / alg.value / dataset_name\n\t    probs = {}\n\t    graphs = {}\n", "    extras = {}\n\t    # First sample graphs (aids reproducibility).\n\t    for split in DATA_SETTINGS.keys():\n\t        num_nodes = DATA_SETTINGS[split]['length']\n\t        probs[split] = max(graph_density, 1.25*log(num_nodes)/num_nodes)\n\t        distrib = partial(distrib, outer_prob=outer_prob)\n\t        graphs[split] = []\n\t        for _ in range(DATA_SETTINGS[split]['num_samples']):\n\t            adj = distrib(num_nodes=num_nodes,\n\t                          prob=probs[split] if graph_distrib != 'bipartite' else rng.uniform(low=graph_density, high=1),\n", "                          directed=directed,\n\t                          weighted=weighted,\n\t                          rng=rng)\n\t            graphs[split].append(adj)\n\t    # Then run the algorithm for each of them.\n\t    for split in DATA_SETTINGS.keys():\n\t        extras[split] = dict()\n\t        data = []\n\t        num_nodes = DATA_SETTINGS[split]['length']\n\t        with typer.progressbar(range(DATA_SETTINGS[split]['num_samples']), label=split) as progress:\n", "            for i in progress:\n\t                params = init_fn(graphs[split][i],\n\t                                 rng,\n\t                                 random_st=graph_distrib != 'bipartite',\n\t                                 capacity=graph_distrib != 'bipartite')\n\t                data = accum_samples(alg, params, data)\n\t        key = list(data[0]['hint']['node'].keys())[0]\n\t        avg_length = []\n\t        for d in data:\n\t            avg_length.append(d['hint']['node'][key]['data'].shape[0])\n", "        # print statistics\n\t        extras[split]['max'] = max(avg_length)\n\t        extras[split]['avg'] = sum(avg_length) / len(avg_length)\n\t        print(\"[avg] traj len:\", extras[split]['avg'])\n\t        print(\"[max] traj len:\", extras[split]['max'])\n\t        dump(data, save_path / f'{split}_{alg}.pkl')\n\t    dump(dict(seed=seed,\n\t              graph_density=probs,\n\t              **extras),\n\t         save_path / 'config.json')\n", "if __name__ == '__main__':\n\t    typer.run(main)\n"]}
{"filename": "test/test_mpnn.py", "chunked_list": ["import os, sys; sys.path.insert(0, os.getcwd())  # noqa: E401, E702\n\tfrom nn.layers import MpnnConv\n\timport os.path as osp\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch_geometric.datasets import Planetoid\n\timport torch_geometric.transforms as T\n\tfrom torch_geometric.utils import to_dense_adj\n\tfrom torch.nn import Sequential, Linear\n\tdef set_seed(seed):\n", "    import torch\n\t    import random\n\t    import numpy as np\n\t    torch.manual_seed(seed)\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\tset_seed(0)\n\tdataset = 'Cora'\n\ttransform = T.Compose([\n\t    T.RandomNodeSplit(num_val=500, num_test=500),\n", "    T.TargetIndegree(),\n\t])\n\tpath = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', dataset)\n\tdataset = Planetoid(path, dataset, transform=transform)\n\tdata = dataset[0]\n\tnum_nodes, num_node_features = data.x.shape\n\tdata.edge_attr = to_dense_adj(data.edge_index, edge_attr=data.edge_attr, max_num_nodes=num_nodes)\n\tdata.edge_index = to_dense_adj(data.edge_index, max_num_nodes=num_nodes)\n\tdata.x = data.x.unsqueeze(0)\n\tclass Net(torch.nn.Module):\n", "    def __init__(self):\n\t        super().__init__()\n\t        self.conv1 = MpnnConv(in_channels=dataset.num_features,\n\t                              edge_channels=data.num_edge_features,\n\t                              mid_channels=16,\n\t                              out_channels=16,\n\t                              net=Sequential(Linear(16, 16)),\n\t                              mid_activation=F.relu,\n\t                              aggregator='max',\n\t                              activation=F.elu)\n", "        self.conv2 = MpnnConv(in_channels=16,\n\t                              edge_channels=data.num_edge_features,\n\t                              mid_channels=16,\n\t                              out_channels=dataset.num_classes,\n\t                              net=Sequential(Linear(16, 16)),\n\t                              mid_activation=F.relu,\n\t                              aggregator='max',\n\t                              activation=None)\n\t    def forward(self):\n\t        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n", "        x = F.dropout(x, training=self.training)\n\t        x = F.elu(self.conv1(x, edge_index, edge_attr))\n\t        x = F.dropout(x, training=self.training)\n\t        x = self.conv2(x, edge_index, edge_attr)\n\t        x = x.squeeze()\n\t        return F.log_softmax(x, dim=1)\n\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\tmodel, data = Net().to(device), data.to(device)\n\toptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n\tdef train():\n", "    model.train()\n\t    optimizer.zero_grad()\n\t    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n\t    optimizer.step()\n\t@torch.no_grad()\n\tdef test():\n\t    model.eval()\n\t    log_probs, accs = model(), []\n\t    for _, mask in data('train_mask', 'test_mask'):\n\t        pred = log_probs[mask].max(1)[1]\n", "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n\t        accs.append(acc)\n\t    return accs\n\tfor epoch in range(200):\n\t    train()\n\t    train_acc, test_acc = test()\n\t    print(f'Epoch: {epoch+1:03d}, Train: {train_acc:.4f}, Test: {test_acc:.4f}')\n"]}
{"filename": "utils/types.py", "chunked_list": ["from enum import Enum\n\tclass Algorithm(str, Enum):\n\t    bfs = 'bfs'\n\t    ek_bfs = 'ek_bfs'\n\t    dijkstra = 'dijkstra'\n\t    astar = 'a_star'\n\t    ff = 'ford_fulkerson'\n\t    ffmc = 'ford_fulkerson_mincut'\n\tclass SearchType(str, Enum):\n\t    large = 'large'\n", "    small = 'small'\n\t    one = 'one'\n\tclass ModelType(str, Enum):\n\t    dual_edp = 'dual_edp'\n\t    uniform = 'uniform'\n\t    normal = 'normal'\n"]}
{"filename": "utils/__init__.py", "chunked_list": ["def set_seed(seed):\n\t    import numpy as np\n\t    import torch\n\t    import random\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n\tdef get_date():\n\t    from datetime import datetime\n", "    return datetime.utcnow().isoformat()[:-7]\n\tdef is_not_done_broadcast(lengths, i, tensor):\n\t    import torch\n\t    is_not_done = torch.as_tensor((lengths > i + 1) * 1.0, dtype=torch.float32).to(tensor.device)\n\t    while len(is_not_done.shape) < len(tensor.shape):\n\t        is_not_done = is_not_done.unsqueeze(-1)\n\t    return is_not_done\n"]}
{"filename": "utils/metrics/__init__.py", "chunked_list": ["import numpy as np\n\tfrom ._heuristic import constraints_accuracy, objective_node_accuracy, overall_accuracy  # noqa: F401\n\tfrom config.vars import CLASSIFICATION_ERROR_DECIMALS, REGRESSION_ERROR_DECIMALS\n\tdef accuracy(y_pred, y_true):\n\t    error = (y_pred == y_true) * 1.0\n\t    error = np.mean(error)\n\t    return round(error, CLASSIFICATION_ERROR_DECIMALS)\n\tdef eval_categorical(y_pred, y_true):\n\t    return accuracy(y_pred.argmax(-1), y_true.argmax(-1))\n\tdef mse(y_pred, y_true):\n", "    error = (y_pred - y_true)**2\n\t    error = np.mean(error)\n\t    return round(error, REGRESSION_ERROR_DECIMALS)\n\tdef mae(y_pred, y_true):\n\t    error = np.abs((y_pred - y_true)).mean()\n\t    return round(error, REGRESSION_ERROR_DECIMALS)\n\tdef masked_mae(y_pred, y_true):\n\t    mask = y_true != 0\n\t    error = np.abs(y_pred - y_true)[mask].mean()\n\t    return round(error, REGRESSION_ERROR_DECIMALS)\n", "def masked_mse(y_pred, y_true):\n\t    mask = y_true != 0\n\t    error = (y_pred - y_true)**2\n\t    error = error[mask].mean()\n\t    return round(error, REGRESSION_ERROR_DECIMALS)\n\tdef dual_objective(y_pred, inputs):\n\t    for inp in inputs:\n\t        if inp.name == 'adj':\n\t            adj = inp.data.numpy()\n\t        elif inp.name == 'A':\n", "            weights = inp.data.numpy()\n\t    return round(constraints_accuracy(y_pred, weights, adj), CLASSIFICATION_ERROR_DECIMALS)\n\tdef mask_fn(pred, truth):\n\t    tp = ((pred > 0.5) * (truth > 0.5)) * 1.0\n\t    fp = ((pred > 0.5) * (truth < 0.5)) * 1.0\n\t    fn = ((pred < 0.5) * (truth > 0.5)) * 1.0\n\t    tp = np.sum(tp)\n\t    fp = np.sum(fp)\n\t    fn = np.sum(fn)\n\t    if tp + fp + fn == 0:\n", "        return 1.\n\t    if tp == 0:\n\t        return 0.\n\t    precision = tp / (tp + fp)\n\t    recall = tp / (tp + fn)\n\t    f_1 = 2.0 * precision * recall / (precision + recall)\n\t    return round(f_1, CLASSIFICATION_ERROR_DECIMALS)\n\tdef eval_one(pred, truth):\n\t    error = np.argmax(pred, -1) == np.argmax(truth, -1)\n\t    error = np.mean(error)\n", "    return round(error, CLASSIFICATION_ERROR_DECIMALS)\n"]}
{"filename": "utils/metrics/_heuristic.py", "chunked_list": ["def constraints_accuracy(y, w, adj):\n\t    from numpy import expand_dims, transpose\n\t    y = expand_dims(y, axis=-1)\n\t    cons_value = transpose(y, (0, 2, 1)) - y\n\t    cons = ((cons_value * adj <= w))\n\t    return (cons * adj).sum() / adj.sum()\n\tdef objective_node_accuracy(path_preds, truth):\n\t    accuracy = 0.0\n\t    for path_a, path_b in zip(path_preds, truth):\n\t        if path_a[-1] == path_b[-1]:\n", "            accuracy += 1\n\t    return accuracy / len(truth)\n\tdef overall_accuracy(path_preds, truth):\n\t    accuracy = 0.0\n\t    for path_a, path_b in zip(path_preds, truth):\n\t        if len(path_a) == len(path_b) and path_a == path_b:\n\t            accuracy += 1\n\t    return accuracy / len(truth)\n"]}
{"filename": "utils/data/_helpers.py", "chunked_list": ["import clrs\n\timport numpy as np\n\tfrom typing import List, Tuple\n\tfrom norm.performance import Timer\n\t_DataPoint = clrs.DataPoint\n\t_Trajectory = clrs.Trajectory\n\t_Trajectories = List[_Trajectory]\n\tdef _maybe_show_progress(length):\n\t    import os\n\t    if 'SHOW_LOADER_PROGRESS' in os.environ:\n", "        import typer\n\t        return typer.progressbar(length)\n\t    return None\n\tdef _update(progress):\n\t    if progress is None:\n\t        return\n\t    progress.update(1)\n\tdef batch_hints_helper(traj_hints: _Trajectories) -> Tuple[_Trajectory, List[int]]:\n\t    \"\"\"Batches a trajectory of hints samples along the time axis per probe.\n\t    Unlike i/o, hints have a variable-length time dimension. Before batching, each\n", "    trajectory is padded to the maximum trajectory length.\n\t    Args:\n\t    traj_hints: A hint trajectory of `DataPoints`s indexed by time then probe\n\t    Returns:\n\t    A |num probes| list of `DataPoint`s with the time axis stacked into `data`,\n\t    and a |sample| list containing the length of each trajectory.\n\t    \"\"\"\n\t    max_steps = 0\n\t    assert traj_hints  # non-empty\n\t    for sample_hint in traj_hints:\n", "        for dp in sample_hint:\n\t            assert dp.data.shape[1] == 1  # batching axis\n\t            if dp.data.shape[0] > max_steps:\n\t                max_steps = dp.data.shape[0]\n\t    n_samples = len(traj_hints)\n\t    batched_traj = traj_hints[0]  # construct batched trajectory in-place\n\t    hint_lengths = np.zeros(len(traj_hints))\n\t    for i in range(len(traj_hints[0])):\n\t        hint_i = traj_hints[0][i]\n\t        assert batched_traj[i].name == hint_i.name\n", "        batched_traj[i] = _DataPoint(\n\t            name=batched_traj[i].name,\n\t            location=batched_traj[i].location,\n\t            type_=batched_traj[i].type_,\n\t            data=np.zeros((max_steps, n_samples) + hint_i.data.shape[2:]))\n\t        batched_traj[i].data[:hint_i.data.shape[0], :1] = hint_i.data\n\t        if i > 0:\n\t            assert hint_lengths[0] == hint_i.data.shape[0]\n\t        else:\n\t            hint_lengths[0] = hint_i.data.shape[0]\n", "    progress = _maybe_show_progress(traj_hints[1:])\n\t    for hint_ind, cur_hint in enumerate(traj_hints[1:], start=1):\n\t        for i in range(len(cur_hint)):\n\t            assert batched_traj[i].name == cur_hint[i].name\n\t            batched_traj[i].data[:cur_hint[i].data.shape[0], hint_ind:hint_ind+1] = cur_hint[i].data\n\t            if i > 0:\n\t                assert hint_lengths[hint_ind] == cur_hint[i].data.shape[0]\n\t            else:\n\t                hint_lengths[hint_ind] = cur_hint[i].data.shape[0]\n\t        _update(progress)\n", "    return batched_traj, hint_lengths\n"]}
{"filename": "utils/data/graphs.py", "chunked_list": ["import numpy as np\n\timport networkx as nx\n\tfrom itertools import product\n\tfrom numpy.typing import NDArray\n\tfrom numpy.random import Generator, default_rng\n\tfrom typing import Optional\n\tdef _make_weights_undirected(w):\n\t    w = np.triu(w)\n\t    return np.triu(w) + np.triu(w, 1).T\n\tdef _erdos_renyi(num_nodes: int,\n", "                 prob: float,\n\t                 directed: bool = False,\n\t                 weighted: bool = False,\n\t                 rng: Optional[Generator] = None) -> NDArray:\n\t    assert num_nodes >= 0 and 0 < prob <= 1\n\t    if rng is None:\n\t        rng = default_rng()\n\t    adj_matrix = rng.random((num_nodes, num_nodes)) <= prob\n\t    if not directed:\n\t        adj_matrix = adj_matrix + adj_matrix.T\n", "    weights = None\n\t    if weighted:\n\t        weights = rng.uniform(low=0.0, high=1.0, size=(num_nodes, num_nodes))\n\t        if not directed:\n\t            weights = _make_weights_undirected(weights)\n\t    return adj_matrix, weights\n\tdef erdos_renyi_full(num_nodes: int,\n\t                     prob: float,\n\t                     directed: bool = False,\n\t                     weighted: bool = False,\n", "                     rng: Optional[Generator] = None) -> NDArray:\n\t    adj_matrix, weights = _erdos_renyi(num_nodes=num_nodes,\n\t                                       prob=prob,\n\t                                       directed=directed,\n\t                                       weighted=weighted,\n\t                                       rng=rng)\n\t    adj_matrix = adj_matrix.astype(dtype=np.float32)\n\t    return adj_matrix * weights if weighted else adj_matrix\n\tdef two_community(num_nodes: int,\n\t                  prob: float,\n", "                  outer_prob: float,\n\t                  directed: bool = False,\n\t                  weighted: bool = False,\n\t                  rng: Optional[Generator] = None) -> NDArray:\n\t    assert num_nodes % 2 == 0\n\t    adj_matrix_1, _ = _erdos_renyi(num_nodes=num_nodes // 2,\n\t                                   prob=prob,\n\t                                   directed=directed,\n\t                                   weighted=weighted,\n\t                                   rng=rng)\n", "    adj_matrix_2, _ = _erdos_renyi(num_nodes=num_nodes // 2,\n\t                                   prob=prob,\n\t                                   directed=directed,\n\t                                   weighted=weighted,\n\t                                   rng=rng)\n\t    adj_matrix = np.zeros((num_nodes, num_nodes))\n\t    N = num_nodes // 2\n\t    adj_matrix[:N, :N] = adj_matrix_1\n\t    adj_matrix[N:, N:] = adj_matrix_2\n\t    cart = list(product(range(N), range(N, num_nodes)))\n", "    mask = rng.binomial(n=1, p=outer_prob, size=len(cart))\n\t    n = 0\n\t    while mask.sum() == 0:  # prevent disconnected graph\n\t        mask = rng.binomial(n=1, p=outer_prob + (0.01 * n), size=len(cart))\n\t        n += 1\n\t    for i, e in enumerate(cart):\n\t        if mask[i]:\n\t            u, v = e\n\t            adj_matrix[u, v] = 1.\n\t    if not directed:\n", "        adj_matrix = np.maximum(adj_matrix, adj_matrix.T)\n\t    return adj_matrix\n\tdef bipartite(num_nodes: int,\n\t              prob: float,\n\t              outer_prob: float = None,  # unused param\n\t              directed: bool = True,\n\t              weighted: bool = False,\n\t              rng: Optional[Generator] = None) -> NDArray:\n\t    if rng is None:\n\t        rng = default_rng()\n", "    N = (num_nodes-2) // 2\n\t    adj_matrix = np.zeros((num_nodes, num_nodes))\n\t    set_a = list(range(1, N+1))\n\t    set_b = list(range(N+1, num_nodes-1))\n\t    cart = list(product(set_a, set_b))\n\t    mask = rng.binomial(n=1, p=prob, size=len(cart))\n\t    connected_nodes_b = np.unique(\n\t        [y for x, y in np.array(cart)[np.argwhere(mask)].squeeze()]\n\t    )\n\t    n = 0\n", "    while len(set_b) != len(connected_nodes_b):  # prevent disconnected graph\n\t        mask = rng.binomial(n=1, p=prob + (0.01 * n), size=len(cart))\n\t        connected_nodes_b = np.unique(\n\t            [y for x, y in np.array(cart)[np.argwhere(mask)].squeeze()]\n\t        )\n\t        n += 1\n\t    for i, e in enumerate(cart):\n\t        if mask[i]:\n\t            u, v = e\n\t            adj_matrix[u, v] = 1.\n", "    adj_matrix[0, :max(set_a)+1] = 1.\n\t    adj_matrix[min(set_b):, -1] = 1.\n\t    if not directed:\n\t        adj_matrix = np.maximum(adj_matrix, adj_matrix.T)\n\t    assert nx.is_connected(nx.from_numpy_matrix(adj_matrix))\n\t    return adj_matrix\n"]}
{"filename": "utils/data/_loader.py", "chunked_list": ["import clrs\n\timport numpy as np\n\timport torch\n\tfrom ._helpers import batch_hints_helper\n\tfrom .algorithms import SPECS\n\tfrom clrs._src.probing import split_stages\n\tfrom norm.io import load\n\tfrom pathlib import Path\n\tfrom torch import as_tensor, index_select\n\tfrom typing import List, Optional, Tuple, Union\n", "from utils.types import Algorithm\n\t_DataPoint = clrs.DataPoint\n\t_Spec = clrs.Spec\n\t_Type = clrs.Type\n\t_Trajectory = clrs.Trajectory\n\t_Trajectories = List[_Trajectory]\n\t_Features = clrs.Features\n\t_Feedback = clrs.Feedback\n\tdef _batch_io(traj_io):\n\t    from clrs._src.samplers import _batch_io as _batch_io_helper\n", "    batched_traj = _batch_io_helper(traj_io)\n\t    for traj in batched_traj:\n\t        traj.data = as_tensor(traj.data, dtype=torch.float32)\n\t    return batched_traj\n\tdef _batch_hints(traj_hints):\n\t    batched_traj, hint_lengths = batch_hints_helper(traj_hints)\n\t    for traj in batched_traj:\n\t        traj.data = as_tensor(traj.data, dtype=torch.float32)\n\t    return batched_traj, hint_lengths\n\tdef _subsample_data(trajectory, indices, axis=0):\n", "    sampled_traj = []\n\t    for dp in trajectory:\n\t        sampled_data = index_select(dp.data, dim=axis, index=indices)\n\t        sampled_traj.append(_DataPoint(dp.name, dp.location, dp.type_, sampled_data))\n\t    return sampled_traj\n\tdef _load_probes(file_name: Path, spec: _Spec):\n\t    inputs = []\n\t    outputs = []\n\t    hints = []\n\t    file_probes = load(file_name)\n", "    for probes in file_probes:\n\t        inp, outp, hint = split_stages(probes, spec)\n\t        inputs.append(inp)\n\t        outputs.append(outp)\n\t        hints.append(hint)\n\t    return inputs, outputs, hints\n\tclass Loader:\n\t    def __init__(self, file_name: Union[Path, List[Path]], spec: _Spec):\n\t        if isinstance(file_name, list):\n\t            inputs, outputs, hints = [], [], []\n", "            for name in file_name:\n\t                inp, out, hin = _load_probes(name, spec)\n\t                inputs.extend(inp)\n\t                outputs.extend(out)\n\t                hints.extend(hin)\n\t        else:\n\t            inputs, outputs, hints = _load_probes(file_name, spec)\n\t        # Batch and pad trajectories to max(T).\n\t        self._inputs = _batch_io(inputs)\n\t        self._outputs = _batch_io(outputs)\n", "        self._hints, self._lengths = _batch_hints(hints)\n\t        self._num_samples = len(inputs)\n\t        self._file_name = file_name\n\t        self._spec = spec\n\t    def __add__(self, other):\n\t        return Loader([self._file_name, other._file_name], self._spec)\n\t    def next(self, batch_size: Optional[int] = None) -> _Feedback:\n\t        \"\"\"Subsamples trajectories from the pre-generated dataset.\n\t        Args:\n\t          batch_size: Optional batch size. If `None`, returns entire dataset.\n", "        Returns:\n\t          Subsampled trajectories.\n\t        \"\"\"\n\t        if batch_size:\n\t            if batch_size > self._num_samples:\n\t                raise ValueError(\n\t                    f'Batch size {batch_size} > dataset size {self._num_samples}.')\n\t            # Returns a fixed-size random batch.\n\t            raw_indices = np.random.choice(self._num_samples, (batch_size,), replace=True)\n\t            indices = as_tensor(raw_indices, dtype=torch.long)\n", "            inputs = _subsample_data(self._inputs, indices, axis=0)\n\t            outputs = _subsample_data(self._outputs, indices, axis=0)\n\t            hints = _subsample_data(self._hints, indices, axis=1)\n\t            lengths = self._lengths[raw_indices]\n\t        else:\n\t            # Returns the full dataset.\n\t            inputs = self._inputs\n\t            hints = self._hints\n\t            lengths = self._lengths\n\t            outputs = self._outputs\n", "        return _Feedback(_Features(inputs, hints, lengths), outputs)\n\t    def get(self, index: int) -> _Feedback:\n\t        index = torch.LongTensor([index])\n\t        inputs = _subsample_data(self._inputs, index, axis=0)\n\t        outputs = _subsample_data(self._outputs, index, axis=0)\n\t        hints = _subsample_data(self._hints, index, axis=1)\n\t        lengths = self._lengths[index]\n\t        for hint in hints:\n\t            hint.data = hint.data[:int(lengths)]\n\t        return _Feedback(_Features(inputs, hints, lengths), outputs)\n", "def load_dataset(split: str,\n\t                 algorithm: Algorithm,\n\t                 folder: Path) -> Tuple[Loader, _Spec]:\n\t    if algorithm not in SPECS:\n\t        raise NotImplementedError(f\"No implementation of algorithm {algorithm}\")\n\t    spec = SPECS[algorithm]\n\t    loader = Loader(file_name=folder / f'{split}_{algorithm}.pkl', spec=spec)\n\t    return loader, spec\n"]}
{"filename": "utils/data/__init__.py", "chunked_list": ["from ._loader import load_dataset, Loader  # noqa: F401\n\tdef adj_mat(features):\n\t    for inp in features.inputs:\n\t        if inp.name == \"adj\":\n\t            return inp.data\n\tdef edge_attr_mat(features):\n\t    for inp in features.inputs:\n\t        if inp.name == \"A\":\n\t            return inp.data\n"]}
{"filename": "utils/data/algorithms/__init__.py", "chunked_list": ["from ._graphs import a_star, dijkstra, ford_fulkerson, ford_fulkerson_mincut, max_flow_min_cut_lp, max_flow_lp, min_cut_lp, ek_bfs  # noqa: F401\n\tfrom ._specs import SPECS, ALGS  # noqa: F401\n\tfrom clrs._src.algorithms import bfs  # noqa: F401\n"]}
{"filename": "utils/data/algorithms/_specs.py", "chunked_list": ["import clrs\n\timport types\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n\t_Type = clrs.Type\n\tSPECS = types.MappingProxyType({\n\t    **clrs._src.specs.SPECS,\n\t    'dijkstra': {\n\t        'pos': (_Stage.INPUT, _Location.NODE, _Type.SCALAR),\n\t        's': (_Stage.INPUT, _Location.NODE, _Type.MASK_ONE),\n", "        't': (_Stage.INPUT, _Location.NODE, _Type.MASK_ONE),\n\t        'A': (_Stage.INPUT, _Location.EDGE, _Type.SCALAR),\n\t        'adj': (_Stage.INPUT, _Location.EDGE, _Type.MASK),\n\t        'pi': (_Stage.OUTPUT, _Location.NODE, _Type.POINTER),\n\t        'pi_h': (_Stage.HINT, _Location.NODE, _Type.POINTER),\n\t        'd': (_Stage.HINT, _Location.NODE, _Type.SCALAR),\n\t        'mark': (_Stage.HINT, _Location.NODE, _Type.MASK),\n\t        'in_queue': (_Stage.HINT, _Location.NODE, _Type.MASK),\n\t        'u': (_Stage.HINT, _Location.NODE, _Type.MASK_ONE)\n\t    },\n", "    'ford_fulkerson': {\n\t        'pos': (_Stage.INPUT, _Location.NODE, _Type.SCALAR),\n\t        's': (_Stage.INPUT, _Location.NODE, _Type.MASK_ONE),\n\t        't': (_Stage.INPUT, _Location.NODE, _Type.MASK_ONE),\n\t        'A': (_Stage.INPUT, _Location.EDGE, _Type.SCALAR),\n\t        'adj': (_Stage.INPUT, _Location.EDGE, _Type.MASK),\n\t        'w': (_Stage.INPUT, _Location.EDGE, _Type.SCALAR),\n\t        'mask': (_Stage.HINT, _Location.NODE, _Type.MASK),\n\t        'pi_h': (_Stage.HINT, _Location.NODE, _Type.POINTER),\n\t        '__is_bfs_op': (_Stage.HINT, _Location.GRAPH, _Type.MASK),\n", "        'f_h': (_Stage.HINT, _Location.EDGE, _Type.SCALAR),\n\t        'f': (_Stage.OUTPUT, _Location.EDGE, _Type.SCALAR)\n\t    },\n\t    'ford_fulkerson_mincut': {\n\t        'pos': (_Stage.INPUT, _Location.NODE, _Type.SCALAR),\n\t        's': (_Stage.INPUT, _Location.NODE, _Type.MASK_ONE),\n\t        't': (_Stage.INPUT, _Location.NODE, _Type.MASK_ONE),\n\t        'A': (_Stage.INPUT, _Location.EDGE, _Type.SCALAR),\n\t        'adj': (_Stage.INPUT, _Location.EDGE, _Type.MASK),\n\t        'w': (_Stage.INPUT, _Location.EDGE, _Type.SCALAR),\n", "        'mask': (_Stage.HINT, _Location.NODE, _Type.MASK),\n\t        'pi_h': (_Stage.HINT, _Location.NODE, _Type.POINTER),\n\t        'f_h': (_Stage.HINT, _Location.EDGE, _Type.SCALAR),\n\t        'c_h': (_Stage.HINT, _Location.NODE, _Type.CATEGORICAL),\n\t        '__is_bfs_op': (_Stage.HINT, _Location.GRAPH, _Type.MASK),\n\t        'f': (_Stage.OUTPUT, _Location.EDGE, _Type.SCALAR),\n\t        'c': (_Stage.OUTPUT, _Location.NODE, _Type.CATEGORICAL),\n\t    },\n\t})\n\tALGS = [*clrs._src.specs.CLRS_30_ALGS,\n", "        'ford_fulkerson',\n\t        'ford_fulkerson_min_cut']\n"]}
{"filename": "utils/data/algorithms/_graphs.py", "chunked_list": ["import clrs\n\timport numpy as np\n\timport chex\n\timport networkx as nx\n\tfrom ._specs import SPECS\n\tfrom clrs._src import probing\n\tfrom clrs._src.probing import ProbesDict\n\tfrom typing import Tuple\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n", "_Type = clrs.Type\n\t_Array = np.ndarray\n\t_Out = Tuple[_Array, ProbesDict]\n\t_OutputClass = clrs.OutputClass\n\tdef a_star(A: _Array, h: _Array, s: int, t: int, max_iter=5000) -> _Out:\n\t    \"\"\"A* search algorithm (Hart et al., 1968)\"\"\"\n\t    chex.assert_rank(A, 2)\n\t    probes = probing.initialize(SPECS['a_star'])\n\t    A_pos = np.arange(A.shape[0])\n\t    probing.push(\n", "        probes,\n\t        _Stage.INPUT,\n\t        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / A.shape[0],\n\t            's': probing.mask_one(s, A.shape[0]),\n\t            't': probing.mask_one(t, A.shape[0]),\n\t            'A': np.copy(A),\n\t            'adj': probing.graph(np.copy(A))\n\t        })\n\t    d = np.zeros(A.shape[0])\n", "    mark = np.zeros(A.shape[0])\n\t    in_queue = np.zeros(A.shape[0])\n\t    f = np.zeros(A.shape[0])\n\t    pi = np.arange(A.shape[0])\n\t    d[s] = 0\n\t    f[s] = h[s]\n\t    in_queue[s] = 1\n\t    probing.push(\n\t        probes,\n\t        _Stage.HINT,\n", "        next_probe={\n\t            'pi_h': np.copy(pi),\n\t            'd': np.copy(d),\n\t            'f': np.copy(f),\n\t            'mark': np.copy(mark),\n\t            'in_queue': np.copy(in_queue),\n\t            'u': probing.mask_one(s, A.shape[0])\n\t        })\n\t    while in_queue.any():\n\t        u = np.argsort(f + (1.0 - in_queue) * 1e9)[0]  # drop-in for extract-min\n", "        if in_queue[u] == 0 or u == t:\n\t            break\n\t        in_queue[u] = 0\n\t        mark[u] = 1\n\t        for v in range(A.shape[0]):\n\t            if A[u, v] != 0:\n\t                if mark[v] == 0 or d[u] + A[u, v] < d[v]:\n\t                    pi[v] = u\n\t                    d[v] = d[u] + A[u, v]\n\t                    f[v] = d[v] - h[v]\n", "                    mark[v] = 1\n\t                    in_queue[v] = 1\n\t        probing.push(\n\t            probes,\n\t            _Stage.HINT,\n\t            next_probe={\n\t                'pi_h': np.copy(pi),\n\t                'd': np.copy(d),\n\t                'f': np.copy(f),\n\t                'mark': np.copy(mark),\n", "                'in_queue': np.copy(in_queue),\n\t                'u': probing.mask_one(u, A.shape[0])\n\t            })\n\t    probing.push(probes, _Stage.OUTPUT, next_probe={'pi': np.copy(pi)})\n\t    probing.finalize(probes)\n\t    return pi, probes\n\tdef dijkstra(A: _Array, s: int, t: int, early_stop: bool = False) -> _Out:\n\t    \"\"\"Dijkstra's single-source shortest path (Dijkstra, 1959).\"\"\"\n\t    chex.assert_rank(A, 2)\n\t    probes = probing.initialize(SPECS['dijkstra'])\n", "    A_pos = np.arange(A.shape[0])\n\t    probing.push(\n\t        probes,\n\t        _Stage.INPUT,\n\t        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / A.shape[0],\n\t            's': probing.mask_one(s, A.shape[0]),\n\t            't': probing.mask_one(t, A.shape[0]),\n\t            'A': np.copy(A),\n\t            'adj': probing.graph(np.copy(A))\n", "        })\n\t    d = np.zeros(A.shape[0])\n\t    mark = np.zeros(A.shape[0])\n\t    in_queue = np.zeros(A.shape[0])\n\t    pi = np.arange(A.shape[0])\n\t    d[s] = 0\n\t    in_queue[s] = 1\n\t    probing.push(\n\t        probes,\n\t        _Stage.HINT,\n", "        next_probe={\n\t            'pi_h': np.copy(pi),\n\t            'd': np.copy(d),\n\t            'mark': np.copy(mark),\n\t            'in_queue': np.copy(in_queue),\n\t            'u': probing.mask_one(s, A.shape[0])\n\t        })\n\t    for _ in range(A.shape[0]):\n\t        u = np.argsort(d + (1.0 - in_queue) * 1e9)[0]  # drop-in for extract-min\n\t        if in_queue[u] == 0 or (early_stop and u == t):\n", "            break\n\t        mark[u] = 1\n\t        in_queue[u] = 0\n\t        for v in range(A.shape[0]):\n\t            if A[u, v] != 0:\n\t                if mark[v] == 0 and (in_queue[v] == 0 or d[u] + A[u, v] < d[v]):\n\t                    pi[v] = u\n\t                    d[v] = d[u] + A[u, v]\n\t                    in_queue[v] = 1\n\t        probing.push(\n", "            probes,\n\t            _Stage.HINT,\n\t            next_probe={\n\t                'pi_h': np.copy(pi),\n\t                'd': np.copy(d),\n\t                'mark': np.copy(mark),\n\t                'in_queue': np.copy(in_queue),\n\t                'u': probing.mask_one(u, A.shape[0])\n\t            })\n\t    probing.push(probes, _Stage.OUTPUT, next_probe={'pi': np.copy(pi)})\n", "    probing.finalize(probes)\n\t    return pi, probes\n\tdef max_flow_lp(adj: _Array, capacity: _Array, s: int, t: int):\n\t    \"\"\"Max flow LP formulation.\"\"\"\n\t    chex.assert_rank(adj, 2)\n\t    probes = probing.initialize(SPECS['max_flow_lp'])\n\t    A_pos = np.arange(adj.shape[0])\n\t    probing.push(\n\t        probes,\n\t        _Stage.INPUT,\n", "        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / adj.shape[0],\n\t            's': probing.mask_one(s, adj.shape[0]),\n\t            't': probing.mask_one(t, adj.shape[0]),\n\t            'A': np.copy(capacity),\n\t            'adj': probing.graph(np.copy(adj))\n\t        })\n\t    probing.push(\n\t        probes,\n\t        _Stage.OUTPUT,\n", "        next_probe={\n\t            'unsup_lp_flow': np.empty(1)\n\t        }\n\t    )\n\t    probing.finalize(probes)\n\t    return None, probes\n\tdef min_cut_lp(adj: _Array, capacity: _Array, s: int, t: int):\n\t    \"\"\"Min-Cut LP formulation.\"\"\"\n\t    chex.assert_rank(adj, 2)\n\t    probes = probing.initialize(SPECS['min_cut_lp'])\n", "    A_pos = np.arange(adj.shape[0])\n\t    probing.push(\n\t        probes,\n\t        _Stage.INPUT,\n\t        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / adj.shape[0],\n\t            's': probing.mask_one(s, adj.shape[0]),\n\t            't': probing.mask_one(t, adj.shape[0]),\n\t            'A': np.copy(capacity),\n\t            'adj': probing.graph(np.copy(adj))\n", "        })\n\t    probing.push(\n\t        probes,\n\t        _Stage.OUTPUT,\n\t        next_probe={\n\t            's': np.array([[1, 0]]).repeat(adj.shape[0], axis=0),\n\t        }\n\t    )\n\t    probing.finalize(probes)\n\t    return None, probes\n", "def max_flow_min_cut_lp(adj: _Array, capacity: _Array, s: int, t: int):\n\t    chex.assert_rank(adj, 2)\n\t    probes = probing.initialize(SPECS['max_flow_min_cut_lp'])\n\t    A_pos = np.arange(adj.shape[0])\n\t    probing.push(\n\t        probes,\n\t        _Stage.INPUT,\n\t        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / adj.shape[0],\n\t            's': probing.mask_one(s, adj.shape[0]),\n", "            't': probing.mask_one(t, adj.shape[0]),\n\t            'A': np.copy(capacity),\n\t            'adj': probing.graph(np.copy(adj))\n\t        })\n\t    probing.push(\n\t        probes,\n\t        _Stage.OUTPUT,\n\t        next_probe={\n\t            'unsup_lp_flow': np.empty(1),\n\t            's': np.array([[1, 0]]).repeat(adj.shape[0], axis=0),\n", "        }\n\t    )\n\t    probing.finalize(probes)\n\t    return None, probes\n\tdef _ff_impl(A: _Array, s: int, t: int, probes, w):\n\t    f = np.zeros((A.shape[0], A.shape[0]))\n\t    df = np.array(0)\n\t    C = _minimum_cut(A, s, t)\n\t    def reverse(pi):\n\t        u, v = pi[t], t\n", "        while u != v:\n\t            yield u, v\n\t            v = u\n\t            u = pi[u]\n\t    d = np.zeros(A.shape[0])\n\t    msk = np.zeros(A.shape[0])\n\t    pi = np.arange(A.shape[0])\n\t    d[s] = 0\n\t    msk[s] = 1\n\t    probing.push(\n", "        probes,\n\t        _Stage.HINT,\n\t        next_probe={\n\t            'mask': np.copy(msk),\n\t            'd': np.copy(d),\n\t            'pi_h': np.copy(pi),\n\t            'f_h': np.copy(f),\n\t            'df': np.copy(df),\n\t            'c_h': np.copy(C),\n\t            '__is_bfs_op': np.copy([1])\n", "        })\n\t    while True:\n\t        for _ in range(A.shape[0]):\n\t            prev_d = np.copy(d)\n\t            prev_msk = np.copy(msk)\n\t            for u in range(A.shape[0]):\n\t                for v in range(A.shape[0]):\n\t                    if prev_msk[u] == 1 and A[u, v] - abs(f[u, v]) > 0:\n\t                        if msk[v] == 0 or prev_d[u] + w[u, v] < d[v]:\n\t                            d[v] = prev_d[u] + w[u, v]\n", "                            pi[v] = u\n\t                        msk[v] = 1\n\t            probing.push(\n\t                probes,\n\t                _Stage.HINT,\n\t                next_probe={\n\t                    'pi_h': np.copy(pi),\n\t                    'd': np.copy(prev_d),\n\t                    'mask': np.copy(msk),\n\t                    'f_h': np.copy(f),\n", "                    'df': np.copy(df),\n\t                    'c_h': np.copy(C),\n\t                    '__is_bfs_op': np.copy([1])\n\t                })\n\t            if np.all(d == prev_d):\n\t                break\n\t        if pi[t] == t:\n\t            break\n\t        df = min([\n\t            A[u, v] - f[u, v]\n", "            for u, v in reverse(pi)\n\t        ])\n\t        for u, v in reverse(pi):\n\t            f[u, v] += df\n\t            f[v, u] -= df\n\t        d = np.zeros(A.shape[0])\n\t        msk = np.zeros(A.shape[0])\n\t        pi = np.arange(A.shape[0])\n\t        d[s] = 0\n\t        msk[s] = 1\n", "        probing.push(\n\t            probes,\n\t            _Stage.HINT,\n\t            next_probe={\n\t                'pi_h': np.copy(pi),\n\t                'd': np.copy(d),\n\t                'mask': np.copy(msk),\n\t                'f_h': np.copy(f),\n\t                'df': np.copy(df),\n\t                'c_h': np.copy(C),\n", "                '__is_bfs_op': np.array([0])\n\t            })\n\t    return f, probes\n\tdef ford_fulkerson(A: _Array, s: int, t: int):\n\t    chex.assert_rank(A, 2)\n\t    probes = probing.initialize(SPECS['ford_fulkerson'])\n\t    A_pos = np.arange(A.shape[0])\n\t    rng = np.random.default_rng(0)\n\t    w = rng.random(size=A.shape)\n\t    w = np.maximum(w, w.T) * probing.graph(np.copy(A))\n", "    probing.push(\n\t        probes,\n\t        _Stage.INPUT,\n\t        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / A.shape[0],\n\t            's': probing.mask_one(s, A.shape[0]),\n\t            't': probing.mask_one(t, A.shape[0]),\n\t            'A': np.copy(A),\n\t            'adj': probing.graph(np.copy(A)),\n\t            'w': np.copy(w),\n", "        })\n\t    f, probes = _ff_impl(A, s, t, probes, w)\n\t    probing.push(\n\t        probes,\n\t        _Stage.OUTPUT,\n\t        next_probe={\n\t            'f': np.copy(f)\n\t        }\n\t    )\n\t    probing.finalize(probes)\n", "    return f, probes\n\tdef ford_fulkerson_mincut(A: _Array, s: int, t: int):\n\t    chex.assert_rank(A, 2)\n\t    probes = probing.initialize(SPECS['ford_fulkerson_mincut'])\n\t    A_pos = np.arange(A.shape[0])\n\t    rng = np.random.default_rng(0)\n\t    w = rng.random(size=A.shape)\n\t    w = np.maximum(w, w.T) * probing.graph(np.copy(A))\n\t    probing.push(\n\t        probes,\n", "        _Stage.INPUT,\n\t        next_probe={\n\t            'pos': np.copy(A_pos) * 1.0 / A.shape[0],\n\t            's': probing.mask_one(s, A.shape[0]),\n\t            't': probing.mask_one(t, A.shape[0]),\n\t            'A': np.copy(A),\n\t            'adj': probing.graph(np.copy(A)),\n\t            'w': np.copy(w)\n\t        })\n\t    f, probes = _ff_impl(A, s, t, probes, w)\n", "    probing.push(\n\t        probes,\n\t        _Stage.OUTPUT,\n\t        next_probe={\n\t            'f': np.copy(f),\n\t            'c': _minimum_cut(A, s, t)\n\t        }\n\t    )\n\t    probing.finalize(probes)\n\t    return f, probes\n", "def _minimum_cut(A, s, t):\n\t    C = np.zeros((A.shape[0], 2))\n\t    graph = nx.from_numpy_matrix(A)\n\t    nx.set_edge_attributes(graph, {(i, j): A[i, j] for i, j in zip(*A.nonzero())},\n\t                           name='capacity')\n\t    _, cuts = nx.minimum_cut(graph, s, t)\n\t    for v in cuts[0]:\n\t        C[v][0] = 1\n\t    for v in cuts[1]:\n\t        C[v][1] = 1\n", "    return C\n\tdef _masked_array(a):\n\t    a = np.empty_like(a)\n\t    a.fill(_OutputClass.MASKED)\n\t    return a\n\t# TEST PURPOSE\n\tdef ek_bfs(A: _Array, s: int, t: int):\n\t    chex.assert_rank(A, 2)\n\t    A_pos = np.arange(A.shape[0])\n\t    f = np.zeros((A.shape[0], A.shape[0]))\n", "    df = np.array(0)\n\t    def reverse(pi):\n\t        u, v = pi[t], t\n\t        while u != v:\n\t            yield u, v\n\t            v = u\n\t            u = pi[u]\n\t    while True:\n\t        probes = probing.initialize(SPECS['ek_bfs'])\n\t        probing.push(\n", "            probes,\n\t            _Stage.INPUT,\n\t            next_probe={\n\t                'pos': np.copy(A_pos) * 1.0 / A.shape[0],\n\t                's': probing.mask_one(s, A.shape[0]),\n\t                't': probing.mask_one(t, A.shape[0]),\n\t                'A': np.copy(A),\n\t                'adj': probing.graph(np.copy(A)),\n\t                'f_h': np.copy(f)\n\t            })\n", "        pi, probes = _bfs_ek_impl(A, s, f, probes)\n\t        probing.push(\n\t            probes,\n\t            _Stage.OUTPUT,\n\t            next_probe={\n\t                'pi': np.copy(pi)\n\t            }\n\t        )\n\t        if pi[t] == t:\n\t            break\n", "        df = min([\n\t            A[u, v] - f[u, v]\n\t            for u, v in reverse(pi)\n\t        ])\n\t        for u, v in reverse(pi):\n\t            f[u, v] += df\n\t            f[v, u] -= df\n\t        probing.finalize(probes)\n\t        yield pi, probes\n\t    probing.finalize(probes)\n", "    yield pi, probes\n\t    return\n\tdef _bfs_ek_impl(A: _Array, s: int, f: int, probes):\n\t    reach = np.zeros(A.shape[0])\n\t    reach[s] = 1\n\t    pi = np.arange(A.shape[0])\n\t    probing.push(\n\t        probes,\n\t        _Stage.HINT,\n\t        next_probe={\n", "            'reach_h': np.copy(reach),\n\t            'pi_h': np.copy(pi),\n\t        })\n\t    while True:\n\t        prev_reach = np.copy(reach)\n\t        for i in range(A.shape[0]):\n\t            for j in range(A.shape[0]):\n\t                if A[i, j] - f[i, j] > 0 and prev_reach[i] == 1:\n\t                    if pi[j] == j and j != s:\n\t                        pi[j] = i\n", "                    reach[j] = 1\n\t        probing.push(\n\t            probes,\n\t            _Stage.HINT,\n\t            next_probe={\n\t                'reach_h': np.copy(reach),\n\t                'pi_h': np.copy(pi),\n\t            })\n\t        if np.all(reach == prev_reach):\n\t            break\n", "    return pi, probes\n"]}
{"filename": "utils/experiments/__init__.py", "chunked_list": ["from ._evaluation import evaluate, EVAL_FN  # noqa: F401\n"]}
{"filename": "utils/experiments/_evaluation.py", "chunked_list": ["import clrs\n\tfrom utils.metrics import accuracy, eval_one, mse, mask_fn, eval_categorical, masked_mae, masked_mse\n\t_Type = clrs.Type\n\tdef evaluate(model, feedback, extras=None, verbose=False):\n\t    out = {}\n\t    predictions, (raw_preds, aux) = model.predict(feedback.features)\n\t    if extras:\n\t        out.update(extras)\n\t    if verbose and aux:\n\t        losses, total_loss = model.verbose_loss(feedback, raw_preds, aux)\n", "        out.update(losses)\n\t        out.update({'val_loss': total_loss})\n\t    out.update(_eval_preds(predictions, feedback, verbose))\n\t    return out\n\tdef _eval_preds(preds, feedback, verbose=False):\n\t    evals = {}\n\t    extras = {}\n\t    for truth in feedback.outputs:\n\t        assert truth.name in preds\n\t        pred = preds[truth.name]\n", "        assert pred.name == truth.name\n\t        assert pred.location == truth.location\n\t        assert pred.type_ == truth.type_\n\t        y_hat = pred.data.cpu().numpy()\n\t        y = truth.data.numpy()\n\t        if truth.type_ == clrs.Type.SCALAR:\n\t            evals[truth.name + \"_mae\"] = masked_mae(y_hat, y).item()\n\t            evals[truth.name + \"_mse\"] = masked_mse(y_hat, y).item()\n\t        else:\n\t            evals[truth.name] = EVAL_FN[truth.type_](y_hat, y).item()\n", "    evals['score'] = evals['f_mse']  # sum([v for v in evals.values()]) / len(evals)\n\t    if verbose:\n\t        evals = {\n\t            **evals,\n\t            **extras,\n\t        }\n\t    return evals\n\tEVAL_FN = {\n\t    clrs.Type.SCALAR: mse,\n\t    clrs.Type.MASK: mask_fn,\n", "    clrs.Type.MASK_ONE: eval_one,\n\t    clrs.Type.CATEGORICAL: eval_categorical,\n\t    clrs.Type.POINTER: accuracy\n\t}\n"]}
{"filename": "config/vars.py", "chunked_list": ["REGRESSION_ERROR_DECIMALS = 5\n\tCLASSIFICATION_ERROR_DECIMALS = 3\n"]}
{"filename": "config/data.py", "chunked_list": ["DATA_SETTINGS = {\n\t    'train': {\n\t        'num_samples': 1000,\n\t        'length': 16\n\t    },\n\t    'val': {\n\t        'num_samples': 128,\n\t        'length': 16\n\t    },\n\t    'test': {\n", "        'num_samples': 128,\n\t        'length': 16\n\t    },\n\t    'test_2x': {\n\t        'num_samples': 128,\n\t        'length': 32\n\t    },\n\t    'test_4x': {\n\t        'num_samples': 128,\n\t        'length': 64\n", "    },\n\t    # 'test_6x': {\n\t    #     'num_samples': 128,\n\t    #     'length': 96\n\t    # },\n\t    # 'test_8x': {\n\t    #     'num_samples': 128,\n\t    #     'length': 128\n\t    # },\n\t    # 'test_10x': {\n", "    #     'num_samples': 128,\n\t    #     'length': 160\n\t    # },\n\t    # 'test_12x': {\n\t    #     'num_samples': 128,\n\t    #     'length': 192\n\t    # },\n\t    # 'test_14x': {\n\t    #     'num_samples': 128,\n\t    #     'length': 224\n", "    # },\n\t    # 'test_16x': {\n\t    #     'num_samples': 128,\n\t    #     'length': 256\n\t    # }\n\t}\n"]}
{"filename": "config/hyperparameters.py", "chunked_list": ["from functools import partial\n\tfrom norm.experiments.samplers import integer, uniform\n\t_LARGE = {\n\t    'model': dict(num_hidden=partial(integer, a=16, b=512),\n\t                  alpha=partial(uniform, a=0, b=0)),\n\t    'optim': dict(\n\t        lr=partial(uniform, a=1e-5, b=1e-1),\n\t        weight_decay=partial(uniform, a=1e-5, b=1e-1)\n\t    )\n\t}\n", "_SMALL_FF = {\n\t    'model': dict(num_hidden=partial(integer, a=64, b=72),  # 114-138\n\t                  alpha=partial(uniform, a=0, b=1)),\n\t    'optim': dict(\n\t        lr=partial(uniform, 1e-3, b=1e-2),\n\t        weight_decay=partial(uniform, a=1e-3, b=4e-3)\n\t    )\n\t}\n\t_ONE_FF = {\n\t    'model': dict(num_hidden=lambda: 68,  # 114-138\n", "                  alpha=partial(uniform, a=0, b=0)),\n\t    'optim': dict(\n\t        lr=lambda: 0.009341493994646139,\n\t        weight_decay=lambda: 0.003420373065077989,\n\t    )\n\t}\n\t_ONE_FF_MC = {\n\t    'model': dict(num_hidden=lambda: 65,\n\t                  alpha=partial(uniform, a=0, b=0)),\n\t    'optim': dict(\n", "        lr=lambda: 0.009868199084919982,\n\t        weight_decay=lambda: 0.0017345516681916279\n\t    )\n\t}\n\tHP_SPACE = {\n\t    'dual_sp_large': _LARGE,\n\t    'ff_large': _LARGE,\n\t    'ff_mc_large': _LARGE,\n\t    'ff_small': _SMALL_FF,\n\t    'ff_mc_small': _SMALL_FF,\n", "    'ff_one': _ONE_FF,\n\t    'ff_mc_one': _ONE_FF_MC\n\t}\n"]}
{"filename": "nn/__init__.py", "chunked_list": ["from .models import EncodeProcessDecode  # noqa: 401\n"]}
{"filename": "nn/losses/__init__.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom utils.data import adj_mat\n\tfrom nn.models.impl import _expand_to\n\t_Feedback = clrs.Feedback\n\t_Location = clrs.Location\n\t_OutputClass = clrs.OutputClass\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Type = clrs.Type\n", "_DataPoint = clrs.DataPoint\n\tEPS = 1e-12\n\tdef cross_entropy(y_pred, y_true, num_classes):\n\t    from torch import mean, sum\n\t    from torch.nn.functional import one_hot, log_softmax\n\t    return mean(-sum(\n\t        one_hot(y_true, num_classes) * log_softmax(y_pred, dim=-1),\n\t        dim=-1\n\t    ), dim=-1)\n\tdef mse_loss(y_pred, y_true):\n", "    from torch import mean\n\t    return mean((y_pred - y_true)**2, dim=-1)\n\tdef mask_loss(y_pred, y_true):\n\t    from torch import abs, exp, log1p, maximum, zeros_like\n\t    return maximum(y_pred, zeros_like(y_pred)) - y_pred * y_true + log1p(exp(-abs(y_pred)))\n\tdef dual_loss(dual_y, inputs, alpha, device='cpu'):\n\t    from torch import mean, sum, take_along_dim\n\t    from torch.linalg import vector_norm\n\t    for inp in inputs:\n\t        if inp.name == 'adj':\n", "            adj = inp.data.to(device)\n\t        elif inp.name == 'A':\n\t            weights = inp.data.to(device)\n\t        elif inp.name == 's':\n\t            source = inp.data.to(device)\n\t        elif inp.name == 't':\n\t            target = inp.data.to(device)\n\t    source_idxs = source.argmax(dim=-1, keepdims=True)\n\t    target_idxs = target.argmax(dim=-1, keepdims=True)\n\t    y_s = take_along_dim(dual_y, source_idxs, dim=1)\n", "    y_t = take_along_dim(dual_y, target_idxs, dim=1)\n\t    dual_y = dual_y.unsqueeze(-1)\n\t    # main objective: max y(t) - y(s)\n\t    e1 = (y_t - y_s).unsqueeze(-1)\n\t    # penalty term constraint: \\forall u,v : y(v) - y(u) <= w_uv\n\t    e2 = dual_y.permute((0, 2, 1)) - dual_y\n\t    e2 = e2 * adj\n\t    e2 = (e2 - weights) * (e2 > weights).float()\n\t    # penalty term constraint: \\forall u: y(u) >= 0\n\t    e3 = -dual_y * ((dual_y < 0) * 1.0)\n", "    return -e1.mean() + mean(sum(e2, dim=-1)) + mean(sum(e3, dim=-1)) + alpha * vector_norm(e1, ord=2)**2\n\tdef max_flow(x, inputs, device='cpu'):\n\t    from torch import bmm, mean, sum\n\t    batch_len, num_nodes, _ = x.shape\n\t    for inp in inputs:\n\t        if inp.name == 's':\n\t            source = inp.data.to(device)\n\t        elif inp.name == 't':\n\t            target = inp.data.to(device)\n\t    # main objective: max sum_{(s,v) \\in E} x_sv\n", "    e1 = bmm(source.unsqueeze(1), x).squeeze()\n\t    e2 = bmm(target.unsqueeze(1), x).squeeze()\n\t    e1 = -mean(sum(e1, dim=-1))\n\t    e2 = mean(sum(e2, dim=-1))\n\t    return e1 + e2\n\tdef min_cut(S, inputs, device='cpu', reducer=torch.mean):\n\t    import torch\n\t    for inp in inputs:\n\t        if inp.name == 'A':\n\t            capacity = inp.data.to(device)\n", "        elif inp.name == 's':\n\t            source = inp.data.to(device)\n\t        elif inp.name == 't':\n\t            target = inp.data.to(device)\n\t    num_cuts = S.shape[-1]\n\t    S = S.softmax(-1)\n\t    S_t = S.transpose(1, 2)\n\t    l_cut = _3d_trace(S_t @ capacity @ S) / _3d_trace(S_t @ _3d_diag(capacity.sum(-1)) @ S)\n\t    l_ort = torch.linalg.matrix_norm(S_t @ S / torch.linalg.matrix_norm(S_t @ S, keepdims=True) -\n\t                                     torch.eye(num_cuts, device=device) / torch.tensor(num_cuts, device=device).sqrt()\n", "                                     )\n\t    source = torch.bmm(source.unsqueeze(1), S).squeeze()\n\t    target = torch.bmm(target.unsqueeze(1), S).squeeze()\n\t    l_dot = (source * target).sum(-1)  # dot-product\n\t    loss = -l_cut + l_ort + l_dot\n\t    return reducer(loss) if reducer else loss, {\n\t        \"l_cut\": reducer(-l_cut).detach() if reducer else (-l_cut).detach(),\n\t        \"l_ort\": reducer(l_ort).detach() if reducer else l_ort.detach(),\n\t        \"l_dot\": reducer(l_dot).detach() if reducer else l_dot.detach()\n\t    }\n", "def hint_loss(preds, truth, feedback, alpha, device):\n\t    import torch\n\t    import numpy as np\n\t    losses = []\n\t    hint_mask = []\n\t    adj = adj_mat(feedback.features).to(device)\n\t    for i in range(truth.data.shape[0] - 1):\n\t        y = truth.data[i + 1].to(device)\n\t        y_pred = preds[i][truth.name]\n\t        h_mask = (y_pred != clrs.OutputClass.MASKED) * 1.0\n", "        if truth.type_ == _Type.SCALAR:\n\t            loss = (y_pred - (y * adj))**2\n\t            # if truth.name == \"f_h\":\n\t            #    loss = alpha * loss + (1-alpha) * max_flow(y_pred, feedback.features.inputs, device=device\n\t            if truth.name == \"f_h\":\n\t                hint_mask.append(h_mask.all(-1).all(-1))\n\t                loss = (loss * h_mask).sum(-1).sum(-1) / adj.sum(-1).sum(-1).to(device)\n\t            else:\n\t                hint_mask.append(h_mask.all(-1))\n\t        elif truth.type_ == _Type.MASK:\n", "            hint_mask.append(h_mask.all(-1))\n\t            loss = mask_loss(y_pred, y)\n\t            mask = (truth.data[i + 1] != _OutputClass.MASKED).float().to(device)\n\t            mask *= h_mask\n\t            loss = torch.sum(loss * mask, dim=-1) / (torch.sum(mask, dim=-1) + EPS)\n\t        elif truth.type_ == _Type.MASK_ONE:\n\t            loss = -torch.sum(y * torch.nn.functional.log_softmax(y_pred, dim=-1) * h_mask, dim=-1)\n\t        elif truth.type_ == _Type.POINTER:\n\t            from torch.nn.functional import log_softmax, one_hot\n\t            hint_mask.append(h_mask.all(-1).all(-1))\n", "            # cross entropy\n\t            loss = one_hot(y.long(), y_pred.shape[-1]) * log_softmax(y_pred, dim=-1)\n\t            loss = -torch.sum(loss * h_mask, dim=-1).mean(-1)\n\t        elif truth.type_ == _Type.CATEGORICAL:\n\t            from torch.nn.functional import log_softmax, one_hot\n\t            # cross entropy\n\t            hint_mask.append(h_mask.all(-1).all(-1))\n\t            loss = one_hot(y.argmax(-1).long(), y_pred.shape[-1]) * log_softmax(y_pred, dim=-1)\n\t            loss = -torch.sum(loss * h_mask, dim=-1).mean(-1)\n\t        losses.append(loss)\n", "    losses = torch.stack(losses)\n\t    hint_mask = torch.stack(hint_mask) * 1.0\n\t    is_not_done = _is_not_done_broadcast(feedback.features.lengths, np.arange(truth.data.shape[0] - 1)[:, None], losses)\n\t    mask = is_not_done * _expand_to(hint_mask, len(is_not_done.shape))\n\t    return (losses * mask).sum() / mask.sum()\n\tdef output_loss(preds, truth, feedback, alpha, device):\n\t    import torch\n\t    y_pred = preds[truth.name]\n\t    y = truth.data.to(device)\n\t    adj = adj_mat(feedback.features).to(device)\n", "    if truth.name == \"unsup_lp_flow\":\n\t        return max_flow(y_pred, feedback.features.inputs, alpha=alpha, device=device)\n\t    elif truth.type_ == _Type.POINTER:\n\t        y = y.long()\n\t        return torch.mean(cross_entropy(y_pred, y, num_classes=y_pred.shape[-1]))\n\t    elif truth.type_ == _Type.CATEGORICAL:\n\t        return torch.mean(cross_entropy(y_pred, y.argmax(-1).long(), num_classes=y_pred.shape[-1]))\n\t    elif truth.location == _Location.EDGE and truth.type_ == _Type.SCALAR:\n\t        loss = ((y_pred - (y * adj))**2).sum() / adj_mat(feedback.features).sum()\n\t        # if truth.name == \"f\":\n", "        #    loss = alpha * loss + (1-alpha) * max_flow(y_pred, feedback.features.inputs, device=device)\n\t        return loss\n\t    assert False\n\tdef _capacity_constraint(pred, inputs, device):\n\t    for inp in inputs:\n\t        if inp.name == 'A':\n\t            capacity = inp.data.to(device)\n\t    return pred * (pred > capacity) * 1.0\n\tdef _3d_trace(A):\n\t    return A.diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)\n", "def _3d_diag(A):\n\t    return A.diag_embed(offset=0, dim1=-2, dim2=-1)\n\tdef _is_not_done_broadcast(lengths, i, tensor):\n\t    import torch\n\t    is_not_done = torch.as_tensor((lengths > i + 1) * 1.0, dtype=torch.float32).to(tensor.device)\n\t    while len(is_not_done.shape) < len(tensor.shape):\n\t        is_not_done = is_not_done.unsqueeze(-1)\n\t    return is_not_done\n\tdef _bfs_op_mask(hints):\n\t    for dp in hints:\n", "        if dp.name == '__is_bfs_op':\n\t            return dp.data\n"]}
{"filename": "nn/layers/mpnn.py", "chunked_list": ["import torch\n\tfrom typing import Callable, List\n\tfrom torch.nn import Linear, Module, Sequential\n\tfrom torch.nn import functional as F\n\tInf = 1e6\n\tclass MpnnConv(Module):\n\t    def __init__(self,\n\t                 in_channels: int,\n\t                 edge_channels: int,\n\t                 mid_channels: int,\n", "                 out_channels: int,\n\t                 net: Sequential,\n\t                 aggregator: str,\n\t                 mid_activation: Callable = None,\n\t                 activation: Callable = None,\n\t                 bias: bool = True):\n\t        super(MpnnConv, self).__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.m_1 = Linear(in_features=in_channels,\n", "                          out_features=mid_channels,\n\t                          bias=bias)\n\t        self.m_2 = Linear(in_features=in_channels,\n\t                          out_features=mid_channels,\n\t                          bias=bias)\n\t        self.m_e = Linear(in_features=edge_channels,\n\t                          out_features=mid_channels,\n\t                          bias=bias)\n\t        self.o1 = Linear(in_features=in_channels,\n\t                         out_features=out_channels,\n", "                         bias=bias)\n\t        self.o2 = Linear(in_features=mid_channels,\n\t                         out_features=out_channels,\n\t                         bias=bias)\n\t        self.net = net\n\t        self.mid_activation = mid_activation\n\t        self.activation = activation\n\t        self.aggregator = aggregator\n\t        if aggregator == 'max':\n\t            self.reduce = torch.amax\n", "        elif aggregator == 'sum':\n\t            self.reduce = torch.sum\n\t        elif aggregator == 'mean':\n\t            self.reduce = torch.mean\n\t        else:\n\t            raise NotImplementedError(\"Invalid type of aggregator function.\")\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n\t        self.m_1.reset_parameters()\n\t        self.m_2.reset_parameters()\n", "        self.m_e.reset_parameters()\n\t        self.o1.reset_parameters()\n\t        self.o2.reset_parameters()\n\t    def forward(self, x, adj, edge_attr):\n\t        \"\"\"\n\t        x : Tensor\n\t            node feature matrix (batch_size x num_nodes x num_nodes_features)\n\t        adj : Tensor\n\t            adjacency matrix (batch_size x num_nodes x num_nodes)\n\t        edge_attr : Tensor\n", "            edge attributes (batch_size x num_nodes x num_nodes x num_edge_features)\n\t        \"\"\"\n\t        batch_size, num_nodes, num_features = x.shape\n\t        _, _, _, num_edge_features = edge_attr.shape\n\t        msg_1 = self.m_1(x)\n\t        msg_2 = self.m_2(x)\n\t        msg_e = self.m_e(edge_attr)\n\t        msg = (\n\t            msg_1.unsqueeze(1) +\n\t            msg_2.unsqueeze(2) +\n", "            msg_e\n\t        )\n\t        if self.net is not None:\n\t            msg = self.net(F.relu(msg))\n\t        if self.mid_activation is not None:\n\t            msg = self.mid_activation(msg)\n\t        if self.aggregator == \"mean\":\n\t            msg = (msg * adj.unsqueeze(-1)).sum(1)\n\t            msg = msg / torch.sum(adj, dim=-1, keepdims=True)\n\t        elif self.aggregator == \"max\":\n", "            max_arg = torch.where(adj.unsqueeze(-1).bool(),\n\t                                  msg,\n\t                                  torch.tensor(-Inf).to(msg.device))\n\t            msg = self.reduce(max_arg, dim=1)\n\t        else:\n\t            msg = self.reduce(msg * adj.unsqueeze(-1), dim=1)\n\t        h_1 = self.o1(x)\n\t        h_2 = self.o2(msg)\n\t        out = h_1 + h_2\n\t        if self.activation is not None:\n", "            out = self.activation(out)\n\t        return out\n\tclass SparseMpnnConv(torch.nn.Module):\n\t    def __init__(self,\n\t                 in_channels: int,\n\t                 edge_channels: int,\n\t                 mid_channels: int,\n\t                 out_channels: int,\n\t                 net: Sequential,\n\t                 aggregator: str,\n", "                 devices: List[str] = ['cpu', 'cpu'],\n\t                 mid_activation: Callable = None,\n\t                 activation: Callable = None,\n\t                 bias: bool = True):\n\t        super().__init__()\n\t        from torch_geometric.nn.aggr import MaxAggregation\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.m_1 = Linear(in_features=in_channels,\n\t                          out_features=mid_channels,\n", "                          bias=bias)\n\t        self.m_2 = Linear(in_features=in_channels,\n\t                          out_features=mid_channels,\n\t                          bias=bias)\n\t        self.m_e = Linear(in_features=edge_channels,\n\t                          out_features=mid_channels,\n\t                          bias=bias)\n\t        self.o1 = Linear(in_features=in_channels,\n\t                         out_features=out_channels,\n\t                         bias=bias)\n", "        self.o2 = Linear(in_features=mid_channels,\n\t                         out_features=out_channels,\n\t                         bias=bias)\n\t        self.net = net\n\t        self.mid_activation = mid_activation\n\t        self.activation = activation\n\t        self.devices = devices\n\t        self.aggregator = aggregator\n\t        assert aggregator == 'max', \"Invalid type of aggregator function.\"\n\t        self.reduce = MaxAggregation()\n", "    def reset_parameters(self):\n\t        self.m_1.reset_parameters()\n\t        self.m_2.reset_parameters()\n\t        self.m_e.reset_parameters()\n\t        self.o1.reset_parameters()\n\t        self.o2.reset_parameters()\n\t    def forward(self, x, adj_t, edge_attr, reporter=None):\n\t        \"\"\"\n\t        x : Tensor\n\t            node feature matrix (num_nodes x num_nodes_features)\n", "        adj_t : SparseTensor | Tensor\n\t            sparse adjacency matrix (num_nodes x num_nodes)\n\t        edge_attr : Tensor\n\t            edge attributes (num_edges x num_edge_features)\n\t        \"\"\"\n\t        x = x.to(self.devices[0])\n\t        edge_attr = edge_attr.to(self.devices[0])\n\t        adj_t = adj_t.to(self.devices[1])\n\t        msg_1 = self.m_1.to(self.devices[0])(x)\n\t        msg_2 = self.m_2.to(self.devices[0])(x)\n", "        msg_e = self.m_e.to(self.devices[0])(edge_attr)\n\t        # for each (u, v) : m1(u) + m2(v) + m_e(uv)\n\t        msg = torch.relu(\n\t            msg_1[adj_t.storage.row()] +\n\t            msg_2[adj_t.storage.col()] +\n\t            msg_e\n\t        )\n\t        msg = self.net.to(self.devices[1])(\n\t            msg.to(self.devices[1])\n\t        ).to(self.devices[1])\n", "        msg = self.reduce(msg,\n\t                          adj_t.storage.row(),\n\t                          ptr=adj_t.storage.rowptr(),\n\t                          dim_size=adj_t.storage.row().max().item() + 1,\n\t                          dim=-2)\n\t        h_1 = self.o1.to(self.devices[1])(x.to(self.devices[1]))\n\t        h_2 = self.o2.to(self.devices[1])(msg)\n\t        out = h_1 + h_2\n\t        if self.activation is not None:\n\t            out = self.activation(out)\n", "        return out\n\t# class SparseMpnnConv(MessagePassing):\n\t#     def __init__(self,\n\t#                  in_channels: int,\n\t#                  edge_channels: int,\n\t#                  mid_channels: int,\n\t#                  out_channels: int,\n\t#                  net: Sequential,\n\t#                  aggregator: str,\n\t#                  mid_activation: Callable = None,\n", "#                  activation: Callable = None,\n\t#                  bias: bool = True):\n\t#         super().__init__(aggr=aggregator)\n\t#         self.in_channels = in_channels\n\t#         self.out_channels = out_channels\n\t#         self.m_1 = Linear(in_features=in_channels,\n\t#                           out_features=mid_channels,\n\t#                           bias=bias)\n\t#         self.m_2 = Linear(in_features=in_channels,\n\t#                           out_features=mid_channels,\n", "#                           bias=bias)\n\t#         self.m_e = Linear(in_features=edge_channels,\n\t#                           out_features=mid_channels,\n\t#                           bias=bias)\n\t#         self.o1 = Linear(in_features=in_channels,\n\t#                          out_features=out_channels,\n\t#                          bias=bias)\n\t#         self.o2 = Linear(in_features=mid_channels,\n\t#                          out_features=out_channels,\n\t#                          bias=bias)\n", "#         self.net = net\n\t#         self.mid_activation = mid_activation\n\t#         self.activation = activation\n\t#         self.aggregator = aggregator\n\t#         if aggregator == 'max':\n\t#             self.reduce = torch.amax\n\t#         elif aggregator == 'sum':\n\t#             self.reduce = torch.sum\n\t#         elif aggregator == 'mean':\n\t#             self.reduce = torch.mean\n", "#         else:\n\t#             raise NotImplementedError(\"Invalid type of aggregator function.\")\n\t#     def reset_parameters(self):\n\t#         self.m_1.reset_parameters()\n\t#         self.m_2.reset_parameters()\n\t#         self.m_e.reset_parameters()\n\t#         self.o1.reset_parameters()\n\t#         self.o2.reset_parameters()\n\t#     def message(self, x_i, x_j, e_ij):\n\t#         msg = self.m_1(x_i) + self.m_2(x_j) + self.m_e(e_ij)\n", "#         msg = self.net(F.relu(msg))\n\t#         return msg\n\t#     def forward(self, x, adj_t, edge_attr):\n\t#         \"\"\"\n\t#         x : Tensor\n\t#             node feature matrix (num_nodes x num_nodes_features)\n\t#         adj_t : SparseTensor | Tensor\n\t#             sparse adjacency matrix (num_nodes x num_nodes) |\n\t#             coo coordinates tensor (2 x num_edges)\n\t#         edge_attr : Tensor\n", "#             edge attributes (num_edges x num_edge_features)\n\t#         \"\"\"\n\t#         msg = self.propagate(edge_index=adj_t,\n\t#                              x=x,\n\t#                              e_ij=edge_attr)\n\t#         h_1 = self.o1(x)\n\t#         h_2 = self.o2(msg)\n\t#         out = h_1 + h_2\n\t#         if self.activation is not None:\n\t#             out = self.activation(out)\n", "#         return out\n"]}
{"filename": "nn/layers/__init__.py", "chunked_list": ["from .mpnn import MpnnConv, SparseMpnnConv  # noqa\n"]}
{"filename": "nn/models/epd.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom nn import losses as loss\n\tfrom nn.models.impl import _dimensions, _expand, _hints_i, _own_hints_i\n\tfrom nn.models.impl import decoders\n\tfrom nn.models.impl import encoders\n\tfrom nn.models.impl import processors\n\tfrom utils import is_not_done_broadcast\n\tfrom utils.data import adj_mat, edge_attr_mat\n\tfrom typing import Callable, Dict, List\n", "from torch.nn import Module, ModuleDict\n\tfrom torch.nn.functional import relu\n\tResult = Dict[str, clrs.DataPoint]\n\t_Feedback = clrs.Feedback\n\t_Location = clrs.Location\n\t_OutputClass = clrs.OutputClass\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Type = clrs.Type\n\t_Type = clrs.Type\n", "_DataPoint = clrs.DataPoint\n\tclass EncodeProcessDecode(clrs.Model):\n\t    def __init__(self,\n\t                 spec: _Spec,\n\t                 num_hidden: int,\n\t                 optim_fn: Callable,\n\t                 dummy_trajectory: _Feedback,\n\t                 alpha: float,\n\t                 processor: str,\n\t                 aggregator: str,\n", "                 no_feats: List = ['adj'],\n\t                 add_noise: bool = False,\n\t                 decode_hints: bool = True,\n\t                 encode_hints: bool = True,\n\t                 max_steps: int = None):\n\t        super().__init__(spec=spec)\n\t        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\t        self.net_ = EncodeProcessDecode_Impl(spec=spec,\n\t                                             dummy_trajectory=dummy_trajectory,\n\t                                             num_hidden=num_hidden,\n", "                                             encode_hints=encode_hints,\n\t                                             decode_hints=decode_hints,\n\t                                             processor=processor,\n\t                                             aggregator=aggregator,\n\t                                             max_steps=max_steps,\n\t                                             no_feats=no_feats,\n\t                                             add_noise=add_noise,\n\t                                             device=self.device)\n\t        self.optimizer = optim_fn(self.net_.parameters())\n\t        self.alpha = alpha\n", "        self.no_feats = lambda x: x in no_feats or x.startswith('__')\n\t        self.encode_hints = encode_hints\n\t        self.decode_hints = decode_hints\n\t    def dump_model(self, path):\n\t        torch.save(self.net_.state_dict(), path)\n\t    def restore_model(self, path, device):\n\t        self.net_.load_state_dict(torch.load(path, map_location=device))\n\t    def _train_step(self, feedback: _Feedback):\n\t        self.net_.train()\n\t        self.optimizer.zero_grad()\n", "        preds, hint_preds = self.net_(feedback.features)\n\t        total_loss = 0.0\n\t        n_hints = 0\n\t        if self.decode_hints:\n\t            hint_loss = 0.0\n\t            for truth in feedback.features.hints:\n\t                if self.no_feats(truth.name):\n\t                    continue\n\t                n_hints += 1\n\t                hint_loss += loss.hint_loss(hint_preds, truth, feedback, self.alpha, self.device)\n", "            total_loss += hint_loss / n_hints\n\t        for truth in feedback.outputs:\n\t            total_loss += loss.output_loss(preds, truth, feedback, self.alpha, self.device)\n\t        total_loss.backward()\n\t        self.optimizer.step()\n\t        return total_loss.item()\n\t    def feedback(self, feedback: _Feedback) -> float:\n\t        loss = self._train_step(feedback)\n\t        return loss\n\t    @torch.no_grad()\n", "    def predict(self, features: clrs.Features) -> Result:\n\t        self.net_.eval()\n\t        raw_preds, aux = self.net_(features)\n\t        preds = decoders.postprocess(raw_preds, self._spec)\n\t        return preds, (raw_preds, aux)\n\t    @torch.no_grad()\n\t    def verbose_loss(self, feedback: _Feedback, preds, aux_preds):\n\t        losses = {}\n\t        total_loss = 0\n\t        n_hints = 0\n", "        for truth in feedback.features.hints:\n\t            if self.no_feats(truth.name):\n\t                continue\n\t            n_hints += 1\n\t            losses[\"aux_\" + truth.name] = loss.hint_loss(aux_preds, truth, feedback, self.alpha, self.device).cpu().item()\n\t            total_loss += losses[\"aux_\" + truth.name]\n\t        total_loss /= n_hints\n\t        for truth in feedback.outputs:\n\t            total_loss += loss.output_loss(preds, truth, feedback, self.alpha, self.device)\n\t        return losses, total_loss.item()\n", "class EncodeProcessDecode_Impl(Module):\n\t    def __init__(self,\n\t                 spec: _Spec,\n\t                 dummy_trajectory: _Feedback,\n\t                 num_hidden: int,\n\t                 encode_hints: bool,\n\t                 decode_hints: bool,\n\t                 processor: str,\n\t                 aggregator: str,\n\t                 no_feats: List,\n", "                 add_noise: bool = False,\n\t                 bias: bool = True,\n\t                 max_steps: int = None,\n\t                 device: str = 'cpu'):\n\t        super().__init__()\n\t        self.num_hidden = num_hidden\n\t        self.decode_hints = decode_hints\n\t        self.encoders = ModuleDict({})\n\t        self.decoders = ModuleDict({})\n\t        self.hint_decoders = ModuleDict({})\n", "        self.max_steps = max_steps\n\t        self.no_feats = lambda x: x in no_feats or x.startswith('__') # noqa\n\t        for inp in dummy_trajectory.features.inputs:\n\t            if self.no_feats(inp.name):\n\t                continue\n\t            self.encoders[inp.name] = encoders.Encoder(\n\t                in_features=_expand(inp.data, inp.location).shape[-1],\n\t                out_features=self.num_hidden,\n\t                bias=False)\n\t        if encode_hints:\n", "            for hint in dummy_trajectory.features.hints:\n\t                if self.no_feats(hint.name):\n\t                    continue\n\t                self.encoders[hint.name] = encoders.Encoder(\n\t                    in_features=_expand(hint.data[0], hint.location).shape[-1],\n\t                    out_features=self.num_hidden,\n\t                    bias=False)\n\t        self.process = processors.PROCESSORS[processor](num_hidden=num_hidden,\n\t                                                        aggregator=aggregator,\n\t                                                        activation=relu)\n", "        if decode_hints:\n\t            for hint in dummy_trajectory.features.hints:\n\t                if self.no_feats(hint.name):\n\t                    continue\n\t                self.hint_decoders[hint.name] = decoders.new_decoder(spec[hint.name],\n\t                                                                     num_hidden,\n\t                                                                     num_classes=hint.data.shape[-1])\n\t        for out in dummy_trajectory.outputs:\n\t            self.decoders[out.name] = decoders.new_decoder(spec[out.name],\n\t                                                           num_hidden,\n", "                                                           num_classes=out.data.shape[-1])\n\t        self.device = device\n\t        self.spec = spec\n\t        self.encode_hints = encode_hints\n\t        self.to(device)\n\t    def step(self, trajectories, h, adj):\n\t        # ~~~ init ~~~\n\t        batch_size, num_nodes = _dimensions(trajectories[0])\n\t        x = torch.zeros((batch_size, num_nodes, self.num_hidden)).to(self.device)\n\t        edge_attr = torch.zeros((batch_size, num_nodes, num_nodes, self.num_hidden)).to(self.device)\n", "        # ~~~ encode ~~~\n\t        for trajectory in trajectories:\n\t            for dp in trajectory:\n\t                if self.no_feats(dp.name) or dp.name not in self.encoders:\n\t                    continue\n\t                data = encoders.preprocess(dp, num_nodes).to(self.device)\n\t                encoder = self.encoders[dp.name]\n\t                x = encoders.accum_node_fts(encoder, dp, data, x)\n\t                edge_attr = encoders.accum_edge_fts(encoder, dp, data, edge_attr, adj)\n\t                # graph_fts = encoders.accum_graph_fts(encoder, dp, data, graph_fts)\n", "        # ~~~ process ~~~\n\t        z = torch.cat([x, h], dim=-1)\n\t        hiddens = self.process(z, adj, edge_attr)\n\t        h_t = torch.cat([z, hiddens], dim=-1)\n\t        self.h_t = h_t\n\t        self.edge_attr = edge_attr\n\t        # ~~~ decode ~~~\n\t        if not self.decode_hints:\n\t            hint_preds = {}\n\t        else:\n", "            hint_preds = {\n\t                name: decoders.decode_from_latents(\n\t                    name,\n\t                    self.spec[name],\n\t                    self.hint_decoders[name],\n\t                    h_t,\n\t                    adj,\n\t                    edge_attr)\n\t                for name in self.hint_decoders.keys()\n\t            }\n", "        output_preds = {\n\t            name: decoders.decode_from_latents(\n\t                name,\n\t                self.spec[name],\n\t                self.decoders[name],\n\t                h_t,\n\t                adj,\n\t                edge_attr)\n\t            for name in self.decoders.keys()\n\t        }\n", "        return output_preds, hiddens, hint_preds\n\t    def forward(self, features):\n\t        output_preds = {}\n\t        hint_preds = []\n\t        num_steps = self.max_steps if self.max_steps else features.hints[0].data.shape[0] - 1\n\t        batch_size, num_nodes = _dimensions(features.inputs)\n\t        h = torch.zeros((batch_size, num_nodes, self.num_hidden)).to(self.device)\n\t        adj = adj_mat(features).to(self.device)\n\t        A = edge_attr_mat(features).to(self.device)\n\t        for i in range(num_steps):\n", "            cur_hint = _hints_i(features.hints, i) if self.training or i == 0 else _own_hints_i(hint_preds[-1], self.spec, features, i)\n\t            trajectories = [features.inputs]\n\t            if self.encode_hints:\n\t                trajectories.append(cur_hint)\n\t            cand, h, h_preds = self.step(trajectories, h, adj)\n\t            if \"f\" in cand:\n\t                cand[\"f\"] = A * cand[\"f\"]\n\t            if \"f_h\" in h_preds:\n\t                h_preds[\"f_h\"] = A * h_preds[\"f_h\"]\n\t            hint_preds.append(h_preds)\n", "            for name in cand:\n\t                if i == 0 or features.lengths.sum() == 0:\n\t                    # if the algorithm has no hints, bypass the following check\n\t                    output_preds[name] = cand[name]\n\t                else:\n\t                    is_not_done = is_not_done_broadcast(features.lengths, i, cand[name])\n\t                    output_preds[name] = is_not_done * cand[name] + \\\n\t                        (1.0 - is_not_done) * output_preds[name]\n\t        return output_preds, hint_preds\n"]}
{"filename": "nn/models/mf_net_pipeline.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom nn import losses as loss\n\tfrom nn.models.impl import _dimensions, _bfs_op_mask, _expand_to, \\\n\t    _get_fts, _hints_i, _own_hints_i, _reset_hints\n\tfrom nn.models.impl import decoders\n\tfrom nn.models.epd import EncodeProcessDecode_Impl as Net\n\tfrom random import random\n\tfrom typing import Callable, Dict, List\n\tfrom utils import is_not_done_broadcast\n", "from utils.data import adj_mat, edge_attr_mat\n\tResult = Dict[str, clrs.DataPoint]\n\t_INFINITY = 1e5\n\t_Feedback = clrs.Feedback\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n\t_Type = clrs.Type\n\t_DataPoint = clrs.DataPoint\n\tclass MF_Net(clrs.Model):\n", "    def __init__(self,\n\t                 spec: _Spec,\n\t                 num_hidden: int,\n\t                 optim_fn: Callable,\n\t                 dummy_trajectory: _Feedback,\n\t                 alpha: float,\n\t                 processor: str,\n\t                 aggregator: str,\n\t                 no_feats: List = ['adj'],\n\t                 add_noise: bool = False,\n", "                 decode_hints: bool = True,\n\t                 encode_hints: bool = True,\n\t                 max_steps: int = None):\n\t        super().__init__(spec=spec)\n\t        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\t        self.net_ = MFNet_Impl(spec=spec,\n\t                               dummy_trajectory=dummy_trajectory,\n\t                               processor=processor,\n\t                               aggregator=aggregator,\n\t                               num_hidden=num_hidden,\n", "                               encode_hints=encode_hints,\n\t                               decode_hints=decode_hints,\n\t                               max_steps=max_steps,\n\t                               no_feats=no_feats,\n\t                               add_noise=add_noise,\n\t                               device=self.device)\n\t        self.optimizer = optim_fn(self.net_.parameters())\n\t        self.alpha = alpha\n\t        self.no_feats = lambda x: x in no_feats or x.startswith('__')\n\t        self.encode_hints = encode_hints\n", "        self.decode_hints = decode_hints\n\t    def dump_model(self, path):\n\t        torch.save(self.net_.state_dict(), path)\n\t    def restore_model(self, path, device):\n\t        self.net_.load_state_dict(torch.load(path, map_location=device))\n\t    def _train_step(self, feedback: _Feedback):\n\t        self.net_.train()\n\t        self.optimizer.zero_grad()\n\t        preds, hint_preds = self.net_(feedback.features)\n\t        total_loss = 0.0\n", "        n_hints = 0\n\t        if self.decode_hints:\n\t            hint_loss = 0.0\n\t            for truth in feedback.features.hints:\n\t                if self.no_feats(truth.name):\n\t                    continue\n\t                n_hints += 1\n\t                hint_loss += loss.hint_loss(hint_preds, truth, feedback, self.alpha, self.device)\n\t            total_loss += hint_loss / n_hints\n\t        for truth in feedback.outputs:\n", "            total_loss += loss.output_loss(preds, truth, feedback, self.alpha, self.device)\n\t        total_loss.backward()\n\t        self.optimizer.step()\n\t        return total_loss.item()\n\t    def feedback(self, feedback: _Feedback) -> float:\n\t        loss = self._train_step(feedback)\n\t        return loss\n\t    @torch.no_grad()\n\t    def predict(self, features: clrs.Features) -> Result:\n\t        self.net_.eval()\n", "        raw_preds, aux = self.net_(features)\n\t        preds = decoders.postprocess(raw_preds, self._spec)\n\t        return preds, (raw_preds, aux)\n\t    @torch.no_grad()\n\t    def verbose_loss(self, feedback: _Feedback, preds, aux_preds):\n\t        losses = {}\n\t        total_loss = 0\n\t        n_hints = 0\n\t        for truth in feedback.features.hints:\n\t            if self.no_feats(truth.name):\n", "                continue\n\t            n_hints += 1\n\t            losses[\"aux_\" + truth.name] = loss.hint_loss(aux_preds, truth, feedback, self.alpha, self.device).cpu().item()\n\t            total_loss += losses[\"aux_\" + truth.name]\n\t        total_loss /= n_hints\n\t        for truth in feedback.outputs:\n\t            total_loss += loss.output_loss(preds, truth, feedback, self.alpha, self.device)\n\t        return losses, total_loss.item()\n\tclass MFNet_Impl(torch.nn.Module):\n\t    def __init__(self,\n", "                 spec: _Spec,\n\t                 dummy_trajectory: _Feedback,\n\t                 num_hidden: int,\n\t                 encode_hints: bool,\n\t                 decode_hints: bool,\n\t                 processor: str,\n\t                 aggregator: str,\n\t                 no_feats: List,\n\t                 add_noise: bool = False,\n\t                 bias: bool = True,\n", "                 max_steps: int = None,\n\t                 load_path: str = None,\n\t                 annealing: bool = True,\n\t                 device: str = 'cpu'):\n\t        super().__init__()\n\t        self.num_hidden = num_hidden\n\t        self.decode_hints = decode_hints\n\t        self.max_steps = max_steps\n\t        self.no_feats = lambda x: x in no_feats or x.startswith('__')  # noqa\n\t        self.bfs_net = Net(spec,\n", "                           dummy_trajectory,\n\t                           num_hidden,\n\t                           encode_hints,\n\t                           decode_hints,\n\t                           processor,\n\t                           aggregator,\n\t                           no_feats,\n\t                           add_noise=add_noise,\n\t                           device=device)\n\t        self.flow_net = Net(spec,\n", "                            dummy_trajectory,\n\t                            num_hidden,\n\t                            encode_hints,\n\t                            decode_hints,\n\t                            processor,\n\t                            aggregator,\n\t                            no_feats,\n\t                            add_noise=add_noise,\n\t                            device=device)\n\t        c = _get_fts(dummy_trajectory.features.hints, name='c_h')\n", "        if c is not None:\n\t            print(c.data.shape[2])\n\t            self.mincut_net = Net(\n\t                spec,\n\t                dummy_trajectory,\n\t                num_hidden,\n\t                encode_hints=False,\n\t                decode_hints=False,\n\t                processor=processor,\n\t                aggregator=aggregator,\n", "                max_steps=c.data.shape[2] + 1,\n\t                no_feats=no_feats,\n\t                device=device\n\t            )\n\t            del self.flow_net.decoders['c']\n\t            del self.flow_net.hint_decoders['c_h']\n\t            del self.bfs_net.hint_decoders['c_h']\n\t            del self.mincut_net.decoders['f']\n\t        self.is_annealing_enabled = annealing\n\t        self.annealing_state = 0\n", "        self.device = device\n\t        self.spec = spec\n\t        self.encode_hints = encode_hints\n\t        self.to(device)\n\t    def op(self, trajectories, h_bfs, adj, is_bfs_op):\n\t        _, h_bfs, h_preds_bfs = self.bfs_net.step(trajectories, h_bfs, adj)\n\t        cand, _, h_preds_fnet = self.flow_net.step(trajectories, h_bfs.detach(), adj)\n\t        for ignored_key in ['f_h', 'c_h']:\n\t            if ignored_key in self.bfs_net.hint_decoders.keys():\n\t                h_preds_bfs[ignored_key] = h_preds_bfs[ignored_key].new_full(h_preds_bfs[ignored_key].shape, clrs.OutputClass.MASKED)\n", "        for ignored_key in ['reach_h', 'pi_h']:\n\t            if ignored_key in self.flow_net.hint_decoders.keys():\n\t                h_preds_fnet[ignored_key] = h_preds_fnet[ignored_key].new_full(h_preds_fnet[ignored_key].shape, clrs.OutputClass.MASKED)\n\t        if self.decode_hints:\n\t            hint_preds = {}\n\t            idx_bfs = is_bfs_op.flatten().nonzero()\n\t            idx_f = (1 - is_bfs_op).flatten().nonzero()\n\t            for name in h_preds_bfs.keys():\n\t                n_dims = len(h_preds_bfs[name].shape)\n\t                hint_preds[name] = torch.empty_like(h_preds_bfs[name]).to(self.device)\n", "                hint_preds[name].fill_(clrs.OutputClass.MASKED)\n\t                hint_preds[name][idx_bfs] = h_preds_bfs[name][idx_bfs]\n\t                hint_preds[name][idx_f] = h_preds_fnet[name][idx_f]\n\t        # attempt to reset h_bfs\n\t        reset = torch.zeros_like(h_bfs)\n\t        h_bfs = h_bfs.masked_scatter(_expand_to(is_bfs_op.bool(), n_dims), reset)\n\t        assert h_bfs[is_bfs_op.flatten().nonzero()].sum().item() == 0\n\t        for name in cand.keys():\n\t            n_dims = len(cand[name].shape)\n\t            mask = torch.zeros_like(cand[name])\n", "            mask.fill_(clrs.OutputClass.MASKED)\n\t            cand[name] = cand[name].masked_scatter(_expand_to(is_bfs_op.bool(), n_dims), mask)\n\t        return cand, h_bfs, hint_preds\n\t    def forward(self, features):\n\t        output_preds = {}\n\t        hint_preds = []\n\t        num_steps = self.max_steps if self.max_steps else features.hints[0].data.shape[0] - 1\n\t        batch_size, num_nodes = _dimensions(features.inputs)\n\t        h_bfs = torch.zeros((batch_size, num_nodes, self.num_hidden)).to(self.device)\n\t        adj = adj_mat(features).to(self.device)\n", "        A = edge_attr_mat(features).to(self.device)\n\t        def next_hint(i):\n\t            use_teacher_forcing = self.training\n\t            first_step = i == 0\n\t            if self.is_annealing_enabled:\n\t                self.annealing_state += 1\n\t                use_teacher_forcing = use_teacher_forcing and not(random() > (0.999 ** self.annealing_state))\n\t            if use_teacher_forcing or first_step:\n\t                return _hints_i(features.hints, i)\n\t            else:\n", "                return _own_hints_i(last_valid_hints, self.spec, features, i)\n\t        prev_is_flow_op = None\n\t        # if not self.training:\n\t        last_valid_hints = {hint.name: hint.data.to(self.device) for hint in next_hint(0)}\n\t        last_valid_hints['pi_h'] = torch.nn.functional.one_hot(last_valid_hints['pi_h'].long(),\n\t                                                               num_nodes).float()\n\t        del last_valid_hints['__is_bfs_op']\n\t        if self.mincut_net is not None:\n\t            mc_out, _ = self.mincut_net(features)\n\t            output_preds['c'] = mc_out['c']\n", "            last_valid_hints['c_h'] = mc_out['c']\n\t        for i in range(num_steps):\n\t            # ~~~ init ~~~\n\t            trajectories = [features.inputs]\n\t            if self.encode_hints:\n\t                cur_hint = next_hint(i)\n\t                if i > 0 and not self.training:\n\t                    assert prev_is_flow_op is not None\n\t                    first_bfs_step = prev_is_flow_op\n\t                    reach_h, pi_h = _reset_hints(cur_hint, _get_fts(features.inputs, \"s\").data)\n", "                    for hint in cur_hint:\n\t                        if hint.name == 'reach_h':\n\t                            hint.data[first_bfs_step.flatten().nonzero()] = reach_h.data.to(self.device)[first_bfs_step.flatten().nonzero()]\n\t                        elif hint.name == 'pi_h':\n\t                            hint.data[first_bfs_step.flatten().nonzero()] = pi_h.data.to(self.device)[first_bfs_step.flatten().nonzero()]\n\t                trajectories.append(cur_hint)\n\t            if self.decode_hints:\n\t                is_bfs_op = _bfs_op_mask(next_hint(i)).data.to(self.device)\n\t                is_flow_op = (1 - is_bfs_op)\n\t            else:\n", "                is_bfs_op = None\n\t            cand, h_bfs, h_preds = self.op(trajectories, h_bfs, adj, is_bfs_op)\n\t            if self.mincut_net is not None:\n\t                h_preds['c_h'] = mc_out['c']\n\t            if \"f\" in cand:\n\t                idx = is_flow_op.flatten().nonzero()\n\t                cand[\"f\"][idx] = (A * cand['f'])[idx]\n\t            if \"f_h\" in h_preds:\n\t                idx = is_flow_op.flatten().nonzero()\n\t                h_preds[\"f_h\"][idx] = (A * h_preds['f_h'])[idx]\n", "            # if not self.training:\n\t            for name in last_valid_hints.keys():\n\t                is_masked = (h_preds[name] == clrs.OutputClass.MASKED) * 1.0\n\t                last_valid_hints[name] = is_masked * last_valid_hints[name] + (1.0 - is_masked) * h_preds[name]\n\t            hint_preds.append(h_preds)\n\t            for name in cand:\n\t                if name not in output_preds or features.lengths.sum() == 0:\n\t                    output_preds[name] = cand[name]\n\t                else:\n\t                    is_not_done = is_not_done_broadcast(features.lengths, i, cand[name])\n", "                    mask = is_not_done * _expand_to(is_flow_op, len(is_not_done.shape))\n\t                    output_preds[name] = mask * cand[name] + \\\n\t                        (1.0 - mask) * output_preds[name]\n\t            prev_is_flow_op = is_flow_op\n\t        return output_preds, hint_preds\n"]}
{"filename": "nn/models/baselines.py", "chunked_list": ["import clrs\n\t_Features = clrs.Features\n\tclass UniformRandom:\n\t    def predict(self, features: _Features):\n\t        from numpy.random import rand\n\t        for inp in features.inputs:\n\t            if inp.location in [clrs.Location.NODE, clrs.Location.EDGE]:\n\t                batch_size, num_nodes = inp.data.shape[0], inp.data.shape[1]\n\t                break\n\t        return rand(batch_size, num_nodes)\n", "class NormalRandom:\n\t    def predict(self, features: _Features):\n\t        from numpy.random import randn\n\t        for inp in features.inputs:\n\t            if inp.location in [clrs.Location.NODE, clrs.Location.EDGE]:\n\t                batch_size, num_nodes = inp.data.shape[0], inp.data.shape[1]\n\t                break\n\t        return randn(batch_size, num_nodes)\n"]}
{"filename": "nn/models/__init__.py", "chunked_list": ["from .epd import EncodeProcessDecode  # noqa: F401\n\tfrom .mf_net import MF_Net  # noqa: F401\n\tfrom .mf_net_pipeline import MF_Net as MF_NetPipeline  # noqa: F401\n\tfrom .baselines import NormalRandom, UniformRandom  # noqa: F401\n"]}
{"filename": "nn/models/mf_net.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom nn import losses as loss\n\tfrom nn.models.impl import _dimensions, _bfs_op_mask, _expand_to, \\\n\t    _get_fts, _hints_i, _own_hints_i, _reset_hints\n\tfrom nn.models.impl import decoders\n\tfrom nn.models.epd import EncodeProcessDecode_Impl as Net\n\tfrom random import random\n\tfrom typing import Callable, Dict, List\n\tfrom utils import is_not_done_broadcast\n", "from utils.data import adj_mat, edge_attr_mat\n\tResult = Dict[str, clrs.DataPoint]\n\t_INFINITY = 1e5\n\t_Feedback = clrs.Feedback\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n\t_Type = clrs.Type\n\t_DataPoint = clrs.DataPoint\n\t_ANNEALING_STATE = 0\n", "class MF_Net(clrs.Model):\n\t    def __init__(self,\n\t                 spec: _Spec,\n\t                 num_hidden: int,\n\t                 optim_fn: Callable,\n\t                 dummy_trajectory: _Feedback,\n\t                 alpha: float,\n\t                 processor: str,\n\t                 aggregator: str,\n\t                 no_feats: List = ['adj'],\n", "                 add_noise: bool = False,\n\t                 decode_hints: bool = True,\n\t                 encode_hints: bool = True,\n\t                 max_steps: int = None,):\n\t        super().__init__(spec=spec)\n\t        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\t        self.net_ = MFNet_Impl(spec=spec,\n\t                               dummy_trajectory=dummy_trajectory,\n\t                               processor=processor,\n\t                               aggregator=aggregator,\n", "                               num_hidden=num_hidden,\n\t                               encode_hints=encode_hints,\n\t                               decode_hints=decode_hints,\n\t                               max_steps=max_steps,\n\t                               no_feats=no_feats,\n\t                               add_noise=add_noise,\n\t                               device=self.device)\n\t        self.optimizer = optim_fn(self.net_.parameters())\n\t        self.alpha = alpha\n\t        self.no_feats = lambda x: x in no_feats or x.startswith('__')\n", "        self.encode_hints = encode_hints\n\t        self.decode_hints = decode_hints\n\t        self.spec = spec\n\t    def dump_model(self, path):\n\t        torch.save(self.net_.state_dict(), path)\n\t    def restore_model(self, path, device):\n\t        self.net_.load_state_dict(torch.load(path, map_location=device))\n\t    def _train_step(self, feedback: _Feedback):\n\t        self.net_.train()\n\t        self.optimizer.zero_grad()\n", "        preds, hint_preds = self.net_(feedback.features)\n\t        total_loss = 0.0\n\t        n_hints = 0\n\t        if self.decode_hints:\n\t            hint_loss = 0.0\n\t            for truth in feedback.features.hints:\n\t                if self.no_feats(truth.name):\n\t                    continue\n\t                n_hints += 1\n\t                hint_loss += loss.hint_loss(hint_preds, truth, feedback, self.alpha, self.device)\n", "            total_loss += hint_loss / n_hints\n\t        for truth in feedback.outputs:\n\t            total_loss += loss.output_loss(preds, truth, feedback, self.alpha, self.device)\n\t        total_loss.backward()\n\t        self.optimizer.step()\n\t        return total_loss.item()\n\t    def feedback(self, feedback: _Feedback) -> float:\n\t        loss = self._train_step(feedback)\n\t        return loss\n\t    @torch.no_grad()\n", "    def predict(self, features: clrs.Features):\n\t        self.net_.eval()\n\t        raw_preds, aux = self.net_(features)\n\t        preds = decoders.postprocess(raw_preds, self.spec)\n\t        return preds, (raw_preds, aux)\n\t    @torch.no_grad()\n\t    def verbose_loss(self, feedback: _Feedback, preds, aux_preds):\n\t        losses = {}\n\t        total_loss = 0\n\t        n_hints = 0\n", "        for truth in feedback.features.hints:\n\t            if self.no_feats(truth.name):\n\t                continue\n\t            n_hints += 1\n\t            losses[\"aux_\" + truth.name] = loss.hint_loss(aux_preds, truth, feedback, self.alpha, self.device).cpu().item()\n\t            total_loss += losses[\"aux_\" + truth.name]\n\t        total_loss /= n_hints\n\t        for truth in feedback.outputs:\n\t            total_loss += loss.output_loss(preds, truth, feedback, self.alpha, self.device)\n\t        return losses, total_loss.item()\n", "class MFNet_Impl(torch.nn.Module):\n\t    def __init__(self,\n\t                 spec: _Spec,\n\t                 dummy_trajectory: _Feedback,\n\t                 num_hidden: int,\n\t                 encode_hints: bool,\n\t                 decode_hints: bool,\n\t                 processor: str,\n\t                 aggregator: str,\n\t                 no_feats: List,\n", "                 add_noise: bool = False,\n\t                 bias: bool = True,\n\t                 max_steps: int = None,\n\t                 load_path: str = None,\n\t                 annealing: bool = True,\n\t                 device: str = 'cpu'):\n\t        super().__init__()\n\t        self.num_hidden = num_hidden\n\t        self.decode_hints = decode_hints\n\t        self.max_steps = max_steps\n", "        self.no_feats = lambda x: x in no_feats or x.startswith('__')  # noqa\n\t        self.bfs_net = Net(spec, dummy_trajectory, num_hidden,\n\t                           encode_hints, decode_hints,\n\t                           processor, aggregator, no_feats,\n\t                           add_noise=add_noise,\n\t                           device=device)\n\t        self.flow_net = Net(spec, dummy_trajectory, num_hidden,\n\t                            encode_hints, decode_hints,\n\t                            processor, aggregator, no_feats,\n\t                            add_noise=add_noise,\n", "                            device=device)\n\t        del self.bfs_net.encoders['c_h']\n\t        self.is_annealing_enabled = annealing\n\t        self.annealing_state = 0\n\t        self.device = device\n\t        self.spec = spec\n\t        self.encode_hints = encode_hints\n\t        self.to(device)\n\t        self.hiddens = None\n\t    def op(self, trajectories, h_bfs, adj, is_bfs_op):\n", "        _, h_bfs, h_preds_bfs = self.bfs_net.step(trajectories, h_bfs, adj)\n\t        cand, hiddens, h_preds_fnet = self.flow_net.step(trajectories, h_bfs.detach(), adj)\n\t        for ignored_key in ['f_h', 'c_h']:\n\t            if ignored_key in self.bfs_net.hint_decoders.keys():\n\t                h_preds_bfs[ignored_key] = h_preds_bfs[ignored_key].new_full(h_preds_bfs[ignored_key].shape, clrs.OutputClass.MASKED)\n\t        for ignored_key in ['reach_h', 'pi_h']:\n\t            if ignored_key in self.flow_net.hint_decoders.keys():\n\t                h_preds_fnet[ignored_key] = h_preds_fnet[ignored_key].new_full(h_preds_fnet[ignored_key].shape, clrs.OutputClass.MASKED)\n\t        if self.decode_hints:\n\t            hint_preds = {}\n", "            idx_bfs = is_bfs_op.flatten().nonzero()\n\t            idx_f = (1 - is_bfs_op).flatten().nonzero()\n\t            for name in h_preds_bfs.keys():\n\t                n_dims = len(h_preds_bfs[name].shape)\n\t                hint_preds[name] = torch.empty_like(h_preds_bfs[name]).to(self.device)\n\t                hint_preds[name].fill_(clrs.OutputClass.MASKED)\n\t                hint_preds[name][idx_bfs] = h_preds_bfs[name][idx_bfs]\n\t                hint_preds[name][idx_f] = h_preds_fnet[name][idx_f]\n\t        # attempt to reset h_bfs\n\t        reset = torch.zeros_like(h_bfs)\n", "        h_bfs = h_bfs.masked_scatter(_expand_to(is_bfs_op.bool(), len(h_bfs.shape)), reset)\n\t        assert h_bfs[is_bfs_op.flatten().nonzero()].sum().item() == 0\n\t        for name in cand.keys():\n\t            n_dims = len(cand[name].shape)\n\t            mask = torch.zeros_like(cand[name])\n\t            mask.fill_(clrs.OutputClass.MASKED)\n\t            cand[name] = cand[name].masked_scatter(_expand_to(is_bfs_op.bool(), n_dims), mask)\n\t        return cand, h_bfs, hint_preds, hiddens\n\t    def forward(self, features):\n\t        output_preds = {}\n", "        hint_preds = []\n\t        num_steps = self.max_steps if self.max_steps else features.hints[0].data.shape[0] - 1\n\t        batch_size, num_nodes = _dimensions(features.inputs)\n\t        h_bfs = torch.zeros((batch_size, num_nodes, self.num_hidden)).to(self.device)\n\t        adj = adj_mat(features).to(self.device)\n\t        A = edge_attr_mat(features).to(self.device)\n\t        def next_hint(i):\n\t            use_teacher_forcing = self.training\n\t            first_step = i == 0\n\t            if self.is_annealing_enabled:\n", "                self.annealing_state += 1\n\t                use_teacher_forcing = use_teacher_forcing and not(random() > (0.999 ** self.annealing_state))\n\t            if use_teacher_forcing or first_step:\n\t                return _hints_i(features.hints, i)\n\t            else:\n\t                return _own_hints_i(last_valid_hints, self.spec, features, i)\n\t        prev_is_flow_op = None\n\t        last_valid_hints = {hint.name: hint.data.to(self.device) for hint in next_hint(0)}\n\t        last_valid_hints['pi_h'] = torch.nn.functional.one_hot(last_valid_hints['pi_h'].long(),\n\t                                                               num_nodes).float()\n", "        del last_valid_hints['__is_bfs_op']\n\t        for i in range(num_steps):\n\t            # ~~~ init ~~~\n\t            trajectories = [features.inputs]\n\t            if self.encode_hints:\n\t                cur_hint = next_hint(i)\n\t                if i > 0 and not self.training:\n\t                    assert prev_is_flow_op is not None\n\t                    first_bfs_step = prev_is_flow_op\n\t                    reach_h, pi_h = _reset_hints(cur_hint, _get_fts(features.inputs, \"s\").data)\n", "                    for hint in cur_hint:\n\t                        if hint.name == 'reach_h':\n\t                            hint.data[first_bfs_step.flatten().nonzero()] = reach_h.data.to(self.device)[first_bfs_step.flatten().nonzero()]\n\t                        elif hint.name == 'pi_h':\n\t                            hint.data[first_bfs_step.flatten().nonzero()] = pi_h.data.to(self.device)[first_bfs_step.flatten().nonzero()]\n\t                trajectories.append(cur_hint)\n\t            if self.decode_hints:\n\t                is_bfs_op = _bfs_op_mask(next_hint(i)).data.to(self.device)\n\t                is_flow_op = (1 - is_bfs_op)\n\t            else:\n", "                is_bfs_op = None\n\t            cand, h_bfs, h_preds, _ = self.op(trajectories, h_bfs, adj, is_bfs_op)\n\t            if \"f\" in cand:\n\t                idx = is_flow_op.flatten().nonzero()\n\t                cand[\"f\"][idx] = (A * cand['f'])[idx]\n\t            if \"f_h\" in h_preds:\n\t                idx = is_flow_op.flatten().nonzero()\n\t                h_preds[\"f_h\"][idx] = (A * h_preds['f_h'])[idx]\n\t            for name in last_valid_hints.keys():\n\t                if self.no_feats(name):\n", "                    continue\n\t                is_masked = (h_preds[name] == clrs.OutputClass.MASKED) * 1.0\n\t                last_valid_hints[name] = is_masked * last_valid_hints[name] + (1.0 - is_masked) * h_preds[name]\n\t            hint_preds.append(h_preds)\n\t            for name in cand:\n\t                if name not in output_preds or features.lengths.sum() == 0:\n\t                    output_preds[name] = cand[name]\n\t                else:\n\t                    is_not_done = is_not_done_broadcast(features.lengths, i, cand[name])\n\t                    mask = is_not_done * _expand_to(is_flow_op, len(is_not_done.shape))\n", "                    output_preds[name] = mask * cand[name] + \\\n\t                        (1.0 - mask) * output_preds[name]\n\t            prev_is_flow_op = is_flow_op\n\t        return output_preds, hint_preds\n"]}
{"filename": "nn/models/impl/encoders.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom torch.nn import Module, Sequential, Linear\n\t_DataPoint = clrs.DataPoint\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n\t_Type = clrs.Type\n\t_Tensor = torch.FloatTensor\n\tclass Encoder(Module):\n", "    def __init__(self,\n\t                 in_features,\n\t                 out_features,\n\t                 bias=True):\n\t        super(Encoder, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.net = Sequential(\n\t            Linear(in_features=in_features, out_features=out_features, bias=bias)\n\t        )\n", "    def forward(self, x):\n\t        return self.net(x)\n\tdef preprocess(dp: _DataPoint, nb_nodes: int) -> _Tensor:\n\t    from torch.nn.functional import one_hot\n\t    if dp.type_ == _Type.POINTER:\n\t        return one_hot(dp.data.long(), nb_nodes).float()\n\t    return dp.data\n\tdef accum_edge_fts(encoder, dp: _DataPoint, data: _Tensor,\n\t                   edge_fts: _Tensor, adj: _Tensor) -> _Tensor:\n\t    encoding = _encode_inputs(encoder, dp, data)\n", "    if dp.location == _Location.NODE and dp.type_ == _Type.POINTER:\n\t        edge_fts += encoding\n\t    elif dp.location == _Location.EDGE:\n\t        assert dp.type_ != _Type.POINTER\n\t        edge_fts += encoding\n\t    return edge_fts\n\tdef accum_node_fts(encoder, dp: _DataPoint, data: _Tensor,\n\t                   node_fts: _Tensor) -> _Tensor:\n\t    encoding = _encode_inputs(encoder, dp, data)\n\t    if dp.location == _Location.NODE and dp.type_ != _Type.POINTER:\n", "        node_fts += encoding\n\t    return node_fts\n\tdef accum_graph_fts(encoders, dp: _DataPoint, data: _Tensor,\n\t                    graph_fts: _Tensor) -> _Tensor:\n\t    encoding = _encode_inputs(encoders, dp, data)\n\t    if dp.location == _Location.GRAPH and dp.type_ != _Type.POINTER:\n\t        graph_fts += encoding\n\t    return graph_fts\n\tdef _encode_inputs(encoder, dp: _DataPoint, data: _Tensor) -> _Tensor:\n\t    if dp.type_ in [_Type.CATEGORICAL]:\n", "        encoding = encoder(data)\n\t    else:\n\t        encoding = encoder(data.unsqueeze(-1))\n\t    return encoding\n"]}
{"filename": "nn/models/impl/__init__.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom . import decoders\n\t_Feedback = clrs.Feedback\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n\t_Type = clrs.Type\n\t_DataPoint = clrs.DataPoint\n\tdef _dimensions(inputs):\n", "    for input_ in inputs:\n\t        if input_.location in [clrs.Location.NODE, clrs.Location.EDGE]:\n\t            return input_.data.shape[0], input_.data.shape[1]\n\t    assert False\n\tdef _bfs_op_mask(hints):\n\t    for dp in hints:\n\t        if dp.name == '__is_bfs_op':\n\t            return dp\n\tdef _hints_i(hints, i):\n\t    hints_i = [_DataPoint(dp.name, dp.location, dp.type_, dp.data[i]) for dp in hints]\n", "    for h in hints_i:\n\t        if h.name == 'f_h':\n\t            zero_c_h = 1 - (h.data == 0).all(-1).all(-1) * 1.0\n\t            break\n\t    for h in hints_i:\n\t        if h.name == 'c_h':\n\t            h.data = h.data * (1 - zero_c_h.unsqueeze(-1).unsqueeze(-1))\n\t            break\n\t    return hints_i\n\tdef _own_hints_i(preds, spec, features, i):\n", "    hints = list(decoders.postprocess(preds, spec).values())\n\t    hints.append(_bfs_op_mask(_hints_i(features.hints, i)))\n\t    return hints\n\tdef _expand(tensor, loc):\n\t    if loc == _Location.NODE:\n\t        n_dims = 3\n\t    elif loc == _Location.EDGE:\n\t        n_dims = 4\n\t    elif loc == _Location.GRAPH:\n\t        n_dims = 2\n", "    else:\n\t        assert False\n\t    return _expand_to(tensor, n_dims)\n\tdef _expand_to(tensor, num_dims):\n\t    while len(tensor.shape) < num_dims:\n\t        tensor = tensor.unsqueeze(-1)\n\t    return tensor\n\tdef _get_fts(trajectory, name):\n\t    for dp in trajectory:\n\t        if dp.name == name:\n", "            return dp\n\t    return None\n\tdef _reset_hints(hints, source):\n\t    b, n = _dimensions(hints)\n\t    reach_h = torch.zeros((b, n))\n\t    for i, s in enumerate(source):\n\t        reach_h[i][s.argmax().item()] = 1\n\t    pi = torch.stack([torch.arange(n)] * b)\n\t    return [_DataPoint('reach_h', _Location.NODE, _Type.MASK, reach_h),\n\t            _DataPoint('pi_h', _Location.NODE, _Type.POINTER, pi)]\n"]}
{"filename": "nn/models/impl/processors.py", "chunked_list": ["import torch\n\tfrom nn.layers import MpnnConv\n\tfrom torch.nn import Linear, Module, ReLU, Sequential\n\tclass MPNN(Module):\n\t    def __init__(self,\n\t                 num_hidden: int,\n\t                 aggregator: str,\n\t                 activation: callable = None,\n\t                 bias: bool = True):\n\t        super(MPNN, self).__init__()\n", "        self.conv = MpnnConv(\n\t            in_channels=num_hidden * 2,\n\t            edge_channels=num_hidden,\n\t            mid_channels=num_hidden,\n\t            out_channels=num_hidden,\n\t            net=Sequential(\n\t                Linear(in_features=num_hidden, out_features=num_hidden),\n\t                ReLU(),\n\t                Linear(in_features=num_hidden, out_features=num_hidden),\n\t            ),\n", "            aggregator=aggregator,\n\t            mid_activation=activation,\n\t            activation=activation,\n\t            bias=bias\n\t        )\n\t    def forward(self, x, adj, edge_attr):\n\t        adj = torch.ones_like(adj).to(adj.device)\n\t        x = self.conv(x, adj, edge_attr)\n\t        return x\n\tclass PGN(Module):\n", "    def __init__(self,\n\t                 num_hidden: int,\n\t                 aggregator: str,\n\t                 activation: callable = None,\n\t                 bias: bool = True):\n\t        super(PGN, self).__init__()\n\t        self.conv = MpnnConv(\n\t            in_channels=num_hidden * 2,\n\t            edge_channels=num_hidden,\n\t            mid_channels=num_hidden,\n", "            out_channels=num_hidden,\n\t            net=Sequential(\n\t                Linear(in_features=num_hidden, out_features=num_hidden),\n\t                ReLU(),\n\t                Linear(in_features=num_hidden, out_features=num_hidden),\n\t            ),\n\t            aggregator=aggregator,\n\t            mid_activation=activation,\n\t            activation=activation,\n\t            bias=bias\n", "        )\n\t    def forward(self, x, adj, edge_attr):\n\t        x = self.conv(x, adj, edge_attr)\n\t        return x\n\tPROCESSORS = {\n\t    'pgn': PGN,\n\t    'mpnn': MPNN\n\t}\n"]}
{"filename": "nn/models/impl/decoders.py", "chunked_list": ["import clrs\n\timport torch\n\tfrom torch.nn import Module, Sequential, Linear\n\tfrom typing import Dict\n\t_INFINITY = 1e5\n\t_DataPoint = clrs.DataPoint\n\t_Spec = clrs.Spec\n\t_Stage = clrs.Stage\n\t_Location = clrs.Location\n\t_Type = clrs.Type\n", "_Tensor = torch.Tensor\n\tclass Decoder(Module):\n\t    def __init__(self,\n\t                 in_features,\n\t                 out_features):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.net = Sequential(\n\t            Linear(in_features=in_features, out_features=out_features)\n", "        )\n\t    def forward(self, x):\n\t        return self.net(x)\n\tclass DecoderPair(Module):\n\t    def __init__(self,\n\t                 in_features,\n\t                 out_features):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n", "        self.first = Sequential(\n\t            Linear(in_features=in_features, out_features=out_features)\n\t        )\n\t        self.second = Sequential(\n\t            Linear(in_features=in_features, out_features=out_features)\n\t        )\n\t    def forward(self, x):\n\t        return self.first(x), self.second(x)\n\tclass DecoderEdge(Module):\n\t    def __init__(self,\n", "                 in_features,\n\t                 e_features,\n\t                 out_features):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.first = Sequential(\n\t            Linear(in_features=in_features, out_features=out_features)\n\t        )\n\t        self.second = Sequential(\n", "            Linear(in_features=in_features, out_features=out_features)\n\t        )\n\t        self.third = Sequential(\n\t            Linear(in_features=e_features, out_features=out_features)\n\t        )\n\t    def forward(self, x, edge_feats):\n\t        return self.first(x), self.second(x), self.third(edge_feats)\n\tdef new_decoder(spec, num_hidden, num_classes=None):\n\t    stage, location, type_ = spec\n\t    if location == _Location.NODE:\n", "        if type_ in [_Type.SCALAR, _Type.MASK, _Type.MASK_ONE]:\n\t            return Decoder(in_features=num_hidden*3, out_features=1)\n\t        if type_ == _Type.POINTER:\n\t            return DecoderPair(in_features=num_hidden*3, out_features=num_hidden)\n\t        if type_ == _Type.CATEGORICAL:\n\t            assert num_classes is not None\n\t            return Decoder(in_features=num_hidden*3, out_features=num_classes)\n\t    if location == _Location.EDGE:\n\t        if type_ == _Type.SCALAR:\n\t            return DecoderEdge(in_features=num_hidden*3,\n", "                               e_features=num_hidden,\n\t                               out_features=1)\n\t        if type_ == _Type.CATEGORICAL:\n\t            assert num_classes is not None\n\t            return DecoderEdge(in_features=num_hidden*3, e_features=num_hidden,\n\t                               out_features=num_classes)\n\t    if location == _Location.GRAPH:\n\t        if type_ in [_Type.SCALAR, _Type.MASK, _Type.MASK_ONE]:\n\t            return Decoder(in_features=num_hidden*3, out_features=1)\n\t    raise ValueError(\"Unrecognized specs during decoder creation.\")\n", "def decode_from_latents(name, spec, decoder, h_t, adj, edge_attr):\n\t    stage, location, type_ = spec\n\t    if location == _Location.NODE:\n\t        if type_ in [_Type.SCALAR, _Type.MASK, _Type.MASK_ONE]:\n\t            return decoder(h_t).squeeze(-1)\n\t        if type_ == _Type.POINTER:\n\t            p_1, p_2 = decoder(h_t)\n\t            p = torch.matmul(p_1, torch.permute(p_2, (0, 2, 1)))\n\t            # p = p.masked_fill(~adj.bool(), -_INFINITY)\n\t            return p\n", "        if type_ == _Type.CATEGORICAL:\n\t            return decoder(h_t)\n\t    if location == _Location.EDGE:\n\t        assert edge_attr is not None\n\t        if type_ == _Type.SCALAR:\n\t            p_1, p_2, p_e = decoder(h_t, edge_attr)\n\t            p = (p_1.unsqueeze(-2) + p_2.unsqueeze(-3) + p_e).squeeze() * adj\n\t            if name in [\"f\", \"f_h\"]:\n\t                p = p - p.transpose(1, 2)  # TODO: remove and generalise\n\t                p = torch.tanh(p)\n", "            return p\n\t        if type_ == _Type.CATEGORICAL:\n\t            p_1, p_2, p_e = decoder(h_t, edge_attr)\n\t            return p_1.unsqueeze(-2) + p_2.unsqueeze(-3) + p_e\n\t    if location == _Location.GRAPH:\n\t        if type_ in [_Type.SCALAR, _Type.MASK, _Type.MASK_ONE]:\n\t            g_h = torch.amax(h_t, dim=-2)\n\t            return decoder(g_h).squeeze(-1)\n\t    raise ValueError(\"Unrecognized specs during decoding from latents.\")\n\tdef postprocess(preds: Dict[str, _Tensor], spec: _Spec) -> Dict[str, _DataPoint]:\n", "    result = {}\n\t    for name in preds.keys():\n\t        _, location, type_ = spec[name]\n\t        data = preds[name]\n\t        if type_ == _Type.SCALAR:\n\t            data = data\n\t        elif type_ == _Type.MASK:\n\t            data = ((data > 0.0) * 1.0)\n\t        elif type_ in [_Type.MASK_ONE, _Type.CATEGORICAL]:\n\t            data = torch.nn.functional.one_hot(data.argmax(-1), data.shape[-1]).float()\n", "        elif type_ == _Type.POINTER:\n\t            data = data.argmax(dim=-1)\n\t        result[name] = _DataPoint(name, location, type_, data)\n\t    return result\n"]}
