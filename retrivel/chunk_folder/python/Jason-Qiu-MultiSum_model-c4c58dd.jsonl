{"filename": "preprocessing/video_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\n\timport os\n\timport glob\n\timport torch\n\timport torchvision.transforms as transforms\n\timport clip\n\timport PIL\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom torch.utils.data import Dataset, DataLoader\n", "from tqdm import tqdm\n\timport json\n\ttorch.set_num_threads(2)\n\tlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\t# Load the CLIP model\n\tdevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\tmodel, preprocess = clip.load('ViT-B/32', device=device)\n\t# Modify the model to output features of size 2048\n\tmodel.visual.output_dim = 2048\n\t# Define the transform to preprocess the input frames\n", "transform = transforms.Compose([\n\t    transforms.Resize(224),\n\t    transforms.CenterCrop(224),\n\t    preprocess\n\t])\n\tclass VideoFramesDataset(Dataset):\n\t    def __init__(self, frames, transform):\n\t        self.frames = frames\n\t        self.transform = transform\n\t    def __len__(self):\n", "        return len(self.frames)\n\t    def __getitem__(self, idx):\n\t        frame = self.frames[idx]\n\t        preprocessed_frame = self.transform(Image.fromarray(frame))\n\t        return preprocessed_frame\n\tlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\n\tmodel.to(device)\n\tsave_np_dic = {}\n\tbatch_size = 128\n\tcorrupted_videos = []\n", "count = 0\n\tfor annotation in tqdm(list_of_annotations, desc = 'Extracting features: '):\n\t    json_file = open_file(annotation)\n\t    id = json_file['info']['video_id']\n\t    keyframes = json_file['summary']\n\t    start_time_seconds = 0\n\t    end_time_seconds = time_to_seconds(json_file['info']['duration'])\n\t    path_to_video = f\"../../jielin/msmo/video/{json_file['info']['category']}/{json_file['info']['sub_category']}/{json_file['info']['video_id']}.mp4\"\n\t    frames = extract_frames(path_to_video, start_time_seconds, end_time_seconds, 100)\n\t    # print(len(frames))\n", "    if len(frames) == 0:\n\t        corrupted_videos.append(path_to_video)\n\t        print('Corrupted ... ')\n\t        pass\n\t    else:\n\t        dataset = VideoFramesDataset(frames, transform)\n\t        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\t        features_list = []\n\t        with torch.no_grad():\n\t            for batch in dataloader:\n", "                batch = batch.to(device)\n\t                features = model.encode_image(batch)\n\t                features = linear(features.to(device))\n\t                features_list.append(features.cpu())\n\t        features = torch.cat(features_list, dim=0)\n\t        save_np_dic[f'{id}'] = features.numpy()\n\t    # count +=1 \n\t    # if count == 50:\n\t    #     break\n\t    print(save_np_dic)\n", "# The features tensor has shape [num_frames, feature_size]\n\twith open('corrupted_videos.json', 'w') as f:\n\t    json.dump(corrupted_videos, f)\n\tnp.save('msmo_clip_features.npy', save_np_dic)"]}
{"filename": "preprocessing/keyframe_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\n\timport os\n\timport glob\n\timport torch\n\timport torchvision.transforms as transforms\n\timport clip\n\timport PIL\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom torch.utils.data import Dataset, DataLoader\n", "from tqdm import tqdm\n\timport json\n\ttorch.set_num_threads(2)\n\tlist_of_keyframes = glob.glob('../../jielin/msmo/keyframe/*/*/*')\n\t# Load the CLIP model\n\tdevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\tmodel, preprocess = clip.load('ViT-B/32', device=device)\n\t# Modify the model to output features of size 2048\n\tmodel.visual.output_dim = 2048\n\t# Define the transform to preprocess the input frames\n", "transform = transforms.Compose([\n\t    transforms.Resize(224),\n\t    transforms.CenterCrop(224),\n\t    preprocess\n\t])\n\tclass VideoFramesDataset(Dataset):\n\t    def __init__(self, frames, transform):\n\t        self.frames = frames\n\t        self.transform = transform\n\t    def __len__(self):\n", "        return len(self.frames)\n\t    def __getitem__(self, idx):\n\t        frame = self.frames[idx]\n\t        preprocessed_frame = self.transform(Image.fromarray(frame))\n\t        return preprocessed_frame\n\tlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\n\tmodel.to(device)\n\tsave_np_dic = {}\n\tbatch_size = 128\n\tcorrupted_videos = []\n", "count = 0\n\tfor path in tqdm(list_of_keyframes, desc = 'Extracting features ...'):\n\t    id = path.split('/')[-1]\n\t    image_data = []\n\t    for image in os.listdir(path):\n\t        image = Image.open(os.path.join(path, image))\n\t        image_array = np.array(image)\n\t        image_data.append(image_array)\n\t    stacked_array = np.stack(image_data, axis = 0)\n\t    dataset = VideoFramesDataset(stacked_array, transform)\n", "    dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n\t    features_list = []\n\t    with torch.no_grad():\n\t        for batch in dataloader:\n\t            batch = batch.to(device)\n\t            features = model.encode_image(batch)\n\t            features = linear(features.to(device))\n\t            features_list.append(features.cpu())\n\t    features = torch.cat(features_list, dim=0)\n\t    save_np_dic[f'{id}'] = features.numpy()\n", "    # count +=1 \n\t    # if count == 50:\n\t    #     break\n\t    # print(save_np_dic)\n\t# The features tensor has shape [num_frames, feature_size]\n\tnp.save('msmo_clip_summ_features.npy', save_np_dic)"]}
{"filename": "preprocessing/gpu.py", "chunked_list": ["import torch\n\t#If CUDA is available, prints True. Else, False.\n\tprint(torch.cuda.is_available())\n\t# If CUDA is availalbe, prints integer of CUDA devices that are available.\n\tprint(torch.cuda.device_count())\n\t# If CUDA is available, prints the ID of the current device that is ready for usage.\n\tprint(torch.cuda.current_device())\n\tID = torch.cuda.current_device()\n\t# If CUDA is available, instantiate device you will be using.\n\tdevice = torch.device(f'cuda:{ID}')\n", "print(device)\n\t# If CUDA is available, get the name of the device.\n\tprint(torch.cuda.get_device_name(ID))\n\t# If CUDA is available, move or create tensor using device\n\t# Move\n\tx = torch.tensor(1).to(device)\n\tprint(x)\n\t# Create\n\ty = torch.tensor(1, device = device)\n\tprint(y)\n", "# If x or y printed \"tensor(1, device='cuda:0')\" you should be good to go!"]}
{"filename": "preprocessing/utils.py", "chunked_list": ["import os\n\timport json\n\timport openai\n\timport tiktoken\n\timport time\n\timport backoff\n\timport random\n\timport h5py\n\timport numpy as np\n\timport cv2\n", "from multiprocessing import Pool\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tdef check_video_file(video_file):\n\t    # Check if file exists\n\t    if not os.path.isfile(video_file):\n\t        print(\"File does not exist\")\n\t        return False\n\t    # Check if file is a valid video file\n\t    try:\n\t        cap = cv2.VideoCapture(video_file)\n", "        if not cap.isOpened():\n\t            print(\"Could not open video file\")\n\t            return False\n\t        else:\n\t            print(\"Video file opened successfully\")\n\t        # Check the encoding format of the video file\n\t        fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n\t        encoding_format = chr(fourcc & 0xFF) + chr((fourcc & 0xFF00) >> 8) + chr((fourcc & 0xFF0000) >> 16) + chr((fourcc & 0xFF000000) >> 24)\n\t        print(\"Video file encoding format:\", encoding_format)\n\t        # Check for the required codecs\n", "        codec = cv2.VideoWriter_fourcc(*'XVID')\n\t        if not cv2.VideoWriter_fourcc(*encoding_format) == codec:\n\t            print(\"Required codec not found\")\n\t            return False\n\t        # Check if the file is corrupted or incomplete\n\t        ret, frame = cap.read()\n\t        if not ret:\n\t            print(\"Video file is corrupted or incomplete\")\n\t            return False\n\t        # Release the video capture object\n", "        cap.release()\n\t    except:\n\t        print(\"An error occurred while checking the video file\")\n\t        return False\n\t    return True\n\tdef extract_frame(cap, frame_num):\n\t    # Set the frame position to the given frame number\n\t    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\t    # Grab the next frame\n\t    success = cap.grab()\n", "    if not success:\n\t        return None\n\t    # Decode and return the grabbed frame\n\t    _, frame = cap.retrieve()\n\t    return frame\n\tdef extract_frames(video_file, start_time, end_time, num_frames):\n\t    # Open the video file\n\t    cap = cv2.VideoCapture(video_file)\n\t    # Check if the video file was opened successfully\n\t    if not cap.isOpened():\n", "        print(f\"Could not open video file {video_file}\")\n\t        return []\n\t    # Get the frame rate and total number of frames in the video\n\t    fps = cap.get(cv2.CAP_PROP_FPS)\n\t    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\t    # Calculate the start and end frame numbers based on the given start and end times\n\t    start_frame = int(start_time * fps)\n\t    end_frame = int(end_time * fps)\n\t    # Calculate the frame numbers for the key frames\n\t    frame_nums = np.linspace(start_frame, end_frame, num_frames, dtype=np.int32)\n", "    # Extract the key frames sequentially\n\t    frames = []\n\t    # frames2 = []\n\t    for frame_num in frame_nums:\n\t        # Set the frame position to the given frame number\n\t        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\t        # Read the current frame\n\t        ret, frame = cap.read()\n\t        if not ret:\n\t            continue\n", "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\t        # Append the frame to the list of frames\n\t        frames.append(frame_rgb)\n\t        # frames2.append(frame_rgb.tolist())\n\t    # Release the video capture object\n\t    cap.release()\n\t    return frames#, np.array(frames2)\n\tdef time_to_seconds(time_string):\n\t    hours, minutes, seconds = map(int, time_string.split(\":\"))\n\t    total_seconds = hours * 3600 + minutes * 60 + seconds\n", "    return total_seconds\n\tdef num_tokens_from_string(string: str, encoding_name: str) -> int:\n\t    \"\"\"Returns the number of tokens in a text string.\"\"\"\n\t    encoding = tiktoken.encoding_for_model(encoding_name)\n\t    num_tokens = len(encoding.encode(string))\n\t    return num_tokens\n\tdef open_file(path_to_file):\n\t    file_extension = os.path.splitext(path_to_file)[1]\n\t    file_reader = {\n\t        '.json': lambda: json.load(open(path_to_file)),\n", "        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n\t        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n\t        '.h5': lambda: h5py.File(path_to_file, 'r')\n\t    }\n\t    return file_reader.get(file_extension, lambda: None)()\n\tdef retry_with_exponential_backoff(\n\t    func,\n\t    initial_delay: float = 1,\n\t    exponential_base: float = 2,\n\t    jitter: bool = True,\n", "    max_retries: int = 10,\n\t    errors: tuple = (openai.error.RateLimitError,openai.error.APIError),\n\t):\n\t    def wrapper(*args, **kwargs):\n\t        num_retries = 0\n\t        delay = initial_delay\n\t        while True:\n\t            try:\n\t                return func(*args, **kwargs)\n\t            except errors as e:\n", "                num_retries += 1\n\t                if num_retries > max_retries:\n\t                    raise Exception(\n\t                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n\t                    )\n\t                delay =1\n\t                time.sleep(delay)\n\t            except Exception as e:\n\t                raise e\n\t    return wrapper\n", "@retry_with_exponential_backoff\n\tdef completions_with_backoff(**kwargs):\n\t    return openai.ChatCompletion.create(**kwargs)"]}
{"filename": "preprocessing/video_feature_multisum.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\n\timport os\n\timport glob\n\timport torch\n\timport torchvision.transforms as transforms\n\timport clip\n\timport PIL\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom torch.utils.data import Dataset, DataLoader\n", "from tqdm import tqdm\n\timport json\n\ttorch.set_num_threads(4)\n\tlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\t# Load the CLIP model\n\tdevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\tmodel, preprocess = clip.load('ViT-B/32', device=device)\n\t# Modify the model to output features of size 2048\n\tmodel.visual.output_dim = 2048\n\t# Define the transform to preprocess the input frames\n", "transform = transforms.Compose([\n\t    transforms.Resize(224),\n\t    transforms.CenterCrop(224),\n\t    preprocess\n\t])\n\tclass VideoFramesDataset(Dataset):\n\t    def __init__(self, frames, transform):\n\t        self.frames = frames\n\t        self.transform = transform\n\t    def __len__(self):\n", "        return len(self.frames)\n\t    def __getitem__(self, idx):\n\t        frame = self.frames[idx]\n\t        preprocessed_frame = self.transform(Image.fromarray(frame))\n\t        return preprocessed_frame\n\tlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\n\tmodel.to(device)\n\tsave_np_dic = {}\n\tbatch_size = 128\n\tcorrupted_videos = []\n", "count_1 = 0\n\tfor annotation in tqdm(list_of_annotations, desc = 'Extracting features: '):\n\t    json_file = open_file(annotation)\n\t    id = json_file['info']['video_id']\n\t    keyframes = json_file['summary']\n\t    start_time_seconds = 0\n\t    end_time_seconds = time_to_seconds(json_file['info']['duration'])\n\t    path_to_video = f\"../../jielin/msmo/video/{json_file['info']['category']}/{json_file['info']['sub_category']}/{json_file['info']['video_id']}.mp4\"\n\t    frames = extract_frames(path_to_video, start_time_seconds, end_time_seconds, 100)\n\t    frames = frames[:99]\n", "    # print(len(frames))\n\t    count = 0\n\t    if len(frames) == 0:\n\t        corrupted_videos.append(path_to_video)\n\t        print('Corrupted ... ')\n\t        pass\n\t    else:\n\t        dataset = VideoFramesDataset(frames, transform)\n\t        dataloader = DataLoader(dataset, batch_size=99, shuffle=False)\n\t        features_list = []\n", "        with torch.no_grad():\n\t            for batch in dataloader:\n\t                batch = batch.to(device)\n\t                # image.save(f'image_{count}_{count_1}.png')\n\t                features = model.encode_image(batch)\n\t                features = linear(features.to(device))\n\t                features_list.append(features.cpu())\n\t                count +=1\n\t        features = torch.cat(features_list, dim=0)\n\t        assert(features.shape[0] == len(frames))\n", "        np.save(f'MLASK/src/data/videos_frame/{id}.npy', features.numpy())\n\t        # print(frames)\n\t        np.save(f'MLASK/src/data/frames/{id}.npy', frames)\n\t        del frames\n\t        del features\n\t        # save_np_dic[f'{id}'] = features.numpy()\n\t    # count +=1 \n\t    # if count == 50:\n\t    #     break\n\t# The features tensor has shape [num_frames, feature_size]\n", "with open('corrupted_videos.json', 'w') as f:\n\t    json.dump(corrupted_videos, f)\n\t# np.save('msmo_clip_features.npy', save_np_dic)"]}
{"filename": "preprocessing/text_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds\n\timport os\n\timport glob\n\timport torch\n\timport torchvision.transforms as transforms\n\timport PIL\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom torch.utils.data import Dataset, DataLoader\n\tfrom tqdm import tqdm\n", "import json\n\tfrom transformers import RobertaTokenizer, RobertaModel\n\ttorch.set_num_threads(2)\n\t# Load the pre-trained RoBERTa model and tokenizer\n\tmodel = RobertaModel.from_pretrained('roberta-base')\n\ttokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\tlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\t# Load the CLIP model\n\tdevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ttrain_dic = {}\n", "test_dic = {}\n\tcount = 0\n\tfor annotation in tqdm(list_of_annotations, desc = 'Extracting text features: '):\n\t    json_file = open_file(annotation)\n\t    key = json_file['info']['video_id']\n\t    keyframes = json_file['summary']\n\t    summary_sequence = [item['summary'] for item in json_file['transcript']]\n\t    # Tokenize the summary sequence and convert to IDs\n\t    inputs = tokenizer(summary_sequence, padding=True, truncation=True, return_tensors='pt')\n\t    input_ids = inputs['input_ids']\n", "    # Pass the input IDs through the model to get the output embeddings\n\t    with torch.no_grad():\n\t        outputs = model(input_ids)\n\t        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n\t        if embeddings.ndimension() ==1:\n\t            embeddings = embeddings.unsqueeze(0).numpy()\n\t        else:\n\t            embeddings = embeddings.numpy()\n\t        print(embeddings.shape)\n\t    assert embeddings.shape[0] == len(summary_sequence)\n", "    if key[-4:] == '0021' or key[-4:] == '0022' or key[-4:] == '0023' \\\n\t        or key[-4:] == '0024' or key[-4:] == '0025' or key[-4:] == '0026' \\\n\t        or key[-4:] == '0027' or key[-4:] == '0028' or key[-4:] == '0029':\n\t        test_dic[f'{key}'] = embeddings\n\t    else:\n\t        train_dic[f'{key}'] = embeddings\n\t    # count +=1 \n\t    # if count == 50:\n\t    #     break\n\tnp.save('summary_embeddings_roberta_base_train.npy', train_dic)\n", "np.save('summary_embeddings_roberta_base_test.npy', test_dic)"]}
{"filename": "preprocessing/seg_video_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\n\timport os\n\timport glob\n\timport torch\n\timport torchvision.transforms as transforms\n\timport clip\n\timport PIL\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom torch.utils.data import Dataset, DataLoader\n", "from tqdm import tqdm\n\timport json\n\ttorch.set_num_threads(2)\n\tlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\t# Load the CLIP model\n\tdevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\tmodel, preprocess = clip.load('ViT-B/32', device=device)\n\t# Modify the model to output features of size 2048\n\tmodel.visual.output_dim = 2048\n\t# Define the transform to preprocess the input frames\n", "transform = transforms.Compose([\n\t    transforms.Resize(224),\n\t    transforms.CenterCrop(224),\n\t    preprocess\n\t])\n\tclass VideoFramesDataset(Dataset):\n\t    def __init__(self, frames, transform):\n\t        self.frames = frames\n\t        self.transform = transform\n\t    def __len__(self):\n", "        return len(self.frames)\n\t    def __getitem__(self, idx):\n\t        frame = self.frames[idx]\n\t        preprocessed_frame = self.transform(Image.fromarray(frame))\n\t        return preprocessed_frame\n\tlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\n\tmodel.to(device)\n\tsave_np_dic = {}\n\tbatch_size = 128\n\tcorrupted_videos = []\n", "count = 0\n\tfor annotation in tqdm(list_of_annotations, desc = 'Extracting features: '):\n\t    json_file = open_file(annotation)\n\t    id = json_file['info']['video_id']\n\t    keyframes = json_file['summary']\n\t    for seg in range(len(keyframes)):\n\t        start_time_seconds = time_to_seconds(keyframes[seg]['start_time'])\n\t        end_time_seconds = time_to_seconds(keyframes[seg]['end_time'])\n\t        # print(start_time_seconds)\n\t        # print(end_time_seconds)\n", "        path_to_video = f\"../../jielin/msmo/video/{json_file['info']['category']}/{json_file['info']['sub_category']}/{json_file['info']['video_id']}.mp4\"\n\t        frames = extract_frames(path_to_video, start_time_seconds, end_time_seconds, 100)\n\t    # print(len(frames))\n\t        if len(frames) == 0:\n\t            corrupted_videos.append(path_to_video)\n\t            print('Corrupted ... ')\n\t            count +=1\n\t            pass\n\t        else:\n\t            dataset = VideoFramesDataset(frames, transform)\n", "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\t            features_list = []\n\t            with torch.no_grad():\n\t                for batch in dataloader:\n\t                    batch = batch.to(device)\n\t                    features = model.encode_image(batch)\n\t                    features = linear(features.to(device))\n\t                    features_list.append(features.cpu())\n\t            features = torch.cat(features_list, dim=0)\n\t            # print(features.shape)\n", "            np.save(f'MLASK/src/data/videos3/{id}_{count}.npy', features.numpy())\n\t            count +=1\n\t            # break\n\t    # break\n\t            # save_np_dic[f'{id}'] = features.numpy()\n\t        # count +=1 \n\t        # if count == 50:\n\t        #     break\n\t        # print(save_np_dic)\n\t# # The features tensor has shape [num_frames, feature_size]\n", "with open('corrupted_videos_2.json', 'w') as f:\n\t    json.dump(corrupted_videos, f)\n\t# np.save('msmo_clip_features.npy', save_np_dic)"]}
{"filename": "MultiSum/src/__init__.py", "chunked_list": []}
{"filename": "MultiSum/src/data/__init__.py", "chunked_list": []}
{"filename": "MultiSum/src/data/utils.py", "chunked_list": ["import torch\n\tfrom typing import List, Union\n\tfrom torch import Tensor\n\timport json\n\timport os\n\timport numpy as np\n\timport h5py\n\tdef open_file(path_to_file):\n\t    file_extension = os.path.splitext(path_to_file)[1]\n\t    file_reader = {\n", "        '.json': lambda: json.load(open(path_to_file)),\n\t        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n\t        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n\t        '.h5': lambda: h5py.File(path_to_file, 'r')\n\t    }\n\t    return file_reader.get(file_extension, lambda: None)()\n\tdef split_list(mylist: List, chunk_size: Union[int]):\n\t    \"\"\"\n\t    Splits list into list of lists of given size. The last chunk may be of different size.\n\t    \"\"\"\n", "    return [\n\t        mylist[offs : offs + chunk_size] for offs in range(0, len(mylist), chunk_size)\n\t    ]\n\tdef generate_square_subsequent_mask(sz: int) -> Tensor:\n\t    \"\"\"\n\t    Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n\t    Unmasked positions are filled with float(0.0).\n\t    \"\"\"\n\t    return torch.triu(torch.full((sz, sz), float(\"-inf\")), diagonal=1)\n\t# Copied from transformers.models.encoder_decoder.modeling_encoder_decoder.shift_tokens_right\n", "def shift_tokens_right(input_ids: torch.Tensor, decoder_start_token_id: int):\n\t    \"\"\"\n\t    Shift input ids one token to the right.\n\t    \"\"\"\n\t    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n\t    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n\t    if decoder_start_token_id is None:\n\t        raise ValueError(\n\t            \"Make sure to set the decoder_start_token_id attribute of the model's configuration.\"\n\t        )\n", "    shifted_input_ids[:, 0] = decoder_start_token_id\n\t    return shifted_input_ids\n"]}
{"filename": "MultiSum/src/data/mlask_data.py", "chunked_list": ["from utils import open_file\n\timport glob\n\timport numpy as np\n\tlist_of_features = glob.glob('../../../A2Summ/data/MSMO/feature/*')\n\tlist_of_video_features = [list_of_features[1], list_of_features[3]]\n\tlist_of_keyframe_features = [list_of_features[2], list_of_features[5]]\n\tlist_of_features = list_of_video_features + list_of_keyframe_features\n\tfor path in list_of_features:\n\t    if path == '../../../A2Summ/data/MSMO/feature/msmo_clip_features_test.npy' or \\\n\t        path == '../../../A2Summ/data/MSMO/feature/msmo_clip_features_train.npy':\n", "        dic = open_file(path)\n\t        for key in dic.keys():\n\t            arr = dic[f'{key}']\n\t            # np.save(f'./videos/{key}.npy', arr)\n\t    else:\n\t        dic = open_file(path)\n\t        for key in dic.keys():\n\t            arr = dic[f'{key}']\n\t            # np.save(f'./keyframes/{key}.npy', arr)\n"]}
{"filename": "MultiSum/src/data/data_laoder.py", "chunked_list": ["import csv\n\timport json\n\timport os\n\timport unicodedata\n\timport pytorch_lightning as pl\n\timport torch\n\timport numpy as np\n\timport pandas as pd\n\tfrom torch.utils.data import Dataset, DataLoader\n\tfrom transformers import AutoTokenizer\n", "from utils import generate_square_subsequent_mask\n\tclass MMSDataset(Dataset):\n\t    \"\"\"\n\t    Dataloder used to process the MLASK dataset\n\t    \"\"\"\n\t    def __init__(self, args, mode):\n\t        self.args = args\n\t        assert mode in [\"dev\", \"test\", \"train\"]\n\t        self.mode = mode\n\t        self.ids = None\n", "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\t        self._read_articles()\n\t        self._read_videos()\n\t        self._read_images()\n\t        print(self.use_ig65m)\n\t        print(self.use_s3d_how100m)\n\t    def __len__(self):\n\t        return len(self.src)\n\t    def __getitem__(self, idx):\n\t        _return_dict = {\n", "            \"src\": self.src[idx],\n\t            \"tgt\": self.tgt[idx],\n\t            \"_id\": self._ids[idx],\n\t        }\n\t        if self.use_ig65m:\n\t            _return_dict[\"video_features_ig65m\"] = np.load(self.videos[idx][\"ig65m\"])\n\t        if self.use_s3d_how100m:\n\t            _return_dict[\"video_features_s3d\"] = np.load(self.videos[idx][\"s3d\"])\n\t        if self.use_vit:\n\t            _return_dict[\"src_img_features_vit\"] = np.load(self.src_imgs[idx][\"vit\"])\n", "            _return_dict[\"tgt_img_features_vit\"] = np.load(self.tgt_imgs[idx][\"vit\"])\n\t        if self.use_effnet:\n\t            _return_dict[\"src_img_features_effnet\"] = np.load(\n\t                self.src_imgs[idx][\"effnet\"]\n\t            )\n\t            _return_dict[\"tgt_img_features_effnet\"] = np.load(\n\t                self.tgt_imgs[idx][\"effnet\"]\n\t            )\n\t        return _return_dict\n\t    def _read_videos(self):\n", "        \"\"\"\n\t        Reads the video features\n\t        \"\"\"\n\t        self.use_ig65m = self.args.video_ig65m_path is not None\n\t        self.use_s3d_how100m = self.args.video_s3d_path is not None\n\t        # At least one video feature must be used\n\t        assert self.use_ig65m or self.use_s3d_how100m\n\t        # This function should be called only after reading textual data\n\t        assert self.ids is not None\n\t        self.videos = []\n", "        self._ids = []\n\t        for _id in self.ids:\n\t            _video_paths = {}\n\t            _video_dir = _id\n\t            if self.use_ig65m:\n\t                _video_paths[\"ig65m\"] = os.path.join(\n\t                    self.args.video_ig65m_path, _video_dir + \".npy\"\n\t                )\n\t            if self.use_s3d_how100m:\n\t                _video_paths[\"s3d\"] = os.path.join(\n", "                    self.args.video_s3d_path, _video_dir + \".npy\"\n\t                )\n\t            self.videos.append(_video_paths)\n\t            self._ids.append(str(_id))\n\t    def _read_images(self):\n\t        \"\"\"\n\t        Reads the image features\n\t        \"\"\"\n\t        self.use_vit = (\n\t            self.args.img_extract_vit_path is not None\n", "            and self.args.img_tgt_vit_path is not None\n\t        )\n\t        self.use_effnet = (\n\t            self.args.img_extract_eff_path is not None\n\t            and self.args.img_tgt_eff_path is not None\n\t        )\n\t        # At least one image feature must be used\n\t        assert self.use_vit or self.use_effnet\n\t        # This function should be called only after reading textual data\n\t        assert self.ids is not None\n", "        self.src_imgs = []\n\t        self.tgt_imgs = []\n\t        for _id in self.ids:\n\t            _src_img_paths = {}\n\t            _tgt_img_paths = {}\n\t            # All the data instances are stored in a simple tree-like structure\n\t            _img_dir = _id\n\t            if self.use_vit:\n\t                _src_img_paths[\"vit\"] = os.path.join(\n\t                    self.args.img_extract_vit_path, _img_dir + \".npy\"\n", "                )\n\t                _tgt_img_paths[\"vit\"] = os.path.join(\n\t                    self.args.img_tgt_vit_path, _img_dir + \".npy\"\n\t                )\n\t            if self.use_effnet:\n\t                _src_img_paths[\"effnet\"] = os.path.join(\n\t                    self.args.img_extract_eff_path, _img_dir + \".npy\"\n\t                )\n\t                _tgt_img_paths[\"effnet\"] = os.path.join(\n\t                    self.args.img_tgt_eff_path, _img_dir + \".npy\"\n", "                )\n\t            self.src_imgs.append(_src_img_paths)\n\t            self.tgt_imgs.append(_tgt_img_paths)\n\t    def _read_articles(self):\n\t        \"\"\"\n\t        Read textual documents\n\t        \"\"\"\n\t        path = self.args.articles_path\n\t        model_headline = self.args.model_headline\n\t        df = pd.read_csv(\n", "            os.path.join(path, f\"{self.mode}_mms_joined.tsv\"),\n\t            sep=\"\\t\",\n\t            quoting=csv.QUOTE_NONE,\n\t        )\n\t        df.columns = [\"id\", \"date\", \"headline\", \"article\", \"abstract\"]\n\t        self.ids = df.id.values\n\t        self.src = df.article.values\n\t        if model_headline:\n\t            self.tgt = df.headline.values\n\t        else:\n", "            self.tgt = df.abstract.values\n\t    def collate_fn(self, batch):\n\t        max_src_len = self.args.max_src_len\n\t        max_tgt_len = self.args.max_tgt_len\n\t        # Source tokens\n\t        src_encoded = self.tokenizer(\n\t            [_item[\"src\"] for _item in batch],\n\t            padding=\"longest\",\n\t            truncation=True,\n\t            max_length=max_src_len,\n", "        )\n\t        src_ids = torch.tensor(src_encoded[\"input_ids\"])\n\t        src_mask = torch.tensor(src_encoded[\"attention_mask\"])\n\t        # Target tokens\n\t        tgt_encoded = self.tokenizer(\n\t            [_item[\"tgt\"] for _item in batch],\n\t            padding=\"longest\",\n\t            truncation=True,\n\t            max_length=max_tgt_len,\n\t        )\n", "        tgt_ids = torch.tensor(tgt_encoded[\"input_ids\"])\n\t        tgt_mask = torch.tensor(tgt_encoded[\"attention_mask\"])\n\t        _return_dict = {\n\t            \"src\": [_item[\"src\"] for _item in batch],\n\t            \"src_ids\": src_ids,\n\t            \"src_mask\": src_mask,\n\t            \"tgt\": [_item[\"tgt\"] for _item in batch],\n\t            \"tgt_ids\": tgt_ids,\n\t            \"tgt_mask\": tgt_mask,\n\t            \"_id\": [_item[\"_id\"] for _item in batch],\n", "        }\n\t        # Video features, maximal length is taken care of during feature extraction\n\t        if self.use_s3d_how100m:\n\t            video_features_s3d = np.zeros(\n\t                [\n\t                    len(batch),\n\t                    max([_item[\"video_features_s3d\"].shape[0] for _item in batch]),\n\t                    batch[0][\"video_features_s3d\"].shape[-1],\n\t                ]\n\t            )\n", "            video_mask_s3d = np.full(video_features_s3d.shape[:2], float(\"-inf\"))\n\t        if self.use_ig65m:\n\t            video_features_ig65m = np.zeros(\n\t                [\n\t                    len(batch),\n\t                    max([_item[\"video_features_ig65m\"].shape[0] for _item in batch]),\n\t                    batch[0][\"video_features_ig65m\"].shape[-1],\n\t                ]\n\t            )\n\t            video_mask_ig65m = np.full(video_features_ig65m.shape[:2], float(\"-inf\"))\n", "        for _iter, _item in enumerate(batch):\n\t            if self.use_s3d_how100m:\n\t                video_features_s3d[_iter][\n\t                    : _item[\"video_features_s3d\"].shape[0],\n\t                    : _item[\"video_features_s3d\"].shape[1],\n\t                ] = _item[\"video_features_s3d\"]\n\t                video_mask_s3d[_iter][: _item[\"video_features_s3d\"].shape[0]] = 0.0\n\t            if self.use_ig65m:\n\t                video_features_ig65m[_iter][\n\t                    : _item[\"video_features_ig65m\"].shape[0],\n", "                    : _item[\"video_features_ig65m\"].shape[1],\n\t                ] = _item[\"video_features_ig65m\"]\n\t                video_mask_ig65m[_iter][: _item[\"video_features_ig65m\"].shape[0]] = 0.0\n\t        if self.use_s3d_how100m and self.use_ig65m:\n\t            assert np.array_equal(video_mask_s3d, video_mask_ig65m)\n\t        if self.use_s3d_how100m:\n\t            _return_dict[\"video_features_s3d\"] = torch.tensor(\n\t                video_features_s3d\n\t            ).float()\n\t            _return_dict[\"video_mask\"] = torch.tensor(video_mask_s3d)\n", "        if self.use_ig65m:\n\t            _return_dict[\"video_features_ig65m\"] = torch.tensor(\n\t                video_features_ig65m\n\t            ).float()\n\t            _return_dict[\"video_mask\"] = torch.tensor(video_mask_ig65m)\n\t        # Image features extracted from source video - we do extract a single feature for every 1s of video, up to 300 features\n\t        if self.use_vit:\n\t            src_img_features_vit = np.zeros(\n\t                [\n\t                    len(batch),\n", "                    max([_item[\"src_img_features_vit\"].shape[0] for _item in batch]),\n\t                    batch[0][\"src_img_features_vit\"].shape[-1],\n\t                ]\n\t            )\n\t            src_img_mask_vit = np.full(src_img_features_vit.shape[:2], 1.0)\n\t            tgt_img_features_vit = np.stack(\n\t                [_item[\"tgt_img_features_vit\"][0] for _item in batch]\n\t            )\n\t        if self.use_effnet:\n\t            src_img_features_effnet = np.zeros(\n", "                [\n\t                    len(batch),\n\t                    max([_item[\"src_img_features_effnet\"].shape[0] for _item in batch]),\n\t                    batch[0][\"src_img_features_effnet\"].shape[-1],\n\t                ]\n\t            )\n\t            src_img_mask_effnet = np.full(src_img_features_effnet.shape[:2], 1.0)\n\t            tgt_img_features_effnet = np.stack(\n\t                [_item[\"tgt_img_features_effnet\"][0] for _item in batch]\n\t            )\n", "        for _iter, _item in enumerate(batch):\n\t            if self.use_vit:\n\t                src_img_features_vit[_iter][\n\t                    : _item[\"src_img_features_vit\"].shape[0],\n\t                    : _item[\"src_img_features_vit\"].shape[1],\n\t                ] = _item[\"src_img_features_vit\"]\n\t                src_img_mask_vit[_iter][: _item[\"src_img_features_vit\"].shape[0]] = 0.0\n\t            if self.use_effnet:\n\t                src_img_features_effnet[_iter][\n\t                    : _item[\"src_img_features_effnet\"].shape[0],\n", "                    : _item[\"src_img_features_effnet\"].shape[1],\n\t                ] = _item[\"src_img_features_effnet\"]\n\t                src_img_mask_effnet[_iter][\n\t                    : _item[\"src_img_features_effnet\"].shape[0]\n\t                ] = 0.0\n\t        if self.use_s3d_how100m and self.use_ig65m:\n\t            assert np.array_equal(src_img_mask_vit, src_img_mask_effnet)\n\t        if self.use_vit:\n\t            _return_dict[\"src_img_features_vit\"] = torch.tensor(\n\t                src_img_features_vit\n", "            ).float()\n\t            _return_dict[\"src_img_mask\"] = torch.tensor(src_img_mask_vit)\n\t            _return_dict[\"tgt_img_features_vit\"] = torch.tensor(\n\t                tgt_img_features_vit\n\t            ).float()\n\t        if self.use_effnet:\n\t            _return_dict[\"src_img_features_effnet\"] = torch.tensor(\n\t                src_img_features_effnet\n\t            ).float()\n\t            _return_dict[\"src_img_mask\"] = torch.tensor(src_img_mask_effnet)\n", "            _return_dict[\"tgt_img_features_effnet\"] = torch.tensor(\n\t                tgt_img_features_effnet\n\t            ).float()\n\t        return _return_dict\n\tclass MMSDataModule(pl.LightningDataModule):\n\t    def __init__(self, args):\n\t        super().__init__()\n\t        train_set = MMSDataset(args, \"train\")\n\t        val_set = MMSDataset(args, \"dev\")\n\t        test_set = MMSDataset(args, \"test\")\n", "        self.train_loader = DataLoader(\n\t            dataset=train_set,\n\t            batch_size=args.train_batch_size,\n\t            num_workers=args.num_workers,\n\t            shuffle=True,\n\t            collate_fn=train_set.collate_fn,\n\t        )\n\t        self.val_loader = DataLoader(\n\t            dataset=val_set,\n\t            batch_size=args.val_batch_size,\n", "            num_workers=args.num_workers,\n\t            shuffle=False,\n\t            collate_fn=val_set.collate_fn,\n\t        )\n\t        self.test_loader = DataLoader(\n\t            dataset=test_set,\n\t            batch_size=args.val_batch_size,\n\t            num_workers=args.num_workers,\n\t            shuffle=False,\n\t            collate_fn=test_set.collate_fn,\n", "        )\n\t    def train_dataloader(self):\n\t        return self.train_loader\n\t    def val_dataloader(self):\n\t        return self.val_loader\n\t    def test_dataloader(self):\n\t        return self.test_loader\n"]}
{"filename": "MultiSum/src/runtime/generate_thumbnail.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport matplotlib.pyplot as plt\n\tfrom PIL import Image, ImageDraw, ImageFont\n\timport glob\n\tfrom tqdm import tqdm\n\timport random\n\timport cv2\n\ttorch.set_num_threads(2)\n\timport os\n", "import json\n\timport numpy as np\n\timport h5py\n\tdef search_path_by_id(id, path_list):\n\t    for path in path_list:\n\t        if id in path:\n\t            return path\n\t    return None \n\tdef open_file(path_to_file):\n\t    file_extension = os.path.splitext(path_to_file)[1]\n", "    file_reader = {\n\t        '.json': lambda: json.load(open(path_to_file)),\n\t        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n\t        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n\t        '.h5': lambda: h5py.File(path_to_file, 'r')\n\t    }\n\t    return file_reader.get(file_extension, lambda: None)()\n\tdef time_to_seconds(time_string):\n\t    hours, minutes, seconds = map(int, time_string.split(\":\"))\n\t    total_seconds = hours * 3600 + minutes * 60 + seconds\n", "    return total_seconds\n\tdef extract_frame(cap, frame_num):\n\t    # Set the frame position to the given frame number\n\t    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\t    # Grab the next frame\n\t    success = cap.grab()\n\t    if not success:\n\t        return None\n\t    # Decode and return the grabbed frame\n\t    _, frame = cap.retrieve()\n", "    return frame\n\tdef extract_frames(video_file, start_time, end_time, num_frames):\n\t    # Open the video file\n\t    cap = cv2.VideoCapture(video_file)\n\t    # Check if the video file was opened successfully\n\t    if not cap.isOpened():\n\t        print(f\"Could not open video file {video_file}\")\n\t        return []\n\t    # Get the frame rate and total number of frames in the video\n\t    fps = cap.get(cv2.CAP_PROP_FPS)\n", "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\t    # Calculate the start and end frame numbers based on the given start and end times\n\t    start_frame = int(start_time * fps)\n\t    end_frame = int(end_time * fps)\n\t    # Calculate the frame numbers for the key frames\n\t    frame_nums = np.linspace(start_frame, end_frame, num_frames, dtype=np.int32)\n\t    # Extract the key frames sequentially\n\t    frames = []\n\t    # frames2 = []\n\t    for frame_num in frame_nums:\n", "        # Set the frame position to the given frame number\n\t        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\t        # Read the current frame\n\t        ret, frame = cap.read()\n\t        if not ret:\n\t            continue\n\t        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\t        # Append the frame to the list of frames\n\t        frames.append(frame_rgb)\n\t        # frames2.append(frame_rgb.tolist())\n", "    # Release the video capture object\n\t    cap.release()\n\t    return frames#, np.array(frames2)\n\tpath_to_videos = glob.glob('../../../../../jielin/msmo/video/*/*/*')\n\tpath_to_annotations = glob.glob('../../../../../jielin/msmo/annotation/*/*/*')\n\tcount = 0\n\tavailable_fonts =glob.glob('fonts/*')\n\tfor i in tqdm(range(54)):\n\t    results_whole_decoder = np.load(f'results/results_whole_{i}.npy', allow_pickle=True).item()\n\t    sentences = results_whole_decoder['sentences']\n", "    references = results_whole_decoder['references']\n\t    selected_frames = results_whole_decoder['selected_frames']\n\t    ids = results_whole_decoder['ids']\n\t    count += len(ids)\n\t    for j in range(len(sentences)):\n\t        sentence = sentences[j]\n\t        reference = references[j]\n\t        selected_frame = selected_frames[j]\n\t        id = ids[j]\n\t        video = search_path_by_id(id, path_to_videos)\n", "        annotation = search_path_by_id(id, path_to_annotations)\n\t        # print(annotation)\n\t        # print(sentence)\n\t        # print(reference)\n\t        # print(selected_frame)\n\t        # print(id)\n\t        # print(video)\n\t        json_file = open_file(annotation)\n\t        id = json_file['info']['video_id']\n\t        keyframes = json_file['summary']\n", "        start_time_seconds = 0\n\t        end_time_seconds = time_to_seconds(json_file['info']['duration'])\n\t        frames = extract_frames(video, start_time_seconds, end_time_seconds, 100)\n\t        # frames = frames[:99]\n\t        frame_indexed = frames[selected_frame]\n\t        image = Image.fromarray(frame_indexed)\n\t        image.save(f'run2/outputs2/{id}.png')\n\t        # Create a drawing object\n\t        draw = ImageDraw.Draw(image)\n\t        # Select a random font and size\n", "        font_name = random.choice(available_fonts)\n\t        font_size = random.randint(25, 200)\n\t        # Specify the text content\n\t        text = sentence\n\t        # Get the font object with the randomly selected font and size\n\t        font = ImageFont.truetype(font_name, size=font_size)\n\t        # Get the size of the image\n\t        image_width, image_height = image.size\n\t        # Generate random position for the text\n\t        text_width, text_height = draw.textsize(text, font=font)\n", "        x = random.randint(0, abs(image_width - text_width))\n\t        y = random.randint(0, abs(image_height - text_height))\n\t        # Add the text to the image with random font, size, and position\n\t        draw.text((x, y), text, font=font, fill=(255, 255, 255))\n\t        # Save the modified image\n\t        image.save(f\"run2/outputs/{id}.png\")"]}
{"filename": "MultiSum/src/runtime/train_mms_model.py", "chunked_list": ["#!/usr/bin/env python\n\timport pytorch_lightning as pl\n\timport os\n\timport sys\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"../data\"))\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"../model\"))\n\timport os\n\t_data_base = '../'\n\tfrom model_mms import MultimodalTransformer\n\tfrom data_laoder import MMSDataset, MMSDataModule\n", "from torch.utils.data import Dataset, DataLoader\n\tfrom pytorch_lightning.loggers import TensorBoardLogger\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint\n\tfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\tfrom transformers import AutoTokenizer\n\timport argparse\n\timport numpy as np\n\timport torch\n\ttorch.set_num_threads(2)\n\tparser = argparse.ArgumentParser(description=\"MMS training parameters.\")\n", "parser.add_argument(\n\t    \"--start_with_text_frozen\",\n\t    type=int,\n\t    default=0,\n\t    help=\"Number of epochs with text encoder/decoder frozen\",\n\t)\n\tparser.add_argument(\n\t    \"--mask_video_features\",\n\t    action=\"store_true\",\n\t    help=\"Whether to mask the video features during training/inference.\",\n", ")\n\tparser.add_argument(\n\t    \"--use_video_ig65m\",\n\t    # default = None,\n\t    action=\"store_true\",\n\t    help=\"Whether to use the video_ig65m features.\",\n\t)\n\tparser.add_argument(\n\t    \"--use_video_s3d\",\n\t    # default = None,\n", "    action=\"store_true\",\n\t    help=\"Whether to use the video_s3d features.\",\n\t)\n\tparser.add_argument(\n\t    \"--use_image_vit\",\n\t    # default = None,\n\t    action=\"store_true\",\n\t    help=\"Whether to use the image_vit features.\",\n\t)\n\tparser.add_argument(\n", "    \"--use_image_effnet\",\n\t    # default = None,\n\t    action=\"store_true\",\n\t    help=\"Whether to use the image_effnet features.\",\n\t)\n\tparser.add_argument(\n\t    \"--smooth_cos_labels\",\n\t    # default = None,\n\t    action=\"store_true\",\n\t    help=\"Whether to use the smooothed targets to train as opposed to a single closest target.\",\n", ")\n\tparser.add_argument(\n\t    \"--use_pretrained_summarizer\",\n\t    action=\"store_true\",\n\t    help=\"Whether to use the model pre-trained on text summarization.\",\n\t)\n\tparser.add_argument(\n\t    \"--version\",\n\t    type=int,\n\t    default=2,\n", "    help=\"Manual versioning, to be able to compute variance for several runs.\",\n\t)\n\tparser.add_argument(\n\t    \"--env\",\n\t    type=str,\n\t    default='whole',\n\t    help=\"Please choose text parsing setting; either whole or segment\",\n\t)\n\tmms_args = parser.parse_args()\n\ttraining_name = (\n", "    f\"version={mms_args.version}_ep_txt_fr={mms_args.start_with_text_frozen}\"\n\t)\n\tif mms_args.use_video_ig65m:\n\t    training_name += \"_v=ig65m\"\n\tif mms_args.use_video_s3d:\n\t    training_name += \"_v=s3d\"\n\tif mms_args.use_image_vit:\n\t    training_name += \"_i=vit\"\n\tif mms_args.use_image_effnet:\n\t    training_name += \"_i=eff\"\n", "if mms_args.smooth_cos_labels:\n\t    training_name += \"_smooth\"\n\tif mms_args.use_pretrained_summarizer:\n\t    training_name += \"_pretrain\"\n\tif mms_args.mask_video_features:\n\t    training_name += \"v_masked\"\n\tROUGE_RAW_L_checkpoint = ModelCheckpoint(\n\t    filename=\"{epoch}-{step}-{ROUGE_RAW_L_F:.2f}\",\n\t    monitor=\"ROUGE_RAW_L_F\",\n\t    # monitor = 'BLEU',\n", "    mode=\"max\",\n\t    save_top_k=1,\n\t)\n\tROUGE_RAW_L_stop = EarlyStopping(monitor=\"ROUGE_RAW_L_F\", mode=\"max\", patience=5)\n\t# ROUGE_RAW_L_stop = EarlyStopping(monitor=\"BLEU\", mode=\"max\", patience=5)\n\t# Section 6.3 in MLASK paper\n\tsummeCzech_ckpt = \"__PATH_TO_mT5_FINE-TUNED_ON_SumeCzech_DATASET__\"\n\tmms_data = MMSDataModule(\n\t    argparse.Namespace(\n\t        articles_path=f\"{_data_base}/data/\",\n", "        video_ig65m_path=f\"{_data_base}/data/videos\",\n\t        video_s3d_path = None,\n\t        img_extract_vit_path=f\"{_data_base}/data/keyframes\",\n\t        img_tgt_vit_path=f\"{_data_base}/data/thumbnails\",\n\t        img_extract_eff_path = None,\n\t        img_tgt_eff_path = None,\n\t        model_headline=False,\n\t        max_src_len=1536,\n\t        max_tgt_len=256,\n\t        train_batch_size=2,\n", "        val_batch_size=16,\n\t        num_workers=16,\n\t    )\n\t)\n\ttrain_loader = mms_data.train_dataloader()\n\tval_loader = mms_data.val_dataloader()\n\ttb_logger = TensorBoardLogger(\"trainings\", name=\"mms_novinky_tb\", version=training_name)\n\ttrainer = pl.Trainer(\n\t    max_epochs=50,\n\t    gpus=1,\n", "    logger=tb_logger,\n\t    log_every_n_steps=50,\n\t    val_check_interval=1.0,\n\t    gradient_clip_val=5,\n\t    accumulate_grad_batches=16,\n\t    callbacks=[ROUGE_RAW_L_checkpoint, ROUGE_RAW_L_stop],\n\t)\n\tmodel = MultimodalTransformer(\n\t    num_video_enc_layers=4,\n\t    use_video_ig65m=mms_args.use_video_ig65m,\n", "    use_video_s3d=mms_args.use_video_s3d,\n\t    use_image_vit=mms_args.use_image_vit,\n\t    use_image_effnet=mms_args.use_image_effnet,\n\t    smooth_cos_labels=mms_args.smooth_cos_labels,\n\t    lr_max_val=0.0005,\n\t    lr_init_val=0,\n\t    lr_warmup_steps=8000,\n\t    pre_trained_summeczech_ckpt=summeCzech_ckpt\n\t    if mms_args.use_pretrained_summarizer\n\t    else \"\",\n", "    start_with_text_frozen=mms_args.start_with_text_frozen,\n\t    mask_video_features=mms_args.mask_video_features,\n\t    args = mms_args\n\t)\n\ttrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"]}
{"filename": "MultiSum/src/runtime/__init__.py", "chunked_list": []}
{"filename": "MultiSum/src/runtime/test_mms_model.py", "chunked_list": ["#!/usr/bin/env python\n\timport pytorch_lightning as pl\n\timport sys\n\timport os\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"../data\"))\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"../model\"))\n\timport os\n\t_data_base = '../'\n\tfrom model_mms import MultimodalTransformer\n\tfrom data_laoder import MMSDataset, MMSDataModule\n", "from torch.utils.data import Dataset, DataLoader\n\tfrom pytorch_lightning.loggers import TensorBoardLogger\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint\n\tfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\tfrom transformers import AutoTokenizer\n\timport argparse\n\timport numpy as np\n\timport torch\n\ttorch.set_num_threads(2)\n\tprint(sys.argv)\n", "# CKPT_PATH = './trainings/mms_novinky_tb/version=2_ep_txt_fr=0_v=ig65m_i=vit/checkpoints/epoch=0-step=834-ROUGE_RAW_L_F=0.08.ckpt' # seg\n\tCKPT_PATH = './trainings/mms_novinky_tb/version=1_ep_txt_fr=0_v=ig65m_i=vit/checkpoints/epoch=4-step=559-ROUGE_RAW_L_F=1.65.ckpt' # whole\n\tTEST_OR_VAL = 'val'\n\tROUGE_RAW_L_checkpoint = ModelCheckpoint(\n\t    filename=\"{epoch}-{step}-{ROUGE_RAW_L_F:.2f}\",\n\t    monitor=\"ROUGE_RAW_L_F\",\n\t    mode=\"max\",\n\t    save_top_k=1,\n\t)\n\tROUGE_RAW_L_stop = EarlyStopping(monitor=\"ROUGE_RAW_L_F\", mode=\"max\", patience=5)\n", "mms_data = MMSDataModule(\n\t    argparse.Namespace(\n\t        articles_path=f\"{_data_base}/data/\",\n\t        video_ig65m_path=f\"{_data_base}/data/videos\",\n\t        # frames = f'{_data_base}/data/frames',\n\t        # video_s3d_path=f\"{_data_base}/video_mp4/s3d_how100m\",\n\t        video_s3d_path = None,\n\t        img_extract_vit_path=f\"{_data_base}/data/keyframes\",\n\t        img_tgt_vit_path=f\"{_data_base}/data/thumbnails\",\n\t        # img_extract_eff_path=f\"{_data_base}/video_mp4/efficientnet_b5\",\n", "        img_extract_eff_path = None,\n\t        # img_tgt_eff_path=f\"{_data_base}/image_jpeg/efficientnet_b5\",\n\t        img_tgt_eff_path = None,\n\t        model_headline=False,\n\t        max_src_len=1536,\n\t        max_tgt_len=256,\n\t        train_batch_size=2,\n\t        val_batch_size=16,\n\t        num_workers=16,\n\t    )\n", ")\n\tif TEST_OR_VAL == \"val\":\n\t    test_loader = mms_data.val_dataloader()\n\telif TEST_OR_VAL == \"test\":\n\t    test_loader = mms_data.test_dataloader()\n\telse:\n\t    sys.exit(1)\n\ttrainer = pl.Trainer(\n\t    max_epochs=50,\n\t    gpus=1,\n", "    log_every_n_steps=50,\n\t    # max_steps = 1,\n\t    val_check_interval=1.0,\n\t    gradient_clip_val=5,\n\t    accumulate_grad_batches=16,\n\t    callbacks=[ROUGE_RAW_L_checkpoint, ROUGE_RAW_L_stop],\n\t)\n\tmodel = MultimodalTransformer.load_from_checkpoint(CKPT_PATH)\n\ttrainer.validate(model, dataloaders=test_loader, ckpt_path=CKPT_PATH)\n"]}
{"filename": "MultiSum/src/model/mms_modeling_t5.py", "chunked_list": ["# coding=utf-8\n\t# Based on https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n\t# that is licensed under Apache-2.0 license, i.e.\n\t# Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n", "#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\" PyTorch T5 model.\"\"\"\n\timport copy\n\timport math\n\timport os\n", "import warnings\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import CrossEntropyLoss\n\tfrom torch.utils.checkpoint import checkpoint\n\tfrom transformers.activations import ACT2FN\n\tfrom transformers.file_utils import (\n\t    DUMMY_INPUTS,\n\t    DUMMY_MASK,\n\t    add_start_docstrings,\n", "    add_start_docstrings_to_model_forward,\n\t    is_torch_fx_proxy,\n\t    replace_return_docstrings,\n\t)\n\tfrom transformers.modeling_outputs import (\n\t    BaseModelOutput,\n\t    BaseModelOutputWithPastAndCrossAttentions,\n\t    Seq2SeqLMOutput,\n\t    Seq2SeqModelOutput,\n\t)\n", "from transformers.modeling_utils import (\n\t    PreTrainedModel,\n\t    find_pruneable_heads_and_indices,\n\t    prune_linear_layer,\n\t)\n\tfrom transformers.utils import logging\n\tfrom transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n\tfrom transformers.models.t5.configuration_t5 import T5Config\n\tfrom transformers.models.t5.modeling_t5 import *\n\tlogger = logging.get_logger(__name__)\n", "MMS_IG65M_EM_SIZE = 2048\n\tMMS_S3D_EMB_SIZE = 512\n\tMMS_VIT_EMB_SIZE = 2048\n\tMMS_EFFNET_EMB_SIZE = 2048\n\t# Based on  https://pytorch.org/tutorials/beginner/translation_transformer.html\n\tclass PositionalEncoding(nn.Module):\n\t    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n\t        super().__init__()\n\t        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n\t        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n", "        pos_embedding = torch.zeros((maxlen, emb_size))\n\t        pos_embedding[:, 0::2] = torch.sin(pos * den)\n\t        pos_embedding[:, 1::2] = torch.cos(pos * den)\n\t        pos_embedding = pos_embedding.unsqueeze(-2)\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.register_buffer(\"pos_embedding\", pos_embedding)\n\t    def forward(self, token_embedding):\n\t        return self.dropout(\n\t            token_embedding + self.pos_embedding[: token_embedding.size(0), :]\n\t        )\n", "class T5PreTrainedModel(PreTrainedModel):\n\t    \"\"\"\n\t    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n\t    models.\n\t    \"\"\"\n\t    config_class = T5Config\n\t    load_tf_weights = load_tf_weights_in_t5\n\t    base_model_prefix = \"transformer\"\n\t    is_parallelizable = True\n\t    supports_gradient_checkpointing = True\n", "    @property\n\t    def dummy_inputs(self):\n\t        input_ids = torch.tensor(DUMMY_INPUTS)\n\t        input_mask = torch.tensor(DUMMY_MASK)\n\t        dummy_inputs = {\n\t            \"decoder_input_ids\": input_ids,\n\t            \"input_ids\": input_ids,\n\t            \"decoder_attention_mask\": input_mask,\n\t        }\n\t        return dummy_inputs\n", "    def _init_weights(self, module):\n\t        \"\"\"Initialize the weights\"\"\"\n\t        factor = (\n\t            self.config.initializer_factor\n\t        )  # Used for testing weights initialization\n\t        if isinstance(module, T5LayerNorm):\n\t            module.weight.data.fill_(factor * 1.0)\n\t        elif isinstance(module, (T5Model, T5ForConditionalGeneration, T5EncoderModel)):\n\t            # Mesh TensorFlow embeddings initialization\n\t            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n", "            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n\t        elif isinstance(module, T5DenseReluDense):\n\t            # Mesh TensorFlow FF initialization\n\t            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n\t            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n\t            module.wi.weight.data.normal_(\n\t                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n\t            )\n\t            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n\t                module.wi.bias.data.zero_()\n", "            module.wo.weight.data.normal_(\n\t                mean=0.0, std=factor * ((self.config.d_ff) ** -0.5)\n\t            )\n\t            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n\t                module.wo.bias.data.zero_()\n\t        elif isinstance(module, T5DenseGatedGeluDense):\n\t            module.wi_0.weight.data.normal_(\n\t                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n\t            )\n\t            if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n", "                module.wi_0.bias.data.zero_()\n\t            module.wi_1.weight.data.normal_(\n\t                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n\t            )\n\t            if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n\t                module.wi_1.bias.data.zero_()\n\t            module.wo.weight.data.normal_(\n\t                mean=0.0, std=factor * ((self.config.d_ff) ** -0.5)\n\t            )\n\t            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n", "                module.wo.bias.data.zero_()\n\t        elif isinstance(module, T5Attention):\n\t            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n\t            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n\t            d_model = self.config.d_model\n\t            key_value_proj_dim = self.config.d_kv\n\t            n_heads = self.config.num_heads\n\t            module.q.weight.data.normal_(\n\t                mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5)\n\t            )\n", "            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n\t            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n\t            module.o.weight.data.normal_(\n\t                mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5)\n\t            )\n\t            if module.has_relative_attention_bias:\n\t                module.relative_attention_bias.weight.data.normal_(\n\t                    mean=0.0, std=factor * ((d_model) ** -0.5)\n\t                )\n\t    def _set_gradient_checkpointing(self, module, value=False):\n", "        if isinstance(module, (T5Attention, T5Stack)):\n\t            module.gradient_checkpointing = value\n\t    def _shift_right(self, input_ids):\n\t        decoder_start_token_id = self.config.decoder_start_token_id\n\t        pad_token_id = self.config.pad_token_id\n\t        assert (\n\t            decoder_start_token_id is not None\n\t        ), \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. See T5 docs for more information\"\n\t        # shift inputs to the right\n\t        if is_torch_fx_proxy(input_ids):\n", "            # Item assignment is not supported natively for proxies.\n\t            shifted_input_ids = torch.full(\n\t                input_ids.shape[:-1] + (1,), decoder_start_token_id\n\t            )\n\t            shifted_input_ids = torch.cat(\n\t                [shifted_input_ids, input_ids[..., :-1]], dim=-1\n\t            )\n\t        else:\n\t            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n\t            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n", "            shifted_input_ids[..., 0] = decoder_start_token_id\n\t        assert (\n\t            pad_token_id is not None\n\t        ), \"self.model.config.pad_token_id has to be defined.\"\n\t        # replace possible -100 values in labels by `pad_token_id`\n\t        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\t        assert torch.all(\n\t            shifted_input_ids >= 0\n\t        ).item(), \"Verify that `shifted_input_ids` has only positive values\"\n\t        return shifted_input_ids\n", "class MMST5Stack(T5PreTrainedModel): #encoder\n\t    def __init__(\n\t        self,\n\t        config,\n\t        embed_tokens,\n\t        num_video_enc_layers=None,\n\t        use_video_ig65m=None,\n\t        use_video_s3d=None,\n\t        use_image_vit=None,\n\t        use_image_effnet=None,\n", "        use_image_self_attention=False,\n\t    ):\n\t        super().__init__(config)\n\t        self.embed_tokens = embed_tokens\n\t        self.is_decoder = config.is_decoder\n\t        self.use_image_self_attention = use_image_self_attention\n\t        # --MMS-- Text-Video Attention\n\t        if not self.is_decoder:\n\t            # Video encoder\n\t            self.mms_video_positional_encoding = PositionalEncoding(\n", "                emb_size=config.d_model, dropout=config.dropout_rate\n\t            )\n\t            mms_encoder_layer = nn.modules.transformer.TransformerEncoderLayer(\n\t                d_model=config.d_model,\n\t                nhead=8,\n\t                dim_feedforward=2 * config.d_model,\n\t                dropout=config.dropout_rate,\n\t                batch_first=True,\n\t            )\n\t            self.mms_video_encoder = nn.modules.transformer.TransformerEncoder(\n", "                encoder_layer=mms_encoder_layer,\n\t                num_layers=num_video_enc_layers,\n\t            )\n\t            if self.use_image_self_attention:\n\t                self.mms_image_self_attention = (\n\t                    nn.modules.transformer.TransformerEncoder(\n\t                        encoder_layer=mms_encoder_layer,\n\t                        num_layers=num_video_enc_layers,\n\t                    )\n\t                )\n", "            # Cross-modal attention\n\t            self.mms_video_text_attention = nn.modules.activation.MultiheadAttention(\n\t                embed_dim=config.d_model,\n\t                num_heads=8,\n\t                dropout=config.dropout_rate,\n\t                batch_first=True,\n\t            )\n\t            self.mms_sigmoid = nn.Sigmoid()\n\t            self.mms_forget_gate = nn.modules.linear.Linear(\n\t                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n", "            )\n\t            self.mms_ff_forget_gate = nn.modules.linear.Linear(\n\t                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n\t            )\n\t            if use_video_ig65m:\n\t                if use_video_s3d:\n\t                    self.mms_video_rotate = nn.modules.linear.Linear(\n\t                        in_features=MMS_IG65M_EM_SIZE + MMS_S3D_EMB_SIZE,\n\t                        out_features=config.d_model,\n\t                        bias=False,\n", "                    )\n\t                else:\n\t                    self.mms_video_rotate = nn.modules.linear.Linear(\n\t                        in_features=MMS_IG65M_EM_SIZE,\n\t                        out_features=config.d_model,\n\t                        bias=False,\n\t                    )\n\t            else:\n\t                self.mms_video_rotate = nn.modules.linear.Linear(\n\t                    in_features=MMS_S3D_EMB_SIZE,\n", "                    out_features=config.d_model,\n\t                    bias=False,\n\t                )\n\t            self.mms_image_source_attention = nn.modules.activation.MultiheadAttention(\n\t                embed_dim=config.d_model,\n\t                num_heads=8,\n\t                dropout=config.dropout_rate,\n\t                batch_first=True,\n\t            )\n\t            self.mms_forget_gate_image = nn.modules.linear.Linear(\n", "                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n\t            )\n\t            self.mms_ff_forget_gate_image = nn.modules.linear.Linear(\n\t                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n\t            )\n\t            if use_image_vit:\n\t                if use_image_effnet:\n\t                    self.mms_image_rotate = nn.modules.linear.Linear(\n\t                        in_features=MMS_VIT_EMB_SIZE + MMS_EFFNET_EMB_SIZE,\n\t                        out_features=config.d_model,\n", "                        bias=False,\n\t                    )\n\t                else:\n\t                    self.mms_image_rotate = nn.modules.linear.Linear(\n\t                        in_features=MMS_VIT_EMB_SIZE,\n\t                        out_features=config.d_model,\n\t                        bias=False,\n\t                    )\n\t            else:\n\t                self.mms_image_rotate = nn.modules.linear.Linear(\n", "                    in_features=MMS_EFFNET_EMB_SIZE,\n\t                    out_features=config.d_model,\n\t                    bias=False,\n\t                )\n\t            self.mms_image_selector_projection = nn.modules.linear.Linear(\n\t                in_features=config.d_model,\n\t                out_features=1,\n\t                bias=False,\n\t            )\n\t        # --MMS-- Text-Video Attention\n", "        self.block = nn.ModuleList(\n\t            [\n\t                T5Block(config, has_relative_attention_bias=bool(i == 0))\n\t                for i in range(config.num_layers)\n\t            ]\n\t        )\n\t        self.final_layer_norm = T5LayerNorm(\n\t            config.d_model, eps=config.layer_norm_epsilon\n\t        )\n\t        self.dropout = nn.Dropout(config.dropout_rate)\n", "        # Initialize weights and apply final processing\n\t        self.post_init()\n\t        # Model parallel\n\t        self.model_parallel = False\n\t        self.device_map = None\n\t        self.gradient_checkpointing = False\n\t    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n\t    def parallelize(self, device_map=None):\n\t        # Check validity of device_map\n\t        self.device_map = (\n", "            get_device_map(len(self.block), range(torch.cuda.device_count()))\n\t            if device_map is None\n\t            else device_map\n\t        )\n\t        assert_device_map(self.device_map, len(self.block))\n\t        self.model_parallel = True\n\t        self.first_device = (\n\t            \"cpu\"\n\t            if \"cpu\" in self.device_map.keys()\n\t            else \"cuda:\" + str(min(self.device_map.keys()))\n", "        )\n\t        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n\t        # Load onto devices\n\t        for k, v in self.device_map.items():\n\t            for layer in v:\n\t                cuda_device = \"cuda:\" + str(k)\n\t                self.block[layer] = self.block[layer].to(cuda_device)\n\t        # Set embed_tokens to first layer\n\t        self.embed_tokens = self.embed_tokens.to(self.first_device)\n\t        # Set final layer norm to last device\n", "        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n\t    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n\t    def deparallelize(self):\n\t        self.model_parallel = False\n\t        self.device_map = None\n\t        self.first_device = \"cpu\"\n\t        self.last_device = \"cpu\"\n\t        for i in range(len(self.block)):\n\t            self.block[i] = self.block[i].to(\"cpu\")\n\t        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n", "        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n\t        torch.cuda.empty_cache()\n\t    def get_input_embeddings(self):\n\t        return self.embed_tokens\n\t    def set_input_embeddings(self, new_embeddings):\n\t        self.embed_tokens = new_embeddings\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        attention_mask=None,\n", "        encoder_hidden_states=None,\n\t        encoder_attention_mask=None,\n\t        inputs_embeds=None,\n\t        head_mask=None,\n\t        cross_attn_head_mask=None,\n\t        past_key_values=None,\n\t        use_cache=None,\n\t        output_attentions=None,\n\t        output_hidden_states=None,\n\t        return_dict=None,\n", "        video_ig65m_emb=None,\n\t        video_s3d_emb=None,\n\t        image_vit_emb=None,\n\t        image_effnet_emb=None,\n\t        video_padding_mask=None,\n\t        image_padding_mask=None,\n\t    ):\n\t        # Model parallel\n\t        if self.model_parallel:\n\t            torch.cuda.set_device(self.first_device)\n", "            self.embed_tokens = self.embed_tokens.to(self.first_device)\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        output_attentions = (\n\t            output_attentions\n\t            if output_attentions is not None\n\t            else self.config.output_attentions\n\t        )\n\t        output_hidden_states = (\n\t            output_hidden_states\n\t            if output_hidden_states is not None\n", "            else self.config.output_hidden_states\n\t        )\n\t        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        if input_ids is not None and inputs_embeds is not None:\n\t            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n\t            raise ValueError(\n\t                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n\t            )\n", "        elif input_ids is not None:\n\t            input_shape = input_ids.size()\n\t            input_ids = input_ids.view(-1, input_shape[-1])\n\t        elif inputs_embeds is not None:\n\t            input_shape = inputs_embeds.size()[:-1]\n\t        else:\n\t            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n\t            raise ValueError(\n\t                f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\n\t            )\n", "        if inputs_embeds is None:\n\t            assert (\n\t                self.embed_tokens is not None\n\t            ), \"You have to initialize the model with valid token embeddings\"\n\t            inputs_embeds = self.embed_tokens(input_ids)\n\t        batch_size, seq_length = input_shape\n\t        # required mask seq length can be calculated via length of past\n\t        mask_seq_length = (\n\t            past_key_values[0][0].shape[2] + seq_length\n\t            if past_key_values is not None\n", "            else seq_length\n\t        )\n\t        if use_cache is True:\n\t            assert (\n\t                self.is_decoder\n\t            ), f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n\t        if attention_mask is None:\n\t            attention_mask = torch.ones(batch_size, mask_seq_length).to(\n\t                inputs_embeds.device\n\t            )\n", "        if (\n\t            self.is_decoder\n\t            and encoder_attention_mask is None\n\t            and encoder_hidden_states is not None\n\t        ):\n\t            encoder_seq_length = encoder_hidden_states.shape[1]\n\t            encoder_attention_mask = torch.ones(\n\t                batch_size,\n\t                encoder_seq_length,\n\t                device=inputs_embeds.device,\n", "                dtype=torch.long,\n\t            )\n\t        # initialize past_key_values with `None` if past does not exist\n\t        if past_key_values is None:\n\t            past_key_values = [None] * len(self.block)\n\t        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n\t        # ourselves in which case we just need to make it broadcastable to all heads.\n\t        extended_attention_mask = self.get_extended_attention_mask(\n\t            attention_mask, input_shape, inputs_embeds.device\n\t        )\n", "        # If a 2D or 3D attention mask is provided for the cross-attention\n\t        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n\t        if self.is_decoder and encoder_hidden_states is not None:\n\t            (\n\t                encoder_batch_size,\n\t                encoder_sequence_length,\n\t                _,\n\t            ) = encoder_hidden_states.size()\n\t            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n\t            if encoder_attention_mask is None:\n", "                encoder_attention_mask = torch.ones(\n\t                    encoder_hidden_shape, device=inputs_embeds.device\n\t                )\n\t            encoder_extended_attention_mask = self.invert_attention_mask(\n\t                encoder_attention_mask\n\t            )\n\t        else:\n\t            encoder_extended_attention_mask = None\n\t        # Prepare head mask if needed\n\t        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n", "        cross_attn_head_mask = self.get_head_mask(\n\t            cross_attn_head_mask, self.config.num_layers\n\t        )\n\t        present_key_value_states = () if use_cache else None\n\t        all_hidden_states = () if output_hidden_states else None\n\t        all_attentions = () if output_attentions else None\n\t        all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n\t        position_bias = None\n\t        encoder_decoder_position_bias = None\n\t        hidden_states = self.dropout(inputs_embeds)\n", "        for i, (layer_module, past_key_value) in enumerate(\n\t            zip(self.block, past_key_values)\n\t        ):\n\t            layer_head_mask = head_mask[i]\n\t            cross_attn_layer_head_mask = cross_attn_head_mask[i]\n\t            # Model parallel\n\t            if self.model_parallel:\n\t                torch.cuda.set_device(hidden_states.device)\n\t                # Ensure that attention_mask is always on the same device as hidden_states\n\t                if attention_mask is not None:\n", "                    attention_mask = attention_mask.to(hidden_states.device)\n\t                if position_bias is not None:\n\t                    position_bias = position_bias.to(hidden_states.device)\n\t                if encoder_hidden_states is not None:\n\t                    encoder_hidden_states = encoder_hidden_states.to(\n\t                        hidden_states.device\n\t                    )\n\t                if encoder_extended_attention_mask is not None:\n\t                    encoder_extended_attention_mask = (\n\t                        encoder_extended_attention_mask.to(hidden_states.device)\n", "                    )\n\t                if encoder_decoder_position_bias is not None:\n\t                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(\n\t                        hidden_states.device\n\t                    )\n\t                if layer_head_mask is not None:\n\t                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n\t                if cross_attn_layer_head_mask is not None:\n\t                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(\n\t                        hidden_states.device\n", "                    )\n\t            if output_hidden_states:\n\t                all_hidden_states = all_hidden_states + (hidden_states,)\n\t            if self.gradient_checkpointing and self.training:\n\t                if use_cache:\n\t                    logger.warning(\n\t                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n\t                    )\n\t                    use_cache = False\n\t                def create_custom_forward(module):\n", "                    def custom_forward(*inputs):\n\t                        return tuple(module(*inputs, use_cache, output_attentions))\n\t                    return custom_forward\n\t                layer_outputs = checkpoint(\n\t                    create_custom_forward(layer_module),\n\t                    hidden_states,\n\t                    extended_attention_mask,\n\t                    position_bias,\n\t                    encoder_hidden_states,\n\t                    encoder_extended_attention_mask,\n", "                    encoder_decoder_position_bias,\n\t                    layer_head_mask,\n\t                    cross_attn_layer_head_mask,\n\t                    None,  # past_key_value is always None with gradient checkpointing\n\t                )\n\t            else:\n\t                layer_outputs = layer_module(\n\t                    hidden_states,\n\t                    attention_mask=extended_attention_mask,\n\t                    position_bias=position_bias,\n", "                    encoder_hidden_states=encoder_hidden_states,\n\t                    encoder_attention_mask=encoder_extended_attention_mask,\n\t                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n\t                    layer_head_mask=layer_head_mask,\n\t                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n\t                    past_key_value=past_key_value,\n\t                    use_cache=use_cache,\n\t                    output_attentions=output_attentions,\n\t                )\n\t            # layer_outputs is a tuple with:\n", "            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n\t            if use_cache is False:\n\t                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n\t            hidden_states, present_key_value_state = layer_outputs[:2]\n\t            # We share the position biases between the layers - the first layer store them\n\t            # layer_outputs=hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n\t            # (cross-attention position bias), (cross-attention weights)\n\t            position_bias = layer_outputs[2]\n\t            if self.is_decoder and encoder_hidden_states is not None:\n\t                encoder_decoder_position_bias = layer_outputs[\n", "                    4 if output_attentions else 3\n\t                ]\n\t            # append next layer key value states\n\t            if use_cache:\n\t                present_key_value_states = present_key_value_states + (\n\t                    present_key_value_state,\n\t                )\n\t            if output_attentions:\n\t                all_attentions = all_attentions + (layer_outputs[3],)\n\t                if self.is_decoder:\n", "                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n\t            # Model Parallel: If it's the last layer for that device, put things on the next device\n\t            if self.model_parallel:\n\t                for k, v in self.device_map.items():\n\t                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n\t                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n\t        # print('hidden_states')\n\t        # print(hidden_states.shape)\n\t        hidden_states = self.final_layer_norm(hidden_states)\n\t        hidden_states = self.dropout(hidden_states)\n", "        # --MMS-- Text-Video Attention\n\t        if not self.is_decoder:\n\t            if video_ig65m_emb is not None:\n\t                if video_s3d_emb is not None:\n\t                    video_features = torch.cat((video_ig65m_emb, video_s3d_emb), -1)\n\t                else:\n\t                    video_features = video_ig65m_emb\n\t            else:\n\t                video_features = video_s3d_emb\n\t            # Project to the same dimension as text\n", "            video_features = self.mms_video_rotate(video_features)\n\t            # Encode with transformer model - Eq (3) in MLASK paper\n\t            transformer_encoded_video = self.mms_video_encoder(\n\t                src=self.mms_video_positional_encoding(video_features),\n\t                src_key_padding_mask=video_padding_mask,\n\t            )\n\t            transformer_encoded_video = self.dropout(transformer_encoded_video)\n\t            # Compute the text-queried visual features - same shape as hidden_states. Eq (9) in MLASK paper\n\t            transformer_encoded_video, _ = self.mms_video_text_attention(\n\t                query=hidden_states,\n", "                key=transformer_encoded_video,\n\t                value=transformer_encoded_video,\n\t                key_padding_mask=video_padding_mask,\n\t                need_weights=False,\n\t            )\n\t            # Use the forget gate - https://arxiv.org/pdf/2109.02401.pdf section 3.3\n\t            transformer_encoded_video_sigmoid = self.mms_sigmoid(\n\t                self.mms_forget_gate(\n\t                    torch.cat((hidden_states, transformer_encoded_video), -1)\n\t                )\n", "            )\n\t            transformer_encoded_video = transformer_encoded_video.mul(\n\t                transformer_encoded_video_sigmoid\n\t            )\n\t            # Apply the forget gate, Eq (10) in MLASK paper\n\t            hidden_states = self.mms_ff_forget_gate(\n\t                torch.cat((hidden_states, transformer_encoded_video), -1)\n\t            )\n\t            hidden_states = self.dropout(hidden_states)\n\t            # Frame selection part\n", "            if image_vit_emb is not None:\n\t                if image_effnet_emb is not None:\n\t                    image_seq_emb = torch.cat((image_vit_emb, image_effnet_emb), -1)\n\t                else:\n\t                    image_seq_emb = image_vit_emb\n\t            else:\n\t                image_seq_emb = image_effnet_emb\n\t            # Project to the same dimension as text\n\t            original_image_seq_emb = self.mms_image_rotate(image_seq_emb)\n\t            if self.use_image_self_attention:\n", "                # Eq (5) in MLASK paper\n\t                original_image_seq_emb = self.mms_image_self_attention(\n\t                    src=self.mms_video_positional_encoding(original_image_seq_emb),\n\t                    src_key_padding_mask=image_padding_mask,\n\t                )\n\t            encoder2image_mask = torch.ones_like(attention_mask)\n\t            # Different masking compared to huggingface modules\n\t            encoder2image_mask = (\n\t                encoder2image_mask.bool()\n\t                .masked_fill(attention_mask == 0, True)\n", "                .masked_fill(attention_mask == 1, False)\n\t            )\n\t            # Compute attention to fused video+text representation\n\t            image_seq_emb, _ = self.mms_image_source_attention(\n\t                query=original_image_seq_emb,\n\t                key=hidden_states,\n\t                value=hidden_states,\n\t                key_padding_mask=encoder2image_mask,\n\t                need_weights=False,\n\t            )\n", "            # Use the forget gate - https://arxiv.org/pdf/2109.02401.pdf section 3.3\n\t            image_seq_emb_sigmoid = self.mms_sigmoid(\n\t                self.mms_forget_gate_image(\n\t                    torch.cat((original_image_seq_emb, image_seq_emb), -1)\n\t                )\n\t            )\n\t            image_seq_emb = image_seq_emb.mul(image_seq_emb_sigmoid)\n\t            # $\\widehat{V}_{frame}$, Section 4.3 from MLASK paper\n\t            image_seq_emb = self.mms_ff_forget_gate_image(\n\t                torch.cat((original_image_seq_emb, image_seq_emb), -1)\n", "            )\n\t            # B x Num_Img_Frames\n\t            image_seq_emb_flattened = torch.squeeze(\n\t                self.mms_image_selector_projection(image_seq_emb)\n\t            )\n\t        # --MMS-- Text-Video Attention\n\t        # Add last layer\n\t        if output_hidden_states:\n\t            all_hidden_states = all_hidden_states + (hidden_states,)\n\t        if not return_dict:\n", "            return tuple(\n\t                v\n\t                for v in [\n\t                    hidden_states,\n\t                    present_key_value_states,\n\t                    all_hidden_states,\n\t                    all_attentions,\n\t                    all_cross_attentions,\n\t                    image_seq_emb_flattened,\n\t                ]\n", "                if v is not None\n\t            )\n\t        else:\n\t            return BaseModelOutputWithPastAndCrossAttentions(\n\t                last_hidden_state=hidden_states,\n\t                past_key_values=present_key_value_states,\n\t                hidden_states=all_hidden_states,\n\t                attentions=all_attentions,\n\t                cross_attentions=all_cross_attentions,\n\t            )\n", "@add_start_docstrings(\n\t    \"\"\"T5 Model with a `language modeling` head on top.\"\"\", T5_START_DOCSTRING\n\t)\n\tclass MMST5ForConditionalGeneration(T5PreTrainedModel):\n\t    _keys_to_ignore_on_load_missing = [\n\t        r\"encoder\\.embed_tokens\\.weight\",\n\t        r\"decoder\\.embed_tokens\\.weight\",\n\t        r\"lm_head\\.weight\",\n\t    ]\n\t    _keys_to_ignore_on_load_unexpected = [\n", "        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n\t    ]\n\t    def __init__(\n\t        self,\n\t        config,\n\t        num_video_enc_layers: int = 2,\n\t        use_video_ig65m: bool = True,\n\t        use_video_s3d: bool = True,\n\t        use_image_vit: bool = True,\n\t        use_image_effnet: bool = True,\n", "        smooth_cos_labels: bool = True,\n\t        use_image_self_attention: bool = False,\n\t    ):\n\t        super().__init__(config)\n\t        self.model_dim = config.d_model\n\t        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\t        encoder_config = copy.deepcopy(config)\n\t        encoder_config.is_decoder = False\n\t        encoder_config.use_cache = False\n\t        encoder_config.is_encoder_decoder = False\n", "        self.encoder = MMST5Stack(\n\t            encoder_config,\n\t            self.shared,\n\t            num_video_enc_layers,\n\t            use_video_ig65m,\n\t            use_video_s3d,\n\t            use_image_vit,\n\t            use_image_effnet,\n\t            use_image_self_attention,\n\t        )\n", "        self.smooth_cos_labels = smooth_cos_labels\n\t        self.use_image_self_attention = use_image_self_attention\n\t        self.cosine_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n\t        decoder_config = copy.deepcopy(config)\n\t        decoder_config.is_decoder = True\n\t        decoder_config.is_encoder_decoder = False\n\t        decoder_config.num_layers = config.num_decoder_layers\n\t        self.decoder = T5Stack(decoder_config, self.shared)\n\t        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\t        # Initialize weights and apply final processing\n", "        self.post_init()\n\t        # Model parallel\n\t        self.model_parallel = False\n\t        self.device_map = None\n\t    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n\t    def parallelize(self, device_map=None):\n\t        self.device_map = (\n\t            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n\t            if device_map is None\n\t            else device_map\n", "        )\n\t        assert_device_map(self.device_map, len(self.encoder.block))\n\t        self.encoder.parallelize(self.device_map)\n\t        self.decoder.parallelize(self.device_map)\n\t        self.lm_head = self.lm_head.to(self.decoder.first_device)\n\t        self.model_parallel = True\n\t    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n\t    def deparallelize(self):\n\t        self.encoder.deparallelize()\n\t        self.decoder.deparallelize()\n", "        self.encoder = self.encoder.to(\"cpu\")\n\t        self.decoder = self.decoder.to(\"cpu\")\n\t        self.lm_head = self.lm_head.to(\"cpu\")\n\t        self.model_parallel = False\n\t        self.device_map = None\n\t        torch.cuda.empty_cache()\n\t    def get_input_embeddings(self):\n\t        return self.shared\n\t    def set_input_embeddings(self, new_embeddings):\n\t        self.shared = new_embeddings\n", "        self.encoder.set_input_embeddings(new_embeddings)\n\t        self.decoder.set_input_embeddings(new_embeddings)\n\t    def set_output_embeddings(self, new_embeddings):\n\t        self.lm_head = new_embeddings\n\t    def get_output_embeddings(self):\n\t        return self.lm_head\n\t    def get_encoder(self):\n\t        return self.encoder\n\t    def get_decoder(self):\n\t        return self.decoder\n", "    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        attention_mask=None,\n\t        decoder_input_ids=None,\n\t        decoder_attention_mask=None,\n\t        head_mask=None,\n\t        decoder_head_mask=None,\n\t        cross_attn_head_mask=None,\n", "        encoder_outputs=None,\n\t        past_key_values=None,\n\t        inputs_embeds=None,\n\t        decoder_inputs_embeds=None,\n\t        labels=None,\n\t        use_cache=None,\n\t        output_attentions=None,\n\t        output_hidden_states=None,\n\t        return_dict=None,\n\t        video_ig65m_emb=None,\n", "        video_s3d_emb=None,\n\t        image_vit_emb=None,\n\t        image_effnet_emb=None,\n\t        video_padding_mask=None,\n\t        image_padding_mask=None,\n\t        tgt_img_cosine_scores=None,\n\t        tgt_image_vit_emb=None,\n\t        tgt_image_effnet_emb=None,\n\t    ):\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n", "        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n\t        if head_mask is not None and decoder_head_mask is None:\n\t            if self.config.num_layers == self.config.num_decoder_layers:\n\t                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n\t                decoder_head_mask = head_mask\n\t        image_selection_loss = None\n\t        # Encode if needed (training, first prediction pass)\n", "        if encoder_outputs is None:\n\t            # Convert encoder inputs in embeddings if needed\n\t            encoder_outputs = self.encoder(\n\t                input_ids=input_ids,\n\t                attention_mask=attention_mask,\n\t                inputs_embeds=inputs_embeds,\n\t                head_mask=head_mask,\n\t                output_attentions=output_attentions,\n\t                output_hidden_states=output_hidden_states,\n\t                return_dict=return_dict,\n", "                video_ig65m_emb=video_ig65m_emb,\n\t                video_s3d_emb=video_s3d_emb,\n\t                image_vit_emb=image_vit_emb,\n\t                image_effnet_emb=image_effnet_emb,\n\t                video_padding_mask=video_padding_mask,\n\t                image_padding_mask=image_padding_mask,\n\t            )\n\t            # While training\n\t            if tgt_img_cosine_scores is not None:\n\t                image_seq_emb_flattened = encoder_outputs[-1]\n", "                image_summary_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n\t                if image_vit_emb is not None:\n\t                    vit_cosine_sim = self.cosine_sim(\n\t                        image_vit_emb, torch.unsqueeze(tgt_image_vit_emb, 1)\n\t                    )\n\t                if image_effnet_emb is not None:\n\t                    effnet_cosine_sim = self.cosine_sim(\n\t                        image_effnet_emb, torch.unsqueeze(tgt_image_effnet_emb, 1)\n\t                    )\n\t                    if image_vit_emb is not None:\n", "                        cosine_sim = (vit_cosine_sim + effnet_cosine_sim) / 2\n\t                    else:\n\t                        cosine_sim = effnet_cosine_sim\n\t                else:\n\t                    cosine_sim = vit_cosine_sim\n\t                cosine_sim = torch.where(\n\t                    cosine_sim > 0, cosine_sim, torch.zeros_like(cosine_sim)\n\t                )\n\t                tgt_img_cosine_scores = cosine_sim\n\t                # Section 4.4 in MLASK paper, $C_{max}$ or $C_{smooth}$\n", "                if not self.smooth_cos_labels:\n\t                    tgt_image_labels = (\n\t                        nn.functional.one_hot(\n\t                            torch.argmax(tgt_img_cosine_scores, dim=1),\n\t                            num_classes=tgt_img_cosine_scores.shape[1],\n\t                        )\n\t                        .float()\n\t                        .view(-1)\n\t                    )\n\t                else:\n", "                    tgt_image_labels = tgt_img_cosine_scores.view(-1)\n\t                image_selection_loss = image_summary_loss(\n\t                    input=image_seq_emb_flattened.view(-1),\n\t                    target=tgt_img_cosine_scores.view(-1),\n\t                )\n\t                # Mask the loss - don't do this per batch but per flattened sequence\n\t                image_selection_loss = torch.mean(\n\t                    torch.where(\n\t                        image_padding_mask.view(-1) == 0,\n\t                        image_selection_loss,\n", "                        torch.zeros_like(image_selection_loss),\n\t                    )\n\t                )\n\t        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n\t            encoder_outputs = BaseModelOutput(\n\t                last_hidden_state=encoder_outputs[0],\n\t                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n\t                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n\t            )\n\t        hidden_states = encoder_outputs[0]\n", "        if self.model_parallel:\n\t            torch.cuda.set_device(self.decoder.first_device)\n\t        if (\n\t            labels is not None\n\t            and decoder_input_ids is None\n\t            and decoder_inputs_embeds is None\n\t        ):\n\t            # get decoder inputs from shifting lm labels to the right\n\t            decoder_input_ids = self._shift_right(labels)\n\t        # Set device for model parallelism\n", "        if self.model_parallel:\n\t            torch.cuda.set_device(self.decoder.first_device)\n\t            hidden_states = hidden_states.to(self.decoder.first_device)\n\t            if decoder_input_ids is not None:\n\t                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n\t            if attention_mask is not None:\n\t                attention_mask = attention_mask.to(self.decoder.first_device)\n\t            if decoder_attention_mask is not None:\n\t                decoder_attention_mask = decoder_attention_mask.to(\n\t                    self.decoder.first_device\n", "                )\n\t        # Decode\n\t        decoder_outputs = self.decoder(\n\t            input_ids=decoder_input_ids,\n\t            attention_mask=decoder_attention_mask,\n\t            inputs_embeds=decoder_inputs_embeds,\n\t            past_key_values=past_key_values,\n\t            encoder_hidden_states=hidden_states,\n\t            encoder_attention_mask=attention_mask,\n\t            head_mask=decoder_head_mask,\n", "            cross_attn_head_mask=cross_attn_head_mask,\n\t            use_cache=use_cache,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t        )\n\t        sequence_output = decoder_outputs[0]\n\t        # Set device for model parallelism\n\t        if self.model_parallel:\n\t            torch.cuda.set_device(self.encoder.first_device)\n", "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n\t            sequence_output = sequence_output.to(self.lm_head.weight.device)\n\t        if self.config.tie_word_embeddings:\n\t            # Rescale output before projecting on vocab\n\t            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n\t            sequence_output = sequence_output * (self.model_dim**-0.5)\n\t        lm_logits = self.lm_head(sequence_output)\n\t        loss = None\n\t        if labels is not None:\n\t            loss_fct = CrossEntropyLoss(ignore_index=-100)\n", "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n\t        if not return_dict:\n\t            output = (\n\t                (lm_logits,)\n\t                + decoder_outputs\n\t                + decoder_outputs[1:]\n\t                + encoder_outputs\n\t                + (image_seq_emb_flattened,)\n\t            )\n\t            if loss is not None:\n", "                assert image_selection_loss is not None\n\t                return (\n\t                    loss,\n\t                    image_selection_loss,\n\t                ) + output\n\t            else:\n\t                return (output,)\n\t        else:\n\t            return Seq2SeqLMOutput(\n\t                loss=loss,\n", "                logits=lm_logits,\n\t                past_key_values=decoder_outputs.past_key_values,\n\t                decoder_hidden_states=decoder_outputs.hidden_states,\n\t                decoder_attentions=decoder_outputs.attentions,\n\t                cross_attentions=decoder_outputs.cross_attentions,\n\t                encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n\t                encoder_hidden_states=encoder_outputs.hidden_states,\n\t                encoder_attentions=encoder_outputs.attentions,\n\t            )\n\t    def prepare_inputs_for_generation(\n", "        self,\n\t        input_ids,\n\t        past=None,\n\t        attention_mask=None,\n\t        head_mask=None,\n\t        decoder_head_mask=None,\n\t        cross_attn_head_mask=None,\n\t        use_cache=None,\n\t        encoder_outputs=None,\n\t        **kwargs,\n", "    ):\n\t        # cut decoder_input_ids if past is used\n\t        if past is not None:\n\t            input_ids = input_ids[:, -1:]\n\t        return {\n\t            \"decoder_input_ids\": input_ids,\n\t            \"past_key_values\": past,\n\t            \"encoder_outputs\": encoder_outputs,\n\t            \"attention_mask\": attention_mask,\n\t            \"head_mask\": head_mask,\n", "            \"decoder_head_mask\": decoder_head_mask,\n\t            \"cross_attn_head_mask\": cross_attn_head_mask,\n\t            \"use_cache\": use_cache,\n\t        }\n\t    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n\t        return self._shift_right(labels)\n\t    def _reorder_cache(self, past, beam_idx):\n\t        # if decoder past is not included in output\n\t        # speedy decoding is disabled and no need to reorder\n\t        if past is None:\n", "            logger.warning(\n\t                \"You might want to consider setting `use_cache=True` to speed up decoding\"\n\t            )\n\t            return past\n\t        reordered_decoder_past = ()\n\t        for layer_past_states in past:\n\t            # get the correct batch idx from layer past batch dim\n\t            # batch dim of `past` is at 2nd position\n\t            reordered_layer_past_states = ()\n\t            for layer_past_state in layer_past_states:\n", "                # need to set correct `past` for each of the four key / value states\n\t                reordered_layer_past_states = reordered_layer_past_states + (\n\t                    layer_past_state.index_select(\n\t                        0, beam_idx.to(layer_past_state.device)\n\t                    ),\n\t                )\n\t            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n\t            assert len(reordered_layer_past_states) == len(layer_past_states)\n\t            reordered_decoder_past = reordered_decoder_past + (\n\t                reordered_layer_past_states,\n", "            )\n\t        return reordered_decoder_past\n"]}
{"filename": "MultiSum/src/model/__init__.py", "chunked_list": []}
{"filename": "MultiSum/src/model/model_mms.py", "chunked_list": ["import json\n\timport os\n\timport unicodedata\n\timport math\n\tfrom typing import Union\n\timport sys\n\tfrom PIL import Image, ImageDraw, ImageFont\n\tfrom statistics import mean\n\timport pytorch_lightning as pl\n\timport torch\n", "import numpy as np\n\timport pandas as pd\n\timport nltk\n\tfrom torch import Tensor\n\tfrom torch import nn\n\tfrom torch.utils.data import Dataset, DataLoader\n\tfrom transformers import AutoTokenizer\n\tfrom pycocoevalcap.spice.spice import Spice\n\tfrom pycocoevalcap.cider.cider import Cider\n\tfrom evaluate import load\n", "from sacrebleu.metrics import BLEU\n\tfrom rouge_raw import RougeRaw\n\tfrom torchmetrics import RetrievalMAP, RetrievalRecall, RetrievalPrecision\n\tfrom scipy.stats import pearsonr, kendalltau\n\t# from nlgeval import NLGEval\n\tfrom nltk.translate.meteor_score import meteor_score\n\tfrom pycocotools.coco import COCO\n\tfrom image_similarity_measures.quality_metrics import rmse, psnr, ssim, sre\n\tfrom pycocoevalcap.eval import COCOEvalCap\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"../data\"))\n", "sys.path.append(os.path.join(os.path.dirname(__file__), \"../model\"))\n\tspice_scorer = Spice()\n\tcider_scorer = Cider()\n\tbertscore = load('bertscore')\n\tnltk.download('wordnet')\n\tnltk.download('omw-1.4')\n\ttorch.set_num_threads(2)\n\timport re\n\tdef clean_text(text):\n\t    # Remove non-alphanumeric characters\n", "    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n\t    # Remove extra whitespaces\n\t    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n\t    return cleaned_text\n\tdef parse_lists(predictions, references, parse_num):\n\t    parsed_predictions = []\n\t    parsed_references = []\n\t    for pred, refs in zip(predictions, references):\n\t        if len(pred) <= parse_num:\n\t            parsed_predictions.append(pred)\n", "            parsed_references.append(refs)\n\t        else:\n\t            pred_parts = [pred[i:i+parse_num] for i in range(0, len(pred), parse_num)]\n\t            parsed_predictions.extend(pred_parts)\n\t            parsed_references.extend([refs] * len(pred_parts))\n\t    return parsed_predictions, parsed_references\n\tfrom utils import generate_square_subsequent_mask\n\tfrom mms_modeling_t5 import MMST5ForConditionalGeneration\n\tIG65M_EM_SIZE = 512\n\tS3D_EMB_SIZE = 512\n", "VIT_EMB_SIZE = 768\n\tEFFNET_EMB_SIZE = 2048\n\tMAX_TGT_LEN = 250\n\tclass MultimodalTransformer(pl.LightningModule):\n\t    def __init__(\n\t        self,\n\t        num_video_enc_layers: int,\n\t        use_video_ig65m: bool = True,\n\t        use_video_s3d: bool = True,\n\t        use_image_vit: bool = True,\n", "        use_image_effnet: bool = True,\n\t        smooth_cos_labels: bool = False,\n\t        lr_max_val: float = 0.0005,\n\t        lr_init_val: float = 0,\n\t        lr_warmup_steps: int = 4000,\n\t        pre_trained_summeczech_ckpt: str = \"\",\n\t        start_with_text_frozen=0,\n\t        mask_video_features=False,\n\t        use_image_self_attention=True,\n\t        args = None,\n", "    ):\n\t        super().__init__()\n\t        # Sanity checks\n\t        assert use_video_ig65m or use_video_s3d\n\t        assert use_image_vit or use_image_effnet\n\t        self.save_hyperparameters()\n\t        self.model = None\n\t        self.args = args\n\t        self._create_model()\n\t    def _create_model(self):\n", "        # Used with pre-trained MT5 checkpoint\n\t        if self.hparams.pre_trained_summeczech_ckpt != \"\":\n\t            self.model = MMST5ForConditionalGeneration.from_pretrained(\n\t                self.hparams.pre_trained_summeczech_ckpt,\n\t                num_video_enc_layers=self.hparams.num_video_enc_layers,\n\t                use_video_ig65m=self.hparams.use_video_ig65m,\n\t                use_video_s3d=self.hparams.use_video_s3d,\n\t                use_image_vit=self.hparams.use_image_vit,\n\t                use_image_effnet=self.hparams.use_image_effnet,\n\t                smooth_cos_labels=self.hparams.smooth_cos_labels,\n", "                use_image_self_attention=self.hparams.use_image_self_attention,\n\t            )\n\t        else:\n\t            self.model = MMST5ForConditionalGeneration.from_pretrained(\n\t                \"google/mt5-small\",\n\t                num_video_enc_layers=self.hparams.num_video_enc_layers,\n\t                use_video_ig65m=self.hparams.use_video_ig65m,\n\t                use_video_s3d=self.hparams.use_video_s3d,\n\t                use_image_vit=self.hparams.use_image_vit,\n\t                use_image_effnet=self.hparams.use_image_effnet,\n", "                smooth_cos_labels=self.hparams.smooth_cos_labels,\n\t                use_image_self_attention=self.hparams.use_image_self_attention,\n\t            )\n\t        self.tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\t        self.cosine_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n\t        # Evaluation metrics\n\t        self.sacrebleu = BLEU()\n\t        self.rouge = RougeRaw()\n\t        self.rMAP = RetrievalMAP()\n\t        self.rRec_1 = RetrievalRecall(k=1)\n", "        self.rRec_5 = RetrievalRecall(k=5)\n\t        self.rRec_10 = RetrievalRecall(k=10)\n\t        self.rPre_1 = RetrievalPrecision(k=1)\n\t        self.rPre_5 = RetrievalPrecision(k=5)\n\t        self.rPre_10 = RetrievalPrecision(k=10)\n\t    def forward(\n\t        self,\n\t        input_ids=None,\n\t        attention_mask=None,\n\t        decoder_input_ids=None,\n", "        decoder_attention_mask=None,\n\t        head_mask=None,\n\t        decoder_head_mask=None,\n\t        cross_attn_head_mask=None,\n\t        encoder_outputs=None,\n\t        past_key_values=None,\n\t        inputs_embeds=None,\n\t        decoder_inputs_embeds=None,\n\t        labels=None,\n\t        use_cache=None,\n", "        output_attentions=None,\n\t        output_hidden_states=None,\n\t        return_dict=None,\n\t        video_ig65m_emb=None,\n\t        video_s3d_emb=None,\n\t        image_vit_emb=None,\n\t        image_effnet_emb=None,\n\t        video_padding_mask=None,\n\t        image_padding_mask=None,\n\t        tgt_img_cosine_scores=None,\n", "        tgt_image_vit_emb=None,\n\t        tgt_image_effnet_emb=None,\n\t    ):\n\t        return self.model.forward(\n\t            input_ids,\n\t            attention_mask,\n\t            decoder_input_ids,\n\t            decoder_attention_mask,\n\t            head_mask,\n\t            decoder_head_mask,\n", "            cross_attn_head_mask,\n\t            encoder_outputs,\n\t            past_key_values,\n\t            inputs_embeds,\n\t            decoder_inputs_embeds,\n\t            labels,\n\t            use_cache,\n\t            output_attentions,\n\t            output_hidden_states,\n\t            return_dict,\n", "            video_ig65m_emb,\n\t            video_s3d_emb,\n\t            image_vit_emb,\n\t            image_effnet_emb,\n\t            video_padding_mask,\n\t            image_padding_mask,\n\t            tgt_img_cosine_scores,\n\t            tgt_image_vit_emb,\n\t            tgt_image_effnet_emb,\n\t        )\n", "    def training_step(self, batch, batch_idx):\n\t        src_tokens = batch[\"src_ids\"]\n\t        src_padding_mask = batch[\"src_mask\"]\n\t        tgt_tokens = batch[\"tgt_ids\"]\n\t        tgt_padding_mask = batch[\"tgt_mask\"]\n\t        batch['src_img_cosine'] = []\n\t        tgt_img_cosine_scores = batch[\"src_img_cosine\"]\n\t        video_padding_mask = batch[\"video_mask\"]\n\t        image_padding_mask = batch[\"src_img_mask\"]\n\t        # Video features\n", "        if self.hparams.use_video_ig65m:\n\t            video_ig65m_emb = batch[\"video_features_ig65m\"]\n\t        else:\n\t            video_ig65m_emb = None\n\t        if self.hparams.use_video_s3d:\n\t            video_s3d_emb = batch[\"video_features_s3d\"]\n\t        else:\n\t            video_s3d_emb = None\n\t        # Image features\n\t        if self.hparams.use_image_vit:\n", "            image_vit_emb = batch[\"src_img_features_vit\"]\n\t            tgt_image_vit_emb = batch[\"tgt_img_features_vit\"]\n\t        else:\n\t            image_vit_emb = None\n\t            tgt_image_vit_emb = None\n\t        if self.hparams.use_image_effnet:\n\t            image_effnet_emb = batch[\"src_img_features_effnet\"]\n\t            tgt_image_effnet_emb = batch[\"tgt_img_features_effnet\"]\n\t        else:\n\t            image_effnet_emb = None\n", "            tgt_image_effnet_emb = None\n\t        if self.hparams.mask_video_features:\n\t            video_ig65m_emb = torch.randn_like(video_ig65m_emb)\n\t            video_s3d_emb = torch.randn_like(video_s3d_emb)\n\t        # Compute text summary loss\n\t        text_summary_loss, image_selection_loss, *_ = self.forward(\n\t            input_ids=src_tokens,\n\t            attention_mask=src_padding_mask,\n\t            decoder_attention_mask=tgt_padding_mask,\n\t            labels=tgt_tokens,\n", "            return_dict=False,\n\t            video_ig65m_emb=video_ig65m_emb,\n\t            video_s3d_emb=video_s3d_emb,\n\t            image_vit_emb=image_vit_emb,\n\t            image_effnet_emb=image_effnet_emb,\n\t            video_padding_mask=video_padding_mask,\n\t            image_padding_mask=image_padding_mask,\n\t            tgt_img_cosine_scores=tgt_img_cosine_scores,\n\t            tgt_image_vit_emb=tgt_image_vit_emb,\n\t            tgt_image_effnet_emb=tgt_image_effnet_emb,\n", "        )\n\t        self.log(\n\t            \"img_loss\",\n\t            image_selection_loss,\n\t            on_step=True,\n\t            on_epoch=False,\n\t            batch_size=len(batch),\n\t        )\n\t        self.log(\n\t            \"summary_loss\",\n", "            text_summary_loss,\n\t            on_step=True,\n\t            on_epoch=False,\n\t            batch_size=len(batch),\n\t        )\n\t        return text_summary_loss + image_selection_loss\n\t    def prediction_step(self, batch, batch_idx):\n\t        src_tokens = batch[\"src_ids\"]\n\t        src_padding_mask = batch[\"src_mask\"]\n\t        video_padding_mask = batch[\"video_mask\"]\n", "        image_padding_mask = batch[\"src_img_mask\"]\n\t        # Video features\n\t        if self.hparams.use_video_ig65m:\n\t            video_ig65m_emb = batch[\"video_features_ig65m\"]\n\t        else:\n\t            video_ig65m_emb = None\n\t        if self.hparams.use_video_s3d:\n\t            video_s3d_emb = batch[\"video_features_s3d\"]\n\t        else:\n\t            video_s3d_emb = None\n", "        # Image features\n\t        if self.hparams.use_image_vit:\n\t            image_vit_emb = batch[\"src_img_features_vit\"]\n\t        else:\n\t            image_vit_emb = None\n\t        if self.hparams.use_image_effnet:\n\t            image_effnet_emb = batch[\"src_img_features_effnet\"]\n\t        else:\n\t            image_effnet_emb = None\n\t        if self.hparams.mask_video_features:\n", "            video_ig65m_emb = torch.randn_like(video_ig65m_emb)\n\t            video_s3d_emb = torch.randn_like(video_s3d_emb)\n\t        # Generate the summary using the text and video features\n\t        txt_summary_tokens = self.model.generate(\n\t            input_ids=src_tokens,\n\t            attention_mask=src_padding_mask,\n\t            video_ig65m_emb=video_ig65m_emb,\n\t            video_s3d_emb=video_s3d_emb,\n\t            video_padding_mask=video_padding_mask,\n\t            image_effnet_emb=image_effnet_emb,\n", "            image_padding_mask=image_padding_mask,\n\t            image_vit_emb=image_vit_emb,\n\t            num_beams=4,\n\t            max_length=256,\n\t            repetition_penalty=2.5,\n\t            length_penalty=1.0,\n\t        )\n\t        predicted_sent = self.tokenizer.batch_decode(\n\t            txt_summary_tokens, skip_special_tokens=True\n\t        )\n", "        # Use the encoder explicitly to obtain the per-frame scores\n\t        *out2, per_frame_logits = self.model.encoder(\n\t            input_ids=src_tokens,\n\t            attention_mask=src_padding_mask,\n\t            video_ig65m_emb=video_ig65m_emb,\n\t            video_s3d_emb=video_s3d_emb,\n\t            video_padding_mask=video_padding_mask,\n\t            image_effnet_emb=image_effnet_emb,\n\t            image_padding_mask=image_padding_mask,\n\t            image_vit_emb=image_vit_emb,\n", "            return_dict=False,\n\t        )\n\t        per_frame_logits = per_frame_logits.masked_fill(image_padding_mask != 0.0, -1e6)\n\t        return {\"hyp\": predicted_sent, \"frame_scores\": per_frame_logits, \n\t                'encoder_hidden_state': out2[0]}\n\t    def validation_step(self, batch, batch_idx):\n\t        predictions = self.prediction_step(batch, batch_idx)\n\t        if self.hparams.use_image_vit:\n\t            if self.args.env == 'whole':\n\t                vit_cosine_sim = self.cosine_sim(\n", "                    batch[\"src_img_features_vit\"],\n\t                    torch.unsqueeze(batch[\"tgt_img_features_vit\"], 1), # if training whole\n\t                )\n\t            else:\n\t                vit_cosine_sim = self.cosine_sim(\n\t                    batch[\"src_img_features_vit\"],\n\t                    batch[\"tgt_img_features_vit\"], \n\t                )\n\t        if self.hparams.use_image_effnet:\n\t            effnet_cosine_sim = self.cosine_sim(\n", "                batch[\"src_img_features_effnet\"],\n\t                torch.unsqueeze(batch[\"tgt_img_features_effnet\"], 1),\n\t            )\n\t            if self.hparams.use_image_vit:\n\t                cosine_sim = (vit_cosine_sim + effnet_cosine_sim) / 2\n\t            else:\n\t                cosine_sim = effnet_cosine_sim\n\t        else:\n\t            cosine_sim = vit_cosine_sim\n\t        cosine_sim = torch.where(\n", "            cosine_sim > 0, cosine_sim, torch.zeros_like(cosine_sim)\n\t        ).cpu()\n\t        cosine_sim_raw = cosine_sim.detach().clone()\n\t        top_1_frame = torch.argmax(cosine_sim, dim=1)\n\t        ids = batch['_id']\n\t        hyp = predictions[\"hyp\"],\n\t        targ = batch[\"tgt\"]\n\t        sent = hyp[0]\n\t        refs = [sent for sent in targ]\n\t        save_dic = {\n", "            'sentences' : sent,\n\t            'references' : refs,\n\t            'selected_frames' : top_1_frame.tolist(),\n\t            'ids'       : ids\n\t        }\n\t        np.save(f'results_whole/results_whole_{batch_idx}.npy', save_dic)\n\t        top_1_frame_onehot = torch.nn.functional.one_hot(\n\t            top_1_frame, num_classes=cosine_sim.shape[-1]\n\t        )\n\t        # We want to select frames based on threshold, but also make sure\n", "        # that at least one frame (the most similar one, whatever the value) is chosen\n\t        cosine_sim.index_add_(\n\t            0,\n\t            torch.arange(0, cosine_sim.shape[0]).type(torch.int64),\n\t            top_1_frame_onehot.type(cosine_sim.type()),\n\t        )\n\t        mask = torch.ones(cosine_sim.size())\n\t        above_threshold_9 = (cosine_sim >= 0.9) * mask\n\t        above_threshold_75 = (cosine_sim >= 0.75) * mask\n\t        indices = torch.repeat_interleave(\n", "            torch.arange(0, cosine_sim.shape[0]).view(cosine_sim.shape[0], 1),\n\t            cosine_sim.shape[1],\n\t            dim=1,\n\t        ).view(-1)\n\t        # Average cosine similarity between top1 frame and target\n\t        cnn_cos_scores = []\n\t        for ind, top1_ind in enumerate(\n\t            np.array(\n\t                torch.topk(predictions[\"frame_scores\"].cpu(), dim=-1, k=1)[1].view(-1)\n\t            )\n", "        ):\n\t            if self.hparams.use_image_effnet:\n\t                _cos1 = self.cosine_sim(\n\t                    batch[\"tgt_img_features_effnet\"][ind],\n\t                    batch[\"src_img_features_effnet\"][ind][top1_ind],\n\t                )\n\t            if self.hparams.use_image_vit:\n\t                if self.args.env == 'whole':\n\t                    _cos2 = self.cosine_sim(\n\t                        batch[\"tgt_img_features_vit\"][ind],\n", "                        batch[\"src_img_features_vit\"][ind][top1_ind],\n\t                    )\n\t                else:\n\t                    _cos2 = self.cosine_sim(\n\t                        batch['tgt_img_features_vit'][ind],\n\t                        batch[\"src_img_features_vit\"][ind][0],\n\t                    )\n\t                if self.hparams.use_image_effnet:\n\t                    cnn_cos_scores.append((_cos1 + _cos2) / 2)\n\t                else:\n", "                    cnn_cos_scores.append(_cos2)\n\t            else:\n\t                cnn_cos_scores.append(_cos1)\n\t        # Correlation between model predictions and frame-scores\n\t        # We consider only the non-masked values\n\t        pearson_r_scores = []\n\t        kendall_tau_scores = []\n\t        for ind in range(predictions['frame_scores'].shape[0]):\n\t            _nonmasked_vals = (\n\t                (predictions[\"frame_scores\"][ind] == -1e6).nonzero().squeeze().view(-1)\n", "            )\n\t            if _nonmasked_vals.shape[0]:\n\t                _nonmasked_ind = _nonmasked_vals[0]\n\t                try:\n\t                    pearsonr_score = pearsonr(\n\t                                        cosine_sim_raw[ind][:_nonmasked_ind].numpy(),\n\t                                        predictions[\"frame_scores\"][ind][:_nonmasked_ind]\n\t                                        .cpu()\n\t                                        .detach()\n\t                                        .numpy(),\n", "                                        )[0]\n\t                    if math.isnan(pearsonr_score):\n\t                        pearson_r_scores.append(0)\n\t                    else:    \n\t                        pearson_r_scores.append(pearsonr_score)\n\t                except:\n\t                    pearson_r_scores.append(0)\n\t                kendall_tau_scores.append(\n\t                    kendalltau(\n\t                        cosine_sim_raw[ind][:_nonmasked_ind].numpy(),\n", "                        predictions[\"frame_scores\"][ind][:_nonmasked_ind]\n\t                        .cpu()\n\t                        .detach()\n\t                        .numpy(),\n\t                    )[0]\n\t                )\n\t            else:\n\t                pearson_r_scores.append(\n\t                    pearsonr(\n\t                        cosine_sim_raw[ind].numpy(),\n", "                        predictions[\"frame_scores\"][ind].cpu().detach().numpy(),\n\t                    )[0]\n\t                )\n\t                kendall_tau_scores.append(\n\t                    kendalltau(\n\t                        cosine_sim_raw[ind].numpy(),\n\t                        predictions[\"frame_scores\"][ind].cpu().detach().numpy(),\n\t                    )[0]\n\t                )\n\t        rMAP_top = self.rMAP(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rMAP_threshold_9 = self.rMAP(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_9.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rMAP_threshold_75 = self.rMAP(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_75.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_1_top = self.rRec_1(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_5_top = self.rRec_5(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_10_top = self.rRec_10(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rPre_1_top = self.rPre_1(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rPre_5_top = self.rPre_5(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rPre_10_top = self.rPre_10(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=top_1_frame_onehot.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_1_threshold_9 = self.rRec_1(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_9.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_1_threshold_75 = self.rRec_1(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_75.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_5_threshold_9 = self.rRec_5(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_9.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_5_threshold_75 = self.rRec_5(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_75.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_10_threshold_9 = self.rRec_10(\n\t            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_9.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rRec_10_threshold_75 = self.rRec_10(\n", "            preds=predictions[\"frame_scores\"].cpu().view(-1),\n\t            target=above_threshold_75.view(-1),\n\t            indexes=indices,\n\t        )\n\t        rmse_scores = []\n\t        psnr_scores = []\n\t        ssim_scores = []\n\t        sre_scores = []\n\t        for i in range(batch['tgt_img_features_vit'].shape[0]):\n\t            flattened_tensor = predictions['encoder_hidden_state'][i].flatten()\n", "            subsampled_tensor = flattened_tensor[torch.randperm(flattened_tensor.numel())[:2048]]\n\t            pred_img = subsampled_tensor.reshape(32, 64).unsqueeze(2)\n\t            orig_img = batch['tgt_img_features_vit'][i].reshape(32, 64).unsqueeze(2)\n\t            rmse_scores.append(rmse(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n\t            psnr_scores.append(psnr(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n\t            ssim_scores.append(ssim(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n\t            sre_scores.append(sre(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n\t        rmse_score = sum(rmse_scores)/len(rmse_scores)\n\t        psnr_score = sum(psnr_scores)/len(psnr_scores)\n\t        ssim_score = sum(ssim_scores)/len(ssim_scores)\n", "        sre_score = sum(sre_scores)/len(sre_scores)\n\t        return {\n\t            \"hyp\": predictions[\"hyp\"],\n\t            \"ref\": batch[\"tgt\"],\n\t            \"cnn_cos_scores\": np.mean([_score.cpu() for _score in cnn_cos_scores]),\n\t            \"mAP_top\": rMAP_top.numpy(),\n\t            \"mAP_threshold_0.9\": rMAP_threshold_9.numpy(),\n\t            \"mAP_threshold_0.75\": rMAP_threshold_75.numpy(),\n\t            \"Rec_1_top\": rRec_1_top.numpy(),\n\t            \"Rec_5_top\": rRec_5_top.numpy(),\n", "            \"Rec_10_top\": rRec_10_top.numpy(),\n\t            \"Pre_1_top\": rPre_1_top.numpy(),\n\t            \"Pre_5_top\": rPre_5_top.numpy(),\n\t            \"Pre_10_top\": rPre_10_top.numpy(),\n\t            \"Rec_1_threshold_0.9\": rRec_1_threshold_9.numpy(),\n\t            \"Rec_5_threshold_0.75\": rRec_5_threshold_75.numpy(),\n\t            \"Rec_10_threshold_0.9\": rRec_10_threshold_9.numpy(),\n\t            \"Rec_1_threshold_0.75\": rRec_1_threshold_75.numpy(),\n\t            \"Rec_5_threshold_0.9\": rRec_5_threshold_9.numpy(),\n\t            \"Rec_10_threshold_0.75\": rRec_10_threshold_75.numpy(),\n", "            \"Pearson_r\": np.mean(pearson_r_scores),\n\t            \"Kendall_tau\": np.mean(kendall_tau_scores),\n\t            'RMSE_score' : rmse_score,\n\t            'PSNR_score' : psnr_score,\n\t            'SSIM_score' : ssim_score,\n\t            'SRE_score'  : sre_score,\n\t            'encoder_hidden_state' : predictions['encoder_hidden_state'],\n\t        }\n\t    def validation_epoch_end(self, outputs):\n\t        # Frame selection evaluation\n", "        cnn_cos_scores = np.mean([_r[\"cnn_cos_scores\"] for _r in outputs])\n\t        mAP_top = np.mean([_r[\"mAP_top\"] for _r in outputs])\n\t        mAP_threshold_9 = np.mean([_r[\"mAP_threshold_0.9\"] for _r in outputs])\n\t        mAP_threshold_75 = np.mean([_r[\"mAP_threshold_0.75\"] for _r in outputs])\n\t        Rec_1_top = np.mean([_r[\"Rec_1_top\"] for _r in outputs])\n\t        Rec_5_top = np.mean([_r[\"Rec_5_top\"] for _r in outputs])\n\t        Rec_10_top = np.mean([_r[\"Rec_10_top\"] for _r in outputs])\n\t        Pre_1_top = np.mean([_r[\"Pre_1_top\"] for _r in outputs])\n\t        Pre_5_top = np.mean([_r[\"Pre_5_top\"] for _r in outputs])\n\t        Pre_10_top = np.mean([_r[\"Pre_10_top\"] for _r in outputs])\n", "        Rec_1_threshold_9 = np.mean([_r[\"Rec_1_threshold_0.9\"] for _r in outputs])\n\t        Rec_1_threshold_75 = np.mean([_r[\"Rec_1_threshold_0.75\"] for _r in outputs])\n\t        Rec_5_threshold_9 = np.mean([_r[\"Rec_5_threshold_0.9\"] for _r in outputs])\n\t        Rec_5_threshold_75 = np.mean([_r[\"Rec_5_threshold_0.75\"] for _r in outputs])\n\t        Rec_10_threshold_9 = np.mean([_r[\"Rec_10_threshold_0.9\"] for _r in outputs])\n\t        Rec_10_threshold_75 = np.mean([_r[\"Rec_10_threshold_0.75\"] for _r in outputs])\n\t        Pearson_r = np.mean([_r[\"Pearson_r\"] for _r in outputs])\n\t        Kendall_tau = np.mean([_r[\"Kendall_tau\"] for _r in outputs])\n\t        PSNR = np.mean([_r[\"PSNR_score\"] for _r in outputs])\n\t        RMSE = np.mean([_r[\"RMSE_score\"] for _r in outputs])\n", "        SSIM = np.mean([_r[\"SSIM_score\"] for _r in outputs])\n\t        SRE = np.mean([_r[\"SRE_score\"] for _r in outputs])\n\t        self.log('PSNR', PSNR, on_step = False, on_epoch = True, sync_dist = True)\n\t        self.log('RMSE', RMSE, on_step = False, on_epoch = True, sync_dist = True)\n\t        self.log('SSIM', SSIM, on_step = False, on_epoch = True, sync_dist = True)\n\t        self.log('SRE', SRE, on_step = False, on_epoch = True, sync_dist = True)\n\t        self.log(\n\t            \"cnn_cos_scores\",\n\t            cnn_cos_scores,\n\t            on_step=False,\n", "            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\"Pearson_r\", Pearson_r, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\n\t            \"Kendall_tau\", Kendall_tau, on_step=False, on_epoch=True, sync_dist=True\n\t        )\n\t        self.log(\"mAP_top\", mAP_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\n\t            \"mAP_threshold_0.9\",\n", "            mAP_threshold_9,\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"mAP_threshold_0.75\",\n\t            mAP_threshold_75,\n\t            on_step=False,\n\t            on_epoch=True,\n", "            sync_dist=True,\n\t        )\n\t        self.log(\"Rec_1_top\", Rec_1_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"Rec_5_top\", Rec_5_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"Rec_10_top\", Rec_10_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"Pre_1_top\", Pre_1_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"Pre_5_top\", Pre_5_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"Pre_10_top\", Pre_10_top, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"F1_1_top\", (2*Pre_1_top*Rec_1_top)/(Pre_1_top+Rec_1_top), on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\"F1_5_top\", (2*Pre_5_top*Rec_5_top)/(Pre_5_top+Rec_5_top), on_step=False, on_epoch=True, sync_dist=True)\n", "        self.log(\"F1_10_top\", (2*Pre_10_top*Rec_10_top)/(Pre_10_top+Rec_10_top), on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\n\t            \"Rec_1_threshold_0.9\",\n\t            Rec_1_threshold_9,\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"Rec_1_threshold_0.75\",\n", "            Rec_1_threshold_75,\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"Rec_5_threshold_0.9\",\n\t            Rec_5_threshold_9,\n\t            on_step=False,\n\t            on_epoch=True,\n", "            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"Rec_5_threshold_0.75\",\n\t            Rec_5_threshold_75,\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n", "            \"Rec_10_threshold_0.9\",\n\t            Rec_10_threshold_9,\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"Rec_10_threshold_0.75\",\n\t            Rec_10_threshold_75,\n\t            on_step=False,\n", "            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        # Text summarization evaluation\n\t        predictions = [sent for _item in outputs for sent in _item[\"hyp\"]]\n\t        refs = [sent for _item in outputs for sent in _item[\"ref\"]]\n\t        bleu_score = self.sacrebleu.corpus_score(predictions, refs).score\n\t        rouge_score = self.rouge.corpus(refs, predictions)\n\t        bert_score = bertscore.compute(predictions = predictions, references = refs, lang=\"en\")\n\t        refs = [[s] for s in refs]\n", "        meteor = meteor_score(refs, predictions)\n\t        b_p = mean(bert_score['precision'])\n\t        b_f1 = mean(bert_score['f1'])\n\t        b_r = mean(bert_score['recall'])\n\t        self.log('BERTSCORE_F1', b_f1, on_step = False, on_epoch=True, sync_dist=True)\n\t        self.log('BERTSCORE_P', b_p, on_step = False, on_epoch=True, sync_dist=True)\n\t        self.log('BERTSCORE_R', b_r, on_step = False, on_epoch=True, sync_dist=True)\n\t        self.log('METEOR', meteor, on_step = False, on_epoch=True, sync_dist=True)\n\t        refs = {str(i): r for i, r in enumerate(refs)}\n\t        preds = {str(i): [clean_text(p)] for i, p in enumerate(predictions)}\n", "        try:\n\t            spice_score, _ = spice_scorer.compute_score(refs, preds)\n\t        except:\n\t            spice_score = 0\n\t        cider_score, _ = cider_scorer.compute_score(refs, preds)\n\t        self.log('SPICE', spice_score, on_step = False, on_epoch=True, sync_dist=True)\n\t        self.log('CIDER', cider_score, on_step = False, on_epoch=True, sync_dist=True)\n\t        self.log(\"BLEU\", bleu_score, on_step=False, on_epoch=True, sync_dist=True)\n\t        self.log(\n\t            \"ROUGE_RAW_1_P\",\n", "            round(100 * rouge_score[\"1\"].p, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"ROUGE_RAW_1_R\",\n\t            round(100 * rouge_score[\"1\"].r, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n", "            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"ROUGE_RAW_1_F\",\n\t            round(100 * rouge_score[\"1\"].f, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n", "            \"ROUGE_RAW_2_P\",\n\t            round(100 * rouge_score[\"2\"].p, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"ROUGE_RAW_2_R\",\n\t            round(100 * rouge_score[\"2\"].r, 2),\n\t            on_step=False,\n", "            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"ROUGE_RAW_2_F\",\n\t            round(100 * rouge_score[\"2\"].f, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n", "        self.log(\n\t            \"ROUGE_RAW_L_P\",\n\t            round(100 * rouge_score[\"L\"].p, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"ROUGE_RAW_L_R\",\n\t            round(100 * rouge_score[\"L\"].r, 2),\n", "            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n\t        )\n\t        self.log(\n\t            \"ROUGE_RAW_L_F\",\n\t            round(100 * rouge_score[\"L\"].f, 2),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            sync_dist=True,\n", "        )\n\t    def configure_optimizers(self):\n\t        # Enable different learning rate for pre-trained weights\n\t        model_params = list(self.model.named_parameters())\n\t        def is_mms(n):\n\t            return \"mms_\" in n\n\t        grouped_parameters = [\n\t            {\"params\": [p for n, p in model_params if is_mms(n)]},\n\t            {\"params\": [p for n, p in model_params if not is_mms(n)]},\n\t        ]\n", "        return torch.optim.Adam(\n\t            grouped_parameters,\n\t            lr=self.hparams.lr_init_val,\n\t            betas=(0.9, 0.98),\n\t            eps=1e-09,\n\t        )\n\t    def optimizer_step(\n\t        self,\n\t        epoch,\n\t        batch_idx,\n", "        optimizer,\n\t        optimizer_idx,\n\t        optimizer_closure,\n\t        on_tpu=False,\n\t        using_native_amp=False,\n\t        using_lbfgs=False,\n\t    ):\n\t        optimizer.step(closure=optimizer_closure)\n\t        # Linear increase for the first warmup_steps, then inverse square root decrease\n\t        if self.trainer.global_step < self.hparams.lr_warmup_steps:\n", "            lr = self.hparams.lr_init_val + self.trainer.global_step * (\n\t                (self.hparams.lr_max_val - self.hparams.lr_init_val)\n\t                / self.hparams.lr_warmup_steps\n\t            )\n\t        else:\n\t            lr = (\n\t                self.hparams.lr_max_val\n\t                * self.hparams.lr_warmup_steps**0.5\n\t                * self.trainer.global_step**-0.5\n\t            )\n", "        lr_mms = lr\n\t        lr_text = lr\n\t        if (\n\t            self.hparams.start_with_text_frozen\n\t            and self.trainer.current_epoch < self.hparams.start_with_text_frozen\n\t        ):\n\t            lr_text = 0.0\n\t        # MMS params\n\t        optimizer.param_groups[0][\"lr\"] = lr_mms\n\t        # Pre-trained text params\n", "        optimizer.param_groups[1][\"lr\"] = lr_text\n\t        self.log(\"learning_rate_mms\", lr_mms, on_step=True, on_epoch=False)\n\t        self.log(\"learning_rate_text\", lr_text, on_step=True, on_epoch=False)\n"]}
{"filename": "MultiSum/src/model/rouge_raw.py", "chunked_list": ["#!/usr/bin/env python3\n\t#\n\t# This file is part of SumeCzech corpus <http://hdl.handle.net/11234/1-2615>.\n\t#\n\t# Copyright 2018 Institute of Formal and Applied Linguistics, Faculty of\n\t# Mathematics and Physics, Charles University in Prague, Czech Republic.\n\t#\n\t# This Source Code Form is subject to the terms of the Mozilla Public\n\t# License, v. 2.0. If a copy of the MPL was not distributed with this\n\t# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n", "import re\n\tclass RougeRaw:\n\t    \"\"\"Compute RougeRAW-1, RougeRAW-2, RougeRAW-L metrics.\"\"\"\n\t    class FScore:\n\t        \"\"\"F1 score representation.\"\"\"\n\t        def __init__(self, correct, gold, system):\n\t            self.p = correct / system if system else 0.\n\t            self.r = correct / gold if gold else 0.\n\t            self.f = 2 * correct / (system + gold) if system + gold else 0.\n\t    def _rouge_n(self, n, gold_words, system_words):\n", "        \"\"\"Compute Rouge-n for given words.\"\"\"\n\t        def n_grams(n, words):\n\t            ngrams = {}\n\t            total = 0\n\t            for i in range(len(words) - n + 1):\n\t                ngram = \"\\t\".join(words[i:i + n])\n\t                ngrams[ngram] = 1 + ngrams.get(ngram, 0)\n\t                total += 1\n\t            return ngrams, total\n\t        gold_ngrams, gold_total = n_grams(n, gold_words)\n", "        system_ngrams, system_total = n_grams(n, system_words)\n\t        intersection = 0\n\t        for ngram in system_ngrams:\n\t            intersection += min(system_ngrams[ngram], gold_ngrams.get(ngram, 0))\n\t        return self.FScore(intersection, gold_total, system_total)\n\t    def _rouge_l(self, gold_words, system_words):\n\t        \"\"\"Compute Rouge-L for given words.\"\"\"\n\t        lcs = [[0] * len(system_words) for _ in gold_words]\n\t        for r in range(len(gold_words)):\n\t            for s in range(len(system_words)):\n", "                if gold_words[r] == system_words[s]:\n\t                    lcs[r][s] = 1 + (lcs[r - 1][s - 1] if r and s else 0)\n\t                lcs[r][s] = max(lcs[r][s], lcs[r - 1][s] if r else 0)\n\t                lcs[r][s] = max(lcs[r][s], lcs[r][s - 1] if s else 0)\n\t        return self.FScore(lcs[-1][-1], len(gold_words), len(system_words))\n\t    def _tokenize(self, text):\n\t        \"\"\"Tokenize given text.\"\"\"\n\t        return re.sub(r\"\\s+\", \" \", re.sub(r\"\\b\", \" \", text, re.UNICODE), re.UNICODE).strip().split(\" \")\n\t    def document(self, gold, system):\n\t        \"\"\"Compute RougeRAW-1, RougeRAW-2, RougeRAW-L for given documents.\n", "        Each document should be a string.\n\t        \"\"\"\n\t        assert isinstance(gold, str) and isinstance(system, str), \"Expected string arguments\"\n\t        lc_gold_words = [word.lower() for word in self._tokenize(gold)]\n\t        lc_system_words = [word.lower() for word in self._tokenize(system)]\n\t        return {\n\t            \"1\": self._rouge_n(1, lc_gold_words, lc_system_words),\n\t            \"2\": self._rouge_n(2, lc_gold_words, lc_system_words),\n\t            \"L\": self._rouge_l(lc_gold_words, lc_system_words),\n\t        }\n", "    def corpus(self, gold, system):\n\t        \"\"\"Compute RougeRAW-1, RougeRAW-2, RougeRAW-L for given corpora.\n\t        Each corpus should be a collection of documents, each document a string.\n\t        \"\"\"\n\t        assert isinstance(gold, list) and isinstance(system, list), \"Expected list arguments\"\n\t        assert len(gold) == len(system), \"Given corpora should be of the same length\"\n\t        rouge = {key: self.FScore(0, 0, 0) for key in [\"1\", \"2\", \"L\"]}\n\t        if len(gold):\n\t            for gold_document, system_document in zip(gold, system):\n\t                for key, value in self.document(gold_document, system_document).items():\n", "                    rouge[key].p += value.p\n\t                    rouge[key].r += value.r\n\t                    rouge[key].f += value.f\n\t            for key in rouge:\n\t                rouge[key].p /= len(gold)\n\t                rouge[key].r /= len(gold)\n\t                rouge[key].f /= len(gold)\n\t        return rouge\n\tif __name__ == \"__main__\":\n\t    import argparse\n", "    import json\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"gold\", type=str, help=\"Gold jsonl file path\")\n\t    parser.add_argument(\"system\", type=str, help=\"System jsonl output file\")\n\t    parser.add_argument(\"field\", type=str, help=\"Which jsonl field to compare\")\n\t    args = parser.parse_args()\n\t    gold = []\n\t    with open(args.gold, \"r\", encoding=\"utf-8\") as gold_file:\n\t        for gold_line in gold_file:\n\t            gold.append(json.loads(gold_line)[args.field])\n", "    system = []\n\t    with open(args.system, \"r\", encoding=\"utf-8\") as system_file:\n\t        for system_line in system_file:\n\t            system.append(json.loads(system_line)[args.field])\n\t    rouge = RougeRaw().corpus(gold, system)\n\t    print(\"  RougeRAW-1      RougeRAW-2      RougeRAW-L\")\n\t    print(\"  P    R    F     P    R    F     P    R    F\")\n\t    for metric in [\"1\", \"2\", \"L\"]:\n\t        print(\"{:04.1f} {:04.1f} {:04.1f}{}\".format(\n\t            100 * rouge[metric].p,\n", "            100 * rouge[metric].r,\n\t            100 * rouge[metric].f,\n\t            \"\\n\" if metric == \"L\" else \"  \"), end=\"\")\n"]}
