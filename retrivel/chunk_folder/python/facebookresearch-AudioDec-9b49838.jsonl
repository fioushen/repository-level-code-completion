{"filename": "demoStream.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t### useage ###\n\t# (run w/ gpu): python dempStream.py --tx_cuda 1 --rx_cuda 2 --model libritts_v1 --input_device x --output_device o \n\t# (run w/ cpu): python dempStream.py --tx_cuda -1 --rx_cuda -1 --model libritts_sym --input_device x --output_device o \n", "import torch\n\timport argparse\n\tfrom utils.audiodec import AudioDec, AudioDecStreamer, assign_model\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--model\", type=str, default=\"libritts_sym\")\n\t    parser.add_argument(\"-i\", \"--input\", type=str, default=\"input.wav\")\n\t    parser.add_argument(\"-o\", \"--output\", type=str, default=\"output.wav\")\n\t    parser.add_argument('--tx_cuda', type=int, default=-1 )\n\t    parser.add_argument('--rx_cuda', type=int, default=-1 )\n", "    parser.add_argument('--input_device', type=int, default=1)\n\t    parser.add_argument('--output_device', type=int, default=4)\n\t    parser.add_argument('--frame_size', type=int, default=1200)\n\t    parser.add_argument('--num_threads', type=int, default=4)\n\t    args = parser.parse_args()\n\t    # device assignment\n\t    if args.tx_cuda < 0:\n\t        tx_device = f'cpu'\n\t    else:\n\t        tx_device = f'cuda:{args.tx_cuda}'\n", "    if args.rx_cuda < 0:\n\t        rx_device = f'cpu'\n\t    else:\n\t        rx_device = f'cuda:{args.rx_cuda}'\n\t    torch.set_num_threads(args.num_threads)\n\t    # model assignment\n\t    sample_rate, encoder_checkpoint, decoder_checkpoint = assign_model(args.model)\n\t    # AudioDec initinalize\n\t    print(\"AudioDec initinalizing!\")\n\t    audiodec = AudioDec(tx_device=tx_device, rx_device=rx_device)\n", "    audiodec.load_transmitter(encoder_checkpoint)\n\t    audiodec.load_receiver(encoder_checkpoint, decoder_checkpoint)\n\t    # Streamer initinalize\n\t    print(\"Streamer initinalizing!\")\n\t    streamer = AudioDecStreamer(\n\t        input_device=args.input_device,\n\t        output_device=args.output_device,\n\t        frame_size=args.frame_size,\n\t        sample_rate=sample_rate,\n\t        tx_encoder=audiodec.tx_encoder,\n", "        tx_device=tx_device,\n\t        rx_encoder=audiodec.rx_encoder,\n\t        decoder=audiodec.decoder,\n\t        rx_device=rx_device,\n\t    )\n\t    streamer.enable_filedump(\n\t        input_stream_file=args.input,\n\t        output_stream_file=args.output,\n\t    )\n\t    # run\n", "    print(\"Ready to run!\")\n\t    latency=\"low\"\n\t    # TODO this is responsible for ~100ms latency, seems to be driver dependent. latency=0 works on Mac but not on Windows\n\t    streamer.run(latency)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "codecStatistic.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\timport os\n", "import sys\n\timport yaml\n\timport torch\n\timport logging\n\timport argparse\n\timport numpy as np\n\timport soundfile as sf\n\tfrom tqdm import tqdm\n\tfrom sklearn.preprocessing import StandardScaler\n\tfrom dataloader import SingleDataset\n", "from models.autoencoder.AudioDec import Generator as generator_audiodec\n\tclass StatisticMain(object):\n\t    def __init__(self, args,):\n\t        # set logger\n\t        logging.basicConfig(\n\t            level=logging.INFO,\n\t            stream=sys.stdout,\n\t            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n\t        )\n\t        # device\n", "        if not torch.cuda.is_available():\n\t            self.device = torch.device('cpu')\n\t            logging.info(f\"device: cpu\")\n\t        else:\n\t            self.device = torch.device('cuda')\n\t            logging.info(f\"device: gpu\")\n\t        # initialize config\n\t        with open(args.config, 'r') as f:\n\t            self.config = yaml.load(f, Loader=yaml.FullLoader)\n\t        # initialize attribute\n", "        self.stats_path = self.config['stats']\n\t        self.analyzer_checkpoint = self.config['analyzer']\n\t        self.analyzer_config = self._load_config(self.analyzer_checkpoint)\n\t        self.model_type = self.analyzer_config.get('model_type', 'symAudioDec')\n\t        os.makedirs(os.path.dirname(self.stats_path), exist_ok=True) \n\t    def _load_config(self, checkpoint, config_name='config.yml'):\n\t        dirname = os.path.dirname(checkpoint)\n\t        config_path = os.path.join(dirname, config_name)\n\t        with open(config_path) as f:\n\t            config = yaml.load(f, Loader=yaml.Loader)\n", "        return config\n\t    def load_dataset(self, subset, subset_num):\n\t        audio_path = os.path.join(\n\t            self.config['data']['path'], \n\t            self.config['data']['subset'][subset],\n\t        )\n\t        assert os.path.exists(audio_path), f\"{audio_path} does not exist!\"\n\t        self.dataset = SingleDataset(\n\t            files=audio_path,\n\t            query=\"*.wav\",\n", "            load_fn=sf.read,\n\t            return_utt_id=False,\n\t            subset_num=subset_num,\n\t        )\n\t        logging.info(f\"The number of {subset} audio files = {len(self.dataset)}.\")\n\t    def load_analyzer(self):\n\t        if self.model_type in ['symAudioDec', 'symAudioDecUniv']:\n\t            analyzer = generator_audiodec\n\t        else:     \n\t            raise NotImplementedError(f\"Analyzer {self.model_type} is not supported!\")\n", "        self.analyzer = analyzer(**self.analyzer_config['generator_params'])\n\t        self.analyzer.load_state_dict(\n\t            torch.load(self.analyzer_checkpoint, map_location='cpu')['model']['generator'])\n\t        self.analyzer = self.analyzer.eval().to(self.device)\n\t        logging.info(f\"Loaded Analyzer from {self.analyzer_checkpoint}.\")\n\t    def audio_analysis(self, audio):\n\t        x = torch.tensor(audio, dtype=torch.float).to(self.device)\n\t        x = x.transpose(1, 0).unsqueeze(0) # (T, C) -> (1, C, T)\n\t        x = self.analyzer.encoder(x)\n\t        z = self.analyzer.projector(x)\n", "        zq, _, _ = self.analyzer.quantizer(z)\n\t        return zq.squeeze(0).transpose(1, 0).cpu().numpy() # (T', C)\n\t    def run(self):\n\t        with torch.no_grad(), tqdm(self.dataset, desc=\"[statistic]\") as pbar:\n\t            scaler = StandardScaler()\n\t            for idx, x in enumerate(pbar, 1):\n\t                zq = self.audio_analysis(x)\n\t                scaler.partial_fit(zq)\n\t            stats = np.stack([scaler.mean_, scaler.scale_], axis=0)\n\t            np.save(\n", "                    self.stats_path,\n\t                    stats.astype(np.float32),\n\t                    allow_pickle=False,\n\t            )   \n\t        logging.info(f\"Finished statistical calculation of {idx} utterances.\")\n\tdef main():\n\t    \"\"\"Run feature extraction process.\"\"\"\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-c', '--config', type=str, required=True)\n\t    parser.add_argument(\"--subset\", type=str, default=\"train\")\n", "    parser.add_argument(\"--subset_num\", type=int, default=-1)\n\t    args = parser.parse_args()\n\t    # initial statistic_main\n\t    statistic_main = StatisticMain(args=args) \n\t    # load dataset\n\t    statistic_main.load_dataset(args.subset, args.subset_num)\n\t    # load analyzer\n\t    statistic_main.load_analyzer()\n\t    # run testing\n\t    statistic_main.run()\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "demoFile.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t### useage ###\n\t# (run w/ gpu): python demoFile.py --model libritts_v1 -i xxx.wav -o ooo.wav\n\t# (run w/ cpu): python demoFile.py --cuda -1 --model libritts_sym -i xxx.wav -o ooo.wav\n", "import os\n\timport torch\n\timport argparse\n\timport numpy as np\n\timport soundfile as sf\n\tfrom utils.audiodec import AudioDec, assign_model\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--model\", type=str, default=\"libritts_v1\")\n\t    parser.add_argument(\"-i\", \"--input\", type=str, required=True)\n", "    parser.add_argument(\"-o\", \"--output\", type=str, required=True)\n\t    parser.add_argument('--cuda', type=int, default=0 )\n\t    parser.add_argument('--num_threads', type=int, default=4)\n\t    args = parser.parse_args()\n\t    # device assignment\n\t    if args.cuda < 0:\n\t        tx_device = f'cpu'\n\t        rx_device = f'cpu'\n\t    else:\n\t        tx_device = f'cuda:{args.cuda}'\n", "        rx_device = f'cuda:{args.cuda}'\n\t    torch.set_num_threads(args.num_threads)\n\t    # model assignment\n\t    sample_rate, encoder_checkpoint, decoder_checkpoint = assign_model(args.model)\n\t    # AudioDec initinalize\n\t    print(\"AudioDec initinalizing!\")\n\t    audiodec = AudioDec(tx_device=tx_device, rx_device=rx_device)\n\t    audiodec.load_transmitter(encoder_checkpoint)\n\t    audiodec.load_receiver(encoder_checkpoint, decoder_checkpoint)\n\t    with torch.no_grad():\n", "        if os.path.exists(args.input):\n\t            data, fs = sf.read(args.input, always_2d=True)\n\t        else:\n\t            raise ValueError(f'Input file {args.input} does not exist!')\n\t        assert fs == sample_rate, f\"data ({fs}Hz) is not matched to model ({sample_rate}Hz)!\"\n\t        x = np.expand_dims(data.transpose(1, 0), axis=1) # (T, C) -> (C, 1, T)\n\t        x = torch.tensor(x, dtype=torch.float).to(tx_device)\n\t        print(\"Encode/Decode...\")\n\t        z = audiodec.tx_encoder.encode(x)\n\t        idx = audiodec.tx_encoder.quantize(z)\n", "        zq = audiodec.rx_encoder.lookup(idx)\n\t        y = audiodec.decoder.decode(zq)[:, :, :x.size(-1)]\n\t        y = y.squeeze(1).transpose(1, 0).cpu().numpy() # T x C\n\t        sf.write(\n\t            args.output,\n\t            y,\n\t            fs,\n\t            \"PCM_16\",\n\t        )\n\t        print(f\"Output {args.output}!\")\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "codecTrain.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\timport os\n", "import logging\n\timport argparse\n\timport torch\n\timport soundfile as sf\n\tfrom torch.utils.data import DataLoader\n\tfrom dataloader import CollaterAudio, CollaterAudioPair\n\tfrom dataloader import SingleDataset, MultiDataset\n\tfrom models.autoencoder.AudioDec import Generator as generator_audiodec\n\tfrom models.vocoder.HiFiGAN import Generator as generator_hifigan\n\tfrom models.vocoder.HiFiGAN import Discriminator as discriminator_hifigan\n", "from models.vocoder.UnivNet import Discriminator as discriminator_univnet\n\tfrom trainer.autoencoder import Trainer as TrainerAutoEncoder\n\tfrom trainer.vocoder import Trainer as TrainerVocoder\n\tfrom trainer.denoise import Trainer as TrainerDenoise\n\tfrom bin.train import TrainGAN\n\tfrom losses import DiscriminatorAdversarialLoss\n\tfrom losses import FeatureMatchLoss\n\tfrom losses import GeneratorAdversarialLoss\n\tfrom losses import MultiResolutionSTFTLoss\n\tfrom losses import MultiMelSpectrogramLoss\n", "from losses import MultiWindowShapeLoss\n\tclass TrainMain(TrainGAN):\n\t    def __init__(self, args,):\n\t        super(TrainMain, self).__init__(args=args,)\n\t        self.train_mode = self.config.get('train_mode', 'autoencoder')\n\t        self.model_type = self.config.get('model_type', 'symAudioDec')\n\t        self.data_path = self.config['data']['path']\n\t    # DATA LOADER\n\t    def initialize_data_loader(self):\n\t        logging.info(f\"Loading datasets... (batch_lenght: {self.batch_length})\")\n", "        if self.train_mode in ['autoencoder', 'vocoder']:\n\t            train_set = self._audio('train')\n\t            valid_set = self._audio('valid')\n\t            collater = CollaterAudio(batch_length=self.batch_length)\n\t        elif self.train_mode in ['denoise']:\n\t            train_set = self._audio_pair('noisy_train', 'clean_train')\n\t            valid_set = self._audio_pair('noisy_valid', 'clean_valid')\n\t            collater = CollaterAudioPair(batch_length=self.batch_length)\n\t        else:\n\t            raise NotImplementedError(f\"Train mode: {self.train_mode} is not supported!\")\n", "        logging.info(f\"The number of training files = {len(train_set)}.\")\n\t        logging.info(f\"The number of validation files = {len(valid_set)}.\")\n\t        dataset = {'train': train_set, 'dev': valid_set}\n\t        self._data_loader(dataset, collater)\n\t    def _data_loader(self, dataset, collater):\n\t        self.data_loader = {\n\t            'train': DataLoader(\n\t                dataset=dataset['train'],\n\t                shuffle=True,\n\t                collate_fn=collater,\n", "                batch_size=self.config['batch_size'],\n\t                num_workers=self.config['num_workers'],\n\t                pin_memory=self.config['pin_memory'],\n\t            ),\n\t            'dev': DataLoader(\n\t                dataset=dataset['dev'],\n\t                shuffle=False,\n\t                collate_fn=collater,\n\t                batch_size=self.config['batch_size'],\n\t                num_workers=self.config['num_workers'],\n", "                pin_memory=self.config['pin_memory'],\n\t            ),\n\t        }\n\t    def _audio(self, subset, subset_num=-1, return_utt_id=False):\n\t        audio_dir = os.path.join(\n\t            self.data_path, self.config['data']['subset'][subset])\n\t        params = {\n\t            'files': audio_dir,\n\t            'query': \"*.wav\",\n\t            'load_fn': sf.read,\n", "            'return_utt_id': return_utt_id,\n\t            'subset_num': subset_num,\n\t        }\n\t        return SingleDataset(**params)\n\t    def _audio_pair(self, subset_n, subset_c, subset_num=-1, return_utt_id=False):\n\t        audio_n_dir = os.path.join(\n\t            self.data_path, self.config['data']['subset'][subset_n])\n\t        audio_c_dir = os.path.join(\n\t            self.data_path, self.config['data']['subset'][subset_c])\n\t        params = {\n", "            'multi_files': [audio_c_dir, audio_n_dir], # (main, sub)\n\t            'queries': [\"*.wav\"]*2,\n\t            'load_fns': [sf.read]*2,\n\t            'return_utt_id': return_utt_id,\n\t            'subset_num': subset_num,\n\t        }\n\t        return MultiDataset(**params)\n\t    # MODEL ARCHITECTURE\n\t    def define_model(self):\n\t        # generator\n", "        generator = self._define_generator(self.model_type)\n\t        self.model['generator'] = generator.to(self.device)\n\t        # discriminator\n\t        discriminator = self._define_discriminator(self.model_type)\n\t        self.model['discriminator'] = discriminator.to(self.device)\n\t        # optimizer\n\t        self._define_optimizer_scheduler()\n\t        #self._show_setting()\n\t    def _define_generator(self, model_type):\n\t        if model_type in ['symAudioDec', 'symAudioDecUniv']:\n", "            generator = generator_audiodec\n\t        elif model_type in ['HiFiGAN', 'UnivNet']:\n\t            generator = generator_hifigan\n\t        else:\n\t            raise NotImplementedError(f\"Model type: {model_type} is not supported for the generator!\")\n\t        return generator(**self.config['generator_params'])\n\t    def _define_discriminator(self, model_type):\n\t        if model_type in ['symAudioDec', 'HiFiGAN']:\n\t            discriminator = discriminator_hifigan\n\t        elif model_type in ['symAudioDecUniv', 'UnivNet']:\n", "            discriminator = discriminator_univnet\n\t        else:\n\t            raise NotImplementedError(f\"Model type: {model_type} is not supported for the discriminator!\")\n\t        return discriminator(**self.config['discriminator_params'])\n\t    def _define_optimizer_scheduler(self):\n\t        generator_optimizer_class = getattr(\n\t            torch.optim, \n\t            self.config['generator_optimizer_type']\n\t        )\n\t        discriminator_optimizer_class = getattr(\n", "            torch.optim, \n\t            self.config['discriminator_optimizer_type']\n\t        )\n\t        self.optimizer = {\n\t            'generator': generator_optimizer_class(\n\t                self.model['generator'].parameters(),\n\t                **self.config['generator_optimizer_params'],\n\t            ),\n\t            'discriminator': discriminator_optimizer_class(\n\t                self.model['discriminator'].parameters(),\n", "                **self.config['discriminator_optimizer_params'],\n\t            ),\n\t        }\n\t        generator_scheduler_class = getattr(\n\t            torch.optim.lr_scheduler,\n\t            self.config.get('generator_scheduler_type', \"StepLR\"),\n\t        )\n\t        discriminator_scheduler_class = getattr(\n\t            torch.optim.lr_scheduler,\n\t            self.config.get('discriminator_scheduler_type', \"StepLR\"),\n", "        )\n\t        self.scheduler = {\n\t            'generator': generator_scheduler_class(\n\t                optimizer=self.optimizer['generator'],\n\t                **self.config['generator_scheduler_params'],\n\t            ),\n\t            'discriminator': discriminator_scheduler_class(\n\t                optimizer=self.optimizer['discriminator'],\n\t                **self.config['discriminator_scheduler_params'],\n\t            ),\n", "        }\n\t    # CRITERIA\n\t    def define_criterion(self):\n\t        self.criterion = {\n\t        'gen_adv': GeneratorAdversarialLoss(\n\t            **self.config['generator_adv_loss_params']).to(self.device),\n\t        'dis_adv': DiscriminatorAdversarialLoss(\n\t            **self.config['discriminator_adv_loss_params']).to(self.device),\n\t        }\n\t        if self.config.get('use_feat_match_loss', False):\n", "            self.criterion['feat_match'] = FeatureMatchLoss(\n\t                **self.config.get('feat_match_loss_params', {}),\n\t            ).to(self.device)\n\t        if self.config.get('use_mel_loss', False):\n\t            self.criterion['mel'] = MultiMelSpectrogramLoss(\n\t                **self.config['mel_loss_params'],\n\t            ).to(self.device)\n\t        if self.config.get('use_stft_loss', False):\n\t            self.criterion['stft'] = MultiResolutionSTFTLoss(\n\t                **self.config['stft_loss_params'],\n", "            ).to(self.device)\n\t        if self.config.get('use_shape_loss', False):\n\t            self.criterion['shape'] = MultiWindowShapeLoss(\n\t                **self.config['shape_loss_params'],\n\t            ).to(self.device)\n\t    # TRAINER\n\t    def define_trainer(self):\n\t        if self.train_mode in ['autoencoder']:\n\t            trainer = TrainerAutoEncoder\n\t        elif self.train_mode in ['vocoder']:\n", "            trainer = TrainerVocoder\n\t        elif self.train_mode in ['denoise']:\n\t            trainer = TrainerDenoise\n\t        else:\n\t            raise NotImplementedError(f\"Train mode: {self.train_mode} is not supported for Trainer!\")\n\t        trainer_parameters = {}\n\t        trainer_parameters['steps'] = 0\n\t        trainer_parameters['epochs'] = 0\n\t        trainer_parameters['data_loader'] = self.data_loader\n\t        trainer_parameters['model'] = self.model\n", "        trainer_parameters['criterion'] = self.criterion\n\t        trainer_parameters['optimizer'] = self.optimizer\n\t        trainer_parameters['scheduler'] = self.scheduler\n\t        trainer_parameters['config'] = self.config\n\t        trainer_parameters['device'] = self.device\n\t        self.trainer = trainer(**trainer_parameters)\n\t    # MODEL INITIALIZATION\n\t    def initialize_model(self):\n\t        initial = self.config.get(\"initial\", \"\") \n\t        if os.path.exists(self.resume): # resume from trained model\n", "            self.trainer.load_checkpoint(self.resume)\n\t            logging.info(f\"Successfully resumed from {self.resume}.\")\n\t        elif os.path.exists(initial): # initial new model with the pre-trained model\n\t            self.trainer.load_checkpoint(initial, load_only_params=True)\n\t            logging.info(f\"Successfully initialize parameters from {initial}.\")\n\t        else:\n\t            logging.info(\"Train from scrach\")\n\t        # load the pre-trained encoder for vocoder training\n\t        if self.train_mode in ['vocoder']:\n\t            analyzer_checkpoint = self.config.get(\"analyzer\", \"\")\n", "            assert os.path.exists(analyzer_checkpoint), f\"Analyzer {analyzer_checkpoint} does not exist!\"\n\t            analyzer_config = self._load_config(analyzer_checkpoint)\n\t            self._initialize_analyzer(analyzer_config, analyzer_checkpoint)\n\t    def _initialize_analyzer(self, config, checkpoint):\n\t        model_type = config.get('model_type', 'symAudioDec')\n\t        if model_type in ['symAudioDec', 'symAudioDecUniv']:\n\t            analyzer = generator_audiodec\n\t        else:\n\t            raise NotImplementedError(f\"Model type: {model_type} is not supported for the analyzer!\")\n\t        self.model['analyzer'] = analyzer(**config['generator_params']).to(self.device)\n", "        self.model['analyzer'].load_state_dict(\n\t            torch.load(checkpoint, map_location='cpu')['model']['generator'])\n\t        logging.info(f\"Successfully load analyzer from {checkpoint}.\")\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-c', '--config', type=str, required=True)\n\t    parser.add_argument(\"--tag\", type=str, required=True)\n\t    parser.add_argument(\"--exp_root\", type=str, default=\"exp\")\n\t    parser.add_argument(\"--resume\", default=\"\", type=str, nargs=\"?\",\n\t        help='checkpoint file path to resume training. (default=\"\")',\n", "    )\n\t    parser.add_argument('--seed', default=1337, type=int)\n\t    parser.add_argument('--disable_cudnn', choices=('True','False'), default='False', help='Disable CUDNN')\n\t    args = parser.parse_args()\n\t    # initial train_main\n\t    train_main = TrainMain(args=args)   \n\t    # get dataset\n\t    train_main.initialize_data_loader()\n\t    # define models, optimizers, and schedulers\n\t    train_main.define_model()\n", "    # define criteria\n\t    train_main.define_criterion()\n\t    # define trainer\n\t    train_main.define_trainer()\n\t    # model initialization\n\t    train_main.initialize_model()\n\t    # run training loop\n\t    train_main.run()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "codecTest.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\timport os\n", "import torch\n\timport logging\n\timport argparse\n\timport soundfile as sf\n\tfrom dataloader import SingleDataset\n\tfrom models.autoencoder.AudioDec import Generator as generator_audiodec\n\tfrom models.vocoder.HiFiGAN import Generator as generator_hifigan\n\tfrom bin.test import TestGEN\n\tclass TestMain(TestGEN):\n\t    def __init__(self, args,):\n", "        super(TestMain, self).__init__(args=args,)\n\t        self.encoder_type = self.encoder_config.get('model_type', 'symAudioDec')\n\t        self.decoder_type = self.decoder_config.get('model_type', 'symAudioDec')\n\t        if self.encoder_config['generator_params']['input_channels'] > 1:\n\t            self.multi_channel = True\n\t        else:\n\t            self.multi_channel = False\n\t    # LOAD DATASET\n\t    def load_dataset(self, subset, subset_num):\n\t        data_path = os.path.join(\n", "            self.encoder_config['data']['path'], \n\t            self.encoder_config['data']['subset'][subset]\n\t        )\n\t        assert os.path.exists(data_path), f\"{data_path} does not exist!\"\n\t        self.dataset = SingleDataset(\n\t            files=data_path,\n\t            query=\"*.wav\",\n\t            load_fn=sf.read,\n\t            return_utt_id=True,\n\t            subset_num=subset_num,\n", "        )\n\t        logging.info(f\"The number of utterances = {len(self.dataset)}.\")\n\t    # LOAD MODEL\n\t    def load_encoder(self):\n\t        if self.encoder_type in ['symAudioDec', 'symAudioDecUniv']:\n\t            encoder = generator_audiodec\n\t        else:     \n\t            raise NotImplementedError(f\"Encoder {self.encoder_type} is not supported!\")\n\t        self.encoder = encoder(**self.encoder_config['generator_params'])\n\t        self.encoder.load_state_dict(\n", "            torch.load(self.encoder_checkpoint, map_location='cpu')['model']['generator'])\n\t        self.encoder = self.encoder.eval().to(self.device)\n\t        logging.info(f\"Loaded Encoder from {self.encoder_checkpoint}.\")\n\t    def load_decoder(self):\n\t        if self.decoder_type in ['symAudioDec', 'symAudioDecUniv']:\n\t            decoder = generator_audiodec\n\t        elif self.decoder_type in ['HiFiGAN', 'UnivNet']:\n\t            decoder = generator_hifigan\n\t        else:     \n\t            raise NotImplementedError(f\"Decoder {self.decoder_type} is not supported!\")\n", "        self.decoder = decoder(**self.decoder_config['generator_params'])\n\t        self.decoder.load_state_dict(\n\t            torch.load(self.decoder_checkpoint, map_location='cpu')['model']['generator'])\n\t        self.decoder = self.decoder.eval().to(self.device)\n\t        logging.info(f\"Loaded Decoder from {self.decoder_checkpoint}.\")\n\t    def encode(self, audio):\n\t        x = torch.tensor(audio, dtype=torch.float).to(self.device)\n\t        if self.multi_channel:\n\t            x = x.transpose(1, 0).unsqueeze(0) # (T, C) -> (1, C, T)\n\t        else:\n", "            x = x.transpose(1, 0).unsqueeze(1) # (T, C) -> (C, 1, T)\n\t        x = self.encoder.encoder(x)\n\t        z = self.encoder.projector(x)\n\t        zq, _, _ = self.encoder.quantizer(z)\n\t        return zq\n\t    def decode(self, zq):\n\t        if self.decoder_type in ['HiFiGAN', 'UnivNet']:\n\t            y = self.decoder(zq)\n\t        else:\n\t            y = self.decoder.decoder(zq)\n", "        return y\n\t    # INITIAL FOLDER\n\t    def initial_folder(self, subset, output_name):\n\t        # model name\n\t        encoder = os.path.dirname(self.encoder_checkpoint).split('/')[-1]\n\t        decoder = os.path.dirname(self.decoder_checkpoint).split('/')[-1]\n\t        # model checkpoint\n\t        encoder_checkpoint = os.path.basename(self.encoder_checkpoint).split('steps')[0].split('-')[-1]\n\t        decoder_checkpoint = os.path.basename(self.decoder_checkpoint).split('steps')[0].split('-')[-1]\n\t        testdir = f\"{encoder}-{decoder}_{encoder_checkpoint}-{decoder_checkpoint}\"\n", "        # testing set\n\t        setdir = self.encoder_config['data']['subset'][subset]\n\t        self.outdir = os.path.join(output_name, testdir, setdir)\n\t        if not os.path.exists(self.outdir):\n\t            os.makedirs(self.outdir, exist_ok=True)    \n\tdef main():\n\t    \"\"\"Run testing process.\"\"\"\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--subset\", type=str, default=\"clean_test\")\n\t    parser.add_argument(\"--subset_num\", type=int, default=-1)\n", "    parser.add_argument(\"--encoder\", type=str, required=True)\n\t    parser.add_argument(\"--decoder\", type=str, required=True)\n\t    parser.add_argument(\"--output_dir\", type=str, required=True)\n\t    args = parser.parse_args()\n\t    # initial test_main\n\t    test_main = TestMain(args=args)\n\t    # load dataset\n\t    test_main.load_dataset(args.subset, args.subset_num)\n\t    # load model\n\t    test_main.load_encoder()\n", "    test_main.load_decoder()\n\t    # initial folder\n\t    test_main.initial_folder(args.subset, args.output_dir)\n\t    # run testing\n\t    test_main.run()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "losses/waveform_loss.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Waveform-based loss modules.\"\"\"\n\timport torch\n\tclass WaveformShapeLoss(torch.nn.Module):\n", "    \"\"\"Waveform shape loss.\"\"\"\n\t    def __init__(self, winlen):\n\t        super().__init__()\n\t        self.loss = torch.nn.L1Loss()\n\t        self.winlen = winlen\n\t        self.maxpool = torch.nn.MaxPool1d(self.winlen)\n\t    def forward(self, y_hat, y):\n\t        \"\"\"Calculate L1 loss.\n\t        Args:\n\t            y_hat (Tensor): Generated single tensor (B, 1, T).\n", "            y (Tensor): Groundtruth single tensor (B, 1, T).\n\t        Returns:\n\t            Tensor: L1 loss value.\n\t        \"\"\"\n\t        ys = self.maxpool(torch.abs(y))\n\t        ys_hat = self.maxpool(torch.abs(y_hat))\n\t        loss = self.loss(ys_hat, ys)\n\t        return loss\n\tclass MultiWindowShapeLoss(torch.nn.Module):\n\t    \"\"\"Multi-window-lengthe waveform shape loss.\"\"\"\n", "    def __init__(\n\t        self,\n\t        winlen=[300, 200, 100],\n\t    ):\n\t        \"\"\"Initialize Multi window shape loss module.\n\t        Args:\n\t            winlen (list): List of window lengths.\n\t        \"\"\"\n\t        super(MultiWindowShapeLoss, self).__init__()\n\t        self.shape_losses = torch.nn.ModuleList()\n", "        for wl in winlen:\n\t            self.shape_losses += [WaveformShapeLoss(wl)]\n\t    def forward(self, y_hat, y):\n\t        \"\"\"Calculate L1 loss.\n\t        Args:\n\t            y_hat (Tensor): Generated single tensor (B, 1, T).\n\t            y (Tensor): Groundtruth single tensor (B, 1, T).\n\t        Returns:\n\t            Tensor: L2 loss value.\n\t        \"\"\"\n", "        loss = 0.0\n\t        for f in self.shape_losses:\n\t            loss += f(y_hat, y)\n\t        loss /= len(self.shape_losses)\n\t        return loss"]}
{"filename": "losses/__init__.py", "chunked_list": ["from .adversarial_loss import *  # NOQA\n\tfrom .feat_match_loss import *  # NOQA\n\tfrom .mel_loss import *  # NOQA\n\tfrom .stft_loss import *  # NOQA\n\tfrom .waveform_loss import *  # NOQA"]}
{"filename": "losses/adversarial_loss.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright 2021 Tomoki Hayashi\n\t#  MIT License (https://opensource.org/licenses/MIT)\n\t\"\"\"Adversarial loss modules.\"\"\"\n\timport torch\n\timport torch.nn.functional as F\n\tclass GeneratorAdversarialLoss(torch.nn.Module):\n\t    \"\"\"Generator adversarial loss module.\"\"\"\n\t    def __init__(\n", "        self,\n\t        average_by_discriminators=True,\n\t        loss_type=\"mse\",\n\t    ):\n\t        \"\"\"Initialize GeneratorAversarialLoss module.\"\"\"\n\t        super().__init__()\n\t        self.average_by_discriminators = average_by_discriminators\n\t        assert loss_type in [\"mse\", \"hinge\"], f\"{loss_type} is not supported.\"\n\t        if loss_type == \"mse\":\n\t            self.criterion = self._mse_loss\n", "        else:\n\t            self.criterion = self._hinge_loss\n\t    def forward(self, outputs):\n\t        \"\"\"Calcualate generator adversarial loss.\n\t        Args:\n\t            outputs (Tensor or list): Discriminator outputs or list of\n\t                discriminator outputs.\n\t        Returns:\n\t            Tensor: Generator adversarial loss value.\n\t        \"\"\"\n", "        if isinstance(outputs, (tuple, list)):\n\t            adv_loss = 0.0\n\t            for i, outputs_ in enumerate(outputs):\n\t                if isinstance(outputs_, (tuple, list)):\n\t                    # NOTE(kan-bayashi): case including feature maps\n\t                    outputs_ = outputs_[-1]\n\t                adv_loss += self.criterion(outputs_)\n\t            if self.average_by_discriminators:\n\t                adv_loss /= i + 1\n\t        else:\n", "            adv_loss = self.criterion(outputs)\n\t        return adv_loss\n\t    def _mse_loss(self, x):\n\t        return F.mse_loss(x, x.new_ones(x.size()))\n\t    def _hinge_loss(self, x):\n\t        return -x.mean()\n\tclass DiscriminatorAdversarialLoss(torch.nn.Module):\n\t    \"\"\"Discriminator adversarial loss module.\"\"\"\n\t    def __init__(\n\t        self,\n", "        average_by_discriminators=True,\n\t        loss_type=\"mse\",\n\t    ):\n\t        \"\"\"Initialize DiscriminatorAversarialLoss module.\"\"\"\n\t        super().__init__()\n\t        self.average_by_discriminators = average_by_discriminators\n\t        assert loss_type in [\"mse\", \"hinge\"], f\"{loss_type} is not supported.\"\n\t        if loss_type == \"mse\":\n\t            self.fake_criterion = self._mse_fake_loss\n\t            self.real_criterion = self._mse_real_loss\n", "        else:\n\t            self.fake_criterion = self._hinge_fake_loss\n\t            self.real_criterion = self._hinge_real_loss\n\t    def forward(self, outputs_hat, outputs):\n\t        \"\"\"Calcualate discriminator adversarial loss.\n\t        Args:\n\t            outputs_hat (Tensor or list): Discriminator outputs or list of\n\t                discriminator outputs calculated from generator outputs.\n\t            outputs (Tensor or list): Discriminator outputs or list of\n\t                discriminator outputs calculated from groundtruth.\n", "        Returns:\n\t            Tensor: Discriminator real loss value.\n\t            Tensor: Discriminator fake loss value.\n\t        \"\"\"\n\t        if isinstance(outputs, (tuple, list)):\n\t            real_loss = 0.0\n\t            fake_loss = 0.0\n\t            for i, (outputs_hat_, outputs_) in enumerate(zip(outputs_hat, outputs)):\n\t                if isinstance(outputs_hat_, (tuple, list)):\n\t                    # NOTE(kan-bayashi): case including feature maps\n", "                    outputs_hat_ = outputs_hat_[-1]\n\t                    outputs_ = outputs_[-1]\n\t                real_loss += self.real_criterion(outputs_)\n\t                fake_loss += self.fake_criterion(outputs_hat_)\n\t            if self.average_by_discriminators:\n\t                fake_loss /= i + 1\n\t                real_loss /= i + 1\n\t        else:\n\t            real_loss = self.real_criterion(outputs)\n\t            fake_loss = self.fake_criterion(outputs_hat)\n", "        return real_loss, fake_loss\n\t    def _mse_real_loss(self, x):\n\t        return F.mse_loss(x, x.new_ones(x.size()))\n\t    def _mse_fake_loss(self, x):\n\t        return F.mse_loss(x, x.new_zeros(x.size()))\n\t    def _hinge_real_loss(self, x):\n\t        return -torch.mean(torch.min(x - 1, x.new_zeros(x.size())))\n\t    def _hinge_fake_loss(self, x):\n\t        return -torch.mean(torch.min(-x - 1, x.new_zeros(x.size())))\n"]}
{"filename": "losses/stft_loss.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"STFT-based loss modules.\"\"\"\n", "import torch\n\timport torch.nn.functional as F\n\tdef stft(x, fft_size, hop_size, win_length, window, eps=1e-7):\n\t    \"\"\"Perform STFT and convert to magnitude spectrogram.\n\t    Args:\n\t        x (Tensor): Input signal tensor (B, T).\n\t        fft_size (int): FFT size.\n\t        hop_size (int): Hop size.\n\t        win_length (int): Window length.\n\t        window (str): Window function type.\n", "    Returns:\n\t        Tensor: Magnitude spectrogram (B, #frames, fft_size // 2 + 1).\n\t    \"\"\"\n\t    x_stft = torch.stft(x, fft_size, hop_size, win_length, window, return_complex=True)\n\t    x_power = x_stft.real ** 2 + x_stft.imag ** 2\n\t    return torch.sqrt(torch.clamp(x_power, min=eps)).transpose(2, 1)\n\tclass SpectralConvergenceLoss(torch.nn.Module):\n\t    \"\"\"Spectral convergence loss module.\"\"\"\n\t    def __init__(self):\n\t        \"\"\"Initilize spectral convergence loss module.\"\"\"\n", "        super(SpectralConvergenceLoss, self).__init__()\n\t    def forward(self, x_mag, y_mag):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n\t            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\t        Returns:\n\t            Tensor: Spectral convergence loss value.\n\t        \"\"\"\n\t        return torch.norm(y_mag - x_mag, p=\"fro\") / torch.norm(y_mag, p=\"fro\")\n", "class LogSTFTMagnitudeLoss(torch.nn.Module):\n\t    \"\"\"Log STFT magnitude loss module.\"\"\"\n\t    def __init__(self):\n\t        \"\"\"Initilize los STFT magnitude loss module.\"\"\"\n\t        super(LogSTFTMagnitudeLoss, self).__init__()\n\t    def forward(self, x_mag, y_mag):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n\t            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n", "        Returns:\n\t            Tensor: Log STFT magnitude loss value.\n\t        \"\"\"\n\t        return F.l1_loss(torch.log(y_mag), torch.log(x_mag))\n\tclass STFTLoss(torch.nn.Module):\n\t    \"\"\"STFT loss module.\"\"\"\n\t    def __init__(\n\t        self, \n\t        fft_size=1024, \n\t        hop_size=120, \n", "        win_length=600, \n\t        window=\"hann_window\",\n\t    ):\n\t        \"\"\"Initialize STFT loss module.\"\"\"\n\t        super(STFTLoss, self).__init__()\n\t        self.fft_size = fft_size\n\t        self.hop_size = hop_size\n\t        self.win_length = win_length\n\t        self.spectral_convergence_loss = SpectralConvergenceLoss()\n\t        self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss()\n", "        self.register_buffer(\"window\", getattr(torch, window)(win_length))\n\t    def forward(self, x, y):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Predicted signal (B, T).\n\t            y (Tensor): Groundtruth signal (B, T).\n\t        Returns:\n\t            Tensor: Spectral convergence loss value.\n\t            Tensor: Log STFT magnitude loss value.\n\t        \"\"\"\n", "        x_mag = stft(x, self.fft_size, self.hop_size, self.win_length, self.window)\n\t        y_mag = stft(y, self.fft_size, self.hop_size, self.win_length, self.window)\n\t        sc_loss = self.spectral_convergence_loss(x_mag, y_mag)\n\t        mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)\n\t        return sc_loss, mag_loss\n\tclass MultiResolutionSTFTLoss(torch.nn.Module):\n\t    \"\"\"Multi resolution STFT loss module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        fft_sizes=[1024, 2048, 512],\n", "        hop_sizes=[120, 240, 50],\n\t        win_lengths=[600, 1200, 240],\n\t        window=\"hann_window\",\n\t    ):\n\t        \"\"\"Initialize Multi resolution STFT loss module.\n\t        Args:\n\t            fft_sizes (list): List of FFT sizes.\n\t            hop_sizes (list): List of hop sizes.\n\t            win_lengths (list): List of window lengths.\n\t            window (str): Window function type.\n", "        \"\"\"\n\t        super(MultiResolutionSTFTLoss, self).__init__()\n\t        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n\t        self.stft_losses = torch.nn.ModuleList()\n\t        for fft_size, hop_size, win_length in zip(fft_sizes, hop_sizes, win_lengths):\n\t            self.stft_losses += [STFTLoss(fft_size, hop_size, win_length, window)]\n\t    def forward(self, x, y):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Predicted signal (B, T) or (B, #subband, T).\n", "            y (Tensor): Groundtruth signal (B, T) or (B, #subband, T).\n\t        Returns:\n\t            Tensor: Multi resolution spectral convergence loss value.\n\t            Tensor: Multi resolution log STFT magnitude loss value.\n\t        \"\"\"\n\t        if len(x.shape) == 3:\n\t            x = x.view(-1, x.size(2))  # (B, C, T) -> (B x C, T)\n\t            y = y.view(-1, y.size(2))  # (B, C, T) -> (B x C, T)\n\t        sc_loss = 0.0\n\t        mag_loss = 0.0\n", "        for f in self.stft_losses:\n\t            sc_l, mag_l = f(x, y)\n\t            sc_loss += sc_l\n\t            mag_loss += mag_l\n\t        sc_loss /= len(self.stft_losses)\n\t        mag_loss /= len(self.stft_losses)\n\t        return sc_loss, mag_loss\n"]}
{"filename": "losses/mel_loss.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Mel-spectrogram loss modules.\"\"\"\n", "import librosa\n\timport torch\n\timport torch.nn.functional as F\n\tclass MelSpectrogram(torch.nn.Module):\n\t    \"\"\"Calculate Mel-spectrogram.\"\"\"\n\t    def __init__(\n\t        self,\n\t        fs=22050,\n\t        fft_size=1024,\n\t        hop_size=256,\n", "        win_length=None,\n\t        window=\"hann_window\",\n\t        num_mels=80,\n\t        fmin=80,\n\t        fmax=7600,\n\t        center=True,\n\t        normalized=False,\n\t        onesided=True,\n\t        eps=1e-10,\n\t        log_base=10.0,\n", "    ):\n\t        \"\"\"Initialize MelSpectrogram module.\"\"\"\n\t        super().__init__()\n\t        self.fft_size = fft_size\n\t        self.hop_size = hop_size\n\t        if win_length is not None:\n\t            self.win_length = win_length\n\t        else:\n\t            self.win_length = fft_size\n\t        self.center = center\n", "        self.normalized = normalized\n\t        self.onesided = onesided\n\t        self.register_buffer(\"window\", getattr(torch, window)(self.win_length))\n\t        self.eps = eps\n\t        fmin = 0 if fmin is None else fmin\n\t        fmax = fs / 2 if fmax is None else fmax\n\t        melmat = librosa.filters.mel(\n\t            sr=fs,\n\t            n_fft=fft_size,\n\t            n_mels=num_mels,\n", "            fmin=fmin,\n\t            fmax=fmax,\n\t        )\n\t        self.register_buffer(\"melmat\", torch.from_numpy(melmat.T).float())\n\t        self.log_base = log_base\n\t        if self.log_base is None:\n\t            self.log = torch.log\n\t        elif self.log_base == 2.0:\n\t            self.log = torch.log2\n\t        elif self.log_base == 10.0:\n", "            self.log = torch.log10\n\t        else:\n\t            raise ValueError(f\"log_base: {log_base} is not supported.\")\n\t    def forward(self, x):\n\t        \"\"\"Calculate Mel-spectrogram.\n\t        Args:\n\t            x (Tensor): Input waveform tensor (B, T) or (B, C, T).\n\t        Returns:\n\t            Tensor: Mel-spectrogram (B, #mels, #frames).\n\t        \"\"\"\n", "        if x.dim() == 3:\n\t            # (B, C, T) -> (B*C, T)\n\t            x = x.reshape(-1, x.size(2))\n\t        x_stft = torch.stft(x, self.fft_size, self.hop_size, self.win_length, self.window, return_complex=True)\n\t        x_power = x_stft.real ** 2 + x_stft.imag ** 2\n\t        x_amp = torch.sqrt(torch.clamp(x_power, min=self.eps)).transpose(2, 1) # (B, D, T') -> (B, T', D)\n\t        x_mel = torch.matmul(x_amp, self.melmat)\n\t        x_mel = torch.clamp(x_mel, min=self.eps)\n\t        return self.log(x_mel).transpose(1, 2) # (B, D, T')\n\tclass MultiMelSpectrogramLoss(torch.nn.Module):\n", "    \"\"\"Multi resolution Mel-spectrogram loss.\"\"\"\n\t    def __init__(\n\t        self,\n\t        fs=22050,\n\t        fft_sizes=[1024, 2048, 512],\n\t        hop_sizes=[120, 240, 50],\n\t        win_lengths=[600, 1200, 240],\n\t        window=\"hann_window\",\n\t        num_mels=80,\n\t        fmin=80,\n", "        fmax=7600,\n\t        center=True,\n\t        normalized=False,\n\t        onesided=True,\n\t        eps=1e-10,\n\t        log_base=10.0,\n\t    ):\n\t        \"\"\"Initialize Mel-spectrogram loss.\"\"\"\n\t        super().__init__()\n\t        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n", "        self.mel_transfers = torch.nn.ModuleList()\n\t        for fft_size, hop_size, win_length in zip(fft_sizes, hop_sizes, win_lengths):\n\t            self.mel_transfers += [\n\t                MelSpectrogram(\n\t                    fs=fs,\n\t                    fft_size=fft_size,\n\t                    hop_size=hop_size,\n\t                    win_length=win_length,\n\t                    window=window,\n\t                    num_mels=num_mels,\n", "                    fmin=fmin,\n\t                    fmax=fmax,\n\t                    center=center,\n\t                    normalized=normalized,\n\t                    onesided=onesided,\n\t                    eps=eps,\n\t                    log_base=log_base,\n\t                )\n\t            ]\n\t    def forward(self, y_hat, y):\n", "        \"\"\"Calculate Mel-spectrogram loss.\n\t        Args:\n\t            y_hat (Tensor): Generated single tensor (B, C, T).\n\t            y (Tensor): Groundtruth single tensor (B, C, T).\n\t        Returns:\n\t            Tensor: Mel-spectrogram loss value.\n\t        \"\"\"\n\t        mel_loss = 0.0\n\t        for f in self.mel_transfers:\n\t            mel_loss += F.l1_loss(f(y_hat), f(y))\n", "        mel_loss /= len(self.mel_transfers)\n\t        return mel_loss"]}
{"filename": "losses/feat_match_loss.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright 2021 Tomoki Hayashi\n\t#  MIT License (https://opensource.org/licenses/MIT)\n\t\"\"\"Feature matching loss modules.\"\"\"\n\timport torch\n\timport torch.nn.functional as F\n\tclass FeatureMatchLoss(torch.nn.Module):\n\t    \"\"\"Feature matching loss module.\"\"\"\n\t    def __init__(\n", "        self,\n\t        average_by_layers=True,\n\t        average_by_discriminators=True,\n\t        include_final_outputs=False,\n\t    ):\n\t        \"\"\"Initialize FeatureMatchLoss module.\"\"\"\n\t        super().__init__()\n\t        self.average_by_layers = average_by_layers\n\t        self.average_by_discriminators = average_by_discriminators\n\t        self.include_final_outputs = include_final_outputs\n", "    def forward(self, feats_hat, feats):\n\t        \"\"\"Calcualate feature matching loss.\n\t        Args:\n\t            feats_hat (list): List of list of discriminator outputs\n\t                calcuated from generater outputs.\n\t            feats (list): List of list of discriminator outputs\n\t                calcuated from groundtruth.\n\t        Returns:\n\t            Tensor: Feature matching loss value.\n\t        \"\"\"\n", "        feat_match_loss = 0.0\n\t        for i, (feats_hat_, feats_) in enumerate(zip(feats_hat, feats)):\n\t            feat_match_loss_ = 0.0\n\t            if not self.include_final_outputs:\n\t                feats_hat_ = feats_hat_[:-1]\n\t                feats_ = feats_[:-1]\n\t            for j, (feat_hat_, feat_) in enumerate(zip(feats_hat_, feats_)):\n\t                feat_match_loss_ += F.l1_loss(feat_hat_, feat_.detach())\n\t            if self.average_by_layers:\n\t                feat_match_loss_ /= j + 1\n", "            feat_match_loss += feat_match_loss_\n\t        if self.average_by_discriminators:\n\t            feat_match_loss /= i + 1\n\t        return feat_match_loss\n"]}
{"filename": "bin/train.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Training stage template.\"\"\"\n", "import os\n\timport abc\n\timport sys\n\timport yaml\n\timport random\n\timport logging\n\timport torch\n\timport numpy as np\n\tfrom bin.utils import load_config\n\tclass TrainGAN(abc.ABC):\n", "    def __init__(\n\t        self,\n\t        args,\n\t    ):\n\t        # set logger\n\t        logging.basicConfig(\n\t            level=logging.INFO,\n\t            stream=sys.stdout,\n\t            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n\t        )\n", "        # Fix seed and make backends deterministic\n\t        random.seed(args.seed)\n\t        np.random.seed(args.seed)\n\t        torch.manual_seed(args.seed)\n\t        if not torch.cuda.is_available():\n\t            self.device = torch.device('cpu')\n\t            logging.info(f\"device: cpu\")\n\t        else:\n\t            self.device = torch.device('cuda')\n\t            logging.info(f\"device: gpu\")\n", "            torch.cuda.manual_seed_all(args.seed)\n\t            if args.disable_cudnn == \"False\":\n\t                torch.backends.cudnn.benchmark = True\n\t        # initialize config\n\t        with open(args.config, 'r') as f:\n\t            self.config = yaml.load(f, Loader=yaml.FullLoader)\n\t        self.config.update(vars(args))\n\t        # initialize model folder\n\t        expdir = os.path.join(args.exp_root, args.tag)\n\t        os.makedirs(expdir, exist_ok=True)\n", "        self.config[\"outdir\"] = expdir\n\t        # save config\n\t        with open(os.path.join(expdir, \"config.yml\"), \"w\") as f:\n\t            yaml.dump(self.config, f, Dumper=yaml.Dumper)\n\t        for key, value in self.config.items():\n\t            logging.info(f\"[TrainGAN] {key} = {value}\")\n\t        # initialize attribute\n\t        self.resume = args.resume\n\t        self.data_loader = None\n\t        self.model = {}\n", "        self.criterion = None\n\t        self.optimizer = None\n\t        self.scheduler = None\n\t        self.trainer = None\n\t        # initialize batch_length\n\t        self.batch_length = self.config['batch_length']\n\t    @abc.abstractmethod    \n\t    def initialize_data_loader(self):\n\t        pass\n\t    @abc.abstractmethod\n", "    def define_model(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def define_trainer(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def initialize_model(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def define_criterion(self):\n", "        pass\n\t    def run(self):\n\t        try:\n\t            logging.info(f\"The current training step: {self.trainer.steps}\")\n\t            self.trainer.train_max_steps = self.config[\"train_max_steps\"]\n\t            if not self.trainer._check_train_finish():\n\t                self.trainer.run()\n\t            if self.config.get(\"adv_train_max_steps\", False) and self.config.get(\"adv_batch_length\", False):\n\t                self.batch_length = self.config['adv_batch_length']\n\t                logging.info(f\"Reload dataloader for adversarial training.\")                \n", "                self.initialize_data_loader()\n\t                self.trainer.data_loader = self.data_loader\n\t                self.trainer.train_max_steps = self.config[\"adv_train_max_steps\"]\n\t                self.trainer.run()\n\t        finally:\n\t            self.trainer.save_checkpoint(\n\t                os.path.join(self.config[\"outdir\"], f\"checkpoint-{self.trainer.steps}steps.pkl\")\n\t            )\n\t            logging.info(f\"Successfully saved checkpoint @ {self.trainer.steps}steps.\")\n\t    def _show_setting(self):\n", "        logging.info(self.model['generator'])\n\t        logging.info(self.model['discriminator'])\n\t        logging.info(self.optimizer['generator'])\n\t        logging.info(self.optimizer['discriminator'])\n\t        logging.info(self.scheduler['generator'])\n\t        logging.info(self.scheduler['discriminator'])\n\t        for criterion_ in self.criterion.values():\n\t            logging.info(criterion_)\n\t    def _load_config(self, checkpoint, config_name='config.yml'):\n\t        return load_config(checkpoint, config_name)\n"]}
{"filename": "bin/utils.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Utility modules.\"\"\"\n\timport os\n\timport yaml\n", "def load_config(checkpoint, config_name='config.yml'):\n\t    dirname = os.path.dirname(checkpoint)\n\t    config_path = os.path.join(dirname, config_name)\n\t    with open(config_path) as f:\n\t        config = yaml.load(f, Loader=yaml.Loader)\n\t    return config\n"]}
{"filename": "bin/stream.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport os\n\timport abc\n\timport yaml\n", "import time\n\timport queue\n\timport threading\n\timport torch\n\timport torchaudio\n\timport numpy as np\n\tfrom typing import Union\n\tclass AudioCodec(abc.ABC):\n\t    def __init__(\n\t        self,\n", "        tx_device: str = \"cpu\",\n\t        rx_device: str = \"cpu\",\n\t        receptive_length: int = 8192,\n\t    ):\n\t        self.tx_device = tx_device\n\t        self.rx_device = rx_device\n\t        self.receptive_length = receptive_length\n\t        self.tx_encoder = None\n\t        self.rx_encoder = None\n\t        self.decoder = None\n", "    @abc.abstractmethod\n\t    def _load_encoder(self, checkpoint):\n\t        pass\n\t    @abc.abstractmethod\n\t    def _load_decoder(self, checkpoint):\n\t        pass\n\t    def _load_config(self, checkpoint, config_name='config.yml'):\n\t        dirname = os.path.dirname(checkpoint)\n\t        config_path = os.path.join(dirname, config_name)\n\t        with open(config_path) as f:\n", "            config = yaml.load(f, Loader=yaml.Loader)\n\t        return config\n\t    def load_transmitter(self, encoder_checkpoint):\n\t        # load transmitter model(s)\n\t        assert os.path.exists(encoder_checkpoint), f'{encoder_checkpoint} does not exist!'\n\t        self.tx_encoder = self._load_encoder(encoder_checkpoint)\n\t        self.tx_encoder.eval().to(self.tx_device)\n\t        self.tx_encoder.initial_encoder(self.receptive_length, self.tx_device)\n\t        print(\"Load tx_encoder: %s\" % (encoder_checkpoint))\n\t    def load_receiver(self, encoder_checkpoint, decoder_checkpoint):\n", "        # load receiver model(s)\n\t        assert os.path.exists(encoder_checkpoint), f'{encoder_checkpoint} does not exist!'\n\t        self.rx_encoder = self._load_encoder(encoder_checkpoint)\n\t        self.rx_encoder.eval().to(self.rx_device)\n\t        zq = self.rx_encoder.initial_encoder(self.receptive_length, self.rx_device)\n\t        print(\"Load rx_encoder: %s\" % (encoder_checkpoint))\n\t        assert os.path.exists(decoder_checkpoint), f'{decoder_checkpoint} does not exist!'\n\t        self.decoder = self._load_decoder(decoder_checkpoint)\n\t        self.decoder.eval().to(self.rx_device)\n\t        self.decoder.initial_decoder(zq)\n", "        print(\"Load decoder: %s\" % (decoder_checkpoint))\n\tclass AudioCodecStreamer(abc.ABC):\n\t    \"\"\"\n\t    Streams audio from an input microphone to headpones/speakers.\n\t    For each model that can be optionally provided (encoder, decoder), the input audio is processed by the forward call of these models.\n\t    Main functions (see function definition for detailed documentation):\n\t    * __init__\n\t    * enable_filedump\n\t    * set_tx_rx_poses\n\t    * run\n", "    Example usage:\n\t        streamer = AudioCodecStreamer(\n\t            input_device=1,\n\t            output_device=4,\n\t            frame_size=512,\n\t            encoder=my_encoder_network,\n\t            tx_device=\"cuda:0\",\n\t            decoder=my_decoder_network,\n\t            rx_device=\"cuda:1\",\n\t        )\n", "        streamer.enable_filedump(input_stream_file=\"input.wav\", output_stream_file=\"output.wav\")\n\t        streamer.run()\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        input_device: Union[str, int],\n\t        output_device: Union[str, int],\n\t        input_channels: int = 1,\n\t        output_channels: int = 1,\n\t        frame_size: int = 512,\n", "        sample_rate: int = 48000,\n\t        gain: int = 1.0,\n\t        max_latency: float = 0.1,\n\t        # Transmitter params\n\t        tx_encoder = None,\n\t        tx_device: str = \"cpu\",\n\t        # Receiver params\n\t        rx_encoder = None,\n\t        decoder = None,\n\t        rx_device: str = \"cpu\",\n", "    ):\n\t        \"\"\"\n\t        Sounddevice parameters\n\t        :param input_device:    int or str, name or index of the input device.\n\t                                To get a list of all input devices call python3 -m sounddevice.\n\t        :param output_device:   int or str, name or index of the output device.\n\t                                To get a list of all output devices call python3 -m sounddevice.\n\t        :param input_channels:  number of input channels, usually 1 but might be multiple microphones as well\n\t        :param output_channels: number of output channels, usually 2 for binaural audio\n\t        :param frame_size:      number of audio samples in a frame\n", "        :param sample_rate:     sample rate of the audio signal\n\t        :param gain:            linear factor to scale the input audio by\n\t        :param max_latency:     maximal accepted latency in seconds before frames get dropped\n\t        #######\n\t        Transmitter parameters\n\t        :param tx_encoder:      encoder network in the transimtter side\n\t                                Is an instance of torch.nn.Module and must be fully initialized and loaded.\n\t                                Must have a forward function that expects a batch x input_channels x frame_size tensor as input.\n\t                                Default: None (input tensor is forwarded to decoder without change)\n\t        :param tx_device:       device on transmitter (cpu, cuda:0, cuda:1, ...)\n", "        #######\n\t        Receiver parameters\n\t        :param rx_encoder:      encoder network in the receiver side\n\t                                Is an instance of torch.nn.Module and must be fully initialized and loaded.\n\t                                Must have a forward function that expects a batch x input_channels x frame_size tensor as input.\n\t                                Default: None (input tensor is forwarded to decoder without change)\n\t        :param decoder:         decoder network\n\t                                Is an instance of torch.nn.Module and must be fully initialized and loaded.\n\t                                Must have a forward function that expects a tensor of the shape produced by the encoder.\n\t                                Default: None (input tensor is forwarded to binauralizer without change)\n", "        :param rx_device:       device on receiver (cpu, cuda:0, cuda:1, ...)\n\t        \"\"\"\n\t        self.input_device = input_device\n\t        self.output_device = output_device\n\t        self.input_channels = input_channels\n\t        self.output_channels = output_channels\n\t        self.frame_size = frame_size\n\t        self.sample_rate = sample_rate\n\t        self.gain = gain\n\t        self.max_latency = max_latency\n", "        # encoder\n\t        self.tx_encoder = tx_encoder\n\t        self.tx_device = tx_device\n\t        print(f'Encoder device: {tx_device}')\n\t        # decoder\n\t        self.rx_encoder = rx_encoder\n\t        self.decoder = decoder\n\t        self.rx_device = rx_device\n\t        print(f'Decoder device: {rx_device}')\n\t        # queues for encoder, decoder, and output\n", "        self.encoder_queue = queue.Queue()\n\t        self.decoder_queue = queue.Queue()\n\t        self.output_queue = queue.Queue()\n\t        # file dump if requested\n\t        self.input_dump = []\n\t        self.output_dump = []\n\t        self.input_dump_filename = None\n\t        self.output_dump_filename = None\n\t        # streaming statistics\n\t        self.frame_drops = 0\n", "        self.n_frames = 0\n\t        self.encoder_times = []\n\t        self.decoder_times = []\n\t        self.latency_queue = queue.Queue()\n\t        self.latencies = []\n\t    @abc.abstractmethod\n\t    def _encode(self, x):\n\t        pass\n\t    @abc.abstractmethod\n\t    def _decode(self, x):\n", "        pass\n\t    def _run_encoder(self):\n\t        while threading.main_thread().is_alive():\n\t            try:\n\t                x = self.encoder_queue.get(timeout=1)\n\t            except:\n\t                continue\n\t            start = time.time()\n\t            x = x.to(self.tx_device)\n\t            with torch.no_grad():\n", "                if self.tx_encoder is not None:\n\t                    x = self._encode(x)\n\t            self.encoder_times.append(time.time() - start)\n\t            self.decoder_queue.put(x)\n\t    def _run_decoder(self):\n\t        while threading.main_thread().is_alive():\n\t            try:\n\t                x = self.decoder_queue.get(timeout=1)\n\t            except:\n\t                continue\n", "            start = time.time()\n\t            x = x.to(self.rx_device)\n\t            with torch.no_grad():\n\t                if (self.rx_encoder is not None) and (self.decoder is not None):\n\t                    x = self._decode(x)\n\t            self.decoder_times.append(time.time() - start)\n\t            self.output_queue.put(x)\n\t    def _process(self, data):\n\t        data = data * self.gain\n\t        input_data = torch.from_numpy(data).transpose(1, 0).contiguous()  # channels x frame_size\n", "        if self.input_dump_filename is not None:\n\t            self.input_dump.append(input_data)\n\t        # add batch dimension\n\t        input_data = input_data.unsqueeze(0)\n\t        # process data\n\t        self.encoder_queue.put(input_data)\n\t        self.latency_queue.put(time.time())\n\t        try:\n\t            output_data = self.output_queue.get_nowait()\n\t            latency = time.time() - self.latency_queue.get_nowait()\n", "            self.latencies.append(latency)\n\t            # clear queues if latency get too high; this will lead to frame drops\n\t            if latency > self.max_latency:\n\t                self.encoder_queue.queue.clear()\n\t                self.decoder_queue.queue.clear()\n\t                self.output_queue.queue.clear()\n\t                while not self.latency_queue.empty():\n\t                    self.frame_drops += 1\n\t                    self.latency_queue.get_nowait()\n\t        except queue.Empty:\n", "            output_data = torch.zeros(1, self.output_channels, self.frame_size)\n\t        output_data = output_data.squeeze(0).detach().cpu()\n\t        self.n_frames += 1\n\t        if self.output_dump_filename is not None:\n\t            self.output_dump.append(output_data)\n\t        data = output_data.transpose(1, 0).contiguous().numpy()\n\t        return data\n\t    def _callback(self, indata, outdata, frames, _time, status):\n\t        if status:\n\t            print(status)\n", "        outdata[:] = self._process(indata)\n\t    def _exit(self):\n\t        # dump data to file if required\n\t        if self.input_dump_filename is not None:\n\t            audio = torch.clamp(torch.cat(self.input_dump, dim=-1), min=-1, max=1)\n\t            torchaudio.save(self.input_dump_filename, audio, self.sample_rate)\n\t        if self.output_dump_filename is not None:\n\t            audio = torch.clamp(torch.cat(self.output_dump, dim=-1), min=-1, max=1)\n\t            torchaudio.save(self.output_dump_filename, audio, self.sample_rate)\n\t        # compute statistics\n", "        with threading.Lock():\n\t            encoder_mean = np.mean(np.array(self.encoder_times) * 1000.0)\n\t            encoder_std = np.std(np.array(self.encoder_times) * 1000.0)\n\t            decoder_mean = np.mean(np.array(self.decoder_times) * 1000.0)\n\t            decoder_std = np.std(np.array(self.decoder_times) * 1000.0)\n\t            latency_mean = np.mean(np.array(self.latencies) * 1000.0)\n\t            latency_std = np.std(np.array(self.latencies) * 1000.0)\n\t        frame_drops_ratio = self.frame_drops / self.n_frames\n\t        # print statistics\n\t        print('#' * 80)\n", "        print(f\"encoder processing time (ms):      {encoder_mean:.2f} +- {encoder_std:.2f}\")\n\t        print(f\"decoder processing time (ms):      {decoder_mean:.2f} +- {decoder_std:.2f}\")\n\t        print(f\"system latency (ms):               {latency_mean:.2f} +- {latency_std:.2f}\")\n\t        print(f\"frame drops:                       {self.frame_drops} ({frame_drops_ratio * 100:.2f}%)\")\n\t        print('#' * 80)\n\t    def enable_filedump(self, input_stream_file: str = None, output_stream_file: str = None):\n\t        \"\"\"\n\t        dumps input/output audio to file if input/output filenames are specified\n\t        call this function before run()\n\t        :param input_stream_file:   name of the file to dump input audio to\n", "        :param output_stream_file:  name of the file to dump output audio to\n\t        at least one of the files needs to be specified\n\t        \"\"\"\n\t        if input_stream_file is None and output_stream_file is None:\n\t            raise Exception(\"At least one of input_stream_file and output_stream_file must be specified.\")\n\t        if input_stream_file is not None:\n\t            if not input_stream_file[-4:] == \".wav\":\n\t                input_stream_file += \".wav\"\n\t            self.input_dump_filename = input_stream_file\n\t        if output_stream_file is not None:\n", "            if not output_stream_file[-4:] == \".wav\":\n\t                output_stream_file += \".wav\"\n\t            self.output_dump_filename = output_stream_file\n\t    def run(self, latency):\n\t        \"\"\"\n\t        start streaming from the input device and forward the processed audio to the output device\n\t        prints statistics about mean processing time, standard deviation of each processing pass, and percentage of buffer underflows\n\t        \"\"\"\n\t        # start encoder and decoder threads\n\t        encoder_thread = threading.Thread(target=self._run_encoder, daemon=True)\n", "        encoder_thread.start()\n\t        decoder_thread = threading.Thread(target=self._run_decoder, daemon=True)\n\t        decoder_thread.start()\n\t        try:\n\t            # import device\n\t            import sounddevice as sd\n\t            with sd.Stream(\n\t                device=(self.input_device, self.output_device),\n\t                samplerate=self.sample_rate,\n\t                blocksize=self.frame_size,\n", "                dtype=np.float32,\n\t                latency=latency,\n\t                channels=(self.input_channels, self.output_channels),\n\t                callback=self._callback\n\t            ):\n\t                print('### starting stream [press Return to quit] ###')\n\t                input()\n\t                self._exit()\n\t        except KeyboardInterrupt:\n\t            self._exit()\n", "        except Exception as e:\n\t            print(type(e).__name__ + ': ' + str(e))\n"]}
{"filename": "bin/test.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Testing stage template.\"\"\"\n", "import os\n\timport abc\n\timport sys\n\timport time\n\timport yaml\n\timport torch\n\timport logging\n\timport soundfile as sf\n\tfrom tqdm import tqdm\n\tfrom bin.utils import load_config\n", "class TestGEN(abc.ABC):\n\t    def __init__(\n\t        self,\n\t        args,\n\t    ):\n\t        # set logger\n\t        logging.basicConfig(\n\t            level=logging.INFO,\n\t            stream=sys.stdout,\n\t            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n", "        )\n\t        # device\n\t        if not torch.cuda.is_available():\n\t            self.device = torch.device('cpu')\n\t            logging.info(f\"device: cpu\")\n\t        else:\n\t            self.device = torch.device('cuda')\n\t            logging.info(f\"device: gpu\")\n\t        # initialize attribute\n\t        if hasattr(args, 'encoder'):\n", "            self.encoder_checkpoint = args.encoder\n\t            self.encoder_config = self._load_config(args.encoder)\n\t        if hasattr(args, 'decoder'):\n\t            self.decoder_checkpoint = args.decoder\n\t            self.decoder_config = self._load_config(args.decoder)\n\t        self.encoder = None\n\t        self.decoder = None\n\t        self.dataset = None\n\t        self.outdir = None\n\t    @abc.abstractmethod\n", "    def initial_folder(self, output_name):\n\t        pass\n\t    @abc.abstractmethod    \n\t    def load_dataset(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def load_encoder(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def load_decoder(self):\n", "        pass\n\t    @abc.abstractmethod\n\t    def encode(self, x):\n\t        pass\n\t    @abc.abstractmethod\n\t    def decode(self, z):\n\t        pass\n\t    def run(self):\n\t        total_rtf = 0.0\n\t        with torch.no_grad(), tqdm(self.dataset, desc=\"[test]\") as pbar:\n", "            for idx, (utt_id, x) in enumerate(pbar, 1):\n\t                start = time.time()\n\t                zq = self.encode(x)\n\t                y = self.decode(zq)\n\t                y = y.squeeze(1).transpose(1, 0).cpu().numpy() # T x C\n\t                rtf = (time.time() - start) / (len(y) / self.decoder_config['sampling_rate'])\n\t                pbar.set_postfix({\"RTF\": rtf})\n\t                total_rtf += rtf\n\t                # output wav file\n\t                self._save_wav(os.path.join(self.outdir, f\"{utt_id}_output.wav\"), y)\n", "        logging.info(\n\t            \"Finished generation of %d utterances (RTF = %.03f).\" % (idx, (total_rtf / idx))\n\t        )\n\t    def _save_wav(self, file_name, audio):\n\t        sf.write(\n\t            file_name,\n\t            audio,\n\t            self.decoder_config['sampling_rate'],\n\t            \"PCM_16\",\n\t        )\n", "    def _load_config(self, checkpoint, config_name='config.yml'):\n\t        return load_config(checkpoint, config_name)\n"]}
{"filename": "utils/audiodec.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport os\n\timport torch\n\tfrom typing import Union\n", "from models.autoencoder.AudioDec import StreamGenerator as generator_audiodec\n\tfrom models.vocoder.HiFiGAN import StreamGenerator as generator_hifigan\n\tfrom bin.stream import AudioCodec, AudioCodecStreamer\n\tclass AudioDec(AudioCodec):\n\t    def __init__(\n\t        self,\n\t        tx_device: str = \"cpu\",\n\t        rx_device: str = \"cpu\",\n\t        receptive_length: int = 8192, # actual number is 7209 for symAD_vctk_48000_hop300\n\t    ):\n", "        super(AudioDec, self).__init__(\n\t            tx_device = tx_device,\n\t            rx_device = rx_device,\n\t            receptive_length = receptive_length,\n\t        )\n\t    def _load_encoder(self, checkpoint):\n\t        # load config\n\t        config = self._load_config(checkpoint)\n\t        # load model\n\t        if config['model_type'] in ['symAudioDec', 'symAudioDecUniv']:\n", "            encoder = generator_audiodec\n\t        else:\n\t            raise NotImplementedError(f\"Encoder type {config['model_type']} is not supported!\")\n\t        encoder = encoder(**config['generator_params'])\n\t        encoder.load_state_dict(torch.load(checkpoint, map_location='cpu')['model']['generator'])\n\t        return encoder\n\t    def _load_decoder(self, checkpoint):\n\t        # load config\n\t        config = self._load_config(checkpoint)\n\t        # load model\n", "        if config['model_type'] in ['symAudioDec', 'symAudioDecUniv']:\n\t            decoder = generator_audiodec\n\t        elif config['model_type'] in ['HiFiGAN', 'UnivNet']:\n\t            decoder = generator_hifigan\n\t        else:\n\t            raise NotImplementedError(f\"Decoder {config['model_type']} is not supported!\")\n\t        decoder = decoder(**config['generator_params'])\n\t        decoder.load_state_dict(torch.load(checkpoint, map_location='cpu')['model']['generator'])\n\t        return decoder\n\tclass AudioDecStreamer(AudioCodecStreamer):\n", "    def __init__(\n\t        self,\n\t        input_device: Union[str, int],\n\t        output_device: Union[str, int],\n\t        input_channels: int = 1,\n\t        output_channels: int = 1,\n\t        frame_size: int = 512,\n\t        sample_rate: int = 48000,\n\t        gain: int = 1.0,\n\t        max_latency: float = 0.1,\n", "        # encoder params\n\t        tx_encoder = None,\n\t        tx_device: str = \"cpu\",\n\t        # decoder params\n\t        rx_encoder = None,\n\t        decoder = None,\n\t        rx_device: str = \"cpu\",\n\t    ):\n\t        super(AudioDecStreamer, self).__init__(\n\t            input_device = input_device,\n", "            output_device = output_device,\n\t            input_channels = input_channels,\n\t            output_channels = output_channels,\n\t            frame_size = frame_size,\n\t            sample_rate = sample_rate,\n\t            gain = gain,\n\t            max_latency = max_latency,\n\t            tx_encoder = tx_encoder,\n\t            tx_device = tx_device,\n\t            rx_encoder = rx_encoder,\n", "            decoder = decoder,\n\t            rx_device = rx_device,\n\t        )\n\t    def _encode(self, x):\n\t        x = self.tx_encoder.encode(x)\n\t        return self.tx_encoder.quantize(x)\n\t    def _decode(self, x):\n\t        x = self.rx_encoder.lookup(x)\n\t        return self.decoder.decode(x)\n\tdef assign_model(model):\n", "    if model == 'libritts_v1':\n\t        sample_rate = 24000\n\t        tx_steps = 500000\n\t        rx_steps = 500000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_libritts_24000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v1_symAD_libritts_24000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n\t    elif model == 'libritts_sym':\n\t        sample_rate = 24000\n\t        tx_steps = 500000\n\t        rx_steps = 1000000\n", "        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_libritts_24000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_libritts_24000_hop300', f\"checkpoint-{rx_steps}steps.pkl\")\n\t    elif model == 'vctk_v1':\n\t        sample_rate = 48000\n\t        tx_steps = 200000\n\t        rx_steps = 500000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v1_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n\t    elif model == 'vctk_sym':\n\t        sample_rate = 48000\n", "        tx_steps = 200000\n\t        rx_steps = 700000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{rx_steps}steps.pkl\") \n\t    elif model == 'vctk_v0':\n\t        sample_rate = 48000\n\t        tx_steps = 200000\n\t        rx_steps = 500000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v0_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n", "    elif model == 'vctk_v2':\n\t        sample_rate = 48000\n\t        tx_steps = 200000\n\t        rx_steps = 500000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v2_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n\t    elif model == 'vctk_denoise':\n\t        sample_rate = 48000\n\t        tx_steps = 200000\n\t        rx_steps = 500000\n", "        encoder_checkpoint = os.path.join('exp', 'denoise', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v1_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\")\n\t    elif model == 'vctk_univ':\n\t        sample_rate = 48000\n\t        tx_steps = 500000\n\t        rx_steps = 500000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symADuniv_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v3_symADuniv_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\")\n\t    elif model == 'vctk_univ_sym':\n\t        sample_rate = 48000\n", "        tx_steps = 500000\n\t        rx_steps = 1000000\n\t        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symADuniv_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n\t        decoder_checkpoint = os.path.join('exp', 'autoencoder', 'symADuniv_vctk_48000_hop300', f\"checkpoint-{rx_steps}steps.pkl\")\n\t    else:\n\t        raise NotImplementedError(f'Model {model} is not supported!')\n\t    return sample_rate, encoder_checkpoint, decoder_checkpoint\n"]}
{"filename": "layers/conv_layer.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Convolution layers.\"\"\"\n", "import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass Conv1d1x1(nn.Conv1d):\n\t    \"\"\"1x1 Conv1d.\"\"\"\n\t    def __init__(self, in_channels, out_channels, bias=True):\n\t        super(Conv1d1x1, self).__init__(in_channels, out_channels, kernel_size=1, bias=bias)\n\tclass NonCausalConv1d(nn.Module):\n\t    \"\"\"1D noncausal convloution w/ 2-sides padding.\"\"\"\n\t    def __init__(\n", "            self, \n\t            in_channels, \n\t            out_channels, \n\t            kernel_size, \n\t            stride=1, \n\t            padding=-1, \n\t            dilation=1,\n\t            groups=1,\n\t            bias=True):\n\t        super().__init__()\n", "        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.kernel_size = kernel_size\n\t        if padding < 0:\n\t            padding = (kernel_size - 1) // 2 * dilation\n\t        self.dilation = dilation\n\t        self.conv = nn.Conv1d(\n\t            in_channels=in_channels, \n\t            out_channels=out_channels, \n\t            kernel_size=kernel_size,\n", "            stride=stride, \n\t            padding=padding, \n\t            dilation=dilation, \n\t            groups=groups,\n\t            bias=bias,\n\t        )\n\t    def forward(self, x):\n\t        \"\"\"\n\t        Args:\n\t            x (Tensor): Float tensor variable with the shape  (B, C, T).\n", "        Returns:\n\t            Tensor: Float tensor variable with the shape (B, C, T).\n\t        \"\"\"\n\t        x = self.conv(x)\n\t        return x\n\tclass NonCausalConvTranspose1d(nn.Module):\n\t    \"\"\"1D noncausal transpose convloution.\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels,\n", "        out_channels,\n\t        kernel_size,\n\t        stride,\n\t        padding=-1,\n\t        output_padding=-1,\n\t        groups=1,\n\t        bias=True,\n\t    ):\n\t        super().__init__()\n\t        if padding < 0:\n", "            padding = (stride+1) // 2\n\t        if output_padding < 0:\n\t            output_padding = 1 if stride % 2 else 0\n\t        self.deconv = nn.ConvTranspose1d(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n\t            stride=stride,\n\t            padding=padding,\n\t            output_padding=output_padding,\n", "            groups=groups,\n\t            bias=bias,\n\t        )\n\t    def forward(self, x):\n\t        \"\"\"\n\t        Args:\n\t            x (Tensor): Float tensor variable with the shape  (B, C, T).\n\t        Returns:\n\t            Tensor: Float tensor variable with the shape (B, C', T').\n\t        \"\"\"\n", "        x = self.deconv(x)\n\t        return x\n\tclass CausalConv1d(NonCausalConv1d):\n\t    \"\"\"1D causal convloution w/ 1-side padding.\"\"\"\n\t    def __init__(\n\t        self, \n\t        in_channels, \n\t        out_channels, \n\t        kernel_size, \n\t        stride=1, \n", "        dilation=1, \n\t        groups=1,\n\t        bias=True,\n\t        pad_buffer=None,\n\t    ):\n\t        super(CausalConv1d, self).__init__(\n\t            in_channels=in_channels, \n\t            out_channels=out_channels, \n\t            kernel_size=kernel_size, \n\t            stride=stride, \n", "            padding=0,\n\t            dilation=dilation, \n\t            groups=groups,\n\t            bias=bias,\n\t        )\n\t        self.stride = stride\n\t        self.pad_length = (kernel_size - 1) * dilation\n\t        if pad_buffer is None:\n\t            pad_buffer = torch.zeros(1, in_channels, self.pad_length)\n\t        self.register_buffer(\"pad_buffer\", pad_buffer)\n", "    def forward(self, x):\n\t        pad = nn.ConstantPad1d((self.pad_length, 0), 0.0)\n\t        x = pad(x)\n\t        return self.conv(x)\n\t    def inference(self, x):\n\t        x = torch.cat((self.pad_buffer, x), -1)\n\t        self.pad_buffer = x[:, :, -self.pad_length:]\n\t        return self.conv(x)\n\t    def reset_buffer(self):\n\t        self.pad_buffer.zero_()\n", "class CausalConvTranspose1d(NonCausalConvTranspose1d):\n\t    \"\"\"1D causal transpose convloution.\"\"\"\n\t    def __init__(\n\t        self, \n\t        in_channels, \n\t        out_channels, \n\t        kernel_size, \n\t        stride, \n\t        bias=True,\n\t        pad_buffer=None,\n", "    ):\n\t        super(CausalConvTranspose1d, self).__init__(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n\t            stride=stride,\n\t            padding=0,\n\t            output_padding=0,\n\t            bias=bias,\n\t        )\n", "        self.stride = stride\n\t        self.pad_length = 1\n\t        if pad_buffer is None:\n\t            pad_buffer = torch.zeros(1, in_channels, self.pad_length)\n\t        self.register_buffer(\"pad_buffer\", pad_buffer)\n\t    def forward(self, x):\n\t        pad = nn.ReplicationPad1d((self.pad_length, 0))\n\t        x = pad(x)\n\t        return self.deconv(x)[:, :, self.stride : -self.stride]\n\t    def inference(self, x):\n", "        x = torch.cat((self.pad_buffer, x), -1)\n\t        self.pad_buffer = x[:, :, -self.pad_length:]\n\t        return self.deconv(x)[:, :, self.stride : -self.stride]\n\t    def reset_buffer(self):\n\t        self.pad_buffer.zero_()\n"]}
{"filename": "layers/vq_module.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/lucidrains/vector-quantize-pytorch/)\n\t\"\"\"Vector quantizer.\"\"\"\n", "import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass VectorQuantize(nn.Module):\n\t    \"\"\"Vector quantization w/ exponential moving averages (EMA)\"\"\"\n\t    def __init__(\n\t        self,\n\t        dim,\n\t        codebook_size,\n\t        decay = 0.8,\n", "        commitment = 1.,\n\t        eps = 1e-5,\n\t        n_embed = None,\n\t    ):\n\t        super().__init__()\n\t        n_embed = self.default(n_embed, codebook_size)\n\t        self.dim = dim\n\t        self.n_embed = n_embed\n\t        self.decay = decay\n\t        self.eps = eps\n", "        self.commitment = commitment\n\t        embed = torch.randn(dim, n_embed)\n\t        self.register_buffer('embed', embed)\n\t        self.register_buffer('cluster_size', torch.zeros(n_embed))\n\t        self.register_buffer('embed_avg', embed.clone())\n\t    @property\n\t    def codebook(self):\n\t        return self.embed.transpose(0, 1)\n\t    def exists(self,val):\n\t        return val is not None\n", "    def default(self, val, d):\n\t        return val if self.exists(val) else d\n\t    def ema_inplace(self, moving_avg, new, decay):\n\t        moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))\n\t    def laplace_smoothing(self, x, n_categories, eps=1e-5):\n\t        return (x + eps) / (x.sum() + n_categories * eps)\n\t    def forward(self, input):\n\t        dtype = input.dtype\n\t        flatten = input.reshape(-1, self.dim)\n\t        dist = (\n", "            flatten.pow(2).sum(1, keepdim=True)\n\t            - 2 * flatten @ self.embed\n\t            + self.embed.pow(2).sum(0, keepdim=True)\n\t        )\n\t        _, embed_ind = (-dist).max(1)\n\t        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n\t        embed_ind = embed_ind.view(*input.shape[:-1])\n\t        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n\t        if self.training:\n\t            self.ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n", "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n\t            self.ema_inplace(self.embed_avg, embed_sum, self.decay)\n\t            cluster_size = self.laplace_smoothing(self.cluster_size, self.n_embed, self.eps) * self.cluster_size.sum()\n\t            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n\t            self.embed.data.copy_(embed_normalized)\n\t        loss = F.mse_loss(quantize.detach(), input) * self.commitment\n\t        quantize = input + (quantize - input).detach()\n\t        avg_probs = torch.mean(embed_onehot, dim=0)\n\t        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\t        return quantize, loss, perplexity\n", "    def forward_index(self, input):\n\t        dtype = input.dtype\n\t        flatten = input.reshape(-1, self.dim)\n\t        dist = (\n\t            flatten.pow(2).sum(1, keepdim=True)\n\t            - 2 * flatten @ self.embed\n\t            + self.embed.pow(2).sum(0, keepdim=True)\n\t        )\n\t        _, embed_ind = (-dist).max(1)\n\t        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n", "        embed_ind = embed_ind.view(*input.shape[:-1])\n\t        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n\t        quantize = input + (quantize - input).detach()\n\t        return quantize, embed_ind\n\tclass ResidualVQ(nn.Module):\n\t    \"\"\" Residual VQ following algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf \"\"\"\n\t    def __init__(\n\t        self,\n\t        *,\n\t        num_quantizers,\n", "        **kwargs\n\t    ):\n\t        super().__init__()\n\t        self.layers = nn.ModuleList([VectorQuantize(**kwargs) for _ in range(num_quantizers)])\n\t    def forward(self, x):\n\t        quantized_out = 0.\n\t        residual = x\n\t        all_losses = []\n\t        all_perplexities = []\n\t        for layer in self.layers:\n", "            quantized, loss, perplexity = layer(residual)\n\t            # Issue: https://github.com/lucidrains/vector-quantize-pytorch/issues/33\n\t            # We found considering only the 1st layer VQ's graident results in better performance\n\t            #residual = residual - quantized.detach() # considering all layers' graidents\n\t            residual = residual - quantized # considering only the first layer's graident \n\t            quantized_out = quantized_out + quantized\n\t            all_losses.append(loss)\n\t            all_perplexities.append(perplexity)\n\t        all_losses, all_perplexities = map(torch.stack, (all_losses, all_perplexities))\n\t        return quantized_out, all_losses, all_perplexities\n", "    def forward_index(self, x, flatten_idx=False):\n\t        quantized_out = 0.\n\t        residual = x\n\t        all_indices = []\n\t        for i, layer in enumerate(self.layers):\n\t            quantized, indices = layer.forward_index(residual)\n\t            #residual = residual - quantized.detach()\n\t            residual = residual - quantized\n\t            quantized_out = quantized_out + quantized\n\t            if flatten_idx:\n", "                indices += (self.codebook_size * i)\n\t            all_indices.append(indices)\n\t        all_indices= torch.stack(all_indices)\n\t        return quantized_out, all_indices.squeeze(1)\n\t    def initial(self):\n\t        self.codebook = []\n\t        for layer in self.layers:\n\t            self.codebook.append(layer.codebook)\n\t        self.codebook_size = self.codebook[0].size(0)\n\t        self.codebook = torch.stack(self.codebook)\n", "        self.codebook = self.codebook.reshape(-1, self.codebook.size(-1))\n\t    def lookup(self, indices):\n\t        quantized_out = F.embedding(indices, self.codebook) # Num x T x C\n\t        return  torch.sum(quantized_out, dim=0,keepdim=True)\n"]}
{"filename": "models/utils.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Utility modules.\"\"\"\n\tdef check_mode(mode, method):\n\t    stream_modes = ['causal']\n", "    assert mode in stream_modes, f\"Mode {mode} does not support {method}!\"\n"]}
{"filename": "models/vocoder/HiFiGAN.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t# Reference (https://github.com/jik876/hifi-gan)\n", "\"\"\"HiFi-GAN Modules. (Causal)\"\"\"\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d\n\tfrom models.vocoder.modules.multi_fusion import MultiReceptiveField, MultiGroupConv1d\n\tfrom models.vocoder.modules.discriminator import HiFiGANMultiScaleDiscriminator\n", "from models.vocoder.modules.discriminator import HiFiGANMultiPeriodDiscriminator\n\tclass Generator(nn.Module):\n\t    \"\"\"HiFiGAN causal generator module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels=80,\n\t        out_channels=1,\n\t        channels=512,\n\t        kernel_size=7,\n\t        upsample_scales=(8, 8, 2, 2),\n", "        upsample_kernel_sizes=(16, 16, 4, 4),\n\t        resblock_kernel_sizes=(3, 7, 11),\n\t        resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)],\n\t        groups=1,\n\t        bias=True,\n\t        use_additional_convs=True,\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t        use_weight_norm=True,\n\t        stats=None,\n", "    ):\n\t        \"\"\"Initialize HiFiGANGenerator module.\n\t        Args:\n\t            in_channels (int): Number of input channels.\n\t            out_channels (int): Number of output channels.\n\t            channels (int): Number of hidden representation channels.\n\t            kernel_size (int): Kernel size of initial and final conv layer.\n\t            upsample_scales (list): List of upsampling scales.\n\t            upsample_kernel_sizes (list): List of kernel sizes for upsampling layers.\n\t            resblock_kernel_sizes (list): List of kernel sizes for residual blocks.\n", "            resblock_dilations (list): List of dilation list for residual blocks.\n\t            groups (int): Number of groups of residual conv\n\t            bias (bool): Whether to add bias parameter in convolution layers.\n\t            use_additional_convs (bool): Whether to use additional conv layers in residual blocks.\n\t            nonlinear_activation (str): Activation function module name.\n\t            nonlinear_activation_params (dict): Hyperparameters for activation function.\n\t            use_weight_norm (bool): Whether to use weight norm.\n\t                If set to true, it will be applied to all of the conv layers.\n\t            stats (str): File name of the statistic file\n\t        \"\"\"\n", "        super().__init__()\n\t        # check hyperparameters are valid\n\t        assert kernel_size % 2 == 1, \"Kernel size must be odd number.\"\n\t        assert len(upsample_scales) == len(upsample_kernel_sizes)\n\t        assert len(resblock_dilations) == len(resblock_kernel_sizes)\n\t        # Group conv or MRF\n\t        if (len(resblock_dilations) == len(resblock_kernel_sizes) == 1) and (groups > 1):\n\t            multi_fusion = MultiGroupConv1d\n\t        else:\n\t            multi_fusion = MultiReceptiveField\n", "        # define modules\n\t        self.num_upsamples = len(upsample_kernel_sizes)\n\t        self.input_conv = CausalConv1d(\n\t            in_channels,\n\t            channels,\n\t            kernel_size,\n\t            stride=1,\n\t        )\n\t        self.upsamples = nn.ModuleList()\n\t        self.blocks = nn.ModuleList()\n", "        self.activation_upsamples = getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)\n\t        for i in range(len(upsample_kernel_sizes)):\n\t            assert upsample_kernel_sizes[i] == 2 * upsample_scales[i]\n\t            self.upsamples += [\n\t                CausalConvTranspose1d(\n\t                    channels // (2 ** i),\n\t                    channels // (2 ** (i + 1)),\n\t                    kernel_size=upsample_kernel_sizes[i],\n\t                    stride=upsample_scales[i],\n\t                )\n", "            ]\n\t            self.blocks += [\n\t                multi_fusion(\n\t                    channels=channels // (2 ** (i + 1)),\n\t                    resblock_kernel_sizes=resblock_kernel_sizes,\n\t                    resblock_dilations=resblock_dilations,\n\t                    groups=groups,\n\t                    bias=bias,\n\t                    use_additional_convs=use_additional_convs,\n\t                    nonlinear_activation=nonlinear_activation,\n", "                    nonlinear_activation_params=nonlinear_activation_params,\n\t                )\n\t            ]\n\t        self.activation_output1 = nn.LeakyReLU()\n\t        self.activation_output2 = nn.Tanh()\n\t        self.output_conv = CausalConv1d(\n\t            channels // (2 ** (i + 1)),\n\t            out_channels,\n\t            kernel_size,\n\t            stride=1,\n", "        )\n\t        # load stats\n\t        if stats is not None:\n\t            self.register_stats(stats)\n\t            self.norm = True\n\t        else:\n\t            self.norm = False\n\t        logging.info(f\"Input normalization: {self.norm}\")\n\t        # apply weight norm\n\t        if use_weight_norm:\n", "            self.apply_weight_norm()\n\t        # reset parameters\n\t        self.reset_parameters()\n\t    def forward(self, c):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            c (Tensor): Input tensor (B, in_channels, T).\n\t        Returns:\n\t            Tensor: Output tensor (B, out_channels, T).\n\t        \"\"\"\n", "        if self.norm:\n\t            c = (c.transpose(2, 1) - self.mean) / self.scale\n\t            c = c.transpose(2, 1)\n\t        c = self.input_conv(c)\n\t        for i in range(self.num_upsamples):\n\t            c = self.upsamples[i](self.activation_upsamples(c))\n\t            c = self.blocks[i](c)\n\t        c = self.output_conv(self.activation_output1(c))\n\t        c = self.activation_output2(c)\n\t        return c\n", "    def reset_parameters(self):\n\t        \"\"\"Reset parameters.\n\t        This initialization follows the official implementation manner.\n\t        https://github.com/jik876/hifi-gan/blob/master/models.py\n\t        \"\"\"\n\t        def _reset_parameters(m):\n\t            if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n\t                m.weight.data.normal_(0.0, 0.01)\n\t                logging.debug(f\"Reset parameters in {m}.\")\n\t        self.apply(_reset_parameters)\n", "    def remove_weight_norm(self):\n\t        \"\"\"Remove weight normalization module from all of the layers.\"\"\"\n\t        def _remove_weight_norm(m):\n\t            try:\n\t                logging.debug(f\"Weight norm is removed from {m}.\")\n\t                nn.utils.remove_weight_norm(m)\n\t            except ValueError:  # this module didn't have weight norm\n\t                return\n\t        self.apply(_remove_weight_norm)\n\t    def apply_weight_norm(self):\n", "        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\t        def _apply_weight_norm(m):\n\t            if isinstance(m, nn.Conv1d) or isinstance(\n\t                m, nn.ConvTranspose1d\n\t            ):\n\t                nn.utils.weight_norm(m)\n\t                logging.debug(f\"Weight norm is applied to {m}.\")\n\t        self.apply(_apply_weight_norm)\n\t    def register_stats(self, stats):\n\t        \"\"\"Register stats for de-normalization as buffer.\n", "        Args:\n\t            stats (str): Path of statistics file (\".npy\" or \".h5\").\n\t        \"\"\"\n\t        assert stats.endswith(\".h5\") or stats.endswith(\".npy\")\n\t        assert os.path.exists(stats), f\"Stats {stats} does not exist!\"\n\t        mean = np.load(stats)[0].reshape(-1)\n\t        scale = np.load(stats)[1].reshape(-1)\n\t        self.register_buffer(\"mean\", torch.from_numpy(mean).float())\n\t        self.register_buffer(\"scale\", torch.from_numpy(scale).float())\n\t        logging.info(\"Successfully registered stats as buffer.\")\n", "class StreamGenerator(Generator):\n\t    \"\"\"HiFiGAN streaming generator.\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels=80,\n\t        out_channels=1,\n\t        channels=512,\n\t        kernel_size=7,\n\t        upsample_scales=(8, 8, 2, 2),\n\t        upsample_kernel_sizes=(16, 16, 4, 4),\n", "        resblock_kernel_sizes=(3, 7, 11),\n\t        resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)],\n\t        groups=1,\n\t        bias=True,\n\t        use_additional_convs=True,\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t        use_weight_norm=True,\n\t        stats=None,\n\t    ):\n", "        super(StreamGenerator, self).__init__(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            channels=channels,\n\t            kernel_size=kernel_size,\n\t            upsample_scales=upsample_scales,\n\t            upsample_kernel_sizes=upsample_kernel_sizes,\n\t            resblock_kernel_sizes=resblock_kernel_sizes,\n\t            resblock_dilations=resblock_dilations,\n\t            groups=groups,\n", "            bias=bias,\n\t            use_additional_convs=use_additional_convs,\n\t            nonlinear_activation=nonlinear_activation,\n\t            nonlinear_activation_params=nonlinear_activation_params,\n\t            use_weight_norm=use_weight_norm,\n\t            stats=stats,\n\t        )\n\t        self.reset_buffer()\n\t    def initial_decoder(self, c):\n\t        self.decode(c)\n", "    def decode(self, c):\n\t        c = self.decode_norm(c)\n\t        c = self.decode_input(c.transpose(2, 1))\n\t        c = self.decode_upsample(c)\n\t        c = self.decode_output(c)\n\t        return c\n\t    def decode_norm(self, c):\n\t        if self.norm:\n\t            c = (c - self.mean) / self.scale\n\t        return c \n", "    def decode_input(self, c):\n\t        c = self.input_conv.inference(c)\n\t        return c\n\t    def decode_upsample(self, c):        \n\t        for i in range(self.num_upsamples):\n\t            c = self.upsamples[i].inference(self.activation_upsamples(c))\n\t            c = self.blocks[i].inference(c)\n\t        return c\n\t    def decode_output(self, c):\n\t        c = self.output_conv.inference(self.activation_output1(c))\n", "        return self.activation_output2(c)\n\t    def reset_buffer(self):\n\t        \"\"\"Apply weight normalization module from all layers.\"\"\"\n\t        def _reset_buffer(m):\n\t            if isinstance(m, CausalConv1d) or isinstance(m, CausalConvTranspose1d):\n\t                m.reset_buffer()\n\t        self.apply(_reset_buffer)\n\tclass Discriminator(nn.Module):\n\t    \"\"\"HiFi-GAN multi-scale + multi-period discriminator module.\"\"\"\n\t    def __init__(\n", "        self,\n\t        # Multi-scale discriminator related\n\t        scales=3,\n\t        scale_downsample_pooling=\"AvgPool1d\",\n\t        scale_downsample_pooling_params={\n\t            \"kernel_size\": 4,\n\t            \"stride\": 2,\n\t            \"padding\": 2,\n\t        },\n\t        scale_discriminator_params={\n", "            \"in_channels\": 1,\n\t            \"out_channels\": 1,\n\t            \"kernel_sizes\": [15, 41, 5, 3],\n\t            \"channels\": 128,\n\t            \"max_downsample_channels\": 1024,\n\t            \"max_groups\": 16,\n\t            \"bias\": True,\n\t            \"downsample_scales\": [2, 2, 4, 4, 1],\n\t            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n", "        },\n\t        follow_official_norm=True,\n\t        # Multi-period discriminator related\n\t        periods=[2, 3, 5, 7, 11],\n\t        period_discriminator_params={\n\t            \"in_channels\": 1,\n\t            \"out_channels\": 1,\n\t            \"kernel_sizes\": [5, 3],\n\t            \"channels\": 32,\n\t            \"downsample_scales\": [3, 3, 3, 3, 1],\n", "            \"max_downsample_channels\": 1024,\n\t            \"bias\": True,\n\t            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n\t            \"use_weight_norm\": True,\n\t            \"use_spectral_norm\": False,\n\t        },\n\t    ):\n\t        \"\"\"Initilize HiFiGAN multi-scale + multi-period discriminator module.\n\t        Args:\n", "            scales (int): Number of multi-scales.\n\t            scale_downsample_pooling (str): Pooling module name for downsampling of the inputs.\n\t            scale_downsample_pooling_params (dict): Parameters for the above pooling module.\n\t            scale_discriminator_params (dict): Parameters for hifi-gan scale discriminator module.\n\t            follow_official_norm (bool): Whether to follow the norm setting of the official\n\t                implementaion. The first discriminator uses spectral norm and the other\n\t                discriminators use weight norm.\n\t            periods (list): List of periods.\n\t            period_discriminator_params (dict): Parameters for hifi-gan period discriminator module.\n\t                The period parameter will be overwritten.\n", "        \"\"\"\n\t        super().__init__()\n\t        self.msd = HiFiGANMultiScaleDiscriminator(\n\t            scales=scales,\n\t            downsample_pooling=scale_downsample_pooling,\n\t            downsample_pooling_params=scale_downsample_pooling_params,\n\t            discriminator_params=scale_discriminator_params,\n\t            follow_official_norm=follow_official_norm,\n\t        )\n\t        self.mpd = HiFiGANMultiPeriodDiscriminator(\n", "            periods=periods,\n\t            discriminator_params=period_discriminator_params,\n\t        )\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input noise signal (B, C, T).\n\t        Returns:\n\t            List: List of list of each discriminator outputs,\n\t                which consists of each layer output tensors.\n", "                Multi scale and multi period ones are concatenated.\n\t        \"\"\"\n\t        (batch, channel, time) = x.size()\n\t        if channel != 1:\n\t            x = x.reshape(batch * channel, 1, time)\n\t        msd_outs = self.msd(x)\n\t        mpd_outs = self.mpd(x)\n\t        return msd_outs + mpd_outs\n"]}
{"filename": "models/vocoder/UnivNet.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t# Reference (https://github.com/chomeyama/SiFiGAN)\n", "\"\"\"HiFi-GAN Modules. (Causal)\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom models.vocoder.modules.discriminator import UnivNetMultiResolutionSpectralDiscriminator\n\tfrom models.vocoder.modules.discriminator import HiFiGANMultiPeriodDiscriminator\n\tclass Discriminator(nn.Module):\n\t    \"\"\"UnivNet multi-resolution spectrogram + multi-period discriminator module.\"\"\"\n\t    def __init__(\n\t        self,\n", "        # Multi-resolution spectrogram discriminator related\n\t        fft_sizes=[1024, 2048, 512],\n\t        hop_sizes=[120, 240, 50],\n\t        win_lengths=[600, 1200, 240],\n\t        window=\"hann_window\",\n\t        spectral_discriminator_params={\n\t            \"channels\": 32,\n\t            \"kernel_sizes\": [(3, 9), (3, 9), (3, 9), (3, 9), (3, 3), (3, 3)],\n\t            \"strides\": [(1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1)],\n\t            \"bias\": True,\n", "            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.2},\n\t        },\n\t        # Multi-period discriminator related\n\t        periods=[2, 3, 5, 7, 11],\n\t        period_discriminator_params={\n\t            \"in_channels\": 1,\n\t            \"out_channels\": 1,\n\t            \"kernel_sizes\": [5, 3],\n\t            \"channels\": 32,\n", "            \"downsample_scales\": [3, 3, 3, 3, 1],\n\t            \"max_downsample_channels\": 1024,\n\t            \"bias\": True,\n\t            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n\t            \"use_weight_norm\": True,\n\t            \"use_spectral_norm\": False,\n\t        },\n\t        flat_channel=False,\n\t    ):\n", "        \"\"\"Initilize HiFiGAN multi-scale + multi-period discriminator module.\n\t        Args:\n\t            fft_sizes (list): FFT sizes for each spectral discriminator.\n\t            hop_sizes (list): Hop sizes for each spectral discriminator.\n\t            win_lengths (list): Window lengths for each spectral discriminator.\n\t            window (stt): Name of window function.\n\t            sperctral_discriminator_params (dict): Parameters for hifi-gan scale discriminator module.\n\t            periods (list): List of periods.\n\t            period_discriminator_params (dict): Parameters for hifi-gan period discriminator module.\n\t                The period parameter will be overwritten.\n", "            flat_channel (bool):set true to flat multi-channel input to one-channel with multi-batch\n\t        \"\"\"\n\t        super().__init__()\n\t        self.flat_channel = flat_channel\n\t        self.mrsd = UnivNetMultiResolutionSpectralDiscriminator(\n\t            fft_sizes=fft_sizes,\n\t            hop_sizes=hop_sizes,\n\t            win_lengths=win_lengths,\n\t            window=window,\n\t            discriminator_params=spectral_discriminator_params,\n", "        )\n\t        self.mpd = HiFiGANMultiPeriodDiscriminator(\n\t            periods=periods,\n\t            discriminator_params=period_discriminator_params,\n\t        )\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input noise signal (B, C, T).\n\t        Returns:\n", "            List: List of list of each discriminator outputs,\n\t                which consists of each layer output tensors.\n\t                Multi scale and multi period ones are concatenated.\n\t        \"\"\"\n\t        (batch, channel, time) = x.size()\n\t        if channel != 1 and self.flat_channel:\n\t            x = x.reshape(batch * channel, 1, time)\n\t        mrsd_outs = self.mrsd(x)\n\t        mpd_outs = self.mpd(x)\n\t        return mrsd_outs + mpd_outs\n"]}
{"filename": "models/vocoder/modules/multi_fusion.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t# Reference (https://github.com/r9y9/wavenet_vocoder)\n", "# Reference (https://github.com/jik876/hifi-gan)\n\t\"\"\"Multi-fusion modules.\"\"\"\n\timport math\n\timport torch\n\timport torch.nn as nn\n\tfrom layers.conv_layer import CausalConv1d, Conv1d1x1\n\tfrom models.vocoder.modules.residual_block import HiFiGANResidualBlock\n\tclass MultiReceptiveField(nn.Module):\n\t    \"\"\"Multi-receptive field module in HiFiGAN.\"\"\"\n\t    def __init__(\n", "        self,\n\t        channels=512,\n\t        resblock_kernel_sizes=(3, 7, 11),\n\t        resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)],\n\t        groups=1,\n\t        bias=True,\n\t        use_additional_convs=True,\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t    ):\n", "        assert len(resblock_kernel_sizes) == len(resblock_dilations)\n\t        super().__init__()\n\t        self.num_blocks = len(resblock_kernel_sizes)\n\t        self.blocks = nn.ModuleList()\n\t        for i in range(self.num_blocks):\n\t            self.blocks += [\n\t                HiFiGANResidualBlock(\n\t                    kernel_size=resblock_kernel_sizes[i],\n\t                    channels=channels,\n\t                    dilations=resblock_dilations[i],\n", "                    groups=groups,\n\t                    bias=bias,\n\t                    use_additional_convs=use_additional_convs,\n\t                    nonlinear_activation=nonlinear_activation,\n\t                    nonlinear_activation_params=nonlinear_activation_params,\n\t                )\n\t            ]\n\t    def forward(self, c):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n", "            c (Tensor): Input tensor (B, channels, T).\n\t        Returns:\n\t            Tensor: Output tensor (B, channels, T).\n\t        \"\"\"\n\t        cs = 0.0  # initialize\n\t        for i in range(self.num_blocks):\n\t            cs += self.blocks[i](c)\n\t        c = cs / self.num_blocks\n\t        return c\n\t    def inference(self, c):\n", "        cs = 0.0  # initialize\n\t        for i in range(self.num_blocks):\n\t            cs += self.blocks[i].inference(c)\n\t        c = cs / self.num_blocks\n\t        return c\n\tclass MultiGroupConv1d(HiFiGANResidualBlock):\n\t    \"\"\"Multi-group convolution module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        channels=512,\n", "        resblock_kernel_sizes=(3),\n\t        resblock_dilations=[(1, 3, 5)],\n\t        groups=3,\n\t        bias=True,\n\t        use_additional_convs=True,\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t    ):\n\t        assert len(resblock_kernel_sizes) == len(resblock_dilations) == 1\n\t        super(MultiGroupConv1d, self).__init__(\n", "            kernel_size=resblock_kernel_sizes[0],\n\t            channels=channels*groups,\n\t            dilations=resblock_dilations[0],\n\t            groups=groups,\n\t            bias=bias,\n\t            use_additional_convs=use_additional_convs,\n\t            nonlinear_activation=nonlinear_activation,\n\t            nonlinear_activation_params=nonlinear_activation_params,\n\t        )\n\t        self.groups = groups\n", "        self.conv_out = Conv1d1x1(\n\t            in_channels=channels*groups, \n\t            out_channels=channels,\n\t            bias=False,\n\t        )\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input tensor (B, channels, T).\n\t        Returns:\n", "            Tensor: Output tensor (B, channels, T).\n\t        \"\"\"\n\t        x = x.repeat(1, self.groups, 1) # (B, n*C, T)\n\t        for idx in range(self.num_layer):\n\t            xt = self.convs1[idx](self.activation(x))\n\t            if self.use_additional_convs:\n\t                xt = self.convs2[idx](self.activation(xt))\n\t            x = xt + x\n\t        x = self.conv_out(x) # (B, C, T)\n\t        return x\n", "    def inference(self, x):\n\t        x = x.repeat(1, self.groups, 1) # (B, n*C, T)\n\t        for idx in range(self.num_layer):\n\t            xt = self.convs1[idx].inference(self.activation(x))\n\t            if self.use_additional_convs:\n\t                xt = self.convs2[idx].inference(self.activation(xt))\n\t            x = xt + x\n\t        x = self.conv_out(x) # (B, C, T)\n\t        return x"]}
{"filename": "models/vocoder/modules/discriminator.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t# Reference (https://github.com/jik876/hifi-gan)\n", "# Reference (https://github.com/chomeyama/SiFiGAN)\n\t\"\"\"GAN-based Discriminators\"\"\"\n\timport copy\n\timport logging\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torchaudio.functional import spectrogram\n\tclass HiFiGANPeriodDiscriminator(nn.Module):\n", "    \"\"\"HiFiGAN period discriminator module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels=1,\n\t        out_channels=1,\n\t        period=3,\n\t        kernel_sizes=[5, 3],\n\t        channels=32,\n\t        downsample_scales=[3, 3, 3, 3, 1],\n\t        max_downsample_channels=1024,\n", "        bias=True,\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t        use_weight_norm=True,\n\t        use_spectral_norm=False,\n\t    ):\n\t        \"\"\"Initialize HiFiGANPeriodDiscriminator module.\n\t        Args:\n\t            in_channels (int): Number of input channels.\n\t            out_channels (int): Number of output channels.\n", "            period (int): Period.\n\t            kernel_sizes (list): Kernel sizes of initial conv layers and the final conv layer.\n\t            channels (int): Number of initial channels.\n\t            downsample_scales (list): List of downsampling scales.\n\t            max_downsample_channels (int): Number of maximum downsampling channels.\n\t            use_additional_convs (bool): Whether to use additional conv layers in residual blocks.\n\t            bias (bool): Whether to add bias parameter in convolution layers.\n\t            nonlinear_activation (str): Activation function module name.\n\t            nonlinear_activation_params (dict): Hyperparameters for activation function.\n\t            use_weight_norm (bool): Whether to use weight norm.\n", "                If set to true, it will be applied to all of the conv layers.\n\t            use_spectral_norm (bool): Whether to use spectral norm.\n\t                If set to true, it will be applied to all of the conv layers.\n\t        \"\"\"\n\t        super().__init__()\n\t        assert len(kernel_sizes) == 2\n\t        assert kernel_sizes[0] % 2 == 1, \"Kernel size must be odd number.\"\n\t        assert kernel_sizes[1] % 2 == 1, \"Kernel size must be odd number.\"\n\t        self.period = period\n\t        self.convs = nn.ModuleList()\n", "        in_chs = in_channels\n\t        out_chs = channels\n\t        for downsample_scale in downsample_scales:\n\t            self.convs += [\n\t                torch.nn.Sequential(\n\t                    torch.nn.Conv2d(\n\t                        in_chs,\n\t                        out_chs,\n\t                        (kernel_sizes[0], 1),\n\t                        (downsample_scale, 1),\n", "                        padding=((kernel_sizes[0] - 1) // 2, 0),\n\t                    ),\n\t                    getattr(torch.nn, nonlinear_activation)(\n\t                        **nonlinear_activation_params\n\t                    ),\n\t                )\n\t            ]\n\t            in_chs = out_chs\n\t            # NOTE(kan-bayashi): Use downsample_scale + 1?\n\t            out_chs = min(out_chs * 4, max_downsample_channels)\n", "        self.output_conv = torch.nn.Conv2d(\n\t            out_chs,\n\t            out_channels,\n\t            (kernel_sizes[1] - 1, 1),\n\t            1,\n\t            padding=((kernel_sizes[1] - 1) // 2, 0),\n\t        )\n\t        if use_weight_norm and use_spectral_norm:\n\t            raise ValueError(\"Either use use_weight_norm or use_spectral_norm.\")\n\t        # apply weight norm\n", "        if use_weight_norm:\n\t            self.apply_weight_norm()\n\t        # apply spectral norm\n\t        if use_spectral_norm:\n\t            self.apply_spectral_norm()\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            c (Tensor): Input tensor (B, in_channels, T).\n\t        Returns:\n", "            list: List of each layer's tensors.\n\t        \"\"\"\n\t        # transform 1d to 2d -> (B, C, T/P, P)\n\t        b, c, t = x.shape\n\t        if t % self.period != 0:\n\t            n_pad = self.period - (t % self.period)\n\t            x = F.pad(x, (0, n_pad), \"reflect\")\n\t            t += n_pad\n\t        x = x.view(b, c, t // self.period, self.period)\n\t        # forward conv\n", "        outs = []\n\t        for layer in self.convs:\n\t            x = layer(x)\n\t            outs += [x]\n\t        x = self.output_conv(x)\n\t        x = torch.flatten(x, 1, -1)\n\t        outs += [x]\n\t        return outs\n\t    def apply_weight_norm(self):\n\t        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n", "        def _apply_weight_norm(m):\n\t            if isinstance(m, torch.nn.Conv2d):\n\t                torch.nn.utils.weight_norm(m)\n\t                logging.debug(f\"Weight norm is applied to {m}.\")\n\t        self.apply(_apply_weight_norm)\n\t    def apply_spectral_norm(self):\n\t        \"\"\"Apply spectral normalization module from all of the layers.\"\"\"\n\t        def _apply_spectral_norm(m):\n\t            if isinstance(m, torch.nn.Conv2d):\n\t                torch.nn.utils.spectral_norm(m)\n", "                logging.debug(f\"Spectral norm is applied to {m}.\")\n\t        self.apply(_apply_spectral_norm)\n\tclass HiFiGANMultiPeriodDiscriminator(nn.Module):\n\t    \"\"\"HiFiGAN multi-period discriminator module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        periods=[2, 3, 5, 7, 11],\n\t        discriminator_params={\n\t            \"in_channels\": 1,\n\t            \"out_channels\": 1,\n", "            \"kernel_sizes\": [5, 3],\n\t            \"channels\": 32,\n\t            \"downsample_scales\": [3, 3, 3, 3, 1],\n\t            \"max_downsample_channels\": 1024,\n\t            \"bias\": True,\n\t            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n\t            \"use_weight_norm\": True,\n\t            \"use_spectral_norm\": False,\n\t        },\n", "    ):\n\t        \"\"\"Initialize HiFiGANMultiPeriodDiscriminator module.\n\t        Args:\n\t            periods (list): List of periods.\n\t            discriminator_params (dict): Parameters for hifi-gan period discriminator module.\n\t                The period parameter will be overwritten.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.discriminators = nn.ModuleList()\n\t        for period in periods:\n", "            params = copy.deepcopy(discriminator_params)\n\t            params[\"period\"] = period\n\t            self.discriminators += [HiFiGANPeriodDiscriminator(**params)]\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input noise signal (B, 1, T).\n\t        Returns:\n\t            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n\t        \"\"\"\n", "        outs = []\n\t        for f in self.discriminators:\n\t            outs += [f(x)]\n\t        return outs\n\tclass HiFiGANScaleDiscriminator(nn.Module):\n\t    \"\"\"HiFi-GAN scale discriminator module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels=1,\n\t        out_channels=1,\n", "        kernel_sizes=[15, 41, 5, 3],\n\t        channels=128,\n\t        max_downsample_channels=1024,\n\t        max_groups=16,\n\t        bias=True,\n\t        downsample_scales=[2, 2, 4, 4, 1],\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t        use_weight_norm=True,\n\t        use_spectral_norm=False,\n", "    ):\n\t        \"\"\"Initilize HiFiGAN scale discriminator module.\n\t        Args:\n\t            in_channels (int): Number of input channels.\n\t            out_channels (int): Number of output channels.\n\t            kernel_sizes (list): List of four kernel sizes. The first will be used for the first conv layer,\n\t                and the second is for downsampling part, and the remaining two are for output layers.\n\t            channels (int): Initial number of channels for conv layer.\n\t            max_downsample_channels (int): Maximum number of channels for downsampling layers.\n\t            bias (bool): Whether to add bias parameter in convolution layers.\n", "            downsample_scales (list): List of downsampling scales.\n\t            nonlinear_activation (str): Activation function module name.\n\t            nonlinear_activation_params (dict): Hyperparameters for activation function.\n\t            use_weight_norm (bool): Whether to use weight norm.\n\t                If set to true, it will be applied to all of the conv layers.\n\t            use_spectral_norm (bool): Whether to use spectral norm.\n\t                If set to true, it will be applied to all of the conv layers.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.layers = nn.ModuleList()\n", "        # check kernel size is valid\n\t        assert len(kernel_sizes) == 4\n\t        for ks in kernel_sizes:\n\t            assert ks % 2 == 1\n\t        # add first layer\n\t        self.layers += [\n\t            torch.nn.Sequential(\n\t                torch.nn.Conv1d(\n\t                    in_channels,\n\t                    channels,\n", "                    # NOTE(kan-bayashi): Use always the same kernel size\n\t                    kernel_sizes[0],\n\t                    bias=bias,\n\t                    padding=(kernel_sizes[0] - 1) // 2,\n\t                ),\n\t                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n\t            )\n\t        ]\n\t        # add downsample layers\n\t        in_chs = channels\n", "        out_chs = channels\n\t        # NOTE(kan-bayashi): Remove hard coding?\n\t        groups = 4\n\t        for downsample_scale in downsample_scales:\n\t            self.layers += [\n\t                torch.nn.Sequential(\n\t                    torch.nn.Conv1d(\n\t                        in_chs,\n\t                        out_chs,\n\t                        kernel_size=kernel_sizes[1],\n", "                        stride=downsample_scale,\n\t                        padding=(kernel_sizes[1] - 1) // 2,\n\t                        groups=groups,\n\t                        bias=bias,\n\t                    ),\n\t                    getattr(torch.nn, nonlinear_activation)(\n\t                        **nonlinear_activation_params\n\t                    ),\n\t                )\n\t            ]\n", "            in_chs = out_chs\n\t            # NOTE(kan-bayashi): Remove hard coding?\n\t            out_chs = min(in_chs * 2, max_downsample_channels)\n\t            # NOTE(kan-bayashi): Remove hard coding?\n\t            groups = min(groups * 4, max_groups)\n\t        # add final layers\n\t        out_chs = min(in_chs * 2, max_downsample_channels)\n\t        self.layers += [\n\t            torch.nn.Sequential(\n\t                torch.nn.Conv1d(\n", "                    in_chs,\n\t                    out_chs,\n\t                    kernel_size=kernel_sizes[2],\n\t                    stride=1,\n\t                    padding=(kernel_sizes[2] - 1) // 2,\n\t                    bias=bias,\n\t                ),\n\t                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n\t            )\n\t        ]\n", "        self.layers += [\n\t            torch.nn.Conv1d(\n\t                out_chs,\n\t                out_channels,\n\t                kernel_size=kernel_sizes[3],\n\t                stride=1,\n\t                padding=(kernel_sizes[3] - 1) // 2,\n\t                bias=bias,\n\t            ),\n\t        ]\n", "        if use_weight_norm and use_spectral_norm:\n\t            raise ValueError(\"Either use use_weight_norm or use_spectral_norm.\")\n\t        # apply weight norm\n\t        if use_weight_norm:\n\t            self.apply_weight_norm()\n\t        # apply spectral norm\n\t        if use_spectral_norm:\n\t            self.apply_spectral_norm()\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n", "        Args:\n\t            x (Tensor): Input noise signal (B, 1, T).\n\t        Returns:\n\t            List: List of output tensors of each layer.\n\t        \"\"\"\n\t        outs = []\n\t        for f in self.layers:\n\t            x = f(x)\n\t            outs += [x]\n\t        return outs\n", "    def apply_weight_norm(self):\n\t        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\t        def _apply_weight_norm(m):\n\t            if isinstance(m, torch.nn.Conv2d):\n\t                torch.nn.utils.weight_norm(m)\n\t                logging.debug(f\"Weight norm is applied to {m}.\")\n\t        self.apply(_apply_weight_norm)\n\t    def apply_spectral_norm(self):\n\t        \"\"\"Apply spectral normalization module from all of the layers.\"\"\"\n\t        def _apply_spectral_norm(m):\n", "            if isinstance(m, torch.nn.Conv2d):\n\t                torch.nn.utils.spectral_norm(m)\n\t                logging.debug(f\"Spectral norm is applied to {m}.\")\n\t        self.apply(_apply_spectral_norm)\n\tclass HiFiGANMultiScaleDiscriminator(nn.Module):\n\t    \"\"\"HiFi-GAN multi-scale discriminator module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        scales=3,\n\t        downsample_pooling=\"AvgPool1d\",\n", "        # follow the official implementation setting\n\t        downsample_pooling_params={\n\t            \"kernel_size\": 4,\n\t            \"stride\": 2,\n\t            \"padding\": 2,\n\t        },\n\t        discriminator_params={\n\t            \"in_channels\": 1,\n\t            \"out_channels\": 1,\n\t            \"kernel_sizes\": [15, 41, 5, 3],\n", "            \"channels\": 128,\n\t            \"max_downsample_channels\": 1024,\n\t            \"max_groups\": 16,\n\t            \"bias\": True,\n\t            \"downsample_scales\": [2, 2, 4, 4, 1],\n\t            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n\t        },\n\t        follow_official_norm=False,\n\t    ):\n", "        \"\"\"Initilize HiFiGAN multi-scale discriminator module.\n\t        Args:\n\t            scales (int): Number of multi-scales.\n\t            downsample_pooling (str): Pooling module name for downsampling of the inputs.\n\t            downsample_pooling_params (dict): Parameters for the above pooling module.\n\t            discriminator_params (dict): Parameters for hifi-gan scale discriminator module.\n\t            follow_official_norm (bool): Whether to follow the norm setting of the official\n\t                implementaion. The first discriminator uses spectral norm and the other\n\t                discriminators use weight norm.\n\t        \"\"\"\n", "        super().__init__()\n\t        self.discriminators = nn.ModuleList()\n\t        # add discriminators\n\t        for i in range(scales):\n\t            params = copy.deepcopy(discriminator_params)\n\t            if follow_official_norm:\n\t                if i == 0:\n\t                    params[\"use_weight_norm\"] = False\n\t                    params[\"use_spectral_norm\"] = True\n\t                else:\n", "                    params[\"use_weight_norm\"] = True\n\t                    params[\"use_spectral_norm\"] = False\n\t            self.discriminators += [HiFiGANScaleDiscriminator(**params)]\n\t        self.pooling = getattr(torch.nn, downsample_pooling)(\n\t            **downsample_pooling_params\n\t        )\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input noise signal (B, 1, T).\n", "        Returns:\n\t            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n\t        \"\"\"\n\t        outs = []\n\t        for f in self.discriminators:\n\t            outs += [f(x)]\n\t            x = self.pooling(x)\n\t        return outs\n\tclass UnivNetSpectralDiscriminator(nn.Module):\n\t    \"\"\"UnivNet spectral discriminator module.\"\"\"\n", "    def __init__(\n\t        self,\n\t        fft_size,\n\t        hop_size,\n\t        win_length,\n\t        window=\"hann_window\",\n\t        kernel_sizes=[(3, 9), (3, 9), (3, 9), (3, 9), (3, 3), (3, 3)],\n\t        strides=[(1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1)],\n\t        channels=32,\n\t        bias=True,\n", "        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.2},\n\t        use_weight_norm=True,\n\t    ):\n\t        \"\"\"Initilize HiFiGAN scale discriminator module.\n\t        Args:\n\t            fft_size (list): FFT size.\n\t            hop_size (int): Hop size.\n\t            win_length (int): Window length.\n\t            window (stt): Name of window function.\n", "            kernel_sizes (list): List of kernel sizes in down-sampling CNNs.\n\t            strides (list): List of stride sizes in down-sampling CNNs.\n\t            channels (int): Number of channels for conv layer.\n\t            bias (bool): Whether to add bias parameter in convolution layers.\n\t            nonlinear_activation (str): Activation function module name.\n\t            nonlinear_activation_params (dict): Hyperparameters for activation function.\n\t            use_weight_norm (bool): Whether to use weight norm.\n\t                If set to true, it will be applied to all of the conv layers.\n\t        \"\"\"\n\t        super().__init__()\n", "        self.fft_size = fft_size\n\t        self.hop_size = hop_size\n\t        self.win_length = win_length\n\t        self.register_buffer(\"window\", getattr(torch, window)(win_length))\n\t        self.layers = nn.ModuleList()\n\t        # check kernel size is valid\n\t        assert len(kernel_sizes) == len(strides)\n\t        # add first layer\n\t        self.layers += [\n\t            nn.Sequential(\n", "                nn.Conv2d(\n\t                    1,\n\t                    channels,\n\t                    kernel_sizes[0],\n\t                    stride=strides[0],\n\t                    bias=bias,\n\t                ),\n\t                getattr(nn, nonlinear_activation)(**nonlinear_activation_params),\n\t            )\n\t        ]\n", "        for i in range(1, len(kernel_sizes) - 2):\n\t            self.layers += [\n\t                nn.Sequential(\n\t                    nn.Conv2d(\n\t                        channels,\n\t                        channels,\n\t                        kernel_size=kernel_sizes[i],\n\t                        stride=strides[i],\n\t                        bias=bias,\n\t                    ),\n", "                    getattr(nn, nonlinear_activation)(**nonlinear_activation_params),\n\t                )\n\t            ]\n\t        # add final layers\n\t        self.layers += [\n\t            nn.Sequential(\n\t                nn.Conv2d(\n\t                    channels,\n\t                    channels,\n\t                    kernel_size=kernel_sizes[-2],\n", "                    stride=strides[-2],\n\t                    bias=bias,\n\t                ),\n\t                getattr(nn, nonlinear_activation)(**nonlinear_activation_params),\n\t            )\n\t        ]\n\t        self.layers += [\n\t            nn.Conv2d(\n\t                channels,\n\t                1,\n", "                kernel_size=kernel_sizes[-1],\n\t                stride=strides[-1],\n\t                bias=bias,\n\t            )\n\t        ]\n\t        # apply weight norm\n\t        if use_weight_norm:\n\t            self.apply_weight_norm()\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n", "        Args:\n\t            x (Tensor): Input noise signal (B, 1, T).\n\t        Returns:\n\t            List: List of output tensors of each layer.\n\t        \"\"\"\n\t        x = spectrogram(\n\t            x,\n\t            pad=self.win_length // 2,\n\t            window=self.window,\n\t            n_fft=self.fft_size,\n", "            hop_length=self.hop_size,\n\t            win_length=self.win_length,\n\t            power=1.0,\n\t            normalized=False,\n\t        ).transpose(-1, -2)\n\t        for f in self.layers:\n\t            x = f(x)\n\t        return x\n\t    def apply_weight_norm(self):\n\t        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n", "        def _apply_weight_norm(m):\n\t            if isinstance(m, torch.nn.Conv2d):\n\t                torch.nn.utils.weight_norm(m)\n\t                logging.debug(f\"Weight norm is applied to {m}.\")\n\t        self.apply(_apply_weight_norm)\n\tclass UnivNetMultiResolutionSpectralDiscriminator(nn.Module):\n\t    \"\"\"UnivNet multi-resolution spectral discriminator module.\"\"\"\n\t    def __init__(\n\t        self,\n\t        fft_sizes=[1024, 2048, 512],\n", "        hop_sizes=[120, 240, 50],\n\t        win_lengths=[600, 1200, 240],\n\t        window=\"hann_window\",\n\t        discriminator_params={\n\t            \"channels\": 32,\n\t            \"kernel_sizes\": [(3, 9), (3, 9), (3, 9), (3, 9), (3, 3), (3, 3)],\n\t            \"strides\": [(1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1)],\n\t            \"bias\": True,\n\t            \"nonlinear_activation\": \"LeakyReLU\",\n\t            \"nonlinear_activation_params\": {\"negative_slope\": 0.2},\n", "        },\n\t    ):\n\t        \"\"\"Initilize UnivNetMultiResolutionSpectralDiscriminator module.\n\t        Args:\n\t            fft_sizes (list): FFT sizes for each spectral discriminator.\n\t            hop_sizes (list): Hop sizes for each spectral discriminator.\n\t            win_lengths (list): Window lengths for each spectral discriminator.\n\t            window (stt): Name of window function.\n\t            discriminator_params (dict): Parameters for univ-net spectral discriminator module.\n\t        \"\"\"\n", "        super().__init__()\n\t        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n\t        self.discriminators = nn.ModuleList()\n\t        # add discriminators\n\t        for i in range(len(fft_sizes)):\n\t            params = copy.deepcopy(discriminator_params)\n\t            self.discriminators += [\n\t                UnivNetSpectralDiscriminator(\n\t                    fft_size=fft_sizes[i],\n\t                    hop_size=hop_sizes[i],\n", "                    win_length=win_lengths[i],\n\t                    window=window,\n\t                    **params,\n\t                )\n\t            ]\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input noise signal (B, 1, T).\n\t        Returns:\n", "            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n\t        \"\"\"\n\t        outs = []\n\t        for f in self.discriminators:\n\t            out = f(x)\n\t            outs.append(out)\n\t        return outs"]}
{"filename": "models/vocoder/modules/residual_block.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t# Reference (https://github.com/r9y9/wavenet_vocoder)\n", "# Reference (https://github.com/jik876/hifi-gan)\n\t\"\"\"Residual block modules.\"\"\"\n\timport math\n\timport torch\n\timport torch.nn as nn\n\tfrom layers.conv_layer import CausalConv1d, Conv1d1x1   \n\tclass HiFiGANResidualBlock(nn.Module):\n\t    \"\"\"Causal Residual block module in HiFiGAN.\"\"\"\n\t    def __init__(\n\t        self,\n", "        kernel_size=3,\n\t        channels=512,\n\t        dilations=(1, 3, 5),\n\t        groups=1,\n\t        bias=True,\n\t        use_additional_convs=True,\n\t        nonlinear_activation=\"LeakyReLU\",\n\t        nonlinear_activation_params={\"negative_slope\": 0.1},\n\t    ):\n\t        \"\"\"Initialize CausalResidualBlock module.\n", "        Args:\n\t            kernel_size (int): Kernel size of dilation convolution layer.\n\t            channels (int): Number of channels for convolution layer.\n\t            dilations (List[int]): List of dilation factors.\n\t            use_additional_convs (bool): Whether to use additional convolution layers.\n\t            groups (int): The group number of conv1d (default: 1)\n\t            bias (bool): Whether to add bias parameter in convolution layers.\n\t            nonlinear_activation (str): Activation function module name.\n\t            nonlinear_activation_params (dict): Hyperparameters for activation function.\n\t        \"\"\"\n", "        super().__init__()\n\t        self.use_additional_convs = use_additional_convs\n\t        self.convs1 = nn.ModuleList()\n\t        if use_additional_convs:\n\t            self.convs2 = nn.ModuleList()\n\t        assert kernel_size % 2 == 1, \"Kernel size must be odd number.\"\n\t        self.activation = getattr(nn, nonlinear_activation)(**nonlinear_activation_params)\n\t        for dilation in dilations:\n\t            self.convs1 += [\n\t                CausalConv1d(\n", "                    in_channels=channels,\n\t                    out_channels=channels,\n\t                    kernel_size=kernel_size,\n\t                    stride=1,\n\t                    dilation=dilation,\n\t                    groups=groups,\n\t                    bias=bias,\n\t                )\n\t            ]\n\t            if use_additional_convs:\n", "                self.convs2 += [\n\t                    CausalConv1d(\n\t                        in_channels=channels,\n\t                        out_channels=channels,\n\t                        kernel_size=kernel_size,\n\t                        stride=1,\n\t                        dilation=1,\n\t                        groups=groups,\n\t                        bias=bias,\n\t                    )\n", "                ]\n\t        self.num_layer = len(self.convs1)\n\t    def forward(self, x):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            x (Tensor): Input tensor (B, channels, T).\n\t        Returns:\n\t            Tensor: Output tensor (B, channels, T).\n\t        \"\"\"\n\t        for idx in range(self.num_layer):\n", "            xt = self.convs1[idx](self.activation(x))\n\t            if self.use_additional_convs:\n\t                xt = self.convs2[idx](self.activation(xt))\n\t            x = xt + x\n\t        return x\n\t    def inference(self, x):\n\t        for idx in range(self.num_layer):\n\t            xt = self.convs1[idx].inference(self.activation(x))\n\t            if self.use_additional_convs:\n\t                xt = self.convs2[idx].inference(self.activation(xt))\n", "            x = xt + x\n\t        return x"]}
{"filename": "models/autoencoder/AudioDec.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"AudioDec model.\"\"\"\n", "import torch\n\timport inspect\n\tfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d\n\tfrom models.autoencoder.modules.encoder import Encoder\n\tfrom models.autoencoder.modules.decoder import Decoder\n\tfrom models.autoencoder.modules.projector import Projector\n\tfrom models.autoencoder.modules.quantizer import Quantizer\n\tfrom models.utils import check_mode\n\t### GENERATOR ###\n\tclass Generator(torch.nn.Module):\n", "    \"\"\"AudioDec generator.\"\"\"\n\t    def __init__(\n\t        self,\n\t        input_channels=1,\n\t        output_channels=1,\n\t        encode_channels=32,\n\t        decode_channels=32,\n\t        code_dim=64,\n\t        codebook_num=8,\n\t        codebook_size=1024,\n", "        bias=True,\n\t        enc_ratios=(2, 4, 8, 16),\n\t        dec_ratios=(16, 8, 4, 2),\n\t        enc_strides=(3, 4, 5, 5),\n\t        dec_strides=(5, 5, 4, 3),\n\t        mode='causal',\n\t        codec='audiodec',\n\t        projector='conv1d',\n\t        quantier='residual_vq',\n\t    ):\n", "        super().__init__()\n\t        if codec == 'audiodec':\n\t            encoder = Encoder\n\t            decoder = Decoder\n\t        else:\n\t            raise NotImplementedError(f\"Codec ({codec}) is not supported!\")\n\t        self.mode = mode\n\t        self.input_channels = input_channels\n\t        self.encoder = encoder(\n\t            input_channels=input_channels,\n", "            encode_channels=encode_channels,\n\t            channel_ratios=enc_ratios,\n\t            strides=enc_strides,\n\t            kernel_size=7,\n\t            bias=bias,\n\t            mode=self.mode,\n\t        )\n\t        self.decoder = decoder(\n\t            code_dim=code_dim,\n\t            output_channels=output_channels,\n", "            decode_channels=decode_channels,\n\t            channel_ratios=dec_ratios,\n\t            strides=dec_strides,\n\t            kernel_size=7,\n\t            bias=bias,\n\t            mode=self.mode,\n\t        )\n\t        self.projector = Projector(\n\t            input_channels=self.encoder.out_channels,\n\t            code_dim=code_dim,\n", "            kernel_size=3,\n\t            stride=1,\n\t            bias=False,\n\t            mode=self.mode,\n\t            model=projector,\n\t        )\n\t        self.quantizer = Quantizer(\n\t            code_dim=code_dim,\n\t            codebook_num=codebook_num,\n\t            codebook_size=codebook_size,\n", "            model=quantier,\n\t        )\n\t    def forward(self, x):\n\t        (batch, channel, length) = x.size()\n\t        if channel != self.input_channels: \n\t            x = x.reshape(-1, self.input_channels, length) # (B, C, T) -> (B', C', T)\n\t        x = self.encoder(x)\n\t        z = self.projector(x)\n\t        zq, vqloss, perplexity = self.quantizer(z)\n\t        y = self.decoder(zq)\n", "        return y, zq, z, vqloss, perplexity\n\t# STREAMING\n\tclass StreamGenerator(Generator):\n\t    \"\"\"AudioDec streaming generator.\"\"\"\n\t    def __init__(\n\t        self,\n\t        input_channels=1,\n\t        output_channels=1,\n\t        encode_channels=32,\n\t        decode_channels=32,\n", "        code_dim=64,\n\t        codebook_num=8,\n\t        codebook_size=1024,\n\t        bias=True,\n\t        enc_ratios=(2, 4, 8, 16),\n\t        dec_ratios=(16, 8, 4, 2),\n\t        enc_strides=(3, 4, 5, 5),\n\t        dec_strides=(5, 5, 4, 3),\n\t        mode='causal',\n\t        codec='audiodec',\n", "        projector='conv1d',\n\t        quantier='residual_vq',\n\t    ):\n\t        super(StreamGenerator, self).__init__(\n\t            input_channels=input_channels,\n\t            output_channels=output_channels,\n\t            encode_channels=encode_channels,\n\t            decode_channels=decode_channels,\n\t            code_dim=code_dim,\n\t            codebook_num=codebook_num,\n", "            codebook_size=codebook_size,\n\t            bias=bias,\n\t            enc_ratios=enc_ratios,\n\t            dec_ratios=dec_ratios,\n\t            enc_strides=enc_strides,\n\t            dec_strides=dec_strides,\n\t            mode=mode,\n\t            codec=codec,\n\t            projector=projector,\n\t            quantier=quantier,\n", "        )\n\t        check_mode(mode, \"AudioDec Streamer\")\n\t        self.reset_buffer()\n\t    def initial_encoder(self, receptive_length, device):\n\t        self.quantizer.initial()\n\t        z = self.encode(torch.zeros(1, self.input_channels, receptive_length).to(device))\n\t        idx = self.quantize(z)\n\t        zq = self.lookup(idx)\n\t        return zq\n\t    def initial_decoder(self, zq):\n", "        self.decode(zq)\n\t    def encode(self, x):\n\t        (batch, channel, length) = x.size()\n\t        if channel != self.input_channels: \n\t            x = x.reshape(-1, self.input_channels, length) # (B, C, T) -> (B', C', T)\n\t        x = self.encoder.encode(x)\n\t        z = self.projector.encode(x)\n\t        return z\n\t    def quantize(self, z):\n\t        zq, idx = self.quantizer.encode(z)\n", "        return idx\n\t    def lookup(self, idx):\n\t        return self.quantizer.decode(idx)\n\t    def decode(self, zq):\n\t        return self.decoder.decode(zq.transpose(2, 1))\n\t    def reset_buffer(self):\n\t        \"\"\"Apply weight normalization module from all layers.\"\"\"\n\t        def _reset_buffer(m):\n\t            if isinstance(m, CausalConv1d) or isinstance(m, CausalConvTranspose1d):\n\t                m.reset_buffer()\n", "        self.apply(_reset_buffer)\n"]}
{"filename": "models/autoencoder/modules/decoder.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://ieeexplore.ieee.org/document/9625818)\n\t\"\"\"Decoder modules.\"\"\"\n", "import torch\n\timport inspect\n\tfrom layers.conv_layer import NonCausalConv1d, NonCausalConvTranspose1d\n\tfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d\n\tfrom models.autoencoder.modules.residual_unit import NonCausalResidualUnit\n\tfrom models.autoencoder.modules.residual_unit import CausalResidualUnit\n\tfrom models.utils import check_mode\n\tclass DecoderBlock(torch.nn.Module):\n\t    \"\"\" Decoder block (upsampling) \"\"\"\n\t    def __init__(\n", "        self,\n\t        in_channels,\n\t        out_channels,\n\t        stride,\n\t        dilations=(1, 3, 9),\n\t        bias=True,\n\t        mode='causal',\n\t    ):\n\t        super().__init__()\n\t        self.mode = mode\n", "        if self.mode == 'noncausal':\n\t            ResidualUnit = NonCausalResidualUnit\n\t            ConvTranspose1d = NonCausalConvTranspose1d\n\t        elif self.mode == 'causal':\n\t            ResidualUnit = CausalResidualUnit\n\t            ConvTranspose1d = CausalConvTranspose1d\n\t        else:\n\t            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\t        self.conv = ConvTranspose1d(\n\t            in_channels=in_channels,\n", "            out_channels=out_channels,\n\t            kernel_size=(2 * stride),\n\t            stride=stride,\n\t            bias=bias,\n\t        )\n\t        self.res_units = torch.nn.ModuleList()\n\t        for idx, dilation in enumerate(dilations):\n\t            self.res_units += [\n\t                ResidualUnit(out_channels, out_channels, dilation=dilation)]\n\t        self.num_res = len(self.res_units)\n", "    def forward(self, x):\n\t        x = self.conv(x)\n\t        for idx in range(self.num_res):\n\t            x = self.res_units[idx](x)\n\t        return x\n\t    def inference(self, x):\n\t        check_mode(self.mode, inspect.stack()[0][3])\n\t        x = self.conv.inference(x)\n\t        for idx in range(self.num_res):\n\t            x = self.res_units[idx].inference(x)\n", "        return x\n\tclass Decoder(torch.nn.Module):\n\t    def __init__(self,\n\t        code_dim, \n\t        output_channels,\n\t        decode_channels,\n\t        channel_ratios=(16, 8, 4, 2),\n\t        strides=(5, 5, 4, 3),\n\t        kernel_size=7,\n\t        bias=True,\n", "        mode='causal',\n\t    ):\n\t        super().__init__()\n\t        assert len(channel_ratios) == len(strides)\n\t        self.mode = mode\n\t        if self.mode == 'noncausal':\n\t            Conv1d = NonCausalConv1d\n\t        elif self.mode == 'causal':\n\t            Conv1d = CausalConv1d\n\t        else:\n", "            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\t        self.conv1 = Conv1d(\n\t            in_channels=code_dim, \n\t            out_channels=(decode_channels * channel_ratios[0]), \n\t            kernel_size=kernel_size, \n\t            stride=1, \n\t            bias=False)\n\t        self.conv_blocks = torch.nn.ModuleList()\n\t        for idx, stride in enumerate(strides):\n\t            in_channels = decode_channels * channel_ratios[idx]\n", "            if idx < (len(channel_ratios)-1):\n\t                out_channels = decode_channels * channel_ratios[idx+1]\n\t            else:\n\t                out_channels = decode_channels\n\t            self.conv_blocks += [\n\t                DecoderBlock(in_channels, out_channels, stride, bias=bias, mode=self.mode)]\n\t        self.num_blocks = len(self.conv_blocks)\n\t        self.conv2 = Conv1d(out_channels, output_channels, kernel_size, 1, bias=False)\n\t    def forward(self, z):\n\t        x = self.conv1(z)\n", "        for i in range(self.num_blocks):\n\t            x = self.conv_blocks[i](x)\n\t        x = self.conv2(x)\n\t        return x\n\t    def decode(self, z):\n\t        check_mode(self.mode, inspect.stack()[0][3])\n\t        x = self.conv1.inference(z)\n\t        for i in range(self.num_blocks):\n\t            x = self.conv_blocks[i].inference(x)\n\t        x = self.conv2.inference(x)\n", "        return x"]}
{"filename": "models/autoencoder/modules/residual_unit.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://ieeexplore.ieee.org/document/9625818)\n\t\"\"\"Residual Units.\"\"\"\n", "import torch\n\timport torch.nn as nn\n\tfrom layers.conv_layer import Conv1d1x1, NonCausalConv1d, CausalConv1d\n\tclass NonCausalResidualUnit(nn.Module):\n\t    def __init__(\n\t        self, \n\t        in_channels, \n\t        out_channels, \n\t        kernel_size=7,\n\t        dilation=1,\n", "        bias=False,\n\t        nonlinear_activation=\"ELU\",\n\t        nonlinear_activation_params={}, \n\t    ):\n\t        super().__init__()\n\t        self.activation = getattr(nn, nonlinear_activation)(**nonlinear_activation_params)\n\t        self.conv1 = NonCausalConv1d(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n", "            stride=1,\n\t            dilation=dilation,\n\t            bias=bias,\n\t        )\n\t        self.conv2 = Conv1d1x1(out_channels, out_channels, bias)\n\t    def forward(self, x):\n\t        y = self.conv1(self.activation(x))\n\t        y = self.conv2(self.activation(y))\n\t        return x + y\n\tclass CausalResidualUnit(NonCausalResidualUnit):\n", "    def __init__(\n\t        self, \n\t        in_channels, \n\t        out_channels, \n\t        kernel_size=7,\n\t        dilation=1,\n\t        bias=False,\n\t        nonlinear_activation=\"ELU\",\n\t        nonlinear_activation_params={}, \n\t    ):\n", "        super(CausalResidualUnit, self).__init__(\n\t            in_channels=in_channels, \n\t            out_channels=out_channels, \n\t            kernel_size=kernel_size, \n\t            dilation=dilation, \n\t            bias=bias,\n\t            nonlinear_activation=nonlinear_activation,\n\t            nonlinear_activation_params=nonlinear_activation_params, \n\t        )\n\t        self.conv1 = CausalConv1d(\n", "            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n\t            stride=1,\n\t            dilation=dilation,\n\t            bias=bias,\n\t        )\n\t    def inference(self, x):\n\t        y = self.conv1.inference(self.activation(x))\n\t        y = self.conv2(self.activation(y))\n", "        return x + y"]}
{"filename": "models/autoencoder/modules/encoder.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://ieeexplore.ieee.org/document/9625818)\n\t\"\"\"Encoder modules.\"\"\"\n", "import torch\n\timport inspect\n\tfrom layers.conv_layer import NonCausalConv1d\n\tfrom layers.conv_layer import CausalConv1d\n\tfrom models.autoencoder.modules.residual_unit import NonCausalResidualUnit\n\tfrom models.autoencoder.modules.residual_unit import CausalResidualUnit\n\tfrom models.utils import check_mode\n\tclass EncoderBlock(torch.nn.Module):\n\t    \"\"\" Encoder block (downsampling) \"\"\"\n\t    def __init__(\n", "        self,\n\t        in_channels,\n\t        out_channels,\n\t        stride,\n\t        dilations=(1, 3, 9),\n\t        bias=True,\n\t        mode='causal',\n\t    ):\n\t        super().__init__()\n\t        self.mode = mode\n", "        if self.mode == 'noncausal':\n\t            ResidualUnit = NonCausalResidualUnit\n\t            Conv1d = NonCausalConv1d\n\t        elif self.mode == 'causal':\n\t            ResidualUnit = CausalResidualUnit\n\t            Conv1d = CausalConv1d\n\t        else:\n\t            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\t        self.res_units = torch.nn.ModuleList()\n\t        for dilation in dilations:\n", "            self.res_units += [\n\t                ResidualUnit(in_channels, in_channels, dilation=dilation)]\n\t        self.num_res = len(self.res_units)\n\t        self.conv = Conv1d(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=(2 * stride),\n\t            stride=stride,\n\t            bias=bias,\n\t        )\n", "    def forward(self, x):\n\t        for idx in range(self.num_res):\n\t            x = self.res_units[idx](x)\n\t        x = self.conv(x)\n\t        return x\n\t    def inference(self, x):\n\t        check_mode(self.mode, inspect.stack()[0][3])\n\t        for idx in range(self.num_res):\n\t            x = self.res_units[idx].inference(x)\n\t        x = self.conv.inference(x)\n", "        return x\n\tclass Encoder(torch.nn.Module):\n\t    def __init__(self,\n\t        input_channels,\n\t        encode_channels,\n\t        channel_ratios=(2, 4, 8, 16),\n\t        strides=(3, 4, 5, 5),\n\t        kernel_size=7,\n\t        bias=True,\n\t        mode='causal',\n", "    ):\n\t        super().__init__()\n\t        assert len(channel_ratios) == len(strides)\n\t        self.mode = mode\n\t        if self.mode == 'noncausal':\n\t            Conv1d = NonCausalConv1d\n\t        elif self.mode == 'causal':\n\t            Conv1d = CausalConv1d\n\t        else:\n\t            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n", "        self.conv = Conv1d(\n\t            in_channels=input_channels, \n\t            out_channels=encode_channels, \n\t            kernel_size=kernel_size, \n\t            stride=1, \n\t            bias=False)\n\t        self.conv_blocks = torch.nn.ModuleList()\n\t        in_channels = encode_channels\n\t        for idx, stride in enumerate(strides):\n\t            out_channels = encode_channels * channel_ratios[idx]\n", "            self.conv_blocks += [\n\t                EncoderBlock(in_channels, out_channels, stride, bias=bias, mode=self.mode)]\n\t            in_channels = out_channels\n\t        self.num_blocks = len(self.conv_blocks)\n\t        self.out_channels = out_channels\n\t    def forward(self, x): \n\t        x = self.conv(x)\n\t        for i in range(self.num_blocks):\n\t            x = self.conv_blocks[i](x)\n\t        return x\n", "    def encode(self, x):\n\t        check_mode(self.mode, inspect.stack()[0][3])\n\t        x = self.conv.inference(x)\n\t        for i in range(self.num_blocks):\n\t            x = self.conv_blocks[i].inference(x)\n\t        return x\n"]}
{"filename": "models/autoencoder/modules/quantizer.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport torch\n\tfrom layers.vq_module import ResidualVQ\n\tclass Quantizer(torch.nn.Module):\n", "    def __init__(self,\n\t        code_dim,\n\t        codebook_num,\n\t        codebook_size,\n\t        model='residual_vq',\n\t        ):\n\t        super().__init__()\n\t        # speech\n\t        if model == 'residual_vq':\n\t            self.codebook = ResidualVQ(dim=code_dim, num_quantizers=codebook_num, codebook_size=codebook_size)\n", "        else:\n\t            raise NotImplementedError(f\"Model ({model}) is not supported!\")\n\t    def initial(self):\n\t        self.codebook.initial()    \n\t    def forward(self, z):\n\t        zq, vqloss, perplexity = self.codebook(z.transpose(2, 1))\n\t        zq = zq.transpose(2, 1)        \n\t        return zq, vqloss, perplexity\n\t    def inference(self, z):  \n\t        zq, indices = self.codebook.forward_index(z.transpose(2, 1))\n", "        zq = zq.transpose(2, 1)\n\t        return zq, indices\n\t    def encode(self, z):  \n\t        zq, indices = self.codebook.forward_index(z.transpose(2, 1), flatten_idx=True)\n\t        return zq, indices\n\t    def decode(self, indices):  \n\t        z = self.codebook.lookup(indices)\n\t        return z\n"]}
{"filename": "models/autoencoder/modules/projector.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Projector modules.\"\"\"\n\timport torch\n\timport inspect\n", "from layers.conv_layer import NonCausalConv1d\n\tfrom layers.conv_layer import CausalConv1d\n\tfrom models.utils import check_mode\n\tclass Projector(torch.nn.Module):\n\t    def __init__(self,\n\t        input_channels,\n\t        code_dim, \n\t        kernel_size=3,\n\t        stride=1,\n\t        bias=False,\n", "        mode='causal',\n\t        model='conv1d',\n\t    ):\n\t        super().__init__()\n\t        self.mode = mode\n\t        if self.mode == 'noncausal':\n\t            Conv1d = NonCausalConv1d\n\t        elif self.mode == 'causal':\n\t            Conv1d = CausalConv1d\n\t        else:\n", "            raise NotImplementedError(f\"Mode ({mode}) is not supported!\")\n\t        if model == 'conv1d':\n\t            self.project = Conv1d(input_channels, code_dim, kernel_size=kernel_size, stride=stride, bias=bias)\n\t        elif model == 'conv1d_bn':\n\t            self.project = torch.nn.Sequential(\n\t                Conv1d(input_channels, code_dim, kernel_size=kernel_size, stride=stride, bias=bias),\n\t                torch.nn.BatchNorm1d(code_dim)\n\t            )\n\t        else:\n\t            raise NotImplementedError(f\"Model ({model}) is not supported!\")\n", "    def forward(self, x): \n\t        return self.project(x)\n\t    def encode(self, x):\n\t        check_mode(self.mode, inspect.stack()[0][3])\n\t        return self.project.inference(x)\n"]}
{"filename": "dataloader/collater.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Customized collater modules for Pytorch DataLoader.\"\"\"\n", "import torch\n\timport numpy as np\n\tclass CollaterAudio(object):\n\t    \"\"\"Customized collater for loading single audio.\"\"\"\n\t    def __init__(\n\t        self,\n\t        batch_length=9600,\n\t    ):\n\t        \"\"\"\n\t        Args:\n", "            batch_length (int): The length of audio signal batch.\n\t        \"\"\"\n\t        self.batch_length = batch_length\n\t    def __call__(self, batch):\n\t        # filter short batch\n\t        xs = [b for b in batch if len(b) > self.batch_length]\n\t        # random cut\n\t        starts, ends = self._random_segment(xs)\n\t        x_batch = self._cut(xs, starts, ends)\n\t        return x_batch\n", "    def _random_segment(self, xs):\n\t        x_lengths = [len(x) for x in xs]\n\t        start_offsets = np.array(\n\t            [\n\t                np.random.randint(0, xl - self.batch_length)\n\t                for xl in x_lengths\n\t            ]\n\t        )\n\t        starts = start_offsets\n\t        ends = starts + self.batch_length\n", "        return starts, ends\n\t    def _cut(self, xs, starts, ends):\n\t        x_batch = np.array([x[start:end] for x, start, end in zip(xs, starts, ends)])\n\t        x_batch = torch.tensor(x_batch, dtype=torch.float).transpose(2, 1)  # (B, C, T)\n\t        return x_batch\n\tclass CollaterAudioPair(CollaterAudio):\n\t    \"\"\"Customized collater for loading audio pair.\"\"\"\n\t    def __init__(\n\t        self,\n\t        batch_length=9600,\n", "    ):\n\t        super().__init__(\n\t            batch_length=batch_length\n\t        )\n\t    def __call__(self, batch):\n\t        batch = [\n\t            b for b in batch if (len(b[0]) > self.batch_length) and (len(b[0]) == len(b[1]))\n\t        ]\n\t        assert len(batch) > 0, f\"No qualified audio pairs.!\"\n\t        xs, ns = [b[0] for b in batch], [b[1] for b in batch]\n", "        # random cut\n\t        starts, ends = self._random_segment(xs)\n\t        x_batch = self._cut(xs, starts, ends)\n\t        n_batch = self._cut(ns, starts, ends)\n\t        return n_batch, x_batch # (input, output)\n"]}
{"filename": "dataloader/dataset.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"PyTorch compatible dataset modules.\"\"\"\n", "import os\n\timport soundfile as sf\n\tfrom torch.utils.data import Dataset\n\tfrom dataloader.utils import find_files\n\tclass SingleDataset(Dataset):\n\t    def __init__(\n\t        self,\n\t        files,\n\t        query=\"*.wav\",\n\t        load_fn=sf.read,\n", "        return_utt_id=False,\n\t        subset_num=-1,\n\t    ):\n\t        self.return_utt_id = return_utt_id\n\t        self.load_fn = load_fn\n\t        self.subset_num = subset_num\n\t        self.filenames = self._load_list(files, query)\n\t        self.utt_ids = self._load_ids(self.filenames)\n\t    def __getitem__(self, idx):\n\t        utt_id = self.utt_ids[idx]\n", "        data = self._data(idx)\n\t        if self.return_utt_id:\n\t            items = utt_id, data\n\t        else:\n\t            items = data\n\t        return items\n\t    def __len__(self):\n\t        return len(self.filenames)\n\t    def _read_list(self, listfile):\n\t        filenames = []\n", "        with open(listfile) as f:\n\t            for line in f:\n\t                line = line.strip()\n\t                if len(line):\n\t                    filenames.append(line)\n\t        return filenames\n\t    def _load_list(self, files, query):\n\t        if isinstance(files, list):\n\t            filenames = files\n\t        else:\n", "            if os.path.isdir(files):\n\t                filenames = sorted(find_files(files, query))\n\t            elif os.path.isfile(files):\n\t                filenames = sorted(self._read_list(files))\n\t            else:\n\t                raise ValueError(f\"{files} is not a list / existing folder or file!\")\n\t        if self.subset_num > 0:\n\t            filenames = filenames[:self.subset_num]\n\t        assert len(filenames) != 0, f\"File list in empty!\"\n\t        return filenames\n", "    def _load_ids(self, filenames):\n\t        utt_ids = [\n\t            os.path.splitext(os.path.basename(f))[0] for f in filenames\n\t        ]\n\t        return utt_ids\n\t    def _data(self, idx):\n\t        return self._load_data(self.filenames[idx], self.load_fn)\n\t    def _load_data(self, filename, load_fn):\n\t        if load_fn == sf.read:\n\t            data, _ = load_fn(filename, always_2d=True) # (T, C)\n", "        else:\n\t            data = load_fn(filename)\n\t        return data\n\tclass MultiDataset(SingleDataset):\n\t    def __init__(\n\t        self,\n\t        multi_files,\n\t        queries,\n\t        load_fns,\n\t        return_utt_id=False,\n", "        subset_num=-1,\n\t    ):\n\t        errmsg = f\"multi_files({len(multi_files)}), queries({len(queries)}), and load_fns({len(load_fns)}) are length mismatched!\"\n\t        assert len(multi_files) == len(queries) == len(load_fns), errmsg\n\t        super(MultiDataset, self).__init__(\n\t            files=multi_files,\n\t            query=queries,\n\t            load_fn=load_fns,\n\t            return_utt_id=return_utt_id,\n\t            subset_num=subset_num,\n", "        )\n\t        self._check_length(self.filenames)\n\t    def _load_list(self, multi_files, queries):\n\t        multi_filenames = []\n\t        if isinstance(multi_files, list):\n\t            for files, query in zip(multi_files, queries):\n\t                multi_filenames.append(super()._load_list(files, query))\n\t        else:\n\t            raise ValueError(f\"{multi_files} should be a list!\")\n\t        return multi_filenames\n", "    def _load_ids(self, multi_filenames):\n\t        return super()._load_ids(multi_filenames[0])\n\t    def _data(self, idx):\n\t        filenames = [\n\t            f[idx] for f in self.filenames\n\t        ]\n\t        data = []\n\t        for filename, load_fn in zip(filenames, self.load_fn):\n\t            data.append(self._load_data(filename, load_fn))\n\t        return data\n", "    def _check_length(self, multi_filenames):\n\t        errmsg = f\"Not all lists have the same number of files!\"\n\t        self.file_num = len(multi_filenames[0])\n\t        assert all(len(x)==self.file_num for x in multi_filenames), errmsg\n\t    def __len__(self):\n\t        return self.file_num\n"]}
{"filename": "dataloader/__init__.py", "chunked_list": ["from .dataset import *  # NOQA\n\tfrom .collater import *  # NOQA\n\tfrom .utils import * # NOQA"]}
{"filename": "dataloader/utils.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\timport os\n", "import fnmatch\n\timport logging\n\timport numpy as np\n\tdef find_files(root_dir, query=\"*.wav\", include_root_dir=True):\n\t    \"\"\"Find files recursively.\n\t        Args:\n\t            root_dir (str): Root root_dir to find.\n\t            query (str): Query to find.\n\t            include_root_dir (bool): If False, root_dir name is not included.\n\t        Returns:\n", "            list: List of found filenames.\n\t    \"\"\"\n\t    files = []\n\t    for root, dirnames, filenames in os.walk(root_dir, followlinks=True):\n\t        for filename in fnmatch.filter(filenames, query):\n\t            files.append(os.path.join(root, filename))\n\t    if not include_root_dir:\n\t        files = [file_.replace(root_dir + \"/\", \"\") for file_ in files]\n\t    return files\n\tdef load_files(data_path, query=\"*.wav\", num_core=40):\n", "    # sort all files    \n\t    file_list = sorted(find_files(data_path, query))\n\t    logging.info(f\"The number of {os.path.basename(data_path)} files = {len(file_list)}.\")\n\t    # divide\n\t    if num_core < len(file_list):\n\t        file_lists = np.array_split(file_list, num_core)\n\t        file_lists = [f_list.tolist() for f_list in file_lists]\n\t    else:\n\t        file_lists = [file_list]\n\t    return file_lists"]}
{"filename": "trainer/vocoder.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Training flow of GAN-based vocoder.\"\"\"\n", "import logging\n\timport torch\n\tfrom trainer.trainerGAN import TrainerGAN\n\tclass Trainer(TrainerGAN):\n\t    def __init__(\n\t        self,\n\t        steps,\n\t        epochs,\n\t        data_loader,\n\t        model,\n", "        criterion,\n\t        optimizer,\n\t        scheduler,\n\t        config,\n\t        device=torch.device(\"cpu\"),\n\t    ):\n\t        super(Trainer, self).__init__(\n\t           steps=steps,\n\t           epochs=epochs,\n\t           data_loader=data_loader,\n", "           model=model,\n\t           criterion=criterion,\n\t           optimizer=optimizer,\n\t           scheduler=scheduler,\n\t           config=config,\n\t           device=device,\n\t        )\n\t        self.fix_analyzer = False\n\t        self.generator_start = config.get(\"generator_train_start_steps\", 0)\n\t        self.discriminator_start = config.get(\"discriminator_train_start_steps\", 0)\n", "    def _train_step(self, batch):\n\t        \"\"\"Train model one step.\"\"\"\n\t        mode = 'train'\n\t        x = batch\n\t        x = x.to(self.device)\n\t        # fix analyzer\n\t        if not self.fix_analyzer:    \n\t            for parameter in self.model[\"analyzer\"].parameters():\n\t                parameter.requires_grad = False\n\t            self.fix_analyzer = True\n", "            logging.info(\"Analyzer is fixed!\")\n\t        self.model[\"analyzer\"].eval()\n\t        #######################\n\t        #      Generator      #\n\t        #######################\n\t        if self.steps > self.generator_start:\n\t            # initialize generator loss\n\t            gen_loss = 0.0\n\t            # main genertor operation\n\t            e = self.model[\"analyzer\"].encoder(x)\n", "            z = self.model[\"analyzer\"].projector(e)\n\t            zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n\t            y_ = self.model[\"generator\"](zq)\n\t            # metric loss\n\t            gen_loss += self._metric_loss(y_, x, mode=mode)\n\t            # adversarial loss\n\t            if self.steps > self.discriminator_start:\n\t                p_ = self.model[\"discriminator\"](y_)\n\t                if self.config[\"use_feat_match_loss\"]:\n\t                    with torch.no_grad():\n", "                        p = self.model[\"discriminator\"](x)\n\t                else:\n\t                    p = None\n\t                gen_loss += self._adv_loss(p_, p, mode=mode)\n\t            # update generator\n\t            self._record_loss('generator_loss', gen_loss, mode=mode)\n\t            self._update_generator(gen_loss)\n\t        #######################\n\t        #    Discriminator    #\n\t        #######################\n", "        if self.steps > self.discriminator_start:\n\t            # re-compute y_ which leads better quality\n\t            with torch.no_grad():\n\t                e = self.model[\"analyzer\"].encoder(x)\n\t                z = self.model[\"analyzer\"].projector(e)\n\t                zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n\t                y_ = self.model[\"generator\"](zq)\n\t            p = self.model[\"discriminator\"](x)\n\t            p_ = self.model[\"discriminator\"](y_.detach())\n\t            # discriminator loss & update discriminator\n", "            self._update_discriminator(self._dis_loss(p_, p, mode=mode))\n\t        # update counts\n\t        self.steps += 1\n\t        self.tqdm.update(1)\n\t        self._check_train_finish()\n\t    @torch.no_grad()\n\t    def _eval_step(self, batch):\n\t        \"\"\"Single step of evaluation.\"\"\"\n\t        mode = 'eval'\n\t        x = batch\n", "        x = x.to(self.device)\n\t        # initialize generator loss\n\t        gen_loss = 0.0\n\t        # main genertor operation\n\t        e = self.model[\"analyzer\"].encoder(x)\n\t        z = self.model[\"analyzer\"].projector(e)\n\t        zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n\t        y_ = self.model[\"generator\"](zq)\n\t        # metric loss\n\t        gen_loss += self._metric_loss(y_, x, mode=mode)\n", "        # adversarial loss & feature matching loss\n\t        if self.steps > self.discriminator_start:\n\t            p_ = self.model[\"discriminator\"](y_)\n\t            if self.config[\"use_feat_match_loss\"]:\n\t                p = self.model[\"discriminator\"](x)\n\t            else:\n\t                p = None\n\t            gen_loss += self._adv_loss(p_, p, mode=mode)\n\t            # discriminator loss\n\t            self._dis_loss(p_, p, mode=mode)\n", "        # generator loss\n\t        self._record_loss('generator_loss', gen_loss, mode=mode)\n"]}
{"filename": "trainer/trainerGAN.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Template GAN training flow.\"\"\"\n", "import logging\n\timport os\n\timport abc\n\timport torch\n\tfrom collections import defaultdict\n\tfrom tensorboardX import SummaryWriter\n\tfrom tqdm import tqdm\n\tclass TrainerGAN(abc.ABC):\n\t    def __init__(\n\t        self,\n", "        steps,\n\t        epochs,\n\t        data_loader,\n\t        model,\n\t        criterion,\n\t        optimizer,\n\t        scheduler,\n\t        config,\n\t        device=torch.device(\"cpu\"),\n\t    ):\n", "        \"\"\"Initialize trainer.\n\t        Args:\n\t            steps (int): Initial global steps.\n\t            epochs (int): Initial global epochs.\n\t            data_loader (dict): Dict of data loaders. It must contrain \"train\" and \"dev\" loaders.\n\t            model (dict): Dict of models. It must contrain \"generator\" and \"discriminator\" models.\n\t            criterion (dict): Dict of criterions. It must contrain \"stft\" and \"mse\" criterions.\n\t            optimizer (dict): Dict of optimizers. It must contrain \"generator\" and \"discriminator\" optimizers.\n\t            scheduler (dict): Dict of schedulers. It must contrain \"generator\" and \"discriminator\" schedulers.\n\t            config (dict): Config dict loaded from yaml format configuration file.\n", "            device (torch.deive): Pytorch device instance.\n\t        \"\"\"\n\t        self.steps = steps\n\t        self.epochs = epochs\n\t        self.data_loader = data_loader\n\t        self.model = model\n\t        self.criterion = criterion\n\t        self.optimizer = optimizer\n\t        self.scheduler = scheduler\n\t        self.config = config\n", "        self.device = device\n\t        self.writer = SummaryWriter(config[\"outdir\"])\n\t        self.total_train_loss = defaultdict(float)\n\t        self.total_eval_loss = defaultdict(float)\n\t        self.train_max_steps = config.get(\"train_max_steps\", 0)\n\t    @abc.abstractmethod\n\t    def _train_step(self, batch):\n\t        \"\"\"Single step of training.\"\"\"\n\t        pass\n\t    @abc.abstractmethod\n", "    def _eval_step(self, batch):\n\t        \"\"\"Single step of evaluation.\"\"\"\n\t        pass\n\t    def run(self):\n\t        \"\"\"Run training.\"\"\"\n\t        self.finish_train = False\n\t        self.tqdm = tqdm(\n\t            initial=self.steps, total=self.train_max_steps, desc=\"[train]\"\n\t        )\n\t        while True:\n", "            self._train_epoch()\n\t            # check whether training is finished\n\t            if self.finish_train:\n\t                break\n\t        self.tqdm.close()\n\t        logging.info(\"Finished training.\")\n\t    def save_checkpoint(self, checkpoint_path):\n\t        \"\"\"Save checkpoint.\n\t        Args:\n\t            checkpoint_path (str): Checkpoint path to be saved.\n", "        \"\"\"\n\t        state_dict = {\n\t            \"optimizer\": {\n\t                \"generator\": self.optimizer[\"generator\"].state_dict(),\n\t                \"discriminator\": self.optimizer[\"discriminator\"].state_dict(),\n\t            },\n\t            \"scheduler\": {\n\t                \"generator\": self.scheduler[\"generator\"].state_dict(),\n\t                \"discriminator\": self.scheduler[\"discriminator\"].state_dict(),\n\t            },\n", "            \"steps\": self.steps,\n\t            \"epochs\": self.epochs,\n\t        }\n\t        state_dict[\"model\"] = {\n\t            \"generator\": self.model[\"generator\"].state_dict(),\n\t            \"discriminator\": self.model[\"discriminator\"].state_dict(),\n\t        }\n\t        if not os.path.exists(os.path.dirname(checkpoint_path)):\n\t            os.makedirs(os.path.dirname(checkpoint_path))\n\t        torch.save(state_dict, checkpoint_path)\n", "    def load_checkpoint(self, checkpoint_path, strict=True, load_only_params=False, load_discriminator=True):\n\t        \"\"\"Load checkpoint.\n\t        Args:\n\t            checkpoint_path (str): Checkpoint path to be loaded.\n\t            load_only_params (bool): Whether to load only model parameters.\n\t            load_discriminator (bool): Whether to load optimizer and scheduler of the discriminators.\n\t        \"\"\"\n\t        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n\t        self.model[\"generator\"].load_state_dict(\n\t            state_dict[\"model\"][\"generator\"], strict=strict)\n", "        self.model[\"discriminator\"].load_state_dict(\n\t            state_dict[\"model\"][\"discriminator\"], strict=strict)\n\t        if not load_only_params:\n\t            self.steps = state_dict[\"steps\"]\n\t            self.epochs = state_dict[\"epochs\"]\n\t            self.optimizer[\"generator\"].load_state_dict(\n\t                state_dict[\"optimizer\"][\"generator\"])\n\t            self.scheduler[\"generator\"].load_state_dict(\n\t                state_dict[\"scheduler\"][\"generator\"])\n\t            if load_discriminator:\n", "                self.optimizer[\"discriminator\"].load_state_dict(\n\t                    state_dict[\"optimizer\"][\"discriminator\"])\n\t                self.scheduler[\"discriminator\"].load_state_dict(\n\t                    state_dict[\"scheduler\"][\"discriminator\"])\n\t    def _train_epoch(self):\n\t        \"\"\"One epoch of training.\"\"\"\n\t        for train_steps_per_epoch, batch in enumerate(self.data_loader[\"train\"], 1):\n\t            # train one step\n\t            self._train_step(batch)\n\t            # check interval\n", "            self._check_log_interval()\n\t            self._check_eval_interval()\n\t            self._check_save_interval()\n\t            # check whether training is finished\n\t            if self.finish_train:\n\t                return\n\t        # update\n\t        self.epochs += 1\n\t        self.train_steps_per_epoch = train_steps_per_epoch\n\t        if train_steps_per_epoch > 200:\n", "            logging.info(\n\t                f\"(Steps: {self.steps}) Finished {self.epochs} epoch training \"\n\t                f\"({self.train_steps_per_epoch} steps per epoch).\"\n\t            )\n\t    def _eval_epoch(self):\n\t        \"\"\"One epoch of evaluation.\"\"\"\n\t        logging.info(f\"(Steps: {self.steps}) Start evaluation.\")\n\t        # change mode\n\t        for key in self.model.keys():\n\t            self.model[key].eval()\n", "        # calculate loss for each batch\n\t        for eval_steps_per_epoch, batch in enumerate(\n\t            tqdm(self.data_loader[\"dev\"], desc=\"[eval]\"), 1\n\t        ):\n\t            # eval one step\n\t            self._eval_step(batch)\n\t        logging.info(\n\t            f\"(Steps: {self.steps}) Finished evaluation \"\n\t            f\"({eval_steps_per_epoch} steps per epoch).\"\n\t        )\n", "        # average loss\n\t        for key in self.total_eval_loss.keys():\n\t            self.total_eval_loss[key] /= eval_steps_per_epoch\n\t            logging.info(\n\t                f\"(Steps: {self.steps}) {key} = {self.total_eval_loss[key]:.4f}.\"\n\t            )\n\t        # record\n\t        self._write_to_tensorboard(self.total_eval_loss)\n\t        # reset\n\t        self.total_eval_loss = defaultdict(float)\n", "        # restore mode\n\t        for key in self.model.keys():\n\t            self.model[key].train()\n\t    def _metric_loss(self, predict_y, natural_y, mode='train'):\n\t        \"\"\"Metric losses.\"\"\"\n\t        metric_loss=0.0\n\t        # mel spectrogram loss\n\t        if self.config.get('use_mel_loss', False):\n\t            mel_loss = self.criterion[\"mel\"](predict_y, natural_y)\n\t            mel_loss *= self.config[\"lambda_mel_loss\"]\n", "            self._record_loss('mel_loss', mel_loss, mode=mode)\n\t            metric_loss += mel_loss\n\t        # multi-resolution sfft loss\n\t        if self.config.get('use_stft_loss', False):\n\t            sc_loss, mag_loss = self.criterion[\"stft\"](predict_y, natural_y)\n\t            sc_loss *= self.config[\"lambda_stft_loss\"]\n\t            mag_loss *= self.config[\"lambda_stft_loss\"]\n\t            self._record_loss('spectral_convergence_loss', sc_loss, mode=mode)\n\t            self._record_loss('log_stft_magnitude_loss', mag_loss, mode=mode)\n\t            metric_loss += (sc_loss + mag_loss)\n", "        # waveform shape loss\n\t        if self.config.get(\"use_shape_loss\", False):\n\t            shape_loss = self.criterion[\"shape\"](predict_y, natural_y)\n\t            shape_loss *= self.config[\"lambda_shape_loss\"]\n\t            self._record_loss('shape_loss', shape_loss, mode=mode)\n\t            metric_loss += shape_loss\n\t        return metric_loss\n\t    def _adv_loss(self, predict_p, natural_p=None, mode='train'):\n\t        \"\"\"Adversarial loss.\"\"\"\n\t        adv_loss = self.criterion[\"gen_adv\"](predict_p)\n", "        # feature matching loss\n\t        if natural_p is not None:\n\t            fm_loss = self.criterion[\"feat_match\"](predict_p, natural_p)\n\t            self._record_loss('feature_matching_loss', fm_loss, mode=mode)\n\t            adv_loss += self.config[\"lambda_feat_match\"] * fm_loss\n\t        adv_loss *= self.config[\"lambda_adv\"]\n\t        self._record_loss('adversarial_loss', adv_loss, mode=mode)\n\t        return adv_loss\n\t    def _dis_loss(self, predict_p, natural_p, mode='train'):\n\t        \"\"\"Discriminator loss.\"\"\"\n", "        real_loss, fake_loss = self.criterion[\"dis_adv\"](predict_p, natural_p)\n\t        dis_loss = real_loss + fake_loss\n\t        self._record_loss('real_loss', real_loss, mode=mode)\n\t        self._record_loss('fake_loss', fake_loss, mode=mode)\n\t        self._record_loss('discriminator_loss', dis_loss, mode=mode)\n\t        return dis_loss\n\t    def _update_generator(self, gen_loss):\n\t        \"\"\"Update generator.\"\"\"\n\t        self.optimizer[\"generator\"].zero_grad()\n\t        gen_loss.backward()\n", "        if self.config[\"generator_grad_norm\"] > 0:\n\t            torch.nn.utils.clip_grad_norm_(\n\t                self.model[\"generator\"].parameters(),\n\t                self.config[\"generator_grad_norm\"],\n\t            )\n\t        self.optimizer[\"generator\"].step()\n\t        self.scheduler[\"generator\"].step()  \n\t    def _update_discriminator(self, dis_loss):\n\t        \"\"\"Update discriminator.\"\"\"\n\t        self.optimizer[\"discriminator\"].zero_grad()\n", "        dis_loss.backward()\n\t        if self.config[\"discriminator_grad_norm\"] > 0:\n\t            torch.nn.utils.clip_grad_norm_(\n\t                self.model[\"discriminator\"].parameters(),\n\t                self.config[\"discriminator_grad_norm\"],\n\t            )\n\t        self.optimizer[\"discriminator\"].step()\n\t        self.scheduler[\"discriminator\"].step()\n\t    def _record_loss(self, name, loss, mode='train'):\n\t        \"\"\"Record loss.\"\"\"\n", "        if torch.is_tensor(loss):\n\t            loss = loss.item()\n\t        if mode == 'train':\n\t            self.total_train_loss[f\"train/{name}\"] += loss\n\t        elif mode == 'eval':\n\t            self.total_eval_loss[f\"eval/{name}\"] += loss\n\t        else:\n\t            raise NotImplementedError(f\"Mode ({mode}) is not supported!\")\n\t    def _write_to_tensorboard(self, loss):\n\t        \"\"\"Write to tensorboard.\"\"\"\n", "        for key, value in loss.items():\n\t            self.writer.add_scalar(key, value, self.steps)\n\t    def _check_save_interval(self):\n\t        if self.steps and (self.steps % self.config[\"save_interval_steps\"] == 0):\n\t            self.save_checkpoint(\n\t                os.path.join(self.config[\"outdir\"], f\"checkpoint-{self.steps}steps.pkl\")\n\t            )\n\t            logging.info(f\"Successfully saved checkpoint @ {self.steps} steps.\")\n\t    def _check_eval_interval(self):\n\t        if self.steps % self.config[\"eval_interval_steps\"] == 0:\n", "            self._eval_epoch()\n\t    def _check_log_interval(self):\n\t        if self.steps % self.config[\"log_interval_steps\"] == 0:\n\t            for key in self.total_train_loss.keys():\n\t                self.total_train_loss[key] /= self.config[\"log_interval_steps\"]\n\t                logging.info(\n\t                    f\"(Steps: {self.steps}) {key} = {self.total_train_loss[key]:.4f}.\"\n\t                )\n\t            self._write_to_tensorboard(self.total_train_loss)\n\t            # reset\n", "            self.total_train_loss = defaultdict(float)\n\t    def _check_train_finish(self):\n\t        if self.steps >= self.train_max_steps:\n\t            self.finish_train = True\n\t        else:\n\t            self.finish_train = False\n\t        return self.finish_train\n\tclass TrainerVQGAN(TrainerGAN):\n\t    def __init__(\n\t        self,\n", "        steps,\n\t        epochs,\n\t        data_loader,\n\t        model,\n\t        criterion,\n\t        optimizer,\n\t        scheduler,\n\t        config,\n\t        device=torch.device(\"cpu\"),\n\t    ):\n", "        super(TrainerVQGAN, self).__init__(\n\t            steps=steps,\n\t            epochs=epochs,\n\t            data_loader=data_loader,\n\t            model=model,\n\t            criterion=criterion,\n\t            optimizer=optimizer,\n\t            scheduler=scheduler,\n\t            config=config,\n\t            device=device,\n", "        )\n\t    # perplexity info\n\t    def _perplexity(self, perplexity, label=None, mode='train'):\n\t        if label:\n\t            name = f\"{mode}/ppl_{label}\"\n\t        else:\n\t            name = f\"{mode}/ppl\"\n\t        if torch.numel(perplexity) > 1:\n\t            perplexity = perplexity.tolist()\n\t            for idx, ppl in enumerate(perplexity):\n", "                self._record_loss(f\"{name}_{idx}\", ppl, mode=mode)\n\t        else:\n\t            self._record_loss(name, perplexity, mode=mode)\n\t    # vq loss\n\t    def _vq_loss(self, vqloss, label=None, mode='train'):\n\t        if label:\n\t            name = f\"{mode}/vqloss_{label}\"\n\t        else:\n\t            name = f\"{mode}/vqloss\"\n\t        vqloss = torch.sum(vqloss)\n", "        vqloss *= self.config[\"lambda_vq_loss\"]\n\t        self._record_loss(name, vqloss, mode=mode)\n\t        return vqloss\n"]}
{"filename": "trainer/autoencoder.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Training flow of symmetric codec.\"\"\"\n", "import logging\n\timport torch\n\tfrom trainer.trainerGAN import TrainerVQGAN\n\tclass Trainer(TrainerVQGAN):\n\t    def __init__(\n\t        self,\n\t        steps,\n\t        epochs,\n\t        data_loader,\n\t        model,\n", "        criterion,\n\t        optimizer,\n\t        scheduler,\n\t        config,\n\t        device=torch.device(\"cpu\"),\n\t    ):\n\t        super(Trainer, self).__init__(\n\t           steps=steps,\n\t           epochs=epochs,\n\t           data_loader=data_loader,\n", "           model=model,\n\t           criterion=criterion,\n\t           optimizer=optimizer,\n\t           scheduler=scheduler,\n\t           config=config,\n\t           device=device,\n\t        )\n\t        self.fix_encoder = False\n\t        self.paradigm = config.get('paradigm', 'efficient') \n\t        self.generator_start = config.get('start_steps', {}).get('generator', 0)\n", "        self.discriminator_start = config.get('start_steps', {}).get('discriminator', 200000)\n\t    def _train_step(self, batch):\n\t        \"\"\"Single step of training.\"\"\"\n\t        mode = 'train'\n\t        x = batch\n\t        x = x.to(self.device)\n\t        # check generator step\n\t        if self.steps < self.generator_start:\n\t            self.generator_train = False\n\t        else:\n", "            self.generator_train = True\n\t        # check discriminator step\n\t        if self.steps < self.discriminator_start:\n\t            self.discriminator_train = False\n\t        else:\n\t            self.discriminator_train = True\n\t            if (not self.fix_encoder) and (self.paradigm == 'efficient'):\n\t                # fix encoder, quantizer, and codebook\n\t                for parameter in self.model[\"generator\"].encoder.parameters():\n\t                    parameter.requires_grad = False\n", "                for parameter in self.model[\"generator\"].projector.parameters():\n\t                    parameter.requires_grad = False\n\t                for parameter in self.model[\"generator\"].quantizer.parameters():\n\t                    parameter.requires_grad = False\n\t                self.fix_encoder = True\n\t                logging.info(\"Encoder, projector, quantizer, and codebook are fixed\")\n\t        # check codebook updating\n\t        if self.fix_encoder:\n\t            self.model[\"generator\"].quantizer.codebook.eval()\n\t        #######################\n", "        #      Generator      #\n\t        #######################\n\t        if self.generator_train:\n\t            # initialize generator loss\n\t            gen_loss = 0.0\n\t            # main genertor operation\n\t            y_, zq, z, vqloss, perplexity = self.model[\"generator\"](x)\n\t            # perplexity info\n\t            self._perplexity(perplexity, mode=mode)\n\t            # vq loss\n", "            gen_loss += self._vq_loss(vqloss, mode=mode)\n\t            # metric loss\n\t            gen_loss += self._metric_loss(y_, x, mode=mode)\n\t            # adversarial loss\n\t            if self.discriminator_train:\n\t                p_ = self.model[\"discriminator\"](y_)\n\t                if self.config[\"use_feat_match_loss\"]:\n\t                    with torch.no_grad():\n\t                        p = self.model[\"discriminator\"](x)\n\t                else:\n", "                    p = None\n\t                gen_loss += self._adv_loss(p_, p, mode=mode)\n\t            # update generator\n\t            self._record_loss('generator_loss', gen_loss, mode=mode)\n\t            self._update_generator(gen_loss)\n\t        #######################\n\t        #    Discriminator    #\n\t        #######################\n\t        if self.discriminator_train:\n\t            # re-compute y_ which leads better quality\n", "            with torch.no_grad():\n\t                y_, _, _, _, _ = self.model[\"generator\"](x)\n\t            p = self.model[\"discriminator\"](x)\n\t            p_ = self.model[\"discriminator\"](y_.detach())\n\t            # discriminator loss & update discriminator\n\t            self._update_discriminator(self._dis_loss(p_, p, mode=mode))\n\t        # update counts\n\t        self.steps += 1\n\t        self.tqdm.update(1)\n\t        self._check_train_finish()\n", "    @torch.no_grad()\n\t    def _eval_step(self, batch):\n\t        \"\"\"Single step of evaluation.\"\"\"\n\t        mode = 'eval'\n\t        x = batch\n\t        x = x.to(self.device)\n\t        # initialize generator loss\n\t        gen_loss = 0.0\n\t        # main genertor operation\n\t        y_, zq, z, vqloss, perplexity = self.model[\"generator\"](x)\n", "        # perplexity info\n\t        self._perplexity(perplexity, mode=mode)\n\t        # vq_loss\n\t        gen_loss += self._vq_loss(vqloss, mode=mode)\n\t        # metric loss\n\t        gen_loss += self._metric_loss(y_, x, mode=mode)\n\t        if self.discriminator_train:\n\t            # adversarial loss\n\t            p_ = self.model[\"discriminator\"](y_)\n\t            p = self.model[\"discriminator\"](x)\n", "            gen_loss += self._adv_loss(p_, p, mode=mode)\n\t            # discriminator loss\n\t            self._dis_loss(p_, p, mode=mode)\n\t        # generator loss\n\t        self._record_loss('generator_loss', gen_loss, mode=mode)\n"]}
{"filename": "trainer/denoise.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\t\"\"\"Training flow of symmetric codec.\"\"\"\n", "import logging\n\timport torch\n\tfrom trainer.trainerGAN import TrainerVQGAN\n\tclass Trainer(TrainerVQGAN):\n\t    def __init__(\n\t        self,\n\t        steps,\n\t        epochs,\n\t        data_loader,\n\t        model,\n", "        criterion,\n\t        optimizer,\n\t        scheduler,\n\t        config,\n\t        device=torch.device(\"cpu\"),\n\t    ):\n\t        super(Trainer, self).__init__(\n\t           steps=steps,\n\t           epochs=epochs,\n\t           data_loader=data_loader,\n", "           model=model,\n\t           criterion=criterion,\n\t           optimizer=optimizer,\n\t           scheduler=scheduler,\n\t           config=config,\n\t           device=device,\n\t        )\n\t        # fix quantizer\n\t        for parameter in self.model[\"generator\"].quantizer.parameters():\n\t            parameter.requires_grad = False\n", "        # fix decoder\n\t        for parameter in self.model[\"generator\"].decoder.parameters():\n\t            parameter.requires_grad = False\n\t        logging.info(\"Quantizer, codebook, and decoder are fixed\")\n\t    def _train_step(self, batch):\n\t        \"\"\"Single step of training.\"\"\"\n\t        mode = 'train'\n\t        x_n, x_c = batch\n\t        x_n = x_n.to(self.device)\n\t        x_c = x_c.to(self.device)\n", "        # fix codebook\n\t        self.model[\"generator\"].quantizer.codebook.eval()\n\t        # initialize generator loss\n\t        gen_loss = 0.0\n\t        # main genertor operation\n\t        y_nc, zq, z, vqloss, perplexity = self.model[\"generator\"](x_n)\n\t        # perplexity info\n\t        self._perplexity(perplexity, mode=mode)\n\t        # vq loss\n\t        gen_loss += self._vq_loss(vqloss, mode=mode)\n", "        # metric loss\n\t        gen_loss += self._metric_loss(y_nc, x_c, mode=mode)\n\t        # update generator\n\t        self._record_loss('generator_loss', gen_loss, mode=mode)\n\t        self._update_generator(gen_loss)\n\t        # update counts\n\t        self.steps += 1\n\t        self.tqdm.update(1)\n\t        self._check_train_finish()\n\t    @torch.no_grad()\n", "    def _eval_step(self, batch):\n\t        \"\"\"Single step of evaluation.\"\"\"\n\t        mode = 'eval'\n\t        x_n, x_c = batch\n\t        x_n = x_n.to(self.device)\n\t        x_c = x_c.to(self.device)\n\t        # initialize generator loss\n\t        gen_loss = 0.0\n\t        # main genertor operation\n\t        y_nc, zq, z, vqloss, perplexity = self.model[\"generator\"](x_n)\n", "        # perplexity info\n\t        self._perplexity(perplexity, mode=mode)\n\t        # vq_loss\n\t        gen_loss += self._vq_loss(vqloss, mode=mode)\n\t        # metric loss\n\t        gen_loss += self._metric_loss(y_nc, x_c, mode=mode)\n\t        # generator loss\n\t        self._record_loss('generator_loss', gen_loss, mode=mode)\n"]}
