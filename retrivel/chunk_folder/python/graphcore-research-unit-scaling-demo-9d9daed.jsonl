{"filename": "run_experiment.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Run a single experiment.\"\"\"\n\timport dataclasses\n\timport json\n\timport os\n\tfrom pathlib import Path\n\timport scmm as S\n\t# pylint:disable=invalid-name\n\t# ssub -t mk2 -n 1 -- python run_experiment.py\n\tif __name__ == \"__main__\":\n", "    out, profile, sweep = None, None, False\n\t    # profile = Path(\"out/profiles/dev\")\n\t    # out = Path(\"out/training/dev\")\n\t    # sweep = True\n\t    settings = S.experiments.Settings(\n\t        # data=S.experiments.DataSettings(Path(\"scmm/tests/data\"), kind=\"test\"),\n\t        data=S.experiments.DataSettings(\n\t            Path(\"/home/research-datasets/wikitext103_raw\")\n\t        ),\n\t        model=S.models.Settings(\n", "            hidden_size=128,\n\t            depth=8,\n\t            residual=S.models.Residual(norm=\"pre\", alpha=\"mean\"),\n\t            sequence=S.models.Attention(\n\t                heads=2, head_size=64, frequencies=128, max_period=1024\n\t            ),\n\t            token=S.models.FFN(multiple=4),\n\t            dtype=\"float32\",\n\t            vocab_size=None,  # type:ignore[arg-type]\n\t            seed=None,  # type:ignore[arg-type]\n", "        ),\n\t        training=S.training.Settings(\n\t            batch=S.datasets.BatchSettings(\n\t                sequences=8, sequence_length=256, overlap_length=32, loop_seed=None\n\t            ),\n\t            steps=int(2**20),\n\t            valid_interval=int(2**14),\n\t            optimiser=S.training.AdamW(\n\t                learning_rate=2**-6, learning_rate_decay=2**-16\n\t            ),\n", "            loss_scale=1,\n\t        ),\n\t        unit_scale=\"0.4\",\n\t        target=S.pedal.xpu.IpuSettings(iterations_per_loop=int(2**10)),\n\t        output=S.experiments.OutputSettings(\n\t            stderr=False,\n\t            wandb=True,\n\t            log=out and out / \"log.jsonl\",\n\t            checkpoint=out and out / \"model.npz\",\n\t        ),\n", "        metadata=dict(experiment=\"dev\"),\n\t        seed=None,  # type:ignore[arg-type]\n\t    )\n\t    ####################\n\t    # Common\n\t    if profile:\n\t        profile.mkdir(parents=True, exist_ok=True)\n\t        # os.environ[\"POPLIBS_LOG_LEVEL\"] = \"INFO\"\n\t        os.environ[\"POPLAR_ENGINE_OPTIONS\"] = json.dumps(\n\t            {\n", "                \"autoReport.all\": True,\n\t                # \"autoReport.outputExecutionProfile\": False,\n\t                \"autoReport.outputArchive\": False,\n\t                \"autoReport.directory\": str(profile),\n\t                \"debug.allowOutOfMemory\": True,\n\t                \"profiler.replicaToProfile\": 0,\n\t            }\n\t        )\n\t        os.environ[\"PVTI_OPTIONS\"] = json.dumps(\n\t            dict(enable=True, directory=str(profile))\n", "        )\n\t        settings = dataclasses.replace(\n\t            settings,\n\t            # Switch out the data to avoid a large delay \"loading\"\n\t            data=S.experiments.DataSettings(Path(\"scmm/tests/data\")),\n\t            model=dataclasses.replace(settings.model, vocab_size=5008),\n\t            training=dataclasses.replace(\n\t                settings.training, steps=2, valid_interval=None\n\t            ),\n\t            target=dataclasses.replace(settings.target, iterations_per_loop=int(2)),\n", "            output=S.experiments.OutputSettings(\n\t                stderr=True, wandb=False, log=profile / \"log.jsonl\", checkpoint=None\n\t            ),\n\t        )\n\t    else:\n\t        os.environ[\"TF_POPLAR_FLAGS\"] = (\n\t            \"--show_progress_bar=false\"\n\t            f\" --executable_cache_path=/a/scratch/{os.environ['USER']}_research/tmp/cache\"\n\t        )\n\t    if sweep:\n", "        sweep_settings = S.experiments.LrSweep(settings, step=4, threshold=0.05, reps=1)\n\t        S.experiments.find_learning_rate(settings=sweep_settings)\n\t    else:\n\t        # Run in subprocess so that the PVTI options \"take\"\n\t        S.pedal.utility.run_in_subprocess(S.experiments.run, settings=settings)\n"]}
{"filename": "run_sweep.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Run a multi-axis hyperparameter sweep.\"\"\"\n\timport copy\n\timport dataclasses\n\timport itertools as it\n\timport multiprocessing\n\timport multiprocessing.pool\n\timport os\n\timport sys\n\tfrom pathlib import Path\n", "from typing import Any, Dict, Iterable, List, Optional, Union\n\timport scmm as S\n\t# pylint:disable=redefined-outer-name\n\tclass Sweeper:\n\t    \"\"\"Utility for sweeping multiple settings axes.\"\"\"\n\t    def __init__(\n\t        self,\n\t        settings: Union[S.experiments.Settings, S.experiments.LrSweep],\n\t        n_workers: int,\n\t        reps: int,\n", "    ):\n\t        self.n_workers = n_workers\n\t        self.reps = reps\n\t        if isinstance(settings, S.experiments.LrSweep):\n\t            self.base_settings = settings.base\n\t            self.lr_settings: Optional[S.experiments.LrSweep] = settings\n\t        else:\n\t            self.base_settings = settings\n\t            self.lr_settings = None\n\t        self.axes: List[List[Dict[str, Any]]] = []\n", "    def add(self, values: Iterable[Dict[str, Any]]) -> None:\n\t        \"\"\"Add an independent axis to the sweep.\"\"\"\n\t        self.axes.append(list(values))\n\t    @staticmethod\n\t    def _recursive_assign(\n\t        settings: S.experiments.Settings, path: str, value: Any\n\t    ) -> None:\n\t        # Nested lookup\n\t        *prefix, last = path.split(\".\")\n\t        node = settings\n", "        for key in prefix:\n\t            node = getattr(node, key)\n\t        # Dataclass type checking\n\t        expected_type: Any = {f.name: f.type for f in dataclasses.fields(node)}.get(\n\t            last\n\t        )\n\t        if expected_type is float:\n\t            expected_type = (int, float)\n\t        if getattr(expected_type, \"__origin__\", None) is Union:\n\t            expected_type = expected_type.__args__\n", "        if not isinstance(value, expected_type):\n\t            raise ValueError(\n\t                f\"Expected {path} to be {expected_type}, actual {value} (type {type(value)})\"\n\t            )\n\t        setattr(node, last, value)\n\t    @property\n\t    def configs(self) -> Iterable[Union[S.experiments.Settings, S.experiments.LrSweep]]:\n\t        \"\"\"Iterate through all settings configurations included in the sweep.\"\"\"\n\t        for overrides in it.product(*self.axes):\n\t            settings = copy.deepcopy(self.base_settings)\n", "            for override in overrides:\n\t                for path, value in override.items():\n\t                    self._recursive_assign(settings, path, value)\n\t            if self.lr_settings is not None:\n\t                yield dataclasses.replace(self.lr_settings, base=settings)\n\t            else:\n\t                yield settings\n\t    def run(self) -> None:\n\t        \"\"\"Run a parallel sweep.\"\"\"\n\t        # os.environ[\"TMPDIR\"] = \"/localdata/tmp\"\n", "        os.environ[\"TF_POPLAR_FLAGS\"] = (\n\t            \"--show_progress_bar=false\"\n\t            f\" --executable_cache_path=/a/scratch/{os.environ['USER']}_research/tmp/cache/sweep\"\n\t        )\n\t        with multiprocessing.pool.ThreadPool(self.n_workers) as pool:\n\t            for _ in range(self.reps):\n\t                for setting in self.configs:\n\t                    target = (\n\t                        S.experiments.run\n\t                        if isinstance(setting, S.experiments.Settings)\n", "                        else S.experiments.find_learning_rate\n\t                    )\n\t                    pool.apply_async(\n\t                        S.pedal.utility.run_in_subprocess,\n\t                        kwds=dict(command=target, settings=setting),\n\t                    )\n\t            pool.close()\n\t            pool.join()\n\t# Run sweep\n\t#   ssub -n 16 -p ipu-large -- python run_sweep.py\n", "if __name__ == \"__main__\":\n\t    settings = S.experiments.Settings(\n\t        data=S.experiments.DataSettings(\n\t            Path(\"/home/research-datasets/wikitext103_raw\")\n\t        ),\n\t        model=S.models.Settings(\n\t            hidden_size=128,\n\t            depth=8,\n\t            residual=S.models.Residual(norm=\"pre\", alpha=\"mean\"),\n\t            sequence=S.models.Attention(\n", "                heads=2, head_size=64, frequencies=128, max_period=1024\n\t            ),\n\t            token=S.models.FFN(multiple=4),\n\t            dtype=\"float32\",\n\t            vocab_size=None,  # type:ignore[arg-type]\n\t            seed=None,  # type:ignore[arg-type]\n\t        ),\n\t        training=S.training.Settings(\n\t            batch=S.datasets.BatchSettings(\n\t                sequences=8,\n", "                sequence_length=256,\n\t                overlap_length=32,\n\t                loop_seed=None,\n\t            ),\n\t            steps=int(2**19),\n\t            valid_interval=int(2**14),\n\t            optimiser=S.training.AdamW(\n\t                learning_rate=2**-14,\n\t                learning_rate_decay=2**-16,\n\t            ),\n", "            loss_scale=1,\n\t        ),\n\t        unit_scale=None,\n\t        target=S.pedal.xpu.IpuSettings(\n\t            iterations_per_loop=int(2**10),\n\t            stochastic_rounding=True,\n\t        ),\n\t        output=S.experiments.OutputSettings(\n\t            wandb=True, stderr=False, log=None, checkpoint=None\n\t        ),\n", "        seed=None,  # type:ignore[arg-type]\n\t        metadata=dict(experiment=\"20230115_large_p0\"),\n\t    )\n\t    sweeper = Sweeper(\n\t        S.experiments.LrSweep(settings, step=2, threshold=0.1, reps=3),\n\t        n_workers=16,\n\t        reps=1,\n\t    )\n\t    def _all_settings() -> Iterable[Dict[str, Any]]:\n\t        # pylint:disable=too-many-nested-blocks\n", "        attention = S.models.Attention(\n\t            heads=2, head_size=64, frequencies=128, max_period=1024\n\t        )\n\t        conv = S.models.Conv(kernel_size=7, groups=8)\n\t        rnn = S.models.RNN(rebias=1)\n\t        for sequence in [rnn, conv, attention]:\n\t            for norm in [\"pre\", \"post\"]:\n\t                for dtype in [\"float16\", \"float32\"]:\n\t                    for unit_scale in [None, \"0.4\"]:\n\t                        for loss_scale in [1, 2048]:\n", "                            sequence_kind = sequence.kind  # type:ignore[attr-defined]\n\t                            if (\n\t                                unit_scale or dtype == \"float32\"\n\t                            ) and loss_scale != 1:  # unnecessary\n\t                                continue\n\t                            if norm == \"post\" and sequence_kind != \"attention\":\n\t                                continue  # only run post-norm for attention\n\t                            yield {\n\t                                \"model.residual.norm\": norm,\n\t                                \"model.depth\": 2 if sequence_kind == \"rnn\" else 8,\n", "                                \"model.sequence\": sequence,\n\t                                \"model.dtype\": dtype,\n\t                                \"unit_scale\": unit_scale,\n\t                                \"training.loss_scale\": loss_scale,\n\t                                \"training.optimiser.learning_rate\": (\n\t                                    2**-14 if unit_scale is None else 2**-8\n\t                                ),\n\t                            }\n\t    sweeper.add(_all_settings())\n\t    # This also runs basic checks on `configs` (e.g. in --dry-run)\n", "    print(\n\t        f\"Sweeping {sum(1 for _ in sweeper.configs)} settings, {sweeper.reps} reps,\"\n\t        f\" as {sweeper.base_settings.metadata['experiment']!r}\",\n\t        file=sys.stderr,\n\t    )\n\t    if not set(sys.argv) & {\"-d\", \"--dry-run\", \"--dryrun\"}:\n\t        # subprocess.check_call([\"ulimit\", \"-u\", \"16384\"], shell=True)\n\t        sweeper.run()\n"]}
{"filename": "scmm/layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"General purpose layers and functions.\"\"\"\n\tfrom typing import Iterable, List, Optional, Tuple, Type\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tdef batched_gather(tables: tf.Tensor, indices: tf.Tensor) -> tf.Tensor:\n\t    \"\"\"Simulate tf.gather(tables, indices, batch_dims=indices.ndim).\n\t    Better compilation on IPU vs `tf.gather(logp, ids, batch_dims=2)`\n\t    \"\"\"\n", "    # Implemented here and in uscale.ops to avoid circular dependency issues\n\t    # pylint:disable=R0801\n\t    assert len(tables.shape) == len(indices.shape) + 1\n\t    # Use a one-hot encoding to save code memory\n\t    return tf.squeeze(\n\t        tf.one_hot(indices, tables.shape[-1], dtype=tables.dtype)[..., tf.newaxis, :]\n\t        @ tables[..., tf.newaxis],\n\t        [-1, -2],\n\t    )\n\tdef softmax_cross_entropy(\n", "    scores: tf.Tensor, ids: tf.Tensor, mask: tf.Tensor\n\t) -> Tuple[tf.Tensor, tf.Tensor]:\n\t    \"\"\"Compute masked softmax cross entropy loss.\n\t    returns -- (average_loss, n_items) -- average_loss always in fp32\n\t    \"\"\"\n\t    assert mask.shape == ids.shape, \"mask should match target ids\"\n\t    # Use float32 for local computation - keeping things simple\n\t    logp = tf.nn.log_softmax(tf.cast(scores, tf.float32))\n\t    target_logp = batched_gather(logp, ids)\n\t    total_loss = tf.reduce_sum(tf.cast(mask, target_logp.dtype) * -target_logp)\n", "    n_ids = tf.reduce_sum(tf.cast(mask, tf.int32))\n\t    loss = total_loss / tf.cast(n_ids, total_loss.dtype)\n\t    return loss, n_ids\n\tclass LayerNormalization(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"A FP16-safe variant of keras.layers.LayerNormalization.\"\"\"\n\t    def __init__(self, epsilon: float = 0.001, dtype: tf.DType = tf.float32):\n\t        super().__init__(dtype=dtype)\n\t        self.epsilon = epsilon\n\t        self.beta: tf.Variable = None\n\t        self.beta_initializer = keras.initializers.zeros()\n", "        self.gamma: tf.Variable = None\n\t        self.gamma_initializer = keras.initializers.ones()\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.beta = self.add_weight(\n\t            \"beta\", shape=input_shape[-1], initializer=self.beta_initializer\n\t        )\n\t        self.gamma = self.add_weight(\n\t            \"gamma\", shape=input_shape[-1], initializer=self.gamma_initializer\n\t        )\n", "    def _normalize(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        inputs_fp32 = tf.cast(inputs, tf.float32)\n\t        z = inputs_fp32 - tf.reduce_mean(inputs_fp32, axis=-1, keepdims=True)\n\t        normed = z / tf.sqrt(\n\t            tf.reduce_mean(z**2, axis=-1, keepdims=True) + self.epsilon\n\t        )\n\t        return tf.cast(normed, inputs.dtype)\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        return self.gamma * self._normalize(inputs) + self.beta\n\tclass ResidualLayer(keras.layers.Layer):  # type:ignore[misc]\n", "    \"\"\"A residual layer, supporting PreNorm, PostNorm, NoNorm & interpolation.\n\t    norm_type -- None | \"pre\" | \"post\"\n\t    alpha -- None | <float>  -- interpolation constant, higher to incorporate\n\t                                more of the residual branch, lower to preserve\n\t                                the skip connection.\n\t        y = sqrt(1 - alpha) * x + sqrt(alpha) * f(x)\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        body: keras.layers.Layer,\n", "        norm_type: Optional[str],\n\t        alpha: Optional[float],\n\t        dtype: tf.DType = tf.float32,\n\t        norm_cls: Type[keras.layers.Layer] = LayerNormalization,\n\t    ):\n\t        super().__init__(dtype=dtype)\n\t        self.body = body\n\t        self.norm_type = norm_type\n\t        self.alpha_value = alpha\n\t        self.alpha: tf.Variable = None\n", "        assert norm_type in {None, \"pre\", \"post\"}, f\"unexpected norm_type {norm_type}\"\n\t        self.norm_cls = norm_cls\n\t        self.norm: keras.layers.Layer = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.body.build(input_shape)\n\t        if self.norm_type is not None:\n\t            self.norm = self.norm_cls(dtype=self.dtype)\n\t            self.norm.build(input_shape)\n\t        if self.alpha_value is not None:\n", "            # Turn alpha into a non-trainable variable, for sake of outlining\n\t            self.alpha = self.add_weight(\n\t                name=\"alpha\",\n\t                shape=(),\n\t                initializer=keras.initializers.constant(self.alpha_value),\n\t                trainable=False,\n\t            )\n\t    def call(self, x: tf.Tensor) -> tf.Tensor:\n\t        branch = x\n\t        if self.norm_type == \"pre\":\n", "            branch = self.norm(branch)\n\t        branch = self.body(branch)\n\t        if self.alpha is not None:\n\t            y = (1 - self.alpha) ** 0.5 * x + self.alpha**0.5 * branch\n\t        else:\n\t            y = x + branch\n\t        if self.norm_type == \"post\":\n\t            y = self.norm(y)\n\t        return y\n\tclass FFNLayer(keras.layers.Layer):  # type:ignore[misc]\n", "    \"\"\"A pointwise expansion FFN layer (a la Transformer, https://arxiv.org/abs/1706.03762).\"\"\"\n\t    def __init__(\n\t        self,\n\t        multiple: float,\n\t        dtype: tf.DType = tf.float32,\n\t        seeds: Optional[Tuple[int, int]] = None,\n\t    ):\n\t        super().__init__(dtype=dtype)\n\t        self.multiple = multiple\n\t        self.seeds = seeds or (None, None)\n", "        self.up: Optional[keras.layers.Layer] = None  # pylint:disable=invalid-name\n\t        self.down: Optional[keras.layers.Layer] = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        hidden_size = input_shape[-1]\n\t        intermediate_size = int(self.multiple * hidden_size)\n\t        self.up = keras.layers.Dense(\n\t            intermediate_size,\n\t            dtype=self.dtype,\n\t            kernel_initializer=keras.initializers.GlorotUniform(seed=self.seeds[0]),\n", "        )\n\t        self.up.build(input_shape[:-1] + (hidden_size,))\n\t        self.down = keras.layers.Dense(\n\t            hidden_size,\n\t            dtype=self.dtype,\n\t            kernel_initializer=keras.initializers.GlorotUniform(seed=self.seeds[1]),\n\t        )\n\t        self.down.build(input_shape[:-1] + (intermediate_size,))\n\t    def call(self, x: tf.Tensor) -> tf.Tensor:\n\t        return self.down(tf.nn.relu(self.up(x)))  # type:ignore[misc]\n", "class PadAndShiftLayer(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"Shifts sequence features one place to the right with a trainable padding vector.\"\"\"\n\t    def __init__(self, dtype: tf.DType = tf.float32) -> None:\n\t        super().__init__(dtype=dtype)\n\t        self.padding: tf.Variable = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        if len(input_shape) != 3:\n\t            raise ValueError(\n\t                f\"Input should be 3D (batch, sequence, feature), actual shape {input_shape}\"\n", "            )\n\t        self.padding = self.add_weight(\n\t            name=\"padding\",\n\t            shape=input_shape[-1],\n\t            initializer=keras.initializers.zeros,\n\t        )\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        pad = tf.tile(self.padding[tf.newaxis, tf.newaxis], [inputs.shape[0], 1, 1])\n\t        return tf.concat([pad, inputs[:, :-1, :]], axis=1)\n\tclass Isotropic(keras.layers.Layer):  # type:ignore[misc]\n", "    \"\"\"Like keras.models.Sequential, but isotropic & with friendly names for each layer.\"\"\"\n\t    def __init__(self, dtype: tf.DType = tf.float32, **layers: keras.layers.Layer):\n\t        super().__init__(dtype=dtype)\n\t        self._layers = layers\n\t        for name, layer in layers.items():\n\t            setattr(self, name, layer)\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        for layer in self._layers.values():\n\t            layer.build(input_shape)\n", "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        outputs = inputs\n\t        for layer in self._layers.values():\n\t            outputs = layer(outputs)\n\t        return outputs\n\t####################\n\t# Attention\n\tdef sinusoid_embedding(\n\t    sequence_length: int, frequencies: int, max_period: int\n\t) -> np.ndarray:\n", "    \"\"\"Generate a family of sin/cos embeddings.\n\t    See \"Attention Is All You Need\", Vaswani et al., section 3.5.\n\t    sequence_length -- output dimension (number of indices)\n\t    frequencies -- number of components to generate\n\t    max_period -- the period (in indices) of the lowest frequency component\n\t    returns -- array(sequence_length x frequencies)\n\t    \"\"\"\n\t    index = np.arange(frequencies)\n\t    frequency = np.pi * (2 / max_period) ** ((index // 2) / (frequencies // 2 - 1))\n\t    phase = np.pi / 2 * (index % 2)\n", "    time = np.arange(sequence_length)\n\t    return np.sin(frequency * time[:, np.newaxis] + phase)\n\tdef relative_causal_reshape(scores: tf.Tensor) -> tf.Tensor:\n\t    \"\"\"Transform relative scores to an attention matrix.\n\t    Fills the lower-left quadrant of the result with scores\n\t        result[..., i, j] = scores[..., i, i - j]\n\t    \"\"\"\n\t    sequence_length = scores.shape[-1]\n\t    ndim = len(scores.shape)\n\t    padded = tf.pad(scores[..., ::-1], [(0, 0)] * (ndim - 1) + [(0, sequence_length)])\n", "    # A reshaping and slicing trick to move to relative positions\n\t    tmp = tf.reshape(padded, padded.shape[:-2] + (2 * sequence_length**2,))\n\t    tmp = tmp[..., :-sequence_length]\n\t    tmp = tf.reshape(tmp, tmp.shape[:-1] + (sequence_length, 2 * sequence_length - 1))\n\t    tmp = tmp[..., sequence_length - 1 :]\n\t    return tmp\n\tdef causal_mask(attention: tf.Tensor, mask_value: float) -> tf.Tensor:\n\t    \"\"\"Apply a causal mask to an attention matrix of shape (*, L, L).\"\"\"\n\t    sequence_length = attention.shape[-1]\n\t    return attention + tf.constant(\n", "        np.triu(np.full((sequence_length, sequence_length), mask_value), k=1),\n\t        dtype=attention.dtype,\n\t    )\n\tclass MultiHeadAttention(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"Multi-head self attention a la Transformer.\n\t    With causal masking.\n\t    With relative-positional embeddings a la Transformer XL.\n\t    \"\"\"\n\t    # pylint:disable=too-many-instance-attributes\n\t    def __init__(\n", "        self,\n\t        heads: int,\n\t        head_size: int,\n\t        frequencies: int,\n\t        max_period: int,\n\t        dtype: tf.DType = tf.float32,\n\t        seeds: Optional[Tuple[int, int, int]] = None,\n\t    ):\n\t        super().__init__(dtype=dtype)\n\t        self.heads = heads\n", "        self.head_size = head_size\n\t        self.frequencies = frequencies\n\t        self.max_period = max_period\n\t        self.seeds = (None, None, None) if seeds is None else seeds\n\t        self.qkv: tf.Variable = None\n\t        self.q_bias: tf.Variable = None\n\t        self.positional: tf.Variable = None\n\t        self.out: keras.layers.Layer = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n", "        input_size = input_shape[-1]\n\t        qkv_scale = np.sqrt(3) * input_size**-0.5\n\t        self.qkv = self.add_weight(\n\t            name=\"qkv\",\n\t            shape=(input_size, 3, self.heads, self.head_size),\n\t            initializer=keras.initializers.random_uniform(\n\t                -qkv_scale, qkv_scale, seed=self.seeds[0]\n\t            ),\n\t        )\n\t        self.q_bias = self.add_weight(\n", "            name=\"q_bias\",\n\t            shape=(self.heads, self.head_size),\n\t            initializer=keras.initializers.zeros(),\n\t        )\n\t        positional_scale = np.sqrt(3) * self.frequencies**-0.5\n\t        self.positional = self.add_weight(\n\t            name=\"positional\",\n\t            shape=(self.frequencies, self.heads, self.head_size),\n\t            initializer=keras.initializers.random_uniform(\n\t                -positional_scale, positional_scale, seed=self.seeds[1]\n", "            ),\n\t        )\n\t        self.out = keras.layers.Dense(\n\t            input_size,\n\t            dtype=self.dtype,\n\t            kernel_initializer=keras.initializers.GlorotUniform(seed=self.seeds[2]),\n\t        )\n\t        self.out.build(input_shape[:-1] + (self.heads * self.head_size,))\n\t    def _positional_weights(self, query: tf.Tensor) -> tf.Tensor:\n\t        sequence_length = query.shape[-2]\n", "        sins = tf.constant(\n\t            sinusoid_embedding(sequence_length, self.frequencies, self.max_period),\n\t            dtype=query.dtype,\n\t        )\n\t        embeddings = tf.einsum(\"sf,fnh->nsh\", sins, self.positional)\n\t        scores = tf.einsum(\"bnqh,nvh->bnqv\", query, embeddings) * self.head_size**-0.5\n\t        return relative_causal_reshape(scores)\n\t    def call(self, input: tf.Tensor) -> tf.Tensor:\n\t        # pylint:disable=invalid-name\n\t        q, k, v = tf.unstack(tf.einsum(\"bsx,xAnh -> Abnsh\", input, self.qkv))\n", "        q += self.q_bias[:, tf.newaxis, :]\n\t        a = tf.einsum(\"bnqh,bnkh->bnqk\", q, k) * self.head_size**-0.5\n\t        a += self._positional_weights(q)\n\t        # Note: oddly, -1e3 can be insufficient in FP16 with no LS, causing \"cheating\"\n\t        a = causal_mask(a, mask_value=-3e4)\n\t        a = tf.nn.softmax(a, axis=-1)\n\t        o = tf.einsum(\"bnqk,bnkh->bqnh\", a, v)\n\t        return self.out(tf.reshape(o, o.shape[:-2] + (self.head_size * self.heads,)))\n\t####################\n\t# RNN\n", "class RecurrentHighwayCell(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"A recurrent highway cell from https://arxiv.org/abs/1607.03474.\"\"\"\n\t    def __init__(\n\t        self,\n\t        hidden_size: int,\n\t        rebias: float,\n\t        dtype: tf.DType = tf.float32,\n\t        seed: Optional[int] = None,\n\t    ):\n\t        super().__init__(name=type(self).__name__, dtype=dtype)\n", "        self.hidden_size = hidden_size\n\t        self.carry_rebias = rebias\n\t        self.update_rebias = -rebias\n\t        self.seed = seed\n\t        self.gates: tf.Variable = None\n\t        self.gates_bias: tf.Variable = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        input_size = input_shape[-1]\n\t        scale = (3 / (input_size + self.hidden_size)) ** 0.5\n", "        self.gates = self.add_weight(\n\t            \"gates\",\n\t            shape=(2, input_size + self.hidden_size, self.hidden_size),\n\t            initializer=keras.initializers.random_uniform(\n\t                -scale, scale, seed=self.seed\n\t            ),\n\t        )\n\t        self.gates_bias = self.add_weight(\n\t            \"gates_bias\",\n\t            shape=(2, self.hidden_size),\n", "            initializer=keras.initializers.zeros(),\n\t        )\n\t    def call(self, input: tf.Tensor, hidden: tf.Tensor) -> tf.Tensor:\n\t        transform, update = tf.unstack(\n\t            tf.concat([input, hidden], axis=1) @ self.gates\n\t            + self.gates_bias[:, tf.newaxis]\n\t        )\n\t        update = tf.sigmoid(update + self.update_rebias)\n\t        return (1 - update) * hidden + update * tf.tanh(transform)\n\tclass RNN(keras.layers.Layer):  # type:ignore[misc]\n", "    \"\"\"A basic, unidirectional RNN.\n\t    Expects inputs of shape (batch, sequence, feature), and produces outputs of shape\n\t    (batch, sequence, hidden).\n\t    Note that this implementation has patholological memory usage on IPU, due to missing\n\t    recomputation.\n\t    \"\"\"\n\t    def __init__(self, cell: keras.layers.Layer):\n\t        super().__init__(name=type(self).__name__, dtype=cell.dtype)\n\t        self.cell = cell\n\t        self.initial_hidden: tf.Variable = None\n", "    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.cell.build(tf.TensorShape([input_shape[0], input_shape[2]]))\n\t        self.initial_hidden = self.add_weight(\n\t            \"initial_hidden\",\n\t            shape=(self.cell.hidden_size,),\n\t            initializer=keras.initializers.zeros(),\n\t        )\n\t    def call(self, input: tf.Tensor) -> tf.Tensor:\n\t        # Note: sbh = (sequence, batch, hidden)\n", "        input_sbh = tf.transpose(input, (1, 0, 2))\n\t        output_sbh = tf.scan(\n\t            lambda hidden, input: self.cell(input, hidden),\n\t            input_sbh,\n\t            initializer=tf.tile(self.initial_hidden[tf.newaxis], (input.shape[0], 1)),\n\t        )\n\t        return tf.transpose(output_sbh, (1, 0, 2))\n\t####################\n\t# Optimizers\n\tclass _Optimizer(keras.optimizers.Optimizer):  # type:ignore[misc]\n", "    \"\"\"A small extension of the keras base optimizer.\"\"\"\n\t    # pylint:disable=too-few-public-methods\n\t    def __init__(self, name: str):\n\t        super().__init__(name=name)\n\t        self._step_variable: tf.Variable = None\n\t    @property\n\t    def _step(self) -> tf.Variable:\n\t        if self._step_variable is None:\n\t            with tf.name_scope(self._name):\n\t                self._step_variable = tf.Variable(\n", "                    0, dtype=tf.int32, name=\"step\", trainable=False\n\t                )\n\t        return self._step_variable\n\t    @staticmethod\n\t    def _hyperparameter(value: float) -> tf.Variable:\n\t        \"\"\"Create a training hyperparmaeter, as a Variable (for sake of executable caching).\"\"\"\n\t        return tf.Variable(value, dtype=tf.float32, trainable=False)\n\t    def _add_slot_with_dtype(\n\t        self, variable: tf.Variable, name: str, dtype: tf.DType\n\t    ) -> tf.Variable:\n", "        # pylint:disable=protected-access\n\t        key = variable._shared_name if variable._in_graph_mode else variable._unique_id\n\t        result = self._slots.setdefault(key, {}).get(name)\n\t        if result is None:\n\t            result = tf.Variable(\n\t                tf.zeros(variable.shape, dtype=dtype),\n\t                name=f\"{key}/{name}\",\n\t                trainable=False,\n\t            )\n\t            self._slots[key][name] = result\n", "        return result\n\tclass SgdM(_Optimizer):\n\t    \"\"\"SGD with momentum and loss scaling support.\"\"\"\n\t    # pylint:disable=too-few-public-methods\n\t    def __init__(\n\t        self,\n\t        learning_rate: float = 0.01,\n\t        learning_rate_decay: float = 0.0,\n\t        scale_vector_learning_rate: bool = False,\n\t        loss_scale: float = 1,\n", "        momentum: float = 0,\n\t        name: str = \"SGD\",\n\t    ):\n\t        super().__init__(name=name)\n\t        self.learning_rate = self._hyperparameter(learning_rate)\n\t        self.learning_rate_decay = self._hyperparameter(learning_rate_decay)\n\t        self.scale_vector_learning_rate = scale_vector_learning_rate\n\t        self.loss_scale = self._hyperparameter(loss_scale)\n\t        self.momentum = self._hyperparameter(momentum)\n\t    def _update(\n", "        self, gradient: tf.Tensor, variable: tf.Variable, scale: tf.Tensor\n\t    ) -> List[tf.Operation]:\n\t        if isinstance(gradient, tf.IndexedSlices):\n\t            # Convert to dense gradient, which is probably fine\n\t            gradient = tf.math.unsorted_segment_sum(\n\t                gradient.values, gradient.indices, gradient.shape[0]\n\t            )\n\t        with tf.name_scope(self._name):\n\t            momentum_prev = self._add_slot_with_dtype(\n\t                variable, \"momentum\", dtype=variable.dtype\n", "            )\n\t        # This FP32 dance probably isn't of much importance/help here\n\t        momentum_next = self.momentum * tf.cast(momentum_prev, tf.float32) + tf.cast(\n\t            gradient, tf.float32\n\t        )\n\t        if self.scale_vector_learning_rate and len(variable.shape) == 1:\n\t            scale = scale / np.sqrt(variable.shape[0])\n\t        variable_next = tf.cast(variable, tf.float32) - scale * momentum_next\n\t        return [\n\t            variable.assign(tf.cast(variable_next, variable.dtype)),\n", "            momentum_prev.assign(tf.cast(momentum_next, momentum_prev.dtype)),\n\t        ]\n\t    def apply_gradients(\n\t        self,\n\t        grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]],\n\t        name: Optional[str] = None,\n\t    ) -> tf.Operation:\n\t        step_prev = self._step\n\t        decay = tf.pow(2.0, -tf.cast(step_prev, tf.float32) * self.learning_rate_decay)\n\t        step = step_prev.assign(step_prev + 1)\n", "        return tf.group(\n\t            step,\n\t            *(\n\t                self._update(\n\t                    grad, variable, scale=self.learning_rate * decay / self.loss_scale\n\t                )\n\t                for grad, variable in grads_and_vars\n\t            ),\n\t            name=name,\n\t        )\n", "class AdamW(_Optimizer):\n\t    \"\"\"AdamW (https://arxiv.org/abs/1711.05101).\"\"\"\n\t    # pylint:disable=too-few-public-methods\n\t    def __init__(  # pylint:disable=too-many-arguments\n\t        self,\n\t        learning_rate: float = 0.001,\n\t        learning_rate_decay: float = 0.0,\n\t        scale_vector_learning_rate: bool = False,\n\t        weight_decay: float = 0.004,\n\t        beta_1: float = 0.9,\n", "        beta_2: float = 0.999,\n\t        epsilon: float = 1e-7,\n\t        name: str = \"AdamW\",\n\t    ):\n\t        super().__init__(name=name)\n\t        self.learning_rate = self._hyperparameter(learning_rate)\n\t        self.learning_rate_decay = self._hyperparameter(learning_rate_decay)\n\t        self.scale_vector_learning_rate = scale_vector_learning_rate\n\t        self.weight_decay = self._hyperparameter(weight_decay)\n\t        self.beta_1 = self._hyperparameter(beta_1)\n", "        self.beta_2 = self._hyperparameter(beta_2)\n\t        self.epsilon = self._hyperparameter(epsilon)\n\t    def _update(\n\t        self, gradient: tf.Tensor, variable: tf.Variable, scale: tf.Tensor\n\t    ) -> List[tf.Operation]:\n\t        if self.scale_vector_learning_rate and len(variable.shape) == 1:\n\t            scale = scale / np.sqrt(variable.shape[0])\n\t        if isinstance(gradient, tf.IndexedSlices):\n\t            # Convert to dense gradient, which is probably fine\n\t            gradient = tf.math.unsorted_segment_sum(\n", "                gradient.values, gradient.indices, gradient.shape[0]\n\t            )\n\t        with tf.name_scope(self._name):\n\t            m_prev = self._add_slot_with_dtype(variable, \"adam_m\", dtype=variable.dtype)\n\t            v_prev = self._add_slot_with_dtype(variable, \"adam_v\", dtype=tf.float32)\n\t        gradient_fp32 = tf.cast(gradient, tf.float32)\n\t        m_next = (\n\t            self.beta_1 * tf.cast(m_prev, tf.float32)\n\t            + (1 - self.beta_1) * gradient_fp32\n\t        )\n", "        v_next = (\n\t            self.beta_2 * tf.cast(v_prev, tf.float32)\n\t            + (1 - self.beta_2) * gradient_fp32**2\n\t        )\n\t        variable_fp32 = tf.cast(variable, tf.float32)\n\t        variable_next = (\n\t            variable_fp32\n\t            - self.learning_rate * self.weight_decay * variable_fp32\n\t            - scale * m_next / (tf.sqrt(v_next) + self.epsilon)\n\t        )\n", "        return [\n\t            variable.assign(tf.cast(variable_next, variable.dtype)),\n\t            m_prev.assign(tf.cast(m_next, m_prev.dtype)),\n\t            v_prev.assign(tf.cast(v_next, v_prev.dtype)),\n\t        ]\n\t    def apply_gradients(\n\t        self,\n\t        grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]],\n\t        name: Optional[str] = None,\n\t    ) -> tf.Operation:\n", "        step_prev = self._step\n\t        decay = tf.pow(2.0, -tf.cast(step_prev, tf.float32) * self.learning_rate_decay)\n\t        step = step_prev.assign(step_prev + 1)\n\t        scale = (\n\t            self.learning_rate\n\t            * decay\n\t            * tf.sqrt(1 - self.beta_2 ** tf.cast(step, tf.float32))\n\t            / (1 - self.beta_1 ** tf.cast(step, tf.float32))\n\t        )\n\t        return tf.group(\n", "            step,\n\t            *(\n\t                self._update(grad, variable, scale=scale)\n\t                for grad, variable in grads_and_vars\n\t            ),\n\t            name=name,\n\t        )\n"]}
{"filename": "scmm/models.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Core model definitions.\"\"\"\n\timport itertools as it\n\tfrom dataclasses import dataclass\n\tfrom typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Union\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tfrom . import layers, uscale\n\tfrom .pedal import utility, xpu\n", "@dataclass\n\tclass Residual:\n\t    \"\"\"Residual settings.\"\"\"\n\t    norm: Optional[str]  # None | \"pre\" | \"post\"\n\t    alpha: Union[None, str, float]  # None | \"mean\" | <float>\n\t@dataclass\n\tclass Conv:\n\t    \"\"\"Convolution (sequence mixing) settings.\"\"\"\n\t    kernel_size: int\n\t    groups: int\n", "    kind: str = \"conv\"\n\t@dataclass\n\tclass Attention:\n\t    \"\"\"Attention (sequence mixing) settings.\"\"\"\n\t    heads: int\n\t    head_size: int\n\t    frequencies: int\n\t    max_period: int\n\t    kind: str = \"attention\"\n\t@dataclass\n", "class RNN:\n\t    \"\"\"Recurrence (sequence mixing) settings.\"\"\"\n\t    rebias: float\n\t    kind: str = \"rnn\"\n\t@dataclass\n\tclass FFN:\n\t    \"\"\"FFN (token mixing) settings.\"\"\"\n\t    multiple: float\n\t    kind: str = \"ffn\"\n\t@dataclass\n", "class Settings:\n\t    \"\"\"Model configuration.\"\"\"\n\t    vocab_size: int\n\t    hidden_size: int\n\t    depth: int\n\t    residual: Optional[Residual]\n\t    sequence: Union[Conv, Attention, RNN]\n\t    token: Optional[FFN]\n\t    dtype: str\n\t    seed: int\n", "class _ModelFactory:  # pylint:disable=missing-function-docstring\n\t    \"\"\"Builds the various kinds of model from settings.\"\"\"\n\t    def __init__(self, settings: Settings, unit_scale: bool, seeds: Iterator[int]):\n\t        self.settings = settings\n\t        self.unit_scale = unit_scale\n\t        self.dtype = tf.as_dtype(settings.dtype)\n\t        self.seeds = seeds\n\t    def kernel_initializer(self) -> keras.initializers.Initializer:\n\t        assert not self.unit_scale, \"unit scale shouldn't use Glorot\"\n\t        return keras.initializers.GlorotUniform(seed=next(self.seeds))\n", "    def embed(self) -> keras.layers.Layer:\n\t        if self.unit_scale:\n\t            return uscale.layers.Embedding(\n\t                self.settings.vocab_size,\n\t                self.settings.hidden_size,\n\t                dtype=self.dtype,\n\t                seed=next(self.seeds),\n\t            )\n\t        # Unit variance embeddings make sense in any case\n\t        return keras.layers.Embedding(\n", "            self.settings.vocab_size,\n\t            self.settings.hidden_size,\n\t            dtype=self.dtype,\n\t            embeddings_initializer=keras.initializers.RandomUniform(\n\t                -np.sqrt(3), np.sqrt(3), seed=next(self.seeds)\n\t            ),\n\t        )\n\t    def conv(self, settings: Conv) -> keras.layers.Layer:\n\t        if self.unit_scale:\n\t            return uscale.layers.CausalConv1D(\n", "                self.settings.hidden_size,\n\t                kernel_size=settings.kernel_size,\n\t                groups=settings.groups,\n\t                activation=\"relu\",\n\t                dtype=self.dtype,\n\t                seed=next(self.seeds),\n\t            )\n\t        return keras.layers.Conv1D(\n\t            self.settings.hidden_size,\n\t            kernel_size=settings.kernel_size,\n", "            groups=settings.groups,\n\t            activation=\"relu\",\n\t            padding=\"causal\",\n\t            dtype=self.dtype,\n\t            kernel_initializer=self.kernel_initializer(),\n\t        )\n\t    def attention(self, settings: Attention) -> keras.layers.Layer:\n\t        cls = (\n\t            uscale.layers.MultiHeadAttention\n\t            if self.unit_scale\n", "            else layers.MultiHeadAttention\n\t        )\n\t        return cls(\n\t            heads=settings.heads,\n\t            head_size=settings.head_size,\n\t            frequencies=settings.frequencies,\n\t            max_period=settings.max_period,\n\t            dtype=self.dtype,\n\t            seeds=(next(self.seeds), next(self.seeds), next(self.seeds)),\n\t        )\n", "    def rnn(self, settings: RNN) -> keras.layers.Layer:\n\t        (cls, cell_cls) = (\n\t            (uscale.layers.RNN, uscale.layers.RecurrentHighwayCell)\n\t            if self.unit_scale\n\t            else (layers.RNN, layers.RecurrentHighwayCell)\n\t        )\n\t        return cls(\n\t            cell_cls(\n\t                hidden_size=self.settings.hidden_size,\n\t                rebias=settings.rebias,\n", "                dtype=self.dtype,\n\t                seed=next(self.seeds),\n\t            )\n\t        )\n\t    def sequence_layer(self) -> keras.layers.Layer:\n\t        if isinstance(self.settings.sequence, Conv):\n\t            return self.conv(self.settings.sequence)\n\t        if isinstance(self.settings.sequence, Attention):\n\t            return self.attention(self.settings.sequence)\n\t        if isinstance(self.settings.sequence, RNN):\n", "            return self.rnn(self.settings.sequence)\n\t        assert False, f\"unexpected sequence settings {self.settings.sequence}\"\n\t    def token_layer(self) -> keras.layers.Layer:\n\t        assert self.settings.token is not None\n\t        cls = uscale.layers.FFNLayer if self.unit_scale else layers.FFNLayer\n\t        return cls(\n\t            self.settings.token.multiple,\n\t            dtype=self.dtype,\n\t            seeds=(next(self.seeds), next(self.seeds)),\n\t        )\n", "    def residual(self, body: keras.layers.Layer, index: int) -> keras.layers.Layer:\n\t        if self.settings.residual is None:\n\t            return body\n\t        if self.settings.residual.alpha is None:\n\t            alpha = None\n\t        elif self.settings.residual.alpha == \"mean\":\n\t            alpha = 1 / (1 + index)\n\t        elif isinstance(self.settings.residual.alpha, (float, int)):\n\t            alpha = self.settings.residual.alpha\n\t        else:\n", "            assert False, f\"unexpected residual.alpha {self.settings.residual.alpha}\"\n\t        layer_cls = (\n\t            uscale.layers.ResidualLayer if self.unit_scale else layers.ResidualLayer\n\t        )\n\t        return layer_cls(\n\t            body, norm_type=self.settings.residual.norm, alpha=alpha, dtype=self.dtype\n\t        )\n\t    def trunk_layer(self, index: Iterator[int]) -> keras.layers.Layer:\n\t        # Relying heavily on dict ordering...\n\t        parts = dict(sequence=self.residual(self.sequence_layer(), next(index)))\n", "        if self.settings.token:\n\t            parts[\"token\"] = self.residual(self.token_layer(), next(index))\n\t        return layers.Isotropic(dtype=self.dtype, **parts)\n\t    def trunk(self) -> List[keras.layers.Layer]:\n\t        index = iter(it.count())\n\t        return [self.trunk_layer(index) for _ in range(self.settings.depth)]\n\t    def norm(self) -> keras.layers.Layer:\n\t        return (\n\t            uscale.layers.LayerNormalization(dtype=self.dtype)\n\t            if self.unit_scale\n", "            else layers.LayerNormalization(dtype=self.dtype)\n\t        )\n\t    def predict(self) -> keras.layers.Layer:\n\t        if self.unit_scale:\n\t            return uscale.layers.Dense(\n\t                self.settings.vocab_size,\n\t                scale_for=\"separate\",\n\t                dtype=self.dtype,\n\t                seed=next(self.seeds),\n\t            )\n", "        return keras.layers.Dense(\n\t            self.settings.vocab_size,\n\t            dtype=self.dtype,\n\t            kernel_initializer=self.kernel_initializer(),\n\t        )\n\t    def predict_padding(self) -> keras.layers.Layer:\n\t        if self.unit_scale:\n\t            return uscale.layers.PadAndShiftLayer(dtype=self.dtype)\n\t        return layers.PadAndShiftLayer(dtype=self.dtype)\n\t    def loss(\n", "        self,\n\t    ) -> Callable[[tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor, tf.Tensor]]:\n\t        return (\n\t            uscale.ops.softmax_cross_entropy\n\t            if self.unit_scale\n\t            else layers.softmax_cross_entropy\n\t        )\n\tclass Model(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"Base language model.\"\"\"\n\t    # pylint:disable=too-many-instance-attributes\n", "    def __init__(self, settings: Settings, unit_scale: bool):\n\t        super().__init__()\n\t        self.settings = settings\n\t        factory = _ModelFactory(\n\t            settings,\n\t            unit_scale=unit_scale,\n\t            seeds=iter(utility.split_seed(settings.seed, 1000)),  # plenty of seeds\n\t        )\n\t        self.embed = factory.embed()\n\t        self.embed_norm = factory.norm()\n", "        self.trunk = factory.trunk()\n\t        self.norm = factory.norm()\n\t        self.predict = factory.predict()\n\t        self.predict_padding = factory.predict_padding()\n\t        self.loss = factory.loss()\n\t        # Our base model is always pre-built\n\t        self.build((None, None))\n\t        for name, layer in utility.named_layers(self):\n\t            assert layer.built, f\"layer {name} ({layer}) was not built\"\n\t        for layer in self.trunk:\n", "            xpu.current_context().outline(layer)\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.embed.build(input_shape)\n\t        hidden_shape = tuple(input_shape) + (self.settings.hidden_size,)\n\t        self.embed_norm.build(hidden_shape)\n\t        for layer in self.trunk:\n\t            layer.build(hidden_shape)\n\t        self.norm.build(hidden_shape)\n\t        self.predict.build(hidden_shape)\n", "        prediction_shape = tuple(input_shape) + (self.settings.vocab_size,)\n\t        self.predict_padding.build(prediction_shape)\n\t    def run(self, tokens: tf.Tensor, mask: tf.Tensor) -> Dict[str, tf.Tensor]:\n\t        \"\"\"Run the language model for cross entropy loss.\"\"\"\n\t        hiddens = self.embed_norm(self.embed(tokens))\n\t        for layer in self.trunk:\n\t            hiddens = layer(hiddens)\n\t        scores = self.predict_padding(self.predict(self.norm(hiddens)))\n\t        loss, n_tokens = self.loss(scores, tokens, mask)\n\t        return dict(\n", "            loss=loss,\n\t            n_tokens=n_tokens,\n\t            act_hiddens_final=tf.math.reduce_std(hiddens),\n\t        )\n\t    def weight_stats(self) -> Dict[str, Any]:\n\t        \"\"\"Stats regarding weights in the model.\"\"\"\n\t        shapes = {k: tuple(v.shape) for k, v in utility.named_weights(self)}\n\t        return dict(\n\t            n_weights=sum(np.prod(v) for v in shapes.values()),\n\t            n_weights_no_embedding=sum(\n", "                np.prod(v)\n\t                for k, v in shapes.items()\n\t                if k not in {\"embed.embeddings\", \"predict.kernel\"}\n\t            ),\n\t            weight_shapes=shapes,\n\t        )\n\t    def save(self) -> Dict[str, np.ndarray]:\n\t        \"\"\"Save model weights to a dictionary of numpy arrays.\"\"\"\n\t        return {k: np.array(v) for k, v in utility.named_weights(self)}\n\t    def load(self, weights: Dict[str, np.ndarray]) -> None:\n", "        \"\"\"Load model weights from a dictionary of numpy arrays.\"\"\"\n\t        variables = dict(utility.named_weights(self))\n\t        if variables.keys() != weights.keys():\n\t            raise ValueError(\n\t                \"Load does not set correct weights\"\n\t                f\", extra: {weights.keys() - variables.keys()}\"\n\t                f\", missing: {variables.keys() - weights.keys()}\"\n\t            )\n\t        for k in weights:\n\t            variables[k].assign(weights[k])\n"]}
{"filename": "scmm/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Scaled matmuls experimentation.\"\"\"\n\tfrom . import (  # NOQA: F401\n\t    datasets,\n\t    experiments,\n\t    layers,\n\t    models,\n\t    pedal,\n\t    training,\n\t    uscale,\n", ")\n"]}
{"filename": "scmm/training.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Top-line training logic.\"\"\"\n\timport collections\n\timport dataclasses\n\timport datetime\n\timport itertools as it\n\tfrom dataclasses import dataclass\n\tfrom typing import Any, Dict, Iterable, Optional, Union\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n", "from . import datasets, layers, models, uscale\n\tfrom .pedal import xpu\n\t@dataclass\n\tclass SgdM:\n\t    \"\"\"SGD with momentum.\"\"\"\n\t    learning_rate: float\n\t    learning_rate_decay: float\n\t    momentum: float\n\t    kind: str = \"sgdm\"\n\t@dataclass\n", "class AdamW:\n\t    \"\"\"Adam with weight decay.\"\"\"\n\t    learning_rate: float\n\t    learning_rate_decay: float\n\t    beta_1: float = 0.9\n\t    beta_2: float = 0.999\n\t    weight_decay: float = 0\n\t    kind: str = \"adamw\"\n\tOptimiser = Union[AdamW, SgdM]\n\t@dataclass\n", "class Settings:\n\t    \"\"\"Training settings.\"\"\"\n\t    batch: datasets.BatchSettings\n\t    steps: int\n\t    valid_interval: Optional[int]\n\t    optimiser: Optimiser\n\t    loss_scale: float\n\tdef eval_summary(results: Iterable[datasets.Batch]) -> Dict[str, float]:\n\t    \"\"\"Summarise evaluation results, weighted averages according to \"n_tokens\".\"\"\"\n\t    total_tokens = 0\n", "    stats: Dict[str, float] = collections.defaultdict(float)\n\t    for result in results:\n\t        n_tokens = int(result[\"n_tokens\"])\n\t        total_tokens += n_tokens\n\t        for key, value in result.items():\n\t            if key != \"n_tokens\":\n\t                stats[key] += value * n_tokens\n\t    # Normalisation\n\t    for key in stats:\n\t        stats[key] /= total_tokens\n", "    stats[\"n_tokens\"] = total_tokens\n\t    return stats\n\tdef _get_optimiser(\n\t    settings: Optimiser,\n\t    loss_scale: float,\n\t    unit_scale: bool,\n\t) -> keras.optimizers.Optimizer:\n\t    if isinstance(settings, AdamW):\n\t        # Note that Adam updates are invariant to fixed gradient scale, so\n\t        # loss_scale is safely ignored\n", "        return layers.AdamW(\n\t            learning_rate=settings.learning_rate,\n\t            learning_rate_decay=settings.learning_rate_decay,\n\t            scale_vector_learning_rate=unit_scale,\n\t            weight_decay=settings.weight_decay,\n\t            beta_1=settings.beta_1,\n\t            beta_2=settings.beta_2,\n\t        )\n\t    if isinstance(settings, SgdM):\n\t        return layers.SgdM(\n", "            learning_rate=settings.learning_rate,\n\t            learning_rate_decay=settings.learning_rate_decay,\n\t            scale_vector_learning_rate=unit_scale,\n\t            loss_scale=loss_scale,\n\t            momentum=settings.momentum,\n\t        )\n\t    assert False, f\"unknown optimiser {settings}\"\n\tdef train(\n\t    model: models.Model,\n\t    data: datasets.Data,\n", "    context: xpu.Context,\n\t    settings: Settings,\n\t    unit_scale: bool,\n\t) -> Iterable[Dict[str, Any]]:\n\t    \"\"\"Train a model.\"\"\"\n\t    assert (\n\t        settings.batch.loop_seed is not None\n\t    ), \"please specify a seed for training batches\"\n\t    optimiser = _get_optimiser(\n\t        settings.optimiser, loss_scale=settings.loss_scale, unit_scale=unit_scale\n", "    )\n\t    def _log(kind: str, step: int, data: Dict[str, Any]) -> Dict[str, Any]:\n\t        return dict(kind=kind, step=step, time=datetime.datetime.now(), **data)\n\t    def _validate(step: int, test: bool) -> Iterable[Dict[str, Any]]:\n\t        for part in [\"valid\", \"train\"] + ([\"test\"] if test else []):\n\t            batch_settings = settings.batch\n\t            if part in [\"valid\", \"test\"]:\n\t                batch_settings = dataclasses.replace(batch_settings, loop_seed=None)\n\t            batches = data.batches(part, batch_settings)\n\t            if part == \"train\":\n", "                batches = it.islice(\n\t                    batches,\n\t                    1 + data.parts[\"valid\"].size // settings.batch.target_tokens,\n\t                )\n\t            results = eval_summary(context.loop(model.run, batches))\n\t            results = {f\"{part}_{k}\": v for k, v in results.items()}\n\t            yield _log(f\"eval_{part}\", step, results)\n\t    def _training_step(**batch: tf.Tensor) -> Dict[str, tf.Tensor]:\n\t        with tf.GradientTape() as tape:\n\t            result = model.run(**batch)\n", "            loss = uscale.ops.scaling(backward=settings.loss_scale)(result[\"loss\"])\n\t        gradients = tape.gradient(loss, model.trainable_variables)\n\t        optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n\t        return result\n\t    train_steps = iter(\n\t        context.loop(_training_step, data.batches(\"train\", settings.batch))\n\t    )\n\t    for step in it.count():\n\t        if step >= settings.steps:\n\t            yield from _validate(step, test=True)\n", "            break\n\t        if settings.valid_interval is not None and step % settings.valid_interval == 0:\n\t            yield from _validate(step, test=False)\n\t        results = next(train_steps)  # pylint:disable=stop-iteration-return\n\t        yield _log(\"train_step\", step, {k: v.tolist() for k, v in results.items()})\n"]}
{"filename": "scmm/datasets.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Data loading and batching.\"\"\"\n\timport itertools as it\n\timport json\n\tfrom dataclasses import dataclass\n\tfrom pathlib import Path\n\tfrom typing import Dict, Iterable, Optional, Tuple\n\timport numpy as np\n\tBatch = Dict[str, np.ndarray]\n\tVocab = Tuple[str, ...]\n", "def to_ids(data: str, vocab: Vocab, dtype: np.dtype) -> np.ndarray:\n\t    \"\"\"Use a (complete) vocabulary to map characters onto term IDs.\"\"\"\n\t    assert len(vocab) < np.iinfo(dtype).max\n\t    ch_to_idx = {ch: idx for idx, ch in enumerate(vocab)}\n\t    return np.array([ch_to_idx[ch] for ch in data], dtype=dtype)\n\tdef to_str(terms: np.ndarray, vocab: Vocab) -> str:\n\t    \"\"\"Map term IDs back to characters.\"\"\"\n\t    return \"\".join(vocab[idx] for idx in terms)\n\t@dataclass\n\tclass BatchSettings:\n", "    \"\"\"Settings for a stream of batches.\"\"\"\n\t    sequences: int\n\t    sequence_length: int\n\t    overlap_length: int\n\t    loop_seed: Optional[int]\n\t    @property\n\t    def shape(self) -> Tuple[int, int]:\n\t        \"\"\"The 2D shape of a batch.\"\"\"\n\t        return (self.sequences, self.sequence_length)\n\t    @property\n", "    def target_length(self) -> int:\n\t        \"\"\"The maximum number of target tokens per sequence.\"\"\"\n\t        return self.sequence_length - self.overlap_length\n\t    @property\n\t    def target_tokens(self) -> int:\n\t        \"\"\"The maximum number of target tokens.\"\"\"\n\t        return self.sequences * self.target_length\n\t@dataclass\n\tclass Data:\n\t    \"\"\"A dataset that can generate batches.\"\"\"\n", "    vocab: Vocab\n\t    parts: Dict[str, np.ndarray]\n\t    def batches(self, part: str, settings: BatchSettings) -> Iterable[Batch]:\n\t        \"\"\"Batch with overlapping sequences.\n\t        Note - if `loop_seed` is non-None, generates an infinite stream of batches, sampled\n\t        with replacement.\n\t        \"\"\"\n\t        data = self.parts[part]\n\t        batch_tokens = []\n\t        batch_mask = []\n", "        idxs = np.arange(settings.sequence_length)\n\t        if settings.loop_seed is None:\n\t            starts: Iterable[int] = range(0, len(data), settings.target_length)\n\t        else:\n\t            random = np.random.Generator(np.random.PCG64(settings.loop_seed))\n\t            starts = (\n\t                random.integers(len(data) - settings.target_length) for _ in it.count()\n\t            )\n\t        for start in starts:\n\t            begin = max(0, start - settings.overlap_length)\n", "            sequence = data[begin : start + settings.target_length]\n\t            # \"token padding\"\n\t            npad = settings.sequence_length - len(sequence)\n\t            sequence = np.pad(sequence, ((0, npad),))\n\t            mask = ((start - begin) <= idxs) & (idxs < (len(sequence) - npad))\n\t            batch_tokens.append(sequence.astype(np.int32))\n\t            batch_mask.append(mask.astype(np.int32))\n\t            if settings.sequences <= len(batch_tokens):\n\t                yield dict(tokens=np.stack(batch_tokens), mask=np.stack(batch_mask))\n\t                batch_tokens.clear()\n", "                batch_mask.clear()\n\t        # Incomplete final batch - needs \"sequence padding\"\n\t        if batch_tokens:\n\t            npad = settings.sequences - len(batch_tokens)\n\t            batch_tokens.extend(npad * [batch_tokens[-1]])\n\t            batch_mask.extend(npad * [np.zeros_like(batch_mask[-1])])\n\t            yield dict(tokens=np.stack(batch_tokens), mask=np.stack(batch_mask))\n\tdef load_character(root: Path, **parts: str) -> Data:\n\t    \"\"\"Load a character-based dataset.\n\t    e.g. given parts=dict(train=\"train.txt\", valid=\"valid.txt\")\n", "    root/\n\t        vocab.json\n\t        train.txt\n\t        valid.txt\n\t    \"\"\"\n\t    vocab = tuple(json.loads((root / \"vocab.json\").read_text()))\n\t    return Data(\n\t        vocab=vocab,\n\t        parts={\n\t            name: to_ids((root / path).read_text(\"utf8\"), vocab, np.uint16)\n", "            for name, path in parts.items()\n\t        },\n\t    )\n"]}
{"filename": "scmm/experiments.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Top-level experiment running.\"\"\"\n\timport contextlib\n\timport copy\n\timport dataclasses\n\timport itertools as it\n\timport json\n\timport os\n\timport sys\n\tfrom dataclasses import dataclass\n", "from pathlib import Path\n\tfrom typing import Any, Dict, Generator, Iterable, Optional, Tuple\n\timport numpy as np\n\timport wandb\n\tfrom . import datasets, models, training\n\tfrom .pedal import utility, xpu\n\t@contextlib.contextmanager\n\tdef log_wandb() -> Generator[utility.Logger, None, None]:\n\t    \"\"\"Log to weights & biases.\"\"\"\n\t    def _log(item: Dict[str, Any]) -> None:\n", "        if item[\"kind\"] == \"settings\":\n\t            Path(\"out\").mkdir(exist_ok=True, parents=True)\n\t            wandb.init(\n\t                config=utility.remove_keys(item, \"kind\"),\n\t                dir=\"out\",\n\t                project=\"scaled-matmuls\",\n\t                reinit=True,\n\t            )\n\t        elif item[\"kind\"] == \"stats\":\n\t            wandb.run.summary.update(  # type:ignore[union-attr]\n", "                utility.remove_keys(item, \"kind\")\n\t            )\n\t        elif item[\"kind\"] == \"train_step\":\n\t            pass  # skip training steps (too large)\n\t        else:\n\t            wandb.log(item, step=item[\"step\"])\n\t    try:\n\t        yield _log\n\t    except Exception as exc:\n\t        wandb.run.summary.update(dict(error=repr(exc)))  # type:ignore[union-attr]\n", "        wandb.finish(1)\n\t        raise\n\t    # Always call finish(), otherwise we hang (when started in a subprocess from a sweep)\n\t    wandb.finish(0)\n\t@contextlib.contextmanager\n\tdef log_jsonl(path: Path) -> Generator[utility.Logger, None, None]:\n\t    \"\"\"Log to file.\"\"\"\n\t    path.parent.mkdir(exist_ok=True, parents=True)\n\t    with path.open(\"w\") as f:\n\t        yield lambda item: print(json.dumps(item, default=utility.to_jsonable), file=f)\n", "def log_checkpoint(path: Path, model: models.Model) -> utility.Logger:\n\t    \"\"\"Save model checkpoints whenever validation runs.\"\"\"\n\t    def log(item: Dict[str, Any]) -> None:\n\t        if item[\"kind\"] == \"eval_valid\":\n\t            np.savez(path, step=item[\"step\"], **model.save())\n\t    return log\n\tdef log_stderr(item: Dict[str, Any]) -> None:\n\t    \"\"\"Log to terminal.\"\"\"\n\t    if item[\"kind\"] == \"train_step\":\n\t        return\n", "    print(\n\t        str(item) + \" \" * 20,\n\t        file=sys.stderr,\n\t        end=\"\\r\" if item[\"kind\"] == \"train_step\" else \"\\n\",\n\t    )\n\t@dataclass\n\tclass DataSettings:\n\t    \"\"\"Dataset settings.\"\"\"\n\t    path: Path\n\t    kind: str = \"wikitext-103-raw\"\n", "@dataclass\n\tclass OutputSettings:\n\t    \"\"\"Output control settings.\"\"\"\n\t    stderr: bool\n\t    wandb: bool\n\t    log: Optional[Path]\n\t    checkpoint: Optional[Path]\n\t@dataclass\n\tclass Settings:\n\t    \"\"\"Top-level settings.\"\"\"\n", "    data: DataSettings\n\t    model: models.Settings\n\t    training: training.Settings\n\t    unit_scale: Optional[str]\n\t    target: xpu.Settings\n\t    output: OutputSettings\n\t    metadata: Dict[str, Any]\n\t    seed: int\n\t    def set_defaults(self, data: datasets.Data) -> None:\n\t        \"\"\"Fill in all optional fields.\"\"\"\n", "        # Seeds\n\t        if self.seed is None:\n\t            self.seed = int(np.random.SeedSequence().generate_state(1)[0])\n\t        model_seed, batching_seed = utility.split_seed(self.seed, 2)\n\t        if self.model.seed is None:\n\t            self.model.seed = model_seed\n\t        if self.training.batch.loop_seed is None:\n\t            self.training.batch.loop_seed = batching_seed\n\t        # Model\n\t        if self.model.vocab_size is None:\n", "            self.model.vocab_size = len(data.vocab)\n\t        # Metadata\n\t        if \"SSUB_UID\" in os.environ:\n\t            self.metadata.setdefault(\"ssub_id\", os.environ[\"SSUB_UID\"])\n\t        if \"SLURM_JOB_ID\" in os.environ:\n\t            self.metadata.setdefault(\"slurm_job_id\", os.environ[\"SLURM_JOB_ID\"])\n\tdef _loggers(settings: OutputSettings, model: models.Model) -> Iterable[utility.Logger]:\n\t    if settings.stderr:\n\t        yield log_stderr\n\t    if settings.wandb:\n", "        yield log_wandb()\n\t    if settings.log:\n\t        yield log_jsonl(settings.log)\n\t    if settings.checkpoint:\n\t        yield log_checkpoint(settings.checkpoint, model)\n\tdef _settings_line(settings: Settings) -> Dict[str, Any]:\n\t    return dict(\n\t        kind=\"settings\",\n\t        **utility.remove_keys(dataclasses.asdict(settings), \"output\"),\n\t    )\n", "def run(settings: Settings) -> Dict[str, Any]:\n\t    \"\"\"Run an experiment, logging results as requested.\"\"\"\n\t    data = datasets.load_character(\n\t        settings.data.path, train=\"train.txt\", valid=\"valid.txt\", test=\"test.txt\"\n\t    )\n\t    settings = copy.deepcopy(settings)\n\t    settings.set_defaults(data)\n\t    assert settings.unit_scale in {None, \"0.4\"}\n\t    last_eval_valid: Optional[Dict[str, Any]] = None\n\t    with xpu.context(settings.target) as context:\n", "        model = models.Model(settings.model, unit_scale=bool(settings.unit_scale))\n\t        with utility.logging(*_loggers(settings.output, model)) as log:\n\t            log(_settings_line(settings))\n\t            log(dict(kind=\"stats\", **model.weight_stats()))\n\t            for item in training.train(\n\t                model, data, context, settings.training, bool(settings.unit_scale)\n\t            ):\n\t                log(item)\n\t                if item[\"kind\"] == \"eval_valid\":\n\t                    last_eval_valid = item\n", "    assert last_eval_valid is not None\n\t    return last_eval_valid\n\t####################\n\t# LR sweep\n\t@dataclass\n\tclass LrSweep:\n\t    \"\"\"Learning rate sweep settings.\"\"\"\n\t    base: Settings\n\t    step: float\n\t    threshold: float\n", "    reps: int\n\tdef find_learning_rate(\n\t    settings: LrSweep, run_in_subprocess: bool = True\n\t) -> Tuple[Settings, float]:\n\t    \"\"\"Perform a LR sweep, starting from `base`.\n\t    Tries: initial, initial * step, initial * step^2, ...\n\t    Until the validation loss is more than `best_loss + threshold`.\n\t    Run in subprocess (by default) for sake of isolation & memory use.\n\t    \"\"\"\n\t    def run_test(test_settings: Settings) -> float:\n", "        if run_in_subprocess:\n\t            # Run in subprocess for sake of isolation & memory use\n\t            results = utility.run_in_subprocess(\n\t                run, settings=test_settings\n\t            )  # pragma: no cover\n\t        else:\n\t            results = run(settings=test_settings)\n\t        return results[\"valid_loss\"]  # type:ignore[no-any-return]\n\t    best_loss, best_settings = None, None\n\t    for n in it.count():\n", "        test_settings = copy.deepcopy(settings.base)\n\t        test_settings.training.optimiser.learning_rate *= settings.step**n\n\t        loss = np.median([run_test(test_settings) for _ in range(settings.reps)])\n\t        print(\n\t            f\"LR {test_settings.training.optimiser.learning_rate} -> {loss}\",\n\t            file=sys.stderr,\n\t        )\n\t        if best_loss is None or loss < best_loss:\n\t            best_loss = loss\n\t            best_settings = test_settings\n", "        if np.isnan(loss) or best_loss + settings.threshold < loss:\n\t            return best_settings, best_loss\n\t    assert False, \"unreachable code (infinite loop)\"\n"]}
{"filename": "scmm/pedal/xpu.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"General utilities to unify CPU/IPU programming.\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom types import TracebackType\n\tfrom typing import Any, Callable, Dict, Iterable, Iterator, Optional, Type, Union\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\ttry:\n\t    from tensorflow.python import ipu\n", "    IPU = True\n\texcept ImportError:  # pragma: no cover\n\t    IPU = False\n\tFunction = Callable[..., Any]\n\tOperation = Callable[..., Dict[str, tf.Tensor]]\n\tBatch = Dict[str, np.ndarray]\n\tFunctionCache = Callable[[Any], Callable[[Function], Function]]\n\tdef _make_cache(**function_args: Any) -> FunctionCache:\n\t    \"\"\"Make a decorator that calls tf.function, with a user-keyed cache.\n\t    E.g.\n", "        cache = make_cache(experimental_compile=True)\n\t        body = ...\n\t        @cache(key=(\"model\", body))\n\t        def model(x: tf.Tensor) -> tf.Tensor:\n\t            return 2 * body(x)\n\t    \"\"\"\n\t    _cache: Dict[Any, Function] = {}\n\t    def wrap(key: Any) -> Callable[[Function], Function]:\n\t        def wrapper(fn: Operation) -> Operation:\n\t            if key not in _cache:\n", "                _cache[key] = tf.function(**function_args)(fn)\n\t            return _cache[key]\n\t        return wrapper\n\t    return wrap\n\t@dataclass\n\tclass CpuSettings:\n\t    \"\"\"CPU-specific settings.\"\"\"\n\t    compile: bool = False\n\t    type: str = \"cpu\"\n\t@dataclass\n", "class IpuSettings:\n\t    \"\"\"IPU-specific settings.\"\"\"\n\t    iterations_per_loop: int\n\t    available_memory_proportion: Optional[float] = None\n\t    stochastic_rounding: bool = False\n\t    type: str = \"ipu\"\n\tSettings = Union[CpuSettings, IpuSettings]\n\tclass Context:\n\t    \"\"\"Manages target setup and a cache for compiled functions.\"\"\"\n\t    _CURRENT: Optional[\"Context\"] = None\n", "    def __init__(self, strategy: tf.distribute.Strategy, compile: bool):\n\t        self.strategy = strategy\n\t        self._scope = self.strategy.scope()\n\t        self._cache = (\n\t            _make_cache(experimental_compile=True)\n\t            if compile\n\t            else (lambda key: lambda fn: fn)\n\t        )\n\t    def __enter__(self) -> \"Context\":\n\t        assert Context._CURRENT is None, \"xpu.context scopes cannot be nested\"\n", "        Context._CURRENT = self\n\t        self._scope.__enter__()\n\t        return self\n\t    def __exit__(\n\t        self,\n\t        exc_type: Optional[Type[BaseException]],\n\t        exc_val: Optional[BaseException],\n\t        exc_tb: Optional[TracebackType],\n\t    ) -> None:\n\t        self._scope.__exit__(exc_type, exc_val, exc_tb)\n", "        assert Context._CURRENT is self, \"exiting a scope with the wrong context\"\n\t        Context._CURRENT = None\n\t    def loop(self, operation: Operation, inputs: Iterable[Batch]) -> Iterable[Batch]:\n\t        \"\"\"Stream inputs into an operation and return all outputs.\n\t        operation -- callable as `result = operation(**input)`,\n\t                        where `result` is a `dict`\n\t        \"\"\"\n\t        return loop_cpu(operation, inputs, strategy=self.strategy, cache=self._cache)\n\t    @staticmethod\n\t    def outline(layer: keras.layers.Layer) -> None:\n", "        \"\"\"Mark a layer for outlining on IPU, do nothing on CPU.\"\"\"\n\tdef context(settings: Settings) -> Context:\n\t    \"\"\"Create an execution context with the given settings.\n\t    Should generally be used in an immediate `with` scope, e.g.\n\t        with xpu.context(xpu.CpuSettings(compile=False)) as context:\n\t            ...\n\t            # also accessible as xpu.current_context()\n\t    \"\"\"\n\t    if isinstance(settings, CpuSettings):\n\t        return Context(tf.distribute.OneDeviceStrategy(\"\"), compile=settings.compile)\n", "    if isinstance(settings, IpuSettings):\n\t        if not IPU:  # pragma: no cover\n\t            raise ValueError(\n\t                \"Cannot create IPU context - tensorflow.python.ipu could not be imported\"\n\t            )\n\t        return _create_ipu_context(settings)\n\t    assert False, f\"Unexpected Context settings type {settings}\"\n\tdef current_context() -> Context:\n\t    \"\"\"Get the currently in-scope Context.\"\"\"\n\t    # pylint:disable=protected-access\n", "    assert Context._CURRENT is not None, \"there is no context in scope\"\n\t    return Context._CURRENT\n\tdef loop_cpu(\n\t    operation: Operation,\n\t    inputs: Iterable[Batch],\n\t    strategy: tf.distribute.Strategy,\n\t    cache: FunctionCache,\n\t) -> Iterable[Batch]:\n\t    \"\"\"Stream inputs into an operation and return all outputs.\n\t    operation -- callable as `result = operation(**input)`,\n", "                    where `result` is a `dict`\n\t    \"\"\"\n\t    fn = cache(key=operation)(operation)  # type:ignore[call-arg]\n\t    for input_ in inputs:\n\t        yield {k: np.array(v) for k, v in strategy.run(fn, kwargs=input_).items()}\n\tif IPU:\n\t    class _IpuContext(Context):\n\t        def __init__(self, settings: IpuSettings):\n\t            super().__init__(ipu.ipu_strategy.IPUStrategy(), compile=True)\n\t            self.settings = settings\n", "        def loop(\n\t            self, operation: Operation, inputs: Iterable[Batch]\n\t        ) -> Iterable[Batch]:\n\t            return loop_ipu(\n\t                operation,\n\t                inputs,\n\t                strategy=self.strategy,\n\t                cache=self._cache,\n\t                iterations_per_loop=self.settings.iterations_per_loop,\n\t            )\n", "        @staticmethod\n\t        def outline(layer: keras.layers.Layer) -> None:\n\t            inner_call = layer.call\n\t            def outlined_call(*args: Any, **kwargs: Any) -> Any:\n\t                @ipu.outlined_function  # type:ignore[misc]\n\t                def call() -> Any:\n\t                    return inner_call(*args, **kwargs)\n\t                return call()\n\t            layer.call = outlined_call\n\t    def _create_ipu_context(settings: IpuSettings) -> Context:\n", "        config = ipu.config.IPUConfig()\n\t        config.auto_select_ipus = 1\n\t        config.floating_point_behaviour.esr = (\n\t            ipu.config.StochasticRoundingBehaviour.from_bool(\n\t                settings.stochastic_rounding\n\t            )\n\t        )\n\t        config.device_connection.type = ipu.config.DeviceConnectionType.ON_DEMAND\n\t        if settings.available_memory_proportion is not None:\n\t            config.matmuls.poplar_options[\"availableMemoryProportion\"] = str(\n", "                settings.available_memory_proportion\n\t            )\n\t        ipu.utils.configure_ipu_system(config)\n\t        return _IpuContext(settings)\n\t    def _padded_dataset(inputs: Iterable[Batch]) -> tf.data.Dataset:\n\t        iterator = iter(inputs)\n\t        head = next(iterator)\n\t        def generator() -> Iterable[Dict[str, np.ndarray]]:  # pragma: no cover\n\t            yield dict(**head, _pad=np.array(False))\n\t            for item in iterator:\n", "                yield dict(**item, _pad=np.array(False))\n\t            while True:  # padding\n\t                yield dict(**head, _pad=np.array(True))\n\t        signature = {\n\t            k: tf.TensorSpec(shape=v.shape, dtype=v.dtype) for k, v in head.items()\n\t        }\n\t        signature[\"_pad\"] = tf.TensorSpec(shape=(), dtype=np.bool)\n\t        return tf.data.Dataset.from_generator(generator, output_signature=signature)\n\t    def loop_ipu(\n\t        operation: Operation,\n", "        inputs: Iterable[Batch],\n\t        strategy: tf.distribute.Strategy,\n\t        cache: FunctionCache,\n\t        iterations_per_loop: int,\n\t    ) -> Iterable[Dict[str, np.ndarray]]:\n\t        \"\"\"Stream inputs into an operation and return all outputs.\n\t        operation -- callable as `result = operation(**input)`,\n\t                     where `result` is a `dict`\n\t        \"\"\"\n\t        @cache(  # type:ignore[call-arg]\n", "            key=(\"loop_ipu\", operation, iterations_per_loop)\n\t        )\n\t        def _loop(\n\t            iterator: Iterator[Dict[str, tf.Tensor]],\n\t            outfeed: ipu.ipu_outfeed_queue.IPUOutfeedQueue,\n\t        ) -> None:  # pragma: no cover\n\t            for _ in tf.range(iterations_per_loop):\n\t                batch = next(iterator)\n\t                pad = batch.pop(\"_pad\")\n\t                results = operation(**batch)\n", "                results[\"_pad\"] = pad\n\t                outfeed.enqueue(results)\n\t        iterator = iter(_padded_dataset(inputs))\n\t        outfeed = ipu.ipu_outfeed_queue.IPUOutfeedQueue()\n\t        while True:\n\t            strategy.run(_loop, (iterator, outfeed))\n\t            for item in outfeed:\n\t                if item.pop(\"_pad\"):\n\t                    # Prevent: Error occurred when finalizing GeneratorDataset iterator\n\t                    del iterator\n", "                    del outfeed\n\t                    return\n\t                yield {k: np.array(v) for k, v in item.items()}\n"]}
{"filename": "scmm/pedal/utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"General standalone utilities.\"\"\"\n\timport contextlib\n\timport datetime\n\timport multiprocessing\n\tfrom pathlib import Path\n\tfrom typing import (\n\t    Any,\n\t    Callable,\n\t    ContextManager,\n", "    Dict,\n\t    Generator,\n\t    Iterable,\n\t    Sequence,\n\t    Tuple,\n\t    TypeVar,\n\t    Union,\n\t)\n\timport numpy as np\n\timport tensorflow as tf\n", "from tensorflow import keras\n\tdef split_seed(seed: int, n: int) -> Tuple[int, ...]:\n\t    \"\"\"Split a random seed into n seeds.\n\t    Note that the original seed should not be used after calling this.\n\t    \"\"\"\n\t    return tuple(\n\t        int(seq.generate_state(1)[0]) for seq in np.random.SeedSequence(seed).spawn(n)\n\t    )\n\tT = TypeVar(\"T\")\n\tdef remove_keys(dict_: Dict[str, T], *keys: str) -> Dict[str, T]:\n", "    \"\"\"Return a new dictionary with specific keys removed.\"\"\"\n\t    return {k: v for k, v in dict_.items() if k not in keys}\n\tdef to_jsonable(obj: Any) -> Any:\n\t    \"\"\"A decent default=? function for json.dump.\"\"\"\n\t    if isinstance(obj, Path):\n\t        return str(obj)\n\t    if isinstance(obj, (np.ndarray, np.number)):\n\t        return obj.tolist()\n\t    if isinstance(obj, datetime.date):  # datetime.datetime is a subclass\n\t        return obj.isoformat()\n", "    raise TypeError(f\"Type '{type(obj).__name__}' is not JSON-serialisable\")\n\tLogger = Callable[..., None]\n\t@contextlib.contextmanager\n\tdef logging(\n\t    *loggers: Union[ContextManager[Logger], Logger]\n\t) -> Generator[Logger, None, None]:\n\t    \"\"\"A context manager that delegates logging calls to multiple \"loggers\".\n\t    Arguments are either:\n\t     - Callable actions\n\t     - Context managers that return callable actions\n", "    For example:\n\t        @contextlib.contextmanager\n\t        def log_to_file(path: Path) -> None:\n\t            with path.open(\"w\") as f:\n\t                yield lambda item: print(item, file=f)\n\t        with logging(print, log_to_file(Path(\"log.txt\"))) as log:\n\t            log(\"item one\")\n\t            log(\"item two\")\n\t    \"\"\"\n\t    with contextlib.ExitStack() as stack:\n", "        functions = [\n\t            stack.enter_context(logger)  # type:ignore[arg-type]\n\t            if hasattr(logger, \"__enter__\")\n\t            else logger\n\t            for logger in loggers\n\t        ]\n\t        def apply(*args: Any, **kwargs: Any) -> None:\n\t            for fn in functions:\n\t                fn(*args, **kwargs)\n\t        yield apply\n", "def named_layers(\n\t    layer: Union[keras.layers.Layer, Sequence[keras.layers.Layer]],\n\t    prefix: Tuple[str, ...] = (),\n\t) -> Iterable[Tuple[str, keras.layers.Layer]]:\n\t    \"\"\"Walk a layer, recursively trying to find sublayers.\"\"\"\n\t    if isinstance(layer, (list, tuple)):\n\t        for n, child in enumerate(layer):\n\t            yield from named_layers(child, prefix + (str(n),))\n\t    if isinstance(layer, keras.layers.Layer):\n\t        yield (\".\".join(prefix), layer)\n", "        for attr, child in vars(layer).items():\n\t            if attr.startswith(\"_\"):\n\t                continue\n\t            if isinstance(child, (list, tuple, keras.layers.Layer)):\n\t                yield from named_layers(child, prefix + (attr,))\n\tdef named_weights(\n\t    layer: keras.layers.Layer, recursive: bool = True\n\t) -> Iterable[Tuple[str, tf.Variable]]:\n\t    \"\"\"Walk a layer to find weight variables with full path names.\n\t    recursive -- bool -- if `False`, only look at direct weights owned by this layer\n", "    \"\"\"\n\t    sublayers = named_layers(layer) if recursive else [(\"\", layer)]\n\t    for name, sublayer in sublayers:\n\t        for attr, child in vars(sublayer).items():\n\t            if not attr.startswith(\"_\") and isinstance(child, tf.Variable):\n\t                yield (f\"{name}.{attr}\" if name else attr, child)\n\tdef _runner(\n\t    queue: multiprocessing.Queue,  # type:ignore[type-arg]\n\t    command: Callable[..., T],\n\t    args: Dict[str, Any],\n", ") -> None:\n\t    queue.put_nowait(command(**args))\n\tdef run_in_subprocess(command: Callable[..., T], **args: Any) -> T:\n\t    \"\"\"Run a command synchronously in a (non-daemon) subprocess.\"\"\"\n\t    # We'd prefer to use a simple multiprocessing.Pool here, but I can't find a\n\t    # way to make the workers non-daemonic\n\t    queue = multiprocessing.Manager().Queue()\n\t    process = multiprocessing.get_context(\"spawn\").Process(\n\t        target=_runner, args=(queue, command, args)\n\t    )\n", "    process.start()\n\t    process.join()\n\t    if process.exitcode:\n\t        raise multiprocessing.ProcessError(\n\t            f\"Process exited with code {process.exitcode}\"\n\t        )\n\t    return queue.get()  # type:ignore[no-any-return]\n"]}
{"filename": "scmm/pedal/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Press the pedal to go faster.\"\"\"\n\tfrom . import utility, xpu  # NOQA: F401\n"]}
{"filename": "scmm/pedal/tests/test_xpu.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\tfrom typing import Dict, List\n\timport numpy as np\n\timport pytest\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tfrom .. import xpu\n\tSETTINGS: List[xpu.Settings] = [\n\t    xpu.CpuSettings(compile=False),\n\t    xpu.CpuSettings(compile=True),\n", "]\n\tif xpu.IPU:\n\t    SETTINGS.extend(\n\t        [\n\t            xpu.IpuSettings(iterations_per_loop=1),\n\t            xpu.IpuSettings(\n\t                iterations_per_loop=4,\n\t                available_memory_proportion=0.2,\n\t                stochastic_rounding=True,\n\t            ),\n", "        ]\n\t    )\n\t@pytest.mark.parametrize(\"settings\", SETTINGS)\n\tdef test_context(settings: xpu.Settings):\n\t    traces = 0\n\t    def model(x: tf.Tensor) -> Dict[str, tf.Tensor]:\n\t        nonlocal traces\n\t        traces += 1\n\t        return dict(y=2 * x)\n\t    data = (dict(x=np.array(x, dtype=np.float32)) for x in range(5))\n", "    with xpu.context(settings) as context:\n\t        results = list(context.loop(model, data))\n\t        np.testing.assert_equal(results, [dict(y=2 * x) for x in range(5)])\n\t        if not (isinstance(settings, xpu.CpuSettings) and not settings.compile):\n\t            assert traces == 1\n\t@pytest.mark.parametrize(\n\t    \"settings\",\n\t    filter(\n\t        None,\n\t        [\n", "            xpu.CpuSettings(compile=False),\n\t            xpu.IpuSettings(iterations_per_loop=1) if xpu.IPU else None,\n\t        ],\n\t    ),\n\t)\n\tdef test_outline(settings: xpu.Settings):\n\t    with xpu.context(settings) as context:\n\t        assert xpu.current_context() is context\n\t        layer1 = keras.layers.Dense(10)\n\t        layer2 = keras.layers.Dense(10)\n", "        context.outline(layer1)\n\t        context.outline(layer2)\n\t        results = list(\n\t            context.loop(\n\t                lambda x: dict(y=layer2(layer1(x))),\n\t                [dict(x=np.ones((1, 10), dtype=np.float32))],\n\t            )\n\t        )\n\t        assert results[0][\"y\"].shape == (1, 10)\n"]}
{"filename": "scmm/pedal/tests/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n"]}
{"filename": "scmm/pedal/tests/test_utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\timport contextlib\n\timport datetime\n\timport json\n\timport multiprocessing\n\tfrom pathlib import Path\n\tfrom typing import Dict\n\timport numpy as np\n\timport pytest\n\tfrom tensorflow import keras\n", "from .. import utility\n\tdef test_split_seed():\n\t    seeds = utility.split_seed(123456789, 3)\n\t    assert len(seeds) == 3\n\t    assert len(set(seeds)) == 3\n\tdef test_remove_keys():\n\t    assert utility.remove_keys(dict(a=1, b=2, c=3), \"b\", \"d\") == dict(a=1, c=3)\n\tdef test_to_jsonable():\n\t    t0 = datetime.datetime.now()\n\t    data = json.dumps(\n", "        dict(time=t0, array=np.ones(2), path=Path(\"fake/path\")),\n\t        default=utility.to_jsonable,\n\t    )\n\t    assert json.loads(data) == dict(time=t0.isoformat(), array=[1, 1], path=\"fake/path\")\n\t    with pytest.raises(TypeError) as exc:\n\t        json.dumps(dict(obj=object()), default=utility.to_jsonable)\n\t    assert \"object\" in str(exc)\n\tdef test_logging():\n\t    history = []\n\t    @contextlib.contextmanager\n", "    def logger():\n\t        history.append(\"pre\")\n\t        yield history.append\n\t        history.append(\"post\")\n\t    with utility.logging(\n\t        logger(), lambda line: history.append(f\"lambda {line}\")\n\t    ) as log:\n\t        log(123)\n\t        log(456)\n\t    assert history == [\"pre\", 123, \"lambda 123\", 456, \"lambda 456\", \"post\"]\n", "def test_named_layers_and_weights():\n\t    class TestModel(keras.layers.Layer):\n\t        # pylint:disable=too-few-public-methods\n\t        def __init__(self):\n\t            super().__init__()\n\t            self.projection = keras.layers.Dense(20)\n\t            self.projection.build((10,))\n\t            self.transforms = [\n\t                keras.layers.LayerNormalization(),\n\t                keras.layers.Dense(20, use_bias=False),\n", "            ]\n\t            for transform in self.transforms:\n\t                transform.build((20,))\n\t            self.final_bias = self.add_weight(\n\t                name=\"final_bias\", shape=(7,), initializer=\"zeros\"\n\t            )\n\t    model = TestModel()\n\t    assert {k: type(v).__name__ for k, v in utility.named_layers(model)} == {\n\t        \"\": \"TestModel\",\n\t        \"projection\": \"Dense\",\n", "        \"transforms.0\": \"LayerNormalization\",\n\t        \"transforms.1\": \"Dense\",\n\t    }\n\t    assert {k: tuple(v.shape) for k, v in utility.named_weights(model)} == {\n\t        \"projection.kernel\": (10, 20),\n\t        \"projection.bias\": (20,),\n\t        \"transforms.0.beta\": (20,),\n\t        \"transforms.0.gamma\": (20,),\n\t        \"transforms.1.kernel\": (20, 20),\n\t        \"final_bias\": (7,),\n", "    }\n\t    assert dict(utility.named_weights(model, recursive=False)).keys() == {\"final_bias\"}\n\tdef _stub(x: int) -> Dict[str, int]:\n\t    if x % 2 == 0:\n\t        raise ValueError(\"x is even\")\n\t    return dict(y=5 * x)\n\tdef test_run_in_subprocess():\n\t    assert utility.run_in_subprocess(_stub, x=3) == dict(y=15)\n\t    with pytest.raises(multiprocessing.ProcessError):\n\t        utility.run_in_subprocess(_stub, x=4)\n"]}
{"filename": "scmm/tests/test_models.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\timport dataclasses\n\timport numpy as np\n\timport pytest\n\tfrom .. import models\n\tfrom ..pedal import xpu\n\tSETTINGS = models.Settings(\n\t    vocab_size=100,\n\t    hidden_size=8,\n\t    depth=2,\n", "    residual=None,\n\t    sequence=models.Conv(kernel_size=5, groups=1),\n\t    token=None,\n\t    dtype=\"float32\",\n\t    seed=100,\n\t)\n\t@pytest.fixture\n\tdef cpu_context():\n\t    with xpu.context(xpu.CpuSettings(compile=False)) as context:\n\t        yield context\n", "MODEL_SETTINGS = [\n\t    SETTINGS,\n\t    dataclasses.replace(\n\t        SETTINGS,\n\t        residual=models.Residual(norm=None, alpha=None),\n\t        token=models.FFN(multiple=1.5),\n\t    ),\n\t    dataclasses.replace(\n\t        SETTINGS,\n\t        residual=models.Residual(norm=\"pre\", alpha=\"mean\"),\n", "        sequence=models.Attention(heads=2, head_size=4, frequencies=16, max_period=16),\n\t    ),\n\t    dataclasses.replace(\n\t        SETTINGS,\n\t        residual=models.Residual(norm=\"post\", alpha=0.5),\n\t        sequence=models.RNN(rebias=1),\n\t    ),\n\t]\n\t@pytest.mark.parametrize(\n\t    \"settings,unit_scale\",\n", "    [\n\t        (dataclasses.replace(settings, dtype=dtype), unit_scale)\n\t        for settings in MODEL_SETTINGS\n\t        for dtype in [\"float32\", \"float16\"]\n\t        for unit_scale in [None, \"0.4\"]\n\t    ],\n\t    ids=repr,\n\t)\n\tdef test_model(cpu_context: xpu.Context, settings: models.Settings, unit_scale: bool):\n\t    if unit_scale and settings.residual is not None and settings.residual.alpha is None:\n", "        pytest.skip(\"unsupported combination\")\n\t    batch_sequences = 3\n\t    sequence_length = 12\n\t    random = np.random.Generator(np.random.PCG64(200))\n\t    tokens = random.integers(\n\t        settings.vocab_size, size=(batch_sequences, sequence_length)\n\t    )\n\t    mask = random.random(size=(batch_sequences, sequence_length)) < 0.9\n\t    model = models.Model(settings, unit_scale=unit_scale)\n\t    result = model.run(tokens=tokens, mask=mask)\n", "    assert 0 < float(result[\"loss\"]) < 1.5 * np.log(settings.vocab_size)\n\t    assert int(result[\"n_tokens\"]) == np.sum(mask)\n\t    # Same seed - expect same weights & results\n\t    model2 = models.Model(settings, unit_scale=unit_scale)\n\t    result2 = model2.run(tokens=tokens, mask=mask)\n\t    np.testing.assert_allclose(float(result2[\"loss\"]), float(result[\"loss\"]))\n\t    np.testing.assert_equal(model2.save(), model.save())\n\tdef test_model_load_save(cpu_context: xpu.Context):\n\t    # Change seed, expect different weights\n\t    base = models.Model(SETTINGS, unit_scale=False)\n", "    other = models.Model(\n\t        dataclasses.replace(SETTINGS, seed=SETTINGS.seed + 1), unit_scale=False\n\t    )\n\t    base_weights = base.save()\n\t    other_weights = other.save()\n\t    assert any(np.any(other_weights[k] != base_weights[k]) for k in base_weights)\n\t    with pytest.raises(ValueError) as error:\n\t        other.load(dict(**base_weights, non_existent_weight=np.zeros(2)))\n\t    assert \"non_existent_weight\" in str(error)\n\t    other.load(base_weights)\n", "    np.testing.assert_equal(other.save(), base_weights)\n"]}
{"filename": "scmm/tests/test_datasets.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\timport itertools as it\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom .. import datasets\n\tdef test_to_ids_to_str():\n\t    vocab = tuple(\" abcd\")\n\t    original = \"bad cad\"\n\t    ids = datasets.to_ids(original, vocab, dtype=np.uint16)\n\t    np.testing.assert_equal(ids, [2, 1, 4, 0, 3, 1, 4])\n", "    assert datasets.to_str(ids, vocab) == original\n\tdef test_data():\n\t    vocab = tuple(\" abcd\")\n\t    parts = dict(\n\t        train=\"a bad cad abba a bad dad\",\n\t        valid=\"a bb ccc dddd\",\n\t    )\n\t    data = datasets.Data(\n\t        vocab=vocab,\n\t        parts={k: datasets.to_ids(v, vocab, np.uint16) for k, v in parts.items()},\n", "    )\n\t    S = datasets.BatchSettings\n\t    for settings in [\n\t        S(sequences=3, sequence_length=6, overlap_length=2, loop_seed=None),\n\t        S(sequences=3, sequence_length=6, overlap_length=2, loop_seed=100),\n\t        S(sequences=1, sequence_length=8, overlap_length=0, loop_seed=None),\n\t        S(sequences=1, sequence_length=8, overlap_length=0, loop_seed=200),\n\t    ]:\n\t        for part, text in parts.items():\n\t            max_batch = 25\n", "            batches = list(it.islice(data.batches(part, settings), max_batch))\n\t            for batch in batches:\n\t                assert batch[\"tokens\"].shape == settings.shape\n\t                assert batch[\"tokens\"].dtype == np.int32\n\t                assert batch[\"mask\"].shape == settings.shape\n\t                assert batch[\"mask\"].dtype == np.int32\n\t                assert np.sum(batch[\"mask\"]) >= (\n\t                    1 if settings.loop_seed is None else settings.target_tokens\n\t                )\n\t            if settings.loop_seed is None:\n", "                flat_tokens = np.ravel([b[\"tokens\"] for b in batches])\n\t                flat_mask = np.ravel([b[\"mask\"] for b in batches]).astype(np.bool)\n\t                assert datasets.to_str(flat_tokens[flat_mask], vocab) == text\n\t            else:\n\t                assert len(batches) == max_batch\n\tdef test_load():\n\t    # Path(\"data/vocab.json\").write_text(json.dumps(sorted({\n\t    #     ch for path in Path(\"data\").glob(\"*.txt\") for ch in path.read_text(\"utf8\")\n\t    # })))\n\t    folder = Path(__file__).parent / \"data\"\n", "    data = datasets.load_character(folder, train=\"train.txt\", valid=\"valid.txt\")\n\t    assert datasets.to_str(data.parts[\"valid\"], data.vocab) == (\n\t        folder / \"valid.txt\"\n\t    ).read_text(\"utf8\")\n\t    assert len(data.parts[\"train\"]) > len(data.parts[\"valid\"])\n"]}
{"filename": "scmm/tests/test_training.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\timport numpy as np\n\timport pytest\n\tfrom .. import datasets, models, training\n\tfrom ..pedal import xpu\n\t@pytest.mark.parametrize(\n\t    \"optimiser\",\n\t    [\n\t        training.AdamW(0.1, learning_rate_decay=0.0),\n\t        training.SgdM(0.1, learning_rate_decay=0.0, momentum=0.9),\n", "    ],\n\t    ids=lambda s: s.kind,\n\t)\n\tdef test_training(optimiser: training.Optimiser):\n\t    data_sequence = np.arange(100) % 3\n\t    data = datasets.Data(\n\t        (\"a\", \"b\", \"c\"),\n\t        dict(train=data_sequence, valid=data_sequence, test=data_sequence),\n\t    )\n\t    with xpu.context(xpu.CpuSettings()) as context:\n", "        model = models.Model(\n\t            models.Settings(\n\t                vocab_size=len(data.vocab),\n\t                hidden_size=32,\n\t                depth=2,\n\t                residual=None,\n\t                sequence=models.Conv(2, groups=1),\n\t                token=None,\n\t                dtype=\"float32\",\n\t                seed=100,\n", "            ),\n\t            unit_scale=False,\n\t        )\n\t        settings = training.Settings(\n\t            datasets.BatchSettings(2, 8, 2, loop_seed=200),\n\t            steps=10,\n\t            valid_interval=None,\n\t            optimiser=optimiser,\n\t            loss_scale=1e3,\n\t        )\n", "        log = list(training.train(model, data, context, settings, unit_scale=False))\n\t        train_log = [line for line in log if line[\"kind\"] == \"train_step\"]\n\t        assert 0.5 * np.log(3) < train_log[0][\"loss\"]\n\t        assert train_log[-1][\"loss\"] < 0.01 * np.log(3)\n"]}
{"filename": "scmm/tests/testing.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\tfrom typing import Callable, Dict, Tuple\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tfrom ..pedal import utility\n\tdef assert_unit_scale(value: np.ndarray, tol: float, err_msg: str = \"\") -> None:\n\t    \"\"\"Check that a tensor has unit std.\"\"\"\n\t    np.testing.assert_allclose(np.std(value), 1, atol=tol, err_msg=err_msg)\n\tdef weight_shapes(layer: keras.layers.Layer) -> Dict[str, Tuple[int, ...]]:\n", "    \"\"\"A map of name to weight shape.\"\"\"\n\t    return {name: tuple(w.shape) for name, w in utility.named_weights(layer)}\n\tdef correlated_batch_random(\n\t    random: np.random.RandomState, shape: Tuple[int, ...]\n\t) -> np.ndarray:\n\t    \"\"\"Create a random tensor tiled over all 'batch' dimensions (all except last).\"\"\"\n\t    # return random.normal(size=shape).astype(np.float32)\n\t    return np.tile(random.normal(size=shape[-1]).astype(np.float32), shape[:-1] + (1,))\n\tdef output_and_gradients(\n\t    layer: Callable[..., tf.Tensor], input_shape: Tuple[int, ...], seed: int\n", ") -> Dict[str, np.ndarray]:\n\t    \"\"\"Randomly generate inputs and grads (unit norm), and return everything.\n\t    Creates identical outputs and gradients across the batch axis, since\n\t    realistic gradients are better modelled by perfect correlation than no\n\t    correlation.\n\t    \"\"\"\n\t    random = np.random.Generator(np.random.PCG64(seed))\n\t    inputs = tf.constant(correlated_batch_random(random, input_shape))\n\t    with tf.GradientTape() as tape:\n\t        tape.watch(inputs)\n", "        outputs = layer(inputs)\n\t    grad_outputs = correlated_batch_random(random, outputs.shape)\n\t    gradient_tensors = {f\"grad_{k}\": v for k, v in utility.named_weights(layer)}\n\t    gradient_tensors[\"grad_inputs\"] = inputs\n\t    gradients = tape.gradient(outputs, gradient_tensors, grad_outputs)\n\t    return dict(inputs=inputs, outputs=outputs, grad_outputs=grad_outputs, **gradients)\n"]}
{"filename": "scmm/tests/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n"]}
{"filename": "scmm/tests/test_experiments.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\timport collections\n\timport contextlib\n\timport json\n\timport os\n\timport unittest.mock as um\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict\n\timport numpy as np\n\timport pytest\n", "from .. import datasets, experiments, models, training\n\tfrom ..pedal import xpu\n\tdef test_log_wandb_finish_on_error():\n\t    with pytest.raises(ValueError), um.patch(\"wandb.init\"), um.patch(\n\t        \"wandb.run\"\n\t    ) as wandb_run, um.patch(\"wandb.finish\") as wandb_finish:\n\t        with experiments.log_wandb():\n\t            raise ValueError(\"Bad things happened\")\n\t    wandb_finish.assert_called_once_with(1)\n\t    wandb_run.summary.update.assert_called_once_with(\n", "        dict(error=\"ValueError('Bad things happened')\")\n\t    )\n\tdef _test_settings(path: Path) -> experiments.Settings:\n\t    return experiments.Settings(\n\t        data=experiments.DataSettings(Path(__file__).parent / \"data\"),\n\t        model=models.Settings(\n\t            hidden_size=64,\n\t            depth=1,\n\t            residual=None,\n\t            sequence=models.Conv(kernel_size=5, groups=1),\n", "            token=None,\n\t            dtype=\"float32\",\n\t            vocab_size=None,  # type:ignore[arg-type]\n\t            seed=None,  # type:ignore[arg-type]\n\t        ),\n\t        training=training.Settings(\n\t            batch=datasets.BatchSettings(\n\t                sequences=10, sequence_length=32, overlap_length=8, loop_seed=None\n\t            ),\n\t            steps=100,\n", "            valid_interval=50,\n\t            optimiser=training.AdamW(learning_rate=0.05, learning_rate_decay=1e-3),\n\t            loss_scale=1,\n\t        ),\n\t        unit_scale=\"0.4\",\n\t        target=xpu.CpuSettings(compile=False),\n\t        output=experiments.OutputSettings(\n\t            wandb=True,\n\t            stderr=True,\n\t            log=path / \"log.jsonl\",\n", "            checkpoint=path / \"model.npz\",\n\t        ),\n\t        metadata=dict(experiment=\"testxp\"),\n\t        seed=None,  # type:ignore[arg-type]\n\t    )\n\tdef test_run_experiment(tmp_path: Path):  # pylint:disable=too-many-locals\n\t    with contextlib.ExitStack() as stack:\n\t        wandb_init = stack.enter_context(um.patch(\"wandb.init\"))\n\t        wandb_log = stack.enter_context(um.patch(\"wandb.log\"))\n\t        stack.enter_context(um.patch(\"wandb.run\"))\n", "        wandb_finish = stack.enter_context(um.patch(\"wandb.finish\"))\n\t        stack.enter_context(\n\t            um.patch.dict(\n\t                os.environ, {\"SSUB_UID\": \"ssub123\", \"SLURM_JOB_ID\": \"slurm123\"}\n\t            )\n\t        )\n\t        experiments.run(_test_settings(tmp_path))\n\t    wandb_init.assert_called_once()\n\t    wandb_init_args = wandb_init.call_args[1]\n\t    assert wandb_init_args.get(\"project\") == \"scaled-matmuls\"\n", "    assert wandb_init_args[\"config\"][\"metadata\"][\"ssub_id\"] == \"ssub123\"\n\t    assert wandb_log.call_count == 2 * 3 + 1\n\t    wandb_finish.assert_called_once()\n\t    # Checkpoint\n\t    checkpoint = np.load(tmp_path / \"model.npz\")\n\t    assert checkpoint[\"step\"] == 100\n\t    assert len(checkpoint) >= 2\n\t    # Log\n\t    log_by_kind = collections.defaultdict(list)\n\t    with (tmp_path / \"log.jsonl\").open() as file_:\n", "        for line in file_:\n\t            item = json.loads(line)\n\t            log_by_kind[item[\"kind\"]].append(item)\n\t    (log_settings,) = log_by_kind[\"settings\"]\n\t    assert log_settings[\"metadata\"][\"experiment\"] == \"testxp\"\n\t    assert isinstance(log_settings[\"model\"][\"seed\"], int)\n\t    (log_stats,) = log_by_kind[\"stats\"]\n\t    assert 5 * 64 * 64 < log_stats[\"n_weights\"]\n\t    assert len(log_stats[\"weight_shapes\"])\n\t    assert len(log_by_kind[\"train_step\"]) == 100\n", "    first_step, *_, last_step = log_by_kind[\"train_step\"]\n\t    assert first_step[\"step\"] == 0\n\t    assert last_step[\"step\"] == 99\n\t    assert last_step[\"loss\"] < first_step[\"loss\"]\n\t    assert [x[\"valid_n_tokens\"] for x in log_by_kind[\"eval_valid\"]] == 3 * [7665]\n\t    assert log_by_kind[\"eval_valid\"][-1][\"valid_loss\"] < 2.5\n\t    assert len([x[\"train_n_tokens\"] for x in log_by_kind[\"eval_train\"]]) == 3\n\t    assert log_by_kind[\"eval_train\"][-1][\"train_loss\"] < 2.5\n\tdef test_find_learning_rate():\n\t    called_with_lr = []\n", "    def _fake_run(settings: experiments.Settings) -> Dict[str, Any]:\n\t        learning_rate = settings.training.optimiser.learning_rate\n\t        called_with_lr.append(learning_rate)\n\t        return dict(valid_loss=(15 - learning_rate) ** 2)\n\t    base = _test_settings(Path(\"fake\"))\n\t    base.training.optimiser.learning_rate = 3\n\t    with um.patch(\"scmm.experiments.run\", new_callable=lambda: _fake_run):\n\t        # LR = 3, 6, 12, 24, 48, ...\n\t        # Note - disable run_in_subprocess, otherwise our um.patch() wouldn't work\n\t        best, best_loss = experiments.find_learning_rate(\n", "            experiments.LrSweep(base, step=2, threshold=30, reps=3),\n\t            run_in_subprocess=False,\n\t        )\n\t    np.testing.assert_allclose(best.training.optimiser.learning_rate, 12)\n\t    np.testing.assert_allclose(best_loss, 9)\n\t    np.testing.assert_allclose(called_with_lr, np.repeat([3, 6, 12, 24], 3))\n"]}
{"filename": "scmm/tests/test_layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\tfrom typing import List, Optional\n\timport numpy as np\n\timport pytest\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tfrom .. import layers\n\tfrom . import testing\n\tdef test_batched_gather():\n\t    tables = tf.reshape(tf.range(2 * 3 * 4), (2, 3, 4))\n", "    indices = tf.constant([[0, 0, 3], [2, 2, 3]])\n\t    np.testing.assert_equal(\n\t        np.array(layers.batched_gather(tables, indices)),\n\t        [[0 + 0, 4 + 0, 8 + 3], [12 + 2, 16 + 2, 20 + 3]],\n\t    )\n\t# Also tests layers.LayerNormalization\n\t@pytest.mark.parametrize(\n\t    [\"norm_type\", \"alpha\"], [(None, 0.5), (\"pre\", None), (\"post\", 0.1)]\n\t)\n\tdef test_residual_layer(norm_type: Optional[str], alpha: Optional[float]):\n", "    layer = layers.ResidualLayer(\n\t        keras.layers.Dense(7), norm_type=norm_type, alpha=alpha\n\t    )\n\t    layer.build((None, None, 7))\n\t    assert layer.body.kernel.shape == (7, 7)\n\t    assert layer(tf.ones((2, 3, 7))).shape == (2, 3, 7)\n\tdef test_ffn_layer():\n\t    layer = layers.FFNLayer(3, seeds=(100, 200))\n\t    layer.build((7,))\n\t    assert layer.up.kernel.shape == (7, 21)  # type:ignore[union-attr]\n", "    assert layer.down.kernel.shape == (21, 7)  # type:ignore[union-attr]\n\t    assert layer(tf.ones((2, 3, 7))).shape == (2, 3, 7)\n\tdef test_softmax_cross_entropy():\n\t    loss, n_ids = layers.softmax_cross_entropy(\n\t        tf.ones((2, 3, 20)),\n\t        tf.constant([[0, 9, 19], [2, 2, 2]]),\n\t        tf.constant([[True, True, True], [True, False, False]]),\n\t    )\n\t    assert int(n_ids) == 4\n\t    np.testing.assert_allclose(float(loss), np.log(20))\n", "def test_layer_normalization():\n\t    layer = layers.LayerNormalization()\n\t    layer.build((None, None, 4))\n\t    np.testing.assert_allclose(\n\t        layer(tf.constant([0.0, 0, 4, 4])[tf.newaxis, tf.newaxis, :]),\n\t        tf.constant([-1.0, -1, 1, 1])[tf.newaxis, tf.newaxis, :],\n\t        rtol=1e-3,\n\t    )\n\t    np.testing.assert_allclose(layer(tf.ones((2, 3, 4))), tf.zeros((2, 3, 4)))\n\tdef test_pad_and_shift_layer():\n", "    layer = layers.PadAndShiftLayer()\n\t    layer.build((None, None, 11))\n\t    assert layer.padding.shape == (11,)\n\t    output = layer(tf.ones((3, 5, 11)))\n\t    assert output.shape == (3, 5, 11)\n\t    np.testing.assert_allclose(output[:, 0, :], 0)\n\t    np.testing.assert_allclose(output[:, 1:, :], 1)\n\t    with pytest.raises(ValueError):\n\t        layers.PadAndShiftLayer().build((None, 11))\n\tdef test_isotropic():\n", "    layer = layers.Isotropic(\n\t        dense=keras.layers.Dense(15), norm=keras.layers.LayerNormalization()\n\t    )\n\t    layer.build((None, 15))\n\t    assert layer.dense.built\n\t    assert layer.norm.built\n\t    random = np.random.Generator(np.random.PCG64(seed=500))\n\t    assert layer(random.normal(size=(7, 15))).shape == (7, 15)\n\t####################\n\t# Attention\n", "def test_sinusoid_embedding():\n\t    embedding = layers.sinusoid_embedding(\n\t        sequence_length=32, frequencies=6, max_period=16\n\t    )\n\t    assert embedding.shape == (32, 6)\n\t    np.testing.assert_allclose(embedding[:, 0], 0, atol=1e-6)\n\t    np.testing.assert_allclose(\n\t        embedding[:, 1], np.cos(np.pi * np.arange(32)), atol=1e-6\n\t    )\n\t    np.testing.assert_allclose(\n", "        embedding[:, -2], np.sin(2 * np.pi / 16 * np.arange(32)), atol=1e-6\n\t    )\n\t    np.testing.assert_allclose(\n\t        embedding[:, -1], np.cos(2 * np.pi / 16 * np.arange(32)), atol=1e-6\n\t    )\n\tdef test_relative_causal_reshape():\n\t    scores = tf.constant(\n\t        [\n\t            [1, 2, 3, 4],\n\t            [11, 12, 13, 14],\n", "            [21, 22, 23, 24],\n\t            [31, 32, 33, 34],\n\t        ]\n\t    )\n\t    attention = layers.relative_causal_reshape(scores)\n\t    np.testing.assert_equal(\n\t        attention.numpy(),\n\t        [\n\t            [1, 0, 0, 0],\n\t            [12, 11, 0, 0],\n", "            [23, 22, 21, 0],\n\t            [34, 33, 32, 31],\n\t        ],\n\t    )\n\tdef test_multi_head_attention():\n\t    random = np.random.Generator(np.random.PCG64(387232))\n\t    layer = layers.MultiHeadAttention(\n\t        heads=5, head_size=4, frequencies=13, max_period=16, seeds=(287, 918, 734)\n\t    )\n\t    inputs = random.normal(size=(11, 7, 8))\n", "    result = layer(inputs)\n\t    # Hard to check too much here - just make sure it's broadly sensible\n\t    assert result.shape == inputs.shape\n\t    np.testing.assert_array_less(np.std(result, axis=-1), 10)\n\t    assert testing.weight_shapes(layer) == {\n\t        \"qkv\": (8, 3, 5, 4),\n\t        \"q_bias\": (5, 4),\n\t        \"positional\": (13, 5, 4),\n\t        \"out.kernel\": (5 * 4, 8),\n\t        \"out.bias\": (8,),\n", "    }\n\t    # Check out the causal masking, by perturbing inputs[i, i+1]\n\t    # in which case inputs[i, :i+1] should be unchanged\n\t    perturbed = layer(inputs + np.eye(11, 7, k=1)[..., np.newaxis])\n\t    mask = np.tril(np.ones((11, 7)), k=0)[..., np.newaxis]\n\t    np.testing.assert_allclose(perturbed * mask, result * mask)\n\t    assert not np.allclose(perturbed, result)\n\t####################\n\t# RNN\n\tdef test_recurrent_highway_cell():\n", "    random = np.random.Generator(np.random.PCG64(387232))\n\t    layer = layers.RecurrentHighwayCell(\n\t        hidden_size=16, rebias=1, seed=random.integers(10000)\n\t    )\n\t    layer.build((3, 8))\n\t    assert testing.weight_shapes(layer) == dict(gates=(2, 24, 16), gates_bias=(2, 16))\n\t    output = layer(random.random(size=(3, 8)), random.random(size=(3, 16)))\n\t    assert output.shape == (3, 16)\n\t    assert np.std(output) < 10\n\tdef test_rnn():\n", "    layer = layers.RNN(layers.RecurrentHighwayCell(hidden_size=20, rebias=0, seed=439))\n\t    out = testing.output_and_gradients(layer, (3, 5, 8), seed=1341)\n\t    assert out[\"outputs\"].shape == (3, 5, 20)\n\t####################\n\t# Optimizers\n\tdef _train_sample_model(optimizer: keras.optimizers.Optimizer) -> List[float]:\n\t    random = np.random.Generator(np.random.PCG64(983532))\n\t    xs = random.normal(size=(1000, 20))\n\t    ys = xs @ random.normal(size=(20, 10))\n\t    model = keras.layers.Dense(\n", "        10, kernel_initializer=keras.initializers.GlorotUniform(seed=87321)\n\t    )\n\t    losses = []\n\t    for _ in range(10):\n\t        with tf.GradientTape() as tape:\n\t            loss = keras.losses.mse(ys.flatten(), tf.reshape(model(xs), -1))\n\t        optimizer.minimize(loss, model.trainable_variables, tape=tape)\n\t        losses.append(float(loss))\n\t    return losses\n\tdef test_sgdm():\n", "    np.testing.assert_allclose(\n\t        _train_sample_model(layers.SgdM(0.1, momentum=0.9)),\n\t        _train_sample_model(keras.optimizers.SGD(0.1, momentum=0.9)),\n\t        atol=1e-4,\n\t    )\n\tdef test_adamw():\n\t    reference_loss = _train_sample_model(keras.optimizers.Adam(0.1))\n\t    custom_loss = _train_sample_model(layers.AdamW(0.1, weight_decay=0))\n\t    decay_loss = _train_sample_model(layers.AdamW(0.1, weight_decay=0.1))\n\t    np.testing.assert_allclose(custom_loss, reference_loss, atol=1e-4)\n", "    assert (\n\t        1e-3 + reference_loss[-1] < decay_loss[-1]\n\t    ), \"decayed should be slightly worse\"\n\tdef test_adamw_indexed_slices():\n\t    optimizer = layers.AdamW(0.1)\n\t    table = tf.Variable(np.zeros((5, 3), dtype=np.float32))\n\t    indices = tf.constant([0, 2, 2, 4])\n\t    optimizer.minimize(\n\t        lambda: tf.reduce_sum(tf.gather(table, indices)), var_list=[table]\n\t    )\n", "    np.testing.assert_equal(table[:, 0].numpy() < 0, [True, False, True, False, True])\n\tdef test_optimiser_decay_and_vector_learning_rate():\n\t    for optimiser, learning_rate in [(layers.SgdM, 1.0), (layers.AdamW, 0.1)]:\n\t        log = _train_sample_model(\n\t            optimiser(\n\t                learning_rate, learning_rate_decay=0.1, scale_vector_learning_rate=True\n\t            )\n\t        )\n\t        assert log[-1] < log[0] / 2, log\n"]}
{"filename": "scmm/uscale/utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Utilities to support unit scaling development.\"\"\"\n\timport collections\n\timport functools\n\timport sys\n\tfrom dataclasses import dataclass\n\tfrom typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n", "from ..pedal import utility\n\tclass ActivationTracker:\n\t    \"\"\"Track activations (and gradients) for layers in a model.\n\t    Note that this only works in eager mode, and is designed for offline\n\t    use.\n\t        layer = keras.layers.Dense(10)\n\t        layer.build((None, 20))\n\t        tracker = ActivationTracker(layer)\n\t        with tf.GradientTape() as tape:\n\t            loss = tf.reduce_sum(layer(tf.zeros((3, 20))))\n", "        grads_and_vars = zip(\n\t            tape.gradient(loss, layer.trainable_variables),\n\t            layer.trainable_variables)\n\t        tracker.log_gradients(grads_and_vars)  # for weight gradients only\n\t        print({t.name: np.std(t.gradient) for t in tracker.trace})\n\t    \"\"\"\n\t    @dataclass\n\t    class Trace:\n\t        \"\"\"Forward and backward pass tensors from a single edge in the graph.\"\"\"\n\t        name: str\n", "        activation: np.ndarray\n\t        gradient: Optional[np.ndarray]\n\t    @dataclass\n\t    class LayerTrace(Trace):\n\t        \"\"\"Forward and backward pass information for a layer (with one output).\"\"\"\n\t        layer: keras.layers.Layer\n\t        weights: Tuple[\"ActivationTracker.Trace\", ...]\n\t    def __init__(self, *layers_to_track: keras.layers.Layer):\n\t        self._layers: Dict[str, keras.layers.Layer] = {}\n\t        self._variable_to_weight_name: Dict[tf.Variable, Tuple[str, str]] = {}\n", "        self._weights: Dict[str, Dict[str, np.ndarray]] = collections.defaultdict(dict)\n\t        self._weight_gradients: Dict[\n\t            str, Dict[str, List[np.ndarray]]\n\t        ] = collections.defaultdict(lambda: collections.defaultdict(list))\n\t        self._activations: Dict[str, List[np.ndarray]] = collections.defaultdict(list)\n\t        self._gradients: Dict[str, List[np.ndarray]] = collections.defaultdict(list)\n\t        for layer in layers_to_track:\n\t            self.track(layer)\n\t    def _track_layer(self, name: str, layer: keras.layers.Layer) -> None:\n\t        self._layers[name] = layer\n", "        for weight_name, weight in utility.named_weights(layer, recursive=False):\n\t            self._weights[name][weight_name] = weight.numpy()\n\t            self._variable_to_weight_name[weight.ref()] = (name, weight_name)\n\t        @tf.custom_gradient  # type:ignore[misc]\n\t        def identity_log(x: tf.Tensor) -> tf.Tensor:\n\t            self._activations[name].append(x.numpy())\n\t            def grad(upstream: tf.Tensor) -> tf.Tensor:\n\t                self._gradients[name].append(upstream.numpy())\n\t                return upstream\n\t            return x, grad\n", "        original_call = layer.call\n\t        @functools.wraps(original_call)\n\t        def wrapper(*args: Any, **kwargs: Any) -> tf.Tensor:\n\t            output = original_call(*args, **kwargs)\n\t            if not isinstance(output, tf.Tensor):\n\t                raise ValueError(\n\t                    \"Expected a layer to output a single tensor, actual output\"\n\t                    f\" {type(output)} from layer {layer}\"\n\t                )\n\t            return identity_log(output)\n", "        layer.call = wrapper\n\t    def track(self, layer: keras.layers.Layer) -> None:\n\t        \"\"\"Start track this layer's output and any (recursive) sublayers.\"\"\"\n\t        for name, sublayer in utility.named_layers(layer):\n\t            self._track_layer(name, sublayer)\n\t    def log_gradients(\n\t        self, grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]]\n\t    ) -> None:\n\t        \"\"\"Log weight gradients (optional call).\"\"\"\n\t        for grad, variable in grads_and_vars:\n", "            if isinstance(grad, tf.IndexedSlices):\n\t                grad = tf.math.unsorted_segment_sum(\n\t                    grad.values, grad.indices, grad.shape[0]\n\t                )\n\t            layer_name, weight_name = self._variable_to_weight_name[variable.ref()]\n\t            self._weight_gradients[layer_name][weight_name].append(grad.numpy())\n\t    @staticmethod\n\t    def _stack_optional(items: Sequence[np.ndarray]) -> Optional[np.ndarray]:\n\t        return np.stack(items) if items else None\n\t    @property\n", "    def trace(self) -> Tuple[LayerTrace, ...]:\n\t        \"\"\"Get activation and gradient traces for each layer (ordered by forward pass).\"\"\"\n\t        return tuple(\n\t            self.LayerTrace(\n\t                name=layer_name,\n\t                activation=np.stack(self._activations[layer_name]),\n\t                gradient=self._stack_optional(self._gradients[layer_name]),\n\t                layer=self._layers[layer_name],\n\t                weights=tuple(\n\t                    self.Trace(\n", "                        name=weight_name,\n\t                        activation=weight,\n\t                        gradient=self._stack_optional(\n\t                            self._weight_gradients[layer_name][weight_name]\n\t                        ),\n\t                    )\n\t                    for weight_name, weight in self._weights[layer_name].items()\n\t                ),\n\t            )\n\t            for layer_name in self._activations  # forward pass ordering\n", "        )\n\tdef printing(\n\t    name: str, summary: Callable[[tf.Tensor], Any] = np.std\n\t) -> Callable[[tf.Tensor], tf.Tensor]:\n\t    \"\"\"Utility for printing forward/backward pass statistics.\n\t    E.g.\n\t        x = printing(\"x\")(x)\n\t    \"\"\"\n\t    @tf.custom_gradient  # type:ignore[misc]\n\t    def operation(x: tf.Tensor) -> tf.Tensor:\n", "        print(f\"{name} forward {summary.__name__}\", summary(x), file=sys.stderr)\n\t        def grad(upstream: tf.Tensor) -> tf.Tensor:\n\t            print(\n\t                f\"{name} backward {summary.__name__}\",\n\t                summary(upstream),\n\t                file=sys.stderr,\n\t            )\n\t            return upstream\n\t        return x, grad\n\t    return operation  # type:ignore[no-any-return]\n"]}
{"filename": "scmm/uscale/layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Keras layers replacements with unit scaling.\"\"\"\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tfrom .. import layers\n\tfrom . import ops\n\tclass initializers:  # pylint:disable=invalid-name\n\t    \"\"\"Unit-variance initializers.\"\"\"\n", "    @staticmethod\n\t    def uniform(seed: Optional[int]) -> keras.initializers.Initializer:\n\t        \"\"\"Uniform distribution (symmetric about 0).\"\"\"\n\t        return keras.initializers.RandomUniform(-np.sqrt(3), np.sqrt(3), seed=seed)\n\t    @staticmethod\n\t    def normal(seed: Optional[int]) -> keras.initializers.Initializer:\n\t        \"\"\"Standard normal distribution.\"\"\"\n\t        return keras.initializers.RandomNormal(stddev=1, seed=seed)\n\tclass Dense(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"A scaled (and more restrictive) version of keras.layers.Dense.\"\"\"\n", "    def __init__(\n\t        self,\n\t        units: int,\n\t        activation: Optional[str] = None,\n\t        scale_for: str = \"both\",\n\t        dtype: tf.DType = tf.float32,\n\t        seed: Optional[int] = None,\n\t    ):\n\t        super().__init__(dtype=dtype)\n\t        self.units = units\n", "        self.scale_for = scale_for\n\t        self.kernel: tf.Variable = None\n\t        self.kernel_initializer = initializers.uniform(seed)\n\t        self.bias: tf.Variable = None\n\t        self.bias_initializer = keras.initializers.zeros()\n\t        self.activation = keras.activations.get(activation)\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.kernel = self.add_weight(\n\t            \"kernel\",\n", "            shape=(input_shape[-1], self.units),\n\t            initializer=self.kernel_initializer,\n\t        )\n\t        self.bias = self.add_weight(\n\t            \"bias\",\n\t            shape=self.units,\n\t            initializer=self.bias_initializer,\n\t        )\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        return self.activation(\n", "            ops.add_bias(\n\t                ops.pointwise(inputs, self.kernel, scale_for=self.scale_for), self.bias\n\t            )\n\t        )\n\tclass CausalConv1D(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"A scaled causal 1D convolution.\"\"\"\n\t    # pylint:disable=too-many-instance-attributes\n\t    def __init__(\n\t        self,\n\t        filters: int,\n", "        kernel_size: int,\n\t        groups: Optional[int] = None,\n\t        activation: Optional[str] = None,\n\t        dtype: tf.DType = tf.float32,\n\t        seed: Optional[int] = None,\n\t    ):\n\t        super().__init__(dtype=dtype)\n\t        self.filters = filters\n\t        self.kernel_size = kernel_size\n\t        self.groups = groups or 1\n", "        if filters % self.groups != 0:\n\t            raise ValueError(\n\t                f\"Filters ({filters}) must be evenly divisible by groups ({self.groups})\"\n\t            )\n\t        self.kernel: tf.Variable = None\n\t        self.kernel_initializer = initializers.uniform(seed)\n\t        self.bias: tf.Variable = None\n\t        self.bias_initializer = keras.initializers.zeros()\n\t        self.activation = keras.activations.get(activation)\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n", "        super().build(input_shape)\n\t        input_features = input_shape[-1]\n\t        if input_features % self.groups != 0:\n\t            raise ValueError(\n\t                f\"Input feature size ({input_features}) must be evenly divisible\"\n\t                f\" by groups ({self.groups})\"\n\t            )\n\t        self.kernel = self.add_weight(\n\t            \"kernel\",\n\t            shape=(self.kernel_size, input_shape[-1] // self.groups, self.filters),\n", "            initializer=self.kernel_initializer,\n\t        )\n\t        self.bias = self.add_weight(\n\t            \"bias\", shape=self.filters, initializer=self.bias_initializer\n\t        )\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        padded = tf.pad(inputs, [(0, 0), (self.kernel_size - 1, 0), (0, 0)])\n\t        return self.activation(\n\t            ops.add_bias(ops.conv1d(padded, self.kernel, padding=\"VALID\"), self.bias)\n\t        )\n", "class Embedding(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"A scaled variant of keras.layers.Embedding.\"\"\"\n\t    def __init__(\n\t        self,\n\t        table_size: int,\n\t        embeddings_size: int,\n\t        dtype: tf.DType = tf.float32,\n\t        seed: Optional[int] = None,\n\t    ):\n\t        super().__init__(dtype=dtype)\n", "        self.table_size = table_size\n\t        self.embeddings_size = embeddings_size\n\t        self.embeddings: tf.Variable = None\n\t        self.embeddings_initializer = keras.initializers.RandomUniform(\n\t            -np.sqrt(3), np.sqrt(3), seed=seed\n\t        )\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.embeddings = self.add_weight(\n\t            \"embeddings\",\n", "            shape=(self.table_size, self.embeddings_size),\n\t            initializer=self.embeddings_initializer,\n\t        )\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        # We don't need to worry about inputs scaling, as it is non-differentiable\n\t        batch_size = np.prod(inputs.shape)\n\t        # Scaling is based on \"batch size per row\"\n\t        return tf.gather(\n\t            ops.scaling(backward=self.table_size / batch_size)(self.embeddings),\n\t            inputs,\n", "        )\n\tclass LayerNormalization(layers.LayerNormalization):\n\t    \"\"\"A scaled variant of keras.layers.LayerNormalization.\"\"\"\n\t    def __init__(self, epsilon: float = 0.001, dtype: tf.DType = tf.float32):\n\t        super().__init__(epsilon=epsilon, dtype=dtype)\n\t        # Overwritten from base\n\t        self.beta_initializer = keras.initializers.zeros()\n\t        self.gamma_initializer = keras.initializers.ones()\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        return ops.add_bias(\n", "            ops.multiply_scale(self._normalize(inputs), self.gamma), self.beta\n\t        )\n\tclass ResidualLayer(layers.ResidualLayer):\n\t    \"\"\"A scaled (interpolation) residual layer.\"\"\"\n\t    def __init__(\n\t        self,\n\t        body: keras.layers.Layer,\n\t        norm_type: Optional[str],\n\t        alpha: float,\n\t        dtype: tf.DType = tf.float32,\n", "    ):\n\t        super().__init__(\n\t            body,\n\t            norm_type=norm_type,\n\t            alpha=alpha,\n\t            dtype=dtype,\n\t            norm_cls=LayerNormalization,\n\t        )\n\t    def call(self, x: tf.Tensor) -> tf.Tensor:\n\t        assert (\n", "            self.alpha is not None\n\t        ), \"cannot preserve variance with plain residual (please set 'alpha')\"\n\t        residual_scale = self.alpha**0.5\n\t        branch = ops.scaling(backward=residual_scale)(x)\n\t        if self.norm_type == \"pre\":\n\t            branch = self.norm(branch)\n\t        branch = self.body(branch)\n\t        y = (1 - self.alpha) ** 0.5 * x + ops.scaling(forward=residual_scale)(branch)\n\t        if self.norm_type == \"post\":\n\t            y = self.norm(y)\n", "        return y\n\tclass FFNLayer(layers.FFNLayer):\n\t    \"\"\"A scaled FFN layer.\"\"\"\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        hidden_size = input_shape[-1]\n\t        intermediate_size = int(self.multiple * hidden_size)\n\t        self.up = Dense(intermediate_size, dtype=self.dtype, seed=self.seeds[0])\n\t        self.up.build(input_shape[:-1] + (hidden_size,))\n\t        self.down = Dense(hidden_size, dtype=self.dtype, seed=self.seeds[1])\n", "        self.down.build(input_shape[:-1] + (intermediate_size,))\n\t    def call(self, x: tf.Tensor) -> tf.Tensor:\n\t        return self.down(keras.activations.relu(self.up(x)))  # type:ignore[misc]\n\tclass MultiHeadAttention(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"Scaled multi-head self attention a la Transformer.\n\t    With causal masking.\n\t    With relative-positional embeddings a la Transformer XL.\n\t    \"\"\"\n\t    # pylint:disable=too-many-instance-attributes\n\t    # pylint:disable=R0801\n", "    def __init__(\n\t        self,\n\t        heads: int,\n\t        head_size: int,\n\t        frequencies: int,\n\t        max_period: int,\n\t        dtype: tf.DType = tf.float32,\n\t        seeds: Optional[Tuple[int, int, int]] = None,\n\t    ):\n\t        super().__init__(dtype=dtype)\n", "        self.heads = heads\n\t        self.head_size = head_size\n\t        self.frequencies = frequencies\n\t        self.max_period = max_period\n\t        self.seeds = (None, None, None) if seeds is None else seeds\n\t        self.qkv: tf.Variable = None\n\t        self.q_bias: tf.Variable = None\n\t        self.positional: tf.Variable = None\n\t        self.out: keras.layers.Layer = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n", "        super().build(input_shape)\n\t        input_size = input_shape[-1]\n\t        self.qkv = self.add_weight(\n\t            name=\"qkv\",\n\t            shape=(input_size, 3, self.heads, self.head_size),\n\t            initializer=initializers.uniform(self.seeds[0]),\n\t        )\n\t        self.q_bias = self.add_weight(\n\t            name=\"q_bias\",\n\t            shape=(self.heads, self.head_size),\n", "            initializer=keras.initializers.zeros(),\n\t        )\n\t        self.positional = self.add_weight(\n\t            name=\"positional\",\n\t            shape=(self.frequencies, self.heads, self.head_size),\n\t            initializer=initializers.uniform(self.seeds[1]),\n\t        )\n\t        self.out = Dense(input_size, dtype=self.dtype, seed=self.seeds[2])\n\t        self.out.build(input_shape[:-1] + (self.heads * self.head_size,))\n\t    def _positional_weights(self, query: tf.Tensor) -> tf.Tensor:\n", "        sequence_length = query.shape[-2]\n\t        sins = tf.constant(\n\t            np.sqrt(2)\n\t            * layers.sinusoid_embedding(\n\t                sequence_length, self.frequencies, self.max_period\n\t            ),\n\t            dtype=query.dtype,\n\t        )\n\t        embeddings = tf.einsum(\n\t            \"sf,fnh->nsh\",\n", "            sins,\n\t            ops.scaling(\n\t                forward=self.frequencies**-0.5, backward=sequence_length**-1.0\n\t            )(self.positional),\n\t        )\n\t        scores = tf.einsum(\"bnqh,nvh->bnqv\", query, embeddings) * self.head_size**-0.5\n\t        return layers.relative_causal_reshape(scores)\n\t    def call(self, input: tf.Tensor) -> tf.Tensor:\n\t        # pylint:disable=invalid-name\n\t        batch_size, sequence_length, input_size = input.shape\n", "        q, k, v = tf.unstack(\n\t            tf.einsum(\n\t                \"bsx,xAnh -> Abnsh\",\n\t                input,\n\t                ops.scaling(\n\t                    forward=(3 * input_size * self.head_size * self.heads) ** -0.25,\n\t                    backward=(batch_size * sequence_length) ** -1.0,\n\t                )(self.qkv),\n\t            )\n\t        )\n", "        q += ops.scaling(backward=(batch_size * sequence_length) ** -1.0)(\n\t            self.q_bias[:, tf.newaxis, :]\n\t        )\n\t        a = tf.einsum(\"bnqh,bnkh->bnqk\", q, k) * self.head_size**-0.5\n\t        a += self._positional_weights(q)\n\t        # Note: oddly, -1e3 can be insufficient in FP16 with no LS, causing \"cheating\"\n\t        a = layers.causal_mask(a, mask_value=-3e4)\n\t        a = tf.nn.softmax(a, axis=-1)\n\t        o = tf.einsum(\"bnqk,bnkh->bqnh\", a, v)\n\t        return self.out(tf.reshape(o, o.shape[:-2] + (self.head_size * self.heads,)))\n", "class RecurrentHighwayCell(keras.layers.Layer):  # type:ignore[misc]\n\t    \"\"\"Scaled recurrent highway cell from https://arxiv.org/abs/1607.03474.\"\"\"\n\t    # pylint:disable=R0801\n\t    def __init__(\n\t        self,\n\t        hidden_size: int,\n\t        rebias: float,\n\t        dtype: tf.DType = tf.float32,\n\t        seed: Optional[int] = None,\n\t    ):\n", "        super().__init__(name=type(self).__name__, dtype=dtype)\n\t        self.hidden_size = hidden_size\n\t        self.carry_rebias = rebias\n\t        self.update_rebias = -rebias\n\t        self.seed = seed\n\t        self.gates: tf.Variable = None\n\t        self.gates_bias: tf.Variable = None\n\t    def build(self, input_shape: tf.TensorShape) -> None:\n\t        super().build(input_shape)\n\t        self.gates = self.add_weight(\n", "            \"gates\",\n\t            shape=(2, input_shape[-1] + self.hidden_size, self.hidden_size),\n\t            initializer=initializers.uniform(seed=self.seed),\n\t        )\n\t        self.gates_bias = self.add_weight(\n\t            \"gates_bias\",\n\t            shape=(2, self.hidden_size),\n\t            initializer=keras.initializers.zeros(),\n\t        )\n\t    def call(\n", "        self, input: tf.Tensor, hidden: tf.Tensor, sequence_length: int\n\t    ) -> tf.Tensor:\n\t        batch_size = input.shape[0] * sequence_length\n\t        gates_scale = (\n\t            2 * (input.shape[1] + self.hidden_size) * self.hidden_size\n\t        ) ** -0.25\n\t        gate_outputs = tf.concat([input, hidden], axis=1) @ ops.scaling(\n\t            forward=gates_scale, backward=batch_size**-1.0\n\t        )(self.gates)\n\t        gate_outputs += ops.scaling(backward=batch_size**-1.0)(\n", "            self.gates_bias[:, tf.newaxis]\n\t        )\n\t        transform, update = tf.unstack(gate_outputs)\n\t        update = tf.sigmoid(update + self.update_rebias)\n\t        return (1 - update) * hidden + update * tf.tanh(transform)\n\tclass RNN(layers.RNN):\n\t    \"\"\"A scaled, basic unidirectional RNN.\"\"\"\n\t    def call(self, input: tf.Tensor) -> tf.Tensor:\n\t        batch_size, sequence_length, _ = input.shape\n\t        # Note: sbh = (sequence, batch, hidden)\n", "        input_sbh = tf.transpose(input, (1, 0, 2))\n\t        initial_hidden = tf.tile(\n\t            ops.scaling(backward=batch_size**-1.0)(self.initial_hidden[tf.newaxis]),\n\t            (batch_size, 1),\n\t        )\n\t        output_sbh = tf.scan(\n\t            lambda hidden, input: self.cell(\n\t                input, hidden, sequence_length=sequence_length\n\t            ),\n\t            input_sbh,\n", "            initializer=initial_hidden,\n\t        )\n\t        return tf.transpose(output_sbh, (1, 0, 2))\n\tclass PadAndShiftLayer(layers.PadAndShiftLayer):\n\t    \"\"\"Shifts sequence features one place to the right with a trainable padding vector.\"\"\"\n\t    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n\t        npad = inputs.shape[0]\n\t        pad = tf.tile(\n\t            ops.scaling(backward=npad**-1.0)(self.padding[tf.newaxis, tf.newaxis]),\n\t            [npad, 1, 1],\n", "        )\n\t        return tf.concat([pad, inputs[:, :-1, :]], axis=1)\n"]}
{"filename": "scmm/uscale/ops.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Unit scaling - basic, portable operations.\"\"\"\n\tfrom typing import Callable, Optional, Tuple\n\timport numpy as np\n\timport tensorflow as tf\n\tdef scaling(\n\t    forward: Optional[float] = None, backward: Optional[float] = None\n\t) -> Callable[[tf.Tensor], tf.Tensor]:\n\t    \"\"\"Perform arbitary *seperate* scaling in the forward and backward passes.\n\t    E.g.\n", "        y = scaling(forward=2, backward=3)(x)\n\t    \"\"\"\n\t    @tf.custom_gradient  # type:ignore[misc]\n\t    def operation(input: tf.Tensor) -> tf.Tensor:\n\t        def grad(upstream: tf.Tensor) -> tf.Tensor:\n\t            grad_input = upstream\n\t            if backward is not None:\n\t                if isinstance(grad_input, tf.IndexedSlices):\n\t                    grad_input = tf.IndexedSlices(\n\t                        values=grad_input.values\n", "                        * tf.cast(backward, grad_input.values.dtype),\n\t                        indices=upstream.indices,\n\t                        dense_shape=upstream.dense_shape,\n\t                    )\n\t                else:\n\t                    grad_input = grad_input * tf.cast(backward, grad_input.dtype)\n\t            return grad_input\n\t        output = input\n\t        if forward is not None:\n\t            output = output * tf.cast(forward, output.dtype)\n", "        return output, grad\n\t    return operation  # type:ignore[no-any-return]\n\tdef pointwise(\n\t    inputs: tf.Tensor, weights: tf.Tensor, scale_for: str = \"both\"\n\t) -> tf.Tensor:\n\t    \"\"\"A scaled pointwise transformation.\n\t    inputs -- activations, will receive consistent gradients between forward & backward passes\n\t    weights -- will receive scaled gradients\n\t    scale_for -- how should the forward/backward-inputs pass scale be chosen?\n\t                \"forward\" -- preserve variance in the forward pass\n", "                \"backward\" -- preserve variance in the backward pass\n\t                \"both\" -- trade off forward and backward pass variance\n\t                \"both_arithmetic\" -- ditto, using arithmetic mean\n\t                \"both_min\" - ditto, using the minimum scale between forward and backward\n\t                \"separate\" -- separate scaling factors in the forward and backward-inputs passes\n\t                              WARNING - when using skip connections, this may cause inconsistent\n\t                              gradients.\n\t    \"\"\"\n\t    assert len(weights.shape) == 2, \"pointwise requires 2D rhs `weights`\"\n\t    input_size, output_size = weights.shape\n", "    backward_weights_scale = np.prod(inputs.shape[:-1]) ** -1.0\n\t    if scale_for == \"separate\":\n\t        return scaling(forward=input_size**-0.5)(\n\t            scaling(backward=output_size**-0.5)(inputs)\n\t            @ scaling(backward=backward_weights_scale)(weights)\n\t        )\n\t    # Note \"both\" is different from Glorot's sqrt(2 / (input_size + output_size)), as this\n\t    # should preserves scale better after boom_down(boom_up(x))\n\t    forward_scale = dict(\n\t        forward=input_size**-0.5,\n", "        backward=output_size**-0.5,\n\t        both=(input_size * output_size) ** -0.25,\n\t        both_arithmetic=((input_size + output_size) / 2) ** -0.5,\n\t        both_min=max(input_size, output_size) ** -0.5,\n\t    )[scale_for]\n\t    return inputs @ scaling(forward=forward_scale, backward=backward_weights_scale)(\n\t        weights\n\t    )\n\tdef conv1d(\n\t    input: tf.Tensor, filters: tf.Tensor, stride: int = 1, padding: str = \"SAME\"\n", ") -> tf.Tensor:\n\t    \"\"\"Scaling version of tf.nn.conv1d.\"\"\"\n\t    # pylint:disable=too-many-locals\n\t    batch_size, input_length, channels_in = input.shape\n\t    filter_width, filter_channels_in, channels_out = filters.shape\n\t    # See https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2\n\t    output_length = dict(\n\t        SAME=np.ceil(input_length / stride),\n\t        VALID=np.ceil((input_length + 1 - filter_width) / stride),\n\t    )[padding]\n", "    n_groups = channels_in // filter_channels_in\n\t    forward_contraction = filter_width * channels_in // n_groups\n\t    backward_contraction = filter_width / stride * channels_out // n_groups\n\t    forward_scale = (forward_contraction * backward_contraction) ** -0.25\n\t    backward_scale = (output_length * batch_size) ** -1.0\n\t    return tf.nn.conv1d(\n\t        input,\n\t        scaling(forward=forward_scale, backward=backward_scale)(filters),\n\t        stride=stride,\n\t        padding=padding,\n", "    )\n\tdef conv2d(\n\t    input: tf.Tensor, filters: tf.Tensor, strides: int = 1, padding: str = \"SAME\"\n\t) -> tf.Tensor:\n\t    \"\"\"Scaling version of tf.nn.conv2d.\"\"\"\n\t    # pylint:disable=too-many-locals\n\t    batch_size, height, width, channels_in = input.shape\n\t    kernel_height, kernel_width, filter_channels_in, channels_out = filters.shape\n\t    # See https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2\n\t    output_area = dict(\n", "        SAME=np.ceil(height / strides) * np.ceil(width / strides),\n\t        VALID=np.ceil((height + 1 - kernel_height) / strides)\n\t        * np.ceil((width + 1 - kernel_width) / strides),\n\t    )[padding]\n\t    n_groups = channels_in // filter_channels_in\n\t    forward_contraction = kernel_height * kernel_width * channels_in // n_groups\n\t    backward_contraction = (\n\t        (kernel_height / strides) * (kernel_width / strides) * channels_out // n_groups\n\t    )\n\t    forward_scale = (forward_contraction * backward_contraction) ** -0.25\n", "    backward_scale = (output_area * batch_size) ** -1.0\n\t    return tf.nn.conv2d(\n\t        input,\n\t        scaling(forward=forward_scale, backward=backward_scale)(filters),\n\t        strides=strides,\n\t        padding=padding,\n\t    )\n\tdef add_bias(features: tf.Tensor, bias: tf.Tensor) -> tf.Tensor:\n\t    \"\"\"Add a bias (which should be zero-initialized), with a scaled backward pass.\"\"\"\n\t    assert len(bias.shape) == 1, \"bias should be 1D\"\n", "    batch_size = np.prod(features.shape[:-1])\n\t    return features + scaling(backward=batch_size**-1.0)(bias)\n\tdef multiply_scale(features: tf.Tensor, scale: tf.Tensor) -> tf.Tensor:\n\t    \"\"\"Multiply by a scale tensor (which should be unit-initialized), with a scaled backward pass.\"\"\"\n\t    assert len(scale.shape) == 1, \"scale should be 1D\"\n\t    batch_size = np.prod(features.shape[:-1])\n\t    return features * scaling(backward=batch_size**-1.0)(scale)\n\tdef batched_gather(tables: tf.Tensor, indices: tf.Tensor) -> tf.Tensor:\n\t    \"\"\"Simulate tf.gather(tables, indices, batch_dims=indices.ndim).\n\t    Better compilation on IPU vs `tf.gather(logp, ids, batch_dims=2)`\n", "    \"\"\"\n\t    # Implemented here and in scmm.layers to avoid circular dependency issues\n\t    assert len(tables.shape) == len(indices.shape) + 1\n\t    # Use a one-hot encoding to save code memory\n\t    return tf.squeeze(\n\t        tf.one_hot(indices, tables.shape[-1], dtype=tables.dtype)[..., tf.newaxis, :]\n\t        @ tables[..., tf.newaxis],\n\t        [-1, -2],\n\t    )\n\tdef softmax_cross_entropy(\n", "    scores: tf.Tensor, ids: tf.Tensor, mask: tf.Tensor\n\t) -> Tuple[tf.Tensor, tf.Tensor]:\n\t    \"\"\"Compute masked softmax cross entropy loss.\n\t    Note that we abandon unit scaling in the forward pass, since this is\n\t    designed as a final loss term.\n\t    returns -- (average_loss, n_items) -- average_loss always in fp32\n\t    \"\"\"\n\t    assert mask.shape == ids.shape, \"mask should match target ids\"\n\t    # Use float32 for local computation - keeping things simple\n\t    logp = tf.nn.log_softmax(tf.cast(scores, tf.float32), axis=-1)\n", "    target_logp = batched_gather(logp, ids)\n\t    total_loss = tf.reduce_sum(tf.cast(mask, target_logp.dtype) * -target_logp)\n\t    n_ids = tf.reduce_sum(tf.cast(mask, tf.int32))\n\t    n_classes = scores.shape[-1]\n\t    loss = scaling(backward=np.prod(mask.shape) * n_classes / np.sqrt(n_classes - 1))(\n\t        total_loss / tf.cast(n_ids, total_loss.dtype)\n\t    )\n\t    return loss, n_ids\n"]}
{"filename": "scmm/uscale/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\t\"\"\"Core components for unit-scaling activations and gradients.\"\"\"\n\tfrom . import layers, ops, utility  # NOQA: F401\n"]}
{"filename": "scmm/uscale/tests/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n"]}
{"filename": "scmm/uscale/tests/test_utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\tfrom typing import Tuple\n\timport numpy as np\n\timport pytest\n\timport tensorflow as tf\n\tfrom tensorflow import keras\n\tfrom .. import utility\n\tdef test_activation_tracker_example():\n\t    # Sync with docstring\n\t    layer = keras.layers.Dense(10)\n", "    layer.build((None, 20))\n\t    tracker = utility.ActivationTracker(layer)\n\t    with tf.GradientTape() as tape:\n\t        loss = tf.reduce_sum(layer(tf.zeros((3, 20))))\n\t    grads_and_vars = zip(\n\t        tape.gradient(loss, layer.trainable_variables), layer.trainable_variables\n\t    )\n\t    tracker.log_gradients(grads_and_vars)  # only needed for weight gradients\n\t    # Checks\n\t    (trace,) = tracker.trace\n", "    assert trace.name == \"\"\n\t    assert trace.layer is layer\n\t    assert trace.activation.shape == (1, 3, 10)\n\t    assert trace.gradient.shape == (1, 3, 10)  # type:ignore[union-attr]\n\t    weights = {t.name: t for t in trace.weights}\n\t    assert weights[\"kernel\"].activation.shape == (20, 10)\n\t    assert weights[\"kernel\"].gradient.shape == (1, 20, 10)  # type:ignore[union-attr]\n\t    assert weights[\"bias\"].activation.shape == (10,)\n\t    assert weights[\"bias\"].gradient.shape == (1, 10)  # type:ignore[union-attr]\n\tdef test_activation_tracker_multiple_outputs():\n", "    class Duplicate(keras.layers.Layer):\n\t        # pylint:disable=too-few-public-methods\n\t        def call(self, x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n\t            return x, x\n\t    layer = Duplicate()\n\t    utility.ActivationTracker(layer)\n\t    with pytest.raises(ValueError) as error:\n\t        layer(tf.ones(3))\n\t    assert \"tuple\" in str(error)\n\tdef test_activation_tracker_embedding_gradient():\n", "    layer = keras.layers.Embedding(5, 8)\n\t    layer.build((None,))\n\t    tracker = utility.ActivationTracker(layer)\n\t    with tf.GradientTape() as tape:\n\t        loss = tf.reduce_sum(layer(tf.constant([0, 3, 3])))\n\t    grads_and_vars = zip(\n\t        tape.gradient(loss, layer.trainable_variables), layer.trainable_variables\n\t    )\n\t    tracker.log_gradients(grads_and_vars)\n\t    embedding_trace = tracker.trace[0].weights[0]\n", "    assert embedding_trace.name == \"embeddings\"\n\t    assert embedding_trace.gradient.shape == (1, 5, 8)  # type:ignore[union-attr]\n\t    np.testing.assert_equal(\n\t        embedding_trace.gradient[0, :, 0],  # type:ignore[index]\n\t        [1, 0, 0, 2, 0],\n\t    )\n\tdef test_printing(capsys):\n\t    with tf.GradientTape() as tape:\n\t        x = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n\t        tape.watch(x)\n", "        y = tf.reduce_sum(utility.printing(\"x\")(x) ** 2)\n\t    tape.gradient(y, x)\n\t    assert capsys.readouterr().err == \"x forward std 0.5\\nx backward std 1.0\\n\"\n"]}
{"filename": "scmm/uscale/tests/test_ops.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\timport functools\n\tfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\timport numpy as np\n\timport pytest\n\timport tensorflow as tf\n\tfrom ...tests import testing\n\tfrom .. import ops\n\tdef test_scaling():\n\t    with tf.GradientTape() as tape:\n", "        x = tf.range(3, dtype=tf.float32)\n\t        tape.watch(x)\n\t        y = ops.scaling(forward=2, backward=3)(x)\n\t    grad_x = tape.gradient(y, x, tf.ones_like(y))\n\t    np.testing.assert_allclose(y, [0, 2, 4])\n\t    np.testing.assert_allclose(grad_x, [3, 3, 3])\n\tdef test_scaling_indexed_slices():\n\t    with tf.GradientTape() as tape:\n\t        table = tf.range(10, dtype=tf.float32)\n\t        tape.watch(table)\n", "        y = tf.gather(ops.scaling(backward=5)(table), tf.constant([1, 1, 2]))\n\t    grad_table = tape.gradient(y, table, tf.ones_like(y))\n\t    np.testing.assert_allclose(y, [1, 1, 2])\n\t    np.testing.assert_allclose(grad_table.indices, [1, 1, 2])\n\t    np.testing.assert_allclose(grad_table.values, [5, 5, 5])\n\tdef assert_scaled_allclose(\n\t    actual: np.ndarray, desired: np.ndarray, atol: float, err_msg: str = \"\"\n\t) -> None:\n\t    np.testing.assert_allclose(\n\t        actual / np.std(actual), desired / np.std(desired), atol=atol, err_msg=err_msg\n", "    )\n\tdef check_op(\n\t    scaled: Callable[..., tf.Tensor],\n\t    reference: Callable[..., tf.Tensor],\n\t    seed: int,\n\t    args: Dict[str, Union[np.ndarray, Tuple[int, ...]]],\n\t    extra_args: Optional[Dict[str, Any]] = None,\n\t    shifted: bool = False,\n\t) -> Dict[str, tf.Tensor]:\n\t    # pylint:disable=too-many-locals\n", "    random = np.random.Generator(np.random.PCG64(seed))\n\t    inputs = {}\n\t    for key, value in args.items():\n\t        if isinstance(value, np.ndarray):\n\t            inputs[key] = tf.constant(value)\n\t        elif key in {\"input\", \"inputs\"}:\n\t            # This is pretty hacky & in need of a refactor...\n\t            inputs[key] = tf.constant(testing.correlated_batch_random(random, value))\n\t        else:\n\t            inputs[key] = tf.constant(random.normal(size=value).astype(np.float32))\n", "    with tf.GradientTape() as tape:\n\t        tape.watch(inputs.values())\n\t        scaled_out = scaled(**inputs, **(extra_args or {}))\n\t    # output_grad = random.normal(size=scaled_out.shape).astype(np.float32)\n\t    output_grad = testing.correlated_batch_random(random, scaled_out.shape)\n\t    scaled_grad = tape.gradient(scaled_out, inputs, output_grad)\n\t    with tf.GradientTape() as tape:\n\t        tape.watch(inputs.values())\n\t        reference_out = reference(**inputs, **(extra_args or {}))\n\t    reference_grad = tape.gradient(reference_out, inputs, output_grad)\n", "    assert_scaled_allclose(\n\t        scaled_out - shifted * np.mean(scaled_out),\n\t        reference_out - shifted * np.mean(reference_out),\n\t        atol=1e-3,\n\t    )\n\t    for arg in scaled_grad:\n\t        assert_scaled_allclose(\n\t            scaled_grad[arg], reference_grad[arg], atol=1e-3, err_msg=f\"for grad {arg}\"\n\t        )\n\t    return dict(\n", "        out=scaled_out,\n\t        grad=scaled_grad,\n\t        reference_out=reference_out,\n\t        reference_grad=reference_grad,\n\t    )\n\tdef test_add_bias():\n\t    out = check_op(\n\t        ops.add_bias,\n\t        lambda features, bias: features + bias,\n\t        seed=842,\n", "        args=dict(features=(1000,), bias=np.zeros(1000, dtype=np.float32)),\n\t    )\n\t    testing.assert_unit_scale(out[\"grad\"][\"bias\"], tol=0.05)\n\tdef test_multiply_scale():\n\t    out = check_op(\n\t        ops.multiply_scale,\n\t        lambda features, scale: features * scale,\n\t        seed=2345,\n\t        args=dict(features=(1000,), scale=np.ones(1000, dtype=np.float32)),\n\t    )\n", "    testing.assert_unit_scale(out[\"grad\"][\"scale\"], tol=0.05)\n\t@pytest.mark.parametrize(\n\t    \"scale_for\",\n\t    [\"forward\", \"backward\", \"both\", \"both_arithmetic\", \"both_min\", \"separate\"],\n\t)\n\tdef test_pointwise(scale_for):\n\t    out = check_op(\n\t        functools.partial(ops.pointwise, scale_for=scale_for),\n\t        lambda inputs, weights: tf.matmul(  # pylint:disable=unnecessary-lambda\n\t            inputs, weights\n", "        ),\n\t        seed=300,\n\t        args=dict(inputs=(3, 33, 1000), weights=(1000, 500)),\n\t    )\n\t    testing.assert_unit_scale(out[\"grad\"][\"weights\"], tol=0.05)\n\t    std_out = np.std(out[\"out\"])\n\t    std_grad_inputs = np.std(out[\"grad\"][\"inputs\"])\n\t    if scale_for in {\"forward\", \"separate\"}:\n\t        np.testing.assert_allclose(std_out, 1, atol=0.05)\n\t    if scale_for in {\"backward\", \"separate\"}:\n", "        np.testing.assert_allclose(std_grad_inputs, 1, atol=0.05)\n\t    if scale_for == \"both\":\n\t        np.testing.assert_allclose(std_out * std_grad_inputs, 1, atol=0.05)\n\t    if scale_for == \"both_min\":\n\t        assert std_out < 1.05 and std_grad_inputs < 1.05\n\t    if scale_for == \"both_arithmetic\":\n\t        assert (std_out < 1) != (std_grad_inputs < 1), \"signs should be swapped\"\n\t@pytest.mark.parametrize(\n\t    [\"padding\", \"stride\"], [(\"SAME\", 1), (\"SAME\", 2), (\"VALID\", 1), (\"VALID\", 2)]\n\t)\n", "def test_conv1d(padding, stride):\n\t    # Note: we're a bit sloppy about padding when using \"SAME\"\n\t    out = check_op(\n\t        ops.conv1d,\n\t        tf.nn.conv1d,\n\t        seed=400,\n\t        args=dict(input=(7, 27, 300), filters=(5, 300, 450)),\n\t        extra_args=dict(stride=stride, padding=padding),\n\t    )\n\t    np.testing.assert_allclose(\n", "        np.std(out[\"out\"]) * np.std(out[\"grad\"][\"input\"]), 1, atol=0.1\n\t    )\n\t    testing.assert_unit_scale(out[\"grad\"][\"filters\"], tol=0.1)\n\t@pytest.mark.parametrize(\n\t    [\"padding\", \"stride\"], [(\"SAME\", 1), (\"SAME\", 2), (\"VALID\", 1), (\"VALID\", 2)]\n\t)\n\tdef test_conv2d(padding, stride):\n\t    # Note: we're a bit sloppy about padding when using \"SAME\"\n\t    out = check_op(\n\t        ops.conv2d,\n", "        tf.nn.conv2d,\n\t        seed=500,\n\t        args=dict(input=(1, 10, 15, 300), filters=(2, 3, 300, 500)),\n\t        extra_args=dict(strides=stride, padding=padding),\n\t    )\n\t    np.testing.assert_allclose(\n\t        np.std(out[\"out\"]) * np.std(out[\"grad\"][\"input\"]), 1, atol=0.2\n\t    )\n\t    testing.assert_unit_scale(out[\"grad\"][\"filters\"], tol=0.2)\n\tdef test_batched_gather():\n", "    tables = tf.reshape(tf.range(2 * 3 * 4), (2, 3, 4))\n\t    indices = tf.constant([[0, 0, 3], [2, 2, 3]])\n\t    np.testing.assert_equal(\n\t        np.array(ops.batched_gather(tables, indices)),\n\t        [[0 + 0, 4 + 0, 8 + 3], [12 + 2, 16 + 2, 20 + 3]],\n\t    )\n\tdef test_softmax_cross_entropy():\n\t    batch_size, sequence_length, n_classes = (10, 20, 5)\n\t    random = np.random.Generator(np.random.PCG64(seed=1000))\n\t    scores = tf.constant(random.normal(size=(batch_size, sequence_length, n_classes)))\n", "    ids = tf.constant(random.integers(n_classes, size=(batch_size, sequence_length)))\n\t    with tf.GradientTape() as tape:\n\t        tape.watch(scores)\n\t        loss, n_ids = ops.softmax_cross_entropy(scores, ids, tf.ones_like(ids))\n\t    assert int(n_ids) == batch_size * sequence_length\n\t    assert 0 < float(loss) < 2 * np.log(n_classes)\n\t    grad_scores = tape.gradient(loss, scores)\n\t    testing.assert_unit_scale(grad_scores, 0.1)\n"]}
{"filename": "scmm/uscale/tests/test_layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\tfrom typing import Optional\n\timport numpy as np\n\timport pytest\n\timport tensorflow as tf\n\tfrom ...pedal import utility\n\tfrom ...tests import testing\n\tfrom .. import layers\n\tdef test_initializers():\n\t    testing.assert_unit_scale(layers.initializers.uniform(100)((1000,)), 0.05)\n", "    testing.assert_unit_scale(layers.initializers.normal(200)((1000,)), 0.05)\n\tdef test_layer_dense():\n\t    layer = layers.Dense(400, seed=123)\n\t    out = testing.output_and_gradients(layer, (3, 200), seed=456)\n\t    assert out[\"outputs\"].shape == (3, 400)\n\t    np.testing.assert_allclose(\n\t        np.std(out[\"outputs\"]) * np.std(out[\"grad_inputs\"]), 1, atol=0.05\n\t    )\n\t    testing.assert_unit_scale(out[\"grad_kernel\"], 0.05)\n\t    testing.assert_unit_scale(out[\"grad_bias\"], 0.05)\n", "def test_layer_causalconv1d():\n\t    # Choose kernel size << sequence length to reduce the effect of padding\n\t    # (which we don't account for in variance preservation)\n\t    layer = layers.CausalConv1D(filters=400, kernel_size=3, seed=321)\n\t    out = testing.output_and_gradients(layer, (4, 19, 200), seed=654)\n\t    assert out[\"outputs\"].shape == (4, 19, 400)\n\t    np.testing.assert_allclose(\n\t        np.std(out[\"outputs\"]) * np.std(out[\"grad_inputs\"]), 1, atol=0.1\n\t    )\n\t    testing.assert_unit_scale(out[\"grad_kernel\"], 0.05)\n", "    testing.assert_unit_scale(out[\"grad_bias\"], 0.05)\n\t    with pytest.raises(ValueError) as error:\n\t        layers.CausalConv1D(filters=16, kernel_size=5, groups=3)\n\t    assert \"Filters (16)\" in str(error)\n\t    assert \"groups (3)\" in str(error)\n\t    layer = layers.CausalConv1D(filters=15, kernel_size=5, groups=3)\n\t    with pytest.raises(ValueError) as error:\n\t        layer(tf.zeros((1, 10, 16)))\n\t    assert \"feature size (16)\" in str(error)\n\t    assert \"groups (3)\" in str(error)\n", "def test_layer_embedding():\n\t    random = np.random.Generator(np.random.PCG64(seed=200))\n\t    layer = layers.Embedding(table_size=40, embeddings_size=300, seed=100)\n\t    with tf.GradientTape() as tape:\n\t        outputs = layer(random.integers(layer.table_size, size=(3, 190)))\n\t    assert outputs.shape == (3, 190, 300)\n\t    testing.assert_unit_scale(outputs, tol=0.1)\n\t    grad_embeddings = tape.gradient(\n\t        outputs,\n\t        layer.embeddings,\n", "        testing.correlated_batch_random(random, outputs.shape),\n\t    )\n\t    grad_embeddings = tf.math.unsorted_segment_sum(\n\t        grad_embeddings.values, grad_embeddings.indices, grad_embeddings.shape[0]\n\t    )\n\t    testing.assert_unit_scale(grad_embeddings, tol=0.1)\n\t@pytest.mark.parametrize(\n\t    [\"norm_type\", \"alpha\"], [(\"pre\", 0.1), (\"post\", 0.1), (None, 0.1), (None, 0.9)]\n\t)\n\tdef test_layer_residual(\n", "    norm_type: Optional[str], alpha: float\n\t):  # also tests LayerNormalization\n\t    layer = layers.ResidualLayer(\n\t        layers.Dense(250, seed=4832), norm_type=norm_type, alpha=alpha\n\t    )\n\t    out = testing.output_and_gradients(layer, (29, 19, 250), seed=2393)\n\t    testing.assert_unit_scale(out[\"outputs\"], tol=0.1)\n\t    testing.assert_unit_scale(out[\"grad_inputs\"], tol=0.1)\n\t    for name, variable in utility.named_weights(layer):\n\t        if variable.trainable:\n", "            testing.assert_unit_scale(\n\t                out[f\"grad_{name}\"], tol=0.1, err_msg=f\"for grad_{name}\"\n\t            )\n\tdef test_layer_ffn():\n\t    layer = layers.FFNLayer(2, seeds=(48723, 7428))\n\t    out = testing.output_and_gradients(layer, (5, 6, 7), seed=200)\n\t    assert out[\"outputs\"].shape == (5, 6, 7)\n\t    assert {k: v.shape for k, v in utility.named_weights(layer)} == {\n\t        \"up.kernel\": (7, 14),\n\t        \"up.bias\": (14,),\n", "        \"down.kernel\": (14, 7),\n\t        \"down.bias\": (7,),\n\t    }\n\tdef test_layer_multi_head_attention():\n\t    layer = layers.MultiHeadAttention(\n\t        heads=5, head_size=16, frequencies=50, max_period=90, seeds=(50, 60, 70)\n\t    )\n\t    out = testing.output_and_gradients(layer, (29, 90, 64), seed=300)\n\t    assert out[\"outputs\"].shape == (29, 90, 64)\n\t    for key, _ in utility.named_weights(layer):\n", "        # When inputs are constant over the sequence axis, these gradients are\n\t        # approximately zero\n\t        if key not in {\"positional\", \"q_bias\"}:\n\t            # Nowhere near unit scale, but we'll just check a broad range\n\t            assert 0.1 < np.std(out[f\"grad_{key}\"]) < 10, f\"for {key}\"\n\tdef test_rnn():  # also tests RecurrentHighwayCell\n\t    layer = layers.RNN(\n\t        layers.RecurrentHighwayCell(hidden_size=128, rebias=1, seed=3893)\n\t    )\n\t    out = testing.output_and_gradients(layer, (29, 90, 64), seed=1398)\n", "    assert out[\"outputs\"].shape == (29, 90, 128)\n\t    for key, _ in utility.named_weights(layer):\n\t        # Nowhere near unit scale, but we'll just check a broad range\n\t        assert 0.1 < np.std(out[f\"grad_{key}\"]) < 10\n\tdef test_pad_and_shift_layer():\n\t    layer = layers.PadAndShiftLayer()\n\t    out = testing.output_and_gradients(layer, (21, 5, 128), seed=49824)\n\t    assert out[\"outputs\"].shape == (21, 5, 128)\n\t    testing.assert_unit_scale(out[\"grad_inputs\"][:, :-1, :], 0.05)\n\t    testing.assert_unit_scale(out[\"grad_padding\"], 0.05)\n"]}
