{"filename": "sdk/setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t    name='havenpy',\n\t    version='0.2.0',\n\t    author='Haven Technologies Inc.',\n\t    author_email='hello@havenllm.com',\n\t    description='Haven SDK',\n\t    packages=find_packages(),\n\t    install_requires=[\n\t        'grpcio==1.54.2',\n", "        'protobuf==4.23.3'\n\t    ],\n\t)\n"]}
{"filename": "sdk/test.py", "chunked_list": ["from havenpy import Haven\n\tclient = Haven(\"localhost:50051\", \"insecure\")\n\t#with open(\"./key.json\", \"r\") as f:\n\t\t#client.setup(f.read())\n\t\"\"\"arr = client.list_workers()\n\tfor a in arr.workers:\n\t\tprint(\"DELETE \", a.worker_name)\n\t\tclient.delete_inference_worker(a.worker_name)\"\"\"\n\t# print(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1))\n\t\"\"\"\n", "print(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\n\tprint(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-instruct\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1))\n\tprint(client.create_inference_worker(model_name=\"@huggingface/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\", quantization=\"int8\", gpu_type=\"T4\", gpu_count=1))\n\tprint(client.create_inference_worker(model_name=\"@huggingface/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\n\tprint(client.create_inference_worker(model_name=\"@huggingface/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1))\n\t\"\"\"\n\t\"\"\"client.chat_completion(\"haven-example-base-lj7ieyhi\", messages=[{\n\t    \"content\": \"Give me a recipe for cake!\",\n\t    \"role\": \"USER\"\n\t}], stream=True)\"\"\"\n", "# print(client.list_models())\n\tprint(client.list_workers())\n\t# print(client.create_inference_worker(\"@huggingface/lmsys/vicuna-7b-v1.3\", gpu_type=\"A100\", gpu_count=1, quantization=\"float16\"))\n\t# print(client.create_inference_worker(\"@huggingface/lmsys/vicuna-7b-v1.3\", gpu_type=\"T4\", gpu_count=1, quantization=\"float16\"))\n\t\"\"\"res = client.chat_completion(\"haven-w-vicuna-7b-v1-3-ljm41836\", messages=[{\n\t    \"content\": \"Write a newspaper article about Marc Zuckerberg.\",\n\t    \"role\": \"USER\"\n\t}], stream=True)\n\tfor r in res:\n\t    print(r.text, end=\"\", flush=True)\"\"\"\n", "# print(client.create_inference_worker(model_name=\"@huggingface/togethercomputer/RedPajama-INCITE-Chat-3B-v1\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\n\t\"\"\"history = []\n\thistory.append({\n\t\t\"content\": my_description,\n\t\t\"role\": \"USER\"\n\t})\n\twhile True:\n\t\tres = client.chat_completion(\"haven-redpajama-incite-chat-3b-v1-ljg7qwku\", messages=history, stream=True)\n\t\tmessage = \"\"\n\t\tfor r in res:\n", "\t\tmessage += r.text\n\t\t\tprint(r.text, end=\"\", flush=True)\n\t\tprint()\n\t\thistory.append({\n\t\t\t\"content\": message,\n\t\t\t\"role\": \"ASSISTANT\"\n\t\t})\n\t\tuser_input = input(\"Your response: \")\n\t\thistory.append({\n\t\t\t\"content\": user_input,\n", "\t\t\"role\": \"USER\"\n\t\t})\"\"\"\n\t#key_file = open(\"key.txt\", \"r\")\n\t#client.setup(key_file=key_file.read())\n\t# client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1)\n\t#res = client.generate(\"haven-example-base-lj7ieyhi\", \"Give me a recipe for cake!\", stream=True)\n\t#for r in res:\n\t\t#print(r.text)\n\t# client.pause_inference_worker(\"haven-example-base-lj7ieyhi\")\n\t#client.resume_inference_worker(\"haven-example-base-lj7ieyhi\")\n", "# client.delete_inference_worker(\"haven-example-base-lj7ieyhi\")\n\t# res = client.generate(\"test2\", \"Schreibe einen Nachrichtenartikel Ã¼ber die aktuelle Lage in Deutschland.\", stream=False)\n\t# print(res)\"\"\"\n"]}
{"filename": "sdk/havenpy/run.py", "chunked_list": ["import grpc\n\tfrom .interceptor import add_header\n\tfrom .pb import manager_pb2_grpc, manager_pb2\n\tfrom typing import List\n\tdef stream_to_string(stream: manager_pb2.CompletionResponse) -> str:\n\t\tres: str = \"\"\n\t\tfor response in stream:\n\t\t\tres += response.text\n\t\treturn res\n\tclass Haven:\n", "\tclient: manager_pb2_grpc.HavenStub\n\t\tdef __init__(self, url: str, token: str):\n\t\t\tchannel = grpc.insecure_channel(url)\n\t\t\tinterceptor = add_header('authorization', f'Bearer {token}')\n\t\t\tchannel = grpc.intercept_channel(channel, interceptor)\n\t\t\tself.client = manager_pb2_grpc.HavenStub(channel)\n\t\t\tself.setup()\n\t\tdef setup(self, key_file: str = None) -> None:\n\t\t\trequest = manager_pb2.SetupRequest(key_file=key_file)\n\t\t\tresponse: manager_pb2.SetupResponse = self.client.Setup(request)\n", "\t\tif hasattr(response, \"message\") and response.message != \"\":\n\t\t\t\tprint(response.message)\n\t\tdef chat_completion(self, worker_name: str, messages: List[manager_pb2.Message], stream: bool = False, max_tokens: int = -1, top_p: float = -1, top_k: int = -1, temperature: float = -1) -> manager_pb2.CompletionResponse or str:\n\t\t\trequest = manager_pb2.ChatCompletionRequest(worker_name=worker_name, messages=messages, max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t\t\tresponseStream: manager_pb2.CompletionResponse = self.client.ChatCompletion(request)\n\t\t\tif stream:\n\t\t\t\treturn responseStream\n\t\t\treturn stream_to_string(responseStream)\n\t\tdef completion(self, worker_name: str, prompt: str, stream: bool = False, max_tokens: int = -1, top_p: float = -1, top_k: int = -1, temperature: float = -1) -> manager_pb2.CompletionResponse or str:\n\t\t\t# TODO: we're currently not using stop_tokens\n", "\t\trequest = manager_pb2.CompletionRequest(worker_name=worker_name, prompt=prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t\t\tresponseStream: manager_pb2.CompletionResponse = self.client.Completion(request)\n\t\t\tif stream:\n\t\t\t\treturn responseStream\n\t\t\treturn stream_to_string(responseStream)\n\t\tdef list_models(self) -> manager_pb2.ListModelsResponse:\n\t\t\trequest = manager_pb2.Empty()\n\t\t\treturn self.client.ListModels(request)\n\t\tdef add_model(self, architecture: str, name: str, tokenizer: str, system_prompt: str = None, instruction_prefix: str = None, instruction_postfix: str = None, output_prefix: str = None, output_postfix: str = None) -> manager_pb2.Empty:\n\t\t\trequest = manager_pb2.Model(architecture=architecture, name=name, tokenizer=tokenizer, system_prompt=system_prompt, instruction_prefix=instruction_prefix, instruction_postfix=instruction_postfix, output_prefix=output_prefix, output_postfix=output_postfix)\n", "\t\treturn self.client.AddModel(request)\n\t\tdef delete_model(self, name: str) -> manager_pb2.Empty:\n\t\t\trequest = manager_pb2.ModelName(name=name)\n\t\t\treturn self.client.DeleteModel(request)\n\t\tdef list_workers(self) -> manager_pb2.ListWorkersResponse:\n\t\t\trequest = manager_pb2.Empty()\n\t\t\tresponse = self.client.ListWorkers(request)\n\t\t\t# Response is of a weird GRPC type, so we transform it to a list of dicts\n\t\t\t# with worker_name and status as string attributes\n\t\t\tworkers = []\n", "\t\tfor worker in response.workers:\n\t\t\t\t# Convert enum to string representation\n\t\t\t\tstatus = manager_pb2.Status.Name(worker.status)\n\t\t\t\tworkers.append({\n\t\t\t\t\t\"worker_name\": worker.worker_name,\n\t\t\t\t\t\"status\": status\n\t\t\t\t})\n\t\t\treturn workers\n\t\tdef create_inference_worker(self, model_name: str, quantization: str, worker_name: str = None, gpu_type: manager_pb2.GpuType = None, gpu_count: int = None, zone: str = None) -> manager_pb2.InferenceWorker:\n\t\t\trequest = manager_pb2.CreateInferenceWorkerRequest(model_name=model_name, quantization=quantization, worker_name=worker_name, gpu_type=gpu_type, gpu_count=gpu_count, zone=zone)\n", "\t\tresponse = self.client.CreateInferenceWorker(request)\n\t\t\treturn response.worker_name\n\t\tdef pause_inference_worker(self, worker_name: str) -> manager_pb2.InferenceWorker:\n\t\t\trequest = manager_pb2.InferenceWorker(worker_name=worker_name)\n\t\t\treturn self.client.PauseInferenceWorker(request)\n\t\tdef resume_inference_worker(self, worker_name: str) -> manager_pb2.InferenceWorker:\n\t\t\trequest = manager_pb2.InferenceWorker(worker_name=worker_name)\n\t\t\treturn self.client.ResumeInferenceWorker(request)\n\t\tdef delete_inference_worker(self, worker_name: str) -> manager_pb2.InferenceWorker:\n\t\t\trequest = manager_pb2.InferenceWorker(worker_name=worker_name)\n", "\t\treturn self.client.DeleteInferenceWorker(request)\n"]}
{"filename": "sdk/havenpy/interceptor.py", "chunked_list": ["#\n\t# See https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers\n\t#\n\timport grpc\n\timport collections\n\tclass _GenericClientInterceptor(grpc.UnaryUnaryClientInterceptor,\n\t                                grpc.UnaryStreamClientInterceptor,\n\t                                grpc.StreamUnaryClientInterceptor,\n\t                                grpc.StreamStreamClientInterceptor):\n\t    def __init__(self, interceptor_function):\n", "        self._fn = interceptor_function\n\t    def intercept_unary_unary(self, continuation, client_call_details, request):\n\t        new_details, new_request_iterator, postprocess = self._fn(\n\t            client_call_details, iter((request,)), False, False)\n\t        response = continuation(new_details, next(new_request_iterator))\n\t        return postprocess(response) if postprocess else response\n\t    def intercept_unary_stream(self, continuation, client_call_details,\n\t                               request):\n\t        new_details, new_request_iterator, postprocess = self._fn(\n\t            client_call_details, iter((request,)), False, True)\n", "        response_it = continuation(new_details, next(new_request_iterator))\n\t        return postprocess(response_it) if postprocess else response_it\n\t    def intercept_stream_unary(self, continuation, client_call_details,\n\t                               request_iterator):\n\t        new_details, new_request_iterator, postprocess = self._fn(\n\t            client_call_details, request_iterator, True, False)\n\t        response = continuation(new_details, new_request_iterator)\n\t        return postprocess(response) if postprocess else response\n\t    def intercept_stream_stream(self, continuation, client_call_details,\n\t                                request_iterator):\n", "        new_details, new_request_iterator, postprocess = self._fn(\n\t            client_call_details, request_iterator, True, True)\n\t        response_it = continuation(new_details, new_request_iterator)\n\t        return postprocess(response_it) if postprocess else response_it\n\tclass _ClientCallDetails(\n\t        collections.namedtuple(\n\t            '_ClientCallDetails',\n\t            ('method', 'timeout', 'metadata', 'credentials')),\n\t        grpc.ClientCallDetails):\n\t    pass\n", "def add_header(key, value):\n\t\tdef intercept_call(client_call_details, request_iterator, request_streaming, response_streaming):\n\t\t\tmetadata = []\n\t\t\tif client_call_details.metadata != None:\n\t\t\t\tmetadata = list(client_call_details.metadata)\n\t\t\tmetadata.append((key, value))\n\t\t\tclient_call_details = _ClientCallDetails(\n\t\t\t\tclient_call_details.method, client_call_details.timeout, metadata, \n\t\t\t\tclient_call_details.credentials)\n\t\t\treturn client_call_details, request_iterator, None\n", "\treturn _GenericClientInterceptor(intercept_call)\n"]}
{"filename": "sdk/havenpy/__init__.py", "chunked_list": ["from .run import Haven"]}
{"filename": "sdk/havenpy/pb/manager_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: manager.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\rmanager.proto\\x12\\x05haven\\\"\\x07\\n\\x05\\x45mpty\\\"2\\n\\x0cSetupRequest\\x12\\x15\\n\\x08key_file\\x18\\x01 \\x01(\\tH\\x00\\x88\\x01\\x01\\x42\\x0b\\n\\t_key_file\\\"1\\n\\rSetupResponse\\x12\\x14\\n\\x07message\\x18\\x01 \\x01(\\tH\\x00\\x88\\x01\\x01\\x42\\n\\n\\x08_message\\\"5\\n\\x07Message\\x12\\x19\\n\\x04role\\x18\\x01 \\x01(\\x0e\\x32\\x0b.haven.Role\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t\\\"\\xdc\\x01\\n\\x15\\x43hatCompletionRequest\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t\\x12 \\n\\x08messages\\x18\\x02 \\x03(\\x0b\\x32\\x0e.haven.Message\\x12\\x17\\n\\nmax_tokens\\x18\\x03 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x04 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x05 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x06 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\xdb\\x01\\n\\x11\\x43ompletionRequest\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06prompt\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\x0bstop_tokens\\x18\\x07 \\x03(\\t\\x12\\x17\\n\\nmax_tokens\\x18\\x03 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x04 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x05 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x06 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\\"\\n\\x12\\x43ompletionResponse\\x12\\x0c\\n\\x04text\\x18\\x01 \\x01(\\t\\\"\\xbc\\x02\\n\\x05Model\\x12\\x14\\n\\x0c\\x61rchitecture\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\ttokenizer\\x18\\x03 \\x01(\\t\\x12\\x1a\\n\\rsystem_prompt\\x18\\x04 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12\\x1f\\n\\x12instruction_prefix\\x18\\x05 \\x01(\\tH\\x01\\x88\\x01\\x01\\x12 \\n\\x13instruction_postfix\\x18\\x06 \\x01(\\tH\\x02\\x88\\x01\\x01\\x12\\x1a\\n\\routput_prefix\\x18\\x07 \\x01(\\tH\\x03\\x88\\x01\\x01\\x12\\x1b\\n\\x0eoutput_postfix\\x18\\x08 \\x01(\\tH\\x04\\x88\\x01\\x01\\x42\\x10\\n\\x0e_system_promptB\\x15\\n\\x13_instruction_prefixB\\x16\\n\\x14_instruction_postfixB\\x10\\n\\x0e_output_prefixB\\x11\\n\\x0f_output_postfix\\\"\\x19\\n\\tModelName\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\\"2\\n\\x12ListModelsResponse\\x12\\x1c\\n\\x06models\\x18\\x01 \\x03(\\x0b\\x32\\x0c.haven.Model\\\"<\\n\\x06Worker\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t\\x12\\x1d\\n\\x06status\\x18\\x02 \\x01(\\x0e\\x32\\r.haven.Status\\\"5\\n\\x13ListWorkersResponse\\x12\\x1e\\n\\x07workers\\x18\\x01 \\x03(\\x0b\\x32\\r.haven.Worker\\\"\\xe8\\x01\\n\\x1c\\x43reateInferenceWorkerRequest\\x12\\x12\\n\\nmodel_name\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0cquantization\\x18\\x02 \\x01(\\t\\x12\\x18\\n\\x0bworker_name\\x18\\x03 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12%\\n\\x08gpu_type\\x18\\x04 \\x01(\\x0e\\x32\\x0e.haven.GpuTypeH\\x01\\x88\\x01\\x01\\x12\\x16\\n\\tgpu_count\\x18\\x06 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x11\\n\\x04zone\\x18\\x07 \\x01(\\tH\\x03\\x88\\x01\\x01\\x42\\x0e\\n\\x0c_worker_nameB\\x0b\\n\\t_gpu_typeB\\x0c\\n\\n_gpu_countB\\x07\\n\\x05_zone\\\"&\\n\\x0fInferenceWorker\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t*\\x1f\\n\\x04Role\\x12\\r\\n\\tASSISTANT\\x10\\x00\\x12\\x08\\n\\x04USER\\x10\\x01*8\\n\\x06Status\\x12\\n\\n\\x06ONLINE\\x10\\x00\\x12\\x0b\\n\\x07LOADING\\x10\\x01\\x12\\n\\n\\x06PAUSED\\x10\\x02\\x12\\t\\n\\x05\\x45RROR\\x10\\x03**\\n\\x07GpuType\\x12\\x08\\n\\x04\\x41\\x31\\x30\\x30\\x10\\x00\\x12\\r\\n\\tA100_80GB\\x10\\x01\\x12\\x06\\n\\x02T4\\x10\\x02\\x32\\xda\\x05\\n\\x05Haven\\x12\\x34\\n\\x05Setup\\x12\\x13.haven.SetupRequest\\x1a\\x14.haven.SetupResponse\\\"\\x00\\x12M\\n\\x0e\\x43hatCompletion\\x12\\x1c.haven.ChatCompletionRequest\\x1a\\x19.haven.CompletionResponse\\\"\\x00\\x30\\x01\\x12\\x45\\n\\nCompletion\\x12\\x18.haven.CompletionRequest\\x1a\\x19.haven.CompletionResponse\\\"\\x00\\x30\\x01\\x12\\x37\\n\\nListModels\\x12\\x0c.haven.Empty\\x1a\\x19.haven.ListModelsResponse\\\"\\x00\\x12(\\n\\x08\\x41\\x64\\x64Model\\x12\\x0c.haven.Model\\x1a\\x0c.haven.Empty\\\"\\x00\\x12/\\n\\x0b\\x44\\x65leteModel\\x12\\x10.haven.ModelName\\x1a\\x0c.haven.Empty\\\"\\x00\\x12\\x39\\n\\x0bListWorkers\\x12\\x0c.haven.Empty\\x1a\\x1a.haven.ListWorkersResponse\\\"\\x00\\x12V\\n\\x15\\x43reateInferenceWorker\\x12#.haven.CreateInferenceWorkerRequest\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x12H\\n\\x14PauseInferenceWorker\\x12\\x16.haven.InferenceWorker\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x12I\\n\\x15ResumeInferenceWorker\\x12\\x16.haven.InferenceWorker\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x12I\\n\\x15\\x44\\x65leteInferenceWorker\\x12\\x16.haven.InferenceWorker\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x62\\x06proto3')\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'manager_pb2', globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t  DESCRIPTOR._options = None\n\t  _ROLE._serialized_start=1462\n\t  _ROLE._serialized_end=1493\n\t  _STATUS._serialized_start=1495\n\t  _STATUS._serialized_end=1551\n\t  _GPUTYPE._serialized_start=1553\n", "  _GPUTYPE._serialized_end=1595\n\t  _EMPTY._serialized_start=24\n\t  _EMPTY._serialized_end=31\n\t  _SETUPREQUEST._serialized_start=33\n\t  _SETUPREQUEST._serialized_end=83\n\t  _SETUPRESPONSE._serialized_start=85\n\t  _SETUPRESPONSE._serialized_end=134\n\t  _MESSAGE._serialized_start=136\n\t  _MESSAGE._serialized_end=189\n\t  _CHATCOMPLETIONREQUEST._serialized_start=192\n", "  _CHATCOMPLETIONREQUEST._serialized_end=412\n\t  _COMPLETIONREQUEST._serialized_start=415\n\t  _COMPLETIONREQUEST._serialized_end=634\n\t  _COMPLETIONRESPONSE._serialized_start=636\n\t  _COMPLETIONRESPONSE._serialized_end=670\n\t  _MODEL._serialized_start=673\n\t  _MODEL._serialized_end=989\n\t  _MODELNAME._serialized_start=991\n\t  _MODELNAME._serialized_end=1016\n\t  _LISTMODELSRESPONSE._serialized_start=1018\n", "  _LISTMODELSRESPONSE._serialized_end=1068\n\t  _WORKER._serialized_start=1070\n\t  _WORKER._serialized_end=1130\n\t  _LISTWORKERSRESPONSE._serialized_start=1132\n\t  _LISTWORKERSRESPONSE._serialized_end=1185\n\t  _CREATEINFERENCEWORKERREQUEST._serialized_start=1188\n\t  _CREATEINFERENCEWORKERREQUEST._serialized_end=1420\n\t  _INFERENCEWORKER._serialized_start=1422\n\t  _INFERENCEWORKER._serialized_end=1460\n\t  _HAVEN._serialized_start=1598\n", "  _HAVEN._serialized_end=2328\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "sdk/havenpy/pb/__init__.py", "chunked_list": []}
{"filename": "sdk/havenpy/pb/manager_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\tfrom . import manager_pb2 as manager__pb2\n\tclass HavenStub(object):\n\t    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n\t            channel: A grpc.Channel.\n", "        \"\"\"\n\t        self.Setup = channel.unary_unary(\n\t                '/haven.Haven/Setup',\n\t                request_serializer=manager__pb2.SetupRequest.SerializeToString,\n\t                response_deserializer=manager__pb2.SetupResponse.FromString,\n\t                )\n\t        self.ChatCompletion = channel.unary_stream(\n\t                '/haven.Haven/ChatCompletion',\n\t                request_serializer=manager__pb2.ChatCompletionRequest.SerializeToString,\n\t                response_deserializer=manager__pb2.CompletionResponse.FromString,\n", "                )\n\t        self.Completion = channel.unary_stream(\n\t                '/haven.Haven/Completion',\n\t                request_serializer=manager__pb2.CompletionRequest.SerializeToString,\n\t                response_deserializer=manager__pb2.CompletionResponse.FromString,\n\t                )\n\t        self.ListModels = channel.unary_unary(\n\t                '/haven.Haven/ListModels',\n\t                request_serializer=manager__pb2.Empty.SerializeToString,\n\t                response_deserializer=manager__pb2.ListModelsResponse.FromString,\n", "                )\n\t        self.AddModel = channel.unary_unary(\n\t                '/haven.Haven/AddModel',\n\t                request_serializer=manager__pb2.Model.SerializeToString,\n\t                response_deserializer=manager__pb2.Empty.FromString,\n\t                )\n\t        self.DeleteModel = channel.unary_unary(\n\t                '/haven.Haven/DeleteModel',\n\t                request_serializer=manager__pb2.ModelName.SerializeToString,\n\t                response_deserializer=manager__pb2.Empty.FromString,\n", "                )\n\t        self.ListWorkers = channel.unary_unary(\n\t                '/haven.Haven/ListWorkers',\n\t                request_serializer=manager__pb2.Empty.SerializeToString,\n\t                response_deserializer=manager__pb2.ListWorkersResponse.FromString,\n\t                )\n\t        self.CreateInferenceWorker = channel.unary_unary(\n\t                '/haven.Haven/CreateInferenceWorker',\n\t                request_serializer=manager__pb2.CreateInferenceWorkerRequest.SerializeToString,\n\t                response_deserializer=manager__pb2.InferenceWorker.FromString,\n", "                )\n\t        self.PauseInferenceWorker = channel.unary_unary(\n\t                '/haven.Haven/PauseInferenceWorker',\n\t                request_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t                response_deserializer=manager__pb2.InferenceWorker.FromString,\n\t                )\n\t        self.ResumeInferenceWorker = channel.unary_unary(\n\t                '/haven.Haven/ResumeInferenceWorker',\n\t                request_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t                response_deserializer=manager__pb2.InferenceWorker.FromString,\n", "                )\n\t        self.DeleteInferenceWorker = channel.unary_unary(\n\t                '/haven.Haven/DeleteInferenceWorker',\n\t                request_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t                response_deserializer=manager__pb2.InferenceWorker.FromString,\n\t                )\n\tclass HavenServicer(object):\n\t    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t    def Setup(self, request, context):\n\t        \"\"\"Setup (first time starting the manager)\n", "        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def ChatCompletion(self, request, context):\n\t        \"\"\"Generate text.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n", "    def Completion(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def ListModels(self, request, context):\n\t        \"\"\"Get the list of models and their descriptions.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n", "        raise NotImplementedError('Method not implemented!')\n\t    def AddModel(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def DeleteModel(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n", "        raise NotImplementedError('Method not implemented!')\n\t    def ListWorkers(self, request, context):\n\t        \"\"\"Get the list of workers and their statuses.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def CreateInferenceWorker(self, request, context):\n\t        \"\"\"Inference worker management.\n\t        \"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def PauseInferenceWorker(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def ResumeInferenceWorker(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def DeleteInferenceWorker(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\tdef add_HavenServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n", "            'Setup': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.Setup,\n\t                    request_deserializer=manager__pb2.SetupRequest.FromString,\n\t                    response_serializer=manager__pb2.SetupResponse.SerializeToString,\n\t            ),\n\t            'ChatCompletion': grpc.unary_stream_rpc_method_handler(\n\t                    servicer.ChatCompletion,\n\t                    request_deserializer=manager__pb2.ChatCompletionRequest.FromString,\n\t                    response_serializer=manager__pb2.CompletionResponse.SerializeToString,\n\t            ),\n", "            'Completion': grpc.unary_stream_rpc_method_handler(\n\t                    servicer.Completion,\n\t                    request_deserializer=manager__pb2.CompletionRequest.FromString,\n\t                    response_serializer=manager__pb2.CompletionResponse.SerializeToString,\n\t            ),\n\t            'ListModels': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.ListModels,\n\t                    request_deserializer=manager__pb2.Empty.FromString,\n\t                    response_serializer=manager__pb2.ListModelsResponse.SerializeToString,\n\t            ),\n", "            'AddModel': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.AddModel,\n\t                    request_deserializer=manager__pb2.Model.FromString,\n\t                    response_serializer=manager__pb2.Empty.SerializeToString,\n\t            ),\n\t            'DeleteModel': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.DeleteModel,\n\t                    request_deserializer=manager__pb2.ModelName.FromString,\n\t                    response_serializer=manager__pb2.Empty.SerializeToString,\n\t            ),\n", "            'ListWorkers': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.ListWorkers,\n\t                    request_deserializer=manager__pb2.Empty.FromString,\n\t                    response_serializer=manager__pb2.ListWorkersResponse.SerializeToString,\n\t            ),\n\t            'CreateInferenceWorker': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.CreateInferenceWorker,\n\t                    request_deserializer=manager__pb2.CreateInferenceWorkerRequest.FromString,\n\t                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t            ),\n", "            'PauseInferenceWorker': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.PauseInferenceWorker,\n\t                    request_deserializer=manager__pb2.InferenceWorker.FromString,\n\t                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t            ),\n\t            'ResumeInferenceWorker': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.ResumeInferenceWorker,\n\t                    request_deserializer=manager__pb2.InferenceWorker.FromString,\n\t                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t            ),\n", "            'DeleteInferenceWorker': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.DeleteInferenceWorker,\n\t                    request_deserializer=manager__pb2.InferenceWorker.FromString,\n\t                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n\t            ),\n\t    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\n\t            'haven.Haven', rpc_method_handlers)\n\t    server.add_generic_rpc_handlers((generic_handler,))\n\t # This class is part of an EXPERIMENTAL API.\n", "class Haven(object):\n\t    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t    @staticmethod\n\t    def Setup(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n", "            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/Setup',\n\t            manager__pb2.SetupRequest.SerializeToString,\n\t            manager__pb2.SetupResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def ChatCompletion(request,\n", "            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_stream(request, target, '/haven.Haven/ChatCompletion',\n", "            manager__pb2.ChatCompletionRequest.SerializeToString,\n\t            manager__pb2.CompletionResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def Completion(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n", "            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_stream(request, target, '/haven.Haven/Completion',\n\t            manager__pb2.CompletionRequest.SerializeToString,\n\t            manager__pb2.CompletionResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n", "    @staticmethod\n\t    def ListModels(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n", "            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/ListModels',\n\t            manager__pb2.Empty.SerializeToString,\n\t            manager__pb2.ListModelsResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def AddModel(request,\n\t            target,\n\t            options=(),\n", "            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/AddModel',\n\t            manager__pb2.Model.SerializeToString,\n\t            manager__pb2.Empty.FromString,\n", "            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def DeleteModel(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n", "            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/DeleteModel',\n\t            manager__pb2.ModelName.SerializeToString,\n\t            manager__pb2.Empty.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def ListWorkers(request,\n", "            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/ListWorkers',\n", "            manager__pb2.Empty.SerializeToString,\n\t            manager__pb2.ListWorkersResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def CreateInferenceWorker(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n", "            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/CreateInferenceWorker',\n\t            manager__pb2.CreateInferenceWorkerRequest.SerializeToString,\n\t            manager__pb2.InferenceWorker.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n", "    @staticmethod\n\t    def PauseInferenceWorker(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n", "            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/PauseInferenceWorker',\n\t            manager__pb2.InferenceWorker.SerializeToString,\n\t            manager__pb2.InferenceWorker.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def ResumeInferenceWorker(request,\n\t            target,\n\t            options=(),\n", "            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/ResumeInferenceWorker',\n\t            manager__pb2.InferenceWorker.SerializeToString,\n\t            manager__pb2.InferenceWorker.FromString,\n", "            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def DeleteInferenceWorker(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n", "            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/haven.Haven/DeleteInferenceWorker',\n\t            manager__pb2.InferenceWorker.SerializeToString,\n\t            manager__pb2.InferenceWorker.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n"]}
{"filename": "worker/app/__main__.py", "chunked_list": ["import os\n\timport json\n\timport asyncio\n\timport grpc\n\tfrom transformers import TextIteratorStreamer\n\tfrom app.pb import worker_pb2, worker_pb2_grpc\n\tfrom app.worker.inference_worker import InferenceClient\n\tfrom app.worker.models.inference_utils.parameter_passing import get_inference_parameter_dict\n\tABSOLUTE_PATH = os.path.dirname(__file__)\n\tpath_to_config =os.path.join(ABSOLUTE_PATH, \"../config.json\")\n", "config = json.load(open(path_to_config, \"r\"))\n\tinference_client = InferenceClient(config=config)\n\trunning = True\n\tclass WorkerService(worker_pb2_grpc.WorkerServiceServicer):\n\t    def Health(self, request, context):\n\t        global running\n\t        status = worker_pb2.WorkerStatus.OK if running else worker_pb2.WorkerStatus.STOPPING\n\t        return worker_pb2.HealthResponse(status=status)\n\t    def Shutdown(self, request, context):\n\t        global running\n", "        running = False\n\t        return worker_pb2.ShutdownResponse()\n\t    async def ChatCompletion(self, request: worker_pb2.ChatCompletionRequest, context):\n\t        \"\"\"\n\t            Haven supports chat-models and non-chat models. We can distinguish between the two \n\t            by checking if the instructionPrefix is part of the config that is passed to the worker.\n\t        \"\"\"\n\t        if \"instructionPrefix\" not in config:\n\t            await context.abort(grpc.StatusCode.FAILED_PRECONDITION, \"This worker only supports non-chat completion requests. Refer to the documentation if you are unsure what this means.\")\n\t            return\n", "        # Now we can handle the request\n\t        messages = list(request.messages)\n\t        inference_params = get_inference_parameter_dict(dict(max_tokens=request.max_tokens, top_p=request.top_p, top_k=request.top_k, temperature=request.temperature))\n\t        streamer = inference_client.complete_chat(messages=messages, inference_params=inference_params)\n\t        if isinstance(streamer, TextIteratorStreamer):\n\t            for text in streamer:\n\t                if inference_client.model_engine.model_config[\"instructionPrefix\"] in text:\n\t                    break\n\t                yield worker_pb2.ChatCompletionResponse(text=text)\n\t        else:\n", "            potential_stop_string = \"\"\n\t            async for text in streamer:\n\t                if potential_stop_string+text in inference_client.model_engine.model_config[\"instructionPrefix\"]:\n\t                            potential_stop_string += text\n\t                            continue\n\t                yield worker_pb2.CompletionResponse(text=potential_stop_string+text)\n\t                potential_stop_string = \"\"\n\t    async def Completion(self, request: worker_pb2.CompletionRequest, context):\n\t        prompt = request.prompt\n\t        stop_tokens = list(request.stop_tokens)\n", "        inference_params = get_inference_parameter_dict(dict(max_tokens=request.max_tokens, top_p=request.top_p, top_k=request.top_k, temperature=request.temperature))\n\t        streamer = inference_client.complete(prompt=prompt, stop_tokens=stop_tokens, inference_params=inference_params)\n\t        if isinstance(streamer, TextIteratorStreamer):\n\t            for text in streamer:\n\t                yield worker_pb2.CompletionResponse(text=text)\n\t        else:\n\t            async for text in streamer:                \n\t                yield worker_pb2.CompletionResponse(text=text)\n\tasync def serve():\n\t    server = grpc.aio.server()\n", "    worker_pb2_grpc.add_WorkerServiceServicer_to_server(\n\t        WorkerService(), server)\n\t    listen_address = '[::]:50051'  # Set your desired listening address\n\t    server.add_insecure_port(listen_address)\n\t    await server.start()\n\t    await server.wait_for_termination()\n\tif __name__ == '__main__':\n\t    print(\"starting server...\")\n\t    asyncio.run(serve())"]}
{"filename": "worker/app/__init__.py", "chunked_list": []}
{"filename": "worker/app/pb/worker_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\tfrom . import worker_pb2 as worker__pb2\n\tclass WorkerServiceStub(object):\n\t    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n\t            channel: A grpc.Channel.\n", "        \"\"\"\n\t        self.Health = channel.unary_unary(\n\t                '/worker.WorkerService/Health',\n\t                request_serializer=worker__pb2.HealthRequest.SerializeToString,\n\t                response_deserializer=worker__pb2.HealthResponse.FromString,\n\t                )\n\t        self.Shutdown = channel.unary_unary(\n\t                '/worker.WorkerService/Shutdown',\n\t                request_serializer=worker__pb2.ShutdownRequest.SerializeToString,\n\t                response_deserializer=worker__pb2.ShutdownResponse.FromString,\n", "                )\n\t        self.ChatCompletion = channel.unary_stream(\n\t                '/worker.WorkerService/ChatCompletion',\n\t                request_serializer=worker__pb2.ChatCompletionRequest.SerializeToString,\n\t                response_deserializer=worker__pb2.CompletionResponse.FromString,\n\t                )\n\t        self.Completion = channel.unary_stream(\n\t                '/worker.WorkerService/Completion',\n\t                request_serializer=worker__pb2.CompletionRequest.SerializeToString,\n\t                response_deserializer=worker__pb2.CompletionResponse.FromString,\n", "                )\n\tclass WorkerServiceServicer(object):\n\t    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t    def Health(self, request, context):\n\t        \"\"\"Check health of the worker.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def Shutdown(self, request, context):\n", "        \"\"\"Shutdown the worker.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def ChatCompletion(self, request, context):\n\t        \"\"\"Generate text from chat history.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n", "        raise NotImplementedError('Method not implemented!')\n\t    def Completion(self, request, context):\n\t        \"\"\"Generate text from a prompt.\n\t        \"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\tdef add_WorkerServiceServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t            'Health': grpc.unary_unary_rpc_method_handler(\n", "                    servicer.Health,\n\t                    request_deserializer=worker__pb2.HealthRequest.FromString,\n\t                    response_serializer=worker__pb2.HealthResponse.SerializeToString,\n\t            ),\n\t            'Shutdown': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.Shutdown,\n\t                    request_deserializer=worker__pb2.ShutdownRequest.FromString,\n\t                    response_serializer=worker__pb2.ShutdownResponse.SerializeToString,\n\t            ),\n\t            'ChatCompletion': grpc.unary_stream_rpc_method_handler(\n", "                    servicer.ChatCompletion,\n\t                    request_deserializer=worker__pb2.ChatCompletionRequest.FromString,\n\t                    response_serializer=worker__pb2.CompletionResponse.SerializeToString,\n\t            ),\n\t            'Completion': grpc.unary_stream_rpc_method_handler(\n\t                    servicer.Completion,\n\t                    request_deserializer=worker__pb2.CompletionRequest.FromString,\n\t                    response_serializer=worker__pb2.CompletionResponse.SerializeToString,\n\t            ),\n\t    }\n", "    generic_handler = grpc.method_handlers_generic_handler(\n\t            'worker.WorkerService', rpc_method_handlers)\n\t    server.add_generic_rpc_handlers((generic_handler,))\n\t # This class is part of an EXPERIMENTAL API.\n\tclass WorkerService(object):\n\t    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t    @staticmethod\n\t    def Health(request,\n\t            target,\n\t            options=(),\n", "            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/worker.WorkerService/Health',\n\t            worker__pb2.HealthRequest.SerializeToString,\n\t            worker__pb2.HealthResponse.FromString,\n", "            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def Shutdown(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n", "            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/worker.WorkerService/Shutdown',\n\t            worker__pb2.ShutdownRequest.SerializeToString,\n\t            worker__pb2.ShutdownResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def ChatCompletion(request,\n", "            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_stream(request, target, '/worker.WorkerService/ChatCompletion',\n", "            worker__pb2.ChatCompletionRequest.SerializeToString,\n\t            worker__pb2.CompletionResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def Completion(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n", "            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_stream(request, target, '/worker.WorkerService/Completion',\n\t            worker__pb2.CompletionRequest.SerializeToString,\n\t            worker__pb2.CompletionResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n"]}
{"filename": "worker/app/pb/__init__.py", "chunked_list": []}
{"filename": "worker/app/pb/worker_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: worker.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x0cworker.proto\\x12\\x06worker\\\"\\x0f\\n\\rHealthRequest\\\"6\\n\\x0eHealthResponse\\x12$\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x14.worker.WorkerStatus\\\"\\x11\\n\\x0fShutdownRequest\\\"\\x12\\n\\x10ShutdownResponse\\\"6\\n\\x07Message\\x12\\x1a\\n\\x04role\\x18\\x01 \\x01(\\x0e\\x32\\x0c.worker.Role\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t\\\"\\xc8\\x01\\n\\x15\\x43hatCompletionRequest\\x12!\\n\\x08messages\\x18\\x01 \\x03(\\x0b\\x32\\x0f.worker.Message\\x12\\x17\\n\\nmax_tokens\\x18\\x02 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x03 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x04 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x05 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\xc6\\x01\\n\\x11\\x43ompletionRequest\\x12\\x0e\\n\\x06prompt\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bstop_tokens\\x18\\x06 \\x03(\\t\\x12\\x17\\n\\nmax_tokens\\x18\\x02 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x03 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x04 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x05 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\\"\\n\\x12\\x43ompletionResponse\\x12\\x0c\\n\\x04text\\x18\\x01 \\x01(\\t*1\\n\\x0cWorkerStatus\\x12\\x06\\n\\x02OK\\x10\\x00\\x12\\x0c\\n\\x08STOPPING\\x10\\x01\\x12\\x0b\\n\\x07OFFLINE\\x10\\x63*\\x1f\\n\\x04Role\\x12\\r\\n\\tASSISTANT\\x10\\x00\\x12\\x08\\n\\x04USER\\x10\\x01\\x32\\xa5\\x02\\n\\rWorkerService\\x12\\x39\\n\\x06Health\\x12\\x15.worker.HealthRequest\\x1a\\x16.worker.HealthResponse\\\"\\x00\\x12?\\n\\x08Shutdown\\x12\\x17.worker.ShutdownRequest\\x1a\\x18.worker.ShutdownResponse\\\"\\x00\\x12O\\n\\x0e\\x43hatCompletion\\x12\\x1d.worker.ChatCompletionRequest\\x1a\\x1a.worker.CompletionResponse\\\"\\x00\\x30\\x01\\x12G\\n\\nCompletion\\x12\\x19.worker.CompletionRequest\\x1a\\x1a.worker.CompletionResponse\\\"\\x00\\x30\\x01\\x62\\x06proto3')\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'worker_pb2', globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t  DESCRIPTOR._options = None\n\t  _WORKERSTATUS._serialized_start=632\n\t  _WORKERSTATUS._serialized_end=681\n\t  _ROLE._serialized_start=683\n\t  _ROLE._serialized_end=714\n\t  _HEALTHREQUEST._serialized_start=24\n", "  _HEALTHREQUEST._serialized_end=39\n\t  _HEALTHRESPONSE._serialized_start=41\n\t  _HEALTHRESPONSE._serialized_end=95\n\t  _SHUTDOWNREQUEST._serialized_start=97\n\t  _SHUTDOWNREQUEST._serialized_end=114\n\t  _SHUTDOWNRESPONSE._serialized_start=116\n\t  _SHUTDOWNRESPONSE._serialized_end=134\n\t  _MESSAGE._serialized_start=136\n\t  _MESSAGE._serialized_end=190\n\t  _CHATCOMPLETIONREQUEST._serialized_start=193\n", "  _CHATCOMPLETIONREQUEST._serialized_end=393\n\t  _COMPLETIONREQUEST._serialized_start=396\n\t  _COMPLETIONREQUEST._serialized_end=594\n\t  _COMPLETIONRESPONSE._serialized_start=596\n\t  _COMPLETIONRESPONSE._serialized_end=630\n\t  _WORKERSERVICE._serialized_start=717\n\t  _WORKERSERVICE._serialized_end=1010\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "worker/app/worker/training_worker.py", "chunked_list": ["import json\n\tfrom models.model_registry import ModelRegistry\n\tclass TrainingClient:\n\t    def __init__(self, config):\n\t        with open(config, 'r') as f:\n\t            self.config = json.load(f)\n\t        self.model_engine = ModelRegistry.REGISTRY[self.config[\"base_class\"]](self.config)\n\t        self.model_engine.prepare_model_for_training()\n\t        self.model_engine.prepare_data_for_training()\n\t    def train(self):\n", "        self.model_engine.train()\n"]}
{"filename": "worker/app/worker/__init__.py", "chunked_list": []}
{"filename": "worker/app/worker/inference_worker.py", "chunked_list": ["from .models.model_registry import ModelRegistry\n\tclass InferenceClient:\n\t    def __init__(self, config):\n\t        self.config = config\n\t        self.model_engine = ModelRegistry.REGISTRY[self.config[\"architecture\"]](self.config)\n\t        self.model_engine.prepare_for_inference()\n\t    def complete_chat(self, messages, inference_params):\n\t        prompt = self.model_engine.create_prompt_from_messages(messages)\n\t        return self.model_engine.generate_stream(prompt, **inference_params)\n\t    def complete(self, prompt, stop_tokens, inference_params):\n", "        return self.model_engine.generate_stream(prompt, stop_tokens=stop_tokens, **inference_params)\n"]}
{"filename": "worker/app/worker/models/model_registry.py", "chunked_list": ["class ModelRegistry(type):\n\t    REGISTRY = {}\n\t    def __new__(cls, name, bases, attrs):\n\t        # instantiate a new type corresponding to the type of class being defined\n\t        # this is currently RegisterBase but in child classes will be the child class\n\t        new_cls = type.__new__(cls, name, bases, attrs)\n\t        cls.REGISTRY[new_cls.architecture_name] = new_cls\n\t        return new_cls\n\t    @classmethod\n\t    def get_registry(cls):\n", "        return dict(cls.REGISTRY)\n\tclass RegisteredModel(metaclass=ModelRegistry):\n\t    architecture_name = \"model\"\n\t# REGISTER ALL MODELS\n\tfrom .base_causal import AutoCausalModel\n\tfrom .bigcode_15b import BigCode15B\n\tfrom .falcon_7b import Falcon7BModel\n\tfrom .gpt_neox_3b import GPTNeoX3B\n\tfrom .gpt_neox_7b import GPTNeoX7B\n\tfrom .gpt_neox_12b import GPTNeoX12B\n", "from .llama_7b import Llama7B\n\tfrom .llama_13b import Llama13B\n\tfrom .mpt_7b import Mpt7B\n\tfrom .mpt_30b import Mpt30B\n\tfrom .vllm_causal import VllmCausalModel\n"]}
{"filename": "worker/app/worker/models/bigcode_15b.py", "chunked_list": ["from typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tclass BigCode15B(VllmCausalModel):\n\t    architecture_name = \"bigcode_15b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n", "        super().prepare_for_inference()\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 8000, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=stop_tokens, max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text\n"]}
{"filename": "worker/app/worker/models/gpt_neox_12b.py", "chunked_list": ["from typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tclass GPTNeoX12B(VllmCausalModel):\n\t    architecture_name = \"gpt_neox_12b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n", "        super().prepare_for_inference()\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text\n"]}
{"filename": "worker/app/worker/models/mpt_30b.py", "chunked_list": ["from typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tclass Mpt30B(VllmCausalModel):\n\t    architecture_name = \"mpt_30b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n", "        super().prepare_for_inference()\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text\n"]}
{"filename": "worker/app/worker/models/llama_7b.py", "chunked_list": ["import os\n\tfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\tfrom typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tfrom vllm.engine.arg_utils import AsyncEngineArgs\n\tfrom vllm.engine.async_llm_engine import AsyncLLMEngine\n\tclass Llama7B(VllmCausalModel):\n\t    architecture_name = \"llama_7b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n", "    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n\t        if self.model_config[\"quantization\"] == \"int8\":\n\t            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\t        elif self.model_config[\"quantization\"] == \"float16\":\n\t            \"\"\"\n\t                The GPU types are\n\t                0: A100\n", "                1: A100 80GB\n\t                2: T4\n\t                TODO: We're not covering A100 80GB yet\n\t            \"\"\"\n\t            if self.model_config[\"gpuType\"] == 2:\n\t                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\")\n\t            elif self.model_config[\"gpuType\"] == 0:\n\t                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"])\n\t            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\t        else:\n", "            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text"]}
{"filename": "worker/app/worker/models/vllm_causal.py", "chunked_list": ["import transformers\n\tfrom transformers import TextIteratorStreamer, StoppingCriteriaList, Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n\tfrom threading import Thread\n\tfrom typing import List\n\tfrom peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n\timport re\n\timport os\n\tfrom app.pb import worker_pb2, worker_pb2_grpc\n\tfrom vllm.engine.arg_utils import AsyncEngineArgs\n\tfrom vllm.engine.async_llm_engine import AsyncLLMEngine\n", "from vllm.sampling_params import SamplingParams\n\tfrom vllm.utils import random_uuid\n\tfrom .model_registry import RegisteredModel\n\tfrom .inference_utils.stopping_criteria import StopOnTokens\n\tfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\n\tfrom .training_utils.data_processing import make_supervised_data_module\n\tclass VllmCausalModel(RegisteredModel):\n\t    architecture_name = \"causal_vllm_model\"\n\t    def __init__(self, config):\n\t        self.model_config = config\n", "        if config[\"name\"].startswith(\"@huggingface\"):\n\t            self.model_config[\"huggingface_name\"] = \"/\".join(config[\"name\"].split(\"/\")[1:])\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n\t        if self.model_config[\"quantization\"] == \"int8\":\n\t            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\t        elif self.model_config[\"quantization\"] == \"float16\":\n\t            \"\"\"\n", "                The GPU types are\n\t                0: A100\n\t                1: A100 80GB\n\t                2: T4\n\t                TODO: We're not covering A100 80GB yet\n\t            \"\"\"\n\t            if self.model_config[\"gpuType\"] == 2:\n\t                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\", swap_space=1)\n\t            elif self.model_config[\"gpuType\"] == 0:\n\t                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tensor_parallel_size=self.model_config[\"gpuCount\"])\n", "            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\t        else:\n\t            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        sampling_params = SamplingParams(\n\t                max_tokens=max_tokens if max_tokens < self.model_config[\"contextSize\"] else self.model_config[\"contextSize\"],\n\t                top_p=1 if temperature==0 else top_p, \n\t                temperature=temperature, \n\t                stop=self.get_stopword_list() + stop_tokens\n\t            )\n", "        id = random_uuid()\n\t        results_generator = self.model_vllm_engine.generate(prompt, sampling_params, id)\n\t        prev_text = \"\"\n\t        async for request_output in results_generator:\n\t            text = request_output.outputs[0].text\n\t            text = text.replace(prev_text, \"\")\n\t            if not text in prev_text:\n\t                yield text\n\t                prev_text = request_output.outputs[0].text\n\t            else:\n", "                yield \"\"\n\t    def create_prompt_from_messages(self, messages):\n\t        if messages[-1].role == worker_pb2.ASSISTANT:\n\t            raise Exception(\"Last message should be from user, not assistant\")\n\t        prompt = self.model_config[\"systemPrompt\"]\n\t        for message_obj in messages:\n\t            if message_obj.role == worker_pb2.USER:\n\t                prompt += self.model_config[\"instructionPrefix\"] + message_obj.content + self.model_config[\"instructionPostfix\"]\n\t            elif message_obj.role == worker_pb2.ASSISTANT:\n\t                prompt += self.model_config[\"outputPrefix\"] + message_obj.content + self.model_config[\"outputPostfix\"]\n", "        prompt += self.model_config[\"outputPrefix\"]\n\t        return prompt\n\t    def get_stopword_list(self):\n\t        if not \"outputPostfix\" in self.model_config:\n\t            return [\"<|endoftext|>\"]\n\t        if all(char.isspace() for char in self.model_config[\"outputPostfix\"]):\n\t            return [self.model_config[\"instructionPrefix\"].strip(), \"<|endoftext|>\"]\n\t        else:\n\t            return [self.model_config[\"outputPostfix\"].strip(), \"<|endoftext|>\"]\n\t    ##############################\n", "    ### FINETUNING   #############\n\t    ##############################\n\t    def prepare_model_for_training(self):\n\t        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=self.model_config[\"lora\"])\n\t        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\t        if self.tokenizer.pad_token is None:\n\t            resize_tokenizer_and_embeddings(\n\t                tokenizer=self.tokenizer,\n\t                model=self.model,\n\t            )\n", "        if self.model_config[\"lora\"]:\n\t            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=self.model_config[\"lora_params\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n\t            self.model = prepare_model_for_int8_training(self.model)\n\t            self.model = get_peft_model(self.model, lora_config)\n\t    def prepare_data_for_training(self):\n\t        self.train_dataset, self.collator = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"train_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n\t        self.eval_dataset, _ = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"eval_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n\t    def train(self):\n\t        training_args = TrainingArguments(learning_rate=self.model_config[\"learning_rate\"], per_device_train_batch_size=self.model_config[\"batch_size\"], per_device_eval_batch_size=self.model_config[\"batch_size\"], output_dir=\"out\")\n\t        trainer = Trainer(\n", "            model=self.model, \n\t            tokenizer=self.tokenizer,\n\t            train_dataset=self.train_dataset, \n\t            eval_dataset=self.eval_dataset,\n\t            data_collator=self.collator,\n\t            args=training_args\n\t        )\n\t        trainer.train()\n\t        self.model.save_pretrained(self.model_config[\"trained_model_path\"])\n\t        self.tokenizer.save_pretrained(self.model_config[\"trained_model_path\"])"]}
{"filename": "worker/app/worker/models/__init__.py", "chunked_list": []}
{"filename": "worker/app/worker/models/base_causal.py", "chunked_list": ["import transformers\n\tfrom transformers import TextIteratorStreamer, StoppingCriteriaList, Trainer, TrainingArguments\n\tfrom threading import Thread\n\tfrom typing import List\n\tfrom peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n\tfrom app.pb import worker_pb2, worker_pb2_grpc\n\tfrom .model_registry import RegisteredModel\n\tfrom .inference_utils.stopping_criteria import StopOnTokens\n\tfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\n\tfrom .training_utils.data_processing import make_supervised_data_module\n", "class AutoCausalModel(RegisteredModel):\n\t    architecture_name = \"causal_model\"\n\t    def __init__(self, config):\n\t        self.model_config = config\n\t        if config[\"name\"].startswith(\"@huggingface\"):\n\t            self.model_config[\"huggingface_name\"] = \"/\".join(config[\"name\"].split(\"/\")[1:])\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n", "        if self.model_config[\"quantization\"] == \"int8\":\n\t            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=True)\n\t        elif self.model_config[\"quantization\"] == \"float16\":\n\t            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\")\n\t        else:\n\t            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\t        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\t        self.stopping_criteria = StoppingCriteriaList([StopOnTokens(self.tokenizer, [self.model_config[\"instructionPrefix\"]]+[self.tokenizer.eos_token])])\n\t    def generate_stream(self, prompt: str, stop_tokens=[], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        input_tokenized = self.tokenizer([prompt], return_tensors='pt').input_ids.to('cuda')\n", "        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n\t        if temperature == 0:\n\t            sample = False\n\t            temperature = 1\n\t        else:\n\t            sample = True\n\t        generation_kwargs=dict(\n\t            inputs=input_tokenized,\n\t            streamer=streamer,\n\t            do_sample=sample,\n", "            top_p=top_p, \n\t            top_k=top_k, \n\t            temperature=temperature, \n\t            stopping_criteria=self.stopping_criteria,\n\t            max_length=max_tokens if max_tokens < self.model_config[\"contextSize\"] else self.model_config[\"contextSize\"],\n\t        )\n\t        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n\t        thread.start()\n\t        return streamer\n\t    def create_prompt_from_messages(self, messages):\n", "        if messages[-1].role == worker_pb2.ASSISTANT:\n\t            raise Exception(\"Last message should be from user, not assistant\")\n\t        prompt = self.model_config[\"systemPrompt\"]\n\t        for message_obj in messages:\n\t            if message_obj.role == worker_pb2.USER:\n\t                prompt += self.model_config[\"instructionPrefix\"] + message_obj.content + self.model_config[\"instructionPostfix\"]\n\t            elif message_obj.role == worker_pb2.ASSISTANT:\n\t                prompt += self.model_config[\"outputPrefix\"] + message_obj.content + self.model_config[\"outputPostfix\"]\n\t        prompt += self.model_config[\"outputPrefix\"]\n\t        return prompt\n", "    ##############################\n\t    ### FINETUNING   #############\n\t    ##############################\n\t    def prepare_model_for_training(self):\n\t        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=self.model_config[\"lora\"])\n\t        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\t        if self.tokenizer.pad_token is None:\n\t            resize_tokenizer_and_embeddings(\n\t                tokenizer=self.tokenizer,\n\t                model=self.model,\n", "            )\n\t        if self.model_config[\"lora\"]:\n\t            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=self.model_config[\"lora_params\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n\t            self.model = prepare_model_for_int8_training(self.model)\n\t            self.model = get_peft_model(self.model, lora_config)\n\t    def prepare_data_for_training(self):\n\t        self.train_dataset, self.collator = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"train_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n\t        self.eval_dataset, _ = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"eval_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n\t    def train(self):\n\t        training_args = TrainingArguments(learning_rate=self.model_config[\"learning_rate\"], per_device_train_batch_size=self.model_config[\"batch_size\"], per_device_eval_batch_size=self.model_config[\"batch_size\"], output_dir=\"out\")\n", "        trainer = Trainer(\n\t            model=self.model, \n\t            tokenizer=self.tokenizer,\n\t            train_dataset=self.train_dataset, \n\t            eval_dataset=self.eval_dataset,\n\t            data_collator=self.collator,\n\t            args=training_args\n\t        )\n\t        trainer.train()\n\t        self.model.save_pretrained(self.model_config[\"trained_model_path\"])\n", "        self.tokenizer.save_pretrained(self.model_config[\"trained_model_path\"])\n"]}
{"filename": "worker/app/worker/models/falcon_7b.py", "chunked_list": ["import transformers\n\tfrom transformers import TextIteratorStreamer, StoppingCriteriaList\n\tfrom threading import Thread\n\timport torch\n\tfrom peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n\tfrom typing import List\n\tfrom .base_causal import AutoCausalModel\n\tfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\n\tfrom .training_utils.data_processing import make_supervised_data_module\n\tfrom .inference_utils.stopping_criteria import StopOnTokens\n", "class Falcon7BModel(AutoCausalModel):\n\t    architecture_name = \"falcon_7b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n\t        if self.model_config[\"quantization\"] == \"int8\":\n\t            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=True, trust_remote_code=True, torch_dtype=torch.bfloat16)\n", "        elif self.model_config[\"quantization\"] == \"float16\":\n\t            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n\t        else:\n\t            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\t        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\t        self.stopping_criteria = StoppingCriteriaList([StopOnTokens(self.tokenizer, [self.model_config[\"instructionPrefix\"]]+[self.tokenizer.eos_token])])\n\t    def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        return super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t    ##############################\n\t    ### INFERENCE    #############\n", "    ##############################\n\t    def prepare_model_for_training(self):\n\t        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=self.model_config[\"lora\"], trust_remote_code=True, torch_dtype=torch.bfloat16)\n\t        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\t        if self.tokenizer.pad_token is None:\n\t            resize_tokenizer_and_embeddings(\n\t                tokenizer=self.tokenizer,\n\t                model=self.model,\n\t            )\n\t        if self.model_config[\"lora\"]:\n", "            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=self.model_config[\"lora_params\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n\t            self.model = prepare_model_for_int8_training(self.model)\n\t            self.model = get_peft_model(self.model, lora_config)\n\t    def prepare_data_for_training(self):\n\t        super().prepare_data_for_training()\n\t    def train(self):\n\t        super().train()\n"]}
{"filename": "worker/app/worker/models/gpt_neox_7b.py", "chunked_list": ["import os\n\tfrom typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\tfrom vllm.engine.arg_utils import AsyncEngineArgs\n\tfrom vllm.engine.async_llm_engine import AsyncLLMEngine\n\tclass GPTNeoX7B(VllmCausalModel):\n\t    architecture_name = \"gpt_neox_7b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n", "    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n\t        super().prepare_for_inference()\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text\n"]}
{"filename": "worker/app/worker/models/llama_13b.py", "chunked_list": ["import os\n\tfrom typing import List\n\tfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\tfrom .vllm_causal import VllmCausalModel\n\tfrom vllm.engine.arg_utils import AsyncEngineArgs\n\tfrom vllm.engine.async_llm_engine import AsyncLLMEngine\n\tfrom vllm.sampling_params import SamplingParams\n\tfrom vllm.utils import random_uuid\n\tclass Llama13B(VllmCausalModel):\n\t    architecture_name = \"llama_13b\"\n", "    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n\t        if self.model_config[\"quantization\"] == \"int8\":\n\t            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\t        elif self.model_config[\"quantization\"] == \"float16\":\n\t            \"\"\"\n", "                The GPU types are\n\t                0: A100\n\t                1: A100 80GB\n\t                2: T4\n\t                TODO: We're not covering A100 80GB yet\n\t            \"\"\"\n\t            if self.model_config[\"gpuType\"] == 2:\n\t                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\")\n\t            elif self.model_config[\"gpuType\"] == 0:\n\t                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"])\n", "            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\t        else:\n\t            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text"]}
{"filename": "worker/app/worker/models/mpt_7b.py", "chunked_list": ["from typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tclass Mpt7B(VllmCausalModel):\n\t    architecture_name = \"mpt_7b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n", "        super().prepare_for_inference()\n\t    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 28000, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text\n"]}
{"filename": "worker/app/worker/models/gpt_neox_3b.py", "chunked_list": ["from typing import List\n\tfrom .vllm_causal import VllmCausalModel\n\tclass GPTNeoX3B(VllmCausalModel):\n\t    architecture_name = \"gpt_neox_3b\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t    ##############################\n\t    ### INFERENCE    #############\n\t    ##############################\n\t    def prepare_for_inference(self):\n", "        super().prepare_for_inference()\n\t    async def generate_stream(self, prompt: str, stop_tokens= [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n\t        stream = super().generate_stream(prompt, max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t        async for text in stream:\n\t            yield text\n"]}
{"filename": "worker/app/worker/models/training_utils/tokenizer_resize.py", "chunked_list": ["from typing import Dict\n\timport transformers\n\tdef resize_tokenizer_and_embeddings(\n\t    tokenizer: transformers.PreTrainedTokenizer,\n\t    model: transformers.PreTrainedModel,\n\t):\n\t    \"\"\"Resize tokenizer and embedding.\n\t    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n\t    \"\"\"\n\t    special_tokens_dict = dict(pad_token=\"[PAD]\")\n", "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\t    model.resize_token_embeddings(len(tokenizer))\n\t    if num_new_tokens > 0:\n\t        input_embeddings = model.get_input_embeddings().weight.data\n\t        output_embeddings = model.get_output_embeddings().weight.data\n\t        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\t        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\t        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n\t        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n"]}
{"filename": "worker/app/worker/models/training_utils/data_processing.py", "chunked_list": ["import copy\n\timport logging\n\tfrom dataclasses import dataclass, field\n\tfrom typing import Optional, Dict, Sequence\n\timport torch\n\timport transformers\n\tfrom torch.utils.data import Dataset\n\timport io\n\timport json\n\tdef _make_r_io_base(f, mode: str):\n", "    if not isinstance(f, io.IOBase):\n\t        f = open(f, mode=mode)\n\t    return f\n\tdef jload(f, mode=\"r\"):\n\t    \"\"\"Load a .json file into a dictionary.\"\"\"\n\t    f = _make_r_io_base(f, mode)\n\t    jdict = json.load(f)\n\t    f.close()\n\t    return jdict\n\t# SUPERVISED FINETUNING\n", "class SupervisedDataset(Dataset):\n\t    \"\"\"\n\t    Dataset for supervised fine-tuning.\n\t    \"\"\"\n\t    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, instruction_prefix, output_prefix):\n\t        super(SupervisedDataset, self).__init__()\n\t        logging.warning(\"Loading data...\")\n\t        list_data_dict = jload(data_path)\n\t        logging.warning(\"Formatting inputs...\")\n\t        sources = [example[\"prompt\"] for example in list_data_dict]\n", "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n\t        logging.warning(\"Tokenizing inputs... This may take some time...\")\n\t        data_dict = preprocess(sources, targets, tokenizer, instruction_prefix, output_prefix)\n\t        self.input_ids = data_dict[\"input_ids\"]\n\t        self.labels = data_dict[\"labels\"]\n\t    def __len__(self):\n\t        return len(self.input_ids)\n\t    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n\t        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n\t@dataclass\n", "class DataCollatorForSupervisedDataset(object):\n\t    \"\"\"\n\t    Collate examples for supervised fine-tuning.\n\t    \"\"\"\n\t    tokenizer: transformers.PreTrainedTokenizer\n\t    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n\t        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n\t        input_ids = torch.nn.utils.rnn.pad_sequence(\n\t            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n\t        )\n", "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\t        return dict(\n\t            input_ids=input_ids,\n\t            labels=labels,\n\t            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n\t        )\n\tdef preprocess(sources: Sequence[str], targets: Sequence[str], tokenizer: transformers.PreTrainedTokenizer, instruction_prefix, output_prefix) -> Dict:\n\t    \"\"\"\n\t    Preprocess the data by tokenizing.\n\t    \"\"\"\n", "    examples = [instruction_prefix + s + output_prefix + t + tokenizer.eos_token for s, t in zip(sources, targets)]\n\t    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n\t    input_ids = examples_tokenized[\"input_ids\"]\n\t    labels = copy.deepcopy(input_ids)\n\t    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n\t        label[:source_len] = label[:source_len]\n\t    return dict(input_ids=input_ids, labels=labels)\n\tdef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_path, instruction_prefix, output_prefix) -> Dict:\n\t    \"\"\"\n\t    Make dataset and collator for supervised fine-tuning.\n", "    \"\"\"\n\t    dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path, instruction_prefix=instruction_prefix, output_prefix=output_prefix)\n\t    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n\t    return dataset, data_collator\n\tdef _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n\t    \"\"\"\n\t    Tokenize a list of strings.\n\t    \"\"\"\n\t    tokenized_list = [\n\t        tokenizer(\n", "            text,\n\t            return_tensors=\"pt\",\n\t            padding=\"longest\",\n\t            max_length=tokenizer.model_max_length,\n\t            truncation=True,\n\t        )\n\t        for text in strings\n\t    ]\n\t    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n\t    input_ids_lens = labels_lens = [\n", "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n\t    ]\n\t    return dict(\n\t        input_ids=input_ids,\n\t        labels=labels,\n\t        input_ids_lens=input_ids_lens,\n\t        labels_lens=labels_lens,\n\t    )"]}
{"filename": "worker/app/worker/models/inference_utils/parameter_passing.py", "chunked_list": ["from typing import Dict\n\tdef get_inference_parameter_dict(params: Dict):\n\t    inference_params = dict()\n\t    for p in params.keys():\n\t        if params[p] != -1:\n\t            inference_params[p] = params[p]\n\t    return inference_params\n"]}
{"filename": "worker/app/worker/models/inference_utils/stopping_criteria.py", "chunked_list": ["from transformers import StoppingCriteria\n\timport torch\n\tclass StopOnTokens(StoppingCriteria):\n\t    def __init__(self, tokenizer, stop_token_list):\n\t        super(StopOnTokens, self).__init__()\n\t        self.stopping_ids = tokenizer.convert_tokens_to_ids(stop_token_list)\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n\t        for stop_id in self.stopping_ids:\n\t            if input_ids[0][-1] == stop_id:\n\t                return True\n", "        return False\n"]}
{"filename": "worker/app/worker/models/inference_utils/__init__.py", "chunked_list": []}
{"filename": "llamatune/setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t    name='llamatune',\n\t    version='0.1.1',\n\t    author='Haven Technologies Inc.',\n\t    author_email='hello@havenllm.com',\n\t    description=\"Haven\\'s Tuning Library for LLM finetuning\",\n\t    packages=find_packages(),\n\t    install_requires=[\n\t        'torch==2.0.1',\n", "        'bitsandbytes==0.40.0',\n\t        'einops==0.6.1',\n\t        'evaluate==0.4.0',\n\t        'scikit-learn==1.2.2',\n\t        'sentencepiece==0.1.99',\n\t        'wandb==0.15.3',\n\t        'accelerate==0.21.0',\n\t        'posthog==2.5.0'\n\t    ],\n\t)\n"]}
{"filename": "llamatune/llamatune/train.py", "chunked_list": ["from llamatune import ChatTrainer\n\tfrom transformers import HfArgumentParser\n\tfrom dataclasses import dataclass, field\n\tfrom typing import Optional\n\timport torch\n\t@dataclass\n\tclass TrainingConfig:\n\t    model_name: str = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": 'Huggingface Name of the model you want to train'})\n\t    data_path: str = field(default=\"data.json\", metadata={\"help\": 'Path towards your training data'})\n\t    output_dir: str = field(default='./trained_model', metadata={\"help\": 'The output dir for logs and checkpoints'})\n", "    training_recipe: str = field(default=\"lora\", metadata={\"help\": \"Lora Training or Full Training\"})\n\t    optim: str = field(default='paged_adamw_8bit', metadata={\"help\": 'The optimizer to be used'})\n\t    batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU. Increase for better speed.'})\n\t    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'How many gradients to accumulate before to perform an optimizer step'})\n\t    n_epochs: int = field(default=3, metadata={\"help\": 'How many optimizer update steps to take'})\n\t    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) \n\t    learning_rate: float = field(default=1e-4, metadata={\"help\": 'The learning rate'})\n\t    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n\t    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n\t    do_train: bool = field(default=True, metadata={\"help\": 'To train or not to train, that is the question?'})\n", "    lr_scheduler_type: str = field(default='cosine', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n\t    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n\t    logging_steps: int = field(default=1, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n\t    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n\t    save_strategy: str = field(default='epoch', metadata={\"help\": 'When to save checkpoints'})\n\t    save_total_limit: int = field(default=3, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n\t    fp16: bool = field(default=True, metadata={\"help\": 'Whether to use fp16 mixed precision training'})\n\t    tokenizer_type: str = field(default=\"llama\", metadata={\"help\": \"Tokenizer type. Should be \\\"llama\\\" for llama models to address tokenizer issue\"})\n\t    trust_remote_code: str = field(default=False, metadata={\"help\": \"Whether to trust remote code.\"})\n\t    compute_dtype: torch.dtype = field(default=torch.float16, metadata={\"help\":\"Compute Datatype for models, either float16 or float32.\"})\n", "    max_tokens: int = field(default=4096, metadata={\"help\":\"Max tokens\"})\n\t    do_eval: bool = field(default=True, metadata={\"help\": \"Whether to evaluate or not\"})\n\t    evaluation_strategy: str = field(default=\"epoch\", metadata={\"help\": \"When to evaluate, after certain number of steps or each epoch\"})\n\t    use_auth_token: str = field(default=None, metadata={\"help\": \"auth token\"})\n\t    use_fast: bool = field(default=False, metadata={\"help\": \"Whether to use fast tokenizer\"})\n\t    bits: Optional[int] = field(default=4, metadata={\"help\": \"Number of bits to quantize the model to\"})\n\t    double_quant: bool = field(default=True, metadata={\"help\": \"Compress the quantization statistics through double quantization.\"})\n\t    quant_type: str = field(default=\"nf4\", metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"})\n\t    lora_r: int = field(default=64, metadata={\"help\": \"Lora R dimension.\"})\n\t    lora_alpha: float = field(default=16, metadata={\"help\": \" Lora alpha.\"})\n", "    lora_dropout: float = field(default=0.0, metadata={\"help\":\"Lora dropout.\"})\n\thfparser = HfArgumentParser((TrainingConfig))\n\targs = hfparser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n\ttrainer = ChatTrainer(training_config=args)\n\ttrainer.train()"]}
{"filename": "llamatune/llamatune/__init__.py", "chunked_list": ["from .trainer import ChatTrainer"]}
{"filename": "llamatune/llamatune/utils.py", "chunked_list": ["import torch\n\tdef _get_compute_dtype():\n\t    major, minor = torch.cuda.get_device_capability()\n\t    if major >= 8:\n\t        return torch.bfloat16\n\t    else:\n\t        return torch.float16\n\tdef print_trainable_parameters(model):\n\t    \"\"\"\n\t    Prints the number of trainable parameters in the model.\n", "    \"\"\"\n\t    trainable_params = 0\n\t    all_param = 0\n\t    for _, param in model.named_parameters():\n\t        all_param += param.numel()\n\t        if param.requires_grad:\n\t            trainable_params += param.numel()\n\t    print(\n\t        f\"trainable params: {trainable_params} || \"\n\t        f\"all params: {all_param} || \"\n", "        f\"trainable: {100 * trainable_params / all_param}\"\n\t    )\n"]}
{"filename": "llamatune/llamatune/trainer.py", "chunked_list": ["import transformers\n\tfrom typing import Dict\n\tfrom dataclasses import dataclass, field\n\timport torch\n\tfrom llamatune.model_engines.llama_model_engine import LlamaEngine\n\tfrom llamatune.data.chat_data_module import ChatDataModule\n\tclass ChatTrainer:\n\t    def __init__(self, training_config: dataclass):\n\t        self.training_config = training_config\n\t        self.model_engine : LlamaEngine = LlamaEngine(training_config.model_name, training_config=training_config)\n", "        self.data_path = training_config.data_path\n\t    def train(self):\n\t        self.model_engine.prepare_model_for_training()\n\t        self.data_module = ChatDataModule(\n\t            tokenizer=self.model_engine.tokenizer,\n\t            data_path_train=self.data_path,\n\t            max_tokens=self.model_engine.config.max_tokens\n\t        )\n\t        self.model_engine.train(data_module=self.data_module)\n"]}
{"filename": "llamatune/llamatune/data/__init__.py", "chunked_list": []}
{"filename": "llamatune/llamatune/data/chat_data_module.py", "chunked_list": ["import torch\n\timport transformers\n\timport io\n\timport json\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Dict, Sequence\n\tfrom torch.utils.data import Dataset\n\tfrom llamatune.data.data_module import DataModule\n\tSYS_PREFIX = \"<<SYS>>\\n\"\n", "SYS_POSTFIX = \"\\n<</SYS>>\\n\\n\"\n\tINST_PREFIX = \"<s>[INST] \"\n\tINST_POSTFIX = \" \"\n\tOUTPUT_PREFIX = \"[/INST] \"\n\tOUTPUT_POSTFIX = \"</s>\"\n\tdef _make_r_io_base(f, mode: str):\n\t    if not isinstance(f, io.IOBase):\n\t        f = open(f, mode=mode)\n\t    return f\n\tdef jload(f, mode=\"r\"):\n", "    \"\"\"Load a .json file into a dictionary.\"\"\"\n\t    f = _make_r_io_base(f, mode)\n\t    jdict = json.load(f)\n\t    f.close()\n\t    return jdict\n\tclass ChatDataset(Dataset):\n\t    def __init__(self, data_path: str, tokenizer: transformers.AutoTokenizer, max_tokens=None):\n\t        super(ChatDataset, self).__init__()\n\t        logging.warning(\"Loading data...\")\n\t        conversations = jload(data_path)\n", "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n\t        data_dict = preprocess(conversations, tokenizer, max_tokens)\n\t        self.input_ids = data_dict[\"input_ids\"]\n\t        self.labels = data_dict[\"labels\"]\n\t    def __len__(self):\n\t        return len(self.input_ids)\n\t    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n\t        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n\t@dataclass\n\tclass DataCollatorForChatDataset(object):\n", "    \"\"\"\n\t    Collate examples for supervised fine-tuning.\n\t    \"\"\"\n\t    tokenizer: transformers.PreTrainedTokenizer\n\t    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n\t        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n\t        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n\t        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\t        return dict(\n\t            input_ids=input_ids,\n", "            labels=labels,\n\t            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n\t        )\n\tclass ChatDataModule(DataModule):\n\t    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, data_path_train: str, max_tokens = None):\n\t        self.train_dataset = ChatDataset(tokenizer=tokenizer, data_path=data_path_train, max_tokens=max_tokens)\n\t        self.data_collator = DataCollatorForChatDataset(tokenizer=tokenizer)\n\tdef preprocess(conversations: Sequence[Sequence[dict]], tokenizer: transformers.PreTrainedTokenizer, max_tokens=None) -> Dict:\n\t    \"\"\"\n\t    Preprocess the data by tokenizing.\n", "    \"\"\"\n\t    all_input_ids = []\n\t    all_labels = []\n\t    for conv in conversations:\n\t        roles = [msg[\"role\"] for msg in conv]\n\t        messages = [msg[\"content\"] for msg in conv]\n\t        assert roles[0] != \"ASSISTANT\"\n\t        assert roles[-1] == \"ASSISTANT\"\n\t        input_messages = []\n\t        if roles[0] == \"SYSTEM\":\n", "            input_messages.append(SYS_PREFIX+messages[0]+SYS_POSTFIX)\n\t        for role, msg in zip(roles, messages):\n\t            if role == \"ASSISTANT\":\n\t                input_messages.append(msg + OUTPUT_POSTFIX)\n\t            elif role == \"USER\":\n\t                input_messages.append(INST_PREFIX + msg + INST_POSTFIX + OUTPUT_PREFIX)\n\t        tokenized_input = tokenizer(input_messages, add_special_tokens=False)\n\t        input_ids = []\n\t        labels = []\n\t        if roles[0] == \"SYSTEM\":\n", "            input_ids.extend(tokenized_input.input_ids[0])\n\t            labels.extend([-100]*len(tokenized_input.input_ids[0]))\n\t        for role, msg in zip(roles, tokenized_input.input_ids):\n\t            if role == \"USER\":\n\t                labels.extend([-100]*len(msg))\n\t                input_ids.extend(msg)\n\t            elif role == \"ASSISTANT\":\n\t                labels.extend(msg)\n\t                input_ids.extend(msg)\n\t        if max_tokens is None:\n", "            max_tokens = tokenizer.model_max_length\n\t        input_ids = torch.LongTensor(input_ids)[:max_tokens]\n\t        labels = torch.LongTensor(labels)[:max_tokens]\n\t        assert input_ids.shape == labels.shape   \n\t        all_input_ids.append(input_ids)  \n\t        all_labels.append(labels)\n\t    return dict(input_ids=all_input_ids, labels=all_labels)\n"]}
{"filename": "llamatune/llamatune/data/data_module.py", "chunked_list": ["import torch\n\tclass DataModule:\n\t    def __init__():\n\t        pass\n"]}
{"filename": "llamatune/llamatune/model_engines/llama_model_engine.py", "chunked_list": ["import os\n\timport torch\n\timport transformers\n\timport bitsandbytes as bnb\n\tfrom dataclasses import dataclass, field\n\tfrom typing import Optional, Dict\n\tfrom transformers import BitsAndBytesConfig\n\tfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\tfrom transformers import Trainer\n\tfrom llamatune.utils import _get_compute_dtype, print_trainable_parameters\n", "from llamatune.telemetry.send_event import capture_event\n\tclass LlamaEngine:\n\t    def __init__(self, model_name,  training_config):\n\t        if not training_config.training_recipe in [\"lora\", \"full_training\"]:\n\t            raise Exception(f\"{training_config.training_recipe} is not a valid training recipe. Please choose either \\\"lora\\\" or \\\"full_training\\\"\")\n\t        self.config = training_config\n\t        self.training_recipe = training_config.training_recipe\n\t        self.model_name = model_name\n\t    def train(self, data_module):\n\t        print(\"config\", self.config)\n", "        training_args = self.construct_training_arguments()\n\t        trainer = Trainer(\n\t            model=self.model,\n\t            tokenizer=self.tokenizer,\n\t            train_dataset=data_module.train_dataset,\n\t            data_collator=data_module.data_collator,\n\t            args=training_args\n\t        )\n\t        trainer.train()\n\t        capture_event(\"end-training\", {})\n", "    def prepare_model_for_training(self):\n\t        capture_event(\"start-training\", {})\n\t        if self.config.training_recipe == \"lora\":\n\t            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n\t                self.model_name,\n\t                load_in_4bit=self.config.bits == 4,\n\t                load_in_8bit=self.config.bits == 8,\n\t                device_map=self._get_device_map(),\n\t                torch_dtype=self.config.compute_dtype,\n\t                trust_remote_code=self.config.trust_remote_code,\n", "                use_auth_token=self.config.use_auth_token,\n\t                quantization_config=BitsAndBytesConfig(\n\t                    load_in_4bit=self.config.bits == 4,\n\t                    load_in_8bit=self.config.bits == 8,\n\t                    llm_int8_threshold=6.0,\n\t                    llm_int8_has_fp16_weight=False,\n\t                    bnb_4bit_compute_dtype=self.config.compute_dtype,\n\t                    bnb_4bit_use_double_quant=self.config.double_quant,\n\t                    bnb_4bit_quant_type=self.config.quant_type,\n\t                    trust_remote_code=self.config.trust_remote_code\n", "                )\n\t            )\n\t        elif self.config.training_recipe == \"full_training\":\n\t            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n\t                self.model_name,\n\t                device_map=self._get_device_map(),\n\t                torch_dtype=self.config.compute_dtype,\n\t                trust_remote_code=self.config.trust_remote_code,\n\t                use_auth_token=self.config.use_auth_token\n\t            )\n", "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, padding_side=\"right\", use_fast=self.config.use_fast, tokenizer_type=self.config.tokenizer_type, trust_remote_code=self.config.trust_remote_code, use_auth_token=self.config.use_auth_token)\n\t        self._smart_tokenizer_and_embedding_resize()\n\t        if self.training_recipe == \"lora\":\n\t            self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=self.config.gradient_checkpointing)\n\t        if self.config.gradient_checkpointing:\n\t            self.model.gradient_checkpointing_enable()\n\t        if self.training_recipe == \"lora\":\n\t            modules = self._find_all_linear_names()\n\t            lora_config = LoraConfig(\n\t                r=self.config.lora_r,\n", "                lora_alpha=self.config.lora_alpha,\n\t                target_modules=modules,\n\t                lora_dropout=self.config.lora_dropout,\n\t                bias=\"none\",\n\t                task_type=\"CAUSAL_LM\",\n\t            )\n\t            self.model = get_peft_model(self.model, lora_config)\n\t        print(\"Model ready for training!\")\n\t        print_trainable_parameters(self.model)\n\t    def _get_device_map(self):\n", "        device_map = \"auto\"\n\t        if os.environ.get(\"LOCAL_RANK\") is not None:\n\t            device_map = {'': int(os.environ.get('LOCAL_RANK', '0'))}\n\t        return device_map\n\t    def construct_training_arguments(self):\n\t        args=transformers.TrainingArguments(\n\t                output_dir = self.config.output_dir,\n\t                optim = self.config.optim,\n\t                per_device_train_batch_size = self.config.batch_size,\n\t                gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n", "                num_train_epochs = self.config.n_epochs,\n\t                weight_decay = self.config.weight_decay,\n\t                learning_rate = self.config.learning_rate,\n\t                max_grad_norm = self.config.max_grad_norm,\n\t                gradient_checkpointing = self.config.gradient_checkpointing,\n\t                do_train = self.config.do_train,\n\t                lr_scheduler_type = self.config.lr_scheduler_type,\n\t                warmup_ratio = self.config.warmup_ratio,\n\t                logging_steps = self.config.logging_steps,\n\t                group_by_length = self.config.group_by_length,\n", "                save_strategy = self.config.save_strategy,\n\t            )\n\t        return args\n\t    def _find_all_linear_names(self):\n\t        cls = bnb.nn.Linear4bit if self.config.bits == 4 else (bnb.nn.Linear8bitLt if self.config.bits == 8 else torch.nn.Linear)\n\t        lora_module_names = set()\n\t        for name, module in self.model.named_modules():\n\t            if isinstance(module, cls):\n\t                names = name.split('.')\n\t                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n", "        if \"lm_head\" in lora_module_names: # needed for 16-bit\n\t            lora_module_names.remove(\"lm_head\")\n\t        return list(lora_module_names)\n\t    def _smart_tokenizer_and_embedding_resize(self):\n\t        if self.tokenizer.pad_token is None:\n\t            num_new_tokens = self.tokenizer.add_special_tokens(dict(pad_token=\"[PAD]\"))\n\t            self.model.resize_token_embeddings(len(self.tokenizer))\n\t            if num_new_tokens > 0:\n\t                input_embeddings = self.model.get_input_embeddings().weight.data\n\t                output_embeddings = self.model.get_output_embeddings().weight.data\n", "                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\t                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\t                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n\t                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n"]}
{"filename": "llamatune/llamatune/model_engines/__init__.py", "chunked_list": []}
{"filename": "llamatune/llamatune/telemetry/send_event.py", "chunked_list": ["from posthog import Posthog\n\timport os\n\tposthog = Posthog('phc_YpKoFD7smPe4SXRtVyMW766uP9AjUwnuRJ8hh2EJcVv',\n\t                  host='https://eu.posthog.com')\n\tdef capture_event(event_name, event_properties):\n\t    if not os.environ.get('EVENT_CAPTURE') == \"disable\":\n\t        posthog.capture(\"not_distinct\", event_name, event_properties)"]}
{"filename": "llamatune/llamatune/telemetry/__init__.py", "chunked_list": []}
