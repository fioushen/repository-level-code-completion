{"filename": "convo_agents/convo_agents.py", "chunked_list": ["\"\"\"Run Llama conversation agents.\n\tThe goal of this is to simulate conversation between two agents.\n\t\"\"\"\n\tfrom llama_index import (\n\t    GPTVectorStoreIndex, GPTListIndex, Document, ServiceContext\n\t)\n\tfrom llama_index.indices.base import BaseGPTIndex\n\tfrom llama_index.data_structs import Node\n\tfrom llama_index.prompts.prompts import QuestionAnswerPrompt\n\tfrom collections import deque\n", "from pydantic import BaseModel, Field\n\tfrom typing import Optional, Dict\n\tdef format_text(text: str, user: str) -> str:\n\t    return user + \": \" + text\n\tDEFAULT_USER_PREFIX_TMPL = (\n\t    \"Your name is {name}. \"\n\t    \"We provide conversation context between you and other users below. \"\\\n\t    \"You are on a date with someone else. \\n\"\n\t    # \"The user is the plaintiff and the other user is the defendant.\"\n\t)\n", "DEFAULT_PROMPT_TMPL = (\n\t    \"---------------------\\n\"\n\t    \"{context_str}\"\n\t    \"\\n---------------------\\n\"\n\t    \"Given the context information, perform the following task.\\n\"\n\t    \"Task: {query_str}\\n\"\n\t    \"You: \"\n\t    # \"Here's an example:\\n\"\n\t    # \"Previous line: Hi Bob, good to meet you!\\n\"\n\t    # \"You: Good to meet you too!\\n\\n\"\n", "    # \"Previous line: {query_str}\\n\"\n\t    # \"You: \"\n\t)\n\tDEFAULT_PROMPT = QuestionAnswerPrompt(DEFAULT_PROMPT_TMPL)\n\tclass ConvoAgent(BaseModel):\n\t    \"\"\"Basic abstraction for a conversation agent.\"\"\"\n\t    name: str\n\t    st_memory: deque\n\t    lt_memory: BaseGPTIndex\n\t    lt_memory_query_kwargs: Dict = Field(default_factory=dict)\n", "    service_context: ServiceContext\n\t    st_memory_size: int = 10\n\t    # qa_prompt: QuestionAnswerPrompt = DEFAULT_PROMPT\n\t    user_prefix_tmpl: str = DEFAULT_USER_PREFIX_TMPL\n\t    qa_prompt_tmpl: str = DEFAULT_PROMPT_TMPL\n\t    class Config:\n\t        arbitrary_types_allowed = True\n\t    @classmethod\n\t    def from_defaults(\n\t        cls,\n", "        name: Optional[str] = None,\n\t        st_memory: Optional[deque] = None,\n\t        lt_memory: Optional[BaseGPTIndex] = None,\n\t        service_context: Optional[ServiceContext] = None,\n\t        **kwargs\n\t    ) -> \"ConvoAgent\":\n\t        name = name or \"Agent\"\n\t        st_memory = st_memory or deque()\n\t        lt_memory = lt_memory or GPTVectorStoreIndex([])\n\t        service_context = service_context or ServiceContext.from_defaults()\n", "        return cls(\n\t            name=name,\n\t            st_memory=st_memory,\n\t            lt_memory=lt_memory,\n\t            service_context=service_context,\n\t            **kwargs\n\t        )\n\t    def add_message(self, message: str, user: str) -> None:\n\t        \"\"\"Add message from another user.\"\"\"\n\t        fmt_message = format_text(message, user)\n", "        self.st_memory.append(fmt_message) \n\t        while len(self.st_memory) > self.st_memory_size:\n\t            self.st_memory.popleft()\n\t        self.lt_memory.insert(Document(fmt_message))\n\t    def generate_message(self, prev_message: Optional[str] = None) -> str:\n\t        \"\"\"Generate a new message.\"\"\"\n\t        # if prev_message is None, get previous message using short-term memory\n\t        if prev_message is None:\n\t            prev_message = self.st_memory[-1]\n\t        st_memory_text = \"\\n\".join([l for l in self.st_memory])\n", "        summary_response = self.lt_memory.as_query_engine(**self.lt_memory_query_kwargs).query(\n\t            f\"Tell me a bit more about any context that's relevant \"\n\t            f\"to the current messages: \\n{st_memory_text}\"\n\t        )\n\t        # add both the long-term memory summary and the short-term conversation\n\t        list_builder = GPTListIndex([])\n\t        list_builder.insert_nodes([Node(str(summary_response))])\n\t        list_builder.insert_nodes([Node(st_memory_text)])\n\t        # question-answer prompt\n\t        full_qa_prompt_tmpl = (\n", "            self.user_prefix_tmpl.format(name=self.name) + \"\\n\" +\n\t            self.qa_prompt_tmpl\n\t        )\n\t        qa_prompt = QuestionAnswerPrompt(full_qa_prompt_tmpl)\n\t        response = list_builder.as_query_engine(text_qa_template=qa_prompt).query(\n\t            \"Generate the next message in the conversation.\"\n\t        )    \n\t        return str(response)\n"]}
{"filename": "llama_agi/llama_agi/__init__.py", "chunked_list": []}
{"filename": "llama_agi/llama_agi/utils.py", "chunked_list": ["from typing import Any, List, Optional\n\tfrom llama_index import GPTVectorStoreIndex, GPTListIndex, ServiceContext, Document\n\tfrom llama_index.indices.base import BaseGPTIndex\n\tdef initialize_task_list_index(\n\t    documents: List[Document], service_context: Optional[ServiceContext] = None\n\t) -> BaseGPTIndex[Any]:\n\t    return GPTListIndex.from_documents(documents, service_context=service_context)\n\tdef initialize_search_index(\n\t    documents: List[Document], service_context: Optional[ServiceContext] = None\n\t) -> BaseGPTIndex[Any]:\n", "    return GPTVectorStoreIndex.from_documents(\n\t        documents, service_context=service_context\n\t    )\n\tdef log_current_status(\n\t    cur_task: str,\n\t    result: str,\n\t    completed_tasks_summary: str,\n\t    task_list: List[Document],\n\t    return_str: bool = False,\n\t) -> Optional[str]:\n", "    status_string = f\"\"\"\n\t    __________________________________\n\t    Completed Tasks Summary: {completed_tasks_summary.strip()}\n\t    Current Task: {cur_task.strip()}\n\t    Result: {result.strip()}\n\t    Task List: {\", \".join([x.get_text().strip() for x in task_list])}\n\t    __________________________________\n\t    \"\"\"\n\t    if return_str:\n\t        return status_string\n", "    else:\n\t        print(status_string, flush=True)\n\t        return None\n"]}
{"filename": "llama_agi/llama_agi/default_task_prompts.py", "chunked_list": ["#############################################\n\t##### AGI Prefix #####\n\t#############################################\n\tPREFIX = (\n\t    \"You are an autonomous artificial intelligence, capable of planning and executing tasks to achieve an objective.\\n\"\n\t    \"When given an objective, you can plan and execute any number tasks that will help achieve your original objective.\\n\"\n\t)\n\t#############################################\n\t##### Initial Completed Tasks Summary #####\n\t#############################################\n", "NO_COMPLETED_TASKS_SUMMARY = \"You haven't completed any tasks yet.\"\n\t#############################################\n\t##### Langchain - Execution Agent #####\n\t#############################################\n\tLC_PREFIX = PREFIX + \"You have access to the following tools:\"\n\tLC_SUFFIX = (\n\t    \"This is your current objective: {objective}\\n\"\n\t    \"Take into account what you have already achieved: {completed_tasks_summary}\\n\"\n\t    \"Using your current objective, your previously completed tasks, and your available tools,\"\n\t    \"Complete the current task.\\n\"\n", "    \"Begin!\\n\"\n\t    \"Task: {cur_task}\\n\"\n\t    \"Thought: {agent_scratchpad}\"\n\t)\n\t#############################################\n\t##### Langchain - Execution Chain #####\n\t#############################################\n\tLC_EXECUTION_PROMPT = (\n\t    \"You are an AI who performs one task based on the following objective: {objective}\\n.\"\n\t    \"Take into account this summary of previously completed tasks: {completed_tasks_summary}\\n.\"\n", "    \"Your task: {task}\\n\"\n\t    \"Response: \"\n\t)\n\t#############################################\n\t##### LlamaIndex -- Task Creation #####\n\t#############################################\n\tDEFAULT_TASK_CREATE_TMPL = (\n\t    f\"{PREFIX}\"\n\t    \"Your current objective is as follows: {query_str}\\n\"\n\t    \"Most recently, you completed the task '{prev_task}', which had the result of '{prev_result}'. \"\n", "    \"A description of your current incomplete tasks are below: \\n\"\n\t    \"---------------------\\n\"\n\t    \"{context_str}\"\n\t    \"\\n---------------------\\n\"\n\t    \"Given the current objective, the current incomplete tasks, and the latest completed task, \"\n\t    \"create new tasks to be completed that do not overlap with incomplete tasks. \"\n\t    \"Return the tasks as an array.\"\n\t)\n\t# TASK_CREATE_PROMPT = QuestionAnswerPrompt(DEFAULT_TASK_CREATE_TMPL)\n\tDEFAULT_REFINE_TASK_CREATE_TMPL = (\n", "    f\"{PREFIX}\"\n\t    \"Your current objective is as follows: {query_str}\\n\"\n\t    \"Most recently, you completed the task '{prev_task}', which had the result of '{prev_result}'. \"\n\t    \"A description of your current incomplete tasks are below: \\n\"\n\t    \"---------------------\\n\"\n\t    \"{context_msg}\"\n\t    \"\\n---------------------\\n\"\n\t    \"Currently, you have created the following new tasks: {existing_answer}\"\n\t    \"Given the current objective, the current incomplete tasks, list of newly created tasks, and the latest completed task, \"\n\t    \"add new tasks to be completed that do not overlap with incomplete tasks. \"\n", "    \"Return the tasks as an array. If you have no more tasks to add, repeat the existing list of new tasks.\"\n\t)\n\t# REFINE_TASK_CREATE_PROMPT = RefinePrompt(DEFAULT_REFINE_TASK_CREATE_TMPL)\n\t#############################################\n\t##### LlamaIndex -- Task Prioritization #####\n\t#############################################\n\tDEFAULT_TASK_PRIORITIZE_TMPL = (\n\t    f\"{PREFIX}\"\n\t    \"Your current objective is as follows: {query_str}\\n\"\n\t    \"A list of your current incomplete tasks are below: \\n\"\n", "    \"---------------------\\n\"\n\t    \"{context_str}\"\n\t    \"\\n---------------------\\n\"\n\t    \"Given the current objective, prioritize the current list of tasks. \"\n\t    \"Do not remove or add any tasks. Return the results as a numbered list, like:\\n\"\n\t    \"#. First task\\n\"\n\t    \"#. Second task\\n\"\n\t    \"... continue until all tasks are prioritized. \"\n\t    \"Start the task list with number 1.\"\n\t)\n", "DEFAULT_REFINE_TASK_PRIORITIZE_TMPL = (\n\t    f\"{PREFIX}\"\n\t    \"Your current objective is as follows: {query_str}\\n\"\n\t    \"A list of additional incomplete tasks are below: \\n\"\n\t    \"---------------------\\n\"\n\t    \"{context_msg}\"\n\t    \"\\n---------------------\\n\"\n\t    \"Currently, you also have the following list of prioritized tasks: {existing_answer}\"\n\t    \"Given the current objective and existing list, prioritize the current list of tasks. \"\n\t    \"Do not remove or add any tasks. Return the results as a numbered list, like:\\n\"\n", "    \"#. First task\\n\"\n\t    \"#. Second task\\n\"\n\t    \"... continue until all tasks are prioritized. \"\n\t    \"Start the task list with number 1.\"\n\t)\n"]}
{"filename": "llama_agi/llama_agi/tools/NoteTakingTools.py", "chunked_list": ["from langchain.agents import tool\n\tfrom llama_index import Document\n\tfrom llama_agi.utils import initialize_search_index\n\tnote_index = initialize_search_index([])\n\t@tool(\"Record Note\")\n\tdef record_note(note: str) -> str:\n\t    \"\"\"Useful for when you need to record a note or reminder for yourself to reference in the future.\"\"\"\n\t    global note_index\n\t    note_index.insert(Document(note))\n\t    return \"Note successfully recorded.\"\n", "@tool(\"Search Notes\")\n\tdef search_notes(query_str: str) -> str:\n\t    \"\"\"Useful for searching through notes that you previously recorded.\"\"\"\n\t    global note_index\n\t    response = note_index.as_query_engine(\n\t        similarity_top_k=3,\n\t    ).query(query_str)\n\t    return str(response)\n"]}
{"filename": "llama_agi/llama_agi/tools/WebpageSearchTool.py", "chunked_list": ["from langchain.agents import tool\n\tfrom llama_index import download_loader, ServiceContext\n\tfrom llama_agi.utils import initialize_search_index\n\tBeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n\t@tool(\"Search Webpage\")\n\tdef search_webpage(prompt: str) -> str:\n\t    \"\"\"Useful for searching a specific webpage. The input to the tool should be URL and query, separated by a newline.\"\"\"\n\t    loader = BeautifulSoupWebReader()\n\t    if len(prompt.split(\"\\n\")) < 2:\n\t        return \"The input to search_webpage should be a URL and a query, separated by a newline.\"\n", "    url = prompt.split(\"\\n\")[0]\n\t    query_str = \" \".join(prompt.split(\"\\n\")[1:])\n\t    try:\n\t        documents = loader.load_data(urls=[url])\n\t        service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n\t        index = initialize_search_index(documents, service_context=service_context)\n\t        query_result = index.as_query_engine(similarity_top_k=3).query(query_str)\n\t        return str(query_result)\n\t    except ValueError as e:\n\t        return str(e)\n", "    except Exception:\n\t        return \"Encountered an error while searching the webpage.\"\n"]}
{"filename": "llama_agi/llama_agi/tools/__init__.py", "chunked_list": ["from .NoteTakingTools import record_note, search_notes\n\tfrom .WebpageSearchTool import search_webpage\n\t__all__ = [record_note, search_notes, search_webpage]\n"]}
{"filename": "llama_agi/llama_agi/task_manager/LlamaTaskManager.py", "chunked_list": ["import re\n\timport json\n\tfrom typing import List, Tuple, Optional\n\tfrom llama_index import Document, ServiceContext\n\tfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n\tfrom llama_agi.task_manager.base import BaseTaskManager, LlamaTaskPrompts\n\tfrom llama_agi.utils import initialize_task_list_index\n\tfrom llama_agi.default_task_prompts import NO_COMPLETED_TASKS_SUMMARY\n\tclass LlamaTaskManager(BaseTaskManager):\n\t    \"\"\"Llama Task Manager\n", "    This task manager uses LlamaIndex to create and prioritize tasks. Using\n\t    the LlamaTaskPrompts, the task manager will create tasks that work\n\t    towards achieving an overall objective.\n\t    New tasks are created based on the prev task+result, completed tasks summary,\n\t    and the overall objective.\n\t    Tasks are then prioritized using the overall objective and current list of tasks.\n\t    Args:\n\t        tasks (List[str]): The initial list of tasks to complete.\n\t        prompts: (LlamaTaskPrompts): The prompts to control the task creation\n\t        and prioritization.\n", "        tasK_service_context (ServiceContext): The LlamaIndex service context to use\n\t        for task creation and prioritization.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        tasks: List[str],\n\t        prompts: LlamaTaskPrompts = LlamaTaskPrompts(),\n\t        task_service_context: Optional[ServiceContext] = None,\n\t    ) -> None:\n\t        super().__init__(\n", "            tasks=tasks, prompts=prompts, task_service_context=task_service_context\n\t        )\n\t        self.current_tasks_index = initialize_task_list_index(\n\t            self.current_tasks, service_context=self.task_service_context\n\t        )\n\t        self.completed_tasks_index = initialize_task_list_index(\n\t            self.completed_tasks, service_context=self.task_service_context\n\t        )\n\t        self.task_create_qa_template = self.prompts.task_create_qa_template\n\t        self.task_create_refine_template = self.prompts.task_create_refine_template\n", "        self.task_prioritize_qa_template = self.prompts.task_prioritize_qa_template\n\t        self.task_prioritize_refine_template = (\n\t            self.prompts.task_prioritize_refine_template\n\t        )\n\t    def _get_task_create_templates(\n\t        self, prev_task: str, prev_result: str\n\t    ) -> Tuple[QuestionAnswerPrompt, RefinePrompt]:\n\t        \"\"\"Fetch the task create prompts as llama_index objects.\"\"\"\n\t        text_qa_template = self.task_create_qa_template.format(\n\t            prev_result=prev_result,\n", "            prev_task=prev_task,\n\t            query_str=\"{query_str}\",\n\t            context_str=\"{context_str}\",\n\t        )\n\t        llama_text_qa_template = QuestionAnswerPrompt(text_qa_template)\n\t        refine_template = self.task_create_refine_template.format(\n\t            prev_result=prev_result,\n\t            prev_task=prev_task,\n\t            query_str=\"{query_str}\",\n\t            context_msg=\"{context_msg}\",\n", "            existing_answer=\"{existing_answer}\",\n\t        )\n\t        llama_refine_template = RefinePrompt(refine_template)\n\t        return (llama_text_qa_template, llama_refine_template)\n\t    def _get_task_prioritize_templates(\n\t        self,\n\t    ) -> Tuple[QuestionAnswerPrompt, RefinePrompt]:\n\t        \"\"\"Fetch the task prioritize prompts as llama_index objects.\"\"\"\n\t        return (\n\t            QuestionAnswerPrompt(self.task_prioritize_qa_template),\n", "            RefinePrompt(self.task_prioritize_refine_template),\n\t        )\n\t    def parse_task_list(self, task_list_str: str) -> List[str]:\n\t        \"\"\"Parse new tasks generated by the agent.\"\"\"\n\t        new_tasks: List[str] = []\n\t        try:\n\t            new_tasks = json.loads(task_list_str)\n\t            new_tasks = [x.strip() for x in new_tasks if len(x.strip()) > 10]\n\t        except Exception:\n\t            new_tasks = str(task_list_str).split(\"\\n\")\n", "            new_tasks = [\n\t                re.sub(r\"^[0-9]+\\.\", \"\", x).strip()\n\t                for x in str(new_tasks)\n\t                if len(x.strip()) > 10 and x[0].isnumeric()\n\t            ]\n\t        return new_tasks\n\t    def get_completed_tasks_summary(self) -> str:\n\t        \"\"\"Generate a summary of completed tasks.\"\"\"\n\t        if len(self.completed_tasks) == 0:\n\t            return NO_COMPLETED_TASKS_SUMMARY\n", "        summary = self.completed_tasks_index.as_query_engine(\n\t            response_mode=\"tree_summarize\"\n\t        ).query(\n\t            \"Summarize the current completed tasks\",\n\t        )\n\t        return str(summary)\n\t    def prioritize_tasks(self, objective: str) -> None:\n\t        \"\"\"Prioritize the current list of incomplete tasks.\"\"\"\n\t        (text_qa_template, refine_template) = self._get_task_prioritize_templates()\n\t        prioritized_tasks = self.current_tasks_index.as_query_engine(\n", "            text_qa_template=text_qa_template, refine_template=refine_template\n\t        ).query(objective)\n\t        new_tasks = []\n\t        for task in str(prioritized_tasks).split(\"\\n\"):\n\t            task = re.sub(r\"^[0-9]+\\.\", \"\", task).strip()\n\t            if len(task) > 10:\n\t                new_tasks.append(task)\n\t        self.current_tasks = [Document(x) for x in new_tasks]\n\t        self.current_tasks_index = initialize_task_list_index(\n\t            self.current_tasks, service_context=self.task_service_context\n", "        )\n\t    def generate_new_tasks(\n\t        self, objective: str, prev_task: str, prev_result: str\n\t    ) -> None:\n\t        \"\"\"Generate new tasks given the previous task and result.\"\"\"\n\t        (text_qa_template, refine_template) = self._get_task_create_templates(\n\t            prev_task, prev_result\n\t        )\n\t        task_list_response = self.completed_tasks_index.as_query_engine(\n\t            text_qa_template=text_qa_template, refine_template=refine_template\n", "        ).query(objective)\n\t        new_tasks = self.parse_task_list(str(task_list_response))\n\t        self.add_new_tasks(new_tasks)\n\t    def get_next_task(self) -> str:\n\t        \"\"\"Get the next task to complete.\"\"\"\n\t        next_task = self.current_tasks.pop().get_text()\n\t        self.current_tasks_index = initialize_task_list_index(\n\t            self.current_tasks, service_context=self.task_service_context\n\t        )\n\t        return next_task\n", "    def add_new_tasks(self, tasks: List[str]) -> None:\n\t        \"\"\"Add new tasks to the task manager.\"\"\"\n\t        for task in tasks:\n\t            if task not in self.current_tasks and task not in self.completed_tasks:\n\t                self.current_tasks.append(Document(task))\n\t        self.current_tasks_index = initialize_task_list_index(\n\t            self.current_tasks, service_context=self.task_service_context\n\t        )\n\t    def add_completed_task(self, task: str, result: str) -> None:\n\t        \"\"\"Add a task as completed.\"\"\"\n", "        document = Document(f\"Task: {task}\\nResult: {result}\\n\")\n\t        self.completed_tasks.append(document)\n\t        self.completed_tasks_index = initialize_task_list_index(\n\t            self.completed_tasks, service_context=self.task_service_context\n\t        )\n"]}
{"filename": "llama_agi/llama_agi/task_manager/base.py", "chunked_list": ["from abc import abstractmethod\n\tfrom dataclasses import dataclass\n\tfrom typing import List, Optional\n\tfrom llama_index import Document, ServiceContext\n\tfrom llama_agi.default_task_prompts import (\n\t    DEFAULT_TASK_PRIORITIZE_TMPL,\n\t    DEFAULT_REFINE_TASK_PRIORITIZE_TMPL,\n\t    DEFAULT_TASK_CREATE_TMPL,\n\t    DEFAULT_REFINE_TASK_CREATE_TMPL,\n\t)\n", "@dataclass\n\tclass LlamaTaskPrompts:\n\t    task_create_qa_template: str = DEFAULT_TASK_CREATE_TMPL\n\t    task_create_refine_template: str = DEFAULT_REFINE_TASK_CREATE_TMPL\n\t    task_prioritize_qa_template: str = DEFAULT_TASK_PRIORITIZE_TMPL\n\t    task_prioritize_refine_template: str = DEFAULT_REFINE_TASK_PRIORITIZE_TMPL\n\tclass BaseTaskManager:\n\t    \"\"\"Base Task Manager\n\t    Args:\n\t        tasks (List[str]): The initial list of tasks to complete.\n", "        prompts: (LlamaTaskPrompts): The prompts to control the task creation\n\t        and prioritization.\n\t        tasK_service_context (ServiceContext): The LlamaIndex service context to use\n\t        for task creation and prioritization.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        tasks: List[str],\n\t        prompts: LlamaTaskPrompts = LlamaTaskPrompts(),\n\t        task_service_context: Optional[ServiceContext] = None,\n", "    ) -> None:\n\t        self.current_tasks = [Document(x) for x in tasks]\n\t        self.completed_tasks: List[Document] = []\n\t        self.prompts = prompts\n\t        self.task_service_context = task_service_context\n\t    @abstractmethod\n\t    def parse_task_list(self, task_list_str: str) -> List[str]:\n\t        \"\"\"Parse new tasks generated by the agent.\"\"\"\n\t    @abstractmethod\n\t    def get_completed_tasks_summary(self) -> str:\n", "        \"\"\"Generate a summary of completed tasks.\"\"\"\n\t    @abstractmethod\n\t    def prioritize_tasks(self, objective: str) -> None:\n\t        \"\"\"Prioritize the current list of incomplete tasks.\"\"\"\n\t    @abstractmethod\n\t    def generate_new_tasks(\n\t        self, objective: str, prev_task: str, prev_result: str\n\t    ) -> None:\n\t        \"\"\"Generate new tasks given the previous task and result.\"\"\"\n\t    @abstractmethod\n", "    def get_next_task(self) -> str:\n\t        \"\"\"Get the next task to complete.\"\"\"\n\t    @abstractmethod\n\t    def add_new_tasks(self, tasks: List[str]) -> None:\n\t        \"\"\"Add new tasks to the task manager.\"\"\"\n\t    @abstractmethod\n\t    def add_completed_task(self, task: str, result: str) -> None:\n\t        \"\"\"Add a task as completed.\"\"\"\n"]}
{"filename": "llama_agi/llama_agi/task_manager/__init__.py", "chunked_list": ["from .LlamaTaskManager import LlamaTaskManager\n\t__all__ = [\n\t    LlamaTaskManager,\n\t]\n"]}
{"filename": "llama_agi/llama_agi/execution_agent/base.py", "chunked_list": ["from abc import abstractmethod\n\tfrom dataclasses import dataclass\n\tfrom typing import Any, Dict, List, Optional, Union\n\tfrom langchain.agents.tools import Tool\n\tfrom langchain.llms import OpenAI, BaseLLM\n\tfrom langchain.chat_models.base import BaseChatModel\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom llama_agi.default_task_prompts import LC_PREFIX, LC_SUFFIX, LC_EXECUTION_PROMPT\n\t@dataclass\n\tclass LlamaAgentPrompts:\n", "    execution_prompt: str = LC_EXECUTION_PROMPT\n\t    agent_prefix: str = LC_PREFIX\n\t    agent_suffix: str = LC_SUFFIX\n\tclass BaseExecutionAgent:\n\t    \"\"\"Base Execution Agent\n\t    Args:\n\t        llm (Union[BaseLLM, BaseChatModel]): The langchain LLM class to use.\n\t        model_name: (str): The name of the OpenAI model to use, if the LLM is\n\t        not provided.\n\t        max_tokens: (int): The maximum number of tokens the LLM can generate.\n", "        prompts: (LlamaAgentPrompts): The prompt templates used during execution.\n\t        tools: (List[Tool]): The list of langchain tools for the execution\n\t        agent to use.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        llm: Optional[Union[BaseLLM, BaseChatModel]] = None,\n\t        model_name: str = \"text-davinci-003\",\n\t        max_tokens: int = 512,\n\t        prompts: LlamaAgentPrompts = LlamaAgentPrompts(),\n", "        tools: Optional[List[Tool]] = None,\n\t    ) -> None:\n\t        if llm:\n\t            self._llm = llm\n\t        elif model_name == \"text-davinci-003\":\n\t            self._llm = OpenAI(\n\t                temperature=0, model_name=model_name, max_tokens=max_tokens\n\t            )\n\t        else:\n\t            self._llm = ChatOpenAI(\n", "                temperature=0, model_name=model_name, max_tokens=max_tokens\n\t            )\n\t        self.max_tokens = max_tokens\n\t        self.prompts = prompts\n\t        self.tools = tools if tools else []\n\t    @abstractmethod\n\t    def execute_task(self, **prompt_kwargs: Any) -> Dict[str, str]:\n\t        \"\"\"Execute a task.\"\"\"\n"]}
{"filename": "llama_agi/llama_agi/execution_agent/ToolExecutionAgent.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Union\n\tfrom string import Formatter\n\tfrom langchain.agents import AgentExecutor, ZeroShotAgent\n\tfrom langchain.agents.tools import Tool\n\tfrom langchain.chains import LLMChain\n\tfrom langchain.llms import BaseLLM\n\tfrom langchain.chat_models.base import BaseChatModel\n\tfrom llama_agi.execution_agent.base import BaseExecutionAgent, LlamaAgentPrompts\n\tclass ToolExecutionAgent(BaseExecutionAgent):\n\t    \"\"\"Tool Execution Agent\n", "    This agent is a wrapper around the zero-shot agent from Langchain. Using\n\t    a set of tools, the agent is expected to carry out and complete some task\n\t    that will help achieve an overall objective.\n\t    The agents overall behavior is controlled by the LlamaAgentPrompts.agent_prefix\n\t    and LlamaAgentPrompts.agent_suffix prompt templates.\n\t    The execution template kwargs are automatically extracted and expected to be\n\t    specified in execute_task().\n\t    execute_task() also returns the intermediate steps, for additional debugging and is\n\t    used for the streamlit example.\n\t    Args:\n", "        llm (Union[BaseLLM, BaseChatModel]): The langchain LLM class to use.\n\t        model_name: (str): The name of the OpenAI model to use, if the LLM is\n\t        not provided.\n\t        max_tokens: (int): The maximum number of tokens the LLM can generate.\n\t        prompts: (LlamaAgentPrompts): The prompt templates used during execution.\n\t        The Tool Execution Agent uses LlamaAgentPrompts.agent_prefix and\n\t        LlamaAgentPrompts.agent_suffix.\n\t        tools: (List[Tool]): The list of langchain tools for the execution agent to use.\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        llm: Optional[Union[BaseLLM, BaseChatModel]] = None,\n\t        model_name: str = \"text-davinci-003\",\n\t        max_tokens: int = 512,\n\t        prompts: LlamaAgentPrompts = LlamaAgentPrompts(),\n\t        tools: Optional[List[Tool]] = None,\n\t    ) -> None:\n\t        super().__init__(\n\t            llm=llm,\n\t            model_name=model_name,\n", "            max_tokens=max_tokens,\n\t            prompts=prompts,\n\t            tools=tools,\n\t        )\n\t        self.agent_prefix = self.prompts.agent_prefix\n\t        self.agent_suffix = self.prompts.agent_suffix\n\t        # create the agent\n\t        input_variables = [\n\t            fn for _, fn, _, _ in Formatter().parse(self.agent_prefix) if fn is not None\n\t        ] + [\n", "            fn for _, fn, _, _ in Formatter().parse(self.agent_suffix) if fn is not None\n\t        ]\n\t        self._agent_prompt = ZeroShotAgent.create_prompt(\n\t            self.tools,\n\t            prefix=self.agent_prefix,\n\t            suffix=self.agent_suffix,\n\t            input_variables=input_variables,\n\t        )\n\t        self._llm_chain = LLMChain(llm=self._llm, prompt=self._agent_prompt)\n\t        self._agent = ZeroShotAgent(\n", "            llm_chain=self._llm_chain, tools=self.tools, verbose=True\n\t        )\n\t        self._execution_chain = AgentExecutor.from_agent_and_tools(\n\t            agent=self._agent,\n\t            tools=self.tools,\n\t            verbose=True,\n\t            return_intermediate_steps=True,\n\t        )\n\t    def execute_task(self, **prompt_kwargs: Any) -> Dict[str, str]:\n\t        \"\"\"Execute a task, using tools.\"\"\"\n", "        result = self._execution_chain(prompt_kwargs)\n\t        return result\n"]}
{"filename": "llama_agi/llama_agi/execution_agent/SimpleExecutionAgent.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Union\n\tfrom string import Formatter\n\tfrom langchain.agents.tools import Tool\n\tfrom langchain.chains import LLMChain\n\tfrom langchain.llms import BaseLLM\n\tfrom langchain.chat_models.base import BaseChatModel\n\tfrom langchain.prompts import PromptTemplate\n\tfrom llama_agi.execution_agent.base import BaseExecutionAgent, LlamaAgentPrompts\n\tclass SimpleExecutionAgent(BaseExecutionAgent):\n\t    \"\"\"Simple Execution Agent\n", "    This agent uses an LLM to execute a basic action without tools.\n\t    The LlamaAgentPrompts.execution_prompt defines how this execution agent\n\t    behaves.\n\t    Usually, this is used for simple tasks, like generating the initial list of tasks.\n\t    The execution template kwargs are automatically extracted and expected to be\n\t    specified in execute_task().\n\t    Args:\n\t        llm (Union[BaseLLM, BaseChatModel]): The langchain LLM class to use.\n\t        model_name: (str): The name of the OpenAI model to use, if the LLM is\n\t        not provided.\n", "        max_tokens: (int): The maximum number of tokens the LLM can generate.\n\t        prompts: (LlamaAgentPrompts): The prompt templates used during execution.\n\t        The only prompt used byt the SimpleExecutionAgent is\n\t        LlamaAgentPrompts.execution_prompt.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        llm: Optional[Union[BaseLLM, BaseChatModel]] = None,\n\t        model_name: str = \"text-davinci-003\",\n\t        max_tokens: int = 512,\n", "        prompts: LlamaAgentPrompts = LlamaAgentPrompts(),\n\t        tools: Optional[List[Tool]] = None,\n\t    ) -> None:\n\t        super().__init__(\n\t            llm=llm,\n\t            model_name=model_name,\n\t            max_tokens=max_tokens,\n\t            prompts=prompts,\n\t            tools=tools,\n\t        )\n", "        self.execution_prompt = self.prompts.execution_prompt\n\t        input_variables = [\n\t            fn\n\t            for _, fn, _, _ in Formatter().parse(self.execution_prompt)\n\t            if fn is not None\n\t        ]\n\t        self._prompt_template = PromptTemplate(\n\t            template=self.execution_prompt,\n\t            input_variables=input_variables,\n\t        )\n", "        self._execution_chain = LLMChain(llm=self._llm, prompt=self._prompt_template)\n\t    def execute_task(self, **prompt_kwargs: Any) -> Dict[str, str]:\n\t        \"\"\"Execute a task.\"\"\"\n\t        result = self._execution_chain.predict(**prompt_kwargs)\n\t        return {\"output\": result}\n"]}
{"filename": "llama_agi/llama_agi/execution_agent/__init__.py", "chunked_list": ["from .SimpleExecutionAgent import SimpleExecutionAgent\n\tfrom .ToolExecutionAgent import ToolExecutionAgent\n\t__all__ = [SimpleExecutionAgent, ToolExecutionAgent]\n"]}
{"filename": "llama_agi/llama_agi/runners/AutoAGIRunner.py", "chunked_list": ["import time\n\tfrom typing import List, Optional\n\tfrom llama_agi.runners.base import BaseAGIRunner\n\tfrom llama_agi.execution_agent.SimpleExecutionAgent import SimpleExecutionAgent\n\tfrom llama_agi.utils import log_current_status\n\tclass AutoAGIRunner(BaseAGIRunner):\n\t    def run(\n\t        self,\n\t        objective: str,\n\t        initial_task: str,\n", "        sleep_time: int,\n\t        initial_task_list: Optional[List[str]] = None,\n\t    ) -> None:\n\t        # get initial list of tasks\n\t        if initial_task_list:\n\t            self.task_manager.add_new_tasks(initial_task_list)\n\t        else:\n\t            initial_completed_tasks_summary = (\n\t                self.task_manager.get_completed_tasks_summary()\n\t            )\n", "            initial_task_prompt = initial_task + \"\\nReturn the list as an array.\"\n\t            # create simple execution agent using current agent\n\t            simple_execution_agent = SimpleExecutionAgent(\n\t                llm=self.execution_agent._llm,\n\t                max_tokens=self.execution_agent.max_tokens,\n\t                prompts=self.execution_agent.prompts,\n\t            )\n\t            initial_task_list_result = simple_execution_agent.execute_task(\n\t                objective=objective,\n\t                task=initial_task_prompt,\n", "                completed_tasks_summary=initial_completed_tasks_summary,\n\t            )\n\t            initial_task_list = self.task_manager.parse_task_list(\n\t                initial_task_list_result[\"output\"]\n\t            )\n\t            # add tasks to the task manager\n\t            self.task_manager.add_new_tasks(initial_task_list)\n\t        # prioritize initial tasks\n\t        self.task_manager.prioritize_tasks(objective)\n\t        completed_tasks_summary = initial_completed_tasks_summary\n", "        while True:\n\t            # Get the next task\n\t            cur_task = self.task_manager.get_next_task()\n\t            # Execute current task\n\t            result = self.execution_agent.execute_task(\n\t                objective=objective,\n\t                cur_task=cur_task,\n\t                completed_tasks_summary=completed_tasks_summary,\n\t            )[\"output\"]\n\t            # store the task and result as completed\n", "            self.task_manager.add_completed_task(cur_task, result)\n\t            # generate new task(s), if needed\n\t            self.task_manager.generate_new_tasks(objective, cur_task, result)\n\t            # Summarize completed tasks\n\t            completed_tasks_summary = self.task_manager.get_completed_tasks_summary()\n\t            # log state of AGI to terminal\n\t            log_current_status(\n\t                cur_task,\n\t                result,\n\t                completed_tasks_summary,\n", "                self.task_manager.current_tasks,\n\t            )\n\t            # Quit the loop?\n\t            if len(self.task_manager.current_tasks) == 0:\n\t                print(\"Out of tasks! Objective Accomplished?\")\n\t                break\n\t            # wait a bit to let you read what's happening\n\t            time.sleep(sleep_time)\n"]}
{"filename": "llama_agi/llama_agi/runners/base.py", "chunked_list": ["from abc import abstractmethod\n\tfrom typing import List, Optional\n\tfrom llama_agi.execution_agent.base import BaseExecutionAgent\n\tfrom llama_agi.task_manager.base import BaseTaskManager\n\tclass BaseAGIRunner:\n\t    def __init__(\n\t        self, task_manager: BaseTaskManager, execution_agent: BaseExecutionAgent\n\t    ) -> None:\n\t        self.task_manager = task_manager\n\t        self.execution_agent = execution_agent\n", "    @abstractmethod\n\t    def run(\n\t        self,\n\t        objective: str,\n\t        initial_task: str,\n\t        sleep_time: int,\n\t        initial_task_list: Optional[List[str]] = None,\n\t    ) -> None:\n\t        \"\"\"Run the task manager and execution agent in a loop.\"\"\"\n"]}
{"filename": "llama_agi/llama_agi/runners/__init__.py", "chunked_list": ["from .AutoAGIRunner import AutoAGIRunner\n\tfrom .AutoStreamlitAGIRunner import AutoStreamlitAGIRunner\n\t__all__ = [AutoAGIRunner, AutoStreamlitAGIRunner]\n"]}
{"filename": "llama_agi/llama_agi/runners/AutoStreamlitAGIRunner.py", "chunked_list": ["import json\n\timport streamlit as st\n\timport time\n\tfrom typing import List, Optional\n\tfrom llama_agi.runners.base import BaseAGIRunner\n\tfrom llama_agi.execution_agent.SimpleExecutionAgent import SimpleExecutionAgent\n\tfrom llama_agi.utils import log_current_status\n\tdef make_intermediate_steps_pretty(json_str: str) -> List[str]:\n\t    steps = json.loads(json_str)\n\t    output = []\n", "    for action_set in steps:\n\t        for step in action_set:\n\t            if isinstance(step, list):\n\t                output.append(step[-1])\n\t            else:\n\t                output.append(step)\n\t    return output\n\tclass AutoStreamlitAGIRunner(BaseAGIRunner):\n\t    def run(\n\t        self,\n", "        objective: str,\n\t        initial_task: str,\n\t        sleep_time: int,\n\t        initial_task_list: Optional[List[str]] = None,\n\t        max_iterations: Optional[int] = None,\n\t    ) -> None:\n\t        run_initial_task = False\n\t        if \"logs\" not in st.session_state:\n\t            st.session_state[\"logs\"] = []\n\t            st.session_state[\"state_str\"] = \"No state yet!\"\n", "            st.session_state[\"tasks_summary\"] = \"\"\n\t            run_initial_task = True\n\t        logs_col, state_col = st.columns(2)\n\t        with logs_col:\n\t            st.subheader(\"Execution Log\")\n\t            st_logs = st.empty()\n\t        st_logs.write(st.session_state[\"logs\"])\n\t        with state_col:\n\t            st.subheader(\"AGI State\")\n\t            st_state = st.empty()\n", "        st_state.write(st.session_state[\"state_str\"])\n\t        if run_initial_task:\n\t            # get initial list of tasks\n\t            if initial_task_list:\n\t                self.task_manager.add_new_tasks(initial_task_list)\n\t            else:\n\t                initial_completed_tasks_summary = (\n\t                    self.task_manager.get_completed_tasks_summary()\n\t                )\n\t                initial_task_prompt = initial_task + \"\\nReturn the list as an array.\"\n", "                # create simple execution agent using current agent\n\t                simple_execution_agent = SimpleExecutionAgent(\n\t                    llm=self.execution_agent._llm,\n\t                    max_tokens=self.execution_agent.max_tokens,\n\t                    prompts=self.execution_agent.prompts,\n\t                )\n\t                initial_task_list_result = simple_execution_agent.execute_task(\n\t                    objective=objective,\n\t                    task=initial_task_prompt,\n\t                    completed_tasks_summary=initial_completed_tasks_summary,\n", "                )\n\t                initial_task_list = self.task_manager.parse_task_list(\n\t                    initial_task_list_result[\"output\"]\n\t                )\n\t                # add tasks to the task manager\n\t                self.task_manager.add_new_tasks(initial_task_list)\n\t            # prioritize initial tasks\n\t            self.task_manager.prioritize_tasks(objective)\n\t            tasks_summary = initial_completed_tasks_summary\n\t            st.session_state[\"tasks_summary\"] = tasks_summary\n", "            # update streamlit state\n\t            st.session_state[\"state_str\"] = log_current_status(\n\t                initial_task,\n\t                initial_task_list_result[\"output\"],\n\t                tasks_summary,\n\t                self.task_manager.current_tasks,\n\t                return_str=True,\n\t            )\n\t            if st.session_state[\"state_str\"]:\n\t                st_state.markdown(st.session_state[\"state_str\"].replace(\"\\n\", \"\\n\\n\"))\n", "        for _ in range(0, max_iterations):\n\t            # Get the next task\n\t            cur_task = self.task_manager.get_next_task()\n\t            # Execute current task\n\t            result_dict = self.execution_agent.execute_task(\n\t                objective=objective,\n\t                cur_task=cur_task,\n\t                completed_tasks_summary=st.session_state[\"tasks_summary\"],\n\t            )\n\t            result = result_dict[\"output\"]\n", "            # update logs\n\t            log = make_intermediate_steps_pretty(\n\t                json.dumps(result_dict[\"intermediate_steps\"])\n\t            ) + [result]\n\t            st.session_state[\"logs\"].append(log)\n\t            st_logs.write(st.session_state[\"logs\"])\n\t            # store the task and result as completed\n\t            self.task_manager.add_completed_task(cur_task, result)\n\t            # generate new task(s), if needed\n\t            self.task_manager.generate_new_tasks(objective, cur_task, result)\n", "            # Summarize completed tasks\n\t            completed_tasks_summary = self.task_manager.get_completed_tasks_summary()\n\t            st.session_state[\"tasks_summary\"] = completed_tasks_summary\n\t            # log state of AGI to streamlit\n\t            st.session_state[\"state_str\"] = log_current_status(\n\t                cur_task,\n\t                result,\n\t                completed_tasks_summary,\n\t                self.task_manager.current_tasks,\n\t                return_str=True,\n", "            )\n\t            if st.session_state[\"state_str\"] is not None:\n\t                st_state.markdown(st.session_state[\"state_str\"].replace(\"\\n\", \"\\n\\n\"))\n\t            # Quit the loop?\n\t            if len(self.task_manager.current_tasks) == 0:\n\t                st.success(\"Out of tasks! Objective Accomplished?\")\n\t                break\n\t            # wait a bit to let you read what's happening\n\t            time.sleep(sleep_time)\n"]}
{"filename": "llama_agi/examples/streamlit_runner_example.py", "chunked_list": ["import os\n\timport streamlit as st\n\tfrom langchain.agents import load_tools\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.llms import OpenAI\n\tfrom llama_agi.execution_agent import ToolExecutionAgent\n\tfrom llama_agi.runners import AutoStreamlitAGIRunner\n\tfrom llama_agi.task_manager import LlamaTaskManager\n\tfrom llama_index import ServiceContext, LLMPredictor\n\tst.set_page_config(layout=\"wide\")\n", "st.header(\"ðŸ¤– Llama AGI ðŸ¦™\")\n\tst.markdown(\"This demo uses the [llama-agi](https://github.com/run-llama/llama-lab/tree/main/llama_agi) package to create an AutoGPT-like agent, powered by [LlamaIndex](https://github.com/jerryjliu/llama_index) and Langchain. The AGI has access to tools that search the web and record notes, as it works to achieve an objective. Use the setup tab to configure your LLM settings and initial objective+tasks. Then use the Launch tab to run the AGI. Kill the AGI by refreshing the page.\")\n\tsetup_tab, launch_tab = st.tabs([\"Setup\", \"Launch\"])\n\twith setup_tab:\n\t    if 'init' in st.session_state:\n\t        st.success(\"Initialized!\")\n\t    st.subheader(\"LLM Setup\")\n\t    col1, col2, col3 = st.columns(3)\n\t    with col1:\n\t        openai_api_key = st.text_input(\"Enter your OpenAI API key here\", type=\"password\")\n", "        llm_name = st.selectbox(\n\t            \"Which LLM?\", [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"]\n\t        )\n\t    with col2:\n\t        google_api_key = st.text_input(\"Enter your Google API key here\", type=\"password\")\n\t        model_temperature = st.slider(\n\t            \"LLM Temperature\", min_value=0.0, max_value=1.0, step=0.1, value=0.0\n\t        )\n\t    with col3:\n\t        google_cse_id = st.text_input(\"Enter your Google CSE ID key here\", type=\"password\")\n", "        max_tokens = st.slider(\n\t            \"LLM Max Tokens\", min_value=256, max_value=1024, step=8, value=512\n\t        )\n\t    st.subheader(\"AGI Setup\")\n\t    objective = st.text_input(\"Objective:\", value=\"Solve world hunger\")\n\t    initial_task = st.text_input(\"Initial Task:\", value=\"Create a list of tasks\")\n\t    max_iterations = st.slider(\"Iterations until pause\", value=1, min_value=1, max_value=10, step=1)\n\t    if st.button('Initialize?'):\n\t        os.environ['OPENAI_API_KEY'] = openai_api_key\n\t        os.environ['GOOGLE_API_KEY'] = google_api_key\n", "        os.environ['GOOGLE_CSE_ID'] = google_cse_id\n\t        if llm_name == \"text-davinci-003\":\n\t            llm = OpenAI(\n\t                temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens\n\t            )\n\t        else:\n\t            llm= ChatOpenAI(\n\t                temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens\n\t            )\n\t        service_context = ServiceContext.from_defaults(\n", "            llm_predictor=LLMPredictor(llm=llm), chunk_size_limit=512\n\t        )\n\t        st.session_state['task_manager'] = LlamaTaskManager(\n\t            [initial_task], task_service_context=service_context\n\t        )\n\t        from llama_agi.tools import search_notes, record_note, search_webpage\n\t        tools = load_tools([\"google-search-results-json\"])\n\t        tools = tools + [search_notes, record_note, search_webpage]\n\t        st.session_state['execution_agent'] = ToolExecutionAgent(llm=llm, tools=tools)\n\t        st.session_state['initial_task'] = initial_task\n", "        st.session_state['objective'] = objective\n\t        st.session_state['init'] = True\n\t        st.experimental_rerun()\n\twith launch_tab:\n\t    st.subheader(\"AGI Status\")\n\t    if st.button(f\"Continue for {max_iterations} Steps\"):\n\t        if st.session_state.get('init', False):\n\t            # launch the auto runner\n\t            with st.spinner(\"Running!\"):\n\t                runner = AutoStreamlitAGIRunner(st.session_state['task_manager'], st.session_state['execution_agent'])\n", "                runner.run(st.session_state['objective'], st.session_state['initial_task'], 2, max_iterations=max_iterations)\n"]}
{"filename": "llama_agi/examples/auto_runner_example.py", "chunked_list": ["import argparse\n\tfrom langchain.agents import load_tools\n\tfrom langchain.llms import OpenAI\n\tfrom llama_agi.execution_agent import ToolExecutionAgent\n\tfrom llama_agi.runners import AutoAGIRunner\n\tfrom llama_agi.task_manager import LlamaTaskManager\n\tfrom llama_agi.tools import search_notes, record_note, search_webpage\n\tfrom llama_index import ServiceContext, LLMPredictor\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(\n", "        prog=\"Llama AGI\",\n\t        description=\"A baby-agi/auto-gpt inspired application, powered by Llama Index!\",\n\t    )\n\t    parser.add_argument(\n\t        \"-it\",\n\t        \"--initial-task\",\n\t        default=\"Create a list of tasks\",\n\t        help=\"The initial task for the system to carry out. Default='Create a list of tasks'\",\n\t    )\n\t    parser.add_argument(\n", "        \"-o\",\n\t        \"--objective\",\n\t        default=\"Solve world hunger\",\n\t        help=\"The overall objective for the system. Default='Solve world hunger'\",\n\t    )\n\t    parser.add_argument(\n\t        \"--sleep-time\",\n\t        default=2,\n\t        help=\"Sleep time (in seconds) between each task loop. Default=2\",\n\t        type=int,\n", "    )\n\t    args = parser.parse_args()\n\t    # LLM setup\n\t    llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n\t    service_context = ServiceContext.from_defaults(\n\t        llm_predictor=LLMPredictor(llm=llm), chunk_size_limit=512\n\t    )\n\t    # llama_agi setup\n\t    task_manager = LlamaTaskManager(\n\t        [args.initial_task], task_service_context=service_context\n", "    )\n\t    tools = load_tools([\"google-search-results-json\"])\n\t    tools = tools + [search_notes, record_note, search_webpage]\n\t    execution_agent = ToolExecutionAgent(llm=llm, tools=tools)\n\t    # launch the auto runner\n\t    runner = AutoAGIRunner(task_manager, execution_agent)\n\t    runner.run(args.objective, args.initial_task, args.sleep_time)\n"]}
{"filename": "auto_llama/auto_llama/actions.py", "chunked_list": ["import json\n\timport os\n\tfrom duckduckgo_search import ddg\n\tfrom llama_index.readers.web import BeautifulSoupWebReader\n\tfrom llama_index import GPTListIndex\n\tfrom auto_llama.data_models import Response\n\tfrom typing import Dict\n\tfrom auto_llama.const import SEARCH_RESULTS_TEMPLATE, format_web_download\n\tfrom llama_index import Document\n\tfrom llama_index.indices.composability import ComposableGraph\n", "from llama_index import GPTListIndex, LLMPredictor, ServiceContext\n\tfrom langchain.llms.base import BaseLLM\n\tfrom llama_index.logger import LlamaLogger\n\tdef run_command(user_query: str, command: str, args: Dict, llm: BaseLLM) -> str:\n\t    llama_logger = LlamaLogger()\n\t    service_context = ServiceContext.from_defaults(\n\t        llm_predictor=LLMPredictor(llm), llama_logger=llama_logger\n\t    )\n\t    if command == \"search\":\n\t        search_terms = args[\"search_terms\"]\n", "        print(\"Searching...\\n\")\n\t        results = search_web(search_terms)\n\t        response = analyze_search_results(\n\t            user_query, search_terms, results, service_context\n\t        )\n\t        print(response + \"\\n\")\n\t        return response\n\t    elif command == \"download\":\n\t        url = args[\"url\"]\n\t        doc_name = args[\"doc_name\"]\n", "        print(\"Downloading web page...\\n\")\n\t        if isinstance(url, str) and \"[\" in url and \"]\" in url:  # list parsing case\n\t            url = url.strip(\"[\").strip(\"]\").split(\", \")\n\t            doc_name = doc_name.strip(\"[\").strip(\"]\").split(\", \")\n\t        if isinstance(url, list):\n\t            if len(url) != len(doc_name):\n\t                raise ValueError(\"url and doc_name must have the same length\")\n\t            results = []\n\t            if os.path.exists(\"data/web_summary_cache.json\"):\n\t                with open(\"data/web_summary_cache.json\", \"r\") as f:\n", "                    web_summary_cache = json.load(f)\n\t            else:\n\t                web_summary_cache = {}\n\t            for i in range(len(url)):\n\t                web_summary = download_web(url[i], doc_name[i], service_context)\n\t                results.append(format_web_download(url[i], doc_name[i], web_summary))\n\t                web_summary_cache[doc_name[i]] = web_summary\n\t            print(\"Writing web summary cache to file\")\n\t            with open(\"data/web_summary_cache.json\", \"w\") as f:\n\t                json.dump(web_summary_cache, f)\n", "            response = \"\\n\".join(results)\n\t            print(response)\n\t            return response\n\t        else:\n\t            if os.path.exists(\"data/web_summary_cache.json\"):\n\t                with open(\"data/web_summary_cache.json\", \"r\") as f:\n\t                    web_summary_cache = json.load(f)\n\t            else:\n\t                web_summary_cache = {}\n\t            web_summary = download_web(url, doc_name, service_context)\n", "            web_summary_cache[doc_name] = web_summary\n\t            print(\"Writing web summary cache to file\")\n\t            with open(\"data/web_summary_cache.json\", \"w\") as f:\n\t                json.dump(web_summary_cache, f)\n\t            response = format_web_download(url, doc_name, web_summary)\n\t            print(response)\n\t            return response\n\t    elif command == \"query\":\n\t        print(\"Querying...\\n\")\n\t        response = query_docs(args[\"docs\"], args[\"query\"], service_context)\n", "        print(response)\n\t        return response\n\t    elif command == \"write\":\n\t        print(\"Writing to file...\\n\")\n\t        return write_to_file(args[\"file_name\"], args[\"data\"])\n\t    elif command == \"exit\":\n\t        print(\"Exiting...\\n\")\n\t        return \"exit\"\n\t    else:\n\t        raise ValueError(f\"Unknown command: {command}\")\n", "def search_web(search_terms, max_results=5):\n\t    \"\"\"Search the Web and obtain a list of web results.\"\"\"\n\t    results = ddg(search_terms, max_results=max_results)\n\t    return results\n\tdef analyze_search_results(user_query, search_terms, results, service_context):\n\t    \"\"\"Analyze the results of the search using llm.\"\"\"\n\t    doc = Document(json.dumps(results))\n\t    index = GPTListIndex.from_documents([doc], service_context=service_context)\n\t    response = index.query(\n\t        SEARCH_RESULTS_TEMPLATE.format(search_terms=search_terms, user_query=user_query)\n", "    )\n\t    return response.response\n\tdef download_web(url: str, doc_name: str, service_context: ServiceContext):\n\t    \"\"\"Download the html of the url and save a reference under doc_name.\n\t    Return the summary of the web page.\n\t    \"\"\"\n\t    reader = BeautifulSoupWebReader()\n\t    docs = reader.load_data([url])\n\t    index = GPTListIndex.from_documents(docs, service_context=service_context)\n\t    if not os.path.exists(\"data\"):\n", "        os.mkdir(\"data\")\n\t    index.save_to_disk(\"data/\" + doc_name + \".json\")\n\t    summary = index.query(\n\t        \"Summarize the contents of this web page.\", response_mode=\"tree_summarize\", use_async=True\n\t    )\n\t    return summary.response\n\tdef query_docs(docs, query, service_context):\n\t    query_configs = [\n\t        {\n\t            \"index_struct_type\": \"list\",\n", "            \"query_mode\": \"default\",\n\t            \"query_kwargs\": {\"response_mode\": \"tree_summarize\", \"use_async\": True},\n\t        }\n\t    ]\n\t    print(\"Opening web summary cache\")\n\t    with open(\"data/web_summary_cache.json\", \"r\") as f:\n\t        doc_summary_cache = json.load(f)\n\t    if isinstance(docs, list):\n\t        indices = []\n\t        for doc_name in docs:\n", "            index = GPTListIndex.load_from_disk(\n\t                \"data/\" + doc_name + \".json\", service_context=service_context\n\t            )\n\t            indices.append((index, doc_summary_cache[doc_name]))\n\t        graph = ComposableGraph.from_indices(\n\t            GPTListIndex,\n\t            [index[0] for index in indices],\n\t            index_summaries=[index[1] for index in indices],\n\t            service_context=service_context,\n\t        )\n", "        response = graph.query(\n\t            query, query_configs=query_configs, service_context=service_context\n\t        )\n\t        return response.response\n\t    else:\n\t        index = GPTListIndex.load_from_disk(\n\t            \"data/\" + docs + \".json\", service_context=service_context\n\t        )\n\t        response = index.query(query, service_context=service_context)\n\t        return response.response\n", "def write_to_file(file_name, data):\n\t    print(\"Writing to file\" + file_name)\n\t    with open(file_name, \"w\") as f:\n\t        f.write(data)\n\t    return \"done\"\n"]}
{"filename": "auto_llama/auto_llama/data_models.py", "chunked_list": ["from pydantic import BaseModel, Field, root_validator\n\tfrom typing import Dict, Union, List\n\timport json\n\tclass Command(BaseModel):\n\t    action: str = Field(description=\"This is the current action\")\n\t    args: Dict = Field(description=\"This is the command's arguments\")\n\t    @root_validator\n\t    def validate_all(cls, values):\n\t        # print(f\"{values}\")\n\t        if values[\"action\"] == \"search\" and \"search_terms\" not in values[\"args\"]:\n", "            raise ValueError(\"malformed search args\")\n\t        if values[\"action\"] == \"download\" and (\n\t            \"url\" not in values[\"args\"] or \"doc_name\" not in values[\"args\"]\n\t        ):\n\t            raise ValueError(\"malformed download args\")\n\t        if values[\"action\"] == \"query\" and (\n\t            \"docs\" not in values[\"args\"] or \"query\" not in values[\"args\"]\n\t        ):\n\t            raise ValueError(\"malformed query args\")\n\t        if values[\"action\"] == \"write\" and (\n", "            \"file_name\" not in values[\"args\"] or \"data\" not in values[\"args\"]\n\t        ):\n\t            raise ValueError(\"malformed write args\")\n\t        return values\n\t    def toJSON(self):\n\t        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n\tclass Response(BaseModel):\n\t    remember: str = Field(description=\"This is what the AI just accomplished. Probably should not do it again\")\n\t    thoughts: str = Field(description=\"This what the AI is currently thinking.\")\n\t    reasoning: str = Field(\n", "        description=\"This is why the AI thinks it will help lead to the user's desired result\"\n\t    )\n\t    plan: Union[str, object] = Field(\n\t        description=\"This is the AI's current plan of action\"\n\t    )\n\t    command: Command = Field(description=\"This is the AI's current command\")\n"]}
{"filename": "auto_llama/auto_llama/__main__.py", "chunked_list": ["import json\n\tfrom auto_llama.agent import Agent\n\timport auto_llama.const as const\n\tfrom auto_llama.utils import print_pretty\n\tfrom auto_llama.actions import run_command\n\tfrom langchain.chat_models import ChatOpenAI\n\timport logging\n\tdef main():\n\t    logger = logging.getLogger()\n\t    logger.level = logging.WARN\n", "    # # Enter your OpenAI API key here:\n\t    # import os\n\t    # os.environ[\"OPENAI_API_KEY\"] = 'YOUR OPENAI API KEY'\n\t    openaichat = ChatOpenAI(\n\t        model_name=\"gpt-4\",\n\t        temperature=0.0,\n\t        max_tokens=400,\n\t    )\n\t    user_query = input(\"Enter what you would like AutoLlama to do:\\n\")\n\t    if user_query == \"\":\n", "        user_query = \"Summarize the financial news from the past week.\"\n\t        print(\"I will summarize the financial news from the past week.\\n\")\n\t    agent = Agent(const.DEFAULT_AGENT_PREAMBLE, user_query, openaichat)\n\t    while True:\n\t        print(\"Thinking...\")\n\t        response = agent.get_response()\n\t        print_pretty(response)\n\t        action, args = response.command.action, response.command.args\n\t        user_confirm = input(\n\t            'Should I run the command \"'\n", "            + action\n\t            + '\" with args '\n\t            + json.dumps(args)\n\t            + \"? (y/[N])\\n\"\n\t        )\n\t        if user_confirm == \"y\":\n\t            action_results = run_command(user_query, action, args, openaichat)\n\t            # print(action_results)\n\t            agent.memory.append(action_results)\n\t            if action_results == \"exit\" or action_results == \"done\":\n", "                break\n\t        else:\n\t            break\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "auto_llama/auto_llama/agent.py", "chunked_list": ["from auto_llama.utils import get_date\n\tfrom langchain.output_parsers import PydanticOutputParser\n\tfrom auto_llama.data_models import Response\n\tfrom langchain.prompts import (\n\t    ChatPromptTemplate,\n\t    SystemMessagePromptTemplate,\n\t    HumanMessagePromptTemplate,\n\t)\n\tfrom langchain.schema import AIMessage\n\tfrom typing import List\n", "from auto_llama.tokens import count_tokens\n\tclass Agent:\n\t    \"\"\"A class representing an agent.\n\t    Attributes:\n\t        desc(str):\n\t            A description of the agent used in the preamble.\n\t        task(str):\n\t            The task the agent is supposed to perform.\n\t        memory(list):\n\t            A list of the agent's memories.\n", "        llm(BaseLLM):\n\t            The LLM used by the agent.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        desc,\n\t        task,\n\t        llm,\n\t        memory=[],\n\t    ):\n", "        \"\"\"Initialize the agent.\"\"\"\n\t        self.desc = desc\n\t        self.task = task\n\t        self.memory = memory\n\t        self.llm = llm\n\t        memory.append(\"Here is a list of your previous actions:\")\n\t    def get_response(self) -> Response:\n\t        \"\"\"Get the response given the agent's current state.\"\"\"\n\t        parser: PydanticOutputParser = PydanticOutputParser(pydantic_object=Response)\n\t        format_instructions = parser.get_format_instructions()\n", "        llm_input = self.create_chat_messages(\n\t            self.desc, self.task, self.memory, format_instructions\n\t        ).to_messages()\n\t        # print(llm_input)\n\t        output: AIMessage = self.llm(llm_input)\n\t        # print(output.content)\n\t        self.memory.append(\"Old thought: \" + output.content)\n\t        response_obj = parser.parse(output.content)\n\t        # print(response_obj)\n\t        return response_obj\n", "    def create_chat_messages(\n\t        self, desc: str, task: str, memory: List[str], format_instructions: str\n\t    ):\n\t        \"\"\"Create the messages for the agent.\"\"\"\n\t        messages = []\n\t        system_template = \"{desc}\\n{memory}\\n{date}\\n{format_instructions}\"\n\t        system_message_prompt = SystemMessagePromptTemplate.from_template(\n\t            system_template\n\t        )\n\t        messages.append(system_message_prompt)\n", "        human_template = \"{text}\"\n\t        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\t        messages.append(human_message_prompt)\n\t        prompt_template = ChatPromptTemplate.from_messages(messages)\n\t        date_str = \"The current date is \" + get_date()\n\t        recent_memories = self.create_memories(memory)\n\t        # print(recent_memories)\n\t        prompt = prompt_template.format_prompt(\n\t            desc=desc,\n\t            memory=recent_memories,\n", "            date=date_str,\n\t            format_instructions=format_instructions,\n\t            text=task,\n\t        )\n\t        return prompt\n\t    def create_memories(self, memory: List[str], max_tokens: int = 2000):\n\t        # print(memory)\n\t        token_counter = 0\n\t        memories: List[str] = []\n\t        memories.insert(0, memory[0])  # always include memories header.\n", "        token_counter += count_tokens(memory[0])\n\t        memory_index = len(memory) - 1\n\t        while memory_index > 0 and token_counter < max_tokens:\n\t            memories.insert(1, memory[memory_index])\n\t            token_counter += count_tokens(memory[memory_index])\n\t            memory_index -= 1\n\t        return \"\\n\".join(memories)\n"]}
{"filename": "auto_llama/auto_llama/tokens.py", "chunked_list": ["import tiktoken\n\tdef count_tokens(input: str):\n\t    encoder = tiktoken.get_encoding(\"cl100k_base\")\n\t    return len(encoder.encode(input))\n"]}
{"filename": "auto_llama/auto_llama/__init__.py", "chunked_list": []}
{"filename": "auto_llama/auto_llama/utils.py", "chunked_list": ["import datetime\n\timport json\n\tfrom auto_llama.data_models import Response\n\tdef get_date():\n\t    return datetime.datetime.now().strftime(\"%Y-%m-%d\")\n\tdef print_pretty(response: Response):\n\t    print(\"Thoughts: \" + response.thoughts + \"\\n\")\n\t    print(\"Remember: \" + response.remember + \"\\n\")\n\t    print(\"Reasoning: \" + response.reasoning + \"\\n\")\n\t    print(\"Plan: \" + json.dumps(response.plan) + \"\\n\")\n", "    print(\"Command: \" + response.command.toJSON() + \"\\n\")\n"]}
{"filename": "auto_llama/auto_llama/const.py", "chunked_list": ["DEFAULT_AGENT_PREAMBLE = \"\"\"\n\tI am an AI assistant with chain of thought reasoning that only responds in JSON.\n\tI should never respond with a natural language sentence.\n\tI may take the following actions with my response:\n\t1. Search the Web and obtain a list of web results.\n\t2. Download the contents of a web page and read its summary.\n\t3. Query the contents over one or more web pages in order to answer the user's request.\n\t4. Write results to a file.\n\tAll my responses should be in the following format and contain all the fields:\n\t{\n", "    \"remember\": This is what I just accomplished. I probably should not do it again,\n\t    \"thoughts\": This is what I'm thinking right now,\n\t    \"reasoning\": This is why I'm thinking it will help lead to the user's desired result,\n\t    \"plan\": This is a description of my current plan of actions,\n\t    \"command\": {\n\t        \"action\": My current action,\n\t        \"args\": [command_arg1, command_arg2, ...]\n\t    }\n\t}\n\tcommand_action should exclusively consist of these commands:\n", "{\"action\": \"search\", \"args\": {\"search_terms\": search_terms: str}}\n\t{\"action\": \"download\", \"args\": {\"url\": url: list[str], \"doc_name\": doc_name: list[str]}}\n\t{\"action\": \"query\", \"args\": {\"docs\": [doc_name1: str, doc_name2: str, ...], \"query\": query: str}}\n\t{\"action\": \"write\", \"args\": {\"file_name\": file_name: str, \"data\": data: str}}\n\t{\"action\": \"exit\"}\n\tIf you already got good search results, you should not need to search again.\n\t\"\"\"\n\tSEARCH_RESULTS_TEMPLATE = \"\"\"I searched for {search_terms} and found the following results.\n\tIf any of these results help to answer the user's query {user_query}\n\tI should respond with which web urls I should download and state I don't need\n", "more searching. Otherwise I should suggest different search terms.\"\"\"\n\tWEB_DOWNLOAD = (\n\t    \"\"\"Downloaded the contents of {url} to {doc_name}. To summarize: {summary}\"\"\"\n\t)\n\tdef format_web_download(url, doc_name, summary):\n\t    return WEB_DOWNLOAD.format(url=url, doc_name=doc_name, summary=summary)\n"]}
