{"filename": "endpoint.py", "chunked_list": ["from .utils import here, run_command, comfy_mode\n\tfrom aiohttp import web\n\tfrom .log import mklog\n\timport sys\n\tendlog = mklog(\"mtb endpoint\")\n\t# - ACTIONS\n\timport requirements\n\tdef ACTIONS_installDependency(dependency_names=None):\n\t    if dependency_names is None:\n\t        return {\"error\": \"No dependency name provided\"}\n", "    endlog.debug(f\"Received Install Dependency request for {dependency_names}\")\n\t    reqs = []\n\t    if comfy_mode == \"embeded\":\n\t        reqs = list(requirements.parse((here / \"reqs_portable.txt\").read_text()))\n\t    else:\n\t        reqs = list(requirements.parse((here / \"reqs.txt\").read_text()))\n\t    print([x.specs for x in reqs])\n\t    print(\n\t        \"\\n\".join([f\"{x.line} {''.join(x.specs[0] if x.specs else '')}\" for x in reqs])\n\t    )\n", "    for dependency_name in dependency_names:\n\t        for req in reqs:\n\t            if req.name == dependency_name:\n\t                endlog.debug(f\"Dependency {dependency_name} installed\")\n\t                break\n\t    return {\"success\": True}\n\tdef ACTIONS_getStyles(style_name=None):\n\t    from .nodes.conditions import StylesLoader\n\t    styles = StylesLoader.options\n\t    match_list = [\"name\"]\n", "    if styles:\n\t        filtered_styles = {\n\t            key: value\n\t            for key, value in styles.items()\n\t            if not key.startswith(\"__\") and key not in match_list\n\t        }\n\t        if style_name:\n\t            return filtered_styles.get(style_name, {\"error\": \"Style not found\"})\n\t        return filtered_styles\n\t    return {\"error\": \"No styles found\"}\n", "async def do_action(request) -> web.Response:\n\t    endlog.debug(\"Init action request\")\n\t    request_data = await request.json()\n\t    name = request_data.get(\"name\")\n\t    args = request_data.get(\"args\")\n\t    endlog.debug(f\"Received action request: {name} {args}\")\n\t    method_name = f\"ACTIONS_{name}\"\n\t    method = globals().get(method_name)\n\t    if callable(method):\n\t        result = method(args) if args else method()\n", "        endlog.debug(f\"Action result: {result}\")\n\t        return web.json_response({\"result\": result})\n\t    available_methods = [\n\t        attr[len(\"ACTIONS_\") :] for attr in globals() if attr.startswith(\"ACTIONS_\")\n\t    ]\n\t    return web.json_response(\n\t        {\"error\": \"Invalid method name.\", \"available_methods\": available_methods}\n\t    )\n\t# - HTML UTILS\n\tdef dependencies_button(name, dependencies):\n", "    deps = \",\".join([f\"'{x}'\" for x in dependencies])\n\t    return f\"\"\"\n\t        <button class=\"dependency-button\" onclick=\"window.mtb_action('installDependency',[{deps}])\">Install {name} deps</button>\n\t        \"\"\"\n\tdef render_table(table_dict, sort=True, title=None):\n\t    table_dict = sorted(\n\t        table_dict.items(), key=lambda item: item[0]\n\t    )  # Sort the dictionary by keys\n\t    table_rows = \"\"\n\t    for name, item in table_dict:\n", "        if isinstance(item, dict):\n\t            if \"dependencies\" in item:\n\t                table_rows += f\"<tr><td>{name}</td><td>\"\n\t                table_rows += f\"{dependencies_button(name,item['dependencies'])}\"\n\t                table_rows += \"</td></tr>\"\n\t            else:\n\t                table_rows += f\"<tr><td>{name}</td><td>{render_table(item)}</td></tr>\"\n\t        # elif isinstance(item, str):\n\t        #     table_rows += f\"<tr><td>{name}</td><td>{item}</td></tr>\"\n\t        else:\n", "            table_rows += f\"<tr><td>{name}</td><td>{item}</td></tr>\"\n\t    return f\"\"\"\n\t        <div class=\"table-container\">\n\t        {\"\" if title is None else f\"<h1>{title}</h1>\"}\n\t        <table>\n\t            <thead>\n\t                <tr>\n\t                    <th>Name</th>\n\t                    <th>Description</th>\n\t                </tr>\n", "            </thead>\n\t            <tbody>\n\t                {table_rows}\n\t            </tbody>\n\t        </table>      \n\t        </div>\n\t        \"\"\"\n\tdef render_base_template(title, content):\n\t    css_content = \"\"\n\t    css_path = here / \"html\" / \"style.css\"\n", "    if css_path:\n\t        with open(css_path, \"r\") as css_file:\n\t            css_content = css_file.read()\n\t    github_icon_svg = \"\"\"<svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"whitesmoke\" height=\"3em\" viewBox=\"0 0 496 512\"><path d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/></svg>\"\"\"\n\t    return f\"\"\"\n\t    <!DOCTYPE html>\n\t    <html>\n\t    <head>\n\t        <title>{title}</title>\n\t        <style>\n", "            {css_content}\n\t        </style>\n\t    </head>\n\t    <script type=\"module\">\n\t        import {{ api }} from '/scripts/api.js'\n\t        const mtb_action = async (action, args) =>{{\n\t            console.log(`Sending ${{action}} with args: ${{args}}`)\n\t            }}\n\t        window.mtb_action = async (action, args) =>{{\n\t            console.log(`Sending ${{action}} with args: ${{args}} to the API`)\n", "            const res = await api.fetchApi('/actions', {{\n\t                method: 'POST',\n\t                body: JSON.stringify({{\n\t                  name: action,\n\t                  args,\n\t                }}),\n\t            }})\n\t              const output = await res.json()\n\t              console.debug(`Received ${{action}} response:`, output)\n\t              if (output?.result?.error){{\n", "                  alert(`An error occured: {{output?.result?.error}}`)\n\t              }}\n\t              return output?.result\n\t        }}\n\t    </script>\n\t    <body>\n\t        <header>\n\t        <a href=\"/\">Back to Comfy</a>\n\t        <div class=\"mtb_logo\">\n\t            <img src=\"https://repository-images.githubusercontent.com/649047066/a3eef9a7-20dd-4ef9-b839-884502d4e873\" alt=\"Comfy MTB Logo\" height=\"70\" width=\"128\">\n", "            <span class=\"title\">Comfy MTB</span></div>\n\t            <a style=\"width:128px;text-align:center\" href=\"https://www.github.com/melmass/comfy_mtb\">\n\t                {github_icon_svg}\n\t            </a>\n\t        </header>\n\t        <main>\n\t            {content}\n\t        </main>\n\t        <footer>\n\t            <!-- Shared footer content here -->\n", "        </footer>\n\t    </body>\n\t    </html>\n\t    \"\"\"\n"]}
{"filename": "log.py", "chunked_list": ["import logging\n\timport re\n\timport os\n\tbase_log_level = logging.DEBUG if os.environ.get(\"MTB_DEBUG\") else logging.INFO\n\t# Custom object that discards the output\n\tclass NullWriter:\n\t    def write(self, text):\n\t        pass\n\tclass Formatter(logging.Formatter):\n\t    grey = \"\\x1b[38;20m\"\n", "    cyan = \"\\x1b[36;20m\"\n\t    purple = \"\\x1b[35;20m\"\n\t    yellow = \"\\x1b[33;20m\"\n\t    red = \"\\x1b[31;20m\"\n\t    bold_red = \"\\x1b[31;1m\"\n\t    reset = \"\\x1b[0m\"\n\t    # format = \"%(asctime)s - [%(name)s] - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n\t    format = \"[%(name)s] | %(levelname)s -> %(message)s\"\n\t    FORMATS = {\n\t        logging.DEBUG: purple + format + reset,\n", "        logging.INFO: cyan + format + reset,\n\t        logging.WARNING: yellow + format + reset,\n\t        logging.ERROR: red + format + reset,\n\t        logging.CRITICAL: bold_red + format + reset,\n\t    }\n\t    def format(self, record):\n\t        log_fmt = self.FORMATS.get(record.levelno)\n\t        formatter = logging.Formatter(log_fmt)\n\t        return formatter.format(record)\n\tdef mklog(name, level=base_log_level):\n", "    logger = logging.getLogger(name)\n\t    logger.setLevel(level)\n\t    for handler in logger.handlers:\n\t        logger.removeHandler(handler)\n\t    ch = logging.StreamHandler()\n\t    ch.setLevel(level)\n\t    ch.setFormatter(Formatter())\n\t    logger.addHandler(ch)\n\t    # Disable log propagation\n\t    logger.propagate = False\n", "    return logger\n\t# - The main app logger\n\tlog = mklog(__package__, base_log_level)\n\tdef log_user(arg):\n\t    print(\"\\033[34mComfy MTB Utils:\\033[0m {arg}\")\n\tdef get_summary(docstring):\n\t    return docstring.strip().split(\"\\n\\n\", 1)[0]\n\tdef blue_text(text):\n\t    return f\"\\033[94m{text}\\033[0m\"\n\tdef cyan_text(text):\n", "    return f\"\\033[96m{text}\\033[0m\"\n\tdef get_label(label):\n\t    words = re.findall(r\"(?:^|[A-Z])[a-z]*\", label)\n\t    return \" \".join(words).strip()\n"]}
{"filename": "__init__.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding:utf-8 -*-\n\t###\n\t# File: __init__.py\n\t# Project: comfy_mtb\n\t# Author: Mel Massadian\n\t# Copyright (c) 2023 Mel Massadian\n\t#\n\t###\n\timport os\n", "# todo: don't override this if the user has that setup already\n\tos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\tos.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n\timport traceback\n\tfrom .log import log, blue_text, cyan_text, get_summary, get_label\n\tfrom .utils import here\n\tfrom .utils import comfy_dir\n\timport importlib\n\timport os\n\timport ast\n", "import json\n\tNODE_CLASS_MAPPINGS = {}\n\tNODE_DISPLAY_NAME_MAPPINGS = {}\n\tNODE_CLASS_MAPPINGS_DEBUG = {}\n\t__version__ = \"0.1.4\"\n\tdef extract_nodes_from_source(filename):\n\t    source_code = \"\"\n\t    with open(filename, \"r\") as file:\n\t        source_code = file.read()\n\t    nodes = []\n", "    try:\n\t        parsed = ast.parse(source_code)\n\t        for node in ast.walk(parsed):\n\t            if isinstance(node, ast.Assign) and len(node.targets) == 1:\n\t                target = node.targets[0]\n\t                if isinstance(target, ast.Name) and target.id == \"__nodes__\":\n\t                    value = ast.get_source_segment(source_code, node.value)\n\t                    node_value = ast.parse(value).body[0].value\n\t                    if isinstance(node_value, (ast.List, ast.Tuple)):\n\t                        nodes.extend(\n", "                            element.id\n\t                            for element in node_value.elts\n\t                            if isinstance(element, ast.Name)\n\t                        )\n\t                    break\n\t    except SyntaxError:\n\t        log.error(\"Failed to parse\")\n\t    return nodes\n\tdef load_nodes():\n\t    errors = []\n", "    nodes = []\n\t    nodes_failed = []\n\t    for filename in (here / \"nodes\").iterdir():\n\t        if filename.suffix == \".py\":\n\t            module_name = filename.stem\n\t            try:\n\t                module = importlib.import_module(\n\t                    f\".nodes.{module_name}\", package=__package__\n\t                )\n\t                _nodes = getattr(module, \"__nodes__\")\n", "                nodes.extend(_nodes)\n\t                log.debug(f\"Imported {module_name} nodes\")\n\t            except AttributeError:\n\t                pass  # wip nodes\n\t            except Exception:\n\t                error_message = traceback.format_exc().splitlines()[-1]\n\t                errors.append(\n\t                    f\"Failed to import module {module_name} because {error_message}\"\n\t                )\n\t                # Read __nodes__ variable from the source file\n", "                nodes_failed.extend(extract_nodes_from_source(filename))\n\t    if errors:\n\t        log.info(\n\t            f\"Some nodes failed to load:\\n\\t\"\n\t            + \"\\n\\t\".join(errors)\n\t            + \"\\n\\n\"\n\t            + \"Check that you properly installed the dependencies.\\n\"\n\t            + \"If you think this is a bug, please report it on the github page (https://github.com/melMass/comfy_mtb/issues)\"\n\t        )\n\t    return (nodes, nodes_failed)\n", "# - REGISTER WEB EXTENSIONS\n\tweb_extensions_root = comfy_dir / \"web\" / \"extensions\"\n\tweb_mtb = web_extensions_root / \"mtb\"\n\tif web_mtb.exists():\n\t    log.debug(f\"Web extensions folder found at {web_mtb}\")\n\t    if not os.path.islink(web_mtb.as_posix()):\n\t        log.warn(\n\t            f\"Web extensions folder at {web_mtb} is not a symlink, if updating please delete it before\"\n\t        )\n\telif web_extensions_root.exists():\n", "    web_tgt = here / \"web\"\n\t    src = web_tgt.as_posix()\n\t    dst = web_mtb.as_posix()\n\t    try:\n\t        if os.name == \"nt\":\n\t            import _winapi\n\t            _winapi.CreateJunction(src, dst)\n\t        else:\n\t            os.symlink(web_tgt.as_posix(), web_mtb.as_posix())\n\t    except OSError:\n", "        log.warn(f\"Failed to create symlink to {web_mtb}, trying to copy it\")\n\t        try:\n\t            import shutil\n\t            shutil.copytree(web_tgt, web_mtb)\n\t            log.info(f\"Successfully copied {web_tgt} to {web_mtb}\")\n\t        except Exception as e:\n\t            log.warn(\n\t                f\"Failed to symlink and copy {web_tgt} to {web_mtb}. Please copy the folder manually.\"\n\t            )\n\t            log.warn(e)\n", "    except Exception as e:\n\t        log.warn(\n\t            f\"Failed to create symlink to {web_mtb}. Please copy the folder manually.\"\n\t        )\n\t        log.warn(e)\n\telse:\n\t    log.warn(\n\t        f\"Comfy root probably not found automatically, please copy the folder {web_mtb} manually in the web/extensions folder of ComfyUI\"\n\t    )\n\t# - REGISTER NODES\n", "nodes, failed = load_nodes()\n\tfor node_class in nodes:\n\t    class_name = node_class.__name__\n\t    node_label = f\"{get_label(class_name)} (mtb)\"\n\t    NODE_CLASS_MAPPINGS[node_label] = node_class\n\t    NODE_DISPLAY_NAME_MAPPINGS[class_name] = node_label\n\t    NODE_CLASS_MAPPINGS_DEBUG[node_label] = node_class.__doc__\n\t    # TODO: I removed this, I find it more convenient to write without spaces, but it breaks every of my workflows\n\t    # TODO (cont): and until I find a way to automate the conversion, I'll leave it like this\n\t    if os.environ.get(\"MTB_EXPORT\"):\n", "        with open(here / \"node_list.json\", \"w\") as f:\n\t            f.write(\n\t                json.dumps(\n\t                    {\n\t                        k: NODE_CLASS_MAPPINGS_DEBUG[k]\n\t                        for k in sorted(NODE_CLASS_MAPPINGS_DEBUG.keys())\n\t                    },\n\t                    indent=4,\n\t                )\n\t            )\n", "log.info(\n\t    f\"Loaded the following nodes:\\n\\t\"\n\t    + \"\\n\\t\".join(\n\t        f\"{cyan_text(k)}: {blue_text(get_summary(doc)) if doc else '-'}\"\n\t        for k, doc in NODE_CLASS_MAPPINGS_DEBUG.items()\n\t    )\n\t)\n\t# - ENDPOINT\n\tfrom server import PromptServer\n\tfrom .log import log\n", "from aiohttp import web\n\tfrom importlib import reload\n\timport logging\n\tfrom .endpoint import endlog\n\tif hasattr(PromptServer, \"instance\"):\n\t    restore_deps = [\"basicsr\"]\n\t    swap_deps = [\"insightface\", \"onnxruntime\"]\n\t    node_dependency_mapping = {\n\t        \"FaceSwap\": swap_deps,\n\t        \"LoadFaceSwapModel\": swap_deps,\n", "        \"LoadFaceAnalysisModel\": restore_deps,\n\t    }\n\t    @PromptServer.instance.routes.get(\"/mtb/status\")\n\t    async def get_full_library(request):\n\t        from . import endpoint\n\t        reload(endpoint)\n\t        endlog.debug(\"Getting node registration status\")\n\t        # Check if the request prefers HTML content\n\t        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n\t            # # Return an HTML page\n", "            html_response = endpoint.render_table(\n\t                NODE_CLASS_MAPPINGS_DEBUG, title=\"Registered\"\n\t            )\n\t            html_response += endpoint.render_table(\n\t                {\n\t                    k: {\"dependencies\": node_dependency_mapping.get(k)}\n\t                    if node_dependency_mapping.get(k)\n\t                    else \"-\"\n\t                    for k in failed\n\t                },\n", "                title=\"Failed to load\",\n\t            )\n\t            return web.Response(\n\t                text=endpoint.render_base_template(\"MTB\", html_response),\n\t                content_type=\"text/html\",\n\t            )\n\t        return web.json_response(\n\t            {\n\t                \"registered\": NODE_CLASS_MAPPINGS_DEBUG,\n\t                \"failed\": failed,\n", "            }\n\t        )\n\t    @PromptServer.instance.routes.post(\"/mtb/debug\")\n\t    async def set_debug(request):\n\t        json_data = await request.json()\n\t        enabled = json_data.get(\"enabled\")\n\t        if enabled:\n\t            os.environ[\"MTB_DEBUG\"] = \"true\"\n\t            log.setLevel(logging.DEBUG)\n\t            log.debug(\"Debug mode set from API (/mtb/debug POST route)\")\n", "        elif \"MTB_DEBUG\" in os.environ:\n\t            # del os.environ[\"MTB_DEBUG\"]\n\t            os.environ.pop(\"MTB_DEBUG\")\n\t            log.setLevel(logging.INFO)\n\t        return web.json_response(\n\t            {\"message\": f\"Debug mode {'set' if enabled else 'unset'}\"}\n\t        )\n\t    @PromptServer.instance.routes.get(\"/mtb\")\n\t    async def get_home(request):\n\t        from . import endpoint\n", "        reload(endpoint)\n\t        # Check if the request prefers HTML content\n\t        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n\t            # # Return an HTML page\n\t            html_response = \"\"\"\n\t            <div class=\"flex-container menu\">\n\t                <a href=\"/mtb/debug\">debug</a>\n\t                <a href=\"/mtb/status\">status</a>\n\t            </div>            \n\t            \"\"\"\n", "            return web.Response(\n\t                text=endpoint.render_base_template(\"MTB\", html_response),\n\t                content_type=\"text/html\",\n\t            )\n\t        # Return JSON for other requests\n\t        return web.json_response({\"message\": \"Welcome to MTB!\"})\n\t    @PromptServer.instance.routes.get(\"/mtb/debug\")\n\t    async def get_debug(request):\n\t        from . import endpoint\n\t        reload(endpoint)\n", "        enabled = \"MTB_DEBUG\" in os.environ\n\t        # Check if the request prefers HTML content\n\t        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n\t            # # Return an HTML page\n\t            html_response = f\"\"\"\n\t                <h1>MTB Debug Status: {'Enabled' if enabled else 'Disabled'}</h1>\n\t            \"\"\"\n\t            return web.Response(\n\t                text=endpoint.render_base_template(\"Debug\", html_response),\n\t                content_type=\"text/html\",\n", "            )\n\t        # Return JSON for other requests\n\t        return web.json_response({\"enabled\": enabled})\n\t    @PromptServer.instance.routes.get(\"/mtb/actions\")\n\t    async def no_route(request):\n\t        from . import endpoint\n\t        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n\t            html_response = \"\"\"\n\t            <h1>Actions has no get for now...</h1>\n\t            \"\"\"\n", "            return web.Response(\n\t                text=endpoint.render_base_template(\"Actions\", html_response),\n\t                content_type=\"text/html\",\n\t            )\n\t        return web.json_response({\"message\": \"actions has no get for now\"})\n\t    @PromptServer.instance.routes.post(\"/mtb/actions\")\n\t    async def do_action(request):\n\t        from . import endpoint\n\t        reload(endpoint)\n\t        return await endpoint.do_action(request)\n", "# - WAS Dictionary\n\tMANIFEST = {\n\t    \"name\": \"MTB Nodes\",  # The title that will be displayed on Node Class menu,. and Node Class view\n\t    \"version\": (0, 1, 0),  # Version of the custom_node or sub module\n\t    \"author\": \"Mel Massadian\",  # Author or organization of the custom_node or sub module\n\t    \"project\": \"https://github.com/melMass/comfy_mtb\",  # The address that the `name` value will link to on Node Class Views\n\t    \"description\": \"Set of nodes that enhance your animation workflow and provide a range of useful tools including features such as manipulating bounding boxes, perform color corrections, swap faces in images, interpolate frames for smooth animation, export to ProRes format, apply various image operations, work with latent spaces, generate QR codes, and create normal and height maps for textures.\",\n\t}\n"]}
{"filename": "install.py", "chunked_list": ["import requests\n\timport os\n\timport ast\n\timport argparse\n\timport sys\n\timport subprocess\n\tfrom importlib import import_module\n\timport platform\n\tfrom pathlib import Path\n\timport sys\n", "import stat\n\timport threading\n\timport signal\n\tfrom contextlib import suppress\n\tfrom queue import Queue, Empty\n\tfrom contextlib import contextmanager\n\there = Path(__file__).parent\n\texecutable = sys.executable\n\t# - detect mode\n\tmode = None\n", "if os.environ.get(\"COLAB_GPU\"):\n\t    mode = \"colab\"\n\telif \"python_embeded\" in executable:\n\t    mode = \"embeded\"\n\telif \".venv\" in executable:\n\t    mode = \"venv\"\n\tif mode is None:\n\t    mode = \"unknown\"\n\t# - Constants\n\trepo_url = \"https://github.com/melmass/comfy_mtb.git\"\n", "repo_owner = \"melmass\"\n\trepo_name = \"comfy_mtb\"\n\tshort_platform = {\n\t    \"windows\": \"win_amd64\",\n\t    \"linux\": \"linux_x86_64\",\n\t}\n\tcurrent_platform = platform.system().lower()\n\t# region ansi\n\t# ANSI escape sequences for text styling\n\tANSI_FORMATS = {\n", "    \"reset\": \"\\033[0m\",\n\t    \"bold\": \"\\033[1m\",\n\t    \"dim\": \"\\033[2m\",\n\t    \"italic\": \"\\033[3m\",\n\t    \"underline\": \"\\033[4m\",\n\t    \"blink\": \"\\033[5m\",\n\t    \"reverse\": \"\\033[7m\",\n\t    \"strike\": \"\\033[9m\",\n\t}\n\tANSI_COLORS = {\n", "    \"black\": \"\\033[30m\",\n\t    \"red\": \"\\033[31m\",\n\t    \"green\": \"\\033[32m\",\n\t    \"yellow\": \"\\033[33m\",\n\t    \"blue\": \"\\033[34m\",\n\t    \"magenta\": \"\\033[35m\",\n\t    \"cyan\": \"\\033[36m\",\n\t    \"white\": \"\\033[37m\",\n\t    \"bright_black\": \"\\033[30;1m\",\n\t    \"bright_red\": \"\\033[31;1m\",\n", "    \"bright_green\": \"\\033[32;1m\",\n\t    \"bright_yellow\": \"\\033[33;1m\",\n\t    \"bright_blue\": \"\\033[34;1m\",\n\t    \"bright_magenta\": \"\\033[35;1m\",\n\t    \"bright_cyan\": \"\\033[36;1m\",\n\t    \"bright_white\": \"\\033[37;1m\",\n\t    \"bg_black\": \"\\033[40m\",\n\t    \"bg_red\": \"\\033[41m\",\n\t    \"bg_green\": \"\\033[42m\",\n\t    \"bg_yellow\": \"\\033[43m\",\n", "    \"bg_blue\": \"\\033[44m\",\n\t    \"bg_magenta\": \"\\033[45m\",\n\t    \"bg_cyan\": \"\\033[46m\",\n\t    \"bg_white\": \"\\033[47m\",\n\t    \"bg_bright_black\": \"\\033[40;1m\",\n\t    \"bg_bright_red\": \"\\033[41;1m\",\n\t    \"bg_bright_green\": \"\\033[42;1m\",\n\t    \"bg_bright_yellow\": \"\\033[43;1m\",\n\t    \"bg_bright_blue\": \"\\033[44;1m\",\n\t    \"bg_bright_magenta\": \"\\033[45;1m\",\n", "    \"bg_bright_cyan\": \"\\033[46;1m\",\n\t    \"bg_bright_white\": \"\\033[47;1m\",\n\t}\n\tdef apply_format(text, *formats):\n\t    \"\"\"Apply ANSI escape sequences for the specified formats to the given text.\"\"\"\n\t    formatted_text = text\n\t    for format in formats:\n\t        formatted_text = f\"{ANSI_FORMATS.get(format, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n\t    return formatted_text\n\tdef apply_color(text, color=None, background=None):\n", "    \"\"\"Apply ANSI escape sequences for the specified color and background to the given text.\"\"\"\n\t    formatted_text = text\n\t    if color:\n\t        formatted_text = f\"{ANSI_COLORS.get(color, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n\t    if background:\n\t        formatted_text = f\"{ANSI_COLORS.get(background, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n\t    return formatted_text\n\tdef print_formatted(text, *formats, color=None, background=None, **kwargs):\n\t    \"\"\"Print the given text with the specified formats, color, and background.\"\"\"\n\t    formatted_text = apply_format(text, *formats)\n", "    formatted_text = apply_color(formatted_text, color, background)\n\t    file = kwargs.get(\"file\", sys.stdout)\n\t    header = \"[mtb install] \"\n\t    # Handle console encoding for Unicode characters (utf-8)\n\t    encoded_header = header.encode(sys.stdout.encoding, errors=\"replace\").decode(\n\t        sys.stdout.encoding\n\t    )\n\t    encoded_text = formatted_text.encode(sys.stdout.encoding, errors=\"replace\").decode(\n\t        sys.stdout.encoding\n\t    )\n", "    print(\n\t        \" \" * len(encoded_header)\n\t        if kwargs.get(\"no_header\")\n\t        else apply_color(apply_format(encoded_header, \"bold\"), color=\"yellow\"),\n\t        encoded_text,\n\t        file=file,\n\t    )\n\t# endregion\n\t# region utils\n\tdef enqueue_output(out, queue):\n", "    for char in iter(lambda: out.read(1), b\"\"):\n\t        queue.put(char)\n\t    out.close()\n\tdef run_command(cmd, ignored_lines_start=None):\n\t    if ignored_lines_start is None:\n\t        ignored_lines_start = []\n\t    if isinstance(cmd, str):\n\t        shell_cmd = cmd\n\t    elif isinstance(cmd, list):\n\t        shell_cmd = \"\"\n", "        for arg in cmd:\n\t            if isinstance(arg, Path):\n\t                arg = arg.as_posix()\n\t            shell_cmd += f\"{arg} \"\n\t    else:\n\t        raise ValueError(\n\t            \"Invalid 'cmd' argument. It must be a string or a list of arguments.\"\n\t        )\n\t    process = subprocess.Popen(\n\t        shell_cmd,\n", "        stdout=subprocess.PIPE,\n\t        stderr=subprocess.PIPE,\n\t        universal_newlines=True,\n\t        shell=True,\n\t    )\n\t    # Create separate threads to read standard output and standard error streams\n\t    stdout_queue = Queue()\n\t    stderr_queue = Queue()\n\t    stdout_thread = threading.Thread(\n\t        target=enqueue_output, args=(process.stdout, stdout_queue)\n", "    )\n\t    stderr_thread = threading.Thread(\n\t        target=enqueue_output, args=(process.stderr, stderr_queue)\n\t    )\n\t    stdout_thread.daemon = True\n\t    stderr_thread.daemon = True\n\t    stdout_thread.start()\n\t    stderr_thread.start()\n\t    interrupted = False\n\t    def signal_handler(signum, frame):\n", "        nonlocal interrupted\n\t        interrupted = True\n\t        print(\"Command execution interrupted.\")\n\t    # Register the signal handler for keyboard interrupts (SIGINT)\n\t    signal.signal(signal.SIGINT, signal_handler)\n\t    stdout_buffer = \"\"\n\t    stderr_buffer = \"\"\n\t    # Process output from both streams until the process completes or interrupted\n\t    while not interrupted and (\n\t        process.poll() is None or not stdout_queue.empty() or not stderr_queue.empty()\n", "    ):\n\t        with suppress(Empty):\n\t            stdout_char = stdout_queue.get_nowait()\n\t            stdout_buffer += stdout_char\n\t            if stdout_char == \"\\n\":\n\t                if not any(\n\t                    stdout_buffer.startswith(ign) for ign in ignored_lines_start\n\t                ):\n\t                    print(stdout_buffer.strip())\n\t                stdout_buffer = \"\"\n", "        with suppress(Empty):\n\t            stderr_char = stderr_queue.get_nowait()\n\t            stderr_buffer += stderr_char\n\t            if stderr_char == \"\\n\":\n\t                print(stderr_buffer.strip())\n\t                stderr_buffer = \"\"\n\t    # Print any remaining content in buffers\n\t    if stdout_buffer and not any(\n\t        stdout_buffer.startswith(ign) for ign in ignored_lines_start\n\t    ):\n", "        print(stdout_buffer.strip())\n\t    if stderr_buffer:\n\t        print(stderr_buffer.strip())\n\t    return_code = process.returncode\n\t    if return_code == 0 and not interrupted:\n\t        print(\"Command executed successfully!\")\n\t    else:\n\t        if not interrupted:\n\t            print(f\"Command failed with return code: {return_code}\")\n\t# endregion\n", "try:\n\t    import requirements\n\texcept ImportError:\n\t    print_formatted(\"Installing requirements-parser...\", \"italic\", color=\"yellow\")\n\t    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"requirements-parser\"])\n\t    import requirements\n\t    print_formatted(\"Done.\", \"italic\", color=\"green\")\n\ttry:\n\t    from tqdm import tqdm\n\texcept ImportError:\n", "    print_formatted(\"Installing tqdm...\", \"italic\", color=\"yellow\")\n\t    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"tqdm\"])\n\t    from tqdm import tqdm\n\tpip_map = {\n\t    \"onnxruntime-gpu\": \"onnxruntime\",\n\t    \"opencv-contrib\": \"cv2\",\n\t    \"tb-nightly\": \"tensorboard\",\n\t    \"protobuf\": \"google.protobuf\",\n\t    # Add more mappings as needed\n\t}\n", "def is_pipe():\n\t    if not sys.stdin.isatty():\n\t        return False\n\t    if sys.platform == \"win32\":\n\t        try:\n\t            import msvcrt\n\t            return msvcrt.get_osfhandle(0) != -1\n\t        except ImportError:\n\t            return False\n\t    else:\n", "        try:\n\t            mode = os.fstat(0).st_mode\n\t            return (\n\t                stat.S_ISFIFO(mode)\n\t                or stat.S_ISREG(mode)\n\t                or stat.S_ISBLK(mode)\n\t                or stat.S_ISSOCK(mode)\n\t            )\n\t        except OSError:\n\t            return False\n", "@contextmanager\n\tdef suppress_std():\n\t    with open(os.devnull, \"w\") as devnull:\n\t        old_stdout = sys.stdout\n\t        old_stderr = sys.stderr\n\t        sys.stdout = devnull\n\t        sys.stderr = devnull\n\t        try:\n\t            yield\n\t        finally:\n", "            sys.stdout = old_stdout\n\t            sys.stderr = old_stderr\n\t# Get the version from __init__.py\n\tdef get_local_version():\n\t    init_file = os.path.join(os.path.dirname(__file__), \"__init__.py\")\n\t    if os.path.isfile(init_file):\n\t        with open(init_file, \"r\") as f:\n\t            tree = ast.parse(f.read())\n\t            for node in ast.walk(tree):\n\t                if isinstance(node, ast.Assign):\n", "                    for target in node.targets:\n\t                        if (\n\t                            isinstance(target, ast.Name)\n\t                            and target.id == \"__version__\"\n\t                            and isinstance(node.value, ast.Str)\n\t                        ):\n\t                            return node.value.s\n\t    return None\n\tdef download_file(url, file_name):\n\t    with requests.get(url, stream=True) as response:\n", "        response.raise_for_status()\n\t        total_size = int(response.headers.get(\"content-length\", 0))\n\t        with open(file_name, \"wb\") as file, tqdm(\n\t            desc=file_name.stem,\n\t            total=total_size,\n\t            unit=\"B\",\n\t            unit_scale=True,\n\t            unit_divisor=1024,\n\t        ) as progress_bar:\n\t            for chunk in response.iter_content(chunk_size=8192):\n", "                file.write(chunk)\n\t                progress_bar.update(len(chunk))\n\tdef get_requirements(path: Path):\n\t    with open(path.resolve(), \"r\") as requirements_file:\n\t        requirements_txt = requirements_file.read()\n\t    try:\n\t        parsed_requirements = requirements.parse(requirements_txt)\n\t    except AttributeError:\n\t        print_formatted(\n\t            f\"Failed to parse {path}. Please make sure the file is correctly formatted.\",\n", "            \"bold\",\n\t            color=\"red\",\n\t        )\n\t        return\n\t    return parsed_requirements\n\tdef try_import(requirement):\n\t    dependency = requirement.name.strip()\n\t    import_name = pip_map.get(dependency, dependency)\n\t    installed = False\n\t    pip_name = dependency\n", "    pip_spec = \"\".join(specs[0]) if (specs := requirement.specs) else \"\"\n\t    try:\n\t        with suppress_std():\n\t            import_module(import_name)\n\t        print_formatted(\n\t            f\"\\t✅ Package {pip_name} already installed (import name: '{import_name}').\",\n\t            \"bold\",\n\t            color=\"green\",\n\t            no_header=True,\n\t        )\n", "        installed = True\n\t    except ImportError:\n\t        print_formatted(\n\t            f\"\\t⛔ Package {pip_name} is missing (import name: '{import_name}').\",\n\t            \"bold\",\n\t            color=\"red\",\n\t            no_header=True,\n\t        )\n\t    return (installed, pip_name, pip_spec, import_name)\n\tdef import_or_install(requirement, dry=False):\n", "    installed, pip_name, pip_spec, import_name = try_import(requirement)\n\t    pip_install_name = pip_name + pip_spec\n\t    if not installed:\n\t        print_formatted(f\"Installing package {pip_name}...\", \"italic\", color=\"yellow\")\n\t        if dry:\n\t            print_formatted(\n\t                f\"Dry-run: Package {pip_install_name} would be installed (import name: '{import_name}').\",\n\t                color=\"yellow\",\n\t            )\n\t        else:\n", "            try:\n\t                run_command([sys.executable, \"-m\", \"pip\", \"install\", pip_install_name])\n\t                print_formatted(\n\t                    f\"Package {pip_install_name} installed successfully using pip package name  (import name: '{import_name}')\",\n\t                    \"bold\",\n\t                    color=\"green\",\n\t                )\n\t            except subprocess.CalledProcessError as e:\n\t                print_formatted(\n\t                    f\"Failed to install package {pip_install_name} using pip package name  (import name: '{import_name}'). Error: {str(e)}\",\n", "                    \"bold\",\n\t                    color=\"red\",\n\t                )\n\tdef get_github_assets(tag=None):\n\t    if tag:\n\t        tag_url = (\n\t            f\"https://api.github.com/repos/{repo_owner}/{repo_name}/releases/tags/{tag}\"\n\t        )\n\t    else:\n\t        tag_url = (\n", "            f\"https://api.github.com/repos/{repo_owner}/{repo_name}/releases/latest\"\n\t        )\n\t    response = requests.get(tag_url)\n\t    if response.status_code == 404:\n\t        # print_formatted(\n\t        #     f\"Tag version '{apply_color(version,'cyan')}' not found for {owner}/{repo} repository.\"\n\t        # )\n\t        print_formatted(\"Error retrieving the release assets.\", color=\"red\")\n\t        sys.exit()\n\t    tag_data = response.json()\n", "    tag_name = tag_data[\"name\"]\n\t    return tag_data, tag_name\n\t# Install dependencies from requirements.txt\n\tdef install_dependencies(dry=False):\n\t    parsed_requirements = get_requirements(here / \"reqs.txt\")\n\t    if not parsed_requirements:\n\t        return\n\t    print_formatted(\n\t        \"Installing dependencies from reqs.txt...\", \"italic\", color=\"yellow\"\n\t    )\n", "    for requirement in parsed_requirements:\n\t        import_or_install(requirement, dry=dry)\n\tif __name__ == \"__main__\":\n\t    full = False\n\t    if len(sys.argv) == 1:\n\t        print_formatted(\n\t            \"No arguments provided, doing a full install/update...\",\n\t            \"italic\",\n\t            color=\"yellow\",\n\t        )\n", "        full = True\n\t    # Parse command-line arguments\n\t    parser = argparse.ArgumentParser(description=\"Comfy_mtb install script\")\n\t    parser.add_argument(\n\t        \"--path\",\n\t        \"-p\",\n\t        type=str,\n\t        help=\"Path to clone the repository to (i.e the absolute path to ComfyUI/custom_nodes)\",\n\t    )\n\t    parser.add_argument(\n", "        \"--wheels\", \"-w\", action=\"store_true\", help=\"Install wheel dependencies\"\n\t    )\n\t    parser.add_argument(\n\t        \"--requirements\", \"-r\", action=\"store_true\", help=\"Install requirements.txt\"\n\t    )\n\t    parser.add_argument(\n\t        \"--dry\",\n\t        action=\"store_true\",\n\t        help=\"Print what will happen without doing it (still making requests to the GH Api)\",\n\t    )\n", "    # - keep\n\t    # parser.add_argument(\n\t    #     \"--version\",\n\t    #     default=get_local_version(),\n\t    #     help=\"Version to check against the GitHub API\",\n\t    # )\n\t    print_formatted(\"mtb install\", \"bold\", color=\"yellow\")\n\t    args = parser.parse_args()\n\t    # wheels_directory = here / \"wheels\"\n\t    print_formatted(f\"Detected environment: {apply_color(mode,'cyan')}\")\n", "    if args.path:\n\t        clone_dir = Path(args.path)\n\t        if not clone_dir.exists():\n\t            print_formatted(\n\t                \"The path provided does not exist on disk... It must be pointing to ComfyUI's custom_nodes directory\"\n\t            )\n\t            sys.exit()\n\t        else:\n\t            repo_dir = clone_dir / repo_name\n\t            if not repo_dir.exists():\n", "                print_formatted(f\"Cloning to {repo_dir}...\", \"italic\", color=\"yellow\")\n\t                run_command([\"git\", \"clone\", \"--recursive\", repo_url, repo_dir])\n\t            else:\n\t                print_formatted(\n\t                    f\"Directory {repo_dir} already exists, we will update it...\"\n\t                )\n\t                run_command([\"git\", \"pull\", \"-C\", repo_dir])\n\t        # os.chdir(clone_dir)\n\t        here = clone_dir\n\t        full = True\n", "    # Install dependencies from requirements.txt\n\t    # if args.requirements or mode == \"venv\":\n\t    # if (not args.wheels and mode not in [\"colab\", \"embeded\"]) and not full:\n\t    #     print_formatted(\n\t    #         \"Skipping wheel installation. Use --wheels to install wheel dependencies. (only needed for Comfy embed)\",\n\t    #         \"italic\",\n\t    #         color=\"yellow\",\n\t    #     )\n\t    #     install_dependencies(dry=args.dry)\n\t    #     sys.exit()\n", "    # if mode in [\"colab\", \"embeded\"]:\n\t    #     print_formatted(\n\t    #         f\"Downloading and installing release wheels since we are in a Comfy {apply_color(mode,'cyan')} environment\",\n\t    #         \"italic\",\n\t    #         color=\"yellow\",\n\t    #     )\n\t    # if full:\n\t    #     print_formatted(\n\t    #         f\"Downloading and installing release wheels since no arguments where provided\",\n\t    #         \"italic\",\n", "    #         color=\"yellow\",\n\t    #     )\n\t    print_formatted(\"Checking environment...\", \"italic\", color=\"yellow\")\n\t    missing_deps = []\n\t    if parsed_requirements := get_requirements(here / \"reqs.txt\"):\n\t        for requirement in parsed_requirements:\n\t            installed, pip_name, pip_spec, import_name = try_import(requirement)\n\t            if not installed:\n\t                missing_deps.append(pip_name.split(\"-\")[0])\n\t    if not missing_deps:\n", "        print_formatted(\n\t            \"All requirements are already installed. Enjoy 🚀\",\n\t            \"italic\",\n\t            color=\"green\",\n\t        )\n\t        sys.exit()\n\t    # # - Get the tag version from the GitHub API\n\t    # tag_data, tag_name = get_github_assets(tag=None)\n\t    # # - keep\n\t    # version = args.version\n", "    # # Compare the local and tag versions\n\t    # if version and tag_name:\n\t    #     if re.match(r\"v?(\\d+(\\.\\d+)+)\", version) and re.match(\n\t    #         r\"v?(\\d+(\\.\\d+)+)\", tag_name\n\t    #     ):\n\t    #         version_parts = [int(part) for part in version.lstrip(\"v\").split(\".\")]\n\t    #         tag_version_parts = [int(part) for part in tag_name.lstrip(\"v\").split(\".\")]\n\t    #         if version_parts > tag_version_parts:\n\t    #             print_formatted(\n\t    #                 f\"Local version ({version}) is greater than the release version ({tag_name}).\",\n", "    #                 \"bold\",\n\t    #                 \"yellow\",\n\t    #             )\n\t    #             sys.exit()\n\t    # matching_assets = [\n\t    #     asset\n\t    #     for asset in tag_data[\"assets\"]\n\t    #     if asset[\"name\"].endswith(\".whl\")\n\t    #     and (\n\t    #         \"any\" in asset[\"name\"] or short_platform[current_platform] in asset[\"name\"]\n", "    #     )\n\t    # ]\n\t    # if not matching_assets:\n\t    #     print_formatted(\n\t    #         f\"Unsupported operating system: {current_platform}\", color=\"yellow\"\n\t    #     )\n\t    # wheel_order_asset = next(\n\t    #     (asset for asset in tag_data[\"assets\"] if asset[\"name\"] == \"wheel_order.txt\"),\n\t    #     None,\n\t    # )\n", "    # if wheel_order_asset is not None:\n\t    #     print_formatted(\n\t    #         \"⚙️ Sorting the release wheels using wheels order\", \"italic\", color=\"yellow\"\n\t    #     )\n\t    #     response = requests.get(wheel_order_asset[\"browser_download_url\"])\n\t    #     if response.status_code == 200:\n\t    #         wheel_order = [line.strip() for line in response.text.splitlines()]\n\t    #         def get_order_index(val):\n\t    #             try:\n\t    #                 return wheel_order.index(val)\n", "    #             except ValueError:\n\t    #                 return len(wheel_order)\n\t    #         matching_assets = sorted(\n\t    #             matching_assets,\n\t    #             key=lambda x: get_order_index(x[\"name\"].split(\"-\")[0]),\n\t    #         )\n\t    #     else:\n\t    #         print(\"Failed to fetch wheel_order.txt. Status code:\", response.status_code)\n\t    # missing_deps_urls = []\n\t    # for whl_file in matching_assets:\n", "    #     # check if installed\n\t    #     missing_deps_urls.append(whl_file[\"browser_download_url\"])\n\t    install_cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n\t    # - Install all deps\n\t    if not args.dry:\n\t        if platform.system() == \"Windows\":\n\t            wheel_cmd = install_cmd + [\"-r\", (here / \"reqs_windows.txt\")]\n\t        else:\n\t            wheel_cmd = install_cmd + [\"-r\", (here / \"reqs.txt\")]\n\t        run_command(wheel_cmd)\n", "        print_formatted(\n\t            \"✅ Successfully installed all dependencies.\", \"italic\", color=\"green\"\n\t        )\n\t    else:\n\t        print_formatted(\n\t            f\"Would have run the following command:\\n\\t{apply_color(' '.join(install_cmd),'cyan')}\",\n\t            \"italic\",\n\t            color=\"yellow\",\n\t        )\n"]}
{"filename": "utils.py", "chunked_list": ["from PIL import Image\n\timport numpy as np\n\timport torch\n\tfrom pathlib import Path\n\timport sys\n\tfrom typing import List\n\timport signal\n\tfrom contextlib import suppress\n\tfrom queue import Queue, Empty\n\timport subprocess\n", "import threading\n\timport os\n\timport math\n\timport functools\n\timport socket\n\timport requests\n\ttry:\n\t    from .log import log\n\texcept ImportError:\n\t    try:\n", "        from log import log\n\t        log.warn(\"Imported log without relative path\")\n\t    except ImportError:\n\t        import logging\n\t        log = logging.getLogger(\"comfy mtb utils\")\n\t        log.warn(\"[comfy mtb] You probably called the file outside a module.\")\n\tclass IPChecker:\n\t    def __init__(self):\n\t        self.ips = list(self.get_local_ips())\n\t        log.debug(f\"Found {len(self.ips)} local ips\")\n", "        self.checked_ips = set()\n\t    def get_working_ip(self, test_url_template):\n\t        for ip in self.ips:\n\t            if ip not in self.checked_ips:\n\t                self.checked_ips.add(ip)\n\t                test_url = test_url_template.format(ip)\n\t                if self._test_url(test_url):\n\t                    return ip\n\t        return None\n\t    @staticmethod\n", "    def get_local_ips(prefix=\"192.168.\"):\n\t        hostname = socket.gethostname()\n\t        log.debug(f\"Getting local ips for {hostname}\")\n\t        for info in socket.getaddrinfo(hostname, None):\n\t            # Filter out IPv6 addresses if you only want IPv4\n\t            log.debug(info)\n\t            # if info[1] == socket.SOCK_STREAM and\n\t            if info[0] == socket.AF_INET and info[4][0].startswith(prefix):\n\t                yield info[4][0]\n\t    def _test_url(self, url):\n", "        try:\n\t            response = requests.get(url)\n\t            return response.status_code == 200\n\t        except Exception:\n\t            return False\n\t# region MISC Utilities\n\t@functools.lru_cache(maxsize=1)\n\tdef get_server_info():\n\t    from comfy.cli_args import args\n\t    ip_checker = IPChecker()\n", "    base_url = args.listen\n\t    if base_url == \"0.0.0.0\":\n\t        log.debug(\"Server set to 0.0.0.0, we will try to resolve the host IP\")\n\t        base_url = ip_checker.get_working_ip(f\"http://{{}}:{args.port}/history\")\n\t        log.debug(f\"Setting ip to {base_url}\")\n\t    return (base_url, args.port)\n\tdef hex_to_rgb(hex_color):\n\t    try:\n\t        hex_color = hex_color.lstrip(\"#\")\n\t        return tuple(int(hex_color[i : i + 2], 16) for i in (0, 2, 4))\n", "    except ValueError:\n\t        log.error(f\"Invalid hex color: {hex_color}\")\n\t        return (0, 0, 0)\n\tdef add_path(path, prepend=False):\n\t    if isinstance(path, list):\n\t        for p in path:\n\t            add_path(p, prepend)\n\t        return\n\t    if isinstance(path, Path):\n\t        path = path.resolve().as_posix()\n", "    if path not in sys.path:\n\t        if prepend:\n\t            sys.path.insert(0, path)\n\t        else:\n\t            sys.path.append(path)\n\tdef enqueue_output(out, queue):\n\t    for line in iter(out.readline, b\"\"):\n\t        queue.put(line)\n\t    out.close()\n\tdef run_command(cmd):\n", "    if isinstance(cmd, str):\n\t        shell_cmd = cmd\n\t    elif isinstance(cmd, list):\n\t        shell_cmd = \"\"\n\t        for arg in cmd:\n\t            if isinstance(arg, Path):\n\t                arg = arg.as_posix()\n\t            shell_cmd += f\"{arg} \"\n\t    else:\n\t        raise ValueError(\n", "            \"Invalid 'cmd' argument. It must be a string or a list of arguments.\"\n\t        )\n\t    process = subprocess.Popen(\n\t        shell_cmd,\n\t        stdout=subprocess.PIPE,\n\t        stderr=subprocess.PIPE,\n\t        universal_newlines=True,\n\t        shell=True,\n\t    )\n\t    # Create separate threads to read standard output and standard error streams\n", "    stdout_queue = Queue()\n\t    stderr_queue = Queue()\n\t    stdout_thread = threading.Thread(\n\t        target=enqueue_output, args=(process.stdout, stdout_queue)\n\t    )\n\t    stderr_thread = threading.Thread(\n\t        target=enqueue_output, args=(process.stderr, stderr_queue)\n\t    )\n\t    stdout_thread.daemon = True\n\t    stderr_thread.daemon = True\n", "    stdout_thread.start()\n\t    stderr_thread.start()\n\t    interrupted = False\n\t    def signal_handler(signum, frame):\n\t        nonlocal interrupted\n\t        interrupted = True\n\t        print(\"Command execution interrupted.\")\n\t    # Register the signal handler for keyboard interrupts (SIGINT)\n\t    signal.signal(signal.SIGINT, signal_handler)\n\t    # Process output from both streams until the process completes or interrupted\n", "    while not interrupted and (\n\t        process.poll() is None or not stdout_queue.empty() or not stderr_queue.empty()\n\t    ):\n\t        with suppress(Empty):\n\t            stdout_line = stdout_queue.get_nowait()\n\t            if stdout_line.strip() != \"\":\n\t                print(stdout_line.strip())\n\t        with suppress(Empty):\n\t            stderr_line = stderr_queue.get_nowait()\n\t            if stderr_line.strip() != \"\":\n", "                print(stderr_line.strip())\n\t    return_code = process.returncode\n\t    if return_code == 0 and not interrupted:\n\t        print(\"Command executed successfully!\")\n\t    else:\n\t        if not interrupted:\n\t            print(f\"Command failed with return code: {return_code}\")\n\t# todo use the requirements library\n\treqs_map = {\n\t    \"onnxruntime\": \"onnxruntime-gpu==1.15.1\",\n", "    \"basicsr\": \"basicsr==1.4.2\",\n\t    \"rembg\": \"rembg==2.0.50\",\n\t    \"qrcode\": \"qrcode[pil]\",\n\t}\n\tdef import_install(package_name):\n\t    from pip._internal import main as pip_main\n\t    try:\n\t        __import__(package_name)\n\t    except ImportError:\n\t        package_spec = reqs_map.get(package_name)\n", "        if package_spec is None:\n\t            print(f\"Installing {package_name}\")\n\t            package_spec = package_name\n\t        pip_main([\"install\", package_spec])\n\t        __import__(package_name)\n\t# endregion\n\t# region GLOBAL VARIABLES\n\t# - detect mode\n\tcomfy_mode = None\n\tif os.environ.get(\"COLAB_GPU\"):\n", "    comfy_mode = \"colab\"\n\telif \"python_embeded\" in sys.executable:\n\t    comfy_mode = \"embeded\"\n\telif \".venv\" in sys.executable:\n\t    comfy_mode = \"venv\"\n\t# - Get the absolute path of the parent directory of the current script\n\there = Path(__file__).parent.resolve()\n\t# - Construct the absolute path to the ComfyUI directory\n\tcomfy_dir = here.parent.parent\n\t# - Construct the path to the font file\n", "font_path = here / \"font.ttf\"\n\t# - Add extern folder to path\n\textern_root = here / \"extern\"\n\tadd_path(extern_root)\n\tfor pth in extern_root.iterdir():\n\t    if pth.is_dir():\n\t        add_path(pth)\n\t# - Add the ComfyUI directory and custom nodes path to the sys.path list\n\tadd_path(comfy_dir)\n\tadd_path((comfy_dir / \"custom_nodes\"))\n", "PIL_FILTER_MAP = {\n\t    \"nearest\": Image.Resampling.NEAREST,\n\t    \"box\": Image.Resampling.BOX,\n\t    \"bilinear\": Image.Resampling.BILINEAR,\n\t    \"hamming\": Image.Resampling.HAMMING,\n\t    \"bicubic\": Image.Resampling.BICUBIC,\n\t    \"lanczos\": Image.Resampling.LANCZOS,\n\t}\n\t# endregion\n\t# region TENSOR UTILITIES\n", "def tensor2pil(image: torch.Tensor) -> List[Image.Image]:\n\t    batch_count = image.size(0) if len(image.shape) > 3 else 1\n\t    if batch_count > 1:\n\t        out = []\n\t        for i in range(batch_count):\n\t            out.extend(tensor2pil(image[i]))\n\t        return out\n\t    return [\n\t        Image.fromarray(\n\t            np.clip(255.0 * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8)\n", "        )\n\t    ]\n\tdef pil2tensor(image: Image.Image | List[Image.Image]) -> torch.Tensor:\n\t    if isinstance(image, list):\n\t        return torch.cat([pil2tensor(img) for img in image], dim=0)\n\t    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)\n\tdef np2tensor(img_np: np.ndarray | List[np.ndarray]) -> torch.Tensor:\n\t    if isinstance(img_np, list):\n\t        return torch.cat([np2tensor(img) for img in img_np], dim=0)\n\t    return torch.from_numpy(img_np.astype(np.float32) / 255.0).unsqueeze(0)\n", "def tensor2np(tensor: torch.Tensor) -> List[np.ndarray]:\n\t    batch_count = tensor.size(0) if len(tensor.shape) > 3 else 1\n\t    if batch_count > 1:\n\t        out = []\n\t        for i in range(batch_count):\n\t            out.extend(tensor2np(tensor[i]))\n\t        return out\n\t    return [np.clip(255.0 * tensor.cpu().numpy().squeeze(), 0, 255).astype(np.uint8)]\n\t# endregion\n\t# region MODEL Utilities\n", "def download_antelopev2():\n\t    antelopev2_url = \"https://drive.google.com/uc?id=18wEUfMNohBJ4K3Ly5wpTejPfDzp-8fI8\"\n\t    try:\n\t        import gdown\n\t        import folder_paths\n\t        log.debug(\"Loading antelopev2 model\")\n\t        dest = Path(folder_paths.models_dir) / \"insightface\"\n\t        archive = dest / \"antelopev2.zip\"\n\t        final_path = dest / \"models\" / \"antelopev2\"\n\t        if not final_path.exists():\n", "            log.info(f\"antelopev2 not found, downloading to {dest}\")\n\t            gdown.download(\n\t                antelopev2_url,\n\t                archive.as_posix(),\n\t                resume=True,\n\t            )\n\t            log.info(f\"Unzipping antelopev2 to {final_path}\")\n\t            if archive.exists():\n\t                # we unzip it\n\t                import zipfile\n", "                with zipfile.ZipFile(archive.as_posix(), \"r\") as zip_ref:\n\t                    zip_ref.extractall(final_path.parent.as_posix())\n\t    except Exception as e:\n\t        log.error(\n\t            f\"Could not load or download antelopev2 model, download it manually from {antelopev2_url}\"\n\t        )\n\t        raise e\n\t# endregion\n\t# region UV Utilities\n\tdef create_uv_map_tensor(width=512, height=512):\n", "    u = torch.linspace(0.0, 1.0, steps=width)\n\t    v = torch.linspace(0.0, 1.0, steps=height)\n\t    U, V = torch.meshgrid(u, v)\n\t    uv_map = torch.zeros(height, width, 3, dtype=torch.float32)\n\t    uv_map[:, :, 0] = U.t()\n\t    uv_map[:, :, 1] = V.t()\n\t    return uv_map.unsqueeze(0)\n\t# endregion\n\t# region ANIMATION Utilities\n\tdef apply_easing(value, easing_type):\n", "    if value < 0 or value > 1:\n\t        raise ValueError(\"The value should be between 0 and 1.\")\n\t    if easing_type == \"Linear\":\n\t        return value\n\t    # Back easing functions\n\t    def easeInBack(t):\n\t        s = 1.70158\n\t        return t * t * ((s + 1) * t - s)\n\t    def easeOutBack(t):\n\t        s = 1.70158\n", "        return ((t - 1) * t * ((s + 1) * t + s)) + 1\n\t    def easeInOutBack(t):\n\t        s = 1.70158 * 1.525\n\t        if t < 0.5:\n\t            return (t * t * (t * (s + 1) - s)) * 2\n\t        return ((t - 2) * t * ((s + 1) * t + s) + 2) * 2\n\t    # Elastic easing functions\n\t    def easeInElastic(t):\n\t        if t == 0:\n\t            return 0\n", "        if t == 1:\n\t            return 1\n\t        p = 0.3\n\t        s = p / 4\n\t        return -(math.pow(2, 10 * (t - 1)) * math.sin((t - 1 - s) * (2 * math.pi) / p))\n\t    def easeOutElastic(t):\n\t        if t == 0:\n\t            return 0\n\t        if t == 1:\n\t            return 1\n", "        p = 0.3\n\t        s = p / 4\n\t        return math.pow(2, -10 * t) * math.sin((t - s) * (2 * math.pi) / p) + 1\n\t    def easeInOutElastic(t):\n\t        if t == 0:\n\t            return 0\n\t        if t == 1:\n\t            return 1\n\t        p = 0.3 * 1.5\n\t        s = p / 4\n", "        t = t * 2\n\t        if t < 1:\n\t            return -0.5 * (\n\t                math.pow(2, 10 * (t - 1)) * math.sin((t - 1 - s) * (2 * math.pi) / p)\n\t            )\n\t        return (\n\t            0.5 * math.pow(2, -10 * (t - 1)) * math.sin((t - 1 - s) * (2 * math.pi) / p)\n\t            + 1\n\t        )\n\t    # Bounce easing functions\n", "    def easeInBounce(t):\n\t        return 1 - easeOutBounce(1 - t)\n\t    def easeOutBounce(t):\n\t        if t < (1 / 2.75):\n\t            return 7.5625 * t * t\n\t        elif t < (2 / 2.75):\n\t            t -= 1.5 / 2.75\n\t            return 7.5625 * t * t + 0.75\n\t        elif t < (2.5 / 2.75):\n\t            t -= 2.25 / 2.75\n", "            return 7.5625 * t * t + 0.9375\n\t        else:\n\t            t -= 2.625 / 2.75\n\t            return 7.5625 * t * t + 0.984375\n\t    def easeInOutBounce(t):\n\t        if t < 0.5:\n\t            return easeInBounce(t * 2) * 0.5\n\t        return easeOutBounce(t * 2 - 1) * 0.5 + 0.5\n\t    # Quart easing functions\n\t    def easeInQuart(t):\n", "        return t * t * t * t\n\t    def easeOutQuart(t):\n\t        t -= 1\n\t        return -(t**2 * t * t - 1)\n\t    def easeInOutQuart(t):\n\t        t *= 2\n\t        if t < 1:\n\t            return 0.5 * t * t * t * t\n\t        t -= 2\n\t        return -0.5 * (t**2 * t * t - 2)\n", "    # Cubic easing functions\n\t    def easeInCubic(t):\n\t        return t * t * t\n\t    def easeOutCubic(t):\n\t        t -= 1\n\t        return t**2 * t + 1\n\t    def easeInOutCubic(t):\n\t        t *= 2\n\t        if t < 1:\n\t            return 0.5 * t * t * t\n", "        t -= 2\n\t        return 0.5 * (t**2 * t + 2)\n\t    # Circ easing functions\n\t    def easeInCirc(t):\n\t        return -(math.sqrt(1 - t * t) - 1)\n\t    def easeOutCirc(t):\n\t        t -= 1\n\t        return math.sqrt(1 - t**2)\n\t    def easeInOutCirc(t):\n\t        t *= 2\n", "        if t < 1:\n\t            return -0.5 * (math.sqrt(1 - t**2) - 1)\n\t        t -= 2\n\t        return 0.5 * (math.sqrt(1 - t**2) + 1)\n\t    # Sine easing functions\n\t    def easeInSine(t):\n\t        return -math.cos(t * (math.pi / 2)) + 1\n\t    def easeOutSine(t):\n\t        return math.sin(t * (math.pi / 2))\n\t    def easeInOutSine(t):\n", "        return -0.5 * (math.cos(math.pi * t) - 1)\n\t    easing_functions = {\n\t        \"Sine In\": easeInSine,\n\t        \"Sine Out\": easeOutSine,\n\t        \"Sine In/Out\": easeInOutSine,\n\t        \"Quart In\": easeInQuart,\n\t        \"Quart Out\": easeOutQuart,\n\t        \"Quart In/Out\": easeInOutQuart,\n\t        \"Cubic In\": easeInCubic,\n\t        \"Cubic Out\": easeOutCubic,\n", "        \"Cubic In/Out\": easeInOutCubic,\n\t        \"Circ In\": easeInCirc,\n\t        \"Circ Out\": easeOutCirc,\n\t        \"Circ In/Out\": easeInOutCirc,\n\t        \"Back In\": easeInBack,\n\t        \"Back Out\": easeOutBack,\n\t        \"Back In/Out\": easeInOutBack,\n\t        \"Elastic In\": easeInElastic,\n\t        \"Elastic Out\": easeOutElastic,\n\t        \"Elastic In/Out\": easeInOutElastic,\n", "        \"Bounce In\": easeInBounce,\n\t        \"Bounce Out\": easeOutBounce,\n\t        \"Bounce In/Out\": easeInOutBounce,\n\t    }\n\t    function_ease = easing_functions.get(easing_type)\n\t    if function_ease:\n\t        return function_ease(value)\n\t    log.error(f\"Unknown easing type: {easing_type}\")\n\t    log.error(f\"Available easing types: {list(easing_functions.keys())}\")\n\t    raise ValueError(f\"Unknown easing type: {easing_type}\")\n", "# endregion\n"]}
{"filename": "nodes/deep_bump.py", "chunked_list": ["import onnxruntime as ort\n\timport numpy as np\n\timport pathlib\n\timport onnxruntime as ort\n\timport numpy as np\n\tfrom .. import utils as utils_inference\n\tfrom ..log import log\n\t# Disable MS telemetry\n\tort.disable_telemetry_events()\n\t# - COLOR to NORMALS\n", "def color_to_normals(color_img, overlap, progress_callback):\n\t    \"\"\"Computes a normal map from the given color map. 'color_img' must be a numpy array\n\t    in C,H,W format (with C as RGB). 'overlap' must be one of 'SMALL', 'MEDIUM', 'LARGE'.\n\t    \"\"\"\n\t    # Remove alpha & convert to grayscale\n\t    img = np.mean(color_img[:3], axis=0, keepdimss=True)\n\t    # Split image in tiles\n\t    log.debug(\"DeepBump Color → Normals : tilling\")\n\t    tile_size = 256\n\t    overlaps = {\n", "        \"SMALL\": tile_size // 6,\n\t        \"MEDIUM\": tile_size // 4,\n\t        \"LARGE\": tile_size // 2,\n\t    }\n\t    stride_size = tile_size - overlaps[overlap]\n\t    tiles, paddings = utils_inference.tiles_split(\n\t        img, (tile_size, tile_size), (stride_size, stride_size)\n\t    )\n\t    # Load model\n\t    log.debug(\"DeepBump Color → Normals : loading model\")\n", "    addon_path = str(pathlib.Path(__file__).parent.absolute())\n\t    ort_session = ort.InferenceSession(f\"{addon_path}/models/deepbump256.onnx\")\n\t    # Predict normal map for each tile\n\t    log.debug(\"DeepBump Color → Normals : generating\")\n\t    pred_tiles = utils_inference.tiles_infer(\n\t        tiles, ort_session, progress_callback=progress_callback\n\t    )\n\t    # Merge tiles\n\t    log.debug(\"DeepBump Color → Normals : merging\")\n\t    pred_img = utils_inference.tiles_merge(\n", "        pred_tiles,\n\t        (stride_size, stride_size),\n\t        (3, img.shape[1], img.shape[2]),\n\t        paddings,\n\t    )\n\t    # Normalize each pixel to unit vector\n\t    pred_img = utils_inference.normalize(pred_img)\n\t    return pred_img\n\t# - NORMALS to CURVATURE\n\tdef conv_1d(array, kernel_1d):\n", "    \"\"\"Performs row by row 1D convolutions of the given 2D image with the given 1D kernel.\"\"\"\n\t    # Input kernel length must be odd\n\t    k_l = len(kernel_1d)\n\t    assert k_l % 2 != 0\n\t    # Convolution is repeat-padded\n\t    extended = np.pad(array, k_l // 2, mode=\"wrap\")\n\t    # Output has same size as input (padded, valid-mode convolution)\n\t    output = np.empty(array.shape)\n\t    for i in range(array.shape[0]):\n\t        output[i] = np.convolve(extended[i + (k_l // 2)], kernel_1d, mode=\"valid\")\n", "    return output * -1\n\tdef gaussian_kernel(length, sigma):\n\t    \"\"\"Returns a 1D gaussian kernel of size 'length'.\"\"\"\n\t    space = np.linspace(-(length - 1) / 2, (length - 1) / 2, length)\n\t    kernel = np.exp(-0.5 * np.square(space) / np.square(sigma))\n\t    return kernel / np.sum(kernel)\n\tdef normalize(np_array):\n\t    \"\"\"Normalize all elements of the given numpy array to [0,1]\"\"\"\n\t    return (np_array - np.min(np_array)) / (np.max(np_array) - np.min(np_array))\n\tdef normals_to_curvature(normals_img, blur_radius, progress_callback):\n", "    \"\"\"Computes a curvature map from the given normal map. 'normals_img' must be a numpy array\n\t    in C,H,W format (with C as RGB). 'blur_radius' must be one of 'SMALLEST', 'SMALLER', 'SMALL',\n\t    'MEDIUM', 'LARGE', 'LARGER', 'LARGEST'.\"\"\"\n\t    # Convolutions on normal map red & green channels\n\t    if progress_callback is not None:\n\t        progress_callback(0, 4)\n\t    diff_kernel = np.array([-1, 0, 1])\n\t    h_conv = conv_1d(normals_img[0, :, :], diff_kernel)\n\t    if progress_callback is not None:\n\t        progress_callback(1, 4)\n", "    v_conv = conv_1d(-1 * normals_img[1, :, :].T, diff_kernel).T\n\t    if progress_callback is not None:\n\t        progress_callback(2, 4)\n\t    # Sum detected edges\n\t    edges_conv = h_conv + v_conv\n\t    # Blur radius size is proportional to img sizes\n\t    blur_factors = {\n\t        \"SMALLEST\": 1 / 256,\n\t        \"SMALLER\": 1 / 128,\n\t        \"SMALL\": 1 / 64,\n", "        \"MEDIUM\": 1 / 32,\n\t        \"LARGE\": 1 / 16,\n\t        \"LARGER\": 1 / 8,\n\t        \"LARGEST\": 1 / 4,\n\t    }\n\t    assert blur_radius in blur_factors\n\t    blur_radius_px = int(np.mean(normals_img.shape[1:3]) * blur_factors[blur_radius])\n\t    # If blur radius too small, do not blur\n\t    if blur_radius_px < 2:\n\t        edges_conv = normalize(edges_conv)\n", "        return np.stack([edges_conv, edges_conv, edges_conv])\n\t    # Make sure blur kernel length is odd\n\t    if blur_radius_px % 2 == 0:\n\t        blur_radius_px += 1\n\t    # Blur curvature with separated convolutions\n\t    sigma = blur_radius_px // 8\n\t    if sigma == 0:\n\t        sigma = 1\n\t    g_kernel = gaussian_kernel(blur_radius_px, sigma)\n\t    h_blur = conv_1d(edges_conv, g_kernel)\n", "    if progress_callback is not None:\n\t        progress_callback(3, 4)\n\t    v_blur = conv_1d(h_blur.T, g_kernel).T\n\t    if progress_callback is not None:\n\t        progress_callback(4, 4)\n\t    # Normalize to [0,1]\n\t    curvature = normalize(v_blur)\n\t    # Expand single channel the three channels (RGB)\n\t    return np.stack([curvature, curvature, curvature])\n\t# - NORMALS to HEIGHT\n", "def normals_to_grad(normals_img):\n\t    return (normals_img[0] - 0.5) * 2, (normals_img[1] - 0.5) * 2\n\tdef copy_flip(grad_x, grad_y):\n\t    \"\"\"Concat 4 flipped copies of input gradients (makes them wrap).\n\t    Output is twice bigger in both dimensions.\"\"\"\n\t    grad_x_top = np.hstack([grad_x, -np.flip(grad_x, axis=1)])\n\t    grad_x_bottom = np.hstack([np.flip(grad_x, axis=0), -np.flip(grad_x)])\n\t    new_grad_x = np.vstack([grad_x_top, grad_x_bottom])\n\t    grad_y_top = np.hstack([grad_y, np.flip(grad_y, axis=1)])\n\t    grad_y_bottom = np.hstack([-np.flip(grad_y, axis=0), -np.flip(grad_y)])\n", "    new_grad_y = np.vstack([grad_y_top, grad_y_bottom])\n\t    return new_grad_x, new_grad_y\n\tdef frankot_chellappa(grad_x, grad_y, progress_callback=None):\n\t    \"\"\"Frankot-Chellappa depth-from-gradient algorithm.\"\"\"\n\t    if progress_callback is not None:\n\t        progress_callback(0, 3)\n\t    rows, cols = grad_x.shape\n\t    rows_scale = (np.arange(rows) - (rows // 2 + 1)) / (rows - rows % 2)\n\t    cols_scale = (np.arange(cols) - (cols // 2 + 1)) / (cols - cols % 2)\n\t    u_grid, v_grid = np.meshgrid(cols_scale, rows_scale)\n", "    u_grid = np.fft.ifftshift(u_grid)\n\t    v_grid = np.fft.ifftshift(v_grid)\n\t    if progress_callback is not None:\n\t        progress_callback(1, 3)\n\t    grad_x_F = np.fft.fft2(grad_x)\n\t    grad_y_F = np.fft.fft2(grad_y)\n\t    if progress_callback is not None:\n\t        progress_callback(2, 3)\n\t    nominator = (-1j * u_grid * grad_x_F) + (-1j * v_grid * grad_y_F)\n\t    denominator = (u_grid**2) + (v_grid**2) + 1e-16\n", "    Z_F = nominator / denominator\n\t    Z_F[0, 0] = 0.0\n\t    Z = np.real(np.fft.ifft2(Z_F))\n\t    if progress_callback is not None:\n\t        progress_callback(3, 3)\n\t    return (Z - np.min(Z)) / (np.max(Z) - np.min(Z))\n\tdef normals_to_height(normals_img, seamless, progress_callback):\n\t    \"\"\"Computes a height map from the given normal map. 'normals_img' must be a numpy array\n\t    in C,H,W format (with C as RGB). 'seamless' is a bool that should indicates if 'normals_img'\n\t    is seamless.\"\"\"\n", "    # Flip height axis\n\t    flip_img = np.flip(normals_img, axis=1)\n\t    # Get gradients from normal map\n\t    grad_x, grad_y = normals_to_grad(flip_img)\n\t    grad_x = np.flip(grad_x, axis=0)\n\t    grad_y = np.flip(grad_y, axis=0)\n\t    # If non-seamless chosen, expand gradients\n\t    if not seamless:\n\t        grad_x, grad_y = copy_flip(grad_x, grad_y)\n\t    # Compute height\n", "    pred_img = frankot_chellappa(-grad_x, grad_y, progress_callback=progress_callback)\n\t    # Cut to valid part if gradients were expanded\n\t    if not seamless:\n\t        height, width = normals_img.shape[1], normals_img.shape[2]\n\t        pred_img = pred_img[:height, :width]\n\t    # Expand single channel the three channels (RGB)\n\t    return np.stack([pred_img, pred_img, pred_img])\n\t# - ADDON\n\tclass DeepBump:\n\t    \"\"\"Normal & height maps generation from single pictures\"\"\"\n", "    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"mode\": (\n\t                    [\"Color to Normals\", \"Normals to Curvature\", \"Normals to Height\"],\n\t                ),\n\t                \"color_to_normals_overlap\": ([\"SMALL\", \"MEDIUM\", \"LARGE\"],),\n\t                \"normals_to_curvature_blur_radius\": (\n", "                    [\n\t                        \"SMALLEST\",\n\t                        \"SMALLER\",\n\t                        \"SMALL\",\n\t                        \"MEDIUM\",\n\t                        \"LARGE\",\n\t                        \"LARGER\",\n\t                        \"LARGEST\",\n\t                    ],\n\t                ),\n", "                \"normals_to_height_seamless\": (\"BOOLEAN\", {\"default\": False}),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"apply\"\n\t    CATEGORY = \"mtb/textures\"\n\t    def apply(\n\t        self,\n\t        image,\n\t        mode=\"Color to Normals\",\n", "        color_to_normals_overlap=\"SMALL\",\n\t        normals_to_curvature_blur_radius=\"SMALL\",\n\t        normals_to_height_seamless=True,\n\t    ):\n\t        image = utils_inference.tensor2pil(image)\n\t        in_img = np.transpose(image, (2, 0, 1)) / 255\n\t        log.debug(f\"Input image shape: {in_img.shape}\")\n\t        # Apply processing\n\t        if mode == \"Color to Normals\":\n\t            out_img = color_to_normals(in_img, color_to_normals_overlap, None)\n", "        if mode == \"Normals to Curvature\":\n\t            out_img = normals_to_curvature(\n\t                in_img, normals_to_curvature_blur_radius, None\n\t            )\n\t        if mode == \"Normals to Height\":\n\t            out_img = normals_to_height(in_img, normals_to_height_seamless, None)\n\t        out_img = (np.transpose(out_img, (1, 2, 0)) * 255).astype(np.uint8)\n\t        return (utils_inference.pil2tensor(out_img),)\n\t__nodes__ = [DeepBump]\n"]}
{"filename": "nodes/faceenhance.py", "chunked_list": ["from gfpgan import GFPGANer\n\timport cv2\n\timport numpy as np\n\timport os\n\tfrom pathlib import Path\n\timport folder_paths\n\tfrom ..utils import pil2tensor, np2tensor, tensor2np\n\tfrom basicsr.utils import imwrite\n\tfrom PIL import Image\n\timport torch\n", "from ..log import NullWriter, log\n\tfrom comfy import model_management\n\timport comfy\n\timport comfy.utils\n\tfrom typing import Tuple\n\tclass LoadFaceEnhanceModel:\n\t    \"\"\"Loads a GFPGan or RestoreFormer model for face enhancement.\"\"\"\n\t    def __init__(self) -> None:\n\t        pass\n\t    @classmethod\n", "    def get_models_root(cls):\n\t        fr = Path(folder_paths.models_dir) / \"face_restore\"\n\t        if fr.exists():\n\t            return (fr, None)\n\t        um = Path(folder_paths.models_dir) / \"upscale_models\"\n\t        return (fr, um) if um.exists() else (None, None)\n\t    @classmethod\n\t    def get_models(cls):\n\t        fr_models_path, um_models_path = cls.get_models_root()\n\t        if fr_models_path is None and um_models_path is None:\n", "            log.warning(\"Face restoration models not found.\")\n\t            return []\n\t        if not fr_models_path.exists():\n\t            log.warning(\n\t                f\"No Face Restore checkpoints found at {fr_models_path} (if you've used mtb before these checkpoints were saved in upscale_models before)\"\n\t            )\n\t            log.warning(\n\t                \"For now we fallback to upscale_models but this will be removed in a future version\"\n\t            )\n\t            if um_models_path.exists():\n", "                return [\n\t                    x\n\t                    for x in um_models_path.iterdir()\n\t                    if x.name.endswith(\".pth\")\n\t                    and (\"GFPGAN\" in x.name or \"RestoreFormer\" in x.name)\n\t                ]\n\t            return []\n\t        return [\n\t            x\n\t            for x in fr_models_path.iterdir()\n", "            if x.name.endswith(\".pth\")\n\t            and (\"GFPGAN\" in x.name or \"RestoreFormer\" in x.name)\n\t        ]\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"model_name\": (\n\t                    [x.name for x in cls.get_models()],\n\t                    {\"default\": \"None\"},\n", "                ),\n\t                \"upscale\": (\"INT\", {\"default\": 1}),\n\t            },\n\t            \"optional\": {\"bg_upsampler\": (\"UPSCALE_MODEL\", {\"default\": None})},\n\t        }\n\t    RETURN_TYPES = (\"FACEENHANCE_MODEL\",)\n\t    RETURN_NAMES = (\"model\",)\n\t    FUNCTION = \"load_model\"\n\t    CATEGORY = \"mtb/facetools\"\n\t    def load_model(self, model_name, upscale=2, bg_upsampler=None):\n", "        basic = \"RestoreFormer\" not in model_name\n\t        fr_root, um_root = self.get_models_root()\n\t        if bg_upsampler is not None:\n\t            log.warning(\n\t                f\"Upscale value overridden to {bg_upsampler.scale} from bg_upsampler\"\n\t            )\n\t            upscale = bg_upsampler.scale\n\t            bg_upsampler = BGUpscaleWrapper(bg_upsampler)\n\t        sys.stdout = NullWriter()\n\t        model = GFPGANer(\n", "            model_path=(\n\t                (fr_root if fr_root.exists() else um_root) / model_name\n\t            ).as_posix(),\n\t            upscale=upscale,\n\t            arch=\"clean\" if basic else \"RestoreFormer\",  # or original for v1.0 only\n\t            channel_multiplier=2,  # 1 for v1.0 only\n\t            bg_upsampler=bg_upsampler,\n\t        )\n\t        sys.stdout = sys.__stdout__\n\t        return (model,)\n", "class BGUpscaleWrapper:\n\t    def __init__(self, upscale_model) -> None:\n\t        self.upscale_model = upscale_model\n\t    def enhance(self, img: Image.Image, outscale=2):\n\t        device = model_management.get_torch_device()\n\t        self.upscale_model.to(device)\n\t        tile = 128 + 64\n\t        overlap = 8\n\t        imgt = np2tensor(img)\n\t        imgt = imgt.movedim(-1, -3).to(device)\n", "        steps = imgt.shape[0] * comfy.utils.get_tiled_scale_steps(\n\t            imgt.shape[3], imgt.shape[2], tile_x=tile, tile_y=tile, overlap=overlap\n\t        )\n\t        log.debug(f\"Steps: {steps}\")\n\t        pbar = comfy.utils.ProgressBar(steps)\n\t        s = comfy.utils.tiled_scale(\n\t            imgt,\n\t            lambda a: self.upscale_model(a),\n\t            tile_x=tile,\n\t            tile_y=tile,\n", "            overlap=overlap,\n\t            upscale_amount=self.upscale_model.scale,\n\t            pbar=pbar,\n\t        )\n\t        self.upscale_model.cpu()\n\t        s = torch.clamp(s.movedim(-3, -1), min=0, max=1.0)\n\t        return (tensor2np(s)[0],)\n\timport sys\n\tclass RestoreFace:\n\t    \"\"\"Uses GFPGan to restore faces\"\"\"\n", "    def __init__(self) -> None:\n\t        pass\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"restore\"\n\t    CATEGORY = \"mtb/facetools\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n", "                \"model\": (\"FACEENHANCE_MODEL\",),\n\t                # Input are aligned faces\n\t                \"aligned\": (\"BOOLEAN\", {\"default\": False}),\n\t                # Only restore the center face\n\t                \"only_center_face\": (\"BOOLEAN\", {\"default\": False}),\n\t                # Adjustable weights\n\t                \"weight\": (\"FLOAT\", {\"default\": 0.5}),\n\t                \"save_tmp_steps\": (\"BOOLEAN\", {\"default\": True}),\n\t            }\n\t        }\n", "    def do_restore(\n\t        self,\n\t        image: torch.Tensor,\n\t        model: GFPGANer,\n\t        aligned,\n\t        only_center_face,\n\t        weight,\n\t        save_tmp_steps,\n\t    ) -> torch.Tensor:\n\t        pimage = tensor2np(image)[0]\n", "        width, height = pimage.shape[1], pimage.shape[0]\n\t        source_img = cv2.cvtColor(np.array(pimage), cv2.COLOR_RGB2BGR)\n\t        sys.stdout = NullWriter()\n\t        cropped_faces, restored_faces, restored_img = model.enhance(\n\t            source_img,\n\t            has_aligned=aligned,\n\t            only_center_face=only_center_face,\n\t            paste_back=True,\n\t            # TODO: weight has no effect in 1.3 and 1.4 (only tested these for now...)\n\t            weight=weight,\n", "        )\n\t        sys.stdout = sys.__stdout__\n\t        log.warning(f\"Weight value has no effect for now. (value: {weight})\")\n\t        if save_tmp_steps:\n\t            self.save_intermediate_images(cropped_faces, restored_faces, height, width)\n\t        output = None\n\t        if restored_img is not None:\n\t            output = Image.fromarray(cv2.cvtColor(restored_img, cv2.COLOR_BGR2RGB))\n\t            # imwrite(restored_img, save_restore_path)\n\t        return pil2tensor(output)\n", "    def restore(\n\t        self,\n\t        image: torch.Tensor,\n\t        model: GFPGANer,\n\t        aligned=False,\n\t        only_center_face=False,\n\t        weight=0.5,\n\t        save_tmp_steps=True,\n\t    ) -> Tuple[torch.Tensor]:\n\t        out = [\n", "            self.do_restore(\n\t                image[i], model, aligned, only_center_face, weight, save_tmp_steps\n\t            )\n\t            for i in range(image.size(0))\n\t        ]\n\t        return (torch.cat(out, dim=0),)\n\t    def get_step_image_path(self, step, idx):\n\t        (\n\t            full_output_folder,\n\t            filename,\n", "            counter,\n\t            _subfolder,\n\t            _filename_prefix,\n\t        ) = folder_paths.get_save_image_path(\n\t            f\"{step}_{idx:03}\",\n\t            folder_paths.temp_directory,\n\t        )\n\t        file = f\"{filename}_{counter:05}_.png\"\n\t        return os.path.join(full_output_folder, file)\n\t    def save_intermediate_images(self, cropped_faces, restored_faces, height, width):\n", "        for idx, (cropped_face, restored_face) in enumerate(\n\t            zip(cropped_faces, restored_faces)\n\t        ):\n\t            face_id = idx + 1\n\t            file = self.get_step_image_path(\"cropped_faces\", face_id)\n\t            imwrite(cropped_face, file)\n\t            file = self.get_step_image_path(\"cropped_faces_restored\", face_id)\n\t            imwrite(restored_face, file)\n\t            file = self.get_step_image_path(\"cropped_faces_compare\", face_id)\n\t            # save comparison image\n", "            cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n\t            imwrite(cmp_img, file)\n\t__nodes__ = [RestoreFace, LoadFaceEnhanceModel]\n"]}
{"filename": "nodes/latent_processing.py", "chunked_list": ["import torch\n\tclass LatentLerp:\n\t    \"\"\"Linear interpolation (blend) between two latent vectors\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"A\": (\"LATENT\",),\n\t                \"B\": (\"LATENT\",),\n\t                \"t\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n", "            }\n\t        }\n\t    RETURN_TYPES = (\"LATENT\",)\n\t    FUNCTION = \"lerp_latent\"\n\t    CATEGORY = \"mtb/latent\"\n\t    def lerp_latent(self, A, B, t):\n\t        a = A.copy()\n\t        b = B.copy()\n\t        torch.lerp(a[\"samples\"], b[\"samples\"], t, out=a[\"samples\"])\n\t        return (a,)\n", "__nodes__ = [\n\t    LatentLerp,\n\t]\n"]}
{"filename": "nodes/transform.py", "chunked_list": ["import torch\n\timport torchvision.transforms.functional as TF\n\tfrom ..utils import log, hex_to_rgb, tensor2pil, pil2tensor\n\tfrom math import sqrt, ceil\n\tfrom typing import cast\n\tfrom PIL import Image\n\tclass TransformImage:\n\t    \"\"\"Save torch tensors (image, mask or latent) to disk, useful to debug things outside comfy\n\t    it return a tensor representing the transformed images with the same shape as the input tensor\n\t    \"\"\"\n", "    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"x\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096}),\n\t                \"y\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096}),\n\t                \"zoom\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.001, \"step\": 0.01}),\n\t                \"angle\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -360, \"max\": 360}),\n\t                \"shear\": (\n", "                    \"FLOAT\",\n\t                    {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096},\n\t                ),\n\t                \"border_handling\": (\n\t                    [\"edge\", \"constant\", \"reflect\", \"symmetric\"],\n\t                    {\"default\": \"edge\"},\n\t                ),\n\t                \"constant_color\": (\"COLOR\", {\"default\": \"#000000\"}),\n\t            },\n\t        }\n", "    FUNCTION = \"transform\"\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    CATEGORY = \"mtb/transform\"\n\t    def transform(\n\t        self,\n\t        image: torch.Tensor,\n\t        x: float,\n\t        y: float,\n\t        zoom: float,\n\t        angle: float,\n", "        shear: float,\n\t        border_handling=\"edge\",\n\t        constant_color=None,\n\t    ):\n\t        x = int(x)\n\t        y = int(y)\n\t        angle = int(angle)\n\t        log.debug(f\"Zoom: {zoom} | x: {x}, y: {y}, angle: {angle}, shear: {shear}\")\n\t        if image.size(0) == 0:\n\t            return (torch.zeros(0),)\n", "        transformed_images = []\n\t        frames_count, frame_height, frame_width, frame_channel_count = image.size()\n\t        new_height, new_width = int(frame_height * zoom), int(frame_width * zoom)\n\t        log.debug(f\"New height: {new_height}, New width: {new_width}\")\n\t        # - Calculate diagonal of the original image\n\t        diagonal = sqrt(frame_width**2 + frame_height**2)\n\t        max_padding = ceil(diagonal * zoom - min(frame_width, frame_height))\n\t        # Calculate padding for zoom\n\t        pw = int(frame_width - new_width)\n\t        ph = int(frame_height - new_height)\n", "        pw += abs(max_padding)\n\t        ph += abs(max_padding)\n\t        padding = [max(0, pw + x), max(0, ph + y), max(0, pw - x), max(0, ph - y)]\n\t        constant_color = hex_to_rgb(constant_color)\n\t        log.debug(f\"Fill Tuple: {constant_color}\")\n\t        for img in tensor2pil(image):\n\t            img = TF.pad(\n\t                img,  # transformed_frame,\n\t                padding=padding,\n\t                padding_mode=border_handling,\n", "                fill=constant_color or 0,\n\t            )\n\t            img = cast(\n\t                Image.Image,\n\t                TF.affine(img, angle=angle, scale=zoom, translate=[x, y], shear=shear),\n\t            )\n\t            left = abs(padding[0])\n\t            upper = abs(padding[1])\n\t            right = img.width - abs(padding[2])\n\t            bottom = img.height - abs(padding[3])\n", "            # log.debug(\"crop is [:,top:bottom, left:right] for tensors\")\n\t            log.debug(\"crop is [left, top, right, bottom] for PIL\")\n\t            log.debug(f\"crop is {left}, {upper}, {right}, {bottom}\")\n\t            img = img.crop((left, upper, right, bottom))\n\t            transformed_images.append(img)\n\t        return (pil2tensor(transformed_images),)\n\t__nodes__ = [TransformImage]\n"]}
{"filename": "nodes/image_interpolation.py", "chunked_list": ["from typing import List\n\tfrom pathlib import Path\n\timport os\n\timport glob\n\timport folder_paths\n\tfrom ..log import log\n\timport torch\n\tfrom frame_interpolation.eval import util, interpolator\n\timport numpy as np\n\timport comfy\n", "import comfy.utils\n\timport tensorflow as tf\n\timport comfy.model_management as model_management\n\tclass LoadFilmModel:\n\t    \"\"\"Loads a FILM model\"\"\"\n\t    @staticmethod\n\t    def get_models() -> List[Path]:\n\t        models_path = os.path.join(folder_paths.models_dir, \"FILM/*\")\n\t        models = glob.glob(models_path)\n\t        models = [Path(x) for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")]\n", "        return models\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"film_model\": (\n\t                    [\"L1\", \"Style\", \"VGG\"],\n\t                    {\"default\": \"Style\"},\n\t                ),\n\t            },\n", "        }\n\t    RETURN_TYPES = (\"FILM_MODEL\",)\n\t    FUNCTION = \"load_model\"\n\t    CATEGORY = \"mtb/frame iterpolation\"\n\t    def load_model(self, film_model: str):\n\t        model_path = Path(folder_paths.models_dir) / \"FILM\" / film_model\n\t        if not (model_path / \"saved_model.pb\").exists():\n\t            model_path = model_path / \"saved_model\"\n\t        if not model_path.exists():\n\t            log.error(f\"Model {model_path} does not exist\")\n", "            raise ValueError(f\"Model {model_path} does not exist\")\n\t        log.info(f\"Loading model {model_path}\")\n\t        return (interpolator.Interpolator(model_path.as_posix(), None),)\n\tclass FilmInterpolation:\n\t    \"\"\"Google Research FILM frame interpolation for large motion\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"images\": (\"IMAGE\",),\n", "                \"interpolate\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 50}),\n\t                \"film_model\": (\"FILM_MODEL\",),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"do_interpolation\"\n\t    CATEGORY = \"mtb/frame iterpolation\"\n\t    def do_interpolation(\n\t        self,\n\t        images: torch.Tensor,\n", "        interpolate: int,\n\t        film_model: interpolator.Interpolator,\n\t    ):\n\t        n = images.size(0)\n\t        # check if images is an empty tensor and return it...\n\t        if n == 0:\n\t            return (images,)\n\t        # check if tensorflow GPU is available\n\t        available_gpus = tf.config.list_physical_devices(\"GPU\")\n\t        if not len(available_gpus):\n", "            log.warning(\n\t                \"Tensorflow GPU not available, falling back to CPU this will be very slow\"\n\t            )\n\t        else:\n\t            log.debug(f\"Tensorflow GPU available, using {available_gpus}\")\n\t        num_frames = (n - 1) * (2 ** (interpolate) - 1)\n\t        log.debug(f\"Will interpolate into {num_frames} frames\")\n\t        in_frames = [images[i] for i in range(n)]\n\t        out_tensors = []\n\t        pbar = comfy.utils.ProgressBar(num_frames)\n", "        for frame in util.interpolate_recursively_from_memory(\n\t            in_frames, interpolate, film_model\n\t        ):\n\t            out_tensors.append(\n\t                torch.from_numpy(frame) if isinstance(frame, np.ndarray) else frame\n\t            )\n\t            model_management.throw_exception_if_processing_interrupted()\n\t            pbar.update(1)\n\t        out_tensors = torch.cat([tens.unsqueeze(0) for tens in out_tensors], dim=0)\n\t        log.debug(f\"Returning {len(out_tensors)} tensors\")\n", "        log.debug(f\"Output shape {out_tensors.shape}\")\n\t        log.debug(f\"Output type {out_tensors.dtype}\")\n\t        return (out_tensors,)\n\tclass ConcatImages:\n\t    \"\"\"Add images to batch\"\"\"\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"concat_images\"\n\t    CATEGORY = \"mtb/image\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n", "        return {\n\t            \"required\": {\n\t                \"imageA\": (\"IMAGE\",),\n\t                \"imageB\": (\"IMAGE\",),\n\t            },\n\t        }\n\t    @classmethod\n\t    def concatenate_tensors(cls, A: torch.Tensor, B: torch.Tensor):\n\t        # Get the batch sizes of A and B\n\t        batch_size_A = A.size(0)\n", "        batch_size_B = B.size(0)\n\t        # Concatenate the tensors along the batch dimension\n\t        concatenated = torch.cat((A, B), dim=0)\n\t        # Update the batch size in the concatenated tensor\n\t        concatenated_size = list(concatenated.size())\n\t        concatenated_size[0] = batch_size_A + batch_size_B\n\t        concatenated = concatenated.view(*concatenated_size)\n\t        return concatenated\n\t    def concat_images(self, imageA: torch.Tensor, imageB: torch.Tensor):\n\t        log.debug(f\"Concatenating A ({imageA.shape}) and B ({imageB.shape})\")\n", "        return (self.concatenate_tensors(imageA, imageB),)\n\t__nodes__ = [LoadFilmModel, FilmInterpolation, ConcatImages]\n"]}
{"filename": "nodes/faceswap.py", "chunked_list": ["# region imports\n\timport onnxruntime\n\tfrom pathlib import Path\n\tfrom PIL import Image\n\tfrom typing import List, Set, Union, Optional\n\timport cv2\n\timport folder_paths\n\timport glob\n\timport insightface\n\timport numpy as np\n", "import os\n\timport torch\n\tfrom insightface.model_zoo.inswapper import INSwapper\n\tfrom ..utils import pil2tensor, tensor2pil, download_antelopev2\n\tfrom ..log import mklog, NullWriter\n\timport sys\n\timport comfy.model_management as model_management\n\t# endregion\n\tlog = mklog(__name__)\n\tclass LoadFaceAnalysisModel:\n", "    \"\"\"Loads a face analysis model\"\"\"\n\t    models = []\n\t    @staticmethod\n\t    def get_models() -> List[str]:\n\t        models_path = os.path.join(folder_paths.models_dir, \"insightface/*\")\n\t        models = glob.glob(models_path)\n\t        models = [\n\t            Path(x).name for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")\n\t        ]\n\t        return models\n", "    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"faceswap_model\": (\n\t                    [\"antelopev2\", \"buffalo_l\", \"buffalo_m\", \"buffalo_sc\"],\n\t                    {\"default\": \"buffalo_l\"},\n\t                ),\n\t            },\n\t        }\n", "    RETURN_TYPES = (\"FACE_ANALYSIS_MODEL\",)\n\t    FUNCTION = \"load_model\"\n\t    CATEGORY = \"mtb/facetools\"\n\t    def load_model(self, faceswap_model: str):\n\t        if faceswap_model == \"antelopev2\":\n\t            download_antelopev2()\n\t        face_analyser = insightface.app.FaceAnalysis(\n\t            name=faceswap_model,\n\t            root=os.path.join(folder_paths.models_dir, \"insightface\"),\n\t        )\n", "        return (face_analyser,)\n\tclass LoadFaceSwapModel:\n\t    \"\"\"Loads a faceswap model\"\"\"\n\t    @staticmethod\n\t    def get_models() -> List[Path]:\n\t        models_path = os.path.join(folder_paths.models_dir, \"insightface/*\")\n\t        models = glob.glob(models_path)\n\t        models = [Path(x) for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")]\n\t        return models\n\t    @classmethod\n", "    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"faceswap_model\": (\n\t                    [x.name for x in cls.get_models()],\n\t                    {\"default\": \"None\"},\n\t                ),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\"FACESWAP_MODEL\",)\n", "    FUNCTION = \"load_model\"\n\t    CATEGORY = \"mtb/facetools\"\n\t    def load_model(self, faceswap_model: str):\n\t        model_path = os.path.join(\n\t            folder_paths.models_dir, \"insightface\", faceswap_model\n\t        )\n\t        log.info(f\"Loading model {model_path}\")\n\t        return (\n\t            INSwapper(\n\t                model_path,\n", "                onnxruntime.InferenceSession(\n\t                    path_or_bytes=model_path,\n\t                    providers=onnxruntime.get_available_providers(),\n\t                ),\n\t            ),\n\t        )\n\t# region roop node\n\tclass FaceSwap:\n\t    \"\"\"Face swap using deepinsight/insightface models\"\"\"\n\t    model = None\n", "    model_path = None\n\t    def __init__(self) -> None:\n\t        pass\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"reference\": (\"IMAGE\",),\n\t                \"faces_index\": (\"STRING\", {\"default\": \"0\"}),\n", "                \"faceanalysis_model\": (\"FACE_ANALYSIS_MODEL\", {\"default\": \"None\"}),\n\t                \"faceswap_model\": (\"FACESWAP_MODEL\", {\"default\": \"None\"}),\n\t            },\n\t            \"optional\": {},\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"swap\"\n\t    CATEGORY = \"mtb/facetools\"\n\t    def swap(\n\t        self,\n", "        image: torch.Tensor,\n\t        reference: torch.Tensor,\n\t        faces_index: str,\n\t        faceanalysis_model,\n\t        faceswap_model,\n\t    ):\n\t        def do_swap(img):\n\t            model_management.throw_exception_if_processing_interrupted()\n\t            img = tensor2pil(img)[0]\n\t            ref = tensor2pil(reference)[0]\n", "            face_ids = {\n\t                int(x) for x in faces_index.strip(\",\").split(\",\") if x.isnumeric()\n\t            }\n\t            sys.stdout = NullWriter()\n\t            swapped = swap_face(faceanalysis_model, ref, img, faceswap_model, face_ids)\n\t            sys.stdout = sys.__stdout__\n\t            return pil2tensor(swapped)\n\t        batch_count = image.size(0)\n\t        log.info(f\"Running insightface swap (batch size: {batch_count})\")\n\t        if reference.size(0) != 1:\n", "            raise ValueError(\"Reference image must have batch size 1\")\n\t        if batch_count == 1:\n\t            image = do_swap(image)\n\t        else:\n\t            image_batch = [do_swap(image[i]) for i in range(batch_count)]\n\t            image = torch.cat(image_batch, dim=0)\n\t        return (image,)\n\t# endregion\n\t# region face swap utils\n\tdef get_face_single(\n", "    face_analyser, img_data: np.ndarray, face_index=0, det_size=(640, 640)\n\t):\n\t    face_analyser.prepare(ctx_id=0, det_size=det_size)\n\t    face = face_analyser.get(img_data)\n\t    if len(face) == 0 and det_size[0] > 320 and det_size[1] > 320:\n\t        log.debug(\"No face ed, trying again with smaller image\")\n\t        det_size_half = (det_size[0] // 2, det_size[1] // 2)\n\t        return get_face_single(\n\t            face_analyser, img_data, face_index=face_index, det_size=det_size_half\n\t        )\n", "    try:\n\t        return sorted(face, key=lambda x: x.bbox[0])[face_index]\n\t    except IndexError:\n\t        return None\n\tdef swap_face(\n\t    face_analyser,\n\t    source_img: Union[Image.Image, List[Image.Image]],\n\t    target_img: Union[Image.Image, List[Image.Image]],\n\t    face_swapper_model,\n\t    faces_index: Optional[Set[int]] = None,\n", ") -> Image.Image:\n\t    if faces_index is None:\n\t        faces_index = {0}\n\t    log.debug(f\"Swapping faces: {faces_index}\")\n\t    result_image = target_img\n\t    if face_swapper_model is not None:\n\t        cv_source_img = cv2.cvtColor(np.array(source_img), cv2.COLOR_RGB2BGR)\n\t        cv_target_img = cv2.cvtColor(np.array(target_img), cv2.COLOR_RGB2BGR)\n\t        source_face = get_face_single(face_analyser, cv_source_img, face_index=0)\n\t        if source_face is not None:\n", "            result = cv_target_img\n\t            for face_num in faces_index:\n\t                target_face = get_face_single(\n\t                    face_analyser, cv_target_img, face_index=face_num\n\t                )\n\t                if target_face is not None:\n\t                    sys.stdout = NullWriter()\n\t                    result = face_swapper_model.get(result, target_face, source_face)\n\t                    sys.stdout = sys.__stdout__\n\t                else:\n", "                    log.warning(f\"No target face found for {face_num}\")\n\t            result_image = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n\t        else:\n\t            log.warning(\"No source face found\")\n\t    else:\n\t        log.error(\"No face swap model provided\")\n\t    return result_image\n\t# endregion face swap utils\n\t__nodes__ = [FaceSwap, LoadFaceSwapModel, LoadFaceAnalysisModel]\n"]}
{"filename": "nodes/__init__.py", "chunked_list": []}
{"filename": "nodes/crop.py", "chunked_list": ["import torch\n\tfrom ..utils import tensor2pil, pil2tensor, tensor2np, np2tensor\n\tfrom PIL import Image, ImageFilter, ImageDraw, ImageChops\n\timport numpy as np\n\tfrom ..log import log\n\tclass Bbox:\n\t    \"\"\"The bounding box (BBOX) custom type used by other nodes\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n", "            \"required\": {\n\t                # \"bbox\": (\"BBOX\",),\n\t                \"x\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n\t                \"y\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n\t                \"width\": (\n\t                    \"INT\",\n\t                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n\t                ),\n\t                \"height\": (\n\t                    \"INT\",\n", "                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"BBOX\",)\n\t    FUNCTION = \"do_crop\"\n\t    CATEGORY = \"mtb/crop\"\n\t    def do_crop(self, x, y, width, height):  # bbox\n\t        return (x, y, width, height)\n\t        # return bbox\n", "class BboxFromMask:\n\t    \"\"\"From a mask extract the bounding box\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"mask\": (\"MASK\",),\n\t            },\n\t            \"optional\": {\n\t                \"image\": (\"IMAGE\",),\n", "            },\n\t        }\n\t    RETURN_TYPES = (\n\t        \"BBOX\",\n\t        \"IMAGE\",\n\t    )\n\t    RETURN_NAMES = (\n\t        \"bbox\",\n\t        \"image (optional)\",\n\t    )\n", "    FUNCTION = \"extract_bounding_box\"\n\t    CATEGORY = \"mtb/crop\"\n\t    def extract_bounding_box(self, mask: torch.Tensor, image=None):\n\t        # if image != None:\n\t        #     if mask.size(0) != image.size(0):\n\t        #         if mask.size(0) != 1:\n\t        #             log.error(\n\t        #                 f\"Batch count mismatch for mask and image, it can either be 1 mask for X images, or X masks for X images (mask: {mask.shape} | image: {image.shape})\"\n\t        #             )\n\t        #             raise Exception(\n", "        #                 f\"Batch count mismatch for mask and image, it can either be 1 mask for X images, or X masks for X images (mask: {mask.shape} | image: {image.shape})\"\n\t        #             )\n\t        _mask = tensor2pil(1.0 - mask)[0]\n\t        # we invert it\n\t        alpha_channel = np.array(_mask)\n\t        non_zero_indices = np.nonzero(alpha_channel)\n\t        min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n\t        min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n\t        # Create a bounding box tuple\n\t        if image != None:\n", "            # Convert the image to a NumPy array\n\t            imgs = tensor2np(image)\n\t            out = []\n\t            for img in imgs:\n\t                # Crop the image from the bounding box\n\t                img = img[min_y:max_y, min_x:max_x, :]\n\t                log.debug(f\"Cropped image to shape {img.shape}\")\n\t                out.append(img)\n\t            image = np2tensor(out)\n\t            log.debug(f\"Cropped images shape: {image.shape}\")\n", "        bounding_box = (min_x, min_y, max_x - min_x, max_y - min_y)\n\t        return (\n\t            bounding_box,\n\t            image,\n\t        )\n\tclass Crop:\n\t    \"\"\"Crops an image and an optional mask to a given bounding box\n\t    The bounding box can be given as a tuple of (x, y, width, height) or as a BBOX type\n\t    The BBOX input takes precedence over the tuple input\n\t    \"\"\"\n", "    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t            },\n\t            \"optional\": {\n\t                \"mask\": (\"MASK\",),\n\t                \"x\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n\t                \"y\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n", "                \"width\": (\n\t                    \"INT\",\n\t                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n\t                ),\n\t                \"height\": (\n\t                    \"INT\",\n\t                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n\t                ),\n\t                \"bbox\": (\"BBOX\",),\n\t            },\n", "        }\n\t    RETURN_TYPES = (\"IMAGE\", \"MASK\", \"BBOX\")\n\t    FUNCTION = \"do_crop\"\n\t    CATEGORY = \"mtb/crop\"\n\t    def do_crop(\n\t        self, image: torch.Tensor, mask=None, x=0, y=0, width=256, height=256, bbox=None\n\t    ):\n\t        image = image.numpy()\n\t        if mask:\n\t            mask = mask.numpy()\n", "        if bbox != None:\n\t            x, y, width, height = bbox\n\t        cropped_image = image[:, y : y + height, x : x + width, :]\n\t        cropped_mask = mask[y : y + height, x : x + width] if mask != None else None\n\t        crop_data = (x, y, width, height)\n\t        return (\n\t            torch.from_numpy(cropped_image),\n\t            torch.from_numpy(cropped_mask) if mask != None else None,\n\t            crop_data,\n\t        )\n", "# def calculate_intersection(rect1, rect2):\n\t#     x_left = max(rect1[0], rect2[0])\n\t#     y_top = max(rect1[1], rect2[1])\n\t#     x_right = min(rect1[2], rect2[2])\n\t#     y_bottom = min(rect1[3], rect2[3])\n\t#     return (x_left, y_top, x_right, y_bottom)\n\tdef bbox_check(bbox, target_size=None):\n\t    if not target_size:\n\t        return bbox\n\t    new_bbox = (\n", "        bbox[0],\n\t        bbox[1],\n\t        min(target_size[0] - bbox[0], bbox[2]),\n\t        min(target_size[1] - bbox[1], bbox[3]),\n\t    )\n\t    if new_bbox != bbox:\n\t        log.warn(f\"BBox too big, constrained to {new_bbox}\")\n\t    return new_bbox\n\tdef bbox_to_region(bbox, target_size=None):\n\t    bbox = bbox_check(bbox, target_size)\n", "    # to region\n\t    return (bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3])\n\tclass Uncrop:\n\t    \"\"\"Uncrops an image to a given bounding box\n\t    The bounding box can be given as a tuple of (x, y, width, height) or as a BBOX type\n\t    The BBOX input takes precedence over the tuple input\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n", "                \"image\": (\"IMAGE\",),\n\t                \"crop_image\": (\"IMAGE\",),\n\t                \"bbox\": (\"BBOX\",),\n\t                \"border_blending\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 0.25, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n", "    FUNCTION = \"do_crop\"\n\t    CATEGORY = \"mtb/crop\"\n\t    def do_crop(self, image, crop_image, bbox, border_blending):\n\t        def inset_border(image, border_width=20, border_color=(0)):\n\t            width, height = image.size\n\t            bordered_image = Image.new(image.mode, (width, height), border_color)\n\t            bordered_image.paste(image, (0, 0))\n\t            draw = ImageDraw.Draw(bordered_image)\n\t            draw.rectangle(\n\t                (0, 0, width - 1, height - 1), outline=border_color, width=border_width\n", "            )\n\t            return bordered_image\n\t        single = image.size(0) == 1\n\t        if image.size(0) != crop_image.size(0):\n\t            if not single:\n\t                raise ValueError(\n\t                    \"The Image batch count is greater than 1, but doesn't match the crop_image batch count. If using batches they should either match or only crop_image must be greater than 1\"\n\t                )\n\t        images = tensor2pil(image)\n\t        crop_imgs = tensor2pil(crop_image)\n", "        out_images = []\n\t        for i, crop in enumerate(crop_imgs):\n\t            if single:\n\t                img = images[0]\n\t            else:\n\t                img = images[i]\n\t            # uncrop the image based on the bounding box\n\t            bb_x, bb_y, bb_width, bb_height = bbox\n\t            paste_region = bbox_to_region((bb_x, bb_y, bb_width, bb_height), img.size)\n\t            # log.debug(f\"Paste region: {paste_region}\")\n", "            # new_region = adjust_paste_region(img.size, paste_region)\n\t            # log.debug(f\"Adjusted paste region: {new_region}\")\n\t            # # Check if the adjusted paste region is different from the original\n\t            crop_img = crop.convert(\"RGB\")\n\t            log.debug(f\"Crop image size: {crop_img.size}\")\n\t            log.debug(f\"Image size: {img.size}\")\n\t            if border_blending > 1.0:\n\t                border_blending = 1.0\n\t            elif border_blending < 0.0:\n\t                border_blending = 0.0\n", "            blend_ratio = (max(crop_img.size) / 2) * float(border_blending)\n\t            blend = img.convert(\"RGBA\")\n\t            mask = Image.new(\"L\", img.size, 0)\n\t            mask_block = Image.new(\"L\", (bb_width, bb_height), 255)\n\t            mask_block = inset_border(mask_block, int(blend_ratio / 2), (0))\n\t            mask.paste(mask_block, paste_region)\n\t            log.debug(f\"Blend size: {blend.size} | kind {blend.mode}\")\n\t            log.debug(f\"Crop image size: {crop_img.size} | kind {crop_img.mode}\")\n\t            log.debug(f\"BBox: {paste_region}\")\n\t            blend.paste(crop_img, paste_region)\n", "            mask = mask.filter(ImageFilter.BoxBlur(radius=blend_ratio / 4))\n\t            mask = mask.filter(ImageFilter.GaussianBlur(radius=blend_ratio / 4))\n\t            blend.putalpha(mask)\n\t            img = Image.alpha_composite(img.convert(\"RGBA\"), blend)\n\t            out_images.append(img.convert(\"RGB\"))\n\t        return (pil2tensor(out_images),)\n\t__nodes__ = [BboxFromMask, Bbox, Crop, Uncrop]\n"]}
{"filename": "nodes/image_processing.py", "chunked_list": ["import torch\n\tfrom skimage.filters import gaussian\n\tfrom skimage.util import compare_images\n\timport numpy as np\n\timport torch.nn.functional as F\n\tfrom PIL import Image\n\tfrom ..utils import tensor2pil, pil2tensor, tensor2np\n\timport torch\n\timport folder_paths\n\tfrom PIL.PngImagePlugin import PngInfo\n", "import json\n\timport os\n\timport math\n\t# try:\n\t#     from cv2.ximgproc import guidedFilter\n\t# except ImportError:\n\t#     log.warning(\"cv2.ximgproc.guidedFilter not found, use opencv-contrib-python\")\n\tclass ColorCorrect:\n\t    \"\"\"Various color correction methods\"\"\"\n\t    @classmethod\n", "    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"clamp\": ([True, False], {\"default\": True}),\n\t                \"gamma\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n\t                ),\n\t                \"contrast\": (\n", "                    \"FLOAT\",\n\t                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n\t                ),\n\t                \"exposure\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 0.0, \"min\": -5.0, \"max\": 5.0, \"step\": 0.01},\n\t                ),\n\t                \"offset\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 0.0, \"min\": -5.0, \"max\": 5.0, \"step\": 0.01},\n", "                ),\n\t                \"hue\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 0.0, \"min\": -0.5, \"max\": 0.5, \"step\": 0.01},\n\t                ),\n\t                \"saturation\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n\t                ),\n\t                \"value\": (\n", "                    \"FLOAT\",\n\t                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"correct\"\n\t    CATEGORY = \"mtb/image processing\"\n\t    @staticmethod\n\t    def gamma_correction_tensor(image, gamma):\n", "        gamma_inv = 1.0 / gamma\n\t        return image.pow(gamma_inv)\n\t    @staticmethod\n\t    def contrast_adjustment_tensor(image, contrast):\n\t        contrasted = (image - 0.5) * contrast + 0.5\n\t        return torch.clamp(contrasted, 0.0, 1.0)\n\t    @staticmethod\n\t    def exposure_adjustment_tensor(image, exposure):\n\t        return image * (2.0**exposure)\n\t    @staticmethod\n", "    def offset_adjustment_tensor(image, offset):\n\t        return image + offset\n\t    @staticmethod\n\t    def hsv_adjustment(image: torch.Tensor, hue, saturation, value):\n\t        images = tensor2pil(image)\n\t        out = []\n\t        for img in images:\n\t            hsv_image = img.convert(\"HSV\")\n\t            h, s, v = hsv_image.split()\n\t            h = h.point(lambda x: (x + hue * 255) % 256)\n", "            s = s.point(lambda x: int(x * saturation))\n\t            v = v.point(lambda x: int(x * value))\n\t            hsv_image = Image.merge(\"HSV\", (h, s, v))\n\t            rgb_image = hsv_image.convert(\"RGB\")\n\t            out.append(rgb_image)\n\t        return pil2tensor(out)\n\t    @staticmethod\n\t    def hsv_adjustment_tensor_not_working(image: torch.Tensor, hue, saturation, value):\n\t        \"\"\"Abandonning for now\"\"\"\n\t        image = image.squeeze(0).permute(2, 0, 1)\n", "        max_val, _ = image.max(dim=0, keepdim=True)\n\t        min_val, _ = image.min(dim=0, keepdim=True)\n\t        delta = max_val - min_val\n\t        hue_image = torch.zeros_like(max_val)\n\t        mask = delta != 0.0\n\t        r, g, b = image[0], image[1], image[2]\n\t        hue_image[mask & (max_val == r)] = ((g - b) / delta)[\n\t            mask & (max_val == r)\n\t        ] % 6.0\n\t        hue_image[mask & (max_val == g)] = ((b - r) / delta)[\n", "            mask & (max_val == g)\n\t        ] + 2.0\n\t        hue_image[mask & (max_val == b)] = ((r - g) / delta)[\n\t            mask & (max_val == b)\n\t        ] + 4.0\n\t        saturation_image = delta / (max_val + 1e-7)\n\t        value_image = max_val\n\t        hue_image = (hue_image + hue) % 1.0\n\t        saturation_image = torch.where(\n\t            mask, saturation * saturation_image, saturation_image\n", "        )\n\t        value_image = value * value_image\n\t        c = value_image * saturation_image\n\t        x = c * (1 - torch.abs((hue_image % 2) - 1))\n\t        m = value_image - c\n\t        prime_image = torch.zeros_like(image)\n\t        prime_image[0] = torch.where(\n\t            max_val == r, c, torch.where(max_val == g, x, prime_image[0])\n\t        )\n\t        prime_image[1] = torch.where(\n", "            max_val == r, x, torch.where(max_val == g, c, prime_image[1])\n\t        )\n\t        prime_image[2] = torch.where(\n\t            max_val == g, x, torch.where(max_val == b, c, prime_image[2])\n\t        )\n\t        rgb_image = prime_image + m\n\t        rgb_image = rgb_image.permute(1, 2, 0).unsqueeze(0)\n\t        return rgb_image\n\t    def correct(\n\t        self,\n", "        image: torch.Tensor,\n\t        clamp: bool,\n\t        gamma: float = 1.0,\n\t        contrast: float = 1.0,\n\t        exposure: float = 0.0,\n\t        offset: float = 0.0,\n\t        hue: float = 0.0,\n\t        saturation: float = 1.0,\n\t        value: float = 1.0,\n\t    ):\n", "        # Apply color correction operations\n\t        image = self.gamma_correction_tensor(image, gamma)\n\t        image = self.contrast_adjustment_tensor(image, contrast)\n\t        image = self.exposure_adjustment_tensor(image, exposure)\n\t        image = self.offset_adjustment_tensor(image, offset)\n\t        image = self.hsv_adjustment(image, hue, saturation, value)\n\t        if clamp:\n\t            image = torch.clamp(image, 0.0, 1.0)\n\t        return (image,)\n\tclass ImageCompare:\n", "    \"\"\"Compare two images and return a difference image\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"imageA\": (\"IMAGE\",),\n\t                \"imageB\": (\"IMAGE\",),\n\t                \"mode\": (\n\t                    [\"checkerboard\", \"diff\", \"blend\"],\n\t                    {\"default\": \"checkerboard\"},\n", "                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"compare\"\n\t    CATEGORY = \"mtb/image\"\n\t    def compare(self, imageA: torch.Tensor, imageB: torch.Tensor, mode):\n\t        imageA = imageA.numpy()\n\t        imageB = imageB.numpy()\n\t        imageA = imageA.squeeze()\n", "        imageB = imageB.squeeze()\n\t        image = compare_images(imageA, imageB, method=mode)\n\t        image = np.expand_dims(image, axis=0)\n\t        return (torch.from_numpy(image),)\n\timport requests\n\tclass LoadImageFromUrl:\n\t    \"\"\"Load an image from the given URL\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n", "            \"required\": {\n\t                \"url\": (\n\t                    \"STRING\",\n\t                    {\n\t                        \"default\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Example.jpg/800px-Example.jpg\"\n\t                    },\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n", "    FUNCTION = \"load\"\n\t    CATEGORY = \"mtb/IO\"\n\t    def load(self, url):\n\t        # get the image from the url\n\t        image = Image.open(requests.get(url, stream=True).raw)\n\t        return (pil2tensor(image),)\n\tclass Blur:\n\t    \"\"\"Blur an image using a Gaussian filter.\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n", "        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"sigmaX\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 3.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01},\n\t                ),\n\t                \"sigmaY\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 3.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01},\n", "                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"blur\"\n\t    CATEGORY = \"mtb/image processing\"\n\t    def blur(self, image: torch.Tensor, sigmaX, sigmaY):\n\t        image = image.numpy()\n\t        image = image.transpose(1, 2, 3, 0)\n\t        image = gaussian(image, sigma=(sigmaX, sigmaY, 0, 0))\n", "        image = image.transpose(3, 0, 1, 2)\n\t        return (torch.from_numpy(image),)\n\t# https://github.com/lllyasviel/AdverseCleaner/blob/main/clean.py\n\t# def deglaze_np_img(np_img):\n\t#     y = np_img.copy()\n\t#     for _ in range(64):\n\t#         y = cv2.bilateralFilter(y, 5, 8, 8)\n\t#     for _ in range(4):\n\t#         y = guidedFilter(np_img, y, 4, 16)\n\t#     return y\n", "# class DeglazeImage:\n\t#     \"\"\"Remove adversarial noise from images\"\"\"\n\t#     @classmethod\n\t#     def INPUT_TYPES(cls):\n\t#         return {\"required\": {\"image\": (\"IMAGE\",)}}\n\t#     CATEGORY = \"mtb/image processing\"\n\t#     RETURN_TYPES = (\"IMAGE\",)\n\t#     FUNCTION = \"deglaze_image\"\n\t#     def deglaze_image(self, image):\n\t#         return (np2tensor(deglaze_np_img(tensor2np(image))),)\n", "class MaskToImage:\n\t    \"\"\"Converts a mask (alpha) to an RGB image with a color and background\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"mask\": (\"MASK\",),\n\t                \"color\": (\"COLOR\",),\n\t                \"background\": (\"COLOR\", {\"default\": \"#000000\"}),\n\t            }\n", "        }\n\t    CATEGORY = \"mtb/generate\"\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"render_mask\"\n\t    def render_mask(self, mask, color, background):\n\t        mask = tensor2np(mask)\n\t        mask = Image.fromarray(mask).convert(\"L\")\n\t        image = Image.new(\"RGBA\", mask.size, color=color)\n\t        # apply the mask\n\t        image = Image.composite(\n", "            image, Image.new(\"RGBA\", mask.size, color=background), mask\n\t        )\n\t        # image = ImageChops.multiply(image, mask)\n\t        # apply over background\n\t        # image = Image.alpha_composite(Image.new(\"RGBA\", image.size, color=background), image)\n\t        image = pil2tensor(image.convert(\"RGB\"))\n\t        return (image,)\n\tclass ColoredImage:\n\t    \"\"\"Constant color image of given size\"\"\"\n\t    def __init__(self) -> None:\n", "        pass\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"color\": (\"COLOR\",),\n\t                \"width\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": 8160}),\n\t                \"height\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": 8160}),\n\t            }\n\t        }\n", "    CATEGORY = \"mtb/generate\"\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"render_img\"\n\t    def render_img(self, color, width, height):\n\t        image = Image.new(\"RGB\", (width, height), color=color)\n\t        image = pil2tensor(image)\n\t        return (image,)\n\tclass ImagePremultiply:\n\t    \"\"\"Premultiply image with mask\"\"\"\n\t    @classmethod\n", "    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"mask\": (\"MASK\",),\n\t                \"invert\": (\"BOOLEAN\", {\"default\": False}),\n\t            }\n\t        }\n\t    CATEGORY = \"mtb/image\"\n\t    RETURN_TYPES = (\"IMAGE\",)\n", "    FUNCTION = \"premultiply\"\n\t    def premultiply(self, image, mask, invert):\n\t        images = tensor2pil(image)\n\t        if invert:\n\t            masks = tensor2pil(mask)  # .convert(\"L\")\n\t        else:\n\t            masks = tensor2pil(1.0 - mask)\n\t        single = False\n\t        if len(mask) == 1:\n\t            single = True\n", "        masks = [x.convert(\"L\") for x in masks]\n\t        out = []\n\t        for i, img in enumerate(images):\n\t            cur_mask = masks[0] if single else masks[i]\n\t            img.putalpha(cur_mask)\n\t            out.append(img)\n\t        # if invert:\n\t        #     image = Image.composite(image,Image.new(\"RGBA\", image.size, color=(0,0,0,0)), mask)\n\t        # else:\n\t        #     image = Image.composite(Image.new(\"RGBA\", image.size, color=(0,0,0,0)), image, mask)\n", "        return (pil2tensor(out),)\n\tclass ImageResizeFactor:\n\t    \"\"\"Extracted mostly from WAS Node Suite, with a few edits (most notably multiple image support) and less features.\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"factor\": (\n\t                    \"FLOAT\",\n", "                    {\"default\": 2, \"min\": 0.01, \"max\": 16.0, \"step\": 0.01},\n\t                ),\n\t                \"supersample\": (\"BOOLEAN\", {\"default\": True}),\n\t                \"resampling\": (\n\t                    [\n\t                        \"nearest\",\n\t                        \"linear\",\n\t                        \"bilinear\",\n\t                        \"bicubic\",\n\t                        \"trilinear\",\n", "                        \"area\",\n\t                        \"nearest-exact\",\n\t                    ],\n\t                    {\"default\": \"nearest\"},\n\t                ),\n\t            },\n\t            \"optional\": {\n\t                \"mask\": (\"MASK\",),\n\t            },\n\t        }\n", "    CATEGORY = \"mtb/image\"\n\t    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n\t    FUNCTION = \"resize\"\n\t    def resize(\n\t        self,\n\t        image: torch.Tensor,\n\t        factor: float,\n\t        supersample: bool,\n\t        resampling: str,\n\t        mask=None,\n", "    ):\n\t        # Check if the tensor has the correct dimension\n\t        if len(image.shape) not in [3, 4]:  # HxWxC or BxHxWxC\n\t            raise ValueError(\"Expected image tensor of shape (H, W, C) or (B, H, W, C)\")\n\t        # Transpose to CxHxW or BxCxHxW for PyTorch\n\t        if len(image.shape) == 3:\n\t            image = image.permute(2, 0, 1).unsqueeze(0)  # CxHxW\n\t        else:\n\t            image = image.permute(0, 3, 1, 2)  # BxCxHxW\n\t        # Compute new dimensions\n", "        B, C, H, W = image.shape\n\t        new_H, new_W = int(H * factor), int(W * factor)\n\t        align_corner_filters = (\"linear\", \"bilinear\", \"bicubic\", \"trilinear\")\n\t        # Resize the image\n\t        resized_image = F.interpolate(\n\t            image,\n\t            size=(new_H, new_W),\n\t            mode=resampling,\n\t            align_corners=resampling in align_corner_filters,\n\t        )\n", "        # Optionally supersample\n\t        if supersample:\n\t            resized_image = F.interpolate(\n\t                resized_image,\n\t                scale_factor=2,\n\t                mode=resampling,\n\t                align_corners=resampling in align_corner_filters,\n\t            )\n\t        # Transpose back to the original format: BxHxWxC or HxWxC\n\t        if len(image.shape) == 4:\n", "            resized_image = resized_image.permute(0, 2, 3, 1)\n\t        else:\n\t            resized_image = resized_image.squeeze(0).permute(1, 2, 0)\n\t        # Apply mask if provided\n\t        if mask is not None:\n\t            if len(mask.shape) != len(resized_image.shape):\n\t                raise ValueError(\n\t                    \"Mask tensor should have the same dimensions as the image tensor\"\n\t                )\n\t            resized_image = resized_image * mask\n", "        return (resized_image,)\n\tclass SaveImageGrid:\n\t    \"\"\"Save all the images in the input batch as a grid of images.\"\"\"\n\t    def __init__(self):\n\t        self.output_dir = folder_paths.get_output_directory()\n\t        self.type = \"output\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n", "                \"images\": (\"IMAGE\",),\n\t                \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyUI\"}),\n\t                \"save_intermediate\": (\"BOOLEAN\", {\"default\": False}),\n\t            },\n\t            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n\t        }\n\t    RETURN_TYPES = ()\n\t    FUNCTION = \"save_images\"\n\t    OUTPUT_NODE = True\n\t    CATEGORY = \"mtb/IO\"\n", "    def create_image_grid(self, image_list):\n\t        total_images = len(image_list)\n\t        # Calculate the grid size based on the square root of the total number of images\n\t        grid_size = (\n\t            int(math.sqrt(total_images)),\n\t            int(math.ceil(math.sqrt(total_images))),\n\t        )\n\t        # Get the size of the first image to determine the grid size\n\t        image_width, image_height = image_list[0].size\n\t        # Create a new blank image to hold the grid\n", "        grid_width = grid_size[0] * image_width\n\t        grid_height = grid_size[1] * image_height\n\t        grid_image = Image.new(\"RGB\", (grid_width, grid_height))\n\t        # Iterate over the images and paste them onto the grid\n\t        for i, image in enumerate(image_list):\n\t            x = (i % grid_size[0]) * image_width\n\t            y = (i // grid_size[0]) * image_height\n\t            grid_image.paste(image, (x, y, x + image_width, y + image_height))\n\t        return grid_image\n\t    def save_images(\n", "        self,\n\t        images,\n\t        filename_prefix=\"Grid\",\n\t        save_intermediate=False,\n\t        prompt=None,\n\t        extra_pnginfo=None,\n\t    ):\n\t        (\n\t            full_output_folder,\n\t            filename,\n", "            counter,\n\t            subfolder,\n\t            filename_prefix,\n\t        ) = folder_paths.get_save_image_path(\n\t            filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0]\n\t        )\n\t        image_list = []\n\t        batch_counter = counter\n\t        metadata = PngInfo()\n\t        if prompt is not None:\n", "            metadata.add_text(\"prompt\", json.dumps(prompt))\n\t        if extra_pnginfo is not None:\n\t            for x in extra_pnginfo:\n\t                metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\t        for idx, image in enumerate(images):\n\t            i = 255.0 * image.cpu().numpy()\n\t            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n\t            image_list.append(img)\n\t            if save_intermediate:\n\t                file = f\"{filename}_batch-{idx:03}_{batch_counter:05}_.png\"\n", "                img.save(\n\t                    os.path.join(full_output_folder, file),\n\t                    pnginfo=metadata,\n\t                    compress_level=4,\n\t                )\n\t            batch_counter += 1\n\t        file = f\"{filename}_{counter:05}_.png\"\n\t        grid = self.create_image_grid(image_list)\n\t        grid.save(\n\t            os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=4\n", "        )\n\t        results = [{\"filename\": file, \"subfolder\": subfolder, \"type\": self.type}]\n\t        return {\"ui\": {\"images\": results}}\n\t__nodes__ = [\n\t    ColorCorrect,\n\t    ImageCompare,\n\t    Blur,\n\t    # DeglazeImage,\n\t    MaskToImage,\n\t    ColoredImage,\n", "    ImagePremultiply,\n\t    ImageResizeFactor,\n\t    SaveImageGrid,\n\t    LoadImageFromUrl,\n\t]\n"]}
{"filename": "nodes/mask.py", "chunked_list": ["from rembg import remove\n\tfrom ..utils import pil2tensor, tensor2pil\n\tfrom PIL import Image\n\timport comfy.utils\n\tclass ImageRemoveBackgroundRembg:\n\t    \"\"\"Removes the background from the input using Rembg.\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n", "                \"image\": (\"IMAGE\",),\n\t                \"alpha_matting\": (\n\t                    \"BOOLEAN\",\n\t                    {\"default\": False},\n\t                ),\n\t                \"alpha_matting_foreground_threshold\": (\n\t                    \"INT\",\n\t                    {\"default\": 240, \"min\": 0, \"max\": 255},\n\t                ),\n\t                \"alpha_matting_background_threshold\": (\n", "                    \"INT\",\n\t                    {\"default\": 10, \"min\": 0, \"max\": 255},\n\t                ),\n\t                \"alpha_matting_erode_size\": (\n\t                    \"INT\",\n\t                    {\"default\": 10, \"min\": 0, \"max\": 255},\n\t                ),\n\t                \"post_process_mask\": (\n\t                    \"BOOLEAN\",\n\t                    {\"default\": False},\n", "                ),\n\t                \"bgcolor\": (\n\t                    \"COLOR\",\n\t                    {\"default\": \"#000000\"},\n\t                ),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\n\t        \"IMAGE\",\n\t        \"MASK\",\n", "        \"IMAGE\",\n\t    )\n\t    RETURN_NAMES = (\n\t        \"Image (rgba)\",\n\t        \"Mask\",\n\t        \"Image\",\n\t    )\n\t    FUNCTION = \"remove_background\"\n\t    CATEGORY = \"mtb/image\"\n\t    # bgcolor: Optional[Tuple[int, int, int, int]]\n", "    def remove_background(\n\t        self,\n\t        image,\n\t        alpha_matting,\n\t        alpha_matting_foreground_threshold,\n\t        alpha_matting_background_threshold,\n\t        alpha_matting_erode_size,\n\t        post_process_mask,\n\t        bgcolor,\n\t    ):\n", "        pbar = comfy.utils.ProgressBar(image.size(0))\n\t        images = tensor2pil(image)\n\t        out_img = []\n\t        out_mask = []\n\t        out_img_on_bg = []\n\t        for img in images:\n\t            img_rm = remove(\n\t                data=img,\n\t                alpha_matting=alpha_matting,\n\t                alpha_matting_foreground_threshold=alpha_matting_foreground_threshold,\n", "                alpha_matting_background_threshold=alpha_matting_background_threshold,\n\t                alpha_matting_erode_size=alpha_matting_erode_size,\n\t                session=None,\n\t                only_mask=False,\n\t                post_process_mask=post_process_mask,\n\t                bgcolor=None,\n\t            )\n\t            # extract the alpha to a new image\n\t            mask = img_rm.getchannel(3)\n\t            # add our bgcolor behind the image\n", "            image_on_bg = Image.new(\"RGBA\", img_rm.size, bgcolor)\n\t            image_on_bg.paste(img_rm, mask=mask)\n\t            out_img.append(img_rm)\n\t            out_mask.append(mask)\n\t            out_img_on_bg.append(image_on_bg)\n\t            pbar.update(1)\n\t        return (pil2tensor(out_img), pil2tensor(out_mask), pil2tensor(out_img_on_bg))\n\t__nodes__ = [\n\t    ImageRemoveBackgroundRembg,\n\t]\n"]}
{"filename": "nodes/graph_utils.py", "chunked_list": ["from ..log import log\n\tfrom PIL import Image\n\timport urllib.request\n\timport urllib.parse\n\timport torch\n\timport json\n\tfrom ..utils import pil2tensor, apply_easing, get_server_info\n\timport io\n\timport numpy as np\n\tdef get_image(filename, subfolder, folder_type):\n", "    log.debug(\n\t        f\"Getting image {filename} from foldertype {folder_type} {f'in subfolder: {subfolder}' if subfolder else ''}\"\n\t    )\n\t    data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n\t    base_url, port = get_server_info()\n\t    url_values = urllib.parse.urlencode(data)\n\t    url = f\"http://{base_url}:{port}/view?{url_values}\"\n\t    log.debug(f\"Fetching image from {url}\")\n\t    with urllib.request.urlopen(url) as response:\n\t        return io.BytesIO(response.read())\n", "class GetBatchFromHistory:\n\t    \"\"\"Very experimental node to load images from the history of the server.\n\t    Queue items without output are ignored in the count.\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"enable\": (\"BOOLEAN\", {\"default\": True}),\n\t                \"count\": (\"INT\", {\"default\": 1, \"min\": 0}),\n\t                \"offset\": (\"INT\", {\"default\": 0, \"min\": -1e9, \"max\": 1e9}),\n", "                \"internal_count\": (\"INT\", {\"default\": 0}),\n\t            },\n\t            \"optional\": {\n\t                \"passthrough_image\": (\"IMAGE\",),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    RETURN_NAMES = (\"images\",)\n\t    CATEGORY = \"mtb/animation\"\n\t    FUNCTION = \"load_from_history\"\n", "    def load_from_history(\n\t        self,\n\t        enable=True,\n\t        count=0,\n\t        offset=0,\n\t        internal_count=0,  # hacky way to invalidate the node\n\t        passthrough_image=None,\n\t    ):\n\t        if not enable or count == 0:\n\t            if passthrough_image is not None:\n", "                log.debug(\"Using passthrough image\")\n\t                return (passthrough_image,)\n\t            log.debug(\"Load from history is disabled for this iteration\")\n\t            return (torch.zeros(0),)\n\t        frames = []\n\t        base_url, port = get_server_info()\n\t        history_url = f\"http://{base_url}:{port}/history\"\n\t        log.debug(f\"Fetching history from {history_url}\")\n\t        output = torch.zeros(0)\n\t        with urllib.request.urlopen(history_url) as response:\n", "            output = self.load_batch_frames(response, offset, count, frames)\n\t        if output.size(0) == 0:\n\t            log.warn(\"No output found in history\")\n\t        return (output,)\n\t    def load_batch_frames(self, response, offset, count, frames):\n\t        history = json.loads(response.read())\n\t        output_images = []\n\t        for run in history.values():\n\t            for node_output in run[\"outputs\"].values():\n\t                if \"images\" in node_output:\n", "                    for image in node_output[\"images\"]:\n\t                        image_data = get_image(\n\t                            image[\"filename\"], image[\"subfolder\"], image[\"type\"]\n\t                        )\n\t                        output_images.append(image_data)\n\t        if not output_images:\n\t            return torch.zeros(0)\n\t        # Directly get desired range of images\n\t        start_index = max(len(output_images) - offset - count, 0)\n\t        end_index = len(output_images) - offset\n", "        selected_images = output_images[start_index:end_index]\n\t        frames = [Image.open(image) for image in selected_images]\n\t        if not frames:\n\t            return torch.zeros(0)\n\t        elif len(frames) != count:\n\t            log.warning(f\"Expected {count} images, got {len(frames)} instead\")\n\t        return pil2tensor(frames)\n\tclass AnyToString:\n\t    \"\"\"Tries to take any input and convert it to a string\"\"\"\n\t    @classmethod\n", "    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\"input\": (\"*\")},\n\t        }\n\t    RETURN_TYPES = (\"STRING\",)\n\t    FUNCTION = \"do_str\"\n\t    CATEGORY = \"mtb/converters\"\n\t    def do_str(self, input):\n\t        if isinstance(input, str):\n\t            return (input,)\n", "        elif isinstance(input, torch.Tensor):\n\t            return (f\"Tensor of shape {input.shape} and dtype {input.dtype}\",)\n\t        elif isinstance(input, Image.Image):\n\t            return (f\"PIL Image of size {input.size} and mode {input.mode}\",)\n\t        elif isinstance(input, np.ndarray):\n\t            return (f\"Numpy array of shape {input.shape} and dtype {input.dtype}\",)\n\t        elif isinstance(input, dict):\n\t            return (f\"Dictionary of {len(input)} items, with keys {input.keys()}\",)\n\t        else:\n\t            log.debug(f\"Falling back to string conversion of {input}\")\n", "            return (str(input),)\n\tclass StringReplace:\n\t    \"\"\"Basic string replacement\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"string\": (\"STRING\", {\"forceInput\": True}),\n\t                \"old\": (\"STRING\", {\"default\": \"\"}),\n\t                \"new\": (\"STRING\", {\"default\": \"\"}),\n", "            }\n\t        }\n\t    FUNCTION = \"replace_str\"\n\t    RETURN_TYPES = (\"STRING\",)\n\t    CATEGORY = \"mtb/string\"\n\t    def replace_str(self, string: str, old: str, new: str):\n\t        log.debug(f\"Current string: {string}\")\n\t        log.debug(f\"Find string: {old}\")\n\t        log.debug(f\"Replace string: {new}\")\n\t        string = string.replace(old, new)\n", "        log.debug(f\"New string: {string}\")\n\t        return (string,)\n\tclass FitNumber:\n\t    \"\"\"Fit the input float using a source and target range\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"value\": (\"FLOAT\", {\"default\": 0, \"forceInput\": True}),\n\t                \"clamp\": (\"BOOLEAN\", {\"default\": False}),\n", "                \"source_min\": (\"FLOAT\", {\"default\": 0.0}),\n\t                \"source_max\": (\"FLOAT\", {\"default\": 1.0}),\n\t                \"target_min\": (\"FLOAT\", {\"default\": 0.0}),\n\t                \"target_max\": (\"FLOAT\", {\"default\": 1.0}),\n\t                \"easing\": (\n\t                    [\n\t                        \"Linear\",\n\t                        \"Sine In\",\n\t                        \"Sine Out\",\n\t                        \"Sine In/Out\",\n", "                        \"Quart In\",\n\t                        \"Quart Out\",\n\t                        \"Quart In/Out\",\n\t                        \"Cubic In\",\n\t                        \"Cubic Out\",\n\t                        \"Cubic In/Out\",\n\t                        \"Circ In\",\n\t                        \"Circ Out\",\n\t                        \"Circ In/Out\",\n\t                        \"Back In\",\n", "                        \"Back Out\",\n\t                        \"Back In/Out\",\n\t                        \"Elastic In\",\n\t                        \"Elastic Out\",\n\t                        \"Elastic In/Out\",\n\t                        \"Bounce In\",\n\t                        \"Bounce Out\",\n\t                        \"Bounce In/Out\",\n\t                    ],\n\t                    {\"default\": \"Linear\"},\n", "                ),\n\t            }\n\t        }\n\t    FUNCTION = \"set_range\"\n\t    RETURN_TYPES = (\"FLOAT\",)\n\t    CATEGORY = \"mtb/math\"\n\t    def set_range(\n\t        self,\n\t        value: float,\n\t        clamp: bool,\n", "        source_min: float,\n\t        source_max: float,\n\t        target_min: float,\n\t        target_max: float,\n\t        easing: str,\n\t    ):\n\t        normalized_value = (value - source_min) / (source_max - source_min)\n\t        eased_value = apply_easing(normalized_value, easing)\n\t        # - Convert the eased value to the target range\n\t        res = target_min + (target_max - target_min) * eased_value\n", "        if clamp:\n\t            if target_min > target_max:\n\t                res = max(min(res, target_min), target_max)\n\t            else:\n\t                res = max(min(res, target_max), target_min)\n\t        return (res,)\n\t__nodes__ = [StringReplace, FitNumber, GetBatchFromHistory, AnyToString]\n"]}
{"filename": "nodes/number.py", "chunked_list": ["class IntToBool:\n\t    \"\"\"Basic int to bool conversion\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"int\": (\n\t                    \"INT\",\n\t                    {\n\t                        \"default\": 0,\n", "                    },\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"BOOLEAN\",)\n\t    FUNCTION = \"int_to_bool\"\n\t    CATEGORY = \"mtb/number\"\n\t    def int_to_bool(self, int):\n\t        return (bool(int),)\n\tclass IntToNumber:\n", "    \"\"\"Node addon for the WAS Suite. Converts a \"comfy\" INT to a NUMBER.\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"int\": (\n\t                    \"INT\",\n\t                    {\n\t                        \"default\": 0,\n\t                        \"min\": -1e9,\n", "                        \"max\": 1e9,\n\t                        \"step\": 1,\n\t                        \"forceInput\": True,\n\t                    },\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"NUMBER\",)\n\t    FUNCTION = \"int_to_number\"\n\t    CATEGORY = \"mtb/number\"\n", "    def int_to_number(self, int):\n\t        return (int,)\n\tclass FloatToNumber:\n\t    \"\"\"Node addon for the WAS Suite. Converts a \"comfy\" FLOAT to a NUMBER.\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"float\": (\n\t                    \"FLOAT\",\n", "                    {\n\t                        \"default\": 0,\n\t                        \"min\": -1e9,\n\t                        \"max\": 1e9,\n\t                        \"step\": 1,\n\t                        \"forceInput\": True,\n\t                    },\n\t                ),\n\t            }\n\t        }\n", "    RETURN_TYPES = (\"NUMBER\",)\n\t    FUNCTION = \"float_to_number\"\n\t    CATEGORY = \"mtb/number\"\n\t    def float_to_number(self, float):\n\t        return (float,)\n\t        return (int,)\n\t__nodes__ = [\n\t    FloatToNumber,\n\t    IntToBool,\n\t    IntToNumber,\n", "]\n"]}
{"filename": "nodes/io.py", "chunked_list": ["from ..utils import tensor2np, PIL_FILTER_MAP\n\timport uuid\n\timport folder_paths\n\tfrom ..log import log\n\timport comfy.model_management as model_management\n\timport subprocess\n\timport torch\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom PIL import Image\n", "from typing import Optional, List\n\tclass ExportWithFfmpeg:\n\t    \"\"\"Export with FFmpeg (Experimental)\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"images\": (\"IMAGE\",),\n\t                # \"frames\": (\"FRAMES\",),\n\t                \"fps\": (\"FLOAT\", {\"default\": 24, \"min\": 1}),\n", "                \"prefix\": (\"STRING\", {\"default\": \"export\"}),\n\t                \"format\": ([\"mov\", \"mp4\", \"mkv\", \"avi\"], {\"default\": \"mov\"}),\n\t                \"codec\": (\n\t                    [\"prores_ks\", \"libx264\", \"libx265\"],\n\t                    {\"default\": \"prores_ks\"},\n\t                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"VIDEO\",)\n\t    OUTPUT_NODE = True\n", "    FUNCTION = \"export_prores\"\n\t    CATEGORY = \"mtb/IO\"\n\t    def export_prores(\n\t        self,\n\t        images: torch.Tensor,\n\t        fps: float,\n\t        prefix: str,\n\t        format: str,\n\t        codec: str,\n\t    ):\n", "        if images.size(0) == 0:\n\t            return (\"\",)\n\t        output_dir = Path(folder_paths.get_output_directory())\n\t        pix_fmt = \"rgb48le\" if codec == \"prores_ks\" else \"yuv420p\"\n\t        file_ext = format\n\t        file_id = f\"{prefix}_{uuid.uuid4()}.{file_ext}\"\n\t        log.debug(f\"Exporting to {output_dir / file_id}\")\n\t        frames = tensor2np(images)\n\t        log.debug(f\"Frames type {type(frames[0])}\")\n\t        log.debug(f\"Exporting {len(frames)} frames\")\n", "        frames = [frame.astype(np.uint16) * 257 for frame in frames]\n\t        height, width, _ = frames[0].shape\n\t        out_path = (output_dir / file_id).as_posix()\n\t        # Prepare the FFmpeg command\n\t        command = [\n\t            \"ffmpeg\",\n\t            \"-y\",\n\t            \"-f\",\n\t            \"rawvideo\",\n\t            \"-vcodec\",\n", "            \"rawvideo\",\n\t            \"-s\",\n\t            f\"{width}x{height}\",\n\t            \"-pix_fmt\",\n\t            pix_fmt,\n\t            \"-r\",\n\t            str(fps),\n\t            \"-i\",\n\t            \"-\",\n\t            \"-c:v\",\n", "            codec,\n\t            \"-r\",\n\t            str(fps),\n\t            \"-y\",\n\t            out_path,\n\t        ]\n\t        process = subprocess.Popen(command, stdin=subprocess.PIPE)\n\t        for frame in frames:\n\t            model_management.throw_exception_if_processing_interrupted()\n\t            process.stdin.write(frame.tobytes())\n", "        process.stdin.close()\n\t        process.wait()\n\t        return (out_path,)\n\tdef prepare_animated_batch(\n\t    batch: torch.Tensor,\n\t    pingpong=False,\n\t    resize_by=1.0,\n\t    resample_filter: Optional[Image.Resampling] = None,\n\t    image_type=np.uint8,\n\t) -> List[Image.Image]:\n", "    images = tensor2np(batch)\n\t    images = [frame.astype(image_type) for frame in images]\n\t    height, width, _ = batch[0].shape\n\t    if pingpong:\n\t        reversed_frames = images[::-1]\n\t        images.extend(reversed_frames)\n\t    pil_images = [Image.fromarray(frame) for frame in images]\n\t    # Resize frames if necessary\n\t    if abs(resize_by - 1.0) > 1e-6:\n\t        new_width = int(width * resize_by)\n", "        new_height = int(height * resize_by)\n\t        pil_images_resized = [\n\t            frame.resize((new_width, new_height), resample=resample_filter)\n\t            for frame in pil_images\n\t        ]\n\t        pil_images = pil_images_resized\n\t    return pil_images\n\t# todo: deprecate for apng\n\tclass SaveGif:\n\t    \"\"\"Save the images from the batch as a GIF\"\"\"\n", "    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"image\": (\"IMAGE\",),\n\t                \"fps\": (\"INT\", {\"default\": 12, \"min\": 1, \"max\": 120}),\n\t                \"resize_by\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.1}),\n\t                \"optimize\": (\"BOOLEAN\", {\"default\": False}),\n\t                \"pingpong\": (\"BOOLEAN\", {\"default\": False}),\n\t            },\n", "            \"optional\": {\n\t                \"resample_filter\": (list(PIL_FILTER_MAP.keys()),),\n\t            },\n\t        }\n\t    RETURN_TYPES = ()\n\t    OUTPUT_NODE = True\n\t    CATEGORY = \"mtb/IO\"\n\t    FUNCTION = \"save_gif\"\n\t    def save_gif(\n\t        self,\n", "        image,\n\t        fps=12,\n\t        resize_by=1.0,\n\t        optimize=False,\n\t        pingpong=False,\n\t        resample_filter=None,\n\t    ):\n\t        if image.size(0) == 0:\n\t            return (\"\",)\n\t        if resample_filter is not None:\n", "            resample_filter = PIL_FILTER_MAP.get(resample_filter)\n\t        pil_images = prepare_animated_batch(\n\t            image,\n\t            pingpong,\n\t            resize_by,\n\t            resample_filter,\n\t        )\n\t        ruuid = uuid.uuid4()\n\t        ruuid = ruuid.hex[:10]\n\t        out_path = f\"{folder_paths.output_directory}/{ruuid}.gif\"\n", "        # Create the GIF from PIL images\n\t        pil_images[0].save(\n\t            out_path,\n\t            save_all=True,\n\t            append_images=pil_images[1:],\n\t            optimize=optimize,\n\t            duration=int(1000 / fps),\n\t            loop=0,\n\t        )\n\t        results = [{\"filename\": f\"{ruuid}.gif\", \"subfolder\": \"\", \"type\": \"output\"}]\n", "        return {\"ui\": {\"gif\": results}}\n\t__nodes__ = [SaveGif, ExportWithFfmpeg]\n"]}
{"filename": "nodes/debug.py", "chunked_list": ["from ..utils import tensor2pil\n\tfrom ..log import log\n\timport io, base64\n\timport torch\n\timport folder_paths\n\tfrom typing import Optional\n\tfrom pathlib import Path\n\tclass Debug:\n\t    \"\"\"Experimental node to debug any Comfy values, support for more types and widgets is planned\"\"\"\n\t    @classmethod\n", "    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\"anything_1\": (\"*\")},\n\t        }\n\t    RETURN_TYPES = (\"STRING\",)\n\t    FUNCTION = \"do_debug\"\n\t    CATEGORY = \"mtb/debug\"\n\t    OUTPUT_NODE = True\n\t    def do_debug(self, **kwargs):\n\t        output = {\n", "            \"ui\": {\"b64_images\": [], \"text\": []},\n\t            \"result\": (\"A\"),\n\t        }\n\t        for k, v in kwargs.items():\n\t            anything = v\n\t            text = \"\"\n\t            if isinstance(anything, torch.Tensor):\n\t                log.debug(f\"Tensor: {anything.shape}\")\n\t                # write the images to temp\n\t                image = tensor2pil(anything)\n", "                b64_imgs = []\n\t                for im in image:\n\t                    buffered = io.BytesIO()\n\t                    im.save(buffered, format=\"PNG\")\n\t                    b64_imgs.append(\n\t                        \"data:image/png;base64,\"\n\t                        + base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\t                    )\n\t                output[\"ui\"][\"b64_images\"] += b64_imgs\n\t                log.debug(f\"Input {k} contains {len(b64_imgs)} images\")\n", "            elif isinstance(anything, bool):\n\t                log.debug(f\"Input {k} contains boolean: {anything}\")\n\t                output[\"ui\"][\"text\"] += [\"True\" if anything else \"False\"]\n\t            else:\n\t                text = str(anything)\n\t                log.debug(f\"Input {k} contains text: {text}\")\n\t                output[\"ui\"][\"text\"] += [text]\n\t        return output\n\tclass SaveTensors:\n\t    \"\"\"Save torch tensors (image, mask or latent) to disk, useful to debug things outside comfy\"\"\"\n", "    def __init__(self):\n\t        self.output_dir = folder_paths.get_output_directory()\n\t        self.type = \"mtb/debug\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyPickle\"}),\n\t            },\n\t            \"optional\": {\n", "                \"image\": (\"IMAGE\",),\n\t                \"mask\": (\"MASK\",),\n\t                \"latent\": (\"LATENT\",),\n\t            },\n\t        }\n\t    FUNCTION = \"save\"\n\t    OUTPUT_NODE = True\n\t    RETURN_TYPES = ()\n\t    CATEGORY = \"mtb/debug\"\n\t    def save(\n", "        self,\n\t        filename_prefix,\n\t        image: Optional[torch.Tensor] = None,\n\t        mask: Optional[torch.Tensor] = None,\n\t        latent: Optional[torch.Tensor] = None,\n\t    ):\n\t        (\n\t            full_output_folder,\n\t            filename,\n\t            counter,\n", "            subfolder,\n\t            filename_prefix,\n\t        ) = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n\t        full_output_folder = Path(full_output_folder)\n\t        if image is not None:\n\t            image_file = f\"{filename}_image_{counter:05}.pt\"\n\t            torch.save(image, full_output_folder / image_file)\n\t            # np.save(full_output_folder/ image_file, image.cpu().numpy())\n\t        if mask is not None:\n\t            mask_file = f\"{filename}_mask_{counter:05}.pt\"\n", "            torch.save(mask, full_output_folder / mask_file)\n\t            # np.save(full_output_folder/ mask_file, mask.cpu().numpy())\n\t        if latent is not None:\n\t            # for latent we must use pickle\n\t            latent_file = f\"{filename}_latent_{counter:05}.pt\"\n\t            torch.save(latent, full_output_folder / latent_file)\n\t            # pickle.dump(latent, open(full_output_folder/ latent_file, \"wb\"))\n\t            # np.save(full_output_folder/ latent_file, latent[\"\"].cpu().numpy())\n\t        return f\"{filename_prefix}_{counter:05}\"\n\t__nodes__ = [Debug, SaveTensors]\n"]}
{"filename": "nodes/conditions.py", "chunked_list": ["from ..utils import here\n\tfrom ..log import log\n\timport folder_paths\n\tfrom pathlib import Path\n\timport shutil\n\timport csv\n\tclass InterpolateClipSequential:\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n", "            \"required\": {\n\t                \"base_text\": (\"STRING\", {\"multiline\": True}),\n\t                \"text_to_replace\": (\"STRING\", {\"default\": \"\"}),\n\t                \"clip\": (\"CLIP\",),\n\t                \"interpolation_strength\": (\n\t                    \"FLOAT\",\n\t                    {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n\t                ),\n\t            }\n\t        }\n", "    RETURN_TYPES = (\"CONDITIONING\",)\n\t    FUNCTION = \"interpolate_encodings_sequential\"\n\t    CATEGORY = \"mtb/conditioning\"\n\t    def interpolate_encodings_sequential(\n\t        self, base_text, text_to_replace, clip, interpolation_strength, **replacements\n\t    ):\n\t        log.debug(f\"Received interpolation_strength: {interpolation_strength}\")\n\t        # - Ensure interpolation strength is within [0, 1]\n\t        interpolation_strength = max(0.0, min(1.0, interpolation_strength))\n\t        # - Check if replacements were provided\n", "        if not replacements:\n\t            raise ValueError(\"At least one replacement should be provided.\")\n\t        num_replacements = len(replacements)\n\t        log.debug(f\"Number of replacements: {num_replacements}\")\n\t        segment_length = 1.0 / num_replacements\n\t        log.debug(f\"Calculated segment_length: {segment_length}\")\n\t        # - Find the segment that the interpolation_strength falls into\n\t        segment_index = min(\n\t            int(interpolation_strength // segment_length), num_replacements - 1\n\t        )\n", "        log.debug(f\"Segment index: {segment_index}\")\n\t        # - Calculate the local strength within the segment\n\t        local_strength = (\n\t            interpolation_strength - (segment_index * segment_length)\n\t        ) / segment_length\n\t        log.debug(f\"Local strength: {local_strength}\")\n\t        # - If it's the first segment, interpolate between base_text and the first replacement\n\t        if segment_index == 0:\n\t            replacement_text = list(replacements.values())[0]\n\t            log.debug(\"Using the base text a the base blend\")\n", "            # -  Start with the base_text condition\n\t            tokens = clip.tokenize(base_text)\n\t            cond_from, pooled_from = clip.encode_from_tokens(tokens, return_pooled=True)\n\t        else:\n\t            base_replace = list(replacements.values())[segment_index - 1]\n\t            log.debug(f\"Using {base_replace} a the base blend\")\n\t            # - Start with the base_text condition replaced by the closest replacement\n\t            tokens = clip.tokenize(base_text.replace(text_to_replace, base_replace))\n\t            cond_from, pooled_from = clip.encode_from_tokens(tokens, return_pooled=True)\n\t            replacement_text = list(replacements.values())[segment_index]\n", "        interpolated_text = base_text.replace(text_to_replace, replacement_text)\n\t        tokens = clip.tokenize(interpolated_text)\n\t        cond_to, pooled_to = clip.encode_from_tokens(tokens, return_pooled=True)\n\t        # - Linearly interpolate between the two conditions\n\t        interpolated_condition = (\n\t            1.0 - local_strength\n\t        ) * cond_from + local_strength * cond_to\n\t        interpolated_pooled = (\n\t            1.0 - local_strength\n\t        ) * pooled_from + local_strength * pooled_to\n", "        return ([[interpolated_condition, {\"pooled_output\": interpolated_pooled}]],)\n\tclass SmartStep:\n\t    \"\"\"Utils to control the steps start/stop of the KAdvancedSampler in percentage\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"step\": (\n\t                    \"INT\",\n\t                    {\"default\": 20, \"min\": 1, \"max\": 10000, \"step\": 1},\n", "                ),\n\t                \"start_percent\": (\n\t                    \"INT\",\n\t                    {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1},\n\t                ),\n\t                \"end_percent\": (\n\t                    \"INT\",\n\t                    {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1},\n\t                ),\n\t            }\n", "        }\n\t    RETURN_TYPES = (\"INT\", \"INT\", \"INT\")\n\t    RETURN_NAMES = (\"step\", \"start\", \"end\")\n\t    FUNCTION = \"do_step\"\n\t    CATEGORY = \"mtb/conditioning\"\n\t    def do_step(self, step, start_percent, end_percent):\n\t        start = int(step * start_percent / 100)\n\t        end = int(step * end_percent / 100)\n\t        return (step, start, end)\n\tdef install_default_styles(force=False):\n", "    styles_dir = Path(folder_paths.base_path) / \"styles\"\n\t    styles_dir.mkdir(parents=True, exist_ok=True)\n\t    default_style = here / \"styles.csv\"\n\t    dest_style = styles_dir / \"default.csv\"\n\t    if force or not dest_style.exists():\n\t        log.debug(f\"Copying default style to {dest_style}\")\n\t        shutil.copy2(default_style.as_posix(), dest_style.as_posix())\n\t    return dest_style\n\tclass StylesLoader:\n\t    \"\"\"Load csv files and populate a dropdown from the rows (à la A111)\"\"\"\n", "    options = {}\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        if not cls.options:\n\t            input_dir = Path(folder_paths.base_path) / \"styles\"\n\t            if not input_dir.exists():\n\t                install_default_styles()\n\t            if not (files := [f for f in input_dir.iterdir() if f.suffix == \".csv\"]):\n\t                log.warn(\n\t                    \"No styles found in the styles folder, place at least one csv file in the styles folder at the root of ComfyUI (for instance ComfyUI/styles/mystyle.csv)\"\n", "                )\n\t            for file in files:\n\t                with open(file, \"r\", encoding=\"utf8\") as f:\n\t                    parsed = csv.reader(f)\n\t                    for row in parsed:\n\t                        log.debug(f\"Adding style {row[0]}\")\n\t                        cls.options[row[0]] = (row[1], row[2])\n\t        else:\n\t            log.debug(f\"Using cached styles (count: {len(cls.options)})\")\n\t        return {\n", "            \"required\": {\n\t                \"style_name\": (list(cls.options.keys()),),\n\t            }\n\t        }\n\t    CATEGORY = \"mtb/conditioning\"\n\t    RETURN_TYPES = (\"STRING\", \"STRING\")\n\t    RETURN_NAMES = (\"positive\", \"negative\")\n\t    FUNCTION = \"load_style\"\n\t    def load_style(self, style_name):\n\t        return (self.options[style_name][0], self.options[style_name][1])\n", "__nodes__ = [SmartStep, StylesLoader, InterpolateClipSequential]\n"]}
{"filename": "nodes/animation.py", "chunked_list": ["from ..log import log\n\tclass AnimationBuilder:\n\t    \"\"\"Convenient way to manage basic animation maths at the core of many of my workflows\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"total_frames\": (\"INT\", {\"default\": 100, \"min\": 0}),\n\t                # \"fps\": (\"INT\", {\"default\": 12, \"min\": 0}),\n\t                \"scale_float\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0}),\n", "                \"loop_count\": (\"INT\", {\"default\": 1, \"min\": 0}),\n\t                \"raw_iteration\": (\"INT\", {\"default\": 0, \"min\": 0}),\n\t                \"raw_loop\": (\"INT\", {\"default\": 0, \"min\": 0}),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\"INT\", \"FLOAT\", \"INT\", \"BOOLEAN\")\n\t    RETURN_NAMES = (\"frame\", \"0-1 (scaled)\", \"count\", \"loop_ended\")\n\t    CATEGORY = \"mtb/animation\"\n\t    FUNCTION = \"build_animation\"\n\t    def build_animation(\n", "        self,\n\t        total_frames=100,\n\t        # fps=12,\n\t        scale_float=1.0,\n\t        loop_count=1,  # set in js\n\t        raw_iteration=0,  # set in js\n\t        raw_loop=0,  # set in js\n\t    ):\n\t        frame = raw_iteration % (total_frames)\n\t        scaled = (frame / (total_frames - 1)) * scale_float\n", "        # if frame == 0:\n\t        #     log.debug(\"Reseting history\")\n\t        #     PromptServer.instance.prompt_queue.wipe_history()\n\t        log.debug(f\"frame: {frame}/{total_frames}  scaled: {scaled}\")\n\t        return (frame, scaled, raw_loop, (frame == (total_frames - 1)))\n\t__nodes__ = [AnimationBuilder]\n"]}
{"filename": "nodes/video.py", "chunked_list": ["import os\n\timport re\n\timport torch\n\timport numpy as np\n\timport hashlib\n\tfrom PIL import Image, ImageOps\n\tfrom PIL.PngImagePlugin import PngInfo\n\timport folder_paths\n\tfrom pathlib import Path\n\timport json\n", "from ..log import log\n\tclass LoadImageSequence:\n\t    \"\"\"Load an image sequence from a folder. The current frame is used to determine which image to load.\n\t    Usually used in conjunction with the `Primitive` node set to increment to load a sequence of images from a folder.\n\t    Use -1 to load all matching frames as a batch.\n\t    \"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n", "                \"path\": (\"STRING\", {\"default\": \"videos/####.png\"}),\n\t                \"current_frame\": (\n\t                    \"INT\",\n\t                    {\"default\": 0, \"min\": -1, \"max\": 9999999},\n\t                ),\n\t            }\n\t        }\n\t    CATEGORY = \"mtb/IO\"\n\t    FUNCTION = \"load_image\"\n\t    RETURN_TYPES = (\n", "        \"IMAGE\",\n\t        \"MASK\",\n\t        \"INT\",\n\t    )\n\t    RETURN_NAMES = (\n\t        \"image\",\n\t        \"mask\",\n\t        \"current_frame\",\n\t    )\n\t    def load_image(self, path=None, current_frame=0):\n", "        load_all = current_frame == -1\n\t        if load_all:\n\t            log.debug(f\"Loading all frames from {path}\")\n\t            frames = resolve_all_frames(path)\n\t            log.debug(f\"Found {len(frames)} frames\")\n\t            imgs = []\n\t            masks = []\n\t            for frame in frames:\n\t                img, mask = img_from_path(frame)\n\t                imgs.append(img)\n", "                masks.append(mask)\n\t            out_img = torch.cat(imgs, dim=0)\n\t            out_mask = torch.cat(masks, dim=0)\n\t            return (\n\t                out_img,\n\t                out_mask,\n\t            )\n\t        log.debug(f\"Loading image: {path}, {current_frame}\")\n\t        print(f\"Loading image: {path}, {current_frame}\")\n\t        resolved_path = resolve_path(path, current_frame)\n", "        image_path = folder_paths.get_annotated_filepath(resolved_path)\n\t        image, mask = img_from_path(image_path)\n\t        return (\n\t            image,\n\t            mask,\n\t            current_frame,\n\t        )\n\t    @staticmethod\n\t    def IS_CHANGED(path=\"\", current_frame=0):\n\t        print(f\"Checking if changed: {path}, {current_frame}\")\n", "        resolved_path = resolve_path(path, current_frame)\n\t        image_path = folder_paths.get_annotated_filepath(resolved_path)\n\t        if os.path.exists(image_path):\n\t            m = hashlib.sha256()\n\t            with open(image_path, \"rb\") as f:\n\t                m.update(f.read())\n\t            return m.digest().hex()\n\t        return \"NONE\"\n\t    # @staticmethod\n\t    # def VALIDATE_INPUTS(path=\"\", current_frame=0):\n", "    #     print(f\"Validating inputs: {path}, {current_frame}\")\n\t    #     resolved_path = resolve_path(path, current_frame)\n\t    #     if not folder_paths.exists_annotated_filepath(resolved_path):\n\t    #         return f\"Invalid image file: {resolved_path}\"\n\t    #     return True\n\timport glob\n\tdef img_from_path(path):\n\t    img = Image.open(path)\n\t    img = ImageOps.exif_transpose(img)\n\t    image = img.convert(\"RGB\")\n", "    image = np.array(image).astype(np.float32) / 255.0\n\t    image = torch.from_numpy(image)[None,]\n\t    if \"A\" in img.getbands():\n\t        mask = np.array(img.getchannel(\"A\")).astype(np.float32) / 255.0\n\t        mask = 1.0 - torch.from_numpy(mask)\n\t    else:\n\t        mask = torch.zeros((64, 64), dtype=torch.float32, device=\"cpu\")\n\t    return (\n\t        image,\n\t        mask,\n", "    )\n\tdef resolve_all_frames(pattern):\n\t    folder_path, file_pattern = os.path.split(pattern)\n\t    log.debug(f\"Resolving all frames in {folder_path}\")\n\t    frames = []\n\t    hash_count = file_pattern.count(\"#\")\n\t    frame_pattern = re.sub(r\"#+\", \"*\", file_pattern)\n\t    log.debug(f\"Found pattern: {frame_pattern}\")\n\t    matching_files = glob.glob(os.path.join(folder_path, frame_pattern))\n\t    log.debug(f\"Found {len(matching_files)} matching files\")\n", "    frame_regex = re.escape(file_pattern).replace(r\"\\#\", r\"(\\d+)\")\n\t    frame_number_regex = re.compile(frame_regex)\n\t    for file in matching_files:\n\t        match = frame_number_regex.search(file)\n\t        if match:\n\t            frame_number = match.group(1)\n\t            log.debug(f\"Found frame number: {frame_number}\")\n\t            # resolved_file = pattern.replace(\"*\" * frame_number.count(\"#\"), frame_number)\n\t            frames.append(file)\n\t    frames.sort()  # Sort frames alphabetically\n", "    return frames\n\tdef resolve_path(path, frame):\n\t    hashes = path.count(\"#\")\n\t    padded_number = str(frame).zfill(hashes)\n\t    return re.sub(\"#+\", padded_number, path)\n\tclass SaveImageSequence:\n\t    \"\"\"Save an image sequence to a folder. The current frame is used to determine which image to save.\n\t    This is merely a wrapper around the `save_images` function with formatting for the output folder and filename.\n\t    \"\"\"\n\t    def __init__(self):\n", "        self.output_dir = folder_paths.get_output_directory()\n\t        self.type = \"output\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n\t                \"images\": (\"IMAGE\",),\n\t                \"filename_prefix\": (\"STRING\", {\"default\": \"Sequence\"}),\n\t                \"current_frame\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 9999999}),\n\t            },\n", "            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n\t        }\n\t    RETURN_TYPES = ()\n\t    FUNCTION = \"save_images\"\n\t    OUTPUT_NODE = True\n\t    CATEGORY = \"mtb/IO\"\n\t    def save_images(\n\t        self,\n\t        images,\n\t        filename_prefix=\"Sequence\",\n", "        current_frame=0,\n\t        prompt=None,\n\t        extra_pnginfo=None,\n\t    ):\n\t        # full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n\t        # results = list()\n\t        # for image in images:\n\t        #     i = 255. * image.cpu().numpy()\n\t        #     img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n\t        #     metadata = PngInfo()\n", "        #     if prompt is not None:\n\t        #         metadata.add_text(\"prompt\", json.dumps(prompt))\n\t        #     if extra_pnginfo is not None:\n\t        #         for x in extra_pnginfo:\n\t        #             metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\t        #     file = f\"{filename}_{counter:05}_.png\"\n\t        #     img.save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=4)\n\t        #     results.append({\n\t        #         \"filename\": file,\n\t        #         \"subfolder\": subfolder,\n", "        #         \"type\": self.type\n\t        #     })\n\t        #     counter += 1\n\t        if len(images) > 1:\n\t            raise ValueError(\"Can only save one image at a time\")\n\t        resolved_path = Path(self.output_dir) / filename_prefix\n\t        resolved_path.mkdir(parents=True, exist_ok=True)\n\t        resolved_img = resolved_path / f\"{filename_prefix}_{current_frame:05}.png\"\n\t        output_image = images[0].cpu().numpy()\n\t        img = Image.fromarray(np.clip(output_image * 255.0, 0, 255).astype(np.uint8))\n", "        metadata = PngInfo()\n\t        if prompt is not None:\n\t            metadata.add_text(\"prompt\", json.dumps(prompt))\n\t        if extra_pnginfo is not None:\n\t            for x in extra_pnginfo:\n\t                metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\t        img.save(resolved_img, pnginfo=metadata, compress_level=4)\n\t        return {\n\t            \"ui\": {\n\t                \"images\": [\n", "                    {\n\t                        \"filename\": resolved_img.name,\n\t                        \"subfolder\": resolved_path.name,\n\t                        \"type\": self.type,\n\t                    }\n\t                ]\n\t            }\n\t        }\n\t__nodes__ = [\n\t    LoadImageSequence,\n", "    SaveImageSequence,\n\t]\n"]}
{"filename": "nodes/generate.py", "chunked_list": ["import qrcode\n\tfrom ..utils import pil2tensor\n\tfrom ..utils import comfy_dir\n\tfrom typing import cast\n\tfrom PIL import Image\n\tfrom ..log import log\n\t# class MtbExamples:\n\t#     \"\"\"MTB Example Images\"\"\"\n\t#     def __init__(self):\n\t#         pass\n", "#     @classmethod\n\t#     @lru_cache(maxsize=1)\n\t#     def get_root(cls):\n\t#         return here / \"examples\" / \"samples\"\n\t#     @classmethod\n\t#     def INPUT_TYPES(cls):\n\t#         input_dir = cls.get_root()\n\t#         files = [f.name for f in input_dir.iterdir() if f.is_file()]\n\t#         return {\n\t#             \"required\": {\"image\": (sorted(files),)},\n", "#         }\n\t#     RETURN_TYPES = (\"IMAGE\", \"MASK\")\n\t#     FUNCTION = \"do_mtb_examples\"\n\t#     CATEGORY = \"fun\"\n\t#     def do_mtb_examples(self, image, index):\n\t#         image_path = (self.get_root() / image).as_posix()\n\t#         i = Image.open(image_path)\n\t#         i = ImageOps.exif_transpose(i)\n\t#         image = i.convert(\"RGB\")\n\t#         image = np.array(image).astype(np.float32) / 255.0\n", "#         image = torch.from_numpy(image)[None,]\n\t#         if \"A\" in i.getbands():\n\t#             mask = np.array(i.getchannel(\"A\")).astype(np.float32) / 255.0\n\t#             mask = 1.0 - torch.from_numpy(mask)\n\t#         else:\n\t#             mask = torch.zeros((64, 64), dtype=torch.float32, device=\"cpu\")\n\t#         return (image, mask)\n\t#     @classmethod\n\t#     def IS_CHANGED(cls, image):\n\t#         image_path = (cls.get_root() / image).as_posix()\n", "#         m = hashlib.sha256()\n\t#         with open(image_path, \"rb\") as f:\n\t#             m.update(f.read())\n\t#         return m.digest().hex()\n\tclass UnsplashImage:\n\t    \"\"\"Unsplash Image given a keyword and a size\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n\t            \"required\": {\n", "                \"width\": (\"INT\", {\"default\": 512, \"max\": 8096, \"min\": 0, \"step\": 1}),\n\t                \"height\": (\"INT\", {\"default\": 512, \"max\": 8096, \"min\": 0, \"step\": 1}),\n\t                \"random_seed\": (\"INT\", {\"default\": 0, \"max\": 1e5, \"min\": 0, \"step\": 1}),\n\t            },\n\t            \"optional\": {\n\t                \"keyword\": (\"STRING\", {\"default\": \"nature\"}),\n\t            },\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"do_unsplash_image\"\n", "    CATEGORY = \"mtb/generate\"\n\t    def do_unsplash_image(self, width, height, random_seed, keyword=None):\n\t        import requests\n\t        import io\n\t        base_url = \"https://source.unsplash.com/random/\"\n\t        if width and height:\n\t            base_url += f\"/{width}x{height}\"\n\t        if keyword:\n\t            keyword = keyword.replace(\" \", \"%20\")\n\t            base_url += f\"?{keyword}&{random_seed}\"\n", "        else:\n\t            base_url += f\"?&{random_seed}\"\n\t        try:\n\t            log.debug(f\"Getting unsplash image from {base_url}\")\n\t            response = requests.get(base_url)\n\t            response.raise_for_status()\n\t            image = Image.open(io.BytesIO(response.content))\n\t            return (\n\t                pil2tensor(\n\t                    image,\n", "                ),\n\t            )\n\t        except requests.exceptions.RequestException as e:\n\t            print(\"Error retrieving image:\", e)\n\t            return (None,)\n\tclass QrCode:\n\t    \"\"\"Basic QR Code generator\"\"\"\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        return {\n", "            \"required\": {\n\t                \"url\": (\"STRING\", {\"default\": \"https://www.github.com\"}),\n\t                \"width\": (\n\t                    \"INT\",\n\t                    {\"default\": 256, \"max\": 8096, \"min\": 0, \"step\": 1},\n\t                ),\n\t                \"height\": (\n\t                    \"INT\",\n\t                    {\"default\": 256, \"max\": 8096, \"min\": 0, \"step\": 1},\n\t                ),\n", "                \"error_correct\": ((\"L\", \"M\", \"Q\", \"H\"), {\"default\": \"L\"}),\n\t                \"box_size\": (\"INT\", {\"default\": 10, \"max\": 8096, \"min\": 0, \"step\": 1}),\n\t                \"border\": (\"INT\", {\"default\": 4, \"max\": 8096, \"min\": 0, \"step\": 1}),\n\t                \"invert\": ((\"BOOLEAN\",), {\"default\": False}),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    FUNCTION = \"do_qr\"\n\t    CATEGORY = \"mtb/generate\"\n\t    def do_qr(self, url, width, height, error_correct, box_size, border, invert):\n", "        log.warning(\n\t            \"This node will soon be deprecated, there are much better alternatives like https://github.com/coreyryanhanson/comfy-qr\"\n\t        )\n\t        if error_correct == \"L\" or error_correct not in [\"M\", \"Q\", \"H\"]:\n\t            error_correct = qrcode.constants.ERROR_CORRECT_L\n\t        elif error_correct == \"M\":\n\t            error_correct = qrcode.constants.ERROR_CORRECT_M\n\t        elif error_correct == \"Q\":\n\t            error_correct = qrcode.constants.ERROR_CORRECT_Q\n\t        else:\n", "            error_correct = qrcode.constants.ERROR_CORRECT_H\n\t        qr = qrcode.QRCode(\n\t            version=1,\n\t            error_correction=error_correct,\n\t            box_size=box_size,\n\t            border=border,\n\t        )\n\t        qr.add_data(url)\n\t        qr.make(fit=True)\n\t        back_color = (255, 255, 255) if invert else (0, 0, 0)\n", "        fill_color = (0, 0, 0) if invert else (255, 255, 255)\n\t        code = img = qr.make_image(back_color=back_color, fill_color=fill_color)\n\t        # that we now resize without filtering\n\t        code = code.resize((width, height), Image.NEAREST)\n\t        return (pil2tensor(code),)\n\tdef bbox_dim(bbox):\n\t    left, upper, right, lower = bbox\n\t    width = right - left\n\t    height = lower - upper\n\t    return width, height\n", "class TextToImage:\n\t    \"\"\"Utils to convert text to image using a font\n\t    The tool looks for any .ttf file in the Comfy folder hierarchy.\n\t    \"\"\"\n\t    fonts = {}\n\t    def __init__(self):\n\t        # - This is executed when the graph is executed, we could conditionaly reload fonts there\n\t        pass\n\t    @classmethod\n\t    def CACHE_FONTS(cls):\n", "        font_extensions = [\"*.ttf\", \"*.otf\", \"*.woff\", \"*.woff2\", \"*.eot\"]\n\t        fonts = []\n\t        for extension in font_extensions:\n\t            fonts.extend(comfy_dir.glob(f\"**/{extension}\"))\n\t        if not fonts:\n\t            log.warn(\n\t                \"> No fonts found in the comfy folder, place at least one font file somewhere in ComfyUI's hierarchy\"\n\t            )\n\t        else:\n\t            log.debug(f\"> Found {len(fonts)} fonts\")\n", "        for font in fonts:\n\t            log.debug(f\"Adding font {font}\")\n\t            cls.fonts[font.stem] = font.as_posix()\n\t    @classmethod\n\t    def INPUT_TYPES(cls):\n\t        if not cls.fonts:\n\t            cls.CACHE_FONTS()\n\t        else:\n\t            log.debug(f\"Using cached fonts (count: {len(cls.fonts)})\")\n\t        return {\n", "            \"required\": {\n\t                \"text\": (\n\t                    \"STRING\",\n\t                    {\"default\": \"Hello world!\"},\n\t                ),\n\t                \"font\": ((sorted(cls.fonts.keys())),),\n\t                \"wrap\": (\n\t                    \"INT\",\n\t                    {\"default\": 120, \"min\": 0, \"max\": 8096, \"step\": 1},\n\t                ),\n", "                \"font_size\": (\n\t                    \"INT\",\n\t                    {\"default\": 12, \"min\": 1, \"max\": 2500, \"step\": 1},\n\t                ),\n\t                \"width\": (\n\t                    \"INT\",\n\t                    {\"default\": 512, \"min\": 1, \"max\": 8096, \"step\": 1},\n\t                ),\n\t                \"height\": (\n\t                    \"INT\",\n", "                    {\"default\": 512, \"min\": 1, \"max\": 8096, \"step\": 1},\n\t                ),\n\t                # \"position\": ([\"INT\"], {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1}),\n\t                \"color\": (\n\t                    \"COLOR\",\n\t                    {\"default\": \"black\"},\n\t                ),\n\t                \"background\": (\n\t                    \"COLOR\",\n\t                    {\"default\": \"white\"},\n", "                ),\n\t            }\n\t        }\n\t    RETURN_TYPES = (\"IMAGE\",)\n\t    RETURN_NAMES = (\"image\",)\n\t    FUNCTION = \"text_to_image\"\n\t    CATEGORY = \"mtb/generate\"\n\t    def text_to_image(\n\t        self, text, font, wrap, font_size, width, height, color, background\n\t    ):\n", "        from PIL import Image, ImageDraw, ImageFont\n\t        import textwrap\n\t        font = self.fonts[font]\n\t        font = cast(ImageFont.FreeTypeFont, ImageFont.truetype(font, font_size))\n\t        if wrap == 0:\n\t            wrap = width / font_size\n\t        lines = textwrap.wrap(text, width=wrap)\n\t        log.debug(f\"Lines: {lines}\")\n\t        line_height = bbox_dim(font.getbbox(\"hg\"))[1]\n\t        img_height = height  # line_height * len(lines)\n", "        img_width = width  # max(font.getsize(line)[0] for line in lines)\n\t        img = Image.new(\"RGBA\", (img_width, img_height), background)\n\t        draw = ImageDraw.Draw(img)\n\t        y_text = 0\n\t        # - bbox is [left, upper, right, lower]\n\t        for line in lines:\n\t            width, height = bbox_dim(font.getbbox(line))\n\t            draw.text((0, y_text), line, color, font=font)\n\t            y_text += height\n\t        # img.save(os.path.join(folder_paths.base_path, f'{str(uuid.uuid4())}.png'))\n", "        return (pil2tensor(img),)\n\t__nodes__ = [\n\t    QrCode,\n\t    UnsplashImage,\n\t    TextToImage\n\t    #  MtbExamples,\n\t]\n"]}
{"filename": "scripts/download_models.py", "chunked_list": ["import os\n\timport requests\n\tfrom rich.console import Console\n\tfrom tqdm import tqdm\n\timport subprocess\n\timport sys\n\ttry:\n\t    import folder_paths\n\texcept ModuleNotFoundError:\n\t    import sys\n", "    sys.path.append(os.path.join(os.path.dirname(__file__), \"../../..\"))\n\t    import folder_paths\n\tmodels_to_download = {\n\t    \"DeepBump\": {\n\t        \"size\": 25.5,\n\t        \"download_url\": \"https://github.com/HugoTini/DeepBump/raw/master/deepbump256.onnx\",\n\t        \"destination\": \"deepbump\",\n\t    },\n\t    \"Face Swap\": {\n\t        \"size\": 660,\n", "        \"download_url\": [\n\t            \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_mobilenet0.25_Final.pth\",\n\t            \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth\",\n\t            \"https://huggingface.co/deepinsight/inswapper/resolve/main/inswapper_128.onnx\",\n\t        ],\n\t        \"destination\": \"insightface\",\n\t    },\n\t    \"GFPGAN (face enhancement)\": {\n\t        \"size\": 332,\n\t        \"download_url\": [\n", "            \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\",\n\t            \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\n\t            # TODO: provide a way to selectively download models from \"packs\"\n\t            # https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth\n\t            # https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\n\t            # https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth\n\t        ],\n\t        \"destination\": \"face_restore\",\n\t    },\n\t    \"FILM: Frame Interpolation for Large Motion\": {\n", "        \"size\": 402,\n\t        \"download_url\": [\n\t            \"https://drive.google.com/drive/folders/131_--QrieM4aQbbLWrUtbO2cGbX8-war\"\n\t        ],\n\t        \"destination\": \"FILM\",\n\t    },\n\t}\n\tconsole = Console()\n\tfrom urllib.parse import urlparse\n\tfrom pathlib import Path\n", "def download_model(download_url, destination):\n\t    if isinstance(download_url, list):\n\t        for url in download_url:\n\t            download_model(url, destination)\n\t        return\n\t    filename = os.path.basename(urlparse(download_url).path)\n\t    response = None\n\t    if \"drive.google.com\" in download_url:\n\t        try:\n\t            import gdown\n", "        except ImportError:\n\t            print(\"Installing gdown\")\n\t            subprocess.check_call(\n\t                [\n\t                    sys.executable,\n\t                    \"-m\",\n\t                    \"pip\",\n\t                    \"install\",\n\t                    \"git+https://github.com/melMass/gdown@main\",\n\t                ]\n", "            )\n\t            import gdown\n\t        if \"/folders/\" in download_url:\n\t            # download folder\n\t            try:\n\t                gdown.download_folder(download_url, output=destination, resume=True)\n\t            except TypeError:\n\t                gdown.download_folder(download_url, output=destination)\n\t            return\n\t        # download from google drive\n", "        gdown.download(download_url, destination, quiet=False, resume=True)\n\t        return\n\t    response = requests.get(download_url, stream=True)\n\t    total_size = int(response.headers.get(\"content-length\", 0))\n\t    destination_path = os.path.join(destination, filename)\n\t    with open(destination_path, \"wb\") as file:\n\t        with tqdm(\n\t            total=total_size, unit=\"B\", unit_scale=True, desc=destination_path, ncols=80\n\t        ) as progress_bar:\n\t            for data in response.iter_content(chunk_size=4096):\n", "                file.write(data)\n\t                progress_bar.update(len(data))\n\t    console.print(\n\t        f\"Downloaded model from {download_url} to {destination_path}\",\n\t        style=\"bold green\",\n\t    )\n\tdef ask_user_for_downloads(models_to_download):\n\t    console.print(\"Choose models to download:\")\n\t    choices = {}\n\t    for i, model_name in enumerate(models_to_download.keys(), start=1):\n", "        choices[str(i)] = model_name\n\t        console.print(f\"{i}. {model_name}\")\n\t    console.print(\n\t        \"Enter the numbers of the models you want to download (comma-separated):\"\n\t    )\n\t    user_input = console.input(\">> \")\n\t    selected_models = user_input.split(\",\")\n\t    models_to_download_selected = {}\n\t    for choice in selected_models:\n\t        choice = choice.strip()\n", "        if choice in choices:\n\t            model_name = choices[choice]\n\t            models_to_download_selected[model_name] = models_to_download[model_name]\n\t        elif choice == \"\":\n\t            # download all\n\t            models_to_download_selected = models_to_download\n\t        else:\n\t            console.print(f\"Invalid choice: {choice}. Skipping.\")\n\t    return models_to_download_selected\n\tdef handle_interrupt():\n", "    console.print(\"Interrupted by user.\", style=\"bold red\")\n\tdef main(models_to_download, skip_input=False):\n\t    try:\n\t        models_to_download_selected = {}\n\t        def check_destination(urls, destination):\n\t            if isinstance(urls, list):\n\t                for url in urls:\n\t                    check_destination(url, destination)\n\t                return\n\t            filename = os.path.basename(urlparse(urls).path)\n", "            destination = os.path.join(folder_paths.models_dir, destination)\n\t            if not os.path.exists(destination):\n\t                os.makedirs(destination)\n\t            destination_path = os.path.join(destination, filename)\n\t            if os.path.exists(destination_path):\n\t                url_name = os.path.basename(urlparse(urls).path)\n\t                console.print(\n\t                    f\"Checkpoint '{url_name}' for {model_name} already exists in '{destination}'\"\n\t                )\n\t            else:\n", "                model_details[\"destination\"] = destination\n\t                models_to_download_selected[model_name] = model_details\n\t        for model_name, model_details in models_to_download.items():\n\t            destination = model_details[\"destination\"]\n\t            download_url = model_details[\"download_url\"]\n\t            check_destination(download_url, destination)\n\t        if not models_to_download_selected:\n\t            console.print(\"No new models to download.\")\n\t            return\n\t        models_to_download_selected = (\n", "            ask_user_for_downloads(models_to_download_selected)\n\t            if not skip_input\n\t            else models_to_download_selected\n\t        )\n\t        for model_name, model_details in models_to_download_selected.items():\n\t            download_url = model_details[\"download_url\"]\n\t            destination = model_details[\"destination\"]\n\t            console.print(f\"Downloading {model_name}...\")\n\t            download_model(download_url, destination)\n\t    except KeyboardInterrupt:\n", "        handle_interrupt()\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-y\", \"--yes\", action=\"store_true\", help=\"skip user input\")\n\t    args = parser.parse_args()\n\t    main(models_to_download, args.yes)\n"]}
{"filename": "scripts/comfy_meta.py", "chunked_list": ["import argparse\n\timport json\n\tfrom PIL import Image, PngImagePlugin\n\tfrom rich.console import Console\n\tfrom rich import print\n\tfrom rich_argparse import RichHelpFormatter\n\timport os\n\tfrom pathlib import Path\n\tconsole = Console()\n\t# BNK_CutoffSetRegions\n", "# BNK_CutoffRegionsToConditioning\n\t# BNK_CutoffBasePrompt\n\t# Extracts metadata from a PNG image and returns it as a dictionary\n\tdef extract_metadata(image_path):\n\t    image = Image.open(image_path)\n\t    prompt = image.info.get(\"prompt\", \"\")\n\t    workflow = image.info.get(\"workflow\", \"\")\n\t    if workflow:\n\t        workflow = json.loads(workflow)\n\t    if prompt:\n", "        prompt = json.loads(prompt)\n\t    console.print(f\"Metadata extracted from [cyan]{image_path}[/cyan].\")\n\t    return {\n\t        \"prompt\": prompt,\n\t        \"workflow\": workflow,\n\t    }\n\t# Embeds metadata into a PNG image\n\tdef embed_metadata(image_path, metadata):\n\t    image = Image.open(image_path)\n\t    o_metadata = image.info\n", "    pnginfo = PngImagePlugin.PngInfo()\n\t    if prompt := metadata.get(\"prompt\"):\n\t        pnginfo.add_text(\"prompt\", json.dumps(prompt))\n\t    elif \"prompt\" in o_metadata:\n\t        pnginfo.add_text(\"prompt\", o_metadata[\"prompt\"])\n\t    if workflow := metadata.get(\"workflow\"):\n\t        pnginfo.add_text(\"workflow\", json.dumps(workflow))\n\t    elif \"workflow\" in o_metadata:\n\t        pnginfo.add_text(\"workflow\", o_metadata[\"workflow\"])\n\t    imgp = Path(image_path)\n", "    output = imgp.with_stem(f\"{imgp.stem}_comfy_embed\")\n\t    index = 1\n\t    while output.exists():\n\t        output = imgp.with_stem(f\"{imgp.stem}_{index}_comfy_embed\").with_suffix(\".png\")\n\t        index += 1\n\t    image.save(output, pnginfo=pnginfo)\n\t    console.print(f\"Metadata embedded into [cyan]{output}[/cyan].\")\n\t# CLI subcommand: extract\n\tdef extract(args):\n\t    input_files = []\n", "    for input_path in args.input:\n\t        if os.path.isdir(input_path):\n\t            folder_path = input_path\n\t            input_files.extend(\n\t                [\n\t                    os.path.join(folder_path, file_name)\n\t                    for file_name in os.listdir(folder_path)\n\t                    if file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n\t                ]\n\t            )\n", "        else:\n\t            input_files.append(input_path)\n\t    if len(input_files) == 1:\n\t        metadata = extract_metadata(input_files[0])\n\t        if args.print_output:\n\t            print(json.dumps(metadata, indent=4))\n\t        else:\n\t            if not args.output:\n\t                output = Path(input_files[0]).with_suffix(\".json\")\n\t                index = 1\n", "                while output.exists():\n\t                    output = (\n\t                        Path(input_files[0])\n\t                        .with_stem(f\"{Path(input_files[0]).stem}_{index}\")\n\t                        .with_suffix(\".json\")\n\t                    )\n\t                    index += 1\n\t            else:\n\t                output = args.output\n\t            with open(output, \"w\") as file:\n", "                json.dump(metadata, file, indent=4)\n\t            console.print(f\"Metadata extracted and saved to [cyan]{output}[/cyan].\")\n\t    else:\n\t        metadata_dict = {}\n\t        for input_file in input_files:\n\t            metadata = extract_metadata(input_file)\n\t            filename = os.path.basename(input_file)\n\t            output = (\n\t                Path(args.output) / f\"{filename}.json\"\n\t                if args.output\n", "                else Path(input_file).with_suffix(\".json\")\n\t            )\n\t            index = 1\n\t            while output.exists():\n\t                output = Path(args.output).parent / f\"{filename}_{index}.json\"\n\t                index += 1\n\t            with open(output, \"w\") as file:\n\t                json.dump(metadata, file, indent=4)\n\t            metadata_dict[filename] = metadata\n\t        if args.output:\n", "            with open(args.output, \"w\") as file:\n\t                json.dump(metadata_dict, file, indent=4)\n\t            console.print(\n\t                f\"Metadata extracted and saved to [cyan]{args.output}[/cyan].\"\n\t            )\n\t        else:\n\t            console.print(\"Multiple metadata files created.\")\n\t# CLI subcommand: embed\n\tdef embed(args):\n\t    input_files = []\n", "    for input_path in args.input:\n\t        if os.path.isdir(input_path):\n\t            folder_path = input_path\n\t            input_files.extend(\n\t                [\n\t                    os.path.join(folder_path, file_name)\n\t                    for file_name in os.listdir(folder_path)\n\t                    if file_name.lower().endswith(\".json\")\n\t                ]\n\t            )\n", "        else:\n\t            input_files.append(input_path)\n\t    for input_file in input_files:\n\t        with open(input_file) as file:\n\t            metadata = json.load(file)\n\t        image_path = input_file.replace(\".json\", \".png\")\n\t        if args.output:\n\t            output_dir = args.output\n\t            if os.path.isdir(output_dir):\n\t                output_path = os.path.join(output_dir, os.path.basename(image_path))\n", "                index = 1\n\t                while os.path.exists(output_path):\n\t                    output_path = os.path.join(\n\t                        output_dir,\n\t                        f\"{os.path.basename(image_path)}_{index}.png\",\n\t                    )\n\t                    index += 1\n\t            else:\n\t                output_path = output_dir\n\t        else:\n", "            output_path = image_path.replace(\".png\", \"_comfy_embed.png\")\n\t        embed_metadata(image_path, metadata)\n\t        # os.rename(image_path, output_path)\n\t        console.print(f\"Metadata embedded into [cyan]{output_path}[/cyan].\")\n\tif __name__ == \"__main__\":\n\t    # Create the main CLI parser\n\t    parser = argparse.ArgumentParser(\n\t        prog=\"image-metadata-cli\", formatter_class=RichHelpFormatter\n\t    )\n\t    subparsers = parser.add_subparsers(title=\"subcommands\")\n", "    # Parser for the \"extract\" subcommand\n\t    extract_parser = subparsers.add_parser(\n\t        \"extract\",\n\t        help=\"Extract metadata from PNG image(s) or folder\",\n\t        formatter_class=RichHelpFormatter,\n\t    )\n\t    extract_parser.add_argument(\n\t        \"input\", nargs=\"+\", help=\"Input PNG image file(s) or folder path\"\n\t    )\n\t    extract_parser.add_argument(\n", "        \"--print\",\n\t        dest=\"print_output\",\n\t        action=\"store_true\",\n\t        help=\"Print the output to stdout\",\n\t    )\n\t    extract_parser.add_argument(\"--output\", help=\"Output JSON file(s) or directory\")\n\t    extract_parser.set_defaults(func=extract)\n\t    # Parser for the \"embed\" subcommand\n\t    embed_parser = subparsers.add_parser(\n\t        \"embed\",\n", "        help=\"Embed metadata into PNG image(s) or folder\",\n\t        formatter_class=RichHelpFormatter,\n\t    )\n\t    embed_parser.add_argument(\n\t        \"input\", nargs=\"+\", help=\"Input JSON file(s) or folder path\"\n\t    )\n\t    embed_parser.add_argument(\"--output\", help=\"Output PNG image file(s) or directory\")\n\t    embed_parser.set_defaults(func=embed)\n\t    # Parse the command-line arguments and execute the appropriate subcommand\n\t    args = parser.parse_args()\n", "    if hasattr(args, \"func\"):\n\t        try:\n\t            args.func(args)\n\t        except ValueError as e:\n\t            console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n\t    else:\n\t        parser.print_help()\n"]}
{"filename": "scripts/a111_extract.py", "chunked_list": ["from pathlib import Path\n\tfrom PIL import Image\n\tfrom PIL.PngImagePlugin import PngImageFile, PngInfo\n\timport json\n\tfrom pprint import pprint\n\timport argparse\n\tfrom rich.console import Console\n\tfrom rich.progress import Progress\n\tfrom rich_argparse import RichHelpFormatter\n\tdef parse_a111(params, verbose=False):\n", "    # params = [p.split(\": \") for p in params.split(\"\\n\")]\n\t    params = params.split(\"\\n\")\n\t    prompt = params[0].strip()\n\t    neg = params[1].split(\":\")[1].strip()\n\t    settings = {}\n\t    try:\n\t        settings = {\n\t            s.split(\":\")[0].strip(): s.split(\":\")[1].strip()\n\t            for s in params[2].split(\",\")\n\t        }\n", "    except IndexError:\n\t        settings = {\"raw\": params[2].strip()}\n\t    if verbose:\n\t        print(f\"PROMPT: {prompt}\")\n\t        print(f\"NEG: {neg}\")\n\t        print(\"SETTINGS:\")\n\t        pprint(settings, indent=4)\n\t    return {\"prompt\": prompt, \"negative\": neg, \"settings\": settings}\n\timport glob\n\tif __name__ == \"__main__\":\n", "    parser = argparse.ArgumentParser(\n\t        description=\"Crude metadata extractor from A111 pngs\",\n\t        formatter_class=RichHelpFormatter\n\t    )\n\t    parser.add_argument(\"inputs\", nargs=\"*\", help=\"Input image files\")\n\t    parser.add_argument(\"--output\", help=\"Output JSON file\")\n\t    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose mode\")\n\t    parser.add_argument(\n\t        \"--glob\", help=\"Enable glob pattern matching\", metavar=\"PATTERN\"\n\t    )\n", "    args = parser.parse_args()\n\t    # - checks\n\t    if not args.glob and not args.inputs:\n\t        parser.error(\"Either --glob flag or inputs must be provided.\")\n\t    if args.glob:\n\t        glob_pattern = args.glob\n\t        try:\n\t            pattern_path = str(Path(glob_pattern).expanduser().resolve())\n\t            if not any(glob.glob(pattern_path)):\n\t                raise ValueError(f\"No files found for glob pattern: {glob_pattern}\")\n", "        except Exception as e:\n\t            console = Console()\n\t            console.print(\n\t                f\"[bold red]Error: Invalid glob pattern '{glob_pattern}': {e}[/bold red]\"\n\t            )\n\t            exit(1)\n\t    else:\n\t        glob_pattern = None\n\t    input_files = []\n\t    if glob_pattern:\n", "        input_files = list(glob.glob(str(Path(glob_pattern).expanduser().resolve())))\n\t    else:\n\t        input_files = [Path(p) for p in args.inputs]\n\t    console = Console()\n\t    console.print(\"Input Files:\", style=\"bold\", end=\" \")\n\t    console.print(f\"{len(input_files):03d} files\", style=\"cyan\")\n\t    # for input_file in args.inputs:\n\t    #     console.print(f\"- {input_file}\", style=\"cyan\")\n\t    console.print(\"\\nOutput File:\", style=\"bold\", end=\" \")\n\t    console.print(f\"{Path(args.output).resolve().absolute()}\", style=\"cyan\")\n", "    with Progress(console=console, auto_refresh=True) as progress:\n\t        # files = Path(pth).rglob(\"*.png\")\n\t        unique_info = {}\n\t        last = None\n\t        task = progress.add_task(\"[cyan]Extracting meta...\", total=len(input_files) + 1)\n\t        for p in input_files:\n\t            im = Image.open(p)\n\t            parsed = parse_a111(im.info[\"parameters\"], args.verbose)\n\t            if parsed != last:\n\t                unique_info[Path(p).stem] = parsed\n", "            last = parsed\n\t            progress.update(task, advance=1)\n\t            progress.refresh()\n\t        unique_info = json.dumps(unique_info, indent=4)\n\t        with open(args.output, \"w\") as f:\n\t            f.write(unique_info)\n\t            progress.update(task, advance=1)\n\t            progress.refresh()\n\t    console.print(\"\\nProcessing completed!\", style=\"bold green\")\n"]}
{"filename": "scripts/interpolate_frames.py", "chunked_list": ["import glob\n\tfrom pathlib import Path\n\timport uuid\n\timport sys\n\tfrom typing import List\n\tsys.path.append((Path(__file__).parent / \"extern\").as_posix())\n\timport argparse\n\tfrom rich_argparse import RichHelpFormatter\n\tfrom rich.console import Console\n\tfrom rich.progress import Progress\n", "import numpy as np\n\timport subprocess\n\tdef write_prores_444_video(output_file, frames: List[np.ndarray], fps):\n\t    # Convert float images to the range of 0-65535 (12-bit color depth)\n\t    frames = [(frame * 65535).clip(0, 65535).astype(np.uint16) for frame in frames]\n\t    height, width, _ = frames[0].shape\n\t    # Prepare the FFmpeg command\n\t    command = [\n\t        \"ffmpeg\",\n\t        \"-y\",  # Overwrite output file if it already exists\n", "        \"-f\",\n\t        \"rawvideo\",\n\t        \"-vcodec\",\n\t        \"rawvideo\",\n\t        \"-s\",\n\t        f\"{width}x{height}\",\n\t        \"-pix_fmt\",\n\t        \"rgb48le\",\n\t        \"-r\",\n\t        str(fps),\n", "        \"-i\",\n\t        \"-\",\n\t        \"-c:v\",\n\t        \"prores_ks\",\n\t        \"-profile:v\",\n\t        \"4\",\n\t        \"-pix_fmt\",\n\t        \"yuva444p10le\",\n\t        \"-r\",\n\t        str(fps),\n", "        \"-y\",  # Overwrite output file if it already exists\n\t        output_file,\n\t    ]\n\t    process = subprocess.Popen(command, stdin=subprocess.PIPE)\n\t    for frame in frames:\n\t        process.stdin.write(frame.tobytes())\n\t    process.stdin.close()\n\t    process.wait()\n\tif __name__ == \"__main__\":\n\t    default_output = f\"./output_{uuid.uuid4()}.mov\"\n", "    parser = argparse.ArgumentParser(\n\t        description=\"FILM frame interpolation\", formatter_class=RichHelpFormatter\n\t    )\n\t    parser.add_argument(\"inputs\", nargs=\"*\", help=\"Input image files\")\n\t    parser.add_argument(\"--output\", help=\"Output JSON file\", default=default_output)\n\t    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose mode\")\n\t    parser.add_argument(\n\t        \"--glob\", help=\"Enable glob pattern matching\", metavar=\"PATTERN\"\n\t    )\n\t    parser.add_argument(\n", "        \"--interpolate\", type=int, default=4, help=\"Time for interpolated frames\"\n\t    )\n\t    parser.add_argument(\"--fps\", type=int, default=30, help=\"Out FPS\")\n\t    align = 64\n\t    block_width = 2\n\t    block_height = 2\n\t    args = parser.parse_args()\n\t    # - checks\n\t    if not args.glob and not args.inputs:\n\t        parser.error(\"Either --glob flag or inputs must be provided.\")\n", "    if args.glob:\n\t        glob_pattern = args.glob\n\t        try:\n\t            pattern_path = str(Path(glob_pattern).expanduser().resolve())\n\t            if not any(glob.glob(pattern_path)):\n\t                raise ValueError(f\"No files found for glob pattern: {glob_pattern}\")\n\t        except Exception as e:\n\t            console = Console()\n\t            console.print(\n\t                f\"[bold red]Error: Invalid glob pattern '{glob_pattern}': {e}[/bold red]\"\n", "            )\n\t            exit(1)\n\t    else:\n\t        glob_pattern = None\n\t    input_files: List[Path] = []\n\t    if glob_pattern:\n\t        input_files = [\n\t            Path(p)\n\t            for p in list(glob.glob(str(Path(glob_pattern).expanduser().resolve())))\n\t        ]\n", "    else:\n\t        input_files = [Path(p) for p in args.inputs]\n\t    console = Console()\n\t    console.print(\"Input Files:\", style=\"bold\", end=\" \")\n\t    console.print(f\"{len(input_files):03d} files\", style=\"cyan\")\n\t    # for input_file in args.inputs:\n\t    #     console.print(f\"- {input_file}\", style=\"cyan\")\n\t    console.print(\"\\nOutput File:\", style=\"bold\", end=\" \")\n\t    console.print(f\"{Path(args.output).resolve().absolute()}\", style=\"cyan\")\n\t    with Progress(console=console, auto_refresh=True) as progress:\n", "        from frame_interpolation.eval import util\n\t        from frame_interpolation.eval import util, interpolator\n\t        # files = Path(pth).rglob(\"*.png\")\n\t        model = interpolator.Interpolator(\n\t            \"G:/MODELS/FILM/pretrained_models/film_net/Style\", None\n\t        )  # [2,2]\n\t        task = progress.add_task(\"[cyan]Interpolating frames...\", total=1)\n\t        frames = list(\n\t            util.interpolate_recursively_from_files(\n\t                [x.as_posix() for x in input_files], args.interpolate, model\n", "            )\n\t        )\n\t        # mediapy.write_video(args.output, frames, fps=args.fps)\n\t        write_prores_444_video(args.output, frames, fps=args.fps)\n\t        progress.update(task, advance=1)\n\t        progress.refresh()\n"]}
{"filename": "scripts/get_deps.py", "chunked_list": ["import os\n\timport ast\n\timport json\n\timport sys\n\tfrom rich.console import Console\n\tfrom rich.table import Table\n\tfrom rich.progress import Progress\n\tconsole = Console(stderr=True)\n\tdef get_imported_modules(filename):\n\t    with open(filename, \"r\") as file:\n", "        tree = ast.parse(file.read())\n\t    imported_modules = []\n\t    for node in ast.walk(tree):\n\t        if isinstance(node, ast.Import):\n\t            imported_modules.extend(\n\t                (alias.name, alias.name in sys.builtin_module_names)\n\t                for alias in node.names\n\t            )\n\t        elif isinstance(node, ast.ImportFrom):\n\t            if node.module:\n", "                imported_modules.append(\n\t                    (node.module, node.module in sys.builtin_module_names)\n\t                )\n\t    return imported_modules\n\tdef list_imported_modules(folder):\n\t    modules = []\n\t    file_count = sum(len(files) for _, _, files in os.walk(folder))\n\t    progress = Progress()\n\t    task = progress.add_task(\"[cyan]Scanning files...\", total=file_count)\n\t    for root, _, files in os.walk(folder):\n", "        for file in files:\n\t            if file.endswith(\".py\"):\n\t                file_path = os.path.join(root, file)\n\t                imported_modules = get_imported_modules(file_path)\n\t                modules.extend(imported_modules)\n\t            progress.update(task, advance=1)\n\t    progress.stop()\n\t    return modules\n\tif __name__ == \"__main__\":\n\t    if len(sys.argv) < 2:\n", "        console.print(\n\t            \"[bold red]Please provide the folder path as a command-line argument.[/bold red]\"\n\t        )\n\t        sys.exit(1)\n\t    # folder_path = input(\"Enter the folder path: \")\n\t    # while not os.path.isdir(folder_path):\n\t    #     console.print(\"[bold red]Invalid folder path![/bold red]\")\n\t    #     folder_path = input(\"Enter the folder path: \")\n\t    folder_path = sys.argv[1]\n\t    if not os.path.isdir(folder_path):\n", "        console.print(\"[bold red]Invalid folder path![/bold red]\")\n\t        sys.exit(1)\n\t    console.print(\"[bold green]=== Python Imported Modules ===[/bold green]\\n\")\n\t    console.print(f\"Scanning folder: [bold]{folder_path}[/bold]\\n\")\n\t    imported_modules = list_imported_modules(folder_path)\n\t    console.print(f\"\\n[bold green]Imported Modules:[/bold green]\\n\")\n\t    table = Table(show_header=True, header_style=\"bold cyan\")\n\t    table.add_column(\"Module\")\n\t    table.add_column(\"Type\")\n\t    for module, is_builtin in imported_modules:\n", "        module_type = \"Built-in\" if is_builtin else \"External\"\n\t        table.add_row(module, module_type)\n\t    console.print(table)\n\t    json_data = json.dumps(\n\t        [\n\t            {\"module\": module, \"type\": \"Built-in\" if is_builtin else \"External\"}\n\t            for module, is_builtin in imported_modules\n\t        ],\n\t        indent=4,\n\t    )\n", "    print(json_data)\n"]}
