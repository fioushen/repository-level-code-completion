{"filename": "ops/scripts/rename_whl.py", "chunked_list": ["\"\"\"Rename Python wheel to contain commit ID\"\"\"\n\timport argparse\n\timport pathlib\n\tdef main(args: argparse.Namespace) -> None:\n\t    \"\"\"Rename Python wheel\"\"\"\n\t    wheel_dir = pathlib.Path(args.wheel_dir).expanduser().resolve()\n\t    if not wheel_dir.is_dir():\n\t        raise ValueError(\"wheel_dir argument must be a directory\")\n\t    for whl_path in wheel_dir.glob(\"*.whl\"):\n\t        basename = whl_path.name\n", "        tokens = basename.split(\"-\")\n\t        assert len(tokens) == 5\n\t        keywords = {\n\t            \"pkg_name\": tokens[0],\n\t            \"version\": tokens[1],\n\t            \"commit_id\": args.commit_id,\n\t            \"platform_tag\": args.platform_tag,\n\t        }\n\t        new_name = (\n\t            \"{pkg_name}-{version}+{commit_id}-py3-none-{platform_tag}.whl\".format(\n", "                **keywords\n\t            )\n\t        )\n\t        new_path = whl_path.parent / new_name\n\t        print(f\"Renaming {whl_path} to {new_path}...\")\n\t        whl_path.rename(whl_path.parent / new_path)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(\n\t        description=(\n\t            \"Script to rename wheel(s) using a commit ID and platform tag.\"\n", "            \"Note: This script will not recurse into subdirectories.\"\n\t        )\n\t    )\n\t    parser.add_argument(\"wheel_dir\", type=str, help=\"Directory containing wheels\")\n\t    parser.add_argument(\"commit_id\", type=str, help=\"Hash of current git commit\")\n\t    parser.add_argument(\n\t        \"platform_tag\", type=str, help=\"Platform tag, PEP 425 compliant\"\n\t    )\n\t    parsed_args = parser.parse_args()\n\t    main(parsed_args)\n"]}
{"filename": "tests/python/test_model_query.py", "chunked_list": ["\"\"\"Tests for rich model query functions\"\"\"\n\timport collections\n\timport pytest\n\timport tl2cgen\n\tfrom .metadata import format_libpath_for_example_model, load_example_model\n\tfrom .util import os_compatible_toolchains, os_platform\n\tModelFact = collections.namedtuple(\n\t    \"ModelFact\",\n\t    \"num_tree num_feature num_class pred_transform global_bias sigmoid_alpha ratio_c \"\n\t    \"threshold_type leaf_output_type\",\n", ")\n\tMODEL_FACTS = {\n\t    \"mushroom\": ModelFact(2, 127, 1, \"sigmoid\", 0.0, 1.0, 1.0, \"float32\", \"float32\"),\n\t    \"dermatology\": ModelFact(60, 33, 6, \"softmax\", 0.5, 1.0, 1.0, \"float32\", \"float32\"),\n\t    \"toy_categorical\": ModelFact(\n\t        30, 2, 1, \"identity\", 0.0, 1.0, 1.0, \"float64\", \"float64\"\n\t    ),\n\t    \"sparse_categorical\": ModelFact(\n\t        1, 5057, 1, \"sigmoid\", 0.0, 1.0, 1.0, \"float64\", \"float64\"\n\t    ),\n", "}\n\t@pytest.mark.parametrize(\n\t    \"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\", \"sparse_categorical\"]\n\t)\n\tdef test_model_query(tmpdir, dataset):\n\t    \"\"\"Test all query functions for every example model\"\"\"\n\t    if dataset == \"sparse_categorical\":\n\t        if os_platform() == \"windows\":\n\t            pytest.xfail(\"MSVC cannot handle long if conditional\")\n\t        elif os_platform() == \"osx\":\n", "            pytest.xfail(\"Apple Clang cannot handle long if conditional\")\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n\t    model = load_example_model(dataset)\n\t    assert model.num_feature == MODEL_FACTS[dataset].num_feature\n\t    assert model.num_class == MODEL_FACTS[dataset].num_class\n\t    assert model.num_tree == MODEL_FACTS[dataset].num_tree\n\t    # pylint: disable=R0801\n\t    toolchain = os_compatible_toolchains()[0]\n\t    tl2cgen.export_lib(\n\t        model,\n", "        toolchain=toolchain,\n\t        libpath=libpath,\n\t        params={\"quantize\": 1, \"parallel_comp\": model.num_tree},\n\t        verbose=True,\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    assert predictor.num_feature == MODEL_FACTS[dataset].num_feature\n\t    assert predictor.num_class == MODEL_FACTS[dataset].num_class\n\t    assert predictor.pred_transform == MODEL_FACTS[dataset].pred_transform\n\t    assert predictor.global_bias == MODEL_FACTS[dataset].global_bias\n", "    assert predictor.sigmoid_alpha == MODEL_FACTS[dataset].sigmoid_alpha\n\t    assert predictor.ratio_c == MODEL_FACTS[dataset].ratio_c\n\t    assert predictor.threshold_type == MODEL_FACTS[dataset].threshold_type\n\t    assert predictor.leaf_output_type == MODEL_FACTS[dataset].leaf_output_type\n"]}
{"filename": "tests/python/test_model_builder.py", "chunked_list": ["\"\"\"Tests for model builder interface\"\"\"\n\timport os\n\timport pathlib\n\timport numpy as np\n\timport pytest\n\timport treelite\n\timport tl2cgen\n\tfrom tl2cgen.contrib.util import _libext\n\tfrom .util import os_compatible_toolchains\n\t@pytest.mark.parametrize(\"test_round_trip\", [True, False])\n", "@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_node_insert_delete(tmpdir, toolchain, test_round_trip):\n\t    # pylint: disable=R0914\n\t    \"\"\"Test ability to add and remove nodes\"\"\"\n\t    num_feature = 3\n\t    builder = treelite.ModelBuilder(num_feature=num_feature)\n\t    builder.append(treelite.ModelBuilder.Tree())\n\t    builder[0][1].set_root()\n\t    builder[0][1].set_numerical_test_node(\n\t        feature_id=2,\n", "        opname=\"<\",\n\t        threshold=-0.5,\n\t        default_left=True,\n\t        left_child_key=5,\n\t        right_child_key=10,\n\t    )\n\t    builder[0][5].set_leaf_node(-1)\n\t    builder[0][10].set_numerical_test_node(\n\t        feature_id=0,\n\t        opname=\"<=\",\n", "        threshold=0.5,\n\t        default_left=False,\n\t        left_child_key=7,\n\t        right_child_key=8,\n\t    )\n\t    builder[0][7].set_leaf_node(0.0)\n\t    builder[0][8].set_leaf_node(1.0)\n\t    del builder[0][1]\n\t    del builder[0][5]\n\t    builder[0][5].set_categorical_test_node(\n", "        feature_id=1,\n\t        left_categories=[1, 2, 4],\n\t        default_left=True,\n\t        left_child_key=20,\n\t        right_child_key=10,\n\t    )\n\t    builder[0][20].set_leaf_node(2.0)\n\t    builder[0][5].set_root()\n\t    model = builder.commit()\n\t    if test_round_trip:\n", "        checkpoint_path = os.path.join(tmpdir, \"checkpoint.bin\")\n\t        model.serialize(checkpoint_path)\n\t        model = treelite.Model.deserialize(checkpoint_path)\n\t    assert model.num_feature == num_feature\n\t    assert model.num_class == 1\n\t    assert model.num_tree == 1\n\t    libpath = pathlib.Path(tmpdir) / (\"libtest\" + _libext())\n\t    tl2cgen.export_lib(model, toolchain=toolchain, libpath=libpath, verbose=True)\n\t    predictor = tl2cgen.Predictor(libpath=libpath)\n\t    assert predictor.num_feature == num_feature\n", "    assert predictor.num_class == 1\n\t    assert predictor.pred_transform == \"identity\"\n\t    assert predictor.global_bias == 0.0\n\t    assert predictor.sigmoid_alpha == 1.0\n\t    assert predictor.ratio_c == 1.0\n\t    for f0 in [-0.5, 0.5, 1.5, np.nan]:\n\t        for f1 in [0, 1, 2, 3, 4, np.nan]:\n\t            for f2 in [-1.0, -0.5, 1.0, np.nan]:\n\t                x = np.array([[f0, f1, f2]])\n\t                dmat = tl2cgen.DMatrix(x, dtype=\"float32\")\n", "                pred = predictor.predict(dmat)\n\t                if f1 in [1, 2, 4] or np.isnan(f1):\n\t                    expected_pred = 2.0\n\t                elif f0 <= 0.5 and not np.isnan(f0):\n\t                    expected_pred = 0.0\n\t                else:\n\t                    expected_pred = 1.0\n\t                if pred != expected_pred:\n\t                    raise ValueError(\n\t                        f\"Prediction wrong for f0={f0}, f1={f1}, f2={f2}: \"\n", "                        + f\"expected_pred = {expected_pred} vs actual_pred = {pred}\"\n\t                    )\n"]}
{"filename": "tests/python/test_lightgbm_integration.py", "chunked_list": ["\"\"\"Tests for LightGBM integration\"\"\"\n\tfrom __future__ import annotations\n\timport pathlib\n\tfrom typing import Any, Dict, Optional, Union\n\timport numpy as np\n\timport pytest\n\timport scipy.sparse\n\timport treelite\n\tfrom hypothesis import given, settings\n\tfrom hypothesis.strategies import sampled_from\n", "from sklearn.datasets import load_iris, load_svmlight_file\n\tfrom sklearn.model_selection import train_test_split\n\timport tl2cgen\n\tfrom tl2cgen.contrib.util import _libext\n\tfrom .hypothesis_util import standard_regression_datasets, standard_settings\n\tfrom .metadata import (\n\t    example_model_db,\n\t    format_libpath_for_example_model,\n\t    load_example_model,\n\t)\n", "from .util import (\n\t    TemporaryDirectory,\n\t    check_predictor,\n\t    has_pandas,\n\t    os_compatible_toolchains,\n\t    os_platform,\n\t)\n\ttry:\n\t    import lightgbm\n\texcept ImportError:\n", "    # skip this test suite if LightGBM is not installed\n\t    pytest.skip(\"LightGBM not installed; skipping\", allow_module_level=True)\n\tdef _compile_lightgbm_model(\n\t    *,\n\t    model_path: Optional[pathlib.Path] = None,\n\t    model_obj: Optional[lightgbm.Booster] = None,\n\t    libname: str,\n\t    prefix: Union[pathlib.Path, str],\n\t    toolchain: str,\n\t    params: Dict[str, Any],\n", "    verbose: bool = False,\n\t) -> tl2cgen.Predictor:\n\t    if model_path is not None:\n\t        model = treelite.Model.load(str(model_path), model_format=\"lightgbm\")\n\t    elif model_obj is not None:\n\t        model = treelite.Model.from_lightgbm(model_obj)\n\t    else:\n\t        raise RuntimeError(\"Either model_path or model_obj must be provided\")\n\t    libpath = pathlib.Path(prefix).expanduser().resolve().joinpath(libname + _libext())\n\t    tl2cgen.export_lib(\n", "        model, toolchain=toolchain, libpath=libpath, params=params, verbose=verbose\n\t    )\n\t    return tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t@given(\n\t    toolchain=sampled_from(os_compatible_toolchains()),\n\t    objective=sampled_from([\"regression\", \"regression_l1\", \"huber\"]),\n\t    reg_sqrt=sampled_from([True, False]),\n\t    dataset=standard_regression_datasets(),\n\t)\n\t@settings(**standard_settings())\n", "def test_lightgbm_regression(toolchain, objective, reg_sqrt, dataset):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Test a regressor\"\"\"\n\t    X, y = dataset\n\t    X_train, X_test, y_train, y_test = train_test_split(\n\t        X, y, test_size=0.2, shuffle=False\n\t    )\n\t    dtrain = lightgbm.Dataset(X_train, y_train, free_raw_data=False)\n\t    dtest = lightgbm.Dataset(X_test, y_test, reference=dtrain, free_raw_data=False)\n\t    param = {\n", "        \"task\": \"train\",\n\t        \"boosting_type\": \"gbdt\",\n\t        \"objective\": objective,\n\t        \"reg_sqrt\": reg_sqrt,\n\t        \"metric\": \"rmse\",\n\t        \"num_leaves\": 31,\n\t        \"learning_rate\": 0.05,\n\t    }\n\t    bst = lightgbm.train(\n\t        param,\n", "        dtrain,\n\t        num_boost_round=10,\n\t        valid_sets=[dtrain, dtest],\n\t        valid_names=[\"train\", \"test\"],\n\t    )\n\t    with TemporaryDirectory() as tmpdir:\n\t        predictor = _compile_lightgbm_model(\n\t            model_obj=bst,\n\t            libname=f\"regression_{objective}\",\n\t            prefix=tmpdir,\n", "            toolchain=toolchain,\n\t            params={\"quantize\": 1},\n\t            verbose=True,\n\t        )\n\t        dmat = tl2cgen.DMatrix(X_test, dtype=\"float64\")\n\t        out_pred = predictor.predict(dmat)\n\t        expected_pred = bst.predict(X_test).reshape((X_test.shape[0], -1))\n\t        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=4)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"boosting_type\", [\"gbdt\", \"rf\"])\n", "@pytest.mark.parametrize(\"objective\", [\"multiclass\", \"multiclassova\"])\n\tdef test_lightgbm_multiclass_classification(\n\t    tmpdir, objective, boosting_type, toolchain\n\t):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Test a multi-class classifier\"\"\"\n\t    model_path = pathlib.Path(tmpdir) / \"iris_lightgbm.txt\"\n\t    X, y = load_iris(return_X_y=True)\n\t    X_train, X_test, y_train, y_test = train_test_split(\n\t        X, y, test_size=0.2, shuffle=False\n", "    )\n\t    dtrain = lightgbm.Dataset(X_train, y_train, free_raw_data=False)\n\t    dtest = lightgbm.Dataset(X_test, y_test, reference=dtrain, free_raw_data=False)\n\t    param = {\n\t        \"task\": \"train\",\n\t        \"boosting\": boosting_type,\n\t        \"objective\": objective,\n\t        \"metric\": \"multi_logloss\",\n\t        \"num_class\": 3,\n\t        \"num_leaves\": 31,\n", "        \"learning_rate\": 0.05,\n\t    }\n\t    if boosting_type == \"rf\":\n\t        param.update({\"bagging_fraction\": 0.8, \"bagging_freq\": 1})\n\t    bst = lightgbm.train(\n\t        param,\n\t        dtrain,\n\t        num_boost_round=10,\n\t        valid_sets=[dtrain, dtest],\n\t        valid_names=[\"train\", \"test\"],\n", "    )\n\t    bst.save_model(model_path)\n\t    predictor = _compile_lightgbm_model(\n\t        model_path=model_path,\n\t        libname=f\"iris_{objective}\",\n\t        prefix=tmpdir,\n\t        toolchain=toolchain,\n\t        params={\"quantize\": 1},\n\t        verbose=True,\n\t    )\n", "    dmat = tl2cgen.DMatrix(X_test, dtype=\"float64\")\n\t    out_pred = predictor.predict(dmat)\n\t    expected_pred = bst.predict(X_test)\n\t    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"objective\", [\"binary\", \"xentlambda\", \"xentropy\"])\n\tdef test_lightgbm_binary_classification(tmpdir, objective, toolchain):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Test a binary classifier\"\"\"\n\t    dataset = \"mushroom\"\n", "    model_path = pathlib.Path(tmpdir) / \"mushroom_lightgbm.txt\"\n\t    dtest_path = example_model_db[dataset].dtest\n\t    dtrain = lightgbm.Dataset(example_model_db[dataset].dtrain)\n\t    dtest = lightgbm.Dataset(dtest_path, reference=dtrain)\n\t    param = {\n\t        \"task\": \"train\",\n\t        \"boosting_type\": \"gbdt\",\n\t        \"objective\": objective,\n\t        \"metric\": \"auc\",\n\t        \"num_leaves\": 7,\n", "        \"learning_rate\": 0.1,\n\t    }\n\t    bst = lightgbm.train(\n\t        param,\n\t        dtrain,\n\t        num_boost_round=10,\n\t        valid_sets=[dtrain, dtest],\n\t        valid_names=[\"train\", \"test\"],\n\t    )\n\t    bst.save_model(model_path)\n", "    shape = (dtest.num_data(), -1)\n\t    expected_prob = bst.predict(dtest_path).reshape(shape)\n\t    expected_margin = bst.predict(dtest_path, raw_score=True).reshape(shape)\n\t    predictor = _compile_lightgbm_model(\n\t        model_path=model_path,\n\t        libname=f\"agaricus_{objective}\",\n\t        prefix=tmpdir,\n\t        toolchain=toolchain,\n\t        params={},\n\t        verbose=True,\n", "    )\n\t    dmat = tl2cgen.DMatrix(\n\t        load_svmlight_file(dtest_path, zero_based=True)[0], dtype=\"float64\"\n\t    )\n\t    out_prob = predictor.predict(dmat)\n\t    np.testing.assert_almost_equal(out_prob, expected_prob, decimal=5)\n\t    out_margin = predictor.predict(dmat, pred_margin=True)\n\t    np.testing.assert_almost_equal(out_margin, expected_margin, decimal=5)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"parallel_comp\", [None, 2])\n", "@pytest.mark.parametrize(\"quantize\", [True, False])\n\tdef test_categorical_data(tmpdir, quantize, parallel_comp, toolchain):\n\t    \"\"\"\n\t    LightGBM is able to produce categorical splits directly, so that\n\t    categorical data don't have to be one-hot encoded. Test if Treelite is\n\t    able to handle categorical splits.\n\t    This toy example contains two features, both of which are categorical.\n\t    The first has cardinality 3 and the second 5. The label was generated using\n\t    the formula\n\t       y = f(x0) + g(x1) + [noise with std=0.1]\n", "    where f and g are given by the tables\n\t       x0  f(x0)        x1  g(x1)\n\t        0    -20         0     -2\n\t        1    -10         1     -1\n\t        2      0         2      0\n\t                         3      1\n\t                         4      2\n\t    \"\"\"\n\t    dataset = \"toy_categorical\"\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n", "    model = load_example_model(dataset)\n\t    # pylint: disable=R0801\n\t    params = {\n\t        \"quantize\": (1 if quantize else 0),\n\t        \"parallel_comp\": (parallel_comp if parallel_comp else 0),\n\t    }\n\t    tl2cgen.export_lib(\n\t        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n", "    check_predictor(predictor, dataset)\n\t@pytest.mark.skipif(not has_pandas(), reason=\"Pandas required\")\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_categorical_data_pandas_df_with_dummies(tmpdir, toolchain):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"\n\t    This toy example contains two features, both of which are categorical.\n\t    The first has cardinality 3 and the second 5. The label was generated using\n\t    the formula\n\t       y = f(x0) + g(x1) + [noise with std=0.1]\n", "    where f and g are given by the tables\n\t       x0  f(x0)        x1  g(x1)\n\t        0    -20         0     -2\n\t        1    -10         1     -1\n\t        2      0         2      0\n\t                         3      1\n\t                         4      2\n\t    Unlike test_categorical_data(), this test will convert the categorical features into\n\t    dummies using OneHotEncoder and store them into a Pandas Dataframe.\n\t    \"\"\"\n", "    import pandas as pd  # pylint: disable=import-outside-toplevel\n\t    rng = np.random.default_rng(seed=0)\n\t    n_row = 100\n\t    x0 = rng.integers(low=0, high=3, size=n_row)\n\t    x1 = rng.integers(low=0, high=5, size=n_row)\n\t    noise = rng.standard_normal(size=n_row) * 0.1\n\t    y = (x0 * 10 - 20) + (x1 - 2) + noise\n\t    df = pd.DataFrame({\"x0\": x0, \"x1\": x1}).astype(\"category\")\n\t    df_dummies = pd.get_dummies(df, prefix=[\"x0\", \"x1\"], prefix_sep=\"=\")\n\t    assert df_dummies.columns.tolist() == [\n", "        \"x0=0\",\n\t        \"x0=1\",\n\t        \"x0=2\",\n\t        \"x1=0\",\n\t        \"x1=1\",\n\t        \"x1=2\",\n\t        \"x1=3\",\n\t        \"x1=4\",\n\t    ]\n\t    model_path = pathlib.Path(tmpdir) / \"toy_dummies.txt\"\n", "    dtrain = lightgbm.Dataset(df_dummies, label=y)\n\t    param = {\n\t        \"task\": \"train\",\n\t        \"boosting_type\": \"gbdt\",\n\t        \"objective\": \"regression\",\n\t        \"metric\": \"rmse\",\n\t        \"num_leaves\": 7,\n\t        \"learning_rate\": 0.1,\n\t    }\n\t    bst = lightgbm.train(\n", "        param, dtrain, num_boost_round=15, valid_sets=[dtrain], valid_names=[\"train\"]\n\t    )\n\t    lgb_out = bst.predict(df_dummies).reshape((df_dummies.shape[0], -1))\n\t    bst.save_model(model_path)\n\t    predictor = _compile_lightgbm_model(\n\t        model_path=model_path,\n\t        libname=\"dummies\",\n\t        prefix=tmpdir,\n\t        toolchain=toolchain,\n\t        params={},\n", "        verbose=True,\n\t    )\n\t    tl_out = predictor.predict(tl2cgen.DMatrix(df_dummies.values, dtype=\"float32\"))\n\t    np.testing.assert_almost_equal(lgb_out, tl_out, decimal=5)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"quantize\", [True, False])\n\tdef test_sparse_ranking_model(tmpdir, quantize, toolchain):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Generate a LightGBM ranking model with highly sparse data.\n\t    This example is inspired by https://github.com/dmlc/treelite/issues/222. It verifies that\n", "    Treelite is able to accommodate the unique behavior of LightGBM when it comes to handling\n\t    missing values.\n\t    LightGBM offers two modes of handling missing values:\n\t    1. Assign default direction per each test node: This is similar to how XGBoost handles missing\n\t       values.\n\t    2. Replace missing values with zeroes (0.0): This behavior is unique to LightGBM.\n\t    The mode is controlled by the missing_value_to_zero_ field of each test node.\n\t    This example is crafted so as to invoke the second mode of missing value handling.\n\t    \"\"\"\n\t    rng = np.random.default_rng(seed=2020)\n", "    X = scipy.sparse.random(\n\t        m=10, n=206947, format=\"csr\", dtype=np.float64, random_state=0, density=0.0001\n\t    )\n\t    X.data = rng.standard_normal(size=X.data.shape[0], dtype=np.float64)\n\t    y = rng.integers(low=0, high=5, size=X.shape[0])\n\t    params = {\n\t        \"objective\": \"lambdarank\",\n\t        \"num_leaves\": 32,\n\t        \"lambda_l1\": 0.0,\n\t        \"lambda_l2\": 0.0,\n", "        \"min_gain_to_split\": 0.0,\n\t        \"learning_rate\": 1.0,\n\t        \"min_data_in_leaf\": 1,\n\t    }\n\t    model_path = pathlib.Path(tmpdir) / \"sparse_ranking_lightgbm.txt\"\n\t    dtrain = lightgbm.Dataset(X, label=y, group=[X.shape[0]])\n\t    bst = lightgbm.train(params, dtrain, num_boost_round=1)\n\t    lgb_out = bst.predict(X).reshape((X.shape[0], -1))\n\t    bst.save_model(model_path)\n\t    predictor = _compile_lightgbm_model(\n", "        model_path=model_path,\n\t        libname=\"sparse_ranking_lgb\",\n\t        prefix=tmpdir,\n\t        toolchain=toolchain,\n\t        params={\"quantize\": (1 if quantize else 0)},\n\t        verbose=True,\n\t    )\n\t    dmat = tl2cgen.DMatrix(X)\n\t    out = predictor.predict(dmat)\n\t    np.testing.assert_almost_equal(out, lgb_out)\n", "@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"quantize\", [True, False])\n\tdef test_sparse_categorical_model(tmpdir, quantize, toolchain):\n\t    \"\"\"\n\t    LightGBM is able to produce categorical splits directly, so that\n\t    categorical data don't have to be one-hot encoded. Test if Treelite is\n\t    able to handle categorical splits.\n\t    This example produces a model with high-cardinality categorical variables.\n\t    The training data has many missing values, so we need to match LightGBM\n\t    when it comes to handling missing values\n", "    \"\"\"\n\t    if toolchain == \"clang\":\n\t        pytest.xfail(reason=\"Clang cannot handle long if conditional\")\n\t    if os_platform() == \"windows\":\n\t        pytest.xfail(reason=\"MSVC cannot handle long if conditional\")\n\t    if os_platform() == \"osx\":\n\t        pytest.xfail(reason=\"Apple Clang cannot handle long if conditional\")\n\t    dataset = \"sparse_categorical\"\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n\t    model = load_example_model(dataset)\n", "    params = {\"quantize\": (1 if quantize else 0)}\n\t    tl2cgen.export_lib(\n\t        model,\n\t        toolchain=toolchain,\n\t        libpath=libpath,\n\t        params=params,\n\t        verbose=True,\n\t        options=[\"-O0\"],\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n", "    check_predictor(predictor, dataset)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_nan_handling_with_categorical_splits(tmpdir, toolchain):\n\t    \"\"\"Test that NaN inputs are handled correctly in categorical splits\"\"\"\n\t    # Test case taken from https://github.com/dmlc/treelite/issues/277\n\t    X = np.array(30 * [[1]] + 30 * [[2]] + 30 * [[0]])\n\t    y = np.array(60 * [5] + 30 * [10])\n\t    train_data = lightgbm.Dataset(X, label=y, categorical_feature=[0])\n\t    bst = lightgbm.train({}, train_data, 1)\n\t    model_path = pathlib.Path(tmpdir) / \"dummy_categorical.txt\"\n", "    input_with_nan = np.array([[np.NaN], [0.0]])\n\t    lgb_pred = bst.predict(input_with_nan).reshape((input_with_nan.shape[0], -1))\n\t    bst.save_model(model_path)\n\t    predictor = _compile_lightgbm_model(\n\t        model_path=model_path,\n\t        libname=\"dummy_categorical_lgb\",\n\t        prefix=tmpdir,\n\t        toolchain=toolchain,\n\t        params={},\n\t        verbose=True,\n", "    )\n\t    dmat = tl2cgen.DMatrix(input_with_nan)\n\t    tl_pred = predictor.predict(dmat)\n\t    np.testing.assert_almost_equal(tl_pred, lgb_pred)\n"]}
{"filename": "tests/python/test_basic.py", "chunked_list": ["\"\"\"Suite of basic tests\"\"\"\n\timport itertools\n\timport os\n\timport pathlib\n\timport subprocess\n\timport sys\n\tfrom zipfile import ZipFile\n\timport pytest\n\tfrom scipy.sparse import csr_matrix\n\timport tl2cgen\n", "from .metadata import (\n\t    example_model_db,\n\t    format_libpath_for_example_model,\n\t    load_example_model,\n\t)\n\tfrom .util import check_predictor, does_not_raise, os_compatible_toolchains, os_platform\n\t@pytest.mark.parametrize(\n\t    \"dataset,use_annotation,parallel_comp,quantize,toolchain\",\n\t    list(\n\t        itertools.product(\n", "            [\"mushroom\", \"dermatology\"],\n\t            [True, False],\n\t            [None, 4],\n\t            [True, False],\n\t            os_compatible_toolchains(),\n\t        )\n\t    )\n\t    + [\n\t        (\"toy_categorical\", False, 30, True, os_compatible_toolchains()[0]),\n\t    ],\n", ")\n\tdef test_basic(\n\t    tmpdir, annotation, dataset, use_annotation, quantize, parallel_comp, toolchain\n\t):\n\t    # pylint: disable=too-many-arguments\n\t    \"\"\"Basic test for C codegen\"\"\"\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n\t    model = load_example_model(dataset)\n\t    annotation_path = os.path.join(tmpdir, \"annotation.json\")\n\t    if use_annotation:\n", "        if annotation[dataset] is None:\n\t            pytest.skip(\"No training data available. Skipping annotation\")\n\t        with open(annotation_path, \"w\", encoding=\"UTF-8\") as f:\n\t            f.write(annotation[dataset])\n\t    params = {\n\t        \"annotate_in\": (annotation_path if use_annotation else \"NULL\"),\n\t        \"quantize\": (1 if quantize else 0),\n\t        \"parallel_comp\": (parallel_comp if parallel_comp else 0),\n\t    }\n\t    tl2cgen.export_lib(\n", "        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    check_predictor(predictor, dataset)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"use_elf\", [True, False])\n\t@pytest.mark.parametrize(\"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\"])\n\tdef test_failsafe_compiler(tmpdir, dataset, use_elf, toolchain):\n\t    \"\"\"Test 'failsafe' compiler\"\"\"\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n", "    model = load_example_model(dataset)\n\t    params = {\"dump_array_as_elf\": (1 if use_elf else 0)}\n\t    is_linux = sys.platform.startswith(\"linux\")\n\t    # Expect TL2cgen to throw error if we try to use dump_array_as_elf on non-Linux OS\n\t    # Also, failsafe compiler is only available for XGBoost models\n\t    if ((not is_linux) and use_elf) or example_model_db[dataset].format != \"xgboost\":\n\t        expect_raises = pytest.raises(tl2cgen.TL2cgenError)\n\t    else:\n\t        expect_raises = does_not_raise()\n\t    with expect_raises:\n", "        tl2cgen.export_lib(\n\t            model,\n\t            compiler=\"failsafe\",\n\t            toolchain=toolchain,\n\t            libpath=libpath,\n\t            params=params,\n\t            verbose=True,\n\t        )\n\t        predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t        check_predictor(predictor, dataset)\n", "@pytest.mark.skipif(os_platform() == \"windows\", reason=\"Make unavailable on Windows\")\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\"])\n\tdef test_srcpkg(tmpdir, dataset, toolchain):\n\t    \"\"\"Test feature to export a source tarball\"\"\"\n\t    pkgpath = pathlib.Path(tmpdir) / \"srcpkg.zip\"\n\t    model = load_example_model(dataset)\n\t    tl2cgen.export_srcpkg(\n\t        model,\n\t        toolchain=toolchain,\n", "        pkgpath=pkgpath,\n\t        libname=example_model_db[dataset].libname,\n\t        params={\"parallel_comp\": 4},\n\t        verbose=True,\n\t    )\n\t    with ZipFile(pkgpath, \"r\") as zip_ref:\n\t        zip_ref.extractall(tmpdir)\n\t    nproc = os.cpu_count()\n\t    subprocess.check_call(\n\t        [\"make\", \"-C\", example_model_db[dataset].libname, f\"-j{nproc}\"], cwd=tmpdir\n", "    )\n\t    libpath = pathlib.Path(tmpdir) / example_model_db[dataset].libname\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    check_predictor(predictor, dataset)\n\t@pytest.mark.parametrize(\"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\"])\n\tdef test_srcpkg_cmake(tmpdir, dataset):  # pylint: disable=R0914\n\t    \"\"\"Test feature to export a source tarball\"\"\"\n\t    pkgpath = pathlib.Path(tmpdir) / \"srcpkg.zip\"\n\t    model = load_example_model(dataset)\n\t    tl2cgen.export_srcpkg(\n", "        model,\n\t        toolchain=\"cmake\",\n\t        pkgpath=pkgpath,\n\t        libname=example_model_db[dataset].libname,\n\t        params={\"parallel_comp\": 4},\n\t        verbose=True,\n\t    )\n\t    with ZipFile(pkgpath, \"r\") as zip_ref:\n\t        zip_ref.extractall(tmpdir)\n\t    build_dir = pathlib.Path(tmpdir) / example_model_db[dataset].libname / \"build\"\n", "    build_dir.mkdir()\n\t    nproc = os.cpu_count()\n\t    win_opts = [\"-A\", \"x64\"] if os_platform() == \"windows\" else []\n\t    subprocess.check_call([\"cmake\", \"..\"] + win_opts, cwd=build_dir)\n\t    subprocess.check_call(\n\t        [\"cmake\", \"--build\", \".\", \"--config\", \"Release\", \"--parallel\", str(nproc)],\n\t        cwd=build_dir,\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=build_dir, verbose=True)\n\t    check_predictor(predictor, dataset)\n", "def test_deficient_matrix(tmpdir):\n\t    \"\"\"Test if TL2cgen correctly handles sparse matrix with fewer columns than the training data\n\t    used for the model. In this case, the matrix should be padded with zeros.\"\"\"\n\t    libpath = format_libpath_for_example_model(\"mushroom\", prefix=tmpdir)\n\t    model = load_example_model(\"mushroom\")\n\t    toolchain = os_compatible_toolchains()[0]\n\t    tl2cgen.export_lib(\n\t        model,\n\t        toolchain=toolchain,\n\t        libpath=libpath,\n", "        params={\"quantize\": 1},\n\t        verbose=True,\n\t    )\n\t    X = csr_matrix(([], ([], [])), shape=(3, 3))\n\t    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    assert predictor.num_feature == 127\n\t    predictor.predict(dmat)  # should not crash\n\tdef test_too_wide_matrix(tmpdir):\n\t    \"\"\"Test if TL2cgen correctly handles sparse matrix with more columns than the training data\n", "    used for the model. In this case, an exception should be thrown\"\"\"\n\t    libpath = format_libpath_for_example_model(\"mushroom\", prefix=tmpdir)\n\t    model = load_example_model(\"mushroom\")\n\t    toolchain = os_compatible_toolchains()[0]\n\t    tl2cgen.export_lib(\n\t        model,\n\t        toolchain=toolchain,\n\t        libpath=libpath,\n\t        params={\"quantize\": 1},\n\t        verbose=True,\n", "    )\n\t    X = csr_matrix(([], ([], [])), shape=(3, 1000))\n\t    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    assert predictor.num_feature == 127\n\t    pytest.raises(tl2cgen.TL2cgenError, predictor.predict, dmat)\n"]}
{"filename": "tests/python/metadata.py", "chunked_list": ["\"\"\"Metadata for datasets and models used for testing\"\"\"\n\timport collections\n\timport pathlib\n\tfrom typing import Optional, Union\n\timport treelite\n\tfrom tl2cgen.contrib.util import _libext\n\tCURRENT_DIR = pathlib.Path(__file__).parent.expanduser().resolve()\n\tDPATH = CURRENT_DIR.parent / \"example_data\"\n\tDataset = collections.namedtuple(\n\t    \"Dataset\",\n", "    \"model format dtrain dtest libname expected_prob expected_margin is_multiclass dtype\",\n\t)\n\t_dataset_db = {\n\t    \"mushroom\": Dataset(\n\t        model=\"mushroom.model\",\n\t        format=\"xgboost\",\n\t        dtrain=\"agaricus.train\",\n\t        dtest=\"agaricus.test\",\n\t        libname=\"agaricus\",\n\t        expected_prob=\"agaricus.test.prob\",\n", "        expected_margin=\"agaricus.test.margin\",\n\t        is_multiclass=False,\n\t        dtype=\"float32\",\n\t    ),\n\t    \"dermatology\": Dataset(\n\t        model=\"dermatology.model\",\n\t        format=\"xgboost\",\n\t        dtrain=\"dermatology.train\",\n\t        dtest=\"dermatology.test\",\n\t        libname=\"dermatology\",\n", "        expected_prob=\"dermatology.test.prob\",\n\t        expected_margin=\"dermatology.test.margin\",\n\t        is_multiclass=True,\n\t        dtype=\"float32\",\n\t    ),\n\t    \"toy_categorical\": Dataset(\n\t        model=\"toy_categorical_model.txt\",\n\t        format=\"lightgbm\",\n\t        dtrain=None,\n\t        dtest=\"toy_categorical.test\",\n", "        libname=\"toycat\",\n\t        expected_prob=None,\n\t        expected_margin=\"toy_categorical.test.pred\",\n\t        is_multiclass=False,\n\t        dtype=\"float64\",\n\t    ),\n\t    \"sparse_categorical\": Dataset(\n\t        model=\"sparse_categorical_model.txt\",\n\t        format=\"lightgbm\",\n\t        dtrain=None,\n", "        dtest=\"sparse_categorical.test\",\n\t        libname=\"sparsecat\",\n\t        expected_prob=None,\n\t        expected_margin=\"sparse_categorical.test.margin\",\n\t        is_multiclass=False,\n\t        dtype=\"float64\",\n\t    ),\n\t    \"xgb_toy_categorical\": Dataset(\n\t        model=\"xgb_toy_categorical_model.json\",\n\t        format=\"xgboost_json\",\n", "        dtrain=None,\n\t        dtest=\"xgb_toy_categorical.test\",\n\t        libname=\"xgbtoycat\",\n\t        expected_prob=None,\n\t        expected_margin=\"xgb_toy_categorical.test.pred\",\n\t        is_multiclass=False,\n\t        dtype=\"float32\",\n\t    ),\n\t}\n\tdef _qualify_path(prefix: str, path: Optional[str]) -> Optional[str]:\n", "    if path is None:\n\t        return None\n\t    new_path = DPATH / prefix / path\n\t    assert new_path.exists()\n\t    return str(new_path)\n\tdef format_libpath_for_example_model(\n\t    example_id: str,\n\t    prefix: Union[pathlib.Path, str],\n\t) -> pathlib.Path:\n\t    \"\"\"Format path for the shared lib corresponding to an example model\"\"\"\n", "    prefix = pathlib.Path(prefix).expanduser().resolve()\n\t    return prefix.joinpath(example_model_db[example_id].libname + _libext())\n\tdef load_example_model(example_id: str) -> treelite.Model:\n\t    \"\"\"Load an example model\"\"\"\n\t    return treelite.Model.load(\n\t        example_model_db[example_id].model,\n\t        model_format=example_model_db[example_id].format,\n\t    )\n\texample_model_db = {\n\t    k: v._replace(\n", "        model=_qualify_path(k, v.model),\n\t        dtrain=_qualify_path(k, v.dtrain),\n\t        dtest=_qualify_path(k, v.dtest),\n\t        expected_prob=_qualify_path(k, v.expected_prob),\n\t        expected_margin=_qualify_path(k, v.expected_margin),\n\t    )\n\t    for k, v in _dataset_db.items()\n\t}\n"]}
{"filename": "tests/python/hypothesis_util.py", "chunked_list": ["\"\"\"Utility functions for hypothesis-based testing\"\"\"\n\t# pylint: disable=differing-param-doc,missing-type-doc\n\tfrom sys import platform as _platform\n\timport numpy as np\n\tfrom hypothesis import assume\n\tfrom hypothesis.strategies import composite, integers, just, none\n\tfrom sklearn.datasets import make_classification, make_regression\n\tdef _get_limits(strategy):\n\t    \"\"\"Try to find the strategy's limits.\n\t    Raises AttributeError if limits cannot be determined.\n", "    Credit: Carl Simon Adorf (@csadorf)\n\t    https://github.com/rapidsai/cuml/blob/447bded/python/cuml/testing/strategies.py\n\t    \"\"\"\n\t    # unwrap if lazy\n\t    strategy = getattr(strategy, \"wrapped_strategy\", strategy)\n\t    try:\n\t        yield getattr(strategy, \"value\")  # just(...)\n\t    except AttributeError:\n\t        # assume numbers strategy\n\t        yield strategy.start\n", "        yield strategy.stop\n\t@composite\n\tdef standard_classification_datasets(\n\t    draw,\n\t    n_samples=integers(min_value=100, max_value=200),\n\t    n_features=integers(min_value=10, max_value=20),\n\t    *,\n\t    n_informative=None,\n\t    n_redundant=None,\n\t    n_repeated=just(0),\n", "    n_classes=just(2),\n\t    n_clusters_per_class=just(2),\n\t    weights=none(),\n\t    flip_y=just(0.01),\n\t    class_sep=just(1.0),\n\t    hypercube=just(True),\n\t    shift=just(0.0),\n\t    scale=just(1.0),\n\t    shuffle=just(True),\n\t    random_state=None,\n", "):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"\n\t    Returns a strategy to generate classification problem input datasets.\n\t    Note:\n\t    This function uses the sklearn.datasets.make_classification function to\n\t    generate the classification problem from the provided search strategies.\n\t    Credit: Carl Simon Adorf (@csadorf)\n\t    https://github.com/rapidsai/cuml/blob/447bded/python/cuml/testing/strategies.py\n\t    Parameters\n", "    ----------\n\t    draw:\n\t        Callback function, to be used internally by Hypothesis\n\t    n_samples: SearchStrategy[int]\n\t        Returned arrays will have number of rows drawn from these values.\n\t    n_features: SearchStrategy[int]\n\t        Returned arrays will have number of columns drawn from these values.\n\t    n_informative: SearchStrategy[int], default=none\n\t        A search strategy for the number of informative features. If none,\n\t        will use 10% of the actual number of features, but not less than 1\n", "        unless the number of features is zero.\n\t    n_redundant: SearchStrategy[int], default=none\n\t        A search strategy for the number of redundant features. Redundant features\n\t        will be generated as linear combinations of informative features. If none,\n\t        will use 10% of the actual number of features, but not less than 1\n\t        unless the number of features is zero.\n\t    n_repeated: SearchStrategy[int], default=just(0)\n\t        A search strategy for the number of duplicated features.\n\t    n_classes: SearchStrategy[int], default=just(2)\n\t        A search strategy for the number of classes in the classification problem.\n", "    n_clusters_per_class: SearchStrategy[int], default=just(2)\n\t        A search strategy for the number of clusters per class\n\t    weights: SearchStrategy[array], default=none\n\t        A search strategy for the proportions of samples assigned to each class. If\n\t        none, always generate classification problems with balanced classes.\n\t    flip_y: SearchStrategy[float], default=just(0.01)\n\t        A search strategy for the fraction of samples whose class is assigned randomly.\n\t        Larger value for this value introduces noise in the labels and make the\n\t        classification problem harder.\n\t    class_sep: SearchStrategy[float], default=just(1.0)\n", "        A search strategy for the parameter class_sep.\n\t        See sklearn.dataset.make_classification() for a detailed explanation of this\n\t        parameter.\n\t    hypercube: SearchStrategy[bool], default=just(True)\n\t        A search strategy for the parameter hypercube.\n\t        See sklearn.dataset.make_classification() for a detailed explanation of this\n\t        parameter.\n\t    shift: SearchStrategy[float], default=just(0.0)\n\t        A search strategy for the parameter shift.\n\t        See sklearn.dataset.make_classification() for a detailed explanation of this\n", "        parameter.\n\t    scale: SearchStrategy[float], default=just(1.0)\n\t        A search strategy for the parameter scale.\n\t        See sklearn.dataset.make_classification() for a detailed explanation of this\n\t        parameter.\n\t    shuffle: SearchStrategy[bool], default=just(True)\n\t        A boolean search strategy to determine whether samples and features\n\t        are shuffled.\n\t    random_state: int, RandomState instance or None, default=None\n\t        Pass a random state or integer to determine the random number\n", "        generation for data set generation.\n\t    Returns\n\t    -------\n\t    (X, y):  SearchStrategy[array], SearchStrategy[array]\n\t        A tuple of search strategies for arrays subject to the constraints of\n\t        the provided parameters.\n\t    \"\"\"\n\t    n_features_ = draw(n_features)\n\t    if n_informative is None:\n\t        try:\n", "            # Try to meet:\n\t            #   log_2(n_classes * n_clusters_per_class) <= n_informative\n\t            n_classes_min = min(_get_limits(n_classes))\n\t            n_clusters_per_class_min = min(_get_limits(n_clusters_per_class))\n\t            n_informative_min = int(\n\t                np.ceil(np.log2(n_classes_min * n_clusters_per_class_min))\n\t            )\n\t        except AttributeError:\n\t            # Otherwise aim for 10% of n_features, but at least 1.\n\t            n_informative_min = max(1, int(0.1 * n_features_))\n", "        n_informative = just(min(n_features_, n_informative_min))\n\t    if n_redundant is None:\n\t        n_redundant = just(max(min(n_features_, 1), int(0.1 * n_features_)))\n\t    # Check whether the\n\t    #   log_2(n_classes * n_clusters_per_class) <= n_informative\n\t    # inequality can in principle be met.\n\t    try:\n\t        n_classes_min = min(_get_limits(n_classes))\n\t        n_clusters_per_class_min = min(_get_limits(n_clusters_per_class))\n\t        n_informative_max = max(_get_limits(n_informative))\n", "    except AttributeError:\n\t        pass  # unable to determine limits\n\t    else:\n\t        if np.log2(n_classes_min * n_clusters_per_class_min) > n_informative_max:\n\t            raise ValueError(\n\t                \"Assumptions cannot be met, the following inequality must \"\n\t                \"hold: log_2(n_classes * n_clusters_per_class) \"\n\t                \"<= n_informative .\"\n\t            )\n\t    # Check base assumption concerning the composition of feature vectors.\n", "    n_informative_ = draw(n_informative)\n\t    n_redundant_ = draw(n_redundant)\n\t    n_repeated_ = draw(n_repeated)\n\t    assume(n_informative_ + n_redundant_ + n_repeated_ <= n_features_)\n\t    # Check base assumption concerning relationship of number of clusters and\n\t    # informative features.\n\t    n_classes_ = draw(n_classes)\n\t    n_clusters_per_class_ = draw(n_clusters_per_class)\n\t    assume(np.log2(n_classes_ * n_clusters_per_class_) <= n_informative_)\n\t    X, y = make_classification(\n", "        n_samples=draw(n_samples),\n\t        n_features=n_features_,\n\t        n_informative=n_informative_,\n\t        n_redundant=n_redundant_,\n\t        n_repeated=n_repeated_,\n\t        n_classes=n_classes_,\n\t        n_clusters_per_class=n_clusters_per_class_,\n\t        weights=draw(weights),\n\t        flip_y=draw(flip_y),\n\t        class_sep=draw(class_sep),\n", "        hypercube=draw(hypercube),\n\t        shift=draw(shift),\n\t        scale=draw(scale),\n\t        shuffle=draw(shuffle),\n\t        random_state=random_state,\n\t    )\n\t    return X.astype(np.float32), y.astype(np.int32)\n\t@composite\n\tdef standard_regression_datasets(\n\t    draw,\n", "    n_samples=integers(min_value=100, max_value=200),\n\t    n_features=integers(min_value=100, max_value=200),\n\t    *,\n\t    n_informative=None,\n\t    n_targets=just(1),\n\t    bias=just(0.0),\n\t    effective_rank=none(),\n\t    tail_strength=just(0.5),\n\t    noise=just(0.0),\n\t    shuffle=just(True),\n", "    random_state=None,\n\t):\n\t    \"\"\"\n\t    Returns a strategy to generate regression problem input datasets.\n\t    Note:\n\t    This function uses the sklearn.datasets.make_regression function to\n\t    generate the regression problem from the provided search strategies.\n\t    Credit: Carl Simon Adorf (@csadorf)\n\t    https://github.com/rapidsai/cuml/blob/447bded/python/cuml/testing/strategies.py\n\t    Parameters\n", "    ----------\n\t    draw:\n\t        Callback function, to be used internally by Hypothesis\n\t    n_samples: SearchStrategy[int]\n\t        Returned arrays will have number of rows drawn from these values.\n\t    n_features: SearchStrategy[int]\n\t        Returned arrays will have number of columns drawn from these values.\n\t    n_informative: SearchStrategy[int], default=none\n\t        A search strategy for the number of informative features. If none,\n\t        will use 10% of the actual number of features, but not less than 1\n", "        unless the number of features is zero.\n\t    n_targets: SearchStrategy[int], default=just(1)\n\t        A search strategy for the number of targets, that means the number of\n\t        columns of the returned y output array.\n\t    bias: SearchStrategy[float], default=just(0.0)\n\t        A search strategy for the bias term.\n\t    effective_rank:\n\t        If not None, a search strategy for the effective rank of the input data\n\t        for the regression problem. See sklearn.dataset.make_regression() for a\n\t        detailed explanation of this parameter.\n", "    tail_strength: SearchStrategy[float], default=just(0.5)\n\t        See sklearn.dataset.make_regression() for a detailed explanation of\n\t        this parameter.\n\t    noise: SearchStrategy[float], default=just(0.0)\n\t        A search strategy for the standard deviation of the gaussian noise.\n\t    shuffle: SearchStrategy[bool], default=just(True)\n\t        A boolean search strategy to determine whether samples and features\n\t        are shuffled.\n\t    random_state: int, RandomState instance or None, default=None\n\t        Pass a random state or integer to determine the random number\n", "        generation for data set generation.\n\t    Returns\n\t    -------\n\t    (X, y):  SearchStrategy[array], SearchStrategy[array]\n\t        A tuple of search strategies for arrays subject to the constraints of\n\t        the provided parameters.\n\t    \"\"\"\n\t    n_features_ = draw(n_features)\n\t    if n_informative is None:\n\t        n_informative = just(max(min(n_features_, 1), int(0.1 * n_features_)))\n", "    # pylint: disable=unbalanced-tuple-unpacking\n\t    X, y = make_regression(\n\t        n_samples=draw(n_samples),\n\t        n_features=n_features_,\n\t        n_informative=draw(n_informative),\n\t        n_targets=draw(n_targets),\n\t        bias=draw(bias),\n\t        effective_rank=draw(effective_rank),\n\t        tail_strength=draw(tail_strength),\n\t        noise=draw(noise),\n", "        shuffle=draw(shuffle),\n\t        random_state=random_state,\n\t    )\n\t    return X.astype(np.float32), y.astype(np.float32)\n\tdef standard_settings():\n\t    \"\"\"Default hypothesis settings. Set a smaller max_examples on Windows\"\"\"\n\t    kwargs = {\n\t        \"deadline\": None,\n\t        \"max_examples\": 20,\n\t        \"print_blob\": True,\n", "    }\n\t    if _platform == \"win32\":\n\t        kwargs[\"max_examples\"] = 3\n\t    return kwargs\n"]}
{"filename": "tests/python/__init__.py", "chunked_list": []}
{"filename": "tests/python/test_code_folding.py", "chunked_list": ["\"\"\"Tests for reading/writing Protocol Buffers\"\"\"\n\timport itertools\n\timport os\n\timport pytest\n\timport tl2cgen\n\tfrom .metadata import format_libpath_for_example_model, load_example_model\n\tfrom .util import check_predictor, os_compatible_toolchains\n\t@pytest.mark.parametrize(\"code_folding_factor\", [0.0, 1.0, 2.0, 3.0])\n\t@pytest.mark.parametrize(\n\t    \"dataset,toolchain\",\n", "    list(\n\t        itertools.product(\n\t            [\"dermatology\", \"toy_categorical\"], os_compatible_toolchains()\n\t        )\n\t    ),\n\t)\n\tdef test_code_folding(tmpdir, annotation, dataset, toolchain, code_folding_factor):\n\t    \"\"\"Test suite for testing code folding feature\"\"\"\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n\t    model = load_example_model(dataset)\n", "    annotation_path = os.path.join(tmpdir, \"annotation.json\")\n\t    if annotation[dataset] is None:\n\t        annotation_path = None\n\t    else:\n\t        with open(annotation_path, \"w\", encoding=\"UTF-8\") as f:\n\t            f.write(annotation[dataset])\n\t    params = {\n\t        \"annotate_in\": (annotation_path if annotation_path else \"NULL\"),\n\t        \"quantize\": 1,\n\t        \"parallel_comp\": model.num_tree,\n", "        \"code_folding_req\": code_folding_factor,\n\t    }\n\t    tl2cgen.export_lib(\n\t        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    check_predictor(predictor, dataset)\n"]}
{"filename": "tests/python/test_invalid_categorical_input.py", "chunked_list": ["\"\"\"Test whether TL2cgen handles invalid category values correctly\"\"\"\n\timport pathlib\n\timport numpy as np\n\timport pytest\n\timport treelite\n\timport tl2cgen\n\tfrom tl2cgen.contrib.util import _libext\n\tfrom .util import os_compatible_toolchains\n\t@pytest.fixture(name=\"toy_model\")\n\tdef toy_model_fixture():\n", "    \"\"\"A toy model with a single tree containing a categorical split\"\"\"\n\t    builder = treelite.ModelBuilder(num_feature=2)\n\t    tree = treelite.ModelBuilder.Tree()\n\t    tree[0].set_categorical_test_node(\n\t        feature_id=1,\n\t        left_categories=[0],\n\t        default_left=True,\n\t        left_child_key=1,\n\t        right_child_key=2,\n\t    )\n", "    tree[1].set_leaf_node(-1.0)\n\t    tree[2].set_leaf_node(1.0)\n\t    tree[0].set_root()\n\t    builder.append(tree)\n\t    model = builder.commit()\n\t    return model\n\t@pytest.fixture(name=\"test_data\")\n\tdef test_data_fixture():\n\t    \"\"\"Generate test data consisting of two columns\"\"\"\n\t    categorical_column = np.array(\n", "        [-1, -0.6, -0.5, 0, 0.3, 0.7, 1, np.nan, np.inf, 1e10, -1e10], dtype=np.float32\n\t    )\n\t    dummy_column = np.zeros(categorical_column.shape[0], dtype=np.float32)\n\t    return np.column_stack((dummy_column, categorical_column))\n\t@pytest.fixture(name=\"ref_pred\")\n\tdef ref_pred_fixture():\n\t    \"\"\"Return correct output for the test data\"\"\"\n\t    # Negative inputs are mapped to the right child node\n\t    # 0.3 and 0.7 are mapped to the left child node, since they get rounded toward the zero.\n\t    # Missing value gets mapped to the left child node, since default_left=True\n", "    # inf, 1e10, and -1e10 don't match any element of left_categories, so they get mapped to the\n\t    # right child.\n\t    return np.array([1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1], dtype=np.float32).reshape(\n\t        (-1, 1)\n\t    )\n\tdef test_invalid_categorical_input(tmpdir, toy_model, test_data, ref_pred):\n\t    \"\"\"Test whether TL2cgen handles invalid category values correctly\"\"\"\n\t    libpath = pathlib.Path(tmpdir).joinpath(\"mylib\" + _libext())\n\t    toolchain = os_compatible_toolchains()[0]\n\t    tl2cgen.export_lib(toy_model, toolchain=toolchain, libpath=libpath)\n", "    predictor = tl2cgen.Predictor(libpath=libpath)\n\t    dmat = tl2cgen.DMatrix(test_data)\n\t    pred = predictor.predict(dmat)\n\t    np.testing.assert_equal(pred, ref_pred)\n"]}
{"filename": "tests/python/test_sklearn_integration.py", "chunked_list": ["\"\"\"Tests for scikit-learn importer\"\"\"\n\timport os\n\timport pathlib\n\timport numpy as np\n\timport pytest\n\timport treelite\n\tfrom hypothesis import given, settings\n\tfrom hypothesis.strategies import sampled_from\n\tfrom sklearn.datasets import load_breast_cancer, load_iris\n\tfrom sklearn.ensemble import (\n", "    ExtraTreesClassifier,\n\t    ExtraTreesRegressor,\n\t    GradientBoostingClassifier,\n\t    GradientBoostingRegressor,\n\t    IsolationForest,\n\t    RandomForestClassifier,\n\t    RandomForestRegressor,\n\t)\n\timport tl2cgen\n\tfrom tl2cgen.contrib.util import _libext\n", "from .hypothesis_util import standard_regression_datasets, standard_settings\n\tfrom .util import TemporaryDirectory, os_compatible_toolchains\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\n\t    \"clazz\", [RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier]\n\t)\n\tdef test_skl_converter_multiclass_classifier(tmpdir, clazz, toolchain):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Convert scikit-learn multi-class classifier\"\"\"\n\t    X, y = load_iris(return_X_y=True)\n", "    kwargs = {}\n\t    if clazz == GradientBoostingClassifier:\n\t        kwargs[\"init\"] = \"zero\"\n\t    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n\t    clf.fit(X, y)\n\t    expected_prob = clf.predict_proba(X)\n\t    model = treelite.sklearn.import_model(clf)\n\t    assert model.num_feature == clf.n_features_in_\n\t    assert model.num_class == clf.n_classes_\n\t    assert model.num_tree == clf.n_estimators * (\n", "        clf.n_classes_ if clazz == GradientBoostingClassifier else 1\n\t    )\n\t    dtrain = tl2cgen.DMatrix(X, dtype=\"float64\")\n\t    annotation_path = pathlib.Path(tmpdir) / \"annotation.json\"\n\t    tl2cgen.annotate_branch(model, dtrain, path=annotation_path, verbose=True)\n\t    libpath = pathlib.Path(tmpdir) / (\"skl\" + _libext())\n\t    tl2cgen.export_lib(\n\t        model,\n\t        toolchain=toolchain,\n\t        libpath=libpath,\n", "        params={\"annotate_in\": annotation_path},\n\t        verbose=True,\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath)\n\t    assert predictor.num_feature == clf.n_features_in_\n\t    assert predictor.num_class == clf.n_classes_\n\t    assert predictor.pred_transform == (\n\t        \"softmax\" if clazz == GradientBoostingClassifier else \"identity_multiclass\"\n\t    )\n\t    assert predictor.global_bias == 0.0\n", "    assert predictor.sigmoid_alpha == 1.0\n\t    out_prob = predictor.predict(dtrain)\n\t    np.testing.assert_almost_equal(out_prob, expected_prob)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\t@pytest.mark.parametrize(\n\t    \"clazz\", [RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier]\n\t)\n\tdef test_skl_converter_binary_classifier(tmpdir, clazz, toolchain):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Convert scikit-learn binary classifier\"\"\"\n", "    X, y = load_breast_cancer(return_X_y=True)\n\t    kwargs = {}\n\t    if clazz == GradientBoostingClassifier:\n\t        kwargs[\"init\"] = \"zero\"\n\t    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n\t    clf.fit(X, y)\n\t    expected_prob = clf.predict_proba(X)[:, 1:]\n\t    model = treelite.sklearn.import_model(clf)\n\t    assert model.num_feature == clf.n_features_in_\n\t    assert model.num_class == 1\n", "    assert model.num_tree == clf.n_estimators\n\t    dtrain = tl2cgen.DMatrix(X, dtype=\"float64\")\n\t    libpath = pathlib.Path(tmpdir) / (\"skl\" + _libext())\n\t    tl2cgen.export_lib(\n\t        model,\n\t        toolchain=toolchain,\n\t        libpath=libpath,\n\t        verbose=True,\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath)\n", "    assert predictor.num_feature == clf.n_features_in_\n\t    assert model.num_class == 1\n\t    assert predictor.pred_transform == (\n\t        \"sigmoid\" if clazz == GradientBoostingClassifier else \"identity\"\n\t    )\n\t    assert predictor.global_bias == 0.0\n\t    assert predictor.sigmoid_alpha == 1.0\n\t    out_prob = predictor.predict(dtrain)\n\t    np.testing.assert_almost_equal(out_prob, expected_prob)\n\t@given(\n", "    toolchain=sampled_from(os_compatible_toolchains()),\n\t    clazz=sampled_from(\n\t        [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor]\n\t    ),\n\t    dataset=standard_regression_datasets(),\n\t)\n\t@settings(**standard_settings())\n\tdef test_skl_converter_regressor(toolchain, clazz, dataset):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Convert scikit-learn regressor\"\"\"\n", "    X, y = dataset\n\t    kwargs = {}\n\t    if clazz == GradientBoostingRegressor:\n\t        kwargs[\"init\"] = \"zero\"\n\t    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n\t    clf.fit(X, y)\n\t    expected_pred = clf.predict(X).reshape((X.shape[0], -1))\n\t    model = treelite.sklearn.import_model(clf)\n\t    assert model.num_feature == clf.n_features_in_\n\t    assert model.num_class == 1\n", "    assert model.num_tree == clf.n_estimators\n\t    with TemporaryDirectory() as tmpdir:\n\t        dtrain = tl2cgen.DMatrix(X, dtype=\"float32\")\n\t        libpath = os.path.join(tmpdir, \"skl\" + _libext())\n\t        tl2cgen.export_lib(\n\t            model,\n\t            toolchain=toolchain,\n\t            libpath=libpath,\n\t            verbose=True,\n\t        )\n", "        predictor = tl2cgen.Predictor(libpath=libpath)\n\t        assert predictor.num_feature == clf.n_features_in_\n\t        assert model.num_class == 1\n\t        assert predictor.pred_transform == \"identity\"\n\t        assert predictor.global_bias == 0.0\n\t        assert predictor.sigmoid_alpha == 1.0\n\t        out_pred = predictor.predict(dtrain)\n\t        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)\n\t@given(\n\t    toolchain=sampled_from(os_compatible_toolchains()),\n", "    dataset=standard_regression_datasets(),\n\t)\n\t@settings(**standard_settings())\n\tdef test_skl_converter_iforest(toolchain, dataset):  # pylint: disable=W0212\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Convert scikit-learn Isolation forest\"\"\"\n\t    X, _ = dataset\n\t    clf = IsolationForest(max_samples=64, random_state=0, n_estimators=10)\n\t    clf.fit(X)\n\t    expected_pred = clf._compute_chunked_score_samples(X)  # pylint: disable=W0212\n", "    expected_pred = expected_pred.reshape((X.shape[0], -1))\n\t    model = treelite.sklearn.import_model(clf)\n\t    assert model.num_feature == clf.n_features_in_\n\t    assert model.num_class == 1\n\t    assert model.num_tree == clf.n_estimators\n\t    with TemporaryDirectory() as tmpdir:\n\t        dtrain = tl2cgen.DMatrix(X, dtype=\"float32\")\n\t        annotation_path = os.path.join(tmpdir, \"annotation.json\")\n\t        tl2cgen.annotate_branch(model, dmat=dtrain, path=annotation_path, verbose=True)\n\t        libpath = os.path.join(tmpdir, \"skl\" + _libext())\n", "        tl2cgen.export_lib(\n\t            model,\n\t            toolchain=toolchain,\n\t            libpath=libpath,\n\t            params={\"annotate_in\": annotation_path},\n\t            verbose=True,\n\t        )\n\t        predictor = tl2cgen.Predictor(libpath=libpath)\n\t        assert predictor.num_feature == clf.n_features_in_\n\t        assert model.num_class == 1\n", "        assert predictor.pred_transform == \"exponential_standard_ratio\"\n\t        assert predictor.global_bias == 0.0\n\t        assert predictor.sigmoid_alpha == 1.0\n\t        assert predictor.ratio_c > 0\n\t        assert predictor.ratio_c < 64\n\t        out_pred = predictor.predict(dtrain)\n\t        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=2)\n"]}
{"filename": "tests/python/util.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"Utility functions for tests\"\"\"\n\timport os\n\timport tempfile\n\tfrom contextlib import contextmanager\n\tfrom sys import platform as _platform\n\tfrom typing import Iterator, List, Optional, Tuple\n\timport numpy as np\n\tfrom sklearn.datasets import load_svmlight_file\n\timport tl2cgen\n", "from tl2cgen.contrib.util import _libext\n\tfrom .metadata import example_model_db\n\tdef load_txt(filename: str) -> Optional[np.ndarray]:\n\t    \"\"\"Get 1D array from text file\"\"\"\n\t    if filename is None:\n\t        return None\n\t    content = []\n\t    with open(filename, \"r\", encoding=\"UTF-8\") as f:\n\t        for line in f:\n\t            content.append(float(line))\n", "    return np.array(content, dtype=np.float32)\n\tdef os_compatible_toolchains() -> List[str]:\n\t    \"\"\"Get the list of C compilers to test with the current OS\"\"\"\n\t    if _platform == \"darwin\":\n\t        gcc = os.environ.get(\"GCC_PATH\", \"gcc\")\n\t        toolchains = [gcc]\n\t    elif _platform == \"win32\":\n\t        toolchains = [\"msvc\"]\n\t    else:\n\t        toolchains = [\"gcc\", \"clang\"]\n", "    return toolchains\n\tdef os_platform() -> str:\n\t    \"\"\"Detect OS that's running this program\"\"\"\n\t    if _platform == \"darwin\":\n\t        return \"osx\"\n\t    if _platform in [\"win32\", \"cygwin\"]:\n\t        return \"windows\"\n\t    return \"unix\"\n\tdef libname(fmt: str) -> str:\n\t    \"\"\"Format name for a shared library, using appropriate file extension\"\"\"\n", "    return fmt.format(_libext())\n\t@contextmanager\n\tdef does_not_raise() -> Iterator[None]:\n\t    \"\"\"Placeholder to indicate that a section of code is not expected to raise any exception\"\"\"\n\t    yield\n\tdef has_pandas():\n\t    \"\"\"Check whether pandas is available\"\"\"\n\t    try:\n\t        import pandas  # pylint: disable=unused-import,import-outside-toplevel\n\t        return True\n", "    except ImportError:\n\t        return False\n\tdef check_predictor(predictor: tl2cgen.Predictor, dataset: str) -> None:\n\t    \"\"\"Check whether a predictor produces correct predictions for a given dataset\"\"\"\n\t    dmat = tl2cgen.DMatrix(\n\t        load_svmlight_file(example_model_db[dataset].dtest, zero_based=True)[0],\n\t        dtype=example_model_db[dataset].dtype,\n\t    )\n\t    out_margin = predictor.predict(dmat, pred_margin=True)\n\t    out_prob = predictor.predict(dmat)\n", "    check_predictor_output(dataset, dmat.shape, out_margin, out_prob)\n\tdef check_predictor_output(\n\t    dataset: str, shape: Tuple[int, ...], out_margin: np.ndarray, out_prob: np.ndarray\n\t) -> None:\n\t    \"\"\"Check whether a predictor produces correct predictions\"\"\"\n\t    expected_margin = load_txt(example_model_db[dataset].expected_margin)\n\t    if expected_margin is not None:\n\t        if example_model_db[dataset].is_multiclass:\n\t            expected_margin = expected_margin.reshape((shape[0], -1))\n\t        else:\n", "            expected_margin = expected_margin.reshape((-1, 1))\n\t        assert (\n\t            out_margin.shape == expected_margin.shape\n\t        ), f\"out_margin.shape = {out_margin.shape}, expected_margin.shape = {expected_margin.shape}\"\n\t        np.testing.assert_almost_equal(out_margin, expected_margin, decimal=5)\n\t    expected_prob = load_txt(example_model_db[dataset].expected_prob)\n\t    if expected_prob is not None:\n\t        if example_model_db[dataset].is_multiclass:\n\t            expected_prob = expected_prob.reshape((shape[0], -1))\n\t        else:\n", "            expected_prob = expected_prob.reshape((-1, 1))\n\t        np.testing.assert_almost_equal(out_prob, expected_prob, decimal=5)\n\t@contextmanager\n\tdef TemporaryDirectory(*args, **kwargs) -> Iterator[str]:\n\t    # pylint: disable=C0103\n\t    \"\"\"\n\t    Simulate the effect of 'ignore_cleanup_errors' parameter of tempfile.TemporaryDirectory.\n\t    The parameter is only available for Python >= 3.10.\n\t    \"\"\"\n\t    if \"PYTEST_TMPDIR\" in os.environ and \"dir\" not in kwargs:\n", "        kwargs[\"dir\"] = os.environ[\"PYTEST_TMPDIR\"]\n\t    tmpdir = tempfile.TemporaryDirectory(*args, **kwargs)\n\t    try:\n\t        yield tmpdir.name\n\t    finally:\n\t        try:\n\t            tmpdir.cleanup()\n\t        except (PermissionError, NotADirectoryError):\n\t            if _platform != \"win32\":\n\t                raise\n"]}
{"filename": "tests/python/test_xgboost_integration.py", "chunked_list": ["\"\"\"Tests for XGBoost integration\"\"\"\n\t# pylint: disable=R0201, R0915\n\timport math\n\timport os\n\timport numpy as np\n\timport pytest\n\timport treelite\n\tfrom hypothesis import assume, given, settings\n\tfrom hypothesis.strategies import integers, sampled_from\n\tfrom sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split\n\timport tl2cgen\n\tfrom tl2cgen.contrib.util import _libext\n\tfrom .hypothesis_util import standard_regression_datasets, standard_settings\n\tfrom .metadata import format_libpath_for_example_model, load_example_model\n\tfrom .util import TemporaryDirectory, check_predictor, os_compatible_toolchains\n\ttry:\n\t    import xgboost as xgb\n\texcept ImportError:\n\t    # skip this test suite if XGBoost is not installed\n", "    pytest.skip(\"XGBoost not installed; skipping\", allow_module_level=True)\n\t@given(\n\t    toolchain=sampled_from(os_compatible_toolchains()),\n\t    objective=sampled_from(\n\t        [\n\t            \"reg:linear\",\n\t            \"reg:squarederror\",\n\t            \"reg:squaredlogerror\",\n\t            \"reg:pseudohubererror\",\n\t        ]\n", "    ),\n\t    model_format=sampled_from([\"binary\", \"json\"]),\n\t    num_parallel_tree=integers(min_value=1, max_value=10),\n\t    dataset=standard_regression_datasets(),\n\t)\n\t@settings(**standard_settings())\n\tdef test_xgb_regression(toolchain, objective, model_format, num_parallel_tree, dataset):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Test a random regression dataset\"\"\"\n\t    X, y = dataset\n", "    if objective == \"reg:squaredlogerror\":\n\t        assume(np.all(y > -1))\n\t    X_train, X_test, y_train, y_test = train_test_split(\n\t        X, y, test_size=0.2, shuffle=False\n\t    )\n\t    dtrain = xgb.DMatrix(X_train, label=y_train)\n\t    dtest = xgb.DMatrix(X_test, label=y_test)\n\t    param = {\n\t        \"max_depth\": 8,\n\t        \"eta\": 1,\n", "        \"verbosity\": 0,\n\t        \"objective\": objective,\n\t        \"num_parallel_tree\": num_parallel_tree,\n\t    }\n\t    num_round = 10\n\t    bst = xgb.train(\n\t        param,\n\t        dtrain,\n\t        num_boost_round=num_round,\n\t        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n", "    )\n\t    with TemporaryDirectory() as tmpdir:\n\t        if model_format == \"json\":\n\t            model_name = \"regression.json\"\n\t            model_path = os.path.join(tmpdir, model_name)\n\t            bst.save_model(model_path)\n\t            model = treelite.Model.load(\n\t                filename=model_path, model_format=\"xgboost_json\"\n\t            )\n\t        else:\n", "            model_name = \"regression.model\"\n\t            model_path = os.path.join(tmpdir, model_name)\n\t            bst.save_model(model_path)\n\t            model = treelite.Model.load(filename=model_path, model_format=\"xgboost\")\n\t        assert model.num_feature == dtrain.num_col()\n\t        assert model.num_class == 1\n\t        assert model.num_tree == num_round * num_parallel_tree\n\t        libpath = os.path.join(tmpdir, \"regression\" + _libext())\n\t        tl2cgen.export_lib(\n\t            model,\n", "            toolchain=toolchain,\n\t            libpath=libpath,\n\t            params={\"parallel_comp\": model.num_tree},\n\t            verbose=True,\n\t        )\n\t        predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t        assert predictor.num_feature == dtrain.num_col()\n\t        assert predictor.num_class == 1\n\t        assert predictor.pred_transform == \"identity\"\n\t        assert predictor.global_bias == 0.5\n", "        assert predictor.sigmoid_alpha == 1.0\n\t        dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n\t        out_pred = predictor.predict(dmat)\n\t        expected_pred = bst.predict(dtest, strict_shape=True)\n\t        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=3)\n\t@pytest.mark.parametrize(\"num_parallel_tree\", [1, 3, 5])\n\t@pytest.mark.parametrize(\"model_format\", [\"binary\", \"json\"])\n\t@pytest.mark.parametrize(\n\t    \"objective,expected_pred_transform\",\n\t    [(\"multi:softmax\", \"max_index\"), (\"multi:softprob\", \"softmax\")],\n", "    ids=[\"multi:softmax\", \"multi:softprob\"],\n\t)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_xgb_iris(\n\t    tmpdir,\n\t    toolchain,\n\t    objective,\n\t    model_format,\n\t    expected_pred_transform,\n\t    num_parallel_tree,\n", "):\n\t    # pylint: disable=too-many-locals, too-many-arguments\n\t    \"\"\"Test Iris data (multi-class classification)\"\"\"\n\t    X, y = load_iris(return_X_y=True)\n\t    X_train, X_test, y_train, y_test = train_test_split(\n\t        X, y, test_size=0.2, shuffle=False\n\t    )\n\t    dtrain = xgb.DMatrix(X_train, label=y_train)\n\t    dtest = xgb.DMatrix(X_test, label=y_test)\n\t    num_class = 3\n", "    num_round = 10\n\t    param = {\n\t        \"max_depth\": 6,\n\t        \"eta\": 0.05,\n\t        \"num_class\": num_class,\n\t        \"verbosity\": 0,\n\t        \"objective\": objective,\n\t        \"metric\": \"mlogloss\",\n\t        \"num_parallel_tree\": num_parallel_tree,\n\t    }\n", "    bst = xgb.train(\n\t        param,\n\t        dtrain,\n\t        num_boost_round=num_round,\n\t        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n\t    )\n\t    if model_format == \"json\":\n\t        model_name = \"iris.json\"\n\t        model_path = os.path.join(tmpdir, model_name)\n\t        bst.save_model(model_path)\n", "        model = treelite.Model.load(filename=model_path, model_format=\"xgboost_json\")\n\t    else:\n\t        model_name = \"iris.model\"\n\t        model_path = os.path.join(tmpdir, model_name)\n\t        bst.save_model(model_path)\n\t        model = treelite.Model.load(filename=model_path, model_format=\"xgboost\")\n\t    assert model.num_feature == dtrain.num_col()\n\t    assert model.num_class == num_class\n\t    assert model.num_tree == num_round * num_class * num_parallel_tree\n\t    libpath = os.path.join(tmpdir, \"iris\" + _libext())\n", "    tl2cgen.export_lib(\n\t        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    assert predictor.num_feature == dtrain.num_col()\n\t    assert predictor.num_class == num_class\n\t    assert predictor.pred_transform == expected_pred_transform\n\t    assert predictor.global_bias == 0.5\n\t    assert predictor.sigmoid_alpha == 1.0\n\t    dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n", "    out_pred = predictor.predict(dmat)\n\t    expected_pred = bst.predict(dtest, strict_shape=True)\n\t    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)\n\t@pytest.mark.parametrize(\"model_format\", [\"binary\", \"json\"])\n\t@pytest.mark.parametrize(\n\t    \"objective,max_label,expected_global_bias\",\n\t    [\n\t        (\"binary:logistic\", 2, 0),\n\t        (\"binary:hinge\", 2, 0.5),\n\t        (\"binary:logitraw\", 2, 0.5),\n", "        (\"count:poisson\", 4, math.log(0.5)),\n\t        (\"rank:pairwise\", 5, 0.5),\n\t        (\"rank:ndcg\", 5, 0.5),\n\t        (\"rank:map\", 5, 0.5),\n\t    ],\n\t    ids=[\n\t        \"binary:logistic\",\n\t        \"binary:hinge\",\n\t        \"binary:logitraw\",\n\t        \"count:poisson\",\n", "        \"rank:pairwise\",\n\t        \"rank:ndcg\",\n\t        \"rank:map\",\n\t    ],\n\t)\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_nonlinear_objective(\n\t    tmpdir, objective, max_label, expected_global_bias, toolchain, model_format\n\t):\n\t    # pylint: disable=too-many-locals,too-many-arguments\n", "    \"\"\"Test non-linear objectives with dummy data\"\"\"\n\t    np.random.seed(0)\n\t    nrow = 16\n\t    ncol = 8\n\t    X = np.random.randn(nrow, ncol)\n\t    y = np.random.randint(0, max_label, size=nrow)\n\t    assert np.min(y) == 0\n\t    assert np.max(y) == max_label - 1\n\t    num_round = 4\n\t    dtrain = xgb.DMatrix(X, label=y)\n", "    if objective.startswith(\"rank:\"):\n\t        dtrain.set_group([nrow])\n\t    bst = xgb.train(\n\t        {\"objective\": objective, \"base_score\": 0.5, \"seed\": 0},\n\t        dtrain=dtrain,\n\t        num_boost_round=num_round,\n\t    )\n\t    objective_tag = objective.replace(\":\", \"_\")\n\t    if model_format == \"json\":\n\t        model_name = f\"nonlinear_{objective_tag}.json\"\n", "    else:\n\t        model_name = f\"nonlinear_{objective_tag}.bin\"\n\t    model_path = os.path.join(tmpdir, model_name)\n\t    bst.save_model(model_path)\n\t    model = treelite.Model.load(\n\t        filename=model_path,\n\t        model_format=(\"xgboost_json\" if model_format == \"json\" else \"xgboost\"),\n\t    )\n\t    assert model.num_feature == dtrain.num_col()\n\t    assert model.num_class == 1\n", "    assert model.num_tree == num_round\n\t    libpath = os.path.join(tmpdir, objective_tag + _libext())\n\t    tl2cgen.export_lib(\n\t        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n\t    )\n\t    expected_pred_transform = {\n\t        \"binary:logistic\": \"sigmoid\",\n\t        \"binary:hinge\": \"hinge\",\n\t        \"binary:logitraw\": \"identity\",\n\t        \"count:poisson\": \"exponential\",\n", "        \"rank:pairwise\": \"identity\",\n\t        \"rank:ndcg\": \"identity\",\n\t        \"rank:map\": \"identity\",\n\t    }\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    assert predictor.num_feature == dtrain.num_col()\n\t    assert predictor.num_class == 1\n\t    assert predictor.pred_transform == expected_pred_transform[objective]\n\t    np.testing.assert_almost_equal(\n\t        predictor.global_bias, expected_global_bias, decimal=5\n", "    )\n\t    assert predictor.sigmoid_alpha == 1.0\n\t    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n\t    out_pred = predictor.predict(dmat)\n\t    expected_pred = bst.predict(dtrain, strict_shape=True)\n\t    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)\n\t@given(\n\t    toolchain=sampled_from(os_compatible_toolchains()),\n\t    dataset=standard_regression_datasets(),\n\t)\n", "@settings(**standard_settings())\n\tdef test_xgb_deserializers(toolchain, dataset):\n\t    # pylint: disable=too-many-locals\n\t    \"\"\"Test a random regression dataset and test serializers\"\"\"\n\t    X, y = dataset\n\t    X_train, X_test, y_train, y_test = train_test_split(\n\t        X, y, test_size=0.2, shuffle=False\n\t    )\n\t    dtrain = xgb.DMatrix(X_train, label=y_train)\n\t    dtest = xgb.DMatrix(X_test, label=y_test)\n", "    param = {\"max_depth\": 8, \"eta\": 1, \"silent\": 1, \"objective\": \"reg:linear\"}\n\t    num_round = 10\n\t    bst = xgb.train(\n\t        param,\n\t        dtrain,\n\t        num_boost_round=num_round,\n\t        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n\t    )\n\t    with TemporaryDirectory() as tmpdir:\n\t        # Serialize xgboost model\n", "        model_bin_path = os.path.join(tmpdir, \"serialized.model\")\n\t        bst.save_model(model_bin_path)\n\t        model_json_path = os.path.join(tmpdir, \"serialized.json\")\n\t        bst.save_model(model_json_path)\n\t        model_json_str = bst.save_raw(raw_format=\"json\")\n\t        # Construct Treelite models from xgboost serializations\n\t        model_bin = treelite.Model.load(model_bin_path, model_format=\"xgboost\")\n\t        model_json = treelite.Model.load(model_json_path, model_format=\"xgboost_json\")\n\t        model_json_str = treelite.Model.from_xgboost_json(model_json_str)\n\t        # Compile models to libraries\n", "        libext = _libext()\n\t        model_bin_lib = os.path.join(tmpdir, f\"bin{libext}\")\n\t        tl2cgen.export_lib(\n\t            model_bin,\n\t            toolchain=toolchain,\n\t            libpath=model_bin_lib,\n\t            params={\"parallel_comp\": model_bin.num_tree},\n\t        )\n\t        model_json_lib = os.path.join(tmpdir, f\"json{libext}\")\n\t        tl2cgen.export_lib(\n", "            model_json,\n\t            toolchain=toolchain,\n\t            libpath=model_json_lib,\n\t            params={\"parallel_comp\": model_json.num_tree},\n\t        )\n\t        model_json_str_lib = os.path.join(tmpdir, f\"json_str{libext}\")\n\t        tl2cgen.export_lib(\n\t            model_json_str,\n\t            toolchain=toolchain,\n\t            libpath=model_json_str_lib,\n", "            params={\"parallel_comp\": model_json_str.num_tree},\n\t        )\n\t        # Generate predictors from compiled libraries\n\t        predictor_bin = tl2cgen.Predictor(model_bin_lib)\n\t        assert predictor_bin.num_feature == dtrain.num_col()\n\t        assert predictor_bin.num_class == 1\n\t        assert predictor_bin.pred_transform == \"identity\"\n\t        assert predictor_bin.global_bias == pytest.approx(0.5)\n\t        assert predictor_bin.sigmoid_alpha == pytest.approx(1.0)\n\t        predictor_json = tl2cgen.Predictor(model_json_lib)\n", "        assert predictor_json.num_feature == dtrain.num_col()\n\t        assert predictor_json.num_class == 1\n\t        assert predictor_json.pred_transform == \"identity\"\n\t        assert predictor_json.global_bias == pytest.approx(0.5)\n\t        assert predictor_json.sigmoid_alpha == pytest.approx(1.0)\n\t        predictor_json_str = tl2cgen.Predictor(model_json_str_lib)\n\t        assert predictor_json_str.num_feature == dtrain.num_col()\n\t        assert predictor_json_str.num_class == 1\n\t        assert predictor_json_str.pred_transform == \"identity\"\n\t        assert predictor_json_str.global_bias == pytest.approx(0.5)\n", "        assert predictor_json_str.sigmoid_alpha == pytest.approx(1.0)\n\t        # Run inference with each predictor\n\t        dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n\t        bin_pred = predictor_bin.predict(dmat)\n\t        json_pred = predictor_json.predict(dmat)\n\t        json_str_pred = predictor_json_str.predict(dmat)\n\t        expected_pred = bst.predict(dtest, strict_shape=True)\n\t        np.testing.assert_almost_equal(bin_pred, expected_pred, decimal=4)\n\t        np.testing.assert_almost_equal(json_pred, expected_pred, decimal=4)\n\t        np.testing.assert_almost_equal(json_str_pred, expected_pred, decimal=4)\n", "@pytest.mark.parametrize(\"parallel_comp\", [None, 5])\n\t@pytest.mark.parametrize(\"quantize\", [True, False])\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_xgb_categorical_split(tmpdir, toolchain, quantize, parallel_comp):\n\t    \"\"\"Test toy XGBoost model with categorical splits\"\"\"\n\t    dataset = \"xgb_toy_categorical\"\n\t    model = load_example_model(dataset)\n\t    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n\t    params = {\n\t        \"quantize\": (1 if quantize else 0),\n", "        \"parallel_comp\": (parallel_comp if parallel_comp else 0),\n\t    }\n\t    tl2cgen.export_lib(\n\t        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath)\n\t    check_predictor(predictor, dataset)\n\t@pytest.mark.parametrize(\"model_format\", [\"binary\", \"json\"])\n\t@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n\tdef test_xgb_dart(tmpdir, toolchain, model_format):\n", "    # pylint: disable=too-many-locals,too-many-arguments\n\t    \"\"\"Test dart booster with dummy data\"\"\"\n\t    np.random.seed(0)\n\t    nrow = 16\n\t    ncol = 8\n\t    X = np.random.randn(nrow, ncol)\n\t    y = np.random.randint(0, 2, size=nrow)\n\t    assert np.min(y) == 0\n\t    assert np.max(y) == 1\n\t    num_round = 50\n", "    dtrain = xgb.DMatrix(X, label=y)\n\t    param = {\n\t        \"booster\": \"dart\",\n\t        \"max_depth\": 5,\n\t        \"learning_rate\": 0.1,\n\t        \"objective\": \"binary:logistic\",\n\t        \"sample_type\": \"uniform\",\n\t        \"normalize_type\": \"tree\",\n\t        \"rate_drop\": 0.1,\n\t        \"skip_drop\": 0.5,\n", "    }\n\t    bst = xgb.train(param, dtrain=dtrain, num_boost_round=num_round)\n\t    if model_format == \"json\":\n\t        model_json_path = os.path.join(tmpdir, \"serialized.json\")\n\t        bst.save_model(model_json_path)\n\t        model = treelite.Model.load(model_json_path, model_format=\"xgboost_json\")\n\t    else:\n\t        model_bin_path = os.path.join(tmpdir, \"serialized.model\")\n\t        bst.save_model(model_bin_path)\n\t        model = treelite.Model.load(model_bin_path, model_format=\"xgboost\")\n", "    assert model.num_feature == dtrain.num_col()\n\t    assert model.num_class == 1\n\t    assert model.num_tree == num_round\n\t    libpath = os.path.join(tmpdir, \"dart\" + _libext())\n\t    tl2cgen.export_lib(\n\t        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n\t    )\n\t    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n\t    assert predictor.num_feature == dtrain.num_col()\n\t    assert predictor.num_class == 1\n", "    assert predictor.pred_transform == \"sigmoid\"\n\t    np.testing.assert_almost_equal(predictor.global_bias, 0, decimal=5)\n\t    assert predictor.sigmoid_alpha == 1.0\n\t    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n\t    out_pred = predictor.predict(dmat)\n\t    expected_pred = bst.predict(dtrain, strict_shape=True)\n\t    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)\n"]}
{"filename": "tests/python/conftest.py", "chunked_list": ["\"\"\"Pytest fixtures to initialize tests\"\"\"\n\timport pathlib\n\timport tempfile\n\timport pytest\n\timport treelite\n\tfrom sklearn.datasets import load_svmlight_file\n\timport tl2cgen\n\tfrom .metadata import example_model_db\n\t@pytest.fixture(scope=\"session\")\n\tdef annotation():\n", "    \"\"\"Pre-computed branch annotation information for example datasets\"\"\"\n\t    with tempfile.TemporaryDirectory(dir=\".\") as tmpdir:\n\t        def compute_annotation(dataset):\n\t            model = treelite.Model.load(\n\t                example_model_db[dataset].model,\n\t                model_format=example_model_db[dataset].format,\n\t            )\n\t            if example_model_db[dataset].dtrain is None:\n\t                return None\n\t            dtrain = tl2cgen.DMatrix(\n", "                load_svmlight_file(example_model_db[dataset].dtrain, zero_based=True)[0]\n\t            )\n\t            annotation_path = pathlib.Path(tmpdir) / f\"{dataset}.json\"\n\t            tl2cgen.annotate_branch(\n\t                model=model,\n\t                dmat=dtrain,\n\t                path=annotation_path,\n\t                verbose=True,\n\t            )\n\t            with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n", "                return f.read()\n\t        annotation_db = {k: compute_annotation(k) for k in example_model_db}\n\t    return annotation_db\n"]}
{"filename": "java_runtime/tl2cgen4j/create_jni.py", "chunked_list": ["#!/usr/bin/env python\n\t\"\"\"Build native lib for tl2cgen4j\"\"\"\n\t# pylint: disable=invalid-name\n\timport errno\n\timport os\n\timport shutil\n\timport subprocess\n\timport sys\n\tfrom contextlib import contextmanager\n\t@contextmanager\n", "def cd(path):\n\t    \"\"\"Change current working directory temporarily\"\"\"\n\t    path = normpath(path)\n\t    cwd = os.getcwd()\n\t    os.chdir(path)\n\t    print(\"cd \" + path)\n\t    try:\n\t        yield path\n\t    finally:\n\t        os.chdir(cwd)\n", "def maybe_makedirs(path):\n\t    \"\"\"Make directory if not exist\"\"\"\n\t    path = normpath(path)\n\t    print(\"mkdir -p \" + path)\n\t    try:\n\t        os.makedirs(path)\n\t    except OSError as e:\n\t        if e.errno != errno.EEXIST:\n\t            raise\n\tdef run(command, **kwargs):\n", "    \"\"\"Run command\"\"\"\n\t    print(command)\n\t    subprocess.check_call(command, shell=True, **kwargs)\n\tdef cp(source, target):\n\t    \"\"\"Copy file\"\"\"\n\t    source = normpath(source)\n\t    target = normpath(target)\n\t    print(f\"cp {source} {target}\")\n\t    shutil.copy(source, target)\n\tdef normpath(path):\n", "    \"\"\"Normalize UNIX path to a native path.\"\"\"\n\t    normalized = os.path.join(*path.split(\"/\"))\n\t    if os.path.isabs(path):\n\t        return os.path.abspath(\"/\") + normalized\n\t    return normalized\n\tif __name__ == \"__main__\":\n\t    if sys.platform == \"darwin\":\n\t        os.environ[\"JAVA_HOME\"] = (\n\t            subprocess.check_output(\"/usr/libexec/java_home\").strip().decode()\n\t        )\n", "    print(\"Building tl2cgen4j library\")\n\t    with cd(\"../..\"):\n\t        maybe_makedirs(\"build\")\n\t        with cd(\"build\"):\n\t            if sys.platform == \"win32\":\n\t                maybe_generator = ' -G\"Visual Studio 17 2022\" -A x64'\n\t            else:\n\t                maybe_generator = \"\"\n\t            if sys.platform == \"linux\":\n\t                maybe_parallel_build = \" -- -j$(nproc)\"\n", "            else:\n\t                maybe_parallel_build = \"\"\n\t            if \"cpp-coverage\" in sys.argv:\n\t                maybe_generator += \" -DTEST_COVERAGE=ON\"\n\t            run(\n\t                \"cmake .. -DBUILD_JVM_RUNTIME=ON -DCMAKE_VERBOSE_MAKEFILE=ON\"\n\t                + maybe_generator\n\t            )\n\t            run(\"cmake --build . --config Release\" + maybe_parallel_build)\n\t    print(\"Copying tl2cgen4j library\")\n", "    library_name = {\n\t        \"win32\": \"tl2cgen4j.dll\",\n\t        \"darwin\": \"libtl2cgen4j.dylib\",\n\t        \"linux\": \"libtl2cgen4j.so\",\n\t    }[sys.platform]\n\t    maybe_makedirs(\"src/main/resources/lib\")\n\t    cp(\"../../build/java_runtime/\" + library_name, \"src/main/resources/lib\")\n\t    print(\"building mushroom example\")\n\t    with cd(\"src/test/resources/mushroom_example\"):\n\t        run(\"cmake . \" + maybe_generator)\n", "        run(\"cmake --build . --config Release\")\n"]}
{"filename": "python/hatch_build.py", "chunked_list": ["\"\"\"\n\tCustom hook to customize the behavior of Hatchling.\n\tHere, we customize the tag of the generated wheels.\n\t\"\"\"\n\timport sysconfig\n\tfrom typing import Any, Dict\n\tfrom hatchling.builders.hooks.plugin.interface import BuildHookInterface\n\tdef get_tag() -> str:\n\t    \"\"\"Get appropriate wheel tag according to system\"\"\"\n\t    tag_platform = sysconfig.get_platform().replace(\"-\", \"_\").replace(\".\", \"_\")\n", "    return f\"py3-none-{tag_platform}\"\n\tclass CustomBuildHook(BuildHookInterface):\n\t    \"\"\"A custom build hook\"\"\"\n\t    def initialize(self, version: str, build_data: Dict[str, Any]) -> None:\n\t        \"\"\"This step ccurs immediately before each build.\"\"\"\n\t        build_data[\"tag\"] = get_tag()\n"]}
{"filename": "python/tl2cgen/libloader.py", "chunked_list": ["# coding: utf-8\n\t\"\"\"Find the path to TL2cgen dynamic library files.\"\"\"\n\timport ctypes\n\timport os\n\timport pathlib\n\timport sys\n\timport warnings\n\tfrom typing import List\n\tfrom .exception import TL2cgenError, TL2cgenLibraryNotFound\n\tfrom .util import py_str\n", "def _find_lib_path() -> List[pathlib.Path]:\n\t    \"\"\"Find the path to TL2cgen dynamic library files.\n\t    Returns\n\t    -------\n\t    lib_path\n\t       List of all found library path to TL2cgen\n\t    \"\"\"\n\t    curr_path = pathlib.Path(__file__).expanduser().absolute().parent\n\t    dll_path = [\n\t        # When installed, libtl2cgen will be installed in <site-package-dir>/lib\n", "        curr_path / \"lib\",\n\t        # Editable installation\n\t        curr_path.parent.parent / \"build\",\n\t        # Use libtl2cgen from a system prefix, if available. This should be the last option.\n\t        pathlib.Path(sys.prefix).expanduser().resolve() / \"lib\",\n\t    ]\n\t    if sys.platform == \"win32\":\n\t        # On Windows, Conda may install libs in different paths\n\t        sys_prefix = pathlib.Path(sys.prefix)\n\t        dll_path.extend(\n", "            [\n\t                sys_prefix / \"bin\",\n\t                sys_prefix / \"Library\",\n\t                sys_prefix / \"Library\" / \"bin\",\n\t                sys_prefix / \"Library\" / \"lib\",\n\t            ]\n\t        )\n\t        dll_path = [p.joinpath(\"tl2cgen.dll\") for p in dll_path]\n\t    elif sys.platform.startswith((\"linux\", \"freebsd\", \"emscripten\", \"OS400\")):\n\t        dll_path = [p.joinpath(\"libtl2cgen.so\") for p in dll_path]\n", "    elif sys.platform == \"darwin\":\n\t        dll_path = [p.joinpath(\"libtl2cgen.dylib\") for p in dll_path]\n\t    elif sys.platform == \"cygwin\":\n\t        dll_path = [p.joinpath(\"cygtl2cgen.dll\") for p in dll_path]\n\t    else:\n\t        raise RuntimeError(f\"Unrecognized platform: {sys.platform}\")\n\t    lib_path = [p for p in dll_path if p.exists() and p.is_file()]\n\t    # TL2CGEN_BUILD_DOC is defined by sphinx conf.\n\t    if not lib_path and not os.environ.get(\"TL2CGEN_BUILD_DOC\", False):\n\t        link = \"https://tl2cgen.readthedocs.io/en/latest/install.html\"\n", "        msg = (\n\t            \"Cannot find TL2cgen Library in the candidate path.  \"\n\t            + \"List of candidates:\\n- \"\n\t            + (\"\\n- \".join(str(x) for x in dll_path))\n\t            + \"\\nTL2cgen Python package path: \"\n\t            + str(curr_path)\n\t            + \"\\nsys.prefix: \"\n\t            + sys.prefix\n\t            + \"\\nSee: \"\n\t            + link\n", "            + \" for installing TL2cgen.\"\n\t        )\n\t        raise TL2cgenLibraryNotFound(msg)\n\t    return lib_path\n\t@ctypes.CFUNCTYPE(None, ctypes.c_char_p)\n\tdef _log_callback(msg: bytes) -> None:\n\t    \"\"\"Redirect logs from native library into Python console\"\"\"\n\t    print(msg.decode(\"utf-8\"))\n\t@ctypes.CFUNCTYPE(None, ctypes.c_char_p)\n\tdef _warn_callback(msg: bytes) -> None:\n", "    \"\"\"Redirect warnings from native library into Python console\"\"\"\n\t    warnings.warn(msg.decode(\"utf-8\"))\n\tdef _load_lib() -> ctypes.CDLL:\n\t    \"\"\"Load TL2cgen Library.\"\"\"\n\t    lib_paths = _find_lib_path()\n\t    if not lib_paths:\n\t        # This happens only when building document.\n\t        return None  # type: ignore\n\t    if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n\t        # pylint: disable=no-member\n", "        os.add_dll_directory(\n\t            os.path.join(os.path.normpath(sys.prefix), \"Library\", \"bin\")\n\t        )\n\t    os_error_list = []\n\t    lib = None\n\t    for lib_path in lib_paths:\n\t        try:\n\t            lib = ctypes.cdll.LoadLibrary(str(lib_path))\n\t            setattr(lib, \"path\", lib_path)\n\t        except OSError as e:\n", "            os_error_list.append(str(e))\n\t            continue\n\t    if not lib:\n\t        libname = lib_paths[0].name\n\t        raise TL2cgenError(\n\t            f\"\"\"\n\tTL2cgen Library ({libname}) could not be loaded.\n\tLikely causes:\n\t  * OpenMP runtime is not installed\n\t    - vcomp140.dll or libgomp-1.dll for Windows\n", "    - libomp.dylib for Mac OSX\n\t    - libgomp.so for Linux and other UNIX-like OSes\n\t    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\t  * You are running 32-bit Python on a 64-bit OS\n\tError message(s): {os_error_list}\n\t\"\"\"\n\t        )\n\t    lib.TL2cgenGetLastError.restype = ctypes.c_char_p\n\t    lib.log_callback = _log_callback  # type: ignore\n\t    lib.warn_callback = _warn_callback  # type: ignore\n", "    if lib.TL2cgenRegisterLogCallback(lib.log_callback) != 0:\n\t        raise TL2cgenError(py_str(lib.TL2cgenGetLastError()))\n\t    if lib.TL2cgenRegisterWarningCallback(lib.warn_callback) != 0:\n\t        raise TL2cgenError(py_str(lib.TL2cgenGetLastError()))\n\t    return lib\n\tdef _check_call(ret):\n\t    \"\"\"Check the return value of C API call\n\t    This function will raise exception when error occurs.\n\t    Wrap every API call with this function\n\t    Parameters\n", "    ----------\n\t    ret : int\n\t        return value from API calls\n\t    \"\"\"\n\t    if ret != 0:\n\t        raise TL2cgenError(_LIB.TL2cgenGetLastError().decode(\"utf-8\"))\n\t# Load native library in the global scope\n\t_LIB = _load_lib()\n"]}
{"filename": "python/tl2cgen/exception.py", "chunked_list": ["\"\"\"Exception classes used in TL2cgen\"\"\"\n\tclass TL2cgenError(Exception):\n\t    \"\"\"Error thrown by TL2cgen\"\"\"\n\tclass TL2cgenLibraryNotFound(Exception):\n\t    \"\"\"Error thrown by when TL2cgen is not found\"\"\"\n"]}
{"filename": "python/tl2cgen/dtypes.py", "chunked_list": ["\"\"\"Utility functions to handle types\"\"\"\n\timport ctypes\n\tfrom typing import Any, Dict\n\timport numpy as np\n\timport numpy.typing as npt\n\t_CTYPES_TYPE_TABLE: Dict[str, Any] = {\n\t    \"uint32\": ctypes.c_uint32,\n\t    \"float32\": ctypes.c_float,\n\t    \"float64\": ctypes.c_double,\n\t}\n", "_NUMPY_TYPE_TABLE: Dict[str, npt.DTypeLike] = {\n\t    \"uint32\": np.uint32,\n\t    \"float32\": np.float32,\n\t    \"float64\": np.float64,\n\t}\n\tdef type_info_to_ctypes_type(type_info: str) -> Any:\n\t    \"\"\"Obtain ctypes type corresponding to a given TypeInfo\"\"\"\n\t    return _CTYPES_TYPE_TABLE[type_info]\n\tdef type_info_to_numpy_type(type_info: str) -> npt.DTypeLike:\n\t    \"\"\"Obtain ctypes type corresponding to a given TypeInfo\"\"\"\n", "    return _NUMPY_TYPE_TABLE[type_info]\n\tdef numpy_type_to_type_info(type_info: npt.DTypeLike) -> str:\n\t    \"\"\"Obtain TypeInfo corresponding to a given NumPy type\"\"\"\n\t    if type_info == np.uint32:\n\t        return \"uint32\"\n\t    if type_info == np.float32:\n\t        return \"float32\"\n\t    if type_info == np.float64:\n\t        return \"float64\"\n\t    raise ValueError(f\"Unrecognized NumPy type: {type_info}\")\n"]}
{"filename": "python/tl2cgen/create_shared.py", "chunked_list": ["\"\"\"Launcher for C compiler to build shared libs\"\"\"\n\timport pathlib\n\timport time\n\timport warnings\n\tfrom multiprocessing import cpu_count\n\tfrom typing import List, Optional, Union\n\tfrom .contrib.gcc import _create_shared_gcc\n\tfrom .contrib.msvc import _create_shared_msvc\n\tfrom .contrib.util import _toolchain_exist_check\n\tfrom .exception import TL2cgenError\n", "from .util import _open_and_validate_recipe, _process_options\n\tdef create_shared(\n\t    toolchain: str,\n\t    dirpath: Union[str, pathlib.Path],\n\t    *,\n\t    nthread: Optional[int] = None,\n\t    verbose: bool = False,\n\t    options: Optional[List[str]] = None,\n\t    long_build_time_warning: bool = True,\n\t):  # pylint: disable=R0914\n", "    \"\"\"Create shared library.\n\t    Parameters\n\t    ----------\n\t    toolchain :\n\t        Which toolchain to use. You may choose one of \"msvc\", \"clang\", and \"gcc\".\n\t        You may also specify a specific variation of clang or gcc (e.g. \"gcc-7\")\n\t    dirpath :\n\t        Directory containing the header and source files previously generated\n\t        by :py:meth:`generate_c_code`. The directory must contain recipe.json\n\t        which specifies build dependencies.\n", "    nthread :\n\t        Number of threads to use in creating the shared library.\n\t        Defaults to the number of cores in the system.\n\t    verbose :\n\t        Whether to produce extra messages\n\t    options :\n\t        Additional options to pass to toolchain\n\t    long_build_time_warning :\n\t        If set to False, suppress the warning about potentially long build time\n\t    Returns\n", "    -------\n\t    libpath :\n\t        Absolute path of created shared library\n\t    Example\n\t    -------\n\t    The following command uses Visual C++ toolchain to generate\n\t    ``./my/model/model.dll``:\n\t    .. code-block:: python\n\t        tl2cgen.generate_c_code(model, dirpath=\"./my/model\",\n\t                                params={})\n", "        tl2cgen.create_shared(toolchain=\"msvc\", dirpath=\"./my/model\")\n\t    Later, the shared library can be referred to by its directory name:\n\t    .. code-block:: python\n\t        predictor = tl2cgen.Predictor(libpath=\"./my/model\")\n\t        # looks for ./my/model/model.dll\n\t    Alternatively, one may specify the library down to its file name:\n\t    .. code-block:: python\n\t        predictor = tl2cgen.Predictor(libpath=\"./my/model/model.dll\")\n\t    \"\"\"\n\t    # pylint: disable=R0912\n", "    if nthread is None or nthread <= 0:\n\t        ncore = cpu_count()\n\t        nthread = ncore\n\t    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n\t    if not dirpath.exists() or not dirpath.is_dir():\n\t        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n\t    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n\t    options = _process_options(options)\n\t    # Write warning for potentially long compile time\n\t    if long_build_time_warning:\n", "        warn = False\n\t        for source in recipe[\"sources\"]:\n\t            if int(source[\"length\"]) > 10000:\n\t                warn = True\n\t                break\n\t        if warn:\n\t            warnings.warn(\n\t                \"WARNING: some of the source files are long. Expect long build time. \"\n\t                \"You may want to adjust the parameter 'parallel_comp'.\"\n\t            )\n", "    tstart = time.perf_counter()\n\t    _toolchain_exist_check(toolchain)\n\t    if toolchain == \"msvc\":\n\t        _create_shared = _create_shared_msvc\n\t    else:\n\t        _create_shared = _create_shared_gcc\n\t    libpath = _create_shared(\n\t        dirpath, toolchain, recipe, nthread=nthread, options=options, verbose=verbose\n\t    )\n\t    if verbose:\n", "        elapsed_time = time.perf_counter() - tstart\n\t        print(f\"Generated shared library in {elapsed_time:.2f} seconds\")\n\t    return libpath\n"]}
{"filename": "python/tl2cgen/__init__.py", "chunked_list": ["# coding: utf-8\n\t\"\"\"\n\tTL2cgen (TreeLite 2 C GENerator):\n\tModel compiler for decision tree ensembles\n\t\"\"\"\n\tfrom .core import _py_version, annotate_branch, generate_c_code\n\tfrom .create_shared import create_shared\n\tfrom .data import DMatrix\n\tfrom .exception import TL2cgenError\n\tfrom .generate_makefile import generate_cmakelists, generate_makefile\n", "from .predictor import Predictor\n\tfrom .shortcuts import export_lib, export_srcpkg\n\t__version__ = _py_version()\n\t__all__ = [\n\t    \"annotate_branch\",\n\t    \"create_shared\",\n\t    \"export_lib\",\n\t    \"export_srcpkg\",\n\t    \"generate_c_code\",\n\t    \"generate_cmakelists\",\n", "    \"generate_makefile\",\n\t    \"DMatrix\",\n\t    \"Predictor\",\n\t    \"TL2cgenError\",\n\t]\n"]}
{"filename": "python/tl2cgen/core.py", "chunked_list": ["\"\"\"Core module of TL2cgen\"\"\"\n\timport pathlib\n\tfrom typing import Any, Dict, Optional, Union\n\timport treelite\n\tfrom .data import DMatrix\n\tfrom .handle_class import _Annotator, _Compiler, _TreeliteModel\n\tdef _py_version() -> str:\n\t    \"\"\"Get the TL2cgen version from Python version file.\"\"\"\n\t    version_file = pathlib.Path(__file__).parent / \"VERSION\"\n\t    with open(version_file, encoding=\"utf-8\") as f:\n", "        return f.read().strip()\n\tdef generate_c_code(\n\t    model: treelite.Model,\n\t    dirpath: Union[str, pathlib.Path],\n\t    params: Optional[Dict[str, Any]],\n\t    compiler: str = \"ast_native\",\n\t    *,\n\t    verbose: bool = False,\n\t) -> None:\n\t    \"\"\"\n", "    Generate prediction code from a tree ensemble model. The code will be C99\n\t    compliant. One header file (.h) will be generated, along with one or more\n\t    source files (.c). Use :py:meth:`create_shared` method to package\n\t    prediction code as a dynamic shared library (.so/.dll/.dylib).\n\t    Parameters\n\t    ----------\n\t    model :\n\t        Model to convert to C code\n\t    dirpath :\n\t        Directory to store header and source files\n", "    params :\n\t        Parameters for compiler. See :py:doc:`this page </compiler_param>`\n\t        for the list of compiler parameters.\n\t    compiler :\n\t        Kind of C code generator to use. Currently, there are two possible values:\n\t        {\"ast_native\", \"failsafe\"}\n\t    verbose :\n\t        Whether to print extra messages during compilation\n\t    Example\n\t    -------\n", "    The following populates the directory ``./model`` with source and header\n\t    files:\n\t    .. code-block:: python\n\t       tl2cgen.compile(model, dirpath=\"./my/model\", params={}, verbose=True)\n\t    If parallel compilation is enabled (parameter ``parallel_comp``), the files\n\t    are in the form of ``./my/model/header.h``, ``./my/model/main.c``,\n\t    ``./my/model/tu0.c``, ``./my/model/tu1.c`` and so forth, depending on\n\t    the value of ``parallel_comp``. Otherwise, there will be exactly two files:\n\t    ``./model/header.h``, ``./my/model/main.c``\n\t    \"\"\"\n", "    _model = _TreeliteModel(model)\n\t    compiler_obj = _Compiler(params, compiler, verbose)\n\t    compiler_obj.compile(_model, dirpath)\n\tdef annotate_branch(\n\t    model: treelite.Model,\n\t    dmat: DMatrix,\n\t    path: Union[str, pathlib.Path],\n\t    *,\n\t    nthread: Optional[int] = None,\n\t    verbose: bool = False,\n", ") -> None:\n\t    \"\"\"\n\t    Annotate branches in a given model using frequency patterns in the training data and save\n\t    the annotation data to a JSON file. Each node gets the count of the instances that belong to it.\n\t    Parameters\n\t    ----------\n\t    dmat :\n\t        Data matrix representing the training data\n\t    path :\n\t        Location of JSON file\n", "    model :\n\t        Model to annotate\n\t    nthread :\n\t        Number of threads to use while annotating. If missing, use all physical cores in the system.\n\t    verbose :\n\t        Whether to print extra messages\n\t    \"\"\"\n\t    _model = _TreeliteModel(model)\n\t    nthread = nthread if nthread is not None else 0\n\t    annotator = _Annotator(_model, dmat, nthread, verbose)\n", "    annotator.save(path)\n"]}
{"filename": "python/tl2cgen/data.py", "chunked_list": ["\"\"\"Data matrix\"\"\"\n\timport ctypes\n\tfrom typing import Optional, Tuple, Union\n\timport numpy as np\n\timport numpy.typing as npt\n\timport scipy  # type: ignore\n\tfrom .dtypes import (\n\t    numpy_type_to_type_info,\n\t    type_info_to_ctypes_type,\n\t    type_info_to_numpy_type,\n", ")\n\tfrom .exception import TL2cgenError\n\tfrom .libloader import _LIB, _check_call\n\tfrom .util import c_str\n\tclass DMatrix:\n\t    \"\"\"Data matrix used in TL2cgen.\n\t    Parameters\n\t    ----------\n\t    data :\n\t        Data source\n", "    dtype :\n\t        If specified, the data will be casted into the corresponding data type.\n\t    missing :\n\t        Value in the data that represents a missing entry. If set to ``None``,\n\t        ``numpy.nan`` will be used.\n\t    \"\"\"\n\t    # pylint: disable=R0902,R0903,R0913\n\t    def __init__(\n\t        self,\n\t        data: Union[str, npt.NDArray, scipy.sparse.csr_matrix],\n", "        *,\n\t        dtype: Optional[str] = None,\n\t        missing: Optional[float] = None,\n\t    ):\n\t        if data is None:\n\t            raise TL2cgenError(\"'data' argument cannot be None\")\n\t        self.handle = ctypes.c_void_p()\n\t        if isinstance(data, (str,)):\n\t            raise TL2cgenError(\n\t                \"'data' argument cannot be a string. Did you mean to load data from a text file? \"\n", "                \"Please use the following packages to load the text file:\\n\"\n\t                \"   * CSV file: Use pandas.read_csv() or numpy.loadtxt()\\n\"\n\t                \"   * LIBSVM file: Use sklearn.datasets.load_svmlight_file()\"\n\t            )\n\t        if isinstance(data, scipy.sparse.csr_matrix):\n\t            self._init_from_csr(data, dtype=dtype)\n\t        elif isinstance(data, scipy.sparse.csc_matrix):\n\t            self._init_from_csr(data.tocsr(), dtype=dtype)\n\t        elif isinstance(data, np.ndarray):\n\t            self._init_from_npy2d(data, missing=missing, dtype=dtype)\n", "        else:  # any type that's convertible to CSR matrix is O.K.\n\t            try:\n\t                csr = scipy.sparse.csr_matrix(data)\n\t                self._init_from_csr(csr, dtype=dtype)\n\t            except Exception as e:\n\t                raise TypeError(\n\t                    f\"Cannot initialize DMatrix from {type(data).__name__}\"\n\t                ) from e\n\t        num_row, num_col, nelem = self._get_dims()\n\t        self.shape = (num_row, num_col)\n", "        self.size = nelem\n\t    def _init_from_csr(\n\t        self, csr: scipy.sparse.csr_matrix, *, dtype: Optional[str] = None\n\t    ) -> None:\n\t        \"\"\"Initialize data from a CSR (Compressed Sparse Row) matrix\"\"\"\n\t        if len(csr.indices) != len(csr.data):\n\t            raise ValueError(\n\t                f\"indices and data not of same length: {len(csr.indices)} vs {len(csr.data)}\"\n\t            )\n\t        if len(csr.indptr) != csr.shape[0] + 1:\n", "            raise ValueError(\n\t                \"len(indptr) must be equal to 1 + [number of rows]\"\n\t                f\"len(indptr) = {len(csr.indptr)} vs 1 + [number of rows] = {1 + csr.shape[0]}\"\n\t            )\n\t        if csr.indptr[-1] != len(csr.data):\n\t            raise ValueError(\n\t                \"last entry of indptr must be equal to len(data)\"\n\t                f\"indptr[-1] = {csr.indptr[-1]} vs len(data) = {len(csr.data)}\"\n\t            )\n\t        if dtype is None:\n", "            data_type = csr.data.dtype\n\t        else:\n\t            data_type = type_info_to_numpy_type(dtype)\n\t        data_type_code = numpy_type_to_type_info(data_type)\n\t        data_ptr_type = ctypes.POINTER(type_info_to_ctypes_type(data_type_code))\n\t        if data_type_code not in [\"float32\", \"float64\"]:\n\t            raise ValueError(\"data should be either float32 or float64 type\")\n\t        data = np.array(csr.data, copy=False, dtype=data_type, order=\"C\")\n\t        indices = np.array(csr.indices, copy=False, dtype=np.uintc, order=\"C\")\n\t        indptr = np.array(csr.indptr, copy=False, dtype=np.uintp, order=\"C\")\n", "        _check_call(\n\t            _LIB.TL2cgenDMatrixCreateFromCSR(\n\t                data.ctypes.data_as(data_ptr_type),\n\t                c_str(data_type_code),\n\t                indices.ctypes.data_as(ctypes.POINTER(ctypes.c_uint)),\n\t                indptr.ctypes.data_as(ctypes.POINTER(ctypes.c_size_t)),\n\t                ctypes.c_size_t(csr.shape[0]),\n\t                ctypes.c_size_t(csr.shape[1]),\n\t                ctypes.byref(self.handle),\n\t            )\n", "        )\n\t    def _init_from_npy2d(\n\t        self,\n\t        mat: npt.NDArray,\n\t        *,\n\t        missing: Optional[float] = None,\n\t        dtype: Optional[str] = None,\n\t    ) -> None:\n\t        \"\"\"\n\t        Initialize data from a 2-D numpy matrix.\n", "        If ``mat`` does not have ``order='C'`` (also known as row-major) or is not\n\t        contiguous, a temporary copy will be made.\n\t        If ``mat`` does not have ``dtype=numpy.float32``, a temporary copy will be\n\t        made also.\n\t        Thus, as many as two temporary copies of data can be made. One should set\n\t        input layout and type judiciously to conserve memory.\n\t        \"\"\"\n\t        if len(mat.shape) != 2:\n\t            raise ValueError(\"Input numpy.ndarray must be two-dimensional\")\n\t        data_type: npt.DTypeLike = (\n", "            mat.dtype if dtype is None else type_info_to_numpy_type(dtype)\n\t        )\n\t        data_type_code = numpy_type_to_type_info(data_type)\n\t        data_ptr_type = ctypes.POINTER(type_info_to_ctypes_type(data_type_code))\n\t        if data_type_code not in [\"float32\", \"float64\"]:\n\t            raise ValueError(\"data should be either float32 or float64 type\")\n\t        # flatten the array by rows and ensure it is float32.\n\t        # we try to avoid data copies if possible\n\t        # (reshape returns a view when possible and we explicitly tell np.array to\n\t        #  avoid copying)\n", "        data = np.array(mat.reshape(mat.size), copy=False, dtype=data_type)\n\t        missing = missing if missing is not None else np.nan\n\t        missing_ar = np.array([missing], dtype=data_type, order=\"C\")\n\t        _check_call(\n\t            _LIB.TL2cgenDMatrixCreateFromMat(\n\t                data.ctypes.data_as(data_ptr_type),\n\t                c_str(data_type_code),\n\t                ctypes.c_size_t(mat.shape[0]),\n\t                ctypes.c_size_t(mat.shape[1]),\n\t                missing_ar.ctypes.data_as(data_ptr_type),\n", "                ctypes.byref(self.handle),\n\t            )\n\t        )\n\t    def _get_dims(self) -> Tuple[int, int, int]:\n\t        num_row = ctypes.c_size_t()\n\t        num_col = ctypes.c_size_t()\n\t        nelem = ctypes.c_size_t()\n\t        _check_call(\n\t            _LIB.TL2cgenDMatrixGetDimension(\n\t                self.handle,\n", "                ctypes.byref(num_row),\n\t                ctypes.byref(num_col),\n\t                ctypes.byref(nelem),\n\t            )\n\t        )\n\t        return num_row.value, num_col.value, nelem.value\n\t    def __del__(self):\n\t        if self.handle:\n\t            _check_call(_LIB.TL2cgenDMatrixFree(self.handle))\n\t            self.handle = None\n", "    def __repr__(self):\n\t        return (\n\t            f\"<{self.shape[0]}x{self.shape[1]} sparse matrix of type tl2cgen.DMatrix\\n\"\n\t            f\"        with {self.size} stored elements in Compressed Sparse Row format>\"\n\t        )\n"]}
{"filename": "python/tl2cgen/util.py", "chunked_list": ["\"\"\"Utility functions\"\"\"\n\timport ctypes\n\timport json\n\timport pathlib\n\tfrom typing import Any, Dict, List, Optional\n\tfrom .exception import TL2cgenError\n\tdef c_str(string):\n\t    \"\"\"Convert a Python string to C string\"\"\"\n\t    return ctypes.c_char_p(string.encode(\"utf-8\"))\n\tdef py_str(string):\n", "    \"\"\"Convert C string back to Python string\"\"\"\n\t    return string.decode(\"utf-8\")\n\tdef _open_and_validate_recipe(recipe_path: pathlib.Path) -> Dict[str, Any]:\n\t    \"\"\"Ensure that the build recipe contains necessary fields\"\"\"\n\t    try:\n\t        with open(recipe_path, \"r\", encoding=\"utf-8\") as f:\n\t            recipe = json.load(f)\n\t    except IOError as e:\n\t        raise TL2cgenError(\"Failed to open recipe.json\") from e\n\t    if \"sources\" not in recipe or \"target\" not in recipe:\n", "        raise TL2cgenError(\"Malformed recipe.json\")\n\t    return recipe\n\tdef _process_options(options: Optional[List[str]]) -> List[str]:\n\t    \"\"\"Ensure that options are in the form of list of strings.\"\"\"\n\t    if options is not None:\n\t        try:\n\t            _ = iter(options)\n\t            options = [str(x) for x in options]\n\t        except TypeError as e:\n\t            raise TL2cgenError(\"options must be a list of string\") from e\n", "    else:\n\t        options = []\n\t    return options\n"]}
{"filename": "python/tl2cgen/shortcuts.py", "chunked_list": ["\"\"\"Convenience functions\"\"\"\n\timport pathlib\n\timport shutil\n\tfrom tempfile import TemporaryDirectory\n\tfrom typing import Any, Dict, List, Optional, Union\n\timport treelite\n\tfrom .contrib.util import _toolchain_exist_check\n\tfrom .core import generate_c_code\n\tfrom .create_shared import create_shared\n\tfrom .generate_makefile import generate_cmakelists, generate_makefile\n", "def export_lib(\n\t    model: treelite.Model,\n\t    toolchain: str,\n\t    libpath: Union[str, pathlib.Path],\n\t    params: Optional[Dict[str, Any]] = None,\n\t    compiler: str = \"ast_native\",\n\t    *,\n\t    nthread: Optional[int] = None,\n\t    verbose: bool = False,\n\t    options: Optional[List[str]] = None,\n", "):\n\t    \"\"\"\n\t    Convenience function: Generate prediction code and immediately turn it\n\t    into a dynamic shared library. A temporary directory will be created to\n\t    hold the source files.\n\t    Parameters\n\t    ----------\n\t    model :\n\t        Model to convert to C code\n\t    toolchain :\n", "        Which toolchain to use. You may choose one of 'msvc', 'clang', and 'gcc'.\n\t        You may also specify a specific variation of clang or gcc (e.g. 'gcc-7')\n\t    libpath :\n\t        Location to save the generated dynamic shared library\n\t    params :\n\t        Parameters to be passed to the compiler. See\n\t        :py:doc:`this page </compiler_param>` for the list of compiler\n\t        parameters.\n\t    compiler :\n\t        Kind of C code generator to use. Currently, there are two possible values:\n", "        {\"ast_native\", \"failsafe\"}\n\t    nthread :\n\t        Number of threads to use in creating the shared library.\n\t        Defaults to the number of cores in the system.\n\t    verbose :\n\t        Whether to produce extra messages\n\t    options :\n\t        Additional options to pass to toolchain\n\t    Example\n\t    -------\n", "    The one-line command\n\t    .. code-block:: python\n\t        tl2cgen.export_lib(model, toolchain=\"msvc\", libpath=\"./mymodel.dll\",\n\t                           params={})\n\t    is equivalent to the following sequence of commands:\n\t    .. code-block:: python\n\t        tl2cgen.generate_c_code(model, dirpath=\"/temporary/directory\",\n\t                                params={})\n\t        tl2cgen.create_shared(toolchain=\"msvc\",\n\t                              dirpath=\"/temporary/directory\")\n", "        # Move the library out of the temporary directory\n\t        shutil.move(\"/temporary/directory/mymodel.dll\", \"./mymodel.dll\")\n\t    \"\"\"\n\t    _toolchain_exist_check(toolchain)\n\t    libpath = pathlib.Path(libpath).expanduser().resolve()\n\t    long_build_time_warning = not (params and \"parallel_comp\" in params)\n\t    with TemporaryDirectory() as tempdir:\n\t        generate_c_code(model, tempdir, params, compiler, verbose=verbose)\n\t        temp_libpath = create_shared(\n\t            toolchain,\n", "            tempdir,\n\t            nthread=nthread,\n\t            verbose=verbose,\n\t            options=options,\n\t            long_build_time_warning=long_build_time_warning,\n\t        )\n\t        if libpath.is_file():\n\t            libpath.unlink()\n\t        shutil.move(temp_libpath, libpath)\n\tdef export_srcpkg(\n", "    model: treelite.Model,\n\t    toolchain: str,\n\t    pkgpath: Union[str, pathlib.Path],\n\t    libname: str,\n\t    params: Optional[Dict[str, Any]] = None,\n\t    compiler: str = \"ast_native\",\n\t    *,\n\t    verbose: bool = False,\n\t    options: Optional[List[str]] = None,\n\t):  # pylint: disable=R0913\n", "    \"\"\"\n\t    Convenience function: Generate prediction code and create a zipped source\n\t    package for deployment. The resulting zip file will also contain a Makefile\n\t    (or CMakeLists.txt, if you set toolchain=\"cmake\").\n\t    Parameters\n\t    ----------\n\t    model :\n\t        Model to convert to C code\n\t    toolchain :\n\t        Which toolchain to use. You may choose one of \"msvc\", \"clang\", \"gcc\", and \"cmake\".\n", "        You may also specify a specific variation of clang or gcc (e.g. \"gcc-7\")\n\t    pkgpath :\n\t        Location to save the zipped source package\n\t    libname :\n\t        Name of model shared library to be built\n\t    params :\n\t        Parameters to be passed to the compiler. See\n\t        :py:doc:`this page </compiler_param>` for the list of compiler\n\t        parameters.\n\t    compiler :\n", "        Name of compiler to use in C code generation\n\t    verbose :\n\t        Whether to produce extra messages\n\t    options :\n\t        Additional options to pass to toolchain\n\t    Example\n\t    -------\n\t    The one-line command\n\t    .. code-block:: python\n\t        tl2cgen.export_srcpkg(model, toolchain=\"gcc\",\n", "                              pkgpath=\"./mymodel_pkg.zip\",\n\t                              libname=\"mymodel.so\", params={})\n\t    is equivalent to the following sequence of commands:\n\t    .. code-block:: python\n\t        tl2cgen.generate_c_code(model, dirpath=\"/temporary/directory/mymodel\",\n\t                                params={})\n\t        tl2cgen.generate_makefile(dirpath=\"/temporary/directory/mymodel\",\n\t                                  toolchain=\"gcc\")\n\t        # Zip the directory containing C code and Makefile\n\t        shutil.make_archive(base_name=\"./mymodel_pkg\", format=\"zip\",\n", "                            root_dir=\"/temporary/directory\",\n\t                            base_dir=\"mymodel/\")\n\t    \"\"\"\n\t    # Check for file extension\n\t    pkgpath = pathlib.Path(pkgpath).expanduser().resolve()\n\t    if pkgpath.suffix != \".zip\":\n\t        raise ValueError(\"Source package file should have .zip extension\")\n\t    if toolchain != \"cmake\":\n\t        _toolchain_exist_check(toolchain)\n\t    with TemporaryDirectory() as temp_dir:\n", "        target = pathlib.Path(libname).stem\n\t        # Create a child directory to get desired name for target\n\t        dirpath = pathlib.Path(temp_dir) / target\n\t        dirpath.mkdir()\n\t        if params is None:\n\t            params = {}\n\t        params[\"native_lib_name\"] = target\n\t        generate_c_code(model, dirpath, params, compiler, verbose=verbose)\n\t        if toolchain == \"cmake\":\n\t            generate_cmakelists(dirpath, options)\n", "        else:\n\t            generate_makefile(dirpath, toolchain, options)\n\t        shutil.make_archive(\n\t            base_name=str(pkgpath.with_suffix(\"\")),\n\t            format=\"zip\",\n\t            root_dir=temp_dir,\n\t            base_dir=target,\n\t        )\n"]}
{"filename": "python/tl2cgen/predictor.py", "chunked_list": ["\"\"\"\n\tPredictor module\n\t\"\"\"\n\timport ctypes\n\timport pathlib\n\tfrom typing import Optional, Union\n\tfrom .contrib.util import _libext\n\tfrom .data import DMatrix\n\tfrom .exception import TL2cgenError\n\tfrom .handle_class import _OutputVector\n", "from .libloader import _LIB, _check_call\n\tfrom .util import c_str, py_str\n\tclass Predictor:\n\t    \"\"\"\n\t    Predictor class is a convenient wrapper for loading shared libs.\n\t    TL2cgen uses OpenMP to launch multiple CPU threads to perform predictions\n\t    in parallel.\n\t    Parameters\n\t    ----------\n\t    libpath :\n", "        location of dynamic shared library (.dll/.so/.dylib)\n\t    nthread :\n\t        number of worker threads to use; if unspecified, use maximum number of\n\t        hardware threads\n\t    verbose :\n\t        Whether to print extra messages during construction\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        libpath: Union[str, pathlib.Path],\n", "        *,\n\t        nthread: Optional[int] = None,\n\t        verbose: bool = False,\n\t    ):\n\t        nthread = nthread if nthread is not None else -1\n\t        libpath = pathlib.Path(libpath).expanduser().resolve()\n\t        if libpath.is_dir():\n\t            # directory is given; locate shared library inside it\n\t            resolved_libpath = None\n\t            ext = _libext()\n", "            for candidate in libpath.glob(f\"*{ext}\"):\n\t                try:\n\t                    resolved_libpath = candidate.resolve(strict=True)\n\t                    break\n\t                except FileNotFoundError:\n\t                    continue\n\t            if not resolved_libpath:\n\t                raise TL2cgenError(\n\t                    f\"Directory {libpath} doesn't appear to have any dynamic shared library.\"\n\t                )\n", "        else:  # libpath is actually the name of shared library file\n\t            if not libpath.exists():\n\t                raise TL2cgenError(f\"Shared library not found at location {libpath}\")\n\t            resolved_libpath = libpath\n\t        self.handle = ctypes.c_void_p()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorLoad(\n\t                c_str(str(resolved_libpath)),\n\t                ctypes.c_int(nthread),\n\t                ctypes.byref(self.handle),\n", "            )\n\t        )\n\t        self._load_metadata(self.handle)\n\t        if verbose:\n\t            print(\n\t                f\"Dynamic shared library {resolved_libpath} has been successfully \"\n\t                \"loaded into memory\"\n\t            )\n\t    def __del__(self):\n\t        if self.handle:\n", "            _check_call(_LIB.TL2cgenPredictorFree(self.handle))\n\t            self.handle = None\n\t    @property\n\t    def num_feature(self):\n\t        \"\"\"Query number of features used in the model\"\"\"\n\t        return self.num_feature_\n\t    @property\n\t    def num_class(self):\n\t        \"\"\"Query number of output groups of the model\"\"\"\n\t        return self.num_class_\n", "    @property\n\t    def pred_transform(self):\n\t        \"\"\"Query pred transform of the model\"\"\"\n\t        return self.pred_transform_\n\t    @property\n\t    def global_bias(self):\n\t        \"\"\"Query global bias of the model\"\"\"\n\t        return self.global_bias_\n\t    @property\n\t    def sigmoid_alpha(self):\n", "        \"\"\"Query sigmoid alpha of the model\"\"\"\n\t        return self.sigmoid_alpha_\n\t    @property\n\t    def ratio_c(self):\n\t        \"\"\"Query sigmoid alpha of the model\"\"\"\n\t        return self.ratio_c_\n\t    @property\n\t    def threshold_type(self):\n\t        \"\"\"Query threshold type of the model\"\"\"\n\t        return self.threshold_type_\n", "    @property\n\t    def leaf_output_type(self):\n\t        \"\"\"Query threshold type of the model\"\"\"\n\t        return self.leaf_output_type_\n\t    def predict(\n\t        self,\n\t        dmat: DMatrix,\n\t        *,\n\t        verbose: bool = False,\n\t        pred_margin: bool = False,\n", "    ):\n\t        \"\"\"\n\t        Perform batch prediction with a 2D sparse data matrix. Worker threads will\n\t        internally divide up work for batch prediction. **Note that this function\n\t        may be called by only one thread at a time.**\n\t        Parameters\n\t        ----------\n\t        dmat:\n\t            Batch of rows for which predictions will be made\n\t        verbose :\n", "            Whether to print extra messages during prediction\n\t        pred_margin:\n\t            Whether to produce raw margins rather than transformed probabilities\n\t        \"\"\"\n\t        if not isinstance(dmat, DMatrix):\n\t            raise TL2cgenError(\"dmat must be of type DMatrix\")\n\t        result_size = ctypes.c_size_t()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryResultSize(\n\t                self.handle, dmat.handle, ctypes.byref(result_size)\n", "            )\n\t        )\n\t        out_result = _OutputVector(\n\t            predictor_handle=self.handle,\n\t            dmat_handle=dmat.handle,\n\t        )\n\t        out_result_size = ctypes.c_size_t()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorPredictBatch(\n\t                self.handle,\n", "                dmat.handle,\n\t                ctypes.c_int(1 if verbose else 0),\n\t                ctypes.c_int(1 if pred_margin else 0),\n\t                out_result.handle,\n\t                ctypes.byref(out_result_size),\n\t            )\n\t        )\n\t        out_result_array = out_result.toarray()\n\t        idx = int(out_result_size.value)\n\t        res = out_result_array[0:idx].reshape((dmat.shape[0], -1))\n", "        if self.num_class_ > 1 and dmat.shape[0] != idx:\n\t            res = res.reshape((-1, self.num_class_))\n\t        return res\n\t    def _load_metadata(self, handle: ctypes.c_void_p) -> None:\n\t        # Save # of features\n\t        num_feature = ctypes.c_size_t()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryNumFeature(handle, ctypes.byref(num_feature))\n\t        )\n\t        self.num_feature_ = num_feature.value\n", "        # Save # of classes\n\t        num_class = ctypes.c_size_t()\n\t        _check_call(_LIB.TL2cgenPredictorQueryNumClass(handle, ctypes.byref(num_class)))\n\t        self.num_class_ = num_class.value\n\t        # Save # of pred transform\n\t        pred_transform = ctypes.c_char_p()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryPredTransform(\n\t                handle, ctypes.byref(pred_transform)\n\t            )\n", "        )\n\t        self.pred_transform_ = py_str(pred_transform.value)\n\t        # Save # of sigmoid alpha\n\t        sigmoid_alpha = ctypes.c_float()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQuerySigmoidAlpha(handle, ctypes.byref(sigmoid_alpha))\n\t        )\n\t        self.sigmoid_alpha_ = sigmoid_alpha.value\n\t        # Save # of ratio C\n\t        ratio_c = ctypes.c_float()\n", "        _check_call(_LIB.TL2cgenPredictorQueryRatioC(handle, ctypes.byref(ratio_c)))\n\t        self.ratio_c_ = ratio_c.value\n\t        # Save # of global bias\n\t        global_bias = ctypes.c_float()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryGlobalBias(handle, ctypes.byref(global_bias))\n\t        )\n\t        self.global_bias_ = global_bias.value\n\t        # Save threshold type\n\t        threshold_type = ctypes.c_char_p()\n", "        _check_call(\n\t            _LIB.TL2cgenPredictorQueryThresholdType(\n\t                handle, ctypes.byref(threshold_type)\n\t            )\n\t        )\n\t        self.threshold_type_ = py_str(threshold_type.value)\n\t        # Save leaf output type\n\t        leaf_output_type = ctypes.c_char_p()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryLeafOutputType(\n", "                handle, ctypes.byref(leaf_output_type)\n\t            )\n\t        )\n\t        self.leaf_output_type_ = py_str(leaf_output_type.value)\n"]}
{"filename": "python/tl2cgen/generate_makefile.py", "chunked_list": ["\"\"\"Generator for Makefile and CMakeLists.txt\"\"\"\n\timport pathlib\n\tfrom typing import List, Optional, Union\n\tfrom .contrib import gcc, msvc\n\tfrom .contrib.util import _libext, _toolchain_exist_check\n\tfrom .exception import TL2cgenError\n\tfrom .util import _open_and_validate_recipe, _process_options\n\tdef generate_makefile(\n\t    dirpath: Union[str, pathlib.Path],\n\t    toolchain: str,\n", "    options: Optional[List[str]] = None,\n\t) -> None:\n\t    \"\"\"\n\t    Generate a Makefile for a given directory of headers and sources. The\n\t    resulting Makefile will be stored in the directory. This function is useful\n\t    for deploying a model on a different machine.\n\t    Parameters\n\t    ----------\n\t    dirpath :\n\t        Directory containing the header and source files previously generated\n", "        by :py:meth:`Model.compile`. The directory must contain recipe.json\n\t        which specifies build dependencies.\n\t    toolchain :\n\t        Which toolchain to use. You may choose one of 'msvc', 'clang', and 'gcc'.\n\t        You may also specify a specific variation of clang or gcc (e.g. 'gcc-7')\n\t    options :\n\t        Additional options to pass to toolchain\n\t    \"\"\"\n\t    # pylint: disable=R0912,R0914,W0212\n\t    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n", "    if not dirpath.is_dir():\n\t        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n\t    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n\t    options = _process_options(options)\n\t    # Determine file extensions for object and library files\n\t    _toolchain_exist_check(toolchain)\n\t    if toolchain == \"msvc\":\n\t        _obj_ext = msvc._obj_ext\n\t        _obj_cmd = msvc._obj_cmd\n\t        _lib_cmd = msvc._lib_cmd\n", "    else:\n\t        _obj_ext = gcc._obj_ext\n\t        _obj_cmd = gcc._obj_cmd\n\t        _lib_cmd = gcc._lib_cmd\n\t    obj_ext = _obj_ext()\n\t    lib_ext = _libext()\n\t    with open(dirpath / \"Makefile\", \"w\", encoding=\"UTF-8\") as f:\n\t        objects = [x[\"name\"] + obj_ext for x in recipe[\"sources\"]]\n\t        objects.extend(recipe.get(\"extra\", []))\n\t        target = recipe[\"target\"] + lib_ext\n", "        objects_str = \" \".join(objects)\n\t        lib_cmd = _lib_cmd(\n\t            objects=objects,\n\t            target=recipe[\"target\"],\n\t            lib_ext=lib_ext,\n\t            toolchain=toolchain,\n\t            options=options,\n\t        )\n\t        print(f\"{target}: {objects_str}\", file=f)\n\t        print(f\"\\t{lib_cmd}\", file=f)\n", "        for source in recipe[\"sources\"]:\n\t            source_file = source[\"name\"] + \".c\"\n\t            obj_file = source[\"name\"] + obj_ext\n\t            obj_cmd = _obj_cmd(\n\t                source=source[\"name\"], toolchain=toolchain, options=options\n\t            )\n\t            print(f\"{obj_file}: {source_file}\", file=f)\n\t            print(f\"\\t{obj_cmd}\", file=f)\n\tdef generate_cmakelists(\n\t    dirpath: Union[str, pathlib.Path],\n", "    options: Optional[List[str]] = None,\n\t) -> None:\n\t    \"\"\"\n\t    Generate a CMakeLists.txt for a given directory of headers and sources. The\n\t    resulting CMakeLists.txt will be stored in the directory. This function is useful\n\t    for deploying a model on a different machine.\n\t    Parameters\n\t    ----------\n\t    dirpath :\n\t        Directory containing the header and source files previously generated\n", "        by :py:meth:`Model.compile`. The directory must contain recipe.json\n\t        which specifies build dependencies.\n\t    options :\n\t        Additional options to pass to toolchain\n\t    \"\"\"\n\t    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n\t    if not dirpath.is_dir():\n\t        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n\t    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n\t    options = _process_options(options)\n", "    target = recipe[\"target\"]\n\t    sources = \" \".join([x[\"name\"] + \".c\" for x in recipe[\"sources\"]])\n\t    options_str = \" \".join(options)\n\t    with open(dirpath / \"CMakeLists.txt\", \"w\", encoding=\"UTF-8\") as f:\n\t        print(\"cmake_minimum_required(VERSION 3.13)\", file=f)\n\t        print(\"project(mushroom LANGUAGES C)\\n\", file=f)\n\t        print(f\"add_library({target} SHARED)\", file=f)\n\t        print(f\"target_sources({target} PRIVATE header.h {sources})\", file=f)\n\t        print(f\"target_compile_options({target} PRIVATE {options_str})\", file=f)\n\t        print(\n", "            f'target_include_directories({target} PRIVATE \"${{PROJECT_BINARY_DIR}}\")',\n\t            file=f,\n\t        )\n\t        print(f\"set_target_properties({target} PROPERTIES\", file=f)\n\t        print(\n\t            \"\"\"POSITION_INDEPENDENT_CODE ON\n\t            C_STANDARD 99\n\t            C_STANDARD_REQUIRED ON\n\t            PREFIX \"\"\n\t            RUNTIME_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}\"\n", "            RUNTIME_OUTPUT_DIRECTORY_DEBUG \"${PROJECT_BINARY_DIR}\"\n\t            RUNTIME_OUTPUT_DIRECTORY_RELEASE \"${PROJECT_BINARY_DIR}\"\n\t            RUNTIME_OUTPUT_DIRECTORY_RELWITHDEBINFO \"${PROJECT_BINARY_DIR}\"\n\t            RUNTIME_OUTPUT_DIRECTORY_MINSIZEREL \"${PROJECT_BINARY_DIR}\"\n\t            LIBRARY_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}\"\n\t            LIBRARY_OUTPUT_DIRECTORY_DEBUG \"${PROJECT_BINARY_DIR}\"\n\t            LIBRARY_OUTPUT_DIRECTORY_RELEASE \"${PROJECT_BINARY_DIR}\"\n\t            LIBRARY_OUTPUT_DIRECTORY_RELWITHDEBINFO \"${PROJECT_BINARY_DIR}\"\n\t            LIBRARY_OUTPUT_DIRECTORY_MINSIZEREL \"${PROJECT_BINARY_DIR}\")\n\t        \"\"\",\n", "            file=f,\n\t        )\n"]}
{"filename": "python/tl2cgen/handle_class.py", "chunked_list": ["\"\"\"Internal classes to hold native handles\"\"\"\n\timport ctypes\n\timport json\n\timport pathlib\n\tfrom typing import Any, Dict, Optional, Union\n\timport numpy as np\n\timport treelite\n\tfrom .data import DMatrix\n\tfrom .dtypes import type_info_to_ctypes_type\n\tfrom .libloader import _LIB, _check_call\n", "from .util import c_str, py_str\n\tclass _TreeliteModel:\n\t    \"\"\"\n\t    Internal class holding a handle to Treelite model. We maintain a separate internal class,\n\t    to maintain **loose coupling** between Treelite and TL2cgen. This way, TL2cgen can support\n\t    past and future versions of Treelite (within the same major version).\n\t    \"\"\"\n\t    def __init__(self, model: treelite.Model):\n\t        model_bytes = model.serialize_bytes()\n\t        model_bytes_len = len(model_bytes)\n", "        buffer = ctypes.create_string_buffer(model_bytes, model_bytes_len)\n\t        self.handle = ctypes.c_void_p()\n\t        _check_call(\n\t            _LIB.TL2cgenLoadTreeliteModelFromBytes(\n\t                ctypes.pointer(buffer),\n\t                ctypes.c_size_t(model_bytes_len),\n\t                ctypes.byref(self.handle),\n\t            )\n\t        )\n\t        major_ver, minor_ver, patch_ver = (\n", "            ctypes.c_int32(),\n\t            ctypes.c_int32(),\n\t            ctypes.c_int32(),\n\t        )\n\t        _check_call(\n\t            _LIB.TL2cgenQueryTreeliteModelVersion(\n\t                self.handle,\n\t                ctypes.byref(major_ver),\n\t                ctypes.byref(minor_ver),\n\t                ctypes.byref(patch_ver),\n", "            )\n\t        )\n\t        self.__version__ = f\"{major_ver.value}.{minor_ver.value}.{patch_ver.value}\"\n\t    def __del__(self):\n\t        if self.handle:\n\t            _check_call(_LIB.TL2cgenFreeTreeliteModel(self.handle))\n\t            self.handle = None\n\tclass _Annotator:\n\t    \"\"\"Annotator object\"\"\"\n\t    def __init__(\n", "        self,\n\t        model: _TreeliteModel,\n\t        dmat: DMatrix,\n\t        nthread: int,\n\t        verbose: bool = False,\n\t    ):\n\t        self.handle = ctypes.c_void_p()\n\t        _check_call(\n\t            _LIB.TL2cgenAnnotateBranch(\n\t                model.handle,\n", "                dmat.handle,\n\t                ctypes.c_int(nthread),\n\t                ctypes.c_int(1 if verbose else 0),\n\t                ctypes.byref(self.handle),\n\t            )\n\t        )\n\t    def save(self, path: Union[str, pathlib.Path]):\n\t        \"\"\"Save annotation data to a JSON file\"\"\"\n\t        path = pathlib.Path(path).expanduser().resolve()\n\t        _check_call(_LIB.TL2cgenAnnotationSave(self.handle, c_str(str(path))))\n", "    def __del__(self):\n\t        if self.handle:\n\t            _check_call(_LIB.TL2cgenAnnotationFree(self.handle))\n\t            self.handle = None\n\tclass _Compiler:\n\t    \"\"\"Compiler object\"\"\"\n\t    def __init__(\n\t        self,\n\t        params: Optional[Dict[str, Any]],\n\t        compiler: str = \"ast_native\",\n", "        verbose: bool = False,\n\t    ):\n\t        self.handle = ctypes.c_void_p()\n\t        if params is None:\n\t            params = {}\n\t        if verbose:\n\t            params[\"verbose\"] = 1\n\t        if isinstance(params.get(\"annotate_in\"), pathlib.Path):\n\t            params[\"annotate_in\"] = str(params[\"annotate_in\"])\n\t        params_json_str = json.dumps(params)\n", "        _check_call(\n\t            _LIB.TL2cgenCompilerCreate(\n\t                c_str(compiler), c_str(params_json_str), ctypes.byref(self.handle)\n\t            )\n\t        )\n\t    def compile(self, model: _TreeliteModel, dirpath: Union[str, pathlib.Path]) -> None:\n\t        \"\"\"Generate prediction code\"\"\"\n\t        dirpath = pathlib.Path(dirpath).expanduser().resolve()\n\t        _check_call(\n\t            _LIB.TL2cgenCompilerGenerateCode(\n", "                self.handle, model.handle, c_str(str(dirpath))\n\t            )\n\t        )\n\t    def __del__(self):\n\t        if self.handle:\n\t            _check_call(_LIB.TL2cgenCompilerFree(self.handle))\n\t            self.handle = None\n\tclass _OutputVector:\n\t    \"\"\"Output vector object, used to hold prediction results from Predictor\"\"\"\n\t    def __init__(\n", "        self,\n\t        predictor_handle: ctypes.c_void_p,\n\t        dmat_handle: ctypes.c_void_p,\n\t    ):\n\t        self.handle = ctypes.c_void_p()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorCreateOutputVector(\n\t                predictor_handle, dmat_handle, ctypes.byref(self.handle)\n\t            )\n\t        )\n", "        type_str = ctypes.c_char_p()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryLeafOutputType(\n\t                predictor_handle, ctypes.byref(type_str)\n\t            )\n\t        )\n\t        self.typestr_ = py_str(type_str.value)\n\t        length = ctypes.c_size_t()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorQueryResultSize(\n", "                predictor_handle, dmat_handle, ctypes.byref(length)\n\t            )\n\t        )\n\t        self.length_ = length.value\n\t    def __del__(self):\n\t        if self.handle:\n\t            _check_call(_LIB.TL2cgenPredictorDeleteOutputVector(self.handle))\n\t            self.handle = None\n\t    def toarray(self):\n\t        \"\"\"Convert to NumPy array\"\"\"\n", "        ptr = ctypes.c_void_p()\n\t        _check_call(\n\t            _LIB.TL2cgenPredictorGetRawPointerFromOutputVector(\n\t                self.handle, ctypes.byref(ptr)\n\t            )\n\t        )\n\t        ptr_type = ctypes.POINTER(type_info_to_ctypes_type(self.typestr_))\n\t        casted_ptr = ctypes.cast(ptr, ptr_type)\n\t        return np.copy(\n\t            np.ctypeslib.as_array(casted_ptr, shape=(self.length_,)), order=\"C\"\n", "        )\n"]}
{"filename": "python/tl2cgen/contrib/msvc.py", "chunked_list": ["\"\"\"\n\tTools to interact with Microsoft Visual C++ (MSVC)\n\t\"\"\"\n\timport itertools\n\timport os\n\timport pathlib\n\timport re\n\timport sys\n\tfrom typing import Any, Dict, List\n\tfrom packaging.version import parse as parse_version\n", "from .create_shared import _create_shared_base\n\tfrom .util import _libext\n\tLIBEXT = _libext()\n\tif sys.platform != \"win32\":\n\t    class WindowsError(Exception):  # pylint: disable=C0115,W0622\n\t        pass\n\tdef _is_64bit_windows() -> bool:\n\t    return \"PROGRAMFILES(X86)\" in os.environ\n\tdef _varsall_bat_path() -> pathlib.Path:  # pylint: disable=R0912\n\t    if sys.platform != \"win32\":\n", "        raise RuntimeError(\"_varsall_bat_path() supported only on Windows\")\n\t    # if a custom location is given, try that first\n\t    if \"TL2CGEN_VCVARSALL\" in os.environ:\n\t        candidate = pathlib.Path(os.environ[\"TL2CGEN_VCVARSALL\"]).resolve()\n\t        if candidate.name.lower() != \"vcvarsall.bat\":\n\t            raise OSError(\n\t                \"Environment variable TL2CGEN_VCVARSALL must point to file vcvarsall.bat\"\n\t            )\n\t        if candidate.is_file():\n\t            return candidate\n", "        raise OSError(\n\t            \"Environment variable TL2CGEN_VCVARSALL does not refer to existing vcvarsall.bat\"\n\t        )\n\t    # == Bunch of heuristics to locate vcvarsall.bat ==\n\t    # List of possible paths to vcvarsall.bat\n\t    candidate_paths = []\n\t    try:\n\t        import winreg  # pylint: disable=E0401,C0415\n\t        if _is_64bit_windows():\n\t            key_name = r\"SOFTWARE\\Wow6432Node\\Microsoft\\VisualStudio\\SxS\\VS7\"\n", "        else:\n\t            key_name = r\"SOFTWARE\\Microsoft\\VisualStudio\\SxS\\VC7\"\n\t        key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key_name)\n\t        i = 0\n\t        while True:\n\t            try:\n\t                version, vcroot, _ = winreg.EnumValue(key, i)\n\t                vcroot = pathlib.Path(vcroot).resolve()\n\t                if parse_version(version) >= parse_version(\"15.0\"):\n\t                    # Visual Studio 2017 revamped directory structure\n", "                    candidate_paths.append(vcroot / r\"VC\\Auxiliary\\Build\\vcvarsall.bat\")\n\t                else:\n\t                    candidate_paths.append(vcroot / r\"VC\\vcvarsall.bat\")\n\t            except WindowsError:  # pylint: disable=E0602\n\t                break\n\t            i += 1\n\t    except FileNotFoundError:\n\t        pass  # No registry key found\n\t    except ImportError:\n\t        pass  # No winreg module\n", "    for candidate in candidate_paths:\n\t        if candidate.is_file():\n\t            return candidate\n\t    # If registry method fails, try a bunch of pre-defined paths\n\t    # Visual Studio 2017 and higher\n\t    candidate_paths = list(\n\t        itertools.chain(\n\t            pathlib.Path(r\"C:\\Program Files (x86)\\Microsoft Visual Studio\").glob(\"*\"),\n\t            pathlib.Path(r\"C:\\Program Files\\Microsoft Visual Studio\").glob(\"*\"),\n\t        )\n", "    )\n\t    for vcroot in candidate_paths:\n\t        if re.fullmatch(r\"[0-9]+\", vcroot.name):\n\t            for candidate in vcroot.glob(r\"*\\VC\\Auxiliary\\Build\\vcvarsall.bat\"):\n\t                if candidate.is_file():\n\t                    return candidate\n\t    # Previous versions of Visual Studio\n\t    candidate_paths = list(\n\t        itertools.chain(\n\t            pathlib.Path(r\"C:\\Program Files (x86)\").glob(\n", "                r\"Microsoft Visual Studio*\\VC\\vcvarsall.bat\"\n\t            ),\n\t            pathlib.Path(r\"C:\\Program Files\").glob(\n\t                r\"Microsoft Visual Studio*\\VC\\vcvarsall.bat\"\n\t            ),\n\t        )\n\t    )\n\t    for candidate in candidate_paths:\n\t        if candidate.is_file():\n\t            return candidate\n", "    raise OSError(\n\t        \"vcvarsall.bat not found; please specify its full path in the environment \"\n\t        \"variable TL2CGEN_VCVARSALL\"\n\t    )\n\tdef _obj_ext() -> str:\n\t    return \".obj\"\n\t# pylint: disable=W0613\n\tdef _obj_cmd(\n\t    source: str,\n\t    toolchain: str,\n", "    options: List[str],\n\t):\n\t    source_file = source + \".c\"\n\t    options_str = \" \".join(options)\n\t    return f\"cl.exe /c /openmp /Ox {source_file} {options_str}\"\n\t# pylint: disable=W0613\n\tdef _lib_cmd(\n\t    objects: List[str],\n\t    target: str,\n\t    lib_ext: str,\n", "    toolchain: str,\n\t    options: List[str],\n\t) -> str:\n\t    objects_str = \" \".join(objects)\n\t    options_str = \" \".join(options)\n\t    return f\"cl.exe /LD /Fe{target} /openmp {objects_str} {options_str}\"\n\t# pylint: disable=R0913\n\tdef _create_shared_msvc(\n\t    dirpath: pathlib.Path,\n\t    toolchain: str,\n", "    recipe: Dict[str, Any],\n\t    *,\n\t    nthread: int,\n\t    options: List[str],\n\t    verbose: bool,\n\t) -> pathlib.Path:\n\t    # Specify command to compile an object file\n\t    recipe[\"object_ext\"] = _obj_ext()\n\t    recipe[\"library_ext\"] = LIBEXT\n\t    # pylint: disable=R0801\n", "    def _obj_cmd_wrapped(source: str) -> str:\n\t        return _obj_cmd(source, toolchain, options)\n\t    def _lib_cmd_wrapped(objects: List[str], target: str) -> str:\n\t        return _lib_cmd(objects, target, LIBEXT, toolchain, options)\n\t    recipe[\"create_object_cmd\"] = _obj_cmd_wrapped\n\t    recipe[\"create_library_cmd\"] = _lib_cmd_wrapped\n\t    plat_target = \"amd64\" if _is_64bit_windows() else \"x86\"\n\t    recipe[\"initial_cmd\"] = f'\"{_varsall_bat_path()}\" {plat_target}'\n\t    return _create_shared_base(dirpath, recipe, nthread=nthread, verbose=verbose)\n"]}
{"filename": "python/tl2cgen/contrib/create_shared.py", "chunked_list": ["\"\"\"Logic for launching C compiler to build shared libs\"\"\"\n\timport pathlib\n\timport subprocess\n\tfrom typing import Any, Dict\n\tfrom ..exception import TL2cgenError\n\tfrom .util import (\n\t    _create_log_cmd_unix,\n\t    _create_log_cmd_windows,\n\t    _is_windows,\n\t    _save_retcode_cmd_unix,\n", "    _save_retcode_cmd_windows,\n\t    _shell,\n\t)\n\tdef _enqueue(args: Dict[str, Any]) -> subprocess.Popen:\n\t    tid = args[\"tid\"]\n\t    queue = args[\"queue\"]\n\t    dirpath = args[\"dirpath\"]\n\t    init_cmd = args[\"init_cmd\"]\n\t    create_log_cmd = args[\"create_log_cmd\"]\n\t    save_retcode_cmd = args[\"save_retcode_cmd\"]\n", "    # pylint: disable=R1732\n\t    proc = subprocess.Popen(\n\t        _shell(),\n\t        shell=True,\n\t        stdin=subprocess.PIPE,\n\t        stdout=subprocess.PIPE,\n\t        stderr=subprocess.STDOUT,\n\t        cwd=dirpath,\n\t    )\n\t    assert proc.stdin is not None\n", "    proc.stdin.write((init_cmd + \"\\n\").encode())\n\t    proc.stdin.write(create_log_cmd(f\"retcode_cpu{tid}.txt\" + \"\\n\").encode())\n\t    for command in queue:\n\t        proc.stdin.write((command + \"\\n\").encode())\n\t        proc.stdin.write((save_retcode_cmd(f\"retcode_cpu{tid}.txt\") + \"\\n\").encode())\n\t    proc.stdin.flush()\n\t    return proc\n\tdef _wait(proc: subprocess.Popen, args: Dict[str, Any]):\n\t    tid = args[\"tid\"]\n\t    dirpath = args[\"dirpath\"]\n", "    stdout, _ = proc.communicate()\n\t    with open(dirpath / f\"retcode_cpu{tid}.txt\", \"r\", encoding=\"UTF-8\") as f:\n\t        retcode = [int(line) for line in f]\n\t    return {\"stdout\": stdout.decode(), \"retcode\": retcode}\n\tdef _create_shared_base(\n\t    dirpath: pathlib.Path,\n\t    recipe: Dict[str, Any],\n\t    *,\n\t    nthread: int,\n\t    verbose: bool,\n", "):  # pylint: disable=R0914\n\t    # Fetch toolchain-specific commands\n\t    obj_cmd = recipe[\"create_object_cmd\"]\n\t    lib_cmd = recipe[\"create_library_cmd\"]\n\t    create_log_cmd = _create_log_cmd_windows if _is_windows() else _create_log_cmd_unix\n\t    save_retcode_cmd = (\n\t        _save_retcode_cmd_windows if _is_windows() else _save_retcode_cmd_unix\n\t    )\n\t    # 1. Compile sources in parallel\n\t    if verbose:\n", "        obj_ext = recipe[\"object_ext\"]\n\t        print(\n\t            f\"Compiling sources files in directory {dirpath} into object files (*{obj_ext})...\"\n\t        )\n\t    workqueue = [\n\t        {\n\t            \"tid\": tid,\n\t            \"queue\": [],\n\t            \"dirpath\": dirpath,\n\t            \"init_cmd\": recipe[\"initial_cmd\"],\n", "            \"create_log_cmd\": create_log_cmd,\n\t            \"save_retcode_cmd\": save_retcode_cmd,\n\t        }\n\t        for tid in range(nthread)\n\t    ]\n\t    for i, source in enumerate(recipe[\"sources\"]):\n\t        workqueue[i % nthread][\"queue\"].append(obj_cmd(source[\"name\"]))\n\t    proc = [_enqueue(workqueue[tid]) for tid in range(nthread)]\n\t    result = []\n\t    for tid in range(nthread):\n", "        result.append(_wait(proc[tid], workqueue[tid]))\n\t    for tid in range(nthread):\n\t        if not all(x == 0 for x in result[tid][\"retcode\"]):\n\t            log_path = dirpath / f\"log_cpu{tid}.txt\"\n\t            with open(log_path, \"w\", encoding=\"UTF-8\") as f:\n\t                f.write(result[tid][\"stdout\"] + \"\\n\")\n\t            raise TL2cgenError(\n\t                f\"Error occured in worker #{tid}: \" + result[tid][\"stdout\"]\n\t            )\n\t    # 2. Package objects into a dynamic shared library\n", "    full_libpath = dirpath.joinpath(recipe[\"target\"] + recipe[\"library_ext\"])\n\t    if verbose:\n\t        print(f\"Generating dynamic shared library {full_libpath}...\")\n\t    objects = [x[\"name\"] + recipe[\"object_ext\"] for x in recipe[\"sources\"]]\n\t    objects.extend(recipe.get(\"extra\", []))\n\t    workqueue = [\n\t        {\n\t            \"tid\": 0,\n\t            \"queue\": [lib_cmd(objects, recipe[\"target\"])],\n\t            \"dirpath\": dirpath,\n", "            \"init_cmd\": recipe[\"initial_cmd\"],\n\t            \"create_log_cmd\": create_log_cmd,\n\t            \"save_retcode_cmd\": save_retcode_cmd,\n\t        }\n\t    ]\n\t    proc = [_enqueue(workqueue[0])]\n\t    result = [_wait(proc[0], workqueue[0])]\n\t    if result[0][\"retcode\"][0] != 0:\n\t        with open(dirpath / \"log_cpu0.txt\", \"w\", encoding=\"UTF-8\") as f:\n\t            f.write(result[0][\"stdout\"] + \"\\n\")\n", "        raise TL2cgenError(\n\t            \"Error occured while creating dynamic library: \" + result[0][\"stdout\"]\n\t        )\n\t    # 3. Clean up\n\t    for tid in range(nthread):\n\t        dirpath.joinpath(f\"retcode_cpu{tid}.txt\").unlink()\n\t    # Return full path of shared library\n\t    return dirpath.joinpath(full_libpath)\n"]}
{"filename": "python/tl2cgen/contrib/__init__.py", "chunked_list": ["\"\"\"\n\tContrib APIs of Treelite python package.\n\tContrib API provides ways to interact with third-party libraries and tools.\n\t\"\"\"\n"]}
{"filename": "python/tl2cgen/contrib/util.py", "chunked_list": ["\"\"\"Utilities for contrib module\"\"\"\n\timport os\n\timport subprocess\n\tfrom sys import platform as _platform\n\tdef _is_windows() -> bool:\n\t    return _platform == \"win32\"\n\tdef _shell() -> str:\n\t    if _is_windows():\n\t        return \"cmd.exe\"\n\t    if \"SHELL\" in os.environ:\n", "        return os.environ[\"SHELL\"]\n\t    return \"/bin/sh\"  # use POSIX-compliant shell if SHELL is not set\n\tdef _libext():\n\t    if _platform == \"darwin\":\n\t        return \".dylib\"\n\t    if _platform in (\"win32\", \"cygwin\"):\n\t        return \".dll\"\n\t    return \".so\"\n\tdef _toolchain_exist_check(toolchain: str) -> None:\n\t    if toolchain != \"msvc\":\n", "        retcode = subprocess.call(\n\t            f\"{toolchain} --version\",\n\t            shell=True,\n\t            stdin=subprocess.DEVNULL,\n\t            stdout=subprocess.DEVNULL,\n\t            stderr=subprocess.DEVNULL,\n\t        )\n\t        if retcode != 0:\n\t            raise ValueError(\n\t                f\"Toolchain {toolchain} not found. Ensure that it is installed and \"\n", "                \"that it is a variant of GCC or Clang.\"\n\t            )\n\tdef _create_log_cmd_unix(logfile: str) -> str:\n\t    return f\"true > {logfile}\"\n\tdef _save_retcode_cmd_unix(logfile: str) -> str:\n\t    if _shell().endswith(\"fish\"):  # special handling for fish shell\n\t        return f\"echo $status >> {logfile}\"\n\t    return f\"echo $? >> {logfile}\"\n\tdef _create_log_cmd_windows(logfile: str) -> str:\n\t    return f\"type NUL > {logfile}\"\n", "def _save_retcode_cmd_windows(logfile: str) -> str:\n\t    return f\"echo %errorlevel% >> {logfile}\"\n"]}
{"filename": "python/tl2cgen/contrib/gcc.py", "chunked_list": ["\"\"\"\n\tTools to interact with toolchains GCC, Clang, and other UNIX compilers\n\t\"\"\"\n\timport pathlib\n\tfrom typing import Any, Dict, List\n\tfrom .create_shared import _create_shared_base\n\tfrom .util import _libext\n\tLIBEXT = _libext()\n\tdef _obj_ext() -> str:\n\t    return \".o\"\n", "def _obj_cmd(\n\t    source: str,\n\t    toolchain: str,\n\t    options: List[str],\n\t) -> str:\n\t    obj_ext = _obj_ext()\n\t    source_file = source + \".c\"\n\t    obj_file = source + obj_ext\n\t    options_str = \" \".join(options)\n\t    return (\n", "        f\"{toolchain} -c -O3 -o {obj_file} {source_file} -fPIC -std=c99 {options_str}\"\n\t    )\n\tdef _lib_cmd(\n\t    objects: List[str],\n\t    target: str,\n\t    lib_ext: str,\n\t    toolchain: str,\n\t    options: List[str],\n\t) -> str:\n\t    objects_str = \" \".join(objects)\n", "    options_str = \" \".join(options)\n\t    return f\"{toolchain} -shared -O3 -o {target + lib_ext} {objects_str} -std=c99 {options_str}\"\n\tdef _create_shared_gcc(\n\t    dirpath: pathlib.Path,\n\t    toolchain: str,\n\t    recipe: Dict[str, Any],\n\t    *,\n\t    nthread: int,\n\t    options: List[str],\n\t    verbose: bool,\n", ") -> pathlib.Path:\n\t    options += [\"-lm\"]\n\t    # Specify command to compile an object file\n\t    recipe[\"object_ext\"] = _obj_ext()\n\t    recipe[\"library_ext\"] = LIBEXT\n\t    # pylint: disable=R0801\n\t    def _obj_cmd_wrapped(source: str) -> str:\n\t        return _obj_cmd(source, toolchain, options)\n\t    def _lib_cmd_wrapped(objects: List[str], target: str) -> str:\n\t        return _lib_cmd(objects, target, LIBEXT, toolchain, options)\n", "    recipe[\"create_object_cmd\"] = _obj_cmd_wrapped\n\t    recipe[\"create_library_cmd\"] = _lib_cmd_wrapped\n\t    recipe[\"initial_cmd\"] = \"\"\n\t    return _create_shared_base(dirpath, recipe, nthread=nthread, verbose=verbose)\n"]}
{"filename": "python/packager/nativelib.py", "chunked_list": ["\"\"\"\n\tFunctions for building libtl2cgen\n\t\"\"\"\n\timport logging\n\timport os\n\timport pathlib\n\timport shutil\n\timport subprocess\n\timport sys\n\tfrom platform import system\n", "from typing import Optional\n\tfrom .build_config import BuildConfiguration\n\tdef _lib_name() -> str:\n\t    \"\"\"Return platform dependent shared object name.\"\"\"\n\t    if system() in [\"Linux\", \"OS400\"] or system().upper().endswith(\"BSD\"):\n\t        name = \"libtl2cgen.so\"\n\t    elif system() == \"Darwin\":\n\t        name = \"libtl2cgen.dylib\"\n\t    elif system() == \"Windows\":\n\t        name = \"tl2cgen.dll\"\n", "    else:\n\t        raise NotImplementedError(f\"System {system()} not supported\")\n\t    return name\n\tdef build_libtl2cgen(\n\t    cpp_src_dir: pathlib.Path,\n\t    build_dir: pathlib.Path,\n\t    build_config: BuildConfiguration,\n\t) -> pathlib.Path:\n\t    \"\"\"Build libtl2cgen in a temporary directory and obtain the path to built libtl2cgen\"\"\"\n\t    logger = logging.getLogger(\"tl2cgen.packager.build_libtl2cgen\")\n", "    if not cpp_src_dir.is_dir():\n\t        raise RuntimeError(f\"Expected {cpp_src_dir} to be a directory\")\n\t    logger.info(\n\t        \"Building %s from the C++ source files in %s...\", _lib_name(), str(cpp_src_dir)\n\t    )\n\t    def _build(*, generator: str) -> None:\n\t        cmake_cmd = [\n\t            \"cmake\",\n\t            str(cpp_src_dir),\n\t            generator,\n", "        ]\n\t        cmake_cmd.extend(build_config.get_cmake_args())\n\t        # Flag for cross-compiling for Apple Silicon\n\t        # We use environment variable because it's the only way to pass down custom flags\n\t        # through the cibuildwheel package, which calls `pip wheel` command.\n\t        if \"CIBW_TARGET_OSX_ARM64\" in os.environ:\n\t            cmake_cmd.extend(\n\t                [\"-DCMAKE_OSX_ARCHITECTURES=arm64\", \"-DDETECT_CONDA_ENV=OFF\"]\n\t            )\n\t        logger.info(\"CMake args: %s\", str(cmake_cmd))\n", "        subprocess.check_call(cmake_cmd, cwd=build_dir)\n\t        if system() == \"Windows\":\n\t            subprocess.check_call(\n\t                [\"cmake\", \"--build\", \".\", \"--config\", \"Release\"], cwd=build_dir\n\t            )\n\t        else:\n\t            nproc = os.cpu_count()\n\t            assert build_tool is not None\n\t            subprocess.check_call([build_tool, f\"-j{nproc}\"], cwd=build_dir)\n\t    if system() == \"Windows\":\n", "        supported_generators = (\n\t            \"-GVisual Studio 17 2022\",\n\t            \"-GVisual Studio 16 2019\",\n\t            \"-GVisual Studio 15 2017\",\n\t            \"-GMinGW Makefiles\",\n\t        )\n\t        for generator in supported_generators:\n\t            try:\n\t                _build(generator=generator)\n\t                logger.info(\n", "                    \"Successfully built %s using generator %s\", _lib_name(), generator\n\t                )\n\t                break\n\t            except subprocess.CalledProcessError as e:\n\t                logger.info(\n\t                    \"Tried building with generator %s but failed with exception %s\",\n\t                    generator,\n\t                    str(e),\n\t                )\n\t                # Empty build directory\n", "                shutil.rmtree(build_dir)\n\t                build_dir.mkdir()\n\t        else:\n\t            raise RuntimeError(\n\t                \"None of the supported generators produced a successful build!\"\n\t                f\"Supported generators: {supported_generators}\"\n\t            )\n\t    else:\n\t        build_tool = \"ninja\" if shutil.which(\"ninja\") else \"make\"\n\t        generator = \"-GNinja\" if build_tool == \"ninja\" else \"-GUnix Makefiles\"\n", "        try:\n\t            _build(generator=generator)\n\t        except subprocess.CalledProcessError as e:\n\t            logger.info(\"Failed to build with OpenMP. Exception: %s\", str(e))\n\t            build_config.use_openmp = False\n\t            _build(generator=generator)\n\t    return build_dir / _lib_name()\n\tdef locate_local_libtl2cgen(\n\t    toplevel_dir: pathlib.Path,\n\t    logger: logging.Logger,\n", ") -> Optional[pathlib.Path]:\n\t    \"\"\"\n\t    Locate libtl2cgen from the local project directory's lib/ subdirectory.\n\t    \"\"\"\n\t    libtl2cgen = toplevel_dir.parent / \"build\" / _lib_name()\n\t    if libtl2cgen.exists():\n\t        logger.info(\"Found %s at %s\", libtl2cgen.name, str(libtl2cgen.parent))\n\t        return libtl2cgen\n\t    logger.info(\"Did not find %s at %s\", libtl2cgen.name, str(libtl2cgen.parent))\n\t    return None\n", "def locate_or_build_libtl2cgen(\n\t    toplevel_dir: pathlib.Path,\n\t    build_dir: pathlib.Path,\n\t    build_config: BuildConfiguration,\n\t) -> pathlib.Path:\n\t    \"\"\"Locate libtl2cgen; if not exist, build it\"\"\"\n\t    logger = logging.getLogger(\"tl2cgen.packager.locate_or_build_libtl2cgen\")\n\t    if build_config.use_system_libtl2cgen:\n\t        # Find libtl2cgen from system prefix\n\t        sys_prefix = pathlib.Path(sys.prefix)\n", "        sys_prefix_candidates = [\n\t            sys_prefix / \"lib\",\n\t            # Paths possibly used on Windows\n\t            sys_prefix / \"bin\",\n\t            sys_prefix / \"Library\",\n\t            sys_prefix / \"Library\" / \"bin\",\n\t            sys_prefix / \"Library\" / \"lib\",\n\t        ]\n\t        sys_prefix_candidates = [\n\t            p.expanduser().resolve() for p in sys_prefix_candidates\n", "        ]\n\t        for candidate_dir in sys_prefix_candidates:\n\t            libtl2cgen_sys = candidate_dir / _lib_name()\n\t            if libtl2cgen_sys.exists():\n\t                logger.info(\"Using system tl2cgen: %s\", str(libtl2cgen_sys))\n\t                return libtl2cgen_sys\n\t        raise RuntimeError(\n\t            f\"use_system_libtl2cgen was specified but {_lib_name()} is \"\n\t            f\"not found. Paths searched (in order): \\n\"\n\t            + \"\\n\".join([f\"* {str(p)}\" for p in sys_prefix_candidates])\n", "        )\n\t    libtl2cgen = locate_local_libtl2cgen(toplevel_dir, logger=logger)\n\t    if libtl2cgen is not None:\n\t        return libtl2cgen\n\t    if toplevel_dir.joinpath(\"cpp_src\").exists():\n\t        # Source distribution; all C++ source files to be found in cpp_src/\n\t        cpp_src_dir = toplevel_dir.joinpath(\"cpp_src\")\n\t    else:\n\t        # Probably running \"pip install .\" from python-package/\n\t        cpp_src_dir = toplevel_dir.parent\n", "        if not cpp_src_dir.joinpath(\"CMakeLists.txt\").exists():\n\t            raise RuntimeError(f\"Did not find CMakeLists.txt from {cpp_src_dir}\")\n\t    return build_libtl2cgen(cpp_src_dir, build_dir=build_dir, build_config=build_config)\n"]}
{"filename": "python/packager/pep517.py", "chunked_list": ["\"\"\"\n\tCustom build backend for TL2cgen Python package.\n\tBuilds source distribution and binary wheels, following PEP 517 / PEP 660.\n\tReuses components of Hatchling (https://github.com/pypa/hatch/tree/master/backend) for the sake\n\tof brevity.\n\t\"\"\"\n\timport dataclasses\n\timport logging\n\timport os\n\timport pathlib\n", "import tempfile\n\tfrom contextlib import contextmanager\n\tfrom typing import Any, Dict, Iterator, Optional, Union\n\timport hatchling.build\n\tfrom .build_config import BuildConfiguration\n\tfrom .nativelib import locate_local_libtl2cgen, locate_or_build_libtl2cgen\n\tfrom .sdist import copy_cpp_src_tree\n\tfrom .util import copy_with_logging, copytree_with_logging\n\t@contextmanager\n\tdef cd(path: Union[str, pathlib.Path]) -> Iterator[str]:  # pylint: disable=C0103\n", "    \"\"\"\n\t    Temporarily change working directory.\n\t    TODO(hcho3): Remove this once we adopt Python 3.11, which implements contextlib.chdir.\n\t    \"\"\"\n\t    path = str(path)\n\t    path = os.path.realpath(path)\n\t    cwd = os.getcwd()\n\t    os.chdir(path)\n\t    try:\n\t        yield path\n", "    finally:\n\t        os.chdir(cwd)\n\tTOPLEVEL_DIR = pathlib.Path(__file__).parent.parent.absolute().resolve()\n\tlogging.basicConfig(level=logging.INFO)\n\t# Aliases\n\tget_requires_for_build_sdist = hatchling.build.get_requires_for_build_sdist\n\tget_requires_for_build_wheel = hatchling.build.get_requires_for_build_wheel\n\tget_requires_for_build_editable = hatchling.build.get_requires_for_build_editable\n\tdef build_wheel(\n\t    wheel_directory: str,\n", "    config_settings: Optional[Dict[str, Any]] = None,\n\t    metadata_directory: Optional[str] = None,\n\t) -> str:\n\t    \"\"\"Build a wheel\"\"\"\n\t    logger = logging.getLogger(\"tl2cgen.packager.build_wheel\")\n\t    build_config = BuildConfiguration()\n\t    build_config.update(config_settings)\n\t    logger.info(\"Parsed build configuration: %s\", dataclasses.asdict(build_config))\n\t    # Create tempdir with Python package + libtl2cgen\n\t    with tempfile.TemporaryDirectory() as td:\n", "        td_path = pathlib.Path(td)\n\t        build_dir = td_path / \"libbuild\"\n\t        build_dir.mkdir()\n\t        workspace = td_path / \"whl_workspace\"\n\t        workspace.mkdir()\n\t        logger.info(\"Copying project files to temporary directory %s\", str(workspace))\n\t        copy_with_logging(TOPLEVEL_DIR / \"pyproject.toml\", workspace, logger=logger)\n\t        copy_with_logging(TOPLEVEL_DIR / \"hatch_build.py\", workspace, logger=logger)\n\t        copy_with_logging(TOPLEVEL_DIR / \"README.rst\", workspace, logger=logger)\n\t        pkg_path = workspace / \"tl2cgen\"\n", "        copytree_with_logging(TOPLEVEL_DIR / \"tl2cgen\", pkg_path, logger=logger)\n\t        lib_path = pkg_path / \"lib\"\n\t        lib_path.mkdir()\n\t        libtl2cgen = locate_or_build_libtl2cgen(\n\t            TOPLEVEL_DIR, build_dir=build_dir, build_config=build_config\n\t        )\n\t        if not build_config.use_system_libtl2cgen:\n\t            copy_with_logging(libtl2cgen, lib_path, logger=logger)\n\t        with cd(workspace):\n\t            wheel_name = hatchling.build.build_wheel(\n", "                wheel_directory, config_settings, metadata_directory\n\t            )\n\t    return wheel_name\n\tdef build_sdist(\n\t    sdist_directory: str,\n\t    config_settings: Optional[Dict[str, Any]] = None,\n\t) -> str:\n\t    \"\"\"Build a source distribution\"\"\"\n\t    logger = logging.getLogger(\"tl2cgen.packager.build_sdist\")\n\t    if config_settings:\n", "        raise NotImplementedError(\n\t            \"TL2cgen's custom build backend doesn't support config_settings option \"\n\t            f\"when building sdist. {config_settings=}\"\n\t        )\n\t    cpp_src_dir = TOPLEVEL_DIR.parent\n\t    if not cpp_src_dir.joinpath(\"CMakeLists.txt\").exists():\n\t        raise RuntimeError(f\"Did not find CMakeLists.txt from {cpp_src_dir}\")\n\t    # Create tempdir with Python package + C++ sources\n\t    with tempfile.TemporaryDirectory() as td:\n\t        td_path = pathlib.Path(td)\n", "        workspace = td_path / \"sdist_workspace\"\n\t        workspace.mkdir()\n\t        logger.info(\"Copying project files to temporary directory %s\", str(workspace))\n\t        copy_with_logging(TOPLEVEL_DIR / \"pyproject.toml\", workspace, logger=logger)\n\t        copy_with_logging(TOPLEVEL_DIR / \"hatch_build.py\", workspace, logger=logger)\n\t        copy_with_logging(TOPLEVEL_DIR / \"README.rst\", workspace, logger=logger)\n\t        copytree_with_logging(\n\t            TOPLEVEL_DIR / \"tl2cgen\", workspace / \"tl2cgen\", logger=logger\n\t        )\n\t        copytree_with_logging(\n", "            TOPLEVEL_DIR / \"packager\", workspace / \"packager\", logger=logger\n\t        )\n\t        temp_cpp_src_dir = workspace / \"cpp_src\"\n\t        copy_cpp_src_tree(cpp_src_dir, target_dir=temp_cpp_src_dir, logger=logger)\n\t        with cd(workspace):\n\t            sdist_name = hatchling.build.build_sdist(sdist_directory, config_settings)\n\t    return sdist_name\n\tdef build_editable(\n\t    wheel_directory: str,\n\t    config_settings: Optional[Dict[str, Any]] = None,\n", "    metadata_directory: Optional[str] = None,\n\t) -> str:\n\t    \"\"\"Build an editable installation. We mostly delegate to Hatchling.\"\"\"\n\t    logger = logging.getLogger(\"tl2cgen.packager.build_editable\")\n\t    if config_settings:\n\t        raise NotImplementedError(\n\t            \"TL2cgen's custom build backend doesn't support config_settings option \"\n\t            f\"when building editable installation. {config_settings=}\"\n\t        )\n\t    if locate_local_libtl2cgen(TOPLEVEL_DIR, logger=logger) is None:\n", "        raise RuntimeError(\n\t            \"To use the editable installation, first build libtl2cgen with CMake. \"\n\t            \"See https://tl2cgen.readthedocs.io/en/latest/build.html for detailed instructions.\"\n\t        )\n\t    return hatchling.build.build_editable(\n\t        wheel_directory, config_settings, metadata_directory\n\t    )\n"]}
{"filename": "python/packager/build_config.py", "chunked_list": ["\"\"\"Build configuration\"\"\"\n\timport dataclasses\n\tfrom typing import Any, Dict, List, Optional\n\t@dataclasses.dataclass\n\tclass BuildConfiguration:  # pylint: disable=R0902\n\t    \"\"\"Configurations use when building libtl2cgen\"\"\"\n\t    # Whether to enable OpenMP\n\t    use_openmp: bool = True\n\t    # Whether to hide C++ symbols\n\t    hide_cxx_symbols: bool = True\n", "    # Whether to use the TL2cgen library that's installed in the system prefix\n\t    use_system_libtl2cgen: bool = False\n\t    def _set_config_setting(self, config_settings: Dict[str, Any]) -> None:\n\t        for field_name in config_settings:\n\t            setattr(\n\t                self,\n\t                field_name,\n\t                (config_settings[field_name].lower() in [\"true\", \"1\", \"on\"]),\n\t            )\n\t    def update(self, config_settings: Optional[Dict[str, Any]]) -> None:\n", "        \"\"\"Parse config_settings from Pip (or other PEP 517 frontend)\"\"\"\n\t        if config_settings is not None:\n\t            self._set_config_setting(config_settings)\n\t    def get_cmake_args(self) -> List[str]:\n\t        \"\"\"Convert build configuration to CMake args\"\"\"\n\t        cmake_args = []\n\t        for field_name in [x.name for x in dataclasses.fields(self)]:\n\t            if field_name in [\"use_system_libtl2cgen\"]:\n\t                continue\n\t            cmake_option = field_name.upper()\n", "            cmake_value = \"ON\" if getattr(self, field_name) is True else \"OFF\"\n\t            cmake_args.append(f\"-D{cmake_option}={cmake_value}\")\n\t        return cmake_args\n"]}
{"filename": "python/packager/__init__.py", "chunked_list": []}
{"filename": "python/packager/util.py", "chunked_list": ["\"\"\"\n\tUtility functions for implementing PEP 517 backend\n\t\"\"\"\n\timport logging\n\timport pathlib\n\timport shutil\n\tdef copytree_with_logging(\n\t    src: pathlib.Path, dest: pathlib.Path, logger: logging.Logger\n\t) -> None:\n\t    \"\"\"Call shutil.copytree() with logging\"\"\"\n", "    logger.info(\"Copying %s -> %s\", str(src), str(dest))\n\t    shutil.copytree(src, dest)\n\tdef copy_with_logging(\n\t    src: pathlib.Path, dest: pathlib.Path, logger: logging.Logger\n\t) -> None:\n\t    \"\"\"Call shutil.copy() with logging\"\"\"\n\t    if dest.is_dir():\n\t        logger.info(\"Copying %s -> %s\", str(src), str(dest / src.name))\n\t    else:\n\t        logger.info(\"Copying %s -> %s\", str(src), str(dest))\n", "    shutil.copy(src, dest)\n"]}
{"filename": "python/packager/sdist.py", "chunked_list": ["\"\"\"\n\tFunctions for building sdist\n\t\"\"\"\n\timport logging\n\timport pathlib\n\tfrom .util import copy_with_logging, copytree_with_logging\n\tdef copy_cpp_src_tree(\n\t    cpp_src_dir: pathlib.Path, target_dir: pathlib.Path, logger: logging.Logger\n\t) -> None:\n\t    \"\"\"Copy C++ source tree into build directory\"\"\"\n", "    for subdir in [\n\t        \"src\",\n\t        \"include\",\n\t        \"cmake\",\n\t    ]:\n\t        copytree_with_logging(cpp_src_dir / subdir, target_dir / subdir, logger=logger)\n\t    for filename in [\"CMakeLists.txt\", \"LICENSE\"]:\n\t        copy_with_logging(cpp_src_dir.joinpath(filename), target_dir, logger=logger)\n"]}
{"filename": "dev/prepare_pypi_release.py", "chunked_list": ["\"\"\"Simple script for preparing a PyPI release.\n\tIt fetches Python wheels from the CI pipelines.\n\ttqdm, packaging are required to run this script.\n\t\"\"\"\n\timport argparse\n\timport pathlib\n\timport subprocess\n\tfrom typing import List, Optional\n\tfrom urllib.request import urlretrieve\n\timport tqdm\n", "from packaging import version\n\tPREFIX = \"https://tl2cgen-wheels.s3.amazonaws.com/\"\n\tPROJECT_ROOT = pathlib.Path(__file__).expanduser().resolve().parent.parent\n\tDIST_DIR = PROJECT_ROOT / \"python\" / \"dist\"\n\tpbar = None  # pylint: disable=invalid-name\n\tdef show_progress(block_num, block_size, total_size):\n\t    \"\"\"Show file download progress.\"\"\"\n\t    global pbar  # pylint: disable=global-statement\n\t    if pbar is None:\n\t        pbar = tqdm.tqdm(total=total_size / 1024, unit=\"kB\")\n", "    downloaded = block_num * block_size\n\t    if downloaded < total_size:\n\t        upper = (total_size - downloaded) / 1024\n\t        pbar.update(min(block_size / 1024, upper))\n\t    else:\n\t        pbar.close()\n\t        pbar = None\n\tdef retrieve(url, filename=None):\n\t    \"\"\"Download a file from URL\"\"\"\n\t    print(f\"{url} -> {filename}\")\n", "    return urlretrieve(url, filename, reporthook=show_progress)\n\tdef latest_hash() -> str:\n\t    \"\"\"Get latest commit hash.\"\"\"\n\t    ret = subprocess.run([\"git\", \"rev-parse\", \"HEAD\"], capture_output=True, check=True)\n\t    assert ret.returncode == 0, \"Failed to get latest commit hash.\"\n\t    commit_hash = ret.stdout.decode(\"utf-8\").strip()\n\t    return commit_hash\n\tdef download_wheels(\n\t    platforms: List[str],\n\t    url_prefix: str,\n", "    dest_dir: pathlib.Path,\n\t    src_filename_prefix: str,\n\t    target_filename_prefix: str,\n\t    ext: str = \"whl\",\n\t) -> List[str]:\n\t    \"\"\"Download all binary wheels. url_prefix is the URL for remote directory storing\n\t    the release wheels\n\t    \"\"\"\n\t    # pylint: disable=too-many-arguments\n\t    filenames = []\n", "    for platform in platforms:\n\t        src_wheel = src_filename_prefix + platform + \".\" + ext\n\t        url = url_prefix + src_wheel\n\t        target_wheel = target_filename_prefix + platform + \".\" + ext\n\t        print(f\"{src_wheel} -> {target_wheel}\")\n\t        filename = dest_dir / target_wheel\n\t        filenames.append(str(filename))\n\t        retrieve(url=url, filename=filename)\n\t    return filenames\n\tdef download_py_packages(version_str: str, commit_hash: str) -> None:\n", "    \"\"\"Download Python packages\"\"\"\n\t    platforms = [\n\t        \"win_amd64\",\n\t        \"manylinux2014_x86_64\",\n\t        \"macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64\",\n\t        \"macosx_12_0_arm64\",\n\t    ]\n\t    if not DIST_DIR.exists():\n\t        DIST_DIR.mkdir()\n\t    # Binary wheels (*.whl)\n", "    src_filename_prefix = f\"tl2cgen-{version_str}%2B{commit_hash}-py3-none-\"\n\t    target_filename_prefix = f\"tl2cgen-{version_str}-py3-none-\"\n\t    filenames = download_wheels(\n\t        platforms, PREFIX, DIST_DIR, src_filename_prefix, target_filename_prefix\n\t    )\n\t    print(f\"List of downloaded wheels: {filenames}\\n\")\n\t    # Source distribution (*.tar.gz)\n\t    src_filename_prefix = f\"tl2cgen-{version_str}%2B{commit_hash}\"\n\t    target_filename_prefix = f\"tl2cgen-{version_str}\"\n\t    filenames = download_wheels(\n", "        [\"\"],\n\t        PREFIX,\n\t        DIST_DIR,\n\t        src_filename_prefix,\n\t        target_filename_prefix,\n\t        \"tar.gz\",\n\t    )\n\t    print(f\"List of downloaded sdist: {filenames}\\n\")\n\t    print(\n\t        \"\"\"\n", "Following steps should be done manually:\n\t- Upload pypi package by `python -m twine upload python/dist/*` for all wheels.\n\t- Check the uploaded files on `https://pypi.org/project/tl2cgen/<VERSION>/#files` and `pip\n\t  install tl2cgen==<VERSION>` \"\"\"\n\t    )\n\tdef main(args: argparse.Namespace) -> None:\n\t    \"\"\"Entry function\"\"\"\n\t    rel = version.parse(args.release)\n\t    assert isinstance(rel, version.Version)\n\t    if not rel.is_prerelease:\n", "        # Major release\n\t        rc: Optional[str] = None\n\t    else:\n\t        # RC release\n\t        assert rel.pre is not None\n\t        rc, _ = rel.pre\n\t        assert rc == \"rc\"\n\t    commit_hash = latest_hash()\n\t    download_py_packages(args.release, commit_hash)\n\tif __name__ == \"__main__\":\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--release\",\n\t        type=str,\n\t        required=True,\n\t        help=\"Version tag, e.g. '1.3.2', or '1.5.0rc1'\",\n\t    )\n\t    parsed_args = parser.parse_args()\n\t    main(parsed_args)\n"]}
{"filename": "dev/run_pylint.py", "chunked_list": ["\"\"\"Wrapper for Pylint\"\"\"\n\timport os\n\timport pathlib\n\timport subprocess\n\timport sys\n\tROOT_PATH = pathlib.Path(__file__).parent.parent.expanduser().resolve()\n\tPYPKG_PATH = ROOT_PATH / \"python\"\n\tPYLINTRC_PATH = PYPKG_PATH / \".pylintrc\"\n\tdef main():\n\t    \"\"\"Wrapper for Pylint. Add tl2cgen to PYTHONPATH so that pylint doesn't error out\"\"\"\n", "    new_env = os.environ.copy()\n\t    new_env[\"PYTHONPATH\"] = str(PYPKG_PATH)\n\t    # sys.argv[1:]: List of source files to check\n\t    subprocess.run(\n\t        [\"pylint\", \"-rn\", \"-sn\", \"--rcfile\", str(PYLINTRC_PATH)] + sys.argv[1:],\n\t        check=True,\n\t        env=new_env,\n\t    )\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "dev/change_version.py", "chunked_list": ["\"\"\"\n\tThis script changes the version field in different parts of the code base.\n\t\"\"\"\n\timport argparse\n\timport pathlib\n\timport re\n\tfrom typing import Optional, TypeVar\n\tR = TypeVar(\"R\")\n\tROOT = pathlib.Path(__file__).parent.parent.expanduser().resolve()\n\tPY_PACKAGE = ROOT / \"python\"\n", "JAVA_PACKAGE = ROOT / \"java_runtime\"\n\tdef update_cmake(major: int, minor: int, patch: int) -> None:\n\t    \"\"\"Change version in CMakeLists.txt\"\"\"\n\t    version = f\"{major}.{minor}.{patch}\"\n\t    with open(ROOT / \"CMakeLists.txt\", \"r\", encoding=\"utf-8\") as fd:\n\t        cmakelist = fd.read()\n\t    pattern = r\"project\\(tl2cgen LANGUAGES .* VERSION ([0-9]+\\.[0-9]+\\.[0-9]+)\\)\"\n\t    matched = re.search(pattern, cmakelist)\n\t    assert matched, \"Couldn't find the version string in CMakeLists.txt.\"\n\t    cmakelist = cmakelist[: matched.start(1)] + version + cmakelist[matched.end(1) :]\n", "    with open(ROOT / \"CMakeLists.txt\", \"w\", encoding=\"utf-8\") as fd:\n\t        fd.write(cmakelist)\n\tdef update_pypkg(\n\t    major: int,\n\t    minor: int,\n\t    patch: int,\n\t    *,\n\t    is_rc: bool,\n\t    is_dev: bool,\n\t    rc_ver: Optional[int] = None,\n", ") -> None:\n\t    \"\"\"Change version in the Python package\"\"\"\n\t    version = f\"{major}.{minor}.{patch}\"\n\t    if is_rc:\n\t        assert rc_ver\n\t        version = version + f\"rc{rc_ver}\"\n\t    if is_dev:\n\t        version = version + \"-dev\"\n\t    pyver_path = PY_PACKAGE / \"tl2cgen\" / \"VERSION\"\n\t    with open(pyver_path, \"w\", encoding=\"utf-8\") as fd:\n", "        fd.write(version + \"\\n\")\n\t    pyprj_path = PY_PACKAGE / \"pyproject.toml\"\n\t    with open(pyprj_path, \"r\", encoding=\"utf-8\") as fd:\n\t        pyprj = fd.read()\n\t    matched = re.search('version = \"' + r\"([0-9]+\\.[0-9]+\\.[0-9]+.*)\" + '\"', pyprj)\n\t    assert matched, \"Couldn't find version string in pyproject.toml.\"\n\t    pyprj = pyprj[: matched.start(1)] + version + pyprj[matched.end(1) :]\n\t    with open(pyprj_path, \"w\", encoding=\"utf-8\") as fd:\n\t        fd.write(pyprj)\n\tdef update_java_pkg(\n", "    major: int,\n\t    minor: int,\n\t    patch: int,\n\t    *,\n\t    is_rc: bool,\n\t    is_dev: bool,\n\t    rc_ver: Optional[int] = None,\n\t) -> None:\n\t    \"\"\"Change version in the Java package\"\"\"\n\t    version = f\"{major}.{minor}.{patch}\"\n", "    if is_rc:\n\t        assert rc_ver\n\t        version = version + f\"-RC{rc_ver}\"\n\t    if is_dev:\n\t        version = version + \"-SNAPSHOT\"\n\t    pom_path = JAVA_PACKAGE / \"tl2cgen4j\" / \"pom.xml\"\n\t    with open(pom_path, \"r\", encoding=\"utf-8\") as fd:\n\t        pom = fd.read()\n\t    matched = re.search(\"<version>\" + r\"([0-9]+\\.[0-9]+\\.[0-9]+.*)\" + \"</version>\", pom)\n\t    assert matched, \"Couldn't find version string in pom.xml.\"\n", "    pom = pom[: matched.start(1)] + version + pom[matched.end(1) :]\n\t    with open(pom_path, \"w\", encoding=\"utf-8\") as fd:\n\t        fd.write(pom)\n\tdef main(args: argparse.Namespace) -> None:\n\t    \"\"\"Perform version change in all relevant parts of the code base.\"\"\"\n\t    if args.is_rc and args.is_dev:\n\t        raise ValueError(\"A release version cannot be both RC and dev.\")\n\t    if args.is_rc:\n\t        assert args.rc is not None, \"rc field must be specified if is_rc is specified\"\n\t        assert args.rc >= 1, \"RC version must start from 1.\"\n", "    else:\n\t        assert args.rc is None, \"is_rc must be specified in order to specify rc field\"\n\t    update_cmake(args.major, args.minor, args.patch)\n\t    update_pypkg(\n\t        args.major,\n\t        args.minor,\n\t        args.patch,\n\t        is_rc=args.is_rc,\n\t        is_dev=args.is_dev,\n\t        rc_ver=args.rc,\n", "    )\n\t    update_java_pkg(\n\t        args.major,\n\t        args.minor,\n\t        args.patch,\n\t        is_rc=args.is_rc,\n\t        is_dev=args.is_dev,\n\t        rc_ver=args.rc,\n\t    )\n\tif __name__ == \"__main__\":\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--major\", type=int, required=True)\n\t    parser.add_argument(\"--minor\", type=int, required=True)\n\t    parser.add_argument(\"--patch\", type=int, required=True)\n\t    parser.add_argument(\"--rc\", type=int)\n\t    parser.add_argument(\"--is-rc\", type=int, choices=[0, 1], default=0)\n\t    parser.add_argument(\"--is-dev\", type=int, choices=[0, 1], default=0)\n\t    parsed_args = parser.parse_args()\n\t    main(parsed_args)\n"]}
{"filename": "docs/conf.py", "chunked_list": ["# pylint: skip-file\n\t# documentation build configuration file\n\timport os\n\timport pathlib\n\timport re\n\timport shutil\n\timport subprocess\n\timport sys\n\timport warnings\n\tfrom sh.contrib import git\n", "PROJECT_ROOT = pathlib.Path(__file__).expanduser().resolve().parent.parent\n\tCURR_PATH = PROJECT_ROOT / \"docs\"\n\tDOX_DIR = PROJECT_ROOT / \"doxygen\"\n\tdef run_doxygen():\n\t    \"\"\"Run the doxygen make command in the designated folder.\"\"\"\n\t    tmpdir = CURR_PATH / \"tmp\"\n\t    if tmpdir.exists():\n\t        shutil.rmtree(tmpdir)\n\t    else:\n\t        tmpdir.mkdir()\n", "    try:\n\t        if not DOX_DIR.exists():\n\t            DOX_DIR.mkdir()\n\t        os.chdir(os.path.join(PROJECT_ROOT, DOX_DIR))\n\t        subprocess.run(\n\t            [\"cmake\", \"..\", \"-DBUILD_DOXYGEN=ON\", \"-GNinja\"], check=True, cwd=DOX_DIR\n\t        )\n\t        subprocess.run([\"ninja\", \"tl2cgen_doc_doxygen\"], check=True, cwd=DOX_DIR)\n\t        shutil.copytree(DOX_DIR / \"doc_doxygen\" / \"html\", tmpdir / \"dev\")\n\t    except OSError as e:\n", "        raise RuntimeError(f\"Doxygen execution failed {str(e)}\") from e\n\tdef is_readthedocs_build():\n\t    if os.environ.get(\"READTHEDOCS\", None) == \"True\":\n\t        return True\n\t    warnings.warn(\n\t        \"Skipping Doxygen build... You won't have documentation for C/C++ functions. \"\n\t        \"Set environment variable READTHEDOCS=True if you want to build Doxygen. \"\n\t        \"(If you do opt in, make sure to install Doxygen, Graphviz, CMake, and C++ compiler \"\n\t        \"on your system.)\"\n\t    )\n", "    return False\n\tif is_readthedocs_build():\n\t    run_doxygen()\n\tgit_branch_env = os.getenv(\"SPHINX_GIT_BRANCH\", default=None)\n\tif not git_branch_env:\n\t    # If SPHINX_GIT_BRANCH environment variable is not given, run git\n\t    # to determine branch name\n\t    git_branch = [\n\t        re.sub(r\"origin/\", \"\", x.lstrip(\" \"))\n\t        for x in str(git.branch(\"-r\", \"--contains\", \"HEAD\")).rstrip(\"\\n\").split(\"\\n\")\n", "    ]\n\t    git_branch = [x for x in git_branch if \"HEAD\" not in x]\n\telse:\n\t    git_branch = [git_branch_env]\n\tprint(f\"git_branch = {git_branch[0]}\")\n\t# If extensions (or modules to document with autodoc) are in another directory,\n\t# add these directories to sys.path here. If the directory is relative to the\n\t# documentation root, use os.path.abspath to make it absolute, like shown here.\n\tlibpath = PROJECT_ROOT / \"python\"\n\tsys.path.insert(0, str(libpath))\n", "sys.path.insert(0, str(CURR_PATH))\n\t# -- General configuration ------------------------------------------------\n\t# General information about the project.\n\tproject = \"TL2cgen\"\n\tauthor = f\"{project} developers\"\n\tcopyright = f\"2023, {author}\"\n\tgithub_doc_root = \"https://github.com/dmlc/tl2cgen/tree/mainline/docs/\"\n\tos.environ[\"TL2CGEN_BUILD_DOC\"] = \"1\"\n\t# Version information.\n\twith open(PROJECT_ROOT / \"python\" / \"tl2cgen\" / \"VERSION\", \"r\", encoding=\"utf-8\") as f:\n", "    version = f.read().rstrip()\n\trelease = version\n\t# Add any Sphinx extension module names here, as strings. They can be\n\t# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones\n\textensions = [\n\t    \"matplotlib.sphinxext.plot_directive\",\n\t    \"sphinxcontrib.jquery\",\n\t    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx.ext.mathjax\",\n", "    \"sphinx.ext.intersphinx\",\n\t    \"sphinx_gallery.gen_gallery\",\n\t    \"breathe\",\n\t    \"autodocsumm\",\n\t]\n\tsphinx_gallery_conf = {\n\t    # path to your example scripts\n\t    \"examples_dirs\": [],\n\t    # path to where to save gallery generated output\n\t    \"gallery_dirs\": [],\n", "    \"matplotlib_animations\": True,\n\t}\n\tautodoc_typehints = \"description\"\n\tautodoc_default_options = {\n\t    \"autosummary\": True,\n\t}\n\tgraphviz_output_format = \"png\"\n\tplot_formats = [(\"svg\", 300), (\"png\", 100), (\"hires.png\", 300)]\n\tplot_html_show_source_link = False\n\tplot_html_show_formats = False\n", "# Breathe extension variables\n\tbreathe_projects = {}\n\tif is_readthedocs_build():\n\t    breathe_projects = {\n\t        \"tl2cgen\": os.path.join(PROJECT_ROOT, DOX_DIR, \"doc_doxygen/xml\")\n\t    }\n\tbreathe_default_project = \"tl2cgen\"\n\t# Add any paths that contain templates here, relative to this directory.\n\ttemplates_path = [\"_templates\"]\n\t# The suffix(es) of source filenames.\n", "# You can specify multiple suffix as a list of string:\n\tsource_suffix = [\".rst\", \".md\"]\n\t# The encoding of source files.\n\t# source_encoding = 'utf-8-sig'\n\t# The master toctree document.\n\tmaster_doc = \"index\"\n\t# The language for content autogenerated by Sphinx. Refer to documentation\n\t# for a list of supported languages.\n\t#\n\t# This is also used if you do content translation via gettext catalogs.\n", "# Usually you set \"language\" from the command line for these cases.\n\tlanguage = \"en\"\n\tautoclass_content = \"both\"\n\t# There are two options for replacing |today|: either, you set today to some\n\t# non-false value, then it is used:\n\t# today = ''\n\t# Else, today_fmt is used as the format for a strftime call.\n\t# today_fmt = '%B %d, %Y'\n\t# List of patterns, relative to source directory, that match files and\n\t# directories to ignore when looking for source files.\n", "exclude_patterns = [\"_build\"]\n\thtml_extra_path = []\n\tif is_readthedocs_build():\n\t    html_extra_path = [os.path.join(CURR_PATH, \"tmp\")]\n\t# The reST default role (used for this markup: `text`) to use for all\n\t# documents.\n\t# default_role = None\n\t# If true, '()' will be appended to :func: etc. cross-reference text.\n\t# add_function_parentheses = True\n\t# If true, the current module name will be prepended to all description\n", "# unit titles (such as .. function::).\n\t# add_module_names = True\n\t# If true, sectionauthor and moduleauthor directives will be shown in the\n\t# output. They are ignored by default.\n\t# show_authors = False\n\t# The name of the Pygments (syntax highlighting) style to use.\n\tpygments_style = \"sphinx\"\n\t# A list of ignored prefixes for module index sorting.\n\t# modindex_common_prefix = []\n\t# If true, keep warnings as \"system message\" paragraphs in the built documents.\n", "# keep_warnings = False\n\t# If true, `todo` and `todoList` produce output, else they produce nothing.\n\ttodo_include_todos = False\n\t# -- Options for HTML output ----------------------------------------------\n\t# The theme to use for HTML and HTML Help pages.  See the documentation for\n\t# a list of builtin themes.\n\thtml_theme = \"sphinx_rtd_theme\"\n\thtml_theme_options = {\"logo_only\": False}\n\thtml_css_files = [\"css/custom.css\"]\n\thtml_sidebars = {\"**\": [\"logo-text.html\", \"globaltoc.html\", \"searchbox.html\"]}\n", "# Add any paths that contain custom static files (such as style sheets) here,\n\t# relative to this directory. They are copied after the builtin static files,\n\t# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n\thtml_static_path = [\"_static\"]\n\t# Output file base name for HTML help builder.\n\thtmlhelp_basename = project + \"doc\"\n\t# -- Options for LaTeX output ---------------------------------------------\n\tlatex_elements = {}  # type: ignore\n\t# Grouping the document tree into LaTeX files. List of tuples\n\t# (source start file, target name, title,\n", "#  author, documentclass [howto, manual, or own class]).\n\tlatex_documents = [\n\t    (master_doc, f\"{project}.tex\", project, author, \"manual\"),\n\t]\n\tintersphinx_mapping = {\n\t    \"python\": (\"https://docs.python.org/3.8\", None),\n\t    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n\t    \"scipy\": (\"https://docs.scipy.org/doc/scipy/\", None),\n\t    \"pandas\": (\"https://pandas.pydata.org/pandas-docs/stable/\", None),\n\t    \"sklearn\": (\"https://scikit-learn.org/stable\", None),\n", "    \"treelite\": (\"https://treelite.readthedocs.io/en/latest/\", None),\n\t}\n\tdef setup(app):\n\t    app.add_css_file(\"custom.css\")\n"]}
