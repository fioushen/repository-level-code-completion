{"filename": "run.py", "chunked_list": ["import argparse\n\timport numpy as np\n\timport os\n\timport sys\n\timport yaml\n\timport envs.env_builder as env_builder\n\timport learning.agent_builder as agent_builder\n\timport util.util as util\n\tdef set_np_formatting():\n\t    np.set_printoptions(edgeitems=30, infstr='inf',\n", "                        linewidth=4000, nanstr='nan', precision=2,\n\t                        suppress=False, threshold=10000, formatter=None)\n\t    return\n\tdef load_args(argv):\n\t    parser = argparse.ArgumentParser(description=\"Train or test control policies.\")\n\t    parser.add_argument(\"--rand_seed\", dest=\"rand_seed\", type=int, default=None)\n\t    parser.add_argument(\"--mode\", dest=\"mode\", type=str, default=\"train\")\n\t    parser.add_argument(\"--visualize\", dest=\"visualize\", action=\"store_true\", default=False)\n\t    parser.add_argument(\"--env_config\", dest=\"env_config\")\n\t    parser.add_argument(\"--agent_config\", dest=\"agent_config\")\n", "    parser.add_argument(\"--device\", dest=\"device\", type=str, default=\"cpu\")\n\t    parser.add_argument(\"--log_file\", dest=\"log_file\", type=str, default=\"output/log.txt\")\n\t    parser.add_argument(\"--out_model_file\", dest=\"out_model_file\", type=str, default=\"output/model.pt\")\n\t    parser.add_argument(\"--int_output_dir\", dest=\"int_output_dir\", type=str, default=\"\")\n\t    parser.add_argument(\"--model_file\", dest=\"model_file\", type=str, default=\"\")\n\t    parser.add_argument(\"--max_samples\", dest=\"max_samples\", type=np.int64, default=np.iinfo(np.int64).max)\n\t    parser.add_argument(\"--test_episodes\", dest=\"test_episodes\", type=np.int64, default=np.iinfo(np.int64).max)\n\t    args = parser.parse_args()\n\t    if (args.rand_seed is not None):\n\t        util.set_rand_seed(args.rand_seed)\n", "    return args\n\tdef build_env(args, device, visualize):\n\t    env_file = args.env_config\n\t    env = env_builder.build_env(env_file, device, visualize)\n\t    return env\n\tdef build_agent(args, env, device):\n\t    agent_file = args.agent_config\n\t    agent = agent_builder.build_agent(agent_file, env, device)\n\t    return agent\n\tdef train(agent, max_samples, out_model_file, int_output_dir, log_file):\n", "    agent.train_model(max_samples=max_samples, out_model_file=out_model_file, \n\t                      int_output_dir=int_output_dir, log_file=log_file)\n\t    return\n\tdef test(agent, test_episodes):\n\t    result = agent.test_model(num_episodes=test_episodes)\n\t    print(\"Mean Return: {}\".format(result[\"mean_return\"]))\n\t    print(\"Mean Episode Length: {}\".format(result[\"mean_ep_len\"]))\n\t    print(\"Episodes: {}\".format(result[\"episodes\"]))\n\t    return result\n\tdef create_output_dirs(out_model_file, int_output_dir):\n", "    output_dir = os.path.dirname(out_model_file)\n\t    if (output_dir != \"\" and (not os.path.exists(output_dir))):\n\t        os.makedirs(output_dir, exist_ok=True)\n\t    if (int_output_dir != \"\" and (not os.path.exists(int_output_dir))):\n\t        os.makedirs(int_output_dir, exist_ok=True)\n\t    return\n\tdef main(argv):\n\t    set_np_formatting()\n\t    args = load_args(argv)\n\t    mode = args.mode\n", "    device = args.device\n\t    visualize = args.visualize\n\t    log_file = args.log_file\n\t    out_model_file = args.out_model_file\n\t    int_output_dir = args.int_output_dir\n\t    model_file = args.model_file\n\t    create_output_dirs(out_model_file, int_output_dir)\n\t    env = build_env(args, device, visualize)\n\t    agent = build_agent(args, env, device)\n\t    if (model_file != \"\"):\n", "        agent.load(model_file)\n\t    if (mode == \"train\"):\n\t        max_samples = args.max_samples\n\t        train(agent=agent, max_samples=max_samples, out_model_file=out_model_file, \n\t              int_output_dir=int_output_dir, log_file=log_file)\n\t    elif (mode == \"test\"):\n\t        test_episodes = args.test_episodes\n\t        test(agent=agent, test_episodes=test_episodes)\n\t    else:\n\t        assert(False), \"Unsupported mode: {}\".format(mode)\n", "    return\n\tif __name__ == \"__main__\":\n\t    main(sys.argv)\n"]}
{"filename": "tools/util/plot_util.py", "chunked_list": ["import numpy as np\n\timport matplotlib.pyplot as plt\n\tfrom functools import reduce\n\tdef plot_line(x_data, y_data, std_data=[], label='', line_style=None, color=None, draw_band=True):\n\t    x_data = x_data if isinstance(x_data, list) else [x_data] \n\t    y_data = y_data if isinstance(y_data, list) else [y_data] \n\t    std_data = std_data if isinstance(std_data, list) else [std_data] \n\t    min_x = np.inf\n\t    max_x = -np.inf\n\t    min_y = np.inf\n", "    max_y = -np.inf\n\t    min_len = int(reduce(lambda x, y: np.minimum(x, len(y)), x_data, np.inf))\n\t    if draw_band:\n\t        x_data = list(map(lambda x: x[:min_len], x_data))\n\t        y_data = list(map(lambda x: x[:min_len], y_data))\n\t        std_data = list(map(lambda x: x[:min_len], std_data))\n\t        xs = np.mean(x_data, axis=0)\n\t        ys = np.mean(y_data, axis=0)\n\t        min_x = np.min(xs)\n\t        max_x = np.max(xs)\n", "        min_y = np.min(ys)\n\t        max_y = np.max(ys)\n\t        stds = None\n\t        if (len(y_data) > 1):\n\t            stds = np.std(y_data, axis=0)\n\t        elif (len(std_data) > 0):\n\t            stds = np.mean(std_data, axis=0)\n\t        curr_line = plt.plot(xs, ys, label=label, linestyle=line_style, color=color)\n\t        if stds is not None:\n\t            plt.fill_between(xs, ys - stds, ys + stds, alpha=0.25, linewidth=0,\n", "                             facecolor=curr_line[0].get_color(), label='_nolegend_')\n\t    else:\n\t        for i in range(len(x_data)):\n\t            alpha = 0.8 if (len(x_data) > 1) else 1.0\n\t            xs = x_data[i]\n\t            ys = y_data[i]\n\t            min_x = np.minimum(np.min(xs), min_x)\n\t            max_x = np.maximum(np.max(xs), max_x)\n\t            min_y = np.minimum(np.min(ys), min_y)\n\t            max_y = np.maximum(np.max(ys), max_y)\n", "            if (i == 0):\n\t                curr_line = plt.plot(xs, ys, label=label, alpha=alpha, linestyle=line_style, color=color)\n\t            else:\n\t                curr_line = plt.plot(xs, ys, color=curr_line[0].get_color(), linestyle=line_style, label='_nolegend_', alpha=alpha)\n\t            if len(std_data) > 0:\n\t                stds = std_data[i]\n\t                plt.fill_between(xs, ys - stds, ys + stds, alpha=0.25, linewidth=0,\n\t                                 facecolor=curr_line[0].get_color(), label='_nolegend_')\n\t    return min_x, max_x, min_y, max_y"]}
{"filename": "tools/plot_log/plot_log.py", "chunked_list": ["import numpy as np\n\timport os\n\timport matplotlib.pyplot as plt\n\tfrom functools import reduce\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport tools.util.plot_util as plot_util\n\tfiles = [\n\t    \"output/log.txt\",\n\t]\n", "draw_band = True\n\tx_key = \"Samples\"\n\ty_key = \"Test_Return\"\n\tplot_title = \"BC - Cheetah\"\n\tstd_key = None\n\tdef filter_data(x, window_size):\n\t    n = len(x)\n\t    filter_n = n // window_size\n\t    x = x[:filter_n * window_size]\n\t    x = np.reshape(x, [filter_n, window_size])\n", "    filter_x = np.mean(x, axis=-1)\n\t    return filter_x\n\tplt.figure(figsize=(5.5 * 0.8, 4 * 0.8))\n\tmin_x = np.inf\n\tmax_x = -np.inf\n\tfilter_window_size = 1\n\tfor f, file_group in enumerate(files):\n\t    if not isinstance(file_group, list):\n\t        if os.path.isdir(file_group):\n\t            files = os.listdir(file_group)\n", "            files = filter(lambda f: \"log\" in f, files)\n\t            file_group = list(map(lambda f: file_group + \"/\" + f, files))\n\t        else:\n\t            file_group = [file_group]\n\t    x_data = []\n\t    y_data = []\n\t    std_data = []\n\t    num_files = len(file_group)\n\t    for file in file_group:\n\t        with open(file, \"r\") as file_data:\n", "            clean_lines = [line.replace(\",\", \"\\t\") for line in file_data]\n\t            data = np.genfromtxt(clean_lines, delimiter=None, dtype=None, names=True)\n\t        data_x_key = x_key\n\t        data_y_key = y_key\n\t        curr_window_size = filter_window_size\n\t        if data_x_key in data.dtype.names and data_y_key in data.dtype.names:\n\t            xs = data[data_x_key]\n\t            ys = data[data_y_key]\n\t            xs = filter_data(xs, curr_window_size)\n\t            ys = filter_data(ys, curr_window_size)\n", "            stds = None\n\t            if std_key is not None and std_key in data.dtype.names:\n\t                stds = data[std_key]\n\t                stds = filter_data(stds, curr_window_size)\n\t                std_data.append(stds)\n\t            x_data.append(xs)\n\t            y_data.append(ys)\n\t    label = os.path.basename(file_group[0])\n\t    label = os.path.splitext(label)[0]\n\t    line_col = None\n", "    curr_min_x, curr_max_x, _, _ = plot_util.plot_line(x_data, y_data, std_data, label, color=line_col,\n\t                                                      draw_band=draw_band)\n\t    min_len = int(reduce(lambda x, y: np.minimum(x, len(y)), x_data, np.inf))\n\t    x_final = x_data[0][min_len - 1]\n\t    y_final = np.array([y[min_len - 1] for y in y_data])\n\t    print(\"Final value: {:.2f}, {:.5f} +/- {:.5f}\".format(x_final, np.mean(y_final), np.std(y_final)))\n\t    min_x = min(curr_min_x, min_x)\n\t    max_x = max(curr_max_x, max_x)\n\tax = plt.gca()\n\tplt.xlabel(x_key)\n", "plt.ylabel(y_key)\n\tplt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n\tplt.grid(linestyle='dotted')\n\tax.xaxis.grid(True)\n\tax.yaxis.grid(True)\n\tplt.legend()\n\tplt.title(plot_title)\n\tplt.tight_layout()\n\tplt.show()"]}
{"filename": "util/torch_util.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef add_torch_dict(in_dict, out_dict):\n\t    for k, v in in_dict.items():\n\t        if (v.requires_grad):\n\t            v = v.detach()\n\t        if (k in out_dict):\n\t            out_dict[k] += v\n\t        else:\n\t            out_dict[k] = v\n", "    return\n\tdef scale_torch_dict(scale, out_dict):\n\t    for k in out_dict.keys():\n\t        out_dict[k] *= scale\n\t    return\n\tdef calc_layers_out_size(layers):\n\t    modules = list(layers.modules())\n\t    for m in reversed(modules):\n\t        if hasattr(m, \"out_features\"):\n\t            out_size = m.out_features\n", "            break\n\t    return out_size\n\tdef torch_dtype_to_numpy(torch_dtype):\n\t    if (torch_dtype == torch.float32):\n\t        numpy_dtype = np.float32\n\t    elif (torch_dtype == torch.float64):\n\t        numpy_dtype = np.float64\n\t    elif (torch_dtype == torch.uint8):\n\t        numpy_dtype = np.uint8\n\t    elif (torch_dtype == torch.int64):\n", "        numpy_dtype = np.int64\n\t    else:\n\t        assert(False), \"Unsupported type {}\".format(torch_dtype)\n\t    return numpy_dtype\n\tdef numpy_dtype_to_torch(numpy_dtype):\n\t    if (numpy_dtype == np.float32):\n\t        torch_dtype = torch.float32\n\t    elif (numpy_dtype == np.float64):\n\t        torch_dtype = torch.float64\n\t    elif (numpy_dtype == np.uint8):\n", "        torch_dtype = torch.uint8\n\t    elif (numpy_dtype == np.int64):\n\t        torch_dtype = torch.int64\n\t    else:\n\t        assert(False), \"Unsupported type {}\".format(numpy_dtype)\n\t    return torch_dtype\n\tclass UInt8ToFloat(torch.nn.Module):\n\t    def forward(self, x):\n\t        float_x = x.type(torch.float32) / 255.0\n\t        return float_x"]}
{"filename": "util/tb_logger.py", "chunked_list": ["import os\n\timport subprocess\n\timport tensorboardX\n\timport time\n\timport util.logger as logger\n\tclass TBLogger(logger.Logger):\n\t    MISC_TAG = \"Misc\"\n\t    def __init__(self, run_tb=False):\n\t        super().__init__()\n\t        self._writer = None\n", "        self._step_var_key = None\n\t        self._collections = dict()\n\t        self._run_tb = run_tb\n\t        self._tb_proc = None\n\t        return\n\t    def __del__(self):\n\t        if (self._tb_proc is not None):\n\t            self._tb_proc.kill()\n\t        return\n\t    def reset(self):\n", "        super().reset()\n\t        return\n\t    def configure_output_file(self, filename=None):\n\t        super().configure_output_file(filename)\n\t        output_dir = os.path.dirname(filename)\n\t        self._delete_event_files(output_dir)\n\t        self._writer = tensorboardX.SummaryWriter(output_dir)\n\t        if (self._run_tb):\n\t            self._run_tensorboard(output_dir)\n\t        return\n", "    def set_step_key(self, var_key):\n\t        self._step_key = var_key\n\t        return\n\t    def log(self, key, val, collection=None, quiet=False):\n\t        super().log(key, val, quiet)\n\t        if (collection is not None):\n\t            self._add_collection(collection, key)\n\t        return\n\t    def write_log(self):\n\t        row_count = self._row_count\n", "        super().write_log()\n\t        if (self._writer is not None):\n\t            if (row_count == 0):\n\t                self._key_tags = self._build_key_tags()\n\t            curr_time = time.time()\n\t            step_val = row_count\n\t            if (self._step_key is not None):\n\t                step_val = self.log_current_row.get(self._step_key, \"\").val\n\t            vals = []\n\t            for i, key in enumerate(self.log_headers):\n", "                if (key != self._step_key):\n\t                    entry = self.log_current_row.get(key, \"\")\n\t                    val = entry.val\n\t                    tag = self._key_tags[i]\n\t                    self._writer.add_scalar(tag, val, step_val)\n\t        return\n\t    def _add_collection(self, name, key):\n\t        if (name not in self._collections):\n\t            self._collections[name] = []\n\t        self._collections[name].append(key)\n", "        return\n\t    def _delete_event_files(self, dir):\n\t        if (os.path.exists(dir)):\n\t            files = os.listdir(dir)\n\t            for file in files:\n\t                if (\"events.out.tfevents.\" in file):\n\t                    file_path = os.path.join(dir, file)\n\t                    print(\"Deleting event file: {:s}\".format(file_path))\n\t                    os.remove(file_path)\n\t        return\n", "    def _build_key_tags(self):\n\t        tags = []\n\t        for key in self.log_headers:\n\t            curr_tag = TBLogger.MISC_TAG\n\t            for col_tag, col_keys in self._collections.items():\n\t                if key in col_keys:\n\t                    curr_tag = col_tag\n\t            curr_tags = \"{:s}/{:s}\".format(curr_tag, key)\n\t            tags.append(curr_tags)\n\t        return tags\n", "    def _run_tensorboard(self, output_dir):\n\t        cmd = \"tensorboard --logdir {:s}\".format(output_dir)\n\t        self._tb_proc = subprocess.Popen(cmd, shell=True)\n\t        return"]}
{"filename": "util/logger.py", "chunked_list": ["import os.path as osp, shutil, time, atexit, os, subprocess\n\tclass Logger:\n\t    class Entry:\n\t        def __init__(self, val, quiet=False):\n\t            self.val = val\n\t            self.quiet = quiet\n\t            return\n\t    def __init__(self):\n\t        self.output_file = None\n\t        self.log_headers = []\n", "        self.log_current_row = {}\n\t        self._dump_str_template = \"\"\n\t        self._max_key_len = 0\n\t        self._row_count = 0\n\t        return\n\t    def reset(self):\n\t        self._row_count = 0\n\t        self.log_headers = []\n\t        self.log_current_row = {}\n\t        if self.output_file is not None:\n", "            self.output_file = open(output_path, 'w')\n\t        return\n\t    def configure_output_file(self, filename=None):\n\t        \"\"\"\n\t        Set output directory to d, or to /tmp/somerandomnumber if d is None\n\t        \"\"\"\n\t        self._row_count = 0\n\t        self.log_headers = []\n\t        self.log_current_row = {}\n\t        output_path = filename or \"output/log_%i.txt\"%int(time.time())\n", "        out_dir = os.path.dirname(output_path)\n\t        if (not os.path.exists(out_dir)):\n\t            os.makedirs(out_dir, exist_ok=True)\n\t        self.output_file = open(output_path, 'w')\n\t        assert osp.exists(output_path)\n\t        atexit.register(self.output_file.close)\n\t        print(\"Logging data to \" + self.output_file.name)\n\t        return\n\t    def log(self, key, val, quiet=False):\n\t        \"\"\"\n", "        Log a value of some diagnostic\n\t        Call this once for each diagnostic quantity, each iteration\n\t        \"\"\"\n\t        if (self._row_count == 0) and key not in self.log_headers:\n\t            self.log_headers.append(key)\n\t            self._max_key_len = max(self._max_key_len, len(key))\n\t        else:\n\t            assert key in self.log_headers, \"Trying to introduce a new key %s that you didn't include in the first iteration\"%key\n\t        self.log_current_row[key] = Logger.Entry(val, quiet)\n\t        return\n", "    def get_num_keys(self):\n\t        return len(self.log_headers)\n\t    def print_log(self):\n\t        \"\"\"\n\t        Print all of the diagnostics from the current iteration\n\t        \"\"\"\n\t        key_spacing = self._max_key_len\n\t        format_str = \"| %\" + str(key_spacing) + \"s | %15s |\"\n\t        vals = []\n\t        print(\"-\" * (22 + key_spacing))\n", "        for key in self.log_headers:\n\t            entry = self.log_current_row.get(key, \"\")\n\t            if not (entry.quiet):\n\t                val = entry.val\n\t                if isinstance(val, float):\n\t                    valstr = \"%8.3g\"%val\n\t                elif isinstance(val, int):\n\t                    valstr = str(val)\n\t                else: \n\t                    valstr = val\n", "                print(format_str%(key, valstr))\n\t                vals.append(val)\n\t        print(\"-\" * (22 + key_spacing))\n\t        return\n\t    def write_log(self):\n\t        \"\"\"\n\t        Write all of the diagnostics from the current iteration\n\t        \"\"\"\n\t        if (self._row_count == 0):\n\t            self._dump_str_template = self._build_str_template()\n", "        vals = []\n\t        for key in self.log_headers:\n\t            entry = self.log_current_row.get(key, \"\")\n\t            val = entry.val\n\t            vals.append(val)\n\t        if self.output_file is not None:\n\t            if (self._row_count == 0):\n\t                header_str = self._dump_str_template.format(*self.log_headers)\n\t                self.output_file.write(header_str + \"\\r\")\n\t            val_str = self._dump_str_template.format(*map(str,vals))\n", "            self.output_file.write(val_str + \"\\r\")\n\t            self.output_file.flush()\n\t        self._row_count += 1\n\t        return\n\t    def has_key(self, key):\n\t        return key in self.log_headers\n\t    def get_current_val(self, key):\n\t        val = None\n\t        if (key in self.log_current_row.keys()):\n\t            entry = self.log_current_row[key]\n", "            val = entry.val\n\t        return val\n\t    def _build_str_template(self):\n\t        num_keys = self.get_num_keys()\n\t        template = \"{:<25}\" * num_keys\n\t        return template"]}
{"filename": "util/util.py", "chunked_list": ["import random\n\timport numpy as np\n\timport torch\n\tdef set_rand_seed(seed):\n\t    print(\"Setting seed: {}\".format(seed))\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n", "    return"]}
{"filename": "util/math_util.py", "chunked_list": ["import numpy as np\n\tRAD_TO_DEG = 180.0 / np.pi\n\tDEG_TO_RAD = np.pi/ 180.0\n\tINVALID_IDX = -1\n\tdef lerp(x, y, t):\n\t    return (1 - t) * x + t * y\n\tdef log_lerp(x, y, t):\n\t    return np.exp(lerp(np.log(x), np.log(y), t))\n\tdef flatten(arr_list):\n\t    return np.concatenate([np.reshape(a, [-1]) for a in arr_list], axis=0)\n", "def flip_coin(p):\n\t    rand_num = np.random.binomial(1, p, 1)\n\t    return rand_num[0] == 1\n\tdef add_average(avg0, count0, avg1, count1):\n\t\ttotal = count0 + count1\n\t\treturn (float(count0) / total) * avg0 + (float(count1) / total) * avg1\n\tdef smooth_step(x):\n\t    smooth_x = x * x * x * (x * (x * 6 - 15) + 10)\n\t    return smooth_x"]}
{"filename": "a3/dqn_agent.py", "chunked_list": ["import gym\n\timport numpy as np\n\timport torch\n\timport envs.base_env as base_env\n\timport learning.base_agent as base_agent\n\timport learning.dqn_model as dqn_model\n\timport util.torch_util as torch_util\n\tclass DQNAgent(base_agent.BaseAgent):\n\t    NAME = \"DQN\"\n\t    def __init__(self, config, env, device):\n", "        super().__init__(config, env, device)\n\t        return\n\t    def _load_params(self, config):\n\t        super()._load_params(config)\n\t        buffer_size = config[\"exp_buffer_size\"]\n\t        self._exp_buffer_length = int(buffer_size)\n\t        self._exp_buffer_length = max(self._exp_buffer_length, self._steps_per_iter)\n\t        self._updates_per_iter = config[\"updates_per_iter\"]\n\t        self._batch_size = config[\"batch_size\"]\n\t        self._init_samples = config[\"init_samples\"]\n", "        self._tar_net_update_iters = config[\"tar_net_update_iters\"]\n\t        self._exp_anneal_samples = config.get(\"exp_anneal_samples\", np.inf)\n\t        self._exp_prob_beg = config.get(\"exp_prob_beg\", 1.0)\n\t        self._exp_prob_end = config.get(\"exp_prob_end\", 1.0)\n\t        return\n\t    def _build_model(self, config):\n\t        model_config = config[\"model\"]\n\t        self._model = dqn_model.DQNModel(model_config, self._env)\n\t        self._tar_model = dqn_model.DQNModel(model_config, self._env)\n\t        for param in self._tar_model.parameters():\n", "            param.requires_grad = False\n\t        self._sync_tar_model()\n\t        return\n\t    def _get_exp_buffer_length(self):\n\t        return self._exp_buffer_length\n\t    def _decide_action(self, obs, info):\n\t        norm_obs = self._obs_norm.normalize(obs)\n\t        qs = self._model.eval_q(norm_obs)\n\t        if (self._mode == base_agent.AgentMode.TRAIN):\n\t            a = self._sample_action(qs)\n", "        elif (self._mode == base_agent.AgentMode.TEST):\n\t            a = torch.argmax(qs, dim=-1)\n\t        else:\n\t            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n\t        a_info = {}\n\t        return a, a_info\n\t    def _init_train(self):\n\t        super()._init_train()\n\t        self._collect_init_samples(self._init_samples)\n\t        return\n", "    def _collect_init_samples(self, samples):\n\t        self.eval()\n\t        self.set_mode(base_agent.AgentMode.TRAIN)\n\t        self._rollout_train(samples)\n\t        return\n\t    def _update_model(self):\n\t        self.train()\n\t        train_info = dict()\n\t        for i in range(self._updates_per_iter):\n\t            batch = self._exp_buffer.sample(self._batch_size)\n", "            loss_info = self._compute_loss(batch)\n\t            self._optimizer.zero_grad()\n\t            loss = loss_info[\"loss\"]\n\t            loss.backward()\n\t            self._optimizer.step()\n\t            torch_util.add_torch_dict(loss_info, train_info)\n\t        torch_util.scale_torch_dict(1.0 / self._updates_per_iter, train_info)\n\t        if (self._iter % self._tar_net_update_iters == 0):\n\t            self._sync_tar_model()\n\t        return train_info\n", "    def _log_train_info(self, train_info, test_info, start_time):\n\t        super()._log_train_info(train_info, test_info, start_time)\n\t        self._logger.log(\"Exp_Prob\", self._get_exp_prob())\n\t        return\n\t    def _compute_loss(self, batch):\n\t        r = batch[\"reward\"]\n\t        done = batch[\"done\"]\n\t        a = batch[\"action\"]\n\t        norm_obs = self._obs_norm.normalize(batch[\"obs\"])\n\t        norm_next_obs = self._obs_norm.normalize(batch[\"next_obs\"])\n", "        tar_vals = self._compute_tar_vals(r, norm_next_obs, done)\n\t        q_loss = self._compute_q_loss(norm_obs, a, tar_vals)\n\t        info = {\"loss\": q_loss}\n\t        return info\n\t    def _get_exp_prob(self):\n\t        '''\n\t        TODO 1.1: Calculate the epsilon-greedy exploration probability given the current sample\n\t        count. The exploration probability should start with a value of self._exp_prob_beg, and\n\t        then linearly annealed to self._exp_prob_end over the course of self._exp_anneal_samples\n\t        timesteps.\n", "        '''\n\t        # placeholder\n\t        prob = 1.0\n\t        return prob\n\t    def _sample_action(self, qs):\n\t        '''\n\t        TODO 1.2: Sample actions according to the Q-values of each action (qs). Implement epsilon\n\t        greedy exploration, where the probability of sampling a random action (epsilon) is specified\n\t        by self._get_exp_prob(). With probability 1 - epsilon, greedily select the action with\n\t        the highest Q-value. With probability epsilon, select a random action uniformly from the\n", "        set of possible actions. The output (a) should be a tensor containing the index of the selected\n\t        action.\n\t        '''\n\t        exp_prob = self._get_exp_prob()\n\t        # placeholder\n\t        a = torch.zeros(qs.shape[0], device=self._device, dtype=torch.int64)\n\t        return a\n\t    def _compute_tar_vals(self, r, norm_next_obs, done):\n\t        '''\n\t        TODO 1.3: Compute target values for updating the Q-function. The inputs consist of a tensor\n", "        of rewards (r), normalized observations at the next timestep (norm_next_obs), and done flags\n\t        (done) indicating if a timestep is the last timestep of an episode. The output (tar_vals)\n\t        should be a tensor containing the target values for each sample. The target values should\n\t        be calculated using the target model (self._tar_model), not the main model (self._model).\n\t        The Q-function can be queried by using the method eval_q(norm_obs).\n\t        '''\n\t        # placeholder\n\t        tar_vals = torch.zeros_like(r)\n\t        return tar_vals\n\t    def _compute_q_loss(self, norm_obs, a, tar_vals):\n", "        '''\n\t        TODO 1.4: Compute a loss for updating the Q-function. The inputs consist of a tensor of\n\t        normalized observations (norm_obs), a tensor containing the indices of actions selected\n\t        at each timestep (a), and target values for each timestep (tar_vals). The output (loss)\n\t        should be a scalar tensor containing the loss for updating the Q-function.\n\t        '''\n\t        # placeholder\n\t        loss = torch.zeros(1)\n\t        return loss\n\t    def _sync_tar_model(self):\n", "        '''\n\t        TODO 1.5: Update the target model by copying the parameters from the main model. The\n\t        main model is given by self._model, and the target model is given by self._tar_model.\n\t        HINT: self._model.parameters() can be used to retrieve a list of tensors containing\n\t        the parameters of a model.\n\t        '''\n\t        return"]}
{"filename": "learning/bc_model.py", "chunked_list": ["import torch\n\timport learning.base_model as base_model\n\timport learning.nets.net_builder as net_builder\n\tclass BCModel(base_model.BaseModel):\n\t    def __init__(self, config, env):\n\t        super().__init__(config, env)\n\t        self._build_nets(config, env)\n\t        return\n\t    def _build_nets(self, config, env):\n\t        self._build_actor(config, env)\n", "        return\n\t    def _build_actor(self, config, env):\n\t        net_name = config[\"actor_net\"]\n\t        input_dict = self._build_actor_input_dict(env)\n\t        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n\t                                                                activation=self._activation)\n\t        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n\t        return\n\t    def _build_actor_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n", "        input_dict = {\"obs\": obs_space}\n\t        return input_dict\n\t    def eval_actor(self, obs):\n\t        h = self._actor_layers(obs)\n\t        a_dist = self._action_dist(h)\n\t        return a_dist  "]}
{"filename": "learning/expert_agent.py", "chunked_list": ["import learning.base_agent as base_agent\n\timport learning.expert_model as expert_model\n\tclass ExpertAgent(base_agent.BaseAgent):\n\t    NAME = \"Expert\"\n\t    def __init__(self, config, env, device):\n\t        super().__init__(config, env, device)\n\t        return\n\t    def _build_model(self, config):\n\t        model_config = config[\"model\"]\n\t        self._model = expert_model.ExpertModel(model_config, self._env)\n", "        return\n\t    def _get_exp_buffer_length(self):\n\t        return 1\n\t    def _decide_action(self, obs, info):\n\t        norm_obs = self._obs_norm.normalize(obs)\n\t        norm_action_dist = self._model.eval_actor(norm_obs)\n\t        if (self._mode == base_agent.AgentMode.TRAIN):\n\t            norm_a = norm_action_dist.sample()\n\t        elif (self._mode == base_agent.AgentMode.TEST):\n\t            norm_a = norm_action_dist.mode\n", "        else:\n\t            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n\t        norm_a_logp = norm_action_dist.log_prob(norm_a)\n\t        norm_a = norm_a.detach()\n\t        norm_a_logp = norm_a_logp.detach()\n\t        a = self._a_norm.unnormalize(norm_a)\n\t        a_info = {\n\t            \"a_logp\": norm_a_logp\n\t        }\n\t        return a, a_info"]}
{"filename": "learning/base_agent.py", "chunked_list": ["import abc\n\timport enum\n\timport gym\n\timport numpy as np\n\timport os\n\timport time\n\timport torch\n\timport envs.base_env as base_env\n\timport learning.experience_buffer as experience_buffer\n\timport learning.normalizer as normalizer\n", "import learning.return_tracker as return_tracker\n\timport util.tb_logger as tb_logger\n\timport util.torch_util as torch_util\n\timport learning.distribution_gaussian_diag as distribution_gaussian_diag\n\tclass AgentMode(enum.Enum):\n\t    TRAIN = 0\n\t    TEST = 1\n\tclass BaseAgent(torch.nn.Module):\n\t    NAME = \"base\"\n\t    def __init__(self, config, env, device):\n", "        super().__init__()\n\t        self._env = env\n\t        self._device = device\n\t        self._iter = 0\n\t        self._sample_count = 0\n\t        self._load_params(config)\n\t        self._build_normalizers()\n\t        self._build_model(config)\n\t        self._build_optimizer(config)\n\t        self._build_exp_buffer(config)\n", "        self._build_return_tracker()\n\t        self._mode = AgentMode.TRAIN\n\t        self._curr_obs = None\n\t        self._curr_info = None\n\t        self.to(self._device)\n\t        return\n\t    def train_model(self, max_samples, out_model_file, int_output_dir, log_file):\n\t        start_time = time.time()\n\t        self._curr_obs, self._curr_info = self._env.reset()\n\t        self._logger = self._build_logger(log_file)\n", "        self._init_train()\n\t        test_info = None\n\t        while self._sample_count < max_samples:\n\t            train_info = self._train_iter()\n\t            output_iter = (self._iter % self._iters_per_output == 0)\n\t            if (output_iter):\n\t                test_info = self.test_model(self._test_episodes)\n\t            self._sample_count = self._update_sample_count()\n\t            self._log_train_info(train_info, test_info, start_time) \n\t            self._logger.print_log()\n", "            if (output_iter):\n\t                self._logger.write_log()\n\t                self._output_train_model(self._iter, out_model_file, int_output_dir)\n\t                self._train_return_tracker.reset()\n\t                self._curr_obs, self._curr_info = self._env.reset()\n\t            self._iter += 1\n\t        return\n\t    def test_model(self, num_episodes):\n\t        self.eval()\n\t        self.set_mode(AgentMode.TEST)\n", "        self._curr_obs, self._curr_info = self._env.reset()\n\t        test_info = self._rollout_test(num_episodes)\n\t        return test_info\n\t    def get_action_size(self):\n\t        a_space = self._env.get_action_space()\n\t        if (isinstance(a_space, gym.spaces.Box)):\n\t            a_size = np.prod(a_space.shape)\n\t        elif (isinstance(a_space, gym.spaces.Discrete)):\n\t            a_size = 1\n\t        else:\n", "            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n\t        return a_size\n\t    def set_mode(self, mode):\n\t        self._mode = mode\n\t        if (self._mode == AgentMode.TRAIN):\n\t            self._env.set_mode(base_env.EnvMode.TRAIN)\n\t        elif (self._mode == AgentMode.TEST):\n\t            self._env.set_mode(base_env.EnvMode.TEST)\n\t        else:\n\t            assert(False), \"Unsupported agent mode: {}\".format(mode)\n", "        return\n\t    def save(self, out_file):\n\t        state_dict = self.state_dict()\n\t        torch.save(state_dict, out_file)\n\t        return\n\t    def load(self, in_file):\n\t        state_dict = torch.load(in_file, map_location=self._device)\n\t        self.load_state_dict(state_dict)\n\t        return\n\t    def _load_params(self, config):\n", "        self._discount = config[\"discount\"]\n\t        self._steps_per_iter = config[\"steps_per_iter\"]\n\t        self._iters_per_output = config[\"iters_per_output\"]\n\t        self._test_episodes = config[\"test_episodes\"]\n\t        self._normalizer_samples = config.get(\"normalizer_samples\", np.inf)\n\t        return\n\t    @abc.abstractmethod\n\t    def _build_model(self, config):\n\t        return\n\t    def _build_normalizers(self):\n", "        obs_space = self._env.get_obs_space()\n\t        obs_dtype = torch_util.numpy_dtype_to_torch(obs_space.dtype)\n\t        self._obs_norm = normalizer.Normalizer(obs_space.shape, device=self._device, dtype=obs_dtype)\n\t        self._a_norm = self._build_action_normalizer()\n\t        return\n\t    def _build_action_normalizer(self):\n\t        a_space = self._env.get_action_space()\n\t        a_dtype = torch_util.numpy_dtype_to_torch(a_space.dtype)\n\t        if (isinstance(a_space, gym.spaces.Box)):\n\t            a_mean = torch.tensor(0.5 * (a_space.high + a_space.low), device=self._device, dtype=a_dtype)\n", "            a_std = torch.tensor(0.5 * (a_space.high - a_space.low), device=self._device, dtype=a_dtype)\n\t            a_norm = normalizer.Normalizer(a_mean.shape, device=self._device, init_mean=a_mean, \n\t                                                 init_std=a_std, dtype=a_dtype)\n\t        elif (isinstance(a_space, gym.spaces.Discrete)):\n\t            a_mean = torch.tensor([0], device=self._device, dtype=a_dtype)\n\t            a_std = torch.tensor([1], device=self._device, dtype=a_dtype)\n\t            a_norm = normalizer.Normalizer(a_mean.shape, device=self._device, init_mean=a_mean, \n\t                                                 init_std=a_std, eps=0, dtype=a_dtype)\n\t        else:\n\t            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n", "        return a_norm\n\t    def _build_optimizer(self, config):\n\t        lr = float(config[\"learning_rate\"])\n\t        params = list(self.parameters())\n\t        params = [p for p in params if p.requires_grad]\n\t        self._optimizer = torch.optim.SGD(params, lr, momentum=0.9)\n\t        return\n\t    def _build_exp_buffer(self, config):\n\t        buffer_length = self._get_exp_buffer_length()\n\t        self._exp_buffer = experience_buffer.ExperienceBuffer(buffer_length=buffer_length,\n", "                                                              device=self._device)\n\t        obs_space = self._env.get_obs_space()\n\t        obs_dtype = torch_util.numpy_dtype_to_torch(obs_space.dtype)\n\t        obs_buffer = torch.zeros([buffer_length] + list(obs_space.shape), device=self._device, dtype=obs_dtype)\n\t        self._exp_buffer.add_buffer(\"obs\", obs_buffer)\n\t        next_obs_buffer = torch.zeros_like(obs_buffer)\n\t        self._exp_buffer.add_buffer(\"next_obs\", next_obs_buffer)\n\t        a_space = self._env.get_action_space()\n\t        a_dtype = torch_util.numpy_dtype_to_torch(a_space.dtype)\n\t        action_buffer = torch.zeros([buffer_length] + list(a_space.shape), device=self._device, dtype=a_dtype)\n", "        self._exp_buffer.add_buffer(\"action\", action_buffer)\n\t        reward_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n\t        self._exp_buffer.add_buffer(\"reward\", reward_buffer)\n\t        done_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.int)\n\t        self._exp_buffer.add_buffer(\"done\", done_buffer)\n\t        return\n\t    def _build_return_tracker(self):\n\t        self._train_return_tracker = return_tracker.ReturnTracker(self._device)\n\t        return\n\t    @abc.abstractmethod\n", "    def _get_exp_buffer_length(self):\n\t        return 0\n\t    def _build_logger(self, log_file):\n\t        log = tb_logger.TBLogger()\n\t        log.set_step_key(\"Samples\")\n\t        log.configure_output_file(log_file)\n\t        return log\n\t    def _update_sample_count(self):\n\t        return self._exp_buffer.get_total_samples()\n\t    def _init_train(self):\n", "        self._iter = 0\n\t        self._sample_count = 0\n\t        self._exp_buffer.clear()\n\t        self._train_return_tracker.reset()\n\t        return\n\t    def _train_iter(self):\n\t        self._init_iter()\n\t        self.eval()\n\t        self.set_mode(AgentMode.TRAIN)\n\t        self._rollout_train(self._steps_per_iter)\n", "        data_info = self._build_train_data()\n\t        train_info = self._update_model()\n\t        if (self._need_normalizer_update()):\n\t            self._update_normalizers()\n\t        info = {**train_info, **data_info}\n\t        info[\"mean_return\"] = self._train_return_tracker.get_mean_return().item()\n\t        info[\"mean_ep_len\"] = self._train_return_tracker.get_mean_ep_len().item()\n\t        info[\"episodes\"] = self._train_return_tracker.get_episodes()\n\t        return info\n\t    def _init_iter(self):\n", "        return\n\t    def _rollout_train(self, num_steps):\n\t        for i in range(num_steps):\n\t            action, action_info = self._decide_action(self._curr_obs, self._curr_info)\n\t            self._record_data_pre_step(self._curr_obs, self._curr_info, action, action_info)\n\t            next_obs, r, done, next_info = self._step_env(action)\n\t            self._train_return_tracker.update(r, done)\n\t            self._record_data_post_step(next_obs, r, done, next_info)\n\t            self._curr_obs = next_obs\n\t            if done != base_env.DoneFlags.NULL.value:  \n", "                self._curr_obs, self._curr_info = self._env.reset()\n\t            self._exp_buffer.inc()\n\t        return\n\t    def _rollout_test(self, num_episodes):\n\t        sum_rets = 0.0\n\t        sum_ep_lens = 0\n\t        self._curr_obs, self._curr_info = self._env.reset()\n\t        for e in range(num_episodes):\n\t            curr_ret = 0.0\n\t            curr_ep_len = 0\n", "            while True:\n\t                action, action_info = self._decide_action(self._curr_obs, self._curr_info)\n\t                self._curr_obs, r, done, self._curr_info = self._step_env(action)\n\t                curr_ret += r.item()\n\t                curr_ep_len += 1\n\t                if done != base_env.DoneFlags.NULL.value:\n\t                    sum_rets += curr_ret\n\t                    sum_ep_lens += curr_ep_len\n\t                    self._curr_obs, self._curr_info = self._env.reset()\n\t                    break\n", "        mean_return = sum_rets / max(1, num_episodes)\n\t        mean_ep_len = sum_ep_lens / max(1, num_episodes)\n\t        test_info = {\n\t            \"mean_return\": mean_return,\n\t            \"mean_ep_len\": mean_ep_len,\n\t            \"episodes\": num_episodes\n\t        }\n\t        return test_info\n\t    @abc.abstractmethod\n\t    def _decide_action(self, obs, info):\n", "        a = None\n\t        a_info = dict()\n\t        return a, a_info\n\t    def _step_env(self, action):\n\t        obs, r, done, info = self._env.step(action)\n\t        return obs, r, done, info\n\t    def _record_data_pre_step(self, obs, info, action, action_info):\n\t        self._exp_buffer.record(\"obs\", obs)\n\t        self._exp_buffer.record(\"action\", action)\n\t        if (self._need_normalizer_update()):\n", "            self._obs_norm.record(obs.unsqueeze(0))\n\t        return\n\t    def _record_data_post_step(self, next_obs, r, done, next_info):\n\t        self._exp_buffer.record(\"next_obs\", next_obs)\n\t        self._exp_buffer.record(\"reward\", r)\n\t        self._exp_buffer.record(\"done\", done)\n\t        return\n\t    def _reset_done_envs(self, done):\n\t        done_indices = (done != base_env.DoneFlags.NULL.value).nonzero(as_tuple=False)\n\t        done_indices = torch.flatten(done_indices)\n", "        obs, info = self._env.reset(done_indices)\n\t        return obs, info\n\t    def _need_normalizer_update(self):\n\t        return self._sample_count < self._normalizer_samples\n\t    def _update_normalizers(self):\n\t        self._obs_norm.update()\n\t        return\n\t    def _build_train_data(self):\n\t        return dict()\n\t    @abc.abstractmethod\n", "    def _update_model(self):\n\t        return\n\t    def _compute_succ_val(self):\n\t        r_succ = self._env.get_reward_succ()\n\t        val_succ = r_succ / (1.0 - self._discount)\n\t        return val_succ\n\t    def _compute_fail_val(self):\n\t        r_fail = self._env.get_reward_fail()\n\t        val_fail = r_fail / (1.0 - self._discount)\n\t        return val_fail\n", "    def _compute_val_bound(self):\n\t        r_min, r_max = self._env.get_reward_bounds()\n\t        val_min = r_min / (1.0 - self._discount)\n\t        val_max = r_max / (1.0 - self._discount)\n\t        return val_min, val_max\n\t    def _log_train_info(self, train_info, test_info, start_time):\n\t        wall_time = (time.time() - start_time) / (60 * 60) # store time in hours\n\t        self._logger.log(\"Iteration\", self._iter, collection=\"1_Info\")\n\t        self._logger.log(\"Wall_Time\", wall_time, collection=\"1_Info\")\n\t        self._logger.log(\"Samples\", self._sample_count, collection=\"1_Info\")\n", "        test_return = test_info[\"mean_return\"]\n\t        test_ep_len = test_info[\"mean_ep_len\"]\n\t        test_eps = test_info[\"episodes\"]\n\t        self._logger.log(\"Test_Return\", test_return, collection=\"0_Main\")\n\t        self._logger.log(\"Test_Episode_Length\", test_ep_len, collection=\"0_Main\", quiet=True)\n\t        self._logger.log(\"Test_Episodes\", test_eps, collection=\"1_Info\", quiet=True)\n\t        train_return = train_info.pop(\"mean_return\")\n\t        train_ep_len = train_info.pop(\"mean_ep_len\")\n\t        train_eps = train_info.pop(\"episodes\")\n\t        self._logger.log(\"Train_Return\", train_return, collection=\"0_Main\")\n", "        self._logger.log(\"Train_Episode_Length\", train_ep_len, collection=\"0_Main\", quiet=True)\n\t        self._logger.log(\"Train_Episodes\", train_eps, collection=\"1_Info\", quiet=True)\n\t        for k, v in train_info.items():\n\t            val_name = k.title()\n\t            if torch.is_tensor(v):\n\t                v = v.item()\n\t            self._logger.log(val_name, v)\n\t        return\n\t    def _compute_action_bound_loss(self, norm_a_dist):\n\t        loss = None\n", "        action_space = self._env.get_action_space()\n\t        if (isinstance(action_space, gym.spaces.Box)):\n\t            a_low = action_space.low\n\t            a_high = action_space.high\n\t            valid_bounds = np.all(np.isfinite(a_low)) and np.all(np.isfinite(a_high))\n\t            if (valid_bounds):\n\t                assert(isinstance(norm_a_dist, distribution_gaussian_diag.DistributionGaussianDiag))\n\t                # assume that actions have been normalized between [-1, 1]\n\t                bound_min = -1\n\t                bound_max = 1\n", "                violation_min = torch.clamp_max(norm_a_dist.mode - bound_min, 0.0)\n\t                violation_max = torch.clamp_min(norm_a_dist.mode - bound_max, 0)\n\t                violation = torch.sum(torch.square(violation_min), dim=-1) \\\n\t                            + torch.sum(torch.square(violation_max), dim=-1)\n\t                loss = violation\n\t        return loss\n\t    def _output_train_model(self, iter, out_model_file, int_output_dir):\n\t        self.save(out_model_file)\n\t        if (int_output_dir != \"\"):\n\t            int_model_file = os.path.join(int_output_dir, \"model_{:010d}.pt\".format(iter))\n", "            self.save(int_model_file)\n\t        return\n"]}
{"filename": "learning/experience_buffer.py", "chunked_list": ["import torch\n\tclass ExperienceBuffer():\n\t    def __init__(self, buffer_length, device):\n\t        self._buffer_length = buffer_length\n\t        self._device = device\n\t        self._buffer_head = 0\n\t        self._total_samples = 0\n\t        self._buffers = dict()\n\t        self._sample_buf = torch.randperm(self._buffer_length, device=self._device,\n\t                                          dtype=torch.long)\n", "        self._sample_buf_head = 0\n\t        self._reset_sample_buf()\n\t        return\n\t    def add_buffer(self, name, buffer):\n\t        assert(len(buffer.shape) >= 1)\n\t        assert(buffer.shape[0] == self._buffer_length)\n\t        assert(name not in self._buffers)\n\t        self._buffers[name] = buffer\n\t        return\n\t    def reset(self):\n", "        self._buffer_head = 0\n\t        self._reset_sample_buf()\n\t        return\n\t    def clear(self):\n\t        self.reset()\n\t        self._total_samples = 0\n\t        return\n\t    def inc(self):\n\t        self._buffer_head = (self._buffer_head + 1) % self._buffer_length\n\t        self._total_samples += 1\n", "        return\n\t    def get_total_samples(self):\n\t        return self._total_samples\n\t    def get_sample_count(self):\n\t        sample_count = min(self._total_samples, self._buffer_length)\n\t        return sample_count\n\t    def record(self, name, data):\n\t        data_buf = self._buffers[name]\n\t        data_buf[self._buffer_head] = data\n\t        return\n", "    def set_data(self, name, data):\n\t        data_buf = self._buffers[name]\n\t        assert(data_buf.shape[0] == data.shape[0])\n\t        data_buf[:] = data\n\t        return\n\t    def get_data(self, name):\n\t        return self._buffers[name]\n\t    def sample(self, n):\n\t        output = dict()\n\t        rand_idx = self._sample_rand_idx(n)\n", "        for key, data in self._buffers.items():\n\t            batch_data = data[rand_idx]\n\t            output[key] = batch_data\n\t        return output\n\t    def _reset_sample_buf(self):\n\t        self._sample_buf[:] = torch.randperm(self._buffer_length, device=self._device, dtype=torch.long)\n\t        self._sample_buf_head = 0\n\t        return\n\t    def _sample_rand_idx(self, n):\n\t        buffer_len = self._sample_buf.shape[0]\n", "        assert(n <= buffer_len)\n\t        if (self._sample_buf_head + n <= buffer_len):\n\t            rand_idx = self._sample_buf[self._sample_buf_head:self._sample_buf_head + n]\n\t            self._sample_buf_head += n\n\t        else:\n\t            rand_idx0 = self._sample_buf[self._sample_buf_head:]\n\t            remainder = n - (buffer_len - self._sample_buf_head)\n\t            self._reset_sample_buf()\n\t            rand_idx1 = self._sample_buf[:remainder]\n\t            rand_idx = torch.cat([rand_idx0, rand_idx1], dim=0)\n", "            self._sample_buf_head = remainder\n\t        sample_count = self.get_sample_count()\n\t        rand_idx = torch.remainder(rand_idx, sample_count)\n\t        return rand_idx"]}
{"filename": "learning/pg_model.py", "chunked_list": ["import torch\n\timport learning.base_model as base_model\n\timport learning.nets.net_builder as net_builder\n\timport util.torch_util as torch_util\n\tclass PGModel(base_model.BaseModel):\n\t    def __init__(self, config, env):\n\t        super().__init__(config, env)\n\t        self._build_nets(config, env)\n\t        return\n\t    def eval_actor(self, obs):\n", "        h = self._actor_layers(obs)\n\t        a_dist = self._action_dist(h)\n\t        return a_dist\n\t    def eval_critic(self, obs):\n\t        h = self._critic_layers(obs)\n\t        val = self._critic_out(h)\n\t        return val\n\t    def _build_nets(self, config, env):\n\t        self._build_actor(config, env)\n\t        self._build_critic(config, env)\n", "        return\n\t    def _build_actor(self, config, env):\n\t        net_name = config[\"actor_net\"]\n\t        input_dict = self._build_actor_input_dict(env)\n\t        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n\t                                                                activation=self._activation)\n\t        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n\t        return\n\t    def _build_critic(self, config, env):\n\t        net_name = config[\"critic_net\"]\n", "        input_dict = self._build_critic_input_dict(env)\n\t        self._critic_layers, layers_info = net_builder.build_net(net_name, input_dict,\n\t                                                                 activation=self._activation)\n\t        layers_out_size = torch_util.calc_layers_out_size(self._critic_layers)\n\t        self._critic_out = torch.nn.Linear(layers_out_size, 1)\n\t        torch.nn.init.zeros_(self._critic_out.bias)\n\t        return\n\t    def _build_actor_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n\t        input_dict = {\"obs\": obs_space}\n", "        return input_dict\n\t    def _build_critic_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n\t        input_dict = {\"obs\": obs_space}\n\t        return input_dict\n"]}
{"filename": "learning/normalizer.py", "chunked_list": ["import numpy as np\n\timport torch\n\tclass Normalizer(torch.nn.Module):\n\t    def __init__(self, shape, device, init_mean=None, init_std=None, eps=1e-4, clip=np.inf, dtype=torch.float):\n\t        super().__init__()\n\t        self._eps = eps\n\t        self._clip = clip\n\t        self.dtype = dtype\n\t        self._build_params(shape, device, init_mean, init_std)\n\t        return\n", "    def record(self, x):\n\t        shape = self.get_shape()\n\t        assert len(x.shape) > len(shape)\n\t        x = x.flatten(start_dim=0, end_dim=len(x.shape) - len(shape) - 1)\n\t        self._new_count += x.shape[0]\n\t        self._new_sum += torch.sum(x, axis=0)\n\t        self._new_sum_sq += torch.sum(torch.square(x), axis=0)\n\t        return\n\t    def update(self):\n\t        if (self._mean_sq is None):\n", "            self._mean_sq = self._calc_mean_sq(self._mean, self._std)\n\t        new_count = self._new_count\n\t        new_mean = self._new_sum / new_count\n\t        new_mean_sq = self._new_sum_sq / new_count\n\t        new_total = self._count + new_count\n\t        w_old = self._count.type(torch.float) / new_total.type(torch.float)\n\t        w_new = float(new_count) / new_total.type(torch.float)\n\t        self._mean[:] = w_old * self._mean + w_new * new_mean\n\t        self._mean_sq[:] = w_old * self._mean_sq + w_new * new_mean_sq\n\t        self._count[:] = new_total\n", "        self._std[:] = self._calc_std(self._mean, self._mean_sq)\n\t        self._new_count = 0\n\t        self._new_sum[:] = 0\n\t        self._new_sum_sq[:] = 0\n\t        return\n\t    def get_shape(self):\n\t        return self._mean.shape\n\t    def get_count(self):\n\t        return self._count\n\t    def get_mean(self):\n", "        return self._mean\n\t    def get_std(self):\n\t        return self._std\n\t    def set_mean_std(self, mean, std):\n\t        shape = self.get_shape()\n\t        is_array = isinstance(mean, np.ndarray) and isinstance(std, np.ndarray)\n\t        assert mean.shape == shape and std.shape == shape, \\\n\t            print(\"Normalizer shape mismatch, expecting size {:d}, but got {:d} and {:d}\".format(shape, mean.shape, std.shape))\n\t        self._mean[:] = mean\n\t        self._std[:] = std\n", "        self._mean_sq[:] = self._calc_mean_sq(self._mean, self._std)\n\t        return\n\t    def normalize(self, x):\n\t        norm_x = (x - self._mean) / (self._std + self._eps)\n\t        norm_x = torch.clamp(norm_x, -self._clip, self._clip)\n\t        return norm_x.type(self.dtype)\n\t    def unnormalize(self, norm_x):\n\t        x = norm_x * self._std + self._mean\n\t        return x.type(self.dtype)\n\t    def _calc_std(self, mean, mean_sq):\n", "        var = mean_sq - torch.square(mean)\n\t        # some time floating point errors can lead to small negative numbers\n\t        var = torch.clamp_min(var, 0.0)\n\t        std = torch.sqrt(var)\n\t        std = std.type(self.dtype)\n\t        return std\n\t    def _calc_mean_sq(self, mean, std):\n\t        mean_sq = torch.square(std) + torch.square(self._mean)\n\t        mean_sq = mean_sq.type(self.dtype)\n\t        return mean_sq\n", "    def _build_params(self, shape, device, init_mean, init_std):\n\t        self._count = torch.nn.Parameter(torch.zeros([1], device=device, requires_grad=False, dtype=torch.long), requires_grad=False)\n\t        self._mean = torch.nn.Parameter(torch.zeros(shape, device=device, requires_grad=False, dtype=self.dtype), requires_grad=False)\n\t        self._std = torch.nn.Parameter(torch.ones(shape, device=device, requires_grad=False, dtype=self.dtype), requires_grad=False)\n\t        if init_mean is not None:\n\t            assert init_mean.shape == shape, \\\n\t            print('Normalizer init mean shape mismatch, expecting {:d}, but got {:d}'.shape(size, init_mean.shape))\n\t            self._mean[:] = init_mean\n\t        if init_std is not None:\n\t            assert init_std.shape == shape, \\\n", "            print('Normalizer init std shape mismatch, expecting {:d}, but got {:d}'.format(shape, init_std.shape))\n\t            self._std[:] = init_std\n\t        self._mean_sq = None\n\t        self._new_count = 0\n\t        self._new_sum = torch.zeros_like(self._mean)\n\t        self._new_sum_sq = torch.zeros_like(self._mean)\n\t        return"]}
{"filename": "learning/agent_builder.py", "chunked_list": ["import yaml\n\timport learning.expert_agent as expert_agent\n\timport a1.bc_agent as bc_agent\n\timport a2.cem_agent as cem_agent\n\timport a2.pg_agent as pg_agent\n\timport a3.dqn_agent as dqn_agent\n\tdef build_agent(agent_file, env, device):\n\t    agent_config = load_agent_file(agent_file)\n\t    agent_name = agent_config[\"agent_name\"]\n\t    print(\"Building {} agent\".format(agent_name))\n", "    if (agent_name == bc_agent.BCAgent.NAME):\n\t        agent = bc_agent.BCAgent(config=agent_config, env=env, device=device)\n\t    elif (agent_name == expert_agent.ExpertAgent.NAME):\n\t        agent = expert_agent.ExpertAgent(config=agent_config, env=env, device=device)\n\t    elif (agent_name == cem_agent.CEMAgent.NAME):\n\t        agent = cem_agent.CEMAgent(config=agent_config, env=env, device=device)\n\t    elif (agent_name == pg_agent.PGAgent.NAME):\n\t        agent = pg_agent.PGAgent(config=agent_config, env=env, device=device)\n\t    elif (agent_name == dqn_agent.DQNAgent.NAME):\n\t        agent = dqn_agent.DQNAgent(config=agent_config, env=env, device=device) \n", "    else:\n\t        assert(False), \"Unsupported agent: {}\".format(agent_name)\n\t    return agent\n\tdef load_agent_file(file):\n\t    with open(file, \"r\") as stream:\n\t        agent_config = yaml.safe_load(stream)\n\t    return agent_config\n"]}
{"filename": "learning/return_tracker.py", "chunked_list": ["import torch\n\timport envs.base_env as base_env\n\tclass ReturnTracker():\n\t    def __init__(self, device):\n\t        num_envs = 1\n\t        self._device = device\n\t        self._episodes = 0\n\t        self._mean_return = torch.zeros([1], device=device, dtype=torch.float32)\n\t        self._mean_ep_len = torch.zeros([1], device=device, dtype=torch.float32)\n\t        self._return_buf = torch.zeros([num_envs], device=device, dtype=torch.float32)\n", "        self._ep_len_buf = torch.zeros([num_envs], device=device, dtype=torch.long)\n\t        self._eps_per_env_buf = torch.zeros([num_envs], device=device, dtype=torch.long)\n\t        return\n\t    def get_mean_return(self):\n\t        return self._mean_return\n\t    def get_mean_ep_len(self):\n\t        return self._mean_ep_len\n\t    def get_episodes(self):\n\t        return self._episodes\n\t    def get_eps_per_env(self):\n", "        return self._eps_per_env_buf\n\t    def reset(self):\n\t        self._episodes = 0\n\t        self._eps_per_env_buf[:] = 0\n\t        self._mean_return[:] = 0.0\n\t        self._mean_ep_len[:] = 0.0\n\t        self._return_buf[:] = 0.0\n\t        self._ep_len_buf[:] = 0\n\t        return\n\t    def update(self, reward, done):\n", "        assert(reward.shape == self._return_buf.shape)\n\t        assert(done.shape == self._return_buf.shape)\n\t        self._return_buf += reward\n\t        self._ep_len_buf += 1\n\t        reset_mask = done != base_env.DoneFlags.NULL.value\n\t        reset_ids = reset_mask.nonzero(as_tuple=False).flatten()\n\t        num_resets = len(reset_ids)\n\t        if (num_resets > 0):\n\t            new_mean_return = torch.mean(self._return_buf[reset_ids])\n\t            new_mean_ep_len = torch.mean(self._ep_len_buf[reset_ids].type(torch.float))\n", "            new_count = self._episodes + num_resets\n\t            w_new = float(num_resets) / new_count\n\t            w_old = float(self._episodes) / new_count\n\t            self._mean_return = w_new * new_mean_return + w_old * self._mean_return\n\t            self._mean_ep_len = w_new * new_mean_ep_len + w_old * self._mean_ep_len\n\t            self._episodes += num_resets\n\t            self._return_buf[reset_ids] = 0.0\n\t            self._ep_len_buf[reset_ids] = 0\n\t            self._eps_per_env_buf[reset_ids] += 1\n\t        return"]}
{"filename": "learning/expert_model.py", "chunked_list": ["import torch\n\timport learning.base_model as base_model\n\timport learning.nets.net_builder as net_builder\n\timport util.torch_util as torch_util\n\tclass ExpertModel(base_model.BaseModel):\n\t    def __init__(self, config, env):\n\t        super().__init__(config, env)\n\t        self._build_nets(config, env)\n\t        return\n\t    def eval_actor(self, obs):\n", "        h = self._actor_layers(obs)\n\t        a_dist = self._action_dist(h)\n\t        return a_dist\n\t    def eval_critic(self, obs):\n\t        h = self._critic_layers(obs)\n\t        val = self._critic_out(h)\n\t        return val\n\t    def _build_nets(self, config, env):\n\t        self._build_actor(config, env)\n\t        self._build_critic(config, env)\n", "        return\n\t    def _build_actor(self, config, env):\n\t        net_name = config[\"actor_net\"]\n\t        input_dict = self._build_actor_input_dict(env)\n\t        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n\t                                                                activation=self._activation)\n\t        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n\t        return\n\t    def _build_critic(self, config, env):\n\t        net_name = config[\"critic_net\"]\n", "        input_dict = self._build_critic_input_dict(env)\n\t        self._critic_layers, layers_info = net_builder.build_net(net_name, input_dict,\n\t                                                                 activation=self._activation)\n\t        layers_out_size = torch_util.calc_layers_out_size(self._critic_layers)\n\t        self._critic_out = torch.nn.Linear(layers_out_size, 1)\n\t        torch.nn.init.zeros_(self._critic_out.bias)\n\t        return\n\t    def _build_actor_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n\t        input_dict = {\"obs\": obs_space}\n", "        return input_dict\n\t    def _build_critic_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n\t        input_dict = {\"obs\": obs_space}\n\t        return input_dict\n"]}
{"filename": "learning/cem_model.py", "chunked_list": ["import torch\n\timport learning.base_model as base_model\n\timport learning.nets.net_builder as net_builder\n\tclass CEMModel(base_model.BaseModel):\n\t    def __init__(self, config, env):\n\t        super().__init__(config, env)\n\t        self._build_nets(config, env)\n\t        return\n\t    def eval_actor(self, obs):\n\t        h = self._actor_layers(obs)\n", "        a_dist = self._action_dist(h)\n\t        return a_dist\n\t    def _build_nets(self, config, env):\n\t        self._build_actor(config, env)\n\t        return\n\t    def _build_actor(self, config, env):\n\t        net_name = config[\"actor_net\"]\n\t        input_dict = self._build_actor_input_dict(env)\n\t        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n\t                                                                activation=self._activation)\n", "        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n\t        return\n\t    def _build_actor_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n\t        input_dict = {\"obs\": obs_space}\n\t        return input_dict"]}
{"filename": "learning/distribution_gaussian_diag.py", "chunked_list": ["import enum\n\timport numpy as np\n\timport torch\n\tclass StdType(enum.Enum):\n\t    FIXED = 0\n\t    CONSTANT = 1\n\t    VARIABLE = 2\n\tclass DistributionGaussianDiagBuilder(torch.nn.Module):\n\t    def __init__(self, in_size, out_size, std_type, init_std, init_output_scale=0.01):\n\t        super().__init__()\n", "        self._std_type = std_type\n\t        self._build_params(in_size, out_size, init_std, init_output_scale)\n\t        return\n\t    def _build_params(self, in_size, out_size, init_std, init_output_scale):\n\t        self._mean_net = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.uniform_(self._mean_net.weight, -init_output_scale, init_output_scale)\n\t        torch.nn.init.zeros_(self._mean_net.bias)\n\t        logstd = np.log(init_std)\n\t        if (self._std_type == StdType.FIXED):\n\t            self._logstd_net = torch.nn.Parameter(torch.zeros(out_size, requires_grad=False, dtype=torch.float32), requires_grad=False)\n", "            torch.nn.init.constant_(self._logstd_net, logstd)\n\t        elif (self._std_type == StdType.CONSTANT):\n\t            self._logstd_net = torch.nn.Parameter(torch.zeros(out_size, requires_grad=True, dtype=torch.float32), requires_grad=True)\n\t            torch.nn.init.constant_(self._logstd_net, logstd)\n\t        elif (self._std_type == StdType.VARIABLE):\n\t            self._logstd_net = torch.nn.Linear(in_size, out_size)\n\t            torch.nn.init.uniform_(self._logstd_net.weight, -init_output_scale, init_output_scale) \n\t            torch.nn.init.constant_(self._logstd_net.bias, logstd)\n\t        else:\n\t            assert(False), \"Unsupported StdType: {}\".format(self._std_type)\n", "        return\n\t    def forward(self, input):\n\t        mean = self._mean_net(input)\n\t        if (self._std_type == StdType.FIXED or self._std_type == StdType.CONSTANT):\n\t            logstd = torch.broadcast_to(self._logstd_net, mean.shape)\n\t        elif (self._std_type == StdType.VARIABLE):\n\t            logstd = self._logstd_net(input)\n\t        else:\n\t            assert(False), \"Unsupported StdType: {}\".format(self._std_type)\n\t        dist = DistributionGaussianDiag(mean=mean, logstd=logstd)\n", "        return dist\n\tclass DistributionGaussianDiag():\n\t    def __init__(self, mean, logstd):\n\t        self._mean = mean\n\t        self._logstd = logstd\n\t        self._std = torch.exp(self._logstd)\n\t        self._dim = self._mean.shape[-1]\n\t        return\n\t    @property\n\t    def stddev(self):\n", "        return self._std\n\t    @property\n\t    def logstd(self):\n\t        return self._logstd\n\t    @property\n\t    def mean(self):\n\t        return self._mean\n\t    @property\n\t    def mode(self):\n\t        return self._mean\n", "    def sample(self):\n\t        noise = torch.normal(torch.zeros_like(self._mean), torch.ones_like(self._std))\n\t        x = self._mean + self._std * noise\n\t        return x\n\t    def log_prob(self, x):\n\t        diff = x - self._mean\n\t        logp = -0.5 * torch.sum(torch.square(diff / self._std), dim=-1)\n\t        logp += -0.5 * self._dim * np.log(2.0 * np.pi) - torch.sum(self._logstd, dim=-1)\n\t        return logp\n\t    def entropy(self):\n", "        ent = torch.sum(self._logstd, dim=-1)\n\t        ent += 0.5 * self._dim * np.log(2.0 * np.pi * np.e)\n\t        return ent\n\t    def kl(self, other):\n\t        assert(isinstance(other, GaussianDiagDist))\n\t        other_var = torch.square(other.stddev)\n\t        res = torch.sum(other.logstd - self._logstd + (torch.square(self._std) + torch.square(self._mean - other.mean)) / (2.0 * other_var), dim=-1)\n\t        res += -0.5 * self._dim\n\t        return res\n\t    def param_reg(self):\n", "        # only regularize mean, covariance is regularized via entropy reg\n\t        reg = torch.sum(torch.square(self._mean), dim=-1)\n\t        return reg"]}
{"filename": "learning/base_model.py", "chunked_list": ["import gym\n\timport numpy as np\n\timport torch\n\timport learning.distribution_gaussian_diag as distribution_gaussian_diag\n\timport learning.distribution_categorical as distribution_categorical\n\timport util.torch_util as torch_util\n\tclass BaseModel(torch.nn.Module):\n\t    def __init__(self, config, env):\n\t        super().__init__()\n\t        self._activation = torch.nn.ReLU\n", "        return\n\t    def _build_action_distribution(self, config, env, input):\n\t        a_space = env.get_action_space()\n\t        in_size = torch_util.calc_layers_out_size(input)\n\t        if (isinstance(a_space, gym.spaces.Box)):\n\t            a_size = np.prod(a_space.shape)\n\t            a_init_output_scale = config[\"actor_init_output_scale\"]\n\t            a_std_type = distribution_gaussian_diag.StdType[config[\"actor_std_type\"]]\n\t            a_std = config[\"action_std\"]\n\t            a_dist = distribution_gaussian_diag.DistributionGaussianDiagBuilder(in_size, a_size, std_type=a_std_type,\n", "                                                                            init_std=a_std, init_output_scale=a_init_output_scale)\n\t        elif (isinstance(a_space, gym.spaces.Discrete)):\n\t            num_actions = a_space.n\n\t            a_init_output_scale = config[\"actor_init_output_scale\"]\n\t            a_dist = distribution_categorical.DistributionCategoricalBuilder(in_size, num_actions, \n\t                                                                             init_output_scale=a_init_output_scale)\n\t        else:\n\t            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n\t        return a_dist"]}
{"filename": "learning/dqn_model.py", "chunked_list": ["import gym\n\timport torch\n\timport learning.base_model as base_model\n\timport learning.nets.net_builder as net_builder\n\timport util.torch_util as torch_util\n\tclass DQNModel(base_model.BaseModel):\n\t    def __init__(self, config, env):\n\t        super().__init__(config, env)\n\t        self._build_nets(config, env)\n\t        return\n", "    def eval_q(self, obs):\n\t        h = self._q_layers(obs)\n\t        q = self._q_out(h)\n\t        return q\n\t    def _build_nets(self, config, env):\n\t        net_name = config[\"q_net\"]\n\t        q_init_output_scale = config[\"q_init_output_scale\"]\n\t        input_dict = self._build_q_input_dict(env)\n\t        a_space = env.get_action_space()\n\t        assert(isinstance(a_space, gym.spaces.Discrete))\n", "        num_actions = a_space.n\n\t        self._q_layers, _ = net_builder.build_net(net_name, input_dict,\n\t                                                  activation=self._activation)\n\t        layers_out_size = torch_util.calc_layers_out_size(self._q_layers)\n\t        self._q_out = torch.nn.Linear(layers_out_size, num_actions)\n\t        torch.nn.init.uniform_(self._q_out.weight, -q_init_output_scale, q_init_output_scale)\n\t        torch.nn.init.zeros_(self._q_out.bias)\n\t        return\n\t    def _build_q_input_dict(self, env):\n\t        obs_space = env.get_obs_space()\n", "        input_dict = {\"obs\": obs_space}\n\t        return input_dict"]}
{"filename": "learning/distribution_categorical.py", "chunked_list": ["import enum\n\timport numpy as np\n\timport torch\n\tclass DistributionCategoricalBuilder(torch.nn.Module):\n\t    def __init__(self, in_size, out_size, init_output_scale=0.01):\n\t        super().__init__()\n\t        self._build_params(in_size, out_size, init_output_scale)\n\t        return\n\t    def _build_params(self, in_size, out_size, init_output_scale):\n\t        self._logit_net = torch.nn.Linear(in_size, out_size)\n", "        torch.nn.init.uniform_(self._logit_net.weight, -init_output_scale, init_output_scale)\n\t        return\n\t    def forward(self, input):\n\t        logits = self._logit_net(input)\n\t        dist = DistributionCategorical(logits=logits)\n\t        return dist\n\tclass DistributionCategorical(torch.distributions.Categorical):\n\t    def __init__(self, logits):\n\t        logits = logits.unsqueeze(-2)\n\t        super().__init__(logits=logits)\n", "        return\n\t    @property\n\t    def mode(self):\n\t        arg_max = torch.argmax(self.logits, dim=-1)\n\t        return arg_max\n\t    def sample(self):\n\t        x = super().sample()\n\t        return x\n\t    def log_prob(self, x):\n\t        logp = super().log_prob(x)\n", "        logp = logp.squeeze(-1)\n\t        return logp\n\t    def entropy(self):\n\t        ent = super().entropy()\n\t        ent = ent.squeeze(-1)\n\t        return ent\n\t    def param_reg(self):\n\t        # only regularize mean, covariance is regularized via entropy reg\n\t        reg = torch.sum(torch.square(self.logits), dim=-1)\n\t        reg = reg.squeeze(-1)\n", "        return reg\n"]}
{"filename": "learning/nets/fc_2layers_1024units.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef build_net(input_dict, activation):\n\t    layer_sizes = [1024, 512]\n\t    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n\t    in_size = input_dim\n\t    layers = []\n\t    for out_size in layer_sizes:\n\t        curr_layer = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n", "        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_size = out_size\n\t    net = torch.nn.Sequential(*layers)\n\t    info = dict()\n\t    return net, info\n"]}
{"filename": "learning/nets/cnn_3conv_1fc_0.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport util.torch_util as torch_util\n\tdef build_net(input_dict, activation):\n\t    conv_kernel_size = [8, 4, 3]\n\t    conv_channels= [32, 64, 64]\n\t    conv_stride = [4, 2, 1]\n\t    fc_sizes = [512]\n\t    assert(len(input_dict) == 1)\n\t    obs_space = input_dict[\"obs\"]\n", "    in_channels = obs_space.shape[-3]\n\t    in_size = np.array(obs_space.shape[-2:], dtype=np.float32)\n\t    layers = []\n\t    if (obs_space.dtype == np.uint8):\n\t        to_float_layer = torch_util.UInt8ToFloat()\n\t        layers.append(to_float_layer)\n\t    for i in range(len(conv_kernel_size)):\n\t        kernel_size = conv_kernel_size[i]\n\t        channels = conv_channels[i]\n\t        stride = conv_stride[i]\n", "        curr_layer = torch.nn.Conv2d(in_channels=in_channels, \n\t                                     out_channels=channels, \n\t                                     kernel_size=kernel_size,\n\t                                     stride=stride)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n\t        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_channels = channels\n\t        in_size = np.ceil((in_size - kernel_size + 1) / stride)\n\t    layers.append(torch.nn.Flatten(start_dim=-3, end_dim=-1))\n", "    in_size = int(np.prod(in_size) * channels)\n\t    for out_size in fc_sizes:\n\t        curr_layer = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n\t        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_size = out_size\n\t    net = torch.nn.Sequential(*layers)\n\t    info = dict()\n\t    return net, info\n"]}
{"filename": "learning/nets/fc_2layers_128units.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef build_net(input_dict, activation):\n\t    layer_sizes = [128, 64]\n\t    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n\t    in_size = input_dim\n\t    layers = []\n\t    for out_size in layer_sizes:\n\t        curr_layer = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n", "        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_size = out_size\n\t    net = torch.nn.Sequential(*layers)\n\t    info = dict()\n\t    return net, info\n"]}
{"filename": "learning/nets/__init__.py", "chunked_list": ["from os.path import dirname, basename, isfile, join\n\timport glob\n\tmodules = glob.glob(join(dirname(__file__), \"*.py\"))\n\t__all__ = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(\"__init__.py\") and not f.endswith(\"net_builder.py\")]\n"]}
{"filename": "learning/nets/net_builder.py", "chunked_list": ["import torch\n\tfrom learning.nets import *\n\tdef build_net(net_name, input_dict, activation=torch.nn.ReLU):\n\t    if (net_name in globals()):\n\t        net_func = globals()[net_name]\n\t        net, info = net_func.build_net(input_dict, activation)\n\t    else:\n\t        assert(False), \"Unsupported net: {}\".format(net_name)\n\t    return net, info\n"]}
{"filename": "learning/nets/fc_1layers_16units.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef build_net(input_dict, activation):\n\t    layer_sizes = [16]\n\t    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n\t    in_size = input_dim\n\t    layers = []\n\t    for out_size in layer_sizes:\n\t        curr_layer = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n", "        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_size = out_size\n\t    net = torch.nn.Sequential(*layers)\n\t    info = dict()\n\t    return net, info\n"]}
{"filename": "learning/nets/fc_1layers_32units.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef build_net(input_dict, activation):\n\t    layer_sizes = [32]\n\t    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n\t    in_size = input_dim\n\t    layers = []\n\t    for out_size in layer_sizes:\n\t        curr_layer = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n", "        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_size = out_size\n\t    net = torch.nn.Sequential(*layers)\n\t    info = dict()\n\t    return net, info\n"]}
{"filename": "learning/nets/fc_2layers_64units.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef build_net(input_dict, activation):\n\t    layer_sizes = [64, 32]\n\t    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n\t    in_size = input_dim\n\t    layers = []\n\t    for out_size in layer_sizes:\n\t        curr_layer = torch.nn.Linear(in_size, out_size)\n\t        torch.nn.init.zeros_(curr_layer.bias)\n", "        layers.append(curr_layer)\n\t        layers.append(activation())\n\t        in_size = out_size\n\t    net = torch.nn.Sequential(*layers)\n\t    info = dict()\n\t    return net, info\n"]}
{"filename": "envs/atari_env.py", "chunked_list": ["import envs.base_env as base_env\n\timport envs.atari_wrappers as atari_wrappers\n\timport gym\n\timport numpy as np\n\timport torch\n\tclass AtariEnv(base_env.BaseEnv):\n\t    def __init__(self, config, device, visualize):\n\t        super().__init__(visualize)\n\t        self._device = device\n\t        self._curr_obs = None\n", "        self._curr_info = None\n\t        self._env = self._build_atari_env(config, visualize)\n\t        self.reset()\n\t        return\n\t    def reset(self):\n\t        obs, info = self._env.reset()\n\t        self._curr_obs = self._convert_obs(obs)\n\t        self._curr_info = info\n\t        return self._curr_obs, self._curr_info\n\t    def step(self, action):\n", "        a = action.cpu().numpy()[0]\n\t        obs, reward, terminated, truncated, info = self._env.step(a)\n\t        self._curr_obs = self._convert_obs(obs)\n\t        self._curr_info = info\n\t        # train with clipped reward and then test with unclipped rewards\n\t        if (self._mode is base_env.EnvMode.TRAIN):\n\t            reward = reward[1] # clipped reward\n\t        else:\n\t            reward = reward[0] # unclipped reward\n\t        reward = torch.tensor([reward], device=self._device)\n", "        done = self._compute_done(terminated, truncated) \n\t        return self._curr_obs, reward, done, self._curr_info \n\t    def _build_atari_env(self, config, visualize):\n\t        env_name = config[\"env_name\"]\n\t        max_timesteps = config.get(\"max_timesteps\", None)\n\t        assert(env_name.startswith(\"atari_\"))\n\t        env_name = env_name[6:]\n\t        env_name = env_name + \"NoFrameskip-v4\"\n\t        atari_env = atari_wrappers.make_atari(env_name, \n\t                                              max_episode_steps=max_timesteps,\n", "                                              visualize=visualize)\n\t        atari_env = atari_wrappers.wrap_deepmind(atari_env,\n\t                                                 warp_frame=True,\n\t                                                 frame_stack=True)\n\t        return atari_env\n\t    def _convert_obs(self, obs):\n\t        num_frames = obs.count()\n\t        frames = [np.expand_dims(obs.frame(i), axis=-3) for i in range(num_frames)]\n\t        frames = np.concatenate(frames, axis=-3)\n\t        obs = torch.tensor(frames, device=self._device)\n", "        obs = obs.unsqueeze(0)\n\t        return  obs\n\t    def _compute_done(self, terminated, truncated):\n\t        if (terminated): \n\t            done = torch.tensor([base_env.DoneFlags.FAIL.value], device=self._device)\n\t        elif (truncated): \n\t            done = torch.tensor([base_env.DoneFlags.TIME.value], device=self._device)\n\t        else:\n\t            done = torch.tensor([base_env.DoneFlags.NULL.value], device=self._device)\n\t        return done\n", "    def get_obs_space(self):\n\t        obs_space = self._env.observation_space\n\t        obs_shape = list(obs_space.shape)\n\t        obs_shape = obs_shape[-1:] + obs_shape[:-1]\n\t        obs_space = gym.spaces.Box(\n\t            low=obs_space.low.min(),\n\t            high=obs_space.high.max(),\n\t            shape=obs_shape,\n\t            dtype=obs_space.dtype,\n\t        )\n", "        return obs_space\n\t    def get_action_space(self):\n\t        a_space = self._env.action_space\n\t        a_space._shape = (1,)\n\t        return a_space\n\t    def get_reward_bounds(self):\n\t        return (-1.0, 1.0)"]}
{"filename": "envs/env_builder.py", "chunked_list": ["import yaml\n\timport envs.atari_env as atari_env\n\timport envs.env_dm as env_dm\n\tdef build_env(env_file, device, visualize):\n\t    env_config = load_env_file(env_file)\n\t    env_name = env_config[\"env_name\"]\n\t    print(\"Building {} env\".format(env_name))\n\t    if (env_name.startswith(\"dm_\")):\n\t        env = env_dm.DMEnv(config=env_config, device=device, visualize=visualize)\n\t    elif (env_name.startswith(\"atari_\")):\n", "        env = atari_env.AtariEnv(config=env_config, device=device, visualize=visualize)\n\t    else:\n\t        assert(False), \"Unsupported env: {}\".format(env_name)\n\t    return env\n\tdef load_env_file(file):\n\t    with open(file, \"r\") as stream:\n\t        env_config = yaml.safe_load(stream)\n\t    return env_config\n"]}
{"filename": "envs/env_dm.py", "chunked_list": ["import envs.base_env as base_env\n\tfrom dm_control import suite\n\timport dm_env\n\timport gym\n\timport multiprocessing\n\timport numpy as np\n\timport pygame\n\timport torch\n\tSCREEN_WIDTH = 640\n\tSCREEN_HEIGHT = 360\n", "class DMEnv(base_env.BaseEnv):\n\t    def __init__(self, config, device, visualize):\n\t        super().__init__(visualize)\n\t        self._device = device\n\t        self._visualize = visualize\n\t        self._time_limit = config['time_limit']\n\t        self._mode = None\n\t        self._env = self._build_dm_env(config)\n\t        if (self._visualize):\n\t            self._init_render()\n", "        self.reset()\n\t        return\n\t    def reset(self):\n\t        time_step = self._env.reset()\n\t        obs = self._convert_obs(time_step.observation)\n\t        return obs, {}\n\t    def step(self, action):\n\t        time_step = self._env.step(action.cpu().numpy())\n\t        obs = self._convert_obs(time_step.observation)\n\t        reward = 0 if time_step.reward == None else time_step.reward\n", "        reward = torch.tensor([reward], device=self._device)\n\t        done = self._compute_done(time_step.last()) \n\t        info = dict()\n\t        if self._visualize:\n\t            self._render()\n\t        return obs, reward, done, info \n\t    def _build_dm_env(self, config):\n\t        env_name = config['env_name']\n\t        task_name = config['task']\n\t        assert(env_name.startswith(\"dm_\"))\n", "        env_name = env_name[3:]\n\t        dm_env = suite.load(env_name, task_name)\n\t        return dm_env\n\t    def _convert_obs(self, obs):\n\t        obs_list = []\n\t        for v in obs.values():\n\t            if not isinstance(v, np.ndarray): \n\t                obs_list.append(np.array([v]))\n\t            else:  \n\t                obs_list.append(v)\n", "        flat_obs = np.concatenate(obs_list, axis=-1)\n\t        flat_obs = flat_obs.astype(np.float32)\n\t        flat_obs = torch.from_numpy(flat_obs)\n\t        flat_obs = flat_obs.to(device=self._device)\n\t        return  flat_obs\n\t    def _compute_done(self, last):\n\t        if self._env._physics.time() >= self._time_limit: \n\t            done = torch.tensor([base_env.DoneFlags.TIME.value], device=self._device)\n\t        elif last: \n\t            done = torch.tensor([base_env.DoneFlags.FAIL.value], device=self._device)\n", "        else:\n\t            done = torch.tensor([base_env.DoneFlags.NULL.value], device=self._device)\n\t        return done\n\t    def get_action_space(self):\n\t        action_spec = self._env.action_spec()\n\t        if (isinstance(action_spec, dm_env.specs.BoundedArray)):\n\t            low = action_spec.minimum\n\t            high = action_spec.maximum\n\t            action_space = gym.spaces.Box(low=low, high=high, shape=low.shape)\n\t        else:\n", "            assert(False), \"Unsupported DM action type\"\n\t        return action_space\n\t    def get_reward_bounds(self):\n\t        return (0.0, 1.0)\n\t    def _init_render(self):\n\t        self._clock = pygame.time.Clock()\n\t        self._render_queue = multiprocessing.Queue()\n\t        self._render_proc = multiprocessing.Process(target=render_worker, args=(SCREEN_WIDTH, SCREEN_HEIGHT, self._render_queue))\n\t        self._render_proc.start()\n\t        return\n", "    def _render(self):\n\t        im = self._env.physics.render(SCREEN_HEIGHT, SCREEN_WIDTH, camera_id=0)\n\t        im = np.transpose(im, axes=[1, 0, 2])\n\t        self._render_queue.put(im)\n\t        fps = 1.0 / self._env.control_timestep()\n\t        self._clock.tick(fps)\n\t        return\n\tdef render_worker(screen_w, screen_h, render_queue):\n\t    pygame.init()     \n\t    window = pygame.display.set_mode((screen_w, screen_h))\n", "    pygame.display.set_caption(\"DeepMind Control Suite\")\n\t    done = False\n\t    while not done:\n\t        for event in pygame.event.get():\n\t            if event.type == pygame.QUIT:\n\t                done = True\n\t        im = render_queue.get()\n\t        surface = pygame.pixelcopy.make_surface(im)\n\t        window.blit(surface, (0, 0))\n\t        pygame.display.update()\n", "    pygame.quit()\n\t    return\n"]}
{"filename": "envs/base_env.py", "chunked_list": ["import abc\n\timport enum\n\timport gym\n\timport numpy as np\n\timport util.torch_util as torch_util\n\tclass EnvMode(enum.Enum):\n\t    TRAIN = 0\n\t    TEST = 1\n\tclass DoneFlags(enum.Enum):\n\t    NULL = 0\n", "    FAIL = 1\n\t    SUCC = 2\n\t    TIME = 3\n\tclass BaseEnv(abc.ABC):\n\t    NAME = \"base\"\n\t    def __init__(self, visualize):\n\t        self._mode = EnvMode.TRAIN\n\t        self._visualize = visualize\n\t        self._action_space = None\n\t        return\n", "    @abc.abstractmethod\n\t    def reset(self, env_ids=None):\n\t        return\n\t    @abc.abstractmethod\n\t    def step(self, action):\n\t        return\n\t    def get_obs_space(self):\n\t        obs, _ = self.reset()\n\t        obs_shape = list(obs.shape)\n\t        obs_dtype = torch_util.torch_dtype_to_numpy(obs.dtype)\n", "        obs_space = gym.spaces.Box(\n\t            low=-np.inf,\n\t            high=np.inf,\n\t            shape=obs_shape,\n\t            dtype=obs_dtype,\n\t        )\n\t        return obs_space\n\t    def get_action_space(self):\n\t        return self._action_space\n\t    def set_mode(self, mode):\n", "        self._mode = mode\n\t        return\n\t    def get_num_envs(self):\n\t        return int(1)\n\t    def get_reward_bounds(self):\n\t        return (-np.inf, np.inf)\n\t    def get_reward_fail(self):\n\t        return 0.0\n\t    def get_reward_succ(self):\n\t        return 0.0\n", "    def get_visualize(self):\n\t        return self._visualize\n"]}
{"filename": "envs/atari_wrappers.py", "chunked_list": ["import numpy as np\n\tfrom collections import deque\n\timport gym\n\tfrom gym import spaces\n\timport cv2\n\tcv2.ocl.setUseOpenCL(False)\n\tfrom gym.wrappers import TimeLimit\n\tclass NoopResetEnv(gym.Wrapper):\n\t    def __init__(self, env, noop_max=30):\n\t        \"\"\"Sample initial states by taking random number of no-ops on reset.\n", "        No-op is assumed to be action 0.\n\t        \"\"\"\n\t        gym.Wrapper.__init__(self, env)\n\t        self.noop_max = noop_max\n\t        self.override_num_noops = None\n\t        self.noop_action = 0\n\t        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n\t    def reset(self, **kwargs):\n\t        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n\t        self.env.reset(**kwargs)\n", "        if self.override_num_noops is not None:\n\t            noops = self.override_num_noops\n\t        else:\n\t            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n\t        assert noops > 0\n\t        obs = None\n\t        info = None\n\t        for _ in range(noops):\n\t            obs, _, terminated, truncated, info = self.env.step(self.noop_action)\n\t            done = terminated or truncated\n", "            if done:\n\t                obs, info = self.env.reset(**kwargs)\n\t        return obs, info\n\t    def step(self, ac):\n\t        return self.env.step(ac)\n\tclass FireResetEnv(gym.Wrapper):\n\t    def __init__(self, env):\n\t        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n\t        gym.Wrapper.__init__(self, env)\n\t        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n", "        assert len(env.unwrapped.get_action_meanings()) >= 3\n\t    def reset(self, **kwargs):\n\t        self.env.reset(**kwargs)\n\t        obs, _, terminated, truncated, info = self.env.step(1)\n\t        done = terminated or truncated\n\t        if done:\n\t            self.env.reset(**kwargs)\n\t        obs, _, terminated, truncated, info = self.env.step(1)\n\t        done = terminated or truncated\n\t        if done:\n", "            self.env.reset(**kwargs)\n\t        return obs, info\n\t    def step(self, ac):\n\t        return self.env.step(ac)\n\tclass EpisodicLifeEnv(gym.Wrapper):\n\t    def __init__(self, env):\n\t        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n\t        Done by DeepMind for the DQN and co. since it helps value estimation.\n\t        \"\"\"\n\t        gym.Wrapper.__init__(self, env)\n", "        self.lives = 0\n\t        self.was_real_done  = True\n\t    def step(self, action):\n\t        obs, reward, terminated, truncated, info = self.env.step(action)\n\t        done = terminated or truncated\n\t        self.was_real_done = done\n\t        # check current lives, make loss of life terminal,\n\t        # then update lives to handle bonus lives\n\t        lives = self.env.unwrapped.ale.lives()\n\t        if lives < self.lives and lives > 0:\n", "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n\t            # so it's important to keep lives > 0, so that we only reset once\n\t            # the environment advertises done.\n\t            done = True\n\t        self.lives = lives\n\t        return obs, reward, terminated, truncated, info\n\t    def reset(self, **kwargs):\n\t        \"\"\"Reset only when lives are exhausted.\n\t        This way all states are still reachable even though lives are episodic,\n\t        and the learner need not know about any of this behind-the-scenes.\n", "        \"\"\"\n\t        if self.was_real_done:\n\t            obs, info = self.env.reset(**kwargs)\n\t        else:\n\t            # no-op step to advance from terminal/lost life state\n\t            obs, _, _, _, info = self.env.step(0)\n\t        self.lives = self.env.unwrapped.ale.lives()\n\t        return obs, info\n\tclass MaxAndSkipEnv(gym.Wrapper):\n\t    def __init__(self, env, skip=4):\n", "        \"\"\"Return only every `skip`-th frame\"\"\"\n\t        gym.Wrapper.__init__(self, env)\n\t        # most recent raw observations (for max pooling across time steps)\n\t        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n\t        self._skip       = skip\n\t    def step(self, action):\n\t        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n\t        total_reward = 0.0\n\t        done = None\n\t        for i in range(self._skip):\n", "            obs, reward, terminated, truncated, info = self.env.step(action)\n\t            done = terminated or truncated\n\t            if i == self._skip - 2: self._obs_buffer[0] = obs\n\t            if i == self._skip - 1: self._obs_buffer[1] = obs\n\t            total_reward += reward\n\t            if done:\n\t                break\n\t        # Note that the observation on the done=True frame\n\t        # doesn't matter\n\t        max_frame = self._obs_buffer.max(axis=0)\n", "        return max_frame, total_reward, terminated, truncated, info\n\t    def reset(self, **kwargs):\n\t        return self.env.reset(**kwargs)\n\tclass ClipRewardEnv(gym.RewardWrapper):\n\t    def __init__(self, env):\n\t        gym.RewardWrapper.__init__(self, env)\n\t        return\n\t    def reward(self, reward):\n\t        unclipped_reward = reward\n\t        clipped_reward = np.clip(reward, -1, 1)\n", "        return (unclipped_reward, clipped_reward)\n\tclass WarpFrame(gym.ObservationWrapper):\n\t    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n\t        \"\"\"\n\t        Warp frames to 84x84 as done in the Nature paper and later work.\n\t        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n\t        observation should be warped.\n\t        \"\"\"\n\t        super().__init__(env)\n\t        self._width = width\n", "        self._height = height\n\t        self._grayscale = grayscale\n\t        self._key = dict_space_key\n\t        if self._grayscale:\n\t            num_colors = 1\n\t        else:\n\t            num_colors = 3\n\t        new_space = gym.spaces.Box(\n\t            low=0,\n\t            high=255,\n", "            shape=(self._height, self._width, num_colors),\n\t            dtype=np.uint8,\n\t        )\n\t        if self._key is None:\n\t            original_space = self.observation_space\n\t            self.observation_space = new_space\n\t        else:\n\t            original_space = self.observation_space.spaces[self._key]\n\t            self.observation_space.spaces[self._key] = new_space\n\t        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n", "    def observation(self, obs):\n\t        if self._key is None:\n\t            frame = obs\n\t        else:\n\t            frame = obs[self._key]\n\t        if self._grayscale:\n\t            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n\t        frame = cv2.resize(\n\t            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n\t        )\n", "        if self._grayscale:\n\t            frame = np.expand_dims(frame, -1)\n\t        if self._key is None:\n\t            obs = frame\n\t        else:\n\t            obs = obs.copy()\n\t            obs[self._key] = frame\n\t        return obs\n\tclass FrameStack(gym.Wrapper):\n\t    def __init__(self, env, k):\n", "        \"\"\"Stack k last frames.\n\t        Returns lazy array, which is much more memory efficient.\n\t        See Also\n\t        --------\n\t        baselines.common.atari_wrappers.LazyFrames\n\t        \"\"\"\n\t        gym.Wrapper.__init__(self, env)\n\t        self.k = k\n\t        self.frames = deque([], maxlen=k)\n\t        shp = env.observation_space.shape\n", "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n\t    def reset(self):\n\t        ob, info = self.env.reset()\n\t        for _ in range(self.k):\n\t            self.frames.append(ob)\n\t        return self._get_ob(), info\n\t    def step(self, action):\n\t        ob, reward, terminated, truncated, info = self.env.step(action)\n\t        self.frames.append(ob)\n\t        return self._get_ob(), reward, terminated, truncated, info\n", "    def _get_ob(self):\n\t        assert len(self.frames) == self.k\n\t        return LazyFrames(list(self.frames))\n\tclass ScaledFloatFrame(gym.ObservationWrapper):\n\t    def __init__(self, env):\n\t        gym.ObservationWrapper.__init__(self, env)\n\t        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n\t    def observation(self, observation):\n\t        # careful! This undoes the memory optimization, use\n\t        # with smaller replay buffers only.\n", "        return np.array(observation).astype(np.float32) / 255.0\n\tclass LazyFrames(object):\n\t    def __init__(self, frames):\n\t        \"\"\"This object ensures that common frames between the observations are only stored once.\n\t        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n\t        buffers.\n\t        This object should only be converted to numpy array before being passed to the model.\n\t        You'd not believe how complex the previous solution was.\"\"\"\n\t        self._frames = frames\n\t        self._out = None\n", "    def _force(self):\n\t        if self._out is None:\n\t            self._out = np.concatenate(self._frames, axis=-1)\n\t            self._frames = None\n\t        return self._out\n\t    def __array__(self, dtype=None):\n\t        out = self._force()\n\t        if dtype is not None:\n\t            out = out.astype(dtype)\n\t        return out\n", "    def __len__(self):\n\t        return len(self._force())\n\t    def __getitem__(self, i):\n\t        return self._force()[i]\n\t    def count(self):\n\t        frames = self._force()\n\t        return frames.shape[frames.ndim - 1]\n\t    def frame(self, i):\n\t        return self._force()[..., i]\n\tdef make_atari(env_id, max_episode_steps=None, visualize=False):\n", "    if (visualize):\n\t        render_mode = \"human\"\n\t    else:\n\t        render_mode = None\n\t    env = gym.make(env_id, render_mode=render_mode)\n\t    assert 'NoFrameskip' in env.spec.id\n\t    env = NoopResetEnv(env, noop_max=30)\n\t    env = MaxAndSkipEnv(env, skip=4)\n\t    if max_episode_steps is not None:\n\t        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n", "    return env\n\tdef wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, warp_frame=True, scale=False):\n\t    \"\"\"Configure environment for DeepMind-style Atari.\n\t    \"\"\"\n\t    if episode_life:\n\t        env = EpisodicLifeEnv(env)\n\t    if 'FIRE' in env.unwrapped.get_action_meanings():\n\t        env = FireResetEnv(env)\n\t    if warp_frame:\n\t        env = WarpFrame(env)\n", "    if scale:\n\t        env = ScaledFloatFrame(env)\n\t    if clip_rewards:\n\t        env = ClipRewardEnv(env)\n\t    if frame_stack:\n\t        env = FrameStack(env, 4)\n\t    return env\n"]}
{"filename": "a2/pg_agent.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport envs.base_env as base_env\n\timport learning.base_agent as base_agent\n\timport learning.pg_model as pg_model\n\timport util.torch_util as torch_util\n\tclass PGAgent(base_agent.BaseAgent):\n\t    NAME = \"PG\"\n\t    def __init__(self, config, env, device):\n\t        super().__init__(config, env, device)\n", "        return\n\t    def _load_params(self, config):\n\t        super()._load_params(config)\n\t        self._batch_size = config[\"batch_size\"]\n\t        self._critic_update_epoch = config[\"critic_update_epoch\"]\n\t        self._norm_adv_clip = config[\"norm_adv_clip\"]\n\t        self._action_bound_weight = config[\"action_bound_weight\"]\n\t        return\n\t    def _build_model(self, config):\n\t        model_config = config[\"model\"]\n", "        self._model = pg_model.PGModel(model_config, self._env)\n\t        return\n\t    def _get_exp_buffer_length(self):\n\t        return self._steps_per_iter\n\t    def _build_exp_buffer(self, config):\n\t        super()._build_exp_buffer(config)\n\t        buffer_length = self._get_exp_buffer_length()\n\t        tar_val_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n\t        self._exp_buffer.add_buffer(\"tar_val\", tar_val_buffer)\n\t        adv_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n", "        self._exp_buffer.add_buffer(\"adv\", adv_buffer)\n\t        norm_obs_buffer = torch.zeros_like(self._exp_buffer.get_data(\"obs\"))\n\t        self._exp_buffer.add_buffer(\"norm_obs\", norm_obs_buffer)\n\t        norm_action_buffer = torch.zeros_like(self._exp_buffer.get_data(\"action\"))\n\t        self._exp_buffer.add_buffer(\"norm_action\", norm_action_buffer)\n\t        return\n\t    def _init_iter(self):\n\t        super()._init_iter()\n\t        self._exp_buffer.reset()\n\t        return\n", "    def _build_optimizer(self, config):\n\t        actor_lr = float(config[\"actor_learning_rate\"])\n\t        critic_lr = float(config[\"critic_learning_rate\"])\n\t        actor_params = list(self._model._actor_layers.parameters())+list(self._model._action_dist.parameters())\n\t        actor_params_grad = [p for p in actor_params if p.requires_grad]\n\t        self._actor_optimizer = torch.optim.SGD(actor_params_grad, actor_lr, momentum=0.9)\n\t        critic_params = list(self._model._critic_layers.parameters())+list(self._model._critic_out.parameters())\n\t        critic_params_grad = [p for p in critic_params if p.requires_grad]\n\t        self._critic_optimizer = torch.optim.SGD(critic_params_grad, critic_lr, momentum=0.9)\n\t        return\n", "    def _decide_action(self, obs, info):\n\t        norm_obs = self._obs_norm.normalize(obs)\n\t        norm_action_dist = self._model.eval_actor(norm_obs)\n\t        if (self._mode == base_agent.AgentMode.TRAIN):\n\t            norm_a = norm_action_dist.sample()\n\t        elif (self._mode == base_agent.AgentMode.TEST):\n\t            norm_a = norm_action_dist.mode\n\t        else:\n\t            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n\t        norm_a = norm_a.detach()\n", "        a = self._a_norm.unnormalize(norm_a)\n\t        info = dict()\n\t        return a, info\n\t    def _build_train_data(self):\n\t        self.eval()\n\t        obs = self._exp_buffer.get_data(\"obs\")\n\t        r = self._exp_buffer.get_data(\"reward\")\n\t        done = self._exp_buffer.get_data(\"done\")\n\t        action = self._exp_buffer.get_data(\"action\")\n\t        norm_action = self._a_norm.normalize(action)\n", "        norm_obs = self._obs_norm.normalize(obs)\n\t        ret = self._calc_return(r, done)\n\t        adv = self._calc_adv(norm_obs, ret)\n\t        adv_std, adv_mean = torch.std_mean(adv)\n\t        norm_adv = (adv - adv_mean) / torch.clamp_min(adv_std, 1e-5)\n\t        norm_adv = torch.clamp(norm_adv, -self._norm_adv_clip, self._norm_adv_clip)\n\t        self._exp_buffer.set_data(\"tar_val\", ret)\n\t        self._exp_buffer.set_data(\"adv\", norm_adv)\n\t        self._exp_buffer.set_data(\"norm_obs\", norm_obs)\n\t        self._exp_buffer.set_data(\"norm_action\", norm_action)\n", "        info = {\n\t            \"adv_mean\": adv_mean,\n\t            \"adv_std\": adv_std\n\t        }\n\t        return info\n\t    def _update_model(self):\n\t        self.train()\n\t        num_samples = self._exp_buffer.get_sample_count()\n\t        batch_size = self._batch_size \n\t        num_batches = int(np.ceil(float(num_samples) / batch_size))\n", "        train_info = dict()\n\t        for e in range(self._critic_update_epoch):   \n\t            for b in range(num_batches):\n\t                batch = self._exp_buffer.sample(batch_size)\n\t                critic_info = self._update_critic(batch)              \n\t                torch_util.add_torch_dict(critic_info, train_info)\n\t        torch_util.scale_torch_dict(1.0 / num_batches, train_info)\n\t        actor_batch = {\n\t            \"norm_obs\": self._exp_buffer.get_data(\"norm_obs\"),\n\t            \"norm_action\": self._exp_buffer.get_data(\"norm_action\"),\n", "            \"adv\": self._exp_buffer.get_data(\"adv\")\n\t        }\n\t        actor_info = self._update_actor(actor_batch)\n\t        for key, data in actor_info.items():\n\t            train_info[key] = data\n\t        return train_info\n\t    def _update_critic(self, batch):\n\t        norm_obs = batch[\"norm_obs\"]\n\t        tar_val = batch[\"tar_val\"]\n\t        loss = self._calc_critic_loss(norm_obs, tar_val)\n", "        self._critic_optimizer.zero_grad()\n\t        loss.backward()\n\t        self._critic_optimizer.step()\n\t        info = {\n\t            \"critic_loss\": loss\n\t        }\n\t        return info\n\t    def _update_actor(self, batch):\n\t        norm_obs = batch[\"norm_obs\"]\n\t        norm_a = batch[\"norm_action\"]\n", "        adv = batch[\"adv\"]\n\t        loss = self._calc_actor_loss(norm_obs, norm_a, adv)\n\t        info = {\n\t            \"actor_loss\": loss\n\t        }\n\t        if (self._action_bound_weight != 0):\n\t            a_dist = self._model.eval_actor(norm_obs)\n\t            action_bound_loss = self._compute_action_bound_loss(a_dist)\n\t            if (action_bound_loss is not None):\n\t                action_bound_loss = torch.mean(action_bound_loss)\n", "                loss += self._action_bound_weight * action_bound_loss\n\t                info[\"action_bound_loss\"] = action_bound_loss.detach()\n\t        self._actor_optimizer.zero_grad()\n\t        loss.backward()\n\t        self._actor_optimizer.step()\n\t        return info\n\t    def _calc_return(self, r, done):\n\t        '''\n\t        TODO 2.1: Given a tensor of per-timestep rewards (r), and a tensor (done)\n\t        indicating if a timestep is the last timestep of an episode, Output a\n", "        tensor (return_t) containing the return (i.e. reward-to-go) at each timestep.\n\t        '''\n\t        # placeholder\n\t        return_t = torch.zeros_like(r)\n\t        return return_t\n\t    def _calc_adv(self, norm_obs, ret):\n\t        '''\n\t        TODO 2.2: Given the normalized observations (norm_obs) and the return at\n\t        every timestep (ret), output the advantage at each timestep (adv).\n\t        '''\n", "        # placeholder\n\t        adv = torch.zeros_like(ret)\n\t        return adv\n\t    def _calc_critic_loss(self, norm_obs, tar_val):\n\t        '''\n\t        TODO 2.3: Given the normalized observations (norm_obs) and the returns at\n\t        every timestep (tar_val), compute a loss for updating the value\n\t        function (critic).\n\t        '''\n\t        # placeholder\n", "        loss = torch.zeros(1)\n\t        return loss\n\t    def _calc_actor_loss(self, norm_obs, norm_a, adv):\n\t        '''\n\t        TODO 2.4: Given the normalized observations (norm_obs), normalized\n\t        actions (norm_a), and the advantage at every timestep (adv), compute\n\t        a loss for updating the policy (actor).\n\t        '''\n\t        # placeholder\n\t        loss = torch.zeros(1)\n", "        return loss"]}
{"filename": "a2/cem_agent.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport learning.base_agent as base_agent\n\timport learning.cem_model as cem_model\n\tclass CEMAgent(base_agent.BaseAgent):\n\t    NAME = \"CEM\"\n\t    def __init__(self, config, env, device):\n\t        super().__init__(config, env, device)\n\t        self._param_mean = None\n\t        self._param_std = None\n", "        self._best_return = None\n\t        self._best_params = None\n\t        return\n\t    def _load_params(self, config):\n\t        super()._load_params(config)\n\t        self._population_size = config[\"population_size\"]\n\t        self._elite_ratio = config[\"elite_ratio\"]\n\t        self._eps_per_candidate = config[\"eps_per_candidate\"]\n\t        self._min_param_std = config[\"min_param_std\"]\n\t        return\n", "    def _build_optimizer(self, config):\n\t        return\n\t    def _init_train(self):\n\t        super()._init_train()\n\t        self._param_mean = torch.nn.utils.parameters_to_vector(self._model.parameters())\n\t        self._param_std = 0.5 * torch.ones_like(self._param_mean)\n\t        self._best_return = None\n\t        self._best_params = None\n\t        return\n\t    def _build_model(self, config):\n", "        model_config = config[\"model\"]\n\t        self._model = cem_model.CEMModel(model_config, self._env)\n\t        return\n\t    def _decide_action(self, obs, info):\n\t        norm_obs = self._obs_norm.normalize(obs)\n\t        norm_action_dist = self._model.eval_actor(norm_obs)\n\t        norm_a = norm_action_dist.mode\n\t        norm_a = norm_a.detach()\n\t        a = self._a_norm.unnormalize(norm_a)\n\t        info = dict()\n", "        return a, info\n\t    def _update_sample_count(self):\n\t        return self._sample_count\n\t    def _train_iter(self):\n\t        candidates = self._sample_candidates(self._population_size)\n\t        rets, ep_lens = self._eval_candidates(candidates)\n\t        curr_best_idx = np.argmax(rets)\n\t        curr_best_ret = rets[curr_best_idx]\n\t        if ((self._best_params is None) or (curr_best_ret > self._best_return)):\n\t            self._best_params = torch.clone(candidates[curr_best_idx])\n", "            self._best_return = curr_best_ret\n\t        num_samples = self._eps_per_candidate * np.sum(ep_lens)\n\t        self._sample_count += num_samples\n\t        new_mean, new_std = self._compute_new_params(candidates, rets)\n\t        self._param_mean[:] = new_mean\n\t        self._param_std[:] = new_std\n\t        torch.nn.utils.vector_to_parameters(self._best_params, self._model.parameters())\n\t        train_return = np.mean(rets)\n\t        train_ep_len = np.mean(ep_lens)\n\t        num_eps = self._population_size * self._eps_per_candidate\n", "        mean_param_std = torch.mean(new_std)\n\t        train_info = {\n\t            \"mean_return\": train_return,\n\t            \"mean_ep_len\": train_ep_len,\n\t            \"episodes\": num_eps,\n\t            \"param_std\": mean_param_std\n\t        }\n\t        return train_info\n\t    def _sample_candidates(self, n):\n\t        '''\n", "        TODO 1.1: Sample n set of candidate parameters from the current search\n\t        distribution. The search distribution is a guassian distribution with mean\n\t        self._param_mean and standard deviation self._param_std. Output a tensor\n\t        containing parameters for each candidate. The tensor should have dimensions\n\t        [n, param_size].\n\t        '''\n\t        param_size = self._param_mean.shape[0]\n\t        # placeholder\n\t        candidates = torch.zeros([n, param_size], device=self._device)\n\t        return candidates\n", "    def _eval_candidates(self, candidates):\n\t        '''\n\t        TODO 1.2: Evaluate the performance of a set of candidate parameters.\n\t        You can use torch.nn.utils.vector_to_parameters to copy a candidate's\n\t        parameters to the model for the policy (self._model). self._rollout_test\n\t        can then be used to evaluate the performance of that set of parameters.\n\t        Record the average return and average episode length of each candidate\n\t        in the output variables rets and ep_lens.\n\t        '''\n\t        n = candidates.shape[0]\n", "        # placeholder\n\t        rets = np.zeros(n)\n\t        ep_lens = np.zeros(n)\n\t        return rets, ep_lens\n\t    def _compute_new_params(self, params, rets):\n\t        '''\n\t        TODO 1.3: Update the search distribution given a set of candidate\n\t        parameters (params) and their corresponding performance (rets).\n\t        Return the mean (new_mean) and standard deviation (new_std) of\n\t        the new search distribution.\n", "        '''\n\t        param_size = self._param_mean.shape[0]\n\t        # placeholder\n\t        new_mean = torch.zeros(param_size, device=self._device)\n\t        new_std = torch.ones(param_size, device=self._device)\n\t        return new_mean, new_std\n"]}
{"filename": "a1/bc_agent.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport learning.agent_builder as agent_builder\n\timport learning.base_agent as base_agent\n\timport learning.bc_model as bc_model\n\timport util.torch_util as torch_util\n\tclass BCAgent(base_agent.BaseAgent):\n\t    NAME = \"BC\"\n\t    def __init__(self, config, env, device):\n\t        super().__init__(config, env, device)\n", "        return\n\t    def _load_params(self, config):\n\t        super()._load_params(config)\n\t        buffer_size = config[\"exp_buffer_size\"]\n\t        self._exp_buffer_length = max(buffer_size, self._steps_per_iter)\n\t        self._batch_size = config[\"batch_size\"]\n\t        self._update_epochs = config[\"update_epochs\"]\n\t        return\n\t    def _build_model(self, config):\n\t        model_config = config[\"model\"]\n", "        self._model = bc_model.BCModel(model_config, self._env)\n\t        self._build_expert(config)\n\t        self._sync_normalizers()\n\t        return    \n\t    def _build_expert(self, config):\n\t        expert_config = config[\"expert_config\"]\n\t        expert = agent_builder.build_agent(expert_config, self._env, self._device)\n\t        expert_model_file = config[\"expert_model_file\"]\n\t        assert(expert_model_file is not None)\n\t        expert.load(expert_model_file)\n", "        expert.set_mode(base_agent.AgentMode.TEST)\n\t        # putting the expert in a list makes the expert's parameters invisible to the pytorch module\n\t        self._experts = [expert]\n\t        return\n\t    def _sync_normalizers(self):\n\t        expert = self._experts[0]\n\t        self._obs_norm.load_state_dict(expert._obs_norm.state_dict())\n\t        self._a_norm.load_state_dict(expert._a_norm.state_dict())\n\t        return\n\t    def _get_exp_buffer_length(self):\n", "        return self._exp_buffer_length\n\t    def _need_normalizer_update(self):\n\t        return False\n\t    def _build_exp_buffer(self, config):\n\t        super()._build_exp_buffer(config)\n\t        expert_a_buffer = torch.zeros_like(self._exp_buffer.get_data(\"action\"))\n\t        self._exp_buffer.add_buffer(\"expert_a\", expert_a_buffer)\n\t        return\n\t    def _record_data_pre_step(self, obs, info, action, action_info):\n\t        super()._record_data_pre_step(obs, info, action, action_info)\n", "        self._exp_buffer.record(\"expert_a\", action_info[\"expert_a\"])\n\t        return\n\t    def _update_model(self):\n\t        self.train()\n\t        num_samples = self._exp_buffer.get_sample_count()\n\t        batch_size = self._batch_size \n\t        num_batches = int(np.ceil(float(num_samples) / batch_size))\n\t        train_info = dict()\n\t        for i in range(self._update_epochs):\n\t            for b in range(num_batches):\n", "                batch = self._exp_buffer.sample(batch_size)\n\t                loss_info = self._compute_loss(batch)\n\t                self._optimizer.zero_grad()\n\t                loss = loss_info[\"loss\"]\n\t                loss.backward()\n\t                self._optimizer.step()\n\t                torch_util.add_torch_dict(loss_info, train_info)\n\t        num_steps = self._update_epochs * num_batches\n\t        torch_util.scale_torch_dict(1.0 / num_steps, train_info)\n\t        return train_info        \n", "    def _compute_loss(self, batch):\n\t        norm_obs = self._obs_norm.normalize(batch[\"obs\"])\n\t        norm_expert_a = self._a_norm.normalize(batch[\"expert_a\"])\n\t        actor_loss = self._compute_actor_loss(norm_obs, norm_expert_a)\n\t        info = {\n\t            \"loss\": actor_loss\n\t        }\n\t        return info\n\t    def _eval_expert(self, obs):\n\t        info = None\n", "        expert = self._experts[0]\n\t        expert_a, _ = expert._decide_action(obs, info)\n\t        return expert_a\n\t    def _decide_action(self, obs, info):\n\t        '''\n\t        TODO 1.1: Implement code for sampling from the policy and\n\t        querying the expert policy for the expert actions.\n\t        '''\n\t        ## a) sample an action from the policy\n\t        # placeholder\n", "        a_space = self._env.get_action_space()\n\t        a = torch.zeros(a_space.shape, device=self._device)\n\t        ## b) query the expert for an action\n\t        # placeholder\n\t        a_space = self._env.get_action_space()\n\t        expert_a = torch.zeros(a_space.shape, device=self._device)\n\t        a_info = {\n\t            \"expert_a\": expert_a\n\t        }\n\t        return a, a_info\n", "    def _compute_actor_loss(self, norm_obs, norm_expert_a):\n\t        '''\n\t        TODO 1.2: Implement code to calculate the loss for training the policy.\n\t        '''\n\t        # placeholder\n\t        loss = torch.zeros(1)\n\t        return loss\n"]}
