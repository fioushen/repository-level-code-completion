{"filename": "cjjpy.py", "chunked_list": ["ï»¿# -*- coding: utf-8 -*-\n\t'''\n\t@Author : Jiangjie Chen\n\t@Time   : 2022/5/26 19:52\n\t@Contact: jjchen19@fudan.edu.cn\n\t'''\n\timport re\n\timport datetime\n\timport os\n\timport subprocess\n", "import urllib.request, urllib.parse\n\timport argparse\n\tfrom tqdm import tqdm\n\timport sqlite3\n\timport requests\n\timport socket\n\timport logging\n\timport io\n\timport traceback\n\ttry:\n", "    import ujson as json\n\texcept:\n\t    import json\n\tHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\tdef LengthStats(filename, key4json=None):\n\t    len_list = []\n\t    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n\t    with open(filename) as f:\n\t        for line in f:\n\t            if key4json not in ['none', None, 'None']:\n", "                len_list.append(len(json.loads(line)[key4json].split()))\n\t            else:\n\t                len_list.append(len(line.strip().split()))\n\t    stats = {\n\t        'Max': max(len_list),\n\t        'Min': min(len_list),\n\t        'Avg': round(sum(len_list) / len(len_list), 4),\n\t    }\n\t    len_list.sort()\n\t    for t in thresholds:\n", "        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\t    for k in stats:\n\t        print(f\"- {k}: {stats[k]}\")\n\t    return stats\n\tclass AttrDict(dict):\n\t    def __init__(self, *args, **kwargs):\n\t        super(AttrDict, self).__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\tdef TraceBack(error_msg):\n\t    exc = traceback.format_exc()\n", "    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n\t    return msg\n\tdef Now():\n\t    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\tdef TorchHLoad(filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if not filepath.startswith(\"hdfs://\"):\n\t        return torch.load(filepath, **kwargs)\n\t    else:\n\t        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n", "            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\tdef TorchHSave(obj, filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n\t        with tf.io.gfile.GFile(filepath, 'wb') as f:\n\t            buffer = io.BytesIO()\n\t            torch.save(obj, buffer, **kwargs)\n\t            f.write(buffer.getvalue())\n\t    else:\n\t        torch.save(obj, filepath, **kwargs)\n", "def PutHDFS(local: str, remote: str):\n\t    import tensorflow as tf\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    if not tf.io.gfile.exists(remote):\n\t        tf.io.gfile.makedirs(remote)\n\t    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\tdef GetHDFS(remote: str, local: str):\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    os.makedirs(local, exist_ok=True)\n\t    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n", "def RunCmd(command):\n\t    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t    res, err = pipe.communicate()\n\t    res = res.decode('utf-8')\n\t    err = err.decode('utf-8')\n\t    return res, err\n\tdef AbsParentDir(file, parent='..', postfix=None):\n\t    ppath = os.path.abspath(file)\n\t    parent_level = parent.count('.')\n\t    while parent_level > 0:\n", "        ppath = os.path.dirname(ppath)\n\t        parent_level -= 1\n\t    if postfix is not None:\n\t        return os.path.join(ppath, postfix)\n\t    else:\n\t        return ppath\n\tdef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n\t    from coloredlogs import ColoredFormatter\n\t    import tensorflow as tf\n\t    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n", "    log_format = ColoredFormatter(fmt=fmt)\n\t    # log_format = logging.Formatter()\n\t    logger = logging.getLogger()\n\t    logger.setLevel(log_file_level)\n\t    console_handler = logging.StreamHandler()\n\t    console_handler.setFormatter(log_format)\n\t    logger.handlers = [console_handler]\n\t    if log_file and log_file != '':\n\t        if from_scratch and tf.io.gfile.exists(log_file):\n\t            logger.warning('Removing previous log file: %s' % log_file)\n", "            tf.io.gfile.remove(log_file)\n\t        path = os.path.dirname(log_file)\n\t        os.makedirs(path, exist_ok=True)\n\t        file_handler = logging.FileHandler(log_file)\n\t        file_handler.setLevel(log_file_level)\n\t        file_handler.setFormatter(log_format)\n\t        logger.addHandler(file_handler)\n\t    return logger\n\tdef is_qid(x, hard=False):\n\t    if type(x) is not str: return None\n", "    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tdef is_pid(x, hard=False):\n\t    if type(x) is not str: return None\n\t    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tclass MiniLutDB:\n\t    def __init__(self, db, verbose=True):\n\t        self.db = db\n\t        self.conn = None\n", "        self.verbose = verbose\n\t    def dump_lut(self, lut_tuples, verbose=None):\n\t        # lut_tuple: (k, v)+, iterable\n\t        if verbose is None: \n\t            verbose = self.verbose\n\t        self.conn = sqlite3.connect(self.db)\n\t        cur = self.conn.cursor()\n\t        cur.executescript('''\n\t        DROP TABLE IF EXISTS lut;\n\t        CREATE TABLE lut (\n", "        id      TEXT PRIMARY KEY UNIQUE,\n\t        content TEXT)''')\n\t        self.conn.commit()\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n", "            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def update_lut(self, lut_tuples, verbose=None):\n\t        if verbose is None: \n\t            verbose = self.verbose\n", "        self.conn = sqlite3.connect(self.db)\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n\t            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n", "                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def create_index(self):\n\t        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        # sql = ('CREATE INDEX index_lut ON lut(id);')\n\t        # self.cur.execute(sql)\n", "        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n\t        self.conn.commit()\n\t    def get(self, x, default=None):\n\t        if x is None: return default\n\t        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        res = self.query_lut(self.cur, x, False)[0]\n\t        return res if res is not None else default\n\t    def get_chunk(self, xx):\n", "        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        return self.query_lut(self.conn, xx, self.verbose)\n\t    def close(self):\n\t        if self.conn is not None:\n\t            self.conn.close()\n\t            self.conn = None\n\t    def delete_sample(self, key, value=None):\n\t        if self.get(key) is None: return\n", "        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n\t        self.conn.commit()\n\t        assert self.get(key) is None, f'delete failed: {key}'\n\t    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n\t        values = []\n\t        if isinstance(keys, str): keys = [keys]\n\t        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n\t        for k in iter:\n", "            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n\t            val = cur.fetchone()\n\t            val = val[0] if val is not None else None\n\t            values.append(val)\n\t        return values\n\tdef OverWriteCjjPy(root='.'):\n\t    # import difflib\n\t    # diff = difflib.HtmlDiff()\n\t    cnt = 0\n\t    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n", "    # golden_content = open(golden_cjjpy).readlines()\n\t    for dir, folder, file in os.walk(root):\n\t        for f in file:\n\t            if f == 'cjjpy.py':\n\t                cjjpy = '%s/%s' % (dir, f)\n\t                # content = open(cjjpy).readlines()\n\t                # d = diff.make_file(golden_content, content)\n\t                cnt += 1\n\t                print('[%d]: %s' % (cnt, cjjpy))\n\t                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n", "def ReplaceChar(file, replaced, replacer):\n\t    print(file, replaced, replacer)\n\t    with open(file) as f:\n\t        data = f.readlines()\n\t        out = open(file, 'w')\n\t        for line in data:\n\t            out.write(line.replace(replaced, replacer))\n\tdef DeUnicode(line):\n\t    return line.encode('utf-8').decode('unicode_escape')\n\tdef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n", "    '''\n\t    a\\tb\\n, `.dict' file\n\t    '''\n\t    import tensorflow as tf\n\t    assert dict_file.endswith('.dict')\n\t    id2label = {}\n\t    with tf.io.gfile.GFile(dict_file, 'r') as f:\n\t        data = f.read().split('\\n')\n\t        for i, line in enumerate(data):\n\t            if line == '': continue\n", "            try:\n\t                id, label = line.split('\\t')\n\t                if reverse:\n\t                    id, label = label, id\n\t                _val = '_'.join(label.split()) if unify_words else label\n\t                id2label[id] = _val.lower() if lower else _val\n\t            except:\n\t                pass\n\t    return id2label\n\tdef LoadWords(file, is_file=True):\n", "    import tensorflow as tf\n\t    if is_file:\n\t        with tf.io.gfile.GFile(file, 'r') as f:\n\t            data = f.read().splitlines()\n\t    else:\n\t        data = file.splitlines()\n\t    return set(map(lambda x: x.strip(), data))\n\tdef ChangeFileFormat(filename, new_fmt):\n\t    assert type(filename) is str and type(new_fmt) is str\n\t    spt = filename.split('.')\n", "    if len(spt) == 0:\n\t        return filename\n\t    else:\n\t        return filename.replace('.' + spt[-1], new_fmt)\n\tdef CountLines(fname):\n\t    with open(fname, 'rb') as f:\n\t        count = 0\n\t        last_data = '\\n'\n\t        while True:\n\t            data = f.read(0x400000)\n", "            if not data:\n\t                break\n\t            count += data.count(b'\\n')\n\t            last_data = data\n\t        if last_data[-1:] != b'\\n':\n\t            count += 1  # Remove this if a wc-like count is needed\n\t    return count\n\tdef SearchByKey(file, key):\n\t    with open(file, 'r') as fin:\n\t        while True:\n", "            line = fin.readline()\n\t            if not line: break\n\t            if key in line:\n\t                print(line, end='')\n\tdef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n\t    from email.mime.text import MIMEText\n\t    import smtplib\n\t    # receivers got to be list.\n\t    mail_receivers = receivers\n\t    # mail_host = \"smtp.163.com\n", "    mail_host = \"220.181.12.18\"\n\t    mail_user = \"MichaelChen0110@163.com\"\n\t    mail_pass = \"\"\n\t    me = socket.gethostname() + \"<\" + mail_user + \">\"\n\t    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n\t    msg['Subject'] = subject\n\t    msg['From'] = me\n\t    msg['To'] = \";\".join(mail_receivers)\n\t    try:\n\t        server = smtplib.SMTP()\n", "        server.connect(mail_host)\n\t        server.login(mail_user, mail_pass)\n\t        server.sendmail(me, mail_receivers, msg.as_string())\n\t        server.close()\n\t        print('Have sent the email to ' + str(mail_receivers) + '. ')\n\t        return True\n\t    except Exception as e:\n\t        print(str(e))\n\t        return False\n\tdef SortDict(_dict, reverse=True):\n", "    assert type(_dict) is dict\n\t    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\tdef MaxCommLen(str1, str2):\n\t    lstr1 = len(str1)\n\t    lstr2 = len(str2)\n\t    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n\t    max_num = 0\n\t    for i in range(lstr1):\n\t        for j in range(lstr2):\n\t            if str1[i] == str2[j]:\n", "                record[i + 1][j + 1] = record[i][j] + 1\n\t                if record[i + 1][j + 1] > max_num:\n\t                    max_num = record[i + 1][j + 1]\n\t    return max_num, ''\n\tdef lark(content='test'):\n\t    print(content)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--diff', nargs=2,\n\t                        help='show difference between two files, shown in downloads/diff.html')\n", "    parser.add_argument('--de_unicode', action='store_true', default=False,\n\t                        help='remove unicode characters')\n\t    parser.add_argument('--link_entity', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--max_comm_len', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--search', nargs=2,\n\t                        help='search key from file, 2 args: file name & key')\n\t    parser.add_argument('--email', nargs=2,\n\t                        help='sending emails, 2 args: subject & content')\n", "    parser.add_argument('--overwrite', action='store_true', default=None,\n\t                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n\t    parser.add_argument('--replace', nargs=3,\n\t                        help='replace char, 3 args: file name & replaced char & replacer char')\n\t    parser.add_argument('--lark', nargs=1)\n\t    parser.add_argument('--get_hdfs', nargs=2,\n\t                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n\t    parser.add_argument('--put_hdfs', nargs=2,\n\t                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n\t    parser.add_argument('--length_stats', nargs=2,\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\t    args = parser.parse_args()\n\t    if args.overwrite:\n\t        print('* Overwriting cjjpy...')\n\t        OverWriteCjjPy()\n\t    if args.replace:\n\t        print('* Replacing Char...')\n\t        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\t    if args.search:\n\t        file = args.search[0]\n", "        key = args.search[1]\n\t        print('* Searching %s from %s...' % (key, file))\n\t        SearchByKey(file, key)\n\t    if args.email:\n\t        try:\n\t            subj = args.email[0]\n\t            cont = args.email[1]\n\t        except:\n\t            subj = 'running complete'\n\t            cont = ''\n", "        print('* Sending email {%s, %s} to host...' % (subj, cont))\n\t        SendEmail(subj, cont)\n\t    if args.lark:\n\t        try:\n\t            content = args.lark[0]\n\t        except:\n\t            content = 'running complete'\n\t        print(f'* Larking \"{content}\"...')\n\t        lark(content)\n\t    if args.get_hdfs:\n", "        remote = args.get_hdfs[0]\n\t        local = args.get_hdfs[1]\n\t        print(f'* Copying {remote} to {local}...')\n\t        GetHDFS(remote, local)\n\t    if args.put_hdfs:\n\t        local = args.put_hdfs[0]\n\t        remote = args.put_hdfs[1]\n\t        print(f'* Copying {local} to {remote}...')\n\t        PutHDFS(local, remote)\n\t    if args.length_stats:\n", "        file = args.length_stats[0]\n\t        key4json = args.length_stats[1]\n\t        print(f'* Working on {file} lengths statistics...')\n\t        LengthStats(file, key4json)\n"]}
{"filename": "flant5_helper.py", "chunked_list": ["# from transformers import (\n\t#     AutoTokenizer,\n\t#     AutoModelForSeq2SeqLM,\n\t#     DataCollatorForSeq2Seq,\n\t# )\n\t# from datasets import Dataset\n\t# from torch.utils.data import DataLoader\n\t# import torch\n\t# from tqdm import tqdm\n\tfrom base_generator import Seq2SeqBaseGenerator\n", "def flatten_list(chunk_list):\n\t    for chunk in chunk_list:\n\t        if isinstance(chunk, list):\n\t            yield from flatten_list(chunk)\n\t        else:\n\t            yield chunk\n\tdef chunks(lst, n):\n\t    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n\t    for i in range(0, len(lst), n):\n\t        yield lst[i: i + n]\n", "def prompt_flant5(prompt_input: list, model_name='flan-t5-large', max_tokens=128,\n\t                  clean=False, batch_size=16, verbose=False, **kwargs):\n\t    _model_name_or_path = 'google/' + model_name if model_name.startswith('flan-t5') else model_name\n\t    flant5 = Seq2SeqBaseGenerator(_model_name_or_path)\n\t    outputs = flant5.generate(prompt_input,\n\t                              num_return_sequences=kwargs.get('n', 1),\n\t                              beam_size=kwargs.get('n', 1),\n\t                              temperature=kwargs.get('temperature', 0),\n\t                              max_new_tokens=max_tokens,\n\t                              per_device_test_batch_size=batch_size,\n", "                              verbose=verbose,)\n\t    return flatten_list(outputs)\n\t    # tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-xl')\n\t    # model = AutoModelForSeq2SeqLM.from_pretrained(_model_name_or_path, device_map='auto')\n\t    # data_collator = DataCollatorForSeq2Seq(\n\t    #     tokenizer,\n\t    #     model=model,\n\t    #     label_pad_token_id=-100,\n\t    #     pad_to_multiple_of=None,\n\t    # )\n", "    # if isinstance(prompt_input[0], str):\n\t    #     prompt_input = [{'source': x} for x in prompt_input]\n\t    # raw_datasets = Dataset.from_list(prompt_input, split='test')\n\t    # column_names = raw_datasets.column_names\n\t    # def preprocess_function(examples, prefix='', source_column='source'):\n\t    #     padding = 'max_length'\n\t    #     inputs = examples[source_column]\n\t    #     inputs = [prefix + inp for inp in inputs]\n\t    #     model_inputs = tokenizer(inputs, padding=padding, truncation=True)\n\t    #     return model_inputs\n", "    # processed_datasets = raw_datasets.map(\n\t    #     preprocess_function,\n\t    #     batched=True,\n\t    #     num_proc=1,\n\t    #     remove_columns=column_names,\n\t    #     desc=\"Running tokenizer on dataset\",\n\t    # )\n\t    # dataloader = DataLoader(processed_datasets, collate_fn=data_collator, batch_size=batch_size)\n\t    # predictions = []\n\t    # for batch in tqdm(dataloader, total=len(dataloader), disable=not verbose,):\n", "    #     with torch.no_grad():\n\t    #         generated_output = model.generate(\n\t    #             batch[\"input_ids\"],\n\t    #             attention_mask=batch[\"attention_mask\"],\n\t    #             num_beams=kwargs.get('n', 1),\n\t    #             num_return_sequences=kwargs.get('n', 1),\n\t    #             max_new_tokens=max_tokens,\n\t    #             output_scores=True,\n\t    #             return_dict_in_generate=True,\n\t    #             temperature=kwargs.get('temperature', 0)\n", "    #         )\n\t    #         generated_tokens = generated_output.sequences\n\t    #         if isinstance(generated_tokens, tuple):\n\t    #             generated_tokens = generated_tokens[0]\n\t    #         decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\t    #         preds = [pred.strip() for pred in decoded_preds]\n\t    #         predictions += preds\n\t    # predictions = list(map(lambda x: x.strip(), predictions))\n\t    # # TODO: `[:len(test_dataset)]` is a hack to deal with the auto-filling of the last incomplete batch.\n\t    # predictions = list(chunks(predictions, kwargs.get('n', 1)))[:len(processed_datasets)]\n", "    # return flatten_list(predictions)\n\tif __name__ == '__main__':\n\t    res = prompt_flant5([{'source': 'are lions mammal?'}, {'source': 'can birds fly?'}], max_tokens=32)\n\t    print(list(res))"]}
{"filename": "gpt3_helper.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t'''\n\t@Author : Jiangjie Chen\n\t@Time   : 2023/1/2 21:00\n\t@Contact: jjchen19@fudan.edu.cn\n\t'''\n\timport os\n\timport openai\n\timport math\n\timport sys\n", "import time\n\tfrom tqdm import tqdm\n\tfrom typing import Iterable, List, TypeVar\n\tT = TypeVar('T')\n\tKEY_INDEX = 0\n\tKEY_POOL = [os.environ[\"OPENAI_API_KEY\"]] # your key pool\n\topenai.api_key = KEY_POOL[0]\n\tdef batchify(data: Iterable[T], batch_size: int) -> Iterable[List[T]]:\n\t    # function copied from allenai/real-toxicity-prompts\n\t    assert batch_size > 0\n", "    batch = []\n\t    for item in data:\n\t        # Yield next batch\n\t        if len(batch) == batch_size:\n\t            yield batch\n\t            batch = []\n\t        batch.append(item)\n\t    # Yield last un-filled batch\n\t    if len(batch) != 0:\n\t        yield batch\n", "def openai_unit_price(model_name):\n\t    if 'gpt-3.5-turbo' in model_name:\n\t        unit = 0.002\n\t    elif 'davinci' in model_name:\n\t        unit = 0.02\n\t    elif 'curie' in model_name:\n\t        unit = 0.002\n\t    elif 'babbage' in model_name:\n\t        unit = 0.0005\n\t    elif 'ada' in model_name:\n", "        unit = 0.0004\n\t    else:\n\t        unit = -1\n\t    return unit\n\tdef calc_cost_w_tokens(total_tokens: int, model_name: str):\n\t    unit = openai_unit_price(model_name)\n\t    return round(unit * total_tokens / 1000, 2)\n\tdef calc_cost_w_prompt(prompt: str, model_name: str):\n\t    # 750 words == 1000 tokens\n\t    unit = openai_unit_price(model_name)\n", "    return round(len(prompt.split()) / 750 * unit, 2)\n\tdef get_perplexity(logprobs):\n\t    assert len(logprobs) > 0, logprobs\n\t    return math.exp(-sum(logprobs)/len(logprobs))\n\tdef keep_logprobs_before_eos(tokens, logprobs):\n\t    keep_tokens = []\n\t    keep_logprobs = []\n\t    start_flag = False\n\t    for tok, lp in zip(tokens, logprobs):\n\t        if start_flag:\n", "            if tok == \"<|endoftext|>\":\n\t                break\n\t            else:\n\t                keep_tokens.append(tok)\n\t                keep_logprobs.append(lp)\n\t        else:\n\t            if tok != '\\n':\n\t                start_flag = True\n\t                if tok != \"<|endoftext>\":\n\t                    keep_tokens.append(tok)\n", "                    keep_logprobs.append(lp)\n\t    return keep_tokens, keep_logprobs\n\tdef catch_openai_api_error(prompt_input: list):\n\t    global KEY_INDEX\n\t    error = sys.exc_info()[0]\n\t    if error == openai.error.InvalidRequestError:\n\t        # something is wrong: e.g. prompt too long\n\t        print(f\"InvalidRequestError\\nPrompt:\\n\\n{prompt_input}\\n\\n\")\n\t        assert False\n\t    elif error == openai.error.RateLimitError:\n", "        KEY_INDEX = (KEY_INDEX + 1) % len(KEY_POOL)\n\t        openai.api_key = KEY_POOL[KEY_INDEX]\n\t        print(\"RateLimitError, now change the key.\")\n\t    else:\n\t        print(\"API error:\", error)\n\tdef prompt_gpt3(prompt_input: list, model_name='text-davinci-003', max_tokens=128,\n\t                clean=False, batch_size=16, verbose=False, **kwargs):\n\t    # return: output_list, money_cost\n\t    def request_api(prompts: list):\n\t        # prompts: list or str\n", "        while True:\n\t            try:\n\t                return openai.Completion.create(\n\t                    model=model_name,\n\t                    prompt=prompts,\n\t                    max_tokens=max_tokens,\n\t                    # temperature=kwargs.get('temperature', 0.9),\n\t                    # top_p=kwargs.get('top_p', 1),\n\t                    # frequency_penalty=kwargs.get('frequency_penalty', 0),\n\t                    # presence_penalty=kwargs.get('presence_penalty', 0),\n", "                    **kwargs\n\t                )\n\t            except:\n\t                catch_openai_api_error(prompt_input)\n\t                time.sleep(1)\n\t    total_tokens = 0\n\t    results = []\n\t    for batch in tqdm(batchify(prompt_input, batch_size), total=len(prompt_input) // batch_size):\n\t        batch_response = request_api(batch)\n\t        total_tokens += batch_response['usage']['total_tokens']\n", "        if not clean:\n\t            results += batch_response['choices']\n\t        else:\n\t            results += [choice['text'] for choice in batch_response['choices']]\n\t    return results, calc_cost_w_tokens(total_tokens, model_name)\n\tdef prompt_chatgpt(system_input, user_input, history=[], model_name='gpt-3.5-turbo'):\n\t    '''\n\t    :param system_input: \"You are a helpful assistant/translator.\"\n\t    :param user_input: you texts here\n\t    :param history: ends with assistant output.\n", "                    e.g. [{\"role\": \"system\", \"content\": xxx},\n\t                          {\"role\": \"user\": \"content\": xxx},\n\t                          {\"role\": \"assistant\", \"content\": \"xxx\"}]\n\t    return: assistant_output, (updated) history, money cost\n\t    '''\n\t    if len(history) == 0:\n\t        history = [{\"role\": \"system\", \"content\": system_input}]\n\t    history.append({\"role\": \"user\", \"content\": user_input})\n\t    for _ in range(5):\n\t        try:\n", "            completion = openai.ChatCompletion.create(\n\t                model=model_name,\n\t                messages=history\n\t            )\n\t            break\n\t        except:\n\t            catch_openai_api_error()\n\t            time.sleep(1)\n\t    assistant_output = completion['choices'][0]['message']['content']\n\t    history.append({\"role\": \"assistant\", \"content\": assistant_output})\n", "    total_tokens = completion['usage']['total_tokens']\n\t    return assistant_output, history, calc_cost_w_tokens(total_tokens, model_name)\n\tif __name__ == '__main__':\n\t    prompt = 'hello world'\n\t    response = prompt_gpt3([prompt]*4, batch_size=4, clean=True)\n\t    print(response)\n\t    response = prompt_chatgpt('You are a super villian.', 'What is your plan when get caught?')\n\t    print(response)"]}
{"filename": "utils.py", "chunked_list": ["import os\n\timport re\n\timport ujson as json\n\timport cjjpy as cjj\n\tREL_TO_BOOLQ_TEMPLATE = {\n\t    \"IsA\": \"is [w1] a type of [w2]?\",\n\t    'CapableOf': \"can [w1] [w2]?\",\n\t    'UsedFor': \"is [w1] used for [w2]?\",\n\t    \"MadeOf\": \"is [w1] made of [w2]?\",\n\t    'HasProperty': \"does [w1] has the property of [w2]?\",\n", "    'HasSubevent': \"does [w1] have a subevent of [w2]?\",\n\t    \"AtLocation\": \"is [w1] likely to be found in [w2]?\",\n\t    \"PartOf\": \"is [w1] part of [w2]?\",\n\t    \"HasA\": \"does [w1] have [w2]?\",\n\t    # \"ReceivesAction\": \"can [w1] be [w2]?\",\n\t    \"Causes\": \"does [w1] cause [w2]?\",\n\t    # \"HasPrerequisite\": \"in order for [w1] to happen, does [w2] need to happen?\",\n\t    # \"NotCapableOf\": \"is [w1] capable of [w2]?\",\n\t    \"RelatedTo\": \"is [w1] like [w2]?\",\n\t    \"Desires\": \"does [w1] want [w2]?\",\n", "    \"MotivatedByGoal\": \"is [w1] movitated by the goal of [w2]?\",\n\t    # \"NotHasProperty\":  \"does [w1] have the property of [w2]?\",\n\t    \"CreatedBy\": \"is [w1] created by [w2]?\",\n\t    \"CausesDesire\": \"does [w1] make people want [w2]?\",\n\t    # \"NotIsA\": \"is [w1] a type of [w2]?\",\n\t    # \"HasFirstSubevent\": \"is [w2] the first subevent of [w1]?\",\n\t    # \"DefinedAs\": \"is [w1] defined as [w2]?\"\n\t}\n\tUSUALLY_REL_TO_BOOLQ_TEMPLATE = {\n\t    \"IsA\": \"is [w1] a type of [w2]?\",\n", "    'CapableOf': \"can [w1] generally [w2]?\",\n\t    'UsedFor': \"is [w1] generally used for [w2]?\",\n\t    \"MadeOf\": \"is [w1] generally made of [w2]?\",\n\t    'HasProperty': \"does [w1] generally have the property of [w2]?\",\n\t    'HasSubevent': \"does [w1] generally have a subevent of [w2]?\",\n\t    \"AtLocation\": \"is [w1] likely to be found in [w2]?\",\n\t    \"PartOf\": \"is [w1] generally part of [w2]?\",\n\t    \"HasA\": \"does [w1] generally have [w2]?\",\n\t    # \"ReceivesAction\": \"can [w1] generally be [w2]?\",\n\t    \"Causes\": \"does [w1] generally cause [w2]?\",\n", "    # \"HasPrerequisite\": \"in order for [w1] to happen, does [w2] generally need to happen?\",\n\t    # \"NotCapableOf\": \"is [w1] generally capable of [w2]?\",\n\t    \"RelatedTo\": \"is [w1] like [w2]?\",\n\t    \"Desires\": \"does [w1] generally want [w2]?\",\n\t    \"MotivatedByGoal\": \"is [w1] generally movitated by the goal of [w2]?\",\n\t    # \"NotHasProperty\":  \"does [w1] generally have the property of [w2]?\",\n\t    \"CreatedBy\": \"is [w1] generally created by [w2]?\",\n\t    \"CausesDesire\": \"does [w1] generally make people want [w2]?\",\n\t    # \"NotIsA\": \"is [w1] a type of [w2]?\",\n\t    # \"HasFirstSubevent\": \"is [w2] generally the first subevent of [w1]?\",\n", "    # \"DefinedAs\": \"is [w1] generally defined as [w2]?\"\n\t}\n\tREL_TO_NEG_TEMPLATE = {\n\t    \"IsA\": \"[w1] is not a type of [w2]\",\n\t    'CapableOf': \"[w1] can not [w2]\",\n\t    'UsedFor': \"[w1] is not used for [w2]\",\n\t    \"MadeOf\": \"[w1] is not made of [w2]\",\n\t    'HasProperty': \"[w1] is not [w2]\",\n\t    'HasSubevent': \"Something you do when you [w1] is [w2]\",\n\t    \"AtLocation\": \"You are not likely to find [w1] in [w2]\",\n", "    \"PartOf\": \"[w1] is not part of [w2]\",\n\t    \"HasA\": \"[w1] does not have [w2]\",\n\t    \"ReceivesAction\": \"[w1] can not be [w2]\",\n\t    \"Causes\": \"[w1] does not cause [w2]\",\n\t    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs not to happen\",\n\t    \"NotCapableOf\": \"[w1] is capable of [w2]\",\n\t    \"RelatedTo\": \"[w1] is not like [w2]\",\n\t    \"Desires\": \"[w1] does not want [w2]\",\n\t    \"MotivatedByGoal\": \"You would [w1] not because you want to [w2]\",\n\t    \"NotHasProperty\":  \"[w1] has the property of [w2]\",\n", "    \"CreatedBy\": \"[w1] is not created by [w2]\",\n\t    \"CausesDesire\": \"[w1] does not make people want [w2]\",\n\t    \"NotIsA\": \"[w1] is a type of [w2]\",\n\t    \"HasFirstSubevent\": \"the first thing you do when you [w1] is not [w2]\",\n\t    \"DefinedAs\": \"[w1] is not defined as [w2]\"\n\t}\n\tREL_TO_TEMPLATE = {\n\t    \"RelatedTo\": \"[w1] is like [w2]\",\n\t    \"ExternalUrl\": \"[w1] is described at the following URL [w2]\",\n\t    \"FormOf\": \"[w1] is a form of the word [w2]\",\n", "    \"IsA\": \"[w1] is a type of [w2]\",\n\t    \"NotIsA\": \"[w1] is not [w2]\",\n\t    \"PartOf\": \"[w1] is part of [w2]\",\n\t    \"UsedFor\": \"[w1] is used for [w2]\",\n\t    \"CapableOf\": \"[w1] can [w2]\",\n\t    \"AtLocation\": \"You are likely to find [w1] in [w2]\",\n\t    \"Causes\": \"Sometimes [w1] causes [w2]\",\n\t    \"HasA\": \"[w1] has [w2]\",\n\t    \"HasSubevent\": \"Something you do when you [w1] is [w2]\",\n\t    \"HasFirstSubevent\": \"the first thing you do when you [w1] is [w2]\",\n", "    \"HasLastSubevent\": \"the last thing you do when you [w1] is [w2]\",\n\t    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs to happen\",\n\t    \"HasProperty\": \"[w1] is [w2]\",\n\t    \"HasContext\": \"[w1] is a word used in the context of [w2]\",\n\t    \"MotivatedByGoal\": \"You would [w1] because you want to [w2]\",\n\t    \"ObstructedBy\": \"[w1] can be prevented by [w2]\",\n\t    \"Desires\": \"[w1] wants [w2]\",\n\t    \"CreatedBy\": \"[w1] is created by [w2]\",\n\t    \"Synonym\": \"[w1] and [w2] have similar meanings\",\n\t    \"Antonym\": \"[w1] is the opposite of [w2]\",\n", "    \"DistinctFrom\": \"it cannot be both [w1] and [w2]\",\n\t    \"DerivedFrom\": \"the word [w1] is derived from the word [w2]\",\n\t    \"DefinedAs\": \"[w1] is defined as [w2]\",\n\t    \"Entails\": \"if [w1] is happening, [w2] is also happening\",\n\t    \"MannerOf\": \"[w1] is a specific way of doing [w2]\",\n\t    \"LocatedNear\": \"[w1] is located near [w2]\",\n\t    \"dbpedia\": \"[w1] is conceptually related to [w2]\",\n\t    \"SimilarTo\": \"[w1] is similar to [w2]\",\n\t    \"EtymologicallyRelatedTo\": \"the word [w1] and the word [w2] have the same origin\",\n\t    \"EtymologicallyDerivedFrom\": \"the word [w1] comes from the word [w2]\",\n", "    \"CausesDesire\": \"[w1] makes people want [w2]\",\n\t    \"MadeOf\": \"[w1] is made of [w2]\",\n\t    \"ReceivesAction\": \"[w1] can be [w2]\",\n\t    \"InstanceOf\": \"[w1] is an example of [w2]\",\n\t    \"NotDesires\": \"[w1] does not want [w2]\",\n\t    \"NotUsedFor\": \"[w1] is not used for [w2]\",\n\t    \"NotCapableOf\": \"[w1] is not capable of [w2]\",\n\t    \"NotHasProperty\": \"[w1] does not have the property of [w2]\",\n\t    \"NotMadeOf\": \"[w1] is not made of [w2]\"\n\t}\n", "def avg(x):\n\t    return sum(x) / len(x)\n\tdef load_conceptnet_weight(cw_filename=os.path.join(os.environ.get('PJ_HOME', '..'),\n\t                                                    'data/conceptnet/conceptnet_weight.txt'),\n\t                           top_percentage=1.):\n\t    cw_dict = {}\n\t    with open(cw_filename) as f:\n\t        for x in f.readlines():\n\t            c, w = x.strip().split('\\t')\n\t            cw_dict[c] = w\n", "    cw_tuple = cjj.SortDict(cw_dict)\n\t    weight_threshold = cw_tuple[int(top_percentage * len(cw_dict))]\n\t    return cw_dict, weight_threshold[-1]\n\tdef load_jsonl(jsl_or_path):\n\t    if isinstance(jsl_or_path, str):\n\t        with open(jsl_or_path) as f:\n\t            data = [json.loads(line) for line in f]\n\t    else:\n\t        data = jsl_or_path\n\t    return data\n", "def save_jsonl(jsl, output_file):\n\t    with open(output_file, 'w') as f:\n\t        for js in jsl:\n\t            f.write(json.dumps(js, ensure_ascii=False) + '\\n')\n\t    return output_file\n\tdef calc_biclf_metrics(y_pred, y_true):\n\t    from sklearn import metrics\n\t    acc = metrics.accuracy_score(y_true, y_pred)\n\t    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n\t    return {'accuracy': acc, 'tn': tn/(tn+fp), 'fp': fp/(tn+fp), 'fn': fn/(fn+tp), 'tp': tp/(fn+tp)}\n", "def rel2text(rel):\n\t    if rel == 'ReceivesAction':\n\t        rel_text = 'can be'\n\t    else:\n\t        p = re.compile(r'([a-z]|\\d)([A-Z])')\n\t        rel_text = re.sub(p, r'\\1 \\2', rel).lower()\n\t    return rel_text\n\tdef chunks_list_first(lst, n=1):\n\t    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n\t    lst = list(lst)\n", "    for i in range(0, len(lst), n):\n\t        yield lst[i : i + n]\n\tdef answer2bool(text, prefix='Answer'):\n\t    if prefix is not None:\n\t        # find if Yes, yes or No, no exsits in the text following the prefix with re\n\t        x = re.findall(f'{prefix}:\\s*(Yes|yes|No|no)', text)\n\t        x = x[0] if len(x) > 0 else text\n\t    else:\n\t        x = text\n\t    x = x.strip().lower().replace(\"<pad>\", \"\").replace(\"###</s>\", \"\").strip()\n", "    if x.startswith('yes'):\n\t        return 1\n\t    elif x.startswith('no'):\n\t        return 0\n\t    else:\n\t        return -1\n"]}
{"filename": "base_generator.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\timport sys\n\timport os\n\timport torch\n\timport ujson as json\n\tfrom tqdm import tqdm\n\tfrom transformers import (\n\t    AutoTokenizer,\n\t    AutoModelForSeq2SeqLM,\n\t    DataCollatorForSeq2Seq,\n", ")\n\tfrom accelerate import Accelerator\n\tfrom datasets import load_dataset, Dataset\n\tfrom torch.utils.data import DataLoader\n\taccelerator = Accelerator()\n\ttorch.cuda.manual_seed_all(42)\n\tdef chunks(lst, n):\n\t    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n\t    for i in range(0, len(lst), n):\n\t        yield lst[i: i + n]\n", "def save_jsonl(jsl, output_file):\n\t    with open(output_file, 'w') as f:\n\t        for js in jsl:\n\t            f.write(json.dumps(js, ensure_ascii=False) + '\\n')\n\t    return output_file\n\tclass Seq2SeqBaseGenerator:\n\t    def __init__(self, model_name_or_path, cache_dir=None, use_auth_token=False, **model_kwargs):\n\t        # turn on deepspeed if the model is too large\n\t        max_retry_times = 3\n\t        for i in range(max_retry_times):\n", "            try:\n\t                self.tokenizer = AutoTokenizer.from_pretrained(\n\t                    model_name_or_path,\n\t                    cache_dir=cache_dir if cache_dir else None,\n\t                    use_auth_token=use_auth_token\n\t                )\n\t                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n\t                    model_name_or_path,\n\t                    cache_dir=cache_dir if cache_dir else None,\n\t                    use_auth_token=use_auth_token,\n", "                    **model_kwargs\n\t                )\n\t                break\n\t            except Exception as e:\n\t                print(e)\n\t                print(f'* Reaching huggingface: {i}/{max_retry_times}')\n\t        self.data_collator = DataCollatorForSeq2Seq(\n\t            self.tokenizer,\n\t            model=self.model,\n\t            label_pad_token_id=-100,\n", "            pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n\t        )\n\t    def preprocess_function(self, examples, max_source_length):\n\t        padding = 'max_length'\n\t        inputs = examples[self.source_column]\n\t        inputs = [self.prefix + inp for inp in inputs]\n\t        model_inputs = self.tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\t        return model_inputs\n\t    def make_source_from_list(self, data):\n\t        return [{'source': x} for x in data]\n", "    def generate(self,\n\t                 test_data_or_file,\n\t                 output_file=None,\n\t                 prefix='',\n\t                 source_column='source',\n\t                 beam_size=1,\n\t                 num_return_sequences=1,\n\t                 max_source_length=None,\n\t                 per_device_test_batch_size=4,\n\t                 verbose=True,\n", "                 num_proc=1,\n\t                 **generate_kwargs):\n\t        assert num_return_sequences <= beam_size\n\t        self.source_column = source_column\n\t        self.prefix = prefix\n\t        if isinstance(test_data_or_file, list):\n\t            if isinstance(test_data_or_file[0], str):\n\t                test_data_or_file = self.make_source_from_list(test_data_or_file)\n\t            raw_datasets = Dataset.from_list(test_data_or_file, split='test')\n\t        else:   # file\n", "            extension = test_data_or_file.split(\".\")[-1]\n\t            raw_datasets = load_dataset(extension, data_files={'test': test_data_or_file})['test']\n\t        column_names = raw_datasets.column_names\n\t        with accelerator.main_process_first():\n\t            processed_datasets = raw_datasets.map(\n\t                self.preprocess_function,\n\t                batched=True,\n\t                num_proc=num_proc,\n\t                remove_columns=column_names,\n\t                desc=\"Running tokenizer on dataset\",\n", "                fn_kwargs={\n\t                    'max_source_length': max_source_length,\n\t                }\n\t            )\n\t        # test_dataset = processed_datasets['test']\n\t        test_dataset = processed_datasets\n\t        test_dataloader = DataLoader(test_dataset, collate_fn=self.data_collator, batch_size=per_device_test_batch_size)\n\t        self.model, test_dataloader = accelerator.prepare(self.model, test_dataloader)\n\t        predictions = []\n\t        scores = []\n", "        for batch in tqdm(test_dataloader, total=len(test_dataloader),\n\t                          disable=not verbose and not accelerator.is_local_main_process,\n\t                          desc=prefix):\n\t            with torch.no_grad():\n\t                generated_output = accelerator.unwrap_model(self.model).generate(\n\t                    batch[\"input_ids\"],\n\t                    attention_mask=batch[\"attention_mask\"],\n\t                    num_beams=beam_size,\n\t                    num_return_sequences=num_return_sequences,\n\t                    output_scores=True,\n", "                    return_dict_in_generate=True,\n\t                    **generate_kwargs\n\t                )\n\t                generated_tokens = accelerator.pad_across_processes(\n\t                    generated_output.sequences, dim=1, pad_index=self.tokenizer.pad_token_id\n\t                )\n\t                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n\t                if isinstance(generated_tokens, tuple):\n\t                    generated_tokens = generated_tokens[0]\n\t                decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n", "                preds = [pred.strip() for pred in decoded_preds]\n\t                predictions += preds\n\t                # gather scores\n\t                # generated_scores = accelerator.gather(generated_output.scores)\n\t                # generated_scores = (x.cpu().numpy() for x in\n\t                #                     generated_scores)  # before softmax: (max_length-1, [bsz * beam_size, vocab])\n\t                # scores += generated_scores\n\t        predictions = list(map(lambda x: x.strip(), predictions))\n\t        # TODO: `[:len(test_dataset)]` is a hack to deal with the auto-filling of the last incomplete batch.\n\t        predictions = list(chunks(predictions, num_return_sequences))[:len(test_dataset)] \n", "        if output_file is not None:\n\t            accelerator.wait_for_everyone()\n\t            if accelerator.is_main_process:\n\t                with open(output_file, 'w') as f:\n\t                    for pred in predictions:\n\t                        f.write(' '.join(pred) + '\\n')\n\t        return predictions\n\tdef test():\n\t    input_data = [\n\t                     {'source': '1. is time changing globally? (A) yes (B) no', 'answer': 'time'},\n", "                     {'source': '2. is rabbit the largest animal on earth?', 'answer': 'one thing'},\n\t                     {'source': '3. is donald trump american? \\n (A) yes (B) no', 'answer': 'two birds'},\n\t                     {'source': '4. is time changing globally? (A) yes (B) no', 'answer': 'time'},\n\t                     {'source': '5. is rabbit the largest animal on earth?', 'answer': 'one thing'},\n\t                     {'source': '6. is donald trump american? \\n (A) yes (B) no', 'answer': 'two birds'},\n\t                     {'source': '7. is time changing globally? (A) yes (B) no', 'answer': 'time'},\n\t                     {'source': '8. is rabbit the largest animal on earth?', 'answer': 'one thing'},\n\t                     {'source': '9. is donald trump american? \\n (A) yes (B) no', 'answer': 'two birds'},\n\t                    #  {'source': '4. who is the president of america?', 'answer': 'three values'},\n\t                 ]\n", "    generator = Seq2SeqBaseGenerator('t5-small')\n\t    results = generator.generate(input_data, beam_size=2, num_proc=1, prefix='translate: ')\n\t    if accelerator.is_main_process:\n\t        print(results)\n\tif __name__ == '__main__':\n\t    test()\n"]}
{"filename": "llm_utils.py", "chunked_list": ["import os\n\timport re\n\timport random\n\timport ujson as json\n\tfrom utils import load_jsonl, rel2text\n\timport cjjpy as cjj\n\trandom.seed(42)\n\tNEG_EX_FILE = f\"{cjj.AbsParentDir(__file__, '.')}/ICL_examples/neg_example_pool.jsonl\"\n\tPOS_EX_FILE = f\"{cjj.AbsParentDir(__file__, '.')}/ICL_examples/pos_example_pool.jsonl\"\n\t_CG_INSTRUCTIONS = {\n", "    'question': {\n\t        'none': [\n\t            \"Answer the question by writing a short sentence that contains correct common sense knowledge.\",\n\t        ],\n\t        'fact': [\n\t            \"Answer the question by writing a sentence that contains correct common sense knowledge. Find a related fact to help write the sentence:\",\n\t        ],\n\t        'logic': [\n\t            \"Answer the question by writing a sentence that contains correct common sense knowledge. Let's think step by step before writing the sentence:\",\n\t        ]\n", "    },\n\t    'keywords': {\n\t        'none': [\n\t            \"Write a short and factual sentence according to commonsense based on the keywords:\",\n\t            \"Use the keywords to create a short and factual sentence that accurately reflects commonsense knowledge.\",\n\t            \"Create a short, factual sentence based on the keywords and what is generally accepted as true.\",\n\t            \"Construct a factual and concise statement based on the provided keywords and commonsense knowledge.\"\n\t        ],\n\t        'fact': [\n\t            \"Write a short and factual sentence according to commonsense based on the keywords. Find a related fact to help write the sentence:\",\n", "        ],\n\t        'logic': [\n\t            \"Write a short and factual sentence according to commonsense based on the keywords. Let's think step by step before writing the sentence:\",\n\t        ]\n\t    }\n\t}\n\t_QA_INSTRUCTIONS = {\n\t    'question': {\n\t        'none': [\n\t            \"Answer the commonsense questions with yes or no:\",\n", "            \"Choose \\\"yes\\\" or \\\"no\\\" to indicate whether you agree or disagree with the commonsense questions.\",\n\t            \"Respond to the questions using \\\"yes\\\" or \\\"no\\\".\",\n\t            \"Indicate whether the commonsense questions are correct or incorrect by writing \\\"yes\\\" or \\\"no\\\".\",\n\t        ],\n\t        'fact': [\n\t            \"Find a related fact to about the question, then answer it with yes or no:\",\n\t        ],\n\t        'logic': [\n\t            \"Let's think step by step to about the question, then answer it with yes or no:\",\n\t        ]\n", "    },\n\t    'keywords': {\n\t        'none': [\n\t            \"Can these keywords form a truthful commonsense fact? Answer with yes or no:\",\n\t        ],\n\t        'fact': [\n\t            \"Can these keywords form a truthful commonsense fact? Find a helpful and related fact to answer this question. Answer with yes or no:\",\n\t        ],\n\t        'logic': [\n\t            \"Can these keywords form a truthful commonsense fact? Let's think step by step to answer this question. Answer with yes or no:\",\n", "        ]\n\t    }\n\t}\n\tINSTRUCTIONS = {\n\t    'qa': _QA_INSTRUCTIONS,\n\t    'cg': _CG_INSTRUCTIONS\n\t}\n\tTASK_KEY = {\n\t    'qa': {\n\t        'question': 'qa_pred',\n", "        'keywords': 'qa_pred_from_kw',\n\t    },\n\t    'cg': {\n\t        'question': 'cg_pred_from_q',\n\t        'keywords': 'cg_pred',\n\t    }\n\t}\n\tINPUT_TYPE_PARAMS = {\n\t    'qa': {\n\t        'question': {\n", "            'input_key': 'question',\n\t            'input_str': 'Question',\n\t        },\n\t        'keywords': {\n\t            'input_key': 'keywords',\n\t            'input_str': 'Keywords',\n\t        }\n\t    },\n\t    'cg': {\n\t        'question': {\n", "            'input_key': 'question',\n\t            'input_str': 'Question',\n\t        },\n\t        'keywords': {\n\t            'input_key': 'keywords',\n\t            'input_str': 'Keywords',\n\t        }\n\t    }\n\t}\n\tOUTPUT_EXAMPLE_KEY = {  # used when loading examples\n", "    'qa': 'answer',\n\t    'cg': 'sentence'\n\t}\n\tOUTPUT_TYPE_PARAMS = {\n\t    'qa': \"Answer\",\n\t    'cg': \"Sentence\"\n\t}\n\tdef load_examples(input_key, input_str, output_key, output_str, cot_key=None, cot_str=None):\n\t    neg_ex = load_jsonl(NEG_EX_FILE)\n\t    pos_ex = load_jsonl(POS_EX_FILE)\n", "    def assemble_demonstration(x):\n\t        '''\n\t        param x: {'keywords': '...', 'sentence': '...'}\n\t        return: 'Keywords: ... \\nSentence: ...'\n\t        '''\n\t        input_ex = x[input_key].strip() if input_key != 'keywords' else ', '.join(\n\t            x[input_key]).strip()\n\t        ds = f\"{input_str}: {input_ex}\\n\"\n\t        if cot_key is not None and cot_str is not None:\n\t            if x.get(cot_key) is not None:\n", "                ds += f\"{cot_str}: {x[cot_key]}\\n\"\n\t            else:\n\t                print(f'Warning: no {cot_key}:', x)\n\t        ds += f\"{output_str}: {x[output_key].strip()}\"\n\t        return ds\n\t    neg_ex = [assemble_demonstration(x) for x in neg_ex]\n\t    pos_ex = [assemble_demonstration(x) for x in pos_ex]\n\t    return pos_ex, neg_ex\n\tdef prepare_prompt(task, js, k_pos_ex=1, k_neg_ex=1, key_q='text-davinci-002_ex-8', cot='none'):\n\t    assert task in ['qa', 'cg'], task\n", "    assert cot in ['fact', 'logic', 'none'], cot\n\t    input_type = 'keywords' if key_q == 'keywords' else 'question'\n\t    if cot == 'none':\n\t        cot_key, cot_str = None, None\n\t    elif cot == 'fact':\n\t        cot_key, cot_str = 'cot_fact', 'Related fact'\n\t    elif cot == 'logic':\n\t        cot_key, cot_str = 'cot_logic', \"Let's think step by step\"\n\t    else:\n\t        raise NotImplementedError\n", "    examples_pos, examples_neg = load_examples(input_key=INPUT_TYPE_PARAMS[task][input_type]['input_key'],\n\t                                               input_str=INPUT_TYPE_PARAMS[task][input_type]['input_str'],\n\t                                               output_key=OUTPUT_EXAMPLE_KEY[task], output_str=OUTPUT_TYPE_PARAMS[task] if cot == 'none' else 'Therefore',\n\t                                               cot_key=cot_key, cot_str=cot_str)\n\t    if key_q != 'keywords':\n\t        q = js['boolq'][key_q].strip().lower()\n\t    else:\n\t        q = triple2keywords(js['subject'], js['predicate'], js['object']).lower()\n\t    # build demonstrations with random examples\n\t    instruct = INSTRUCTIONS[task][input_type][cot][0]  # TODO: hardcoded\n", "    examples = sample_examples_pos_neg(examples_neg, k_neg_ex, examples_pos, k_pos_ex)\n\t    example_text = examples_to_text(examples, '###')\n\t    _cue = OUTPUT_TYPE_PARAMS[task] if cot_str is None else cot_str\n\t    demonstration = f\"{instruct}\\n\\n{example_text}\\n{INPUT_TYPE_PARAMS[task][input_type]['input_str']}: {q}\\n{_cue}:\"\n\t    return demonstration\n\tdef triple2keywords(s, p, o):\n\t    p_text = rel2text(p)\n\t    return f\"{s}, {p_text}, {o}\"\n\tdef examples_to_text(examples: list, delim=\"###\"):\n\t    if examples == []: \n", "        return ''\n\t    else:\n\t        return f\"\\n{delim}\\n\".join(examples) + f\"\\n{delim}\"\n\tdef sample_examples_pos_neg(ex1: list, k1: int, ex2: list, k2: int):\n\t    ex = random.sample(ex1, k1)\n\t    ex += random.sample(ex2, k2)\n\t    random.shuffle(ex)\n\t    return ex\n\tdef save_llm_results(input_file_or_data, y_pred, task_key, model_key, output_file):\n\t    # support (pseudo) multiprocessing: avoid overwrite, reload in-disk data\n", "    data = load_jsonl(input_file_or_data)\n\t    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\t    with open(output_file, 'w') as fo:\n\t        for x, a in zip(data, y_pred):\n\t            if x.get(task_key) is None:\n\t                x[task_key] = {model_key: a}\n\t            else:\n\t                x[task_key][model_key] = a\n\t            fo.write(json.dumps(x) + '\\n')\n"]}
{"filename": "boolqa/cjjpy.py", "chunked_list": ["ï»¿# -*- coding: utf-8 -*-\n\t'''\n\t@Author : Jiangjie Chen\n\t@Time   : 2022/5/26 19:52\n\t@Contact: jjchen19@fudan.edu.cn\n\t'''\n\timport re\n\timport datetime\n\timport os\n\timport subprocess\n", "import urllib.request, urllib.parse\n\timport argparse\n\tfrom tqdm import tqdm\n\timport sqlite3\n\timport requests\n\timport socket\n\timport logging\n\timport io\n\timport traceback\n\ttry:\n", "    import ujson as json\n\texcept:\n\t    import json\n\tHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\tdef LengthStats(filename, key4json=None):\n\t    len_list = []\n\t    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n\t    with open(filename) as f:\n\t        for line in f:\n\t            if key4json not in ['none', None, 'None']:\n", "                len_list.append(len(json.loads(line)[key4json].split()))\n\t            else:\n\t                len_list.append(len(line.strip().split()))\n\t    stats = {\n\t        'Max': max(len_list),\n\t        'Min': min(len_list),\n\t        'Avg': round(sum(len_list) / len(len_list), 4),\n\t    }\n\t    len_list.sort()\n\t    for t in thresholds:\n", "        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\t    for k in stats:\n\t        print(f\"- {k}: {stats[k]}\")\n\t    return stats\n\tclass AttrDict(dict):\n\t    def __init__(self, *args, **kwargs):\n\t        super(AttrDict, self).__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\tdef TraceBack(error_msg):\n\t    exc = traceback.format_exc()\n", "    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n\t    return msg\n\tdef Now():\n\t    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\tdef TorchHLoad(filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if not filepath.startswith(\"hdfs://\"):\n\t        return torch.load(filepath, **kwargs)\n\t    else:\n\t        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n", "            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\tdef TorchHSave(obj, filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n\t        with tf.io.gfile.GFile(filepath, 'wb') as f:\n\t            buffer = io.BytesIO()\n\t            torch.save(obj, buffer, **kwargs)\n\t            f.write(buffer.getvalue())\n\t    else:\n\t        torch.save(obj, filepath, **kwargs)\n", "def PutHDFS(local: str, remote: str):\n\t    import tensorflow as tf\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    if not tf.io.gfile.exists(remote):\n\t        tf.io.gfile.makedirs(remote)\n\t    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\tdef GetHDFS(remote: str, local: str):\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    os.makedirs(local, exist_ok=True)\n\t    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n", "def RunCmd(command):\n\t    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t    res, err = pipe.communicate()\n\t    res = res.decode('utf-8')\n\t    err = err.decode('utf-8')\n\t    return res, err\n\tdef AbsParentDir(file, parent='..', postfix=None):\n\t    ppath = os.path.abspath(file)\n\t    parent_level = parent.count('.')\n\t    while parent_level > 0:\n", "        ppath = os.path.dirname(ppath)\n\t        parent_level -= 1\n\t    if postfix is not None:\n\t        return os.path.join(ppath, postfix)\n\t    else:\n\t        return ppath\n\tdef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n\t    from coloredlogs import ColoredFormatter\n\t    import tensorflow as tf\n\t    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n", "    log_format = ColoredFormatter(fmt=fmt)\n\t    # log_format = logging.Formatter()\n\t    logger = logging.getLogger()\n\t    logger.setLevel(log_file_level)\n\t    console_handler = logging.StreamHandler()\n\t    console_handler.setFormatter(log_format)\n\t    logger.handlers = [console_handler]\n\t    if log_file and log_file != '':\n\t        if from_scratch and tf.io.gfile.exists(log_file):\n\t            logger.warning('Removing previous log file: %s' % log_file)\n", "            tf.io.gfile.remove(log_file)\n\t        path = os.path.dirname(log_file)\n\t        os.makedirs(path, exist_ok=True)\n\t        file_handler = logging.FileHandler(log_file)\n\t        file_handler.setLevel(log_file_level)\n\t        file_handler.setFormatter(log_format)\n\t        logger.addHandler(file_handler)\n\t    return logger\n\tdef is_qid(x, hard=False):\n\t    if type(x) is not str: return None\n", "    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tdef is_pid(x, hard=False):\n\t    if type(x) is not str: return None\n\t    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tclass MiniLutDB:\n\t    def __init__(self, db, verbose=True):\n\t        self.db = db\n\t        self.conn = None\n", "        self.verbose = verbose\n\t    def dump_lut(self, lut_tuples, verbose=None):\n\t        # lut_tuple: (k, v)+, iterable\n\t        if verbose is None: \n\t            verbose = self.verbose\n\t        self.conn = sqlite3.connect(self.db)\n\t        cur = self.conn.cursor()\n\t        cur.executescript('''\n\t        DROP TABLE IF EXISTS lut;\n\t        CREATE TABLE lut (\n", "        id      TEXT PRIMARY KEY UNIQUE,\n\t        content TEXT)''')\n\t        self.conn.commit()\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n", "            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def update_lut(self, lut_tuples, verbose=None):\n\t        if verbose is None: \n\t            verbose = self.verbose\n", "        self.conn = sqlite3.connect(self.db)\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n\t            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n", "                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def create_index(self):\n\t        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        # sql = ('CREATE INDEX index_lut ON lut(id);')\n\t        # self.cur.execute(sql)\n", "        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n\t        self.conn.commit()\n\t    def get(self, x, default=None):\n\t        if x is None: return default\n\t        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        res = self.query_lut(self.cur, x, False)[0]\n\t        return res if res is not None else default\n\t    def get_chunk(self, xx):\n", "        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        return self.query_lut(self.conn, xx, self.verbose)\n\t    def close(self):\n\t        if self.conn is not None:\n\t            self.conn.close()\n\t            self.conn = None\n\t    def delete_sample(self, key, value=None):\n\t        if self.get(key) is None: return\n", "        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n\t        self.conn.commit()\n\t        assert self.get(key) is None, f'delete failed: {key}'\n\t    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n\t        values = []\n\t        if isinstance(keys, str): keys = [keys]\n\t        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n\t        for k in iter:\n", "            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n\t            val = cur.fetchone()\n\t            val = val[0] if val is not None else None\n\t            values.append(val)\n\t        return values\n\tdef OverWriteCjjPy(root='.'):\n\t    # import difflib\n\t    # diff = difflib.HtmlDiff()\n\t    cnt = 0\n\t    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n", "    # golden_content = open(golden_cjjpy).readlines()\n\t    for dir, folder, file in os.walk(root):\n\t        for f in file:\n\t            if f == 'cjjpy.py':\n\t                cjjpy = '%s/%s' % (dir, f)\n\t                # content = open(cjjpy).readlines()\n\t                # d = diff.make_file(golden_content, content)\n\t                cnt += 1\n\t                print('[%d]: %s' % (cnt, cjjpy))\n\t                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n", "def ReplaceChar(file, replaced, replacer):\n\t    print(file, replaced, replacer)\n\t    with open(file) as f:\n\t        data = f.readlines()\n\t        out = open(file, 'w')\n\t        for line in data:\n\t            out.write(line.replace(replaced, replacer))\n\tdef DeUnicode(line):\n\t    return line.encode('utf-8').decode('unicode_escape')\n\tdef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n", "    '''\n\t    a\\tb\\n, `.dict' file\n\t    '''\n\t    import tensorflow as tf\n\t    assert dict_file.endswith('.dict')\n\t    id2label = {}\n\t    with tf.io.gfile.GFile(dict_file, 'r') as f:\n\t        data = f.read().split('\\n')\n\t        for i, line in enumerate(data):\n\t            if line == '': continue\n", "            try:\n\t                id, label = line.split('\\t')\n\t                if reverse:\n\t                    id, label = label, id\n\t                _val = '_'.join(label.split()) if unify_words else label\n\t                id2label[id] = _val.lower() if lower else _val\n\t            except:\n\t                pass\n\t    return id2label\n\tdef LoadWords(file, is_file=True):\n", "    import tensorflow as tf\n\t    if is_file:\n\t        with tf.io.gfile.GFile(file, 'r') as f:\n\t            data = f.read().splitlines()\n\t    else:\n\t        data = file.splitlines()\n\t    return set(map(lambda x: x.strip(), data))\n\tdef ChangeFileFormat(filename, new_fmt):\n\t    assert type(filename) is str and type(new_fmt) is str\n\t    spt = filename.split('.')\n", "    if len(spt) == 0:\n\t        return filename\n\t    else:\n\t        return filename.replace('.' + spt[-1], new_fmt)\n\tdef CountLines(fname):\n\t    with open(fname, 'rb') as f:\n\t        count = 0\n\t        last_data = '\\n'\n\t        while True:\n\t            data = f.read(0x400000)\n", "            if not data:\n\t                break\n\t            count += data.count(b'\\n')\n\t            last_data = data\n\t        if last_data[-1:] != b'\\n':\n\t            count += 1  # Remove this if a wc-like count is needed\n\t    return count\n\tdef SearchByKey(file, key):\n\t    with open(file, 'r') as fin:\n\t        while True:\n", "            line = fin.readline()\n\t            if not line: break\n\t            if key in line:\n\t                print(line, end='')\n\tdef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n\t    from email.mime.text import MIMEText\n\t    import smtplib\n\t    # receivers got to be list.\n\t    mail_receivers = receivers\n\t    # mail_host = \"smtp.163.com\n", "    mail_host = \"220.181.12.18\"\n\t    mail_user = \"MichaelChen0110@163.com\"\n\t    mail_pass = \"\"\n\t    me = socket.gethostname() + \"<\" + mail_user + \">\"\n\t    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n\t    msg['Subject'] = subject\n\t    msg['From'] = me\n\t    msg['To'] = \";\".join(mail_receivers)\n\t    try:\n\t        server = smtplib.SMTP()\n", "        server.connect(mail_host)\n\t        server.login(mail_user, mail_pass)\n\t        server.sendmail(me, mail_receivers, msg.as_string())\n\t        server.close()\n\t        print('Have sent the email to ' + str(mail_receivers) + '. ')\n\t        return True\n\t    except Exception as e:\n\t        print(str(e))\n\t        return False\n\tdef SortDict(_dict, reverse=True):\n", "    assert type(_dict) is dict\n\t    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\tdef MaxCommLen(str1, str2):\n\t    lstr1 = len(str1)\n\t    lstr2 = len(str2)\n\t    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n\t    max_num = 0\n\t    for i in range(lstr1):\n\t        for j in range(lstr2):\n\t            if str1[i] == str2[j]:\n", "                record[i + 1][j + 1] = record[i][j] + 1\n\t                if record[i + 1][j + 1] > max_num:\n\t                    max_num = record[i + 1][j + 1]\n\t    return max_num, ''\n\tdef lark(content='test'):\n\t    print(content)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--diff', nargs=2,\n\t                        help='show difference between two files, shown in downloads/diff.html')\n", "    parser.add_argument('--de_unicode', action='store_true', default=False,\n\t                        help='remove unicode characters')\n\t    parser.add_argument('--link_entity', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--max_comm_len', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--search', nargs=2,\n\t                        help='search key from file, 2 args: file name & key')\n\t    parser.add_argument('--email', nargs=2,\n\t                        help='sending emails, 2 args: subject & content')\n", "    parser.add_argument('--overwrite', action='store_true', default=None,\n\t                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n\t    parser.add_argument('--replace', nargs=3,\n\t                        help='replace char, 3 args: file name & replaced char & replacer char')\n\t    parser.add_argument('--lark', nargs=1)\n\t    parser.add_argument('--get_hdfs', nargs=2,\n\t                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n\t    parser.add_argument('--put_hdfs', nargs=2,\n\t                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n\t    parser.add_argument('--length_stats', nargs=2,\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\t    args = parser.parse_args()\n\t    if args.overwrite:\n\t        print('* Overwriting cjjpy...')\n\t        OverWriteCjjPy()\n\t    if args.replace:\n\t        print('* Replacing Char...')\n\t        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\t    if args.search:\n\t        file = args.search[0]\n", "        key = args.search[1]\n\t        print('* Searching %s from %s...' % (key, file))\n\t        SearchByKey(file, key)\n\t    if args.email:\n\t        try:\n\t            subj = args.email[0]\n\t            cont = args.email[1]\n\t        except:\n\t            subj = 'running complete'\n\t            cont = ''\n", "        print('* Sending email {%s, %s} to host...' % (subj, cont))\n\t        SendEmail(subj, cont)\n\t    if args.lark:\n\t        try:\n\t            content = args.lark[0]\n\t        except:\n\t            content = 'running complete'\n\t        print(f'* Larking \"{content}\"...')\n\t        lark(content)\n\t    if args.get_hdfs:\n", "        remote = args.get_hdfs[0]\n\t        local = args.get_hdfs[1]\n\t        print(f'* Copying {remote} to {local}...')\n\t        GetHDFS(remote, local)\n\t    if args.put_hdfs:\n\t        local = args.put_hdfs[0]\n\t        remote = args.put_hdfs[1]\n\t        print(f'* Copying {local} to {remote}...')\n\t        PutHDFS(local, remote)\n\t    if args.length_stats:\n", "        file = args.length_stats[0]\n\t        key4json = args.length_stats[1]\n\t        print(f'* Working on {file} lengths statistics...')\n\t        LengthStats(file, key4json)\n"]}
{"filename": "boolqa/llm_boolqg.py", "chunked_list": ["import os\n\timport sys\n\timport random\n\timport ujson as json\n\timport numpy as np\n\timport cjjpy as cjj\n\tsys.path.append('..')\n\tfrom gpt3_helper import prompt_gpt3, calc_cost_w_prompt\n\tfrom utils import load_jsonl, rel2text, chunks_list_first\n\tfrom llm_utils import examples_to_text\n", "np.random.seed(42)\n\trandom.seed(42)\n\tboolqg_instructions = [\n\t    \"Write a question that asks about the validity of a commonsense knowledge triple (subject, relation, object):\",\n\t]\n\tboolqg_examples_generally = [\n\t    \"Triple: (bird, capable of, fly)\\nQuestion: can most birds fly?\",\n\t    \"Triple: (playing tennis, causes, feel relaxed)\\nQuestion: does playing tennis generally make you feel relaxed?\",\n\t    \"Triple: (room, located at, buildings)\\nQuestion: are rooms usually located at buildings?\",\n\t    \"Triple: (fish, capable of, sleep)\\nQuestion: can fish sleep?\",\n", "    \"Triple: (sheepskin, used for, writing)\\nQuestion: can sheepskin be used for writing?\",\n\t    \"Triple: (spicy, is a, pain)\\nQuestion: is spicy a kind of pain?\",\n\t    \"Triple: (water, has property, spicy)\\nQuestion: is water usually spicy?\",\n\t    \"Triple: (shields, made of, grass)\\nQuestion: are shields generally made of grass?\",\n\t    \"Triple: (penguins, is a, mammal)\\nQuestion: are penguins a kind of mammal?\",\n\t    \"Triple: (work, causes desire, rest)\\nQuestion: does working usually make you want to rest?\",\n\t    \"Triple: (sleeves, part of, shoes)\\nQuestion: are sleeves a part of shoes?\",\n\t    \"Triple: (books, part of, kitchen)\\nQuestion: are books usually part of a kitchen?\",\n\t]\n\tboolqg_examples = [\n", "    \"Triple: (bird, capable of, fly)\\nQuestion: can birds fly?\",\n\t    \"Triple: (playing tennis, causes, feel relaxed)\\nQuestion: does playing tennis make you feel relaxed?\",\n\t    \"Triple: (room, located at, buildings)\\nQuestion: are rooms located at buildings?\",\n\t    \"Triple: (fish, capable of, sleep)\\nQuestion: can fish sleep?\",\n\t    \"Triple: (sheepskin, used for, writing)\\nQuestion: can sheepskin be used for writing?\",\n\t    \"Triple: (spicy, is a, pain)\\nQuestion: is spicy a kind of pain?\",\n\t    \"Triple: (water, has property, spicy)\\nQuestion: is water spicy?\",\n\t    \"Triple: (shields, made of, grass)\\nQuestion: are shields made of grass?\",\n\t    \"Triple: (penguins, is a, mammal)\\nQuestion: are penguins a kind of mammal?\",\n\t    \"Triple: (work, causes desire, rest)\\nQuestion: does working make you want to rest?\",\n", "    \"Triple: (sleeves, part of, shoes)\\nQuestion: are sleeves a part of shoes?\",\n\t    \"Triple: (books, part of, kitchen)\\nQuestion: are books part of a kitchen?\",\n\t]\n\tdef prep_triple(subj, pred, obj):\n\t    pred = rel2text(pred)\n\t    return f\"({subj}, {pred}, {obj})\"\n\tdef llm_boolqg(model_name, input_file, output_file=None, k_ex=8, batch_size=8, generally=False):\n\t    data = load_jsonl(input_file)\n\t    prompts = []\n\t    for line in data:\n", "        # TODO: triple\n\t        triple = prep_triple(line['subject'], line['predicate'], line['object'])\n\t        instruct = boolqg_instructions[0]\n\t        qg_ex = boolqg_examples_generally if generally else boolqg_examples\n\t        examples = np.random.choice(qg_ex, k_ex, replace=False).tolist()\n\t        random.shuffle(examples)\n\t        example_text = examples_to_text(examples, '###')\n\t        demonstration = f\"{instruct}\\n\\n{example_text}\\nTriple:\"\n\t        prompts.append(f\"{demonstration} {triple}\\nQuestion:\")\n\t    y_pred = []\n", "    res, money = prompt_gpt3(prompts, model_name=model_name, clean=True, temperature=0., max_tokens=32, batch_size=batch_size, verbose=True)\n\t    for ny, indiv_prompt in zip(chunks_list_first(res), prompts):\n\t        # handle n_returned sequences\n\t        y_pred.append(ny)\n\t        print(indiv_prompt + ny[0])\n\t    general = '_general' if generally else ''\n\t    model_key = f\"{model_name}_ex-{k_ex}{general}\"\n\t    # log predicted answers by LLM $-$!!!\n\t    if output_file is not None:\n\t        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n", "        with open(output_file, 'w') as f:\n\t            for x, a in zip(data, y_pred):\n\t                if isinstance(a, list): a = a[0]\n\t                if x.get('boolq') is None:\n\t                    x['boolq'] = {model_key: a}\n\t                else:\n\t                    x['boolq'][model_key] = a\n\t                # x['prompts'] = p\n\t                f.write(json.dumps(x) + '\\n')\n\t    cjj.lark(f\"This run has cost you {round(money, 2)}$: {model_key}.\")\n", "    return y_pred\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i', type=str, required=True)\n\t    parser.add_argument('--model_name', '-m', type=str, required=True)\n\t    parser.add_argument('--output_file', '-o', type=str, required=True)\n\t    parser.add_argument('--k', '-k', type=int, help='number of examples', default=8, required=True)\n\t    parser.add_argument('--batch_size', '-b', type=int, default=8)\n\t    parser.add_argument('--generally', action='store_true')\n", "    args = parser.parse_args()\n\t    y_pred = llm_boolqg(args.model_name, args.input_file, args.output_file, k_ex=args.k, batch_size=args.batch_size, generally=args.generally)"]}
{"filename": "boolqa/llm_answer_prediction.py", "chunked_list": ["import sys\n\tfrom accelerate import Accelerator\n\timport cjjpy as cjj\n\tsys.path.append(cjj.AbsParentDir(__file__, '..'))\n\tfrom gpt3_helper import prompt_gpt3, calc_cost_w_prompt\n\t# from flant5_helper import prompt_flant5\n\tfrom utils import load_jsonl, chunks_list_first, answer2bool\n\tfrom llm_utils import (\n\t    save_llm_results, \n\t    prepare_prompt,\n", "    TASK_KEY\n\t)\n\taccelerator = Accelerator()\n\tdef judge_ambi_w_prob(prompt, prompt_func, **kwargs):\n\t    # TODO: flan-t5 prob, but flan-t5 will only output yes/no.\n\t    results, money = prompt_func([prompt + ' yes', prompt + 'no'], echo=True, clean=False, logprobs=1, **kwargs)\n\t    results = list(results)\n\t    prob_yes = results[0]['logprobs']['token_logprobs'][-1]\n\t    prob_no = results[1]['logprobs']['token_logprobs'][-1]\n\t    return ' yes' if prob_yes > prob_no else ' no'\n", "def llm_boolqa(model_name, input_file, output_file=None, key_q='text-davinci-002_ex-8', k_pos_ex=2, k_neg_ex=2, batch_size=8, cot='none'):\n\t    data = load_jsonl(input_file)\n\t    prompts = []\n\t    for js in data:\n\t        prompt = prepare_prompt('qa', js, k_pos_ex, k_neg_ex, key_q, cot)\n\t        prompts.append(prompt)\n\t    print(prompts[0])\n\t    num_cot_tokens = 40 if cot is not None else 0   # TODO: approximate number of cot tokens\n\t    if 'flan-t5' in model_name:\n\t        prompt_func = prompt_flant5\n", "    else:\n\t        prompt_func = prompt_gpt3\n\t    res, money = prompt_func(prompts, model_name=model_name, clean=True, temperature=0., \n\t                             max_tokens=5 + num_cot_tokens, verbose=True, batch_size=batch_size, n=1)\n\t    y_pred = []\n\t    for i, (ny, indiv_prompt) in enumerate(zip(chunks_list_first(res, n=1), prompts)):\n\t        # handle n_returned sequences, where n = 1 in QA.\n\t        y = ny[0]\n\t        if answer2bool(y, 'Answer' if cot != 'none' else None) < 0 and 'flan-t5' not in model_name: # if y is undecidable, use probability\n\t            y = judge_ambi_w_prob(indiv_prompt, prompt_func, model_name=model_name, temperature=0., max_tokens=0, batch_size=batch_size, n=1)\n", "            money += calc_cost_w_prompt(indiv_prompt + y + indiv_prompt + y, model_name)\n\t        y_pred.append(y)\n\t        if i == 0: print(indiv_prompt + y)\n\t    if args.instruct_id == 0:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n\t    else:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_i{args.instruct_id}\" # TODO: hard-coded\n\t    if args.temperature == 0:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n\t    else:\n", "        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_t{args.temperature}\" # TODO: hard-coded\n\t    input_type = 'keywords' if key_q == 'keywords' else 'question'\n\t    task_key = TASK_KEY['qa'][input_type]\n\t    if cot != 'none':\n\t        task_key += f'_{cot}'\n\t    if accelerator.is_main_process:\n\t        # save into file, override previous model key.\n\t        save_llm_results(input_file, y_pred, task_key, model_key, output_file)\n\t        if 'bloom' not in model_name or 'flan-t5' not in model_name:\n\t            cjj.lark(f\"This run has cost you {round(money, 2)}$: {task_key}/{model_key}.\")\n", "    return f\"{task_key}/{model_key}\"\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i', type=str, required=True)\n\t    parser.add_argument('--model_name', '-m', type=str, required=True, \n\t                        choices=['flan-t5-large', 'flan-t5-xl', 'flan-t5-xxl',\n\t                                 'davinci', 'curie', 'babbage', 'ada', \n\t                                 'text-davinci-001', 'text-curie-001', 'text-babbage-001', 'text-ada-001',\n\t                                 'text-davinci-002', 'text-davinci-003', 'code-davinci-002'])\n", "    parser.add_argument('--posk', type=int, help='Number of positive examples in the demonstration.')\n\t    parser.add_argument('--negk', type=int, help='Number of negative examples in the demonstration.')\n\t    parser.add_argument('--output_file', '-o', type=str, required=True, \n\t                        help='Note: just for convenience, input and output file are the same.')\n\t    parser.add_argument('--input_key', '-q', default='text-davinci-002_ex-8', type=str, \n\t                        help='Key for input questions (in QA) in the jsonline file. InstructGPT-002 generated questions by default.')\n\t    parser.add_argument('--batch_size', '-b', type=int, default=16)\n\t    parser.add_argument('--temperature', type=float, default=0.0)\n\t    parser.add_argument('--cot', type=str, choices=['fact', 'logic', 'none'], default='none')\n\t    parser.add_argument('--instruct_id', type=int, default=0, \n", "                        help='For testing different instructions. Use the first instruction by default.')\n\t    args = parser.parse_args()\n\t    y_pred = llm_boolqa(args.model_name, args.input_file, args.output_file, \n\t                        args.input_key, k_pos_ex=args.posk, k_neg_ex=args.negk, \n\t                        batch_size=args.batch_size, cot=args.cot)"]}
{"filename": "boolqa/tmpl_boolqg.py", "chunked_list": ["import sys\n\timport argparse\n\tsys.path.append('..')\n\tfrom base_generator import Seq2SeqBaseGenerator\n\tfrom utils import *\n\tdef _triple2boolq(s, p, o, boolq_template):\n\t    boolq = boolq_template[p.lower()].replace('[w1]', s).replace('[w2]', o)\n\t    return boolq\n\tdef _correct_grammar(inputs):\n\t    # requires `huggingface-cli login`\n", "    gf = Seq2SeqBaseGenerator('prithivida/grammar_error_correcter_v1', use_auth_token=True)\n\t    corrected_sentences = gf.generate(inputs, prefix='gec: ', beam_size=7, num_proc=1,\n\t                                      do_sample=True, max_length=128, early_stopping=True, \n\t                                      per_device_test_batch_size=32)\n\t    return [x[0] for x in corrected_sentences]\n\tdef generate_boolq_from_triples(input_file_or_data, output_file, boolq_template):\n\t    # boolq from template => correct grammar => add choices for unifiedqa\n\t    # output_file: input_file (jsonl) with boolq\n\t    data = load_jsonl(input_file_or_data)\n\t    boolqs = []\n", "    for js in data:\n\t        boolq = _triple2boolq(js['subject'], js['predicate'], js['object'], boolq_template)\n\t        boolqs.append(boolq) # potentially boolq is None\n\t    boolqs = _correct_grammar([{'source': x} for x in boolqs])\n\t    if output_file is not None:\n\t        with open(output_file, 'w') as fo:\n\t            for js, q in zip(data, boolqs):\n\t                if q is not None:\n\t                    js['boolq'] = {'rule': q}\n\t                    fo.write(json.dumps(js) + '\\n')\n", "    return boolqs\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i', type=str)\n\t    parser.add_argument('--output_file', '-o', type=str)\n\t    parser.add_argument('--generally', action='store_true')\n\t    args = parser.parse_args()\n\t    template = USUALLY_REL_TO_BOOLQ_TEMPLATE if args.generally else REL_TO_BOOLQ_TEMPLATE\n\t    uncased_template = {k.lower(): v for k, v in template.items()}\n\t    generate_boolq_from_triples(args.input_file, args.output_file, uncased_template)"]}
{"filename": "evaluation/eval_boolqa.py", "chunked_list": ["import argparse\n\timport sys\n\timport random\n\tfrom collections import Counter\n\tsys.path.append('..')\n\timport cjjpy as cjj\n\tfrom utils import answer2bool, load_jsonl, calc_biclf_metrics\n\trandom.seed(42)\n\tdef eval_qa(file_or_data, model_name):\n\t    data = load_jsonl(file_or_data)\n", "    y_pred = []\n\t    y_true = []\n\t    i = 0\n\t    for x in data:\n\t        if '/' in model_name:\n\t            task_key = model_name.split('/')[0]\n\t            model_key = '/'.join(model_name.split('/')[1:])\n\t        else:\n\t            task_key = 'qa_pred'\n\t            model_key = model_name\n", "        if 'fact' in task_key or 'logic' in task_key:\n\t            eval_cot = True\n\t        else:\n\t            eval_cot = False\n\t        pred = x[task_key][model_key]\n\t        if isinstance(pred, str): # TODO: majority vote QA\n\t            pred = [pred]\n\t        vote_pred = []\n\t        for p in pred:\n\t            bool_pred = answer2bool(p, prefix='Answer' if eval_cot else None)\n", "            vote_pred.append(bool_pred)\n\t        voted_pred = Counter(vote_pred).most_common()[0][0]\n\t        y_pred.append(voted_pred)\n\t        if voted_pred < 0: \n\t            # bool_pred = random.choice([0, 1])\n\t            print(f\"[{i}] Undecided: {x}\")\n\t            i += 1\n\t        if x.get('label') is None:\n\t            _label = int('pos' in x['source_kb'])\n\t        else:\n", "            _label = x['label']\n\t        y_true.append(_label)\n\t    y_pred_f, y_true_f = [], []\n\t    for y1, y2 in zip(y_pred, y_true):\n\t        if y1 >= 0:\n\t            y_pred_f.append(y1)\n\t            y_true_f.append(y2)\n\t    scores = calc_biclf_metrics(y_pred_f, y_true_f)\n\t    return scores, y_pred\n\tif __name__ == '__main__':\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i', type=str)\n\t    parser.add_argument('--qa_model', '-m', type=str, help='nested or not: `qa_pred_cot_fact/text-davinci-002_ex-8p8n` or `text-davinci-002_ex-8p8n`')\n\t    parser.add_argument('--log', action='store_true')\n\t    args = parser.parse_args()\n\t    scores, y_pred = eval_qa(args.input_file, args.qa_model)\n\t    print(scores)\n\t    cjj.lark(f\"QA - {args.qa_model} in {args.input_file}: {str(scores)}\")\n\t    if args.log:\n\t        with open('metrics.log', 'a') as f:\n", "            f.write(f\"{args.qa_model}\\t{scores}\\tNone\\n\")"]}
{"filename": "evaluation/cjjpy.py", "chunked_list": ["ï»¿# -*- coding: utf-8 -*-\n\t'''\n\t@Author : Jiangjie Chen\n\t@Time   : 2022/5/26 19:52\n\t@Contact: jjchen19@fudan.edu.cn\n\t'''\n\timport re\n\timport datetime\n\timport os\n\timport subprocess\n", "import urllib.request, urllib.parse\n\timport argparse\n\tfrom tqdm import tqdm\n\timport sqlite3\n\timport requests\n\timport socket\n\timport logging\n\timport io\n\timport traceback\n\ttry:\n", "    import ujson as json\n\texcept:\n\t    import json\n\tHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\tdef LengthStats(filename, key4json=None):\n\t    len_list = []\n\t    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n\t    with open(filename) as f:\n\t        for line in f:\n\t            if key4json not in ['none', None, 'None']:\n", "                len_list.append(len(json.loads(line)[key4json].split()))\n\t            else:\n\t                len_list.append(len(line.strip().split()))\n\t    stats = {\n\t        'Max': max(len_list),\n\t        'Min': min(len_list),\n\t        'Avg': round(sum(len_list) / len(len_list), 4),\n\t    }\n\t    len_list.sort()\n\t    for t in thresholds:\n", "        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\t    for k in stats:\n\t        print(f\"- {k}: {stats[k]}\")\n\t    return stats\n\tclass AttrDict(dict):\n\t    def __init__(self, *args, **kwargs):\n\t        super(AttrDict, self).__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\tdef TraceBack(error_msg):\n\t    exc = traceback.format_exc()\n", "    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n\t    return msg\n\tdef Now():\n\t    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\tdef TorchHLoad(filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if not filepath.startswith(\"hdfs://\"):\n\t        return torch.load(filepath, **kwargs)\n\t    else:\n\t        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n", "            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\tdef TorchHSave(obj, filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n\t        with tf.io.gfile.GFile(filepath, 'wb') as f:\n\t            buffer = io.BytesIO()\n\t            torch.save(obj, buffer, **kwargs)\n\t            f.write(buffer.getvalue())\n\t    else:\n\t        torch.save(obj, filepath, **kwargs)\n", "def PutHDFS(local: str, remote: str):\n\t    import tensorflow as tf\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    if not tf.io.gfile.exists(remote):\n\t        tf.io.gfile.makedirs(remote)\n\t    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\tdef GetHDFS(remote: str, local: str):\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    os.makedirs(local, exist_ok=True)\n\t    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n", "def RunCmd(command):\n\t    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t    res, err = pipe.communicate()\n\t    res = res.decode('utf-8')\n\t    err = err.decode('utf-8')\n\t    return res, err\n\tdef AbsParentDir(file, parent='..', postfix=None):\n\t    ppath = os.path.abspath(file)\n\t    parent_level = parent.count('.')\n\t    while parent_level > 0:\n", "        ppath = os.path.dirname(ppath)\n\t        parent_level -= 1\n\t    if postfix is not None:\n\t        return os.path.join(ppath, postfix)\n\t    else:\n\t        return ppath\n\tdef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n\t    from coloredlogs import ColoredFormatter\n\t    import tensorflow as tf\n\t    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n", "    log_format = ColoredFormatter(fmt=fmt)\n\t    # log_format = logging.Formatter()\n\t    logger = logging.getLogger()\n\t    logger.setLevel(log_file_level)\n\t    console_handler = logging.StreamHandler()\n\t    console_handler.setFormatter(log_format)\n\t    logger.handlers = [console_handler]\n\t    if log_file and log_file != '':\n\t        if from_scratch and tf.io.gfile.exists(log_file):\n\t            logger.warning('Removing previous log file: %s' % log_file)\n", "            tf.io.gfile.remove(log_file)\n\t        path = os.path.dirname(log_file)\n\t        os.makedirs(path, exist_ok=True)\n\t        file_handler = logging.FileHandler(log_file)\n\t        file_handler.setLevel(log_file_level)\n\t        file_handler.setFormatter(log_format)\n\t        logger.addHandler(file_handler)\n\t    return logger\n\tdef is_qid(x, hard=False):\n\t    if type(x) is not str: return None\n", "    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tdef is_pid(x, hard=False):\n\t    if type(x) is not str: return None\n\t    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tclass MiniLutDB:\n\t    def __init__(self, db, verbose=True):\n\t        self.db = db\n\t        self.conn = None\n", "        self.verbose = verbose\n\t    def dump_lut(self, lut_tuples, verbose=None):\n\t        # lut_tuple: (k, v)+, iterable\n\t        if verbose is None: \n\t            verbose = self.verbose\n\t        self.conn = sqlite3.connect(self.db)\n\t        cur = self.conn.cursor()\n\t        cur.executescript('''\n\t        DROP TABLE IF EXISTS lut;\n\t        CREATE TABLE lut (\n", "        id      TEXT PRIMARY KEY UNIQUE,\n\t        content TEXT)''')\n\t        self.conn.commit()\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n", "            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def update_lut(self, lut_tuples, verbose=None):\n\t        if verbose is None: \n\t            verbose = self.verbose\n", "        self.conn = sqlite3.connect(self.db)\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n\t            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n", "                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def create_index(self):\n\t        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        # sql = ('CREATE INDEX index_lut ON lut(id);')\n\t        # self.cur.execute(sql)\n", "        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n\t        self.conn.commit()\n\t    def get(self, x, default=None):\n\t        if x is None: return default\n\t        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        res = self.query_lut(self.cur, x, False)[0]\n\t        return res if res is not None else default\n\t    def get_chunk(self, xx):\n", "        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        return self.query_lut(self.conn, xx, self.verbose)\n\t    def close(self):\n\t        if self.conn is not None:\n\t            self.conn.close()\n\t            self.conn = None\n\t    def delete_sample(self, key, value=None):\n\t        if self.get(key) is None: return\n", "        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n\t        self.conn.commit()\n\t        assert self.get(key) is None, f'delete failed: {key}'\n\t    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n\t        values = []\n\t        if isinstance(keys, str): keys = [keys]\n\t        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n\t        for k in iter:\n", "            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n\t            val = cur.fetchone()\n\t            val = val[0] if val is not None else None\n\t            values.append(val)\n\t        return values\n\tdef OverWriteCjjPy(root='.'):\n\t    # import difflib\n\t    # diff = difflib.HtmlDiff()\n\t    cnt = 0\n\t    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n", "    # golden_content = open(golden_cjjpy).readlines()\n\t    for dir, folder, file in os.walk(root):\n\t        for f in file:\n\t            if f == 'cjjpy.py':\n\t                cjjpy = '%s/%s' % (dir, f)\n\t                # content = open(cjjpy).readlines()\n\t                # d = diff.make_file(golden_content, content)\n\t                cnt += 1\n\t                print('[%d]: %s' % (cnt, cjjpy))\n\t                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n", "def ReplaceChar(file, replaced, replacer):\n\t    print(file, replaced, replacer)\n\t    with open(file) as f:\n\t        data = f.readlines()\n\t        out = open(file, 'w')\n\t        for line in data:\n\t            out.write(line.replace(replaced, replacer))\n\tdef DeUnicode(line):\n\t    return line.encode('utf-8').decode('unicode_escape')\n\tdef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n", "    '''\n\t    a\\tb\\n, `.dict' file\n\t    '''\n\t    import tensorflow as tf\n\t    assert dict_file.endswith('.dict')\n\t    id2label = {}\n\t    with tf.io.gfile.GFile(dict_file, 'r') as f:\n\t        data = f.read().split('\\n')\n\t        for i, line in enumerate(data):\n\t            if line == '': continue\n", "            try:\n\t                id, label = line.split('\\t')\n\t                if reverse:\n\t                    id, label = label, id\n\t                _val = '_'.join(label.split()) if unify_words else label\n\t                id2label[id] = _val.lower() if lower else _val\n\t            except:\n\t                pass\n\t    return id2label\n\tdef LoadWords(file, is_file=True):\n", "    import tensorflow as tf\n\t    if is_file:\n\t        with tf.io.gfile.GFile(file, 'r') as f:\n\t            data = f.read().splitlines()\n\t    else:\n\t        data = file.splitlines()\n\t    return set(map(lambda x: x.strip(), data))\n\tdef ChangeFileFormat(filename, new_fmt):\n\t    assert type(filename) is str and type(new_fmt) is str\n\t    spt = filename.split('.')\n", "    if len(spt) == 0:\n\t        return filename\n\t    else:\n\t        return filename.replace('.' + spt[-1], new_fmt)\n\tdef CountLines(fname):\n\t    with open(fname, 'rb') as f:\n\t        count = 0\n\t        last_data = '\\n'\n\t        while True:\n\t            data = f.read(0x400000)\n", "            if not data:\n\t                break\n\t            count += data.count(b'\\n')\n\t            last_data = data\n\t        if last_data[-1:] != b'\\n':\n\t            count += 1  # Remove this if a wc-like count is needed\n\t    return count\n\tdef SearchByKey(file, key):\n\t    with open(file, 'r') as fin:\n\t        while True:\n", "            line = fin.readline()\n\t            if not line: break\n\t            if key in line:\n\t                print(line, end='')\n\tdef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n\t    from email.mime.text import MIMEText\n\t    import smtplib\n\t    # receivers got to be list.\n\t    mail_receivers = receivers\n\t    # mail_host = \"smtp.163.com\n", "    mail_host = \"220.181.12.18\"\n\t    mail_user = \"MichaelChen0110@163.com\"\n\t    mail_pass = \"\"\n\t    me = socket.gethostname() + \"<\" + mail_user + \">\"\n\t    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n\t    msg['Subject'] = subject\n\t    msg['From'] = me\n\t    msg['To'] = \";\".join(mail_receivers)\n\t    try:\n\t        server = smtplib.SMTP()\n", "        server.connect(mail_host)\n\t        server.login(mail_user, mail_pass)\n\t        server.sendmail(me, mail_receivers, msg.as_string())\n\t        server.close()\n\t        print('Have sent the email to ' + str(mail_receivers) + '. ')\n\t        return True\n\t    except Exception as e:\n\t        print(str(e))\n\t        return False\n\tdef SortDict(_dict, reverse=True):\n", "    assert type(_dict) is dict\n\t    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\tdef MaxCommLen(str1, str2):\n\t    lstr1 = len(str1)\n\t    lstr2 = len(str2)\n\t    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n\t    max_num = 0\n\t    for i in range(lstr1):\n\t        for j in range(lstr2):\n\t            if str1[i] == str2[j]:\n", "                record[i + 1][j + 1] = record[i][j] + 1\n\t                if record[i + 1][j + 1] > max_num:\n\t                    max_num = record[i + 1][j + 1]\n\t    return max_num, ''\n\tdef lark(content='test'):\n\t    print(content)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--diff', nargs=2,\n\t                        help='show difference between two files, shown in downloads/diff.html')\n", "    parser.add_argument('--de_unicode', action='store_true', default=False,\n\t                        help='remove unicode characters')\n\t    parser.add_argument('--link_entity', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--max_comm_len', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--search', nargs=2,\n\t                        help='search key from file, 2 args: file name & key')\n\t    parser.add_argument('--email', nargs=2,\n\t                        help='sending emails, 2 args: subject & content')\n", "    parser.add_argument('--overwrite', action='store_true', default=None,\n\t                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n\t    parser.add_argument('--replace', nargs=3,\n\t                        help='replace char, 3 args: file name & replaced char & replacer char')\n\t    parser.add_argument('--lark', nargs=1)\n\t    parser.add_argument('--get_hdfs', nargs=2,\n\t                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n\t    parser.add_argument('--put_hdfs', nargs=2,\n\t                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n\t    parser.add_argument('--length_stats', nargs=2,\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\t    args = parser.parse_args()\n\t    if args.overwrite:\n\t        print('* Overwriting cjjpy...')\n\t        OverWriteCjjPy()\n\t    if args.replace:\n\t        print('* Replacing Char...')\n\t        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\t    if args.search:\n\t        file = args.search[0]\n", "        key = args.search[1]\n\t        print('* Searching %s from %s...' % (key, file))\n\t        SearchByKey(file, key)\n\t    if args.email:\n\t        try:\n\t            subj = args.email[0]\n\t            cont = args.email[1]\n\t        except:\n\t            subj = 'running complete'\n\t            cont = ''\n", "        print('* Sending email {%s, %s} to host...' % (subj, cont))\n\t        SendEmail(subj, cont)\n\t    if args.lark:\n\t        try:\n\t            content = args.lark[0]\n\t        except:\n\t            content = 'running complete'\n\t        print(f'* Larking \"{content}\"...')\n\t        lark(content)\n\t    if args.get_hdfs:\n", "        remote = args.get_hdfs[0]\n\t        local = args.get_hdfs[1]\n\t        print(f'* Copying {remote} to {local}...')\n\t        GetHDFS(remote, local)\n\t    if args.put_hdfs:\n\t        local = args.put_hdfs[0]\n\t        remote = args.put_hdfs[1]\n\t        print(f'* Copying {local} to {remote}...')\n\t        PutHDFS(local, remote)\n\t    if args.length_stats:\n", "        file = args.length_stats[0]\n\t        key4json = args.length_stats[1]\n\t        print(f'* Working on {file} lengths statistics...')\n\t        LengthStats(file, key4json)\n"]}
{"filename": "evaluation/eval_constrained_generation.py", "chunked_list": ["import sys\n\timport re\n\timport argparse \n\tfrom tqdm import tqdm\n\timport torch\n\tfrom collections import defaultdict, Counter\n\tfrom transformers import pipeline\n\tsys.path.append('..')\n\timport cjjpy as cjj\n\tfrom utils import *\n", "negation_cue = {'nothing', \"no\", \"not\", \"never\", \"none\", \"hardly\", \"rarely\", \"scarcely\", \"seldom\", 'barely',\n\t                \"nor\", \"neither\", \"nothing\", \"nowhere\", \"without\", \"lack\", \"cant\", \"dont\", \"doesnt\",\n\t                \"doesn't\", \"don't\", \"isn't\", \"wasn't\", \"aren't\", \"weren't\", \"haven't\", \"hasn't\", \n\t                \"shouldn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"cannot\", \"unable\"}\n\tdef majority_vote(pred_list):\n\t    return [Counter(x).most_common()[0][0] for x in pred_list]\n\tdef fetch_target_sent(x, prefix):\n\t    # prefix = None: ignore prefix, use the first sentence\n\t    # prefix = xxxx: find the first of that pattern\n\t    if prefix is not None:\n", "        for subx in x.strip().replace('###', '\\n').split('\\n'):\n\t            # find the sentence after prefix\n\t            sent = re.findall(f'{prefix}(.*)', subx)\n\t            if sent != []:\n\t                return sent[0].strip()\n\t    return x.strip().split('\\n')[0].strip()\n\tdef detect_negation(pipe, texts: list, prefix=None, only_rule=False):\n\t    # True for neg, False for pos\n\t    def _has_neg(x):\n\t        for j in x:\n", "            if j['entity'] == 'Y':\n\t                return True\n\t        return False\n\t    labels = []\n\t    for t in texts:\n\t        rule_hit_flag = False\n\t        t1 = fetch_target_sent(t, prefix)\n\t        if only_rule:\n\t            label = False\n\t            for w in t1.split():\n", "                if w.lower() in negation_cue:\n\t                    label = True\n\t                    break\n\t            labels.append(label)\n\t        else:\n\t            # filtering with rules first\n\t            for w in t1.split():\n\t                if w.lower() in negation_cue:\n\t                    labels.append(True)\n\t                    rule_hit_flag = True\n", "                    break\n\t            if not rule_hit_flag:\n\t                labels.append(_has_neg(pipe(t1)))\n\t    return labels\n\tdef eval_negation(model_name, filename, pipe):\n\t    data = load_jsonl(filename)\n\t    y_pred = []\n\t    y_true = []\n\t    y_pred_by_rel = defaultdict(list)\n\t    y_true_by_rel = defaultdict(list)\n", "    if '/' in model_name:\n\t        task_key = model_name.split('/')[0]\n\t        model_key = '/'.join(model_name.split('/')[1:])\n\t    else:\n\t        task_key = 'cg_pred'\n\t        model_key = model_name\n\t    for js in tqdm(data, desc='Negation Detection'):\n\t        if js.get('label') is None:\n\t            _label = int('pos' in js['source_kb'])\n\t        else:\n", "            _label = js['label']\n\t        y_true.append(_label)\n\t        y_true_by_rel[js['predicate']].append(_label)\n\t        prefix = 'Therefore:' if 'logic' in task_key or 'fact' in task_key else None\n\t        _pred = detect_negation(pipe, js[task_key][model_key], prefix=prefix)\n\t        _pred = [int(not x) for x in _pred]\n\t        y_pred.append(_pred)\n\t        y_pred_by_rel[js['predicate']].append(_pred)\n\t    y_score = majority_vote(y_pred)\n\t    scores = calc_biclf_metrics(y_score, y_true)\n", "    return scores, y_score\n\tdef load_negation_detector():\n\t    pipe = pipeline('token-classification', model='spoiled/roberta-large-condaqa-neg-tag-token-classifier', device=0 if torch.cuda.is_available() else -1)\n\t    return pipe\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i')\n\t    parser.add_argument('--model_name', '-m')\n\t    parser.add_argument('--log', action='store_true')\n\t    args = parser.parse_args()\n", "    pipe = load_negation_detector()\n\t    scores, neg_pred = eval_negation(args.model_name, args.input_file, pipe)\n\t    print(scores)\n\t    cjj.lark(f\"CG - {args.model_name} in {args.input_file}: {str(scores)}\")\n\t    if args.log:\n\t        with open('metrics.log', 'a') as f:\n\t            f.write(f\"{args.model_name}\\t{scores}\\tNone\\n\")"]}
{"filename": "evaluation/agreement.py", "chunked_list": ["import sys\n\timport argparse\n\tsys.path.append('..')\n\ttry:\n\t    from ..utils import *\n\t    from .eval_boolqa import eval_qa\n\t    from .eval_constrained_generation import eval_negation, load_negation_detector\n\texcept:\n\t    from utils import *\n\t    from eval_boolqa import eval_qa\n", "    from eval_constrained_generation import eval_negation, load_negation_detector\n\tdef agreement(y_qa, y_cg, y_gt):\n\t    cnt = 0\n\t    agree = 0\n\t    cnt_pos = 0\n\t    agree_pos = 0\n\t    cnt_neg = 0\n\t    agree_neg = 0\n\t    for y1, y2, yt in zip(y_qa, y_cg, y_gt):\n\t        if y1 < 0:\n", "            continue\n\t        if yt == 1:\n\t            cnt_pos += 1\n\t        else:\n\t            cnt_neg += 1\n\t        if y1 == y2:\n\t            agree += 1\n\t            if yt == 1: # pos\n\t                agree_pos += 1\n\t            else:\n", "                agree_neg += 1\n\t        cnt += 1\n\t    return agree / cnt, agree_pos / cnt_pos, agree_neg / cnt_neg\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i', type=str)\n\t    parser.add_argument('--cg_model_name', '-g', type=str)\n\t    parser.add_argument('--qa_model_name', '-q', type=str)\n\t    parser.add_argument('--log', action='store_true')\n\t    args = parser.parse_args()\n", "    qa_score, y_qa = eval_qa(args.input_file, args.qa_model_name)\n\t    print(qa_score)\n\t    pipe = load_negation_detector()\n\t    neg_scores, y_cg = eval_negation(args.cg_model_name, args.input_file, pipe)\n\t    data = load_jsonl(args.input_file)\n\t    y_gt = [int('pos' in x['source_kb']) for x in data]\n\t    print(neg_scores)\n\t    agree, agree_pos, agree_neg = agreement(y_qa, y_cg, y_gt)\n\t    print(agree, agree_pos, agree_neg)\n\t    cjj.lark(f\"Agreement between [{args.cg_model_name}] and [{args.qa_model_name}]:\\nALL: {agree}, POS: {agree_pos}, NEG: {agree_neg}\\nCG score:\\n{neg_scores}\\nQA score:\\n{qa_score}\")\n", "    if args.log:\n\t        with open('metrics.log', 'a') as f:\n\t            f.write(f\"{args.cg_model_name}\\t{neg_scores}\\tALL: {agree}, POS: {agree_pos}, NEG: {agree_neg}\\n\")"]}
{"filename": "preprocessing/conceptnet_helper.py", "chunked_list": ["# From https://github.com/vered1986/self_talk\n\timport os\n\timport csv\n\timport gzip\n\timport json\n\timport tqdm\n\timport math\n\timport logging\n\timport itertools\n\timport numpy as np\n", "from operator import mul\n\tfrom functools import reduce\n\tfrom scipy.sparse import coo_matrix, dok_matrix\n\tfrom collections import defaultdict, namedtuple\n\tlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n\t                    datefmt = '%m/%d/%Y %H:%M:%S',\n\t                    level = logging.INFO)\n\tlogger = logging.getLogger(__name__)\n\tResource = namedtuple('Resource',\n\t                      'index2concept, concept2index, index2relation, relation2index, edges, cooc_mat')\n", "# we'll use infinity as a default distance to nodes.\n\tEdge = namedtuple('Edge', 'start, end, rel, cost')\n\t# Based on \"Commonsense Knowledge Mining from Pretrained Models\".\n\t# Joshua Feldman, Joe Davison, and Alexander M. Rush. 2019.\n\tREL_TO_TEMPLATE = {\n\t    \"RelatedTo\": \"[w1] is like [w2]\",\n\t    \"ExternalUrl\": \"[w1] is described at the following URL [w2]\",\n\t    \"FormOf\": \"[w1] is a form of the word [w2]\",\n\t    \"IsA\": \"[w1] is a type of [w2]\",\n\t    \"NotIsA\": \"[w1] is not [w2]\",\n", "    \"PartOf\": \"[w1] is part of [w2]\",\n\t    \"UsedFor\": \"[w1] is used for [w2]\",\n\t    \"CapableOf\": \"[w1] can [w2]\",\n\t    \"AtLocation\": \"You are likely to find [w1] in [w2]\",\n\t    \"Causes\": \"Sometimes [w1] causes [w2]\",\n\t    \"HasA\": \"[w1] has [w2]\",\n\t    \"HasSubevent\": \"Something you do when you [w1] is [w2]\",\n\t    \"HasFirstSubevent\": \"the first thing you do when you [w1] is [w2]\",\n\t    \"HasLastSubevent\": \"the last thing you do when you [w1] is [w2]\",\n\t    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs to happen\",\n", "    \"HasProperty\": \"[w1] is [w2]\",\n\t    \"HasContext\": \"[w1] is a word used in the context of [w2]\",\n\t    \"MotivatedByGoal\": \"You would [w1] because you want to [w2]\",\n\t    \"ObstructedBy\": \"[w1] can be prevented by [w2]\",\n\t    \"Desires\": \"[w1] wants [w2]\",\n\t    \"CreatedBy\": \"[w1] is created by [w2]\",\n\t    \"Synonym\": \"[w1] and [w2] have similar meanings\",\n\t    \"Antonym\": \"[w1] is the opposite of [w2]\",\n\t    \"DistinctFrom\": \"it cannot be both [w1] and [w2]\",\n\t    \"DerivedFrom\": \"the word [w1] is derived from the word [w2]\",\n", "    \"DefinedAs\": \"[w1] is defined as [w2]\",\n\t    \"Entails\": \"if [w1] is happening, [w2] is also happening\",\n\t    \"MannerOf\": \"[w1] is a specific way of doing [w2]\",\n\t    \"LocatedNear\": \"[w1] is located near [w2]\",\n\t    \"dbpedia\": \"[w1] is conceptually related to [w2]\",\n\t    \"SimilarTo\": \"[w1] is similar to [w2]\",\n\t    \"EtymologicallyRelatedTo\": \"the word [w1] and the word [w2] have the same origin\",\n\t    \"EtymologicallyDerivedFrom\": \"the word [w1] comes from the word [w2]\",\n\t    \"CausesDesire\": \"[w1] makes people want [w2]\",\n\t    \"MadeOf\": \"[w1] is made of [w2]\",\n", "    \"ReceivesAction\": \"[w1] can be [w2]\",\n\t    \"InstanceOf\": \"[w1] is an example of [w2]\",\n\t    \"NotDesires\": \"[w1] does not want [w2]\",\n\t    \"NotUsedFor\": \"[w1] is not used for [w2]\",\n\t    \"NotCapableOf\": \"[w1] is not capable of [w2]\",\n\t    \"NotHasProperty\": \"[w1] does not have the property of [w2]\",\n\t    \"NotMadeOf\": \"[w1] is not made of [w2]\"\n\t}\n\tLOWER_REL_TO_TEMPLATE = {k.lower(): v for k, v in REL_TO_TEMPLATE.items()}\n\tdef build_conceptnet(conceptnet_dir):\n", "    \"\"\"\n\t    Download ConceptNet and build it locally.\n\t    First run:\n\t    !wget https://s3.amazonaws.com/conceptnet/downloads/2019/edges/conceptnet-assertions-5.7.0.csv.gz \\\n\t        -O ~/resources/conceptnet-assertions-5.7.0.csv.gz\n\t    \"\"\"\n\t    # resource_dir = conceptnet_dir.replace(\"conceptnet\", \"\")\n\t    concept2index = defaultdict(itertools.count(0).__next__)\n\t    relation2index = defaultdict(itertools.count(0).__next__)\n\t    # concept -> concept -> relation = weight\n", "    edges = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n\t    with gzip.open(os.path.join(conceptnet_dir, 'conceptnet-assertions-5.7.0.csv.gz'), mode='rt') as f_in:\n\t        csvfile = csv.reader(f_in, delimiter='\\t', quotechar=\"'\")\n\t        for row in tqdm.tqdm(csvfile):\n\t            \"\"\"\n\t             Row format:\n\t                The URI of the whole edge\n\t                The relation expressed by the edge\n\t                The node at the start of the edge\n\t                The node at the end of the edge\n", "                A JSON structure of additional information about the edge, such as its weight\n\t            Example:\n\t                b'/a/[/r/Antonym/,/c/en/arise/,/c/en/repose/]\n\t                /r/Antonym\n\t                /c/en/arise\n\t                /c/en/repose\n\t                {\"dataset\": \"/d/verbosity\", \"license\": \"cc:by/4.0\", \n\t                \"sources\": [{\"contributor\": \"/s/resource/verbosity\"}], \n\t                \"surfaceEnd\": \"repose\", \"surfaceStart\": \"arise\", \n\t                \"surfaceText\": \"[[arise]] is the opposite of [[repose]]\", \"weight\": 0.15}\n", "            \"\"\"\n\t            relation, start, end = row[1:4]\n\t            # Keep only English concepts\n\t            if not start.startswith('/c/en') or not end.startswith('/c/en'):\n\t                continue\n\t            relation_label = os.path.basename(relation).lower()\n\t            edge_info = json.loads(row[-1])\n\t            start_label = edge_info.get('surfaceStart', '').lower().strip()\n\t            end_label = edge_info.get('surfaceEnd', '').lower().strip()\n\t            if len(start_label) > 0 and len(end_label) > 0:\n", "                weight = edge_info['weight']\n\t                start_index, end_index = concept2index[start_label], concept2index[end_label]\n\t                relation_index = relation2index[relation_label]\n\t                edges[start_index][end_index][relation_index] = weight\n\t        index2relation = {index: relation for relation, index in relation2index.items()}\n\t        with open(os.path.join(conceptnet_dir, 'relations.txt'), 'w', encoding='utf-8') as f_out:\n\t            for index in range(len(index2relation)):\n\t                f_out.write(index2relation[index] + '\\n')\n\t        index2concept = {index: concept for concept, index in concept2index.items()}\n\t        with open(os.path.join(conceptnet_dir, 'concepts.txt'), 'w', encoding='utf-8') as f_out:\n", "            for index in range(len(index2concept)):\n\t                f_out.write(index2concept[index] + '\\n')\n\t        with open(os.path.join(conceptnet_dir, 'edges.txt'), 'w', encoding='utf-8') as f_out:\n\t            for start_index in range(len(index2concept)):\n\t                f_out.write(json.dumps(edges[start_index]) + '\\n')\n\t        row_ind, col_ind, cooc_data = zip(*[(c1, c2, 1)\n\t                                            for c1, c1_relations in edges.items()\n\t                                            for c2 in c1_relations.keys()])\n\t        cooc_mat = coo_matrix((cooc_data, (row_ind, col_ind)),\n\t                              shape=(len(concept2index), len(concept2index)))\n", "        np.savez_compressed(os.path.join(conceptnet_dir, 'cooc.npz'),\n\t                            data=cooc_mat.data,\n\t                            row=cooc_mat.row,\n\t                            col=cooc_mat.col,\n\t                            shape=cooc_mat.shape)\n\tdef load_conceptnet(conceptnet_dir):\n\t    \"\"\"\n\t    Load an existing local ConceptNet from this directory\n\t    \"\"\"\n\t    with open(os.path.join(conceptnet_dir, 'concepts.txt'), 'r', encoding='utf-8') as f_in:\n", "        index2concept = [line.strip() for line in f_in]\n\t        concept2index = {c: i for i, c in enumerate(index2concept)}\n\t    with open(os.path.join(conceptnet_dir, 'relations.txt'), 'r', encoding='utf-8') as f_in:\n\t        index2relation = [line.strip() for line in f_in]\n\t        relation2index = {c: i for i, c in enumerate(index2relation)}\n\t    # concept -> concept -> relation = weight\n\t    edges = {}\n\t    with open(os.path.join(conceptnet_dir, 'edges.txt'), 'r', encoding='utf-8') as f_in:\n\t        for c1, line in enumerate(f_in):\n\t            edges[c1] = json.loads(line.strip())\n", "    edges = {int(c1): {\n\t        int(c2): {int(r): float(score) for r, score in relations.items()}\n\t        for c2, relations in c1_rs.items()}\n\t        for c1, c1_rs in edges.items()}\n\t    with np.load(os.path.join(conceptnet_dir, 'cooc.npz')) as loader:\n\t        cooc_mat = coo_matrix((loader['data'], (loader['row'], loader['col'])), shape=loader['shape'])\n\t    return Resource(*(index2concept, concept2index, index2relation,\n\t                      relation2index, edges, cooc_mat))\n\tclass NodesOnPathFinder:\n\t    \"\"\"\n", "    Applies bi-directional search to find the nodes in the shortest paths a pair of terms.\n\t    \"\"\"\n\t    def __init__(self, resource, include_reverse=True):\n\t        \"\"\"\n\t        Init the relevant nodes search\n\t        \"\"\"\n\t        self.adjacency_matrix = resource.cooc_mat\n\t        self.transposed_adjacency_matrix = resource.cooc_mat.T\n\t        # Include reversed relations\n\t        if include_reverse:\n", "            self.adjacency_matrix += self.transposed_adjacency_matrix\n\t            self.transposed_adjacency_matrix = self.adjacency_matrix\n\t    def find_nodes_on_path(self, x, y, max_length=5):\n\t        \"\"\"\n\t        Finds all nodes in the shortest paths between x and y\n\t        subject to the maximum length.\n\t        :param x -- the index of the first term\n\t        :param y -- the index of the second term\n\t        :param max_length -- the maximum path length\n\t        \"\"\"\n", "        m = self.adjacency_matrix\n\t        mT = self.transposed_adjacency_matrix\n\t        dim = m.shape[0]\n\t        n_r = create_one_hot_vector(x, dim)\n\t        n_g = create_one_hot_vector(y, dim)\n\t        return find_nodes(m, mT, n_r, n_g, max_length)\n\tdef find_nodes(m, mT, n_r, n_g, max_len):\n\t    \"\"\"\n\t    Finds all nodes in the shortest paths between x and y\n\t    subject to the maximum length.\n", "    :param m -- the adjacency matrix\n\t    :param mT -- the transposed adjacency matrix\n\t    :param n_r -- the one-hot vector representing the root node\n\t    :param n_g -- the one-hot vector representing the goal node\n\t    :param max_len -- the maximum path length\n\t    \"\"\"\n\t    nodes = set()\n\t    n_x = n_r\n\t    n_y = n_g\n\t    # Stop condition 1 - no paths\n", "    if max_len == 0:\n\t        return nodes\n\t    # Stop condition 2 - the two sides are connected by one edge.\n\t    # Notice that if max_length == 1, then this function will return the two\n\t    # nodes even if they are not connected - this path will be discarded\n\t    # in the second search phase.\n\t    if max_len == 1:\n\t        return set(n_r.nonzero()[1].flatten()).union(set(n_g.nonzero()[1].flatten()))\n\t    # Move one step in each direction until the root and goal meet\n\t    for l in range(max_len + 1):\n", "        # The root and goal met - apply recursively for each half of the path\n\t        if n_r.dot(n_g.T)[0, 0] > 0:\n\t            intersection = n_r.multiply(n_g)\n\t            forward = find_nodes(\n\t                m, mT, n_x, intersection, int(math.ceil((l + 1) / 2.0)))\n\t            backward = find_nodes(\n\t                m, mT, intersection, n_y, int(math.floor((l + 1) / 2.0)))\n\t            return forward.union(backward)\n\t        # Make a step forward\n\t        if l % 2 == 0:\n", "            n_r = n_r.dot(m)\n\t        # Make a step backward\n\t        else:\n\t            n_g = n_g.dot(mT)\n\t    return nodes\n\tdef create_one_hot_vector(x, dim):\n\t    \"\"\"\n\t    Creates the one-hot vector representing this node\n\t    :param x -- the node\n\t    :param dim -- the number of nodes (the adjacency matrix dimension)\n", "    \"\"\"\n\t    n_x = dok_matrix((1, dim), dtype=np.int16)\n\t    n_x[0, x] = 1\n\t    n_x = n_x.tocsr()\n\t    return n_x\n\tclass Graph:\n\t    def __init__(self, edges):\n\t        self.edges = [Edge(*edge) for edge in edges]\n\t    @property\n\t    def nodes(self):\n", "        return set(sum(([edge.start, edge.end] for edge in self.edges), []))\n\t    @property\n\t    def neighbours(self):\n\t        neighbours = {node: set() for node in self.nodes}\n\t        for edge in self.edges:\n\t            neighbours[edge.start].add((edge.end, edge.rel, edge.cost))\n\t        return neighbours\n\t    def bfs(self, start, goal):\n\t        \"\"\"\n\t        Get the shortest path from source to dest\n", "        \"\"\"\n\t        queue = [(start, [''], [start], [1.0])]\n\t        min_len_path = np.inf\n\t        paths = list()\n\t        while queue:\n\t            curr_node, edges_on_path, nodes_on_path, weights_on_path = queue.pop(0)\n\t            for next_node, rel, weight in self.neighbours.get(curr_node, set()):\n\t                if next_node in set(nodes_on_path):\n\t                    continue\n\t                if next_node == goal:\n", "                    curr_path = list(zip(edges_on_path, nodes_on_path, weights_on_path)) + [\n\t                        (rel, next_node, weight)]\n\t                    if len(curr_path) <= min_len_path:\n\t                        min_len_path = len(curr_path)\n\t                        path_weight = reduce(mul, weights_on_path, 1)\n\t                        paths.append((curr_path, path_weight))\n\t                    # Already found shorter paths\n\t                    else:\n\t                        return paths\n\t                else:\n", "                    queue.append((next_node,\n\t                                  edges_on_path + [rel],\n\t                                  nodes_on_path + [next_node],\n\t                                  weights_on_path + [weight]))\n\t        return paths\n\tdef shortest_paths(resource, c1, c2, max_length=10, exclude_relations=None):\n\t    \"\"\"\n\t    Return the shortest paths from c1 to c2, up to max_length edges,\n\t    optionally excluding some relations.\n\t    \"\"\"\n", "    nodes_finder = NodesOnPathFinder(resource, include_reverse=True)\n\t    c1_index = resource.concept2index.get(c1, None)\n\t    c2_index = resource.concept2index.get(c2, None)\n\t    if c1_index is None or c2_index is None:\n\t        logger.warning('{} not found'.format(c1 if c1_index is None else c2))\n\t        return [([], 0)]\n\t    # Find the nodes on the path\n\t    nodes = nodes_finder.find_nodes_on_path(c1_index, c2_index, max_length=max_length)\n\t    # Get all the edges between these nodes in the original graph\n\t    # Get the maximum weight for each start and end\n", "    curr_edges = {resource.index2concept[start]: {} for start in nodes}\n\t    for start, end in itertools.permutations(nodes, 2):\n\t        start_label = resource.index2concept[start]\n\t        end_label = resource.index2concept[end]\n\t        for relation, weight in resource.edges.get(start, {}).get(end, {}).items():\n\t            relation_label = resource.index2relation[relation]\n\t            if exclude_relations is None or relation_label not in exclude_relations:\n\t                if end_label not in curr_edges[start_label] or \\\n\t                        curr_edges[start_label][end_label][1] < weight:\n\t                    curr_edges[start_label][end_label] = (relation_label, weight)\n", "                if start_label not in curr_edges[end_label] or \\\n\t                        curr_edges[end_label][start_label][1] < weight:\n\t                    curr_edges[end_label][start_label] = (relation_label + '-1', weight)\n\t    # Create the subgraph and use Dijkstra to find the shortest weighted path\n\t    edge_list = [(start, end, rel, 1.0 / weight)\n\t                 for start, start_rels in curr_edges.items()\n\t                 for end, (rel, weight) in start_rels.items()]\n\t    graph = Graph(edge_list)\n\t    result = graph.bfs(c1, c2)\n\t    return result\n", "def pretty_print(path):\n\t    \"\"\"\n\t    Print a path in a readable format\n\t    param path: a list of (edge_label, node)\n\t    \"\"\"\n\t    path_str = ''\n\t    if len(path) > 0:\n\t        path_str += path[0][1]\n\t    for rel, node, _ in path[1:]:\n\t        if rel.endswith('-1'):\n", "            path_str += f' <--{rel[:-2]}-- {node}'\n\t        else:\n\t            path_str += f' --{rel}--> {node}'\n\t    return path_str\n\tdef to_natural_language(path):\n\t    \"\"\"\n\t    Print a path in a readable format\n\t    param path: a list of (edge_label, node)\n\t    \"\"\"\n\t    props = []\n", "    for (_, node1, _), (rel, node2, _) in zip(path, path[1:]):\n\t        w1, w2, rel = (node2, node1, rel.replace(\"-1\", \"\")) if rel.endswith('-1') else (node1, node2, rel)\n\t        props.append(LOWER_REL_TO_TEMPLATE[rel].replace(\"[w1]\", w1).replace(\"[w2]\", w2))\n\t    return \". \".join([p[0].upper() + p[1:] for p in props])"]}
{"filename": "preprocessing/cjjpy.py", "chunked_list": ["ï»¿# -*- coding: utf-8 -*-\n\t'''\n\t@Author : Jiangjie Chen\n\t@Time   : 2022/5/26 19:52\n\t@Contact: jjchen19@fudan.edu.cn\n\t'''\n\timport re\n\timport datetime\n\timport os\n\timport subprocess\n", "import urllib.request, urllib.parse\n\timport argparse\n\tfrom tqdm import tqdm\n\timport sqlite3\n\timport requests\n\timport socket\n\timport logging\n\timport io\n\timport traceback\n\ttry:\n", "    import ujson as json\n\texcept:\n\t    import json\n\tHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\tdef LengthStats(filename, key4json=None):\n\t    len_list = []\n\t    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n\t    with open(filename) as f:\n\t        for line in f:\n\t            if key4json not in ['none', None, 'None']:\n", "                len_list.append(len(json.loads(line)[key4json].split()))\n\t            else:\n\t                len_list.append(len(line.strip().split()))\n\t    stats = {\n\t        'Max': max(len_list),\n\t        'Min': min(len_list),\n\t        'Avg': round(sum(len_list) / len(len_list), 4),\n\t    }\n\t    len_list.sort()\n\t    for t in thresholds:\n", "        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\t    for k in stats:\n\t        print(f\"- {k}: {stats[k]}\")\n\t    return stats\n\tclass AttrDict(dict):\n\t    def __init__(self, *args, **kwargs):\n\t        super(AttrDict, self).__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\tdef TraceBack(error_msg):\n\t    exc = traceback.format_exc()\n", "    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n\t    return msg\n\tdef Now():\n\t    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\tdef TorchHLoad(filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if not filepath.startswith(\"hdfs://\"):\n\t        return torch.load(filepath, **kwargs)\n\t    else:\n\t        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n", "            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\tdef TorchHSave(obj, filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n\t        with tf.io.gfile.GFile(filepath, 'wb') as f:\n\t            buffer = io.BytesIO()\n\t            torch.save(obj, buffer, **kwargs)\n\t            f.write(buffer.getvalue())\n\t    else:\n\t        torch.save(obj, filepath, **kwargs)\n", "def PutHDFS(local: str, remote: str):\n\t    import tensorflow as tf\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    if not tf.io.gfile.exists(remote):\n\t        tf.io.gfile.makedirs(remote)\n\t    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\tdef GetHDFS(remote: str, local: str):\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    os.makedirs(local, exist_ok=True)\n\t    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n", "def RunCmd(command):\n\t    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t    res, err = pipe.communicate()\n\t    res = res.decode('utf-8')\n\t    err = err.decode('utf-8')\n\t    return res, err\n\tdef AbsParentDir(file, parent='..', postfix=None):\n\t    ppath = os.path.abspath(file)\n\t    parent_level = parent.count('.')\n\t    while parent_level > 0:\n", "        ppath = os.path.dirname(ppath)\n\t        parent_level -= 1\n\t    if postfix is not None:\n\t        return os.path.join(ppath, postfix)\n\t    else:\n\t        return ppath\n\tdef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n\t    from coloredlogs import ColoredFormatter\n\t    import tensorflow as tf\n\t    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n", "    log_format = ColoredFormatter(fmt=fmt)\n\t    # log_format = logging.Formatter()\n\t    logger = logging.getLogger()\n\t    logger.setLevel(log_file_level)\n\t    console_handler = logging.StreamHandler()\n\t    console_handler.setFormatter(log_format)\n\t    logger.handlers = [console_handler]\n\t    if log_file and log_file != '':\n\t        if from_scratch and tf.io.gfile.exists(log_file):\n\t            logger.warning('Removing previous log file: %s' % log_file)\n", "            tf.io.gfile.remove(log_file)\n\t        path = os.path.dirname(log_file)\n\t        os.makedirs(path, exist_ok=True)\n\t        file_handler = logging.FileHandler(log_file)\n\t        file_handler.setLevel(log_file_level)\n\t        file_handler.setFormatter(log_format)\n\t        logger.addHandler(file_handler)\n\t    return logger\n\tdef is_qid(x, hard=False):\n\t    if type(x) is not str: return None\n", "    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tdef is_pid(x, hard=False):\n\t    if type(x) is not str: return None\n\t    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tclass MiniLutDB:\n\t    def __init__(self, db, verbose=True):\n\t        self.db = db\n\t        self.conn = None\n", "        self.verbose = verbose\n\t    def dump_lut(self, lut_tuples, verbose=None):\n\t        # lut_tuple: (k, v)+, iterable\n\t        if verbose is None: \n\t            verbose = self.verbose\n\t        self.conn = sqlite3.connect(self.db)\n\t        cur = self.conn.cursor()\n\t        cur.executescript('''\n\t        DROP TABLE IF EXISTS lut;\n\t        CREATE TABLE lut (\n", "        id      TEXT PRIMARY KEY UNIQUE,\n\t        content TEXT)''')\n\t        self.conn.commit()\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n", "            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def update_lut(self, lut_tuples, verbose=None):\n\t        if verbose is None: \n\t            verbose = self.verbose\n", "        self.conn = sqlite3.connect(self.db)\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n\t            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n", "                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def create_index(self):\n\t        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        # sql = ('CREATE INDEX index_lut ON lut(id);')\n\t        # self.cur.execute(sql)\n", "        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n\t        self.conn.commit()\n\t    def get(self, x, default=None):\n\t        if x is None: return default\n\t        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        res = self.query_lut(self.cur, x, False)[0]\n\t        return res if res is not None else default\n\t    def get_chunk(self, xx):\n", "        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        return self.query_lut(self.conn, xx, self.verbose)\n\t    def close(self):\n\t        if self.conn is not None:\n\t            self.conn.close()\n\t            self.conn = None\n\t    def delete_sample(self, key, value=None):\n\t        if self.get(key) is None: return\n", "        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n\t        self.conn.commit()\n\t        assert self.get(key) is None, f'delete failed: {key}'\n\t    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n\t        values = []\n\t        if isinstance(keys, str): keys = [keys]\n\t        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n\t        for k in iter:\n", "            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n\t            val = cur.fetchone()\n\t            val = val[0] if val is not None else None\n\t            values.append(val)\n\t        return values\n\tdef OverWriteCjjPy(root='.'):\n\t    # import difflib\n\t    # diff = difflib.HtmlDiff()\n\t    cnt = 0\n\t    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n", "    # golden_content = open(golden_cjjpy).readlines()\n\t    for dir, folder, file in os.walk(root):\n\t        for f in file:\n\t            if f == 'cjjpy.py':\n\t                cjjpy = '%s/%s' % (dir, f)\n\t                # content = open(cjjpy).readlines()\n\t                # d = diff.make_file(golden_content, content)\n\t                cnt += 1\n\t                print('[%d]: %s' % (cnt, cjjpy))\n\t                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n", "def ReplaceChar(file, replaced, replacer):\n\t    print(file, replaced, replacer)\n\t    with open(file) as f:\n\t        data = f.readlines()\n\t        out = open(file, 'w')\n\t        for line in data:\n\t            out.write(line.replace(replaced, replacer))\n\tdef DeUnicode(line):\n\t    return line.encode('utf-8').decode('unicode_escape')\n\tdef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n", "    '''\n\t    a\\tb\\n, `.dict' file\n\t    '''\n\t    import tensorflow as tf\n\t    assert dict_file.endswith('.dict')\n\t    id2label = {}\n\t    with tf.io.gfile.GFile(dict_file, 'r') as f:\n\t        data = f.read().split('\\n')\n\t        for i, line in enumerate(data):\n\t            if line == '': continue\n", "            try:\n\t                id, label = line.split('\\t')\n\t                if reverse:\n\t                    id, label = label, id\n\t                _val = '_'.join(label.split()) if unify_words else label\n\t                id2label[id] = _val.lower() if lower else _val\n\t            except:\n\t                pass\n\t    return id2label\n\tdef LoadWords(file, is_file=True):\n", "    import tensorflow as tf\n\t    if is_file:\n\t        with tf.io.gfile.GFile(file, 'r') as f:\n\t            data = f.read().splitlines()\n\t    else:\n\t        data = file.splitlines()\n\t    return set(map(lambda x: x.strip(), data))\n\tdef ChangeFileFormat(filename, new_fmt):\n\t    assert type(filename) is str and type(new_fmt) is str\n\t    spt = filename.split('.')\n", "    if len(spt) == 0:\n\t        return filename\n\t    else:\n\t        return filename.replace('.' + spt[-1], new_fmt)\n\tdef CountLines(fname):\n\t    with open(fname, 'rb') as f:\n\t        count = 0\n\t        last_data = '\\n'\n\t        while True:\n\t            data = f.read(0x400000)\n", "            if not data:\n\t                break\n\t            count += data.count(b'\\n')\n\t            last_data = data\n\t        if last_data[-1:] != b'\\n':\n\t            count += 1  # Remove this if a wc-like count is needed\n\t    return count\n\tdef SearchByKey(file, key):\n\t    with open(file, 'r') as fin:\n\t        while True:\n", "            line = fin.readline()\n\t            if not line: break\n\t            if key in line:\n\t                print(line, end='')\n\tdef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n\t    from email.mime.text import MIMEText\n\t    import smtplib\n\t    # receivers got to be list.\n\t    mail_receivers = receivers\n\t    # mail_host = \"smtp.163.com\n", "    mail_host = \"220.181.12.18\"\n\t    mail_user = \"MichaelChen0110@163.com\"\n\t    mail_pass = \"\"\n\t    me = socket.gethostname() + \"<\" + mail_user + \">\"\n\t    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n\t    msg['Subject'] = subject\n\t    msg['From'] = me\n\t    msg['To'] = \";\".join(mail_receivers)\n\t    try:\n\t        server = smtplib.SMTP()\n", "        server.connect(mail_host)\n\t        server.login(mail_user, mail_pass)\n\t        server.sendmail(me, mail_receivers, msg.as_string())\n\t        server.close()\n\t        print('Have sent the email to ' + str(mail_receivers) + '. ')\n\t        return True\n\t    except Exception as e:\n\t        print(str(e))\n\t        return False\n\tdef SortDict(_dict, reverse=True):\n", "    assert type(_dict) is dict\n\t    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\tdef MaxCommLen(str1, str2):\n\t    lstr1 = len(str1)\n\t    lstr2 = len(str2)\n\t    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n\t    max_num = 0\n\t    for i in range(lstr1):\n\t        for j in range(lstr2):\n\t            if str1[i] == str2[j]:\n", "                record[i + 1][j + 1] = record[i][j] + 1\n\t                if record[i + 1][j + 1] > max_num:\n\t                    max_num = record[i + 1][j + 1]\n\t    return max_num, ''\n\tdef lark(content='test'):\n\t    print(content)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--diff', nargs=2,\n\t                        help='show difference between two files, shown in downloads/diff.html')\n", "    parser.add_argument('--de_unicode', action='store_true', default=False,\n\t                        help='remove unicode characters')\n\t    parser.add_argument('--link_entity', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--max_comm_len', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--search', nargs=2,\n\t                        help='search key from file, 2 args: file name & key')\n\t    parser.add_argument('--email', nargs=2,\n\t                        help='sending emails, 2 args: subject & content')\n", "    parser.add_argument('--overwrite', action='store_true', default=None,\n\t                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n\t    parser.add_argument('--replace', nargs=3,\n\t                        help='replace char, 3 args: file name & replaced char & replacer char')\n\t    parser.add_argument('--lark', nargs=1)\n\t    parser.add_argument('--get_hdfs', nargs=2,\n\t                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n\t    parser.add_argument('--put_hdfs', nargs=2,\n\t                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n\t    parser.add_argument('--length_stats', nargs=2,\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\t    args = parser.parse_args()\n\t    if args.overwrite:\n\t        print('* Overwriting cjjpy...')\n\t        OverWriteCjjPy()\n\t    if args.replace:\n\t        print('* Replacing Char...')\n\t        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\t    if args.search:\n\t        file = args.search[0]\n", "        key = args.search[1]\n\t        print('* Searching %s from %s...' % (key, file))\n\t        SearchByKey(file, key)\n\t    if args.email:\n\t        try:\n\t            subj = args.email[0]\n\t            cont = args.email[1]\n\t        except:\n\t            subj = 'running complete'\n\t            cont = ''\n", "        print('* Sending email {%s, %s} to host...' % (subj, cont))\n\t        SendEmail(subj, cont)\n\t    if args.lark:\n\t        try:\n\t            content = args.lark[0]\n\t        except:\n\t            content = 'running complete'\n\t        print(f'* Larking \"{content}\"...')\n\t        lark(content)\n\t    if args.get_hdfs:\n", "        remote = args.get_hdfs[0]\n\t        local = args.get_hdfs[1]\n\t        print(f'* Copying {remote} to {local}...')\n\t        GetHDFS(remote, local)\n\t    if args.put_hdfs:\n\t        local = args.put_hdfs[0]\n\t        remote = args.put_hdfs[1]\n\t        print(f'* Copying {local} to {remote}...')\n\t        PutHDFS(local, remote)\n\t    if args.length_stats:\n", "        file = args.length_stats[0]\n\t        key4json = args.length_stats[1]\n\t        print(f'* Working on {file} lengths statistics...')\n\t        LengthStats(file, key4json)\n"]}
{"filename": "preprocessing/make_conceptnet_negatives.py", "chunked_list": ["import os\n\timport ujson as json\n\tpronouns = ['me', 'i', 'I', 'her', 'she', 'him', 'he', 'us', 'we', 'them', 'they', 'it', 'its', 'my', 'your', 'his', 'her', 'their', 'our', 'mine', 'yours', 'hers', 'theirs', 'ours', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves', 'this', 'that', 'these', 'those', 'who', 'whom', 'whose', 'which', 'what', 'where', 'when', 'why', 'how', 'there', 'here', 'anybody', 'anyone', 'anything', 'each', 'either', 'everybody', 'everyone', 'everything', 'neither', 'nobody', 'none', 'nothing', 'one', 'other', 'others', 'somebody', 'someone', 'something', 'both', 'few', 'many', 'several', 'all', 'any', 'any', 'more', 'most', 'none', 'some', 'such']\n\tnegation_cue = ['nothing', \"no\", \"not\", \"never\", \"none\", \"hardly\", \"rarely\", \"scarcely\", \"seldom\", 'barely',\n\t                \"nor\", \"neither\", \"nothing\", \"nowhere\", \"without\", \"lack\", \"cant\", \"dont\", \"doesnt\",\n\t                \"doesn't\", \"don't\", \"isn't\", \"wasn't\", \"aren't\", \"weren't\", \"haven't\", \"hasn't\", \n\t                \"shouldn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"cannot\", \"unable\"]\n\tdef load_ids(file):\n\t    id_lut = {}\n\t    with open(file) as f:\n", "        for line in f.readlines():\n\t            i, text = line.strip().split('\\t')\n\t            id_lut[i] = text\n\t    return id_lut\n\tdef has_pronoun(s):\n\t    for p in pronouns:\n\t        if p.lower() in s.lower().split():\n\t            return True\n\t    return False\n\tdef has_negation(s):\n", "    for p in negation_cue:\n\t        if p in s.lower().split():\n\t            return True\n\t    return False\n\tdef load_triples(file, ent_lut, rel_lut, kb):\n\t    triples = []\n\t    with open(file) as f:\n\t        for line in f.readlines():\n\t            s, p, o = line.strip().split('\\t')\n\t            s_s = ent_lut[s]\n", "            p_s = rel_lut[p]\n\t            o_s = ent_lut[o]\n\t            if has_pronoun(s_s) or has_pronoun(o_s) or has_negation(s_s) or has_pronoun(o_s): \n\t                continue\n\t            if 'or' in o_s.split():\n\t                continue\n\t            triples.append({'subject': s_s, 'predicate': p_s, 'object': o_s, 'source_kb': kb})\n\t    print(f\"* {file} has {len(triples)} valid triples\")\n\t    return triples\n\tdef assemble(negater_dir, output_file):\n", "    rel_lut = load_ids(f'{negater_dir}/relation_ids.txt')\n\t    ent_lut = load_ids(f'{negater_dir}/entity_ids.txt')\n\t    neg_triples = load_triples(f'{negater_dir}/test_negatives.txt', ent_lut, rel_lut, 'conceptnet-neg-test')\n\t    neg_triples += load_triples(f'{negater_dir}/valid_negatives.txt', ent_lut, rel_lut, 'conceptnet-neg-valid')\n\t    pos_triples = load_triples(f'{negater_dir}/test.txt', ent_lut, rel_lut, 'conceptnet-pos-test')\n\t    pos_triples += load_triples(f'{negater_dir}/valid.txt', ent_lut, rel_lut, 'conceptnet-pos-valid')\n\t    min_size = min(len(pos_triples), len(neg_triples))\n\t    triples = pos_triples[:min_size] + neg_triples[:min_size]\n\t    print(f\"* total {len(triples)} triples\")\n\t    with open(output_file, 'w') as f:\n", "        for t in triples:\n\t            f.write(json.dumps(t) + '\\n')\n\tif __name__ == '__main__':\n\t    assemble(f'{os.environ[\"PJ_HOME\"]}/data/NegatER/data/conceptnet/true-neg', f'{os.environ[\"PJ_HOME\"]}/data/probe_datasets/true-neg.jsonl')"]}
{"filename": "preprocessing/remove_nested_keys_from_json.py", "chunked_list": ["import argparse\n\timport ujson as json\n\tdef remove_nested_keys(js, keys):\n\t    nested_keys = keys.split('/')\n\t    for k in nested_keys[:-1]:\n\t        js = js[k]\n\t    rm_key = nested_keys[-1]\n\t    if js.get(rm_key):\n\t        js.pop(rm_key)\n\t    else:\n", "        print(f'no key: {rm_key}')\n\tdef test():\n\t    s = {\"subject\":\"speed boat\",\"predicate\":\"HasProperty\",\"object\":\"dangerous\",\"source_kb\":\"conceptnet-pos-test\",\"boolq\":{\"text-davinci-002_ex-8\":\"is a speed boat dangerous?\"},\"cg_pred\":{\"text-davinci-002_ex-4p4n\":[\" speed boats can be dangerous.\"],\"text-davinci-002_ex-2p2n\":[\" speed boats are dangerous.\"],\"text-davinci-002_ex-2p6n\":[\" speed boats are not dangerous.\"],\"text-davinci-002_ex-6p2n\":[\"\\n\\nspeed boats are dangerous.\"],\"text-davinci-002_ex-1p7n\":[\" speed boats can be dangerous.\"],\"text-davinci-002_ex-0p8n\":[\" speed boats are not dangerous.\"],\"text-davinci-002_ex-8p8n\":[\"speed boats can be dangerous.\\n###\\nKeywords: dollar, used to, buy things\\n\\nDollars are used to buy things.\"]},\"qa_pred\":{\"text-davinci-002_ex-4p4n\":\"yes\",\"text-davinci-002_ex-2p2n\":\"yes\",\"text-davinci-002_ex-2p6n\":\"yes\",\"text-davinci-002_ex-6p2n\":\"yes\"}}\n\t    for k in ['cg_pred/text-davinci-002_ex-8p8n', 'cg_pred/text-davinci-002_ex-6p2n', 'cg_pred/text-davinci-002_ex-0p8den']:\n\t        remove_nested_keys(s, k)\n\t    print(s)\n\t    # print(ss)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-i', type=str, required=True)\n", "    parser.add_argument('-o', type=str)\n\t    parser.add_argument('-k', type=str, help='nested keys, e.g. aaa/bbb/ccc')\n\t    args = parser.parse_args()\n\t    new_jsl = []\n\t    with open(args.i) as f:\n\t        for line in f.readlines():\n\t            try:\n\t                js = json.loads(line)\n\t            except:\n\t                print(line)\n", "                raise ValueError\n\t            remove_nested_keys(js, args.k)\n\t            new_jsl.append(js)\n\t    output_file = args.i if args.o is None else args.o\n\t    with open(output_file, 'w') as fo:\n\t        for js in new_jsl:\n\t            fo.write(json.dumps(js) + '\\n')"]}
{"filename": "preprocessing/calculate_cooccurrence.py", "chunked_list": ["import os\n\timport ujson as json\n\tfrom tqdm import tqdm\n\timport cjjpy as cjj\n\tfrom multiprocessing import Pool\n\tstopwords = cjj.LoadWords(f\"{os.environ['PJ_HOME']}/preprocessing/stopwords.txt\")\n\tdef load_sentences():\n\t    prefix = f\"{os.environ['PJ_HOME']}/data/corpus\"\n\t    sents = []    \n\t    with open(f'{prefix}/omcs_sentences.txt') as f:\n", "        sents += f.read().splitlines()\n\t    with open(f'{prefix}/wiki1m_for_simcse.txt') as f:\n\t        sents += f.read().splitlines()\n\t    sents = [x.lower() for x in sents]\n\t    return sents \n\tdef cooccur_cnt(js):\n\t    hit = 0\n\t    w1 = js['subject'].lower().split()\n\t    w2 = js['object'].lower().split()\n\t    pairs = [(x, y) for x in w1 for y in w2 if x not in stopwords and y not in stopwords]\n", "    for sent in sents:\n\t        for p1, p2 in pairs:\n\t            if p1 in sent and p2 in sent:\n\t                hit += 1\n\t    if pairs == []: hit = 0\n\t    js['cooccur_cnt'] = hit / len(pairs)\n\t    js['cooccur_total'] = hit\n\t    return js\n\tdef callback(x):\n\t    bar.update(1)\n", "    fw.write(json.dumps(x) + '\\n')\n\tif __name__ == \"__main__\":\n\t    sents = load_sentences()\n\t    with open(f'{os.environ[\"PJ_HOME\"]}/data/probe_datasets/true-neg-llm_test.clean.jsonl') as f:\n\t        data = f.readlines()\n\t        print(len(data))\n\t    fw = open(f'{os.environ[\"PJ_HOME\"]}/data/probe_datasets/true-neg-llm_test.clean.cooccur.jsonl', 'w')\n\t    p = Pool(16)\n\t    bar = tqdm(total=len(data))\n\t    for line in data:\n", "        js = json.loads(line)\n\t        p.apply_async(cooccur_cnt, (js,), callback=callback)\n\t    p.close()\n\t    p.join()\n\t    fw.close()\n"]}
{"filename": "constrained_generation/cjjpy.py", "chunked_list": ["ï»¿# -*- coding: utf-8 -*-\n\t'''\n\t@Author : Jiangjie Chen\n\t@Time   : 2022/5/26 19:52\n\t@Contact: jjchen19@fudan.edu.cn\n\t'''\n\timport re\n\timport datetime\n\timport os\n\timport subprocess\n", "import urllib.request, urllib.parse\n\timport argparse\n\tfrom tqdm import tqdm\n\timport sqlite3\n\timport requests\n\timport socket\n\timport logging\n\timport io\n\timport traceback\n\ttry:\n", "    import ujson as json\n\texcept:\n\t    import json\n\tHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\tdef LengthStats(filename, key4json=None):\n\t    len_list = []\n\t    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n\t    with open(filename) as f:\n\t        for line in f:\n\t            if key4json not in ['none', None, 'None']:\n", "                len_list.append(len(json.loads(line)[key4json].split()))\n\t            else:\n\t                len_list.append(len(line.strip().split()))\n\t    stats = {\n\t        'Max': max(len_list),\n\t        'Min': min(len_list),\n\t        'Avg': round(sum(len_list) / len(len_list), 4),\n\t    }\n\t    len_list.sort()\n\t    for t in thresholds:\n", "        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\t    for k in stats:\n\t        print(f\"- {k}: {stats[k]}\")\n\t    return stats\n\tclass AttrDict(dict):\n\t    def __init__(self, *args, **kwargs):\n\t        super(AttrDict, self).__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\tdef TraceBack(error_msg):\n\t    exc = traceback.format_exc()\n", "    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n\t    return msg\n\tdef Now():\n\t    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\tdef TorchHLoad(filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if not filepath.startswith(\"hdfs://\"):\n\t        return torch.load(filepath, **kwargs)\n\t    else:\n\t        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n", "            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\tdef TorchHSave(obj, filepath: str, **kwargs):\n\t    import torch, tensorflow as tf\n\t    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n\t        with tf.io.gfile.GFile(filepath, 'wb') as f:\n\t            buffer = io.BytesIO()\n\t            torch.save(obj, buffer, **kwargs)\n\t            f.write(buffer.getvalue())\n\t    else:\n\t        torch.save(obj, filepath, **kwargs)\n", "def PutHDFS(local: str, remote: str):\n\t    import tensorflow as tf\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    if not tf.io.gfile.exists(remote):\n\t        tf.io.gfile.makedirs(remote)\n\t    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\tdef GetHDFS(remote: str, local: str):\n\t    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n\t    os.makedirs(local, exist_ok=True)\n\t    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n", "def RunCmd(command):\n\t    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t    res, err = pipe.communicate()\n\t    res = res.decode('utf-8')\n\t    err = err.decode('utf-8')\n\t    return res, err\n\tdef AbsParentDir(file, parent='..', postfix=None):\n\t    ppath = os.path.abspath(file)\n\t    parent_level = parent.count('.')\n\t    while parent_level > 0:\n", "        ppath = os.path.dirname(ppath)\n\t        parent_level -= 1\n\t    if postfix is not None:\n\t        return os.path.join(ppath, postfix)\n\t    else:\n\t        return ppath\n\tdef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n\t    from coloredlogs import ColoredFormatter\n\t    import tensorflow as tf\n\t    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n", "    log_format = ColoredFormatter(fmt=fmt)\n\t    # log_format = logging.Formatter()\n\t    logger = logging.getLogger()\n\t    logger.setLevel(log_file_level)\n\t    console_handler = logging.StreamHandler()\n\t    console_handler.setFormatter(log_format)\n\t    logger.handlers = [console_handler]\n\t    if log_file and log_file != '':\n\t        if from_scratch and tf.io.gfile.exists(log_file):\n\t            logger.warning('Removing previous log file: %s' % log_file)\n", "            tf.io.gfile.remove(log_file)\n\t        path = os.path.dirname(log_file)\n\t        os.makedirs(path, exist_ok=True)\n\t        file_handler = logging.FileHandler(log_file)\n\t        file_handler.setLevel(log_file_level)\n\t        file_handler.setFormatter(log_format)\n\t        logger.addHandler(file_handler)\n\t    return logger\n\tdef is_qid(x, hard=False):\n\t    if type(x) is not str: return None\n", "    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tdef is_pid(x, hard=False):\n\t    if type(x) is not str: return None\n\t    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n\t    return None if len(ret) == 0 else ret[0]\n\tclass MiniLutDB:\n\t    def __init__(self, db, verbose=True):\n\t        self.db = db\n\t        self.conn = None\n", "        self.verbose = verbose\n\t    def dump_lut(self, lut_tuples, verbose=None):\n\t        # lut_tuple: (k, v)+, iterable\n\t        if verbose is None: \n\t            verbose = self.verbose\n\t        self.conn = sqlite3.connect(self.db)\n\t        cur = self.conn.cursor()\n\t        cur.executescript('''\n\t        DROP TABLE IF EXISTS lut;\n\t        CREATE TABLE lut (\n", "        id      TEXT PRIMARY KEY UNIQUE,\n\t        content TEXT)''')\n\t        self.conn.commit()\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n", "            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def update_lut(self, lut_tuples, verbose=None):\n\t        if verbose is None: \n\t            verbose = self.verbose\n", "        self.conn = sqlite3.connect(self.db)\n\t        BLOCKSIZE = 100000\n\t        block = []\n\t        i = 0\n\t        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n\t        for x in iter:\n\t            block.append(x)\n\t            i += 1\n\t            if i == BLOCKSIZE:\n\t                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n", "                block = []\n\t                i = 0\n\t        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n\t        self.conn.commit()\n\t        self.close()\n\t    def create_index(self):\n\t        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        # sql = ('CREATE INDEX index_lut ON lut(id);')\n\t        # self.cur.execute(sql)\n", "        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n\t        self.conn.commit()\n\t    def get(self, x, default=None):\n\t        if x is None: return default\n\t        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        res = self.query_lut(self.cur, x, False)[0]\n\t        return res if res is not None else default\n\t    def get_chunk(self, xx):\n", "        if self.conn is None:\n\t            self.conn = sqlite3.connect(self.db)\n\t            self.cur = self.conn.cursor()\n\t        return self.query_lut(self.conn, xx, self.verbose)\n\t    def close(self):\n\t        if self.conn is not None:\n\t            self.conn.close()\n\t            self.conn = None\n\t    def delete_sample(self, key, value=None):\n\t        if self.get(key) is None: return\n", "        self.conn = sqlite3.connect(self.db)\n\t        self.cur = self.conn.cursor()\n\t        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n\t        self.conn.commit()\n\t        assert self.get(key) is None, f'delete failed: {key}'\n\t    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n\t        values = []\n\t        if isinstance(keys, str): keys = [keys]\n\t        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n\t        for k in iter:\n", "            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n\t            val = cur.fetchone()\n\t            val = val[0] if val is not None else None\n\t            values.append(val)\n\t        return values\n\tdef OverWriteCjjPy(root='.'):\n\t    # import difflib\n\t    # diff = difflib.HtmlDiff()\n\t    cnt = 0\n\t    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n", "    # golden_content = open(golden_cjjpy).readlines()\n\t    for dir, folder, file in os.walk(root):\n\t        for f in file:\n\t            if f == 'cjjpy.py':\n\t                cjjpy = '%s/%s' % (dir, f)\n\t                # content = open(cjjpy).readlines()\n\t                # d = diff.make_file(golden_content, content)\n\t                cnt += 1\n\t                print('[%d]: %s' % (cnt, cjjpy))\n\t                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n", "def ReplaceChar(file, replaced, replacer):\n\t    print(file, replaced, replacer)\n\t    with open(file) as f:\n\t        data = f.readlines()\n\t        out = open(file, 'w')\n\t        for line in data:\n\t            out.write(line.replace(replaced, replacer))\n\tdef DeUnicode(line):\n\t    return line.encode('utf-8').decode('unicode_escape')\n\tdef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n", "    '''\n\t    a\\tb\\n, `.dict' file\n\t    '''\n\t    import tensorflow as tf\n\t    assert dict_file.endswith('.dict')\n\t    id2label = {}\n\t    with tf.io.gfile.GFile(dict_file, 'r') as f:\n\t        data = f.read().split('\\n')\n\t        for i, line in enumerate(data):\n\t            if line == '': continue\n", "            try:\n\t                id, label = line.split('\\t')\n\t                if reverse:\n\t                    id, label = label, id\n\t                _val = '_'.join(label.split()) if unify_words else label\n\t                id2label[id] = _val.lower() if lower else _val\n\t            except:\n\t                pass\n\t    return id2label\n\tdef LoadWords(file, is_file=True):\n", "    import tensorflow as tf\n\t    if is_file:\n\t        with tf.io.gfile.GFile(file, 'r') as f:\n\t            data = f.read().splitlines()\n\t    else:\n\t        data = file.splitlines()\n\t    return set(map(lambda x: x.strip(), data))\n\tdef ChangeFileFormat(filename, new_fmt):\n\t    assert type(filename) is str and type(new_fmt) is str\n\t    spt = filename.split('.')\n", "    if len(spt) == 0:\n\t        return filename\n\t    else:\n\t        return filename.replace('.' + spt[-1], new_fmt)\n\tdef CountLines(fname):\n\t    with open(fname, 'rb') as f:\n\t        count = 0\n\t        last_data = '\\n'\n\t        while True:\n\t            data = f.read(0x400000)\n", "            if not data:\n\t                break\n\t            count += data.count(b'\\n')\n\t            last_data = data\n\t        if last_data[-1:] != b'\\n':\n\t            count += 1  # Remove this if a wc-like count is needed\n\t    return count\n\tdef SearchByKey(file, key):\n\t    with open(file, 'r') as fin:\n\t        while True:\n", "            line = fin.readline()\n\t            if not line: break\n\t            if key in line:\n\t                print(line, end='')\n\tdef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n\t    from email.mime.text import MIMEText\n\t    import smtplib\n\t    # receivers got to be list.\n\t    mail_receivers = receivers\n\t    # mail_host = \"smtp.163.com\n", "    mail_host = \"220.181.12.18\"\n\t    mail_user = \"MichaelChen0110@163.com\"\n\t    mail_pass = \"\"\n\t    me = socket.gethostname() + \"<\" + mail_user + \">\"\n\t    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n\t    msg['Subject'] = subject\n\t    msg['From'] = me\n\t    msg['To'] = \";\".join(mail_receivers)\n\t    try:\n\t        server = smtplib.SMTP()\n", "        server.connect(mail_host)\n\t        server.login(mail_user, mail_pass)\n\t        server.sendmail(me, mail_receivers, msg.as_string())\n\t        server.close()\n\t        print('Have sent the email to ' + str(mail_receivers) + '. ')\n\t        return True\n\t    except Exception as e:\n\t        print(str(e))\n\t        return False\n\tdef SortDict(_dict, reverse=True):\n", "    assert type(_dict) is dict\n\t    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\tdef MaxCommLen(str1, str2):\n\t    lstr1 = len(str1)\n\t    lstr2 = len(str2)\n\t    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n\t    max_num = 0\n\t    for i in range(lstr1):\n\t        for j in range(lstr2):\n\t            if str1[i] == str2[j]:\n", "                record[i + 1][j + 1] = record[i][j] + 1\n\t                if record[i + 1][j + 1] > max_num:\n\t                    max_num = record[i + 1][j + 1]\n\t    return max_num, ''\n\tdef lark(content='test'):\n\t    print(content)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--diff', nargs=2,\n\t                        help='show difference between two files, shown in downloads/diff.html')\n", "    parser.add_argument('--de_unicode', action='store_true', default=False,\n\t                        help='remove unicode characters')\n\t    parser.add_argument('--link_entity', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--max_comm_len', action='store_true', default=False,\n\t                        help='')\n\t    parser.add_argument('--search', nargs=2,\n\t                        help='search key from file, 2 args: file name & key')\n\t    parser.add_argument('--email', nargs=2,\n\t                        help='sending emails, 2 args: subject & content')\n", "    parser.add_argument('--overwrite', action='store_true', default=None,\n\t                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n\t    parser.add_argument('--replace', nargs=3,\n\t                        help='replace char, 3 args: file name & replaced char & replacer char')\n\t    parser.add_argument('--lark', nargs=1)\n\t    parser.add_argument('--get_hdfs', nargs=2,\n\t                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n\t    parser.add_argument('--put_hdfs', nargs=2,\n\t                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n\t    parser.add_argument('--length_stats', nargs=2,\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\t    args = parser.parse_args()\n\t    if args.overwrite:\n\t        print('* Overwriting cjjpy...')\n\t        OverWriteCjjPy()\n\t    if args.replace:\n\t        print('* Replacing Char...')\n\t        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\t    if args.search:\n\t        file = args.search[0]\n", "        key = args.search[1]\n\t        print('* Searching %s from %s...' % (key, file))\n\t        SearchByKey(file, key)\n\t    if args.email:\n\t        try:\n\t            subj = args.email[0]\n\t            cont = args.email[1]\n\t        except:\n\t            subj = 'running complete'\n\t            cont = ''\n", "        print('* Sending email {%s, %s} to host...' % (subj, cont))\n\t        SendEmail(subj, cont)\n\t    if args.lark:\n\t        try:\n\t            content = args.lark[0]\n\t        except:\n\t            content = 'running complete'\n\t        print(f'* Larking \"{content}\"...')\n\t        lark(content)\n\t    if args.get_hdfs:\n", "        remote = args.get_hdfs[0]\n\t        local = args.get_hdfs[1]\n\t        print(f'* Copying {remote} to {local}...')\n\t        GetHDFS(remote, local)\n\t    if args.put_hdfs:\n\t        local = args.put_hdfs[0]\n\t        remote = args.put_hdfs[1]\n\t        print(f'* Copying {local} to {remote}...')\n\t        PutHDFS(local, remote)\n\t    if args.length_stats:\n", "        file = args.length_stats[0]\n\t        key4json = args.length_stats[1]\n\t        print(f'* Working on {file} lengths statistics...')\n\t        LengthStats(file, key4json)\n"]}
{"filename": "constrained_generation/llm_constrained_generation.py", "chunked_list": ["import sys\n\timport cjjpy as cjj\n\tsys.path.append(cjj.AbsParentDir(__file__, '..'))\n\tfrom gpt3_helper import prompt_gpt3, calc_cost_w_prompt\n\t# from flant5_helper import prompt_flant5\n\tfrom utils import load_jsonl, chunks_list_first\n\tfrom llm_utils import (\n\t    save_llm_results, \n\t    prepare_prompt,\n\t    TASK_KEY\n", ")\n\tdef llm_constrained_generation(model_name, input_file, output_file=None, key_q='keywords', k_pos_ex=2, k_neg_ex=2, \n\t                               n_repeat=1, temperature=0.9, batch_size=8, cot='none'):\n\t    data = load_jsonl(input_file)\n\t    prompts = []\n\t    for js in data:\n\t        prompt = prepare_prompt('cg', js, k_pos_ex, k_neg_ex, key_q, cot)\n\t        prompts.append(prompt)\n\t    print(prompts[0])\n\t    num_cot_tokens = 64 if cot is not None else 0   # TODO: approximate number of cot tokens\n", "    if 'flan-t5' in model_name:\n\t        prompt_func = prompt_flant5\n\t    else:\n\t        prompt_func = prompt_gpt3\n\t    res, money = prompt_func(prompts, model_name=model_name, clean=True, temperature=temperature, \n\t                            max_tokens=30 + num_cot_tokens, verbose=True, batch_size=batch_size, n=n_repeat)\n\t    y_pred = []\n\t    for i, (ny, indiv_prompt) in enumerate(zip(chunks_list_first(res, n=n_repeat), prompts)):\n\t        # handle n_returned sequences\n\t        if i == 0: print(indiv_prompt + ny[0])\n", "        y_pred.append(ny)\n\t    if args.instruct_id == 0:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n\t    else:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_i{args.instruct_id}\" # TODO: hard-coded\n\t    if args.temperature == 0:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n\t    else:\n\t        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_t{args.temperature}\" # TODO: hard-coded\n\t    input_type = 'keywords' if key_q == 'keywords' else 'question'\n", "    task_key = TASK_KEY['cg'][input_type]\n\t    if cot != 'none':\n\t        task_key += f'_{cot}'\n\t    # save into file, override previous model key.\n\t    save_llm_results(input_file, y_pred, task_key, model_key, output_file)\n\t    if 'bloom' not in model_name or 'flan-t5' not in model_name:\n\t        cjj.lark(f\"This run has cost you {round(money, 2)}$: {task_key}/{model_key}.\")\n\t    return f\"{task_key}/{model_key}\"\n\tif __name__ == \"__main__\":\n\t    import argparse\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--input_file', '-i', type=str, required=True)\n\t    parser.add_argument('--model_name', '-m', type=str, required=True, \n\t                        choices=['flan-t5-large', 'flan-t5-xl', 'flan-t5-xxl',\n\t                                 'davinci', 'curie', 'babbage', 'ada', \n\t                                 'text-davinci-001', 'text-curie-001', 'text-babbage-001', 'text-ada-001',\n\t                                 'text-davinci-002', 'text-davinci-003', 'code-davinci-002'])\n\t    parser.add_argument('--posk', type=int, help='Number of positive examples in the demonstration.')\n\t    parser.add_argument('--negk', type=int, help='Number of negative examples in the demonstration.')\n\t    parser.add_argument('--output_file', '-o', type=str, required=True)\n", "    parser.add_argument('--input_key', '-q', default='keywords', \n\t                        choices=['rule', 'text-davinci-002_ex-8', 'keywords'], type=str,\n\t                        help='Key for input in the jsonline file. Keywords input by default for CG task')\n\t    parser.add_argument('--n_repeat', '-n', type=int, default=1)\n\t    parser.add_argument('--temperature', type=float, default=0.0)\n\t    parser.add_argument('--batch_size', '-b', type=int, default=16)\n\t    parser.add_argument('--cot', type=str, choices=['fact', 'logic', 'none'], default='none')\n\t    parser.add_argument('--instruct_id', type=int, default=0, \n\t                        help='For testing different instructions. Use the first instruction by default.')\n\t    parser.add_argument(\"--local_rank\", required=False, type=int, help=\"used by dist launchers\")\n", "    args = parser.parse_args()\n\t    filename = llm_constrained_generation(args.model_name, args.input_file, args.output_file, key_q=args.input_key,\n\t                                          k_pos_ex=args.posk, k_neg_ex=args.negk, n_repeat=args.n_repeat, \n\t                                          temperature=args.temperature, batch_size=args.batch_size, cot=args.cot)"]}
