{"filename": "spoolman/main.py", "chunked_list": ["\"\"\"Main entrypoint to the server.\"\"\"\n\timport logging\n\timport os\n\timport subprocess\n\tfrom logging.handlers import TimedRotatingFileHandler\n\tfrom pathlib import Path\n\tfrom typing import Union\n\timport uvicorn\n\tfrom fastapi import FastAPI\n\tfrom fastapi.middleware.cors import CORSMiddleware\n", "from fastapi.middleware.gzip import GZipMiddleware\n\tfrom fastapi.staticfiles import StaticFiles\n\tfrom scheduler.asyncio.scheduler import Scheduler\n\tfrom spoolman import env\n\tfrom spoolman.api.v1.router import app as v1_app\n\tfrom spoolman.database import database\n\t# Define a file logger with log rotation\n\tlog_file = env.get_data_dir().joinpath(\"spoolman.log\")\n\tfile_handler = TimedRotatingFileHandler(log_file, when=\"midnight\", backupCount=5)\n\tfile_handler.setFormatter(logging.Formatter(\"%(asctime)s:%(levelname)s:%(message)s\", \"%Y-%m-%d %H:%M:%S\"))\n", "# Define a console logger\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setFormatter(logging.Formatter(\"%(name)-26s %(levelname)-8s %(message)s\"))\n\t# Setup the spoolman logger, which all spoolman modules will use\n\troot_logger = logging.getLogger()\n\troot_logger.setLevel(env.get_logging_level())\n\troot_logger.addHandler(file_handler)\n\troot_logger.addHandler(console_handler)\n\t# Get logger instance for this module\n\tlogger = logging.getLogger(__name__)\n", "# Setup FastAPI\n\tclass SinglePageApplication(StaticFiles):\n\t    \"\"\"Serve a single page application.\"\"\"\n\t    def __init__(self, directory: str) -> None:\n\t        \"\"\"Construct.\"\"\"\n\t        super().__init__(directory=directory, packages=None, html=True, check_dir=True)\n\t    def lookup_path(self, path: str) -> tuple[str, Union[os.stat_result, None]]:\n\t        \"\"\"Return index.html if the requested file cannot be found.\"\"\"\n\t        full_path, stat_result = super().lookup_path(path)\n\t        if stat_result is None:\n", "            return super().lookup_path(\"index.html\")\n\t        return (full_path, stat_result)\n\tapp = FastAPI(\n\t    debug=env.is_debug_mode(),\n\t    title=\"Spoolman\",\n\t    version=env.get_version(),\n\t)\n\tapp.add_middleware(GZipMiddleware)\n\tapp.mount(\"/api/v1\", v1_app)\n\tapp.mount(\"/\", app=SinglePageApplication(directory=\"client/dist\"), name=\"client\")\n", "# Allow all origins if in debug mode\n\tif env.is_debug_mode():\n\t    logger.warning(\"Running in debug mode, allowing all origins.\")\n\t    app.add_middleware(\n\t        CORSMiddleware,\n\t        allow_origins=[\"*\"],\n\t        allow_credentials=True,\n\t        allow_methods=[\"*\"],\n\t        allow_headers=[\"*\"],\n\t    )\n", "@app.on_event(\"startup\")\n\tasync def startup() -> None:\n\t    \"\"\"Run the service's startup sequence.\"\"\"\n\t    logger.info(\"Starting Spoolman v%s...\", app.version)\n\t    logger.info(\"Setting up database...\")\n\t    database.setup_db(database.get_connection_url())\n\t    logger.info(\"Performing migrations...\")\n\t    # Run alembic in a subprocess.\n\t    # There is some issue with the uvicorn worker that causes the process to hang when running alembic directly.\n\t    # See: https://github.com/sqlalchemy/alembic/discussions/1155\n", "    project_root = Path(__file__).parent.parent\n\t    subprocess.run([\"alembic\", \"upgrade\", \"head\"], check=True, cwd=project_root)  # noqa: S603, S607, ASYNC101\n\t    # Setup scheduler\n\t    schedule = Scheduler()\n\t    database.schedule_tasks(schedule)\n\t    logger.info(\"Startup complete.\")\n\tif __name__ == \"__main__\":\n\t    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"]}
{"filename": "spoolman/docs.py", "chunked_list": ["\"\"\"Functions for generating documentation.\"\"\"\n\timport json\n\timport logging\n\tfrom pathlib import Path\n\tfrom typing import Any\n\tfrom fastapi import FastAPI\n\tfrom fastapi.openapi.utils import get_openapi\n\tfrom spoolman.api.v1.router import app as v1_app\n\tlogger = logging.getLogger(__name__)\n\tlogger.setLevel(logging.INFO)\n", "logger.addHandler(logging.StreamHandler())  # Print all log messages to stdout\n\tdef generate_openapi(app: FastAPI) -> dict[str, Any]:\n\t    \"\"\"Generate the OpenAPI document for a specific FastAPI app.\n\t    Args:\n\t        app (FastAPI): The FastAPI app.\n\t    Returns:\n\t        dict[str, Any]: The OpenAPI document.\n\t    \"\"\"\n\t    return get_openapi(\n\t        title=app.title,\n", "        version=app.version,\n\t        openapi_version=app.openapi_version,\n\t        description=app.description,\n\t        routes=app.routes,\n\t        contact=app.contact,\n\t        license_info=app.license_info,\n\t        servers=app.servers,\n\t        tags=app.openapi_tags,\n\t        terms_of_service=app.terms_of_service,\n\t    )\n", "def generate_docs() -> None:\n\t    \"\"\"Generate documentation for this service in the docs/ directory.\"\"\"\n\t    target_dir = Path(\"docs\")\n\t    logger.info('Generating documentation to \"%s\"...', target_dir.resolve())\n\t    target_dir.mkdir(parents=True, exist_ok=True)\n\t    spec = json.dumps(generate_openapi(v1_app))\n\t    with target_dir.joinpath(\"index.html\").open(\"w\") as f:\n\t        f.write(\n\t            f\"\"\"\n\t        <!DOCTYPE html>\n", "        <html>\n\t        <head>\n\t        <title>Spoolman REST API v1 - ReDoc</title>\n\t        <meta charset=\"utf-8\"/>\n\t        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\t        <link href=\"https://fonts.googleapis.com/css?family=Montserrat:300,400,700|Roboto:300,400,700\" rel=\"stylesheet\">\n\t        <link rel=\"shortcut icon\" href=\"https://fastapi.tiangolo.com/img/favicon.png\">\n\t        <style> body {{margin: 0; padding: 0; }} </style>\n\t        </head>\n\t        <body>\n", "            <div id=\"redoc-container\"></div>\n\t            <script src=\"https://cdn.jsdelivr.net/npm/redoc/bundles/redoc.standalone.js\"> </script>\n\t            <script>\n\t                var spec = {spec};\n\t                Redoc.init(spec, {{}}, document.getElementById(\"redoc-container\"));\n\t            </script>\n\t        </body>\n\t        </html>\"\"\",\n\t        )\n\t    logger.info(\"Documentation generated successfully.\")\n"]}
{"filename": "spoolman/__init__.py", "chunked_list": []}
{"filename": "spoolman/env.py", "chunked_list": ["\"\"\"Utilities for grabbing config from environment variables.\"\"\"\n\timport logging\n\timport os\n\tfrom enum import Enum\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\tfrom urllib import parse\n\timport pkg_resources\n\tfrom platformdirs import user_data_dir\n\tclass DatabaseType(Enum):\n", "    \"\"\"The database type.\"\"\"\n\t    POSTGRES = \"postgres\"\n\t    MYSQL = \"mysql\"\n\t    SQLITE = \"sqlite\"\n\t    COCKROACHDB = \"cockroachdb\"\n\t    def to_drivername(self: \"DatabaseType\") -> str:\n\t        \"\"\"Get the drivername for the database type.\n\t        Returns:\n\t            str: The drivername.\n\t        \"\"\"\n", "        if self is DatabaseType.POSTGRES:\n\t            return \"postgresql+asyncpg\"\n\t        if self is DatabaseType.MYSQL:\n\t            return \"mysql+aiomysql\"\n\t        if self is DatabaseType.SQLITE:\n\t            return \"sqlite+aiosqlite\"\n\t        if self is DatabaseType.COCKROACHDB:\n\t            return \"cockroachdb+asyncpg\"\n\t        raise ValueError(f\"Unknown database type '{self}'.\")\n\tdef get_database_type() -> Optional[DatabaseType]:\n", "    \"\"\"Get the database type from environment variables.\n\t    Returns None if no environment variable was set for the database type.\n\t    Returns:\n\t        Optional[DatabaseType]: The database type.\n\t    \"\"\"\n\t    database_type = os.getenv(\"SPOOLMAN_DB_TYPE\")\n\t    if database_type is None:\n\t        return None\n\t    if database_type == \"postgres\":\n\t        return DatabaseType.POSTGRES\n", "    if database_type == \"mysql\":\n\t        return DatabaseType.MYSQL\n\t    if database_type == \"sqlite\":\n\t        return DatabaseType.SQLITE\n\t    if database_type == \"cockroachdb\":\n\t        return DatabaseType.COCKROACHDB\n\t    raise ValueError(f\"Failed to parse SPOOLMAN_DB_TYPE variable: Unknown database type '{database_type}'.\")\n\tdef get_host() -> Optional[str]:\n\t    \"\"\"Get the DB host from environment variables.\n\t    Returns None if no environment variable was set for the host.\n", "    Returns:\n\t        Optional[str]: The host.\n\t    \"\"\"\n\t    return os.getenv(\"SPOOLMAN_DB_HOST\")\n\tdef get_port() -> Optional[int]:\n\t    \"\"\"Get the DB port from environment variables.\n\t    Returns None if no environment variable was set for the port.\n\t    Returns:\n\t        Optional[str]: The port.\n\t    \"\"\"\n", "    port = os.getenv(\"SPOOLMAN_DB_PORT\")\n\t    if port is None:\n\t        return None\n\t    try:\n\t        return int(port)\n\t    except ValueError as exc:\n\t        raise ValueError(f\"Failed to parse SPOOLMAN_DB_PORT variable: {exc!s}\") from exc\n\tdef get_database() -> Optional[str]:\n\t    \"\"\"Get the DB name from environment variables.\n\t    Returns None if no environment variable was set for the name.\n", "    Returns:\n\t        Optional[str]: The name.\n\t    \"\"\"\n\t    return os.getenv(\"SPOOLMAN_DB_NAME\")\n\tdef get_query() -> Optional[dict[str, str]]:\n\t    \"\"\"Get the DB query from environment variables.\n\t    Returns None if no environment variable was set for the query.\n\t    Returns:\n\t        Optional[dict]: The query.\n\t    \"\"\"\n", "    query = os.getenv(\"SPOOLMAN_DB_QUERY\")\n\t    if query is None:\n\t        return None\n\t    try:\n\t        parsed_dict = parse.parse_qs(query, strict_parsing=True)\n\t        return {key: value[0] for key, value in parsed_dict.items()}\n\t    except ValueError as exc:\n\t        raise ValueError(f\"Failed to parse SPOOLMAN_DB_QUERY variable: {exc!s}\") from exc\n\tdef get_username() -> Optional[str]:\n\t    \"\"\"Get the DB username from environment variables.\n", "    Returns None if no environment variable was set for the username.\n\t    Returns:\n\t        Optional[str]: The username.\n\t    \"\"\"\n\t    return os.getenv(\"SPOOLMAN_DB_USERNAME\")\n\tdef get_password() -> Optional[str]:\n\t    \"\"\"Get the DB password from environment variables.\n\t    Returns None if no environment variables were set for the password.\n\t    Raises:\n\t        ValueError: If it failed to read the password from a password file.\n", "    Returns:\n\t        Optional[str]: The password.\n\t    \"\"\"\n\t    # First attempt: grab password from a file. This is the most secure way of storing passwords.\n\t    file_path = os.getenv(\"SPOOLMAN_DB_PASSWORD_FILE\")\n\t    if file_path is not None:\n\t        file = Path(file_path)\n\t        if not file.exists() or not file.is_file():\n\t            raise ValueError(\n\t                \"Failed to parse SPOOLMAN_DB_PASSWORD_FILE variable: \"\n", "                f'Database password file \"{file_path}\" does not exist.',\n\t            )\n\t        try:\n\t            with file.open(encoding=\"utf-8\") as f:\n\t                return f.read()\n\t        except OSError as exc:\n\t            raise ValueError(\n\t                \"Failed to parse SPOOLMAN_DB_PASSWORD_FILE variable: \"\n\t                f'Failed to read password from file \"{file_path}\": {exc!s}.',\n\t            ) from exc\n", "    # Second attempt: grab directly from an environment variable.\n\t    return os.getenv(\"SPOOLMAN_DB_PASSWORD\")\n\tdef get_logging_level() -> int:\n\t    \"\"\"Get the logging level from environment variables.\n\t    Returns \"INFO\" if no environment variable was set for the logging level.\n\t    Returns:\n\t        str: The logging level.\n\t    \"\"\"\n\t    log_level_str = os.getenv(\"SPOOLMAN_LOGGING_LEVEL\", \"INFO\").upper()\n\t    if log_level_str == \"DEBUG\":\n", "        return logging.DEBUG\n\t    if log_level_str == \"INFO\":\n\t        return logging.INFO\n\t    if log_level_str == \"WARNING\":\n\t        return logging.WARNING\n\t    if log_level_str == \"ERROR\":\n\t        return logging.ERROR\n\t    if log_level_str == \"CRITICAL\":\n\t        return logging.CRITICAL\n\t    raise ValueError(f\"Failed to parse SPOOLMAN_LOGGING_LEVEL variable: Unknown logging level '{log_level_str}'.\")\n", "def is_debug_mode() -> bool:\n\t    \"\"\"Get whether debug mode is enabled from environment variables.\n\t    Returns False if no environment variable was set for debug mode.\n\t    Returns:\n\t        bool: Whether debug mode is enabled.\n\t    \"\"\"\n\t    debug_mode = os.getenv(\"SPOOLMAN_DEBUG_MODE\", \"FALSE\").upper()\n\t    if debug_mode == \"FALSE\" or debug_mode == \"0\":\n\t        return False\n\t    if debug_mode == \"TRUE\" or debug_mode == \"1\":\n", "        return True\n\t    raise ValueError(f\"Failed to parse SPOOLMAN_DEBUG_MODE variable: Unknown debug mode '{debug_mode}'.\")\n\tdef is_automatic_backup_enabled() -> bool:\n\t    \"\"\"Get whether automatic backup is enabled from environment variables.\n\t    Returns True if no environment variable was set for automatic backup.\n\t    Returns:\n\t        bool: Whether automatic backup is enabled.\n\t    \"\"\"\n\t    automatic_backup = os.getenv(\"SPOOLMAN_AUTOMATIC_BACKUP\", \"TRUE\").upper()\n\t    if automatic_backup == \"FALSE\" or automatic_backup == \"0\":\n", "        return False\n\t    if automatic_backup == \"TRUE\" or automatic_backup == \"1\":\n\t        return True\n\t    raise ValueError(\n\t        f\"Failed to parse SPOOLMAN_AUTOMATIC_BACKUP variable: Unknown automatic backup '{automatic_backup}'.\",\n\t    )\n\tdef get_data_dir() -> Path:\n\t    \"\"\"Get the data directory.\n\t    Returns:\n\t        Path: The data directory.\n", "    \"\"\"\n\t    data_dir = Path(user_data_dir(\"spoolman\"))\n\t    data_dir.mkdir(parents=True, exist_ok=True)\n\t    return data_dir\n\tdef get_backups_dir() -> Path:\n\t    \"\"\"Get the backups directory.\n\t    Returns:\n\t        Path: The backups directory.\n\t    \"\"\"\n\t    data_dir = get_data_dir()\n", "    backups_dir = data_dir.joinpath(\"backups\")\n\t    backups_dir.mkdir(parents=True, exist_ok=True)\n\t    return backups_dir\n\tdef get_version() -> str:\n\t    \"\"\"Get the version of the package.\n\t    Returns:\n\t        str: The version.\n\t    \"\"\"\n\t    return pkg_resources.get_distribution(\"spoolman\").version\n"]}
{"filename": "spoolman/bump.py", "chunked_list": ["\"\"\"A python script that bumps the version number of a project.\"\"\"\n\t# ruff: noqa: PLR2004, T201, S603, S607\n\timport re\n\timport subprocess\n\timport sys\n\tfrom pathlib import Path\n\tdef bump() -> None:\n\t    \"\"\"Bump the version number of the project.\"\"\"\n\t    project_root = Path(__file__).parent.parent\n\t    if len(sys.argv) < 2:\n", "        print(\"Please specify a bump type, e.g. major, minor, micro.\")\n\t        sys.exit(1)\n\t    if subprocess.run([\"git\", \"diff\", \"--quiet\", \"pyproject.toml\"], cwd=project_root).returncode != 0:\n\t        print(\"The pyproject.toml file is dirty, please commit your changes before bumping the version number.\")\n\t        sys.exit(1)\n\t    if subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"], cwd=project_root).returncode != 0:\n\t        print(\"There are staged changes, please commit them before bumping the version number.\")\n\t        sys.exit(1)\n\t    if subprocess.run([\"pip\", \"show\", \"pdm-bump\"], cwd=project_root, capture_output=True).returncode != 0:\n\t        print(\"Please install pdm-bump using pip.\")\n", "        sys.exit(1)\n\t    # Bump the version number, read the pdm bump output to determine the new version number\n\t    bump_type = sys.argv[1]\n\t    bump_output = subprocess.run([\"pdm\", \"bump\", bump_type], cwd=project_root, capture_output=True, check=True)\n\t    # Example output: \"Performing increment of version: 0.7.0 -> 0.8.0\\nSome more text\"\n\t    # Parse using regex\n\t    new_version_match = re.search(r\"-> ([A-Za-z0-9\\.\\-]+)\", bump_output.stdout.decode())\n\t    if new_version_match is None:\n\t        print(\"Failed to parse pdm bump output, did it fail?\")\n\t        sys.exit(1)\n", "    new_version = new_version_match.group(1)\n\t    # Stage the pyproject.toml file\n\t    subprocess.run([\"git\", \"add\", \"pyproject.toml\"], cwd=project_root, check=True)\n\t    # Commit the changes\n\t    subprocess.run([\"git\", \"commit\", \"-m\", f\"Bump version to {new_version}\"], cwd=project_root, check=True)\n\t    # Tag the commit, prefix with \"v\"\n\t    subprocess.run([\"git\", \"tag\", f\"v{new_version}\"], cwd=project_root, check=True)\n\t    # Notify user that the process is complete\n\t    print(f\"Bumped version to {new_version}.\")\n"]}
{"filename": "spoolman/math.py", "chunked_list": ["\"\"\"Various math-related functions.\"\"\"\n\timport math\n\tdef weight_from_length(*, length: float, diameter: float, density: float) -> float:\n\t    \"\"\"Calculate the weight of a piece of filament.\n\t    Args:\n\t        length (float): Filament length in mm\n\t        diameter (float): Filament diameter in mm\n\t        density (float): Density of filament material in g/cm3\n\t    Returns:\n\t        float: Weight in g\n", "    \"\"\"\n\t    volume_mm3 = length * math.pi * (diameter / 2) ** 2\n\t    volume_cm3 = volume_mm3 / 1000\n\t    return density * volume_cm3\n\tdef length_from_weight(*, weight: float, diameter: float, density: float) -> float:\n\t    \"\"\"Calculate the length of a piece of filament.\n\t    Args:\n\t        weight (float): Filament weight in g\n\t        diameter (float): Filament diameter in mm\n\t        density (float): Density of filament material in g/cm3\n", "    Returns:\n\t        float: Length in mm\n\t    \"\"\"\n\t    volume_cm3 = weight / density\n\t    volume_mm3 = volume_cm3 * 1000\n\t    return volume_mm3 / (math.pi * (diameter / 2) ** 2)\n"]}
{"filename": "spoolman/exceptions.py", "chunked_list": ["\"\"\"Various exceptions used.\"\"\"\n\tclass ItemNotFoundError(Exception):\n\t    pass\n\tclass ItemDeleteError(Exception):\n\t    pass\n\tclass ItemCreateError(Exception):\n\t    pass\n"]}
{"filename": "spoolman/database/database.py", "chunked_list": ["\"\"\"SQLAlchemy database setup.\"\"\"\n\timport datetime\n\timport logging\n\timport shutil\n\timport sqlite3\n\tfrom collections.abc import AsyncGenerator\n\tfrom os import PathLike\n\tfrom pathlib import Path\n\tfrom typing import Optional, Union\n\tfrom scheduler.asyncio.scheduler import Scheduler\n", "from sqlalchemy import URL\n\tfrom sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, async_sessionmaker, create_async_engine\n\tfrom spoolman import env\n\tlogger = logging.getLogger(__name__)\n\tdef get_connection_url() -> URL:\n\t    \"\"\"Construct the connection URL for the database based on environment variables.\"\"\"\n\t    db_type = env.get_database_type()\n\t    host = env.get_host()\n\t    port = env.get_port()\n\t    database = env.get_database()\n", "    query = env.get_query()\n\t    username = env.get_username()\n\t    password = env.get_password()\n\t    if db_type is None:\n\t        db_type = env.DatabaseType.SQLITE\n\t        database = str(env.get_data_dir().joinpath(\"spoolman.db\"))\n\t        logger.info('No database type specified, using a default SQLite database located at \"%s\"', database)\n\t    elif db_type is env.DatabaseType.SQLITE:\n\t        if database is not None:\n\t            raise ValueError(\"Cannot specify a database name when using SQLite.\")\n", "        database = str(env.get_data_dir().joinpath(\"spoolman.db\"))\n\t        logger.info('Using SQLite database located at \"%s\"', database)\n\t    else:\n\t        logger.info('Connecting to database of type \"%s\" at \"%s:%s\"', db_type, host, port)\n\t    return URL.create(\n\t        drivername=db_type.to_drivername(),\n\t        host=host,\n\t        port=port,\n\t        database=database,\n\t        query=query or {},\n", "        username=username,\n\t        password=password,\n\t    )\n\tclass Database:\n\t    connection_url: URL\n\t    engine: Optional[AsyncEngine]\n\t    session_maker: Optional[async_sessionmaker[AsyncSession]]\n\t    def __init__(self: \"Database\", connection_url: URL) -> None:\n\t        \"\"\"Construct the Database wrapper and set config parameters.\"\"\"\n\t        self.connection_url = connection_url\n", "    def is_file_based_sqlite(self: \"Database\") -> bool:\n\t        \"\"\"Return True if the database is file based.\"\"\"\n\t        return (\n\t            self.connection_url.drivername[:6] == \"sqlite\"\n\t            and self.connection_url.database is not None\n\t            and self.connection_url.database != \":memory:\"\n\t        )\n\t    def connect(self: \"Database\") -> None:\n\t        \"\"\"Connect to the database.\"\"\"\n\t        if env.get_logging_level() == logging.DEBUG:\n", "            logging.getLogger(\"sqlalchemy.engine\").setLevel(logging.INFO)\n\t        connect_args = {}\n\t        if self.connection_url.drivername == \"sqlite+aiosqlite\":\n\t            connect_args[\"timeout\"] = 60\n\t        self.engine = create_async_engine(\n\t            self.connection_url,\n\t            connect_args=connect_args,\n\t            pool_pre_ping=True,\n\t        )\n\t        self.session_maker = async_sessionmaker(self.engine, autocommit=False, autoflush=True, expire_on_commit=False)\n", "    def backup(self: \"Database\", target_path: Union[str, PathLike[str]]) -> None:\n\t        \"\"\"Backup the database.\"\"\"\n\t        if not self.is_file_based_sqlite() or self.connection_url.database is None:\n\t            return\n\t        logger.info(\"Backing up SQLite database to %s\", target_path)\n\t        def progress(_: int, remaining: int, total: int) -> None:\n\t            logger.info(\"Copied %d of %d pages.\", total - remaining, total)\n\t        if self.connection_url.database == target_path:\n\t            raise ValueError(\"Cannot backup database to itself.\")\n\t        if Path(target_path).exists():\n", "            raise ValueError(\"Backup target file already exists.\")\n\t        with sqlite3.connect(self.connection_url.database) as src, sqlite3.connect(target_path) as dst:\n\t            src.backup(dst, pages=1, progress=progress)\n\t        logger.info(\"Backup complete.\")\n\t    def backup_and_rotate(\n\t        self: \"Database\",\n\t        backup_folder: Union[str, PathLike[str]],\n\t        num_backups: int = 5,\n\t    ) -> Optional[Path]:\n\t        \"\"\"Backup the database and rotate existing backups.\n", "        Args:\n\t            backup_folder: The folder to store the backups in.\n\t            num_backups: The number of backups to keep.\n\t        Returns:\n\t            The path to the created backup or None if no backup was created.\n\t        \"\"\"\n\t        if not self.is_file_based_sqlite() or self.connection_url.database is None:\n\t            logger.info(\"Skipping backup as the database is not SQLite.\")\n\t            return None\n\t        backup_folder = Path(backup_folder)\n", "        backup_folder.mkdir(parents=True, exist_ok=True)\n\t        # Delete oldest backup\n\t        backup_path = backup_folder.joinpath(f\"spoolman.db.{num_backups}\")\n\t        if backup_path.exists():\n\t            logger.info(\"Deleting oldest backup %s\", backup_path)\n\t            backup_path.unlink()\n\t        # Rotate existing backups\n\t        for i in range(num_backups - 1, -1, -1):\n\t            if i == 0:\n\t                backup_path = backup_folder.joinpath(\"spoolman.db\")\n", "            else:\n\t                backup_path = backup_folder.joinpath(f\"spoolman.db.{i}\")\n\t            if backup_path.exists():\n\t                logger.debug(\"Rotating backup %s to %s\", backup_path, backup_folder.joinpath(f\"spoolman.db.{i + 1}\"))\n\t                shutil.move(backup_path, backup_folder.joinpath(f\"spoolman.db.{i + 1}\"))\n\t        # Create new backup\n\t        backup_path = backup_folder.joinpath(\"spoolman.db\")\n\t        self.backup(backup_path)\n\t        return backup_path\n\t__db: Optional[Database] = None\n", "def setup_db(connection_url: URL) -> None:\n\t    \"\"\"Connect to the database.\n\t    Args:\n\t        connection_url: The URL to connect to the database.\n\t    \"\"\"\n\t    global __db  # noqa: PLW0603\n\t    __db = Database(connection_url)\n\t    __db.connect()\n\tasync def backup_global_db(num_backups: int = 5) -> Optional[Path]:\n\t    \"\"\"Backup the database and rotate existing backups.\n", "    Returns:\n\t        The path to the created backup or None if no backup was created.\n\t    \"\"\"\n\t    if __db is None:\n\t        raise RuntimeError(\"DB is not setup.\")\n\t    return __db.backup_and_rotate(env.get_data_dir().joinpath(\"backups\"), num_backups=num_backups)\n\tasync def _backup_task() -> Optional[Path]:\n\t    \"\"\"Perform scheduled backup of the database.\"\"\"\n\t    logger.info(\"Performing scheduled database backup.\")\n\t    if __db is None:\n", "        raise RuntimeError(\"DB is not setup.\")\n\t    return __db.backup_and_rotate(env.get_data_dir().joinpath(\"backups\"), num_backups=5)\n\tdef schedule_tasks(scheduler: Scheduler) -> None:\n\t    \"\"\"Schedule tasks to be executed by the provided scheduler.\n\t    Args:\n\t        scheduler: The scheduler to use for scheduling tasks.\n\t    \"\"\"\n\t    if __db is None:\n\t        raise RuntimeError(\"DB is not setup.\")\n\t    if not env.is_automatic_backup_enabled():\n", "        return\n\t    if \"sqlite\" in __db.connection_url.drivername:\n\t        logger.info(\"Scheduling automatic database backup for midnight.\")\n\t        # Schedule for midnight\n\t        scheduler.daily(datetime.time(hour=0, minute=0, second=0), _backup_task)  # type: ignore[arg-type]\n\tasync def get_db_session() -> AsyncGenerator[AsyncSession, None]:\n\t    \"\"\"Get a DB session to be used with FastAPI's dependency system.\n\t    Yields:\n\t        The database session.\n\t    \"\"\"\n", "    if __db is None or __db.session_maker is None:\n\t        raise RuntimeError(\"DB is not setup.\")\n\t    async with __db.session_maker() as session:\n\t        try:\n\t            yield session\n\t            await session.commit()\n\t        except Exception as exc:\n\t            await session.rollback()\n\t            raise exc\n\t        finally:\n", "            await session.close()\n"]}
{"filename": "spoolman/database/filament.py", "chunked_list": ["\"\"\"Helper functions for interacting with filament database objects.\"\"\"\n\tfrom typing import Optional\n\tfrom sqlalchemy import select\n\tfrom sqlalchemy.exc import IntegrityError\n\tfrom sqlalchemy.ext.asyncio import AsyncSession\n\tfrom sqlalchemy.orm import contains_eager, joinedload\n\tfrom spoolman.database import models, vendor\n\tfrom spoolman.exceptions import ItemDeleteError, ItemNotFoundError\n\tasync def create(\n\t    *,\n", "    db: AsyncSession,\n\t    density: float,\n\t    diameter: float,\n\t    name: Optional[str] = None,\n\t    vendor_id: Optional[int] = None,\n\t    material: Optional[str] = None,\n\t    price: Optional[float] = None,\n\t    weight: Optional[float] = None,\n\t    spool_weight: Optional[float] = None,\n\t    article_number: Optional[str] = None,\n", "    comment: Optional[str] = None,\n\t    settings_extruder_temp: Optional[int] = None,\n\t    settings_bed_temp: Optional[int] = None,\n\t    color_hex: Optional[str] = None,\n\t) -> models.Filament:\n\t    \"\"\"Add a new filament to the database.\"\"\"\n\t    vendor_item: Optional[models.Vendor] = None  # noqa: FA100\n\t    if vendor_id is not None:\n\t        vendor_item = await vendor.get_by_id(db, vendor_id)\n\t    db_item = models.Filament(\n", "        name=name,\n\t        vendor=vendor_item,\n\t        material=material,\n\t        price=price,\n\t        density=density,\n\t        diameter=diameter,\n\t        weight=weight,\n\t        spool_weight=spool_weight,\n\t        article_number=article_number,\n\t        comment=comment,\n", "        settings_extruder_temp=settings_extruder_temp,\n\t        settings_bed_temp=settings_bed_temp,\n\t        color_hex=color_hex,\n\t    )\n\t    db.add(db_item)\n\t    await db.flush()\n\t    return db_item\n\tasync def get_by_id(db: AsyncSession, filament_id: int) -> models.Filament:\n\t    \"\"\"Get a filament object from the database by the unique ID.\"\"\"\n\t    filament = await db.get(\n", "        models.Filament,\n\t        filament_id,\n\t        options=[joinedload(\"*\")],  # Load all nested objects as well\n\t    )\n\t    if filament is None:\n\t        raise ItemNotFoundError(f\"No filament with ID {filament_id} found.\")\n\t    return filament\n\tasync def find(\n\t    *,\n\t    db: AsyncSession,\n", "    vendor_name: Optional[str] = None,\n\t    vendor_id: Optional[int] = None,\n\t    name: Optional[str] = None,\n\t    material: Optional[str] = None,\n\t    article_number: Optional[str] = None,\n\t) -> list[models.Filament]:\n\t    \"\"\"Find a list of filament objects by search criteria.\"\"\"\n\t    stmt = (\n\t        select(models.Filament)\n\t        .options(contains_eager(models.Filament.vendor))\n", "        .join(models.Filament.vendor, isouter=True)\n\t    )\n\t    if vendor_name is not None:\n\t        stmt = stmt.where(models.Vendor.name.ilike(f\"%{vendor_name}%\"))\n\t    if vendor_id is not None:\n\t        stmt = stmt.where(models.Filament.vendor_id == vendor_id)\n\t    if name is not None:\n\t        stmt = stmt.where(models.Filament.name.ilike(f\"%{name}%\"))\n\t    if material is not None:\n\t        stmt = stmt.where(models.Filament.material.ilike(f\"%{material}%\"))\n", "    if article_number is not None:\n\t        stmt = stmt.where(models.Filament.article_number.ilike(f\"%{article_number}%\"))\n\t    rows = await db.execute(stmt)\n\t    return list(rows.scalars().all())\n\tasync def update(\n\t    *,\n\t    db: AsyncSession,\n\t    filament_id: int,\n\t    data: dict,\n\t) -> models.Filament:\n", "    \"\"\"Update the fields of a filament object.\"\"\"\n\t    filament = await get_by_id(db, filament_id)\n\t    for k, v in data.items():\n\t        if k == \"vendor_id\":\n\t            if v is None:\n\t                filament.vendor = None\n\t            else:\n\t                filament.vendor = await vendor.get_by_id(db, v)\n\t        else:\n\t            setattr(filament, k, v)\n", "    await db.flush()\n\t    return filament\n\tasync def delete(db: AsyncSession, filament_id: int) -> None:\n\t    \"\"\"Delete a filament object.\"\"\"\n\t    filament = await get_by_id(db, filament_id)\n\t    await db.delete(filament)\n\t    try:\n\t        await db.flush()  # Flush immediately so any errors are propagated in this request.\n\t    except IntegrityError as exc:\n\t        await db.rollback()\n", "        raise ItemDeleteError(\"Failed to delete filament.\") from exc\n"]}
{"filename": "spoolman/database/models.py", "chunked_list": ["\"\"\"SQLAlchemy data models.\"\"\"\n\tfrom datetime import datetime\n\tfrom typing import Optional\n\tfrom sqlalchemy import ForeignKey, Integer, String\n\tfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship\n\tfrom sqlalchemy.sql import func\n\tclass Base(DeclarativeBase):\n\t    pass\n\tclass Vendor(Base):\n\t    __tablename__ = \"vendor\"\n", "    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)\n\t    registered: Mapped[datetime] = mapped_column(default=func.now())\n\t    name: Mapped[str] = mapped_column(String(64))\n\t    comment: Mapped[Optional[str]] = mapped_column(String(1024))\n\t    filaments: Mapped[list[\"Filament\"]] = relationship(back_populates=\"vendor\")\n\tclass Filament(Base):\n\t    __tablename__ = \"filament\"\n\t    id: Mapped[int] = mapped_column(primary_key=True, index=True)\n\t    registered: Mapped[datetime] = mapped_column(default=func.now())\n\t    name: Mapped[Optional[str]] = mapped_column(String(64))\n", "    vendor_id: Mapped[Optional[int]] = mapped_column(ForeignKey(\"vendor.id\"))\n\t    vendor: Mapped[Optional[\"Vendor\"]] = relationship(back_populates=\"filaments\")\n\t    spools: Mapped[list[\"Spool\"]] = relationship(back_populates=\"filament\")\n\t    material: Mapped[Optional[str]] = mapped_column(String(64))\n\t    price: Mapped[Optional[float]] = mapped_column()\n\t    density: Mapped[float] = mapped_column()\n\t    diameter: Mapped[float] = mapped_column()\n\t    weight: Mapped[Optional[float]] = mapped_column(comment=\"The filament weight of a full spool (net weight).\")\n\t    spool_weight: Mapped[Optional[float]] = mapped_column(comment=\"The weight of an empty spool.\")\n\t    article_number: Mapped[Optional[str]] = mapped_column(String(64))\n", "    comment: Mapped[Optional[str]] = mapped_column(String(1024))\n\t    settings_extruder_temp: Mapped[Optional[int]] = mapped_column(comment=\"Overridden extruder temperature.\")\n\t    settings_bed_temp: Mapped[Optional[int]] = mapped_column(comment=\"Overridden bed temperature.\")\n\t    color_hex: Mapped[Optional[str]] = mapped_column(String(8))\n\tclass Spool(Base):\n\t    __tablename__ = \"spool\"\n\t    id: Mapped[int] = mapped_column(primary_key=True, index=True)\n\t    registered: Mapped[datetime] = mapped_column(default=func.now())\n\t    first_used: Mapped[Optional[datetime]] = mapped_column()\n\t    last_used: Mapped[Optional[datetime]] = mapped_column()\n", "    filament_id: Mapped[int] = mapped_column(ForeignKey(\"filament.id\"))\n\t    filament: Mapped[\"Filament\"] = relationship(back_populates=\"spools\")\n\t    used_weight: Mapped[float] = mapped_column()\n\t    location: Mapped[Optional[str]] = mapped_column(String(64))\n\t    lot_nr: Mapped[Optional[str]] = mapped_column(String(64))\n\t    comment: Mapped[Optional[str]] = mapped_column(String(1024))\n\t    archived: Mapped[Optional[bool]] = mapped_column()\n"]}
{"filename": "spoolman/database/vendor.py", "chunked_list": ["\"\"\"Helper functions for interacting with vendor database objects.\"\"\"\n\tfrom typing import Optional\n\tfrom sqlalchemy import select\n\tfrom sqlalchemy.ext.asyncio import AsyncSession\n\tfrom spoolman.database import models\n\tfrom spoolman.exceptions import ItemNotFoundError\n\tasync def create(\n\t    *,\n\t    db: AsyncSession,\n\t    name: Optional[str] = None,\n", "    comment: Optional[str] = None,\n\t) -> models.Vendor:\n\t    \"\"\"Add a new vendor to the database.\"\"\"\n\t    db_item = models.Vendor(\n\t        name=name,\n\t        comment=comment,\n\t    )\n\t    db.add(db_item)\n\t    await db.flush()\n\t    return db_item\n", "async def get_by_id(db: AsyncSession, vendor_id: int) -> models.Vendor:\n\t    \"\"\"Get a vendor object from the database by the unique ID.\"\"\"\n\t    vendor = await db.get(models.Vendor, vendor_id)\n\t    if vendor is None:\n\t        raise ItemNotFoundError(f\"No vendor with ID {vendor_id} found.\")\n\t    return vendor\n\tasync def find(\n\t    *,\n\t    db: AsyncSession,\n\t    name: Optional[str] = None,\n", ") -> list[models.Vendor]:\n\t    \"\"\"Find a list of vendor objects by search criteria.\"\"\"\n\t    stmt = select(models.Vendor)\n\t    if name is not None:\n\t        stmt = stmt.where(models.Vendor.name.ilike(f\"%{name}%\"))\n\t    rows = await db.execute(stmt)\n\t    return list(rows.scalars().all())\n\tasync def update(\n\t    *,\n\t    db: AsyncSession,\n", "    vendor_id: int,\n\t    data: dict,\n\t) -> models.Vendor:\n\t    \"\"\"Update the fields of a vendor object.\"\"\"\n\t    vendor = await get_by_id(db, vendor_id)\n\t    for k, v in data.items():\n\t        setattr(vendor, k, v)\n\t    await db.flush()\n\t    return vendor\n\tasync def delete(db: AsyncSession, vendor_id: int) -> None:\n", "    \"\"\"Delete a vendor object.\"\"\"\n\t    vendor = await get_by_id(db, vendor_id)\n\t    await db.delete(vendor)\n"]}
{"filename": "spoolman/database/__init__.py", "chunked_list": []}
{"filename": "spoolman/database/spool.py", "chunked_list": ["\"\"\"Helper functions for interacting with spool database objects.\"\"\"\n\tfrom datetime import datetime, timezone\n\tfrom typing import Optional\n\timport sqlalchemy\n\tfrom sqlalchemy import case\n\tfrom sqlalchemy.exc import NoResultFound\n\tfrom sqlalchemy.ext.asyncio import AsyncSession\n\tfrom sqlalchemy.orm import contains_eager, joinedload\n\tfrom spoolman.database import filament, models\n\tfrom spoolman.exceptions import ItemCreateError, ItemNotFoundError\n", "from spoolman.math import weight_from_length\n\tdef utc_timezone_naive(dt: datetime) -> datetime:\n\t    \"\"\"Convert a datetime object to UTC and remove timezone info.\"\"\"\n\t    return dt.astimezone(tz=timezone.utc).replace(tzinfo=None)\n\tasync def create(\n\t    *,\n\t    db: AsyncSession,\n\t    filament_id: int,\n\t    remaining_weight: Optional[float] = None,\n\t    used_weight: Optional[float] = None,\n", "    first_used: Optional[datetime] = None,\n\t    last_used: Optional[datetime] = None,\n\t    location: Optional[str] = None,\n\t    lot_nr: Optional[str] = None,\n\t    comment: Optional[str] = None,\n\t    archived: bool = False,\n\t) -> models.Spool:\n\t    \"\"\"Add a new spool to the database. Leave weight empty to assume full spool.\"\"\"\n\t    filament_item = await filament.get_by_id(db, filament_id)\n\t    if used_weight is None:\n", "        if remaining_weight is not None:\n\t            if filament_item.weight is None:\n\t                raise ItemCreateError(\"remaining_weight can only be used if the filament type has a weight set.\")\n\t            used_weight = max(filament_item.weight - remaining_weight, 0)\n\t        else:\n\t            used_weight = 0\n\t    # Convert datetime values to UTC and remove timezone info\n\t    if first_used is not None:\n\t        first_used = utc_timezone_naive(first_used)\n\t    if last_used is not None:\n", "        last_used = utc_timezone_naive(last_used)\n\t    db_item = models.Spool(\n\t        filament=filament_item,\n\t        used_weight=used_weight,\n\t        first_used=first_used,\n\t        last_used=last_used,\n\t        location=location,\n\t        lot_nr=lot_nr,\n\t        comment=comment,\n\t        archived=archived,\n", "    )\n\t    db.add(db_item)\n\t    await db.flush()\n\t    return db_item\n\tasync def get_by_id(db: AsyncSession, spool_id: int) -> models.Spool:\n\t    \"\"\"Get a spool object from the database by the unique ID.\"\"\"\n\t    spool = await db.get(\n\t        models.Spool,\n\t        spool_id,\n\t        options=[joinedload(\"*\")],  # Load all nested objects as well\n", "    )\n\t    if spool is None:\n\t        raise ItemNotFoundError(f\"No spool with ID {spool_id} found.\")\n\t    return spool\n\tasync def find(\n\t    *,\n\t    db: AsyncSession,\n\t    filament_name: Optional[str] = None,\n\t    filament_id: Optional[int] = None,\n\t    filament_material: Optional[str] = None,\n", "    vendor_name: Optional[str] = None,\n\t    vendor_id: Optional[int] = None,\n\t    location: Optional[str] = None,\n\t    lot_nr: Optional[str] = None,\n\t    allow_archived: bool = False,\n\t) -> list[models.Spool]:\n\t    \"\"\"Find a list of spool objects by search criteria.\"\"\"\n\t    stmt = (\n\t        sqlalchemy.select(models.Spool)\n\t        .join(models.Spool.filament, isouter=True)\n", "        .join(models.Filament.vendor, isouter=True)\n\t        .options(contains_eager(models.Spool.filament).contains_eager(models.Filament.vendor))\n\t    )\n\t    if filament_name is not None:\n\t        stmt = stmt.where(models.Filament.name.ilike(f\"%{filament_name}%\"))\n\t    if filament_id is not None:\n\t        stmt = stmt.where(models.Spool.filament_id == filament_id)\n\t    if filament_material is not None:\n\t        stmt = stmt.where(models.Filament.material.ilike(f\"%{filament_material}%\"))\n\t    if vendor_name is not None:\n", "        stmt = stmt.where(models.Vendor.name.ilike(f\"%{vendor_name}%\"))\n\t    if vendor_id is not None:\n\t        stmt = stmt.where(models.Filament.vendor_id == vendor_id)\n\t    if location is not None:\n\t        stmt = stmt.where(models.Spool.location.ilike(f\"%{location}%\"))\n\t    if lot_nr is not None:\n\t        stmt = stmt.where(models.Spool.lot_nr.ilike(f\"%{lot_nr}%\"))\n\t    if not allow_archived:\n\t        # Since the archived field is nullable, and default is false, we need to check for both false or null\n\t        stmt = stmt.where(\n", "            sqlalchemy.or_(\n\t                models.Spool.archived.is_(False),  # noqa: FBT003\n\t                models.Spool.archived.is_(None),\n\t            ),\n\t        )\n\t    rows = await db.execute(stmt)\n\t    return list(rows.scalars().all())\n\tasync def update(\n\t    *,\n\t    db: AsyncSession,\n", "    spool_id: int,\n\t    data: dict,\n\t) -> models.Spool:\n\t    \"\"\"Update the fields of a spool object.\"\"\"\n\t    spool = await get_by_id(db, spool_id)\n\t    for k, v in data.items():\n\t        if k == \"filament_id\":\n\t            spool.filament = await filament.get_by_id(db, v)\n\t        elif k == \"remaining_weight\":\n\t            if spool.filament.weight is None:\n", "                raise ItemCreateError(\"remaining_weight can only be used if the filament type has a weight set.\")\n\t            spool.used_weight = max(spool.filament.weight - v, 0)\n\t        elif isinstance(v, datetime):\n\t            setattr(spool, k, utc_timezone_naive(v))\n\t        else:\n\t            setattr(spool, k, v)\n\t    await db.flush()\n\t    return spool\n\tasync def delete(db: AsyncSession, spool_id: int) -> None:\n\t    \"\"\"Delete a spool object.\"\"\"\n", "    spool = await get_by_id(db, spool_id)\n\t    await db.delete(spool)\n\tasync def use_weight_safe(db: AsyncSession, spool_id: int, weight: float) -> None:\n\t    \"\"\"Consume filament from a spool by weight in a way that is safe against race conditions.\n\t    Args:\n\t        db (AsyncSession): Database session\n\t        spool_id (int): Spool ID\n\t        weight (float): Filament weight to consume, in grams\n\t    \"\"\"\n\t    await db.execute(\n", "        sqlalchemy.update(models.Spool)\n\t        .where(models.Spool.id == spool_id)\n\t        .values(\n\t            used_weight=case(\n\t                (models.Spool.used_weight + weight >= 0.0, models.Spool.used_weight + weight),  # noqa: PLR2004\n\t                else_=0.0,  # Set used_weight to 0 if the result would be negative\n\t            ),\n\t        ),\n\t    )\n\tasync def use_weight(db: AsyncSession, spool_id: int, weight: float) -> models.Spool:\n", "    \"\"\"Consume filament from a spool by weight.\n\t    Increases the used_weight attribute of the spool.\n\t    Updates the first_used and last_used attributes where appropriate.\n\t    Args:\n\t        db (AsyncSession): Database session\n\t        spool_id (int): Spool ID\n\t        weight (float): Filament weight to consume, in grams\n\t    Returns:\n\t        models.Spool: Updated spool object\n\t    \"\"\"\n", "    await use_weight_safe(db, spool_id, weight)\n\t    spool = await get_by_id(db, spool_id)\n\t    if spool.first_used is None:\n\t        spool.first_used = datetime.utcnow()\n\t    spool.last_used = datetime.utcnow()\n\t    await db.flush()\n\t    return spool\n\tasync def use_length(db: AsyncSession, spool_id: int, length: float) -> models.Spool:\n\t    \"\"\"Consume filament from a spool by length.\n\t    Increases the used_weight attribute of the spool.\n", "    Updates the first_used and last_used attributes where appropriate.\n\t    Args:\n\t        db (AsyncSession): Database session\n\t        spool_id (int): Spool ID\n\t        length (float): Length of filament to consume, in mm\n\t    Returns:\n\t        models.Spool: Updated spool object\n\t    \"\"\"\n\t    # Get filament diameter and density\n\t    result = await db.execute(\n", "        sqlalchemy.select(models.Filament.diameter, models.Filament.density)\n\t        .join(models.Spool, models.Spool.filament_id == models.Filament.id)\n\t        .where(models.Spool.id == spool_id),\n\t    )\n\t    try:\n\t        filament_info = result.one()\n\t    except NoResultFound as exc:\n\t        raise ItemNotFoundError(\"Filament not found for spool.\") from exc\n\t    # Calculate and use weight\n\t    weight = weight_from_length(\n", "        length=length,\n\t        diameter=filament_info[0],\n\t        density=filament_info[1],\n\t    )\n\t    await use_weight_safe(db, spool_id, weight)\n\t    # Get spool with new weight and update first_used and last_used\n\t    spool = await get_by_id(db, spool_id)\n\t    if spool.first_used is None:\n\t        spool.first_used = datetime.utcnow()\n\t    spool.last_used = datetime.utcnow()\n", "    await db.flush()\n\t    return spool\n"]}
{"filename": "spoolman/api/__init__.py", "chunked_list": []}
{"filename": "spoolman/api/v1/filament.py", "chunked_list": ["\"\"\"Filament related endpoints.\"\"\"\n\timport logging\n\tfrom typing import Annotated, Optional\n\tfrom fastapi import APIRouter, Depends, Query\n\tfrom fastapi.exceptions import RequestValidationError\n\tfrom fastapi.responses import JSONResponse\n\tfrom pydantic import BaseModel, Field, validator\n\tfrom pydantic.error_wrappers import ErrorWrapper\n\tfrom sqlalchemy.ext.asyncio import AsyncSession\n\tfrom spoolman.api.v1.models import Filament, Message\n", "from spoolman.database import filament\n\tfrom spoolman.database.database import get_db_session\n\tfrom spoolman.exceptions import ItemDeleteError\n\tlogger = logging.getLogger(__name__)\n\trouter = APIRouter(\n\t    prefix=\"/filament\",\n\t    tags=[\"filament\"],\n\t)\n\t# ruff: noqa: D103, B008\n\tclass FilamentParameters(BaseModel):\n", "    name: Optional[str] = Field(\n\t        max_length=64,\n\t        description=(\n\t            \"Filament name, to distinguish this filament type among others from the same vendor.\"\n\t            \"Should contain its color for example.\"\n\t        ),\n\t        example=\"PolyTerra™ Charcoal Black\",\n\t    )\n\t    vendor_id: Optional[int] = Field(description=\"The ID of the vendor of this filament type.\")\n\t    material: Optional[str] = Field(\n", "        max_length=64,\n\t        description=\"The material of this filament, e.g. PLA.\",\n\t        example=\"PLA\",\n\t    )\n\t    price: Optional[float] = Field(\n\t        ge=0,\n\t        description=\"The price of this filament in the system configured currency.\",\n\t        example=20.0,\n\t    )\n\t    density: float = Field(gt=0, description=\"The density of this filament in g/cm3.\", example=1.24)\n", "    diameter: float = Field(gt=0, description=\"The diameter of this filament in mm.\", example=1.75)\n\t    weight: Optional[float] = Field(\n\t        gt=0,\n\t        description=\"The weight of the filament in a full spool, in grams. (net weight)\",\n\t        example=1000,\n\t    )\n\t    spool_weight: Optional[float] = Field(gt=0, description=\"The empty spool weight, in grams.\", example=140)\n\t    article_number: Optional[str] = Field(\n\t        max_length=64,\n\t        description=\"Vendor article number, e.g. EAN, QR code, etc.\",\n", "        example=\"PM70820\",\n\t    )\n\t    comment: Optional[str] = Field(\n\t        max_length=1024,\n\t        description=\"Free text comment about this filament type.\",\n\t        example=\"\",\n\t    )\n\t    settings_extruder_temp: Optional[int] = Field(\n\t        ge=0,\n\t        description=\"Overridden extruder temperature, in °C.\",\n", "        example=210,\n\t    )\n\t    settings_bed_temp: Optional[int] = Field(\n\t        ge=0,\n\t        description=\"Overridden bed temperature, in °C.\",\n\t        example=60,\n\t    )\n\t    color_hex: Optional[str] = Field(\n\t        description=\"Hexadecimal color code of the filament, e.g. FF0000 for red. Supports alpha channel at the end.\",\n\t        example=\"FF0000\",\n", "    )\n\t    @validator(\"color_hex\")\n\t    @classmethod\n\t    def color_hex_validator(cls, v: Optional[str]) -> Optional[str]:  # noqa: ANN102\n\t        \"\"\"Validate the color_hex field.\"\"\"\n\t        if not v:\n\t            return None\n\t        if v.startswith(\"#\"):\n\t            v = v[1:]\n\t        v = v.upper()\n", "        for c in v:\n\t            if c not in \"0123456789ABCDEF\":\n\t                raise ValueError(\"Invalid character in color code.\")\n\t        if len(v) not in (6, 8):\n\t            raise ValueError(\"Color code must be 6 or 8 characters long.\")\n\t        return v\n\tclass FilamentUpdateParameters(FilamentParameters):\n\t    density: Optional[float] = Field(gt=0, description=\"The density of this filament in g/cm3.\", example=1.24)\n\t    diameter: Optional[float] = Field(gt=0, description=\"The diameter of this filament in mm.\", example=1.75)\n\t@router.get(\n", "    \"\",\n\t    name=\"Find filaments\",\n\t    description=\"Get a list of filaments that matches the search query.\",\n\t    response_model_exclude_none=True,\n\t)\n\tasync def find(\n\t    *,\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    vendor_name: Optional[str] = Query(\n\t        default=None,\n", "        title=\"Vendor Name\",\n\t        description=\"Partial case-insensitive search term for the filament vendor name.\",\n\t    ),\n\t    vendor_id: Optional[int] = Query(\n\t        default=None,\n\t        title=\"Vendor ID\",\n\t        description=\"Match an exact vendor ID.\",\n\t    ),\n\t    name: Optional[str] = Query(\n\t        default=None,\n", "        title=\"Filament Name\",\n\t        description=\"Partial case-insensitive search term for the filament name.\",\n\t    ),\n\t    material: Optional[str] = Query(\n\t        default=None,\n\t        title=\"Filament Material\",\n\t        description=\"Partial case-insensitive search term for the filament material.\",\n\t    ),\n\t    article_number: Optional[str] = Query(\n\t        default=None,\n", "        title=\"Filament Article Number\",\n\t        description=\"Partial case-insensitive search term for the filament article number.\",\n\t    ),\n\t) -> list[Filament]:\n\t    db_items = await filament.find(\n\t        db=db,\n\t        vendor_name=vendor_name,\n\t        vendor_id=vendor_id,\n\t        name=name,\n\t        material=material,\n", "        article_number=article_number,\n\t    )\n\t    return [Filament.from_db(db_item) for db_item in db_items]\n\t@router.get(\n\t    \"/{filament_id}\",\n\t    name=\"Get filament\",\n\t    description=\"Get a specific filament.\",\n\t    response_model_exclude_none=True,\n\t    responses={404: {\"model\": Message}},\n\t)\n", "async def get(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    filament_id: int,\n\t) -> Filament:\n\t    db_item = await filament.get_by_id(db, filament_id)\n\t    return Filament.from_db(db_item)\n\t@router.post(\n\t    \"\",\n\t    name=\"Add filament\",\n\t    description=\"Add a new filament to the database.\",\n", "    response_model_exclude_none=True,\n\t)\n\tasync def create(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    body: FilamentParameters,\n\t) -> Filament:\n\t    db_item = await filament.create(\n\t        db=db,\n\t        density=body.density,\n\t        diameter=body.diameter,\n", "        name=body.name,\n\t        vendor_id=body.vendor_id,\n\t        material=body.material,\n\t        price=body.price,\n\t        weight=body.weight,\n\t        spool_weight=body.spool_weight,\n\t        article_number=body.article_number,\n\t        comment=body.comment,\n\t        settings_extruder_temp=body.settings_extruder_temp,\n\t        settings_bed_temp=body.settings_bed_temp,\n", "        color_hex=body.color_hex,\n\t    )\n\t    return Filament.from_db(db_item)\n\t@router.patch(\n\t    \"/{filament_id}\",\n\t    name=\"Update filament\",\n\t    description=\"Update any attribute of a filament. Only fields specified in the request will be affected.\",\n\t    response_model_exclude_none=True,\n\t    responses={404: {\"model\": Message}},\n\t)\n", "async def update(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    filament_id: int,\n\t    body: FilamentUpdateParameters,\n\t) -> Filament:\n\t    patch_data = body.dict(exclude_unset=True)\n\t    if \"density\" in patch_data and body.density is None:\n\t        raise RequestValidationError([ErrorWrapper(ValueError(\"density cannot be unset\"), (\"query\", \"density\"))])\n\t    if \"diameter\" in patch_data and body.diameter is None:\n\t        raise RequestValidationError([ErrorWrapper(ValueError(\"diameter cannot be unset\"), (\"query\", \"diameter\"))])\n", "    db_item = await filament.update(\n\t        db=db,\n\t        filament_id=filament_id,\n\t        data=patch_data,\n\t    )\n\t    return Filament.from_db(db_item)\n\t@router.delete(\n\t    \"/{filament_id}\",\n\t    name=\"Delete filament\",\n\t    description=\"Delete a filament.\",\n", "    response_model=Message,\n\t    responses={\n\t        403: {\"model\": Message},\n\t        404: {\"model\": Message},\n\t    },\n\t)\n\tasync def delete(  # noqa: ANN201\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    filament_id: int,\n\t):\n", "    try:\n\t        await filament.delete(db, filament_id)\n\t    except ItemDeleteError:\n\t        logger.exception(\"Failed to delete filament.\")\n\t        return JSONResponse(\n\t            status_code=403,\n\t            content={\"message\": \"Failed to delete filament, see server logs for more information.\"},\n\t        )\n\t    return Message(message=\"Success!\")\n"]}
{"filename": "spoolman/api/v1/models.py", "chunked_list": ["\"\"\"Pydantic data models for typing the FastAPI request/responses.\"\"\"\n\tfrom datetime import datetime\n\tfrom typing import Optional\n\tfrom pydantic import BaseModel, Field\n\tfrom spoolman.database import models\n\tfrom spoolman.math import length_from_weight\n\tclass Message(BaseModel):\n\t    message: str = Field()\n\tclass Vendor(BaseModel):\n\t    id: int = Field(description=\"Unique internal ID of this vendor.\")\n", "    registered: datetime = Field(description=\"When the vendor was registered in the database. UTC Timezone.\")\n\t    name: str = Field(max_length=64, description=\"Vendor name.\", example=\"Polymaker\")\n\t    comment: Optional[str] = Field(max_length=1024, description=\"Free text comment about this vendor.\", example=\"\")\n\t    @staticmethod\n\t    def from_db(item: models.Vendor) -> \"Vendor\":\n\t        \"\"\"Create a new Pydantic vendor object from a database vendor object.\"\"\"\n\t        return Vendor(\n\t            id=item.id,\n\t            registered=item.registered,\n\t            name=item.name,\n", "            comment=item.comment,\n\t        )\n\tclass Filament(BaseModel):\n\t    id: int = Field(description=\"Unique internal ID of this filament type.\")\n\t    registered: datetime = Field(description=\"When the filament was registered in the database. UTC Timezone.\")\n\t    name: Optional[str] = Field(\n\t        max_length=64,\n\t        description=(\n\t            \"Filament name, to distinguish this filament type among others from the same vendor.\"\n\t            \"Should contain its color for example.\"\n", "        ),\n\t        example=\"PolyTerra™ Charcoal Black\",\n\t    )\n\t    vendor: Optional[Vendor] = Field(description=\"The vendor of this filament type.\")\n\t    material: Optional[str] = Field(\n\t        max_length=64,\n\t        description=\"The material of this filament, e.g. PLA.\",\n\t        example=\"PLA\",\n\t    )\n\t    price: Optional[float] = Field(\n", "        ge=0,\n\t        description=\"The price of this filament in the system configured currency.\",\n\t        example=20.0,\n\t    )\n\t    density: float = Field(gt=0, description=\"The density of this filament in g/cm3.\", example=1.24)\n\t    diameter: float = Field(gt=0, description=\"The diameter of this filament in mm.\", example=1.75)\n\t    weight: Optional[float] = Field(\n\t        gt=0,\n\t        description=\"The weight of the filament in a full spool, in grams.\",\n\t        example=1000,\n", "    )\n\t    spool_weight: Optional[float] = Field(gt=0, description=\"The empty spool weight, in grams.\", example=140)\n\t    article_number: Optional[str] = Field(\n\t        max_length=64,\n\t        description=\"Vendor article number, e.g. EAN, QR code, etc.\",\n\t        example=\"PM70820\",\n\t    )\n\t    comment: Optional[str] = Field(\n\t        max_length=1024,\n\t        description=\"Free text comment about this filament type.\",\n", "        example=\"\",\n\t    )\n\t    settings_extruder_temp: Optional[int] = Field(\n\t        ge=0,\n\t        description=\"Overridden extruder temperature, in °C.\",\n\t        example=210,\n\t    )\n\t    settings_bed_temp: Optional[int] = Field(\n\t        ge=0,\n\t        description=\"Overridden bed temperature, in °C.\",\n", "        example=60,\n\t    )\n\t    color_hex: Optional[str] = Field(\n\t        min_length=6,\n\t        max_length=8,\n\t        description=\"Hexadecimal color code of the filament, e.g. FF0000 for red. Supports alpha channel at the end.\",\n\t        example=\"FF0000\",\n\t    )\n\t    @staticmethod\n\t    def from_db(item: models.Filament) -> \"Filament\":\n", "        \"\"\"Create a new Pydantic filament object from a database filament object.\"\"\"\n\t        return Filament(\n\t            id=item.id,\n\t            registered=item.registered,\n\t            name=item.name,\n\t            vendor=Vendor.from_db(item.vendor) if item.vendor is not None else None,\n\t            material=item.material,\n\t            price=item.price,\n\t            density=item.density,\n\t            diameter=item.diameter,\n", "            weight=item.weight,\n\t            spool_weight=item.spool_weight,\n\t            article_number=item.article_number,\n\t            comment=item.comment,\n\t            settings_extruder_temp=item.settings_extruder_temp,\n\t            settings_bed_temp=item.settings_bed_temp,\n\t            color_hex=item.color_hex,\n\t        )\n\tclass Spool(BaseModel):\n\t    id: int = Field(description=\"Unique internal ID of this spool of filament.\")\n", "    registered: datetime = Field(description=\"When the spool was registered in the database. UTC Timezone.\")\n\t    first_used: Optional[datetime] = Field(description=\"First logged occurence of spool usage. UTC Timezone.\")\n\t    last_used: Optional[datetime] = Field(description=\"Last logged occurence of spool usage. UTC Timezone.\")\n\t    filament: Filament = Field(description=\"The filament type of this spool.\")\n\t    remaining_weight: Optional[float] = Field(\n\t        default=None,\n\t        ge=0,\n\t        description=(\n\t            \"Estimated remaining weight of filament on the spool in grams. \"\n\t            \"Only set if the filament type has a weight set.\"\n", "        ),\n\t        example=500.6,\n\t    )\n\t    used_weight: float = Field(ge=0, description=\"Consumed weight of filament from the spool in grams.\", example=500.3)\n\t    remaining_length: Optional[float] = Field(\n\t        default=None,\n\t        ge=0,\n\t        description=(\n\t            \"Estimated remaining length of filament on the spool in millimeters.\"\n\t            \" Only set if the filament type has a weight set.\"\n", "        ),\n\t        example=5612.4,\n\t    )\n\t    used_length: float = Field(\n\t        ge=0,\n\t        description=\"Consumed length of filament from the spool in millimeters.\",\n\t        example=50.7,\n\t    )\n\t    location: Optional[str] = Field(max_length=64, description=\"Where this spool can be found.\", example=\"Shelf A\")\n\t    lot_nr: Optional[str] = Field(\n", "        max_length=64,\n\t        description=\"Vendor manufacturing lot/batch number of the spool.\",\n\t        example=\"52342\",\n\t    )\n\t    comment: Optional[str] = Field(\n\t        max_length=1024,\n\t        description=\"Free text comment about this specific spool.\",\n\t        example=\"\",\n\t    )\n\t    archived: bool = Field(description=\"Whether this spool is archived and should not be used anymore.\")\n", "    @staticmethod\n\t    def from_db(item: models.Spool) -> \"Spool\":\n\t        \"\"\"Create a new Pydantic spool object from a database spool object.\"\"\"\n\t        filament = Filament.from_db(item.filament)\n\t        remaining_weight: Optional[float] = None  # noqa: FA100\n\t        remaining_length: Optional[float] = None  # noqa: FA100\n\t        if filament.weight is not None:\n\t            remaining_weight = max(filament.weight - item.used_weight, 0)\n\t            remaining_length = length_from_weight(\n\t                weight=remaining_weight,\n", "                density=filament.density,\n\t                diameter=filament.diameter,\n\t            )\n\t        used_length = length_from_weight(\n\t            weight=item.used_weight,\n\t            density=filament.density,\n\t            diameter=filament.diameter,\n\t        )\n\t        return Spool(\n\t            id=item.id,\n", "            registered=item.registered,\n\t            first_used=item.first_used,\n\t            last_used=item.last_used,\n\t            filament=filament,\n\t            used_weight=item.used_weight,\n\t            used_length=used_length,\n\t            remaining_weight=remaining_weight,\n\t            remaining_length=remaining_length,\n\t            location=item.location,\n\t            lot_nr=item.lot_nr,\n", "            comment=item.comment,\n\t            archived=item.archived if item.archived is not None else False,\n\t        )\n\tclass Info(BaseModel):\n\t    version: str = Field(example=\"0.7.0\")\n\t    debug_mode: bool = Field(example=False)\n\t    automatic_backups: bool = Field(example=True)\n\t    data_dir: str = Field(example=\"/home/app/.local/share/spoolman\")\n\t    backups_dir: str = Field(example=\"/home/app/.local/share/spoolman/backups\")\n\t    db_type: str = Field(example=\"sqlite\")\n", "class HealthCheck(BaseModel):\n\t    status: str = Field(example=\"healthy\")\n\tclass BackupResponse(BaseModel):\n\t    path: str = Field(\n\t        default=None,\n\t        description=\"Path to the created backup file.\",\n\t        example=\"/home/app/.local/share/spoolman/backups/spoolman.db\",\n\t    )\n"]}
{"filename": "spoolman/api/v1/vendor.py", "chunked_list": ["\"\"\"Vendor related endpoints.\"\"\"\n\tfrom typing import Annotated, Optional\n\tfrom fastapi import APIRouter, Depends, Query\n\tfrom fastapi.exceptions import RequestValidationError\n\tfrom pydantic import BaseModel, Field\n\tfrom pydantic.error_wrappers import ErrorWrapper\n\tfrom sqlalchemy.ext.asyncio import AsyncSession\n\tfrom spoolman.api.v1.models import Message, Vendor\n\tfrom spoolman.database import vendor\n\tfrom spoolman.database.database import get_db_session\n", "router = APIRouter(\n\t    prefix=\"/vendor\",\n\t    tags=[\"vendor\"],\n\t)\n\t# ruff: noqa: D103,B008\n\tclass VendorParameters(BaseModel):\n\t    name: str = Field(max_length=64, description=\"Vendor name.\", example=\"Polymaker\")\n\t    comment: Optional[str] = Field(\n\t        max_length=1024,\n\t        description=\"Free text comment about this vendor.\",\n", "        example=\"\",\n\t    )\n\tclass VendorUpdateParameters(VendorParameters):\n\t    name: Optional[str] = Field(max_length=64, description=\"Vendor name.\", example=\"Polymaker\")\n\t    comment: Optional[str] = Field(\n\t        max_length=1024,\n\t        description=\"Free text comment about this vendor.\",\n\t        example=\"\",\n\t    )\n\t@router.get(\n", "    \"\",\n\t    name=\"Find vendor\",\n\t    description=\"Get a list of vendors that matches the search query.\",\n\t    response_model_exclude_none=True,\n\t)\n\tasync def find(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    name: Optional[str] = Query(\n\t        default=None,\n\t        title=\"Vendor Name\",\n", "        description=\"Partial case-insensitive search term for the vendor name.\",\n\t    ),\n\t) -> list[Vendor]:\n\t    db_items = await vendor.find(\n\t        db=db,\n\t        name=name,\n\t    )\n\t    return [Vendor.from_db(db_item) for db_item in db_items]\n\t@router.get(\n\t    \"/{vendor_id}\",\n", "    name=\"Get vendor\",\n\t    description=\"Get a specific vendor.\",\n\t    response_model_exclude_none=True,\n\t    responses={404: {\"model\": Message}},\n\t)\n\tasync def get(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    vendor_id: int,\n\t) -> Vendor:\n\t    db_item = await vendor.get_by_id(db, vendor_id)\n", "    return Vendor.from_db(db_item)\n\t@router.post(\n\t    \"\",\n\t    name=\"Add vendor\",\n\t    description=\"Add a new vendor to the database.\",\n\t    response_model_exclude_none=True,\n\t)\n\tasync def create(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    body: VendorParameters,\n", ") -> Vendor:\n\t    db_item = await vendor.create(\n\t        db=db,\n\t        name=body.name,\n\t        comment=body.comment,\n\t    )\n\t    return Vendor.from_db(db_item)\n\t@router.patch(\n\t    \"/{vendor_id}\",\n\t    name=\"Update vendor\",\n", "    description=\"Update any attribute of a vendor. Only fields specified in the request will be affected.\",\n\t    response_model_exclude_none=True,\n\t    responses={404: {\"model\": Message}},\n\t)\n\tasync def update(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    vendor_id: int,\n\t    body: VendorUpdateParameters,\n\t) -> Vendor:\n\t    patch_data = body.dict(exclude_unset=True)\n", "    if \"name\" in patch_data and body.name is None:\n\t        raise RequestValidationError([ErrorWrapper(ValueError(\"name cannot be unset\"), (\"query\", \"name\"))])\n\t    db_item = await vendor.update(\n\t        db=db,\n\t        vendor_id=vendor_id,\n\t        data=patch_data,\n\t    )\n\t    return Vendor.from_db(db_item)\n\t@router.delete(\n\t    \"/{vendor_id}\",\n", "    name=\"Delete vendor\",\n\t    description=(\n\t        \"Delete a vendor. The vendor attribute of any filaments who refer to the deleted vendor will be cleared.\"\n\t    ),\n\t    responses={404: {\"model\": Message}},\n\t)\n\tasync def delete(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    vendor_id: int,\n\t) -> Message:\n", "    await vendor.delete(db, vendor_id)\n\t    return Message(message=\"Success!\")\n"]}
{"filename": "spoolman/api/v1/router.py", "chunked_list": ["\"\"\"Router setup for the v1 version of the API.\"\"\"\n\t# ruff: noqa: D103\n\timport logging\n\tfrom fastapi import FastAPI\n\tfrom fastapi.responses import JSONResponse\n\tfrom starlette.requests import Request\n\tfrom starlette.responses import Response\n\tfrom spoolman import env\n\tfrom spoolman.database.database import backup_global_db\n\tfrom spoolman.exceptions import ItemNotFoundError\n", "from . import filament, models, spool, vendor\n\tlogger = logging.getLogger(__name__)\n\tapp = FastAPI(\n\t    title=\"Spoolman REST API v1\",\n\t    version=\"1.0.0\",\n\t    root_path_in_servers=False,\n\t)\n\t@app.exception_handler(ItemNotFoundError)\n\tasync def itemnotfounderror_exception_handler(_request: Request, exc: ItemNotFoundError) -> Response:\n\t    logger.debug(exc, exc_info=True)\n", "    return JSONResponse(\n\t        status_code=404,\n\t        content={\"message\": exc.args[0]},\n\t    )\n\t# Add a general info endpoint\n\t@app.get(\"/info\")\n\tasync def info() -> models.Info:\n\t    \"\"\"Return general info about the API.\"\"\"\n\t    return models.Info(\n\t        version=env.get_version(),\n", "        debug_mode=env.is_debug_mode(),\n\t        automatic_backups=env.is_automatic_backup_enabled(),\n\t        data_dir=str(env.get_data_dir().resolve()),\n\t        backups_dir=str(env.get_backups_dir().resolve()),\n\t        db_type=str(env.get_database_type() or \"sqlite\"),\n\t    )\n\t# Add health check endpoint\n\t@app.get(\"/health\")\n\tasync def health() -> models.HealthCheck:\n\t    \"\"\"Return a health check.\"\"\"\n", "    return models.HealthCheck(status=\"healthy\")\n\t# Add endpoint for triggering a db backup\n\t@app.post(\n\t    \"/backup\",\n\t    description=\"Trigger a database backup. Only applicable for SQLite databases.\",\n\t    response_model=models.BackupResponse,\n\t    responses={500: {\"model\": models.Message}},\n\t)\n\tasync def backup():  # noqa: ANN201\n\t    \"\"\"Trigger a database backup.\"\"\"\n", "    path = await backup_global_db()\n\t    if path is None:\n\t        return JSONResponse(\n\t            status_code=500,\n\t            content={\"message\": \"Backup failed. See server logs for more information.\"},\n\t        )\n\t    return models.BackupResponse(path=str(path))\n\t# Add routers\n\tapp.include_router(filament.router)\n\tapp.include_router(spool.router)\n", "app.include_router(vendor.router)\n"]}
{"filename": "spoolman/api/v1/__init__.py", "chunked_list": []}
{"filename": "spoolman/api/v1/spool.py", "chunked_list": ["\"\"\"Spool related endpoints.\"\"\"\n\timport logging\n\tfrom datetime import datetime\n\tfrom typing import Annotated, Optional\n\tfrom fastapi import APIRouter, Depends, Query\n\tfrom fastapi.exceptions import RequestValidationError\n\tfrom fastapi.responses import JSONResponse\n\tfrom pydantic import BaseModel, Field\n\tfrom pydantic.error_wrappers import ErrorWrapper\n\tfrom sqlalchemy.ext.asyncio import AsyncSession\n", "from spoolman.api.v1.models import Message, Spool\n\tfrom spoolman.database import spool\n\tfrom spoolman.database.database import get_db_session\n\tfrom spoolman.exceptions import ItemCreateError\n\tlogger = logging.getLogger(__name__)\n\trouter = APIRouter(\n\t    prefix=\"/spool\",\n\t    tags=[\"spool\"],\n\t)\n\t# ruff: noqa: D103,B008\n", "class SpoolParameters(BaseModel):\n\t    first_used: Optional[datetime] = Field(description=\"First logged occurence of spool usage.\")\n\t    last_used: Optional[datetime] = Field(description=\"Last logged occurence of spool usage.\")\n\t    filament_id: int = Field(description=\"The ID of the filament type of this spool.\")\n\t    remaining_weight: Optional[float] = Field(\n\t        ge=0,\n\t        description=(\n\t            \"Remaining weight of filament on the spool. Can only be used if the filament type has a weight set.\"\n\t        ),\n\t        example=800,\n", "    )\n\t    used_weight: Optional[float] = Field(ge=0, description=\"Used weight of filament on the spool.\", example=200)\n\t    location: Optional[str] = Field(max_length=64, description=\"Where this spool can be found.\", example=\"Shelf A\")\n\t    lot_nr: Optional[str] = Field(\n\t        max_length=64,\n\t        description=\"Vendor manufacturing lot/batch number of the spool.\",\n\t        example=\"52342\",\n\t    )\n\t    comment: Optional[str] = Field(\n\t        max_length=1024,\n", "        description=\"Free text comment about this specific spool.\",\n\t        example=\"\",\n\t    )\n\t    archived: bool = Field(default=False, description=\"Whether this spool is archived and should not be used anymore.\")\n\tclass SpoolUpdateParameters(SpoolParameters):\n\t    filament_id: Optional[int] = Field(description=\"The ID of the filament type of this spool.\")\n\tclass SpoolUseParameters(BaseModel):\n\t    use_length: Optional[float] = Field(description=\"Length of filament to reduce by, in mm.\", example=2.2)\n\t    use_weight: Optional[float] = Field(description=\"Filament weight to reduce by, in g.\", example=5.3)\n\t@router.get(\n", "    \"\",\n\t    name=\"Find spool\",\n\t    description=\"Get a list of spools that matches the search query.\",\n\t    response_model_exclude_none=True,\n\t)\n\tasync def find(\n\t    *,\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    filament_name: Optional[str] = Query(\n\t        default=None,\n", "        title=\"Filament Name\",\n\t        description=\"Partial case-insensitive search term for the filament name.\",\n\t    ),\n\t    filament_id: Optional[int] = Query(\n\t        default=None,\n\t        title=\"Filament ID\",\n\t        description=\"Match an exact filament ID.\",\n\t    ),\n\t    filament_material: Optional[str] = Query(\n\t        default=None,\n", "        title=\"Filament Material\",\n\t        description=\"Partial case-insensitive search term for the filament material.\",\n\t    ),\n\t    vendor_name: Optional[str] = Query(\n\t        default=None,\n\t        title=\"Vendor Name\",\n\t        description=\"Partial case-insensitive search term for the filament vendor name.\",\n\t    ),\n\t    vendor_id: Optional[int] = Query(\n\t        default=None,\n", "        title=\"Vendor ID\",\n\t        description=\"Match an exact vendor ID.\",\n\t    ),\n\t    location: Optional[str] = Query(\n\t        default=None,\n\t        title=\"Location\",\n\t        description=\"Partial case-insensitive search term for the spool location.\",\n\t    ),\n\t    lot_nr: Optional[str] = Query(\n\t        default=None,\n", "        title=\"Lot/Batch Number\",\n\t        description=\"Partial case-insensitive search term for the spool lot number.\",\n\t    ),\n\t    allow_archived: bool = Query(\n\t        default=False,\n\t        title=\"Allow Archived\",\n\t        description=\"Whether to include archived spools in the search results.\",\n\t    ),\n\t) -> list[Spool]:\n\t    db_items = await spool.find(\n", "        db=db,\n\t        filament_name=filament_name,\n\t        filament_id=filament_id,\n\t        filament_material=filament_material,\n\t        vendor_name=vendor_name,\n\t        vendor_id=vendor_id,\n\t        location=location,\n\t        lot_nr=lot_nr,\n\t        allow_archived=allow_archived,\n\t    )\n", "    return [Spool.from_db(db_item) for db_item in db_items]\n\t@router.get(\n\t    \"/{spool_id}\",\n\t    name=\"Get spool\",\n\t    description=\"Get a specific spool.\",\n\t    response_model_exclude_none=True,\n\t    responses={404: {\"model\": Message}},\n\t)\n\tasync def get(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n", "    spool_id: int,\n\t) -> Spool:\n\t    db_item = await spool.get_by_id(db, spool_id)\n\t    return Spool.from_db(db_item)\n\t@router.post(\n\t    \"\",\n\t    name=\"Add spool\",\n\t    description=(\n\t        \"Add a new spool to the database. \"\n\t        \"Only specify either remaining_weight or used_weight. \"\n", "        \"If no weight is set, the spool will be assumed to be full.\"\n\t    ),\n\t    response_model_exclude_none=True,\n\t    response_model=Spool,\n\t    responses={\n\t        400: {\"model\": Message},\n\t    },\n\t)\n\tasync def create(  # noqa: ANN201\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n", "    body: SpoolParameters,\n\t):\n\t    if body.remaining_weight is not None and body.used_weight is not None:\n\t        return JSONResponse(\n\t            status_code=400,\n\t            content={\"message\": \"Only specify either remaining_weight or used_weight.\"},\n\t        )\n\t    try:\n\t        db_item = await spool.create(\n\t            db=db,\n", "            filament_id=body.filament_id,\n\t            remaining_weight=body.remaining_weight,\n\t            used_weight=body.used_weight,\n\t            first_used=body.first_used,\n\t            last_used=body.last_used,\n\t            location=body.location,\n\t            lot_nr=body.lot_nr,\n\t            comment=body.comment,\n\t            archived=body.archived,\n\t        )\n", "        return Spool.from_db(db_item)\n\t    except ItemCreateError:\n\t        logger.exception(\"Failed to create spool.\")\n\t        return JSONResponse(\n\t            status_code=400,\n\t            content={\"message\": \"Failed to create spool, see server logs for more information.\"},\n\t        )\n\t@router.patch(\n\t    \"/{spool_id}\",\n\t    name=\"Update spool\",\n", "    description=(\n\t        \"Update any attribute of a spool. \"\n\t        \"Only fields specified in the request will be affected. \"\n\t        \"remaining_weight and used_weight can't be set at the same time.\"\n\t    ),\n\t    response_model_exclude_none=True,\n\t    response_model=Spool,\n\t    responses={\n\t        400: {\"model\": Message},\n\t        404: {\"model\": Message},\n", "    },\n\t)\n\tasync def update(  # noqa: ANN201\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    spool_id: int,\n\t    body: SpoolUpdateParameters,\n\t):\n\t    patch_data = body.dict(exclude_unset=True)\n\t    if body.remaining_weight is not None and body.used_weight is not None:\n\t        return JSONResponse(\n", "            status_code=400,\n\t            content={\"message\": \"Only specify either remaining_weight or used_weight.\"},\n\t        )\n\t    if \"filament_id\" in patch_data and body.filament_id is None:\n\t        raise RequestValidationError(\n\t            [ErrorWrapper(ValueError(\"filament_id cannot be unset\"), (\"query\", \"filament_id\"))],\n\t        )\n\t    try:\n\t        db_item = await spool.update(\n\t            db=db,\n", "            spool_id=spool_id,\n\t            data=patch_data,\n\t        )\n\t    except ItemCreateError:\n\t        logger.exception(\"Failed to update spool.\")\n\t        return JSONResponse(\n\t            status_code=400,\n\t            content={\"message\": \"Failed to update spool, see server logs for more information.\"},\n\t        )\n\t    return Spool.from_db(db_item)\n", "@router.delete(\n\t    \"/{spool_id}\",\n\t    name=\"Delete spool\",\n\t    description=\"Delete a spool.\",\n\t    responses={404: {\"model\": Message}},\n\t)\n\tasync def delete(\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    spool_id: int,\n\t) -> Message:\n", "    await spool.delete(db, spool_id)\n\t    return Message(message=\"Success!\")\n\t@router.put(\n\t    \"/{spool_id}/use\",\n\t    name=\"Use spool filament\",\n\t    description=(\n\t        \"Use some length or weight of filament from the spool. Specify either a length or a weight, not both.\"\n\t    ),\n\t    response_model_exclude_none=True,\n\t    response_model=Spool,\n", "    responses={\n\t        400: {\"model\": Message},\n\t        404: {\"model\": Message},\n\t    },\n\t)\n\tasync def use(  # noqa: ANN201\n\t    db: Annotated[AsyncSession, Depends(get_db_session)],\n\t    spool_id: int,\n\t    body: SpoolUseParameters,\n\t):\n", "    if body.use_weight is not None and body.use_length is not None:\n\t        return JSONResponse(\n\t            status_code=400,\n\t            content={\"message\": \"Only specify either use_weight or use_length.\"},\n\t        )\n\t    if body.use_weight is not None:\n\t        db_item = await spool.use_weight(db, spool_id, body.use_weight)\n\t        return Spool.from_db(db_item)\n\t    if body.use_length is not None:\n\t        db_item = await spool.use_length(db, spool_id, body.use_length)\n", "        return Spool.from_db(db_item)\n\t    return JSONResponse(\n\t        status_code=400,\n\t        content={\"message\": \"Either use_weight or use_length must be specified.\"},\n\t    )\n"]}
{"filename": "migrations/__init__.py", "chunked_list": ["\"\"\"Database migrations system.\"\"\"\n"]}
{"filename": "migrations/env.py", "chunked_list": ["\"\"\"Alembic environment file.\"\"\"\n\timport asyncio\n\tfrom logging.config import fileConfig\n\tfrom alembic import context\n\tfrom sqlalchemy.engine import Connection\n\tfrom spoolman.database.database import Database, get_connection_url\n\tfrom spoolman.database.models import Base\n\tconfig = context.config\n\tif config.config_file_name is not None:\n\t    fileConfig(config.config_file_name)\n", "target_metadata = Base.metadata\n\tdef run_migrations_offline() -> None:\n\t    \"\"\"Run migrations in 'offline' mode.\n\t    This configures the context with just a URL\n\t    and not an Engine, though an Engine is acceptable\n\t    here as well.  By skipping the Engine creation\n\t    we don't even need a DBAPI to be available.\n\t    Calls to context.execute() here emit the given string to the\n\t    script output.\n\t    \"\"\"\n", "    context.configure(\n\t        url=get_connection_url(),\n\t        target_metadata=target_metadata,\n\t        literal_binds=True,\n\t        dialect_opts={\"paramstyle\": \"named\"},\n\t        render_as_batch=True,\n\t    )\n\t    with context.begin_transaction():\n\t        context.run_migrations()\n\tdef do_run_migrations(connection: Connection) -> None:\n", "    \"\"\"Run migrations in 'online' mode.\"\"\"\n\t    context.configure(connection=connection, target_metadata=target_metadata)\n\t    with context.begin_transaction():\n\t        context.run_migrations()\n\tasync def run_async_migrations() -> None:\n\t    \"\"\"In this scenario we need to create an Engine and associate a connection with the context.\"\"\"\n\t    db = Database(get_connection_url())\n\t    db.connect()\n\t    if db.engine is None:\n\t        raise RuntimeError(\"Engine not created.\")\n", "    async with db.engine.connect() as connection:\n\t        await connection.run_sync(do_run_migrations)\n\t    await db.engine.dispose()\n\tif context.is_offline_mode():\n\t    run_migrations_offline()\n\telse:\n\t    asyncio.run(run_async_migrations())\n"]}
{"filename": "migrations/versions/2023_08_12_2121-92793c8a937c_color_hex_alpha.py", "chunked_list": ["\"\"\"color_hex alpha.\n\tRevision ID: 92793c8a937c\n\tRevises: 92186a5f7b0f\n\tCreate Date: 2023-08-12 21:21:08.536216\n\t\"\"\"\n\timport sqlalchemy as sa\n\tfrom alembic import op\n\t# revision identifiers, used by Alembic.\n\trevision = \"92793c8a937c\"\n\tdown_revision = \"92186a5f7b0f\"\n", "branch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    \"\"\"Perform the upgrade.\"\"\"\n\t    with op.batch_alter_table(\"filament\") as batch_op:\n\t        batch_op.alter_column(\"color_hex\", type_=sa.String(length=8), existing_nullable=True)\n\tdef downgrade() -> None:\n\t    \"\"\"Perform the downgrade.\"\"\"\n\t    with op.batch_alter_table(\"filament\") as batch_op:\n\t        batch_op.alter_column(\"color_hex\", type_=sa.String(length=6), existing_nullable=True)\n"]}
{"filename": "migrations/versions/2023_05_28_2136-b47376d60c6d_add_extruder_and_bed_temperature_.py", "chunked_list": ["\"\"\"add extruder and bed temperature override.\n\tRevision ID: b47376d60c6d\n\tRevises: 684d32cf7e4d\n\tCreate Date: 2023-05-28 21:36:53.452067\n\t\"\"\"\n\timport sqlalchemy as sa\n\tfrom alembic import op\n\t# revision identifiers, used by Alembic.\n\trevision = \"b47376d60c6d\"\n\tdown_revision = \"684d32cf7e4d\"\n", "branch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    \"\"\"Perform the upgrade.\"\"\"\n\t    op.add_column(\n\t        \"filament\",\n\t        sa.Column(\"settings_extruder_temp\", sa.Integer(), nullable=True, comment=\"Overridden extruder temperature.\"),\n\t    )\n\t    op.add_column(\n\t        \"filament\",\n", "        sa.Column(\"settings_bed_temp\", sa.Integer(), nullable=True, comment=\"Overridden bed temperature.\"),\n\t    )\n\tdef downgrade() -> None:\n\t    \"\"\"Perform the downgrade.\"\"\"\n\t    op.drop_column(\"filament\", \"settings_bed_temp\")\n\t    op.drop_column(\"filament\", \"settings_extruder_temp\")\n"]}
{"filename": "migrations/versions/2023_07_14_1217-92186a5f7b0f_add_spool_archived_field.py", "chunked_list": ["\"\"\"add spool archived field.\n\tRevision ID: 92186a5f7b0f\n\tRevises: db385b808a20\n\tCreate Date: 2023-07-14 12:17:13.162618\n\t\"\"\"\n\timport sqlalchemy as sa\n\tfrom alembic import op\n\t# revision identifiers, used by Alembic.\n\trevision = \"92186a5f7b0f\"\n\tdown_revision = \"db385b808a20\"\n", "branch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    \"\"\"Perform the upgrade.\"\"\"\n\t    op.add_column(\"spool\", sa.Column(\"archived\", sa.Boolean(), nullable=True))\n\tdef downgrade() -> None:\n\t    \"\"\"Perform the downgrade.\"\"\"\n\t    op.drop_column(\"spool\", \"archived\")\n"]}
{"filename": "migrations/versions/__init__.py", "chunked_list": ["\"\"\"Database migration versions.\"\"\"\n"]}
{"filename": "migrations/versions/2023_05_27_2146-684d32cf7e4d_initial.py", "chunked_list": ["\"\"\"initial.\n\tRevision ID: 684d32cf7e4d\n\tCreate Date: 2023-05-27 21:46:24.361353\n\t\"\"\"\n\timport sqlalchemy as sa\n\tfrom alembic import op\n\tfrom sqlalchemy.engine.reflection import Inspector\n\t# revision identifiers, used by Alembic.\n\trevision = \"684d32cf7e4d\"\n\tdown_revision = None\n", "branch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    \"\"\"Perform the upgrade.\"\"\"\n\t    conn = op.get_bind()\n\t    inspector = Inspector.from_engine(conn)  # type: ignore[arg-type]\n\t    tables = inspector.get_table_names()\n\t    if \"vendor\" in tables:\n\t        # If the vendor table exists, we assume that the initial migration has already been performed.\n\t        return\n", "    op.create_table(\n\t        \"vendor\",\n\t        sa.Column(\"id\", sa.Integer(), nullable=False),\n\t        sa.Column(\"registered\", sa.DateTime(), nullable=False, server_default=sa.func.now()),\n\t        sa.Column(\"name\", sa.String(length=64), nullable=False),\n\t        sa.Column(\"comment\", sa.String(length=1024), nullable=True),\n\t        sa.PrimaryKeyConstraint(\"id\"),\n\t    )\n\t    op.create_index(op.f(\"ix_vendor_id\"), \"vendor\", [\"id\"], unique=False)\n\t    op.create_table(\n", "        \"filament\",\n\t        sa.Column(\"id\", sa.Integer(), nullable=False),\n\t        sa.Column(\"registered\", sa.DateTime(), nullable=False, server_default=sa.func.now()),\n\t        sa.Column(\"name\", sa.String(length=64), nullable=True),\n\t        sa.Column(\"vendor_id\", sa.Integer(), nullable=True),\n\t        sa.Column(\"material\", sa.String(length=64), nullable=True),\n\t        sa.Column(\"price\", sa.Float(), nullable=True),\n\t        sa.Column(\"density\", sa.Float(), nullable=False),\n\t        sa.Column(\"diameter\", sa.Float(), nullable=False),\n\t        sa.Column(\"weight\", sa.Float(), nullable=True, comment=\"The filament weight of a full spool (net weight).\"),\n", "        sa.Column(\"spool_weight\", sa.Float(), nullable=True, comment=\"The weight of an empty spool.\"),\n\t        sa.Column(\"article_number\", sa.String(length=64), nullable=True),\n\t        sa.Column(\"comment\", sa.String(length=1024), nullable=True),\n\t        sa.ForeignKeyConstraint(\n\t            [\"vendor_id\"],\n\t            [\"vendor.id\"],\n\t        ),\n\t        sa.PrimaryKeyConstraint(\"id\"),\n\t    )\n\t    op.create_index(op.f(\"ix_filament_id\"), \"filament\", [\"id\"], unique=False)\n", "    op.create_table(\n\t        \"spool\",\n\t        sa.Column(\"id\", sa.Integer(), nullable=False),\n\t        sa.Column(\"registered\", sa.DateTime(), nullable=False, server_default=sa.func.now()),\n\t        sa.Column(\"first_used\", sa.DateTime(), nullable=True),\n\t        sa.Column(\"last_used\", sa.DateTime(), nullable=True),\n\t        sa.Column(\"filament_id\", sa.Integer(), nullable=False),\n\t        sa.Column(\"used_weight\", sa.Float(), nullable=False),\n\t        sa.Column(\"location\", sa.String(length=64), nullable=True),\n\t        sa.Column(\"lot_nr\", sa.String(length=64), nullable=True),\n", "        sa.Column(\"comment\", sa.String(length=1024), nullable=True),\n\t        sa.ForeignKeyConstraint(\n\t            [\"filament_id\"],\n\t            [\"filament.id\"],\n\t        ),\n\t        sa.PrimaryKeyConstraint(\"id\"),\n\t    )\n\t    op.create_index(op.f(\"ix_spool_id\"), \"spool\", [\"id\"], unique=False)\n\tdef downgrade() -> None:\n\t    \"\"\"Perform the downgrade.\"\"\"\n", "    op.drop_index(op.f(\"ix_spool_id\"), table_name=\"spool\")\n\t    op.drop_table(\"spool\")\n\t    op.drop_index(op.f(\"ix_filament_id\"), table_name=\"filament\")\n\t    op.drop_table(\"filament\")\n\t    op.drop_index(op.f(\"ix_vendor_id\"), table_name=\"vendor\")\n\t    op.drop_table(\"vendor\")\n"]}
{"filename": "migrations/versions/2023_06_01_1953-db385b808a20_add_filament_color_code.py", "chunked_list": ["\"\"\"add filament color code.\n\tRevision ID: db385b808a20\n\tRevises: b47376d60c6d\n\tCreate Date: 2023-06-01 19:53:44.440616\n\t\"\"\"\n\timport sqlalchemy as sa\n\tfrom alembic import op\n\t# revision identifiers, used by Alembic.\n\trevision = \"db385b808a20\"\n\tdown_revision = \"b47376d60c6d\"\n", "branch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    \"\"\"Perform the upgrade.\"\"\"\n\t    op.add_column(\"filament\", sa.Column(\"color_hex\", sa.String(length=6), nullable=True))\n\tdef downgrade() -> None:\n\t    \"\"\"Perform the downgrade.\"\"\"\n\t    op.drop_column(\"filament\", \"color_hex\")\n"]}
{"filename": "tests_integration/run.py", "chunked_list": ["\"\"\"Build and run the integration tests.\"\"\"\n\t# ruff: noqa: S605, S607, T201\n\timport os\n\timport sys\n\tif __name__ == \"__main__\":\n\t    print(\"Building and running integration tests...\")\n\t    print(\"Building Spoolman...\")\n\t    if os.system(\"docker build -t donkie/spoolman:test .\") > 0:\n\t        print(\"Failed to build Spoolman!\")\n\t        sys.exit(1)\n", "    print(\"Building Spoolman tester...\")\n\t    if os.system(\"docker build -t donkie/spoolman-tester:latest tests_integration\") > 0:\n\t        print(\"Failed to build Spoolman tester!\")\n\t        sys.exit(1)\n\t    # Support input arguments for running only specific tests\n\t    if len(sys.argv) > 1:\n\t        targets = sys.argv[1:]\n\t        # Check that all targets are valid\n\t        for target in targets:\n\t            if target not in [\"postgres\", \"sqlite\", \"mariadb\", \"cockroachdb\"]:\n", "                print(f\"Unknown target: {target}\")\n\t                sys.exit(1)\n\t    else:\n\t        print(\"No targets specified, running all tests...\")\n\t        targets = [\n\t            \"postgres\",\n\t            \"sqlite\",\n\t            \"mariadb\",\n\t            \"cockroachdb\",\n\t        ]\n", "    for target in targets:\n\t        print(f\"Running integration tests against {target}...\")\n\t        os.system(f\"docker-compose -f tests_integration/docker-compose-{target}.yml down -v\")\n\t        if (\n\t            os.system(f\"docker-compose -f tests_integration/docker-compose-{target}.yml up --abort-on-container-exit\")\n\t            > 0\n\t        ):\n\t            print(f\"Integration tests against {target} failed!\")\n\t            sys.exit(1)\n\t    print(\"Integration tests passed!\")\n"]}
{"filename": "tests_integration/__init__.py", "chunked_list": ["\"\"\"Integration tests root.\"\"\"\n"]}
{"filename": "tests_integration/tests/test_vendor.py", "chunked_list": ["\"\"\"Integration tests for the Vendor API endpoint.\"\"\"\n\timport httpx\n\tURL = \"http://spoolman:8000\"\n\tdef test_add_vendor():\n\t    \"\"\"Test adding a vendor to the database.\"\"\"\n\t    # Execute\n\t    name = \"John\"\n\t    comment = \"abcdefghåäö\"\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n", "        json={\"name\": name, \"comment\": comment},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    vendor = result.json()\n\t    assert vendor == {\n\t        \"id\": vendor[\"id\"],\n\t        \"registered\": vendor[\"registered\"],\n\t        \"name\": name,\n\t        \"comment\": comment,\n", "    }\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/vendor/{vendor['id']}\").raise_for_status()\n\tdef test_add_vendor_required():\n\t    \"\"\"Test adding a vendor with only the required fields to the database.\"\"\"\n\t    # Execute\n\t    name = \"John\"\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n\t        json={\"name\": name},\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    vendor = result.json()\n\t    assert vendor == {\n\t        \"id\": vendor[\"id\"],\n\t        \"registered\": vendor[\"registered\"],\n\t        \"name\": name,\n\t    }\n\t    # Clean up\n", "    httpx.delete(f\"{URL}/api/v1/vendor/{vendor['id']}\").raise_for_status()\n\tdef test_get_vendor():\n\t    \"\"\"Test getting a vendor from the database.\"\"\"\n\t    # Setup\n\t    name = \"John\"\n\t    comment = \"abcdefghåäö\"\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n\t        json={\"name\": name, \"comment\": comment},\n\t    )\n", "    result.raise_for_status()\n\t    added_vendor = result.json()\n\t    # Execute\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/vendor/{added_vendor['id']}\",\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    vendor = result.json()\n\t    assert vendor[\"name\"] == name\n", "    assert vendor[\"comment\"] == comment\n\t    assert vendor[\"id\"] == added_vendor[\"id\"]\n\t    assert vendor[\"registered\"] == added_vendor[\"registered\"]\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/vendor/{vendor['id']}\").raise_for_status()\n\tdef test_get_vendor_not_found():\n\t    \"\"\"Test getting a vendor that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.get(f\"{URL}/api/v1/vendor/123456789\")\n\t    # Verify\n", "    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"vendor\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n\tdef test_find_vendors():\n\t    \"\"\"Test finding vendors from the database.\"\"\"\n\t    # Setup\n\t    name_1 = \"John\"\n\t    comment_1 = \"abcdefghåäö\"\n", "    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n\t        json={\"name\": name_1, \"comment\": comment_1},\n\t    )\n\t    result.raise_for_status()\n\t    vendor_1 = result.json()\n\t    name_2 = \"Stan\"\n\t    comment_2 = \"gfdadfg\"\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n", "        json={\"name\": name_2, \"comment\": comment_2},\n\t    )\n\t    result.raise_for_status()\n\t    vendor_2 = result.json()\n\t    added_vendors_by_id = {\n\t        vendor_1[\"id\"]: vendor_1,\n\t        vendor_2[\"id\"]: vendor_2,\n\t    }\n\t    # Execute - Find all vendors\n\t    result = httpx.get(\n", "        f\"{URL}/api/v1/vendor\",\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    vendors = result.json()\n\t    assert len(vendors) == 2\n\t    for vendor in vendors:\n\t        assert vendor == added_vendors_by_id[vendor[\"id\"]]\n\t    # Execute - Find a specific vendor\n\t    result = httpx.get(\n", "        f\"{URL}/api/v1/vendor\",\n\t        params={\"name\": name_1},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    vendors = result.json()\n\t    assert len(vendors) == 1\n\t    vendor = vendors[0]\n\t    assert vendor[\"name\"] == name_1\n\t    assert vendor[\"comment\"] == comment_1\n", "    assert vendor[\"id\"] == vendor_1[\"id\"]\n\t    assert vendor[\"registered\"] == vendor_1[\"registered\"]\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/vendor/{vendor_1['id']}\").raise_for_status()\n\t    httpx.delete(f\"{URL}/api/v1/vendor/{vendor_2['id']}\").raise_for_status()\n\tdef test_delete_vendor():\n\t    \"\"\"Test deleting a vendor from the database.\"\"\"\n\t    # Setup\n\t    name = \"John\"\n\t    comment = \"abcdefghåäö\"\n", "    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n\t        json={\"name\": name, \"comment\": comment},\n\t    )\n\t    result.raise_for_status()\n\t    added_vendor = result.json()\n\t    # Execute\n\t    httpx.delete(\n\t        f\"{URL}/api/v1/vendor/{added_vendor['id']}\",\n\t    ).raise_for_status()\n", "    # Verify\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/vendor/{added_vendor['id']}\",\n\t    )\n\t    assert result.status_code == 404\n\tdef test_delete_vendor_not_found():\n\t    \"\"\"Test deleting a vendor that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.delete(f\"{URL}/api/v1/vendor/123456789\")\n\t    # Verify\n", "    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"vendor\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n\tdef test_update_vendor():\n\t    \"\"\"Test update a vendor in the database.\"\"\"\n\t    # Setup\n\t    name = \"John\"\n\t    comment = \"abcdefghåäö\"\n", "    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n\t        json={\"name\": name, \"comment\": comment},\n\t    )\n\t    result.raise_for_status()\n\t    added_vendor = result.json()\n\t    # Execute\n\t    new_name = \"Stan\"\n\t    new_comment = \"gfdadfg\"\n\t    result = httpx.patch(\n", "        f\"{URL}/api/v1/vendor/{added_vendor['id']}\",\n\t        json={\"name\": new_name, \"comment\": new_comment},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    vendor = result.json()\n\t    assert vendor[\"name\"] == new_name\n\t    assert vendor[\"comment\"] == new_comment\n\t    assert vendor[\"id\"] == added_vendor[\"id\"]\n\t    assert vendor[\"registered\"] == added_vendor[\"registered\"]\n", "    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/vendor/{vendor['id']}\").raise_for_status()\n\tdef test_update_vendor_not_found():\n\t    \"\"\"Test updating a vendor that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.patch(f\"{URL}/api/v1/vendor/123456789\", json={\"name\": \"John\"})\n\t    # Verify\n\t    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"vendor\" in message\n", "    assert \"id\" in message\n\t    assert \"123456789\" in message\n"]}
{"filename": "tests_integration/tests/test_filament.py", "chunked_list": ["\"\"\"Integration tests for the Filament API endpoint.\"\"\"\n\tfrom typing import Any\n\timport httpx\n\tURL = \"http://spoolman:8000\"\n\tdef test_add_filament(random_vendor: dict[str, Any]):\n\t    \"\"\"Test adding a filament to the database.\"\"\"\n\t    # Execute\n\t    name = \"Filament X\"\n\t    material = \"PLA\"\n\t    price = 100\n", "    density = 1.25\n\t    diameter = 1.75\n\t    weight = 1000\n\t    spool_weight = 250\n\t    article_number = \"123456789\"\n\t    comment = \"abcdefghåäö\"\n\t    settings_extruder_temp = 200\n\t    settings_bed_temp = 60\n\t    color_hex = \"FF0000\"\n\t    result = httpx.post(\n", "        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"name\": name,\n\t            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": material,\n\t            \"price\": price,\n\t            \"density\": density,\n\t            \"diameter\": diameter,\n\t            \"weight\": weight,\n\t            \"spool_weight\": spool_weight,\n", "            \"article_number\": article_number,\n\t            \"comment\": comment,\n\t            \"settings_extruder_temp\": settings_extruder_temp,\n\t            \"settings_bed_temp\": settings_bed_temp,\n\t            \"color_hex\": color_hex,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filament = result.json()\n", "    assert filament == {\n\t        \"id\": filament[\"id\"],\n\t        \"registered\": filament[\"registered\"],\n\t        \"name\": name,\n\t        \"vendor\": random_vendor,\n\t        \"material\": material,\n\t        \"price\": price,\n\t        \"density\": density,\n\t        \"diameter\": diameter,\n\t        \"weight\": weight,\n", "        \"spool_weight\": spool_weight,\n\t        \"article_number\": article_number,\n\t        \"comment\": comment,\n\t        \"settings_extruder_temp\": settings_extruder_temp,\n\t        \"settings_bed_temp\": settings_bed_temp,\n\t        \"color_hex\": color_hex,\n\t    }\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/filament/{filament['id']}\").raise_for_status()\n\tdef test_add_filament_required():\n", "    \"\"\"Test adding a filament with only the required fields to the database.\"\"\"\n\t    # Execute\n\t    density = 1.25\n\t    diameter = 1.75\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"density\": density,\n\t            \"diameter\": diameter,\n\t        },\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filament = result.json()\n\t    assert filament == {\n\t        \"id\": filament[\"id\"],\n\t        \"registered\": filament[\"registered\"],\n\t        \"density\": density,\n\t        \"diameter\": diameter,\n\t    }\n", "    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/filament/{filament['id']}\").raise_for_status()\n\tdef test_get_filament(random_vendor: dict[str, Any]):\n\t    \"\"\"Test getting a filament from the database.\"\"\"\n\t    # Setup\n\t    name = \"Filament X\"\n\t    material = \"PLA\"\n\t    price = 100\n\t    density = 1.25\n\t    diameter = 1.75\n", "    weight = 1000\n\t    spool_weight = 250\n\t    article_number = \"123456789\"\n\t    comment = \"abcdefghåäö\"\n\t    settings_extruder_temp = 200\n\t    settings_bed_temp = 60\n\t    color_hex = \"FF0000\"\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n", "            \"name\": name,\n\t            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": material,\n\t            \"price\": price,\n\t            \"density\": density,\n\t            \"diameter\": diameter,\n\t            \"weight\": weight,\n\t            \"spool_weight\": spool_weight,\n\t            \"article_number\": article_number,\n\t            \"comment\": comment,\n", "            \"settings_extruder_temp\": settings_extruder_temp,\n\t            \"settings_bed_temp\": settings_bed_temp,\n\t            \"color_hex\": color_hex,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    added_filament = result.json()\n\t    # Execute\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament/{added_filament['id']}\",\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filament = result.json()\n\t    assert filament == {\n\t        \"id\": added_filament[\"id\"],\n\t        \"registered\": added_filament[\"registered\"],\n\t        \"name\": name,\n\t        \"vendor\": random_vendor,\n\t        \"material\": material,\n", "        \"price\": price,\n\t        \"density\": density,\n\t        \"diameter\": diameter,\n\t        \"weight\": weight,\n\t        \"spool_weight\": spool_weight,\n\t        \"article_number\": article_number,\n\t        \"comment\": comment,\n\t        \"settings_extruder_temp\": settings_extruder_temp,\n\t        \"settings_bed_temp\": settings_bed_temp,\n\t        \"color_hex\": color_hex,\n", "    }\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/filament/{filament['id']}\").raise_for_status()\n\tdef test_get_filament_not_found():\n\t    \"\"\"Test getting a filament that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament/123456789\",\n\t    )\n\t    # Verify\n", "    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"filament\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n\tdef test_find_filaments(random_vendor: dict[str, Any]):\n\t    \"\"\"Test finding filaments from the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n", "        json={\n\t            \"name\": \"Filament X\",\n\t            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": \"PLA\",\n\t            \"price\": 100,\n\t            \"density\": 1.25,\n\t            \"diameter\": 1.75,\n\t            \"weight\": 1000,\n\t            \"spool_weight\": 250,\n\t            \"article_number\": \"123456789\",\n", "            \"comment\": \"abcdefghåäö\",\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    filament_1 = result.json()\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"name\": \"Filament Y\",\n\t            \"vendor_id\": random_vendor[\"id\"],\n", "            \"material\": \"ABS\",\n\t            \"price\": 200,\n\t            \"density\": 1.25,\n\t            \"diameter\": 1.75,\n\t            \"weight\": 1000,\n\t            \"spool_weight\": 250,\n\t            \"article_number\": \"987654321\",\n\t            \"comment\": \"abcdefghåäö\",\n\t        },\n\t    )\n", "    result.raise_for_status()\n\t    filament_2 = result.json()\n\t    added_filaments_by_id = {\n\t        filament_1[\"id\"]: filament_1,\n\t        filament_2[\"id\"]: filament_2,\n\t    }\n\t    # Execute - find all filaments\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament\",\n\t    )\n", "    result.raise_for_status()\n\t    # Verify\n\t    filaments = result.json()\n\t    assert len(filaments) == 2\n\t    for filament in filaments:\n\t        assert filament == added_filaments_by_id[filament[\"id\"]]\n\t    # Execute - find filaments by name\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament\",\n\t        params={\"name\": \"Filament X\"},\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filaments = result.json()\n\t    assert len(filaments) == 1\n\t    assert filaments[0] == added_filaments_by_id[filament_1[\"id\"]]\n\t    # Execute - find filaments by material\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament\",\n\t        params={\"material\": \"abs\"},\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filaments = result.json()\n\t    assert len(filaments) == 1\n\t    assert filaments[0] == added_filaments_by_id[filament_2[\"id\"]]\n\t    # Execute - find filaments by vendor id\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament\",\n\t        params={\"vendor_id\": random_vendor[\"id\"]},\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filaments = result.json()\n\t    assert len(filaments) == 2\n\t    for filament in filaments:\n\t        assert filament == added_filaments_by_id[filament[\"id\"]]\n\t    # Execute - find filaments by vendor name\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament\",\n", "        params={\"vendor_name\": random_vendor[\"name\"]},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filaments = result.json()\n\t    assert len(filaments) == 2\n\t    for filament in filaments:\n\t        assert filament == added_filaments_by_id[filament[\"id\"]]\n\t    # Execute - find filaments by article number\n\t    result = httpx.get(\n", "        f\"{URL}/api/v1/filament\",\n\t        params={\"article_number\": \"321\"},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filaments = result.json()\n\t    assert len(filaments) == 1\n\t    assert filaments[0] == added_filaments_by_id[filament_2[\"id\"]]\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/filament/{filament_1['id']}\").raise_for_status()\n", "    httpx.delete(f\"{URL}/api/v1/filament/{filament_2['id']}\").raise_for_status()\n\tdef test_delete_filament(random_vendor: dict[str, Any]):\n\t    \"\"\"Test deleting a filament from the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"name\": \"Filament X\",\n\t            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": \"PLA\",\n", "            \"price\": 100,\n\t            \"density\": 1.25,\n\t            \"diameter\": 1.75,\n\t            \"weight\": 1000,\n\t            \"spool_weight\": 250,\n\t            \"article_number\": \"123456789\",\n\t            \"comment\": \"abcdefghåäö\",\n\t        },\n\t    )\n\t    result.raise_for_status()\n", "    added_filament = result.json()\n\t    # Execute\n\t    httpx.delete(\n\t        f\"{URL}/api/v1/filament/{added_filament['id']}\",\n\t    ).raise_for_status()\n\t    # Verify\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/filament/{added_filament['id']}\",\n\t    )\n\t    assert result.status_code == 404\n", "def test_delete_filament_not_found():\n\t    \"\"\"Test deleting a filament that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.delete(f\"{URL}/api/v1/filament/123456789\")\n\t    # Verify\n\t    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"filament\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n", "def test_update_filament(random_vendor: dict[str, Any]):\n\t    \"\"\"Test updating a filament in the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"name\": \"Filament X\",\n\t            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": \"PLA\",\n\t            \"price\": 100,\n", "            \"density\": 1.25,\n\t            \"diameter\": 1.75,\n\t            \"weight\": 1000,\n\t            \"spool_weight\": 250,\n\t            \"article_number\": \"123456789\",\n\t            \"comment\": \"abcdefghåäö\",\n\t            \"settings_extruder_temp\": 200,\n\t            \"settings_bed_temp\": 60,\n\t            \"color_hex\": \"FF0000\",\n\t        },\n", "    )\n\t    result.raise_for_status()\n\t    added_filament = result.json()\n\t    # Execute\n\t    new_name = \"Filament Y\"\n\t    new_material = \"ABS\"\n\t    new_price = 200\n\t    new_density = 4.2\n\t    new_diameter = 0.12\n\t    new_weight = 5431\n", "    new_spool_weight = 123\n\t    new_article_number = \"987654321\"\n\t    new_comment = \"test\"\n\t    new_settings_extruder_temp = 210\n\t    new_settings_bed_temp = 70\n\t    new_color_hex = \"00FF00\"\n\t    result = httpx.patch(\n\t        f\"{URL}/api/v1/filament/{added_filament['id']}\",\n\t        json={\n\t            \"name\": new_name,\n", "            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": new_material,\n\t            \"price\": new_price,\n\t            \"density\": new_density,\n\t            \"diameter\": new_diameter,\n\t            \"weight\": new_weight,\n\t            \"spool_weight\": new_spool_weight,\n\t            \"article_number\": new_article_number,\n\t            \"comment\": new_comment,\n\t            \"settings_extruder_temp\": new_settings_extruder_temp,\n", "            \"settings_bed_temp\": new_settings_bed_temp,\n\t            \"color_hex\": new_color_hex,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filament = result.json()\n\t    assert filament == {\n\t        \"id\": added_filament[\"id\"],\n\t        \"registered\": added_filament[\"registered\"],\n", "        \"name\": new_name,\n\t        \"vendor\": random_vendor,\n\t        \"material\": new_material,\n\t        \"price\": new_price,\n\t        \"density\": new_density,\n\t        \"diameter\": new_diameter,\n\t        \"weight\": new_weight,\n\t        \"spool_weight\": new_spool_weight,\n\t        \"article_number\": new_article_number,\n\t        \"comment\": new_comment,\n", "        \"settings_extruder_temp\": new_settings_extruder_temp,\n\t        \"settings_bed_temp\": new_settings_bed_temp,\n\t        \"color_hex\": new_color_hex,\n\t    }\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/filament/{filament['id']}\").raise_for_status()\n\tdef test_update_filament_not_found():\n\t    \"\"\"Test updating a filament that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.patch(\n", "        f\"{URL}/api/v1/filament/123456789\",\n\t        json={\"name\": \"Filament Y\"},\n\t    )\n\t    # Verify\n\t    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"filament\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n\tdef test_add_filament_color_hex_alpha():\n", "    \"\"\"Test adding a filament with an alpha channel in the color hex.\"\"\"\n\t    color_hex = \"FF000088\"\n\t    # Execute\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"density\": 1.25,\n\t            \"diameter\": 1.75,\n\t            \"color_hex\": color_hex,\n\t        },\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    filament = result.json()\n\t    assert filament[\"color_hex\"] == color_hex\n"]}
{"filename": "tests_integration/tests/__init__.py", "chunked_list": ["\"\"\"Tests for the integration of the application.\"\"\"\n"]}
{"filename": "tests_integration/tests/test_spool.py", "chunked_list": ["\"\"\"Integration tests for the Spool API endpoint.\"\"\"\n\timport asyncio\n\timport math\n\tfrom typing import Any\n\timport httpx\n\timport pytest\n\tfrom .conftest import length_from_weight\n\tURL = \"http://spoolman:8000\"\n\tdef test_add_spool_remaining_weight(random_filament: dict[str, Any]):\n\t    \"\"\"Test adding a spool to the database.\"\"\"\n", "    # Execute\n\t    remaining_weight = 750\n\t    location = \"The Pantry\"\n\t    lot_nr = \"123456789\"\n\t    comment = \"abcdefghåäö\"\n\t    archived = True\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"first_used\": \"2023-01-02T12:00:00+01:00\",\n", "            \"last_used\": \"2023-01-02T11:00:00Z\",\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": remaining_weight,\n\t            \"location\": location,\n\t            \"lot_nr\": lot_nr,\n\t            \"comment\": comment,\n\t            \"archived\": archived,\n\t        },\n\t    )\n\t    result.raise_for_status()\n", "    # Verify\n\t    used_weight = random_filament[\"weight\"] - remaining_weight\n\t    used_length = length_from_weight(\n\t        weight=used_weight,\n\t        density=random_filament[\"density\"],\n\t        diameter=random_filament[\"diameter\"],\n\t    )\n\t    remaining_length = length_from_weight(\n\t        weight=remaining_weight,\n\t        density=random_filament[\"density\"],\n", "        diameter=random_filament[\"diameter\"],\n\t    )\n\t    spool = result.json()\n\t    assert spool == {\n\t        \"id\": spool[\"id\"],\n\t        \"registered\": spool[\"registered\"],\n\t        \"first_used\": \"2023-01-02T11:00:00\",\n\t        \"last_used\": \"2023-01-02T11:00:00\",\n\t        \"filament\": random_filament,\n\t        \"remaining_weight\": pytest.approx(remaining_weight),\n", "        \"used_weight\": pytest.approx(used_weight),\n\t        \"remaining_length\": pytest.approx(remaining_length),\n\t        \"used_length\": pytest.approx(used_length),\n\t        \"location\": location,\n\t        \"lot_nr\": lot_nr,\n\t        \"comment\": comment,\n\t        \"archived\": archived,\n\t    }\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n", "def test_add_spool_used_weight(random_filament: dict[str, Any]):\n\t    \"\"\"Test adding a spool to the database.\"\"\"\n\t    # Execute\n\t    first_used = \"2023-01-01T00:00:00\"\n\t    last_used = \"2023-01-02T00:00:00\"\n\t    used_weight = 250\n\t    location = \"The Pantry\"\n\t    lot_nr = \"123456789\"\n\t    comment = \"abcdefghåäö\"\n\t    archived = True\n", "    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"first_used\": first_used,\n\t            \"last_used\": last_used,\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"used_weight\": used_weight,\n\t            \"location\": location,\n\t            \"lot_nr\": lot_nr,\n\t            \"comment\": comment,\n", "            \"archived\": archived,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    remaining_weight = random_filament[\"weight\"] - used_weight\n\t    used_length = length_from_weight(\n\t        weight=used_weight,\n\t        density=random_filament[\"density\"],\n\t        diameter=random_filament[\"diameter\"],\n", "    )\n\t    remaining_length = length_from_weight(\n\t        weight=remaining_weight,\n\t        density=random_filament[\"density\"],\n\t        diameter=random_filament[\"diameter\"],\n\t    )\n\t    spool = result.json()\n\t    assert spool == {\n\t        \"id\": spool[\"id\"],\n\t        \"registered\": spool[\"registered\"],\n", "        \"first_used\": first_used,\n\t        \"last_used\": last_used,\n\t        \"filament\": random_filament,\n\t        \"remaining_weight\": pytest.approx(remaining_weight),\n\t        \"used_weight\": pytest.approx(used_weight),\n\t        \"remaining_length\": pytest.approx(remaining_length),\n\t        \"used_length\": pytest.approx(used_length),\n\t        \"location\": location,\n\t        \"lot_nr\": lot_nr,\n\t        \"comment\": comment,\n", "        \"archived\": archived,\n\t    }\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\tdef test_add_spool_required(random_filament: dict[str, Any]):\n\t    \"\"\"Test adding a spool with only the required fields to the database.\"\"\"\n\t    # Execute\n\t    used_weight = 250\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n", "        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"used_weight\": used_weight,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    remaining_weight = random_filament[\"weight\"] - used_weight\n\t    used_length = length_from_weight(\n\t        weight=used_weight,\n", "        density=random_filament[\"density\"],\n\t        diameter=random_filament[\"diameter\"],\n\t    )\n\t    remaining_length = length_from_weight(\n\t        weight=remaining_weight,\n\t        density=random_filament[\"density\"],\n\t        diameter=random_filament[\"diameter\"],\n\t    )\n\t    spool = result.json()\n\t    assert spool == {\n", "        \"id\": spool[\"id\"],\n\t        \"registered\": spool[\"registered\"],\n\t        \"filament\": random_filament,\n\t        \"used_weight\": pytest.approx(used_weight),\n\t        \"remaining_weight\": pytest.approx(remaining_weight),\n\t        \"used_length\": pytest.approx(used_length),\n\t        \"remaining_length\": pytest.approx(remaining_length),\n\t        \"archived\": False,\n\t    }\n\t    # Clean up\n", "    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\tdef test_add_spool_both_used_and_remaining_weight(random_filament: dict[str, Any]):\n\t    \"\"\"Test adding a spool to the database.\"\"\"\n\t    # Execute\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": 750,\n\t            \"used_weight\": 250,\n", "        },\n\t    )\n\t    assert result.status_code == 400  # Cannot set both used and remaining weight\n\tdef test_get_spool(random_filament: dict[str, Any]):\n\t    \"\"\"Test getting a spool from the database.\"\"\"\n\t    # Setup\n\t    first_used = \"2023-01-01T00:00:00\"\n\t    last_used = \"2023-01-02T00:00:00\"\n\t    remaining_weight = 750\n\t    location = \"The Pantry\"\n", "    lot_nr = \"123456789\"\n\t    comment = \"abcdefghåäö\"\n\t    archived = True\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"first_used\": first_used,\n\t            \"last_used\": last_used,\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": remaining_weight,\n", "            \"location\": location,\n\t            \"lot_nr\": lot_nr,\n\t            \"comment\": comment,\n\t            \"archived\": archived,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n\t    result = httpx.get(f\"{URL}/api/v1/spool/{spool['id']}\")\n", "    result.raise_for_status()\n\t    # Verify\n\t    assert result.json() == spool\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\tdef test_get_spool_not_found():\n\t    \"\"\"Test getting a spool that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.get(f\"{URL}/api/v1/spool/123456789\")\n\t    # Verify\n", "    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"spool\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n\tdef test_find_spools(random_filament: dict[str, Any]):  # noqa: PLR0915\n\t    \"\"\"Test finding spools in the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n", "        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": 1000,\n\t            \"location\": \"The Pantry\",\n\t            \"lot_nr\": \"123456789\",\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool_1 = result.json()\n\t    result = httpx.post(\n", "        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": 1000,\n\t            \"location\": \"Living Room\",\n\t            \"lot_nr\": \"987654321\",\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool_2 = result.json()\n", "    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": 1000,\n\t            \"archived\": True,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool_3 = result.json()\n", "    added_spools_by_id = {\n\t        spool_1[\"id\"]: spool_1,\n\t        spool_2[\"id\"]: spool_2,\n\t        spool_3[\"id\"]: spool_3,\n\t    }\n\t    # Execute - find all spools\n\t    result = httpx.get(f\"{URL}/api/v1/spool\")\n\t    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n", "    assert len(spools) == 2\n\t    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t        assert spool[\"archived\"] is False\n\t    # Execute - find all spools, including archived\n\t    result = httpx.get(f\"{URL}/api/v1/spool?allow_archived=true\")\n\t    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n\t    assert len(spools) == 3\n", "    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t    # Execute - find spools by filament name\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n\t        params={\"filament_name\": random_filament[\"name\"]},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n", "    assert len(spools) == 2\n\t    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t    # Execute - find spools by filament id\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n\t        params={\"filament_id\": random_filament[\"id\"]},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n", "    spools = result.json()\n\t    assert len(spools) == 2\n\t    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t    # Execute - find spools by filament material\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n\t        params={\"filament_material\": random_filament[\"material\"]},\n\t    )\n\t    result.raise_for_status()\n", "    # Verify\n\t    spools = result.json()\n\t    assert len(spools) == 2\n\t    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t    # Execute - find spools by filament vendor name\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n\t        params={\"vendor_name\": random_filament[\"vendor\"][\"name\"]},\n\t    )\n", "    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n\t    assert len(spools) == 2\n\t    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t    # Execute - find spools by filament vendor id\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n\t        params={\"vendor_id\": random_filament[\"vendor\"][\"id\"]},\n", "    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n\t    assert len(spools) == 2\n\t    for spool in spools:\n\t        assert spool == added_spools_by_id[spool[\"id\"]]\n\t    # Execute - find spools by location\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n", "        params={\"location\": \"The Pantry\"},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n\t    assert len(spools) == 1\n\t    assert spools[0] == added_spools_by_id[spool_1[\"id\"]]\n\t    # Execute - find spools by lot nr\n\t    result = httpx.get(\n\t        f\"{URL}/api/v1/spool\",\n", "        params={\"lot_nr\": \"123456789\"},\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    spools = result.json()\n\t    assert len(spools) == 1\n\t    assert spools[0] == added_spools_by_id[spool_1[\"id\"]]\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool_1['id']}\").raise_for_status()\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool_2['id']}\").raise_for_status()\n", "    httpx.delete(f\"{URL}/api/v1/spool/{spool_3['id']}\").raise_for_status()\n\tdef test_delete_spool(random_filament: dict[str, Any]):\n\t    \"\"\"Test deleting a spool from the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": 1000,\n\t            \"location\": \"The Pantry\",\n", "            \"lot_nr\": \"123456789\",\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\t    # Verify\n\t    result = httpx.get(f\"{URL}/api/v1/spool/{spool['id']}\")\n\t    assert result.status_code == 404\n", "def test_delete_spool_not_found():\n\t    \"\"\"Test deleting a spool that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.delete(f\"{URL}/api/v1/spool/123456789\")\n\t    # Verify\n\t    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"spool\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n", "def test_update_spool(random_filament: dict[str, Any]):\n\t    \"\"\"Test updating a spool in the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": 1000,\n\t            \"location\": \"The Pantry\",\n\t            \"lot_nr\": \"123456789\",\n", "        },\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n\t    first_used = \"2023-01-01T12:00:00+02:00\"\n\t    last_used = \"2023-01-02T12:00:00+02:00\"\n\t    remaining_weight = 750\n\t    location = \"Living Room\"\n\t    lot_nr = \"987654321\"\n", "    comment = \"abcdefghåäö\"\n\t    archived = True\n\t    result = httpx.patch(\n\t        f\"{URL}/api/v1/spool/{spool['id']}\",\n\t        json={\n\t            \"first_used\": first_used,\n\t            \"last_used\": last_used,\n\t            \"remaining_weight\": remaining_weight,\n\t            \"location\": location,\n\t            \"lot_nr\": lot_nr,\n", "            \"comment\": comment,\n\t            \"archived\": archived,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    used_weight = random_filament[\"weight\"] - remaining_weight\n\t    used_length = length_from_weight(\n\t        weight=used_weight,\n\t        density=random_filament[\"density\"],\n", "        diameter=random_filament[\"diameter\"],\n\t    )\n\t    remaining_length = length_from_weight(\n\t        weight=remaining_weight,\n\t        density=random_filament[\"density\"],\n\t        diameter=random_filament[\"diameter\"],\n\t    )\n\t    spool = result.json()\n\t    assert spool[\"first_used\"] == \"2023-01-01T10:00:00\"\n\t    assert spool[\"last_used\"] == \"2023-01-02T10:00:00\"\n", "    assert spool[\"remaining_weight\"] == pytest.approx(remaining_weight)\n\t    assert spool[\"used_weight\"] == pytest.approx(used_weight)\n\t    assert spool[\"remaining_length\"] == pytest.approx(remaining_length)\n\t    assert spool[\"used_length\"] == pytest.approx(used_length)\n\t    assert spool[\"location\"] == location\n\t    assert spool[\"lot_nr\"] == lot_nr\n\t    assert spool[\"comment\"] == comment\n\t    assert spool[\"archived\"] == archived\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n", "def test_update_spool_both_used_and_remaining_weight(random_filament: dict[str, Any]):\n\t    \"\"\"Test updating a spool in the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\"filament_id\": random_filament[\"id\"]},\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n", "    result = httpx.patch(\n\t        f\"{URL}/api/v1/spool/{spool['id']}\",\n\t        json={\n\t            \"remaining_weight\": 750,\n\t            \"used_weight\": 250,\n\t        },\n\t    )\n\t    assert result.status_code == 400  # Cannot update both used and remaining weight\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n", "def test_update_spool_not_found(random_filament: dict[str, Any]):\n\t    \"\"\"Test updating a spool that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.patch(\n\t        f\"{URL}/api/v1/spool/123456789\",\n\t        json={\"filament_id\": random_filament[\"id\"]},\n\t    )\n\t    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"spool\" in message\n", "    assert \"id\" in message\n\t    assert \"123456789\" in message\n\t@pytest.mark.parametrize(\"use_weight\", [0, 0.05, -0.05, 1500])  # 1500 is big enough to use all filament\n\tdef test_use_spool_weight(random_filament: dict[str, Any], use_weight: float):\n\t    \"\"\"Test using a spool in the database.\"\"\"\n\t    # Setup\n\t    filament_net_weight = random_filament[\"weight\"]\n\t    start_weight = 1000\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n", "        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": start_weight,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n\t    result = httpx.put(\n\t        f\"{URL}/api/v1/spool/{spool['id']}/use\",\n", "        json={\n\t            \"use_weight\": use_weight,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    spool = result.json()\n\t    # remaining_weight should be clamped so it's never negative, but used_weight should not be clamped to the net weight\n\t    assert spool[\"used_weight\"] == pytest.approx(max(use_weight, 0))\n\t    assert spool[\"remaining_weight\"] == pytest.approx(min(max(start_weight - use_weight, 0), filament_net_weight))\n", "    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\t@pytest.mark.parametrize(\"use_length\", [0, 10, -10, 500e3])  # 500e3 is big enough to use all the filament\n\tdef test_use_spool_length(random_filament: dict[str, Any], use_length: float):\n\t    \"\"\"Test using a spool in the database.\"\"\"\n\t    # Setup\n\t    filament_net_weight = random_filament[\"weight\"]\n\t    start_weight = 1000\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n", "        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": start_weight,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n\t    result = httpx.put(\n\t        f\"{URL}/api/v1/spool/{spool['id']}/use\",\n", "        json={\n\t            \"use_length\": use_length,\n\t        },\n\t    )\n\t    result.raise_for_status()\n\t    # Verify\n\t    spool = result.json()\n\t    use_weight = (\n\t        random_filament[\"density\"] * (use_length * 1e-1) * math.pi * ((random_filament[\"diameter\"] * 1e-1 / 2) ** 2)\n\t    )\n", "    # remaining_weight should be clamped so it's never negative, but used_weight should not be clamped to the net weight\n\t    assert spool[\"used_weight\"] == pytest.approx(max(use_weight, 0))\n\t    assert spool[\"remaining_weight\"] == pytest.approx(min(max(start_weight - use_weight, 0), filament_net_weight))\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\tdef test_use_spool_weight_and_length(random_filament: dict[str, Any]):\n\t    \"\"\"Test using a spool in the database.\"\"\"\n\t    # Setup\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/spool\",\n", "        json={\"filament_id\": random_filament[\"id\"]},\n\t    )\n\t    result.raise_for_status()\n\t    spool = result.json()\n\t    # Execute\n\t    result = httpx.put(\n\t        f\"{URL}/api/v1/spool/{spool['id']}/use\",\n\t        json={\n\t            \"use_weight\": 0.05,\n\t            \"use_length\": 10,\n", "        },\n\t    )\n\t    assert result.status_code == 400  # Cannot use both weight and length\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()\n\tdef test_use_spool_not_found():\n\t    \"\"\"Test using a spool that does not exist.\"\"\"\n\t    # Execute\n\t    result = httpx.put(\n\t        f\"{URL}/api/v1/spool/123456789/use\",\n", "        json={\"use_weight\": 0.05},\n\t    )\n\t    assert result.status_code == 404\n\t    message = result.json()[\"message\"].lower()\n\t    assert \"spool\" in message\n\t    assert \"id\" in message\n\t    assert \"123456789\" in message\n\t@pytest.mark.asyncio()\n\tasync def test_use_spool_concurrent(random_filament: dict[str, Any]):\n\t    \"\"\"Test using a spool with many concurrent requests.\"\"\"\n", "    # Setup\n\t    start_weight = 1000\n\t    result = httpx.post(  # noqa: ASYNC100\n\t        f\"{URL}/api/v1/spool\",\n\t        json={\n\t            \"filament_id\": random_filament[\"id\"],\n\t            \"remaining_weight\": start_weight,\n\t        },\n\t    )\n\t    result.raise_for_status()\n", "    spool = result.json()\n\t    # Execute\n\t    requests = 100\n\t    used_weight = 0.5\n\t    async with httpx.AsyncClient() as client:\n\t        await asyncio.gather(\n\t            *(\n\t                client.put(\n\t                    f\"{URL}/api/v1/spool/{spool['id']}/use\",\n\t                    json={\n", "                        \"use_weight\": used_weight,\n\t                    },\n\t                    timeout=60,\n\t                )\n\t                for _ in range(requests)\n\t            ),\n\t        )\n\t    # Verify\n\t    result = httpx.get(f\"{URL}/api/v1/spool/{spool['id']}\")  # noqa: ASYNC100\n\t    result.raise_for_status()\n", "    spool = result.json()\n\t    assert spool[\"remaining_weight\"] == pytest.approx(start_weight - (used_weight * requests))\n\t    # Clean up\n\t    httpx.delete(f\"{URL}/api/v1/spool/{spool['id']}\").raise_for_status()  # noqa: ASYNC100\n"]}
{"filename": "tests_integration/tests/test_backup.py", "chunked_list": ["\"\"\"Integration tests for the Vendor API endpoint.\"\"\"\n\timport httpx\n\tfrom .conftest import DbType, get_db_type\n\tURL = \"http://spoolman:8000\"\n\tdef test_backup():\n\t    \"\"\"Test triggering an automatic database backup.\"\"\"\n\t    if get_db_type() != DbType.SQLITE:\n\t        return\n\t    # Trigger backup\n\t    result = httpx.post(f\"{URL}/api/v1/backup\")\n", "    result.raise_for_status()\n"]}
{"filename": "tests_integration/tests/conftest.py", "chunked_list": ["\"\"\"Test fixtures for integration tests.\"\"\"\n\timport math\n\timport os\n\timport time\n\tfrom enum import StrEnum\n\tfrom typing import Any\n\timport httpx\n\timport pytest\n\tTIMEOUT = 10\n\tURL = \"http://spoolman:8000\"\n", "class DbType(StrEnum):\n\t    \"\"\"Enum for database types.\"\"\"\n\t    SQLITE = \"sqlite\"\n\t    POSTGRES = \"postgres\"\n\t    MYSQL = \"mysql\"\n\t    COCKROACHDB = \"cockroachdb\"\n\tdef get_db_type() -> DbType:\n\t    \"\"\"Return the database type from environment variables.\"\"\"\n\t    env_db_type = os.environ.get(\"DB_TYPE\")\n\t    if env_db_type is None:\n", "        raise RuntimeError(\"DB_TYPE environment variable not set\")\n\t    try:\n\t        db_type = DbType(env_db_type)\n\t    except ValueError as e:\n\t        raise RuntimeError(f\"Unknown database type: {env_db_type}\") from e\n\t    return db_type\n\t@pytest.fixture(scope=\"session\", autouse=True)\n\tdef _wait_for_server():  # noqa: ANN202\n\t    \"\"\"Wait for the server to start up.\"\"\"\n\t    start_time = time.time()\n", "    while True:\n\t        try:\n\t            response = httpx.get(\"http://spoolman:8000\", timeout=1)\n\t            response.raise_for_status()\n\t        except httpx.HTTPError:  # noqa: PERF203\n\t            if time.time() - start_time > TIMEOUT:\n\t                raise\n\t        else:\n\t            break\n\t@pytest.fixture()\n", "def random_vendor():\n\t    \"\"\"Return a random vendor.\"\"\"\n\t    # Add vendor\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/vendor\",\n\t        json={\"name\": \"John\"},\n\t    )\n\t    result.raise_for_status()\n\t    vendor = result.json()\n\t    yield vendor\n", "    # Delete vendor\n\t    httpx.delete(f\"{URL}/api/v1/vendor/{vendor['id']}\").raise_for_status()\n\t@pytest.fixture()\n\tdef random_filament(random_vendor: dict[str, Any]):\n\t    \"\"\"Return a random filament.\"\"\"\n\t    # Add filament\n\t    result = httpx.post(\n\t        f\"{URL}/api/v1/filament\",\n\t        json={\n\t            \"name\": \"Filament X\",\n", "            \"vendor_id\": random_vendor[\"id\"],\n\t            \"material\": \"PLA\",\n\t            \"price\": 100,\n\t            \"density\": 1.25,\n\t            \"diameter\": 1.75,\n\t            \"weight\": 1000,\n\t            \"spool_weight\": 250,\n\t            \"article_number\": \"123456789\",\n\t            \"comment\": \"abcdefghåäö\",\n\t        },\n", "    )\n\t    result.raise_for_status()\n\t    filament = result.json()\n\t    yield filament\n\t    # Delete filament\n\t    httpx.delete(f\"{URL}/api/v1/filament/{filament['id']}\").raise_for_status()\n\tdef length_from_weight(*, weight: float, diameter: float, density: float) -> float:\n\t    \"\"\"Calculate the length of a piece of filament.\n\t    Args:\n\t        weight (float): Filament weight in g\n", "        diameter (float): Filament diameter in mm\n\t        density (float): Density of filament material in g/cm3\n\t    Returns:\n\t        float: Length in mm\n\t    \"\"\"\n\t    volume_cm3 = weight / density\n\t    volume_mm3 = volume_cm3 * 1000\n\t    return volume_mm3 / (math.pi * (diameter / 2) ** 2)\n"]}
