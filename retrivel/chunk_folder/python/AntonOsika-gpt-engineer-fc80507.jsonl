{"filename": "scripts/clean_benchmarks.py", "chunked_list": ["# list all folders in benchmark folder\n\t# for each folder, run the benchmark\n\timport os\n\timport shutil\n\tfrom pathlib import Path\n\tfrom typer import run\n\tdef main():\n\t    benchmarks = Path(\"benchmark\")\n\t    for benchmark in benchmarks.iterdir():\n\t        if benchmark.is_dir():\n", "            print(f\"Cleaning {benchmark}\")\n\t            for path in benchmark.iterdir():\n\t                if path.name in [\"prompt\", \"main_prompt\"]:\n\t                    continue\n\t                # Get filename of Path object\n\t                if path.is_dir():\n\t                    # delete the entire directory\n\t                    shutil.rmtree(path)\n\t                else:\n\t                    # delete the file\n", "                    os.remove(path)\n\tif __name__ == \"__main__\":\n\t    run(main)\n"]}
{"filename": "scripts/print_chat.py", "chunked_list": ["import json\n\timport typer\n\tfrom termcolor import colored\n\tapp = typer.Typer()\n\tdef pretty_print_conversation(messages):\n\t    role_to_color = {\n\t        \"system\": \"red\",\n\t        \"user\": \"green\",\n\t        \"assistant\": \"blue\",\n\t        \"function\": \"magenta\",\n", "    }\n\t    formatted_messages = []\n\t    for message in messages:\n\t        if message[\"role\"] == \"function\":\n\t            formatted_messages.append(\n\t                f\"function ({message['name']}): {message['content']}\\n\"\n\t            )\n\t        else:\n\t            assistant_content = (\n\t                message[\"function_call\"]\n", "                if message.get(\"function_call\")\n\t                else message[\"content\"]\n\t            )\n\t            role_to_message = {\n\t                \"system\": f\"system: {message['content']}\\n\",\n\t                \"user\": f\"user: {message['content']}\\n\",\n\t                \"assistant\": f\"assistant: {assistant_content}\\n\",\n\t            }\n\t            formatted_messages.append(role_to_message[message[\"role\"]])\n\t    for formatted_message in formatted_messages:\n", "        role = messages[formatted_messages.index(formatted_message)][\"role\"]\n\t        color = role_to_color[role]\n\t        print(colored(formatted_message, color))\n\t@app.command()\n\tdef main(\n\t    messages_path: str,\n\t):\n\t    with open(messages_path) as f:\n\t        messages = json.load(f)\n\t    pretty_print_conversation(messages)\n", "if __name__ == \"__main__\":\n\t    app()\n"]}
{"filename": "scripts/rerun_edited_message_logs.py", "chunked_list": ["import json\n\timport pathlib\n\tfrom typing import Union\n\timport typer\n\tfrom gpt_engineer.ai import AI\n\tfrom gpt_engineer.chat_to_files import to_files\n\tapp = typer.Typer()\n\t@app.command()\n\tdef main(\n\t    messages_path: str,\n", "    out_path: Union[str, None] = None,\n\t    model: str = \"gpt-4\",\n\t    temperature: float = 0.1,\n\t):\n\t    ai = AI(\n\t        model_name=model,\n\t        temperature=temperature,\n\t    )\n\t    with open(messages_path) as f:\n\t        messages = json.load(f)\n", "    messages = ai.next(messages, step_name=\"rerun\")\n\t    if out_path:\n\t        to_files(messages[-1][\"content\"], out_path)\n\t        with open(pathlib.Path(out_path) / \"all_output.txt\", \"w\") as f:\n\t            json.dump(messages[-1][\"content\"], f)\n\tif __name__ == \"__main__\":\n\t    app()\n"]}
{"filename": "scripts/benchmark.py", "chunked_list": ["# list all folders in benchmark folder\n\t# for each folder, run the benchmark\n\timport contextlib\n\timport json\n\timport os\n\timport subprocess\n\tfrom datetime import datetime\n\tfrom itertools import islice\n\tfrom pathlib import Path\n\tfrom typing import Iterable, Union\n", "from tabulate import tabulate\n\tfrom typer import run\n\tdef main(\n\t    n_benchmarks: Union[int, None] = None,\n\t):\n\t    path = Path(\"benchmark\")\n\t    folders: Iterable[Path] = path.iterdir()\n\t    if n_benchmarks:\n\t        folders = islice(folders, n_benchmarks)\n\t    benchmarks = []\n", "    for bench_folder in folders:\n\t        if os.path.isdir(bench_folder):\n\t            print(f\"Running benchmark for {bench_folder}\")\n\t            log_path = bench_folder / \"log.txt\"\n\t            log_file = open(log_path, \"w\")\n\t            process = subprocess.Popen(\n\t                [\n\t                    \"python\",\n\t                    \"-u\",  # Unbuffered output\n\t                    \"-m\",\n", "                    \"gpt_engineer.main\",\n\t                    bench_folder,\n\t                    \"--steps\",\n\t                    \"benchmark\",\n\t                ],\n\t                stdout=log_file,\n\t                stderr=log_file,\n\t                bufsize=0,\n\t            )\n\t            benchmarks.append((bench_folder, process, log_file))\n", "            print(\"You can stream the log file by running:\")\n\t            print(f\"tail -f {log_path}\")\n\t            print()\n\t    for bench_folder, process, file in benchmarks:\n\t        process.wait()\n\t        file.close()\n\t        print(\"process\", bench_folder.name, \"finished with code\", process.returncode)\n\t        print(\"Running it. Original benchmark prompt:\")\n\t        print()\n\t        with open(bench_folder / \"prompt\") as f:\n", "            print(f.read())\n\t        print()\n\t        with contextlib.suppress(KeyboardInterrupt):\n\t            subprocess.run(\n\t                [\n\t                    \"python\",\n\t                    \"-m\",\n\t                    \"gpt_engineer.main\",\n\t                    bench_folder,\n\t                    \"--steps\",\n", "                    \"evaluate\",\n\t                ],\n\t            )\n\t    generate_report(benchmarks, path)\n\tdef generate_report(benchmarks, benchmark_path):\n\t    headers = [\"Benchmark\", \"Ran\", \"Works\", \"Perfect\", \"Notes\"]\n\t    rows = []\n\t    for bench_folder, _, _ in benchmarks:\n\t        memory = bench_folder / \"memory\"\n\t        with open(memory / \"review\") as f:\n", "            review = json.loads(f.read())\n\t            rows.append(\n\t                [\n\t                    bench_folder.name,\n\t                    to_emoji(review.get(\"ran\", None)),\n\t                    to_emoji(review.get(\"works\", None)),\n\t                    to_emoji(review.get(\"perfect\", None)),\n\t                    review.get(\"comments\", None),\n\t                ]\n\t            )\n", "    table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n\t    print(\"\\nBenchmark report:\\n\")\n\t    print(table)\n\t    print()\n\t    append_to_results = ask_yes_no(\"Append report to the results file?\")\n\t    if append_to_results:\n\t        results_path = benchmark_path / \"RESULTS.md\"\n\t        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n\t        insert_markdown_section(results_path, current_date, table, 2)\n\tdef to_emoji(value: bool) -> str:\n", "    return \"\\U00002705\" if value else \"\\U0000274C\"\n\tdef insert_markdown_section(file_path, section_title, section_text, level):\n\t    with open(file_path, \"r\") as file:\n\t        lines = file.readlines()\n\t    header_prefix = \"#\" * level\n\t    new_section = f\"{header_prefix} {section_title}\\n\\n{section_text}\\n\\n\"\n\t    # Find the first section with the specified level\n\t    line_number = -1\n\t    for i, line in enumerate(lines):\n\t        if line.startswith(header_prefix):\n", "            line_number = i\n\t            break\n\t    if line_number != -1:\n\t        lines.insert(line_number, new_section)\n\t    else:\n\t        print(\n\t            f\"Markdown file was of unexpected format. No section of level {level} found. \"\n\t            \"Did not write results.\"\n\t        )\n\t        return\n", "    # Write the file\n\t    with open(file_path, \"w\") as file:\n\t        file.writelines(lines)\n\tdef ask_yes_no(question: str) -> bool:\n\t    while True:\n\t        response = input(question + \" (y/n): \").lower().strip()\n\t        if response == \"y\":\n\t            return True\n\t        elif response == \"n\":\n\t            return False\n", "        else:\n\t            print(\"Please enter either 'y' or 'n'.\")\n\tif __name__ == \"__main__\":\n\t    run(main)\n"]}
{"filename": "tests/test_ai.py", "chunked_list": ["import pytest\n\tfrom gpt_engineer.ai import AI\n\t@pytest.mark.xfail(reason=\"Constructor assumes API access\")\n\tdef test_ai():\n\t    AI()\n\t    # TODO Assert that methods behave and not only constructor.\n"]}
{"filename": "tests/test_db.py", "chunked_list": ["import pytest\n\tfrom gpt_engineer.db import DB, DBs\n\tdef test_DB_operations(tmp_path):\n\t    # Test initialization\n\t    db = DB(tmp_path)\n\t    # Test __setitem__\n\t    db[\"test_key\"] = \"test_value\"\n\t    assert (tmp_path / \"test_key\").is_file()\n\t    # Test __getitem__\n\t    val = db[\"test_key\"]\n", "    assert val == \"test_value\"\n\tdef test_DBs_initialization(tmp_path):\n\t    dir_names = [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n\t    directories = [tmp_path / name for name in dir_names]\n\t    # Create DB objects\n\t    dbs = [DB(dir) for dir in directories]\n\t    # Create DB instance\n\t    dbs_instance = DBs(*dbs)\n\t    assert isinstance(dbs_instance.memory, DB)\n\t    assert isinstance(dbs_instance.logs, DB)\n", "    assert isinstance(dbs_instance.preprompts, DB)\n\t    assert isinstance(dbs_instance.input, DB)\n\t    assert isinstance(dbs_instance.workspace, DB)\n\t    assert isinstance(dbs_instance.archive, DB)\n\tdef test_invalid_path():\n\t    with pytest.raises((PermissionError, OSError)):\n\t        # Test with a path that will raise a permission error\n\t        DB(\"/root/test\")\n\tdef test_large_files(tmp_path):\n\t    db = DB(tmp_path)\n", "    large_content = \"a\" * (10**6)  # 1MB of data\n\t    # Test write large files\n\t    db[\"large_file\"] = large_content\n\t    # Test read large files\n\t    assert db[\"large_file\"] == large_content\n\tdef test_concurrent_access(tmp_path):\n\t    import threading\n\t    db = DB(tmp_path)\n\t    num_threads = 10\n\t    num_writes = 1000\n", "    def write_to_db(thread_id):\n\t        for i in range(num_writes):\n\t            key = f\"thread{thread_id}_write{i}\"\n\t            db[key] = str(i)\n\t    threads = []\n\t    for thread_id in range(num_threads):\n\t        t = threading.Thread(target=write_to_db, args=(thread_id,))\n\t        t.start()\n\t        threads.append(t)\n\t    for t in threads:\n", "        t.join()\n\t    # Verify that all expected data was written\n\t    for thread_id in range(num_threads):\n\t        for i in range(num_writes):\n\t            key = f\"thread{thread_id}_write{i}\"\n\t            assert key in db  # using __contains__ now\n\t            assert db[key] == str(i)\n\tdef test_error_messages(tmp_path):\n\t    db = DB(tmp_path)\n\t    # Test error on getting non-existent key\n", "    with pytest.raises(KeyError):\n\t        db[\"non_existent\"]\n\t    with pytest.raises(AssertionError) as e:\n\t        db[\"key\"] = [\"Invalid\", \"value\"]\n\t    assert str(e.value) == \"val must be str\"\n\tdef test_DBs_dataclass_attributes(tmp_path):\n\t    dir_names = [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n\t    directories = [tmp_path / name for name in dir_names]\n\t    # Create DB objects\n\t    dbs = [DB(dir) for dir in directories]\n", "    # Create DBs instance\n\t    dbs_instance = DBs(*dbs)\n\t    assert dbs_instance.memory == dbs[0]\n\t    assert dbs_instance.logs == dbs[1]\n\t    assert dbs_instance.preprompts == dbs[2]\n\t    assert dbs_instance.input == dbs[3]\n\t    assert dbs_instance.workspace == dbs[4]\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_chat_to_files.py", "chunked_list": ["import textwrap\n\tfrom gpt_engineer.chat_to_files import to_files\n\tdef test_to_files():\n\t    chat = textwrap.dedent(\n\t        \"\"\"\n\t    This is a sample program.\n\t    file1.py\n\t    ```python\n\t    print(\"Hello, World!\")\n\t    ```\n", "    file2.py\n\t    ```python\n\t    def add(a, b):\n\t        return a + b\n\t    ```\n\t    \"\"\"\n\t    )\n\t    workspace = {}\n\t    to_files(chat, workspace)\n\t    assert workspace[\"all_output.txt\"] == chat\n", "    expected_files = {\n\t        \"file1.py\": 'print(\"Hello, World!\")\\n',\n\t        \"file2.py\": \"def add(a, b):\\n    return a + b\\n\",\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\nfile1.py\\n\",\n\t    }\n\t    for file_name, file_content in expected_files.items():\n\t        assert workspace[file_name] == file_content\n\tdef test_to_files_with_square_brackets():\n\t    chat = textwrap.dedent(\n\t        \"\"\"\n", "    This is a sample program.\n\t    [file1.py]\n\t    ```python\n\t    print(\"Hello, World!\")\n\t    ```\n\t    [file2.py]\n\t    ```python\n\t    def add(a, b):\n\t        return a + b\n\t    ```\n", "    \"\"\"\n\t    )\n\t    workspace = {}\n\t    to_files(chat, workspace)\n\t    assert workspace[\"all_output.txt\"] == chat\n\t    expected_files = {\n\t        \"file1.py\": 'print(\"Hello, World!\")\\n',\n\t        \"file2.py\": \"def add(a, b):\\n    return a + b\\n\",\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\n[file1.py]\\n\",\n\t    }\n", "    for file_name, file_content in expected_files.items():\n\t        assert workspace[file_name] == file_content\n\tdef test_files_with_brackets_in_name():\n\t    chat = textwrap.dedent(\n\t        \"\"\"\n\t    This is a sample program.\n\t    [id].jsx\n\t    ```javascript\n\t    console.log(\"Hello, World!\")\n\t    ```\n", "    \"\"\"\n\t    )\n\t    workspace = {}\n\t    to_files(chat, workspace)\n\t    assert workspace[\"all_output.txt\"] == chat\n\t    expected_files = {\n\t        \"[id].jsx\": 'console.log(\"Hello, World!\")\\n',\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\n[id].jsx\\n\",\n\t    }\n\t    for file_name, file_content in expected_files.items():\n", "        assert workspace[file_name] == file_content\n\tdef test_files_with_file_colon():\n\t    chat = textwrap.dedent(\n\t        \"\"\"\n\t    This is a sample program.\n\t    [FILE: file1.py]\n\t    ```python\n\t    print(\"Hello, World!\")\n\t    ```\n\t    \"\"\"\n", "    )\n\t    workspace = {}\n\t    to_files(chat, workspace)\n\t    assert workspace[\"all_output.txt\"] == chat\n\t    expected_files = {\n\t        \"file1.py\": 'print(\"Hello, World!\")\\n',\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\n[FILE: file1.py]\\n\",\n\t    }\n\t    for file_name, file_content in expected_files.items():\n\t        assert workspace[file_name] == file_content\n", "def test_files_with_back_tick():\n\t    chat = textwrap.dedent(\n\t        \"\"\"\n\t    This is a sample program.\n\t    `file1.py`\n\t    ```python\n\t    print(\"Hello, World!\")\n\t    ```\n\t    \"\"\"\n\t    )\n", "    workspace = {}\n\t    to_files(chat, workspace)\n\t    assert workspace[\"all_output.txt\"] == chat\n\t    expected_files = {\n\t        \"file1.py\": 'print(\"Hello, World!\")\\n',\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\n`file1.py`\\n\",\n\t    }\n\t    for file_name, file_content in expected_files.items():\n\t        assert workspace[file_name] == file_content\n\tdef test_files_with_newline_between():\n", "    chat = textwrap.dedent(\n\t        \"\"\"\n\t    This is a sample program.\n\t    file1.py\n\t    ```python\n\t    print(\"Hello, World!\")\n\t    ```\n\t    \"\"\"\n\t    )\n\t    workspace = {}\n", "    to_files(chat, workspace)\n\t    assert workspace[\"all_output.txt\"] == chat\n\t    expected_files = {\n\t        \"file1.py\": 'print(\"Hello, World!\")\\n',\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\nfile1.py\\n\\n\",\n\t    }\n\t    for file_name, file_content in expected_files.items():\n\t        assert workspace[file_name] == file_content\n\tdef test_files_with_newline_between_header():\n\t    chat = textwrap.dedent(\n", "        \"\"\"\n\t    This is a sample program.\n\t    ## file1.py\n\t    ```python\n\t    print(\"Hello, World!\")\n\t    ```\n\t    \"\"\"\n\t    )\n\t    workspace = {}\n\t    to_files(chat, workspace)\n", "    assert workspace[\"all_output.txt\"] == chat\n\t    expected_files = {\n\t        \"file1.py\": 'print(\"Hello, World!\")\\n',\n\t        \"README.md\": \"\\nThis is a sample program.\\n\\n## file1.py\\n\\n\",\n\t    }\n\t    for file_name, file_content in expected_files.items():\n\t        assert workspace[file_name] == file_content\n"]}
{"filename": "tests/test_collect.py", "chunked_list": ["import json\n\timport os\n\tfrom unittest.mock import MagicMock\n\timport pytest\n\timport rudderstack.analytics as rudder_analytics\n\tfrom gpt_engineer.collect import collect_learnings, steps_file_hash\n\tfrom gpt_engineer.db import DB, DBs\n\tfrom gpt_engineer.learning import extract_learning\n\tfrom gpt_engineer.steps import gen_code_after_unit_tests\n\tdef test_collect_learnings(monkeypatch):\n", "    monkeypatch.setattr(os, \"environ\", {\"COLLECT_LEARNINGS_OPT_IN\": \"true\"})\n\t    monkeypatch.setattr(rudder_analytics, \"track\", MagicMock())\n\t    model = \"test_model\"\n\t    temperature = 0.5\n\t    steps = [gen_code_after_unit_tests]\n\t    dbs = DBs(DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"))\n\t    dbs.input = {\n\t        \"prompt\": \"test prompt\\n with newlines\",\n\t        \"feedback\": \"test feedback\",\n\t    }\n", "    code = \"this is output\\n\\nit contains code\"\n\t    dbs.logs = {\n\t        gen_code_after_unit_tests.__name__: json.dumps(\n\t            [{\"role\": \"system\", \"content\": code}]\n\t        )\n\t    }\n\t    dbs.workspace = {\"all_output.txt\": \"test workspace\\n\" + code}\n\t    collect_learnings(model, temperature, steps, dbs)\n\t    learnings = extract_learning(\n\t        model, temperature, steps, dbs, steps_file_hash=steps_file_hash()\n", "    )\n\t    assert rudder_analytics.track.call_count == 1\n\t    assert rudder_analytics.track.call_args[1][\"event\"] == \"learning\"\n\t    a = {\n\t        k: v\n\t        for k, v in rudder_analytics.track.call_args[1][\"properties\"].items()\n\t        if k != \"timestamp\"\n\t    }\n\t    b = {k: v for k, v in learnings.to_dict().items() if k != \"timestamp\"}\n\t    assert a == b\n", "    assert json.dumps(code) in learnings.logs\n\t    assert code in learnings.workspace\n\tif __name__ == \"__main__\":\n\t    pytest.main([\"-v\"])\n"]}
{"filename": "tests/steps/__init__.py", "chunked_list": []}
{"filename": "tests/steps/test_archive.py", "chunked_list": ["import datetime\n\timport os\n\tfrom unittest.mock import MagicMock\n\tfrom gpt_engineer.db import DB, DBs, archive\n\tdef freeze_at(monkeypatch, time):\n\t    datetime_mock = MagicMock(wraps=datetime.datetime)\n\t    datetime_mock.now.return_value = time\n\t    monkeypatch.setattr(datetime, \"datetime\", datetime_mock)\n\tdef setup_dbs(tmp_path, dir_names):\n\t    directories = [tmp_path / name for name in dir_names]\n", "    # Create DB objects\n\t    dbs = [DB(dir) for dir in directories]\n\t    # Create DBs instance\n\t    return DBs(*dbs)\n\tdef test_archive(tmp_path, monkeypatch):\n\t    dbs = setup_dbs(\n\t        tmp_path, [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n\t    )\n\t    freeze_at(monkeypatch, datetime.datetime(2020, 12, 25, 17, 5, 55))\n\t    archive(dbs)\n", "    assert not os.path.exists(tmp_path / \"memory\")\n\t    assert not os.path.exists(tmp_path / \"workspace\")\n\t    assert os.path.isdir(tmp_path / \"archive\" / \"20201225_170555\")\n\t    dbs = setup_dbs(\n\t        tmp_path, [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n\t    )\n\t    freeze_at(monkeypatch, datetime.datetime(2022, 8, 14, 8, 5, 12))\n\t    archive(dbs)\n\t    assert not os.path.exists(tmp_path / \"memory\")\n\t    assert not os.path.exists(tmp_path / \"workspace\")\n", "    assert os.path.isdir(tmp_path / \"archive\" / \"20201225_170555\")\n\t    assert os.path.isdir(tmp_path / \"archive\" / \"20220814_080512\")\n"]}
{"filename": "evals/evals_existing_code.py", "chunked_list": ["import os\n\timport subprocess\n\tfrom datetime import datetime\n\tfrom pathlib import Path\n\timport yaml\n\tfrom eval_tools import check_evaluation_component\n\tfrom tabulate import tabulate\n\tfrom gpt_engineer.chat_to_files import parse_chat\n\tfrom gpt_engineer.db import DB\n\tEVAL_LIST_NAME = \"evaluations\"  # the top level list in the YAML file\n", "def single_evaluate(eval_ob: dict) -> list[bool]:\n\t    \"\"\"Evaluates a single prompt.\"\"\"\n\t    print(f\"running evaluation: {eval_ob['name']}\")\n\t    # Step 1. Setup known project\n\t    # load the known files into the project\n\t    # the files can be anywhere in the projects folder\n\t    workspace = DB(eval_ob[\"project_root\"])\n\t    file_list_string = \"\"\n\t    code_base_abs = Path(os.getcwd()) / eval_ob[\"project_root\"]\n\t    files = parse_chat(open(eval_ob[\"code_blob\"]).read())\n", "    for file_name, file_content in files:\n\t        absolute_path = code_base_abs / file_name\n\t        print(\"creating: \", absolute_path)\n\t        workspace[absolute_path] = file_content\n\t        file_list_string += str(absolute_path) + \"\\n\"\n\t    # create file_list.txt (should be full paths)\n\t    workspace[\"file_list.txt\"] = file_list_string\n\t    # create the prompt\n\t    workspace[\"prompt\"] = eval_ob[\"improve_code_prompt\"]\n\t    # Step 2.  run the project in improve code mode,\n", "    # make sure the flag -sf is set to skip feedback\n\t    print(f\"Modifying code for {eval_ob['project_root']}\")\n\t    log_path = code_base_abs / \"log.txt\"\n\t    log_file = open(log_path, \"w\")\n\t    process = subprocess.Popen(\n\t        [\n\t            \"python\",\n\t            \"-u\",  # Unbuffered output\n\t            \"-m\",\n\t            \"gpt_engineer.main\",\n", "            eval_ob[\"project_root\"],\n\t            \"--steps\",\n\t            \"eval_improve_code\",\n\t        ],\n\t        stdout=log_file,\n\t        stderr=log_file,\n\t        bufsize=0,\n\t    )\n\t    print(f\"waiting for {eval_ob['name']} to finish.\")\n\t    process.wait()  # we want to wait until it finishes.\n", "    # Step 3. Run test of modified code, tests\n\t    print(\"running tests on modified code\")\n\t    evaluation_results = []\n\t    for test_case in eval_ob[\"expected_results\"]:\n\t        print(f\"checking: {test_case['type']}\")\n\t        test_case[\"project_root\"] = Path(eval_ob[\"project_root\"])\n\t        evaluation_results.append(check_evaluation_component(test_case))\n\t    return evaluation_results\n\tdef to_emoji(value: bool) -> str:\n\t    return \"\\U00002705\" if value else \"\\U0000274C\"\n", "def generate_report(evals: list[dict], res: list[list[bool]]) -> None:\n\t    # High level shows if all the expected_results passed\n\t    # Detailed shows all the test cases and a pass/fail for each\n\t    output_lines = []\n\t    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n\t    output_lines.append(f\"## {current_date}\\n\\n\")\n\t    # Create a summary table\n\t    headers = [\"Project\", \"Evaluation\", \"All Tests Pass\"]\n\t    rows = []\n\t    for i, eval_ob in enumerate(evals):\n", "        rows.append(\n\t            [eval_ob[\"project_root\"], eval_ob[\"name\"], to_emoji(all(res[i]))]\n\t        )  # logical AND of all tests\n\t    table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n\t    title = \"Existing Code Evaluation Summary:\"\n\t    print(f\"\\n{title}\\n\")\n\t    print(table)\n\t    print()\n\t    output_lines.append(f\"### {title}\\n\\n{table}\\n\\n\")\n\t    # Create a detailed table\n", "    headers = [\"Project\", \"Evaluation\", \"Test\", \"Pass\"]\n\t    rows = []\n\t    for i, eval_ob in enumerate(evals):\n\t        for j, test in enumerate(eval_ob[\"expected_results\"]):\n\t            rows.append(\n\t                [\n\t                    eval_ob[\"project_root\"],\n\t                    eval_ob[\"name\"],\n\t                    eval_ob[\"expected_results\"][j][\"type\"],\n\t                    to_emoji(res[i][j]),\n", "                ]\n\t            )\n\t    detail_table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n\t    title = \"Detailed Test Results:\"\n\t    print(f\"\\n{title} \\n\")\n\t    print(detail_table)\n\t    print()\n\t    output_lines.append(f\"### {title}\\n\\n{detail_table}\\n\\n\")\n\t    with open(\"evals/IMPROVE_CODE_RESULTS.md\", \"a\") as file:\n\t        file.writelines(output_lines)\n", "def load_evaluations_from_file(file_path):\n\t    \"\"\"Loads the evaluations from a YAML file.\"\"\"\n\t    try:\n\t        with open(file_path, \"r\") as file:\n\t            data = yaml.safe_load(file)\n\t            if EVAL_LIST_NAME in data:\n\t                return data[EVAL_LIST_NAME]\n\t            else:\n\t                print(f\"'{EVAL_LIST_NAME}' not found in {file_path}\")\n\t    except FileNotFoundError:\n", "        print(f\"File not found: {file_path}\")\n\tdef run_all_evaluations(eval_list: list[dict]) -> None:\n\t    results = []\n\t    for eval_ob in eval_list:\n\t        results.append(single_evaluate(eval_ob))\n\t    # Step 4. Generate Report\n\t    generate_report(eval_list, results)\n\tif __name__ == \"__main__\":\n\t    eval_list = load_evaluations_from_file(\"evals/existing_code_eval.yaml\")\n\t    run_all_evaluations(eval_list)\n"]}
{"filename": "evals/eval_tools.py", "chunked_list": ["\"\"\"\n\tThis library is used for the evaluation of gpt-engineer's performance, on\n\tediting and creating code.  This is very low level in that it looks at the\n\tcode written.  It is possible that the AI could solve the problem in ways\n\tthat we cannot forsee, with this in mind higher level tests are always\n\tbetter than lower.\n\tThe scope will bre relatively limited to a few languages but this could\n\tbe expanded.\n\t\"\"\"\n\tdef check_language(eval_d: dict) -> None:\n", "    if eval_d[\"language\"] != \"python\":\n\t        raise Exception(f\"Language: {eval_d['language']} is not supported.\")\n\tdef assert_exists_in_source_code(eval_d: dict) -> bool:\n\t    \"\"\"Checks of some text exists in the source code.\"\"\"\n\t    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n\t    return source_body.find(eval_d[\"existing_string\"]) > -1\n\tdef run_code_class_has_property(eval_d: dict) -> bool:\n\t    \"\"\"Will execute code, then check if the code has the desired proprty.\"\"\"\n\t    check_language(eval_d)\n\t    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n", "    exec(source_body)\n\t    class_ref = locals().get(eval_d[\"class_name\"])\n\t    ob = class_ref()\n\t    return hasattr(ob, eval_d[\"property_name\"])\n\tdef run_code_class_has_property_w_value(eval_d: dict) -> bool:\n\t    \"\"\"Will execute code, then check if the code has the desired proprty.\"\"\"\n\t    check_language(eval_d)\n\t    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n\t    exec(source_body)\n\t    class_ref = locals().get(eval_d[\"class_name\"])\n", "    ob = class_ref()\n\t    assert hasattr(ob, eval_d[\"property_name\"])\n\t    return getattr(ob, eval_d[\"property_name\"]) == eval_d[\"expected_value\"]\n\tdef run_code_eval_function(eval_d) -> bool:\n\t    \"\"\"Similar to run_code_class_has_property() except is evaluates a function call.\"\"\"\n\t    check_language(eval_d)\n\t    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n\t    exec(source_body)\n\t    function_ref = globals().get(eval_d[\"function_name\"])\n\t    # TODO: add the ability to have function arguments\n", "    return function_ref() == eval_d[\"expected_value\"]\n\tdef check_evaluation_component(eval_d: dict) -> bool:\n\t    \"\"\"Switch on evaluation components\"\"\"\n\t    test_type = eval_d.get(\"type\")\n\t    if test_type == \"assert_exists_in_source_code\":\n\t        return assert_exists_in_source_code(eval_d)\n\t    elif test_type == \"run_code_class_has_property\":\n\t        return run_code_class_has_property(eval_d)\n\t    elif test_type == \"run_code_class_has_property_w_value\":\n\t        return run_code_class_has_property_w_value(eval_d)\n", "    elif test_type == \"run_code_eval_function\":\n\t        return run_code_eval_function(eval_d)\n\t    else:\n\t        raise Exception(f\"Test type '{test_type}' is not recognized.\")\n"]}
{"filename": "evals/__init__.py", "chunked_list": []}
{"filename": "gpt_engineer/main.py", "chunked_list": ["import logging\n\timport os\n\tfrom pathlib import Path\n\timport openai\n\timport typer\n\tfrom dotenv import load_dotenv\n\tfrom gpt_engineer.ai import AI\n\tfrom gpt_engineer.collect import collect_learnings\n\tfrom gpt_engineer.db import DB, DBs, archive\n\tfrom gpt_engineer.learning import collect_consent\n", "from gpt_engineer.steps import STEPS, Config as StepsConfig\n\tapp = typer.Typer()  # creates a CLI app\n\tdef load_env_if_needed():\n\t    if os.getenv(\"OPENAI_API_KEY\") is None:\n\t        load_dotenv()\n\t    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\t@app.command()\n\tdef main(\n\t    project_path: str = typer.Argument(\"projects/example\", help=\"path\"),\n\t    model: str = typer.Argument(\"gpt-4\", help=\"model id string\"),\n", "    temperature: float = 0.1,\n\t    steps_config: StepsConfig = typer.Option(\n\t        StepsConfig.DEFAULT, \"--steps\", \"-s\", help=\"decide which steps to run\"\n\t    ),\n\t    improve_option: bool = typer.Option(\n\t        False,\n\t        \"--improve\",\n\t        \"-i\",\n\t        help=\"Improve code from existing project.\",\n\t    ),\n", "    azure_endpoint: str = typer.Option(\n\t        \"\",\n\t        \"--azure\",\n\t        \"-a\",\n\t        help=\"\"\"Endpoint for your Azure OpenAI Service (https://xx.openai.azure.com).\n\t            In that case, the given model is the deployment name chosen in the Azure AI Studio.\"\"\",\n\t    ),\n\t    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n\t):\n\t    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n", "    # For the improve option take current project as path and add .gpteng folder\n\t    if improve_option:\n\t        # The default option for the --improve is the IMPROVE_CODE, not DEFAULT\n\t        if steps_config == StepsConfig.DEFAULT:\n\t            steps_config = StepsConfig.IMPROVE_CODE\n\t    load_env_if_needed()\n\t    ai = AI(\n\t        model_name=model,\n\t        temperature=temperature,\n\t        azure_endpoint=azure_endpoint,\n", "    )\n\t    input_path = Path(project_path).absolute()\n\t    memory_path = input_path / \"memory\"\n\t    workspace_path = input_path / \"workspace\"\n\t    archive_path = input_path / \"archive\"\n\t    dbs = DBs(\n\t        memory=DB(memory_path),\n\t        logs=DB(memory_path / \"logs\"),\n\t        input=DB(input_path),\n\t        workspace=DB(workspace_path),\n", "        preprompts=DB(\n\t            Path(__file__).parent / \"preprompts\"\n\t        ),  # Loads preprompts from the preprompts directory\n\t        archive=DB(archive_path),\n\t    )\n\t    if steps_config not in [\n\t        StepsConfig.EXECUTE_ONLY,\n\t        StepsConfig.USE_FEEDBACK,\n\t        StepsConfig.EVALUATE,\n\t        StepsConfig.IMPROVE_CODE,\n", "    ]:\n\t        archive(dbs)\n\t        if not dbs.input.get(\"prompt\"):\n\t            dbs.input[\"prompt\"] = input(\n\t                \"\\nWhat application do you want gpt-engineer to generate?\\n\"\n\t            )\n\t    steps = STEPS[steps_config]\n\t    for step in steps:\n\t        messages = step(ai, dbs)\n\t        dbs.logs[step.__name__] = AI.serialize_messages(messages)\n", "    if collect_consent():\n\t        collect_learnings(model, temperature, steps, dbs)\n\t    dbs.logs[\"token_usage\"] = ai.format_token_usage_log()\n\tif __name__ == \"__main__\":\n\t    app()\n"]}
{"filename": "gpt_engineer/chat_to_files.py", "chunked_list": ["import os\n\timport re\n\tfrom typing import List, Tuple\n\tfrom gpt_engineer.db import DB\n\tdef parse_chat(chat) -> List[Tuple[str, str]]:\n\t    \"\"\"\n\t    Extracts all code blocks from a chat and returns them\n\t    as a list of (filename, codeblock) tuples.\n\t    Parameters\n\t    ----------\n", "    chat : str\n\t        The chat to extract code blocks from.\n\t    Returns\n\t    -------\n\t    List[Tuple[str, str]]\n\t        A list of tuples, where each tuple contains a filename and a code block.\n\t    \"\"\"\n\t    # Get all ``` blocks and preceding filenames\n\t    regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n\t    matches = re.finditer(regex, chat, re.DOTALL)\n", "    files = []\n\t    for match in matches:\n\t        # Strip the filename of any non-allowed characters and convert / to \\\n\t        path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n\t        # Remove leading and trailing brackets\n\t        path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n\t        # Remove leading and trailing backticks\n\t        path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n\t        # Remove trailing ]\n\t        path = re.sub(r\"[\\]\\:]$\", \"\", path)\n", "        # Get the code\n\t        code = match.group(2)\n\t        # Add the file to the list\n\t        files.append((path, code))\n\t    # Get all the text before the first ``` block\n\t    readme = chat.split(\"```\")[0]\n\t    files.append((\"README.md\", readme))\n\t    # Return the files\n\t    return files\n\tdef to_files(chat: str, workspace: DB):\n", "    \"\"\"\n\t    Parse the chat and add all extracted files to the workspace.\n\t    Parameters\n\t    ----------\n\t    chat : str\n\t        The chat to parse.\n\t    workspace : DB\n\t        The workspace to add the files to.\n\t    \"\"\"\n\t    workspace[\"all_output.txt\"] = chat  # TODO store this in memory db instead\n", "    files = parse_chat(chat)\n\t    for file_name, file_content in files:\n\t        workspace[file_name] = file_content\n\tdef overwrite_files(chat, dbs):\n\t    \"\"\"\n\t    Replace the AI files with the older local files.\n\t    Parameters\n\t    ----------\n\t    chat : str\n\t        The chat containing the AI files.\n", "    dbs : DBs\n\t        The database containing the workspace.\n\t    \"\"\"\n\t    dbs.workspace[\"all_output.txt\"] = chat  # TODO store this in memory db instead\n\t    files = parse_chat(chat)\n\t    for file_name, file_content in files:\n\t        if file_name == \"README.md\":\n\t            dbs.workspace[\n\t                \"LAST_MODIFICATION_README.md\"\n\t            ] = file_content  # TODO store this in memory db instead\n", "        else:\n\t            dbs.workspace[file_name] = file_content\n\tdef get_code_strings(input: DB) -> dict[str, str]:\n\t    \"\"\"\n\t    Read file_list.txt and return file names and their content.\n\t    Parameters\n\t    ----------\n\t    input : dict\n\t        A dictionary containing the file_list.txt.\n\t    Returns\n", "    -------\n\t    dict[str, str]\n\t        A dictionary mapping file names to their content.\n\t    \"\"\"\n\t    files_paths = input[\"file_list.txt\"].strip().split(\"\\n\")\n\t    files_dict = {}\n\t    for full_file_path in files_paths:\n\t        with open(full_file_path, \"r\") as file:\n\t            file_data = file.read()\n\t        if file_data:\n", "            file_name = os.path.relpath(full_file_path, input.path)\n\t            files_dict[file_name] = file_data\n\t    return files_dict\n\tdef format_file_to_input(file_name: str, file_content: str) -> str:\n\t    \"\"\"\n\t    Format a file string to use as input to the AI agent.\n\t    Parameters\n\t    ----------\n\t    file_name : str\n\t        The name of the file.\n", "    file_content : str\n\t        The content of the file.\n\t    Returns\n\t    -------\n\t    str\n\t        The formatted file string.\n\t    \"\"\"\n\t    file_str = f\"\"\"\n\t    {file_name}\n\t    ```\n", "    {file_content}\n\t    ```\n\t    \"\"\"\n\t    return file_str\n"]}
{"filename": "gpt_engineer/domain.py", "chunked_list": ["from typing import Callable, List, TypeVar\n\tfrom gpt_engineer.ai import AI\n\tfrom gpt_engineer.db import DBs\n\tStep = TypeVar(\"Step\", bound=Callable[[AI, DBs], List[dict]])\n"]}
{"filename": "gpt_engineer/db.py", "chunked_list": ["import datetime\n\timport shutil\n\tfrom dataclasses import dataclass\n\tfrom pathlib import Path\n\tfrom typing import Any, Optional, Union\n\t# This class represents a simple database that stores its data as files in a directory.\n\tclass DB:\n\t    \"\"\"A simple key-value store, where keys are filenames and values are file contents.\"\"\"\n\t    def __init__(self, path: Union[str, Path]):\n\t        \"\"\"\n", "        Initialize the DB class.\n\t        Parameters\n\t        ----------\n\t        path : Union[str, Path]\n\t            The path to the directory where the database files are stored.\n\t        \"\"\"\n\t        self.path: Path = Path(path).absolute()\n\t        self.path.mkdir(parents=True, exist_ok=True)\n\t    def __contains__(self, key: str) -> bool:\n\t        \"\"\"\n", "        Check if a file with the specified name exists in the database.\n\t        Parameters\n\t        ----------\n\t        key : str\n\t            The name of the file to check.\n\t        Returns\n\t        -------\n\t        bool\n\t            True if the file exists, False otherwise.\n\t        \"\"\"\n", "        return (self.path / key).is_file()\n\t    def __getitem__(self, key: str) -> str:\n\t        \"\"\"\n\t        Get the content of a file in the database.\n\t        Parameters\n\t        ----------\n\t        key : str\n\t            The name of the file to get the content of.\n\t        Returns\n\t        -------\n", "        str\n\t            The content of the file.\n\t        Raises\n\t        ------\n\t        KeyError\n\t            If the file does not exist in the database.\n\t        \"\"\"\n\t        full_path = self.path / key\n\t        if not full_path.is_file():\n\t            raise KeyError(f\"File '{key}' could not be found in '{self.path}'\")\n", "        with full_path.open(\"r\", encoding=\"utf-8\") as f:\n\t            return f.read()\n\t    def get(self, key: str, default: Optional[Any] = None) -> Any:\n\t        \"\"\"\n\t        Get the content of a file in the database, or a default value if the file does not exist.\n\t        Parameters\n\t        ----------\n\t        key : str\n\t            The name of the file to get the content of.\n\t        default : any, optional\n", "            The default value to return if the file does not exist, by default None.\n\t        Returns\n\t        -------\n\t        any\n\t            The content of the file, or the default value if the file does not exist.\n\t        \"\"\"\n\t        try:\n\t            return self[key]\n\t        except KeyError:\n\t            return default\n", "    def __setitem__(self, key: Union[str, Path], val: str) -> None:\n\t        \"\"\"\n\t        Set the content of a file in the database.\n\t        Parameters\n\t        ----------\n\t        key : Union[str, Path]\n\t            The name of the file to set the content of.\n\t        val : str\n\t            The content to set.\n\t        Raises\n", "        ------\n\t        TypeError\n\t            If val is not string.\n\t        \"\"\"\n\t        if str(key).startswith(\"../\"):\n\t            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\t        assert isinstance(val, str), \"val must be str\"\n\t        full_path = self.path / key\n\t        full_path.parent.mkdir(parents=True, exist_ok=True)\n\t        full_path.write_text(val, encoding=\"utf-8\")\n", "# dataclass for all dbs:\n\t@dataclass\n\tclass DBs:\n\t    memory: DB\n\t    logs: DB\n\t    preprompts: DB\n\t    input: DB\n\t    workspace: DB\n\t    archive: DB\n\tdef archive(dbs: DBs) -> None:\n", "    \"\"\"\n\t    Archive the memory and workspace databases.\n\t    Parameters\n\t    ----------\n\t    dbs : DBs\n\t        The databases to archive.\n\t    \"\"\"\n\t    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\t    shutil.move(\n\t        str(dbs.memory.path), str(dbs.archive.path / timestamp / dbs.memory.path.name)\n", "    )\n\t    shutil.move(\n\t        str(dbs.workspace.path),\n\t        str(dbs.archive.path / timestamp / dbs.workspace.path.name),\n\t    )\n\t    return []\n"]}
{"filename": "gpt_engineer/steps.py", "chunked_list": ["import inspect\n\timport re\n\timport subprocess\n\tfrom enum import Enum\n\tfrom typing import List, Union\n\tfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\n\tfrom termcolor import colored\n\tfrom gpt_engineer.ai import AI\n\tfrom gpt_engineer.chat_to_files import (\n\t    format_file_to_input,\n", "    get_code_strings,\n\t    overwrite_files,\n\t    to_files,\n\t)\n\tfrom gpt_engineer.db import DBs\n\tfrom gpt_engineer.file_selector import FILE_LIST_NAME, ask_for_files\n\tfrom gpt_engineer.learning import human_review_input\n\tMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\tdef setup_sys_prompt(dbs: DBs) -> str:\n\t    \"\"\"\n", "    Primes the AI with instructions as to how it should\n\t    generate code and the philosophy to follow\n\t    \"\"\"\n\t    return (\n\t        dbs.preprompts[\"roadmap\"]\n\t        + dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"])\n\t        + \"\\nUseful to know:\\n\"\n\t        + dbs.preprompts[\"philosophy\"]\n\t    )\n\tdef setup_sys_prompt_existing_code(dbs: DBs) -> str:\n", "    \"\"\"\n\t    Similar to code generation, but using an existing code base.\n\t    \"\"\"\n\t    return (\n\t        dbs.preprompts[\"improve\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"])\n\t        + \"\\nUseful to know:\\n\"\n\t        + dbs.preprompts[\"philosophy\"]\n\t    )\n\tdef curr_fn() -> str:\n\t    \"\"\"\n", "    Get the name of the current function\n\t    NOTE: This will be the name of the function that called this function,\n\t    so it serves to ensure we don't hardcode the function name in the step,\n\t    but allow the step names to be refactored\n\t    \"\"\"\n\t    return inspect.stack()[1].function\n\t# All steps below have the Step signature\n\tdef simple_gen(ai: AI, dbs: DBs) -> List[Message]:\n\t    \"\"\"Run the AI on the main prompt and save the results\"\"\"\n\t    messages = ai.start(setup_sys_prompt(dbs), dbs.input[\"prompt\"], step_name=curr_fn())\n", "    to_files(messages[-1].content.strip(), dbs.workspace)\n\t    return messages\n\tdef clarify(ai: AI, dbs: DBs) -> List[Message]:\n\t    \"\"\"\n\t    Ask the user if they want to clarify anything and save the results to the workspace\n\t    \"\"\"\n\t    messages: List[Message] = [ai.fsystem(dbs.preprompts[\"clarify\"])]\n\t    user_input = dbs.input[\"prompt\"]\n\t    while True:\n\t        messages = ai.next(messages, user_input, step_name=curr_fn())\n", "        msg = messages[-1].content.strip()\n\t        if msg == \"Nothing more to clarify.\":\n\t            break\n\t        if msg.lower().startswith(\"no\"):\n\t            print(\"Nothing more to clarify.\")\n\t            break\n\t        print()\n\t        user_input = input('(answer in text, or \"c\" to move on)\\n')\n\t        print()\n\t        if not user_input or user_input == \"c\":\n", "            print(\"(letting gpt-engineer make its own assumptions)\")\n\t            print()\n\t            messages = ai.next(\n\t                messages,\n\t                \"Make your own assumptions and state them explicitly before starting\",\n\t                step_name=curr_fn(),\n\t            )\n\t            print()\n\t            return messages\n\t        user_input += (\n", "            \"\\n\\n\"\n\t            \"Is anything else unclear? If yes, only answer in the form:\\n\"\n\t            \"{remaining unclear areas} remaining questions.\\n\"\n\t            \"{Next question}\\n\"\n\t            'If everything is sufficiently clear, only answer \"Nothing more to clarify.\".'\n\t        )\n\t    print()\n\t    return messages\n\tdef gen_spec(ai: AI, dbs: DBs) -> List[Message]:\n\t    \"\"\"\n", "    Generate a spec from the main prompt + clarifications and save the results to\n\t    the workspace\n\t    \"\"\"\n\t    messages = [\n\t        ai.fsystem(setup_sys_prompt(dbs)),\n\t        ai.fsystem(f\"Instructions: {dbs.input['prompt']}\"),\n\t    ]\n\t    messages = ai.next(messages, dbs.preprompts[\"spec\"], step_name=curr_fn())\n\t    dbs.memory[\"specification\"] = messages[-1].content.strip()\n\t    return messages\n", "def respec(ai: AI, dbs: DBs) -> List[Message]:\n\t    \"\"\"Asks the LLM to review the specs so far and reiterate them if necessary\"\"\"\n\t    messages = AI.deserialize_messages(dbs.logs[gen_spec.__name__])\n\t    messages += [ai.fsystem(dbs.preprompts[\"respec\"])]\n\t    messages = ai.next(messages, step_name=curr_fn())\n\t    messages = ai.next(\n\t        messages,\n\t        (\n\t            \"Based on the conversation so far, please reiterate the specification for \"\n\t            \"the program. \"\n", "            \"If there are things that can be improved, please incorporate the \"\n\t            \"improvements. \"\n\t            \"If you are satisfied with the specification, just write out the \"\n\t            \"specification word by word again.\"\n\t        ),\n\t        step_name=curr_fn(),\n\t    )\n\t    dbs.memory[\"specification\"] = messages[-1].content.strip()\n\t    return messages\n\tdef gen_unit_tests(ai: AI, dbs: DBs) -> List[dict]:\n", "    \"\"\"\n\t    Generate unit tests based on the specification, that should work.\n\t    \"\"\"\n\t    messages = [\n\t        ai.fsystem(setup_sys_prompt(dbs)),\n\t        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n\t        ai.fuser(f\"Specification:\\n\\n{dbs.memory['specification']}\"),\n\t    ]\n\t    messages = ai.next(messages, dbs.preprompts[\"unit_tests\"], step_name=curr_fn())\n\t    dbs.memory[\"unit_tests\"] = messages[-1].content.strip()\n", "    to_files(dbs.memory[\"unit_tests\"], dbs.workspace)\n\t    return messages\n\tdef gen_clarified_code(ai: AI, dbs: DBs) -> List[dict]:\n\t    \"\"\"Takes clarification and generates code\"\"\"\n\t    messages = AI.deserialize_messages(dbs.logs[clarify.__name__])\n\t    messages = [\n\t        ai.fsystem(setup_sys_prompt(dbs)),\n\t    ] + messages[\n\t        1:\n\t    ]  # skip the first clarify message, which was the original clarify priming prompt\n", "    messages = ai.next(\n\t        messages,\n\t        dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"]),\n\t        step_name=curr_fn(),\n\t    )\n\t    to_files(messages[-1].content.strip(), dbs.workspace)\n\t    return messages\n\tdef gen_code_after_unit_tests(ai: AI, dbs: DBs) -> List[dict]:\n\t    \"\"\"Generates project code after unit tests have been produced\"\"\"\n\t    messages = [\n", "        ai.fsystem(setup_sys_prompt(dbs)),\n\t        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n\t        ai.fuser(f\"Specification:\\n\\n{dbs.memory['specification']}\"),\n\t        ai.fuser(f\"Unit tests:\\n\\n{dbs.memory['unit_tests']}\"),\n\t    ]\n\t    messages = ai.next(\n\t        messages,\n\t        dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"]),\n\t        step_name=curr_fn(),\n\t    )\n", "    to_files(messages[-1].content.strip(), dbs.workspace)\n\t    return messages\n\tdef execute_entrypoint(ai: AI, dbs: DBs) -> List[dict]:\n\t    command = dbs.workspace[\"run.sh\"]\n\t    print()\n\t    print(\n\t        colored(\n\t            \"Do you want to execute this code? (y/n)\",\n\t            \"red\",\n\t        )\n", "    )\n\t    print()\n\t    print(command)\n\t    print()\n\t    print(\"To execute, you can also press enter.\")\n\t    print()\n\t    if input() not in [\"\", \"y\", \"yes\"]:\n\t        print(\"Ok, not executing the code.\")\n\t        return []\n\t    print(\"Executing the code...\")\n", "    print()\n\t    print(\n\t        colored(\n\t            \"Note: If it does not work as expected, consider running the code\"\n\t            + \" in another way than above.\",\n\t            \"green\",\n\t        )\n\t    )\n\t    print()\n\t    print(\"You can press ctrl+c *once* to stop the execution.\")\n", "    print()\n\t    p = subprocess.Popen(\"bash run.sh\", shell=True, cwd=dbs.workspace.path)\n\t    try:\n\t        p.wait()\n\t    except KeyboardInterrupt:\n\t        print()\n\t        print(\"Stopping execution.\")\n\t        print(\"Execution stopped.\")\n\t        p.kill()\n\t        print()\n", "    return []\n\tdef gen_entrypoint(ai: AI, dbs: DBs) -> List[dict]:\n\t    messages = ai.start(\n\t        system=(\n\t            \"You will get information about a codebase that is currently on disk in \"\n\t            \"the current folder.\\n\"\n\t            \"From this you will answer with code blocks that includes all the necessary \"\n\t            \"unix terminal commands to \"\n\t            \"a) install dependencies \"\n\t            \"b) run all necessary parts of the codebase (in parallel if necessary).\\n\"\n", "            \"Do not install globally. Do not use sudo.\\n\"\n\t            \"Do not explain the code, just give the commands.\\n\"\n\t            \"Do not use placeholders, use example values (like . for a folder argument) \"\n\t            \"if necessary.\\n\"\n\t        ),\n\t        user=\"Information about the codebase:\\n\\n\" + dbs.workspace[\"all_output.txt\"],\n\t        step_name=curr_fn(),\n\t    )\n\t    print()\n\t    regex = r\"```\\S*\\n(.+?)```\"\n", "    matches = re.finditer(regex, messages[-1].content.strip(), re.DOTALL)\n\t    dbs.workspace[\"run.sh\"] = \"\\n\".join(match.group(1) for match in matches)\n\t    return messages\n\tdef use_feedback(ai: AI, dbs: DBs):\n\t    messages = [\n\t        ai.fsystem(setup_sys_prompt(dbs)),\n\t        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n\t        ai.fassistant(\n\t            dbs.workspace[\"all_output.txt\"]\n\t        ),  # reload previously generated code\n", "    ]\n\t    if dbs.input[\"feedback\"]:\n\t        messages = ai.next(messages, dbs.input[\"feedback\"], step_name=curr_fn())\n\t        to_files(messages[-1].content.strip(), dbs.workspace)\n\t        return messages\n\t    else:\n\t        print(\n\t            \"No feedback was found in the input folder. Please create a file \"\n\t            + \"called 'feedback' in the same folder as the prompt file.\"\n\t        )\n", "        exit(1)\n\tdef set_improve_filelist(ai: AI, dbs: DBs):\n\t    \"\"\"Sets the file list for files to work with in existing code mode.\"\"\"\n\t    ask_for_files(dbs.input)  # stores files as full paths.\n\t    return []\n\tdef assert_files_ready(ai: AI, dbs: DBs):\n\t    \"\"\"Checks that the required files are present for headless\n\t    improve code execution.\"\"\"\n\t    assert (\n\t        \"file_list.txt\" in dbs.input\n", "    ), \"For auto_mode file_list.txt need to be in your project folder.\"\n\t    assert \"prompt\" in dbs.input, \"For auto_mode a prompt file must exist.\"\n\t    return []\n\tdef get_improve_prompt(ai: AI, dbs: DBs):\n\t    \"\"\"\n\t    Asks the user what they would like to fix.\n\t    \"\"\"\n\t    if not dbs.input.get(\"prompt\"):\n\t        dbs.input[\"prompt\"] = input(\n\t            \"\\nWhat do you need to improve with the selected files?\\n\"\n", "        )\n\t    confirm_str = \"\\n\".join(\n\t        [\n\t            \"-----------------------------\",\n\t            \"The following files will be used in the improvement process:\",\n\t            f\"{FILE_LIST_NAME}:\",\n\t            str(dbs.input[\"file_list.txt\"]),\n\t            \"\",\n\t            \"The inserted prompt is the following:\",\n\t            f\"'{dbs.input['prompt']}'\",\n", "            \"-----------------------------\",\n\t            \"\",\n\t            \"You can change these files in your project before proceeding.\",\n\t            \"\",\n\t            \"Press enter to proceed with modifications.\",\n\t            \"\",\n\t        ]\n\t    )\n\t    input(confirm_str)\n\t    return []\n", "def improve_existing_code(ai: AI, dbs: DBs):\n\t    \"\"\"\n\t    After the file list and prompt have been aquired, this function is called\n\t    to sent the formatted prompt to the LLM.\n\t    \"\"\"\n\t    files_info = get_code_strings(dbs.input)  # this only has file names not paths\n\t    messages = [\n\t        ai.fsystem(setup_sys_prompt_existing_code(dbs)),\n\t    ]\n\t    # Add files as input\n", "    for file_name, file_str in files_info.items():\n\t        code_input = format_file_to_input(file_name, file_str)\n\t        messages.append(ai.fuser(f\"{code_input}\"))\n\t    messages.append(ai.fuser(f\"Request: {dbs.input['prompt']}\"))\n\t    messages = ai.next(messages, step_name=curr_fn())\n\t    overwrite_files(messages[-1].content.strip(), dbs)\n\t    return messages\n\tdef fix_code(ai: AI, dbs: DBs):\n\t    messages = AI.deserialize_messages(dbs.logs[gen_code_after_unit_tests.__name__])\n\t    code_output = messages[-1].content.strip()\n", "    messages = [\n\t        ai.fsystem(setup_sys_prompt(dbs)),\n\t        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n\t        ai.fuser(code_output),\n\t        ai.fsystem(dbs.preprompts[\"fix_code\"]),\n\t    ]\n\t    messages = ai.next(\n\t        messages, \"Please fix any errors in the code above.\", step_name=curr_fn()\n\t    )\n\t    to_files(messages[-1].content.strip(), dbs.workspace)\n", "    return messages\n\tdef human_review(ai: AI, dbs: DBs):\n\t    \"\"\"Collects and stores human review of the code\"\"\"\n\t    review = human_review_input()\n\t    if review is not None:\n\t        dbs.memory[\"review\"] = review.to_json()  # type: ignore\n\t    return []\n\tclass Config(str, Enum):\n\t    DEFAULT = \"default\"\n\t    BENCHMARK = \"benchmark\"\n", "    SIMPLE = \"simple\"\n\t    TDD = \"tdd\"\n\t    TDD_PLUS = \"tdd+\"\n\t    CLARIFY = \"clarify\"\n\t    RESPEC = \"respec\"\n\t    EXECUTE_ONLY = \"execute_only\"\n\t    EVALUATE = \"evaluate\"\n\t    USE_FEEDBACK = \"use_feedback\"\n\t    IMPROVE_CODE = \"improve_code\"\n\t    EVAL_IMPROVE_CODE = \"eval_improve_code\"\n", "# Define the steps to run for different configs\n\tSTEPS = {\n\t    Config.DEFAULT: [\n\t        clarify,\n\t        gen_clarified_code,\n\t        gen_entrypoint,\n\t        execute_entrypoint,\n\t        human_review,\n\t    ],\n\t    Config.BENCHMARK: [\n", "        simple_gen,\n\t        gen_entrypoint,\n\t    ],\n\t    Config.SIMPLE: [\n\t        simple_gen,\n\t        gen_entrypoint,\n\t        execute_entrypoint,\n\t    ],\n\t    Config.TDD: [\n\t        gen_spec,\n", "        gen_unit_tests,\n\t        gen_code_after_unit_tests,\n\t        gen_entrypoint,\n\t        execute_entrypoint,\n\t        human_review,\n\t    ],\n\t    Config.TDD_PLUS: [\n\t        gen_spec,\n\t        gen_unit_tests,\n\t        gen_code_after_unit_tests,\n", "        fix_code,\n\t        gen_entrypoint,\n\t        execute_entrypoint,\n\t        human_review,\n\t    ],\n\t    Config.CLARIFY: [\n\t        clarify,\n\t        gen_clarified_code,\n\t        gen_entrypoint,\n\t        execute_entrypoint,\n", "        human_review,\n\t    ],\n\t    Config.RESPEC: [\n\t        gen_spec,\n\t        respec,\n\t        gen_unit_tests,\n\t        gen_code_after_unit_tests,\n\t        fix_code,\n\t        gen_entrypoint,\n\t        execute_entrypoint,\n", "        human_review,\n\t    ],\n\t    Config.USE_FEEDBACK: [use_feedback, gen_entrypoint, execute_entrypoint, human_review],\n\t    Config.EXECUTE_ONLY: [execute_entrypoint],\n\t    Config.EVALUATE: [execute_entrypoint, human_review],\n\t    Config.IMPROVE_CODE: [\n\t        set_improve_filelist,\n\t        get_improve_prompt,\n\t        improve_existing_code,\n\t    ],\n", "    Config.EVAL_IMPROVE_CODE: [assert_files_ready, improve_existing_code],\n\t}\n\t# Future steps that can be added:\n\t# run_tests_and_fix_files\n\t# execute_entrypoint_and_fix_files_if_it_results_in_error\n"]}
{"filename": "gpt_engineer/__init__.py", "chunked_list": []}
{"filename": "gpt_engineer/file_selector.py", "chunked_list": ["import os\n\timport re\n\timport sys\n\timport tkinter as tk\n\timport tkinter.filedialog as fd\n\tfrom pathlib import Path\n\tfrom typing import List, Union\n\tIGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\"}\n\tFILE_LIST_NAME = \"file_list.txt\"\n\tclass DisplayablePath(object):\n", "    \"\"\"\n\t    A class representing a displayable path in a file explorer.\n\t    \"\"\"\n\t    display_filename_prefix_middle = \" \"\n\t    display_filename_prefix_last = \" \"\n\t    display_parent_prefix_middle = \"    \"\n\t    display_parent_prefix_last = \"   \"\n\t    def __init__(\n\t        self, path: Union[str, Path], parent_path: \"DisplayablePath\", is_last: bool\n\t    ):\n", "        \"\"\"\n\t        Initialize a DisplayablePath object.\n\t        Args:\n\t            path (Union[str, Path]): The path of the file or directory.\n\t            parent_path (DisplayablePath): The parent path of the file or directory.\n\t            is_last (bool): Whether the file or directory is the last child of its parent.\n\t        \"\"\"\n\t        self.depth: int = 0\n\t        self.path = Path(str(path))\n\t        self.parent = parent_path\n", "        self.is_last = is_last\n\t        if self.parent:\n\t            self.depth = self.parent.depth + 1\n\t    @property\n\t    def display_name(self) -> str:\n\t        \"\"\"\n\t        Get the display name of the file or directory.\n\t        Returns:\n\t            str: The display name.\n\t        \"\"\"\n", "        if self.path.is_dir():\n\t            return self.path.name + \"/\"\n\t        return self.path.name\n\t    @classmethod\n\t    def make_tree(cls, root: Union[str, Path], parent=None, is_last=False, criteria=None):\n\t        \"\"\"\n\t        Generate a tree of DisplayablePath objects.\n\t        Args:\n\t            root: The root path of the tree.\n\t            parent: The parent path of the root path. Defaults to None.\n", "            is_last: Whether the root path is the last child of its parent.\n\t            criteria: The criteria function to filter the paths. Defaults to None.\n\t        Yields:\n\t            DisplayablePath: The DisplayablePath objects in the tree.\n\t        \"\"\"\n\t        root = Path(str(root))\n\t        criteria = criteria or cls._default_criteria\n\t        displayable_root = cls(root, parent, is_last)\n\t        yield displayable_root\n\t        children = sorted(\n", "            list(path for path in root.iterdir() if criteria(path)),\n\t            key=lambda s: str(s).lower(),\n\t        )\n\t        count = 1\n\t        for path in children:\n\t            is_last = count == len(children)\n\t            if path.is_dir() and path.name not in IGNORE_FOLDERS:\n\t                yield from cls.make_tree(\n\t                    path, parent=displayable_root, is_last=is_last, criteria=criteria\n\t                )\n", "            else:\n\t                yield cls(path, displayable_root, is_last)\n\t            count += 1\n\t    @classmethod\n\t    def _default_criteria(cls, path: Path) -> bool:\n\t        \"\"\"\n\t        The default criteria function to filter the paths.\n\t        Args:\n\t            path: The path to check.\n\t        Returns:\n", "            bool: True if the path should be included, False otherwise.\n\t        \"\"\"\n\t        return True\n\t    def displayable(self) -> str:\n\t        \"\"\"\n\t        Get the displayable string representation of the file or directory.\n\t        Returns:\n\t            str: The displayable string representation.\n\t        \"\"\"\n\t        if self.parent is None:\n", "            return self.display_name\n\t        _filename_prefix = (\n\t            self.display_filename_prefix_last\n\t            if self.is_last\n\t            else self.display_filename_prefix_middle\n\t        )\n\t        parts = [\"{!s} {!s}\".format(_filename_prefix, self.display_name)]\n\t        parent = self.parent\n\t        while parent and parent.parent is not None:\n\t            parts.append(\n", "                self.display_parent_prefix_middle\n\t                if parent.is_last\n\t                else self.display_parent_prefix_last\n\t            )\n\t            parent = parent.parent\n\t        return \"\".join(reversed(parts))\n\tclass TerminalFileSelector:\n\t    def __init__(self, root_folder_path: Path) -> None:\n\t        self.number_of_selectable_items = 0\n\t        self.selectable_file_paths: dict[int, str] = {}\n", "        self.file_path_list: list = []\n\t        self.db_paths = DisplayablePath.make_tree(\n\t            root_folder_path, parent=None, criteria=is_in_ignoring_extensions\n\t        )\n\t    def display(self):\n\t        \"\"\"\n\t        Select files from a directory and display the selected files.\n\t        \"\"\"\n\t        count = 0\n\t        file_path_enumeration = {}\n", "        file_path_list = []\n\t        for path in self.db_paths:\n\t            n_digits = len(str(count))\n\t            n_spaces = 3 - n_digits\n\t            if n_spaces < 0:\n\t                # We can only print 1000 aligned files. I think it is decent enough\n\t                n_spaces = 0\n\t            spaces_str = \" \" * n_spaces\n\t            if not path.path.is_dir():\n\t                print(f\"{count}. {spaces_str}{path.displayable()}\")\n", "                file_path_enumeration[count] = path.path\n\t                file_path_list.append(path.path)\n\t                count += 1\n\t            else:\n\t                # By now we do not accept selecting entire dirs.\n\t                # But could add that in the future. Just need to add more functions\n\t                # and remove this else block...\n\t                number_space = \" \" * n_digits\n\t                print(f\"{number_space}  {spaces_str}{path.displayable()}\")\n\t        self.number_of_selectable_items = count\n", "        self.file_path_list = file_path_list\n\t        self.selectable_file_paths = file_path_enumeration\n\t    def ask_for_selection(self) -> List[str]:\n\t        \"\"\"\n\t        Ask user to select files from the terminal after displaying it\n\t        Returns:\n\t            List[str]: list of selected paths\n\t        \"\"\"\n\t        user_input = input(\n\t            \"\\n\".join(\n", "                [\n\t                    \"Select files by entering the numbers separated by commas/spaces or\",\n\t                    \"specify range with a dash. \",\n\t                    \"Example: 1,2,3-5,7,9,13-15,18,20 (enter 'all' to select everything)\",\n\t                    \"\\n\\nSelect files:\",\n\t                ]\n\t            )\n\t        )\n\t        selected_paths = []\n\t        regex = r\"\\d+(-\\d+)?([, ]\\d+(-\\d+)?)*\"\n", "        if user_input.lower() == \"all\":\n\t            selected_paths = self.file_path_list\n\t        elif re.match(regex, user_input):\n\t            try:\n\t                user_input = (\n\t                    user_input.replace(\" \", \",\") if \" \" in user_input else user_input\n\t                )\n\t                selected_files = user_input.split(\",\")\n\t                for file_number_str in selected_files:\n\t                    if \"-\" in file_number_str:\n", "                        start_str, end_str = file_number_str.split(\"-\")\n\t                        start = int(start_str)\n\t                        end = int(end_str)\n\t                        for num in range(start, end + 1):\n\t                            selected_paths.append(str(self.selectable_file_paths[num]))\n\t                    else:\n\t                        num = int(file_number_str)\n\t                        selected_paths.append(str(self.selectable_file_paths[num]))\n\t            except ValueError:\n\t                pass\n", "        else:\n\t            print(\"Please use a valid number/series of numbers.\\n\")\n\t            sys.exit(1)\n\t        return selected_paths\n\tdef is_in_ignoring_extensions(path: Path) -> bool:\n\t    \"\"\"\n\t    Check if a path is not hidden or in the __pycache__ directory.\n\t    Args:\n\t        path: The path to check.\n\t    Returns:\n", "        bool: True if the path is not in ignored rules. False otherwise.\n\t    \"\"\"\n\t    is_hidden = not path.name.startswith(\".\")\n\t    is_pycache = \"__pycache__\" not in path.name\n\t    return is_hidden and is_pycache\n\tdef ask_for_files(db_input) -> None:\n\t    \"\"\"\n\t    Ask user to select files to improve.\n\t    It can be done by terminal, gui, or using the old selection.\n\t    Returns:\n", "        dict[str, str]: Dictionary where key = file name and value = file path\n\t    \"\"\"\n\t    use_last_string = \"\"\n\t    if \"file_list.txt\" in db_input:\n\t        use_last_string = (\n\t            \"3. Use previous file list (available at \"\n\t            + f\"{os.path.join(db_input.path, 'file_list.txt')})\\n\"\n\t        )\n\t        selection_number = 3\n\t    else:\n", "        selection_number = 1\n\t    selection_str = \"\\n\".join(\n\t        [\n\t            \"How do you want to select the files?\",\n\t            \"\",\n\t            \"1. Use File explorer.\",\n\t            \"2. Use Command-Line.\",\n\t            use_last_string if len(use_last_string) > 1 else \"\",\n\t            f\"Select option and press Enter (default={selection_number}): \",\n\t        ]\n", "    )\n\t    file_path_list = []\n\t    selected_number_str = input(selection_str)\n\t    if selected_number_str:\n\t        try:\n\t            selection_number = int(selected_number_str)\n\t        except ValueError:\n\t            print(\"Invalid number. Select a number from the list above.\\n\")\n\t            sys.exit(1)\n\t    if selection_number == 1:\n", "        # Open GUI selection\n\t        file_path_list = gui_file_selector()\n\t    elif selection_number == 2:\n\t        # Open terminal selection\n\t        file_path_list = terminal_file_selector()\n\t    if (\n\t        selection_number <= 0\n\t        or selection_number > 3\n\t        or (selection_number == 3 and not use_last_string)\n\t    ):\n", "        print(\"Invalid number. Select a number from the list above.\\n\")\n\t        sys.exit(1)\n\t    if not selection_number == 3:\n\t        db_input[\"file_list.txt\"] = \"\\n\".join(file_path_list)\n\tdef gui_file_selector() -> List[str]:\n\t    \"\"\"\n\t    Display a tkinter file selection window to select context files.\n\t    \"\"\"\n\t    root = tk.Tk()\n\t    root.withdraw()\n", "    root.call(\"wm\", \"attributes\", \".\", \"-topmost\", True)\n\t    file_list = list(\n\t        fd.askopenfilenames(\n\t            parent=root,\n\t            initialdir=os.getcwd(),\n\t            title=\"Select files to improve (or give context):\",\n\t        )\n\t    )\n\t    return file_list\n\tdef terminal_file_selector() -> List[str]:\n", "    \"\"\"\n\t    Display a terminal file selection to select context files.\n\t    \"\"\"\n\t    file_selector = TerminalFileSelector(Path(os.getcwd()))\n\t    file_selector.display()\n\t    selected_list = file_selector.ask_for_selection()\n\t    return selected_list\n"]}
{"filename": "gpt_engineer/collect.py", "chunked_list": ["import hashlib\n\tfrom typing import List\n\tfrom gpt_engineer import steps\n\tfrom gpt_engineer.db import DBs\n\tfrom gpt_engineer.domain import Step\n\tfrom gpt_engineer.learning import Learning, extract_learning\n\tdef send_learning(learning: Learning):\n\t    \"\"\"\n\t    Send the learning data to RudderStack for analysis.\n\t    Note:\n", "    This function is only called if consent is given to share data.\n\t    Data is not shared to a third party. It is used with the sole purpose of\n\t    improving gpt-engineer, and letting it handle more use cases.\n\t    Consent logic is in gpt_engineer/learning.py\n\t    Parameters\n\t    ----------\n\t    learning : Learning\n\t        The learning data to send.\n\t    \"\"\"\n\t    import rudderstack.analytics as rudder_analytics\n", "    rudder_analytics.write_key = \"2Re4kqwL61GDp7S8ewe6K5dbogG\"\n\t    rudder_analytics.dataPlaneUrl = \"https://gptengineerezm.dataplane.rudderstack.com\"\n\t    rudder_analytics.track(\n\t        user_id=learning.session,\n\t        event=\"learning\",\n\t        properties=learning.to_dict(),  # type: ignore\n\t    )\n\tdef collect_learnings(model: str, temperature: float, steps: List[Step], dbs: DBs):\n\t    \"\"\"\n\t    Collect the learning data and send it to RudderStack for analysis.\n", "    Parameters\n\t    ----------\n\t    model : str\n\t        The name of the model used.\n\t    temperature : float\n\t        The temperature used.\n\t    steps : List[Step]\n\t        The list of steps.\n\t    dbs : DBs\n\t        The database containing the workspace.\n", "    \"\"\"\n\t    learnings = extract_learning(\n\t        model, temperature, steps, dbs, steps_file_hash=steps_file_hash()\n\t    )\n\t    try:\n\t        send_learning(learnings)\n\t    except RuntimeError as e:\n\t        # try to remove some parts of learning that might be too big\n\t        # rudderstack max event size is 32kb\n\t        overflow = len(learnings.to_json()) - (32 << 10)  # type: ignore\n", "        assert overflow > 0, f\"encountered error {e} but overflow is {overflow}\"\n\t        learnings.logs = (\n\t            learnings.logs[: -overflow - 200] + f\"\\n\\n[REMOVED {overflow} CHARACTERS]\"\n\t        )\n\t        print(\n\t            \"WARNING: learning too big, removing some parts. \"\n\t            \"Please report if this results in a crash.\"\n\t        )\n\t        send_learning(learnings)\n\tdef steps_file_hash():\n", "    \"\"\"\n\t    Compute the SHA-256 hash of the steps file.\n\t    Returns\n\t    -------\n\t    str\n\t        The SHA-256 hash of the steps file.\n\t    \"\"\"\n\t    with open(steps.__file__, \"r\") as f:\n\t        content = f.read()\n\t        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n"]}
{"filename": "gpt_engineer/learning.py", "chunked_list": ["import json\n\timport random\n\timport tempfile\n\tfrom dataclasses import dataclass, field\n\tfrom datetime import datetime\n\tfrom pathlib import Path\n\tfrom typing import List, Optional\n\tfrom dataclasses_json import dataclass_json\n\tfrom termcolor import colored\n\tfrom gpt_engineer.db import DB, DBs\n", "from gpt_engineer.domain import Step\n\t@dataclass_json\n\t@dataclass\n\tclass Review:\n\t    ran: Optional[bool]\n\t    perfect: Optional[bool]\n\t    works: Optional[bool]\n\t    comments: str\n\t    raw: str\n\t@dataclass_json\n", "@dataclass\n\tclass Learning:\n\t    model: str\n\t    temperature: float\n\t    steps: str\n\t    steps_file_hash: str\n\t    prompt: str\n\t    logs: str\n\t    workspace: str\n\t    feedback: Optional[str]\n", "    session: str\n\t    review: Optional[Review]\n\t    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n\t    version: str = \"0.3\"\n\tTERM_CHOICES = (\n\t    colored(\"y\", \"green\")\n\t    + \"/\"\n\t    + colored(\"n\", \"red\")\n\t    + \"/\"\n\t    + colored(\"u\", \"yellow\")\n", "    + \"(ncertain): \"\n\t)\n\tdef human_review_input() -> Review:\n\t    \"\"\"\n\t    Ask the user to review the generated code and return their review.\n\t    Returns\n\t    -------\n\t    Review\n\t        The user's review of the generated code.\n\t    \"\"\"\n", "    print()\n\t    if not check_consent():\n\t        return None\n\t    print()\n\t    print(\n\t        colored(\"To help gpt-engineer learn, please answer 3 questions:\", \"light_green\")\n\t    )\n\t    print()\n\t    ran = input(\"Did the generated code run at all? \" + TERM_CHOICES)\n\t    while ran not in (\"y\", \"n\", \"u\"):\n", "        ran = input(\"Invalid input. Please enter y, n, or u: \")\n\t    perfect = \"\"\n\t    useful = \"\"\n\t    if ran == \"y\":\n\t        perfect = input(\n\t            \"Did the generated code do everything you wanted? \" + TERM_CHOICES\n\t        )\n\t        while perfect not in (\"y\", \"n\", \"u\"):\n\t            perfect = input(\"Invalid input. Please enter y, n, or u: \")\n\t        if perfect != \"y\":\n", "            useful = input(\"Did the generated code do anything useful? \" + TERM_CHOICES)\n\t            while useful not in (\"y\", \"n\", \"u\"):\n\t                useful = input(\"Invalid input. Please enter y, n, or u: \")\n\t    comments = \"\"\n\t    if perfect != \"y\":\n\t        comments = input(\n\t            \"If you have time, please explain what was not working \"\n\t            + colored(\"(ok to leave blank)\\n\", \"light_green\")\n\t        )\n\t    return Review(\n", "        raw=\", \".join([ran, perfect, useful]),\n\t        ran={\"y\": True, \"n\": False, \"u\": None, \"\": None}[ran],\n\t        works={\"y\": True, \"n\": False, \"u\": None, \"\": None}[useful],\n\t        perfect={\"y\": True, \"n\": False, \"u\": None, \"\": None}[perfect],\n\t        comments=comments,\n\t    )\n\tdef check_consent() -> bool:\n\t    \"\"\"\n\t    Check if the user has given consent to store their data.\n\t    If not, ask for their consent.\n", "    \"\"\"\n\t    path = Path(\".gpte_consent\")\n\t    if path.exists() and path.read_text() == \"true\":\n\t        return True\n\t    answer = input(\"Is it ok if we store your prompts to learn? (y/n)\")\n\t    while answer.lower() not in (\"y\", \"n\"):\n\t        answer = input(\"Invalid input. Please enter y or n: \")\n\t    if answer.lower() == \"y\":\n\t        path.write_text(\"true\")\n\t        print(colored(\"Thank you\", \"light_green\"))\n", "        print()\n\t        print(\"(If you change your mind, delete the file .gpte_consent)\")\n\t        return True\n\t    else:\n\t        print(colored(\"We understand \", \"light_green\"))\n\t        return False\n\tdef collect_consent() -> bool:\n\t    \"\"\"\n\t    Check if the user has given consent to store their data.\n\t    If not, ask for their consent.\n", "    Returns\n\t    -------\n\t    bool\n\t        True if the user has given consent, False otherwise.\n\t    \"\"\"\n\t    consent_flag = Path(\".gpte_consent\")\n\t    has_given_consent = consent_flag.exists() and consent_flag.read_text() == \"true\"\n\t    if has_given_consent:\n\t        return True\n\t    if ask_if_can_store():\n", "        consent_flag.write_text(\"true\")\n\t        print()\n\t        print(\"(If you change your mind, delete the file .gpte_consent)\")\n\t        return True\n\t    return False\n\tdef ask_if_can_store() -> bool:\n\t    \"\"\"\n\t    Ask the user if their data can be stored.\n\t    Returns\n\t    -------\n", "    bool\n\t        True if the user agrees to have their data stored, False otherwise.\n\t    \"\"\"\n\t    print()\n\t    can_store = input(\n\t        \"Have you understood and agree to that \"\n\t        + colored(\"OpenAI \", \"light_green\")\n\t        + \"and \"\n\t        + colored(\"gpt-engineer \", \"light_green\")\n\t        + \"store anonymous learnings about how gpt-engineer is used \"\n", "        + \"(with the sole purpose of improving it)?\\n(y/n)\"\n\t    ).lower()\n\t    while can_store not in (\"y\", \"n\"):\n\t        can_store = input(\"Invalid input. Please enter y or n: \").lower()\n\t    if can_store == \"n\":\n\t        print(colored(\"Ok we understand\", \"light_green\"))\n\t    return can_store == \"y\"\n\tdef logs_to_string(steps: List[Step], logs: DB) -> str:\n\t    \"\"\"\n\t    Convert the logs of the steps to a string.\n", "    Parameters\n\t    ----------\n\t    steps : List[Step]\n\t        The list of steps.\n\t    logs : DB\n\t        The database containing the logs.\n\t    Returns\n\t    -------\n\t    str\n\t        The logs of the steps as a string.\n", "    \"\"\"\n\t    chunks = []\n\t    for step in steps:\n\t        chunks.append(f\"--- {step.__name__} ---\\n\")\n\t        chunks.append(logs[step.__name__])\n\t    return \"\\n\".join(chunks)\n\tdef extract_learning(\n\t    model: str, temperature: float, steps: List[Step], dbs: DBs, steps_file_hash\n\t) -> Learning:\n\t    \"\"\"\n", "    Extract the learning data from the steps and databases.\n\t    Parameters\n\t    ----------\n\t    model : str\n\t        The name of the model used.\n\t    temperature : float\n\t        The temperature used.\n\t    steps : List[Step]\n\t        The list of steps.\n\t    dbs : DBs\n", "        The databases containing the input, logs, memory, and workspace.\n\t    steps_file_hash : str\n\t        The hash of the steps file.\n\t    Returns\n\t    -------\n\t    Learning\n\t        The extracted learning data.\n\t    \"\"\"\n\t    review = None\n\t    if \"review\" in dbs.memory:\n", "        review = Review.from_json(dbs.memory[\"review\"])  # type: ignore\n\t    learning = Learning(\n\t        prompt=dbs.input[\"prompt\"],\n\t        model=model,\n\t        temperature=temperature,\n\t        steps=json.dumps([step.__name__ for step in steps]),\n\t        steps_file_hash=steps_file_hash,\n\t        feedback=dbs.input.get(\"feedback\"),\n\t        session=get_session(),\n\t        logs=logs_to_string(steps, dbs.logs),\n", "        workspace=dbs.workspace[\"all_output.txt\"],\n\t        review=review,\n\t    )\n\t    return learning\n\tdef get_session() -> str:\n\t    \"\"\"\n\t    Returns a unique user id for the current user project (session).\n\t    Returns\n\t    -------\n\t    str\n", "        The unique user id.\n\t    \"\"\"\n\t    path = Path(tempfile.gettempdir()) / \"gpt_engineer_user_id.txt\"\n\t    try:\n\t        if path.exists():\n\t            user_id = path.read_text()\n\t        else:\n\t            # random uuid:\n\t            user_id = str(random.randint(0, 2**32))\n\t            path.write_text(user_id)\n", "        return user_id\n\t    except IOError:\n\t        return \"ephemeral_\" + str(random.randint(0, 2**32))\n"]}
{"filename": "gpt_engineer/ai.py", "chunked_list": ["from __future__ import annotations\n\timport json\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom typing import List, Optional, Union\n\timport openai\n\timport tiktoken\n\tfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\tfrom langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n\tfrom langchain.chat_models.base import BaseChatModel\n", "from langchain.schema import (\n\t    AIMessage,\n\t    HumanMessage,\n\t    SystemMessage,\n\t    messages_from_dict,\n\t    messages_to_dict,\n\t)\n\tMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\tlogger = logging.getLogger(__name__)\n\t@dataclass\n", "class TokenUsage:\n\t    step_name: str\n\t    in_step_prompt_tokens: int\n\t    in_step_completion_tokens: int\n\t    in_step_total_tokens: int\n\t    total_prompt_tokens: int\n\t    total_completion_tokens: int\n\t    total_tokens: int\n\tclass AI:\n\t    def __init__(self, model_name=\"gpt-4\", temperature=0.1, azure_endpoint=\"\"):\n", "        \"\"\"\n\t        Initialize the AI class.\n\t        Parameters\n\t        ----------\n\t        model_name : str, optional\n\t            The name of the model to use, by default \"gpt-4\".\n\t        temperature : float, optional\n\t            The temperature to use for the model, by default 0.1.\n\t        \"\"\"\n\t        self.temperature = temperature\n", "        self.azure_endpoint = azure_endpoint\n\t        self.model_name = (\n\t            fallback_model(model_name) if azure_endpoint == \"\" else model_name\n\t        )\n\t        self.llm = create_chat_model(self, self.model_name, self.temperature)\n\t        self.tokenizer = get_tokenizer(self.model_name)\n\t        logger.debug(f\"Using model {self.model_name} with llm {self.llm}\")\n\t        # initialize token usage log\n\t        self.cumulative_prompt_tokens = 0\n\t        self.cumulative_completion_tokens = 0\n", "        self.cumulative_total_tokens = 0\n\t        self.token_usage_log = []\n\t    def start(self, system: str, user: str, step_name: str) -> List[Message]:\n\t        \"\"\"\n\t        Start the conversation with a system message and a user message.\n\t        Parameters\n\t        ----------\n\t        system : str\n\t            The content of the system message.\n\t        user : str\n", "            The content of the user message.\n\t        step_name : str\n\t            The name of the step.\n\t        Returns\n\t        -------\n\t        List[Message]\n\t            The list of messages in the conversation.\n\t        \"\"\"\n\t        messages: List[Message] = [\n\t            SystemMessage(content=system),\n", "            HumanMessage(content=user),\n\t        ]\n\t        return self.next(messages, step_name=step_name)\n\t    def fsystem(self, msg: str) -> SystemMessage:\n\t        \"\"\"\n\t        Create a system message.\n\t        Parameters\n\t        ----------\n\t        msg : str\n\t            The content of the message.\n", "        Returns\n\t        -------\n\t        SystemMessage\n\t            The created system message.\n\t        \"\"\"\n\t        return SystemMessage(content=msg)\n\t    def fuser(self, msg: str) -> HumanMessage:\n\t        \"\"\"\n\t        Create a user message.\n\t        Parameters\n", "        ----------\n\t        msg : str\n\t            The content of the message.\n\t        Returns\n\t        -------\n\t        HumanMessage\n\t            The created user message.\n\t        \"\"\"\n\t        return HumanMessage(content=msg)\n\t    def fassistant(self, msg: str) -> AIMessage:\n", "        \"\"\"\n\t        Create an AI message.\n\t        Parameters\n\t        ----------\n\t        msg : str\n\t            The content of the message.\n\t        Returns\n\t        -------\n\t        AIMessage\n\t            The created AI message.\n", "        \"\"\"\n\t        return AIMessage(content=msg)\n\t    def next(\n\t        self,\n\t        messages: List[Message],\n\t        prompt: Optional[str] = None,\n\t        *,\n\t        step_name: str,\n\t    ) -> List[Message]:\n\t        \"\"\"\n", "        Advances the conversation by sending message history\n\t        to LLM and updating with the response.\n\t        Parameters\n\t        ----------\n\t        messages : List[Message]\n\t            The list of messages in the conversation.\n\t        prompt : Optional[str], optional\n\t            The prompt to use, by default None.\n\t        step_name : str\n\t            The name of the step.\n", "        Returns\n\t        -------\n\t        List[Message]\n\t            The updated list of messages in the conversation.\n\t        \"\"\"\n\t        \"\"\"\n\t        Advances the conversation by sending message history\n\t        to LLM and updating with the response.\n\t        \"\"\"\n\t        if prompt:\n", "            messages.append(self.fuser(prompt))\n\t        logger.debug(f\"Creating a new chat completion: {messages}\")\n\t        callsbacks = [StreamingStdOutCallbackHandler()]\n\t        response = self.llm(messages, callbacks=callsbacks)  # type: ignore\n\t        messages.append(response)\n\t        logger.debug(f\"Chat completion finished: {messages}\")\n\t        self.update_token_usage_log(\n\t            messages=messages, answer=response.content, step_name=step_name\n\t        )\n\t        return messages\n", "    @staticmethod\n\t    def serialize_messages(messages: List[Message]) -> str:\n\t        \"\"\"\n\t        Serialize a list of messages to a JSON string.\n\t        Parameters\n\t        ----------\n\t        messages : List[Message]\n\t            The list of messages to serialize.\n\t        Returns\n\t        -------\n", "        str\n\t            The serialized messages as a JSON string.\n\t        \"\"\"\n\t        return json.dumps(messages_to_dict(messages))\n\t    @staticmethod\n\t    def deserialize_messages(jsondictstr: str) -> List[Message]:\n\t        \"\"\"\n\t        Deserialize a JSON string to a list of messages.\n\t        Parameters\n\t        ----------\n", "        jsondictstr : str\n\t            The JSON string to deserialize.\n\t        Returns\n\t        -------\n\t        List[Message]\n\t            The deserialized list of messages.\n\t        \"\"\"\n\t        return list(messages_from_dict(json.loads(jsondictstr)))  # type: ignore\n\t    def update_token_usage_log(\n\t        self, messages: List[Message], answer: str, step_name: str\n", "    ) -> None:\n\t        \"\"\"\n\t        Update the token usage log with the number of tokens used in the current step.\n\t        Parameters\n\t        ----------\n\t        messages : List[Message]\n\t            The list of messages in the conversation.\n\t        answer : str\n\t            The answer from the AI.\n\t        step_name : str\n", "            The name of the step.\n\t        \"\"\"\n\t        prompt_tokens = self.num_tokens_from_messages(messages)\n\t        completion_tokens = self.num_tokens(answer)\n\t        total_tokens = prompt_tokens + completion_tokens\n\t        self.cumulative_prompt_tokens += prompt_tokens\n\t        self.cumulative_completion_tokens += completion_tokens\n\t        self.cumulative_total_tokens += total_tokens\n\t        self.token_usage_log.append(\n\t            TokenUsage(\n", "                step_name=step_name,\n\t                in_step_prompt_tokens=prompt_tokens,\n\t                in_step_completion_tokens=completion_tokens,\n\t                in_step_total_tokens=total_tokens,\n\t                total_prompt_tokens=self.cumulative_prompt_tokens,\n\t                total_completion_tokens=self.cumulative_completion_tokens,\n\t                total_tokens=self.cumulative_total_tokens,\n\t            )\n\t        )\n\t    def format_token_usage_log(self) -> str:\n", "        \"\"\"\n\t        Format the token usage log as a CSV string.\n\t        Returns\n\t        -------\n\t        str\n\t            The token usage log formatted as a CSV string.\n\t        \"\"\"\n\t        result = \"step_name,\"\n\t        result += \"prompt_tokens_in_step,completion_tokens_in_step,total_tokens_in_step\"\n\t        result += \",total_prompt_tokens,total_completion_tokens,total_tokens\\n\"\n", "        for log in self.token_usage_log:\n\t            result += log.step_name + \",\"\n\t            result += str(log.in_step_prompt_tokens) + \",\"\n\t            result += str(log.in_step_completion_tokens) + \",\"\n\t            result += str(log.in_step_total_tokens) + \",\"\n\t            result += str(log.total_prompt_tokens) + \",\"\n\t            result += str(log.total_completion_tokens) + \",\"\n\t            result += str(log.total_tokens) + \"\\n\"\n\t        return result\n\t    def num_tokens(self, txt: str) -> int:\n", "        \"\"\"\n\t        Get the number of tokens in a text.\n\t        Parameters\n\t        ----------\n\t        txt : str\n\t            The text to count the tokens in.\n\t        Returns\n\t        -------\n\t        int\n\t            The number of tokens in the text.\n", "        \"\"\"\n\t        return len(self.tokenizer.encode(txt))\n\t    def num_tokens_from_messages(self, messages: List[Message]) -> int:\n\t        \"\"\"\n\t        Get the total number of tokens used by a list of messages.\n\t        Parameters\n\t        ----------\n\t        messages : List[Message]\n\t            The list of messages to count the tokens in.\n\t        Returns\n", "        -------\n\t        int\n\t            The total number of tokens used by the messages.\n\t        \"\"\"\n\t        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n\t        n_tokens = 0\n\t        for message in messages:\n\t            n_tokens += (\n\t                4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n\t            )\n", "            n_tokens += self.num_tokens(message.content)\n\t        n_tokens += 2  # every reply is primed with <im_start>assistant\n\t        return n_tokens\n\tdef fallback_model(model: str) -> str:\n\t    \"\"\"\n\t    Retrieve the specified model, or fallback to \"gpt-3.5-turbo\" if the model is not available.\n\t    Parameters\n\t    ----------\n\t    model : str\n\t        The name of the model to retrieve.\n", "    Returns\n\t    -------\n\t    str\n\t        The name of the retrieved model, or \"gpt-3.5-turbo\" if the specified model is not available.\n\t    \"\"\"\n\t    try:\n\t        openai.Model.retrieve(model)\n\t        return model\n\t    except openai.InvalidRequestError:\n\t        print(\n", "            f\"Model {model} not available for provided API key. Reverting \"\n\t            \"to gpt-3.5-turbo. Sign up for the GPT-4 wait list here: \"\n\t            \"https://openai.com/waitlist/gpt-4-api\\n\"\n\t        )\n\t        return \"gpt-3.5-turbo\"\n\tdef create_chat_model(self, model: str, temperature) -> BaseChatModel:\n\t    \"\"\"\n\t    Create a chat model with the specified model name and temperature.\n\t    Parameters\n\t    ----------\n", "    model : str\n\t        The name of the model to create.\n\t    temperature : float\n\t        The temperature to use for the model.\n\t    Returns\n\t    -------\n\t    BaseChatModel\n\t        The created chat model.\n\t    \"\"\"\n\t    if self.azure_endpoint:\n", "        return AzureChatOpenAI(\n\t            openai_api_base=self.azure_endpoint,\n\t            openai_api_version=\"2023-05-15\",  # might need to be flexible in the future\n\t            deployment_name=model,\n\t            openai_api_type=\"azure\",\n\t            streaming=True,\n\t        )\n\t    # Fetch available models from OpenAI API\n\t    supported = [model[\"id\"] for model in openai.Model.list()[\"data\"]]\n\t    if model not in supported:\n", "        raise ValueError(\n\t            f\"Model {model} is not supported, supported models are: {supported}\"\n\t        )\n\t    return ChatOpenAI(\n\t        model=model,\n\t        temperature=temperature,\n\t        streaming=True,\n\t        client=openai.ChatCompletion,\n\t    )\n\tdef get_tokenizer(model: str):\n", "    \"\"\"\n\t    Get the tokenizer for the specified model.\n\t    Parameters\n\t    ----------\n\t    model : str\n\t        The name of the model to get the tokenizer for.\n\t    Returns\n\t    -------\n\t    Tokenizer\n\t        The tokenizer for the specified model.\n", "    \"\"\"\n\t    if \"gpt-4\" in model or \"gpt-3.5\" in model:\n\t        return tiktoken.encoding_for_model(model)\n\t    logger.debug(\n\t        f\"No encoder implemented for model {model}.\"\n\t        \"Defaulting to tiktoken cl100k_base encoder.\"\n\t        \"Use results only as estimates.\"\n\t    )\n\t    return tiktoken.get_encoding(\"cl100k_base\")\n\tdef serialize_messages(messages: List[Message]) -> str:\n", "    return AI.serialize_messages(messages)\n"]}
{"filename": "docs/conf.py", "chunked_list": ["#!/usr/bin/env python\n\t#\n\t# file_processor documentation build configuration file, created by\n\t# sphinx-quickstart on Fri Jun  9 13:47:02 2017.\n\t#\n\t# This file is execfile()d with the current directory set to its\n\t# containing dir.\n\t#\n\t# Note that not all possible configuration values are present in this\n\t# autogenerated file.\n", "#\n\t# All configuration values have a default; values that are commented out\n\t# serve to show the default.\n\t# If extensions (or modules to document with autodoc) are in another\n\t# directory, add these directories to sys.path here. If the directory is\n\t# relative to the documentation root, use os.path.abspath to make it\n\t# absolute, like shown here.\n\t#\n\timport os\n\timport sys\n", "from pathlib import Path\n\timport toml\n\tsys.path.insert(0, os.path.abspath(\"..\"))\n\tROOT_DIR = Path(__file__).parents[1].absolute()\n\twith open(\"../pyproject.toml\") as f:\n\t    data = toml.load(f)\n\t# The master toctree document.\n\tmaster_doc = \"index\"\n\t# General information about the project.\n\tproject = data[\"project\"][\"name\"]\n", "copyright = \"2023 Anton Osika\"\n\tauthor = \" Anton Osika & Contributors\"\n\t# The version info for the project you're documenting, acts as replacement\n\t# for |version| and |release|, also used in various other places throughout\n\t# the built documents.\n\t#\n\t# The short X.Y version.\n\tversion = data[\"project\"][\"version\"]\n\t# The full version, including alpha/beta/rc tags.\n\trelease = data[\"project\"][\"version\"]\n", "# -- General configuration ---------------------------------------------\n\t# If your documentation needs a minimal Sphinx version, state it here.\n\t#\n\t# needs_sphinx = '1.0'\n\t# Add any Sphinx extension module names here, as strings. They can be\n\t# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n\textensions = [\n\t    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.autodoc.typehints\",\n\t    \"sphinx.ext.autosummary\",\n", "    \"sphinx.ext.napoleon\",\n\t    \"sphinx.ext.viewcode\",\n\t    \"sphinx_copybutton\",\n\t    \"sphinx_panels\",\n\t    \"myst_parser\",\n\t    \"IPython.sphinxext.ipython_console_highlighting\",\n\t]\n\t# The suffix(es) of source filenames.\n\t# You can specify multiple suffix as a list of string:\n\tsource_suffix = [\".rst\", \".md\"]\n", "autodoc_pydantic_model_show_json = False\n\tautodoc_pydantic_field_list_validators = False\n\tautodoc_pydantic_config_members = False\n\tautodoc_pydantic_model_show_config_summary = False\n\tautodoc_pydantic_model_show_validator_members = False\n\tautodoc_pydantic_model_show_validator_summary = False\n\tautodoc_pydantic_model_signature_prefix = \"class\"\n\tautodoc_pydantic_field_signature_prefix = \"param\"\n\tautodoc_member_order = \"groupwise\"\n\tautoclass_content = \"both\"\n", "autodoc_typehints_format = \"short\"\n\tautodoc_default_options = {\n\t    \"members\": True,\n\t    \"show-inheritance\": True,\n\t    \"inherited-members\": \"BaseModel\",\n\t    \"undoc-members\": True,\n\t    \"special-members\": \"__call__\",\n\t}\n\t# Add any paths that contain templates here, relative to this directory.\n\ttemplates_path = [\"_templates\"]\n", "# source_suffix = '.rst'\n\t# The language for content autogenerated by Sphinx. Refer to documentation\n\t# for a list of supported languages.\n\t#\n\t# This is also used if you do content translation via gettext catalogs.\n\t# Usually you set \"language\" from the command line for these cases.\n\tlanguage = None\n\t# List of patterns, relative to source directory, that match files and\n\t# directories to ignore when looking for source files.\n\t# This patterns also effect to html_static_path and html_extra_path\n", "exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\t# The name of the Pygments (syntax highlighting) style to use.\n\tpygments_style = \"sphinx\"\n\t# If true, `todo` and `todoList` produce output, else they produce nothing.\n\ttodo_include_todos = False\n\t# -- Options for HTML output -------------------------------------------\n\t# The theme to use for HTML and HTML Help pages.  See the documentation for\n\t# a list of builtin themes.\n\t#\n\t# html_theme = 'alabaster'\n", "html_theme = \"sphinx_rtd_theme\"\n\t# Theme options are theme-specific and customize the look and feel of a\n\t# theme further.  For a list of options available for each theme, see the\n\t# documentation.\n\t#\n\t# html_theme_options = {}\n\t# Add any paths that contain custom static files (such as style sheets) here,\n\t# relative to this directory. They are copied after the builtin static files,\n\t# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n\thtml_static_path = [\"_static\"]\n", "# -- Options for HTMLHelp output ---------------------------------------\n\t# Output file base name for HTML help builder.\n\thtmlhelp_basename = \"gpt_engineerdoc\"\n\t# -- Options for LaTeX output ------------------------------------------\n\tlatex_elements = {\n\t    # The paper size ('letterpaper' or 'a4paper').\n\t    #\n\t    # 'papersize': 'letterpaper',\n\t    # The font size ('10pt', '11pt' or '12pt').\n\t    #\n", "    # 'pointsize': '10pt',\n\t    # Additional stuff for the LaTeX preamble.\n\t    #\n\t    # 'preamble': '',\n\t    # Latex figure (float) alignment\n\t    #\n\t    # 'figure_align': 'htbp',\n\t}\n\t# Grouping the document tree into LaTeX files. List of tuples\n\t# (source start file, target name, title, author, documentclass\n", "# [howto, manual, or own class]).\n\tlatex_documents = [\n\t    (master_doc, \"gpt_engineer.tex\", \"GPT-ENgineer Documentation\", \"manual\"),\n\t]\n\t# -- Options for manual page output ------------------------------------\n\t# One entry per manual page. List of tuples\n\t# (source start file, name, description, authors, manual section).\n\tman_pages = [(master_doc, \"gpt_engineer\", \"GPT-Engineer Documentation\", [author], 1)]\n\t# -- Options for Texinfo output ----------------------------------------\n\t# Grouping the document tree into Texinfo files. List of tuples\n", "# (source start file, target name, title, author,\n\t#  dir menu entry, description, category)\n\ttexinfo_documents = [\n\t    (\n\t        master_doc,\n\t        \"gpt_engineer\",\n\t        \"GPT-Engineer Documentation\",\n\t        author,\n\t        \"gpt_engineer\",\n\t        \"One line description of project.\",\n", "        \"Miscellaneous\",\n\t    ),\n\t]\n\t# generate autosummary even if no references\n\tautosummary_generate = True\n\tmyst_enable_extensions = [\n\t    \"colon_fence\",\n\t]\n"]}
{"filename": "docs/create_api_rst.py", "chunked_list": ["\"\"\"Script for auto-generating api_reference.rst\"\"\"\n\timport glob\n\timport re\n\tfrom pathlib import Path\n\tROOT_DIR = Path(__file__).parents[1].absolute()\n\tprint(ROOT_DIR)\n\tPKG_DIR = ROOT_DIR / \"gpt_engineer\"\n\tWRITE_FILE = Path(__file__).parent / \"api_reference.rst\"\n\tdef load_members() -> dict:\n\t    members: dict = {}\n", "    for py in glob.glob(str(PKG_DIR) + \"/**/*.py\", recursive=True):\n\t        module = py[len(str(PKG_DIR)) + 1 :].replace(\".py\", \"\").replace(\"/\", \".\")\n\t        top_level = module.split(\".\")[0]\n\t        if top_level not in members:\n\t            members[top_level] = {\"classes\": [], \"functions\": []}\n\t        with open(py, \"r\") as f:\n\t            for line in f.readlines():\n\t                cls = re.findall(r\"^class ([^_].*)\\(\", line)\n\t                members[top_level][\"classes\"].extend([module + \".\" + c for c in cls])\n\t                func = re.findall(r\"^def ([^_].*)\\(\", line)\n", "                afunc = re.findall(r\"^async def ([^_].*)\\(\", line)\n\t                func_strings = [module + \".\" + f for f in func + afunc]\n\t                members[top_level][\"functions\"].extend(func_strings)\n\t    return members\n\tdef construct_doc(members: dict) -> str:\n\t    full_doc = \"\"\"\\\n\t.. _api_reference:\n\t=============\n\tAPI Reference\n\t=============\n", "\"\"\"\n\t    for module, _members in sorted(members.items(), key=lambda kv: kv[0]):\n\t        classes = _members[\"classes\"]\n\t        functions = _members[\"functions\"]\n\t        if not (classes or functions):\n\t            continue\n\t        module_title = module.replace(\"_\", \" \").title()\n\t        if module_title == \"Llms\":\n\t            module_title = \"LLMs\"\n\t        section = f\":mod:`gpt_engineer.{module}`: {module_title}\"\n", "        full_doc += f\"\"\"\\\n\t{section}\n\t{'=' * (len(section) + 1)}\n\t.. automodule:: gpt_engineer.{module}\n\t    :no-members:\n\t    :no-inherited-members:\n\t\"\"\"\n\t        if classes:\n\t            cstring = \"\\n    \".join(sorted(classes))\n\t            full_doc += f\"\"\"\\\n", "Classes\n\t--------------\n\t.. currentmodule:: gpt_engineer\n\t.. autosummary::\n\t    :toctree: {module}\n\t    :template: class.rst\n\t    {cstring}\n\t\"\"\"\n\t        if functions:\n\t            fstring = \"\\n    \".join(sorted(functions))\n", "            full_doc += f\"\"\"\\\n\tFunctions\n\t--------------\n\t.. currentmodule:: gpt_engineer\n\t.. autosummary::\n\t    :toctree: {module}\n\t    {fstring}\n\t\"\"\"\n\t    return full_doc\n\tdef main() -> None:\n", "    members = load_members()\n\t    full_doc = construct_doc(members)\n\t    with open(WRITE_FILE, \"w\") as f:\n\t        f.write(full_doc)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
