{"filename": "evaluation.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport gym\n\timport numpy as np\n\timport os\n\timport argparse\n\timport torch\n\timport json\n\timport sys\n\timport pybulletgym\n", "from stable_baselines3 import DDPG\n\tfrom stable_baselines3.common.noise import NormalActionNoise\n\tfrom stable_baselines3.common.monitor import Monitor\n\tfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecMonitor\n\tfrom stable_baselines3.td3.policies import TD3Policy\n\tfrom stable_baselines3 import SAC\n\tfrom stable_baselines3 import TD3\n\tfrom stable_baselines3.common.callbacks import EvalCallback\n\tfrom stable_baselines3.common.logger import configure\n\timport gym\n", "import numpy as np\n\timport os\n\timport argparse\n\timport torch\n\timport json\n\timport sys\n\timport pybulletgym\n\tfrom stable_baselines3 import DDPG\n\tfrom stable_baselines3.common.noise import NormalActionNoise\n\tfrom stable_baselines3.common.monitor import Monitor\n", "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecMonitor\n\tfrom stable_baselines3.td3.policies import TD3Policy\n\tfrom stable_baselines3 import SAC\n\tfrom stable_baselines3 import TD3\n\tfrom stable_baselines3.common.callbacks import EvalCallback\n\tfrom stable_baselines3.common.logger import configure\n\tfrom action_constrained_rl.env_wrapper import ConstraintEnvWrapper\n\tfrom action_constrained_rl.env_wrapper import MemorizeCenterEnvWrapper\n\tfrom action_constrained_rl.ddpg.projection_ddpg import ProjectionDDPG\n\tfrom action_constrained_rl.td3.projection_td3 import ProjectionTD3\n", "from action_constrained_rl.sac.projection_sac import ProjectionSAC\n\tfrom action_constrained_rl.ddpg.noise_insertion_ddpg import NoiseInsertionDDPG\n\tfrom action_constrained_rl.ddpg.logging_gradient import LoggingGradientDDPG\n\tfrom action_constrained_rl.ddpg.logging_gradient import DDPGWithOutputPenalty\n\tfrom action_constrained_rl.ddpg.nfwpo import NFWPO\n\tfrom action_constrained_rl.ddpg.ddpg_with_penalty import DDPGWithPenalty\n\tfrom action_constrained_rl.td3.td3_with_penalty import TD3WithPenalty\n\tfrom action_constrained_rl.td3.td3_output_penalty import TD3WithOutputPenalty\n\tfrom action_constrained_rl.td3.noise_insertion_td3 import NoiseInsertionTD3\n\tfrom action_constrained_rl.sac.logging_gradient import LoggingGradientSAC\n", "from action_constrained_rl.sac.logging_gradient import SACWithOutputPenalty\n\tfrom action_constrained_rl.sac.safe_sampling_sac import SafeSamplingSAC\n\tfrom action_constrained_rl.nn.opt_layer.opt_layer import OptLayer\n\tfrom action_constrained_rl.nn.opt_layer.opt_layer_policy import OptLayerPolicy\n\tfrom action_constrained_rl.nn.additional_layers.alpha_projection import AlphaProjectionLayer\n\tfrom action_constrained_rl.nn.additional_layers.radial_squash import SquashLayer\n\tfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaGaussianDistribution\n\tfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaStateDependentNoiseDistribution\n\tfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedGaussianDistribution\n\tfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedStateDependentNoiseDistribution\n", "from action_constrained_rl.nn.additional_layer_policy import AdditionalLayerPolicy\n\tfrom action_constrained_rl.nn.additional_layer_sac_policy import AdditionalLayerSACPolicy\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tfrom action_constrained_rl.utils.arithmatic_series import ArithmaticSeries\n\tfrom action_constrained_rl.utils.geometric_series import GeometricSeries\n\tfrom action_constrained_rl.utils.log_series import LogSeries\n\tfrom action_constrained_rl.constraint.box_constraint import BoxConstraint\n\tfrom action_constrained_rl.constraint.power_constraint import PowerConstraint\n\tfrom action_constrained_rl.constraint.power_constraint import OrthoplexConstraint\n\tfrom action_constrained_rl.constraint.power_constraint import DecelerationConstraint\n", "from action_constrained_rl.constraint.sphere_constraint import SphericalConstraint\n\tfrom action_constrained_rl.constraint.tip_constraint import TipConstraint\n\tfrom action_constrained_rl.constraint.MA_constraint import MAConstraint\n\tfrom action_constrained_rl.constraint.combined_constraint import CombinedConstraint\n\tfrom action_constrained_rl.constraint.sin2_constraint import Sin2Constraint\n\timport gurobipy as gp\n\tgp.setParam('OutputFlag', 0)\n\tdef nameToConstraint(args):\n\t    name = args.env\n\t    c_name = args.constraint\n", "    if c_name == \"Box\" or c_name == \"Power\" or c_name == \"Orthoplex\" or c_name == \"Deceleration\" or c_name == \"Sphere\":\n\t        if name == \"Hopper-v3\":\n\t            offset = 8\n\t            scale = (1, 1, 1)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 11\n\t        elif name == \"ReacherPyBulletEnv-v0\":\n\t            offset = 6\n\t            scale = (1, 1)\n\t            indices = [6, 8]\n", "            s_dim = 9\n\t        elif name == 'Ant-v3':\n\t            offset = 19\n\t            scale = (1.,1.,1.,1.,1.,1.,1.,1.)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 27\n\t        elif name == 'HalfCheetah-v3':\n\t            offset = 11\n\t            scale = (1., 1., 1., 1., 1., 1.)\n\t            indices = list(range(offset, offset+len(scale)))\n", "            s_dim = 17\n\t        elif name == 'Swimmer-v3':\n\t            offset = 6\n\t            scale = (1.,1.)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 8\n\t        elif name == 'Walker2d-v3':\n\t            offset = 11\n\t            scale = (1., 1., 1., 1., 1., 1.)\n\t            indices = list(range(offset, offset+len(scale)))\n", "            s_dim = 17\n\t        if c_name == \"Box\":\n\t            return BoxConstraint(len(scale)) # R+N\n\t        elif c_name == \"Power\":\n\t            return PowerConstraint(indices, scale, args.max_power, s_dim) # R+M, H+M, W+M\n\t        elif c_name == \"Orthoplex\":\n\t            return OrthoplexConstraint(indices, scale, args.max_power, s_dim) # R+O03, R+O10, R+O30\n\t        elif c_name == \"Deceleration\":\n\t            return DecelerationConstraint(indices, scale, args.max_power, s_dim) #unused\n\t        elif c_name == \"Sphere\":\n", "            return SphericalConstraint(len(scale), args.max_power) #R+L2\n\t    elif c_name == 'Tip':\n\t        return TipConstraint(args.max_power)# R+T\n\t    elif c_name == 'MA':\n\t        return MAConstraint(args.max_power) # HC+MA\n\t    elif c_name == 'O+S':\n\t        if name == \"Hopper-v3\":\n\t            offset_p = 2\n\t            scale = (1, 1, 1)\n\t            indices_p = list(range(offset_p, offset_p+len(scale)))\n", "            offset_v = 8\n\t            indices_v = list(range(offset_v, offset_v+len(scale)))\n\t            s_dim = 11\n\t        elif name == 'Walker2d-v3':\n\t            offset_p = 2\n\t            scale = (1., 1., 1., 1., 1., 1.)\n\t            indices_p = list(range(offset_p, offset_p+len(scale)))\n\t            offset_v = 11\n\t            indices_v = list(range(offset_v, offset_v+len(scale)))\n\t            s_dim = 17\n", "        return CombinedConstraint(OrthoplexConstraint(indices_v, scale, args.max_power[0], s_dim),\n\t                                  Sin2Constraint(indices_p, args.max_power[1], s_dim)) # H+O+S, W+O+S\n\t    else:\n\t        raise\n\tdef nameToEnv(name, seed=0):\n\t    env = gym.make(name)\n\t    env.seed(seed)\n\t    return env\n\t# unused\n\tdef pickConstraintCoefficient(args):\n", "    if args.d > 0.0:\n\t        constraint_penalty = ArithmaticSeries(args.a0, args.d)\n\t    elif args.r > 0.0:\n\t        constraint_penalty = GeometricSeries(args.a0, args.r)\n\t    elif args.use_log_series:\n\t        constraint_penalty = LogSeries(args.a0)\n\t    else:\n\t        constraint_penalty = ConstantFunction(args.c)\n\t    return constraint_penalty\n\tparser = argparse.ArgumentParser()\n", "parser.add_argument(\"--log_dir\", action=\"store\", default=\"tmp\")\n\tparser.add_argument(\"--prob_id\", action=\"store\", default = \"\");\n\tparser.add_argument(\"--algo_id\", action=\"store\", default = \"\");\n\tparser.add_argument(\"--env\", action=\"store\", default=\"HalfCheetahDynamic\")\n\tparser.add_argument(\"--constraint\", action=\"store\", default=\"Normal\")\n\tparser.add_argument(\"--max_power\", action=\"store\", default=1., type=float)\n\tparser.add_argument(\"--solver\", action=\"store\", default=\"TD3\", choices=[\"DDPG\", \"SAC\", \"TD3\"])\n\tparser.add_argument(\"--num_time_steps\", action=\"store\", default=1e6, type=int)\n\tparser.add_argument(\"--batch_size\", action=\"store\", default=100, type=int)\n\tparser.add_argument(\"--use_env_wrapper\", action=\"store_true\", default=False, help=\"use projection inside environments\")\n", "parser.add_argument(\"--use_action_restriction\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--sigma\", action=\"store\", default=0.01, type=float, help=\"stddev for Gaussian Action Noise\")\n\tparser.add_argument(\"--a0\", action=\"store\", default=0.001, type=float)\n\tparser.add_argument(\"--dual_learning_rate\", action=\"store\", default=0.0, type=float)\n\tparser.add_argument(\"--learning_rate\", action=\"store\", default=1e-3, type=float)\n\tparser.add_argument(\"--init_ent_coef\", action=\"store\", default=2.0, type=float)\n\tparser.add_argument(\"--n\", action=\"store\", default=1, type=int, help=\"Update the penalty coefficient c every n episodes\")\n\tparser.add_argument(\"--verbose\", action=\"store\", default=1, type=int)\n\tparser.add_argument(\"--seed\", action=\"store\", default=0, type=int)\n\tparser.add_argument(\"--eval_freq\", action=\"store\", default=5000, type=int, help=\"run evaluation episodes every eval_freq time steps\")\n", "parser.add_argument(\"--n_eval_episodes\", action=\"store\", default=5, type=int)\n\tparser.add_argument(\"--normalize_constraint\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--device\", action=\"store\", default='auto')\n\tparser.add_argument(\"--squash_output\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--use_my_mlppolicy\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--infinity_action_space\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--use_NFWPO\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--fw_learning_rate\", action=\"store\", default=0.01, type=float)\n\tparser.add_argument(\"--logging_gradient\", action=\"store\", default=True, type=bool)\n\tparser.add_argument(\"--output_stdout\", action=\"store_true\", default=False)\n", "group = parser.add_mutually_exclusive_group()\n\tgroup.add_argument(\"--c\", action=\"store\", default=0.0, type=float, help=\"Constant penalty coefficient\")\n\tgroup.add_argument(\"--d\", action=\"store\", default=-1.0, type=float, help=\"add d to the penalty coefficient\")\n\tgroup.add_argument(\"--r\", action=\"store\", default=-1.0, type=float)\n\tgroup.add_argument(\"--use_log_series\", action=\"store_true\", default=False)\n\tgroup = parser.add_mutually_exclusive_group()\n\tgroup.add_argument(\"--use_static_constraint_net\", action=\"store_true\", default=False)\n\tgroup.add_argument(\"--use_opt_layer\", action=\"store_true\", default=False)\n\tgroup.add_argument(\"--use_alpha_projection_layer\", action=\"store_true\", default=False)\n\tgroup.add_argument(\"--use_squash_layer\", action=\"store_true\", default=False)\n", "parser.add_argument(\"--proj_type\", action=\"store\", default=\"QP\", choices=[\"QP\", \"alpha\", \"squash\"])\n\targs = parser.parse_args()\n\t# from problem id, set problem arguments\n\tif args.prob_id != \"\":\n\t    if args.prob_id == \"R+N\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Box\"\n\t    elif args.prob_id == \"R+L2\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Sphere\"\n", "        args.max_power = 0.05\n\t    elif args.prob_id == \"R+O03\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 0.3\n\t    elif args.prob_id == \"R+O10\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 1.0\n\t    elif args.prob_id == \"R+O30\":\n", "        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 3.0\n\t    elif args.prob_id == \"R+M\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Power\"\n\t        args.max_power = 1.0 \n\t    elif args.prob_id == \"R+T\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Tip\"\n", "        args.max_power = 0.05 \n\t    elif args.prob_id == \"HC+O\" or args.prob_id == \"HC+O-16\":\n\t        args.env = \"HalfCheetah-v3\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 20.\n\t    elif args.prob_id == \"H+M\" or args.prob_id == \"H+M-16\":\n\t        args.env = \"Hopper-v3\"\n\t        args.constraint = \"Power\"\n\t        args.max_power = 10.\n\t    elif args.prob_id == \"W+M\" or args.prob_id == \"W+M-16\":\n", "        args.env = \"Walker2d-v3\"\n\t        args.constraint = \"Power\"\n\t        args.max_power = 10.\n\t    elif args.prob_id == \"HC+MA\":\n\t        args.env = \"HalfCheetah-v3\"\n\t        args.constraint = \"MA\"\n\t        args.max_power = 5.\n\t    elif args.prob_id == \"H+O+S\":\n\t        args.env = \"Hopper-v3\"\n\t        args.constraint = \"O+S\"\n", "        args.max_power = (10., 0.1)\n\t    elif args.prob_id == \"W+O+S\":\n\t        args.env = \"Walker2d-v3\"\n\t        args.constraint = \"O+S\"\n\t        args.max_power = (10., 0.1)\n\t    else: raise ValueError(\"unknown problem id\")\n\t# from algorithm id, set algorithm arguments\n\tif args.algo_id != \"\":\n\t    if args.algo_id == \"DPro\":\n\t        args.use_action_restriction = True\n", "    elif args.algo_id == \"DPro+\":\n\t        args.use_action_restriction = True\n\t        args.c = 1.\n\t    elif args.algo_id == \"DPre\":\n\t        args.use_env_wrapper = True\n\t    elif args.algo_id == \"DPre+\":\n\t        args.use_env_wrapper = True\n\t        args.c = 1.\n\t    elif args.algo_id == \"DOpt\":\n\t        args.use_opt_layer = True\n", "        args.squash_output = True\n\t    elif args.algo_id == \"DOpt+\":\n\t        args.use_opt_layer = True\n\t        args.squash_output = True\n\t        args.c = 1.\n\t    elif args.algo_id == \"NFW\":\n\t        args.use_NFWPO = True\n\t    elif args.algo_id == \"DAlpha\":\n\t        args.use_alpha_projection_layer = True\n\t    elif args.algo_id == \"DRad\":\n", "        args.use_squash_layer = True\n\t    elif args.algo_id == \"SPre\":\n\t        args.use_env_wrapper = True\n\t        args.solver = \"SAC\"\n\t    elif args.algo_id == \"SPre+\":\n\t        args.use_env_wrapper = True\n\t        args.solver = \"SAC\"\n\t        args.c = 1.\n\t    elif args.algo_id == \"SAlpha\":\n\t        args.use_alpha_projection_layer = True\n", "        args.solver = \"SAC\"\n\t    elif args.algo_id == \"SRad\":\n\t        args.use_squash_layer = True\n\t        args.solver = \"SAC\"\n\t    else:\n\t        raise ValueError(\"unknown algo id\")\n\tif args.proj_type == \"squash\":\n\t    assert args.infinity_action_space\n\tif args.use_squash_layer:\n\t    assert not args.squash_output\n", "if args.use_opt_layer:\n\t    assert args.squash_output\n\tlog_dir = args.log_dir\n\tos.makedirs(log_dir, exist_ok=True)\n\tif not args.output_stdout:\n\t    sys.stdout = open(log_dir+\"/log.txt\", \"w\")\n\t    sys.stderr = open(log_dir+\"/error_log.txt\", \"w\")\n\tprint(args)\n\twith open(f'{log_dir}/commandline_args.txt', 'w') as f:\n\t    json.dump(args.__dict__, f, indent=2)\n", "env = nameToEnv(args.env, args.seed)\n\tconstraint = nameToConstraint(args)\n\tconstraint.proj_type = args.proj_type\n\tconstraint_penalty = pickConstraintCoefficient(args) # penalty coefficient function for output penalty\n\tif args.use_alpha_projection_layer or args.use_squash_layer: # wrapper to memorize the centers\n\t    EnvWrapper = MemorizeCenterEnvWrapper\n\t    env = EnvWrapper(constraint, env, n=args.n, dual_learning_rate=args.dual_learning_rate)\n\t    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/monitor.csv\")\n\t    eval_env = EnvWrapper(constraint, nameToEnv(args.env, args.seed), n=args.n, dual_learning_rate=args.dual_learning_rate)\n\t    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)\n", "else:  # wrapper to project actions. We do not use reward penalty\n\t    env = ConstraintEnvWrapper(constraint, env, constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=log_dir + \"/monitor.csv\", n=args.n, dual_learning_rate=args.dual_learning_rate, normalize=args.normalize_constraint, infinity_action_space = args.infinity_action_space)\n\t    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/vec_monitor.csv\")\n\t    eval_env = ConstraintEnvWrapper(constraint, nameToEnv(args.env, args.seed), constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=None, n=args.n, infinity_action_space = args.infinity_action_space)\n\t    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)\n\teval_callback = EvalCallback(eval_env, best_model_save_path=log_dir,\n\t                             log_path=log_dir, eval_freq=args.eval_freq, n_eval_episodes = args.n_eval_episodes,\n\t                         deterministic=True, render=False)\n\t# set rl-zoo hyperparameters\n\tn_actions = env.action_space.shape[-1]\n", "if args.env == \"ReacherPyBulletEnv-v0\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 3e5\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"gamma\": 0.98, \"buffer_size\": 200000, \"learning_starts\": 10000,\n\t                 \"action_noise\": action_noise, \"gradient_steps\": -1, \"train_freq\": (1, \"episode\"),\n\t                 \"learning_rate\": 1e-3, \"policy_kwargs\": {\"net_arch\":[400, 300]}}\n\t    elif args.solver == \"SAC\":\n\t        n_timesteps = 3e5\n\t        kargs = {\"learning_rate\": 7.3e-4, \"buffer_size\": 300000, \"batch_size\": 256,\n", "                 \"ent_coef\": 'auto', \"gamma\": 0.98, \"tau\": 0.02, \"train_freq\": 8,\n\t                 \"gradient_steps\": 8, \"learning_starts\": 10000,\n\t                 \"use_sde\": True, \"policy_kwargs\": dict(log_std_init=-3, net_arch=[400, 300])}\n\telif args.env == \"HalfCheetah-v3\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 1e6\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n\t    elif args.solver == \"SAC\":\n\t        n_timesteps = 1e6\n", "        kargs = {\"learning_starts\": 10000}\n\telif args.env == \"Hopper-v3\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 1e6\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise, \"train_freq\": 1,\n\t                 \"gradient_steps\": 1, \"learning_rate\": 3e-4, \"batch_size\": 256}\n\t    elif args.solver == \"SAC\":\n\t        n_timesteps = 1e6\n\t        kargs = {\"learning_starts\": 10000}\n", "elif args.env == \"Walker2d-v3\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 1e6\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n\t    elif args.solver == \"SAC\":\n\t        n_timesteps = 1e6\n\t        kargs = {\"learning_starts\": 10000}\n\telse:\n\t    raise\n", "kargs[\"verbose\"]=args.verbose\n\tif not \"policy_kwargs\" in kargs:\n\t    kargs[\"policy_kwargs\"]={}\n\tif args.prob_id[-3:] == \"-16\":\n\t    print(\"batch_size: 16\")\n\t    kargs.update({\"batch_size\": 16})\n\tdef pickModel(constraint):\n\t    # select model according to arguments\n\t    seed = args.seed\n\t    if args.use_NFWPO: #NFW\n", "        if args.prob_id[:2] == \"R+\":\n\t            fw_learning_rate = 0.05\n\t        else:\n\t            fw_learning_rate = 0.01\n\t        model = NFWPO(constraint, \"MlpPolicy\", env, fw_learning_rate = fw_learning_rate,\n\t                      device = args.device, seed = seed, **kargs)\n\t    elif args.use_action_restriction: #DPro, DPro+\n\t        if args.solver == \"DDPG\":\n\t            algo = ProjectionDDPG\n\t        elif args.solver == \"TD3\":\n", "            algo = ProjectionTD3\n\t        elif args.solver == \"SAC\":\n\t            algo = ProjectionSAC\n\t        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n\t    elif args.use_alpha_projection_layer or args.use_squash_layer: # DAlpha, DRad, SAlpha, SRad\n\t        kargs[\"policy_kwargs\"].update({\"constraint\": constraint})\n\t        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n\t            if args.solver == \"DDPG\":\n\t                algo = NoiseInsertionDDPG\n\t            else:\n", "                algo = NoiseInsertionTD3\n\t            policy = AdditionalLayerPolicy\n\t            if args.use_alpha_projection_layer:\n\t                layer_type = AlphaProjectionLayer\n\t            elif args.use_squash_layer:\n\t                layer_type = SquashLayer\n\t            kargs[\"policy_kwargs\"].update({\"layer_type\": layer_type, \"squash_output\": args.squash_output})\n\t        else:\n\t            algo = SafeSamplingSAC\n\t            action_noise = None\n", "            policy = AdditionalLayerSACPolicy\n\t            if args.use_alpha_projection_layer:\n\t                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n\t                    distribution_class = AlphaStateDependentNoiseDistribution\n\t                else:\n\t                    distribution_class = AlphaGaussianDistribution\n\t            if args.use_squash_layer:\n\t                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n\t                    distribution_class = ShrinkedStateDependentNoiseDistribution\n\t                else:\n", "                    distribution_class = ShrinkedGaussianDistribution\n\t            kargs[\"policy_kwargs\"].update({\"distribution_class\": distribution_class})\n\t        model = algo(policy, env, device = args.device, seed = seed, **kargs)\n\t    elif args.use_opt_layer: # DOpt, DOpt+\n\t        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n\t            if args.solver == \"DDPG\":\n\t                #algo = NoiseInsertionDDPG\n\t                algo = DDPGWithPenalty\n\t            else:\n\t                algo = TD3WithPenalty\n", "        else:\n\t            algo = SafeSamplingSAC\n\t            action_noise = None\n\t        kargs[\"policy_kwargs\"].update({\"constraint\": constraint, \"squash_output\": args.squash_output})\n\t        model = algo(constraint, OptLayerPolicy, env, use_center_wrapper = False, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n\t    else:\n\t        if args.solver == \"DDPG\":\n\t            algo = DDPGWithOutputPenalty\n\t        elif args.solver == \"TD3\":\n\t            algo = TD3WithOutputPenalty # DPre, DPre+\n", "        elif args.solver == \"SAC\":\n\t            algo = SACWithOutputPenalty # SPre, SPre+\n\t        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty,  device = args.device, seed = seed, **kargs)\n\t    return model\n\tfrom stable_baselines3.common.evaluation import evaluate_policy\n\tmodel = pickModel(constraint)\n\trewards = []\n\tfor seed in range(1,11):\n\t    ev = np.load(args.log_dir+'-'+str(seed)+\"/evaluations.npz\")\n\t    model.set_parameters(args.log_dir+'-'+str(seed)+\"/best_model.zip\")\n", "    rewards.append(evaluate_policy(model, env, n_eval_episodes = 50)[0])\n\trewards = np.array(rewards)\n\twith open(args.log_dir+\"/result.npy\", \"wb\") as result_file:\n\t    np.save(result_file, rewards)\n"]}
{"filename": "train.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport gym\n\timport numpy as np\n\timport os\n\timport argparse\n\timport torch\n\timport json\n\timport sys\n\timport pybulletgym\n", "from stable_baselines3 import DDPG\n\tfrom stable_baselines3.common.noise import NormalActionNoise\n\tfrom stable_baselines3.common.monitor import Monitor\n\tfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecMonitor\n\tfrom stable_baselines3.td3.policies import TD3Policy\n\tfrom stable_baselines3 import SAC\n\tfrom stable_baselines3 import TD3\n\tfrom stable_baselines3.common.callbacks import EvalCallback\n\tfrom stable_baselines3.common.logger import configure\n\tfrom action_constrained_rl.env_wrapper import ConstraintEnvWrapper\n", "from action_constrained_rl.env_wrapper import MemorizeCenterEnvWrapper\n\tfrom action_constrained_rl.ddpg.projection_ddpg import ProjectionDDPG\n\tfrom action_constrained_rl.td3.projection_td3 import ProjectionTD3\n\tfrom action_constrained_rl.sac.projection_sac import ProjectionSAC\n\tfrom action_constrained_rl.ddpg.noise_insertion_ddpg import NoiseInsertionDDPG\n\tfrom action_constrained_rl.ddpg.logging_gradient import LoggingGradientDDPG\n\tfrom action_constrained_rl.ddpg.logging_gradient import DDPGWithOutputPenalty\n\tfrom action_constrained_rl.ddpg.nfwpo import NFWPO\n\tfrom action_constrained_rl.ddpg.ddpg_with_penalty import DDPGWithPenalty\n\tfrom action_constrained_rl.td3.td3_with_penalty import TD3WithPenalty\n", "from action_constrained_rl.td3.td3_output_penalty import TD3WithOutputPenalty\n\tfrom action_constrained_rl.td3.noise_insertion_td3 import NoiseInsertionTD3\n\tfrom action_constrained_rl.sac.logging_gradient import LoggingGradientSAC\n\tfrom action_constrained_rl.sac.logging_gradient import SACWithOutputPenalty\n\tfrom action_constrained_rl.sac.safe_sampling_sac import SafeSamplingSAC\n\tfrom action_constrained_rl.nn.opt_layer.opt_layer import OptLayer\n\tfrom action_constrained_rl.nn.opt_layer.opt_layer_policy import OptLayerPolicy\n\tfrom action_constrained_rl.nn.additional_layers.alpha_projection import AlphaProjectionLayer\n\tfrom action_constrained_rl.nn.additional_layers.radial_squash import SquashLayer\n\tfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaGaussianDistribution\n", "from action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaStateDependentNoiseDistribution\n\tfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedGaussianDistribution\n\tfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedStateDependentNoiseDistribution\n\tfrom action_constrained_rl.nn.additional_layer_policy import AdditionalLayerPolicy\n\tfrom action_constrained_rl.nn.additional_layer_sac_policy import AdditionalLayerSACPolicy\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tfrom action_constrained_rl.utils.arithmatic_series import ArithmaticSeries\n\tfrom action_constrained_rl.utils.geometric_series import GeometricSeries\n\tfrom action_constrained_rl.utils.log_series import LogSeries\n\tfrom action_constrained_rl.constraint.box_constraint import BoxConstraint\n", "from action_constrained_rl.constraint.power_constraint import PowerConstraint\n\tfrom action_constrained_rl.constraint.power_constraint import OrthoplexConstraint\n\tfrom action_constrained_rl.constraint.power_constraint import DecelerationConstraint\n\tfrom action_constrained_rl.constraint.sphere_constraint import SphericalConstraint\n\tfrom action_constrained_rl.constraint.tip_constraint import TipConstraint\n\tfrom action_constrained_rl.constraint.MA_constraint import MAConstraint\n\tfrom action_constrained_rl.constraint.combined_constraint import CombinedConstraint\n\tfrom action_constrained_rl.constraint.sin2_constraint import Sin2Constraint\n\timport gurobipy as gp\n\tgp.setParam('OutputFlag', 0)\n", "def nameToConstraint(args):\n\t    name = args.env\n\t    c_name = args.constraint\n\t    if c_name == \"Box\" or c_name == \"Power\" or c_name == \"Orthoplex\" or c_name == \"Deceleration\" or c_name == \"Sphere\":\n\t        if name == \"Hopper-v3\":\n\t            offset = 8\n\t            scale = (1, 1, 1)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 11\n\t        elif name == \"ReacherPyBulletEnv-v0\":\n", "            offset = 6\n\t            scale = (1, 1)\n\t            indices = [6, 8]\n\t            s_dim = 9\n\t        elif name == 'Ant-v3':\n\t            offset = 19\n\t            scale = (1.,1.,1.,1.,1.,1.,1.,1.)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 27\n\t        elif name == 'HalfCheetah-v3':\n", "            offset = 11\n\t            scale = (1., 1., 1., 1., 1., 1.)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 17\n\t        elif name == 'Swimmer-v3':\n\t            offset = 6\n\t            scale = (1.,1.)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 8\n\t        elif name == 'Walker2d-v3':\n", "            offset = 11\n\t            scale = (1., 1., 1., 1., 1., 1.)\n\t            indices = list(range(offset, offset+len(scale)))\n\t            s_dim = 17\n\t        if c_name == \"Box\":\n\t            return BoxConstraint(len(scale)) # R+N\n\t        elif c_name == \"Power\":\n\t            return PowerConstraint(indices, scale, args.max_power, s_dim) # R+M, H+M, W+M\n\t        elif c_name == \"Orthoplex\":\n\t            return OrthoplexConstraint(indices, scale, args.max_power, s_dim) # R+O03, R+O10, R+O30\n", "        elif c_name == \"Deceleration\":\n\t            return DecelerationConstraint(indices, scale, args.max_power, s_dim) #unused\n\t        elif c_name == \"Sphere\":\n\t            return SphericalConstraint(len(scale), args.max_power) #R+L2\n\t    elif c_name == 'Tip':\n\t        return TipConstraint(args.max_power)# R+T\n\t    elif c_name == 'MA':\n\t        return MAConstraint(args.max_power) # HC+MA\n\t    elif c_name == 'O+S':\n\t        if name == \"Hopper-v3\":\n", "            offset_p = 2\n\t            scale = (1, 1, 1)\n\t            indices_p = list(range(offset_p, offset_p+len(scale)))\n\t            offset_v = 8\n\t            indices_v = list(range(offset_v, offset_v+len(scale)))\n\t            s_dim = 11\n\t        elif name == 'Walker2d-v3':\n\t            offset_p = 2\n\t            scale = (1., 1., 1., 1., 1., 1.)\n\t            indices_p = list(range(offset_p, offset_p+len(scale)))\n", "            offset_v = 11\n\t            indices_v = list(range(offset_v, offset_v+len(scale)))\n\t            s_dim = 17\n\t        return CombinedConstraint(OrthoplexConstraint(indices_v, scale, args.max_power[0], s_dim),\n\t                                  Sin2Constraint(indices_p, args.max_power[1], s_dim)) # H+O+S, W+O+S\n\t    else:\n\t        raise\n\tdef nameToEnv(name, seed=0):\n\t    env = gym.make(name)\n\t    env.seed(seed)\n", "    return env\n\t# unused\n\tdef pickConstraintCoefficient(args):\n\t    if args.d > 0.0:\n\t        constraint_penalty = ArithmaticSeries(args.a0, args.d)\n\t    elif args.r > 0.0:\n\t        constraint_penalty = GeometricSeries(args.a0, args.r)\n\t    elif args.use_log_series:\n\t        constraint_penalty = LogSeries(args.a0)\n\t    else:\n", "        constraint_penalty = ConstantFunction(args.c)\n\t    return constraint_penalty\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--log_dir\", action=\"store\", default=\"tmp\")\n\tparser.add_argument(\"--prob_id\", action=\"store\", default = \"\");\n\tparser.add_argument(\"--algo_id\", action=\"store\", default = \"\");\n\tparser.add_argument(\"--env\", action=\"store\", default=\"HalfCheetahDynamic\")\n\tparser.add_argument(\"--constraint\", action=\"store\", default=\"Normal\")\n\tparser.add_argument(\"--max_power\", action=\"store\", default=1., type=float)\n\tparser.add_argument(\"--solver\", action=\"store\", default=\"TD3\", choices=[\"DDPG\", \"SAC\", \"TD3\"])\n", "parser.add_argument(\"--num_time_steps\", action=\"store\", default=1e6, type=int)\n\tparser.add_argument(\"--batch_size\", action=\"store\", default=100, type=int)\n\tparser.add_argument(\"--use_env_wrapper\", action=\"store_true\", default=False, help=\"use projection inside environments\")\n\tparser.add_argument(\"--use_action_restriction\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--sigma\", action=\"store\", default=0.01, type=float, help=\"stddev for Gaussian Action Noise\")\n\tparser.add_argument(\"--a0\", action=\"store\", default=0.001, type=float)\n\tparser.add_argument(\"--dual_learning_rate\", action=\"store\", default=0.0, type=float)\n\tparser.add_argument(\"--learning_rate\", action=\"store\", default=1e-3, type=float)\n\tparser.add_argument(\"--init_ent_coef\", action=\"store\", default=2.0, type=float)\n\tparser.add_argument(\"--n\", action=\"store\", default=1, type=int, help=\"Update the penalty coefficient c every n episodes\")\n", "parser.add_argument(\"--verbose\", action=\"store\", default=1, type=int)\n\tparser.add_argument(\"--seed\", action=\"store\", default=0, type=int)\n\tparser.add_argument(\"--eval_freq\", action=\"store\", default=5000, type=int, help=\"run evaluation episodes every eval_freq time steps\")\n\tparser.add_argument(\"--n_eval_episodes\", action=\"store\", default=5, type=int)\n\tparser.add_argument(\"--normalize_constraint\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--device\", action=\"store\", default='auto')\n\tparser.add_argument(\"--squash_output\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--use_my_mlppolicy\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--infinity_action_space\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--use_NFWPO\", action=\"store_true\", default=False)\n", "parser.add_argument(\"--fw_learning_rate\", action=\"store\", default=0.01, type=float)\n\tparser.add_argument(\"--logging_gradient\", action=\"store\", default=True, type=bool)\n\tparser.add_argument(\"--output_stdout\", action=\"store_true\", default=False)\n\tgroup = parser.add_mutually_exclusive_group()\n\tgroup.add_argument(\"--c\", action=\"store\", default=0.0, type=float, help=\"Constant penalty coefficient\")\n\tgroup.add_argument(\"--d\", action=\"store\", default=-1.0, type=float, help=\"add d to the penalty coefficient\")\n\tgroup.add_argument(\"--r\", action=\"store\", default=-1.0, type=float)\n\tgroup.add_argument(\"--use_log_series\", action=\"store_true\", default=False)\n\tgroup = parser.add_mutually_exclusive_group()\n\tgroup.add_argument(\"--use_static_constraint_net\", action=\"store_true\", default=False)\n", "group.add_argument(\"--use_opt_layer\", action=\"store_true\", default=False)\n\tgroup.add_argument(\"--use_alpha_projection_layer\", action=\"store_true\", default=False)\n\tgroup.add_argument(\"--use_squash_layer\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--proj_type\", action=\"store\", default=\"QP\", choices=[\"QP\", \"alpha\", \"squash\"])\n\targs = parser.parse_args()\n\t# from problem id, set problem arguments\n\tif args.prob_id != \"\":\n\t    if args.prob_id == \"R+N\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Box\"\n", "    elif args.prob_id == \"R+L2\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Sphere\"\n\t        args.max_power = 0.05\n\t    elif args.prob_id == \"R+O03\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 0.3\n\t    elif args.prob_id == \"R+O10\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n", "        args.constraint = \"Orthoplex\"\n\t        args.max_power = 1.0\n\t    elif args.prob_id == \"R+O30\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 3.0\n\t    elif args.prob_id == \"R+M\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Power\"\n\t        args.max_power = 1.0 \n", "    elif args.prob_id == \"R+T\":\n\t        args.env = \"ReacherPyBulletEnv-v0\"\n\t        args.constraint = \"Tip\"\n\t        args.max_power = 0.05 \n\t    elif args.prob_id == \"HC+O\" or args.prob_id == \"HC+O-16\":\n\t        args.env = \"HalfCheetah-v3\"\n\t        args.constraint = \"Orthoplex\"\n\t        args.max_power = 20.\n\t    elif args.prob_id == \"H+M\" or args.prob_id == \"H+M-16\":\n\t        args.env = \"Hopper-v3\"\n", "        args.constraint = \"Power\"\n\t        args.max_power = 10.\n\t    elif args.prob_id == \"W+M\" or args.prob_id == \"W+M-16\":\n\t        args.env = \"Walker2d-v3\"\n\t        args.constraint = \"Power\"\n\t        args.max_power = 10.\n\t    elif args.prob_id == \"HC+MA\":\n\t        args.env = \"HalfCheetah-v3\"\n\t        args.constraint = \"MA\"\n\t        args.max_power = 5.\n", "    elif args.prob_id == \"H+O+S\":\n\t        args.env = \"Hopper-v3\"\n\t        args.constraint = \"O+S\"\n\t        args.max_power = (10., 0.1)\n\t    elif args.prob_id == \"W+O+S\":\n\t        args.env = \"Walker2d-v3\"\n\t        args.constraint = \"O+S\"\n\t        args.max_power = (10., 0.1)\n\t    else: raise ValueError(\"unknown problem id\")\n\t# from algorithm id, set algorithm arguments\n", "if args.algo_id != \"\":\n\t    if args.algo_id == \"DPro\":\n\t        args.use_action_restriction = True\n\t    elif args.algo_id == \"DPro+\":\n\t        args.use_action_restriction = True\n\t        args.c = 1.\n\t    elif args.algo_id == \"DPre\":\n\t        args.use_env_wrapper = True\n\t    elif args.algo_id == \"DPre+\":\n\t        args.use_env_wrapper = True\n", "        args.c = 1.\n\t    elif args.algo_id == \"DOpt\":\n\t        args.use_opt_layer = True\n\t        args.squash_output = True\n\t    elif args.algo_id == \"DOpt+\":\n\t        args.use_opt_layer = True\n\t        args.squash_output = True\n\t        args.c = 1.\n\t    elif args.algo_id == \"NFW\":\n\t        args.use_NFWPO = True\n", "    elif args.algo_id == \"DAlpha\":\n\t        args.use_alpha_projection_layer = True\n\t    elif args.algo_id == \"DRad\":\n\t        args.use_squash_layer = True\n\t    elif args.algo_id == \"SPre\":\n\t        args.use_env_wrapper = True\n\t        args.solver = \"SAC\"\n\t    elif args.algo_id == \"SPre+\":\n\t        args.use_env_wrapper = True\n\t        args.solver = \"SAC\"\n", "        args.c = 1.\n\t    elif args.algo_id == \"SAlpha\":\n\t        args.use_alpha_projection_layer = True\n\t        args.solver = \"SAC\"\n\t    elif args.algo_id == \"SRad\":\n\t        args.use_squash_layer = True\n\t        args.solver = \"SAC\"\n\t    else:\n\t        raise ValueError(\"unknown algo id\")\n\tif args.proj_type == \"squash\":\n", "    assert args.infinity_action_space\n\tif args.use_squash_layer:\n\t    assert not args.squash_output\n\tif args.use_opt_layer:\n\t    assert args.squash_output\n\tlog_dir = args.log_dir\n\tos.makedirs(log_dir, exist_ok=True)\n\tif not args.output_stdout:\n\t    sys.stdout = open(log_dir+\"/log.txt\", \"w\")\n\t    sys.stderr = open(log_dir+\"/error_log.txt\", \"w\")\n", "print(args)\n\twith open(f'{log_dir}/commandline_args.txt', 'w') as f:\n\t    json.dump(args.__dict__, f, indent=2)\n\tenv = nameToEnv(args.env, args.seed)\n\tconstraint = nameToConstraint(args)\n\tconstraint.proj_type = args.proj_type\n\tconstraint_penalty = pickConstraintCoefficient(args) # penalty coefficient function for output penalty\n\tif args.use_alpha_projection_layer or args.use_squash_layer: # wrapper to memorize the centers\n\t    EnvWrapper = MemorizeCenterEnvWrapper\n\t    env = EnvWrapper(constraint, env, n=args.n, dual_learning_rate=args.dual_learning_rate)\n", "    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/monitor.csv\")\n\t    eval_env = EnvWrapper(constraint, nameToEnv(args.env, args.seed), n=args.n, dual_learning_rate=args.dual_learning_rate)\n\t    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)\n\telse:  # wrapper to project actions. We do not use reward penalty\n\t    env = ConstraintEnvWrapper(constraint, env, constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=log_dir + \"/monitor.csv\", n=args.n, dual_learning_rate=args.dual_learning_rate, normalize=args.normalize_constraint, infinity_action_space = args.infinity_action_space)\n\t    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/vec_monitor.csv\")\n\t    eval_env = ConstraintEnvWrapper(constraint, nameToEnv(args.env, args.seed), constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=None, n=args.n, infinity_action_space = args.infinity_action_space)\n\t    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)\n\teval_callback = EvalCallback(eval_env, best_model_save_path=log_dir,\n\t                             log_path=log_dir, eval_freq=args.eval_freq, n_eval_episodes = args.n_eval_episodes,\n", "                         deterministic=True, render=False)\n\t# set rl-zoo hyperparameters\n\tn_actions = env.action_space.shape[-1]\n\tif args.env == \"ReacherPyBulletEnv-v0\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 3e5\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"gamma\": 0.98, \"buffer_size\": 200000, \"learning_starts\": 10000,\n\t                 \"action_noise\": action_noise, \"gradient_steps\": -1, \"train_freq\": (1, \"episode\"),\n\t                 \"learning_rate\": 1e-3, \"policy_kwargs\": {\"net_arch\":[400, 300]}}\n", "    elif args.solver == \"SAC\":\n\t        n_timesteps = 3e5\n\t        kargs = {\"learning_rate\": 7.3e-4, \"buffer_size\": 300000, \"batch_size\": 256,\n\t                 \"ent_coef\": 'auto', \"gamma\": 0.98, \"tau\": 0.02, \"train_freq\": 8,\n\t                 \"gradient_steps\": 8, \"learning_starts\": 10000,\n\t                 \"use_sde\": True, \"policy_kwargs\": dict(log_std_init=-3, net_arch=[400, 300])}\n\telif args.env == \"HalfCheetah-v3\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 1e6\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n", "        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n\t    elif args.solver == \"SAC\":\n\t        n_timesteps = 1e6\n\t        kargs = {\"learning_starts\": 10000}\n\telif args.env == \"Hopper-v3\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 1e6\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise, \"train_freq\": 1,\n\t                 \"gradient_steps\": 1, \"learning_rate\": 3e-4, \"batch_size\": 256}\n", "    elif args.solver == \"SAC\":\n\t        n_timesteps = 1e6\n\t        kargs = {\"learning_starts\": 10000}\n\telif args.env == \"Walker2d-v3\":\n\t    if args.solver == \"TD3\" or args.use_NFWPO:\n\t        n_timesteps = 1e6\n\t        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n\t        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n\t    elif args.solver == \"SAC\":\n\t        n_timesteps = 1e6\n", "        kargs = {\"learning_starts\": 10000}\n\telse:\n\t    raise\n\tkargs[\"verbose\"]=args.verbose\n\tif not \"policy_kwargs\" in kargs:\n\t    kargs[\"policy_kwargs\"]={}\n\tif args.prob_id[-3:] == \"-16\":\n\t    print(\"batch_size: 16\")\n\t    kargs.update({\"batch_size\": 16})\n\tdef pickModel(constraint):\n", "    # select model according to arguments\n\t    seed = args.seed\n\t    if args.use_NFWPO: #NFW\n\t        if args.prob_id[:2] == \"R+\":\n\t            fw_learning_rate = 0.05\n\t        else:\n\t            fw_learning_rate = 0.01\n\t        model = NFWPO(constraint, \"MlpPolicy\", env, fw_learning_rate = fw_learning_rate,\n\t                      device = args.device, seed = seed, **kargs)\n\t    elif args.use_action_restriction: #DPro, DPro+\n", "        if args.solver == \"DDPG\":\n\t            algo = ProjectionDDPG\n\t        elif args.solver == \"TD3\":\n\t            algo = ProjectionTD3\n\t        elif args.solver == \"SAC\":\n\t            algo = ProjectionSAC\n\t        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n\t    elif args.use_alpha_projection_layer or args.use_squash_layer: # DAlpha, DRad, SAlpha, SRad\n\t        kargs[\"policy_kwargs\"].update({\"constraint\": constraint})\n\t        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n", "            if args.solver == \"DDPG\":\n\t                algo = NoiseInsertionDDPG\n\t            else:\n\t                algo = NoiseInsertionTD3\n\t            policy = AdditionalLayerPolicy\n\t            if args.use_alpha_projection_layer:\n\t                layer_type = AlphaProjectionLayer\n\t            elif args.use_squash_layer:\n\t                layer_type = SquashLayer\n\t            kargs[\"policy_kwargs\"].update({\"layer_type\": layer_type, \"squash_output\": args.squash_output})\n", "        else:\n\t            algo = SafeSamplingSAC\n\t            action_noise = None\n\t            policy = AdditionalLayerSACPolicy\n\t            if args.use_alpha_projection_layer:\n\t                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n\t                    distribution_class = AlphaStateDependentNoiseDistribution\n\t                else:\n\t                    distribution_class = AlphaGaussianDistribution\n\t            if args.use_squash_layer:\n", "                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n\t                    distribution_class = ShrinkedStateDependentNoiseDistribution\n\t                else:\n\t                    distribution_class = ShrinkedGaussianDistribution\n\t            kargs[\"policy_kwargs\"].update({\"distribution_class\": distribution_class})\n\t        model = algo(policy, env, device = args.device, seed = seed, **kargs)\n\t    elif args.use_opt_layer: # DOpt, DOpt+\n\t        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n\t            if args.solver == \"DDPG\":\n\t                #algo = NoiseInsertionDDPG\n", "                algo = DDPGWithPenalty\n\t            else:\n\t                algo = TD3WithPenalty\n\t        else:\n\t            algo = SafeSamplingSAC\n\t            action_noise = None\n\t        kargs[\"policy_kwargs\"].update({\"constraint\": constraint, \"squash_output\": args.squash_output})\n\t        model = algo(constraint, OptLayerPolicy, env, use_center_wrapper = False, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n\t    else:\n\t        if args.solver == \"DDPG\":\n", "            algo = DDPGWithOutputPenalty\n\t        elif args.solver == \"TD3\":\n\t            algo = TD3WithOutputPenalty # DPre, DPre+\n\t        elif args.solver == \"SAC\":\n\t            algo = SACWithOutputPenalty # SPre, SPre+\n\t        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty,  device = args.device, seed = seed, **kargs)\n\t    return model\n\tmodel = pickModel(constraint)\n\tif args.logging_gradient:\n\t    logger = configure(args.log_dir)\n", "    model.set_logger(logger)\n\tmodel.learn(total_timesteps=n_timesteps, callback=eval_callback)\n\tdel model # remove to demonstrate saving and loading\n"]}
{"filename": "action_constrained_rl/cvxpy_variables.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tclass CVXPYVariables:\n\t    \"\"\"\n\t    class to store cvxpy proplem data\n\t    \"\"\"\n\t    def __init__(self, x, q, s, cons, obj, prob):\n\t        self.x = x\n\t        self.q = q\n\t        self.s = s\n", "        self.cons = cons\n\t        self.obj = obj\n\t        self.prob = prob\n"]}
{"filename": "action_constrained_rl/utils/constant_function.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tclass ConstantFunction:\n\t    def __init__(self, x):\n\t        self.x = x\n\t    def __call__(self, t):\n\t        return self.x\n"]}
{"filename": "action_constrained_rl/utils/arithmatic_series.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tclass ArithmaticSeries:\n\t    def __init__(self, a0, d):\n\t        self.a0 = a0\n\t        self.d = d\n\t    def __call__(self, t):\n\t        assert(t > 0)\n\t        return self.a0 + (t-1) * self.d\n\tif __name__ == \"__main__\":\n", "    a0 = 0.001\n\t    d = 0.02\n\t    a = ArithmaticSeries(a0, d)\n\t    print(a(1))\n\t    print(a(2))\n\t    print(a(100))\n"]}
{"filename": "action_constrained_rl/utils/geometric_series.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tclass GeometricSeries:\n\t    def __init__(self, a0, gamma):\n\t        self.a0 = a0\n\t        self.gamma = gamma\n\t    def __call__(self, t):\n\t        assert(t > 0)\n\t        return self.a0 * self.gamma ** (t-1)\n\tif __name__ == \"__main__\":\n", "    a0 = 1.0\n\t    d = 2.0\n\t    a = GeometricSeries(a0, d)\n\t    print(a(1))\n\t    print(a(2))\n\t    print(a(10))\n"]}
{"filename": "action_constrained_rl/utils/log_series.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\tclass LogSeries:\n\t    def __init__(self, a0):\n\t        self.a0 = a0\n\t    def __call__(self, t):\n\t        assert(t > 0)\n\t        return self.a0 + np.log(t)\n\tif __name__ == \"__main__\":\n", "    a0 = 0.1\n\t    a = LogSeries(a0)\n\t    print(a(1))\n\t    print(a(2))\n\t    print(a(3))\n\t    print(a(100))\n\t    print(a(1000))\n"]}
{"filename": "action_constrained_rl/sac/projection_sac.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n", "from stable_baselines3 import SAC\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass ProjectionSAC(SAC):\n\t    \"\"\"\n\t    Unused\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.constraint = constraint\n\t        self.penalty_coeff = constraint_penalty\n", "    def _sample_action(\n\t        self,\n\t        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n\t    ) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"\n\t        Sample an action according to the exploration policy.\n\t        This is either done by sampling the probability distribution of the policy,\n\t        or sampling a random action (from a uniform distribution over the action space)\n", "        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n\t            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n\t        :param n_envs:\n\t        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n\t            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n\t        \"\"\"\n", "        # Select action randomly or according to policy\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n\t            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t        else:\n\t            # Note: when using continuous actions,\n\t            # we assume that the policy uses tanh to scale the action\n\t            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n\t            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\t        # Rescale the action from [low, high] to [-1, 1]\n", "        if isinstance(self.action_space, gym.spaces.Box):\n\t            obs = self._last_obs[-1]\n\t            #print(\"unscaled: {}\".format(unscaled_action))\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            #print(\"scaled: {}\".format(scaled_action))\n\t            # Add noise to the action (improve exploration)\n\t            if action_noise is not None:\n\t                scaled_action = scaled_action + action_noise()\n\t            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n\t            # We store the scaled action in the buffer\n", "            buffer_action = scaled_action\n\t            action = self.policy.unscale_action(scaled_action)\n\t        else:\n\t            # Discrete case, no need to normalize or clip\n\t            buffer_action = unscaled_action\n\t            action = buffer_action\n\t        return action, buffer_action\n\t    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n", "        # Update optimizers learning rate\n\t        optimizers = [self.actor.optimizer, self.critic.optimizer]\n\t        if self.ent_coef_optimizer is not None:\n\t            optimizers += [self.ent_coef_optimizer]\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate(optimizers)\n\t        ent_coef_losses, ent_coefs = [], []\n\t        actor_losses, critic_losses = [], []\n\t        for gradient_step in range(gradient_steps):\n\t            # Sample replay buffer\n", "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            # We need to sample because `log_std` may have changed between two gradient steps\n\t            if self.use_sde:\n\t                self.actor.reset_noise()\n\t            # Action by the current actor for the sampled state\n\t            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n\t            log_prob = log_prob.reshape(-1, 1)\n\t            ent_coef_loss = None\n\t            if self.ent_coef_optimizer is not None:\n\t                # Important: detach the variable from the graph\n", "                # so we don't change it with other losses\n\t                # see https://github.com/rail-berkeley/softlearning/issues/60\n\t                ent_coef = th.exp(self.log_ent_coef.detach())\n\t                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n\t                ent_coef_losses.append(ent_coef_loss.item())\n\t            else:\n\t                ent_coef = self.ent_coef_tensor\n\t            ent_coefs.append(ent_coef.item())\n\t            # Optimize entropy coefficient, also called\n\t            # entropy temperature or alpha in the paper\n", "            if ent_coef_loss is not None:\n\t                self.ent_coef_optimizer.zero_grad()\n\t                ent_coef_loss.backward()\n\t                self.ent_coef_optimizer.step()\n\t            with th.no_grad():\n\t                # Select action according to policy\n\t                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n\t                # Compute the next Q values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n", "                # add entropy term\n\t                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n\t                # td error + entropy term\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            # using action from the replay buffer\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n", "            # Optimize the critic\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Compute actor loss\n\t            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n\t            # Mean over all critic networks\n\t            actions_pi.retain_grad()\n\t            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n\t            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n", "            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\t            # calculate penaty\n\t            actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, actions_pi).mean()\n\t            actor_losses.append(actor_loss.item())\n\t            # Optimize the actor\n\t            self.actor.optimizer.zero_grad()\n\t            actor_loss.backward()\n\t            self.actor.optimizer.step()\n\t            # Update target networks\n\t            if gradient_step % self.target_update_interval == 0:\n", "                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t            # Logging gradients\n\t            for tag, value in self.actor.named_parameters():\n\t                if value.grad is not None:\n\t                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t            action_grad = actions_pi.grad.norm(dim=1).sum()\n\t            self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self._n_updates += gradient_steps\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n", "        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n\t        if len(ent_coef_losses) > 0:\n\t            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))\n"]}
{"filename": "action_constrained_rl/sac/safe_sampling_sac.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport math\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3 import SAC\n\tfrom stable_baselines3.common.preprocessing import get_action_dim\n\tfrom .logging_gradient import LoggingGradientSAC\n", "class SafeSamplingSAC(LoggingGradientSAC):\n\t    \"\"\"\n\t    modified SAC to project random sampled actions\n\t    \"\"\"\n\t    def _sample_action(\n\t        self,\n\t        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n\t    ) -> Tuple[np.ndarray, np.ndarray]:\n", "        \"\"\"\n\t        Sample an action according to the exploration policy.\n\t        This is either done by sampling the probability distribution of the policy,\n\t        or sampling a random action (from a uniform distribution over the action space)\n\t        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n\t            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n\t        :param n_envs:\n", "        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n\t            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n\t        \"\"\"\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n\t            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            action_dim = get_action_dim(self.action_space)\n\t            scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-action_dim], self._last_obs[i,-action_dim:], scaled_action[i]) for i in range(n_envs)])\n", "            # We store the scaled action in the buffer\n\t            buffer_action = scaled_action\n\t            action = self.policy.unscale_action(scaled_action)\n\t        else:\n\t            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            buffer_action = scaled_action\n\t            action = unscaled_action\n\t        return action, buffer_action\n"]}
{"filename": "action_constrained_rl/sac/logging_gradient.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n", "from stable_baselines3 import SAC\n\tclass LoggingGradientSAC(SAC):\n\t    \"\"\"\n\t    modified SAC to log gradient\n\t    \"\"\"\n\t    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update optimizers learning rate\n\t        optimizers = [self.actor.optimizer, self.critic.optimizer]\n", "        if self.ent_coef_optimizer is not None:\n\t            optimizers += [self.ent_coef_optimizer]\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate(optimizers)\n\t        ent_coef_losses, ent_coefs = [], []\n\t        actor_losses, critic_losses = [], []\n\t        for gradient_step in range(gradient_steps):\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            # We need to sample because `log_std` may have changed between two gradient steps\n", "            if self.use_sde:\n\t                self.actor.reset_noise()\n\t            # Action by the current actor for the sampled state\n\t            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n\t            log_prob = log_prob.reshape(-1, 1)\n\t            ent_coef_loss = None\n\t            if self.ent_coef_optimizer is not None:\n\t                # Important: detach the variable from the graph\n\t                # so we don't change it with other losses\n\t                # see https://github.com/rail-berkeley/softlearning/issues/60\n", "                ent_coef = th.exp(self.log_ent_coef.detach())\n\t                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n\t                ent_coef_losses.append(ent_coef_loss.item())\n\t            else:\n\t                ent_coef = self.ent_coef_tensor\n\t            ent_coefs.append(ent_coef.item())\n\t            # Optimize entropy coefficient, also called\n\t            # entropy temperature or alpha in the paper\n\t            if ent_coef_loss is not None:\n\t                self.ent_coef_optimizer.zero_grad()\n", "                ent_coef_loss.backward()\n\t                self.ent_coef_optimizer.step()\n\t            with th.no_grad():\n\t                # Select action according to policy\n\t                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n\t                # Compute the next Q values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                # add entropy term\n\t                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n", "                # td error + entropy term\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            # using action from the replay buffer\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critic\n\t            self.critic.optimizer.zero_grad()\n", "            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Compute actor loss\n\t            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n\t            # Mean over all critic networks\n\t            actions_pi.retain_grad()\n\t            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n\t            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n\t            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\t            actor_losses.append(actor_loss.item())\n", "            # Optimize the actor\n\t            self.actor.optimizer.zero_grad()\n\t            actor_loss.backward()\n\t            self.actor.optimizer.step()\n\t            # Update target networks\n\t            if gradient_step % self.target_update_interval == 0:\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t            # Logging gradients\n\t            for tag, value in self.actor.named_parameters():\n\t                if value.grad is not None:\n", "                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t            action_grad = actions_pi.grad.norm(dim=1).sum()\n\t            self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self._n_updates += gradient_steps\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n\t        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n\t        if len(ent_coef_losses) > 0:\n\t            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))\n", "from action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass SACWithOutputPenalty(SAC):\n\t    \"\"\"\n\t    modified SAC to add penalty to violation of constraints\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.constraint = constraint\n\t        self.constraint_penalty = constraint_penalty\n\t        self.penalty_coeff = constraint_penalty\n", "    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update optimizers learning rate\n\t        optimizers = [self.actor.optimizer, self.critic.optimizer]\n\t        if self.ent_coef_optimizer is not None:\n\t            optimizers += [self.ent_coef_optimizer]\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate(optimizers)\n\t        ent_coef_losses, ent_coefs = [], []\n", "        actor_losses, critic_losses = [], []\n\t        for gradient_step in range(gradient_steps):\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            # We need to sample because `log_std` may have changed between two gradient steps\n\t            if self.use_sde:\n\t                self.actor.reset_noise()\n\t            # Action by the current actor for the sampled state\n\t            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n\t            log_prob = log_prob.reshape(-1, 1)\n", "            ent_coef_loss = None\n\t            if self.ent_coef_optimizer is not None:\n\t                # Important: detach the variable from the graph\n\t                # so we don't change it with other losses\n\t                # see https://github.com/rail-berkeley/softlearning/issues/60\n\t                ent_coef = th.exp(self.log_ent_coef.detach())\n\t                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n\t                ent_coef_losses.append(ent_coef_loss.item())\n\t            else:\n\t                ent_coef = self.ent_coef_tensor\n", "            ent_coefs.append(ent_coef.item())\n\t            # Optimize entropy coefficient, also called\n\t            # entropy temperature or alpha in the paper\n\t            if ent_coef_loss is not None:\n\t                self.ent_coef_optimizer.zero_grad()\n\t                ent_coef_loss.backward()\n\t                self.ent_coef_optimizer.step()\n\t            with th.no_grad():\n\t                # Select action according to policy\n\t                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n", "                # Compute the next Q values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                # add entropy term\n\t                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n\t                # td error + entropy term\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            # using action from the replay buffer\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n", "            # Compute critic loss\n\t            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critic\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Compute actor loss\n\t            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n\t            # Mean over all critic networks\n", "            actions_pi.retain_grad()\n\t            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n\t            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n\t            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\t            # calculate penaty\n\t            actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, actions_pi).mean()\n\t            actor_losses.append(actor_loss.item())\n\t            # Optimize the actor\n\t            self.actor.optimizer.zero_grad()\n\t            actor_loss.backward()\n", "            self.actor.optimizer.step()\n\t            # Update target networks\n\t            if gradient_step % self.target_update_interval == 0:\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t            # Logging gradients\n\t            for tag, value in self.actor.named_parameters():\n\t                if value.grad is not None:\n\t                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t            action_grad = actions_pi.grad.norm(dim=1).sum()\n\t            self.logger.record(\"train/action_grad\", action_grad.item())\n", "        self._n_updates += gradient_steps\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n\t        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n\t        if len(ent_coef_losses) > 0:\n\t            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))\n"]}
{"filename": "action_constrained_rl/ddpg/ddpg_with_penalty.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n", "from stable_baselines3 import DDPG\n\tfrom .noise_insertion_ddpg import NoiseInsertionDDPG\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass DDPGWithPenalty(NoiseInsertionDDPG):\n\t    \"\"\"\n\t    modified DDPG to add penalty to violation of constraints of outputs before final layer\n\t    This class is used for DOpt+\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, use_center_wrapper:bool = True, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super(DDPGWithPenalty, self).__init__(*args, use_center_wrapper = use_center_wrapper, **kwargs)\n", "        self.constraint = constraint\n\t        self.penalty_coeff = constraint_penalty\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n", "            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n", "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n", "            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n\t                outputs = self.actor(replay_data.observations)\n\t                before_projection = (self.actor.forward_before_projection(replay_data.observations))\n\t                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\t                actor_losses.append(actor_loss.item())\n\t                # Optimize the actor\n", "                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n\t                self.logger.record(\"train/action_grad\", action_grad.item())\n", "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n"]}
{"filename": "action_constrained_rl/ddpg/noise_insertion_ddpg.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport math\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3 import DDPG\n\tfrom stable_baselines3.common.preprocessing import get_action_dim\n\tfrom .logging_gradient import LoggingGradientDDPG\n", "class NoiseInsertionDDPG(LoggingGradientDDPG):\n\t    \"\"\"\n\t    DDPG to project random samplied actions and to add noise before final layer\n\t    \"\"\"\n\t    def __init__(self, *args, use_center_wrapper:bool = True, **kwargs):\n\t        super(NoiseInsertionDDPG, self).__init__(*args, **kwargs)\n\t        self.action_dim = get_action_dim(self.action_space)\n\t        self.use_center_wrapper = use_center_wrapper\n\t    def _sample_action(\n\t        self,\n", "        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n\t    ) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"\n\t        Sample an action according to the exploration policy.\n\t        This is either done by sampling the probability distribution of the policy,\n\t        or sampling a random action (from a uniform distribution over the action space)\n\t        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n", "            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n\t        :param n_envs:\n\t        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n\t            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n\t        \"\"\"\n\t        # Select action randomly or according to policy\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n", "            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            # project actions\n\t            if self.use_center_wrapper: # use alpha projection\n\t                scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-self.action_dim], self._last_obs[i,-self.action_dim:], scaled_action[i]) for i in range(n_envs)])\n\t            else: # use closest-point projection\n\t                scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n\t        else:\n\t            scaled_action = self.policy.actor.undeformed_predict(self._last_obs) # output before final layer\n", "            # Add noise to the action (improve exploration)\n\t            if action_noise is not None:\n\t                scaled_action = scaled_action + action_noise()\n\t            # Deform action by final layer\n\t            scaled_action = self.policy.actor.deform_action(scaled_action, self._last_obs)\n\t        buffer_action = scaled_action\n\t        action = self.policy.unscale_action(scaled_action)\n\t        return action, buffer_action\n"]}
{"filename": "action_constrained_rl/ddpg/nfwpo.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n", "from stable_baselines3.common.utils import get_schedule_fn, update_learning_rate\n\tfrom stable_baselines3 import TD3\n\tfrom stable_baselines3.common.type_aliases import Schedule\n\timport gurobipy as gp\n\tdef FW_update(action, state, grad, constraint, lr):\n\t    ## Solve LP\n\t    a_dim = action.shape[0]\n\t    with gp.Model() as model:\n\t        x = []\n\t        for _ in range(a_dim):\n", "            x.append(model.addVar(lb=-1, ub =1, vtype = gp.GRB.CONTINUOUS))\n\t        obj = gp.LinExpr()\n\t        for i in range(a_dim):\n\t            obj+=grad[i]*x[i]\n\t        model.setObjective(obj, sense = gp.GRB.MAXIMIZE)\n\t        constraint.gp_constraints(model, x, state)\n\t        model.optimize()\n\t        x_value = np.array(model.X[0:a_dim])\n\t    return x_value*lr + action * (1-lr)\n\tclass NFWPO(TD3):\n", "    \"\"\"\n\t    TD3-based implimentation of NFWPO\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, fw_learning_rate:float = 0.01, actor_learning_rate:float = 1e-3, critic_learning_rate:float = 1e-3, **kargs):\n\t        self.actor_learning_rate = actor_learning_rate\n\t        self.critic_learning_rate = critic_learning_rate\n\t        super().__init__(*args, **kargs)\n\t        self.constraint = constraint\n\t        self.fw_learning_rate = fw_learning_rate\n\t    def _setup_model(self) -> None:\n", "        super()._setup_model()\n\t        self.policy.actor.optimizer = self.policy.optimizer_class(self.policy.actor.parameters(), lr=self.actor_lr_schedule(1), **self.policy.optimizer_kwargs)\n\t        self.policy.critic.optimizer = self.policy.optimizer_class(self.policy.critic.parameters(), lr=self.critic_lr_schedule(1), **self.policy.optimizer_kwargs)\n\t    def _setup_lr_schedule(self) -> None:\n\t        \"\"\"Transform to callable if needed.\"\"\"\n\t        self.actor_lr_schedule = get_schedule_fn(self.actor_learning_rate)\n\t        self.critic_lr_schedule = get_schedule_fn(self.critic_learning_rate)\n\t        self.lr_schedule = self.actor_lr_schedule ## dummy\n\t    def _update_learning_rate(self, optimizers: List[th.optim.Optimizer]) -> None:\n\t        \"\"\"\n", "        Update the optimizers learning rate using the current learning rate schedule\n\t        and the current progress remaining (from 1 to 0).\n\t        :param optimizers:\n\t            a list of optimizers.\n\t        \"\"\"\n\t        # Log the current learning rate\n\t        self.logger.record(\"train/actor_learning_rate\", self.actor_lr_schedule(self._current_progress_remaining))\n\t        self.logger.record(\"train/critic_learning_rate\", self.critic_lr_schedule(self._current_progress_remaining))\n\t        update_learning_rate(optimizers[0], self.actor_lr_schedule(self._current_progress_remaining))\n\t        update_learning_rate(optimizers[1], self.critic_lr_schedule(self._current_progress_remaining))\n", "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n", "            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n", "            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n", "                lr = self.fw_learning_rate\n\t                outputs = self.actor(replay_data.observations)\n\t                # Let actions in constraints\n\t                actions = outputs.cpu().detach().numpy()                \n\t                states = replay_data.observations.cpu().detach().numpy()\n\t                for i in range(batch_size):\n\t                    actions[i] = self.constraint.enforceConstraintIfNeed(states[i], actions[i])\n\t                # Compute Q value grad\n\t                action_tensors = th.tensor(actions, device = outputs.device, dtype = outputs.dtype, requires_grad = True)\n\t                q_value = self.critic.q1_forward(replay_data.observations, action_tensors).mean()\n", "                q_value.backward()\n\t                grads = action_tensors.grad.cpu().detach().numpy()\n\t                # Compute optimized action in CPU\n\t                action_table = np.zeros(actions.shape)\n\t                for i in range(batch_size):\n\t                    action_table[i]=FW_update(actions[i], states[i], grads[i], self.constraint, lr)\n\t                action_table = th.tensor(action_table, device = outputs.device, dtype = outputs.dtype)\n\t                outputs.retain_grad()\n\t                actor_loss = F.mse_loss(outputs, action_table)\n\t                actor_losses.append(actor_loss.item())\n", "                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().detach().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n", "                self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n\t    def _sample_action(\n\t        self,\n\t        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n", "    ) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"\n\t        Sample an action according to the exploration policy.\n\t        This is either done by sampling the probability distribution of the policy,\n\t        or sampling a random action (from a uniform distribution over the action space)\n\t        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n\t            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n", "        :param n_envs:\n\t        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n\t            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n\t        \"\"\"\n\t        # Select action randomly or according to policy\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n\t            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n", "        else:\n\t            unscaled_action, _ = self.policy.predict(self._last_obs)\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            # Add noise to the action (improve exploration)\n\t            if action_noise is not None:\n\t                scaled_action = scaled_action + action_noise()\n\t        scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n\t        buffer_action = scaled_action\n\t        action = self.policy.unscale_action(scaled_action)\n\t        return action, buffer_action\n", "    def predict(\n\t        self,\n\t        observation: np.ndarray,\n\t        state: Optional[Tuple[np.ndarray, ...]] = None,\n\t        episode_start: Optional[np.ndarray] = None,\n\t        deterministic: bool = False,\n\t    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n\t        \"\"\"\n\t        Get the policy action from an observation (and optional hidden state).\n\t        Includes sugar-coating to handle different observations (e.g. normalizing images).\n", "        :param observation: the input observation\n\t        :param state: The last hidden states (can be None, used in recurrent policies)\n\t        :param episode_start: The last masks (can be None, used in recurrent policies)\n\t            this correspond to beginning of episodes,\n\t            where the hidden states of the RNN must be reset.\n\t        :param deterministic: Whether or not to return deterministic actions.\n\t        :return: the model's action and the next hidden state\n\t            (used in recurrent policies)\n\t        \"\"\"\n\t        unscaled_action, states = self.policy.predict(observation, state, episode_start, deterministic)\n", "        scaled_action = self.policy.scale_action(unscaled_action)\n\t        scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(observation[i], scaled_action[i]) for i in range(len(self.env.envs))])\n\t        return self.policy.unscale_action(scaled_action), states\n\tif __name__ == \"__main__\":\n\t    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    import cvxpy as cp\n\t    cons = HalfCheetahDynamicConstraint()\n\t    action = np.random.rand(6)\n\t    state = 10*np.random.rand(17)\n\t    grad = np.random.rand(6)\n", "    gp.setParam('OutputFlag', 0)\n\t    print(action, state[11:], grad)\n\t    x_value = FW_update(action, state, grad, cons, 1)\n\t    print(x_value)\n\t    x = cp.Variable(6)\n\t    obj = cp.Maximize(grad.T @ x)\n\t    prob = cp.Problem(obj, cons.cvxpy_constraints(x, state))\n\t    prob.solve(solver=cp.GUROBI)\n\t    print(x.value)\n"]}
{"filename": "action_constrained_rl/ddpg/projection_ddpg.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport math\n\timport gym\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n", "from stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3 import DDPG\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass ProjectionDDPG(DDPG):\n\t    \"\"\"\n\t    class for DPro\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.constraint = constraint\n", "        self.penalty_coeff = constraint_penalty\n\t    def _sample_action(\n\t        self,\n\t        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n\t    ) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"\n\t        Sample an action according to the exploration policy.\n\t        This is either done by sampling the probability distribution of the policy,\n", "        or sampling a random action (from a uniform distribution over the action space)\n\t        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n\t            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n\t        :param n_envs:\n\t        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n\t            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n", "        \"\"\"\n\t        # Select action randomly or according to policy\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n\t            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t        else:\n\t            # Note: when using continuous actions,\n\t            # we assume that the policy uses tanh to scale the action\n\t            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n\t            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n", "        # Rescale the action from [low, high] to [-1, 1]\n\t        if isinstance(self.action_space, gym.spaces.Box):\n\t            obs = self._last_obs[-1]\n\t            #print(\"unscaled: {}\".format(unscaled_action))\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            #print(\"scaled: {}\".format(scaled_action))\n\t            # Add noise to the action (improve exploration)\n\t            if action_noise is not None:\n\t                scaled_action = scaled_action + action_noise()\n\t            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n", "            # We store the scaled action in the buffer\n\t            buffer_action = scaled_action\n\t            action = self.policy.unscale_action(scaled_action)\n\t        else:\n\t            # Discrete case, no need to normalize or clip\n\t            buffer_action = unscaled_action\n\t            action = buffer_action\n\t        return action, buffer_action\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n", "        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n", "                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n", "            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n\t                outputs = self.actor(replay_data.observations)\n", "                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                # calculate penaty\n\t                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\t                actor_losses.append(actor_loss.item())\n\t                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n", "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n\t                self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n", "if __name__ == \"__main__\":\n\t    import gym\n\t    import numpy as np\n\t    import os\n\t    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\t    from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n\t    video_folder = 'log/videos/'\n\t    video_length = 1000\n\t    env = gym.make(\"IdealizedPendulum-v0\", max_speed=8.0, max_torque=2.0, normalization_factor=100.0, l=1.0, initial_state=np.array([0.25 * np.pi, 1.0]), dt=0.01)\n\t#    env = DummyVecEnv([lambda: env])\n", "#    env = VecVideoRecorder(env, video_folder,\n\t#                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n\t#                       name_prefix=f\"ddpg-cgf-pendulum\")\n\t# The noise objects for DDPG\n\t    n_actions = env.action_space.shape[-1]\n\t    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n\t    cbf = PendulumCBF(normalization_factor=100.0)\n\t    model = ProjectionDDPG(cbf, 'MlpPolicy', env, action_noise=action_noise, verbose=1)\n\t    model.learn(total_timesteps=1000)\n\t    env = model.get_env()\n", "    del model # remove to demonstrate saving and loading\n"]}
{"filename": "action_constrained_rl/ddpg/logging_gradient.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n", "from stable_baselines3 import DDPG\n\tclass LoggingGradientDDPG(DDPG):\n\t    \"\"\"\n\t    modified DDPG to log gradient\n\t    \"\"\"\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n", "        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n", "                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n", "            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n\t                outputs = self.actor(replay_data.observations)\n\t                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                actor_losses.append(actor_loss.item())\n", "                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n", "                self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass DDPGWithOutputPenalty(DDPG):\n\t    \"\"\"\n\t    modified DDPG to add penalty to violation of constraints\n\t    \"\"\"\n", "    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.constraint = constraint\n\t        self.penalty_coeff = constraint_penalty\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n", "        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n", "                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n", "            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n\t                outputs = self.actor(replay_data.observations)\n\t                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                # calculate penaty\n\t                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n", "                actor_losses.append(actor_loss.item())\n\t                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n", "                action_grad = outputs.grad.norm(dim=1).sum()\n\t                self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n"]}
{"filename": "action_constrained_rl/env_wrapper/memorize_center_env_wrapper.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport gym\n\timport numpy as np\n\tfrom gym import spaces\n\timport math\n\timport time\n\tfrom stable_baselines3.common.monitor import ResultsWriter\n\tclass MemorizeCenterEnvWrapper(gym.Wrapper):\n\t    \"\"\"\n", "    env wrapper to calculate centers of action spaces and to concatenate it with observations\n\t    \"\"\"\n\t    def __init__(self, constraint, env, n=1, dual_learning_rate=0.0):\n\t        super().__init__(env)\n\t        self.env = env\n\t        self.constraint = constraint\n\t        self.n=n\n\t        self.dual_learning_rate = dual_learning_rate\n\t        act_space = self.env.action_space\n\t        obs_space = self.env.observation_space\n", "        self.observation_space = gym.spaces.Box(low = np.concatenate((obs_space.low, act_space.low)),\n\t                                                high  = np.concatenate((obs_space.high, act_space.high)),\n\t                                                shape = (obs_space.shape[0] + act_space.shape[0],),\n\t                                                dtype = obs_space.dtype)\n\t    def observation(self, obs):\n\t        return np.concatenate((obs, self.constraint.get_center(obs)))\n\t    def reset(self, **kwargs):\n\t        \"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\n\t        self.prev_obs = self.env.reset(**kwargs)\n\t        return self.observation(self.prev_obs)\n", "    def step(self, action):\n\t        state = self.prev_obs.squeeze()\n\t        assert self.constraint.isConstraintSatisfied(state, action), f\"constraint violated state={state} action{action} \"\n\t        next_state, reward, done, info = self.env.step(action)\n\t        self.prev_obs = next_state\n\t        return self.observation(next_state), reward, done, info\n"]}
{"filename": "action_constrained_rl/env_wrapper/__init__.py", "chunked_list": ["from .constraint_wrapper import ConstraintEnvWrapper\n\tfrom .memorize_center_env_wrapper import MemorizeCenterEnvWrapper\n"]}
{"filename": "action_constrained_rl/env_wrapper/constraint_wrapper.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport gym\n\timport numpy as np\n\tfrom gym import spaces\n\timport math\n\timport time\n\tfrom stable_baselines3.common.monitor import ResultsWriter\n\tfrom ..utils.constant_function import ConstantFunction\n\tdef quadraticPenalty(x):\n", "    return np.sum(np.square(x))\n\tclass ConstraintEnvWrapper(gym.Wrapper):\n\t    \"\"\"\n\t    wrapper to project actions\n\t    \"\"\"\n\t    def __init__(self, constraint, env, constraint_penalty=ConstantFunction(0.0), enforce_constraint=False, filename=None, n=1, dual_learning_rate=0.0, normalize=False, infinity_action_space = False):\n\t        super().__init__(env)\n\t        self.env = env\n\t        self.constraint = constraint\n\t        self.prev_obs = None\n", "        self.constraint_penalty = constraint_penalty\n\t        self.num_pre_projection_constraint_violation = 0\n\t        self.enforce_constraint = enforce_constraint\n\t        self.dual_learning_rate = dual_learning_rate\n\t        self.n = n\n\t        self.normalize = normalize\n\t        self.t_start = time.time()\n\t        if filename is not None:\n\t            self.results_writer = ResultsWriter(\n\t                filename,\n", "                header={\"t_start\": self.t_start, \"env_id\": env.spec and env.spec.id},\n\t                extra_keys=(\"v\",\"c\",\"sum_term\",\"quadratic_sum_term\", \"quadratic_sum_term_normalized\")\n\t            )\n\t        else:\n\t            self.results_writer = None\n\t        self.rewards = None\n\t        self.violations = None\n\t        self.quadratic_sum_violations = None\n\t        self.quadratic_sum_violations_normalized = None\n\t        self.needs_reset = True\n", "        self.total_steps = 0\n\t        self.lagrange_multiplier = np.zeros(self.constraint.numConstraints())\n\t        self.episode =1\n\t        self.infinity_action_space = infinity_action_space\n\t    def logConstraintViolation(self, state, action):\n\t        self.violations.append(self.constraint.constraintViolation(state, action))\n\t        self.quadratic_sum_violations.append(quadraticPenalty(self.constraint.constraintViolation(state, action)))\n\t        self.quadratic_sum_violations_normalized.append(quadraticPenalty(self.constraint.constraintViolation(state, action, normalize=True)))\n\t    def step(self, action):\n\t        state = self.prev_obs.squeeze()\n", "        penalty = 0.0\n\t        lagrange_term = 0.0\n\t        if self.infinity_action_space:\n\t            action = np.arctanh(np.clip(action,-1+1e-6,1-1e-6))\n\t        self.logConstraintViolation(state, action)\n\t        if self.enforce_constraint and self.constraint.proj_type == \"shrinkage\":\n\t            action = self.constraint.enforceConstraint(state, action)\n\t        elif not self.constraint.isConstraintSatisfied(state, action):\n\t            self.num_pre_projection_constraint_violation += 1\n\t            if self.enforce_constraint:\n", "                g = self.constraint.constraintViolation(state, action, normalize=self.normalize)\n\t                penalty -= self.constraint_penalty(self.episode // self.n + 1) * quadraticPenalty(g)\n\t                action = self.constraint.enforceConstraint(state, action)\n\t                #lagrange_term += np.dot(self.lagrange_multiplier, g)\n\t        assert self.constraint.isConstraintSatisfied(state, action), f\"constraint violated state={state} action{action} \"\n\t        next_state, reward, done, info = self.env.step(action)\n\t        self.rewards.append(reward)\n\t        if done:\n\t            if self.episode % self.n == 0:\n\t                pass\n", "                #self.lagrange_multiplier = np.minimum(0.0, self.lagrange_multiplier -self.dual_learning_rate * sum(self.violations))\n\t                #print(\"violations: {}\".format(sum(self.constraint_violations)))\n\t                #print(\"multiplier: {}\".format(self.lagrange_multiplier))\n\t            self.needs_reset = True\n\t            ep_info = self.getEpInfo()\n\t            if self.results_writer:\n\t                self.results_writer.write_row(ep_info)\n\t            info[\"episode\"] = ep_info\n\t            self.episode += 1\n\t        self.total_steps += 1\n", "        self.prev_obs = next_state\n\t        return next_state, reward + lagrange_term + penalty, done, info\n\t    def getEpInfo(self):\n\t        ep_rew = sum(self.rewards)\n\t        ep_len = len(self.rewards)\n\t        ep_info = {\"r\": round(ep_rew, 6), \"l\": ep_len, \"t\": round(time.time() - self.t_start, 6), \"v\": self.num_pre_projection_constraint_violation, \"c\": round(self.constraint_penalty(self.episode // self.n + 1), 6), \"sum_term\": round(sum(sum(self.violations)), 6), \"quadratic_sum_term\": round(sum(self.quadratic_sum_violations), 6), \"quadratic_sum_term_normalized\": round(sum(self.quadratic_sum_violations_normalized), 6)}\n\t        return ep_info\n\t    def reset(self, **kwargs):\n\t        self.rewards = []\n\t        self.violations = []\n", "        self.quadratic_sum_violations = []\n\t        self.quadratic_sum_violations_normalized = []\n\t        self.num_pre_projection_constraint_violation = 0\n\t        self.needs_reset = False\n\t        self.prev_obs = self.env.reset()\n\t        return self.prev_obs\n\tif __name__ == \"__main__\":\n\t    import math\n\t    import time\n\t    import gym\n", "    from ..idealized_pendulum.cbf import PendulumCBF\n\t    normalization_factor = 500.0 \n\t    env = ConstraintEnvWrapper(PendulumCBF(normalization_factor=normalization_factor), gym.make(\"IdealizedPendulum-v0\",  normalization_factor=normalization_factor, dt=0.01, max_speed=1000.0))\n\t    observation = env.reset()\n\t    observation = env.setState(np.array([0.25 * np.pi, 1.0]))\n\t    frames = []\n\t    for _ in range(1000):\n\t        action = np.random.uniform(-1.0, 1.0)\n\t        observation, reward, done, info = env.step(np.array([action]))\n\t        print(observation)\n", "        env.render()\n\t        time.sleep(0.03)\n\t#        frames.append(env.render(\"rgb_array\"))\n\t    env.close()\n"]}
{"filename": "action_constrained_rl/constraint/constraint.py", "chunked_list": ["import numpy as np\n\timport torch as th\n\timport math\n\tfrom abc import ABC, abstractmethod\n\tfrom .normalize_constraint import normalizeConstraint\n\tfrom ..nn.additional_layers.chebyshev_center import calc_chebyshev_center\n\timport cvxpy as cp\n\tfrom ..cvxpy_variables import CVXPYVariables\n\timport gurobipy as gp\n\tdef to_tensors(a):\n", "    return th.tensor(np.expand_dims(a,0))\n\tEPS=1e-9\n\tclass Constraint(ABC):\n\t    \"\"\"\n\t    Abstract class for all action constraints\n\t    \"\"\"\n\t    def __init__(self, a_dim:int, s_dim:int = -1):\n\t        self.a_dim = a_dim\n\t        self.s_dim = s_dim\n\t    @abstractmethod\n", "    def isConstraintSatisfied(self, state, a):\n\t        pass\n\t    def enforceConstraint(self, state, a):\n\t        with gp.Model() as model:\n\t            x = []\n\t            for _ in range(self.a_dim):\n\t                x.append(model.addVar(lb=-1, ub =1, vtype = gp.GRB.CONTINUOUS))\n\t            obj = gp.QuadExpr()\n\t            for i in range(self.a_dim):\n\t                obj+=(0.5*x[i]-a[i])*x[i]\n", "            model.setObjective(obj, sense = gp.GRB.MINIMIZE)\n\t            self.gp_constraints(model, x, state)\n\t            model.optimize()\n\t            x_value = np.array(model.X[0:self.a_dim])\n\t        return x_value\n\t    def enforceConstraintIfNeed(self, state, a):\n\t        if self.isConstraintSatisfied(state, a):\n\t            return a\n\t        else:\n\t            return self.enforceConstraint(state, a)\n", "    @abstractmethod\n\t    def numConstraints(self):\n\t        pass\n\t    @abstractmethod\n\t    def getL(self, states, centers, v, get_grad:bool = False):\n\t        pass\n\t    @abstractmethod\n\t    def get_center(self, state):\n\t        pass\n\t    def project(self, state, center, a):\n", "        L = self.getL(to_tensors(state), to_tensors(center), to_tensors(a-center)).numpy()[0]\n\t        return center + (a-center) / max(L, 1)\n\t    @abstractmethod\n\t    def cvxpy_constraints(self, x, state = None):\n\t        pass\n\t    @abstractmethod\n\t    def gp_constraints(self, x, state = None):\n\t        pass\n\t    def cvxpy_variables(self):\n\t        q = cp.Parameter(self.a_dim)  # input action\n", "        if self.s_dim > 0:\n\t            s = cp.Parameter(self.s_dim)  # input: state\n\t        else:\n\t            s = None\n\t        x = cp.Variable(self.a_dim)  # output\n\t        obj = cp.Minimize(0.5*cp.sum_squares(x) - q.T @ x)\n\t        cons = self.cvxpy_constraints(x, s)\n\t        prob = cp.Problem(obj, cons)\n\t        return CVXPYVariables(x, q, s, cons, obj, prob)\n\tclass LinearConstraint(Constraint):\n", "    \"\"\"\n\t    Abstract class for linear constraints with:\n\t        Ax = b,\n\t        Cx <= d\n\t    When inequality representations are needed, A, b, C, and d are aggregated into Ex <= f\n\t    \"\"\"\n\t    def __init__(self, a_dim:int, s_dim:int = -1, proj_type: str = \"QP\"):\n\t        super().__init__(a_dim, s_dim)\n\t        self.proj_type = proj_type\n\t    @abstractmethod\n", "    def E(self, state):\n\t        pass\n\t    @abstractmethod\n\t    def f(self, state):\n\t        pass\n\t    def C(self, state):\n\t        if isinstance(state, np.ndarray):\n\t            return self.tensor_C(to_tensors(state)).numpy()[0]\n\t        return self.tensor_C(state)\n\t    def d(self, state):\n", "        if isinstance(state, np.ndarray):\n\t            return self.tensor_d(to_tensors(state)).numpy()[0]\n\t        return self.tensor_d(state)\n\t    @abstractmethod\n\t    def tensor_C(self, state):\n\t        pass\n\t    @abstractmethod\n\t    def tensor_d(self, state):\n\t        pass\n\t    def getL(self, states, centers, v, get_grad:bool = False):\n", "        C = self.tensor_C(states)\n\t        d = self.tensor_d(states)\n\t        div = d-(centers[:,None,:]*C).sum(axis=2)\n\t        maxs = ((v[:,None,:]*C).sum(axis=2) / (div+EPS)).max(axis = 1)\n\t        if not get_grad:\n\t            return maxs[0]\n\t        else:\n\t            indices_2 = maxs[1][:,None].expand(-1,1)\n\t            indices_3 = maxs[1][:,None,None].expand(-1,1,C.shape[2])\n\t            gradL = th.gather(C, 1, indices_3).squeeze(dim=1) / th.gather(div, 1, indices_2).squeeze(dim=1)[:,None]\n", "            return maxs[0], gradL\n\t    def get_center(self, state):\n\t        C = self.C(state)\n\t        d = self.d(state)\n\t        return calc_chebyshev_center(C,d)\n\t    def A(self, state):\n\t        None\n\t    def b(self, state):\n\t        None\n\t    def isConstraintSatisfied(self, state, a, err=1e-1):\n", "        if state.ndim == 2:\n\t            return ((self.E(state) * a[:,None,:]).sum(axis = 2) <= self.f(state) + err).all()\n\t        else:\n\t            return (np.matmul(self.E(state), a) <= self.f(state) + err).all()\n\t    def enforceConstraint(self, state, a):\n\t        if self.proj_type == \"QP\":\n\t            return super().enforceConstraint(state, a)\n\t        elif self.proj_type == \"alpha\":\n\t            C = self.C(state)\n\t            d = self.d(state)\n", "            center = calc_chebyshev_center(C,d)\n\t            v = a - center\n\t            return center + v / max((C @ v / (d - C @ center) ).max(), 1)\n\t        elif self.proj_type == \"shrinkage\":\n\t            C = self.C(state)\n\t            d = self.d(state)\n\t            center = calc_chebyshev_center(C,d)\n\t            v = a - center\n\t            L = (C @ v / (d - C @ center) ).max()\n\t            return center + math.tanh(L) / L * v\n", "        else:\n\t            raise ValueError(self.proj_type)\n\t    def constraintViolation(self, state, a, err=1e-2, normalize=False):\n\t        if normalize:\n\t            E = self.E(state)\n\t            f = self.f(state)\n\t            normalized_E, normalized_f = normalizeConstraint(E, f)\n\t            return np.maximum(0.0, np.matmul(normalized_E, a) - normalized_f - err)\n\t        else:\n\t            return np.maximum(0.0, np.matmul(self.E(state), a) - self.f(state) - err)\n", "    def constraintViolationBatch(self, states, actions):\n\t        C = self.tensor_C(states)\n\t        d = self.tensor_d(states)\n\t        return th.maximum(((C*actions[:,None,:]).sum(dim=2)-d)/(C.norm(dim=2)+EPS),th.tensor(0.)).norm(dim=1)\n\tif __name__ == \"__main__\":\n\t    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    cons_pq = HalfCheetahDynamicConstraint(\"QP\")\n\t    cons_al = HalfCheetahDynamicConstraint(\"alpha\")\n\t    cons_sh = HalfCheetahDynamicConstraint(\"shrinkage\")\n\t    from ..nn.opt_layer.opt_layer import OptLayer\n", "    from ..nn.additional_layers.alpha_projection import AlphaProjectionLayer\n\t    from ..nn.additional_layers.radial_shrinkage import ShrinkageLayer\n\t    layer_pq = OptLayer(cons_pq)\n\t    layer_al = AlphaProjectionLayer(cons_pq)\n\t    layer_sh = ShrinkageLayer(cons_pq)\n\t    actions = th.rand(100, 6)\n\t    states = th.rand(100, 17)\n\t    y_pq = layer_pq(actions, states)\n\t    y_al = layer_al(actions, states)\n\t    y_sh = layer_sh(actions, states)\n", "    for i in range(100):\n\t        state = states[i].numpy()\n\t        action = actions[i].numpy()\n\t        projected_pq = cons_pq.enforceConstraint(state, action)\n\t        projected_al = cons_al.enforceConstraint(state, action)\n\t        projected_sh = cons_sh.enforceConstraint(state, action)\n\t        assert cons_pq.isConstraintSatisfied(state, projected_pq) and (not cons_pq.isConstraintSatisfied(state, action) or np.allclose(action, projected_pq, rtol = 1e-3))\n\t        assert cons_al.isConstraintSatisfied(state, projected_al) and (not cons_al.isConstraintSatisfied(state, action) or np.allclose(action, projected_al))\n\t        assert cons_al.isConstraintSatisfied(state, projected_sh)\n\t        assert np.allclose(y_pq[i].numpy(), projected_pq, rtol = 1e-3)\n", "        assert np.allclose(y_al[i].numpy(), projected_al, rtol = 1e-3)\n\t        assert np.allclose(y_sh[i].numpy(), projected_sh, rtol = 1e-3)\n\t    #print(cons.constraintViolation(None, np.array([2.0, 0.0, 3.0, -4.0, 0.0, 5.0])))\n\t    #print(cons.constraintViolation(None, np.array([2.0, 0.0, 3.0, -4.0, 0.0, 5.0]), normalize=True))\n"]}
{"filename": "action_constrained_rl/constraint/combined_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\timport math\n\tfrom abc import ABC, abstractmethod\n\tfrom .normalize_constraint import normalizeConstraint\n\tfrom ..nn.additional_layers.chebyshev_center import calc_chebyshev_center\n\timport cvxpy as cp\n\tfrom ..cvxpy_variables import CVXPYVariables\n", "from .constraint import Constraint\n\timport gurobipy as gp\n\tEPS=1e-9\n\tclass CombinedConstraint(Constraint):\n\t    \"\"\"\n\t    class for combined two constraints\n\t    \"\"\"\n\t    def __init__(self, cons1, cons2):\n\t        super().__init__(cons1.a_dim, cons1.s_dim)\n\t        self.cons1=cons1\n", "        self.cons2=cons2\n\t    def isConstraintSatisfied(self, state, a):\n\t        return self.cons1.isConstraintSatisfied(state, a) and self.cons2.isConstraintSatisfied(state,a)\n\t    def numConstraints(self):\n\t        return self.cons1.numConstraints() + self.cons2.numConstraints()\n\t    def getL(self, states, centers, v, get_grad:bool = False):\n\t        if get_grad:\n\t            L1, grad1 = self.cons1.getL(states, centers, v, get_grad = True)\n\t            L2, grad2 = self.cons2.getL(states, centers, v, get_grad = True)\n\t            L = th.maximum(L1, L2)\n", "            grad = th.where(th.ge(L1,L2)[:,None], grad1, grad2)\n\t            return L, grad\n\t        else:\n\t            L1 = self.cons1.getL(states, centers, v)\n\t            L2 = self.cons2.getL(states, centers, v)\n\t            return th.maximum(L1, L2)\n\t    def get_center(self, state):\n\t        return self.cons2.get_center(state)\n\t    def cvxpy_constraints(self, x, state = None):\n\t        return self.cons1.cvxpy_constraints(x, state)+self.cons2.cvxpy_constraints(x,state)\n", "    def gp_constraints(self, model, x, state = None):\n\t        self.cons1.gp_constraints(model, x, state)\n\t        self.cons2.gp_constraints(model, x, state)\n\t    def constraintViolation(self, state, a, err=1e-2, normalize=False):\n\t        return self.cons1.constraintViolation(state, a, err, normalize)+self.cons2.constraintViolation(state, a, err, normalize)\n\t    def constraintViolationBatch(self, states, actions):\n\t        return self.cons1.constraintViolationBatch(states, actions)+self.cons2.constraintViolationBatch(states, actions)\n\tif __name__ == \"__main__\":\n\t    from .power_constraint import OrthoplexConstraint\n\t    from .sin2_constraint import Sin2Constraint\n", "    offset_p = 2\n\t    scale = (1., 1., 1., 1., 1., 1.)\n\t    indices_p = list(range(offset_p, offset_p+len(scale)))\n\t    offset_v = 11\n\t    indices_v = list(range(offset_v, offset_v+len(scale)))\n\t    s_dim = 17\n\t    cons = CombinedConstraint(OrthoplexConstraint(indices_v, scale, 10., s_dim),\n\t                              Sin2Constraint(indices_p, 0.1, s_dim))\n\t    state=np.array([8.07938070e-01,  1.66712368e-01, -3.53352869e-01, -2.05882562e+00, 7.96720395e-01, -8.13423423e-01, -5.51193071e-01, -2.03170841e-01,  9.15822510e-01, -1.39466434e+00, -2.43234536e+00,  3.60834351e+00, -1.00000000e+01,  2.21396068e-06,  9.28677242e-01, -9.96848688e+00,  7.87601474e+00])\n\t    action=np.array([0.26813674, -0.16751897, 0.7599944, -0.00297695, -0.07156372, -0.1420452 ])\n", "    print(cons.get_center(state))\n"]}
{"filename": "action_constrained_rl/constraint/power_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .constraint import LinearConstraint\n\timport torch\n\timport cvxpy as cp\n\timport gurobipy as gp\n\tfrom ..cvxpy_variables import CVXPYVariables\n\tdef make_compatible(a, b):\n\t    if a.device != b.device:\n\t        a=a.to(b.device)\n", "    if a.dtype != b.dtype:\n\t        a=a.to(b.dtype)\n\t    return a\n\tclass PowerConstraint(LinearConstraint):\n\t    \"\"\"\n\t    State-dependent Action Constraints with the from\n\t    $`\\sum max{w_i a_i, 0} \\leq M, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n\t    \"\"\"\n\t    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n\t        super().__init__(len(scale), s_dim, **kargs)\n", "        self.indices = torch.tensor(indices)\n\t        self.K = torch.zeros((2 ** self.a_dim -1, self.a_dim))\n\t        self.scale = scale\n\t        self.s_dim = s_dim\n\t        for i in range(2 ** self.a_dim -1):\n\t            for j in range(self.a_dim):\n\t                if i // (2 ** j) % 2 == 0:\n\t                    self.K[i,j] = scale[j]\n\t        self.max_power = max_power\n\t        self.d_value = torch.hstack((self.max_power * torch.ones(self.K.shape[0]), torch.ones(2*self.a_dim)))\n", "    def tensor_C(self, state):\n\t        size = state.shape[0]\n\t        device = state.device\n\t        self.K = make_compatible(self.K, state)\n\t        if self.indices.device != state.device:\n\t            self.indices=self.indices.to(state.device)\n\t        C = self.K[None, :, :] * torch.index_select(state, 1, self.indices)[:,None,:]\n\t        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n\t        return torch.concat((C, eyes, -eyes), axis = 1)\n\t    def tensor_d(self, state):\n", "        size = state.shape[0]\n\t        device = state.device\n\t        self.d_value = make_compatible(self.d_value, state)\n\t        return self.d_value.repeat(size, 1)\n\t    def numConstraints(self):\n\t        return self.K.shape[0] + 2 * self.a_dim\n\t    def E(self, state):\n\t        return self.C(state)\n\t    def f(self, state):\n\t        return self.d(state)\n", "    def cvxpy_constraints(self, x, state):\n\t        cons = [sum([cp.maximum(self.scale[j] * x[j] * state[self.indices[j].item()], 0.) for j in range(self.a_dim)]) <= self.max_power]\n\t        for i in range(self.a_dim):\n\t            cons.append(x[i] <= 1.)\n\t            cons.append(-x[i] <= 1.)\n\t        return cons\n\t    def gp_constraints(self, model, x, s):\n\t        max_vars = []\n\t        for i in range(self.a_dim):\n\t            mul_var = model.addVar(lb = -gp.GRB.INFINITY, ub = gp.GRB.INFINITY,\n", "                                   vtype = gp.GRB.CONTINUOUS)\n\t            model.addConstr(mul_var == self.scale[i]*x[i]*s[self.indices[i].item()])\n\t            max_var = model.addVar(lb=0, ub = gp.GRB.INFINITY,\n\t                                   vtype = gp.GRB.CONTINUOUS)\n\t            model.addConstr(max_var == gp.max_(mul_var, 0))\n\t            max_vars.append(max_var)\n\t        model.addConstr(sum(max_vars) <= self.max_power)\n\tclass OrthoplexConstraint(LinearConstraint):\n\t    \"\"\"\n\t    State-dependent Action Constraints with the from\n", "    $`\\sum |w_i a_i| \\leq M, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n\t    \"\"\"\n\t    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n\t        super().__init__(len(scale), s_dim, **kargs)\n\t        self.indices = torch.tensor(indices)\n\t        self.K = torch.zeros((2 ** self.a_dim, self.a_dim))\n\t        self.scale = scale\n\t        self.s_dim = s_dim\n\t        for i in range(2 ** self.a_dim):\n\t            for j in range(self.a_dim):\n", "                if i // (2 ** j) % 2 == 0:\n\t                    self.K[i,j] = scale[j]\n\t                else:\n\t                    self.K[i,j] = -scale[j]\n\t        self.max_power = max_power\n\t        self.d_value = torch.hstack((self.max_power * torch.ones(self.K.shape[0]), torch.ones(2*self.a_dim)))\n\t    def tensor_C(self, state):\n\t        size = state.shape[0]\n\t        device = state.device\n\t        self.K = make_compatible(self.K, state)\n", "        if self.indices.device != state.device:\n\t            self.indices=self.indices.to(state.device)\n\t        C = self.K[None, :, :] * torch.index_select(state, 1, self.indices)[:, None, :]\n\t        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n\t        return torch.concat((C, eyes, -eyes), axis = 1)\n\t    def tensor_d(self, state):\n\t        size = state.shape[0]\n\t        device = state.device\n\t        self.d_value = make_compatible(self.d_value, state)\n\t        return self.d_value.repeat(size, 1)\n", "    def numConstraints(self):\n\t        return self.K.shape[0] + 2 * self.a_dim\n\t    def E(self, state):\n\t        return self.C(state)\n\t    def f(self, state):\n\t        return self.d(state)\n\t    def cvxpy_constraints(self, x, state):\n\t        cons = [sum([cp.abs(self.scale[j] * x[j] * state[self.indices[j].item()]) for j in range(self.a_dim)]) <= self.max_power]\n\t        for i in range(self.a_dim):\n\t            cons.append(x[i] <= 1.)\n", "            cons.append(-x[i] <= 1.)\n\t        return cons\n\t    def gp_constraints(self, model, x, s):\n\t        abs_vars = []\n\t        for i in range(self.a_dim):\n\t            mul_var = model.addVar(lb = -gp.GRB.INFINITY, ub = gp.GRB.INFINITY,\n\t                                   vtype = gp.GRB.CONTINUOUS)\n\t            model.addConstr(mul_var == self.scale[i]*x[i]*s[self.indices[i].item()])\n\t            abs_var = model.addVar(lb=0, ub = gp.GRB.INFINITY,\n\t                                   vtype = gp.GRB.CONTINUOUS)\n", "            model.addGenConstrAbs(abs_var, mul_var)\n\t            abs_vars.append(abs_var)\n\t        model.addConstr(sum(abs_vars) <= self.max_power)\n\tclass DecelerationConstraint(LinearConstraint):\n\t    \"\"\"\n\t    State-dependent Action Constraints with the from\n\t    $`\\sum w_i a_i \\leq M - \\sum |w_i|, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n\t    \"\"\"\n\t    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n\t        super().__init__(len(scale), s_dim, **kargs)\n", "        self.indices = torch.tensor(indices)\n\t        self.scale = torch.tensor(scale)\n\t        self.s_dim = s_dim\n\t        self.max_power = max_power\n\t    def tensor_C(self, state):\n\t        size = state.shape[0]\n\t        device = state.device\n\t        self.scale = make_compatible(self.scale, state)\n\t        if self.indices.device != state.device:\n\t            self.indices=self.indices.to(state.device)\n", "        C = (self.scale[None,:] * torch.index_select(state, 1, self.indices)).unsqueeze(1)\n\t        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n\t        return torch.concat((C, eyes, -eyes), axis = 1)\n\t    def tensor_d(self, state):\n\t        size = state.shape[0]\n\t        device = state.device\n\t        self.scale = make_compatible(self.scale, state)\n\t        d = (self.max_power * torch.ones(size, device = device) - torch.abs(self.scale[None,:] * torch.index_select(state, 1, self.indices)).sum(1)).unsqueeze(1)\n\t        return torch.concat((d, torch.ones((size, 2*self.a_dim), device = device)), 1)\n\t    def numConstraints(self):\n", "        return 1 + 2 * self.a_dim\n\t    def E(self, state):\n\t        return self.C(state)\n\t    def f(self, state):\n\t        return self.d(state)\n\t    def cvxpy_variables(self):\n\t        q = cp.Parameter(self.a_dim)  # input action\n\t        s = cp.Parameter(self.s_dim)  # input: state\n\t        x = cp.Variable(self.a_dim)  # output\n\t        obj = cp.Minimize(0.5*cp.sum_squares(x) - q.T @ x)\n", "        cons = []\n\t        for i in range(1<<self.a_dim):\n\t            sg = []\n\t            for j in range(self.a_dim):\n\t                if i // (2 ** j) % 2 == 0:\n\t                    sg.append(1)\n\t                else:\n\t                    sg.append(-1)\n\t            cons.append(sum([x[j]*self.scale[j]*s[self.indices[j].item()] for j in range(self.a_dim)])\n\t                        <= self.max_power - sum([sg[j]*self.scale[j]*s[self.indices[j].item()] for j in range(self.a_dim)]))\n", "        prob = cp.Problem(obj, cons)\n\t        return CVXPYVariables(x, q, s, cons, obj, prob)\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    cons = PowerConstraint(6, (1., 1.), 1., 11)\n\t    action = np.random.rand(2)\n\t    state = 5*np.random.rand(11)\n\t    print(cons.get_center(state))\n\t    x = cp.Variable(2)  # output\n\t    r = cp.Variable()\n", "    obj = cp.Maximize(r)\n\t    C = cons.C(state)\n\t    d = cons.d(state)\n\t    norm=np.linalg.norm(C, axis =1)\n\t    cons = [C @ x + norm * r <= d]\n\t    prob = cp.Problem(obj, cons)\n\t    prob.solve(solver = cp.GUROBI)\n\t    print(x.value)\n\t    exit()\n\t    p_action = cons.enforceConstraintIfNeed(state, action)\n", "    print(p_action, 0.5*np.sum(p_action**2)-p_action.dot(action), p_action.dot(state[6:8]))\n\t    x = cp.Variable(2)  # output\n\t    obj = cp.Minimize(0.5*cp.sum_squares(x) - action.T @ x)\n\t    cons = [cp.maximum(x[0]*state[6],0.)+cp.maximum(x[1]*state[7],0)<=1., x[0]<=1., -x[0] <= 1., x[1]<=1., -x[1]<=1.]\n\t    prob = cp.Problem(obj, cons)\n\t    prob.solve(solver = cp.GUROBI)\n\t    print(x.value, 0.5*np.sum(x.value**2)-x.value.dot(action), x.value.dot(state[6:8]))\n"]}
{"filename": "action_constrained_rl/constraint/tip_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .quadratic_constraint import QuadraticConstraint\n\timport torch as th\n\timport cvxpy as cp\n\timport gurobipy as gp\n\timport math\n\tfrom ..cvxpy_variables import CVXPYVariables\n\tfrom .power_constraint import make_compatible\n\tclass TipConstraint(QuadraticConstraint):\n", "    \"\"\"\n\t    State-dependent Action Constraints with the from\n\t    $a_0^2+2a_0(a_0+a+1)\\cos\\theta_2+(a_0+a_1)^2$.\n\t    \"\"\"\n\t    def __init__(self, max_M, **kargs):\n\t        super().__init__(max_M, 2, 9, **kargs)\n\t    def getTensorQ(self, states):\n\t        Q=th.zeros((states.shape[0],2,2),device = states.device)\n\t        cosg = th.cos(states[:,7])\n\t        Q[:,0,0] = 2 + 2 *cosg\n", "        Q[:,0,1]=Q[:,1,0]=1+cosg\n\t        Q[:,1,1]=1\n\t        return Q\n\t    def cvxpy_constraints(self, x, state = None):\n\t        cons = [cp.square(x[0])+cp.square(x[0]+x[1])+2*x[0]*(x[0]+x[1])*\n\t                (1-cp.power(state[7],2)/2+cp.power(state[7],4)/24\n\t                 -cp.power(state[7],6)/720+cp.power(state[7],8)/40320\n\t                 -cp.power(state[7],10)/3628800+cp.power(state[7],12)/479001600)\n\t                <= self.max_M]\n\t        print(cons)\n", "        return cons\n\t    def gp_constraints(self, model, x, s):\n\t        Sq = gp.QuadExpr()\n\t        cosg = math.cos(s[7])\n\t        Sq+=x[0]*x[0]+(x[0]+x[1])*(x[0]+x[1])+2*x[0]*(x[0]+x[1])*cosg\n\t        model.addConstr(Sq <= self.max_M)\n\tif __name__ == \"__main__\":\n\t    constraint = TipConstraint(0.05)\n\t    states=th.tensor([[ 1.8320e-01, -1.5764e-01, -3.7405e-02,  1.0267e-02,  5.1187e-01,\n\t         -8.5906e-01, -4.5078e-01,  1.5164e-01,  8.1063e-01],\n", "        [-1.2841e-01, -1.0619e-01,  5.6862e-02, -6.6311e-02,  8.9963e-02,\n\t         -9.9595e-01, -1.0231e-01, -3.3542e-01, -3.3383e-01],\n\t        [-9.6614e-02,  1.2436e-01, -8.8864e-02, -2.5948e-02, -8.8712e-01,\n\t          4.6154e-01,  1.6002e-01, -8.9188e-03, -3.7412e-01],\n\t        [ 1.4109e-01,  4.5607e-02, -1.0881e-01,  2.2218e-02, -6.8323e-01,\n\t          7.3020e-01, -8.2575e-01, -7.5437e-01,  9.2613e-01],\n\t        [ 1.3156e-01,  1.1313e-01, -2.3143e-02,  8.6857e-03,  2.8196e-02,\n\t          9.9960e-01, -1.3445e-01, -4.4995e-01,  9.2404e-02],\n\t        [ 2.3610e-01,  2.7571e-02, -1.5175e-01,  4.3976e-02, -1.9267e-01,\n\t          9.8126e-01, -2.0409e-01, -6.7312e-01,  9.9564e-02],\n", "        [ 9.5906e-02, -7.2765e-02, -1.8310e-01,  1.1592e-01, -7.9309e-01,\n\t         -6.0911e-01, -4.1237e-01, -7.7382e-01, -8.4190e-01],\n\t        [-2.6493e-02, -1.1129e-01, -9.7531e-02,  2.8042e-01, -5.3531e-01,\n\t          8.4466e-01, -8.9416e-02,  4.6762e-02,  2.3578e-01],\n\t        [ 5.2737e-02, -1.6635e-01,  6.7426e-02,  2.9614e-01,  2.0968e-01,\n\t          9.7777e-01, -3.9352e-01, -3.8360e-01, -6.4619e-02],\n\t        [ 4.1588e-02, -9.6503e-02, -1.0680e-01,  4.2423e-02,  3.3904e-01,\n\t         -9.4077e-01, -2.5552e-01, -7.7012e-01,  3.5925e-02],\n\t        [-1.9536e-01, -1.0919e-01,  2.3155e-01,  4.2231e-02, -7.3762e-01,\n\t         -6.7522e-01,  4.2337e-03,  8.1003e-01,  1.4798e-01],\n", "        [-1.7193e-01,  2.6708e-01,  1.2781e-01, -1.5978e-01,  6.0622e-01,\n\t          7.9529e-01,  1.0703e-01,  6.4415e-01, -2.5370e-01],\n\t        [ 2.1081e-01,  1.8435e-01, -3.6944e-02, -6.6586e-02,  8.3082e-01,\n\t          5.5654e-01,  1.1817e-02, -4.0813e-04, -8.9231e-02],\n\t        [-2.2798e-01,  2.6716e-02,  2.2169e-01, -1.4165e-02, -3.5069e-01,\n\t         -9.3649e-01,  1.9723e-02, -1.0159e+00,  0.0000e+00],\n\t        [ 1.4427e-01,  1.2218e-01,  1.6949e-02, -1.1593e-04,  9.3954e-01,\n\t          3.4243e-01, -7.1718e-02,  1.8670e-01,  8.1036e-02],\n\t        [ 2.4308e-01,  1.5988e-01, -7.6241e-02, -5.8548e-02,  9.8955e-01,\n\t          1.4416e-01, -2.4803e-02,  2.5290e-01,  1.4468e-02],\n", "        [-2.1995e-01,  2.4489e-01,  4.7054e-02, -2.8255e-01, -9.1998e-01,\n\t          3.9196e-01, -1.0542e-01,  3.7617e-01, -6.1844e-02],\n\t        [ 9.5906e-02, -7.2765e-02, -8.4770e-02,  7.7624e-02, -9.5543e-01,\n\t          2.9520e-01, -4.4436e-01, -1.0253e+00,  0.0000e+00],\n\t        [-1.6567e-02, -1.1778e-01, -6.0481e-02,  1.3783e-01, -5.1129e-01,\n\t         -8.5941e-01, -7.7052e-02, -8.3015e-01, -7.3317e-01],\n\t        [ 1.8572e-01,  2.0678e-01, -7.9348e-03, -1.5137e-01,  9.7423e-01,\n\t         -2.2557e-01, -1.4413e-01,  3.3027e-01,  1.7400e-01],\n\t        [ 1.9805e-01, -1.2779e-01, -3.2554e-04,  6.6961e-02,  8.8112e-01,\n\t         -4.7289e-01, -7.6104e-02,  1.1023e-01, -9.5515e-02],\n", "        [ 2.4602e-02, -2.1116e-01, -2.6810e-02,  1.9340e-01,  7.7527e-01,\n\t          6.3163e-01,  1.7385e+00, -1.0000e+00,  3.9291e-14],\n\t        [ 2.0714e-01, -1.5270e-02, -6.5214e-03, -4.7860e-03,  9.2134e-01,\n\t         -3.8875e-01, -1.1639e-02,  1.8540e-01, -6.9389e-02],\n\t        [ 2.0401e-01,  1.3602e-01, -3.3934e-01, -1.8223e-01, -4.0614e-01,\n\t         -9.1381e-01, -3.0699e-01, -5.8965e-01, -7.4583e-01],\n\t        [-7.5421e-02, -1.6144e-02,  4.1996e-02,  5.0506e-03,  4.3251e-01,\n\t         -9.0163e-01,  2.7007e-02, -9.3872e-01,  1.2478e-02],\n\t        [ 1.0846e-01,  9.9337e-02, -3.3758e-02, -6.2992e-02, -1.1327e-01,\n\t          9.9356e-01, -2.7986e-01, -7.5972e-01,  3.2931e-01],\n", "        [-5.8949e-02, -2.0931e-01,  7.5779e-02,  2.1469e-01, -8.3897e-01,\n\t          5.4417e-01,  9.4286e-01, -1.0007e+00,  3.1399e-03],\n\t        [ 1.9137e-01, -2.1385e-01, -7.0773e-03,  1.8877e-01,  7.9730e-01,\n\t         -6.0358e-01, -2.6127e-02,  3.1761e-01, -8.4677e-02],\n\t        [-2.5788e-01, -1.5271e-01,  2.5780e-01,  2.1453e-01,  9.9917e-01,\n\t         -4.0851e-02, -1.1021e+00,  9.0781e-01,  1.0428e+00],\n\t        [-2.2205e-01,  3.0545e-02,  2.3723e-01, -3.8390e-02, -8.8281e-01,\n\t         -4.6973e-01, -2.8692e-01,  9.9921e-01, -7.1263e-02],\n\t        [ 1.1406e-01,  3.0687e-02,  1.2989e-03,  2.3440e-02,  1.5227e-01,\n\t          9.8834e-01, -2.1426e-02, -6.1160e-01,  3.4739e-02],\n", "        [ 5.9732e-02,  2.5014e-01, -4.0865e-02, -2.4222e-01, -7.4692e-01,\n\t          6.6491e-01,  1.3502e-01, -1.0045e+00, -2.5752e-01],\n\t        [-1.9243e-01,  1.4527e-01,  1.9735e-01, -1.6079e-01,  6.6832e-01,\n\t          7.4387e-01, -4.0629e-01, -1.0051e+00,  2.3042e-02],\n\t        [ 4.6327e-02,  3.0584e-02, -5.0195e-02, -1.7612e-01, -8.3896e-01,\n\t         -5.4420e-01, -6.8624e-01,  4.9996e-01, -6.7928e-01],\n\t        [-2.6227e-01, -1.8429e-01,  2.7370e-01,  1.7832e-01, -9.9454e-01,\n\t          1.0437e-01, -2.0936e+00,  1.0213e+00,  0.0000e+00],\n\t        [-1.4551e-01,  4.4989e-02,  1.4488e-01, -1.6085e-01,  8.7471e-01,\n\t         -4.8465e-01,  6.0875e-02, -6.6676e-01, -1.4619e-01],\n", "        [ 1.3737e-01,  1.3935e-01,  6.1545e-03, -1.3965e-01,  6.3909e-01,\n\t         -7.6913e-01, -2.9661e-02,  5.4604e-01, -5.3186e-03],\n\t        [ 2.5603e-01, -1.4094e-01, -7.4301e-02,  1.7236e-01,  9.3303e-01,\n\t         -3.5981e-01, -8.5294e-02,  3.2390e-01, -1.6248e-01],\n\t        [-1.0117e-01, -1.9024e-01,  7.8590e-02,  2.4379e-01, -9.5423e-01,\n\t         -2.9906e-01,  8.9652e-02, -8.7500e-01, -2.1984e-01],\n\t        [-1.7206e-01, -7.8571e-02,  1.6677e-01, -4.6847e-02, -8.4842e-01,\n\t         -5.2933e-01,  1.4997e-01,  6.2589e-01,  8.7443e-02],\n\t        [ 2.5588e-01, -1.6800e-01, -4.1788e-01,  2.4987e-01, -9.9468e-01,\n\t         -1.0298e-01,  9.5747e-02, -3.6193e-01, -1.8553e-01],\n", "        [ 7.6981e-02, -3.5777e-02, -8.0278e-02,  5.5422e-02, -8.6179e-01,\n\t         -5.0727e-01, -4.6081e-01, -1.0090e+00, -3.0229e-01],\n\t        [ 3.1120e-02, -2.1374e-02, -1.1562e-01,  6.8736e-02,  1.5641e-01,\n\t          9.8769e-01, -1.9128e-01,  7.5827e-01,  5.3134e-01],\n\t        [ 2.6689e-01, -1.3015e-01, -1.5124e-01, -3.3241e-02,  3.0154e-01,\n\t         -9.5345e-01,  7.7906e-02,  2.0443e-01, -9.1836e-03],\n\t        [ 1.5794e-01, -1.6047e-01, -1.7740e-01,  1.4733e-01,  8.5832e-01,\n\t         -5.1312e-01,  6.1850e-01, -9.6909e-01,  1.9071e-01],\n\t        [ 1.9805e-01, -1.2779e-01, -8.8190e-02, -5.1159e-02,  4.8637e-01,\n\t         -8.7375e-01, -1.6041e-01,  8.5268e-05, -1.8568e-01],\n", "        [-2.3627e-01, -1.8825e-01,  2.3476e-01,  3.5532e-01,  7.3609e-01,\n\t          6.7689e-01, -8.9894e-01,  4.8513e-01,  9.2152e-01],\n\t        [-8.0491e-04,  2.5744e-01, -3.3972e-02, -2.0605e-01,  7.4207e-01,\n\t          6.7032e-01, -1.0938e-02,  8.2298e-01, -4.8571e-01],\n\t        [-1.4558e-01,  5.6405e-02,  9.1604e-03,  8.7207e-02, -9.0843e-01,\n\t          4.1803e-01,  1.4570e-01, -2.5292e-01, -4.8987e-01],\n\t        [ 1.5794e-01, -1.6047e-01, -1.6855e-01,  1.7482e-01, -3.4020e-01,\n\t         -9.4035e-01,  3.9129e-01, -1.0001e+00,  5.8146e-04],\n\t        [-1.0272e-01,  1.3983e-01,  1.1016e-01, -7.2283e-02, -9.6495e-01,\n\t          2.6245e-01,  1.8502e-01, -8.6011e-01, -5.5235e-01],\n", "        [ 1.8644e-01,  1.6413e-02, -1.8327e-01, -2.6501e-02,  1.2385e-01,\n\t          9.9230e-01, -4.8969e-01,  1.0581e+00,  0.0000e+00],\n\t        [-2.2467e-01,  1.2656e-01,  1.2444e-01, -1.1338e-01, -2.6591e-01,\n\t          9.6400e-01, -8.1238e-02,  6.2769e-01, -1.5642e+00],\n\t        [ 1.0696e-01,  2.5812e-01, -1.1316e-01, -2.5016e-01,  6.6828e-01,\n\t         -7.4391e-01,  1.3040e+00, -1.0428e+00,  0.0000e+00],\n\t        [-5.2173e-02, -1.4447e-01,  1.8761e-01,  2.3472e-01,  2.7679e-01,\n\t          9.6093e-01, -1.2907e-01, -4.3628e-01,  3.6993e-01],\n\t        [-1.2841e-01, -1.0619e-01,  6.4605e-02,  1.3630e-01, -5.8368e-01,\n\t         -8.1198e-01,  1.3919e-01, -7.8860e-01,  5.8789e-01],\n", "        [ 2.1081e-01,  1.8435e-01, -4.8475e-02, -7.2713e-02,  9.6667e-01,\n\t          2.5603e-01,  1.6961e-01,  2.2385e-01, -2.1960e-01],\n\t        [ 1.1092e-01,  2.4205e-01, -2.1314e-02, -5.4704e-02,  5.6766e-01,\n\t          8.2326e-01, -6.6414e-03,  9.6259e-02, -5.9421e-02],\n\t        [ 1.9046e-01,  1.6985e-01, -2.7190e-02, -4.0215e-02,  8.4937e-01,\n\t          5.2780e-01,  6.8751e-02,  7.4697e-02, -1.0401e-01],\n\t        [ 1.0989e-01, -1.0523e-01, -3.6461e-02,  4.2261e-02, -2.8819e-01,\n\t         -9.5757e-01,  1.9405e-01,  7.3822e-01,  1.5771e-01],\n\t        [ 1.6965e-01,  2.0781e-01, -6.6745e-02, -3.5913e-01,  8.9288e-01,\n\t         -4.5029e-01, -2.0470e-01, -3.7416e-01, -5.7983e-01],\n", "        [-5.1685e-02,  5.0972e-02, -6.5238e-02, -7.5389e-02, -3.5952e-01,\n\t         -9.3314e-01, -2.3778e-01, -6.4139e-01,  6.4520e-02],\n\t        [-2.0239e-01, -8.1387e-02,  3.2574e-01,  1.2562e-01,  2.2458e-01,\n\t          9.7446e-01,  2.5393e-01, -6.0578e-01, -1.2533e-01],\n\t        [-7.5421e-02, -1.6144e-02,  5.0420e-02,  1.9265e-02,  1.9355e-01,\n\t         -9.8109e-01,  1.4493e-01, -9.7084e-01,  4.9250e-02],\n\t        [ 1.6701e-01,  9.1604e-02, -4.0769e-02, -3.0067e-02,  2.4395e-01,\n\t          9.6979e-01, -1.3173e-01, -5.1368e-01,  8.3635e-01],\n\t        [ 1.3156e-01,  1.1313e-01, -1.5625e-01, -8.0319e-02, -7.8229e-01,\n\t         -6.2291e-01, -1.3983e-01, -9.4268e-01, -4.1545e-01],\n", "        [-5.8949e-02, -2.0931e-01,  3.9744e-02,  2.0774e-01,  4.9230e-01,\n\t         -8.7042e-01, -1.9549e-01, -9.6385e-01,  5.6265e-01],\n\t        [-5.2173e-02, -1.4447e-01,  1.2499e-01,  2.6095e-01,  9.9971e-01,\n\t          2.3921e-02, -4.4858e-01,  6.3995e-01,  1.2230e+00],\n\t        [ 7.0893e-02,  1.2322e-02, -6.4323e-02, -2.0734e-02, -3.3465e-01,\n\t          9.4234e-01,  3.2380e-01, -1.0351e+00,  3.5233e-03],\n\t        [-2.0239e-01, -8.1387e-02,  2.9158e-01,  1.4889e-01, -2.0730e-01,\n\t          9.7828e-01,  2.7321e-01, -6.8015e-01, -1.1346e-01],\n\t        [ 5.2737e-02, -1.6635e-01, -5.1120e-02,  1.7999e-01,  5.2518e-01,\n\t         -8.5099e-01, -7.9028e-01,  1.0173e+00,  0.0000e+00],\n", "        [ 2.6689e-01, -1.3015e-01, -1.4593e-01, -2.6749e-02,  2.8306e-01,\n\t         -9.5910e-01, -8.8583e-02,  2.2614e-01,  9.3122e-03],\n\t        [-1.2841e-01, -1.0619e-01,  1.0166e-01, -9.9029e-02, -3.6151e-01,\n\t         -9.3237e-01, -3.7250e-01,  2.0520e-01,  1.6625e+00],\n\t        [ 2.5431e-01, -1.1782e-01, -2.4572e-01,  1.0933e-01, -1.4669e-01,\n\t          9.8918e-01, -9.8805e-02,  1.0685e+00, -2.9642e-03],\n\t        [-2.2370e-01, -1.5470e-01,  1.4518e-01, -1.8860e-02,  1.1537e-01,\n\t         -9.9332e-01,  5.0572e-01, -3.2005e-01, -5.4727e-01],\n\t        [-1.7176e-01,  1.8518e-01,  1.3073e-01, -9.9114e-02,  6.6989e-01,\n\t          7.4246e-01,  1.0031e-01,  7.2699e-01, -1.3682e-01],\n", "        [ 1.8644e-01,  1.6413e-02, -1.9379e-01,  3.6640e-02,  9.8955e-01,\n\t          1.4419e-01, -3.9230e-01,  8.3653e-01, -7.5699e-01],\n\t        [ 1.6254e-01, -1.5441e-01, -1.3764e-01, -4.6595e-02, -1.7109e-01,\n\t         -9.8526e-01, -8.7949e-02,  1.6710e-01, -2.0584e-01],\n\t        [-1.7772e-02,  5.5986e-02,  4.0756e-02, -1.2489e-01, -8.5372e-01,\n\t         -5.2073e-01,  3.2339e-02,  7.9590e-01, -3.2529e-01],\n\t        [ 1.3156e-01,  1.1313e-01,  2.7841e-02,  1.9793e-02,  8.6371e-01,\n\t          5.0399e-01, -3.9161e-02,  1.0646e-01,  8.0312e-02],\n\t        [-9.6614e-02,  1.2436e-01, -7.4913e-02, -1.5917e-01, -6.8609e-01,\n\t         -7.2752e-01, -2.0491e-02, -3.8136e-01,  1.7209e-01],\n", "        [ 5.3381e-02,  1.5518e-01,  5.2611e-02, -3.5173e-02,  9.9240e-01,\n\t          1.2306e-01,  1.2814e-01,  4.6558e-01, -7.6158e-02],\n\t        [ 2.5244e-01,  8.4347e-02, -4.0940e-01, -4.9351e-02, -9.4536e-01,\n\t         -3.2602e-01, -1.1401e+00, -3.7587e-01,  1.6452e+00],\n\t        [ 2.5780e-01, -1.5807e-01, -1.0912e-01,  5.0225e-02,  3.6717e-01,\n\t         -9.3015e-01, -2.0676e-01,  3.2349e-01, -2.6042e-01],\n\t        [-1.7057e-01, -2.5330e-01,  1.0561e-01,  2.2011e-01,  2.0264e-01,\n\t         -9.7925e-01, -2.7661e-01, -7.8458e-01,  5.1177e-01],\n\t        [ 4.8916e-02, -1.0787e-01, -5.0131e-02,  1.2707e-01,  9.3071e-01,\n\t         -3.6576e-01,  1.7344e-01,  9.8406e-01, -1.9716e-01],\n", "        [ 2.0202e-01, -3.9029e-03, -8.9125e-02, -1.7311e-01,  5.5159e-01,\n\t         -8.3411e-01, -4.0858e-02, -3.5048e-03,  2.0575e-01],\n\t        [ 2.0809e-01,  1.7872e-01, -5.5922e-02, -7.2710e-02,  9.9367e-01,\n\t          1.1230e-01,  1.1095e-01,  3.2193e-01, -7.3183e-02],\n\t        [ 1.1997e-01, -1.2303e-01, -6.6191e-02,  2.8993e-02,  9.9807e-01,\n\t          6.2107e-02,  2.0943e-02, -6.8947e-01, -4.2546e-02],\n\t        [ 9.4314e-02, -2.0381e-01, -1.2926e-01,  1.8637e-01,  5.5225e-01,\n\t         -8.3368e-01,  2.9412e-01, -8.9692e-01,  5.3850e-01],\n\t        [ 1.3156e-01,  1.1313e-01, -6.3705e-02,  6.6367e-03, -3.4719e-01,\n\t          9.3779e-01, -2.5571e-01, -5.5399e-01,  3.1866e-01],\n", "        [-2.4580e-01,  9.2489e-02,  4.7536e-02, -2.3359e-02, -9.2019e-01,\n\t          3.9148e-01, -2.9157e-01,  2.8959e-02,  3.1105e-01],\n\t        [ 2.4662e-01, -8.7919e-02, -2.9131e-01,  1.0427e-01, -2.7906e-01,\n\t         -9.6027e-01,  5.1581e-01, -8.8904e-01,  1.6501e-01],\n\t        [ 4.1381e-02,  9.9785e-02,  1.3328e-01, -1.9956e-01,  9.7123e-01,\n\t         -2.3813e-01, -1.6613e-01, -1.6541e-01,  5.2801e-01],\n\t        [ 4.9837e-02, -3.2174e-02, -3.3871e-02,  2.2361e-02,  4.5312e-02,\n\t          9.9897e-01,  1.2342e-01, -9.9689e-01, -2.0943e-03],\n\t        [ 4.6537e-02,  2.6297e-01, -3.5891e-02, -5.3864e-02,  1.3673e-01,\n\t          9.9061e-01, -3.2710e-02,  5.7895e-02,  1.1680e-01],\n", "        [ 1.9656e-02,  7.1967e-02,  1.8609e-02, -1.0322e-01, -6.0140e-01,\n\t         -7.9895e-01,  3.1162e-02,  8.7325e-01, -3.4123e-01],\n\t        [-5.9224e-02, -1.9975e-01,  9.1893e-02,  1.6842e-01,  6.4324e-01,\n\t          7.6567e-01,  3.6364e-01, -9.6045e-01, -9.9280e-01],\n\t        [ 4.6537e-02,  2.6297e-01, -9.1370e-02, -2.7300e-01, -1.4171e-01,\n\t          9.8991e-01, -4.7941e-01,  9.6689e-01,  1.1510e+00],\n\t        [ 1.5794e-01, -1.6047e-01, -2.7301e-02, -2.8689e-04,  4.5119e-01,\n\t         -8.9243e-01, -2.4893e-01,  1.0593e-01, -7.7573e-02]])\n\t    Q = constraint.getQ(states)\n\t    v = th.tensor([[ 1.1877, -2.3243],\n", "        [ 1.7959, -4.3229],\n\t        [ 0.7542, -1.9867],\n\t        [ 0.9488, -1.2668],\n\t        [ 0.3369, -0.3681],\n\t        [-1.5717,  2.3981],\n\t        [ 2.3537, -2.5683],\n\t        [-1.5644,  3.7083],\n\t        [ 0.7733, -1.5146],\n\t        [ 4.5917, -8.2694],\n\t        [-0.1183, -1.0292],\n", "        [ 1.2631, -2.6133],\n\t        [-0.5344,  1.4256],\n\t        [-2.9314,  5.5788],\n\t        [ 0.4514, -1.0473],\n\t        [ 0.3028, -0.6701],\n\t        [-1.3386,  1.1386],\n\t        [-1.4933,  2.7295],\n\t        [ 4.0325, -7.6104],\n\t        [-0.7822,  2.7495],\n\t        [-0.0188,  0.5993],\n", "        [-3.4582,  6.0066],\n\t        [ 0.0167,  0.0568],\n\t        [-1.6121,  1.3926],\n\t        [-0.9225,  1.0189],\n\t        [ 0.6311, -0.5272],\n\t        [ 1.2427, -2.7783],\n\t        [ 1.0319, -2.5678],\n\t        [-5.2975, 10.0474],\n\t        [ 1.4269, -2.9111],\n\t        [ 0.3023, -0.9060],\n", "        [-2.8003,  5.5046],\n\t        [ 0.3941, -1.4517],\n\t        [ 1.8764, -3.7361],\n\t        [-2.7421,  4.9254],\n\t        [-0.7130,  0.3959],\n\t        [ 0.9533, -0.9312],\n\t        [ 0.5112, -1.0822],\n\t        [ 2.9078, -5.0902],\n\t        [ 0.8757, -2.0521],\n\t        [ 3.8278, -7.8939],\n", "        [ 1.2113, -2.0263],\n\t        [-2.8169,  5.3518],\n\t        [ 0.8417, -1.0173],\n\t        [-3.6530,  5.9265],\n\t        [-5.9884, 11.9783],\n\t        [-3.8574,  9.2997],\n\t        [ 1.2505, -1.8271],\n\t        [-0.9896,  2.3350],\n\t        [ 3.4117, -4.8462],\n\t        [-0.1826,  1.3147],\n", "        [-0.7548,  1.0839],\n\t        [-1.2892,  2.9177],\n\t        [-2.1583,  3.2695],\n\t        [-2.7347,  5.7706],\n\t        [-0.6775,  1.0023],\n\t        [ 0.5251, -1.4952],\n\t        [-0.0960,  0.5027],\n\t        [ 0.4441, -1.2032],\n\t        [ 0.5062, -1.0083],\n\t        [ 2.8411, -7.2339],\n", "        [ 0.2819, -2.3598],\n\t        [ 2.6288, -4.4024],\n\t        [-2.0797,  3.3121],\n\t        [ 0.9550, -2.2754],\n\t        [-1.2370,  0.9994],\n\t        [ 2.3387, -3.4464],\n\t        [-2.7986,  5.9041],\n\t        [-1.1930,  2.0790],\n\t        [ 2.5329, -4.1267],\n\t        [ 2.1468, -3.7458],\n", "        [-0.2907,  1.5725],\n\t        [ 2.2964, -4.7565],\n\t        [-6.6629, 12.4310],\n\t        [ 1.6675, -3.2404],\n\t        [ 1.9704, -4.1947],\n\t        [ 0.3274, -0.3166],\n\t        [-4.1403,  8.6956],\n\t        [-1.8100,  3.6279],\n\t        [ 1.1904, -2.6021],\n\t        [-1.3071,  1.7721],\n", "        [ 0.8844, -1.8934],\n\t        [ 1.8460, -3.0603],\n\t        [-2.9637,  6.1326],\n\t        [ 2.1313, -3.7318],\n\t        [-3.3370,  5.8759],\n\t        [ 0.0935,  0.0552],\n\t        [ 0.8720, -2.2909],\n\t        [-0.8439,  2.3420],\n\t        [-1.0208,  0.9889],\n\t        [ 0.3767, -0.4332],\n", "        [-0.0614,  0.7950],\n\t        [ 3.1372, -5.7051],\n\t        [-1.2770,  3.3321],\n\t        [-1.0459,  1.6059],\n\t        [ 0.3424, -0.7144],\n\t        [-0.5434,  2.0249],\n\t        [-3.5058,  5.6909],\n\t        [-4.1882,  7.5286],\n\t        [-5.5352, 11.1384]])\n\t    value = (v[:,:,None]*Q*v[:,None,:]).sum(dim=2).sum(dim=1)\n", "    L = th.sqrt((value+1e-9)/constraint.max_M)\n\t    print(value, L)\n"]}
{"filename": "action_constrained_rl/constraint/sphere_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .constraint import Constraint\n\timport numpy as np\n\timport torch as th\n\timport cvxpy as cp\n\timport gurobipy as gp\n\tclass SphericalConstraint(Constraint):\n\t    \"\"\"\n\t    Action Constraints with the from $|a| \\leq M$\n", "    \"\"\"\n\t    def __init__(self, a_dim, r2):\n\t        super().__init__(a_dim)\n\t        self.r2 = r2\n\t        self.r = r2 ** 0.5\n\t    def isConstraintSatisfied(self, state, a, err=1e-3):\n\t        return np.sum(np.square(a)) <= self.r2 + err\n\t    def enforceConstraint(self, state, a):\n\t        return min(self.r / np.linalg.norm(a), 1.) * a\n\t    def numConstraints(self):\n", "        return 1\n\t    def getL(self, states, centers, v, get_grad:bool = False):\n\t        L = v.norm(dim=1)/self.r\n\t        if not get_grad:\n\t            return L\n\t        else:\n\t            return L, v/L[:,None]/self.r**2\n\t    def constraintViolation(self, state, a, err=1e-3, normalize=False):\n\t        return np.expand_dims(np.maximum(0.0, np.sqrt(np.sum(np.square(a))) - self.r - err),0)\n\t    def constraintViolationBatch(self, states, actions):\n", "        return th.maximum(actions.norm(dim=1)-self.r, th.tensor(0.))\n\t    def get_center(self, state):\n\t        return np.zeros(self.a_dim)\n\t    def cvxpy_constraints(self, x, state = None):\n\t        return [cp.sum_squares(x) <= self.r2]\n\t    def gp_constraints(self, model, x, state = None):\n\t        Sq = gp.QuadExpr()\n\t        for i in range(self.a_dim):\n\t            Sq+=x[i]*x[i]\n\t        model.addConstr(Sq <= self.r2)\n"]}
{"filename": "action_constrained_rl/constraint/box_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .constraint import LinearConstraint\n\timport torch\n\timport cvxpy as cp\n\timport gurobipy as gp\n\tfrom ..cvxpy_variables import CVXPYVariables\n\tfrom .power_constraint import make_compatible\n\tclass BoxConstraint(LinearConstraint):\n\t    \"\"\"\n", "    Action Constraints with the from\n\t    $|a_i| \\leq 1`$ \n\t    \"\"\"\n\t    def __init__(self, a_dim):\n\t        super().__init__(a_dim, -1)\n\t        eyes = torch.eye(self.a_dim)\n\t        self.C_value = torch.concat((eyes, -eyes), axis = 0)\n\t        self.d_value = torch.ones(2*self.a_dim)\n\t    def tensor_C(self, state):\n\t        size = state.shape[0]\n", "        self.C_value = make_compatible(self.C_value, state)\n\t        return self.C_value.repeat(size, 1, 1)\n\t    def tensor_d(self, state):\n\t        size = state.shape[0]\n\t        self.d_value = make_compatible(self.d_value, state)\n\t        return self.d_value.repeat(size, 1)\n\t    def numConstraints(self):\n\t        return 2 * self.a_dim\n\t    def E(self, state):\n\t        return self.C(state)\n", "    def f(self, state):\n\t        return self.d(state)\n\t    def cvxpy_constraints(self, x, state):\n\t        cons = []\n\t        for i in range(self.a_dim):\n\t            cons.append(x[i] <= 1.)\n\t            cons.append(-x[i] <= 1.)\n\t        return cons\n\t    def gp_constraints(self, model, x, s):\n\t        pass\n"]}
{"filename": "action_constrained_rl/constraint/sin2_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .quadratic_constraint import QuadraticConstraint\n\timport torch as th\n\timport cvxpy as cp\n\timport gurobipy as gp\n\timport math\n\tfrom ..cvxpy_variables import CVXPYVariables\n\tfrom .power_constraint import make_compatible\n\tclass Sin2Constraint(QuadraticConstraint):\n", "    \"\"\"\n\t    State-dependent Action Constraints with the from\n\t    $\\sum a_i^2\\sin^2\\theta_i  \\leq M$ where $\\theta_i$ is the angle corresponding to $a_i$\n\t    \"\"\"\n\t    def __init__(self, index, max_M, s_dim, **kargs):\n\t        self.index = index\n\t        super().__init__(max_M, len(index), s_dim, **kargs)\n\t    def getTensorQ(self, states):\n\t        Q=th.zeros((states.shape[0],self.a_dim,self.a_dim),device = states.device)\n\t        for i in range(self.a_dim):\n", "            sin2 = th.sin(states[:,self.index[i]])**2\n\t            Q[:,i,i] = sin2\n\t        return Q\n\t    def cvxpy_constraints(self, x, state = None):\n\t        pass\n\t    def gp_constraints(self, model, x, s):\n\t        Sq = gp.QuadExpr()\n\t        for i in range(self.a_dim):\n\t            sin2 = math.sin(s[self.index[i]])**2\n\t            Sq+=sin2*x[i]*x[i]\n", "        model.addConstr(Sq <= self.max_M)\n"]}
{"filename": "action_constrained_rl/constraint/normalize_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\tdef normalizeConstraint(A, b):\n\t    norms = (np.linalg.norm(A, axis=1))\n\t    return (A/norms[:,None], b/norms)\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    A = np.array([[3.0, 4.0], [50.0, 3.0]])\n\t    b = np.array([1.0, 2.0])\n", "    norms = (np.linalg.norm(A, axis=1))\n\t    print(A / norms)\n\t    print(b / norms)\n"]}
{"filename": "action_constrained_rl/constraint/MA_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .constraint import LinearConstraint\n\timport torch as th\n\timport cvxpy as cp\n\timport gurobipy as gp\n\timport math\n\tclass MAConstraint(LinearConstraint):\n\t    \"\"\"\n\t    State-dependent Action Constraints with the from\n", "    $w_0a_0\\sin(theta_0+theta_1+theta_2)+w_3a_3\\sin(\\theta_3+\\theta_4+\\theta_5) \\leq M, |a_i| \\leq 1`$\n\t    \"\"\"\n\t    def __init__(self, max_power, **kargs):\n\t        super().__init__(6, 17, **kargs)\n\t        self.max_power = max_power\n\t        self.d_value = th.hstack((self.max_power * th.ones(1), th.ones(2*self.a_dim)))\n\t    def tensor_C(self, state):\n\t        size = state.shape[0]\n\t        device = state.device\n\t        C = th.zeros((size, 1, 6), device = device)\n", "        C[:,0,0] = state[:,11]*th.sin(state[:,2]+state[:,3]+state[:,4])\n\t        C[:,0,3] = state[:,14]*th.sin(state[:,5]+state[:,6]+state[:,7])\n\t        eyes = th.eye(self.a_dim, device = device).repeat(size,1,1) \n\t        return th.concat((C, eyes, -eyes), axis = 1)\n\t    def tensor_d(self, state):\n\t        size = state.shape[0]\n\t        if self.d_value.device != state.device:\n\t            self.d_value = self.d_value.to(state.device)\n\t        return self.d_value.repeat(size, 1)\n\t    def numConstraints(self):\n", "        return 1 + 2 * self.a_dim\n\t    def E(self, state):\n\t        return self.C(state)\n\t    def f(self, state):\n\t        return self.d(state)\n\t    def cvxpy_constraints(self, x, state):\n\t        pass\n\t    def numConstraints(self):\n\t        return 1 + 2 * self.a_dim\n\t    def gp_constraints(self, model, x, s):\n", "        model.addConstr(s[11]*math.sin(s[2]+s[3]+s[4])*x[0]\n\t                        + s[14]*math.sin(s[5]+s[6]+s[7])*x[3]<= self.max_power)\n"]}
{"filename": "action_constrained_rl/constraint/quadratic_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom .constraint import Constraint, to_tensors\n\timport torch as th\n\timport numpy as np\n\timport math\n\tfrom abc import abstractmethod\n\timport cvxpy as cp\n\timport gurobipy as gp\n\tfrom ..cvxpy_variables import CVXPYVariables\n", "from .power_constraint import make_compatible\n\tclass QuadraticConstraint(Constraint):\n\t    \"\"\"\n\t    State-dependent Action Constraints with the from\n\t    $`\\sum Q_ij a_i a_j \\leq M`$ \n\t    \"\"\"\n\t    def __init__(self, max_M, a_dim, s_dim, **kargs):\n\t        super().__init__(a_dim, s_dim, **kargs)\n\t        self.max_M = max_M\n\t        self.sr_max_M = math.sqrt(max_M)\n", "    @abstractmethod\n\t    def getTensorQ(self, state):\n\t        pass\n\t    def getQ(self, state):\n\t        if isinstance(state, np.ndarray):\n\t            return self.getTensorQ(to_tensors(state)).numpy()[0]\n\t        return self.getTensorQ(state)    \n\t    def isConstraintSatisfied(self, state, a, err=1e-2):\n\t        Q = self.getQ(state)\n\t        return a.transpose()@Q@a <= self.max_M + err\n", "    def enforceConstraint(self, state, a):\n\t        Q = self.getQ(state)\n\t        value = a.transpose()@Q@a\n\t        if value <= self.max_M:\n\t            return a\n\t        else:\n\t            return math.sqrt(self.max_M / value) * a\n\t    def numConstraints(self):\n\t        return 1\n\t    def getL(self, states, centers, v, get_grad:bool = False):\n", "        Q = self.getQ(states)\n\t        value = (v[:,:,None]*Q*v[:,None,:]).sum(dim=2).sum(dim=1).clamp(min=1e-3)\n\t        L = th.sqrt(value/self.max_M)\n\t        if not get_grad:\n\t            return L\n\t        else:\n\t            return L, (Q*v[:,None,:]).sum(dim=2)/L[:,None]/self.max_M\n\t    def constraintViolation(self, state, a, err=1e-3, normalize=False):\n\t        Q = self.getQ(state)\n\t        scale = np.sqrt(self.a_dim / np.trace(Q)+1e-6)\n", "        value = a.transpose()@Q@a\n\t        return np.expand_dims(np.maximum(0.0, scale*(np.sqrt(value) - self.sr_max_M) - err),0)\n\t    def constraintViolationBatch(self, states, actions):\n\t        Q = self.getQ(states)\n\t        scale = th.sqrt(self.a_dim / Q.diagonal(dim1=1, dim2=2).sum(axis=1)[:,None,None]+1e-6)\n\t        value = (actions[:,:,None]*Q*actions[:,None,:]).sum(dim=2).sum(dim=1).clamp(min=1e-3)\n\t        return th.maximum(scale*(th.sqrt(value)-self.sr_max_M), th.tensor(0.))\n\t    def get_center(self, state):\n\t        return np.zeros(self.a_dim)\n"]}
{"filename": "action_constrained_rl/nn/additional_layer_sac_policy.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Type, Union, Tuple\n\timport gym\n\timport torch as th\n\timport numpy as np\n\tfrom torch import nn\n\tfrom stable_baselines3.common.type_aliases import Schedule\n\tfrom stable_baselines3.common.policies import BasePolicy\n\tfrom stable_baselines3.sac.policies import Actor, SACPolicy\n", "from stable_baselines3.common.preprocessing import get_action_dim\n\tfrom stable_baselines3.common.torch_layers import (\n\t    BaseFeaturesExtractor,\n\t    FlattenExtractor,\n\t    create_mlp,\n\t)\n\tfrom .additional_layer_policy import TruncateExtractor\n\t# CAP the standard deviation of the actor\n\tLOG_STD_MAX = 2\n\tLOG_STD_MIN = -20\n", "class AdditionalLayerSACActor(Actor):\n\t    \"\"\"\n\t    Actor for SAlpha or SRad\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        net_arch: List[int],\n\t        features_extractor: nn.Module,\n", "        features_dim: int,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        use_sde: bool = False,\n\t        log_std_init: float = -3,\n\t        full_std: bool = True,\n\t        use_expln: bool = False,\n\t        clip_mean: float = 2.0,\n\t        normalize_images: bool = True,\n\t        distribution_class = None, # deformed distribution class\n\t        constraint = None\n", "    ):\n\t        assert distribution_class != None\n\t        super(Actor, self).__init__(\n\t            observation_space,\n\t            action_space,\n\t            features_extractor=features_extractor,\n\t            normalize_images=normalize_images,\n\t            squash_output=True,\n\t        )\n\t        # Save arguments to re-create object at loading\n", "        self.use_sde = use_sde\n\t        self.sde_features_extractor = None\n\t        self.net_arch = net_arch\n\t        action_dim = get_action_dim(self.action_space)\n\t        self.a_dim = action_dim\n\t        self.features_dim = features_dim\n\t        self.activation_fn = activation_fn\n\t        self.log_std_init = log_std_init\n\t        self.use_expln = use_expln\n\t        self.full_std = full_std\n", "        self.clip_mean = clip_mean\n\t        latent_pi_net = create_mlp(features_dim, -1, net_arch, activation_fn)\n\t        self.latent_pi = nn.Sequential(*latent_pi_net)\n\t        last_layer_dim = net_arch[-1] if len(net_arch) > 0 else features_dim\n\t        if self.use_sde:\n\t            self.action_dist = distribution_class(\n\t                action_dim, constraint, full_std=full_std, use_expln=use_expln, learn_features=True, squash_output=False\n\t            )\n\t            self.mu, self.log_std = self.action_dist.proba_distribution_net(\n\t                latent_dim=last_layer_dim, latent_sde_dim=last_layer_dim, log_std_init=log_std_init\n", "            )\n\t            # Avoid numerical issues by limiting the mean of the Gaussian\n\t            # to be in [-clip_mean, clip_mean]\n\t            if clip_mean > 0.0:\n\t                self.mu = nn.Sequential(self.mu, nn.Hardtanh(min_val=-clip_mean, max_val=clip_mean))\n\t        else:\n\t            self.action_dist = distribution_class(action_dim, constraint)\n\t            self.mu = nn.Linear(last_layer_dim, action_dim)\n\t            self.log_std = nn.Linear(last_layer_dim, action_dim)\n\t        self.flatten = nn.Flatten()\n", "    def get_action_dist_params(self, obs: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Dict[str, th.Tensor]]:\n\t        \"\"\"\n\t        Get the parameters for the action distribution.\n\t        :param obs:\n\t        :return:\n\t            Mean, standard deviation and optional keyword arguments.\n\t        \"\"\"\n\t        # split obs to features and centers\n\t        features = self.extract_features(obs)\n\t        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n", "        latent_pi = self.latent_pi(features)\n\t        mean_actions = self.mu(latent_pi)\n\t        if self.use_sde:\n\t            return mean_actions, self.log_std, dict(states=features, centers=centers, latent_sde=latent_pi)\n\t        # Unstructured exploration (Original implementation)\n\t        log_std = self.log_std(latent_pi)\n\t        # Original Implementation to cap the standard deviation\n\t        log_std = th.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n\t        return mean_actions, log_std, {\"states\": features, \"centers\": centers}\n\tclass AdditionalLayerSACPolicy(SACPolicy):\n", "    \"\"\"\n\t    Policy for SAlpha or SRad\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        lr_schedule: Schedule,\n\t        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n", "        use_sde: bool = False,\n\t        log_std_init: float = -3,\n\t        sde_net_arch: Optional[List[int]] = None,\n\t        use_expln: bool = False,\n\t        clip_mean: float = 2.0,\n\t        features_extractor_class: Type[BaseFeaturesExtractor] = TruncateExtractor,\n\t        features_extractor_kwargs: Optional[Dict[str, Any]] = {},\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n", "        n_critics: int = 2,\n\t        share_features_extractor: bool = False,\n\t        **kwargs\n\t    ):\n\t        self.additional_actor_kwargs = kwargs\n\t        a_dim = get_action_dim(action_space)\n\t        features_extractor_kwargs.update({\"a_dim\": a_dim})\n\t        super(AdditionalLayerSACPolicy, self).__init__(\n\t            observation_space,\n\t            action_space,\n", "            lr_schedule,\n\t            net_arch,\n\t            activation_fn,\n\t            use_sde,\n\t            log_std_init,\n\t            sde_net_arch,\n\t            use_expln,\n\t            clip_mean,\n\t            features_extractor_class,\n\t            features_extractor_kwargs,\n", "            normalize_images,\n\t            optimizer_class,\n\t            optimizer_kwargs,\n\t            n_critics,\n\t            share_features_extractor,\n\t        )\n\t    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None) -> Actor:\n\t        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n\t        actor_kwargs.update(self.additional_actor_kwargs)\n\t        return AdditionalLayerSACActor(**actor_kwargs).to(self.device)\n", "if __name__ == \"__main__\":\n\t    import numpy as np\n\t    from stable_baselines3 import DDPG\n\t    from ..ddpg.projection_ddpg import ProjectionDDPG\n\t    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\t    from ..env_wrapper.constraint_wrapper import ConstraintEnvWrapper\n\t    cons = HalfCheetahDynamicConstraint()\n\t    env = gym.make(\"HalfCheetah-v2\")\n\t    env = ConstraintEnvWrapper(cons, env)\n", "    n_actions = env.action_space.shape[-1]\n\t    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.01 * np.ones(n_actions))\n\t    model = ProjectionDDPG(cons, AdditionalLayerPolicy, env, action_noise=action_noise, verbose=2, batch_size=8, policy_kwargs = {\"constraint\": cons, \"layer_type\": None})\n\t    model.learn(total_timesteps=1000)\n"]}
{"filename": "action_constrained_rl/nn/additional_layer_actor.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Type, Union\n\timport gym\n\timport torch as th\n\timport numpy as np\n\tfrom torch import nn\n\tfrom stable_baselines3.common.policies import BasePolicy, ContinuousCritic, register_policy\n\tfrom stable_baselines3.td3.policies import Actor\n\tfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n", "from stable_baselines3.common.preprocessing import get_action_dim\n\tfrom stable_baselines3.common.torch_layers import (\n\t    BaseFeaturesExtractor,\n\t    CombinedExtractor,\n\t    FlattenExtractor,\n\t    NatureCNN,\n\t    create_mlp,\n\t    get_actor_critic_arch,\n\t)\n\tclass AdditionalLayerActor(Actor):\n", "    \"\"\"\n\t    Actor network (policy) for DAlpha, DRad.\n\t    :param observation_space: Obervation space\n\t    :param action_space: Action space\n\t    :param net_arch: Network architecture\n\t    :param features_extractor: Network to extract features\n\t        (a CNN when using images, a nn.Flatten() layer otherwise)\n\t    :param features_dim: Number of features\n\t    :param activation_fn: Activation function\n\t    :param normalize_images: Whether to normalize images or not,\n", "         dividing by 255.0 (True by default)\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        net_arch: List[int],\n\t        features_extractor: nn.Module,\n\t        features_dim: int,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n", "        normalize_images: bool = True,\n\t        squash_output: bool = False,\n\t        constraint = None,\n\t        layer_type = None,\n\t    ):\n\t        if constraint is None:\n\t            raise (\"constraint should not be None\")\n\t        super(Actor, self).__init__(\n\t            observation_space,\n\t            action_space,\n", "            features_extractor=features_extractor,\n\t            normalize_images=normalize_images,\n\t            squash_output=squash_output,\n\t        )\n\t        self.net_arch = net_arch\n\t        action_dim = get_action_dim(self.action_space)\n\t        self.a_dim = action_dim\n\t        self.features_dim = features_dim\n\t        self.activation_fn = activation_fn\n\t        self.constraint = constraint\n", "        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=squash_output)\n\t        # Deterministic action\n\t        self.first_layers = nn.Sequential(*actor_net)\n\t        self.flatten = nn.Flatten()\n\t        self.additional_layer = layer_type(self.constraint)\n\t    def forward(self, obs: th.Tensor) -> th.Tensor:\n\t        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n\t        # split obs to features and centers\n\t        features = self.extract_features(obs)\n\t        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n", "        output = self.first_layers(features)\n\t        output = self.additional_layer(output, features, centers)\n\t        return output\n\t    def undeformed_predict(self, obs: np.ndarray) -> th.Tensor:\n\t        # return output before the final layer\n\t        obs, _ = self.obs_to_tensor(obs)\n\t        features = self.extract_features(obs)\n\t        output = self.first_layers(features)\n\t        return output.cpu().detach().numpy()\n\t    def deform_action(self, actions: np.ndarray, obs: np.ndarray) -> th.Tensor:\n", "        # apply the final layer\n\t        obs, _ = self.obs_to_tensor(obs)\n\t        features = self.extract_features(obs)\n\t        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n\t        actions = th.tensor(actions, device = self.device).float()\n\t        return self.additional_layer(actions, features, centers).cpu().detach().numpy()\n"]}
{"filename": "action_constrained_rl/nn/__init__.py", "chunked_list": []}
{"filename": "action_constrained_rl/nn/additional_layer_policy.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Type, Union\n\timport gym\n\timport torch as th\n\timport numpy as np\n\tfrom torch import nn\n\tfrom stable_baselines3.td3.policies import TD3Policy\n\tfrom stable_baselines3.common.torch_layers import (\n\t    BaseFeaturesExtractor,\n", ")\n\tfrom stable_baselines3.common.preprocessing import get_flattened_obs_dim, get_action_dim\n\tfrom stable_baselines3.common.type_aliases import Schedule\n\tfrom .additional_layer_actor import AdditionalLayerActor\n\tclass TruncateExtractor(BaseFeaturesExtractor):\n\t    \"\"\"\n\t    Extractor to take original observations from concatenated observations with centers\n\t    \"\"\"\n\t    def __init__(self, observation_space: gym.Space, a_dim: int):\n\t        super().__init__(observation_space, get_flattened_obs_dim(observation_space) - a_dim)\n", "        self.flatten = nn.Flatten()\n\t        self.a_dim = a_dim\n\t    def forward(self, observations: th.Tensor) -> th.Tensor:\n\t        return self.flatten(observations)[:,:-self.a_dim]\n\tclass AdditionalLayerPolicy(TD3Policy):\n\t    \"\"\"\n\t    policy for DAlpha, DRad\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        lr_schedule: Schedule,\n\t        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        features_extractor_class: Type[BaseFeaturesExtractor] = TruncateExtractor,\n\t        features_extractor_kwargs: Optional[Dict[str, Any]] = {},\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n", "        n_critics: int = 2,\n\t        share_features_extractor: bool = True,\n\t        **kwargs\n\t    ):\n\t        self.kwargs = kwargs\n\t        a_dim = get_action_dim(action_space)\n\t        features_extractor_kwargs.update({\"a_dim\": a_dim})\n\t        super(AdditionalLayerPolicy, self).__init__(\n\t            observation_space,\n\t            action_space,\n", "            lr_schedule,\n\t            net_arch,\n\t            activation_fn,\n\t            features_extractor_class,\n\t            features_extractor_kwargs,\n\t            normalize_images,\n\t            optimizer_class,\n\t            optimizer_kwargs,\n\t            n_critics,\n\t            share_features_extractor,\n", "        )\n\t    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None):\n\t        self.actor_kwargs.update(self.kwargs)\n\t        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n\t        return AdditionalLayerActor(**actor_kwargs).to(self.device)\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    from stable_baselines3 import DDPG\n\t    from ..ddpg.projection_ddpg import ProjectionDDPG\n\t    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n", "    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\t    from ..env_wrapper.constraint_wrapper import ConstraintEnvWrapper\n\t    cons = HalfCheetahDynamicConstraint()\n\t    env = gym.make(\"HalfCheetah-v2\")\n\t    env = ConstraintEnvWrapper(cons, env)\n\t    n_actions = env.action_space.shape[-1]\n\t    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.01 * np.ones(n_actions))\n\t    model = ProjectionDDPG(cons, AdditionalLayerPolicy, env, action_noise=action_noise, verbose=2, batch_size=8, policy_kwargs = {\"constraint\": cons, \"layer_type\": None})\n\t    model.learn(total_timesteps=1000)\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/chebyshev_center.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport gurobipy as gp\n\timport numpy as np\n\tdef calc_chebyshev_center(C, d):\n\t    m, n = C.shape\n\t    with gp.Model() as model:\n\t        x = []\n\t        for i in range(n):\n\t            x.append(model.addVar(lb = -1, ub = 1, vtype = gp.GRB.CONTINUOUS))\n", "        r = model.addVar(lb = 0, ub = gp.GRB.INFINITY, vtype = gp.GRB.CONTINUOUS)\n\t        model.setObjective(r, sense = gp.GRB.MAXIMIZE)\n\t        norms = np.linalg.norm(C, axis=1)\n\t        for j in range(m):\n\t            exp = gp.LinExpr()\n\t            for i in range(n):\n\t                exp+=C[j,i]*x[i]\n\t            exp+=norms[j]*r\n\t            model.addConstr(exp <= d[j])\n\t        model.optimize()\n", "        x_value = np.array(model.X[0:n])\n\t    return x_value\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/radial_squash.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom .analytic_center import calc_analytic_center\n\tclass SquashLayer(torch.nn.Module):\n\t    def __init__(self, cons):\n\t        super(SquashLayer, self).__init__()\n\t        self.cons = cons\n", "    def forward(self, actions, states, centers):\n\t        v = actions - centers\n\t        L = self.cons.getL(states, centers, v)\n\t        return centers + (torch.tanh(L) / (L+1e-9))[:,None] * v\n\t# unused layer\n\tclass SmoothSquashLayer(torch.nn.Module):\n\t    def __init__(self, cons):\n\t        super(SmoothSquashLayer, self).__init__()\n\t        self.cons = cons\n\t    def forward(self, actions, states, centers):\n", "        C=self.cons.C(states)\n\t        d=self.cons.d(states)\n\t        v = actions - centers\n\t        b = torch.maximum((v[:,None,:]*C).sum(axis=2) / (d-(centers[:,None,:]*C).sum(axis=2)), torch.tensor(0.))\n\t        r = torch.linalg.norm(v, axis = 1)\n\t        p = torch.minimum(2*torch.exp(r).detach(),torch.tensor(10.))\n\t        return centers + (torch.pow(torch.pow(b,p[:,None]).sum(axis=1),-1/p) * torch.tanh(torch.linalg.norm(b, axis = 1)))[:,None] * v\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n", "    cons = HalfCheetahDynamicConstraint()\n\t    state = np.repeat(10.0, 17)\n\t    C=cons.C(state)\n\t    d=cons.d(state)\n\t    layer=SquashLayer(cons)\n\t    print(C,d)\n\t    for t in range(100):\n\t        action=np.random.rand(6)\n\t        print(action)\n\t        result = layer.forward(action, state)\n", "        print(result)\n\t        assert cons.isConstraintSatisfied(state, result)\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/visualize.py", "chunked_list": ["import matplotlib\n\timport matplotlib.pyplot as plt\n\timport matplotlib.patches as patches\n\timport numpy as np\n\timport torch\n\tfrom ...constraint import LinearConstraint\n\tclass TestConstraint(LinearConstraint):\n\t    def __init__(self, err=1e-3):\n\t        self.err = err\n\t    def numConstraints(self):\n", "        return 4\n\t    def E(self, state):\n\t        return self.C(state)\n\t    def f(self, state):\n\t        return self.d(state)\n\t    def tensor_C(self, state):\n\t        return torch.tensor([[1.,1.],[-2.,1.],[0.,-1.],[1.,0.]]).repeat(state.shape[0], 1, 1)\n\t    def tensor_d(self, state):\n\t        return torch.tensor([1.,1.,1.,1.]).repeat(state.shape[0], 1)\n\tif __name__ == \"__main__\":\n", "    from .alpha_projection import AlphaProjectionLayer\n\t    from .radial_shrinkage import ShrinkageLayer\n\t    from .radial_shrinkage import SmoothShrinkageLayer\n\t    import torch\n\t    import matplotlib.path\n\t    cons = TestConstraint()\n\t    a_layer = AlphaProjectionLayer(cons)\n\t    s_layer = ShrinkageLayer(cons)\n\t    ss_layer = SmoothShrinkageLayer(cons)\n\t    N=100\n", "    actions = torch.Tensor(N, 2)\n\t    states = torch.Tensor(N, 2)\n\t    for i in range(N):\n\t        actions[i] = torch.Tensor(3 * torch.rand(2) - torch.tensor([1.5, 1.5]))\n\t        states[i] = torch.Tensor([0.,0.])\n\t    for layer in [ss_layer, s_layer, a_layer]:\n\t        res = layer.forward(actions, states)\n\t        fig, ax = plt.subplots()\n\t        ax.add_patch(patches.Polygon([[0,1], [-1,-1], [1, -1], [1,0]], fill = False))\n\t        plt.xlim(-2,2)\n", "        plt.ylim(-2,2)\n\t        for i in range(N):\n\t            ver1 = actions[i].detach().numpy()\n\t            ver2 = res[i].detach().numpy()\n\t            path = matplotlib.path.Path([ver1, ver2])\n\t            ax.add_patch(patches.Circle(ver1, radius = 0.01, color = 'r'))\n\t            ax.add_patch(patches.Circle(ver2, radius = 0.01, color = 'b'))\n\t            ax.add_patch(patches.PathPatch(path))\n\t        plt.show()\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/alpha_distribution.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n\tfrom stable_baselines3.common.distributions import DiagGaussianDistribution, StateDependentNoiseDistribution\n\timport torch as th\n\timport numpy as np\n\timport sys\n\tfrom torch.special import erf\n\tsqrt2 = np.sqrt(2.)\n\tlog2 = np.log(2.)\n", "sqrtpi = np.sqrt(np.pi)\n\tsqrt2pi = sqrt2*sqrtpi\n\tlgsqrt2pi = np.log(sqrt2pi)\n\tlg2sqrtpi = np.log(2*sqrtpi)\n\tLOG_STD_MAX = 2\n\t# LOG_PROB_MAX = 1e6\n\tdef radial_integral_gaussian(means, log_std, centers, v, epsilon:float = 1e-300):\n\t    # calculate integration of gaussian distribution\n\t    dtp=means.dtype\n\t    log_std=log_std.to(th.float64)\n", "    v=v.to(th.float64)\n\t    dim = means.shape[1]\n\t    std = log_std.exp()\n\t    a = v / std / sqrt2\n\t    b = (centers-means).to(th.float64) / std / sqrt2\n\t    A = a.norm(dim=1)\n\t    na = a / A[:,None]\n\t    B = (na*b).sum(dim=1)\n\t    Cross = na[:,:,None]*b[:,None,:]-b[:,:,None]*na[:,None,:]\n\t    C = 1./2. * Cross.square().sum(dim=(1,2)) # C = (b*b).sum(axis=1) - B*B\n", "    log_prob = dim*th.log(v.norm(dim=1)+epsilon) - log_std.sum(dim=1) - dim*lgsqrt2pi - C\n\t    IA = A**-1\n\t    BA = IA*B\n\t    D = IA*th.exp(-(A+B)**2)\n\t    E = sqrtpi * (erf(A+B)-1.)\n\t    if dim == 1:\n\t        log_prob += th.log(-IA*E + epsilon) - log2\n\t    elif dim == 2:\n\t        log_prob += th.log(IA*D + IA*BA * E + epsilon) - log2\n\t    elif dim == 3:\n", "        log_prob += th.log(2 * IA*(IA-BA)*D - IA*(2 * BA ** 2 + IA**2) * E + epsilon) - 2 * log2\n\t    elif dim == 4:\n\t        log_prob += th.log(2 * IA*(IA**2 + 1 - BA + BA**2)*D + IA*BA*(2 * BA ** 2 + 3*IA**2) * E + epsilon) - 2 * log2\n\t    elif dim == 5:\n\t        log_prob += th.log((6 * IA**3 + 4*IA - 10 * IA**3*BA - 4 * IA*BA + 4*IA*BA**2-4*IA*BA**3)*D - (3*IA**5+ 12 * IA**3*BA ** 2 + 4*IA*BA**4) * E + epsilon) - 3 * log2\n\t    elif dim == 6:\n\t        log_prob += th.log(2*IA*(2 + (2+9*IA**2)*BA**2-(2+7*IA**2)*BA+4*IA**2-2*BA**3+2*BA**4+4*IA**4) * D + IA*BA*(4*BA**4+20*IA**2*BA**2+15*IA**4) * E + epsilon) - 3 * log2\n\t    else:\n\t        raise ValueError(\"We do not implemented for this action space dimension\")\n\t    return log_prob.to(dtp)\n", "class AlphaGaussianDistribution(DiagGaussianDistribution):\n\t    \"\"\"\n\t    Gaussian distribution with diagonal covariance matrix, followed by a alpha projection to ensure bounds.\n\t    :param action_dim: Dimension of the action space.\n\t    \"\"\"\n\t    def __init__(self, action_dim: int, constraint, epsilon: float = 1e-300):\n\t        super().__init__(action_dim)\n\t        # Avoid NaN (prevents division by zero or log of zero)\n\t        self.epsilon = epsilon\n\t        self.cons = constraint\n", "        self.gaussian_actions = None\n\t    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n\t        assert states != None and centers != None\n\t        self.proba_distribution(mean_actions, log_std)\n\t        if deterministic:\n\t            self.gaussian_actions = super().mode()\n\t        else:\n\t            self.gaussian_actions = super().sample()\n\t        self.v = self.gaussian_actions - centers\n\t        if not calc_prob:\n", "            self.L = self.cons.getL(states, centers, self.v)\n\t        else:\n\t            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n\t        actions = centers + self.v / th.maximum(self.L, th.tensor(1.))[:,None]\n\t        return actions\n\t    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n\t        assert states != None and centers != None\n\t        actions = self.actions_from_params(mean_actions, log_std, states, centers, calc_prob = True)\n\t        inside = th.lt(self.L, 1.)\n\t        inside_log_prob = super().log_prob(self.gaussian_actions) # log_prob for inside of constraints\n", "        # calculate log_prob for outside of constraints\n\t        # calculate measure unit\n\t        v_norm = self.v.norm(dim=1)\n\t        coss = (self.gradL*self.v).sum(axis=1)/self.gradL.norm(dim=1)/v_norm\n\t        darea = v_norm/self.L/coss\n\t        outside_log_prob = radial_integral_gaussian(mean_actions, log_std, centers, self.v/self.L[:,None], self.epsilon) - th.log(darea)\n\t        log_prob = th.where(inside, inside_log_prob, outside_log_prob)\n\t        return actions, log_prob\n\tclass AlphaStateDependentNoiseDistribution(StateDependentNoiseDistribution):\n\t    \"\"\"\n", "    Gaussian distribution with diagonal covariance matrix, followed by a alpha projection to ensure bounds.\n\t    State dependent version\n\t    :param action_dim: Dimension of the action space.\n\t    \"\"\"\n\t    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-300, **kargs):\n\t        super().__init__(action_dim, **kargs)\n\t        # Avoid NaN (prevents division by zero or log of zero)\n\t        self._epsilon = _epsilon\n\t        self.cons = constraint\n\t        self.gaussian_actions = None\n", "    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n\t        assert states != None and centers != None\n\t#        mean_actions = th.where(mean_actions.isnan(), centers, mean_actions)\n\t#        log_std = th.where(log_std.isnan(), LOG_STD_MAX, log_std)\n\t        self.proba_distribution(mean_actions, log_std, latent_sde)\n\t        if deterministic:\n\t            self.gaussian_actions = super().mode()\n\t        else:\n\t            self.gaussian_actions = super().sample()\n\t        self.v = self.gaussian_actions - centers\n", "        if not calc_prob:\n\t            self.L = self.cons.getL(states, centers, self.v)\n\t        else:\n\t            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n\t        actions = centers + self.v / th.maximum(self.L, th.tensor(1.))[:,None]\n\t        return actions\n\t    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n\t        assert states != None and centers != None\n\t        actions = self.actions_from_params(mean_actions, log_std, latent_sde, states, centers, calc_prob = True)\n\t        inside = th.lt(self.L, 1.)\n", "        inside_log_prob = super().log_prob(self.gaussian_actions) # log_prob for inside of constraints\n\t        # calculate log_prob for outside of constraints\n\t        # calculate measure unit\n\t        v_norm = self.v.norm(dim=1)\n\t        coss = (self.gradL*self.v).sum(axis=1)/self.gradL.norm(dim=1)/v_norm\n\t        darea = v_norm/self.L/coss\n\t        self._latent_sde = latent_sde if self.learn_features else latent_sde.detach()\n\t        variance = th.mm(self._latent_sde**2, self.get_std(log_std) ** 2)\n\t        distribution_std = th.sqrt(variance + self.epsilon)\n\t        outside_log_prob = radial_integral_gaussian(mean_actions, distribution_std, centers, self.v/self.L[:,None], self._epsilon) - th.log(darea)\n", "        log_prob = th.where(inside, inside_log_prob, outside_log_prob)\n\t        return actions, log_prob\n\tif __name__ == \"__main__\":\n\t    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    from ...constraint.power_constraint import PowerConstraint\n\t    from torch import autograd\n\t    cons = PowerConstraint(11, (1,1,1,1,1,1), 10.,17)\n\t    dist = AlphaGaussianDistribution(6, constraint = cons)\n\t    N=1\n\t    mean_actions = th.tensor([[-6.2313e+08,  3.6953e+08,  5.6804e+08,  3.2498e+08, -5.9962e+08,\n", "         3.3148e+08]], dtype = th.float32, requires_grad=True)\n\t    log_std = th.tensor([[  2.,   2.,   2., -20.,   2., -20.]], dtype = th.float32, requires_grad=True)\n\t    states = th.tensor([[  1.1587,  -0.6799,  -0.7305,  -0.2797,   0.6688,  -0.4330,  -0.3629,\n\t          0.6346,  -1.9232,  -2.1278, -10.0000, -10.0000,   7.1536,  10.0000,\n\t        -10.0000,  -9.6341, -10.0000]], dtype = th.float32)\n\t    centers = th.tensor([[ 0.1698, -0.1698, -0.1698,  0.1698,  0.1698,  0.1698]], dtype = th.float32)\n\t    actions, log_prob = dist.log_prob_from_params(mean_actions, log_std, states = states, centers = centers)\n\t    print(actions, log_prob)\n\t    with autograd.detect_anomaly():\n\t        log_prob.backward()\n", "    print(mean_actions.grad, log_std.grad)\n\t    exit()\n\t    import matplotlib\n\t    import matplotlib.pyplot as plt\n\t    import matplotlib.patches as patches\n\t    probs = log_prob.exp().numpy()\n\t    colors = [[1,0,0,min(i,10.)/10.] for i in probs]\n\t    plt.scatter(actions[:,0].numpy(), actions[:,1].numpy(), c = colors)\n\t    plt.xlim([-2,2])\n\t    plt.ylim([-2,2])\n", "    plt.show()\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/analytic_center.py", "chunked_list": ["import cvxpy\n\timport torch\n\tdef calc_analytic_center(C, d):\n\t    n = C.shape[1]\n\t    x = cvxpy.Variable(n)\n\t    obj = cvxpy.Maximize(cvxpy.sum(cvxpy.log((d - C @ x))))\n\t    prob = cvxpy.Problem(obj, [C @ x <= d])\n\t    prob.solve()\n\t    return x.value\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/alpha_projection.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom .analytic_center import calc_analytic_center\n\tfrom .chebyshev_center import calc_chebyshev_center\n\tclass AlphaProjectionLayer(torch.nn.Module):\n\t    def __init__(self, cons):\n\t        super(AlphaProjectionLayer, self).__init__()\n", "        self.cons = cons\n\t    def forward(self, actions, states, centers):\n\t        v = actions - centers\n\t        L = self.cons.getL(states, centers, v)\n\t        ret = centers + v / torch.maximum(L, torch.tensor(1))[:,None]\n\t        return centers + v / torch.maximum(L, torch.tensor(1))[:,None]\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    cons = HalfCheetahDynamicConstraint()\n", "    states = torch.tensor(10.0).repeat(10,17)\n\t    C=cons.C(states)\n\t    d=cons.d(states)\n\t    layer=AlphaProjectionLayer(cons)\n\t    print(C,d)\n\t    actions = torch.rand((10,6))\n\t    print(actions)\n\t    results = layer.forward(actions, states)\n\t    print(results)\n\t    for t in range(10):\n", "        assert cons.isConstraintSatisfied(states[t], results[t])\n\t        assert not cons.isConstraintSatisfied(states[t], actions[t]) or torch.allclose(actions[t], results[t])\n"]}
{"filename": "action_constrained_rl/nn/additional_layers/shrinked_distribution.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n\tfrom stable_baselines3.common.distributions import DiagGaussianDistribution, StateDependentNoiseDistribution\n\timport torch as th\n\timport numpy as np\n\tfrom torch.special import erf\n\tclass ShrinkedGaussianDistribution(DiagGaussianDistribution):\n\t    \"\"\"\n\t    Gaussian distribution with diagonal covariance matrix, followed by a radial shrinkage to ensure bounds.\n", "    :param action_dim: Dimension of the action space.\n\t    \"\"\"\n\t    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-6):\n\t        super().__init__(action_dim)\n\t        # Avoid NaN (prevents division by zero or log of zero)\n\t        self._epsilon = _epsilon\n\t        self.cons = constraint\n\t        self.gaussian_actions = None\n\t    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n\t        assert states != None and centers != None\n", "        self.proba_distribution(mean_actions, log_std)\n\t        if deterministic:\n\t            self.gaussian_actions = super().mode()\n\t        else:\n\t            self.gaussian_actions = super().sample()\n\t        self.v = self.gaussian_actions - centers\n\t        if not calc_prob:\n\t            self.L = self.cons.getL(states, centers, self.v)\n\t        else:\n\t            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n", "        self.tanhL = th.tanh(self.L)\n\t        actions = centers + (self.tanhL / (self.L+1e-9))[:,None] * self.v\n\t        return actions\n\t    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n\t        assert states != None and centers != None\n\t        device = mean_actions.device\n\t        actions = self.actions_from_params(mean_actions, log_std, states, centers, calc_prob = True)\n\t        log_prob = super().log_prob(self.gaussian_actions)\n\t        # calculate gradient of L\n\t        gradsc = ((self.L*(1-self.tanhL**2)-self.tanhL)/((self.L+1e-9)**2))[:,None] * self.gradL\n", "        jacob = (self.tanhL/(self.L+1e-9))[:,None,None] * th.eye(self.action_dim, device = device)[None,:,:]\n\t        jacob += gradsc[:,None,:] * self.v[:,:,None]\n\t        log_prob -= th.log(th.linalg.det(jacob)+self._epsilon)\n\t        return actions, log_prob\n\tclass ShrinkedStateDependentNoiseDistribution(StateDependentNoiseDistribution):\n\t    \"\"\"\n\t    Gaussian distribution with diagonal covariance matrix, followed by a radial shrinkage to ensure bounds.\n\t    State dependent version\n\t    :param action_dim: Dimension of the action space.\n\t    \"\"\"\n", "    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-6, **kargs):\n\t        super().__init__(action_dim, **kargs)\n\t        # Avoid NaN (prevents division by zero or log of zero)\n\t        self._epsilon = _epsilon\n\t        self.cons = constraint\n\t        self.gaussian_actions = None\n\t    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n\t        assert states != None and centers != None\n\t        self.proba_distribution(mean_actions, log_std, latent_sde)\n\t        if deterministic:\n", "            self.gaussian_actions = super().mode()\n\t        else:\n\t            self.gaussian_actions = super().sample()\n\t        self.v = self.gaussian_actions - centers\n\t        if not calc_prob:\n\t            self.L = self.cons.getL(states, centers, self.v)\n\t        else:\n\t            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n\t        self.tanhL = th.tanh(self.L)\n\t        actions = centers + (self.tanhL / (self.L+1e-9))[:,None] * self.v\n", "        return actions\n\t    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n\t        assert states != None and centers != None\n\t        device = mean_actions.device\n\t        actions = self.actions_from_params(mean_actions, log_std, latent_sde, states, centers, calc_prob = True)\n\t        log_prob = super().log_prob(self.gaussian_actions)\n\t        # calculate gradient of L\n\t        gradsc = ((self.L*(1-self.tanhL**2)-self.tanhL)/((self.L+1e-9)**2))[:,None] * self.gradL\n\t        jacob = (self.tanhL/(self.L+1e-9))[:,None,None] * th.eye(self.action_dim, device = device)[None,:,:]\n\t        jacob += gradsc[:,None,:] * self.v[:,:,None]\n", "        log_prob -= th.log(th.linalg.det(jacob)+self._epsilon)\n\t        return actions, log_prob\n\tif __name__ == \"__main__\":\n\t    from .visualize import TestConstraint\n\t    cons = TestConstraint()\n\t    dist = ShrinkedGaussianDistribution(2, constraint = cons)\n\t    N=1000\n\t    mean_actions = th.tensor([[0.,0.]], dtype = th.float32).repeat(N,1)\n\t    log_std = th.tensor([[-1,-1]], dtype = th.float32).repeat(N,1)\n\t    states = th.zeros((N,2))\n", "    centers = th.zeros((N,2))\n\t    actions, log_prob = dist.log_prob_from_params(mean_actions, log_std, states = states, centers = centers)\n\t    import matplotlib\n\t    import matplotlib.pyplot as plt\n\t    import matplotlib.patches as patches\n\t    probs = log_prob.exp().numpy()\n\t    colors = [[1,0,0,min(i,10.)/10.] for i in probs]\n\t    plt.scatter(actions[:,0].numpy(), actions[:,1].numpy(), c = colors)\n\t    plt.xlim([-2,2])\n\t    plt.ylim([-2,2])\n", "    plt.show()\n"]}
{"filename": "action_constrained_rl/nn/opt_layer/opt_layer.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom cvxpylayers.torch import CvxpyLayer\n\timport cvxpy as cp\n\tfrom ...cvxpy_variables import CVXPYVariables\n\tclass OptLayer(torch.nn.Module):\n\t    \"\"\"\n", "    wrapper of cvxpylayer\n\t    \"\"\"\n\t    def __init__(self, cons):\n\t        super(OptLayer, self).__init__()\n\t        cvxpy_vars = cons.cvxpy_variables()\n\t        if cvxpy_vars.s is None:\n\t            self.state_dependence = False\n\t            self.layer = CvxpyLayer(cvxpy_vars.prob, parameters=[cvxpy_vars.q], variables=[cvxpy_vars.x])\n\t        else:\n\t            self.state_dependence = True\n", "            self.layer = CvxpyLayer(cvxpy_vars.prob, parameters=[cvxpy_vars.q, cvxpy_vars.s], variables=[cvxpy_vars.x])\n\t    def forward(self, x, s):\n\t        if self.state_dependence:\n\t            return self.layer(x, s)[0]\n\t        else:\n\t            return self.layer(x)[0]\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    cons = HalfCheetahDynamicConstraint()\n", "    layer = OptLayer(cons)\n\t    x = torch.rand((100, 6))\n\t    s = torch.rand((100, 17))\n\t    y = layer(x,s)\n\t    for t in range(x.shape[0]):\n\t        assert cons.isConstraintSatisfied(s[t], y[t])\n\t        assert not cons.isConstraintSatisfied(s[t], x[t]) or torch.allclose(x[t], y[t], rtol = 1e-2)\n\t    # import diffcp\n\t    # diffcp.solve_and_derivative_batch()\n\t    for param in layer.parameters():\n", "        print(param)\n"]}
{"filename": "action_constrained_rl/nn/opt_layer/opt_layer_actor.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Type, Union\n\timport gym\n\timport torch as th\n\timport numpy as np\n\tfrom torch import nn\n\tfrom stable_baselines3.common.policies import BasePolicy, ContinuousCritic, register_policy\n\tfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\tfrom stable_baselines3.common.preprocessing import get_action_dim\n", "from stable_baselines3.common.torch_layers import (\n\t    BaseFeaturesExtractor,\n\t    CombinedExtractor,\n\t    FlattenExtractor,\n\t    NatureCNN,\n\t    create_mlp,\n\t    get_actor_critic_arch,\n\t)\n\tfrom .opt_layer import OptLayer\n\tclass OptLayerActor(BasePolicy):\n", "    \"\"\"\n\t    Actor network (policy) for DOpt.\n\t    :param observation_space: Obervation space\n\t    :param action_space: Action space\n\t    :param net_arch: Network architecture\n\t    :param features_extractor: Network to extract features\n\t        (a CNN when using images, a nn.Flatten() layer otherwise)\n\t    :param features_dim: Number of features\n\t    :param activation_fn: Activation function\n\t    :param normalize_images: Whether to normalize images or not,\n", "         dividing by 255.0 (True by default)\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        net_arch: List[int],\n\t        features_extractor: nn.Module,\n\t        features_dim: int,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n", "        normalize_images: bool = True,\n\t        squash_output = True,\n\t        constraint = None,\n\t    ):\n\t        if constraint is None:\n\t            raise (\"constraint should not be None\")\n\t        super(OptLayerActor, self).__init__(\n\t            observation_space,\n\t            action_space,\n\t            features_extractor=features_extractor,\n", "            normalize_images=normalize_images,\n\t            squash_output=squash_output,\n\t        )\n\t        self.net_arch = net_arch\n\t        self.features_dim = features_dim\n\t        self.activation_fn = activation_fn\n\t        self.constraint = constraint\n\t        action_dim = get_action_dim(self.action_space)\n\t        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=squash_output)\n\t        # Deterministic action\n", "        self.first_layers = nn.Sequential(*actor_net)\n\t        self.opt_layer = OptLayer(self.constraint)\n\t        self.tanh = nn.Tanh()\n\t    def _get_constructor_parameters(self) -> Dict[str, Any]:\n\t        data = super()._get_constructor_parameters()\n\t        data.update(\n\t            dict(\n\t                net_arch=self.net_arch,\n\t                features_dim=self.features_dim,\n\t                activation_fn=self.activation_fn,\n", "                features_extractor=self.features_extractor,\n\t            )\n\t        )\n\t        return data\n\t    def forward(self, obs: th.Tensor) -> th.Tensor:\n\t        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n\t        features = self.extract_features(obs)\n\t        output = self.first_layers(features)\n\t        output = self.opt_layer(output, features)\n\t        return output\n", "    def forward_before_projection(self, obs: th.Tensor) -> th.Tensor:\n\t        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n\t        features = self.extract_features(obs)\n\t        output = self.first_layers(features)\n\t        return output\n\t    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n\t        return self.forward(observation)\n\t    def undeformed_predict(self, obs: np.ndarray) -> th.Tensor:\n\t        # return output before the final layer\n\t        obs, _ = self.obs_to_tensor(obs)\n", "        features = self.extract_features(obs)\n\t        output = self.first_layers(features)\n\t        return output.cpu().detach().numpy()\n\t    def deform_action(self, actions: np.ndarray, obs: np.ndarray) -> th.Tensor:\n\t        # apply the final layer\n\t        obs, _ = self.obs_to_tensor(obs)\n\t        features = self.extract_features(obs)\n\t        actions = th.tensor(actions, device = self.device).float()\n\t        return self.opt_layer(actions, features).cpu().detach().numpy()\n\tif __name__ == \"__main__\":\n", "    import numpy as np\n\t    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    env = gym.make(\"HalfCheetah-v2\")\n\t    cons = HalfCheetahDynamicConstraint()\n\t    actor = OptLayerActor(env.observation_space, env.action_space, [30, 20], FlattenExtractor(env.observation_space), 17, squash_output=False, constraint = cons)\n\t    print(env.observation_space)\n\t    obs = th.ones(1, 17)\n\t    output = actor(obs)\n\t    output.sum().backward()\n\t    for param in actor.parameters():\n", "        print(param)\n\t        print(param.grad)\n"]}
{"filename": "action_constrained_rl/nn/opt_layer/opt_layer_policy.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\tfrom typing import Any, Dict, List, Optional, Type, Union\n\timport gym\n\timport torch as th\n\timport numpy as np\n\tfrom torch import nn\n\tfrom stable_baselines3.td3.policies import TD3Policy\n\tfrom stable_baselines3.common.torch_layers import (\n\t    BaseFeaturesExtractor,\n", "    FlattenExtractor,\n\t)\n\tfrom stable_baselines3.common.type_aliases import Schedule\n\tfrom .opt_layer_actor import OptLayerActor\n\tclass OptLayerPolicy(TD3Policy):\n\t    \"\"\"\n\t    Policy for DOpt\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        lr_schedule: Schedule,\n\t        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n\t        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n", "        n_critics: int = 2,\n\t        share_features_extractor: bool = True,\n\t        **kwargs\n\t    ):\n\t        self.kwargs = kwargs\n\t        super(OptLayerPolicy, self).__init__(\n\t            observation_space,\n\t            action_space,\n\t            lr_schedule,\n\t            net_arch,\n", "            activation_fn,\n\t            features_extractor_class,\n\t            features_extractor_kwargs,\n\t            normalize_images,\n\t            optimizer_class,\n\t            optimizer_kwargs,\n\t            n_critics,\n\t            share_features_extractor,\n\t        )\n\t    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None):\n", "        self.actor_kwargs.update(self.kwargs)\n\t        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n\t        return OptLayerActor(**actor_kwargs).to(self.device)\n\t    def forward(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n\t        return self.actor.forward(observation)\n\t    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n\t        # Note: the deterministic deterministic parameter is ignored in the case of TD3.\n\t        #   Predictions are always deterministic.\n\t        return self.actor._predict(observation)\n\tif __name__ == \"__main__\":\n", "    import numpy as np\n\t    from stable_baselines3 import DDPG\n\t    from ...ddpg.projection_ddpg import ProjectionDDPG\n\t    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\t    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\t    from ...env_wrapper.constraint_wrapper import ConstraintEnvWrapper\n\t    cons = HalfCheetahDynamicConstraint()\n\t    env = gym.make(\"HalfCheetah-v2\")\n\t    env = ConstraintEnvWrapper(cons, env)\n\t    n_actions = env.action_space.shape[-1]\n", "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.01 * np.ones(n_actions))\n\t    model = ProjectionDDPG(cons, OptLayerPolicy, env, action_noise=action_noise, verbose=2, batch_size=8, policy_kwargs = {\"constraint\": cons})\n\t    model.learn(total_timesteps=1000)\n"]}
{"filename": "action_constrained_rl/nn/opt_layer/tests/test_opt_layer.py", "chunked_list": ["import torch\n\tfrom ..opt_layer import OptLayer\n\tfrom pytest import approx\n\tfrom ....half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\tdef test_opt_layer_half_cheetah():\n\t    import numpy as np\n\t    cons = HalfCheetahDynamicConstraint()\n\t    layer = OptLayer(cons)\n\t    a = torch.tensor(5.0 * np.ones(6))\n\t    s = torch.tensor(np.ones(17))\n", "    assert layer(a, s).sum() == approx(20.0)"]}
{"filename": "action_constrained_rl/td3/noise_insertion_td3.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport math\n\timport gym\n\timport torch as th\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3 import TD3\n\tfrom stable_baselines3.common.preprocessing import get_action_dim\n", "from ..nn.additional_layers.alpha_projection import AlphaProjectionLayer\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.utils import polyak_update\n\tclass NoiseInsertionTD3(TD3):\n\t    \"\"\"\n\t    TD3 to project random samplied actions and to add noise before final layer\n\t    \"\"\"\n\t    def __init__(self, policy, env, use_center_wrapper:bool = True, **kwargs):\n\t        super(NoiseInsertionTD3, self).__init__(policy, env, **kwargs)\n\t        self.action_dim = get_action_dim(self.action_space)\n", "        self.alpha_layer = AlphaProjectionLayer(env.envs[0].constraint)\n\t        self.use_center_wrapper = use_center_wrapper\n\t    def _sample_action(\n\t        self,\n\t        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n\t    ) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"\n\t        Sample an action according to the exploration policy.\n", "        This is either done by sampling the probability distribution of the policy,\n\t        or sampling a random action (from a uniform distribution over the action space)\n\t        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n\t            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n\t        :param n_envs:\n\t        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n", "            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n\t        \"\"\"\n\t        # Select action randomly or according to policy\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n\t            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            # project actions\n\t            if self.use_center_wrapper: # use alpha projection\n\t                scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-self.action_dim], self._last_obs[i,-self.action_dim:], scaled_action[i]) for i in range(n_envs)])\n", "            else: # use closest-point projection\n\t                scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n\t        else:\n\t            scaled_action = self.policy.actor.undeformed_predict(self._last_obs) # output before final layer\n\t            # Add noise to the action (improve exploration)\n\t            if action_noise is not None:\n\t                scaled_action = scaled_action + action_noise()\n\t            # Deform action by final layer\n\t            scaled_action = self.policy.actor.deform_action(scaled_action, self._last_obs)\n\t        buffer_action = scaled_action\n", "        action = self.policy.unscale_action(scaled_action)\n\t        return action, buffer_action\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n", "            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                # instead of clamping, project by alpha projection layer\n\t                next_actions = self.actor_target(replay_data.next_observations) + noise\n\t                next_observations = replay_data.next_observations.to(next_actions.dtype)\n\t                next_actions = self.alpha_layer(next_actions,\n", "                                                next_observations[:, :-self.action_dim],\n\t                                                next_observations[:, -self.action_dim:])\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n", "            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, self.actor(replay_data.observations)).mean()\n\t                actor_losses.append(actor_loss.item())\n", "                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n"]}
{"filename": "action_constrained_rl/td3/td3_with_penalty.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n", "from stable_baselines3 import TD3\n\tfrom .noise_insertion_td3 import NoiseInsertionTD3\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass TD3WithPenalty(NoiseInsertionTD3):\n\t    \"\"\"\n\t    modified TD3 to add penalty to violation of constraints of outputs before final layer\n\t    This class is used for DOpt+\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, use_center_wrapper:bool = True, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super(TD3WithPenalty, self).__init__(*args, use_center_wrapper = use_center_wrapper, **kwargs)\n", "        self.constraint = constraint\n\t        self.penalty_coeff = constraint_penalty\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n", "            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n", "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n", "            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n\t                outputs = self.actor(replay_data.observations)\n\t                before_projection = (self.actor.forward_before_projection(replay_data.observations))\n\t                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\t                actor_losses.append(actor_loss.item())\n\t                # Optimize the actor\n", "                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n\t                self.logger.record(\"train/action_grad\", action_grad.item())\n", "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n"]}
{"filename": "action_constrained_rl/td3/td3_output_penalty.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\timport gym\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n\tfrom stable_baselines3.common.noise import ActionNoise\n", "from stable_baselines3 import TD3\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass TD3WithOutputPenalty(TD3):\n\t    \"\"\"\n\t    modified TD3 to add penalty to violation of constraints\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.constraint = constraint\n\t        self.penalty_coeff = constraint_penalty\n", "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n\t        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n", "            with th.no_grad():\n\t                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n", "            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\t            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n", "                # Compute actor loss\n\t                outputs = self.actor(replay_data.observations)\n\t                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                # calculate penaty\n\t                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\t                actor_losses.append(actor_loss.item())\n\t                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n\t                actor_loss.backward()\n", "                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n\t                self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n\t        if len(actor_losses) > 0:\n", "            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n"]}
{"filename": "action_constrained_rl/td3/projection_td3.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n\t# Author: Shuwa Miura, Kazumi Kasaura\n\timport numpy as np\n\timport math\n\timport gym\n\timport torch as th\n\tfrom torch.nn import functional as F\n\tfrom stable_baselines3.common.buffers import ReplayBuffer\n\tfrom stable_baselines3.common.utils import polyak_update\n\tfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\n", "from stable_baselines3.common.noise import ActionNoise\n\tfrom stable_baselines3 import TD3\n\tfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\tclass ProjectionTD3(TD3):\n\t    \"\"\"\n\t    class for DPro\n\t    \"\"\"\n\t    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self.constraint = constraint\n", "        self.penalty_coeff = constraint_penalty\n\t    def _sample_action(\n\t        self,\n\t        learning_starts: int,\n\t        action_noise: Optional[ActionNoise] = None,\n\t        n_envs: int = 1,\n\t    ) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"\n\t        Sample an action according to the exploration policy.\n\t        This is either done by sampling the probability distribution of the policy,\n", "        or sampling a random action (from a uniform distribution over the action space)\n\t        or by adding noise to the deterministic output.\n\t        :param action_noise: Action noise that will be used for exploration\n\t            Required for deterministic policy (e.g. TD3). This can also be used\n\t            in addition to the stochastic policy for SAC.\n\t        :param learning_starts: Number of steps before learning for the warm-up phase.\n\t        :param n_envs:\n\t        :return: action to take in the environment\n\t            and scaled action that will be stored in the replay buffer.\n\t            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n", "        \"\"\"\n\t        # Select action randomly or according to policy\n\t        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n\t            # Warmup phase\n\t            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n\t        else:\n\t            # Note: when using continuous actions,\n\t            # we assume that the policy uses tanh to scale the action\n\t            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n\t            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n", "            assert not np.isnan(unscaled_action).any()\n\t        # Rescale the action from [low, high] to [-1, 1]\n\t        if isinstance(self.action_space, gym.spaces.Box):\n\t            obs = self._last_obs[-1]\n\t            #print(\"unscaled: {}\".format(unscaled_action))\n\t            scaled_action = self.policy.scale_action(unscaled_action)\n\t            #print(\"scaled: {}\".format(scaled_action))\n\t            # Add noise to the action (improve exploration)\n\t            if action_noise is not None:\n\t                scaled_action = scaled_action + action_noise()\n", "            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n\t            # We store the scaled action in the buffer\n\t            buffer_action = scaled_action\n\t            action = self.policy.unscale_action(scaled_action)\n\t        else:\n\t            # Discrete case, no need to normalize or clip\n\t            buffer_action = unscaled_action\n\t            action = buffer_action\n\t        return action, buffer_action\n\t    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n", "        # Switch to train mode (this affects batch norm / dropout)\n\t        self.policy.set_training_mode(True)\n\t        # Update learning rate according to lr schedule\n\t        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\t        actor_losses, critic_losses = [], []\n\t        for _ in range(gradient_steps):\n\t            self._n_updates += 1\n\t            # Sample replay buffer\n\t            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\t            with th.no_grad():\n", "                # Select action according to policy and add clipped noise\n\t                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n\t                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n\t                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\t                # Compute the next Q-values: min over all critics targets\n\t                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n\t                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n\t                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\t            # Get current Q-values estimates for each critic network\n\t            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n", "            # Compute critic loss\n\t            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n\t            critic_losses.append(critic_loss.item())\n\t            # Optimize the critics\n\t            self.critic.optimizer.zero_grad()\n\t            critic_loss.backward()\n\t            self.critic.optimizer.step()\n\t            # Delayed policy updates\n\t            if self._n_updates % self.policy_delay == 0:\n\t                # Compute actor loss\n", "                outputs = self.actor(replay_data.observations)\n\t                outputs.retain_grad()\n\t                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\t                assert not th.isnan(actor_loss).any()\n\t                # calculate penaty\n\t                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\t                assert not th.isnan(actor_loss).any()\n\t                actor_losses.append(actor_loss.item())\n\t                # Optimize the actor\n\t                self.actor.optimizer.zero_grad()\n", "                actor_loss.backward()\n\t                self.actor.optimizer.step()\n\t                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\t                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\t                for tag, value in self.actor.named_parameters():\n\t                    if value.grad is not None:\n\t                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\t                action_grad = outputs.grad.norm(dim=1).sum()\n\t                self.logger.record(\"train/action_grad\", action_grad.item())\n\t        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n", "        if len(actor_losses) > 0:\n\t            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n\t        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n"]}
{"filename": "plot_scripts/plot_experiment_reacher.py", "chunked_list": ["from stable_baselines3.common.results_plotter import load_results, ts2xy, window_func\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\timport csv\n\tfrom scipy.stats import mannwhitneyu\n\timport bootstrapped.bootstrap as bs\n\tfrom bootstrapped import stats_functions\n\timport matplotlib\n\tmatplotlib.rcParams['pdf.fonttype'] = 42\n\tmatplotlib.rcParams['ps.fonttype'] = 42\n", "def loadEvaluations(filename):\n\t    npz = np.load(filename)\n\t    return npz['timesteps'], npz['results']\n\tdef loadEvaluationsN(stem, n):\n\t    rows = []\n\t    timesteps = None\n\t    for i in range(1, n+1):\n\t        filename = f\"{stem}-{i}/evaluations.npz\"\n\t        timesteps, results = loadEvaluations(filename)\n\t        rows.append(np.mean(results, axis=1))\n", "    L=min([len(row) for row in rows])\n\t    for i in range(len(rows)):\n\t        rows[i] = rows[i][:L]\n\t    timesteps=timesteps[:L]\n\t    matrix = np.vstack(rows)\n\t    return (timesteps, matrix)\n\tdef plot_results(log_folder, title='Learning Curve'):\n\t    \"\"\"\n\t    plot the results\n\t    :param log_folder: (str) the save location of the results to plot\n", "    :param title: (str) the title of the task to plot\n\t    \"\"\"\n\t    fig = plt.figure(title, figsize = (6,4))\n\t    N=10\n\t    R = []\n\t    labels = []\n\t    cmap=plt.get_cmap('tab20')\n\t    index = -1\n\t    colors=[0,2,4,6,3,5,8,10,12,14,16,18,1]\n\t    for label in [\"DPro\", \"DPro+\",\"DPre\", \"DPre+\", \"DOpt\", \n", "                  \"DOpt\", \"NFW\", \"DAlpha\", \"DRad\", \"SPre\", \"SPre+\", \"SAlpha\", \"SRad\",\n\t    ]:\n\t        prefix = \"re-exp/logs-R-L2/\" + label\n\t        index+=1\n\t        x, y = loadEvaluationsN(prefix, N)\n\t        #means = [bs.bootstrap(y[:,i], stat_func=stats_functions.mean) for i in range(len(x))]\n\t        #plt.errorbar(x, [mean.value for mean in means], [[mean.lower_bound for mean in means], [mean.upper_bound for mean in means]], label = label, capsize = 2, elinewidth = 1)\n\t        #plt.errorbar(x, np.mean(y, axis=0), np.std(y, axis=0)/np.sqrt(N), label = label, capsize = 2, color=cmap(index))\n\t        plt.plot(x, np.mean(y, axis=0), label = label, color=cmap(colors[index]))\n\t        #R.append(np.sum(y, axis=1))\n", "        labels.append(label)\n\t    plt.xlabel('Number of timesteps')\n\t    plt.ylabel('Rewards')\n\t    plt.ylim([-10.,25.0])\n\t    #plt.title(title)\n\t    plt.legend(ncol=2, loc='lower right')\n\t    plt.subplots_adjust(left=0.11,right=0.95,top=0.95,bottom=0.13)\n\t    plt.savefig(\"../../../miura/figures/reacher.pdf\")\n\t    plt.show()\n\tplot_results(\"./\", \"Learning Curve Reacher\")\n"]}
{"filename": "plot_scripts/plot_experiment_halfcheetah.py", "chunked_list": ["from stable_baselines3.common.results_plotter import load_results, ts2xy, window_func\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\timport csv\n\tfrom scipy.stats import mannwhitneyu\n\timport bootstrapped.bootstrap as bs\n\tfrom bootstrapped import stats_functions\n\timport matplotlib\n\tmatplotlib.rcParams['pdf.fonttype'] = 42\n\tmatplotlib.rcParams['ps.fonttype'] = 42\n", "def loadEvaluations(filename):\n\t    npz = np.load(filename)\n\t    return npz['timesteps'], npz['results']\n\tdef loadEvaluationsN(stem, n):\n\t    rows = []\n\t    timesteps = None\n\t    for i in range(1, n+1):\n\t        filename = f\"{stem}-{i}/evaluations.npz\"\n\t        timesteps, results = loadEvaluations(filename)\n\t        rows.append(np.mean(results, axis=1))\n", "    L=min([len(row) for row in rows])\n\t    for i in range(len(rows)):\n\t        rows[i] = rows[i][:L]\n\t    timesteps=timesteps[:L]\n\t    matrix = np.vstack(rows)\n\t    return (timesteps, matrix)\n\tdef plot_results(log_folder, title='Learning Curve'):\n\t    \"\"\"\n\t    plot the results\n\t    :param log_folder: (str) the save location of the results to plot\n", "    :param title: (str) the title of the task to plot\n\t    \"\"\"\n\t    fig = plt.figure(title, figsize = (6,4))\n\t    N=10\n\t    R = []\n\t    labels = []\n\t    cmap=plt.get_cmap('tab20')\n\t    index = -1\n\t    for prefix, label in [\n\t            [\"../logs/main/half_cheetah/stupid_proj\", \"DPro\"], \n", "            [\"../logs/main/half_cheetah/stupid_proj-P\", \"DPro+\"], \n\t            [\"../logs/main/half_cheetah/wrapper-DDPG\", \"DPre\"], \n\t            [\"../logs/main/half_cheetah/wrapper-P-DDPG\", \"DPre+\"], \n\t                          [\"../logs/main/half_cheetah/OptSq\", \"DOpt\"], \n\t                          [\"../logs/main/half_cheetah/OptSqP\", \"DOpt+\"], \n\t                          [\"../logs/main/half_cheetah/NFWPO100\", \"NFW\"],\n\t                          [\"../logs/main/half_cheetah/NFWPOOriginal100\", \"NFW*\"],\n\t            [\"../logs/main/half_cheetah/alpha-DDPG\", \"DAlpha\"],\n\t                          [\"../logs/main/half_cheetah/shrinkage-DDPG\", \"DRad\"],\n\t                          [\"../logs/main/half_cheetah/wrapper-TrueSAC\", \"SPre\"], \n", "                          [\"../logs/main/half_cheetah/wraper-P-TrueSAC\", \"SPre+\"], \n\t                          [\"../logs/main/half_cheetah/alpha-TrueSAC\", \"SAlpha\"],\n\t                          [\"../logs/main/half_cheetah/shrinkage-TrueSAC\", \"SRad\"],\n\t   ]:\n\t        index+=1\n\t        if label == \"DOpt\" or label == \"DOpt+\":\n\t            continue\n\t        x, y = loadEvaluationsN(prefix, N)\n\t        #means = [bs.bootstrap(y[:,i], stat_func=stats_functions.mean) for i in range(len(x))]\n\t        #plt.errorbar(x, [mean.value for mean in means], [[mean.lower_bound for mean in means], [mean.upper_bound for mean in means]], label = label, capsize = 2, elinewidth = 1)\n", "        #plt.errorbar(x, np.mean(y, axis=0), np.std(y, axis=0)/np.sqrt(N), label = label, capsize = 2, color=cmap(index))\n\t        plt.plot(x, np.mean(y, axis=0), label = label, color=cmap(index))\n\t        #R.append(np.sum(y, axis=1))\n\t        labels.append(label)\n\t    plt.xlabel('Number of timesteps')\n\t    plt.ylabel('Rewards')\n\t    plt.ticklabel_format(scilimits=[-1,1])\n\t    plt.ylim([-1000,9000])\n\t    #plt.title(title)\n\t    plt.legend(ncol=2, loc='lower right')\n", "    plt.subplots_adjust(left=0.11,right=0.95,top=0.95,bottom=0.13)\n\t    plt.savefig(\"../../../miura/figures/half_cheetah.pdf\")\n\t    plt.show()\n\tplot_results(\"./\", \"Learning Curve Half_Cheetah\")\n"]}
