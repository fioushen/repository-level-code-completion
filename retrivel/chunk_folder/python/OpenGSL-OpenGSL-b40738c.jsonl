{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tREQUIRES = \"\"\"\n\truamel.yaml\n\tpandas\n\tscipy\n\tscikit-learn\n\tpyro-api==0.1.2\n\tpyro-ppl==1.8.0\n\tnumba\n\t\"\"\"\n", "def get_install_requires():\n\t    reqs = [req for req in REQUIRES.split(\"\\n\") if len(req) > 0]\n\t    return reqs\n\twith open(\"README.md\", encoding=\"utf-8\") as f:\n\t    readme = f.read()\n\tdef do_setup():\n\t    setup(\n\t        name=\"opengsl\",\n\t        version=\"0.0.5\",\n\t        description=\"A comprehensive benchmark for Graph Structure Learning.\",\n", "        url=\"https://github.com/OpenGSL/OpenGSL\",\n\t        author='Zhiyao Zhou, Sheng Zhou, Bochao Mao, Xuanyi Zhou',\n\t        long_description=readme,\n\t        long_description_content_type=\"text/markdown\",\n\t        install_requires=get_install_requires(),\n\t        python_requires=\">=3.7.0\",\n\t        packages=find_packages(),\n\t        include_package_data=True,\n\t        keywords=[\"AI\", \"GNN\", \"graph structure learning\"],\n\t        classifiers=[\n", "            \"Programming Language :: Python :: 3.7\",\n\t            \"Programming Language :: Python :: 3.8\",\n\t            \"Programming Language :: Python :: 3.9\",\n\t            \"Programming Language :: Python :: 3.10\",\n\t            \"Intended Audience :: Developers\",\n\t            \"Intended Audience :: Science/Research\",\n\t        ]\n\t    )\n\tif __name__ == \"__main__\":\n\t    do_setup()"]}
{"filename": "paper/main_results.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\timport pandas as pd\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--data', type=str, default='cora',\n\t                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n\t                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',\n\t                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n\t                             'blogcatalog', 'flickr'], help='dataset')\n", "parser.add_argument('--method', type=str, default='gcn', choices=['gcn', 'appnp', 'gt', 'gat', 'prognn', 'gen',\n\t                                                                  'gaug', 'idgl', 'grcn', 'sgc', 'jknet', 'slaps',\n\t                                                                  'gprgnn', 'nodeformer', 'segsl', 'sublime',\n\t                                                                  'stable', 'cogsl', 'lpa', 'link', 'linkx', 'wsgnn'], help=\"Select methods\")\n\tparser.add_argument('--config', type=str, default=None)\n\tparser.add_argument('--debug', action='store_true')\n\tparser.add_argument('--gpu', type=str, default='0', help=\"Visible GPU\")\n\targs = parser.parse_args()\n\tos.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\tfrom opengsl.config import load_conf\n", "from opengsl.data import Dataset\n\tfrom opengsl import ExpManager\n\tfrom opengsl.method import *\n\tif args.config is None:\n\t    conf = load_conf(method=args.method, dataset=args.data)\n\telse:\n\t    conf = load_conf(args.config)\n\tconf.analysis['save_graph'] = False\n\tprint(conf)\n\tdataset = Dataset(args.data, feat_norm=conf.dataset['feat_norm'], path='data')\n", "method = eval('{}Solver(conf, dataset)'.format(args.method.upper()))\n\texp = ExpManager(method,  save_path='records')\n\tacc_save, std_save = exp.run(n_runs=10, debug=args.debug)\n\tif not os.path.exists('results'):\n\t    os.makedirs('results')\n\tif os.path.exists('results/performance.csv'):\n\t    records = pd.read_csv('results/performance.csv')\n\t    records.loc[len(records)] = {'method':args.method, 'data':args.data, 'acc':acc_save, 'std':std_save}\n\t    records.to_csv('results/performance.csv', index=False)\n\telse:\n", "    records = pd.DataFrame([[args.method, args.data, acc_save, std_save]], columns=['method', 'data', 'acc', 'std'])\n\t    records.to_csv('results/performance.csv', index=False)"]}
{"filename": "paper/generalizability.py", "chunked_list": ["import argparse\n\timport os\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--data', type=str, default='cora',\n\t                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n\t                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',\n\t                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n\t                             'blogcatalog', 'flickr'], help='dataset')\n\tparser.add_argument('--gsl', type=str, default='grcn', choices=['gt', 'prognn', 'gen', 'gaug', 'idgl', 'grcn', 'slaps',  'nodeformer', 'segsl', 'sublime', 'stable', 'cogsl'], help=\"Select methods\")\n\tparser.add_argument('--gnn', type=str, default='gcn', choices=['gcn', 'sgc', 'jknet', 'appnp', 'gprgnn', 'lpa', 'link'])\n", "parser.add_argument('--debug', action='store_true')\n\tparser.add_argument('--gpu', type=str, default='0', help=\"Visible GPU\")\n\targs = parser.parse_args()\n\tos.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\timport opengsl\n\tconf = opengsl.config.load_conf(method=args.gnn, dataset=args.data)\n\t# specify some settings\n\tconf.analysis['load_graph'] = True\n\tconf.analysis['load_graph_path'] = 'results/graph/{}'.format(args.gsl)\n\tif args.gsl in ['sublime', 'idgl']:\n", "    conf.dataset['normalize'] = False\n\telse:\n\t    conf.dataset['normalize'] = True\n\tif args.gsl in ['grcn', 'sublime', 'idgl']:\n\t    conf.dataset['add_loop'] = False\n\telse:\n\t    conf.dataset['add_loop'] = True\n\tprint(conf)\n\tdataset = opengsl.data.Dataset(args.data, feat_norm=conf.dataset['feat_norm'], path='data')\n\tmethod = eval('opengsl.method.{}(conf, dataset)'.format(args.gnn))\n", "exp = opengsl.ExpManager(method,  save_path='records')\n\texp.run(n_runs=1, debug=args.debug)"]}
{"filename": "paper/homophily.py", "chunked_list": ["import argparse\n\timport os\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--data', type=str, default='cora',\n\t                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n\t                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',\n\t                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n\t                             'blogcatalog', 'flickr'], help='dataset')\n\tparser.add_argument('--method', type=str, default='gcn', choices=['gcn', 'appnp', 'gt', 'gat', 'prognn', 'gen',\n\t                                                                  'gaug', 'idgl', 'grcn', 'sgc', 'jknet', 'slaps',\n", "                                                                  'gprgnn', 'nodeformer', 'segsl', 'sublime',\n\t                                                                  'stable', 'cogsl', 'lpa', 'link', 'linkx'], help=\"Select methods\")\n\targs = parser.parse_args()\n\timport opengsl\n\timport numpy as np\n\timport torch\n\tconf = opengsl.config.load_conf(method=args.method, dataset=args.data)\n\tprint(conf)\n\tdata = opengsl.data.Dataset(args.data, feat_norm=conf.dataset['feat_norm'], path='data')\n\tfill = None\n", "h = []\n\tprint(opengsl.utils.get_homophily(data.labels.cpu(), data.adj.to_dense().cpu(), type='edge', fill=fill))\n\tfor i in range(10):\n\t    adj = torch.load(os.path.join('results/graph/{}'.format(args.method), '{}_{}_{}.pth'.format(args.data, 0, i)))\n\t    h.append(opengsl.utils.get_homophily(data.labels.cpu(), adj.cpu(), type='edge', fill=fill))\n\t    print(h)\n\th = np.array(h)\n\tprint(f'{h.mean():.4f} Â± {h.std():.4f}')"]}
{"filename": "opengsl/__init__.py", "chunked_list": ["from .expmanager.ExpManager import ExpManager\n\tfrom . import data as data\n\tfrom . import method as method\n\tfrom . import config as config\n\tfrom . import utils as utils"]}
{"filename": "opengsl/utils/logger.py", "chunked_list": ["import torch\n\tclass Logger(object):\n\t    \"\"\"\n\t    Logger Class.\n\t    Parameters\n\t    ----------\n\t    runs : int\n\t        Total experimental runs.\n\t    \"\"\"\n\t    def __init__(self, runs):\n", "        self.results = [[] for _ in range(runs)]\n\t    def add_result(self, run, result_dict):\n\t        '''\n\t        Add performance of a new run.\n\t        Parameters\n\t        ----------\n\t        run : int\n\t            Id of the new run.\n\t        result_dict : dict\n\t            A dict containing training, valid and test performances.\n", "        '''\n\t        assert \"train\" in result_dict.keys()\n\t        assert \"valid\" in result_dict.keys()\n\t        assert \"test\"  in result_dict.keys()\n\t        assert run >= 0 and run < len(self.results)\n\t        self.results[run].append(result_dict[\"train\"])\n\t        self.results[run].append(result_dict[\"valid\"])\n\t        self.results[run].append(result_dict[\"test\"])\n\t    def print_statistics(self, run=None):\n\t        '''\n", "        Function to output the statistics.\n\t        Parameters\n\t        ----------\n\t        run : int\n\t            Id of a run. If not specified, output the statistics of all runs.\n\t        Returns\n\t        -------\n\t            The statistics of a given run or all runs.\n\t        '''\n\t        if run is not None:\n", "            result = 100 * torch.tensor(self.results[run])\n\t            print(f'Run {run + 1:02d}:')\n\t            print(f'Highest Train: {result[0]:.2f}')\n\t            print(f'Highest Valid: {result[1]:.2f}')\n\t            print(f'   Final Test: {result[2]:.2f}')\n\t            return  result[2]\n\t        else:\n\t            best_result = 100 * torch.tensor(self.results)\n\t            print(f'All runs:')\n\t            r = best_result[:, 0]\n", "            print(f'Highest Train: {r.mean():.2f} Â± {r.std():.2f}')\n\t            r = best_result[:, 1]\n\t            print(f'Highest Valid: {r.mean():.2f} Â± {r.std():.2f}')\n\t            r = best_result[:, 2]\n\t            print(f'   Final Test: {r.mean():.2f} Â± {r.std():.2f}')\n\t            return r.mean(), r.std()"]}
{"filename": "opengsl/utils/__init__.py", "chunked_list": ["from opengsl.utils.utils import accuracy\n\tfrom opengsl.utils.utils import scipy_sparse_to_sparse_tensor\n\tfrom opengsl.utils.utils import sparse_tensor_to_scipy_sparse\n\tfrom opengsl.utils.utils import set_seed\n\tfrom opengsl.utils.utils import get_node_homophily\n\tfrom opengsl.utils.utils import get_edge_homophily\n\tfrom opengsl.utils.utils import get_homophily\n\tfrom opengsl.utils.utils import get_adjusted_homophily\n\tfrom opengsl.utils.utils import get_label_informativeness"]}
{"filename": "opengsl/utils/utils.py", "chunked_list": ["import dgl.random\n\timport torch\n\timport os\n\timport numpy as np\n\timport scipy.sparse as sp\n\timport random\n\tdef accuracy(labels, logits):\n\t    '''\n\t    Compute the accuracy score given true labels and predicted labels.\n\t    Parameters\n", "    ----------\n\t    labels: np.array\n\t        Ground truth labels.\n\t    logits : np.array\n\t        Predicted labels.\n\t    Returns\n\t    -------\n\t    accuracy : np.float\n\t        The Accuracy score.\n\t    '''\n", "    return np.sum(logits.argmax(1)==labels)/len(labels)\n\tdef scipy_sparse_to_sparse_tensor(sparse_mx):\n\t    '''\n\t    Convert a scipy sparse matrix to a torch sparse tensor.\n\t    Parameters\n\t    ----------\n\t    sparse_mx : scipy.sparse_matrix\n\t        Sparse matrix to convert.\n\t    Returns\n\t    -------\n", "    sparse_tensor: torch.Tensor in sparse form\n\t        A tensor stored in sparse form.\n\t    '''\n\t    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n\t    indices = torch.from_numpy(\n\t        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n\t    values = torch.from_numpy(sparse_mx.data)\n\t    shape = torch.Size(sparse_mx.shape)\n\t    return torch.sparse.FloatTensor(indices, values, shape)\n\tdef sparse_tensor_to_scipy_sparse(sparse_tensor):\n", "    '''\n\t    Convert a torch sparse tensor to a scipy sparse matrix.\n\t    Parameters\n\t    ----------\n\t    sparse_tensor : torch.Tensor in sparse form\n\t        A tensor stored in sparse form to convert.\n\t    Returns\n\t    -------\n\t    sparse_mx : scipy.sparse_matrix\n\t        Sparse matrix.\n", "    '''\n\t    sparse_tensor = sparse_tensor.cpu()\n\t    row = sparse_tensor.coalesce().indices()[0].numpy()\n\t    col = sparse_tensor.coalesce().indices()[1].numpy()\n\t    values = sparse_tensor.coalesce().values().numpy()\n\t    return sp.coo_matrix((values, (row, col)), shape=sparse_tensor.shape)\n\tdef set_seed(seed):\n\t    '''\n\t    Set seed to make sure the results can be repetitive.\n\t    Parameters\n", "    ----------\n\t    seed : int\n\t        Random seed to set.\n\t    '''\n\t    dgl.random.seed(seed)\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n", "    torch.backends.cudnn.deterministic = True\n\t    torch.backends.cudnn.benchmark = False\n\tdef get_node_homophily(label, adj):\n\t    '''\n\t    Calculate the node homophily of a graph.\n\t    Parameters\n\t    ----------\n\t    label : torch.tensor\n\t        The ground truth labels.\n\t    adj : torch.tensor\n", "        The adjacency matrix in dense form.\n\t    Returns\n\t    -------\n\t    homophily : torch.float\n\t        The node homophily of the graph.\n\t    '''\n\t    label = label.cpu().numpy()\n\t    adj = adj.cpu().numpy()\n\t    num_node = len(label)\n\t    label = label.repeat(num_node).reshape(num_node, -1)\n", "    n = (np.multiply((label == label.T), adj)).sum(axis=1)\n\t    d = adj.sum(axis=1)\n\t    homos = []\n\t    for i in range(num_node):\n\t        if d[i] > 0:\n\t            homos.append(n[i] * 1. / d[i])\n\t    return np.mean(homos)\n\tdef get_edge_homophily(label, adj):\n\t    '''\n\t    Calculate the node homophily of a graph.\n", "    Parameters\n\t    ----------\n\t    label : torch.tensor\n\t        The ground truth labels.\n\t    adj : torch.tensor\n\t        The adjacency matrix in dense form.\n\t    Returns\n\t    -------\n\t    homophily : torch.float\n\t        The edge homophily of the graph.\n", "    '''\n\t    num_edge = adj.sum()\n\t    cnt = 0\n\t    for i, j in adj.nonzero():\n\t        if label[i] == label[j]:\n\t            cnt += adj[i, j]\n\t    return cnt/num_edge\n\tdef get_homophily(label, adj, type='node', fill=None):\n\t    '''\n\t    Calculate node or edge homophily of a graph.\n", "    Parameters\n\t    ----------\n\t    label : torch.tensor\n\t        The ground truth labels.\n\t    adj : torch.tensor\n\t        The adjacency matrix in dense form.\n\t    type : str\n\t        This decides whether to calculate node homo or edge homo.\n\t    fill : str\n\t        The value to fill in the diagonal of `adj`. If set to `None`, the operation won't be done.\n", "    Returns\n\t    -------\n\t    homophily : np.float\n\t        The node or edge homophily of a graph.\n\t    '''\n\t    if fill:\n\t        np.fill_diagonal(adj, fill)\n\t    return eval('get_'+type+'_homophily(label, adj)')\n\tdef get_adjusted_homophily(_label, adj):\n\t    '''\n", "    Calculate adjusted homophily of a graph.\n\t    Parameters\n\t    ----------\n\t    _label : torch.tensor\n\t        The ground truth labels.\n\t    adj : torch.tensor\n\t        The adjacency matrix in dense form.\n\t    Returns\n\t    -------\n\t    homophily : np.float\n", "        The adjusted homophily of a graph.\n\t    '''\n\t    label = _label.long()\n\t    labels = label.max() + 1\n\t    d = adj.sum(1)\n\t    E = d.sum()\n\t    D = torch.zeros(labels)\n\t    for i in range(adj.shape[0]):\n\t        D[label[i]] += d[i]\n\t    h_edge = get_edge_homophily(label, adj)\n", "    sum_pk = ((D / E) ** 2).sum()\n\t    return (h_edge - sum_pk) / (1 - sum_pk)\n\tdef get_label_informativeness(_label, adj):\n\t    '''\n\t    Calculate label informativeness of a graph.\n\t    Parameters\n\t    ----------\n\t    _label : torch.tensor\n\t        The ground truth labels.\n\t    adj : torch.tensor\n", "        The adjacency matrix in dense form.\n\t    Returns\n\t    -------\n\t    label_informativeness : np.float\n\t        The label informativeness of a graph.\n\t    '''\n\t    label = _label.long()\n\t    labels = label.max() + 1\n\t    LI_1 = 0\n\t    LI_2 = 0\n", "    p = torch.zeros((labels, labels))\n\t    for i, j in adj.nonzero():\n\t        p[label[i]][label[j]] = p[label[i]][label[j]] + adj[i][j]\n\t    d = adj.sum(1)\n\t    E = d.sum()\n\t    D = torch.zeros(labels)\n\t    for i in range(adj.shape[0]):\n\t        D[label[i]] = D[label[i]] + d[i]\n\t    for i in range(labels):\n\t        for j in range(labels):\n", "            p[i][j] = p[i][j] / E\n\t    p_ = D / E\n\t    LI_2 = (p_ * torch.log(p_)).sum()\n\t    for i in range(labels):\n\t        for j in range(labels):\n\t            if (p[i][j] != 0):\n\t                LI_1 += p[i][j] * torch.log(p[i][j] / (p_[i] * p_[j]))\n\t    return -LI_1 / LI_2"]}
{"filename": "opengsl/utils/recorder.py", "chunked_list": ["class Recorder:\n\t    \"\"\"\n\t    Recorder Class.\n\t    This records the performances of epochs in a single run. It determines whether the training has improved based\n\t    on the provided `criterion` and determines whether the earlystop `patience` has been achieved.\n\t    Parameters\n\t    ----------\n\t    patience : int\n\t        The maximum epochs to keep training since last improvement.\n\t    criterion : str\n", "        The criterion to determine whether whether the training has improvement.\n\t        - ``None``: Improvement will be considered achieved in any case.\n\t        - ``loss``: Improvement will be considered achieved when loss decreases.\n\t        - ``metric``: Improvement will be considered achieved when metric increases.\n\t        - ``either``: Improvement will be considered achieved if either loss decreases or metric increases.\n\t        - ``both``: Improvement will be considered achieved if both loss decreases and metric increases.\n\t    \"\"\"\n\t    def __init__(self, patience=100, criterion=None):\n\t        self.patience = patience\n\t        self.criterion = criterion\n", "        self.best_loss = 100\n\t        self.best_metric = 0\n\t        self.wait = 0\n\t    def add(self, loss_val, metric_val):\n\t        '''\n\t        Function to add the loss and metric of a new epoch.\n\t        Parameters\n\t        ----------\n\t        loss_val : float\n\t        metric_val : float\n", "        Returns\n\t        -------\n\t        flag : bool\n\t            Whether improvement has been achieved in the epoch.\n\t        flag_earlystop: bool\n\t            Whether training needs earlystopping.\n\t        '''\n\t        flag = False\n\t        if self.criterion is None:\n\t            flag = True\n", "        elif self.criterion == 'loss':\n\t            flag = loss_val < self.best_loss\n\t        elif self.criterion == 'metric':\n\t            flag = metric_val > self.best_metric\n\t        elif self.criterion == 'either':\n\t            flag = loss_val < self.best_loss or metric_val > self.best_metric\n\t        elif self.criterion == 'both':\n\t            flag = loss_val < self.best_loss and metric_val > self.best_metric\n\t        else:\n\t            raise NotImplementedError\n", "        if flag:\n\t            self.best_metric = metric_val\n\t            self.best_loss = loss_val\n\t            self.wait = 0\n\t        else:\n\t            self.wait += 1\n\t        flag_earlystop = self.patience and self.wait >= self.patience\n\t        return flag, flag_earlystop"]}
{"filename": "opengsl/config/__init__.py", "chunked_list": ["from opengsl.config.util import load_conf, save_conf"]}
{"filename": "opengsl/config/util.py", "chunked_list": ["import ruamel.yaml as yaml\n\timport argparse\n\timport os\n\tdef load_conf(path:str = None, method:str = None, dataset:str = None):\n\t    '''\n\t    Function to load config file.\n\t    Parameters\n\t    ----------\n\t    path : str\n\t        Path to load config file. Load default configuration if set to `None`.\n", "    method : str\n\t        Name of the used mathod. Necessary if ``path`` is set to `None`.\n\t    dataset : str\n\t        Name of the corresponding dataset. Necessary if ``path`` is set to `None`.\n\t    Returns\n\t    -------\n\t    conf : argparse.Namespace\n\t        The config file converted to Namespace.\n\t    '''\n\t    if path == None and method == None:\n", "        raise KeyError\n\t    if path == None and dataset == None:\n\t        raise KeyError\n\t    if path == None:\n\t        method_name = ['gcn', 'sgc', 'gat', 'jknet', 'appnp', 'gprgnn', 'prognn', 'idgl', 'grcn', 'gaug', 'slaps',\n\t                       'gen', 'gt', 'nodeformer', 'cogsl', 'sublime', 'stable', 'segsl', 'lpa', 'link', 'wsgnn']\n\t        data_name = ['cora', 'pubmed', 'citeseer','blogcatalog', 'flickr', 'amazon-ratings', 'questions', 'minesweeper', 'roman-empire', 'wiki-cooc']\n\t        assert method in method_name\n\t        assert dataset in data_name\n\t        dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"config\")\n", "        if method in [\"link\", \"lpa\"]:\n\t            path = os.path.join(dir, method, method+\".yaml\")\n\t        else:\n\t            path = os.path.join(dir, method, method+'_'+dataset+\".yaml\")\n\t        if os.path.exists(path) == False:\n\t            raise KeyError(\"The configuration file is not provided.\")\n\t    conf = open(path, \"r\").read()\n\t    conf = yaml.safe_load(conf)\n\t    conf = argparse.Namespace(**conf)\n\t    return conf\n", "def save_conf(path, conf):\n\t    '''\n\t    Function to save the config file.\n\t    Parameters\n\t    ----------\n\t    path : str\n\t        Path to save config file.\n\t    conf : dict\n\t        The config dict.\n\t    '''\n", "    with open(path, \"w\", encoding=\"utf-8\") as f:\n\t        yaml.dump(vars(conf), f)"]}
{"filename": "opengsl/data/__init__.py", "chunked_list": ["from .dataset.dataset import Dataset\n\tfrom . import preprocess as preprocess"]}
{"filename": "opengsl/data/dataset/hetero_load.py", "chunked_list": ["import numpy as np\n\timport os\n\timport torch\n\timport dgl\n\timport urllib.request\n\tdef hetero_load(name, path='./data/hetero_data'):\n\t    file_name = f'{name.replace(\"-\", \"_\")}.npz'\n\t    if not os.path.exists(path):\n\t        os.makedirs(path)\n\t    if not os.path.exists(os.path.join(path, file_name)):\n", "        download(file_name, path)\n\t    data = np.load(os.path.join(path, f'{name.replace(\"-\", \"_\")}.npz'))\n\t    node_features = torch.tensor(data['node_features'])\n\t    labels = torch.tensor(data['node_labels'])\n\t    edges = torch.tensor(data['edges'])\n\t    train_masks = torch.tensor(data['train_masks'])\n\t    val_masks = torch.tensor(data['val_masks'])\n\t    test_masks = torch.tensor(data['test_masks'])\n\t    train_indices = [torch.nonzero(x, as_tuple=False).squeeze().numpy() for x in train_masks]\n\t    val_indices = [torch.nonzero(x, as_tuple=False).squeeze().numpy() for x in val_masks]\n", "    test_indices = [torch.nonzero(x, as_tuple=False).squeeze().numpy() for x in test_masks]\n\t    n_nodes = node_features.shape[0]\n\t    graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(node_features), idtype=torch.long)\n\t    graph = dgl.to_bidirected(graph)\n\t    adj = graph.adj()\n\t    num_classes = len(labels.unique())\n\t    num_targets = 1 if num_classes == 2 else num_classes\n\t    if num_targets == 1:\n\t        labels = labels.float()\n\t    return node_features, adj, labels, (train_indices, val_indices, test_indices)\n", "def download(name, path):\n\t    url = 'https://github.com/OpenGSL/HeterophilousDatasets/raw/main/data/'\n\t    try:\n\t        print('Downloading', url+name)\n\t        urllib.request.urlretrieve(url + name, os.path.join(path, name))\n\t        print('Done!')\n\t    except:\n\t        raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n\tif __name__ == '__main__':\n\t    print(hetero_load('minesweeper', 'tmp'))"]}
{"filename": "opengsl/data/dataset/pyg_load.py", "chunked_list": ["'''\n\tload data via pyg\n\t'''\n\tfrom torch_geometric.datasets import Planetoid, Amazon, Coauthor, WikiCS, WikipediaNetwork, WebKB, Actor, AttributedGraphDataset\n\timport os\n\tdef pyg_load_dataset(name, path='./data/'):\n\t    dic = {'cora': 'Cora',\n\t           'citeseer': 'CiteSeer',\n\t           'pubmed': 'PubMed',\n\t           'amazoncom': 'Computers',\n", "           'amazonpho': 'Photo',\n\t           'coauthorcs': 'CS',\n\t           'coauthorph': 'Physics',\n\t           'wikics': 'WikiCS',\n\t           'chameleon': 'Chameleon',\n\t           'squirrel': 'Squirrel',\n\t           'cornell': 'Cornell',\n\t           'texas': 'Texas',\n\t           'wisconsin': 'Wisconsin',\n\t           'actor': 'Actor',\n", "           'blogcatalog':'blogcatalog',\n\t           'flickr':'flickr'}\n\t    name = dic[name]\n\t    if name in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n\t        dataset = Planetoid(root=os.path.join(path, name), name=name)\n\t    elif name in [\"Computers\", \"Photo\"]:\n\t        dataset = Amazon(root=os.path.join(path, name), name=name)\n\t    elif name in [\"CS\", \"Physics\"]:\n\t        dataset = Coauthor(root=os.path.join(path, name), name=name)\n\t    elif name in ['WikiCS']:\n", "        dataset = WikiCS(root=os.path.join(path, name))\n\t    elif name in ['Chameleon', 'Squirrel', 'Crocodile']:\n\t        dataset = WikipediaNetwork(root=os.path.join(path, name), name=name)\n\t    elif name in ['Cornell', 'Texas', 'Wisconsin']:\n\t        dataset = WebKB(root=os.path.join(path, name), name=name)\n\t    elif name == 'Actor':\n\t        dataset = Actor(root=os.path.join(path, name))\n\t    elif name in ['blogcatalog', 'flickr']:\n\t        dataset = AttributedGraphDataset(root=os.path.join(path, name), name=name)\n\t    else:\n", "        exit(\"wrong dataset\")\n\t    return dataset"]}
{"filename": "opengsl/data/dataset/dataset.py", "chunked_list": ["import torch\n\tfrom .pyg_load import pyg_load_dataset\n\tfrom .hetero_load import hetero_load\n\tfrom .split import get_split\n\tfrom opengsl.data.preprocess.normalize import normalize\n\timport numpy as np\n\tfrom opengsl.data.preprocess.control_homophily import control_homophily\n\timport pickle\n\timport os\n\timport urllib.request\n", "class Dataset:\n\t    '''\n\t    Dataset Class.\n\t    This class loads, preprocesses and splits various datasets.\n\t    Parameters\n\t    ----------\n\t    data : str\n\t        The name of dataset.\n\t    feat_norm : bool\n\t        Whether to normalize the features.\n", "    verbose : bool\n\t        Whether to print statistics.\n\t    n_splits : int\n\t        Number of data splits.\n\t    homophily_control : float\n\t        The homophily ratio `control homophily` receives. If set to `None`, the original adj will be kept unchanged.\n\t    path : str\n\t        Path to save dataset files.\n\t    '''\n\t    def __init__(self, data, feat_norm=False, verbose=True, n_splits=1, homophily_control=None, path='./data/'):\n", "        self.name = data\n\t        self.path = path\n\t        self.device = torch.device('cuda')\n\t        self.prepare_data(data, feat_norm, verbose)\n\t        self.split_data(n_splits, verbose)\n\t        if homophily_control:\n\t            self.adj = control_homophily(self.adj, self.labels.cpu().numpy(), homophily_control)\n\t    def prepare_data(self, ds_name, feat_norm=False, verbose=True):\n\t        '''\n\t        Function to Load various datasets.\n", "        Homophilous datasets are loaded via pyg, while heterophilous datasets are loaded with `hetero_load`.\n\t        The results are saved as `self.feats, self.adj, self.labels, self.train_masks, self.val_masks, self.test_masks`.\n\t        Noth that `self.adj` is undirected and has no self loops.\n\t        Parameters\n\t        ----------\n\t        ds_name : str\n\t            The name of dataset.\n\t        feat_norm : bool\n\t            Whether to normalize the features.\n\t        verbose : bool\n", "            Whether to print statistics.\n\t        '''\n\t        if ds_name in ['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho', 'coauthorcs', 'coauthorph', 'blogcatalog',\n\t                       'flickr']:\n\t            self.data_raw = pyg_load_dataset(ds_name, path=self.path)\n\t            self.g = self.data_raw[0]\n\t            self.feats = self.g.x  # unnormalized\n\t            if ds_name == 'flickr':\n\t                self.feats = self.feats.to_dense()\n\t            self.n_nodes = self.feats.shape[0]\n", "            self.dim_feats = self.feats.shape[1]\n\t            self.labels = self.g.y\n\t            self.adj = torch.sparse.FloatTensor(self.g.edge_index, torch.ones(self.g.edge_index.shape[1]),\n\t                                                [self.n_nodes, self.n_nodes])\n\t            self.n_edges = self.g.num_edges/2\n\t            self.n_classes = self.data_raw.num_classes\n\t            self.feats = self.feats.to(self.device)\n\t            self.labels = self.labels.to(self.device)\n\t            self.adj = self.adj.to(self.device)\n\t            # normalize features\n", "            if feat_norm:\n\t                self.feats = normalize(self.feats, style='row')\n\t        elif ds_name in ['amazon-ratings', 'questions', 'chameleon-filtered', 'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'tolokers']:\n\t            self.feats, self.adj, self.labels, self.splits = hetero_load(ds_name, path=self.path)\n\t            self.feats = self.feats.to(self.device)\n\t            self.labels = self.labels.to(self.device)\n\t            self.adj = self.adj.to(self.device)\n\t            self.n_nodes = self.feats.shape[0]\n\t            self.dim_feats = self.feats.shape[1]\n\t            self.n_edges = len(self.adj.coalesce().values())/2\n", "            if feat_norm:\n\t                self.feats = normalize(self.feats, style='row')\n\t                # exit(0)\n\t            self.n_classes = len(self.labels.unique())\n\t        else:\n\t            print('dataset not implemented')\n\t            exit(0)\n\t        if verbose:\n\t            print(\"\"\"----Data statistics------'\n\t                #Nodes %d\n", "                #Edges %d\n\t                #Classes %d\"\"\" %\n\t                  (self.n_nodes, self.n_edges, self.n_classes))\n\t        self.num_targets = self.n_classes\n\t        if self.num_targets == 2:\n\t            self.num_targets = 1\n\t    def split_data(self, n_splits, verbose=True):\n\t        '''\n\t        Function to conduct data splitting for various datasets.\n\t        Parameters\n", "        ----------\n\t        n_splits : int\n\t            Number of data splits.\n\t        verbose : bool\n\t            Whether to print statistics.\n\t        '''\n\t        self.train_masks = []\n\t        self.val_masks = []\n\t        self.test_masks = []\n\t        if self.name in ['blogcatalog', 'flickr']:\n", "            def load_obj(file_name):\n\t                with open(file_name, 'rb') as f:\n\t                    return pickle.load(f)\n\t            def download(name):\n\t                url = 'https://github.com/zhao-tong/GAug/raw/master/data/graphs/'\n\t                try:\n\t                    print('Downloading', url + name)\n\t                    urllib.request.urlretrieve(url + name, os.path.join(self.path, name))\n\t                    print('Done!')\n\t                except:\n", "                    raise Exception(\n\t                        '''Download failed! Make sure you have stable Internet connection and enter the right name''')\n\t            split_file = self.name + '_tvt_nids.pkl'\n\t            if not os.path.exists(os.path.join(self.path, split_file)):\n\t                download(split_file)\n\t            train_indices, val_indices, test_indices = load_obj(os.path.join(self.path, split_file))\n\t            for i in range(n_splits):\n\t                self.train_masks.append(train_indices)\n\t                self.val_masks.append(val_indices)\n\t                self.test_masks.append(test_indices)\n", "        elif self.name in ['coauthorcs', 'coauthorph', 'amazoncom', 'amazonpho']:\n\t            for i in range(n_splits):\n\t                np.random.seed(i)\n\t                train_indices, val_indices, test_indices = get_split(self.labels.cpu().numpy(), train_examples_per_class=20, val_examples_per_class=30)  # é»è®¤éå20-30-restè¿ç§åå\n\t                self.train_masks.append(train_indices)\n\t                self.val_masks.append(val_indices)\n\t                self.test_masks.append(test_indices)\n\t        elif self.name in ['cora', 'citeseer', 'pubmed']:\n\t            for i in range(n_splits):\n\t                self.train_masks.append(torch.nonzero(self.g.train_mask, as_tuple=False).squeeze().numpy())\n", "                self.val_masks.append(torch.nonzero(self.g.val_mask, as_tuple=False).squeeze().numpy())\n\t                self.test_masks.append(torch.nonzero(self.g.test_mask, as_tuple=False).squeeze().numpy())\n\t        elif self.name in ['amazon-ratings', 'questions', 'chameleon-filtered', 'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'tolokers']:\n\t            assert n_splits < 10 , 'n_splits > splits provided'\n\t            self.train_masks = self.splits[0][:n_splits]\n\t            self.val_masks = self.splits[1][:n_splits]\n\t            self.test_masks = self.splits[2][:n_splits]\n\t        # elif ds_name in ['penn94']:\n\t        #     train_indices = self.splits[seed]['train']\n\t        #     val_indices = self.splits[seed]['valid']\n", "        #     test_indices = self.splits[seed]['test']\n\t        #     self.train_mask = generate_mask_tensor(sample_mask(train_indices, self.n_nodes))\n\t        #     self.val_mask = generate_mask_tensor(sample_mask(val_indices, self.n_nodes))\n\t        #     self.test_mask = generate_mask_tensor(sample_mask(test_indices, self.n_nodes))\n\t        else:\n\t            print('dataset not implemented')\n\t            exit(0)\n\t        if verbose:\n\t            print(\"\"\"----Split statistics of %d splits------'\n\t                #Train samples %d\n", "                #Val samples %d\n\t                #Test samples %d\"\"\" %\n\t                  (n_splits, len(self.train_masks[0]), len(self.val_masks[0]), len(self.test_masks[0])))"]}
{"filename": "opengsl/data/dataset/split.py", "chunked_list": ["'''\n\tThis file is to split Coauthor/Amazon dataset\n\t'''\n\timport numpy as np\n\tdef sample_per_class(labels, num_examples_per_class, forbidden_indices=None):\n\t    num_samples = len(labels)\n\t    num_classes = labels.max() + 1\n\t    sample_indices_per_class = {index: [] for index in range(num_classes)}\n\t    # get indices sorted by class\n\t    for class_index in range(num_classes):\n", "        for sample_index in range(num_samples):\n\t            if labels[sample_index] == class_index:\n\t                if forbidden_indices is None or sample_index not in forbidden_indices:\n\t                    sample_indices_per_class[class_index].append(sample_index)\n\t    # get specified number of indices for each class\n\t    return np.concatenate(\n\t        [np.random.choice(sample_indices_per_class[class_index], num_examples_per_class, replace=False)\n\t         for class_index in range(len(sample_indices_per_class))\n\t         ])\n\tdef get_split(labels, train_examples_per_class=None, val_examples_per_class=None, test_examples_per_class=None,\n", "              train_size=None, val_size=None, test_size=None):\n\t    num_samples = len(labels)\n\t    num_classes = labels.max() + 1\n\t    remaining_indices = list(range(num_samples))\n\t    if train_examples_per_class is not None:\n\t        train_indices = sample_per_class(labels, train_examples_per_class)\n\t    else:\n\t        # select train examples with no respect to class distribution\n\t        train_indices = np.random.choice(remaining_indices, train_size, replace=False)\n\t    if val_examples_per_class is not None:\n", "        val_indices = sample_per_class(labels, val_examples_per_class, forbidden_indices=train_indices)\n\t    else:\n\t        remaining_indices = np.setdiff1d(remaining_indices, train_indices)\n\t        val_indices = np.random.choice(remaining_indices, val_size, replace=False)\n\t    forbidden_indices = np.concatenate((train_indices, val_indices))\n\t    if test_examples_per_class is not None:\n\t        test_indices = sample_per_class(labels, test_examples_per_class, forbidden_indices=forbidden_indices)\n\t    elif test_size is not None:\n\t        remaining_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n\t        test_indices = np.random.choice(remaining_indices, test_size, replace=False)\n", "    else:\n\t        test_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n\t    # assert that there are no duplicates in sets\n\t    assert len(set(train_indices)) == len(train_indices)\n\t    assert len(set(val_indices)) == len(val_indices)\n\t    assert len(set(test_indices)) == len(test_indices)\n\t    # assert sets are mutually exclusive\n\t    assert len(set(train_indices) - set(val_indices)) == len(set(train_indices))\n\t    assert len(set(train_indices) - set(test_indices)) == len(set(train_indices))\n\t    assert len(set(val_indices) - set(test_indices)) == len(set(val_indices))\n", "    if test_size is None and test_examples_per_class is None:\n\t        # all indices must be part of the split\n\t        assert len(np.concatenate((train_indices, val_indices, test_indices))) == num_samples\n\t    return train_indices, val_indices, test_indices"]}
{"filename": "opengsl/data/dataset/__init__.py", "chunked_list": ["from opengsl.data.dataset.dataset import Dataset"]}
{"filename": "opengsl/data/preprocess/normalize.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport scipy.sparse as sp\n\tfrom opengsl.utils.utils import scipy_sparse_to_sparse_tensor ,sparse_tensor_to_scipy_sparse\n\tdef normalize(mx, style='symmetric', add_loop=True, sparse=False):\n\t    '''\n\t    Normalize the feature matrix or adj matrix.\n\t    Parameters\n\t    ----------\n\t    mx : torch.tensor\n", "        Feature matrix or adj matrix to normalize.\n\t    style: str\n\t        If set as ``row``, `mx` will be row-wise normalized.\n\t        If set as ``symmetric``, `mx` will be normalized as in GCN.\n\t    add_loop : bool\n\t        Whether to add self loop.\n\t    sparse : bool\n\t        Whether the matrix is stored in sparse form. The returned tensor will be the same form.\n\t    Returns\n\t    -------\n", "    normalized_mx : torch.tensor\n\t        The normalized matrix.\n\t    '''\n\t    if style == 'row':\n\t        return row_nomalize(mx)\n\t    elif style == 'symmetric':\n\t        if sparse:\n\t            return normalize_sp_tensor(mx, add_loop)\n\t        else:\n\t            return normalize_tensor(mx, add_loop)\n", "def row_nomalize(mx):\n\t    \"\"\"Row-normalize sparse matrix.\n\t    \"\"\"\n\t    device = mx.device\n\t    mx = mx.cpu().numpy()\n\t    r_sum = np.array(mx.sum(1))\n\t    r_inv = np.power(r_sum, -1).flatten()\n\t    r_inv[np.isinf(r_inv)] = 0.\n\t    r_mat_inv = sp.diags(r_inv)\n\t    mx = r_mat_inv.dot(mx)\n", "    mx = torch.tensor(mx).to(device)\n\t    return mx\n\tdef normalize_tensor(adj, add_loop=True):\n\t    device = adj.device\n\t    adj_loop = adj + torch.eye(adj.shape[0]).to(device) if add_loop else adj\n\t    rowsum = adj_loop.sum(1)\n\t    r_inv = rowsum.pow(-1/2).flatten()\n\t    r_inv[torch.isinf(r_inv)] = 0.\n\t    r_mat_inv = torch.diag(r_inv)\n\t    A = r_mat_inv @ adj_loop\n", "    A = A @ r_mat_inv\n\t    return A\n\tdef normalize_sp_tensor(adj, add_loop=True):\n\t    device = adj.device\n\t    adj = sparse_tensor_to_scipy_sparse(adj)\n\t    adj = normalize_sp_matrix(adj, add_loop)\n\t    adj = scipy_sparse_to_sparse_tensor(adj).to(device)\n\t    return adj\n\tdef normalize_sp_matrix(adj, add_loop=True):\n\t    mx = adj + sp.eye(adj.shape[0]) if add_loop else adj\n", "    rowsum = np.array(mx.sum(1))\n\t    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n\t    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n\t    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n\t    new = mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n\t    return new"]}
{"filename": "opengsl/data/preprocess/__init__.py", "chunked_list": ["from opengsl.data.preprocess.normalize import normalize\n\tfrom opengsl.data.preprocess.control_homophily import control_homophily"]}
{"filename": "opengsl/data/preprocess/control_homophily.py", "chunked_list": ["from opengsl.utils.utils import get_homophily\n\timport numpy as np\n\tdef control_homophily(adj, labels, homophily):\n\t    '''\n\t    Control the homophily of original structure by adding edges.\n\t    More ways to add perturbations will be implemented soon.\n\t    Parameters\n\t    ----------\n\t    adj : torch.tensor\n\t        The original structure in sparse form.\n", "    labels : torch.tensor\n\t        Ground truth labels.\n\t    homophily : float\n\t        Homophily ratio.\n\t    Returns\n\t    -------\n\t    new_adj : torch.tensor\n\t        The perturbed structure in sparse form.\n\t    '''\n\t    np.random.seed(0)\n", "    # change homophily through adding edges\n\t    adj = adj.to_dense()\n\t    n_edges = adj.sum()/2\n\t    n_nodes = len(labels)\n\t    homophily_orig = get_homophily(labels, adj, 'edge')\n\t    # print(homophily_orig)\n\t    if homophily<homophily_orig:\n\t        # add noisy edges\n\t        n_add_edges = int(n_edges*homophily_orig/homophily-n_edges)\n\t        while n_add_edges>0:\n", "            u = np.random.randint(0, n_nodes)\n\t            vs = np.arange(0, n_nodes)[labels!=labels[u]]\n\t            v = np.random.choice(vs)\n\t            if adj[u, v]==0:\n\t                adj[u,v]=adj[v,u]=1\n\t                n_add_edges-=1\n\t    if homophily>homophily_orig:\n\t        # add helpful edges\n\t        n_add_edges = int(n_edges*(1-homophily_orig)/(1-homophily)-n_edges)\n\t        while n_add_edges > 0:\n", "            u = np.random.randint(0, n_nodes)\n\t            vs = np.arange(0, n_nodes)[labels==labels[u]]\n\t            v = np.random.choice(vs)\n\t            if u==v:\n\t                continue\n\t            if adj[u,v]==0:\n\t                adj[u,v]=adj[v,u]=1\n\t                n_add_edges -= 1\n\t    return adj.to_sparse()\n"]}
{"filename": "opengsl/expmanager/__init__.py", "chunked_list": []}
{"filename": "opengsl/expmanager/ExpManager.py", "chunked_list": ["import torch\n\tfrom opengsl.utils.utils import set_seed\n\tfrom opengsl.utils.logger import Logger\n\tfrom opengsl.config.util import save_conf\n\timport os\n\timport time as time\n\tclass ExpManager:\n\t    '''\n\t    Experiment management class to enable running multiple experiment,\n\t    loading learned structures and saving results.\n", "    Parameters\n\t    ----------\n\t    solver : opengsl.method.Solver\n\t        Solver of the method to solve the task.\n\t    n_splits : int\n\t        Number of data splits to run experiment on.\n\t    n_runs : int\n\t        Number of experiment runs each split.\n\t    save_path : str\n\t        Path to save the config file.\n", "    debug : bool\n\t        Whether to print statistics during training.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.dataset.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('gcn', 'cora')\n", "    >>> # create solver\n\t    >>> import opengsl.method.SGCSolver\n\t    >>> solver = SGCSolver(conf, dataset)\n\t    >>>\n\t    >>> import opengsl.ExpManager\n\t    >>> exp = ExpManager(solver)\n\t    >>> exp.run(n_runs=10, debug=True)\n\t    '''\n\t    def __init__(self, solver=None, save_path=None):\n\t        self.solver = solver\n", "        self.conf = solver.conf\n\t        self.method = solver.method_name\n\t        self.dataset = solver.dataset\n\t        self.data = self.dataset.name\n\t        self.device = torch.device('cuda')\n\t        # you can change random seed here\n\t        self.train_seeds = [i for i in range(400)]\n\t        self.split_seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\t        self.save_path = None\n\t        self.save_graph_path = None\n", "        self.load_graph_path = None\n\t        if save_path:\n\t            if not os.path.exists(save_path):\n\t                os.makedirs(save_path)\n\t            self.save_path = save_path\n\t        if 'save_graph' in self.conf.analysis and self.conf.analysis['save_graph']:\n\t            assert 'save_graph_path' in self.conf.analysis and self.conf.analysis['save_graph_path'] is not None, 'Specify the path to save graph'\n\t            self.save_graph_path = os.path.join(self.conf.analysis['save_graph_path'], self.method)\n\t        if 'load_graph' in self.conf.analysis and self.conf.analysis['load_graph']:\n\t            assert 'load_graph_path' in self.conf.analysis and self.conf.analysis[\n", "                'load_graph_path'] is not None, 'Specify the path to load graph'\n\t            self.load_graph_path = self.conf.analysis['load_graph_path']\n\t        assert self.save_graph_path is None or self.load_graph_path is None, 'GNN does not save graph, GSL does not load graph'\n\t    def run(self, n_splits=1, n_runs=1, debug=False):\n\t        total_runs = n_runs * n_splits\n\t        assert n_splits <= len(self.split_seeds)\n\t        assert total_runs <= len(self.train_seeds)\n\t        logger = Logger(runs=total_runs)\n\t        for i in range(n_splits):\n\t            succeed = 0\n", "            for j in range(400):\n\t                idx = i * n_runs + j\n\t                print(\"Exp {}/{}\".format(idx, total_runs))\n\t                set_seed(self.train_seeds[idx])\n\t                # load graph\n\t                if self.load_graph_path:\n\t                    self.solver.adj = torch.load(os.path.join(self.load_graph_path,'{}_{}_{}.pth'.format(self.data, i, self.train_seeds[idx]))).to(self.device)\n\t                    if self.conf.dataset['sparse']:\n\t                        self.solver.adj = self.solver.adj.to_sparse()\n\t                # run an exp\n", "                try:\n\t                    result, graph = self.solver.run_exp(split=i, debug=debug)\n\t                except ValueError:\n\t                    continue\n\t                logger.add_result(succeed, result)\n\t                # save graph\n\t                if self.save_graph_path:\n\t                    if not os.path.exists(self.save_graph_path):\n\t                        os.makedirs(self.save_graph_path)\n\t                    torch.save(graph.cpu(), os.path.join(self.save_graph_path, '{}_{}_{}.pth'.format(self.data, i, self.train_seeds[succeed])))\n", "                succeed += 1\n\t                if succeed == total_runs:\n\t                    break\n\t        self.acc_save, self.std_save = logger.print_statistics()\n\t        if self.save_path:\n\t            save_conf(os.path.join(self.save_path, '{}-{}-'.format(self.method, self.data) +\n\t                                   time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime()) + '.yaml'), self.conf)\n\t        return float(self.acc_save), float(self.std_save)\n"]}
{"filename": "opengsl/method/solver.py", "chunked_list": ["import torch\n\tfrom copy import deepcopy\n\timport time\n\tfrom ..utils.utils import accuracy\n\tfrom ..utils.recorder import Recorder\n\tfrom sklearn.metrics import roc_auc_score\n\timport torch.nn.functional as F\n\tclass Solver:\n\t    '''\n\t    Base solver class to conduct a single experiment. It defines the abstract training procedures\n", "    which can be overwritten in subclass solver for each method.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Configuration file.\n\t    dataset : opengsl.data.Dataset\n\t        Dataset to be conduct an experiment on.\n\t    Attributes\n\t    ----------\n\t    conf : argparse.Namespace\n", "        Configuration file.\n\t    dataset : opengsl.data.Dataset\n\t        Dataset to be conduct an experiment on.\n\t    model : nn.Module\n\t        Model of the method.\n\t    loss_fn : function\n\t        Loss function, either `F.binary_cross_entropy_with_logits` or `F.cross_entropy`.\n\t    metric : functrion\n\t        Metric function, either 'roc_auc_score' or 'accuracy'.\n\t    '''\n", "    def __init__(self, conf, dataset):\n\t        self.dataset = dataset\n\t        self.conf = conf\n\t        self.device = torch.device('cuda')\n\t        self.n_nodes = dataset.n_nodes\n\t        self.dim_feats = dataset.dim_feats\n\t        self.num_targets = dataset.num_targets\n\t        self.n_classes = dataset.n_classes\n\t        self.model = None\n\t        self.loss_fn = F.binary_cross_entropy_with_logits if self.num_targets == 1 else F.cross_entropy\n", "        self.metric = roc_auc_score if self.num_targets == 1 else accuracy\n\t        self.model = None\n\t        self.feats = dataset.feats\n\t        self.adj = dataset.adj if self.conf.dataset['sparse'] else dataset.adj.to_dense()\n\t        self.labels = dataset.labels\n\t        self.train_masks = dataset.train_masks\n\t        self.val_masks = dataset.val_masks\n\t        self.test_masks = dataset.test_masks\n\t    def run_exp(self, split=None, debug=False):\n\t        '''\n", "        Function to start an experiment.\n\t        Parameters\n\t        ----------\n\t        split : int\n\t            Specify the idx of a split among mutiple splits. Set to 0 if not specified.\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n", "            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure. `None` for GNN methods.\n\t        '''\n\t        self.set(split)\n\t        return self.learn(debug)\n\t    def set(self, split):\n\t        '''\n\t        This conducts necessary operations for an experiment, including the setting specified split,\n\t        variables to record statistics, models.\n", "        Parameters\n\t        ----------\n\t        split : int\n\t            Specify the idx of a split among mutiple splits. Set to 0 if not specified.\n\t        '''\n\t        if split is None:\n\t            print('split set to default 0.')\n\t            split=0\n\t        assert split<len(self.train_masks), 'error, split id is larger than number of splits'\n\t        self.train_mask = self.train_masks[split]\n", "        self.val_mask = self.val_masks[split]\n\t        self.test_mask = self.test_masks[split]\n\t        self.total_time = 0\n\t        self.best_val_loss = 10\n\t        self.weights = None\n\t        self.best_graph = None\n\t        self.result = {'train': 0, 'valid': 0, 'test': 0}\n\t        self.start_time = time.time()\n\t        self.recoder = Recorder(self.conf.training['patience'], self.conf.training['criterion'])\n\t        self.set_method()\n", "    def set_method(self):\n\t        '''\n\t        This sets model and other members, which is overwritten for each method.\n\t        '''\n\t        self.model = None\n\t        self.optim = None\n\t    def learn(self, debug=False):\n\t        '''\n\t        This is the common learning procedure, which is overwritten for special learning procedure.\n\t        Parameters\n", "        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure. `None` for GNN methods.\n\t        '''\n", "        for epoch in range(self.conf.training['n_epochs']):\n\t            improve = ''\n\t            t0 = time.time()\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n\t            output = self.model(self.input_distributer())\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(),\n\t                                    output[self.train_mask].detach().cpu().numpy())\n", "            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val = self.evaluate(self.val_mask)\n\t            flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\t            # save\n\t            if flag:\n\t                improve = '*'\n\t                self.total_time = time.time() - self.start_time\n\t                self.best_val_loss = loss_val\n", "                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t                self.weights = deepcopy(self.model.state_dict())\n\t            elif flag_earlystop:\n\t                break\n\t            if debug:\n\t                print(\n\t                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n", "        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, None\n\t    def evaluate(self, val_mask):\n\t        '''\n\t        This is the common evaluation procedure, which is overwritten for special evaluation procedure.\n\t        Parameters\n\t        ----------\n", "        val_mask : torch.tensor\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n", "            output = self.model(self.input_distributer())\n\t        logits = output[val_mask]\n\t        labels = self.labels[val_mask]\n\t        loss = self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\t    def input_distributer(self):\n\t        '''\n\t        This distributes different input in `learn` for different methods, which is overwritten for each method.\n\t        '''\n\t        return None\n", "    def test(self):\n\t        '''\n\t        This is the common test procedure, which is overwritten for special test procedure.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Test loss.\n\t        metric : float\n\t            Test metric.\n\t        '''\n", "        self.model.load_state_dict(self.weights)\n\t        return self.evaluate(self.test_mask)\n"]}
{"filename": "opengsl/method/__init__.py", "chunked_list": ["'''\n\tmethod_name = ['gcn', 'sgc', 'gat', 'jknet', 'appnp', 'gprgnn',\n\t'prognn', 'idgl', 'grcn', 'gaug', 'slaps', 'gen', 'gt', 'nodeformer', 'cogsl', 'sublime', 'stable', 'segsl']\n\t'''\n\tfrom opengsl.method.solver import Solver\n\tfrom opengsl.method.gnnsolver import GCNSolver\n\tfrom opengsl.method.gnnsolver import SGCSolver\n\tfrom opengsl.method.gnnsolver import GATSolver\n\tfrom opengsl.method.gnnsolver import JKNetSolver\n\tfrom opengsl.method.gnnsolver import APPNPSolver\n", "from opengsl.method.gnnsolver import GPRGNNSolver\n\tfrom opengsl.method.gnnsolver import LINKSolver\n\tfrom opengsl.method.gnnsolver import LPASolver\n\tfrom opengsl.method.gslsolver import WSGNNSolver\n\tfrom opengsl.method.gslsolver import PROGNNSolver\n\tfrom opengsl.method.gslsolver import IDGLSolver\n\tfrom opengsl.method.gslsolver import GRCNSolver\n\tfrom opengsl.method.gslsolver import GAUGSolver\n\tfrom opengsl.method.gslsolver import SLAPSSolver\n\tfrom opengsl.method.gslsolver import GENSolver\n", "from opengsl.method.gslsolver import GTSolver\n\tfrom opengsl.method.gslsolver import NODEFORMERSolver\n\tfrom opengsl.method.gslsolver import COGSLSolver\n\tfrom opengsl.method.gslsolver import SUBLIMESolver\n\tfrom opengsl.method.gslsolver import STABLESolver\n\tfrom opengsl.method.gslsolver import SEGSLSolver"]}
{"filename": "opengsl/method/gslsolver.py", "chunked_list": ["import scipy.sparse as sp\n\tfrom sklearn.metrics.pairwise import cosine_similarity as cos\n\timport numpy as np\n\timport random\n\tfrom copy import deepcopy\n\tfrom .models.gcn import GCN\n\tfrom .models.gnn_modules import APPNP\n\tfrom .models.grcn import GRCN\n\tfrom .models.gaug import GAug, eval_edge_pred, MultipleOptimizer, get_lr_schedule_by_sigmoid\n\tfrom .models.gen import EstimateAdj as GENEstimateAdj, prob_to_adj\n", "from .models.idgl import IDGL, sample_anchors, diff, compute_anchor_adj\n\tfrom .models.prognn import PGD, prox_operators, EstimateAdj, feature_smoothing\n\tfrom .models.gt import GT\n\tfrom .models.slaps import SLAPS\n\tfrom .models.nodeformer import NodeFormer, adj_mul\n\tfrom .models.segsl import knn_maxE1, add_knn, get_weight, get_adj_matrix, PartitionTree, get_community, reshape\n\tfrom .models.sublime import torch_sparse_to_dgl_graph, FGP_learner, ATT_learner, GNN_learner, MLP_learner, GCL, get_feat_mask, split_batch, dgl_graph_to_torch_sparse, GCN_SUB\n\tfrom .models.stable import DGI, preprocess_adj, aug_random_edge, get_reliable_neighbors\n\tfrom .models.cogsl import CoGSL\n\tfrom .models.wsgnn import WSGNN, ELBONCLoss\n", "import torch\n\timport torch.nn.functional as F\n\timport time\n\tfrom .solver import Solver\n\tfrom ..utils.utils import get_homophily, sparse_tensor_to_scipy_sparse, scipy_sparse_to_sparse_tensor\n\tfrom opengsl.data.preprocess.normalize import normalize, normalize_sp_matrix\n\tfrom ..utils.recorder import Recorder\n\timport dgl\n\timport copy\n\timport os\n", "from os.path import dirname\n\tclass GRCNSolver(Solver):\n\t    '''\n\t        A solver to train, evaluate, test GRCN in a run.\n\t        Parameters\n\t        ----------\n\t        conf : argparse.Namespace\n\t            Config file.\n\t        dataset : opengsl.data.Dataset\n\t            The dataset.\n", "        Attributes\n\t        ----------\n\t        method_name : str\n\t            The name of the method.\n\t        Examples\n\t        --------\n\t        >>> # load dataset\n\t        >>> import opengsl.dataset\n\t        >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t        >>> # load config file\n", "        >>> import opengsl.config.load_conf\n\t        >>> conf = opengsl.config.load_conf('grcn', 'cora')\n\t        >>>\n\t        >>> solver = GRCNSolver(conf, dataset)\n\t        >>> # Conduct a experiment run.\n\t        >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t        '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"grcn\"\n", "        print(\"Solver Version : [{}]\".format(\"grcn\"))\n\t        edge_index = self.adj.coalesce().indices().cpu()\n\t        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n\t        edges = torch.cat([edge_index, loop_edge_index], dim=1)\n\t        self.adj = torch.sparse.FloatTensor(edges, torch.ones(edges.shape[1]), [self.n_nodes, self.n_nodes]).to(self.device).coalesce()\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of GRCN.\n\t        Parameters\n\t        ----------\n", "        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n\t        for epoch in range(self.conf.training['n_epochs']):\n", "            improve = ''\n\t            t0 = time.time()\n\t            self.model.train()\n\t            self.optim1.zero_grad()\n\t            self.optim2.zero_grad()\n\t            # forward and backward\n\t            output, _ = self.model(self.feats, self.adj)\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n", "            self.optim1.step()\n\t            self.optim2.step()\n\t            # Evaluate\n\t            loss_val, acc_val, adj = self.evaluate(self.val_mask)\n\t            # save\n\t            if acc_val > self.result['valid']:\n\t                self.total_time = time.time() - self.start_time\n\t                improve = '*'\n\t                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n", "                self.result['train'] = acc_train\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                if self.conf.analysis['save_graph']:\n\t                    self.best_graph = deepcopy(adj.to_dense())\n\t            # print\n\t            if debug:\n\t                print(\n\t                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n", "        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test, _ = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, self.best_graph\n\t    def evaluate(self, test_mask):\n\t        '''\n\t        Evaluation procedure of GRCN.\n\t        Parameters\n\t        ----------\n", "        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        adj : torch.tensor        \n\t            The learned structure.\n", "        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            output, adj = self.model(self.feats, self.adj)\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss=self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), adj\n\t    def set_method(self):\n\t        '''\n", "        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = GRCN(self.n_nodes, self.dim_feats, self.num_targets, self.device, self.conf).to(self.device)\n\t        self.optim1 = torch.optim.Adam(self.model.base_parameters(), lr=self.conf.training['lr'],\n\t                                       weight_decay=self.conf.training['weight_decay'])\n\t        self.optim2 = torch.optim.Adam(self.model.graph_parameters(), lr=self.conf.training['lr_graph'])\n\tclass GAUGSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test GAug in a run.\n\t    Parameters\n", "    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n", "    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('gaug', 'cora')\n\t    >>>\n\t    >>> solver = GAUGSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n", "    >>> acc, new_strcuture = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gaug\"\n\t        print(\"Solver Version : [{}]\".format(\"gaug\"))\n\t        self.normalized_adj = normalize(self.adj, sparse=True).to(self.device)\n\t        self.adj_orig = (self.adj.to_dense() + torch.eye(self.n_nodes).to(self.device))  # adj with self loop\n\t        self.conf = conf\n\t    def pretrain_ep_net(self, norm_w, pos_weight, n_epochs, debug=False):\n", "        \"\"\" pretrain the edge prediction network \"\"\"\n\t        optimizer = torch.optim.Adam(self.model.ep_net.parameters(), lr=self.conf.training['lr'])\n\t        self.model.train()\n\t        for epoch in range(n_epochs):\n\t            t = time.time()\n\t            optimizer.zero_grad()\n\t            adj_logits = self.model.ep_net(self.feats, self.normalized_adj)\n\t            loss = norm_w * F.binary_cross_entropy_with_logits(adj_logits, self.adj_orig, pos_weight=pos_weight)\n\t            if not self.conf.gsl['gae']:\n\t                mu = self.model.ep_net.mean\n", "                lgstd = self.model.ep_net.logstd\n\t                kl_divergence = 0.5/adj_logits.size(0) * (1 + 2*lgstd - mu**2 - torch.exp(2*lgstd)).sum(1).mean()\n\t                loss -= kl_divergence\n\t            loss.backward()\n\t            optimizer.step()\n\t            adj_pred = torch.sigmoid(adj_logits.detach()).cpu()\n\t            ep_auc, ep_ap = eval_edge_pred(adj_pred, self.val_edges, self.edge_labels)\n\t            if debug:\n\t                print('EPNet pretrain, Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | auc {:.4f} | ap {:.4f}'\n\t                                 .format(epoch+1, time.time()-t, loss.item(), ep_auc, ep_ap))\n", "    def pretrain_nc_net(self, n_epochs, debug=False):\n\t        \"\"\" pretrain the node classification network \"\"\"\n\t        optimizer = torch.optim.Adam(self.model.nc_net.parameters(),\n\t                                     lr=self.conf.training['lr'],\n\t                                     weight_decay=self.conf.training['weight_decay'])\n\t        # loss function for node classification\n\t        for epoch in range(n_epochs):\n\t            t = time.time()\n\t            improve = ''\n\t            self.model.train()\n", "            optimizer.zero_grad()\n\t            # forward and backward\n\t            hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            optimizer.step()\n\t            # evaluate\n\t            self.model.eval()\n\t            with torch.no_grad():\n", "                hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))\n\t                loss_val = self.loss_fn(output[self.val_mask], self.labels[self.val_mask])\n\t            acc_val = self.metric(self.labels[self.val_mask].cpu().numpy(), output[self.val_mask].detach().cpu().numpy())\n\t            if acc_val > self.result['valid']:\n\t                self.total_time = time.time() - self.start_time\n\t                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t                improve = '*'\n\t                self.weights = deepcopy(self.model.state_dict())\n", "                self.best_graph = self.adj.to_dense()\n\t            # print\n\t            if debug:\n\t                print(\"NCNet pretrain, Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                    epoch+1, time.time() -t, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of GAUG.\n\t        Parameters\n\t        ----------\n", "        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n\t        patience_step = 0\n", "        # prepare\n\t        adj_t = self.adj_orig\n\t        norm_w = adj_t.shape[0]**2 / float((adj_t.shape[0]**2 - adj_t.sum()) * 2)\n\t        pos_weight = torch.FloatTensor([float(adj_t.shape[0]**2 - adj_t.sum()) / adj_t.sum()]).to(self.device)\n\t        # pretrain\n\t        self.pretrain_ep_net(norm_w, pos_weight, self.conf.training['pretrain_ep'], debug)\n\t        self.pretrain_nc_net(self.conf.training['pretrain_nc'], debug)\n\t        # train\n\t        optims = MultipleOptimizer(torch.optim.Adam(self.model.ep_net.parameters(),\n\t                                                    lr=self.conf.training['lr']),\n", "                                   torch.optim.Adam(self.model.nc_net.parameters(),\n\t                                                    lr=self.conf.training['lr'],\n\t                                                    weight_decay=self.conf.training['weight_decay']))\n\t        # get the learning rate schedule for the optimizer of ep_net if needed\n\t        if self.conf.training['warmup']:\n\t            ep_lr_schedule = get_lr_schedule_by_sigmoid(self.conf.training['n_epochs'], self.conf.training['lr'], self.conf.training['warmup'])\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            t = time.time()\n\t            improve = ''\n\t            # update the learning rate for ep_net if needed\n", "            if self.conf.training['warmup']:\n\t                optims.update_lr(0, ep_lr_schedule[epoch])\n\t            self.model.train()\n\t            optims.zero_grad()\n\t            # forward and backward\n\t            output, adj_logits, adj_new = self.model(self.feats, self.normalized_adj, self.adj_orig)\n\t            loss_train = nc_loss = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            ep_loss = norm_w * F.binary_cross_entropy_with_logits(adj_logits, self.adj_orig, pos_weight=pos_weight)\n\t            loss_train += self.conf.training['beta'] * ep_loss\n", "            loss_train.backward()\n\t            optims.step()\n\t            # validate\n\t            self.model.eval()\n\t            with torch.no_grad():\n\t                hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))   # the author proposed to validate and test on the original adj\n\t                loss_val = self.loss_fn(output[self.val_mask], self.labels[self.val_mask])\n\t            acc_val = self.metric(self.labels[self.val_mask].cpu().numpy(), output[self.val_mask].detach().cpu().numpy())\n\t            adj_pred = torch.sigmoid(adj_logits.detach()).cpu()\n\t            ep_auc, ep_ap = eval_edge_pred(adj_pred, self.val_edges, self.edge_labels)\n", "            # save\n\t            if acc_val > self.result['valid']:\n\t                self.total_time = time.time() - self.start_time\n\t                improve = '*'\n\t                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                self.best_graph = adj_new.clone().detach()\n\t                patience_step = 0\n", "            else:\n\t                patience_step += 1\n\t                if patience_step == self.conf.training['patience']:\n\t                    print('Early stop!')\n\t                    break\n\t            # print\n\t            if debug:\n\t                print(\"Training, Epoch {:05d} | Time(s) {:.4f}\".format(epoch+1, time.time() -t))\n\t                print('    EP Loss {:.4f} | EP AUC {:.4f} | EP AP {:.4f}'.format(ep_loss, ep_auc, ep_ap))\n\t                print('    Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}'.format(nc_loss, acc_train, loss_val, acc_val, improve))\n", "        # test\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        self.model.load_state_dict(self.weights)\n\t        with torch.no_grad():\n\t            hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))\n\t            loss_test = self.loss_fn(output[self.test_mask], self.labels[self.test_mask])\n\t        acc_test = self.metric(self.labels[self.test_mask].cpu().numpy(), output[self.test_mask].detach().cpu().numpy())\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n", "        return self.result, self.best_graph\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        # sample edges\n\t        if self.labels.size(0) > 5000:\n\t            edge_frac = 0.01\n\t        else:\n\t            edge_frac = 0.1\n", "        adj_matrix = sp.csr_matrix(self.adj.to_dense().cpu().numpy())\n\t        adj_matrix.setdiag(1)  # the original code samples 10%(1%) of the total edges(with self loop)\n\t        n_edges_sample = int(edge_frac * adj_matrix.nnz / 2)\n\t        # sample negative edges\n\t        neg_edges = []\n\t        added_edges = set()\n\t        while len(neg_edges) < n_edges_sample:\n\t            i = np.random.randint(0, adj_matrix.shape[0])\n\t            j = np.random.randint(0, adj_matrix.shape[0])\n\t            if i == j:\n", "                continue\n\t            if adj_matrix[i, j] > 0:\n\t                continue\n\t            if (i, j) in added_edges:\n\t                continue\n\t            neg_edges.append([i, j])\n\t            added_edges.add((i, j))\n\t            added_edges.add((j, i))\n\t        neg_edges = np.asarray(neg_edges)\n\t        # sample positive edges\n", "        nz_upper = np.array(sp.triu(adj_matrix, k=1).nonzero()).T\n\t        np.random.shuffle(nz_upper)\n\t        pos_edges = nz_upper[:n_edges_sample]\n\t        self.val_edges = np.concatenate((pos_edges, neg_edges), axis=0)\n\t        self.edge_labels = np.array([1] * n_edges_sample + [0] * n_edges_sample)\n\t        self.model = GAug(self.dim_feats, self.num_targets, self.conf).to(self.device)\n\tclass GENSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test GEN in a run.\n\t    Parameters\n", "    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n", "    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('gen', 'cora')\n\t    >>>\n\t    >>> solver = GENSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n", "    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gen\"\n\t        print(\"Solver Version : [{}]\".format(\"gen\"))\n\t        self.homophily = get_homophily(self.labels.cpu(), self.adj.to_dense().cpu(), type='node')\n\t    def knn(self, feature):\n\t        # Generate a knn graph for input feature matrix. Note that the graph contains self loop.\n\t        adj = np.zeros((self.n_nodes, self.n_nodes), dtype=np.int64)\n", "        dist = cos(feature.detach().cpu().numpy())\n\t        col = np.argpartition(dist, -(self.conf.gsl['k'] + 1), axis=1)[:, -(self.conf.gsl['k'] + 1):].flatten()\n\t        adj[np.arange(self.n_nodes).repeat(self.conf.gsl['k'] + 1), col] = 1\n\t        return adj\n\t    def train_gcn(self, iter, adj, debug=False):\n\t        if debug:\n\t            print('==== Iteration {:04d} ===='.format(iter+1))\n\t        t = time.time()\n\t        improve_1 = ''\n\t        best_loss_val = 10\n", "        best_acc_val = 0\n\t        normalized_adj = normalize(adj, sparse=True)\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            improve_2 = ''\n\t            t0 = time.time()\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n\t            hidden_output, output = self.model((self.feats, normalized_adj, False))\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n", "            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val, hidden_output, output = self.evaluate(self.val_mask, normalized_adj)\n\t            # save\n\t            if acc_val > best_acc_val:\n\t                best_acc_val = acc_val\n\t                best_loss_val = loss_val\n\t                improve_2 = '*'\n", "                if acc_val > self.result['valid']:\n\t                    self.total_time = time.time()-self.start_time\n\t                    improve_1 = '*'\n\t                    self.best_val_loss = loss_val\n\t                    self.result['valid'] = acc_val\n\t                    self.result['train'] = acc_train\n\t                    self.best_iter = iter+1\n\t                    self.hidden_output = hidden_output\n\t                    self.output = output if len(output.shape)>1 else output.unsqueeze(1)\n\t                    self.output = F.log_softmax(self.output, dim=1)\n", "                    self.weights = deepcopy(self.model.state_dict())\n\t                    self.best_graph = deepcopy(adj)\n\t            # print\n\t            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve_2))\n\t        print('Iteration {:04d} | Time(s) {:.4f} | Loss(val):{:.4f} | Acc(val):{:.4f} | {}'.format(iter+1,time.time()-t, best_loss_val, best_acc_val, improve_1))\n\t    def structure_learning(self, iter):\n\t        t=time.time()\n\t        self.estimator.reset_obs()\n", "        self.estimator.update_obs(self.knn(self.feats))   # 2\n\t        self.estimator.update_obs(self.knn(self.hidden_output))   # 3\n\t        self.estimator.update_obs(self.knn(self.output))   # 4\n\t        alpha, beta, O, Q, iterations = self.estimator.EM(self.output.max(1)[1].detach().cpu().numpy(), self.conf.gsl['tolerance'])\n\t        adj = torch.tensor(prob_to_adj(Q, self.conf.gsl['threshold']),dtype=torch.float32, device=self.device).to_sparse()\n\t        print('Iteration {:04d} | Time(s) {:.4f} | EM step {:04d}'.format(iter+1,time.time()-t,self.estimator.count))\n\t        return adj\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of GEN.\n", "        Parameters\n\t        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n", "        '''\n\t        adj = self.adj\n\t        for iter in range(self.conf.training['n_iters']):\n\t            self.train_gcn(iter, adj, debug)\n\t            adj = self.structure_learning(iter)\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test, _, _ = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n", "        return self.result, self.best_graph\n\t    def evaluate(self, test_mask, normalized_adj):\n\t        '''\n\t        Evaluation procedure of GEN.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        normalized_adj : torch.tensor\n\t            Adjacency matrix.\n", "        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        hidden_output : torch.tensor        \n\t            Hidden output of the model.\n\t        output : torch.tensor        \n\t            Output of the model.\n", "        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            hidden_output, output = self.model((self.feats, normalized_adj, False))\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss=self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), hidden_output, output\n\t    def test(self):\n\t        '''\n", "        Test procedure of GEN.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        hidden_output : torch.tensor        \n\t            Hidden output of the model.\n\t        output : torch.tensor        \n", "            Output of the model.\n\t        '''\n\t        self.model.load_state_dict(self.weights)\n\t        normalized_adj = normalize(self.best_graph, sparse=True)\n\t        return self.evaluate(self.test_mask, normalized_adj)\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        if self.conf.model['type']=='gcn':\n", "            self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n\t                             self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n\t                             self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n\t                             self.conf.model['input_layer'], self.conf.model['output_layer'], weight_initializer='glorot',\n\t                             bias_initializer='zeros').to(self.device)\n\t        elif self.conf.model['type']=='appnp':\n\t            self.model = APPNP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets,\n\t                               dropout=self.conf.model['dropout'], K=self.conf.model['K'],\n\t                               alpha=self.conf.model['alpha']).to(self.device)\n\t        self.estimator = GENEstimateAdj(self.n_classes, self.adj.to_dense(), self.train_mask, self.labels, self.homophily)\n", "        self.optim = torch.optim.Adam(self.model.parameters(),\n\t                                      lr=self.conf.training['lr'],\n\t                                      weight_decay=self.conf.training['weight_decay'])\n\t        self.best_iter = 0\n\t        self.hidden_output = None\n\t        self.output = None\n\tclass IDGLSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test IDGL in a run.\n\t    Parameters\n", "    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n", "    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('idgl', 'cora')\n\t    >>>\n\t    >>> solver = IDGLSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n", "    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"idgl\"\n\t        print(\"Solver Version : [{}]\".format(\"idgl\"))\n\t        self.conf = conf\n\t        self.run_epoch = self._scalable_run_whole_epoch if self.conf.model['scalable_run'] else self._run_whole_epoch\n\t        if self.conf.model['scalable_run']:\n\t            self.normalized_adj = normalize(self.adj, sparse=True)\n", "        else:\n\t            self.adj = self.adj.to_dense()\n\t            self.normalized_adj = normalize(self.adj, sparse=False)\n\t    def _run_whole_epoch(self, mode='train', debug=False):\n\t        # prepare\n\t        training = mode == 'train'\n\t        if mode == 'train':\n\t            idx = self.train_mask\n\t        elif mode == 'valid':\n\t            idx = self.val_mask\n", "        else:\n\t            idx = self.test_mask\n\t        self.model.train(training)\n\t        network = self.model\n\t        # The first iter\n\t        features = F.dropout(self.feats, self.conf.gsl['feat_adj_dropout'], training=training)\n\t        init_node_vec = features\n\t        init_adj = self.normalized_adj\n\t        cur_raw_adj, cur_adj = network.learn_graph(network.graph_learner, init_node_vec, self.conf.gsl['graph_skip_conn'], graph_include_self=self.conf.gsl['graph_include_self'], init_adj=init_adj)\n\t        # cur_raw_adjæ¯æ ¹æ®è¾å¥Zç´æ¥äº§ççadj, cur_adjæ¯åèå½ä¸åå¹¶ååå§adjå ææ±åçç»æ\n", "        cur_raw_adj = F.dropout(cur_raw_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n\t        cur_adj = F.dropout(cur_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n\t        node_vec, output = network.encoder(init_node_vec, cur_adj)\n\t        score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n\t        loss1 = self.loss_fn(output[idx], self.labels[idx])\n\t        loss1 += self.get_graph_loss(cur_raw_adj, init_node_vec)\n\t        first_raw_adj, first_adj = cur_raw_adj, cur_adj\n\t        # the following iters\n\t        if training:\n\t            eps_adj = float(self.conf.gsl['eps_adj'])\n", "        else:\n\t            eps_adj = float(self.conf.gsl['test_eps_adj'])\n\t        pre_raw_adj = cur_raw_adj\n\t        pre_adj = cur_adj\n\t        loss = 0\n\t        iter_ = 0\n\t        while (iter_ == 0 or diff(cur_raw_adj, pre_raw_adj, first_raw_adj).item() > eps_adj) and iter_ < self.conf.training['max_iter']:\n\t            iter_ += 1\n\t            pre_adj = cur_adj\n\t            pre_raw_adj = cur_raw_adj\n", "            cur_raw_adj, cur_adj = network.learn_graph(network.graph_learner2, node_vec, self.conf.gsl['graph_skip_conn'], graph_include_self=self.conf.gsl['graph_include_self'], init_adj=init_adj)\n\t            update_adj_ratio = self.conf.gsl['update_adj_ratio']\n\t            cur_adj = update_adj_ratio * cur_adj + (1 - update_adj_ratio) * first_adj   # è¿éä¼¼ä¹åè®ºæä¸­æäºåºå¥ï¼ï¼\n\t            node_vec, output = network.encoder(init_node_vec, cur_adj, self.conf.gsl['gl_dropout'])\n\t            score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n\t            loss += self.loss_fn(output[idx], self.labels[idx])\n\t            loss += self.get_graph_loss(cur_raw_adj, init_node_vec)\n\t        if iter_ > 0:\n\t            loss = loss / iter_ + loss1\n\t        else:\n", "            loss = loss1\n\t        if training:\n\t            self.optimizer.zero_grad()\n\t            loss.backward()\n\t            self.optimizer.step()\n\t        return loss, score, cur_adj\n\t    def _scalable_run_whole_epoch(self, mode='train', debug=False):\n\t        # prepare\n\t        training = mode == 'train'\n\t        if mode == 'train':\n", "            idx = self.train_mask\n\t        elif mode == 'valid':\n\t            idx = self.val_mask\n\t        else:\n\t            idx = self.test_mask\n\t        self.model.train(training)\n\t        network = self.model\n\t        # init\n\t        init_adj = self.normalized_adj\n\t        features = F.dropout(self.feats, self.conf.gsl['feat_adj_dropout'], training=training)\n", "        init_node_vec = features\n\t        init_anchor_vec, sampled_node_idx = sample_anchors(init_node_vec, self.conf.model['num_anchors'])\n\t        # the first iter\n\t        # Compute n x s node-anchor relationship matrix\n\t        cur_node_anchor_adj = network.learn_graph(network.graph_learner, init_node_vec, anchor_features=init_anchor_vec)\n\t        # Compute s x s anchor graph\n\t        cur_anchor_adj = compute_anchor_adj(cur_node_anchor_adj)\n\t        cur_node_anchor_adj = F.dropout(cur_node_anchor_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n\t        cur_anchor_adj = F.dropout(cur_anchor_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n\t        first_init_agg_vec, init_agg_vec, node_vec, output = network.encoder(init_node_vec, init_adj, cur_node_anchor_adj, self.conf.gsl['graph_skip_conn'])\n", "        anchor_vec = node_vec[sampled_node_idx]\n\t        first_node_anchor_adj, first_anchor_adj = cur_node_anchor_adj, cur_anchor_adj\n\t        score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n\t        loss1 = self.loss_fn(output[idx], self.labels[idx])\n\t        loss1 += self.get_graph_loss(cur_anchor_adj, init_anchor_vec)\n\t        # the following iters\n\t        if training:\n\t            eps_adj = float(self.conf.gsl['eps_adj'])\n\t        else:\n\t            eps_adj = float(self.conf.gsl['test_eps_adj'])\n", "        pre_node_anchor_adj = cur_node_anchor_adj\n\t        loss = 0\n\t        iter_ = 0\n\t        while (iter_ == 0 or diff(cur_node_anchor_adj, pre_node_anchor_adj, cur_node_anchor_adj).item() > eps_adj) and iter_ < self.conf.training['max_iter']:\n\t            iter_ += 1\n\t            pre_node_anchor_adj = cur_node_anchor_adj\n\t            # Compute n x s node-anchor relationship matrix\n\t            cur_node_anchor_adj = network.learn_graph(network.graph_learner2, node_vec, anchor_features=anchor_vec)\n\t            # Compute s x s anchor graph\n\t            cur_anchor_adj = compute_anchor_adj(cur_node_anchor_adj)\n", "            update_adj_ratio = self.conf.gsl['update_adj_ratio']\n\t            _, _, node_vec, output = network.encoder(init_node_vec, init_adj, cur_node_anchor_adj, self.conf.gsl['graph_skip_conn'],\n\t                                           first=False, first_init_agg_vec=first_init_agg_vec, init_agg_vec=init_agg_vec, update_adj_ratio=update_adj_ratio,\n\t                                           dropout=self.conf.gsl['gl_dropout'], first_node_anchor_adj=first_node_anchor_adj)\n\t            anchor_vec = node_vec[sampled_node_idx]\n\t            score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n\t            loss += self.loss_fn(output[idx], self.labels[idx])\n\t            loss += self.get_graph_loss(cur_anchor_adj, init_anchor_vec)\n\t        if iter_ > 0:\n\t            loss = loss / iter_ + loss1\n", "        else:\n\t            loss = loss1\n\t        if training:\n\t            self.optimizer.zero_grad()\n\t            loss.backward()\n\t            self.optimizer.step()\n\t        return loss, score, cur_anchor_adj\n\t    def get_graph_loss(self, out_adj, features):\n\t        # Graph regularization\n\t        graph_loss = 0\n", "        L = torch.diagflat(torch.sum(out_adj, -1)) - out_adj\n\t        graph_loss += self.conf.training['smoothness_ratio'] * torch.trace(torch.mm(features.transpose(-1, -2), torch.mm(L, features))) / int(np.prod(out_adj.shape))\n\t        ones_vec = torch.ones(out_adj.size(-1)).to(self.device)\n\t        graph_loss += -self.conf.training['degree_ratio'] * torch.mm(ones_vec.unsqueeze(0), torch.log(torch.mm(out_adj, ones_vec.unsqueeze(-1)) + 1e-12)).squeeze() / out_adj.shape[-1]\n\t        graph_loss += self.conf.training['sparsity_ratio'] * torch.sum(torch.pow(out_adj, 2)) / int(np.prod(out_adj.shape))\n\t        return graph_loss\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of IDGL.\n\t        Parameters\n", "        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n", "        wait = 0\n\t        for epoch in range(self.conf.training['max_epochs']):\n\t            t = time.time()\n\t            improve = ''\n\t            # training phase\n\t            loss_train, acc_train, _ = self.run_epoch(mode='train', debug=debug)\n\t            # validation phase\n\t            with torch.no_grad():\n\t                loss_val, acc_val, adj = self.run_epoch(mode='valid', debug=debug)\n\t            if loss_val < self.best_val_loss:\n", "                wait = 0\n\t                self.total_time = time.time()-self.start_time\n\t                self.best_val_loss = loss_val\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                self.result['train'] = acc_train\n\t                self.result['valid'] = acc_val\n\t                improve = '*'\n\t                self.best_graph = deepcopy(adj.clone().detach())\n\t            else:\n\t                wait += 1\n", "                if wait == self.conf.training['patience']:\n\t                    print('Early stop!')\n\t                    break\n\t            # print\n\t            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                        epoch + 1, time.time() - t, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        # test\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n", "        self.model.load_state_dict(self.weights)\n\t        with torch.no_grad():\n\t            loss_test, acc_test, _ = self.run_epoch(mode='test', debug=debug)\n\t        self.result['test']=acc_test\n\t        print(acc_test)\n\t        return self.result, self.best_graph\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n", "        self.model = IDGL(self.conf, self.dim_feats, self.num_targets).to(self.device)\n\t        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'], weight_decay=self.conf.training['weight_decay'])\n\tclass PROGNNSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test ProGNN in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n", "        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n", "    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('prognn', 'cora')\n\t    >>>\n\t    >>> solver = PROGNNSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n", "        self.method_name = \"prognn\"\n\t        print(\"Solver Version : [{}]\".format(\"prognn\"))\n\t        self.adj = self.adj.to_dense()\n\t    def train_gcn(self, epoch, debug=False):\n\t        normalized_adj = self.estimator.normalize()\n\t        t = time.time()\n\t        improve = ''\n\t        self.model.train()\n\t        self.optimizer.zero_grad()\n\t        # forward and backward\n", "        output = self.model((self.feats, normalized_adj, False))[-1]\n\t        loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t        acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t        loss_train.backward()\n\t        self.optimizer.step()\n\t        # evaluate\n\t        loss_val, acc_val = self.evaluate(self.val_mask, normalized_adj)\n\t        flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\t        # save best model\n\t        if flag:\n", "            self.total_time = time.time()-self.start_time\n\t            self.improve = True\n\t            self.best_val_loss = loss_val\n\t            self.result['train'] = acc_train\n\t            self.result['valid'] = acc_val\n\t            improve = '*'\n\t            self.best_graph = self.estimator.estimated_adj.clone().detach()\n\t            self.weights = deepcopy(self.model.state_dict())\n\t        #print\n\t        if debug:\n", "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                epoch+1, time.time() -t, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t    def train_adj(self, epoch, debug=False):\n\t        estimator = self.estimator\n\t        t = time.time()\n\t        improve = ''\n\t        estimator.train()\n\t        self.optimizer_adj.zero_grad()\n\t        loss_l1 = torch.norm(estimator.estimated_adj, 1)\n\t        loss_fro = torch.norm(estimator.estimated_adj - self.adj, p='fro')\n", "        normalized_adj = estimator.normalize()\n\t        if self.conf.gsl['lambda_']:\n\t            loss_smooth_feat = feature_smoothing(estimator.estimated_adj, self.feats)\n\t        else:\n\t            loss_smooth_feat = 0 * loss_l1\n\t        output = self.model((self.feats, normalized_adj, False))[-1]\n\t        loss_gcn = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t        acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t        #loss_symmetric = torch.norm(estimator.estimated_adj - estimator.estimated_adj.t(), p=\"fro\")\n\t        #loss_differential =  loss_fro + self.conf.gamma * loss_gcn + self.conf.lambda_ * loss_smooth_feat + args.phi * loss_symmetric\n", "        loss_differential = loss_fro + self.conf.gsl['gamma'] * loss_gcn + self.conf.gsl['lambda_'] * loss_smooth_feat\n\t        loss_differential.backward()\n\t        self.optimizer_adj.step()\n\t        # we finish the optimization of the differential part above, next we need to do the optimization of loss_l1 and loss_nuclear\n\t        loss_nuclear =  0 * loss_fro\n\t        if self.conf.gsl['beta'] != 0:\n\t            self.optimizer_nuclear.zero_grad()\n\t            self.optimizer_nuclear.step()\n\t            loss_nuclear = prox_operators.nuclear_norm\n\t        self.optimizer_l1.zero_grad()\n", "        self.optimizer_l1.step()\n\t        total_loss = loss_fro \\\n\t                     + self.conf.gsl['gamma'] * loss_gcn \\\n\t                     + self.conf.gsl['alpha'] * loss_l1 \\\n\t                     + self.conf.gsl['beta'] * loss_nuclear\n\t                     #+ self.conf.phi * loss_symmetric\n\t        estimator.estimated_adj.data.copy_(torch.clamp(estimator.estimated_adj.data, min=0, max=1))\n\t        # evaluate\n\t        self.model.eval()\n\t        normalized_adj = estimator.normalize()\n", "        loss_val, acc_val = self.evaluate(self.val_mask, normalized_adj)\n\t        flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\t        # save the best model\n\t        if flag:\n\t            self.total_time = time.time()-self.start_time\n\t            self.improve = True\n\t            self.best_val_loss = loss_val\n\t            self.result['train'] = acc_train\n\t            self.result['valid'] = acc_val\n\t            improve = '*'\n", "            self.best_graph = estimator.estimated_adj.clone().detach()\n\t            self.weights = deepcopy(self.model.state_dict())\n\t        #print\n\t        if debug:\n\t            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(adj) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                epoch+1, time.time() - t, total_loss.item(), loss_val, acc_val, improve))\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of PROGNN.\n\t        Parameters\n", "        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n", "        for epoch in range(self.conf.training['n_epochs']):\n\t            for i in range(int(self.conf.training['outer_steps'])):\n\t                self.train_adj(epoch, debug=debug)\n\t            for i in range(int(self.conf.training['inner_steps'])):\n\t                self.train_gcn(epoch, debug=debug)\n\t            # we use earlystopping here as prognn is very slow\n\t            if self.improve:\n\t                self.wait = 0\n\t                self.improve = False\n\t            else:\n", "                self.wait += 1\n\t                if self.wait == self.conf.training['patience_iter']:\n\t                    print('Early stop!')\n\t                    break\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, self.best_graph\n", "    def evaluate(self, test_mask, normalized_adj):\n\t        '''\n\t        Evaluation procedure of PROGNN.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        Returns\n", "        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        '''\n\t        self.model.eval()\n\t        self.estimator.eval()\n\t        with torch.no_grad():\n\t            logits = self.model((self.feats, normalized_adj, False))[-1]\n", "        logits = logits[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss=self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\t    def test(self):\n\t        '''\n\t        Test procedure of PROGNN.\n\t        Returns\n\t        -------\n\t        loss : float\n", "            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        '''\n\t        self.model.load_state_dict(self.weights)\n\t        self.estimator.estimated_adj.data.copy_(self.best_graph)\n\t        normalized_adj = self.estimator.normalize()\n\t        return self.evaluate(self.test_mask, normalized_adj)\n\t    def set_method(self):\n\t        '''\n", "        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        if self.conf.model['type'] == 'gcn':\n\t            self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n\t                             self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n\t                             self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n\t                             self.conf.model['input_layer'], self.conf.model['output_layer'],\n\t                             weight_initializer='uniform').to(self.device)\n\t        else:\n\t            self.model = APPNP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets,\n", "                               dropout=self.conf.model['dropout'], K=self.conf.model['K'],\n\t                               alpha=self.conf.model['alpha']).to(self.device)\n\t        self.estimator = EstimateAdj(self.adj, symmetric=self.conf.gsl['symmetric'], device=self.device).to(self.device)\n\t        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                          weight_decay=self.conf.training['weight_decay'])\n\t        self.optimizer_adj = torch.optim.SGD(self.estimator.parameters(), momentum=0.9, lr=self.conf.training['lr_adj'])\n\t        self.optimizer_l1 = PGD(self.estimator.parameters(), proxs=[prox_operators.prox_l1],\n\t                                lr=self.conf.training['lr_adj'], alphas=[self.conf.gsl['alpha']])\n\t        self.optimizer_nuclear = PGD(self.estimator.parameters(), proxs=[prox_operators.prox_nuclear_cuda],\n\t                                     lr=self.conf.training['lr_adj'], alphas=[self.conf.gsl['beta']])\n", "        self.wait = 0\n\tclass GTSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test GT in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n", "    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n", "    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('gt', 'cora')\n\t    >>>\n\t    >>> solver = GTSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gt\"\n", "        print(\"Solver Version : [{}]\".format(\"gt\"))\n\t        # prepare dgl graph\n\t        edges = self.adj.coalesce().indices().cpu()\n\t        self.graph = dgl.graph((edges[0], edges[1]), num_nodes=self.n_nodes, idtype=torch.int)\n\t        self.graph = dgl.add_self_loop(self.graph).to(self.device)\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of GRCN.\n\t        Parameters\n\t        ----------\n", "        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        0 : constant\n\t        '''\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            improve = ''\n", "            t0 = time.time()\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n\t            x, output, _ = self.model(self.feats, self.graph, self.labels.cpu())\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n", "            loss_val, acc_val, _ = self.evaluate(self.val_mask)\n\t            # save\n\t            if acc_val > self.result['valid']:\n\t                improve = '*'\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                self.total_time = time.time() - self.start_time\n\t                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t            # print\n", "            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test, homo_heads = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, 0\n\t    def evaluate(self, test_mask, graph_analysis=False):\n", "        '''\n\t        Evaluation procedure of GT.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        graph_analysis : bool\n\t        Returns\n\t        -------\n\t        loss : float\n", "            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        homo_heads\n\t        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            x, output, homo_heads = self.model(self.feats, self.graph, self.labels.cpu(), graph_analysis)\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n", "        loss = self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), homo_heads\n\t    def test(self):\n\t        '''\n\t        Test procedure of GT.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n", "            Evaluation metric.\n\t        homo_heads\n\t        '''\n\t        self.model.load_state_dict(self.weights)\n\t        return self.evaluate(self.test_mask, graph_analysis=self.conf.analysis['graph_analysis'])\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = GT(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n", "                   self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm_type'],\n\t                   self.conf.model['n_heads'], self.conf.model['act'], input_layer=self.conf.model['input_layer'],\n\t                        ff=self.conf.model['ff'], output_layer=self.conf.model['output_layer'],\n\t                        use_norm=self.conf.model['use_norm'], use_redisual=self.conf.model['use_residual'],\n\t                        hidden_dim_multiplier=self.conf.model['hidden_dim_multiplier']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                 weight_decay=self.conf.training['weight_decay'])\n\tclass SLAPSSolver(Solver):\n\t    '''\n\t        A solver to train, evaluate, test SLAPS in a run.\n", "        Parameters\n\t        ----------\n\t        conf : argparse.Namespace\n\t            Config file.\n\t        dataset : opengsl.data.Dataset\n\t            The dataset.\n\t        Attributes\n\t        ----------\n\t        method_name : str\n\t            The name of the method.\n", "        Examples\n\t        --------\n\t        >>> # load dataset\n\t        >>> import opengsl.dataset\n\t        >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t        >>> # load config file\n\t        >>> import opengsl.config.load_conf\n\t        >>> conf = opengsl.config.load_conf('slaps', 'cora')\n\t        >>>\n\t        >>> solver = SLAPSSolver(conf, dataset)\n", "        >>> # Conduct a experiment run.\n\t        >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t        '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"slaps\"\n\t        print(\"Solver Version : [{}]\".format(\"slaps\"))\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of SLAPS.\n", "        Parameters\n\t        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n", "        '''\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            improve = ''\n\t            t0 = time.time()\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n\t            output, loss_dae, adj = self.model(self.feats)\n\t            if epoch < self.conf.training['n_epochs'] // self.conf.training['epoch_d']:\n\t                self.model.gcn_c.eval()\n", "                loss_train = self.conf.training['lamda'] * loss_dae\n\t            else:\n\t                loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask]) + self.conf.training['lamda'] * loss_dae \n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val = self.evaluate(self.val_mask)\n\t            # save\n\t            if acc_val > self.result['valid']:\n", "                self.total_time = time.time() - self.start_time\n\t                improve = '*'\n\t                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                self.best_graph = adj.clone()\n\t            #print\n\t            if debug:\n\t                print(\n", "                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, self.best_graph\n\t    def evaluate(self, test_mask):\n\t        '''\n", "        Evaluation procedure of SLAPS.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n", "            Evaluation metric.\n\t        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            output, _, _ = self.model(self.feats)\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss = self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\t    def set_method(self):\n", "        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = SLAPS(self.n_nodes, self.dim_feats, self.num_targets, self.feats, self.device, self.conf).to(self.device)\n\t        self.optim = torch.optim.Adam([\n\t            {'params': self.model.gcn_c.parameters(), 'lr': self.conf.training['lr'], 'weight_decay': self.conf.training['weight_decay']},\n\t            {'params': self.model.gcn_dae.parameters(), 'lr': self.conf.training['lr_dae'], 'weight_decay': self.conf.training['weight_decay_dae']}\n\t        ])\n\tclass NODEFORMERSolver(Solver):\n\t    '''\n", "    A solver to train, evaluate, test Nodeformer in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n", "        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('nodeoformer', 'cora')\n\t    >>>\n", "    >>> solver = NODEFORMERSolverSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"nodeformer\"\n\t        print(\"Solver Version : [{}]\".format(\"nodeformer\"))\n\t        edge_index = self.adj.coalesce().indices().cpu()\n\t        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n", "        adj = torch.cat([edge_index, loop_edge_index], dim=1).to(self.device)\n\t        self.adjs = []\n\t        self.adjs.append(adj)\n\t        for i in range(conf.model['rb_order'] - 1):  # edge_index of high order adjacency\n\t            adj = adj_mul(adj, adj, self.n_nodes)\n\t            self.adjs.append(adj)\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n", "        self.model = NodeFormer(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, num_layers=self.conf.model['n_layers'], dropout=self.conf.model['dropout'],\n\t                           num_heads=self.conf.model['n_heads'], use_bn=self.conf.model['use_bn'], nb_random_features=self.conf.model['M'],\n\t                           use_gumbel=self.conf.model['use_gumbel'], use_residual=self.conf.model['use_residual'], use_act=self.conf.model['use_act'],\n\t                           use_jk=self.conf.model['use_jk'],\n\t                           nb_gumbel_sample=self.conf.model['K'], rb_order=self.conf.model['rb_order'], rb_trans=self.conf.model['rb_trans']).to(self.device)\n\t        self.model.reset_parameters()\n\t        self.optim = torch.optim.Adam(self.model.parameters(), weight_decay=self.conf.training['weight_decay'], lr=self.conf.training['lr'])\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of Nodeformer.\n", "        Parameters\n\t        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        0 : constant\n\t        '''\n", "        for epoch in range(self.conf.training['n_epochs']):\n\t            improve = ''\n\t            t0 = time.time()\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n\t            output, link_loss = self.model(self.feats, self.adjs, self.conf.model['tau'])\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(),\n\t                                    output[self.train_mask].detach().cpu().numpy())\n", "            loss_train -= self.conf.training['lambda'] * sum(link_loss) / len(link_loss)\n\t            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val, = self.evaluate(self.val_mask)\n\t            # save\n\t            if acc_val > self.result['valid']:\n\t                improve = '*'\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                self.total_time = time.time() - self.start_time\n", "                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t            # print\n\t            if debug:\n\t                print(\n\t                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n", "        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, 0\n\t    def evaluate(self, test_mask):\n\t        '''\n\t        Evaluation procedure of NODEFORMER.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n", "            A boolean tensor indicating whether the node is in the data set.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n", "            output, _ = self.model(self.feats, self.adjs, self.conf.model['tau'])\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss=self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\tclass SEGSLSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test SEGSL in a run.\n\t    Parameters\n\t    ----------\n", "    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n", "    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('segsl', 'cora')\n\t    >>>\n\t    >>> solver = SEGSLSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n", "    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"segsl\"\n\t        print(\"Solver Version : [{}]\".format(\"segsl\"))\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of SEGSL.\n\t        Parameters\n\t        ----------\n", "        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n\t        adj = self.adj.to_dense()\n", "        adj.fill_diagonal_(1)\n\t        adj = adj.to_sparse()\n\t        for iter in range(self.conf.training['n_iters']):\n\t            logits = self.train_gcn(iter, adj, debug)\n\t            adj = self.structure_learning(logits, adj)\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test, _, _ = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n", "        return self.result, self.best_graph\n\t    def train_gcn(self, iter, adj, debug=False):\n\t        self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n\t                         self.conf.model['dropout'], self.conf.model['input_dropout']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(),\n\t                                      lr=self.conf.training['lr'],\n\t                                      weight_decay=self.conf.training['weight_decay'])\n\t        if debug:\n\t            print('==== Iteration {:04d} ===='.format(iter+1))\n\t        t = time.time()\n", "        improve_1 = ''\n\t        best_loss_val = 10\n\t        best_acc_val = 0\n\t        normalized_adj = normalize(adj, add_loop=False, sparse=True)\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            improve_2 = ''\n\t            t0 = time.time()\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n", "            hidden_output, output = self.model((self.feats, normalized_adj, False))\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val, hidden_output, output = self.evaluate(self.val_mask, normalized_adj)\n\t            # save\n\t            if acc_val > best_acc_val:\n\t                best_acc_val = acc_val\n", "                best_loss_val = loss_val\n\t                improve_2 = '*'\n\t                if acc_val > self.result['valid']:\n\t                    self.total_time = time.time()-self.start_time\n\t                    improve_1 = '*'\n\t                    self.best_val_loss = loss_val\n\t                    self.result['valid'] = acc_val\n\t                    self.result['train'] = acc_train\n\t                    self.best_iter = iter+1\n\t                    self.weights = deepcopy(self.model.state_dict())\n", "                    self.best_graph = deepcopy(adj)\n\t            # print\n\t            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve_2))\n\t        print('Iteration {:04d} | Time(s) {:.4f} | Loss(val):{:.4f} | Acc(val):{:.4f} | {}'.format(iter+1,time.time()-t, best_loss_val, best_acc_val, improve_1))\n\t        return output\n\t    def structure_learning(self, logits, adj):\n\t        edge_index = adj.coalesce().indices().t()\n\t        k = knn_maxE1(edge_index, logits)  # edge indexå¯¹ç§°æèªç¯\n", "        edge_index_2 = add_knn(k, logits, edge_index)\n\t        weight = get_weight(logits, edge_index_2)\n\t        adj_matrix = get_adj_matrix(self.n_nodes, edge_index_2, weight)\n\t        code_tree = PartitionTree(adj_matrix=np.array(adj_matrix))\n\t        code_tree.build_coding_tree(self.conf.gsl['se'])\n\t        community, isleaf = get_community(code_tree)\n\t        new_edge_index = reshape(community, code_tree, isleaf,\n\t                                 self.conf.gsl['k'])\n\t        new_edge_index_2 = reshape(community, code_tree, isleaf,\n\t                                   self.conf.gsl['k'])\n", "        new_edge_index = torch.cat(\n\t            (new_edge_index.t(), new_edge_index_2.t()), dim=0)\n\t        new_edge_index, unique_idx = torch.unique(\n\t            new_edge_index, return_counts=True, dim=0)\n\t        new_edge_index = new_edge_index[unique_idx != 1].t()\n\t        add_num = int(new_edge_index.shape[1])\n\t        new_edge_index = torch.cat(\n\t            (new_edge_index.t(), edge_index.cpu()), dim=0)\n\t        new_edge_index = torch.unique(new_edge_index, dim=0)\n\t        new_edge_index = new_edge_index.t()\n", "        new_weight = get_weight(logits, new_edge_index.t())\n\t        _, delete_idx = torch.topk(new_weight,\n\t                                   k=add_num,\n\t                                   largest=False)\n\t        delete_mask = torch.ones(\n\t            new_edge_index.t().shape[0]).bool()\n\t        delete_mask[delete_idx] = False\n\t        new_edge_index = new_edge_index.t()[delete_mask].t()  # å¾å°æ°çedge_indexäº\n\t        graph = dgl.graph((new_edge_index[0], new_edge_index[1]),\n\t                          num_nodes=self.n_nodes).to(self.device)\n", "        graph = dgl.remove_self_loop(graph)\n\t        graph = dgl.add_self_loop(graph)\n\t        adj = graph.adj().to(self.device)\n\t        return adj\n\t    def evaluate(self, test_mask, normalized_adj):\n\t        '''\n\t        Evaluation procedure of SEGSL.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n", "            A boolean tensor indicating whether the node is in the data set.\n\t        normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        hidden_output : torch.tensor        \n", "            Hidden output of the model.\n\t        output : torch.tensor        \n\t           Output of the model.\n\t        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            hidden_output, output = self.model((self.feats, normalized_adj, False))\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss=self.loss_fn(logits, labels)\n", "        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), hidden_output, output\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.best_iter = 0\n\t    def test(self):\n\t        '''\n\t        Test procedure of SEGSL.\n\t        Returns\n", "        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        hidden_output : torch.tensor        \n\t            Hidden output of the model.\n\t        output : torch.tensor        \n\t           Output of the model.\n\t        '''\n", "        self.model.load_state_dict(self.weights)\n\t        normalized_adj = normalize(self.best_graph, add_loop=False, sparse=True)\n\t        return self.evaluate(self.test_mask, normalized_adj)\n\tclass SUBLIMESolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test SUBLIME in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n", "    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n", "    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('sublime', 'cora')\n\t    >>>\n\t    >>> solver = SUBLIMESolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n", "        super().__init__(conf, dataset)\n\t        self.method_name = \"sublime\"\n\t        print(\"Solver Version : [{}]\".format(\"sublime\"))\n\t    def loss_gcl(self, model, graph_learner, features, anchor_adj):\n\t        # view 1: anchor graph\n\t        if self.conf.maskfeat_rate_anchor:\n\t            mask_v1, _ = get_feat_mask(features, self.conf.maskfeat_rate_anchor)\n\t            features_v1 = features * (1 - mask_v1)\n\t        else:\n\t            features_v1 = copy.deepcopy(features)\n", "        z1, _ = model(features_v1, anchor_adj, 'anchor')\n\t        # view 2: learned graph\n\t        if self.conf.maskfeat_rate_learner:\n\t            mask, _ = get_feat_mask(features, self.conf.maskfeat_rate_learner)\n\t            features_v2 = features * (1 - mask)\n\t        else:\n\t            features_v2 = copy.deepcopy(features)\n\t        learned_adj = graph_learner(features)   # è¿ä¸ªlearned adjæ¯æèªç¯ç\n\t        if not self.conf.sparse:\n\t            learned_adj = (learned_adj + learned_adj.T) / 2\n", "            learned_adj = normalize(learned_adj, add_loop=False, sparse=False)\n\t        z2, _ = model(features_v2, learned_adj, 'learner')\n\t        # compute loss\n\t        if self.conf.contrast_batch_size:\n\t            node_idxs = list(range(features.shape[0]))\n\t            batches = split_batch(node_idxs, self.conf.contrast_batch_size)\n\t            loss = 0\n\t            for batch in batches:\n\t                weight = len(batch) / features.shape[0]\n\t                loss += model.calc_loss(z1[batch], z2[batch]) * weight\n", "        else:\n\t            loss = model.calc_loss(z1, z2)\n\t        return loss, learned_adj\n\t    def train_gcn(self, adj, debug=False):\n\t        model = GCN_SUB(nfeat=self.dim_feats, nhid=self.conf.hidden_dim_cls, nclass=self.num_targets,\n\t                        n_layers=self.conf.n_layers_cls, dropout=self.conf.dropout_cls,\n\t                        dropout_adj=self.conf.dropedge_cls, sparse=self.conf.sparse).to(self.device)\n\t        optim = torch.optim.Adam(model.parameters(), lr=self.conf.lr_cls, weight_decay=self.conf.w_decay_cls)\n\t        t = time.time()\n\t        improve_1 = ''\n", "        best_loss_val = 10\n\t        best_acc_val = 0\n\t        for epoch in range(self.conf.epochs_cls):\n\t            improve_2 = ''\n\t            t0 = time.time()\n\t            model.train()\n\t            optim.zero_grad()\n\t            # forward and backward\n\t            output = model(self.feats, adj)\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n", "            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val = self.evaluate(model, self.val_mask, adj)\n\t            # save\n\t            if acc_val > best_acc_val:\n\t                best_acc_val = acc_val\n\t                best_loss_val = loss_val\n\t                improve_2 = '*'\n", "                if acc_val > self.result['valid']:\n\t                    self.total_time = time.time()-self.start_time\n\t                    improve_1 = '*'\n\t                    self.best_val_loss = loss_val\n\t                    self.result['valid'] = acc_val\n\t                    self.result['train'] = acc_train\n\t                    self.weights = deepcopy(model.state_dict())\n\t                    current_adj = dgl_graph_to_torch_sparse(adj).to_dense() if self.conf.sparse else adj\n\t                    self.best_graph = deepcopy(current_adj)\n\t                    self.best_graph_test = deepcopy(adj)\n", "            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve_2))\n\t        print('Time(s) {:.4f} | Loss(val):{:.4f} | Acc(val):{:.4f} | {}'.format(time.time()-t, best_loss_val, best_acc_val, improve_1))\n\t    def evaluate(self, model, test_mask, adj):\n\t        '''\n\t        Evaluation procedure of GRCN.\n\t        Parameters\n\t        ----------\n\t        model : torch.nn.Module\n", "            model.\n\t        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        adj : torch.tensor\n\t            Adjacency matrix.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n", "            Evaluation metric.\n\t        '''\n\t        model.eval()\n\t        with torch.no_grad():\n\t            output = model(self.feats, adj)\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss=self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\t    def test(self):\n", "        '''\n\t        Test procedure of SUBLIME.\n\t        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        metric : float\n\t            Evaluation metric.\n\t        '''\n\t        model = GCN_SUB(nfeat=self.dim_feats, nhid=self.conf.hidden_dim_cls, nclass=self.num_targets,\n", "                        n_layers=self.conf.n_layers_cls, dropout=self.conf.dropout_cls,\n\t                        dropout_adj=self.conf.dropedge_cls, sparse=self.conf.sparse).to(self.device)\n\t        model.load_state_dict(self.weights)\n\t        adj = self.best_graph_test\n\t        return self.evaluate(model, self.test_mask, adj)\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of SUBLIME.\n\t        Parameters\n\t        ----------\n", "        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n\t        anchor_adj = normalize(self.anchor_adj_raw, add_loop=False, sparse=self.conf.sparse)\n", "        if self.conf.sparse:\n\t            anchor_adj_torch_sparse = copy.deepcopy(anchor_adj)\n\t            anchor_adj = torch_sparse_to_dgl_graph(anchor_adj)\n\t        for epoch in range(1, self.conf.epochs + 1):\n\t            # Contrastive Learning\n\t            self.model.train()\n\t            self.graph_learner.train()\n\t            loss, Adj = self.loss_gcl(self.model, self.graph_learner, self.feats, anchor_adj)   # Adjæ¯æèªç¯ä¸normalized\n\t            self.optimizer_cl.zero_grad()\n\t            self.optimizer_learner.zero_grad()\n", "            loss.backward()\n\t            self.optimizer_cl.step()\n\t            self.optimizer_learner.step()\n\t            # Structure Bootstrapping\n\t            if (1 - self.conf.tau) and (self.conf.c == 0 or epoch % self.conf.c == 0):\n\t                if self.conf.sparse:\n\t                    learned_adj_torch_sparse = dgl_graph_to_torch_sparse(Adj).to(self.device)\n\t                    anchor_adj_torch_sparse = anchor_adj_torch_sparse * self.conf.tau \\\n\t                                              + learned_adj_torch_sparse * (1 - self.conf.tau)\n\t                    anchor_adj = torch_sparse_to_dgl_graph(anchor_adj_torch_sparse)\n", "                else:\n\t                    anchor_adj = anchor_adj * self.conf.tau + Adj.detach() * (1 - self.conf.tau)\n\t            if debug:\n\t                print(\"Epoch {:05d} | CL Loss {:.4f}\".format(epoch, loss.item()))\n\t            # Evaluate via Node Classification\n\t            if epoch % self.conf.eval_freq == 0:\n\t                self.model.eval()\n\t                self.graph_learner.eval()\n\t                f_adj = Adj\n\t                if self.conf.sparse:\n", "                    f_adj.edata['w'] = f_adj.edata['w'].detach()\n\t                else:\n\t                    f_adj = f_adj.detach()\n\t                self.train_gcn(f_adj, debug)\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, self.best_graph\n", "    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        if self.conf.sparse:\n\t            self.anchor_adj_raw = self.adj\n\t        else:\n\t            self.anchor_adj_raw = self.adj.to_dense()\n\t        anchor_adj = normalize(self.anchor_adj_raw, add_loop=False, sparse=self.conf.sparse)\n\t        if self.conf.type_learner == 'fgp':\n", "            self.graph_learner = FGP_learner(self.feats.cpu(), self.conf.k, self.conf.sim_function, 6, self.conf.sparse)\n\t        elif self.conf.type_learner == 'mlp':\n\t            self.graph_learner = MLP_learner(2, self.feats.shape[1], self.conf.k, self.conf.sim_function, 6, self.conf.sparse,\n\t                                 self.conf.activation_learner)\n\t        elif self.conf.type_learner == 'att':\n\t            self.graph_learner = ATT_learner(2, self.feats.shape[1], self.conf.k, self.conf.sim_function, 6, self.conf.sparse,\n\t                                      self.conf.activation_learner)\n\t        elif self.conf.type_learner == 'gnn':\n\t            self.graph_learner = GNN_learner(2, self.feats.shape[1], self.conf.k, self.conf.sim_function, 6, self.conf.sparse,\n\t                                 self.conf.activation_learner, anchor_adj)\n", "        self.graph_learner = self.graph_learner.to(self.device)\n\t        self.model = GCL(nlayers=self.conf.n_layers, in_dim=self.dim_feats, hidden_dim=self.conf.n_hidden,\n\t                    emb_dim=self.conf.n_embed, proj_dim=self.conf.n_proj,\n\t                    dropout=self.conf.dropout, dropout_adj=self.conf.dropedge_rate, sparse=self.conf.sparse,\n\t                         conf=self.conf).to(self.device)\n\t        self.optimizer_cl = torch.optim.Adam(self.model.parameters(), lr=self.conf.lr, weight_decay=self.conf.wd)\n\t        self.optimizer_learner = torch.optim.Adam(self.graph_learner.parameters(), lr=self.conf.lr,\n\t                                             weight_decay=self.conf.wd)\n\tclass STABLESolver(Solver):\n\t    '''\n", "    A solver to train, evaluate, test Stable in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n", "        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('stable', 'cora')\n\t    >>>\n", "    >>> solver = STABLESolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"stable\"\n\t        print(\"Solver Version : [{}]\".format(\"stable\"))\n\t        self.adj = sparse_tensor_to_scipy_sparse(self.adj)\n\t        self.processed_adj = preprocess_adj(self.feats.cpu().numpy(), self.adj, threshold=self.conf.jt)\n", "    def pretrain(self, debug=False):\n\t        # generate 2 augment views\n\t        adj_delete = self.adj - self.processed_adj\n\t        aug_adj1 = aug_random_edge(self.processed_adj, adj_delete=adj_delete, recover_percent=self.conf.recover_percent)  # random drop edges\n\t        aug_adj2 = aug_random_edge(self.processed_adj, adj_delete=adj_delete, recover_percent=self.conf.recover_percent)  # random drop edges\n\t        sp_adj = normalize_sp_matrix(self.processed_adj+(sp.eye(self.n_nodes) * self.conf.beta),\n\t                                  add_loop=False)\n\t        sp_aug_adj1 = normalize_sp_matrix(aug_adj1 + (sp.eye(self.n_nodes) * self.conf.beta),\n\t                                  add_loop=False)\n\t        sp_aug_adj2 = normalize_sp_matrix(aug_adj2 + (sp.eye(self.n_nodes) * self.conf.beta),\n", "                                  add_loop=False)\n\t        sp_adj = scipy_sparse_to_sparse_tensor(sp_adj).to(self.device)\n\t        sp_aug_adj1 = scipy_sparse_to_sparse_tensor(sp_aug_adj1).to(self.device)\n\t        sp_aug_adj2 = scipy_sparse_to_sparse_tensor(sp_aug_adj2).to(self.device)\n\t        # contrastive learning\n\t        weights = None\n\t        wait = 0\n\t        best = 1e9\n\t        best_t = 0\n\t        b_xent = torch.nn.BCEWithLogitsLoss()\n", "        for epoch in range(self.conf.pretrain['n_epochs']):\n\t            self.model.train()\n\t            self.optim.zero_grad()\n\t            idx = np.random.permutation(self.n_nodes)\n\t            shuf_fts = self.feats.unsqueeze(0)[:, idx, :]\n\t            lbl_1 = torch.ones(1, self.n_nodes)\n\t            lbl_2 = torch.zeros(1, self.n_nodes)\n\t            lbl = torch.cat((lbl_1, lbl_2), 1).to(self.device)\n\t            logits = self.model(self.feats.unsqueeze(0), shuf_fts, sp_adj, sp_aug_adj1, sp_aug_adj2)\n\t            loss = b_xent(logits, lbl)\n", "            if debug:\n\t                print(loss)\n\t            if loss < best:\n\t                best = loss\n\t                best_t = epoch\n\t                wait = 0\n\t                weights = copy.deepcopy(self.model.state_dict())\n\t            else:\n\t                wait+=1\n\t            if wait == self.conf.pretrain['patience']:\n", "                print('Early stopping!')\n\t                break\n\t            loss.backward()\n\t            self.optim.step()\n\t        print('Loading {}th epoch'.format(best_t))\n\t        self.model.load_state_dict(weights)\n\t        return self.model.embed(self.feats.unsqueeze(0), sp_adj)\n\t    def train_gcn(self, feats, adj, debug=False):\n\t        def evaluate(model, test_mask):\n\t            model.eval()\n", "            with torch.no_grad():\n\t                output = model((feats, adj, True))\n\t            logits = output[test_mask]\n\t            labels = self.labels[test_mask]\n\t            loss = self.loss_fn(logits, labels)\n\t            return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\t        def test(model):\n\t            return evaluate(model, self.test_mask)\n\t        model = GCN(self.conf.n_embed, self.conf.n_hidden, self.num_targets, self.conf.n_layers, self.conf.dropout).to(self.device)\n\t        optim = torch.optim.Adam(model.parameters(), lr=self.conf.lr, weight_decay=self.conf.weight_decay)\n", "        best_loss_val = 10\n\t        for epoch in range(self.conf.n_epochs):\n\t            improve = ''\n\t            t0 = time.time()\n\t            model.train()\n\t            optim.zero_grad()\n\t            # forward and backward\n\t            output = model((feats, adj, True))\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n", "            loss_train.backward()\n\t            optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val = evaluate(model, self.val_mask)\n\t            # save\n\t            if acc_val > self.result['valid']:\n\t                improve = '*'\n\t                self.total_time = time.time() - self.start_time\n\t                best_loss_val = loss_val\n\t                self.result['valid'] = acc_val\n", "                self.result['train'] = acc_train\n\t                weights = deepcopy(model.state_dict())\n\t            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n\t                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        model.load_state_dict(weights)\n\t        loss_test, acc_test = test(model)\n\t        self.result['test'] = acc_test\n", "        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of STABLE.\n\t        Parameters\n\t        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n", "        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n\t        embeds = self.pretrain(debug)\n\t        embeds = embeds.squeeze(dim=0)\n\t        # prunue the graph\n\t        adj_clean = preprocess_adj(embeds.cpu().numpy(), self.adj, jaccard=False, threshold=self.conf.cos)\n", "        adj_clean = scipy_sparse_to_sparse_tensor(adj_clean).to(self.device).to_dense()\n\t        # add k neighbors\n\t        get_reliable_neighbors(adj_clean, embeds, k=self.conf.k, degree_threshold=self.conf.threshold)\n\t        # å¾å°çæ¯0-1 æ èªç¯çå¾\n\t        normalized_adj_clean = normalize(adj_clean, sparse=False)   # æªä½¿ç¨è®ºæä¸­å¯¹å½ä¸åçæ¹è¿\n\t        result = self.train_gcn(embeds, normalized_adj_clean, debug)\n\t        return result, adj_clean\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n", "        '''\n\t        self.model = DGI(self.dim_feats, self.conf.n_embed, 'prelu').to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.pretrain['lr'], weight_decay=self.conf.pretrain['weight_decay'])\n\tclass COGSLSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test CoGSL in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n", "    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n", "    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('cogsl', 'cora')\n\t    >>>\n\t    >>> solver = COGSLSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n", "        super().__init__(conf, dataset)\n\t        self.method_name = \"cogsl\"\n\t        print(\"Solver Version : [{}]\".format(\"CoGSL\"))\n\t        edge_index = self.adj.coalesce().indices().cpu()\n\t        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n\t        edges = torch.cat([edge_index, loop_edge_index], dim=1)\n\t        self.adj = torch.sparse.FloatTensor(edges, torch.ones(edges.shape[1]), [self.n_nodes, self.n_nodes]).to(\n\t            self.device).coalesce()\n\t        if self.conf.dataset['init'] :\n\t            _view1 = eval(\"self.\"+self.conf.dataset[\"name_view1\"]+\"()\")\n", "            self.view1_indices = self.get_indices(self.conf.dataset[\"view1_indices\"], _view1, self.conf.dataset[\"view1_k\"])\n\t            _view2 = eval(\"self.\"+self.conf.dataset[\"name_view2\"]+\"()\")\n\t            self.view2_indices = self.get_indices(self.conf.dataset[\"view2_indices\"], _view2, self.conf.dataset[\"view2_k\"])\n\t        else:\n\t            _view1 = sp.load_npz(self.conf.dataset['view1_path'])\n\t            _view2 = sp.load_npz(self.conf.dataset['view2_path'])\n\t            self.view1_indices = torch.load(self.conf.dataset['view1_indices_path'])\n\t            self.view2_indices = torch.load(self.conf.dataset['view2_indices_path'])\n\t        self.view1 = scipy_sparse_to_sparse_tensor(normalize_sp_matrix(_view1, False))\n\t        self.view2 = scipy_sparse_to_sparse_tensor(normalize_sp_matrix(_view2, False))\n", "        self.loss_fn = F.binary_cross_entropy if self.num_targets == 1 else F.nll_loss\n\t        #self.train_mask = np.load('/root/dataset/citeseer/train.npy')\n\t        #self.valid_mask = np.load('/root/dataset/citeseer/val.npy')\n\t        #self.test_mask = np.load('/root/dataset/citeseer/test.npy')\n\t        #print(self.view1_indices.shape)\n\t        #print(self.view1.shape)\n\t        #print(self.view2_indices.shape)\n\t        #print(self.view2.shape)\n\t    def view_knn(self):\n\t        adj = np.zeros((self.n_nodes, self.n_nodes), dtype=np.int64)\n", "        dist = cos(self.feats.cpu())\n\t        col = np.argpartition(dist, -(self.conf.dataset['knn_k'] + 1), axis=1)[:, -(self.conf.dataset['knn_k'] + 1):].flatten()\n\t        adj[np.arange(self.n_nodes).repeat(self.conf.dataset['knn_k'] + 1), col] = 1\n\t        return sp.coo_matrix(adj)\n\t    def view_adj(self):\n\t        return sparse_tensor_to_scipy_sparse(self.adj)\n\t    def view_diff(self):\n\t        adj = sparse_tensor_to_scipy_sparse(self.adj)\n\t        at = normalize_sp_matrix(adj,False)\n\t        result = self.conf.dataset['diff_alpha'] * sp.linalg.inv(sp.eye(adj.shape[0]) - (1 - self.conf.dataset['diff_alpha']) * at)\n", "        return result\n\t    def view_sub(self):\n\t        adj = sparse_tensor_to_scipy_sparse(self.adj)\n\t        adj_ = sp.triu(sp.coo_matrix(adj), 1)\n\t        adj_cand = np.array(adj_.nonzero())\n\t        dele_num = int(self.conf.dataset['sub_rate'] * adj_cand.shape[1])\n\t        adj_sele = np.random.choice(np.arange(adj_cand.shape[1]), dele_num, replace=False)\n\t        adj_sele = adj_cand[:, adj_sele]\n\t        adj_new = sp.coo_matrix((np.ones(adj_sele.shape[1]), (adj_sele[0, :], adj_sele[1, :])), shape=adj_.shape)\n\t        adj_new = adj_new + adj_new.T + sp.eye(adj_new.shape[0])\n", "        return adj_new\n\t    def get_khop_indices(self, k, view):\n\t        view = (view.A > 0).astype(\"int32\")\n\t        view_ = view\n\t        for i in range(1, k):\n\t            view_ = (np.matmul(view_, view.T)>0).astype(\"int32\")\n\t        view_ = torch.tensor(view_).to_sparse()\n\t        #print(view_)\n\t        return view_.indices()\n\t    def topk(self, k, _adj):\n", "        adj = _adj.todense()\n\t        pos = np.zeros(adj.shape)\n\t        for i in range(len(adj)):\n\t            one = adj[i].nonzero()[1]\n\t            if len(one)>k:\n\t                oo = np.argsort(-adj[i, one])\n\t                sele = one[oo[0,:k]]\n\t                pos[i, sele] = adj[i, sele]\n\t            else:\n\t                pos[i, one] = adj[i, one]\n", "        return pos\n\t    def get_indices(self, val, adj, k):\n\t        if (k == 0):\n\t            return self.get_khop_indices(val, sp.coo_matrix((adj)))\n\t        else:\n\t            kn = self.topk(k, adj)\n\t            return self.get_khop_indices(val, sp.coo_matrix((kn)))\n\t    def train_mi(self, x, views):\n\t        vv1, vv2, v1v2 = self.model.get_mi_loss(x, views)\n\t        return self.conf.model['mi_coe'] * v1v2 + (vv1 + vv2) * (1 - self.conf.model['mi_coe']) / 2\n", "    def loss_acc(self, output, y):\n\t        loss = self.loss_fn(output, y)\n\t        acc = self.metric(y.cpu().numpy(),output.detach().cpu().numpy())\n\t        return loss, acc\n\t    def train_cls(self):\n\t        new_v1, new_v2 = self.model.get_view(self.view1, self.view1_indices, self.view2, self.view2_indices, self.n_nodes, self.feats)\n\t        logits_v1, logits_v2, prob_v1, prob_v2 = self.model.get_cls_loss(new_v1, new_v2, self.feats)\n\t        curr_v = self.model.get_fusion(new_v1, prob_v1, new_v2, prob_v2)\n\t        logits_v = self.model.get_v_cls_loss(curr_v, self.feats)\n\t        views = [curr_v, new_v1, new_v2]\n", "        loss_v1, _ = self.loss_acc(logits_v1[self.train_mask], self.labels[self.train_mask])\n\t        loss_v2, _ = self.loss_acc(logits_v2[self.train_mask], self.labels[self.train_mask])\n\t        loss_v, _ = self.loss_acc(logits_v[self.train_mask], self.labels[self.train_mask])\n\t        return self.conf.model['cls_coe'] * loss_v + (loss_v1 + loss_v2) * (1 - self.conf.model['cls_coe']) / 2, views\n\t    def learn(self, debug=False):\n\t        '''\n\t        Learning process of CoGSL.\n\t        Parameters\n\t        ----------\n\t        debug : bool\n", "            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        graph : torch.tensor\n\t            The learned structure.\n\t        '''\n\t        self.best_acc_val = 0\n\t        self.best_loss_val = 1e9\n", "        self.best_test = 0\n\t        self.best_v_cls_weight = None\n\t        torch.autograd.set_detect_anomaly(True)\n\t        for epoch in range(self.conf.training['main_epoch']):\n\t            curr = np.log(1 + self.conf.training['temp_r'] * epoch)\n\t            curr = min(max(0.05, curr), 0.1)\n\t            for inner_ne in range(self.conf.training['inner_ne_epoch']):\n\t                self.model.train()\n\t                self.opti_ve.zero_grad()\n\t                cls_loss, views = self.train_cls()\n", "                mi_loss = self.train_mi(self.feats, views)\n\t                loss = cls_loss - curr * mi_loss\n\t                #with torch.autograd.detect_anomaly():\n\t                loss.backward()\n\t                self.opti_ve.step()\n\t            self.scheduler.step()\n\t            for inner_cls in range(self.conf.training['inner_cls_epoch']):\n\t                self.model.train()\n\t                self.opti_cls.zero_grad()\n\t                cls_loss, _ = self.train_cls()\n", "                #with torch.autograd.detect_anomaly():\n\t                cls_loss.backward()\n\t                self.opti_cls.step()\n\t            for inner_mi in range(self.conf.training['inner_mi_epoch']):\n\t                self.model.train()\n\t                self.opti_mi.zero_grad()\n\t                _, views = self.train_cls()\n\t                mi_loss = self.train_mi(self.feats, views)\n\t                mi_loss.backward()\n\t                self.opti_mi.step()\n", "            self.model.eval()\n\t            _, views = self.train_cls()\n\t            self.view = views[0]\n\t            loss_val, acc_val = self.evaluate(self.val_mask)\n\t            loss_train, acc_train = self.evaluate(self.train_mask)\n\t            if acc_val >= self.best_acc_val and self.best_loss_val > loss_val:\n\t                self.best_acc_val = max(acc_val, self.best_acc_val)\n\t                self.best_loss_val = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n", "                self.weights = deepcopy(self.model.cls.encoder_v.state_dict())\n\t                self.best_graph = views[0]\n\t            print(\"EPOCH \",epoch, \"\\tCUR_LOSS_VAL \", loss_val, \"\\tCUR_ACC_Val \", acc_val, \"\\tBEST_ACC_VAL \", self.best_acc_val)\n\t        self.total_time = time.time() - self.start_time\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test = self.test()\n\t        #test_f1_macro, test_f1_micro, auc  = self.test()\n\t        self.result['test'] = acc_test\n\t        #print(\"Test_Macro: \", test_f1_macro, \"\\tTest_Micro: \", test_f1_micro, \"\\tAUC: \", auc)\n", "        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, self.best_graph.to_dense()\n\t    def evaluate(self, test_mask):\n\t        '''\n\t        Evaluation procedure of CoGSL.\n\t        Parameters\n\t        ----------\n\t        test_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the data set.\n\t        Returns\n", "        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        '''\n\t        logits = self.model.get_v_cls_loss(self.view, self.feats)\n\t        return self.loss_acc(logits[test_mask], self.labels[test_mask])\n\t    def set_method(self):\n\t        self.model = CoGSL(self.dim_feats, self.conf.model['cls_hid_1'], self.num_targets, self.conf.model['gen_hid'],\n\t                           self.conf.model['mi_hid_1'], self.conf.model['com_lambda_v1'], self.conf.model['com_lambda_v2'],\n\t                           self.conf.model['lam'], self.conf.model['alpha'], self.conf.model['cls_dropout'],\n", "                           self.conf.model['ve_dropout'], self.conf.model['tau'], self.conf.dataset['pyg'],\n\t                           self.conf.dataset['big'], self.conf.dataset['batch'], self.conf.dataset['name']).to(self.device)\n\t        self.opti_ve = torch.optim.Adam(self.model.ve.parameters(), lr=self.conf.training['ve_lr'], weight_decay=self.conf.training['ve_weight_decay'])\n\t        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opti_ve, 0.99)\n\t        self.opti_cls = torch.optim.Adam(self.model.cls.parameters(), lr=self.conf.training['cls_lr'], weight_decay=self.conf.training['cls_weight_decay'])\n\t        self.opti_mi = torch.optim.Adam(self.model.mi.parameters(), lr=self.conf.training['mi_lr'], weight_decay=self.conf.training['mi_weight_decay'])\n\t        self.view1 = self.view1.to(self.device)\n\t        self.view2 = self.view2.to(self.device)\n\t        self.view1_indices = self.view1_indices.to(self.device)\n\t        self.view2_indices = self.view2_indices.to(self.device)\n", "    #def gen_auc_mima(self, logits, label):\n\t    #        preds = torch.argmax(logits, dim=1)\n\t    #        test_f1_macro = f1_score(label.cpu(), preds.cpu(), average='macro')\n\t    #        test_f1_micro = f1_score(label.cpu(), preds.cpu(), average='micro')\n\t    #        \n\t    #        best_proba = F.softmax(logits, dim=1)\n\t    #        if logits.shape[1] != 2:\n\t    #            auc = roc_auc_score(y_true=label.detach().cpu().numpy(),\n\t    #                                                    y_score=best_proba.detach().cpu().numpy(),\n\t    #                                                    multi_class='ovr'\n", "    #                                                    )\n\t    #        else:\n\t    #            auc = roc_auc_score(y_true=label.detach().cpu().numpy(),\n\t    #                                                    y_score=best_proba[:,1].detach().cpu().numpy()\n\t    #                                                    )\n\t    #        return test_f1_macro, test_f1_micro, auc\n\t    def test(self):\n\t        '''\n\t        Test procedure of CoGSL.\n\t        Returns\n", "        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        '''\n\t        self.model.cls.encoder_v.load_state_dict(self.weights)\n\t        self.model.eval()\n\t        self.view = self.best_graph\n\t        return self.evaluate(self.test_mask)\n\tclass WSGNNSolver(Solver):\n\t    '''\n", "    A solver to train, evaluate, test WSGNN in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n", "        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('wsgnn', 'cora')\n\t    >>>\n", "    >>> solver = WSGNNSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = 'wsgnn'\n\t        self.edge_index = self.adj.coalesce().indices()\n\t    def learn(self, debug=False):\n\t        '''\n", "        Learning process of WSGNN.\n\t        Parameters\n\t        ----------\n\t        debug : bool\n\t            Whether to print statistics during training.\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        '''    \n", "        best_node_val = 0\n\t        best_node_test = 0\n\t        best_node_epoch = -1\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            self.model.train()\n\t            self.optimizer.zero_grad()\n\t            p_y, _, q_y, _ = self.model(self.feats, self.n_nodes, self.edge_index)\n\t            p_y = torch.nn.functional.log_softmax(p_y, dim=1)\n\t            q_y = torch.nn.functional.log_softmax(q_y, dim=1)\n\t            mask = torch.zeros(self.n_nodes, dtype=bool)\n", "            mask[self.train_mask] = 1\n\t            loss = self.criterion(self.labels, mask, p_y, q_y, )\n\t            loss.backward()\n\t            self.optimizer.step()\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), q_y[self.train_mask].detach().cpu().numpy()) \n\t            loss_val, acc_val = self.evaluate(self.val_mask)\n\t            flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\t            if flag:\n\t                self.total_time = time.time() - self.start_time\n\t                best_loss = loss_val\n", "                self.result['train'] = acc_train\n\t                self.result['valid'] = acc_val\n\t                self.weights = deepcopy(self.model.state_dict())\n\t            if (epoch + 1) % 10 == 0:\n\t                print(f'Epoch: {epoch:02d}, '\n\t                    f'Loss: {loss:.4f}, '\n\t                    f'Train_acc: {100 * acc_train:.2f}%, '\n\t                    f'Valid_acc: {100 * acc_val:.2f}%, ')\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n", "        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, None\n\t    def evaluate(self, val_mask):\n\t        '''\n\t        Evaluation procedure of CoGSL.\n\t        Parameters\n\t        ----------\n\t        val_mask : torch.tensor\n", "        Returns\n\t        -------\n\t        loss : float\n\t            Evaluation loss.\n\t        '''\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            p_y, _, q_y, _ = self.model(self.feats, self.n_nodes, self.edge_index)\n\t        p_y = torch.nn.functional.log_softmax(p_y, dim=1)\n\t        q_y = torch.nn.functional.log_softmax(q_y, dim=1)\n", "        mask = torch.zeros(self.n_nodes, dtype=bool)\n\t        mask[val_mask] = 1\n\t        loss = self.criterion(self.labels, mask, p_y, q_y)\n\t        acc = self.metric(self.labels[val_mask].cpu().numpy(), q_y[val_mask].detach().cpu().numpy())\n\t        return loss, acc\n\t    def set_method(self):\n\t        self.model = WSGNN(self.conf.model['graph_skip_conn'], self.conf.model['n_hidden'], self.conf.model['dropout'],self.conf.model['n_layers'], \n\t                           self.conf.model['graph_learn_num_pers'], self.conf.model['mlp_layers'], self.conf.model['no_bn'], self.dim_feats,self.n_nodes,self.num_targets).to(self.device)\n\t        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'], weight_decay=self.conf.training['weight_decay'])\n\t        self.criterion = ELBONCLoss()"]}
{"filename": "opengsl/method/gnnsolver.py", "chunked_list": ["from .models.gnn_modules import SGC, LPA, MLP, LINK, LINKX, APPNP, GPRGNN, GAT\n\timport time\n\tfrom .models.gcn import GCN\n\tfrom .models.jknet import JKNet\n\tfrom .solver import Solver\n\timport torch\n\tfrom copy import deepcopy\n\tfrom opengsl.data.preprocess.normalize import normalize\n\tclass SGCSolver(Solver):\n\t    '''\n", "    A solver to train, evaluate, test SGC in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n", "        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('sgc', 'cora')\n\t    >>>\n", "    >>> solver = SGCSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"sgc\"\n\t    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n", "        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        self.normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        '''\n\t        return self.feats, self.normalized_adj\n\t    def set_method(self):\n\t        '''\n", "        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = SGC(self.dim_feats, self.num_targets, self.conf.model['n_layers']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                           weight_decay=self.conf.training['weight_decay'])\n\t        if self.conf.dataset['normalize']:\n\t            self.normalize = normalize\n\t        else:\n\t            self.normalize = lambda x, y: x\n\t        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])\n", "class GCNSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test GCN in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n", "    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n", "    >>> conf = opengsl.config.load_conf('gcn', 'cora')\n\t    >>>\n\t    >>> solver = GCNSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gcn\"\n\t    def input_distributer(self):\n", "        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n\t        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        self.normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        True : constant bool\n\t        '''\n", "        return self.feats, self.normalized_adj, True\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n\t                    self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n\t                    self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n\t                    self.conf.model['input_layer'], self.conf.model['output_layer']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n", "                                      weight_decay=self.conf.training['weight_decay'])\n\t        if self.conf.dataset['normalize']:\n\t            self.normalize = normalize\n\t        else:\n\t            self.normalize = lambda x, y: x\n\t        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])\n\tclass LPASolver(Solver):\n\t    '''\n\t    A solver to test LPA in a run.\n\t    Parameters\n", "    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n", "    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('lpa', 'cora')\n\t    >>>\n\t    >>> solver = LPASolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n", "    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"lpa\"\n\t    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n\t        Returns\n\t        -------\n", "        self.labels : torch.tensor\n\t            Node labels.\n\t        self.normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        self.train_mask : torch.tensor\n\t            A boolean tensor indicating whether the node is in the training set.\n\t        '''\n\t        return self.labels, self.normalized_adj, self.train_mask\n\t    def set_method(self):\n\t        '''\n", "        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = LPA(self.conf.model['n_layers'], self.conf.model['alpha']).to(self.device)\n\t        self.normalize = normalize if self.conf.dataset['normalize'] else lambda x, y: x\n\t        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])\n\t    def learn(self, split=None, debug=False):\n\t        '''\n\t        Learning process of LPA.\n\t        Parameters\n\t        ----------\n", "        debug : bool\n\t        Returns\n\t        -------\n\t        result : dict\n\t            A dict containing train, valid and test metrics.\n\t        0 : constant\n\t        '''\n\t        y_pred = self.model(self.input_distributer())\n\t        loss_test = self.loss_fn(y_pred[self.test_mask], self.labels[self.test_mask])\n\t        acc_test = self.metric(self.labels[self.test_mask].cpu().numpy(), y_pred[self.test_mask].detach().cpu().numpy())\n", "        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, 0\n\tclass MLPSolver(Solver):\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"mlp\"\n\t    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n", "        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        '''\n\t        return self.feats\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n", "        self.model = MLP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                      weight_decay=self.conf.training['weight_decay'])\n\tclass LINKSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test LINK in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n", "    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n", "    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('link', 'cora')\n\t    >>>\n\t    >>> solver = LINKSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n", "        super().__init__(conf, dataset)\n\t        self.method_name = \"link\"\n\t    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n\t        Returns\n\t        -------\n\t        self.adj : torch.tensor\n\t            Adjacency matrix.\n\t        '''\n", "        return self.adj\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = LINK(self.n_nodes, self.num_targets).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                      weight_decay=self.conf.training['weight_decay'])\n\tclass LINKXSolver(Solver):\n\t    def __init__(self, conf, dataset):\n", "        super().__init__(conf, dataset)\n\t        self.method_name = \"linkx\"\n\t    def input_distributer(self):\n\t        return self.feats, self.adj\n\t    def set_method(self):\n\t        self.model = LINKX(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'], self.n_nodes).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                      weight_decay=self.conf.training['weight_decay'])\n\tclass APPNPSolver(Solver):\n\t    '''\n", "    A solver to train, evaluate, test APPNP in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n", "        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('appnp', 'cora')\n\t    >>>\n", "    >>> solver = APPNPSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"appnp\"\n\t    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n", "        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        self.normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        '''\n\t        return self.feats, self.normalized_adj\n\t    def set_method(self):\n\t        '''\n", "        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = APPNP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, dropout=self.conf.model['dropout'],\n\t                      K=self.conf.model['K'], alpha=self.conf.model['alpha']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                 weight_decay=self.conf.training['weight_decay'])\n\t        if self.conf.dataset['normalize']:\n\t            self.normalize = normalize\n\t        else:\n\t            self.normalize = lambda x, y: x\n", "        self.normalized_adj = self.normalize(self.adj, self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])\n\tclass JKNetSolver(Solver):\n\t    '''\n\t    A solver to train, evaluate, test JKNet in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n", "    Attributes\n\t    ----------\n\t    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n", "    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('jknet', 'cora')\n\t    >>>\n\t    >>> solver = JKNetSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"jknet\"\n", "    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n\t        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        self.normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        True : constant bool\n", "        '''\n\t        return self.feats, self.normalized_adj, True\n\t    def set_method(self):\n\t        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = JKNet(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n\t                           self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n\t                           self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n\t                           self.conf.model['general'],\n", "                           self.conf.model['input_layer'], self.conf.model['output_layer']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                      weight_decay=self.conf.training['weight_decay'])\n\t        if self.conf.dataset['normalize']:\n\t            self.normalize = normalize\n\t        else:\n\t            self.normalize = lambda x, y: x\n\t        self.normalized_adj = self.normalize(self.adj, self.conf.dataset['add_loop'], self.conf.dataset['sparse'])\n\tclass GPRGNNSolver(Solver):\n\t    '''\n", "    A solver to train, evaluate, test GPRGNN in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n\t    method_name : str\n", "        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('gprgnn', 'cora')\n\t    >>>\n", "    >>> solver = GPRGNNSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gprgnn\"\n\t    def input_distributer(self):\n\t        '''\n\t        Function to ditribute input to GNNs, automatically called in function `learn`.\n", "        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        self.normalized_adj : torch.tensor\n\t            Adjacency matrix.\n\t        '''\n\t        return self.feats, self.normalized_adj\n\t    def set_method(self):\n\t        '''\n", "        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = GPRGNN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, dropout=self.conf.model['dropout'],\n\t                      dprate=self.conf.model['dprate'], K=self.conf.model['K'], alpha=self.conf.model['alpha'], init=self.conf.model['init']).to(self.device)\n\t        self.optim = torch.optim.Adam([{\n\t                'params': self.model.lin1.parameters(),\n\t                'weight_decay': self.conf.training['weight_decay'], 'lr': self.conf.training['lr']\n\t            }, {\n\t                'params': self.model.lin2.parameters(),\n\t                'weight_decay': self.conf.training['weight_decay'], 'lr': self.conf.training['lr']\n", "            }, {\n\t                'params': self.model.temp,\n\t                'weight_decay': 0.0, 'lr': self.conf.training['lr']\n\t            }], lr=self.conf.training['lr'])\n\t        if self.conf.dataset['normalize']:\n\t            self.normalize = normalize\n\t        else:\n\t            self.normalize = lambda x, y: x\n\t        self.normalized_adj = self.normalize(self.adj, self.conf.dataset['add_loop'], self.conf.dataset['sparse'])\n\tclass GATSolver(Solver):\n", "    '''\n\t    A solver to train, evaluate, test GAT in a run.\n\t    Parameters\n\t    ----------\n\t    conf : argparse.Namespace\n\t        Config file.\n\t    dataset : opengsl.data.Dataset\n\t        The dataset.\n\t    Attributes\n\t    ----------\n", "    method_name : str\n\t        The name of the method.\n\t    Examples\n\t    --------\n\t    >>> # load dataset\n\t    >>> import opengsl.dataset\n\t    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n\t    >>> # load config file\n\t    >>> import opengsl.config.load_conf\n\t    >>> conf = opengsl.config.load_conf('gat', 'cora')\n", "    >>>\n\t    >>> solver = GATSolver(conf, dataset)\n\t    >>> # Conduct a experiment run.\n\t    >>> acc, _ = solver.run_exp(split=0, debug=True)\n\t    '''\n\t    def __init__(self, conf, dataset):\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gat\"\n\t    def input_distributer(self):\n\t        '''\n", "        Function to ditribute input to GNNs, automatically called in function `learn`.\n\t        Returns\n\t        -------\n\t        self.feats : torch.tensor\n\t            Node features.\n\t        self.edge_index : torch.tensor\n\t            Adjacency matrix.\n\t        '''\n\t        return self.feats, self.edge_index\n\t    def set_method(self):\n", "        '''\n\t        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\t        '''\n\t        self.model = GAT(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n\t                         n_heads=self.conf.model['n_heads'], dropout=self.conf.model['dropout']).to(self.device)\n\t        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                           weight_decay=self.conf.training['weight_decay'])\n\t        # prepare edge index\n\t        self.edge_index = self.adj.coalesce().indices()\n"]}
{"filename": "opengsl/method/models/wsgnn.py", "chunked_list": ["from torch_sparse import SparseTensor\n\tfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch\n\t# from torch_geometric.nn import GCNConv, GATConv, APPNP\n\tfrom torch_geometric.nn import GCNConv, GATConv\n\timport torch_sparse\n\t# from .gnn_modules import APPNP\n\tfrom .gcn import GCN\n", "class GraphLearner(nn.Module):\n\t    def __init__(self, input_size, num_pers=16):\n\t        super(GraphLearner, self).__init__()\n\t        self.weight_tensor = torch.Tensor(num_pers, input_size)\n\t        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n\t    def reset_parameters(self):\n\t        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n\t    def forward(self, context):\n\t        expand_weight_tensor = self.weight_tensor.unsqueeze(1)\n\t        context_fc = context.unsqueeze(0) * expand_weight_tensor\n", "        context_norm = F.normalize(context_fc, p=2, dim=-1)\n\t        attention = torch.matmul(context_norm, context_norm.transpose(-1, -2)).mean(0)\n\t        mask = (attention > 0).detach().float()\n\t        attention = attention * mask + 0 * (1 - mask)\n\t        return attention\n\tclass MLP(nn.Module):\n\t    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n\t    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n\t                 dropout=.5, use_bn=False):\n\t        super(MLP, self).__init__()\n", "        self.use_bn = use_bn\n\t        self.lins = nn.ModuleList()\n\t        self.bns = nn.ModuleList()\n\t        if num_layers == 1:\n\t            # just linear layer i.e. logistic regression\n\t            self.lins.append(nn.Linear(in_channels, out_channels))\n\t        else:\n\t            self.lins.append(nn.Linear(in_channels, hidden_channels))\n\t            self.bns.append(nn.BatchNorm1d(hidden_channels))\n\t            for _ in range(num_layers - 2):\n", "                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n\t                self.bns.append(nn.BatchNorm1d(hidden_channels))\n\t            self.lins.append(nn.Linear(hidden_channels, out_channels))\n\t        self.dropout = dropout\n\t    def reset_parameters(self):\n\t        for lin in self.lins:\n\t            lin.reset_parameters()\n\t        for bn in self.bns:\n\t            bn.reset_parameters()\n\t    def forward(self, x):\n", "        for i, lin in enumerate(self.lins[:-1]):\n\t            x = lin(x)\n\t            x = F.relu(x, inplace=True)\n\t            if self.use_bn:\n\t                x = self.bns[i](x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.lins[-1](x)\n\t        return x\n\t# class DenseAPPNP(nn.Module):\n\t#     def __init__(self, K, alpha):\n", "#         super().__init__()\n\t#         self.K = K\n\t#         self.alpha = alpha\n\t#     def forward(self, x, adj_t):\n\t#         h = x\n\t#         for k in range(self.K):\n\t#             if adj_t.is_sparse:\n\t#                 x = torch_sparse.spmm(adj_t, x)\n\t#             else:\n\t#                 x = torch.matmul(adj_t, x)\n", "#             x = x * (1 - self.alpha)\n\t#             x += self.alpha * h\n\t#         return x\n\t# class Dense_APPNP_Net(nn.Module):\n\t#     def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, K=10, alpha=.1):\n\t#         super(Dense_APPNP_Net, self).__init__()\n\t#         self.lin1 = nn.Linear(in_channels, hidden_channels)\n\t#         self.lin2 = nn.Linear(hidden_channels, out_channels)\n\t#         self.prop1 = DenseAPPNP(K, alpha)\n\t#         self.dropout = dropout\n", "#     def reset_parameters(self):\n\t#         self.lin1.reset_parameters()\n\t#         self.lin2.reset_parameters()\n\t#     def forward(self, x, adj_t):\n\t#         x = F.dropout(x, p=self.dropout, training=self.training)\n\t#         x = F.relu(self.lin1(x))\n\t#         x = F.dropout(x, p=self.dropout, training=self.training)\n\t#         x = self.lin2(x)\n\t#         x = self.prop1(x, adj_t)\n\t#         return x\n", "INF = 1e20\n\tVERY_SMALL_NUMBER = 1e-12\n\tclass QModel(nn.Module):\n\t    def __init__(self, graph_skip_conn, nhid, dropout, n_layers, graph_learn_num_pers, d, n, c):\n\t        super(QModel, self).__init__()\n\t        self.graph_skip_conn = graph_skip_conn\n\t        # self.encoder = APPNP(d,nhid,c,dropout,hops,alpha)\n\t        self.encoder = GCN(d, nhid, c, n_layers, dropout)\n\t        # self.encoder = Dense_APPNP_Net(in_channels=d,\n\t        #                                hidden_channels=nhid,\n", "        #                                out_channels=c,\n\t        #                                dropout=dropout,\n\t        #                                K=hops,\n\t        #                                alpha=alpha)\n\t        self.graph_learner1 = GraphLearner(input_size=d, num_pers=graph_learn_num_pers)\n\t        self.graph_learner2 = GraphLearner(input_size=2 * c, num_pers=graph_learn_num_pers)\n\t    def reset_parameters(self):\n\t        self.encoder.reset_parameters()\n\t        self.graph_learner1.reset_parameters()\n\t        self.graph_learner2.reset_parameters()\n", "    def learn_graph(self, graph_learner, node_features, graph_skip_conn=None, init_adj=None):\n\t        raw_adj = graph_learner(node_features)\n\t        adj = raw_adj / torch.clamp(torch.sum(raw_adj, dim=-1, keepdim=True), min=VERY_SMALL_NUMBER)\n\t        adj = graph_skip_conn * init_adj + (1 - graph_skip_conn) * adj\n\t        return raw_adj, adj\n\t    def forward(self, feats, n_node, edge_index):\n\t        node_features = feats\n\t        train_index = edge_index\n\t        edge_weight = None\n\t        _edge_index, edge_weight = gcn_norm(\n", "            train_index, edge_weight, n_node, False,\n\t            dtype=node_features.dtype)\n\t        row, col = _edge_index\n\t        init_adj_sparse = SparseTensor(row=col, col=row, value=edge_weight,\n\t                                       sparse_sizes=(n_node, n_node))\n\t        init_adj = init_adj_sparse.to_dense()\n\t        raw_adj_1, adj_1 = self.learn_graph(self.graph_learner1, node_features, self.graph_skip_conn, init_adj)\n\t        node_vec_1 = self.encoder([node_features, adj_1, True])\n\t        node_vec_2 = self.encoder([node_features, init_adj, True])\n\t        raw_adj_2, adj_2 = self.learn_graph(self.graph_learner2, torch.cat([node_vec_1, node_vec_2], dim=1),\n", "                                            self.graph_skip_conn, init_adj)\n\t        output = 0.5 * node_vec_1 + 0.5 * node_vec_2\n\t        adj = 0.5 * adj_1 + 0.5 * adj_2\n\t        return output, adj\n\tclass PModel(nn.Module):\n\t    def __init__(self, nhid, dropout, n_layers, graph_learn_num_pers, mlp_layers, no_bn, d, n, c):\n\t        super(PModel, self).__init__()\n\t        # self.encoder1 = APPNP(d,nhid,c,dropout,hops,alpha)\n\t        self.encoder1 = GCN(d, nhid, c, n_layers, dropout)\n\t        # self.encoder1 = Dense_APPNP_Net(in_channels=d,\n", "        #                                 hidden_channels=nhid,\n\t        #                                 out_channels=c,\n\t        #                                 dropout=dropout,\n\t        #                                 K=hops,\n\t        #                                 alpha=alpha)\n\t        self.encoder2 = MLP(in_channels=d,\n\t                            hidden_channels=nhid,\n\t                            out_channels=c,\n\t                            num_layers=mlp_layers,\n\t                            dropout=dropout,\n", "                            use_bn=not no_bn)\n\t        self.graph_learner1 = GraphLearner(input_size=d, num_pers=graph_learn_num_pers)\n\t        self.graph_learner2 = GraphLearner(input_size=2 * c, num_pers=graph_learn_num_pers)\n\t    def reset_parameters(self):\n\t        self.encoder1.reset_parameters()\n\t        self.encoder2.reset_parameters()\n\t        self.graph_learner.reset_parameters()\n\t        self.graph_learner2.reset_parameters()\n\t    def learn_graph(self, graph_learner, node_features):\n\t        raw_adj = graph_learner(node_features)\n", "        adj = raw_adj / torch.clamp(torch.sum(raw_adj, dim=-1, keepdim=True), min=VERY_SMALL_NUMBER)\n\t        return raw_adj, adj\n\t    def forward(self, feats):\n\t        node_features = feats\n\t        raw_adj_1, adj_1 = self.learn_graph(self.graph_learner1, node_features)\n\t        node_vec_1 = self.encoder1([node_features, adj_1, True])\n\t        node_vec_2 = self.encoder2(node_features)\n\t        raw_adj_2, adj_2 = self.learn_graph(self.graph_learner2, torch.cat([node_vec_1, node_vec_2], dim=1))\n\t        output = 0.5 * node_vec_1 + 0.5 * node_vec_2\n\t        adj = 0.5 * adj_1 + 0.5 * adj_2\n", "        return output, adj\n\tclass WSGNN(nn.Module):\n\t    def __init__(self, graph_skip_conn, nhid, dropout, n_layers, graph_learn_num_pers, mlp_layers, no_bn, d, n, c):\n\t        super(WSGNN, self).__init__()\n\t        self.P_Model = PModel(nhid, dropout, n_layers, graph_learn_num_pers, mlp_layers, no_bn, d, n, c)\n\t        self.Q_Model = QModel(graph_skip_conn, nhid, dropout, n_layers, graph_learn_num_pers, d, n, c)\n\t    def reset_parameters(self):\n\t        self.P_Model.reset_parameters()\n\t        self.Q_Model.reset_parameters()\n\t    def forward(self, feats, n_node, edge_index):\n", "        q_y, q_a = self.Q_Model.forward(feats, n_node, edge_index)\n\t        p_y, p_a = self.P_Model.forward(feats)\n\t        return p_y, p_a, q_y, q_a\n\tclass ELBONCLoss(nn.Module):\n\t    def __init__(self):\n\t        super(ELBONCLoss, self).__init__()\n\t    def forward(self, labels, train_mask, log_p_y, log_q_y):\n\t        y_obs = labels[train_mask]\n\t        log_p_y_obs = log_p_y[train_mask]\n\t        p_y_obs = torch.exp(log_p_y_obs)\n", "        log_p_y_miss = log_p_y[train_mask == 0]\n\t        p_y_miss = torch.exp(log_p_y_miss)\n\t        log_q_y_obs = log_q_y[train_mask]\n\t        q_y_obs = torch.exp(log_q_y_obs)\n\t        log_q_y_miss = log_q_y[train_mask == 0]\n\t        q_y_miss = torch.exp(log_q_y_miss)\n\t        loss_p_y = F.nll_loss(log_p_y_obs, y_obs) - torch.mean(q_y_miss * log_p_y_miss)\n\t        loss_q_y = torch.mean(q_y_miss * log_q_y_miss)\n\t        loss_y_obs = 10 * F.nll_loss(log_q_y_obs, y_obs)\n\t        loss = loss_p_y + loss_q_y + loss_y_obs\n", "        return loss"]}
{"filename": "opengsl/method/models/prognn.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.optim import Optimizer\n\tfrom torch.optim.optimizer import required\n\timport scipy.sparse as sp\n\tclass PGD(Optimizer):\n\t    \"\"\"Proximal gradient descent.\n\t    Parameters\n\t    ----------\n", "    params : iterable\n\t        iterable of parameters to optimize or dicts defining parameter groups\n\t    proxs : iterable\n\t        iterable of proximal operators\n\t    alpha : iterable\n\t        iterable of coefficients for proximal gradient descent\n\t    lr : float\n\t        learning rate\n\t    momentum : float\n\t        momentum factor (default: 0)\n", "    weight_decay : float\n\t        weight decay (L2 penalty) (default: 0)\n\t    dampening : float\n\t        dampening for momentum (default: 0)\n\t    \"\"\"\n\t    def __init__(self, params, proxs, alphas, lr=required, momentum=0, dampening=0, weight_decay=0):\n\t        defaults = dict(lr=lr, momentum=0, dampening=0,\n\t                        weight_decay=0, nesterov=False)\n\t        super(PGD, self).__init__(params, defaults)\n\t        for group in self.param_groups:\n", "            group.setdefault('proxs', proxs)\n\t            group.setdefault('alphas', alphas)\n\t    def __setstate__(self, state):\n\t        super(PGD, self).__setstate__(state)\n\t        for group in self.param_groups:\n\t            group.setdefault('nesterov', False)\n\t            group.setdefault('proxs', proxs)\n\t            group.setdefault('alphas', alphas)\n\t    def step(self, delta=0, closure=None):\n\t        for group in self.param_groups:\n", "            lr = group['lr']\n\t            weight_decay = group['weight_decay']\n\t            momentum = group['momentum']\n\t            dampening = group['dampening']\n\t            nesterov = group['nesterov']\n\t            proxs = group['proxs']\n\t            alphas = group['alphas']\n\t            # apply the proximal operator to each parameter in a group\n\t            for param in group['params']:\n\t                for prox_operator, alpha in zip(proxs, alphas):\n", "                    # param.data.add_(lr, -param.grad.data)\n\t                    # param.data.add_(delta)\n\t                    param.data = prox_operator(param.data, alpha=alpha*lr)\n\tclass ProxOperators():\n\t    \"\"\"Proximal Operators.\n\t    \"\"\"\n\t    def __init__(self):\n\t        self.nuclear_norm = None\n\t    def prox_l1(self, data, alpha):\n\t        \"\"\"Proximal operator for l1 norm.\n", "        \"\"\"\n\t        data = torch.mul(torch.sign(data), torch.clamp(torch.abs(data)-alpha, min=0))\n\t        return data\n\t    def prox_nuclear(self, data, alpha):\n\t        \"\"\"Proximal operator for nuclear norm (trace norm).\n\t        \"\"\"\n\t        U, S, V = np.linalg.svd(data.cpu())\n\t        U, S, V = torch.FloatTensor(U).cuda(), torch.FloatTensor(S).cuda(), torch.FloatTensor(V).cuda()\n\t        self.nuclear_norm = S.sum()\n\t        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n", "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n\t        return torch.matmul(torch.matmul(U, diag_S), V)\n\t    def prox_nuclear_truncated_2(self, data, alpha, k=50):\n\t        import tensorly as tl\n\t        tl.set_backend('pytorch')\n\t        U, S, V = tl.truncated_svd(data.cpu(), n_eigenvecs=k)\n\t        U, S, V = torch.FloatTensor(U).cuda(), torch.FloatTensor(S).cuda(), torch.FloatTensor(V).cuda()\n\t        self.nuclear_norm = S.sum()\n\t        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n\t        S = torch.clamp(S-alpha, min=0)\n", "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n\t        # U = torch.spmm(U, diag_S)\n\t        # V = torch.matmul(U, V)\n\t        # make diag_S sparse matrix\n\t        indices = torch.tensor((range(0, len(S)), range(0, len(S)))).cuda()\n\t        values = S\n\t        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size((len(S), len(S))))\n\t        V = torch.spmm(diag_S, V)\n\t        V = torch.matmul(U, V)\n\t        return V\n", "    def prox_nuclear_truncated(self, data, alpha, k=50):\n\t        indices = torch.nonzero(data).t()\n\t        values = data[indices[0], indices[1]] # modify this based on dimensionality\n\t        data_sparse = sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()))\n\t        U, S, V = sp.linalg.svds(data_sparse, k=k)\n\t        U, S, V = torch.FloatTensor(U).cuda(), torch.FloatTensor(S).cuda(), torch.FloatTensor(V).cuda()\n\t        self.nuclear_norm = S.sum()\n\t        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n\t        return torch.matmul(torch.matmul(U, diag_S), V)\n\t    def prox_nuclear_cuda(self, data, alpha):\n", "        U, S, V = torch.svd(data)\n\t        # self.nuclear_norm = S.sum()\n\t        # print(f\"rank = {len(S.nonzero())}\")\n\t        self.nuclear_norm = S.sum()\n\t        S = torch.clamp(S-alpha, min=0)\n\t        indices = torch.tensor([range(0, U.shape[0]),range(0, U.shape[0])]).cuda()\n\t        values = S\n\t        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size(U.shape))\n\t        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n\t        # print(f\"rank_after = {len(diag_S.nonzero())}\")\n", "        V = torch.spmm(diag_S, V.t_())\n\t        V = torch.matmul(U, V)\n\t        return V\n\tclass EstimateAdj(nn.Module):\n\t    \"\"\"Provide a pytorch parameter matrix for estimated\n\t    adjacency matrix and corresponding operations.\n\t    \"\"\"\n\t    def __init__(self, adj, symmetric=False, device='cpu'):\n\t        super(EstimateAdj, self).__init__()\n\t        n = len(adj)\n", "        self.estimated_adj = nn.Parameter(torch.FloatTensor(n, n))\n\t        self._init_estimation(adj)\n\t        self.symmetric = symmetric\n\t        self.device = device\n\t    def _init_estimation(self, adj):\n\t        with torch.no_grad():\n\t            n = len(adj)\n\t            self.estimated_adj.data.copy_(adj)\n\t    def forward(self):\n\t        return self.estimated_adj\n", "    def normalize(self):\n\t        if self.symmetric:\n\t            adj = (self.estimated_adj + self.estimated_adj.t()) / 2\n\t        else:\n\t            adj = self.estimated_adj\n\t        normalized_adj = self._normalize(adj + torch.eye(adj.shape[0]).to(self.device))\n\t        return normalized_adj\n\t    def _normalize(self, mx):\n\t        rowsum = mx.sum(1)\n\t        r_inv = rowsum.pow(-1/2).flatten()\n", "        r_inv[torch.isinf(r_inv)] = 0.\n\t        r_mat_inv = torch.diag(r_inv)\n\t        mx = r_mat_inv @ mx\n\t        mx = mx @ r_mat_inv\n\t        return mx\n\tdef feature_smoothing(adj, X):\n\t    adj = (adj.t() + adj)/2\n\t    rowsum = adj.sum(1)\n\t    r_inv = rowsum.flatten()\n\t    D = torch.diag(r_inv)\n", "    L = D - adj\n\t    r_inv = r_inv  + 1e-3\n\t    r_inv = r_inv.pow(-1/2).flatten()\n\t    r_inv[torch.isinf(r_inv)] = 0.\n\t    r_mat_inv = torch.diag(r_inv)\n\t    # L = r_mat_inv @ L\n\t    L = r_mat_inv @ L @ r_mat_inv\n\t    XLXT = torch.matmul(torch.matmul(X.t(), L), X)\n\t    loss_smooth_feat = torch.trace(XLXT)\n\t    return loss_smooth_feat\n", "prox_operators = ProxOperators()\n"]}
{"filename": "opengsl/method/models/segsl.py", "chunked_list": ["import torch\n\timport dgl\n\timport copy\n\timport math\n\timport heapq\n\timport numba as nb\n\timport numpy as np\n\tdef add_knn(k, node_embed, edge_index):\n\t    # è¿éç¨cosineï¼åè®ºæä¸­ä¸ä¸è´\n\t    knn_g = dgl.knn_graph(node_embed,\n", "                          k,\n\t                          algorithm='bruteforce-sharemem',\n\t                          dist='cosine')\n\t    knn_g = dgl.add_reverse_edges(knn_g)\n\t    knn_edge_index = knn_g.edges()\n\t    knn_edge_index = torch.cat(\n\t        (knn_edge_index[0].reshape(1, -1), knn_edge_index[1].reshape(1, -1)),\n\t        dim=0)\n\t    knn_edge_index = knn_edge_index.t()\n\t    edge_index_2 = torch.cat((edge_index, knn_edge_index), dim=0)\n", "    edge_index_2 = torch.unique(edge_index_2, dim=0)\n\t    return edge_index_2\n\tdef calc_e1(adj: torch.Tensor):\n\t    adj = adj - torch.diag_embed(torch.diag(adj))\n\t    degree = adj.sum(dim=1)\n\t    vol = adj.sum()\n\t    idx = degree.nonzero().reshape(-1)\n\t    g = degree[idx]\n\t    return -((g / vol) * torch.log2(g / vol)).sum()\n\tdef get_adj_matrix(node_num, edge_index, weight) -> torch.Tensor:\n", "    adj_matrix = torch.zeros((node_num, node_num))\n\t    adj_matrix[edge_index.t()[0], edge_index.t()[1]] = weight.float()\n\t    adj_matrix = adj_matrix - torch.diag_embed(torch.diag(adj_matrix))  #å»é¤å¯¹è§çº¿\n\t    return adj_matrix\n\tdef get_weight(node_embedding, edge_index):\n\t    node_num = node_embedding.shape[0]\n\t    links = node_embedding[edge_index]\n\t    weight = []\n\t    for i in range(links.shape[0]):\n\t        # print(links[i])\n", "        # weight.append(torch.corrcoef(links[i])[0, 1])\n\t        weight.append(np.corrcoef(links[i].cpu())[0, 1])\n\t    weight = torch.tensor(weight) + 1\n\t    weight[torch.isnan(weight)] = 0\n\t    M = weight.mean() / (2 * node_num)\n\t    weight = weight + M\n\t    return weight\n\tdef knn_maxE1(edge_index: torch.Tensor, node_embedding: torch.Tensor):\n\t    old_e1 = 0\n\t    node_num = node_embedding.shape[0]\n", "    k = 1\n\t    while k < 50:\n\t        edge_index_k = add_knn(k, node_embedding, edge_index)\n\t        weight = get_weight(node_embedding, edge_index_k)\n\t        # e1 = calc_e1(edge_index_k, weight)\n\t        adj = get_adj_matrix(node_num, edge_index_k, weight)\n\t        e1 = calc_e1(adj)\n\t        if e1 - old_e1 > 0.1:\n\t            k += 5\n\t        elif e1 - old_e1 > 0.01:\n", "            k += 3\n\t        elif e1 - old_e1 > 0.001:\n\t            k += 1\n\t        else:\n\t            break\n\t        old_e1 = e1\n\t    print(f'max1SE k: {k}')\n\t    return k\n\tdef get_id():\n\t    i = 0\n", "    while True:\n\t        yield i\n\t        i += 1\n\tdef graph_parse(adj_matrix):\n\t    g_num_nodes = adj_matrix.shape[0]\n\t    adj_table = {}\n\t    VOL = 0\n\t    node_vol = []\n\t    for i in range(g_num_nodes):\n\t        n_v = 0\n", "        adj = set()\n\t        for j in range(g_num_nodes):\n\t            if adj_matrix[i, j] != 0:\n\t                n_v += adj_matrix[i, j]\n\t                VOL += adj_matrix[i, j]\n\t                adj.add(j)\n\t        adj_table[i] = adj\n\t        node_vol.append(n_v)\n\t    return g_num_nodes, VOL, node_vol, adj_table\n\t@nb.jit(nopython=True)\n", "def cut_volume(adj_matrix, p1, p2):\n\t    c12 = 0\n\t    for i in range(len(p1)):\n\t        for j in range(len(p2)):\n\t            c = adj_matrix[p1[i], p2[j]]\n\t            if c != 0:\n\t                c12 += c\n\t    return c12\n\tdef LayerFirst(node_dict, start_id):\n\t    stack = [start_id]\n", "    while len(stack) != 0:\n\t        node_id = stack.pop(0)\n\t        yield node_id\n\t        if node_dict[node_id].children:\n\t            for c_id in node_dict[node_id].children:\n\t                stack.append(c_id)\n\tdef merge(new_ID, id1, id2, cut_v, node_dict):\n\t    new_partition = node_dict[id1].partition + node_dict[id2].partition\n\t    v = node_dict[id1].vol + node_dict[id2].vol\n\t    g = node_dict[id1].g + node_dict[id2].g - 2 * cut_v\n", "    child_h = max(node_dict[id1].child_h, node_dict[id2].child_h) + 1\n\t    new_node = PartitionTreeNode(ID=new_ID,\n\t                                 partition=new_partition,\n\t                                 children={id1, id2},\n\t                                 g=g,\n\t                                 vol=v,\n\t                                 child_h=child_h,\n\t                                 child_cut=cut_v)\n\t    node_dict[id1].parent = new_ID\n\t    node_dict[id2].parent = new_ID\n", "    node_dict[new_ID] = new_node\n\tdef compressNode(node_dict, node_id, parent_id):\n\t    p_child_h = node_dict[parent_id].child_h\n\t    node_children = node_dict[node_id].children\n\t    node_dict[parent_id].child_cut += node_dict[node_id].child_cut\n\t    node_dict[parent_id].children.remove(node_id)\n\t    node_dict[parent_id].children = node_dict[parent_id].children.union(\n\t        node_children)\n\t    for c in node_children:\n\t        node_dict[c].parent = parent_id\n", "    com_node_child_h = node_dict[node_id].child_h\n\t    node_dict.pop(node_id)\n\t    if (p_child_h - com_node_child_h) == 1:\n\t        while True:\n\t            max_child_h = max([\n\t                node_dict[f_c].child_h for f_c in node_dict[parent_id].children\n\t            ])\n\t            if node_dict[parent_id].child_h == (max_child_h + 1):\n\t                break\n\t            node_dict[parent_id].child_h = max_child_h + 1\n", "            parent_id = node_dict[parent_id].parent\n\t            if parent_id is None:\n\t                break\n\tdef child_tree_deepth(node_dict, nid):\n\t    node = node_dict[nid]\n\t    deepth = 0\n\t    while node.parent is not None:\n\t        node = node_dict[node.parent]\n\t        deepth += 1\n\t    deepth += node_dict[nid].child_h\n", "    return deepth\n\tdef CompressDelta(node1, p_node):\n\t    a = node1.child_cut\n\t    v1 = node1.vol + 1\n\t    v2 = p_node.vol + 1\n\t    return a * math.log2(v2 / v1)\n\tdef CombineDelta(node1, node2, cut_v, g_vol):\n\t    v1 = node1.vol + 1\n\t    v2 = node2.vol + 1\n\t    g1 = node1.g + 1\n", "    g2 = node2.g + 1\n\t    v12 = v1 + v2\n\t    return ((v1 - g1) * math.log2(v12 / v1) + (v2 - g2) * math.log2(v12 / v2) -\n\t            2 * cut_v * math.log2(g_vol / v12)) / g_vol\n\tclass PartitionTreeNode:\n\t    def __init__(self,\n\t                 ID,\n\t                 partition,\n\t                 vol,\n\t                 g,\n", "                 children: set = None,\n\t                 parent=None,\n\t                 child_h=0,\n\t                 child_cut=0):\n\t        self.ID = ID\n\t        self.partition = partition\n\t        self.parent = parent\n\t        self.children = children\n\t        self.vol = vol\n\t        self.g = g\n", "        self.merged = False\n\t        self.child_h = child_h  #ä¸åæ¬è¯¥èç¹çå­æ é«åº¦\n\t        self.child_cut = child_cut\n\t    def __str__(self):\n\t        return \"{\" + \"{}:{}\".format(self.__class__.__name__,\n\t                                    self.gatherAttrs()) + \"}\"\n\t    def gatherAttrs(self):\n\t        return \",\".join(\"{}={}\".format(k, getattr(self, k))\n\t                        for k in self.__dict__.keys())\n\tclass PartitionTree:\n", "    def __init__(self, adj_matrix):\n\t        self.adj_matrix = adj_matrix\n\t        self.tree_node = {}\n\t        self.g_num_nodes, self.VOL, self.node_vol, self.adj_table = graph_parse(\n\t            adj_matrix)\n\t        self.id_g = get_id()\n\t        self.leaves = []\n\t        self.build_leaves()\n\t    def build_leaves(self):\n\t        for vertex in range(self.g_num_nodes):\n", "            ID = next(self.id_g)\n\t            v = self.node_vol[vertex]\n\t            leaf_node = PartitionTreeNode(ID=ID,\n\t                                          partition=[vertex],\n\t                                          g=v,\n\t                                          vol=v)\n\t            self.tree_node[ID] = leaf_node\n\t            self.leaves.append(ID)\n\t    def build_sub_leaves(self, node_list, p_vol):\n\t        subgraph_node_dict = {}\n", "        ori_ent = 0\n\t        for vertex in node_list:\n\t            ori_ent += -(self.tree_node[vertex].g / self.VOL)\\\n\t                       * math.log2((self.tree_node[vertex].vol+1)/(p_vol+1))\n\t            sub_n = set()\n\t            vol = 0\n\t            for vertex_n in node_list:\n\t                c = self.adj_matrix[vertex, vertex_n]\n\t                if c != 0:\n\t                    vol += c\n", "                    sub_n.add(vertex_n)\n\t            sub_leaf = PartitionTreeNode(ID=vertex,\n\t                                         partition=[vertex],\n\t                                         g=vol,\n\t                                         vol=vol)\n\t            subgraph_node_dict[vertex] = sub_leaf\n\t            self.adj_table[vertex] = sub_n\n\t        return subgraph_node_dict, ori_ent\n\t    def build_root_down(self):\n\t        root_child = self.tree_node[self.root_id].children\n", "        subgraph_node_dict = {}\n\t        ori_en = 0\n\t        g_vol = self.tree_node[self.root_id].vol\n\t        for node_id in root_child:\n\t            node = self.tree_node[node_id]\n\t            ori_en += -(node.g / g_vol) * math.log2((node.vol + 1) / g_vol)\n\t            new_n = set()\n\t            for nei in self.adj_table[node_id]:\n\t                if nei in root_child:\n\t                    new_n.add(nei)\n", "            self.adj_table[node_id] = new_n\n\t            new_node = PartitionTreeNode(ID=node_id,\n\t                                         partition=node.partition,\n\t                                         vol=node.vol,\n\t                                         g=node.g,\n\t                                         children=node.children)\n\t            subgraph_node_dict[node_id] = new_node\n\t        return subgraph_node_dict, ori_en\n\t    def entropy(self, node_dict=None):\n\t        if node_dict is None:\n", "            node_dict = self.tree_node\n\t        ent = 0\n\t        for node_id, node in node_dict.items():\n\t            if node.parent is not None:\n\t                node_p = node_dict[node.parent]\n\t                node_vol = node.vol + 1\n\t                node_g = node.g\n\t                node_p_vol = node_p.vol + 1\n\t                ent += -(node_g / self.VOL) * math.log2(node_vol / node_p_vol)\n\t        return ent\n", "    def __build_k_tree(\n\t        self,\n\t        g_vol,\n\t        nodes_dict: dict,\n\t        k=None,\n\t    ):\n\t        min_heap = []\n\t        cmp_heap = []\n\t        nodes_ids = nodes_dict.keys()\n\t        new_id = None\n", "        for i in nodes_ids:\n\t            for j in self.adj_table[i]:\n\t                if j > i:\n\t                    n1 = nodes_dict[i]\n\t                    n2 = nodes_dict[j]\n\t                    if len(n1.partition) == 1 and len(n2.partition) == 1:\n\t                        cut_v = self.adj_matrix[n1.partition[0],\n\t                                                n2.partition[0]]\n\t                    else:\n\t                        cut_v = cut_volume(self.adj_matrix,\n", "                                           p1=np.array(n1.partition),\n\t                                           p2=np.array(n2.partition))\n\t                    diff = CombineDelta(nodes_dict[i], nodes_dict[j], cut_v,\n\t                                        g_vol)\n\t                    heapq.heappush(min_heap, (diff, i, j, cut_v))\n\t        unmerged_count = len(nodes_ids)\n\t        while unmerged_count > 1:\n\t            if len(min_heap) == 0:\n\t                break\n\t            diff, id1, id2, cut_v = heapq.heappop(min_heap)\n", "            if nodes_dict[id1].merged or nodes_dict[id2].merged:\n\t                continue\n\t            nodes_dict[id1].merged = True\n\t            nodes_dict[id2].merged = True\n\t            new_id = next(self.id_g)\n\t            merge(new_id, id1, id2, cut_v, nodes_dict)\n\t            self.adj_table[new_id] = self.adj_table[id1].union(\n\t                self.adj_table[id2])\n\t            for i in self.adj_table[new_id]:\n\t                self.adj_table[i].add(new_id)\n", "            #compress delta\n\t            if nodes_dict[id1].child_h > 0:\n\t                heapq.heappush(cmp_heap, [\n\t                    CompressDelta(nodes_dict[id1], nodes_dict[new_id]), id1,\n\t                    new_id\n\t                ])\n\t            if nodes_dict[id2].child_h > 0:\n\t                heapq.heappush(cmp_heap, [\n\t                    CompressDelta(nodes_dict[id2], nodes_dict[new_id]), id2,\n\t                    new_id\n", "                ])\n\t            unmerged_count -= 1\n\t            for ID in self.adj_table[new_id]:\n\t                if not nodes_dict[ID].merged:\n\t                    n1 = nodes_dict[ID]\n\t                    n2 = nodes_dict[new_id]\n\t                    cut_v = cut_volume(self.adj_matrix, np.array(n1.partition),\n\t                                       np.array(n2.partition))\n\t                    new_diff = CombineDelta(nodes_dict[ID], nodes_dict[new_id],\n\t                                            cut_v, g_vol)\n", "                    heapq.heappush(min_heap, (new_diff, ID, new_id, cut_v))\n\t        root = new_id\n\t        if unmerged_count > 1:\n\t            #combine solitary node\n\t            assert len(min_heap) == 0\n\t            unmerged_nodes = {i for i, j in nodes_dict.items() if not j.merged}\n\t            new_child_h = max([nodes_dict[i].child_h\n\t                               for i in unmerged_nodes]) + 1\n\t            new_id = next(self.id_g)\n\t            new_node = PartitionTreeNode(ID=new_id,\n", "                                         partition=list(nodes_ids),\n\t                                         children=unmerged_nodes,\n\t                                         vol=g_vol,\n\t                                         g=0,\n\t                                         child_h=new_child_h)\n\t            nodes_dict[new_id] = new_node\n\t            for i in unmerged_nodes:\n\t                nodes_dict[i].merged = True\n\t                nodes_dict[i].parent = new_id\n\t                if nodes_dict[i].child_h > 0:\n", "                    heapq.heappush(cmp_heap, [\n\t                        CompressDelta(nodes_dict[i], nodes_dict[new_id]), i,\n\t                        new_id\n\t                    ])\n\t            root = new_id\n\t        if k is not None:\n\t            while nodes_dict[root].child_h > k:\n\t                diff, node_id, p_id = heapq.heappop(cmp_heap)\n\t                if child_tree_deepth(nodes_dict, node_id) <= k:\n\t                    continue\n", "                children = nodes_dict[node_id].children\n\t                compressNode(nodes_dict, node_id, p_id)\n\t                if nodes_dict[root].child_h == k:\n\t                    break\n\t                for e in cmp_heap:\n\t                    if e[1] == p_id:\n\t                        if child_tree_deepth(nodes_dict, p_id) > k:\n\t                            e[0] = CompressDelta(nodes_dict[e[1]],\n\t                                                 nodes_dict[e[2]])\n\t                    if e[1] in children:\n", "                        if nodes_dict[e[1]].child_h == 0:\n\t                            continue\n\t                        if child_tree_deepth(nodes_dict, e[1]) > k:\n\t                            e[2] = p_id\n\t                            e[0] = CompressDelta(nodes_dict[e[1]],\n\t                                                 nodes_dict[p_id])\n\t                heapq.heapify(cmp_heap)\n\t        return root\n\t    def check_balance(self, node_dict, root_id):\n\t        root_c = copy.deepcopy(node_dict[root_id].children)\n", "        for c in root_c:\n\t            if node_dict[c].child_h == 0:\n\t                self.single_up(node_dict, c)\n\t    def single_up(self, node_dict, node_id):\n\t        new_id = next(self.id_g)\n\t        p_id = node_dict[node_id].parent\n\t        grow_node = PartitionTreeNode(ID=new_id,\n\t                                      partition=node_dict[node_id].partition,\n\t                                      parent=p_id,\n\t                                      children={node_id},\n", "                                      vol=node_dict[node_id].vol,\n\t                                      g=node_dict[node_id].g)\n\t        node_dict[node_id].parent = new_id\n\t        node_dict[p_id].children.remove(node_id)\n\t        node_dict[p_id].children.add(new_id)\n\t        node_dict[new_id] = grow_node\n\t        node_dict[new_id].child_h = node_dict[node_id].child_h + 1\n\t        self.adj_table[new_id] = self.adj_table[node_id]\n\t        for i in self.adj_table[node_id]:\n\t            self.adj_table[i].add(new_id)\n", "    def root_down_delta(self):\n\t        if len(self.tree_node[self.root_id].children) < 3:\n\t            return 0, None, None\n\t        subgraph_node_dict, ori_entropy = self.build_root_down()\n\t        g_vol = self.tree_node[self.root_id].vol\n\t        new_root = self.__build_k_tree(g_vol=g_vol,\n\t                                       nodes_dict=subgraph_node_dict,\n\t                                       k=2)\n\t        self.check_balance(subgraph_node_dict, new_root)\n\t        new_entropy = self.entropy(subgraph_node_dict)\n", "        delta = (ori_entropy - new_entropy) / len(\n\t            self.tree_node[self.root_id].children)\n\t        return delta, new_root, subgraph_node_dict\n\t    def leaf_up_entropy(self, sub_node_dict, sub_root_id, node_id):\n\t        ent = 0\n\t        for sub_node_id in LayerFirst(sub_node_dict, sub_root_id):\n\t            if sub_node_id == sub_root_id:\n\t                sub_node_dict[sub_root_id].vol = self.tree_node[node_id].vol\n\t                sub_node_dict[sub_root_id].g = self.tree_node[node_id].g\n\t            elif sub_node_dict[sub_node_id].child_h == 1:\n", "                node = sub_node_dict[sub_node_id]\n\t                inner_vol = node.vol - node.g\n\t                partition = node.partition\n\t                ori_vol = sum(self.tree_node[i].vol for i in partition)\n\t                ori_g = ori_vol - inner_vol\n\t                node.vol = ori_vol\n\t                node.g = ori_g\n\t                node_p = sub_node_dict[node.parent]\n\t                ent += -(node.g / self.VOL) * math.log2(\n\t                    (node.vol + 1) / (node_p.vol + 1))\n", "            else:\n\t                node = sub_node_dict[sub_node_id]\n\t                node.g = self.tree_node[sub_node_id].g\n\t                node.vol = self.tree_node[sub_node_id].vol\n\t                node_p = sub_node_dict[node.parent]\n\t                ent += -(node.g / self.VOL) * math.log2(\n\t                    (node.vol + 1) / (node_p.vol + 1))\n\t        return ent\n\t    def leaf_up(self):\n\t        h1_id = set()\n", "        h1_new_child_tree = {}\n\t        id_mapping = {}\n\t        for l in self.leaves:\n\t            p = self.tree_node[l].parent\n\t            h1_id.add(p)\n\t        delta = 0\n\t        for node_id in h1_id:\n\t            candidate_node = self.tree_node[node_id]\n\t            sub_nodes = candidate_node.partition\n\t            if len(sub_nodes) == 1:\n", "                id_mapping[node_id] = None\n\t            if len(sub_nodes) == 2:\n\t                id_mapping[node_id] = None\n\t            if len(sub_nodes) >= 3:\n\t                sub_g_vol = candidate_node.vol - candidate_node.g\n\t                subgraph_node_dict, ori_ent = self.build_sub_leaves(\n\t                    sub_nodes, candidate_node.vol)\n\t                sub_root = self.__build_k_tree(g_vol=sub_g_vol,\n\t                                               nodes_dict=subgraph_node_dict,\n\t                                               k=2)\n", "                self.check_balance(subgraph_node_dict, sub_root)\n\t                new_ent = self.leaf_up_entropy(subgraph_node_dict, sub_root,\n\t                                               node_id)\n\t                delta += (ori_ent - new_ent)\n\t                h1_new_child_tree[node_id] = subgraph_node_dict\n\t                id_mapping[node_id] = sub_root\n\t        delta = delta / self.g_num_nodes\n\t        return delta, id_mapping, h1_new_child_tree\n\t    def leaf_up_update(self, id_mapping, leaf_up_dict):\n\t        for node_id, h1_root in id_mapping.items():\n", "            if h1_root is None:\n\t                children = copy.deepcopy(self.tree_node[node_id].children)\n\t                for i in children:\n\t                    self.single_up(self.tree_node, i)\n\t            else:\n\t                h1_dict = leaf_up_dict[node_id]\n\t                self.tree_node[node_id].children = h1_dict[h1_root].children\n\t                for h1_c in h1_dict[h1_root].children:\n\t                    assert h1_c not in self.tree_node\n\t                    h1_dict[h1_c].parent = node_id\n", "                h1_dict.pop(h1_root)\n\t                self.tree_node.update(h1_dict)\n\t        self.tree_node[self.root_id].child_h += 1\n\t    def root_down_update(self, new_id, root_down_dict):\n\t        self.tree_node[self.root_id].children = root_down_dict[new_id].children\n\t        for node_id in root_down_dict[new_id].children:\n\t            assert node_id not in self.tree_node\n\t            root_down_dict[node_id].parent = self.root_id\n\t        root_down_dict.pop(new_id)\n\t        self.tree_node.update(root_down_dict)\n", "        self.tree_node[self.root_id].child_h += 1\n\t    def build_coding_tree(self, k=2, mode='v2'):\n\t        if k == 1:\n\t            return\n\t        if mode == 'v1' or k is None:\n\t            self.root_id = self.__build_k_tree(self.VOL, self.tree_node, k=k)\n\t        elif mode == 'v2':\n\t            self.root_id = self.__build_k_tree(self.VOL, self.tree_node, k=2)\n\t            self.check_balance(self.tree_node, self.root_id)\n\t            if self.tree_node[self.root_id].child_h < 2:\n", "                self.tree_node[self.root_id].child_h = 2\n\t            flag = 0\n\t            while self.tree_node[self.root_id].child_h < k:\n\t                if flag == 0:\n\t                    leaf_up_delta, id_mapping, leaf_up_dict = self.leaf_up()\n\t                    root_down_delta, new_id, root_down_dict = self.root_down_delta(\n\t                    )\n\t                elif flag == 1:\n\t                    leaf_up_delta, id_mapping, leaf_up_dict = self.leaf_up()\n\t                elif flag == 2:\n", "                    root_down_delta, new_id, root_down_dict = self.root_down_delta(\n\t                    )\n\t                else:\n\t                    raise ValueError\n\t                if leaf_up_delta < root_down_delta:\n\t                    # print('root down')\n\t                    # root down update and recompute root down delta\n\t                    flag = 2\n\t                    self.root_down_update(new_id, root_down_dict)\n\t                else:\n", "                    # leaf up update\n\t                    # print('leave up')\n\t                    flag = 1\n\t                    # print(self.tree_node[self.root_id].child_h)\n\t                    self.leaf_up_update(id_mapping, leaf_up_dict)\n\t                    # print(self.tree_node[self.root_id].child_h)\n\t                    # update root down leave nodes' children\n\t                    if root_down_delta != 0:\n\t                        for root_down_id, root_down_node in root_down_dict.items(\n\t                        ):\n", "                            if root_down_node.child_h == 0:\n\t                                root_down_node.children = self.tree_node[\n\t                                    root_down_id].children\n\t        count = 0\n\t        for _ in LayerFirst(self.tree_node, self.root_id):\n\t            count += 1\n\t        assert len(self.tree_node) == count\n\t    def get_community(self):\n\t        '''\n\t        éè¦åè¿è¡ç¼ç æ çæå»º build_coding_tree\n", "        k: kç»´ç»æçµ\n\t        è¿åtensorç±»å,åªè¿åç¬¬äºå±çç¤¾åºåå\n\t        '''\n\t        root_id = self.root_id\n\t        partition_id = self.tree_node[root_id].children\n\t        community = torch.zeros(len(self.adj_matrix[0]), dtype=torch.int32)\n\t        community_id = 0\n\t        for e in partition_id:\n\t            partition_node = torch.tensor(self.tree_node[e].partition)\n\t            community[partition_node] = community_id\n", "            community_id += 1\n\t        return community\n\t    def get_community_3(self):\n\t        '''\n\t        éè¦åè¿è¡ç¼ç æ çæå»º build_coding_tree\n\t        k: kç»´ç»æçµ\n\t        è¿åç¬¬ä¸å±çåå\n\t        '''\n\t        root_id = self.root_id\n\t        partition_id = self.tree_node[root_id].children\n", "        partition_2 = set()\n\t        for e in partition_id:\n\t            if self.tree_node[e].children == None:\n\t                partition_2.add(e)\n\t            else:\n\t                partition_2.symmetric_difference_update(\n\t                    self.tree_node[e].children)\n\t        partition_id = partition_2\n\t        community = torch.zeros(len(self.adj_matrix[0]), dtype=torch.int32)\n\t        community_id = 0\n", "        for e in partition_id:\n\t            partition_node = torch.tensor(self.tree_node[e].partition)\n\t            community[partition_node] = community_id\n\t            community_id += 1\n\t        return community\n\t    def deduct_se(self, leaf_id, root_id=None):\n\t        '''\n\t        éåè°ç¨build_code_treeæå»ºç¼ç æ !\n\t        root_id = Noneæ¶è¡¨ç¤ºæ ¹èç¹ä¸ºç¼ç æ æ ¹èç¹\n\t        '''\n", "        node_dict = self.tree_node\n\t        path_id = [leaf_id]\n\t        current_id = leaf_id\n\t        while True:\n\t            parent_id = node_dict[current_id].parent\n\t            if parent_id == root_id:\n\t                break\n\t            if node_dict[parent_id].partition == node_dict[\n\t                    current_id].partition:\n\t                current_id = parent_id\n", "                path_id[-1] = current_id\n\t                continue\n\t            path_id.append(parent_id)\n\t            current_id = parent_id\n\t        if root_id == None:\n\t            path_id = path_id[0:-1]  #å»é¤æ ¹èç¹\n\t        g = []\n\t        vol = []\n\t        parent_vol = []\n\t        for e in path_id:\n", "            g.append(node_dict[e].g)\n\t            vol.append(node_dict[e].vol)\n\t            parent_vol.append(node_dict[node_dict[e].parent].vol)\n\t        g = torch.tensor(g)\n\t        vol = torch.tensor(vol) + 1\n\t        parent_vol = torch.tensor(parent_vol) + 1\n\t        deduct_se = -(g / self.VOL * torch.log2(vol / parent_vol)).sum()\n\t        return deduct_se\n\t    def LCA(self):\n\t        node_dict = self.tree_node\n", "        root_id = self.root_id\n\t        tree_node_num = max(node_dict.keys()) + 1\n\t        first = torch.zeros(tree_node_num)\n\t        height = torch.zeros(tree_node_num)\n\t        visited = torch.zeros(tree_node_num)\n\t        euler = []\n\t        stack = [root_id]\n\t        h = 0\n\t        while stack:\n\t            node_id = stack.pop()\n", "            euler.append(node_id)\n\t            if visited[node_id]:\n\t                h -= 1\n\t                continue\n\t            visited[node_id] = True\n\t            height[node_id] = h\n\t            h += 1\n\t            first[node_id] = len(euler) - 1\n\t            child = node_dict[node_id].children\n\t            if child is None:\n", "                continue\n\t            child = list(child)\n\t            if len(child) == 1 and node_dict[\n\t                    child[0]].partition == node_dict[node_id].partition:\n\t                child = child[0]\n\t                while True:\n\t                    first[child] = first[node_id]\n\t                    child = node_dict[child].children\n\t                    if child is None:\n\t                        break\n", "                    child = list(child)[0]\n\t                continue\n\t            for e in child[::-1]:\n\t                stack.append(node_id)\n\t                stack.append(e)\n\t            # stack.append(child[0])\n\t        self.first = first\n\t        self.height = height\n\t        self.euler = euler\n\t    def query_LCA(self, id1, id2):\n", "        tmp = torch.tensor([self.first[id1], self.first[id2]])\n\t        begin, end = tmp.min(), tmp.max()\n\t        interval = self.euler[begin.int():end.int() + 1]\n\t        height_interval = self.height[interval]\n\t        return interval[height_interval.argmin().int()]\n\tdef get_community(code_tree: PartitionTree):\n\t    node_dict = code_tree.tree_node\n\t    root_id = code_tree.root_id\n\t    tree_node_num = max(node_dict.keys()) + 1\n\t    isleaf = torch.zeros(tree_node_num, dtype=torch.bool)\n", "    stack = [root_id]\n\t    while stack:\n\t        node_id = stack.pop()\n\t        child = node_dict[node_id].children\n\t        if child is None:\n\t            isleaf[node_id] = True\n\t            continue\n\t        child = list(child)\n\t        for e in child[::-1]:\n\t            stack.append(e)\n", "        # stack.append(child[0])\n\t    community = []\n\t    for current_id in range(tree_node_num):\n\t        if isleaf[current_id]:\n\t            while True:\n\t                parent_id = node_dict[current_id].parent\n\t                if node_dict[parent_id].partition == node_dict[\n\t                        current_id].partition:\n\t                    isleaf[parent_id] = True\n\t                    current_id = parent_id\n", "                break\n\t    for e in node_dict.keys():\n\t        if not isleaf[e]:\n\t            community.append(e)\n\t    return community, isleaf\n\tdef get_sedict(community: list, code_tree: PartitionTree):\n\t    node_dict = code_tree.tree_node\n\t    se_dict = {}\n\t    for community_id in community:\n\t        node_list = list(node_dict[community_id].children)\n", "        se = torch.zeros(len(node_list))\n\t        for i, e in enumerate(node_list):\n\t            e = node_dict[e]\n\t            e: PartitionTreeNode\n\t            se[i] = -(e.g / code_tree.VOL) * torch.log2(\n\t                torch.tensor(\n\t                    (e.vol + 1) /\n\t                    (node_dict[e.parent].vol + 1))) + code_tree.deduct_se(\n\t                        community_id, None)\n\t            # se[i] = -(e.g / code_tree.VOL) * torch.log2(\n", "            #     torch.tensor((e.vol + 1) / (node_dict[e.parent].vol + 1)))\n\t        se = torch.softmax(se.float(), dim=0)\n\t        se_dict[community_id] = se\n\t    return se_dict\n\tdef select_link(community_id: int, code_tree: PartitionTree,\n\t                isleaf: torch.Tensor, se_dict):\n\t    node_dict = code_tree.tree_node\n\t    node_list = list(node_dict[community_id].children)\n\t    node_dict = code_tree.tree_node\n\t    se = se_dict[community_id]\n", "    id1, id2 = torch.multinomial(se, num_samples=2, replacement=True)\n\t    link_id1 = node_list[id1]\n\t    link_id2 = node_list[id2]\n\t    link_id = [link_id1, link_id2]\n\t    return link_id\n\tdef select_leaf(node_id, code_tree: PartitionTree, isleaf: torch.Tensor,\n\t                se_dict):\n\t    # print(node_id)\n\t    node_dict = code_tree.tree_node\n\t    while not isleaf[node_id]:\n", "        node_list = list(node_dict[node_id].children)\n\t        if len(node_list) > 1:  #é¿ååªæä¸ä¸ªå­èç¹çéå¶å­èç¹\n\t            se = se_dict[node_id]\n\t            # print(se)\n\t            id = torch.multinomial(se, num_samples=1, replacement=False)\n\t            node_id = node_list[id]\n\t        node_id = node_list[0]\n\t    return (node_dict[node_id].partition)[0]\n\tdef reshape(community: list, code_tree: PartitionTree, isleaf: torch.Tensor,\n\t            k):\n", "    se_dict = {}\n\t    edge_index = []\n\t    node_dict = code_tree.tree_node\n\t    # for k, v in code_tree.tree_node.items():\n\t    #     print(k, v.__dict__)\n\t    se_dict = get_sedict(community, code_tree)\n\t    for community_id in community:\n\t        node_list = list(node_dict[community_id].children)\n\t        if len(node_list) == 1:\n\t            continue\n", "        prefer_edge_num = round(k * len(node_list))\n\t        for i in range(prefer_edge_num):\n\t            id1, id2 = select_link(community_id, code_tree, isleaf, se_dict)\n\t            edge_index.append([\n\t                select_leaf(id1, code_tree, isleaf, se_dict),\n\t                select_leaf(id2, code_tree, isleaf, se_dict)\n\t            ])\n\t    edge_index = torch.tensor(edge_index)\n\t    edge_index = torch.cat((edge_index, torch.flip(edge_index, dims=[1])),\n\t                              dim=0)\n", "    edge_index = torch.unique(edge_index, dim=0)\n\t    return edge_index.t()\n"]}
{"filename": "opengsl/method/models/stable.py", "chunked_list": ["import torch.nn as nn\n\timport torch\n\timport scipy.sparse as sp\n\timport numpy as np\n\timport copy\n\timport random\n\tfrom sklearn.metrics.pairwise import cosine_similarity\n\tdef preprocess_adj(features, adj, threshold=0.03, jaccard=True):\n\t    \"\"\"Drop dissimilar edges.(Faster version using numba)\n\t    \"\"\"\n", "    if not sp.issparse(adj):\n\t        adj = sp.csr_matrix(adj)\n\t    adj_triu = sp.triu(adj, format='csr')\n\t    if sp.issparse(features):\n\t        features = features.todense().A  # make it easier for njit processing\n\t    if jaccard:\n\t        removed_cnt = dropedge_jaccard(adj_triu.data, adj_triu.indptr, adj_triu.indices, features,\n\t                                       threshold=threshold)\n\t    else:\n\t        removed_cnt = dropedge_cosine(adj_triu.data, adj_triu.indptr, adj_triu.indices, features,\n", "                                      threshold=threshold)\n\t    print('removed %s edges in the original graph' % removed_cnt)\n\t    modified_adj = adj_triu + adj_triu.transpose()\n\t    return modified_adj\n\tdef dropedge_jaccard(A, iA, jA, features, threshold):\n\t    removed_cnt = 0\n\t    for row in range(len(iA)-1):\n\t        for i in range(iA[row], iA[row+1]):\n\t            # print(row, jA[i], A[i])\n\t            n1 = row\n", "            n2 = jA[i]\n\t            a, b = features[n1], features[n2]\n\t            intersection = np.count_nonzero(a*b)\n\t            J = intersection * 1.0 / (np.count_nonzero(a) + np.count_nonzero(b) - intersection)\n\t            if J < threshold:\n\t                A[i] = 0\n\t                # A[n2, n1] = 0\n\t                removed_cnt += 1\n\t    return removed_cnt\n\tdef dropedge_cosine(A, iA, jA, features, threshold):\n", "    removed_cnt = 0\n\t    for row in range(len(iA)-1):\n\t        for i in range(iA[row], iA[row+1]):\n\t            # print(row, jA[i], A[i])\n\t            n1 = row\n\t            n2 = jA[i]\n\t            a, b = features[n1], features[n2]\n\t            inner_product = (a * b).sum()\n\t            C = inner_product / (np.sqrt(np.square(a).sum()) * np.sqrt(np.square(b).sum()) + 1e-8)\n\t            if C <= threshold:\n", "                A[i] = 0\n\t                # A[n2, n1] = 0\n\t                removed_cnt += 1\n\t    return removed_cnt\n\tdef aug_random_edge(input_adj, adj_delete, recover_percent=0.2):\n\t    percent = recover_percent\n\t    adj_delete = sp.tril(adj_delete)\n\t    row_idx, col_idx = adj_delete.nonzero()\n\t    edge_num = int(len(row_idx))\n\t    add_edge_num = int(edge_num * percent)\n", "    print(\"the number of recovering edges: {:04d}\" .format(add_edge_num))\n\t    aug_adj = copy.deepcopy(input_adj.todense().tolist())\n\t    edge_list = [(i, j) for i, j in zip(row_idx, col_idx)]\n\t    edge_idx = [i for i in range(edge_num)]\n\t    add_idx = random.sample(edge_idx, add_edge_num)\n\t    for i in add_idx:\n\t        aug_adj[edge_list[i][0]][edge_list[i][1]] = 1\n\t        aug_adj[edge_list[i][1]][edge_list[i][0]] = 1\n\t    aug_adj = np.matrix(aug_adj)\n\t    aug_adj = sp.csr_matrix(aug_adj)\n", "    return aug_adj\n\tdef get_reliable_neighbors(adj, features, k, degree_threshold):\n\t    degree = adj.sum(dim=1)\n\t    degree_mask = degree > degree_threshold\n\t    assert degree_mask.sum().item() >= k\n\t    sim = cosine_similarity(features.to('cpu'))\n\t    sim = torch.FloatTensor(sim).to('cuda')\n\t    sim[:, degree_mask == False] = 0\n\t    _, top_k_indices = sim.topk(k=k, dim=1)\n\t    for i in range(adj.shape[0]):\n", "        adj[i][top_k_indices[i]] = 1\n\t        adj[i][i] = 0\n\t    return\n\tclass DGI(nn.Module):\n\t    def __init__(self, n_in, n_h, activation):\n\t        super(DGI, self).__init__()\n\t        self.gcn = GCN_DGI(n_in, n_h, activation)\n\t        self.read = AvgReadout()\n\t        self.sigm = nn.Sigmoid()\n\t        self.disc = Discriminator(n_h)\n", "    # (features, shuf_fts, aug_features1, aug_features2,\n\t    #  sp_adj if sparse else adj,\n\t    #  sp_aug_adj1 if sparse else aug_adj1,\n\t    #  sp_aug_adj2 if sparse else aug_adj2,\n\t    #  sparse, None, None, None, aug_type=aug_type\n\t    def forward(self, seq1, seq2, adj, aug_adj1, aug_adj2):\n\t        h_0 = self.gcn(seq1, adj)\n\t        h_1 = self.gcn(seq1, aug_adj1)\n\t        h_3 = self.gcn(seq1, aug_adj2)\n\t        c_1 = self.read(h_1)\n", "        c_1 = self.sigm(c_1)\n\t        c_3 = self.read(h_3)\n\t        c_3 = self.sigm(c_3)\n\t        h_2 = self.gcn(seq2, adj)\n\t        ret1 = self.disc(c_1, h_0, h_2)\n\t        ret2 = self.disc(c_3, h_0, h_2)\n\t        ret = ret1 + ret2   # è¿éå®éä¸ä¸ç¬¦åå¬å¼\n\t        return ret\n\t    # Detach the return variables\n\t    def embed(self, seq, adj):\n", "        h_1 = self.gcn(seq, adj)\n\t        # c = self.read(h_1)\n\t        return h_1.detach()\n\tclass GCN_DGI(nn.Module):\n\t    def __init__(self, in_ft, out_ft, act, bias=True):\n\t        super(GCN_DGI, self).__init__()\n\t        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n\t        self.act = nn.PReLU() if act == 'prelu' else act\n\t        if bias:\n\t            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n", "            self.bias.data.fill_(0.0)\n\t        else:\n\t            self.register_parameter('bias', None)\n\t        for m in self.modules():\n\t            self.weights_init(m)\n\t    def weights_init(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            torch.nn.init.xavier_uniform_(m.weight.data)\n\t            if m.bias is not None:\n\t                m.bias.data.fill_(0.0)\n", "    # Shape of seq: (batch, nodes, features)\n\t    def forward(self, seq, adj):\n\t        seq_fts = self.fc(seq)\n\t        out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n\t        if self.bias is not None:\n\t            out += self.bias\n\t        return self.act(out)\n\tclass AvgReadout(nn.Module):\n\t    def __init__(self):\n\t        super(AvgReadout, self).__init__()\n", "    def forward(self, seq):\n\t        return torch.mean(seq, 1)\n\tclass Discriminator(nn.Module):\n\t    def __init__(self, n_h):\n\t        super(Discriminator, self).__init__()\n\t        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\t        for m in self.modules():\n\t            self.weights_init(m)\n\t    def weights_init(self, m):\n\t        if isinstance(m, nn.Bilinear):\n", "            torch.nn.init.xavier_uniform_(m.weight.data)\n\t            if m.bias is not None:\n\t                m.bias.data.fill_(0.0)\n\t    def forward(self, c, h_pl, h_mi):\n\t        # å¤æ­ä¸ä¸ªviewååadjãæ°å¨adjçå³ç³» [1, 2*n_nodes]\n\t        c_x = torch.unsqueeze(c, 1)\n\t        c_x = c_x.expand_as(h_pl)\n\t        tmp = self.f_k(h_pl, c_x)\n\t        sc_1 = torch.squeeze(tmp, 2)\n\t        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n", "        logits = torch.cat((sc_1, sc_2), 1)\n\t        return logits"]}
{"filename": "opengsl/method/models/jknet.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.nn import init\n\tclass GraphConvolution(nn.Module):\n\t    def __init__(self, in_features, out_features, dropout=0.5, n_linear=1, bias=True, spmm_type=1, act='relu'):\n\t        super(GraphConvolution, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.mlp = nn.ModuleList()\n", "        self.mlp.append(nn.Linear(in_features, out_features, bias=bias))\n\t        for i in range(n_linear-1):\n\t            self.mlp.append(nn.Linear(out_features, out_features, bias=bias))\n\t        self.dropout = dropout\n\t        self.spmm = [torch.spmm, torch.sparse.mm][spmm_type]\n\t        self.act = eval('F.'+act) if not act=='identity' else lambda x: x\n\t    def forward(self, input, adj):\n\t        \"\"\" Graph Convolutional Layer forward function\n\t        \"\"\"\n\t        x = self.spmm(adj, input)\n", "        for i in range(len(self.mlp)-1):\n\t            x = self.mlp[i](x)\n\t            x = self.act(x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.mlp[-1](x)\n\t        x = self.act(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        return x\n\t    def __repr__(self):\n\t        return self.__class__.__name__ + ' (' \\\n", "               + str(self.in_features) + ' -> ' \\\n\t               + str(self.out_features) + ')'\n\tclass JKNet(nn.Module):\n\t    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, input_dropout=0.0, norm=None, n_linear=1, spmm_type=0, act='relu', general='concat', input_layer=True, output_layer=True):\n\t        super(JKNet, self).__init__()\n\t        self.nfeat = nfeat\n\t        self.nclass = nclass\n\t        self.n_layers = n_layers\n\t        self.input_layer = input_layer\n\t        self.output_layer = output_layer\n", "        self.n_linear = n_linear\n\t        self.norm_flag = norm['flag']\n\t        self.norm_type = eval(\"nn.\"+norm['norm_type'])\n\t        self.general_aggregation = eval('self.'+general)\n\t        self.act = eval('F.'+act) if not act == 'identity' else lambda x: x\n\t        if input_layer:\n\t            self.input_linear = nn.Linear(in_features=nfeat, out_features=nhid)\n\t            self.input_drop = nn.Dropout(input_dropout)\n\t        if output_layer:\n\t            self.output_linear = nn.Linear(in_features=nhid * n_layers, out_features=nclass)\n", "            self.output_normalization = self.norm_type(nhid)\n\t        self.convs = nn.ModuleList()\n\t        if self.norm_flag:\n\t            self.norms = nn.ModuleList()\n\t        else:\n\t            self.norms = None\n\t        for i in range(n_layers):\n\t            if i == 0 and not self.input_layer:\n\t                in_hidden = nfeat\n\t            else:\n", "                in_hidden = nhid\n\t            out_hidden = nhid\n\t            self.convs.append(GraphConvolution(in_hidden, out_hidden, dropout, n_linear, spmm_type=spmm_type, act=act))\n\t            if self.norm_flag:\n\t                self.norms.append(self.norm_type(in_hidden))\n\t        if (general == 'concat'):\n\t            self.last_layer = nn.Linear(nhid * n_layers, nclass)\n\t        else :\n\t            self.last_layer = nn.Linear(nhid, nclass) \n\t        if (general == 'LSMT'):\n", "            self.lstm = nn.LSTM(nhid, (n_layers * nhid) // 2, bidirectional=True, batch_first=True)\n\t            self.attn = nn.Linear(2 * ((n_layers * nhid) // 2), 1)\n\t    def forward(self, input):\n\t        x=input[0]\n\t        adj=input[1]\n\t        only_z=input[2]\n\t        layer_outputs = []\n\t        if self.input_layer:\n\t            x = self.input_linear(x)\n\t            x = self.input_drop(x)\n", "            x = self.act(x)\n\t        for i, layer in enumerate(self.convs):\n\t            if self.norm_flag:\n\t                x_res = self.norms[i](x)\n\t                x_res = layer(x_res, adj)\n\t                x = x + x_res\n\t            else:\n\t                x = layer(x,adj)\n\t            layer_outputs.append(x)\n\t        mid = self.general_aggregation(layer_outputs)\n", "        if self.output_layer:\n\t            x = self.output_normalization(x)\n\t        x = self.last_layer(mid).squeeze(1)\n\t        if only_z:\n\t            return x\n\t        else:\n\t            return mid, x\n\t    def concat(self, layer_outputs):\n\t        return torch.cat(layer_outputs, dim=1)\n\t    def maxpool(self, layer_outputs):\n", "        return torch.max(torch.stack(layer_outputs, dim=0), dim=0)[0]\n\t    def LSMT(self, layer_outputs):\n\t        x = torch.stack(layer_outputs, dim=1)\n\t        alpha, _ = self.lstm(x)\n\t        alpha = self.attn(alpha).squeeze(-1)\n\t        alpha = torch.softmax(alpha, dim=-1).unsqueeze(-1)\n\t        return (x * alpha).sum(dim=1)\n"]}
{"filename": "opengsl/method/models/cogsl.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\t#from module.view_estimator import View_Estimator\n\t#from module.cls import Classification\n\t#from module.mi_nce import MI_NCE\n\t#from module.fusion import Fusion\n\tfrom torch_geometric.nn import GCNConv\n\tfrom torch_sparse import SparseTensor\n\t#from .gcn import GraphConvolution\n", "import numpy as np\n\tclass GCN_two_pyg(nn.Module):\n\t    def __init__(self, input_dim, hid_dim1, hid_dim2, dropout=0., activation=\"relu\"):\n\t        super(GCN_two_pyg, self).__init__()\n\t        self.conv1 = GCNConv(input_dim, hid_dim1)\n\t        self.conv2 = GCNConv(hid_dim1, hid_dim2)\n\t        self.dropout = dropout\n\t        assert activation in [\"relu\", \"leaky_relu\", \"elu\"]\n\t        self.activation = getattr(F, activation)\n\t    def forward(self, feature, adj):\n", "        adj = SparseTensor.from_torch_sparse_coo_tensor(adj)\n\t        x1 = self.activation(self.conv1(feature, adj))\n\t        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n\t        x2 = self.conv2(x1, adj)\n\t        return x2\n\tclass GCN_one_pyg(nn.Module):\n\t    def __init__(self, in_ft, out_ft, bias=True, activation=None):\n\t        super(GCN_one_pyg, self).__init__()\n\t        self.conv1 = GCNConv(in_ft, out_ft)\n\t        self.activation = activation\n", "        if bias:\n\t            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n\t            self.bias.data.fill_(0.0)\n\t        else:\n\t            self.register_parameter('bias', None)\n\t    def forward(self, feat, adj):\n\t        # print(adj)\n\t        # exit(0)\n\t        adj = SparseTensor.from_torch_sparse_coo_tensor(adj)\n\t        out = self.conv1(feat, adj)\n", "        if self.bias is not None:\n\t            out += self.bias\n\t        if self.activation is not None:\n\t            out = self.activation(out)\n\t        return out\n\tclass GCN_two(nn.Module):\n\t    def __init__(self, input_dim, hid_dim1, hid_dim2, dropout=0., activation=\"relu\"):\n\t        super(GCN_two, self).__init__()\n\t        self.conv1 = GCN_one(input_dim, hid_dim1)\n\t        self.conv2 = GCN_one(hid_dim1, hid_dim2)\n", "        self.dropout = dropout\n\t        assert activation in [\"relu\", \"leaky_relu\", \"elu\"]\n\t        self.activation = getattr(F, activation)\n\t    def forward(self, feature, adj):\n\t        x1 = self.activation(self.conv1(feature, adj))\n\t        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n\t        x2 = self.conv2(x1, adj)\n\t        return x2  # F.log_softmax(x2, dim=1)\n\tclass GCN_one(nn.Module):\n\t    def __init__(self, in_ft, out_ft, bias=True, activation=None):\n", "        super(GCN_one, self).__init__()\n\t        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n\t        self.activation = activation\n\t        if bias:\n\t            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n\t            self.bias.data.fill_(0.0)\n\t        else:\n\t            self.register_parameter('bias', None)\n\t        for m in self.modules():\n\t            self.weights_init(m)\n", "    def weights_init(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            torch.nn.init.xavier_uniform_(m.weight.data)\n\t            if m.bias is not None:\n\t                m.bias.data.fill_(0.0)\n\t    def forward(self, feat, adj):\n\t        adj = adj.to_dense()\n\t        feat = self.fc(feat)\n\t        out = torch.spmm(adj, feat)\n\t        if self.bias is not None:\n", "            out += self.bias\n\t        if self.activation is not None:\n\t            out = self.activation(out)\n\t        return out\n\t#class two_layer_GCN(nn.Module):\n\t#    def __init__(self, num_feature, cls_hid_1, num_class, dropout = 0.5, act = 'relu'):\n\t#        super(two_layer_GCN, self).__init__()\n\t#        self.layer1 = GraphConvolution(num_feature, cls_hid_1, dropout, act= act)\n\t#        self.layer2 = GraphConvolution(cls_hid_1, num_class, dropout=0, last_layer=True)\n\t#\n", "#    def forward(self, feature, adj):\n\t#        x = self.layer1(feature, adj)\n\t#        x = self.layer2(x,adj)\n\t#        return x\n\tclass Classification(nn.Module):\n\t    def __init__(self, num_feature, cls_hid_1, num_class, dropout, pyg):\n\t        super(Classification, self).__init__()\n\t        if pyg==False:\n\t            self.encoder_v1 = GCN_two(num_feature, cls_hid_1, num_class, dropout)\n\t            self.encoder_v2 = GCN_two(num_feature, cls_hid_1, num_class, dropout)\n", "            self.encoder_v = GCN_two(num_feature, cls_hid_1, num_class, dropout)\n\t        else:\n\t            #print(\"pyg\")\n\t            self.encoder_v1 = GCN_two_pyg(num_feature, cls_hid_1, num_class, dropout)\n\t            self.encoder_v2 = GCN_two_pyg(num_feature, cls_hid_1, num_class, dropout)\n\t            self.encoder_v = GCN_two_pyg(num_feature, cls_hid_1, num_class, dropout)\n\t        #self.encoder_v = two_layer_GCN(num_feature, cls_hid_1, num_class, dropout)\n\t        #self.encoder_v1 = two_layer_GCN(num_feature, cls_hid_1, num_class, dropout)\n\t        #self.encoder_v2 = two_layer_GCN(num_feature, cls_hid_1, num_class, dropout)\n\t    def forward(self, feat, view, flag):\n", "        if flag == \"v1\":\n\t            prob = F.softmax(self.encoder_v1(feat, view), dim=1)\n\t        elif flag == \"v2\":\n\t            prob = F.softmax(self.encoder_v2(feat, view), dim=1)\n\t        elif flag == \"v\":\n\t            prob = F.softmax(self.encoder_v(feat, view), dim=1)\n\t        return prob\n\tclass Contrast:\n\t    def __init__(self, tau):\n\t        self.tau = tau\n", "    def sim(self, z1, z2):\n\t        z1_norm = torch.norm(z1, dim=-1, keepdim=True)\n\t        z2_norm = torch.norm(z2, dim=-1, keepdim=True)\n\t        dot_numerator = torch.mm(z1, z2.t())\n\t        dot_denominator = torch.mm(z1_norm, z2_norm.t())\n\t        sim_matrix = torch.exp(dot_numerator / dot_denominator / self.tau)\n\t        return sim_matrix\n\t    def cal(self, z1_proj, z2_proj):\n\t        matrix_z1z2 = self.sim(z1_proj, z2_proj)\n\t        matrix_z2z1 = matrix_z1z2.t()\n", "        matrix_z1z2 = matrix_z1z2 / (torch.sum(matrix_z1z2, dim=1).view(-1, 1) + 1e-8)\n\t        lori_v1v2 = -torch.log(matrix_z1z2.diag()+1e-8).mean()\n\t        matrix_z2z1 = matrix_z2z1 / (torch.sum(matrix_z2z1, dim=1).view(-1, 1) + 1e-8)\n\t        lori_v2v1 = -torch.log(matrix_z2z1.diag()+1e-8).mean()\n\t        return (lori_v1v2 + lori_v2v1) / 2\n\tclass Fusion(nn.Module):\n\t    def __init__(self, lam, alpha, name):\n\t        super(Fusion, self).__init__()\n\t        self.lam = lam\n\t        self.alpha = alpha\n", "        self.name = name\n\t    def get_weight(self, prob):\n\t        if len(prob.shape)==1:\n\t            prob = torch.stack([prob, 1 - prob],dim=1)\n\t        out, _ = prob.topk(2, dim=1, largest=True, sorted=True)\n\t        fir = out[:, 0]\n\t        sec = out[:, 1]\n\t        w = torch.exp(self.alpha*(self.lam*torch.log(fir+1e-8) + (1-self.lam)*torch.log(fir-sec+1e-8)))\n\t        return w\n\t    def forward(self, v1, prob_v1, v2, prob_v2):\n", "        w_v1 = self.get_weight(prob_v1)\n\t        w_v2 = self.get_weight(prob_v2)\n\t        beta_v1 = w_v1 / (w_v1 + w_v2)\n\t        beta_v2 = w_v2 / (w_v1 + w_v2)\n\t        if self.name not in [\"citeseer\", \"digits\", \"polblogs\", \"cora\"]:\n\t            beta_v1 = beta_v1.diag().to_sparse()\n\t            beta_v2 = beta_v2.diag().to_sparse()\n\t            v = torch.sparse.mm(beta_v1, v1) + torch.sparse.mm(beta_v2, v2)\n\t            return v\n\t        else :\n", "            beta_v1 = beta_v1.unsqueeze(1)\n\t            beta_v2 = beta_v2.unsqueeze(1)\n\t            v = beta_v1 * v1.to_dense() + beta_v2 * v2.to_dense()\n\t            return v.to_sparse()\n\tclass GenView(nn.Module):\n\t    def __init__(self, num_feature, hid, com_lambda, dropout, pyg):\n\t        super(GenView, self).__init__()\n\t        if pyg == False:\n\t            self.gen_gcn = GCN_one(num_feature, hid, activation=nn.ReLU())\n\t        else:\n", "            self.gen_gcn = GCN_one_pyg(num_feature, hid, activation=nn.ReLU())  \n\t        #self.gen_gcn = GraphConvolution(num_feature, hid, dropout=0)\n\t        self.gen_mlp = nn.Linear(2 * hid, 1)\n\t        nn.init.xavier_normal_(self.gen_mlp.weight, gain=1.414)\n\t        self.relu = nn.ReLU()\n\t        self.softmax = nn.Softmax(dim=1)\n\t        self.com_lambda = com_lambda\n\t        self.dropout = nn.Dropout(dropout)\n\t    def forward(self, v_ori, feat, v_indices, num_node):\n\t        emb = self.gen_gcn(feat, v_ori)\n", "        f1 = emb[v_indices[0]]\n\t        f2 = emb[v_indices[1]]\n\t        ff = torch.cat([f1, f2], dim=-1)\n\t        temp = self.gen_mlp(self.dropout(ff)).reshape(-1)\n\t        z_matrix = torch.sparse.FloatTensor(v_indices, temp, (num_node, num_node))\n\t        pi = torch.sparse.softmax(z_matrix, dim=1)\n\t        gen_v = v_ori + self.com_lambda * pi \n\t        return gen_v\n\tclass View_Estimator(nn.Module):\n\t    def __init__(self, num_feature, gen_hid, com_lambda_v1, com_lambda_v2, dropout, pyg, big):\n", "        super(View_Estimator, self).__init__()\n\t        self.v1_gen = GenView(num_feature, gen_hid, com_lambda_v1, dropout, pyg)\n\t        self.v2_gen = GenView(num_feature, gen_hid, com_lambda_v2, dropout, pyg)\n\t        if big:\n\t            self.normalize = self.normalize1\n\t        else:\n\t            self.normalize = self.normalize2\n\t    def normalize1(self, adj):\n\t        return (adj + adj.t())\n\t    def normalize2(self, mx):\n", "        mx = mx + mx.t() + torch.eye(mx.shape[0]).to(mx.device).to_sparse()\n\t        mx = mx.to_dense()\n\t        rowsum = mx.sum(1) + 1e-6  # avoid NaN\n\t        r_inv = rowsum.pow(-1 / 2).flatten()\n\t        r_inv[torch.isinf(r_inv)] = 0.\n\t        r_mat_inv = torch.diag(r_inv)\n\t        mx = r_mat_inv @ mx\n\t        mx = mx @ r_mat_inv\n\t        return mx.to_sparse()\n\t    def forward(self, view1, view1_indices, view2, view2_indices, num_nodes, feats):\n", "        new_v1 = self.normalize(self.v1_gen(view1, feats, view1_indices, num_nodes))\n\t        new_v2 = self.normalize(self.v2_gen(view2, feats, view2_indices, num_nodes))\n\t        return new_v1, new_v2\n\tclass MI_NCE(nn.Module):\n\t    def __init__(self, num_feature, mi_hid_1, tau, pyg, big, batch):\n\t        super(MI_NCE, self).__init__()\n\t        if pyg == False:\n\t            self.gcn = GCN_one(num_feature, mi_hid_1, activation=nn.PReLU())\n\t            self.gcn1 = GCN_one(num_feature, mi_hid_1, activation=nn.PReLU())\n\t            self.gcn2 = GCN_one(num_feature, mi_hid_1, activation=nn.PReLU())\n", "        else:\n\t            #print(\"pyg\")\n\t            self.gcn = GCN_one_pyg(num_feature, mi_hid_1, activation=nn.PReLU())\n\t            self.gcn1 = GCN_one_pyg(num_feature, mi_hid_1, activation=nn.PReLU())\n\t            self.gcn2 = GCN_one_pyg(num_feature, mi_hid_1, activation=nn.PReLU())\n\t        #self.gcn = GraphConvolution(num_feature, mi_hid_1, act='relu', dropout=0)\n\t        #self.gcn1 = GraphConvolution(num_feature, mi_hid_1, act='relu', dropout=0)\n\t        #self.gcn2 = GraphConvolution(num_feature, mi_hid_1, act='relu', dropout=0)\n\t        self.proj = nn.Sequential(\n\t            nn.Linear(mi_hid_1, mi_hid_1),\n", "            nn.ELU(),\n\t            nn.Linear(mi_hid_1, mi_hid_1)\n\t        )\n\t        self.con = Contrast(tau)\n\t        self.big = big\n\t        self.batch = batch\n\t    def forward(self, views, feat):\n\t        v_emb = self.proj(self.gcn(feat, views[0]))\n\t        v1_emb = self.proj(self.gcn1(feat, views[1]))\n\t        v2_emb = self.proj(self.gcn2(feat, views[2]))\n", "        # if dataset is so big, we will randomly sample part of nodes to perform MI estimation\n\t        if self.big == True:\n\t            idx = np.random.choice(feat.shape[0], self.batch, replace=False)\n\t            idx.sort()\n\t            v_emb = v_emb[idx]\n\t            v1_emb = v1_emb[idx]\n\t            v2_emb = v2_emb[idx]\n\t        vv1 = self.con.cal(v_emb, v1_emb)\n\t        vv2 = self.con.cal(v_emb, v2_emb)\n\t        v1v2 = self.con.cal(v1_emb, v2_emb)\n", "        return vv1, vv2, v1v2\n\tclass CoGSL(nn.Module):\n\t    def __init__(self, num_feature, cls_hid_1, num_class, gen_hid, mi_hid_1,\n\t                 com_lambda_v1, com_lambda_v2, lam, alpha, cls_dropout, ve_dropout, tau, pyg, big, batch, name):\n\t        super(CoGSL, self).__init__()\n\t        self.cls = Classification(num_feature, cls_hid_1, num_class, cls_dropout, pyg)\n\t        self.ve = View_Estimator(num_feature, gen_hid, com_lambda_v1, com_lambda_v2, ve_dropout, pyg, big)\n\t        self.mi = MI_NCE(num_feature, mi_hid_1, tau, pyg, big, batch)\n\t        self.fusion = Fusion(lam, alpha, name)\n\t    def get_view(self, view1, view1_indices, view2, view2_indices, num_nodes, feats):\n", "        new_v1, new_v2 = self.ve(view1, view1_indices, view2, view2_indices, num_nodes, feats)\n\t        return new_v1, new_v2\n\t    def get_mi_loss(self, feat, views):\n\t        mi_loss = self.mi(views, feat)\n\t        return mi_loss\n\t    def get_cls_loss(self, v1, v2, feat):\n\t        prob_v1 = self.cls(feat, v1, \"v1\")\n\t        prob_v2 = self.cls(feat, v2, \"v2\")\n\t        logits_v1 = torch.log(prob_v1 + 1e-8)\n\t        logits_v2 = torch.log(prob_v2 + 1e-8)\n", "        return logits_v1.squeeze(1), logits_v2.squeeze(1), prob_v1.squeeze(1), prob_v2.squeeze(1)\n\t    def get_v_cls_loss(self, v, feat):\n\t        logits = torch.log(self.cls(feat, v, \"v\") + 1e-8)\n\t        return logits.squeeze(1)\n\t    def get_fusion(self, v1, prob_v1, v2, prob_v2):\n\t        v = self.fusion(v1, prob_v1, v2, prob_v2)\n\t        return v"]}
{"filename": "opengsl/method/models/sublime.py", "chunked_list": ["import dgl\n\timport torch\n\timport torch.nn as nn\n\tfrom sklearn.neighbors import kneighbors_graph\n\tfrom .gcn import GCN\n\tfrom .gnn_modules import APPNP\n\timport dgl.function as fn\n\timport numpy as np\n\timport torch.nn.functional as F\n\timport copy\n", "EOS = 1e-10\n\tdef get_feat_mask(features, mask_rate):\n\t    feat_node = features.shape[1]\n\t    mask = torch.zeros(features.shape)\n\t    samples = np.random.choice(feat_node, size=int(feat_node * mask_rate), replace=False)\n\t    mask[:, samples] = 1\n\t    return mask.cuda(), samples\n\tdef split_batch(init_list, batch_size):\n\t    groups = zip(*(iter(init_list),) * batch_size)\n\t    end_list = [list(i) for i in groups]\n", "    count = len(init_list) % batch_size\n\t    end_list.append(init_list[-count:]) if count != 0 else end_list\n\t    return end_list\n\tdef dgl_graph_to_torch_sparse(dgl_graph):\n\t    values = dgl_graph.edata['w'].cpu().detach()\n\t    rows_, cols_ = dgl_graph.edges()\n\t    indices = torch.cat((torch.unsqueeze(rows_, 0), torch.unsqueeze(cols_, 0)), 0).cpu()\n\t    torch_sparse_mx = torch.sparse.FloatTensor(indices, values)\n\t    return torch_sparse_mx\n\tdef torch_sparse_to_dgl_graph(torch_sparse_mx):\n", "    torch_sparse_mx = torch_sparse_mx.coalesce()\n\t    indices = torch_sparse_mx.indices()\n\t    values = torch_sparse_mx.values()\n\t    rows_, cols_ = indices[0,:], indices[1,:]\n\t    dgl_graph = dgl.graph((rows_, cols_), num_nodes=torch_sparse_mx.shape[0], device='cuda')\n\t    dgl_graph.edata['w'] = values.detach().cuda()\n\t    return dgl_graph\n\tdef nearest_neighbors_pre_elu(X, k, metric, i):\n\t    # è¿ä¸ªåå§åæç¹ä¸çè§£\n\t    adj = kneighbors_graph(X, k, metric=metric)\n", "    adj = np.array(adj.todense(), dtype=np.float32)\n\t    adj += np.eye(adj.shape[0])\n\t    adj = adj * i - i\n\t    return adj\n\tdef knn_fast(X, k, b):\n\t    X = F.normalize(X, dim=1, p=2)\n\t    index = 0\n\t    values = torch.zeros(X.shape[0] * (k + 1)).cuda()\n\t    rows = torch.zeros(X.shape[0] * (k + 1)).cuda()\n\t    cols = torch.zeros(X.shape[0] * (k + 1)).cuda()\n", "    norm_row = torch.zeros(X.shape[0]).cuda()\n\t    norm_col = torch.zeros(X.shape[0]).cuda()\n\t    while index < X.shape[0]:\n\t        if (index + b) > (X.shape[0]):\n\t            end = X.shape[0]\n\t        else:\n\t            end = index + b\n\t        sub_tensor = X[index:index + b]\n\t        similarities = torch.mm(sub_tensor, X.t())\n\t        vals, inds = similarities.topk(k=k + 1, dim=-1)\n", "        values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n\t        cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n\t        rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end).view(-1, 1).repeat(1, k + 1).view(-1)\n\t        norm_row[index: end] = torch.sum(vals, dim=1)\n\t        norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n\t        index += b\n\t    norm = norm_row + norm_col\n\t    rows = rows.long()\n\t    cols = cols.long()\n\t    values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n", "    return rows, cols, values\n\tdef apply_non_linearity(tensor, non_linearity, i):\n\t    if non_linearity == 'elu':\n\t        return F.elu(tensor * i - i) + 1\n\t    elif non_linearity == 'relu':\n\t        return F.relu(tensor)\n\t    elif non_linearity == 'none':\n\t        return tensor\n\t    else:\n\t        raise NameError('We dont support the non-linearity yet')\n", "def cal_similarity_graph(node_embeddings):\n\t    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n\t    return similarity_graph\n\tdef top_k(raw_graph, K):\n\t    values, indices = raw_graph.topk(k=int(K), dim=-1)\n\t    assert torch.max(indices) < raw_graph.shape[1]\n\t    mask = torch.zeros(raw_graph.shape).cuda()\n\t    mask[torch.arange(raw_graph.shape[0]).view(-1, 1), indices] = 1.\n\t    mask.requires_grad = False\n\t    sparse_graph = raw_graph * mask\n", "    return sparse_graph\n\tclass GCNConv_dgl(nn.Module):\n\t    def __init__(self, input_size, output_size):\n\t        super(GCNConv_dgl, self).__init__()\n\t        self.linear = nn.Linear(input_size, output_size)\n\t    def forward(self, x, g):\n\t        with g.local_scope():\n\t            g.ndata['h'] = self.linear(x)\n\t            g.update_all(fn.u_mul_e('h', 'w', 'm'), fn.sum(msg='m', out='h'))\n\t            return g.ndata['h']\n", "class Attentive(nn.Module):\n\t    def __init__(self, isize):\n\t        super(Attentive, self).__init__()\n\t        self.w = nn.Parameter(torch.ones(isize))\n\t    def forward(self, x):\n\t        return x @ torch.diag(self.w)\n\tclass FGP_learner(nn.Module):\n\t    def __init__(self, features, k, knn_metric, i, sparse):\n\t        super(FGP_learner, self).__init__()\n\t        self.k = k\n", "        self.knn_metric = knn_metric\n\t        self.i = i\n\t        self.sparse = sparse\n\t        self.Adj = nn.Parameter(\n\t            torch.from_numpy(nearest_neighbors_pre_elu(features, self.k, self.knn_metric, self.i)))\n\t    def forward(self, h):\n\t        if not self.sparse:\n\t            Adj = F.elu(self.Adj) + 1\n\t        else:\n\t            Adj = self.Adj.coalesce()\n", "            Adj.values = F.elu(Adj.values()) + 1\n\t        return Adj\n\tclass ATT_learner(nn.Module):\n\t    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, mlp_act):\n\t        super(ATT_learner, self).__init__()\n\t        self.i = i\n\t        self.layers = nn.ModuleList()\n\t        for _ in range(nlayers):\n\t            self.layers.append(Attentive(isize))\n\t        self.k = k\n", "        self.knn_metric = knn_metric\n\t        self.non_linearity = 'relu'\n\t        self.sparse = sparse\n\t        self.mlp_act = mlp_act\n\t    def internal_forward(self, h):\n\t        for i, layer in enumerate(self.layers):\n\t            h = layer(h)\n\t            if i != (len(self.layers) - 1):\n\t                if self.mlp_act == \"relu\":\n\t                    h = F.relu(h)\n", "                elif self.mlp_act == \"tanh\":\n\t                    h = F.tanh(h)\n\t        return h\n\t    def forward(self, features):\n\t        if self.sparse:\n\t            embeddings = self.internal_forward(features)\n\t            rows, cols, values = knn_fast(embeddings, self.k, 1000)\n\t            rows_ = torch.cat((rows, cols))\n\t            cols_ = torch.cat((cols, rows))\n\t            values_ = torch.cat((values, values))\n", "            values_ = apply_non_linearity(values_, self.non_linearity, self.i)\n\t            adj = dgl.graph((rows_, cols_), num_nodes=features.shape[0], device='cuda')\n\t            adj.edata['w'] = values_\n\t            return adj\n\t        else:\n\t            embeddings = self.internal_forward(features)\n\t            embeddings = F.normalize(embeddings, dim=1, p=2)\n\t            similarities = cal_similarity_graph(embeddings)\n\t            similarities = top_k(similarities, self.k + 1)\n\t            similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n", "            return similarities\n\tclass MLP_learner(nn.Module):\n\t    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, act):\n\t        super(MLP_learner, self).__init__()\n\t        self.layers = nn.ModuleList()\n\t        if nlayers == 1:\n\t            self.layers.append(nn.Linear(isize, isize))\n\t        else:\n\t            self.layers.append(nn.Linear(isize, isize))\n\t            for _ in range(nlayers - 2):\n", "                self.layers.append(nn.Linear(isize, isize))\n\t            self.layers.append(nn.Linear(isize, isize))\n\t        self.input_dim = isize\n\t        self.output_dim = isize\n\t        self.k = k\n\t        self.knn_metric = knn_metric\n\t        self.non_linearity = 'relu'\n\t        self.param_init()\n\t        self.i = i\n\t        self.sparse = sparse\n", "        self.act = act\n\t    def internal_forward(self, h):\n\t        for i, layer in enumerate(self.layers):\n\t            h = layer(h)\n\t            if i != (len(self.layers) - 1):\n\t                if self.act == \"relu\":\n\t                    h = F.relu(h)\n\t                elif self.act == \"tanh\":\n\t                    h = F.tanh(h)\n\t        return h\n", "    def param_init(self):\n\t        for layer in self.layers:\n\t            layer.weight = nn.Parameter(torch.eye(self.input_dim))\n\t    def forward(self, features):\n\t        if self.sparse:\n\t            embeddings = self.internal_forward(features)\n\t            rows, cols, values = knn_fast(embeddings, self.k, 1000)\n\t            rows_ = torch.cat((rows, cols))\n\t            cols_ = torch.cat((cols, rows))\n\t            values_ = torch.cat((values, values))\n", "            values_ = apply_non_linearity(values_, self.non_linearity, self.i)\n\t            adj = dgl.graph((rows_, cols_), num_nodes=features.shape[0], device='cuda')\n\t            adj.edata['w'] = values_\n\t            return adj\n\t        else:\n\t            embeddings = self.internal_forward(features)\n\t            embeddings = F.normalize(embeddings, dim=1, p=2)\n\t            similarities = cal_similarity_graph(embeddings)\n\t            similarities = top_k(similarities, self.k + 1)\n\t            similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n", "            return similarities\n\tclass GNN_learner(nn.Module):\n\t    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, mlp_act, adj):\n\t        super(GNN_learner, self).__init__()\n\t        self.adj = adj\n\t        self.layers = nn.ModuleList()\n\t        if nlayers == 1:\n\t            self.layers.append(GCNConv_dgl(isize, isize))\n\t        else:\n\t            self.layers.append(GCNConv_dgl(isize, isize))\n", "            for _ in range(nlayers - 2):\n\t                self.layers.append(GCNConv_dgl(isize, isize))\n\t            self.layers.append(GCNConv_dgl(isize, isize))\n\t        self.input_dim = isize\n\t        self.output_dim = isize\n\t        self.k = k\n\t        self.knn_metric = knn_metric\n\t        self.non_linearity = 'relu'\n\t        self.param_init()\n\t        self.i = i\n", "        self.sparse = sparse\n\t        self.mlp_act = mlp_act\n\t    def internal_forward(self, h):\n\t        for i, layer in enumerate(self.layers):\n\t            h = layer(h, self.adj)\n\t            if i != (len(self.layers) - 1):\n\t                if self.mlp_act == \"relu\":\n\t                    h = F.relu(h)\n\t                elif self.mlp_act == \"tanh\":\n\t                    h = F.tanh(h)\n", "        return h\n\t    def param_init(self):\n\t        for layer in self.layers:\n\t            layer.weight = nn.Parameter(torch.eye(self.input_dim))\n\t    def forward(self, features):\n\t        if self.sparse:\n\t            embeddings = self.internal_forward(features)\n\t            rows, cols, values = knn_fast(embeddings, self.k, 1000)\n\t            rows_ = torch.cat((rows, cols))\n\t            cols_ = torch.cat((cols, rows))\n", "            values_ = torch.cat((values, values))\n\t            values_ = apply_non_linearity(values_, self.non_linearity, self.i)\n\t            adj = dgl.graph((rows_, cols_), num_nodes=features.shape[0], device='cuda')\n\t            adj.edata['w'] = values_\n\t            return adj\n\t        else:\n\t            embeddings = self.internal_forward(features)\n\t            embeddings = F.normalize(embeddings, dim=1, p=2)\n\t            similarities = cal_similarity_graph(embeddings)\n\t            similarities = top_k(similarities, self.k + 1)\n", "            similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n\t            return similarities\n\tclass GraphEncoder(nn.Module):\n\t    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, sparse, conf=None):\n\t        super(GraphEncoder, self).__init__()\n\t        self.dropout = dropout\n\t        self.sparse = sparse\n\t        self.gnn_encoder_layers = nn.ModuleList()\n\t        if sparse:\n\t            self.gnn_encoder_layers.append(GCNConv_dgl(in_dim, hidden_dim))\n", "            for _ in range(nlayers - 2):\n\t                self.gnn_encoder_layers.append(GCNConv_dgl(hidden_dim, hidden_dim))\n\t            self.gnn_encoder_layers.append(GCNConv_dgl(hidden_dim, emb_dim))\n\t        else:\n\t            if conf.model['type']=='gcn':\n\t                self.model = GCN(nfeat=in_dim, nhid=hidden_dim, nclass=emb_dim, n_layers=nlayers, dropout=dropout,\n\t                                 input_layer=False, output_layer=False, spmm_type=0)\n\t            elif conf.model['type']=='appnp':\n\t                self.model = APPNP(in_dim, hidden_dim, emb_dim,\n\t                                    dropout=conf.model['dropout'], K=conf.model['K'],\n", "                                    alpha=conf.model['alpha'])\n\t        self.proj_head = nn.Sequential(nn.Linear(emb_dim, proj_dim), nn.ReLU(inplace=True),\n\t                                           nn.Linear(proj_dim, proj_dim))\n\t    def forward(self, x, Adj_):\n\t        if self.sparse:\n\t            for conv in self.gnn_encoder_layers[:-1]:\n\t                x = conv(x, Adj_)\n\t                x = F.relu(x)\n\t                x = F.dropout(x, p=self.dropout, training=self.training)\n\t            x = self.gnn_encoder_layers[-1](x, Adj_)\n", "        else:\n\t            x = self.model((x, Adj_, True))\n\t        z = self.proj_head(x)\n\t        return z, x\n\tclass GCL(nn.Module):\n\t    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj, sparse, conf=None):\n\t        super(GCL, self).__init__()\n\t        self.encoder = GraphEncoder(nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, sparse, conf)\n\t        self.dropout_adj = dropout_adj\n\t        self.sparse = sparse\n", "    def forward(self, x, Adj_, branch=None):\n\t        # edge dropping\n\t        if self.sparse:\n\t            if branch == 'anchor':\n\t                Adj = copy.deepcopy(Adj_)\n\t            else:\n\t                Adj = Adj_\n\t            Adj.edata['w'] = F.dropout(Adj.edata['w'], p=self.dropout_adj, training=self.training)\n\t        else:\n\t            Adj = F.dropout(Adj_, p=self.dropout_adj, training=self.training)\n", "        # get representations\n\t        z, embedding = self.encoder(x, Adj)\n\t        return z, embedding\n\t    @staticmethod\n\t    def calc_loss(x, x_aug, temperature=0.2):\n\t        batch_size, _ = x.size()\n\t        x_abs = x.norm(dim=1)\n\t        x_aug_abs = x_aug.norm(dim=1)\n\t        sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / torch.einsum('i,j->ij', x_abs, x_aug_abs)   # è®¡ç®çæ¯cosç¸ä¼¼åº¦\n\t        sim_matrix = torch.exp(sim_matrix / temperature)\n", "        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n\t        loss_0 = pos_sim / (sim_matrix.sum(dim=0) - pos_sim)\n\t        loss_1 = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n\t        loss_0 = - torch.log(loss_0).mean()\n\t        loss_1 = - torch.log(loss_1).mean()\n\t        loss = (loss_0 + loss_1) / 2.0\n\t        return loss\n\tclass GCN_SUB(nn.Module):\n\t    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, dropout_adj=0.5, sparse=0):\n\t        super(GCN_SUB, self).__init__()\n", "        self.layers = nn.ModuleList()\n\t        self.sparse = sparse\n\t        self.dropout_adj_p = dropout_adj\n\t        self.dropout = dropout\n\t        if sparse:\n\t            self.layers.append(GCNConv_dgl(nfeat, nhid))\n\t            for _ in range(n_layers - 2):\n\t                self.layers.append(GCNConv_dgl(nhid, nhid))\n\t            self.layers.append(GCNConv_dgl(nhid, nclass))\n\t        else:\n", "            self.model = GCN(nfeat=nfeat, nhid=nhid, nclass=nclass, n_layers=n_layers, dropout=dropout,\n\t                             input_layer=False, output_layer=False, spmm_type=0)\n\t    def forward(self, x, Adj):\n\t        if self.sparse:\n\t            Adj = copy.deepcopy(Adj)\n\t            Adj.edata['w'] = F.dropout(Adj.edata['w'], p=self.dropout_adj_p, training=self.training)\n\t        else:\n\t            Adj = F.dropout(Adj, p=self.dropout_adj_p, training=self.training)\n\t        if self.sparse:\n\t            for i, conv in enumerate(self.layers[:-1]):\n", "                x = conv(x, Adj)\n\t                x = F.relu(x)\n\t                x = F.dropout(x, p=self.dropout, training=self.training)\n\t            x = self.layers[-1](x, Adj)\n\t            return x.squeeze(1)\n\t        else:\n\t            return self.model((x, Adj, True))\n"]}
{"filename": "opengsl/method/models/grcn.py", "chunked_list": ["import torch\n\timport torch.nn.functional as F\n\t# from .GCN3 import GraphConvolution, GCN\n\tfrom .gcn import GCN\n\tfrom .gnn_modules import APPNP\n\tclass GCNConv_diag(torch.nn.Module):\n\t    '''\n\t    A GCN convolution layer of diagonal matrix multiplication\n\t    '''\n\t    def __init__(self, input_size):\n", "        super(GCNConv_diag, self).__init__()\n\t        self.W = torch.nn.Parameter(torch.ones(input_size))\n\t        # inds = torch.stack([torch.arange(input_size), torch.arange(input_size)]).to(device)\n\t        # self.mW = torch.sparse.FloatTensor(inds, self.W, torch.Size([input_size,input_size]))\n\t        self.input_size = input_size\n\t    def forward(self, input, A):\n\t        hidden = input @ torch.diag(self.W)\n\t        # hidden = torch.sparse.mm(self.mW, input.t()).t()\n\t        output = torch.sparse.mm(A, hidden)\n\t        return output\n", "class GRCN(torch.nn.Module):\n\t    def __init__(self, num_nodes, num_features, num_classes, device, conf):\n\t        super(GRCN, self).__init__()\n\t        self.num_nodes = num_nodes\n\t        self.num_features = num_features\n\t        if conf.model['type'] == 'gcn':\n\t            self.conv_task = GCN(num_features, conf.model['n_hidden'], num_classes, conf.model['n_layers'],\n\t                                 conf.model['dropout'], conf.model['input_dropout'], conf.model['norm'],\n\t                                 conf.model['n_linear'], conf.model['spmm_type'], conf.model['act'],\n\t                                 conf.model['input_layer'], conf.model['output_layer'])\n", "        else:\n\t            self.conv_task = APPNP(num_features, conf.model['n_hidden'], num_classes,\n\t                               dropout=conf.model['dropout'], K=conf.model['K_APPNP'],\n\t                               alpha=conf.model['alpha'], spmm_type=1)\n\t        self.model_type = conf.gsl['model_type']\n\t        if conf.gsl['model_type'] == 'diag':\n\t            self.conv_graph = GCNConv_diag(num_features)\n\t            self.conv_graph2 = GCNConv_diag(num_features)\n\t        else:\n\t            self.conv_graph = GCN(num_features, conf.gsl['n_hidden_1'], conf.gsl['n_hidden_2'], conf.gsl['n_layers'],\n", "                             conf.gsl['dropout'], conf.gsl['input_dropout'], conf.gsl['norm'],\n\t                             conf.gsl['n_linear'], conf.gsl['spmm_type'], conf.gsl['act'],\n\t                             conf.gsl['input_layer'], conf.gsl['output_layer'])\n\t        self.K = conf.gsl['K']\n\t        self.mask = None\n\t        self.Adj_new = None\n\t        self._normalize = conf.gsl['normalize']   # ç¨æ¥å³å®æ¯å¦å¯¹node embeddingè¿è¡normalize\n\t        self.device = device\n\t    def graph_parameters(self):\n\t        if self.model_type == 'diag':\n", "            return list(self.conv_graph.parameters()) + list(self.conv_graph2.parameters())\n\t        else:\n\t            return list(self.conv_graph.parameters())\n\t    def base_parameters(self):\n\t        return list(self.conv_task.parameters())\n\t    def cal_similarity_graph(self, node_embeddings):\n\t        # ä¸ä¸ª2headçç¸ä¼¼åº¦è®¡ç®\n\t        # similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n\t        similarity_graph = torch.mm(node_embeddings[:, :int(self.num_features/2)], node_embeddings[:, :int(self.num_features/2)].t())\n\t        similarity_graph += torch.mm(node_embeddings[:, int(self.num_features/2):], node_embeddings[:, int(self.num_features/2):].t())\n", "        return similarity_graph\n\t    def normalize(self, adj):\n\t        adj = adj.coalesce()\n\t        inv_sqrt_degree = 1. / (torch.sqrt(torch.sparse.sum(adj, dim=1).values()) + 1e-10)\n\t        D_value = inv_sqrt_degree[adj.indices()[0]] * inv_sqrt_degree[adj.indices()[1]]\n\t        new_values = adj.values() * D_value\n\t        return torch.sparse.FloatTensor(adj.indices(), new_values, adj.size()).to(self.device)\n\t    def _sparse_graph(self, raw_graph, K):\n\t        values, indices = raw_graph.topk(k=int(K), dim=-1)\n\t        assert torch.sum(torch.isnan(values)) == 0\n", "        assert torch.max(indices) < raw_graph.shape[1]\n\t        inds = torch.stack([torch.arange(raw_graph.shape[0]).view(-1,1).expand(-1,int(K)).contiguous().view(1,-1)[0].to(self.device),\n\t                             indices.view(1,-1)[0]])\n\t        inds = torch.cat([inds, torch.stack([inds[1], inds[0]])], dim=1)\n\t        values = torch.cat([values.view(1,-1)[0], values.view(1,-1)[0]])\n\t        return inds, values\n\t    def _node_embeddings(self, input, Adj):\n\t        norm_Adj = self.normalize(Adj)\n\t        if self.model_type == 'diag':\n\t            node_embeddings = torch.tanh(self.conv_graph(input, norm_Adj))\n", "            node_embeddings = self.conv_graph2(node_embeddings, norm_Adj)\n\t        else:\n\t            node_embeddings = self.conv_graph((input, norm_Adj, True))\n\t        if self._normalize:\n\t            node_embeddings = F.normalize(node_embeddings, dim=1, p=2)\n\t        return node_embeddings\n\t    def forward(self, input, Adj):\n\t        Adj.requires_grad = False\n\t        node_embeddings = self._node_embeddings(input, Adj)\n\t        Adj_new = self.cal_similarity_graph(node_embeddings)\n", "        Adj_new_indices, Adj_new_values = self._sparse_graph(Adj_new, self.K)\n\t        new_inds = torch.cat([Adj.indices(), Adj_new_indices], dim=1)\n\t        new_values = torch.cat([Adj.values(), Adj_new_values])\n\t        Adj_new = torch.sparse.FloatTensor(new_inds, new_values, Adj.size()).to(self.device)\n\t        Adj_new_norm = self.normalize(Adj_new)\n\t        # x = self.conv1(input, Adj_new_norm)\n\t        # x = F.dropout(F.relu(x), training=self.training, p=self.dropout)\n\t        # x = self.conv2(x, Adj_new_norm)\n\t        _, x = self.conv_task((input, Adj_new_norm, False))\n\t        return x, Adj_new\n"]}
{"filename": "opengsl/method/models/gt.py", "chunked_list": ["'''\n\tThis is the GT model from UniMP [https://arxiv.org/pdf/2009.03509.pdf]\n\t'''\n\timport numpy as np\n\timport torch\n\tfrom torch import nn\n\tfrom dgl import ops\n\tfrom dgl.nn.functional import edge_softmax\n\timport torch.nn.functional as F\n\timport scipy.sparse as sp\n", "from ...utils.utils import get_homophily\n\tfrom opengsl.utils.utils import scipy_sparse_to_sparse_tensor\n\tclass TransformerAttentionModule(nn.Module):\n\t    def __init__(self, dim, dim_out, num_heads, dropout):\n\t        super().__init__()\n\t        assert dim % num_heads == 0, 'Dimension mismatch: hidden_dim should be a multiple of num_heads.'\n\t        self.dim = dim\n\t        self.num_heads = num_heads\n\t        self.head_dim = dim // num_heads\n\t        self.attn_query = nn.Linear(in_features=dim, out_features=dim)\n", "        self.attn_key = nn.Linear(in_features=dim, out_features=dim)\n\t        self.attn_value = nn.Linear(in_features=dim, out_features=dim)\n\t        self.output_linear = nn.Linear(in_features=dim, out_features=dim_out)\n\t        self.dropout = nn.Dropout(p=dropout)\n\t    def forward(self, x, graph, labels=None, graph_analysis=False):\n\t        queries = self.attn_query(x)\n\t        keys = self.attn_key(x)\n\t        values = self.attn_value(x)\n\t        queries = queries.reshape(-1, self.num_heads, self.head_dim)\n\t        keys = keys.reshape(-1, self.num_heads, self.head_dim)\n", "        values = values.reshape(-1, self.num_heads, self.head_dim)\n\t        attn_scores = ops.u_dot_v(graph, queries, keys) / self.head_dim ** 0.5\n\t        attn_probs = edge_softmax(graph, attn_scores)\n\t        x = ops.u_mul_e_sum(graph, values, attn_probs)\n\t        x = x.reshape(-1, self.dim)\n\t        x = self.output_linear(x)\n\t        x = self.dropout(x)\n\t        if graph_analysis:\n\t            assert labels is not None, 'error'\n\t            homophily = self.compute_homo(graph, attn_probs, labels)\n", "            return x, homophily\n\t        return x, 0\n\t    def compute_homo(self, graph, attn_weights, labels):\n\t        '''\n\t        Args:\n\t            graph: dgl graph\n\t            attn_weights: [n_edges, n_heads, 1], attention weights learned by a transformer layer\n\t        Returns:\n\t            homophily: [n_heads] the homophily of attention weights of each head\n\t        '''\n", "        n_heads = self.num_heads\n\t        n_nodes = graph.num_nodes()\n\t        homophily = np.zeros(n_heads)\n\t        edges = graph.edges()\n\t        row = edges[0].cpu().numpy()\n\t        col = edges[1].cpu().numpy()\n\t        # values = adj.coalesce().values().numpy()\n\t        # return sp.coo_matrix((values, (row, col)), shape=adj.shape)\n\t        for i in range(n_heads):\n\t            values = attn_weights.squeeze()[:, i].cpu().detach().numpy()\n", "            adj = sp.coo_matrix((values, (row, col)), shape=(n_nodes, n_nodes))\n\t            adj = scipy_sparse_to_sparse_tensor(adj)\n\t            homophily[i] = get_homophily(labels, adj)\n\t        return homophily\n\tclass FeedForwardModule(nn.Module):\n\t    def __init__(self, dim, hidden_dim_multiplier, dropout, act):\n\t        super().__init__()\n\t        input_dim = int(dim)\n\t        hidden_dim = int(dim * hidden_dim_multiplier)\n\t        self.linear_1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n", "        self.dropout_1 = nn.Dropout(p=dropout)\n\t        self.act = eval('F.' + act) if not act == 'identity' else lambda x: x\n\t        self.linear_2 = nn.Linear(in_features=hidden_dim, out_features=dim)\n\t        self.dropout_2 = nn.Dropout(p=dropout)\n\t    def forward(self, x):\n\t        x = self.linear_1(x)\n\t        x = self.dropout_1(x)\n\t        x = self.act(x)\n\t        x = self.linear_2(x)\n\t        x = self.dropout_2(x)\n", "        return x\n\tclass GT(nn.Module):\n\t    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, input_dropout=0.0, norm_type='LayerNorm',\n\t                 num_heads=8, act='relu', input_layer=False, output_layer=False, ff=False, hidden_dim_multiplier=2,\n\t                 use_norm=False, use_redisual=False):\n\t        super(GT, self).__init__()\n\t        self.nfeat = nfeat\n\t        self.nclass = nclass\n\t        self.n_layers = n_layers\n\t        self.input_layer = input_layer\n", "        self.output_layer = output_layer\n\t        self.use_norm = use_norm\n\t        self.use_residual = use_redisual\n\t        self.ff = ff\n\t        self.norm_type = eval('nn.' + norm_type)\n\t        self.act = eval('F.' + act) if not act == 'identity' else lambda x: x\n\t        if input_layer:\n\t            self.input_linear = nn.Linear(in_features=nfeat, out_features=nhid)\n\t            self.input_drop = nn.Dropout(input_dropout)\n\t        if output_layer:\n", "            self.output_linear = nn.Linear(in_features=nhid, out_features=nclass)\n\t            self.output_normalization = self.norm_type(nhid)\n\t        self.trans = nn.ModuleList()\n\t        if self.use_norm:\n\t            self.norms_1 = nn.ModuleList()\n\t        if self.ff:\n\t            self.ffns = nn.ModuleList()\n\t            if self.use_norm:\n\t                self.norms_2 = nn.ModuleList()\n\t        for i in range(n_layers):\n", "            if i == 0 and not self.input_layer:\n\t                in_hidden = nfeat\n\t            else:\n\t                in_hidden = nhid\n\t            if i == n_layers - 1 and not self.output_layer:\n\t                out_hidden = nclass\n\t            else:\n\t                out_hidden = nhid\n\t            self.trans.append(TransformerAttentionModule(in_hidden, out_hidden, num_heads, dropout))\n\t            if self.use_norm:\n", "                self.norms_1.append(self.norm_type(in_hidden))\n\t            if self.ff:\n\t                self.ffns.append(FeedForwardModule(in_hidden, hidden_dim_multiplier, dropout, act))\n\t                if self.use_norm:\n\t                    self.norms_2.append(self.norm_type(in_hidden))\n\t    def forward(self, x, graph, labels=None, graph_analysis=False):\n\t        if self.input_layer:\n\t            x = self.input_linear(x)\n\t            x = self.input_drop(x)\n\t            x = self.act(x)\n", "        homo_heads = []\n\t        for i, layer in enumerate(self.trans):\n\t            x_res = self.norms_1[i](x) if self.use_norm else x\n\t            x_res, homophily = layer(x_res, graph, labels, graph_analysis)\n\t            x = x + x_res if self.use_residual else x_res\n\t            if self.ff:\n\t                x_res = self.norms_2[i](x) if self.use_norm else x\n\t                x_res = self.ffns[i](x_res)\n\t                x = x + x_res if self.use_residual else x_res\n\t            if i == self.n_layers - 1:\n", "                mid = x\n\t            if graph_analysis:\n\t                homo_heads.append(homophily)\n\t        if self.output_layer:\n\t            if self.use_norm:\n\t                x = self.output_normalization(x)\n\t            x = self.output_linear(x)\n\t        return mid, x.squeeze(1), homo_heads\n\tclass GraphTransformerAttn(nn.Module):\n\t    def __init__(self, dim, dim_out, num_heads, concat=True):\n", "        super().__init__()\n\t        self.dim = dim\n\t        self.dim_out = dim_out\n\t        self.dim_inner = dim_out * num_heads\n\t        self.num_heads = num_heads\n\t        self.concat = concat\n\t        self.attn_query = nn.Linear(in_features=dim, out_features=self.dim_inner)\n\t        self.attn_key = nn.Linear(in_features=dim, out_features=self.dim_inner)\n\t        self.attn_value = nn.Linear(in_features=dim, out_features=self.dim_inner)\n\t    def forward(self, x, graph, labels=None, graph_analysis=False):\n", "        queries = self.attn_query(x)\n\t        keys = self.attn_key(x)\n\t        values = self.attn_value(x)\n\t        queries = queries.reshape(-1, self.num_heads, self.dim_out)\n\t        keys = keys.reshape(-1, self.num_heads, self.dim_out)\n\t        values = values.reshape(-1, self.num_heads, self.dim_out)\n\t        attn_scores = ops.u_dot_v(graph, queries, keys) / self.dim_out ** 0.5\n\t        attn_probs = edge_softmax(graph, attn_scores)\n\t        x = ops.u_mul_e_sum(graph, values, attn_probs)\n\t        if self.concat:\n", "            x = x.reshape(-1, self.dim_inner)\n\t        else:\n\t            x = torch.mean(x, dim=1)\n\t        if graph_analysis:\n\t            assert labels is not None, 'error'\n\t            homophily = self.compute_homo(graph, attn_probs, labels)\n\t            return x, homophily\n\t        return x\n\t    def compute_homo(self, graph, attn_weights, labels):\n\t        '''\n", "        Args:\n\t            graph: dgl graph\n\t            attn_weights: [n_edges, n_heads, 1], attention weights learned by a transformer layer\n\t        Returns:\n\t            homophily: [n_heads] the homophily of attention weights of each head\n\t        '''\n\t        n_heads = self.num_heads\n\t        n_nodes = graph.num_nodes()\n\t        homophily = np.zeros(n_heads)\n\t        edges = graph.edges()\n", "        row = edges[0].cpu().numpy()\n\t        col = edges[1].cpu().numpy()\n\t        # values = adj.coalesce().values().numpy()\n\t        # return sp.coo_matrix((values, (row, col)), shape=adj.shape)\n\t        for i in range(n_heads):\n\t            values = attn_weights.squeeze()[:, i].cpu().detach().numpy()\n\t            adj = sp.coo_matrix((values, (row, col)), shape=(n_nodes, n_nodes))\n\t            adj = scipy_sparse_to_sparse_tensor(adj)\n\t            homophily[i] = get_homophily(labels, adj)\n\t        return homophily\n", "class GatedResidual(nn.Module):\n\t    \"\"\" This is the implementation of Eq (5), i.e., gated residual connection between block.\n\t    \"\"\"\n\t    def __init__(self, dim_in, dim_out, only_gate=False):\n\t        super().__init__()\n\t        self.lin_res = nn.Linear(dim_in, dim_out)\n\t        self.proj = nn.Sequential(\n\t            nn.Linear(dim_out * 3, 1, bias = False),\n\t            nn.Sigmoid()\n\t        )\n", "        self.norm = nn.LayerNorm(dim_out)\n\t        self.non_lin = nn.ReLU()\n\t        self.only_gate = only_gate\n\t    def forward(self, x, res):\n\t        res = self.lin_res(res)\n\t        gate_input = torch.cat((x, res, x - res), dim = -1)\n\t        gate = self.proj(gate_input) # Eq (5), this is beta in the paper\n\t        if self.only_gate: # This is for Eq (6), a case when normalizaton and non linearity is not used.\n\t            return x * gate + res * (1 - gate)\n\t        return self.non_lin(self.norm(x * gate + res * (1 - gate)))\n", "class GraphTransformerModel(nn.Module):\n\t    \"\"\" This is the overall architecture of the model.\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n\t            n_feats,\n\t            n_class,\n\t            n_hidden,\n\t            n_layers,\n\t            n_heads=8,\n", "    ):\n\t        super().__init__()\n\t        self.n_feats = n_feats\n\t        self.n_class = n_class\n\t        self.n_hidden = n_hidden\n\t        self.n_layers = n_layers\n\t        self.n_heads = n_heads\n\t        self.layers = nn.ModuleList()\n\t        self.input_layer = nn.Linear(n_feats, n_hidden)\n\t        assert n_hidden % n_heads == 0\n", "        for i in range(n_layers):\n\t            if i < n_layers - 1:\n\t                self.layers.append(nn.ModuleList([\n\t                    GraphTransformerAttn(n_hidden, dim_out=int(n_hidden / n_heads), num_heads=n_heads),\n\t                    GatedResidual(n_hidden, n_hidden)\n\t                ]))\n\t            else:\n\t                self.layers.append(nn.ModuleList([\n\t                    GraphTransformerAttn(n_hidden, dim_out=n_class, num_heads=n_heads, concat=False),\n\t                    GatedResidual(n_hidden, n_class, only_gate=True)\n", "                ]))\n\t    def forward(self, input):\n\t        x=input[0]\n\t        graph=input[1]\n\t        x = self.input_layer(x)\n\t        for trans_block in self.layers:\n\t            trans, trans_residual = trans_block\n\t            x = trans_residual(trans(x, graph), x)\n\t        return x\n\tif __name__ == '__main__':\n", "    model = GT(1000, 128, 7, 5, 0.5, 0)\n\t    print(model)"]}
{"filename": "opengsl/method/models/gcn.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch_geometric.nn.dense.linear import Linear\n\tclass GraphConvolution(nn.Module):\n\t    def __init__(self, in_features, out_features, dropout=0.5, n_linear=1, bias=True, spmm_type=1, act='relu',\n\t                 last_layer=False, weight_initializer=None, bias_initializer=None):\n\t        super(GraphConvolution, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n", "        self.mlp = nn.ModuleList()\n\t        self.mlp.append(Linear(in_features, out_features, bias=bias, weight_initializer=weight_initializer, bias_initializer=bias_initializer))\n\t        for i in range(n_linear-1):\n\t            self.mlp.append(Linear(out_features, out_features, bias=bias, weight_initializer=weight_initializer, bias_initializer=bias_initializer))\n\t        self.dropout = dropout\n\t        self.spmm = [torch.spmm, torch.sparse.mm][spmm_type]\n\t        self.act = eval('F.'+act) if not act == 'identity' else lambda x: x\n\t        self.last_layer = last_layer\n\t    def forward(self, input, adj):\n\t        \"\"\" Graph Convolutional Layer forward function\n", "        \"\"\"\n\t        x = self.spmm(adj, input)\n\t        for i in range(len(self.mlp)-1):\n\t            x = self.mlp[i](x)\n\t            x = self.act(x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.mlp[-1](x)\n\t        if not self.last_layer:\n\t            x = self.act(x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n", "        return x\n\t    def __repr__(self):\n\t        return self.__class__.__name__ + ' (' \\\n\t               + str(self.in_features) + ' -> ' \\\n\t               + str(self.out_features) + ')'\n\tclass GCN(nn.Module):\n\t    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, input_dropout=0.0, norm=None, n_linear=1,\n\t                 spmm_type=0, act='relu', input_layer=False, output_layer=False, weight_initializer=None,\n\t                 bias_initializer=None, bias=True):\n\t        super(GCN, self).__init__()\n", "        self.nfeat = nfeat\n\t        self.nclass = nclass\n\t        self.n_layers = n_layers\n\t        self.input_layer = input_layer\n\t        self.output_layer = output_layer\n\t        self.n_linear = n_linear\n\t        if norm is None:\n\t            norm = {'flag':False, 'norm_type':'LayerNorm'}\n\t        self.norm_flag = norm['flag']\n\t        self.norm_type = eval('nn.'+norm['norm_type'])\n", "        self.act = eval('F.'+act) if not act == 'identity' else lambda x: x\n\t        if input_layer:\n\t            self.input_linear = nn.Linear(in_features=nfeat, out_features=nhid)\n\t            self.input_drop = nn.Dropout(input_dropout)\n\t        if output_layer:\n\t            self.output_linear = nn.Linear(in_features=nhid, out_features=nclass)\n\t            self.output_normalization = self.norm_type(nhid)\n\t        self.convs = nn.ModuleList()\n\t        if self.norm_flag:\n\t            self.norms = nn.ModuleList()\n", "        else:\n\t            self.norms = None\n\t        for i in range(n_layers):\n\t            if i == 0 and not self.input_layer:\n\t                in_hidden = nfeat\n\t            else:\n\t                in_hidden = nhid\n\t            if i == n_layers - 1 and not self.output_layer:\n\t                out_hidden = nclass\n\t            else:\n", "                out_hidden = nhid\n\t            self.convs.append(GraphConvolution(in_hidden, out_hidden, dropout, n_linear, spmm_type=spmm_type, act=act,\n\t                                               weight_initializer=weight_initializer, bias_initializer=bias_initializer,\n\t                                               bias=bias))\n\t            if self.norm_flag:\n\t                self.norms.append(self.norm_type(in_hidden))\n\t        self.convs[-1].last_layer = True\n\t    def forward(self, input):\n\t        x=input[0]\n\t        adj=input[1]\n", "        only_z=input[2]\n\t        if self.input_layer:\n\t            x = self.input_linear(x)\n\t            x = self.input_drop(x)\n\t            x = self.act(x)\n\t        for i, layer in enumerate(self.convs):\n\t            if self.norm_flag:\n\t                x_res = self.norms[i](x)\n\t                x_res = layer(x_res, adj)\n\t                x = x + x_res\n", "            else:\n\t                x = layer(x,adj)\n\t            if i == self.n_layers - 1:\n\t                mid = x\n\t        if self.output_layer:\n\t            x = self.output_normalization(x)\n\t            x = self.output_linear(x).squeeze(1)\n\t        if only_z:\n\t            return x.squeeze(1)\n\t        else:\n", "            return mid, x.squeeze(1)"]}
{"filename": "opengsl/method/models/__init__.py", "chunked_list": []}
{"filename": "opengsl/method/models/gcn_idgl.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport math\n\t'''\n\tThis GCN is only used for IGDL for its changeable dropout.\n\t'''\n\tclass GraphConvolution(nn.Module):\n\t    def __init__(self, in_features, out_features, with_bias=True, batch_norm=False):\n\t        super(GraphConvolution, self).__init__()\n", "        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n\t        if with_bias:\n\t            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n\t        else:\n\t            self.register_parameter('bias', None)\n\t        self.bn = nn.BatchNorm1d(out_features) if batch_norm else None\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n", "        stdv = 1. / math.sqrt(self.weight.size(1))\n\t        self.weight.data.uniform_(-stdv, stdv)\n\t        if self.bias is not None:\n\t            self.bias.data.uniform_(-stdv, stdv)\n\t    def init_params(self):\n\t        # initialize weights with xavier uniform and biases with all zeros.\n\t        # This is more recommended than the upper one.\n\t        for param in self.parameters():\n\t            if len(param.size()) == 2:\n\t                nn.init.xavier_uniform_(param)\n", "            else:\n\t                nn.init.constant_(param, 0.0)\n\t    def forward(self, input, adj, batch_norm=True):\n\t        \"\"\" Graph Convolutional Layer forward function\n\t        \"\"\"\n\t        if input.data.is_sparse:\n\t            support = torch.spmm(input, self.weight)\n\t        else:\n\t            support = torch.mm(input, self.weight)\n\t        output = torch.spmm(adj, support)\n", "        if self.bias is not None:\n\t            output = output + self.bias\n\t        if self.bn is not None and batch_norm:\n\t            output = self.compute_bn(output)\n\t        return output\n\t    def compute_bn(self, x):\n\t        if len(x.shape) == 2:\n\t            return self.bn(x)\n\t        else:\n\t            return self.bn(x.view(-1, x.size(-1))).view(x.size())\n", "    def __repr__(self):\n\t        return self.__class__.__name__ + ' (' \\\n\t               + str(self.in_features) + ' -> ' \\\n\t               + str(self.out_features) + ')'\n\tclass GCN(nn.Module):\n\t    def __init__(self, nfeat, nhid, nclass, n_layers=2, dropout=0.5, with_bias=True, batch_norm=False):\n\t        super(GCN, self).__init__()\n\t        self.nfeat = nfeat\n\t        self.hidden_sizes = [nhid]\n\t        self.nclass = nclass\n", "        self.n_layers = n_layers\n\t        self.layers = nn.ModuleList()\n\t        self.layers.append(GraphConvolution(nfeat, nhid, with_bias=with_bias, batch_norm=batch_norm))\n\t        for i in range(n_layers-2):\n\t            self.layers.append(GraphConvolution(nhid, nhid, with_bias=with_bias, batch_norm=batch_norm))\n\t        self.layers.append(GraphConvolution(nhid, nclass, with_bias=with_bias, batch_norm=False))\n\t        self.dropout = dropout\n\t        self.with_bias = with_bias\n\t        self.batch_norm = batch_norm\n\t    def forward(self, x, adj, dropout=None):\n", "        for i, layer in enumerate(self.layers[:-1]):\n\t            x = F.relu(layer(x, adj))\n\t            x = F.dropout(x, dropout if dropout else self.dropout, training=self.training)\n\t        output = self.layers[-1](x, adj)\n\t        return x, output\n\t    def initialize(self):\n\t        \"\"\"Initialize parameters of GCN.\n\t        \"\"\"\n\t        for i in range(self.n_layers):\n\t            self.layers[i].reset_parameters()"]}
{"filename": "opengsl/method/models/gaug.py", "chunked_list": ["from .gcn import GCN\n\tfrom .gnn_modules import APPNP\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport pyro as pyro\n\tfrom opengsl.data.preprocess.normalize import normalize\n\timport numpy as np\n\tfrom sklearn.metrics import roc_auc_score, average_precision_score\n\tclass VGAE(nn.Module):\n", "    \"\"\" GAE/VGAE as edge prediction model \"\"\"\n\t    def __init__(self, dim_feats, conf):\n\t        super(VGAE, self).__init__()\n\t        self.gae = conf.gsl['gae']\n\t        # self.gcn_base = GraphConvolution(dim_feats, dim_h, with_bias=False)\n\t        # self.gcn_mean = GraphConvolution(dim_h, dim_z, with_bias=False)\n\t        # self.gcn_logstd = GraphConvolution(dim_h, dim_z, with_bias=False)\n\t        self.conv_graph = GCN(dim_feats, conf.gsl['n_hidden'], conf.gsl['n_embed'], conf.gsl['n_layers'],\n\t                              conf.gsl['dropout'], conf.gsl['input_dropout'], conf.gsl['norm'],\n\t                              conf.gsl['n_linear'], conf.gsl['spmm_type'], conf.gsl['act'],\n", "                              conf.gsl['input_layer'], conf.gsl['output_layer'], bias=False,\n\t                              weight_initializer='glorot')\n\t    def forward(self, feats, adj):\n\t        # GCN encoder\n\t        # hidden = self.gcn_base(feats, adj)\n\t        # self.mean = F.relu(self.gcn_mean(hidden, adj))\n\t        _, mean = self.conv_graph((feats, adj, False))\n\t        mean = F.relu(mean)\n\t        if self.gae:\n\t            # GAE (no sampling at bottleneck)\n", "            Z = mean\n\t        else:\n\t            # VGAE\n\t            # self.logstd = F.relu(self.gcn_logstd(hidden, adj))\n\t            # gaussian_noise = torch.randn_like(self.mean)\n\t            # sampled_Z = gaussian_noise*torch.exp(self.logstd) + self.mean\n\t            # Z = sampled_Z\n\t            pass\n\t        # inner product decoder\n\t        adj_logits = Z @ Z.T\n", "        return adj_logits\n\tclass GAug(nn.Module):\n\t    def __init__(self,\n\t                 dim_feats,\n\t                 n_classes,\n\t                 conf):\n\t        super(GAug, self).__init__()\n\t        self.temperature = conf.gsl['temperature']\n\t        self.alpha = conf.gsl['alpha']\n\t        # edge prediction network\n", "        self.ep_net = VGAE(dim_feats, conf)\n\t        # node classification network\n\t        # self.nc_net = GCN(dim_feats, dim_h, n_classes, dropout=dropout)\n\t        if conf.model['type']=='gcn':\n\t            self.nc_net = GCN(dim_feats, conf.model['n_hidden'], n_classes, conf.model['n_layers'], conf.model['dropout'],\n\t                              conf.model['input_dropout'], conf.model['norm'], conf.model['n_linear'],\n\t                              conf.model['spmm_type'], conf.model['act'], conf.model['input_layer'],\n\t                              conf.model['output_layer'], weight_initializer='glorot', bias_initializer='zeros')\n\t        elif conf.model['type']=='appnp':\n\t            self.nc_net = APPNP(dim_feats, conf.model['n_hidden'], n_classes,\n", "                               dropout=conf.model['dropout'], K=conf.model['K'],\n\t                               alpha=conf.model['alpha'])\n\t    def sample_adj(self, adj_logits):\n\t        \"\"\" sample an adj from the predicted edge probabilities of ep_net \"\"\"\n\t        edge_probs = adj_logits / torch.max(adj_logits)\n\t        # sampling\n\t        # print(adj_logits)\n\t        # print(edge_probs)\n\t        adj_sampled = pyro.distributions.RelaxedBernoulliStraightThrough(temperature=self.temperature, probs=edge_probs).rsample()\n\t        # making adj_sampled symmetric\n", "        adj_sampled = adj_sampled.triu(1)\n\t        adj_sampled = adj_sampled + adj_sampled.T\n\t        return adj_sampled\n\t    def sample_adj_add_bernoulli(self, adj_logits, adj_orig, alpha):\n\t        edge_probs = adj_logits / torch.max(adj_logits)\n\t        edge_probs = alpha*edge_probs + (1-alpha)*adj_orig\n\t        # sampling\n\t        adj_sampled = pyro.distributions.RelaxedBernoulliStraightThrough(temperature=self.temperature, probs=edge_probs).rsample()\n\t        # making adj_sampled symmetric\n\t        adj_sampled = adj_sampled.triu(1)\n", "        adj_sampled = adj_sampled + adj_sampled.T\n\t        return adj_sampled\n\t    def forward(self, feats, adj, adj_orig):\n\t        # print(feats)\n\t        # print(adj)\n\t        adj_logits = self.ep_net(feats, adj)\n\t        if self.alpha == 1:\n\t            adj_new = self.sample_adj(adj_logits)\n\t        else:\n\t            adj_new = self.sample_adj_add_bernoulli(adj_logits, adj_orig, self.alpha)\n", "        adj_new_normed = normalize(adj_new)\n\t        hidden, output = self.nc_net((feats, adj_new_normed, False))\n\t        return output, adj_logits, adj_new\n\tdef eval_edge_pred(adj_pred, val_edges, edge_labels):\n\t    logits = adj_pred[val_edges.T]\n\t    logits = np.nan_to_num(logits)\n\t    roc_auc = roc_auc_score(edge_labels, logits)\n\t    ap_score = average_precision_score(edge_labels, logits)\n\t    return roc_auc, ap_score\n\tclass MultipleOptimizer():\n", "    \"\"\" a class that wraps multiple optimizers \"\"\"\n\t    def __init__(self, *op):\n\t        self.optimizers = op\n\t    def zero_grad(self):\n\t        for op in self.optimizers:\n\t            op.zero_grad()\n\t    def step(self):\n\t        for op in self.optimizers:\n\t            op.step()\n\t    def update_lr(self, op_index, new_lr):\n", "        \"\"\" update the learning rate of one optimizer\n\t        Parameters: op_index: the index of the optimizer to update\n\t                    new_lr:   new learning rate for that optimizer \"\"\"\n\t        for param_group in self.optimizers[op_index].param_groups:\n\t            param_group['lr'] = new_lr\n\tdef get_lr_schedule_by_sigmoid(n_epochs, lr, warmup):\n\t    \"\"\" schedule the learning rate with the sigmoid function.\n\t    The learning rate will start with near zero and end with near lr \"\"\"\n\t    factors = torch.FloatTensor(np.arange(n_epochs))\n\t    factors = ((factors / factors[-1]) * (warmup * 2)) - warmup\n", "    factors = torch.sigmoid(factors)\n\t    # range the factors to [0, 1]\n\t    factors = (factors - factors[0]) / (factors[-1] - factors[0])\n\t    lr_schedule = factors * lr\n\t    return lr_schedule"]}
{"filename": "opengsl/method/models/nodeformer.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch_sparse import SparseTensor, matmul\n\tfrom torch_geometric.utils import degree\n\tBIG_CONSTANT = 1e8\n\tdef adj_mul(adj_i, adj, N):\n\t    adj_i_sp = torch.sparse_coo_tensor(adj_i, torch.ones(adj_i.shape[1], dtype=torch.float).to(adj.device), (N, N))\n\t    adj_sp = torch.sparse_coo_tensor(adj, torch.ones(adj.shape[1], dtype=torch.float).to(adj.device), (N, N))\n", "    adj_j = torch.sparse.mm(adj_i_sp, adj_sp)\n\t    adj_j = adj_j.coalesce().indices()\n\t    return adj_j\n\tdef create_projection_matrix(m, d, seed=0):\n\t    block_list = []\n\t    current_seed = seed\n\t    torch.manual_seed(current_seed)\n\t    unstructured_block = torch.randn((d, d))\n\t    q, _ = torch.qr(unstructured_block)\n\t    q = torch.t(q)\n", "    block_list.append(q[0:m])\n\t    final_matrix = torch.vstack(block_list)\n\t    current_seed += 1\n\t    torch.manual_seed(current_seed)\n\t    multiplier = torch.norm(torch.randn((m, d)), dim=1)\n\t    return torch.matmul(torch.diag(multiplier), final_matrix)\n\tdef softmax_kernel_transformation(data, is_query, projection_matrix=None, numerical_stabilizer=0.000001):\n\t    data_normalizer = 1.0 / torch.sqrt(torch.sqrt(torch.tensor(data.shape[-1], dtype=torch.float32)))   # ä¸å¤ªçè§£è¿ä¸è¡\n\t    data = data_normalizer * data\n\t    ratio = 1.0 / torch.sqrt(torch.tensor(projection_matrix.shape[0], dtype=torch.float32))\n", "    data_dash = torch.einsum(\"bnhd,md->bnhm\", data, projection_matrix)\n\t    diag_data = torch.square(data)\n\t    diag_data = torch.sum(diag_data, dim=len(data.shape)-1)\n\t    diag_data = diag_data / 2.0\n\t    diag_data = torch.unsqueeze(diag_data, dim=len(data.shape)-1)\n\t    last_dims_t = len(data_dash.shape) - 1\n\t    attention_dims_t = len(data_dash.shape) - 3\n\t    if is_query:\n\t        # ä¸é¢çå è¡ä¹ä¸çè§£\n\t        data_dash = ratio * (\n", "            torch.exp(data_dash - diag_data - torch.max(data_dash, dim=last_dims_t, keepdim=True)[0]) + numerical_stabilizer\n\t        )\n\t    else:\n\t        data_dash = ratio * (\n\t            torch.exp(data_dash - diag_data - torch.max(torch.max(data_dash, dim=last_dims_t, keepdim=True)[0],\n\t                    dim=attention_dims_t, keepdim=True)[0]) + numerical_stabilizer\n\t        )\n\t    return data_dash\n\tdef numerator(qs, ks, vs):\n\t    kvs = torch.einsum(\"nbhm,nbhd->bhmd\", ks, vs) # kvs refers to U_k in the paper\n", "    return torch.einsum(\"nbhm,bhmd->nbhd\", qs, kvs)\n\tdef denominator(qs, ks):\n\t    all_ones = torch.ones([ks.shape[0]]).to(qs.device)\n\t    ks_sum = torch.einsum(\"nbhm,n->bhm\", ks, all_ones) # ks_sum refers to O_k in the paper\n\t    return torch.einsum(\"nbhm,bhm->nbh\", qs, ks_sum)\n\tdef numerator_gumbel(qs, ks, vs):\n\t    kvs = torch.einsum(\"nbhkm,nbhd->bhkmd\", ks, vs) # kvs refers to U_k in the paper\n\t    return torch.einsum(\"nbhm,bhkmd->nbhkd\", qs, kvs)\n\tdef denominator_gumbel(qs, ks):\n\t    all_ones = torch.ones([ks.shape[0]]).to(qs.device)\n", "    ks_sum = torch.einsum(\"nbhkm,n->bhkm\", ks, all_ones) # ks_sum refers to O_k in the paper\n\t    return torch.einsum(\"nbhm,bhkm->nbhk\", qs, ks_sum)\n\tdef kernelized_softmax(query, key, value, projection_matrix=None, edge_index=None, tau=0.25):\n\t    '''\n\t    fast computation of all-pair attentive aggregation with linear complexity\n\t    input: query/key/value [B, N, H, D]\n\t    return: updated node emb, attention weight (for computing edge loss)\n\t    B = graph number (always equal to 1 in Node Classification), N = node number, H = head number,\n\t    M = random feature dimension, D = hidden size\n\t    '''\n", "    query = query / math.sqrt(tau)\n\t    key = key / math.sqrt(tau)\n\t    query_prime = softmax_kernel_transformation(query, True, projection_matrix) # [B, N, H, M]ï¼ åªæsoftmax_kernel_transformationä¸ç§\n\t    key_prime = softmax_kernel_transformation(key, False, projection_matrix) # [B, N, H, M]\n\t    query_prime = query_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n\t    key_prime = key_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n\t    value = value.permute(1, 0, 2, 3) # [N, B, H, D]\n\t    # compute updated node emb, this step requires O(N)\n\t    z_num = numerator(query_prime, key_prime, value)\n\t    z_den = denominator(query_prime, key_prime)\n", "    z_num = z_num.permute(1, 0, 2, 3)  # [B, N, H, D]\n\t    z_den = z_den.permute(1, 0, 2)\n\t    z_den = torch.unsqueeze(z_den, len(z_den.shape))\n\t    z_output = z_num / z_den # [B, N, H, D]\n\t    # ä¸å®æ¯True\n\t    start, end = edge_index\n\t    query_end, key_start = query_prime[end], key_prime[start] # [E, B, H, M]\n\t    edge_attn_num = torch.einsum(\"ebhm,ebhm->ebh\", query_end, key_start) # [E, B, H]\n\t    edge_attn_num = edge_attn_num.permute(1, 0, 2) # [B, E, H]\n\t    attn_normalizer = denominator(query_prime, key_prime) # [N, B, H]\n", "    edge_attn_dem = attn_normalizer[end]  # [E, B, H]\n\t    edge_attn_dem = edge_attn_dem.permute(1, 0, 2) # [B, E, H]\n\t    A_weight = edge_attn_num / edge_attn_dem # [B, E, H]\n\t    return z_output, A_weight\n\tdef kernelized_gumbel_softmax(query, key, value, projection_matrix=None, edge_index=None,\n\t                                K=10, tau=0.25):\n\t    '''\n\t    fast computation of all-pair attentive aggregation with linear complexity\n\t    input: query/key/value [B, N, H, D]\n\t    return: updated node emb, attention weight (for computing edge loss)\n", "    B = graph number (always equal to 1 in Node Classification), N = node number, H = head number,\n\t    M = random feature dimension, D = hidden size, K = number of Gumbel sampling\n\t    '''\n\t    query = query / math.sqrt(tau)\n\t    key = key / math.sqrt(tau)\n\t    query_prime = softmax_kernel_transformation(query, True, projection_matrix) # [B, N, H, M]\n\t    key_prime = softmax_kernel_transformation(key, False, projection_matrix) # [B, N, H, M]\n\t    query_prime = query_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n\t    key_prime = key_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n\t    value = value.permute(1, 0, 2, 3) # [N, B, H, D]\n", "    # compute updated node emb, this step requires O(N)\n\t    gumbels = (\n\t        -torch.empty(key_prime.shape[:-1]+(K, ), memory_format=torch.legacy_contiguous_format).exponential_().log()\n\t    ).to(query.device) / tau # [N, B, H, K]\n\t    key_t_gumbel = key_prime.unsqueeze(3) * gumbels.exp().unsqueeze(4) # [N, B, H, K, M]\n\t    z_num = numerator_gumbel(query_prime, key_t_gumbel, value) # [N, B, H, K, D]\n\t    z_den = denominator_gumbel(query_prime, key_t_gumbel) # [N, B, H, K]\n\t    z_num = z_num.permute(1, 0, 2, 3, 4) # [B, N, H, K, D]\n\t    z_den = z_den.permute(1, 0, 2, 3) # [B, N, H, K]\n\t    z_den = torch.unsqueeze(z_den, len(z_den.shape))\n", "    z_output = torch.mean(z_num / z_den, dim=3) # [B, N, H, D]\n\t    start, end = edge_index\n\t    query_end, key_start = query_prime[end], key_prime[start] # [E, B, H, M]\n\t    edge_attn_num = torch.einsum(\"ebhm,ebhm->ebh\", query_end, key_start) # [E, B, H]\n\t    edge_attn_num = edge_attn_num.permute(1, 0, 2) # [B, E, H]\n\t    attn_normalizer = denominator(query_prime, key_prime) # [N, B, H]\n\t    edge_attn_dem = attn_normalizer[end]  # [E, B, H]\n\t    edge_attn_dem = edge_attn_dem.permute(1, 0, 2) # [B, E, H]\n\t    A_weight = edge_attn_num / edge_attn_dem # [B, E, H]\n\t    return z_output, A_weight\n", "def add_conv_relational_bias(x, edge_index, b, trans='sigmoid'):\n\t    '''\n\t    compute updated result by the relational bias of input adjacency\n\t    the implementation is similar to the Graph Convolution Network with a (shared) scalar weight for each edge\n\t    '''\n\t    row, col = edge_index\n\t    d_in = degree(col, x.shape[1]).float()\n\t    d_norm_in = (1. / d_in[col]).sqrt()\n\t    d_out = degree(row, x.shape[1]).float()\n\t    d_norm_out = (1. / d_out[row]).sqrt()\n", "    conv_output = []\n\t    for i in range(x.shape[2]):\n\t        if trans == 'sigmoid':\n\t            b_i = F.sigmoid(b[i])\n\t        elif trans == 'identity':\n\t            b_i = b[i]\n\t        else:\n\t            raise NotImplementedError\n\t        value = torch.ones_like(row) * b_i * d_norm_in * d_norm_out\n\t        adj_i = SparseTensor(row=col, col=row, value=value, sparse_sizes=(x.shape[1], x.shape[1]))\n", "        conv_output.append(matmul(adj_i, x[:, :, i]) )  # [B, N, D]\n\t    conv_output = torch.stack(conv_output, dim=2) # [B, N, H, D]\n\t    return conv_output\n\tclass NodeFormerConv(nn.Module):\n\t    '''\n\t    one layer of NodeFormer that attentive aggregates all nodes over a latent graph\n\t    return: node embeddings for next layer, edge loss at this layer\n\t    '''\n\t    def __init__(self, in_channels, out_channels, num_heads, nb_random_features=10, use_gumbel=True,\n\t                 nb_gumbel_sample=10, rb_order=0, rb_trans='sigmoid'):\n", "        super(NodeFormerConv, self).__init__()\n\t        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n\t        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n\t        self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n\t        self.Wo = nn.Linear(out_channels * num_heads, out_channels)\n\t        if rb_order >= 1:\n\t            self.b = torch.nn.Parameter(torch.FloatTensor(rb_order, num_heads), requires_grad=True)\n\t        self.out_channels = out_channels\n\t        self.num_heads = num_heads\n\t        self.nb_random_features = nb_random_features\n", "        self.use_gumbel = use_gumbel\n\t        self.nb_gumbel_sample = nb_gumbel_sample\n\t        self.rb_order = rb_order\n\t        self.rb_trans = rb_trans\n\t    def reset_parameters(self):\n\t        self.Wk.reset_parameters()\n\t        self.Wq.reset_parameters()\n\t        self.Wv.reset_parameters()\n\t        self.Wo.reset_parameters()\n\t        if self.rb_order >= 1:\n", "            if self.rb_trans == 'sigmoid':\n\t                torch.nn.init.constant_(self.b, 0.1)\n\t            elif self.rb_trans == 'identity':\n\t                torch.nn.init.constant_(self.b, 1.0)\n\t    def forward(self, z, adjs, tau):\n\t        B, N = z.size(0), z.size(1)\n\t        query = self.Wq(z).reshape(-1, N, self.num_heads, self.out_channels)\n\t        key = self.Wk(z).reshape(-1, N, self.num_heads, self.out_channels)\n\t        value = self.Wv(z).reshape(-1, N, self.num_heads, self.out_channels)\n\t        dim = query.shape[-1]\n", "        seed = torch.ceil(torch.abs(torch.sum(query) * BIG_CONSTANT)).to(torch.int32)\n\t        projection_matrix = create_projection_matrix(\n\t            self.nb_random_features, dim, seed=seed).to(query.device)\n\t        # compute all-pair message passing update and attn weight on input edges, requires O(N) or O(N + E)\n\t        if self.use_gumbel and self.training:  # only using Gumbel noise for training\n\t            z_next, weight = kernelized_gumbel_softmax(query,key,value,projection_matrix,adjs[0], self.nb_gumbel_sample,\n\t                                                       tau)\n\t        else:\n\t            z_next, weight = kernelized_softmax(query, key, value, projection_matrix, adjs[0], tau)\n\t        # compute update by relational bias of input adjacency, requires O(E)\n", "        for i in range(self.rb_order):\n\t            z_next += add_conv_relational_bias(value, adjs[i], self.b[i], self.rb_trans)\n\t        # aggregate results of multiple heads\n\t        z_next = self.Wo(z_next.flatten(-2, -1))\n\t        row, col = adjs[0]\n\t        d_in = degree(col, query.shape[1]).float()\n\t        d_norm = 1. / d_in[col]\n\t        d_norm_ = d_norm.reshape(1, -1, 1).repeat(1, 1, weight.shape[-1])\n\t        link_loss = torch.mean(weight.log() * d_norm_)\n\t        return z_next, link_loss\n", "class NodeFormer(nn.Module):\n\t    '''\n\t    NodeFormer model implementation\n\t    return: predicted node labels, a list of edge losses at every layer\n\t    '''\n\t    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=4, dropout=0.0,\n\t                 nb_random_features=30, use_bn=True, use_gumbel=True, use_residual=True, use_act=False, use_jk=False,\n\t                 nb_gumbel_sample=10, rb_order=0, rb_trans='sigmoid'):\n\t        super(NodeFormer, self).__init__()\n\t        self.convs = nn.ModuleList()\n", "        self.fcs = nn.ModuleList()\n\t        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n\t        self.bns = nn.ModuleList()\n\t        self.bns.append(nn.LayerNorm(hidden_channels))\n\t        for i in range(num_layers):\n\t            self.convs.append(\n\t                NodeFormerConv(hidden_channels, hidden_channels, num_heads=num_heads,\n\t                               nb_random_features=nb_random_features, use_gumbel=use_gumbel,\n\t                               nb_gumbel_sample=nb_gumbel_sample, rb_order=rb_order, rb_trans=rb_trans))\n\t            self.bns.append(nn.LayerNorm(hidden_channels))\n", "        if use_jk:\n\t            self.fcs.append(nn.Linear(hidden_channels * num_layers + hidden_channels, out_channels))\n\t        else:\n\t            self.fcs.append(nn.Linear(hidden_channels, out_channels))\n\t        self.dropout = dropout\n\t        self.activation = F.elu\n\t        self.use_bn = use_bn\n\t        self.use_residual = use_residual\n\t        self.use_act = use_act\n\t        self.use_jk = use_jk\n", "    def reset_parameters(self):\n\t        for conv in self.convs:\n\t            conv.reset_parameters()\n\t        for bn in self.bns:\n\t            bn.reset_parameters()\n\t        for fc in self.fcs:\n\t            fc.reset_parameters()\n\t    def forward(self, x, adjs, tau=1.0):\n\t        x = x.unsqueeze(0) # [B, N, H, D], B=1 denotes number of graph\n\t        layer_ = []\n", "        link_loss_ = []\n\t        z = self.fcs[0](x)\n\t        if self.use_bn:\n\t            z = self.bns[0](z)\n\t        z = self.activation(z)\n\t        z = F.dropout(z, p=self.dropout, training=self.training)\n\t        layer_.append(z)\n\t        for i, conv in enumerate(self.convs):\n\t            z, link_loss = conv(z, adjs, tau)\n\t            link_loss_.append(link_loss)\n", "            if self.use_residual:\n\t                z += layer_[i]\n\t            if self.use_bn:\n\t                z = self.bns[i+1](z)\n\t            if self.use_act:\n\t                z = self.activation(z)\n\t            z = F.dropout(z, p=self.dropout, training=self.training)\n\t            layer_.append(z)\n\t        if self.use_jk: # use jk connection for each layer\n\t            z = torch.cat(layer_, dim=-1)\n", "        x_out = self.fcs[-1](z).squeeze(0)\n\t        return x_out.squeeze(1), link_loss_"]}
{"filename": "opengsl/method/models/gen.py", "chunked_list": ["import numpy as np\n\tfrom collections import Counter\n\tclass EstimateAdj:\n\t    def __init__(self, n_classes, adj, train_mask, labels, homophily):\n\t        self.num_class = n_classes\n\t        self.num_node = adj.shape[0]\n\t        self.idx_train = train_mask\n\t        self.label = labels.cpu().numpy()\n\t        self.adj = adj.cpu().numpy()\n\t        self.output = None\n", "        self.iterations = 0\n\t        self.count = 0\n\t        self.homophily = homophily\n\t    def reset_obs(self):\n\t        self.count = 0\n\t        self.N = 0\n\t        self.E = np.zeros((self.num_node, self.num_node), dtype=np.int64)\n\t    def update_obs(self, graph):\n\t        self.E += graph\n\t        self.N += 1\n", "    def revise_pred(self):\n\t        # For the training node, GT is used, and for the unlabeled node, the predicted label is used\n\t        self.output[self.idx_train] = self.label[self.idx_train]\n\t    def E_step(self, Q):\n\t        \"\"\"Run the Expectation(E) step of the EM algorithm.\n\t        Parameters\n\t        ----------\n\t        Q:\n\t            The current estimation that each edge is actually present (numpy.array)\n\t        Returns\n", "        ----------\n\t        alpha:\n\t            The estimation of true-positive rate (float)\n\t        betaï¼\n\t            The estimation of false-positive rate (float)\n\t        O:\n\t            The estimation of network model parameters (numpy.array)\n\t        \"\"\"\n\t        # Temporary variables to hold the numerators and denominators of alpha and beta\n\t        an = Q * self.E\n", "        an = np.triu(an, 1).sum()\n\t        bn = (1 - Q) * self.E\n\t        bn = np.triu(bn, 1).sum()\n\t        ad = Q * self.N\n\t        ad = np.triu(ad, 1).sum()\n\t        bd = (1 - Q) * self.N\n\t        bd = np.triu(bd, 1).sum()\n\t        # Calculate alpha, beta\n\t        alpha = an * 1. / (ad)\n\t        beta = bn * 1. / (bd)\n", "        O = np.zeros((self.num_class, self.num_class))\n\t        n = []\n\t        counter = Counter(self.output)\n\t        for i in range(self.num_class):\n\t            n.append(counter[i])\n\t        a = self.output.repeat(self.num_node).reshape(self.num_node, -1)\n\t        for j in range(self.num_class):\n\t            c = (a == j)\n\t            for i in range(j + 1):\n\t                b = (a == i)\n", "                O[i, j] = np.triu((b & c.T) * Q, 1).sum()\n\t                if i == j:\n\t                    O[j, j] = 2. / (n[j] * (n[j] - 1)) * O[j, j]\n\t                else:\n\t                    O[i, j] = 1. / (n[i] * n[j]) * O[i, j]\n\t        return (alpha, beta, O)\n\t    def M_step(self, alpha, beta, O):\n\t        \"\"\"Run the Maximization(M) step of the EM algorithm.\n\t        \"\"\"\n\t        O += O.T - np.diag(O.diagonal())   #ä½¿æå¯¹è§åç´ çä¸ä¸è§åä¸ºå¯¹ç§°ç©éµ\n", "        row = self.output.repeat(self.num_node)\n\t        col = np.tile(self.output, self.num_node)\n\t        tmp = O[row, col].reshape(self.num_node, -1)\n\t        p1 = tmp * np.power(alpha, self.E) * np.power(1 - alpha, self.N - self.E)\n\t        p2 = (1 - tmp) * np.power(beta, self.E) * np.power(1 - beta, self.N - self.E)\n\t        Q = p1 * 1. / (p1 + p2 * 1.)\n\t        return Q\n\t    def EM(self, output, tolerance=.000001):\n\t        \"\"\"Run the complete EM algorithm.\n\t        Parameters\n", "        ----------\n\t        tolerance:\n\t            Determine the tolerance in the variantions of alpha, beta and O, which is acceptable to stop iterating (float)\n\t        seed:\n\t            seed for np.random.seed (int)\n\t        Returns\n\t        ----------\n\t        iterations:\n\t            The number of iterations to achieve the tolerance on the parameters (int)\n\t        \"\"\"\n", "        # Record previous values to confirm convergence\n\t        alpha_p = 0\n\t        beta_p = 0\n\t        self.output = output\n\t        self.revise_pred()\n\t        # Do an initial E-step with random alpha, beta and O\n\t        # Beta must be smalller than alpha\n\t        beta, alpha = np.sort(np.random.rand(2))\n\t        O = np.triu(np.random.rand(self.num_class, self.num_class))   #æå¯¹è§åç´ çä¸ä¸è§\n\t        # Calculate initial Q\n", "        Q = self.M_step(alpha, beta, O)\n\t        while abs(alpha_p - alpha) > tolerance or abs(beta_p - beta) > tolerance:\n\t            alpha_p = alpha\n\t            beta_p = beta\n\t            alpha, beta, O = self.E_step(Q)\n\t            Q = self.M_step(alpha, beta, O)\n\t            self.iterations += 1\n\t            self.count += 1\n\t            #print(self.iterations,alpha,beta)\n\t        if self.homophily > 0.5:\n", "            # Make sure that the initial edge will be preserved\n\t            Q += self.adj\n\t        return (alpha, beta, O, Q, self.iterations)\n\tdef get_homophily(label, adj):\n\t    num_node = len(label)\n\t    label = label.repeat(num_node).reshape(num_node, -1)\n\t    n = np.triu((label == label.T) & (adj == 1)).sum(axis=0)\n\t    d = np.triu(adj).sum(axis=0)\n\t    homos = []\n\t    for i in range(num_node):\n", "        if d[i] > 0:\n\t            homos.append(n[i] * 1./d[i])\n\t    return np.mean(homos)\n\tdef prob_to_adj(mx, threshold):\n\t    mx = np.triu(mx, 1)   # the 2 steps here del the self loop\n\t    mx += mx.T\n\t    adj = np.zeros_like(mx)\n\t    adj[mx > threshold] = 1\n\t    return adj\n"]}
{"filename": "opengsl/method/models/slaps.py", "chunked_list": ["import torch\n\timport torch.nn.functional as F\n\t# from .GCN3 import GraphConvolution, GCN\n\timport math\n\timport dgl\n\tfrom .gcn import GCN, GraphConvolution\n\tfrom sklearn.neighbors import kneighbors_graph\n\tfrom .gnn_modules import APPNP\n\timport numpy as np\n\tdef apply_non_linearity(tensor, non_linearity, i):\n", "    if non_linearity == 'elu':\n\t        return F.elu(tensor * i - i) + 1\n\t    elif non_linearity == 'relu':\n\t        return F.relu(tensor)\n\t    elif non_linearity == 'none':\n\t        return tensor\n\t    else:\n\t        raise NameError('We dont support the non-linearity yet')\n\tdef nearest_neighbors(X, k, metric):\n\t    adj = kneighbors_graph(X, k, metric=metric)\n", "    adj = np.array(adj.todense(), dtype=np.float32)\n\t    adj += np.eye(adj.shape[0])\n\t    return adj\n\tdef normalize(adj, mode):\n\t    EOS = 1e-10\n\t    if mode == \"sym\":\n\t        inv_sqrt_degree = 1. / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)\n\t        return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]\n\t    elif mode == \"row\":\n\t        inv_degree = 1. / (adj.sum(dim=1, keepdim=False) + EOS)\n", "        return inv_degree[:, None] * adj\n\t    else:\n\t        exit(\"wrong norm mode\")\n\tdef symmetrize(adj):  # only for non-sparse\n\t    return (adj + adj.T) / 2\n\tdef cal_similarity_graph(node_embeddings):\n\t    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n\t    return similarity_graph\n\tdef top_k(raw_graph, K):\n\t    values, indices = raw_graph.topk(k=int(K), dim=-1)\n", "    assert torch.max(indices) < raw_graph.shape[1]\n\t    mask = torch.zeros(raw_graph.shape, device='cuda')\n\t    mask[torch.arange(raw_graph.shape[0], device='cuda').view(-1, 1), indices] = 1.\n\t    mask.requires_grad = False\n\t    sparse_graph = raw_graph * mask\n\t    return sparse_graph\n\tdef knn_fast(X, k, b):\n\t    X = F.normalize(X, dim=1, p=2)\n\t    index = 0\n\t    values = torch.zeros(X.shape[0] * (k + 1), device='cuda')\n", "    rows = torch.zeros(X.shape[0] * (k + 1), device='cuda')\n\t    cols = torch.zeros(X.shape[0] * (k + 1), device='cuda')\n\t    norm_row = torch.zeros(X.shape[0], device='cuda')\n\t    norm_col = torch.zeros(X.shape[0], device='cuda')\n\t    while index < X.shape[0]:\n\t        if (index + b) > (X.shape[0]):\n\t            end = X.shape[0]\n\t        else:\n\t            end = index + b\n\t        sub_tensor = X[index:index + b]\n", "        similarities = torch.mm(sub_tensor, X.t())\n\t        vals, inds = similarities.topk(k=k + 1, dim=-1)\n\t        values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n\t        cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n\t        rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end, device='cuda').view(-1, 1).repeat(1, k + 1).view(-1)\n\t        norm_row[index: end] = torch.sum(vals, dim=1)\n\t        norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n\t        index += b\n\t    norm = norm_row + norm_col\n\t    rows = rows.long()\n", "    cols = cols.long()\n\t    values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n\t    return rows, cols, values\n\tclass MLP(torch.nn.Module):\n\t    def __init__(self, nlayers, isize, hsize, osize, features, mlp_epochs, k, knn_metric, non_linearity, i, mlp_act):\n\t        super(MLP, self).__init__()\n\t        self.layers = torch.nn.ModuleList()\n\t        if nlayers == 1:\n\t            self.layers.append(torch.nn.Linear(isize, hsize))\n\t        else:\n", "            self.layers.append(torch.nn.Linear(isize, hsize))\n\t            for _ in range(nlayers - 2):\n\t                self.layers.append(torch.nn.Linear(hsize, hsize))\n\t            self.layers.append(torch.nn.Linear(hsize, osize))\n\t        self.input_dim = isize\n\t        self.output_dim = osize\n\t        self.features = features\n\t        self.mlp_epochs = mlp_epochs\n\t        self.k = k\n\t        self.knn_metric = knn_metric\n", "        self.non_linearity = non_linearity\n\t        self.i = i\n\t        self.mlp_act = mlp_act\n\t        self.mlp_knn_init()\n\t    def internal_forward(self, h):\n\t        for i, layer in enumerate(self.layers):\n\t            h = layer(h)\n\t            if i != (len(self.layers) - 1):\n\t                if self.mlp_act == \"relu\":\n\t                    h = F.relu(h)\n", "                elif self.mlp_act == \"tanh\":\n\t                    h = F.tanh(h)\n\t        return h\n\t    def mlp_knn_init(self):\n\t        self.layers.to(self.features.device)\n\t        if self.input_dim == self.output_dim:\n\t            print(\"MLP full\")\n\t            for layer in self.layers:\n\t                layer.weight = torch.nn.Parameter(torch.eye(self.input_dim))\n\t        else:\n", "            optimizer = torch.optim.Adam(self.parameters(), 0.01)\n\t            labels = torch.from_numpy(nearest_neighbors(self.features.cpu(), self.k, self.knn_metric)).cuda()\n\t            for epoch in range(1, self.mlp_epochs):\n\t                self.train()\n\t                logits = self.forward(self.features)\n\t                loss = F.mse_loss(logits, labels, reduction='sum')\n\t                if epoch % 10 == 0:\n\t                    print(\"MLP loss\", loss.item())\n\t                optimizer.zero_grad()\n\t                loss.backward()\n", "                optimizer.step()\n\t    def forward(self, features):\n\t        embeddings = self.internal_forward(features)\n\t        embeddings = F.normalize(embeddings, dim=1, p=2)\n\t        similarities = cal_similarity_graph(embeddings)\n\t        similarities = top_k(similarities, self.k + 1)\n\t        similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n\t        return similarities\n\tclass GCN_DAE(torch.nn.Module):\n\t    def __init__(self, cfg_model, nlayers, in_dim, hidden_dim, nclasses, dropout, dropout_adj, features, k, knn_metric, i_,\n", "                 non_linearity, normalization, mlp_h, mlp_epochs, mlp_act):\n\t        super(GCN_DAE, self).__init__()\n\t        if cfg_model['type'] == 'gcn':\n\t            self.layers = GCN(in_dim, hidden_dim, nclasses, n_layers=nlayers, dropout=dropout, spmm_type=1)\n\t        elif cfg_model['type'] == 'appnp':\n\t            self.layers = APPNP(in_dim, hidden_dim, nclasses, spmm_type=1,\n\t                               dropout=dropout, K=cfg_model['appnp_k'], alpha=cfg_model['appnp_alpha'])\n\t        self.dropout_adj = torch.nn.Dropout(p=dropout_adj)\n\t        self.normalization = normalization\n\t        self.graph_gen = MLP(2, features.shape[1], math.floor(math.sqrt(features.shape[1] * mlp_h)),\n", "                                mlp_h, features, mlp_epochs, k, knn_metric, non_linearity, i_,\n\t                                mlp_act).cuda()\n\t    def get_adj(self, h):\n\t        Adj_ = self.graph_gen(h)\n\t        Adj_ = symmetrize(Adj_)\n\t        Adj_ = normalize(Adj_, self.normalization)\n\t        return Adj_\n\t    def forward(self, features, x):  # x corresponds to masked_features\n\t        Adj_ = self.get_adj(features)\n\t        Adj = self.dropout_adj(Adj_)\n", "        x = self.layers((x, Adj, True))\n\t        return x, Adj_\n\tclass GCN_C(torch.nn.Module):\n\t    def __init__(self, cfg_model, in_channels, hidden_channels, out_channels, num_layers, dropout, dropout_adj):\n\t        super(GCN_C, self).__init__()\n\t        if cfg_model['type'] == 'gcn':\n\t            self.layers = GCN(in_channels, hidden_channels, out_channels, n_layers=num_layers, dropout=dropout, spmm_type=1)\n\t        elif cfg_model['type'] == 'appnp':\n\t            self.layers = APPNP(in_channels, hidden_channels, out_channels, spmm_type=1,\n\t                               dropout=dropout, K=cfg_model['appnp_k'], alpha=cfg_model['appnp_alpha'])\n", "        self.dropout_adj = torch.nn.Dropout(p=dropout_adj)\n\t    def forward(self, x, adj_t):\n\t        Adj = self.dropout_adj(adj_t)\n\t        x = self.layers((x, Adj, True))\n\t        return x\n\tclass SLAPS(torch.nn.Module):\n\t    def __init__(self, num_nodes, num_features, num_classes, features, device, conf):\n\t        super(SLAPS, self).__init__()\n\t        self.num_nodes = num_nodes\n\t        self.num_features = num_features\n", "        self.num_classes = num_classes\n\t        self.device = device\n\t        self.conf = conf\n\t        self.gcn_dae = GCN_DAE(self.conf.model, nlayers=self.conf.model['nlayers_adj'], in_dim=num_features, hidden_dim=self.conf.model['hidden_adj'], nclasses=num_features,\n\t                             dropout=self.conf.model['dropout1'], dropout_adj=self.conf.model['dropout_adj1'],\n\t                             features=features, k=self.conf.model['k'], knn_metric=self.conf.model['knn_metric'], i_=self.conf.model['i'],\n\t                             non_linearity=self.conf.model['non_linearity'], normalization=self.conf.model['normalization'], mlp_h=self.num_features,\n\t                             mlp_epochs=self.conf.model['mlp_epochs'], mlp_act=self.conf.model['mlp_act'])\n\t        self.gcn_c = GCN_C(self.conf.model, in_channels=num_features, hidden_channels=self.conf.model['hidden'], out_channels=num_classes,\n\t                            num_layers=self.conf.model['nlayers'], dropout=self.conf.model['dropout2'], dropout_adj=self.conf.model['dropout_adj2'])\n", "    def forward(self, features):\n\t        loss_dae, Adj = self.get_loss_masked_features(features)\n\t        logits = self.gcn_c(features, Adj)\n\t        if len(logits.shape) > 1:\n\t            logits = logits.squeeze(1)\n\t        return logits, loss_dae, Adj\n\t    def get_loss_masked_features(self, features):\n\t        if self.conf.dataset['feat_type'] == 'binary':\n\t            mask = self.get_random_mask_binary(features, self.conf.training['ratio'], self.conf.training['nr'])\n\t            masked_features = features * (1 - mask)\n", "            logits, Adj = self.gcn_dae(features, masked_features)\n\t            indices = mask > 0\n\t            loss = F.binary_cross_entropy_with_logits(logits[indices], features[indices], reduction='mean')\n\t        elif self.conf.dataset['feat_type'] == 'continuous':\n\t            mask = self.get_random_mask_continuous(features, self.conf.training['ratio'])\n\t            # noise = torch.normal(0.0, 1.0, size=features.shape).cuda()\n\t            # masked_features = features + (noise * mask)\n\t            masked_features = features * (1 - mask)\n\t            logits, Adj = self.gcn_dae(features, masked_features)\n\t            indices = mask > 0\n", "            loss = F.binary_cross_entropy_with_logits(logits[indices], features[indices], reduction='mean')\n\t        else:\n\t            raise ValueError(\"Wrong feat_type in dataset_configure.\")\n\t        return loss, Adj\n\t    def get_random_mask_binary(self, features, r, nr):\n\t        nones = torch.sum(features > 0.0).float()\n\t        nzeros = features.shape[0] * features.shape[1] - nones\n\t        pzeros = nones / nzeros / r * nr\n\t        probs = torch.zeros(features.shape, device='cuda')\n\t        probs[features == 0.0] = pzeros\n", "        probs[features > 0.0] = 1 / r\n\t        mask = torch.bernoulli(probs)\n\t        return mask\n\t    def get_random_mask_continuous(self, features, r):\n\t        probs = torch.full(features.shape, 1 / r, device='cuda')\n\t        mask = torch.bernoulli(probs)\n\t        return mask\n"]}
{"filename": "opengsl/method/models/idgl.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom .gcn_idgl import GCN\n\tclass AnchorGCNLayer(nn.Module):\n\t    def __init__(self, in_features, out_features, with_bias=False, batch_norm=True):\n\t        super(AnchorGCNLayer, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n", "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n\t        if with_bias:\n\t            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n\t        else:\n\t            self.register_parameter('bias', None)\n\t        self.bn = nn.BatchNorm1d(out_features) if batch_norm else None\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n\t        stdv = 1. / math.sqrt(self.weight.size(1))\n\t        self.weight.data.uniform_(-stdv, stdv)\n", "        if self.bias is not None:\n\t            self.bias.data.uniform_(-stdv, stdv)\n\t    def forward(self, input, adj, anchor_mp=True):\n\t        if input.data.is_sparse:\n\t            support = torch.spmm(input, self.weight)\n\t        else:\n\t            support = torch.mm(input, self.weight)\n\t        if anchor_mp:\n\t            node_anchor_adj = adj\n\t            node_norm = node_anchor_adj / torch.clamp(torch.sum(node_anchor_adj, dim=-2, keepdim=True), min=1e-12)\n", "            anchor_norm = node_anchor_adj / torch.clamp(torch.sum(node_anchor_adj, dim=-1, keepdim=True), min=1e-12)\n\t            output = torch.matmul(anchor_norm, torch.matmul(node_norm.transpose(-1, -2), support))\n\t        else:\n\t            output = torch.spmm(adj, support)\n\t        if self.bias is not None:\n\t            output = output + self.bias\n\t        return output\n\t    def compute_bn(self, x):\n\t        if len(x.shape) == 2:\n\t            return self.bn(x)\n", "        else:\n\t            return self.bn(x.view(-1, x.size(-1))).view(x.size())\n\tclass AnchorGCN(nn.Module):\n\t    def __init__(self, nfeat, nhid, nclass, n_layers=3, dropout=0.5, with_bias=False, batch_norm=True):\n\t        super(AnchorGCN, self).__init__()\n\t        self.nfeat = nfeat\n\t        self.hidden_sizes = [nhid]\n\t        self.nclass = nclass\n\t        self.n_layers = n_layers\n\t        self.dropout = dropout\n", "        self.with_bias = with_bias\n\t        self.batch_norm = batch_norm\n\t        self.layers = nn.ModuleList()\n\t        self.layers.append(AnchorGCNLayer(nfeat, nhid, with_bias=with_bias, batch_norm=batch_norm))\n\t        for _ in range(n_layers - 2):\n\t            self.layers.append(AnchorGCNLayer(nhid, nhid, with_bias=with_bias, batch_norm=batch_norm))\n\t        self.layers.append(AnchorGCNLayer(nhid, nclass, with_bias=with_bias, batch_norm=False))\n\t    def forward(self, x, init_adj, cur_node_anchor_adj, graph_skip_conn, first=True, first_init_agg_vec=None,\n\t                init_agg_vec=None, update_adj_ratio=None, dropout=None, first_node_anchor_adj=None):\n\t        if dropout is None:\n", "            dropout = self.dropout\n\t        # layer 1\n\t        first_vec = self.layers[0](x, cur_node_anchor_adj, anchor_mp=True)\n\t        if first:\n\t            init_agg_vec = self.layers[0](x, init_adj, anchor_mp=False)\n\t        else:\n\t            first_vec = update_adj_ratio * first_vec + (1 - update_adj_ratio) * first_init_agg_vec\n\t        node_vec = (1-graph_skip_conn)*first_vec+graph_skip_conn*init_agg_vec\n\t        if self.batch_norm:\n\t            node_vec = self.layers[0].compute_bn(node_vec)\n", "        node_vec = F.dropout(torch.relu(node_vec), dropout, training=self.training)\n\t        # layer 2-n-1\n\t        for encoder in self.layers[1:-1]:\n\t            mid_cur_agg_vec = encoder(node_vec, cur_node_anchor_adj, anchor_mp=True)\n\t            if not first:\n\t                mid_cur_agg_vec = update_adj_ratio*mid_cur_agg_vec+(1-update_adj_ratio)*encoder(node_vec,first_node_anchor_adj,anchor_mp=True)\n\t            node_vec = (1 - graph_skip_conn) * mid_cur_agg_vec + graph_skip_conn * encoder(node_vec, init_adj, anchor_mp=False)\n\t            if self.batch_norm:\n\t                node_vec = encoder.compute_bn(node_vec)\n\t            node_vec = F.dropout(torch.relu(node_vec), dropout, training=self.training)\n", "        # layer n\n\t        cur_agg_vec = self.layers[-1](node_vec, cur_node_anchor_adj, anchor_mp=True)\n\t        if not first:\n\t            cur_agg_vec = update_adj_ratio * cur_agg_vec + (1-update_adj_ratio) * self.layers[-1](node_vec, first_node_anchor_adj, anchor_mp=True)\n\t        output = (1 - graph_skip_conn) * cur_agg_vec + graph_skip_conn * self.layers[-1](node_vec, init_adj, anchor_mp=False)\n\t        output = F.log_softmax(output, dim=-1).squeeze(1)\n\t        return first_vec, init_agg_vec, node_vec, output\n\tclass GraphLearner(nn.Module):\n\t    def __init__(self, input_size, topk=None, epsilon=None, num_pers=16):\n\t        super(GraphLearner, self).__init__()\n", "        self.topk = topk\n\t        self.epsilon = epsilon\n\t        self.weight_tensor = torch.Tensor(num_pers, input_size)\n\t        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n\t    def forward(self, context, anchor=None):\n\t        # return a new adj according to the representation gived\n\t        expand_weight_tensor = self.weight_tensor.unsqueeze(1)\n\t        if len(context.shape) == 3:\n\t            expand_weight_tensor = expand_weight_tensor.unsqueeze(1)\n\t        context_fc = context.unsqueeze(0) * expand_weight_tensor  # (num_pers, num_node, dim)\n", "        context_norm = F.normalize(context_fc, p=2, dim=-1)\n\t        if anchor is None:\n\t            attention = torch.matmul(context_norm, context_norm.transpose(-1, -2)).mean(0)   # (num_node, num_node)\n\t            markoff_value = 0\n\t        else:\n\t            anchors_fc = anchor.unsqueeze(0) * expand_weight_tensor\n\t            anchors_norm = F.normalize(anchors_fc, p=2, dim=-1)\n\t            attention = torch.matmul(context_norm, anchors_norm.transpose(-1, -2)).mean(0)  # (num_node, num_anchor)\n\t            markoff_value = 0\n\t        if self.epsilon is not None:\n", "            attention = self.build_epsilon_neighbourhood(attention, self.epsilon, markoff_value)\n\t        if self.topk is not None:\n\t            attention = self.build_knn_neighbourhood(attention, self.topk, markoff_value)\n\t        return attention\n\t    def build_knn_neighbourhood(self, attention, topk, markoff_value):\n\t        device = attention.device\n\t        topk = min(topk, attention.size(-1))\n\t        knn_val, knn_ind = torch.topk(attention, topk, dim=-1)\n\t        weighted_adjacency_matrix = (markoff_value * torch.ones_like(attention)).scatter_(-1, knn_ind, knn_val).to(device)\n\t        return weighted_adjacency_matrix\n", "    def build_epsilon_neighbourhood(self, attention, epsilon, markoff_value):\n\t        mask = (attention > epsilon).detach().float()\n\t        weighted_adjacency_matrix = attention * mask + markoff_value * (1 - mask)\n\t        return weighted_adjacency_matrix\n\tclass IDGL(nn.Module):\n\t    def __init__(self, conf, nfeat, nclass):\n\t        super(IDGL, self).__init__()\n\t        self.nfeat = nfeat\n\t        self.nclass = nclass\n\t        self.hidden_size = conf.model['n_hidden']\n", "        self.dropout = conf.model['dropout']\n\t        self.scalable_run = conf.model['scalable_run'] if 'scalable_run' in conf.model else False\n\t        self.feat_adj_dropout = conf.gsl['feat_adj_dropout']\n\t        gcn_module = AnchorGCN if self.scalable_run else GCN\n\t        self.encoder = gcn_module(nfeat=nfeat, nhid=conf.model['n_hidden'], nclass=nclass, n_layers=conf.model['n_layers'], dropout=conf.model['dropout'], batch_norm=conf.model['norm'])\n\t        self.graph_learner = GraphLearner(nfeat,\n\t                                        topk=conf.gsl['graph_learn_topk'],\n\t                                        epsilon=conf.gsl['graph_learn_epsilon'],\n\t                                        num_pers=conf.gsl['graph_learn_num_pers'])\n\t        self.graph_learner2 = GraphLearner(self.hidden_size,\n", "                                        topk=conf.gsl['graph_learn_topk2'],\n\t                                        epsilon=conf.gsl['graph_learn_epsilon2'],\n\t                                        num_pers=conf.gsl['graph_learn_num_pers'])\n\t    def learn_graph(self, graph_learner, node_features, graph_skip_conn=None, graph_include_self=False, init_adj=None, anchor_features=None):\n\t        device = node_features.device\n\t        if self.scalable_run:\n\t            node_anchor_adj = graph_learner(node_features, anchor_features)\n\t            return node_anchor_adj\n\t        else:\n\t            raw_adj = graph_learner(node_features)\n", "            assert raw_adj.min().item() >= 0\n\t            adj = raw_adj / torch.clamp(torch.sum(raw_adj, dim=-1, keepdim=True), min=1e-12)   # å½ä¸å\n\t            if graph_skip_conn in (0, None):\n\t                if graph_include_self:\n\t                    adj = adj + torch.eye(adj.size(0)).to(device)\n\t            else:\n\t                adj = graph_skip_conn * init_adj + (1 - graph_skip_conn) * adj\n\t            return raw_adj, adj\n\t    def forward(self, node_features, init_adj=None):\n\t        node_features = F.dropout(node_features, self.feat_adj_dropout, training=self.training)\n", "        raw_adj, adj = self.learn_graph(self.graph_learner, node_features, self.graph_skip_conn, init_adj=init_adj)\n\t        adj = F.dropout(adj, self.feat_adj_dropout, training=self.training)\n\t        node_vec = self.encoder(node_features, adj)\n\t        output = F.log_softmax(node_vec, dim=-1).squeeze(1)\n\t        return output, adj\n\tdef sample_anchors(node_vec, s):\n\t    idx = torch.randperm(node_vec.size(0))[:s]\n\t    return node_vec[idx], idx\n\tdef diff(X, Y, Z):\n\t    assert X.shape == Y.shape\n", "    diff_ = torch.sum(torch.pow(X - Y, 2))\n\t    norm_ = torch.sum(torch.pow(Z, 2))\n\t    diff_ = diff_ / torch.clamp(norm_, min=1e-12)\n\t    return diff_\n\tdef compute_anchor_adj(node_anchor_adj, anchor_mask=None):\n\t    '''Can be more memory-efficient'''\n\t    anchor_node_adj = node_anchor_adj.transpose(-1, -2)   # (num_anchor, num_node)\n\t    anchor_norm = torch.clamp(anchor_node_adj.sum(dim=-2), min=1e-12) ** -1\n\t    # anchor_adj = torch.matmul(anchor_node_adj, torch.matmul(torch.diag(anchor_norm), node_anchor_adj))\n\t    anchor_adj = torch.matmul(anchor_node_adj, anchor_norm.unsqueeze(-1) * node_anchor_adj)\n", "    markoff_value = 0\n\t    if anchor_mask is not None:\n\t        anchor_adj = anchor_adj.masked_fill_(1 - anchor_mask.byte().unsqueeze(-1), markoff_value)\n\t        anchor_adj = anchor_adj.masked_fill_(1 - anchor_mask.byte().unsqueeze(-2), markoff_value)\n\t    return anchor_adj"]}
{"filename": "opengsl/method/models/gnn_modules.py", "chunked_list": ["import torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch\n\tfrom torch.nn.functional import one_hot\n\tfrom torch_geometric.nn import GATConv, GATv2Conv\n\tclass SGC(nn.Module):\n\t    def __init__(self, n_feat, n_class, K=2):\n\t        super(SGC, self).__init__()\n\t        self.K = K\n\t        self.W = nn.Linear(n_feat, n_class)\n", "        self.features = None\n\t    def sgc_precompute(self, features, adj):\n\t        for i in range(self.K):\n\t            features = torch.spmm(adj, features)\n\t            self.features = features\n\t    def forward(self, input):\n\t        # adj is normalized and sparse\n\t        # compute feature propagation only once\n\t        x=input[0]\n\t        adj=input[1]\n", "        if self.features is None:\n\t            self.sgc_precompute(x, adj)\n\t        return self.W(self.features).squeeze(1)\n\tclass LPA(nn.Module):\n\t    def __init__(self, n_layers=3, alpha=0.9):\n\t        super(LPA, self).__init__()\n\t        self.n_layers = n_layers\n\t        self.alpha = alpha\n\t    def forward(self, input):\n\t        # adj is normalized(without self-loop) and sparse\n", "        # check the format of y\n\t        y = input[0]\n\t        adj = input[1]\n\t        mask = input[2]\n\t        if not len(y.shape) == 2:\n\t            # convert to one hot labels\n\t            y = one_hot(y).float()\n\t        if mask is not None:\n\t            out = torch.zeros_like(y)\n\t            out[mask] = y[mask]\n", "        res = (1 - self.alpha) * out\n\t        for _ in range(self.n_layers):\n\t            out = torch.spmm(adj, out)\n\t            out = out * self.alpha + res\n\t            out.clamp_(0., 1.)\n\t        return out\n\tclass LINK(nn.Module):\n\t    \"\"\" logistic regression on adjacency matrix \"\"\"\n\t    def __init__(self, num_nodes, out_channels):\n\t        super(LINK, self).__init__()\n", "        self.W = nn.Linear(num_nodes, out_channels)\n\t        self.num_nodes = num_nodes\n\t    def reset_parameters(self):\n\t        self.W.reset_parameters()\n\t    def forward(self, adj):\n\t        logits = self.W(adj)\n\t        return logits.squeeze(1)\n\tclass MLP(nn.Module):\n\t    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n\t    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n", "                 dropout=.5):\n\t        super(MLP, self).__init__()\n\t        self.lins = nn.ModuleList()\n\t        self.bns = nn.ModuleList()\n\t        if num_layers == 1:\n\t            # just linear layer i.e. logistic regression\n\t            self.lins.append(nn.Linear(in_channels, out_channels))\n\t        else:\n\t            self.lins.append(nn.Linear(in_channels, hidden_channels))\n\t            self.bns.append(nn.BatchNorm1d(hidden_channels))\n", "            for _ in range(num_layers - 2):\n\t                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n\t                self.bns.append(nn.BatchNorm1d(hidden_channels))\n\t            self.lins.append(nn.Linear(hidden_channels, out_channels))\n\t        self.dropout = dropout\n\t    def reset_parameters(self):\n\t        for lin in self.lins:\n\t            lin.reset_parameters()\n\t        for bn in self.bns:\n\t            bn.reset_parameters()\n", "    def forward(self, feats):\n\t        x = feats\n\t        for i, lin in enumerate(self.lins[:-1]):\n\t            x = lin(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = self.bns[i](x)\n\t            x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.lins[-1](x)\n\t        return x\n\tclass LINKX(nn.Module):\n", "    \"\"\" \n\t    adapted from \n\t    a = MLP_1(A), x = MLP_2(X), MLP_3(sigma(W_1[a, x] + a + x))\n\t    \"\"\"\n\t    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, num_nodes, dropout=.5, cache=False, inner_activation=False, inner_dropout=False, init_layers_A=1, init_layers_X=1):\n\t        super(LINKX, self).__init__()\n\t        self.mlpA = MLP(num_nodes, hidden_channels, hidden_channels, init_layers_A, dropout=0)\n\t        self.mlpX = MLP(in_channels, hidden_channels, hidden_channels, init_layers_X, dropout=0)\n\t        self.W = nn.Linear(2*hidden_channels, hidden_channels)\n\t        self.mlp_final = MLP(hidden_channels, hidden_channels, out_channels, num_layers, dropout=dropout)\n", "        self.in_channels = in_channels\n\t        self.num_nodes = num_nodes\n\t        self.A = None\n\t        self.inner_activation = inner_activation\n\t        self.inner_dropout = inner_dropout\n\t    def reset_parameters(self):\n\t        self.mlpA.reset_parameters()\n\t        self.mlpX.reset_parameters()\n\t        self.W.reset_parameters()\n\t        self.mlp_final.reset_parameters()\n", "    def forward(self, input):\n\t        feat=input[0]\n\t        A=input[1]\n\t        xA = self.mlpA(A)\n\t        xX = self.mlpX(feat)\n\t        x = torch.cat((xA, xX), axis=-1)\n\t        x = self.W(x)\n\t        if self.inner_dropout:\n\t            x = F.dropout(x)\n\t        if self.inner_activation:\n", "            x = F.relu(x)\n\t        x = F.relu(x + xA + xX)\n\t        x = self.mlp_final(x)\n\t        return x\n\tclass APPNP(nn.Module):\n\t    '''\n\t    APPNP Implementation\n\t    Weight decay on the first layer and dropout on adj are not used.\n\t    '''\n\t    def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, K=10, alpha=.1, spmm_type=0):\n", "        super(APPNP, self).__init__()\n\t        self.lin1 = nn.Linear(in_channels, hidden_channels)\n\t        self.lin2 = nn.Linear(hidden_channels, out_channels)\n\t        self.dropout = dropout\n\t        self.K = K\n\t        self.alpha = alpha\n\t        self.spmm = [torch.spmm, torch.sparse.mm][spmm_type]\n\t    def reset_parameters(self):\n\t        self.lin1.reset_parameters()\n\t        self.lin2.reset_parameters()\n", "    def forward(self, input):\n\t        # adj is normalized and sparse\n\t        x=input[0]\n\t        adj=input[1]\n\t        only_z = input[2] if len(input) > 2 else True\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = F.relu(self.lin1(x))\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.lin2(x)\n\t        z = x\n", "        for i in range(self.K):\n\t            z = (1-self.alpha)*self.spmm(adj,z)+self.alpha*x\n\t        # x = self.prop1(x, edge_index)\n\t        if only_z:\n\t            return z.squeeze(1)\n\t        else:\n\t            return z, z.squeeze(1)\n\tclass GPRGNN(nn.Module):\n\t    def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, dprate=.5, K=10, alpha=.1, init='SGC'):\n\t        super(GPRGNN, self).__init__()\n", "        self.lin1 = nn.Linear(in_channels, hidden_channels)\n\t        self.lin2 = nn.Linear(hidden_channels, out_channels)\n\t        self.dropout = dropout\n\t        self.dprate = dprate\n\t        self.K = K\n\t        self.alpha = alpha\n\t        assert init in ['SGC', 'PPR', 'NPPR', 'Random']\n\t        if init == 'SGC':\n\t            # SGC-like, note that in this case, alpha has to be a integer. It means where the peak at when initializing GPR weights.\n\t            TEMP = torch.zeros(K+1)\n", "            TEMP[0] = 1.0\n\t        elif init == 'PPR':\n\t            # PPR-like\n\t            TEMP = alpha*(1-alpha)**torch.arange(K+1)\n\t            TEMP[-1] = (1-alpha)**K\n\t        elif init == 'NPPR':\n\t            # Negative PPR\n\t            TEMP = (alpha)**torch.arange(K+1)\n\t            TEMP = TEMP/torch.sum(torch.abs(TEMP))\n\t        elif init == 'Random':\n", "            # Random\n\t            bound = torch.sqrt(3/(K+1))\n\t            TEMP = torch.random.uniform(-bound, bound, K+1)\n\t            TEMP = TEMP/torch.sum(torch.abs(TEMP))\n\t        self.temp = nn.Parameter(TEMP)\n\t    def reset_parameters(self):\n\t        self.lin1.reset_parameters()\n\t        self.lin2.reset_parameters()\n\t        torch.nn.init.zeros_(self.temp)\n\t        self.temp.data[self.k] = self.alpha*(1-self.alpha)**torch.arange(self.K+1)\n", "        self.temp.data[-1] = (1-self.alpha)**self.K\n\t    def forward(self, input):\n\t        # adj is normalized and sparse\n\t        x=input[0]\n\t        adj=input[1]\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = F.relu(self.lin1(x))\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = self.lin2(x)\n\t        x = F.dropout(x, p=self.dprate, training=self.training)\n", "        z = x*self.temp[0]\n\t        for i in range(self.K):\n\t            x = torch.spmm(adj,x)\n\t            z = z + self.temp[i+1]*x\n\t        return z.squeeze(1)\n\tclass GAT(nn.Module):\n\t    def __init__(self, n_feat, n_hidden, n_class, n_layers, n_heads, dropout):\n\t        super(GAT, self).__init__()\n\t        self.convs = nn.ModuleList()\n\t        self.convs.append(GATConv(n_feat, n_hidden, heads=n_heads, dropout=dropout))\n", "        for i in range(n_layers-2):\n\t            self.convs.append(GATConv(n_hidden*n_heads, n_hidden, heads=n_heads, dropout=dropout))\n\t        self.convs.append((GATConv(n_hidden*n_heads, n_class, heads=n_heads, dropout=dropout, concat=False)))\n\t        self.dropout = dropout\n\t    def forward(self, input):\n\t        x = input[0]\n\t        edge_index = input[1]\n\t        x = F.dropout(x, training=self.training, p=self.dropout)\n\t        for i in range(len(self.convs)-1):\n\t            x = self.convs[i](x, edge_index)\n", "            x = F.dropout(x, training=self.training, p=self.dropout)\n\t            x = F.elu(x)\n\t        x = self.convs[-1](x, edge_index)\n\t        return x.squeeze(1)"]}
{"filename": "examples/gcn_cora.py", "chunked_list": ["import opengsl\n\tconf = opengsl.config.load_conf(method=\"gcn\", dataset=\"cora\")\n\tdataset = opengsl.data.Dataset(\"cora\", n_splits=1, feat_norm=conf.dataset['feat_norm'])\n\tsolver = opengsl.method.GCNSolver(conf,dataset)\n\texp = opengsl.ExpManager(solver)\n\texp.run(n_runs = 10)\n"]}
{"filename": "examples/customized_gsl.py", "chunked_list": ["import opengsl\n\timport time\n\tfrom copy import deepcopy\n\timport torch\n\tclass GSL_Model(torch.nn.Module):\n\t    def __init__(self, in_dim, output_dim):\n\t        super(GSL_Model, self).__init__()\n\t        self.layer = torch.nn.Linear(in_dim, output_dim)\n\t        \"\"\"\n\t        init\n", "        \"\"\"\n\t    def forward(self, input, adj):\n\t        x = self.layer(input)\n\t        return x\n\tclass GSL(opengsl.method.Solver):\n\t    def __init__(self, conf, dataset):\n\t        '''\n\t        Create a solver for gsl to train, evaluate, test in a run.\n\t        Parameters\n\t        ----------\n", "        conf : config file\n\t        dataset: dataset object containing all things about dataset\n\t        '''\n\t        super().__init__(conf, dataset)\n\t        self.method_name = \"gsl\"\n\t        print(\"Solver Version : [{}]\".format(\"gsl\"))\n\t    def learn(self, debug=False):\n\t        for epoch in range(self.conf.training['n_epochs']):\n\t            improve = ''\n\t            t0 = time.time()\n", "            self.model.train()\n\t            self.optim.zero_grad()\n\t            # forward and backward\n\t            output = self.model(self.feats, self.adj)\n\t            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n\t            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\t            loss_train.backward()\n\t            self.optim.step()\n\t            # Evaluate\n\t            loss_val, acc_val = self.evaluate(self.val_mask)\n", "            # save\n\t            if acc_val > self.result['valid']:\n\t                improve = '*'\n\t                self.weights = deepcopy(self.model.state_dict())\n\t                self.total_time = time.time() - self.start_time\n\t                self.best_val_loss = loss_val\n\t                self.result['valid'] = acc_val\n\t                self.result['train'] = acc_train\n\t            if debug:\n\t                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n", "                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\t        print('Optimization Finished!')\n\t        print('Time(s): {:.4f}'.format(self.total_time))\n\t        loss_test, acc_test = self.test()\n\t        self.result['test'] = acc_test\n\t        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\t        return self.result, 0\n\t    def evaluate(self, test_mask):\n\t        self.model.eval()\n\t        with torch.no_grad():\n", "            output = self.model(self.feats, self.adj)\n\t        logits = output[test_mask]\n\t        labels = self.labels[test_mask]\n\t        loss = self.loss_fn(logits, labels)\n\t        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\t    def test(self):\n\t        self.model.load_state_dict(self.weights)\n\t        return self.evaluate(self.test_mask)\n\t    def set_method(self):\n\t        self.model = GSL_Model(self.dim_feats, self.num_targets).to(self.device)\n", "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n\t                                 weight_decay=self.conf.training['weight_decay'])\n\tif __name__ == \"__main__\":\n\t    conf = opengsl.config.load_conf(path=\"./configs/gsl_cora.yaml\")\n\t    dataset = opengsl.data.Dataset(\"cora\", n_splits=1, feat_norm=conf.dataset['feat_norm'])\n\t    solver = GSL(conf,dataset)\n\t    exp = opengsl.ExpManager(solver)\n\t    exp.run(n_runs = 10)"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# -- Path setup --------------------------------------------------------------\n\t# If extensions (or modules to document with autodoc) are in another directory,\n\t# add these directories to sys.path here. If the directory is relative to the\n\t# documentation root, use os.path.abspath to make it absolute, like shown here.\n\t#\n\timport os\n\timport sys\n\tsys.path.insert(0, os.path.abspath('../../'))\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n", "project = 'OpenGSL'\n\tcopyright = '2023, Zhiyao Zhou, Sheng Zhou, Bochao Mao, Xuanyi Zhou'\n\tauthor = 'Zhiyao Zhou, Sheng Zhou, Bochao Mao, Xuanyi Zhou'\n\trelease = 'v0.0.4'\n\t# -- General configuration ---------------------------------------------------\n\t# Add any Sphinx extension module names here, as strings. They can be\n\t# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n\t# ones.\n\textensions = [\n\t    'sphinx.ext.autodoc',\n", "    'sphinx.ext.autosummary',\n\t    'sphinx.ext.doctest',\n\t    'sphinx.ext.intersphinx',\n\t    'sphinx.ext.mathjax',\n\t    'sphinx.ext.napoleon',\n\t    'sphinx.ext.viewcode',\n\t    'sphinx.ext.githubpages',\n\t]\n\ttemplates_path = ['_templates']\n\texclude_patterns = []\n", "# -- Options for HTML output -------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\timport sphinx_rtd_theme\n\t# html_theme = 'alabaster'\n\thtml_theme = \"sphinx_rtd_theme\"\n\thtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\thtml_static_path = ['_static']"]}
