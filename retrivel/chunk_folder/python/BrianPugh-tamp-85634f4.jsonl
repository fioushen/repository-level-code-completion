{"filename": "build.py", "chunked_list": ["import os\n\timport shutil\n\tfrom pathlib import Path\n\tallowed_to_fail = os.environ.get(\"CIBUILDWHEEL\", \"0\") != \"1\"\n\tprofile = os.environ.get(\"TAMP_PROFILE\", \"0\") == \"1\"\n\tdef build_cython_extensions():\n\t    import Cython.Compiler.Options\n\t    from Cython.Build import build_ext, cythonize\n\t    from Cython.Compiler.Options import get_directive_defaults\n\t    from setuptools import Extension\n", "    from setuptools.dist import Distribution\n\t    Cython.Compiler.Options.annotate = True\n\t    define_macros = []\n\t    if profile:\n\t        print(\"Setting profiling configuration.\")\n\t        directive_defaults = get_directive_defaults()\n\t        directive_defaults[\"linetrace\"] = True\n\t        directive_defaults[\"binding\"] = True\n\t        define_macros.append((\"CYTHON_TRACE\", \"1\"))\n\t    if os.name == \"nt\":  # Windows\n", "        extra_compile_args = [\n\t            \"/O2\",\n\t        ]\n\t    else:  # UNIX-based systems\n\t        extra_compile_args = [\n\t            \"-O3\",\n\t            \"-Werror\",\n\t            \"-Wno-unreachable-code-fallthrough\",\n\t            \"-Wno-deprecated-declarations\",\n\t            \"-Wno-parentheses-equality\",\n", "        ]\n\t    include_dirs = [\"tamp/_c_src/\", \"tamp/\"]\n\t    extensions = [\n\t        Extension(\n\t            \"tamp._c_compressor\",\n\t            [\n\t                \"tamp/_c_src/tamp/common.c\",\n\t                \"tamp/_c_src/tamp/compressor.c\",\n\t                \"tamp/_c_compressor.pyx\",\n\t            ],\n", "            include_dirs=include_dirs,\n\t            extra_compile_args=extra_compile_args,\n\t            language=\"c\",\n\t            define_macros=define_macros,\n\t        ),\n\t        Extension(\n\t            \"tamp._c_decompressor\",\n\t            [\n\t                \"tamp/_c_src/tamp/common.c\",\n\t                \"tamp/_c_src/tamp/decompressor.c\",\n", "                \"tamp/_c_decompressor.pyx\",\n\t            ],\n\t            include_dirs=include_dirs,\n\t            extra_compile_args=extra_compile_args,\n\t            language=\"c\",\n\t            define_macros=define_macros,\n\t        ),\n\t        Extension(\n\t            \"tamp._c_common\",\n\t            [\n", "                \"tamp/_c_common.pyx\",\n\t            ],\n\t            include_dirs=include_dirs,\n\t            extra_compile_args=extra_compile_args,\n\t            language=\"c\",\n\t            define_macros=define_macros,\n\t        ),\n\t    ]\n\t    include_dirs = set()\n\t    for extension in extensions:\n", "        include_dirs.update(extension.include_dirs)\n\t    include_dirs = list(include_dirs)\n\t    ext_modules = cythonize(extensions, include_path=include_dirs, language_level=3, annotate=True)\n\t    dist = Distribution({\"ext_modules\": ext_modules})\n\t    cmd = build_ext(dist)\n\t    cmd.ensure_finalized()\n\t    cmd.run()\n\t    for output in cmd.get_outputs():\n\t        output = Path(output)\n\t        relative_extension = output.relative_to(cmd.build_lib)\n", "        shutil.copyfile(output, relative_extension)\n\ttry:\n\t    build_cython_extensions()\n\texcept Exception:\n\t    if not allowed_to_fail:\n\t        raise\n"]}
{"filename": "tamp/compressor_viper.py", "chunked_list": ["\"\"\"Micropython optimized for performance over readability.\n\t\"\"\"\n\tfrom io import BytesIO\n\timport micropython\n\tfrom micropython import const\n\tfrom . import ExcessBitsError, bit_size, compute_min_pattern_size, initialize_dictionary\n\t# encodes [2, 15] pattern lengths\n\t_HUFFMAN_CODES = b\"\\x00\\x03\\x08\\x0b\\x14$&+KT\\x94\\x95\\xaa'\"\n\t# These bit lengths pre-add the 1 bit for the 0-value is_literal flag.\n\t_HUFFMAN_BITS = b\"\\x02\\x03\\x05\\x05\\x06\\x07\\x07\\x07\\x08\\x08\\x09\\x09\\x09\\x07\"\n", "_FLUSH_CODE = const(0xAB)  # 8 bits\n\tdef _f_write(f, number, size):\n\t    f.write(number.to_bytes(size, \"big\"))\n\tclass Compressor:\n\t    def __init__(\n\t        self,\n\t        f,\n\t        *,\n\t        window=10,\n\t        literal=8,\n", "        dictionary=None,\n\t    ):\n\t        self._close_f_on_close = False\n\t        if not hasattr(f, \"write\"):  # It's probably a path-like object.\n\t            f = open(str(f), \"wb\")\n\t            self._close_f_on_close = True\n\t        self.window_bits = window\n\t        self.literal_bits = literal\n\t        self.min_pattern_size = compute_min_pattern_size(window, literal)\n\t        # Window Buffer\n", "        if dictionary:\n\t            if bit_size(len(dictionary) - 1) != window:\n\t                raise ValueError\n\t            self.window_buf = dictionary\n\t        else:\n\t            self.window_buf = initialize_dictionary(1 << window)\n\t        self.window_pos = 0\n\t        # Input Buffer\n\t        self.input_buf = bytearray(16)\n\t        self.input_size = 0\n", "        self.input_pos = 0\n\t        # Write header\n\t        self.f = f\n\t        self.f_buf = ((window - 8) << 5 | (literal - 5) << 3 | int(bool(dictionary)) << 2) << (22)\n\t        self.f_pos = 8\n\t    @micropython.viper\n\t    def _compress_input_buffer_single(self) -> int:\n\t        bytes_written = 0\n\t        # Viper-ize everything\n\t        input_buf = ptr8(self.input_buf)\n", "        input_size = int(self.input_size)\n\t        input_pos = int(self.input_pos)\n\t        literal_bits = int(self.literal_bits)\n\t        literal_flag = 1 << literal_bits\n\t        f_buf = int(self.f_buf)\n\t        f_pos = int(self.f_pos)\n\t        huffman_bits_ptr8 = ptr8(_HUFFMAN_BITS)\n\t        huffman_codes_ptr8 = ptr8(_HUFFMAN_CODES)\n\t        window_bits = int(self.window_bits)\n\t        window_buf = ptr8(self.window_buf)\n", "        window_size = 1 << int(window_bits)\n\t        window_pos = int(self.window_pos)\n\t        min_pattern_size = int(self.min_pattern_size)\n\t        f = self.f\n\t        # Find largest match\n\t        search_i = int(0)\n\t        match_size = int(0)\n\t        if input_size >= min_pattern_size:\n\t            for window_index in range(window_size - min_pattern_size + 1):\n\t                if input_buf[input_pos] != window_buf[window_index]:\n", "                    continue  # Significant speed short-cut\n\t                input_index = (input_pos + 1) & 0xF\n\t                if input_buf[input_index] != window_buf[window_index + 1]:\n\t                    continue  # Small Speed short-cut\n\t                current_match_size = int(2)\n\t                for k in range(current_match_size, input_size):\n\t                    input_index = (input_pos + k) & 0xF\n\t                    if input_buf[input_index] != window_buf[window_index + k] or window_index + k >= window_size:\n\t                        break\n\t                    current_match_size = k + 1\n", "                if current_match_size > match_size:\n\t                    match_size = current_match_size\n\t                    search_i = window_index\n\t                    if match_size == input_size:\n\t                        break\n\t        # Write out a literal or a token\n\t        if match_size >= min_pattern_size:\n\t            huffman_index = match_size - min_pattern_size\n\t            # Adds up to 9 bits\n\t            f_pos += huffman_bits_ptr8[huffman_index]\n", "            f_buf |= huffman_codes_ptr8[huffman_index] << (30 - f_pos)\n\t            if f_pos >= 16:\n\t                _f_write(f, f_buf >> 14, 2)\n\t                f_buf = (f_buf & 0x3FFF) << 16\n\t                f_pos -= 16\n\t                bytes_written += 2\n\t            # Adds up to 15 bits\n\t            f_pos += window_bits\n\t            f_buf |= search_i << (30 - f_pos)\n\t        else:\n", "            # Adds up to 9 bits\n\t            match_size = 1\n\t            if input_buf[input_pos] >> literal_bits:\n\t                raise ExcessBitsError\n\t            f_pos += literal_bits + 1\n\t            f_buf |= (input_buf[input_pos] | literal_flag) << (30 - f_pos)\n\t        if f_pos >= 16:\n\t            _f_write(f, f_buf >> 14, 2)\n\t            f_buf = (f_buf & 0x3FFF) << 16\n\t            f_pos -= 16\n", "            bytes_written += 2\n\t        for _ in range(match_size):  # Copy pattern into buffer\n\t            window_buf[window_pos] = input_buf[input_pos]\n\t            input_pos = (input_pos + 1) & 0xF\n\t            window_pos = (window_pos + 1) % window_size\n\t        input_size -= match_size\n\t        self.input_size = input_size\n\t        self.input_pos = input_pos\n\t        self.window_pos = window_pos\n\t        self.f_pos = f_pos\n", "        self.f_buf = f_buf\n\t        return bytes_written\n\t    @micropython.viper\n\t    def write(self, data) -> int:\n\t        bytes_written = 0\n\t        data_l = int(len(data))\n\t        data_p = ptr8(data)\n\t        input_buf = ptr8(self.input_buf)\n\t        max_pattern_size = int(self.min_pattern_size) + 13\n\t        for i in range(data_l):\n", "            input_size = int(self.input_size)\n\t            input_pos = int(self.input_pos)\n\t            pos = (input_pos + input_size) & 0xF\n\t            input_buf[pos] = data_p[i]\n\t            input_size += 1\n\t            self.input_size = input_size\n\t            if input_size == max_pattern_size:\n\t                bytes_written += int(self._compress_input_buffer_single())\n\t        return bytes_written\n\t    def flush(self, write_token=True):\n", "        bytes_written = 0\n\t        while self.input_size:\n\t            bytes_written += self._compress_input_buffer_single()\n\t        if self.f_pos > 0 and write_token:\n\t            self.f_pos += 9\n\t            self.f_buf |= _FLUSH_CODE << (30 - self.f_pos)\n\t        while self.f_pos > 0:\n\t            _f_write(self.f, self.f_buf >> 22, 1)\n\t            self.f_buf = (self.f_buf & 0x3FFFFF) << 8\n\t            self.f_pos -= 8\n", "            bytes_written += 1\n\t        self.f_pos = 0\n\t        return bytes_written\n\t    def close(self):\n\t        bytes_written = self.flush(write_token=False)\n\t        if self._close_f_on_close:\n\t            self.f.close()\n\t        return bytes_written\n\t    def __enter__(self):\n\t        return self\n", "    def __exit__(self, exc_type, exc_value, traceback):\n\t        self.close()\n\tclass TextCompressor(Compressor):\n\t    def write(self, data: str) -> int:\n\t        return super().write(data.encode())\n\tdef compress(data, *args, **kwargs):\n\t    with BytesIO() as f:\n\t        cls = TextCompressor if isinstance(data, str) else Compressor\n\t        c = cls(f, *args, **kwargs)\n\t        c.write(data)\n", "        c.flush(write_token=False)\n\t        f.seek(0)\n\t        return f.read()\n"]}
{"filename": "tamp/decompressor_viper.py", "chunked_list": ["from io import BytesIO\n\timport micropython\n\tfrom micropython import const\n\tfrom . import compute_min_pattern_size, initialize_dictionary\n\t_HUFFMAN_TABLE = b\"BBBBBBBBBBBBBBBBeeee\\x8a\\x8bxxffffmmmmTTTTTTTTyy\\x8c\\x8fggggCCCCCCCCCCCCCCCC\"\n\t_FLUSH = const(15)\n\tclass Decompressor:\n\t    def __init__(self, f, *, dictionary=None):\n\t        self._close_f_on_close = False\n\t        if not hasattr(f, \"read\"):  # It's probably a path-like object.\n", "            f = open(str(f), \"rb\")\n\t            self._close_f_on_close = True\n\t        self.f = f\n\t        self.f_buf = 0\n\t        self.f_pos = 0\n\t        # Read Header\n\t        header = self.f.read(1)[0]\n\t        self.w_bits = (header >> 5) + 8\n\t        self.literal_bits = ((header >> 3) & 0b11) + 5\n\t        uses_custom_dictionary = header & 0b100\n", "        reserved = header & 0b10\n\t        more_header_bytes = header & 0b1\n\t        if reserved or more_header_bytes:\n\t            raise NotImplementedError\n\t        if uses_custom_dictionary:\n\t            if not dictionary:\n\t                raise ValueError\n\t            self.w_buf = dictionary\n\t        else:\n\t            self.w_buf = initialize_dictionary(1 << self.w_bits)\n", "        self.w_pos = 0\n\t        self.min_pattern_size = compute_min_pattern_size(self.w_bits, self.literal_bits)\n\t        self.overflow_buf = bytearray(self.min_pattern_size + 13)\n\t        self.overflow_pos = 0\n\t        self.overflow_size = 0\n\t    def readinto(self, buf):\n\t        raise NotImplementedError\n\t    @micropython.viper\n\t    def _decompress_into(self, out) -> int:\n\t        \"\"\"Attempt to fill out, will place into overflow.\"\"\"\n", "        out_capacity = int(len(out))  # const\n\t        out_buf = ptr8(out)\n\t        huffman_table = ptr8(_HUFFMAN_TABLE)\n\t        literal_bits = int(self.literal_bits)\n\t        overflow_buf = self.overflow_buf\n\t        overflow_buf_ptr = ptr8(overflow_buf)\n\t        overflow_pos = int(self.overflow_pos)\n\t        overflow_size = int(self.overflow_size)\n\t        w_buf = self.w_buf\n\t        w_buf_ptr = ptr8(w_buf)\n", "        w_pos = int(self.w_pos)\n\t        w_bits = int(self.w_bits)\n\t        w_mask = (1 << w_bits) - 1\n\t        f_pos = int(self.f_pos)\n\t        f_buf = int(self.f_buf)\n\t        f = self.f\n\t        min_pattern_size = int(self.min_pattern_size)\n\t        int(len(overflow_buf))\n\t        # Copy overflow into out if available\n\t        overflow_copy = int(min(overflow_size, out_capacity))\n", "        for i in range(overflow_copy):\n\t            out[i] = overflow_buf_ptr[overflow_pos + i]\n\t        out_pos = overflow_copy\n\t        overflow_pos += overflow_copy\n\t        overflow_size -= overflow_copy\n\t        full_mask = int(0x3FFFFFFF)\n\t        # Decompress more\n\t        try:\n\t            while out_pos < out_capacity:\n\t                overflow_pos = 0  # overflow_size is also always zero here\n", "                if f_pos == 0:\n\t                    f_buf = int(f.read(1)[0]) << 22  # Will raise IndexError if out of data.\n\t                    f_pos = 8\n\t                # re-use is_literal flag as match_size so we don't need to explicitly set it\n\t                match_size = f_buf >> 29\n\t                # Don't update f_buf & f_pos here, in case there's not enough data available.\n\t                if match_size:\n\t                    if f_pos < (literal_bits + 1):\n\t                        f_buf |= int(f.read(1)[0]) << (22 - f_pos)\n\t                        f_pos += 8\n", "                    # Now update buffers from is_literal\n\t                    f_buf = (f_buf << 1) & full_mask\n\t                    c = f_buf >> (30 - literal_bits)\n\t                    f_buf = (f_buf << literal_bits) & full_mask\n\t                    f_pos -= literal_bits + 1\n\t                    out_buf[out_pos] = c\n\t                    w_buf_ptr[w_pos] = c\n\t                    out_pos += 1\n\t                else:\n\t                    # Read and decode Huffman; we'll need the bits regardless\n", "                    if f_pos < 9:\n\t                        f_buf |= int(f.read(1)[0]) << (22 - f_pos)\n\t                        f_pos += 8\n\t                    if f_buf >> 28:\n\t                        code = (f_buf >> 21) & 0x7F\n\t                        code = 33 if code >= 64 else huffman_table[code]\n\t                        match_size = code & 0xF\n\t                        if match_size == _FLUSH:\n\t                            f_buf = f_pos = 0\n\t                            continue\n", "                        delta = code >> 4\n\t                    else:\n\t                        match_size = 0\n\t                        delta = 1\n\t                    token_bits = 1 + delta + w_bits\n\t                    while f_pos < token_bits:\n\t                        f_buf |= int(f.read(1)[0]) << (22 - f_pos)\n\t                        f_pos += 8\n\t                    # We now absolutely have enough data to decode token.\n\t                    match_size += min_pattern_size\n", "                    index = (f_buf >> (30 - token_bits)) & w_mask\n\t                    f_buf = (f_buf << token_bits) & full_mask\n\t                    f_pos -= token_bits\n\t                    # Copy entire match to overflow\n\t                    for i in range(match_size):\n\t                        overflow_buf_ptr[i] = w_buf_ptr[index + i]\n\t                    overflow_pos = 0\n\t                    overflow_size = match_size\n\t                    # Copy available amount to out_buf\n\t                    for i in range(match_size):\n", "                        if out_pos < out_capacity:\n\t                            out_buf[out_pos] = overflow_buf_ptr[overflow_pos]\n\t                            out_pos += 1\n\t                            overflow_pos += 1\n\t                            overflow_size -= 1\n\t                        # Copy overflow to window\n\t                        w_buf_ptr[(w_pos + i) & w_mask] = overflow_buf_ptr[i]\n\t                w_pos = (w_pos + match_size) & w_mask\n\t        except IndexError:\n\t            pass\n", "        self.w_pos = w_pos\n\t        self.overflow_pos = overflow_pos\n\t        self.overflow_size = overflow_size\n\t        self.f_buf = f_buf\n\t        self.f_pos = f_pos\n\t        return out_pos\n\t    def read(self, size=-1):\n\t        \"\"\"Return at most ``size`` bytes.\"\"\"\n\t        if size == 0:\n\t            out = bytearray()\n", "        elif size < 0:\n\t            # Read into a bunch of chunks, then do a single join.\n\t            chunks = []\n\t            chunk_size = 1024\n\t            decompressed_bytes = 0\n\t            while True:\n\t                chunk = bytearray(chunk_size)\n\t                chunk_decompressed_bytes = self._decompress_into(chunk)\n\t                if chunk_decompressed_bytes != chunk_size:\n\t                    chunk = chunk[:chunk_decompressed_bytes]\n", "                chunks.append(chunk)\n\t                if chunk_decompressed_bytes != chunk_size:\n\t                    break\n\t            out = b\"\".join(chunks)\n\t        else:\n\t            out = bytearray(size)\n\t            decompressed_bytes = self._decompress_into(out)\n\t            if decompressed_bytes != size:\n\t                out = out[:decompressed_bytes]\n\t        return out\n", "    def close(self):\n\t        if self._close_f_on_close:\n\t            self.f.close()\n\t    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_value, traceback):\n\t        self.close()\n\tclass TextDecompressor(Decompressor):\n\t    def read(self, *args, **kwargs) -> str:\n\t        return super().read(*args, **kwargs).decode()\n", "def decompress(data, *args, **kwargs):\n\t    with BytesIO(data) as f:\n\t        d = Decompressor(f, *args, **kwargs)\n\t        return d.read()\n"]}
{"filename": "tamp/__main__.py", "chunked_list": ["from .cli.main import run_app\n\trun_app(prog_name=\"tamp\")\n"]}
{"filename": "tamp/__init__.py", "chunked_list": ["# Don't manually change, let poetry-dynamic-versioning-plugin handle it.\n\t__version__ = \"0.0.0\"\n\tclass ExcessBitsError(Exception):\n\t    \"\"\"Provided data has more bits than expected ``literal`` bits.\"\"\"\n\tdef bit_size(value):\n\t    for i in range(32):\n\t        if not value:\n\t            return i\n\t        value >>= 1\n\t    return -1\n", "def _xorshift32(seed):\n\t    while True:\n\t        seed ^= (seed << 13) & 0xFFFFFFFF\n\t        seed ^= (seed >> 17) & 0xFFFFFFFF\n\t        seed ^= (seed << 5) & 0xFFFFFFFF\n\t        yield seed\n\tdef initialize_dictionary(size, seed=None):\n\t    if seed is None:\n\t        seed = 3758097560\n\t    elif seed == 0:\n", "        return bytearray(size)\n\t    chars = b\" \\x000ei>to<ans\\nr/.\"  # 16 most common chars in dataset\n\t    def _gen_stream(xorshift32):\n\t        for _ in range(size >> 3):\n\t            value = next(xorshift32)\n\t            for _ in range(8):\n\t                yield chars[value & 0x0F]\n\t                value >>= 4\n\t    return bytearray(_gen_stream(_xorshift32(seed)))\n\tdef compute_min_pattern_size(window, literal):\n", "    \"\"\"Compute whether the minimum pattern length should be 2 or 3.\"\"\"\n\t    \"\"\"\n\t    # Easy to understand version; commented out for smaller optimized version;\n\t    if window > 15 or window < 8:\n\t        raise ValueError\n\t    if literal == 5:\n\t        return 2 + (window > 10)\n\t    elif literal == 6:\n\t        return 2 + (window > 12)\n\t    elif literal == 7:\n", "        return 2 + (window > 14)\n\t    elif literal == 8:\n\t        return 2\n\t    else:\n\t        raise ValueError\n\t    \"\"\"\n\t    if not (7 < window < 16 and 4 < literal < 9):\n\t        raise ValueError\n\t    return 2 + (window > (10 + ((literal - 5) << 1)))\n\ttry:\n", "    from .compressor_viper import Compressor, TextCompressor, compress\n\texcept ImportError:\n\t    try:\n\t        from ._c_compressor import Compressor, TextCompressor, compress\n\t    except ImportError:\n\t        try:\n\t            from .compressor import Compressor, TextCompressor, compress\n\t        except ImportError:\n\t            pass\n\ttry:\n", "    from .decompressor_viper import Decompressor, TextDecompressor, decompress\n\texcept ImportError:\n\t    try:\n\t        from ._c_decompressor import Decompressor, TextDecompressor, decompress\n\t    except ImportError:\n\t        try:\n\t            from .decompressor import Decompressor, TextDecompressor, decompress\n\t        except ImportError:\n\t            pass\n\tdef open(f, mode=\"rb\", **kwargs):\n", "    if \"r\" in mode and \"w\" in mode:\n\t        raise ValueError\n\t    if \"r\" in mode:  # Decompressor\n\t        if \"b\" in mode:\n\t            return Decompressor(f, **kwargs)\n\t        else:\n\t            return TextDecompressor(f, **kwargs)\n\t    elif \"w\" in mode:  # Compressor\n\t        if \"b\" in mode:\n\t            return Compressor(f, **kwargs)\n", "        else:\n\t            return TextCompressor(f, **kwargs)\n\t    else:\n\t        raise ValueError\n"]}
{"filename": "tamp/decompressor.py", "chunked_list": ["from io import BytesIO\n\tfrom . import compute_min_pattern_size, initialize_dictionary\n\t_FLUSH = object()\n\t# Each key here are the huffman codes or'd with 0x80\n\t# This is so that each lookup is easy/quick.\n\thuffman_lookup = {\n\t    0b0: 0,\n\t    0b11: 1,\n\t    0b1000: 2,\n\t    0b1011: 3,\n", "    0b10100: 4,\n\t    0b100100: 5,\n\t    0b100110: 6,\n\t    0b101011: 7,\n\t    0b1001011: 8,\n\t    0b1010100: 9,\n\t    0b10010100: 10,\n\t    0b10010101: 11,\n\t    0b10101010: 12,\n\t    0b100111: 13,\n", "    0b10101011: _FLUSH,\n\t}\n\tclass BitReader:\n\t    \"\"\"Reads bits from a stream.\"\"\"\n\t    def __init__(self, f, close_f_on_close=False):\n\t        self.close_f_on_close = close_f_on_close\n\t        self.f = f\n\t        self.clear()\n\t    def read_huffman(self):\n\t        proposed_code = 0\n", "        lookup = huffman_lookup\n\t        read = self.read\n\t        for _ in range(8):\n\t            proposed_code |= read(1)\n\t            try:\n\t                return lookup[proposed_code]\n\t            except KeyError:\n\t                proposed_code <<= 1\n\t        raise RuntimeError(\"Unable to decode huffman code. Should never happen.\")\n\t    def read(self, num_bits):\n", "        while self.bit_pos < num_bits:\n\t            byte = self.f.read(1)\n\t            if not byte:\n\t                raise EOFError\n\t            byte_value = int.from_bytes(byte, \"little\")\n\t            self.buffer |= byte_value << (24 - self.bit_pos)\n\t            self.bit_pos += 8\n\t            if self.backup_buffer is not None and self.backup_bit_pos is not None:\n\t                self.backup_buffer |= byte_value << (24 - self.backup_bit_pos)\n\t                self.backup_bit_pos += 8\n", "        result = self.buffer >> (32 - num_bits)\n\t        mask = (1 << (32 - num_bits)) - 1\n\t        self.buffer = (self.buffer & mask) << num_bits\n\t        self.bit_pos -= num_bits\n\t        return result\n\t    def clear(self):\n\t        self.buffer = 0\n\t        self.bit_pos = 0\n\t        self.backup_buffer = None\n\t        self.backup_bit_pos = None\n", "    def close(self):\n\t        self.f.close()\n\t        if self.close_f_on_close:\n\t            self.f.close()\n\t    def __len__(self):\n\t        return self.bit_pos\n\t    def __enter__(self):\n\t        # Context manager to restore all read bits in a session if any read fails.\n\t        # backup the buffer\n\t        self.backup_buffer = self.buffer\n", "        self.backup_bit_pos = self.bit_pos\n\t        return self\n\t    def __exit__(self, exception_type, exception_value, exception_traceback):\n\t        if exception_type is not None:\n\t            # restore buffers\n\t            self.buffer = self.backup_buffer\n\t            self.bit_pos = self.backup_bit_pos\n\t        self.backup_buffer = None\n\t        self.backup_bit_pos = None\n\tclass RingBuffer:\n", "    def __init__(self, buffer):\n\t        self.buffer = buffer\n\t        self.size = len(buffer)\n\t        self.pos = 0  # Always pointing to the byte-to-be-overwritten\n\t        self.index = self.buffer.index\n\t    def write_byte(self, byte):  # ~10% of time\n\t        self.buffer[self.pos] = byte\n\t        self.pos += 1\n\t        if self.pos == self.size:\n\t            self.pos = 0\n", "    def write_bytes(self, data):\n\t        for byte in data:\n\t            self.write_byte(byte)\n\tclass Decompressor:\n\t    \"\"\"Decompresses a file or stream of tamp-compressed data.\n\t    Can be used as a context manager to automatically handle file\n\t    opening and closing::\n\t        with tamp.Decompressor(\"compressed.tamp\") as f:\n\t            decompressed_data = f.read()\n\t    \"\"\"\n", "    def __init__(self, f, *, dictionary=None):\n\t        \"\"\"\n\t        Parameters\n\t        ----------\n\t        f: Union[file, str]\n\t            File-like object to read compressed bytes from.\n\t        dictionary: bytearray\n\t            Use dictionary inplace as window buffer.\n\t        \"\"\"\n\t        if not hasattr(f, \"read\"):  # It's probably a path-like object.\n", "            f = open(str(f), \"rb\")\n\t            close_f_on_close = True\n\t        else:\n\t            close_f_on_close = False\n\t        self._bit_reader = BitReader(f, close_f_on_close=close_f_on_close)\n\t        # Read Header\n\t        self.window_bits = self._bit_reader.read(3) + 8\n\t        self.literal_bits = self._bit_reader.read(2) + 5\n\t        uses_custom_dictionary = self._bit_reader.read(1)\n\t        reserved = self._bit_reader.read(1)\n", "        more_header_bytes = self._bit_reader.read(1)\n\t        if reserved:\n\t            raise NotImplementedError\n\t        if more_header_bytes:\n\t            raise NotImplementedError\n\t        if uses_custom_dictionary ^ bool(dictionary):\n\t            raise ValueError\n\t        self._window_buffer = RingBuffer(\n\t            buffer=dictionary if dictionary else initialize_dictionary(1 << self.window_bits),\n\t        )\n", "        self.min_pattern_size = compute_min_pattern_size(self.window_bits, self.literal_bits)\n\t        self.overflow = bytearray()\n\t    def read(self, size=-1) -> bytearray:\n\t        \"\"\"Decompresses data to bytes.\n\t        Parameters\n\t        ----------\n\t        size: int\n\t            Maximum number of bytes to return.\n\t            If a negative value is provided, all data will be returned.\n\t            Defaults to ``-1``.\n", "        Returns\n\t        -------\n\t        bytearray\n\t            Decompressed data.\n\t        \"\"\"\n\t        if size < 0:\n\t            size = 0xFFFFFFFF\n\t        if len(self.overflow) > size:\n\t            out = self.overflow[:size]\n\t            self.overflow = self.overflow[size:]\n", "            return out\n\t        elif self.overflow:\n\t            out = self.overflow\n\t            self.overflow = bytearray()\n\t        else:\n\t            out = bytearray()\n\t        while len(out) < size:\n\t            try:\n\t                with self._bit_reader:\n\t                    is_literal = self._bit_reader.read(1)\n", "                    if is_literal:\n\t                        c = self._bit_reader.read(self.literal_bits)\n\t                        self._window_buffer.write_byte(c)\n\t                        out.append(c)\n\t                    else:\n\t                        match_size = self._bit_reader.read_huffman()\n\t                        if match_size is _FLUSH:\n\t                            self._bit_reader.clear()\n\t                            continue\n\t                        match_size += self.min_pattern_size\n", "                        index = self._bit_reader.read(self.window_bits)\n\t                        string = self._window_buffer.buffer[index : index + match_size]\n\t                        self._window_buffer.write_bytes(string)\n\t                        out.extend(string)\n\t                        if len(out) > size:\n\t                            self.overflow[:] = out[size:]\n\t                            out = out[:size]\n\t                            break\n\t            except EOFError:\n\t                break\n", "        return out\n\t    def close(self):\n\t        \"\"\"Closes the input file or stream.\"\"\"\n\t        self._bit_reader.close()\n\t    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_value, traceback):\n\t        self.close()\n\tclass TextDecompressor(Decompressor):\n\t    \"\"\"Decompresses a file or stream of tamp-compressed data into text.\"\"\"\n", "    def read(self, *args, **kwargs) -> str:\n\t        \"\"\"Decompresses data to text.\n\t        Parameters\n\t        ----------\n\t        size: int\n\t            Maximum number of bytes to return.\n\t            If a negative value is provided, all data will be returned.\n\t            Defaults to ``-1``.\n\t        Returns\n\t        -------\n", "        str\n\t            Decompressed text.\n\t        \"\"\"\n\t        return super().read(*args, **kwargs).decode()\n\tdef decompress(data: bytes, *args, **kwargs) -> bytearray:\n\t    \"\"\"Single-call to decompress data.\n\t    Parameters\n\t    ----------\n\t    data: bytes\n\t        Plaintext data to compress.\n", "    *args: tuple\n\t        Passed along to :class:`Decompressor`.\n\t    **kwargs : dict\n\t        Passed along to :class:`Decompressor`.\n\t    Returns\n\t    -------\n\t    bytearray\n\t        Decompressed data.\n\t    \"\"\"\n\t    with BytesIO(data) as f:\n", "        d = Decompressor(f, *args, **kwargs)\n\t        return d.read()\n"]}
{"filename": "tamp/compressor.py", "chunked_list": ["from collections import deque\n\tfrom io import BytesIO\n\ttry:\n\t    from typing import Union\n\texcept ImportError:\n\t    pass\n\tfrom . import ExcessBitsError, bit_size, compute_min_pattern_size, initialize_dictionary\n\ttry:\n\t    from micropython import const\n\texcept ImportError:\n", "    def const(x):\n\t        return x  # noqa: E721\n\t# encodes [min_pattern_bytes, min_pattern_bytes + 13] pattern lengths\n\thuffman_codes = b\"\\x00\\x03\\x08\\x0b\\x14$&+KT\\x94\\x95\\xaa'\"\n\t# These bit lengths pre-add the 1 bit for the 0-value is_literal flag.\n\thuffman_bits = b\"\\x02\\x03\\x05\\x05\\x06\\x07\\x07\\x07\\x08\\x08\\x09\\x09\\x09\\x07\"\n\tFLUSH_CODE = const(0xAB)  # 8 bits\n\tclass BitWriter:\n\t    \"\"\"Writes bits to a stream.\"\"\"\n\t    def __init__(self, f, close_f_on_close=False):\n", "        self.close_f_on_close = close_f_on_close\n\t        self.f = f\n\t        self.buffer = 0  # Basically a uint24\n\t        self.bit_pos = 0\n\t    def write_huffman(self, pattern_size):\n\t        return self.write(huffman_codes[pattern_size], huffman_bits[pattern_size])\n\t    def write(self, bits, num_bits, flush=True):\n\t        bits &= (1 << num_bits) - 1\n\t        self.bit_pos += num_bits\n\t        self.buffer |= bits << (32 - self.bit_pos)\n", "        bytes_written = 0\n\t        if flush:\n\t            while self.bit_pos >= 8:\n\t                byte = self.buffer >> 24\n\t                self.f.write(byte.to_bytes(1, \"big\"))\n\t                self.buffer = (self.buffer & 0xFFFFFF) << 8\n\t                self.bit_pos -= 8\n\t                bytes_written += 1\n\t        return bytes_written\n\t    def flush(self, write_token=True):\n", "        bytes_written = 0\n\t        if self.bit_pos > 0 and write_token:\n\t            bytes_written += self.write(FLUSH_CODE, 9)\n\t        while self.bit_pos > 0:\n\t            byte = (self.buffer >> 24) & 0xFF\n\t            self.f.write(byte.to_bytes(1, \"big\"))\n\t            self.bit_pos = 0\n\t            self.buffer = 0\n\t            bytes_written += 1\n\t        self.f.flush()\n", "        return bytes_written\n\t    def close(self):\n\t        self.flush(write_token=False)\n\t        if self.close_f_on_close:\n\t            self.f.close()\n\tclass RingBuffer:\n\t    def __init__(self, buffer):\n\t        self.buffer = buffer\n\t        self.size = len(buffer)\n\t        self.pos = 0  # Always pointing to the byte-to-be-overwritten\n", "    def write_byte(self, byte):  # ~10% of time\n\t        self.buffer[self.pos] = byte\n\t        self.pos = (self.pos + 1) % self.size\n\t    def write_bytes(self, data):\n\t        for byte in data:\n\t            self.write_byte(byte)\n\t    def index(self, pattern, start):\n\t        return self.buffer.index(pattern, start)\n\tclass Compressor:\n\t    \"\"\"Compresses data to a file or stream.\"\"\"\n", "    def __init__(\n\t        self,\n\t        f,\n\t        *,\n\t        window=10,\n\t        literal=8,\n\t        dictionary=None,\n\t    ):\n\t        \"\"\"\n\t        Parameters\n", "        ----------\n\t        window: int\n\t            Size of window buffer in bits.\n\t            Defaults to 10 (1024 byte buffer).\n\t        literal: int\n\t            Size of literals in bits.\n\t            Defaults to 8.\n\t        dictionary: Optional[bytearray]\n\t            Use the given initialized buffer inplace.\n\t            At decompression time, the same buffer must be provided.\n", "            ``window`` must agree with the dictionary size.\n\t        \"\"\"\n\t        if not hasattr(f, \"write\"):  # It's probably a path-like object.\n\t            # TODO: then close it on close\n\t            f = open(str(f), \"wb\")\n\t            close_f_on_close = True\n\t        else:\n\t            close_f_on_close = False\n\t        self._bit_writer = BitWriter(f, close_f_on_close=close_f_on_close)\n\t        if dictionary and bit_size(len(dictionary) - 1) != window:\n", "            raise ValueError(\"Dictionary-window size mismatch.\")\n\t        self.window_bits = window\n\t        self.literal_bits = literal\n\t        self.min_pattern_size = compute_min_pattern_size(window, literal)\n\t        self.max_pattern_size = self.min_pattern_size + 13\n\t        self.literal_flag = 1 << self.literal_bits\n\t        self._window_buffer = RingBuffer(\n\t            buffer=dictionary if dictionary else initialize_dictionary(1 << window),\n\t        )\n\t        self._input_buffer = deque(maxlen=self.max_pattern_size)\n", "        # Callbacks for debugging/metric collection; can be externally set.\n\t        self.token_cb = None\n\t        self.literal_cb = None\n\t        self.flush_cb = None\n\t        # Write header\n\t        self._bit_writer.write(window - 8, 3, flush=False)\n\t        self._bit_writer.write(literal - 5, 2, flush=False)\n\t        self._bit_writer.write(bool(dictionary), 1, flush=False)\n\t        self._bit_writer.write(0, 1, flush=False)  # Reserved\n\t        self._bit_writer.write(0, 1, flush=False)  # No other header bytes\n", "    def _compress_input_buffer_single(self) -> int:\n\t        target = bytes(self._input_buffer)\n\t        bytes_written = 0\n\t        search_i = 0\n\t        match_size = 1\n\t        for match_size in range(self.min_pattern_size, len(target) + 1):\n\t            match = target[:match_size]\n\t            try:\n\t                search_i = self._window_buffer.index(match, search_i)\n\t            except ValueError:\n", "                # Not Found\n\t                match_size -= 1\n\t                break\n\t        match = target[:match_size]\n\t        if match_size >= self.min_pattern_size:\n\t            if self.token_cb:\n\t                self.token_cb(\n\t                    search_i,\n\t                    match_size,\n\t                    match,\n", "                )\n\t            bytes_written += self._bit_writer.write_huffman(match_size - self.min_pattern_size)\n\t            bytes_written += self._bit_writer.write(search_i, self.window_bits)\n\t            self._window_buffer.write_bytes(match)\n\t            for _ in range(match_size):\n\t                self._input_buffer.popleft()\n\t        else:\n\t            char = self._input_buffer.popleft()\n\t            if self.literal_cb:\n\t                self.literal_cb(char)\n", "            if char >> self.literal_bits:\n\t                raise ExcessBitsError\n\t            bytes_written += self._bit_writer.write(char | self.literal_flag, self.literal_bits + 1)\n\t            self._window_buffer.write_byte(char)\n\t        return bytes_written\n\t    def write(self, data: bytes) -> int:\n\t        \"\"\"Compress ``data`` to stream.\n\t        Parameters\n\t        ----------\n\t        data: bytes\n", "            Data to be compressed.\n\t        Returns\n\t        -------\n\t        int\n\t            Number of compressed bytes written.\n\t            May be zero when data is filling up internal buffers.\n\t        \"\"\"\n\t        bytes_written = 0\n\t        for char in data:\n\t            self._input_buffer.append(char)\n", "            if len(self._input_buffer) == self._input_buffer.maxlen:\n\t                bytes_written += self._compress_input_buffer_single()\n\t        return bytes_written\n\t    def flush(self, write_token: bool = True) -> int:\n\t        \"\"\"Flushes internal buffers.\n\t        Parameters\n\t        ----------\n\t        write_token: bool\n\t            If appropriate, write a ``FLUSH`` token.\n\t            Defaults to ``True``.\n", "        Returns\n\t        -------\n\t        int\n\t            Number of compressed bytes flushed.\n\t        \"\"\"\n\t        bytes_written = 0\n\t        if self.flush_cb:\n\t            self.flush_cb()\n\t        while self._input_buffer:\n\t            bytes_written += self._compress_input_buffer_single()\n", "        bytes_written += self._bit_writer.flush(write_token=write_token)\n\t        return bytes_written\n\t    def close(self) -> int:\n\t        \"\"\"Flushes internal buffers and close the output file or stream.\n\t        Returns\n\t        -------\n\t        int\n\t            Number of compressed bytes flushed.\n\t        \"\"\"\n\t        bytes_written = 0\n", "        bytes_written += self.flush(write_token=False)\n\t        self._bit_writer.close()\n\t        return bytes_written\n\t    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_value, traceback):\n\t        self.close()\n\tclass TextCompressor(Compressor):\n\t    \"\"\"Compresses text to a file or stream.\"\"\"\n\t    def write(self, data: str) -> int:\n", "        return super().write(data.encode())\n\tdef compress(data: Union[bytes, str], *args, **kwargs) -> bytes:\n\t    \"\"\"Single-call to compress data.\n\t    Parameters\n\t    ----------\n\t    data: Union[str, bytes]\n\t        Data to compress.\n\t    *args: tuple\n\t        Passed along to :class:`Compressor`.\n\t    **kwargs : dict\n", "        Passed along to :class:`Compressor`.\n\t    Returns\n\t    -------\n\t    bytes\n\t        Compressed data\n\t    \"\"\"\n\t    with BytesIO() as f:\n\t        if isinstance(data, str):\n\t            c = TextCompressor(f, *args, **kwargs)\n\t            c.write(data)\n", "        else:\n\t            c = Compressor(f, *args, **kwargs)\n\t            c.write(data)\n\t        c.flush(write_token=False)\n\t        f.seek(0)\n\t        return f.read()\n"]}
{"filename": "tamp/cli/main.py", "chunked_list": ["import sys\n\tfrom pathlib import Path\n\tfrom typing import Callable, Optional\n\timport typer\n\tfrom typer import Argument, Option\n\timport tamp\n\tapp = typer.Typer(no_args_is_help=True, pretty_exceptions_enable=False, add_completion=False)\n\tdef version_callback(value: bool):\n\t    if not value:\n\t        return\n", "    print(tamp.__version__)\n\t    raise typer.Exit()\n\t@app.callback()\n\tdef common(\n\t    version: bool = Option(\n\t        None,\n\t        \"--version\",\n\t        \"-v\",\n\t        callback=version_callback,\n\t        help=\"Print tamp version.\",\n", "    ),\n\t):\n\t    pass\n\tdef read(input_: Optional[Path]) -> bytes:\n\t    data = sys.stdin.buffer.read() if input_ is None else input_.read_bytes()\n\t    if not data:\n\t        raise ValueError(\"No data provided.\")\n\t    return data\n\tdef write(output: Optional[Path], data: bytes):\n\t    if output is None:\n", "        sys.stdout.buffer.write(data)\n\t    else:\n\t        output.write_bytes(data)\n\t@app.command()\n\tdef compress(\n\t    input_path: Optional[Path] = Argument(\n\t        None,\n\t        exists=True,\n\t        readable=True,\n\t        dir_okay=False,\n", "        show_default=False,\n\t        help=\"Input file to compress or decompress. Defaults to stdin.\",\n\t    ),\n\t    output_path: Optional[Path] = Option(\n\t        None,\n\t        \"--output\",\n\t        \"-o\",\n\t        exists=False,\n\t        writable=True,\n\t        dir_okay=True,\n", "        show_default=False,\n\t        help=\"Output file. Defaults to stdout.\",\n\t    ),\n\t    window: int = Option(\n\t        10,\n\t        \"-w\",\n\t        \"--window\",\n\t        min=8,\n\t        max=15,\n\t        help=\"Number of bits used to represent the dictionary window.\",\n", "    ),\n\t    literal: int = Option(\n\t        8,\n\t        \"-l\",\n\t        \"--literal\",\n\t        min=5,\n\t        max=8,\n\t        help=\"Number of bits used to represent a literal.\",\n\t    ),\n\t):\n", "    \"\"\"Compress an input file or stream.\"\"\"\n\t    input_bytes = read(input_path)\n\t    output_bytes = tamp.compress(\n\t        input_bytes,\n\t        window=window,\n\t        literal=literal,\n\t    )\n\t    write(output_path, output_bytes)\n\t@app.command()\n\tdef decompress(\n", "    input_path: Optional[Path] = Argument(\n\t        None,\n\t        exists=True,\n\t        readable=True,\n\t        dir_okay=False,\n\t        show_default=False,\n\t        help=\"Input file. If not provided, reads from stdin.\",\n\t    ),\n\t    output_path: Optional[Path] = Option(\n\t        None,\n", "        \"--output\",\n\t        \"-o\",\n\t        exists=False,\n\t        writable=True,\n\t        dir_okay=True,\n\t        show_default=False,\n\t        help=\"Output file. Defaults to stdout.\",\n\t    ),\n\t):\n\t    \"\"\"Decompress an input file or stream.\"\"\"\n", "    input_bytes = read(input_path)\n\t    output_bytes = tamp.decompress(input_bytes)\n\t    write(output_path, output_bytes)\n\tdef run_app(*args, **kwargs):\n\t    app(*args, **kwargs)\n"]}
{"filename": "tamp/cli/__init__.py", "chunked_list": []}
{"filename": "tools/on-device-compression-benchmark.py", "chunked_list": ["\"\"\"Micropython code to be ran on-device.\n\t\"\"\"\n\timport uprofiler\n\tuprofiler.print_period = 0\n\timport tamp\n\t@uprofiler.profile\n\tdef main():\n\t    block_size = 1024\n\t    decompressed_len = 0\n\t    compressed_len = 0\n", "    with open(\"enwik8-100kb\", \"rb\") as input_file, tamp.open(\"enwik8-100kb.tamp\", \"wb\") as compressed_f:\n\t        while chunk := input_file.read(block_size):\n\t            decompressed_len += len(chunk)\n\t            compressed_len += compressed_f.write(chunk)\n\t        compressed_len += compressed_f.flush(write_token=False)\n\t    print(f\"{decompressed_len=:,}\")\n\t    print(f\"{compressed_len=:,}\")\n\tif __name__ == \"__main__\":\n\t    main()\n\t    uprofiler.print_results()\n"]}
{"filename": "tools/analysis.py", "chunked_list": ["import argparse\n\timport pickle\n\tfrom pathlib import Path\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\tfrom huffman import huffman_coding\n\tdef _generate_ticks(data, num_ticks=20):\n\t    step_size = (len(data) - 1) / (num_ticks - 1)\n\t    xticks_positions = [round(i * step_size) for i in range(num_ticks)]\n\t    xticks_labels = [str(int(pos)) for pos in xticks_positions]\n", "    return (xticks_positions, xticks_labels)\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"window_bits\", type=int)\n\t    parser.add_argument(\"--plot\", action=\"store_true\")\n\t    parser.add_argument(\"--pdb\", action=\"store_true\")\n\t    args = parser.parse_args()\n\t    with Path(f\"build/results-w{args.window_bits}.pkl\").open(\"rb\") as f:\n\t        r = pickle.load(f)  # noqa: S301\n\t        for k, v in r.items():\n", "            if isinstance(v, list):\n\t                r[k] = np.array(v)\n\t    r[\"token_sizes\"] = r[\"token_sizes\"] / r[\"token_sizes\"].sum()\n\t    r[\"token_distances\"] = r[\"token_distances\"] / r[\"token_distances\"].sum()\n\t    for k, v in r.items():\n\t        if isinstance(v, np.ndarray):\n\t            continue\n\t        print(f\"{k}: {v:,}\")\n\t    literal_prob = r[\"n_literals\"] / (r[\"n_literals\"] + r[\"n_tokens\"])\n\t    print(f\"{literal_prob=}\")\n", "    min_pattern_size = r[\"token_sizes\"].nonzero()[0].min()\n\t    max_pattern_size = r[\"token_sizes\"].nonzero()[0].max()\n\t    probs = {i: r[\"token_sizes\"][i] for i in range(min_pattern_size, max_pattern_size + 1)}\n\t    huffman_codes = huffman_coding(probs)\n\t    bits_per_symbols = {}\n\t    shortest_symbol_size = 100\n\t    for k, v in sorted(huffman_codes.items()):\n\t        symbol_size = len(v)\n\t        bits_per_symbols[k] = symbol_size\n\t        if symbol_size < shortest_symbol_size:\n", "            shortest_symbol_size = symbol_size\n\t    # if huffman_codes[shortest_symbol] == \"1\":\n\t    #    # Invert all codes;\n\t    #    for symbol, code in huffman_codes.items():\n\t    #        huffman_codes[symbol] = code.replace(\"0\",\"Z\").replace(\"1\", \"0\").replace(\"Z\", \"1\")\n\t    average_bits_per_symbol = 0\n\t    for code in bits_per_symbols:\n\t        average_bits_per_symbol += bits_per_symbols[code] * probs[code]\n\t    print(f\"Huffman pattern size code: {average_bits_per_symbol=}\")\n\t    print(\"Huffman codes:\")\n", "    huffman_code_array = b\"\"\n\t    huffman_bit_size_array = []\n\t    for char, code in sorted(huffman_codes.items()):\n\t        print(f\"{char}: {code}\")\n\t        huffman_code_array += int(code, 2).to_bytes(1, \"little\")\n\t        huffman_bit_size_array.append(len(code) + 1)  # plus 1 for implicit is_literal flag.\n\t    huffman_bit_size_array = tuple(huffman_bit_size_array)\n\t    print(f\"{huffman_bit_size_array=}\")\n\t    print(f\"{huffman_code_array=}\")\n\t    offset_weighted_average = np.dot(r[\"token_distances\"], np.arange(len(r[\"token_distances\"])))\n", "    print(f\"50% of offsets are under {offset_weighted_average}.\")\n\t    if args.pdb:\n\t        breakpoint()\n\t    if args.plot:\n\t        plt.rc(\"font\", size=30)\n\t        plt.subplot(1, 2, 1)\n\t        data = r[\"token_distances\"]\n\t        indices = range(len(data))\n\t        plt.bar(indices, data, width=1)\n\t        plt.xticks(*_generate_ticks(data))\n", "        plt.xlabel(\"Offset\")\n\t        plt.ylabel(\"Occurrence\")\n\t        plt.title(f\"Token Distances (w={args.window_bits})\")\n\t        plt.subplot(1, 2, 2)\n\t        indices = np.array(sorted(huffman_codes.keys()))\n\t        data = [probs[x] for x in indices]\n\t        plt.bar(indices, data, width=1, align=\"center\")\n\t        plt.yticks(np.arange(0, 0.55, 0.05))\n\t        plt.xticks(\n\t            indices,\n", "            indices,\n\t        )\n\t        plt.xlim(0, max(indices))\n\t        plt.xlabel(\"Match Size\")\n\t        plt.ylabel(\"Occurrence\")\n\t        plt.title(f\"Match Sizes (enwik8, w={args.window_bits})\")\n\t        plt.grid(True)\n\t        plt.show()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/on-device-decompression-benchmark.py", "chunked_list": ["\"\"\"Micropython code to be ran on-device.\n\t\"\"\"\n\timport uprofiler\n\tuprofiler.print_period = 0\n\timport tamp\n\t@uprofiler.profile\n\tdef main():\n\t    block_size = 1024\n\t    decompressed_len = 0\n\t    with tamp.open(\"enwik8-100kb.tamp\", \"rb\") as input_file, open(\"enwik8-100kb-decompressed\", \"wb\") as output_file:\n", "        while True:\n\t            chunk = input_file.read(block_size)\n\t            decompressed_len += len(chunk)\n\t            output_file.write(chunk)\n\t            if len(chunk) < block_size:\n\t                break\n\t    print(f\"{decompressed_len=:,}\")\n\tif __name__ == \"__main__\":\n\t    main()\n\t    uprofiler.print_results()\n"]}
{"filename": "tools/zlib_decompress.py", "chunked_list": ["import sys\n\timport zlib\n\tdef main():\n\t    compressed_data = sys.stdin.buffer.read()\n\t    zlib.decompress(compressed_data)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/zlib_compress.py", "chunked_list": ["import argparse\n\timport sys\n\timport zlib\n\tfrom pathlib import Path\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"input\", type=Path)\n\t    args = parser.parse_args()\n\t    with args.input.open(\"rb\") as f:\n\t        decompressed_data = f.read()\n", "        compressobj = zlib.compressobj(level=9, wbits=10, memLevel=1, strategy=zlib.Z_DEFAULT_STRATEGY)\n\t        compressed_data = compressobj.compress(decompressed_data)\n\t        compressed_data += compressobj.flush()\n\t    sys.stdout.buffer.write(compressed_data)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/collect-data.py", "chunked_list": ["import argparse\n\timport pickle\n\timport time\n\tfrom io import BytesIO\n\tfrom pathlib import Path\n\tfrom tamp.compressor import Compressor\n\tdef timeit(func):\n\t    def wrapper(*args, **kwargs):\n\t        start = time.time()\n\t        result = func(*args, **kwargs)\n", "        end = time.time()\n\t        print(f\"Function {func.__name__} took {end - start:.5f} seconds to execute.\")\n\t        return result\n\t    return wrapper\n\t@timeit\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"window_bits\", type=int)\n\t    args = parser.parse_args()\n\t    window_size = 1 << args.window_bits\n", "    decompressed = Path(\"build/enwik8\").read_bytes()\n\t    results = {\n\t        \"n_literals\": 0,\n\t        \"n_tokens\": 0,\n\t        \"n_flushes\": 0,\n\t        \"token_distances\": [0] * window_size,\n\t        \"token_sizes\": [0] * 20,\n\t        \"decompressed_size\": 0,\n\t        \"compressed_size\": 0,\n\t        \"ratio\": 0,\n", "    }\n\t    def token_cb(offset, match_size, string):\n\t        results[\"n_tokens\"] += 1\n\t        results[\"token_distances\"][offset] += 1\n\t        results[\"token_sizes\"][match_size] += 1\n\t    def literal_cb(char):\n\t        results[\"n_literals\"] += 1\n\t    def flush_cb():\n\t        results[\"n_flushes\"] += 1\n\t    with BytesIO() as compressed_out:\n", "        compressor = Compressor(\n\t            compressed_out,\n\t            window=args.window_bits,\n\t        )\n\t        compressor.token_cb = token_cb\n\t        compressor.literal_cb = literal_cb\n\t        compressor.flush_cb = flush_cb\n\t        compressor.write(decompressed)\n\t        compressor.flush()\n\t        results[\"decompressed_size\"] = len(decompressed)\n", "        results[\"compressed_size\"] = compressed_out.tell()\n\t        results[\"ratio\"] = results[\"decompressed_size\"] / results[\"compressed_size\"]\n\t    with Path(f\"build/results-w{args.window_bits}.pkl\").open(\"wb\") as f:\n\t        pickle.dump(results, f)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/huffman.py", "chunked_list": ["import heapq\n\tfrom typing import Any, Dict, Union\n\tclass Node:\n\t    def __init__(self, char, prob):\n\t        self.char = char\n\t        self.prob = prob\n\t        self.left = None\n\t        self.right = None\n\t    def __lt__(self, other):\n\t        return self.prob < other.prob\n", "def create_priority_queue(probs):\n\t    queue = []\n\t    if not isinstance(probs, dict):\n\t        probs = dict(zip(range(len(probs)), probs))\n\t    for token, prob in probs.items():\n\t        heapq.heappush(queue, Node(token, prob))\n\t    return queue\n\tdef build_huffman_tree(queue):\n\t    while len(queue) > 1:\n\t        left = heapq.heappop(queue)\n", "        right = heapq.heappop(queue)\n\t        internal_node = Node(None, left.prob + right.prob)\n\t        internal_node.left = left\n\t        internal_node.right = right\n\t        heapq.heappush(queue, internal_node)\n\t    return heapq.heappop(queue)\n\tdef generate_huffman_codes(node, code=\"\"):\n\t    if node is None:\n\t        return {}\n\t    if node.char is not None:\n", "        return {node.char: code}\n\t    codes = {}\n\t    codes.update(generate_huffman_codes(node.left, code + \"0\"))\n\t    codes.update(generate_huffman_codes(node.right, code + \"1\"))\n\t    return codes\n\tdef huffman_coding(probs: Union[list, Dict[Any, float]]) -> Dict[int, str]:\n\t    priority_queue = create_priority_queue(probs)\n\t    huffman_tree_root = build_huffman_tree(priority_queue)\n\t    huffman_codes = generate_huffman_codes(huffman_tree_root)\n\t    return huffman_codes\n"]}
{"filename": "tools/huffman_jump_table.py", "chunked_list": ["\"\"\"Goal to increase hufman decoding speed via lookup table.\n\t1. Look at first bit, if it's 0, then stop decoding and return 0. Maybe do the samething for 2nd bit.\n\t2. Else, read the next 7 bits, and use the value to index into a table.\n\t3. Use the upper 4 bits to express the number of bits decoded.\n\t4. Use the lower 4 bits to express the decoded value.\n\t\"\"\"\n\t_FLUSH = 15\n\ttable_bits = 7\n\thuffman_lookup = {\n\t    \"1\": 1,\n", "    \"000\": 2,\n\t    \"011\": 3,\n\t    \"0100\": 4,\n\t    \"00100\": 5,\n\t    \"00110\": 6,\n\t    \"01011\": 7,\n\t    \"001011\": 8,\n\t    \"010100\": 9,\n\t    \"0010100\": 10,\n\t    \"0010101\": 11,\n", "    \"0101010\": 12,\n\t    \"00111\": 13,\n\t    \"0101011\": _FLUSH,\n\t}\n\ttable_size = 1 << table_bits\n\tUNPOPULATED = object()\n\tc_table = [UNPOPULATED] * table_size\n\tpy_table = [UNPOPULATED] * table_size\n\tfor k, v in huffman_lookup.items():\n\t    n_bits = len(k)\n", "    n_pad = table_bits - n_bits\n\t    for j in range(1 << n_pad):\n\t        index_bin_str = k\n\t        if n_pad:\n\t            index_bin_str += format(j, f\"0{n_pad}b\")\n\t        index = int(index_bin_str, 2)\n\t        c_table[index] = v | (n_bits << 4)\n\t        py_table[index] = v | ((n_bits + 1) << 4)\n\tprint(f\"const uint8_t HUFFMAN_TABLE[{table_size}] = {{{str(c_table)[1:-1]}}};\")\n\tprint(f\"_HUFFMAN_TABLE = {bytes(py_table)}\")\n"]}
{"filename": "tools/find_seed.py", "chunked_list": ["import argparse\n\timport collections\n\timport contextlib\n\timport multiprocessing\n\timport random\n\tfrom io import BytesIO\n\tfrom pathlib import Path\n\tfrom tqdm import tqdm\n\tfrom tamp import initialize_dictionary\n\tfrom tamp.compressor import Compressor\n", "def random_slices(data, num_slices, slice_size):\n\t    slices = []\n\t    for _ in range(num_slices):\n\t        start_index = random.randint(0, len(data) - slice_size)  # noqa: S311\n\t        end_index = start_index + slice_size\n\t        slices.append(data[start_index:end_index])\n\t    return slices\n\tdef compute_size(seed, data):\n\t    size = 0\n\t    for window_bits, chunks in data.items():\n", "        for chunk in chunks:\n\t            with BytesIO() as f:\n\t                dictionary = initialize_dictionary(1 << window_bits, seed=seed)\n\t                compressor = Compressor(f, window=window_bits, dictionary=dictionary)\n\t                compressor.write(chunk)\n\t                compressor.flush()\n\t                size += f.tell()\n\t    return size\n\tdef find_seed(best_seed, best_size, lock, data, start_index, end_index):\n\t    random.seed()\n", "    with contextlib.suppress(KeyboardInterrupt):\n\t        generator = range(start_index, end_index)\n\t        if start_index == 0:\n\t            generator = tqdm(generator)\n\t        for seed in generator:\n\t            size = compute_size(seed, data)\n\t            with lock:\n\t                if size < best_size.value:\n\t                    best_seed.value = seed\n\t                    best_size.value = size\n", "                    print(f\"{seed=} {size=}\")\n\tdef read_data():\n\t    input_folder = Path(\"build/silesia\")\n\t    files = list(input_folder.glob(\"*\"))\n\t    data_list = [x.read_bytes() for x in files]\n\t    return data_list\n\tdef generate_data(data_list, chunks_per_source):\n\t    sliced_data = {\n\t        8: [],\n\t        9: [],\n", "        10: [],\n\t    }\n\t    for data in data_list:\n\t        for k in sliced_data:\n\t            sliced_data[k].extend(random_slices(data, chunks_per_source, 1 << k))\n\t    return sliced_data\n\tdef character_finder(data_list, n):\n\t    counter = collections.Counter(b\"\".join(data_list))\n\t    most_common = counter.most_common(n)\n\t    common_bytes = bytes(x[0] for x in most_common)\n", "    print(f\"{common_bytes=}\")\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--seed\", type=int, default=-1, help=\"Confirm seed performance.\")\n\t    parser.add_argument(\"--character-finder\", type=int, default=-1)\n\t    parser.add_argument(\"--processes\", type=int, default=8)\n\t    args = parser.parse_args()\n\t    chunks_per_source = 200\n\t    random.seed(100)\n\t    data_list = read_data()\n", "    sliced_data = generate_data(data_list, chunks_per_source)\n\t    uncompressed_size = 0\n\t    total_chunks = 0\n\t    for v in sliced_data.values():\n\t        uncompressed_size += len(b\"\".join(v))\n\t        total_chunks += len(v)\n\t    print(f\"{uncompressed_size=} {total_chunks=}\")\n\t    if args.seed >= 0:\n\t        seed = args.seed\n\t        size = compute_size(args.seed, sliced_data)\n", "        print(f\"{seed=} {size=}\")\n\t        return\n\t    if args.character_finder >= 0:\n\t        character_finder(data_list, args.character_finder)\n\t        return\n\t    shared_best_seed = multiprocessing.Value(\"i\", 0)\n\t    shared_best_size = multiprocessing.Value(\"i\", 10000000000000)\n\t    lock = multiprocessing.Lock()\n\t    intervals = list(range(0, 0xFFFFFFFF, 0xFFFFFFFF // args.processes))\n\t    processes = []\n", "    for start_index, end_index in zip(intervals[:-1], intervals[1:]):\n\t        processes.append(\n\t            multiprocessing.Process(\n\t                target=find_seed,\n\t                args=(\n\t                    shared_best_seed,\n\t                    shared_best_size,\n\t                    lock,\n\t                    sliced_data,\n\t                    start_index,\n", "                    end_index,\n\t                ),\n\t            )\n\t        )\n\t    with contextlib.suppress(KeyboardInterrupt):\n\t        for process in processes:\n\t            process.start()\n\t        for process in processes:\n\t            process.join()\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "tools/profiler.py", "chunked_list": ["from io import BytesIO\n\timport line_profiler\n\timport tamp\n\timport tamp._c_compressor\n\tprofile = line_profiler.LineProfiler(tamp.Compressor.write)\n\twith open(\"build/enwik8\", \"rb\") as f, BytesIO() as f_compressed:\n\t    data = f.read()\n\t    compressor = tamp._c_compressor.Compressor(f_compressed)\n\t    profile.runcall(compressor.write, data)\n\tprofile.print_stats()\n"]}
{"filename": "tools/micropython-compression-benchmark.py", "chunked_list": ["from pathlib import Path\n\timport tamp\n\tchunk_size = 1 << 20\n\tcompressor = tamp.Compressor(\"build/enwik8.tamp\")\n\twith open(\"build/enwik8\", \"rb\") as f:\n\t    i = 0\n\t    while True:\n\t        i += 1\n\t        print(f\"Chunk {i}\")\n\t        chunk = f.read(chunk_size)\n", "        if not chunk:\n\t            break\n\t        compressor.write(chunk)\n\t    compressor.flush(write_token=False)\n\tprint(\"Complete!\")\n"]}
{"filename": ".belay/dependencies/dev/fnmatch/__init__.py", "chunked_list": ["\"\"\"Filename matching with shell patterns.\n\tfnmatch(FILENAME, PATTERN) matches according to the local convention.\n\tfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\tThe functions operate by translating the pattern into a regular\n\texpression.  They cache the compiled regular expressions for speed.\n\tThe function translate(PATTERN) returns a regular expression\n\tcorresponding to PATTERN.  (It does not compile it.)\n\t\"\"\"\n\timport re\n\ttry:\n", "    from os.path import normcase\n\texcept ImportError:\n\t    def normcase(s):\n\t        \"\"\"\n\t        From os.path.normcase\n\t        Normalize the case of a pathname. On Windows, convert all characters\n\t        in the pathname to lowercase, and also convert forward slashes to\n\t        backward slashes. On other operating systems, return the path unchanged.\n\t        \"\"\"\n\t        return s\n", "__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\tCOMPAT = re.__name__ == \"ure\"\n\tdef fnmatch(name, pat):\n\t    \"\"\"Test whether FILENAME matches PATTERN.\n\t    Patterns are Unix shell style:\n\t    *       matches everything\n\t    ?       matches any single character\n\t    [seq]   matches any character in seq\n\t    [!seq]  matches any char not in seq\n\t    An initial period in FILENAME is not special.\n", "    Both FILENAME and PATTERN are first case-normalized\n\t    if the operating system requires it.\n\t    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n\t    \"\"\"\n\t    name = normcase(name)\n\t    pat = normcase(pat)\n\t    return fnmatchcase(name, pat)\n\t# @functools.lru_cache(maxsize=256, typed=True)\n\tdef _compile_pattern(pat):\n\t    if isinstance(pat, bytes):\n", "        pat_str = str(pat, \"ISO-8859-1\")\n\t        res_str = translate(pat_str)\n\t        res = bytes(res_str, \"ISO-8859-1\")\n\t    else:\n\t        res = translate(pat)\n\t    if COMPAT:\n\t        if res.startswith(\"(?ms)\"):\n\t            res = res[5:]\n\t        if res.endswith(\"\\\\Z\"):\n\t            res = res[:-2] + \"$\"\n", "    return re.compile(res).match\n\tdef filter(names, pat):\n\t    \"\"\"Return the subset of the list NAMES that match PAT.\"\"\"\n\t    result = []\n\t    pat = normcase(pat)\n\t    match = _compile_pattern(pat)\n\t    for name in names:\n\t        if match(normcase(name)):\n\t            result.append(name)\n\t    return result\n", "def fnmatchcase(name, pat):\n\t    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\t    This is a version of fnmatch() which doesn't case-normalize\n\t    its arguments.\n\t    \"\"\"\n\t    match = _compile_pattern(pat)\n\t    return match(name) is not None\n\tdef translate(pat):\n\t    \"\"\"Translate a shell PATTERN to a regular expression.\n\t    There is no way to quote meta-characters.\n", "    \"\"\"\n\t    i, n = 0, len(pat)\n\t    res = \"\"\n\t    while i < n:\n\t        c = pat[i]\n\t        i = i + 1\n\t        if c == \"*\":\n\t            res = res + \".*\"\n\t        elif c == \"?\":\n\t            res = res + \".\"\n", "        elif c == \"[\":\n\t            j = i\n\t            if j < n and pat[j] == \"!\":\n\t                j = j + 1\n\t            if j < n and pat[j] == \"]\":\n\t                j = j + 1\n\t            while j < n and pat[j] != \"]\":\n\t                j = j + 1\n\t            if j >= n:\n\t                res = res + \"\\\\[\"\n", "            else:\n\t                stuff = pat[i:j].replace(\"\\\\\", \"\\\\\\\\\")\n\t                i = j + 1\n\t                if stuff[0] == \"!\":\n\t                    stuff = \"^\" + stuff[1:]\n\t                elif stuff[0] == \"^\":\n\t                    stuff = \"\\\\\" + stuff\n\t                res = \"%s[%s]\" % (res, stuff)\n\t        else:\n\t            try:\n", "                res = res + re.escape(c)\n\t            except AttributeError:\n\t                # Using ure rather than re-pcre\n\t                res = res + re_escape(c)\n\t    # Original patterns is undefined, see http://bugs.python.org/issue21464\n\t    return \"(?ms)\" + res + \"\\Z\"\n\tdef re_escape(pattern):\n\t    # Replacement minimal re.escape for ure compatibility\n\t    return re.sub(r\"([\\^\\$\\.\\|\\?\\*\\+\\(\\)\\[\\\\])\", r\"\\\\\\1\", pattern)\n"]}
{"filename": ".belay/dependencies/dev/uprofiler/__init__.py", "chunked_list": ["from time import ticks_diff, ticks_us  # type: ignore[reportGeneralTypeIssues]\n\t_t_import = ticks_us()\n\t_BOLD = \"\\033[1m\"\n\t_RESET = \"\\033[0m\"\n\t# Default print period\n\tprint_period = 1\n\tdef _ticks_delta(t_start):\n\t    return ticks_diff(ticks_us(), t_start)\n\tclass _Counter:\n\t    registry = {}\n", "    def __init__(self, name, print_period):\n\t        self.name = name\n\t        self.print_period = print_period\n\t        self.n = 0\n\t        self.t_time_us = 0\n\t        self.registry[name] = self\n\t    def record(self, delta):\n\t        self.n += 1\n\t        self.t_time_us += delta\n\t    @property\n", "    def average(self):\n\t        return self.t_time_us / self.n\n\t    def __str__(self):\n\t        t_time_ms = self.t_time_us / 1000\n\t        return f\"{self.name: 24.24} {self.n : >8} calls {t_time_ms:>12.3f}ms total {t_time_ms/self.n:>12.3f}ms average\"\n\t    def print(self):\n\t        pp = self.print_period\n\t        if pp is None:\n\t            pp = print_period\n\t        if pp > 0 and self.n % pp == 0:\n", "            print(self)\n\t# Can't use class; micropython will have issues decorating methods then.\n\tdef profile(f=None, *, name=None, print_period=None):\n\t    \"\"\"Function/Method decorator.\"\"\"\n\t    if f is None:\n\t        # decorated with arguments\n\t        return lambda x: profile(x, name=name, print_period=print_period)\n\t    if name is None:\n\t        # TODO: I think micropython 1.20.0 will have __qualname__\n\t        name = f.__name__\n", "    try:\n\t        counter = _Counter.registry[name]\n\t    except KeyError:\n\t        counter = _Counter(name, print_period)\n\t    def inner(*args, **kwargs):\n\t        t_start = ticks_us()\n\t        result = f(*args, **kwargs)\n\t        delta = _ticks_delta(t_start)\n\t        counter.record(delta)\n\t        counter.print()\n", "        return result\n\t    return inner\n\tdef _table_formatter(name, calls, total_pct, total_ms, avg_ms):\n\t    return f\"{name: 32.32} {calls: >8} {total_pct: >10} {total_ms: >13} {avg_ms: >13}\"\n\tdef print_results():\n\t    \"\"\"Print summary.\n\t    To be called at end of script.\n\t    \"\"\"\n\t    t_total_ms = ticks_diff(ticks_us(), _t_import) / 1000\n\t    print()\n", "    print(f\"{_BOLD}Total-Time:{_RESET} {t_total_ms:6.3f}ms\")\n\t    header = _table_formatter(\n\t        \"Name\", \"Calls\", \"Total (%)\", \"Total (ms)\", \"Average (ms)\"\n\t    )\n\t    print(_BOLD + header + _RESET)\n\t    print(\"-\" * len(header))\n\t    counters = _Counter.registry.values()\n\t    counters = sorted(counters, key=lambda x: x.t_time_us, reverse=True)\n\t    for counter in counters:\n\t        t_counter_total_ms = counter.t_time_us / 1000\n", "        print(\n\t            _table_formatter(\n\t                name=counter.name,\n\t                calls=counter.n,\n\t                total_pct=round(100 * t_counter_total_ms / t_total_ms, 2),\n\t                total_ms=t_counter_total_ms,\n\t                avg_ms=t_counter_total_ms / counter.n if counter.n else 0,\n\t            )\n\t        )\n"]}
{"filename": ".belay/dependencies/dev/tempfile/__init__.py", "chunked_list": ["import errno\n\timport os\n\timport random\n\timport shutil\n\t_ascii_letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\tdef _get_candidate_name(size=8):\n\t    return \"\".join(random.choice(_ascii_letters) for _ in range(size))\n\tdef _sanitize_inputs(suffix, prefix, dir):\n\t    if dir is None:\n\t        dir = \"/tmp\"\n", "    if suffix is None:\n\t        suffix = \"\"\n\t    if prefix is None:\n\t        prefix = \"\"\n\t    return suffix, prefix, dir\n\tdef _try(action, *args, **kwargs):\n\t    try:\n\t        action(*args, **kwargs)\n\t        return True\n\t    except OSError as e:\n", "        if e.errno != errno.EEXIST:\n\t            raise e\n\t    return False\n\tdef mkdtemp(suffix=None, prefix=None, dir=None):\n\t    suffix, prefix, dir = _sanitize_inputs(suffix, prefix, dir)\n\t    _try(os.mkdir, dir)\n\t    while True:\n\t        name = _get_candidate_name()\n\t        file = dir + \"/\" + prefix + name + suffix\n\t        if _try(os.mkdir, file):\n", "            return file\n\tclass TemporaryDirectory:\n\t    def __init__(self, suffix=None, prefix=None, dir=None):\n\t        self.name = mkdtemp(suffix, prefix, dir)\n\t    def __repr__(self):\n\t        return \"<{} {!r}>\".format(self.__class__.__name__, self.name)\n\t    def __enter__(self):\n\t        return self.name\n\t    def __exit__(self, exc, value, tb):\n\t        self.cleanup()\n", "    def cleanup(self):\n\t        _try(shutil.rmtree, self.name)\n"]}
{"filename": ".belay/dependencies/dev/collections/deque.py", "chunked_list": ["class deque:\n\t    def __init__(self, iterable=None, maxlen=None):\n\t        if iterable is None:\n\t            self.q = []\n\t        else:\n\t            self.q = list(iterable)\n\t        if maxlen is not None:\n\t            if not isinstance(maxlen, int):\n\t                raise TypeError(\"\" \"an integer is required\")\n\t            if maxlen < 0:\n", "                raise ValueError(\"\" \"maxlen must be non-negative\")\n\t        self.__maxlen = maxlen\n\t    def popleft(self):\n\t        return self.q.pop(0)\n\t    def pop(self):\n\t        return self.q.pop()\n\t    def remove(self, a):\n\t        return self.q.remove(a)\n\t    def append(self, a):\n\t        self.q.append(a)\n", "        if self.__maxlen is not None and len(self.q) > self.__maxlen:\n\t            self.popleft()\n\t    def appendleft(self, a):\n\t        self.q.insert(0, a)\n\t        if self.__maxlen is not None and len(self.q) > self.__maxlen:\n\t            self.pop()\n\t    def extend(self, a):\n\t        if len(self.q) + len(a) > self.__maxlen:\n\t            raise IndexError\n\t        self.q.extend(a)\n", "    def clear(self):\n\t        self.q.clear()\n\t    @property\n\t    def maxlen(self):\n\t        return self.__maxlen\n\t    def __len__(self):\n\t        return len(self.q)\n\t    def __bool__(self):\n\t        return bool(self.q)\n\t    def __iter__(self):\n", "        yield from self.q\n\t    def __str__(self):\n\t        return \"deque({})\".format(self.q)\n\t    def __getitem__(self, idx):\n\t        return self.q[idx]\n"]}
{"filename": ".belay/dependencies/dev/collections/__init__.py", "chunked_list": ["# Replace built-in collections module.\n\tfrom ucollections import *\n\t# Provide optional dependencies (which may be installed separately).\n\ttry:\n\t    from .defaultdict import defaultdict\n\texcept ImportError:\n\t    pass\n\ttry:\n\t    from .deque import deque\n\texcept ImportError:\n", "    pass\n\tclass MutableMapping:\n\t    pass\n"]}
{"filename": ".belay/dependencies/dev/shutil/__init__.py", "chunked_list": ["# Reimplement, because CPython3.3 impl is rather bloated\n\timport os\n\tfrom collections import namedtuple\n\t_ntuple_diskusage = namedtuple(\"usage\", (\"total\", \"used\", \"free\"))\n\tdef rmtree(d):\n\t    if not d:\n\t        raise ValueError\n\t    for name, type, *_ in os.ilistdir(d):\n\t        path = d + \"/\" + name\n\t        if type & 0x4000:  # dir\n", "            rmtree(path)\n\t        else:  # file\n\t            os.unlink(path)\n\t    os.rmdir(d)\n\tdef copyfileobj(src, dest, length=512):\n\t    if hasattr(src, \"readinto\"):\n\t        buf = bytearray(length)\n\t        while True:\n\t            sz = src.readinto(buf)\n\t            if not sz:\n", "                break\n\t            if sz == length:\n\t                dest.write(buf)\n\t            else:\n\t                b = memoryview(buf)[:sz]\n\t                dest.write(b)\n\t    else:\n\t        while True:\n\t            buf = src.read(length)\n\t            if not buf:\n", "                break\n\t            dest.write(buf)\n\tdef disk_usage(path):\n\t    bit_tuple = os.statvfs(path)\n\t    blksize = bit_tuple[0]  # system block size\n\t    total = bit_tuple[2] * blksize\n\t    free = bit_tuple[3] * blksize\n\t    used = total - free\n\t    return _ntuple_diskusage(total, used, free)\n"]}
{"filename": ".belay/dependencies/dev/pathlib/__init__.py", "chunked_list": ["import errno\n\timport os\n\tfrom micropython import const\n\t_SEP = const(\"/\")\n\tdef _mode_if_exists(path):\n\t    try:\n\t        return os.stat(path)[0]\n\t    except OSError as e:\n\t        if e.errno == errno.ENOENT:\n\t            return 0\n", "        raise e\n\tdef _clean_segment(segment):\n\t    segment = str(segment)\n\t    if not segment:\n\t        return \".\"\n\t    segment = segment.rstrip(_SEP)\n\t    if not segment:\n\t        return _SEP\n\t    while True:\n\t        no_double = segment.replace(_SEP + _SEP, _SEP)\n", "        if no_double == segment:\n\t            break\n\t        segment = no_double\n\t    return segment\n\tclass Path:\n\t    def __init__(self, *segments):\n\t        segments_cleaned = []\n\t        for segment in segments:\n\t            segment = _clean_segment(segment)\n\t            if segment[0] == _SEP:\n", "                segments_cleaned = [segment]\n\t            elif segment == \".\":\n\t                continue\n\t            else:\n\t                segments_cleaned.append(segment)\n\t        self._path = _clean_segment(_SEP.join(segments_cleaned))\n\t    def __truediv__(self, other):\n\t        return Path(self._path, str(other))\n\t    def __repr__(self):\n\t        return f'{type(self).__name__}(\"{self._path}\")'\n", "    def __str__(self):\n\t        return self._path\n\t    def __eq__(self, other):\n\t        return self.absolute() == Path(other).absolute()\n\t    def absolute(self):\n\t        path = self._path\n\t        cwd = os.getcwd()\n\t        if not path or path == \".\":\n\t            return cwd\n\t        if path[0] == _SEP:\n", "            return path\n\t        return _SEP + path if cwd == _SEP else cwd + _SEP + path\n\t    def resolve(self):\n\t        return self.absolute()\n\t    def open(self, mode=\"r\", encoding=None):\n\t        return open(self._path, mode, encoding=encoding)\n\t    def exists(self):\n\t        return bool(_mode_if_exists(self._path))\n\t    def mkdir(self, parents=False, exist_ok=False):\n\t        try:\n", "            os.mkdir(self._path)\n\t            return\n\t        except OSError as e:\n\t            if e.errno == errno.EEXIST and exist_ok:\n\t                return\n\t            elif e.errno == errno.ENOENT and parents:\n\t                pass  # handled below\n\t            else:\n\t                raise e\n\t        segments = self._path.split(_SEP)\n", "        progressive_path = \"\"\n\t        if segments[0] == \"\":\n\t            segments = segments[1:]\n\t            progressive_path = _SEP\n\t        for segment in segments:\n\t            progressive_path += _SEP + segment\n\t            try:\n\t                os.mkdir(progressive_path)\n\t            except OSError as e:\n\t                if e.errno != errno.EEXIST:\n", "                    raise e\n\t    def is_dir(self):\n\t        return bool(_mode_if_exists(self._path) & 0x4000)\n\t    def is_file(self):\n\t        return bool(_mode_if_exists(self._path) & 0x8000)\n\t    def _glob(self, path, pattern, recursive):\n\t        # Currently only supports a single \"*\" pattern.\n\t        n_wildcards = pattern.count(\"*\")\n\t        n_single_wildcards = pattern.count(\"?\")\n\t        if n_single_wildcards:\n", "            raise NotImplementedError(\"? single wildcards not implemented.\")\n\t        if n_wildcards == 0:\n\t            raise ValueError\n\t        elif n_wildcards > 1:\n\t            raise NotImplementedError(\"Multiple * wildcards not implemented.\")\n\t        prefix, suffix = pattern.split(\"*\")\n\t        for name, mode, *_ in os.ilistdir(path):\n\t            full_path = path + _SEP + name\n\t            if name.startswith(prefix) and name.endswith(suffix):\n\t                yield full_path\n", "            if recursive and mode & 0x4000:  # is_dir\n\t                yield from self._glob(full_path, pattern, recursive=recursive)\n\t    def glob(self, pattern):\n\t        \"\"\"Iterate over this subtree and yield all existing files (of any\n\t        kind, including directories) matching the given relative pattern.\n\t        Currently only supports a single \"*\" pattern.\n\t        \"\"\"\n\t        return self._glob(self._path, pattern, recursive=False)\n\t    def rglob(self, pattern):\n\t        return self._glob(self._path, pattern, recursive=True)\n", "    def stat(self):\n\t        return os.stat(self._path)\n\t    def read_bytes(self):\n\t        with open(self._path, \"rb\") as f:\n\t            return f.read()\n\t    def read_text(self, encoding=None):\n\t        with open(self._path, \"r\", encoding=encoding) as f:\n\t            return f.read()\n\t    def rename(self, target):\n\t        os.rename(self._path, target)\n", "    def rmdir(self):\n\t        os.rmdir(self._path)\n\t    def touch(self, exist_ok=True):\n\t        if self.exists():\n\t            if exist_ok:\n\t                return  # TODO: should update timestamp\n\t            else:\n\t                # In lieue of FileExistsError\n\t                raise OSError(errno.EEXIST)\n\t        with open(self._path, \"w\"):\n", "            pass\n\t    def unlink(self, missing_ok=False):\n\t        try:\n\t            os.unlink(self._path)\n\t        except OSError as e:\n\t            if not (missing_ok and e.errno == errno.ENOENT):\n\t                raise e\n\t    def write_bytes(self, data):\n\t        with open(self._path, \"wb\") as f:\n\t            f.write(data)\n", "    def write_text(self, data, encoding=None):\n\t        with open(self._path, \"w\", encoding=encoding) as f:\n\t            f.write(data)\n\t    def with_suffix(self, suffix):\n\t        index = -len(self.suffix) or None\n\t        return Path(self._path[:index] + suffix)\n\t    @property\n\t    def stem(self):\n\t        return self.name.rsplit(\".\", 1)[0]\n\t    @property\n", "    def parent(self):\n\t        tokens = self._path.rsplit(_SEP, 1)\n\t        if len(tokens) == 2:\n\t            if not tokens[0]:\n\t                tokens[0] = _SEP\n\t            return Path(tokens[0])\n\t        return Path(\".\")\n\t    @property\n\t    def name(self):\n\t        return self._path.rsplit(_SEP, 1)[-1]\n", "    @property\n\t    def suffix(self):\n\t        elems = self._path.rsplit(\".\", 1)\n\t        return \"\" if len(elems) == 1 else \".\" + elems[1]\n"]}
{"filename": ".belay/dependencies/dev/unittest/__main__.py", "chunked_list": ["# Extension for \"unittest\" that adds the ability to run via \"micropython -m unittest\".\n\timport argparse\n\timport os\n\timport sys\n\tfrom fnmatch import fnmatch\n\tfrom micropython import const\n\tfrom unittest import TestRunner, TestResult, TestSuite\n\t# Run a single test in a clean environment.\n\tdef _run_test_module(runner: TestRunner, module_name: str, *extra_paths: list[str]):\n\t    module_snapshot = {k: v for k, v in sys.modules.items()}\n", "    path_snapshot = sys.path[:]\n\t    try:\n\t        for path in reversed(extra_paths):\n\t            if path:\n\t                sys.path.insert(0, path)\n\t        module = __import__(module_name)\n\t        suite = TestSuite(module_name)\n\t        suite._load_module(module)\n\t        return runner.run(suite)\n\t    finally:\n", "        sys.path[:] = path_snapshot\n\t        sys.modules.clear()\n\t        sys.modules.update(module_snapshot)\n\t_DIR_TYPE = const(0x4000)\n\tdef _run_all_in_dir(runner: TestRunner, path: str, pattern: str, top: str):\n\t    result = TestResult()\n\t    for fname, ftype, *_ in os.ilistdir(path):\n\t        if fname in (\"..\", \".\"):\n\t            continue\n\t        if ftype == _DIR_TYPE:\n", "            result += _run_all_in_dir(\n\t                runner=runner,\n\t                path=\"/\".join((path, fname)),\n\t                pattern=pattern,\n\t                top=top,\n\t            )\n\t        if fnmatch(fname, pattern):\n\t            module_name = fname.rsplit(\".\", 1)[0]\n\t            result += _run_test_module(runner, module_name, path, top)\n\t    return result\n", "# Implements discovery inspired by https://docs.python.org/3/library/unittest.html#test-discovery\n\tdef _discover(runner: TestRunner):\n\t    parser = argparse.ArgumentParser()\n\t    # parser.add_argument(\n\t    #     \"-v\",\n\t    #     \"--verbose\",\n\t    #     action=\"store_true\",\n\t    #     help=\"Verbose output\",\n\t    # )\n\t    parser.add_argument(\n", "        \"-s\",\n\t        \"--start-directory\",\n\t        dest=\"start\",\n\t        default=\".\",\n\t        help=\"Directory to start discovery\",\n\t    )\n\t    parser.add_argument(\n\t        \"-p\",\n\t        \"--pattern \",\n\t        dest=\"pattern\",\n", "        default=\"test*.py\",\n\t        help=\"Pattern to match test files\",\n\t    )\n\t    parser.add_argument(\n\t        \"-t\",\n\t        \"--top-level-directory\",\n\t        dest=\"top\",\n\t        help=\"Top level directory of project (defaults to start directory)\",\n\t    )\n\t    args = parser.parse_args(args=sys.argv[2:])\n", "    path = args.start\n\t    top = args.top or path\n\t    return _run_all_in_dir(\n\t        runner=runner,\n\t        path=path,\n\t        pattern=args.pattern,\n\t        top=top,\n\t    )\n\t# TODO: Use os.path for path handling.\n\tPATH_SEP = getattr(os, \"sep\", \"/\")\n", "# foo/bar/x.y.z --> foo/bar, x.y\n\tdef _dirname_filename_no_ext(path):\n\t    # Workaround: The Windows port currently reports \"/\" for os.sep\n\t    # (and MicroPython doesn't have os.altsep). So for now just\n\t    # always work with os.sep (i.e. \"/\").\n\t    path = path.replace(\"\\\\\", PATH_SEP)\n\t    split = path.rsplit(PATH_SEP, 1)\n\t    if len(split) == 1:\n\t        dirname, filename = \"\", split[0]\n\t    else:\n", "        dirname, filename = split\n\t    return dirname, filename.rsplit(\".\", 1)[0]\n\tdef discover_main():\n\t    runner = TestRunner()\n\t    if len(sys.argv) == 1 or (\n\t        len(sys.argv) >= 2\n\t        and _dirname_filename_no_ext(sys.argv[0])[1] == \"unittest\"\n\t        and sys.argv[1] == \"discover\"\n\t    ):\n\t        # No args, or `python -m unittest discover ...`.\n", "        result = _discover(runner)\n\t    else:\n\t        result = TestResult()\n\t        for test_spec in sys.argv[1:]:\n\t            try:\n\t                os.stat(test_spec)\n\t                # File exists, strip extension and import with its parent directory in sys.path.\n\t                dirname, module_name = _dirname_filename_no_ext(test_spec)\n\t                res = _run_test_module(runner, module_name, dirname)\n\t            except OSError:\n", "                # Not a file, treat as named module to import.\n\t                res = _run_test_module(runner, test_spec)\n\t            result += res\n\t    if not result.testsRun:\n\t        # If tests are run their results are already printed.\n\t        # Ensure an appropriate output is printed if no tests are found.\n\t        runner.run(TestSuite())\n\t    # Terminate with non zero return code in case of failures.\n\t    sys.exit(result.failuresNum + result.errorsNum)\n\tdiscover_main()\n"]}
{"filename": ".belay/dependencies/dev/unittest/__init__.py", "chunked_list": ["import io\n\timport os\n\timport sys\n\ttry:\n\t    import traceback\n\texcept ImportError:\n\t    traceback = None\n\tclass SkipTest(Exception):\n\t    pass\n\tclass AssertRaisesContext:\n", "    def __init__(self, exc):\n\t        self.expected = exc\n\t    def __enter__(self):\n\t        return self\n\t    def __exit__(self, exc_type, exc_value, tb):\n\t        self.exception = exc_value\n\t        if exc_type is None:\n\t            assert False, \"%r not raised\" % self.expected\n\t        if issubclass(exc_type, self.expected):\n\t            # store exception for later retrieval\n", "            self.exception = exc_value\n\t            return True\n\t        return False\n\t# These are used to provide required context to things like subTest\n\t__current_test__ = None\n\t__test_result__ = None\n\tclass SubtestContext:\n\t    def __init__(self, msg=None, params=None):\n\t        self.msg = msg\n\t        self.params = params\n", "    def __enter__(self):\n\t        pass\n\t    def __exit__(self, *exc_info):\n\t        if exc_info[0] is not None:\n\t            # Exception raised\n\t            global __test_result__, __current_test__\n\t            test_details = __current_test__\n\t            if self.msg:\n\t                test_details += (f\" [{self.msg}]\",)\n\t            if self.params:\n", "                detail = \", \".join(f\"{k}={v}\" for k, v in self.params.items())\n\t                test_details += (f\" ({detail})\",)\n\t            _handle_test_exception(test_details, __test_result__, exc_info, False)\n\t        # Suppress the exception as we've captured it above\n\t        return True\n\tclass NullContext:\n\t    def __enter__(self):\n\t        pass\n\t    def __exit__(self, exc_type, exc_value, traceback):\n\t        pass\n", "class TestCase:\n\t    def __init__(self):\n\t        pass\n\t    def addCleanup(self, func, *args, **kwargs):\n\t        if not hasattr(self, \"_cleanups\"):\n\t            self._cleanups = []\n\t        self._cleanups.append((func, args, kwargs))\n\t    def doCleanups(self):\n\t        if hasattr(self, \"_cleanups\"):\n\t            while self._cleanups:\n", "                func, args, kwargs = self._cleanups.pop()\n\t                func(*args, **kwargs)\n\t    def subTest(self, msg=None, **params):\n\t        return SubtestContext(msg=msg, params=params)\n\t    def skipTest(self, reason):\n\t        raise SkipTest(reason)\n\t    def fail(self, msg=\"\"):\n\t        assert False, msg\n\t    def assertEqual(self, x, y, msg=\"\"):\n\t        if not msg:\n", "            msg = \"%r vs (expected) %r\" % (x, y)\n\t        assert x == y, msg\n\t    def assertNotEqual(self, x, y, msg=\"\"):\n\t        if not msg:\n\t            msg = \"%r not expected to be equal %r\" % (x, y)\n\t        assert x != y, msg\n\t    def assertLessEqual(self, x, y, msg=None):\n\t        if msg is None:\n\t            msg = \"%r is expected to be <= %r\" % (x, y)\n\t        assert x <= y, msg\n", "    def assertGreaterEqual(self, x, y, msg=None):\n\t        if msg is None:\n\t            msg = \"%r is expected to be >= %r\" % (x, y)\n\t        assert x >= y, msg\n\t    def assertAlmostEqual(self, x, y, places=None, msg=\"\", delta=None):\n\t        if x == y:\n\t            return\n\t        if delta is not None and places is not None:\n\t            raise TypeError(\"specify delta or places not both\")\n\t        if delta is not None:\n", "            if abs(x - y) <= delta:\n\t                return\n\t            if not msg:\n\t                msg = \"%r != %r within %r delta\" % (x, y, delta)\n\t        else:\n\t            if places is None:\n\t                places = 7\n\t            if round(abs(y - x), places) == 0:\n\t                return\n\t            if not msg:\n", "                msg = \"%r != %r within %r places\" % (x, y, places)\n\t        assert False, msg\n\t    def assertNotAlmostEqual(self, x, y, places=None, msg=\"\", delta=None):\n\t        if delta is not None and places is not None:\n\t            raise TypeError(\"specify delta or places not both\")\n\t        if delta is not None:\n\t            if not (x == y) and abs(x - y) > delta:\n\t                return\n\t            if not msg:\n\t                msg = \"%r == %r within %r delta\" % (x, y, delta)\n", "        else:\n\t            if places is None:\n\t                places = 7\n\t            if not (x == y) and round(abs(y - x), places) != 0:\n\t                return\n\t            if not msg:\n\t                msg = \"%r == %r within %r places\" % (x, y, places)\n\t        assert False, msg\n\t    def assertIs(self, x, y, msg=\"\"):\n\t        if not msg:\n", "            msg = \"%r is not %r\" % (x, y)\n\t        assert x is y, msg\n\t    def assertIsNot(self, x, y, msg=\"\"):\n\t        if not msg:\n\t            msg = \"%r is %r\" % (x, y)\n\t        assert x is not y, msg\n\t    def assertIsNone(self, x, msg=\"\"):\n\t        if not msg:\n\t            msg = \"%r is not None\" % x\n\t        assert x is None, msg\n", "    def assertIsNotNone(self, x, msg=\"\"):\n\t        if not msg:\n\t            msg = \"%r is None\" % x\n\t        assert x is not None, msg\n\t    def assertTrue(self, x, msg=\"\"):\n\t        if not msg:\n\t            msg = \"Expected %r to be True\" % x\n\t        assert x, msg\n\t    def assertFalse(self, x, msg=\"\"):\n\t        if not msg:\n", "            msg = \"Expected %r to be False\" % x\n\t        assert not x, msg\n\t    def assertIn(self, x, y, msg=\"\"):\n\t        if not msg:\n\t            msg = \"Expected %r to be in %r\" % (x, y)\n\t        assert x in y, msg\n\t    def assertIsInstance(self, x, y, msg=\"\"):\n\t        assert isinstance(x, y), msg\n\t    def assertRaises(self, exc, func=None, *args, **kwargs):\n\t        if func is None:\n", "            return AssertRaisesContext(exc)\n\t        try:\n\t            func(*args, **kwargs)\n\t        except Exception as e:\n\t            if isinstance(e, exc):\n\t                return\n\t            raise\n\t        assert False, \"%r not raised\" % exc\n\t    def assertWarns(self, warn):\n\t        return NullContext()\n", "def skip(msg):\n\t    def _decor(fun):\n\t        # We just replace original fun with _inner\n\t        def _inner(self):\n\t            raise SkipTest(msg)\n\t        return _inner\n\t    return _decor\n\tdef skipIf(cond, msg):\n\t    if not cond:\n\t        return lambda x: x\n", "    return skip(msg)\n\tdef skipUnless(cond, msg):\n\t    if cond:\n\t        return lambda x: x\n\t    return skip(msg)\n\tdef expectedFailure(test):\n\t    def test_exp_fail(*args, **kwargs):\n\t        try:\n\t            test(*args, **kwargs)\n\t        except:\n", "            pass\n\t        else:\n\t            assert False, \"unexpected success\"\n\t    return test_exp_fail\n\tclass TestSuite:\n\t    def __init__(self, name=\"\"):\n\t        self._tests = []\n\t        self.name = name\n\t    def addTest(self, cls):\n\t        self._tests.append(cls)\n", "    def run(self, result):\n\t        for c in self._tests:\n\t            _run_suite(c, result, self.name)\n\t        return result\n\t    def _load_module(self, mod):\n\t        for tn in dir(mod):\n\t            c = getattr(mod, tn)\n\t            if isinstance(c, object) and isinstance(c, type) and issubclass(c, TestCase):\n\t                self.addTest(c)\n\t            elif tn.startswith(\"test\") and callable(c):\n", "                self.addTest(c)\n\tclass TestRunner:\n\t    def run(self, suite: TestSuite):\n\t        res = TestResult()\n\t        suite.run(res)\n\t        res.printErrors()\n\t        print(\"----------------------------------------------------------------------\")\n\t        print(\"Ran %d tests\\n\" % res.testsRun)\n\t        if res.failuresNum > 0 or res.errorsNum > 0:\n\t            print(\"FAILED (failures=%d, errors=%d)\" % (res.failuresNum, res.errorsNum))\n", "        else:\n\t            msg = \"OK\"\n\t            if res.skippedNum > 0:\n\t                msg += \" (skipped=%d)\" % res.skippedNum\n\t            print(msg)\n\t        return res\n\tTextTestRunner = TestRunner\n\tclass TestResult:\n\t    def __init__(self):\n\t        self.errorsNum = 0\n", "        self.failuresNum = 0\n\t        self.skippedNum = 0\n\t        self.testsRun = 0\n\t        self.errors = []\n\t        self.failures = []\n\t        self.skipped = []\n\t        self._newFailures = 0\n\t    def wasSuccessful(self):\n\t        return self.errorsNum == 0 and self.failuresNum == 0\n\t    def printErrors(self):\n", "        if self.errors or self.failures:\n\t            print()\n\t            self.printErrorList(self.errors)\n\t            self.printErrorList(self.failures)\n\t    def printErrorList(self, lst):\n\t        sep = \"----------------------------------------------------------------------\"\n\t        for c, e in lst:\n\t            detail = \" \".join((str(i) for i in c))\n\t            print(\"======================================================================\")\n\t            print(f\"FAIL: {detail}\")\n", "            print(sep)\n\t            print(e)\n\t    def __repr__(self):\n\t        # Format is compatible with CPython.\n\t        return \"<unittest.result.TestResult run=%d errors=%d failures=%d>\" % (\n\t            self.testsRun,\n\t            self.errorsNum,\n\t            self.failuresNum,\n\t        )\n\t    def __add__(self, other):\n", "        self.errorsNum += other.errorsNum\n\t        self.failuresNum += other.failuresNum\n\t        self.skippedNum += other.skippedNum\n\t        self.testsRun += other.testsRun\n\t        self.errors.extend(other.errors)\n\t        self.failures.extend(other.failures)\n\t        self.skipped.extend(other.skipped)\n\t        return self\n\tdef _capture_exc(exc, exc_traceback):\n\t    buf = io.StringIO()\n", "    if hasattr(sys, \"print_exception\"):\n\t        sys.print_exception(exc, buf)\n\t    elif traceback is not None:\n\t        traceback.print_exception(None, exc, exc_traceback, file=buf)\n\t    return buf.getvalue()\n\tdef _handle_test_exception(\n\t    current_test: tuple, test_result: TestResult, exc_info: tuple, verbose=True\n\t):\n\t    exc = exc_info[1]\n\t    traceback = exc_info[2]\n", "    ex_str = _capture_exc(exc, traceback)\n\t    if isinstance(exc, AssertionError):\n\t        test_result.failuresNum += 1\n\t        test_result.failures.append((current_test, ex_str))\n\t        if verbose:\n\t            print(\" FAIL\")\n\t    else:\n\t        test_result.errorsNum += 1\n\t        test_result.errors.append((current_test, ex_str))\n\t        if verbose:\n", "            print(\" ERROR\")\n\t    test_result._newFailures += 1\n\tdef _run_suite(c, test_result: TestResult, suite_name=\"\"):\n\t    if isinstance(c, TestSuite):\n\t        c.run(test_result)\n\t        return\n\t    if isinstance(c, type):\n\t        o = c()\n\t    else:\n\t        o = c\n", "    set_up_class = getattr(o, \"setUpClass\", lambda: None)\n\t    tear_down_class = getattr(o, \"tearDownClass\", lambda: None)\n\t    set_up = getattr(o, \"setUp\", lambda: None)\n\t    tear_down = getattr(o, \"tearDown\", lambda: None)\n\t    exceptions = []\n\t    try:\n\t        suite_name += \".\" + c.__qualname__\n\t    except AttributeError:\n\t        pass\n\t    def run_one(test_function):\n", "        global __test_result__, __current_test__\n\t        print(\"%s (%s) ...\" % (name, suite_name), end=\"\")\n\t        set_up()\n\t        __test_result__ = test_result\n\t        test_container = f\"({suite_name})\"\n\t        __current_test__ = (name, test_container)\n\t        try:\n\t            test_result._newFailures = 0\n\t            test_result.testsRun += 1\n\t            test_function()\n", "            # No exception occurred, test passed\n\t            if test_result._newFailures:\n\t                print(\" FAIL\")\n\t            else:\n\t                print(\" ok\")\n\t        except SkipTest as e:\n\t            reason = e.args[0]\n\t            print(\" skipped:\", reason)\n\t            test_result.skippedNum += 1\n\t            test_result.skipped.append((name, c, reason))\n", "        except Exception as ex:\n\t            _handle_test_exception(\n\t                current_test=(name, c), test_result=test_result, exc_info=(type(ex), ex, None)\n\t            )\n\t            # Uncomment to investigate failure in detail\n\t            # raise\n\t        finally:\n\t            __test_result__ = None\n\t            __current_test__ = None\n\t            tear_down()\n", "            try:\n\t                o.doCleanups()\n\t            except AttributeError:\n\t                pass\n\t    set_up_class()\n\t    try:\n\t        if hasattr(o, \"runTest\"):\n\t            name = str(o)\n\t            run_one(o.runTest)\n\t            return\n", "        for name in dir(o):\n\t            if name.startswith(\"test\"):\n\t                m = getattr(o, name)\n\t                if not callable(m):\n\t                    continue\n\t                run_one(m)\n\t        if callable(o):\n\t            name = o.__name__\n\t            run_one(o)\n\t    finally:\n", "        tear_down_class()\n\t    return exceptions\n\t# This supports either:\n\t#\n\t# >>> import mytest\n\t# >>> unitttest.main(mytest)\n\t#\n\t# >>> unittest.main(\"mytest\")\n\t#\n\t# Or, a script that ends with:\n", "# if __name__ == \"__main__\":\n\t#     unittest.main()\n\t# e.g. run via `mpremote run mytest.py`\n\tdef main(module=\"__main__\", testRunner=None):\n\t    if testRunner is None:\n\t        testRunner = TestRunner()\n\t    elif isinstance(testRunner, type):\n\t        testRunner = testRunner()\n\t    if isinstance(module, str):\n\t        module = __import__(module)\n", "    suite = TestSuite(module.__name__)\n\t    suite._load_module(module)\n\t    return testRunner.run(suite)\n"]}
{"filename": ".belay/dependencies/dev/argparse/__init__.py", "chunked_list": ["\"\"\"\n\tMinimal and functional version of CPython's argparse module.\n\t\"\"\"\n\timport sys\n\tfrom ucollections import namedtuple\n\tclass _ArgError(BaseException):\n\t    pass\n\tclass _Arg:\n\t    def __init__(self, names, dest, action, nargs, const, default, help):\n\t        self.names = names\n", "        self.dest = dest\n\t        self.action = action\n\t        self.nargs = nargs\n\t        self.const = const\n\t        self.default = default\n\t        self.help = help\n\t    def parse(self, optname, args):\n\t        # parse args for this arg\n\t        if self.action == \"store\":\n\t            if self.nargs is None:\n", "                if args:\n\t                    return args.pop(0)\n\t                else:\n\t                    raise _ArgError(\"expecting value for %s\" % optname)\n\t            elif self.nargs == \"?\":\n\t                if args:\n\t                    return args.pop(0)\n\t                else:\n\t                    return self.default\n\t            else:\n", "                if self.nargs == \"*\":\n\t                    n = -1\n\t                elif self.nargs == \"+\":\n\t                    if not args:\n\t                        raise _ArgError(\"expecting value for %s\" % optname)\n\t                    n = -1\n\t                else:\n\t                    n = int(self.nargs)\n\t                ret = []\n\t                stop_at_opt = True\n", "                while args and n != 0:\n\t                    if stop_at_opt and args[0].startswith(\"-\") and args[0] != \"-\":\n\t                        if args[0] == \"--\":\n\t                            stop_at_opt = False\n\t                            args.pop(0)\n\t                        else:\n\t                            break\n\t                    else:\n\t                        ret.append(args.pop(0))\n\t                        n -= 1\n", "                if n > 0:\n\t                    raise _ArgError(\"expecting value for %s\" % optname)\n\t                return ret\n\t        elif self.action == \"store_const\":\n\t            return self.const\n\t        else:\n\t            assert False\n\tdef _dest_from_optnames(opt_names):\n\t    dest = opt_names[0]\n\t    for name in opt_names:\n", "        if name.startswith(\"--\"):\n\t            dest = name\n\t            break\n\t    return dest.lstrip(\"-\").replace(\"-\", \"_\")\n\tclass ArgumentParser:\n\t    def __init__(self, *, description=\"\"):\n\t        self.description = description\n\t        self.opt = []\n\t        self.pos = []\n\t    def add_argument(self, *args, **kwargs):\n", "        action = kwargs.get(\"action\", \"store\")\n\t        if action == \"store_true\":\n\t            action = \"store_const\"\n\t            const = True\n\t            default = kwargs.get(\"default\", False)\n\t        elif action == \"store_false\":\n\t            action = \"store_const\"\n\t            const = False\n\t            default = kwargs.get(\"default\", True)\n\t        else:\n", "            const = kwargs.get(\"const\", None)\n\t            default = kwargs.get(\"default\", None)\n\t        if args and args[0].startswith(\"-\"):\n\t            list = self.opt\n\t            dest = kwargs.get(\"dest\")\n\t            if dest is None:\n\t                dest = _dest_from_optnames(args)\n\t        else:\n\t            list = self.pos\n\t            dest = kwargs.get(\"dest\")\n", "            if dest is None:\n\t                dest = args[0]\n\t            if not args:\n\t                args = [dest]\n\t        list.append(\n\t            _Arg(\n\t                args,\n\t                dest,\n\t                action,\n\t                kwargs.get(\"nargs\", None),\n", "                const,\n\t                default,\n\t                kwargs.get(\"help\", \"\"),\n\t            )\n\t        )\n\t    def usage(self, full):\n\t        # print short usage\n\t        print(\"usage: %s [-h]\" % sys.argv[0], end=\"\")\n\t        def render_arg(arg):\n\t            if arg.action == \"store\":\n", "                if arg.nargs is None:\n\t                    return \" %s\" % arg.dest\n\t                if isinstance(arg.nargs, int):\n\t                    return \" %s(x%d)\" % (arg.dest, arg.nargs)\n\t                else:\n\t                    return \" %s%s\" % (arg.dest, arg.nargs)\n\t            else:\n\t                return \"\"\n\t        for opt in self.opt:\n\t            print(\" [%s%s]\" % (\", \".join(opt.names), render_arg(opt)), end=\"\")\n", "        for pos in self.pos:\n\t            print(render_arg(pos), end=\"\")\n\t        print()\n\t        if not full:\n\t            return\n\t        # print full information\n\t        print()\n\t        if self.description:\n\t            print(self.description)\n\t        if self.pos:\n", "            print(\"\\npositional args:\")\n\t            for pos in self.pos:\n\t                print(\"  %-16s%s\" % (pos.names[0], pos.help))\n\t        print(\"\\noptional args:\")\n\t        print(\"  -h, --help      show this message and exit\")\n\t        for opt in self.opt:\n\t            print(\"  %-16s%s\" % (\", \".join(opt.names) + render_arg(opt), opt.help))\n\t    def parse_args(self, args=None):\n\t        return self._parse_args_impl(args, False)\n\t    def parse_known_args(self, args=None):\n", "        return self._parse_args_impl(args, True)\n\t    def _parse_args_impl(self, args, return_unknown):\n\t        if args is None:\n\t            args = sys.argv[1:]\n\t        else:\n\t            args = args[:]\n\t        try:\n\t            return self._parse_args(args, return_unknown)\n\t        except _ArgError as e:\n\t            self.usage(False)\n", "            print(\"error:\", e)\n\t            sys.exit(2)\n\t    def _parse_args(self, args, return_unknown):\n\t        # add optional args with defaults\n\t        arg_dest = []\n\t        arg_vals = []\n\t        for opt in self.opt:\n\t            arg_dest.append(opt.dest)\n\t            arg_vals.append(opt.default)\n\t        # deal with unknown arguments, if needed\n", "        unknown = []\n\t        def consume_unknown():\n\t            while args and not args[0].startswith(\"-\"):\n\t                unknown.append(args.pop(0))\n\t        # parse all args\n\t        parsed_pos = False\n\t        while args or not parsed_pos:\n\t            if args and args[0].startswith(\"-\") and args[0] != \"-\" and args[0] != \"--\":\n\t                # optional arg\n\t                a = args.pop(0)\n", "                if a in (\"-h\", \"--help\"):\n\t                    self.usage(True)\n\t                    sys.exit(0)\n\t                found = False\n\t                for i, opt in enumerate(self.opt):\n\t                    if a in opt.names:\n\t                        arg_vals[i] = opt.parse(a, args)\n\t                        found = True\n\t                        break\n\t                if not found:\n", "                    if return_unknown:\n\t                        unknown.append(a)\n\t                        consume_unknown()\n\t                    else:\n\t                        raise _ArgError(\"unknown option %s\" % a)\n\t            else:\n\t                # positional arg\n\t                if parsed_pos:\n\t                    if return_unknown:\n\t                        unknown = unknown + args\n", "                        break\n\t                    else:\n\t                        raise _ArgError(\"extra args: %s\" % \" \".join(args))\n\t                for pos in self.pos:\n\t                    arg_dest.append(pos.dest)\n\t                    arg_vals.append(pos.parse(pos.names[0], args))\n\t                parsed_pos = True\n\t                if return_unknown:\n\t                    consume_unknown()\n\t        # build and return named tuple with arg values\n", "        values = namedtuple(\"args\", arg_dest)(*arg_vals)\n\t        return (values, unknown) if return_unknown else values\n"]}
{"filename": "tests/test_compressor.py", "chunked_list": ["\"\"\"Tamp compressor tests.\n\tOur custom huffman size table:\n\t   huffman_coding = {\n\t            2: 0b0,\n\t            3: 0b11,\n\t            4: 0b1000,\n\t            5: 0b1011,\n\t            6: 0b10100,\n\t            7: 0b100100,\n\t            8: 0b100110,\n", "            9: 0b101011,\n\t           10: 0b1001011,\n\t           11: 0b1010100,\n\t           12: 0b10010100,\n\t           13: 0b10010101,\n\t           14: 0b10101010,\n\t           15: 0b100111,\n\t      \"FLUSH\": 0b10101011,\n\t   }\n\t\"\"\"\n", "import io\n\timport unittest\n\tfrom tamp import ExcessBitsError\n\ttry:\n\t    import micropython\n\texcept ImportError:\n\t    micropython = None\n\tCompressors = []\n\tcompresses = []\n\tif micropython:\n", "    from tamp.compressor_viper import Compressor as ViperCompressor\n\t    from tamp.compressor_viper import compress as viper_compress\n\t    Compressors.append(ViperCompressor)\n\t    compresses.append(viper_compress)\n\telse:\n\t    from tamp.compressor import Compressor as PyCompressor\n\t    from tamp.compressor import compress as py_compress\n\t    Compressors.append(PyCompressor)\n\t    compresses.append(py_compress)\n\t    try:\n", "        from tamp._c_compressor import Compressor as CCompressor\n\t        from tamp._c_compressor import compress as c_compress\n\t        Compressors.append(CCompressor)\n\t        compresses.append(c_compress)\n\t    except ImportError:\n\t        pass\n\tclass TestCompressor(unittest.TestCase):\n\t    def test_compressor_default(self):\n\t        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor):\n", "                test_string = b\"foo foo foo\"\n\t                expected = bytes(\n\t                    # fmt: off\n\t                    [\n\t                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0110011,    # literal \"f\"\n\t                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n\t                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b00011_1_00,   # literal \" \"\n", "                        0b100000_0_1,   # There is now \"foo \" at index 0\n\t                        0b000_00000,    # size=4 -> 0b1000\n\t                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t                        0b00000000,     # index 0 -> 0b0000000000\n\t                        0b00_000000,    # 6 bits of zero-padding\n\t                    ]\n\t                    # fmt: on\n\t                )\n\t                bytes_written = 0\n\t                with io.BytesIO() as f:\n", "                    compressor = Compressor(f)\n\t                    bytes_written += compressor.write(test_string)\n\t                    bytes_written += compressor.flush(write_token=False)\n\t                    f.seek(0)\n\t                    actual = f.read()\n\t                    compressor.close()\n\t                self.assertEqual(actual, expected)\n\t                self.assertEqual(bytes_written, len(expected))\n\t                # Test Context Manager\n\t                bytes_written = 0\n", "                with io.BytesIO() as f, Compressor(f) as compressor:\n\t                    bytes_written += compressor.write(test_string)\n\t                    bytes_written += compressor.flush(write_token=False)\n\t                    f.seek(0)\n\t                    actual = f.read()\n\t                self.assertEqual(actual, expected)\n\t                self.assertEqual(bytes_written, len(expected))\n\t    def test_compressor_input_buffer(self):\n\t        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor):\n", "                expected = bytes(\n\t                    # fmt: off\n\t                    [\n\t                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0110011,    # literal \"f\"\n\t                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n\t                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b00011_1_00,   # literal \" \"\n\t                        0b100000_0_1,   # There is now \"foo \" at index 0\n", "                        0b000_00000,    # size=4 -> 0b1000\n\t                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t                        0b00000000,     # index 0 -> 0b0000000000\n\t                        0b00_000000,    # 6 bits of zero-padding\n\t                    ]\n\t                    # fmt: on\n\t                )\n\t                with io.BytesIO() as f:\n\t                    compressor = Compressor(f)\n\t                    compressor.write(b\"f\")\n", "                    compressor.write(b\"oo\")\n\t                    compressor.write(b\" fo\")\n\t                    compressor.write(b\"o foo\")\n\t                    compressor.flush(write_token=False)\n\t                    f.seek(0)\n\t                    actual = f.read()\n\t                self.assertEqual(actual, expected)\n\t    def test_compressor_7bit(self):\n\t        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor):\n", "                test_string = b\"foo foo foo\"\n\t                expected = bytes(\n\t                    # fmt: off\n\t                    [\n\t                        0b010_10_0_0_0,  # header (window_bits=10, literal_bits=7)\n\t                        0b1_1100110,    # literal \"f\"\n\t                        0b0_0_001000,   # the pre-init buffer contains \"oo \" at index 131\n\t                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b0011_1_010,   # literal \" \"\n", "                        0b0000_0_100,   # size=4 -> 0b1000\n\t                        0b0_0000000,\n\t                        0b000_0_11_00,  # Just \"foo\" at index 0; size=3 -> 0b11\n\t                        0b000000000,    # index 0 -> 0b0000000000\n\t                        # no padding!\n\t                    ]\n\t                    # fmt: on\n\t                )\n\t                with io.BytesIO() as f:\n\t                    compressor = Compressor(f, literal=7)\n", "                    compressor.write(test_string)\n\t                    compressor.flush(write_token=False)\n\t                    f.seek(0)\n\t                    actual = f.read()\n\t                self.assertEqual(actual, expected)\n\t    def test_compressor_predefined_dictionary(self):\n\t        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor):\n\t                test_string = b\"foo foo foo\"\n\t                init_string = b\"foo foo foo\"\n", "                dictionary = bytearray(1 << 8)\n\t                dictionary[: len(init_string)] = init_string\n\t                expected = bytes(\n\t                    # fmt: off\n\t                    [\n\t                        0b000_10_1_0_0,  # header (window_bits=8, literal_bits=7, dictionary provided)\n\t                        0b0_1010100,     # match-size 11\n\t                        0b00000000,      # At index 0\n\t                        # no padding!\n\t                    ]\n", "                    # fmt: on\n\t                )\n\t                with io.BytesIO() as f:\n\t                    compressor = Compressor(f, window=8, literal=7, dictionary=dictionary)\n\t                    compressor.write(test_string)\n\t                    compressor.flush(write_token=False)\n\t                    f.seek(0)\n\t                    actual = f.read()\n\t                self.assertEqual(actual, expected)\n\t    def test_compressor_predefined_dictionary_incorrect_size(self):\n", "        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor):\n\t                dictionary = bytearray(1 << 8)\n\t                with io.BytesIO() as f, self.assertRaises(ValueError):\n\t                    Compressor(f, window=9, literal=7, dictionary=dictionary)\n\t    def test_oob_2_byte_pattern(self):\n\t        \"\"\"Viper implementation had a bug where a pattern of length 2 could be detected at the end of a string (going out of bounds by 1 byte).\"\"\"\n\t        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor):\n\t                test_string_extended = bytearray(b\"Q\\x00Q\\x00\")\n", "                test_string = memoryview(test_string_extended)[:3]  # b\"Q\\x00Q\"\n\t                with io.BytesIO() as f:\n\t                    compressor = Compressor(f)\n\t                    compressor.write(test_string)\n\t                    compressor.flush(write_token=False)\n\t                    f.seek(0)\n\t                    actual = f.read()\n\t                # Q == 0b0101_0001\n\t                expected = bytes(\n\t                    [\n", "                        0b010_11_00_0,\n\t                        0b1_0101_000,\n\t                        0b1_1_0000_00,\n\t                        0b00_1_0101_0,\n\t                        0b001_00000,\n\t                    ]\n\t                )\n\t                assert actual == expected\n\t    def test_excess_bits(self):\n\t        for Compressor in Compressors:\n", "            with self.subTest(Compressor=Compressor), io.BytesIO() as f:\n\t                compressor = Compressor(f, literal=7)\n\t                with self.assertRaises(ExcessBitsError):\n\t                    compressor.write(b\"\\xFF\")\n\t                    compressor.flush()\n\t    def test_single_shot_compress_text(self):\n\t        for compress in compresses:\n\t            with self.subTest(compress=compress):\n\t                expected = bytes(\n\t                    # fmt: off\n", "                    [\n\t                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0110011,    # literal \"f\"\n\t                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n\t                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b00011_1_00,   # literal \" \"\n\t                        0b100000_0_1,   # There is now \"foo \" at index 0\n\t                        0b000_00000,    # size=4 -> 0b1000\n\t                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n", "                        0b00000000,     # index 0 -> 0b0000000000\n\t                        0b00_000000,    # 6 bits of zero-padding\n\t                    ]\n\t                    # fmt: on\n\t                )\n\t                self.assertEqual(compress(\"foo foo foo\"), expected)\n\t    def test_single_shot_compress_binary(self):\n\t        for compress in compresses:\n\t            with self.subTest(compress=compress):\n\t                expected = bytes(\n", "                    # fmt: off\n\t                    [\n\t                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0110011,    # literal \"f\"\n\t                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n\t                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b00011_1_00,   # literal \" \"\n\t                        0b100000_0_1,   # There is now \"foo \" at index 0\n\t                        0b000_00000,    # size=4 -> 0b1000\n", "                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t                        0b00000000,     # index 0 -> 0b0000000000\n\t                        0b00_000000,    # 6 bits of zero-padding\n\t                    ]\n\t                    # fmt: on\n\t                )\n\t                self.assertEqual(compress(b\"foo foo foo\"), expected)\n\t    def test_invalid_conf(self):\n\t        for Compressor in Compressors:\n\t            with self.subTest(Compressor=Compressor), io.BytesIO() as f:\n", "                with self.assertRaises(ValueError):\n\t                    Compressor(f, literal=4)\n\t                with self.assertRaises(ValueError):\n\t                    Compressor(f, window=16)\n"]}
{"filename": "tests/test_bit_writer_reader.py", "chunked_list": ["import io\n\timport random\n\timport unittest\n\tfrom tamp.compressor import BitWriter\n\tfrom tamp.decompressor import BitReader\n\tclass TestBitWriterAndReader(unittest.TestCase):\n\t    def test_auto_bit_writer_and_reader(self):\n\t        # Generate a list of random chunks of bits (1~16 bits)\n\t        num_chunks = 1000\n\t        n_bits = [random.randint(1, 16) for _ in range(num_chunks)]\n", "        data = []\n\t        for n_bit in n_bits:\n\t            mask = (1 << n_bit) - 1\n\t            data.append(random.randint(0, 1 << 32) & mask)\n\t        chunks = list(zip(data, n_bits))\n\t        # Write the chunks of bits using BitWriter\n\t        with io.BytesIO() as f:\n\t            writer = BitWriter(f)\n\t            for bits, num_bits in chunks:\n\t                writer.write(bits, num_bits)\n", "            writer.flush(write_token=False)\n\t            # Read the chunks of bits back using BitReader\n\t            f.seek(0)\n\t            reader = BitReader(f)\n\t            for original_bits, num_bits in chunks:\n\t                read_bits = reader.read(num_bits)\n\t                self.assertEqual(read_bits, original_bits)\n\t    def test_writer_correct_size_no_flush_token(self):\n\t        for i in range(1, 8 + 1):\n\t            with io.BytesIO() as f:\n", "                writer = BitWriter(f)\n\t                writer.write(0xFFFF, i)\n\t                writer.flush(write_token=False)\n\t                self.assertEqual(f.tell(), 1)\n\t        for i in range(9, 16 + 1):\n\t            with io.BytesIO() as f:\n\t                writer = BitWriter(f)\n\t                writer.write(0xFFFF, i)\n\t                writer.flush(write_token=False)\n\t                self.assertEqual(f.tell(), 2)\n", "class TestHuffmanReader(unittest.TestCase):\n\t    def test_always_valid(self):\n\t        \"\"\"There should never be an instance where 8bits have been read and\n\t        no huffman code has been decoded. This verifies that.\n\t        \"\"\"\n\t        random_bytes = bytes(random.randint(0, 255) for _ in range(1024 * 1024))\n\t        with io.BytesIO(random_bytes) as f:\n\t            reader = BitReader(f)\n\t            reader.read_huffman()\n"]}
{"filename": "tests/test_compressor_helpers.py", "chunked_list": ["import unittest\n\tfrom tamp import bit_size, compute_min_pattern_size\n\tclass TestCompressorHelpers(unittest.TestCase):\n\t    def test_bit_size(self):\n\t        self.assertEqual(bit_size(0b11), 2)\n\t    def test_bit_size_excess(self):\n\t        self.assertEqual(bit_size(1 << 32), -1)\n\t    def test_min_pattern_size(self):\n\t        self.assertEqual(compute_min_pattern_size(window=10, literal=8), 2)\n\t        self.assertEqual(compute_min_pattern_size(window=15, literal=5), 3)\n", "    def test_min_pattern_size_out_of_range(self):\n\t        with self.assertRaises(ValueError):\n\t            compute_min_pattern_size(0, 0)\n"]}
{"filename": "tests/test_cli.py", "chunked_list": ["import tempfile\n\timport unittest\n\tfrom pathlib import Path\n\ttry:\n\t    import micropython\n\texcept ImportError:\n\t    micropython = None\n\ttry:\n\t    from typer.testing import CliRunner\n\t    from tamp.cli.main import app\n", "except ImportError:\n\t    pass\n\telse:\n\t    runner = CliRunner()\n\tcompressed_foo_foo_foo = bytes(\n\t    # fmt: off\n\t    [\n\t        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n\t        0b1_0110011,    # literal \"f\"\n\t        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n", "                        # size=2 -> 0b0\n\t                        # 131 -> 0b0010000011\n\t        0b00011_1_00,   # literal \" \"\n\t        0b100000_0_1,   # There is now \"foo \" at index 0\n\t        0b000_00000,    # size=4 -> 0b1000\n\t        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t        0b00000000,     # index 0 -> 0b0000000000\n\t        0b00_000000,    # 6 bits of zero-padding\n\t    ]\n\t    # fmt: on\n", ")\n\t@unittest.skipIf(micropython is not None, \"not running cpython\")\n\tclass TestCli(unittest.TestCase):\n\t    def test_compress_file_to_stdout(self):\n\t        with tempfile.TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n\t            test_file = tmp_dir / \"test_input.bin\"\n\t            test_file.write_bytes(b\"foo foo foo\")\n\t            result = runner.invoke(app, [\"compress\", str(test_file)])\n\t            self.assertEqual(result.exit_code, 0)\n", "            self.assertEqual(result.stdout_bytes, compressed_foo_foo_foo)\n\t    def test_compress_stdin_to_stdout(self):\n\t        result = runner.invoke(app, [\"compress\"], input=\"foo foo foo\")\n\t        self.assertEqual(result.exit_code, 0)\n\t        self.assertEqual(result.stdout_bytes, compressed_foo_foo_foo)\n\t    def test_decompress_file_to_stdout(self):\n\t        with tempfile.TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n\t            test_file = tmp_dir / \"test_input.tamp\"\n\t            test_file.write_bytes(compressed_foo_foo_foo)\n", "            result = runner.invoke(app, [\"decompress\", str(test_file)])\n\t            self.assertEqual(result.exit_code, 0)\n\t            self.assertEqual(result.stdout, \"foo foo foo\")\n\t    def test_decompress_stdin_to_stdout(self):\n\t        result = runner.invoke(app, [\"decompress\"], input=compressed_foo_foo_foo)\n\t        self.assertEqual(result.exit_code, 0)\n\t        self.assertEqual(result.stdout, \"foo foo foo\")\n\t    def test_decompress_stdin_to_file(self):\n\t        with tempfile.TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n", "            test_file = tmp_dir / \"test_output.txt\"\n\t            result = runner.invoke(app, [\"decompress\", \"-o\", str(test_file)], input=compressed_foo_foo_foo)\n\t            self.assertEqual(result.exit_code, 0)\n\t            self.assertEqual(test_file.read_text(), \"foo foo foo\")\n\t    def test_version(self):\n\t        result = runner.invoke(app, [\"--version\"])\n\t        self.assertEqual(result.exit_code, 0)\n"]}
{"filename": "tests/test_compressor_decompressor.py", "chunked_list": ["import random\n\timport unittest\n\tfrom io import BytesIO\n\ttry:\n\t    import micropython\n\texcept ImportError:\n\t    micropython = None\n\tfrom tamp.compressor import Compressor as PyCompressor\n\tfrom tamp.decompressor import Decompressor as PyDecompressor\n\tif micropython is None:\n", "    ViperCompressor = None\n\t    ViperDecompressor = None\n\telse:\n\t    from tamp.compressor_viper import Compressor as ViperCompressor\n\t    from tamp.decompressor_viper import Decompressor as ViperDecompressor\n\ttry:\n\t    from tamp._c_compressor import Compressor as CCompressor\n\t    from tamp._c_decompressor import Decompressor as CDecompressor\n\texcept ImportError:\n\t    CCompressor = None\n", "    CDecompressor = None\n\tCompressors = (PyCompressor, CCompressor, ViperCompressor)\n\tDecompressors = (PyDecompressor, CDecompressor, ViperDecompressor)\n\tdef walk_compressors_decompressors():\n\t    \"\"\"Iterate over all available compressor/decompressor combintations.\"\"\"\n\t    for Compressor in Compressors:\n\t        if Compressor is None:\n\t            continue\n\t        for Decompressor in Decompressors:\n\t            if Decompressor is None:\n", "                continue\n\t            yield (Compressor, Decompressor)\n\ttale_of_two_cities = b\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way - in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\"\n\tclass TestCompressorAndDecompressor(unittest.TestCase):\n\t    def _autotest(self, num_bytes, n_bits, compressor_kwargs=None):\n\t        if compressor_kwargs is None:\n\t            compressor_kwargs = {}\n\t        data = bytearray(random.randint(0, (1 << n_bits) - 1) for x in range(num_bytes))\n\t        for Compressor, Decompressor in walk_compressors_decompressors():\n\t            # Compress/Decompress random data\n", "            with BytesIO() as f, self.subTest(\n\t                data=\"Random\",\n\t                Compressor=Compressor,\n\t                Decompressor=Decompressor,\n\t            ):\n\t                c = Compressor(f, **compressor_kwargs)\n\t                c.write(data)\n\t                c.flush()\n\t                f.seek(0)\n\t                d = Decompressor(f)\n", "                actual = d.read()\n\t                self.assertEqual(actual, data)\n\t            # Compress/Decompress\n\t            data = bytearray(1 for _ in range(num_bytes))\n\t            with BytesIO() as f, self.subTest(\n\t                data=\"Sequential\",\n\t                Compressor=Compressor,\n\t                Decompressor=Decompressor,\n\t            ):\n\t                c = Compressor(f, **compressor_kwargs)\n", "                c.write(data)\n\t                c.flush()\n\t                f.seek(0)\n\t                d = Decompressor(f)\n\t                actual = d.read()\n\t                self.assertEqual(len(actual), len(data))\n\t                self.assertEqual(actual, data)\n\t    def test_default(self):\n\t        self._autotest(10_000, 8)\n\t    def test_7bit(self):\n", "        self._autotest(10_000, 7, compressor_kwargs={\"literal\": 7})\n\t    def test_6bit(self):\n\t        self._autotest(10_000, 6, compressor_kwargs={\"literal\": 6})\n\t    def test_5bit(self):\n\t        self._autotest(10_000, 5, compressor_kwargs={\"literal\": 5})\n\t    def test_tale_of_two_cities(self):\n\t        assert len(tale_of_two_cities) > (1 << 8)\n\t        for Compressor, Decompressor in walk_compressors_decompressors():\n\t            with BytesIO() as f:\n\t                c = Compressor(f, window=8)\n", "                c.write(tale_of_two_cities)\n\t                c.flush()\n\t                f.seek(0)\n\t                d = Decompressor(f)\n\t                actual = d.read()\n\t                assert actual == tale_of_two_cities\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "tests/test_file_interface.py", "chunked_list": ["import unittest\n\tfrom pathlib import Path\n\tfrom tempfile import TemporaryDirectory\n\timport tamp\n\tclass TestFileInterface(unittest.TestCase):\n\t    def test_open_wb(self):\n\t        with TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n\t            fn = tmp_dir / \"file.tamp\"\n\t            f = tamp.open(fn, \"wb\")\n", "            self.assertIsInstance(f, tamp.Compressor)\n\t            f.close()\n\t    def test_open_wb_then_rb(self):\n\t        with TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n\t            fn = tmp_dir / \"file.tamp\"\n\t            f = tamp.open(fn, \"wb\")\n\t            f.close()\n\t            f = tamp.open(fn, \"rb\")\n\t            self.assertIsInstance(f, tamp.Decompressor)\n", "            f.close()\n\t    def test_open_context_manager_read_write(self):\n\t        with TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n\t            fn = tmp_dir / \"file.tamp\"\n\t            test_string = b\"test string is best string\"\n\t            with tamp.open(fn, \"wb\") as f:\n\t                f.write(test_string)\n\t            with tamp.open(fn, \"rb\") as f:\n\t                actual = f.read()\n", "            assert test_string == actual\n\t    def test_encoding(self):\n\t        with TemporaryDirectory() as tmp_dir:\n\t            tmp_dir = Path(tmp_dir)\n\t            fn = tmp_dir / \"file.tamp\"\n\t            test_string = \"test string is best string\"\n\t            with tamp.open(fn, \"w\") as f:\n\t                f.write(test_string)\n\t            with tamp.open(fn, \"r\") as f:\n\t                actual = f.read()\n", "            assert test_string == actual\n\t    def test_bad_modes(self):\n\t        with self.assertRaises(ValueError):\n\t            tamp.open(None, \"abc\")  # type: ignore[reportGeneralTypeIssues]\n\t        with self.assertRaises(ValueError):\n\t            tamp.open(None, \"rw\")  # type: ignore[reportGeneralTypeIssues]\n"]}
{"filename": "tests/test_pseudorandom.py", "chunked_list": ["import unittest\n\timport tamp.compressor\n\timport tamp.decompressor\n\tfrom tamp import initialize_dictionary\n\ttry:\n\t    import micropython\n\texcept ImportError:\n\t    micropython = None\n\tclass TestPseudoRandom(unittest.TestCase):\n\t    def test_256_compressor_zero_seed(self):\n", "        self.assertEqual(initialize_dictionary(256, seed=0), bytearray(256))\n\t    def test_256_compressor_nonzero_seed(self):\n\t        self.assertNotEqual(initialize_dictionary(256, seed=1), bytearray(256))\n\t    def test_256_compressor(self):\n\t        actual = tamp.compressor.initialize_dictionary(256)\n\t        self.assertEqual(len(actual), 256)\n\t        self.assertEqual(\n\t            actual,\n\t            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n\t        )\n", "    def test_256_decompressor(self):\n\t        actual = tamp.decompressor.initialize_dictionary(256)\n\t        self.assertEqual(len(actual), 256)\n\t        self.assertEqual(\n\t            actual,\n\t            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n\t        )\n\t    @unittest.skipIf(micropython is None, \"not running micropython\")\n\t    def test_256_compressor_viper(self):\n\t        import tamp.compressor_viper\n", "        actual = tamp.compressor_viper.initialize_dictionary(256)\n\t        self.assertEqual(len(actual), 256)\n\t        self.assertEqual(\n\t            actual,\n\t            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n\t        )\n\t    @unittest.skipIf(micropython is None, \"not running micropython\")\n\t    def test_256_decompressor_viper(self):\n\t        import tamp.decompressor_viper\n\t        actual = tamp.decompressor_viper.initialize_dictionary(256)\n", "        self.assertEqual(len(actual), 256)\n\t        self.assertEqual(\n\t            actual,\n\t            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n\t        )\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "tests/test_decompressor.py", "chunked_list": ["import unittest\n\tfrom io import BytesIO\n\ttry:\n\t    import micropython\n\texcept ImportError:\n\t    micropython = None\n\tDecompressors = []\n\tdecompresses = []\n\tif micropython is None:\n\t    from tamp.decompressor import Decompressor as PyDecompressor\n", "    from tamp.decompressor import decompress as py_decompress\n\t    Decompressors.append(PyDecompressor)\n\t    decompresses.append(py_decompress)\n\t    try:\n\t        from tamp._c_decompressor import Decompressor as CDecompressor\n\t        from tamp._c_decompressor import decompress as c_decompress\n\t        Decompressors.append(CDecompressor)\n\t        decompresses.append(c_decompress)\n\t    except ImportError:\n\t        pass\n", "else:\n\t    from tamp.decompressor_viper import Decompressor as ViperDecompressor\n\t    from tamp.decompressor_viper import decompress as viper_decompress\n\t    Decompressors.append(ViperDecompressor)\n\t    decompresses.append(viper_decompress)\n\tclass TestDecompressor(unittest.TestCase):\n\t    def test_decompressor_basic(self):\n\t        for Decompressor in Decompressors:\n\t            with self.subTest(Decompressor=Decompressor):\n\t                expected = b\"foo foo foo\"\n", "                compressed = bytes(\n\t                    # fmt: off\n\t                    [\n\t                        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0110011,    # literal \"f\"\n\t                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n\t                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b00011_1_00,   # literal \" \"\n\t                        0b100000_0_1,   # There is now \"foo \" at index 0\n", "                        0b000_00000,    # size=4 -> 0b1000\n\t                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t                        0b00000000,     # index 0 -> 0b0000000000\n\t                        0b00_000000,    # 6 bits of zero-padding\n\t                    ]\n\t                    # fmt: on\n\t                )\n\t                with BytesIO(compressed) as f:\n\t                    decompressor = Decompressor(f)\n\t                    actual = decompressor.read()\n", "                self.assertEqual(actual, expected)\n\t    def test_decompressor_restricted_read_size(self):\n\t        for Decompressor in Decompressors:\n\t            with self.subTest(Decompressor=Decompressor):\n\t                compressed = bytes(\n\t                    # fmt: off\n\t                    [\n\t                        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0110011,    # literal \"f\"\n\t                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n", "                                        # size=2 -> 0b0\n\t                                        # 131 -> 0b0010000011\n\t                        0b00011_1_00,   # literal \" \"\n\t                        0b100000_0_1,   # There is now \"foo \" at index 0\n\t                        0b000_00000,    # size=4 -> 0b1000\n\t                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t                        0b00000000,     # index 0 -> 0b0000000000\n\t                        0b00_000000,    # 6 bits of zero-padding\n\t                    ]\n\t                    # fmt: on\n", "                )\n\t                with BytesIO(compressed) as f:\n\t                    decompressor = Decompressor(f)\n\t                    self.assertEqual(decompressor.read(4), b\"foo \")\n\t                    self.assertEqual(decompressor.read(2), b\"fo\")\n\t                    self.assertEqual(decompressor.read(-1), b\"o foo\")\n\t    def test_decompressor_flushing(self):\n\t        for decompress in decompresses:\n\t            with self.subTest(decompress=decompress):\n\t                compressed = bytes(\n", "                    # fmt: off\n\t                    [\n\t                        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n\t                        0b1_0101000,  # literal 'Q'\n\t                        0b1_0_101010,  # FLUSH_CODE\n\t                        0b11_000000,   # FLUSH CONTINUE\n\t                        0b1_0101011,  # literal 'W'\n\t                        0b1_0_101010,  # FLUSH_CODE\n\t                        0b11_000000,   # FLUSH CONTINUE\n\t                    ]\n", "                    # fmt: on\n\t                )\n\t                decoded = decompress(compressed)\n\t                self.assertEqual(decoded, b\"QW\")\n\t    def test_decompressor_missing_dict(self):\n\t        for Decompressor in Decompressors:\n\t            with self.subTest(Decompressor=Decompressor), self.assertRaises(ValueError), BytesIO(\n\t                bytes([0b000_10_1_0_0])\n\t            ) as f:\n\t                Decompressor(f)\n", "    def test_decompressor_full_output_dst_immediately_after_src(self):\n\t        # Decompressor's perspective of window\n\t        # compressed data: b\"z\"\n\t        #    * 0 write literal \"a\"   -> b\"abcd\"\n\t        #    * 1 write pattern \"abc\"\n\t        custom_dictionary = bytearray(1024)\n\t        custom_dictionary_init = b\"abcd\"\n\t        custom_dictionary[: len(custom_dictionary_init)] = custom_dictionary_init\n\t        data = bytes(\n\t            [\n", "                # fmt: off\n\t                # header (window_bits=10, literal_bits=8, custom)\n\t                0b010_11_1_0_0,\n\t                0b1_0110000,       # literal \"a\"\n\t                0b1_0_11_0000,  # token \"abc\"\n\t                0b000000_00,\n\t                # 2-bit padding\n\t                # fmt: on\n\t            ]\n\t        )\n", "        for Decompressor in Decompressors:\n\t            with self.subTest(Decompressor=Decompressor):\n\t                with BytesIO(data) as f:\n\t                    # Sanity check that without limiting output, it decompresses correctly.\n\t                    decompressor = Decompressor(f, dictionary=bytearray(custom_dictionary))\n\t                    self.assertEqual(decompressor.read(), b\"aabc\")\n\t                with BytesIO(data) as f:\n\t                    decompressor = Decompressor(f, dictionary=bytearray(custom_dictionary))\n\t                    self.assertEqual(decompressor.read(1), b\"a\")\n\t                    self.assertEqual(decompressor.read(1), b\"a\")\n", "                    self.assertEqual(decompressor.read(1), b\"b\")\n\t                    self.assertEqual(decompressor.read(1), b\"c\")\n\t    def test_decompressor_partial_token(self):\n\t        compressed = bytes(\n\t            # fmt: off\n\t            [\n\t                0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n\t                0b1_0110011,    # literal \"f\"\n\t                0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n\t                                # size=2 -> 0b0\n", "                                # 131 -> 0b0010000011\n\t                0b00011_1_00,   # literal \" \"\n\t                0b100000_0_1,   # There is now \"foo \" at index 0\n\t                0b000_00000,    # size=4 -> 0b1000\n\t                ####################### - stream-break here\n\t                0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n\t                0b00000000,     # index 0 -> 0b0000000000\n\t                0b00_000000,    # 6 bits of zero-padding\n\t            ]\n\t            # fmt: on\n", "        )\n\t        expected = b\"foo foo foo\"\n\t        for Decompressor in Decompressors:\n\t            with self.subTest(Decompressor=Decompressor), BytesIO(compressed[:6]) as f:\n\t                decompressor = Decompressor(f)\n\t                read0 = decompressor.read()\n\t                f.write(compressed[6:])\n\t                f.seek(6)\n\t                read1 = decompressor.read()\n\t                self.assertEqual(read0 + read1, expected)\n"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# This file only contains a selection of the most common options. For a full\n\t# list see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Path setup --------------------------------------------------------------\n\timport importlib\n\timport inspect\n\timport sys\n\tfrom datetime import date\n", "from pathlib import Path\n\timport git\n\tsys.path.insert(0, str(Path(\"../..\").absolute()))\n\tfrom tamp import __version__\n\tgit_repo = git.Repo(\".\", search_parent_directories=True)\n\tgit_commit = git_repo.head.commit\n\t# -- Project information -----------------------------------------------------\n\tproject = \"tamp\"\n\tcopyright = f\"{date.today().year}, Brian Pugh\"\n\tauthor = \"Brian Pugh\"\n", "# The short X.Y version.\n\tversion = __version__\n\t# The full version, including alpha/beta/rc tags\n\trelease = __version__\n\t# -- General configuration ---------------------------------------------------\n\t# Add any Sphinx extension module names here, as strings. They can be\n\t# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n\t# ones.\n\textensions = [\n\t    \"sphinx_rtd_theme\",\n", "    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx.ext.linkcode\",\n\t    \"sphinx_copybutton\",\n\t]\n\t# Add any paths that contain templates here, relative to this directory.\n\ttemplates_path = [\"_templates\"]\n\t# List of patterns, relative to source directory, that match files and\n\t# directories to ignore when looking for source files.\n\t# This pattern also affects html_static_path and html_extra_path.\n", "exclude_patterns = []\n\tsmartquotes = False\n\t# Napoleon settings\n\tnapoleon_google_docstring = True\n\tnapoleon_numpy_docstring = True\n\tnapoleon_include_init_with_doc = False\n\tnapoleon_include_private_with_doc = False\n\tnapoleon_include_special_with_doc = True\n\tnapoleon_use_admonition_for_examples = False\n\tnapoleon_use_admonition_for_notes = False\n", "napoleon_use_admonition_for_references = False\n\tnapoleon_use_ivar = False\n\tnapoleon_use_param = True\n\tnapoleon_use_rtype = True\n\tnapoleon_preprocess_types = False\n\tnapoleon_type_aliases = None\n\tnapoleon_attr_annotations = True\n\t# Autodoc\n\tautodoc_default_options = {\n\t    \"members\": True,\n", "    \"member-order\": \"bysource\",\n\t    \"undoc-members\": True,\n\t    \"exclude-members\": \"__weakref__\",\n\t    \"inherited-members\": True,\n\t}\n\tautoclass_content = \"both\"\n\t# LinkCode\n\tcode_url = f\"https://github.com/brianpugh/tamp/blob/{git_commit}\"\n\tdef linkcode_resolve(domain, info):\n\t    \"\"\"Link code to github.\n", "    Modified from:\n\t        https://github.com/python-websockets/websockets/blob/778a1ca6936ac67e7a3fe1bbe585db2eafeaa515/docs/conf.py#L100-L134\n\t    \"\"\"\n\t    # Non-linkable objects from the starter kit in the tutorial.\n\t    if domain == \"js\":\n\t        return\n\t    if domain != \"py\":\n\t        raise ValueError(\"expected only Python objects\")\n\t    if not info.get(\"module\"):\n\t        # Documented via py:function::\n", "        return\n\t    mod = importlib.import_module(info[\"module\"])\n\t    if \".\" in info[\"fullname\"]:\n\t        objname, attrname = info[\"fullname\"].split(\".\")\n\t        obj = getattr(mod, objname)\n\t        try:\n\t            # object is a method of a class\n\t            obj = getattr(obj, attrname)\n\t        except AttributeError:\n\t            # object is an attribute of a class\n", "            return None\n\t    else:\n\t        obj = getattr(mod, info[\"fullname\"])\n\t    try:\n\t        file = inspect.getsourcefile(obj)\n\t        lines = inspect.getsourcelines(obj)\n\t    except TypeError:\n\t        # e.g. object is a typing.Union\n\t        return None\n\t    except OSError:\n", "        # Source code is not available (e.g. cython)\n\t        return None\n\t    if file is None:\n\t        return None\n\t    file = Path(file).resolve().relative_to(git_repo.working_dir)\n\t    if file.parts[0] != \"tamp\":\n\t        # e.g. object is a typing.NewType\n\t        return None\n\t    start, end = lines[1], lines[1] + len(lines[0]) - 1\n\t    return f\"{code_url}/{file}#L{start}-L{end}\"\n", "# -- Options for HTML output -------------------------------------------------\n\t# The theme to use for HTML and HTML Help pages.  See the documentation for\n\t# a list of builtin themes.\n\t#\n\thtml_theme = \"sphinx_rtd_theme\"\n\t# Add any paths that contain custom static files (such as style sheets) here,\n\t# relative to this directory. They are copied after the builtin static files,\n\t# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n\thtml_static_path = [\"_static\"]\n\thtml_title = project\n", "html_logo = \"../../assets/logo_300w.png\"\n\thtml_favicon = \"../../assets/favicon-16x16.png\"\n\thtml_theme_options = {\n\t    # \"analytics_id\": \"G-XXXXXXXXXX\",  # Provided by Google in your dashboard\n\t    # \"analytics_anonymize_ip\": False,\n\t    \"logo_only\": True,\n\t    \"display_version\": False,\n\t    \"prev_next_buttons_location\": \"bottom\",\n\t    \"style_external_links\": False,\n\t    \"vcs_pageview_mode\": \"\",\n", "    \"style_nav_header_background\": \"white\",\n\t    # Toc options\n\t    \"collapse_navigation\": True,\n\t    \"sticky_navigation\": True,\n\t    \"navigation_depth\": 4,\n\t    \"includehidden\": True,\n\t    \"titles_only\": False,\n\t}\n\thtml_context = {\n\t    # Github options\n", "    \"display_github\": True,\n\t    \"github_user\": \"BrianPugh\",\n\t    \"github_repo\": \"tamp\",\n\t    \"github_version\": \"main\",\n\t    \"conf_py_path\": \"/docs/source/\",\n\t}\n\thtml_css_files = [\n\t    \"custom.css\",\n\t]\n"]}
