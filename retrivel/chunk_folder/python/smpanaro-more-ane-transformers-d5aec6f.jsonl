{"filename": "convert.py", "chunked_list": ["import torch\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n\timport coremltools as ct\n\timport numpy as np\n\tfrom datetime import datetime\n\tfrom models.gpt2 import GPT as GPT2\n\tfrom models.pythia import GPT as Pythia\n\tfrom src.utils.psnr import compute_psnr\n\tfrom src.utils.trace_warnings import silence_known_trace_warnings\n\timport argparse\n", "import gc\n\timport sys\n\t\"\"\"\n\tConvert a slightly modified nanoGPT or Huggingface pythia to CoreML.\n\t\"\"\"\n\tall_names = GPT2.model_names() + Pythia.model_names()\n\tparser = argparse.ArgumentParser(description='Convert a model to CoreML.')\n\tparser.add_argument('--model_name', choices=all_names, default=\"gpt2\", type=str)\n\tparser.add_argument('--low_memory', help=\"use less memory at the cost of being slower. useful for large models.\", action=\"store_true\")\n\targs = parser.parse_args()\n", "file_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n\tmodel_name = args.model_name\n\tmodel_filename = model_name.split(\"/\")[-1] + \"_\" + file_suffix\n\tretrace = True\n\tif retrace:\n\t    print(f\"Loading model {model_name}...\")\n\t    model_class = GPT2 if model_filename.startswith(\"gpt2\") else Pythia\n\t    token_predictor = model_class.from_pretrained(model_name).eval()\n\t    input_ids = torch.randint(10000, (1,512,))\n\t    output_mask = torch.randint(512, (1,))\n", "    print(f\"Tracing the model with {input_ids.shape}...\")\n\t    with silence_known_trace_warnings(model_name):\n\t        traced_token_predictor = torch.jit.trace(token_predictor, (input_ids, output_mask))\n\telse:\n\t    print(\"Loading from saved file.\")\n\t    traced_token_predictor = torch.jit.load(f\"{model_filename}.pt\")\n\t# print(traced_token_predictor)\n\tprint(\"Trace finished.\")\n\tprint(\"Beginning conversion...\")\n\tdef op_selector(op):\n", "    \"\"\"\n\t    Return true to use float16 for the op. Must be f16 to run on Neural Engine.\n\t    You can find op_type by looking in Netron and/or print out the op type/name here\n\t    (usually the names contain a variable name).\n\t    \"\"\"\n\t    # LayerNorm is where we lose most of our precision. From experiments\n\t    # in optimizing for ANE, it's most likely the computing the first mean,\n\t    # but using the non-optimized version we have to float32 the whole layer norm.\n\t    return op.op_type not in [\"layer_norm\"]\n\tcompute_precision=ct.precision.FLOAT16\n", "if model_name in [\"gpt2\"]:\n\t    print(\"Using float32 for layer_norm otherwise the precision lost is too large.\")\n\t    print(\"Larger models can use all float16.\") #... and run purely on the neural engine.\n\t    compute_precision=ct.transform.FP16ComputePrecision(op_selector)\n\tif args.low_memory:\n\t    del token_predictor\n\t    gc.collect()\n\tmlmodel = ct.convert(\n\t    traced_token_predictor,\n\t    inputs=[\n", "        ct.TensorType(name=\"input_ids\", shape=[1, 512], dtype=np.int32),\n\t        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n\t    ],\n\t    outputs=[\n\t        ct.TensorType(name=\"logits\", dtype=np.float32),\n\t    ],\n\t    compute_precision=compute_precision,\n\t    convert_to=\"mlprogram\",\n\t)\n\tprint(\"Conversion finished.\")\n", "if args.low_memory:\n\t    del traced_token_predictor\n\t    gc.collect()\n\t    print(\"Saving...\")\n\t    mlmodel.save(f\"{model_filename}.mlpackage\")\n\t    del mlmodel\n\t    gc.collect()\n\t    print(\"Adding metadata...\")\n\t    mlmodel = ct.models.MLModel(f\"{model_filename}.mlpackage\", skip_model_load=True)\n\t# TODO: Clean up.\n", "pretty_name = {\n\t    \"gpt2\": \"gpt2 (124M)\",\n\t    \"gpt2-medium\": \"gpt2-medium (350M)\",\n\t    \"gpt2-large\": \"gpt2-large (774M)\",\n\t    \"gpt2-xl\": \"gpt2-xl (1558M)\",\n\t}.get(model_name, model_name)\n\tmodel_family = [x for x in [\"gpt2\", \"pythia\"] if x in model_name][0]\n\teos_token_id = {\"gpt2\": 50256, \"pythia\": 0}[model_family]\n\tbased_on = {\"gpt2\": \"nanoGPT\", \"pythia\": \"the HuggingFace implementation\"}[model_family]\n\tvocab_size = {\"gpt2\": 50257, \"pythia\": 50304}[model_family] if model_name != \"pythia-6.9b\" else 50432\n", "mlmodel.short_description = f\"{pretty_name} for text generation. Based on {based_on}. Optimized for Apple Neural Engine.\"\n\tmlmodel.input_description[\"input_ids\"] = f\"Input tokens. e.g. from the huggingface {model_family} tokenizer. Pad to the full length with {eos_token_id} (eos).\"\n\tmlmodel.input_description[\"output_mask\"] = \"A single element array with the index of your sequence to predict. If your non-padded input length was N, pass [N-1].\"\n\tmlmodel.output_description[\"logits\"] = f\"Predictions for the element of input_ids specified by output_mask in the shape (1, 1, {vocab_size}). \"\n\tmlmodel.user_defined_metadata[\"Converted By\"] = \"http://twitter.com/flat\"\n\tmlmodel.user_defined_metadata[\"URL\"] = \"https://github.com/smpanaro/more-ane-transformers\"\n\tif not args.low_memory:\n\t    print(\"Saving...\")\n\t# Workaround to save metadata: https://github.com/apple/coremltools/issues/1680\n\tto_save = ct.models.MLModel(mlmodel._spec,\n", "                  weights_dir=mlmodel._weights_dir,\n\t                  is_temp_package=True)\n\tto_save.save(f\"{model_filename}.mlpackage\")\n\tif args.low_memory:\n\t    print(\"Skipping model comparison due to low memory mode.\")\n\t    print(\"Conversion complete.\")\n\t    sys.exit(0)\n\t# Always compare in float32 so we don't overflow.\n\twith torch.no_grad():\n\t    og_out = token_predictor(input_ids, output_mask).to(torch.float32)\n", "    tr_out = traced_token_predictor(input_ids, output_mask).to(torch.float32)\n\tinput_ids = input_ids.int()\n\toutput_mask = output_mask.int()\n\t# Hanging here? It's very likely your intputs are the wrong shape and/or types.\n\tprint(\"predicting with mlmodel\")#, input_ids.shape, input_ids.dtype)\n\tcm_out = mlmodel.predict({\"input_ids\": input_ids.numpy(), \"output_mask\": output_mask.numpy()})\n\tcm_out = torch.from_numpy(cm_out[\"logits\"]).to(torch.float32)\n\tassert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\n\tassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"\n\ttrace_psnr = compute_psnr(og_out, tr_out)\n", "if trace_psnr < 200:\n\t    print(f\"tracing PSNR too low ({trace_psnr}), CoreML model will likely be unusable\")\n\tprint(\"\\nfinished. these should be >60, ideally much higher (inf is perfect). lower and the model may not be usable\")\n\tprint(\"coreml-traced   psnr:\", compute_psnr(tr_out.numpy(), cm_out.numpy()))\n\tprint(\"coreml-original psnr:\", compute_psnr(og_out.numpy(), cm_out.numpy()))\n\tif model_name in [\"gpt2-xl\"]:\n\t    print(\"\\nðŸ‘‹ This model is big. It will run fast if you have a recent Mac with a fast GPU.\")\n\t    print(\"If not you can download a version that runs on the Neural Engine from the releases tab on GitHub.\")\n\t    print(\"If you want to build it yourself follow these steps:\")\n\t    print(\"1. Install coremltools >= 6.3\")\n", "    print(f\"2. Run: python -m src.experiments.chunk_model --mlpackage-path {model_filename}.mlpackage -o .\")\n\t    print(\"3. Edit src/experiments/make_pipeline.py to use the chunked files written by the above command.\")\n\t    print(\"4. Run: python -m src.experiments.make_pipeline\")\n\t    print(\"Use the output *-pipeline.mlpackage with generate.py as usual.\")"]}
{"filename": "generate.py", "chunked_list": ["from src.ml_ane_transformers.ane_gpt2 import GPT as AneGPT\n\tfrom src.utils.model_proxy import MLModelProxy\n\tfrom transformers import AutoTokenizer\n\timport torch\n\timport torch.nn.functional as F\n\timport numpy as np\n\timport coremltools as ct\n\tfrom stopwatch import Stopwatch\n\tfrom models.gpt2 import GPT as GPT2\n\tfrom models.pythia import GPT as Pythia\n", "import argparse\n\timport sys\n\timport os\n\timport glob\n\tfrom collections import OrderedDict\n\timport subprocess\n\t\"\"\"\n\tLoad a CoreML model and use it to generate text.\n\t\"\"\"\n\tos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n", "compute_unit_by_name = OrderedDict([\n\t    (\"All\", ct.ComputeUnit.ALL),\n\t    (\"CPUOnly\", ct.ComputeUnit.CPU_ONLY),\n\t    (\"CPUAndGPU\", ct.ComputeUnit.CPU_AND_GPU),\n\t    (\"CPUAndANE\", ct.ComputeUnit.CPU_AND_NE),\n\t])\n\tparser = argparse.ArgumentParser(description='Load a CoreML modelpackage and generate some text.')\n\tparser.add_argument('--model_path', help='path to .mlpackage file', default=\"gpt2.mlpackage\", type=str)\n\tparser.add_argument('--input_prompt', help='input prompt for the model', default=\"Before boarding your rocket to Mars, remember to pack these items:\", type=str)\n\tparser.add_argument('--compute_unit', help='compute unit', type=str, choices=list(compute_unit_by_name.keys()), default=\"All\")\n", "parser.add_argument('--length', help='number of new tokens to generate', type=int, default=40)\n\tparser.add_argument('--verbose', help='print verbose logs', type=bool, default=False)\n\tparser.add_argument('--wait', help='wait for confirmation before loading the model (ie to attach a debugger)', action=\"store_true\")\n\tparser.add_argument('--use-mlpackage', help='don\\'t automatically generate a mlmodelc and use it. dramatically slower but useful for debugging this script.', action=\"store_true\")\n\targs = parser.parse_args()\n\tif not args.model_path.endswith('.mlpackage') and not args.model_path.endswith('.mlmodelc') :\n\t    print('Error: Model path must end in .mlpackage (or .mlmodelc if you know what you\\'re doing)')\n\t    sys.exit(1)\n\t# Special handling for first-time run.\n\tif not os.path.exists(args.model_path) and args.model_path == \"gpt2.mlpackage\":\n", "    files = glob.glob('gpt2*.mlpackage')\n\t    files = sorted(files, key=lambda x: os.path.getmtime(x))\n\t    if len(files) == 0:\n\t        print(f\"Couldn't find {args.model_path}. Either use the --model_path argument or run convert.py to generate one.\")\n\t        sys.exit(1)\n\t    args.model_path = files[-1]\n\tcompute_unit = compute_unit_by_name[args.compute_unit]\n\tdef vprint(*pargs, **kwargs):\n\t    if args.verbose:\n\t        print(*pargs, **kwargs)\n", "def get_tokenizer_name(model_path):\n\t    names = GPT2.model_names() + Pythia.model_names()\n\t    tokenizer_lookup = {**GPT2.tokenizer_by_name(), **Pythia.tokenizer_by_name()}\n\t    for n in sorted(names, key=len):\n\t        if model_path.startswith(n):\n\t            return tokenizer_lookup[n]\n\t    print(f\"No tokenizer found for {model_path}\")\n\t    print(f\"Model name must start with one of:\")\n\t    print(names)\n\t    return None\n", "tokenizer_name = get_tokenizer_name(args.model_path)\n\tif tokenizer_name is None:\n\t    sys.exit(1)\n\tvprint(\"Loading tokenizer...\")\n\ttok = AutoTokenizer.from_pretrained(tokenizer_name)\n\ttok.pad_token_id = tok.eos_token_id\n\tvprint(\"Loaded tokenizer.\")\n\tif args.wait:\n\t    print(f\"Current PID: {os.getpid()}\")\n\t    input(\"Waiting. Press Enter to continue.\")\n", "# Compile to make generations 2-n much much faster.\n\tbase_path = args.model_path.replace(\".mlpackage/\", \"\").replace(\".mlmodelc/\", \"\").replace(\".mlpackage\", \"\").replace(\".mlmodelc\", \"\")\n\tmlpackage_path = base_path + \".mlpackage\"\n\tmlmodelc_path = base_path + \".mlmodelc\"\n\thas_compiled_model = os.path.exists(mlmodelc_path)\n\tif not has_compiled_model:\n\t    # Looking to turn this off? As far as I know it's not worth it.\n\t    # Generating text from a mlpackage does this same compilation every time (slow) and\n\t    # it doesn't get cached so you will actually use _more_ disk space without this.\n\t    # It's also much faster to load the model this way. For the xl model this will\n", "    # take model loading from 1.5 minutes to 2.5 seconds.\n\t    print(\"Compiling model. This first run will be slow but all subsequent runs will be significantly faster.\")\n\t    cmd = f\"xcrun coremlcompiler compile {mlpackage_path} .\"\n\t    compile_result = subprocess.run(cmd, shell=True)\n\t    has_compiled_model = compile_result.returncode == 0\n\t    if not has_compiled_model:\n\t        print(\"Failed to compile. Please open an issue (https://github.com/smpanaro/more-ane-transformers/issues) and include the following:\")\n\t        print(f\"code: {compile_result.returncode}\\nstdout: {compile_result.stdout}\\nstderr: {compile_result.stderr}\")\n\t        print(\"Predicting using the (slow) mlpackage method.\")\n\tif has_compiled_model and not os.path.exists(mlpackage_path):\n", "    # TODO: Dump metadata to disk instead so you can keep just the compiled model.\n\t    print(f\"No matching mlpackage found for {mlmodelc_path}. Can't predict without that.\")\n\t    print(f\"It should be at: {mlpackage_path}\")\n\t    sys.exit(1)\n\t# nano = NanoGPT.from_pretrained(\"gpt2\").eval()\n\tprint(f\"Loading model from path {mlmodelc_path if has_compiled_model else mlpackage_path} using {compute_unit}...\")\n\tload_stopwatch = Stopwatch(3)\n\tmodel, model_with_metadata = None, None\n\tif has_compiled_model:\n\t    model = MLModelProxy(mlmodelc_path, compute_unit)\n", "    # So we can inspect and see what the inputs are.\n\t    model_with_metadata = ct.models.model.MLModel(mlpackage_path, compute_units=compute_unit, skip_model_load=True)\n\telse:\n\t    model = ct.models.model.MLModel(mlpackage_path, compute_units=compute_unit)\n\t    model_with_metadata = model\n\tload_stopwatch.stop()\n\tprint(f\"Loaded model in {load_stopwatch}.\")\n\t# print(model)\n\tdef sample(logits, temperature=0.85, top_k=80):\n\t    if isinstance(logits, np.ndarray):\n", "        logits = torch.from_numpy(logits)\n\t    # pluck the logits at the final step and scale by desired temperature\n\t    logits = logits[:, -1, :] / temperature\n\t    # optionally crop the logits to only the top k options\n\t    if top_k is not None:\n\t        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n\t        logits[logits < v[:, [-1]]] = -float('Inf')\n\t    probs = torch.nn.functional.softmax(logits, dim=-1)\n\t    return torch.multinomial(probs.squeeze(), num_samples=1)\n\ttext = args.input_prompt\n", "inputs = tok(text, return_tensors=\"pt\")\n\tvprint(\"Tokenized initial inputs:\", inputs[\"input_ids\"].shape)\n\tane_inputs = AneGPT.build_inputs(inputs['input_ids'], pad_to_length=512, pad_token_id=tok.pad_token_id)\n\tvprint(\"Generated initial inputs:\")\n\tvprint({k: v.shape for k,v in ane_inputs.items()})\n\tvprint({k: v.dtype for k,v in ane_inputs.items()})\n\t# vprint({k: v.__class__ for k,v in ane_inputs.items()})\n\tdef get_start_idx(ids):\n\t    ids = ids.tolist()[0]\n\t    if tok.pad_token_id in ids:\n", "        return ids.index(tok.pad_token_id)\n\t    return len(ids)\n\tdef from_numpy(d):\n\t    return {k: torch.from_numpy(v) for k,v in d.items()}\n\tdef without_pad(ids):\n\t    return ids[ids != tok.pad_token_id].unsqueeze(0)\n\tstopwatch = Stopwatch(3)\n\tstopwatch.stop()\n\tstopwatch.reset()\n\tNUM_INFERENCES = args.length\n", "input_keys = set([f.name for f in model_with_metadata.input_description._fd_spec])\n\trelevant_tokens = without_pad(ane_inputs[\"input_ids\"])\n\tfor i in range(NUM_INFERENCES):\n\t    next_index = len(relevant_tokens[0]) - 1\n\t    ane_inputs = AneGPT.build_inputs(relevant_tokens, pad_to_length=512, pad_token_id=tok.pad_token_id)\n\t    ane_inputs = {k:v for k,v in ane_inputs.items() if k in input_keys}\n\t    # attention_mask = ane_inputs[\"k_mask\"].squeeze().unsqueeze(0)\n\t    # print(attention_mask.shape)\n\t    stopwatch.start()\n\t    # Hanging here? It's very likely your intputs are the wrong shape and/or types.\n", "    logits = model.predict(ane_inputs)[\"logits\"] # nano\n\t    # logits = nano(ane_inputs[\"input_ids\"], attention_mask)\n\t    stopwatch.stop()\n\t    # If the model does not pre-select the next token logits, do so now.\n\t    if logits.shape[1] > 1:\n\t        logits = logits[:, [next_index], :]\n\t    ane_next = sample(logits) #ane_inputs['input_ids'], qk_mask=ane_inputs['qk_mask']))\n\t    # Helpful for debugging nonsense generations.\n\t    # print(torch.topk(torch.from_numpy(logits), 20, dim=-1).indices[:, :20, :])\n\t    # print(\"chose\", ane_next, \"from idx:\", next_index)\n", "    relevant_tokens = torch.cat((relevant_tokens.squeeze(), torch.tensor([ane_next]))).unsqueeze(0)\n\t    if i == 0:\n\t        print(f\"\\n\\033[95m[Prompt] {tok.decode(relevant_tokens.squeeze())}\\033[0m\", end=\"\")\n\t    else:\n\t        print(tok.decode(ane_next), end=\"\")\n\t    sys.stdout.flush()\n\tprint(\"\\n\\n---stats---\")\n\tper_inference = \"{:.{}f}ms\".format((stopwatch.duration / NUM_INFERENCES) * 1000, 2)\n\tprint(\"Compute Unit:\", args.compute_unit)\n\tprint(stopwatch, \"total\")\n", "print(f\"{per_inference}/it\")"]}
{"filename": "models/gpt2.py", "chunked_list": ["\"\"\"\n\tOriginally this was going to be nanoGPT with the bare minimum modifications required\n\tso that it can be converted to CoreML and compared to the ANE implementation. Turns\n\tout it's quite fast out of the box, so tweaked it further to gain more speed.\n\tFrom: https://github.com/karpathy/nanoGPT/blob/a82b33b525ca9855d705656387698e13eb8e8d4b/model.py#L1\n\tSee \"ANE:\" comments for interesting things.\n\tOriginal License:\n\tMIT License\n\tCopyright (c) 2022 Andrej Karpathy\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n", "of this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n", "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\"\"\"\n\timport math\n\timport inspect\n\tfrom dataclasses import dataclass\n\tfrom collections import OrderedDict\n\timport torch\n", "import torch.nn as nn\n\tfrom torch.nn import functional as F\n\t@dataclass\n\tclass CacheConfig:\n\t    kv_cache: torch.Tensor # [num_layers, 1, 2*seqlen, n_embd]\n\t    kv_mask: torch.BoolTensor # for masking oldk/oldv [1, seqlen, n_embd], can't build in the model (see comments in Attention class)\n\t    output_mask: torch.Tensor # [1]\n\t    head_index: int\n\t# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n\tdef new_gelu(x):\n", "    \"\"\"\n\t    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n\t    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n\t    \"\"\"\n\t    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\tclass LayerNorm(nn.Module):\n\t    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\t    def __init__(self, ndim, bias):\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(ndim))\n", "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\t    def forward(self, input):\n\t        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\tclass CausalSelfAttention(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.n_embd % config.n_head == 0\n\t        # key, query, value projections for all heads, but in a batch\n\t        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n\t        # output projection\n", "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\t        # regularization\n\t        self.attn_dropout = nn.Dropout(config.dropout)\n\t        self.resid_dropout = nn.Dropout(config.dropout)\n\t        self.n_head = config.n_head\n\t        self.n_embd = config.n_embd\n\t        self.dropout = config.dropout\n\t        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n\t        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n\t        # Don't need this for ANE.\n", "        # if not self.flash:\n\t        #     print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n\t            # causal mask to ensure that attention is only applied to the left in the input sequence\n\t            # self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size, dtype=torch.float16))\n\t            #                             .view(1, 1, config.block_size, config.block_size))\n\t    def forward(self, x, attention_mask, kv_config):\n\t        # kv_cache (B, T*2, C)\n\t        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\t        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n\t        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n", "        if kv_config is not None and kv_config.kv_cache is not None:\n\t            kv_cache = kv_config.kv_cache\n\t            kv_mask = kv_config.kv_mask\n\t            new_q, new_k, new_v = q,k,v # new_q, new_k, new_v = [B, 1, n_embd]\n\t            old_k, old_v = kv_cache.chunk(2, dim=1) # each (B, T, C)\n\t            # replace the row indicated by output_mask with the new values\n\t            # Many ways to overwrite a row in a matrix (old K) with a new value (new K).\n\t            # One of them works on ANE.\n\t            # op not supported\n\t            # k = torch.index_copy(old_k, 1, output_mask, new_k)\n", "            # v = torch.index_copy(old_v, 1, output_mask, new_v)\n\t            # https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\n\t            # oldk.scatter(1, torch.tensor([3]).repeat(4).unsqueeze(0).unsqueeze(0), newk) # something like this\n\t            # scatter doesn't run on ANE though :(\n\t            # idx = output_mask.repeat(old_k.shape[2]).unsqueeze(0).unsqueeze(0)\n\t            # k = old_k.scatter(1, idx, new_k)\n\t            # v = old_v.scatter(1, idx, new_v)\n\t            # Like index_copy but supported by coremltools.\n\t            # Problem is it creates a symbolic variable (the :output_mask slicing) which technically\n\t            # has a different type than the non-cached case, so can't be used in a branched if/else model.\n", "            # Also not sure this is numerically accurate.\n\t            # And also doesn't seem to run on the ANE.\n\t            # k = torch.cat([old_k[:, :output_mask, :], new_k, old_k[:, output_mask+1:, :]], dim=1)\n\t            # v = torch.cat([old_v[:, :output_mask, :], new_v, old_v[:, output_mask+1:, :]], dim=1)\n\t            # Fails in index_put AttributeError: 'NoneType' object has no attribute 'sym_type' (the first colon)\n\t            # k = old_k\n\t            # k[:, output_mask] = new_k\n\t            # v = old_v\n\t            # v[:, output_mask] = new_v\n\t            # Equivalent to cat dim=1, same problems.\n", "            # k = torch.hstack([old_k[:, :output_mask, :], new_k, old_k[:, output_mask+1:, :]])\n\t            # v = torch.hstack([old_v[:, :output_mask, :], new_v, old_v[:, output_mask+1:, :]])\n\t            # Fails in index_put AttributeError: 'NoneType' object has no attribute 'sym_type' (the first colon)\n\t            # This would use scatter_nd under the hood anyways which is not ANE compatible.\n\t            # mask = torch.zeros_like(old_k)\n\t            # mask[:, output_mask, :] = 1\n\t            # k = torch.where(mask.bool(), new_k, old_k)\n\t            # v = torch.where(mask.bool(), new_v, old_v)\n\t            # This works, but you have to pass the mask along (on the plus side, you only compute it once).\n\t            k = torch.where(kv_mask, old_k, new_k)\n", "            v = torch.where(kv_mask, old_v, new_v)\n\t            q = new_q\n\t            B,T,C = k.size()\n\t        current_cache = torch.cat([k,v], dim=1)\n\t        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        q = q.view(B, q.size()[1], self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n\t        if self.flash:\n\t            # efficient attention using Flash Attention CUDA kernels\n", "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n\t        else:\n\t            # manual implementation of attention\n\t            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t            # ANE: Using a stored bias makes the model file larger (from a little to a lot)\n\t            # because it is copied into the model proto for every usage. Additionally, mask\n\t            # fill only runs on the CPU and moving from ANE <-> CPU is sloooow.\n\t            # Instead follow the approach from ml-ane-transformers, subtract a large but\n\t            # float16-friendly value so that masked values are effectively ignored in the softmax.\n\t            # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-1e4'))\n", "            att = att + attention_mask\n\t            att = F.softmax(att, dim=-1)\n\t            att = self.attn_dropout(att)\n\t            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\t        y = y.transpose(1, 2).contiguous().view(B, -1, C) # re-assemble all head outputs side by side\n\t        # output projection\n\t        y = self.resid_dropout(self.c_proj(y))\n\t        return y, current_cache\n\tclass MLP(nn.Module):\n\t    def __init__(self, config):\n", "        super().__init__()\n\t        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n\t        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n\t        self.dropout = nn.Dropout(config.dropout)\n\t    def forward(self, x):\n\t        x = self.c_fc(x)\n\t        x = new_gelu(x)\n\t        x = self.c_proj(x)\n\t        x = self.dropout(x)\n\t        return x\n", "class Block(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n\t        self.attn = CausalSelfAttention(config)\n\t        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n\t        self.mlp = MLP(config)\n\t    def forward(self, x, attention_mask=None, kv_config=None):\n\t        attention_output, new_kv_cache = self.attn(self.ln_1(x), attention_mask, kv_config)\n\t        x = x + attention_output\n", "        x = x + self.mlp(self.ln_2(x))\n\t        return x, new_kv_cache\n\t@dataclass\n\tclass GPTConfig:\n\t    block_size: int = 1024\n\t    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n\t    n_layer: int = 12\n\t    n_head: int = 12\n\t    n_embd: int = 768\n\t    dropout: float = 0.0\n", "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\tclass GPT(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.vocab_size is not None\n\t        assert config.block_size is not None\n\t        self.config = config\n\t        self.transformer = nn.ModuleDict(dict(\n\t            wte = nn.Embedding(config.vocab_size, config.n_embd),\n\t            wpe = nn.Embedding(config.block_size, config.n_embd),\n", "            drop = nn.Dropout(config.dropout),\n\t            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n\t            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n\t        ))\n\t        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\t        # with weight tying when using torch.compile() some warnings get generated:\n\t        # \"UserWarning: functional_call was passed multiple values for tied weights.\n\t        # This behavior is deprecated and will be an error in future versions\"\n\t        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n\t        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n", "        # init all weights\n\t        self.apply(self._init_weights)\n\t        # apply special scaled init to the residual projections, per GPT-2 paper\n\t        for pn, p in self.named_parameters():\n\t            if pn.endswith('c_proj.weight'):\n\t                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\t        # report number of parameters\n\t        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\t    def get_num_params(self, non_embedding=True):\n\t        \"\"\"\n", "        Return the number of parameters in the model.\n\t        For non-embedding count (default), the position embeddings get subtracted.\n\t        The token embeddings would too, except due to the parameter sharing these\n\t        params are actually used as weights in the final layer, so we include them.\n\t        \"\"\"\n\t        n_params = sum(p.numel() for p in self.parameters())\n\t        if non_embedding:\n\t            n_params -= self.transformer.wpe.weight.numel()\n\t        return n_params\n\t    def _init_weights(self, module):\n", "        if isinstance(module, nn.Linear):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t            if module.bias is not None:\n\t                torch.nn.init.zeros_(module.bias)\n\t        elif isinstance(module, nn.Embedding):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t    def forward(self, idx, output_mask=None, kv_cache=None, kv_mask=None, seqlen=512):\n\t        assert kv_cache is None or idx.shape[1] == 1, f\"kv cache is only supported for single token inputs not {idx.shape}\"\n\t        return_kv_cache = kv_cache is not None\n\t        device = idx.device\n", "        b, t = idx.size()\n\t        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\t        # ANE: Since we are only inferring and we only care about predicting the next token,\n\t        # we can use the same triangular mask always. May need to change this to support flexible sizes.\n\t        attention_mask = (1 - torch.tril(torch.ones((1,1,seqlen,seqlen), dtype=torch.float32))) * -1e4\n\t        # kv_cache [# layers, batch size 1, seqlen * 2 = 512*2, hidden size 768]\n\t        if kv_cache is None:\n\t            kv_cache = [None]*len(self.transformer.h)\n\t        else:\n", "            pos = output_mask.unsqueeze(0)\n\t            attention_mask = torch.index_select(attention_mask, 2, output_mask)\n\t        # forward the GPT model itself\n\t        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n\t        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n\t        x = self.transformer.drop(tok_emb + pos_emb)\n\t        kv_mask_bool = kv_mask.bool() if kv_mask is not None else None\n\t        new_kv_cache = []\n\t        head_index = 0\n\t        for (block, cache) in zip(self.transformer.h, kv_cache):\n", "            kv_config = CacheConfig(cache, kv_mask_bool, output_mask, head_index)\n\t            x, block_new_kv_cache = block(x, attention_mask, kv_config)\n\t            new_kv_cache.append(block_new_kv_cache)\n\t            head_index += 1\n\t        # No need to compute anything else for the other length-1 tokens.\n\t        # This shaves ~30% off of gpt2-xl when running on CPU+ANE.\n\t        if output_mask is not None and t > 1:\n\t            x = torch.index_select(x, 1, output_mask)\n\t        x = self.transformer.ln_f(x)\n\t        logits = self.lm_head(x)\n", "        # [# layers, batch size 1, seqlen * 2 = 512*2, hidden size 768]\n\t        # same as input\n\t        new_kv_cache = torch.stack(new_kv_cache)\n\t        if return_kv_cache:\n\t            return logits, new_kv_cache\n\t        return logits\n\t    def crop_block_size(self, block_size):\n\t        # model surgery to decrease the block size if necessary\n\t        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n\t        # but want to use a smaller block size for some smaller, simpler model\n", "        assert block_size <= self.config.block_size\n\t        self.config.block_size = block_size\n\t        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n\t        for block in self.transformer.h:\n\t            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\t    @staticmethod\n\t    def config_args():\n\t        # ckiplab/gpt2-tiny-chinese is a good tiny model for experimenting, vocab size 21128\n\t        return OrderedDict({\n\t            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n", "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n\t            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n\t            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n\t        })\n\t    @staticmethod\n\t    def model_names():\n\t        return list(GPT.config_args().keys())\n\t    @staticmethod\n\t    def tokenizer_by_name():\n\t        return {n:n for n in GPT.model_names()}\n", "    @classmethod\n\t    def from_pretrained(cls, model_type, override_args=None):\n\t        assert model_type in GPT.model_names()\n\t        override_args = override_args or {} # default to empty dict\n\t        # only dropout can be overridden see more notes below\n\t        assert all(k == 'dropout' for k in override_args)\n\t        from transformers import GPT2LMHeadModel\n\t        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\t        # n_layer, n_head and n_embd are determined from model_type\n\t        config_args = GPT.config_args()[model_type]\n", "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n\t        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n\t        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n\t        config_args['bias'] = True # always True for GPT model checkpoints\n\t        # we can override the dropout rate, if desired\n\t        if 'dropout' in override_args:\n\t            print(f\"overriding dropout rate to {override_args['dropout']}\")\n\t            config_args['dropout'] = override_args['dropout']\n\t        # create a from-scratch initialized minGPT model\n\t        config = GPTConfig(**config_args)\n", "        model = GPT(config)\n\t        sd = model.state_dict()\n\t        sd_keys = sd.keys()\n\t        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\t        # init a huggingface/transformers model\n\t        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n\t        sd_hf = model_hf.state_dict()\n\t        # copy while ensuring all of the parameters are aligned and match in names and shapes\n\t        sd_keys_hf = sd_hf.keys()\n\t        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n", "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n\t        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n\t        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n\t        # this means that we have to transpose these weights when we import them\n\t        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n\t        for k in sd_keys_hf:\n\t            if any(k.endswith(w) for w in transposed):\n\t                # special treatment for the Conv1D weights we need to transpose\n\t                assert sd_hf[k].shape[::-1] == sd[k].shape\n\t                with torch.no_grad():\n", "                    sd[k].copy_(sd_hf[k].t())\n\t            else:\n\t                # vanilla copy over the other parameters\n\t                assert sd_hf[k].shape == sd[k].shape\n\t                with torch.no_grad():\n\t                    sd[k].copy_(sd_hf[k])\n\t        return model\n\t    @torch.no_grad()\n\t    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n\t        \"\"\"\n", "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n\t        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n\t        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n\t        \"\"\"\n\t        for _ in range(max_new_tokens):\n\t            # if the sequence context is growing too long we must crop it at block_size\n\t            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n\t            # forward the model to get the logits for the index in the sequence\n\t            logits, _ = self(idx_cond)\n\t            # pluck the logits at the final step and scale by desired temperature\n", "            logits = logits[:, -1, :] / temperature\n\t            # optionally crop the logits to only the top k options\n\t            if top_k is not None:\n\t                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n\t                logits[logits < v[:, [-1]]] = -float('Inf')\n\t            # apply softmax to convert logits to (normalized) probabilities\n\t            probs = F.softmax(logits, dim=-1)\n\t            # sample from the distribution\n\t            idx_next = torch.multinomial(probs, num_samples=1)\n\t            # append sampled index to the running sequence and continue\n", "            idx = torch.cat((idx, idx_next), dim=1)\n\t        return idx\n\tif __name__ == \"__main__\":\n\t    import numpy as np\n\t    def build_kv_mask(output_mask, seqlen=512, hidden_size=768):\n\t        kv_mask = torch.ones(1, seqlen, hidden_size, dtype=torch.int32)\n\t        kv_mask[:, output_mask, :] = 0\n\t        return kv_mask\n\t    model = GPT.from_pretrained(\"gpt2\").eval()\n\t    # Check numeric accuracy before converting to CoreML. It's not exact, might be a bug.\n", "    # if True:\n\t    if False:\n\t        input_ids = torch.randint(10_000, (1, 10,))\n\t        with torch.no_grad():\n\t            base_prediction, _ = model(input_ids, torch.tensor([4]), seqlen=10)\n\t            print(\"^base ------ first v\")\n\t            out, out_cache = model(input_ids, torch.tensor([3]), seqlen=10)\n\t        print(\"output cache:\", out_cache.shape)\n\t        print(\"----\")\n\t        with torch.no_grad():\n", "            out_new, out_new_cache = model(input_ids[:,[4]], torch.tensor([4]), out_cache, seqlen=10)\n\t        print(base_prediction[:, 0:6, 0:4])\n\t        print(out[:, 0, 0:4])\n\t        print(out_new[:, 0, 0:4])\n\t        print(\"cache sizes\", out_cache.shape, out_new_cache.shape)\n\t        print(\"eq?\", torch.equal(out_new[:, 0, :], base_prediction[:, 0, :])) # why aren't these equal?\n\t        print(\"close?\")\n\t        np.testing.assert_allclose(out_new[:, 0, :], base_prediction[:, 0 ,:])\n\t    # Work out the pattern what inputs to pass when.\n\t    # if True:\n", "    if False:\n\t        from transformers import AutoTokenizer\n\t        tok = AutoTokenizer.from_pretrained(\"gpt2\")\n\t        seq = tok(\"would the real slim\", return_tensors=\"pt\")[\"input_ids\"]\n\t        input_ids = torch.cat([\n\t            seq.squeeze(),\n\t            torch.full((20 - seq.shape[-1],), tok.eos_token_id)\n\t        ]).unsqueeze(0)\n\t        kvc = None\n\t        outs = []\n", "        with torch.no_grad():\n\t            for i in range(5):\n\t                seqlen = seq.shape[-1]+i\n\t                print(\"i\", i, seqlen)\n\t                inputs = input_ids if kvc is None else input_ids[:, [seqlen-1]]\n\t                output_mask = torch.tensor([seqlen-1])\n\t                kv_mask = build_kv_mask(output_mask, seqlen=20, hidden_size=768)\n\t                out, kvc = model(inputs, output_mask=output_mask, kv_cache=kvc, kv_mask=kv_mask, seqlen=20)\n\t                input_ids[0][seqlen] = out.argmax()\n\t                outs.append(out)\n", "        print(tok.decode(input_ids.squeeze().tolist()))\n\t    # Try to build a branching model that can be converted to CoreML.\n\t    # Doesn't run on the Neural Engine and includes duplicate copies of the weights.\n\t    # Would also be difficult/impossible to split into a pipeline for gpt2-xl.\n\t    if True:\n\t    # if False:\n\t        import coremltools as ct\n\t        class DoubleGPT(nn.Module):\n\t            def __init__(self, model_name, seqlen, num_layers, hidden_size):\n\t                super().__init__()\n", "                self.cached = GPT.from_pretrained(model_name)\n\t                self.full = self.cached\n\t                input_ids = torch.randint(10_000, (1, seqlen,))\n\t                kv_cache = torch.zeros(num_layers, 1, seqlen*2,hidden_size)\n\t                kv_mask = torch.zeros(1, seqlen, hidden_size)\n\t                self.cached = torch.jit.trace(self.cached, (input_ids[:, [3]], torch.tensor([3]), kv_cache, kv_mask))\n\t                self.full = torch.jit.trace(self.full, (input_ids, torch.tensor([3])))\n\t            def forward(self, x, output_mask, kv_cache, kv_mask):\n\t                if kv_cache.min() != 0:\n\t                    ci = torch.index_select(x, 1, output_mask)\n", "                    y, new_cache = self.cached(ci, output_mask, kv_cache, kv_mask)\n\t                else:\n\t                    y, new_cache = self.full(x, output_mask)\n\t                return y, new_cache\n\t        seqlen = 512\n\t        # model_name = \"gpt2\"\n\t        # num_layers = 12\n\t        # hidden_size = 768\n\t        model_name = \"gpt2-medium\"\n\t        num_layers = 24\n", "        hidden_size = 1024\n\t        input_ids = torch.randint(10_000, (1, seqlen,))\n\t        # traced_model = torch.jit.trace(model, (input_ids, torch.tensor([3]), torch.zeros(num_layers, 1, seqlen*2,hidden_size)))\n\t        with torch.no_grad():\n\t            double = DoubleGPT(model_name, seqlen, num_layers, hidden_size).eval()\n\t            # double(input_ids, torch.tensor([3]), torch.ones(num_layers, 1, seqlen*2,hidden_size))\n\t            kv_cache_shape = (num_layers, 1, seqlen*2,hidden_size)\n\t            kv_mask = torch.ones(1, seqlen, hidden_size)\n\t            # Trace just the cached model.\n\t            traced_cached_model = torch.jit.trace(double, (input_ids, torch.tensor([3]), torch.ones(kv_cache_shape), kv_mask))\n", "            # Trace just the full model.\n\t            traced_full_model = torch.jit.trace(double, (input_ids, torch.tensor([3]), torch.zeros(kv_cache_shape), kv_mask))\n\t            # Script both models.\n\t            # traced_model = torch.jit.script(double)\n\t        def convert_model(traced_model, name):\n\t            print(traced_model.code)\n\t            prog = ct.convert(traced_model,\n\t                inputs=[\n\t                    ct.TensorType(name=\"input_ids\", shape=[1, seqlen], dtype=np.int32),\n\t                    ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n", "                    ct.TensorType(name=\"kv_cache\", shape=[num_layers, 1, seqlen*2, hidden_size], dtype=np.float32),\n\t                    ct.TensorType(name=\"kv_mask\", shape=[1, seqlen, hidden_size], dtype=np.int32),\n\t                ],\n\t                outputs=[\n\t                    ct.TensorType(name=\"logits\", dtype=np.float32),\n\t                    ct.TensorType(name=\"new_kv_cache\", dtype=np.float32),\n\t                ],\n\t                minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n\t                # compute_units=ct.ComputeUnit.CPU_AND_GPU if \"full\" in name else ct.ComputeUnit.CPU_AND_NE,\n\t                compute_precision=ct.precision.FLOAT32,\n", "                convert_to=\"milinternal\")\n\t            # print(prog)\n\t            mlmodel = ct.convert(prog,\n\t                                minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n\t                                convert_to=\"mlprogram\")\n\t            # print(mlmodel.get_spec().description.input)\n\t            # spec = mlmodel.get_spec()\n\t            # spec.description.input[2].type.isOptional = True\n\t            # mlmodel._spec = spec\n\t            # print(mlmodel.get_spec().description.input)\n", "            # mlmodel.save(f\"{name}.mlpackage\")\n\t            return mlmodel\n\t        cached_mlmodel = convert_model(traced_cached_model, \"gpt2kv-cached\")\n\t        full_mlmodel = convert_model(traced_full_model, \"gpt2kv-full\")\n\t        # convert_model(traced_model, \"gpt2kv\")\n\t        from transformers import AutoTokenizer\n\t        tok = AutoTokenizer.from_pretrained(\"gpt2\")\n\t        seq = tok(\"would the real slim\", return_tensors=\"pt\")[\"input_ids\"]\n\t        input_ids = torch.cat([\n\t            seq.squeeze(),\n", "            torch.full((seqlen - seq.shape[-1],), tok.eos_token_id)\n\t        ]).unsqueeze(0)\n\t        original_inputs = {\n\t            \"input_ids\": input_ids.int().numpy(),\n\t            \"output_mask\": torch.tensor([seq.shape[1]-1]).int().numpy(),\n\t            \"kv_cache\": torch.zeros((num_layers, 1, 2*seqlen, hidden_size)).float().numpy(),\n\t        }\n\t        original_inputs[\"kv_mask\"] = build_kv_mask(original_inputs[\"output_mask\"], seqlen=512, hidden_size=hidden_size)\n\t        original_outputs = full_mlmodel.predict(original_inputs)\n\t        # input(\"Press Enter to continue.\")\n", "        from stopwatch import Stopwatch\n\t        stopwatch = Stopwatch(3)\n\t        stopwatch.stop()\n\t        stopwatch.reset()\n\t        input_stopwatch = Stopwatch(3)\n\t        input_stopwatch.stop()\n\t        input_stopwatch.reset()\n\t        # print(\"input_ids\", input_ids)\n\t        # print(\"output_mask\", original_inputs[\"output_mask\"])\n\t        outputs = original_outputs\n", "        num_inferences = 0\n\t        for i in range(min(seqlen, 200) - seq.shape[1]):\n\t            input_stopwatch.start()\n\t            input_ids[0, seq.shape[1]+i] = outputs[\"logits\"].argmax()\n\t            inputs = {\n\t                \"input_ids\": input_ids.int().numpy(),\n\t                \"output_mask\": torch.tensor([seq.shape[1]+i]).int().numpy(),\n\t                \"kv_cache\": outputs[\"new_kv_cache\"], # already numpy floats (from CoreML)\n\t            }\n\t            inputs[\"kv_mask\"] = build_kv_mask(inputs[\"output_mask\"], seqlen=512, hidden_size=hidden_size) # probably cheating to not include this in timing...\n", "            input_stopwatch.stop()\n\t            # print(\"input_ids\", input_ids)\n\t            # print(\"output_mask\", inputs[\"output_mask\"])\n\t            stopwatch.start()\n\t            outputs = cached_mlmodel.predict(inputs)\n\t            # outputs = full_mlmodel.predict(inputs)\n\t            stopwatch.stop()\n\t            num_inferences += 1\n\t        input_ids[0, seq.shape[1]+i] = outputs[\"logits\"].argmax()\n\t        print(\"final input_ids\", input_ids)\n", "        print(tok.decode(input_ids[0, :]))\n\t        print(\"\\n\\n---stats---\")\n\t        per_inference = \"{:.{}f}ms\".format((stopwatch.duration / num_inferences) * 1000, 2)\n\t        print(stopwatch, \"total\")\n\t        print(f\"{per_inference}/it\")\n\t        input_per_inference = \"{:.{}f}ms\".format((input_stopwatch.duration / num_inferences) * 1000, 2)\n\t        print(f\"input prep {input_per_inference}/it\")\n\t        # print(prog)\n"]}
{"filename": "models/pythia.py", "chunked_list": ["\"\"\"\n\tnanoGPT + huggingface inspired-implementation of the pythia models from EleutherAI.\n\tThis includes neox.\n\tPlease read the 'Out-of-scope use' section on the Pythia huggingface pages. Notably:\n\t\"The Pythia Suite is not intended for deployment. It is not a in itself a product and\n\tcannot be used for human-facing interactions.\"\n\tHybrid of HuggingFace implementation and nanoGPT.\n\tHuggingface License:\n\tCopyright 2022 EleutherAI The HuggingFace Inc. team. All rights reserved.\n\tLicensed under the Apache License, Version 2.0 (the \"License\");\n", "you may not use this file except in compliance with the License.\n\tYou may obtain a copy of the License at\n\t    http://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS,\n\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\tSee the License for the specific language governing permissions and\n\tlimitations under the License.\n\tnanoGPT License:\n\tMIT License\n", "Copyright (c) 2022 Andrej Karpathy\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n\tof this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n", "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\"\"\"\n\timport math\n\timport inspect\n\tfrom dataclasses import dataclass\n", "from collections import OrderedDict\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn import functional as F\n\tclass LayerNorm(nn.Module):\n\t    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\t    def __init__(self, ndim, bias):\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(ndim))\n\t        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n", "    def forward(self, input):\n\t        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\tclass CausalSelfAttention(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.hidden_size % config.n_head == 0\n\t        self.n_head = config.n_head\n\t        self.hidden_size = config.hidden_size\n\t        self.head_size = self.hidden_size // self.n_head\n\t        self.rotary_ndims = int(self.head_size * config.rotary_pct)\n", "        self.rotary_emb = RotaryEmbedding(\n\t            self.rotary_ndims, config.max_position_embeddings, base=config.rotary_emb_base\n\t        )\n\t        self.norm_factor = torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32)).to(torch.get_default_dtype())\n\t        self.query_key_value = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n\t        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n\t    @classmethod\n\t    def _split_heads(cls, tensor, num_attention_heads, attn_head_size):\n\t        \"\"\"\n\t        Splits hidden dim into attn_head_size and num_attention_heads\n", "        \"\"\"\n\t        # tensor: [bs, seq_len, hidden_size]\n\t        new_shape = tensor.size()[:-1] + (num_attention_heads, attn_head_size)\n\t        # -> [bs, seq_len, num_attention_heads, attn_head_size]\n\t        tensor = tensor.view(new_shape)\n\t        # -> [bs, num_attention_heads, seq_len, attn_head_size]\n\t        tensor = tensor.permute(0, 2, 1, 3)\n\t        return tensor\n\t    @classmethod\n\t    def _merge_heads(cls, tensor, num_attention_heads, attn_head_size):\n", "        \"\"\"\n\t        Merges attn_head_size dim and num_attn_heads dim into hidden dim\n\t        \"\"\"\n\t        # tensor [bs, num_attention_heads, seq_len, attn_head_size]\n\t        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n\t        # -> [bs, seq_len, num_attention_heads, attn_head_size]\n\t        tensor = tensor.view(tensor.size(0), tensor.size(1), num_attention_heads * attn_head_size)\n\t        # -> [bs, seq_len, hidden_size]\n\t        return tensor\n\t    def forward(self, x, position_ids, attention_mask):\n", "        # Compute QKV\n\t        # Attention heads [batch, seq_len, hidden_size]\n\t        #   --> [batch, seq_len, (np * 3 * head_size)]\n\t        qkv = self.query_key_value(x)\n\t        # assert qkv.shape == torch.Size([1, 11, 512*3]), qkv.shape\n\t        # [batch, seq_len, (num_heads * 3 * head_size)]\n\t        #   --> [batch, seq_len, num_heads, 3 * head_size]\n\t        new_qkv_shape = qkv.size()[:-1] + (self.n_head, 3 * self.head_size)\n\t        qkv = qkv.view(*new_qkv_shape)\n\t        # assert qkv.shape == torch.Size([1, 11, 8, (512//8)*3]), qkv.shape\n", "        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n\t        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n\t        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n\t        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n\t        # Compute rotary embeddings on rotary_ndims\n\t        query_rot = query[..., : self.rotary_ndims]\n\t        query_pass = query[..., self.rotary_ndims :]\n\t        key_rot = key[..., : self.rotary_ndims]\n\t        key_pass = key[..., self.rotary_ndims :]\n\t        # Compute token offset for rotary embeddings (when decoding)\n", "        seq_len = key.shape[-2]\n\t        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n\t        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\t        query = torch.cat((query, query_pass), dim=-1)\n\t        key = torch.cat((key, key_pass), dim=-1)\n\t        # Compute attention\n\t        attn_output, _ = self._attn(query, key, value, attention_mask)\n\t        # Reshape outputs\n\t        attn_output = self._merge_heads(attn_output, self.n_head, self.head_size)\n\t        attn_output = self.dense(attn_output)\n", "        return attn_output\n\t    def _attn(self, query, key, value, attention_mask=None):\n\t        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n\t        # compute causal mask from causal mask buffer\n\t        batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n\t        key_length = key.size(-2)\n\t        # causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n\t        query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)\n\t        key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)\n\t        attn_scores = torch.zeros(\n", "            batch_size * num_attention_heads,\n\t            query_length,\n\t            key_length,\n\t            dtype=query.dtype,\n\t            device=key.device,\n\t        )\n\t        attn_scores = torch.baddbmm(\n\t            attn_scores,\n\t            query,\n\t            key.transpose(1, 2),\n", "            beta=1.0,\n\t            alpha=(torch.tensor(1.0, dtype=self.norm_factor.dtype, device=self.norm_factor.device) / self.norm_factor),\n\t        )\n\t        attn_scores = attn_scores.view(batch_size, num_attention_heads, query_length, key_length)\n\t        if attention_mask is not None:\n\t            # Apply the attention mask\n\t            attn_scores = attn_scores + attention_mask\n\t        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n\t        attn_weights = attn_weights.to(value.dtype)\n\t        attn_output = torch.matmul(attn_weights, value)\n", "        return attn_output, attn_weights\n\tdef rotate_half(x):\n\t    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n\t    x1 = x[..., : x.shape[-1] // 2]\n\t    x2 = x[..., x.shape[-1] // 2 :]\n\t    return torch.cat((-x2, x1), dim=-1)\n\tdef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n\t    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n\t    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n\t    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n", "    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n\t    q_embed = (q * cos) + (rotate_half(q) * sin)\n\t    k_embed = (k * cos) + (rotate_half(k) * sin)\n\t    return q_embed, k_embed\n\tclass RotaryEmbedding(nn.Module):\n\t    def __init__(self, dim, max_position_embeddings, base=10000, device=None):\n\t        super().__init__()\n\t        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n\t        self.register_buffer(\"inv_freq\", inv_freq)\n\t        # Build here to make `torch.jit.trace` work.\n", "        self.max_seq_len_cached = max_position_embeddings\n\t        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n\t        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n\t        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n\t        emb = torch.cat((freqs, freqs), dim=-1)\n\t        self.cos_cached = emb.cos()[None, None, :, :]\n\t        self.sin_cached = emb.sin()[None, None, :, :]\n\t    def forward(self, x, seq_len=None):\n\t        # x: [bs, n_head, seq_len, head_size]\n\t        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n", "        if seq_len > self.max_seq_len_cached:\n\t            self.max_seq_len_cached = seq_len\n\t            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n\t            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n\t            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n\t            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n\t            self.cos_cached = emb.cos()[None, None, :, :]\n\t            self.sin_cached = emb.sin()[None, None, :, :]\n\t        return self.cos_cached[:seq_len, ...].to(x.device), self.sin_cached[:seq_len, ...].to(x.device)\n\tclass MLP(nn.Module):\n", "    def __init__(self, config):\n\t        super().__init__()\n\t        self.dense_h_to_4h = nn.Linear(config.hidden_size, config.intermediate_size)\n\t        self.dense_4h_to_h = nn.Linear(config.intermediate_size, config.hidden_size)\n\t        self.act = nn.GELU() # pretty sure this is the default: approximate=False\n\t    def forward(self, x):\n\t        x = self.dense_h_to_4h(x)\n\t        x = self.act(x)\n\t        x = self.dense_4h_to_h(x)\n\t        return x\n", "class Block(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\t        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\t        self.attention = CausalSelfAttention(config)\n\t        self.mlp = MLP(config)\n\t    def forward(self, x, position_ids, attention_mask=None):\n\t        # Computed in parallel.\n\t        attn_out = self.attention(self.input_layernorm(x), position_ids, attention_mask)\n", "        mlp_output = self.mlp(self.post_attention_layernorm(x))\n\t        x = mlp_output + attn_out + x # the order matters...\n\t        return x\n\t@dataclass\n\tclass GPTConfig:\n\t    block_size: int = 1024\n\t    vocab_size: int = 50304 # Almost all pythia models have the same vocab size.\n\t    n_layer: int = 12\n\t    n_head: int = 12\n\t    hidden_size: int = 512\n", "    intermediate_size: int = 2048\n\t    max_position_embeddings: int = 2048\n\t    dropout: float = 0.0\n\t    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\t    layer_norm_eps: float = 1e-05\n\t    rotary_pct: float = 0.25\n\t    rotary_emb_base: int = 10000\n\tclass GPT(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n", "        assert config.vocab_size is not None\n\t        assert config.block_size is not None\n\t        self.config = config\n\t        self.embed_in = nn.Embedding(config.vocab_size, config.hidden_size)\n\t        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n\t        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\t        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\t        # report number of parameters\n\t        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\t    def get_num_params(self, non_embedding=True):\n", "        \"\"\"\n\t        Return the number of parameters in the model.\n\t        For non-embedding count (default), the position embeddings get subtracted.\n\t        The token embeddings would too, except due to the parameter sharing these\n\t        params are actually used as weights in the final layer, so we include them.\n\t        \"\"\"\n\t        n_params = sum(p.numel() for p in self.parameters())\n\t        return n_params\n\t    def _init_weights(self, module):\n\t        if isinstance(module, nn.Linear):\n", "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t            if module.bias is not None:\n\t                torch.nn.init.zeros_(module.bias)\n\t        elif isinstance(module, nn.Embedding):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t    def forward(self, input_ids, output_mask=None):\n\t        device = input_ids.device\n\t        b, t = input_ids.size()\n\t        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        position_ids = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n", "        # ANE: Since we are only inferring and we only care about predicting the next token,\n\t        # we can use the same triangular mask always. May need to change this to support flexible sizes.\n\t        attention_mask = (1 - torch.tril(torch.ones((1,1,t,t), dtype=torch.float16))) * -1e4\n\t        inputs_embeds = self.embed_in(input_ids)\n\t        x = inputs_embeds\n\t        # forward the GPT model itself\n\t        for l in self.layers:\n\t            x = l(x, position_ids, attention_mask)\n\t        # No need to compute anything else for the other length-1 tokens.\n\t        # This shaves ~30% off of gpt2-xl when running on CPU+ANE.\n", "        if output_mask is not None:\n\t            x = torch.index_select(x, 1, output_mask)\n\t        x = self.final_layer_norm(x)\n\t        logits = self.lm_head(x)\n\t        return logits\n\t    @staticmethod\n\t    def config_args():\n\t        return OrderedDict({\n\t            'pythia-70m':      dict(n_layer=6,  n_head=8,  hidden_size=512,  intermediate_size=2048),\n\t            'pythia-160m':     dict(n_layer=12, n_head=12, hidden_size=768,  intermediate_size=3072),\n", "            'pythia-410m':     dict(n_layer=24, n_head=16, hidden_size=1024, intermediate_size=4096),\n\t            'pythia-1b':       dict(n_layer=16, n_head=8,  hidden_size=2048, intermediate_size=8192),\n\t            'pythia-1.4b':     dict(n_layer=24, n_head=16, hidden_size=2048, intermediate_size=8192),\n\t            'pythia-2.8b':     dict(n_layer=32, n_head=32, hidden_size=2560, intermediate_size=10240),\n\t            'pythia-6.9b':     dict(n_layer=32, n_head=32, hidden_size=4096, intermediate_size=16384, vocab_size=50432),\n\t        })\n\t    @staticmethod\n\t    def model_names():\n\t        return list(GPT.config_args().keys())\n\t    @staticmethod\n", "    def tokenizer_by_name():\n\t        return {n:f\"EleutherAI/{n}\" for n in GPT.model_names()}\n\t    @classmethod\n\t    def from_pretrained(cls, model_type, override_args=None):\n\t        model_type = model_type.replace('EleutherAI/', '')\n\t        assert model_type in GPT.model_names()\n\t        model_type = 'EleutherAI/' + model_type\n\t        override_args = override_args or {} # default to empty dict\n\t        # only dropout can be overridden see more notes below\n\t        assert all(k == 'dropout' for k in override_args)\n", "        from transformers import GPTNeoXForCausalLM\n\t        print(\"loading weights from pretrained model: %s\" % model_type)\n\t        # n_layer, n_head and hidden_size are determined from model_type\n\t        config_args = GPT.config_args()[model_type.replace('EleutherAI/', '')]\n\t        # create a from-scratch initialized model\n\t        config = GPTConfig(**config_args)\n\t        model = GPT(config)\n\t        sd = model.state_dict()\n\t        sd_keys = sd.keys()\n\t        sd_keys = [k for k in sd_keys]\n", "        # init a huggingface/transformers model\n\t        model_hf = GPTNeoXForCausalLM.from_pretrained(model_type)\n\t        sd_hf = model_hf.state_dict()\n\t        for k in list(sd_hf.keys()):\n\t            if k.startswith(\"gpt_neox.\"):\n\t                newk = k.replace(\"gpt_neox.\", \"\")\n\t                assert newk not in sd_hf\n\t                sd_hf[newk] = sd_hf.pop(k)\n\t        for k in list(sd_hf.keys()):\n\t            if k.startswith(\"embed_out.\"):\n", "                newk = k.replace(\"embed_out.\", \"lm_head.\")\n\t                sd_hf[newk] = sd_hf.pop(k)\n\t            elif k.endswith(\".masked_bias\"):\n\t                del sd_hf[k]\n\t            elif k.endswith(\".attention.bias\"): #FIXME what is this\n\t                del sd_hf[k]\n\t        in_hf = set(sd_hf.keys()) - set(sd_keys)\n\t        missing_hf = set(sd_keys) - set(sd_hf.keys())\n\t        if len(in_hf) != 0 or len(missing_hf) != 0:\n\t            print([x for x in in_hf if \"\" in x])\n", "            print([x for x in missing_hf if \"\" in x])\n\t        # copy while ensuring all of the parameters are aligned and match in names and shapes\n\t        sd_keys_hf = sd_hf.keys()\n\t        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n\t        for k in sd_keys_hf:\n\t            assert sd_hf[k].shape == sd[k].shape, f\"{k}: {sd_hf[k].shape} != {sd[k].shape}\"\n\t            with torch.no_grad():\n\t                sd[k].copy_(sd_hf[k])\n\t        return model\n\tif __name__ == \"__main__\":\n", "    import argparse\n\t    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\t    from src.utils.psnr import compute_psnr\n\t    parser = argparse.ArgumentParser(description='Convert a model to CoreML.')\n\t    parser.add_argument('--model_name', choices=GPT.model_names(), default=\"pythia-160m\", type=str)\n\t    args = parser.parse_args()\n\t    model_name = 'EleutherAI/' + args.model_name\n\t    nano = GPT.from_pretrained(model_name).eval()\n\t    hf = GPTNeoXForCausalLM.from_pretrained(model_name, use_cache=False).eval()\n\t    tok = AutoTokenizer.from_pretrained(model_name)\n", "    inputs = tok(\"this washed coffee comes from huila, colombia\", return_tensors=\"pt\")\n\t    with torch.no_grad():\n\t        # print(inputs)\n\t        # inputs = torch.rand((1,1,512), requires_grad=False)\n\t        # position_ids = torch.arange(0, inputs.shape[1], dtype=torch.long).unsqueeze(0) # shape (1, t)\n\t        # attention_mask = (1 - torch.tril(torch.ones((1,1,inputs.shape[1],inputs.shape[1]), dtype=torch.float16))) * -1e4\n\t        # attention_mask = (1 - torch.tril(torch.ones((1,1,inputs.shape[1],inputs.shape[1]), dtype=torch.float32))) * -1e9\n\t        # hf_out_a = hf.gpt_neox.layers[0].attention(hf.gpt_neox.layers[0].input_layernorm(inputs), None)[0]\n\t        # nano_out_a = nano.layers[0].attention(nano.layers[0].input_layernorm(inputs), position_ids, attention_mask)\n\t        # assert torch.equal(hf_out_a, nano_out_a), \"zzz\"\n", "        # hf_out_mlp = hf.gpt_neox.layers[0].mlp(hf.gpt_neox.layers[0].post_attention_layernorm(inputs))\n\t        # nano_out_mlp = nano.layers[0].mlp(nano.layers[0].post_attention_layernorm(inputs))\n\t        # assert torch.equal(hf_out_mlp, nano_out_mlp), \"lkjlkjlkj\"\n\t        # hf_out = hf_out_mlp + hf_out_a + inputs\n\t        # nano_out = nano_out_mlp + nano_out_a + inputs\n\t        # assert torch.equal(hf_out, nano_out), \"wqwwqwqwq\"\n\t        # hf_out_l = hf.gpt_neox.layers[0](inputs, None)[0]\n\t        # nano_out_l = nano.layers[0](inputs, position_ids, attention_mask)\n\t        # wtf = nano_out_a + nano_out_mlp + inputs\n\t        # print(wtf - nano_out_l)\n", "        # assert torch.equal(hf_out_l, hf_out)\n\t        # assert torch.equal(wtf, nano_out_l)\n\t        # assert torch.equal(hf_out_l, nano_out_l)\n\t        inputs = {k:v for k,v in inputs.items() if k in [\"input_ids\"]}\n\t        hf_out = hf(**inputs)['logits']\n\t        nano_out = nano(**inputs)\n\t        # nano = nano.to(device=\"mps\",dtype=torch.float16)\n\t        # inputs = {k: v.to(device=\"mps\") for k, v in inputs.items()}\n\t        # nano_out = nano(**inputs).cpu().float()\n\t    assert hf_out.shape == nano_out.shape, f\"{hf_out.shape} != {nano_out.shape}\"\n", "    # psnr should be ~240 if perfect.\n\t    print(\"psnr:\", compute_psnr(hf_out, nano_out))\n\t    print(\"eq\", torch.equal(hf_out, nano_out))\n"]}
{"filename": "src/__init__.py", "chunked_list": []}
{"filename": "src/utils/psnr.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef compute_psnr(a, b):\n\t    \"\"\"\n\t    Compute Peak-Signal-to-Noise-Ratio across two numpy.ndarray objects\n\t    a: the approximation\n\t    b: the ground truth\n\t    From ml-ane-transformers.\n\t    Original License:\n\t    Copyright (C) 2022 Apple Inc. All Rights Reserved.\n", "    IMPORTANT: This Apple software is supplied to you by Apple Inc. (\"Apple\") in consideration of your agreement to the following terms, and your use, installation, modification or redistribution of this Apple software constitutes acceptance of these terms. If you do not agree with these terms, please do not use, install, modify or redistribute this Apple software.\n\t    In consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple's copyrights in this original Apple software (the \"Apple Software\"), to use, reproduce, modify and redistribute the Apple Software, with or without modifications, in source and/or binary forms; provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly stated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated.\n\t    The Apple Software is provided by Apple on an \"AS IS\" basis. APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\t    IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\t    \"\"\"\n\t    if torch.is_tensor(a):\n\t        a = a.numpy()\n\t    if torch.is_tensor(b):\n\t        b = b.numpy()\n\t    max_b = np.abs(b).max()\n", "    sumdeltasq = 0.0\n\t    sumdeltasq = ((a - b) * (a - b)).sum()\n\t    sumdeltasq /= b.size\n\t    sumdeltasq = np.sqrt(sumdeltasq)\n\t    eps = 1e-5\n\t    eps2 = 1e-10\n\t    psnr = 20 * np.log10((max_b + eps) / (sumdeltasq + eps2))\n\t    return psnr"]}
{"filename": "src/utils/model_proxy.py", "chunked_list": ["import torch\n\timport numpy as np\n\tclass MLModelProxy:\n\t    \"\"\"Just the fun bits from coremltools that allows us to use pre-compiled models.\"\"\"\n\t    def __init__(self, model_path, compute_unit):\n\t        from coremltools.libcoremlpython import _MLModelProxy\n\t        self.__proxy__ = _MLModelProxy(model_path, compute_unit.name)\n\t    def _update_float16_multiarray_input_to_float32(self, input_data):\n\t        for k, v in input_data.items():\n\t            if isinstance(v, np.ndarray) and v.dtype == np.float16:\n", "                input_data[k] = v.astype(np.float32)\n\t    def _convert_tensor_to_numpy(self, input_dict):\n\t        _HAS_TORCH = True\n\t        def convert(given_input):\n\t            if isinstance(given_input, np.ndarray):\n\t                sanitized_input = given_input\n\t            elif _HAS_TORCH and isinstance(given_input, torch.Tensor):\n\t                sanitized_input = given_input.detach().numpy()\n\t            # elif (_HAS_TF_1 or _HAS_TF_2) and isinstance(given_input, _tf.Tensor):\n\t            #     sanitized_input = given_input.eval(session=_tf.compat.v1.Session())\n", "            else:\n\t                sanitized_input = np.array(given_input)\n\t            return sanitized_input\n\t        model_input_to_types = {}\n\t        # Don't pass bad input!\n\t        # TODO: Steal a spec out of the mlpackage.\n\t        # for inp in self._spec.description.input:\n\t        #     type_value = inp.type.multiArrayType.dataType\n\t        #     type_name = inp.type.multiArrayType.ArrayDataType.Name(type_value)\n\t        #     if type_name != \"INVALID_ARRAY_DATA_TYPE\":\n", "        #         model_input_to_types[inp.name] = type_name\n\t        for given_input_name, given_input in input_dict.items():\n\t            # if not given_input_name in model_input_to_types:\n\t            #     continue\n\t            input_dict[given_input_name] = convert(given_input)\n\t    def predict(self, data):\n\t        \"\"\"\n\t        You are responsible for not passing bad input! If you're not sure and the\n\t        model seems to be hanging, try running with a mlpackage\n\t        \"\"\"\n", "        # self._verify_input_dict(data)\n\t        self._convert_tensor_to_numpy(data)\n\t        self._update_float16_multiarray_input_to_float32(data)\n\t        return self.__proxy__.predict(data)\n"]}
{"filename": "src/utils/__init__.py", "chunked_list": []}
{"filename": "src/utils/trace_warnings.py", "chunked_list": ["import warnings\n\tfrom contextlib import contextmanager\n\tdef _gpt2_warning_filter():\n\t    patterns = [\n\t        # TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\t        # assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        # if this fails the trace will fail\n\t        # TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\t        # assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        # TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n", "        # if output_mask is not None and t > 1:\n\t        (r\".*Converting a tensor to a Python boolean might cause the trace to be incorrect.*\", [275, 280, 311]),\n\t        # UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n\t        #   k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        # UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n\t        #   q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        # UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n\t        #   v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        # C and n_head will never be negative\n\t        (r\".*__floordiv__ is deprecated.*\", [155, 156, 157]),\n", "        # TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\t        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t        # k will always be the same size\n\t        (r\".*Converting a tensor to a Python float.*\", [165])\n\t    ]\n\t    for pattern, linenos in patterns:\n\t        for lineno in linenos:\n\t            warnings.filterwarnings(\"ignore\", pattern, lineno=lineno)\n\t@contextmanager\n\tdef silence_known_trace_warnings(model_name: str):\n", "    \"\"\"\n\t    Silence known and safe to ignore warnings.\n\t    \"\"\"\n\t    with warnings.catch_warnings():\n\t        if model_name in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]:\n\t            _gpt2_warning_filter()\n\t        yield\n"]}
{"filename": "src/ml_ane_transformers/layerwise_comparison.py", "chunked_list": ["from src.ml_ane_transformers.vanilla_gpt2 import GPT as Vanilla\n\tfrom src.ml_ane_transformers.ane_gpt2 import GPT as ANE\n\timport torch\n\timport numpy as np\n\tfrom src.utils.psnr import compute_psnr\n\timport os\n\t\"\"\"\n\tWhat is this file?\n\tIt's a step-by-step, layer-by-layer process of debugging why the Apple Neural Engine (ANE) optimized\n\tmodel does not match the reference (aka vanilla) gpt2 model.\n", "The PSNR (peak signal-to-noise ratio) is taken from the ane transformers repo and is a way\n\tto measure how well the two models match. It seems like 150-200 is good (since these should be mathematically equivalent\n\t-- e.g. in the Apple repo 60 is the minimum bar they set for the lossy conversion to CoreML).\n\tDespite some horrific bugs I never got lower than 20 or so. Lower than 60 and something is likely wrong.\n\tRun this file, look at the PSNR logs and if any are <<150, start debugging that layer.\n\t\"\"\"\n\tane = ANE.from_pretrained(\"gpt2\")\n\tvan = Vanilla.from_pretrained(\"gpt2\")\n\tane.eval()\n\tvan.eval()\n", "seqlen = 20\n\tidx = torch.randint(30000, (1, seqlen,))\n\tane_inputs = ane.build_inputs(idx)\n\tqk_mask, k_mask = [ane_inputs[k] for k in [\"qk_mask\", \"k_mask\"]]\n\tk_mask = None\n\tprint(\"---\\n\")\n\tdef print_stats(name: str, a, v, should_be_equal=False, end=\"\\n\"):\n\t    if torch.equal(a,v) and not should_be_equal:\n\t        print(\"BUG: most likely passed the wrong things\")\n\t    print(name, a.shape, v.shape, \"psnr\", compute_psnr(a, v), end)\n", "def bc1s_to_bsc(x):\n\t    assert len(x.shape) == 4\n\t    assert x.shape[0] == 1 # batch is 1\n\t    assert x.shape[2] == 1 # third dim is always 1\n\t    return x.permute(0, 3, 1, 2).squeeze(-1)\n\twith torch.no_grad():\n\t    ane_embd = ane.transformer.wte(idx)\n\t    van_embd = van.transformer.wte(idx)\n\t    print(\"wte\", ane_embd.shape, van_embd.shape, torch.equal(ane_embd, van_embd))\n\t    ane_preb = ane.prepare_for_block(idx)\n", "    van_preb = van.prepare_for_block(idx)\n\t    print_stats(\"pre block\", bc1s_to_bsc(ane_preb), van_preb)\n\t    ane_b1 = ane.transformer.h[0](ane.prepare_for_block(idx), qk_mask=qk_mask, k_mask=k_mask)\n\t    van_b1 = van.transformer.h[0](van.prepare_for_block(idx))\n\t    print_stats(\"layer 1\", bc1s_to_bsc(ane_b1), van_b1)\n\t    ane_b1_ln_1 = ane.transformer.h[0].ln_1(ane.prepare_for_block(idx))\n\t    van_b1_ln_1 = van.transformer.h[0].ln_1(van.prepare_for_block(idx))\n\t    print_stats(\"layer 1 ln1\", bc1s_to_bsc(ane_b1_ln_1), van_b1_ln_1,)\n\t    ane_b1_attn = ane.transformer.h[0].attn(ane_b1_ln_1, qk_mask=qk_mask, k_mask=k_mask)\n\t    van_b1_attn = van.transformer.h[0].attn(van_b1_ln_1)\n", "    print_stats(\"layer 1 attn\", bc1s_to_bsc(ane_b1_attn), van_b1_attn)\n\t    ane_b1_attn_qproj = ane.transformer.h[0].attn.q_proj(ane_b1_ln_1)\n\t    van_b1_attn_qproj = van.transformer.h[0].attn.q_proj(van_b1_ln_1)\n\t    print_stats(\"layer 1 qproj\", bc1s_to_bsc(ane_b1_attn_qproj), van_b1_attn_qproj)\n\t    ane_b1_attn_kproj = ane.transformer.h[0].attn.k_proj(ane_b1_ln_1)\n\t    van_b1_attn_kproj = van.transformer.h[0].attn.k_proj(van_b1_ln_1)\n\t    print_stats(\"layer 1 kproj\", bc1s_to_bsc(ane_b1_attn_kproj), van_b1_attn_kproj)\n\t    ane_b1_attn_vproj = ane.transformer.h[0].attn.v_proj(ane_b1_ln_1)\n\t    van_b1_attn_vproj = van.transformer.h[0].attn.v_proj(van_b1_ln_1)\n\t    print_stats(\"layer 1 vproj\", bc1s_to_bsc(ane_b1_attn_vproj), van_b1_attn_vproj)\n", "    ane_b1_attn_atnn = ane.transformer.h[0].attn._attention_fn(ane_b1_attn_qproj, ane_b1_attn_kproj, ane_b1_attn_vproj, qk_mask, k_mask, False)[0].contiguous().view(1, 768, 1, seqlen)\n\t    van_b1_attn_atnn = van.transformer.h[0].attn._attention_fn(van_b1_ln_1, van_b1_attn_qproj, van_b1_attn_kproj, van_b1_attn_vproj)\n\t    print_stats(\"layer 1 inner attn\", bc1s_to_bsc(ane_b1_attn_atnn), van_b1_attn_atnn)\n\t    # Not sure I fully trust this due to the change to add cat in ane + the permute.\n\t    ane_b1_attn_qxk = ane.transformer.h[0].attn._qk(ane_b1_attn_qproj, ane_b1_attn_kproj, ane_b1_attn_vproj)\n\t    van_b1_attn_qxk = van.transformer.h[0].attn._qk(van_b1_ln_1, van_b1_attn_qproj, van_b1_attn_kproj, van_b1_attn_vproj)\n\t    assert len(ane_b1_attn_qxk) == len(van_b1_attn_qxk)\n\t    print_stats(\"layer 1 q@k\", ane_b1_attn_qxk.permute(0, 2, 3, 1), van_b1_attn_qxk)\n\t    ane_b1_attn_qxk_sm = ane.transformer.h[0].attn._qk_softmax(ane_b1_attn_qproj, ane_b1_attn_kproj, ane_b1_attn_vproj, qk_mask=qk_mask, k_mask=k_mask)\n\t    van_b1_attn_qxk_sm = van.transformer.h[0].attn._qk_softmax(van_b1_ln_1, van_b1_attn_qproj, van_b1_attn_kproj, van_b1_attn_vproj)\n", "    assert len(ane_b1_attn_qxk_sm) == len(van_b1_attn_qxk_sm)\n\t    print_stats(\"layer 1 q@k softmax\", ane_b1_attn_qxk_sm.permute(0, 2, 3, 1), van_b1_attn_qxk_sm)\n\t    # Bubble back up to middle of block.\n\t    ane_b1_x_post_attn = ane_b1_ln_1 + ane_b1_attn\n\t    van_b1_x_post_attn = van_b1_ln_1 + van_b1_attn\n\t    print_stats(\"layer 1 x + attn\", ane_b1_attn_qxk_sm.permute(0, 2, 3, 1), van_b1_attn_qxk_sm)\n\t    ane_b1_ln2 = ane.transformer.h[0].ln_2(ane_b1_x_post_attn)\n\t    van_b1_ln2 = van.transformer.h[0].ln_2(van_b1_x_post_attn)\n\t    print_stats(\"layer 1 ln2\", bc1s_to_bsc(ane_b1_ln2), van_b1_ln2)\n\t    ane_b1_mlp = ane.transformer.h[0].mlp(ane_b1_ln2)\n", "    van_b1_mlp = van.transformer.h[0].mlp(van_b1_ln2)\n\t    print_stats(\"layer 1 mlp\", bc1s_to_bsc(ane_b1_mlp), van_b1_mlp)\n\t    ane_all_blocks = ane.prepare_for_block(idx)\n\t    van_all_blocks = van.prepare_for_block(idx)\n\t    for nh, (ab, vb) in enumerate(zip(ane.transformer.h, van.transformer.h)):\n\t        ane_all_blocks = ab(ane_all_blocks, qk_mask=qk_mask, k_mask=k_mask)\n\t        van_all_blocks = vb(van_all_blocks)\n\t        print_stats(f\"layer {nh}\", bc1s_to_bsc(ane_all_blocks), van_all_blocks, end=\"\")\n\t    ane_lm = ane.lm_head(ane_all_blocks.permute(3, 1, 0, 2)).squeeze().unsqueeze(0)\n\t    van_lm = van.lm_head(van_all_blocks)\n", "    print_stats(\"lm\", ane_lm, van_lm)\n\t# E2E - with k_mask to make sure that works.\n\tane_inputs = ane.build_inputs(idx, pad_to_length=40, pad_token_id=350)\n\tane_idx = ane_inputs[\"input_ids\"].to(\"mps\")\n\tqk_mask, k_mask, output_mask = [ane_inputs[k] for k in [\"qk_mask\", \"k_mask\", \"output_mask\"]]\n\tos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n\twith torch.no_grad():\n\t    ane_out = ane.to(\"mps\", torch.half)(ane_idx, output_mask=output_mask.to(ane_idx.device), qk_mask=None, k_mask=None)[:, [-1], :].float().cpu()\n\t    van_out = van(idx)[0]\n\tassert ane_out.shape == van_out.shape, f\"{ane_out.shape} != {van_out.shape}\"\n", "print(\"shapes:\", van_out.shape, ane_out.shape)\n\tprint(\"softmax argmax:\", torch.argmax(van_out.softmax(2)), torch.argmax(ane_out.softmax(2)))\n\tprint(\"e2e psnr\", compute_psnr(ane_out.softmax(2).numpy(), van_out.softmax(2).numpy()))\n\t# sim = torch.nn.functional.cosine_similarity(van_out[0], ane_out[0])\n\t# print(\"similarity\", sim, torch.mean(sim)) # Turns out this is mostly useless, it's quite easy to get upper 0.9s."]}
{"filename": "src/ml_ane_transformers/trace_psnr.py", "chunked_list": ["import torch\n\tfrom ane_gpt2 import GPT as ANEGPT\n\timport numpy as np\n\timport coremltools as ct\n\tfrom psnr import compute_psnr\n\t\"\"\"\n\tCompare the PSNR for a saved mlpackage with a model\n\tand its traced version. Useful for ruling out tracing\n\tas the source of error.\n\t\"\"\"\n", "token_predictor = ANEGPT.from_pretrained(\"gpt2\").eval()\n\trandom_tokens = torch.randint(10000, (1,10,))\n\tinputs_dict = token_predictor.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n\tprint(token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"]).shape)\n\tprint(f\"Tracing the model with {inputs_dict['input_ids']}\")\n\ttraced_token_predictor = torch.jit.trace(token_predictor, (inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"]))\n\tprint(\"Traced, diffing...\")\n\trandom_tokens = torch.randint(10000, (1,10,))\n\tinputs_dict = ANEGPT.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n\twith torch.no_grad():\n", "    og_out = token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"])\n\t    tr_out = traced_token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"])\n\tassert og_out.shape == tr_out.shape\n\tpsnr = compute_psnr(og_out.numpy(), tr_out.numpy())\n\tprint(\"psnr:\", psnr)\n\tprint(\"Comparing with mlpackage\")\n\tmodel = ct.models.model.MLModel(\"gpt2_2023_03_24-08_02_53_PM.mlpackage\",\n\t                                compute_units=ct.ComputeUnit.ALL)\n\tcm_out = model.predict(inputs_dict)[\"logits\"]\n\tassert tr_out.shape == cm_out.shape\n", "print(\"coreml-traced psnr:\", compute_psnr(tr_out.numpy(), cm_out))\n\tprint(\"coreml-og     psnr:\", compute_psnr(og_out.numpy(), cm_out))"]}
{"filename": "src/ml_ane_transformers/vanilla_gpt2.py", "chunked_list": ["\"\"\"\n\tAn almost-verbatim unmodified version of nanoGPT. Useful for comparing if ANE-modifications\n\tare mathematically equivalent.\n\tFull definition of a GPT Language Model, all of it in this single file.\n\tReferences:\n\t1) the official GPT-2 TensorFlow implementation released by OpenAI:\n\thttps://github.com/openai/gpt-2/blob/master/src/model.py\n\t2) huggingface/transformers PyTorch implementation:\n\thttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\tOriginal License:\n", "MIT License\n\tCopyright (c) 2022 Andrej Karpathy\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n\tof this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n", "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\"\"\"\n\timport math\n\timport inspect\n", "from dataclasses import dataclass\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn import functional as F\n\t# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n\tdef new_gelu(x):\n\t    \"\"\"\n\t    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n\t    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n\t    \"\"\"\n", "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\tclass LayerNorm(nn.Module):\n\t    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\t    def __init__(self, ndim, bias):\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(ndim))\n\t        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\t    def forward(self, input):\n\t        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\tclass CausalSelfAttention(nn.Module):\n", "    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.n_embd % config.n_head == 0\n\t        # key, query, value projections for all heads, but in a batch\n\t        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n\t        # output projection\n\t        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\t        # regularization\n\t        self.attn_dropout = nn.Dropout(config.dropout)\n\t        self.resid_dropout = nn.Dropout(config.dropout)\n", "        self.n_head = config.n_head\n\t        self.n_embd = config.n_embd\n\t        self.dropout = config.dropout\n\t        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n\t        # self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n\t        # if not self.flash:\n\t        # coremltools does not support scaled_dot_product_attention.\n\t        if True:\n\t            # print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n\t            # causal mask to ensure that attention is only applied to the left in the input sequence\n", "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n\t                                        .view(1, 1, config.block_size, config.block_size))\n\t    def q_proj(self, x):\n\t        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\t        return q\n\t    def k_proj(self, x):\n\t        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\t        return k\n\t    def v_proj(self, x):\n\t        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n", "        return v\n\t    def _qk(self, x, q, k, v):\n\t        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\t        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t        return att\n\t    def _qk_softmax(self, x, q, k, v):\n\t        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n", "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n\t        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(-1e4))\n\t        att = F.softmax(att, dim=-1)\n\t        att = self.attn_dropout(att)\n\t        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\t        return y\n", "    def _attention_fn(self, x, q, k, v):\n\t        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\t        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n\t        att = F.softmax(att, dim=-1)\n\t        att = self.attn_dropout(att)\n\t        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n", "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\t        return y\n\t    def forward(self, x):\n\t        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\t        # B - batch size\n\t        # T - sequence length (number of tokens)\n\t        # C - embedding dimensionality, n_embd, 768\n\t        # nh - number of heads\n\t        # hs - head size\n\t        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n", "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\t        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n\t        if False:# self.flash:\n\t            # efficient attention using Flash Attention CUDA kernels\n\t            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n\t        else:\n\t            # manual implementation of attention\n", "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n\t            att = F.softmax(att, dim=-1)\n\t            att = self.attn_dropout(att)\n\t            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\t        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\t        # output projection\n\t        y = self.resid_dropout(self.c_proj(y))\n\t        return y\n\tclass MLP(nn.Module):\n", "    def __init__(self, config):\n\t        super().__init__()\n\t        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n\t        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n\t        self.dropout = nn.Dropout(config.dropout)\n\t    def forward(self, x):\n\t        x = self.c_fc(x)\n\t        x = new_gelu(x)\n\t        x = self.c_proj(x)\n\t        x = self.dropout(x)\n", "        return x\n\tclass Block(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n\t        self.attn = CausalSelfAttention(config)\n\t        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n\t        self.mlp = MLP(config)\n\t    def forward(self, x):\n\t        x = x + self.attn(self.ln_1(x))\n", "        x = x + self.mlp(self.ln_2(x))\n\t        return x\n\t@dataclass\n\tclass GPTConfig:\n\t    block_size: int = 1024\n\t    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n\t    n_layer: int = 12\n\t    n_head: int = 12\n\t    n_embd: int = 768\n\t    dropout: float = 0.0\n", "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\tclass GPT(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.vocab_size is not None\n\t        assert config.block_size is not None\n\t        self.config = config\n\t        self.transformer = nn.ModuleDict(dict(\n\t            wte = nn.Embedding(config.vocab_size, config.n_embd),\n\t            wpe = nn.Embedding(config.block_size, config.n_embd),\n", "            drop = nn.Dropout(config.dropout),\n\t            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n\t            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n\t        ))\n\t        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\t        # with weight tying when using torch.compile() some warnings get generated:\n\t        # \"UserWarning: functional_call was passed multiple values for tied weights.\n\t        # This behavior is deprecated and will be an error in future versions\"\n\t        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n\t        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n", "        # init all weights\n\t        self.apply(self._init_weights)\n\t        # apply special scaled init to the residual projections, per GPT-2 paper\n\t        for pn, p in self.named_parameters():\n\t            if pn.endswith('c_proj.weight'):\n\t                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\t        # report number of parameters\n\t        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\t    def get_num_params(self, non_embedding=True):\n\t        \"\"\"\n", "        Return the number of parameters in the model.\n\t        For non-embedding count (default), the position embeddings get subtracted.\n\t        The token embeddings would too, except due to the parameter sharing these\n\t        params are actually used as weights in the final layer, so we include them.\n\t        \"\"\"\n\t        n_params = sum(p.numel() for p in self.parameters())\n\t        if non_embedding:\n\t            n_params -= self.transformer.wpe.weight.numel()\n\t        return n_params\n\t    def _init_weights(self, module):\n", "        if isinstance(module, nn.Linear):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t            if module.bias is not None:\n\t                torch.nn.init.zeros_(module.bias)\n\t        elif isinstance(module, nn.Embedding):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t    def prepare_for_block(self, idx):\n\t        device = idx.device\n\t        b, t = idx.size()\n\t        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n", "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\t        # forward the GPT model itself\n\t        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n\t        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n\t        return self.transformer.drop(tok_emb + pos_emb)\n\t    def forward(self, idx, targets=None):\n\t        device = idx.device\n\t        b, t = idx.size()\n\t        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n", "        # forward the GPT model itself\n\t        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n\t        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n\t        x = self.transformer.drop(tok_emb + pos_emb)\n\t        for block in self.transformer.h:\n\t            x = block(x)\n\t        x = self.transformer.ln_f(x)\n\t        if targets is not None:\n\t            # if we are given some desired targets also calculate the loss\n\t            logits = self.lm_head(x)\n", "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\t        else:\n\t            # inference-time mini-optimization: only forward the lm_head on the very last position\n\t            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n\t            loss = None\n\t        return logits, loss\n\t    def crop_block_size(self, block_size):\n\t        # model surgery to decrease the block size if necessary\n\t        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n\t        # but want to use a smaller block size for some smaller, simpler model\n", "        assert block_size <= self.config.block_size\n\t        self.config.block_size = block_size\n\t        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n\t        for block in self.transformer.h:\n\t            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\t    @classmethod\n\t    def from_pretrained(cls, model_type, override_args=None):\n\t        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n\t        override_args = override_args or {} # default to empty dict\n\t        # only dropout can be overridden see more notes below\n", "        assert all(k == 'dropout' for k in override_args)\n\t        from transformers import GPT2LMHeadModel\n\t        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\t        # n_layer, n_head and n_embd are determined from model_type\n\t        config_args = {\n\t            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n\t            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n\t            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n\t            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n\t        }[model_type]\n", "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n\t        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n\t        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n\t        config_args['bias'] = True # always True for GPT model checkpoints\n\t        # we can override the dropout rate, if desired\n\t        if 'dropout' in override_args:\n\t            print(f\"overriding dropout rate to {override_args['dropout']}\")\n\t            config_args['dropout'] = override_args['dropout']\n\t        # create a from-scratch initialized minGPT model\n\t        config = GPTConfig(**config_args)\n", "        model = GPT(config)\n\t        sd = model.state_dict()\n\t        sd_keys = sd.keys()\n\t        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\t        # init a huggingface/transformers model\n\t        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n\t        sd_hf = model_hf.state_dict()\n\t        # copy while ensuring all of the parameters are aligned and match in names and shapes\n\t        sd_keys_hf = sd_hf.keys()\n\t        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n", "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n\t        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n\t        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n\t        # this means that we have to transpose these weights when we import them\n\t        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n\t        for k in sd_keys_hf:\n\t            if any(k.endswith(w) for w in transposed):\n\t                # special treatment for the Conv1D weights we need to transpose\n\t                assert sd_hf[k].shape[::-1] == sd[k].shape\n\t                with torch.no_grad():\n", "                    sd[k].copy_(sd_hf[k].t())\n\t            else:\n\t                # vanilla copy over the other parameters\n\t                assert sd_hf[k].shape == sd[k].shape\n\t                with torch.no_grad():\n\t                    sd[k].copy_(sd_hf[k])\n\t        return model\n\t    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n\t        \"\"\"\n\t        This long function is unfortunately doing something very simple and is being very defensive:\n", "        We are separating out all parameters of the model into two buckets: those that will experience\n\t        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n\t        We are then returning the PyTorch optimizer object.\n\t        \"\"\"\n\t        # separate out all parameters to those that will and won't experience regularizing weight decay\n\t        decay = set()\n\t        no_decay = set()\n\t        whitelist_weight_modules = (torch.nn.Linear, )\n\t        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n\t        for mn, m in self.named_modules():\n", "            for pn, p in m.named_parameters():\n\t                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n\t                # random note: because named_modules and named_parameters are recursive\n\t                # we will see the same tensors p many many times. but doing it this way\n\t                # allows us to know which parent module any tensor p belongs to...\n\t                if pn.endswith('bias'):\n\t                    # all biases will not be decayed\n\t                    no_decay.add(fpn)\n\t                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n\t                    # weights of whitelist modules will be weight decayed\n", "                    decay.add(fpn)\n\t                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n\t                    # weights of blacklist modules will NOT be weight decayed\n\t                    no_decay.add(fpn)\n\t        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n\t        # will appear in the no_decay and decay sets respectively after the above.\n\t        # In addition, because named_parameters() doesn't return duplicates, it\n\t        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n\t        # so let's manually remove 'lm_head.weight' from decay set. This will include\n\t        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n", "        decay.remove('lm_head.weight')\n\t        # validate that we considered every parameter\n\t        param_dict = {pn: p for pn, p in self.named_parameters()}\n\t        inter_params = decay & no_decay\n\t        union_params = decay | no_decay\n\t        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n\t        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n\t                                                    % (str(param_dict.keys() - union_params), )\n\t        # create the pytorch optimizer object\n\t        optim_groups = [\n", "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n\t            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n\t        ]\n\t        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n\t        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n\t        print(f\"using fused AdamW: {use_fused}\")\n\t        extra_args = dict(fused=True) if use_fused else dict()\n\t        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n\t        return optimizer\n\t    def estimate_mfu(self, fwdbwd_per_iter, dt):\n", "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n\t        # first estimate the number of flops we do per iteration.\n\t        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n\t        N = self.get_num_params()\n\t        cfg = self.config\n\t        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n\t        flops_per_token = 6*N + 12*L*H*Q*T\n\t        flops_per_fwdbwd = flops_per_token * T\n\t        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n\t        # express our flops throughput as ratio of A100 bfloat16 peak flops\n", "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n\t        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n\t        mfu = flops_achieved / flops_promised\n\t        return mfu\n\t    @torch.no_grad()\n\t    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n\t        \"\"\"\n\t        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n\t        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n\t        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n", "        \"\"\"\n\t        for _ in range(max_new_tokens):\n\t            # if the sequence context is growing too long we must crop it at block_size\n\t            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n\t            # forward the model to get the logits for the index in the sequence\n\t            logits, _ = self(idx_cond)\n\t            # pluck the logits at the final step and scale by desired temperature\n\t            logits = logits[:, -1, :] / temperature\n\t            # optionally crop the logits to only the top k options\n\t            if top_k is not None:\n", "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n\t                logits[logits < v[:, [-1]]] = -float('Inf')\n\t            # apply softmax to convert logits to (normalized) probabilities\n\t            probs = F.softmax(logits, dim=-1)\n\t            # sample from the distribution\n\t            idx_next = torch.multinomial(probs, num_samples=1)\n\t            # append sampled index to the running sequence and continue\n\t            idx = torch.cat((idx, idx_next), dim=1)\n\t        return idx"]}
{"filename": "src/ml_ane_transformers/__init__.py", "chunked_list": []}
{"filename": "src/ml_ane_transformers/ane_gpt2.py", "chunked_list": ["\"\"\"\n\tA fairly modified version of nanoGPT that attempts to apply the principles\n\tfrom the ml-ane-transformers repo. Functional, but not as fast as I had hoped.\n\tReferences:\n\thttps://github.com/karpathy/nanoGPT/blob/a82b33b525ca9855d705656387698e13eb8e8d4b/model.py#L1\n\thttps://github.com/apple/ml-ane-transformers/tree/main/ane_transformers/reference\n\tOriginal nanoGPT License:\n\tMIT License\n\tCopyright (c) 2022 Andrej Karpathy\n\tPermission is hereby granted, free of charge, to any person obtaining a copy\n", "of this software and associated documentation files (the \"Software\"), to deal\n\tin the Software without restriction, including without limitation the rights\n\tto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\tcopies of the Software, and to permit persons to whom the Software is\n\tfurnished to do so, subject to the following conditions:\n\tThe above copyright notice and this permission notice shall be included in all\n\tcopies or substantial portions of the Software.\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n", "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\tOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\tSOFTWARE.\n\t\"\"\"\n\timport math\n\timport inspect\n\tfrom dataclasses import dataclass\n\timport torch\n\timport torch.nn as nn\n", "from torch.nn import functional as F\n\tfrom .ane.multihead_attention import SelfAttention as AneSelfAttention\n\t# from ane.multihead_attention import MultiHeadAttention as AneMultiHeadAttention\n\tfrom .ane.layer_norm import LayerNormANE as AneLayerNormx\n\tfrom .ane.dummy_layer_norm import DummyLayerNormANE\n\tfrom .ane.kahan_layer_norm import KahanLayerNormANE\n\tfrom .ane.ffn import FFN as AneMLP\n\t# Torch does not support layer norm over the 1st dimension. But the MIL (almost) does.\n\t# Override this to use the native 1st dimension MIL layer norm.\n\tOVERRIDE_LAYER_NORM = True\n", "layer_norm_cls = DummyLayerNormANE if OVERRIDE_LAYER_NORM else AneLayerNormx\n\tfrom coremltools.converters.mil import register_torch_op\n\tfrom coremltools.converters.mil.mil import Builder as mb\n\tfrom coremltools.converters.mil.frontend.torch.ops import _get_inputs\n\tfrom coremltools.converters.mil.frontend.torch.torch_op_registry import _TORCH_OPS_REGISTRY\n\tif OVERRIDE_LAYER_NORM:\n\t    if \"batch_norm\" in _TORCH_OPS_REGISTRY:\n\t        del _TORCH_OPS_REGISTRY[\"batch_norm\"]\n\t    @register_torch_op\n\t    def batch_norm(context, node):\n", "        inputs = _get_inputs(context, node, expected=9)\n\t        _input = inputs[0]\n\t        weight = inputs[1]\n\t        bias = inputs[2]\n\t        # running_mean = inputs[3]\n\t        # running_var = inputs[4]\n\t        # training = inputs[5].val\n\t        eps = inputs[7]\n\t        # Ideally would not need the transposes, but axes=[1] doesn't work\n\t        # on the Neural Engine. https://developer.apple.com/forums/thread/728931\n", "        # node.name has to go on the last op, that's also the one that gets added to the context\n\t        rs1 = mb.transpose(x=_input, perm=[0,3,2,1])\n\t        ln = mb.layer_norm(x=rs1, axes=[3], epsilon=eps, gamma=weight, beta=bias)\n\t        # context.add(ln)\n\t        rs2 = mb.transpose(x=ln, perm=[0,3,2,1], name=node.name)\n\t        context.add(rs2)\n\t# Note: torch.nn.LayerNorm and ane_transformers.reference.layer_norm.LayerNormANE\n\t# apply scale and bias terms in opposite orders. In order to accurately restore a\n\t# state_dict trained using the former into the the latter, we adjust the bias term\n\tdef correct_for_bias_scale_order_inversion(state_dict, prefix, local_metadata,\n", "                                           strict, missing_keys,\n\t                                           unexpected_keys, error_msgs):\n\t    if not OVERRIDE_LAYER_NORM:\n\t        state_dict[prefix +'bias'] = state_dict[prefix + 'bias'] / state_dict[prefix +'weight']\n\t    return state_dict\n\t# class AneLayerNorm(AneLayerNormx):\n\tclass AneLayerNorm(layer_norm_cls):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self._register_load_state_dict_pre_hook(\n", "            correct_for_bias_scale_order_inversion)\n\t# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n\tdef new_gelu(x):\n\t    \"\"\"\n\t    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n\t    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n\t    \"\"\"\n\t    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\tclass LayerNorm(nn.Module):\n\t    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n", "    def __init__(self, ndim, bias):\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(ndim))\n\t        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\t    def forward(self, input):\n\t        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\tclass CausalSelfAttention(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.n_embd % config.n_head == 0\n", "        # key, query, value projections for all heads, but in a batch\n\t        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n\t        # output projection\n\t        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\t        # regularization\n\t        self.attn_dropout = nn.Dropout(config.dropout)\n\t        self.resid_dropout = nn.Dropout(config.dropout)\n\t        self.n_head = config.n_head\n\t        self.n_embd = config.n_embd\n\t        self.dropout = config.dropout\n", "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n\t        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n\t        if not self.flash:\n\t            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n\t            # causal mask to ensure that attention is only applied to the left in the input sequence\n\t            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n\t                                        .view(1, 1, config.block_size, config.block_size))\n\t    def forward(self, x):\n\t        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\t        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n", "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\t        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\t        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n\t        if self.flash:\n\t            # efficient attention using Flash Attention CUDA kernels\n\t            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n\t        else:\n\t            # manual implementation of attention\n", "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\t            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n\t            att = F.softmax(att, dim=-1)\n\t            att = self.attn_dropout(att)\n\t            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\t        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\t        # output projection\n\t        y = self.resid_dropout(self.c_proj(y))\n\t        return y\n\tclass MLP(nn.Module):\n", "    def __init__(self, config):\n\t        super().__init__()\n\t        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n\t        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n\t        self.dropout = nn.Dropout(config.dropout)\n\t    def forward(self, x):\n\t        x = self.c_fc(x)\n\t        x = new_gelu(x)\n\t        x = self.c_proj(x)\n\t        x = self.dropout(x)\n", "        return x\n\tclass Block(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        # self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n\t        self.ln_1 = AneLayerNorm(config.n_embd)\n\t        # self.attn = CausalSelfAttention(config)\n\t        self.attn = AneSelfAttention(embed_dim=config.n_embd, n_head=config.n_head, dropout=config.dropout, return_weights=False)\n\t        # self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n\t        self.ln_2 = AneLayerNorm(config.n_embd)\n", "        self.mlp = AneMLP(config.n_embd, 4 * config.n_embd, dropout=config.dropout)\n\t    def forward(self, x, qk_mask=None, k_mask=None):\n\t        \"\"\"\n\t        x: (batch, embed_dim, 1, seqlen) aka BC1S\n\t        qk_mask, k_mask as in AneSelfAttention\n\t        \"\"\"\n\t        # print(\"xin\", x.shape)\n\t        # print(\"self.ln_1(x)\", self.ln_1(x).shape)\n\t        # print(\" self.attn(self.ln_1(x))\",  self.attn(self.ln_1(x)).shape)\n\t        x = x + self.attn(self.ln_1(x), qk_mask=qk_mask, k_mask=k_mask)\n", "        x = x + self.mlp(self.ln_2(x))\n\t        # print(\"xout\", x.shape)\n\t        return x\n\t@dataclass\n\tclass GPTConfig:\n\t    block_size: int = 1024\n\t    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n\t    n_layer: int = 12\n\t    n_head: int = 12\n\t    n_embd: int = 768\n", "    dropout: float = 0.0\n\t    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\tclass GPT(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        assert config.vocab_size is not None\n\t        assert config.block_size is not None\n\t        self.config = config\n\t        self.transformer = nn.ModuleDict(dict(\n\t            wte = nn.Embedding(config.vocab_size, config.n_embd),\n", "            wpe = nn.Embedding(config.block_size, config.n_embd),\n\t            drop = nn.Dropout(config.dropout),\n\t            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n\t            # ln_f = LayerNorm(config.n_embd, bias=config.bias),\n\t            ln_f = AneLayerNorm(config.n_embd),\n\t        ))\n\t        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\t        self.lm_head = nn.Conv2d(config.n_embd, config.vocab_size, 1, bias=False) # Confirmed that this is equivalent to ^. See: https://sebastianraschka.com/faq/docs/fc-to-conv.html\n\t        # with weight tying when using torch.compile() some warnings get generated:\n\t        # \"UserWarning: functional_call was passed multiple values for tied weights.\n", "        # This behavior is deprecated and will be an error in future versions\"\n\t        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n\t        # FIXME: This duplicates these (large) weights.\n\t        self.transformer.wte.weight = nn.Parameter(self.lm_head.weight.squeeze()) # https://paperswithcode.com/method/weight-tying\n\t        # init all weights\n\t        self.apply(self._init_weights)\n\t        # apply special scaled init to the residual projections, per GPT-2 paper\n\t        for pn, p in self.named_parameters():\n\t            if pn.endswith('c_proj.weight'):\n\t                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n", "        self.qk_mask = ((1 - torch.tril(torch.ones((512,512), dtype=torch.float32))).t() * -1e4).view(1, 512, 1, 512)\n\t        # report number of parameters\n\t        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\t    def get_num_params(self, non_embedding=True):\n\t        \"\"\"\n\t        Return the number of parameters in the model.\n\t        For non-embedding count (default), the position embeddings get subtracted.\n\t        The token embeddings would too, except due to the parameter sharing these\n\t        params are actually used as weights in the final layer, so we include them.\n\t        \"\"\"\n", "        n_params = sum(p.numel() for p in self.parameters())\n\t        if non_embedding:\n\t            n_params -= self.transformer.wpe.weight.numel()\n\t        return n_params\n\t    def _init_weights(self, module):\n\t        if isinstance(module, nn.Linear):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t            if module.bias is not None:\n\t                torch.nn.init.zeros_(module.bias)\n\t        elif isinstance(module, nn.Embedding):\n", "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\t    def prepare_for_block(self, idx):\n\t        device = idx.device\n\t        b, t = idx.size()\n\t        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\t        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n\t        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n\t        x = self.transformer.drop(tok_emb + pos_emb)\n\t        # print(\"x pre-blocks\", x.shape)\n", "        # BSC -> BC1S (most conducive to ANE)\n\t        # Batch=1, Sequence=NumTokens, Channels=NumEmbed aka Hidden Size\n\t        return x.transpose(1, 2).unsqueeze(2)\n\t    def forward(self, idx, output_mask=None, qk_mask=None, k_mask=None, targets=None):\n\t        device = idx.device\n\t        b, t = idx.size()\n\t        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\t        if qk_mask is None:\n\t            # qk_mask = ((1 - torch.tril(torch.ones((512,512), dtype=torch.float32))).t() * -1e4).view(1, 512, 1, 512)\n", "            qk_mask = self.qk_mask[:, :t, :, :t].to(device)\n\t        # print(\"forwardz\", idx.shape, self.transformer.wte.weight.shape)\n\t        # forward the GPT model itself\n\t        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n\t        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n\t        x = self.transformer.drop(tok_emb + pos_emb)\n\t        # print(\"x pre-blocks\", x.shape)\n\t        # BSC -> BC1S (most conducive to ANE)\n\t        # Batch=1, Sequence=NumTokens, Channels=NumEmbed aka Hidden Size\n\t        x = x.transpose(1, 2).unsqueeze(2)\n", "        for block in self.transformer.h:\n\t            x = block(x, qk_mask=qk_mask, k_mask=k_mask)\n\t        x = self.transformer.ln_f(x)\n\t        # No need to pass back the other length-1 results we don't care about.\n\t        # Big speed boost.\n\t        if output_mask is not None:\n\t            x = torch.index_select(x, 3, output_mask)\n\t            # TODO: Sprinkle in a softmax here? Or does that prevent us from doing top-p/top-k\n\t            loss = None\n\t        # logits = x\n", "        old = False\n\t        if old:\n\t            # Old slow way.\n\t            x = x.permute(3, 1, 0, 2) # (S, C, B, 1)\n\t            logits = self.lm_head(x).squeeze().unsqueeze(0).unsqueeze(0)\n\t        else:\n\t            # # New fast way.\n\t            # This part is actually quite fast. 4ms out of 75ms.\n\t            x = x.permute(0,2,3,1).squeeze(0)\n\t            # ANE can only load tensors with dim size of at most 16,384 - divide the vocab size to be less than that\n", "            # gpt2 vocab size is a product of two primes smh\n\t            splits = self.transformer.wte.weight.split(self.transformer.wte.weight.shape[0]//29, dim=0)\n\t            logits = torch.cat([torch.einsum('bid,jd->bij', x, split) for split in splits], dim=-1)#.view(*x.shape[:2], -1)\n\t        return logits\n\t    def crop_block_size(self, block_size):\n\t        # model surgery to decrease the block size if necessary\n\t        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n\t        # but want to use a smaller block size for some smaller, simpler model\n\t        assert block_size <= self.config.block_size\n\t        self.config.block_size = block_size\n", "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n\t        for block in self.transformer.h:\n\t            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\t    @classmethod\n\t    def from_pretrained(cls, model_type, override_args=None):\n\t        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ckiplab/gpt2-tiny-chinese'}\n\t        override_args = override_args or {} # default to empty dict\n\t        # only dropout can be overridden see more notes below\n\t        assert all(k == 'dropout' for k in override_args)\n\t        from transformers import GPT2LMHeadModel\n", "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\t        # n_layer, n_head and n_embd are determined from model_type\n\t        config_args = {\n\t            'ckiplab/gpt2-tiny-chinese': dict(n_layer=4, n_head=12, n_embd=312), # 18M, vocab size 21128\n\t            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n\t            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n\t            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n\t            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n\t        }[model_type]\n\t        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n", "        config_args['vocab_size'] = 21128 if \"chinese\" in model_type else 50257 # always 50257 for GPT model checkpoints\n\t        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n\t        config_args['bias'] = True # always True for GPT model checkpoints\n\t        # we can override the dropout rate, if desired\n\t        if 'dropout' in override_args:\n\t            print(f\"overriding dropout rate to {override_args['dropout']}\")\n\t            config_args['dropout'] = override_args['dropout']\n\t        # create a from-scratch initialized minGPT model\n\t        config = GPTConfig(**config_args)\n\t        model = GPT(config)\n", "        sd = model.state_dict()\n\t        sd_keys = sd.keys()\n\t        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\t        if OVERRIDE_LAYER_NORM:\n\t            sd_keys = [k for k in sd_keys if not k.endswith(\"running_mean\")]\n\t            sd_keys = [k for k in sd_keys if not k.endswith(\"running_var\")]\n\t            sd_keys = [k for k in sd_keys if not k.endswith(\"num_batches_tracked\")]\n\t        # init a huggingface/transformers model\n\t        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n\t        sd_hf = model_hf.state_dict()\n", "        transform_hf_weights(sd_hf)\n\t        # print(\"ane\", sd['transformer.h.0.attn.out_proj.weight'].shape)\n\t        # print(\"hf\", sd_hf['transformer.h.0.attn.c_attn.weight'].shape)\n\t        # HF uses one weights matrix for QKV (called C), but ANE uses 3.\n\t        # TODO: Maybe an opportunity to simplify? IDK\n\t        # for k in list(sd_hf.keys()):\n\t        #     # attention input\n\t        #     if \"attn.c_attn\" in k:\n\t        #         new_k = lambda nk: k.replace(\"c_attn\", nk)\n\t        #         catt = sd_hf[k]\n", "        #         dim = -1 # bias\n\t        #         if \"weight\" in k:\n\t        #             # catt = catt.unsqueeze(-1).unsqueeze(-1)\n\t        #             dim = -3 # weight\n\t        #         # FIXME: Not sure about the order.\n\t        #         # From HF: query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\t        #         qt, kt, vt = catt.chunk(3, dim=dim)\n\t        #         print(\"catt shape\", catt.shape)\n\t        #         assert qt.shape == sd[new_k(\"k_proj\")].shape,\\\n\t        #             f\"{k}: {qt.shape} != {sd[new_k('k_proj')].shape}\"\n", "        #         sd_hf[new_k(\"k_proj\")] = kt\n\t        #         sd_hf[new_k(\"q_proj\")] = qt\n\t        #         sd_hf[new_k(\"v_proj\")] = vt\n\t        #         del sd_hf[k]\n\t        #     # attention output\n\t        #     elif \"attn.c_proj\" in k:\n\t        #         new_k = lambda nk: k.replace(\"c_proj\", nk)\n\t        #         catt = sd_hf[k]\n\t        #         # if \"weight\" in k:\n\t        #         #     catt = catt.unsqueeze(-1).unsqueeze(-1)\n", "        #         new_out = catt\n\t        #         assert new_out.shape == sd[new_k(\"out_proj\")].shape,\\\n\t        #             f\"{k}: {new_out.shape} != {sd[new_k('out_proj')].shape}\"\n\t        #         sd_hf[new_k(\"out_proj\")] = new_out\n\t        #         del sd_hf[k]\n\t        # copy while ensuring all of the parameters are aligned and match in names and shapes\n\t        sd_keys_hf = sd_hf.keys()\n\t        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n\t        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n\t        if len(set(sd_keys) ^ set(sd_keys_hf)) > 0:\n", "            print(set(sd_keys_hf) - set(sd_keys))\n\t            print(set(sd_keys) - set(sd_keys_hf))\n\t        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n\t        for k in sd_keys_hf:\n\t            # if any(k.endswith(w) for w in transposed):\n\t            #     print(\"tpose\", k, sd_hf[k].shape)\n\t            #     # special treatment for the Conv1D weights we need to transpose\n\t            #     assert sd_hf[k].shape[::-1] == sd[k].shape, f\"{k}: {sd_hf[k].shape[::-1]} == {sd[k].shape}\"\n\t            #     with torch.no_grad():\n\t            #         sd[k].copy_(sd_hf[k].t())\n", "            # if any(k.endswith(w) for w in transposed_and_unsqueezed):\n\t            #     print(\"tpose+unsq\", k)\n\t            #     # special treatment for the Conv1D weights we need to transpose and unsqueeze\n\t            #     # :-2 to drop the last 2 singleton dimensions\n\t            #     # ::-1 to transpose\n\t            #     assert sd_hf[k].shape[::-1] == sd[k].shape[:-2], f\"{k}: {sd_hf[k].shape[::-1]} == {sd[k].shape[:-2]}\"\n\t            #     with torch.no_grad():\n\t            #         sd[k].copy_(sd_hf[k].t().unsqueeze(-1).unsqueeze(-1))\n\t            # else:\n\t                # vanilla copy over the other parameters\n", "            assert sd_hf[k].shape == sd[k].shape, f\"{k}: {sd_hf[k].shape} != {sd[k].shape}\"\n\t            with torch.no_grad():\n\t                sd[k].copy_(sd_hf[k])\n\t        return model\n\t    @torch.no_grad()\n\t    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n\t        \"\"\"\n\t        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n\t        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n\t        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n", "        \"\"\"\n\t        for _ in range(max_new_tokens):\n\t            # if the sequence context is growing too long we must crop it at block_size\n\t            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n\t            # forward the model to get the logits for the index in the sequence\n\t            logits, _ = self(idx_cond)\n\t            # pluck the logits at the final step and scale by desired temperature\n\t            logits = logits[:, -1, :] / temperature\n\t            # optionally crop the logits to only the top k options\n\t            if top_k is not None:\n", "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n\t                logits[logits < v[:, [-1]]] = -float('Inf')\n\t            # apply softmax to convert logits to (normalized) probabilities\n\t            probs = F.softmax(logits, dim=-1)\n\t            # sample from the distribution\n\t            idx_next = torch.multinomial(probs, num_samples=1)\n\t            # append sampled index to the running sequence and continue\n\t            idx = torch.cat((idx, idx_next), dim=1)\n\t        return idx\n\t    @staticmethod\n", "    def build_inputs(seq, pad_to_length=None, pad_token_id=-1, dtype=torch.float32, device=\"cpu\"):\n\t        seqlen = seq.shape[1]\n\t        if not pad_to_length:\n\t            pad_to_length = seqlen\n\t        length = pad_to_length\n\t        assert length == seqlen or pad_token_id != -1, \"pad token must be provided when padding\"\n\t        # Upper triangular mask but in the BC1S format. aka causal mask\n\t        # Note the transpose! Don't really get it, but easy to see when comparing the results of applying the mask.\n\t        qk_mask = ((1 - torch.tril(torch.ones((length,length), dtype=dtype))).t() * -1e4).view(1, length, 1, length)\n\t        # We want to attend to the whole sequence, but not the padding. aka attention mask\n", "        k_mask = torch.cat([\n\t            torch.zeros(seqlen, dtype=dtype),\n\t            torch.full((pad_to_length - seqlen,), float(-1e4), dtype=dtype)\n\t        ]).view(1,length,1,1)\n\t        # Pad the sequence itself too.\n\t        input_ids = torch.cat([\n\t            seq.squeeze(),\n\t            torch.full((pad_to_length - seqlen,), pad_token_id)\n\t        ]).unsqueeze(0)\n\t        # Used to mask outputs before they exit the model.\n", "        # input_ids: [0,1,2,3] length = 4, result is in index 3\n\t        output_mask = torch.tensor([seqlen-1], dtype=torch.int32)\n\t        return {\n\t            \"input_ids\": input_ids.int().to(device),\n\t            \"qk_mask\": qk_mask.to(device),\n\t            \"k_mask\": k_mask.to(device),\n\t            \"output_mask\": output_mask.to(device),\n\t        }\n\t# def linear_to_conv2d_map(state_dict):\n\t#     \"\"\" Unsqueeze twice to map nn.Linear weights to nn.Conv2d weights\n", "#     \"\"\"\n\t#     for k in state_dict:\n\t#         print(k, state_dict[k].shape)\n\t#         # is_internal_proj = all(substr in k for substr in ['lin', '.weight'])\n\t#         is_internal_proj = all(substr in k for substr in ['attn', '.weight'])\n\t#         is_output_proj = all(substr in k\n\t#                              for substr in ['classifier', '.weight'])\n\t#         is_lm_head = all(substr in k for substr in['lm_head', '.weight'])\n\t#         if is_internal_proj or is_output_proj or is_lm_head:\n\t#             if len(state_dict[k].shape) == 2:\n", "#                 if \"attn.c_proj\" in k: # or \"attn.c_attn\" in k:\n\t#                     print(\"YOOOOOOOO\", k)\n\t#                     state_dict[k] = state_dict[k].t()\n\t#                 state_dict[k] = state_dict[k][:, :, None, None]\n\tdef transform_hf_weights(state_dict):\n\t    for k in list(state_dict.keys()):\n\t        # print(k, state_dict[k].shape)\n\t        # In HF defined as:\n\t        # self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n\t        # Here:\n", "        # self.out_proj = nn.Conv2d(self.d_v, self.d_out, 1)\n\t        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n\t        if \"attn.c_proj.weight\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            newk = k.replace(\"attn.c_proj.weight\", \"attn.out_proj.weight\")\n\t            state_dict[newk] = state_dict.pop(k).t()[:, :, None, None]\n\t            # after = state_dict[newk].shape\n\t            # print(before, \"->\", after)\n\t        # Similar to c_proj.weight\n", "        elif \"attn.c_proj.bias\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            newk = k.replace(\"attn.c_proj.bias\", \"attn.out_proj.bias\")\n\t            state_dict[newk] = state_dict.pop(k)\n\t            # after = state_dict[newk].shape\n\t            # print(before, \"->\", after)\n\t        # In HF defined as:\n\t        # self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n\t        # Here:\n", "        # self.q_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n\t        # self.v_proj = nn.Conv2d(embed_dim, self.d_v, 1)\n\t        # self.k_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n\t        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n\t        elif \"attn.c_attn.weight\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            # From HF: query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\t            qm,km,vm = state_dict.pop(k).t()[:, :, None, None].chunk(3, dim=0)\n\t            state_dict[k.replace(\"c_attn\", \"q_proj\")] = qm\n", "            state_dict[k.replace(\"c_attn\", \"k_proj\")] = km\n\t            state_dict[k.replace(\"c_attn\", \"v_proj\")] = vm\n\t            # newks = [k.replace(\"c_attn\", f\"{l}_proj\") for l in [\"q\", \"k\", \"v\"]]\n\t            # afters = [state_dict[newk].shape for newk in newks]\n\t            # print(before, \"->\", afters)\n\t        # Similar to c_attn.weight\n\t        elif \"attn.c_attn.bias\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            qm,km,vm = state_dict.pop(k).chunk(3, dim=0)\n", "            state_dict[k.replace(\"c_attn\", \"q_proj\")] = qm\n\t            state_dict[k.replace(\"c_attn\", \"k_proj\")] = km\n\t            state_dict[k.replace(\"c_attn\", \"v_proj\")] = vm\n\t            # newks = [k.replace(\"c_attn\", f\"{l}_proj\") for l in [\"q\", \"k\", \"v\"]]\n\t            # afters = [state_dict[newk].shape for newk in newks]\n\t            # print(before, \"->\", afters)\n\t        # In HF defined as:\n\t        # self.c_fc = Conv1D(intermediate_size, embed_dim)\n\t        # Here:\n\t        # self.c_fc = nn.Conv2d(embed_dim, ffn_dim, 1)\n", "        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n\t        elif \"mlp.c_fc.weight\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            state_dict[k] = state_dict[k].t()[:, :, None, None]\n\t            # after = state_dict[k].shape\n\t            # print(before, \"->\", after)\n\t        # In HF defined as:\n\t        # self.c_proj = Conv1D(embed_dim, intermediate_size)\n\t        # Here:\n", "        # self.c_proj = nn.Conv2d(ffn_dim, embed_dim, 1)\n\t        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n\t        elif \"mlp.c_proj.weight\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            state_dict[k] = state_dict[k].t()[:, :, None, None]\n\t            # after = state_dict[k].shape\n\t            # print(before, \"->\", after)\n\t        # In HF defined as:\n\t        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n", "        # Here:\n\t        # self.lm_head = nn.Conv2d(config.n_embd, config.vocab_size, 1, bias=False)\n\t        # To go from Linear to Conv2d, we need to unsqueeze twice (NO TRANSPOSE).\n\t        elif \"lm_head.weight\" in k:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            state_dict[k] = state_dict[k][:, :, None, None]\n\t            # after = state_dict[k].shape\n\t            # print(before, \"->\", after)\n\t        # Note: torch.nn.LayerNorm and ane_transformers.reference.layer_norm.LayerNormANE\n", "        # apply scale and bias terms in opposite orders. In order to accurately restore a\n\t        # state_dict trained using the former into the the latter, we adjust the bias term\n\t        elif \".ln_\" in k and \".bias\" in k and not OVERRIDE_LAYER_NORM:\n\t            # print(k, state_dict[k].shape)\n\t            # before = state_dict[k].shape\n\t            weight_key = k.replace(\".bias\", \".weight\")\n\t            state_dict[k] = state_dict[k] / state_dict[weight_key]\n\t            # after = state_dict[k].shape\n\t            # print(before, \"->\", after)\n\t        elif \".ln_\" in k and OVERRIDE_LAYER_NORM:\n", "            newk = k.replace(\".weight\", \".bn.weight\")\n\t            newk = newk.replace(\".bias\", \".bn.bias\")\n\t            state_dict[newk] = state_dict.pop(k)\n\t        # else:\n\t        #     if \"bias\" in k:\n\t        #         print(k)\n"]}
{"filename": "src/ml_ane_transformers/convert-ane-layers.py", "chunked_list": ["import torch\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n\timport coremltools as ct\n\timport numpy as np\n\tfrom datetime import datetime\n\tfrom ane_gpt2 import GPT as ANEGPT\n\t\"\"\"\n\tExperimental setup for going layer-by-layer to see which\n\tlayers cause a drop in PSNR after conversion to CoreML.\n\tTurns out it's the LayerNorm (specifically computing the sum in float16\n", "before dividing it for the mean).\n\t\"\"\"\n\tdef compute_psnr(a, b):\n\t    \"\"\" Compute Peak-Signal-to-Noise-Ratio across two numpy.ndarray objects\n\t    \"\"\"\n\t    max_b = np.abs(b).max()\n\t    sumdeltasq = 0.0\n\t    sumdeltasq = ((a - b) * (a - b)).sum()\n\t    sumdeltasq /= b.size\n\t    sumdeltasq = np.sqrt(sumdeltasq)\n", "    eps = 1e-5\n\t    eps2 = 1e-10\n\t    psnr = 20 * np.log10((max_b + eps) / (sumdeltasq + eps2))\n\t    return psnr\n\tfile_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n\tclass TestNet(torch.nn.Module):\n\t    \"\"\"\n\t    Wraps a ANEGPT model so individual layers and partial\n\t    combinations of layers can be tested.\n\t    \"\"\"\n", "    def __init__(self, ane: ANEGPT):\n\t        super().__init__()\n\t        self.ane = ane\n\t    def forward(self, x, qk_mask=None, k_mask=None, output_mask=None):\n\t        device = x.device\n\t        b, t = x.size()\n\t        # assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\t        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\t        tok_emb = self.ane.transformer.wte(x) # token embeddings of shape (b, t, n_embd)\n\t        pos_emb = self.ane.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n", "        x = self.ane.transformer.drop(tok_emb + pos_emb) # psnr: f32:212, f16:99\n\t        x = x.transpose(1, 2).unsqueeze(2)\n\t        for h in self.ane.transformer.h[:1]:\n\t            x = h(x, qk_mask, k_mask)\n\t            # x = h.ln_1(x)\n\t            # x = h.attn(x, qk_mask=qk_mask, k_mask=k_mask) # psnr f16:89\n\t            # x = h.ln_2(x) # psnr f16:77\n\t            # x = h.mlp(x)  # psnr f16:62\n\t        x = x.permute(3, 1, 0, 2)\n\t        x = self.ane.lm_head(x).squeeze().unsqueeze(0)\n", "        if output_mask is not None:\n\t            x = torch.index_select(x, 1, output_mask)\n\t        return x\n\tmodel_name = \"gpt2\"\n\tprint(f\"Loading model {model_name}\")\n\tane = ANEGPT.from_pretrained(model_name).eval()\n\ttoken_predictor = TestNet(ane).eval()\n\t# token_predictor = ane # test the whole model\n\trandom_tokens = torch.randint(30000, (1,10,))\n\tinputs_dict = ANEGPT.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n", "output_mask = torch.tensor([13], dtype=torch.int32)\n\tprint(\"output_mask\", output_mask)\n\tinput_ids, qk_mask, k_mask = inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"]\n\tprint(f\"Tracing the model with {input_ids.shape}\")\n\ttraced_token_predictor = torch.jit.trace(token_predictor, (input_ids, qk_mask, k_mask, output_mask))\n\tprint(traced_token_predictor)\n\tprint(\"Trace finished\")\n\tprint(\"Beginning conversion\")\n\tmlmodel = ct.convert(\n\t    traced_token_predictor,\n", "    inputs=[\n\t        ct.TensorType(name=\"input_ids\", shape=input_ids.shape, dtype=np.int32),\n\t        ct.TensorType(name=\"qk_mask\", shape=[1, 512, 1, 512], dtype=np.float32),\n\t        ct.TensorType(name=\"k_mask\", shape=[1, 512, 1, 1], dtype=np.float32),\n\t        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n\t    ],\n\t    outputs=[\n\t        ct.TensorType(name=\"logits\", dtype=np.float32),\n\t    ],\n\t    compute_precision=ct.precision.FLOAT16,\n", "    # minimum_deployment_target=ct.target.macOS13,\n\t    convert_to=\"mlprogram\",\n\t)\n\tprint(\"Conversion finished\")\n\twith torch.no_grad():\n\t    og_out = token_predictor(input_ids, qk_mask, k_mask, output_mask).to(torch.float32)\n\t    tr_out = traced_token_predictor(input_ids, qk_mask, k_mask, output_mask).to(torch.float32)\n\tprint(\"output_mask\", output_mask.shape, output_mask.dtype)\n\tcm_out = mlmodel.predict({\"input_ids\": input_ids, \"qk_mask\": qk_mask, \"k_mask\": k_mask, \"output_mask\": output_mask})\n\tcm_out = torch.from_numpy(cm_out[\"logits\"]).to(torch.float32)\n", "assert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\n\tassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"\n\tprint(\"traced-og     psnr:\", compute_psnr(og_out.numpy(), tr_out.numpy()))\n\tprint(\"coreml-traced psnr:\", compute_psnr(tr_out.numpy(), cm_out.numpy()))\n\tprint(\"coreml-og     psnr:\", compute_psnr(og_out.numpy(), cm_out.numpy()))\n"]}
{"filename": "src/ml_ane_transformers/convert-ane.py", "chunked_list": ["import torch\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n\timport coremltools as ct\n\timport numpy as np\n\tfrom datetime import datetime\n\tfrom .ane_gpt2 import GPT as ANEGPT\n\tfrom src.utils.psnr import compute_psnr\n\t\"\"\"\n\tConvert a ANE-optimized nanoGPT to CoreML.\n\t\"\"\"\n", "file_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n\tmodel_name = \"gpt2\"\n\t# model_name = \"ckiplab/gpt2-tiny-chinese\"\n\tmodel_filename = model_name.split(\"/\")[-1] + \"_\" + file_suffix\n\tretrace = True\n\tif retrace:\n\t    print(f\"Loading model {model_name}\")\n\t    # token_predictor = AutoModelForCausalLM.from_pretrained(model_name, torchscript=True).eval()\n\t    token_predictor = ANEGPT.from_pretrained(model_name).eval()\n\t    random_tokens = torch.randint(10000, (1,10,))\n", "    inputs_dict = token_predictor.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n\t    print(f\"Tracing the model with {inputs_dict['input_ids']}\")\n\t    input_ids, qk_mask, k_mask, output_mask = [inputs_dict[k] for k in\\\n\t                                            [\"input_ids\", \"qk_mask\", \"k_mask\", \"output_mask\"]]\n\t    del inputs_dict[\"k_mask\"]\n\t    # Exclude k_mask. It's a no-op for next-token prediction.\n\t    traced_token_predictor = torch.jit.trace(token_predictor, (input_ids, output_mask))\n\t    #traced_token_predictor.save(f\"{model_filename}.pt\")\n\telse:\n\t    print(\"Loading from saved file\")\n", "    traced_token_predictor = torch.jit.load(f\"{model_filename}.pt\")\n\tprint(traced_token_predictor)\n\tprint(\"Trace finished\")\n\tprint(\"Beginning conversion\")\n\t# Going totally F16 is too much. The PSNR drops dramatically and generation is completely garbled.\n\tdef op_selector(op):\n\t    \"\"\"\n\t    Return true to use float16 for the op. Must be f16 to run on Neural Engine.\n\t    You can find op_type by looking in Netron and print out the op names here\n\t    (usually they contain a variable name).\n", "    \"\"\"\n\t    # All the ops involved in LayerNorm. Keep this in f32.\n\t    # LayerNorm is where we lose most of our precision. Interestingly, it seems\n\t    # the first mean contributes almost all the error.\n\t    # TODO: This may only be a problem for \"gpt\", never tried the larger variants.\n\t    return op.op_type not in [\"reduce_mean\"] or \"channels_mean\" not in op.name\n\tcompute_precision=ct.precision.FLOAT16\n\t# compute_precision=ct.transform.FP16ComputePrecision(op_selector)\n\tmlmodel = ct.convert(\n\t    traced_token_predictor,\n", "    inputs=[\n\t        ct.TensorType(name=\"input_ids\", shape=[1, 512], dtype=np.int32),\n\t        # ct.TensorType(name=\"qk_mask\", shape=[1, 512, 1, 512], dtype=np.float32),\n\t        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n\t    ],\n\t    outputs=[\n\t        ct.TensorType(name=\"logits\", dtype=np.float32),\n\t    ],\n\t    compute_precision=compute_precision,\n\t    # minimum_deployment_target=ct.target.macOS13,\n", "    convert_to=\"mlprogram\",\n\t)\n\tprint(\"Conversion finished\")\n\tsuffix = \"\"\n\tif compute_precision == ct.precision.FLOAT32:\n\t    suffix=\"-f32\"\n\t# Save first, sometimes CoreML segfaults.\n\tprint(\"Saving\")\n\tmlmodel.save(f\"{model_filename}{suffix}-trash-nosplit-allf16.mlpackage\")\n\t# Always compare in float32 so we don't overflow.\n", "with torch.no_grad():\n\t    og_out = token_predictor(input_ids, output_mask=output_mask).to(torch.float32)\n\t    tr_out = traced_token_predictor(input_ids, output_mask=output_mask).to(torch.float32)\n\tprint({k: f\"{v.shape}-{v.dtype}\" for k,v in inputs_dict.items()})\n\tdel inputs_dict[\"qk_mask\"]\n\t# del inputs_dict[\"k_mask\"]\n\tcm_out = mlmodel.predict(inputs_dict)\n\tcm_out = torch.from_numpy(cm_out[\"logits\"]).to(torch.float32)\n\tassert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\n\tassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"\n", "print(\"this should be quite high. probably >200 or more.\")\n\tprint(\"traced-original psnr:\", compute_psnr(tr_out.numpy(), og_out.numpy()))\n\tprint(\"\\nthese should be >60, ideally much higher.\") # otherwise you're only going to generate gibberish\n\tprint(\"coreml-traced   psnr:\", compute_psnr(cm_out.numpy(), tr_out.numpy()))\n\tprint(\"coreml-original psnr:\", compute_psnr(cm_out.numpy(), og_out.numpy()))\n\t# np.testing.assert_allclose(og_out.numpy(), tr_out.numpy(), atol=1e-5, rtol=1e-4)\n\t# np.testing.assert_allclose(cm_out, tr_out.numpy(), atol=1e-5, rtol=1e-4)\n"]}
{"filename": "src/ml_ane_transformers/ane/layer_norm.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE.md file.\n\t# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\t#\n\timport torch\n\timport torch.nn as nn\n\tclass LayerNormANE(nn.Module):\n\t    \"\"\" LayerNorm optimized for Apple Neural Engine (ANE) execution\n\t    Note: This layer only supports normalization over the final dim. It expects `num_channels`\n\t    as an argument and not `normalized_shape` which is used by `torch.nn.LayerNorm`.\n", "    \"\"\"\n\t    def __init__(self,\n\t                 num_channels,\n\t                 clip_mag=None,\n\t                 eps=1e-5,\n\t                 elementwise_affine=True):\n\t        \"\"\"\n\t        Args:\n\t            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n\t            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n", "                                If specified, helps reduce risk of overflow.\n\t            eps:                Small value to avoid dividing by zero\n\t            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n\t        \"\"\"\n\t        super().__init__()\n\t        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n\t        self.expected_rank = len('BC1S')\n\t        self.num_channels = num_channels\n\t        self.eps = eps\n\t        self.clip_mag = clip_mag\n", "        self.elementwise_affine = elementwise_affine\n\t        if self.elementwise_affine:\n\t            self.weight = nn.Parameter(torch.Tensor(num_channels))\n\t            self.bias = nn.Parameter(torch.Tensor(num_channels))\n\t        self._reset_parameters()\n\t    def _reset_parameters(self):\n\t        if self.elementwise_affine:\n\t            nn.init.ones_(self.weight)\n\t            nn.init.zeros_(self.bias)\n\t    def check_overunderflow(self, x):\n", "        f16_min = torch.finfo(torch.float16).min\n\t        f16_max = torch.finfo(torch.float16).max\n\t        # Check for values that are outside the range of float16\n\t        ouf = torch.any(x < f16_min) or torch.any(x > f16_max) or torch.isnan(x).any() or torch.isinf(x).any()\n\t        if ouf:\n\t            print(\"x is out of bounds\", x)\n\t    def forward(self, inputs):\n\t        input_rank = len(inputs.size())\n\t        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n\t        # Migrate the data format from BSC to BC1S (most conducive to ANE)\n", "        if input_rank == 3 and inputs.size(2) == self.num_channels:\n\t            inputs = inputs.transpose(1, 2).unsqueeze(2)\n\t            input_rank = len(inputs.size())\n\t        assert input_rank == self.expected_rank\n\t        assert inputs.size(1) == self.num_channels\n\t        assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\t        if self.clip_mag is not None:\n\t            inputs.clamp_(-self.clip_mag, self.clip_mag)\n\t        channels_mean = inputs.mean(dim=1, keepdims=True)\n\t        zero_mean = inputs - channels_mean\n", "        zero_mean_sq = zero_mean * zero_mean\n\t        denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\t        out = zero_mean * denom\n\t        if self.elementwise_affine:\n\t            out = (out + self.bias.view(1, self.num_channels, 1, 1)\n\t                   ) * self.weight.view(1, self.num_channels, 1, 1)\n\t        # Deviate from ANE implementation, match nn.\n\t        # if self.elementwise_affine:\n\t        #     out = (out * self.weight.view(1, self.num_channels, 1, 1)\n\t        #            ) + self.bias.view(1, self.num_channels, 1, 1)\n", "        return out"]}
{"filename": "src/ml_ane_transformers/ane/ffn.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE.md file.\n\t# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\t#\n\timport torch\n\timport torch.nn as nn\n\timport math\n\tfrom .layer_norm import LayerNormANE\n\tdef new_gelu(x):\n\t    \"\"\"\n", "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n\t    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n\t    \"\"\"\n\t    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\tclass FFN(nn.Module):\n\t    def __init__(self, embed_dim, ffn_dim, dropout=0.1, **kwargs):\n\t        super().__init__()\n\t        self.c_fc = nn.Conv2d(embed_dim, ffn_dim, 1)\n\t        # gelu\n\t        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n", "        self.c_proj = nn.Conv2d(ffn_dim, embed_dim, 1)\n\t    def _forward_impl(self, x, **kwargs):\n\t        x = self.c_fc(x)\n\t        # Match OpenAI GPT implementation, gelu instead of relu.\n\t        x = new_gelu(x)\n\t        x = self.dropout(x)\n\t        x = self.c_proj(x)\n\t        return x\n\t    def forward(self, x):\n\t        return self._forward_impl(x)\n", "class ResidualFFN(FFN):\n\t    def __init__(self, embed_dim, dropout=0.1, drop_fn=nn.Dropout, **kwargs):\n\t        super().__init__(embed_dim, dropout=dropout, **kwargs)\n\t        self.rdropout = drop_fn(dropout) if dropout > 0. else nn.Identity()\n\t        self.rnorm = LayerNormANE(embed_dim)\n\t    def forward(self, x):\n\t        residual = self._forward_impl(x)\n\t        return self.rnorm(self.rdropout(residual) + x)\n\tclass PreNormResidualFFN(ResidualFFN):\n\t    def forward(self, x):\n", "        residual = self.rdropout(self._forward_impl(self.rnorm(x)))\n\t        return x + residual"]}
{"filename": "src/ml_ane_transformers/ane/kahan_layer_norm.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE.md file.\n\t# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\t#\n\timport torch\n\timport torch.nn as nn\n\tclass KahanLayerNormANE(nn.Module):\n\t    \"\"\"\n\t    LayerNorm optimized for Apple Neural Engine (ANE) execution, using Kahan summation\n\t    to attempt to reduce numerical error. Unfortunately in practice, it slightly increases it.\n", "    Note: This layer only supports normalization over the final dim. It expects `num_channels`\n\t    as an argument and not `normalized_shape` which is used by `torch.nn.LayerNorm`.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 num_channels,\n\t                 clip_mag=None,\n\t                 eps=1e-5,\n\t                 elementwise_affine=True):\n\t        \"\"\"\n\t        Args:\n", "            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n\t            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n\t                                If specified, helps reduce risk of overflow.\n\t            eps:                Small value to avoid dividing by zero\n\t            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n\t        \"\"\"\n\t        super().__init__()\n\t        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n\t        self.expected_rank = len('BC1S')\n\t        self.num_channels = num_channels\n", "        self.eps = eps\n\t        self.clip_mag = clip_mag\n\t        self.elementwise_affine = elementwise_affine\n\t        if self.elementwise_affine:\n\t            self.weight = nn.Parameter(torch.Tensor(num_channels))\n\t            self.bias = nn.Parameter(torch.Tensor(num_channels))\n\t        self._reset_parameters()\n\t    def _reset_parameters(self):\n\t        if self.elementwise_affine:\n\t            nn.init.ones_(self.weight)\n", "            nn.init.zeros_(self.bias)\n\t    def check_overunderflow(self, x):\n\t        f16_min = torch.finfo(torch.float16).min\n\t        f16_max = torch.finfo(torch.float16).max\n\t        # Check for values that are outside the range of float16\n\t        ouf = torch.any(x < f16_min) or torch.any(x > f16_max) or torch.isnan(x).any() or torch.isinf(x).any()\n\t        if ouf:\n\t            print(\"x is out of bounds\", x)\n\t    @staticmethod\n\t    def kahan_mean(inputs, size: int = 4):\n", "        assert inputs.size(1) % size == 0, \"Batch size must be divisible by channels size.\"\n\t        s = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # total\n\t        c = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # compensation\n\t        rc = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # compensation\n\t        # inputs = torch.sort(inputs, dim=1).values\n\t        for batch in inputs.chunk(size, dim=1):\n\t            # print(\"\\n----\\nbatch\", batch)\n\t            # print(\"batch_sum:\", batch.sum(dim=1, keepdims=True))\n\t            # print(\"c:\", c)\n\t            batch_sum = batch.sum(dim=1, keepdims=True)\n", "            # y = batch_sum - c\n\t            t = batch_sum\n\t            # KBN sum\n\t            abss = torch.abs(s)\n\t            abst = torch.abs(batch_sum)\n\t            max_st = torch.where(abss >= abst, s, batch_sum)\n\t            min_st = torch.where(abss < abst, s, batch_sum)\n\t            # print(max_st, min_st)\n\t            c += (max_st - batch_sum) + min_st\n\t            # print(\"y = batch_sum - c\\n\", y)\n", "            # t = s + y\n\t            # print(\"t = s + y\\n\", t)\n\t            # print(c.shape, t.shape, s.shape, y.shape)\n\t            # print(c.dtype, t.dtype, s.dtype, y.dtype)\n\t            # c = (t - s) - y\n\t            # rc += c\n\t            # print(\"c = (t-s) - y\\n\", c)\n\t            s = t\n\t            # print(\"s = t\\n\", s)\n\t            # batch_sum = batch.sum(dim=1, keepdims=True)\n", "            # print(\"batch\")\n\t            # print(batch)\n\t            # print(\"batch_sum\")\n\t            # print(batch_sum)\n\t            # batch_comp = batch_sum - torch.sum(batch - c)\n\t            # s += batch_sum\n\t            # c = batch_comp\n\t        # print(\"c\", c)\n\t        # print(s, rc)\n\t        # print(\"torch.mean\", inputs.mean(dim=1, keepdims=True))\n", "        # print(\"kahan mean\", s / inputs.size(1))\n\t        return (s / inputs.size(1)) + (c / inputs.size(1))\n\t    @staticmethod\n\t    def stable_mean(inputs, size: int = 4):\n\t        assert inputs.size(1) % size == 0, \"Batch size must be divisible by channels size.\"\n\t        # (x+y+z) / 3 = (x+y)/3 + z/3\n\t        m = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # total\n\t        for batch in inputs.chunk(size, dim=1):\n\t            m += batch.sum(dim=1, keepdims=True) / inputs.size(1)\n\t        # print(\"torch.mean\", inputs.mean(dim=1, keepdims=True))\n", "        # print(\"stable mean\", m)\n\t        return m\n\t    def forward(self, inputs):\n\t        input_rank = len(inputs.size())\n\t        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n\t        # Migrate the data format from BSC to BC1S (most conducive to ANE)\n\t        if input_rank == 3 and inputs.size(2) == self.num_channels:\n\t            inputs = inputs.transpose(1, 2).unsqueeze(2)\n\t            input_rank = len(inputs.size())\n\t        assert input_rank == self.expected_rank\n", "        assert inputs.size(1) == self.num_channels\n\t        assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\t        if self.clip_mag is not None:\n\t            inputs.clamp_(-self.clip_mag, self.clip_mag)\n\t        # Reference implementation.\n\t        # channels_mean = inputs.mean(dim=1, keepdims=True)\n\t        # zero_mean = inputs - channels_mean\n\t        # zero_mean_sq = zero_mean * zero_mean\n\t        # denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\t        # out = zero_mean * denom\n", "        # Kahan implementation.\n\t        size = 4\n\t        channels_mean = self.kahan_mean(inputs, size=size)\n\t        # channels_mean = self.stable_mean(inputs, size=size)\n\t        zero_mean = inputs - channels_mean\n\t        zero_mean_sq = zero_mean * zero_mean\n\t        denom = (self.kahan_mean(zero_mean_sq, size=size) + self.eps).rsqrt()\n\t        # denom = (self.stable_mean(zero_mean_sq, size=size) + self.eps).rsqrt()\n\t        # denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\t        out = zero_mean * denom\n", "        if self.elementwise_affine:\n\t            out = (out + self.bias.view(1, self.num_channels, 1, 1)\n\t                   ) * self.weight.view(1, self.num_channels, 1, 1)\n\t        # Deviate from ANE implementation, match nn.\n\t        # if self.elementwise_affine:\n\t        #     out = (out * self.weight.view(1, self.num_channels, 1, 1)\n\t        #            ) + self.bias.view(1, self.num_channels, 1, 1)\n\t        return out"]}
{"filename": "src/ml_ane_transformers/ane/dummy_layer_norm.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE.md file.\n\t# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\t#\n\timport torch\n\timport torch.nn as nn\n\tclass DummyLayerNormANE(nn.Module):\n\t    \"\"\"\n\t    This looks like the ANELayerNorm class, but it just calls through to the\n\t    torch batch norm (mostly same inputs/outputs as layer norm) and we replace\n", "    that with a custom coremltools mb (torch layer norm doesn't support\n\t    the 1st dimension, MIL does).\n\t    \"\"\"\n\t    def __init__(self,\n\t                 num_channels,\n\t                 clip_mag=None,\n\t                 eps=1e-5,\n\t                 elementwise_affine=True):\n\t        \"\"\"\n\t        Args:\n", "            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n\t            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n\t                                If specified, helps reduce risk of overflow.\n\t            eps:                Small value to avoid dividing by zero\n\t            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n\t        \"\"\"\n\t        super().__init__()\n\t        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n\t        self.expected_rank = len('BC1S')\n\t        self.num_channels = num_channels\n", "        self.eps = eps\n\t        self.clip_mag = clip_mag\n\t        self.elementwise_affine = elementwise_affine\n\t        self.bn = nn.BatchNorm2d(num_channels, eps=self.eps, affine=self.elementwise_affine)\n\t    def forward(self, inputs):\n\t        out = self.bn(inputs)\n\t        return out"]}
{"filename": "src/ml_ane_transformers/ane/__init__.py", "chunked_list": []}
{"filename": "src/ml_ane_transformers/ane/multihead_attention.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE.md file.\n\t# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\t#\n\timport torch\n\timport torch.nn as nn\n\tfrom enum import Enum\n\tfrom .layer_norm import LayerNormANE\n\tclass AttentionImplementations(Enum):\n\t    #ORIGINAL = \"ORIGINAL\"\n", "    EINSUM = \"EINSUM\"\n\t    SPLIT_EINSUM = \"SPLIT_EINSUM\"\n\tATTENTION_IMPLEMENTATION_IN_EFFECT = AttentionImplementations.SPLIT_EINSUM\n\tclass MultiHeadAttention(nn.Module):\n\t    \"\"\" Multi-Head Attention optimized for efficient ANE deployment\n\t    \"\"\"\n\t    def __init__(self,\n\t                 embed_dim,\n\t                 d_qk=None,\n\t                 d_v=None,\n", "                 d_out=None,\n\t                 n_head=8,\n\t                 dropout=0.1,\n\t                 **kwargs):\n\t        \"\"\"\n\t        Args:\n\t            embed_dim:          Dimensionality of the input embeddings\n\t            d_qk:               Dimensionality of the query and key embeddings. They must match in order to compute\n\t                                dot product attention. If None, it is set to that of the input tensors, i.e. `embed_dim`\n\t            d_v:                Dimensionality of the value embeddings. It may differ from that of the query and\n", "                                key embeddings. If None, it is set to that of the input tensors.\n\t            d_out:              Dimensionality of the output projection. If None, it is set to that of the input tensors\n\t            n_head:             The number of different attention heads to compute in parallel, uses :math:`d_qk/n_head`\n\t                                channel groups in parallel to learn different attention patterns in a single layer\n\t            dropout:            The probability that each attention weight is zero-masked independent of other weights\n\t        \"\"\"\n\t        super().__init__()\n\t        self.d_qk = d_qk or embed_dim\n\t        self.d_v = d_v or embed_dim\n\t        self.d_out = d_out or embed_dim\n", "        self.n_head = n_head\n\t        if self.d_qk % self.n_head != 0 or self.d_v % self.n_head != 0:\n\t            raise ValueError(\n\t                f\"Either query-key dimensions ({self.d_qk}) or the value embeddings \"\n\t                f\"dimensions ({self.d_v}) is not divisible by n_head ({self.n_head})\"\n\t            )\n\t        self.q_normalize_fact = float(self.d_qk // self.n_head)**-0.5\n\t        self.q_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n\t        self.v_proj = nn.Conv2d(embed_dim, self.d_v, 1)\n\t        self.k_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n", "        self.out_proj = nn.Conv2d(self.d_v, self.d_out, 1)\n\t        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n\t        self.apply(self._reset_parameters)\n\t    @staticmethod\n\t    def _reset_parameters(module):\n\t        if isinstance(module, nn.Conv2d):\n\t            nn.init.xavier_uniform_(module.weight)\n\t            nn.init.constant_(module.bias, 0.)\n\t    def _qk(self, q, k, v):\n\t        # Principle 2: Chunking Large Intermediate Tensors  (machinelearning.apple.com/research/apple-neural-engine)\n", "        # Split q, k and v to compute a list of single-head attention functions\n\t        mh_q = q.split(\n\t            self.d_qk // self.n_head,\n\t            dim=1)  # n_head * (batch_size, d_qk/n_head, 1, tgt_seq_len)\n\t        # Principle 3: Minimizing Memory Copies\n\t        # Avoid as many transposes and reshapes as possible\n\t        mh_k = k.transpose(1, 3).split(\n\t            self.d_qk // self.n_head,\n\t            dim=3)  # n_head * (batch_size, src_seq_len, 1, d_qk/n_head)\n\t        mh_v = v.split(\n", "            self.d_v // self.n_head,\n\t            dim=1)  # n_head * (batch_size, d_v/n_head, 1, src_seq_len)\n\t        # `qk = q @ k`\n\t        attn_weights = [\n\t            torch.einsum('bchq,bkhc->bkhq', [qi, ki]) * self.q_normalize_fact\n\t            for qi, ki in zip(mh_q, mh_k)\n\t        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\t        attn_weights = torch.cat(attn_weights, dim=2)\n\t        return attn_weights\n\t    def _qk_softmax(self, q, k, v, qk_mask=None, k_mask=None):\n", "        # Principle 2: Chunking Large Intermediate Tensors  (machinelearning.apple.com/research/apple-neural-engine)\n\t        # Split q, k and v to compute a list of single-head attention functions\n\t        # Do the scalar multiply before the matmul so it can be folded into the conv.\n\t        q = q * self.q_normalize_fact\n\t        mh_q = q.split(\n\t            self.d_qk // self.n_head,\n\t            dim=1)  # n_head * (batch_size, d_qk/n_head, 1, tgt_seq_len)\n\t        # Principle 3: Minimizing Memory Copies\n\t        # Avoid as many transposes and reshapes as possible\n\t        mh_k = k.transpose(1, 3).split(\n", "            self.d_qk // self.n_head,\n\t            dim=3)  # n_head * (batch_size, src_seq_len, 1, d_qk/n_head)\n\t        mh_v = v.split(\n\t            self.d_v // self.n_head,\n\t            dim=1)  # n_head * (batch_size, d_v/n_head, 1, src_seq_len)\n\t        # `qk = q @ k`\n\t        attn_weights = [\n\t            torch.einsum('bchq,bkhc->bkhq', [qi, ki])\n\t            for qi, ki in zip(mh_q, mh_k)\n\t        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n", "        # Apply attention masking\n\t        if qk_mask is not None:\n\t            for head_idx in range(self.n_head):\n\t                attn_weights[head_idx] = attn_weights[head_idx] + qk_mask\n\t                # attn_weights[head_idx] = attn_weights[head_idx].masked_fill(torch.tril(torch.ones(5, 5)).t().view(1,5,1,5) == 0, float(\"-inf\"))#-1e4)\n\t        if k_mask is not None:\n\t            for head_idx in range(self.n_head):\n\t                attn_weights[head_idx] = attn_weights[head_idx] + k_mask\n\t        attn_weights = [aw.softmax(dim=1) for aw in attn_weights\n\t                        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n", "        mh_w = [self.dropout(aw) for aw in attn_weights\n\t                ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\t        # (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n\t        attn = [\n\t            torch.einsum('bkhq,bchk->bchq', wi, vi)\n\t            for wi, vi in zip(mh_w, mh_v)\n\t        ]  # n_head * (batch_size, d_v/n_head, 1, tgt_seq_len)\n\t        attn = torch.cat(attn, dim=2)  # (batch_size, d_v, 1, tgt_seq_len)\n\t        return attn\n\t    def _attention_fn(self, q, k, v, qk_mask, k_mask, return_weights):\n", "        \"\"\"Core routine for computing multi-head attention\n\t        Args:\n\t            q:              Projected query embeddings of shape (batch_size, d_qk, 1, tgt_seq_len)\n\t            k:              Projected key embeddings of shape (batch_size, d_qk, 1, src_seq_len)\n\t            v:              Projected value embeddings of shape (batch_size, d_v, 1, src_seq_len)\n\t            qk_mask:        Float tensor of shape (batch_size, src_seq_len, 1, tgt_seq_len).\n\t                            Indices with the a high negative value, e.g. -1e4, are excluded from attention\n\t            k_mask:         Float tensor of shape (batch_size, src_seq_len, 1, 1).\n\t                            Indices with the a high negative value, e.g. -1e4, are excluded from attention\n\t        Returns:\n", "            attn:           Attention embeddings of shape (batch_size, d_v, 1, tgt_seq_len)\n\t            attn_weights:   If `return_weights` is True, returns the softmax attention weights used to compute the attention matrix\n\t        \"\"\"\n\t        if ATTENTION_IMPLEMENTATION_IN_EFFECT == AttentionImplementations.EINSUM:\n\t            # Stolen/inspired from ml-stable-diffusion repo. No clue if it works,\n\t            # had a bug that turned it off by mistake so never tried.\n\t            bs = q.size(0)\n\t            dim_head = self.d_qk // self.n_head\n\t            mh_q = q.view(bs, self.n_head, dim_head, -1)\n\t            mh_k = k.view(bs, self.n_head, dim_head, -1)\n", "            mh_v = v.view(bs, self.n_head, dim_head, -1)\n\t            # print(\"qkv\", q.shape, k.shape, v.shape)\n\t            # print(\"mhqkv\", mh_q.shape, mh_k.shape, mh_v.shape)\n\t            attn_weights = torch.einsum(\"bhcq,bhck->bhqk\", [mh_q, mh_k]) # 1,64,12,20 @ 1,64,20,12 = 1,64,20,20\n\t            attn_weights.mul_(self.q_normalize_fact)\n\t            # print(\"attn,qk\", attn_weights.shape, qk_mask.shape, k_mask.shape if k_mask is not None else None)\n\t            if qk_mask is not None:\n\t                qk_mask = qk_mask.squeeze(2).unsqueeze(0)\n\t                attn_weights = attn_weights + qk_mask\n\t            if k_mask is not None:\n", "                k_mask = k_mask.squeeze(2).unsqueeze(0)\n\t                attn_weights = attn_weights + k_mask\n\t            attn_weights = attn_weights.softmax(dim=3)\n\t            attn = torch.einsum(\"bhqk,bhck->bhcq\", [attn_weights, mh_v])\n\t            attn = attn.contiguous().view(bs, self.d_out, 1, -1)\n\t        elif ATTENTION_IMPLEMENTATION_IN_EFFECT == AttentionImplementations.SPLIT_EINSUM:\n\t            # Do the scalar multiply before the matmul so it can be folded into the conv.\n\t            q = q * self.q_normalize_fact\n\t            # Principle 2: Chunking Large Intermediate Tensors  (machinelearning.apple.com/research/apple-neural-engine)\n\t            # Split q, k and v to compute a list of single-head attention functions\n", "            mh_q = q.split(\n\t                self.d_qk // self.n_head,\n\t                dim=1)  # n_head * (batch_size, d_qk/n_head, 1, tgt_seq_len)\n\t            # Principle 3: Minimizing Memory Copies\n\t            # Avoid as many transposes and reshapes as possible\n\t            mh_k = k.transpose(1, 3).split(\n\t                self.d_qk // self.n_head,\n\t                dim=3)  # n_head * (batch_size, src_seq_len, 1, d_qk/n_head)\n\t            mh_v = v.split(\n\t                self.d_v // self.n_head,\n", "                dim=1)  # n_head * (batch_size, d_v/n_head, 1, src_seq_len)\n\t            # `qk = q @ k`\n\t            attn_weights = [\n\t                torch.einsum('bchq,bkhc->bkhq', [qi, ki])\n\t                for qi, ki in zip(mh_q, mh_k)\n\t            ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\t            # Apply attention masking\n\t            if qk_mask is not None:\n\t                for head_idx in range(self.n_head):\n\t                    attn_weights[head_idx] = attn_weights[head_idx] + qk_mask\n", "            if k_mask is not None:\n\t                for head_idx in range(self.n_head):\n\t                    attn_weights[head_idx] = attn_weights[head_idx] + k_mask\n\t            # `w = F.softmax(qk.float(), dim=-1)`\n\t            attn_weights = [aw.softmax(dim=1) for aw in attn_weights\n\t                            ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\t            mh_w = [self.dropout(aw) for aw in attn_weights\n\t                    ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\t            # (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n\t            attn = [\n", "                torch.einsum('bkhq,bchk->bchq', wi, vi)\n\t                for wi, vi in zip(mh_w, mh_v)\n\t            ]  # n_head * (batch_size, d_v/n_head, 1, tgt_seq_len)\n\t            attn = torch.cat(attn, dim=1)  # (batch_size, d_v, 1, tgt_seq_len)\n\t        if return_weights:\n\t            return attn, attn_weights\n\t        return attn, None\n\t    def _forward_impl(\n\t        self,\n\t        q,\n", "        k,\n\t        v,\n\t        qpos=None,\n\t        kpos=None,\n\t        vpos=None,\n\t        qk_mask=None,\n\t        k_mask=None,\n\t        return_weights=True,\n\t    ):\n\t        \"\"\"\n", "        Args:\n\t            q:                  Query embeddings of shape (batch_size, embed_dim, 1, tgt_seq_len)\n\t            k:                  Key embeddings of shape (batch_size, embed_dim, 1, src_seq_len)\n\t            v:                  Value embeddings of shape (batch_size, embed_dim, 1, src_seq_len)\n\t            qpos:               Positional encodings for the query embeddings with same shape as `q`\n\t            kpos:               Positional encodings for the key embeddings with same shape as `k`\n\t            vpos:               Positional encodings for the key embeddings with same shape as `v`\n\t            qk_mask:            Float tensor with shape (batch_size, src_seq_len, 1, tgt_seq_len). Example use case: for causal masking\n\t                                in generative language models (e.g. GPT), fill the upper triangular part with a high negative value (e.g. -1e4).\n\t                                Indices with the a high negative value, e.g. -1e4, are excluded from attention\n", "            k_mask:             Float tensor with shape (batch_size, src_seq_len, 1, 1). Example use case: when excluding embeddings that\n\t                                correspond to zero-padded pixels in an image or unused tokens in a text token sequence from attention.\n\t                                Indices with the a high negative value, e.g. -1e4, are excluded from attention\n\t            return_weights:     If True, returns the intermediate attention weights\n\t        Note: If any of q,k,v has shape (batch_size, embed_dim, height, width) that represent a 2-d feature map, this will\n\t        be flattened to (batch_size, embed_dim, 1, height * width)\n\t        Note: `attn_weights` are never passed downstream even when return_weights=True because all the attn_weights\n\t        are harvested from the outermost module (e.g. ane_transformers.model#Transformer) by means of forward hooks\n\t        \"\"\"\n\t        # Parse tensor shapes for source and target sequences\n", "        assert len(q.size()) == 4 and len(k.size()) == 4 and len(v.size()) == 4, f\"q:{q.size()} k:{k.size()} v:{v.size()}\"\n\t        b, ct, ht, wt = q.size()\n\t        b, cs, hs, ws = k.size()\n\t        tgt_seq_len = ht * wt\n\t        src_seq_len = hs * ws\n\t        # Add positional encodings if any\n\t        if qpos is not None:\n\t            q = q + qpos\n\t        if kpos is not None:\n\t            k = k + kpos\n", "        if vpos is not None:\n\t            v = v + kpos\n\t        # Project q,k,v\n\t        q = self.q_proj(q)\n\t        k = self.k_proj(k)\n\t        v = self.v_proj(v)\n\t        # Validate qk_mask (`attn_mask` in `torch.nn.MultiheadAttention`)\n\t        expected_qk_mask_shape = [b, src_seq_len, 1, tgt_seq_len]\n\t        if qk_mask is not None:\n\t            if qk_mask.dtype != torch.float32:\n", "                raise RuntimeError(\n\t                    f\"`qk_mask` must be of type torch.float32, received {qk_mask.dtype}\"\n\t                )\n\t            if list(qk_mask.size()) != expected_qk_mask_shape:\n\t                raise RuntimeError(\n\t                    f\"Invalid shape for `qk_mask` (Expected {expected_qk_mask_shape}, got {list(qk_mask.size())}\"\n\t                )\n\t        # Validate k_mask (`key_padding_mask` in `torch.nn.MultiheadAttention`)\n\t        expected_k_mask_shape = [b, src_seq_len, 1, 1]\n\t        if k_mask is not None:\n", "            if k_mask.dtype != torch.float32:\n\t                raise RuntimeError(\n\t                    f\"`k_mask` must be of type torch.float32, received {k_mask.dtype}\"\n\t                )\n\t            if list(k_mask.size()) != expected_k_mask_shape:\n\t                raise RuntimeError(\n\t                    f\"Invalid shape for `k_mask` (Expected {expected_k_mask_shape}, got {list(k_mask.size())}\"\n\t                )\n\t        # Call the attention function\n\t        attn, attn_weights = self._attention_fn(q, k, v, qk_mask, k_mask,\n", "                                                return_weights)\n\t        # Revert to original dimension permutation\n\t        attn = attn.contiguous().view(b, self.d_v, ht, wt)\n\t        attn = self.out_proj(attn)\n\t        # if return_weights:\n\t        #     return attn, attn_weights\n\t        return attn #, None\n\t    def forward(self, q, k, v, **kwargs):\n\t        return self._forward_impl(q, k, v, **kwargs)\n\tclass ResidualMultiHeadAttention(MultiHeadAttention):\n", "    def __init__(self, embed_dim, dropout=0.1, drop_fn=nn.Dropout, **kwargs):\n\t        super().__init__(embed_dim, dropout=dropout, **kwargs)\n\t        self.drop_fn = drop_fn(dropout) if dropout > 0. else nn.Identity()\n\t        self.norm = LayerNormANE(embed_dim)\n\t    def forward(self, q, k, v, **kwargs):\n\t        attn, attn_weights = self._forward_impl(q, k, v, **kwargs)\n\t        return self.norm(self.drop_fn(attn) + q), attn_weights\n\tclass SelfAttention(MultiHeadAttention):\n\t    def forward(self, qkv, **kwargs):\n\t        return super()._forward_impl(qkv, qkv, qkv, **kwargs)\n", "class ResidualSelfAttention(ResidualMultiHeadAttention):\n\t    def forward(self, qkv, **kwargs):\n\t        attn, attn_weights = self._forward_impl(qkv, qkv, qkv, **kwargs)\n\t        return self.norm(self.drop_fn(attn) + qkv), attn_weights\n\tclass PreNormResidualSelfAttention(ResidualSelfAttention):\n\t    def forward(self, qkv, **kwargs):\n\t        norm_qkv = self.norm(qkv)\n\t        attn, attn_weights = self._forward_impl(norm_qkv, norm_qkv, norm_qkv,\n\t                                                **kwargs)\n\t        result = self.drop_fn(attn) + qkv\n", "        return result, attn_weights"]}
{"filename": "src/experiments/masks_comparison.py", "chunked_list": ["import torch\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n\timport coremltools as ct\n\timport numpy as np\n\tfrom datetime import datetime\n\tfrom src.utils.psnr import compute_psnr\n\tfrom src.ml_ane_transformers.ane_gpt2 import GPT as ANEGPT\n\t\"\"\"\n\tExperiment to see if we need k, qk or both when we only care about the\n\tnext token prediction.\n", "PSNR between just qk + both is very high, so seems that k is unneeded.\n\t\"\"\"\n\tane = ANEGPT.from_pretrained(\"gpt2\").eval()\n\trandom_tokens = torch.randint(10000, (1,10,))\n\tinputs_dict = ane.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n\tinput_ids, qk_mask, k_mask, output_mask = [inputs_dict[k] for k in\\\n\t                                            [\"input_ids\", \"qk_mask\", \"k_mask\", \"output_mask\"]]\n\tassert output_mask[0] == 9, f\"{output_mask} is not as expected\"\n\twith torch.no_grad():\n\t    k_only = ane(input_ids, qk_mask=None, k_mask=k_mask, output_mask=output_mask)\n", "    qk_only = ane(input_ids, qk_mask=qk_mask, k_mask=None, output_mask=output_mask)\n\t    both = ane(input_ids, qk_mask=qk_mask, k_mask=k_mask, output_mask=output_mask)\n\tassert k_only.shape == qk_only.shape\n\tassert k_only.shape == both.shape\n\tprint(\"output shape:\", both.shape)\n\tprint(\"k v. qk psnr:\", compute_psnr(k_only, qk_only))\n\tprint(\"k v. both psnr:\", compute_psnr(k_only, both))\n\tprint(\"qk v. both psnr:\", compute_psnr(qk_only, both))\n"]}
{"filename": "src/experiments/replace_row_model.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport coremltools as ct\n\timport numpy as np\n\t\"\"\"\n\tTrying to find a way to replace a row in a matrix that can run on the Neural Engine.\n\t\"\"\"\n\t# new_q, new_k, new_v = q,k,v # new_q, new_k, new_v = [B, 1, n_embd]\n\t# old_k, old_v = kv_cache.chunk(2, dim=1) # each (B, T, C)\n\tclass Net(nn.Module):\n", "    def forward(self, new_k, old_k, mask):\n\t        # new_k [B, 1, n_embd]\n\t        # old_k [B, T, n_mbd]\n\t        # insert_at [1]\n\t        # mask = torch.zeros_like(old_k)\n\t        # mask[:, insert_at, :] = 1 # Using a symbol here fails an assert, the first : is also not cool.\n\t        # ValueError: Type mismatch <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'> vs. <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n\t        # mask = torch.zeros_like(old_k)\n\t        # new_mask = torch.ones_like(new_k)\n\t        # mask = torch.cat([mask[:, :insert_at, :], new_mask, mask[:, insert_at+1:, :]], dim=1)\n", "        # mask = torch.zeros_like(old_k)\n\t        # new_mask = torch.ones_like(new_k)\n\t        # mask = torch.cat([mask[:, :2, :], new_mask, mask[:, 2+1:, :]], dim=1)\n\t        return torch.where(mask.bool(), new_k, old_k)\n\tnet = Net().eval()\n\tnew_k = torch.tensor([[[99.,98,97]]])\n\told_k = torch.tensor([[[0.,1,2],[3,4,5],[6,7,8],[9,10,11]]])\n\tinsert_at = torch.tensor([2])\n\tmask = torch.zeros_like(old_k)\n\tmask[:, insert_at, :] = 1\n", "print(mask.shape)\n\tresult = net(new_k, old_k, mask)\n\tprint(result)\n\t# assert torch.equal(result[0, 2, :], new_k.squeeze())\n\ttraced = torch.jit.trace(net, (new_k, old_k, mask))\n\tprog = ct.convert(traced,\n\t    inputs=[\n\t        ct.TensorType(name=\"new_k\", shape=new_k.shape, dtype=np.float16),\n\t        ct.TensorType(name=\"old_k\", shape=old_k.shape, dtype=np.float16),\n\t        # ct.TensorType(name=\"insert_at\", shape=insert_at.shape, dtype=np.int32),\n", "        ct.TensorType(name=\"mask\", shape=insert_at.shape, dtype=np.float16),\n\t    ],\n\t    outputs=[\n\t        ct.TensorType(name=\"result\", dtype=np.float16),\n\t    ],\n\t    minimum_deployment_target=ct.target.iOS16, # float16 for less casting\n\t    convert_to=\"milinternal\")\n\tprint(prog)\n\tmlmodel = ct.convert(prog,\n\t    minimum_deployment_target=ct.target.iOS16, # float16 for less casting\n", "    convert_to=\"mlprogram\")\n\tmlmodel.save(\"replace-row-model.mlpackage\")"]}
{"filename": "src/experiments/embedding_conv.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport numpy as np\n\t\"\"\"\n\tFigure if/how to replace embedding + lm head layers with conv so we can\n\tget to the preferred ANE shape earlier/faster.\n\tResult is the embedding is definitely not possible and the split einsum a la\n\tWhisper works well enough.\n\t\"\"\"\n\t# input BT\n", "# gather weight TC\n\t# gather output BSC\n\t# block input (ANE optimal) BC1S\n\t# output from blocks: BC1S\n\t# ...\n\t# desired output BST\n\tB,C,S,T = 1, 312, 512, 2000\n\t# B,C,S,T = 1, 4, 512, 2000\n\t# B,C,S,T = 1, 4, 6, 10\n\tassert C % 2 == 0\n", "assert T % 2 == 0\n\ttorch.manual_seed(42)\n\tconv = nn.Conv2d(C, T, 1, bias=False) # lm_head\n\temb = nn.Embedding(T, C)\n\temb.weight = nn.Parameter(conv.weight.squeeze())\n\tlin = nn.Linear(C,T, bias=False)\n\tlin.weight = nn.Parameter(conv.weight.squeeze())\n\tx = torch.randn((B, S, C))\n\tlin_res = lin(x)\n\tx = x.transpose(1, 2).unsqueeze(2)\n", "assert x.shape == torch.Size([B,C,1,S])\n\t# print(\"lin_res\", lin_res)\n\t# conv_res = conv(x)\n\t# print(\"conv_res.shape\", conv_res.shape)\n\t# conv_res = conv_res.permute(0,2,3,1).squeeze(0)\n\t# print(\"conv_res\", conv_res)\n\t# print(\"lin vs conv\", torch.equal(lin_res, conv_res))\n\t# print(lin_res - conv_res)\n\t# conv2_res = conv(x.permute(3,1,0,2)).squeeze().unsqueeze(0)\n\t# print(\"conv_res2\", conv_res)\n", "# print(\"lin vs conv2\", torch.equal(lin_res, conv2_res))\n\t# print(lin_res - conv_res)\n\tassert x.shape == torch.Size([B,C,1,S])\n\tx = x.permute(0,2,3,1).squeeze(0)\n\tassert x.shape == torch.Size([B,S,C])\n\t# ANE can only load tensors with dim size of at most 16,384 - divide the vocab size to be less than that\n\t# gpt2 vocab size is a product of two primes smh\n\tprint(lin.weight.shape)\n\tsplits = emb.weight.split(emb.weight.shape[0]//2, dim=0)\n\tecat = torch.cat([torch.einsum('bid,jd->bij', x, split) for split in splits], dim=-1)#.view(*x.shape[:2], -1)\n", "print(ecat.shape)\n\t# ecat = torch.einsum('bid,jd->bij', x, emb.weight)\n\tprint(ecat.shape)\n\tprint(lin_res - ecat)\n\tprint(torch.allclose(lin_res, ecat, atol=1e-6))\n\t# @torch.no_grad()\n\t# def naive_way(x):\n\t#     assert x.shape == torch.Size([B,C,1,S])\n\t#     x = x.permute(3, 1, 0, 2) # shape = (S, C, B, 1)\n\t#     logits = conv(x) # shape = (S, T, B, 1)\n", "#     logits = logits.squeeze().unsqueeze(0) # shape = (B, S, T)\n\t#     return logits\n\t# # no faster than naive way\n\t# @torch.no_grad()\n\t# def post_permute_way(x):\n\t#     assert x.shape == torch.Size([B,C,1,S])\n\t#     logits = conv(x) # shape = (B, T, 1, S)\n\t#     assert logits.shape == torch.Size([B,T,1,S])\n\t#     logits = logits.permute(0,2,3,1).squeeze(1) # shape = (B, S, T)\n\t#      # .permute(0,3,1,2).squeeze(-1)\n", "#     return logits\n\t# x = torch.randn((B, C, 1, S))\n\t# print(\"naive\", naive_way(x).shape)\n\t# print(\"post_permute\", post_permute_way(x).shape)\n\t# # print(naive_way(x))\n\t# # print(post_permute_way(x))\n\t# print(torch.allclose(naive_way(x), post_permute_way(x), atol=1e-6))\n\t# print(\"default tol:\", torch.allclose(naive_way(x), post_permute_way(x)))\n\t# # @torch.no_grad()\n\t# # def emb_way(x):\n", "# #     assert x.shape == torch.Size([B,S])\n\t# #     return emb(x)\n\t# # @torch.no_grad()\n\t# # def conv_emb_way(x):\n\t# #     assert x.shape == torch.Size([B,S])\n\t# #     return conv(x)\n\t# # x = torch.randint(2000, (B, S))\n\t# # print(\"emb\", emb_way(x).shape)\n\t# # print(\"conv emb\", conv_emb_way(x).shape)\n\t# # print(torch.allclose(emb_way(x), conv_emb_way(x), atol=1e-6))\n"]}
{"filename": "src/experiments/chaining_models.py", "chunked_list": ["import torch\n\timport coremltools as ct\n\timport numpy as np\n\timport argparse\n\tfrom stopwatch import Stopwatch\n\tfrom collections import OrderedDict\n\timport time\n\t\"\"\"\n\tTry loading multiple models at the same time and then predict on\n\tthem in sequence.\n", "\"\"\"\n\tcompute_unit_by_name = OrderedDict([\n\t    (\"All\", ct.ComputeUnit.ALL),\n\t    (\"CPUOnly\", ct.ComputeUnit.CPU_ONLY),\n\t    (\"CPUAndGPU\", ct.ComputeUnit.CPU_AND_GPU),\n\t    (\"CPUAndANE\", ct.ComputeUnit.CPU_AND_NE),\n\t])\n\tall_models = [\n\t    # \"test-net-22-loops.mlpackage\",\n\t    # \"test-net-22-loops-2.mlpackage\",\n", "    \"test-net-21-loops.mlpackage\",\n\t    \"test-net-21-loops-2.mlpackage\",\n\t    \"test-net-20-loops.mlpackage\",\n\t    \"test-net-20-loops-2.mlpackage\",\n\t    \"test-net-20-loops-3.mlpackage\",\n\t    \"test-net-20-loops-4.mlpackage\",\n\t]\n\tparser = argparse.ArgumentParser(description='Load many models and predict on them in sequence.')\n\tparser.add_argument('total_models', help='total models', type=int, choices=[x+1 for x in range(len(all_models))])\n\tparser.add_argument('compute_unit', help='compute unit', type=str, choices=list(compute_unit_by_name.keys()), default=\"All\")\n", "args = parser.parse_args()\n\tcompute_unit = compute_unit_by_name[args.compute_unit]\n\tprint(\"Compute Unit:\", args.compute_unit)\n\tmodel_paths = all_models[:args.total_models]\n\tprint(f\"Running with {len(model_paths)} models.\")\n\tinput_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n\tmodels = []\n\tload_times = []\n\tinitial_predict_times = []\n\tgroup_predict_times = []\n", "# Load each model and predict on it once.\n\tfor p in model_paths:\n\t    print(f\"Loading {p}\")\n\t    load_sw = Stopwatch(2)\n\t    mlmodel = ct.models.MLModel(p, compute_units=compute_unit)\n\t    load_sw.stop()\n\t    load_times.append(load_sw.duration)\n\t    models.append(mlmodel)\n\t    inital_pred_sw = Stopwatch(2)\n\t    mlmodel.predict({\"input_ids\": input_ids})\n", "    inital_pred_sw.stop()\n\t    initial_predict_times.append(inital_pred_sw.duration)\n\tprint(\"Sleeping...\")\n\ttime.sleep(5)\n\tprint(\"Warm up group predict...\")\n\tfor m in models:\n\t    m.predict({\"input_ids\": input_ids})\n\tfor m in models:\n\t    m.predict({\"input_ids\": input_ids})\n\tprint(\"Predicting back to back.\")\n", "group_predict_sw = Stopwatch(2)\n\tfor m in models:\n\t    m.predict({\"input_ids\": input_ids})\n\tgroup_predict_sw.stop()\n\tprint(f\"{(np.average(load_times) * 1000):.2f}ms avg load time ({load_times})\")\n\tprint(f\"{(np.average(initial_predict_times) * 1000):.2f}ms avg initial predict ({initial_predict_times})\")\n\tprint(f\"{(group_predict_sw.duration * 1000):.2f}ms total group predict (avg: {1000*group_predict_sw.duration / float(len(models)):.2f}ms)\")\n"]}
{"filename": "src/experiments/chunk_model.py", "chunked_list": ["#\n\t# For licensing see accompanying LICENSE.md file.\n\t# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\t#\n\t\"\"\"\n\tBased on Apple's ml-stable-diffusion script to split the unet model into chunks,\n\tbut adapted to support splitting models into >2 chunks.\n\tOriginal License:\n\tCopyright (C) 2022 Apple Inc. All Rights Reserved.\n\tIMPORTANT: This Apple software is supplied to you by Apple Inc. (\"Apple\") in consideration of your agreement to the following terms, and your use, installation, modification or redistribution of this Apple software constitutes acceptance of these terms. If you do not agree with these terms, please do not use, install, modify or redistribute this Apple software.\n", "In consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple's copyrights in this original Apple software (the \"Apple Software\"), to use, reproduce, modify and redistribute the Apple Software, with or without modifications, in source and/or binary forms; provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly stated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated.\n\tThe Apple Software is provided by Apple on an \"AS IS\" basis. APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\tIN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\t\"\"\"\n\timport argparse\n\tfrom collections import OrderedDict\n\timport coremltools as ct\n\tfrom coremltools.converters.mil import Block, Program, Var\n\tfrom coremltools.converters.mil.frontend.milproto.load import load as _milproto_to_pymil\n\tfrom coremltools.converters.mil.mil import Builder as mb\n", "from coremltools.converters.mil.mil import Placeholder\n\tfrom coremltools.converters.mil.mil import types as types\n\tfrom coremltools.converters.mil.mil.passes.helper import block_context_manager\n\tfrom coremltools.converters.mil.mil.passes.pass_registry import PASS_REGISTRY\n\t# from coremltools.converters.mil.testing_utils import random_gen_input_feature_type\n\timport gc\n\timport logging\n\tlogging.basicConfig()\n\tlogger = logging.getLogger(__name__)\n\tlogger.setLevel(logging.INFO)\n", "import numpy as np\n\timport os\n\timport shutil\n\timport time\n\tdef _verify_output_correctness_of_chunks(full_model, first_chunk_model,\n\t                                         second_chunk_model):\n\t    \"\"\" Verifies the end-to-end output correctness of full (original) model versus chunked models\n\t    \"\"\"\n\t    # Generate inputs for first chunk and full model\n\t    # input_dict = {}\n", "    # for input_desc in full_model._spec.description.input:\n\t    #     input_dict[input_desc.name] = random_gen_input_feature_type(input_desc)\n\t    # # Generate outputs for first chunk and full model\n\t    # outputs_from_full_model = full_model.predict(input_dict)\n\t    # outputs_from_first_chunk_model = first_chunk_model.predict(input_dict)\n\t    # # Prepare inputs for second chunk model from first chunk's outputs and regular inputs\n\t    # second_chunk_input_dict = {}\n\t    # for input_desc in second_chunk_model._spec.description.input:\n\t    #     if input_desc.name in outputs_from_first_chunk_model:\n\t    #         second_chunk_input_dict[\n", "    #             input_desc.name] = outputs_from_first_chunk_model[\n\t    #                 input_desc.name]\n\t    #     else:\n\t    #         second_chunk_input_dict[input_desc.name] = input_dict[\n\t    #             input_desc.name]\n\t    # # Generate output for second chunk model\n\t    # outputs_from_second_chunk_model = second_chunk_model.predict(\n\t    #     second_chunk_input_dict)\n\t    # Verify correctness across all outputs from second chunk and full model\n\t    print(\"Skipping output correctness check.\")\n", "    # for out_name in outputs_from_full_model.keys():\n\t    #     torch2coreml.report_correctness(\n\t    #         original_outputs=outputs_from_full_model[out_name],\n\t    #         final_outputs=outputs_from_second_chunk_model[out_name],\n\t    #         log_prefix=f\"{out_name}\")\n\tdef _load_prog_from_mlmodel(model):\n\t    \"\"\" Load MIL Program from an MLModel\n\t    \"\"\"\n\t    model_spec = model.get_spec()\n\t    start_ = time.time()\n", "    logger.info(\n\t        \"Loading MLModel object into a MIL Program object (including the weights)..\"\n\t    )\n\t    prog = _milproto_to_pymil(\n\t        model_spec=model_spec,\n\t        specification_version=model_spec.specificationVersion,\n\t        file_weights_dir=model.weights_dir,\n\t    )\n\t    logger.info(f\"Program loaded in {time.time() - start_:.1f} seconds\")\n\t    return prog\n", "class Chunk():\n\t    def __init__(self, start_op_idx, end_op_idx, cumulative_size_in_mb):\n\t        self.start_op_idx = start_op_idx\n\t        self.end_op_idx = end_op_idx\n\t        self.cumulative_size_in_mb = cumulative_size_in_mb\n\t    def save(self, prog, name):\n\t        mlmodel = ct.convert(\n\t            prog,\n\t            convert_to=\"mlprogram\",\n\t            compute_units=ct.ComputeUnit.CPU_ONLY,\n", "            minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n\t        )\n\t        mlmodel.save(name)\n\t    def __repr__(self):\n\t        return f\"<Chunk start={self.start_op_idx} end={self.end_op_idx} cumulative_size={self.cumulative_size_in_mb}MB>\"\n\tdef _get_op_idx_split_locations(prog: Program):\n\t    \"\"\" Find the op that approximately bisects the graph as measure by weights size on each side\n\t    \"\"\"\n\t    main_block = prog.functions[\"main\"]\n\t    total_size_in_mb = 0\n", "    for op in main_block.operations:\n\t        if op.op_type == \"const\" and isinstance(op.val.val, np.ndarray):\n\t            size_in_mb = op.val.val.size * op.val.val.itemsize / (1024 * 1024)\n\t            total_size_in_mb += size_in_mb\n\t    if total_size_in_mb < 1800:\n\t        print(\"You /may/ not need to split this model in order to get it to run on the Neural Engine.\")\n\t    chunk_size = 1800 # Under XXMB. 2400 too high. 1800 works for gpt2-xl.\n\t    # 670 for 8 chunks of 2.8b\n\t    next_split_size = chunk_size\n\t    print(f\"Total size: {total_size_in_mb}MB\")\n", "    print(f\"Target split size: {chunk_size}MB\")\n\t    # Find the first non const op (single child), where the total cumulative size exceeds\n\t    # the half size for the first time\n\t    chunks = []\n\t    cumulative_size_in_mb = 0\n\t    for op in main_block.operations:\n\t        if op.op_type == \"const\" and isinstance(op.val.val, np.ndarray):\n\t            size_in_mb = op.val.val.size * op.val.val.itemsize / (1024 * 1024)\n\t            cumulative_size_in_mb += size_in_mb\n\t        # TODO: Maybe reverse this order?\n", "        if (cumulative_size_in_mb > next_split_size and op.op_type != \"const\"\n\t                and len(op.outputs) == 1\n\t                and len(op.outputs[0].child_ops) == 1):\n\t            op_idx = main_block.operations.index(op)\n\t            start_op_idx = 0 if len(chunks) == 0 else chunks[-1].end_op_idx\n\t            chunks.append(Chunk(start_op_idx, op_idx, cumulative_size_in_mb))\n\t            next_split_size = cumulative_size_in_mb + chunk_size\n\t            logger.info(f\"current_size = {cumulative_size_in_mb}MB next split size = {next_split_size}MB\")\n\t            # return op_idx, cumulative_size_in_mb, total_size_in_mb\n\t    # TODO: Handle more gracefully.\n", "    op_idx = main_block.operations.index(main_block.operations[-1])\n\t    start_op_idx = 0 if len(chunks) == 0 else chunks[-1].end_op_idx\n\t    chunks.append(Chunk(start_op_idx, op_idx, cumulative_size_in_mb))\n\t    next_split_size = cumulative_size_in_mb + chunk_size\n\t    logger.info(f\"current_size = {cumulative_size_in_mb}MB next split size = {next_split_size}MB\")\n\t    return chunks\n\tdef _get_first_chunk_outputs(block, start_op_idx, op_idx):\n\t    # Get the list of all vars that go across from first program (all ops from 0 to op_idx (inclusive))\n\t    # to the second program (all ops from op_idx+1 till the end). These all vars need to be made the output\n\t    # of the first program and the input of the second program\n", "    boundary_vars = set()\n\t    for i in range(start_op_idx, op_idx + 1):\n\t        op = block.operations[i]\n\t        for var in op.outputs:\n\t            if var.val is None:  # only consider non const vars\n\t                for child_op in var.child_ops:\n\t                    child_op_idx = block.operations.index(child_op)\n\t                    if child_op_idx > op_idx:\n\t                        boundary_vars.add(var)\n\t    return list(boundary_vars)\n", "@block_context_manager\n\tdef _add_fp32_casts(block, boundary_vars):\n\t    new_boundary_vars = []\n\t    for var in boundary_vars:\n\t        if var.dtype != types.fp16:\n\t            new_boundary_vars.append(var)\n\t        else:\n\t            fp32_var = mb.cast(x=var, dtype=\"fp32\", name=var.name)\n\t            new_boundary_vars.append(fp32_var)\n\t    return new_boundary_vars\n", "def _make_first_chunk_prog(prog, op_idx):\n\t    \"\"\" Build first chunk by declaring early outputs and removing unused subgraph\n\t    \"\"\"\n\t    block = prog.functions[\"main\"]\n\t    boundary_vars = _get_first_chunk_outputs(block, 0, op_idx)\n\t    # Due to possible numerical issues, cast any fp16 var to fp32\n\t    new_boundary_vars = _add_fp32_casts(block, boundary_vars)\n\t    block.outputs.clear()\n\t    block.set_outputs(new_boundary_vars)\n\t    PASS_REGISTRY[\"common::dead_code_elimination\"](prog)\n", "    return prog\n\tdef _make_second_chunk_prog(prog, previous_start_op_idx, start_op_idx, end_op_idx, is_last=False):\n\t    \"\"\" Build second chunk by rebuilding a pristine MIL Program from MLModel\n\t    \"\"\"\n\t    block = prog.functions[\"main\"]\n\t    block.opset_version = ct.target.iOS16\n\t    # First chunk outputs are second chunk inputs (e.g. skip connections)\n\t    boundary_vars = _get_first_chunk_outputs(block, 0, start_op_idx)\n\t    output_boundary_vars = _get_first_chunk_outputs(block, start_op_idx, end_op_idx)\n\t    if not is_last:\n", "        new_boundary_vars = _add_fp32_casts(block, output_boundary_vars)\n\t        block.outputs.clear()\n\t        block.set_outputs(new_boundary_vars)\n\t    # This op will not be included in this program. Its output var will be made into an input\n\t    boundary_op = block.operations[start_op_idx]\n\t    logger.info(f\"boundary vars (inputs) {previous_start_op_idx}-{start_op_idx} {[v.name for v in boundary_vars]}\")\n\t    logger.info(f\"boundary op: {boundary_op}\".rstrip())\n\t    logger.info(f\"boundary vars (outputs) {start_op_idx}-{end_op_idx} {[v.name for v in output_boundary_vars]}\")\n\t    # Add all boundary ops as inputs\n\t    with block:\n", "        for var in boundary_vars:\n\t            new_placeholder = Placeholder(\n\t                sym_shape=var.shape,\n\t                dtype=var.dtype if var.dtype != types.fp16 else types.fp32,\n\t                name=var.name,\n\t            )\n\t            # logger.info(f\"New placeholder: {var.name}\")\n\t            block._input_dict[\n\t                new_placeholder.outputs[0].name] = new_placeholder.outputs[0]\n\t            block.function_inputs = tuple(block._input_dict.values())\n", "            new_var = None\n\t            if var.dtype == types.fp16:\n\t                new_var = mb.cast(x=new_placeholder.outputs[0],\n\t                                  dtype=\"fp16\",\n\t                                  before_op=var.op)\n\t            else:\n\t                new_var = new_placeholder.outputs[0]\n\t            # logger.info(f\"Replacing {var.name} with {new_var.name}\")\n\t            block.replace_uses_of_var_after_op(\n\t                anchor_op=boundary_op,\n", "                old_var=var,\n\t                new_var=new_var,\n\t                # force_replace=True # For quantized models.\n\t            )\n\t    PASS_REGISTRY[\"common::dead_code_elimination\"](prog)\n\t    # Remove any unused inputs\n\t    new_input_dict = OrderedDict()\n\t    for k, v in block._input_dict.items():\n\t        if len(v.child_ops) > 0:\n\t            new_input_dict[k] = v\n", "    block._input_dict = new_input_dict\n\t    block.function_inputs = tuple(block._input_dict.values())\n\t    logger.info(f\"final inputs: {[v.name for v in block.function_inputs]}\")\n\t    return prog\n\tdef save_chunk(prog_chunk, filename):\n\t    model_chunk = ct.convert(\n\t        prog_chunk,\n\t        convert_to=\"mlprogram\",\n\t        compute_units=ct.ComputeUnit.CPU_ONLY,\n\t        minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n", "    )\n\t    model_chunk.save(filename)\n\tdef main(args):\n\t    os.makedirs(args.o, exist_ok=True)\n\t    # Check filename extension\n\t    mlpackage_name = os.path.basename(args.mlpackage_path)\n\t    name, ext = os.path.splitext(mlpackage_name)\n\t    assert ext == \".mlpackage\", f\"`--mlpackage-path` (args.mlpackage_path) is not an .mlpackage file\"\n\t    # Load CoreML model\n\t    logger.info(\"Loading model from {}\".format(args.mlpackage_path))\n", "    start_ = time.time()\n\t    model = ct.models.MLModel(\n\t        args.mlpackage_path,\n\t        compute_units=ct.ComputeUnit.CPU_ONLY,\n\t    )\n\t    logger.info(\n\t        f\"Loading {args.mlpackage_path} took {time.time() - start_:.1f} seconds\"\n\t    )\n\t    # Load the MIL Program from MLModel\n\t    prog = _load_prog_from_mlmodel(model)\n", "    logger.info(f\"Total ops: {len(prog.functions['main'].operations)}\")\n\t    chunks: list[Chunk] = _get_op_idx_split_locations(prog)\n\t    logger.info(f\"{args.mlpackage_path} will chunked into {len(chunks)} pieces.\")\n\t    for c in chunks:\n\t        logger.info(c)\n\t    # From start to first chunk op idx.\n\t    # prog_chunk1 = _make_first_chunk_prog(prog, chunks[0].end_op_idx)\n\t    # logger.info(f\"Converting and saving first chunk up to op {chunks[0].end_op_idx}\")\n\t    # filename = f\"{name}_chunk1.mlpackage\"\n\t    # chunks[0].save(prog_chunk1, filename)\n", "    # logger.info(f\"Saved first chunk {filename} of {len(chunks)} total\")\n\t    # del prog_chunk1\n\t    # gc.collect()\n\t    prev_start_index = 0\n\t    for idx, chunk in [(i+1, c) for i,c in enumerate(chunks)]:\n\t        filename = f\"{name}_chunk{idx}.mlpackage\"\n\t        full_prog = _load_prog_from_mlmodel(model)\n\t        logger.info(f\"Converting and saving chunk #{idx} from op {chunk.start_op_idx}-{chunk.end_op_idx}\")\n\t        is_last = chunk.end_op_idx == chunks[-1].end_op_idx\n\t        prog_chunkn = _make_second_chunk_prog(full_prog, prev_start_index, chunk.start_op_idx, chunk.end_op_idx, is_last=is_last)\n", "        chunk.save(prog_chunkn, filename)\n\t        del full_prog\n\t        del prog_chunkn\n\t        gc.collect()\n\t        logger.info(f\"Saved chunk {filename} of {len(chunks)} total\")\n\t        prev_start_index = chunk.start_op_idx\n\t    # Compute the incision point by bisecting the program based on weights size\n\t    # op_idx, first_chunk_weights_size, total_weights_size = _get_op_idx_split_location(\n\t    #     prog)\n\t    # main_block = prog.functions[\"main\"]\n", "    # incision_op = main_block.operations[op_idx]\n\t    # logger.info(f\"{args.mlpackage_path} will chunked into two pieces.\")\n\t    # logger.info(\n\t    #     f\"The incision op: name={incision_op.name}, type={incision_op.op_type}, index={op_idx}/{len(main_block.operations)}\"\n\t    # )\n\t    # logger.info(f\"First  chunk size = {first_chunk_weights_size:.2f} MB\")\n\t    # logger.info(\n\t    #     f\"Second chunk size = {total_weights_size - first_chunk_weights_size:.2f} MB\"\n\t    # )\n\t    # # Build first chunk (in-place modifies prog by declaring early exits and removing unused subgraph)\n", "    # prog_chunk1 = _make_first_chunk_prog(prog, op_idx)\n\t    # # Build the second chunk\n\t    # prog_chunk2 = _make_second_chunk_prog(_load_prog_from_mlmodel(model),\n\t    #                                       op_idx)\n\t    # if not args.check_output_correctness:\n\t    #     # Original model no longer needed in memory\n\t    #     del model\n\t    #     gc.collect()\n\t    # # Convert the MIL Program objects into MLModels\n\t    # logger.info(\"Converting the two programs\")\n", "    # model_chunk1 = ct.convert(\n\t    #     prog_chunk1,\n\t    #     convert_to=\"mlprogram\",\n\t    #     compute_units=ct.ComputeUnit.CPU_ONLY,\n\t    #     minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n\t    # )\n\t    # del prog_chunk1\n\t    # gc.collect()\n\t    # logger.info(\"Conversion of first chunk done.\")\n\t    # model_chunk2 = ct.convert(\n", "    #     prog_chunk2,\n\t    #     convert_to=\"mlprogram\",\n\t    #     compute_units=ct.ComputeUnit.CPU_ONLY,\n\t    #     minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n\t    # )\n\t    # del prog_chunk2\n\t    # gc.collect()\n\t    # logger.info(\"Conversion of second chunk done.\")\n\t    # # Verify output correctness\n\t    # if args.check_output_correctness:\n", "    #     logger.info(\"Verifying output correctness of chunks\")\n\t    #     _verify_output_correctness_of_chunks(\n\t    #         full_model=model,\n\t    #         first_chunk_model=model_chunk1,\n\t    #         second_chunk_model=model_chunk2,\n\t    #     )\n\t    # # Remove original (non-chunked) model if requested\n\t    # if args.remove_original:\n\t    #     logger.info(\n\t    #         \"Removing original (non-chunked) model at {args.mlpackage_path}\")\n", "    #     shutil.rmtree(args.mlpackage_path)\n\t    #     logger.info(\"Done.\")\n\t    # # Save the chunked models to disk\n\t    # out_path_chunk1 = os.path.join(args.o, name + \"_chunk1.mlpackage\")\n\t    # out_path_chunk2 = os.path.join(args.o, name + \"_chunk2.mlpackage\")\n\t    # logger.info(\n\t    #     f\"Saved chunks in {args.o} with the suffix _chunk1.mlpackage and _chunk2.mlpackage\"\n\t    # )\n\t    # model_chunk1.save(out_path_chunk1)\n\t    # model_chunk2.save(out_path_chunk2)\n", "    logger.info(\"Done.\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--mlpackage-path\",\n\t        required=True,\n\t        help=\n\t        \"Path to the mlpackage file to be split into two mlpackages of approximately same file size.\",\n\t    )\n\t    parser.add_argument(\n", "        \"-o\",\n\t        required=True,\n\t        help=\n\t        \"Path to output directory where the two model chunks should be saved.\",\n\t    )\n\t    parser.add_argument(\n\t        \"--remove-original\",\n\t        action=\"store_true\",\n\t        help=\n\t        \"If specified, removes the original (non-chunked) model to avoid duplicating storage.\"\n", "    )\n\t    parser.add_argument(\n\t        \"--check-output-correctness\",\n\t        action=\"store_true\",\n\t        help=\n\t        (\"If specified, compares the outputs of original Core ML model with that of pipelined CoreML model chunks and reports PSNR in dB. \",\n\t         \"Enabling this feature uses more memory. Disable it if your machine runs out of memory.\"\n\t         ))\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "src/experiments/kahan_layer_norm.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport numpy as np\n\tfrom src.ml_ane_transformers.ane.layer_norm import LayerNormANE as LayerNorm\n\tfrom src.ml_ane_transformers.ane.kahan_layer_norm import KahanLayerNormANE as KahanLayerNorm\n\timport coremltools as ct\n\tfrom src.utils.psnr import compute_psnr\n\tfrom coremltools.converters.mil import Builder as mb\n\timport sys\n\t\"\"\"\n", "Compare and test a Kahan summation implementation of layer norm vs.\n\tthe default ANE-optimized one.\n\tSeems that for some reason my Kahan implementation is slightly less accurate.\n\tEnded up finding that the model builder layer_norm can take a 4D input and do\n\tthe norm on the channel dimension. Unfortunately doesn't work on the Neural Engine afaict.\n\t\"\"\"\n\ttorch.manual_seed(42)\n\tB,C,S = 1, 1024, 512\n\t# B,C,S = 1, 6, 1\n\t# B,C,S = 1,3,1\n", "# B,C,S = 1,3,2\n\t# x = torch.FloatTensor(B,C,1,S).uniform_(torch.finfo(torch.half).min*0.9, torch.finfo(torch.half).max*0.9)\n\tx = torch.randn((B,C,1,S), dtype=torch.float16).float().cpu()\n\t# x = torch.tensor([[[[10000.0]], [[3.14159]], [[2.71828]]]], dtype=torch.float16).float().cpu()\n\t# x = torch.tensor([[[[10000.0, 2.71828]], [[3.14159, 10000.0]], [[2.71828, 3.14159]]]], dtype=torch.float16).float().cpu()\n\t# print(x.shape, x.to(\"mps\").half().cumsum(dim=1))\n\t# Ignore learnable params.\n\tclip_mag = None#1e7\n\tln = LayerNorm(C, clip_mag=clip_mag, elementwise_affine=False)\n\tkln = KahanLayerNorm(C, clip_mag=clip_mag, elementwise_affine=False)\n", "nnln = nn.LayerNorm(C, elementwise_affine=False)\n\tdef print_stats(normal, kahan):\n\t    assert normal.shape == kahan.shape\n\t    print(\"all close?\", torch.allclose(normal, kahan))\n\t    print(\"equal?\", torch.equal(normal, kahan))\n\t    print(\"mean diff\", torch.mean(normal - kahan))\n\t    print(\"max diff\", torch.max(torch.abs(normal - kahan)))\n\t    # print(\"psnr\", compute_psnr(normal, kahan))\n\t    print(\"psnr\", compute_psnr(kahan, normal))\n\t    print(\"num close:\", torch.sum(torch.isclose(normal, kahan)))\n", "with torch.no_grad():\n\t    km = kln.kahan_mean(x.to(\"mps\").half(), 4).float().cpu()\n\t    hm = x.to(\"mps\").half().mean(dim=1, keepdim=True).float().cpu()\n\t    m = x.to(\"mps\").float().mean(dim=1, keepdim=True).float().cpu()\n\t    dm = x.double().mean(dim=1, keepdim=True)\n\tprint(\"mean vs kahan mean half\\n----\")\n\tprint_stats(m, km)\n\tprint_stats(m, hm)\n\t# print(\"kahan\", km)\n\t# print(\"exactly:\", m)\n", "with torch.no_grad():\n\t    ln_res = ln(x.float())\n\t    kln_res = kln(x.float())\n\t# print(\"float32\\n----\")\n\t# print_stats(ln_res, kln_res)\n\twith torch.no_grad():\n\t    y = x.half().to(\"mps\")\n\t    ln_res_half = ln(y).float().cpu()\n\t    kln_res_half = kln(y).float().cpu()\n\t# print(\"\\nfloat16\\n----\")\n", "# print_stats(ln_res_half, kln_res_half)\n\tprint(\"\\nfloat16 normal v float32 normal\\n----\")\n\tprint_stats(ln_res, ln_res_half)\n\tprint(\"\\nfloat16 kahan v float32 normal\\n----\")\n\tprint_stats(ln_res, kln_res_half)\n\tdef convert_bc1s_norm(n, f32=False, skip_trace=False):\n\t    if not skip_trace:\n\t        traced = torch.jit.trace(n, (x,))\n\t        mlp = ct.convert(traced,\n\t                        inputs=[ct.TensorType(name=\"x\", shape=(B,C,1,S), dtype=np.float32)],\n", "                        outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n\t                        compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n\t                        compute_units=ct.ComputeUnit.CPU_AND_NE,\n\t                        convert_to=\"milinternal\")\n\t    else:\n\t        mlp = n\n\t    print(n.__class__)\n\t    print(mlp)\n\t    return ct.convert(mlp,\n\t                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n", "                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n\t                    convert_to=\"mlprogram\")\n\tdef convert_bsc_norm(n, f32=False):\n\t    traced = torch.jit.trace(n, (x.permute(0,3,1,2).squeeze(-1),))\n\t    mlp = ct.convert(traced,\n\t                    inputs=[ct.TensorType(name=\"x\", shape=(B,S,C), dtype=np.float32)],\n\t                    outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n\t                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n\t                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n\t                    convert_to=\"milinternal\")\n", "    print(n.__class__)\n\t    print(mlp)\n\t    return ct.convert(mlp,\n\t                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n\t                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n\t                    convert_to=\"mlprogram\")\n\t# Interesting...\n\t@mb.program(input_specs=[mb.TensorSpec(shape=(B,C,1,S)),])\n\tdef ln_prog(x):\n\t    # x = mb.squeeze(x=x, axes=[2], name='squeeze')\n", "    x = mb.layer_norm(x=x, axes=[1], name=\"y\")\n\t    # x = mb.expand_dims(x=x, axes=[2], name=\"y\")\n\t    return x\n\tcln = convert_bc1s_norm(ln, False)\n\t# ckln = convert_bc1s_norm(kln, False)\n\tlnp = convert_bc1s_norm(ln_prog, False, skip_trace=True)\n\t# half_nnln = convert_bsc_norm(nn.LayerNorm(C, elementwise_affine=False))\n\t# nnln = convert_bsc_norm(nn.LayerNorm(C, elementwise_affine=False),f32=True)\n\tinp = {\"x\": x.float().numpy()}\n\tcoreml_ln = torch.from_numpy(cln.predict(inp)[\"y\"])\n", "coreml_kln = torch.from_numpy(ckln.predict(inp)[\"y\"])\n\tprint(lnp.predict(inp))\n\tcoreml_lnp = torch.from_numpy(lnp.predict(inp)[\"y\"])\n\tcoreml_half_nnln = half_nnln.predict({\"x\": x.permute(0,3,1,2).squeeze(-1).float().numpy()})[\"y\"]\n\tcoreml_half_nnln = torch.from_numpy(coreml_half_nnln).permute(0,2,1).unsqueeze(2)\n\tcoreml_nnln = nnln.predict({\"x\": x.permute(0,3,1,2).squeeze(-1).float().numpy()})[\"y\"]\n\tcoreml_nnln = torch.from_numpy(coreml_nnln).permute(0,2,1).unsqueeze(2)\n\tprint(\"\\coreml nn ln vs kln\\n----\")\n\tprint_stats(coreml_nnln, coreml_kln)\n\tprint(\"\\coreml nn ln vs ln\\n----\")\n", "print_stats(coreml_nnln, coreml_ln)\n\tprint(\"\\coreml nn ln vs half nn\\n----\")\n\tprint_stats(coreml_nnln, coreml_half_nnln)\n\tprint(\"\\coreml nn ln vs ln prog\\n----\")\n\tprint_stats(coreml_nnln, coreml_lnp)\n\t# Output of coreml norms for 1x1024x1x512 input with a 512 chunks.\n\t# Took forever to run and I think basically shows that Kahan accumulates too much error.\n\t# \\coreml nn ln vs kln\n\t# ----\n\t# all close? False\n", "# equal? False\n\t# mean diff tensor(2.1594e-06)\n\t# max diff tensor(0.0187)\n\t# psnr 67.48296284743999\n\t# num close: tensor(2398)\n\t# \\coreml nn ln vs ln\n\t# ----\n\t# all close? False\n\t# equal? False\n\t# mean diff tensor(2.3021e-06)\n", "# max diff tensor(0.0057)\n\t# psnr 77.59771092144952\n\t# num close: tensor(7922)\n"]}
{"filename": "src/experiments/dummy_layer_norm.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport numpy as np\n\tfrom src.ml_ane_transformers.ane.dummy_layer_norm import DummyLayerNormANE as DummyLN\n\tfrom src.ml_ane_transformers.ane.layer_norm import LayerNormANE\n\timport coremltools as ct\n\tfrom src.utils.psnr import compute_psnr\n\tfrom coremltools.converters.mil import Builder as mb\n\timport sys\n\t\"\"\"\n", "Test out the DummyLayerNormANE and see if it's possible to hot swap\n\tby overriding the batch_norm op.\n\tIt is.\n\t\"\"\"\n\ttorch.manual_seed(42)\n\tfrom coremltools.converters.mil import register_torch_op\n\tfrom coremltools.converters.mil.mil import Builder as mb\n\tfrom coremltools.converters.mil.frontend.torch.ops import _get_inputs\n\tfrom coremltools.converters.mil.frontend.torch.torch_op_registry import _TORCH_OPS_REGISTRY\n\tif \"batch_norm\" in _TORCH_OPS_REGISTRY:\n", "    del _TORCH_OPS_REGISTRY[\"batch_norm\"]\n\t@register_torch_op\n\tdef batch_norm(context, node):\n\t    inputs = _get_inputs(context, node, expected=9)\n\t    _input = inputs[0]\n\t    weight = inputs[1]\n\t    bias = inputs[2]\n\t    running_mean = inputs[3]\n\t    running_var = inputs[4]\n\t    training = inputs[5].val\n", "    eps = inputs[7]\n\t    context.add(eps)\n\t    context.add(weight)\n\t    context.add(bias)\n\t    context.add(_input)\n\t    print(\"weight.shape\", weight.shape)\n\t    ln = mb.layer_norm(x=_input, axes=[1], epsilon=eps, gamma=weight, beta=bias, name=node.name)\n\t    context.add(ln)\n\tB,C,S = 1, 5, 3\n\tdummy = DummyLN(C)\n", "ln = nn.LayerNorm(C, elementwise_affine=False)\n\tane = LayerNormANE(C, elementwise_affine=False)\n\tdummy_trace = torch.jit.trace(dummy, torch.randn((B,C,1,S)))\n\tprog = ct.convert(dummy_trace,\n\t                inputs=[ct.TensorType(\"x\", shape=(B,C,1,S))],\n\t                outputs=[ct.TensorType(\"y\")],\n\t                # pass_pipeline=ct.PassPipeline(pipeline_name=\"empty\"),\n\t                compute_precision=ct.precision.FLOAT32, # The float16 loss is real.\n\t                convert_to=\"milinternal\")\n\tprint(prog)\n", "mlmodel = ct.convert(prog,\n\t                inputs=[ct.TensorType(\"x\", shape=(B,C,1,S))],\n\t                outputs=[ct.TensorType(\"y\")],\n\t                compute_precision=ct.precision.FLOAT32,\n\t                convert_to=\"mlprogram\")\n\tx = torch.randn((B,C,1,S))\n\tprint(\"x\", x)\n\tprint(\"y\", mlmodel.predict({\"x\": x.numpy()})[\"y\"])\n\tprint(\"ane\", ane(x))\n\tprint(\"ln\", ane(x.permute(0,3,1,2).squeeze(-1)))\n"]}
{"filename": "src/experiments/diff_chunked_models.py", "chunked_list": ["import coremltools as ct\n\timport argparse\n\timport os\n\timport sys\n\timport torch\n\tfrom src.utils.psnr import compute_psnr\n\t\"\"\"\n\tImport a chunked pipeline model and an original model, perform predictions\n\ton both and ensure that they are identical.\n\t\"\"\"\n", "parser = argparse.ArgumentParser(description='Load a pipeline CoreML modelpackage and compare it with it\\'s non-pipeline version.')\n\tparser.add_argument('pipeline_model_path', help='path to *-pipeline.mlpackage file', type=str)\n\t# parser.add_argument('normal_model_path', help='path to non-pipelined *.mlpackage file', type=str)\n\targs = parser.parse_args()\n\tmodel_path = args.pipeline_model_path.replace(\"-pipeline\", \"\")\n\t# if os.path.exists(model_path):\n\t#     print(f\"non-pipelined model not found at {model_path}\")\n\t#     sys.exit(1)\n\tpipeline_path = args.pipeline_model_path\n\t# TODO: Need to add the model proxy for this.\n", "# if os.path.exists(pipeline_path.replace('.mlpackage', '.mlmodelc')):\n\t#     pipeline_path = pipeline_path.replace('.mlpackage', '.mlmodelc')\n\t# if os.path.exists(model_path.replace('.mlpackage', '.mlmodelc')):\n\t#     model_path = model_path.replace('.mlpackage', '.mlmodelc')\n\tprint(f\"Loading pipeline from {pipeline_path}...\")\n\tpipeline = ct.models.MLModel(args.pipeline_model_path, compute_units=ct.ComputeUnit.CPU_ONLY)\n\tprint(f\"Loading normal from {model_path}...\")\n\tmodel = ct.models.MLModel(model_path, compute_units=ct.ComputeUnit.CPU_ONLY)\n\tprint(\"Loaded both models.\")\n\t# input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n", "input_ids = torch.randint(10000, (1,512,)).int()\n\toutput_mask = torch.tensor([2]).int()\n\tprint(\"input_ids.shape\", input_ids.shape)\n\tprint(\"output_mask.shape\", output_mask.shape)\n\tpo = pipeline.predict({\"input_ids\": input_ids, \"output_mask\": output_mask})[\"logits\"]\n\tprint(\"Predicted on pipeline.\")\n\tmo = model.predict({\"input_ids\": input_ids, \"output_mask\": output_mask})[\"logits\"]\n\tprint(\"psnr: \", compute_psnr(po, mo))\n\tprint(\"equal:\",  torch.equal(torch.from_numpy(mo), torch.from_numpy(po)))"]}
{"filename": "src/experiments/__init__.py", "chunked_list": []}
{"filename": "src/experiments/norm_chunk_test.py", "chunked_list": ["import torch\n\t\"\"\"\n\tExperiment with splitting the input into chunks and computing\n\tthe mean on each chunk separately. If overflowing was a problem,\n\tit might help but you lose a lot of precision.\n\t\"\"\"\n\tn_embd = num_channels = 768\n\tseqlen = 512\n\teps = 1e-5\n\tweight = torch.rand(n_embd)\n", "bias = torch.rand(n_embd)\n\tx = torch.rand([1, n_embd, 1, seqlen])\n\t# torch.Size([1, 768, 1, 512]) torch.Size([768]) torch.Size([768])\n\t# x, weight, bias ^\n\tdef baseline(inputs):\n\t    input_rank = len(inputs.size())\n\t    assert input_rank == 4\n\t    assert inputs.size(1) == num_channels\n\t    assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\t    # if self.clip_mag is not None:\n", "    #     inputs.clamp_(-self.clip_mag, self.clip_mag)\n\t    channels_mean = inputs.mean(dim=1, keepdims=True) # shape [1,1,1,S]\n\t    zero_mean = inputs - channels_mean # shape [1,C,1,S]\n\t    zero_mean_sq = zero_mean * zero_mean # shape [1,C,1,S]\n\t    denom = (zero_mean_sq.mean(dim=1, keepdims=True) + eps).rsqrt() # [1,1,1,S]\n\t    out = zero_mean * denom # shape [1,C,1,S]\n\t    out = (out + bias.view(1, num_channels, 1, 1)\n\t            ) * weight.view(1, num_channels, 1, 1)\n\t    return out\n\tdef chunked_mean(inputs, num_chunks):\n", "    \"\"\"\n\t    Compute a mean in chunks to avoid overflows while summing. Assumes BC1S format.\n\t    NOTE: This causes a large loss of precision in float16. Not worth it as far as I can tell.\n\t    \"\"\"\n\t    num_channels = inputs.shape[1]\n\t    assert num_chunks > 0, \"num_chunks must be positive\"\n\t    assert num_channels % num_chunks == 0, \"num_chunks must evenly divide num_channels\"\n\t    if num_chunks == 1:\n\t        return inputs.mean(dim=1, keepdim=True)\n\t    channel_chunks = inputs.split(num_channels//num_chunks, dim=1)\n", "    mean = 0\n\t    for chunk in channel_chunks:\n\t        chunk_sum = chunk.sum(dim=1, keepdim=True) # shape [1,1,1,S]\n\t        chunk_mean = chunk_sum / num_channels\n\t        mean = mean + chunk_mean\n\t    return mean\n\tdef chunked(inputs, num_chunks, scale_factor=1e3):\n\t    channels_mean = chunked_mean(inputs / scale_factor, num_chunks) # shape [1,1,1,S]\n\t    channels_mean *= scale_factor\n\t    zero_mean = inputs - channels_mean # shape [1,C,1,S]\n", "    zero_mean_sq = zero_mean * zero_mean # shape [1,C,1,S]\n\t    mean_zero_mean_sq = chunked_mean(zero_mean_sq / scale_factor, num_chunks) * scale_factor\n\t    denom = (mean_zero_mean_sq + eps).rsqrt() # [1,1,1,S]\n\t    # denom = (chunked_mean(zero_mean_sq, num_chunks) + eps).rsqrt() # [1,1,1,S]\n\t    out = zero_mean * denom # shape [1,C,1,S]\n\t    out = (out + bias.view(1, num_channels, 1, 1)\n\t            ) * weight.view(1, num_channels, 1, 1)\n\t    return out\n\tbl = baseline(x)\n\tch = chunked(x, 1)\n", "print(\"baseline:\", bl.shape)\n\tprint(\"chunked :\", ch.shape)\n\tprint(\"equal?  :\", torch.equal(bl, ch))\n\tdelta = bl - ch\n\tprint(\"max delta:\", delta.max(), \"median:\", delta.median(), \"mean:\", delta.mean())\n"]}
{"filename": "src/experiments/conv_test.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\t\"\"\"\n\tExperiment to convince myself about how to appropriately translate\n\tweights for a Conv1D to Conv2d.\n\t\"\"\"\n\tEMBED_DIM = 768\n\tclass Conv1D(nn.Module):\n\t    \"\"\"\n\t    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n", "    Basically works like a linear layer but the weights are transposed.\n\t    Args:\n\t        nf (`int`): The number of output features.\n\t        nx (`int`): The number of input features.\n\t    \"\"\"\n\t    def __init__(self, nf, nx):\n\t        super().__init__()\n\t        self.nf = nf\n\t        self.weight = nn.Parameter(torch.empty(nx, nf))\n\t        self.bias = nn.Parameter(torch.zeros(nf))\n", "        nn.init.normal_(self.weight, std=0.02)\n\t    def forward(self, x):\n\t        size_out = x.size()[:-1] + (self.nf,)\n\t        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n\t        x = x.view(size_out)\n\t        return x\n\tc_proj = Conv1D(EMBED_DIM, EMBED_DIM)\n\t#c_proj = nn.Conv1d(EMBED_DIM, EMBED_DIM, 1)\n\tprint(\"1d weight shape:\", c_proj.weight.shape)\n\tprint(\"1d bias shape:\", c_proj.bias.shape)\n", "out_proj = nn.Conv2d(EMBED_DIM, EMBED_DIM, 1)\n\tprint(\"2d weight shape:\", out_proj.weight.shape)\n\tprint(\"2d bias shape:\", out_proj.bias.shape)\n\twith torch.no_grad():\n\t    out_proj.weight.copy_(c_proj.weight.t().unsqueeze(-1).unsqueeze(-1))\n\t    out_proj.bias.copy_(c_proj.bias)\n\tt = torch.rand(EMBED_DIM, EMBED_DIM)\n\toned_out = c_proj(t)\n\ttwod_out = out_proj(t.unsqueeze(-1).unsqueeze(-1))\n\tprint(\"1d out\", oned_out.shape)\n", "print(\"2d out\", twod_out.shape)\n\ttwod_out = twod_out.squeeze(-1).squeeze(-1)\n\t# TODO: cosine similarity is kind of garbage, did I mess this conversion up in the ane-optimized model?\n\tprint(nn.CosineSimilarity(dim=-1)(oned_out, twod_out).mean())\n\tprint(torch.equal(oned_out, twod_out))\n"]}
{"filename": "src/experiments/quant_chunk_test.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport coremltools as ct\n\timport numpy as np\n\t\"\"\"\n\tBuild a test model, quantize it and use it for debugging\n\tchunking.\n\t\"\"\"\n\tclass Net(nn.Module):\n\t    def __init__(self):\n", "        super().__init__()\n\t        self.ls = nn.ModuleList([\n\t            nn.Linear(1600, 6400),\n\t            nn.GELU(),\n\t            nn.Linear(6400, 1600),\n\t            nn.GELU(),\n\t            nn.Linear(1600, 6400),\n\t            nn.GELU(),\n\t            nn.Linear(6400, 1600),\n\t        ])\n", "    def forward(self, x):\n\t        for l in self.ls:\n\t            x = l(x)\n\t        return x\n\tif __name__ == \"__main__\":\n\t    model = Net().eval()\n\t    input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n\t    traced = torch.jit.trace(model, (input_ids))\n\t    mlmodel = ct.convert(\n\t        traced,\n", "        inputs=[\n\t            ct.TensorType(name=\"input_ids\", shape=input_ids.shape, dtype=np.float32),\n\t        ],\n\t        outputs=[\n\t            ct.TensorType(name=\"logits\", dtype=np.float32),\n\t        ],\n\t        compute_precision=ct.precision.FLOAT16,\n\t        convert_to=\"mlprogram\",\n\t    )\n\t    quantized = ct.compression_utils.palettize_weights(mlmodel, nbits=2, mode=\"kmeans\")\n", "    quantized.save(f\"test-quant-net.mlpackage\")"]}
{"filename": "src/experiments/make_pipeline.py", "chunked_list": ["import coremltools as ct\n\timport argparse\n\timport glob\n\timport re\n\timport sys\n\t\"\"\"\n\tCompose multiple test networks into a single pipeline to see if\n\tit will do anything clever when running on the ANE.\n\t** Requires coremltools 6.3 **\n\t\"\"\"\n", "parser = argparse.ArgumentParser(description='Stitch multiple chunked models into a single pipeline')\n\tparser.add_argument('model_path', help='path to any of the chunked model .mlpackage files', type=str)\n\tparser.add_argument('--range', help=\"comma-separated range of models to include\", type=str)\n\targs = parser.parse_args()\n\tmodel_range = None\n\tif args.range is not None:\n\t    model_range = [int(x) for x in args.range.split(\",\")]\n\t    assert len(model_range) == 2, f\"range must have two elements: {model_range}\"\n\tdef chunk_num(path):\n\t    match = re.search(r'_chunk(\\d+)\\.mlpackage$', path)\n", "    if match:\n\t        return int(match.group(1))\n\t    return None\n\tpattern = re.sub(r'_chunk\\d+', '*', args.model_path)\n\tmatching_files = [(chunk_num(path), path) for path in glob.glob(pattern)]\n\tmatching_files = [x for x in matching_files if x[0] is not None]\n\tmodel_paths = [x[1] for x in sorted(matching_files, key=lambda x: x[0])]\n\toutput_filename =  model_paths[0].replace(\"_chunk1\", \"-pipeline\")\n\tif model_range is not None:\n\t    # Shift left since chunks are 1-indexed.\n", "    start, end = model_range[0]-1, model_range[1]\n\t    model_paths = model_paths[start:end]\n\tprint(\"Will create a pipeline from the following models in this order:\")\n\tfor mp in model_paths:\n\t    print(mp)\n\tcorrect = input(\"Is that correct? (y/n) \") in [\"y\", \"Y\"]\n\tif correct:\n\t    print(\"Creating pipeline.\")\n\telse:\n\t    sys.exit(1)\n", "models = [ct.models.MLModel(model_path, skip_model_load=True) for model_path in model_paths]\n\tfilename =  model_paths[0].replace(\"_chunk1\", \"-pipeline\")\n\tif model_range is not None:\n\t    output_filename = output_filename.replace(\".mlpackage\", f\"-{model_range[0]}to{model_range[1]}.mlpackage\")\n\t# Erroring here? You need coremltools >= 6.3\n\tpipeline = ct.utils.make_pipeline(*models)\n\tprint(f\"Saving pipeline model to {output_filename}\")\n\tpipeline.save(output_filename)\n"]}
{"filename": "src/experiments/repro_channel_layer_norm.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport coremltools as ct\n\tfrom coremltools.converters.mil import Builder as mb\n\timport numpy as np\n\t\"\"\"\n\tReproduce layer norm on the first dimension of a 4D tensor that\n\tworks on CPU but fails on Neural Engine.\n\t\"\"\"\n\teps = 1e-5\n", "# B,C,S = 1,4,2 # This will look like it works because it's too small for the ANE.\n\tB,C,S = 1,1800,16 # This will fail since it's large enough for the ANE. Make it bigger if it doesn't.\n\tg,b = 1, 0 # It's not these, the results change predictably with them.\n\t@mb.program(input_specs=[mb.TensorSpec(shape=(B,C,1,S)),])\n\tdef ln_prog(x):\n\t    gamma = (torch.ones((C,), dtype=torch.float32) * g).tolist()\n\t    beta = (torch.ones((C), dtype=torch.float32) * b).tolist()\n\t    x = mb.layer_norm(x=x, axes=[1], gamma=gamma, beta=beta, name=\"y\")\n\t    return x\n\tx = torch.arange(B*C*S).reshape(B,C,1,S).float()\n", "# x = torch.cat([torch.ones((1,1,1,S)).float()] + [torch.zeros((1,1,1,S)).float() for i in range(C-1)], dim=1)\n\t# x = torch.cat([torch.ones((1,C,1,1)).float() * C*2] + [torch.zeros((1,C,1,1)).float() for i in range(S-1)], dim=3)\n\t# print(x.squeeze(2).permute(0,2,1))\n\t# x = x[torch.randperm(x.shape[0])]\n\t# x = torch.randn((B,C,1,S)).float() + 1000\n\tdef make_model(prog, compute_units):\n\t    return ct.convert(\n\t        prog,\n\t        inputs=[ct.TensorType(name=\"x\", shape=(B,C,1,S), dtype=np.float32)],\n\t        outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n", "        compute_units=compute_units,\n\t        # iOS 16 doesn't help.\n\t        convert_to=\"mlprogram\",\n\t    )\n\tdef predict(model, x):\n\t    return torch.from_numpy(model.predict({\"x\": x.numpy()})[\"y\"])\n\tcpu_model = make_model(ln_prog, ct.ComputeUnit.CPU_ONLY)\n\tane_model = make_model(ln_prog, ct.ComputeUnit.CPU_AND_NE)\n\tprint(\"input\\n-----\\n\",x,\"\\n\")\n\tdef ane_layer_norm(x):\n", "    channels_mean = x.mean(dim=1, keepdims=True)\n\t    zero_mean = x - channels_mean\n\t    zero_mean_sq = zero_mean * zero_mean\n\t    denom = (zero_mean_sq.mean(dim=1, keepdims=True) + eps).rsqrt()\n\t    return zero_mean * denom\n\tane_layer_norm_out = ane_layer_norm(x)\n\tprint(\"ml-ane-transformers LayerNorm result\\n-----\\n\",ane_layer_norm_out,\"\\n\")\n\tcpu_out = predict(cpu_model, x)\n\tprint(\"mb.program CPU_ONLY result\\n-----\\n\",cpu_out,\"\\n\")\n\t# Will be slightly different because of float16.\n", "ane_out = predict(ane_model, x)\n\tprint(\"mb.program CPU_AND_NE result\\n-----\\n\",ane_out,\"\\n\")\n\tdef custom_norm(x, dim):\n\t    channels_mean = x.mean(dim=dim, keepdims=True)  # B11S\n\t    zero_mean = x - channels_mean                   # BC1S\n\t    zero_mean_sq = zero_mean * zero_mean            # BC1S\n\t    denom = (zero_mean_sq.mean(dim=dim, keepdims=True) + eps).rsqrt() # B11S\n\t    return zero_mean * denom\n\t# for i in range(len(x.shape)):\n\t#     print(i, \"result\\n-----\\n\",custom_norm(x, i),\"\\n\")\n", "#     break\n\t# x_norms = torch.norm(x, p=2, dim=1, keepdim=True)\n\t# print(x_norms.shape)\n\t# x_norm = x / ((x_norms + eps) * 2 * C)\n\t# print(x_norms)\n\t# Trying to find the pattern in how the ANE output is different.\n\tprint(cpu_out.mean())\n\tprint(B,C,S, (ane_out / cpu_out).mean())\n\t# 1 1 2048 tensor(inf) (ane_out = inf, cpu_out = 0)\n\t# 1 2 2048 tensor(3.9978)\n", "# 1 3 2048 tensor(nan) (eyeballing it looks like ~8 but there's a bunch of zeros)\n\t# 1 4 2048 tensor(8.9392)\n\t# 1 5 2048 tensor(nan) (eyeballing it looks like ~1.26 but there's a bunch of zeros)\n\t# 1 6 2048 tensor(13.6535)\n\t# 1 7 2048 tensor(17.2813) # median\n\t# 1 8 2048 tensor(18.3189)\n\t# 1 10 2048 tensor(22.9630)\n\t# 1 14 2048 tensor(32.2279)\n\t# 1 15 2048 tensor(7249.1562) (maybe median is a better measure, that's 35)\n\t# 1 16 2048 tensor(36.8549)\n", "# 1 20 2048 tensor(46.0989)\n\t# 1 30 2048 tensor(69.1986)\n\t# 1 20 1024 tensor(23.0494)\n\t# 1 1800 1 tensor(2.2537)\n\t# 1 1800 2 tensor(4.2657)\n\t# 1 1800 3 tensor(6.2848)\n\t# 1 1800 4 tensor(8.3176)\n\t# 1 1800 5 tensor(10.3583)\n\t# 1 1800 16 tensor(32.6669)\n\t# 1 3600 8 tensor(32.6650) # BCS (no 1)\n", "# 1 3600 1 tensor(4.2657)"]}
{"filename": "src/experiments/repro_pipeline_compilation_bug.py", "chunked_list": ["import coremltools as ct\n\tfrom coremltools.converters.mil.mil import Function, Program\n\tfrom coremltools.converters.mil.mil import Builder as mb\n\timport numpy as np\n\t\"\"\"\n\tFor filing a GH issue about how compiled pipelines include\n\tmultiple copies of the weights.\n\t\"\"\"\n\tdef _make_model(input_name, input_length,\n\t                output_name, output_length,\n", "                convert_to):\n\t    weight_tensor = np.arange(input_length * output_length, dtype='float32')\n\t    weight_tensor = weight_tensor.reshape(output_length, input_length)\n\t    prog = Program()\n\t    func_inputs = {input_name: mb.placeholder(shape=(input_length,))}\n\t    with Function(func_inputs) as ssa_fun:\n\t        input = ssa_fun.inputs[input_name]\n\t        y = mb.linear(x=input, weight=weight_tensor, name=output_name)\n\t        ssa_fun.set_outputs([y])\n\t        prog.add_function(\"main\", ssa_fun)\n", "    return ct.convert(prog, convert_to=convert_to)\n\tif __name__ == \"__main__\":\n\t    # Create models\n\t    m1 = _make_model(\"x\", 20, \"y1\", 10, \"mlprogram\")\n\t    m2 = _make_model(\"y1\", 10, \"y2\", 2, \"mlprogram\")\n\t    pipeline_model = ct.utils.make_pipeline(m1, m2)\n\t    pipeline_model.save(\"test-pipeline.mlpackage\")"]}
{"filename": "src/experiments/print_model_weight_stats.py", "chunked_list": ["import numpy as np\n\tfrom models.pythia import GPT as Pythia\n\timport torch\n\timport plotille\n\t\"\"\"\n\tPrint out statistics about model weights.\n\t\"\"\"\n\tmodels = [\n\t    # \"pythia-70m\",\n\t    # \"pythia-160m\",\n", "    \"pythia-410m\",\n\t    \"pythia-1b\",\n\t    \"pythia-1.4b\",\n\t    \"pythia-2.8b\",\n\t]\n\t@torch.no_grad()\n\tdef print_weight_stats(model_name):\n\t    print(\"model\", model_name)\n\t    model = Pythia.from_pretrained(model_name).eval()\n\t    # Print the min/max/mean across all params.\n", "    # for name, param in model.named_parameters():\n\t    #     print(name, param.shape, param.dtype, param.min(), param.max(), param.mean())\n\t    all_params = []\n\t    for name, param in model.named_parameters():\n\t        all_params.append(param.flatten()) # a tensor\n\t    all_params = torch.cat(all_params, dim=0)\n\t    print(\"all params\", all_params.shape, all_params.dtype, all_params.min(), all_params.max(), all_params.mean())\n\t    counts, bins = np.histogram(all_params.numpy(), bins=10)\n\t    print(plotille.hist_aggregated(\n\t        counts,\n", "        bins,\n\t        width=80,\n\t        log_scale=True,\n\t        linesep='\\n',\n\t        lc=None,\n\t        bg=None,\n\t        color_mode='names',\n\t    ))\n\t    print(\"------\\n\")\n\tfor m in models:\n", "    print_weight_stats(m)\n"]}
{"filename": "src/experiments/memory_speedup.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport numpy as np\n\timport coremltools as ct\n\timport time\n\t\"\"\"\n\tGenerate a large-ish model for evaluating the speed of\n\tct.convert on different versions of coremltools.\n\t\"\"\"\n\tclass Net(nn.Module):\n", "    def __init__(self, num_loops):\n\t        super().__init__()\n\t        self.ls = nn.ModuleList([\n\t            l for _ in range(num_loops)\n\t            for l in [nn.Linear(1600, 6400), nn.GELU(), nn.Linear(6400, 1600)]\n\t        ])\n\t    def forward(self, x):\n\t        for l in self.ls:\n\t            x = l(x)\n\t        return x\n", "if __name__ == \"__main__\":\n\t    # 1600*6400*2 = 20.4M params + 40.96MB per loop\n\t    num_loops = 20\n\t    net = Net(num_loops).eval()\n\t    input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n\t    traced = torch.jit.trace(net, (input_ids))\n\t    total_params = sum(p.numel() for p in traced.parameters())\n\t    print(f\"{total_params / 1000 / 1000:0.3f}M params\")\n\t    print(f\"{(total_params * 16 / 8) / 1024 / 1024:0.1f} MB @ f16\")\n\t    start = time.perf_counter()\n", "    ct.convert(\n\t        traced,\n\t        inputs=[\n\t            ct.TensorType(name=\"inputs\", shape=input_ids.shape, dtype=np.float32),\n\t        ],\n\t        outputs=[\n\t            ct.TensorType(name=\"outputs\", dtype=np.float32),\n\t        ],\n\t        compute_precision=ct.precision.FLOAT16, # FLOAT32 is faster.\n\t        convert_to=\"mlprogram\",\n", "    )\n\t    end = time.perf_counter()\n\t    print(f\"convert took: {end - start:0.4f}s\")"]}
{"filename": "src/experiments/max_size_model.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport numpy as np\n\timport coremltools as ct\n\timport sys\n\t\"\"\"\n\tGenerate large models (either number of parameters or number of ops) to\n\ttry and find the limits of the neural engine. It seems to be size-based.\n\t\"\"\"\n\tclass Net(nn.Module):\n", "    def __init__(self, num_loops):\n\t        super().__init__()\n\t        # Many weights.\n\t        self.ls = nn.ModuleList([\n\t            l for _ in range(num_loops)\n\t            for l in [nn.Linear(1600, 6400), nn.GELU(), nn.Linear(6400, 1600)]\n\t        ])\n\t        # Many ops.\n\t        # self.ls = nn.ModuleList([\n\t        #     l for _ in range(num_loops)\n", "        #     for l in [nn.Linear(20, 10), nn.GELU(), nn.Linear(10, 20)]\n\t        # ])\n\t    def forward(self, x):\n\t        for l in self.ls:\n\t            x = l(x)\n\t        return x\n\tif __name__ == \"__main__\":\n\t    size = 20 # 20 gets you about 800MB.\n\t    net = Net(size).eval()\n\t    input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n", "    if input_ids.shape[-1] > 20 and size > 200:\n\t        print(\"Comment this out if you really want to make a 10GB+ model.\")\n\t        sys.exit(1)\n\t    traced = torch.jit.trace(net, (input_ids))\n\t    total_params = sum(p.numel() for p in traced.parameters())\n\t    print(f\"{total_params} params\")\n\t    print(f\"{(total_params * 16 / 8) / 1024 / 1024} MB @ f16\")\n\t    mlmodel = ct.convert(\n\t        traced,\n\t        inputs=[\n", "            ct.TensorType(name=\"input_ids\", shape=input_ids.shape, dtype=np.float32),\n\t        ],\n\t        outputs=[\n\t            ct.TensorType(name=\"logits\", dtype=np.float32),\n\t        ],\n\t        compute_precision=ct.precision.FLOAT16,\n\t        convert_to=\"mlprogram\",\n\t    )\n\t    mlmodel.save(f\"test-net-{size}-loops.mlpackage\")"]}
{"filename": "src/experiments/custom_softmax.py", "chunked_list": ["import torch\n\timport coremltools as ct\n\tfrom coremltools.converters.mil import Builder as mb\n\timport numpy as np\n\tfrom coremltools.converters.mil.mil import types\n\t\"\"\"\n\tConfirm that coremltools uses the softmax subtraction trick (it must, right?).\n\t\"\"\"\n\tS = 3\n\t@mb.program(input_specs=[mb.TensorSpec(shape=(S,), dtype=types.fp16),], opset_version=ct.target.iOS16)\n", "def sm_prog(x):\n\t    return mb.softmax(x=x, axis=-1, name=\"y\")\n\tf32 = False\n\tmlmodel = ct.convert(sm_prog,\n\t                    inputs=[ct.TensorType(name=\"x\", shape=(S,), dtype=types.fp16)],\n\t                    outputs=[ct.TensorType(name=\"y\", dtype=types.fp16)],\n\t                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n\t                    compute_units=ct.ComputeUnit.CPU_ONLY,\n\t                    minimum_deployment_target=ct.target.iOS16,\n\t                    convert_to=\"mlprogram\")\n", "# print(mlmodel)\n\tx = torch.tensor([700_000, 700_000, 30_000]).float()\n\tinputs = {\"x\": x.numpy()}\n\tml_y = mlmodel.predict(inputs)[\"y\"]\n\ttorch_y = x.softmax(-1)\n\tmanual_y = torch.exp(x - x.max()) / torch.exp(x-x.max()).sum(-1, keepdim=True)\n\tprint(\"ml_y\", ml_y)\n\tprint(\"torch_y\", torch_y)\n\tprint(\"manual_y\", manual_y)\n"]}
{"filename": "src/experiments/dump_weights.py", "chunked_list": ["import coremltools as ct\n\timport argparse\n\timport glob\n\timport re\n\timport sys\n\timport os\n\timport numpy as np\n\tfrom coremltools.libmilstoragepython import _BlobStorageReader as BlobReader\n\timport coremltools as ct\n\tfrom coremltools.converters.mil.mil import types\n", "from coremltools.converters.mil.frontend.milproto.helper import proto_to_types\n\t\"\"\"\n\tParse out the ops that use weights from weight.bin from a .proto mlpackage proto file.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse .mlpackage to inspect weights')\n\tparser.add_argument('model_path', help='path to a .mlmodelc file', type=str)\n\targs = parser.parse_args()\n\tdef get_nn(spec):\n\t    if spec.WhichOneof(\"Type\") == \"neuralNetwork\":\n\t        nn_spec = spec.neuralNetwork\n", "    elif spec.WhichOneof(\"Type\") in \"neuralNetworkClassifier\":\n\t        nn_spec = spec.neuralNetworkClassifier\n\t    elif spec.WhichOneof(\"Type\") in \"neuralNetworkRegressor\":\n\t        nn_spec = spec.neuralNetworkRegressor\n\t    elif spec.WhichOneof(\"Type\") in \"mlProgram\":\n\t        nn_spec = spec.mlProgram\n\t    else:\n\t        raise ValueError(f\"Invalid neural network specification for the model: {spec.WhichOneof('Type')}\")\n\t    return nn_spec\n\tspec =  ct.utils.load_spec(args.model_path)\n", "nn_spec = get_nn(spec)\n\t# print(dir(nn_spec))\n\tprint(f\"attributes: {nn_spec.attributes}\\ndocString: {nn_spec.docString}\\nversion: {nn_spec.version}\")\n\tprint(f\"{len(nn_spec.functions)} functions\")\n\tassert len(nn_spec.functions) == 1, \"haven't seen a spec with > 1 function\"\n\tfn_name, fn = list(nn_spec.functions.items())[0]\n\tprint(f\"{fn_name} function\\n------\")\n\tprint(f\"opset: {fn.opset}\")\n\tprint(f\"attributes: {fn.attributes}\")\n\tprint(f\"block_specializations: {list(fn.block_specializations.keys())}\")\n", "block = fn.block_specializations[fn.opset]\n\tprint(f\"{fn.opset} block\\n------\")\n\tprint(f\"attributes: {block.attributes}\")\n\tprint(f\"operations count: {len(block.operations)}\")\n\ttotal_bits = 0\n\tblob_reader = BlobReader(os.path.join(args.model_path, \"Data/com.apple.CoreML/weights/weight.bin\"))\n\tfor op in block.operations:\n\t    for name, att in op.attributes.items():\n\t        if att.WhichOneof(\"value\") != \"blobFileValue\":\n\t            continue\n", "        valuetype = proto_to_types(att.type)\n\t        is_tensor = types.is_tensor(valuetype)\n\t        if not is_tensor:\n\t            print(f\"Skipping non-tensor type: {att.type.WhichOneof('type')}\")\n\t            continue\n\t        if op.type in [\"const\"]: # constexpr too?\n\t            # print(op.type, att.blobFileValue.fileName, att.blobFileValue.offset, op.outputs[0].name)\n\t            offset = att.blobFileValue.offset\n\t            dtype = valuetype if not is_tensor else valuetype.get_primitive()\n\t            shape = () if not is_tensor else valuetype.get_shape()\n", "            if dtype == types.uint8:\n\t                value_bits = 8\n\t            elif dtype == types.int8:\n\t                value_bits = 8\n\t            elif dtype == types.fp16:\n\t                value_bits = 16\n\t            elif dtype == types.fp32:\n\t                value_bits = 32\n\t            else:\n\t                raise ValueError(f\"Invalid dtype for blob file value type: {dtype}\")\n", "            num_values = np.product(shape)\n\t            print(op.outputs[0].name, num_values, value_bits)\n\t            total_bits += (num_values * value_bits)\n\t            # if dtype == types.uint8:\n\t            #     np_value = np.array(blob_reader.read_uint8_data(offset), np.uint8)\n\t            # elif dtype == types.int8:\n\t            #     np_value = np.array(blob_reader.read_int8_data(offset), np.int8)\n\t            # elif dtype == types.fp16:\n\t            #     np_value_uint16 = np.array(blob_reader.read_fp16_data(offset), np.uint16)\n\t            #     np_value = np.frombuffer(np_value_uint16.tobytes(), np.float16)\n", "            # elif dtype == types.fp32:\n\t            #     np_value = np.array(blob_reader.read_float_data(offset), np.float32)\n\t            # else:\n\t            #     raise ValueError(f\"Invalid dtype for blob file value type: {dtype}\")\n\t            # value = np_value\n\t            # if dtype in (types.fp16, types.int8, types.uint8, types.uint32):\n\t            #     value = np.frombuffer(value, types.nptype_from_builtin(dtype)).reshape(\n\t            #         shape\n\t            #     )\n\t            # elif dtype == types.str and shape == ():\n", "            #     value = str(value[0])\n\t            # elif dtype in (types.fp32, types.str, types.bool, types.int32, types.int64):\n\t            #     value = (\n\t            #         np.array(value).astype(types.nptype_from_builtin(dtype)).reshape(shape)\n\t            #     )\n\t            # else:\n\t            #     raise ValueError(f\"Invalid dtype for tensor value: {dtype}\")\n\t            # print(value)\n\tprint(f\"total: {total_bits} bits\")"]}
{"filename": "src/experiments/dump_mil_weights.py", "chunked_list": ["import coremltools as ct\n\timport argparse\n\timport glob\n\timport re\n\timport sys\n\tfrom coremltools.libmilstoragepython import _BlobStorageReader as BlobReader\n\t\"\"\"\n\tParse out the ops that use weights from weight.bin from a .mil file.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse .mil to inspect weights')\n", "parser.add_argument('model_path', help='path to a .mlmodelc file', type=str)\n\targs = parser.parse_args()\n\tmil_path = f\"{args.model_path}/model.mil\"\n\tweight_lines = []\n\twith open(mil_path, 'r') as f:\n\t    for line in f:\n\t        if \"BLOBFILE\" in line:\n\t            weight_lines.append(line)\n\tweight_names = []\n\tfor l in weight_lines:\n", "    name = re.search(r'>\\s*(.*?)\\s*=', l).group(1)\n\t    weight_names.append(name)\n\tfor n in weight_names:\n\t    print(n)\n"]}
{"filename": "src/experiments/check_psnr.py", "chunked_list": ["import coremltools as ct\n\timport argparse\n\timport torch\n\timport numpy as np\n\tfrom src.utils.model_proxy import MLModelProxy\n\tfrom models.gpt2 import GPT as GPT2\n\tfrom models.pythia import GPT as Pythia\n\tfrom src.utils.psnr import compute_psnr\n\t\"\"\"\n\tCheck PSNR between a CoreML model and a non-CoreML model.\n", "Over 60 means there was little loss in the conversion process.\n\t\"\"\"\n\tall_names = GPT2.model_names() + Pythia.model_names()\n\tparser = argparse.ArgumentParser(description='Load a CoreML modelpackage and generate some text.')\n\tparser.add_argument('mlmodelc_path', help='path to .mlpackage file', default=\"gpt2.mlpackage\", type=str)\n\tparser.add_argument('model_name', choices=all_names, default=\"gpt2\", type=str)\n\targs = parser.parse_args()\n\tmodel_class = GPT2 if \"gpt2\" in args.model_name else Pythia\n\tbaseline_model = model_class.from_pretrained(args.model_name)\n\t# Not all models work on CPU_ONLY (e.g. pythia-70m)\n", "mlmodel = MLModelProxy(args.mlmodelc_path, ct.ComputeUnit.CPU_AND_NE)\n\tdef jaccard(x,y):\n\t    z=set(x).intersection(set(y))\n\t    a=float(len(z))/(len(x)+len(y)-len(z))\n\t    return a\n\tpsnrs = []\n\tfor i in range(5):\n\t    input_ids = torch.randint(10_000, (1,512,))\n\t    output_mask = torch.randint(512, (1,))\n\t    with torch.no_grad():\n", "        baseline_out = baseline_model(input_ids, output_mask).to(torch.float32)\n\t    input_ids = input_ids.int()\n\t    output_mask = output_mask.int()\n\t    # Hanging here? It's very likely your intputs are the wrong shape and/or types.\n\t    print(\"predicting with mlmodel\")#, input_ids.shape, input_ids.dtype)\n\t    mlmodel_out = mlmodel.predict({\"input_ids\": input_ids.numpy(), \"output_mask\": output_mask.numpy()})\n\t    mlmodel_out = torch.from_numpy(mlmodel_out[\"logits\"]).to(torch.float32)\n\t    assert baseline_out.shape == mlmodel_out.shape, f\"{baseline_out.shape} != {mlmodel_out.shape}\"\n\t    assert baseline_out.dtype == mlmodel_out.dtype, f\"{baseline_out.dtype} != {mlmodel_out.dtype}\"\n\t    psnr = compute_psnr(baseline_out, mlmodel_out)\n", "    print(\"PSNR:\", psnr)\n\t    psnrs.append(psnr)\n\t    for k in [80]: #range(40, 400, 40):\n\t        print(\"k:\", k)\n\t        baseline_topk = torch.topk(torch.nn.functional.softmax(baseline_out, dim=-1), k)\n\t        coreml_topk = torch.topk(torch.nn.functional.softmax(mlmodel_out, dim=-1), k)\n\t        # print(\"baseline_topk:\", baseline_topk.indices)\n\t        # print(\"coreml_topk:\", coreml_topk.indices)\n\t        topk_psnr = compute_psnr(baseline_out[:, :, baseline_topk.indices], mlmodel_out[:, :, baseline_topk.indices])\n\t        print(\"topk PSNR:\", topk_psnr)\n", "        # closer to 1 is better\n\t        print(\"jaccard topk\", jaccard(baseline_topk.indices.flatten().tolist(), coreml_topk.indices.flatten().tolist()))\n\t        kl_div = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(mlmodel_out, dim=-1), torch.nn.functional.softmax(baseline_out, dim=-1), reduction=\"batchmean\")\n\t        # clsoer to 0 is better\n\t        print(\"kl div\", kl_div.item())\n\t    print(\"\")\n\tprint(\"Mean PSNR:\", np.average(psnrs))\n\tprint(\"Median PSNR:\", np.median(psnrs))"]}
