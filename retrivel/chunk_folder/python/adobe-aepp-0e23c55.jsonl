{"filename": "setup.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport codecs\n", "import os\n\timport setuptools\n\tdef read(rel_path: str):\n\t    here = os.path.abspath(os.path.dirname(__file__))\n\t    with codecs.open(os.path.join(here, rel_path), 'r') as fp:\n\t        return fp.read()\n\tdef get_version(rel_path: str):\n\t    for line in read(rel_path).splitlines():\n\t        if line.startswith('__version__'):\n\t            delim = '\"' if '\"' in line else \"'\"\n", "            return line.split(delim)[1]\n\t    else:\n\t        raise RuntimeError(\"Unable to find version string.\")\n\twith open(\"README.md\", \"r\") as fh:\n\t    long_description = fh.read()\n\tsetuptools.setup(\n\t    name=\"aepp\",  # Replace with your own username\n\t    version=get_version(\"aepp/__version__.py\"),\n\t    author=\"Julien Piccini\",\n\t    author_email=\"piccini.julien@gmail.com\",\n", "    description=\"Package to manage AEP API endpoint and some helper functions\",\n\t    long_description=long_description,\n\t    long_description_content_type=\"text/markdown\",\n\t    url=\"https://github.com/adobe/aepp\",\n\t    packages=setuptools.find_packages(),\n\t    include_package_data=True,\n\t    classifiers=[\n\t        \"Programming Language :: Python :: 3\",\n\t        \"Operating System :: OS Independent\",\n\t        \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n", "        \"Topic :: Utilities\",\n\t        \"Topic :: Internet\",\n\t        \"Topic :: Software Development :: Libraries\",\n\t        \"Development Status :: 2 - Pre-Alpha\"\n\t    ],\n\t    python_requires='>=3.6',\n\t    install_requires=['pandas', \"requests\",\n\t                      \"PyJWT\", \"pathlib2\", \"pathlib\", \"PyJWT[crypto]\", \"tenacity\"],\n\t)\n"]}
{"filename": "tests/schema_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.schema import Schema\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock\n\tclass SchemaTest(unittest.TestCase):\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_schema_get_resource(self, mock_connector):\n\t        instance_conn = mock_connector.return_value\n\t        instance_conn.getData.return_value = \"foo\"\n\t        schema_obj = Schema()\n\t        result = schema_obj.getResource(MagicMock(), MagicMock(), MagicMock(), MagicMock())\n\t        assert(result is not None)\n", "        instance_conn.getData.assert_called_once()\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_schema_update_sandbox(self, mock_connector):\n\t        schema_obj = Schema()\n\t        test_sandbox = \"prod\"\n\t        schema_obj.updateSandbox(test_sandbox)\n\t        assert(schema_obj.sandbox == test_sandbox)\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_schema_get_stats(self, mock_connector):\n\t        instance_conn = mock_connector.return_value\n", "        instance_conn.getData.return_value = MagicMock()\n\t        schema_obj = Schema()\n\t        stats_result = schema_obj.getStats()\n\t        assert (stats_result is not None)\n\t        assert(stats_result == instance_conn.getData.return_value)\n\t        instance_conn.getData.assert_called_once()\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_schema_get_tenant_id(self, mock_connector):\n\t        instance_conn = mock_connector.return_value\n\t        instance_conn.getStats.return_value = MagicMock()\n", "        schema_obj = Schema()\n\t        tenant_id_result = schema_obj.getTenantId()\n\t        assert (tenant_id_result is not None)\n\t        instance_conn.getData.assert_called_once()\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_schema_get_behavior(self, mock_connector):\n\t        instance_conn = mock_connector.return_value\n\t        instance_conn.getData.return_value = {\"results\":[1234,5678]}\n\t        schema_obj = Schema()\n\t        stats_result = schema_obj.getBehaviors()\n", "        assert (stats_result is not None)\n\t        assert (stats_result == instance_conn.getData.return_value.get(\"results\"))\n\t        instance_conn.getData.assert_called_once()"]}
{"filename": "tests/catalog_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.schema import Schema\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock\n\tclass CatalogTest(unittest.TestCase):\n\t    def test_catalog_get_catalog__resource(self):\n\t        assert True\n\t    def test_catalog_decode_stream_batch(self):\n\t        assert True\n\t    def test_catalog_json_stream_messages(self):\n\t        assert True\n\t    def test_catalog_get_batches(self):\n", "        assert True\n\t    def test_catalog_get_failed_batches_df(self):\n\t        assert True\n\t    def test_catalog_get_batch(self):\n\t        assert True\n\t    def test_catalog_create_batch(self):\n\t        assert True\n\t    def test_catalog_get_resources(self):\n\t        assert True\n\t    def test_catalog_get_data_sets(self):\n", "        assert True\n\t    def test_catalog_create_datasets(self):\n\t        assert True\n\t    def test_catalog_get_data_set(self):\n\t        assert True\n\t    def test_catalog_delete_data_set(self):\n\t        assert True\n\t    def test_catalog_get_data_set_views(self):\n\t        assert True\n\t    def test_catalog_get_data_set_view(self):\n", "        assert True\n\t    def test_catalog_get_dataset_view_files(self):\n\t        assert True\n\t    def test_catalog_enable_dataset_profile(self):\n\t       assert True\n\t    def test_catalog_enable_dataset_identity(self):\n\t        assert True\n\t    def test_catalog_disable_dataset_profile(self):\n\t        assert True\n\t    def test_catalog_disable_dataset_identity(self):\n", "        assert True\n\t    def test_catalog_create_union_profile_dataset(self):\n\t        assert True\n\t    def test_catalog_get_mapper_errors(self):\n\t        assert True"]}
{"filename": "tests/__init__.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n"]}
{"filename": "tests/destinationinstanceservice_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.destinationinstanceservice import DestinationInstanceService\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock, ANY\n\tclass DestinationInstanceServiceTest(unittest.TestCase):\n\t    ADHOC_INPUT = {\"flow1\": [\"dataset1\"], \"flow2\": [\"dataset2\", \"dataset3\"]}\n\t    ADHOC_EXPECTED_PAYLOAD = {'activationInfo': {'destinations': [{'flowId': 'flow1', 'datasets': [{'id': 'dataset1'}]}, {'flowId': 'flow2', 'datasets': [{'id': 'dataset2'}, {'id': 'dataset3'}]}]}}\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_create_adhoc_dataset_export(self, mock_connector):\n\t        instance_conn = mock_connector.return_value\n\t        instance_conn.postData.return_value = {'foo'}\n\t        destination_instance_service_obj = DestinationInstanceService()\n", "        result = destination_instance_service_obj.createAdHocDatasetExport(self.ADHOC_INPUT)\n\t        assert(result is not None)\n\t        instance_conn.postData.assert_called_once()\n\t        instance_conn.postData.assert_called_with(ANY, data=self.ADHOC_EXPECTED_PAYLOAD)\n\t    @patch(\"aepp.connector.AdobeRequest\")\n\t    def test_create_adhoc_dataset_export_invalid_input(self, mock_connector):\n\t        destination_instance_service_obj = DestinationInstanceService()\n\t        with self.assertRaises(Exception) as cm:\n\t            destination_instance_service_obj.createAdHocDatasetExport(None)\n\t        self.assertEqual('Require a dict for defining the flowId to datasetIds mapping', str(cm.exception))\n"]}
{"filename": "tests/datasets_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.schema import Schema\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock\n\tclass DatasetsTest(unittest.TestCase):\n\t    def test_datasets_get_label_schema(self):\n\t        assert True\n\t    def test_datasets_head_label(self):\n\t        assert True\n\t    def test_datasets_delete_labels(self):\n\t        assert True\n\t    def test_datasets_create_labels(self):\n", "        assert True\n\t    def test_datasets_update_labels(self):\n\t        assert True\n"]}
{"filename": "tests/flowservice_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.schema import Schema\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock\n\tclass FlowserviceTest(unittest.TestCase):\n\t    def test_flowservice_get_resource(self):\n\t        assert True\n\t    def test_flowservice_get_connections(self):\n\t        assert True\n\t    def test_flowservice_create_connection(self):\n\t        assert True\n\t    def test_flowservice_create_streaming_connection(self):\n", "        assert True\n\t    def test_flowservice_get_connection(self):\n\t        assert True\n\t    def test_flowservice_connection(self):\n\t        assert True\n\t    def test_flowservice_delete_connection(self):\n\t        assert True\n\t    def test_flowservice_get_connection_specs(self):\n\t        assert True\n\t    def test_flowservice_get_connection_specs_map(self):\n", "        assert True\n\t    def test_flowservice_get_connection_spec(self):\n\t        assert True\n\t    def test_flowservice_get_connection_specid_from_name(self):\n\t        assert True\n\t    def test_flowservice_get_flows(self):\n\t        assert True\n\t    def test_flowservice_get_flow(self):\n\t        assert True\n\t    def test_flowservice_delete_flow(self):\n", "        assert True\n\t    def test_flowservice_create_flow(self):\n\t        assert True\n\t    def test_flowservice_create_flow_data_lake_to_data_landing_zone(self):\n\t        assert True\n\t    def test_flowservice_create_data_landing_zone_to_datalake(self):\n\t        assert True\n\t    def test_flowservice_updateFlow(self):\n\t        assert True\n\t    def test_flowservice_get_flow_specs(self):\n", "        assert True\n\t    def test_flowservice_get_flow_spec_id_from_names(self):\n\t         assert True\n\t    def test_flowservice_get_flow_spec(self):\n\t        assert True\n\t    def test_flowservice_get_runs(self):\n\t        assert True\n\t    def test_flowservice_create_run(self):\n\t        assert True\n\t    def test_flowservice_get_run(self):\n", "        assert True\n\t    def test_flowservice_get_source_connections(self):\n\t        assert True\n\t    def test_flowservice_get_source_connection(self):\n\t        assert True\n\t    def test_flowsevrice_delete_source_connection(self):\n\t        assert True\n\t    def test_flowservice_create_source_connection(self):\n\t        assert True\n\t    def test_flowservice_create_source_connection_streaming(self):\n", "        assert True\n\t    def test_flowservice_create_source_connectionDataLandingZone(self):\n\t        assert True\n\t    def test_flowservice_create_source_connection_datalake(self):\n\t        assert True\n\t    def test_flowservice_update_source_connection(self):\n\t        assert True\n\t    def test_flowservice_get_target_connections(self):\n\t        assert True\n\t    def test_flowservice_get_target_connection(self):\n", "        assert True\n\t    def test_flowservice_delete_target_connection(self):\n\t        assert True\n\t    def test_flowservice_create_target_connection(self):\n\t        assert True\n\t    def test_flowservice_create_target_connection_data_landin_zone(self):\n\t        assert True\n\t    def test_flowservice_create_target_connection_datalake(self):\n\t        assert True\n\t    def test_flowservice_update_target_connection(self):\n", "        assert True\n\t    def test_flowservice_update_policy(self):\n\t        assert True\n\t    def test_flowservice_get_landing_zone_container(self):\n\t        assert True\n\t    def test_flowservice_get_landing_zone_credential(self):\n\t        assert True\n\t    def test_flowservice_explore_landing_zone(self):\n\t        assert True\n\t    def test_flowservice_get_landing_zone_content(self):\n", "        assert True\n"]}
{"filename": "tests/exportDatasetToDatalandingZone_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.exportDatasetToDataLandingZone import ExportDatasetToDataLandingZone\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock, ANY\n\tfrom tenacity import RetryError\n\tclass ExportDatasetToDataLandingZoneTest(unittest.TestCase):\n\t    flow_response = {\n\t        \"id\": \"df193495-9837-407f-9d50-e51efc067cb5\",\n\t        \"sourceConnectionIds\": [\n\t            \"78754315-852c-4c4e-8f1a-d12b83264f3f\"\n\t        ],\n\t        \"state\": \"disabled\",\n", "        \"etag\": \"test_etag\"}\n\t    connection_spec = {\n\t        \"name\": \"test\",\n\t        \"id\": \"test_connection_id\"\n\t    }\n\t    source_connection = {\n\t        \"id\": \"test_source_connection_id\",\n\t        \"params\": {\n\t            \"datasets\": [{\n\t                \"dataSetId\": \"test\"\n", "            }]}\n\t    }\n\t    base_connection = {\n\t        \"id\": \"test_base_connection_id\"\n\t    }\n\t    target_connection = {\n\t        \"id\": \"test_target_connection_id\"\n\t    }\n\t    flow_run = {\n\t        \"items\": [\n", "            {\n\t                \"id\": \"test_id\",\n\t                \"metrics\": {\n\t                    \"durationSummary\": {\n\t                        \"startedAtUTC\": 1691184699,\n\t                        \"completedAtUTC\": 1691184819\n\t                    },\n\t                    \"fileSummary\": {\n\t                        \"outputFileCount\": 24\n\t                    },\n", "                    \"sizeSummary\": {\n\t                        \"outputBytes\": 77494320\n\t                    },\n\t                    \"recordSummary\": {\n\t                        \"outputRecordCount\": 1065802\n\t                    }\n\t                }\n\t            }\n\t        ]\n\t    }\n", "    adhoc_success_response = {\n\t        \"destinations\":[\n\t            {\n\t                \"datasets\":[\n\t                    {\n\t                        \"id\":\"641ce00b8f31811b98dd3b56\",\n\t                        \"statusURL\":\"https: //platform-stage.adobe.io/data/foundation/flowservice/runs/aa39ad3d-24ba-4579-9cdc-55536c408721\",\n\t                        \"flowId\":\"3eaa2c0b-e24b-46bd-8eee-bbaf9d6cf2cb\"\n\t                    }\n\t                ]\n", "            }\n\t        ]\n\t    }\n\t    adhoc_non_retry_error = {\n\t        \"error_code\": \"401013\",\n\t        \"message\": \"Oauth token is not valid\"\n\t    }\n\t    adhoc_retry_error = {\n\t        \"message\": \"Following order ID(s) are not ready for dataset export\"\n\t    }\n", "    config = {\n\t        \"org_id\": \"3ADF23C463D98F640A494032@AdobeOrg\",\n\t        \"client_id\": \"35e6e4d205274c4ca1418805ac41153b\",\n\t        \"tech_id\": \"test005@techacct.adobe.com\",\n\t        \"pathToKey\": \"/Users/Downloads/config/private.key\",\n\t        \"auth_code\": \"\",\n\t        \"secret\": \"test\",\n\t        \"date_limit\": 0,\n\t        \"sandbox\": \"prod\",\n\t        \"environment\": \"stage\",\n", "        \"token\": \"token\",\n\t        \"jwtTokenEndpoint\": \"https://ims-na1.adobelogin.com/ims/exchange/jwt/\",\n\t        \"oauthTokenEndpoint\": \"\",\n\t        \"imsEndpoint\": \"https://ims-na1-stg1.adobelogin.com\",\n\t        \"private_key\": \"\"\n\t    }\n\t    @patch('aepp.utils.Utils.check_if_exists', MagicMock(return_value = \"test_dataflow_id\"))\n\t    @patch('aepp.flowservice.FlowService.getFlow', MagicMock(return_value = flow_response))\n\t    @patch('aepp.flowservice.FlowService.getSourceConnection', MagicMock(return_value = source_connection))\n\t    @patch('aepp.flowservice.FlowService.getRun', MagicMock(return_value = flow_run))\n", "    @patch('aepp.destinationinstanceservice.DestinationInstanceService.createAdHocDatasetExport', MagicMock(return_value = adhoc_success_response))\n\t    @patch(\"aepp.connector.AdobeRequest\", MagicMock())\n\t    def test_create_dataflow_if_exist(self):\n\t        export_obj = ExportDatasetToDataLandingZone(config= self.config, header= MagicMock())\n\t        export_obj.createDataFlowRunIfNotExists(dataset_id=\"test\", compression_type=\"gzip\", data_format=\"parquet\", export_path=\"test\", on_schedule=False, config_path=\"test\", entity_name=\"test\", initial_delay=0)\n\t    @patch('aepp.utils.Utils.check_if_exists', MagicMock(return_value = \"test_dataflow_id\"))\n\t    @patch('aepp.flowservice.FlowService.getFlow', MagicMock(return_value = flow_response))\n\t    @patch('aepp.flowservice.FlowService.getSourceConnection', MagicMock(return_value = source_connection))\n\t    @patch(\"aepp.connector.AdobeRequest\", MagicMock())\n\t    def test_create_dataflow_on_schedule(self):\n", "        export_obj = ExportDatasetToDataLandingZone(config= self.config, header= MagicMock())\n\t        export_obj.createDataFlowRunIfNotExists(dataset_id=\"test\", compression_type=\"gzip\", data_format=\"parquet\", export_path=\"test\", on_schedule=True, config_path=\"test\",entity_name=\"test\", initial_delay=0)\n\t    @patch('aepp.flowservice.FlowService.createConnection', MagicMock(return_value = base_connection))\n\t    @patch('aepp.flowservice.FlowService.getConnectionSpecIdFromName', MagicMock(return_value = connection_spec))\n\t    @patch('aepp.flowservice.FlowService.createSourceConnection', MagicMock(return_value = source_connection))\n\t    @patch('aepp.flowservice.FlowService.createTargetConnection', MagicMock(return_value = target_connection))\n\t    @patch('aepp.flowservice.FlowService.createFlow', MagicMock(return_value = flow_response))\n\t    @patch('aepp.utils.Utils.save_field_in_config', MagicMock())\n\t    @patch(\"aepp.connector.AdobeRequest\", MagicMock())\n\t    def test_create_dataflow_if_not_exist(self):\n", "        export_obj = ExportDatasetToDataLandingZone(config= self.config, header= MagicMock())\n\t        return_dataflow_id = export_obj.createDataFlow(dataset_id=\"test\", compression_type=\"gzip\", data_format=\"parquet\", export_path=\"test\", on_schedule=False, config_path=\"test\", entity_name=\"test\")\n\t        assert (return_dataflow_id == self.flow_response[\"id\"])\n\t    @patch('aepp.destinationinstanceservice.DestinationInstanceService.createAdHocDatasetExport', MagicMock(return_value = adhoc_success_response))\n\t    @patch(\"aepp.connector.AdobeRequest\", MagicMock())\n\t    def test_retry_on_success_response(self):\n\t        export_obj = ExportDatasetToDataLandingZone(config= self.config, header= MagicMock())\n\t        assert(export_obj.retryOnNotReadyException(\"test\", \"test\", 1, 1) == self.adhoc_success_response)\n\t    @patch('aepp.destinationinstanceservice.DestinationInstanceService.createAdHocDatasetExport', MagicMock(return_value = adhoc_non_retry_error))\n\t    @patch(\"aepp.connector.AdobeRequest\", MagicMock())\n", "    def test_no_retry_error(self):\n\t        export_obj = ExportDatasetToDataLandingZone(config= self.config, header= MagicMock())\n\t        assert (export_obj.retryOnNotReadyException(\"test\", \"test\", 1, 1) == self.adhoc_non_retry_error)\n\t    @patch('aepp.destinationinstanceservice.DestinationInstanceService.createAdHocDatasetExport', MagicMock(return_value = adhoc_retry_error))\n\t    @patch(\"aepp.connector.AdobeRequest\", MagicMock())\n\t    def test_retry_error(self):\n\t        export_obj = ExportDatasetToDataLandingZone(config= self.config, header= MagicMock())\n\t        try:\n\t            export_obj.retryOnNotReadyException(\"test\", \"test\", 1, 1)\n\t            self.fail(\"expect a retry error\")\n", "        except RetryError:\n\t            pass"]}
{"filename": "tests/dataaccess_test.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\tfrom aepp.schema import Schema\n", "import unittest\n\tfrom unittest.mock import patch, MagicMock\n\tclass DataAccessTest(unittest.TestCase):\n\t    def test_dataaccess_get_batch_files(self):\n\t        assert True\n\t    def test_dataaccess_get_batch_failed(self):\n\t        assert True\n\t    def test_dataaccess_get_batch_meta(self):\n\t        assert True\n\t    def test_dataaccess_getHeadFile(self):\n", "        assert True\n\t    def test_dataaccess_get_files(self):\n\t        assert True\n\t    def test_dataaccess_get_preview(self):\n\t        assert True\n\t    def test_dataaccess_get_resource(self):\n\t        assert True"]}
{"filename": "aepp/catalog.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from dataclasses import dataclass\n\tfrom aepp import connector\n\timport pandas as pd\n\tfrom copy import deepcopy\n\tfrom typing import Union\n\timport time\n\timport codecs\n\timport json\n\timport logging\n\tfrom itertools import zip_longest\n", "import re\n\tfrom .configs import ConnectObject\n\t@dataclass\n\tclass _Data:\n\t    def __init__(self):\n\t        self.table_names = {}\n\t        self.schema_ref = {}\n\t        self.ids = {}\n\tclass Catalog:\n\t    \"\"\"\n", "    Catalog class from the AEP API. This class helps you to find where the data are coming from in AEP.\n\t    More details here : https://www.adobe.io/apis/experienceplatform/home/api-reference.html#\n\t    It possess a data attribute that is containing information about your datasets. \n\t    Arguments:\n\t        config : OPTIONAL : config object in the config module (DO NOT MODIFY)\n\t        header : OPTIONAL : header object  in the config module (DO NOT MODIFY)\n\t        loggingObject : OPTIONAL : If you want to set logging capability for your actions.\n\t    kwargs:\n\t        kwargs value will update the header\n\t    \"\"\"\n", "    loggingEnabled = False\n\t    logger = None\n\t    def __init__(self,\n\t                config : Union[dict,ConnectObject]=aepp.config.config_object,\n\t                header : dict=aepp.config.header,\n\t                loggingObject:dict=None,\n\t                **kwargs):\n\t        if loggingObject is not None and sorted([\"level\",\"stream\",\"format\",\"filename\",\"file\"]) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n", "            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n", "                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config, \n", "            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger)\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n", "        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = aepp.config.endpoints['global']+aepp.config.endpoints[\"catalog\"]\n\t        self.data = _Data()\n\t    def getResource(self,endpoint:str=None,params:dict=None,format:str='json',save:bool=False,**kwargs)->dict:\n\t        \"\"\"\n\t        Template for requesting data with a GET method.\n\t        Arguments:\n\t            endpoint : REQUIRED : The URL to GET\n\t            params: OPTIONAL : dictionary of the params to fetch\n", "            format : OPTIONAL : Type of response returned. Possible values:\n\t                json : default\n\t                txt : text file\n\t                raw : a response object from the requests module\n\t        \"\"\"\n\t        if endpoint is None:\n\t            raise ValueError(\"Require an endpoint\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Using getResource with following format ({format}) to the following endpoint: {endpoint}\")\n\t        res = self.connector.getData(endpoint,params=params,format=format)\n", "        if save:\n\t            if format == 'json':\n\t                aepp.saveFile(module=\"catalog\",file=res,filename=f\"resource_{int(time.time())}\",type_file=\"json\",encoding=kwargs.get(\"encoding\",'utf-8'))\n\t            elif format == 'txt':\n\t                aepp.saveFile(module=\"catalog\",file=res,filename=f\"resource_{int(time.time())}\",type_file=\"txt\",encoding=kwargs.get(\"encoding\",'utf-8'))\n\t            else:\n\t                print(\"element is an object. Output is unclear. No save made.\\nPlease save this element manually\")\n\t        return res\n\t    def decodeStreamBatch(self,message:str)->dict:\n\t        \"\"\"\n", "        Decode the full txt batch via the codecs module.\n\t        Usually the full batch is returned by the getResource method with format == \"txt\".\n\t        Arguments:\n\t            message: REQUIRED : the text file return from the failed batch message.\n\t        return None when issue is raised\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting decodeStreamBatch\")\n\t        try: \n\t            decodeMessage = codecs.escape_decode(message)[0].decode().replace('\"body\":\"{','\"body\":{').replace('}\",\"header\":\"{','},\"header\":{').replace('}\",\"_errors\":\"{','},\"_errors\":{').replace('}\"','}')\n", "            return decodeMessage\n\t        except:\n\t            print(\"Issue decoding the message.\")\n\t            return None\n\t    def jsonStreamMessages(self,message:str,verbose:bool = False)->list:\n\t        \"\"\"\n\t        Try to create a list of dictionary messages from the decoded stream batch extracted from the decodeStreamBatch method.\n\t        Arguments:\n\t            message : REQUIRED : a decoded text file, usually returned from the decodeStreamBatch method\n\t            verbose : OPTIONAL : print errors and information on the decoding.\n", "        return None when issue is raised\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting jsonStreamMessages\")\n\t        try:\n\t            myList = []\n\t            myYield:iter = (line for line in message.split(\"\\n\"))\n\t            countLine,countErrors = 0,0\n\t            for element in myYield:\n\t                countLine +=1\n", "                try:\n\t                    myList.append(json.loads(element))\n\t                except Exception as e:\n\t                    countErrors+=1\n\t                    if verbose:\n\t                        print(e)\n\t            if verbose:\n\t                print(f\"error rate is {(countErrors/countLine)*100:.2f}%\")\n\t            return myList\n\t        except:\n", "            print(\"Issue creating a stream of messages.\")\n\t            if self.loggingEnabled:\n\t                self.logger.info(f\"Issue creating a stream of messages\")\n\t            return None\n\t    def getLastBatches(self,dataSetId:str=None)->list:\n\t        \"\"\"\n\t        Returns the last batch from a specific datasetId.\n\t        Arguments:\n\t            dataSetId : OPTIONAL : the datasetId to be retrieved the batch about\n\t        \"\"\"\n", "        path = \"/lastBatches\"\n\t        params = {}\n\t        if dataSetId is not None:\n\t            params['dataSetId'] = dataSetId\n\t        res = self.connector.getData(self.endpoint+path,params=params)\n\t        return res\n\t    def getBatches(self,limit:int=10, n_results:int=None,output:str='raw',**kwargs)->Union[pd.DataFrame,dict]:\n\t        \"\"\"\n\t        Retrieve a list of batches.\n\t        Arguments:\n", "            limit : Limit response to a specified positive number of objects. Ex. limit=10 (max = 100)\n\t            n_results : OPTIONAL :  number of result you want to get in total. (will loop)\n\t            output : OPTIONAL : Can be \"raw\" response (dict) or \"dataframe\".\n\t        Possible kwargs:\n\t            created : Filter by the Unix timestamp (in milliseconds) when this object was persisted.\n\t            createdAfter : Exclusively filter records created after this timestamp. \n\t            createdBefore : Exclusively filter records created before this timestamp.\n\t            start : Returns results from a specific offset of objects. This was previously called offset. (see next line)\n\t                offset : Will offset to the next limit (sort of pagination)        \n\t            updated : Filter by the Unix timestamp (in milliseconds) for the time of last modification.\n", "            createdUser : Filter by the ID of the user who created this object.\n\t            dataSet : Used to filter on the related object: &dataSet=dataSetId.\n\t            version : Filter by Semantic version of the account. Updated when the object is modified.\n\t            status : Filter by the current (mutable) status of the batch.\n\t            orderBy : Sort parameter and direction for sorting the response. \n\t                Ex. orderBy=asc:created,updated. This was previously called sort.\n\t            properties : A comma separated whitelist of top-level object properties to be returned in the response. \n\t                Used to cut down the number of properties and amount of data returned in the response bodies.\n\t            size : The number of bytes processed in the batch.\n\t        # /Batches/get_batch\n", "        more details : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n\t        \"\"\"\n\t        path = \"/batches\"\n\t        limit = min([limit,100])\n\t        params = {'limit':limit,**kwargs}\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getBatches with output format {output}\")\n\t        ## looping to retrieve pagination\n\t        if n_results is not None:\n\t            list_return = {}\n", "            params['start'] = 0\n\t            res = self.connector.getData(self.endpoint+path,\n\t                            headers=self.header, params=params)\n\t            list_return.update(**res)\n\t            while len(list_return) < n_results and len(res) != 0:\n\t                params['start'] += limit\n\t                res = self.connector.getData(self.endpoint+path,\n\t                            headers=self.header, params=params)\n\t                list_return.update(**res)\n\t            if output==\"dataframe\":\n", "                return pd.DataFrame(list_return).T\n\t            return list_return\n\t        res = self.connector.getData(self.endpoint+path,\n\t                            headers=self.header, params=params)\n\t        if output==\"dataframe\":\n\t            return pd.DataFrame(res).T\n\t        return res\n\t    def getFailedBatchesDF(self,limit:int=10,n_results: str=None,orderBy:str=\"desc:created\",**kwargs)->pd.DataFrame:\n\t        \"\"\"\n\t        Abstraction of getBatches method that focus on failed batches and return a dataframe with the batchId and errors.\n", "        Also adding some meta data information from the batch information provided.\n\t        Arguments:\n\t            limit : Limit response to a specified positive number of objects. Ex. limit=10 (max = 100)\n\t            n_results : OPTIONAL :  number of result you want to get in total. (will loop)\n\t            orderBy : OPTIONAL : The order of the batch. Default \"desc:created\"\n\t        Possible kwargs: Any additional parameter for filtering the requests\n\t        \"\"\"\n\t        res = self.getBatches(status=\"failed\",orderBy=orderBy,limit=limit,n_results=n_results,**kwargs)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFailedBatchesDF\")\n", "        dict_failed = {}\n\t        for batch in res:\n\t            if res.get(batch,{}).get('relatedObjects',[{'type':'unknown'}])[0]['type'] == \"dataSet\":\n\t                datasetId = res[batch]['relatedObjects'][0]['id']\n\t            dict_failed[batch] = {\n\t                \"timestamp\" : res[batch]['created'],\n\t                \"recordsSize\" : res[batch].get('metrics',{}).get('recordsSize',0),\n\t                \"invalidRecordsProfile\" : res[batch].get('metrics',{}).get('invalidRecordsProfile',0),\n\t                \"invalidRecordsIdentity\" : res[batch].get('metrics',{}).get('invalidRecordsIdentity',0),\n\t                \"invalidRecordCount\" : res[batch].get('metrics',{}).get('invalidRecordCount',0),\n", "                \"invalidRecordsStreamingValidation\" : res[batch].get('metrics',{}).get('invalidRecordsStreamingValidation',0),\n\t                \"invalidRecordsMapper\" : res[batch].get('metrics',{}).get('invalidRecordsMapper',0),\n\t                \"invalidRecordsUnknown\" : res[batch].get('metrics',{}).get('invalidRecordsUnknown',0),\n\t                \"errorCode\" : [er['code'] for er in res[batch]['errors']],\n\t                \"errorMessage\" : [er['description'] for er in res[batch]['errors']],\n\t                \"flowId\" : res[batch].get('tags',{}).get('flowId',[None])[0],\n\t                \"dataSetId\" : datasetId,\n\t                \"sandbox\" : res[batch]['sandboxId'],\n\t            }\n\t        df = pd.DataFrame(dict_failed).T\n", "        return df\n\t    def getBatch(self, batch_id: str = None)->dict:\n\t        \"\"\"\n\t        Get a specific batch id.\n\t        Arguments:\n\t            batch_id : REQUIRED : batch ID to be retrieved.\n\t        \"\"\"\n\t        if batch_id is None:\n\t            raise Exception(\"batch_id parameter is required.\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getBatch for the following batch : {batch_id}\")\n\t        path = f\"/batches/{batch_id}\"\n\t        res = self.connector.getData(self.endpoint+path,\n\t                            headers=self.header)\n\t        return res\n\t    def createBatch(self, object:dict=None,**kwargs) -> dict:\n\t        \"\"\"\n\t        Create a new batch.\n\t        Arguments:\n\t            object : REQUIRED : Object that define the data to be onboarded.\n", "                see reference here: https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Batches/postBatch\n\t        \"\"\"\n\t        if object is None:\n\t            raise Exception('expecting a definition of the data to be uploaded.')\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createBatch\")\n\t        path = \"/batches\"\n\t        res = self.connector.postData(self.endpoint+path,data=object,\n\t                            headers=self.header)\n\t        return res\n", "    def getResources(self, **kwargs)->list:\n\t        \"\"\"\n\t        Retrieve a list of resource links for the Catalog Service.\n\t        Possible kwargs:\n\t            limit : Limit response to a specified positive number of objects. Ex. limit=10\n\t            orderBy : Sort parameter and direction for sorting the response. \n\t                Ex. orderBy=asc:created,updated. This was previously called sort.\n\t            property : A comma separated whitelist of top-level object properties to be returned in the response. \n\t                Used to cut down the number of properties and amount of data returned in the response bodies.\n\t        \"\"\"\n", "        path = \"/\"\n\t        params = {**kwargs}\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getResources\")\n\t        res = self.connector.getData(self.endpoint+path,\n\t                            headers=self.header, params=params)\n\t        return res\n\t    def getDataSets(self,limit:int=100,output:str=\"raw\",**kwargs)->dict:\n\t        \"\"\"\n\t        Return a list of a datasets.\n", "        Arguments:\n\t            limit : REQUIRED : amount of dataset to be retrieved per call. \n\t            output : OPTIONAL : Default is \"raw\", other option is \"df\" for dataframe output\n\t        Possible kwargs:\n\t            state : The state related to a dataset.\n\t            created : Filter by the Unix timestamp (in milliseconds) when this object was persisted.\n\t            updated : Filter by the Unix timestamp (in milliseconds) for the time of last modification.\n\t            name : Filter by the a descriptive, human-readable name for this DataSet.\n\t            namespace : One of the registered platform acronyms that identify the platform.\n\t            version : Filter by Semantic version of the account. Updated when the object is modified.\n", "            property : Regex used to filter objects in the response. Ex. property=name~^test.\n\t            # /Datasets/get_data_sets\n\t            more possibilities : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n\t        \"\"\"\n\t        path = \"/dataSets\"\n\t        params = {\"limit\":limit,**kwargs}\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataSets\")\n\t        res = self.connector.getData(self.endpoint+path, params=params)\n\t        data = deepcopy(res)\n", "        ## prepare pagination if needed\n\t        start = 1\n\t        while len(res) == limit:\n\t            start +=limit\n\t            params = {\"limit\":limit,\"start\":start,**kwargs}\n\t            res = self.connector.getData(self.endpoint+path, params=params)\n\t            data.update(res)\n\t        try:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"Starting caching results\")\n", "            self.data.table_names = {\n\t                data[key]['name']: data[key]['tags']['adobe/pqs/table'] for key in data}\n\t            self.data.schema_ref = {\n\t                data[key]['name']: data[key]['schemaRef']\n\t                for key in data if 'schemaRef' in data[key].keys()\n\t            }\n\t            self.data.ids = {\n\t                data[key]['name']: key for key in data\n\t            }\n\t        except Exception as e:\n", "            if self.loggingEnabled:\n\t                self.logger.warning(f\"Error caching results : {e}\")\n\t            print(e)\n\t            print(\"Couldn't populate the data object from the instance.\")\n\t        if output == \"df\":\n\t            df = pd.DataFrame(data).T\n\t            return df\n\t        return data\n\t    def createDataSets(self, \n\t                data: dict = None,\n", "                name:str=None, \n\t                schemaId:str=None, \n\t                profileEnabled:bool=False,\n\t                identityEnabled:bool=False,\n\t                upsert:bool=False,\n\t                tags:dict=None,\n\t                systemLabels:list[str]=None,\n\t                **kwargs)-> dict:\n\t        \"\"\"\n\t        Create a new dataSets based either on preconfigured setup or by passing the full dictionary for creation.\n", "        Arguments:\n\t            data : REQUIRED : If you want to pass the dataset object directly (not require the name and schemaId then)\n\t                more info: https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Datasets/postDataset\n\t            name : REQUIRED : if you wish to create a dataset via autocompletion. Provide a name.\n\t            schemaId : REQUIRED : The schema $id reference for creating your dataSet.\n\t            profileEnabled : OPTIONAL : If the dataset to be created with profile enbaled\n\t            identityEnabled : OPTIONAL : If the dataset should create new identities\n\t            upsert : OPTIONAL : If the dataset to be created with profile enbaled and Upsert capability.\n\t            tags : OPTIONAL : set of attribute to add as tags.\n\t            systemLabels : OPTIONAL : A list of string to attribute system based label on creation.\n", "        possible kwargs\n\t            requestDataSource : Set to true if you want Catalog to create a dataSource on your behalf; otherwise, pass a dataSourceId in the body.\n\t        \"\"\"\n\t        path = \"/dataSets\"\n\t        params = {\"requestDataSource\": kwargs.get(\"requestDataSource\", False)}\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDataSets\")\n\t        if data is not None or isinstance(data, dict) == True:\n\t            res = self.connector.postData(self.endpoint+path, params=params,\n\t                             data=data)\n", "        elif name is not None and schemaId is not None:\n\t            data = {\n\t                \"name\":name,\n\t                \"schemaRef\": {\n\t                    \"id\": schemaId,\n\t                    \"contentType\": \"application/vnd.adobe.xed+json;version=1\"\n\t                },\n\t                \"fileDescription\": {\n\t                    \"persisted\": True,\n\t                    \"containerFormat\": \"parquet\",\n", "                    \"format\": \"parquet\"\n\t                },\n\t                \"tags\" : {}\n\t            }\n\t            if profileEnabled:\n\t                data['tags'][\"unifiedProfile\"] = [\"enabled: true\"]\n\t            if identityEnabled:\n\t                data['tags'][\"unifiedIdentity\"] = [\"enabled: true\"]\n\t            if upsert:\n\t                data['tags']['unifiedProfile'] = [\"enabled: true\",\"isUpsert: true\"]\n", "            if tags is not None and type(tags) == dict:\n\t                for key in tags:\n\t                    data['tags'][key] = tags[key]\n\t            if systemLabels is not None and type(systemLabels) == list:\n\t                data[\"systemLabels\"] = systemLabels\n\t            res = self.connector.postData(self.endpoint+path, params=params,\n\t                             data=data)\n\t        return res\n\t    def getDataSet(self, datasetId: str = None) -> dict:\n\t        \"\"\"\n", "        Return a single dataset.\n\t        Arguments:\n\t            datasetId : REQUIRED : Id of the dataset to be retrieved.\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise Exception(\"Expected a datasetId argument\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataset for : {datasetId}\")\n\t        path = f\"/dataSets/{datasetId}\"\n\t        res = self.connector.getData(self.endpoint+path, headers=self.header)\n", "        return res\n\t    def getDataSetObservableSchema(self, datasetId: str = None,appendDatasetInfo:bool=False) -> dict:\n\t        \"\"\"\n\t        Return a single dataset observable schema.\n\t        Which means that the fields that has been used in that dataset.\n\t        Arguments:\n\t            datasetId : REQUIRED : Id of the dataset for which the observable schema should be retrieved.\n\t            appendDatasetInfo : OPTIONAL : If set to True, it will append the \"datasetId\" into the dictionary return\n\t        \"\"\"\n\t        if datasetId is None:\n", "            raise Exception(\"Expected a datasetId argument\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataset for : {datasetId}\")\n\t        path = f\"/dataSets/{datasetId}\"\n\t        params = {\"properties\" : \"observableSchema\"}\n\t        res = self.connector.getData(self.endpoint+path,params=params, headers=self.header)\n\t        data = res[list(res.keys())[0]] ## accessing the observableSchema\n\t        if appendDatasetInfo:\n\t            data['datasetId'] = datasetId\n\t        return data\n", "    def deleteDataSet(self, datasetId: str = None) -> None:\n\t        \"\"\"\n\t        Delete a dataset by its id.\n\t        Arguments:\n\t            datasetId : REQUIRED : Id of the dataset to be deleted.\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise Exception(\"Expected a datasetId argument\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteDataset for : {datasetId}\")\n", "        path = f\"/dataSets/{datasetId}\"\n\t        res = self.connector.deleteData(self.endpoint+path, headers=self.header)\n\t        return res\n\t    ## Apparently deprecated.\n\t    def getDataSetViews(self, datasetId: str = None, **kwargs):\n\t        \"\"\"\n\t        Get views of the datasets.\n\t        Arguments:\n\t            datasetId : REQUIRED : Id of the dataset to be looked down.\n\t        Possible kwargs:\n", "            limit : Limit response to a specified positive number of objects. Ex. limit=10\n\t            orderBy : Sort parameter and direction for sorting the response. Ex. orderBy=asc:created,updated.\n\t            start : Returns results from a specific offset of objects. This was previously called offset. Ex. start=3.\n\t            property : Regex used to filter objects in the response. Ex. property=name~^test.\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise Exception(\"Expected a datasetId argument\")\n\t        path = f\"/dataSets/{datasetId}/views\"\n\t        params = {**kwargs}\n\t        res = self.connector.getData(self.endpoint+path, headers=self.header)\n", "        return res\n\t    def getDataSetView(self, datasetId: str = None, viewId: str = None):\n\t        \"\"\"\n\t        Get a specific view on a specific dataset.\n\t        Arguments:\n\t            datasetId : REQUIRED : ID of the dataset to be looked down.\n\t            viewId : REQUIRED : ID of the view to be look upon.\n\t        \"\"\"\n\t        if datasetId is None or viewId is None:\n\t            raise Exception(\"Expected a datasetId and an viewId argument\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataSetView for datasetId: {datasetId} & viewId: {viewId}\")\n\t        path = f\"/dataSets/{datasetId}/views/{viewId}\"\n\t        res = self.connector.getData(self.endpoint+path, headers=self.header)\n\t        return res\n\t    def getDataSetViewFiles(self, datasetId: str = None, viewId: str = None):\n\t        \"\"\"\n\t        Returns the list of files attached to a view in a Dataset.\n\t        Arguments:\n\t            datasetId : REQUIRED : ID of the dataset to be looked down.\n", "            viewId : REQUIRED : ID of the view to be look upon.\n\t        \"\"\"\n\t        if datasetId is None or viewId is None:\n\t            raise Exception(\"Expected a datasetId and an viewId argument\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataSetViewFiles for datasetId: {datasetId} & viewId: {viewId}\")\n\t        path = f\"/dataSets/{datasetId}/views/{viewId}/files\"\n\t        res = self.connector.getData(self.endpoint+path, headers=self.header)\n\t        return res\n\t    def enableDatasetProfile(self,datasetId:str=None,upsert:bool=False)->dict:\n", "        \"\"\"\n\t        Enable a dataset for profile with upsert.\n\t        Arguments:\n\t            datasetId : REQUIRED : Dataset ID to be enabled for profile\n\t            upsert : OPTIONAL : If you wish to enabled the dataset for upsert.\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise ValueError(\"Require a datasetId\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting enableDatasetProfile for datasetId: {datasetId}\")\n", "        path = f\"/dataSets/{datasetId}\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader['Content-Type'] = \"application/json-patch+json\"\n\t        data = [\n\t            { \n\t                \"op\": \"add\", \n\t                \"path\": \"/tags/unifiedProfile\",\n\t                \"value\": [\"enabled:true\"] }\n\t            ]\n\t        if upsert:\n", "            data[0]['value'] = [\"enabled:true\",\"isUpsert:true\"]\n\t        res = self.connector.patchData(self.endpoint+path, data=data,headers=privateHeader)\n\t        return res\n\t    def enableDatasetIdentity(self,datasetId:str=None)->dict:\n\t        \"\"\"\n\t        Enable a dataset for profile with upsert.\n\t        Arguments:\n\t            datasetId : REQUIRED : Dataset ID to be enabled for Identity\n\t        \"\"\"\n\t        if datasetId is None:\n", "            raise ValueError(\"Require a datasetId\")\n\t        path = f\"/dataSets/{datasetId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting enableDatasetIdentity for datasetId: {datasetId}\")\n\t        data = [\n\t            { \n\t                \"op\": \"add\", \n\t                \"path\": \"/tags/unifiedIdentity\",\n\t                \"value\": [\"enabled:true\"] }\n\t            ]\n", "        privateHeader = deepcopy(self.header)\n\t        privateHeader['Content-Type'] = \"application/json-patch+json\"\n\t        res = self.connector.patchData(self.endpoint+path, data=data,headers=privateHeader)\n\t        return res\n\t    def disableDatasetProfile(self,datasetId: str = None)->dict:\n\t        \"\"\"\n\t        Disable the dataset for Profile ingestion.\n\t        Arguments:\n\t            datasetId : REQUIRED : Dataset ID to be disabled for profile\n\t        \"\"\"\n", "        path = f\"/dataSets/{datasetId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting disableDatasetProfile for datasetId: {datasetId}\")\n\t        data = [\n\t            { \n\t                \"op\": \"replace\", \n\t                \"path\": \"/tags/unifiedProfile\",\n\t                \"value\": [\"enabled:false\"] }\n\t            ]\n\t        res = self.connector.patchData(self.endpoint+path, data=data)\n", "        return res\n\t    def disableDatasetIdentity(self,datasetId:str=None)->dict:\n\t        \"\"\"\n\t        Enable a dataset for profile with upsert.\n\t        Arguments:\n\t            datasetId : REQUIRED : Dataset ID to be disabled for Identity\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise ValueError(\"Require a datasetId\")\n\t        path = f\"/dataSets/{datasetId}\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting disableDatasetIdentity for datasetId: {datasetId}\")\n\t        data = [\n\t            { \n\t                \"op\": \"add\", \n\t                \"path\": \"/tags/unifiedIdentity\",\n\t                \"value\": [\"enabled:false\"] }\n\t            ]\n\t        res = self.connector.patchData(self.endpoint+path, data=data)\n\t        return res\n", "    def createUnionProfileDataset(self)->dict:\n\t        \"\"\"\n\t        Create a dataset with an union Profile schema.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createUnionProfileDataset\")\n\t        path = \"/dataSets/\"\n\t        data = {\n\t        \"name\": \"Profile Data Export\",\n\t        \"schemaRef\": {\n", "          \"id\": \"https://ns.adobe.com/xdm/context/profile__union\",\n\t          \"contentType\": \"application/vnd.adobe.xed+json;version=1\"\n\t            }\n\t        }\n\t        res = self.connector.postData(self.endpoint+path, data=data)\n\t        return res\n\t    def getMapperErrors(self,limit:int=100,n_results:str=None,**kwargs)->pd.DataFrame:\n\t        \"\"\"\n\t        Get failed batches for Mapper errors, based on error code containing \"MAPPER\".\n\t        Arguments:\n", "            limit : OPTIONAL : Number of results per requests\n\t            n_results : OPTIONAL : Total number of results wanted.\n\t        Possible kwargs:\n\t            created : Filter by the Unix timestamp (in milliseconds) when this object was persisted.\n\t            createdAfter : Exclusively filter records created after this timestamp. \n\t            createdBefore : Exclusively filter records created before this timestamp.\n\t            start : Returns results from a specific offset of objects. This was previously called offset. (see next line)\n\t                offset : Will offset to the next limit (sort of pagination)        \n\t            updated : Filter by the Unix timestamp (in milliseconds) for the time of last modification.\n\t            createdUser : Filter by the ID of the user who created this object.\n", "            dataSet : Used to filter on the related object: &dataSet=dataSetId.\n\t            version : Filter by Semantic version of the account. Updated when the object is modified.\n\t            status : Filter by the current (mutable) status of the batch.\n\t            orderBy : Sort parameter and direction for sorting the response. \n\t                Ex. orderBy=asc:created,updated. This was previously called sort.\n\t            properties : A comma separated whitelist of top-level object properties to be returned in the response. \n\t                Used to cut down the number of properties and amount of data returned in the response bodies.\n\t            size : The number of bytes processed in the batch.\n\t        \"\"\"\n\t        df = self.getFailedBatchesDF(limit=limit,n_results=n_results,**kwargs)\n", "        df['errorCodeStr'] = df['errorCode'].astype(str)\n\t        df_mapper = df[df['errorCodeStr'].str.contains('DPMAP')]\n\t        del df_mapper[\"errorCodeStr\"]\n\t        dict_result = {}\n\t        for index,row in df_mapper.iterrows():\n\t            errorCodes = []\n\t            errorMessages = []\n\t            destinationPaths = []\n\t            expectedTypes = []\n\t            actualTypes = []\n", "            sourceFields = []\n\t            destinationFields = []\n\t            for code,message in zip_longest(row['errorCode'],row['errorMessage']):\n\t                if 'MAPPER' in code:\n\t                    errorMessages.append(message)\n\t                    errorCodes.append(code)\n\t                    matchDestPath = re.search('destination path (.+?)\\. ',message)\n\t                    if matchDestPath:\n\t                        destinationPaths.append(matchDestPath.group(1))\n\t                    matchExpectedTypes = re.search('expected data type was: ([A-Z]+?), ',message)\n", "                    if matchExpectedTypes:\n\t                        expectedTypes.append(matchExpectedTypes.group(1))\n\t                    matchActualTypes = re.search('actual data type was: ([A-Z]+?)\\. ',message)\n\t                    if matchActualTypes:\n\t                        actualTypes.append(matchActualTypes.group(1))\n\t                    matchSourceField = re.search('sourceField: (.+?) destinationField',message)\n\t                    if matchSourceField:\n\t                        sourceFields.append(matchSourceField.group(1))\n\t                    matchDestinationField = re.search('destinationField: (.+?)$',message)\n\t                    if matchDestinationField:\n", "                        destinationFields.append(matchDestinationField.group(1))\n\t            for message,code,destPath,expType,actType,source,dest in zip_longest(errorMessages,errorCodes,destinationPaths,expectedTypes,actualTypes,sourceFields,destinationFields):\n\t                dict_result[f\"{row.name}-{code}\"] = {\n\t                    \"timestamp\" : row['timestamp'],\n\t                    \"batchId\" : row.name,\n\t                    \"datasetId\": row['dataSetId'],\n\t                    \"flowId\":row[\"flowId\"],\n\t                    \"invalidRecordCount\" : row[\"invalidRecordCount\"],\n\t                    \"invalidRecordsMapper\": row[\"invalidRecordsMapper\"],\n\t                    \"errorCode\" : code,\n", "                    \"errorMessage\":message,\n\t                    \"destinationPath\" : destPath,\n\t                    \"expectedType\":expType,\n\t                    \"actualType\":actType,\n\t                    \"sourceField\":source,\n\t                    \"destinationField\":dest\n\t                }\n\t        df_final = pd.DataFrame(dict_result).T\n\t        return df_final\n\tclass ObservableSchemaManager:\n", "    def __init__(self,observableSchema:dict=None)->None:\n\t        \"\"\"\n\t        Arguments:\n\t            observableSchema : dictionary of the data stored in the \"observableSchema\" key\n\t        \"\"\"\n\t        if 'observableSchema' in observableSchema.keys():\n\t            self.observableSchema = observableSchema['observableSchema']\n\t        else:\n\t            self.observableSchema = observableSchema\n\t        self.schemaId = self.observableSchema.get('$id')\n", "        self.title = self.observableSchema.get('title')\n\t    def __str__(self)->str:\n\t        return json.dumps(self.observableSchema,indent=2)\n\t    def __repr__(self)->dict:\n\t        return json.dumps(self.observableSchema,indent=2)\n\t    def __simpleDeepMerge__(self,base:dict,append:dict)->dict:\n\t        \"\"\"\n\t        Loop through the keys of 2 dictionary and append the new found key of append to the base.\n\t        Arguments:\n\t            base : The base you want to extend\n", "            append : the new dictionary to append\n\t        \"\"\"\n\t        if type(append) == list:\n\t            append = append[0]\n\t        for key in append:\n\t            if type(base)==dict:\n\t                if key in base.keys():\n\t                    self.__simpleDeepMerge__(base[key],append[key])\n\t                else:\n\t                    base[key] = append[key]\n", "            elif type(base)==list:\n\t                base = base[0]\n\t                if type(base) == dict:\n\t                    if key in base.keys():\n\t                        self.__simpleDeepMerge__(base[key],append[key])\n\t                    else:\n\t                        base[key] = append[key]\n\t        return base\n\t    def __accessorAlgo__(self,mydict:dict,path:list=None)->dict:\n\t        \"\"\"\n", "        recursive method to retrieve all the elements.\n\t        Arguments:\n\t            mydict : REQUIRED : The dictionary containing the elements to fetch (in \"properties\" key)\n\t            path : the path with dot notation.\n\t        \"\"\"\n\t        path = self.__cleanPath__(path)\n\t        pathSplit = path.split('.')\n\t        key = pathSplit[0]\n\t        if 'customFields' in mydict.keys():\n\t            level = self.__accessorAlgo__(mydict.get('customFields',{}).get('properties',{}),'.'.join(pathSplit))\n", "            if 'error' not in level.keys():\n\t                return level\n\t        if 'property' in mydict.keys() :\n\t            level = self.__accessorAlgo__(mydict.get('property',{}).get('properties',{}),'.'.join(pathSplit))\n\t            return level\n\t        level = mydict.get(key,None)\n\t        if level is not None:\n\t            if level[\"type\"] == \"object\":\n\t                levelProperties = mydict[key].get('properties',None)\n\t                if levelProperties is not None:\n", "                    level = self.__accessorAlgo__(levelProperties,'.'.join(pathSplit[1:]))\n\t                return level\n\t            elif level[\"type\"] == \"array\":\n\t                levelProperties = mydict[key]['items'].get('properties',None)\n\t                if levelProperties is not None:\n\t                    level = self.__accessorAlgo__(levelProperties,'.'.join(pathSplit[1:]))\n\t                return level\n\t            else:\n\t                if len(pathSplit) > 1: \n\t                    return {'error':f'cannot find the key \"{pathSplit[1]}\"'}\n", "                return level\n\t        else:\n\t            if key == \"\":\n\t                return mydict\n\t            return {'error':f'cannot find the key \"{key}\"'}\n\t    def __searchAlgo__(self,mydict:dict,string:str=None,partialMatch:bool=False,caseSensitive:bool=False,results:list=None,path:str=None,completePath:str=None)->list:\n\t        \"\"\"\n\t        recursive method to retrieve all the elements.\n\t        Arguments:\n\t            mydict : REQUIRED : The dictionary containing the elements to fetch (start with fieldGroup definition)\n", "            string : the string to look for with dot notation.\n\t            partialMatch : if you want to use partial match\n\t            caseSensitive : to see if we should lower case everything\n\t            results : the list of results to return\n\t            path : the path currently set\n\t            completePath : the complete path from the start.\n\t        \"\"\"\n\t        finalPath = None\n\t        if results is None:\n\t            results=[]\n", "        for key in mydict:\n\t            if caseSensitive == False:\n\t                keyComp = key.lower()\n\t                string = string.lower()\n\t            else:\n\t                keyComp = key\n\t                string = string\n\t            if partialMatch:\n\t                if string in keyComp:\n\t                    ### checking if element is an array without deeper object level\n", "                    if mydict[key].get('type') == 'array' and mydict[key]['items'].get('properties',None) is None:\n\t                        finalPath = path + f\".{key}[]\"\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = f\"{key}\"\n\t                    else:\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n", "                            finalPath = f\"{key}\"\n\t                    value = deepcopy(mydict[key])\n\t                    value['path'] = finalPath\n\t                    value['queryPath'] = self.__cleanPath__(finalPath)\n\t                    if completePath is None:\n\t                        value['completePath'] = f\"/definitions/{key}\"\n\t                    else:\n\t                        value['completePath'] = completePath + \"/\" + key\n\t                    results.append({key:value})\n\t            else:\n", "                if caseSensitive == False:\n\t                    if keyComp == string:\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = key\n\t                        value = deepcopy(mydict[key])\n\t                        value['path'] = finalPath\n\t                        value['queryPath'] = self.__cleanPath__(finalPath)\n\t                        if completePath is None:\n", "                            value['completePath'] = f\"/definitions/{key}\"\n\t                        else:\n\t                            value['completePath'] = completePath + \"/\" + key\n\t                        results.append({key:value})\n\t                else:\n\t                    if keyComp == string:\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = key\n", "                        value = deepcopy(mydict[key])\n\t                        value['path'] = finalPath\n\t                        value['queryPath'] = self.__cleanPath__(finalPath)\n\t                        if completePath is None:\n\t                            value['completePath'] = f\"/definitions/{key}\"\n\t                        else:\n\t                            value['completePath'] = completePath + \"/\" + key\n\t                        results.append({key:value})\n\t            ## loop through keys\n\t            if mydict[key].get(\"type\") == \"object\" or 'properties' in mydict[key].keys():\n", "                levelProperties = mydict[key].get('properties',{})\n\t                if levelProperties != dict():\n\t                    if completePath is None:\n\t                        tmp_completePath = f\"/definitions/{key}\"\n\t                    else:\n\t                        tmp_completePath = f\"{completePath}/{key}\"\n\t                    tmp_completePath += f\"/properties\"\n\t                    if path is None:\n\t                        if key != \"property\" and key != \"customFields\" :\n\t                            tmp_path = key\n", "                        else:\n\t                            tmp_path = None\n\t                    else:\n\t                        tmp_path = f\"{path}.{key}\"\n\t                    results = self.__searchAlgo__(levelProperties,string,partialMatch,caseSensitive,results,tmp_path,tmp_completePath)\n\t            elif mydict[key].get(\"type\") == \"array\":\n\t                levelProperties = mydict[key]['items'].get('properties',{})\n\t                if levelProperties != dict():\n\t                    if completePath is None:\n\t                        tmp_completePath = f\"/definitions/{key}\"\n", "                    else:\n\t                        tmp_completePath = f\"{completePath}/{key}\"\n\t                    tmp_completePath += f\"/items/properties\"\n\t                    if levelProperties is not None:\n\t                        if path is None:\n\t                            if key != \"property\" and key != \"customFields\":\n\t                                tmp_path = key\n\t                            else:\n\t                                tmp_path = None\n\t                        else:\n", "                            tmp_path = f\"{path}.{key}[]{{}}\"\n\t                        results = self.__searchAlgo__(levelProperties,string,partialMatch,caseSensitive,results,tmp_path,tmp_completePath)\n\t        return results\n\t    def __transformationDict__(self,mydict:dict=None,typed:bool=False,dictionary:dict=None)->dict:\n\t        \"\"\"\n\t        Transform the current XDM schema to a dictionary.\n\t        \"\"\"\n\t        if dictionary is None:\n\t            dictionary = {}\n\t        else:\n", "            dictionary = dictionary\n\t        for key in mydict:\n\t            if type(mydict[key]) == dict:\n\t                if mydict[key].get('type') == 'object' or 'properties' in mydict[key].keys():\n\t                    properties = mydict[key].get('properties',None)\n\t                    if properties is not None:\n\t                        if key != \"property\" and key != \"customFields\":\n\t                            if key not in dictionary.keys():\n\t                                dictionary[key] = {}\n\t                            self.__transformationDict__(mydict[key]['properties'],typed,dictionary=dictionary[key])\n", "                        else:\n\t                            self.__transformationDict__(mydict[key]['properties'],typed,dictionary=dictionary)\n\t                elif mydict[key].get('type') == 'array':\n\t                    levelProperties = mydict[key]['items'].get('properties',None)\n\t                    if levelProperties is not None:\n\t                        dictionary[key] = [{}]\n\t                        self.__transformationDict__(levelProperties,typed,dictionary[key][0])\n\t                    else:\n\t                        if typed:\n\t                            dictionary[key] = [mydict[key]['items'].get('type','object')]\n", "                        else:\n\t                            dictionary[key] = []\n\t                else:\n\t                    if typed:\n\t                        dictionary[key] = mydict[key].get('type','object')\n\t                    else:\n\t                        dictionary[key] = \"\"\n\t        return dictionary \n\t    def __transformationDF__(self,mydict:dict=None,dictionary:dict=None,path:str=None,queryPath:bool=False,description:bool=False,xdmType:bool=False)->dict:\n\t        \"\"\"\n", "        Transform the current XDM schema to a dictionary.\n\t        Arguments:\n\t            mydict : the fieldgroup\n\t            dictionary : the dictionary that gather the paths\n\t            path : path that is currently being developed\n\t            queryPath: boolean to tell if we want to add the query path\n\t            description : boolean to tell if you want to retrieve the description\n\t            xdmType : boolean to know if you want to retrieve the xdm Type\n\t        \"\"\"\n\t        if dictionary is None:\n", "            dictionary = {'path':[],'type':[]}\n\t            if queryPath:\n\t                dictionary['querypath'] = []\n\t            if description:\n\t                dictionary['description'] = []\n\t        else:\n\t            dictionary = dictionary\n\t        for key in mydict:\n\t            if type(mydict[key]) == dict:\n\t                if mydict[key].get('type') == 'object':\n", "                    if path is None:\n\t                        if key != \"property\" and key != \"customFields\":\n\t                            tmp_path = key\n\t                        else:\n\t                            tmp_path = None\n\t                    else:\n\t                        tmp_path = f\"{path}.{key}\"\n\t                    if tmp_path is not None:\n\t                        dictionary[\"path\"].append(tmp_path)\n\t                        dictionary[\"type\"].append(f\"{mydict[key].get('type')}\")\n", "                        if queryPath:\n\t                            dictionary[\"querypath\"].append(self.__cleanPath__(tmp_path))\n\t                        if description:\n\t                            dictionary[\"description\"].append(f\"{mydict[key].get('description','')}\")\n\t                    properties = mydict[key].get('properties',None)\n\t                    if properties is not None:\n\t                        self.__transformationDF__(properties,dictionary,tmp_path,queryPath,description)\n\t                elif mydict[key].get('type') == 'array':\n\t                    levelProperties = mydict[key]['items'].get('properties',None)\n\t                    if levelProperties is not None:\n", "                        if path is None:\n\t                            tmp_path = key\n\t                        else :\n\t                            tmp_path = f\"{path}.{key}[]{{}}\"\n\t                        dictionary[\"path\"].append(tmp_path)\n\t                        dictionary[\"type\"].append(f\"[{mydict[key]['items'].get('type')}]\")\n\t                        if queryPath and tmp_path is not None:\n\t                            dictionary[\"querypath\"].append(self.__cleanPath__(tmp_path))\n\t                        if description and tmp_path is not None:\n\t                            dictionary[\"description\"].append(mydict[key]['items'].get('description',''))\n", "                        self.__transformationDF__(levelProperties,dictionary,tmp_path,queryPath,description)\n\t                    else:\n\t                        finalpath = f\"{path}.{key}\"\n\t                        dictionary[\"path\"].append(finalpath)\n\t                        dictionary[\"type\"].append(f\"[{mydict[key]['items'].get('type')}]\")\n\t                        if queryPath and finalpath is not None:\n\t                            dictionary[\"querypath\"].append(self.__cleanPath__(finalpath))\n\t                        if description and finalpath is not None:\n\t                            dictionary[\"description\"].append(mydict[key]['items'].get('description',''))\n\t                else:\n", "                    if path is not None:\n\t                        finalpath = f\"{path}.{key}\"\n\t                    else:\n\t                        finalpath = f\"{key}\"\n\t                    dictionary[\"path\"].append(finalpath)\n\t                    dictionary[\"type\"].append(mydict[key].get('type','object'))\n\t                    if queryPath and finalpath is not None:\n\t                        dictionary[\"querypath\"].append(self.__cleanPath__(finalpath))\n\t                    if description and finalpath is not None:\n\t                        dictionary[\"description\"].append(mydict[key].get('description',''))\n", "        return dictionary\n\t    def searchField(self,string:str=None,partialMatch:bool=True,caseSensitive:bool=False)-> dict:\n\t        \"\"\"\n\t        Search a field in the observable schema.\n\t        Arguments:\n\t            string : REQUIRED : the string to look for for one of the field\n\t            partialMatch : OPTIONAL : if you want to look for complete string or not. (default True)\n\t            caseSensitive : OPTIONAL : if you want to compare with case sensitivity or not. (default False)\n\t        \"\"\"\n\t        definition = self.observableSchema.get('properties',{})\n", "        data = self.__searchAlgo__(definition,string,partialMatch,caseSensitive)\n\t        return data\n\t    def to_dataframe(self,save:bool=False,queryPath:bool=False,description:bool=False)->pd.DataFrame:\n\t        \"\"\"\n\t        Generate a dataframe with the row representing each possible path.\n\t        Arguments:\n\t            save : OPTIONAL : If you wish to save it with the title used by the field group.\n\t                save as csv with the title used. Not title, used \"unknown_fieldGroup_\" + timestamp.\n\t            queryPath : OPTIONAL : If you want to have the query path to be used.\n\t            description : OPTIONAL : If you want to have the description used\n", "        \"\"\"\n\t        definition = self.observableSchema.get('properties',{})\n\t        data = self.__transformationDF__(definition,queryPath=queryPath,description=description)\n\t        df = pd.DataFrame(data)\n\t        if save:\n\t            title = self.observableSchema.get('title',f'unknown_fieldGroup_{str(int(time.time()))}')\n\t            df.to_csv(f\"{title}.csv\",index=False)\n\t        return df\n\t    def to_dict(self,typed:bool=True,save:bool=False)->dict:\n\t        \"\"\"\n", "        Generate a dictionary representing the field group constitution\n\t        Arguments:\n\t            typed : OPTIONAL : If you want the type associated with the field group to be given.\n\t            save : OPTIONAL : If you wish to save the dictionary in a JSON file\n\t        \"\"\"\n\t        definition = self.observableSchema.get('properties',{})\n\t        data = self.__transformationDict__(definition,typed)\n\t        if save:\n\t            filename = self.observableSchema.get('title',f'unknown_fieldGroup_{str(int(time.time()))}')\n\t            aepp.saveFile(module='catalog',file=data,filename=f\"{filename}.json\",type_file='json')\n", "        return data\n\t    def compareSchemaAvailability(self,schemaManager:'SchemaManager'=None)->dict:\n\t        \"\"\"\n\t        A method to compare the existing schema with the observable schema and find out the difference in them.\n\t        It output a dataframe with all of the path, the field group, the type (if provided) and the part availability (in that dataset)\n\t        Arguments:\n\t            SchemaManager : REQUIRED : the SchemaManager class instance for that schema.\n\t        \"\"\"\n\t        if schemaManager is None:\n\t            raise ValueError(\"Require a SchemaManager class instance\")\n", "        df_schema = schemaManager.to_dataframe()\n\t        df_obs = self.to_dataframe()\n\t        df_merge = df_schema.merge(df_obs,left_on='path',right_on='path',how='left',indicator=True)\n\t        df_merge = df_merge.rename(columns={\"_merge\": \"availability\",'type_x':'type'})\n\t        df_merge = df_merge.drop(\"type_y\",axis=1)\n\t        df_merge['availability'] = df_merge['availability'].str.replace('left_only','schema_only')\n\t        df_merge['availability'] = df_merge['availability'].str.replace('both','schema_dataset')\n\t        return df_merge"]}
{"filename": "aepp/schema.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "import aepp\n\tfrom dataclasses import dataclass\n\tfrom aepp import connector\n\tfrom copy import deepcopy\n\tfrom typing import Union\n\timport time\n\timport logging\n\timport pandas as pd\n\timport json\n\timport re\n", "from .configs import ConnectObject\n\tjson_extend = [\n\t    {\n\t        \"op\": \"replace\",\n\t        \"path\": \"/meta:intendedToExtend\",\n\t        \"value\": [\n\t            \"https://ns.adobe.com/xdm/context/profile\",\n\t            \"https://ns.adobe.com/xdm/context/experienceevent\",\n\t        ],\n\t    }\n", "]\n\t@dataclass\n\tclass _Data:\n\t    def __init__(self):\n\t        self.schemas = {}\n\t        self.schemas_id = {}\n\t        self.schemas_altId = {}\n\t        self.fieldGroups_id = {}\n\t        self.fieldGroups_altId = {}\n\t        self.fieldGroups = {}\n", "class Schema:\n\t    \"\"\"\n\t    This class is a wrapper around the schema registry API for Adobe Experience Platform.\n\t    More documentation on these endpoints can be found here :\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/schema-registry.yaml\n\t    When Patching a schema, you can use the PATCH_OBJ reference to help you.\n\t    \"\"\"\n\t    schemas = {}  # caching\n\t    ## logging capability\n\t    loggingEnabled = False\n", "    logger = None\n\t    _schemaClasses = {\n\t        \"event\": \"https://ns.adobe.com/xdm/context/experienceevent\",\n\t        \"profile\": \"https://ns.adobe.com/xdm/context/profile\",\n\t    }\n\t    PATCH_OBJ = [{\"op\": \"add\", \"path\": \"/meta:immutableTags-\", \"value\": \"union\"}]\n\t    DESCRIPTOR_TYPES =[\"xdm:descriptorIdentity\",\"xdm:alternateDisplayInfo\",\"xdm:descriptorOneToOne\",\"xdm:descriptorReferenceIdentity\",\"xdm:descriptorDeprecated\"]\n\t    def __init__(\n\t        self,\n\t        containerId: str = \"tenant\",\n", "        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header=aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Copy the token and header and initiate the object to retrieve schema elements.\n\t        Arguments:\n\t            containerId : OPTIONAL : \"tenant\"(default) or \"global\"\n\t            loggingObject : OPTIONAL : logging object to log messages.\n", "            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t        possible kwargs:\n\t            x-sandbox-name : name of the sandbox you want to use (default : \"prod\").\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n", "            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n", "                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n", "            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header[\"Accept\"] = \"application/vnd.adobe.xed+json\"\n\t        self.connector.header['Accept'] = \"application/vnd.adobe.xed+json\"\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n", "            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.header.update(**kwargs)\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"schemas\"]\n\t        )\n\t        self.container = containerId\n\t        self.data = _Data()\n", "    def getResource(\n\t        self,\n\t        endpoint: str = None,\n\t        params: dict = None,\n\t        format: str = \"json\",\n\t        save: bool = False,\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Template for requesting data with a GET method.\n", "        Arguments:\n\t            endpoint : REQUIRED : The URL to GET\n\t            params: OPTIONAL : dictionary of the params to fetch\n\t            format : OPTIONAL : Type of response returned. Possible values:\n\t                json : default\n\t                txt : text file\n\t                raw : a response object from the requests module\n\t        \"\"\"\n\t        if endpoint is None:\n\t            raise ValueError(\"Require an endpoint\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getResource\")\n\t        res = self.connector.getData(endpoint, params=params, format=format)\n\t        if save:\n\t            if format == \"json\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"json\",\n", "                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n\t            elif format == \"txt\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"txt\",\n\t                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n", "            else:\n\t                print(\n\t                    \"element is an object. Output is unclear. No save made.\\nPlease save this element manually\"\n\t                )\n\t        return res\n\t    def updateSandbox(self, sandbox: str = None) -> None:\n\t        \"\"\"\n\t        Update the sandbox used in your request.\n\t        Arguments:\n\t            sandbox : REQUIRED : name of the sandbox to be used\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateSandbox\")\n\t        if not sandbox:\n\t            raise ValueError(\"`sandbox` must be specified in the arguments.\")\n\t        self.header[\"x-sandbox-name\"] = sandbox\n\t        self.sandbox = sandbox\n\t    def getStats(self) -> list:\n\t        \"\"\"\n\t        Returns a list of the last actions realized on the Schema for this instance of AEP.\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getStats\")\n\t        path = \"/stats/\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def getTenantId(self) -> str:\n\t        \"\"\"\n\t        Return the tenantID for the AEP instance.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getTenantId\")\n\t        res = self.getStats()\n\t        tenant = res[\"tenantId\"]\n\t        return tenant\n\t    def getBehaviors(self)->list:\n\t        \"\"\"\n\t        Return a list of behaviors.\n\t        \"\"\"\n\t        path = \"/global/behaviors\"\n", "        res = self.connector.getData(self.endpoint + path)\n\t        data = res.get(\"results\",[])\n\t        return data\n\t    def getBehavior(self,behaviorId:str=None)->dict:\n\t        \"\"\"\n\t        Retrieve a specific behavior for class creation.\n\t        Arguments:\n\t            behaviorId : REQUIRED : the behavior ID to be retrieved.\n\t        \"\"\"\n\t        if behaviorId is None:\n", "            raise Exception(\"Require a behavior ID\")\n\t        path = f\"/global/behaviors/{behaviorId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getSchemas(\n\t            self, \n\t            classFilter: str = None,\n\t            excludeAdhoc: bool = False,\n\t            output: str = 'raw',\n\t            **kwargs\n", "    ) -> list:\n\t        \"\"\"\n\t        Returns the list of schemas retrieved for that instances in a \"results\" list.\n\t        Arguments:\n\t            classFilter : OPTIONAL : filter to a specific class.\n\t                Example :\n\t                    https://ns.adobe.com/xdm/context/experienceevent\n\t                    https://ns.adobe.com/xdm/context/profile\n\t                    https://ns.adobe.com/xdm/data/adhoc\n\t            excludeAdhoc : OPTIONAL : exclude the adhoc schemas\n", "            output : OPTIONAL : either \"raw\" for a list or \"df\" for dataframe\n\t        Possible kwargs:\n\t            debug : if set to true, will print the result when error happens\n\t            format : if set to \"xed\", returns the full JSON for each resource (default : \"xed-id\" -  short summary)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchemas\")\n\t        path = f\"/{self.container}/schemas/\"\n\t        start = kwargs.get(\"start\", 0)\n\t        params = {\"start\": start}\n", "        if classFilter is not None:\n\t            params[\"property\"] = f\"meta:intendedToExtend=={classFilter}\"\n\t        if excludeAdhoc:\n\t            params[\"property\"] = \"meta:extends!=https://ns.adobe.com/xdm/data/adhoc\"\n\t        verbose = kwargs.get(\"debug\", False)\n\t        privateHeader = deepcopy(self.header)\n\t        format = kwargs.get(\"format\", \"xed-id\")\n\t        privateHeader[\"Accept\"] = f\"application/vnd.adobe.{format}+json\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=privateHeader, verbose=verbose\n", "        )\n\t        if kwargs.get(\"debug\", False):\n\t            if \"results\" not in res.keys():\n\t                print(res)\n\t        data = res.get(\"results\",[])\n\t        if len(data) == 0:\n\t            return res\n\t        page = res.get(\"_page\",{})\n\t        nextPage = page.get('next',None)\n\t        while nextPage is not None:\n", "            params['start'] = nextPage\n\t            res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=privateHeader, verbose=verbose\n\t            )\n\t            data += res.get('results',[])\n\t            page = res.get(\"_page\",{'next':None})\n\t            nextPage = page.get('next',None)\n\t        self.data.schemas_id = {schem[\"title\"]: schem[\"$id\"] for schem in data}\n\t        self.data.schemas_altId = {\n\t            schem[\"title\"]: schem[\"meta:altId\"] for schem in data\n", "        }\n\t        if output == 'df':\n\t            df = pd.DataFrame(data)\n\t            return df\n\t        return data\n\t    def getSchema(\n\t        self,\n\t        schemaId: str = None,\n\t        version: int = 1,\n\t        full: bool = True,\n", "        desc: bool = False,\n\t        deprecated:bool=False,\n\t        schema_type: str = \"xdm\",\n\t        flat: bool = False,\n\t        save: bool = False,\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Get the Schema. Requires a schema id.\n\t        Response provided depends on the header set, you can change the Accept header with kwargs.\n", "        Arguments:\n\t            schemaId : REQUIRED : $id or meta:altId\n\t            version : OPTIONAL : Version of the Schema asked (default 1)\n\t            full : OPTIONAL : True (default) will return the full schema.False just the relationships.\n\t            desc : OPTIONAL : If set to True, return the identity used as the descriptor.\n\t            deprecated : OPTIONAL : Display the deprecated field from that schema\n\t            flat : OPTIONAL : If set to True, return a flat schema for pathing.\n\t            schema_type : OPTIONAL : set the type of output you want (xdm or xed) Default : xdm.\n\t            save : OPTIONAL : save the result in json file (default False)\n\t        Possible kwargs:\n", "            Accept : Accept header to change the type of response.\n\t            # /Schemas/lookup_schema\n\t            more details held here : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchema\")\n\t        privateHeader = deepcopy(self.header)\n\t        if schemaId is None:\n\t            raise Exception(\"Require a schemaId as a parameter\")\n\t        update_full,update_desc,update_flat,update_deprecated=\"\",\"\",\"\",\"\"\n", "        if full:\n\t            update_full = \"-full\"\n\t        if desc:\n\t            update_desc = \"-desc\"\n\t        if flat:\n\t            update_flat = \"-flat\"\n\t        if deprecated:\n\t            update_deprecated = \"-deprecated\"\n\t        if schema_type != \"xdm\" and schema_type != \"xed\":\n\t            raise ValueError(\"schema_type parameter can only be xdm or xed\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchema\")\n\t        privateHeader['Accept'] = f\"application/vnd.adobe.{schema_type}{update_full}{update_desc}{update_flat}{update_deprecated}+json; version={version}\"\n\t        if kwargs.get(\"Accept\", None) is not None:\n\t            privateHeader[\"Accept\"] = kwargs.get(\"Accept\", self.header[\"Accept\"])\n\t        privateHeader[\"Accept-Encoding\"] = \"identity\"\n\t        if schemaId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            schemaId = parse.quote_plus(schemaId)\n\t        path = f\"/{self.container}/schemas/{schemaId}\"\n", "        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if \"title\" not in res.keys() and \"notext\" not in privateHeader[\"Accept\"]:\n\t            print(\"Issue with the request. See response.\")\n\t            return res\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\", file=res, filename=res[\"title\"], type_file=\"json\"\n\t            )\n\t        if \"title\" in res.keys():\n\t            self.data.schemas[res[\"title\"]] = res\n", "        else:\n\t            print(\"no title in the response. Not saved in the data object.\")\n\t        return res\n\t    def getSchemaPaths(\n\t        self, schemaId: str, simplified: bool = True, save: bool = False\n\t    ) -> list:\n\t        \"\"\"\n\t        Returns a list of the path available in your schema. BETA.\n\t        Arguments:\n\t            schemaId : REQUIRED : The schema you want to retrieve the paths for\n", "            simplified : OPTIONAL : Default True, only returns the list of paths for your schemas.\n\t            save : OPTIONAL : Save your schema paths in a file. Always the NOT simplified version.\n\t        \"\"\"\n\t        if schemaId is None:\n\t            raise Exception(\"Require a schemaId as a parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchemaPaths\")\n\t        res = self.getSchema(schemaId, flat=True)\n\t        keys = res[\"properties\"].keys()\n\t        paths = [\n", "            key.replace(\"/\", \".\").replace(\"xdm:\", \"\").replace(\"@\", \"_\") for key in keys\n\t        ]\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\",\n\t                file=res,\n\t                filename=f\"{res['title']}_paths\",\n\t                type_file=\"json\",\n\t            )\n\t        if simplified:\n", "            return paths\n\t        return res\n\t    def getSchemaSample(\n\t        self, schemaId: str = None, save: bool = False, version: int = 1\n\t    ) -> dict:\n\t        \"\"\"\n\t        Generate a sample data from a schema id.\n\t        Arguments:\n\t            schema_id : REQUIRED : The schema ID for the sample data to be created.\n\t            save : OPTIONAL : save the result in json file (default False)\n", "            version : OPTIONAL : version of the schema to request\n\t        \"\"\"\n\t        privateHeader = deepcopy(self.header)\n\t        import random\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchemaSample\")\n\t        rand_number = random.randint(1, 10e10)\n\t        if schemaId is None:\n\t            raise Exception(\"Require an ID for the schema\")\n\t        if schemaId.startswith(\"https://\"):\n", "            from urllib import parse\n\t            schemaId = parse.quote_plus(schemaId)\n\t        path = f\"/rpc/sampledata/{schemaId}\"\n\t        accept_update = f\"application/vnd.adobe.xed+json; version={version}\"\n\t        privateHeader[\"Accept\"] = accept_update\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if save:\n\t            schema = self.getSchema(schemaId=schemaId, full=False)\n\t            aepp.saveFile(\n\t                module=\"schema\",\n", "                file=res,\n\t                filename=f\"{schema['title']}_{rand_number}\",\n\t                type_file=\"json\",\n\t            )\n\t        return res\n\t    def patchSchema(self, schemaId: str = None, changes: list = None, **kwargs) -> dict:\n\t        \"\"\"\n\t        Enable to patch the Schema with operation.\n\t        Arguments:\n\t            schema_id : REQUIRED : $id or meta:altId\n", "            change : REQUIRED : List of changes that need to take place.\n\t            Example:\n\t                [\n\t                    {\n\t                        \"op\": \"add\",\n\t                        \"path\": \"/allOf\",\n\t                        \"value\": {'$ref': 'https://ns.adobe.com/emeaconsulting/mixins/fb5b3cd49707d27367b93e07d1ac1f2f7b2ae8d051e65f8d',\n\t                    'type': 'object',\n\t                    'meta:xdmType': 'object'}\n\t                    }\n", "                ]\n\t        information : http://jsonpatch.com/\n\t        \"\"\"\n\t        if schemaId is None:\n\t            raise Exception(\"Require an ID for the schema\")\n\t        if type(changes) == dict:\n\t            changes = list(changes)\n\t        if schemaId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            schemaId = parse.quote_plus(schemaId)\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchSchema\")\n\t        path = f\"/{self.container}/schemas/{schemaId}\"\n\t        res = self.connector.patchData(\n\t            self.endpoint + path, data=changes)\n\t        return res\n\t    def putSchema(self, schemaId: str = None, schemaDef: dict = None, **kwargs) -> dict:\n\t        \"\"\"\n\t        A PUT request essentially re-writes the schema, therefore the request body must include all fields required to create (POST) a schema.\n\t        This is especially useful when updating a lot of information in the schema at once.\n", "        Arguments:\n\t            schemaId : REQUIRED : $id or meta:altId\n\t            schemaDef : REQUIRED : dictionary of the new schema.\n\t            It requires a allOf list that contains all the attributes that are required for creating a schema.\n\t            #/Schemas/replace_schema\n\t            More information on : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n\t        \"\"\"\n\t        if schemaId is None:\n\t            raise Exception(\"Require an ID for the schema\")\n\t        if schemaId.startswith(\"https://\"):\n", "            from urllib import parse\n\t            schemaId = parse.quote_plus(schemaId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting putSchema\")\n\t        path = f\"/{self.container}/schemas/{schemaId}\"\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=schemaDef, headers=self.header\n\t        )\n\t        return res\n\t    def deleteSchema(self, schemaId: str = None, **kwargs) -> str:\n", "        \"\"\"\n\t        Delete the request\n\t        Arguments:\n\t            schema_id : REQUIRED : $id or meta:altId\n\t            It requires a allOf list that contains all the attributes that are required for creating a schema.\n\t            #/Schemas/replace_schema\n\t            More information on : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n\t        \"\"\"\n\t        if schemaId is None:\n\t            raise Exception(\"Require an ID for the schema\")\n", "        if schemaId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            schemaId = parse.quote_plus(schemaId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteSchema\")\n\t        path = f\"/{self.container}/schemas/{schemaId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createSchema(self, schema: dict = None) -> dict:\n\t        \"\"\"\n", "        Create a Schema based on the data that are passed in the Argument.\n\t        Arguments:\n\t            schema : REQUIRED : The schema definition that needs to be created.\n\t        \"\"\"\n\t        path = f\"/{self.container}/schemas/\"\n\t        if type(schema) != dict:\n\t            raise TypeError(\"Expecting a dictionary\")\n\t        if \"allOf\" not in schema.keys():\n\t            raise Exception(\n\t                \"The schema must include an ‘allOf’ attribute (a list) referencing the $id of the base class the schema will implement.\"\n", "            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSchema\")\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=schema\n\t        )\n\t        return res\n\t    def createExperienceEventSchema(\n\t        self,\n\t        name: str = None,\n", "        mixinIds: Union[list, dict] = None,\n\t        fieldGroupIds : Union[list, dict] = None,\n\t        description: str = \"\",\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create an ExperienceEvent schema based on the list mixin ID provided.\n\t        Arguments:\n\t            name : REQUIRED : Name of your schema\n\t            mixinIds : REQUIRED : dict of mixins $id and their type [\"object\" or \"array\"] to create the ExperienceEvent schema\n\t                Example {'mixinId1':'object','mixinId2':'array'}\n", "                if just a list is passed, it infers a 'object type'\n\t            fieldGroupIds : REQUIRED : List of fieldGroup $id to create the Indiviudal Profile schema\n\t                Example {'fgId1':'object','fgId2':'array'}\n\t                if just a list is passed, it infers a 'object type'\n\t            description : OPTIONAL : Schema description\n\t        \"\"\"\n\t        if name is None:\n\t            raise ValueError(\"Require a name\")\n\t        if mixinIds is None and fieldGroupIds is None:\n\t            raise ValueError(\"Require a mixin ids or a field group id\")\n", "        if mixinIds is None and fieldGroupIds is not None:\n\t            mixinIds = fieldGroupIds\n\t        obj = {\n\t            \"title\": name,\n\t            \"description\": description,\n\t            \"allOf\": [\n\t                {\n\t                    \"$ref\": \"https://ns.adobe.com/xdm/context/experienceevent\",\n\t                    \"type\": \"object\",\n\t                    \"meta:xdmType\": \"object\",\n", "                }\n\t            ],\n\t        }\n\t        if type(mixinIds) == list:\n\t            for mixin in mixinIds:\n\t                obj[\"allOf\"].append(\n\t                    {\"$ref\": mixin, \"type\": \"object\", \"meta:xdmType\": \"object\"}\n\t                )\n\t        if type(mixinIds) == dict:\n\t            for mixin in mixinIds:\n", "                if mixinIds[mixin] == \"array\":\n\t                    subObj = {\n\t                        \"$ref\": mixin,\n\t                        \"type\": mixinIds[mixin],\n\t                        \"meta:xdmType\": mixinIds[mixin],\n\t                        \"items\": {\"$ref\": mixin},\n\t                    }\n\t                    obj[\"allOf\"].append(subObj)\n\t                else:\n\t                    subObj = {\n", "                        \"$ref\": mixin,\n\t                        \"type\": mixinIds[mixin],\n\t                        \"meta:xdmType\": mixinIds[mixin],\n\t                    }\n\t                    obj[\"allOf\"].append(subObj)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createExperienceEventSchema\")\n\t        res = self.createSchema(obj)\n\t        return res\n\t    def createProfileSchema(\n", "        self,\n\t        name: str = None,\n\t        mixinIds: Union[list, dict] = None,\n\t        fieldGroupIds : Union[list, dict] = None,\n\t        description: str = \"\",\n\t        **kwargs\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create an IndividualProfile schema based on the list mixin ID provided.\n\t        Arguments:\n", "            name : REQUIRED : Name of your schema\n\t            mixinIds : REQUIRED : List of mixins $id to create the Indiviudal Profile schema\n\t                Example {'mixinId1':'object','mixinId2':'array'}\n\t                if just a list is passed, it infers a 'object type'\n\t            fieldGroupIds : REQUIRED : List of fieldGroup $id to create the Indiviudal Profile schema\n\t                Example {'fgId1':'object','fgId2':'array'}\n\t                if just a list is passed, it infers a 'object type'\n\t            description : OPTIONAL : Schema description\n\t        \"\"\"\n\t        if name is None:\n", "            raise ValueError(\"Require a name\")\n\t        if mixinIds is None and fieldGroupIds is None:\n\t            raise ValueError(\"Require a mixin ids or a field group id\")\n\t        if mixinIds is None and fieldGroupIds is not None:\n\t            mixinIds = fieldGroupIds\n\t        obj = {\n\t            \"title\": name,\n\t            \"description\": description,\n\t            \"allOf\": [\n\t                {\n", "                    \"$ref\": \"https://ns.adobe.com/xdm/context/profile\",\n\t                    \"type\": \"object\",\n\t                    \"meta:xdmType\": \"object\",\n\t                }\n\t            ],\n\t        }\n\t        if type(mixinIds) == list:\n\t            for mixin in mixinIds:\n\t                obj[\"allOf\"].append(\n\t                    {\"$ref\": mixin, \"type\": \"object\", \"meta:xdmType\": \"object\"}\n", "                )\n\t        if type(mixinIds) == dict:\n\t            for mixin in mixinIds:\n\t                if mixinIds[mixin] == \"array\":\n\t                    subObj = {\n\t                        \"$ref\": mixin,\n\t                        \"type\": mixinIds[mixin],\n\t                        \"meta:xdmType\": mixinIds[mixin],\n\t                        \"items\": {\"$ref\": mixin},\n\t                    }\n", "                    obj[\"allOf\"].append(subObj)\n\t                else:\n\t                    subObj = {\n\t                        \"$ref\": mixin,\n\t                        \"type\": mixinIds[mixin],\n\t                        \"meta:xdmType\": mixinIds[mixin],\n\t                    }\n\t                    obj[\"allOf\"].append(subObj)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createProfileSchema\")\n", "        res = self.createSchema(obj)\n\t        return res\n\t    def addFieldGroupToSchema(self,schemaId:str=None,fieldGroupIds:Union[list,dict]=None)->dict:\n\t        \"\"\"\n\t        Take the list of field group ID to extend the schema.\n\t        Return the definition of the new schema with added field groups.\n\t        Arguments\n\t            schemaId : REQUIRED : The ID of the schema (alt:metaId or $id)\n\t            fieldGroupIds : REQUIRED : The IDs of the fields group to add. It can be a list or dictionary.\n\t                Example {'fgId1':'object','fgId2':'array'}\n", "                if just a list is passed, it infers a 'object type'\n\t        \"\"\"\n\t        if schemaId is None:\n\t            raise ValueError(\"Require a schema ID\")\n\t        if fieldGroupIds is None:\n\t            raise ValueError(\"Require a list of field group to add\")\n\t        schemaDef = self.getSchema(schemaId,full=False)\n\t        allOf = schemaDef.get('allOf',[])\n\t        if type(allOf) != list:\n\t            raise TypeError(\"Expecting a list for 'allOf' key\")\n", "        if type(fieldGroupIds) == list:\n\t            for mixin in fieldGroupIds:\n\t                allOf.append(\n\t                    {\"$ref\": mixin, \"type\": \"object\", \"meta:xdmType\": \"object\"}\n\t                )\n\t        if type(fieldGroupIds) == dict:\n\t            for mixin in fieldGroupIds:\n\t                if fieldGroupIds[mixin] == \"array\":\n\t                    subObj = {\n\t                        \"$ref\": mixin,\n", "                        \"type\": fieldGroupIds[mixin],\n\t                        \"meta:xdmType\": fieldGroupIds[mixin],\n\t                        \"items\": {\"$ref\": mixin},\n\t                    }\n\t                    allOf.append(subObj)\n\t                else:\n\t                    subObj = {\n\t                        \"$ref\": mixin,\n\t                        \"type\": fieldGroupIds[mixin],\n\t                        \"meta:xdmType\": fieldGroupIds[mixin],\n", "                    }\n\t                    allOf.append(subObj)\n\t        res = self.putSchema(schemaId,schemaDef)\n\t        return res        \n\t    def getClasses(self, \n\t                   prop:str=None,\n\t                   orderBy:str=None,\n\t                   limit:int=300, \n\t                   output:str='raw',\n\t                   excludeAdhoc: bool = False,\n", "                   **kwargs):\n\t        \"\"\"\n\t        Return the classes of the AEP Instances.\n\t        Arguments:\n\t            prop : OPTIONAL : A comma-separated list of top-level object properties to be returned in the response. \n\t                            For example, property=meta:intendedToExtend==https://ns.adobe.com/xdm/context/profile\n\t            oderBy : OPTIONAL : Sort the listed resources by specified fields. For example orderby=title\n\t            limit : OPTIONAL : Number of resources to return per request, default 300 - the max.\n\t            excludeAdhoc : OPTIONAL : Exlcude the Adhoc classes that have been created.\n\t            output : OPTIONAL : type of output, default \"raw\", can be \"df\" for dataframe.\n", "        kwargs:\n\t            debug : if set to True, will print result for errors\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getClasses\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader.update({\"Accept\": \"application/vnd.adobe.xdm-id+json\"})\n\t        params = {\"limit\":limit}\n\t        if excludeAdhoc:\n\t            params[\"property\"] = \"meta:extends!=https://ns.adobe.com/xdm/data/adhoc\"\n", "        if prop is not None:\n\t            if 'property' not in params.keys():\n\t                params[\"property\"] = prop\n\t            else:\n\t                params[\"property\"] += prop\n\t        if orderBy is not None:\n\t            params['orderby'] = orderBy\n\t        path = f\"/{self.container}/classes/\"\n\t        verbose = kwargs.get(\"verbose\", False)\n\t        res = self.connector.getData(\n", "            self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t        )\n\t        if kwargs.get(\"debug\", False):\n\t            if \"results\" not in res.keys():\n\t                print(res)\n\t        data = res[\"results\"]\n\t        page = res[\"_page\"]\n\t        while page[\"next\"] is not None:\n\t            params[\"start\"]= page[\"next\"]\n\t            res = self.connector.getData(\n", "                self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t            )\n\t            data += res[\"results\"]\n\t            page = res[\"_page\"]\n\t        if output==\"df\":\n\t            df = pd.DataFrame(data)\n\t            return df\n\t        return data\n\t    def getClassesGlobal(self, \n\t                   prop:str=None,\n", "                   orderBy:str=None,\n\t                   limit:int=300, \n\t                   output:str='raw',\n\t                   **kwargs):\n\t        \"\"\"\n\t        Return the classes of the AEP Instances.\n\t        Arguments:\n\t            prop : OPTIONAL : A comma-separated list of top-level object properties to be returned in the response. \n\t                            For example, property=meta:intendedToExtend==https://ns.adobe.com/xdm/context/profile\n\t            oderBy : OPTIONAL : Sort the listed resources by specified fields. For example orderby=title\n", "            limit : OPTIONAL : Number of resources to return per request, default 300 - the max.\n\t            output : OPTIONAL : type of output, default \"raw\", can be \"df\" for dataframe.\n\t        kwargs:\n\t            debug : if set to True, will print result for errors\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getClasses\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader.update({\"Accept\": \"application/vnd.adobe.xdm-id+json\"})\n\t        params = {\"limit\":limit}\n", "        if prop is not None:\n\t            if 'property' not in params.keys():\n\t                params[\"property\"] = prop\n\t            else:\n\t                params[\"property\"] += prop\n\t        if orderBy is not None:\n\t            params['orderby'] = orderBy\n\t        path = f\"/global/classes/\"\n\t        verbose = kwargs.get(\"verbose\", False)\n\t        res = self.connector.getData(\n", "            self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t        )\n\t        if kwargs.get(\"debug\", False):\n\t            if \"results\" not in res.keys():\n\t                print(res)\n\t        data = res[\"results\"]\n\t        page = res[\"_page\"]\n\t        while page[\"next\"] is not None:\n\t            params[\"start\"]= page[\"next\"]\n\t            res = self.connector.getData(\n", "                self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t            )\n\t            data += res[\"results\"]\n\t            page = res[\"_page\"]\n\t        if output==\"df\":\n\t            df = pd.DataFrame(data)\n\t            return df\n\t        return data\n\t    def getClass(\n\t        self,\n", "        classId: str = None,\n\t        full: bool = True,\n\t        desc: bool = False,\n\t        deprecated: bool = False,\n\t        xtype : str = \"xdm\",\n\t        version: int = 1,\n\t        save: bool = False,\n\t    ):\n\t        \"\"\"\n\t        Return a specific class.\n", "        Arguments:\n\t            classId : REQUIRED : the meta:altId or $id from the class\n\t            full : OPTIONAL : True (default) will return the full schema.False just the relationships.\n\t            desc : OPTIONAL : If set to True, return the descriptors.\n\t            deprecated : OPTIONAL : Display the deprecated field from that schema (False by default)\n\t            xtype : OPTIONAL : either \"xdm\" (default) or \"xed\". \n\t            version : OPTIONAL : the version of the class to retrieve.\n\t            save : OPTIONAL : To save the result of the request in a JSON file.\n\t        \"\"\"\n\t        privateHeader = deepcopy(self.header)\n", "        if classId is None:\n\t            raise Exception(\"Require a class_id\")\n\t        if classId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            classId = parse.quote_plus(classId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getClass\")\n\t        privateHeader[\"Accept-Encoding\"] = \"identity\"\n\t        updateFull,updateDesc, updateDeprecated = \"\",\"\",\"\"\n\t        if full:\n", "            updateFull = \"-full\"\n\t        if desc:\n\t            updateDesc = \"-desc\"\n\t        if deprecated:\n\t            updateDeprecated = \"-deprecated\"\n\t        privateHeader.update(\n\t                {\"Accept\": f\"application/vnd.adobe.{xtype}{updateFull}{updateDesc}{updateDeprecated}+json; version=\" + str(version)}\n\t            )\n\t        path = f\"/{self.container}/classes/{classId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n", "        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\", file=res, filename=res[\"title\"], type_file=\"json\"\n\t            )\n\t        return res\n\t    def createClass(self, class_obj: dict = None,title:str=None, class_template:str=None, **kwargs):\n\t        \"\"\"\n\t        Create a class based on the object pass. It should include the \"allOff\" element.\n\t        Arguments:\n\t            class_obj : REQUIRED : You can pass a complete object to create a class, include a title and a \"allOf\" element.\n", "            title : REQUIRED : Title of the class if you want to pass individual elements\n\t            class_template : REQUIRED : type of behavior for the class, either \"https://ns.adobe.com/xdm/data/record\" or \"https://ns.adobe.com/xdm/data/time-series\"\n\t        Possible kwargs: \n\t            description : To add a description to a class.\n\t        \"\"\"\n\t        path = f\"/{self.container}/classes/\"\n\t        if class_obj is not None:\n\t            if type(class_obj) != dict:\n\t                raise TypeError(\"Expecting a dictionary\")\n\t            if \"allOf\" not in class_obj.keys():\n", "                raise Exception(\n\t                    \"The class object must include an ‘allOf’ attribute (a list) referencing the $id of the base class the schema will implement.\"\n\t                )\n\t        elif class_obj is None and title is not None and class_template is not None:\n\t            class_obj = {\n\t                \"type\": \"object\",\n\t                \"title\": title,\n\t                \"description\": \"Generated by aepp\",\n\t                \"allOf\": [\n\t                    {\n", "                    \"$ref\": class_template\n\t                    }\n\t                ]\n\t                }\n\t        if kwargs.get(\"descriptor\",\"\") != \"\":\n\t            class_obj['descriptor'] = kwargs.get(\"descriptor\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createClass\")\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=class_obj\n", "        )\n\t        return res\n\t    def putClass(self,classId:str=None,class_obj:dict=None)->dict:\n\t        \"\"\"\n\t        Replace the current definition with the new definition.\n\t        Arguments:\n\t            classId : REQUIRED : The class to be updated ($id or meta:altId)\n\t            class_obj : REQUIRED : The dictionary defining the new class definition\n\t        \"\"\"\n\t        if classId is None:\n", "            raise Exception(\"Require a classId\")\n\t        if classId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            classId = parse.quote_plus(classId)\n\t        if class_obj is None:\n\t            raise Exception(\"Require a new definition for the class\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting putClass\")\n\t        path = f\"/{self.container}/classes/{classId}\"\n\t        res = self.connector.putData(self.endpoint + path,data=class_obj)\n", "        return res\n\t    def patchClass(self,classId:str=None,operation:list=None)->dict:\n\t        \"\"\"\n\t        Patch a class with the operation specified such as:\n\t        update = [{\n\t            \"op\": \"replace\",\n\t            \"path\": \"title\",\n\t            \"value\": \"newTitle\"\n\t        }]\n\t        Possible operation value : \"replace\", \"remove\", \"add\"\n", "        Arguments:\n\t            classId : REQUIRED : The class to be updated  ($id or meta:altId)\n\t            operation : REQUIRED : List of operation to realize on the class\n\t        \"\"\"\n\t        if classId is None:\n\t            raise Exception(\"Require a classId\")\n\t        if classId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            classId = parse.quote_plus(classId)\n\t        if operation is None or type(operation) != list:\n", "            raise Exception(\"Require a list of operation for the class\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchClass\")\n\t        path = f\"/{self.container}/classes/{classId}\"\n\t        res = self.connector.patchData(self.endpoint + path,data=operation)\n\t        return res\n\t    def deleteClass(self,classId: str = None)->str:\n\t        \"\"\"\n\t        Delete a class based on the its ID.\n\t        Arguments:\n", "            classId : REQUIRED : The class to be deleted  ($id or meta:altId)\n\t        \"\"\"\n\t        if classId is None:\n\t            raise Exception(\"Require a classId\")\n\t        if classId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            classId = parse.quote_plus(classId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchClass\")\n\t        path = f\"/{self.container}/classes/{classId}\"\n", "        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getFieldGroups(self, format: str = \"xdm\", **kwargs) -> list:\n\t        \"\"\"\n\t        returns the fieldGroups of the account.\n\t        Arguments:\n\t            format : OPTIONAL : either \"xdm\" or \"xed\" format\n\t        kwargs:\n\t            debug : if set to True, will print result for errors\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFieldGroups\")\n\t        path = f\"/{self.container}/fieldgroups/\"\n\t        start = kwargs.get(\"start\", 0)\n\t        params = {\"start\": start}\n\t        verbose = kwargs.get(\"debug\", False)\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = f\"application/vnd.adobe.{format}+json\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n", "        )\n\t        if kwargs.get(\"verbose\", False):\n\t            if \"results\" not in res.keys():\n\t                print(res)\n\t        data = res[\"results\"]\n\t        page = res.get(\"_page\",{})\n\t        nextPage = page.get('next',None)\n\t        while nextPage is not None:\n\t            params['start'] = nextPage\n\t            res = self.connector.getData(\n", "            self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t            )\n\t            data += res.get(\"results\")\n\t            page = res.get(\"_page\",{})\n\t            nextPage = page.get('next',None)\n\t        self.data.fieldGroups_id = {mix[\"title\"]: mix[\"$id\"] for mix in data}\n\t        self.data.fieldGroups_altId = {mix[\"title\"]: mix[\"meta:altId\"] for mix in data}\n\t        return data\n\t    def getFieldGroupsGlobal(self,format: str = \"xdm\",output:str='raw', **kwargs)->list:\n\t        \"\"\"\n", "        returns the global fieldGroups of the account.\n\t        Arguments:\n\t            format : OPTIONAL : either \"xdm\" or \"xed\" format\n\t            output : OPTIONAL : either \"raw\" (default) or \"df\" for dataframe \n\t        kwargs:\n\t            debug : if set to True, will print result for errors\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFieldGroups\")\n\t        path = f\"/global/fieldgroups/\"\n", "        start = kwargs.get(\"start\", 0)\n\t        params = {\"start\": start}\n\t        verbose = kwargs.get(\"debug\", False)\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = f\"application/vnd.adobe.{format}+json\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t        )\n\t        if kwargs.get(\"verbose\", False):\n\t            if \"results\" not in res.keys():\n", "                print(res)\n\t        data = res[\"results\"]\n\t        page = res.get(\"_page\",{})\n\t        nextPage = page.get('next',None)\n\t        while nextPage is not None:\n\t            params['start'] = nextPage\n\t            res = self.connector.getData(\n\t            self.endpoint + path, headers=privateHeader, params=params, verbose=verbose\n\t            )\n\t            data += res.get(\"results\")\n", "            page = res.get(\"_page\",{})\n\t            nextPage = page.get('next',None)\n\t        self.data.fieldGroups_id = {mix[\"title\"]: mix[\"$id\"] for mix in data}\n\t        self.data.fieldGroups_altId = {mix[\"title\"]: mix[\"meta:altId\"] for mix in data}\n\t        if output == 'df':\n\t            df = pd.DataFrame(data)\n\t            return df\n\t        return data\n\t    # def getMixin(\n\t    #     self,\n", "    #     mixinId: str = None,\n\t    #     version: int = 1,\n\t    #     full: bool = True,\n\t    #     save: bool = False,\n\t    # ):\n\t    #     \"\"\"\n\t    #     Returns a specific mixin / field group.\n\t    #     Arguments:\n\t    #         mixinId : REQUIRED : meta:altId or $id\n\t    #         version : OPTIONAL : version of the mixin\n", "    #         full : OPTIONAL : True (default) will return the full schema.False just the relationships.\n\t    #     \"\"\"\n\t    #     if mixinId.startswith(\"https://\"):\n\t    #         from urllib import parse\n\t    #         mixinId = parse.quote_plus(mixinId)\n\t    #     if self.loggingEnabled:\n\t    #         self.logger.debug(f\"Starting getMixin\")\n\t    #     privateHeader = deepcopy(self.header)\n\t    #     privateHeader[\"Accept-Encoding\"] = \"identity\"\n\t    #     if full:\n", "    #         accept_full = \"-full\"\n\t    #     else:\n\t    #         accept_full = \"\"\n\t    #     update_accept = (\n\t    #         f\"application/vnd.adobe.xed{accept_full}+json; version={version}\"\n\t    #     )\n\t    #     privateHeader.update({\"Accept\": update_accept})\n\t    #     path = f\"/{self.container}/mixins/{mixinId}\"\n\t    #     res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t    #     if save:\n", "    #         aepp.saveFile(\n\t    #             module=\"schema\", file=res, filename=res[\"title\"], type_file=\"json\"\n\t    #         )\n\t    #     if \"title\" in res.keys():\n\t    #         self.data.mixins[res[\"title\"]] = res\n\t    #     return res\n\t    def getFieldGroup(\n\t        self,\n\t        fieldGroupId: str = None,\n\t        version: int = 1,\n", "        full: bool = True,\n\t        desc: bool = False,\n\t        type: str = 'xed',\n\t        flat: bool = False,\n\t        deprecated: bool = False,\n\t        save: bool = False,\n\t    ):\n\t        \"\"\"\n\t        Returns a specific mixin / field group.\n\t        Arguments:\n", "            fieldGroupId : REQUIRED : meta:altId or $id\n\t            version : OPTIONAL : version of the mixin\n\t            full : OPTIONAL : True (default) will return the full schema.False just the relationships\n\t            desc : OPTIONAL : Add descriptor of the field group\n\t            type : OPTIONAL : Either \"xed\" (default) or \"xdm\"\n\t            flat : OPTIONAL : if the fieldGroup is flat (false by default)\n\t            deprecated : OPTIONAL : Display the deprecated fields from that schema\n\t            save : Save the fieldGroup to a JSON file\n\t        \"\"\"\n\t        if fieldGroupId.startswith(\"https://\"):\n", "            from urllib import parse\n\t            fieldGroupId = parse.quote_plus(fieldGroupId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFieldGroup\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept-Encoding\"] = \"identity\"\n\t        accept_full, accept_desc,accept_flat,accept_deprec= \"\",\"\",\"\",\"\"\n\t        if full:\n\t            accept_full = \"-full\"\n\t        if desc:\n", "            accept_desc = \"-desc\"\n\t        if flat:\n\t            accept_flat = \"-flat\"\n\t        if deprecated:\n\t            accept_deprec = \"-deprecated\"\n\t        update_accept = (\n\t            f\"application/vnd.adobe.{type}{accept_full}{accept_desc}{accept_flat}{accept_deprec}+json; version={version}\"\n\t        )\n\t        privateHeader.update({\"Accept\": update_accept})\n\t        path = f\"/{self.container}/fieldgroups/{fieldGroupId}\"\n", "        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\", file=res, filename=res[\"title\"], type_file=\"json\"\n\t            )\n\t        if \"title\" in res.keys():\n\t            self.data.fieldGroups[res[\"title\"]] = res\n\t        return res\n\t    def copyMixin(\n\t        self, mixin: dict = None, tenantId: str = None, title: str = None\n", "    ) -> dict:\n\t        \"\"\"\n\t        Copy the dictionary returned by getMixin to the only required elements for copying it over.\n\t        Arguments:\n\t            mixin : REQUIRED : the object retrieved from the getMixin.\n\t            tenantId : OPTIONAL : if you want to change the tenantId (if None doesn't rename)\n\t            name : OPTIONAL : rename your mixin (if None, doesn't rename it)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting copyMixin\")\n", "        if mixin is None:\n\t            raise ValueError(\"Require a mixin  object\")\n\t        mixin_obj = deepcopy(mixin)\n\t        oldTenant = mixin_obj[\"meta:tenantNamespace\"]\n\t        if \"definitions\" in mixin_obj.keys():\n\t            obj = {\n\t                \"type\": mixin_obj[\"type\"],\n\t                \"title\": title or mixin_obj[\"title\"],\n\t                \"description\": mixin_obj[\"description\"],\n\t                \"meta:intendedToExtend\": mixin_obj[\"meta:intendedToExtend\"],\n", "                \"definitions\": mixin_obj.get(\"definitions\"),\n\t                \"allOf\": mixin_obj.get(\n\t                    \"allOf\",\n\t                    [\n\t                        {\n\t                            \"$ref\": \"#/definitions/property\",\n\t                            \"type\": \"object\",\n\t                            \"meta:xdmType\": \"object\",\n\t                        }\n\t                    ],\n", "                ),\n\t            }\n\t        elif \"properties\" in mixin_obj.keys():\n\t            obj = {\n\t                \"type\": mixin_obj[\"type\"],\n\t                \"title\": title or mixin_obj[\"title\"],\n\t                \"description\": mixin_obj[\"description\"],\n\t                \"meta:intendedToExtend\": mixin_obj[\"meta:intendedToExtend\"],\n\t                \"definitions\": {\n\t                    \"property\": {\n", "                        \"properties\": mixin_obj[\"properties\"],\n\t                        \"type\": \"object\",\n\t                        \"['meta:xdmType']\": \"object\",\n\t                    }\n\t                },\n\t                \"allOf\": mixin_obj.get(\n\t                    \"allOf\",\n\t                    [\n\t                        {\n\t                            \"$ref\": \"#/definitions/property\",\n", "                            \"type\": \"object\",\n\t                            \"meta:xdmType\": \"object\",\n\t                        }\n\t                    ],\n\t                ),\n\t            }\n\t        if tenantId is not None:\n\t            if tenantId.startswith(\"_\") == False:\n\t                tenantId = f\"_{tenantId}\"\n\t            obj[\"definitions\"][\"property\"][\"properties\"][tenantId] = obj[\"definitions\"][\n", "                \"property\"\n\t            ][\"properties\"][oldTenant]\n\t            del obj[\"definitions\"][\"property\"][\"properties\"][oldTenant]\n\t        return obj\n\t    def copyFieldGroup(\n\t        self, fieldGroup: dict = None, tenantId: str = None, title: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Copy the dictionary returned by getMixin to the only required elements for copying it over.\n\t        Arguments:\n", "            fieldGroup : REQUIRED : the object retrieved from the getFieldGroup.\n\t            tenantId : OPTIONAL : if you want to change the tenantId (if None doesn't rename)\n\t            name : OPTIONAL : rename your mixin (if None, doesn't rename it)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting copyFieldGroup\")\n\t        if fieldGroup is None:\n\t            raise ValueError(\"Require a mixin  object\")\n\t        mixin_obj = deepcopy(fieldGroup)\n\t        oldTenant = mixin_obj[\"meta:tenantNamespace\"]\n", "        if \"definitions\" in mixin_obj.keys():\n\t            obj = {\n\t                \"type\": mixin_obj[\"type\"],\n\t                \"title\": title or mixin_obj[\"title\"],\n\t                \"description\": mixin_obj[\"description\"],\n\t                \"meta:intendedToExtend\": mixin_obj[\"meta:intendedToExtend\"],\n\t                \"definitions\": mixin_obj.get(\"definitions\"),\n\t                \"allOf\": mixin_obj.get(\n\t                    \"allOf\",\n\t                    [\n", "                        {\n\t                            \"$ref\": \"#/definitions/property\",\n\t                            \"type\": \"object\",\n\t                            \"meta:xdmType\": \"object\",\n\t                        }\n\t                    ],\n\t                ),\n\t            }\n\t        elif \"properties\" in mixin_obj.keys():\n\t            obj = {\n", "                \"type\": mixin_obj[\"type\"],\n\t                \"title\": title or mixin_obj[\"title\"],\n\t                \"description\": mixin_obj[\"description\"],\n\t                \"meta:intendedToExtend\": mixin_obj[\"meta:intendedToExtend\"],\n\t                \"definitions\": {\n\t                    \"property\": {\n\t                        \"properties\": mixin_obj[\"properties\"],\n\t                        \"type\": \"object\",\n\t                        \"['meta:xdmType']\": \"object\",\n\t                    }\n", "                },\n\t                \"allOf\": mixin_obj.get(\n\t                    \"allOf\",\n\t                    [\n\t                        {\n\t                            \"$ref\": \"#/definitions/property\",\n\t                            \"type\": \"object\",\n\t                            \"meta:xdmType\": \"object\",\n\t                        }\n\t                    ],\n", "                ),\n\t            }\n\t        if tenantId is not None:\n\t            if tenantId.startswith(\"_\") == False:\n\t                tenantId = f\"_{tenantId}\"\n\t            if 'property' in obj[\"definitions\"].keys():\n\t                obj[\"definitions\"][\"property\"][\"properties\"][tenantId] = obj[\"definitions\"][\"property\"][\"properties\"][oldTenant]\n\t                del obj[\"definitions\"][\"property\"][\"properties\"][oldTenant]\n\t            elif 'customFields' in obj[\"definitions\"].keys():\n\t                obj[\"definitions\"][\"customFields\"][\"properties\"][tenantId] = obj[\"definitions\"][\"customFields\"][\"properties\"][oldTenant]\n", "                del obj[\"definitions\"][\"customFields\"][\"properties\"][oldTenant]\n\t        return obj\n\t    def createMixin(self, mixin_obj: dict = None) -> dict:\n\t        \"\"\"\n\t        Create a mixin based on the dictionary passed.\n\t        Arguments :\n\t            mixin_obj : REQUIRED : the object required for creating the mixin.\n\t            Should contain title, type, definitions\n\t        \"\"\"\n\t        if mixin_obj is None:\n", "            raise Exception(\"Require a mixin object\")\n\t        if (\n\t            \"title\" not in mixin_obj\n\t            or \"type\" not in mixin_obj\n\t            or \"definitions\" not in mixin_obj\n\t        ):\n\t            raise AttributeError(\n\t                \"Require to have at least title, type, definitions set in the object.\"\n\t            )\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting createMixin\")\n\t        path = f\"/{self.container}/mixins/\"\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=mixin_obj)\n\t        return res\n\t    def createFieldGroup(self, fieldGroup_obj: dict = None) -> dict:\n\t        \"\"\"\n\t        Create a mixin based on the dictionary passed.\n\t        Arguments :\n\t            fieldGroup_obj : REQUIRED : the object required for creating the field group.\n", "            Should contain title, type, definitions\n\t        \"\"\"\n\t        if fieldGroup_obj is None:\n\t            raise Exception(\"Require a mixin object\")\n\t        if (\n\t            \"title\" not in fieldGroup_obj\n\t            or \"type\" not in fieldGroup_obj\n\t            or \"definitions\" not in fieldGroup_obj\n\t        ):\n\t            raise AttributeError(\n", "                \"Require to have at least title, type, definitions set in the object.\"\n\t            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createFieldGroup\")\n\t        path = f\"/{self.container}/fieldgroups/\"\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=fieldGroup_obj)\n\t        return res\n\t    def deleteMixin(self, mixinId: str = None):\n\t        \"\"\"\n", "        Arguments:\n\t            mixinId : meta:altId or $id\n\t        \"\"\"\n\t        if mixinId is None:\n\t            raise Exception(\"Require an ID\")\n\t        if mixinId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            mixinId = parse.quote_plus(mixinId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteMixin\")\n", "        path = f\"/{self.container}/mixins/{mixinId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def deleteFieldGroup(self, fieldGroupId: str = None):\n\t        \"\"\"\n\t        Arguments:\n\t            fieldGroupId : meta:altId or $id\n\t        \"\"\"\n\t        if fieldGroupId is None:\n\t            raise Exception(\"Require an ID\")\n", "        if fieldGroupId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            fieldGroupId = parse.quote_plus(fieldGroupId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteFieldGroup\")\n\t        path = f\"/{self.container}/fieldgroups/{fieldGroupId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def patchMixin(self, mixinId: str = None, changes: list = None):\n\t        \"\"\"\n", "        Update the mixin with the operation described in the changes.\n\t        Arguments:\n\t            mixinId : REQUIRED : meta:altId or $id\n\t            changes : REQUIRED : dictionary on what to update on that mixin.\n\t            Example:\n\t                [\n\t                    {\n\t                        \"op\": \"add\",\n\t                        \"path\": \"/allOf\",\n\t                        \"value\": {'$ref': 'https://ns.adobe.com/emeaconsulting/mixins/fb5b3cd49707d27367b93e07d1ac1f2f7b2ae8d051e65f8d',\n", "                    'type': 'object',\n\t                    'meta:xdmType': 'object'}\n\t                    }\n\t                ]\n\t        information : http://jsonpatch.com/\n\t        \"\"\"\n\t        if mixinId is None or changes is None:\n\t            raise Exception(\"Require an ID and changes\")\n\t        if mixinId.startswith(\"https://\"):\n\t            from urllib import parse\n", "            mixinId = parse.quote_plus(mixinId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchMixin\")\n\t        path = f\"/{self.container}/mixins/{mixinId}\"\n\t        if type(changes) == dict:\n\t            changes = list(changes)\n\t        res = self.connector.patchData(\n\t            self.endpoint + path, data=changes)\n\t        return res\n\t    def patchFieldGroup(self, fieldGroupId: str = None, changes: list = None):\n", "        \"\"\"\n\t        Update the mixin with the operation described in the changes.\n\t        Arguments:\n\t            fieldGroupId : REQUIRED : meta:altId or $id\n\t            changes : REQUIRED : dictionary on what to update on that mixin.\n\t            Example:\n\t                [\n\t                    {\n\t                        \"op\": \"add\",\n\t                        \"path\": \"/allOf\",\n", "                        \"value\": {'$ref': 'https://ns.adobe.com/emeaconsulting/mixins/fb5b3cd49707d27367b93e07d1ac1f2f7b2ae8d051e65f8d',\n\t                    'type': 'object',\n\t                    'meta:xdmType': 'object'}\n\t                    }\n\t                ]\n\t        information : http://jsonpatch.com/\n\t        \"\"\"\n\t        if fieldGroupId is None or changes is None:\n\t            raise Exception(\"Require an ID and changes\")\n\t        if fieldGroupId.startswith(\"https://\"):\n", "            from urllib import parse\n\t            fieldGroupId = parse.quote_plus(fieldGroupId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchFieldGroup\")\n\t        path = f\"/{self.container}/fieldgroups/{fieldGroupId}\"\n\t        if type(changes) == dict:\n\t            changes = list(changes)\n\t        res = self.connector.patchData(\n\t            self.endpoint + path, data=changes)\n\t        return res\n", "    def putMixin(self, mixinId: str = None, mixinObj: dict = None, **kwargs) -> dict:\n\t        \"\"\"\n\t        A PUT request essentially re-writes the schema, therefore the request body must include all fields required to create (POST) a schema.\n\t        This is especially useful when updating a lot of information in the schema at once.\n\t        Arguments:\n\t            mixinId : REQUIRED : $id or meta:altId\n\t            mixinObj : REQUIRED : dictionary of the new schema.\n\t            It requires a allOf list that contains all the attributes that are required for creating a schema.\n\t            #/Schemas/replace_schema\n\t            More information on : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n", "        \"\"\"\n\t        if mixinId is None:\n\t            raise Exception(\"Require an ID for the schema\")\n\t        if mixinId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            mixinId = parse.quote_plus(mixinId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting putMixin\")\n\t        path = f\"/{self.container}/mixins/{mixinId}\"\n\t        res = self.connector.putData(\n", "            self.endpoint + path, data=mixinObj)\n\t        return res\n\t    def putFieldGroup(\n\t        self, fieldGroupId: str = None, fieldGroupObj: dict = None, **kwargs\n\t    ) -> dict:\n\t        \"\"\"\n\t        A PUT request essentially re-writes the schema, therefore the request body must include all fields required to create (POST) a schema.\n\t        This is especially useful when updating a lot of information in the schema at once.\n\t        Arguments:\n\t            fieldGroupId : REQUIRED : $id or meta:altId\n", "            fieldGroupObj : REQUIRED : dictionary of the new Field Group.\n\t            It requires a allOf list that contains all the attributes that are required for creating a schema.\n\t            #/Schemas/replace_schema\n\t            More information on : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n\t        \"\"\"\n\t        if fieldGroupId is None:\n\t            raise Exception(\"Require an ID for the schema\")\n\t        if fieldGroupId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            fieldGroupId = parse.quote_plus(fieldGroupId)\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting putMixin\")\n\t        path = f\"/{self.container}/fieldgroups/{fieldGroupId}\"\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=fieldGroupObj)\n\t        return res\n\t    def getUnions(self, **kwargs):\n\t        \"\"\"\n\t        Get all of the unions that has been set for the tenant.\n\t        Returns a dictionary.\n", "        Possibility to add option using kwargs\n\t        \"\"\"\n\t        path = f\"/{self.container}/unions\"\n\t        params = {}\n\t        if len(kwargs) > 0:\n\t            for key in kwargs.key():\n\t                if key == \"limit\":\n\t                    if int(kwargs[\"limit\"]) > 500:\n\t                        kwargs[\"limit\"] = 500\n\t                params[key] = kwargs.get(key, \"\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getUnions\")\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params)\n\t        data = res[\"results\"]  # issue when requesting directly results.\n\t        return data\n\t    def getUnion(self, union_id: str = None, version: int = 1):\n\t        \"\"\"\n\t        Get a specific union type. Returns a dictionnary\n\t        Arguments :\n", "            union_id : REQUIRED :  meta:altId or $id\n\t            version : OPTIONAL : version of the union schema required.\n\t        \"\"\"\n\t        if union_id is None:\n\t            raise Exception(\"Require an ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getUnion\")\n\t        if union_id.startswith(\"https://\"):\n\t            from urllib import parse\n\t            union_id = parse.quote_plus(union_id)\n", "        path = f\"/{self.container}/unions/{union_id}\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader.update(\n\t            {\"Accept\": \"application/vnd.adobe.xdm-full+json; version=\" + str(version)}\n\t        )\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        return res\n\t    def getXDMprofileSchema(self):\n\t        \"\"\"\n\t        Returns a list of all schemas that are part of the XDM Individual Profile.\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getXDMprofileSchema\")\n\t        path = \"/tenant/schemas?property=meta:immutableTags==union&property=meta:class==https://ns.adobe.com/xdm/context/profile\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getDataTypes(self, **kwargs):\n\t        \"\"\"\n\t        Get the data types from a container.\n\t        Possible kwargs:\n", "            properties : str :limit the amount of properties return by comma separated list.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataTypes\")\n\t        path = f\"/{self.container}/datatypes/\"\n\t        params = {}\n\t        if kwargs.get(\"properties\", None) is not None:\n\t            params = {\"properties\": kwargs.get(\"properties\", \"title,$id\")}\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader.update({\"Accept\": \"application/vnd.adobe.xdm-id+json\"})\n", "        res = self.connector.getData(\n\t            self.endpoint + path, headers=privateHeader, params=params\n\t        )\n\t        data = res[\"results\"]\n\t        page = res.get(\"_page\",{})\n\t        nextPage = page.get('next',None)\n\t        while nextPage is not None:\n\t            res = self.connector.getData(\n\t            self.endpoint + path, headers=privateHeader, params=params\n\t            )\n", "            data += res.get(\"results\",[])\n\t            page = res.get(\"_page\",{})\n\t            nextPage = page.get('next',None)\n\t        return data\n\t    def getDataType(\n\t        self, dataTypeId: str = None, version: str = \"1\", save: bool = False\n\t    ):\n\t        \"\"\"\n\t        Retrieve a specific data type id\n\t        Argument:\n", "            dataTypeId : REQUIRED : The resource meta:altId or URL encoded $id URI.\n\t        \"\"\"\n\t        if dataTypeId is None:\n\t            raise Exception(\"Require a dataTypeId\")\n\t        if dataTypeId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            dataTypeId = parse.quote_plus(dataTypeId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDataType\")\n\t        privateHeader = deepcopy(self.header)\n", "        privateHeader.update(\n\t            {\"Accept\": \"application/vnd.adobe.xdm-full+json; version=\" + version}\n\t        )\n\t        path = f\"/{self.container}/datatypes/{dataTypeId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\", file=res, filename=res[\"title\"], type_file=\"json\"\n\t            )\n\t        return res\n", "    def createDataType(self, dataTypeObj: dict = None)->dict:\n\t        \"\"\"\n\t        Create Data Type based on the object passed.\n\t        \"\"\"\n\t        if dataTypeObj is None:\n\t            raise Exception(\"Require a dictionary to create the Data Type\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDataTypes\")\n\t        path = f\"/{self.container}/datatypes/\"\n\t        res = self.connector.postData(\n", "            self.endpoint + path, data=dataTypeObj)\n\t        return res\n\t    def patchDataType(self,dataTypeId:str=None,operations:list=None)->dict:\n\t        \"\"\"\n\t        Patch an existing data type with the operation provided.\n\t        Arguments:\n\t            dataTypeId : REQUIRED : The Data Type ID to be used\n\t            operations : REQUIRED : The list of operation to be applied on that Data Type.\n\t                    Example : '[\n\t                                {\n", "                                \"op\": \"replace\",\n\t                                \"path\": \"/loyaltyLevel/meta:enum\",\n\t                                \"value\": {\n\t                                    \"ultra-platinum\": \"Ultra Platinum\",\n\t                                    \"platinum\": \"Platinum\",\n\t                                    \"gold\": \"Gold\",\n\t                                    \"silver\": \"Silver\",\n\t                                    \"bronze\": \"Bronze\"\n\t                                }\n\t                                }\n", "                            ]'\n\t        \"\"\"\n\t        if dataTypeId is None:\n\t            raise Exception(\"Require a a data type ID\")\n\t        if operations is None:\n\t            raise Exception(\"Require a list of operation to patch\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchDataType\")\n\t        path = f\"/{self.container}/datatypes/{dataTypeId}\"\n\t        res = self.connector.patchData(\n", "            self.endpoint + path, data=operations)\n\t        return res\n\t    def putDataType(self,dataTypeId:str=None,dataTypeObj:dict=None)->dict:\n\t        \"\"\"\n\t        Replace an existing data type definition with the new definition provided.\n\t        Arguments:\n\t            dataTypeId : REQUIRED : The Data Type ID to be replaced\n\t            dataTypeObj : REQUIRED : The new Data Type definition.\n\t        \"\"\"\n\t        if dataTypeId is None:\n", "            raise Exception(\"Require a a data type ID\")\n\t        if dataTypeObj is None:\n\t            raise Exception(\"Require a dictionary to replace the Data Type definition\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting putDataType\")\n\t        path = f\"/{self.container}/datatypes/{dataTypeId}\"\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=dataTypeObj)\n\t        return res\n\t    def getDescriptors(\n", "        self,\n\t        type_desc: str = None,\n\t        id_desc: bool = False,\n\t        link_desc: bool = False,\n\t        save: bool = False,\n\t        **kwargs,\n\t    ) -> list:\n\t        \"\"\"\n\t        Return a list of all descriptors contains in that tenant id.\n\t        By default return a v2 for pagination.\n", "        Arguments:\n\t            type_desc : OPTIONAL : if you want to filter for a specific type of descriptor. None default.\n\t                (possible value : \"xdm:descriptorIdentity\")\n\t            id_desc : OPTIONAL : if you want to return only the id.\n\t            link_desc : OPTIONAL : if you want to return only the paths.\n\t            save : OPTIONAL : Boolean that would save your descriptors in the schema folder. (default False)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDescriptors\")\n\t        path = f\"/{self.container}/descriptors/\"\n", "        params = {\"start\": kwargs.get(\"start\", 0)}\n\t        if type_desc is not None:\n\t            params[\"property\"] = f\"@type=={type_desc}\"\n\t        if id_desc:\n\t            update_id = \"-id\"\n\t        else:\n\t            update_id = \"\"\n\t        if link_desc:\n\t            update_link = \"-link\"\n\t        else:\n", "            update_link = \"\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\n\t            \"Accept\"\n\t        ] = f\"application/vnd.adobe.xdm-v2{update_link}{update_id}+json\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=privateHeader\n\t        )\n\t        data = res[\"results\"]\n\t        page = res[\"_page\"]\n", "        while page[\"next\"] is not None:\n\t            data += self.getDescriptors(start=page[\"next\"])\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\", file=data, filename=\"descriptors\", type_file=\"json\"\n\t            )\n\t        return data\n\t    def getDescriptor(self, descriptorId: str = None, save: bool = False) -> dict:\n\t        \"\"\"\n\t        Return a specific descriptor\n", "        Arguments:\n\t            descriptorId : REQUIRED : descriptor ID to return (@id).\n\t            save : OPTIONAL : Boolean that would save your descriptors in the schema folder. (default False)\n\t        \"\"\"\n\t        if descriptorId is None:\n\t            raise Exception(\"Require a descriptor id\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDescriptor\")\n\t        path = f\"/{self.container}/descriptors/{descriptorId}\"\n\t        privateHeader = deepcopy(self.header)\n", "        privateHeader[\"Accept\"] = f\"application/vnd.adobe.xdm+json\"\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"schema\",\n\t                file=res,\n\t                filename=f'{res[\"@id\"]}_descriptors',\n\t                type_file=\"json\",\n\t            )\n\t        return res\n", "    def createDescriptor(\n\t        self,\n\t        descriptorObj:dict = None,\n\t        desc_type: str = \"xdm:descriptorIdentity\",\n\t        sourceSchema: str = None,\n\t        sourceProperty: str = None,\n\t        namespace: str = None,\n\t        primary: bool = None,\n\t        **kwargs,\n\t    ) -> dict:\n", "        \"\"\"\n\t        Create a descriptor attached to a specific schema.\n\t        Arguments:\n\t            descriptorObj : REQUIRED : If you wish to pass the whole object.\n\t            desc_type : REQUIRED : the type of descriptor to create.(default Identity)\n\t            sourceSchema : REQUIRED : the schema attached to your identity ()\n\t            sourceProperty : REQUIRED : the path to the field\n\t            namespace : REQUIRED : the namespace used for the identity\n\t            primary : OPTIONAL : Boolean (True or False) to define if it is a primary identity or not (default None).\n\t        possible kwargs:\n", "            version : version of the creation (default 1)\n\t            xdm:property : type of property\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDescriptor\")\n\t        path = f\"/tenant/descriptors\"\n\t        if descriptorObj:\n\t            res = self.connector.postData(\n\t            self.endpoint + path, data=descriptorObj)\n\t        else:\n", "            if sourceSchema is None or sourceProperty is None:\n\t                raise Exception(\"Missing required arguments.\")\n\t            obj = {\n\t                \"@type\": desc_type,\n\t                \"xdm:sourceSchema\": sourceSchema,\n\t                \"xdm:sourceVersion\": kwargs.get(\"version\", 1),\n\t                \"xdm:sourceProperty\": sourceProperty,\n\t            }\n\t            if namespace is not None:\n\t                obj[\"xdm:namespace\"] = namespace\n", "            if primary is not None:\n\t                obj[\"xdm:isPrimary\"] = primary\n\t            for key in kwargs:\n\t                if 'xdm:' in key:\n\t                    obj[key] = kwargs.get(key)\n\t            res = self.connector.postData(\n\t                self.endpoint + path, data=obj)\n\t        return res\n\t    def deleteDescriptor(self, descriptor_id: str = None) -> str:\n\t        \"\"\"\n", "        Delete a specific descriptor.\n\t        Arguments:\n\t            descriptor_id : REQUIRED : the descriptor id to delete\n\t        \"\"\"\n\t        if descriptor_id is None:\n\t            raise Exception(\"Require a descriptor id\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteDescriptor\")\n\t        path = f\"/{self.container}/descriptors/{descriptor_id}\"\n\t        privateHeader = deepcopy(self.header)\n", "        privateHeader[\"Accept\"] = f\"application/vnd.adobe.xdm+json\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=privateHeader)\n\t        return res\n\t    def putDescriptor(\n\t        self,\n\t        descriptorId: str = None,\n\t        descriptorObj:dict = None,\n\t        desc_type: str = \"xdm:descriptorIdentity\",\n\t        sourceSchema: str = None,\n\t        sourceProperty: str = None,\n", "        namespace: str = None,\n\t        xdmProperty: str = \"xdm:code\",\n\t        primary: bool = False,\n\t        **kwargs\n\t    ) -> dict:\n\t        \"\"\"\n\t        Replace the descriptor with the new definition. It updates the whole definition.\n\t        Arguments:\n\t            descriptorId : REQUIRED : the descriptor id to replace\n\t            descriptorObj : REQUIRED : The full descriptor object if you want to pass it directly.\n", "            desc_type : REQUIRED : the type of descriptor to create.(default Identity)\n\t            sourceSchema : REQUIRED : the schema attached to your identity ()\n\t            sourceProperty : REQUIRED : the path to the field\n\t            namespace : REQUIRED : the namespace used for the identity\n\t            xdmProperty : OPTIONAL : xdm code for the descriptor (default : xdm:code)\n\t            primary : OPTIONAL : Boolean to define if it is a primary identity or not (default False).\n\t        \"\"\"\n\t        if descriptorId is None:\n\t            raise Exception(\"Require a descriptor id\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting putDescriptor\")\n\t        path = f\"/{self.container}/descriptors/{descriptorId}\"\n\t        if sourceSchema is None or sourceProperty is None or namespace is None:\n\t            raise Exception(\"Missing required arguments.\")\n\t        if descriptorObj is not None and type(descriptorObj) == dict:\n\t            obj = descriptorObj\n\t        else:\n\t            obj = {\n\t            \"@type\": desc_type,\n\t            \"xdm:sourceSchema\": sourceSchema,\n", "            \"xdm:sourceVersion\": 1,\n\t            \"xdm:sourceProperty\": sourceProperty,\n\t            \"xdm:namespace\": namespace,\n\t            \"xdm:property\": xdmProperty,\n\t            \"xdm:isPrimary\": primary,\n\t            }\n\t            for key in kwargs:\n\t                if 'xdm:' in key:\n\t                    obj[key] = kwargs.get(key)\n\t        res = self.connector.putData(\n", "            self.endpoint + path, data=obj)\n\t        return res\n\t    def getAuditLogs(self, resourceId: str = None) -> list:\n\t        \"\"\"\n\t        Returns the list of the changes made to a ressource (schema, class, mixin).\n\t        Arguments:\n\t            resourceId : REQUIRED : The \"$id\" or \"meta:altId\" of the resource.\n\t        \"\"\"\n\t        if not resourceId:\n\t            raise ValueError(\"resourceId should be included as a parameter\")\n", "        if resourceId.startswith(\"https://\"):\n\t            from urllib import parse\n\t            resourceId = parse.quote_plus(resourceId)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDescriptor\")\n\t        path: str = f\"/rpc/auditlog/{resourceId}\"\n\t        res: list = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def exportResource(self,resourceId:str=None,Accept:str=\"application/vnd.adobe.xed+json; version=1\")->dict:\n\t        \"\"\"\n", "        Return all the associated references required for importing the resource in a new sandbox or a new Org.\n\t        Argument:\n\t            resourceId : REQUIRED : The $id or meta:altId of the resource to export.\n\t            Accept : OPTIONAL : If you want to change the Accept header of the request.\n\t        \"\"\"\n\t        if resourceId is None:\n\t            raise ValueError(\"Require a resource ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting exportResource for resourceId : {resourceId}\")\n\t        if resourceId.startswith(\"https://\"):\n", "            from urllib import parse\n\t            resourceId = parse.quote_plus(resourceId)\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader['Accept'] = Accept\n\t        path = f\"/rpc/export/{resourceId}\"\n\t        res = self.connector.getData(self.endpoint +path,headers=privateHeader)\n\t        return res\n\t    def importResource(self,dataResource:dict = None)->dict:\n\t        \"\"\"\n\t        Import a resource based on the export method.\n", "        Arguments:\n\t            dataResource : REQUIRED : dictionary of the resource retrieved.\n\t        \"\"\"\n\t        if dataResource is None:\n\t            raise ValueError(\"a dictionary presenting the resource to be imported should be included as a parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting importResource\")\n\t        path: str = f\"/rpc/export/\"\n\t        res: list = self.connector.postData(self.endpoint + path, data=dataResource)\n\t        return res\n", "    def extendFieldGroup(self,fieldGroupId:str=None,values:list=None,tenant:str='tenant')->dict:\n\t        \"\"\"\n\t        Patch a Field Group to extend its compatibility with ExperienceEvents, IndividualProfile and Record.\n\t        Arguments:\n\t            fieldGroupId : REQUIRED : meta:altId or $id of the field group.\n\t            values : OPTIONAL : If you want to pass the behavior you want to extend the field group to.\n\t                Examples: [\"https://ns.adobe.com/xdm/context/profile\",\n\t                      \"https://ns.adobe.com/xdm/context/experienceevent\",\n\t                    ]\n\t                by default profile and experienceEvent will be added to the FieldGroup.\n", "            tenant : OPTIONAL : default \"tenant\", possible value 'global'\n\t        \"\"\"\n\t        if fieldGroupId is None:\n\t            raise Exception(\"Require a field Group ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting extendFieldGroup\")\n\t        path = f\"/{tenant}/fieldgroups/{fieldGroupId}\"\n\t        if values is not None:\n\t            list_fgs = values\n\t        else:\n", "            list_fgs = [\"https://ns.adobe.com/xdm/context/profile\",\n\t                      \"https://ns.adobe.com/xdm/context/experienceevent\"]\n\t        operation = [\n\t           { \n\t            \"op\": \"replace\",\n\t            \"path\": \"/meta:intendedToExtend\",\n\t            \"value\": list_fgs\n\t            }\n\t        ]\n\t        res = self.connector.patchData(self.endpoint + path,data=operation)\n", "        return res\n\t    def enableSchemaForRealTime(self,schemaId:str=None)->dict:\n\t        \"\"\"\n\t        Enable a schema for real time based on its ID.\n\t        Arguments:\n\t            schemaId : REQUIRED : The schema ID required to be updated\n\t        \"\"\"\n\t        if schemaId is None:\n\t            raise Exception(\"Require a schema ID\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting enableSchemaForRealTime\")\n\t        path = f\"/{self.container}/schemas/{schemaId}/\"\n\t        operation = [\n\t           { \n\t            \"op\": \"add\",\n\t            \"path\": \"/meta:immutableTags\",\n\t            \"value\": [\"union\"]\n\t            }\n\t        ]\n\t        res = self.connector.patchData(self.endpoint + path,data=operation)\n", "        return res\n\t    def FieldGroupManager(self,fieldGroup:Union[dict,str,None],title:str=None,fg_class:list=[\"experienceevent\",\"profile\"]) -> 'FieldGroupManager':\n\t         \"\"\"\n\t         Generate a field group Manager instance using the information provided by the schema instance.\n\t         Arguments:\n\t             fieldGroup : OPTIONAL : the field group definition as dictionary OR the ID to access it OR nothing if you want to start from scratch\n\t             title : OPTIONAL : If you wish to change the tile of the field group.\n\t         \"\"\"\n\t         tenantId = self.getTenantId()\n\t         return FieldGroupManager(tenantId=tenantId,fieldGroup=fieldGroup,title=title,fg_class=fg_class,schemaAPI=self)\n", "    def SchemaManager(self,schema:Union[dict,str],fieldGroups:list=None) -> 'FieldGroupManager':\n\t         \"\"\"\n\t         Generate a Schema Manager instance using the information provided by the schema instance.\n\t         Arguments:\n\t            schema : OPTIONAL : the schema definition as dictionary OR the ID to access it OR Nothing if you want to start from scratch\n\t            fieldGroups : OPTIONAL : If you wish to add a list of fieldgroups.\n\t            fgManager : OPTIONAL : If you wish to handle the different field group passed into a Field Group Manager instance and have additional methods available.\n\t         \"\"\"\n\t         return SchemaManager(schema=schema,fieldGroups=fieldGroups,schemaAPI=self)\n\t    def compareDFschemas(self,df1,df2,**kwargs)->dict:\n", "        \"\"\"\n\t        Compare 2 schema dataframe returned by the SchemaManager `to_dataframe` method.\n\t        Arguments:\n\t            df1 : REQUIRED : the first schema dataframe to compare\n\t            df2 : REQUIRED : the second schema dataframe to compare\n\t        possible keywords:\n\t            title1 : title of the schema used in the dataframe 1 (default df1)\n\t            title2 : title of the schema used in the dataframe 2 (default df2)\n\t        The title1 and title2 will be used instead of df1 or df2 in the results keys presented below.\n\t        Results: \n", "            Results are stored in a dictionary with these keys:\n\t            - df1 (or title1) : copy of the dataframe 1 passed\n\t            - df2 (or title2) : copy of the dataframe 2 passed\n\t            - fielgroups: dictionary containing\n\t                - aligned : boolean to define if the schema dataframes contain the same field groups\n\t                - df1_missingFieldGroups : tuple of field groups missing on df1 compare to df2\n\t                - df2_missingFieldGroups : tuple of field groups missing on df2 compare to df1\n\t            - paths: dictionary containing\n\t                - aligned : boolean to define if the schema dataframes contain the same fields.\n\t                - df1_missing : tuple of the paths missing in df1 compare to df2\n", "                - df2_missing : tuple of the paths missing in df2 compare to df1\n\t            - type_issues: list of all the paths that are not of the same type in both schemas.\n\t        \"\"\"\n\t        if type(df1) != pd.DataFrame or type(df2) != pd.DataFrame:\n\t            raise TypeError('Require dataframes to be passed')\n\t        if 'path' not in df1.columns or 'type' not in df1.columns or 'fieldGroup' not in df1.columns:\n\t            raise AttributeError('Your data frame 1 is incomplete, it does not contain one of the following columns : path, type, fieldGroup')\n\t        if 'path' not in df2.columns or 'type' not in df2.columns or 'fieldGroup' not in df2.columns:\n\t            raise AttributeError('Your data frame 2 is incomplete, it does not contain one of the following columns : path, type, fieldGroup')\n\t        name1 = kwargs.get('title1','df1')\n", "        name2 = kwargs.get('title2','df2')\n\t        dict_result = {f'{name1}':df1.copy(),f'{name2}':df2.copy()}\n\t        fieldGroups1 = tuple(sorted(df1.fieldGroup.unique()))\n\t        fieldGroups2 = tuple(sorted(df2.fieldGroup.unique()))\n\t        if fieldGroups1 == fieldGroups2:\n\t            dict_result['fieldGroups'] = {'aligned':True}\n\t        else:\n\t            dict_result['fieldGroups'] = {'aligned':False}\n\t            dict_result['fieldGroups'][f'{name1}_missingFieldGroups'] = tuple(set(fieldGroups2).difference(set(fieldGroups1)))\n\t            dict_result['fieldGroups'][f'{name2}_missingFieldGroups'] = tuple(set(fieldGroups1).difference(set(fieldGroups2)))\n", "        path_df1 = tuple(sorted(df1.path.unique()))\n\t        path_df2 = tuple(sorted(df2.path.unique()))\n\t        if path_df1 == path_df2:\n\t            dict_result['paths'] = {'aligned':True}\n\t        else:\n\t            dict_result['paths'] = {'aligned':False}\n\t            list_path_missing_from_df2 = list(set(path_df2).difference(set(path_df1)))\n\t            list_path_missing_from_df1 = tuple(set(path_df1).difference(set(path_df2)))\n\t            dict_result['paths'][f'{name1}_missing'] = df2[df2[\"path\"].isin(list_path_missing_from_df2)]\n\t            dict_result['paths'][f'{name2}_missing'] = df1[df1[\"path\"].isin(list_path_missing_from_df1)]\n", "        common_paths = tuple(set(path_df2).intersection(set(path_df1)))\n\t        dict_result['type_issues'] = [] \n\t        for path in common_paths:\n\t            if df1[df1['path'] == path]['type'].values[0] != df2[df2['path'] == path]['type'].values[0]:\n\t                dict_result['type_issues'].append(path)\n\t        return dict_result\n\tclass FieldGroupManager:\n\t    \"\"\"\n\t    Class that reads and generate custom field groups\n\t    \"\"\"\n", "    def __init__(self,\n\t                fieldGroup:Union[dict,str]=None,\n\t                title:str=None,\n\t                fg_class:list=[\"experienceevent\",\"profile\"],\n\t                schemaAPI:'Schema'=None,\n\t                config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t                )->None:\n\t        \"\"\"\n\t        Instantiator for field group creation.\n\t        Arguments:\n", "            fieldGroup : OPTIONAL : the field group definition as dictionary OR the $id/altId to access it.\n\t                If you pass the $id or altId, you should pass the schemaAPI instance or have uploaded a configuration file.\n\t            title : OPTIONAL : If you want to name the field group.\n\t            fg_class : OPTIONAL : the class that will support this field group.\n\t                by default events and profile, possible value : \"record\"\n\t            schemaAPI : OPTIONAL : The instance of the Schema class. Provide a way to connect to the API.\n\t            config : OPTIONAL : The config object in case you want to override the configuration.\n\t        \"\"\"\n\t        self.EDITABLE = False\n\t        self.fieldGroup = {}\n", "        if schemaAPI is not None and type(schemaAPI) == Schema:\n\t            self.schemaAPI = schemaAPI\n\t        else:\n\t            self.schemaAPI = Schema(config=config)\n\t        self.tenantId = f\"_{self.schemaAPI.getTenantId()}\"\n\t        if fieldGroup is not None:\n\t            if type(fieldGroup) == dict:\n\t                if fieldGroup.get(\"meta:resourceType\",None) == \"mixins\":\n\t                    if fieldGroup.get('definitions',None) is not None:\n\t                        if 'mixins' in fieldGroup.get('$id'):\n", "                            self.fieldGroup = self.schemaAPI.getFieldGroup(fieldGroup['$id'],full=False)\n\t                            if '/datatypes/' in str(self.fieldGroup): ## if custom datatype used in Field Group \n\t                                self.fieldGroup = self.schemaAPI.getFieldGroup(fieldGroup['$id'],full=True)\n\t                            else:\n\t                                self.EDITABLE = True\n\t                        else:\n\t                            tmp_def = self.schemaAPI.getFieldGroup(fieldGroup['$id'],full=True) ## handling default mixins\n\t                            tmp_def['definitions'] = tmp_def['properties']\n\t                            self.fieldGroup = tmp_def\n\t                    else:\n", "                        self.fieldGroup = self.schemaAPI.getFieldGroup(fieldGroup['$id'],full=False)\n\t            elif type(fieldGroup) == str and (fieldGroup.startswith('https:') or fieldGroup.startswith(f'{self.tenantId}.')):\n\t                if self.schemaAPI is None:\n\t                    raise Exception(\"You try to retrieve the fieldGroup definition from the id, but no API has been passed in the schemaAPI parameter.\")\n\t                if 'mixins' in fieldGroup:\n\t                    self.fieldGroup = self.schemaAPI.getFieldGroup(fieldGroup,full=False)\n\t                    if '/datatypes/' in str(self.fieldGroup): ## if custom datatype used in Field Group \n\t                        self.fieldGroup = self.schemaAPI.getFieldGroup(fieldGroup['$id'],full=True)\n\t                    else:\n\t                        self.EDITABLE = True\n", "                else: ## handling default mixins\n\t                    tmp_def = self.schemaAPI.getFieldGroup(fieldGroup,full=True) ## handling default mixins\n\t                    tmp_def['definitions'] = tmp_def['properties']\n\t                    self.fieldGroup = tmp_def\n\t            else:\n\t                raise ValueError(\"the element pass is not a field group definition\")\n\t        else:\n\t            self.EDITABLE = True\n\t            self.fieldGroup = {\n\t                \"title\" : \"\",\n", "                \"meta:resourceType\":\"mixins\",\n\t                \"description\" : \"\",\n\t                \"type\": \"object\",\n\t                \"definitions\":{\n\t                    \"customFields\":{\n\t                        \"type\" : \"object\",\n\t                        \"properties\":{\n\t                            self.tenantId:{\n\t                                \"properties\":{},\n\t                                \"type\" : \"object\"\n", "                            },\n\t                        }\n\t                    },\n\t                    \"property\":{\n\t                        \"type\" : \"object\",\n\t                        \"properties\":{\n\t                            self.tenantId:{\n\t                                \"properties\":{},\n\t                                \"type\" : \"object\"\n\t                            },\n", "                        }\n\t                    },\n\t                },\n\t                'allOf':[{\n\t                    \"$ref\": \"#/definitions/customFields\",\n\t                    \"type\": \"object\"\n\t                },\n\t                {\n\t                    \"$ref\": \"#/definitions/property\",\n\t                    \"type\": \"object\"\n", "                }],\n\t                \"meta:intendedToExtend\":[],\n\t                \"meta:containerId\": \"tenant\",\n\t                \"meta:tenantNamespace\": self.tenantId,\n\t            }\n\t            if self.fieldGroup.get(\"meta:intendedToExtend\") == []:\n\t                for cls in fg_class:\n\t                    if 'experienceevent' in cls or \"https://ns.adobe.com/xdm/context/experienceevent\" ==cls:\n\t                        self.fieldGroup[\"meta:intendedToExtend\"].append(\"https://ns.adobe.com/xdm/context/experienceevent\")\n\t                    elif \"profile\" in cls or \"https://ns.adobe.com/xdm/context/profile\" == cls:\n", "                        self.fieldGroup[\"meta:intendedToExtend\"].append(\"https://ns.adobe.com/xdm/context/profile\")\n\t                    elif \"record\" in cls or \"https://ns.adobe.com/xdm/data/record\" == cls:\n\t                        self.fieldGroup[\"meta:intendedToExtend\"].append(\"https://ns.adobe.com/xdm/context/profile\")\n\t        if len(self.fieldGroup.get('allOf',[]))>1:\n\t            ### handling the custom field group based on existing ootb field groups\n\t            for element in self.fieldGroup.get('allOf'):\n\t                if element.get('$ref') != '#/definitions/customFields' and element.get('$ref') != '#/definitions/property':\n\t                    additionalDefinition = self.schemaAPI.getFieldGroup(element.get('$ref'),full=True)\n\t                    self.fieldGroup['definitions'] = self.__simpleDeepMerge__(self.fieldGroup['definitions'],additionalDefinition.get('properties'))\n\t        self.__setAttributes__(self.fieldGroup)\n", "        if title is not None:\n\t            self.fieldGroup['title'] = title\n\t            self.title = title\n\t    def __setAttributes__(self,fieldGroup:dict)->None:\n\t        uniqueId = fieldGroup.get('id',str(int(time.time()*100))[-7:])\n\t        self.title = self.fieldGroup.get('title',f'unknown:{uniqueId}')\n\t        if self.fieldGroup.get('$id',False):\n\t            self.id = self.fieldGroup.get('$id')\n\t        if self.fieldGroup.get('meta:altId',False):\n\t            self.altId = self.fieldGroup.get('meta:altId')\n", "    def __str__(self)->str:\n\t        return json.dumps(self.fieldGroup,indent=2)\n\t    def __repr__(self)->dict:\n\t        return json.dumps(self.fieldGroup,indent=2)\n\t    def __simpleDeepMerge__(self,base:dict,append:dict)->dict:\n\t        \"\"\"\n\t        Loop through the keys of 2 dictionary and append the new found key of append to the base.\n\t        Arguments:\n\t            base : The base you want to extend\n\t            append : the new dictionary to append\n", "        \"\"\"\n\t        if type(append) == list:\n\t            append = append[0]\n\t        for key in append:\n\t            if type(base)==dict:\n\t                if key in base.keys():\n\t                    self.__simpleDeepMerge__(base[key],append[key])\n\t                else:\n\t                    base[key] = append[key]\n\t            elif type(base)==list:\n", "                base = base[0]\n\t                if type(base) == dict:\n\t                    if key in base.keys():\n\t                        self.__simpleDeepMerge__(base[key],append[key])\n\t                    else:\n\t                        base[key] = append[key]\n\t        return base\n\t    def __accessorAlgo__(self,mydict:dict,path:list=None)->dict:\n\t        \"\"\"\n\t        recursive method to retrieve all the elements.\n", "        Arguments:\n\t            mydict : REQUIRED : The dictionary containing the elements to fetch (in \"properties\" key)\n\t            path : the path with dot notation.\n\t        \"\"\"\n\t        path = self.__cleanPath__(path)\n\t        pathSplit = path.split('.')\n\t        key = pathSplit[0]\n\t        if 'customFields' in mydict.keys():\n\t            level = self.__accessorAlgo__(mydict.get('customFields',{}).get('properties',{}),'.'.join(pathSplit))\n\t            if 'error' not in level.keys():\n", "                return level\n\t        if 'property' in mydict.keys() :\n\t            level = self.__accessorAlgo__(mydict.get('property',{}).get('properties',{}),'.'.join(pathSplit))\n\t            return level\n\t        level = mydict.get(key,None)\n\t        if level is not None:\n\t            if level[\"type\"] == \"object\":\n\t                levelProperties = mydict[key].get('properties',None)\n\t                if levelProperties is not None:\n\t                    level = self.__accessorAlgo__(levelProperties,'.'.join(pathSplit[1:]))\n", "                return level\n\t            elif level[\"type\"] == \"array\":\n\t                levelProperties = mydict[key]['items'].get('properties',None)\n\t                if levelProperties is not None:\n\t                    level = self.__accessorAlgo__(levelProperties,'.'.join(pathSplit[1:]))\n\t                return level\n\t            else:\n\t                if len(pathSplit) > 1: \n\t                    return {'error':f'cannot find the key \"{pathSplit[1]}\"'}\n\t                return level\n", "        else:\n\t            if key == \"\":\n\t                return mydict\n\t            return {'error':f'cannot find the key \"{key}\"'}\n\t    def __searchAlgo__(self,mydict:dict,string:str=None,partialMatch:bool=False,caseSensitive:bool=False,results:list=None,path:str=None,completePath:str=None)->list:\n\t        \"\"\"\n\t        recursive method to retrieve all the elements.\n\t        Arguments:\n\t            mydict : REQUIRED : The dictionary containing the elements to fetch (start with fieldGroup definition)\n\t            string : the string to look for with dot notation.\n", "            partialMatch : if you want to use partial match\n\t            caseSensitive : to see if we should lower case everything\n\t            results : the list of results to return\n\t            path : the path currently set\n\t            completePath : the complete path from the start.\n\t        \"\"\"\n\t        finalPath = None\n\t        if results is None:\n\t            results=[]\n\t        for key in mydict:\n", "            if caseSensitive == False:\n\t                keyComp = key.lower()\n\t                string = string.lower()\n\t            else:\n\t                keyComp = key\n\t                string = string\n\t            if partialMatch:\n\t                if string in keyComp:\n\t                    ### checking if element is an array without deeper object level\n\t                    if mydict[key].get('type') == 'array' and mydict[key]['items'].get('properties',None) is None:\n", "                        finalPath = path + f\".{key}[]\"\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = f\"{key}\"\n\t                    else:\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = f\"{key}\"\n", "                    value = deepcopy(mydict[key])\n\t                    value['path'] = finalPath\n\t                    value['queryPath'] = self.__cleanPath__(finalPath)\n\t                    if completePath is None:\n\t                        value['completePath'] = f\"/definitions/{key}\"\n\t                    else:\n\t                        value['completePath'] = completePath + \"/\" + key\n\t                    results.append({key:value})\n\t            else:\n\t                if caseSensitive == False:\n", "                    if keyComp == string:\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = key\n\t                        value = deepcopy(mydict[key])\n\t                        value['path'] = finalPath\n\t                        value['queryPath'] = self.__cleanPath__(finalPath)\n\t                        if completePath is None:\n\t                            value['completePath'] = f\"/definitions/{key}\"\n", "                        else:\n\t                            value['completePath'] = completePath + \"/\" + key\n\t                        results.append({key:value})\n\t                else:\n\t                    if keyComp == string:\n\t                        if path is not None:\n\t                            finalPath = path + f\".{key}\"\n\t                        else:\n\t                            finalPath = key\n\t                        value = deepcopy(mydict[key])\n", "                        value['path'] = finalPath\n\t                        value['queryPath'] = self.__cleanPath__(finalPath)\n\t                        if completePath is None:\n\t                            value['completePath'] = f\"/definitions/{key}\"\n\t                        else:\n\t                            value['completePath'] = completePath + \"/\" + key\n\t                        results.append({key:value})\n\t            ## loop through keys\n\t            if mydict[key].get(\"type\") == \"object\" or 'properties' in mydict[key].keys():\n\t                levelProperties = mydict[key].get('properties',{})\n", "                if levelProperties != dict():\n\t                    if completePath is None:\n\t                        tmp_completePath = f\"/definitions/{key}\"\n\t                    else:\n\t                        tmp_completePath = f\"{completePath}/{key}\"\n\t                    tmp_completePath += f\"/properties\"\n\t                    if path is None:\n\t                        if key != \"property\" and key != \"customFields\" :\n\t                            tmp_path = key\n\t                        else:\n", "                            tmp_path = None\n\t                    else:\n\t                        tmp_path = f\"{path}.{key}\"\n\t                    results = self.__searchAlgo__(levelProperties,string,partialMatch,caseSensitive,results,tmp_path,tmp_completePath)\n\t            elif mydict[key].get(\"type\") == \"array\":\n\t                levelProperties = mydict[key]['items'].get('properties',{})\n\t                if levelProperties != dict():\n\t                    if completePath is None:\n\t                        tmp_completePath = f\"/definitions/{key}\"\n\t                    else:\n", "                        tmp_completePath = f\"{completePath}/{key}\"\n\t                    tmp_completePath += f\"/items/properties\"\n\t                    if levelProperties is not None:\n\t                        if path is None:\n\t                            if key != \"property\" and key != \"customFields\":\n\t                                tmp_path = key\n\t                            else:\n\t                                tmp_path = None\n\t                        else:\n\t                            tmp_path = f\"{path}.{key}[]{{}}\"\n", "                        results = self.__searchAlgo__(levelProperties,string,partialMatch,caseSensitive,results,tmp_path,tmp_completePath)\n\t        return results\n\t    def __searchAttrAlgo__(self,mydict:dict,key:str=None,value:str=None,regex:bool=False, originalField:str=None, results:list=None)->list:\n\t        \"\"\"\n\t        recursive method to retrieve all the elements.\n\t        Arguments:\n\t            mydict : REQUIRED : The dictionary containing the elements to fetch (start with fieldGroup definition)\n\t            key : key of the attribute\n\t            value : the value of that key to look for.\n\t            regex : if the regex match should be used.\n", "            originalField : the key used to dig deeper.\n\t            results : the list of results to return\n\t        \"\"\"\n\t        if results is None:\n\t            results=[]\n\t        for k in mydict:\n\t            if key == k:\n\t                if regex:\n\t                    checkValue = deepcopy(mydict[k])\n\t                    if type(checkValue) == list or type(checkValue) == dict:\n", "                        checkValue = json.dumps(checkValue)\n\t                    if re.match(value,checkValue):\n\t                        if originalField is not None and originalField != 'property' and originalField != 'properties' and originalField != 'items':\n\t                            results.append(originalField)\n\t                else:\n\t                    if mydict[k] == value:\n\t                        if originalField is not None and originalField != 'property' and originalField != 'properties' and originalField != 'items':\n\t                            results.append(originalField)\n\t            ## recursive action for objects and array\n\t            if type(mydict[k]) == dict:\n", "                if k == \"properties\" or k == 'items':\n\t                    self.__searchAttrAlgo__(mydict[k],key,value,regex,originalField,results)\n\t                else:\n\t                    self.__searchAttrAlgo__(mydict[k],key,value,regex,k,results)\n\t        return results\n\t    def __transformationDict__(self,mydict:dict=None,typed:bool=False,dictionary:dict=None)->dict:\n\t        \"\"\"\n\t        Transform the current XDM schema to a dictionary.\n\t        \"\"\"\n\t        if dictionary is None:\n", "            dictionary = {}\n\t        else:\n\t            dictionary = dictionary\n\t        for key in mydict:\n\t            if type(mydict[key]) == dict:\n\t                if mydict[key].get('type') == 'object' or 'properties' in mydict[key].keys():\n\t                    properties = mydict[key].get('properties',None)\n\t                    if properties is not None:\n\t                        if key != \"property\" and key != \"customFields\":\n\t                            if key not in dictionary.keys():\n", "                                dictionary[key] = {}\n\t                            self.__transformationDict__(mydict[key]['properties'],typed,dictionary=dictionary[key])\n\t                        else:\n\t                            self.__transformationDict__(mydict[key]['properties'],typed,dictionary=dictionary)\n\t                elif mydict[key].get('type') == 'array':\n\t                    levelProperties = mydict[key]['items'].get('properties',None)\n\t                    if levelProperties is not None:\n\t                        dictionary[key] = [{}]\n\t                        self.__transformationDict__(levelProperties,typed,dictionary[key][0])\n\t                    else:\n", "                        if typed:\n\t                            dictionary[key] = [mydict[key]['items'].get('type','object')]\n\t                        else:\n\t                            dictionary[key] = []\n\t                else:\n\t                    if typed:\n\t                        dictionary[key] = mydict[key].get('type','object')\n\t                    else:\n\t                        dictionary[key] = \"\"\n\t        return dictionary \n", "    def __transformationDF__(self,mydict:dict=None,dictionary:dict=None,path:str=None,queryPath:bool=False,description:bool=False,xdmType:bool=False)->dict:\n\t        \"\"\"\n\t        Transform the current XDM schema to a dictionary.\n\t        Arguments:\n\t            mydict : the fieldgroup\n\t            dictionary : the dictionary that gather the paths\n\t            path : path that is currently being developed\n\t            queryPath: boolean to tell if we want to add the query path\n\t            description : boolean to tell if you want to retrieve the description\n\t            xdmType : boolean to know if you want to retrieve the xdm Type\n", "        \"\"\"\n\t        if dictionary is None:\n\t            dictionary = {'path':[],'type':[]}\n\t            if queryPath:\n\t                dictionary['querypath'] = []\n\t            if description:\n\t                dictionary['description'] = []\n\t        else:\n\t            dictionary = dictionary\n\t        for key in mydict:\n", "            if type(mydict[key]) == dict:\n\t                if mydict[key].get('type') == 'object' or 'properties' in mydict[key].keys():\n\t                    if path is None:\n\t                        if key != \"property\" and key != \"customFields\":\n\t                            tmp_path = key\n\t                        else:\n\t                            tmp_path = None\n\t                    else:\n\t                        tmp_path = f\"{path}.{key}\"\n\t                    if tmp_path is not None:\n", "                        dictionary[\"path\"].append(tmp_path)\n\t                        dictionary[\"type\"].append(f\"{mydict[key].get('type')}\")\n\t                        if queryPath:\n\t                            dictionary[\"querypath\"].append(self.__cleanPath__(tmp_path))\n\t                        if description:\n\t                            dictionary[\"description\"].append(f\"{mydict[key].get('description','')}\")\n\t                    properties = mydict[key].get('properties',None)\n\t                    if properties is not None:\n\t                        self.__transformationDF__(properties,dictionary,tmp_path,queryPath,description)\n\t                elif mydict[key].get('type') == 'array':\n", "                    levelProperties = mydict[key]['items'].get('properties',None)\n\t                    if levelProperties is not None:\n\t                        if path is None:\n\t                            tmp_path = key\n\t                        else :\n\t                            tmp_path = f\"{path}.{key}[]{{}}\"\n\t                        dictionary[\"path\"].append(tmp_path)\n\t                        dictionary[\"type\"].append(f\"[{mydict[key]['items'].get('type')}]\")\n\t                        if queryPath and tmp_path is not None:\n\t                            dictionary[\"querypath\"].append(self.__cleanPath__(tmp_path))\n", "                        if description and tmp_path is not None:\n\t                            dictionary[\"description\"].append(mydict[key]['items'].get('description',''))\n\t                        self.__transformationDF__(levelProperties,dictionary,tmp_path,queryPath,description)\n\t                    else:\n\t                        finalpath = f\"{path}.{key}\"\n\t                        dictionary[\"path\"].append(finalpath)\n\t                        dictionary[\"type\"].append(f\"[{mydict[key]['items'].get('type')}]\")\n\t                        if queryPath and finalpath is not None:\n\t                            dictionary[\"querypath\"].append(self.__cleanPath__(finalpath))\n\t                        if description and finalpath is not None:\n", "                            dictionary[\"description\"].append(mydict[key]['items'].get('description',''))\n\t                else:\n\t                    if path is not None:\n\t                        finalpath = f\"{path}.{key}\"\n\t                    else:\n\t                        finalpath = f\"{key}\"\n\t                    dictionary[\"path\"].append(finalpath)\n\t                    dictionary[\"type\"].append(mydict[key].get('type','object'))\n\t                    if queryPath and finalpath is not None:\n\t                        dictionary[\"querypath\"].append(self.__cleanPath__(finalpath))\n", "                    if description and finalpath is not None:\n\t                        dictionary[\"description\"].append(mydict[key].get('description',''))\n\t        return dictionary\n\t    def __setField__(self,completePathList:list=None,fieldGroup:dict=None,newField:str=None,obj:dict=None)->dict:\n\t        \"\"\"\n\t        Create a field with the attribute provided\n\t        Arguments:\n\t            completePathList : list of path to use for creation of the field.\n\t            fieldGroup : the self.fieldgroup attribute\n\t            newField : name of the new field to create\n", "            obj : the object associated with the new field\n\t        \"\"\"\n\t        foundFlag = False ## Flag to set if the operation has been realized or not\n\t        lastField = completePathList[-1]\n\t        fieldGroup = deepcopy(fieldGroup)\n\t        for key in fieldGroup:\n\t            level = fieldGroup.get(key,None)\n\t            if type(level) == dict and key in completePathList:\n\t                if 'properties' in level.keys():\n\t                    if key != lastField:\n", "                        res,foundFlag = self.__setField__(completePathList,fieldGroup[key]['properties'],newField,obj)\n\t                        fieldGroup[key]['properties'] = res\n\t                    else:\n\t                        fieldGroup[key]['properties'][newField] = obj\n\t                        foundFlag = True\n\t                        return fieldGroup,foundFlag\n\t                elif 'items' in level.keys():\n\t                    if 'properties' in  fieldGroup[key].get('items',{}).keys():\n\t                        if key != lastField:\n\t                            res, foundFlag = self.__setField__(completePathList,fieldGroup[key]['items']['properties'],newField,obj)\n", "                            fieldGroup[key]['items']['properties'] = res\n\t                        else:\n\t                            fieldGroup[key]['items']['properties'][newField] = obj\n\t                            foundFlag = True\n\t                            return fieldGroup,foundFlag\n\t        return fieldGroup,foundFlag\n\t    def __removeKey__(self,completePathList:list=None,fieldGroup:dict=None)->dict:\n\t        \"\"\"\n\t        Remove the key and all element based on the path provided.\n\t        Arugments:\n", "            completePathList : list of path to use for identifying the key to remove\n\t            fieldGroup : the self.fieldgroup attribute\n\t        \"\"\"\n\t        lastField = deepcopy(completePathList).pop()\n\t        success = False\n\t        for key in fieldGroup:\n\t            level = fieldGroup.get(key,None)\n\t            if type(level) == dict and key in completePathList:\n\t                if 'properties' in level.keys():\n\t                    if lastField in level['properties'].keys():\n", "                        level['properties'].pop(lastField)\n\t                        success = True\n\t                        return success\n\t                    else:\n\t                        sucess = self.__removeKey__(completePathList,fieldGroup[key]['properties'])\n\t                        return sucess\n\t                elif 'items' in level.keys():\n\t                    if 'properties' in level.get('items',{}).keys():\n\t                        if lastField in level.get('items',{}).get('properties'):\n\t                            level['items']['properties'].pop(lastField)\n", "                            success = True\n\t                            return success\n\t                        else:\n\t                            success = self.__removeKey__(completePathList,fieldGroup[key]['items']['properties'])\n\t                            return success\n\t        return success \n\t    def __transformFieldType__(self,dataType:str=None)->dict:\n\t        \"\"\"\n\t        return the object with the type and possible meta attribute.\n\t        \"\"\"\n", "        obj = {}\n\t        if dataType == 'double':\n\t            obj['type'] = \"number\"\n\t        elif dataType == 'long':\n\t            obj['type'] = \"integer\"\n\t            obj['maximum'] = 9007199254740991\n\t            obj['minimum'] = -9007199254740991\n\t        elif dataType == \"short\":\n\t            obj['type'] = \"integer\"\n\t            obj['maximum'] = 32768\n", "            obj['minimum'] = -32768\n\t        elif dataType == \"date\":\n\t            obj['type'] = \"string\"\n\t            obj['format'] = \"date\"\n\t        elif dataType == \"DateTime\":\n\t            obj['type'] = \"string\"\n\t            obj['format'] = \"date-time\"\n\t        elif dataType == \"byte\":\n\t            obj['type'] = \"integer\"\n\t            obj['maximum'] = 128\n", "            obj['minimum'] = -128\n\t        else:\n\t            obj['type'] = dataType\n\t        return obj\n\t    def __cleanPath__(self,string:str=None)->str:\n\t        \"\"\"\n\t        An abstraction to clean the path string and remove the following characters : [,],{,}\n\t        Arguments:\n\t            string : REQUIRED : a string \n\t        \"\"\"\n", "        return string.replace('[','').replace(']','').replace(\"{\",'').replace('}','')\n\t    def setTitle(self,name:str=None)->None:\n\t        \"\"\"\n\t        Set a name for the schema.\n\t        Arguments:\n\t            name : REQUIRED : a string to be used for the title of the FieldGroup\n\t        \"\"\"\n\t        self.fieldGroup['title'] = name\n\t        return None\n\t    def getField(self,path:str)->dict:\n", "        \"\"\"\n\t        Returns the field definition you want want to obtain.\n\t        Arguments:\n\t            path : REQUIRED : path with dot notation to which field you want to access\n\t        \"\"\"\n\t        definition = self.fieldGroup.get('definitions',self.fieldGroup.get('properties',{}))\n\t        data = self.__accessorAlgo__(definition,path)\n\t        return data\n\t    def searchField(self,string:str,partialMatch:bool=True,caseSensitive:bool=False)->list:\n\t        \"\"\"\n", "        Search for a field name based the string passed.\n\t        By default, partial match is enabled and allow case sensitivity option.\n\t        Arguments:\n\t            string : REQUIRED : the string to look for for one of the field\n\t            partialMatch : OPTIONAL : if you want to look for complete string or not. (default True)\n\t            caseSensitive : OPTIONAL : if you want to compare with case sensitivity or not. (default False)\n\t        \"\"\"\n\t        definition = self.fieldGroup.get('definitions',self.fieldGroup.get('properties',{}))\n\t        data = self.__searchAlgo__(definition,string,partialMatch,caseSensitive)\n\t        return data\n", "    def searchAttribute(self,attr:dict=None,regex:bool=False,extendedResults:bool=False,joinType:str='outer', **kwargs)->list:\n\t        \"\"\"\n\t        Search for an attribute on the field of the field groups.\n\t        Returns either the list of fields that match this search or their full definitions.\n\t        Arguments:\n\t            attr : REQUIRED : a dictionary of key value pair(s).  Example : {\"type\" : \"string\"} \n\t                NOTE : If you wish to have the array type on top of the array results, use the key \"arrayType\". Example : {\"type\" : \"array\",\"arrayType\":\"string\"}\n\t                        This will automatically set the joinType to \"inner\". Use type for normal search. \n\t            regex : OPTIONAL : if you want your value of your key to be matched via regex.\n\t                Note that regex will turn every comparison value to string for a \"match\" comparison.\n", "            extendedResults : OPTIONAL : If you want to have the result to contain all details of these fields. (default False)\n\t            joinType : OPTIONAL : If you pass multiple key value pairs, how do you want to get the match.\n\t                outer : provide the fields if any of the key value pair is matched.\n\t                inner : provide the fields if all the key value pair matched.\n\t        \"\"\"\n\t        resultsDict = {f\"{key}\":[] for key in attr.keys()}\n\t        if 'arrayType' in attr.keys(): ## forcing inner join\n\t            joinType = 'inner'\n\t        definition = self.fieldGroup.get('definitions',self.fieldGroup.get('properties',{}))\n\t        for key in attr:\n", "            if key == \"arrayType\":\n\t                resultsDict[key] += self.__searchAttrAlgo__(definition,\"type\",attr[key],regex)\n\t            else:\n\t                resultsDict[key] += self.__searchAttrAlgo__(definition,key,attr[key],regex)\n\t        result_combi = []\n\t        if joinType == 'outer':\n\t            for key in resultsDict:\n\t                result_combi += resultsDict[key]\n\t            result_combi = set(result_combi)\n\t        elif joinType == 'inner':\n", "            result_combi = set()\n\t            for key in resultsDict:\n\t                resultsDict[key] = set(resultsDict[key])\n\t                if len(result_combi) == 0:\n\t                    result_combi = resultsDict[key]\n\t                else:\n\t                    result_combi = result_combi.intersection(resultsDict[key]) \n\t        if extendedResults:\n\t            result_extended = []\n\t            for field in result_combi:\n", "                result_extended += self.searchField(field,partialMatch=False,caseSensitive=True)\n\t            return result_extended\n\t        return list(result_combi)\n\t    def addFieldOperation(self,path:str,dataType:str=None,title:str=None,objectComponents:dict=None,array:bool=False,enumValues:dict=None,enumType:bool=None,**kwargs)->None:\n\t        \"\"\"\n\t        Return the operation to be used on the field group with the Patch method (patchFieldGroup), based on the element passed in argument.\n\t        Arguments:\n\t            path : REQUIRED : path with dot notation where you want to create that new field.\n\t                In case of array of objects, use the \"[]{}\" notation\n\t            dataType : REQUIRED : the field type you want to create\n", "                A type can be any of the following: \"string\",\"boolean\",\"double\",\"long\",\"integer\",\"short\",\"byte\",\"date\",\"dateTime\",\"boolean\",\"object\",\"array\"\n\t                NOTE : \"array\" type is to be used for array of objects. If the type is string array, use the boolean \"array\" parameter.\n\t            title : OPTIONAL : if you want to have a custom title.\n\t            objectComponents: OPTIONAL : A dictionary with the name of the fields contain in the \"object\" or \"array of objects\" specify, with their typed.\n\t                Example : {'field1':'string','field2':'double'}\n\t            array : OPTIONAL : Boolean. If the element to create is an array. False by default.\n\t            enumValues : OPTIONAL : If your field is an enum, provid a dictionary of value and display name, such as : {'value':'display'}\n\t            enumType: OPTIONAL: If your field is an enum, indicates whether it is an enum (True) or suggested values (False)\n\t        possible kwargs:\n\t            defaultPath : Define which path to take by default for adding new field on tenant. Default \"property\", possible alternative : \"customFields\"\n", "        \"\"\"\n\t        if self.EDITABLE == False:\n\t            raise Exception(\"The Field Group is not Editable via Field Group Manager\")\n\t        typeTyped = [\"string\",\"boolean\",\"double\",\"long\",\"integer\",\"short\",\"byte\",\"date\",\"dateTime\",\"boolean\",\"object\",'array']\n\t        if dataType not in typeTyped:\n\t            raise TypeError('Expecting one of the following type : \"string\",\"boolean\",\"double\",\"long\",\"integer\",\"short\",\"byte\",\"date\",\"dateTime\",\"boolean\",\"object\"')\n\t        if dataType == 'object' and objectComponents is None:\n\t            raise AttributeError('Require a dictionary providing the object component')       \n\t        if title is None:\n\t            title = self.__cleanPath__(path.split('.').pop())\n", "        if title == 'items' or title == 'properties':\n\t            raise Exception('\"item\" and \"properties\" are 2 reserved keywords')\n\t        pathSplit = path.split('.')\n\t        if pathSplit[0] == '':\n\t            del pathSplit[0]\n\t        completePath = ['definitions',kwargs.get('defaultPath','property')]\n\t        for p in pathSplit:\n\t            if '[]{}' in p:\n\t                completePath.append(self.__cleanPath__(p))\n\t                completePath.append('items')\n", "                completePath.append('properties')\n\t            else:\n\t                completePath.append(self.__cleanPath__(p))\n\t                completePath.append('properties')\n\t        finalPath = '/' + '/'.join(completePath)\n\t        operation = [{\n\t            \"op\" : \"add\",\n\t            \"path\" : finalPath,\n\t            \"value\": {}\n\t        }]\n", "        if dataType != 'object' and dataType != \"array\":\n\t            if array: # array argument set to true\n\t                operation[0]['value']['type'] = 'array'\n\t                operation[0]['value']['items'] = self.__transformFieldType__(dataType)\n\t            else:\n\t                operation[0]['value'] = self.__transformFieldType__(dataType)\n\t        else: \n\t            if dataType == \"object\":\n\t                operation[0]['value']['type'] = self.__transformFieldType__(dataType)\n\t                operation[0]['value']['properties'] = {key:self.__transformFieldType__(value) for key, value in zip(objectComponents.keys(),objectComponents.values())}\n", "        operation[0]['value']['title'] = title\n\t        if enumValues is not None and type(enumValues) == dict:\n\t            if array == False:\n\t                operation[0]['value']['meta:enum'] = enumValues\n\t                if enumType:\n\t                    operation[0]['value']['enum'] = list(enumValues.keys())\n\t            else:\n\t                operation[0]['value']['items']['meta:enum'] = enumValues\n\t                if enumType:\n\t                    operation[0]['value']['items']['enum'] = list(enumValues.keys())\n", "        return operation\n\t    def addField(self,path:str,dataType:str=None,title:str=None,objectComponents:dict=None,array:bool=False,enumValues:dict=None,enumType:bool=None,**kwargs)->dict:\n\t        \"\"\"\n\t        Add the field to the existing fieldgroup definition.\n\t        Returns False when the field could not be inserted.\n\t        Arguments:\n\t            path : REQUIRED : path with dot notation where you want to create that new field. New field name should be included.\n\t            dataType : REQUIRED : the field type you want to create\n\t                A type can be any of the following: \"string\",\"boolean\",\"double\",\"long\",\"integer\",\"short\",\"byte\",\"date\",\"dateTime\",\"boolean\",\"object\",\"array\"\n\t                NOTE : \"array\" type is to be used for array of objects. If the type is string array, use the boolean \"array\" parameter.\n", "            title : OPTIONAL : if you want to have a custom title.\n\t            objectComponents: OPTIONAL : A dictionary with the name of the fields contain in the \"object\" or \"array of objects\" specify, with their typed.\n\t                Example : {'field1:'string','field2':'double'}\n\t            array : OPTIONAL : Boolean. If the element to create is an array. False by default.\n\t            enumValues : OPTIONAL : If your field is an enum, provid a dictionary of value and display name, such as : {'value':'display'}\n\t            enumType: OPTIONAL: If your field is an enum, indicates whether it is an enum (True) or suggested values (False)\n\t        possible kwargs:\n\t            defaultPath : Define which path to take by default for adding new field on tenant. Default \"property\", possible alternative : \"customFields\"\n\t        \"\"\"\n\t        if self.EDITABLE == False:\n", "            raise Exception(\"The Field Group is not Editable via Field Group Manager\")\n\t        if path is None:\n\t            raise ValueError(\"path must provided\")\n\t        typeTyped = [\"string\",\"boolean\",\"double\",\"long\",\"integer\",\"short\",\"byte\",\"date\",\"dateTime\",\"boolean\",\"object\",'array']\n\t        if dataType not in typeTyped:\n\t            raise TypeError('Expecting one of the following type : \"string\",\"boolean\",\"double\",\"long\",\"integer\",\"short\",\"byte\",\"date\",\"dateTime\",\"boolean\",\"object\",\"bytes\"')\n\t        if dataType == 'object' and objectComponents is None:\n\t            raise AttributeError('Require a dictionary providing the object component')\n\t        if title is None:\n\t            title = self.__cleanPath__(path.split('.').pop())\n", "        if title == 'items' or title == 'properties':\n\t            raise Exception('\"item\" and \"properties\" are 2 reserved keywords')\n\t        pathSplit = self.__cleanPath__(path).split('.')\n\t        if pathSplit[0] == '':\n\t            del pathSplit[0]\n\t        newField = pathSplit.pop()\n\t        obj = {}\n\t        if dataType == 'object':\n\t            obj = { 'type':'object', 'title':title,\n\t                'properties':{key:self.__transformFieldType__(objectComponents[key]) for key in objectComponents }\n", "            }\n\t        elif dataType == 'array':\n\t            obj = { 'type':'array', 'title':title,\n\t                \"items\":{\n\t                    'type':'object',\n\t                    'properties':{key:self.__transformFieldType__(objectComponents[key]) for key in objectComponents }\n\t                }\n\t            }\n\t        else:\n\t            obj = self.__transformFieldType__(dataType)\n", "            obj['title']= title\n\t            if array:\n\t                obj['type'] = \"array\"\n\t                obj['items'] = self.__transformFieldType__(dataType)\n\t        if enumValues is not None and type(enumValues) == dict:\n\t            if array == False:\n\t                obj['meta:enum'] = enumValues\n\t                if enumType:\n\t                    obj['enum'] = list(enumValues.keys())\n\t            else:\n", "                obj['items']['meta:enum'] = enumValues\n\t                if enumType:\n\t                    obj['items']['enum'] = list(enumValues.keys())\n\t        completePath:list[str] = [kwargs.get('defaultPath','property')] + pathSplit\n\t        customFields,foundFlag = self.__setField__(completePath, self.fieldGroup['definitions'],newField,obj)\n\t        if foundFlag == False:\n\t            return False\n\t        else:\n\t            self.fieldGroup['definitions'] = customFields\n\t            return self.fieldGroup\n", "    def removeField(self,path:str)->dict:\n\t        \"\"\"\n\t        Remove a field from the definition based on the path provided.\n\t        NOTE: A path that has received data cannot be removed from a schema or field group.\n\t        Argument:\n\t            path : REQUIRED : The path to be removed from the definition.\n\t        \"\"\"\n\t        if self.EDITABLE == False:\n\t            raise Exception(\"The Field Group is not Editable via Field Group Manager\")\n\t        if path is None:\n", "            raise ValueError('Require a path to remove it')\n\t        pathSplit = self.__cleanPath__(path).split('.')\n\t        if pathSplit[0] == '':\n\t            del pathSplit[0]\n\t        success = False\n\t        ## Try customFields\n\t        completePath:list[str] = ['customFields'] + pathSplit\n\t        success = self.__removeKey__(completePath,self.fieldGroup['definitions'])\n\t        ## Try property\n\t        if success == False:\n", "            completePath:list[str] = ['property'] + pathSplit\n\t            success = self.__removeKey__(completePath,self.fieldGroup['definitions'])\n\t        return success\n\t    def to_dict(self,typed:bool=True,save:bool=False)->dict:\n\t        \"\"\"\n\t        Generate a dictionary representing the field group constitution\n\t        Arguments:\n\t            typed : OPTIONAL : If you want the type associated with the field group to be given.\n\t            save : OPTIONAL : If you wish to save the dictionary in a JSON file\n\t        \"\"\"\n", "        definition = self.fieldGroup.get('definitions',self.fieldGroup.get('properties',{}))\n\t        data = self.__transformationDict__(definition,typed)\n\t        if save:\n\t            filename = self.fieldGroup.get('title',f'unknown_fieldGroup_{str(int(time.time()))}')\n\t            aepp.saveFile(module='schema',file=data,filename=f\"{filename}.json\",type_file='json')\n\t        return data\n\t    def to_dataframe(self,save:bool=False,queryPath:bool=False,description:bool=False)->pd.DataFrame:\n\t        \"\"\"\n\t        Generate a dataframe with the row representing each possible path.\n\t        Arguments:\n", "            save : OPTIONAL : If you wish to save it with the title used by the field group.\n\t                save as csv with the title used. Not title, used \"unknown_fieldGroup_\" + timestamp.\n\t            queryPath : OPTIONAL : If you want to have the query path to be used.\n\t            description : OPTIONAL : If you want to have the description used\n\t        \"\"\"\n\t        definition = self.fieldGroup.get('definitions',self.fieldGroup.get('properties',{}))\n\t        data = self.__transformationDF__(definition,queryPath=queryPath,description=description)\n\t        df = pd.DataFrame(data)\n\t        if save:\n\t            title = self.fieldGroup.get('title',f'unknown_fieldGroup_{str(int(time.time()))}')\n", "            df.to_csv(f\"{title}.csv\",index=False)\n\t        return df\n\t    def to_xdm(self)->dict:\n\t        \"\"\"\n\t        Return the fieldgroup definition as XDM\n\t        \"\"\"\n\t        return self.fieldGroup\n\t    def patchFieldGroup(self,operations:list=None)->dict:\n\t        \"\"\"\n\t        Patch the field group with the given operation.\n", "        Arguments:\n\t            operation : REQUIRED : The list of operation to realise\n\t        \"\"\"\n\t        if self.EDITABLE == False:\n\t            raise Exception(\"The Field Group is not Editable via Field Group Manager\")\n\t        if operations is None or type(operations) != list:\n\t            raise ValueError('Require a list of operations')\n\t        if self.schemaAPI is None:\n\t            Exception('Require a schema API connection. Pass the instance of a Schema class or import a configuration file.')\n\t        res = self.schemaAPI.patchFieldGroup(self.id,operations)\n", "        if 'status' in res.keys():\n\t            if res['status'] >= 400:\n\t                print(res['title'])\n\t                return res\n\t            else:\n\t                return res\n\t        self.fieldGroup = res\n\t        self.__setAttributes__(self.fieldGroup)\n\t        return res\n\t    def updateFieldGroup(self)->dict:\n", "        \"\"\"\n\t        Use the PUT method to push the current field group representation to AEP via API request.\n\t        \"\"\"\n\t        if self.EDITABLE == False:\n\t            raise Exception(\"The Field Group is not Editable via Field Group Manager\")\n\t        if self.schemaAPI is None:\n\t            Exception('Require a schema API connection. Pass the instance of a Schema class or import a configuration file.')\n\t        res = self.schemaAPI.putFieldGroup(self.id,self.to_xdm())\n\t        if 'status' in res.keys():\n\t            if res['status'] >= 400:\n", "                print(res['title'])\n\t                return res\n\t            else:\n\t                return res\n\t        self.fieldGroup = res\n\t        self.__setAttributes__(self.fieldGroup)\n\t        return res\n\t    def createFieldGroup(self)->dict:\n\t        \"\"\"\n\t        Use the POST method to create the field group in the organization.\n", "        \"\"\"\n\t        if self.schemaAPI is None:\n\t            Exception('Require a schema API connection. Pass the instance of a Schema class or import a configuration file.')\n\t        res = self.schemaAPI.createFieldGroup(self.to_xdm())\n\t        if 'status' in res.keys():\n\t            if res['status'] >= 400:\n\t                print(res['title'])\n\t                return res\n\t            else:\n\t                return res\n", "        self.fieldGroup = res\n\t        self.__setAttributes__(self.fieldGroup)\n\t        return res\n\tclass SchemaManager:\n\t    \"\"\"\n\t    A class to handle the schema management.\n\t    \"\"\"\n\t    DESCRIPTOR_TYPES =[\"xdm:descriptorIdentity\",\"xdm:alternateDisplayInfo\",\"xdm:descriptorOneToOne\",\"xdm:descriptorReferenceIdentity\",\"xdm:descriptorDeprecated\"]\n\t    def __init__(self,schema:Union[str,dict],\n\t                fieldGroups:list=None,\n", "                schemaAPI:'Schema'=None,\n\t                schemaClass:str=None,\n\t                config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t                )->None:\n\t        \"\"\"\n\t        Instantiate the Schema Manager instance.\n\t        Arguments:\n\t            schemaId : OPTIONAL : Either a schemaId ($id or altId) or the schema dictionary itself.\n\t                If schemaId is passed, you need to provide the schemaAPI connection as well.\n\t            fieldGroups : OPTIONAL : Possible to specify a list of fieldGroup. \n", "                Either a list of fieldGroupIds (schemaAPI should be provided as well) or list of dictionary definition \n\t            schemaAPI : OPTIONAL : It is required if $id or altId are used. It is the instance of the Schema class.\n\t            schemaClass : OPTIONAL : If you want to set the class to be a specific class.\n\t                Default value is profile: \"https://ns.adobe.com/xdm/context/profile\", can be replaced with any class definition.\n\t                Possible default value: \"https://ns.adobe.com/xdm/context/experienceevent\", \"https://ns.adobe.com/xdm/context/segmentdefinition\"\n\t            config : OPTIONAL : The config object in case you want to override the configuration.\n\t        \"\"\"\n\t        self.fieldGroupIds=[]\n\t        self.fieldGroupsManagers = []\n\t        self.title = None\n", "        if schemaAPI is not None:\n\t            self.schemaAPI = schemaAPI\n\t        else:\n\t            self.schemaAPI = Schema(config=config)\n\t        if type(schema) == dict:\n\t            self.schema = schema\n\t            self.__setAttributes__(self.schema)\n\t            allOf = self.schema.get(\"allOf\",[])\n\t            if len(allOf) == 0:\n\t                Warning(\"You have passed a schema with -full attribute, you should pass one referencing the fieldGroups.\\n Using the meta:extends reference if possible\")\n", "                self.fieldGroupIds = [ref for ref in self.schema['meta:extends'] if ('/mixins/' in ref or '/experience/' in ref or '/context/' in ref) and ref != self.classId]\n\t                self.schema['allOf'] = [{\"$ref\":ref} for ref in self.schema['meta:extends'] if ('/mixins/' in ref or 'xdm/class' in ref or 'xdm/context/' in ref) and ref != self.classId]\n\t            else:\n\t                self.fieldGroupIds = [obj['$ref'] for obj in allOf if ('/mixins/' in obj['$ref'] or '/experience/' in obj['$ref'] or '/context/' in obj['$ref']) and obj['$ref'] != self.classId]\n\t            if self.schemaAPI is None:\n\t                Warning(\"No schema instance has been passed or config file imported.\\n Aborting the creation of field Group Manager\")\n\t            else:\n\t                for ref in self.fieldGroupIds:\n\t                    if '/mixins/' in ref:\n\t                        definition = self.schemaAPI.getFieldGroup(ref,full=False)\n", "                    else:\n\t                        definition = self.schemaAPI.getFieldGroup(ref,full=True)\n\t                        definition['definitions'] = definition['properties']\n\t                    self.fieldGroupsManagers.append(FieldGroupManager(fieldGroup=definition,schemaAPI=self.schemaAPI))\n\t        elif type(schema) == str:\n\t            if self.schemaAPI is None:\n\t                Warning(\"No schema instance has been passed or config file imported.\\n Aborting the retrieveal of the Schema Definition\")\n\t            else:\n\t                self.schema = self.schemaAPI.getSchema(schema,full=False)\n\t                self.__setAttributes__(self.schema)\n", "                allOf = self.schema.get(\"allOf\",[])\n\t                self.fieldGroupIds = [obj.get('$ref','') for obj in allOf if ('/mixins/' in obj.get('$ref','') or '/experience/' in obj.get('$ref','') or '/context/' in obj.get('$ref','')) and obj.get('$ref','') != self.classId]\n\t                if self.schemaAPI is None:\n\t                    Warning(\"fgManager is set to True but no schema instance has been passed.\\n Aborting the creation of field Group Manager\")\n\t                else:\n\t                    for ref in self.fieldGroupIds:\n\t                        if '/mixins/' in ref:\n\t                            definition = self.schemaAPI.getFieldGroup(ref,full=False)\n\t                        elif ref == '':\n\t                            pass\n", "                        else:\n\t                            ## if the fieldGroup is an OOTB one\n\t                            definition = self.schemaAPI.getFieldGroup(ref,full=True)\n\t                            definition['definitions'] = definition['properties']\n\t                        self.fieldGroupsManagers.append(FieldGroupManager(fieldGroup=definition,schemaAPI=self.schemaAPI))\n\t        elif schema is None:\n\t            self.schema = {\n\t                    \"title\": None,\n\t                    \"description\": \"power by aepp\",\n\t                    \"allOf\": [\n", "                            {\n\t                            \"$ref\": \"https://ns.adobe.com/xdm/context/profile\"\n\t                            }\n\t                        ]\n\t                    }\n\t        if schemaClass is not None:\n\t            self.schema['allOf'][0]['$ref'] = schemaClass\n\t        if fieldGroups is not None and type(fieldGroups) == list:\n\t            if fieldGroups[0] == str:\n\t                for fgId in fieldGroups:\n", "                    self.fieldGroupIds.append(fgId)\n\t                    if self.schemaAPI is None:\n\t                        Warning(\"fgManager is set to True but no schema instance has been passed.\\n Aborting the creation of field Group Manager\")\n\t                    else:\n\t                        definition = self.schemaAPI.getFieldGroup(ref)\n\t                        self.fieldGroupsManagers.append(FieldGroupManager(definition,schemaAPI=self.schemaAPI))\n\t            elif fieldGroups[0] == dict:\n\t                for fg in fieldGroups:\n\t                    self.fieldGroupIds.append(fg.get('$id'))\n\t                    self.fieldGroupsManagers.append(FieldGroupManager(fg,schemaAPI=self.schemaAPI))\n", "        self.fieldGroupTitles= tuple(fg.title for fg in self.fieldGroupsManagers)\n\t        self.fieldGroups = {fg.id:fg.title for fg in self.fieldGroupsManagers}\n\t    def __setAttributes__(self,schemaDef:dict)->None:\n\t        \"\"\"\n\t        Set some basic attributes\n\t        \"\"\"\n\t        if schemaDef.get('title'):\n\t            self.title = schemaDef.get('title')\n\t        if schemaDef.get('$id'):\n\t            self.id = schemaDef.get('$id')\n", "        if schemaDef.get('meta:altId'):\n\t            self.altId = schemaDef.get('meta:altId')\n\t        if schemaDef.get('meta:class'):\n\t            self.classId = schemaDef.get('meta:class')\n\t    def __str__(self)->str:\n\t        return json.dumps(self.schema,indent=2)\n\t    def __repr__(self)->str:\n\t        return json.dumps(self.schema,indent=2)\n\t    def __simpleDeepMerge__(self,base:dict,append:dict)->dict:\n\t        \"\"\"\n", "        Loop through the keys of 2 dictionary and append the new found key of append to the base.\n\t        Arguments:\n\t            base : The base you want to extend\n\t            append : the new dictionary to append\n\t        \"\"\"\n\t        if type(append) == list:\n\t            append = append[0]\n\t        for key in append:\n\t            if type(base)==dict:\n\t                if key in base.keys():\n", "                    self.__simpleDeepMerge__(base[key],append[key])\n\t                else:\n\t                    base[key] = append[key]\n\t            elif type(base)==list:\n\t                base = base[0]\n\t                if type(base) == dict:\n\t                    if key in base.keys():\n\t                        self.__simpleDeepMerge__(base[key],append[key])\n\t                    else:\n\t                        base[key] = append[key]\n", "        return base\n\t    def searchField(self,string:str=None,partialMatch:bool=True,caseSensitive:bool=True)->list:\n\t        \"\"\"\n\t        Search for a field in the different field group.\n\t        You would need to have set fgManager attribute during instantiation or use the convertFieldGroups\n\t        Arguments:\n\t            string : REQUIRED : The string you are looking for\n\t            partialMatch : OPTIONAL : If you want to use partial match (default True)\n\t            caseSensitive : OPTIONAL : If you want to remove the case sensitivity.\n\t        \"\"\"\n", "        myResults = []\n\t        for fgmanager in self.fieldGroupsManagers:\n\t            res = fgmanager.searchField(string,partialMatch,caseSensitive)\n\t            for r in res:\n\t                r['fieldGroup'] = fgmanager.title\n\t            myResults += res\n\t        return myResults\n\t    def searchAttribute(self,attr:dict=None,regex:bool=False,extendedResults:bool=False,joinType:str='outer', **kwargs)->list:\n\t        \"\"\"\n\t        Search for an attribute and its value based on the keyword\n", "        Arguments:\n\t            attr : REQUIRED : a dictionary of key value pair(s).  Example : {\"type\" : \"string\"} \n\t                NOTE : If you wish to have the array type, use the key \"arrayType\". Example : {\"type\" : \"array\",\"arrayType\":\"string\"} \n\t            regex : OPTIONAL : if you want your value of your key to be matched via regex.\n\t                Note that regex will turn every comparison value to string for a \"match\" comparison.\n\t            extendedResults : OPTIONAL : If you want to have the result to contain all details of these fields. (default False)\n\t            joinType : OPTIONAL : If you pass multiple key value pairs, how do you want to get the match.\n\t                outer : provide the fields if any of the key value pair is matched. (default)\n\t                inner : provide the fields if all the key value pair matched.\n\t        \"\"\"\n", "        myResults = []\n\t        for fgmanager in self.fieldGroupsManagers:\n\t            res = fgmanager.searchAttribute(attr=attr,regex=regex,extendedResults=extendedResults,joinType=joinType)\n\t            if extendedResults:\n\t                for r in res:\n\t                    r['fieldGroup'] = fgmanager.title\n\t            myResults += res\n\t        return myResults\n\t    def addFieldGroup(self,fieldGroup:Union[str,dict]=None)->Union[None,'FieldGroupManager']:\n\t        \"\"\"\n", "        Add a field groups to field Group object and the schema. \n\t        return the specific FieldGroup Manager instance.\n\t        Arguments:\n\t            fieldGroup : REQUIRED : The fieldGroup ID or the dictionary definition connecting to the API.\n\t                if a fieldGroup ID is provided, you should have added a schemaAPI previously.\n\t        \"\"\"\n\t        if type(fieldGroup) == dict:\n\t            if fieldGroup.get('$id') not in [fg for fg in self.fieldGroupIds]:\n\t                self.fieldGroupIds.append(fieldGroup['$id'])\n\t                self.schema['allOf'].append({'$ref':fieldGroup['$id'],\"type\": \"object\"})\n", "        elif type(fieldGroup) == str:\n\t            if fieldGroup not in [fg for fg in self.fieldGroupIds]:\n\t                self.fieldGroupIds.append(fieldGroup)\n\t                self.schema['allOf'].append({'$ref':fieldGroup,\"type\": \"object\"})\n\t                if self.schemaAPI is None:\n\t                    raise AttributeError('Missing the schema API attribute. Please use the addSchemaAPI method to add it.')\n\t                else:\n\t                    fieldGroup = self.schemaAPI.getFieldGroup(fieldGroup)\n\t        fbManager = FieldGroupManager(fieldGroup=fieldGroup)\n\t        self.fieldGroupsManagers.append(fbManager)\n", "        self.fieldGroupTitles = tuple(fgm.title for fgm in self.fieldGroupsManagers)\n\t        self.fieldGroups = {fgm.id:fgm.title for fgm in self.fieldGroupsManagers}\n\t        return fbManager\n\t    def getFieldGroupManager(self,fieldgroup:str=None)->'FieldGroupManager':\n\t        \"\"\"\n\t        Return a field group Manager of a specific name.\n\t        Only possible if fgManager was set to True during instanciation.\n\t        Argument:\n\t            fieldgroup : REQUIRED : The title or the $id of the field group to retrieve.\n\t        \"\"\"\n", "        if self.getFieldGroupManager is not None:\n\t            if \"ns.adobe.com\" in fieldgroup: ## id\n\t                return [fg for fg in self.fieldGroupsManagers if fg.id == fieldgroup][0]\n\t            else:\n\t                return [fg for fg in self.fieldGroupsManagers if fg.title == fieldgroup][0]\n\t        else:\n\t            raise Exception(\"The field group manager was not set to True during instanciation. No Field Group Manager to return\")\n\t    def setTitle(self,name:str=None)->None:\n\t        \"\"\"\n\t        Set a name for the schema.\n", "        Arguments:\n\t            name : REQUIRED : a string to be used for the title of the FieldGroup\n\t        \"\"\"\n\t        self.schema['title'] = name\n\t        self.title = name\n\t        return None\n\t    def to_dataframe(self,save:bool=False,queryPath: bool = False,description:bool = False)->pd.DataFrame:\n\t        \"\"\"\n\t        Extract the information from the Field Group to DataFrame. You need to have instanciated the Field Group manager.\n\t        Arguments:\n", "            save : OPTIONAL : If you wish to save it with the title used by the field group.\n\t                save as csv with the title used. Not title, used \"unknown_schema_\" + timestamp.\n\t            queryPath : OPTIONAL : If you want to have the query path to be used.\n\t        \"\"\"\n\t        df = pd.DataFrame({'path':[],'type':[],'fieldGroup':[]})\n\t        for fgmanager in self.fieldGroupsManagers:\n\t            tmp_df = fgmanager.to_dataframe(queryPath=queryPath,description=description)\n\t            tmp_df['fieldGroup'] = fgmanager.title\n\t            df = pd.concat([df,tmp_df],ignore_index=True)\n\t        if save:\n", "            title = self.schema.get('title',f'unknown_schema_{str(int(time.time()))}.csv')\n\t            df.to_csv(f\"{title}.csv\",index=False)\n\t        df = df[~df.duplicated('path')].reset_index(drop=True)\n\t        return df\n\t    def to_dict(self)->dict:\n\t        \"\"\"\n\t        Return a dictionary of the whole schema. You need to have instanciated the Field Group Manager\n\t        \"\"\"\n\t        list_dict = [fbm.to_dict() for fbm in self.fieldGroupsManagers]\n\t        result = {}\n", "        for mydict in list_dict:\n\t            result = self.__simpleDeepMerge__(result,mydict)\n\t        return result\n\t    def createSchema(self)->dict:\n\t        \"\"\"\n\t        Send a createSchema request to AEP to create the schema.\n\t        It removes the \"$id\" if one was provided to avoid overriding existing ID.\n\t        \"\"\"\n\t        if self.schemaAPI is None:\n\t            raise Exception(\"Require a Schema instance to connect to the API\")\n", "        res = self.schemaAPI.createSchema(self.schema)\n\t        self.schema = res\n\t        self.__setAttributes__(self.schema)\n\t        return res\n\t    def updateSchema(self)->dict:\n\t        \"\"\"\n\t        Use the PUT method to replace the existing schema with the new definition.\n\t        \"\"\"\n\t        if self.schemaAPI is None:\n\t            raise Exception(\"Require a Schema instance to connect to the API\")\n", "        res = self.schemaAPI.putSchema(self.id,self.schema)\n\t        if 'status' in res.keys():\n\t            if res['status'] == 400:\n\t                print(res['title'])\n\t                return res\n\t            else:\n\t                return res\n\t        self.schema = res\n\t        self.__setAttributes__(self.schema)\n\t        return res\n", "    def createDescriptorOperation(self,descType:str=None,\n\t                                completePath:str=None,\n\t                                identityNSCode:str=None,\n\t                                identityPrimary:bool=False,\n\t                                alternateTitle:str=None,\n\t                                alternateDescription:str=None,\n\t                                lookupSchema:str=None,\n\t                                targetCompletePath:str=None,\n\t                                )->dict:\n\t        \"\"\"\n", "        Create a descriptor object to be used in the createDescriptor.\n\t        You can see the type of descriptor available in the DESCRIPTOR_TYPES attribute and also on the official documentation:\n\t        https://experienceleague.adobe.com/docs/experience-platform/xdm/api/descriptors.html?lang=en#appendix\n\t        Arguments:\n\t            descType : REQUIRED : The type to be used.\n\t                it can only be one of the following value: \"xdm:descriptorIdentity\",\"xdm:alternateDisplayInfo\",\"xdm:descriptorOneToOne\",\"xdm:descriptorReferenceIdentity\",\"xdm:descriptorDeprecated\"\n\t            completePath : REQUIRED : the complete path of the field you want to attach a descriptor.\n\t                Example: '/definitions/customFields/properties/_tenant/properties/tenantObject/properties/field'\n\t            identityNSCode : OPTIONAL : if the descriptor is identity related, the namespace CODE  used.\n\t            identityPrimary : OPTIONAL : If the primary descriptor added is the primary identity.\n", "            alternateTitle : OPTIONAL : if the descriptor is alternateDisplay, the alternate title to be used.\n\t            alternateDescription : OPTIONAL if you wish to add a new description.\n\t            lookupSchema : OPTIONAL : The schema ID for the lookup if the descriptor is for lookup setup\n\t            targetCompletePath : OPTIONAL : if you have the complete path for the field in the target lookup schema.\n\t        \"\"\"\n\t        if descType not in self.DESCRIPTOR_TYPES:\n\t            raise Exception(f\"The value provided ({descType}) is not supported by this method\")\n\t        if completePath is None:\n\t            raise ValueError(\"Require a field complete path\")\n\t        if descType == \"xdm:descriptorIdentity\":\n", "            obj = {\n\t                \"@type\": descType,\n\t                \"xdm:sourceSchema\": self.id,\n\t                \"xdm:sourceVersion\": 1,\n\t                \"xdm:sourceProperty\": completePath,\n\t                \"xdm:namespace\": identityNSCode,\n\t                \"xdm:property\": \"xdm:code\",\n\t                \"xdm:isPrimary\": identityPrimary\n\t            }\n\t        elif descType == \"xdm:alternateDisplayInfo\":\n", "            if alternateTitle is None:\n\t                raise ValueError(\"Require an alternate title\")\n\t            obj = {\n\t                \"@type\": descType,\n\t                \"xdm:sourceSchema\": self.id,\n\t                \"xdm:sourceVersion\": 1,\n\t                \"xdm:sourceProperty\": completePath,\n\t                \"xdm:title\": {\n\t                    \"en_us\": alternateTitle\n\t                    }\n", "                }\n\t            if alternateDescription is not None:\n\t                obj[\"xdm:description\"] = {\n\t                    \"en_us\":alternateDescription\n\t                }\n\t        elif descType == \"xdm:descriptorOneToOne\":\n\t            obj = {\n\t                \"@type\": descType,\n\t                \"xdm:sourceSchema\":self.id,\n\t                \"xdm:sourceVersion\": 1,\n", "                \"xdm:sourceProperty\":completePath,\n\t                \"xdm:destinationSchema\":lookupSchema,\n\t                \"xdm:destinationVersion\": 1,\n\t            }\n\t            if targetCompletePath is not None:\n\t                obj[\"xdm:destinationProperty\"] = targetCompletePath\n\t        elif descType == \"xdm:descriptorReferenceIdentity\":\n\t            obj = {\n\t                \"@type\": descType,\n\t                \"xdm:sourceSchema\": self.id,\n", "                \"xdm:sourceVersion\": 1,\n\t                \"xdm:sourceProperty\": completePath,\n\t                \"xdm:identityNamespace\": identityNSCode\n\t                }\n\t        elif descType == \"xdm:descriptorDeprecated\":\n\t            obj = {\n\t                \"@type\": descType,\n\t                \"xdm:sourceSchema\": self.id,\n\t                \"xdm:sourceVersion\": 1,\n\t                \"xdm:sourceProperty\": completePath\n", "            }\n\t        return obj\n\t    def createDescriptor(self,descriptor:dict=None)->dict:\n\t        \"\"\"\n\t        Create a descriptor attached to that class bsaed on the creatorDescriptor operation provided. \n\t        Arguments:\n\t            descriptor : REQUIRED : The operation to add a descriptor to the schema.\n\t        \"\"\"\n\t        if descriptor is None:\n\t            raise ValueError('Require an operation to be used')\n", "        res = self.schemaAPI.createDescriptor(descriptor)\n\t        return res\n\t    def compareObservableSchema(self,observableSchemaManager:'ObservableSchemaManager'=None)->pd.DataFrame:\n\t        \"\"\"\n\t        A method to compare the existing schema with the observable schema and find out the difference in them.\n\t        It output a dataframe with all of the path, the field group, the type (if provided) and the part availability (in that dataset)\n\t        Arguments:\n\t            observableSchemaManager : REQUIRED : the ObservableSchemaManager class instance.\n\t        \"\"\"\n\t        df_schema = self.to_dataframe()\n", "        df_obs = observableSchemaManager.to_dataframe()\n\t        df_merge = df_schema.merge(df_obs,left_on='path',right_on='path',how='left',indicator=True)\n\t        df_merge = df_merge.rename(columns={\"_merge\": \"availability\",'type_x':'type'})\n\t        df_merge = df_merge.drop(\"type_y\",axis=1)\n\t        df_merge['availability'] = df_merge['availability'].str.replace('left_only','schema_only')\n\t        df_merge['availability'] = df_merge['availability'].str.replace('both','schema_dataset')\n\t        return df_merge"]}
{"filename": "aepp/identity.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\tfrom copy import deepcopy\n\timport logging\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass Identity:\n\t    \"\"\"\n\t    Class to manage and retrieve Identity information.\n\t    #!acpdr/swagger-specs/id-service-api.yaml\n\t    This is based on the following API reference : https://www.adobe.io/apis/experienceplatform/home/api-reference.html\n", "    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        region: str = \"nld2\",\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n", "        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Require a region.\n\t        By default, the NLD2 will be selected. (other choice : va7)\n\t        Additional kwargs will update the header.\n\t        more info : https://docs.adobe.com/content/help/en/experience-platform/identity/api/getting-started.html\n\t        Arguments:\n\t            region : REQUIRED : either nld2 or va7\n\t            loggingObject : OPTIONAL : logging object to log messages.\n", "            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n", "                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n", "                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n", "            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n", "            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            f\"https://platform-{region}.adobe.io\" + aepp.config.endpoints[\"identity\"]\n\t        )\n\t    def getIdentity(\n\t        self, id_str: str = None, nsid: str = None, namespace: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Given the namespace and an ID in that namespace, returns XID string.\n\t        Arguments:\n", "            id_str : REQUIRED : Id in given namespace (ECID value)\n\t            nsid : REQUIRED : namespace id. (e.g. 411)\n\t            namespace : OPTIONAL : namespace code (e.g. adcloud)\n\t        \"\"\"\n\t        if id_str is None or (namespace is None and nsid is None):\n\t            raise Exception(\n\t                \"Expecting that id_str and (namespace or nsid) arguments to be filled.\"\n\t            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getIdentity\")\n", "        params = {\"id\": id_str}\n\t        if nsid is not None:\n\t            params[\"nsid\"] = nsid\n\t        if namespace is not None:\n\t            params[\"namespace\"] = namespace\n\t        path = \"/identity/identity\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = \"application/json\"\n\t        privateHeader[\"x-uis-cst-ctx\"] = \"stub\"\n\t        res = self.connector.getData(\n", "            self.endpoint + path, headers=privateHeader, params=params\n\t        )\n\t        return res\n\t    def getIdentities(self, only_custom: bool = False, save: bool = False) -> list:\n\t        \"\"\"\n\t        Get the list of all identity namespaces available in the organization.\n\t        Arguments:\n\t            only_custom : OPTIONAL : if set to True, return only customer made identities (default False)\n\t            save : OPTIONAL : if set to True, save the result in its respective folder (default False)\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getIdentities\")\n\t        path = \"/idnamespace/identities\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        if only_custom:\n\t            res = [identity for identity in res if identity[\"custom\"] == True]\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"identity\", file=res, filename=\"identities\", type_file=\"json\"\n\t            )\n", "        return res\n\t    def getIdentityDetail(self, id_str: str = None, save: bool = False) -> dict:\n\t        \"\"\"\n\t        List details of a specific identity namespace by its ID.\n\t        Arguments:\n\t            id_str : REQUIRED : identity of the \"id\" field.\n\t            save : OPTIONAL : if set to True, save the result in a file, in its respective folder (default False)\n\t        \"\"\"\n\t        if id_str is None:\n\t            raise Exception(\"Expected an id for the Identity\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getIdentityDetail\")\n\t        path = f\"/idnamespace/identities/{id_str}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        if save:\n\t            filename = f\"identity_{res['code']}\"\n\t            aepp.saveFile(\n\t                module=\"identity\", file=res, filename=filename, type_file=\"json\"\n\t            )\n\t        return res\n", "    def createIdentity(\n\t        self,\n\t        name: str = None,\n\t        code: str = None,\n\t        idType: str = None,\n\t        description: str = None,\n\t        dict_identity: dict = None,\n\t    ) -> dict:\n\t        \"\"\"\n\t        List details of a specific identity namespace by its ID.\n", "        Arguments:\n\t            name : REQUIRED : Display name of the identity\n\t            code : REQUIRED : Identity Symbol for user interface.\n\t            idType : REQUIRED : one of those : COOKIE, CROSS_DEVICE, DEVICE, EMAIL, MOBILE, NON_PEOPLE or PHONE.\n\t            description : OPTIONAL : description for this identity\n\t            dict_identity : OPTIONAL : you can use this to directly pass the dictionary.\n\t        \"\"\"\n\t        creation_dict = {}\n\t        if name is None or code is None or idType is None:\n\t            raise Exception(\n", "                \"Expecting that name, code and idType to be filled with value\"\n\t            )\n\t        creation_dict[\"name\"] = name\n\t        creation_dict[\"code\"] = code\n\t        creation_dict[\"idType\"] = idType\n\t        if description is not None:\n\t            creation_dict[\"description\"] = description\n\t        if \" \" in code:\n\t            raise TypeError(\"code can only contain one word with letter and numbers\")\n\t        if idType not in [\n", "            \"COOKIE\",\n\t            \"CROSS_DEVICE\",\n\t            \"DEVICE\",\n\t            \"EMAIL\",\n\t            \"MOBILE\",\n\t            \"NON_PEOPLE\",\n\t            \"PHONE\",\n\t        ]:\n\t            raise TypeError(\n\t                \"idType could only be one of those : COOKIE, CROSS_DEVICE, DEVICE, EMAIL, MOBILE, NON_PEOPLE, PHONE\"\n", "            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createIdentity\")\n\t        if dict_identity is not None:\n\t            creation_dict = dict_identity\n\t        path = \"/idnamespace/identities\"\n\t        res = self.connector.postData(\n\t            self.endpoint + path, headers=self.header, data=creation_dict\n\t        )\n\t        return res\n", "    def updateIdentity(\n\t        self,\n\t        id_str: str = None,\n\t        name: str = None,\n\t        code: str = None,\n\t        idType: str = None,\n\t        description: str = None,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Update identity based on its ID.\n", "        Arguments:\n\t            id_str: REQUIRED : ID of the identity namespace to update.\n\t            name : REQUIRED : Display name of the identity\n\t            code : REQUIRED : Identity Symbol for user interface.\n\t            idType : REQUIRED : one of those : COOKIE, CROSS_DEVICE, DEVICE, EMAIL, MOBILE, NON_PEOPLE or PHONE.\n\t            description : OPTIONAL : description for this identity\n\t        \"\"\"\n\t        if id_str is None:\n\t            raise Exception(\"Require an id\")\n\t        if name is None or code is None or idType is None:\n", "            raise Exception(\n\t                \"Expecting that name, code and idType to be filled with value\"\n\t            )\n\t        if idType not in [\n\t            \"COOKIE\",\n\t            \"CROSS_DEVICE\",\n\t            \"DEVICE\",\n\t            \"EMAIL\",\n\t            \"MOBILE\",\n\t            \"NON_PEOPLE\",\n", "            \"PHONE\",\n\t        ]:\n\t            raise TypeError(\n\t                \"idType could only be one of those : COOKIE, CROSS_DEVICE, DEVICE, EMAIL, MOBILE, NON_PEOPLE, PHONE\"\n\t            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateIdentity\")\n\t        path = f\"/idnamespace/identities/{id_str}\"\n\t        data = {\n\t            \"name\": name,\n", "            \"code\": code,\n\t            \"idType\": idType,\n\t            \"description\": description,\n\t        }\n\t        res = self.connector.putData(\n\t            self.endpoint + path, headers=self.header, data=data\n\t        )\n\t        return res\n\t    def getIdentitiesIMS(self, imsOrg: str = None) -> list:\n\t        \"\"\"\n", "        Returns all identities from the IMS Org itself.\n\t        Only shared ones if IMS Org doesn't match the IMS Org sent in the header.\n\t        Arguments:\n\t            imsOrg : OPTIONAL : the IMS org. If not set, takes the current one automatically.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getIdentitiesIMS\")\n\t        ims_org = imsOrg or self.connector.config[\"org_id\"]\n\t        path = f\"/idnamespace/orgs/{ims_org}/identities\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n", "        return res\n\t    def getClustersMembers(\n\t        self,\n\t        xid: str = None,\n\t        nsid: str = \"411\",\n\t        namespace: str = \"adcloud\",\n\t        id_value: str = None,\n\t        graphType: str = \"private\",\n\t    ) -> dict:\n\t        \"\"\"\n", "        Given an XID return all XIDs, in the same or other namespaces, that are linked to it by the device graph type.\n\t        The related XIDs are considered to be part of the same cluster.\n\t        It is required to pass either xid or (namespace/nsid & id) pair to get cluster members.\n\t        Arguments:\n\t            xid : REQUIRED : Identity string returns by the getIdentity method.\n\t            nsid : OPTIONAL : namespace id (default : 411)\n\t            namespace : OPTIONAL : namespace code. (default : adcloud)\n\t            id_value : OPTIONAL : ID of the customer in given namespace.\n\t            graphType : OPTIONAL : Graph type (output type) you want to get the cluster from. (default private)\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getClustersMembers\")\n\t        temp_header = deepcopy(self.header)\n\t        temp_header[\"Accept\"] = \"application/json\"\n\t        temp_header[\"x-uis-cst-ctx\"] = \"stub\"\n\t        path = \"/identity/cluster/members\"\n\t        params = {}\n\t        if xid is not None:\n\t            params[\"xid\"] = xid\n\t            params[\"graph-type\"] = graphType\n", "            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=temp_header\n\t            )\n\t            return res\n\t        elif xid is None and id_value is not None:\n\t            params[\"nsid\"] = nsid\n\t            params[\"namespace\"] = namespace\n\t            params[\"id\"] = id_value\n\t            params[\"graph-type\"] = graphType\n\t            res = self.connector.getData(\n", "                self.endpoint + path, params=params, headers=temp_header\n\t            )\n\t            return res\n\t    def postClustersMembers(\n\t        self, xids: list = None, version: float = 1.0, graphType: str = \"private\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Given set of identities, returns all linked identities in cluster corresponding to each identity.\n\t        Arguments:\n\t            xids : REQUIRED : list of identity as returned by getIdentity method.\n", "            version : OPTIONAL : Version of the clusterMembers (default 1.0)\n\t            graphType : OPTIONAL : Graph type (output type) you want to get the cluster from. (default private)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting postClustersMembers\")\n\t        temp_header = deepcopy(self.header)\n\t        temp_header[\"Accept\"] = \"application/vnd.adobe.identity+json;version=1.2\"\n\t        temp_header[\"x-uis-cst-ctx\"] = \"stub\"\n\t        path = \"/identity/cluster/members\"\n\t        if type(xids) != list:\n", "            raise TypeError(\"xids must be of type list\")\n\t        list_body = [\n\t            {\"xid\": [{\"xid\": xid}], \"graph-type\": graphType, \"version\": version}\n\t            for xid in xids\n\t        ]\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=list_body, headers=temp_header\n\t        )\n\t        return res\n\t    def getClusterHistory(\n", "        self,\n\t        xid: str = None,\n\t        nsid: int = 411,\n\t        namespace: str = \"adcloud\",\n\t        id_value: str = None,\n\t        graphType: str = \"private\",\n\t    ) -> dict:\n\t        \"\"\"\n\t        Given an XID, return all cluster associations with that XID.\n\t        It is required to pass either xid or (namespace/nsid & id) pair to get cluster history.\n", "        Arguments:\n\t            xid : REQUIRED : Identity string returns by the getIdentity method.\n\t            nsid : OPTIONAL : namespace id (default : 411)\n\t            namespace : OPTIONAL : namespace code. (default : adcloud)\n\t            id_value : OPTIONAL : ID of the customer in given namespace.\n\t            graphType : OPTIONAL : Graph type (output type) you want to get the cluster from. (default private)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getClusterHistory\")\n\t        temp_header = deepcopy(self.header)\n", "        temp_header[\"Accept\"] = \"application/vnd.adobe.identity+json;version=1.2\"\n\t        temp_header[\"x-uis-cst-ctx\"] = \"stub\"\n\t        path = \"/identity/cluster/history\"\n\t        params = {}\n\t        if xid is not None:\n\t            params[\"xid\"] = xid\n\t            params[\"graph-type\"] = graphType\n\t            res = aepp._getData(\n\t                self.endpoint + path, params=params, headers=temp_header\n\t            )\n", "            return res\n\t        elif xid is None and id_value is not None:\n\t            params[\"nsid\"] = nsid\n\t            params[\"namespace\"] = namespace\n\t            params[\"id\"] = id_value\n\t            params[\"graph-type\"] = graphType\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=temp_header\n\t            )\n\t            return res\n", "    def getIdentityMapping(\n\t        self,\n\t        xid: str = None,\n\t        targetNs: int = None,\n\t        nsid: int = 411,\n\t        namespace: str = \"adcloud\",\n\t        id_value: str = None,\n\t        graphType: str = \"private\",\n\t    ) -> dict:\n\t        \"\"\"\n", "        Given an XID, returns all XID mappings in the requested namespace (targetNs).\n\t        It is required to pass either xid or (namespace/nsid & id) pair to get mappings in required namespace.\n\t        Arguments:\n\t            xid : REQUIRED : Identity string returns by the getIdentity method.\n\t            nsid : OPTIONAL : namespace id (default : 411)\n\t            namespace : OPTIONAL : namespace code. (default : adcloud)\n\t            id_value : OPTIONAL : ID of the customer in given namespace.\n\t            graphType : OPTIONAL : Graph type (output type) you want to get the cluster from. (default private)\n\t            targetNs : OPTIONAL : The namespace you want to get the mappings from.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getIdentityMapping\")\n\t        temp_header = deepcopy(self.header)\n\t        temp_header[\"Accept\"] = \"application/vnd.adobe.identity+json;version=1.2\"\n\t        temp_header[\"x-uis-cst-ctx\"] = \"stub\"\n\t        path = \"/identity/mapping\"\n\t        params = {}\n\t        if xid is not None:\n\t            params[\"xid\"] = xid\n\t            params[\"graph-type\"] = graphType\n", "            params[\"targetNs\"] = targetNs\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=temp_header\n\t            )\n\t            return res\n\t        elif xid is None and id_value is not None:\n\t            params[\"nsid\"] = nsid\n\t            params[\"namespace\"] = namespace\n\t            params[\"id\"] = id_value\n\t            params[\"targetNs\"] = targetNs\n", "            params[\"graph-type\"] = graphType\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=temp_header\n\t            )\n\t            return res\n\t    def postIdentityMapping(\n\t        self, xids: list = None, targetNs: int = 411, version: float = 1.0\n\t    ) -> dict:\n\t        \"\"\"\n\t        Given an identity, returns all identity mappings in requested namespace (target namespace).\n", "        Arguments:\n\t            xids : REQUIRED : List of identities\n\t            targetNs : REQUIRED : Target Namespace (default 411)\n\t            version : OPTIONAL : version of the mapping\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting postIdentityMapping\")\n\t        temp_header = deepcopy(self.header)\n\t        temp_header[\"Accept\"] = \"application/vnd.adobe.identity+json;version=1.2\"\n\t        temp_header[\"x-uis-cst-ctx\"] = \"stub\"\n", "        path = \"/identity/mapping\"\n\t        if type(xids) != list:\n\t            raise TypeError(\"xids must be of type list\")\n\t        list_body = [\n\t            {\"xid\": [{\"xid\": xid}], \"version\": version, \"targetNs\": targetNs}\n\t            for xid in xids\n\t        ]\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=list_body, headers=temp_header\n\t        )\n", "        return res\n"]}
{"filename": "aepp/configs.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport json\n", "import os\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\timport json\n\t# Non standard libraries\n\tfrom .config import config_object, header, endpoints\n\tfrom aepp import connector\n\tdef find_path(path: str) -> Optional[Path]:\n\t    \"\"\"Checks if the file denoted by the specified `path` exists and returns the Path object\n\t    for the file.\n", "    If the file under the `path` does not exist and the path denotes an absolute path, tries\n\t    to find the file by converting the absolute path to a relative path.\n\t    If the file does not exist with either the absolute and the relative path, returns `None`.\n\t    \"\"\"\n\t    if Path(path).exists():\n\t        return Path(path)\n\t    elif path.startswith(\"/\") and Path(\".\" + path).exists():\n\t        return Path(\".\" + path)\n\t    elif path.startswith(\"\\\\\") and Path(\".\" + path).exists():\n\t        return Path(\".\" + path)\n", "    else:\n\t        return None\n\tdef createConfigFile(\n\t    destination: str = \"config_aep_template.json\",\n\t    sandbox: str = \"prod\",\n\t    environment: str = \"prod\",\n\t    verbose: object = False,\n\t    auth_type: str = \"oauthV2\",\n\t    **kwargs,\n\t) -> None:\n", "    \"\"\"\n\t    This function will create a 'config_admin.json' file where you can store your access data.\n\t    Arguments:\n\t        destination : OPTIONAL : if you wish to save the file at a specific location.\n\t        sandbox : OPTIONAL : You can directly set your sandbox name in this parameter.\n\t        verbose : OPTIONAL : set to true, gives you a print stateent where is the location.\n\t        auth_type : OPTIONAL : type of authentication, either \"jwt\" or \"oauthV2\" or \"oauthV1\". Default is oauthV2\n\t    \"\"\"\n\t    json_data: dict = {\n\t        \"org_id\": \"<orgID>\",\n", "        \"client_id\": \"<client_id>\",\n\t        \"secret\": \"<YourSecret>\",\n\t        \"sandbox-name\": sandbox,\n\t        \"environment\": environment\n\t    }\n\t    if auth_type == \"jwt\":\n\t        json_data[\"tech_id\"] = \"<something>@techacct.adobe.com\"\n\t        json_data[\"pathToKey\"] = \"<path/to/your/privatekey.key>\"\n\t    elif auth_type == \"oauthV2\":\n\t        json_data[\"scopes\"] = \"<scopes>\"\n", "    elif auth_type == \"oauthV1\":\n\t        json_data[\"auth_code\"] = \"<auth_code>\"\n\t    else:\n\t        raise ValueError(\"unsupported authentication type, currently only jwt, oauthV1 and oauthV2 are supported\")\n\t    if \".json\" not in destination:\n\t        destination: str = f\"{destination}.json\"\n\t    with open(destination, \"w\") as cf:\n\t        cf.write(json.dumps(json_data, indent=4))\n\t    if verbose:\n\t        print(\n", "            f\" file created at this location : {os.getcwd()}{os.sep}{destination}.json\"\n\t        )\n\tdef importConfigFile(\n\t    path: str = None,\n\t    connectInstance: bool = False,\n\t    auth_type: str = None,\n\t    sandbox:str = None,\n\t):\n\t    \"\"\"Reads the file denoted by the supplied `path` and retrieves the configuration information\n\t    from it.\n", "    Arguments:\n\t        path: REQUIRED : path to the configuration file. Can be either a fully-qualified or relative.\n\t        connectInstance : OPTIONAL : If you want to return an instance of the ConnectObject class\n\t        auth_type : OPTIONAL : type of authentication, either \"jwt\" or \"oauthV1\" or \"oauthV2\". Detected based on keys present in config file.\n\t        sandbox : OPTIONAL : The sandbox to connect it.\n\t    Example of path value.\n\t    \"config.json\"\n\t    \"./config.json\"\n\t    \"/my-folder/config.json\"\n\t    \"\"\"\n", "    if path is None:\n\t        raise ValueError(\"require a path to a configuration file to be provided\")\n\t    config_file_path: Optional[Path] = find_path(path)\n\t    if config_file_path is None:\n\t        raise FileNotFoundError(\n\t            f\"Unable to find the configuration file under path `{path}`.\"\n\t        )\n\t    with open(config_file_path, \"r\") as file:\n\t        provided_config = json.load(file)\n\t        provided_keys = list(provided_config.keys())\n", "        if \"api_key\" in provided_keys:\n\t            ## old naming for client_id\n\t            client_id = provided_config[\"api_key\"]\n\t        elif \"client_id\" in provided_keys:\n\t            client_id = provided_config[\"client_id\"]\n\t        else:\n\t            raise RuntimeError(\n\t                f\"Either an `api_key` or a `client_id` should be provided.\"\n\t            )\n\t        if auth_type is None:\n", "            if 'scopes' in provided_keys:\n\t                auth_type = 'oauthV2'\n\t            elif 'tech_id' in provided_keys and \"pathToKey\" in provided_keys:\n\t                auth_type = 'jwt'\n\t            elif 'auth_code' in provided_keys:\n\t                auth_type = 'oauthV1'\n\t        args = {\n\t            \"org_id\": provided_config[\"org_id\"],\n\t            \"client_id\": client_id,\n\t            \"secret\": provided_config[\"secret\"],\n", "            \"sandbox\": provided_config.get(\"sandbox-name\", \"prod\"),\n\t            \"environment\": provided_config.get(\"environment\", \"prod\"),\n\t            \"connectInstance\": connectInstance\n\t        }\n\t        if sandbox is not None: ## overriding sandbox from parameter\n\t            args[\"sandbox\"] = sandbox\n\t        if auth_type == \"jwt\":\n\t            args[\"tech_id\"] = provided_config[\"tech_id\"]\n\t            args[\"path_to_key\"] = provided_config[\"pathToKey\"]\n\t        elif auth_type == \"oauthV2\":\n", "            args[\"scopes\"] = provided_config[\"scopes\"].replace(' ','')\n\t        elif auth_type == \"oauthV1\":\n\t            args[\"auth_code\"] = provided_config[\"auth_code\"]\n\t        else:\n\t            raise ValueError(\"unsupported authentication type, currently only jwt and oauth are supported\")\n\t        myInstance = configure(**args)\n\t    if connectInstance:\n\t        return myInstance\n\tdef configure(\n\t    org_id: str = None,\n", "    tech_id: str = None,\n\t    secret: str = None,\n\t    client_id: str = None,\n\t    path_to_key: str = None,\n\t    private_key: str = None,\n\t    sandbox: str = \"prod\",\n\t    connectInstance: bool = False,\n\t    environment: str = \"prod\",\n\t    scopes: str = None,\n\t    auth_code:str=None\n", "):\n\t    \"\"\"Performs programmatic configuration of the API using provided values.\n\t    Arguments:\n\t        org_id : REQUIRED : Organization ID\n\t        tech_id : OPTIONAL : Technical Account ID\n\t        secret : REQUIRED : secret generated for your connection\n\t        client_id : REQUIRED : The client_id (old api_key) provided by the JWT connection.\n\t        path_to_key : REQUIRED : If you have a file containing your private key value.\n\t        private_key : REQUIRED : If you do not use a file but pass a variable directly.\n\t        sandbox : OPTIONAL : If not provided, default to prod\n", "        connectInstance : OPTIONAL : If you want to return an instance of the ConnectObject class\n\t        environment : OPTIONAL : If not provided, default to prod\n\t        scopes : OPTIONAL : The scope define in your project for your API connection. Oauth V2, for clients and customers.\n\t        auth_code : OPTIONAL : If an authorization code is used directly instead of generating via JWT. Oauth V1 only, for adobe internal services.\n\t    \"\"\"\n\t    if not org_id:\n\t        raise ValueError(\"`org_id` must be specified in the configuration.\")\n\t    if not client_id:\n\t        raise ValueError(\"`client_id` must be specified in the configuration.\")\n\t    if not secret:\n", "        raise ValueError(\"`secret` must be specified in the configuration.\")\n\t    if (scopes is not None and (path_to_key is not None or private_key is not None) and auth_code is not None) \\\n\t            or (scopes is None and path_to_key is None and private_key is None and auth_code is None):\n\t        raise ValueError(\"either `scopes` needs to be specified or one of `private_key` or `path_to_key` or an `auth_code`\")\n\t    config_object[\"org_id\"] = org_id\n\t    header[\"x-gw-ims-org-id\"] = org_id\n\t    config_object[\"client_id\"] = client_id\n\t    header[\"x-api-key\"] = client_id\n\t    config_object[\"tech_id\"] = tech_id\n\t    config_object[\"secret\"] = secret\n", "    config_object[\"pathToKey\"] = path_to_key\n\t    config_object[\"private_key\"] = private_key\n\t    config_object[\"scopes\"] = scopes\n\t    config_object[\"auth_code\"] = auth_code\n\t    config_object[\"sandbox\"] = sandbox\n\t    header[\"x-sandbox-name\"] = sandbox\n\t    # ensure we refer to the right environment endpoints\n\t    config_object[\"environment\"] = environment\n\t    if environment == \"prod\":\n\t        endpoints[\"global\"] = \"https://platform.adobe.io\"\n", "        config_object[\"imsEndpoint\"] = \"https://ims-na1.adobelogin.com\"\n\t    else:\n\t        endpoints[\"global\"] = f\"https://platform-{environment}.adobe.io\"\n\t        config_object[\"imsEndpoint\"] = \"https://ims-na1-stg1.adobelogin.com\"\n\t    endpoints[\"streaming\"][\"inlet\"] = f\"{endpoints['global']}/data/core/edge\"\n\t    config_object[\"jwtTokenEndpoint\"] = f\"{config_object['imsEndpoint']}/ims/exchange/jwt\"\n\t    config_object[\"oauthTokenEndpointV1\"] = f\"{config_object['imsEndpoint']}/ims/token/v1\"\n\t    config_object[\"oauthTokenEndpointV2\"] = f\"{config_object['imsEndpoint']}/ims/token/v2\"\n\t    # ensure the reset of the state by overwriting possible values from previous import.\n\t    config_object[\"date_limit\"] = 0\n", "    config_object[\"token\"] = \"\"\n\t    if connectInstance:\n\t        myInstance = ConnectObject(\n\t            org_id=org_id,\n\t            tech_id=tech_id,\n\t            secret=secret,\n\t            client_id=client_id,\n\t            path_to_key = path_to_key,\n\t            private_key = private_key,\n\t            sandbox=sandbox,\n", "            scopes=scopes,\n\t            auth_code=auth_code\n\t        )\n\t        return myInstance\n\tdef get_private_key_from_config(config: dict) -> str:\n\t    \"\"\"\n\t    Returns the private key directly or read a file to return the private key.\n\t    \"\"\"\n\t    private_key = config.get(\"private_key\")\n\t    if private_key is not None:\n", "        return private_key\n\t    private_key_path = find_path(config[\"pathToKey\"])\n\t    if private_key_path is None:\n\t        raise FileNotFoundError(\n\t            f'Unable to find the private key under path `{config[\"pathToKey\"]}`.'\n\t        )\n\t    with open(Path(private_key_path), \"r\") as f:\n\t        private_key = f.read()\n\t    return private_key\n\tdef generateLoggingObject(level:str=\"WARNING\",filename:str=\"aepp.log\") -> dict:\n", "    \"\"\"\n\t    Generates a dictionary for the logging object with basic configuration.\n\t    You can find the information for the different possible values on the logging documentation.\n\t        https://docs.python.org/3/library/logging.html\n\t    Arguments:\n\t        level : OPTIONAL : Level of the logger to display information (NOTSET, DEBUG,INFO,WARNING,EROR,CRITICAL)\n\t            default WARNING\n\t        filename : OPTIONAL : name of the file for debugging. default aepp.log\n\t    Output:\n\t        level : Level of the logger to display information (NOTSET, DEBUG,INFO,WARNING,EROR,CRITICAL)\n", "        stream : If the logger should display print statements\n\t        file : If the logger should write the messages to a file\n\t        filename : name of the file where log are written\n\t        format : format of the logs\n\t    \"\"\"\n\t    myObject = {\n\t        \"level\": level,\n\t        \"stream\": True,\n\t        \"file\": False,\n\t        \"format\": \"%(asctime)s::%(name)s::%(funcName)s::%(levelname)s::%(message)s::%(lineno)d\",\n", "        \"filename\": filename,\n\t    }\n\t    return myObject\n\tclass ConnectObject:\n\t    \"\"\"\n\t    A connect Object class that keep tracks of the configuration loaded during the importConfigFile operation or during configure operation.\n\t    \"\"\"\n\t    def __init__(self,\n\t            org_id: str = None,\n\t            tech_id: str = None,\n", "            secret: str = None,\n\t            client_id: str = None,\n\t            path_to_key: str = None,\n\t            private_key: str = None,\n\t            scopes:str=None,\n\t            sandbox: str = \"prod\",\n\t            environment: str = \"prod\",\n\t            auth_code:str=None,\n\t            **kwargs)->None:\n\t        \"\"\"\n", "        Take a config object and save the configuration directly in the instance of the class.\n\t        \"\"\"\n\t        self.header = {\"Accept\": \"application/json\",\n\t          \"Content-Type\": \"application/json\",\n\t          \"Authorization\": \"\",\n\t          \"x-api-key\": client_id,\n\t          \"x-gw-ims-org-id\": org_id,\n\t          \"x-sandbox-name\": sandbox\n\t          }\n\t        ## setting environment prod vs non-prod for token generation\n", "        if environment == \"prod\":\n\t            self.globalEndpoint = \"https://platform.adobe.io\"\n\t            self.imsEndpoint = \"https://ims-na1.adobelogin.com\"\n\t        else:\n\t            self.globalEndpoint = f\"https://platform-{environment}.adobe.io\"\n\t            self.imsEndpoint = \"https://ims-na1-stg1.adobelogin.com\"\n\t        self.streamInletEndpoint = f\"{self.globalEndpoint}/data/core/edge\"\n\t        self.jwtEndpoint = f\"{self.imsEndpoint}/ims/exchange/jwt\"\n\t        self.oauthEndpointV1 = f\"{self.imsEndpoint}/ims/token/v1\"\n\t        self.oauthEndpointV2 = f\"{self.imsEndpoint}/ims/token/v2\"\n", "        self.org_id = org_id\n\t        self.tech_id = tech_id\n\t        self.client_id = client_id\n\t        self.secret = secret\n\t        self.pathToKey = path_to_key\n\t        self.privateKey = private_key\n\t        self.sandbox = sandbox\n\t        self.scopes = scopes\n\t        self.token = \"\"\n\t        self.__configObject__ = {\n", "            \"org_id\": self.org_id,\n\t            \"client_id\": self.client_id,\n\t            \"tech_id\": self.tech_id,\n\t            \"pathToKey\": self.pathToKey,\n\t            \"private_key\": self.privateKey,\n\t            \"secret\": self.secret,\n\t            \"date_limit\" : 0,\n\t            \"sandbox\": self.sandbox,\n\t            \"token\": \"\",\n\t            \"imsEndpoint\" : self.imsEndpoint,\n", "            \"jwtTokenEndpoint\" : self.jwtEndpoint,\n\t            \"oauthTokenEndpointV1\" : self.oauthEndpointV1,\n\t            \"oauthTokenEndpointV2\" : self.oauthEndpointV2,\n\t            \"scopes\": self.scopes\n\t        }\n\t    def connect(self)->None:\n\t        \"\"\"\n\t        Generate a token and provide a connector instance in that class.\n\t        \"\"\"\n\t        self.connector = connector.AdobeRequest(self.__configObject__,self.header)\n", "        self.token = self.connector.token\n\t        self.header['Authorization'] = 'bearer '+self.token\n\t    def getConfigObject(self)->dict:\n\t        \"\"\"\n\t        Return the config object expected.\n\t        \"\"\"\n\t        return self.__configObject__\n\t    def getConfigHeader(self)->dict:\n\t        \"\"\"\n\t        Return the default header\n", "        \"\"\"\n\t        return self.header\n\t    def setSandbox(self,sandbox:str=None)->dict:\n\t        \"\"\"\n\t        Update the sandbox used\n\t        \"\"\"\n\t        if sandbox is None:\n\t            return None\n\t        self.sandbox = sandbox\n\t        self.header[\"x-sandbox-name\"] = sandbox\n", "        self.__configObject__[\"sandbox\"] = sandbox\n\t        return self.getConfigObject()\n"]}
{"filename": "aepp/sandboxes.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\timport logging\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass Sandboxes:\n\t    \"\"\"\n\t    A collection of methods to realize actions on the sandboxes.\n\t    It comes from the sandbox API:\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/sandbox-api.yaml\n\t    \"\"\"\n", "    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ):\n", "        \"\"\"\n\t        Instantiate the sandbox class.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t        Additional kwargs will update the header.\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n", "        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n", "                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n", "            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            loggingObject=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n", "            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"sandboxes\"]\n\t        )\n\t    def getSandboxes(self) -> list:\n", "        \"\"\"\n\t        Return the list of all the sandboxes\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSandboxes\")\n\t        path = self.endpoint + \"/sandboxes\"\n\t        res = self.connector.getData(path)\n\t        data = res[\"sandboxes\"]\n\t        return data\n\t    def getSandboxTypes(self) -> list:\n", "        \"\"\"\n\t        Return the list of all the sandboxes types.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSandboxTyoes\")\n\t        path = self.endpoint + \"/sandboxTypes\"\n\t        res = self.connector.getData(path)\n\t        data = res[\"sandboxTypes\"]\n\t        return data\n\t    def createSandbox(\n", "        self, name: str = None, title: str = None, type_sandbox: str = \"development\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a new sandbox in your AEP instance.\n\t        Arguments:\n\t            name : REQUIRED : name of your sandbox\n\t            title : REQUIRED : display name of your sandbox\n\t            type_sandbox : OPTIONAL : type of your sandbox. default : development.\n\t        \"\"\"\n\t        if name is None or title is None:\n", "            raise Exception(\"name and title cannot be empty\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSandbox\")\n\t        path = self.endpoint + \"/sandboxes\"\n\t        data = {\"name\": name, \"title\": title, \"type\": type_sandbox}\n\t        res = self.connector.postData(path, data=data)\n\t        return res\n\t    def getSandbox(self, name: str) -> dict:\n\t        \"\"\"\n\t        retrieve a Sandbox information by name\n", "        Argument:\n\t            name : REQUIRED : name of Sandbox\n\t        \"\"\"\n\t        if name is None:\n\t            raise Exception(\"Expected a name as parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSandbox\")\n\t        path = self.endpoint + f\"/sandboxes/{name}\"\n\t        res = self.connector.getData(path)\n\t        return res\n", "    def getSandboxId(self, name: str) -> str:\n\t        \"\"\"\n\t        Retrieve the ID of a sandbox by name.\n\t        Argument:\n\t            name : REQUIRED : name of Sandbox\n\t        \"\"\"\n\t        return self.getSandbox(name)[\"id\"]\n\t    def deleteSandbox(self, name: str) -> dict:\n\t        \"\"\"\n\t        Delete a sandbox by its name.\n", "        Arguments:\n\t            name : REQUIRED : sandbox to be deleted.\n\t        \"\"\"\n\t        if name is None:\n\t            raise Exception(\"Expected a name as parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteSandbox\")\n\t        path = self.endpoint + f\"/sandboxes/{name}\"\n\t        res = self.connector.deleteData(path)\n\t        return res\n", "    def resetSandbox(self, name: str) -> dict:\n\t        \"\"\"\n\t        Reset a sandbox by its name. Sandbox will be empty.\n\t        Arguments:\n\t            name : REQUIRED : sandbox name to be deleted.\n\t        \"\"\"\n\t        if name is None:\n\t            raise Exception(\"Expected a sandbox name as parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting resetSandbox\")\n", "        path = self.endpoint + f\"/sandboxes/{name}\"\n\t        res = self.connector.putData(path, data={'action':'reset'})\n\t        return res\n\t    def updateSandbox(self, name: str, action: dict = None) -> dict:\n\t        \"\"\"\n\t        Update the Sandbox with the action provided.\n\t        Arguments:\n\t            name : REQUIRED : sandbox name to be updated.\n\t            action : REQUIRED : dictionary defining the action to realize on that sandbox.\n\t        \"\"\"\n", "        if name is None:\n\t            raise Exception(\"Expected a sandbox name as parameter\")\n\t        if action is None or type(action) != dict:\n\t            raise Exception(\"Expected a dictionary to pass the action\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateSandboxes\")\n\t        path = self.endpoint + f\"/sandboxes/{name}\"\n\t        res = self.connector.patchData(path, data=action)\n\t        return res\n"]}
{"filename": "aepp/dataaccess.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\timport logging\n\timport time\n\timport io\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass DataAccess:\n\t    \"\"\"\n\t    A class providing methods based on the Data Access API\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/data-access-api.yaml\n", "    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n", "    ):\n\t        \"\"\"\n\t        Instantiate the DataAccess class.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t        Additional kwargs will update the header.\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n", "            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n", "                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n", "            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            loggingObject=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n", "        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"dataaccess\"]\n\t        )\n", "    def getBatchFiles(\n\t        self, batchId: str = None, verbose: bool = False, **kwargs\n\t    ) -> list:\n\t        \"\"\"\n\t        List all dataset files under a batch.\n\t        Arguments:\n\t            batchId : REQUIRED : The batch ID to look for.\n\t        Possible kwargs:\n\t            limit : A paging parameter to specify number of results per page.\n\t            start : A paging parameter to specify start of new page. For example: page=1\n", "        \"\"\"\n\t        if batchId is None:\n\t            raise ValueError(\"Require a batchId to be specified.\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getBatchFiles\")\n\t        params = {}\n\t        if kwargs.get(\"limit\", None) is not None:\n\t            params[\"limit\"] = str(kwargs.get(\"limit\"))\n\t        if kwargs.get(\"start\", None) is not None:\n\t            params[\"start\"] = str(kwargs.get(\"start\"))\n", "        path = f\"/batches/{batchId}/files\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, verbose=verbose\n\t        )\n\t        try:\n\t            return res[\"data\"]\n\t        except:\n\t            return res\n\t    def getBatchFailed(\n\t        self, batchId: str = None, path: str = None, verbose: bool = False, **kwargs\n", "    ) -> list:\n\t        \"\"\"\n\t        Lists all the dataset files under a failed batch.\n\t        Arguments:\n\t            batchId : REQUIRED : The batch ID to look for.\n\t            path : OPTIONAL : The full name of the file. The contents of the file would be downloaded if this parameter is provided.\n\t                For example: path=profiles.csv\n\t        Possible kwargs:\n\t            limit : A paging parameter to specify number of results per page.\n\t            start : A paging parameter to specify start of new page. For example: page=1\n", "        \"\"\"\n\t        if batchId is None:\n\t            raise ValueError(\"Require a batchId to be specified.\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getBatchFailed\")\n\t        params = {}\n\t        if kwargs.get(\"limit\", None) is not None:\n\t            params[\"limit\"] = str(kwargs.get(\"limit\"))\n\t        if kwargs.get(\"start\", None) is not None:\n\t            params[\"start\"] = str(kwargs.get(\"start\"))\n", "        if path is not None:\n\t            params[\"path\"] = path\n\t        pathEndpoint = f\"/batches/{batchId}/failed\"\n\t        res = self.connector.getData(\n\t            self.endpoint + pathEndpoint, params=params, verbose=verbose\n\t        )\n\t        try:\n\t            return res[\"data\"]\n\t        except:\n\t            return res\n", "    def getBatchMeta(self, batchId: str = None, path: str = None, **kwargs) -> dict:\n\t        \"\"\"\n\t        Lists files under a batch’s meta directory or download a specific file under it. The files under a batch’s meta directory may include the following:\n\t            row_errors: A directory containing 0 or more files with parsing, conversion, and/or validation errors found at the row level.\n\t            input_files: A directory containing metadata for 1 or more input files submitted with the batch.\n\t            row_errors_sample.json: A root level file containing the sampled set of row errors for the UX.\n\t        Arguments:\n\t            batchId : REQUIRED : The batch ID to look for.\n\t            path : OPTIONAL : The full name of the file. The contents of the file would be downloaded if this parameter is provided.\n\t                Possible values for this query include the following:\n", "                    row_errors\n\t                    input_files\n\t                    row_errors_sample.json\n\t        Possible kwargs:\n\t            limit : A paging parameter to specify number of results per page.\n\t            start : A paging parameter to specify start of new page. For example: page=1\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise ValueError(\"Require a batchId to be specified.\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getBatchMeta\")\n\t        params = {}\n\t        if kwargs.get(\"limit\", None) is not None:\n\t            params[\"limit\"] = str(kwargs.get(\"limit\"))\n\t        if kwargs.get(\"start\", None) is not None:\n\t            params[\"start\"] = str(kwargs.get(\"start\"))\n\t        if path is not None:\n\t            params[\"path\"] = path\n\t        pathEndpoint = f\"/batches/{batchId}/meta\"\n\t        res = self.connector.getData(\n", "            self.endpoint + pathEndpoint, headers=self.header, params=params\n\t        )\n\t        return res\n\t    def getHeadFile(\n\t        self,\n\t        dataSetFileId: str = None,\n\t        path: str = None,\n\t        verbose: bool = False,\n\t    ) -> dict:\n\t        \"\"\"\n", "        Get headers regarding a file.\n\t        Arguments:\n\t            dataSetFileId : REQURED : The ID of the dataset file you are retrieving.\n\t            path : REQUIRED : The full name of the file identified.\n\t                For example: path=profiles.csv\n\t        \"\"\"\n\t        if dataSetFileId is None or path is None:\n\t            raise ValueError(\"Require a dataSetFileId and a path for that method\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getHeadFile\")\n", "        params = {\"path\": path}\n\t        pathEndpoint = f\"/files/{dataSetFileId}\"\n\t        res = self.connector.headData(\n\t            self.endpoint + pathEndpoint, params=params, verbose=verbose\n\t        )\n\t        return res\n\t    def getFiles(\n\t        self,\n\t        dataSetFileId: str = None,\n\t        path: str = None,\n", "        range: str = None,\n\t        start: str = None,\n\t        limit: int = None,\n\t    ) -> Union[dict,bytes]:\n\t        \"\"\"\n\t        Returns either a complete file or a directory of chunked data that makes up the file.\n\t        The response contains a data array that may contain a single entry or a list of files belonging to that directory.\n\t        Arguments:\n\t            dataSetFileId : REQUIRED : The ID of the dataset file you are retrieving.\n\t            path : OPTIONAL : The full name of the file. The contents of the file would be downloaded if this parameter is provided.\n", "                For example: path=profiles.csv\n\t                if the extension is .parquet, it will try to return the parquet data decoded (io.BytesIO). \n\t            range : OPTIONAL : The range of bytes requested. For example: Range: bytes=0-100000\n\t            start : OPTIONAL : A paging parameter to specify start of new page. For example: start=fileName.csv\n\t            limit : OPTIONAL : A paging parameter to specify number of results per page. For example: limit=10\n\t        \"\"\"\n\t        if dataSetFileId is None:\n\t            raise ValueError(\"Require a dataSetFileId\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFiles\")\n", "        params = {}\n\t        if path is not None:\n\t            params[\"path\"] = path\n\t        if range is not None:\n\t            params[\"range\"] = range\n\t        if start is not None:\n\t            params[\"start\"] = start\n\t        if limit is not None:\n\t            params[\"limit\"] = limit\n\t        pathEndpoint = f\"/files/{dataSetFileId}\"\n", "        if path is None:\n\t            res: dict = self.connector.getData(\n\t                self.endpoint + pathEndpoint, headers=self.header, params=params\n\t            )\n\t        else:\n\t            if path.endswith('.parquet'):\n\t                data = self.getResource(self.endpoint + pathEndpoint,params={\"path\":path},format='raw')\n\t                res = io.BytesIO(data.content)\n\t            else:\n\t                data = self.getResource(self.endpoint + pathEndpoint,params={\"path\":path},format='raw')\n", "                res = data.content\n\t        return res\n\t    def getPreview(self, datasetId: str = None) -> list:\n\t        \"\"\"\n\t        Give a preview of a specific dataset\n\t        Arguments:\n\t            datasetId : REQUIRED : the dataset ID to preview\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise ValueError(\"Require a datasetId\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPreview\")\n\t        path = f\"/datasets/{datasetId}/preview\"\n\t        res: dict = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        try:\n\t            return res[\"data\"]\n\t        except:\n\t            return res\n\t    def getResource(\n\t        self,\n", "        endpoint: str = None,\n\t        params: dict = None,\n\t        format: str = \"json\",\n\t        save: bool = False,\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Template for requesting data with a GET method.\n\t        Arguments:\n\t            endpoint : REQUIRED : The URL to GET\n", "            params: OPTIONAL : dictionary of the params to fetch\n\t            format : OPTIONAL : Type of response returned. Possible values:\n\t                json : default\n\t                txt : text file\n\t                raw : a response object from the requests module\n\t        \"\"\"\n\t        if endpoint is None:\n\t            raise ValueError(\"Require an endpoint\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n", "                f\"Using getResource with following format ({format}) to the following endpoint: {endpoint}\"\n\t            )\n\t        res = self.connector.getData(endpoint, params=params, format=format)\n\t        if save:\n\t            if format == \"json\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"json\",\n", "                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n\t            elif format == \"txt\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"txt\",\n\t                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n", "            else:\n\t                print(\n\t                    \"element is an object. Output is unclear. No save made.\\nPlease save this element manually\"\n\t                )\n\t        return res\n"]}
{"filename": "aepp/policy.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "import typing\n\tfrom aepp import connector\n\timport logging\n\tfrom .configs import ConnectObject\n\tclass Policy:\n\t    \"\"\"\n\t    Class to manage and retrieve DULE policy.\n\t    This is based on the following API reference : https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/\n\t    \"\"\"\n\t    ## logging capability\n", "    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: typing.Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n", "        Instantiate the class to manage DULE rules and statement directly.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n", "            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n", "            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n", "            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            loggingObject=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n", "            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"policy\"]\n\t        )\n\t    def getEnabledCorePolicies(self) -> dict:\n\t        \"\"\"\n\t        Retrieve a list of all enabled core policies.\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getEnabledCorePolicies\")\n\t        path = \"/enabledCorePolicies\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def createEnabledCorePolicies(self, policyIds: list) -> dict:\n\t        \"\"\"\n\t        Create or update the list of enabled core policies. (PUT)\n\t        Argument:\n", "            policyIds : REQUIRED : list of core policy ID to enable\n\t        \"\"\"\n\t        path = \"/enabledCorePolicies\"\n\t        if policyIds is None or type(policyIds) != list:\n\t            raise ValueError(\"Require a list of policy ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createEnabledCorePolicies\")\n\t        obj = policyIds\n\t        res = self.connector.putData(self.endpoint + path, data=obj)\n\t        return res\n", "    def bulkEval(self, data: dict = None) -> dict:\n\t        \"\"\"\n\t        Enable to pass a list of policies to check against a list of dataSet.\n\t        Argument:\n\t            data : REQUIRED : Dictionary describing the set of label and datasets.\n\t                see https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Bulk_evaluation/bulkEvalPost\n\t        \"\"\"\n\t        path = \"/bulk-eval\"\n\t        if data is None:\n\t            raise Exception(\n", "                \"Requires a dictionary to set the labels and dataSets to check\"\n\t            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting bulkEval\")\n\t        res = self.connector.postData(self.endpoint + path, data=data)\n\t        return res\n\t    def getPoliciesCore(self, **kwargs) -> dict:\n\t        \"\"\"\n\t        Returns the core policies in place in the Organization.\n\t        Possible kwargs:\n", "            limit : A positive integer, providing a hint as to the maximum number of resources to return in one page of results.\n\t            property : Filter responses based on a property and optional existence or relational values.\n\t            Only the ‘name’ property is supported for core resources.\n\t            For custom resources, additional supported property values include 'status’, 'created’, 'createdClient’, 'createdUser’, 'updated’, 'updatedClient’, and 'updatedUser’\n\t            orderby : A comma-separated list of properties by which the returned list of resources will be sorted.\n\t            start : Requests items whose ‘orderby’ property value are strictly greater than the supplied ‘start’ value.\n\t            duleLabels : A comma-separated list of DULE labels. Return only those policies whose \"deny\" expression references any of the labels in this list\n\t            marketingAction : Restrict returned policies to those that reference the given marketing action.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getPoliciesCore\")\n\t        path = \"/policies/core\"\n\t        params = {**kwargs}\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        return res\n\t    def getPoliciesCoreId(self, policy_id: str = None):\n\t        \"\"\"\n\t        Return a specific core policy by its id.\n", "        Arguments:\n\t            policy_id : REQUIRED : policy_id to retrieve.\n\t        \"\"\"\n\t        if policy_id is None:\n\t            raise Exception(\"Expected a policy id\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPoliciesCoreId\")\n\t        path = f\"/policies/core/{policy_id}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n", "    def getPoliciesCustoms(self, **kwargs):\n\t        \"\"\"\n\t        Returns the custom policies in place in the Organization.\n\t        Possible kwargs:\n\t            limit : A positive integer, providing a hint as to the maximum number of resources to return in one page of results.\n\t            property : Filter responses based on a property and optional existence or relational values.\n\t            Only the ‘name’ property is supported for core resources.\n\t            For custom resources, additional supported property values include 'status’, 'created’, 'createdClient’, 'createdUser’, 'updated’, 'updatedClient’, and 'updatedUser’\n\t            orderby : A comma-separated list of properties by which the returned list of resources will be sorted.\n\t            start : Requests items whose ‘orderby’ property value are strictly greater than the supplied ‘start’ value.\n", "            duleLabels : A comma-separated list of DULE labels. Return only those policies whose \"deny\" expression references any of the labels in this list\n\t            marketingAction : Restrict returned policies to those that reference the given marketing action.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPoliciesCustoms\")\n\t        path = \"/policies/custom\"\n\t        params = {**kwargs}\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n", "        return res\n\t    def getPoliciesCustom(self, policy_id: str = None):\n\t        \"\"\"\n\t        Return a specific custom policy by its id.\n\t        Arguments:\n\t            policy_id: REQUIRED: policy_id to retrieve.\n\t        \"\"\"\n\t        if policy_id is None:\n\t            raise Exception(\"Expected a policy id\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getPoliciesCustom\")\n\t        path = f\"/policies/custom/{policy_id}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def createPolicy(self, policy: typing.Union(dict, typing.IO) = None):\n\t        \"\"\"\n\t        Create a custom policy.\n\t        Arguments:\n\t            policy : REQUIRED : A dictionary contaning the policy you would like to implement.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createPolicy\")\n\t        path = \"/policies/custom\"\n\t        res = self.connector.postData(self.endpoint + path, data=policy)\n\t        return res\n\t    def getCoreLabels(self, prop: str = None, limit: int = 100) -> list:\n\t        \"\"\"\n\t        Retrieve a list of core labels.\n\t        Arguments:\n\t            prop : OPTIONAL : Filters responses based on whether a specific property exists, or whose value passes a conditional expression\n", "                Example: prop=\"name==C1\".\n\t                Only the “name” property is supported for core resources.\n\t            limit : OPTIONAL : number of results to be returned. Default 100\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCoreLabels\")\n\t        params = {\"limit\": limit}\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n\t        path = \"/labels/core\"\n", "        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        data = res[\"children\"]\n\t        nextPage = res[\"_links\"][\"page\"].get(\"href\", \"\")\n\t        while nextPage != \"\":\n\t            params[\"start\"] = nextPage\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n\t            )\n", "            data += res[\"children\"]\n\t            nextPage = res[\"_links\"][\"page\"].get(\"href\", \"\")\n\t        return data\n\t    def getCoreLabel(self, labelName: str = None) -> dict:\n\t        \"\"\"\n\t        Returns a specific Label by its name.\n\t        Argument:\n\t            labelName : REQUIRED : The name of the core label.\n\t        \"\"\"\n\t        if labelName is None:\n", "            raise ValueError(\"Require a label name\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCoreLabel\")\n\t        path = f\"/labels/core/{labelName}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getCustomLabels(self, prop: str = None, limit: int = 100) -> list:\n\t        \"\"\"\n\t        Retrieve a list of custom labels.\n\t        Arguments:\n", "            prop : OPTIONAL : Filters responses based on whether a specific property exists, or whose value passes a conditional expression\n\t                Example: prop=\"name==C1\".\n\t                Property values include \"status\", \"created\", \"createdClient\", \"createdUser\", \"updated\", \"updatedClient\", and \"updatedUser\".\n\t            limit : OPTIONAL : number of results to be returned. Default 100\n\t        \"\"\"\n\t        params = {\"limit\": limit}\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCustomLabels\")\n", "        path = \"/labels/custom\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        data = res[\"children\"]\n\t        nextPage = res[\"_links\"][\"page\"].get(\"href\", \"\")\n\t        while nextPage != \"\":\n\t            params[\"start\"] = nextPage\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n", "            )\n\t            data += res[\"children\"]\n\t            nextPage = res[\"_links\"][\"page\"].get(\"href\", \"\")\n\t        return data\n\t    def getCustomLabel(self, labelName: str = None) -> dict:\n\t        \"\"\"\n\t        Returns a specific Label by its name.\n\t        Argument:\n\t            labelName : REQUIRED : The name of the custom label.\n\t        \"\"\"\n", "        if labelName is None:\n\t            raise ValueError(\"Require a label name\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCustomLabel\")\n\t        path = f\"/labels/custom/{labelName}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def updateCustomLabel(self, labelName: str = None, data: dict = None) -> dict:\n\t        \"\"\"\n\t        Update a specific Label by its name. (PUT method)\n", "        Argument:\n\t            labelName : REQUIRED : The name of the custom label.\n\t            data : REQUIRED : Data to replace the old definition\n\t                Example:\n\t                {\n\t                    \"name\": \"L2\",\n\t                    \"category\": \"Custom\",\n\t                    \"friendlyName\": \"Purchase History Data\",\n\t                    \"description\": \"Data containing information on past transactions\"\n\t                }\n", "        \"\"\"\n\t        if labelName is None:\n\t            raise ValueError(\"Require a label name\")\n\t        if data is None or type(data) != dict:\n\t            raise ValueError(\"Require a dictionary data to be passed\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateCustomLabel\")\n\t        path = f\"/labels/custom/{labelName}\"\n\t        res = self.connector.putData(self.endpoint + path, data=data)\n\t        return res\n", "    def getMarketingActionsCores(\n\t        self, prop: str = None, limit: int = 10, **kwargs\n\t    ) -> list:\n\t        \"\"\"\n\t        Retrieve a list of core marketing actions.\n\t        Arguments:\n\t            prop : OPTIONAL : Filters responses based on whether a specific property exists, or whose value passes a conditional expression (e.g. \"prop=name==C1\").\n\t            Only the “name” property is supported for core resources.\n\t            limit : OPTIONAL : number of results to be returned.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMarketingActionsCores\")\n\t        path = \"/marketingActions/core\"\n\t        params = {\"limit\": limit}\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        data = res[\"children\"]\n\t        nextPage = res[\"_links\"][\"page\"].get(\"href\", \"\")\n\t        while nextPage != \"\":\n", "            params[\"start\"] = nextPage\n\t            res = self.connector.getData(self.endpoint + path, params=params)\n\t            data += res[\"children\"]\n\t            nextPage = res[\"_links\"][\"page\"].get(\"href\", \"\")\n\t        return data\n\t    def getMarketingActionsCore(self, mktActionName:str=None)->dict:\n\t        \"\"\"\n\t        Get a specific marketing action core by marketing Action Name.\n\t        Arguments:\n\t            mktActionName : REQUIRED : The marketing action name to be provided.\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMarketingActionsCore\")\n\t        if mktActionName is None:\n\t            raise ValueError(\"Must provide a Marketing Action Name\")\n\t        path = f\"/marketingActions/core/{mktActionName}\"\n\t        data = self.connector.getData(self.endpoint+path)\n\t        return data\n\t    def getCustomMarketingActions(self,prop:str=None,limit:int=10,**kwargs)->list:\n\t        \"\"\"\n", "        Retrieve a list of custom Marketing Actions\n\t        Arguments:\n\t            prop : OPTIONAL : Filters responses based on whether a specific property exists, or whose value passes a conditional expression (e.g. ?property=name==C1). Only the name property is supported for core resources. \n\t                For custom resources, additional supported property values include \"status\", \"created\", \"createdClient\", \"createdUser\", \"updated\", \"updatedClient\", and \"updatedUser\"\n\t        Possible kwargs:\n\t            orderby : A comma-separated list of properties by which the returned list of resources will be sorted.\n\t            start : Indicates the pagination value for the returned list. This value should be obtained from a previous call's _page.next property. Should be omitted for a first page of results.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCustomMarketingActions\")\n", "        params = {\"limit\" : limit}\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n\t        if kwargs.get('orderby',None) is not None:\n\t            params[\"orderby\"] = kwargs.get('orderby')\n\t        if kwargs.get('start',None) is not None:\n\t            params['start'] = kwargs.get('start')\n\t        path = f\"/marketingActions/custom\"\n\t        res = self.connector.getData(self.endpoint + path,params = params)\n\t        data = res['children']\n", "        nextPage = res.get(\"_page\",{}).get('next','')\n\t        while nextPage!= \"\":\n\t            params['start'] = nextPage\n\t            res = self.connector.getData(self.endpoint + path,params = params)\n\t            data += res['children']\n\t            nextPage = res.get(\"_page\",{}).get('next','')\n\t        return data\n\t    def getCustomMarketingAction(self,mktActionName:str=None)->dict:\n\t        \"\"\"\n\t        Return a specific marketing action\n", "        Arguments:\n\t            mktActionName : REQUIRED : The marketing action name to be returned.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCustomMarketingAction\")\n\t        if mktActionName is None:\n\t            raise ValueError(\"Require a custom marketing action name\")\n\t        path = f\"/marketingActions/custom/{mktActionName}\"\n\t        data = self.connector.getData(self.endpoint+path)\n\t        return data\n", "    def createOrupdateCustomMarketingAction(self,name:str=None,description:str=\"\")->dict:\n\t        \"\"\"\n\t        Create or update a custom marketing action based on the parameter provided.\n\t        Arguments:\n\t            name : REQUIRED : The name of the custom marketing action\n\t            description : OPTIONAL : the description for that custom marketing action.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createOrupdateCustomMarketingAction\")\n\t        if name is None:\n", "            raise ValueError(\"Require a name for your custom marketing action\")\n\t        path = f\"/marketingActions/custom/{name}\"\n\t        data = {\n\t            \"name\":name,\n\t            \"description\" : description\n\t        }\n\t        res = self.connector.putData(self.endpoint+path,data=data)\n\t        return res\n\t    def deleteCustomMarketingAction(self,mktActionName:str=None)->dict:\n\t        \"\"\"\n", "        Delete a specific custom Marketing action\n\t        Arguments:\n\t            mktActionName : REQUIRED : The marketing action name to be deleted.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteCustomMarketingAction\")\n\t        if mktActionName is None:\n\t            raise ValueError(\"Require a marketing action name to be deleted\")\n\t        path = f\"/marketingActions/custom/{mktActionName}\"\n\t        res = self.connector.deleteData(self.endpoint+path)\n", "        return res\n\t    def evaluateMarketingActionDataset(self,\n\t                                typeMktAction:str=\"core\",\n\t                                mktActionName:str=None,\n\t                                entityType:str=\"dataSet\",\n\t                                entityId:str=None,\n\t                                entityMeta:list=None,\n\t                                draftEvaluation:bool=False,\n\t                                )->dict:\n\t        \"\"\"\n", "        Evaluate either Marketing Action core or custom based on parameter again some field on a datasetId.\n\t        Arguments\n\t            typeMktAction : REQUIRED : Default to \"core\", can be \"custom\"\n\t            mktActionName : REQUIRED : The name of the marketing action to be evaluated\n\t            entityType : REQUIRED : The type of entity to be tested against. Usually \"dataSet\", so set as default.\n\t            entityId : REQUIRED : The Id of the entity to be tested.\n\t            entityMeta : REQUIRED : A list of field to be tested for the marketing action in case of a dataset.\n\t            draftEvaluation : OPTIONAL : If true, the system checks for policy violations among policies with DRAFT status as well as ENABLED status. Otherwise, only ENABLED policies are checked.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting evaluateMarketingActionDataset\")\n\t        if typeMktAction is None:\n\t            raise ValueError(\"Should have typeMktAction defined. Either 'core' or 'custom'\")\n\t        if mktActionName is None:\n\t            raise ValueError(\"Should have mktActionName defined.\")\n\t        if entityType is None:\n\t            raise ValueError(\"Should have entityType defined.\")\n\t        if entityId is None:\n\t            raise ValueError(\"Should have entityId defined.\")\n\t        params = {}\n", "        path = f\"/marketingActions/{typeMktAction}/{mktActionName}/constraints\"\n\t        data = [\n\t                    {\n\t                    \"entityType\": entityType,\n\t                    \"entityId\": entityId,\n\t                    }\n\t                ]\n\t        if entityMeta is not None:\n\t            data[0]['entityMeta']={\"fields\":entityMeta}\n\t        if draftEvaluation:\n", "            params[\"includeDraft\"] = True\n\t        res = self.connector.postData(self.endpoint+path,data=data)\n\t        return res\n\t    def evaluateMarketingActionUsageLabel(self,\n\t                                          typeMktAction:str='core',\n\t                                          mktActionName:str=None,\n\t                                          duleLabels:str=None,\n\t                                          draftEvaluation:bool=False\n\t                                          )->dict:\n\t        \"\"\"\n", "        This call returns a set of constraints that would govern an attempt to perform the given marketing action on a hypothetical source of data containing specific data usage labels.\n\t        Arguments:\n\t            typeMktAction : REQUIRED : Default to \"core\", can be \"custom\"\n\t            mktActionName : REQUIRED : The name of the marketing action to be evaluated\n\t            duleLabels : REQUIRED: A comma-separated list of data usage labels that would be present on data that you want to test for policy violations.\n\t            draftEvaluation : OPTIONAL : If true, the system checks for policy violations among policies with DRAFT status as well as ENABLED status. Otherwise, only ENABLED policies are checked.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting evaluateMarketingActionDataset\")\n\t        if typeMktAction is None:\n", "            raise ValueError(\"Should have typeMktAction defined. Either 'core' or 'custom'\")\n\t        if mktActionName is None:\n\t            raise ValueError(\"Should have mktActionName defined.\")\n\t        if duleLabels is None:\n\t            raise ValueError(\"Should have duleLabels defined.\")\n\t        path = f\"marketingActions/{typeMktAction}/{mktActionName}/constraints\"\n\t        params = {\"duleLabels\":duleLabels}\n\t        if draftEvaluation:\n\t            params['includeDraft'] = draftEvaluation\n\t        res = self.connector.getData(self.endpoint+path,params=params)\n", "        return res"]}
{"filename": "aepp/config.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "config_object = {\n\t    \"org_id\": \"\",\n\t    \"client_id\": \"\",\n\t    \"tech_id\": \"\",\n\t    \"pathToKey\": \"\",\n\t    \"scopes\": \"\",\n\t    \"secret\": \"\",\n\t    \"date_limit\": 0,\n\t    \"sandbox\": \"\",\n\t    \"environment\": \"\",\n", "    \"token\": \"\",\n\t    \"jwtTokenEndpoint\": \"\",\n\t    \"oauthTokenEndpointV1\": \"\",\n\t    \"oauthTokenEndpointV2\": \"\",\n\t    \"imsEndpoint\": \"\"\n\t}\n\theader = {\"Accept\": \"application/json\",\n\t          \"Content-Type\": \"application/json\",\n\t          \"Authorization\": \"\",\n\t          \"x-api-key\": config_object[\"client_id\"],\n", "          \"x-gw-ims-org-id\": config_object[\"org_id\"],\n\t          \"x-sandbox-name\": \"prod\"\n\t          }\n\t# endpoints\n\tendpoints = {\n\t    # global endpoint is https://platform.adobe.io in prod, otherwise https://platform-$ENV.adobe.io\n\t    \"global\": \"\",\n\t    \"schemas\": \"/data/foundation/schemaregistry\",\n\t    \"query\": \"/data/foundation/query\",\n\t    \"catalog\": \"/data/foundation/catalog\",\n", "    \"policy\": \"/data/foundation/dulepolicy\",\n\t    \"segmentation\": \"/data/core/ups\",\n\t    \"export\": \"/data/foundation/export\",\n\t    \"identity\": \"/data/core/\",\n\t    \"sandboxes\": \"/data/foundation/sandbox-management\",\n\t    \"sensei\": \"/data/sensei\",\n\t    \"access\": \"/data/foundation/access-control\",\n\t    \"flow\": \"/data/foundation/flowservice\",\n\t    \"privacy\": \"/data/core/privacy\",\n\t    \"dataaccess\": \"/data/foundation/export\",\n", "    \"mapping\": \"/data/foundation/conversion\",\n\t    \"policy\": \"/data/foundation/dulepolicy\",\n\t    \"dataset\": \"/data/foundation/dataset\",\n\t    \"ingestion\": \"/data/foundation/import\",\n\t    \"observability\": \"/data/infrastructure/observability/insights\",\n\t    \"destinationAuthoring\": \"/data/core/activation/authoring\",\n\t    \"destinationInstance\" : \"/data/core/activation/disflowprovider\",\n\t    \"streaming\": {\n\t        \"inlet\": \"\",\n\t        \"collection\": \"https://dcs.adobedc.net\"\n", "    },\n\t    \"audit\": \"/data/foundation\"\n\t}"]}
{"filename": "aepp/ingestion.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\tfrom copy import deepcopy\n\timport requests\n\tfrom typing import Union\n\timport logging\n\timport json\n\tfrom pathlib import Path\n\tfrom .configs import ConnectObject\n\tclass DataIngestion:\n\t    \"\"\"\n", "    Class that manages sending data via authenticated methods.\n\t    For Batch and Streaming messages.\n\t    \"\"\"\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n", "        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Instantiate the DataAccess class.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t        Additional kwargs will update the header.\n\t        \"\"\"\n\t        requests.packages.urllib3.disable_warnings()\n", "        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n", "            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n", "        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config, \n\t            header=header,\n\t            logger=self.logger,\n\t            loggingEnabled=self.loggingEnabled)\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n", "        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"ingestion\"]\n\t        )\n", "        self.endpoint_streaming = aepp.config.endpoints[\"streaming\"][\"collection\"]\n\t        self.STREAMING_REFERENCE = {\n\t            \"header\": {\n\t                \"schemaRef\": {\n\t                    \"id\": \"https://ns.adobe.com/{TENANT_ID}/schemas/{SCHEMA_ID}\",\n\t                    \"contentType\": \"application/vnd.adobe.xed-full+json;version={SCHEMA_VERSION}\",\n\t                },\n\t                \"imsOrgId\": \"{IMS_ORG_ID}\",\n\t                \"datasetId\": \"{DATASET_ID}\",\n\t                \"createdAt\": \"1526283801869\",\n", "                \"source\": {\"name\": \"{SOURCE_NAME}\"},\n\t            },\n\t            \"body\": {\n\t                \"xdmMeta\": {\n\t                    \"schemaRef\": {\n\t                        \"id\": \"https://ns.adobe.com/{TENANT_ID}/schemas/{SCHEMA_ID}\",\n\t                        \"contentType\": \"application/vnd.adobe.xed-full+json;version={SCHEMA_VERSION}\",\n\t                    }\n\t                },\n\t                \"xdmEntity\": {\n", "                    \"person\": {\n\t                        \"name\": {\n\t                            \"firstName\": \"Jane\",\n\t                            \"middleName\": \"F\",\n\t                            \"lastName\": \"Doe\",\n\t                        },\n\t                        \"birthDate\": \"1969-03-14\",\n\t                        \"gender\": \"female\",\n\t                    },\n\t                    \"workEmail\": {\n", "                        \"primary\": True,\n\t                        \"address\": \"janedoe@example.com\",\n\t                        \"type\": \"work\",\n\t                        \"status\": \"active\",\n\t                    },\n\t                },\n\t            },\n\t        }\n\t    def createBatch(\n\t        self,\n", "        datasetId: str = None,\n\t        format: str = \"json\",\n\t        multiline: bool = False,\n\t        enableDiagnostic: bool = False,\n\t        partialIngestionPercentage: int = 0,\n\t        **kwargs\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a new batch in Catalog Service.\n\t        Arguments:\n", "            datasetId : REQUIRED : The Dataset ID for the batch to upload data to.\n\t            format : REQUIRED : the format of the data send.(default json)\n\t            multiline : OPTIONAL : If you wish to upload multi-line JSON.\n\t        Possible kwargs:\n\t            replay : the replay object to replay a batch.\n\t            https://experienceleague.adobe.com/docs/experience-platform/ingestion/batch/api-overview.html?lang=en#replay-a-batch\n\t        \"\"\"\n\t        if datasetId is None:\n\t            raise ValueError(\"Require a dataSetId\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Using createBatch with following format ({format})\")\n\t        obj = {\n\t            \"datasetId\": datasetId,\n\t            \"inputFormat\": {\"format\": format, \"isMultiLineJson\": False},\n\t        }\n\t        if len(kwargs.get('replay',{}))>0:\n\t            obj['replay'] = kwargs.get('replay')\n\t        if multiline is True:\n\t            obj[\"inputFormat\"][\"isMultiLineJson\"] = True\n\t        if enableDiagnostic != False:\n", "            obj[\"enableErrorDiagnostics\"] = True\n\t        if partialIngestionPercentage > 0:\n\t            obj[\"partialIngestionPercentage\"] = partialIngestionPercentage\n\t        path = \"/batches\"\n\t        res = self.connector.postData(self.endpoint + path, data=obj)\n\t        return res\n\t    def deleteBatch(self, batchId: str = None) -> str:\n\t        \"\"\"\n\t        Delete a batch by applying the revert action on it.\n\t        Argument:\n", "            batchId : REQUIRED : Batch ID to be deleted\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise ValueError(\"Require a batchId argument\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteBatch for ID: ({batchId})\")\n\t        path = f\"/batches/{batchId}\"\n\t        params = {\"action\": \"REVERT\"}\n\t        res = self.connector.postData(self.endpoint + path, params=params)\n\t        return res\n", "    def replayBatch(self, datasetId: str = None, batchIds: list = None) -> dict:\n\t        \"\"\"\n\t        You can replay a batch that has already been ingested. You need to provide the datasetId and the list of batch to be replay.\n\t        Once specify through that action, you will need to re-upload batch information via uploadSmallFile method with JSON format and then specify the completion.\n\t        You will need to re-use the batchId provided for the re-upload.\n\t        Arguments:\n\t            dataSetId : REQUIRED : The dataset ID attached to the batch\n\t            batchIds : REQUIRED : The list of batchID to replay.\n\t        \"\"\"\n\t        if datasetId is None:\n", "            raise ValueError(\"Require a dataset ID\")\n\t        if batchIds is None or type(batchIds) != list:\n\t            raise ValueError(\"Require a list of batch ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting replayBatch for dataset ID: ({datasetId})\")\n\t        path = \"/batches\"\n\t        predecessors = [f\"${batchId}\" for batchId in batchIds]\n\t        data = {\n\t            \"datasetId\": datasetId,\n\t            \"inputFormat\": {\"format\": \"json\"},\n", "            \"replay\": {\"predecessors\": predecessors, \"reason\": \"replace\"},\n\t        }\n\t        res = self.connector.patchData(self.endpoint + path, data=data)\n\t        return res\n\t    def uploadSmallFile(\n\t        self,\n\t        batchId: str = None,\n\t        datasetId: str = None,\n\t        filePath: str = None,\n\t        data: Union[list, dict,str] = None,\n", "        verbose: bool = False,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Upload a small file (<256 MB) to the filePath location in the dataset.\n\t        Arguments:\n\t            batchId : REQUIRED : The batchId referencing the batch processed created beforehand.\n\t            datasetId : REQUIRED : The dataSetId related to where the data are ingested to.\n\t            filePath : REQUIRED : the filePath that will store the value.\n\t            data : REQUIRED : The data to be uploaded (following the type provided). List or Dictionary, depending if multiline is enabled.\n\t                You can also pass a JSON file path. If the element is a string and ends with \".json\", the file will be loaded and transform automatically to a dictionary. \n", "            verbose: OPTIONAL : if you wish to see comments around the\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise Exception(\"require a batchId\")\n\t        if datasetId is None:\n\t            raise Exception(\"require a dataSetId\")\n\t        if filePath is None:\n\t            raise Exception(\"require a filePath value\")\n\t        if data is None:\n\t            raise Exception(\"require data to be passed\")\n", "        if verbose:\n\t            print(f\"Your data is in {type(data)} format\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"uploadSmallFile as format: ({type(data)})\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Content-Type\"] = \"application/octet-stream\"\n\t        path = f\"/batches/{batchId}/datasets/{datasetId}/files/{filePath}\"\n\t        if type(data) == str:\n\t            if '.json' in data:\n\t                with open(Path(path),'r') as f:\n", "                    data = json.load(f)\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=data, headers=privateHeader\n\t        )\n\t        return res\n\t    def uploadSmallFileFinish(\n\t        self, batchId: str = None, action: str = \"COMPLETE\", verbose: bool = False\n\t    ) -> dict:\n\t        \"\"\"\n\t        Send an action to signify that the import is done.\n", "        Arguments:\n\t            batchId : REQUIRED : The batchId referencing the batch processed created beforehand.\n\t            action : REQUIRED : either one of these actions:\n\t                COMPLETE (default value)\n\t                ABORT\n\t                FAIL\n\t                REVERT\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise Exception(\"require a batchId\")\n", "        if action is None or action not in [\"COMPLETE\", \"ABORT\", \"FAIL\", \"REVERT\"]:\n\t            raise Exception(\"Not a valid action has been passed\")\n\t        path = f\"/batches/{batchId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Finishing upload for batch ID: ({batchId})\")\n\t        params = {\"action\": action}\n\t        res = self.connector.postData(\n\t            self.endpoint + path, params=params, verbose=verbose\n\t        )\n\t        return res\n", "    def uploadLargeFileStartEnd(\n\t        self,\n\t        batchId: str = None,\n\t        datasetId: str = None,\n\t        filePath: str = None,\n\t        action: str = \"INITIALIZE\",\n\t    ) -> dict:\n\t        \"\"\"\n\t        Start / End the upload of a large file with a POST method defining the action (see parameter)\n\t        Arguments:\n", "            batchId : REQUIRED : The batchId referencing the batch processed created beforehand.\n\t            datasetId : REQUIRED : The dataSetId related to where the data are ingested to.\n\t            filePath : REQUIRED : the filePath that will store the value.\n\t            action : REQUIRED : Action to either INITIALIZE or COMPLETE the upload.\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise Exception(\"require a batchId\")\n\t        if datasetId is None:\n\t            raise Exception(\"require a dataSetId\")\n\t        if filePath is None:\n", "            raise Exception(\"require a filePath value\")\n\t        params = {\"action\": action}\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n\t                f\"Starting or Ending large upload for batch ID: ({batchId})\"\n\t            )\n\t        path = f\"/batches/{batchId}/datasets/{datasetId}/files/{filePath}\"\n\t        res = self.connector.postData(self.endpoint + path, params=params)\n\t        return res\n\t    def uploadLargeFilePart(\n", "        self,\n\t        batchId: str = None,\n\t        datasetId: str = None,\n\t        filePath: str = None,\n\t        data: bytes = None,\n\t        contentRange: str = None,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Continue the upload of a large file with a PATCH method.\n\t        Arguments:\n", "            batchId : REQUIRED : The batchId referencing the batch processed created beforehand.\n\t            datasetId : REQUIRED : The dataSetId related to where the data are ingested to.\n\t            filePath : REQUIRED : the filePath that will store the value.\n\t            data : REQUIRED : The data to be uploaded (in bytes)\n\t            contentRange : REQUIRED : The range of bytes of the file being uploaded with this request.\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise Exception(\"require a batchId\")\n\t        if datasetId is None:\n\t            raise Exception(\"require a dataSetId\")\n", "        if filePath is None:\n\t            raise Exception(\"require a filePath value\")\n\t        if data is None:\n\t            raise Exception(\"require data to be passed\")\n\t        if contentRange is None:\n\t            raise Exception(\"require the content range to be passed\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Content-Type\"] = \"application/octet-stream\"\n\t        privateHeader[\"Content-Range\"] = contentRange\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Uploading large part for batch ID: ({batchId})\")\n\t        path = f\"/batches/{batchId}/datasets/{datasetId}/files/{filePath}\"\n\t        res = requests.patch(self.endpoint + path, data=data, headers=privateHeader, verify=False)\n\t        res_json = res.json()\n\t        return res_json\n\t    def headFileStatus(\n\t        self, batchId: str = None, datasetId: str = None, filePath: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Check the status of a large file upload.\n", "        Arguments:\n\t            batchId : REQUIRED : The batchId referencing the batch processed created beforehand.\n\t            datasetId : REQUIRED : The dataSetId related to where the data are ingested to.\n\t            filePath : REQUIRED : the filePath that reference the file.\n\t        \"\"\"\n\t        if batchId is None:\n\t            raise Exception(\"require a batchId\")\n\t        if datasetId is None:\n\t            raise Exception(\"require a dataSetId\")\n\t        if filePath is None:\n", "            raise Exception(\"require a filePath value\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Head File Status batch ID: ({batchId})\")\n\t        path = f\"/batches/{batchId}/datasets/{datasetId}/files/{filePath}\"\n\t        res = self.connector.headData(self.endpoint + path)\n\t        return res\n\t    def getPreviewBatchDataset(\n\t        self,\n\t        batchId: str = None,\n\t        datasetId: str = None,\n", "        format: str = \"json\",\n\t        delimiter: str = \",\",\n\t        quote: str = '\"',\n\t        escape: str = \"\\\\\",\n\t        charset: str = \"utf-8\",\n\t        header: bool = True,\n\t        nrow: int = 5,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Generates a data preview for the files uploaded to the batch so far. The preview can be generated for all the batch datasets collectively or for the selected datasets.\n", "        Arguments:\n\t            batchId : REQUIRED : The batchId referencing the batch processed created beforehand.\n\t            datasetId : REQUIRED : The dataSetId related to where the data are ingested to.\n\t            format : REQUIRED : Format of the file ('json' default)\n\t            delimiter : OPTIONAL : The delimiter to use for parsing column values.\n\t            quote : OPTIONAL : The quote value to use while parsing data.\n\t            escape : OPTIONAL : The escape character to use while parsing data.\n\t            charset : OPTIONAL : The encoding to be used (default utf-8)\n\t            header : OPTIONAL : The flag to indicate if the header is supplied in the dataset files.\n\t            nrow : OPTIONAL : The number of rows to parse. (default 5) - cannot be 10 or greater\n", "        \"\"\"\n\t        if batchId is None:\n\t            raise Exception(\"require a batchId\")\n\t        if datasetId is None:\n\t            raise Exception(\"require a dataSetId\")\n\t        if format is None:\n\t            raise Exception(\"require a format type\")\n\t        params = {\n\t            \"delimiter\": delimiter,\n\t            \"quote\": quote,\n", "            \"escape\": escape,\n\t            \"charset\": charset,\n\t            \"header\": header,\n\t            \"nrow\": nrow,\n\t        }\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"getPreviewBatchDataset for dataset ID: ({datasetId})\")\n\t        path = f\"/batches/{batchId}/datasets/{datasetId}/preview\"\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        return res\n", "    def streamMessage(\n\t        self,\n\t        inletId: str = None,\n\t        data: dict = None,\n\t        flowId: str = None,\n\t        syncValidation: bool = False,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Send a dictionary to the connection for streaming ingestion.\n\t        Arguments:\n", "            inletId : REQUIRED : the connection ID to be used for ingestion\n\t            data : REQUIRED : The data that you want to ingest to Platform.\n\t            flowId : OPTIONAL : The flow ID for the stream inlet.\n\t            syncValidation : OPTIONAL : An optional query parameter, intended for development purposes.\n\t                If set to true, it can be used for immediate feedback to determine if the request was successfully sent.\n\t        \"\"\"\n\t        privateHeader = deepcopy(self.header)\n\t        if inletId is None:\n\t            raise Exception(\"Require a connectionId to be present\")\n\t        if data is None and type(data) != dict:\n", "            raise Exception(\"Require a dictionary to be send for ingestion\")\n\t        if flowId is not None:\n\t            privateHeader['x-adobe-flow-id'] = flowId\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting Streaming single message\")\n\t        params = {\"syncValidation\": syncValidation}\n\t        path = f\"/collection/{inletId}\"\n\t        res = self.connector.postData(\n\t            self.endpoint_streaming + path, data=data, params=params, headers=privateHeader\n\t        )\n", "        return res\n\t    def streamMessages(\n\t        self,\n\t        inletId: str = None,\n\t        data: list = None,\n\t        flowId: str = None,\n\t        syncValidation: bool = False,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Send a dictionary to the connection for streaming ingestion.\n", "        Arguments:\n\t            inletId : REQUIRED : the connection ID to be used for ingestion\n\t            data : REQUIRED : The list of data that you want to ingest to Platform.\n\t            flowId : OPTIONAL : The flow ID for the stream inlet.\n\t            syncValidation : OPTIONAL : An optional query parameter, intended for development purposes.\n\t                If set to true, it can be used for immediate feedback to determine if the request was successfully sent.\n\t        \"\"\"\n\t        privateHeader = deepcopy(self.header)\n\t        if inletId is None:\n\t            raise Exception(\"Require a connectionId to be present\")\n", "        if data is None and type(data) != list:\n\t            raise Exception(\"Require a list of dictionary to be send for ingestion\")\n\t        if flowId is not None:\n\t            privateHeader['x-adobe-flow-id'] = flowId\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting Streaming multiple messages\")\n\t        params = {\"syncValidation\": syncValidation}\n\t        data = {\"messages\": data}\n\t        path = f\"/collection/batch/{inletId}\"\n\t        res = self.connector.postData(\n", "            self.endpoint_streaming + path, data=data, params=params, headers=privateHeader\n\t        )\n\t        return res\n"]}
{"filename": "aepp/customerprofile.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "import aepp\n\tfrom aepp import connector\n\tfrom copy import deepcopy\n\timport pandas as pd\n\timport logging\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass Profile:\n\t    \"\"\"\n\t    A class containing the different methods exposed on Customer Profile API.\n", "    The API documentation is available:\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/real-time-customer-profile.yaml\n\t    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n", "        loggingObject: dict = None,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n\t            This will instantiate the profile class\n\t            Arguments:\n\t                config : OPTIONAL : config object in the config module.\n\t                header : OPTIONAL : header object  in the config module.\n\t                loggingObject : OPTIONAL : logging object to log messages.\n\t        kwargs:\n", "            kwargsvaluewillupdatetheheader\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n", "            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n", "        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n", "        )\n\t        self.header = self.connector.header\n\t        self.header[\"Accept\"] = \"application/vnd.adobe.xdm+json\"\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n", "            self.sandbox = self.connector.config[\"sandbox\"]\n\t        # same endpoint than segmentation\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"segmentation\"]\n\t        )\n\t    def getEntity(\n\t        self,\n\t        schema_name: str = \"_xdm.context.profile\",\n\t        entityId: str = None,\n\t        entityIdNS: str = None,\n", "        mergePoliciyId: str = None,\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns an entity by ID or Namespace.\n\t        Arguments:\n\t            schema_name : REQUIRED : class name of the schema to be retrieved. default : _xdm.context.profile\n\t            entityId : OPTIONAL : identity ID\n\t            entityIdNS : OPTIONAL : Identity Namespace code. Required if entityId is used (except for native identity)\n\t            mergePoliciyId : OPTIONAL : Id of the merge policy.\n", "        Possible kwargs:\n\t            fields : path of the elements to be retrieved, separated by comma. Ex : \"person.name.firstName,person.name.lastName\"\n\t            relatedSchema_name : If schema.name is \"_xdm.context.experienceevent\", this value must specify the schema for the profile entity that the time series events are related to.\n\t            relatedEntityId : ID of the entity that the ExperienceEvents are associated with. Used when looking up ExperienceEvents. For Native XID lookup, use relatedEntityId=<XID> and leave relatedEntityIdNS absent;\n\t            For ID:NS lookup, use both relatedEntityId and relatedEntityIdNS fields.\n\t            relatedEntityIdNS : Identity Namespace code of the related entity ID of ExperienceEvent. Used when looking up ExperienceEvents. If this field is used, entityId cannot be empty.\n\t            startTime : Start time of Time range filter for ExperienceEvents. Should be at millisecond granularity. Included. Default: From beginning.\n\t            endTime : End time of Time range filter for ExperienceEvents. Should be at millisecond granularity. Excluded. Default: To the end.\n\t            limit : Number of records to return from the result. Only for time-series objects. Default: 1000\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getEntity\")\n\t        path = \"/access/entities\"\n\t        params = {\n\t            \"schema.name\": schema_name,\n\t            \"entityId\": entityId,\n\t            \"entityIdNS\": entityIdNS,\n\t            \"mergePoliciyId\": mergePoliciyId,\n\t        }\n\t        if schema_name == \"_xdm.context.experienceevent\":\n", "            params[\"relatedSchema.name\"] = kwargs.get(\n\t                \"relatedSchema_name\", \"_xdm.context.profile\"\n\t            )\n\t            params[\"relatedEntityId\"] = kwargs.get(\"relatedEntityId\", entityId)\n\t            params[\"relatedEntityIdNS\"] = kwargs.get(\"relatedEntityIdNS\", None)\n\t            params[\"limit\"] = kwargs.get(\"limit\", 1000)\n\t            params[\"startTime\"] = kwargs.get(\"startTime\", None)\n\t            params[\"endTime\"] = kwargs.get(\"endTime\", None)\n\t        params[\"fields\"] = kwargs.get(\"fields\", None)\n\t        res = self.connector.getData(\n", "            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        return res\n\t    def getEntities(self, request_data: dict = None) -> dict:\n\t        \"\"\"\n\t        Get a number of different identities from ID or namespaces.\n\t        Argument:\n\t            request_data : Required : A dictionary that contains fields for the search.\n\t            Example\n\t            {\n", "                \"schema\": {\n\t                    \"name\": \"_xdm.context.profile\"\n\t                },\n\t                \"relatedSchema\": {\n\t                    \"name\": \"_xdm.context.profile\"\n\t                },\n\t                \"fields\": [\n\t                    \"person.name.firstName\"\n\t                ],\n\t                \"identities\": [\n", "                    {\n\t                    \"entityId\": \"69935279872410346619186588147492736556\",\n\t                    \"entityIdNS\": {\n\t                        \"code\": \"CRMID\"\n\t                        }\n\t                    },\n\t                    ,\n\t                {\n\t                    \"entityId\":\"89149270342662559642753730269986316900\",\n\t                    \"entityIdNS\":{\n", "                        \"code\":\"ECID\"\n\t                        }\n\t                    ],\n\t                \"timeFilter\": {\n\t                    \"startTime\": 1539838505,\n\t                    \"endTime\": 1539838510\n\t                },\n\t                \"limit\": 10,\n\t                \"orderby\": \"-timestamp\",\n\t                \"withCA\": True\n", "            }\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getEntities\")\n\t        path = \"/access/entities\"\n\t        if request_data is None or type(request_data) != dict:\n\t            raise Exception(\"Expected a dictionary to fetch entities\")\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=request_data, headers=self.header\n\t        )\n", "        return res\n\t    def deleteEntity(\n\t        self, schema_name: str = None, entityId: str = None, entityIdNS: str = None\n\t    ) -> str:\n\t        \"\"\"\n\t        Delete a specific entity\n\t        Arguments:\n\t            schema_name : REQUIRED : Name of the associated XDM schema.\n\t            entityId : REQUIRED : entity ID\n\t            entityIdNS : OPTIONAL : entity ID Namespace\n", "        \"\"\"\n\t        path = \"/access/entities\"\n\t        params = {}\n\t        if schema_name is None:\n\t            raise Exception(\"Expected a schema name\")\n\t        else:\n\t            params[\"schema.name\"] = schema_name\n\t        if entityId is not None:\n\t            params[\"entityId\"] = entityId\n\t        if entityIdNS is not None:\n", "            params[\"entityIdNS\"] = entityIdNS\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteEntity\")\n\t        res = self.connector.deleteData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        return res\n\t    def createExportJob(self,\n\t            exportDefinition:dict=None,\n\t            fields:list=None,\n", "            mergePolicyId:str=None,\n\t            additionalFields:dict=None,\n\t            datasetId:str=None,\n\t            schemaName:str=\"_xdm.context.profile\",\n\t            segmentPerBatch:bool=False,\n\t            )->dict:\n\t        \"\"\"\n\t        Create an export of the profile information of the user.\n\t        You can pass directly the payload or you can pass different arguments to create that export job.\n\t        Documentation: https://experienceleague.adobe.com/docs/experience-platform/profile/api/export-jobs.html?lang=en\n", "        Arguments: \n\t            exportDefinition : OPTIONAL : The complete definition of the export\n\t        If not provided, use the following parameters:\n\t            fields : OPTIONAL : Limits the data fields to be included in the export to only those provided in this parameter. \n\t                    Omitting this value will result in all fields being included in the exported data.\n\t            mergePolicyId : OPTIONAL : MergePolicyId to use for data combination.\n\t            additionalFields : OPTIONAL : Controls the time-series event fields exported for child or associated objects by providing one or more of the following settings:\n\t            datasetId : OPTIONAL : the datasetId to be used for the export.\n\t            schemaName : OPTIONAL : Schema associated with the dataset.\n\t            segmentPerBatch : OPTIONAL : A Boolean value that, if not provided, defaults to false. A value of false exports all segment IDs into a single batch ID. A value of true exports one segment ID into one batch ID.\n", "        \"\"\"\n\t        if exportDefinition is not None and type(exportDefinition) == dict:\n\t            data = exportDefinition\n\t        elif fields is not None and mergePolicyId is not None and datasetId is not None and schemaName is not None:\n\t            data = {}\n\t            data['fields'] = ','.join(fields)\n\t            data['mergePolicy'] = {\n\t                \"id\": mergePolicyId,\n\t                \"version\": 1\n\t            }\n", "            data['destination'] = {\n\t                \"datasetId\": datasetId,\n\t                \"segmentPerBatch\": segmentPerBatch\n\t            }\n\t            data['schema'] = {\n\t                \"name\": schemaName,\n\t            }\n\t            if additionalFields is not None:\n\t                data['additionalFields'] = additionalFields\n\t        else:\n", "            raise ValueError(\"Require the definition or arguments to be used\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createExportJob\")\n\t        path = f\"/export/jobs\"\n\t        res = self.connector.postData(self.endpoint + path, data=data)\n\t        return res\n\t    def getExportJobs(self,limit:int=100)->dict:\n\t        \"\"\"\n\t        Returns all export jobs\n\t        Arguments:\n", "            limit : OPTIONAL : Number of jobs to return\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getExportJobs\")\n\t        path = \"/export/jobs/\"\n\t        params = {\"limit\":limit}\n\t        res = self.connector.getData(self.endpoint+path,params=params)\n\t        return res\n\t    def getExportJob(self, exportId:str=None)->dict:\n\t        \"\"\"\n", "        Returns an export job status\n\t        Arguments:\n\t            exportId : OPTIONAL : The id of the export job you want to access.\n\t        \"\"\"\n\t        if exportId is None:\n\t            raise ValueError(\"Require an export ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getExportJob with ID: {exportId}\")\n\t        path = f\"/export/jobs/{exportId}\"\n\t        res = self.connector.getData(self.endpoint+path)\n", "        return res\n\t    def deleteExportJob(self, exportId:str=None)->dict:\n\t        \"\"\"\n\t        Delete an export job\n\t        Arguments:\n\t            exportId : OPTIONAL : The id of the export job you want to delete.\n\t        \"\"\"\n\t        if exportId is None:\n\t            raise ValueError(\"Require an export ID\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting deleteExportJob with ID: {exportId}\")\n\t        path = f\"/export/jobs/{exportId}\"\n\t        res = self.connector.deleteData(self.endpoint+path)\n\t        return res\n\t    def getMergePolicies(self, limit: int = 100) -> dict:\n\t        \"\"\"\n\t        Returns the list of merge policies hosted in this instance.\n\t        Arguments:\n\t            limit : OPTIONAL : amount of merge policies returned by pages.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMergePolicies\")\n\t        path = \"/config/mergePolicies\"\n\t        params = {\"limit\": limit}\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        data = res[\"children\"]\n\t        nextPage = res[\"_links\"][\"next\"].get(\"href\", \"\")\n\t        while nextPage != \"\":\n", "            path = \"/config/mergePolicies?\" + nextPage.split(\"?\")[1]\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n\t            )\n\t            data += res[\"children\"]\n\t            nextPage = res[\"_links\"][\"next\"].get(\"href\", \"\")\n\t        return data\n\t    def getMergePolicy(self, policy_id: str = None) -> dict:\n\t        \"\"\"\n\t        Return a specific merge policy.\n", "        Arguments:\n\t            policy_id : REQUIRED : id of the the policy id to be returned.\n\t        \"\"\"\n\t        if policy_id is None:\n\t            raise Exception(\"Missing the policy id\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMergePolicy\")\n\t        path = f\"/config/mergePolicies/{policy_id}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n", "    def createMergePolicy(self, policy: dict = None) -> dict:\n\t        \"\"\"\n\t        Arguments:\n\t            policy: REQUIRED : The dictionary defining the policy\n\t        Refer to the documentation : https://experienceleague.adobe.com/docs/experience-platform/profile/api/merge-policies.html\n\t        Example of JSON:\n\t        {\n\t            \"name\": \"real-time-customer-profile-default\",\n\t            \"imsOrgId\": \"1BD6382559DF0C130A49422D@AdobeOrg\",\n\t            \"schema\": {\n", "                \"name\": \"_xdm.context.profile\"\n\t            },\n\t            \"default\": False,\n\t            \"identityGraph\": {\n\t                \"type\": \"pdg\"\n\t            },\n\t            \"attributeMerge\": {\n\t                \"type\": \"timestampOrdered\",\n\t                \"order\": [\n\t                \"string\"\n", "                ]\n\t            },\n\t            \"updateEpoch\": 1234567890\n\t        }\n\t        \"\"\"\n\t        path = \"/config/mergePolicies\"\n\t        if policy is None:\n\t            raise ValueError(\"Require a dictionary\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createMergePolicy\")\n", "        res = self.connector.postData(\n\t            self.endpoint + path, data=policy, headers=self.header\n\t        )\n\t        return res\n\t    def updateMergePolicy(self, mergePolicyId: str = None, policy: dict = None) -> dict:\n\t        \"\"\"\n\t        Update a merge policy by replacing its definition. (PUT method)\n\t        Arguments:\n\t            mergePolicyId : REQUIRED : The merge Policy Id\n\t            policy : REQUIRED : a dictionary giving the definition of the merge policy\n", "            Refer to the documentation : https://experienceleague.adobe.com/docs/experience-platform/profile/api/merge-policies.html\n\t        Example of JSON:\n\t        {\n\t            \"name\": \"real-time-customer-profile-default\",\n\t            \"imsOrgId\": \"1BD6382559DF0C130A49422D@AdobeOrg\",\n\t            \"schema\": {\n\t                \"name\": \"_xdm.context.profile\"\n\t            },\n\t            \"default\": False,\n\t            \"identityGraph\": {\n", "                \"type\": \"pdg\"\n\t            },\n\t            \"attributeMerge\": {\n\t                \"type\": \"timestampOrdered\",\n\t                \"order\": [\n\t                \"string\"\n\t                ]\n\t            },\n\t            \"updateEpoch\": 1234567890\n\t        }\n", "        attributeMerge can be \"dataSetPrecedence\" or \"timestampOrdered\". Please provide a list of dataSetId on \"order\" when using the first option.\n\t        \"\"\"\n\t        if mergePolicyId is None:\n\t            raise ValueError(\"Require a mergePolicyId\")\n\t        if policy is None or type(policy) != dict:\n\t            raise ValueError(\"Require a dictionary to update the merge policy\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateMergePolicy\")\n\t        path = f\"/config/mergePolicies/{mergePolicyId}\"\n\t        res = self.connector.putData(\n", "            self.endpoint + path, data=policy, headers=self.header\n\t        )\n\t        return res\n\t    def patchMergePolicy(\n\t        self, mergePolicyId: str = None, operations: list = None\n\t    ) -> str:\n\t        \"\"\"\n\t        Update a merge policy by replacing its definition.\n\t        Arguments:\n\t            mergePolicyId : REQUIRED : The merge Policy Id\n", "            operations : REQUIRED : a list of operations to realize on the merge policy\n\t            Refer to the documentation : https://experienceleague.adobe.com/docs/experience-platform/profile/api/merge-policies.html\n\t        Example of operation:\n\t        [\n\t            {\n\t                \"op\": \"add\",\n\t                \"path\": \"/identityGraph.type\",\n\t                \"value\": \"pdg\"\n\t            }\n\t        ]\n", "        \"\"\"\n\t        if mergePolicyId is None:\n\t            raise ValueError(\"Require a mergePolicyId\")\n\t        if operations is None or type(operations) != list:\n\t            raise ValueError(\"Require a dictionary to update the merge policy\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchMergePolicy\")\n\t        path = f\"/config/mergePolicies/{mergePolicyId}\"\n\t        res = self.connector.patchData(\n\t            self.endpoint + path, data=operations, headers=self.header\n", "        )\n\t        return res\n\t    def deleteMergePolicy(self, mergePolicyId: str = None) -> str:\n\t        \"\"\"\n\t        Delete a merge policy by its ID.\n\t        Arguments:\n\t            mergePolicyId : REQUIRED : The merge Policy to be deleted\n\t        \"\"\"\n\t        if mergePolicyId is None:\n\t            raise ValueError(\"Require a mergePolicyId\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteMergePolicy\")\n\t        path = f\"/config/mergePolicies/{mergePolicyId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getPreviewStatus(self) -> dict:\n\t        \"\"\"\n\t        View the details for the last successful sample job that was run for the IMS Organization.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getPreviewStatus\")\n\t        path = \"/previewsamplestatus\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = \"application/json\"\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        return res\n\t    def getPreviewDataSet(self, date: str = None, output: str = \"raw\") -> dict:\n\t        \"\"\"\n\t        View a report showing the distribution of profiles by dataset.\n\t        Arguments:\n", "            date : OPTIONAL : Format: YYYY-MM-DD.\n\t                If multiple reports were run on the date, the most recent report for that date will be returned.\n\t                If a report does not exist for the specified date, a 404 error will be returned.\n\t                If no date is specified, the most recent report will be returned.\n\t                Example: date=2024-12-31\n\t            output : OPTIONAL : if you want to have a dataframe returns. Use \"df\", default \"raw\"\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPreviewDataSet\")\n\t        path = \"/previewsamplestatus/report/dataset\"\n", "        params = {}\n\t        if date is not None:\n\t            params[\"date\"] = date\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = \"application/json\"\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if output == \"df\":\n\t            df = pd.DataFrame(res[\"data\"])\n\t            return df\n\t        return res\n", "    def getPreviewDataSetOverlap(self, date: str = None, output: str = \"raw\") -> dict:\n\t        \"\"\"\n\t        Method to find the overlap of identities with datasets.\n\t        Arguments:\n\t            date : OPTIONAL : Format: YYYY-MM-DD.\n\t                If multiple reports were run on the date, the most recent report for that date will be returned.\n\t                If a report does not exist for the specified date, a 404 error will be returned.\n\t                If no date is specified, the most recent report will be returned.\n\t                Example: date=2024-12-31\n\t            output : OPTIONAL : if you want to have a dataframe returns. Use \"df\", default \"raw\"\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPreviewDataSetOverlap\")\n\t        path = \"/previewsamplestatus/report/dataset/overlap\"\n\t        params = {}\n\t        if date is not None:\n\t            params[\"date\"] = date\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = \"application/json\"\n\t        privateHeader[\"x-model-name\"] = \"_xdm.context.profile\"\n", "        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if output == \"df\":\n\t            df = pd.DataFrame(res[\"data\"])\n\t            return df\n\t        return res\n\t    def getPreviewNamespace(self, date: str = None, output: str = \"raw\") -> dict:\n\t        \"\"\"\n\t        View a report showing the distribution of profiles by namespace.\n\t        Arguments:\n\t            date : OPTIONAL : Format: YYYY-MM-DD.\n", "                If multiple reports were run on the date, the most recent report for that date will be returned.\n\t                If a report does not exist for the specified date, a 404 error will be returned.\n\t                If no date is specified, the most recent report will be returned.\n\t                Example: date=2024-12-31\n\t            output : OPTIONAL : if you want to have a dataframe returns. Use \"df\", default \"raw\"\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPreviewNamespace\")\n\t        path = \"/previewsamplestatus/report/namespace\"\n\t        params = {}\n", "        if date is not None:\n\t            params[\"date\"] = date\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"Accept\"] = \"application/json\"\n\t        res = self.connector.getData(self.endpoint + path, headers=privateHeader)\n\t        if output == \"df\":\n\t            df = pd.DataFrame(res[\"data\"])\n\t            return df\n\t        return res\n\t    def createDeleteSystemJob(self, dataSetId: str = None, batchId: str = None) -> dict:\n", "        \"\"\"\n\t        Delete all the data for a batch or a dataSet based on their ids.\n\t        Note: you cannot delete batch from record type dataset. You can overwrite them to correct the issue.\n\t        Only Time Series and record type datasets can be deleted.\n\t        Arguments:\n\t            dataSetId : REQUIRED : dataSetId to be deleted\n\t            batchId : REQUIRED : batchId to be deleted.\n\t        More info: https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Profile_System_Jobs/createDeleteRequest\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting createDeleteSystemJob\")\n\t        path = \"/system/jobs\"\n\t        if dataSetId is not None:\n\t            obj = {\"dataSetId\": dataSetId}\n\t            res = self.connector.postData(self.endpoint + path, data=obj)\n\t            return res\n\t        elif batchId is not None:\n\t            obj = {\"batchId\": batchId}\n\t            res = self.connector.postData(self.endpoint + path, data=obj)\n\t            return res\n", "        else:\n\t            raise ValueError(\"Require a dataSetId or a batchId\")\n\t    def getDeleteSystemJobs(\n\t        self, page: int = 0, limit: int = 100, n_results: int = 100\n\t    ) -> dict:\n\t        \"\"\"\n\t        Retrieve a list of all delete requests (Profile System Jobs) created by your organization.\n\t        Arguments:\n\t            page : OPTIONAL : Return a specific page of results, as per the create time of the request. For example, page=0\n\t            limit : OPTIONAL : Limit response to a specific number of objects. Must be a positive number. For example, limit=10\n", "            n_results : OPTIONAL : Number of total result to retrieve.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDeleteSystemJobs\")\n\t        path = \"/system/jobs\"\n\t        params = {\"page\": page, \"limit\": limit}\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        data = res[\"children\"]\n\t        count = len(data)\n\t        nextPage = res[\"_page\"].get(\"next\", \"\")\n", "        while nextPage != \"\" and count < n_results:\n\t            page += 1\n\t            params = {\"page\": page, \"limit\": limit}\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n\t            )\n\t            count += len(res[\"children\"])\n\t            data += res[\"children\"]\n\t            nextPage = res[\"_page\"].get(\"next\", \"\")\n\t        return data\n", "    def getDeleteSystemJob(self, jobId: str = None) -> dict:\n\t        \"\"\"\n\t        Get a specific delete system job by its ID.\n\t        Arguments:\n\t            jobId : REQUIRED : Job ID to be retrieved.\n\t        \"\"\"\n\t        if jobId is None:\n\t            raise ValueError(\"Require a system Job ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDeleteSystemJob\")\n", "        path = f\"/system/jobs/{jobId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def deleteDeleteSystemJob(self, jobId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific delete system job by its ID.\n\t        Arguments:\n\t            jobId : REQUIRED : Job ID to be deleted.\n\t        \"\"\"\n\t        if jobId is None:\n", "            raise ValueError(\"Require a system Job ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteDeleteSystemJob\")\n\t        path = f\"/system/jobs/{jobId}\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def getComputedAttributes(self) -> list:\n\t        \"\"\"\n\t        Retrieve the list of computed attributes set up in your organization.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getComputedAttributes\")\n\t        path = \"/config/computedAttributes\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        data = res[\"children\"]\n\t        nextPage = res[\"_page\"].get(\"next\", \"\")\n\t        # while nextPage != \"\":\n\t        #     res = self.connector.getData(self.endpoint+path,\n\t        #                     params=params, headers=self.header)\n\t        #     data += res['children']\n", "        #     nextPage = res['_page'].get('next','')\n\t        return res\n\t    def getComputedAttribute(self, attributeId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns a single computed attribute.\n\t        Arguments:\n\t            attributeId : REQUIRED : The computed attribute ID.\n\t        \"\"\"\n\t        if attributeId is None:\n\t            raise ValueError(\"Require a computed attribute ID\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getComputedAttribute\")\n\t        path = f\"/config/computedAttributes/{attributeId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteComputedAttribute(self, attributeId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific computed attribute.\n\t        Arguments:\n\t            attributeId : REQUIRED : The computed attribute ID to be deleted.\n", "        \"\"\"\n\t        if attributeId is None:\n\t            raise ValueError(\"Require a computed attribute ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteComputedAttributes\")\n\t        path = f\"/config/computedAttributes/{attributeId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getDestinations(self) -> dict:\n\t        \"\"\"\n", "        Retrieve a list of edge projection destinations. The latest definitions are returned.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDestinations\")\n\t        path = \"/config/destinations\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def createDestination(self, destinationConfig: dict = None) -> dict:\n\t        \"\"\"\n\t        Create a new edge projection destination. Assume that there is time between creation and propagation of this information to the edge.\n", "        Arguments:\n\t            destinationConfig : REQUIRED : the destination configuration\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDestination\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\n\t            \"Content-Type\"\n\t        ] = \"application/vnd.adobe.platform.projectionDestination+json; version=1\"\n\t        if destinationConfig is None:\n", "            raise ValueError(\"Require a destination configuration object\")\n\t        path = \"/config/destinations\"\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=destinationConfig, headers=privateHeader\n\t        )\n\t        return res\n\t    def getDestination(self, destinationId: str = None) -> dict:\n\t        \"\"\"\n\t        Get a specific destination based on its ID.\n\t        Arguments:\n", "            destinationId : REQUIRED : The destination ID to be retrieved\n\t        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDestination\")\n\t        path = f\"/config/destinations/{destinationId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteDestination(self, destinationId: str = None) -> dict:\n", "        \"\"\"\n\t        Delete a specific destination based on its ID.\n\t        Arguments:\n\t            destinationId : REQUIRED : The destination ID to be deleted\n\t        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteDestination\")\n\t        path = f\"/config/destinations/{destinationId}\"\n", "        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def updateDestination(\n\t        self, destinationId: str = None, destinationConfig: dict = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Update a specific destination based on its ID. (PUT request)\n\t        Arguments:\n\t            destinationId : REQUIRED : The destination ID to be updated\n\t            destinationConfig : REQUIRED : the destination config object to replace the old one.\n", "        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        if destinationConfig is None:\n\t            raise ValueError(\"Require a dictionation for updating the destination\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateDestination\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\n\t            \"Content-Type\"\n", "        ] = \"application/vnd.adobe.platform.projectionDestination+json\"\n\t        path = f\"/config/destinations/{destinationId}\"\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=destinationConfig, headers=privateHeader\n\t        )\n\t        return res\n\t    def getProjections(self, schemaName: str = None, name: str = None) -> dict:\n\t        \"\"\"\n\t        Retrieve a list of edge projection configurations. The latest definitions are returned.\n\t        Arguments:\n", "            schemaName : OPTIONAL : The name of the schema class associated with the projection configuration you want to access.\n\t                example : _xdm.context.profile\n\t            name : OPTIONAL : The name of the projection configuration you want to access.\n\t                if name is specified, schemaName is also required.\n\t        \"\"\"\n\t        path = \"/config/projections\"\n\t        params = {}\n\t        if name is not None and schemaName is None:\n\t            raise AttributeError(\n\t                \"You must specify a schema name when setting a projection name\"\n", "            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getProjections\")\n\t        if schemaName is not None:\n\t            params[\"schemaName\"] = schemaName\n\t        if name is not None:\n\t            params[\"name\"] = name\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.headers\n\t        )\n", "        return res\n\t    def getProjection(self, projectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Retrieve a single projection based on its ID.\n\t        Arguments:\n\t            projectionId : REQUIRED : the projection ID to be retrieved.\n\t        \"\"\"\n\t        if projectionId is None:\n\t            raise ValueError(\"Require a projection ID\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getProjection\")\n\t        path = f\"/config/projections/{projectionId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteProjection(self, projectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a single projection based on its ID.\n\t        Arguments:\n\t            projectionId : REQUIRED : the projection ID to be deleted.\n\t        \"\"\"\n", "        if projectionId is None:\n\t            raise ValueError(\"Require a projection ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteProjection\")\n\t        path = f\"/config/projections/{projectionId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createProjection(\n\t        self, schemaName: str = None, projectionConfig: dict = None\n\t    ) -> dict:\n", "        \"\"\"\n\t        Create a projection\n\t        Arguments:\n\t            schemaName : REQUIRED : XDM schema namess\n\t            projectionConfig : REQUIRED : the object definiing the projection\n\t        \"\"\"\n\t        if schemaName is None:\n\t            raise ValueError(\"Require a schema name specified\")\n\t        if projectionConfig is None:\n\t            raise ValueError(\"Require a projection configuration\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createProjection\")\n\t        path = \"/config/projections\"\n\t        params = {\"schemaName\": schemaName}\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\n\t            \"Content-Type\"\n\t        ] = \"application/vnd.adobe.platform.projectionConfig+json; version=1\"\n\t        res = self.connector.postData(\n\t            self.endpoint + path,\n", "            params=params,\n\t            data=projectionConfig,\n\t            privateHeader=privateHeader,\n\t        )\n\t        return res\n\t    def updateProjection(\n\t        self, projectionId: str = None, projectionConfig: dict = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Update a projection based on its ID.(PUT request)\n", "        Arguments:\n\t            projectionId : REQUIRED : The ID of the projection to be updated.\n\t            projectionConfig : REQUIRED : the object definiing the projection\n\t        \"\"\"\n\t        if projectionId is None:\n\t            raise ValueError(\"Require a projectionId\")\n\t        if projectionConfig is None:\n\t            raise ValueError(\"Require a projection Configuration object\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateProjection\")\n", "        privateHeader = deepcopy(self.header)\n\t        privateHeader[\n\t            \"Content-Type\"\n\t        ] = \"application/vnd.adobe.platform.projectionConfig+json\"\n\t        path = f\"/config/projections/{projectionId}\"\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=projectionConfig, headers=privateHeader\n\t        )\n\t        return res\n"]}
{"filename": "aepp/queryservice.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "from aepp import connector\n\timport pandas as pd\n\tfrom typing import Union\n\timport re\n\timport aepp\n\timport time\n\timport logging\n\tfrom .configs import ConnectObject\n\tclass QueryService:\n\t    \"\"\"\n", "    QueryService class is to be used in order to generate queries, scheduled queries or retrieve past queries.\n\t    \"\"\"\n\t    QUERYSAMPLE = {\n\t        \"dbName\": \"string\",\n\t        \"sql\": \"SELECT $key from $key1 where $key > $key2;\",\n\t        \"queryParameters\": {\"key\": \"value\", \"key1\": \"value1\", \"key2\": \"value2\"},\n\t        \"templateId\": \"123\",\n\t        \"name\": \"string\",\n\t        \"description\": \"powered by aepp\",\n\t        \"insertIntoParameters\": {\"datasetName\": \"string\"},\n", "        \"ctasParameters\": {\n\t            \"datasetName\": \"myDatasetName\",\n\t            \"description\": \"powered by aepp\",\n\t            \"targetSchemaTitle\": \"mySchemaName\",\n\t        },\n\t    }\n\t    SCHEDULESAMPLE = {\n\t        \"query\": {\n\t            \"dbName\": \"string\",\n\t            \"sql\": \"SELECT $key from $key1 where $key > $key2;\",\n", "            \"queryParameters\": {\"key\": \"value\", \"key1\": \"value1\", \"key2\": \"value2\"},\n\t            \"templateId\": \"123\",\n\t            \"name\": \"string\",\n\t            \"description\": \"string\",\n\t            \"insertIntoParameters\": {\"datasetName\": \"string\"},\n\t            \"ctasParameters\": {\n\t                \"datasetName\": \"string\",\n\t                \"description\": \"string\",\n\t                \"targetSchemaTitle\": \"mySchemaName\",\n\t            },\n", "        },\n\t        \"schedule\": {\n\t            \"schedule\": \"string\",\n\t            \"startDate\": \"string\",\n\t            \"endDate\": \"string\",\n\t            \"maxActiveRuns\": 0,\n\t        },\n\t    }\n\t    TEMPLATESAMPLE = {\n\t        \"sql\": \"SELECT $key from $key1 where $key > $key2;\",\n", "        \"queryParameters\": {\"key\": \"value\", \"key1\": \"value1\", \"key2\": \"value2\"},\n\t        \"name\": \"string\",\n\t    }\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header:dict = aepp.config.header,\n", "        loggingObject: dict = None,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Instanciate the class for Query Service call.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t            loggingObject : OPTIONAL : If you want to set logging capability for your actions.\n\t        kwargs:\n", "            kwargs will update the header\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n", "            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n", "        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n", "        )\n\t        self.header = self.connector.header\n\t        # self.header.update({\"Accept\": \"application/json\"})\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n", "            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"query\"]\n\t    def getResource(\n\t        self,\n\t        endpoint: str = None,\n\t        params: dict = None,\n\t        format: str = \"json\",\n\t        save: bool = False,\n\t        **kwargs,\n\t    ) -> dict:\n", "        \"\"\"\n\t        Template for requesting data with a GET method.\n\t        Arguments:\n\t            endpoint : REQUIRED : The URL to GET\n\t            params: OPTIONAL : dictionary of the params to fetch\n\t            format : OPTIONAL : Type of response returned. Possible values:\n\t                json : default\n\t                txt : text file\n\t                raw : a response object from the requests module\n\t        \"\"\"\n", "        if endpoint is None:\n\t            raise ValueError(\"Require an endpoint\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getResource\")\n\t        res = self.connector.getData(endpoint, params=params, format=format)\n\t        if save:\n\t            if format == \"json\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n", "                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"json\",\n\t                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n\t            elif format == \"txt\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"txt\",\n", "                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n\t            else:\n\t                print(\n\t                    \"element is an object. Output is unclear. No save made.\\nPlease save this element manually\"\n\t                )\n\t        return res\n\t    def connection(self) -> dict:\n\t        \"\"\"\n\t        Create a connection for interactive interface.\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Getting a connection\")\n\t        path = \"/connection_parameters\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def getQueries(\n\t        self,\n\t        orderby: str = None,\n\t        limit: int = 1000,\n", "        start: int = None,\n\t        n_results: int = 1000,\n\t        property: str = None,\n\t        **kwargs,\n\t    ) -> list:\n\t        \"\"\"\n\t        Retrieve the queries from your organization.\n\t        Arguments:\n\t            orderby : OPTIONAL : possibility to order by \"created\" or \"updated\".\n\t            prepend with \"+\" for ASC and \"-\" for DESC. To be escaped (default: -)\n", "            limit : OPTIONAL : number of of records to fetch per page. (default 1000)\n\t            start : OPTIONAL : when to start (depending on your orderby)\n\t            property : OPTIONAL : Comma-separated filters. List of properties that allow filtering with all operators:\n\t                        \"created\"\n\t                        \"updated\"\n\t                        \"state\"\n\t                        \"id\"\n\t                    with following operators >, <, >=, <=, ==, !=, ~\n\t                \"referenced_datasets\", \"scheduleId\", \"templateId\", and \"userId\" can only be used with ==. Multiple datasets ID with \"||\"\n\t                \"SQL\" can only be used with ~ (WITHOUT COMMA)\n", "            n_results : OPTIONAL : total number of results returned (default 1000, set to \"inf\" to retrieve everything)\n\t        possible kwargs:\n\t            excludeSoftDeleted: Whether to include any queries that have been soft deleted. Defaults to true.\n\t            excludeHidden : Whether to include any queries that have been found to be not interesting, as they were not user driven. Examples include CURSOR definitions, FETCH, and Metadata queries. Defaults to true.\n\t            isPrevLink : Field indicating if the URI is a previous link.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getQueries\")\n\t        path = \"/queries\"\n\t        arguments = {\"limit\": 1000}\n", "        n_results = float(n_results)\n\t        if orderby is not None:\n\t            if orderby == \"+\":\n\t                orderby = \"%2B\"\n\t            arguments[\"orderby\"] = orderby\n\t        if start is not None:\n\t            arguments[\"start\"] = start\n\t        if limit is not None:\n\t            arguments[\"limit\"] = limit\n\t        if property is not None:\n", "            arguments[\"property\"] = property\n\t        if len(kwargs.keys()) > 0:\n\t            arguments[\"excludeSoftDeleted\"] = kwargs.get(\"excludeSoftDeleted\", True)\n\t            arguments[\"excludeHidden\"] = kwargs.get(\"excludeHidden\", True)\n\t            arguments[\"isPrevLink\"] = kwargs.get(\"isPrevLink\", \"\")\n\t        res:dict = self.connector.getData(self.endpoint + path, params=arguments)\n\t        data:list = res.get(\"queries\",[])\n\t        nextPage = res.get(\"_links\",{}).get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\":\n\t            hrefParams = nextPage.split(\"?\")[1]\n", "            orderBy = re.search(\"orderby=(.+?)(&|$)\", hrefParams)\n\t            start = re.search(\"start=(.+?)(&|$)\", hrefParams)\n\t            arguments[\"start\"] = start.group(1)\n\t            arguments[\"orderby\"] = orderBy.group(1)\n\t            res = self.connector.getData(self.endpoint + path, params=arguments)\n\t            data += res.get(\"queries\", [])\n\t            nextPage = res.get(\"_links\", {}).get(\"next\", {}).get(\"href\", \"\")\n\t            if n_results < float(\n\t                len(data)\n\t            ):  ## forcing exit when reaching number of results asked\n", "                nextPage = \"\"\n\t        return data\n\t    def postQueries(\n\t        self,\n\t        data: dict = None,\n\t        name: str = None,\n\t        dbname: str = \"prod:all\",\n\t        sql: str = None,\n\t        templateId: str = None,\n\t        queryParameters: dict = None,\n", "        insertIntoParameters: dict = None,\n\t        ctasParameters: dict = None,\n\t        description: str = \"\",\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a query.\n\t        Arguments:\n\t            data : OPTIONAL : If you want to pass the full query statement.\n\t            name : REQUIRED : Name of the query\n", "            dbname : REQUIRED : the dataset name (default prod:all)\n\t            sql: REQUIRED : the SQL query as a string.\n\t            queryParameters : OPTIONAL : in case you are using template, providing the paramter in a dictionary.\n\t            insertIntoParameters : OPTIONAL : in case you want to insert the result to an existing dataset\n\t                example : {\n\t                    \"datasetName\": \"string\"\n\t                }\n\t            ctasParameters: OPTIONAL : in case you want to create a dataset out of that query, dictionary is required with \"datasetName\" and \"description\".\n\t                example : {\n\t                    \"datasetName\": \"string\",\n", "                    \"description\": \"string\",\n\t                    \"targetSchemaTitle\":\"string\"\n\t                    }\n\t                    targetSchemaTitle if you want to use a precreated schema.\n\t        \"\"\"\n\t        path = \"/queries\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting postQuery\")\n\t        if data is None:\n\t            if (sql is None and templateId is None) or name is None:\n", "                raise AttributeError(\"You are missing required arguments.\")\n\t            if type(ctasParameters) is dict:\n\t                if (\n\t                    \"datasetName\" not in ctasParameters.keys()\n\t                    or \"description\" not in ctasParameters.keys()\n\t                ):\n\t                    raise KeyError(\n\t                        'Expecting \"datasetName\" and \"description\" as part of the the ctasParameters dictionary.'\n\t                    )\n\t            data = {\n", "                \"name\": name,\n\t                \"description\": description,\n\t                \"dbName\": dbname\n\t            }\n\t            if sql is not None:\n\t                data[\"sql\"] = sql\n\t            if templateId is not None:\n\t                data[\"templateId\"] = templateId\n\t            if queryParameters is not None:\n\t                data[\"queryParameters\"] = queryParameters\n", "            if ctasParameters is not None:\n\t                data[\"ctasParameters\"] = ctasParameters\n\t            if insertIntoParameters is not None:\n\t                data[\"insertIntoParameters\"] = insertIntoParameters\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=data, headers=self.header\n\t        )\n\t        return res\n\t    def getQuery(self, queryId: str = None) -> dict:\n\t        \"\"\"\n", "        Request the query status by ID.\n\t        Argument :\n\t            queryid : REQUIRED : the query id to check\n\t        \"\"\"\n\t        if queryId is None:\n\t            raise AttributeError('Expected \"queryId\" to be filled')\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getQuery\")\n\t        path = f\"/queries/{queryId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n", "        return res\n\t    def cancelQuery(self, queryId: str = None) -> dict:\n\t        \"\"\"\n\t        Cancel a specific query based on its ID.\n\t        Argument:\n\t            queryId : REQUIRED : the query ID to cancel\n\t        \"\"\"\n\t        if queryId is None:\n\t            raise AttributeError('Expected \"queryId\" to be filled')\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting cancelQuery\")\n\t        path = f\"/queries/{queryId}\"\n\t        data = {\"op\": \"cancel\"}\n\t        res = self.connector.patchData(self.endpoint + path, data=data)\n\t        return res\n\t    def deleteQuery(self, queryId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific query based on its ID.\n\t        Argument:\n\t            queryId : REQUIRED : the query ID to delete\n", "        \"\"\"\n\t        if queryId is None:\n\t            raise AttributeError('Expected \"queryId\" to be filled')\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteQuery\")\n\t        path = f\"/queries/{queryId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getSchedules(self, n_results: int = 1000, **kwargs) -> list:\n\t        \"\"\"\n", "        Retrieve a list of scheduled queries for the AEP instance.\n\t        Arguments:\n\t            n_results : OPTIONAL : Total number of scheduled queries retrieved. To get them all, use \"inf\"\n\t        possibile kwargs:\n\t            orderby : + for ASC and - for DESC\n\t            limit : number of record to fetch\n\t            start : specific start (use with orderby)\n\t            property : Comma-separated filters.\n\t                created ; ex : created>2017-04-05T13:30:00Z\n\t                templateId ; ex : templateId==123412354\n", "                userId ; ex : userId==12341235\n\t        \"\"\"\n\t        n_results = float(n_results)\n\t        if kwargs.get(\"orderby\", None) is not None:\n\t            if kwargs.get(\"orderby\", \"-\") == \"+\":\n\t                kwargs.get(\"orderby\", \"-\") == \"%2B\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchedules\")\n\t        path = \"/schedules\"\n\t        params = {**kwargs}\n", "        res = self.connector.getData(self.endpoint + path, params=params)\n\t        data = res[\"schedules\"]\n\t        nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\":\n\t            hrefParams = nextPage.split(\"?\")[1]\n\t            orderBy = re.search(\"orderby=(.+?)(&|$)\", hrefParams)\n\t            start = re.search(\"start=(.+?)(&|$)\", hrefParams)\n\t            params[\"start\"] = start.group(1)\n\t            params[\"orderby\"] = orderBy.group(1)\n\t            res = self.connector.getData(self.endpoint + path, params=params)\n", "            data += res.get(\"schedules\", [])\n\t            nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t            if len(data) > float(n_results):\n\t                nextPage = \"\"\n\t        return data\n\t    def getSchedule(self, scheduleId: str = None) -> dict:\n\t        \"\"\"\n\t        Get a details about a schedule query.\n\t        Arguments:\n\t            scheduleId : REQUIRED : the schedule id\n", "        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"scheduleId is required\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchedule\")\n\t        path = f\"/schedules/{scheduleId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getScheduleRuns(self, scheduleId: str = None) -> list:\n\t        \"\"\"\n", "        Get the different jobs ran for this schedule\n\t        Arguments:\n\t            scheduleId : REQUIRED : the schedule id\n\t        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"scheduleId is required\")\n\t        path = f\"/schedules/{scheduleId}/runs\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getScheduleRuns\")\n\t        params = {}\n", "        res = self.connector.getData(self.endpoint + path)\n\t        data = res[\"runsSchedules\"]\n\t        nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\":\n\t            hrefParams = nextPage.split(\"?\")[1]\n\t            orderBy = re.search(\"orderby=(.+?)(&|$)\", hrefParams)\n\t            start = re.search(\"start=(.+?)(&|$)\", hrefParams)\n\t            params[\"start\"] = start.group(1)\n\t            params[\"orderby\"] = orderBy.group(1)\n\t            res = self.connector.getData(self.endpoint + path, params=params)\n", "            data += res.get(\"runsSchedules\", [])\n\t            nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        return data\n\t    def getScheduleRun(self, scheduleId: str = None, runId: str = None) -> dict:\n\t        \"\"\"\n\t        Get the different jobs ran for this schedule\n\t        Arguments:\n\t            scheduleId : REQUIRED : the schedule id\n\t            runId : REQUIRED : the run ID you want to retrieve.\n\t        \"\"\"\n", "        if scheduleId is None or runId is None:\n\t            raise Exception(\"scheduleId and jobId are required\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getScheduleRun\")\n\t        path = f\"/schedules/{scheduleId}/runs/{runId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def createSchedule(\n\t        self,\n\t        scheduleQuery: dict = None,\n", "        name: str = None,\n\t        dbname: str = \"prod:all\",\n\t        sql: str = None,\n\t        templateId: str = None,\n\t        queryParameters: dict = None,\n\t        insertIntoParameters: dict = None,\n\t        ctasParameters: dict = None,\n\t        schedule: dict = None,\n\t        description: str = \"\",\n\t        **kwargs,\n", "    ) -> dict:\n\t        \"\"\"\n\t        Create a scheduled query.\n\t        Arguments:\n\t            scheduleQuery: REQUIRED : a dictionary containing the query and the schedule.\n\t            name : OPTIONAL : Name of the query\n\t            dbname : OPTIONAL : the dataset name (default prod:all)\n\t            sql: OPTIONAL : the SQL query as a string.\n\t            templateId : OPTIONAL : The Template ID to be used\n\t            queryParameters : OPTIONAL : in case you are using template, providing the paramter in a dictionary.\n", "            ctasParameters: OPTIONAL : in case you want to create a dataset out of that query, dictionary is required with \"datasetName\" and \"description\".\n\t            schedule : OPTIONAL : Dictionary giving the instruction to schedule the query.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSchedule\")\n\t        if scheduleQuery is None:\n\t            if name is None or (sql is None and templateId is None) or schedule is None:\n\t                raise Exception(\n\t                    \"Expecting either scheduleQUery dictionary or data in parameters\"\n\t                )\n", "            scheduleQuery = {\n\t                \"query\": {\n\t                    \"name\": name,\n\t                    \"description\": description,\n\t                    \"dbName\": dbname\n\t                },\n\t                \"schedule\": schedule,\n\t            }\n\t            if sql is not None:\n\t                scheduleQuery[\"query\"][\"sql\"] = sql\n", "            if templateId is not None:\n\t                scheduleQuery[\"query\"][\"templateId\"] = templateId\n\t            if queryParameters is not None:\n\t                scheduleQuery[\"query\"][\"queryParameters\"] = queryParameters\n\t            if ctasParameters is not None:\n\t                scheduleQuery[\"query\"][\"ctasParameters\"] = ctasParameters\n\t            if insertIntoParameters is not None:\n\t                scheduleQuery[\"query\"][\"insertIntoParameters\"] = insertIntoParameters\n\t        if type(scheduleQuery) != dict:\n\t            raise Exception(\"scheduleQuery is required and should be dictionary. \")\n", "        if (\n\t            \"query\" not in scheduleQuery.keys()\n\t            or \"schedule\" not in scheduleQuery.keys()\n\t        ):\n\t            raise Exception(\n\t                'scheduleQuery dictonary is expected to contain \"schedule\" or \"query\" keys.'\n\t            )\n\t        path = \"/schedules\"\n\t        res = self.connector.postData(self.endpoint + path, data=scheduleQuery)\n\t        return res\n", "    def deleteSchedule(self, scheduleId: str = None) -> Union[str, dict]:\n\t        \"\"\"\n\t        Delete a scheduled query.\n\t        Arguments:\n\t            scheduleId : REQUIRED : id of the schedule.\n\t        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"Missing scheduleId\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteSchedules\")\n", "        path = f\"/schedules/{scheduleId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def disableSchedule(self, scheduleId: str = None) -> dict:\n\t        \"\"\"\n\t        Disable a scheduled query.\n\t        Arguments:\n\t            scheduleId : REQUIRED : id of the schedule.\n\t        \"\"\"\n\t        if scheduleId is None:\n", "            raise Exception(\"Missing scheduleId\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting disableSchedules\")\n\t        obj = {\"body\": [{\"op\": \"replace\", \"path\": \"/state\", \"value\": \"disable\"}]}\n\t        path = f\"/schedules/{scheduleId}\"\n\t        res = self.connector.patchData(\n\t            self.endpoint + path, data=obj, headers=self.header\n\t        )\n\t        return res\n\t    def enableSchedule(self, scheduleId: str = None) -> dict:\n", "        \"\"\"\n\t        Enable a scheduled query.\n\t        Arguments:\n\t            scheduleId : REQUIRED : id of the schedule.\n\t        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"Missing scheduleId\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting enableSchedule\")\n\t        obj = {\"body\": [{\"op\": \"replace\", \"path\": \"/state\", \"value\": \"enable\"}]}\n", "        path = f\"/schedules/{scheduleId}\"\n\t        res = self.connector.patchData(\n\t            self.endpoint + path, data=obj, headers=self.header\n\t        )\n\t        return res\n\t    def updateSchedule(self, scheduleId: str = None, update_obj: list = None) -> dict:\n\t        \"\"\"\n\t        Update the schedule query with the object pass.\n\t        Arguments:\n\t            scheduleId : REQUIRED : id of the schedule.\n", "            update_obj : REQUIRED : List of patch operations\n\t        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"Missing scheduleId\")\n\t        if update_obj is None:\n\t            raise Exception(\"Missing update_obj to generate the operation.\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateSchedules\")\n\t        path = f\"/schedules/{scheduleId}\"\n\t        res = self.connector.patchData(\n", "            self.endpoint + path, data=update_obj, headers=self.header\n\t        )\n\t        return res\n\t    def getTemplates(self, n_results: int = 1000, **kwargs) -> dict:\n\t        \"\"\"\n\t        Retrieve the list of template for this instance.\n\t        Arguments:\n\t            n_results : OPTIONAL : number of total results to retrieve. To get them all, use \"inf\".\n\t        possible kwargs:\n\t            limit : Hint on number of records to fetch per page. default (50)\n", "            orderby : Field to order results by. Supported fields: created, updated. Prepend property name with + for ASC,- for DESC order. Default is -created.\n\t            start : Start value of property specified using orderby.\n\t            property : Comma-separated filters.List of properties that allow filtering:\n\t                    name\n\t                    userId\n\t                    lastUpdatedBy\n\t                operations:\n\t                    ‘~’ (contains). This operator can only be used on the name property.\n\t                    ‘==’ (equal to). This operator can be used on both the userId and the lastUpdatedBy properties.\n\t        more details here : https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Query-Templates/get_query_templates\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getTemplates\")\n\t        path = \"/query-templates\"\n\t        n_results = float(n_results)  # changing inf to float inf\n\t        params = {**kwargs}\n\t        params[\"limit\"] = kwargs.get(\"limit\", 50)\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=self.header, params=params\n\t        )\n", "        data = res[\"templates\"]\n\t        nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\":\n\t            hrefParams = nextPage.split(\"?\")[1]\n\t            orderBy = re.search(\"orderby=(.+?)(&|$)\", hrefParams)\n\t            start = re.search(\"start=(.+?)(&|$)\", hrefParams)\n\t            params[\"start\"] = start.group(1)\n\t            params[\"orderby\"] = orderBy.group(1)\n\t            res = self.connector.getData(self.endpoint + path, params=params)\n\t            data += res.get(\"templates\", [])\n", "            nextPage = res.get(\"_links\", {}).get(\"next\", {}).get(\"href\", \"\")\n\t            if len(data) > float(n_results):\n\t                nextPage = \"\"\n\t        return data\n\t    def getTemplate(self, templateId: str = None) -> dict:\n\t        \"\"\"\n\t        Retrieve a specific template ID.\n\t        Arguments:\n\t            templateId : REQUIRED : template ID to be retrieved.\n\t        \"\"\"\n", "        if templateId is None:\n\t            raise ValueError(\"Require a template ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getTemplate\")\n\t        path = f\"/query-templates/{templateId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteTemplate(self, templateId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a template based on its ID.\n", "        Arguments:\n\t            templateId : REQUIRED : template ID to be deleted.\n\t        \"\"\"\n\t        if templateId is None:\n\t            raise ValueError(\"Require a template ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteTemplate\")\n\t        path = f\"/query-templates/{templateId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n", "    def createQueryTemplate(self, queryData: dict = None) -> dict:\n\t        \"\"\"\n\t        Create a query template based on the dictionary passed.\n\t        Arguments:\n\t            queryData : REQUIED : An object that contains \"sql\", \"queryParameter\" and \"name\" keys.\n\t        more info : https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Query-Templates/create_query_template\n\t        \"\"\"\n\t        path = \"/query-templates\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createTemplate\")\n", "        if isinstance(queryData, dict):\n\t            if (\n\t                \"sql\" not in queryData.keys()\n\t                or \"queryParameters\" not in queryData.keys()\n\t                or \"name\" not in queryData.keys()\n\t            ):\n\t                raise KeyError(\n\t                    \"Minimum key value are not respected.\\nPlease see here for more info :\\nhttps://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Query-Templates/create_query_template \"\n\t                )\n\t        else:\n", "            raise Exception(\"expected a dictionary for queryData\")\n\t        res = self.connector.postData(\n\t            self.endpoint + path, headers=self.header, data=queryData\n\t        )\n\t        return res\n\t    def getDatasetStatistics(self,datasetId:str=None,statisticType:str=\"table HTTP/1.1\")->dict:\n\t        \"\"\"\n\t        Returns statistics on the dataset size.\n\t        Arguments:\n\t            datasetId : REQUIRED : The dataset ID to look for.\n", "            statisticType : OPTIONAL : The type of statistic to retrieve.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDatasetStatistics\")\n\t        if datasetId is None:\n\t            raise Exception(\"A datasetId is required\")\n\t        path = \"/statistics\"\n\t        params = {\"dataSet\":datasetId,\"statisticType\":statisticType}\n\t        res = self.connector.getData(self.endpoint + path,params=params)\n\t        return res\n", "    def getAlertSubscriptions(self,n_results: int = 1000, **kwargs)->list:\n\t        \"\"\"\n\t        Get the list of alerts subscriptions.\n\t        Arguments:\n\t            n_results : OPTIONAL : The total number of result you want.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getAlertSubscriptions\")\n\t        params = {\"page\":kwargs.get(\"page\",0)}\n\t        path = f\"/alert-subscriptions\"\n", "        res = self.connector.getData(self.endpoint + path,params=params)\n\t        data = res.get(\"alerts\")\n\t        count = res.get(\"_page\",{}).get('count',0)\n\t        nextpage = res.get('_links',{}).get('next',{}).get('href','')\n\t        while nextpage != \"\" and count <n_results:\n\t            params[\"page\"] += 1\n\t            res = self.connector.getData(self.endpoint + path,params=params)\n\t            data += res.get(\"alerts\")\n\t            count += res.get(\"_page\",{}).get('count',0)\n\t            nextpage = res.get('_links',{}).get('next',{}).get('href','')\n", "        return data\n\t    def createAlertSubscription(self,assetId:str=None,alertType:str=None,emails:list=None,inAEP:bool=True,inEmail:bool=True)->dict:\n\t        \"\"\"\n\t        Create a subscription to an asset (queryID or scheduleID) for a list of predefined users.\n\t        Arguments:\n\t            assetId : REQUIRED : The schedule ID or query ID.\n\t            alertType : REQUIRED : The type of alert to listen to. (start, success, failure)\n\t            emails : REQUIRED : A list of email addresses that subscribes to that alert.\n\t            inAEP : OPTIONAL : If the Alert should show up in AEP UI. (default True)\n\t            inEmail : OPTIONAL : If the Alert should be sent via email. (default True)\n", "                NOTE: Consider setting your email address for notification via this tutorial:\n\t                https://experienceleague.adobe.com/docs/experience-platform/observability/alerts/ui.html?lang=en#enable-email-alerts\n\t        \"\"\"\n\t        if assetId is None:\n\t            raise ValueError(\"Require an asset ID\")\n\t        if alertType is None:\n\t            raise ValueError(\"Require an alert type\")\n\t        if emails is None or type(emails) != list:\n\t            raise ValueError(\"Require a list of email addresses\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting createAlertSubscription for asset: {assetId}\")\n\t        path = f\"/alert-subscriptions\"\n\t        data = {\n\t            \"assetId\": assetId,\n\t            \"alertType\": alertType,\n\t            \"subscriptions\":{\n\t                \"emailIds\" : emails,\n\t                \"inContextNotifications\" : inAEP,\n\t                \"emailNotifications\":inEmail\n\t            }  \n", "        }\n\t        res = self.connector.postData(self.endpoint+path, data=data)\n\t        return res\n\t    def deleteAlertSubscription(self,assetId:str=None, alertType:str=None)->dict:\n\t        \"\"\"\n\t        Delete a subscription for a specific alert on a specifc assetId.\n\t        Arguments\n\t            assetId : REQUIRED : A query ID or a schedule ID that you want to delete the alert for.\n\t            alertType : REQUIRED : The state of query execution that triggers the alert to be deleted. (start, success, failure).\n\t        \"\"\"\n", "        if assetId is None:\n\t            raise ValueError(\"Require an asset ID\")\n\t        if alertType is None:\n\t            raise ValueError(\"Require an alert type\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteAlertSubscription for asset: {assetId}\")\n\t        path = f\"/alert-subscriptions/{assetId}/{alertType}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getAlertSubscriptionsTypeId(self,assetId:str=None, alertType:str=None)->dict:\n", "        \"\"\"\n\t        Retrieve the subscriptions made about a specific asset ID and with or without alertType specification\n\t        Arguments:\n\t            assetId : REQUIRED : A query or schedule ID that you want the subscription information for.\n\t            alertType : OPTIONAL : This property describes the state of query execution that triggers an alert.\n\t                        (start, success, failure).\n\t        \"\"\"\n\t        if assetId is None:\n\t            raise ValueError(\"Require an asset ID\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getAlertSubscriptionsTypeId for asset: {assetId}\")\n\t        if alertType is None:\n\t            path = f\"/alert-subscriptions/{assetId}\"\n\t        elif alertType is not None and type(alertType) == str:\n\t            path = f\"/alert-subscriptions/{assetId}/{alertType}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res.get('alerts')\n\t    def patchAlert(self,assetId:str=None, alertType:str=None,action:str=\"disable\")->dict:\n\t        \"\"\"\n\t        Disable or Enable an alert by providing the assetId and the alertType.\n", "        Arguments:\n\t            assetId : REQUIRED : A query or schedule ID that you want the subscription information for.\n\t            alertType : REQUIRED : This property describes the state of query execution that triggers an alert.\n\t                        (start, success, failure)\n\t            action : OPTIONAL : the action to take on that Alert. \"disable\" (default) or \"enable\"\n\t        \"\"\"\n\t        if assetId is None:\n\t            raise ValueError(\"Require an asset ID\")\n\t        if alertType is None:\n\t            raise ValueError(\"Require an alert Type\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchAlert for asset: {assetId}, action : {action}\")\n\t        path = f\"/alert-subscriptions/{assetId}/{alertType}\"\n\t        data = {\n\t            \"op\":\"replace\",\n\t            \"path\":\"/status\",\n\t            \"value\":action\n\t        }\n\t        res = self.connector.patchData(self.endpoint + path,data=data)\n\t        return res\n", "    def getUserAlerts(self,email:str=None)->list:\n\t        \"\"\"\n\t        Get the alert that a specific user is subscribed to.\n\t        Argument:\n\t            email : REQUIRED : the email address of the user\n\t        \"\"\"\n\t        if email is None:\n\t            raise ValueError(\"Require a valid email address\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getUserAlerts for user: {email}\")\n", "        params = {'page':0}\n\t        path = f\"/alert-subscriptions/user-subscriptions/{email}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        data = res.get('items')\n\t        nextPage = res.get('_links',{}).get('next',{}).get('href','')\n\t        while nextPage != \"\":\n\t            params['page'] +=1\n\t            res = self.connector.getData(self.endpoint + path,params=params)\n\t            data += res.get('items')\n\t            nextPage = res.get('_links',{}).get('next',{}).get('href','')\n", "        return data\n\t    def createAcceleratedQuery(self,name:str=None,dbName:str=None,sql:str=None,templateId:str=None,description:str=\"power by aepp\")->dict:\n\t        \"\"\"\n\t        Create an accelerated query statement based on either an SQL statement or a template ID.\n\t        Arguments:\n\t            name : REQUIRED : Name of your query\n\t            dbName : REQUIRED : The name of the database you are making an accelerated query to. \n\t                        The value for dbName should take the format of {SANDBOX_NAME}:{ACCELERATED_STORE_DATABASE}:{ACCELERATED_STORE_SCHEMA}\n\t            sql : REQUIRED : Either this parameter with a SQL statement or a templateId in the \"templateId\" parameter.\n\t            templateId : REQUIRED : Either this parameter with a template ID or a SQL statement in the \"sql\" parameter.\n", "            description : OPTIONAL : An optional comment on the intent of the query to help other users understand its purpose. Max 1000 bytes.\n\t        \"\"\"\n\t        if name is None:\n\t            raise ValueError(\"Require a name\")\n\t        if dbName is None or len(dbName.split(\":\")) != 3:\n\t            raise ValueError(\"Require a dbName such as : {SANDBOX_NAME}:{ACCELERATED_STORE_DATABASE}:{ACCELERATED_STORE_SCHEMA}\")\n\t        if templateId is None and sql is None:\n\t            raise SyntaxError(\"Require either an sql or a templateId parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createAcceleratedQuery with name: {name}\")\n", "        path = f\"/accelerated-queries\"\n\t        data = {\n\t            \"name\" : name,\n\t            \"dbName\":dbName,\n\t            \"description\":description,    \n\t        }\n\t        if sql is not None:\n\t            data['sql'] = sql\n\t        elif templateId is not None:\n\t            data['templateId'] = templateId\n", "        res = self.connector.postData(self.endpoint+path, data=data)\n\t        return res\n\tclass InteractiveQuery:\n\t    \"\"\"\n\t    Provide the instance connected to PostgreSQL database and you can return the result directly in your notebook.\n\t    This class requires that you have used connection method in the QueryService.\n\t    The object returned by the connection method should be used when creating this object.\n\t    USING PyGreSQL\n\t    \"\"\"\n\t    config_object = {}\n", "    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(self, conn_object: dict = None, loggingObject: dict = None):\n\t        \"\"\"\n\t        Importing the pg (PyGreSQL) library and instantiating the connection via the conn_object pass over the instantiation method.\n\t        Arguments:\n\t            conn_object : REQUIRED : The dictionary returned via the queryservice api \"connection\"\n\t        \"\"\"\n\t        from pg import DB\n", "        if conn_object is None:\n\t            raise AttributeError(\n\t                \"You are missing the conn_object. Use the QueryService to retrieve the object.\"\n\t            )\n\t        self.dbname = conn_object[\"dbName\"]\n\t        self.host = conn_object[\"host\"]\n\t        self.port = conn_object[\"port\"]\n\t        self.user = conn_object[\"username\"]\n\t        self.passwd = conn_object[\"token\"]\n\t        self.websocketHost = conn_object[\"websocketHost\"]\n", "        self.config_object = {\n\t            \"dbname\": self.dbname,\n\t            \"host\": self.host,\n\t            \"user\": self.user,\n\t            \"passwd\": self.passwd,\n\t            \"port\": self.port,\n\t        }\n\t        self.db = DB(**self.config_object)\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n", "        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            formatter = logging.Formatter(loggingObject[\"format\"])\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n", "                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t    def query(\n\t        self, sql: str = None, output: str = \"dataframe\"\n\t    ) -> Union[pd.DataFrame, object]:\n\t        \"\"\"\n\t        Query the database and return different type of data, depending the format parameters.\n\t        Requests are limited to return 50 K rows\n\t        Arguments:\n", "            sql : REQUIRED : the SQL request you want to realize.\n\t            output : OPTIONAL : the format you would like to be returned.\n\t            Possible format:\n\t                \"raw\" : return the instance of the query object.\n\t                \"dataframe\" : return a dataframe with the data. (default)\n\t        \"\"\"\n\t        if sql is None:\n\t            raise Exception(\"Required a SQL query\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting query:\\n {sql}\")\n", "        query = self.db.query(sql)\n\t        if output == \"raw\":\n\t            return query\n\t        elif output == \"dataframe\":\n\t            data = query.getresult()\n\t            columns = query.listfields()\n\t            df = pd.DataFrame(data, columns=columns)\n\t            return df\n\t        else:\n\t            raise KeyError(\"You didn't specify a correct value.\")\n", "    def transformToDataFrame(self, query: object = None) -> pd.DataFrame:\n\t        \"\"\"\n\t        This will return you a dataFrame\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting transformToDataFrame\")\n\t        data = query.getresult()\n\t        columns = query.listfields()\n\t        df = pd.DataFrame(data, columns=columns)\n\t        return df\n", "    def queryIdentity(\n\t        self,\n\t        identityId: str = None,\n\t        fields: list = None,\n\t        tableName: str = None,\n\t        output: str = \"dataframe\",\n\t        fieldId: str = \"ECID\",\n\t        limit: str = None,\n\t        save: bool = False,\n\t        verbose: bool = False,\n", "    ) -> Union[pd.DataFrame, object]:\n\t        \"\"\"\n\t        Return the elements that you have passed in field list and return the output selected.\n\t        Arguments:\n\t            identityId : REQUIRED : The ID you want to retrieve\n\t            fields : REQUIRED : a list of fields you want to return for that ID in your table.\n\t                example : ['person.name']\n\t            tableName : REQUIRED : The dataset table name to use\n\t            output : OPTIONAL : the format you would like to be returned.\n\t            Possible format:\n", "                \"raw\" : return the instance of the query object.\n\t                \"dataframe\" : return a dataframe with the data. (default)\n\t            fieldId : OPTIONAL : If you want your selection to be based on another field than ECID in IdentityMap.\n\t            limit : OPTIONAL : If you wish to set a LIMIT on number of row returned.\n\t            save : OPTIONAL : will save a csv file\n\t            verbose : OPTIONAL : will display some comment\n\t        \"\"\"\n\t        if identityId is None:\n\t            raise ValueError(\"Require an identity value\")\n\t        if type(fields) != list:\n", "            raise ValueError(\"Require a list of fields to be returned\")\n\t        if tableName is None:\n\t            raise ValueError(\"Require a dataset table name\")\n\t        if fieldId == \"ECID\":\n\t            condition = f\"WHERE identityMap['ECID'][0].id = '{identityId}'\"\n\t        elif fieldId != \"ECID\" and fieldId is not None:\n\t            condition = f\"WHERE {fieldId} = '{identityId}'\"\n\t        else:\n\t            condition = \"\"\n\t        if limit is None:\n", "            limit = \"\"\n\t        else:\n\t            limit = f\"LIMIT {limit}\"\n\t        sql = f\"SELECT {','.join(fields)} FROM {tableName} {condition} {limit}\"\n\t        if verbose:\n\t            print(sql)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting queryIdentity:\\n {sql}\")\n\t        res = self.query(sql=sql, output=output)\n\t        if verbose:\n", "            print(f\"Data is returned in the {output} format\")\n\t        if save:\n\t            if isinstance(res, pd.DataFrame):\n\t                res.to_csv(f\"{identityId}.csv\", index=False)\n\t            else:\n\t                data = res.getresult()\n\t                columns = res.listfields()\n\t                df = pd.DataFrame(data, columns=columns)\n\t                df.to_csv(f\"{identityId}.csv\", index=False)\n\t        return res\n", "class InteractiveQuery2:\n\t    \"\"\"\n\t    Provide the instance connected to PostgreSQL database and you can return the result directly in your notebook.\n\t    This class requires that you have used connection method in the QueryService.\n\t    The object returned by the connection method should be used when creating this object.\n\t    USING psycopg2\n\t    \"\"\"\n\t    config_object = {}\n\t    ## logging capability\n\t    loggingEnabled = False\n", "    logger = None\n\t    def __init__(self, conn_object: dict = None, loggingObject: dict = None):\n\t        \"\"\"\n\t        Importing the psycopg2 library and instantiating the connection via the conn_object pass over the instantiation method.\n\t        Arguments:\n\t            conn_object : REQUIRED : The dictionary returned via the queryservice api \"connection\"\n\t        \"\"\"\n\t        import psycopg2\n\t        if conn_object is None:\n\t            raise AttributeError(\n", "                \"You are missing the conn_object. Use the QueryService to retrieve the object.\"\n\t            )\n\t        self.dbname = conn_object[\"dbName\"]\n\t        self.host = conn_object[\"host\"]\n\t        self.port = conn_object[\"port\"]\n\t        self.user = conn_object[\"username\"]\n\t        self.passwd = conn_object[\"token\"]\n\t        self.config_object = {\n\t            \"dbname\": self.dbname,\n\t            \"host\": self.host,\n", "            \"user\": self.user,\n\t            \"password\": self.passwd,\n\t            \"port\": self.port,\n\t        }\n\t        self.connect = psycopg2.connect(**self.config_object)\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n", "            self.logger.setLevel(loggingObject[\"level\"])\n\t            formatter = logging.Formatter(loggingObject[\"format\"])\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n", "    def query(\n\t        self, sql: str = None, output: str = \"dataframe\"\n\t    ) -> Union[pd.DataFrame, object]:\n\t        \"\"\"\n\t        Query the database and return different type of data, depending the format parameters.\n\t        Requests are limited to return 50 K rows\n\t        Arguments:\n\t            sql : REQUIRED : the SQL request you want to realize.\n\t            output : OPTIONAL : the format you would like to be returned.\n\t            Possible format:\n", "                \"raw\" : return the instance of the query object.\n\t                \"dataframe\" : return a dataframe with the data. (default)\n\t        \"\"\"\n\t        if sql is None:\n\t            raise Exception(\"Required a SQL query\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting query:\\n {sql}\")\n\t        cursor = self.connect.cursor()\n\t        cursor.execute(sql)\n\t        if output == \"raw\":\n", "            return cursor\n\t        elif output == \"dataframe\":\n\t            df = pd.DataFrame(cursor.fetchall())\n\t            df.columns = [col[0] for col in cursor.description]\n\t            return df\n\t        else:\n\t            raise KeyError(\"You didn't specify a correct value.\")\n\t    def transformToDataFrame(self, query: object = None) -> pd.DataFrame:\n\t        \"\"\"\n\t        Taking the raw output of the query method use with raw and returning a DataFrame\n", "        Arguments:\n\t            cursor : REQUIRED : The cursor that has been returned \n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting transformToDataFrame\")\n\t        df = pd.DataFrame(query.fetchall())\n\t        df.columns = [col[0] for col in query.description]\n\t        return df\n\t    def queryIdentity(\n\t        self,\n", "        identityId: str = None,\n\t        fields: list = None,\n\t        tableName: str = None,\n\t        output: str = \"dataframe\",\n\t        fieldId: str = \"ECID\",\n\t        limit: str = None,\n\t        save: bool = False,\n\t    ) -> Union[pd.DataFrame, object]:\n\t        \"\"\"\n\t        Return the elements that you have passed in field list and return the output selected.\n", "        Arguments:\n\t            identityId : REQUIRED : The ID you want to retrieve\n\t            fields : REQUIRED : a list of fields you want to return for that ID in your table.\n\t                example : ['person.name']\n\t            tableName : REQUIRED : The dataset table name to use\n\t            output : OPTIONAL : the format you would like to be returned.\n\t            Possible format:\n\t                \"raw\" : return the instance of the query object.\n\t                \"dataframe\" : return a dataframe with the data. (default)\n\t            fieldId : OPTIONAL : If you want your selection to be based on another field than ECID in IdentityMap.\n", "            limit : OPTIONAL : If you wish to set a LIMIT on number of row returned.\n\t            save : OPTIONAL : will save a csv file\n\t        \"\"\"\n\t        if identityId is None:\n\t            raise ValueError(\"Require an identity value\")\n\t        if type(fields) != list:\n\t            raise ValueError(\"Require a list of fields to be returned\")\n\t        if tableName is None:\n\t            raise ValueError(\"Require a dataset table name\")\n\t        if fieldId == \"ECID\":\n", "            condition = f\"WHERE identityMap['ECID'][0].id = '{identityId}'\"\n\t        elif fieldId != \"ECID\" and fieldId is not None:\n\t            condition = f\"WHERE {fieldId} = '{identityId}'\"\n\t        else:\n\t            condition = \"\"\n\t        if limit is None:\n\t            limit = \"\"\n\t        else:\n\t            limit = f\"LIMIT {limit}\"\n\t        sql = f\"SELECT {','.join(fields)} FROM {tableName} {condition} {limit}\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting queryIdentity:\\n {sql}\")\n\t        res = self.query(sql)\n\t        if save:\n\t            if isinstance(res, pd.DataFrame):\n\t                res.to_csv(f\"{identityId}.csv\", index=False)\n\t            else:\n\t                df = pd.DataFrame(res.fetchall())\n\t                df.columns = [col[0] for col in res.description]\n\t                df.to_csv(f\"{identityId}.csv\", index=False)\n", "        return res\n"]}
{"filename": "aepp/accesscontrol.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\timport logging\n\tfrom .configs import ConnectObject\n\tfrom typing import Union\n\tclass AccessControl:\n\t    \"\"\"\n\t    Access Control API endpoints.\n\t    Complete documentation is available:\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/access-control.yaml\n\t    \"\"\"\n", "    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict =aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ) -> None:\n", "        \"\"\"\n\t        Instantiate the access controle API wrapper.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t        kwargs :\n\t            header options\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n", "            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n", "                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n", "            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n", "            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"access\"]\n\t        )\n", "    def getReferences(self) -> dict:\n\t        \"\"\"\n\t        List all available permission names and resource types.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getReferences\")\n\t        path = \"/acl/reference\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def postEffectivePolicies(self, listElements: list = None):\n", "        \"\"\"\n\t        List all effective policies for a user on given resources within a sandbox.\n\t        Arguments:\n\t            listElements : REQUIRED : List of resource urls. Example url : /resource-types/{resourceName} or /permissions/{highLevelPermissionName}\n\t        \"\"\"\n\t        if type(listElements) != list:\n\t            raise TypeError(\"listElements should be a list of elements\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting postEffectivePolicies\")\n\t        path = \"/acl/effective-policies\"\n", "        res = self.connector.postData(\n\t            self.endpoint + path, data=listElements, headers=self.header\n\t        )\n\t        return res\n"]}
{"filename": "aepp/flowservice.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\tfrom copy import deepcopy\n\timport time,json\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\t@dataclass\n\tclass _Data:\n\t    def __init__(self):\n", "        self.flowId = {}\n\t        self.flowSpecId = {}\n\tclass FlowService:\n\t    \"\"\"\n\t    The Flow Service manage the ingestion part of the data in AEP.\n\t    For more information, relate to the API Documentation, you can directly refer to the official documentation:\n\t        https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/flow-service.yaml\n\t        https://experienceleague.adobe.com/docs/experience-platform/sources/home.html\n\t        https://experienceleague.adobe.com/docs/experience-platform/destinations/home.html\n\t    \"\"\"\n", "    PATCH_REFERENCE = [\n\t        {\n\t            \"op\": \"Add\",\n\t            \"path\": \"/auth/params\",\n\t            \"value\": {\n\t                \"description\": \"A new description to provide further context on a specified connection or flow.\"\n\t            },\n\t        }\n\t    ]\n\t    ## logging capability\n", "    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n", "        initialize the Flow Service instance.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t            loggingObject : OPTIONAL : A dictionary presenting the configuration of the logging service.\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n", "            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n", "            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n", "            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n", "            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"flow\"]\n\t        self.endpoint_gloal = aepp.config.endpoints[\"global\"]\n\t        self.data = _Data()\n\t    def getResource(\n\t        self,\n\t        endpoint: str = None,\n", "        params: dict = None,\n\t        format: str = \"json\",\n\t        save: bool = False,\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Template for requesting data with a GET method.\n\t        Arguments:\n\t            endpoint : REQUIRED : The URL to GET\n\t            params: OPTIONAL : dictionary of the params to fetch\n", "            format : OPTIONAL : Type of response returned. Possible values:\n\t                json : default\n\t                txt : text file\n\t                raw : a response object from the requests module\n\t        \"\"\"\n\t        if endpoint is None:\n\t            raise ValueError(\"Require an endpoint\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getResource\")\n\t        res = self.connector.getData(endpoint, params=params, format=format)\n", "        if save:\n\t            if format == \"json\":\n\t                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"json\",\n\t                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n\t            elif format == \"txt\":\n", "                aepp.saveFile(\n\t                    module=\"catalog\",\n\t                    file=res,\n\t                    filename=f\"resource_{int(time.time())}\",\n\t                    type_file=\"txt\",\n\t                    encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t                )\n\t            else:\n\t                print(\n\t                    \"element is an object. Output is unclear. No save made.\\nPlease save this element manually\"\n", "                )\n\t        return res\n\t    def getConnections(\n\t        self, limit: int = 20, n_results: int = 100, count: bool = False, **kwargs\n\t    ) -> list:\n\t        \"\"\"\n\t        Returns the list of connections available.\n\t        Arguments:\n\t            limit : OPTIONAL : number of result returned per request (default 20)\n\t            n_results : OPTIONAL : number of total result returned (default 100, set to \"inf\" for retrieving everything)\n", "            count : OPTIONAL : if set to True, just returns the number of connections\n\t        kwargs will be added as query parameters\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getConnections\")\n\t        params = {\"limit\": limit}\n\t        if count:\n\t            params[\"count\"] = count\n\t        for kwarg in kwargs:\n\t            params[kwarg] = kwargs[kwarg]\n", "        path = \"/connections\"\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        try:\n\t            data = res[\"items\"]\n\t            continuationToken = res.get(\"_links\", {}).get(\"next\", {}).get(\"href\", \"\")\n\t            while continuationToken != \"\" and len(data) < float(n_results):\n\t                res = self.connector.getData(\n\t                    self.endpoint + continuationToken, params=params\n\t                )\n\t                data += res[\"items\"]\n", "                continuationToken = (\n\t                    res.get(\"_links\", {}).get(\"next\", {}).get(\"href\", \"\")\n\t                )\n\t            return data\n\t        except:\n\t            return res\n\t    def createConnection(\n\t        self,\n\t        data: dict = None,\n\t        name: str = None,\n", "        auth: dict = None,\n\t        connectionSpec: dict = None,\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a connection based on either the data being passed or the information passed.\n\t        Arguments:\n\t            data : REQUIRED : dictionary containing the different elements required for the creation of the connection.\n\t            In case you didn't pass a data parameter, you can pass different information.\n\t            name : REQUIRED : name of the connection.\n", "            auth : REQUIRED : dictionary that contains \"specName\" and \"params\"\n\t                specName : string that names of the the type of authentication to be used with the base connection.\n\t                params : dict that contains credentials and values necessary to authenticate and create a connection.\n\t            connectionSpec : REQUIRED : dictionary containing the \"id\" and \"verison\" key.\n\t                id : The specific connection specification ID associated with source\n\t                version : Specifies the version of the connection specification ID. Omitting this value will default to the most recent version\n\t        Possible kwargs:\n\t            responseType : by default json, but you can request 'raw' that return the requests response object.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting createConnection\")\n\t        path = \"/connections\"\n\t        if data is not None:\n\t            if (\n\t                \"name\" not in data.keys()\n\t                or \"auth\" not in data.keys()\n\t                or \"connectionSpec\" not in data.keys()\n\t            ):\n\t                raise Exception(\n\t                    \"Require some keys to be present : name, auth, connectionSpec\"\n", "                )\n\t            obj = data\n\t            res = self.connector.postData(self.endpoint + path, data=obj,format=kwargs.get('responseType','json'))\n\t            return res\n\t        elif data is None:\n\t            if \"specName\" not in auth.keys() or \"params\" not in auth.keys():\n\t                raise Exception(\n\t                    \"Require some keys to be present in auth dict : specName, params\"\n\t                )\n\t            if \"id\" not in connectionSpec.keys():\n", "                raise Exception(\n\t                    \"Require some keys to be present in connectionSpec dict : id\"\n\t                )\n\t            if name is None:\n\t                raise Exception(\"Require a name to be present\")\n\t            obj = {\"name\": name, \"auth\": auth, \"connectionSpec\": connectionSpec}\n\t            res = self.connector.postData(self.endpoint + path, data=obj,format=kwargs.get('responseType','json'))\n\t            return res\n\t    def createStreamingConnection(\n\t        self,\n", "        name: str = None,\n\t        sourceId: str = None,\n\t        dataType: str = \"xdm\",\n\t        paramName: str = None,\n\t        description: str = \"provided by aepp\",\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a Streaming connection based on the following connectionSpec :\n\t        \"connectionSpec\": {\n", "                \"id\": \"bc7b00d6-623a-4dfc-9fdb-f1240aeadaeb\",\n\t                \"version\": \"1.0\",\n\t            },\n\t            with provider ID : 521eee4d-8cbe-4906-bb48-fb6bd4450033\n\t        Arguments:\n\t            name : REQUIRED : Name of the Connection.\n\t            sourceId : REQUIRED : The ID of the streaming connection you want to create (random string possible).\n\t            dataType : REQUIRED : The type of data to ingest (default xdm)\n\t            paramName : REQUIRED : The name of the streaming connection you want to create.\n\t            description : OPTIONAL : if you want to add a description\n", "        kwargs possibility:\n\t            specName : if you want to modify the specification Name.(Default : \"Streaming Connection\")\n\t            responseType : by default json, but you can request 'raw' that return the requests response object.\n\t        \"\"\"\n\t        if name is None:\n\t            raise ValueError(\"Require a name for the connection\")\n\t        if sourceId is None:\n\t            raise Exception(\"Require an ID for the connection\")\n\t        if dataType is None:\n\t            raise Exception(\"Require a dataType specified\")\n", "        if paramName is None:\n\t            raise ValueError(\"Require a name for the Streaming Connection\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createStreamingConnection\")\n\t        obj = {\n\t            \"name\": name,\n\t            \"providerId\": \"521eee4d-8cbe-4906-bb48-fb6bd4450033\",\n\t            \"description\": description,\n\t            \"connectionSpec\": {\n\t                \"id\": \"bc7b00d6-623a-4dfc-9fdb-f1240aeadaeb\",\n", "                \"version\": \"1.0\",\n\t            },\n\t            \"auth\": {\n\t                \"specName\": kwargs.get(\"specName\", \"Streaming Connection\"),\n\t                \"params\": {\n\t                    \"sourceId\": sourceId,\n\t                    \"dataType\": dataType,\n\t                    \"name\": paramName,\n\t                },\n\t            },\n", "        }\n\t        res = self.createConnection(data=obj,responseType=kwargs.get('responseType','json'))\n\t        return res\n\t    def getConnection(self, connectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns a specific connection object.\n\t        Argument:\n\t            connectionId : REQUIRED : The ID of the connection you wish to retrieve.\n\t        \"\"\"\n\t        if connectionId is None:\n", "            raise Exception(\"Require a connectionId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getConnection\")\n\t        path = f\"/connections/{connectionId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def connectionTest(self, connectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Test a specific connection ID.\n\t        Argument:\n", "            connectionId : REQUIRED : The ID of the connection you wish to test.\n\t        \"\"\"\n\t        if connectionId is None:\n\t            raise Exception(\"Require a connectionId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting connectionTest\")\n\t        path: str = f\"/connections/{connectionId}/test\"\n\t        res: dict = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteConnection(self, connectionId: str = None) -> dict:\n", "        \"\"\"\n\t        Delete a specific connection ID.\n\t        Argument:\n\t            connectionId : REQUIRED : The ID of the connection you wish to delete.\n\t        \"\"\"\n\t        if connectionId is None:\n\t            raise Exception(\"Require a connectionId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteConnection\")\n\t        path: str = f\"/connections/{connectionId}\"\n", "        res: dict = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getConnectionSpecs(self) -> list:\n\t        \"\"\"\n\t        Returns the list of connectionSpecs in that instance.\n\t        If that doesn't work, return the response.\n\t        \"\"\"\n\t        path: str = \"/connectionSpecs\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getConnectionSpecs\")\n", "        res: dict = self.connector.getData(self.endpoint + path)\n\t        try:\n\t            data: list = res[\"items\"]\n\t            return data\n\t        except:\n\t            return res\n\t    def getConnectionSpecsMap(self) -> dict:\n\t        \"\"\"\n\t        Returns a mapping of connection spec name to connection spec ID.\n\t        If that doesn't work, return the response.\n", "        \"\"\"\n\t        specs_info = self.getConnectionSpecs()\n\t        return {spec[\"name\"]: spec[\"id\"] for spec in specs_info if \"id\" in spec and \"name\" in spec}\n\t    def getConnectionSpec(self, specId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns the detail for a specific connection.\n\t        Arguments:\n\t            specId : REQUIRED : The specification ID of a connection\n\t        \"\"\"\n\t        if specId is None:\n", "            raise Exception(\"Require a specId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getConnectionSpec\")\n\t        path: str = f\"/connectionSpecs/{specId}\"\n\t        res: dict = self.connector.getData(self.endpoint + path)\n\t        return res.get('items',[{}])[0]\n\t    def getConnectionSpecIdFromName(self, name: str = None) -> int:\n\t        \"\"\"\n\t        Returns the connection spec ID corresponding to a connection spec name.\n\t        Arguments:\n", "            name : REQUIRED : The specification name of a connection\n\t        \"\"\"\n\t        if name is None:\n\t            raise Exception(\"Require a name to be present\")\n\t        spec_name_to_id = self.getConnectionSpecsMap()\n\t        if name not in spec_name_to_id:\n\t            raise Exception(f\"Connection spec name '{name}' not found\")\n\t        return spec_name_to_id[name]\n\t    def getFlows(\n\t        self,\n", "        limit: int = 10,\n\t        n_results: int = 100,\n\t        prop: str = None,\n\t        filterMappingSetIds: list = None,\n\t        filterSourceIds: list = None,\n\t        filterTargetIds: list = None,\n\t        **kwargs,\n\t    ) -> list:\n\t        \"\"\"\n\t        Returns the flows set between Source and Target connection.\n", "        Arguments:\n\t            limit : OPTIONAL : number of results returned\n\t            n_results : OPTIONAL : total number of results returned (default 100, set to \"inf\" for retrieving everything)\n\t            prop : OPTIONAL : comma separated list of top-level object properties to be returned in the response.\n\t                Used to cut down the amount of data returned in the response body.\n\t                For example, prop=id==3416976c-a9ca-4bba-901a-1f08f66978ff,6a8d82bc-1caf-45d1-908d-cadabc9d63a6,3c9b37f8-13a6-43d8-bad3-b863b941fedd.\n\t            filterMappingSetId : OPTIONAL : returns only the flow that possess the mappingSetId passed in a list.\n\t            filterSourceIds : OPTIONAL : returns only the flow that possess the sourceConnectionIds passed in a list.\n\t            filterTargetIds : OPTIONAL : returns only the flow that possess the targetConnectionIds passed in a list.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFlows\")\n\t        params: dict = {\"limit\": limit, \"count\": kwargs.get(\"count\", False)}\n\t        if property is not None:\n\t            params[\"property\"] = prop\n\t        if kwargs.get(\"continuationToken\", False) != False:\n\t            params[\"continuationToken\"] = kwargs.get(\"continuationToken\")\n\t        path: str = \"/flows\"\n\t        res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t        token: str = res.get(\"_links\", {}).get(\"next\", {}).get(\"href\", \"\")\n", "        items = res[\"items\"]\n\t        while token != \"\" and len(items) < float(n_results):\n\t            continuationToken = token.split(\"=\")[1]\n\t            params[\"continuationToken\"] = continuationToken\n\t            res = self.connector.getData(self.endpoint + path, params=params)\n\t            token = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t            items += res[\"items\"]\n\t        self.data.flowId = {item[\"name\"]: item[\"id\"] for item in items}\n\t        self.data.flowSpecId = {item[\"name\"]: item.get(\"flowSpec\",{}).get('id') for item in items}\n\t        if filterMappingSetIds is not None:\n", "            filteredItems = []\n\t            for mappingsetId in filterMappingSetIds:\n\t                for item in items:\n\t                    if \"transformations\" in item.keys():\n\t                        for element in item[\"transformations\"]:\n\t                            if element[\"params\"].get(\"mappingId\", \"\") == mappingsetId:\n\t                                filteredItems.append(item)\n\t            items = filteredItems\n\t        if filterSourceIds is not None:\n\t            filteredItems = []\n", "            for sourceId in filterSourceIds:\n\t                for item in items:\n\t                    if sourceId in item[\"sourceConnectionIds\"]:\n\t                        filteredItems.append(item)\n\t            items = filteredItems\n\t        if filterTargetIds is not None:\n\t            filteredItems = []\n\t            for targetId in filterTargetIds:\n\t                for item in items:\n\t                    if targetId in item[\"targetConnectionIds\"]:\n", "                        filteredItems.append(item)\n\t            items = filteredItems\n\t        return items\n\t    def getFlow(self, flowId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns the details of a specific flow.\n\t        Arguments:\n\t            flowId : REQUIRED : the flow ID to be returned\n\t        \"\"\"\n\t        if flowId is None:\n", "            raise Exception(\"Require a flowId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFlow\")\n\t        path: str = f\"/flows/{flowId}\"\n\t        res: dict = self.connector.getData(self.endpoint + path)\n\t        return res.get('items',[{}])[0]\n\t    def deleteFlow(self, flowId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific flow by its ID.\n\t        Arguments:\n", "            flowId : REQUIRED : the flow ID to be returned\n\t        \"\"\"\n\t        if flowId is None:\n\t            raise Exception(\"Require a flowId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteFlow\")\n\t        path: str = f\"/flows/{flowId}\"\n\t        res: dict = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createFlow(\n", "        self,\n\t        flow_spec_id: str,\n\t        name: str = None,\n\t        source_connection_id: str = None,\n\t        target_connection_id: str = None,\n\t        schedule_start_time: str = None,\n\t        schedule_frequency: str = \"minute\",\n\t        schedule_interval: int = 15,\n\t        transformation_mapping_id: str = None,\n\t        transformation_name: str = None,\n", "        transformation_version: int = 0,\n\t        obj: dict = None,\n\t        version: str = \"1.0\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a flow with the API.\n\t        Arguments:\n\t            obj : REQUIRED : body to create the flow service.\n\t                Details can be seen at https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Flows/postFlow\n\t                requires following keys : name, flowSpec, sourceConnectionIds, targetConnectionIds, transformations, scheduleParams.\n", "        \"\"\"\n\t        if obj is None:\n\t            if any(param is None for param in [name, source_connection_id, target_connection_id]):\n\t                raise KeyError(\"Require either obj or all of 'name', 'source_connection_id', 'target_connection_id'\")\n\t            if schedule_frequency not in (\"minute\", \"hour\"):\n\t                raise ValueError(\"schedule frequency has to be either minute or hour\")\n\t            obj = {\n\t                \"name\": name,\n\t                \"flowSpec\": {\n\t                    \"id\": flow_spec_id,\n", "                    \"version\": version\n\t                },\n\t                \"sourceConnectionIds\": [\n\t                    source_connection_id\n\t                ],\n\t                \"targetConnectionIds\": [\n\t                    target_connection_id\n\t                ],\n\t                \"transformations\": [],\n\t                \"scheduleParams\": {}\n", "            }\n\t            if schedule_start_time is not None:\n\t                obj[\"scheduleParams\"][\"startTime\"] = schedule_start_time\n\t            if schedule_frequency is not None:\n\t                obj[\"scheduleParams\"][\"frequency\"] = schedule_frequency\n\t            if schedule_interval is not None:\n\t                obj[\"scheduleParams\"][\"interval\"] = str(schedule_interval)\n\t            if transformation_mapping_id is not None:\n\t                obj[\"transformations\"] = [\n\t                    {\n", "                        \"name\": transformation_name,\n\t                        \"params\": {\n\t                            \"mappingId\": transformation_mapping_id,\n\t                            \"mappingVersion\": transformation_version\n\t                        }\n\t                    }\n\t                ]\n\t        else:\n\t            if \"name\" not in obj.keys():\n\t                raise KeyError(\"missing 'name' parameter in the dictionary\")\n", "            if \"flowSpec\" not in obj.keys():\n\t                raise KeyError(\"missing 'flowSpec' parameter in the dictionary\")\n\t            if \"sourceConnectionIds\" not in obj.keys():\n\t                raise KeyError(\"missing 'sourceConnectionIds' parameter in the dictionary\")\n\t            if \"targetConnectionIds\" not in obj.keys():\n\t                raise KeyError(\"missing 'targetConnectionIds' parameter in the dictionary\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createFlow\")\n\t        path: str = \"/flows\"\n\t        res: dict = self.connector.postData(self.endpoint + path, data=obj)\n", "        return res\n\t    def createFlowDataLakeToDataLandingZone(\n\t        self,\n\t        name: str,\n\t        source_connection_id: str,\n\t        target_connection_id: str,\n\t        schedule_start_time: str,\n\t        schedule_frequency: str = \"hour\",\n\t        schedule_interval: int = 3,\n\t        transformation_mapping_id: str = None,\n", "        transformation_name: str = None,\n\t        transformation_version: int = 0,\n\t        version: str = \"1.0\",\n\t        flow_spec_name: str = \"Data Landing Zone\",\n\t        source_spec_name: str = \"activation-datalake\",\n\t        target_spec_name: str = \"Data Landing Zone\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a Data Flow to move data from Data Lake to the Data Landing Zone.\n\t        Arguments:\n", "            name : REQUIRED : The name of the Data Flow.\n\t            source_connection_id : REQUIRED : The ID of the source connection tied to Data Lake.\n\t            target_connection_id : REQUIRED : The ID of the target connection tied to Data Landing Zone.\n\t            schedule_start_time : REQUIRED : The time from which the Data Flow should start running.\n\t            schedule_frequency : OPTIONAL : The granularity of the Data Flow. Currently only \"hour\" supported.\n\t            schedule_interval : OPTIONAL : The interval on which the Data Flow runs. Either 3, 6, 9, 12 or 24. Default to 3.\n\t            transformation_mapping_id : OPTIONAL : If a transformation is required, its mapping ID.\n\t            transformation_name : OPTIONAL : If a transformation is required, its name.\n\t            transformation_version : OPTIONAL : If a transformation is required, its version.\n\t            version : OPTIONAL : The version of the Data Flow.\n", "            flow_spec_name : OPTIONAL : The name of the Data Flow specification. Same for all customers.\n\t        \"\"\"\n\t        flow_spec_id = self.getFlowSpecIdFromNames(flow_spec_name, source_spec_name, target_spec_name)\n\t        return self.createFlow(\n\t            flow_spec_id=flow_spec_id,\n\t            name=name,\n\t            source_connection_id=source_connection_id,\n\t            target_connection_id=target_connection_id,\n\t            schedule_start_time=schedule_start_time,\n\t            schedule_frequency=schedule_frequency,\n", "            schedule_interval=schedule_interval,\n\t            transformation_mapping_id=transformation_mapping_id,\n\t            transformation_name=transformation_name,\n\t            transformation_version=transformation_version,\n\t            version=version\n\t        )\n\t    def createFlowDataLandingZoneToDataLake(\n\t        self,\n\t        name: str,\n\t        source_connection_id: str,\n", "        target_connection_id: str,\n\t        schedule_start_time: str,\n\t        schedule_frequency: str = \"minute\",\n\t        schedule_interval: int = 15,\n\t        transformation_mapping_id: str = None,\n\t        transformation_name: str = None,\n\t        transformation_version: int = 0,\n\t        version: str = \"1.0\",\n\t        flow_spec_name: str = \"CloudStorageToAEP\",\n\t        source_spec_name: str = \"landing-zone\",\n", "        target_spec_name: str = \"datalake\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a Data Flow to move data from Data Lake to the Data Landing Zone.\n\t        Arguments:\n\t            name : REQUIRED : The name of the Data Flow.\n\t            source_connection_id : REQUIRED : The ID of the source connection tied to Data Lake.\n\t            target_connection_id : REQUIRED : The ID of the target connection tied to Data Landing Zone.\n\t            schedule_start_time : REQUIRED : The time from which the Data Flow should start running.\n\t            schedule_frequency : OPTIONAL : The granularity of the Data Flow. Can be \"hour\" or \"minute\". Default to \"minute\".\n", "            schedule_interval : OPTIONAL : The interval on which the Data Flow runs. Default to 15\n\t            transformation_mapping_id : OPTIONAL : If a transformation is required, its mapping ID.\n\t            transformation_name : OPTIONAL : If a transformation is required, its name.\n\t            transformation_version : OPTIONAL : If a transformation is required, its version.\n\t            version : OPTIONAL : The version of the Data Flow.\n\t            flow_spec_name : OPTIONAL : The name of the Data Flow specification. Same for all customers.\n\t        \"\"\"\n\t        flow_spec_id = self.getFlowSpecIdFromNames(flow_spec_name, source_spec_name, target_spec_name)\n\t        return self.createFlow(\n\t            flow_spec_id=flow_spec_id,\n", "            name=name,\n\t            source_connection_id=source_connection_id,\n\t            target_connection_id=target_connection_id,\n\t            schedule_start_time=schedule_start_time,\n\t            schedule_frequency=schedule_frequency,\n\t            schedule_interval=schedule_interval,\n\t            transformation_mapping_id=transformation_mapping_id,\n\t            transformation_name=transformation_name,\n\t            transformation_version=transformation_version,\n\t            version=version\n", "        )\n\t    def updateFlow(\n\t        self, flowId: str = None, etag: str = None, updateObj: list = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        update the flow based on the operation provided.\n\t        Arguments:\n\t            flowId : REQUIRED : the ID of the flow to Patch.\n\t            etag : REQUIRED : ETAG value for patching the Flow.\n\t            updateObj : REQUIRED : List of operation to realize on the flow.\n", "            Follow the following structure:\n\t            [\n\t                {\n\t                    \"op\": \"Add\",\n\t                    \"path\": \"/auth/params\",\n\t                    \"value\": {\n\t                    \"description\": \"A new description to provide further context on a specified connection or flow.\"\n\t                    }\n\t                }\n\t            ]\n", "        \"\"\"\n\t        if flowId is None:\n\t            raise Exception(\"Require a flow ID to be present\")\n\t        if etag is None:\n\t            raise Exception(\"Require etag to be present\")\n\t        if updateObj is None:\n\t            raise Exception(\"Require a list with data to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateFlow\")\n\t        privateHeader = deepcopy(self.header)\n", "        privateHeader[\"if-match\"] = etag\n\t        path: str = f\"/flows/{flowId}\"\n\t        res: dict = self.connector.patchData(\n\t            self.endpoint + path, headers=privateHeader, data=updateObj\n\t        )\n\t        return res\n\t    def getFlowSpecs(self, prop: str = None) -> list:\n\t        \"\"\"\n\t        Returns the flow specifications.\n\t        Arguments:\n", "            prop : OPTIONAL : A comma separated list of top-level object properties to be returned in the response.\n\t                Used to cut down the amount of data returned in the response body.\n\t                For example, prop=id==3416976c-a9ca-4bba-901a-1f08f66978ff,6a8d82bc-1caf-45d1-908d-cadabc9d63a6,3c9b37f8-13a6-43d8-bad3-b863b941fedd.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFlowSpecs\")\n\t        path: str = \"/flowSpecs\"\n\t        params = {}\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n", "        res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t        items: list = res[\"items\"]\n\t        return items\n\t    def getFlowSpecIdFromNames(\n\t        self,\n\t        flow_spec_name: str,\n\t        source_spec_name: str = None,\n\t        target_spec_name: str = None\n\t    ) -> str:\n\t        \"\"\"\n", "        Return the Flow specification ID corresponding to some conditions..\n\t        Arguments:\n\t            flow_spec_name : REQUIRED : The flow specification name to look for\n\t            source_spec_name : OPTIONAL : Additional filter to only return a flow with a source specification ID.\n\t            target_spec_name : OPTIONAL : Additional filter to only return a flow with a target specification ID.\n\t        \"\"\"\n\t        flows = self.getFlowSpecs(f\"name=={flow_spec_name}\")\n\t        if source_spec_name is not None:\n\t            source_spec_id = self.getConnectionSpecIdFromName(source_spec_name)\n\t            flows = [flow for flow in flows if source_spec_id in flow[\"sourceConnectionSpecIds\"]]\n", "        if target_spec_name is not None:\n\t            target_spec_id = self.getConnectionSpecIdFromName(target_spec_name)\n\t            flows = [flow for flow in flows if target_spec_id in flow[\"targetConnectionSpecIds\"]]\n\t        if len(flows) != 1:\n\t            raise Exception(f\"Expected a single flow specification mapping to flow name '{flow_spec_name}', \"\n\t                            f\"source spec name '{source_spec_name}' and target spec name '{target_spec_name}'\"\n\t                            f\"but got {len(flows)}\")\n\t        flow_spec_id = flows[0][\"id\"]\n\t        return flow_spec_id\n\t    def getFlowSpec(self, flowSpecId) -> dict:\n", "        \"\"\"\n\t        Return the detail of a specific flow ID Spec\n\t        Arguments:\n\t            flowSpecId : REQUIRED : The flow ID spec to be checked\n\t        \"\"\"\n\t        if flowSpecId is None:\n\t            raise Exception(\"Require a flowSpecId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getFlowSpec\")\n\t        path: str = f\"/flowSpecs/{flowSpecId}\"\n", "        res: dict = self.connector.getData(self.endpoint + path)\n\t        return res.get('items',[{}])[0]\n\t    def getRuns(\n\t        self, limit: int = 10, n_results: int = 100, prop: str = None, **kwargs\n\t    ) -> list:\n\t        \"\"\"\n\t        Returns the list of runs. Runs are instances of a flow execution.\n\t        Arguments:\n\t            limit : OPTIONAL : number of results returned per request\n\t            n_results : OPTIONAL : total number of results returned (default 100, set to \"inf\" for retrieving everything)\n", "            prop : OPTIONAL : comma separated list of top-level object properties to be returned in the response.\n\t                Used to cut down the amount of data returned in the response body.\n\t                For example, prop=id==3416976c-a9ca-4bba-901a-1f08f66978ff,6a8d82bc-1caf-45d1-908d-cadabc9d63a6,3c9b37f8-13a6-43d8-bad3-b863b941fedd.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getRuns\")\n\t        path = \"/runs\"\n\t        params = {\"limit\": limit, \"count\": kwargs.get(\"count\", False)}\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n", "        if kwargs.get(\"continuationToken\", False):\n\t            params[\"continuationToken\"] = kwargs.get(\"continuationToken\")\n\t        res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t        items: list = res.get(\"items\",[])\n\t        nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\" and len(items) < float(n_results):\n\t            token: str = res[\"_links\"][\"next\"].get(\"href\", \"\")\n\t            continuationToken: str = token.split(\"=\")[1]\n\t            params[\"continuationToken\"] = continuationToken\n\t            res = self.connector.getData(self.endpoint + path, params=params)\n", "            items += res.get('items')\n\t            nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        return items\n\t    def createRun(self, flowId: str = None, status: str = \"active\") -> dict:\n\t        \"\"\"\n\t        Generate a run based on the flowId.\n\t        Arguments:\n\t            flowId : REQUIRED : the flow ID to run\n\t            stats : OPTIONAL : Status of the flow\n\t        \"\"\"\n", "        path = \"/runs\"\n\t        if flowId is None:\n\t            raise Exception(\"Require a flowId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createRun\")\n\t        obj = {\"flowId\": flowId, \"status\": status}\n\t        res: dict = self.connector.postData(self.endpoint + path, data=obj)\n\t        return res\n\t    def getRun(self, runId: str = None) -> dict:\n\t        \"\"\"\n", "        Return a specific runId.\n\t        Arguments:\n\t            runId : REQUIRED : the run ID to return\n\t        \"\"\"\n\t        if runId is None:\n\t            raise Exception(\"Require a runId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getRun\")\n\t        path: str = f\"/runs/{runId}\"\n\t        res: dict = self.connector.getData(self.endpoint + path)\n", "        return res\n\t    def getSourceConnections(self, n_results: int = 100, **kwargs) -> list:\n\t        \"\"\"\n\t        Return the list of source connections\n\t        Arguments:\n\t            n_results : OPTIONAL : total number of results returned (default 100, set to \"inf\" for retrieving everything)\n\t        kwargs will be added as query parameterss\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSourceConnections\")\n", "        params = {**kwargs}\n\t        path: str = f\"/sourceConnections\"\n\t        res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t        data: list = res[\"items\"]\n\t        nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\" and len(data) < float(n_results):\n\t            continuationToken = nextPage.split(\"=\")[1]\n\t            params[\"continuationToken\"] = continuationToken\n\t            res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t            data += res[\"items\"]\n", "            nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        return data\n\t    def getSourceConnection(self, sourceConnectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Return detail of the sourceConnection ID\n\t        Arguments:\n\t            sourceConnectionId : REQUIRED : The source connection ID to be retrieved\n\t        \"\"\"\n\t        if sourceConnectionId is None:\n\t            raise Exception(\"Require a sourceConnectionId to be present\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSourceConnection\")\n\t        path: str = f\"/sourceConnections/{sourceConnectionId}\"\n\t        res: dict = self.connector.getData(self.endpoint + path)\n\t        return res.get('items',[{}])[0]\n\t    def deleteSourceConnection(self, sourceConnectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a sourceConnection ID\n\t        Arguments:\n\t            sourceConnectionId : REQUIRED : The source connection ID to be deleted\n", "        \"\"\"\n\t        if sourceConnectionId is None:\n\t            raise Exception(\"Require a sourceConnectionId to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteSourceConnection\")\n\t        path: str = f\"/sourceConnections/{sourceConnectionId}\"\n\t        res: dict = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createSourceConnection(self, data: dict = None) -> dict:\n\t        \"\"\"\n", "        Create a sourceConnection based on the dictionary passed.\n\t        Arguments:\n\t            obj : REQUIRED : the data to be passed for creation of the Source Connection.\n\t                Details can be seen at https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Source_connections/postSourceConnection\n\t                requires following keys : name, baseConnectionId, data, params, connectionSpec.\n\t        \"\"\"\n\t        if data is None:\n\t            raise Exception(\"Require a dictionary with data to be present\")\n\t        if \"name\" not in data.keys():\n\t            raise KeyError(\"Require a 'name' key in the dictionary passed\")\n", "        if \"connectionSpec\" not in data.keys():\n\t            raise KeyError(\"Require a 'connectionSpec' key in the dictionary passed\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSourceConnection\")\n\t        path: str = f\"/sourceConnections\"\n\t        res: dict = self.connector.postData(self.endpoint + path, data=data)\n\t        return res\n\t    def createSourceConnectionStreaming(\n\t        self,\n\t        connectionId: str = None,\n", "        name: str = None,\n\t        format: str = \"delimited\",\n\t        description: str = \"\",\n\t        spec_name: str = \"Streaming Connection\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a source connection based on streaming connection created.\n\t        Arguments:\n\t            connectionId : REQUIRED : The Streaming connection ID.\n\t            name : REQUIRED : Name of the Connection.\n", "            format : REQUIRED : format of the data sent (default : delimited)\n\t            description : REQUIRED : Description of of the Connection Source.\n\t            spec_name : OPTIONAL : The name of the source specification corresponding to Streaming.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSourceConnectionStreaming\")\n\t        spec_id = self.getConnectionSpecIdFromName(spec_name)\n\t        obj = {\n\t            \"name\": name,\n\t            \"providerId\": \"521eee4d-8cbe-4906-bb48-fb6bd4450033\",\n", "            \"description\": description,\n\t            \"baseConnectionId\": connectionId,\n\t            \"connectionSpec\": {\n\t                \"id\": spec_id,\n\t                \"version\": \"1.0\",\n\t            },\n\t            \"data\": {\"format\": format},\n\t        }\n\t        res = self.createSourceConnection(data=obj)\n\t        return res\n", "    def createSourceConnectionDataLandingZone(\n\t        self,\n\t        name: str = None,\n\t        format: str = \"delimited\",\n\t        path: str = None,\n\t        type: str = \"file\",\n\t        recursive: bool = False,\n\t        spec_name: str = \"landing-zone\"\n\t    ) -> dict:\n\t        \"\"\"\n", "        Create a new data landing zone connection.\n\t        Arguments:\n\t            name : REQUIRED : A name for the connection\n\t            format : REQUIRED : The type of data type loaded. Default \"delimited\". Can be \"json\" or \"parquet\" \n\t            path : REQUIRED : The path to the data you want to ingest. Can be a single file or folder.\n\t            type : OPTIONAL : Use \"file\" if path refers to individual file, otherwise \"folder\".\n\t            recursive : OPTIONAL : Whether to look for files recursively under the path or not.\n\t            spec_name : OPTIONAL : The name of the source specification corresponding to Data Landing Zone.\n\t        \"\"\"\n\t        if name is None:\n", "            raise ValueError(\"Require a name for the connection\")\n\t        spec_id = self.getConnectionSpecIdFromName(spec_name)\n\t        obj = {\n\t            \"name\": name,\n\t            \"data\": {\n\t                \"format\": format\n\t            },\n\t            \"params\": {\n\t                \"path\": path,\n\t                \"type\": type,\n", "                \"recursive\": recursive\n\t            },\n\t            \"connectionSpec\": {\n\t                \"id\": spec_id,\n\t                \"version\": \"1.0\"\n\t            }\n\t        }\n\t        res = self.createSourceConnection(obj)\n\t        return res\n\t    def createSourceConnectionDataLake(\n", "        self,\n\t        name: str = None,\n\t        format: str = \"delimited\",\n\t        dataset_ids: list = [],\n\t        spec_name: str = \"activation-datalake\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a new data lake connection.\n\t        Arguments:\n\t            name : REQUIRED : A name for the connection\n", "            format : REQUIRED : The type of data type loaded. Default \"delimited\". Can be \"json\" or \"parquet\"\n\t            dataset_ids : REQUIRED : A list of dataset IDs acting as a source of data.\n\t            spec_name : OPTIONAL : The name of the source specification corresponding to Data Lake.\n\t        \"\"\"\n\t        if name is None:\n\t            raise ValueError(\"Require a name for the connection\")\n\t        if len(dataset_ids) == 0:\n\t            raise ValueError(\"Expected at least 1 dataset ID to be passed\")\n\t        spec_id = self.getConnectionSpecIdFromName(spec_name)\n\t        obj = {\n", "            \"name\": name,\n\t            \"data\": {\n\t                \"format\": format\n\t            },\n\t            \"connectionSpec\": {\n\t                \"id\": spec_id,\n\t                \"version\": \"1.0\"\n\t            },\n\t            \"params\": {\n\t                \"datasets\": [{\n", "                    \"dataSetId\": dataset_id,\n\t                } for dataset_id in dataset_ids]\n\t            }\n\t        }\n\t        res = self.createSourceConnection(obj)\n\t        return res\n\t    def updateSourceConnection(\n\t        self, sourceConnectionId: str = None, etag: str = None, updateObj: list = None\n\t    ) -> dict:\n\t        \"\"\"\n", "        Update a source connection based on the ID provided with the object provided.\n\t        Arguments:\n\t            sourceConnectionId : REQUIRED : The source connection ID to be updated\n\t            etag: REQUIRED : A header containing the etag value of the connection or flow to be updated.\n\t            updateObj : REQUIRED : The operation call used to define the action needed to update the connection. Operations include add, replace, and remove.\n\t        \"\"\"\n\t        if sourceConnectionId is None:\n\t            raise Exception(\"Require a sourceConnection to be present\")\n\t        if etag is None:\n\t            raise Exception(\"Require etag to be present\")\n", "        if updateObj is None:\n\t            raise Exception(\"Require a list with data to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateSourceConnection\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"if-match\"] = etag\n\t        path: str = f\"/sourceConnections/{sourceConnectionId}\"\n\t        res: dict = self.connector.patchData(\n\t            self.endpoint + path, headers=privateHeader, data=updateObj\n\t        )\n", "        return res\n\t    def getTargetConnections(self, n_results: int = 100, **kwargs) -> dict:\n\t        \"\"\"\n\t        Return the target connections\n\t        Arguments:\n\t            n_results : OPTIONAL : total number of results returned (default 100, set to \"inf\" for retrieving everything)\n\t        kwargs will be added as query parameterss\n\t        \"\"\"\n\t        params = {**kwargs}\n\t        path: str = f\"/targetConnections\"\n", "        res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t        data: list = res[\"items\"]\n\t        nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        while nextPage != \"\" and len(data) < float(n_results):\n\t            continuationToken = nextPage.split(\"=\")[1]\n\t            params[\"continuationToken\"] = continuationToken\n\t            res: dict = self.connector.getData(self.endpoint + path, params=params)\n\t            data += res[\"items\"]\n\t            nextPage = res[\"_links\"].get(\"next\", {}).get(\"href\", \"\")\n\t        return data\n", "    def getTargetConnection(self, targetConnectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Retrieve a specific Target connection detail.\n\t        Arguments:\n\t            targetConnectionId : REQUIRED : The target connection ID is a unique identifier used to create a flow.\n\t        \"\"\"\n\t        if targetConnectionId is None:\n\t            raise Exception(\"Require a target connection ID to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getTargetConnection\")\n", "        path: str = f\"/targetConnections/{targetConnectionId}\"\n\t        res: dict = self.connector.getData(self.endpoint + path)\n\t        return res.get('items',[None])[0]\n\t    def deleteTargetConnection(self, targetConnectionId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific Target connection detail\n\t        Arguments:\n\t             targetConnectionId : REQUIRED : The target connection ID to be deleted\n\t        \"\"\"\n\t        if targetConnectionId is None:\n", "            raise Exception(\"Require a target connection ID to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteTargetConnection\")\n\t        path: str = f\"/targetConnections/{targetConnectionId}\"\n\t        res: dict = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createTargetConnection(\n\t        self,\n\t        name: str = None,\n\t        connectionSpecId: str = None,\n", "        datasetId: str = None,\n\t        format: str = \"parquet_xdm\",\n\t        version: str = \"1.0\",\n\t        description: str = \"\",\n\t        data: dict = None,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a new target connection\n\t        Arguments:\n\t                name : REQUIRED : The name of the target connection\n", "                connectionSpecId : REQUIRED : The connectionSpecId to use.\n\t                datasetId : REQUIRED : The dataset ID that is the target\n\t                version : REQUIRED : version to be used (1.0 by default)\n\t                format : REQUIRED : Data format to be used (parquet_xdm by default)\n\t                description : OPTIONAL : description of your target connection\n\t                data : OPTIONAL : If you pass the complete dictionary for creation\n\t        Details can be seen at https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Target_connections/postTargetConnection\n\t        requires following keys : name, data, params, connectionSpec.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting createTargetConnection\")\n\t        path: str = f\"/targetConnections\"\n\t        if data is not None and type(data) == dict:\n\t            obj = data\n\t            res: dict = self.connector.postData(self.endpoint + path, data=obj)\n\t        else:\n\t            if name is None:\n\t                raise ValueError(\"Require a name to be passed\")\n\t            if connectionSpecId is None:\n\t                raise ValueError(\"Require a connectionSpec Id to be passed\")\n", "            if datasetId is None:\n\t                raise ValueError(\"Require a datasetId to be passed\")\n\t            obj = {\n\t                \"name\": name,\n\t                \"description\": description,\n\t                \"connectionSpec\": {\"id\": connectionSpecId, \"version\": version},\n\t                \"data\": {\"format\": format},\n\t                \"params\": {\"dataSetId\": datasetId},\n\t            }\n\t            res: dict = self.connector.postData(self.endpoint + path, data=obj)\n", "        return res\n\t    def createTargetConnectionDataLandingZone(\n\t        self,\n\t        name: str = None,\n\t        format: str = \"delimited\",\n\t        path: str = None,\n\t        type: str = \"file\",\n\t        version: str = \"1.0\",\n\t        description: str = \"\",\n\t        spec_name: str = \"Data Landing Zone\"\n", "    ) -> dict:\n\t        \"\"\"\n\t        Create a target connection to the Data Landing Zone\n\t        Arguments:\n\t                name : REQUIRED : The name of the target connection\n\t                format : REQUIRED : Data format to be used\n\t                path : REQUIRED : The path to the data you want to ingest. Can be a single file or folder.\n\t                type : OPTIONAL : Use \"file\" if path refers to individual file, otherwise \"folder\".\n\t                version : REQUIRED : version of your target destination\n\t                description : OPTIONAL : description of your target destination.\n", "                spec_name : OPTIONAL : The name of the target specification corresponding to Data Lake.\n\t        \"\"\"\n\t        if name is None:\n\t            raise ValueError(\"Require a name for the connection\")\n\t        spec_id = self.getConnectionSpecIdFromName(spec_name)\n\t        obj = {\n\t            \"name\": name,\n\t            \"description\": description,\n\t            \"data\": {\n\t                \"format\": format\n", "            },\n\t            \"params\": {\n\t                \"path\": path,\n\t                \"type\": type\n\t            },\n\t            \"connectionSpec\": {\n\t                \"id\": spec_id,\n\t                \"version\": version\n\t            }\n\t        }\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createTargetConnectionDataLandingZone\")\n\t        res = self.createTargetConnection(data=obj)\n\t        return res\n\t    def createTargetConnectionDataLake(\n\t        self,\n\t        name: str = None,\n\t        datasetId: str = None,\n\t        schemaId: str = None,\n\t        format: str = \"delimited\",\n", "        version: str = \"1.0\",\n\t        description: str = \"\",\n\t        spec_name: str = \"datalake\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a target connection to the AEP Data Lake.\n\t        Arguments:\n\t            name : REQUIRED : The name of your target Destination\n\t            datasetId : REQUIRED : the dataset ID of your target destination.\n\t            schemaId : REQUIRED : The schema ID of your dataSet. (NOT meta:altId)\n", "            format : REQUIRED : format of your data inserted\n\t            version : REQUIRED : version of your target destination\n\t            description : OPTIONAL : description of your target destination.\n\t            spec_name : OPTIONAL : The name of the target specification corresponding to Data Lake.\n\t        \"\"\"\n\t        if name is None:\n\t            raise ValueError(\"Require a name for the connection\")\n\t        spec_id = self.getConnectionSpecIdFromName(spec_name)\n\t        targetObj = {\n\t            \"name\": name,\n", "            \"description\": description,\n\t            \"data\": {\n\t                \"format\": format,\n\t                \"schema\": {\n\t                    \"id\": schemaId,\n\t                    \"version\": \"application/vnd.adobe.xed-full+json;version=1.0\",\n\t                },\n\t            },\n\t            \"params\": {\"dataSetId\": datasetId},\n\t            \"connectionSpec\": {\n", "                \"id\": spec_id,\n\t                \"version\": version,\n\t            },\n\t        }\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createTargetConnectionDataLake\")\n\t        res = self.createTargetConnection(data=targetObj)\n\t        return res\n\t    def updateTargetConnection(\n\t        self, targetConnectionId: str = None, etag: str = None, updateObj: list = None\n", "    ) -> dict:\n\t        \"\"\"\n\t        Update a target connection based on the ID provided with the object provided.\n\t        Arguments:\n\t            targetConnectionId : REQUIRED : The target connection ID to be updated\n\t            etag: REQUIRED : A header containing the etag value of the connection or flow to be updated.\n\t            updateObj : REQUIRED : The operation call used to define the action needed to update the connection. Operations include add, replace, and remove.\n\t        \"\"\"\n\t        if targetConnectionId is None:\n\t            raise Exception(\"Require a sourceConnection to be present\")\n", "        if etag is None:\n\t            raise Exception(\"Require etag to be present\")\n\t        if updateObj is None:\n\t            raise Exception(\"Require a dictionary with data to be present\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateTargetConnection\")\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"if-match\"] = etag\n\t        path: str = f\"/targetConnections/{targetConnectionId}\"\n\t        res: dict = self.connector.patchData(\n", "            self.endpoint + path, headers=privateHeader, data=updateObj\n\t        )\n\t        return res\n\t    def updatePolicy(self,flowId:str=None, policies:Union[list,str]=None, operation:str=\"Replace\")->dict:\n\t        \"\"\"\n\t        By passing the policy IDs as a list, we update the Policies apply to this Flow.\n\t        Arguments:\n\t            flowId : REQUIRED : The Flow ID to be updated\n\t            policies : REQUIRED : The list of policies Id to add to the Flow\n\t                example of value: \"/dulepolicy/marketingActions/06621fe3q-44t3-3zu4t-90c2-y653rt3hk4o499\"\n", "            operation : OPTIONAL : By default \"replace\" the current policies. It can be an \"add\" operation.\n\t        \"\"\"\n\t        if flowId is None:\n\t            raise ValueError(\"Require a Flow ID\")\n\t        if policies is None:\n\t            raise ValueError(\"Require a list of policy ID\")\n\t        if type(policies) == str:\n\t            policies = [policies]\n\t        if type(policies) != list:\n\t            raise TypeError(\"The policiy ID were not passed via a string or a list of string\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updatePolicy\")\n\t        op = [\n\t            {\n\t                \"op\" : operation,\n\t                \"path\":\"/policy\",\n\t                \"value\":{\n\t                    \"enforcementRefs\":policies\n\t                }\n\t            }\n", "        ]\n\t        res = self.updateFlow(flowId=flowId, operation=op)\n\t        return res\n\t    def getLandingZoneContainer(\n\t        self,\n\t        dlz_type: str = \"user_drop_zone\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns a dictionary of the available Data Landing Zone container information.\n\t        Arguments:\n", "            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getLandingZoneContainer\")\n\t        path = f\"/data/foundation/connectors/landingzone\"\n\t        params = {\"type\": dlz_type}\n\t        res = self.connector.getData(self.endpoint_gloal + path, params=params)\n\t        return res\n\t    def getLandingZoneContainerName(\n\t        self,\n", "        dlz_type: str = \"user_drop_zone\"\n\t    ) -> str:\n\t        \"\"\"\n\t        Returns the name of the DLZ container corresponding to this type.\n\t        Arguments:\n\t            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        return self.getLandingZoneContainer(dlz_type=dlz_type)[\"containerName\"]\n\t    def getLandingZoneContainerTTL(\n\t        self,\n", "        dlz_type: str = \"user_drop_zone\"\n\t    ) -> int:\n\t        \"\"\"\n\t        Returns the TTL in days of the DLZ container corresponding to this type.\n\t        Arguments:\n\t            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        return int(self.getLandingZoneContainer(dlz_type=dlz_type)[\"containerTTL\"])\n\t    def getLandingZoneCredential(\n\t        self,\n", "        dlz_type: str = \"user_drop_zone\"\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns a dictionary with the credential to be used in order to create a new zone\n\t        Arguments:\n\t            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getLandingZoneCredential\")\n\t        path = f\"/data/foundation/connectors/landingzone/credentials\"\n", "        params = {\"type\": dlz_type}\n\t        res = self.connector.getData(self.endpoint_gloal + path,params=params)\n\t        return res\n\t    def getLandingZoneSASUri(\n\t        self,\n\t        dlz_type: str = \"user_drop_zone\"\n\t    ) -> str:\n\t        \"\"\"\n\t        Returns the SAS URI of the DLZ container corresponding to this type.\n\t        Arguments:\n", "            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        return self.getLandingZoneCredential(dlz_type=dlz_type)[\"SASUri\"]\n\t    def getLandingZoneSASToken(\n\t        self,\n\t        dlz_type: str = \"user_drop_zone\"\n\t    ) -> str:\n\t        \"\"\"\n\t        Returns the SAS token of the DLZ container corresponding to this type.\n\t        Arguments:\n", "            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        return self.getLandingZoneCredential(dlz_type=dlz_type)[\"SASToken\"]\n\t    def getLandingZoneStorageAccountName(\n\t        self,\n\t        dlz_type: str = \"user_drop_zone\"\n\t    ) -> str:\n\t        \"\"\"\n\t        Returns the storage account name of the DLZ container corresponding to this type.\n\t        Arguments:\n", "            dlz_type : OPTIONAL : The type of DLZ container - default to \"user_drop_zone\" but can be \"dlz_destination\"\n\t        \"\"\"\n\t        return self.getLandingZoneCredential(dlz_type=dlz_type)[\"storageAccountName\"]\n\t    def exploreLandingZone(self,fileType:str='delimited')->list:\n\t        \"\"\"\n\t        Return the structure of your landing zones\n\t        Arguments:\n\t            fileType : OPTIONAL : The type of the file to see.\n\t        \"\"\"\n\t        path =\"/connectionSpecs/26f526f2-58f4-4712-961d-e41bf1ccc0e8/explore\"\n", "        params = {\"objectType\":\"root\"}\n\t        res = self.connector.getData(self.endpoint + path,params=params)\n\t        return res\n\t    def getLandingZoneContent(self,fileType:str=\"delimited\",file:str=None,determineProperties:bool=True,preview:bool=True)->list:\n\t        \"\"\"\n\t        Return the structure of your landing zones\n\t        Arguments:\n\t            fileType : OPTIONAL : The type of the file to see.\n\t                Possible option : \"delimited\", \"json\" or \"parquet\"\n\t            file : OPTIONAL : the path to the specific file.\n", "            determineProperties : OPTIONAL : replace other parameter to auto-detect file properties.\n\t            preview : OPTIONAL : If you wish to see a preview of the file.\n\t        \"\"\"\n\t        path =\"/connectionSpecs/26f526f2-58f4-4712-961d-e41bf1ccc0e8/explore\"\n\t        params = {\"objectType\":\"file\",\"preview\":preview,}\n\t        if determineProperties:\n\t            params['determineProperties'] = True\n\t        if determineProperties == False and fileType is not None:\n\t            params['FILE_TYPE'] = fileType\n\t        if file:\n", "            params['object'] = file\n\t        res = self.connector.getData(self.endpoint + path,params=params)\n\t        return res\n\t    def postFlowAction(self,flowId:str,action:str)->dict:\n\t        if flowId is None:\n\t            raise Exception(\"Requires a flowId to be present\")\n\t        if action is None:\n\t            raise Exception(\"Requires an action to be present\")\n\t        path = \"/flows/\" + flowId + \"/action?op=\" + action\n\t        privateHeader = deepcopy(self.header)\n", "        privateHeader.pop(\"Content-Type\")\n\t        res = self.connector.postData(endpoint=self.endpoint + path, headers=privateHeader)\n\t        return res\n\tclass FlowManager:\n\t    \"\"\"\n\t    A class that abstract the different information retrieved by the Flow ID in order to provide all relationships inside that Flow.\n\t    It takes a flow id and dig to all relationship inside that flow.\n\t    \"\"\"\n\t    def __init__(self,\n\t                flowId:str=None,\n", "                config: dict = aepp.config.config_object,\n\t                header=aepp.config.header)->None:\n\t        \"\"\"\n\t        Instantiate a Flow Manager Instance based on the flow ID.\n\t        Arguments:\n\t            flowId : REQUIRED : A flow ID\n\t        \"\"\"\n\t        from aepp import schema, catalog,dataprep,flowservice\n\t        self.schemaAPI = schema.Schema(config=config)\n\t        self.catalogAPI = catalog.Catalog(config=config)\n", "        self.mapperAPI = dataprep.DataPrep(config=config)\n\t        self.flowAPI = flowservice.FlowService(config=config)\n\t        self.flowData = self.flowAPI.getFlow(flowId)\n\t        self.__setAttributes__(self.flowData)\n\t        self.flowMapping = None\n\t        self.datasetId = None\n\t        self.flowSpec = {'id' : self.flowData.get('flowSpec',{}).get('id')}\n\t        self.flowSourceConnection = {'id' : self.flowData.get('sourceConnectionIds',[None])[0]}\n\t        self.flowTargetConnection = {'id' : self.flowData.get('targetConnectionIds',[None])[0]}\n\t        sourceConnections:list = self.flowData.get('inheritedAttributes',{}).get('sourceConnections',[{}])\n", "        self.connectionInfo = {}\n\t        for element in sourceConnections:\n\t            if 'typeInfo' in element.keys():\n\t                self.connectionInfo = {'id':element.get('id'),'name':element.get('typeInfo',{}).get('id')}\n\t            if 'baseConnection' in element.keys():\n\t                self.connectionInfo = {'id':element.get('baseConnection',{}).get('id'),'name':None}\n\t        if self.connectionInfo.get('id',None) is not None and self.connectionInfo.get('name',None) is None:\n\t            self.connectionInfo['name'] = self.flowAPI.getConnection(self.connectionInfo['id']).get('items',[{}])[0].get('name')\n\t        for trans in self.flowData.get('transformations',[{}]):\n\t            if trans.get('name') == 'Mapping':\n", "                self.flowMapping = {'id':trans.get('params',{}).get('mappingId')}\n\t        ## Flow Spec part\n\t        if self.flowSpec['id'] is not None:\n\t            flowSpecData = self.flowAPI.getFlowSpec(self.flowSpec['id'])\n\t            self.flowSpec['name'] = flowSpecData['name']\n\t            self.flowSpec['frequency'] = flowSpecData.get('attributes',{}).get('frequency')\n\t        ## Source Connection part\n\t        if self.flowSourceConnection['id'] is not None:\n\t            sourceConnData = self.flowAPI.getSourceConnection(self.flowSourceConnection['id'])\n\t            self.flowSourceConnection['data'] = sourceConnData.get('data')\n", "            self.flowSourceConnection['params'] = sourceConnData.get('params')\n\t            self.flowSourceConnection['connectionSpec'] = sourceConnData.get('connectionSpec')\n\t            if self.flowSourceConnection['connectionSpec'].get('id') is not None:\n\t                connSpec = self.flowAPI.getConnectionSpec(self.flowSourceConnection['connectionSpec'].get('id'))\n\t                self.flowSourceConnection['connectionSpec']['name'] = connSpec.get('name')\n\t            if connSpec.get('sourceSpec',{}).get('attributes',{}).get('uiAttributes',{}).get('isSource',False):\n\t                self.connectionType = 'source'\n\t            elif  connSpec.get('attributes',{}).get('isDestination',False):\n\t                self.connectionType = 'destination'\n\t            self.frequency = connSpec.get('sourceSpec',{}).get('attributes',{}).get('uiAttributes',{}).get('frequency',{}).get('key','unknown')\n", "        ## Target Connection part\n\t        if self.flowTargetConnection['id'] is not None:\n\t            targetConnData = self.flowAPI.getTargetConnection(self.flowTargetConnection['id'])\n\t            self.flowTargetConnection['name']:str = targetConnData.get('name')\n\t            self.flowTargetConnection['data']:dict = targetConnData.get('data',{})\n\t            self.flowTargetConnection['params']:dict = targetConnData.get('params',{})\n\t            for key, value in self.flowTargetConnection['params'].items():\n\t                if key == 'datasetId':\n\t                    self.datasetId = value\n\t            self.flowTargetConnection['connectionSpec']:dict = targetConnData.get('connectionSpec',{})\n", "            if self.flowTargetConnection['connectionSpec'].get('id',None) is not None:\n\t                connSpec = self.flowAPI.getConnectionSpec(self.flowSourceConnection['connectionSpec'].get('id'))\n\t                self.flowTargetConnection['connectionSpec']['name'] = connSpec.get('name')\n\t        ## Catalog part\n\t        if 'dataSetId' in self.flowTargetConnection['params'].keys():\n\t            datasetInfo = self.catalogAPI.getDataSet(self.flowTargetConnection['params']['dataSetId'])\n\t            if 'status' in datasetInfo.keys():\n\t                if datasetInfo['status'] == 404:\n\t                    self.flowTargetConnection['params']['datasetName'] = 'DELETED'\n\t            else:\n", "                self.flowTargetConnection['params']['datasetName'] = datasetInfo[list(datasetInfo.keys())[0]].get('name')\n\t        ## Schema part\n\t        if 'schema' in self.flowTargetConnection.get('data',{}).keys():\n\t            if self.flowTargetConnection.get('data',{}).get('schema',None) is not None:\n\t                ## handling inconsistency in the response\n\t                schemaId = self.flowTargetConnection['data']['schema'].get('id',self.flowTargetConnection['data']['schema'].get('schemaId',None))\n\t                if schemaId is not None:\n\t                    schemaInfo = self.schemaAPI.getSchema(schemaId,full=False)\n\t                    self.flowTargetConnection['data']['schema']['name'] = schemaInfo.get('title')\n\t        ## Mapping\n", "        if self.flowMapping is not None:\n\t            mappingInfo = self.mapperAPI.getMappingSet(self.flowMapping['id'])\n\t            self.flowMapping['createdDate'] = time.ctime(mappingInfo.get('createdDate')/1000)\n\t            self.flowMapping['createdDateTS'] = mappingInfo.get('createdDate')\n\t            self.flowMapping['updatedAtTS'] = mappingInfo.get('updatedAt',None)\n\t            if self.flowMapping['updatedAtTS'] is None:\n\t                self.flowMapping['updatedAt'] = None\n\t            else:\n\t                self.flowMapping['updatedAt'] = time.ctime(mappingInfo.get('updatedAt',0)/1000)\n\t            self.getMapping = lambda : self.mapperAPI.getMappingSet(self.flowMapping['id'])\n", "    def __setAttributes__(self,flowData:dict)->None:\n\t        \"\"\"\n\t        Set the attributes\n\t        \"\"\"\n\t        self.id = flowData.get('id')\n\t        self.etag = flowData.get('etag')\n\t        self.sandbox = flowData.get('sandboxName')\n\t        self.name = flowData.get('name')\n\t        self.version = flowData.get('version')\n\t        self.state = flowData.get('state')\n", "    def __repr__(self)->str:\n\t        data = {\n\t                \"id\" : self.id,\n\t                \"name\": self.name,\n\t                \"version\":self.version,\n\t                \"connectionName\" : self.connectionInfo.get('name','unknown'),\n\t                \"frequency\" : self.frequency,\n\t                \"flowSpecs\": self.flowSpec,\n\t                \"sourceConnection\": self.flowSourceConnection,\n\t                \"targetConnection\": self.flowTargetConnection,\n", "            }\n\t        if self.flowMapping is not None:\n\t            data['mapping'] = self.flowMapping\n\t        return json.dumps(data,indent=2)\n\t    def __str__(self)->str:\n\t        data = {\n\t                \"id\" : self.id,\n\t                \"name\": self.name,\n\t                \"version\":self.version,\n\t                \"connectionName\" : self.connectionInfo.get('name','unknown'),\n", "                \"frequency\" : self.frequency,\n\t                \"flowSpecs\": self.flowSpec,\n\t                \"sourceConnection\": self.flowSourceConnection,\n\t                \"targetConnection\": self.flowTargetConnection,\n\t            }\n\t        if self.flowMapping is not None:\n\t            data['mapping'] = self.flowMapping\n\t        return json.dumps(data,indent=2)\n\t    def summary(self):\n\t        data = {\n", "                \"id\" : self.id,\n\t                \"name\": self.name,\n\t                \"version\":self.version,\n\t                \"connectionName\" : self.connectionInfo.get('name','unknown'),\n\t                \"frequency\" : self.frequency,\n\t                \"flowSpecs\": self.flowSpec,\n\t                \"sourceConnection\": self.flowSourceConnection,\n\t                \"targetConnection\": self.flowTargetConnection,\n\t            }\n\t        if self.flowMapping is not None:\n", "            data['mapping'] = self.flowMapping\n\t        return data\n\t    def getFlowSpec(self)->dict:\n\t        \"\"\"\n\t        Return a dictionary of the flow Spec.\n\t        \"\"\"\n\t        if self.flowSpec['id'] is not None:\n\t            flowSpecData = self.flowAPI.getFlowSpec(self.flowSpec['id'])\n\t            return flowSpecData\n\t    def getSourceConnection(self)->dict:\n", "        \"\"\"\n\t        Return a dictionary of the connection information\n\t        \"\"\"\n\t        if self.flowSourceConnection['id'] is not None:\n\t            sourceConnData = self.flowAPI.getSourceConnection(self.flowSourceConnection['id'])\n\t            return sourceConnData\n\t    def getConnectionSpec(self)->dict:\n\t        \"\"\"\n\t        return a dictionary of the source connection spec information\n\t        \"\"\"\n", "        if self.flowSourceConnection['connectionSpec'].get('id') is not None:\n\t            connSpec = self.flowAPI.getConnectionSpec(self.flowSourceConnection['connectionSpec'].get('id'))\n\t            return connSpec\n\t    def getTargetConnection(self)->dict:\n\t        \"\"\"\n\t        return a dictionary of the target connection\n\t        \"\"\"\n\t        if self.flowTargetConnection['id'] is not None:\n\t            targetConnData = self.flowAPI.getTargetConnection(self.flowTargetConnection['id'])\n\t            return targetConnData\n", "    def getTargetConnectionSpec(self)->dict:\n\t        \"\"\"\n\t        return a dictionary of the target connection spec\n\t        \"\"\"\n\t        if self.flowTargetConnection['connectionSpec'].get('id') is not None:\n\t            connSpec = self.flowAPI.getConnectionSpec(self.flowSourceConnection['connectionSpec'].get('id'))\n\t            return connSpec\n\t    def getRuns(self,limit:int=10,n_results=100)->list:\n\t        \"\"\"\n\t        Returns the last run of the flow.\n", "        Arguments:\n\t            limit : OPTIONAL : Amount of item per requests\n\t            n_results : OPTIONAL : Total amount of item to return\n\t        \"\"\"\n\t        runs = self.flowAPI.getRuns(limit,n_results,prop=f\"flowId=={self.id}\")\n\t        return runs\n\t    def updateFlow(self, operations:list=None)->dict:\n\t        \"\"\"\n\t        Update the flow with the operation provided.\n\t        Argument:\n", "            operations : REQUIRED : The operation to set on the PATCH method\n\t                Example : \n\t            [\n\t                {\n\t                    \"op\": \"Add\",\n\t                    \"path\": \"/auth/params\",\n\t                    \"value\": {\n\t                    \"description\": \"A new description to provide further context on a specified connection or flow.\"\n\t                    }\n\t                }\n", "            ]\n\t        \"\"\"\n\t        if operations is None:\n\t            raise ValueError(\"No operations has been passed\")\n\t        res = self.flowAPI.updateFlow(self.id,self.etag,operations)\n\t        self.flowData = res\n\t        self.__setAttributes__(res)\n\t        return res\n\t    def updateFlowMapping(self,mappingId:str)->dict:\n\t        \"\"\"\n", "        Update the flow with the latest version of the mapping Id provided.\n\t        Arguments:\n\t            mappingId : REQUIRED : The mapping Id to be used for update.\n\t        \"\"\"\n\t        transformations = deepcopy(self.flowData.get('transformations',{}))\n\t        myMapping = self.mapperAPI.getMappingSet(mappingId)\n\t        myVersion = myMapping.get('version',0)\n\t        operation = {}\n\t        myIndex = None\n\t        for index, transformation in enumerate(transformations):\n", "            if transformation.get('name') == 'Mapping':\n\t                myIndex=index\n\t                operation['mappingId'] = mappingId\n\t                operation['mappingVersion'] = myVersion\n\t        if myIndex is not None:\n\t            patchOperation = [{'op': 'replace',\n\t            'path': f'/transformations/{myIndex}',\n\t            'value': {'name': 'Mapping',\n\t            'params': {'mappingId': operation['params']['mappingId'],\n\t            'mappingVersion': operation['params']['mappingVersion']}}}\n", "            ]\n\t        else:\n\t            raise Exception('Could not find a mapping transformation in the flow')\n\t        res = self.updateFlow(self.id,self.etag,patchOperation)\n\t        self.flowData = res\n\t        self.__setAttributes__(res)\n\t        return res\n"]}
{"filename": "aepp/__init__.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "from aepp import config\n\tfrom aepp import connector\n\tfrom .configs import *\n\tfrom .__version__ import __version__\n\tfrom typing import Union\n\t## other libraries\n\tfrom copy import deepcopy\n\tfrom pathlib import Path\n\timport json\n\tconnection = None\n", "def home(product: str = None, limit: int = 50):\n\t    \"\"\"\n\t    Return the IMS Organization setup and the container existing for the organization\n\t    Arguments:\n\t        product : OPTIONAL : specify one or more product contexts for which to return containers. If absent, containers for all contexts that you have rights to will be returned. The product parameter can be repeated for multiple contexts. An example of this parameter is product=acp\n\t        limit : OPTIONAL : Optional limit on number of results returned (default = 50).\n\t    \"\"\"\n\t    global connection\n\t    if connection is None:\n\t        connection = connector.AdobeRequest(\n", "            config_object=config.config_object, header=config.header\n\t        )\n\t    endpoint = config.endpoints[\"global\"] + \"/data/core/xcore/\"\n\t    params = {\"product\": product, \"limit\": limit}\n\t    myHeader = deepcopy(connection.header)\n\t    myHeader[\"Accept\"] = \"application/vnd.adobe.platform.xcore.home.hal+json\"\n\t    res = connection.getData(endpoint, params=params, headers=myHeader)\n\t    return res\n\tdef getPlatformEvents(\n\t    limit: int = 50, n_results: Union[int, str] = \"inf\", prop: str = None, **kwargs\n", ") -> dict:\n\t    \"\"\"\n\t    Timestamped records of observed activities in Platform. The API allows you to query events over the last 90 days and create export requests.\n\t    Arguments:\n\t        limit : OPTIONAL : Number of events to retrieve per request (50 by default)\n\t        n_results : OPTIONAL : Number of total event to retrieve per request.\n\t        prop : OPTIONAL : An array that contains one or more of a comma-separated list of properties (prop=\"action==create,assetType==Sandbox\")\n\t            If you want to filter results using multiple values for a single filter, pass in a comma-separated list of values. (prop=\"action==create,update\")\n\t    \"\"\"\n\t    global connection\n", "    if connection is None:\n\t        connection = connector.AdobeRequest(\n\t            config_object=config.config_object, header=config.header\n\t        )\n\t    endpoint = \"https://platform.adobe.io/data/foundation/audit/events\"\n\t    params = {\"limit\": limit}\n\t    if prop is not None:\n\t        params[\"property\"] = prop\n\t    # myHeader = deepcopy(connection.header)\n\t    lastPage = False\n", "    data = list()\n\t    while lastPage != True:\n\t        res = connection.getData(endpoint, params=params)\n\t        data += res.get(\"_embedded\", {}).get(\"events\", [])\n\t        nextPage = res.get(\"_links\", {}).get(\"next\", {}).get('href','')\n\t        if float(len(data)) >= float(n_results):\n\t            lastPage = True\n\t        if nextPage == \"\" and lastPage != True:\n\t            lastPage = True\n\t        else:\n", "            start = nextPage.split(\"start=\")[1].split(\"&\")[0]\n\t            queryId = nextPage.split(\"queryId=\")[1].split(\"&\")[0]\n\t            params[\"queryId\"] = queryId\n\t            params[\"start\"] = start\n\t    return data\n\tdef saveFile(\n\t    module: str = None,\n\t    file: object = None,\n\t    filename: str = None,\n\t    type_file: str = \"json\",\n", "    encoding: str = \"utf-8\",\n\t):\n\t    \"\"\"\n\t    Save the file in the approriate folder depending on the module sending the information.\n\t     Arguments:\n\t          module: REQUIRED: Module requesting the save file.\n\t          file: REQUIRED: an object containing the file to save.\n\t          filename: REQUIRED: the filename to be used.\n\t          type_file: REQUIRED: the type of file to be saveed(default: json)\n\t          encoding : OPTIONAL : encoding used to write the file.\n", "    \"\"\"\n\t    if module is None:\n\t        raise ValueError(\"Require the module to create a folder\")\n\t    if file is None or filename is None:\n\t        raise ValueError(\"Require a object for file and a name for the file\")\n\t    here = Path(Path.cwd())\n\t    folder = module.capitalize()\n\t    new_location = Path.joinpath(here, folder)\n\t    if new_location.exists() == False:\n\t        new_location.mkdir()\n", "    if type_file == \"json\":\n\t        filename = f\"{filename}.json\"\n\t        complete_path = Path.joinpath(new_location, filename)\n\t        with open(complete_path, \"w\", encoding=encoding) as f:\n\t            f.write(json.dumps(file, indent=4))\n\t    else:\n\t        filename = f\"{filename}.txt\"\n\t        complete_path = Path.joinpath(new_location, filename)\n\t        with open(complete_path, \"w\", encoding=encoding) as f:\n\t            f.write(file)\n"]}
{"filename": "aepp/destination.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "import aepp\n\tfrom aepp import connector\n\tfrom aepp import config\n\tfrom copy import deepcopy\n\tfrom typing import Union\n\timport time\n\timport logging\n\tfrom .configs import ConnectObject\n\tclass Authoring:\n\t    \"\"\"\n", "    This class is referring to Destination Authoring capability for AEP. \n\t    It is a suite of configuration APIs that allow you to configure destination integration patterns for Experience Platform to deliver audience and profile data to your endpoint, based on data and authentication formats of your choice.\n\t    More information on the API, available at: https://developer.adobe.com/experience-platform-apis/references/destination-authoring/\n\t    \"\"\"\n\t    def __init__(self, \n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,):\n\t        \"\"\"\n", "        Instanciating the class for Authoring.\n\t        Arguments:\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t        possible kwargs:\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n", "            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n", "                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n", "        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        # self.header.update({\"Accept\": \"application/json\"})\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n", "            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = config.endpoints[\"global\"] + config.endpoints[\"destinationAuthoring\"]\n\t    def getDestinations(self)->list:\n\t        \"\"\"\n\t        Return a list of all destination SDK authored by the organization.\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDestinations\")\n\t        path = \"/destinations\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getDestination(self, destinationId:str = None)->dict:\n\t        \"\"\"\n\t        Return a destination specific configuration.\n\t        Arguments:\n", "            destinationId : REQUIRED : The destination ID to be retrieved\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDestination with ID : {destinationId}\")\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        path = f\"/destinations/{destinationId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteDestination(self, destinationId: str = None)->dict:\n", "        \"\"\"\n\t        Delete a specific destination based on its ID.\n\t        Arguments:\n\t            destinationId : REQUIRED : The destination ID to be deleted\n\t        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteDestination with ID: {destinationId}\")\n\t        path = f\"/destinations/{destinationId}\"\n", "        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createDestination(self, destinationObj: dict = None)->dict:\n\t        \"\"\"\n\t        Create a destination based on the definition passed in argument.\n\t        Arguments:\n\t            destinationObj : REQUIRED : Object containing the definition of the destination.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDestination\")\n", "        if destinationObj is None or type(destinationObj) != dict:\n\t            raise Exception(\"Require a dictionary defining the destination configuration\")\n\t        path = \"/destinations\"\n\t        res = self.connector.postData(self.endpoint + path, data=destinationObj)\n\t        return res\n\t    def updateDestination(self, destinationId:str=None,destinationObj: dict = None)->dict:\n\t        \"\"\"\n\t        Create a destination based on the definition passed in argument.\n\t        Arguments:\n\t            destinationId : REQUIRED : The destination ID to be updated\n", "            destinationObj : REQUIRED : Object containing the definition of the destination.\n\t        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateDestination with ID: {destinationId}\")\n\t        if destinationObj is None or type(destinationObj) != dict:\n\t            raise Exception(\"Require a dictionary defining the destination configuration\")\n\t        path = \"/destinations\"\n\t        res = self.connector.putData(self.endpoint + path, data=destinationObj)\n", "        return res\n\t    def getDestinationServers(self)->list:\n\t        \"\"\"\n\t        Retrieve a list of all destination server configurations for your IMS Organization\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDestinationServers\")\n\t        path = \"/destination-servers\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n", "    def getDestinationServer(self,serverId:str=None)->dict:\n\t        \"\"\"\n\t        Retrieve a specific destination server configuration by its ID.\n\t        Arguments:\n\t            serverId : REQUIRED : destination server ID of the server\n\t        \"\"\"\n\t        if serverId is None:\n\t            raise ValueError(\"Require a server ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getDestinationServer with ID: {serverId}\")\n", "        path = f\"/destination-servers/{serverId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteDestinationServer(self,serverId:str = None)->dict:\n\t        \"\"\"\n\t        Delete a destination server by its ID.\n\t        Arguments:\n\t            serverId : REQUIRED : destination server ID to be deleted\n\t        \"\"\"\n\t        if serverId is None:\n", "            raise ValueError(\"Require a server ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteDestinationServer with ID: {serverId}\")\n\t        path = f\"/destination-servers/{serverId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createDestinationServer(self,serverObj:dict=None)->dict:\n\t        \"\"\"\n\t        Create a new destination server configuration.\n\t        Arguments:\n", "            serverObj : REQUIRED : dictionary containing the server destination configuration\n\t        \"\"\"\n\t        path = \"/destination-servers\"\n\t        if serverObj is None:\n\t            raise ValueError(\"Require a dictionary containing the server configuration\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createDestinationServer\")\n\t        res = self.connector.postData(self.endpoint + path, data=serverObj)\n\t        return res\n\t    def updateDestinationServer(self,serverId:str=None, serverObj: dict = None)->dict:\n", "        \"\"\"\n\t        Update the destination with a new definition (PUT request)\n\t        Arguments:\n\t            serverId : REQUIRED : destination server ID to be updated\n\t            serverObj : REQUIRED : dictionary containing the server configuration\n\t        \"\"\"\n\t        if serverId is None:\n\t            raise ValueError(\"Require a destination server ID\")\n\t        if serverObj is None or type(serverObj) != dict:\n\t            raise Exception(\"Require a dictionary defining the server destination configuration\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateDestinationServer with ID: {serverId}\")\n\t        path = f\"/destination-servers/{serverId}\"\n\t        res = self.connector.putData(self.endpoint+path, data=serverObj)\n\t        return res\n\t    def getAudienceTemplates(self)->list:\n\t        \"\"\"\n\t        Return a list of all audience templates for your IMS Organization\n\t        \"\"\"\n\t        path = \"/audience-templates\"\n", "        res = self.connector.getData(self.endpoint+path)\n\t        return res\n\t    def getAudienceTemplate(self,audienceId:str=None)->dict:\n\t        \"\"\"\n\t        Return a specific Audience Template.\n\t        Arguments:\n\t            audienceId : REQUIRED : The ID of the audience template configuration that you want to retrieve.\n\t        \"\"\"\n\t        if audienceId is None:\n\t            raise ValueError(\"Require an audience ID\")\n", "        path = f\"/audience-templates/{audienceId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getAudienceTemplate with ID: {audienceId}\")\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteAudienceTemplate(self,audienceId:str=None)->dict:\n\t        \"\"\"\n\t        Delete a specific Audience Template.\n\t        Arguments:\n\t            audienceId : REQUIRED : The ID of the audience template configuration that you want to delete\n", "        \"\"\"\n\t        if audienceId is None:\n\t            raise ValueError(\"Require an audience ID\")\n\t        path = f\"/audience-templates/{audienceId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteAudienceTemplate with ID: {audienceId}\")\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createAudienceTemplate(self,templateObj:dict=None)->dict:\n\t        \"\"\"\n", "        Create a specific Audience Template based on a dictionary definition passed as parameter.\n\t        Arguments:\n\t            templateObj : REQUIRED : The ID of the audience template configuration that you want to retrieve.\n\t        \"\"\"\n\t        path = f\"/audience-templates/\"\n\t        if templateObj is None and type(templateObj) != dict:\n\t            raise ValueError(\"Require a dictionary for Audience template definition\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createAudienceTemplate\")\n\t        res = self.connector.postData(self.endpoint + path,data=templateObj)\n", "        return res\n\t    def updateAudienceTemplate(self,audienceId:str=None,templateObj:dict=None)->dict:\n\t        \"\"\"\n\t        Update a specific Audience Template based on a dictionary definition passed as parameter.\n\t        Arguments:\n\t            audienceId : REQUIRED : The ID of the audience template configuration that you want to delete\n\t            templateObj : REQUIRED : The ID of the audience template configuration that you want to retrieve.\n\t        \"\"\"\n\t        path = f\"/audience-templates/{audienceId}\"\n\t        if audienceId is None:\n", "            raise ValueError(\"Require an audience template ID\")\n\t        if templateObj is  None and type(templateObj) != dict:\n\t            raise ValueError(\"Require a dictionary for Audience template definition\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateAudienceTemplate with ID: {audienceId}\")\n\t        res = self.connector.postData(self.endpoint + path,data=templateObj)\n\t        return res\n\t    def getCredentials(self)->list:\n\t        \"\"\"\n\t        Retrieve a list of all credentials configurations for your IMS Organization \n", "        \"\"\"\n\t        path = \"/credentials\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCredentials\")\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getCredential(self,credentialId:str=None)->dict:\n\t        \"\"\"\n\t        Return a specific credential based on its ID.\n\t        Arguments:\n", "            credentialId : REQUIRED : The ID of the credential to retrieve\n\t        \"\"\"\n\t        if credentialId is None:\n\t            raise ValueError(\"Require a credential ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getCredential with ID: {credentialId}\")\n\t        path = f\"/credentials/{credentialId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteCredential(self,credentialId:str=None)->dict:\n", "        \"\"\"\n\t        Delete a specific credential based on its ID\n\t        Arguments:\n\t            credentialId : REQUIRED : Credential ID to be deleted\n\t        \"\"\"\n\t        if credentialId is None:\n\t            raise ValueError(\"Require a credential ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteCredential with ID: {credentialId}\")\n\t        path = f\"/credentials/{credentialId}\"\n", "        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createCredential(self,credentialObj:dict=None)->dict:\n\t        \"\"\"\n\t        Create a credential configuration based on the dictionary passed.\n\t        Arguments:\n\t            credentialObj : REQUIRED : The credential object definition\n\t        \"\"\"\n\t        if credentialObj is None or type(credentialObj) != dict:\n\t            raise ValueError(\"Require a dictionary for definition\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createCredential\")\n\t        path = \"/credentials\"\n\t        res = self.connector.postData(self.endpoint + path,data=credentialObj)\n\t        return res\n\t    def updateCredential(self,credentialId:str=None,credentialObj:dict=None)->dict:\n\t        \"\"\"\n\t        Update the credential configuration based on the dictionary and the credential ID passed.\n\t        Arguments:\n\t            credentialId : REQUIRED : The credentialId to be updated\n", "            credentialObj : REQUIRED : The credential object definition\n\t        \"\"\"\n\t        if credentialObj is None or type(credentialObj) != dict:\n\t            raise ValueError(\"Require a dictionary for definition\")\n\t        path = f\"/credentials/{credentialId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateCredential with ID: {credentialId}\")\n\t        res = self.connector.putData(self.endpoint + path,data=credentialObj)\n\t        return res\n\t    def getSampleProfile(self,destinationInstanceId:str=None,destinationId:str=None,count:int=100)->dict:\n", "        \"\"\"\n\t        Generate a sample profile of a destination given the correct arguments.\n\t        Arguments:\n\t            destinationInstanceId : REQUIRED : Also known as order ID. The ID of the destination instance based on which you are generating sample profiles. (example: \"49966037-32cd-4457-a105-2cbf9c01826a\")\n\t                                    Documentation on how to retrieve it: https://experienceleague.adobe.com/docs/experience-platform/destinations/destination-sdk/api/developer-tools-reference/destination-testing-api.html?lang=en#get-destination-instance-id\n\t            destinationId : REQUIRED : he ID of the destination configuration based on which you are generating sample profiles. The destination ID that you should use here is the ID that corresponds to a destination configuration, created using the createDestination method.\n\t            count : OPTIONAL : The number of sample profiles that you are generating. The parameter can take values between 1 - 1000.\n\t        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n", "        if destinationInstanceId is None:\n\t            raise ValueError(\"Require a destination instance ID\")\n\t        path = \"/sample-profiles\"\n\t        params = {\n\t            \"destinationInstanceId\" : destinationInstanceId,\n\t            \"destinationId\" : destinationId,\n\t            \"count\" : count\n\t        }\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSampleProfile\")\n", "        res = self.connector.getData(self.endpoint + path,params=params)\n\t        return res\n\t    def getSampleDestination(self,destinationConfigId:str=None)->dict:\n\t        \"\"\"\n\t        Returns a sample template corresponding to the destinationID passed.\n\t        Argument:\n\t            destinationConfigId : REQUIRED : The ID of the destination configuration for which you are generating a message transformation template.\n\t                                            The destination ID that you should use here is the ID that corresponds to a destination configuration, created using the createDestination method\n\t        \"\"\"\n\t        if destinationConfigId is None:\n", "            raise ValueError(\"A Destination configuration ID must be specified\")\n\t        path = f\"/testing/template/sample/{destinationConfigId}\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSampleDestination with ID: {destinationConfigId}\")\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def generateTestProfile(self,destinationId:str=None,template:str=None,profiles:list=None)->str:\n\t        \"\"\"\n\t        Generate exported data by making a POST request to the testing/template/render endpoint and providing the destination ID of the destination configuration and the template you created using the sample template API endpoint\n\t        Arguments:\n", "            destinationId : REQUIRED : The ID of the destination configuration for which you are rendering exported data.\n\t            template : REQUIRED : The character-escaped version of the template based on which you are rendering exported data.\n\t            profiles : OPTIONAL : list of dictionary returned by the getSampleProfile method\n\t        \"\"\"\n\t        if destinationId is None:\n\t            raise ValueError(\"Require a destination ID\")\n\t        if template is None and type(template) != str:\n\t            raise ValueError(\"Must provide a string that is an escape version of the template\")\n\t        path = f\"/testing/template/render\"\n\t        data = {\n", "            \"destinationId\": destinationId,\n\t            \"template\": template,\n\t        }\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSampleDestination with ID: {destinationId}\")\n\t        if profiles is not None and type(profiles) == list:\n\t            data['profiles'] = profiles\n\t        res = self.connector.postData(self.endpoint+path, data=data)\n\t        return res\n\t    def sendMessageToPartner(self,destinationInstanceId:str=None,profiles:list=None)->dict:\n", "        \"\"\"\n\t        Test the connection to your destination by sending messages to the partner endpoint.\n\t        Optionally, you can send a list of profiles in the request. If you do not send any profiles, Experience Platform generates those internally. \n\t        In this case, you can view the profiles that were used for validation in the response you receive from your getSampleProfile endpoint.\n\t        Arguments:\n\t            destinationInstanceId : REQUIRED : Also known as order ID. The ID of the destination instance based on which you are generating sample profiles.\n\t                                            See documentation for info on how to retrieve it: https://experienceleague.adobe.com/docs/experience-platform/destinations/destination-sdk/api/developer-tools-reference/destination-testing-api.html?lang=en#get-destination-instance-id\n\t            profiles : OPTIONAL : list of dictionary returned by the getSampleProfile method\n\t        \"\"\"\n\t        if destinationInstanceId is None:\n", "            raise ValueError(\"Require a destination instance ID\")\n\t        path = f\"/testing/destinationInstance/{destinationInstanceId}\"\n\t        data = []\n\t        if profiles is not None:\n\t            data = profiles\n\t        res = self.connector.postData(self.endpoint+path,data=data)\n\t        return res\n\t    def getSubmissions(self)->list:\n\t        \"\"\"\n\t        List of all destinations submitted for publishing for your IMS Organization\n", "        \"\"\"\n\t        path = \"/destinations/publish\"\n\t        res = self.connector.getData(self.endpoint+path)\n\t        return res\n\t    def getSubmission(self,destinationConfigId:str=None)->dict:\n\t        \"\"\"\n\t        Get a specific destination submission status based on the ID passed.\n\t        Argument:\n\t            destinationConfigId : REQUIRED : The ID of the destination configuration you have submitted for publishing.\n\t        \"\"\"\n", "        if destinationConfigId is None:\n\t            raise ValueError(\"Destination configuration ID is required\")\n\t        path = f\"/destinations/publish/{destinationConfigId}\"\n\t        res = self.connector.getData(self.endpoint+path)\n\t        return res\n\t    def SubmitDestination(self,destinationObj:dict=None)->dict:\n\t        \"\"\"\n\t        Submit a destination configuration for publishing\n\t        Arguments:\n\t            destinationObj : REQUIRED : The object defining the destination config. (DestinationId, Access, AllowedOrgs)\n", "        \"\"\"\n\t        path = \"/destinations/publish/\"\n\t        if destinationObj is None:\n\t            raise ValueError(\"A destination object must be specified\")\n\t        res = self.connector.postData(self.endpoint+path,data=destinationObj)\n\t        return res\n\t    def updateSubmissionRequest(self,destinationConfigId:str,destinationObj:dict=None)->dict:\n\t        \"\"\"\n\t        Update the allowed organizations in a destination publish request. \n\t        Arguments:\n", "           destinationConfigId : REQUIRED : The ID of the destination configuration you have submitted for publishing.\n\t           destinationObj : REQUIRED : The object defining the destination config. (DestinationId, Access, AllowedOrgs)\n\t        \"\"\"\n\t        if destinationConfigId is None:\n\t            raise ValueError(\"Require of destinationConfigId value\")\n\t        if destinationObj is None:\n\t            raise ValueError(\"Require a dictionary for defining\")\n\t        path = f\"/destinations/publish/{destinationConfigId}\"\n\t        res = self.connector.putData(self.endpoint+path,data=destinationObj)\n\t        return res\n"]}
{"filename": "aepp/connector.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t## Internal modules\n", "from aepp import config, configs\n\t## External module\n\timport json\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom typing import Dict, Optional, Union\n\tfrom copy import deepcopy\n\timport time\n\timport requests\n\tfrom requests import Response\n", "from pathlib import Path\n\timport jwt\n\t@dataclass\n\tclass TokenInfo:\n\t    \"\"\"\n\t    Represents an IMS token along with metadata associated to it.\n\t    \"\"\"\n\t    token: str\n\t    expiry: int\n\tclass AdobeRequest:\n", "    \"\"\"\n\t    Handle request to Audience Manager and taking care that the request have a valid token set each time.\n\t    \"\"\"\n\t    loggingEnabled = False\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,configs.ConnectObject] = config.config_object,\n\t        header: dict = config.header,\n\t        endpoints: dict = config.endpoints,\n\t        verbose: bool = False,\n", "        loggingEnabled: bool = False,\n\t        logger: object = None,\n\t        retry: int = 0,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Set the connector to be used for handling request to AAM\n\t        Arguments:\n\t            config_object : OPTIONAL : Require the importConfig file to have been used.\n\t            header : OPTIONAL : Header that you are already using.\n", "            endpoints : OPTIONAL : Maps service to their endpoint.\n\t            verbose : OPTIONAL : display comment while running\n\t            loggingEnabled : OPTIONAL : if the logging is enable for that instance.\n\t            logger : OPTIONAL : instance of the logger created\n\t            retry : OPTIONAL : When GET request fails, if set to an int, it will retry this number of time\n\t        \"\"\"\n\t        if type(config) != dict:\n\t            config = config.getConfigObject()\n\t        if config[\"org_id\"] == \"\":\n\t            raise Exception(\n", "                \"You have to upload the configuration file with importConfigFile or configure method.\"\n\t            )\n\t        self.config = deepcopy(config)\n\t        self.header = deepcopy(header)\n\t        self.endpoints = deepcopy(endpoints)\n\t        self.loggingEnabled = loggingEnabled\n\t        self.logger = logger\n\t        self.retry = retry\n\t        requests.packages.urllib3.disable_warnings()\n\t        if self.config[\"token\"] == \"\" or time.time() > self.config[\"date_limit\"]:\n", "            if self.config[\"private_key\"] is not None or self.config[\"pathToKey\"] is not None:\n\t                self.connectionType = 'jwt'\n\t                token_info = self.get_jwt_token_and_expiry_for_config(\n\t                    config=self.config,\n\t                    verbose=verbose,\n\t                    aepScope=kwargs.get(\"aepScope\"),\n\t                    privacyScope=kwargs.get(\"privacyScope\"),\n\t                )\n\t            elif self.config[\"scopes\"] is not None:\n\t                self.connectionType = 'oauthV2'\n", "                token_info = self.get_oauth_token_and_expiry_for_config(\n\t                    config=self.config,\n\t                    verbose=verbose\n\t                )\n\t            else:\n\t                self.connectionType = 'oauthV1'\n\t                token_info = self.get_oauth_token_and_expiry_for_config(\n\t                    config=self.config,\n\t                    verbose=verbose\n\t                )\n", "            self.token = token_info.token\n\t            self.config[\"token\"] = self.token\n\t            if self.connectionType == 'jwt':\n\t                timeScale = 1000 ## jwt returns milliseconds expiry\n\t            elif self.connectionType == 'oauthV1' or self.connectionType == 'oauthV2':\n\t                timeScale = 1 ## oauth returns seconds expiry\n\t            self.config[\"date_limit\"] = (\n\t                time.time() + token_info.expiry / timeScale - 500\n\t            )\n\t            self.header.update({\"Authorization\": f\"Bearer {self.token}\"})\n", "        # x-sandbox-id is required when using non-user token, but forbidden for user token\n\t        if self.connectionType == 'oauthV1' and \"x-sandbox-id\" not in self.header:\n\t            self.update_sandbox_id(self.config[\"sandbox\"])\n\t    def _find_path(self, path: str) -> Optional[Path]:\n\t        \"\"\"Checks if the file denoted by the specified `path` exists and returns the Path object\n\t        for the file.\n\t        If the file under the `path` does not exist and the path denotes an absolute path, tries\n\t        to find the file by converting the absolute path to a relative path.\n\t        If the file does not exist with either the absolute and the relative path, returns `None`.\n\t        \"\"\"\n", "        if Path(path).exists():\n\t            return Path(path)\n\t        elif path.startswith(\"/\") and Path(\".\" + path).exists():\n\t            return Path(\".\" + path)\n\t        elif path.startswith(\"\\\\\") and Path(\".\" + path).exists():\n\t            return Path(\".\" + path)\n\t        else:\n\t            return None\n\t    def get_oauth_token_and_expiry_for_config(\n\t        self,\n", "        config: Union[dict,configs.ConnectObject],\n\t        verbose: bool = False,\n\t        save: bool = False\n\t    ) -> TokenInfo:\n\t        \"\"\"\n\t        Retrieve the access token by using the OAuth information provided by the user\n\t        during the import importConfigFile function.\n\t        Arguments :\n\t            config : REQUIRED : Configuration object.\n\t            verbose : OPTIONAL : Default False. If set to True, print information.\n", "            save : OPTIONAL : Default False. If set to True, save the toke in the .\n\t        \"\"\"\n\t        if type(config)!= dict:\n\t            config = config.getConfigObject()\n\t        if self.connectionType == 'oauthV1':\n\t            oauth_payload = {\n\t                \"grant_type\": \"authorization_code\",\n\t                \"client_id\": config[\"client_id\"],\n\t                \"client_secret\": config[\"secret\"],\n\t                \"code\": config[\"auth_code\"]\n", "            }\n\t            response = requests.post(\n\t                config[\"oauthTokenEndpointV1\"], data=oauth_payload, verify=False\n\t            )\n\t        elif self.connectionType == 'oauthV2':\n\t            oauth_payload = {\n\t                \"grant_type\": \"client_credentials\",\n\t                \"client_id\": config[\"client_id\"],\n\t                \"client_secret\": config[\"secret\"],\n\t                \"scope\": config[\"scopes\"]\n", "            }\n\t            response = requests.post(\n\t                config[\"oauthTokenEndpointV2\"], data=oauth_payload, verify=False\n\t            )\n\t        return self._token_postprocess(response=response, verbose=verbose, save=save)\n\t    def get_jwt_token_and_expiry_for_config(\n\t        self,\n\t        config: Union[dict,configs.ConnectObject],\n\t        verbose: bool = False,\n\t        save: bool = False,\n", "        **kwargs\n\t    ) -> TokenInfo:\n\t        \"\"\"\n\t        Retrieve the access token by using the JWT information provided by the user\n\t        during the import importConfigFile function.\n\t        Arguments :\n\t            config : REQUIRED : Configuration object.\n\t            verbose : OPTIONAL : Default False. If set to True, print information.\n\t            save : OPTIONAL : Default False. If set to True, save the toke in the .\n\t        \"\"\"\n", "        if type(config) != dict:\n\t            config = config.getConfigObject()\n\t        private_key = configs.get_private_key_from_config(config)\n\t        header_jwt = {\n\t            \"cache-control\": \"no-cache\",\n\t            \"content-type\": \"application/x-www-form-urlencoded\",\n\t        }\n\t        now_plus_24h = int(time.time()) + 24 * 60 * 60\n\t        jwt_payload = {\n\t            \"exp\": now_plus_24h,\n", "            \"iss\": config[\"org_id\"],\n\t            \"sub\": config[\"tech_id\"],\n\t            f\"{self.config['imsEndpoint']}/s/ent_dataservices_sdk\": True,\n\t            \"aud\": f'{self.config[\"imsEndpoint\"]}/c/{config[\"client_id\"]}',\n\t        }\n\t        # privacy topic\n\t        if kwargs.get(\"privacyScope\", False):\n\t            jwt_payload[f\"{self.config['imsEndpoint']}/s/ent_gdpr_sdk\"] = True\n\t        if kwargs.get(\"aepScope\", True) is False:\n\t            del jwt_payload[f\"{self.config['imsEndpoint']}/s/ent_dataservices_sdk\"]\n", "        encoded_jwt = self._get_jwt(payload=jwt_payload, private_key=private_key)\n\t        payload = {\n\t            \"client_id\": config[\"client_id\"],\n\t            \"client_secret\": config[\"secret\"],\n\t            \"jwt_token\": encoded_jwt,\n\t        }\n\t        response = requests.post(\n\t            config[\"jwtTokenEndpoint\"], headers=header_jwt, data=payload, verify=False\n\t        )\n\t        return self._token_postprocess(response=response, verbose=verbose, save=save)\n", "    def _token_postprocess(\n\t        self,\n\t        response: Response,\n\t        verbose: bool = False,\n\t        save: bool = False\n\t    ) -> TokenInfo:\n\t        \"\"\"\n\t        Parse the IMS response to extract token information\n\t        Arguments :\n\t            response : REQUIRED : API response payload from IMS.\n", "            verbose : OPTIONAL : Default False. If set to True, print information.\n\t            save : OPTIONAL : Default False. If set to True, save the toke in the .\n\t        \"\"\"\n\t        json_response = response.json()\n\t        try:\n\t            self.token = json_response[\"access_token\"]\n\t            self.config[\"token\"] = self.token\n\t        except KeyError:\n\t            print(\"Issue retrieving token\")\n\t            print(json_response)\n", "        expiry = json_response[\"expires_in\"]\n\t        if save:\n\t            with open(\"token.txt\", \"w\") as f:\n\t                f.write(self.token)\n\t            print(f\"token has been saved here: {os.getcwd()}{os.sep}token.txt\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"token retrieved: {self.token}\")\n\t        if verbose:\n\t            print(\"token valid till : \" + time.ctime(time.time() + expiry))\n\t        if self.loggingEnabled:\n", "            self.logger.debug(\n\t                f\"token valid till : {time.ctime(time.time() + expiry)}\"\n\t            )\n\t        return TokenInfo(token=self.token, expiry=expiry)\n\t    def _get_jwt(self, payload: dict, private_key: str) -> str:\n\t        \"\"\"\n\t        Ensure that jwt enconding return the same type (str) as versions < 2.0.0 returned bytes and >2.0.0 return strings.\n\t        \"\"\"\n\t        token: Union[str, bytes] = jwt.encode(payload, private_key, algorithm=\"RS256\")\n\t        if isinstance(token, bytes):\n", "            return token.decode(\"utf-8\")\n\t        return token\n\t    def _checkingDate(self) -> None:\n\t        \"\"\"\n\t        Checking if the token is still valid\n\t        \"\"\"\n\t        now = time.time()\n\t        if now > self.config[\"date_limit\"]:\n\t            if self.loggingEnabled:\n\t                self.logger.warning(\"token expired. Trying to retrieve a new token\")\n", "            if self.connectionType == 'jwt':\n\t                token_with_expiry = self.get_jwt_token_and_expiry_for_config(config=self.config)\n\t            elif self.connectionType == 'oauthV1' or self.connectionType == 'oauthV2':\n\t                token_with_expiry = self.get_oauth_token_and_expiry_for_config(config=self.config)\n\t            self.token = token_with_expiry[\"token\"]\n\t            self.config[\"token\"] = self.token\n\t            if self.loggingEnabled:\n\t                self.logger.info(\"new token retrieved : {self.token}\")\n\t            self.header.update({\"Authorization\": f\"Bearer {self.token}\"})\n\t            if self.connectionType == 'jwt':\n", "                timeScale = 1000 ## jwt returns milliseconds expiry\n\t            elif self.connectionType == 'oauthV1' or self.connectionType == 'oauthV2':\n\t                timeScale = 1 ## oauth returns seconds expiry\n\t            self.config[\"date_limit\"] = (\n\t                time.time() + token_with_expiry[\"expiry\"] / timeScale - 500\n\t            )\n\t    def updateSandbox(self, sandbox: str) -> None:\n\t        \"\"\"\n\t        Update the sandbox used for the request\n\t        Arguments:\n", "            sandbox : REQUIRED : the sandbox to use for the requests\n\t        \"\"\"\n\t        if not sandbox:\n\t            raise Exception(\"require a sandbox\")\n\t        self.header[\"x-sandbox-name\"] = sandbox\n\t    def update_sandbox_id(self, sandbox: str) -> None:\n\t        \"\"\"\n\t        Update the sandbox ID used for the request.\n\t        This is required when using non-user credentials. Only internal adobe tools and oauth V1 can access this.\n\t        Arguments:\n", "            sandbox : REQUIRED : the sandbox name to use for the requests\n\t        \"\"\"\n\t        if not sandbox:\n\t            raise Exception(\"require a sandbox\")\n\t        endpoint = f\"{self.endpoints['global']}{self.endpoints['sandboxes']}/sandboxes/{sandbox}\"\n\t        res = self.getData(endpoint)\n\t        if \"id\" not in res:\n\t            raise Exception(\"sandbox Id not found\")\n\t        sandbox_id = res[\"id\"]\n\t        self.header[\"x-sandbox-id\"] = sandbox_id\n", "    def getData(\n\t        self,\n\t        endpoint: str,\n\t        params: dict = None,\n\t        data: dict = None,\n\t        headers: dict = None,\n\t        *args,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n", "        Abstraction for getting data\n\t        \"\"\"\n\t        self._checkingDate()\n\t        if headers is None:\n\t            headers = self.header\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n\t                f\"Start GET request to {endpoint} with header: {json.dumps(headers)}\"\n\t            )\n\t        if params is None and data is None:\n", "            res = requests.get(endpoint, headers=headers, verify=False)\n\t        elif params is not None and data is None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n\t            res = requests.get(endpoint, headers=headers, params=params, verify=False)\n\t        elif params is None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.get(endpoint, headers=headers, data=data, verify=False)\n\t        elif params is not None and data is not None:\n", "            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n\t                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.get(endpoint, headers=headers, params=params, data=data, verify=False)\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"endpoint used: {res.request.url}\")\n\t            self.logger.debug(f\"params used: {params}\")\n\t        try:\n\t            if kwargs.get(\"format\", \"json\") == \"json\":\n\t                res_json = res.json()\n", "            if kwargs.get(\"format\", \"json\") == \"txt\":\n\t                res_json = res.text\n\t            elif kwargs.get(\"format\", \"json\") == \"raw\":\n\t                res_json = res\n\t            else:\n\t                res_json = res.json()\n\t        except:\n\t            if kwargs.get(\"verbose\", False):\n\t                print(res.text)\n\t            if self.loggingEnabled:\n", "                self.logger.warning(f\"error: {res.text}\")\n\t            res_json = {\"error\": \"Request Error - could not generate json\"}\n\t            if self.retry > 0:\n\t                if self.loggingEnabled:\n\t                    self.logger.info(f\"starting retry: {self.retry} to do\")\n\t                for each in range(self.retry):\n\t                    if \"error\" in res_json.keys():\n\t                        time.sleep(5)\n\t                        res_json = self.getData(\n\t                            endpoint, params, data, headers, **kwargs\n", "                        )\n\t        try:  ## sometimes list is being returned\n\t            if type(res_json) == dict:\n\t                if \"errorMessage\" in res_json.keys():\n\t                    if self.loggingEnabled:\n\t                        self.logger.error(\n\t                            f\"GET method failed: {res.status_code}, {res['errorMessage']}\"\n\t                        )\n\t                    print(f\"status code : {res.status_code}\")\n\t                    print(f\"error message : {res['errorMessage']}\")\n", "        except:\n\t            pass\n\t        ## returning some errors in the console\n\t        if type(res_json) == dict:\n\t            if 'status' in res_json.keys() and 'report' in res_json.keys():\n\t                if self.loggingEnabled:\n\t                    self.logger.warning(json.dumps(res_json,indent=2))\n\t        return res_json\n\t    def headData(\n\t        self, endpoint: str, params: dict = None, headers: dict = None, *args, **kwargs\n", "    ):\n\t        \"\"\"\n\t        Abstraction for the head method.\n\t        \"\"\"\n\t        self._checkingDate()\n\t        if headers is None:\n\t            headers = self.header\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n\t                f\"Start GET request to {endpoint} with header: {json.dumps(headers)}\"\n", "            )\n\t        if params is None:\n\t            res = requests.head(endpoint, headers=headers, verify=False)\n\t        if params is not None:\n\t            res = requests.head(endpoint, headers=headers, params=params, verify=False)\n\t        try:\n\t            res_header = res.headers()\n\t        except:\n\t            if kwargs.get(\"verbose\", False):\n\t                print(\"error generating the JSON response\")\n", "                print(f\"status: {res.status_code}\")\n\t                print(res.text)\n\t            if res.status_code != 200:\n\t                res_header = res.headers()\n\t            else:\n\t                res_header = {}\n\t        return res_header\n\t    def postData(\n\t        self,\n\t        endpoint: str,\n", "        params: dict = None,\n\t        data: dict = None,\n\t        bytesData: bytes = None,\n\t        headers: dict = None,\n\t        *args,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Abstraction for posting data\n\t        \"\"\"\n", "        self._checkingDate()\n\t        if headers is None:\n\t            headers = self.header\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n\t                f\"Start POST request to {endpoint} with header: {json.dumps(headers)}\"\n\t            )\n\t        if params is None and data is None:\n\t            res = requests.post(endpoint, headers=headers, verify=False)\n\t        elif params is not None and data is None:\n", "            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n\t            res = requests.post(endpoint, headers=headers, params=params, verify=False)\n\t        elif params is None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.post(endpoint, headers=headers, data=json.dumps(data), verify=False)\n\t        elif params is not None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n", "                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.post(\n\t                endpoint, headers=headers, params=params, data=json.dumps(data), verify=False\n\t            )\n\t        elif bytesData is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"bytes data used\")\n\t            res = requests.post(\n\t                endpoint, headers=headers, params=params, data=bytesData, verify=False\n\t            )\n", "        try:\n\t            formatUse = kwargs.get(\"format\", \"json\")\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"format used: {formatUse}\")\n\t            if formatUse == \"json\":\n\t                res_json = res.json()\n\t            elif formatUse == \"txt\":\n\t                res_json = res.text\n\t            elif formatUse == \"raw\":\n\t                res_json = res\n", "            else:\n\t                res_json = res.json()\n\t        except:\n\t            if kwargs.get(\"verbose\", False):\n\t                print(\"error generating the JSON response\")\n\t                print(f\"status: {res.status_code}\")\n\t                print(res.text)\n\t            if res.status_code != 200:\n\t                try:\n\t                    res_json = res.json()\n", "                except:\n\t                    res_json = {\"error\": \"Request Error - could not generate JSON\"}\n\t            else:\n\t                res_json = {}\n\t        try:  ## sometimes list is being returned\n\t            if \"errorMessage\" in res_json.keys():\n\t                print(f\"status code : {res.status_code}\")\n\t        except:\n\t            pass\n\t        return res_json\n", "    def patchData(\n\t        self,\n\t        endpoint: str,\n\t        params: dict = None,\n\t        data=None,\n\t        headers: dict = None,\n\t        *args,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n", "        Abstraction for deleting data\n\t        \"\"\"\n\t        self._checkingDate()\n\t        if headers is None:\n\t            headers = self.header\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n\t                f\"Start PATCH request to {endpoint} with header: {json.dumps(headers)}\"\n\t            )\n\t        if params is not None and data is None:\n", "            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n\t            res = requests.patch(endpoint, headers=headers, params=params, verify=False)\n\t        elif params is None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.patch(endpoint, headers=headers, data=json.dumps(data), verify=False)\n\t        elif params is not None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n", "                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.patch(\n\t                endpoint, headers=headers, params=params, data=json.dumps(data), verify=False\n\t            )\n\t        try:\n\t            res_json = res.json()\n\t        except:\n\t            if kwargs.get(\"verbose\", False):\n\t                print(\"error generating the JSON response\")\n\t                print(f\"status: {res.status_code}\")\n", "                print(res.text)\n\t            if self.loggingEnabled:\n\t                self.logger.error(\n\t                    f\"error with the response {res.status_code}: {res.text}\"\n\t                )\n\t            if res.status_code != 200:\n\t                try:\n\t                    res_json = res.json()\n\t                except:\n\t                    res_json = {\"error\": \"Request Error - could not generate JSON\"}\n", "            else:\n\t                res_json = {}\n\t        return res_json\n\t    def putData(\n\t        self,\n\t        endpoint: str,\n\t        params: dict = None,\n\t        data=None,\n\t        headers: dict = None,\n\t        *args,\n", "        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Abstraction for deleting data\n\t        \"\"\"\n\t        self._checkingDate()\n\t        if headers is None:\n\t            headers = self.header\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n", "                f\"Start PUT request to {endpoint} with header: {json.dumps(headers)}\"\n\t            )\n\t        if params is not None and data is None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n\t            res = requests.put(endpoint, headers=headers, params=params, verify=False)\n\t        elif params is None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.put(endpoint, headers=headers, data=json.dumps(data), verify=False)\n", "        elif params is not None and data is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"params: {json.dumps(params)}\")\n\t                self.logger.debug(f\"data: {json.dumps(data)}\")\n\t            res = requests.put(\n\t                endpoint, headers=headers, params=params, data=json.dumps(data), verify=False\n\t            )\n\t        try:\n\t            res_json = res.json()\n\t        except:\n", "            if kwargs.get(\"verbose\", False):\n\t                print(\"error generating the JSON response\")\n\t                print(f\"status: {res.status_code}\")\n\t                print(res.text)\n\t            if self.loggingEnabled:\n\t                self.logger.error(\n\t                    f\"error with the response {res.status_code}: {res.text}\"\n\t                )\n\t            if res.status_code != 200:\n\t                res_json = {\"error\": res.text}\n", "            else:\n\t                res_json = {}\n\t        return res_json\n\t    def deleteData(\n\t        self, endpoint: str, params: dict = None, headers: dict = None, *args, **kwargs\n\t    ):\n\t        \"\"\"\n\t        Abstraction for deleting data\n\t        \"\"\"\n\t        self._checkingDate()\n", "        if headers is None:\n\t            headers = self.header\n\t        if self.loggingEnabled:\n\t            self.logger.debug(\n\t                f\"Start PUT request to {endpoint} with header: {json.dumps(headers)}\"\n\t            )\n\t        if params is None:\n\t            res = requests.delete(endpoint, headers=headers, verify=False)\n\t        elif params is not None:\n\t            res = requests.delete(endpoint, headers=headers, params=params, verify=False)\n", "        try:\n\t            status_code = res.status_code\n\t            if status_code >= 400:\n\t                if self.loggingEnabled:\n\t                    self.logger.error(\n\t                        f\"error with the response {res.status_code}: {res.text}\"\n\t                    )\n\t                return res.json()\n\t        except:\n\t            if kwargs.get(\"verbose\", False):\n", "                print(res.text)\n\t            status_code = res.status_code\n\t        return status_code\n"]}
{"filename": "aepp/destinationinstanceservice.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "import aepp\n\tfrom aepp import connector\n\tfrom typing import Union\n\timport logging\n\tfrom .configs import ConnectObject\n\tclass DestinationInstanceService:\n\t    loggingEnabled = False\n\t    logger = None\n\t    \"\"\"\n\t    This class is referring to Destination Instance Service capability for AEP.\n", "    \"\"\"\n\t    def __init__(self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,):\n\t        \"\"\"\n\t        Instantiating the class for destination instance service\n\t        Arguments:\n\t            loggingObject : OPTIONAL : logging object to log messages.\n", "            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t        possible kwargs:\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n", "            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n", "                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n", "            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        # self.header.update({\"Accept\": \"application/json\"})\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instantiation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n", "            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"destinationInstance\"]\n\t    def createAdHocDatasetExport(self, flowIdToDatasetIds: dict = None)->dict:\n\t        \"\"\"\n\t        Create an Adhoc Request based on the flowId and the datasetId passed in argument.\n\t        Arguments:\n\t            flowIdToDatasetIds : REQUIRED :  dict containing the definition of flowId to datasetIds\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting creating adhoc dataset export\")\n\t        if flowIdToDatasetIds is None or type(flowIdToDatasetIds) != dict:\n\t            raise Exception(\"Require a dict for defining the flowId to datasetIds mapping\")\n\t        activationInfo = {'activationInfo': {'destinations': []}};\n\t        for flowId, datasetIds in flowIdToDatasetIds.items():\n\t            destination = {'flowId': flowId, 'datasets': []}\n\t            for datasetId in datasetIds:\n\t                dataset = {'id': datasetId}\n\t                destination['datasets'].append(dataset)\n", "            activationInfo['activationInfo']['destinations'].append(destination)\n\t        self.header.update({\"Accept\":\"application/vnd.adobe.adhoc.dataset.activation+json; version=1\"})\n\t        path = \"/adhocrun\"\n\t        res = self.connector.postData(self.endpoint + path, data=activationInfo)\n\t        return res\n"]}
{"filename": "aepp/utils.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# internal library\n", "from configparser import ConfigParser\n\tclass Utils:\n\t    @staticmethod\n\t    def check_if_exists(section, field_name, config_path):\n\t        config = ConfigParser()\n\t        config.read(config_path)\n\t        return config.get(section, field_name) or None\n\t    @staticmethod\n\t    def save_field_in_config(section, field_name, value, config_path):\n\t        config = ConfigParser()\n", "        config.read(config_path)\n\t        config.set(section, field_name, value)\n\t        with open(config_path, \"w\") as configfile:\n\t            config.write(configfile)"]}
{"filename": "aepp/exportDatasetToDataLandingZone.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "import aepp\n\tfrom aepp import flowservice\n\tfrom aepp import destinationinstanceservice\n\timport time\n\timport logging\n\tfrom typing import Union\n\tfrom aepp.utils import Utils\n\tfrom .configs import ConnectObject\n\tfrom tenacity import Retrying\n\tfrom tenacity import retry_if_result\n", "from tenacity import wait_fixed\n\tfrom tenacity import stop_after_attempt\n\tfrom dataclasses import dataclass,asdict,field\n\t@dataclass\n\tclass Flow:\n\t    name: str = None\n\t    flowSpec: dict = field(default_factory=dict)\n\t    sourceConnectionIds: list = field(default_factory=list)\n\t    targetConnectionIds: list= field(default_factory=list)\n\t    scheduleParams: dict = field(default_factory=dict)\n", "    transformations: list = field(default_factory=list)\n\t    def __init__(self,\n\t                 name: str,\n\t                 flow_spec: dict,\n\t                 source_connection_id: str,\n\t                 target_connection_id: str,\n\t                 schedule_params: dict,\n\t                 transformations: dict\n\t                 ):\n\t        self.name = name\n", "        self.sourceConnectionIds = [source_connection_id]\n\t        self.targetConnectionIds = [target_connection_id]\n\t        self.flowSpec = flow_spec\n\t        self.scheduleParams = schedule_params\n\t        self.transformations = transformations\n\t@dataclass\n\tclass ScheduleParams:\n\t    interval: int = 0\n\t    timeUnit: str = None\n\t    startTime: int = 0\n", "    def __init__(self,\n\t                 interval: int,\n\t                 timeUnit: str,\n\t                 startTime: int):\n\t        self.interval = interval\n\t        self.timeUnit = timeUnit\n\t        self.startTime = startTime\n\t@dataclass\n\tclass FlowOp:\n\t    op: str = None\n", "    path: str = None\n\t    value: dict = field(default_factory=dict)\n\t    def __init__(self,\n\t                 op: str,\n\t                 path: str,\n\t                 value: dict):\n\t        self.op = op\n\t        self.path = path\n\t        self.value = value\n\tclass ExportDatasetToDataLandingZone:\n", "    \"\"\"\n\t    A class for exporting dataset to cloud storage functionality\n\t    Attributes\n\t    ----------\n\t    flow_conn : module\n\t        flow service connection\n\t    dis_conn : module\n\t        destination instance service connection\n\t    \"\"\"\n\t    ## logging capability\n", "    loggingEnabled = False\n\t    logger = None\n\t    flow_conn = None\n\t    dis_conn = None\n\t    #fixed connection spec id for Data Landing Zone, reference: https://experienceleague.adobe.com/docs/experience-platform/destinations/api/export-datasets.html?lang=en\n\t    DLZ_CONNECTION_SPEC_ID = \"10440537-2a7b-4583-ac39-ed38d4b848e8\"\n\t    #fixed flow spec id for Data Landing Zone, reference: https://experienceleague.adobe.com/docs/experience-platform/destinations/api/export-datasets.html?lang=en\n\t    DLZ_FLOW_SEPC_ID = \"cd2fc47e-e838-4f38-a581-8fff2f99b63a\"\n\t    def __init__(\n\t            self,\n", "            config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t            header: dict = aepp.config.header,\n\t            loggingObject: dict = None\n\t    ):\n\t        \"\"\"\n\t        initialize the Export Dataset to Data Landing Zone.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t        \"\"\"\n", "        self.flow_conn = flowservice.FlowService(config=config, header=header)\n\t        self.dis_conn = destinationinstanceservice.DestinationInstanceService(config=config, header=header)\n\t        if loggingObject is not None and sorted(\n\t                [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n", "            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n", "    def createDataFlowRunIfNotExists(\n\t            self,\n\t            dataset_id: str,\n\t            compression_type: str,\n\t            data_format: str,\n\t            export_path: str,\n\t            on_schedule: bool,\n\t            config_path: str,\n\t            entity_name: str,\n\t            initial_delay: int = 600) -> str:\n", "        \"\"\"\n\t        Create a data flow if not being saved in config file\n\t        Arguments:\n\t          dataset_id : REQUIRED : The dataset that needs to be exported\n\t          compression_type: REQUIRED : define compression type to use when saving data to file. Possible values:\n\t            gzip\n\t            none\n\t          data_format : REQUIRED : define the data format . Possible values:\n\t            json\n\t            parquet\n", "          export_path : REQUIRED : define the folder path in your dlz container\n\t          on_schedule: REQUIRED: define whether you would like to have the export following a fixed schedule or not\n\t          config_path: REQUIRED: define the path of your aepp config file\n\t        Returns:\n\t            dataflow_id(str)\n\t        \"\"\"\n\t        dataflow_id = Utils.check_if_exists(\"Platform\", \"dataflow_id\", config_path)\n\t        if dataflow_id is not None:\n\t            if self.loggingEnabled:\n\t                self.logger.info(f\"Flow {dataflow_id} has already existed in config\")\n", "            flow = self.flow_conn.getFlow(dataflow_id)\n\t            source_connection_id = flow[\"sourceConnectionIds\"][0]\n\t            source_connection = self.flow_conn.getSourceConnection(source_connection_id)\n\t            exist_dataset_id = source_connection[\"params\"][\"datasets\"][0][\"dataSetId\"]\n\t            if exist_dataset_id != dataset_id:\n\t                dataflow_id = self.createDataFlow(dataset_id, compression_type, data_format, export_path, on_schedule, config_path, entity_name)\n\t            elif flow[\"state\"] == \"disabled\":\n\t                self.flow_conn.postFlowAction(dataflow_id, \"enable\")\n\t                if self.loggingEnabled:\n\t                    self.logger.info(f\"Flow {dataflow_id} with dataset {dataset_id} is enabled\")\n", "        else:\n\t            dataflow_id = self.createDataFlow(dataset_id, compression_type, data_format, export_path, on_schedule, config_path, entity_name)\n\t        self.createFlowRun(dataflow_id, dataset_id, on_schedule, initial_delay)\n\t        return dataflow_id\n\t    def createDataFlow(\n\t            self,\n\t            dataset_id: str,\n\t            compression_type: str,\n\t            data_format: str,\n\t            export_path: str,\n", "            on_schedule: bool,\n\t            config_path: str,\n\t            entity_name: str):\n\t        \"\"\"\n\t        Create a data flow as it is not saved in config\n\t        Arguments:\n\t          dataset_id : REQUIRED : The dataset that needs to be exported\n\t          compression_type: REQUIRED : define compression type to use when saving data to file. Possible values:\n\t            gzip\n\t            none\n", "          data_format : REQUIRED : define the data format . Possible values:\n\t            json\n\t            parquet\n\t          export_path : REQUIRED : define the folder path in your dlz container\n\t          on_schedule: REQUIRED: define whether you would like to have the export following a fixed schedule or not\n\t          config_path: REQUIRED: define the config file path\n\t          entity_name: REQUIRED: define the name of flow entities\n\t        \"\"\"\n\t        base_connection_id = self.createBaseConnection(entity_name)\n\t        source_connection_id = self.createSourceConnection(dataset_id, entity_name)\n", "        target_connection_id = self.createTargetConnection(base_connection_id, compression_type, data_format, export_path, entity_name)\n\t        dataflow_id = self.createFlow(source_connection_id, target_connection_id, on_schedule, entity_name)\n\t        #save dataflow_id to config\n\t        Utils.save_field_in_config(\"Platform\", \"dataflow_id\", dataflow_id, config_path)\n\t        return dataflow_id\n\t    def createBaseConnection(self, entity_name) -> str:\n\t        \"\"\"\n\t        Create a base connection for Data Landing Zone Destination\n\t        Returns:\n\t            base_connection_id(str)\n", "        \"\"\"\n\t        base_connection_res = self.flow_conn.createConnection(data={\n\t            \"name\": entity_name,\n\t            \"auth\": None,\n\t            \"connectionSpec\": {\n\t                \"id\": self.DLZ_CONNECTION_SPEC_ID,\n\t                \"version\": \"1.0\"\n\t            }\n\t        })\n\t        try:\n", "            base_connection_id = base_connection_res[\"id\"]\n\t            if self.loggingEnabled:\n\t                self.logger.info(\"baseConnection \" + base_connection_id + \" has been created\")\n\t            return base_connection_id\n\t        except KeyError:\n\t            raise RuntimeError(f\"Error when creating base connection: {base_connection_res}\")\n\t    def createSourceConnection(self, dataset_id: str, entity_name: str) -> str:\n\t        \"\"\"\n\t        Create a source connection for Data Landing Zone Destination\n\t        Arguments:\n", "          dataset_id : REQUIRED : The dataset that needs to be exported\n\t          entity_name: REQUIRED: define the name of source connection\n\t        Returns:\n\t          source_connection_id(str)\n\t        \"\"\"\n\t        source_res = self.flow_conn.createSourceConnectionDataLake(\n\t            name= entity_name,\n\t            dataset_ids=[dataset_id],\n\t            format=\"parquet\"\n\t        )\n", "        try:\n\t            source_connection_id = source_res[\"id\"]\n\t            if self.loggingEnabled:\n\t                self.logger.info(\"sourceConnection \" + source_connection_id + \" has been created.\")\n\t            return source_connection_id\n\t        except KeyError:\n\t            raise RuntimeError(f\"Error when creating source connection: {source_res}\")\n\t    def createTargetConnection(self, base_connection_id: str, compression_type: str, data_format: str, export_path: str, entity_name: str) -> str:\n\t        \"\"\"\n\t        Create a target connection for Data Landing Zone Destination\n", "        Arguments:\n\t            base_connection_id : REQUIRED : The base connection id created in previous step\n\t            compression_type: REQUIRED : define compression type to use when saving data to file. Possible values:\n\t                gzip\n\t                none\n\t            data_format : REQUIRED : define the data format . Possible values:\n\t                json\n\t                parquet\n\t            export_path : REQUIRED : define the folder path in your dlz container\n\t            entity_name: REQUIRED: define the name of target connection\n", "        Returns:\n\t          target_connection_id(str)\n\t        \"\"\"\n\t        target_res = self.flow_conn.createTargetConnection(\n\t            data={\n\t                \"name\": entity_name,\n\t                \"baseConnectionId\": base_connection_id,\n\t                \"params\": {\n\t                    \"mode\": \"Server-to-server\",\n\t                    \"compression\": compression_type,\n", "                    \"datasetFileType\": data_format,\n\t                    \"path\": export_path\n\t                },\n\t                \"connectionSpec\": {\n\t                    \"id\": self.DLZ_CONNECTION_SPEC_ID,\n\t                    \"version\": \"1.0\"\n\t                }\n\t            }\n\t        )\n\t        try:\n", "            target_connection_id = target_res[\"id\"]\n\t            if self.loggingEnabled:\n\t                self.logger.info(\"targetConnection \" + target_connection_id + \" has been created\")\n\t            return target_connection_id\n\t        except KeyError:\n\t            raise RuntimeError(f\"Error when creating target connection: {target_res}\")\n\t    def createFlow(self, source_connection_id: str, target_connection_id: str, on_schedule: bool, entity_name: str) -> str:\n\t        \"\"\"\n\t        Create a data flow for Data Landing Zone Destination\n\t        Arguments:\n", "          source_connection_id : REQUIRED : The source connection id created in previous step\n\t          target_connection_id : REQUIRED : The target connection id created in previous step\n\t          on_schedule: REQUIRED: define whether you would like to have the export following a fixed schedule or not\n\t          entity_name: REQUIRED: define the name of the flow\n\t        Returns:\n\t          dataflow_id(str)\n\t        \"\"\"\n\t        if on_schedule:\n\t            schedule_params = asdict(ScheduleParams(interval=3, timeUnit=\"hour\", startTime=int(time.time())))\n\t        else:\n", "            schedule_params = asdict(ScheduleParams(interval=1, timeUnit=\"day\", startTime=int(time.time() + 60*60*24*365)))\n\t        flow_data = Flow(\n\t            name = entity_name,\n\t            flow_spec = {\n\t                \"id\": self.DLZ_FLOW_SEPC_ID,\n\t                \"version\": \"1.0\"\n\t            },\n\t            source_connection_id = source_connection_id,\n\t            target_connection_id = target_connection_id,\n\t            schedule_params = schedule_params,\n", "            transformations = []\n\t        )\n\t        flow_obj = asdict(flow_data)\n\t        flow_res = self.flow_conn.createFlow(\n\t            obj = flow_obj,\n\t            flow_spec_id = self.DLZ_FLOW_SEPC_ID\n\t        )\n\t        try:\n\t            dataflow_id = flow_res[\"id\"]\n\t            if self.loggingEnabled:\n", "                self.logger.info(\"dataflow \" + dataflow_id + \" has been created\")\n\t            return dataflow_id\n\t        except KeyError:\n\t            raise RuntimeError(f\"Error when creating data flow: {flow_res}\")\n\t    def createFlowRun(self, dataflow_id: str, dataset_id: str, on_schedule: bool, initial_delay: int = 300):\n\t        \"\"\"\n\t        Create a data flow run\n\t        Arguments:\n\t          dataflow_id : REQUIRED : The flow that needs to be triggered a flow run\n\t          dataset_id : REQUIRED : The dataset that needs to be exported\n", "          on_schedule: REQUIRED: define whether you would like to have the export following a fixed schedule or not\n\t        \"\"\"\n\t        if not on_schedule:\n\t            #set up initial delay due to hollow refresh time interval\n\t            if self.loggingEnabled:\n\t                self.logger.info(f\"Waiting for flow {dataflow_id} to be refreshed after {initial_delay} seconds\")\n\t            time.sleep(initial_delay)\n\t            if self.loggingEnabled:\n\t                self.logger.info(f\"Start create adhoc dataset export for flow {dataflow_id} with dataset {dataset_id }\")\n\t            res = self.retryOnNotReadyException(dataflow_id = dataflow_id, dataset_id= dataset_id)\n", "            flow_run_id = res[\"destinations\"][0][\"datasets\"][0][\"statusURL\"].rsplit('/', 1)[-1]\n\t            #check if flowRun has finished\n\t            finished = False\n\t            while not finished:\n\t                try:\n\t                    run = self.flow_conn.getRun(runId = flow_run_id)[\"items\"][0]\n\t                    run_id = run[\"id\"]\n\t                    run_started_at = run[\"metrics\"][\"durationSummary\"][\"startedAtUTC\"]\n\t                    run_ended_at = run[\"metrics\"][\"durationSummary\"][\"completedAtUTC\"]\n\t                    run_duration_secs = (run_ended_at - run_started_at) / 1000\n", "                    run_size_mb = run[\"metrics\"][\"sizeSummary\"][\"outputBytes\"] / 1024. / 1024.\n\t                    run_num_rows = run[\"metrics\"][\"recordSummary\"][\"outputRecordCount\"]\n\t                    run_num_files = run[\"metrics\"][\"fileSummary\"][\"outputFileCount\"]\n\t                    if self.loggingEnabled:\n\t                        self.logger.info(f\"Run ID {run_id} completed with: duration={run_duration_secs} secs; size={run_size_mb} MB; num_rows={run_num_rows}; num_files={run_num_files}\")\n\t                    finished = True\n\t                except Exception as e:\n\t                    if self.loggingEnabled:\n\t                        self.logger.warn(f\"No runs completed yet for flow {dataflow_id}\")\n\t                    time.sleep(30)\n", "            #push the startDate to a year from now\n\t            start_time = int(time.time() + 60*60*24*365)\n\t            op = [asdict(FlowOp(op=\"Replace\", path=\"/scheduleParams\", value={\"startTime\": start_time}))]\n\t            flow_res = self.flow_conn.getFlow(dataflow_id)\n\t            etag = flow_res[\"etag\"]\n\t            self.flow_conn.updateFlow(dataflow_id,etag,op)\n\t            #disable flow after the adhoc run is completed\n\t            self.flow_conn.postFlowAction(dataflow_id, \"disable\")\n\t    def checkIfRetry(self, res: dict):\n\t        \"\"\"\n", "        check whether retry is needed base on the error message\n\t        Arguments:\n\t        res : REQUIRED : response from adhoc dataset export\n\t        Returns:\n\t          if retry is needed or not(boolean)\n\t        \"\"\"\n\t        error = str(res)\n\t        if self.loggingEnabled:\n\t            self.logger.info(f\"Checking if retry is needed for adhoc dataset export response {res}\")\n\t        return error.find(\"Following order ID(s) are not ready for dataset export\") != -1\n", "    def retryOnNotReadyException(self, dataflow_id: str, dataset_id: str, wait_time: int = 60, max_attempts: int= 5):\n\t        \"\"\"\n\t        retry adhoc dataset export based on the error type\n\t        Arguments:\n\t              dataflow_id : REQUIRED : The flow that needs to be triggered a flow run\n\t              dataset_id : REQUIRED : The dataset that needs to be exported\n\t              wait_time: OPTIONAL: the wait time between each retry\n\t              max_attempts: OPTIONAL: the maximum retry attempts\n\t        \"\"\"\n\t        retryer = Retrying(retry = retry_if_result(self.checkIfRetry),wait= wait_fixed(wait_time), stop=stop_after_attempt(max_attempts))\n", "        return retryer(self.dis_conn.createAdHocDatasetExport, {dataflow_id: [dataset_id]})"]}
{"filename": "aepp/datasets.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\tfrom copy import deepcopy\n\timport logging\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass DataSets:\n\t    \"\"\"\n\t    Class that provides methods to manage labels of datasets.\n\t    A complete documentation ca be found here:\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/dataset-service.yaml\n", "    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    REFERENCE_LABELS_CREATION = {\n\t        \"labels\": [[\"C1\", \"C2\"]],\n\t        \"optionalLabels\": [\n\t            {\n\t                \"option\": {\n\t                    \"id\": \"https://ns.adobe.com/{TENANT_ID}/schemas/{SCHEMA_ID}\",\n", "                    \"contentType\": \"application/vnd.adobe.xed-full+json;version=1\",\n\t                    \"schemaPath\": \"/properties/repositoryCreatedBy\",\n\t                },\n\t                \"labels\": [[\"S1\", \"S2\"]],\n\t            }\n\t        ],\n\t    }\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n", "        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Instantiate the DataSet class.\n\t        Arguments:\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t            loggingObject : OPTIONAL : logging object to log messages.\n", "        Additional kwargs will update the header.\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n", "            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n", "        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n", "        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n", "        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"dataset\"]\n\t        )\n\t    def getLabelSchemaTests(self, dataSetId: str = None) -> dict:\n\t        \"\"\"\n\t        Return the labels assigned to a dataSet\n\t        Argument:\n\t            dataSetId : REQUIRED : the dataSet ID to retrieve the labels\n\t        \"\"\"\n\t        if dataSetId is None:\n", "            raise ValueError(\"Require a dataSet ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getLabels\")\n\t        path = f\"/datasets/{dataSetId}/labels\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def headLabels(self, dataSetId: str = None) -> dict:\n\t        \"\"\"\n\t        Return the head assigned to a dataSet\n\t        Argument:\n", "            dataSetId : REQUIRED : the dataSet ID to retrieve the head data\n\t        \"\"\"\n\t        if dataSetId is None:\n\t            raise ValueError(\"Require a dataSet ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting headLabels\")\n\t        path = f\"/datasets/{dataSetId}/labels\"\n\t        res = self.connector.headData(self.endpoint + path)\n\t        return res\n\t    def deleteLabels(self, dataSetId: str = None, ifMatch: str = None) -> dict:\n", "        \"\"\"\n\t        Delete the labels of a dataset.\n\t        Arguments:\n\t            dataSetId : REQUIRED : The dataset ID to delete the labels for.\n\t            ifMatch : REQUIRED : the value is from the header etag of the headLabels. (use the headLabels method)\n\t        \"\"\"\n\t        if dataSetId is None:\n\t            raise ValueError(\"Require a dataSet ID\")\n\t        if ifMatch is None:\n\t            raise ValueError(\"Require the ifMatch parameter\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteLabels\")\n\t        path = f\"/datasets/{dataSetId}/labels\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"If-Match\"] = ifMatch\n\t        res = self.connector.deleteData(self.endpoint + path, headers=privateHeader)\n\t        return res\n\t    def createLabels(self, dataSetId: str = None, data: dict = None) -> dict:\n\t        \"\"\"\n\t        Assign labels to a dataset.\n", "        Arguments:\n\t            dataSetId : REQUIRED : The dataset ID to delete the labels for.\n\t            data : REQUIRED : Dictionary setting the labels to be added.\n\t                more info https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Datasets/postDatasetLabels\n\t        \"\"\"\n\t        if dataSetId is None:\n\t            raise ValueError(\"Require a dataSet ID\")\n\t        if data is None or type(data) != dict:\n\t            raise ValueError(\"Require a dictionary to pass labels\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting createLabels\")\n\t        path = f\"/datasets/{dataSetId}/labels\"\n\t        res = self.connector.postData(self.endpoint + path, data=data)\n\t        return res\n\t    def updateLabels(\n\t        self, dataSetId: str = None, data: dict = None, ifMatch: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Update the labels (PUT method)\n\t            dataSetId : REQUIRED : The dataset ID to delete the labels for.\n", "            data : REQUIRED : Dictionary setting the labels to be added.\n\t                more info https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Datasets/postDatasetLabels\n\t            ifMatch : REQUIRED : the value is from the header etag of the headLabels.(use the headLabels method)\n\t        \"\"\"\n\t        if dataSetId is None:\n\t            raise ValueError(\"Require a dataSet ID\")\n\t        if data is None or type(data) != dict:\n\t            raise ValueError(\"Require a dictionary to pass labels\")\n\t        if ifMatch is None:\n\t            raise ValueError(\"Require the ifMatch parameter\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateLabels\")\n\t        path = f\"/datasets/{dataSetId}/labels\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\"If-Match\"] = ifMatch\n\t        res = self.connector.putData(\n\t            self.endpoint + path, data=data, headers=privateHeader\n\t        )\n\t        return res\n"]}
{"filename": "aepp/observability.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\timport pandas as pd\n\timport logging\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass Observability:\n\t    \"\"\"\n\t    A class that presents the different methods available on the Observability API from AEP.\n\t    A complete documentation of the methods can be found here:\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/observability-insights.yaml\n", "    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n", "    ) -> None:\n\t        \"\"\"\n\t        Instanciate the observability API methods class.\n\t        Arguments:\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n", "        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n", "                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n", "            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n", "            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"observability\"]\n\t        )\n\t        self.POST_METRICS = {\n", "            \"start\": \"2020-07-14T00:00:00.000Z\",\n\t            \"end\": \"2020-07-22T00:00:00.000Z\",\n\t            \"granularity\": \"day\",\n\t            \"metrics\": [\n\t                {\n\t                    \"name\": \"timeseries.ingestion.dataset.recordsuccess.count\",\n\t                    \"filters\": [\n\t                        {\n\t                            \"name\": \"dataSetId\",\n\t                            \"value\": \"5edcfb2fbb642119194c7d94|5eddb21420f516191b7a8dad\",\n", "                            \"groupBy\": True,\n\t                        }\n\t                    ],\n\t                    \"aggregator\": \"sum\",\n\t                    \"downsample\": \"sum\",\n\t                },\n\t                {\n\t                    \"name\": \"timeseries.ingestion.dataset.dailysize\",\n\t                    \"filters\": [\n\t                        {\n", "                            \"name\": \"dataSetId\",\n\t                            \"value\": \"5eddb21420f516191b7a8dad\",\n\t                            \"groupBy\": False,\n\t                        }\n\t                    ],\n\t                    \"aggregator\": \"sum\",\n\t                    \"downsample\": \"sum\",\n\t                },\n\t            ],\n\t        }\n", "        self._loadREFERENCES()\n\t    def _loadREFERENCES(self):\n\t        \"\"\"\n\t        Load document as attributes if possible\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Loading references\")\n\t        try:\n\t            import importlib.resources as pkg_resources\n\t            pathIdentity = pkg_resources.path(\"aepp\", \"observability_identity.pickle\")\n", "            pathIngestion = pkg_resources.path(\"aepp\", \"observability_ingestion.pickle\")\n\t            pathGDPR = pkg_resources.path(\"aepp\", \"observability_gdpr.pickle\")\n\t            pathQS = pkg_resources.path(\"aepp\", \"observability_queryService.pickle\")\n\t            pathRLTime = pkg_resources.path(\"aepp\", \"observability_realTime.pickle\")\n\t        except ImportError:\n\t            # Try backported to PY<37 with pkg_resources.\n\t            if self.loggingEnabled:\n\t                self.logger.debug(f\"Loading references - ImportError - 2nd try\")\n\t            try:\n\t                import pkg_resources\n", "                pathIdentity = pkg_resources.resource_filename(\n\t                    \"aepp\", \"observability_identity.pickle\"\n\t                )\n\t                pathIngestion = pkg_resources.resource_filename(\n\t                    \"aepp\", \"observability_ingestion.pickle\"\n\t                )\n\t                pathGDPR = pkg_resources.resource_filename(\n\t                    \"aepp\", \"observability_gdpr.pickle\"\n\t                )\n\t                pathQS = pkg_resources.resource_filename(\n", "                    \"aepp\", \"observability_queryService.pickle\"\n\t                )\n\t                pathRLTime = pkg_resources.resource_filename(\n\t                    \"aepp\", \"observability_realTime.pickle\"\n\t                )\n\t            except:\n\t                print(\"no supported files\")\n\t                if self.loggingEnabled:\n\t                    self.logger.debug(f\"Failed loading references\")\n\t        try:\n", "            with pathIdentity as f:\n\t                self.REFERENCE_IDENTITY = pd.read_pickle(f)\n\t                self.REFERENCE_IDENTITY = self.REFERENCE_IDENTITY.style.set_properties(\n\t                    subset=[\"Insights metric\"], **{\"width\": \"100px\"}\n\t                )\n\t            with pathIngestion as f:\n\t                self.REFERENCE_INGESTION = pd.read_pickle(f)\n\t                self.REFERENCE_INGESTION = (\n\t                    self.REFERENCE_INGESTION.style.set_properties(\n\t                        subset=[\"Insights metric\"], **{\"width\": \"100px\"}\n", "                    )\n\t                )\n\t            with pathGDPR as f:\n\t                self.REFERENCE_GDPR = pd.read_pickle(f)\n\t                self.REFERENCE_GDPR = self.REFERENCE_GDPR.style.set_properties(\n\t                    subset=[\"Insights metric\"], **{\"width\": \"100px\"}\n\t                )\n\t            with pathRLTime as f:\n\t                self.REFERENCE_REALTIME = pd.read_pickle(f)\n\t                self.REFERENCE_REALTIME = self.REFERENCE_REALTIME.style.set_properties(\n", "                    subset=[\"Insights metric\"], **{\"width\": \"100px\"}\n\t                )\n\t            with pathQS as f:\n\t                self.REFERENCE_QUERYSERVICE = pd.read_pickle(f)\n\t                self.REFERENCE_QUERYSERVICE = (\n\t                    self.REFERENCE_QUERYSERVICE.style.set_properties(\n\t                        subset=[\"Insights metric\"], **{\"width\": \"100px\"}\n\t                    )\n\t                )\n\t        except:\n", "            if self.loggingEnabled:\n\t                self.logger.debug(f\"Failed loading references - backup to None\")\n\t            self.REFERENCE_IDENTITY = None\n\t            self.REFERENCE_INGESTION = None\n\t            self.REFERENCE_QUERYSERVICE = None\n\t            self.REFERENCE_GDPR = None\n\t            self.REFERENCE_REALTIME = None\n\t    def createMetricsReport(self, data: dict = None) -> dict:\n\t        \"\"\"\n\t        Using the POST method to retrieve metrics specified in the data dictionary.\n", "        Please use the different REFERENCES attributes to know which metrics are supported.\n\t        You have a template for the data dictionary on the POST_METRICS attribute.\n\t        Arguments:\n\t            data : REQUIRED : The metrics requested in the report creation.\n\t                You can use the POST_METRICS attribute to see a template.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createMetricsReport\")\n\t        path = \"/metrics\"\n\t        res = self.connector.postData(self.endpoint + path, data=data)\n", "        return res\n"]}
{"filename": "aepp/privacyservice.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\timport logging\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass Privacy:\n\t    \"\"\"\n\t    Class to instanciate a Privacy API connection. Ensure you have the correct access within you Adobe IO connection.\n\t    Information about that class can be found here : https://docs.adobe.com/content/help/en/experience-platform/privacy/api/privacy-jobs.html\n\t    \"\"\"\n\t    SAMPLE_PAYLOAD = {\n", "        \"companyContexts\": [{\"namespace\": \"imsOrgID\", \"value\": \"{IMS_ORG}\"}],\n\t        \"users\": [\n\t            {\n\t                \"key\": \"DavidSmith\",\n\t                \"action\": [\"access\"],\n\t                \"userIDs\": [\n\t                    {\n\t                        \"namespace\": \"email\",\n\t                        \"value\": \"dsmith@acme.com\",\n\t                        \"type\": \"standard\",\n", "                    },\n\t                    {\n\t                        \"namespace\": \"ECID\",\n\t                        \"type\": \"standard\",\n\t                        \"value\": \"443636576799758681021090721276\",\n\t                        \"isDeletedClientSide\": False,\n\t                    },\n\t                ],\n\t            },\n\t            {\n", "                \"key\": \"user12345\",\n\t                \"action\": [\"access\", \"delete\"],\n\t                \"userIDs\": [\n\t                    {\n\t                        \"namespace\": \"email\",\n\t                        \"value\": \"ajones@acme.com\",\n\t                        \"type\": \"standard\",\n\t                    },\n\t                    {\n\t                        \"namespace\": \"loyaltyAccount\",\n", "                        \"value\": \"12AD45FE30R29\",\n\t                        \"type\": \"integrationCode\",\n\t                    },\n\t                ],\n\t            },\n\t        ],\n\t        \"include\": [\"Analytics\", \"AudienceManager\"],\n\t        \"expandIds\": False,\n\t        \"priority\": \"normal\",\n\t        \"analyticsDeleteMethod\": \"anonymize\",\n", "        \"regulation\": \"ccpa\",\n\t    }\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        privacyScope: bool = True,\n\t        aepScope: bool = False,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n", "        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Instanciate the class for Privacy Service call.\n\t        Arguments:\n\t            privacyScope : REQUIRED : set the connection retrieved process with the Privacy JWT scope (default True).\n\t            aepScope : OPTIONAL : set the connection retrieved process with the AEP JWT scope if set to True (default False).\n\t            config : OPTIONAL : config object in the config module.\n", "            header : OPTIONAL : header object  in the config module.\n\t        kwargs:\n\t            kwargs will update the header\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n", "            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n", "                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n", "            aepScope=aepScope,\n\t            privacyScope=privacyScope,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"privacy\"]\n\t        )\n", "    def getJobs(\n\t        self, regulation: str = None, limit: int = 50, status: str = None, **kwargs\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns the job that are being processed on Adobe.\n\t        Arguments:\n\t            regulation : REQUIRED : The privacy regulation to return jobs from. (gdpr, ccpa, pdpa_tha)\n\t            limit : OPTIONAL : The number of jobs to return in the response body.(default 50)\n\t            status : OPTIONAL : Filters jobs by processing status. (complete, processing, submitted, error)\n\t        Possible kwargs: see documentation : https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Privacy_Jobs/fetchAllJobs\n", "        \"\"\"\n\t        if regulation is None:\n\t            raise Exception(\"Required regulation parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getJobs\")\n\t        path = \"/jobs\"\n\t        params = {\"size\": limit, \"regulation\": regulation}\n\t        if status is not None:\n\t            params[\"status\"] = status\n\t        if len(kwargs) > 0:\n", "            for key in kwargs:\n\t                params[key] = str(kwargs[key])\n\t        res = self.connector.getData(\n\t            self.endpoint + path, params=params, headers=self.header\n\t        )\n\t        return res\n\t    def getJob(self, jobId: str = None) -> dict:\n\t        \"\"\"\n\t        Return a specific job by its job ID.\n\t        Arguments:\n", "            jobId : REQUIRED : the Job ID to fetch\n\t        \"\"\"\n\t        if jobId is None:\n\t            raise Exception(\"Require a job ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getJob\")\n\t        path = f\"/jobs/{jobId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def postJob(self, data: dict = None) -> dict:\n", "        \"\"\"\n\t        Return a specific job by its job ID.\n\t        Arguments:\n\t            data : REQUIRED : data to be send in order to start a job.\n\t            You can use the SAMPLE_PAYLOAD to help you create the data.\n\t        \"\"\"\n\t        if data is None or type(data) != dict:\n\t            raise Exception(\"Require a dictionary to be passed\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting postJob\")\n", "        path = \"/jobs/\"\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=data, headers=self.header\n\t        )\n\t        return res\n\t    def postChildJob(self, jobId: str = None, data: dict = None) -> dict:\n\t        \"\"\"\n\t        This is to add a job on a parent Job.\n\t        Argument:\n\t            jobId : REQUIRED : the Job ID to append the job to\n", "            data : REQUIRED : dictionary of data to be added.\n\t        \"\"\"\n\t        if jobId is None:\n\t            raise Exception(\"Require a job ID\")\n\t        if data is None or type(data) != dict:\n\t            raise Exception(\"Require a dictionary to be passed\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting gpostChildJob\")\n\t        path = f\"/jobs/{jobId}/child-job\"\n\t        res = self.connector.postData(\n", "            self.endpoint + path, data=data, headers=self.header\n\t        )\n\t        return res\n"]}
{"filename": "aepp/__version__.py", "chunked_list": ["__version__ = \"0.3.1-4\"\n"]}
{"filename": "aepp/dataprep.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\timport aepp\n", "from aepp import connector\n\tfrom copy import deepcopy\n\timport pandas as pd\n\tfrom typing import Union\n\timport re\n\timport logging\n\tfrom .configs import ConnectObject\n\tclass DataPrep:\n\t    \"\"\"\n\t    This class instanciate the data prep capability.\n", "    The data prep is mostly use for the mapping service and you can find some documentation on this in the following part:\n\t        https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/data-prep.yaml\n\t        https://experienceleague.adobe.com/docs/experience-platform/data-prep/home.html\n\t    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n", "        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t            This will instantiate the Mapping class\n\t            Arguments:\n\t                config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t                header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t                loggingObject : OPTIONAL : logging object to log messages.\n", "        kwargs:\n\t            kwargs value will update the header\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n", "                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n", "                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n", "            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n", "            self.sandbox = self.connector.config[\"sandbox\"]\n\t        # same endpoint than segmentation\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"mapping\"]\n\t        )\n\t        self.REFERENCE_MAPPING = {\"sourceType\": \"\", \"source\": \"\", \"destination\": \"\"}\n\t    def getXDMBatchConversions(\n\t        self,\n\t        dataSetId: str = None,\n\t        prop: str = None,\n", "        batchId: str = None,\n\t        status: str = None,\n\t        limit: int = 100,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns all XDM conversions\n\t        Arguments:\n\t            dataSetId : OPTIONAL : Destination dataSet ID to filter for.\n\t            property : OPTIONAL : Filters for dataSetId, batchId and Status.\n\t            batchId : OPTIONAL : batchId Filter\n", "            status : OPTIONAL : status of the batch.\n\t            limit : OPTIONAL : number of results to return (default 100)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getXDMBatchConversions\")\n\t        path = \"/xdmBatchConversions\"\n\t        params = {\"limit\": limit}\n\t        if dataSetId is not None:\n\t            params[\"destinationDatasetId\"] = dataSetId\n\t        if prop is not None:\n", "            params[\"property\"] = prop\n\t        if batchId is not None:\n\t            params[\"sourceBatchId\"] = batchId\n\t        if status is not None:\n\t            params[\"status\"] = status\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        return res\n\t    def getXDMBatchConversion(self, conversionId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns XDM Conversion info.\n", "        Arguments:\n\t            conversionId : REQUIRED : Conversion ID to be returned\n\t        \"\"\"\n\t        if conversionId is None:\n\t            raise ValueError(\"Require a conversion ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getXDMBatchConversion\")\n\t        path = f\"/xdmBatchConversions/{conversionId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n", "    def getXDMBatchConversionActivities(self, conversionId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns activities for a XDM Conversion ID.\n\t        Arguments:\n\t            conversionId : REQUIRED : Conversion ID for activities to be returned\n\t        \"\"\"\n\t        if conversionId is None:\n\t            raise ValueError(\"Require a conversion ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getXDMBatchConversionActivities\")\n", "        path = f\"/xdmBatchConversions/{conversionId}/activities\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getXDMBatchConversionRequestActivities(\n\t        self, requestId: str = None, activityId: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns conversion activities for given request\n\t        Arguments:\n\t            requestId : REQUIRED : the request ID to look for\n", "            activityId : REQUIRED : the activity ID to look for\n\t        \"\"\"\n\t        if requestId is None:\n\t            raise ValueError(\"Require a request ID\")\n\t        if activityId is None:\n\t            raise ValueError(\"Require a activity ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getXDMBatchConversionRequestActivities\")\n\t        path = f\"/xdmBatchConversions/{requestId}/activities/{activityId}\"\n\t        res = self.connector.getData(self.endpoint + path + path)\n", "        return res\n\t    def createXDMConversion(\n\t        self, dataSetId: str = None, batchId: str = None, mappingId: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a XDM conversion request.\n\t        Arguments:\n\t            dataSetId : REQUIRED : destination dataSet ID\n\t            batchId : REQUIRED : Source Batch ID\n\t            mappingSetId : REQUIRED : Mapping ID to be used\n", "        \"\"\"\n\t        if dataSetId is None:\n\t            raise ValueError(\"Require a destination dataSet ID\")\n\t        if batchId is None:\n\t            raise ValueError(\"Require a source batch ID\")\n\t        if mappingId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createXDMConversion\")\n\t        path = \"/xdmBatchConversions\"\n", "        params = {\n\t            \"destinationDataSetId\": dataSetId,\n\t            \"sourceBatchId\": batchId,\n\t            \"mappingSetId\": mappingId,\n\t        }\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        return res\n\t    def copyMappingRules(\n\t        self, mapping: Union[dict, list] = None, tenantId: str = None\n\t    ) -> list:\n", "        \"\"\"\n\t        create a copy of the mapping based on the mapping information passed.\n\t        Argument:\n\t            mapping : REQUIRED : either the list of mapping or the dictionary returned from the getMappingSetMapping\n\t            tenantid : REQUIRED : in case tenant is present, replace the existing one with new one.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting copyMapping\")\n\t        if tenantId.startswith(\"_\") == False:\n\t            tenantId = f\"_{tenantId}\"\n", "        if mapping is None:\n\t            raise ValueError(\"Require a mapping object\")\n\t        if type(mapping) == list:\n\t            new_list = [\n\t                {\n\t                    \"sourceType\": map[\"sourceType\"],\n\t                    \"source\": map[\"source\"],\n\t                    \"destination\": re.sub(\n\t                        \"^_[\\w]+\\.\", f\"{tenantId}.\", map[\"destination\"]\n\t                    ),\n", "                }\n\t                for map in mapping\n\t            ]\n\t            return new_list\n\t        if type(mapping) == dict:\n\t            if \"mappings\" in mapping.keys():\n\t                mappings = mapping[\"mappings\"]\n\t                new_list = [\n\t                    {\n\t                        \"sourceType\": map[\"sourceType\"],\n", "                        \"source\": map[\"source\"],\n\t                        \"destination\": re.sub(\n\t                            \"^_[\\w]+\\.\", f\"{tenantId}.\", map[\"destination\"]\n\t                        ),\n\t                    }\n\t                    for map in mappings\n\t                ]\n\t                return new_list\n\t            else:\n\t                print(\"Couldn't find a mapping information to copy\")\n", "                return None\n\t    def cleanMappingRules(self, mapping: Union[dict, list] = None\n\t    ) -> list:\n\t        \"\"\"\n\t        create a clean copy of the mapping based on the mapping list information passed.\n\t        Argument:\n\t            mapping : REQUIRED : either the list of mapping or the dictionary returned from the getMappingSetMapping\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting copyMapping\")\n", "        if mapping is None:\n\t            raise ValueError(\"Require a mapping object\")\n\t        if type(mapping) == list:\n\t            new_list = [\n\t                {\n\t                    \"sourceType\": map[\"sourceType\"],\n\t                    \"source\": map[\"source\"],\n\t                    \"destination\":  map[\"destination\"]\n\t                }\n\t                for map in mapping\n", "            ]\n\t            return new_list\n\t        if type(mapping) == dict:\n\t            if \"mappings\" in mapping.keys():\n\t                mappings = mapping[\"mappings\"]\n\t                new_list = [\n\t                    {\n\t                        \"sourceType\": map[\"sourceType\"],\n\t                        \"source\": map[\"source\"],\n\t                        \"destination\": map[\"destination\"]\n", "                    }\n\t                    for map in mappings\n\t                ]\n\t                return new_list\n\t            else:\n\t                print(\"Couldn't find a mapping information to clean\")\n\t                return None\n\t    def getMappingSets(\n\t        self, name: str = None, prop: str = None, limit: int = 100\n\t    ) -> list:\n", "        \"\"\"\n\t        Returns all mapping sets for given IMS Org Id\n\t        Arguments:\n\t            name : OPTIONAL : Filtering by name\n\t            prop : OPTIONAL : property filter. Supported fields are: xdmSchema, status.\n\t                Example : prop=\"status==success\"\n\t            limit : OPTIONAL : number of result to retun. Default 100.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMappingSets\")\n", "        params = {\"limit\": limit}\n\t        if name is not None:\n\t            params[\"name\"] = name\n\t        if prop is not None:\n\t            params[\"property\"] = prop\n\t        path = \"/mappingSets\"\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        data = res[\"data\"]\n\t        return data\n\t    def getMappingSuggestions(\n", "        self, dataSetId: str = None, batchId: str = None, excludeUnmapped: bool = True\n\t    ) -> dict:\n\t        \"\"\"\n\t        Returns non-persisted mapping set suggestion for review\n\t        Arguments:\n\t            dataSetId : OPTIONAL : Id of destination DataSet. Must be a DataSet with schema.\n\t            batchId : OPTIONAL : Id of source Batch.\n\t            excludeUnmapped : OPTIONAL : Exclude unmapped source attributes (default True)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getMappingSuggestions\")\n\t        path = \"/mappingSets/suggestion\"\n\t        params = {\"excludeUnmapped\": excludeUnmapped}\n\t        if dataSetId is not None:\n\t            params[\"datasetId\"] = dataSetId\n\t        if batchId is not None:\n\t            params[\"batchId\"] = batchId\n\t        res = self.connector.getData(self.endpoint + path, params=params)\n\t        return res\n\t    def getMappingSet(\n", "            self, \n\t            mappingSetId: str = None,\n\t            save: bool = False,\n\t            saveMappingRules: bool = False, \n\t            mappingRulesOnly: bool = False,\n\t            **kwargs\n\t    ) -> dict:\n\t        \"\"\"\n\t        Get a specific mappingSet by its ID.\n\t        Argument:\n", "            mappingSetId : REQUIRED : mappingSet ID to be retrieved.\n\t            save : OPTIONAL : save your mapping set defintion in a JSON file.\n\t            saveMappingRules : OPTIONAL : save your mapping rules only in a JSON file\n\t            mappingRulesOnly : OPTIONAL : If set to True, return only the mapping rules.\n\t        optional kwargs:\n\t            encoding : possible to set encoding for the file.\n\t        \"\"\"\n\t        if mappingSetId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getMappingSet\")\n\t        path = f\"/mappingSets/{mappingSetId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        if save:\n\t            aepp.saveFile(\n\t                module=\"dataPrep\",\n\t                file=res,\n\t                filename=f\"mapping_{res['id']}\",\n\t                type_file=\"json\",\n\t                encoding=kwargs.get(\"encoding\", \"utf-8\"),\n", "            )\n\t        if saveMappingRules:\n\t            aepp.saveFile(\n\t                module=\"dataPrep\",\n\t                file=self.cleanMappingRules(res),\n\t                filename=f\"mapping_rules_{res['id']}\",\n\t                type_file=\"json\",\n\t                encoding=kwargs.get(\"encoding\", \"utf-8\"),\n\t            )\n\t        if mappingRulesOnly:\n", "            mappingRules = self.cleanMappingRules(res)\n\t            return mappingRules\n\t        return res\n\t    def deleteMappingSet(self, mappingSetId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific mappingSet by its ID.\n\t        Argument:\n\t            mappingSetId : REQUIRED : mappingSet ID to be deleted.\n\t        \"\"\"\n\t        if mappingSetId is None:\n", "            raise ValueError(\"Require a mapping ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteMappingSet\")\n\t        path = f\"/mappingSets/{mappingSetId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def createMappingSet(\n\t        self,\n\t        schemaId: str = None,\n\t        mappingList: list = None,\n", "        validate: bool = False,\n\t        verbose: bool = False,\n\t        mappingSet: dict = None,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create a mapping set.\n\t        Arguments:\n\t            schemaId : OPTIONAL : schemaId to map to.\n\t            mappingList: OPTIONAL : List of mapping to set.\n\t            validate : OPTIONAL : Validate the mapping.\n", "        if you want to provide a dictionary for mapping set creation, you can pass the following params:\n\t            mappingSet : REQUIRED : A dictionary that creates the mapping info.\n\t                see info on https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/Mappings/createMappingSetUsingPOST\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createMappingSet\")\n\t        path = \"/mappingSets\"\n\t        params = {\"validate\": validate}\n\t        if (mappingSet is None or type(mappingSet) != dict) and (\n\t            schemaId is None and mappingList is None\n", "        ):\n\t            raise ValueError(\n\t                \"Require a dictionary as mapping set or some schema reference ID and a mapping list\"\n\t            )\n\t        if mappingSet is not None:\n\t            res = self.connector.postData(\n\t                self.endpoint + path, params=params, data=mappingSet, verbose=verbose\n\t            )\n\t        elif schemaId is not None and mappingList is not None:\n\t            obj = {\n", "                \"outputSchema\": {\n\t                    \"schemaRef\": {\n\t                        \"id\": schemaId,\n\t                        \"contentType\": \"application/vnd.adobe.xed-full+json;version=1\",\n\t                    }\n\t                },\n\t                \"mappings\": mappingList,\n\t            }\n\t            res = self.connector.postData(\n\t                self.endpoint + path, params=params, data=obj, verbose=verbose\n", "            )\n\t        return res\n\t    def updateMappingSet(\n\t        self, mappingSetId: str = None, mappingRules: list = None,outputSchema:dict=None,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Update a specific Mapping set based on its Id.\n\t        Arguments:\n\t            mappingSetId : REQUIRED : mapping Id to be updated\n\t            mappingRules : REQUIRED : the list of different rule to map\n", "            outputSchema : OPTIONAL : If you wish to change the destination output schema. By default taking the same one.\n\t        \"\"\"\n\t        if mappingSetId is None:\n\t            raise ValueError(\"Require a mappingSet ID\")\n\t        if mappingRules is None:\n\t            raise ValueError(\"Require a list of mappings \")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateMappingSet\")\n\t        path = f\"/mappingSets/{mappingSetId}\"\n\t        if outputSchema is None:\n", "            currMapping = self.getMappingSet(mappingSetId)\n\t            outputSchema = {\n\t                \"schemaRef\" : currMapping.get('outputSchema',{}).get('schemaRef')\n\t            }\n\t        data = {\n\t            \"outputSchema\":outputSchema,\n\t            \"mappings\":mappingRules\n\t        }\n\t        res = self.connector.putData(self.endpoint + path, data=data)\n\t        return res\n", "    def getMappingSetMappings(self, mappingSetId: str = None) -> dict:\n\t        \"\"\"\n\t        Returns all mappings for a mapping set\n\t        Arguments:\n\t            mappingSetId : REQUIRED : the mappingSet ID to retrieved\n\t        \"\"\"\n\t        if mappingSetId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMappingSetMappings\")\n", "        path = f\"/mappingSets/{mappingSetId}/mappings\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def createMappingSetMapping(\n\t        self, mappingSetId: str = None, mapping: dict = None, verbose: bool = False\n\t    ) -> dict:\n\t        \"\"\"\n\t        Create mappings for a mapping set\n\t        Arguments:\n\t            mappingSetId : REQUIRED : the mappingSet ID to attached the mapping\n", "            mapping : REQUIRED : a dictionary to define the new mapping.\n\t        \"\"\"\n\t        if mappingSetId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if mapping is None or type(mapping) != dict:\n\t            raise Exception(\"Require a dictionary as mapping\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createMappingSetMapping\")\n\t        path = f\"/mappingSets/{mappingSetId}/mappings\"\n\t        res = self.connector.postData(\n", "            self.endpoint + path, data=mapping, verbose=verbose\n\t        )\n\t        return res\n\t    def getMappingSetMapping(\n\t        self, mappingSetId: str = None, mappingId: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Get a mapping from a mapping set.\n\t        Arguments:\n\t            mappingSetId : REQUIRED : The mappingSet ID\n", "            mappingId : REQUIRED : The specific Mapping\n\t        \"\"\"\n\t        if mappingSetId is None:\n\t            raise ValueError(\"Require a mappingSet ID\")\n\t        if mappingId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMappingSetMapping\")\n\t        path = f\"/mappingSets/{mappingSetId}/mappings/{mappingId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n", "        return res\n\t    def deleteMappingSetMapping(\n\t        self, mappingSetId: str = None, mappingId: str = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Delete a mapping in a mappingSet\n\t        Arguments:\n\t            mappingSetId : REQUIRED : The mappingSet ID\n\t            mappingId : REQUIRED : The specific Mapping\n\t        \"\"\"\n", "        if mappingSetId is None:\n\t            raise ValueError(\"Require a mappingSet ID\")\n\t        if mappingId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteMappingSetMapping\")\n\t        path = f\"/mappingSets/{mappingSetId}/mappings/{mappingId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def updateMappingSetMapping(\n", "        self, mappingSetId: str = None, mappingId: str = None, mapping: dict = None\n\t    ) -> dict:\n\t        \"\"\"\n\t        Update a mapping for a mappingSet (PUT method)\n\t        Arguments:\n\t            mappingSetId : REQUIRED : The mappingSet ID\n\t            mappingId : REQUIRED : The specific Mapping\n\t            mapping : REQUIRED : dictionary to update\n\t        \"\"\"\n\t        if mappingSetId is None:\n", "            raise ValueError(\"Require a mappingSet ID\")\n\t        if mappingId is None:\n\t            raise ValueError(\"Require a mapping ID\")\n\t        if mapping is None or type(mapping) != dict:\n\t            raise Exception(\"Require a dictionary as mapping\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateMappingSetMapping\")\n\t        path = f\"/mappingSets/{mappingSetId}/mappings/{mappingId}\"\n\t        res = self.connector.putData(self.endpoint + path, data=mapping)\n\t        return res\n", "    def previewDataOutput(self, data: dict = None, mappingSet: dict = None) -> dict:\n\t        \"\"\"\n\t        The data you want to run through as a preview, which will be transformed by the mapping sets within the body.\n\t        Arguments:\n\t            data : REQUIRED : A dictionary containing the data to test.\n\t            mappingSet : REQUIRED : The mappingSet to test.\n\t        Example:\n\t        {\n\t            \"data\": {\n\t                \"id\": 1234,\n", "                \"firstName\": \"Jim\",\n\t                \"lastName\": \"Seltzer\"\n\t            },\n\t            \"mappingSet\": {\n\t                \"outputSchema\": {\n\t                \"schemaRef\": {\n\t                    \"id\": \"https://ns.adobe.com/stardust/schemas/89abc189258b1cb1a816d8f2b2341a6d98000ed8f4008305\",\n\t                    \"contentType\": \"application/vnd.adobe.xed-full+json;version=1\"\n\t                }\n\t                },\n", "                \"mappings\": [\n\t                {\n\t                    \"sourceType\": \"ATTRIBUTE\",\n\t                    \"source\": \"id\",\n\t                    \"destination\": \"_id\",\n\t                    \"name\": \"id\",\n\t                    \"description\": \"Identifier field\"\n\t                },\n\t                {\n\t                    \"sourceType\": \"ATTRIBUTE\",\n", "                    \"source\": \"firstName\",\n\t                    \"destination\": \"person.name.firstName\"\n\t                },\n\t                {\n\t                    \"sourceType\": \"ATTRIBUTE\",\n\t                    \"source\": \"lastName\",\n\t                    \"destination\": \"person.name.lastName\"\n\t                }\n\t                ]\n\t            }\n", "        }\n\t        \"\"\"\n\t        if data is None:\n\t            raise ValueError(\"Require a dictionary that contains the data to be tested\")\n\t        if mappingSet is None:\n\t            raise ValueError(\"Require a dictionary that contains the mapping set\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting previewDataOutput\")\n\t        path = \"/mappingSets/preview\"\n\t        dataObject = {\"data\": data, \"mappingSet\": mappingSet}\n", "        res = self.connector.postData(self.endpoint + path, data=dataObject)\n\t        return res\n\t    def getMappingSetFunctions(\n\t        self,\n\t    ) -> list:\n\t        \"\"\"\n\t        Return list of mapping functions.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMappingSetFunctions\")\n", "        path = \"/languages/el/functions\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def getMappingSetOperators(\n\t        self,\n\t    ) -> list:\n\t        \"\"\"\n\t        Return list of mapping operators.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getMappingSetOperators\")\n\t        path = \"/languages/el/operators\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def validateExpression(\n\t        self,\n\t        expression: str = None,\n\t        mappingSetId: str = None,\n\t        sampleData: str = None,\n\t        sampleDataType: str = None,\n", "    ) -> dict:\n\t        \"\"\"\n\t        Check if the expression that you have passed is valid.\n\t        Arguments:\n\t            expression : REQUIRED : the expression you are trying to validate.\n\t            mappingSetId : OPTIONAL : MappingSetId to integrate this expression.\n\t            sampleData : OPTIONAL : Sample Date to validate\n\t            sampleDataType : OPTIONAL : Data Type of your Sample data.\n\t        \"\"\"\n\t        if expression is None:\n", "            raise ValueError(\"Require an expression\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting validateExpression\")\n\t        path = \"/languages/el/validate\"\n\t        data = {\"expression\": expression}\n\t        if mappingSetId is not None:\n\t            data[\"mappingSetId\"] = mappingSetId\n\t        if sampleData is not None:\n\t            data[\"sampleData\"] = sampleData\n\t        if sampleDataType is not None:\n", "            data[\"sampleDataType\"] = sampleDataType\n\t        res = self.connector.postData(self.endpoint + path, data=data)\n\t        return res\n"]}
{"filename": "aepp/segmentation.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# Internal Library\n", "import aepp\n\tfrom aepp import connector\n\timport time\n\tfrom concurrent import futures\n\timport logging\n\tfrom typing import Union\n\tfrom copy import deepcopy\n\tfrom .configs import ConnectObject\n\tclass Segmentation:\n\t    \"\"\"\n", "    A class containing methods to use on segmentation.\n\t    A complete documentation can be found here:\n\t    https://www.adobe.io/apis/experienceplatform/home/api-reference.html#!acpdr/swagger-specs/segmentation.yaml\n\t    \"\"\"\n\t    ## logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    PLATFORM_AUDIENCE_DICT = {\n\t                    \"name\": \"People who ordered in the last 30 days\",\n\t                    \"profileInstanceId\": \"ups\",\n", "                    \"description\": \"This audience is generated to see people who ordered in the last 30 days.\",\n\t                    \"type\": \"SegmentDefinition\",\n\t                    \"expression\": {\n\t                        \"type\": \"PQL\",\n\t                        \"format\": \"pql/text\",\n\t                        \"value\": \"workAddress.country = \\\"US\\\"\"\n\t                    },\n\t                    \"schema\": {\n\t                        \"name\": \"_xdm.context.profile\"\n\t                    },\n", "                    \"labels\": [\n\t                        \"core/C1\"\n\t                    ],\n\t                    \"ttlInDays\": 60\n\t                }\n\t    EXTERNAL_AUDIENCE_DICT = {\n\t                    \"audienceId\": \"test-external-audience-id\",\n\t                    \"name\": \"externalSegment1\",\n\t                    \"namespace\": \"aam\",\n\t                    \"description\": \"This audience is generated to see people who ordered in the last 30 days.\",\n", "                    \"type\": \"ExternalSegment\",\n\t                    \"lifecycle\": \"published\",\n\t                    \"datasetId\": \"6254cf3c97f8e31b639fb14d\",\n\t                    \"labels\": [\n\t                        \"core/C1\"\n\t                    ],\n\t                    \"audienceMeta\": {\n\t                        \"segmentStatus\": \"ACTIVE\",\n\t                        \"AAMFolderId\": \"325813-testnew\"\n\t                    },\n", "                    \"linkedAudienceRef\": {\n\t                        \"flowId\": \"4685ea90-d2b6-11ec-9d64-0242ac120002\"\n\t                    }\n\t                }  \n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n", "    ) -> None:\n\t        \"\"\"\n\t        Instanciate the segmentation API methods class-\n\t        Arguments:\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t            config : OPTIONAL : config object in the config module. (DO NOT MODIFY)\n\t            header : OPTIONAL : header object  in the config module. (DO NOT MODIFY)\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n\t            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n", "        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n\t                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n", "                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n\t            header = config.getConfigHeader()\n", "            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n", "            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n\t        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"segmentation\"]\n\t        )\n\t        self.SCHEDULE_TEMPLATE = {\n", "            \"name\": \"profile-default\",\n\t            \"type\": \"batch_segmentation-OR-export\",\n\t            \"properties\": {\"segments\": [\"*\"]},\n\t            \"schedule\": \"0 0 1 * * ?\",\n\t            \"state\": \"inactive\",\n\t        }\n\t    def getResource(self,endpoint:str=None, params:dict=None, **kwargs)->dict:\n\t        \"\"\"\n\t        Abstract GET requests with header from the the connection.\n\t        Arguments:\n", "            endpoint : REQUIRED : the endpoint to use\n\t            params : OPTIONAL : the parameters to use in the GET requests\n\t        possible kwargs:\n\t            all kwargs are passed to the header\n\t        \"\"\"\n\t        if endpoint is None:\n\t            raise ValueError(\"Endpoint is required\")\n\t        if type(params) != dict:\n\t            raise TypeError(\"params should be a dictionary\")\n\t        parameters = {**params}\n", "        myPrivateHeader = deepcopy(self.header)\n\t        if kwargs is None:\n\t            for key in kwargs:\n\t                myPrivateHeader[key] = kwargs[key]\n\t        res = self.connector.getData(endpoint, headers=myPrivateHeader,params=parameters)\n\t        return res\n\t    def getSegments(self, onlyRealTime: bool = False, **kwargs) -> list:\n\t        \"\"\"\n\t        Return segment definitions in your experience platfom instance.\n\t        Arguments:\n", "            onlyRealTime : OPTIONAL : If you wish to retrieve only real time compatible segment. (default False)\n\t        Possible arguments:\n\t            - limit : number of segment returned per page\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSegments\")\n\t        params = {\"limit\": kwargs.get(\"limit\", 100)}\n\t        if onlyRealTime:\n\t            params[\"evaluationInfo.continuous.enabled\"] = True\n\t        path = \"/segment/definitions\"\n", "        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        if \"segments\" in res.keys():\n\t            data = res[\"segments\"]\n\t        else:\n\t            data = []\n\t        total_pages = res[\"page\"][\"totalPages\"]\n\t        if total_pages > 1:\n\t            nb_request = total_pages\n\t            max_workers = min((total_pages, 5))\n\t            list_parameters = [\n", "                {\"page\": str(x), **params} for x in range(1, total_pages + 1)\n\t            ]\n\t            urls = [self.endpoint + path for x in range(1, total_pages + 1)]\n\t            with futures.ThreadPoolExecutor(max_workers) as executor:\n\t                res = executor.map(\n\t                    lambda x, y: self.connector.getData(x, params=y),\n\t                    urls,\n\t                    list_parameters,\n\t                )\n\t            res = list(res)\n", "            append_data = [\n\t                val for sublist in [data[\"segments\"] for data in res] for val in sublist\n\t            ]  # flatten list of list\n\t            data = data + append_data\n\t        return data\n\t    def getSegment(self, segment_id: str = None) -> dict:\n\t        \"\"\"\n\t        Return a specific segment definition.\n\t        Argument:\n\t            segment_id : REQUIRED : Segment ID of the segment to be retrieved.\n", "        \"\"\"\n\t        if segment_id is None:\n\t            raise Exception(\"Expecting a segment ID to fetch the segment definition.\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSegment\")\n\t        path = f\"/segment/definitions/{segment_id}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def createSegment(self, segment_data: dict = None) -> dict:\n\t        \"\"\"\n", "        Create a segment based on the information provided by the dictionary passed.\n\t        Argument :\n\t            segment_data : REQUIRED : Dictionary of the segment definition.\n\t                require in the segment_data: name, description, expression, schema, ttlInDays\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSegment\")\n\t        path = \"/segment/definitions\"\n\t        if segment_data is None or type(segment_data) != dict:\n\t            raise Exception(\n", "                \"Expecting data as dictionary format to update the segment.\"\n\t            )\n\t        checks = \"name,description,expression,schema,ttlInDays\".split(\n\t            \",\"\n\t        )  # mandatory elements in segment definition\n\t        if len(set(checks) & set(segment_data.keys())) != len(checks):\n\t            raise Exception(\n\t                \"Segment data doesn't hold one or several mandatory fields:\\n\\\n\t                name, description, expression, schema, ttlInDays\"\n\t            )\n", "        res = self.connector.postData(\n\t            self.endpoint + path, data=segment_data, headers=self.header\n\t        )\n\t        return res\n\t    def deleteSegment(self, segment_id: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific segment definition.\n\t        Argument:\n\t            segment_id : REQUIRED : Segment ID of the segment to be deleted.\n\t        \"\"\"\n", "        if segment_id is None:\n\t            raise Exception(\"Expecting a segment ID to delete the segment.\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteSegment\")\n\t        path = f\"/segment/definitions/{segment_id}\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def updateSegment(self, segment_id: str = None, segment_data: dict = None) -> dict:\n\t        \"\"\"\n\t        Update the segment characteristics from the definition pass to that method.\n", "        Arguments:\n\t            segment_id : REQUIRED : id of the segment to be udpated.\n\t            segment_data : REQUIRED : Dictionary of the segment definition.\n\t                require in the segment_data: name, description, expression, schema, ttlInDays\n\t        \"\"\"\n\t        if segment_id is None:\n\t            raise Exception(\"Expecting a segment ID to update the segment.\")\n\t        elif segment_data is None or type(segment_data) != dict:\n\t            raise Exception(\n\t                \"Expecting data as dictionary format to update the segment.\"\n", "            )\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateSegment\")\n\t        path = f\"/segment/definitions/{segment_id}\"\n\t        checks = \"name,expression,schema,ttlInDays\".split(\n\t            \",\"\n\t        )  # mandatory elements in segment definition\n\t        if len(set(checks) & set(segment_data.keys())) != len(checks):\n\t            raise Exception(\n\t                \"Segment data doesn't hold one or several mandatory fields:\\n\\\n", "                name, description, expression, schema, ttlInDays\"\n\t            )\n\t        update = self.connector.postData(\n\t            self.endpoint + path, headers=self.header, data=segment_data\n\t        )\n\t        return update\n\t    def getMultipleSegments(self,segmentIds:list=None)->dict:\n\t        \"\"\"\n\t        Retrieve multiple segments from a list of segment IDs.\n\t        Arguments:\n", "            segmentIds: REQUIRED : list of segment IDs\n\t        \"\"\"\n\t        if segmentIds is None:\n\t            raise Exception(\"Require a list of segment Ids\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMultipleSegments with this list: {segmentIds}\")\n\t        path = \"/segment/definitions/bulk-get\"\n\t        res = self.connector.postData(self.endpoint + path,data=segmentIds)\n\t        results = res.get('results')\n\t        return results\n", "    def convertSegmentDef(self,name:str=None,expression:dict=None,description:str=None,schemaClass:str=\"_xdm.context.profile\",ttl:int=30,**kwargs)->dict:\n\t        \"\"\"\n\t        This endpoint converts a segment definition from pql/text to pql/json or from pql/json to pql/text.\n\t        Arguments:\n\t            name : REQUIRED : The name of the segment. It should be unique.\n\t            expression : REQUIRED : the expriession regarding the transformation.\n\t                A dictionary such as\n\t                {\n\t                    \"type\" : \"PQL\" (or \"ARL\"),\n\t                    \"format\" : \"pql/text\" (or \"pql/json\")\n", "                    \"value\" : \"your PQL expression\"\n\t                }\n\t            description : OPTIONAL : the description to be used\n\t            schemaClass : OPTIONAL :  the class ID to be used. (ex: default : \"_xdm.context.profile\")\n\t            ttl : OPTIONAL : Time to live per day (default 30)\n\t        possible kwargs:\n\t            additional kwargs will be used as parameter of the body\n\t        \"\"\"\n\t        if name is None:\n\t            raise Exception(\"Require a name\")\n", "        if expression is None and type(expression)!=dict:\n\t            raise Exception(\"Require a dictionary as expression\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting convertSegmentDef\")\n\t        path = \"/segment/conversion\"\n\t        data = {**kwargs}\n\t        data['name'] = name\n\t        data['imsOrgId'] = self.connector.config['org_id']\n\t        data['description'] = description\n\t        data['expression'] = expression\n", "        data['schema'] = {\n\t            \"name\" : schemaClass\n\t        }\n\t        data['ttlInDays'] = ttl\n\t        res = self.connector.postData(self.endpoint + path,data=data)\n\t        return res\n\t    def getExportJobs(self, limit: int = 1000, status: str = None) -> list:\n\t        \"\"\"\n\t        Retrieve a list of all export jobs.\n\t        Arguments:\n", "            limit : OPTIONAL : number of jobs to be returned (default 100)\n\t            status : OPTIONAL : status of the job (NEW, SUCCEEDED, FAILED)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getExportJobs\")\n\t        path = \"/export/jobs\"\n\t        params = {\"limit\": limit}\n\t        if status is not None and status in [\"NEW\", \"SUCEEDED\", \"FAILED\"]:\n\t            params[\"status\"] = status\n\t        lastPage = False\n", "        data = []\n\t        while lastPage != True:\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n\t            )\n\t            data += res[\"records\"]\n\t            nextPage = res.get(\"link\", {}).get(\"next\", \"\")\n\t            if nextPage == \"\":\n\t                lastPage = True\n\t            else:\n", "                offset = nextPage.split(\"offset=\")[1].split(\"&\")[0]\n\t                params[\"offset\"] = offset\n\t        return data\n\t    def createExport(self, export_request: dict = None) -> dict:\n\t        \"\"\"\n\t        Create an exportJob\n\t        Arguments:\n\t            export_request : REQUIRED : number of jobs to be returned (default 100)\n\t            information on the structure of the request here: https://experienceleague.adobe.com/docs/experience-platform/segmentation/api/export-jobs.html?lang=en#get\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createExport\")\n\t        path = \"/export/jobs\"\n\t        if export_request is None:\n\t            raise Exception(\"Expected export data to specify segment to export.\")\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=export_request, headers=self.header\n\t        )\n\t        return res\n\t    def getExport(self, export_id: str = None) -> dict:\n", "        \"\"\"\n\t        Retrieve a specific export Job.\n\t        Arguments:\n\t            export_id : REQUIRED : Export Job to be retrieved.\n\t        \"\"\"\n\t        if export_id is None:\n\t            raise Exception(\"Expected a export_id\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getExport\")\n\t        path = f\"/export/jobs/{export_id}\"\n", "        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def deleteExport(self, export_id: str = None) -> dict:\n\t        \"\"\"\n\t        Cancel or delete an export Job.\n\t        Arguments:\n\t            export_id : REQUIRED : Export Job to be deleted.\n\t        \"\"\"\n\t        if export_id is None:\n\t            raise Exception(\"Expected a export_id\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteExport\")\n\t        path = f\"/export/jobs/{export_id}\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def searchNamespaces(\n\t        self,\n\t        query: str = None,\n\t        schema: str = \"_xdm.context.segmentdefinition\",\n\t        **kwargs,\n", "    ) -> dict:\n\t        \"\"\"\n\t        Return a list of search count results, queried across all namespaces.\n\t        Arguments:\n\t            query : REQUIRED : the search query.\n\t            schema : OPTIONAL : The schema class value associated with the search objects. (default _xdm.context.segmentdefinition)\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting searchNamespaces\")\n\t        path = \"/search/namespaces\"\n", "        if query is None:\n\t            raise Exception(\"Expected a query to search for.\")\n\t        params = {\"schema.name\": schema, \"s\": query}\n\t        self.header[\"x-ups-search-version\"] = \"1.0\"\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=self.header, params=params\n\t        )\n\t        del self.header[\"x-ups-search-version\"]\n\t        return res\n\t    def searchEntities(\n", "        self,\n\t        query: str = None,\n\t        namespace: str = \"ECID\",\n\t        entityId: str = None,\n\t        schema: str = \"_xdm.context.segmentdefinition\",\n\t        **kwargs,\n\t    ) -> dict:\n\t        \"\"\"\n\t        Return the list of objects that are contained  within a namespace.\n\t        Arguments:\n", "            query : REQUIRED : the search query based on Lucene query syntax (ex: name:test) (https://learn.microsoft.com/en-us/azure/search/query-lucene-syntax)\n\t            schema : OPTIONAL : The schema class value associated with the search objects.(defaul _xdm.context.segmentdefinition)\n\t            namespace : OPTIONAL : The namespace you want to search within (default ECID)\n\t            entityId : OPTIONAL : The ID of the folder you want to search for external segments in\n\t        possible kwargs:\n\t            limit : maximum number of result per page. Max 50.\n\t            page : page to be retrieved (start at 0)\n\t            page_limit : maximum number of pages retrieved.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting searchEntity\")\n\t        path = \"/search/entities\"\n\t        if query is None:\n\t            raise Exception(\"Expected a query to search for.\")\n\t        limit = kwargs.get(\"limit\", 50)\n\t        page = kwargs.get(\"page\", 0)\n\t        page_limit = kwargs.get(\"page_limit\", 0)\n\t        self.header[\"x-ups-search-version\"] = \"1.0\"\n\t        params = {\n\t            \"schemaClass\": schema,\n", "            \"namespace\": namespace,\n\t            \"s\": query,\n\t            \"entityId\": entityId,\n\t            \"limit\": limit,\n\t            \"page\": page,\n\t        }\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=self.header, params=params\n\t        )\n\t        data = res[\"entities\"]\n", "        curr_page = res[\"page\"][\"pageOffset\"]\n\t        total_pages = res[\"page\"][\"totalPages\"]\n\t        while curr_page <= page_limit - 1 or curr_page == total_pages:\n\t            res = self.connector.getData(\n\t                self.endpoint + path, headers=self.header, params=params\n\t            )\n\t            data += res[\"entities\"]\n\t            curr_page = res[\"page\"][\"pageOffset\"]\n\t            total_pages = res[\"page\"][\"totalPages\"]\n\t        del self.header[\"x-ups-search-version\"]\n", "        return data\n\t    def getSchedules(\n\t        self, limit: int = 100, n_results: Union[int, str] = \"inf\"\n\t    ) -> list:\n\t        \"\"\"\n\t        Return the list of scheduled segments.\n\t        Arguments:\n\t            limit : OPTIONAL : number of result per request (100 max)\n\t            n_results : OPTIONAL : Total of number of result to retrieve.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchedules\")\n\t        path = \"/config/schedules\"\n\t        params = {\"start\": 0}\n\t        lastPage = False\n\t        data = []\n\t        while lastPage != True:\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n\t            )\n", "            data += res.get(\"children\", [])\n\t            nextPage = res.get(\"_links\", {}).get(\"href\", \"\")\n\t            if nextPage == \"\" or len(data) > float(n_results):\n\t                lastPage = True\n\t            else:\n\t                params[\"start\"] += 1\n\t        return data\n\t    def createSchedule(self, schedule_data: dict = None) -> dict:\n\t        \"\"\"\n\t        Schedule a segment to run.\n", "        Arguments:\n\t            schedule_data : REQUIRED : Definition of the schedule.\n\t            Should contains name, type, properties, schedule.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createSchedule\")\n\t        path = \"/config/schedules\"\n\t        if schedule_data is None or type(schedule_data) != dict:\n\t            raise Exception(\n\t                \"Expected a dictionary data for setting the segment schedule.\"\n", "            )\n\t        min_requirements = \"name,type,properties,schedule\".split(\",\")\n\t        if len(set(min_requirements) & set(schedule_data.keys())) != len(\n\t            min_requirements\n\t        ):\n\t            raise Exception(\n\t                \"Missing one minimal requirements : name, type, properties, schedule\"\n\t            )\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=schedule_data, headers=self.header\n", "        )\n\t        return res\n\t    def getSchedule(self, scheduleId: str = None) -> dict:\n\t        \"\"\"\n\t        Get a specific schedule definition.\n\t        Argument:\n\t            scheduleId : REQUIRED : Segment ID to be retrieved.\n\t        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"Expected a schedule_id\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getSchedule\")\n\t        path = f\"/config/schedules/{scheduleId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def deleteSchedule(self, scheduleId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete a specific schedule definition.\n\t        Argument:\n\t            scheduleId : REQUIRED : Segment ID to be deleted.\n", "        \"\"\"\n\t        if scheduleId is None:\n\t            raise Exception(\"Expected a schedule_id\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteSchedule\")\n\t        path = f\"/config/schedules/{scheduleId}\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def updateSchedule(self, scheduleId: str = None, operations: list = None) -> dict:\n\t        \"\"\"\n", "        Update a schedule with the operation provided.\n\t        Arguments:\n\t            scheduleId : REQUIRED : the schedule ID to update\n\t            operations : REQUIRED : List of operations to realize\n\t                [\n\t                    {\n\t                    \"op\": \"add\",\n\t                    \"path\": \"/state\",\n\t                    \"value\": \"active\"\n\t                    }\n", "                ]\n\t        \"\"\"\n\t        if scheduleId is None:\n\t            raise ValueError(\"Require a schedule ID\")\n\t        if operations is None or type(operations) != list:\n\t            raise ValueError(\"Require a list of operation to run\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting updateSchedule\")\n\t        path = f\"/config/schedules/{scheduleId}\"\n\t        res = self.connector.patchData(self.endpoint + path, data=operations)\n", "        return res\n\t    def getJobs(\n\t        self,\n\t        name: str = None,\n\t        status: str = None,\n\t        limit: int = 100,\n\t        n_results: Union[str, int] = \"inf\",\n\t        **kwargs,\n\t    ) -> list:\n\t        \"\"\"\n", "        Returns the list of segment jobs.\n\t        Arguments:\n\t            name : OPTIONAL : Name of the snapshot\n\t            status : OPTIONAL : Status of the job (PROCESSING,SUCCEEDED)\n\t            limit : OPTIONAL : Amount of jobs to be retrieved per request (100 max)\n\t            n_results : OPTIONAL : How many total jobs do you want to retrieve.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getJobs\")\n\t        path = \"/segment/jobs\"\n", "        params = {\"snapshot.name\": name, \"status\": status, \"limit\": limit, \"start\": 0}\n\t        lastPage = False\n\t        data = []\n\t        while lastPage != True:\n\t            res = self.connector.getData(\n\t                self.endpoint + path, params=params, headers=self.header\n\t            )\n\t            data += res.get(\"children\", [])\n\t            nextPage = res.get(\"_links\", {}).get(\"href\", \"\")\n\t            if nextPage == \"\" or len(data) > float(n_results):\n", "                lastPage = True\n\t            else:\n\t                params[\"start\"] += 1\n\t        return data\n\t    def createJob(self, segmentIds: list = None) -> dict:\n\t        \"\"\"\n\t        Create a new job for a segment.\n\t        Argument:\n\t            segmentIds : REQUIRED : a list of segmentIds.\n\t        \"\"\"\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createJob\")\n\t        path = \"/segment/jobs\"\n\t        if segmentIds is None or type(segmentIds) != list:\n\t            raise Exception(\"Expecting a list of segment ID to run.\")\n\t        jobData = [{\"segmentId\": segId} for segId in segmentIds]\n\t        res = self.connector.postData(\n\t            self.endpoint + path, data=jobData, headers=self.header\n\t        )\n\t        return res\n", "    def getJob(self, job_id: str = None) -> dict:\n\t        \"\"\"\n\t        Retrieve a Segment job by ID.\n\t        Argument:\n\t            job_id: REQUIRED : The job ID to retrieve.\n\t        \"\"\"\n\t        if job_id is None:\n\t            raise ValueError(\"Require a job ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getJob\")\n", "        path = f\"/segment/jobs/{job_id}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def deleteJob(self, job_id: str = None) -> dict:\n\t        \"\"\"\n\t        deleteJob a Segment job by ID.\n\t        Argument:\n\t            job_id: REQUIRED : The job ID to delete.\n\t        \"\"\"\n\t        if job_id is None:\n", "            raise ValueError(\"Require a job ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getJob\")\n\t        path = f\"/segment/jobs/{job_id}\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def createPreview(\n\t        self, pql: str = None, model: str = \"_xdm.context.profile\"\n\t    ) -> dict:\n\t        \"\"\"\n", "        Given a PQL expression genereate a preview of how much data there would be.\n\t        Arguments:\n\t            pql : REQUIRED : The PQL statement that would be your segment definition\n\t            model : OPTIONAL : XDM class the statement is based on. Default : _xdm.context.profile\n\t        \"\"\"\n\t        if pql is None:\n\t            ValueError(\"Require a PQL statement for creation\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createPreview\")\n\t        path = \"/preview\"\n", "        obj = {\n\t            \"predicateExpression\": pql,\n\t            \"predicateType\": \"pql/text\",\n\t            \"predicateModel\": model,\n\t            \"graphType\": \"pdg\",\n\t        }\n\t        res = self.connector.postData(self.endpoint + path, data=obj)\n\t        return res\n\t    def getPreview(self, previewId: str = None) -> dict:\n\t        \"\"\"\n", "        Retrieve the preview once it has been created by the createPreview method.\n\t        Arguments:\n\t            previewId : REQUIRED : The preview ID to used.\n\t        \"\"\"\n\t        if previewId is None:\n\t            raise Exception(\"require a preview ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getPreview\")\n\t        path = f\"/preview/{previewId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n", "        return res\n\t    def deletePreview(self, previewId: str = None) -> dict:\n\t        \"\"\"\n\t        Delete the preview based on its ID.\n\t        Arguments:\n\t            previewId : REQUIRED : The preview ID to deleted.\n\t        \"\"\"\n\t        if previewId is None:\n\t            raise Exception(\"require a preview ID\")\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting deletePreview\")\n\t        path = f\"/preview/{previewId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n\t        return res\n\t    def getEstimate(self, previewId: str = None) -> dict:\n\t        \"\"\"\n\t        Based on the preview ID generated by createPreview, you can look at statistical information of a segment.\n\t        Arguments:\n\t            previewId : REQUIRED : The preview ID to used for estimation\n\t        \"\"\"\n", "        if previewId is None:\n\t            raise Exception(\"require a preview ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getEstimate\")\n\t        path = f\"/estimate/{previewId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def estimateExpression(\n\t        self, pql: str = None, model: str = \"_xdm.context.profile\", wait: int = 60\n\t    ) -> dict:\n", "        \"\"\"\n\t        This method is a combination of the createPreview and getEstimate method so you don't have to build a pipeline for it.\n\t        It automatically fetch the estimate based on the PQL statement passed. Run a loop every minute to fetch the result.\n\t        Arguments:\n\t            pql : REQUIRED : The PQL statement that would be your segment definition\n\t            model : OPTIONAL : XDM class the statement is based on. Default : _xdm.context.profile\n\t            wait : OPTIONAL : How many seconds to wait between 2 call to getEstimate when result are not ready. (default 60)\n\t        \"\"\"\n\t        if pql is None:\n\t            raise ValueError(\"Require a PQL expression\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting estimateExpression\")\n\t        preview = self.createPreview(pql=pql, model=model)\n\t        try:\n\t            previewId = preview[\"previewId\"]\n\t        except:\n\t            print(preview)\n\t            raise KeyError(\"Couldn't retrieve the previewId from the response\")\n\t        estimate = self.getEstimate(previewId)\n\t        while estimate[\"state\"] != \"RESULT_READY\" or estimate[\"state\"] != \"ERROR\":\n", "            time.sleep(60)\n\t            estimate = self.getEstimate(previewId)\n\t        return estimate\n\t    def getAudiences(self,\n\t        limit:int=100,\n\t        metrics:bool=True,\n\t        name:str=None,\n\t        sort:str=None,\n\t        prop:str=None,\n\t        description:str=None,\n", "        **kwargs)->list:\n\t        \"\"\"\n\t        Get the audiences list.\n\t        Arguments:\n\t            name : OPTIONAL : Filter audiences that contains that string in the name, case unsensitive.\n\t            limit : OPTIONAL : The number of audiences to be returned by pages (default: 100)\n\t            sort : OPTIONAL : If you want to sort by a specific attribute (ex: \"updateTime:desc\")\n\t            prop : If you want to test a specific property of the result to filter the data.\n\t                    Ex: \"audienceId==mytestAudienceId\"\n\t            description : OPTIONAL : Filter audiences that contains that string in the description, case unsensitive.\n", "            with metrics : OPTIONAL : If metrics should be returned as well. Default true.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getAudiences\")\n\t        params = {\"limit\":limit,\"withMetrics\":metrics}\n\t        path = \"/audiences\"\n\t        if name is not None:\n\t            params['name'] = name\n\t        if sort is not None:\n\t            params['sort'] = sort\n", "        if prop is not None:\n\t            params['property'] = prop\n\t        if description is not None:\n\t            params['description'] = description\n\t        if kwargs.get('start',0) is not None:\n\t            params['start'] = kwargs.get('start')\n\t        res = self.connector.getData(self.endpoint+path, params=params)\n\t        data = res.get('children',[])\n\t        nextStart = res.get('_pages',{}).get('next',0)\n\t        while nextStart != 0:\n", "            params['start'] = nextStart\n\t            res = self.connector.getData(self.endpoint+path, params=params)\n\t            data += res.get('children',[])\n\t            nextStart = res.get('_pages',{}).get('next',0)\n\t        return data\n\t    def getAudience(self,audienceId:str=None)->dict:\n\t        \"\"\"\n\t        Retrieve a specific audience id.\n\t        Arguments:\n\t            audienceId : REQUIRED : The audience ID to retrieve.\n", "        \"\"\"\n\t        if audienceId is None:\n\t            raise ValueError(\"Require an audience ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getAudience for audienceId: {audienceId}\")\n\t        path = f\"/audiences/{audienceId}\"\n\t        res = self.connector.getData(self.endpoint + path)\n\t        return res\n\t    def deleteAudience(self,audienceId:str=None)->str:\n\t        \"\"\"\n", "        Delete an audience based on its ID.\n\t        Argument:\n\t            audienceId : REQUIRED : The audience ID to delete\n\t        \"\"\"\n\t        if audienceId is None:\n\t            raise ValueError(\"Require an audience ID\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteAudience for audienceId: {audienceId}\")\n\t        path = f\"/audiences/{audienceId}\"\n\t        res = self.connector.deleteData(self.endpoint + path)\n", "        return res\n\t    def createAudience(self,audienceObj:dict=None)->dict:\n\t        \"\"\"\n\t        Create an audience basde on the dictionary passed as argument.\n\t        Argument:\n\t            audienceObj : REQUIRED : Can be either one of the Platform Audience or External Audience\n\t                See constants EXTERNAL_AUDIENCE_DICT & PLATFORM_AUDIENCE_DICT  \n\t        \"\"\"\n\t        if audienceObj is None or type(audienceObj) != dict:\n\t            raise ValueError(\"Require an audience Object as dictionary\")\n", "        path = \"/audiences\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createAudience\")\n\t        res = self.connector.postData(self.endpoint + path, data=audienceObj)\n\t        return res\n\t    def patchAudience(self,audienceId:str=None,operations:list=None)->dict:\n\t        \"\"\"\n\t        PATCH an existing audience with some operation described in parameter.\n\t        Arguments:\n\t            audienceId : REQUIRED : The audience ID to patch\n", "            operations : REQUIRED : A list of operation to apply.\n\t                            Example: \n\t                            [\n\t                                {\n\t                                    \"op\": \"add\",\n\t                                    \"path\": \"/expression\",\n\t                                    \"value\": {\n\t                                    \"type\": \"PQL\",\n\t                                    \"format\": \"pql/text\",\n\t                                    \"value\": \"workAddress.country= \\\"US\\\"\"\n", "                                    }\n\t                                }\n\t                            ]\n\t        \"\"\"\n\t        if audienceId is None:\n\t            raise ValueError(\"Require an audience ID\")\n\t        if operations is None or type(operations) != list:\n\t            raise ValueError(\"Require a list of operations\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting patchAudience for audienceId: {audienceId}\")\n", "        path = f\"/audiences/{audienceId}\"\n\t        res = self.connector.patchData(self.endpoint + path,data=operations)\n\t        return res\n\t    def putAudience(self,audienceId:str=None,audienceObj: dict = None)-> dict:\n\t        \"\"\"\n\t        Replace an existing definition with a new one, with the PUT method.\n\t        Arguments:\n\t            audienceId : REQUIRED : the audience ID to replace\n\t            audienceObj : REQUIRED : the new definition to use\n\t                see EXTERNAL_AUDIENCE_DICT & PLATFORM_AUDIENCE_DICT  \n", "        \"\"\"\n\t        if audienceId is None:\n\t            raise ValueError(\"Require an audience ID\")\n\t        if audienceObj is None or type(audienceObj) != dict:\n\t            raise ValueError(\"Require a new definition\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting putAudience for audienceId: {audienceId}\")\n\t        path = f\"/audiences/{audienceId}\"\n\t        res = self.connector.putData(self.endpoint + path,data=audienceObj)\n\t        return res\n"]}
{"filename": "aepp/sensei.py", "chunked_list": ["#  Copyright 2023 Adobe. All rights reserved.\n\t#  This file is licensed to you under the Apache License, Version 2.0 (the \"License\");\n\t#  you may not use this file except in compliance with the License. You may obtain a copy\n\t#  of the License at http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#  Unless required by applicable law or agreed to in writing, software distributed under\n\t#  the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR REPRESENTATIONS\n\t#  OF ANY KIND, either express or implied. See the License for the specific language\n\t#  governing permissions and limitations under the License.\n\t# internal library\n", "import aepp\n\tfrom aepp import connector\n\timport logging\n\tfrom copy import deepcopy\n\tfrom typing import Union\n\tfrom .configs import ConnectObject\n\tclass Sensei:\n\t    \"\"\"\n\t    This module is based on the Sensei Machine Learning API from Adobe Experience Platform.\n\t    You can find more documentation on the endpoints here : https://www.adobe.io/apis/experienceplatform/home/api-reference.html#/\n", "    \"\"\"\n\t    # logging capability\n\t    loggingEnabled = False\n\t    logger = None\n\t    def __init__(\n\t        self,\n\t        config: Union[dict,ConnectObject] = aepp.config.config_object,\n\t        header: dict = aepp.config.header,\n\t        loggingObject: dict = None,\n\t        **kwargs,\n", "    ) -> None:\n\t        \"\"\"\n\t        Initialize the class with the config header used.\n\t        Arguments:\n\t            loggingObject : OPTIONAL : logging object to log messages.\n\t            config : OPTIONAL : config object in the config module.\n\t            header : OPTIONAL : header object  in the config module.\n\t        Additional kwargs will update the header.\n\t        \"\"\"\n\t        if loggingObject is not None and sorted(\n", "            [\"level\", \"stream\", \"format\", \"filename\", \"file\"]\n\t        ) == sorted(list(loggingObject.keys())):\n\t            self.loggingEnabled = True\n\t            self.logger = logging.getLogger(f\"{__name__}\")\n\t            self.logger.setLevel(loggingObject[\"level\"])\n\t            if type(loggingObject[\"format\"]) == str:\n\t                formatter = logging.Formatter(loggingObject[\"format\"])\n\t            elif type(loggingObject[\"format\"]) == logging.Formatter:\n\t                formatter = loggingObject[\"format\"]\n\t            if loggingObject[\"file\"]:\n", "                fileHandler = logging.FileHandler(loggingObject[\"filename\"])\n\t                fileHandler.setFormatter(formatter)\n\t                self.logger.addHandler(fileHandler)\n\t            if loggingObject[\"stream\"]:\n\t                streamHandler = logging.StreamHandler()\n\t                streamHandler.setFormatter(formatter)\n\t                self.logger.addHandler(streamHandler)\n\t        if type(config) == dict: ## Supporting either default setup or passing a ConnectObject\n\t            config = config\n\t        elif type(config) == ConnectObject:\n", "            header = config.getConfigHeader()\n\t            config = config.getConfigObject()\n\t        self.connector = connector.AdobeRequest(\n\t            config=config,\n\t            header=header,\n\t            loggingEnabled=self.loggingEnabled,\n\t            logger=self.logger,\n\t        )\n\t        self.header = self.connector.header\n\t        self.header[\n", "            \"Accept\"\n\t        ] = \"application/vnd.adobe.platform.sensei+json;profile=mlInstanceListing.v1.json\"\n\t        self.header.update(**kwargs)\n\t        if kwargs.get('sandbox',None) is not None: ## supporting sandbox setup on class instanciation\n\t            self.sandbox = kwargs.get('sandbox')\n\t            self.connector.config[\"sandbox\"] = kwargs.get('sandbox')\n\t            self.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t            self.connector.header.update({\"x-sandbox-name\":kwargs.get('sandbox')})\n\t        else:\n\t            self.sandbox = self.connector.config[\"sandbox\"]\n", "        self.endpoint = (\n\t            aepp.config.endpoints[\"global\"] + aepp.config.endpoints[\"sensei\"]\n\t        )\n\t    def getEngines(self, limit: int = 25, **kwargs) -> list:\n\t        \"\"\"\n\t        Return the list of all engines.\n\t        Arguments:\n\t            limit : OPTIONAL : number of element per requests\n\t        kwargs:\n\t            property : filtering, example value \"name==test.\"\n", "        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getEngines\")\n\t        path = \"/engines\"\n\t        params = {\"limit\": limit}\n\t        if kwargs.get(\"property\", False) != False:\n\t            params[\"property\"] = kwargs.get(\"property\", \"\")\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=self.header, params=params\n\t        )\n", "        data = res[\"children\"]\n\t        return data\n\t    def getEngine(self, engineId: str = None) -> dict:\n\t        \"\"\"\n\t        return a specific engine information based on its id.\n\t        Arguments:\n\t            engineId : REQUIRED : the engineId to return.\n\t        \"\"\"\n\t        if engineId is None:\n\t            raise Exception(\"require an engineId parameter\")\n", "        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getEngine\")\n\t        path = f\"/engines/{engineId}\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def getDockerRegistery(self) -> dict:\n\t        \"\"\"\n\t        Return the docker registery information.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n", "            self.logger.debug(f\"Starting getDockerRegistery\")\n\t        path = \"/engines/dockerRegistry\"\n\t        res = self.connector.getData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def deleteEngine(self, engineId: str = None) -> str:\n\t        \"\"\"\n\t        Delete an engine based on the id passed.\n\t        Arguments:\n\t            engineId : REQUIRED : Engine ID to be deleted.\n\t        \"\"\"\n", "        if engineId is None:\n\t            raise Exception(\"require an engineId parameter\")\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting deleteEngine\")\n\t        path = f\"/engines/{engineId}\"\n\t        res = self.connector.deleteData(self.endpoint + path, headers=self.header)\n\t        return res\n\t    def getMLinstances(self, limit: int = 25) -> list:\n\t        \"\"\"\n\t        Return a list of all of the ml instance\n", "        Arguments:\n\t            limit : OPTIONAL : number of elements retrieved.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting getMLinstances\")\n\t        path = \"/mlInstances\"\n\t        params = {\"limit\": limit}\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=self.header, params=params\n\t        )\n", "        data = res[\"children\"]\n\t        return data\n\t    def createMLinstances(\n\t        self, name: str = None, engineId: str = None, description: str = None\n\t    ):\n\t        \"\"\"\n\t        Create a ML instance with the name and instanceId provided.\n\t        Arguments:\n\t            name : REQUIRED : name of the ML instance\n\t            engineId : REQUIRED : engine attached to the ML instance\n", "            description : OPTIONAL : description of the instance.\n\t        \"\"\"\n\t        if self.loggingEnabled:\n\t            self.logger.debug(f\"Starting createMLinstances\")\n\t        path = \"/mlInstances\"\n\t        privateHeader = deepcopy(self.header)\n\t        privateHeader[\n\t            \"Content\"\n\t        ] = \"application/vnd.adobe.platform.sensei+json;profile=mlInstanceListing.v1.json\"\n\t        if name is None and engineId is None:\n", "            raise Exception(\"Requires a name and an egineId\")\n\t        body = {\"name\": name, \"engineId\": engineId, \"description\": description}\n\t        res = self.connector.getData(\n\t            self.endpoint + path, headers=privateHeader, data=body\n\t        )\n\t        return res\n"]}
