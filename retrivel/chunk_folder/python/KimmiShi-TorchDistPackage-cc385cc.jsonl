{"filename": "setup.py", "chunked_list": ["# from distutils.core import setup,find_packages\n\tfrom setuptools import find_packages, setup\n\tsetup(name='torchdistpackage',\n\t      version='0.1',\n\t      description='TorchDistPackage',\n\t      author='KimmiShi',\n\t      author_email='',\n\t      url='https://github.com/KimmiShi/TorchDistPackage',\n\t      packages=find_packages(include=['torchdistpackage', 'torchdistpackage.*'])\n\t     )"]}
{"filename": "torchdistpackage/__init__.py", "chunked_list": ["from .ddp.naive_ddp import NaiveDDP, moe_dp_iter_step, create_moe_dp_hooks\n\tfrom .ddp.zero_optim import Bf16ZeroOptimizer\n\t# from .ddp.torch_py_ddp import PythonDDP\n\tfrom .dist.launch_from_slurm import setup_distributed_slurm\n\tfrom .dist.process_topo import torch_parallel_context as tpc\n\tfrom .dist.process_topo import test_comm, is_using_pp\n\tfrom .utils import fix_rand\n"]}
{"filename": "torchdistpackage/utils.py", "chunked_list": ["import torch\n\timport os\n\tdef fix_rand(rank=0):\n\t    import numpy as np\n\t    import random\n\t    seed = 2222 + rank\n\t    print(f\"Setting random seed to {seed}\")\n\t    # PyTorch random number generator (for cpu and cuda)\n\t    torch.manual_seed(seed)\n\t    os.environ['PYTHONHASHSEED'] = str(seed)\n", "    # python random\n\t    random.seed(seed)\n\t    # numpy RNG\n\t    np.random.seed(seed)\n\t    # cuda benchmarking\n\t    torch.backends.cudnn.benchmark = False\n\t    # deterministic algos\n\t    # torch.use_deterministic_algorithms(True)\n\t    # cudnn conv deterministic\n\t    torch.backends.cudnn.deterministic = True\n", "    torch.cuda.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n\t    torch.backends.cudnn.enabled = False\n\t    torch.random.manual_seed(seed)\n"]}
{"filename": "torchdistpackage/ddp/naive_ddp.py", "chunked_list": ["import os\n\timport time\n\tfrom threading import Lock\n\timport torch\n\timport torch.distributed as dist\n\tfrom torch.distributed import ReduceOp\n\t__all__ = [\"NaiveDDP\"]\n\tclass NaiveDDP(torch.nn.Module):\n\t    r\"\"\" NaiveDDP wraps torch.nn.Module with distribued data parallel support\n\t    Args:\n", "        module (torch.nn.Module, required):\n\t            model used for computing.\n\t        sync (boolean, default: False):\n\t            True -> the gradient allreduce will happen after backward;\n\t            False -> the gradient allreduce will overlap with backward.\n\t        gradient_as_bucket_view: bucket grads\n\t        process_group: DP process group\n\t        dp_rank0: the first rank (rank0) of dp process group, when used with Model Parallel, rank0 is not always equal to '0'\n\t        reduce_op: 'avg' or 'sum\n\t        kwargs:\n", "            num_grad_acc_iter: only do reduce grad after backward for num_grad_acc_iter times\n\t    Note: for bucket, a warmup iter is needed to build up bucket, and correctly reduce grads\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        module,\n\t        sync=False,\n\t        bucket_cap_mb=25,\n\t        gradient_as_bucket_view=False,\n\t        process_group=None,\n", "        dp_rank0=0,\n\t        reduce_op=\"avg\",\n\t        **kwargs,\n\t    ):\n\t        super(NaiveDDP, self).__init__()\n\t        self.module = module\n\t        if hasattr(module, \"_ddp_params_and_buffers_to_ignore\"):\n\t            self.parameters_to_ignore = module._ddp_params_and_buffers_to_ignore\n\t        else:\n\t            self.parameters_to_ignore = []\n", "        self.group = process_group or None\n\t        self.dp_rank0 = dp_rank0\n\t        self.reduce_op = ReduceOp.SUM if reduce_op.lower == \"sum\" else ReduceOp.AVG\n\t        # Holds all_reduce handles, used when async_reduction is True\n\t        # self.async_handles = set()\n\t        self.broadcast_params()\n\t        self.sync = sync\n\t        self.gradient_as_bucket_view = gradient_as_bucket_view\n\t        self.bucket_cap_bytes = bucket_cap_mb * 1024 * 1024\n\t        self.buckets = {}\n", "        self.buckets_idx = 0\n\t        self.num_iter = 0\n\t        # self.lock = Lock()\n\t        self.reduce_time = 0.0\n\t        self.hook_time = 0.0\n\t        self.verbose = kwargs.get(\"verbose\", False)\n\t        self.num_grad_acc_iter = kwargs.get(\"num_grad_acc_iter\", 1)\n\t        self.grad_reduce_cnts = {}\n\t        self.reduce_stream = torch.cuda.Stream()\n\t        if not sync and dist.get_world_size(self.group) > 1:\n", "            self._grad_accs = []\n\t            self._register_hooks()\n\t    def forward(self, *inputs, **kwargs):\n\t        return self.module(*inputs, **kwargs)\n\t    def _register_hooks(self):\n\t        for i, (name, p) in enumerate(self.module.named_parameters()):\n\t            if p.requires_grad and name not in self.parameters_to_ignore:\n\t                if dist.get_world_size(self._get_group(name, p)) <= 1:\n\t                    continue\n\t                p_tmp = p.expand_as(p)\n", "                grad_acc = p_tmp.grad_fn.next_functions[0][0]\n\t                grad_acc.register_hook(self._make_hook(name, p, i))\n\t                self._grad_accs.append(grad_acc)  # ! very important\n\t    def _get_group(self, name, param):\n\t        return self.group\n\t    def sync_comm(self):\n\t        beg = time.perf_counter()\n\t        self.reduce_stream.synchronize()\n\t        self.reduce_time += time.perf_counter() - beg\n\t    def _reduce_grads(self, grad, group, name):\n", "        if self.sync:\n\t            dist.all_reduce(grad, group=group, op=self.reduce_op)\n\t        else:\n\t            if self.grad_reduce_cnts.get(name, 0) < self.num_grad_acc_iter - 1:\n\t                self.grad_reduce_cnts[name] = self.grad_reduce_cnts.get(name, 0) + 1\n\t                return\n\t            # beg = time.perf_counter()\n\t            stream = self.reduce_stream\n\t            stream.wait_stream(torch.cuda.current_stream())\n\t            # self.reduce_time += time.perf_counter()-beg\n", "            # handle = dist.all_reduce(grad, group=self.group, async_op=True, op=self.reduce_op)\n\t            # self.async_handles.add(handle)\n\t            with torch.cuda.stream(self.reduce_stream):\n\t                try:\n\t                    dist.all_reduce(\n\t                        grad, group=self.group, async_op=False, op=self.reduce_op\n\t                    )\n\t                except Exception as e:\n\t                    import pdb\n\t                    pdb.set_trace()\n", "                    print(\"Exception at _reduce_grads\")\n\t    def reduce_dispatch(self, name, p, idx=None):\n\t        should_bucket = (\n\t            lambda grad: grad.element_size() * grad.numel()\n\t            < self.bucket_cap_bytes * 4 // 5\n\t        )\n\t        if self.gradient_as_bucket_view and should_bucket(p.grad):\n\t            # if param has no \"bucket\", assign a 'bucket' to this param\n\t            if not hasattr(p, \"grad_bucket\"):\n\t                bucket_info = (p.grad.dtype, p.grad.device, self._get_group(name, p))\n", "                bucket = None\n\t                # if bucket_info exists, get a existing bucket\n\t                if bucket_info in self.buckets:\n\t                    bucket = self.buckets[bucket_info]\n\t                # if no existing bucket or bucket cannot hold current param, create a new bucket\n\t                if not bucket or not bucket.can_fit(p.grad):\n\t                    bucket = self.buckets[bucket_info] = GradBucket(\n\t                        f\"grad_bucket_{self.buckets_idx}\",\n\t                        self.bucket_cap_bytes,\n\t                        p.grad.element_size(),\n", "                        bucket_info,\n\t                    )\n\t                    self.buckets_idx += 1\n\t                p.grad = bucket.push(name, p.grad)\n\t                p.grad_bucket = bucket\n\t                # launch a reduce every time a new tensor comes\n\t                self._reduce_grads(\n\t                    p.grad.data, self._get_group(name, p), \"bucket_warmup\"\n\t                )\n\t                # we should remove the full buckets from self to make sure that bucket is not resued?\n", "                #       not needed, since the bucket will be full, and will be replaced by next new bucket\n\t            else:  # if param already has a 'bucket', mark current param ready, and if bucket is ready, reduce the bucket\n\t                bucket = p.grad_bucket\n\t                if bucket.grad_ready():\n\t                    self._reduce_grads(bucket.data, bucket.group, bucket.name)\n\t                    bucket.grad_reset()\n\t        else:\n\t            self._reduce_grads(p.grad.data, self._get_group(name, p), name)\n\t    def _make_hook(self, name, p, i):\n\t        def hook(*ignore):\n", "            # make grad thread safe\n\t            # with self.lock:\n\t            self.reduce_dispatch(name, p, i)\n\t        return hook\n\t    def _reset_iter(self):\n\t        if self.verbose and dist.get_rank(self.group) == 0:\n\t            print(\n\t                \"rank:\",\n\t                dist.get_rank(),\n\t                \" Total Reduce of last iter: \",\n", "                self.reduce_time,\n\t            )\n\t        self.num_iter += 1\n\t        self.reduce_time = 0.0\n\t        # clear grad reduce cnt every iter\n\t        for key in self.grad_reduce_cnts:\n\t            self.grad_reduce_cnts[key] = 0\n\t    def reduce_gradients(self):\n\t        \"\"\" call this after a iter, to reudce grads and sync \"\"\"\n\t        # no need sync when not distributed\n", "        if dist.get_world_size(self.group) <= 1:\n\t            return\n\t        beg = time.perf_counter()\n\t        if self.sync:\n\t            for i, (name, param) in enumerate(self.module.named_parameters()):\n\t                if (\n\t                    name not in self.parameters_to_ignore\n\t                    and param.requires_grad\n\t                    and param.grad is not None\n\t                ):\n", "                    if dist.get_world_size(self._get_group(name, param)) <= 1:\n\t                        continue\n\t                    self.reduce_dispatch(name, param, i)\n\t        # else:\n\t        #     for handle in self.async_handles:\n\t        #         handle.wait()\n\t        #     self.async_handles.clear()\n\t        torch.cuda.synchronize()\n\t        self.reduce_time += time.perf_counter() - beg\n\t        self._reset_iter()\n", "    def broadcast_params(self):\n\t        \"\"\" broadcast model parameters \"\"\"\n\t        for name, param in self.module.state_dict().items():\n\t            if name not in self.parameters_to_ignore:\n\t                dist.broadcast(param, self.dp_rank0, group=self._get_group(name, param))\n\tclass MoEDP(torch.nn.Module):\n\t    r\"\"\" NaiveDDP wraps torch.nn.Module with distribued data parallel support\n\t    Args:\n\t        module (torch.nn.Module, required):\n\t            model used for computing.\n", "        sync (boolean, default: False):\n\t            True -> the gradient allreduce will happen after backward;\n\t            False -> the gradient allreduce will overlap with backward.\n\t        gradient_as_bucket_view: bucket grads\n\t        process_group: DP process group\n\t        dp_rank0: the first rank (rank0) of dp process group, when used with Model Parallel, rank0 is not always equal to '0'\n\t        reduce_op: 'avg' or 'sum\n\t        kwargs:\n\t            num_grad_acc_iter: only do reduce grad after backward for num_grad_acc_iter times\n\t    Note: for bucket, a warmup iter is needed to build up bucket, and correctly reduce grads\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        expert_params,\n\t        sync=True,\n\t        bucket_cap_mb=25,\n\t        gradient_as_bucket_view=False,\n\t        process_group=None,\n\t        dp_rank0=0,\n\t        reduce_op=\"avg\",\n", "        **kwargs,\n\t    ):\n\t        super(MoEDP, self).__init__()\n\t        self.expert_params = expert_params\n\t        self.group = process_group\n\t        self.dp_rank0 = dp_rank0\n\t        self.reduce_op = ReduceOp.SUM if reduce_op.lower == \"sum\" else ReduceOp.AVG\n\t        self.broadcast_params()\n\t        self.sync = sync\n\t        self.gradient_as_bucket_view = gradient_as_bucket_view\n", "        self.bucket_cap_bytes = bucket_cap_mb * 1024 * 1024\n\t        self.buckets = {}\n\t        self.buckets_idx = 0\n\t        self.num_iter = 0\n\t        # self.lock = Lock()\n\t        # Holds all_reduce handles, used when async_reduction is True\n\t        self.async_handles = set()\n\t        self.use_sync_handle = True\n\t        self.reduce_time = 0.0\n\t        self.hook_time = 0.0\n", "        self.verbose = kwargs.get(\"verbose\", False)\n\t        self.num_grad_acc_iter = kwargs.get(\"num_grad_acc_iter\", 1)\n\t        self.grad_reduce_cnts = {}\n\t        self.reduce_stream = torch.cuda.Stream()\n\t        if dist.get_world_size(self.group) > 1:\n\t            self._grad_accs = []\n\t            self._register_hooks()\n\t    def forward(self, *inputs, **kwargs):\n\t        return self.module(*inputs, **kwargs)\n\t    def broadcast_params(self):\n", "        \"\"\" broadcast model parameters \"\"\"\n\t        for param in self.expert_params.values():\n\t            dist.broadcast(param, self.dp_rank0, group=self.group)\n\t    def _register_hooks(self):\n\t        for name, p in self.expert_params.items():\n\t            if p.requires_grad:\n\t                p_tmp = p.expand_as(p)\n\t                grad_acc = p_tmp.grad_fn.next_functions[0][0]\n\t                grad_acc.register_hook(self._make_hook(name, p))\n\t                self._grad_accs.append(grad_acc)  # ! very important\n", "    def _reduce_grads(self, grad, group, name):\n\t        if self.sync:\n\t            dist.all_reduce(grad, group=group, op=self.reduce_op)\n\t        elif self.use_sync_handle:\n\t            if self.grad_reduce_cnts.get(name, 0) < self.num_grad_acc_iter - 1:\n\t                self.grad_reduce_cnts[name] = self.grad_reduce_cnts.get(name, 0) + 1\n\t                return\n\t            # handle = dist.all_reduce(grad, group=group, async_op=True, op=self.reduce_op)\n\t            # self.async_handles.add(handle)\n\t        else:\n", "            stream = self.reduce_stream\n\t            stream.wait_stream(torch.cuda.current_stream())\n\t            with torch.cuda.stream(self.reduce_stream):\n\t                try:\n\t                    dist.all_reduce(\n\t                        grad, group=self.group, async_op=False, op=self.reduce_op\n\t                    )\n\t                except Exception as e:\n\t                    import pdb\n\t                    pdb.set_trace()\n", "                    print(\"Exception at _reduce_grads\")\n\t    def reduce_dispatch(self, name, p):\n\t        should_bucket = (\n\t            lambda grad: grad.element_size() * grad.numel()\n\t            < self.bucket_cap_bytes * 4 // 5\n\t        )\n\t        if self.gradient_as_bucket_view and should_bucket(p.grad):\n\t            # if param has no \"bucket\", assign a 'bucket' to this param\n\t            if not hasattr(p, \"grad_bucket\"):\n\t                bucket_info = (p.grad.dtype, p.grad.device, self.group)\n", "                bucket = None\n\t                # if bucket_info exists, get a existing bucket\n\t                if bucket_info in self.buckets:\n\t                    bucket = self.buckets[bucket_info]\n\t                # if no existing bucket or bucket cannot hold current param, create a new bucket\n\t                if not bucket or not bucket.can_fit(p.grad):\n\t                    bucket = self.buckets[bucket_info] = GradBucket(\n\t                        f\"grad_bucket_{self.buckets_idx}\",\n\t                        self.bucket_cap_bytes,\n\t                        p.grad.element_size(),\n", "                        bucket_info,\n\t                    )\n\t                    self.buckets_idx += 1\n\t                p.grad = bucket.push(name, p.grad)\n\t                p.grad_bucket = bucket\n\t                # launch a reduce every time a new tensor comes\n\t                self._reduce_grads(p.grad.data, self.group, \"bucket_warmup\")\n\t                # we should remove the full buckets from self to make sure that bucket is not resued?\n\t                #       not needed, since the bucket will be full, and will be replaced by next new bucket\n\t            else:  # if param already has a 'bucket', mark current param ready, and if bucket is ready, reduce the bucket\n", "                bucket = p.grad_bucket\n\t                if bucket.grad_ready():\n\t                    self._reduce_grads(bucket.data, bucket.group, bucket.name)\n\t                    bucket.grad_reset()\n\t        else:\n\t            self._reduce_grads(p.grad.data, self.group, name)\n\t    def _make_hook(self, name, p):\n\t        def hook(*ignore):\n\t            # make grad thread safe\n\t            # with self.lock:\n", "            self.reduce_dispatch(name, p)\n\t        return hook\n\t    def reduce_gradients(self):\n\t        \"\"\" call this after a iter, to reudce grads and sync \"\"\"\n\t        # no need sync when not distributed\n\t        if dist.get_world_size(self.group) <= 1:\n\t            return\n\t        if self.sync:\n\t            for name, param in self.expert_params.items():\n\t                if param.requires_grad and param.grad is not None:\n", "                    if dist.get_world_size(self.group) <= 1:\n\t                        continue\n\t                    self.reduce_dispatch(name, param, i)\n\t        else:\n\t            for handle in self.async_handles:\n\t                handle.wait()\n\t            self.async_handles.clear()\n\t        # clear grad reduce cnt every iter\n\t        for key in self.grad_reduce_cnts:\n\t            self.grad_reduce_cnts[key] = 0\n", "        torch.cuda.synchronize()\n\tmoe_dp_mod = None\n\tdef moe_dp_iter_step():\n\t    global moe_dp_mod\n\t    moe_dp_mod.reduce_gradients()\n\tdef create_moe_dp_hooks(\n\t    params: dict,\n\t    moe_dp_group,\n\t    moe_dp_rank0,\n\t    overlap_comm=True,\n", "    reduce_op=\"avg\",\n\t    sync=False,\n\t    num_grad_acc_iter=1,\n\t):\n\t    global moe_dp_mod\n\t    moe_dp_mod = MoEDP(\n\t        params,\n\t        sync=sync,\n\t        process_group=moe_dp_group,\n\t        dp_rank0=moe_dp_rank0,\n", "        reduce_op=reduce_op,\n\t        gradient_as_bucket_view=True,\n\t        num_grad_acc_iter=num_grad_acc_iter,\n\t    )\n\t    return moe_dp_mod\n\tclass GradBucket(object):\n\t    def __init__(self, name, size, element_size, bucket_info):\n\t        self.element_size = element_size\n\t        self.numel = size // self.element_size\n\t        self.name = name\n", "        self.dtype, self.device, self.group = bucket_info\n\t        self.data = torch.zeros(self.numel, dtype=self.dtype, device=self.device)\n\t        self.offset = 0\n\t        self.grads = []\n\t        self.ready = 0\n\t    def get_aligned_size(self, tensor):\n\t        aligned_size = (tensor.element_size() * tensor.numel() + 2 ** 9 - 1) & ~(\n\t            2 ** 9 - 1\n\t        )\n\t        assert aligned_size % tensor.element_size() == 0\n", "        return aligned_size // tensor.element_size()\n\t    def grad_ready(self):\n\t        self.ready += 1\n\t        return self.ready >= len(self.grads)\n\t    def grad_reset(self):\n\t        self.ready = 0\n\t    def can_fit(self, grad):\n\t        return self.offset + self.get_aligned_size(grad) <= self.numel\n\t    def push(self, name, grad):\n\t        new_grad = self.data.narrow(0, self.offset, grad.numel()).view_as(grad)\n", "        new_grad.copy_(grad)\n\t        self.offset += self.get_aligned_size(grad)\n\t        self.grads.append(name)\n\t        return new_grad\n"]}
{"filename": "torchdistpackage/ddp/zero_optim.py", "chunked_list": ["# A simple zero impl that:\n\t#  1. shards the opt states\n\t#  2. shards the grads\n\t#  Supports bf16 only\n\t# work flow of bf16 optim:\n\t#   model param in bf16\n\t#   -> grads in bf16\n\t#   reduce and remove grads not needed in current partition\n\t#   copy grads to fp32\n\t#   optim updates fp32 copy of param using fp32 grad\n", "#   update fp16 param using fp32 param\n\timport torch\n\timport torch.distributed as dist\n\timport math\n\tdef partition_params(params, num_partitions, numel_per_partition):\n\t    \"\"\"partitions params\n\t    Args:\n\t        params (list): the complete list of params to partition\n\t        num_partitions (int): zero dp world size\n\t        numel_per_partition (int): max number of param cnt\n", "    Returns:\n\t        list: list of partitions\n\t    \"\"\"\n\t    partitions = []\n\t    elcnt = 0\n\t    partition_id = 0\n\t    for ind in range(num_partitions):\n\t        partitions.append([])\n\t    for param in params:\n\t        partitions[partition_id].append(param)\n", "        elcnt+=param.numel()\n\t        if elcnt > numel_per_partition:\n\t            partition_id+=1\n\t            elcnt=0\n\t    return partitions\n\tFREE_BUFFERS = []\n\tclass Bucket():\n\t    def __init__(self, dtype, size, reduce_stream, dp_group, reduce_op) -> None:\n\t        self.capacity = size\n\t        self.reduce_stream = reduce_stream#torch.cuda.Stream()\n", "        self.dp_group = dp_group\n\t        self.reduce_op = reduce_op\n\t        self.params_in_bucket = []\n\t        self.numel_in_bucket = 0\n\t        global FREE_BUFFERS\n\t        if len(FREE_BUFFERS) > 0:\n\t            self.buffer = FREE_BUFFERS.pop(0)\n\t        else:\n\t            self.buffer = torch.empty(self.capacity, dtype=dtype).cuda()\n\t    def try_hold(self, param, param_hook):\n", "        self.param_hook = param_hook\n\t        if param.grad.numel() > self.capacity - self.numel_in_bucket:\n\t            self.reduce()\n\t            return False\n\t        else:\n\t            self.push(param)\n\t            return True\n\t    def push(self, param):\n\t        self.params_in_bucket.append(param)\n\t        self.numel_in_bucket+=param.numel()\n", "    def reduce(self):\n\t        param_list = self.params_in_bucket\n\t        self.reduce_stream.wait_stream(torch.cuda.current_stream())\n\t        with torch.cuda.stream(self.reduce_stream):\n\t            pos = 0\n\t            for param in param_list:\n\t                slice = self.buffer.narrow(0, pos, param.grad.numel())\n\t                slice.copy_(param.grad.view(-1))\n\t                # set param.grad back to avoid copy after allreduce\n\t                param.grad = slice.view_as(param.grad)\n", "                pos+=param.grad.numel()\n\t            dist.all_reduce(\n\t                self.buffer, group=self.dp_group, async_op=False, op=self.reduce_op\n\t            )\n\t            pos=0\n\t            # copy reduced grads back, and do master grad update\n\t            for param in param_list:\n\t                self.param_hook(param)\n\t            # clean up\n\t            global FREE_BUFFERS\n", "            FREE_BUFFERS.append(self.buffer.zero_())\n\t            self.buffer = None\n\tclass Bf16ZeroOptimizer():\n\t    \"\"\"Usage:\n\t        1. wrap original optimizer:\n\t            `optimizer = Bf16ZeroOptimizer(optimizer, bf16_master_weights=True, overlap_comm=True)`\n\t        2. use wrapped optim like orignal one\n\t       **Note**:\n\t            1. bf16_master_weights=True is not compatible with bucketize,\n\t                since bucketize requires an copy of master grad\n", "    \"\"\"\n\t    def __init__(self, optim, dp_group=None, bf16_master_weights=False, overlap_comm=False, stage=2,\n\t                 bucket_size=5e8, bucketize=True) -> None:\n\t        self.optim = optim\n\t        self.dp_group = dp_group\n\t        self.bf16_master_weights = bf16_master_weights\n\t        self.partition_grad = stage==2\n\t        self.overlap_comm=overlap_comm\n\t        self.reduce_bucket_size = int(bucket_size)\n\t        self.bucketize = bucketize\n", "        self.reduce_stream = torch.cuda.Stream() if overlap_comm else torch.cuda.current_stream()\n\t        self.reduce_op = dist.ReduceOp.AVG\n\t        self.grad_accs = []\n\t        if torch.distributed.is_initialized():\n\t            self.partition_id = dist.get_rank(self.dp_group)\n\t            num_partitions = dist.get_world_size(self.dp_group)\n\t        else:\n\t            self.partition_id = 0\n\t            num_partitions = 1\n\t        self.num_partitions = num_partitions\n", "        self.all_param_groups_partitions = []\n\t        self.bit16_params_shard_groups = []\n\t        self.master_weight_shard_groups = []\n\t        self.bf16_param_id_in_partition = set()\n\t        self.bf16_param_to_master_weight_map = dict()\n\t        self.param2rank = dict()\n\t        self.original_dtype = optim.param_groups[0]['params'][0].dtype\n\t        if self.bucketize:\n\t            self.working_bucket = self.create_bucket()\n\t        for param_group in self.optim.param_groups:\n", "            trainable_parameters = [param for param in param_group['params'] if param.requires_grad]\n\t            total_num_elements = sum([p.numel() for p in trainable_parameters])\n\t            target_partition_numel = math.ceil(total_num_elements//num_partitions)\n\t            all_partitions = partition_params(trainable_parameters, num_partitions, target_partition_numel)\n\t            self.all_param_groups_partitions.append(all_partitions)\n\t            params_in_cur_partition = all_partitions[self.partition_id]\n\t            self.bit16_params_shard_groups.append(params_in_cur_partition)\n\t            # build param id to rank map\n\t            for rank, partition in enumerate(all_partitions):\n\t                for param in partition:\n", "                    self.param2rank[id(param)] = rank\n\t            for param in params_in_cur_partition:\n\t                self.bf16_param_id_in_partition.add(id(param))\n\t            if bf16_master_weights:\n\t                self.master_weight_shard_groups.append(params_in_cur_partition)\n\t            else:\n\t                fp32_params_shard = [p.clone().detach().float() for p in params_in_cur_partition]\n\t                for ind in range(len(params_in_cur_partition)):\n\t                    self.bf16_param_to_master_weight_map[id(params_in_cur_partition[ind])] = fp32_params_shard[ind]\n\t                #in case the internal optimizer needs it\n", "                for p in fp32_params_shard:\n\t                    p.requires_grad = True\n\t                self.master_weight_shard_groups.append(fp32_params_shard)\n\t            # update optim's param group\n\t            param_group['params'] = self.master_weight_shard_groups[-1]\n\t            for ind,param in enumerate(trainable_parameters):\n\t                    def wrapper(param, ind):\n\t                        param_tmp = param.expand_as(param)\n\t                        grad_acc = param_tmp.grad_fn.next_functions[0][0]\n\t                        def reduce_partition_and_remove_grads(*notneeded):\n", "                            if self.bucketize:\n\t                                reduce_and_remove_grad_bucketized(param)\n\t                            else:\n\t                                reduce_and_remove_grad(param)\n\t                        grad_acc.register_hook(reduce_partition_and_remove_grads)\n\t                        self.grad_accs.append(grad_acc)\n\t                    wrapper(param, ind)\n\t        print(\"Bf16ZeroOptimizer initialized.\")\n\t        # create hook that does reduce & remove grad\n\t        def reduce_and_remove_grad(param):\n", "            if self.num_partitions > 1:\n\t                self.reduce_stream.wait_stream(torch.cuda.current_stream())\n\t                dst_rank = 0\n\t                with torch.cuda.stream(self.reduce_stream):\n\t                    # dist.all_reduce(\n\t                    #     param.grad.data, group=self.dp_group, async_op=False, op=self.reduce_op\n\t                    # )\n\t                    # single reduce, might be more efficientcy than all-reduce\n\t                    dst_rank = self.param2rank[id(param)]\n\t                    dist.reduce(param.grad.data, dst_rank, group=self.dp_group, async_op=False, op=self.reduce_op)\n", "                    self.copy2master_or_free(param)\n\t        def reduce_and_remove_grad_bucketized(param):\n\t            self.bucket_reduce_helper(param)\n\t    def create_bucket(self):\n\t        return Bucket(self.original_dtype, self.reduce_bucket_size, self.reduce_stream,\n\t                      self.dp_group, self.reduce_op)\n\t    def copy2master_or_free(self, param):\n\t        # copy to master if needed and free 16bit grad\n\t        if id(param) in self.bf16_param_id_in_partition:\n\t            if not self.bf16_master_weights:\n", "                master_weight = self.bf16_param_to_master_weight_map[id(param)]\n\t                if master_weight.grad is None:\n\t                    master_weight.grad = param.grad.clone().detach().to(master_weight.dtype)\n\t                else:\n\t                    master_weight.grad.data.copy_(param.grad.data)\n\t                if self.partition_grad:\n\t                    # free 16bit grad\n\t                    param.grad = None\n\t        elif self.partition_grad:\n\t            param.grad = None\n", "    def single_reduce_and_remove(self, param):\n\t        if self.overlap_comm:\n\t            self.reduce_stream.wait_stream(torch.cuda.current_stream())\n\t        with torch.cuda.stream(self.reduce_stream):\n\t            dist.all_reduce(\n\t                param.grad, group=self.dp_group, async_op=False, op=self.reduce_op\n\t            )\n\t            self.copy2master_or_free(param)\n\t    def bucket_reduce_helper(self, param):\n\t        # for extra large param, just launch reduce\n", "        if param.numel() > self.reduce_bucket_size:\n\t            self.single_reduce_and_remove(param)\n\t        else:\n\t            # let bucket to the reduce\n\t            if self.working_bucket.try_hold(param, self.copy2master_or_free):\n\t                pass\n\t            else:\n\t                self.working_bucket = self.create_bucket()\n\t                assert self.working_bucket.try_hold(param, self.copy2master_or_free)\n\t    def finish_bucket(self):\n", "        self.working_bucket.reduce()\n\t        # reset working bucket for next iter\n\t        self.working_bucket = self.create_bucket()\n\t    def step(self):\n\t        # 0. finish not reduced bucket\n\t        if self.bucketize:\n\t            self.finish_bucket()\n\t        self.reduce_stream.synchronize()\n\t        # 1. param update of single partition\n\t        self.optim.step()\n", "        # and relase master grad\n\t        if not self.bf16_master_weights:\n\t            for pg in self.master_weight_shard_groups:\n\t                for master_param in pg:\n\t                    master_param.grad=None\n\t        # 2. update bf16 param with fp32 param in current partition\n\t        if not self.bf16_master_weights:\n\t            for ind in range(len(self.bit16_params_shard_groups)):\n\t                for param_ind in range(len(self.bit16_params_shard_groups[ind])):\n\t                    self.bit16_params_shard_groups[ind][param_ind].data.copy_(self.master_weight_shard_groups[ind][param_ind])\n", "        # 3. all-gather bit16 params\n\t        #    do this by broadcast\n\t        if self.num_partitions ==1:\n\t            return\n\t        for param_partitions in self.all_param_groups_partitions:\n\t            for partition_id in range(self.num_partitions):\n\t                partition = param_partitions[partition_id]\n\t                # broadcast partition from rank partition_id to the rest\n\t                for param in partition:\n\t                    dist.broadcast(param.data, partition_id, self.dp_group)\n", "    def zero_grad(self):\n\t        # self.optim.zero_grad()\n\t        for pg_partitions in self.all_param_groups_partitions:\n\t            for parition in pg_partitions:\n\t                for p in parition:\n\t                    if p.grad is not None:\n\t                        p.grad.zero_()\n\t    # Promote state so it can be retrieved or set via \"fp16_optimizer_instance.state\"\n\t    def _get_state(self):\n\t        return self.optim.state\n", "    def _set_state(self, value):\n\t        self.optim.state = value\n\t    state = property(_get_state, _set_state)\n\t    # Promote param_groups so it can be retrieved or set via \"fp16_optimizer_instance.param_groups\"\n\t    # (for example, to adjust the learning rate)\n\t    def _get_param_groups(self):\n\t        return self.optim.param_groups\n\t    def _set_param_groups(self, value):\n\t        self.optim.param_groups = value\n\t    param_groups = property(_get_param_groups, _set_param_groups)\n"]}
{"filename": "torchdistpackage/ddp/__init__.py", "chunked_list": []}
{"filename": "torchdistpackage/parallel/__init__.py", "chunked_list": ["from .pipeline_parallel.pipeline_sched import forward_backward, forward_eval\n\tfrom .pipeline_parallel.pipeline_helper import partition_uniform, flatten_model\n\tfrom .tensor_parallel.transformer import ParallelBlock,Block\n\tfrom .tensor_parallel.attn import Attention, TpAttention\n\tfrom .tensor_parallel.mlp import Mlp, TpMlp\n\tfrom .tensor_parallel.tp_utils import *"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/attn.py", "chunked_list": ["import torch\n\tfrom torch import nn as nn\n\tfrom .tp_utils import get_tp_group, set_tp_group, TpLinear, RowParallelLinear, ColParallelLinear, \\\n\t    gather_from_sequence_parallel_region\n\tdef _split_heads(tensor, num_heads, attn_head_size):\n\t    \"\"\"\n\t    Splits hidden_size dim into attn_head_size and num_heads\n\t    \"\"\"\n\t    new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n\t    tensor = tensor.view(new_shape)\n", "    return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n\t        super().__init__()\n\t        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n\t        self.num_heads = num_heads\n\t        self.head_dim = dim // num_heads\n\t        self.scale = self.head_dim ** -0.5\n\t        self.qkv = TpLinear(dim, dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n", "        self.proj = TpLinear(dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t    def _naive_attn(self, x):\n\t        B, N, DIM = x.shape\n\t        qkv = self.qkv(x)\n\t        q, k, v = qkv.chunk(3, dim=-1)  # make torchscript happy (cannot use tensor as tuple)\n\t        q= _split_heads(q, self.num_heads, self.head_dim)\n\t        k= _split_heads(k, self.num_heads, self.head_dim)\n\t        v= _split_heads(v, self.num_heads, self.head_dim)\n\t        attn = ((q * self.scale) @ k.transpose(-2, -1))\n", "        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, DIM)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n\t    def forward(self, x):\n\t        x = self._naive_attn(x)\n\t        return x\n\tclass TpAttention(nn.Module):\n", "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.,\n\t                 tp_group = None, sequence_parallel=False):\n\t        super().__init__()\n\t        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n\t        set_tp_group(tp_group)\n\t        self.tp_size = torch.distributed.get_world_size(get_tp_group())\n\t        self.num_heads = num_heads\n\t        self.head_dim = dim // num_heads\n\t        self.scale = self.head_dim ** -0.5\n\t        self.head_num_per_partition = self.num_heads//self.tp_size\n", "        self.qkv = ColParallelLinear(dim, dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = RowParallelLinear(dim, dim, sequence_parallel=sequence_parallel)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t        self.sequence_parallel = sequence_parallel\n\t    def _naive_attn(self, x):\n\t        B, N, DIM = x.shape     # DIM=self.head_dim*self.num_heads\n\t        qkv_out = self.qkv(x)       # B,N,3*DIM\n\t        q, k, v = qkv_out.chunk(3, dim=-1) # B.N.DIM\n\t        q= _split_heads(q, self.head_num_per_partition, self.head_dim)\n", "        k= _split_heads(k, self.head_num_per_partition, self.head_dim)\n\t        v= _split_heads(v, self.head_num_per_partition, self.head_dim)\n\t        attn = ((q * self.scale) @ k.transpose(-2, -1))\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, DIM//self.tp_size)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n\t    def forward(self, x):\n", "        if self.sequence_parallel:\n\t            # assume input tensor is sequence parallel\n\t            x = gather_from_sequence_parallel_region(x)\n\t        x = self._naive_attn(x)\n\t        return x"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/mlp.py", "chunked_list": ["import torch\n\tfrom torch import nn as nn\n\t# from torchdistpackage.parallel import *\n\tfrom .tp_utils import get_tp_group, set_tp_group, TpLinear, RowParallelLinear, ColParallelLinear, \\\n\t    gather_from_sequence_parallel_region\n\tclass Mlp(nn.Module):\n\t    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\t        From timm, but modified to run with tensor parallel and sequence parallel.\n\t    \"\"\"\n\t    def __init__(\n", "            self,\n\t            in_features,\n\t            hidden_features=None,\n\t            out_features=None,\n\t            act_layer=nn.GELU,\n\t            tp_group = None,\n\t            bias=True,\n\t            drop=0.,\n\t    ):\n\t        super().__init__()\n", "        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        bias = bias\n\t        set_tp_group(tp_group)\n\t        self.fc1 = TpLinear(in_features, hidden_features, bias=bias)\n\t        self.act = act_layer()\n\t        self.fc2 = TpLinear(hidden_features, out_features, bias=bias)\n\t        self.drop2 = nn.Dropout(drop)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n", "        x = self.act(x)\n\t        x = self.fc2(x)\n\t        x = self.drop2(x)\n\t        return x\n\tclass TpMlp(nn.Module):\n\t    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\t        From timm, but modified to run with tensor parallel and sequence parallel.\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n", "            in_features,\n\t            hidden_features=None,\n\t            out_features=None,\n\t            act_layer=nn.GELU,\n\t            tp_group = None,\n\t            bias=True,\n\t            drop=0.,\n\t            sequence_parallel=False\n\t    ):\n\t        super().__init__()\n", "        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        bias = bias\n\t        self.sequence_parallel = sequence_parallel\n\t        set_tp_group(tp_group)\n\t        self.fc1 = ColParallelLinear(in_features, hidden_features, bias=bias)\n\t        self.act = act_layer()\n\t        self.fc2 = RowParallelLinear(hidden_features, out_features, bias=bias, sequence_parallel=sequence_parallel)\n\t        self.drop2 = nn.Dropout(drop)\n\t    def forward(self, x):\n", "        if self.sequence_parallel:\n\t            # assume input tensor is sequence parallel\n\t            x = gather_from_sequence_parallel_region(x)\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.fc2(x)\n\t        x = self.drop2(x)\n\t        return x"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/transformer.py", "chunked_list": ["from typing import Optional, Tuple, Union\n\timport torch\n\tfrom torch import nn as nn\n\tfrom .attn import TpAttention,Attention\n\tfrom .mlp import TpMlp,Mlp\n\tfrom .tp_utils import set_sequence_parallel_attr, gather_from_sequence_parallel_region, maybe_split_into_sequence_parallel\n\tclass Block(nn.Module):\n\t    def __init__(self, dim, mlp_ratio=4,num_heads=8, **not_used):\n\t        super().__init__()\n\t        self.ln_1 = nn.LayerNorm(dim)\n", "        self.attn = Attention(dim, num_heads=num_heads)\n\t        self.ln_2 = nn.LayerNorm(dim)\n\t        self.mlp = Mlp(dim, hidden_features=int(dim*mlp_ratio))\n\t    def forward(self, hidden_states):\n\t        residual = hidden_states\n\t        hidden_states = self.ln_1(hidden_states)\n\t        attn_output = self.attn(\n\t            hidden_states\n\t        )\n\t        # residual connection\n", "        hidden_states = attn_output + residual\n\t        residual = hidden_states\n\t        hidden_states = self.ln_2(hidden_states)\n\t        feed_forward_hidden_states = self.mlp(hidden_states)\n\t        # residual connection\n\t        hidden_states = residual + feed_forward_hidden_states\n\t        return hidden_states\n\tclass ParallelBlock(nn.Module):\n\t    def __init__(self, dim, mlp_ratio=4, num_heads=8, sequence_parallel=False):\n\t        super().__init__()\n", "        self.ln_1 = nn.LayerNorm(dim)\n\t        self.attn = TpAttention(dim, num_heads=num_heads, sequence_parallel=sequence_parallel)\n\t        self.ln_2 = nn.LayerNorm(dim)\n\t        self.mlp = TpMlp(dim, hidden_features=int(dim*mlp_ratio), sequence_parallel=sequence_parallel)\n\t        self.sequence_parallel = sequence_parallel\n\t    def forward(self, hidden_states):\n\t        if self.sequence_parallel:\n\t            hidden_states = maybe_split_into_sequence_parallel(hidden_states)\n\t        residual = hidden_states\n\t        hidden_states = self.ln_1(hidden_states)\n", "        # tp block: gather from seq-p and output seq-p\n\t        attn_output = self.attn(\n\t            hidden_states\n\t        )\n\t        # residual connection\n\t        hidden_states = attn_output + residual\n\t        residual = hidden_states\n\t        hidden_states = self.ln_2(hidden_states)\n\t        # tp block: gather from seq-p and output seq-p\n\t        feed_forward_hidden_states = self.mlp(hidden_states)\n", "        # residual connection\n\t        hidden_states = residual + feed_forward_hidden_states\n\t        if self.sequence_parallel:\n\t            set_sequence_parallel_attr(hidden_states)\n\t        return hidden_states\n\t    @torch.no_grad()\n\t    def init_from_full(self, blk):\n\t        self.mlp.fc2.init_weight_from_full(blk.mlp.fc2.weight)\n\t        self.mlp.fc1.init_weight_from_full(blk.mlp.fc1.weight)\n\t        self.attn.qkv.init_weight_from_full_attn(blk.attn.qkv.weight)\n", "        self.attn.proj.init_weight_from_full(blk.attn.proj.weight)\n\t        # lns\n\t        self.ln_1.weight.copy_(blk.ln_1.weight)\n\t        self.ln_1.bias.copy_(blk.ln_1.bias)\n\t        self.ln_2.weight.copy_(blk.ln_2.weight)\n\t        self.ln_2.bias.copy_(blk.ln_2.bias)\n\tclass Transformer(nn.Module):\n\t    def __init__(self, dim, mlp_ratio=4, num_heads=8, depth=12, tensor_parallel=True, sequence_parallel=True):\n\t        super().__init__()\n\t        blk_type = Block if not tensor_parallel else ParallelBlock\n", "        self.blocks = nn.ModuleList([blk_type(dim, mlp_ratio=mlp_ratio, num_heads=num_heads, sequence_parallel=sequence_parallel) for _ in range(depth)])\n\t        self.sequence_parallel = sequence_parallel\n\t    def forward(self, x):\n\t        for blk in self.blocks:\n\t            x = blk(x)\n\t        if self.sequence_parallel:\n\t            x = gather_from_sequence_parallel_region(x)\n\t        return x"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/__init__.py", "chunked_list": []}
{"filename": "torchdistpackage/parallel/tensor_parallel/tp_utils.py", "chunked_list": ["import torch\n\tfrom torch import nn as nn\n\tfrom torch.nn.parameter import Parameter\n\timport torch.distributed as dist\n\tTP_GROUP=None\n\tdef get_tp_group():\n\t    global TP_GROUP\n\t    return TP_GROUP\n\tdef set_tp_group(group):\n\t    if group is not None:\n", "        global TP_GROUP\n\t        TP_GROUP=group\n\tdef get_tensor_model_parallel_world_size():\n\t    return dist.get_world_size(get_tp_group())\n\tdef maybe_gather_from_sequence_parallel(inp):\n\t    if is_squence_parallel_tensor(inp):\n\t        return gather_from_sequence_parallel_region(inp)\n\t    return inp\n\tdef maybe_split_into_sequence_parallel(inp):\n\t    if not is_squence_parallel_tensor(inp):\n", "        return _split_along_first_dim(inp)\n\t    return inp\n\tdef set_sequence_parallel_attr(inp, value=True):\n\t    setattr(inp, \"sequence_parallel\", value)\n\t    return inp\n\tdef is_squence_parallel_tensor(inp):\n\t    return hasattr(inp, \"sequence_parallel\") and inp.sequence_parallel==True\n\t# the following Reduce and Gather functions are adopted from https://github.com/NVIDIA/Megatron-LM\n\tclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n\t    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n", "    @staticmethod\n\t    def forward(ctx, input_):\n\t        torch.distributed.all_reduce(input_, group=get_tp_group())\n\t        return input_\n\t    @staticmethod\n\t    def backward(ctx, grad_output):\n\t        return grad_output\n\tdef _reduce_scatter_along_first_dim(input_):\n\t    \"\"\"Reduce-scatter the input tensor across model parallel group.\"\"\"\n\t    world_size = get_tensor_model_parallel_world_size()\n", "    # Bypass the function if we are using only 1 GPU.\n\t    if world_size == 1:\n\t        return input_\n\t    dim_size = list(input_.size())\n\t    assert dim_size[0] % world_size == 0, \\\n\t        \"First dimension of the tensor should be divisible by tensor parallel size\"\n\t    dim_size[0] = dim_size[0] // world_size\n\t    output = torch.empty(dim_size, dtype=input_.dtype,\n\t                         device=torch.cuda.current_device())\n\t    torch.distributed._reduce_scatter_base(output, input_.contiguous(),\n", "                                           group=get_tp_group())\n\t    return output\n\tdef _gather_along_first_dim(input_):\n\t    \"\"\"Gather tensors and concatinate along the first dimension.\"\"\"\n\t    world_size = get_tensor_model_parallel_world_size()\n\t    # Bypass the function if we are using only 1 GPU.\n\t    if world_size == 1:\n\t        return input_\n\t    dim_size = list(input_.size())\n\t    dim_size[0] = dim_size[0] * world_size\n", "    output = torch.empty(dim_size, dtype=input_.dtype,\n\t                         device=torch.cuda.current_device())\n\t    torch.distributed._all_gather_base(output, input_.contiguous(),\n\t                                       group=get_tp_group())\n\t    return output\n\tdef _split_along_first_dim(input_):\n\t    \"\"\"Split the tensor along its first dimension and keep the\n\t    corresponding slice.\"\"\"\n\t    world_size = get_tensor_model_parallel_world_size()\n\t    # Bypass the function if we are using only 1 GPU.\n", "    if world_size == 1:\n\t        return input_\n\t    # Split along first dimension.\n\t    dim_size = input_.size()[0]\n\t    assert dim_size % world_size == 0, \\\n\t        \"First dimension of the tensor should be divisible by tensor parallel size\"\n\t    local_dim_size = dim_size // world_size\n\t    rank = torch.distributed.get_rank(get_tp_group())\n\t    dim_offset = rank * local_dim_size\n\t    output = input_[dim_offset:dim_offset+local_dim_size].contiguous()\n", "    set_sequence_parallel_attr(output, True)\n\t    return output\n\tclass _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):\n\t    \"\"\"Reduce scatter the input from the model parallel region.\"\"\"\n\t    @staticmethod\n\t    def symbolic(graph, input_):\n\t        return _reduce_scatter_along_first_dim(input_)\n\t    @staticmethod\n\t    def forward(ctx, input_):\n\t        return _reduce_scatter_along_first_dim(input_)\n", "    @staticmethod\n\t    def backward(ctx, grad_output):\n\t        return _gather_along_first_dim(grad_output)\n\tclass _GatherFromSequenceParallelRegion(torch.autograd.Function):\n\t    \"\"\"Gather the input from sequence parallel region and concatinate.\"\"\"\n\t    @staticmethod\n\t    def symbolic(graph, input_, tensor_parallel_output_grad=True):\n\t        return _gather_along_first_dim(input_)\n\t    @staticmethod\n\t    def forward(ctx, input_, tensor_parallel_output_grad=True):\n", "        ctx.tensor_parallel_output_grad = tensor_parallel_output_grad\n\t        return _gather_along_first_dim(input_)\n\t    @staticmethod\n\t    def backward(ctx, grad_output):\n\t        tensor_parallel_output_grad = ctx.tensor_parallel_output_grad\n\t        # If the computation graph after the gather operation is\n\t        # in the tensor parallel mode, output gradients need to reduce\n\t        # scattered and whereas if the computation is duplicated,\n\t        # output gradients need to be scattered.\n\t        if tensor_parallel_output_grad:\n", "            return _reduce_scatter_along_first_dim(grad_output), None\n\t        else:\n\t            return _split_along_first_dim(grad_output), None\n\tdef gather_from_sequence_parallel_region(input_, tensor_parallel_output_grad=True):\n\t    output = _GatherFromSequenceParallelRegion.apply(input_, tensor_parallel_output_grad)\n\t    set_sequence_parallel_attr(output, False)\n\t    return output\n\tdef reduce_scatter_to_sequence_parallel_region(input_):\n\t    out = _ReduceScatterToSequenceParallelRegion.apply(input_)\n\t    set_sequence_parallel_attr(out)\n", "    return out\n\tclass TpLinear(nn.Module):\n\t    def __init__(self, fin, fout, bias=True):\n\t        super(TpLinear, self).__init__()\n\t        self.weight = Parameter(torch.rand((fin, fout)))\n\t        self.bias =None\n\t        if bias:\n\t            self.bias = Parameter(torch.zeros(fout))\n\t    def forward(self, x):\n\t        out = torch.matmul(x, self.weight)\n", "        if self.bias is not None:\n\t            out +=self.bias\n\t        return out\n\tclass ColParallelLinear(nn.Module):\n\t    def __init__(self, fin, fout, bias=True):\n\t        super(ColParallelLinear, self).__init__()\n\t        tp_group=get_tp_group()\n\t        self.tp_world_size = torch.distributed.get_world_size(tp_group)\n\t        assert fout%self.tp_world_size==0\n\t        self.fout = int(fout/self.tp_world_size)\n", "        self.linear = TpLinear(fin, self.fout, bias)\n\t    def forward(self, x):\n\t        \"\"\"\n\t        1. automatically split fout\n\t        2. do linear compute\n\t        3. the output is split , no communication\n\t        \"\"\"\n\t        out = self.linear(x)\n\t        return out\n\t    def init_weight_from_full(self, fullwt):\n", "        cur_rank = torch.distributed.get_rank(get_tp_group())\n\t        start_ind = cur_rank*self.fout\n\t        end_ind = (cur_rank+1)*self.fout\n\t        slice = fullwt[:,start_ind:end_ind]\n\t        with torch.no_grad():\n\t            self.linear.weight.copy_(slice)\n\t    def init_weight_from_full_attn(self, fullwt):\n\t        cur_rank = torch.distributed.get_rank(get_tp_group())\n\t        ws = torch.distributed.get_world_size(get_tp_group())\n\t        dim=fullwt.shape[0]\n", "        dim3=fullwt.shape[1]\n\t        fullwts = fullwt.split(dim3//3, dim=-1) # (q,k,v)\n\t        splits = []\n\t        for wt in fullwts:\n\t            splits.append(wt.split(wt.shape[-1]//ws, dim=-1)[cur_rank])\n\t        cat_full = torch.cat(splits, dim=-1)\n\t        with torch.no_grad():\n\t            self.linear.weight.copy_(cat_full)\n\tclass RowParallelLinear(nn.Module):\n\t    def __init__(self, fin, fout, bias=True, sequence_parallel=False):\n", "        super(RowParallelLinear, self).__init__()\n\t        tp_group=get_tp_group()\n\t        self.tp_world_size = torch.distributed.get_world_size(tp_group)\n\t        assert fin%self.tp_world_size==0\n\t        self.fin = int(fin/self.tp_world_size)\n\t        self.linear = TpLinear(self.fin, fout, bias)\n\t        self.sequence_parallel = sequence_parallel\n\t    def forward(self, x):\n\t        \"\"\"\n\t        1. automatically split fout\n", "        2. do linear compute\n\t        3. the output is allreduced\n\t        \"\"\"\n\t        out = self.linear(x)\n\t        if not self.sequence_parallel:\n\t            out = _ReduceFromModelParallelRegion.apply(out)\n\t        else:\n\t            out = reduce_scatter_to_sequence_parallel_region(out)\n\t        return out\n\t    def init_weight_from_full(self, fullwt):\n", "        cur_rank = torch.distributed.get_rank(get_tp_group())\n\t        start_ind = cur_rank*self.fin\n\t        end_ind = (cur_rank+1)*self.fin\n\t        slice = fullwt[start_ind:end_ind]\n\t        with torch.no_grad():\n\t            self.linear.weight.copy_(slice)\n"]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/comm.py", "chunked_list": ["# Adapted from https://github.com/hpcaitech/ColossalAI\n\tfrom typing import List, Tuple, Union\n\timport torch\n\timport torch.distributed as dist\n\tfrom torchdistpackage import tpc\n\tfrom functools import reduce\n\timport operator\n\tTensorShape = Union[torch.Size, List[int], Tuple[int]]\n\tdef get_current_device() -> torch.device:\n\t    \"\"\"\n", "    Returns currently selected device (gpu/cpu).\n\t    If cuda available, return gpu, otherwise return cpu.\n\t    \"\"\"\n\t    if torch.cuda.is_available():\n\t        return torch.device(f\"cuda:{torch.cuda.current_device()}\")\n\t    else:\n\t        return torch.device(\"cpu\")\n\tdef send_meta_helper(obj, next_rank, tensor_kwargs):\n\t    send_shape = torch.tensor(obj.size(), **tensor_kwargs)\n\t    send_ndims = torch.tensor(len(obj.size()), **tensor_kwargs)\n", "    dist.send(send_ndims, next_rank)\n\t    dist.send(send_shape, next_rank)\n\tdef send_obj_meta(obj, need_meta=True, next_rank=None) -> bool:\n\t    \"\"\"Sends obj meta information before sending a specific obj.\n\t    Since the recipient must know the shape of the obj in p2p communications,\n\t    meta information of the obj should be sent before communications. This function\n\t    synchronizes with :func:`recv_obj_meta`.\n\t    Args:\n\t        obj (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): obj to be sent.\n\t        need_meta (bool, optional): If False, meta information won't be sent.\n", "        next_rank (int): The rank of the next member in pipeline parallel group.\n\t    Returns:\n\t        bool: False\n\t    \"\"\"\n\t    if need_meta:\n\t        if next_rank is None:\n\t            next_rank = tpc.get_next_global_rank(\"pipe\")\n\t        # import pdb;pdb.set_trace()\n\t        tensor_kwargs = {\"dtype\": torch.long, \"device\": get_current_device()}\n\t        if isinstance(obj, torch.Tensor):\n", "            send_obj_nums = torch.tensor(1, **tensor_kwargs)\n\t            dist.send(send_obj_nums, next_rank)\n\t            send_meta_helper(obj, next_rank, tensor_kwargs)\n\t        else:\n\t            send_obj_nums = torch.tensor(len(obj), **tensor_kwargs)\n\t            dist.send(send_obj_nums, next_rank)\n\t            for tensor_to_send in obj:\n\t                send_meta_helper(tensor_to_send, next_rank, tensor_kwargs)\n\t    return False\n\tdef recv_meta_helper(prev_rank, tensor_kwargs):\n", "    recv_ndims = torch.empty((), **tensor_kwargs)\n\t    dist.recv(recv_ndims, prev_rank)\n\t    recv_shape = torch.empty(recv_ndims, **tensor_kwargs)\n\t    dist.recv(recv_shape, prev_rank)\n\t    return recv_shape\n\tdef recv_obj_meta(obj_shape, prev_rank=None) -> torch.Size:\n\t    \"\"\"Receives obj meta information before receiving a specific obj.\n\t    Since the recipient must know the shape of the obj in p2p communications,\n\t    meta information of the obj should be received before communications. This function\n\t    synchronizes with :func:`send_obj_meta`.\n", "    Args:\n\t        obj_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the obj to be received.\n\t        prev_rank (int): The rank of the source of the obj.\n\t    Returns:\n\t        Union[:class:`torch.Size`, List[:class:`torch.Size`]]: The shape of the obj to be received.\n\t    \"\"\"\n\t    if obj_shape is None:\n\t        if prev_rank is None:\n\t            prev_rank = tpc.get_prev_global_rank(\"pipe\")\n\t        tensor_kwargs = {\"dtype\": torch.long, \"device\": get_current_device()}\n", "        recv_obj_nums = torch.empty((), **tensor_kwargs)\n\t        # import pdb;pdb.set_trace()\n\t        dist.recv(recv_obj_nums, prev_rank)\n\t        if recv_obj_nums.item() == 1:\n\t            recv_shape = recv_meta_helper(prev_rank, tensor_kwargs)\n\t            obj_shape = torch.Size(recv_shape)\n\t        else:\n\t            obj_shape = []\n\t            for i in range(recv_obj_nums.item()):\n\t                recv_shape = recv_meta_helper(prev_rank, tensor_kwargs)\n", "                obj_shape.append(torch.Size(recv_shape))\n\t    return obj_shape\n\tdef split_tensor_into_1d_equal_chunks(\n\t    tensor: torch.Tensor, new_buffer=False\n\t) -> torch.Tensor:\n\t    \"\"\"Break a tensor into equal 1D chunks.\n\t    Args:\n\t        tensor (:class:`torch.Tensor`): Tensor to be split before communication.\n\t        new_buffer (bool, optional): Whether to use a new buffer to store sliced tensor.\n\t    Returns:\n", "        :class:`torch.Tensor`: The split tensor\n\t    \"\"\"\n\t    partition_size = torch.numel(tensor) // tpc.get_group_size(\"tensor\")\n\t    start_index = partition_size * tpc.get_group_rank(\"tensor\")\n\t    end_index = start_index + partition_size\n\t    if new_buffer:\n\t        data = torch.empty(\n\t            partition_size,\n\t            dtype=tensor.dtype,\n\t            device=torch.cuda.current_device(),\n", "            requires_grad=False,\n\t        )\n\t        data.copy_(tensor.view(-1)[start_index:end_index])\n\t    else:\n\t        data = tensor.view(-1)[start_index:end_index]\n\t    return data\n\tdef gather_split_1d_tensor(tensor: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"Opposite of above function, gather values from model parallel ranks.\n\t    Args:\n\t        tensor (:class:`torch.Tensor`): Tensor to be gathered after communication.\n", "    Returns:\n\t        :class:`torch.Tensor`: The gathered tensor.\n\t    \"\"\"\n\t    world_size = tpc.get_group_size(\"tensor\")\n\t    numel = torch.numel(tensor)\n\t    numel_gathered = world_size * numel\n\t    gathered = torch.empty(\n\t        numel_gathered,\n\t        dtype=tensor.dtype,\n\t        device=torch.cuda.current_device(),\n", "        requires_grad=False,\n\t    )\n\t    chunks = [gathered[i * numel : (i + 1) * numel] for i in range(world_size)]\n\t    dist.all_gather(chunks, tensor, group=tpc.get_group(\"tensor\"))\n\t    return gathered\n\tdef _get_tensor_shape(\n\t    tensor_shape: TensorShape, chunk_tensor: bool = False\n\t) -> Tuple[TensorShape, bool]:\n\t    \"\"\"get the exact tensor shape when communicating and return whether the tensor is a chunk\n\t    Args:\n", "        tensor_shape (:class:`torch.Size`): shape of tensor\n\t        chunk_tensor (bool, optional): whether to chunk tensor, defaults to False\n\t    Returns:\n\t        Tuple[Union[:class:`torch.Size`, List[int], Tuple[int]], bool]: exact tensor shape, whether to chunk tensor\n\t    \"\"\"\n\t    if chunk_tensor:\n\t        tensor_chunk_shape = reduce(operator.mul, tensor_shape, 1)\n\t        tensor_parallel_world_size = tpc.get_group_size(\"tensor\")\n\t        if tensor_chunk_shape % tensor_parallel_world_size == 0:\n\t            tensor_chunk_shape = tensor_chunk_shape // tensor_parallel_world_size\n", "        else:\n\t            tensor_chunk_shape = tensor_shape\n\t            chunk_tensor = False\n\t    else:\n\t        tensor_chunk_shape = tensor_shape\n\t    return tensor_chunk_shape, chunk_tensor\n\tdef create_recv_buffer_with_shapes(recv_shapes, dtype, scatter_gather_tensors):\n\t    if isinstance(recv_shapes, torch.Size):\n\t        recv_chunk_shape, recv_split = _get_tensor_shape(\n\t            recv_shapes, scatter_gather_tensors\n", "        )\n\t        buffer_recv = torch.empty(\n\t            recv_chunk_shape,\n\t            requires_grad=True,\n\t            device=get_current_device(),\n\t            dtype=dtype,\n\t        )\n\t        return buffer_recv, recv_split\n\t    buffer_recv = []\n\t    for recv_shape in recv_shapes:\n", "        recv_chunk_shape, recv_split = _get_tensor_shape(\n\t            recv_shape, scatter_gather_tensors\n\t        )\n\t        tensor_recv = torch.empty(\n\t            recv_chunk_shape,\n\t            requires_grad=True,\n\t            device=get_current_device(),\n\t            dtype=dtype,\n\t        )\n\t        buffer_recv.append(tensor_recv)\n", "    return buffer_recv, recv_split\n\tdef process_object_to_send(object_send, scatter_gather_tensors):\n\t    if isinstance(object_send, torch.Tensor):\n\t        send_split = _get_tensor_shape(object_send.shape, scatter_gather_tensors)[1]\n\t        if send_split:\n\t            object_send = split_tensor_into_1d_equal_chunks(object_send)\n\t        return object_send\n\t    object_send_list = []\n\t    for tensor_send in object_send:\n\t        send_split = _get_tensor_shape(tensor_send.shape, scatter_gather_tensors)[1]\n", "        if send_split:\n\t            object_send_list.append(split_tensor_into_1d_equal_chunks(tensor_send))\n\t        else:\n\t            object_send_list.append(tensor_send)\n\t    object_send = tuple(object_send_list)\n\t    return object_send\n\tdef filling_ops_queue(obj, comm_op, comm_rank, ops_queue):\n\t    if isinstance(obj, torch.Tensor):\n\t        op_to_add = dist.P2POp(comm_op, obj, comm_rank)\n\t        ops_queue.append(op_to_add)\n", "    else:\n\t        for tensor_to_comm in obj:\n\t            op_to_add = dist.P2POp(comm_op, tensor_to_comm, comm_rank)\n\t            ops_queue.append(op_to_add)\n\tdef _communicate(\n\t    object_send_next: Union[torch.Tensor, List[torch.Tensor]] = None,\n\t    object_send_prev: Union[torch.Tensor, List[torch.Tensor]] = None,\n\t    recv_prev: bool = False,\n\t    recv_next: bool = False,\n\t    recv_prev_shape: Union[torch.Size, List[torch.Size]] = None,\n", "    recv_next_shape: Union[torch.Size, List[torch.Size]] = None,\n\t    prev_rank: int = None,\n\t    next_rank: int = None,\n\t    dtype: torch.dtype = None,\n\t    scatter_gather_tensors: bool = False,\n\t) -> Tuple[Union[torch.Tensor, List[torch.Tensor]]]:\n\t    \"\"\"\n\t    Adapted from megatron.p2p_communication.\n\t    Communicate tensors between stages. Used as helper method in other\n\t    communication methods that are used in pipeline schedule.\n", "    Takes the following arguments:\n\t        object_send_next (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): tensor to send to next rank (no tensor sent if\n\t                          set to None).\n\t        object_send_prev (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): tensor to send to prev rank (no tensor sent if\n\t                          set to None).\n\t        recv_prev (bool): boolean for whether tensor should be received from\n\t                   previous rank.\n\t        recv_next (bool): boolean for whether tensor should be received from\n\t                   next rank.\n\t        recv_prev_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the previous stage, defaults to None.\n", "        recv_next_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the next stage, defaults to None.\n\t        prev_rank (int): the rank of the previous pipeline stage, defaults to None,\n\t        next_rank (int): the rank of the next pipeline stage, defaults to None,\n\t        dtype (torch.dtype): data type of intermediate buffers, defaults to None\n\t        scatter_gather_tensors (bool): whether to scatter and gather tensor between pipeline stages, defaults to False\n\t    Returns:\n\t        Tuple[Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]]: returns tensor_recv_prev, tensor_recv_next\n\t    \"\"\"\n\t    # Create placeholder tensors for receive in forward and backward directions\n\t    # if needed.\n", "    tensor_recv_prev = None\n\t    tensor_recv_next = None\n\t    if recv_prev:\n\t        assert recv_prev_shape is not None\n\t        tensor_recv_prev, recv_prev_split = create_recv_buffer_with_shapes(\n\t            recv_prev_shape, dtype, scatter_gather_tensors\n\t        )\n\t    if recv_next:\n\t        assert recv_next_shape is not None\n\t        tensor_recv_next, recv_next_split = create_recv_buffer_with_shapes(\n", "            recv_next_shape, dtype, scatter_gather_tensors\n\t        )\n\t    if object_send_prev is not None or recv_prev:\n\t        if prev_rank is None:\n\t            prev_rank = tpc.get_prev_global_rank(\"pipe\")\n\t    if object_send_next is not None or recv_next:\n\t        if next_rank is None:\n\t            next_rank = tpc.get_next_global_rank(\"pipe\")\n\t    if object_send_prev is not None:\n\t        object_send_prev = process_object_to_send(\n", "            object_send_prev, scatter_gather_tensors\n\t        )\n\t    if object_send_next is not None:\n\t        object_send_next = process_object_to_send(\n\t            object_send_next, scatter_gather_tensors\n\t        )\n\t    ops = []\n\t    if object_send_prev is not None:\n\t        filling_ops_queue(object_send_prev, dist.isend, prev_rank, ops)\n\t    if tensor_recv_prev is not None:\n", "        filling_ops_queue(tensor_recv_prev, dist.irecv, prev_rank, ops)\n\t    if tensor_recv_next is not None:\n\t        filling_ops_queue(tensor_recv_next, dist.irecv, next_rank, ops)\n\t    if object_send_next is not None:\n\t        filling_ops_queue(object_send_next, dist.isend, next_rank, ops)\n\t    if len(ops) > 0:\n\t        reqs = dist.batch_isend_irecv(ops)\n\t        for req in reqs:\n\t            req.wait()\n\t    # To protect against race condition when using batch_isend_irecv().\n", "    torch.cuda.synchronize()\n\t    if recv_prev and recv_prev_split:\n\t        if isinstance(tensor_recv_prev, torch.Tensor):\n\t            tensor_recv_prev = (\n\t                gather_split_1d_tensor(tensor_recv_prev)\n\t                .view(recv_prev_shape)\n\t                .requires_grad_()\n\t            )\n\t        else:\n\t            for index in range(len(tensor_recv_prev)):\n", "                tensor_recv_prev[index] = (\n\t                    gather_split_1d_tensor(tensor_recv_prev[index])\n\t                    .view(recv_prev_shape[index])\n\t                    .requires_grad_()\n\t                )\n\t    if recv_next and recv_next_split:\n\t        if isinstance(tensor_recv_next, torch.Tensor):\n\t            tensor_recv_next = (\n\t                gather_split_1d_tensor(tensor_recv_next)\n\t                .view(recv_next_shape)\n", "                .requires_grad_()\n\t            )\n\t        else:\n\t            for index in range(len(tensor_recv_next)):\n\t                tensor_recv_next[index] = (\n\t                    gather_split_1d_tensor(tensor_recv_next[index])\n\t                    .view(recv_next_shape[index])\n\t                    .requires_grad_()\n\t                )\n\t    return tensor_recv_prev, tensor_recv_next\n", "def recv_forward(\n\t    input_tensor_shape, prev_rank=None, dtype=torch.float, scatter_gather_tensors=False\n\t) -> Union[torch.Tensor, List[torch.Tensor]]:\n\t    \"\"\"Copy the forward output from the previous stage in pipeline as the input tensor of this stage.\n\t    Args:\n\t        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n\t        prev_rank (int, optional): The rank of the source of the tensor.\n\t    Returns:\n\t        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input tensor or input tensor list.\n\t    \"\"\"\n", "    if tpc.is_first_in_pipeline_group():\n\t        input_tensor = None\n\t    else:\n\t        input_tensor, _ = _communicate(\n\t            recv_prev=True,\n\t            recv_prev_shape=input_tensor_shape,\n\t            prev_rank=prev_rank,\n\t            dtype=dtype,\n\t            scatter_gather_tensors=scatter_gather_tensors,\n\t        )\n", "    return input_tensor\n\tdef recv_backward(\n\t    output_grad_shape, next_rank=None, dtype=torch.float, scatter_gather_tensors=False\n\t) -> Union[torch.Tensor, List[torch.Tensor]]:\n\t    \"\"\"Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage.\n\t    Args:\n\t        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n\t        next_rank (int, optional): The rank of the source of the tensor.\n\t    Returns:\n\t        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor or gradident tensor list.\n", "    \"\"\"\n\t    if tpc.is_last_in_pipeline_group():\n\t        output_tensor_grad = None\n\t    else:\n\t        _, output_tensor_grad = _communicate(\n\t            recv_next=True,\n\t            recv_next_shape=output_grad_shape,\n\t            next_rank=next_rank,\n\t            dtype=dtype,\n\t            scatter_gather_tensors=scatter_gather_tensors,\n", "        )\n\t    return output_tensor_grad\n\tdef send_forward(output_tensor, next_rank=None, scatter_gather_tensors=False) -> None:\n\t    \"\"\"Sends the input tensor to the next stage in pipeline.\n\t    Args:\n\t        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n\t        next_rank (int, optional): The rank of the recipient of the tensor.\n\t    \"\"\"\n\t    if not tpc.is_last_in_pipeline_group():\n\t        _communicate(\n", "            object_send_next=output_tensor,\n\t            next_rank=next_rank,\n\t            scatter_gather_tensors=scatter_gather_tensors,\n\t        )\n\tdef send_backward(\n\t    input_tensor_grad, prev_rank=None, scatter_gather_tensors=False\n\t) -> None:\n\t    \"\"\"Sends the gradient tensor to the previous stage in pipeline.\n\t    Args:\n\t        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent\n", "        prev_rank (int, optional): The rank of the recipient of the tensor\n\t    \"\"\"\n\t    if not tpc.is_first_in_pipeline_group():\n\t        _communicate(\n\t            object_send_prev=input_tensor_grad,\n\t            prev_rank=prev_rank,\n\t            scatter_gather_tensors=scatter_gather_tensors,\n\t        )\n\tdef send_forward_recv_backward(\n\t    output_tensor,\n", "    output_grad_shape,\n\t    recv_next=True,\n\t    next_rank=None,\n\t    dtype=torch.float,\n\t    scatter_gather_tensors=False,\n\t) -> Union[torch.Tensor, List[torch.Tensor]]:\n\t    \"\"\"Batched communication operation. Sends the input tensor to the\n\t    next stage in pipeline, while receives the gradient tensor from the\n\t    next stage in pipeline as the input gradient tensor of this stage.\n\t    Args:\n", "        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n\t        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n\t    Returns:\n\t        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor.\n\t    \"\"\"\n\t    if tpc.is_last_in_pipeline_group():\n\t        output_tensor_grad = None\n\t    else:\n\t        _, output_tensor_grad = _communicate(\n\t            object_send_next=output_tensor,\n", "            recv_next=recv_next,\n\t            recv_next_shape=output_grad_shape,\n\t            next_rank=next_rank,\n\t            dtype=dtype,\n\t            scatter_gather_tensors=scatter_gather_tensors,\n\t        )\n\t    return output_tensor_grad\n\tdef send_backward_recv_forward(\n\t    input_tensor_grad,\n\t    input_tensor_shape,\n", "    recv_prev=True,\n\t    prev_rank=None,\n\t    dtype=torch.float,\n\t    scatter_gather_tensors=False,\n\t) -> Union[torch.Tensor, List[torch.Tensor]]:\n\t    \"\"\"Batched communication operation. Sends the gradient tensor to the\n\t    previous stage in pipeline, while receives the output tensor from the\n\t    previous stage in pipeline as the input of this stage.\n\t    Args:\n\t        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n", "        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n\t    Returns:\n\t        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input tensor.\n\t    \"\"\"\n\t    if tpc.is_first_in_pipeline_group():\n\t        input_tensor = None\n\t    else:\n\t        input_tensor, _ = _communicate(\n\t            object_send_prev=input_tensor_grad,\n\t            recv_prev=recv_prev,\n", "            recv_prev_shape=input_tensor_shape,\n\t            prev_rank=prev_rank,\n\t            dtype=dtype,\n\t            scatter_gather_tensors=scatter_gather_tensors,\n\t        )\n\t    return input_tensor\n\tdef send_forward_recv_forward(\n\t    output_tensor,\n\t    input_tensor_shape,\n\t    recv_prev=True,\n", "    prev_rank=None,\n\t    next_rank=None,\n\t    dtype=torch.float,\n\t    scatter_gather_tensors=False,\n\t) -> Union[torch.Tensor, List[torch.Tensor]]:\n\t    \"\"\"Batched communication operation. Sends the input tensor to the\n\t    next stage in pipeline, while receives the output tensor from the\n\t    previous stage in pipeline as the input of this stage.\n\t    Args:\n\t        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n", "        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n\t    Returns:\n\t        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input tensor.\n\t    \"\"\"\n\t    input_tensor, _ = _communicate(\n\t        object_send_next=output_tensor,\n\t        recv_prev=recv_prev,\n\t        recv_prev_shape=input_tensor_shape,\n\t        prev_rank=prev_rank,\n\t        next_rank=next_rank,\n", "        dtype=dtype,\n\t        scatter_gather_tensors=scatter_gather_tensors,\n\t    )\n\t    return input_tensor\n\tdef send_backward_recv_backward(\n\t    input_tensor_grad,\n\t    output_grad_shape,\n\t    recv_next=True,\n\t    prev_rank=None,\n\t    next_rank=None,\n", "    dtype=torch.float,\n\t    scatter_gather_tensors=False,\n\t) -> Union[torch.Tensor, List[torch.Tensor]]:\n\t    \"\"\"Batched communication operation. Sends the gradient tensor to the\n\t    previous stage in pipeline, while receives the gradient tensor from the\n\t    next member in pipeline as the input of this stage.\n\t    Args:\n\t        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n\t        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n\t    Returns:\n", "        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor.\n\t    \"\"\"\n\t    _, output_tensor_grad = _communicate(\n\t        object_send_prev=input_tensor_grad,\n\t        recv_next=recv_next,\n\t        recv_next_shape=output_grad_shape,\n\t        prev_rank=prev_rank,\n\t        next_rank=next_rank,\n\t        dtype=dtype,\n\t        scatter_gather_tensors=scatter_gather_tensors,\n", "    )\n\t    return output_tensor_grad\n\tdef send_forward_backward_recv_forward_backward(\n\t    output_tensor,\n\t    input_tensor_grad,\n\t    input_tensor_shape,\n\t    output_grad_shape,\n\t    recv_prev=True,\n\t    recv_next=True,\n\t    prev_rank=None,\n", "    next_rank=None,\n\t    dtype=torch.float,\n\t    scatter_gather_tensors=False,\n\t) -> Tuple[Union[torch.Tensor, List[torch.Tensor]]]:\n\t    \"\"\"Batched communication operation. Sends the input tensor to the next stage in pipeline and\n\t    the gradient tensor to the previous stage, while receives the input gradient tensor from the\n\t    next stage and the input tensor from the previous stage.\n\t    Args:\n\t        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor sent to the next.\n\t        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor sent to the previous.\n", "        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor received from the previous.\n\t        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor received from the next.\n\t    Returns:\n\t        Tuple(Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]], Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): (the input tensor, the input gradient tensor)\n\t    \"\"\"\n\t    input_tensor, output_tensor_grad = _communicate(\n\t        object_send_next=output_tensor,\n\t        object_send_prev=input_tensor_grad,\n\t        recv_prev=recv_prev,\n\t        recv_next=recv_next,\n", "        recv_prev_shape=input_tensor_shape,\n\t        recv_next_shape=output_grad_shape,\n\t        prev_rank=prev_rank,\n\t        next_rank=next_rank,\n\t        dtype=dtype,\n\t        scatter_gather_tensors=scatter_gather_tensors,\n\t    )\n\t    return input_tensor, output_tensor_grad\n"]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/pipeline_helper.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom torchdistpackage import tpc\n\tdef partition_uniform(flat_sequence, extra_len=0) -> list:\n\t    \"\"\"\n\t        accept a list and return a list of Modules\n\t    \"\"\"\n\t    rank = tpc.get_group_rank(\"pipe\")\n\t    world_size = tpc.get_group_size(\"pipe\")\n\t    leng = len(flat_sequence) + extra_len\n", "    length = leng // world_size\n\t    beg = rank * length\n\t    end = (rank + 1) * length if rank != world_size - 1 else len(flat_sequence)\n\t    kept_flat_sequence = flat_sequence[beg:end]\n\t    return kept_flat_sequence\n\tdef _heap_addition(weights: list, intervals: list, add_cnt: int):\n\t    import heapq\n\t    def _heap_push(heap, st, ed):\n\t        value = weights[ed - 1]\n\t        if st > 0:\n", "            value -= weights[st - 1]\n\t        heapq.heappush(heap, (-value, st, ed))\n\t    ret_intervals = []\n\t    heap = []\n\t    for st, ed in intervals:\n\t        _heap_push(heap, st, ed)\n\t    while add_cnt > 0:\n\t        _, st, ed = heapq.heappop(heap)\n\t        if ed - st == 1:\n\t            ret_intervals.append((st, ed))\n", "        else:\n\t            l, m, r = _binary_partition(weights, st, ed)\n\t            _heap_push(heap, l, m)\n\t            _heap_push(heap, m, r)\n\t            add_cnt -= 1\n\t    while heap:\n\t        _, st, ed = heapq.heappop(heap)\n\t        ret_intervals.append((st, ed))\n\t    ret_intervals.sort()\n\t    return ret_intervals\n", "def _calc_partitions(weights, value):\n\t    prev = 0\n\t    prefix = 0\n\t    num_block = 0\n\t    intervals = []\n\t    for idx, w in enumerate(weights):\n\t        if weights[idx] - prefix > value:\n\t            intervals.append((prev, idx))\n\t            prev = idx\n\t            prefix = weights[idx - 1]\n", "            num_block += 1\n\t    intervals.append((prev, len(weights)))\n\t    return num_block + 1, intervals\n\tdef _binary_search(weights, num):\n\t    length = len(weights)\n\t    prefix = [1 if w == 0 else w for w in weights]\n\t    for i in range(1, length):\n\t        prefix[i] += prefix[i - 1]\n\t    lower_bound = max(weights)\n\t    upper_bound = prefix[length - 1]\n", "    while upper_bound > lower_bound:\n\t        mid = (upper_bound + lower_bound) // 2\n\t        number, _ = _calc_partitions(prefix, mid)\n\t        if number <= num:\n\t            upper_bound = mid\n\t        else:\n\t            lower_bound = mid + 1\n\t    num_block, intervals = _calc_partitions(prefix, upper_bound)\n\t    if num_block < num:\n\t        intervals = _heap_addition(prefix, intervals, num - num_block)\n", "    return intervals\n\tdef partition_balanced(flat_sequence, sequence, **kwargs):\n\t    rank = tpc.get_group_rank(\"pipe\")\n\t    world_size = tpc.get_group_size(\"pipe\")\n\t    assert len(flat_sequence) >= world_size\n\t    def count_params(model):\n\t        param_count = 0\n\t        for param in model.parameters():\n\t            param_count += param.numel()\n\t        if param_count == 0:\n", "            param_count = 1\n\t        return param_count\n\t    weight_sequence = [count_params(item) for item in flat_sequence]\n\t    del count_params\n\t    intervals = _binary_search(weight_sequence, world_size)\n\t    sequence = flat_sequence[intervals[rank][0] : intervals[rank][1]]\n\t    return sequence\n\tdef flatten_sequence(sequence, level=1):\n\t    \"\"\"\n\t        Flatten a give model of type nn.Sequential, since the child module maybe nn.Sequential\n", "    \"\"\"\n\t    if level == 0:\n\t        if isinstance(sequence, list):\n\t            return sequence\n\t        elif isinstance(sequence, torch.nn.Sequential):\n\t            return [i for i in sequence]\n\t        else:\n\t            return [sequence]\n\t    res = []\n\t    for element in sequence:\n", "        res += flatten_sequence(element, level - 1)\n\t    return res\n\tclass CallableModule(torch.nn.Module):\n\t    \"\"\"\n\t        wraps a Callable into a nn.Module\n\t    \"\"\"\n\t    def __init__(self, fn):\n\t        super(CallableModule, self).__init__()\n\t        self.fn = fn\n\t    def forward(self, x):\n", "        return self.fn(x)\n\tdef flatten_model(model, layer_list, return_list=False):\n\t    \"\"\"\n\t        flatten a model that is not a nn.Sequential, but according to a list of layer name\n\t        Example:\n\t            exec_seq = [\n\t                'conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool',\n\t                lambda x: torch.flatten(x, 1), 'fc'\n\t            ]\n\t            seq_model = flatten_model(model, exec_seq)\n", "    \"\"\"\n\t    module_list = []\n\t    for layer_name in layer_list:\n\t        if isinstance(layer_name, str):\n\t            sub_mod = getattr(model, layer_name)\n\t            if isinstance(sub_mod, torch.nn.modules.container.Sequential) or isinstance(\n\t                sub_mod, torch.nn.ModuleList\n\t            ):\n\t                for op in sub_mod:\n\t                    module_list.append(op)\n", "            else:\n\t                module_list.append(sub_mod)\n\t        elif isinstance(layer_name, torch.nn.Module):\n\t            module_list.append(layer_name)\n\t        elif callable(layer_name):\n\t            # maybe lambda, or functions, this maynot work for nn.Sequential\n\t            module_list.append(CallableModule(layer_name))\n\t        else:\n\t            # unknown\n\t            print(\"flatten_model do not support layer: \", layer_name, type(layer_name))\n", "            raise NotImplementedError()\n\t    if return_list:\n\t        return module_list\n\t    return nn.Sequential(*module_list)\n\tdef flat_and_partition(sequence, flat_level=1, partition_policy=\"uniform\", **kwargs):\n\t    flattened = flatten_sequence(sequence, flat_level)\n\t    partition_fn = eval(f\"partition_{partition_policy}\")\n\t    cur_partition = partition_fn(flattened, **kwargs)\n\t    return cur_partition\n"]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/clip_grad_parallel.py", "chunked_list": ["# adapted from https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_\n\timport torch\n\timport torch.distributed as dist\n\tfrom torchdistpackage import tpc, is_using_pp\n\timport warnings\n\tfrom typing import Union, Iterable, List, Dict, Tuple, Optional\n\timport torch\n\tfrom torch import Tensor, inf\n\tfrom collections import defaultdict\n\tfrom torch.autograd.grad_mode import no_grad\n", "_tensor_or_tensors = Union[torch.Tensor, Iterable[torch.Tensor]]\n\tdef clip_grad_norm_(\n\t        parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0,\n\t        error_if_nonfinite: bool = False) -> torch.Tensor:\n\t    r\"\"\"Clips gradient norm of an iterable of parameters.\n\t    The norm is computed over all gradients together, as if they were\n\t    concatenated into a single vector. Gradients are modified in-place.\n\t    Args:\n\t        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n\t            single Tensor that will have gradients normalized\n", "        max_norm (float or int): max norm of the gradients\n\t        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n\t            infinity norm.\n\t        error_if_nonfinite (bool): if True, an error is thrown if the total\n\t            norm of the gradients from :attr:`parameters` is ``nan``,\n\t            ``inf``, or ``-inf``. Default: False (will switch to True in the future)\n\t    Returns:\n\t        Total norm of the parameter gradients (viewed as a single vector).\n\t    \"\"\"\n\t    if isinstance(parameters, torch.Tensor):\n", "        parameters = [parameters]\n\t    parameters = [p for p in parameters if p.grad is not None]\n\t    max_norm = float(max_norm)\n\t    norm_type = float(norm_type)\n\t    if len(parameters) == 0:\n\t        return torch.tensor(0.)\n\t    device = parameters[0].grad.device\n\t    if norm_type == inf:\n\t        norms = [p.grad.detach().abs().max().to(device) for p in parameters]\n\t        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))\n", "    else:\n\t        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n\t    if error_if_nonfinite and torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n\t        raise RuntimeError(\n\t            f'The total norm of order {norm_type} for gradients from '\n\t            '`parameters` is non-finite, so it cannot be clipped. To disable '\n\t            'this error and scale the gradients by the non-finite norm anyway, '\n\t            'set `error_if_nonfinite=False`')\n\t    # for model parallel, and ZeRO(>=2), we need to collect total_norm from all MP ranks\n\t    if is_using_pp():\n", "        dist.all_reduce(\n\t            total_norm, op=torch.distributed.ReduceOp.SUM, group=tpc.get_group(\"pipe\")\n\t        )\n\t    # TODO(sdx): other parallel mode to be supported\n\t    clip_coef = max_norm / (total_norm + 1e-6)\n\t    # Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\n\t    # avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\n\t    # when the gradients do not reside in CPU memory.\n\t    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n\t    for p in parameters:\n", "        p.grad.detach().mul_(clip_coef_clamped.to(p.grad.device))\n\t    return total_norm\n\t# This util function splits tensors into groups by device and dtype, which is useful before sending\n\t# tensors off to a foreach implementation, which requires tensors to be on one device and dtype.\n\t# If tensorlistlist contains more than one tensorlist, the following assumptions are made BUT NOT verified:\n\t#   - tensorlists CAN be None\n\t#   - all tensors in the first specified list cannot be None\n\t#   - given an index i, all specified tensorlist[i]s match in dtype and device\n\t# with_indices (bool, optional): whether to track previous indices as the last list per dictionary entry.\n\t#   It comes in handy if there are Nones or literals in the tensorlists that are getting scattered out.\n", "#   Whereas mutating a tensor in the resulting split-up tensorlists WILL propagate changes back to the\n\t#   original input tensorlists, changing up Nones/literals WILL NOT propagate, and manual propagation\n\t#   may be necessary. Check out torch/optim/sgd.py for an example.\n\t@no_grad()\n\tdef _group_tensors_by_device_and_dtype(tensorlistlist: List[List[Tensor]],\n\t                                       with_indices: Optional[bool] = False) -> \\\n\t        Dict[Tuple[torch.device, torch.dtype], List[List[Union[Tensor, int]]]]:\n\t    assert all(not x or len(x) == len(tensorlistlist[0]) for x in tensorlistlist), (\n\t           \"all specified tensorlists must match in length\")\n\t    per_device_and_dtype_tensors: Dict[Tuple[torch.device, torch.dtype], List[List[Union[Tensor, int]]]] = defaultdict(\n", "        lambda: [[] for _ in range(len(tensorlistlist) + (1 if with_indices else 0))])\n\t    for i, t in enumerate(tensorlistlist[0]):\n\t        key = (t.device, t.dtype)\n\t        for j in range(len(tensorlistlist)):\n\t            # a tensorlist may be empty/None\n\t            if tensorlistlist[j]:\n\t                per_device_and_dtype_tensors[key][j].append(tensorlistlist[j][i])\n\t        if with_indices:\n\t            # tack on previous index\n\t            per_device_and_dtype_tensors[key][j + 1].append(i)\n", "    return per_device_and_dtype_tensors\n\tclass NativeScalerPP:\n\t    state_dict_key = \"amp_scaler\"\n\t    def __init__(self):\n\t        self._scaler = torch.cuda.amp.GradScaler()\n\t    def __call__(\n\t            self,\n\t            loss,\n\t            optimizer,\n\t            clip_grad=None,\n", "            clip_mode='norm',\n\t            parameters=None,\n\t            create_graph=False,\n\t            need_update=True,\n\t    ):\n\t        self._scaler.scale(loss).backward(create_graph=create_graph)\n\t        # TODO: how to broadcast the scale?\n\t        # if is_using_pp():\n\t        #     cur_scale = self._scaler.get_scale()\n\t        #     if tpc.is_last_in_pipeline_group():\n", "        #         dist.broadcast()\n\t        if need_update:\n\t            if clip_grad is not None:\n\t                assert parameters is not None\n\t                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n\t                clip_grad_norm_(parameters, clip_grad)\n\t            self._scaler.step(optimizer)\n\t            self._scaler.update()\n\t    def state_dict(self):\n\t        return self._scaler.state_dict()\n", "    def load_state_dict(self, state_dict):\n\t        self._scaler.load_state_dict(state_dict)\n"]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/__init__.py", "chunked_list": []}
{"filename": "torchdistpackage/parallel/pipeline_parallel/pipeline_sched.py", "chunked_list": ["import torch\n\tfrom torchdistpackage import tpc\n\tfrom . import comm\n\tdef _forward_step_in_forward_backward(\n\t    input_obj_from_prev, ind, micro_bs, fwd_fn, extra_inputs=[]\n\t):\n\t    \"\"\"\n\t        params:\n\t            input_obj_from_prev: the output of prev stage\n\t            ind: current micro-batch index\n", "            micro_bs: the micro-batch batchsize\n\t            fwd_fn: the fwd func of current stage\n\t            extra_inputs: extra inputs for current stage, will be split into micro batches\n\t    \"\"\"\n\t    cur_inputs = []\n\t    if input_obj_from_prev is not None:\n\t        if isinstance(input_obj_from_prev, torch.Tensor):\n\t            cur_inputs.append(input_obj_from_prev)\n\t        elif isinstance(input_obj_from_prev, list) or isinstance(\n\t            input_obj_from_prev, tuple\n", "        ):\n\t            cur_inputs.append(*input_obj_from_prev)\n\t    for inp in extra_inputs:\n\t        cur_inputs.append(inp[ind * micro_bs : (ind + 1) * micro_bs])\n\t    # inputs made of two parts: the prev stage output, and given mini-batch ( that should be split into micro-batches)\n\t    if len(cur_inputs) == 1:\n\t        return fwd_fn(cur_inputs[0])\n\t    else:\n\t        return fwd_fn(cur_inputs)\n\tdef _backward_step_in_forward_backward(\n", "    input_obj, output_obj, output_obj_grad, backward_fn\n\t):\n\t    \"\"\"\n\t        runs backward using user given function, supports `optimizer.backward(loss)`\n\t    \"\"\"\n\t    # Retain the grad on the input_obj.\n\t    if input_obj is not None:\n\t        if isinstance(input_obj, torch.Tensor):\n\t            input_obj.retain_grad()\n\t        else:\n", "            for in_tensor in input_obj:\n\t                if in_tensor is not None:\n\t                    in_tensor.retain_grad()\n\t    if backward_fn is None:\n\t        if output_obj_grad is None:\n\t            output_obj.backward()  # equal to loss.backward\n\t        else:\n\t            torch.autograd.backward(tensors=output_obj, grad_tensors=output_obj_grad)\n\t    else:\n\t        backward_fn(output_obj, output_obj_grad)\n", "    # Collect the grad of the input_obj.\n\t    input_obj_grad = None\n\t    if input_obj is not None:\n\t        if isinstance(input_obj, torch.Tensor):\n\t            input_obj_grad = input_obj.grad\n\t        else:\n\t            input_obj_grad = []\n\t            for in_tensor in input_obj:\n\t                input_obj_grad.append(in_tensor.grad)\n\t    return input_obj_grad\n", "def forward_backward(\n\t    optimizer,\n\t    fwd_fn,\n\t    bwd_fn,\n\t    inputs,\n\t    num_microbatches=1,\n\t    forward_only=False,\n\t    dtype=torch.bfloat16,\n\t    scatter_gather_tensors=False,\n\t):\n", "    \"\"\"\n\t        params:\n\t            optimizer: the optimizer, used to call optimizer.zero_grad()\n\t            fwd_fn & bwd_fn: the fwd/bwd func of current stage\n\t            inputs: inputs for current stage, for the first stage this must not be None,\n\t                    for other stages, this could be None, and could also have extra inputs\n\t            num_microbatches: the micro-batch number\n\t            forward_only: if run forward_only, no backward is run\n\t            dtype: tensor dtype\n\t            scatter_gather_tensors: for communication\n", "    \"\"\"\n\t    num_warmup_microbatches = (\n\t        tpc.get_group_size(\"pipe\") - tpc.get_group_rank(\"pipe\") - 1\n\t    )\n\t    num_warmup_microbatches = min(num_warmup_microbatches, num_microbatches)\n\t    num_microbatches_remaining = num_microbatches - num_warmup_microbatches\n\t    if isinstance(inputs, torch.Tensor):\n\t        inputs = [inputs]\n\t    elif inputs == None:\n\t        assert (\n", "            not tpc.is_first_in_pipeline_group()\n\t        ), \"pipeline 1st stage should have valid inputs!\"\n\t        inputs = []\n\t    micro_bs = 0\n\t    if len(inputs) > 0:\n\t        mini_bs = inputs[0].size(0)\n\t        micro_bs = int(mini_bs / num_microbatches)\n\t    # Input, output tensors only need to be saved when doing backward passes\n\t    input_objs = None\n\t    output_objs = None\n", "    if not forward_only:\n\t        input_objs = []\n\t        output_objs = []\n\t    # Used for tensor meta information communication\n\t    ft_shapes = None\n\t    bt_shapes = None\n\t    fs_checker = True\n\t    if optimizer:\n\t        optimizer.zero_grad()\n\t    # Run warmup forward passes.\n", "    for i in range(num_warmup_microbatches):\n\t        if not tpc.is_first_in_pipeline_group:\n\t            ft_shapes = comm.recv_obj_meta(ft_shapes)\n\t        input_obj = comm.recv_forward(\n\t            ft_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n\t        )\n\t        output_obj = _forward_step_in_forward_backward(\n\t            input_obj, i, micro_bs, fwd_fn, inputs\n\t        )\n\t        if not tpc.is_last_in_pipeline_group():\n", "            if isinstance(output_obj, torch.Tensor):\n\t                bt_shapes = output_obj.shape\n\t            else:\n\t                bt_shapes = []\n\t                for out_tensor in output_obj:\n\t                    bt_shapes.append(out_tensor.shape)\n\t            fs_checker = comm.send_obj_meta(output_obj, fs_checker)\n\t        comm.send_forward(output_obj, scatter_gather_tensors=scatter_gather_tensors)\n\t        if not forward_only:\n\t            input_objs.append(input_obj)\n", "            output_objs.append(output_obj)\n\t    # Before running 1F1B, need to receive first forward tensor.\n\t    # If all microbatches are run in warmup / cooldown phase, then no need to\n\t    # receive this tensor here.\n\t    if num_microbatches_remaining > 0:\n\t        if not tpc.is_first_in_pipeline_group():\n\t            ft_shapes = comm.recv_obj_meta(ft_shapes)\n\t        input_obj = comm.recv_forward(\n\t            ft_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n\t        )\n", "    # Run 1F1B in steady state.\n\t    for i in range(num_microbatches_remaining):\n\t        last_iteration = i == (num_microbatches_remaining - 1)\n\t        output_obj = _forward_step_in_forward_backward(\n\t            input_obj, i + num_warmup_microbatches, micro_bs, fwd_fn, inputs\n\t        )\n\t        if forward_only:\n\t            comm.send_forward(output_obj, scatter_gather_tensors=scatter_gather_tensors)\n\t            if not last_iteration:\n\t                input_obj = comm.recv_forward(\n", "                    ft_shapes,\n\t                    dtype=dtype,\n\t                    scatter_gather_tensors=scatter_gather_tensors,\n\t                )\n\t        else:\n\t            output_obj_grad = comm.send_forward_recv_backward(\n\t                output_obj,\n\t                bt_shapes,\n\t                dtype=dtype,\n\t                scatter_gather_tensors=scatter_gather_tensors,\n", "            )\n\t            # Add input_obj and output_obj to end of list.\n\t            input_objs.append(input_obj)\n\t            output_objs.append(output_obj)\n\t            # Pop output_obj and output_obj from the start of the list for\n\t            # the backward pass.\n\t            input_obj = input_objs.pop(0)\n\t            output_obj = output_objs.pop(0)\n\t            input_obj_grad = _backward_step_in_forward_backward(\n\t                input_obj, output_obj, output_obj_grad, bwd_fn\n", "            )\n\t            if last_iteration:\n\t                input_obj = None\n\t                comm.send_backward(\n\t                    input_obj_grad, scatter_gather_tensors=scatter_gather_tensors\n\t                )\n\t            else:\n\t                input_obj = comm.send_backward_recv_forward(\n\t                    input_obj_grad,\n\t                    ft_shapes,\n", "                    dtype=dtype,\n\t                    scatter_gather_tensors=scatter_gather_tensors,\n\t                )\n\t    # Run cooldown backward passes.\n\t    if not forward_only:\n\t        for i in range(num_warmup_microbatches):\n\t            input_obj = input_objs.pop(0)\n\t            output_obj = output_objs.pop(0)\n\t            output_obj_grad = comm.recv_backward(\n\t                bt_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n", "            )\n\t            input_obj_grad = _backward_step_in_forward_backward(\n\t                input_obj, output_obj, output_obj_grad, bwd_fn\n\t            )\n\t            comm.send_backward(\n\t                input_obj_grad, scatter_gather_tensors=scatter_gather_tensors\n\t            )\n\t    return output_obj\n\tdef forward_eval(fwd_fn, inputs, dtype, **kwargs):\n\t    \"\"\"\n", "        params:\n\t            fwd_fn: the fwd func of current stage\n\t            inputs: inputs for current stage, for the first stage this must not be None,\n\t                    for other stages, this could be None, and could also have extra inputs\n\t            dtype: tensor dtype\n\t    \"\"\"\n\t    scatter_gather_tensors = False\n\t    fwd_inputs = []\n\t    # receve output from prev stage except for 1st stage\n\t    if not tpc.is_first_in_pipeline_group():\n", "        ft_shapes = None\n\t        ft_shapes = comm.recv_obj_meta(ft_shapes)\n\t        output_from_prev = comm.recv_forward(\n\t            ft_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n\t        )\n\t        fwd_inputs.append(output_from_prev)\n\t    # create input\n\t    if isinstance(inputs, torch.Tensor):\n\t        fwd_inputs.append(inputs)\n\t    elif isinstance(inputs, list):\n", "        for inp in inputs:\n\t            fwd_inputs.append(inp)\n\t    if len(fwd_inputs) == 1:\n\t        fwd_inputs = fwd_inputs[0]\n\t    # run forward\n\t    fwd_output = None\n\t    fwd_output = fwd_fn(fwd_inputs)\n\t    if not tpc.is_last_in_pipeline_group():\n\t        comm.send_obj_meta(fwd_output)\n\t        comm.send_forward(fwd_output, scatter_gather_tensors=scatter_gather_tensors)\n", "    return fwd_output\n"]}
{"filename": "torchdistpackage/dist/py_comm_test.py", "chunked_list": ["import os\n\timport torch\n\timport torch.distributed as dist\n\timport time\n\timport functools\n\tfrom torchdistpackage import setup_distributed_slurm\n\t# reference: https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md\n\t# algbw = Size/time\n\t# bus_bw = algbw * fraction * (n-1)/n\n\tmode_2_frac = dict(\n", "    all_reduce = 2,\n\t    all_gather = 1,\n\t    reduce_scatter = 1,\n\t)\n\tdef test_collection(ele_num_total, mode = 'all_reduce', group=None):\n\t    comm_op = eval(f\"dist.{mode}\")\n\t    ws = dist.get_world_size(group)\n\t    ele_num = ele_num_total\n\t    if mode == \"all_gather\":\n\t        ele_num = int(ele_num_total // dist.get_world_size(group))\n", "    tensor = torch.randn(ele_num).half().cuda()\n\t    if mode == \"all_gather\":\n\t        tensor_list = [torch.randn(ele_num).half().cuda() for _ in range(ws)]\n\t        # import pdb;pdb.set_trace()\n\t        comm_op = functools.partial(comm_op, tensor_list)\n\t    comm_op(tensor, group=group)\n\t    comm_op(tensor, group=group)\n\t    dist.barrier()\n\t    torch.cuda.synchronize()\n\t    bw=0\n", "    frac = mode_2_frac[mode]\n\t    num_repeat = 1\n\t    dist.barrier()\n\t    torch.cuda.synchronize()\n\t    beg=time.perf_counter()\n\t    for _ in range(num_repeat):\n\t        comm_op(tensor, group=group)\n\t    dist.barrier()\n\t    torch.cuda.synchronize()\n\t    time_avg = (time.perf_counter()-beg)/num_repeat\n", "    algbw = (ele_num_total*2/1e9)/time_avg # GB/s\n\t    bw = algbw * frac * ((ws-1)/ws)\n\t    bw = round(bw, 3)\n\t    time_avg = round(time_avg, 3)\n\t    if dist.get_rank()==0:\n\t        print(f\"{mode} repeat={num_repeat}, bandwidth:{bw} GB/s time_avg:{time_avg} s, numel={tensor.numel()}\")\n\t    torch.cuda.synchronize()\n\t    return bw,time_avg\n\tdef test_all2all_balanced(ele_num, group=None):\n\t    tensor = torch.ones(ele_num).cuda() * dist.get_rank()\n", "    output = torch.empty_like(tensor)\n\t    dist.all_to_all_single(output, tensor, group=group)\n\t    dist.barrier()\n\t    torch.cuda.synchronize()\n\t    num_repeat = 1\n\t    beg=time.perf_counter()\n\t    for _ in range(num_repeat):\n\t        dist.all_to_all_single(output, tensor, group=group)\n\t    dist.barrier()\n\t    torch.cuda.synchronize()\n", "    time_avg = (time.perf_counter()-beg)/num_repeat\n\t    if dist.get_rank()==0:\n\t        print(f\"all2all_balanced repeat={num_repeat}, time_avg:{time_avg} s, numel={tensor.numel()}\")\n\tif __name__==\"__main__\":\n\t    setup_distributed_slurm()\n\t    test_collection(1801705472*2, mode='all_gather')\n"]}
{"filename": "torchdistpackage/dist/launch_from_slurm.py", "chunked_list": ["import os\n\timport subprocess\n\timport torch\n\timport torch.distributed as dist\n\t# TODO: this func is not exmamined\n\tdef find_free_port():\n\t    import socket\n\t    from contextlib import closing\n\t    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n\t        s.bind((\"\", 0))\n", "        return str(s.getsockname()[1])\n\tdef setup_distributed_slurm(backend=\"nccl\", port=None):\n\t    \"\"\"Initialize distributed training environment.\n\t    support both slurm and torch.distributed.launch\n\t    see torch.distributed.init_process_group() for more details\n\t    \"\"\"\n\t    import os\n\t    import subprocess\n\t    import torch\n\t    import torch.distributed as dist\n", "    num_gpus = torch.cuda.device_count()\n\t    if \"SLURM_JOB_ID\" in os.environ:\n\t        rank = int(os.environ[\"SLURM_PROCID\"])\n\t        world_size = int(os.environ[\"SLURM_NTASKS\"])\n\t        node_list = os.environ[\"SLURM_NODELIST\"]\n\t        addr = subprocess.getoutput(\n\t            f\"scontrol show hostname {node_list} | head -n1\")\n\t        if \"MASTER_ADDR\" not in os.environ:\n\t            os.environ[\"MASTER_ADDR\"] = addr\n\t        # specify master port\n", "        if port is not None:\n\t            os.environ[\"MASTER_PORT\"] = str(port)\n\t        elif \"MASTER_PORT\" not in os.environ:\n\t            port = 54647\n\t            os.environ[\"MASTER_PORT\"] = str(port)\n\t        else:\n\t            port = int(os.environ[\"MASTER_PORT\"])\n\t        os.environ[\"WORLD_SIZE\"] = str(world_size)\n\t        local_rank = rank % num_gpus\n\t        os.environ[\"LOCAL_RANK\"] = str(local_rank)\n", "        os.environ[\"RANK\"] = str(rank)\n\t    else:\n\t        rank = int(os.environ[\"RANK\"])\n\t        world_size = int(os.environ[\"WORLD_SIZE\"])\n\t        local_rank = rank % num_gpus\n\t    dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n\t    device = rank % torch.cuda.device_count()\n\t    torch.cuda.set_device(device)\n\t    print(f\"dist init done, world_size = {dist.get_world_size()}\")\n\t    return rank, world_size, port, addr"]}
{"filename": "torchdistpackage/dist/__init__.py", "chunked_list": []}
{"filename": "torchdistpackage/dist/utils.py", "chunked_list": ["import ctypes\n\timport time\n\timport math\n\timport warnings\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import init\n\t_cudart = ctypes.CDLL('libcudart.so')\n\tdef cu_prof_start():\n\t    \"\"\"\n", "    This function and cu_prof_stop  are two functions used to do multi process Nsight Profiling.\n\t    Example::\n\t        if self.iter == 20 and torch.distributed.get_rank() == 0:\n\t            cu_prof_start()\n\t        if self.iter >= 40:\n\t            cu_prof_stop()\n\t            exit()\n\t    \"\"\"\n\t    ret = _cudart.cudaProfilerStart()\n\t    if ret != 0:\n", "        raise Exception('cudaProfilerStart() returned %d' % ret)\n\t    else:\n\t        print('cudaProfilerStart() returned %d' % ret)\n\tdef cu_prof_stop():\n\t    ret = _cudart.cudaProfilerStop()\n\t    if ret != 0:\n\t        raise Exception('cudaProfilerStop() returned %d' % ret)\n\t    else:\n\t        print('cudaProfilerStop() returned %d' % ret)\n\t        exit()\n", "def nvtx_decorator(func):\n\t    \"\"\"decorator that causes an NVTX range to be recorded for the duration of the\n\t    function call.\"\"\"\n\t    def wrapped_fn(*args, **kwargs):\n\t        torch.cuda.nvtx.range_push(func.__qualname__)\n\t        ret_val = func(*args, **kwargs)\n\t        torch.cuda.nvtx.range_pop()\n\t        return ret_val\n\t    return wrapped_fn\n\tclass NVTX_Context(object):\n", "    \"\"\" A simple context used to record performance info.\n\t    \"\"\"\n\t    def __init__(self, context_name, record_time=False):\n\t        self.context_name = context_name\n\t        self.start_time = None\n\t        self.exit_time = None\n\t        self.record_time = record_time\n\t        self.synchronize = False\n\t    def __enter__(self):\n\t        torch.cuda.nvtx.range_push(self.context_name)\n", "        if self.record_time:\n\t            torch.cuda.synchronize()\n\t            self.start_time = time.time()\n\t    def __exit__(self, type, value, traceback):\n\t        if self.record_time:\n\t            torch.cuda.synchronize()\n\t            self.exit_time = time.time()\n\t            print(f\"{self.context_name} duration is {self.exit_time - self.start_time}\")\n\t        torch.cuda.nvtx.range_pop()\n\tdef _has_inf_or_nan(x, j=None):\n", "    try:\n\t        # if x is half, the .float() incurs an additional deep copy, but it's necessary if\n\t        # Pytorch's .sum() creates a one-element tensor of the same type as x\n\t        # (which is true for some recent version of pytorch).\n\t        cpu_sum = float(x.float().sum())\n\t        # More efficient version that can be used if .sum() returns a Python scalar\n\t        # cpu_sum = float(x.sum())\n\t    except RuntimeError as instance:\n\t        # We want to check if inst is actually an overflow exception.\n\t        # RuntimeError could come from a different error.\n", "        # If so, we still want the exception to propagate.\n\t        if \"value cannot be converted\" not in instance.args[0]:\n\t            raise\n\t        return True\n\t    else:\n\t        if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n\t            return True\n\t        return False\n\tdef disable_non_master_print(is_master):\n\t    \"\"\"\n", "    This function disables printing when not in master process\n\t    \"\"\"\n\t    import builtins as __builtin__\n\t    builtin_print = __builtin__.print\n\t    def print(*args, **kwargs):\n\t        force = kwargs.pop('force', False)\n\t        if is_master or force:\n\t            builtin_print(*args, **kwargs)\n\t    __builtin__.print = print"]}
{"filename": "torchdistpackage/dist/process_topo.py", "chunked_list": ["from functools import partial\n\tfrom collections import defaultdict\n\timport numpy\n\timport torch.distributed as dist\n\tclass SingletonMeta(type):\n\t    \"\"\"\n\t    The Singleton class can be implemented in different ways in Python. Some\n\t    possible methods include: base class, decorator, metaclass. We will use the\n\t    metaclass because it is best suited for this purpose.\n\t    \"\"\"\n", "    _instances = {}\n\t    def __call__(cls, *args, **kwargs):\n\t        \"\"\"\n\t        Possible changes to the value of the `__init__` argument do not affect\n\t        the returned instance.\n\t        \"\"\"\n\t        if cls not in cls._instances:\n\t            instance = super().__call__(*args, **kwargs)\n\t            cls._instances[cls] = instance\n\t        else:\n", "            assert len(args) == 0 and len(\n\t                kwargs) == 0, f'{cls.__name__} is a singleton class and a instance has been created.'\n\t        return cls._instances[cls]\n\tdef gen_inner_ranks(world_size, group_size):\n\t    num_groups = int(world_size//group_size)\n\t    return [list(range(g*group_size, (g+1)*group_size)) for g in range(num_groups)]\n\tdef gen_groups(world_size, group_size, strides, hook):\n\t    group_size = int(group_size)\n\t    num_groups = int(world_size//group_size)\n\t    if strides is None or len(strides)==0:\n", "        # most inner\n\t        list_of_ranks = gen_inner_ranks(world_size, group_size)\n\t        # print(list_of_ranks)\n\t        for ranks in list_of_ranks:\n\t            hook(ranks)\n\t    else:\n\t        inner_group_size = numpy.prod(strides)\n\t        list_of_inner_ranks = gen_inner_ranks(world_size, inner_group_size)\n\t        # print(list_of_inner_ranks)\n\t        for ind in range(inner_group_size):\n", "            chunks = [list_of_inner_ranks[x:x+group_size] for x in range(0, len(list_of_inner_ranks), group_size)]\n\t            # print(chunks)\n\t            for chunk in chunks:\n\t                cur_ranks = [ranks[ind] for ranks in chunk]\n\t                hook(cur_ranks)\n\tclass ProcessTopology(metaclass=SingletonMeta):\n\t    def __init__(self):\n\t        self._groups = dict()\n\t        self._ranks_in_group = dict()\n\t        self._ranks_all = defaultdict(list)\n", "    def _build_group(self, type, ranks):\n\t        self._ranks_all[type].append(ranks)\n\t        from datetime import timedelta\n\t        grp = dist.new_group(ranks, timeout=timedelta(seconds=100))\n\t        if dist.get_rank() in ranks:\n\t            self._groups[type] = grp\n\t            self._ranks_in_group[type] = ranks\n\t            if dist.get_rank() == ranks[0]:\n\t                print(f\"group {type}, ranks: {ranks}\")\n\t    def setup_process_groups(self, config:list):\n", "        \"\"\"\n\t            Example: setup_process_groups([('data',4), ('pipe',2), ('tensor',2)])   # world_size=16\n\t            Result:\n\t                tensor parallel groups:\n\t                    [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]\n\t                pipeline parallel groups:\n\t                    [0, 2]\n\t                    [4, 6]\n\t                    [8, 10]\n\t                    [12, 14]\n", "                    [1, 3]\n\t                    [5, 7]\n\t                    [9, 11]\n\t                    [13, 15]\n\t                data parallel groups:\n\t                    [0, 4, 8, 12]\n\t                    [1, 5, 9, 13]\n\t                    [2, 6, 10, 14]\n\t                    [3, 7, 11, 15]\n\t            Usage:\n", "                # setup\n\t                dist_init_slurm()\n\t                dist_config = [('data',world_size/(2*pp_size)), ('pipe',pp_size), ('tensor',2)]\n\t                torch_parallel_context.setup_process_groups(dist_config)\n\t                # api example\n\t                test_comm()\n\t        \"\"\"\n\t        dims = [item[0] for item in config]\n\t        sizes = [int(item[1]) for item in config]\n\t        self._groups['global'] = None\n", "        self._ranks_in_group['global'] = list(range(dist.get_world_size()))\n\t        for (dim, size) in config:\n\t            cur_dim_ind = dims.index(dim)\n\t            strides=sizes[cur_dim_ind+1:] if cur_dim_ind+1 < len(sizes) else []\n\t            gen_groups(dist.get_world_size(), size, strides, partial(self._build_group, dim))\n\t        # build Model Parallel Group Automatically\n\t        if \"tensor\" in dims or \"pipe\" in dims:\n\t            for g in range(len(self._ranks_all['data'][0])):\n\t                model_ranks = [dp_ranks[g] for dp_ranks in self._ranks_all['data']]\n\t                self._build_group(\"model\", model_ranks)\n", "    def build_moe_groups(self, moe_dp_size=None, moe_ep_size=None):\n\t        # build for moe: moe_data_parallel, moe_expert_parallel\n\t        # default: moe_expert_parallel group = DDP group\n\t        dp_ranks_all = self._ranks_all['data']\n\t        if moe_dp_size and not moe_ep_size:\n\t            moe_ep_size = int(self.get_dp_size()//moe_dp_size)\n\t        elif moe_ep_size and not moe_dp_size:\n\t            moe_dp_size = int(self.get_dp_size()//moe_ep_size)\n\t        elif moe_dp_size and moe_ep_size:\n\t            assert moe_dp_size*moe_ep_size == self.get_dp_size()\n", "        else:\n\t            print(\"invalid args: \", moe_dp_size, moe_ep_size)\n\t        print(f\"MoE group config: moe_dp_size={moe_dp_size}, moe_ep_size={moe_ep_size}\")\n\t        num_ep_groups = int(self.get_dp_size() // moe_ep_size)\n\t        num_dp_groups = int(self.get_dp_size() // moe_dp_size)\n\t        for dp_ranks in dp_ranks_all:\n\t            for ep_g_id in range(num_ep_groups):\n\t                moe_ep_ranks_id = list(range(ep_g_id*moe_ep_size, (ep_g_id+1)*moe_ep_size))\n\t                moe_ep_ranks = [dp_ranks[i] for i in moe_ep_ranks_id]\n\t                self._build_group(\"moe_ep\", moe_ep_ranks)\n", "            for dp_g_id in range(num_dp_groups):\n\t                moe_dp_ranks_id = list(range(dp_g_id, len(dp_ranks) ,moe_ep_size))\n\t                moe_dp_ranks = [dp_ranks[i] for i in moe_dp_ranks_id]\n\t                self._build_group(\"moe_dp\", moe_dp_ranks)\n\t    def _is_inited(self, mode):\n\t        return mode in self._groups\n\t    def get_group(self, mode):\n\t        if not self._is_inited(mode):\n\t            assert False, f\"{mode} is not initialized!\"\n\t        return self._groups[mode]\n", "    def get_group_rank(self, mode):\n\t        return dist.get_rank(group=self.get_group(mode))\n\t    def get_ranks_in_group(self, mode):\n\t        if not self._is_inited(mode):\n\t            assert False, f\"{mode} is not initialized!\"\n\t        return self._ranks_in_group[mode]\n\t    def get_tp_rank(self):\n\t        return self.get_group_rank('tensor')\n\t    def get_pp_rank(self):\n\t        return self.get_group_rank('pipe')\n", "    def get_dp_rank(self):\n\t        return self.get_group_rank('data')\n\t    def get_mp_rank(self):\n\t        return self.get_group_rank('model')\n\t    def get_group_size(self, mode):\n\t        if not self._is_inited(mode):\n\t            assert False, f\"{mode} is not initialized!\"\n\t        return len(self._ranks_in_group[mode])\n\t    def get_tp_size(self):\n\t        return self.get_group_size('tensor')\n", "    def get_pp_size(self):\n\t        return self.get_group_size('pipe')\n\t    def get_dp_size(self):\n\t        return self.get_group_size('data')\n\t    def get_mp_size(self):\n\t        return self.get_group_size('model')\n\t    def is_first_in_group(self, mode):\n\t        return self.get_group_rank(mode) == 0\n\t    def is_last_in_group(self, mode):\n\t        return dist.get_rank() == self._ranks_in_group[mode][-1]\n", "    def is_first_in_tensor_group(self):\n\t        return self.is_first_in_group('tensor')\n\t    def is_last_in_tensor_group(self):\n\t        return self.is_last_in_group('tensor')\n\t    def is_first_in_pipeline_group(self):\n\t        return self.is_first_in_group('pipe')\n\t    def is_last_in_pipeline_group(self):\n\t        return self.is_last_in_group('pipe')\n\t    def is_first_in_data_group(self):\n\t        return self.is_first_in_group('data')\n", "    def is_last_in_data_group(self):\n\t        return self.is_last_in_group('data')\n\t    def is_first_in_model_group(self):\n\t        return self.is_first_in_group('model')\n\t    def is_last_in_model_group(self):\n\t        return self.is_last_in_group('model')\n\t    def get_prev_global_rank(self, mode = 'pipe'):\n\t        local_rank = self.get_group_rank(mode)\n\t        world_size = self.get_group_size(mode)\n\t        ranks_in_group = self.get_ranks_in_group(mode)\n", "        return ranks_in_group[(local_rank - 1) % world_size]\n\t    def get_next_global_rank(self, mode = 'pipe'):\n\t        local_rank = self.get_group_rank(mode)\n\t        world_size = self.get_group_size(mode)\n\t        ranks_in_group = self.get_ranks_in_group(mode)\n\t        return ranks_in_group[(local_rank + 1) % world_size]\n\t    def is_mode_inited(self, mode):\n\t        return mode in self._groups and self.get_group_size(mode)>1\n\t    def all_dp_ranks(self):\n\t        return self._ranks_all['data']\n", "    def all_ranks(self, mode):\n\t        if not self._is_inited(mode):\n\t            assert False, f\"{mode} is not initialized!\"\n\t        return self._ranks_all[mode]\n\t    def is_first_group(self, mode):\n\t        # process group of 'type' may have several groups,\n\t        # sometime we only need process in the 'first' group to dp sth\n\t        if not self._is_inited(mode):\n\t            assert False, f\"{mode} is not initialized!\"\n\t        group_ranks = self._ranks_in_group[mode]\n", "        if group_ranks == self._ranks_all[mode][0]:\n\t            return True\n\t        else:\n\t            return  False\n\ttorch_parallel_context = ProcessTopology()\n\tdef is_using_pp():\n\t    return torch_parallel_context.is_mode_inited('pipe')\n\tdef test_comm():\n\t    import torch\n\t    tmp = torch.rand([100,1024]).cuda()\n", "    torch.cuda.synchronize()\n\t    dist.all_reduce(tmp, group=None)\n\t    for mode in ['data', 'tensor', 'pipe', 'model', 'moe_dp', 'moe_ep']:\n\t        if torch_parallel_context.is_mode_inited(mode):\n\t            dist.all_reduce(tmp, group=torch_parallel_context.get_group(mode))\n\t            torch.cuda.synchronize()\n\t        print('passed:', mode)\n\t    len_dl_tensor = torch.tensor([0], dtype=torch.long).cuda()\n\t    if torch_parallel_context.is_first_in_group('model'):\n\t        len_dl = 10\n", "        len_dl_tensor = torch.tensor([len_dl], dtype=torch.long).cuda()\n\t    dist.broadcast(len_dl_tensor,0)\n\t    if torch_parallel_context.is_mode_inited('model'):\n\t        dist.broadcast(len_dl_tensor, torch_parallel_context.get_ranks_in_group('model')[0], torch_parallel_context.get_group('model'))\n\t        torch.cuda.synchronize()\n\t    if torch_parallel_context.is_mode_inited('tensor'):\n\t        outs = [torch.rand_like(tmp) for _ in range(torch_parallel_context.get_group_size('tensor'))]\n\t        dist.all_gather(outs, tmp, group=torch_parallel_context.get_group('tensor'))\n\t        torch.cuda.synchronize()\n\t    if torch_parallel_context.is_mode_inited('pipe'):\n", "        if torch_parallel_context.is_first_in_pipeline_group():\n\t            dist.send(tmp, torch_parallel_context.get_next_global_rank('pipe'))\n\t        if torch_parallel_context.is_last_in_pipeline_group():\n\t            dist.recv(tmp, torch_parallel_context.get_prev_global_rank('pipe'))\n\t        torch.cuda.synchronize()\n\t    dist.barrier()\n\t    print(\"Finished test_comm --- \")\n"]}
{"filename": "torchdistpackage/dist/model_parallel_ckpt.py", "chunked_list": ["# this file include functions for saving/loading checkpoints for Model Parallelism\n\tfrom torchdistpackage import tpc\n\tdef get_mp_ckpt_suffix():\n\t    \"\"\"\n\t        get the suffix of state_dict filename, in format: f\"_tp_{tp_rank}_pp_{pp_rank}.pth\"\n\t        example:\n\t            \"_tp_1_pp_2.pth\", mean this partition is Rank 1 in TensorParallel and Rank 2 in PipelineParallel\n\t    \"\"\"\n\t    fname = \"\"\n\t    if is_mode_inited('tensor'):\n", "        tp_rank = tpc.get_group_rank(\"tensor\")\n\t        fname += f\"_tp_{tp_rank}\"\n\t    if is_mode_inited('pipe'):\n\t        pp_rank = tpc.get_group_rank(\"pipe\")\n\t        fname += f\"_pp_{pp_rank}\"\n\t    fname += f\".pth\"\n\t    return fname\n"]}
{"filename": "explore/perf/test_timm.py", "chunked_list": ["import os\n\timport time\n\timport matplotlib.pyplot as plt\n\timport torch\n\tfrom timm.models import create_model\n\tfrom timm.models.vision_transformer import VisionTransformer\n\tfrom torch.cuda.amp import GradScaler, autocast\n\tfrom functorch.compile import memory_efficient_fusion\n\tfrom functorch.compile import min_cut_rematerialization_partition\n\tfrom functorch.compile import ts_compile\n", "from functorch.compile import aot_function, \\\n\t    make_boxed_func, ts_compile\n\tfrom functorch.compile import ts_compile, aot_module\n\tdef train_aot(model,input, amp_enable=False, warmup=False):\n\t    # aot_compiled_mod = aot_module(model,\n\t    #                               fw_compiler=ts_compile,\n\t    #                               bw_compiler=ts_compile)\n\t    aot_compiled_mod=memory_efficient_fusion(model)\n\t    for i in range(0, 3):\n\t        with autocast(enabled=amp_enable):\n", "            out=aot_compiled_mod(input)\n\t        out.sum().backward()\n\t    torch.cuda.synchronize()\n\t    time_start=time.time()\n\t    for i in range(0, 10):\n\t        with autocast(enabled=amp_enable):\n\t            out=aot_compiled_mod(input)\n\t        out.sum().backward()\n\t    torch.cuda.synchronize()\n\t    if not warmup:\n", "        print(f'time used: {time.time()-time_start}')\n\t    return time.time()-time_start\n\tdef train(model, input, amp_enable=False, warmup=False):\n\t    for i in range(0, 3):\n\t        with autocast(enabled=amp_enable):\n\t            out=model(input)\n\t        out.sum().backward()\n\t    torch.cuda.synchronize()\n\t    time_start=time.time()\n\t    for i in range(0, 10):\n", "        with autocast(enabled=amp_enable):\n\t            out=model(input)\n\t        out.sum().backward()\n\t    torch.cuda.synchronize()\n\t    if not warmup:\n\t        print(f'time used: {time.time()-time_start}')\n\t    return time.time()-time_start\n\tdef train_g(model, input, amp_enable=False, warmup=False):\n\t    # Placeholders used for capture\n\t    static_input = torch.randn_like(input, device='cuda')\n", "    g = torch.cuda.CUDAGraph()\n\t    with torch.cuda.graph(g):\n\t        with autocast(enabled=amp_enable):\n\t            out=model(static_input)\n\t        out.sum().backward()\n\t    static_input.copy_(input)\n\t    torch.cuda.synchronize()\n\t    time_start=time.time()\n\t    for i in range(0, 5):\n\t        g.replay()\n", "    torch.cuda.synchronize()\n\t    if not warmup:\n\t        print(f'time used: {time.time()-time_start}')\n\tdef train_g_2(model, input, amp_enable=False, warmup=False):\n\t    torch.cuda.synchronize()\n\t    model_g = torch.cuda.make_graphed_callables(model, (input,))\n\t    time_start=time.time()\n\t    for i in range(0, 5):\n\t        with autocast(enabled=amp_enable):\n\t            out=model_g(input)\n", "        out.sum().backward()\n\t    torch.cuda.synchronize()\n\t    print(f'time used: {time.time()-time_start}')\n\tdef run_train_funcs(train, model, input):\n\t    # warmup, ignore\n\t    train(model, input, warmup=True)\n\t    # torch.backends.cuda.matmul.allow_tf32=False\n\t    # torch.backends.cudnn.allow_tf32=False\n\t    os.environ['NVIDIA_TF32_OVERRIDE']='0'\n\t    print('----train with fp32----')\n", "    train(model, input)\n\t    print('----train with autocast----')\n\t    train(model, input, amp_enable=True)\n\t    print('----train with tf32----')\n\t    os.environ['NVIDIA_TF32_OVERRIDE']='1'\n\t    torch.backends.cuda.matmul.allow_tf32=True\n\t    torch.backends.cudnn.allow_tf32=True\n\t    train(model, input)\n\tprint(torch.backends.cuda.matmul.allow_tf32)\n\tprint(torch.backends.cudnn.allow_tf32)\n", "def enable_tf32():\n\t    torch.backends.cuda.matmul.allow_tf32 = True\n\t    torch.backends.cudnn.allow_tf32 = True\n\tdef disable_tf32():\n\t    torch.backends.cuda.matmul.allow_tf32 = False\n\t    torch.backends.cudnn.allow_tf32 = True\n\tenable_tf32()\n\tmodel=create_model('vit_large_patch16_224', #vit_large_patch16_384\n\t                   pretrained=False,\n\t                   num_classes=None,\n", "                   drop_rate=0,\n\t                   drop_path_rate=0.3)\n\tmodel.cuda().train()\n\tdef gen_inp(batch=64):\n\t    return torch.rand([batch, 3, 224, 224])\n\t# train(model, gen_inp().cuda())\n\t# train(model, gen_inp().cuda())\n\ttrain_aot(model, gen_inp().cuda())\n\tprint(\"max mem\", torch.cuda.max_memory_allocated()/(1e9))"]}
{"filename": "explore/fx/fx_simple.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom torch import fx\n\tfrom torch.fx import symbolic_trace\n\tclass MyModule(nn.Module):\n\t    def __init__(self):\n\t        super(MyModule, self).__init__()\n\t        self.fc1 = nn.Linear(10, 10)\n\t        self.fc2 = nn.Linear(10, 1)\n\t    def forward(self, input):\n", "        out = self.fc1(input)\n\t        return self.fc2(out)\n\tmodule = MyModule()\n\t# \n\t# Symbolic tracing frontend - captures the semantics of the module\n\tsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\t# symbolic_traced.graph.print_tabular()\n\t# # \n\t# # High-level intermediate representation (IR) - Graph representation\n\t# print(symbolic_traced.graph)\n", "# # \n\t# # Code generation - valid Python code\n\t# print(symbolic_traced.code)\n\tdef transform(m: torch.nn.Module,\n\t              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n\t    graph : fx.Graph = tracer_class().trace(m)\n\t    # FX represents its Graph as an ordered list of\n\t    # nodes, so we can iterate through them.\n\t    for node in graph.nodes:\n\t        print(node.op, node.name, node.target)\n", "        # Checks if we're calling a function (i.e:\n\t        # torch.add)\n\t        if node.op == 'call_function':\n\t            # The target attribute is the function\n\t            # that call_function calls.\n\t            if node.target == torch.add:\n\t                node.target = torch.mul\n\t        elif node.op == 'call_module':\n\t            import pdb;pdb.set_trace()\n\t            print(\"module\")\n", "            # if node.starget =\n\t        elif node.op == 'output':\n\t            import pdb;pdb.set_trace()\n\t            print('output')\n\t    graph.lint() # Does some checks to make sure the\n\t                 # Graph is well-formed.\n\t    return fx.GraphModule(m, graph)\n\ttransform(module, torch.fx.Tracer)"]}
{"filename": "explore/fx/torch_fx_profile.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# modified from:\n\t# https://github.com/jamesr66a/tutorials/blob/f8f67243b50c55939cc6153bbfd6158bba5b780d/intermediate_source/fx_profiling_tutorial.py\n\t\"\"\"\n\t(beta) Building a Simple CPU Performance Profiler with FX\n\t*******************************************************\n\t**Author**: `James Reed <https://github.com/jamesr66a>`_\n\tIn this tutorial, we are going to use FX to do the following:\n\t1) Capture PyTorch Python code in a way that we can inspect and gather\n\t   statistics about the structure and execution of the code\n", "2) Build out a small class that will serve as a simple performance \"profiler\",\n\t   collecting runtime statistics about each part of the model from actual\n\t   runs.\n\t\"\"\"\n\t######################################################################\n\t# For this tutorial, we are going to use the torchvision ResNet18 model\n\t# for demonstration purposes.\n\timport torch\n\timport torch.fx\n\timport torchvision.models as models\n", "rn18 = models.resnet18().cuda()\n\trn18.train()\n\t######################################################################\n\t# Now that we have our model, we want to inspect deeper into its\n\t# performance. That is, for the following invocation, which parts\n\t# of the model are taking the longest?\n\tinput = torch.randn(5, 3, 224, 224).cuda()\n\toutput = rn18(input)\n\t######################################################################\n\t# A common way of answering that question is to go through the program\n", "# source, add code that collects timestamps at various points in the\n\t# program, and compare the difference between those timestamps to see\n\t# how long the regions between the timestamps take.\n\t#\n\t# That technique is certainly applicable to PyTorch code, however it\n\t# would be nicer if we didn't have to copy over model code and edit it,\n\t# especially code we haven't written (like this torchvision model).\n\t# Instead, we are going to use FX to automate this \"instrumentation\"\n\t# process without needing to modify any source.\n\t######################################################################\n", "# First, let's get some imports out of the way (we will be using all\n\t# of these later in the code).\n\timport statistics, tabulate, time\n\tfrom typing import Any, Dict, List\n\tfrom torch.fx import Interpreter\n\t######################################################################\n\t# .. note::\n\t#     ``tabulate`` is an external library that is not a dependency of PyTorch.\n\t#     We will be using it to more easily visualize performance data. Please\n\t#     make sure you've installed it from your favorite Python package source.\n", "######################################################################\n\t# Capturing the Model with Symbolic Tracing\n\t# -----------------------------------------\n\t# Next, we are going to use FX's symbolic tracing mechanism to capture\n\t# the definition of our model in a data structure we can manipulate\n\t# and examine.\n\ttraced_rn18 = torch.fx.symbolic_trace(rn18)\n\tprint(traced_rn18.graph)\n\t######################################################################\n\t# This gives us a Graph representation of the ResNet18 model. A Graph\n", "# consists of a series of Nodes connected to each other. Each Node\n\t# represents a call-site in the Python code (whether to a function,\n\t# a module, or a method) and the edges (represented as ``args`` and ``kwargs``\n\t# on each node) represent the values passed between these call-sites. More\n\t# information about the Graph representation and the rest of FX's APIs ca\n\t# be found at the FX documentation https://pytorch.org/docs/master/fx.html.\n\t######################################################################\n\t# Creating a Profiling Interpreter\n\t# --------------------------------\n\t# Next, we are going to create a class that inherits from ``torch.fx.Interpreter``.\n", "# Though the ``GraphModule`` that ``symbolic_trace`` produces compiles Python code\n\t# that is run when you call a ``GraphModule``, an alternative way to run a\n\t# ``GraphModule`` is by executing each ``Node`` in the ``Graph`` one by one. That is\n\t# the functionality that ``Interpreter`` provides: It interprets the graph node-\n\t# by-node.\n\t#\n\t# By inheriting from ``Interpreter``, we can override various functionality and\n\t# install the profiling behavior we want. The goal is to have an object to which\n\t# we can pass a model, invoke the model 1 or more times, then get statistics about\n\t# how long the model and each part of the model took during those runs.\n", "#\n\t# Let's define our ``ProfilingInterpreter`` class:\n\tclass ProfilingInterpreter(Interpreter):\n\t    def __init__(self, gm):\n\t        # Rather than have the user symbolically trace their model,\n\t        # we're going to do it in the constructor. As a result, the\n\t        # user can pass in any ``Module`` without having to worry about\n\t        # symbolic tracing APIs\n\t        # gm = torch.fx.symbolic_trace(mod)\n\t        super().__init__(gm)\n", "        # We are going to store away two things here:\n\t        #\n\t        # 1. A list of total runtimes for ``mod``. In other words, we are\n\t        #    storing away the time ``mod(...)`` took each time this\n\t        #    interpreter is called.\n\t        self.total_runtime_sec : List[float] = []\n\t        # 2. A map from ``Node`` to a list of times (in seconds) that\n\t        #    node took to run. This can be seen as similar to (1) but\n\t        #    for specific sub-parts of the model.\n\t        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n", "    ######################################################################\n\t    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n\t    # method is the top-level entrypoint for execution of the model. We will\n\t    # want to intercept this so that we can record the total runtime of the\n\t    # model.\n\t    def run(self, *args) -> Any:\n\t        # Record the time we started running the model\n\t        torch.cuda.synchronize()\n\t        t_start = time.time()\n\t        # Run the model by delegating back into Interpreter.run()\n", "        return_val = super().run(*args)\n\t        # Record the time we finished running the model\n\t        torch.cuda.synchronize()\n\t        t_end = time.time()\n\t        # Store the total elapsed time this model execution took in the\n\t        # ProfilingInterpreter\n\t        self.total_runtime_sec.append(t_end - t_start)\n\t        return return_val\n\t    ######################################################################\n\t    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n", "    # time it executes a single node. We will intercept this so that we\n\t    # can measure and record the time taken for each individual call in\n\t    # the model.\n\t    def run_node(self, n : torch.fx.Node) -> Any:\n\t        # Record the time we started running the op\n\t        torch.cuda.synchronize()\n\t        t_start = time.time()\n\t        # Run the op by delegating back into Interpreter.run_node()\n\t        return_val = super().run_node(n)\n\t        # Record the time we finished running the op\n", "        torch.cuda.synchronize()\n\t        t_end = time.time()\n\t        # If we don't have an entry for this node in our runtimes_sec\n\t        # data structure, add one with an empty list value.\n\t        self.runtimes_sec.setdefault(n, [])\n\t        # Record the total elapsed time for this single invocation\n\t        # in the runtimes_sec data structure\n\t        self.runtimes_sec[n].append(t_end - t_start)\n\t        n.__setattr__('time_cost', t_end - t_start)\n\t        return return_val\n", "    ######################################################################\n\t    # Finally, we are going to define a method (one which doesn't override\n\t    # any ``Interpreter`` method) that provides us a nice, organized view of\n\t    # the data we have collected.\n\t    def summary(self, should_sort : bool = False) -> str:\n\t        # Build up a list of summary information for each node\n\t        node_summaries : List[List[Any]] = []\n\t        # Calculate the mean runtime for the whole network. Because the\n\t        # network may have been called multiple times during profiling,\n\t        # we need to summarize the runtimes. We choose to use the\n", "        # arithmetic mean for this.\n\t        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\t        # For each node, record summary statistics\n\t        for node, runtimes in self.runtimes_sec.items():\n\t            # Similarly, compute the mean runtime for ``node``\n\t            mean_runtime = statistics.mean(runtimes)\n\t            # For easier understanding, we also compute the percentage\n\t            # time each node took with respect to the whole network.\n\t            pct_total = mean_runtime / mean_total_runtime * 100\n\t            # Record the node's type, name of the node, mean runtime, and\n", "            # percent runtim\n\t            node_summaries.append(\n\t                [node.op, str(node), mean_runtime, pct_total])\n\t        # One of the most important questions to answer when doing performance\n\t        # profiling is \"Which op(s) took the longest?\". We can make this easy\n\t        # to see by providing sorting functionality in our summary view\n\t        if should_sort:\n\t            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\t        # Use the ``tabulate`` library to create a well-formatted table\n\t        # presenting our summary information\n", "        headers : List[str] = [\n\t            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n\t        ]\n\t        return tabulate.tabulate(node_summaries, headers=headers)\n\t######################################################################\n\t# .. note::\n\t#       We use Python's ``time.time`` function to pull wall clock\n\t#       timestamps and compare them. This is not the most accurate\n\t#       way to measure performance, and will only give us a first-\n\t#       order approximation. We use this simple technique only for the\n", "#       purpose of demonstration in this tutorial.\n\t######################################################################\n\t# Investigating the Performance of ResNet18\n\t# -----------------------------------------\n\t# We can now use ``ProfilingInterpreter`` to inspect the performance\n\t# characteristics of our ResNet18 model;\n\tinterp = ProfilingInterpreter(traced_rn18)\n\tinterp.run(input)\n\tprint(interp.summary(True))\n\t######################################################################\n", "# There are two things we should call out here:\n\t#\n\t# * MaxPool2d takes up the most time. This is a known issue:\n\t#   https://github.com/pytorch/pytorch/issues/51393\n\t# * BatchNorm2d also takes up significant time. We can continue this\n\t#   line of thinking and optimize this in the Conv-BN Fusion with FX\n\t#   tutorial TODO: link\n\t#\n\t#\n\t# Conclusion\n", "# ----------\n\t# As we can see, using FX we can easily capture PyTorch programs (even\n\t# ones we don't have the source code for!) in a machine-interpretable\n\t# format and use that for analysis, such as the performance analysis\n\t# we've done here. FX opens up an exiciting world of possibilities for\n\t# working with PyTorch programs.\n\t#\n\t# Finally, since FX is still in beta, we would be happy to hear any\n\t# feedback you have about using it. Please feel free to use the\n\t# PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker\n", "# (https://github.com/pytorch/pytorch/issues) to provide any feedback\n\t# you might have.\n\t# import pdb;pdb.set_trace()\n\tfor node in traced_rn18.graph.nodes:\n\t    # import pdb;pdb.set_trace()\n\t    print(node.op, node.name, node.target, node.time_cost)"]}
{"filename": "explore/fx/colo_fx.py", "chunked_list": ["import torch\n\tfrom torch.fx import symbolic_trace\n\timport colossalai\n\tfrom colossalai.fx.passes.meta_info_prop import MetaInfoProp\n\tBATCH_SIZE = 2\n\tDIM_IN = 4\n\tDIM_HIDDEN = 16\n\tDIM_OUT = 16\n\tmodel = torch.nn.Sequential(\n\t    torch.nn.Linear(DIM_IN, DIM_HIDDEN),\n", "    torch.nn.Linear(DIM_HIDDEN, DIM_OUT),\n\t    )\n\tinput_sample = torch.rand(BATCH_SIZE, DIM_IN)\n\tgm = symbolic_trace(model)\n\tinterp = MetaInfoProp(gm)\n\tinterp.run(input_sample)\n\tprint(interp.summary(unit='kb'))    # don't panic if some statistics are 0.00 MB\n"]}
{"filename": "explore/fx/fx_graph_split.py", "chunked_list": ["import copy\n\timport torch\n\timport torch.nn as nn\n\tfrom torch import fx\n\tfrom torch.fx import symbolic_trace\n\timport statistics, tabulate, time\n\tfrom typing import Any, Dict, List\n\tfrom torch.fx import Interpreter\n\timport torchvision\n\tfrom torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n", "import torchvision.models as models\n\tclass ProfilingInterpreter(Interpreter):\n\t    def __init__(self, gm):\n\t        super().__init__(gm)\n\t        self.total_runtime_sec : List[float] = []\n\t        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n\t    def run(self, *args) -> Any:\n\t        # Record the time we started running the model\n\t        torch.cuda.synchronize()\n\t        t_start = time.perf_counter()\n", "        # Run the model by delegating back into Interpreter.run()\n\t        return_val = super().run(*args)\n\t        # Record the time we finished running the model\n\t        torch.cuda.synchronize()\n\t        t_end = time.perf_counter()\n\t        # Store the total elapsed time this model execution took in the\n\t        # ProfilingInterpreter\n\t        self.total_runtime_sec.append(t_end - t_start)\n\t        self.module.__setattr__('time_cost', t_end - t_start)\n\t        return return_val\n", "    def run_node(self, n : torch.fx.Node) -> Any:\n\t        # Record the time we started running the op\n\t        torch.cuda.synchronize()\n\t        t_start = time.perf_counter()\n\t        # Run the op by delegating back into Interpreter.run_node()\n\t        return_val = super().run_node(n)\n\t        # Record the time we finished running the op\n\t        torch.cuda.synchronize()\n\t        t_end = time.perf_counter()\n\t        # If we don't have an entry for this node in our runtimes_sec\n", "        # data structure, add one with an empty list value.\n\t        self.runtimes_sec.setdefault(n, [])\n\t        # Record the total elapsed time for this single invocation\n\t        # in the runtimes_sec data structure\n\t        self.runtimes_sec[n].append(t_end - t_start)\n\t        n.__setattr__('time_cost', t_end - t_start)\n\t        return return_val\n\t    ######################################################################\n\t    # Finally, we are going to define a method (one which doesn't override\n\t    # any ``Interpreter`` method) that provides us a nice, organized view of\n", "    # the data we have collected.\n\t    def summary(self, should_sort : bool = False) -> str:\n\t        # Build up a list of summary information for each node\n\t        node_summaries : List[List[Any]] = []\n\t        # Calculate the mean runtime for the whole network. Because the\n\t        # network may have been called multiple times during profiling,\n\t        # we need to summarize the runtimes. We choose to use the\n\t        # arithmetic mean for this.\n\t        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\t        # For each node, record summary statistics\n", "        for node, runtimes in self.runtimes_sec.items():\n\t            # Similarly, compute the mean runtime for ``node``\n\t            mean_runtime = statistics.mean(runtimes)\n\t            # For easier understanding, we also compute the percentage\n\t            # time each node took with respect to the whole network.\n\t            pct_total = mean_runtime / mean_total_runtime * 100\n\t            # Record the node's type, name of the node, mean runtime, and\n\t            # percent runtim\n\t            node_summaries.append(\n\t                [node.op, str(node), mean_runtime, pct_total])\n", "        # One of the most important questions to answer when doing performance\n\t        # profiling is \"Which op(s) took the longest?\". We can make this easy\n\t        # to see by providing sorting functionality in our summary view\n\t        if should_sort:\n\t            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\t        # Use the ``tabulate`` library to create a well-formatted table\n\t        # presenting our summary information\n\t        headers : List[str] = [\n\t            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n\t        ]\n", "        return tabulate.tabulate(node_summaries, headers=headers)\n\trn18 = models.resnet18().cuda()\n\trn18.train()\n\tinput = torch.randn(32, 3, 224, 224).cuda()\n\toutput = rn18(input)\n\ttraced_rn18 = torch.fx.symbolic_trace(rn18)\n\t# print(traced_rn18.graph)\n\tinterp = ProfilingInterpreter(traced_rn18)\n\ttraced_out = interp.run(input)\n\ttraced_out = interp.run(input)\n", "traced_out = interp.run(input)\n\tassert torch.allclose(output, traced_out)\n\t# print(interp.summary(True))\n\t# import pdb;pdb.set_trace()\n\ttotal_time_cost = traced_rn18.time_cost\n\taccum_time =0\n\t# residual\n\tdef split_gm_into2(gm, time_map, time_accum):\n\t    accum_time =0\n\t    first_gm = copy.deepcopy(gm)\n", "    split_node_name = None\n\t    new_output_name = None\n\t    for node in first_gm.graph.nodes:\n\t        accum_time += time_map.get(node.name,0)\n\t        if accum_time>=time_accum and not split_node_name:\n\t            # jump nodes which has multiple inputs\n\t            if len(node.args)>1:\n\t                continue\n\t            else:\n\t                new_output=first_gm.graph.output(node)\n", "                split_node_name = node.name\n\t                new_output_name = new_output.name\n\t        if node.op=='output' and node.name != new_output_name:\n\t            first_gm.graph.erase_node(node)\n\t    first_gm.graph.eliminate_dead_code()\n\t    first_gm.recompile()\n\t    found = False\n\t    sencond_gm = copy.deepcopy(gm)\n\t    not_needed_nodes = []\n\t    for node in sencond_gm.graph.nodes:\n", "        if split_node_name != node.name and not found:\n\t            # sencond_gm.graph.erase_node(node)\n\t            not_needed_nodes.append(node)\n\t        elif split_node_name == node.name:\n\t            found = True\n\t            new_input = sencond_gm.graph.placeholder(split_node_name)\n\t            node.replace_all_uses_with(new_input)\n\t            sencond_gm.graph.erase_node(node)\n\t            break\n\t    sencond_gm.graph.eliminate_dead_code()\n", "    sencond_gm.recompile()\n\t    return first_gm,sencond_gm\n\tdef get_splited_model(gm, partition_id:int, num_partitions:int):\n\t    \"\"\"\n\t        partition_id: from 1, e.g. if num_partitions=3, partition_id={1,2,3}\n\t    \"\"\"\n\t    total_time_cost = gm.time_cost\n\t    time_map = {}\n\t    for node in gm.graph.nodes:\n\t        # import pdb;pdb.set_trace()\n", "        time_map[node.name] = node.time_cost\n\t    gms = split_gm_into2(gm, time_map, total_time_cost/num_partitions)\n\t    return gms[partition_id]\n\tgm=get_splited_model(traced_rn18, 1, 2)\n\tprint(gm.graph)"]}
{"filename": "explore/model_parallel/mlp.py", "chunked_list": ["import torch\n\tfrom torch import nn as nn\n\tfrom torch.nn.parameter import Parameter\n\tfrom tp_utils import get_tp_group, set_tp_group, TpLinear, RowParallelLinear, ColParallelLinear, \\\n\t    gather_from_sequence_parallel_region\n\tclass Mlp(nn.Module):\n\t    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\t        From timm, but modified to run with tensor parallel and sequence parallel.\n\t    \"\"\"\n\t    def __init__(\n", "            self,\n\t            in_features,\n\t            hidden_features=None,\n\t            out_features=None,\n\t            act_layer=nn.GELU,\n\t            tp_group = None,\n\t            bias=True,\n\t            drop=0.,\n\t    ):\n\t        super().__init__()\n", "        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        bias = bias\n\t        set_tp_group(tp_group)\n\t        self.fc1 = TpLinear(in_features, hidden_features, bias=bias)\n\t        self.act = act_layer()\n\t        self.fc2 = TpLinear(hidden_features, out_features, bias=bias)\n\t        self.drop2 = nn.Dropout(drop)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n", "        x = self.act(x)\n\t        x = self.fc2(x)\n\t        x = self.drop2(x)\n\t        return x\n\tclass TpMlp(nn.Module):\n\t    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\t        From timm, but modified to run with tensor parallel and sequence parallel.\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n", "            in_features,\n\t            hidden_features=None,\n\t            out_features=None,\n\t            act_layer=nn.GELU,\n\t            tp_group = None,\n\t            bias=True,\n\t            drop=0.,\n\t            sequence_parallel=True\n\t    ):\n\t        super().__init__()\n", "        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        bias = bias\n\t        self.sequence_parallel = sequence_parallel\n\t        set_tp_group(tp_group)\n\t        self.fc1 = ColParallelLinear(in_features, hidden_features, bias=bias)\n\t        self.act = act_layer()\n\t        self.fc2 = RowParallelLinear(hidden_features, out_features, bias=bias, sequence_parallel=sequence_parallel)\n\t        self.drop2 = nn.Dropout(drop)\n\t    def forward(self, x):\n", "        if self.sequence_parallel:\n\t            # assume input tensor is sequence parallel\n\t            x = gather_from_sequence_parallel_region(x)\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.fc2(x)\n\t        x = self.drop2(x)\n\t        return x"]}
{"filename": "explore/moe/ds_fmoe_main.py", "chunked_list": ["# Use this version of deepspeed:\n\t# https://github.com/KimmiShi/DeepSpeed/tree/fmoe_v0.9.0\n\t# Use this version of fastmoe:\n\t# https://github.com/KimmiShi/fastmoe\n\tfrom timm.models import create_model\n\timport deepspeed\n\tdef main():\n\t    model = create_model(...)\n\t    args.ep = 4\n\t    # replace vit.mlp with MoE layer\n", "    from deepspeed.utils import groups\n\t    from deepspeed.comm.comm import init_distributed\n\t    init_distributed(dist_backend='nccl')\n\t    groups._create_expert_and_data_parallel(args.ep)\n\t    for block in model.encoder.blocks:\n\t        original_mlp = block.mlp\n\t        feat_in = original_mlp.fc1.weight.shape[1]\n\t        block.mlp = deepspeed.moe.layer.MoE(hidden_size=feat_in, expert=block.mlp, num_experts=args.ep,\n\t                                            ep_size=args.ep, use_fmoe=True)\n\t    def create_moe_param_groups(model):\n", "        from deepspeed.moe.utils import split_params_into_different_moe_groups_for_optimizer\n\t        parameters = {'params': [p for p in model.parameters()], 'name': 'parameters'}\n\t        return split_params_into_different_moe_groups_for_optimizer(parameters)\n\t    parameters = create_moe_param_groups(model)\n\t    optimizer = create_optimizer_v2(parameters, **optimizer_kwargs(cfg=args))\n\t    model, optimizer, _, _ = deepspeed.initialize(args=args,\n\t                                                    model=model,\n\t                                                    optimizer=optimizer)\n"]}
{"filename": "examples/test_ddp.py", "chunked_list": ["import copy\n\timport torch\n\timport torch.nn as nn\n\timport torch.distributed as dist\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\timport timm\n\tfrom torchdistpackage import setup_distributed_slurm, NaiveDDP, fix_rand\n\tclass MyModule(nn.Module):\n\t    def __init__(self):\n\t        super(MyModule, self).__init__()\n", "        self.fc1 = nn.Linear(10, 10)\n\t        self.fc2 = nn.Linear(10, 1)\n\t    def forward(self, input):\n\t        out = self.fc1(input)\n\t        return self.fc2(out)\n\tsetup_distributed_slurm()\n\trank = dist.get_rank()\n\tdef test_ddp(model, input_shape):\n\t    model = model.cuda()\n\t    model2 = copy.deepcopy(model)\n", "    my_ddp_model = NaiveDDP(model, sync=False, gradient_as_bucket_view=True)\n\t    torch_ddp_model = DDP(model2, broadcast_buffers=False)\n\t    optim = torch.optim.Adam(my_ddp_model.parameters(), lr=0.00015)\n\t    optim2 = torch.optim.Adam(torch_ddp_model.parameters(), lr=0.00015)\n\t    for i in range(10):\n\t        # make sure initial param is equal\n\t        for p1, p2 in zip(my_ddp_model.parameters(), torch_ddp_model.parameters()):\n\t            if not torch.allclose(p1, p2):\n\t                import pdb;pdb.set_trace()\n\t                assert False, \"model param not equal\"\n", "        x = torch.rand(input_shape).cuda()  # + rank should make sure inputs are the same)\n\t        optim.zero_grad()\n\t        optim2.zero_grad()\n\t        out = my_ddp_model(x)\n\t        out.sum().backward()\n\t        if hasattr(my_ddp_model, \"reduce_gradients\"):\n\t            my_ddp_model.reduce_gradients()\n\t        out2 = torch_ddp_model(x)\n\t        out2.sum().backward()\n\t        assert torch.allclose(out2, out)\n", "        # compare grads\n\t        for p1, p2 in zip(my_ddp_model.parameters(), torch_ddp_model.parameters()):\n\t            if (p1.grad - p2.grad).sum() > 1e-6:\n\t                if not torch.allclose(p1.grad, p2.grad):\n\t                    import pdb\n\t                    pdb.set_trace()\n\t                    assert False\n\t        optim.step()\n\t        optim2.step()\n\t        if rank == 0:\n", "            print(\"passed round -- \", i)\n\tdef test_simple_module():\n\t    model = MyModule().cuda()\n\t    test_ddp(model, [3, 10])\n\tdef test_timm_resnet():\n\t    model = timm.create_model(\"resnet50\", pretrained=False).cuda()\n\t    test_ddp(model, [4, 3, 224, 224])\n\tdef test_timm_vit():\n\t    model = timm.create_model(\"vit_large_patch16_224_in21k\", pretrained=False).cuda()\n\t    test_ddp(model, [4, 3, 224, 224])\n", "fix_rand()\n\ttest_simple_module()\n\ttest_timm_resnet()\n\t# test_timm_vit()\n"]}
{"filename": "examples/model_parallel/test_attn.py", "chunked_list": ["import torch\n\timport torch.distributed as dist\n\tfrom torchdistpackage.parallel import Attention,TpAttention\n\tfrom torchdistpackage import fix_rand, setup_distributed_slurm\n\tsetup_distributed_slurm()\n\tfix_rand()\n\tdef test_attn(nh=8, in_dim=1024, drop=0., seq_len=128):\n\t    attn = Attention(in_dim, num_heads=nh, attn_drop=drop, proj_drop=drop).cuda()\n\t    opt = torch.optim.AdamW(attn.parameters())\n\t    tp_attn = TpAttention(in_dim, num_heads=nh, attn_drop=drop, proj_drop=drop).cuda()\n", "    opt_tp = torch.optim.AdamW(tp_attn.parameters())\n\t    tp_attn.qkv.init_weight_from_full_attn(attn.qkv.weight)\n\t    tp_attn.proj.init_weight_from_full(attn.proj.weight)\n\t    for _ in range(2):\n\t        inp = torch.rand((32, seq_len, in_dim)).cuda()\n\t        opt_tp.zero_grad()\n\t        opt.zero_grad()\n\t        out = attn(inp)\n\t        tp_out = tp_attn(inp)\n\t        assert torch.allclose(out, tp_out)\n", "        print(\"fwd passed\")\n\t        out.mean().backward()\n\t        tp_out.mean().backward()\n\t        grad_out_buffer_2 = [torch.empty_like(tp_attn.proj.linear.weight.grad) for _ in range(dist.get_world_size())]\n\t        dist.all_gather(grad_out_buffer_2, tp_attn.proj.linear.weight.grad)\n\t        fc2_grad_full = torch.cat(grad_out_buffer_2, dim=0)\n\t        if not torch.allclose(attn.proj.weight.grad, fc2_grad_full):\n\t            import pdb;pdb.set_trace()\n\t        print(\"bwd passed\")\n\t        opt.step()\n", "        opt_tp.step()\n\ttest_attn(nh=8, in_dim=1024, seq_len=128)"]}
{"filename": "examples/model_parallel/test_tpmlp.py", "chunked_list": ["import torch\n\timport torch.distributed as dist\n\tfrom torchdistpackage import setup_distributed_slurm\n\tfrom torchdistpackage import fix_rand\n\tfrom torchdistpackage.parallel import TpMlp, Mlp\n\tdef test_mlp(dim1=1024, in_feat=2048, outfeat=2048, hiddenfeat=8192):\n\t    input = torch.rand((dim1, in_feat)).cuda()\n\t    dist.broadcast(input, 0)\n\t    mlp = Mlp(in_feat, hiddenfeat, outfeat).cuda()\n\t    tp_mlp = TpMlp(in_feat, hiddenfeat, outfeat).cuda()\n", "    tp_mlp.fc2.init_weight_from_full(mlp.fc2.weight)\n\t    tp_mlp.fc1.init_weight_from_full(mlp.fc1.weight)\n\t    mlp_out = mlp(input)\n\t    tp_mlp_out = tp_mlp(input)\n\t    assert torch.allclose(mlp_out, tp_mlp_out)\n\t    print(\"fwd passed\")\n\t    mlp_out.mean().backward()\n\t    tp_mlp_out.mean().backward()\n\t    grad_out_buffer_2 = [torch.empty_like(tp_mlp.fc2.linear.weight.grad) for _ in range(dist.get_world_size())]\n\t    dist.all_gather(grad_out_buffer_2, tp_mlp.fc2.linear.weight.grad)\n", "    fc2_grad_full = torch.cat(grad_out_buffer_2, dim=0)\n\t    assert torch.allclose(mlp.fc2.weight.grad, fc2_grad_full)\n\t    grad_out_buffer_1 = [torch.empty_like(tp_mlp.fc1.linear.weight.grad) for _ in range(dist.get_world_size())]\n\t    dist.all_gather(grad_out_buffer_1, tp_mlp.fc1.linear.weight.grad)\n\t    fc1_grad_full = torch.cat(grad_out_buffer_1, dim=1)\n\t    assert torch.allclose(mlp.fc1.weight.grad, fc1_grad_full, rtol=1e-04, atol=1e-04)\n\t    print(\"bwd passed\")\n\tif __name__=='__main__':\n\t    fix_rand()\n\t    setup_distributed_slurm()\n", "    test_mlp()\n"]}
{"filename": "examples/model_parallel/test_pipeline.py", "chunked_list": ["import os\n\timport torch\n\timport torch.nn as nn\n\tfrom typing import Tuple\n\timport torch\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom torch.utils.data.distributed import DistributedSampler\n\tfrom torchdistpackage import tpc, setup_distributed_slurm, test_comm\n\tfrom torchdistpackage.parallel import partition_uniform, forward_backward\n\tclass DummyClsDataset(Dataset):\n", "    def __init__(self, shape, num_samples=1000):\n\t        self.num_samples = num_samples\n\t        self.shape = shape\n\t    def __len__(self):\n\t        return self.num_samples\n\t    def __getitem__(self, idx):\n\t        del idx\n\t        img = torch.randn(self.shape)\n\t        label = torch.randint(0, 10, (1,)).squeeze()\n\t        return img, label\n", "def get_dataloaders(\n\t    batch_size, batch_shape, train_samples, test_samples\n\t) -> Tuple[DataLoader, DataLoader]:\n\t    \"\"\" get dataloaders \"\"\"\n\t    train_data = DummyClsDataset(batch_shape, train_samples)\n\t    sampler = DistributedSampler(train_data, shuffle=True)\n\t    train_dataloader = DataLoader(\n\t        dataset=train_data,\n\t        batch_size=batch_size,\n\t        num_workers=0,\n", "        pin_memory=True,\n\t        sampler=sampler,\n\t    )\n\t    test_dataloader = DataLoader(\n\t        DummyClsDataset(batch_shape, test_samples), batch_size=batch_size, shuffle=True\n\t    )\n\t    return train_dataloader, test_dataloader\n\tdef build_seq_model():\n\t    model = nn.Sequential(\n\t        nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10), nn.ReLU(), nn.ReLU()\n", "    )\n\t    return model\n\t# Define some config\n\tBATCH_SIZE = 512\n\tNUM_EPOCHS = 2\n\tNUM_CHUNKS = 2\n\tNUM_MICRO_BATCHES = 4\n\tsetup_distributed_slurm()\n\tworld_size = int(os.environ[\"SLURM_NTASKS\"])\n\tpp_size = 2\n", "dist_config = [(\"pipe\", pp_size), (\"data\", world_size / (pp_size))]\n\ttpc.setup_process_groups(dist_config)\n\tmodel = build_seq_model().cuda()\n\tmodel = nn.Sequential(*partition_uniform(model)).cuda()\n\t# optimizer\n\toptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\t# build dataloader\n\troot = os.environ.get(\"DATA\", \"./data\")\n\ttrain_dataloader, test_dataloader = get_dataloaders(BATCH_SIZE, [10], 4000, 100)\n\ttest_dataloader = None\n", "def pp_fwd_fn(ins):\n\t    input_img = ins\n\t    img_feat = model(input_img)\n\t    if tpc.is_last_in_pipeline_group():\n\t        loss = img_feat.mean()\n\t        return loss\n\t    return img_feat\n\tdef local_forward_backward(inp, model):\n\t    inputs = []\n\t    if tpc.is_first_in_pipeline_group():\n", "        inputs.append(inp)\n\t    forward_backward(\n\t        optimizer,\n\t        pp_fwd_fn,\n\t        None,\n\t        inputs,\n\t        num_microbatches=NUM_MICRO_BATCHES,\n\t        forward_only=False,\n\t        dtype=torch.float32,\n\t        scatter_gather_tensors=False,\n", "    )\n\tfor epoch in range(NUM_EPOCHS):\n\t    for img, label in train_dataloader:\n\t        img = img.cuda()\n\t        label = label.cuda()\n\t        local_forward_backward(img, model)\n\t        optimizer.step()\n\t        print(\"-------------\")\n"]}
{"filename": "examples/model_parallel/test_transformer.py", "chunked_list": ["import torch\n\timport torch.distributed as dist\n\tfrom torchdistpackage.parallel.tensor_parallel.transformer import Transformer\n\tfrom torchdistpackage import fix_rand, setup_distributed_slurm\n\tsetup_distributed_slurm()\n\tfix_rand()\n\tdef test_model(dim, depth, nh=2, dtype=torch.float):\n\t    model = Transformer(dim, depth=depth, num_heads=nh, tensor_parallel=False, sequence_parallel=False).cuda().to(dtype)\n\t    # tp_model = Transformer(dim, depth=depth, num_heads=nh, tensor_parallel=True, sequence_parallel=False).cuda().to(dtype)\n\t    tp_model = Transformer(dim, depth=depth, num_heads=nh, tensor_parallel=True, sequence_parallel=True).cuda().to(dtype)\n", "    opt = torch.optim.AdamW(model.parameters())\n\t    tp_opt = torch.optim.AdamW(tp_model.parameters())\n\t    for ind in range(len(model.blocks)):\n\t        tp_model.blocks[ind].init_from_full(model.blocks[ind])\n\t        # sp_model.blocks[ind].init_from_full(model.blocks[ind])\n\t    for _ in range(10):\n\t        inp = torch.rand((32, 1024, dim)).cuda().to(dtype)\n\t        opt.zero_grad()\n\t        out = model(inp)\n\t        tp_out = tp_model(inp)\n", "        assert torch.allclose(out, tp_out, rtol=1e-1, atol=1e-02)\n\t        import pdb;pdb.set_trace()\n\t        # TODO: fix this mis alignment\n\t        assert torch.allclose(out, tp_out, rtol=1e-2, atol=1e-02)\n\t        assert torch.allclose(out, tp_out, rtol=1e-04, atol=1e-04)\n\t        assert torch.allclose(out, tp_out, rtol=1e-05, atol=1e-05)\n\t        out.mean().backward()\n\t        tp_out.mean().backward()\n\t        opt.step()\n\t        tp_opt.step()\n", "test_model(1024, 8)"]}
