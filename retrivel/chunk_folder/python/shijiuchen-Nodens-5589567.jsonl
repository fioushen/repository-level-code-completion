{"filename": "testRes.py", "chunked_list": ["import os\n\timport time\n\t#To import Grpc APIs\n\tfrom client.client import rpc_adjust_res\n\t#Node-Ip relationship\n\tNodeIpMapper={\n\t    \"cpu-03\":\"10.2.64.3:50052\",\n\t    \"cpu-04\":\"10.2.64.4:50052\",\n\t    \"cpu-07\":\"10.2.64.7:50052\",\n\t    \"cpu-08\":\"10.2.64.8:50052\",\n", "}\n\t#pod-node relationship\n\tpod_node_mapper={'check': 'cpu-04', 'consul': 'cpu-06', 'entering-ms': 'cpu-03', 'frontend-recommend': 'cpu-07', 'frontend-reserve': 'cpu-08', 'frontend-search': 'cpu-04', 'geo': 'cpu-04', 'jaeger': 'cpu-06', 'memcached-check': 'cpu-04', 'memcached-profile': 'cpu-07', 'memcached-rate': 'cpu-04', 'memcached-reservation': 'cpu-08', 'mongodb-check': 'cpu-04', 'mongodb-geo': 'cpu-04', 'mongodb-history': 'cpu-07', 'mongodb-profile': 'cpu-07', 'mongodb-rate': 'cpu-04', 'mongodb-recommendation': 'cpu-07', 'mongodb-reservation': 'cpu-08', 'mongodb-user': 'cpu-08', 'profile': 'cpu-07', 'rank-category': 'cpu-07', 'rank-overall': 'cpu-07', 'rate': 'cpu-04', 'recommendation': 'cpu-07', 'reservation': 'cpu-08', 'search': 'cpu-04', 'user': 'cpu-08'}\n\t#pod-uid relationship\n\tpod_uids={\n\t    \"entering-ms\":['c95afad8-75ea-4cf6-b0a4-07982fad4cf7\\n'],\n\t    \"frontend-search\":['07b00721-06e6-47a6-abae-c6b6cb6512e0\\n'],\n\t    \"frontend-recommend\":['249ce56f-f436-411d-841a-1430135f8cd9\\n'],\n\t    \"frontend-reserve\":['1a545e65-91a4-4f99-96be-993366e7924d\\n'],\n\t    \"search\": ['41be1c59-1531-478f-8e4b-74b826e00c20\\n'],\n", "    \"check\": ['c2c39aef-516c-469d-b31c-c0e9193cea39\\n'],\n\t    \"recommendation\": ['75b476d8-759a-42ec-b02d-9f79ec0c3e99\\n'],\n\t    \"profile\": ['2fdee85b-2f09-4c96-922b-2f22800c1f53\\n'],\n\t    \"user\": ['153e0a9a-8cd5-439d-96e2-a21db4985b44\\n'],\n\t    \"reservation\": ['12ae4870-ceca-46c5-a8de-74992794251f\\n'],\n\t    \"geo\": ['80a37e50-bfbb-4a13-84e7-79a3a3e61879\\n'],\n\t    \"rate\":['1477e6df-edce-435f-8985-5a9d24359693\\n'],\n\t    \"memcached-check\":['bf8c4d95-e879-4116-82e4-75f0aed3469f\\n'],\n\t    \"rank-category\": ['4a0e4a06-74bd-485b-9108-cda1b0bb2cdb\\n'],\n\t    \"rank-overall\": ['604e15ce-e6b7-45d6-8347-9f51954db5cf\\n'],\n", "    \"memcached-profile\": ['5cb477da-db24-4cee-97eb-c0668a6ef455\\n'],\n\t    \"memcached-reservation\": ['b295189c-b513-4ad5-99b6-ff8e6dd4ff09\\n'],\n\t    \"mongodb-reservation\":['67273aa4-7f56-45fc-a721-04ac453833bb\\n'],\n\t    \"memcached-rate\":['a2630e43-dd25-47f6-b14c-8ac73ba386b1\\n']\n\t}\n\t#get pod-node relationship\n\tdef pod_node():\n\t    pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n\t    nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n\t    pod_node_mapper={}\n", "    for i in range(len(pods)):\n\t        pod=pods[i].replace(\"\\n\",\"\")\n\t        node=nodes[i].replace(\"\\n\",\"\")\n\t        if(pod==\"NAME\"):\n\t            continue\n\t        temp=pod.split(\"-\")\n\t        extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n\t        pod_name=pod.replace(extra,\"\")\n\t        pod_node_mapper[pod_name]=node\n\t    print(pod_node_mapper)\n", "#get pod-uid relationship\n\tdef get_pod_uids():\n\t    for svc in pod_uids.keys():\n\t        service=str(svc)\n\t        os.system(\"./get_pod_uid.sh \"+service+\" >/dev/null 2>&1\")\n\t        uids=os.popen(\"cat uid.txt\").readlines()\n\t        print(svc, uids)\n\t#Call Grpc APIs to adjust resources\n\tdef set_cpu(service,cpu,replicas_number=1):\n\t    node=pod_node_mapper[service]\n", "    ip_str=NodeIpMapper[node]\n\t    uids=pod_uids[service]\n\t    res=rpc_adjust_res(ip_str,uids,cpu)\n\t#Adjust the initial state using profiling on own cluster\n\tdef set_QoS_violations():\n\t    set_cpu(\"entering-ms\",24,1)\n\t    set_cpu(\"frontend-search\",9.1,1)\n\t    set_cpu(\"frontend-recommend\",10.5,1)\n\t    set_cpu(\"frontend-reserve\",5.1,1)\n\t    set_cpu(\"search\",6.5,1)\n", "    set_cpu(\"check\",10,1)\n\t    set_cpu(\"recommendation\",6,1)\n\t    set_cpu(\"profile\",6,1)\n\t    set_cpu(\"user\",1.6,1)\n\t    set_cpu(\"reservation\",3.5,1)\n\t    set_cpu(\"geo\",2.5,1)\n\t    set_cpu(\"rate\",4.2,1)\n\t    set_cpu(\"memcached-check\",4,1)\n\t    set_cpu(\"rank-category\",2,1)\n\t    set_cpu(\"rank-overall\",3,1)\n", "    set_cpu(\"memcached-profile\",1.5,1)\n\t    set_cpu(\"memcached-rate\",1.2,1)\n\t    set_cpu(\"memcached-reservation\",1.5,1)\n\t#Adjust the enough resources for the  on own cluster\n\tdef set_enough():\n\t    set_cpu(\"entering-ms\",48,1)\n\t    set_cpu(\"frontend-search\",19,1)\n\t    set_cpu(\"frontend-recommend\",21,1)\n\t    set_cpu(\"frontend-reserve\",11,1)\n\t    set_cpu(\"search\",13.2,1)\n", "    set_cpu(\"check\",20,1)\n\t    set_cpu(\"recommendation\",13.2,1)\n\t    set_cpu(\"profile\",13.5,1)\n\t    set_cpu(\"user\",3.5,1)\n\t    set_cpu(\"reservation\",9,1)\n\t    set_cpu(\"geo\",5,1)\n\t    set_cpu(\"rate\",8.8,1)\n\t    set_cpu(\"memcached-check\",6.6,1)\n\t    set_cpu(\"rank-category\",4,1)\n\t    set_cpu(\"rank-overall\",6,1)\n", "    set_cpu(\"memcached-profile\",3,1)\n\t    set_cpu(\"memcached-rate\",2.4,1)\n\t    set_cpu(\"memcached-reservation\",2,1)\n\tif __name__ == \"__main__\":\n\t    pod_node()\n\t    get_pod_uids()\n\t    set_enough()\n\t    set_QoS_violations()\n\t    print(time.time())"]}
{"filename": "Agent.py", "chunked_list": ["import os\n\timport scipy\n\timport pandas\n\timport numpy as np\n\timport time\n\timport sys\n\tfrom MSDAG import *\n\tfrom LoadMonitor.NetTrafficMonitor import initialize,runOneTime\n\tfrom LoadUpdator.Update import Update_Traffic_Tree,Update_Traffic_Graph\n\tfrom Predict.Predictor import predict_net_to_load, predict_load_to_CPU\n", "from QueryDrainer.AdjustRes import run_set_cpu\n\tfrom QueryDrainer.Compensator import Compensate\n\tfrom testRes import set_QoS_violations, set_enough\n\tfrom multiprocessing import Process\n\tif __name__ == \"__main__\":\n\t    # set enough resources\n\t    set_enough()\n\t    #Traffic monitoritor interval & overhead\n\t    time_interval=1.05\n\t    #Initial DAG\n", "    root=Node(0,\"test\",0,0,0,0,[],[],0,0,0,0,0,0,0)\n\t    ms_dag=MSDAG(root,0, np.zeros((1,1)),np.zeros((1,1)),[],{})\n\t    ms_dag.buildDAG()\n\t    ms_dag.postOrder(ms_dag.root)\n\t    ms_dag.ms_interface()\n\t    ms_dag.display()\n\t    # Run 5 seconds normally\n\t    print(\"begin_time=\",time.time())\n\t    time.sleep(5)\n\t    # Adjust to the resources with initial resouces\n", "    time_monitor_start=time.time()\n\t    print(\"QoS_violaiton_time=\",time_monitor_start)\n\t    set_QoS_violations()\n\t    #Initial monitor\n\t    initialize(ms_dag)\n\t    # Monitor 1 seconds\n\t    time.sleep(1)\n\t    # 1.Get network traffic\n\t    sampling_duration=runOneTime(ms_dag)\n\t    # 2.Get monitor load\n", "    predict_net_to_load(ms_dag)\n\t    ms_dag.root.realLoad=ms_dag.root.monitorLoad\n\t    # 3.Update real load\n\t    Update_Traffic_Graph(ms_dag)\n\t    # 4.Queued query drain\n\t    Compensate(ms_dag,time_interval)\n\t    # 5.Get CPU need\n\t    predict_load_to_CPU(ms_dag)\n\t    # 6.CPU allocation\n\t    run_set_cpu(ms_dag)\n", "    time_set_done=time.time()\n\t    print(\"Adjustment done=\",time_set_done)\n\t    # Run extra time, then set enough\n\t    time.sleep(3-(time.time()-time_monitor_start))\n\t    set_enough()"]}
{"filename": "MSDAG.py", "chunked_list": ["import os\n\timport numpy as np\n\tfrom queue import Queue\n\t#Node refers to each MS\n\tclass Node:\n\t    def __init__(self,index,name,in_traffic,out_traffic,upper_traffic,back_traffic,children,pressure_children,realLoad,monitorLoad,handleLoad,CPU_Allocated,CPU_Need,overLoad,SLAtime):\n\t        self.index=index\n\t        self.name=name\n\t        self.in_traffic=in_traffic\n\t        self.out_traffic=out_traffic\n", "        self.upper_traffic=upper_traffic\n\t        self.back_traffic=back_traffic\n\t        self.children=children\n\t        self.pressure_children=pressure_children\n\t        self.realLoad=realLoad\n\t        self.monitorLoad=monitorLoad\n\t        self.handleLoad=handleLoad\n\t        self.CPU_Allocated=CPU_Allocated\n\t        self.CPU_Need=CPU_Need\n\t        self.overLoad=overLoad\n", "        self.SLAtime=SLAtime\n\t    def display(self):\n\t        print(self.index,self.name,self.in_traffic,self.out_traffic,self.upper_traffic,self.back_traffic,self.children,self.pressure_children,self.realLoad,self.monitorLoad,self.handleLoad,self.CPU_Allocated,self.CPU_Need,self.overLoad,self.SLAtime)\n\t#DAG for all MSs\n\tclass MSDAG:\n\t    def __init__(self, root, MS_number, Traffic_matrix, Load_matrix, Access_Record, ms_interface_mapper, PodNodeMapper={},NodeIpMapper={}):\n\t        self.root=root\n\t        self.MS_number=MS_number\n\t        self.Traffic_matrix=Traffic_matrix\n\t        self.Load_matrix=Load_matrix\n", "        self.Access_Record=Access_Record\n\t        self.ms_interface_mapper=ms_interface_mapper\n\t        self.PodNodeMapper=PodNodeMapper\n\t        self.NodeIpMapper=NodeIpMapper\n\t    #Initial microservices\n\t    def buildDAG(self):\n\t        checkmmc=Node(12,\"memcached-check\",0,0,0,0,[],[],0,0,1000,0,0,0,0)\n\t        check=Node(5,\"check\",0,0,0,0,[checkmmc],[checkmmc],0,0,1000,0,0,0,0)\n\t        ratemmc=Node(18,\"memcached-rate\",0,0,0,0,[],[check],0,0,500,0,0,0,0)\n\t        geo=Node(10,\"geo\",0,0,0,0,[],[check],0,0,500,0,0,0,0)\n", "        rate=Node(11,\"rate\",0,0,0,0,[ratemmc],[ratemmc],0,0,500,0,0,0,0)\n\t        search=Node(4,\"search\",0,0,0,0,[geo,rate],[rate,geo],0,0,1000,0,0,0,0)\n\t        frontendSearch=Node(1,\"frontend-search\",0,0,0,0,[search,check],[search],0,0,1000,0,0,0,0)\n\t        profilemmc=Node(15,\"memcached-profile\",0,0,0,0,[],[],0,0,1000,0,0,0,0)\n\t        profile=Node(7,\"profile\",0,0,0,0,[profilemmc],[profilemmc],0,0,1000,0,0,0,0)\n\t        rank_overall=Node(14,\"rank-overall\",0,0,0,0,[],[profile],0,0,500,0,0,0,0)\n\t        rank_category=Node(13,\"rank-category\",0,0,0,0,[],[profile],0,0,500,0,0,0,0)\n\t        recommend=Node(6,\"recommendation\",0,0,0,0,[rank_category,rank_overall],[rank_category,rank_overall],0,0,1000,0,0,0,0)\n\t        frontendRecommend=Node(2,\"frontend-recommend\",0,0,0,0,[recommend,profile],[recommend],0,0,1000,0,0,0,0)\n\t        reservemongodb=Node(17,\"mongodb-reservation\",0,0,0,0,[],[],0,0,500,0,0,0,0)\n", "        reservemmc=Node(16,\"memcached-reservation\",0,0,0,0,[],[],0,0,500,0,0,0,0)\n\t        reserve=Node(9,\"reservation\",0,0,0,0,[reservemmc,reservemongodb],[reservemmc,reservemongodb],0,0,500,0,0,0,0)\n\t        user=Node(8,\"user\",0,0,0,0,[],[reserve],0,0,500,0,0,0,0)\n\t        frontendReserve=Node(3,\"frontend-reserve\",0,0,0,0,[user,reserve],[user],0,0,500,0,0,0,0)\n\t        EnteringMS=Node(0,\"entering-ms\",0,0,0,0,[frontendSearch,frontendRecommend,frontendReserve],[frontendSearch,frontendRecommend,frontendReserve],3675.0,3675.0,2500,0,0,0,0)\n\t        self.root=EnteringMS\n\t        #Traffic matrix\n\t        self.MS_number=19\n\t        self.Traffic_matrix=np.zeros((self.MS_number,self.MS_number))\n\t        #Load matrix\n", "        self.Load_matrix=np.zeros((self.MS_number,self.MS_number))\n\t        #Node-IP relationship\n\t        self.NodeIpMapper={\n\t            \"cpu-03\":\"10.2.64.3:50052\",\n\t            \"cpu-04\":\"10.2.64.4:50052\",\n\t            \"cpu-07\":\"10.2.64.7:50052\",\n\t            \"cpu-08\":\"10.2.64.8:50052\",\n\t        }\n\t    #Post order microservices\n\t    def postOrder(self,root):\n", "        if not root:\n\t            return\n\t        for target in root.children:\n\t            self.postOrder(target)\n\t        self.Access_Record.append(root)\n\t    #pod-interface relationship\n\t    def ms_interface(self):\n\t        pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n\t        nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n\t        for i in range(len(pods)):\n", "            pod=pods[i].replace(\"\\n\",\"\")\n\t            node=nodes[i].replace(\"\\n\",\"\")\n\t            if(pod==\"NAME\"):\n\t                continue\n\t            id=os.popen(\"kubectl exec -i \"+pod+\" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n\t            print(pod,id)\n\t            idtarget=\"^'\"+id+\":\"+\" \"+\"'\"\n\t            if(node==\"cpu-06\"):\n\t                temp=os.popen(\"ip link show | grep \"+idtarget+\" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n\t            else:\n", "                temp=os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget+\"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n\t            index=temp.find('@')\n\t            iplink=temp[:index]\n\t            temp=pod.split(\"-\")\n\t            extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n\t            pod_name=pod.replace(extra,\"\")\n\t            self.ms_interface_mapper[iplink]=pod_name\n\t            self.PodNodeMapper[pod_name]=node\n\t    #display all\n\t    def display(self):\n", "        print(\"==========DAG Root:==========\")\n\t        self.root.display()\n\t        print(\"==========Microservice Number:==========\")\n\t        print(self.MS_number)\n\t        print(\"==========Load_matrix:==========\")\n\t        for i in range(len(self.Load_matrix)):\n\t            print(i,list(self.Load_matrix[i]))\n\t        print(\"==========All Nodes(post order):==========\")\n\t        for target in self.Access_Record:\n\t            target.display()\n", "        print(\"==========Pod<->Interfaces==========\")\n\t        for key in self.ms_interface_mapper:\n\t            print(key,self.ms_interface_mapper[key])\n\t        print(\"==========Pod<->Node:==========\")\n\t        for key in self.PodNodeMapper:\n\t            print(key,self.PodNodeMapper[key])\n\tif __name__ == \"__main__\":\n\t    root=Node(0,\"test\",0,0,0,0,[],[],0,0,0,0,0,0,0)\n\t    ms_dag=MSDAG(root,0, np.zeros((1,1)),np.zeros((1,1)),[],{})\n\t    ms_dag.buildDAG()\n", "    ms_dag.postOrder(ms_dag.root)\n\t    ms_dag.ms_interface()\n\t    ms_dag.display()"]}
{"filename": "getLatencyRes.py", "chunked_list": ["import os\n\timport numpy as np\n\timport time\n\timport requests\n\timport json\n\timport pandas\n\timport datetime\n\tdef get_time_stamp16(inputTime):\n\t    date_stamp = str(int(time.mktime(inputTime.timetuple())))\n\t    data_microsecond = str(\"%06d\"%inputTime.microsecond)\n", "    date_stamp = date_stamp+data_microsecond\n\t    return int(date_stamp)\n\tdef get_latency(startTs,endTs,period):\n\t    data=requests.get(url='http://127.0.0.1:30910/api/traces?service=enteringMS&start='+str(startTs)+'&end='+str(endTs)+'&prettyPrint=true&limit=6000000').json()\n\t    counter=0\n\t    latency = []\n\t    errorCount=0\n\t    for element in data['data']:\n\t        counter+=1\n\t        spanCount=0\n", "        minTs=0.0\n\t        maxTs=0.0\n\t        flag=1\n\t        for a in element['spans']:\n\t            for b in a['tags']:\n\t                if(b['key']==\"error\" and b['value']==True):\n\t                    flag=0\n\t            if(flag==0):\n\t                errorCount+=1\n\t                break\n", "            spanCount+=1\n\t            if(spanCount==1):\n\t                minTs=float(a['startTime'])\n\t                maxTs=float(a['startTime'])+float(a['duration'])\n\t            else:\n\t                if(minTs>float(a['startTime'])):\n\t                    minTs=float(a['startTime'])\n\t                else:\n\t                    minTs=minTs\n\t                if(maxTs<(float(a['startTime'])+float(a['duration']))):\n", "                    maxTs=float(a['startTime'])+float(a['duration'])\n\t                else:\n\t                    maxTs=maxTs\n\t        if(flag==1):\n\t            time=(maxTs-minTs)/1000\n\t            latency.append(time)\n\t    if(counter==0 or len(latency)==0):\n\t        return [0,0,0,0,0,0]\n\t    return [np.mean(latency),np.percentile(latency,50),np.percentile(latency,99),len(latency)/period,errorCount]\n\tdef latency_analyze(startTime,duration):\n", "    t_start=int(startTime*1000000)\n\t    t_end=int((startTime+duration)*1000000)\n\t    interval=100000#every 100ms requests\n\t    index=t_start\n\t    counter=0\n\t    while(index<=t_end):\n\t        res=get_latency(index,index+interval,0.1)\n\t        print(counter,index,index+interval,res[0],res[1],res[2],res[3],res[4])#average,50th,99th,99.9th,throughput,errorCount\n\t        index+=interval\n\t        counter+=1\n", "if __name__ == \"__main__\":\n\t    latency_analyze(1683713739.2187307,35)\n"]}
{"filename": "profile_example.py", "chunked_list": ["import client.ProfileGet as cc\n\timport os,json\n\tfrom collections import defaultdict\n\timport pandas\n\timport time\n\tfrom multiprocessing import Process\n\timport datetime\n\tNodeIpMapper = {\n\t    \"cpu-03\": \"10.2.64.3:50052\",\n\t    \"cpu-04\": \"10.2.64.4:50052\",\n", "    \"cpu-07\": \"10.2.64.7:50052\",\n\t    \"cpu-08\": \"10.2.64.8:50052\",\n\t}\n\tdef execute_load(cmd):\n\t    print(cmd)\n\t    os.system(cmd)\n\t# analyse the string  format  docker stats \n\tdef getComputeRes(res1):\n\t    Res_mapper={}\n\t    res_name,res_CPU,res_MEM=[],[],[]\n", "    for i in range(len(res1)):\n\t        array_temp1=res1[i].split(\" \")\n\t        counter1=0\n\t        for j in range(1,len(array_temp1)):\n\t            if(array_temp1[j]!=''):\n\t                counter1+=1\n\t                if(counter1==1):\n\t                    res_name.append(array_temp1[j])\n\t                elif(counter1==2):\n\t                    res_CPU.append([array_temp1[j]])\n", "                elif(counter1==3):\n\t                    res_MEM.append([array_temp1[j]])\n\t                    break\n\t    for i in range(len(res_name)):\n\t        ms=res_name[i]\n\t        temp=ms.replace(\"\\n\",\"\").split(\"_\")[1].replace(\"ebc1-\",\"\")\n\t        cpu=float(res_CPU[i][0].replace(\"%\",\"\"))\n\t        if(\"GiB\" in res_MEM[i][0].replace(\"\\n\",\"\")):\n\t            memory=float(float(res_MEM[i][0].replace(\"GiB\",\"\"))*1000)\n\t        else:\n", "            memory=float(res_MEM[i][0].replace(\"MiB\",\"\"))\n\t        if(temp not in Res_mapper.keys()):\n\t            Res_mapper[temp]=[cpu,memory]\n\t        else:\n\t            Res_mapper[temp][0]+=cpu\n\t            Res_mapper[temp][1]+=memory\n\t    return Res_mapper\n\t# run loadgenerator\n\tcmd_load = \"python3 LoadGenerator.py -q 2000'\"\n\tp_load=Process(target=execute_load,args=(cmd_load,))\n", "p_load.start()\n\tdt_time2 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t# get profile\n\tcount=0\n\tmax_count=8\n\tlist_profile=[]\n\twhile(count<max_count):\n\t    lines_str = ''\n\t    for k, v in NodeIpMapper.items():\n\t        str_res = cc.getProfile(v)\n", "        lines_str += str_res\n\t    lines_get_proc = lines_str.split('\\n')\n\t    # print(lines_get_proc)\n\t    profile_map = getComputeRes(lines_get_proc)\n\t    list_profile.append(profile_map)\n\t    count+=1\n\t    # time.sleep(3)\n\tdt_time3 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t# compute the mean value \n\tnew_map=defaultdict(list)\n", "key_new=set()\n\tfor profile_map in list_profile:\n\t    for k,v in profile_map.items():\n\t        key_new.add(k)\n\tl_key_new=list(key_new)\n\tfor k in l_key_new:\n\t    new_map[k].append(0.0)\n\t    new_map[k].append(0.0)\n\t# cpu_usage_all=defaultdict(float)\n\t# mem_usage_all=defaultdict(float)\n", "c_all=0\n\tlist_frame=[]\n\tfor profile_map in list_profile:\n\t    for k,v in profile_map.items():\n\t        new_map[k][0]+=v[0]\n\t        new_map[k][1]+=v[1]\n\t        list_frame.append([c_all,k,v[0],v[1]])\n\t    c_all+=1\n\tfor k,v in new_map.items():\n\t    new_map[k][0]=new_map[k][0]/c_all\n", "    new_map[k][1]=new_map[k][1]/c_all\n\tres_profile_json=json.dumps(new_map)\n\twith open('profile_test.json',mode='w',encoding='utf-8') as f:\n\t    f.write(res_profile_json)\n\tdt_time4 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\tprint(\"2 \",dt_time2)\n\tprint(\"3 \",dt_time3)\n\tprint(\"4 \",dt_time4)\n\tprint(\"end...\")\n"]}
{"filename": "LoadUpdator/Update.py", "chunked_list": ["import os\n\timport scipy\n\timport pandas\n\timport numpy as np\n\timport time\n\timport sys\n\timport copy\n\tfrom queue import Queue\n\t#For tree structure, all in-degree is 1\n\tdef Update_Traffic_Tree(ms_dag):\n", "    print(\"Entering in......\")\n\t    BFS_name=list()\n\t    q=Queue()\n\t    q.put(ms_dag.root)\n\t    BFS_name.append(ms_dag.root.name)\n\t    while(not q.empty()):\n\t        target=q.get()\n\t        rate=target.realLoad/(target.handleLoad-target.overLoad)\n\t        if(rate<1):\n\t            rate=1\n", "        print(target.name)\n\t        for child in target.children:\n\t            if(child.name not in BFS_name):\n\t                child.realLoad=(child.monitorLoad-child.overLoad)*rate\n\t                BFS_name.append(child.name)\n\t                q.put(child)\n\t    print(\"done\")\n\t    ms_dag.display()\n\t#For graph structure, some in-degrees are above 1\n\tdef Update_Traffic_Graph(ms_dag):\n", "    print(\"Entering in......\")\n\t    # mark visit times\n\t    BFS_name={\n\t        \"frontend-search\":1,\n\t        \"frontend-recommend\":1,\n\t        \"frontend-reserve\":1,\n\t        \"search\":1,\n\t        \"check\":2,\n\t        \"recommendation\":1,\n\t        \"profile\":2,\n", "        \"user\":1,\n\t        \"reservation\":1,\n\t        \"geo\":1,\n\t        \"rate\":1,\n\t        \"memcached-check\":1,\n\t        \"rank-category\":1,\n\t        \"rank-overall\":1,\n\t        \"memcached-profile\":1,\n\t        \"memcached-reservation\":1,\n\t        \"mongodb-reservation\":1,\n", "        \"memcached-rate\":1\n\t    }\n\t    q=Queue()\n\t    q.put(ms_dag.root)\n\t    while(not q.empty()):\n\t        target=q.get()\n\t        idx1=target.index\n\t        if(target.index==0):\n\t            rate=target.realLoad/target.handleLoad\n\t        else:\n", "            target.realLoad=np.sum(ms_dag.Load_matrix[:,idx1])\n\t            rate=target.realLoad/min(target.handleLoad,target.monitorLoad)\n\t        if(rate<1):\n\t            rate=1\n\t        for child in target.pressure_children:\n\t            if(BFS_name[child.name]!=0):\n\t                idx2=child.index\n\t                ms_dag.Load_matrix[idx1][idx2]=ms_dag.Load_matrix[idx1][idx2]*rate\n\t                BFS_name[child.name]-=1\n\t                if(BFS_name[child.name]==0):\n", "                    q.put(child)\n\t    print(\"==========done==========\")\n\t    ms_dag.display()\n\tif __name__ == \"__main__\":\n\t    Update_Traffic_Graph()"]}
{"filename": "QueryDrainer/Compensator.py", "chunked_list": ["import os\n\timport pandas\n\timport numpy as np\n\timport time\n\tSLA=3#QoS reovery time, can be changed\n\t#time_interval: monitor interval\n\tdef Compensate(ms_dag,time_interval):\n\t    # Compensate for every microservice\n\t    for ms in ms_dag.Access_Record:\n\t        # this queue\n", "        this_overLoad=ms.realLoad-min(ms.monitorLoad-ms.overLoad,ms.handleLoad-ms.overLoad)\n\t        this_queueRequests=this_overLoad*time_interval\n\t        # history queue\n\t        history_overLoad=ms.overLoad\n\t        history_surplusTime=ms.SLAtime-time_interval\n\t        history_queueRequests=history_overLoad*history_surplusTime\n\t        # all queue\n\t        all_queueRequests=this_queueRequests+history_queueRequests\n\t        # needed total load\n\t        ms.SLAtime=SLA-time_interval\n", "        all_Requests=all_queueRequests+ms.realLoad*ms.SLAtime\n\t        tot_load=all_Requests/ms.SLAtime\n\t        # cal overload\n\t        ms.overLoad=tot_load-ms.realLoad\n\t    print(\"==========Compensate Done.==========\")\n\t    ms_dag.display()\n\tif __name__ == \"__main__\":\n\t    print()"]}
{"filename": "QueryDrainer/AdjustRes.py", "chunked_list": ["import os\n\timport time\n\timport sys\n\tsys.path.append(\".\") \n\t# To import grpc APIs\n\tfrom client.client import rpc_adjust_res\n\t#Node-Ip relationship\n\tpod_node_mapper={'check': 'cpu-04', 'consul': 'cpu-06', 'entering-ms': 'cpu-03', 'frontend-recommend': 'cpu-07', 'frontend-reserve': 'cpu-08', 'frontend-search': 'cpu-04', 'geo': 'cpu-04', 'jaeger': 'cpu-06', 'memcached-check': 'cpu-04', 'memcached-profile': 'cpu-07', 'memcached-rate': 'cpu-04', 'memcached-reservation': 'cpu-08', 'mongodb-check': 'cpu-04', 'mongodb-geo': 'cpu-04', 'mongodb-history': 'cpu-07', 'mongodb-profile': 'cpu-07', 'mongodb-rate': 'cpu-04', 'mongodb-recommendation': 'cpu-07', 'mongodb-reservation': 'cpu-08', 'mongodb-user': 'cpu-08', 'profile': 'cpu-07', 'rank-category': 'cpu-07', 'rank-overall': 'cpu-07', 'rate': 'cpu-04', 'recommendation': 'cpu-07', 'reservation': 'cpu-08', 'search': 'cpu-04', 'user': 'cpu-08'}\n\t#pod-node relationship\n\tpod_uids={\n", "    \"entering-ms\":['c95afad8-75ea-4cf6-b0a4-07982fad4cf7\\n'],\n\t    \"frontend-search\":['07b00721-06e6-47a6-abae-c6b6cb6512e0\\n'],\n\t    \"frontend-recommend\":['249ce56f-f436-411d-841a-1430135f8cd9\\n'],\n\t    \"frontend-reserve\":['1a545e65-91a4-4f99-96be-993366e7924d\\n'],\n\t    \"search\": ['41be1c59-1531-478f-8e4b-74b826e00c20\\n'],\n\t    \"check\": ['c2c39aef-516c-469d-b31c-c0e9193cea39\\n'],\n\t    \"recommendation\": ['75b476d8-759a-42ec-b02d-9f79ec0c3e99\\n'],\n\t    \"profile\": ['2fdee85b-2f09-4c96-922b-2f22800c1f53\\n'],\n\t    \"user\": ['153e0a9a-8cd5-439d-96e2-a21db4985b44\\n'],\n\t    \"reservation\": ['12ae4870-ceca-46c5-a8de-74992794251f\\n'],\n", "    \"geo\": ['80a37e50-bfbb-4a13-84e7-79a3a3e61879\\n'],\n\t    \"rate\":['1477e6df-edce-435f-8985-5a9d24359693\\n'],\n\t    \"memcached-check\":['bf8c4d95-e879-4116-82e4-75f0aed3469f\\n'],\n\t    \"rank-category\": ['4a0e4a06-74bd-485b-9108-cda1b0bb2cdb\\n'],\n\t    \"rank-overall\": ['604e15ce-e6b7-45d6-8347-9f51954db5cf\\n'],\n\t    \"memcached-profile\": ['5cb477da-db24-4cee-97eb-c0668a6ef455\\n'],\n\t    \"memcached-reservation\": ['b295189c-b513-4ad5-99b6-ff8e6dd4ff09\\n'],\n\t    \"mongodb-reservation\":['67273aa4-7f56-45fc-a721-04ac453833bb\\n'],\n\t    \"memcached-rate\":['a2630e43-dd25-47f6-b14c-8ac73ba386b1\\n']\n\t}\n", "#pod-uid relationship\n\tdef get_pod_uids():\n\t    for svc in pod_uids.keys():\n\t        service=str(svc)\n\t        os.system(\"./get_pod_uid.sh \"+service+\" >/dev/null 2>&1\")\n\t        uids=os.popen(\"cat uid.txt\").readlines()\n\t        print(svc, uids)\n\t#Call Grpc APIs to adjust resources\n\tdef set_cpu(ms_dag,service,cpu,replicas_number=1):\n\t    node=ms_dag.PodNodeMapper[service]\n", "    ip_str=ms_dag.NodeIpMapper[node]\n\t    uids=pod_uids[service]\n\t    res=rpc_adjust_res(ip_str,uids,cpu)\n\t#Call set_CPU\n\tdef run_set_cpu(ms_dag):\n\t    for target in ms_dag.Access_Record:\n\t        # if(target.name==\"mongodb-reservation\"):\n\t            # continue\n\t        print(target.name,target.CPU_Allocated)\n\t        set_cpu(ms_dag,target.name,target.CPU_Allocated,1)\n", "        target.handleLoad=target.realLoad+target.overLoad\n\t    print(\"Set all CPU done\")\n\tif __name__ == \"__main__\":\n\t    get_pod_uids()"]}
{"filename": "workerServer/distributed_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: distributed.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x11\\x64istributed.proto\\\"\\x1e\\n\\x0eTrafficRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fTrafficResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\")\\n\\nResRequest\\x12\\x0c\\n\\x04uids\\x18\\x01 \\x03(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01\\\"\\x1d\\n\\x0bResResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\\"\\x1e\\n\\x0eProfileRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fProfileResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\"\\x1e\\n\\x0eNetProcRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fNetProcResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\x32\\xd4\\x01\\n\\x0bGrpcService\\x12\\x32\\n\\x0bget_traffic\\x12\\x0f.TrafficRequest\\x1a\\x10.TrafficResponse\\\"\\x00\\x12(\\n\\tadjustRes\\x12\\x0b.ResRequest\\x1a\\x0c.ResResponse\\\"\\x00\\x12\\x32\\n\\x0bget_profile\\x12\\x0f.ProfileRequest\\x1a\\x10.ProfileResponse\\\"\\x00\\x12\\x33\\n\\x0cget_net_proc\\x12\\x0f.NetProcRequest\\x1a\\x10.NetProcResponse\\\"\\x00\\x42\\x03\\x80\\x01\\x01\\x62\\x06proto3')\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'distributed_pb2', globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t  DESCRIPTOR._options = None\n\t  DESCRIPTOR._serialized_options = b'\\200\\001\\001'\n\t  _TRAFFICREQUEST._serialized_start=21\n\t  _TRAFFICREQUEST._serialized_end=51\n\t  _TRAFFICRESPONSE._serialized_start=53\n\t  _TRAFFICRESPONSE._serialized_end=106\n", "  _RESREQUEST._serialized_start=108\n\t  _RESREQUEST._serialized_end=149\n\t  _RESRESPONSE._serialized_start=151\n\t  _RESRESPONSE._serialized_end=180\n\t  _PROFILEREQUEST._serialized_start=182\n\t  _PROFILEREQUEST._serialized_end=212\n\t  _PROFILERESPONSE._serialized_start=214\n\t  _PROFILERESPONSE._serialized_end=267\n\t  _NETPROCREQUEST._serialized_start=269\n\t  _NETPROCREQUEST._serialized_end=299\n", "  _NETPROCRESPONSE._serialized_start=301\n\t  _NETPROCRESPONSE._serialized_end=354\n\t  _GRPCSERVICE._serialized_start=357\n\t  _GRPCSERVICE._serialized_end=569\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "workerServer/server.py", "chunked_list": ["#! /usr/bin/env python\n\t# coding=utf8\n\timport time\n\tfrom concurrent import futures\n\timport grpc\n\timport os\n\timport distributed_pb2_grpc,distributed_pb2\n\tfrom datetime import datetime,timedelta\n\t_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n\t# service name:corresponds the keys of pod_uids\n", "# cpu: resource of cpu,1 is 0.1 CPU core\n\t# replica_number: replica number of MS\n\tdef set_cpu(uids,cpu):\n\t    cpu=cpu*10000\n\t    cpu=int(cpu)\n\t    cpu_every=cpu//len(uids)\n\t    # print(cpu_every)\n\t    for uid in uids:\n\t        uid=uid.replace(\"\\n\",\"\")\n\t        path = '/sys/fs/cgroup/cpu/kubepods/besteffort/pod' + uid + '/cpu.cfs_quota_us'\n", "        print(path,cpu_every)\n\t        # f = open(path,\"r\")\n\t        # original = int(f.read())\n\t        # f.close()\n\t        if cpu_every<1000:\n\t            cpu_every=1000\n\t        curr_value = str(cpu_every)\n\t        with open(path, \"w+\") as f:\n\t            f.write(curr_value)\n\tclass TestService(distributed_pb2_grpc.GrpcServiceServicer):\n", "    def __init__(self):\n\t        pass\n\t    def adjustRes(self, request, context):\n\t        '''\n\t        adjust resource\n\t        '''\n\t        uids=request.uids\n\t        cpu_value=float(request.value)\n\t        print(uids,cpu_value)\n\t        set_cpu(uids,cpu_value)\n", "        result='1'\n\t        return distributed_pb2.ResResponse(result=str(result))\n\t    def get_profile(self, request, context):\n\t        '''\n\t        get the cpu use of mircoservices\n\t        '''\n\t        svc_name = request.data\n\t        timestampf=datetime.now().timestamp()\n\t        cmd=\"docker stats --no-stream | grep \"+svc_name\n\t        res1=os.popen(cmd).readlines()\n", "        res_net=''.join(res1)\n\t        # res_net=\"success\"\n\t        return distributed_pb2.ProfileResponse(result=res_net,time_stamp=timestampf)\n\t    def get_net_proc(self, request, context):\n\t        '''\n\t        get the total traffic of interface of net\n\t        '''\n\t        src_ip = request.data\n\t        timestampf=datetime.now().timestamp()\n\t        # print(timestampf)\n", "        lines = os.popen(\"cat /proc/net/dev\").readlines()\n\t        res_net=','.join(lines)\n\t        # print(res_net)\n\t        return distributed_pb2.NetProcResponse(result=res_net,time_stamp=timestampf)\n\tdef run():\n\t    '''\n\t    start service\n\t    '''\n\t    server = grpc.server(futures.ThreadPoolExecutor(max_workers=70))\n\t    distributed_pb2_grpc.add_GrpcServiceServicer_to_server(TestService(),server)\n", "    server.add_insecure_port('[::]:50052')\n\t    server.start()\n\t    print(\"start service...\")\n\t    try:\n\t        while True:\n\t            time.sleep(_ONE_DAY_IN_SECONDS)\n\t    except KeyboardInterrupt:\n\t        server.stop(0)\n\tif __name__ == '__main__':\n\t    run()\n"]}
{"filename": "workerServer/distributed_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\timport distributed_pb2 as distributed__pb2\n\tclass GrpcServiceStub(object):\n\t    \"\"\"\n\t    \"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n", "            channel: A grpc.Channel.\n\t        \"\"\"\n\t        self.get_traffic = channel.unary_unary(\n\t                '/GrpcService/get_traffic',\n\t                request_serializer=distributed__pb2.TrafficRequest.SerializeToString,\n\t                response_deserializer=distributed__pb2.TrafficResponse.FromString,\n\t                )\n\t        self.adjustRes = channel.unary_unary(\n\t                '/GrpcService/adjustRes',\n\t                request_serializer=distributed__pb2.ResRequest.SerializeToString,\n", "                response_deserializer=distributed__pb2.ResResponse.FromString,\n\t                )\n\t        self.get_profile = channel.unary_unary(\n\t                '/GrpcService/get_profile',\n\t                request_serializer=distributed__pb2.ProfileRequest.SerializeToString,\n\t                response_deserializer=distributed__pb2.ProfileResponse.FromString,\n\t                )\n\t        self.get_net_proc = channel.unary_unary(\n\t                '/GrpcService/get_net_proc',\n\t                request_serializer=distributed__pb2.NetProcRequest.SerializeToString,\n", "                response_deserializer=distributed__pb2.NetProcResponse.FromString,\n\t                )\n\tclass GrpcServiceServicer(object):\n\t    \"\"\"\n\t    \"\"\"\n\t    def get_traffic(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n", "    def adjustRes(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def get_profile(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n", "    def get_net_proc(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\tdef add_GrpcServiceServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t            'get_traffic': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.get_traffic,\n\t                    request_deserializer=distributed__pb2.TrafficRequest.FromString,\n", "                    response_serializer=distributed__pb2.TrafficResponse.SerializeToString,\n\t            ),\n\t            'adjustRes': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.adjustRes,\n\t                    request_deserializer=distributed__pb2.ResRequest.FromString,\n\t                    response_serializer=distributed__pb2.ResResponse.SerializeToString,\n\t            ),\n\t            'get_profile': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.get_profile,\n\t                    request_deserializer=distributed__pb2.ProfileRequest.FromString,\n", "                    response_serializer=distributed__pb2.ProfileResponse.SerializeToString,\n\t            ),\n\t            'get_net_proc': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.get_net_proc,\n\t                    request_deserializer=distributed__pb2.NetProcRequest.FromString,\n\t                    response_serializer=distributed__pb2.NetProcResponse.SerializeToString,\n\t            ),\n\t    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\n\t            'GrpcService', rpc_method_handlers)\n", "    server.add_generic_rpc_handlers((generic_handler,))\n\t # This class is part of an EXPERIMENTAL API.\n\tclass GrpcService(object):\n\t    \"\"\"\n\t    \"\"\"\n\t    @staticmethod\n\t    def get_traffic(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n", "            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_traffic',\n\t            distributed__pb2.TrafficRequest.SerializeToString,\n\t            distributed__pb2.TrafficResponse.FromString,\n\t            options, channel_credentials,\n", "            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def adjustRes(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n", "            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/adjustRes',\n\t            distributed__pb2.ResRequest.SerializeToString,\n\t            distributed__pb2.ResResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def get_profile(request,\n\t            target,\n", "            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_profile',\n\t            distributed__pb2.ProfileRequest.SerializeToString,\n", "            distributed__pb2.ProfileResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def get_net_proc(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n", "            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_net_proc',\n\t            distributed__pb2.NetProcRequest.SerializeToString,\n\t            distributed__pb2.NetProcResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n"]}
{"filename": "Predict/Predictor.py", "chunked_list": ["import csv\n\timport scipy.stats as st\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport os\n\timport time\n\t#Record the slope and interceptes\n\t#(Examples, need to fit on own clusters)\n\tparams_net_to_load={\n", "    \"entering-ms\":[0.06,0.1],\n\t    \"frontend-search\":[0.06,0.1],\n\t    \"frontend-recommend\":[0.06,0.1],\n\t    \"frontend-reserve\":[0.06,0.1],\n\t    \"search\":[0.06,0.1],\n\t    \"check\":[0.06,0.1],\n\t    \"recommendation\":[0.06,0.1],\n\t    \"profile\":[0.06,0.1],\n\t    \"user\":[0.06,0.1],\n\t    \"reservation\":[0.06,0.1],\n", "    \"geo\":[0.06,0.1],\n\t    \"rate\":[0.06,0.1],\n\t    \"memcached-check\":[0.06,0.1],\n\t    \"rank-category\":[0.06,0.1],\n\t    \"rank-overall\":[0.06,0.1],\n\t    \"memcached-profile\":[0.06,0.1],\n\t    \"memcached-reservation\":[0.06,0.1],\n\t    \"mongodb-reservation\":[0.06,0.1],\n\t    \"memcached-rate\":[0.06,0.1]\n\t}\n", "#Record the slope and interceptes\n\t#(Examples, need to fit on own clusters)\n\tparams_load_to_CPU={\n\t    \"entering-ms\":[0.06,0.1],\n\t    \"frontend-search\":[0.06,0.1],\n\t    \"frontend-recommend\":[0.06,0.1],\n\t    \"frontend-reserve\":[0.06,0.1],\n\t    \"search\":[0.06,0.1],\n\t    \"check\":[0.06,0.1],\n\t    \"recommendation\":[0.06,0.1],\n", "    \"profile\":[0.06,0.1],\n\t    \"user\":[0.06,0.1],\n\t    \"reservation\":[0.06,0.1],\n\t    \"geo\":[0.06,0.1],\n\t    \"rate\":[0.06,0.1],\n\t    \"memcached-check\":[0.06,0.1],\n\t    \"rank-category\":[0.06,0.1],\n\t    \"rank-overall\":[0.06,0.1],\n\t    \"memcached-profile\":[0.06,0.1],\n\t    \"memcached-reservation\":[0.06,0.1],\n", "    \"mongodb-reservation\":[0.06,0.1],\n\t    \"memcached-rate\":[0.06,0.1]\n\t}\n\t# Record corresponding edges\n\tedge_mapper={\n\t    \"geo\": [[4,10]],\n\t    \"memcached-rate\": [[11,18]],\n\t    \"rate\": [[4,11]],\n\t    \"search\": [[1,4]],\n\t    \"memcached-check\": [[5,12]],\n", "    \"check\": [[10,5],[18,5]], #2 in-degrees\n\t    \"frontend-search\": [[0,1]],\n\t    \"rank-category\": [[6,13]],\n\t    \"rank-overall\": [[6,14]],\n\t    \"recommendation\": [[2,6]],\n\t    \"memcached-profile\": [[7,15]],\n\t    \"profile\": [[13,7],[14,7]], #2 in-degrees\n\t    \"frontend-recommend\": [[0,2]],\n\t    \"user\": [[3,8]],\n\t    \"memcached-reservation\": [[9,16]],\n", "    \"mongodb-reservation\": [[9,17]],\n\t    \"reservation\": [[8,9]],\n\t    \"frontend-reserve\": [[0,3]]\n\t}\n\t#Get the monitored load\n\tdef predict_net_to_load(ms_dag):\n\t    print(\"Monitor Load......\")\n\t    for target in ms_dag.Access_Record:\n\t        slope=params_net_to_load[target.name][0]\n\t        intercept=params_net_to_load[target.name][1]\n", "        target.monitorLoad=target.upper_traffic*slope+intercept\n\t        '''\n\t        # Profile values with traffic error to avoid network fluctuations to evaluate\n\t        target.monitorLoad=Base_MonitoredLoad[target.name]*TrafficMonitorError[target.name]\n\t        '''\n\t        print(target.name,target.monitorLoad)\n\t        # no in-degrees of entering ms\n\t        if(target.name==\"entering-ms\"):\n\t            continue\n\t        # Update edges in the Load_matrix\n", "        edges=edge_mapper[target.name]\n\t        if(len(edges)==1):#1 in-degree\n\t            ms_dag.Load_matrix[edges[0][0],edges[0][1]]=target.monitorLoad\n\t        else:# multiple in-degrees\n\t            ratios=[0,0]\n\t            if(target.name==\"check\"):\n\t                load1=min(ms_dag.Access_Record[0].handleLoad,ms_dag.Access_Record[0].monitorLoad)\n\t                load2=min(ms_dag.Access_Record[2].handleLoad,ms_dag.Access_Record[2].monitorLoad)\n\t                ratios[0]=load1/(load1+load2)\n\t                ratios[1]=1-ratios[0]\n", "            elif(target.name==\"profile\"):\n\t                load1=min(ms_dag.Access_Record[7].handleLoad,ms_dag.Access_Record[7].monitorLoad)\n\t                load2=min(ms_dag.Access_Record[8].handleLoad,ms_dag.Access_Record[8].monitorLoad)\n\t                ratios[0]=load1/(load1+load2)\n\t                ratios[1]=1-ratios[0]\n\t            for i in range(len(edges)):\n\t                edge=edges[i]\n\t                ratio=ratios[i]\n\t                ms_dag.Load_matrix[edge[0],edge[1]]=target.monitorLoad*ratio\n\t#Obtain the CPU allocation\n", "def predict_load_to_CPU(ms_dag):\n\t    for target in ms_dag.Access_Record:\n\t        slope=params_load_to_CPU[target.name][0]\n\t        intercept=params_load_to_CPU[target.name][1]\n\t        target.CPU_Need=target.realLoad*slope+intercept\n\t        target.CPU_Allocated=(target.realLoad+target.overLoad)*slope+intercept\n\t        over_CPU=target.CPU_Allocated-target.CPU_Need\n\t        '''\n\t        # Profile CPUs with real-load error to avoid prediction interference to evaluate\n\t        ratio=target.realLoad/Base_RealLoad[target.name]\n", "        target.CPU_Need=Profile_CPUs[target.name]*ratio\n\t        over_CPU=target.overLoad/target.realLoad*target.CPU_Need\n\t        target.CPU_Allocated=over_CPU+target.CPU_Need\n\t        '''\n\t        print(target.name,target.realLoad,target.overLoad,over_CPU,target.CPU_Allocated)\n\tif __name__ == \"__main__\":\n\t    predict_load_to_CPU()"]}
{"filename": "client/TrafficGet_hb.py", "chunked_list": ["import os\n\tfrom collections import defaultdict\n\tfrom client.client import rpc_adjust_res, rpc_get_traffic\n\timport pandas\n\timport time\n\tfrom multiprocessing import Process\n\timport datetime\n\tms_interface_mapper = defaultdict(list)  # {calicoxxx:[10.244.xx.xx,ms-0]}\n\tms_ip_mapper = defaultdict(list)\n\torder_mapper_hb = {\"entering-ms\": [], \"frontend-search\": [\"entering-ms\"], \"frontend-recommend\": [\"entering-ms\"], \"frontend-reserve\": [\n", "    \"entering-ms\"], \"search\": [\"frontend-search\"], \"check\": [\"frontend-search\"], \"recommendation\": [\"frontend-recommend\"], \"profile\": [\"frontend-recommend\"],  \"user\": [\"frontend-reserve\"], \"reservation\": [\"frontend-reserve\"], \"geo\": [\"search\"], \"rate\": [\"search\"], \"memcached-check\": [\"check\"], \"rank-category\": [\"recommendation\"], \"rank-overall\": [\"recommendation\"], \"memcached-profile\": [\"profile\"], \"memcached-reservation\": [\n\t    \"reservation\"], \"mongodb-reservation\": [\"reservation\"], \"memcached-rate\": [\"rate\"]}\n\t# {cpu-04:[calicoxxx,calicoxxx,......]} \n\tNode_interface_mapper = defaultdict(list)\n\tNodeIpMapper = {\n\t    \"cpu-03\": \"10.2.64.3:50052\",\n\t    \"cpu-04\": \"10.2.64.4:50052\",\n\t    \"cpu-07\": \"10.2.64.7:50052\",\n\t    \"cpu-08\": \"10.2.64.8:50052\",\n\t}\n", "def analyse_lines(lines_get_proc):\n\t    global ms_interface_mapper, ms_ip_mapper, order_mapper_ebc2, order_mapper_ebc1, order_mapper_hb\n\t    now_ms = ''\n\t    now_ip = ''\n\t    map_res_traffic = defaultdict(int)\n\t    map_res_num = defaultdict(int)\n\t    for l in lines_get_proc:\n\t        line = l.rstrip('\\n')\n\t        if line == '':\n\t            continue\n", "        if line.startswith('+'):\n\t            ca_name = line[1:]\n\t            if ca_name not in ms_interface_mapper.keys():\n\t                continue\n\t            now_ip = ms_interface_mapper[ca_name][0]\n\t            now_ms = ms_interface_mapper[ca_name][1]\n\t            continue\n\t        if (now_ms not in order_mapper_hb.keys()) or (now_ms == ''):\n\t            continue\n\t        list_line = line.split(':')\n", "        if len(list_line) < 3:\n\t            print(l, \"num is not correct!\")\n\t            return defaultdict(int), defaultdict(int)\n\t        src_ip = list_line[0].split(';')[0]\n\t        dst_ip = list_line[0].split(';')[1]\n\t        traffic_loc = int(list_line[1])\n\t        bag_num = int(list_line[2])\n\t        if now_ms == \"entering-ms\" and (src_ip not in ms_ip_mapper.keys()):\n\t            map_res_traffic[now_ms] += traffic_loc\n\t            map_res_num[now_ms] += bag_num\n", "            continue\n\t        if (src_ip not in ms_ip_mapper) or (dst_ip not in ms_ip_mapper):\n\t            # print(l,\"src or dst is not in json\")\n\t            continue\n\t        src_ms = ms_ip_mapper[src_ip][1]\n\t        dst_ms = ms_ip_mapper[dst_ip][1]\n\t        if dst_ms != now_ms:\n\t            # print(l,\"dst is not now_dst!\")\n\t            continue\n\t        if src_ms in order_mapper_hb[now_ms]:\n", "            map_res_traffic[now_ms] += traffic_loc\n\t            map_res_num[now_ms] += bag_num\n\t    return map_res_traffic, map_res_num\n\tdef ms_interace():\n\t    global ms_interface_mapper, ms_ip_mapper, Node_interface_mapper\n\t    # MS-name, pod-ip, node-name\n\t    pods_ips_nodes = os.popen(\n\t        \"kubectl get pods -o wide | awk '{print $1,$6,$7}'\").readlines()\n\t    for p_ip in pods_ips_nodes:\n\t        p_ip = p_ip.rstrip('\\n')\n", "        l_pip = p_ip.split(' ')\n\t        if len(l_pip) < 3:\n\t            continue\n\t        pod = l_pip[0]\n\t        ip = l_pip[1]\n\t        node = l_pip[2]\n\t        if (pod == \"NAME\"):\n\t            continue\n\t        if (pod[:6] == \"jaeger\"):\n\t            continue\n", "        id = -1\n\t        # Obatin the no of network interface\n\t        try:\n\t            id = os.popen(\"kubectl exec -i \"+pod +\n\t                          \" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n\t        except IndexError:\n\t            print(\"kubectl error\", pod, id)\n\t            continue\n\t        idtarget = \"'^\"+id+\":\"+\" \"+\"'\"\n\t        temp = \"\"\n", "        # Obtain the network interface name\n\t        if (node == \"cpu-06\"):  \n\t            temp = os.popen(\"ip link show | grep \"+idtarget +\n\t                            \" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n\t        else: \n\t            temp = os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget +\n\t                            \"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n\t        index = temp.find('@')\n\t        iplink = temp[:index]\n\t        temp = pod.split(\"-\")\n", "        extra = \"-\"+temp[-2]+\"-\"+temp[-1]\n\t        value = pod.replace(extra, \"\")\n\t        # print(iplink, ip, value)\n\t        ms_interface_mapper[iplink] = [ip, value]\n\t        ms_ip_mapper[ip] = [iplink, value]\n\t        if node != 'cpu-06':\n\t            Node_interface_mapper[node].append(iplink)\n\t    pods_ips_nodes_svc = os.popen(\n\t        \"kubectl get svc | awk '{print $1,$3}'\").readlines()\n\t    for p_ip in pods_ips_nodes_svc:\n", "        p_ip = p_ip.rstrip('\\n')\n\t        l_pip = p_ip.split(' ')\n\t        if len(l_pip) < 2:\n\t            continue\n\t        pod = l_pip[0]\n\t        ip = l_pip[1]\n\t        if (pod == \"NAME\"):\n\t            continue\n\t        if (pod[:6] == \"jaeger\" or pod == \"kubernetes\"):\n\t            continue\n", "        ms_ip_mapper[ip] = [\"\", pod]\n\t    for k, v in ms_interface_mapper.items():\n\t        print(k, v)\n\t    for k, v in ms_ip_mapper.items():\n\t        print(k, v)\n\tdef execute_cmd(node, list_interface, ip_list):\n\t    str_interface = ' '.join(list_interface)\n\t    str_iplist = ' '.join(ip_list)\n\t    cmd = \"ssh \"+node+\" '/state/partition/zxtong_2/TrafficMonitor/TrafficGet/get_traffic_new \" + \\\n\t        str_interface+\" \"+str_iplist+\" 25 300 600'\"\n", "    print(cmd)\n\t    os.system(cmd)\n\tdef start_capture():\n\t    global Node_interface_mapper, ms_interface_mapper\n\t    print(\"Node_interface_mapper\", Node_interface_mapper)\n\t    process_list = []\n\t    for k, v in Node_interface_mapper.items():\n\t        if len(v) == 0:\n\t            continue\n\t        ip_list = []\n", "        for interface_name in v:\n\t            ip_list.append(ms_interface_mapper[interface_name][0])\n\t        p = Process(target=execute_cmd, args=(k, v, ip_list,))\n\t        # print(list_ms_4)\n\t        process_list.append(p)\n\t    for p in process_list:\n\t        p.start()\n\t    return process_list\n\tdef getTraffic():\n\t    global ms_interface_mapper, ms_ip_mapper\n", "    traffic_mapper = defaultdict(int)\n\t    lines_str = ''\n\t    for k, v in NodeIpMapper.items():\n\t        if k not in Node_interface_mapper.keys():\n\t            continue\n\t        str_res = rpc_get_traffic(v)\n\t        lines_str += str_res\n\t    lines_get_proc = lines_str.split('\\n')\n\t    # print(lines_get_proc)\n\t    traffic_mapper, res_tmp = analyse_lines(lines_get_proc)\n", "    return traffic_mapper, res_tmp\n\tif __name__ == '__main__':\n\t    dt_time0 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t    ms_interace()\n\t    dt_time1 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t    process_list = start_capture()\n\t    list_frame = []\n\t    count = 0\n\t    max_count = 20\n\t    lines = []\n", "    old_map_traffic = defaultdict(int)\n\t    old_map_num = defaultdict(int)\n\t    dt_time2 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t    while (count < max_count):\n\t        time.sleep(0.5)\n\t        print(count)\n\t        new_map_traffic, new_map_num = getTraffic()\n\t        for k, v in new_map_traffic.items():\n\t            l_tmp = [count, k, v-old_map_traffic[k],\n\t                     new_map_num[k]-old_map_num[k]]\n", "            list_frame.append(l_tmp)\n\t        old_map_traffic = new_map_traffic\n\t        old_map_num = new_map_num\n\t        count += 1\n\t    dt_time3 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t    for p in process_list:\n\t        p.terminate()\n\t        p.join()\n\t    '''\n\t    '''\n", "    dt_time4 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\t    print(dt_time0)\n\t    print(dt_time1)\n\t    print(dt_time2)\n\t    print(dt_time3)\n\t    print(dt_time4)\n\t    df = pandas.DataFrame(list_frame, columns=[\n\t                          'time', 'direction', 'traffic', 'num'])\n\t    df.to_csv('speed.csv')\n"]}
{"filename": "client/ProfileGet.py", "chunked_list": ["from client.client import rpc_get_profile\n\tdef getProfile(v,svc_name):\n\t    return rpc_get_profile(v,svc_name)\n\tif __name__ == '__main__':\n\t    getProfile(\"10.2.64.8:50052\",\"ebc1\")\n"]}
{"filename": "client/distributed_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: distributed.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x11\\x64istributed.proto\\\"\\x1e\\n\\x0eTrafficRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fTrafficResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\")\\n\\nResRequest\\x12\\x0c\\n\\x04uids\\x18\\x01 \\x03(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01\\\"\\x1d\\n\\x0bResResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\\"\\x1e\\n\\x0eProfileRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fProfileResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\"\\x1e\\n\\x0eNetProcRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fNetProcResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\x32\\xd4\\x01\\n\\x0bGrpcService\\x12\\x32\\n\\x0bget_traffic\\x12\\x0f.TrafficRequest\\x1a\\x10.TrafficResponse\\\"\\x00\\x12(\\n\\tadjustRes\\x12\\x0b.ResRequest\\x1a\\x0c.ResResponse\\\"\\x00\\x12\\x32\\n\\x0bget_profile\\x12\\x0f.ProfileRequest\\x1a\\x10.ProfileResponse\\\"\\x00\\x12\\x33\\n\\x0cget_net_proc\\x12\\x0f.NetProcRequest\\x1a\\x10.NetProcResponse\\\"\\x00\\x42\\x03\\x80\\x01\\x01\\x62\\x06proto3')\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'distributed_pb2', globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t  DESCRIPTOR._options = None\n\t  DESCRIPTOR._serialized_options = b'\\200\\001\\001'\n\t  _TRAFFICREQUEST._serialized_start=21\n\t  _TRAFFICREQUEST._serialized_end=51\n\t  _TRAFFICRESPONSE._serialized_start=53\n\t  _TRAFFICRESPONSE._serialized_end=106\n", "  _RESREQUEST._serialized_start=108\n\t  _RESREQUEST._serialized_end=149\n\t  _RESRESPONSE._serialized_start=151\n\t  _RESRESPONSE._serialized_end=180\n\t  _PROFILEREQUEST._serialized_start=182\n\t  _PROFILEREQUEST._serialized_end=212\n\t  _PROFILERESPONSE._serialized_start=214\n\t  _PROFILERESPONSE._serialized_end=267\n\t  _NETPROCREQUEST._serialized_start=269\n\t  _NETPROCREQUEST._serialized_end=299\n", "  _NETPROCRESPONSE._serialized_start=301\n\t  _NETPROCRESPONSE._serialized_end=354\n\t  _GRPCSERVICE._serialized_start=357\n\t  _GRPCSERVICE._serialized_end=569\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "client/client.py", "chunked_list": ["#! /usr/bin/env python\n\t# coding=utf8\n\timport grpc\n\timport time,json\n\timport client.distributed_pb2_grpc as distributed_pb2_grpc\n\timport  client.distributed_pb2  as distributed_pb2\n\tdef rpc_get_traffic(ip_str):\n\t    '''\n\t    get the traffic \n\t    '''\n", "    time1=time.time()\n\t    conn=grpc.insecure_channel(ip_str)\n\t    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n\t    request = distributed_pb2.TrafficRequest(data=ip_str)\n\t    response = client.get_traffic(request)\n\t    res_net=response.result\n\t    return res_net\n\tdef rpc_adjust_res(ip_str,uids,value):\n\t    '''\n\t    adjust resource from client\n", "    :return:\n\t    '''\n\t    conn=grpc.insecure_channel(ip_str)\n\t    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n\t    # map_res is the resource allocated\n\t    request = distributed_pb2.ResRequest(uids=uids,value=value)\n\t    response = client.adjustRes(request)\n\t    return response.result\n\t    # print(\"func2 received:\",response.result)\n\tdef rpc_get_profile(ip_str,svc_name):\n", "    '''\n\t    get the cpu use from client\n\t    '''\n\t    time1=time.time()\n\t    conn=grpc.insecure_channel(ip_str)\n\t    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n\t    request = distributed_pb2.ProfileRequest(data=svc_name)\n\t    response = client.get_profile(request)\n\t    res_net=response.result\n\t    return res_net\n", "def rpc_get_net_proc(ip_str):\n\t    '''\n\t    get the in traffic of every interface of network\n\t    :return:\n\t    '''\n\t    time1=time.time()\n\t    conn=grpc.insecure_channel(ip_str)\n\t    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n\t    request = distributed_pb2.NetProcRequest(data=ip_str)\n\t    response = client.get_net_proc(request)\n", "    res_str=response.result\n\t    res_net=res_str.split(',')\n\t    return res_net\n\tdef run():\n\t    # test function\n\t    count=0\n\t    test=10\n\t    ip_str='10.3.64.4:50052'\n\t    # time.sleep(2)\n\t    while count<test:\n", "        count+=1\n\t        str1=rpc_get_traffic(ip_str)\n\t        print(str1)\n\t        time.sleep(1)\n\t    # func3(1)\n\tif __name__ == '__main__':\n\t    run()\n"]}
{"filename": "client/__init__.py", "chunked_list": []}
{"filename": "client/Resset.py", "chunked_list": ["import os\n\tfrom collections import defaultdict\n\tfrom client.client import rpc_adjust_res\n\timport pandas\n\timport time\n\tfrom multiprocessing import Process\n\timport datetime\n\tdef setProfile(ip_str,uids,value):\n\t    return rpc_adjust_res(ip_str,uids,value)\n\tif __name__ == '__main__':\n", "    # setProfile(\"10.2.64.8:50052\")\n\t    print(\"Resset\")"]}
{"filename": "client/distributed_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\timport client.distributed_pb2 as distributed__pb2\n\tclass GrpcServiceStub(object):\n\t    \"\"\"\n\t    \"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n", "            channel: A grpc.Channel.\n\t        \"\"\"\n\t        self.get_traffic = channel.unary_unary(\n\t                '/GrpcService/get_traffic',\n\t                request_serializer=distributed__pb2.TrafficRequest.SerializeToString,\n\t                response_deserializer=distributed__pb2.TrafficResponse.FromString,\n\t                )\n\t        self.adjustRes = channel.unary_unary(\n\t                '/GrpcService/adjustRes',\n\t                request_serializer=distributed__pb2.ResRequest.SerializeToString,\n", "                response_deserializer=distributed__pb2.ResResponse.FromString,\n\t                )\n\t        self.get_profile = channel.unary_unary(\n\t                '/GrpcService/get_profile',\n\t                request_serializer=distributed__pb2.ProfileRequest.SerializeToString,\n\t                response_deserializer=distributed__pb2.ProfileResponse.FromString,\n\t                )\n\t        self.get_net_proc = channel.unary_unary(\n\t                '/GrpcService/get_net_proc',\n\t                request_serializer=distributed__pb2.NetProcRequest.SerializeToString,\n", "                response_deserializer=distributed__pb2.NetProcResponse.FromString,\n\t                )\n\tclass GrpcServiceServicer(object):\n\t    \"\"\"\n\t    \"\"\"\n\t    def get_traffic(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n", "    def adjustRes(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\t    def get_profile(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n", "    def get_net_proc(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details('Method not implemented!')\n\t        raise NotImplementedError('Method not implemented!')\n\tdef add_GrpcServiceServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t            'get_traffic': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.get_traffic,\n\t                    request_deserializer=distributed__pb2.TrafficRequest.FromString,\n", "                    response_serializer=distributed__pb2.TrafficResponse.SerializeToString,\n\t            ),\n\t            'adjustRes': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.adjustRes,\n\t                    request_deserializer=distributed__pb2.ResRequest.FromString,\n\t                    response_serializer=distributed__pb2.ResResponse.SerializeToString,\n\t            ),\n\t            'get_profile': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.get_profile,\n\t                    request_deserializer=distributed__pb2.ProfileRequest.FromString,\n", "                    response_serializer=distributed__pb2.ProfileResponse.SerializeToString,\n\t            ),\n\t            'get_net_proc': grpc.unary_unary_rpc_method_handler(\n\t                    servicer.get_net_proc,\n\t                    request_deserializer=distributed__pb2.NetProcRequest.FromString,\n\t                    response_serializer=distributed__pb2.NetProcResponse.SerializeToString,\n\t            ),\n\t    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\n\t            'GrpcService', rpc_method_handlers)\n", "    server.add_generic_rpc_handlers((generic_handler,))\n\t # This class is part of an EXPERIMENTAL API.\n\tclass GrpcService(object):\n\t    \"\"\"\n\t    \"\"\"\n\t    @staticmethod\n\t    def get_traffic(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n", "            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_traffic',\n\t            distributed__pb2.TrafficRequest.SerializeToString,\n\t            distributed__pb2.TrafficResponse.FromString,\n\t            options, channel_credentials,\n", "            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def adjustRes(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n", "            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/adjustRes',\n\t            distributed__pb2.ResRequest.SerializeToString,\n\t            distributed__pb2.ResResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def get_profile(request,\n\t            target,\n", "            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n\t            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_profile',\n\t            distributed__pb2.ProfileRequest.SerializeToString,\n", "            distributed__pb2.ProfileResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\t    @staticmethod\n\t    def get_net_proc(request,\n\t            target,\n\t            options=(),\n\t            channel_credentials=None,\n\t            call_credentials=None,\n\t            insecure=False,\n", "            compression=None,\n\t            wait_for_ready=None,\n\t            timeout=None,\n\t            metadata=None):\n\t        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_net_proc',\n\t            distributed__pb2.NetProcRequest.SerializeToString,\n\t            distributed__pb2.NetProcResponse.FromString,\n\t            options, channel_credentials,\n\t            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n"]}
{"filename": "LoadGenerator/LoadGenerator-dis-hr.py", "chunked_list": ["import requests,os\n\timport math\n\timport random\n\timport time\n\timport numpy as np\n\timport sys\n\tfrom threading import Thread\n\tfrom multiprocessing import Process,Manager\n\timport argparse\n\timport subprocess\n", "import pandas\n\tparser = argparse.ArgumentParser(description='--head,--qps,--node_size')\n\tparser.add_argument('-head','--head', type=int, default=1)\n\tparser.add_argument('-q','--qps', type=int, default=3750)\n\tparser.add_argument('-n','--node_size', type=int, default=3)\n\tparser.add_argument('-d','--duration', type=int, default=30)\n\tparser.add_argument('-p','--process', type=int, default=20)\n\tparser.add_argument('-t','--types', type=int, default=4)\n\tparser.add_argument('-nodeName','--nodeName', type=str, default='cpu-02')\n\tcg_rate_list_all=[  \n", "                  [],\n\t                  [1,1,1,1,1],\n\t                  [0.5,0.5,1,1,2],\n\t                  [0.5,0.5,1.5,0.5,2],\n\t                  [1,0.5,1,0.5,2],\n\t                  [1.5,0.5,0.5,0.5,2]]\n\tcg_rate_list=[]\n\tdata=\"http://10.102.103.173:5000/\"\n\tnode_list=['cpu-02','cpu-09','cpu-10']\n\tdef search(dynamic_rate):\n", "    # random data generation\n\t    in_date=random.randint(9,23)\n\t    # out_date=random.randint(in_date+1,24)\n\t    out_date=in_date+1\n\t    in_date_str=str(in_date)\n\t    if(in_date<=9):\n\t        in_date_str=\"2015-04-0\"+in_date_str\n\t    else:\n\t        in_date_str=\"2015-04-\"+in_date_str\n\t    out_date_str=str(out_date)\n", "    if(out_date<=9):\n\t        out_date_str=\"2015-04-0\"+out_date_str\n\t    else:\n\t        out_date_str=\"2015-04-\"+out_date_str\n\t    lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n\t    lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n\t    # req_param=\"rates\"\n\t    coin=random.random()\n\t    if(coin<dynamic_rate):# search by distance\n\t        req_param=\"nearby\"\n", "    else:#search by rate\n\t        req_param=\"rates\"\n\t    # else:           #search by rate in certain disctance\n\t    #     req_param=\"all\"\n\t    # generate url and params\n\t    url=data+\"hotels\"\n\t    params={\n\t        \"inDate\":in_date_str,\n\t        \"outDate\":out_date_str,\n\t        \"lat\":str(lat),\n", "        \"lon\":str(lon),\n\t        \"require\":req_param,\n\t    }\n\t    return url,params\n\t#hotel-benchmark \"recommend\" generate url with random data\n\tdef recommend(dynamic_rate):\n\t    # random data generation\n\t    coin=random.random()\n\t    if coin<dynamic_rate:\n\t        coin_=random.random()\n", "        if(coin_<0.33):# best fit shortest distance\n\t            req_param=\"dis\"\n\t        elif(coin_<0.66):#best fit hightest rate\n\t            req_param=\"rate\"\n\t        else:#best fit lowest price\n\t            req_param=\"price\"\n\t        lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n\t        lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n\t        # generate url and params\n\t        url=data+\"recommendations\"\n", "        params={\n\t            \"require\":str(req_param),\n\t            \"lat\":str(lat),\n\t            \"lon\":str(lon)\n\t        }\n\t    else:\n\t        req_param=\"overall\"\n\t        lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n\t        lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n\t        id = random.randint(0, 500)\n", "        user_name = \"Cornell_\"+str(id)\n\t        # generate url and params\n\t        url=data+\"recommendations\"\n\t        params={\n\t            \"require\":str(req_param),\n\t            \"lat\":str(lat),\n\t            \"lon\":str(lon),\n\t            \"username\":user_name\n\t        }        \n\t    return url, params\n", "#hotel-benchmark \"user\" generate url with random data\n\tdef user():\n\t    # random data generation\n\t    id = random.randint(0, 500)\n\t    user_name = \"Cornell_\"+str(id)\n\t    pass_word = \"\"\n\t    for i in range(3):\n\t        pass_word=pass_word+str(id)\n\t    # generate url and params\n\t    url=data+\"user\"\n", "    params={\n\t        \"username\":str(user_name),\n\t        \"password\":str(pass_word)\n\t    }\n\t    return url, params\n\t#hotel-benchmark \"reserve\" generate url with random data\n\tdef reserve():\n\t    # random data generation\n\t    in_date=random.randint(9,23)\n\t    out_date=in_date+random.randint(1,5)\n", "    in_date_str=str(in_date)\n\t    if(in_date<=9):\n\t        in_date_str = \"2015-04-0\"+in_date_str \n\t    else:\n\t        in_date_str = \"2015-04-\"+in_date_str\n\t    out_date_str=str(out_date)\n\t    if(out_date<=9):\n\t        out_date_str = \"2015-04-0\"+out_date_str \n\t    else:\n\t        out_date_str = \"2015-04-\"+out_date_str\n", "    hotel_id=str(random.randint(1,80))\n\t    id = random.randint(0, 500)\n\t    user_name = \"Cornell_\"+str(id)\n\t    pass_word = \"\"\n\t    for i in range(10):\n\t        pass_word=pass_word+str(id)\n\t    cust_name=user_name\n\t    num_room=\"1\"\n\t    # generate url and params\n\t    url=data+\"reservation\"\n", "    params={\n\t        \"inDate\":in_date_str,\n\t        \"outDate\":out_date_str,\n\t        \"lat\":\"\",\n\t        \"lon\":\"\",\n\t        \"hotelId\":hotel_id,\n\t        \"customerName\":cust_name,\n\t        \"username\":user_name,\n\t        \"password\":pass_word,\n\t        \"number\":num_room\n", "    }\n\t    return url, params\n\tdef generate_gaussian_arraival():\n\t    # np.random.normal(4,0.08,250)#mu,sigma,sampleNo\n\t    dis_list=[]\n\t    with open(\"GD_norm.csv\") as f:#use already generated data before\n\t        for line in f:\n\t            dis_list.append(float(line.replace(\"\\n\",\"\")))\n\t    return dis_list\n\t#post a request\n", "def post_request(url, params,list_99):\n\t    response=requests.get(url=url,params=params,headers={'Connection':'close'},timeout=(10,10))\n\t    list_99.append([time.time(),response.elapsed.total_seconds()*1000])\n\t# generate requests with dynamic graphs\n\tdef dynamic_graph_rate(dr0,dr1,dr2,dr3,dr4):\n\t    # ratio for 3 kinds of call graphs\n\t    cnt=dr0+dr1+dr2+dr3+dr4\n\t    search_ratio=float(dr0+dr1)/cnt\n\t    recommend_ratio=float(dr2+dr3)/cnt\n\t    reserve_ratio=float(dr4)/cnt\n", "    # for each request, random call graph\n\t    # print(search_ratio,recommend_ratio,reserve_ratio)\n\t    coin=random.random()\n\t    if(coin<search_ratio):\n\t        url, params=search(float(dr0)/(dr0+dr1))\n\t    elif(coin<search_ratio+recommend_ratio):\n\t        url, params=recommend(float(dr2)/(dr2+dr3))\n\t    else:\n\t        url, params=reserve()\n\t    return url, params\n", "def threads_generation(QPS_list,duraion,process_number,list_99_all): #250,20\n\t    plist = []\n\t    query_list = []\n\t    dis_list = []\n\t    list_99=[]\n\t    gs_inter=generate_gaussian_arraival()\n\t    for j in range(0,duraion):\n\t        QOS_this_s=int(QPS_list[j]/process_num)\n\t        for i in range(0,QOS_this_s):\n\t            url, params=dynamic_graph_rate(cg_rate_list[0],cg_rate_list[1],\n", "            cg_rate_list[2],cg_rate_list[3],cg_rate_list[4])#determine dynamic call graph\n\t            p = Thread(target=post_request,args=(url,params,list_99))\n\t            plist.append(p)\n\t            dis_list.append(gs_inter[i%250]* 250/QOS_this_s)\n\t    # print(\"Total %d thread in %d s\"%(len(dis_list),duraion))\n\t    fun_sleep_overhead=0.000075# overhead to call sleep()function\n\t    # print(\"begin\")\n\t    # For each process, control the QPS=250query/s, and apply Gaussian distribution\n\t    waste_time=0\n\t    t1=time.time()\n", "    for i in range(len(plist)):\n\t        t_s=time.time()#10^-3ms, negligible\n\t        plist[i].start()\n\t        t_e=time.time()\n\t        #[thread start time] + [sleep() funtion time] + [sleep time] = dis_list[i]\n\t        sleep_time=dis_list[i]/1000-((t_e-t_s)+fun_sleep_overhead)\n\t        if(sleep_time>0):\n\t            # can compensate\n\t            if(sleep_time+waste_time>=0):\n\t                time.sleep(sleep_time+waste_time)\n", "                waste_time=0\n\t            else:\n\t                waste_time+=sleep_time\n\t        else:\n\t            waste_time+=sleep_time\n\t    t2=time.time()\n\t    print(\"done, time count is %f,should be %d, waste_time %f\\n\"%(t2-t1,duraion,waste_time))\n\t    #Control all the requests ends before statistics\n\t    for item in plist:\n\t        item.join()\n", "    list_99_all.append(list_99)\n\t    # print(np.percentile(np.array(list_99),99))\n\tdef request_test(QPS_list,duraion=20,process_number=20):\n\t    time1=time.time()\n\t    assert len(QPS_list)==int(duraion)\n\t    process_list=[]\n\t    list_99_all=Manager().list()\n\t    for i in range(process_number):\n\t        p=Process(target=threads_generation,args=(QPS_list,duraion,process_number,list_99_all))\n\t        process_list.append(p)\n", "    # start processes\n\t    for p in process_list:\n\t        p.start()\n\t    # print(\"All processes start.\")\n\t    for p in process_list:\n\t        p.join()\n\t    print(\"All processes done.Total is %f\"%(time.time()-time1))    \n\t    latency_this_node=[]\n\t    for i in list_99_all:\n\t        latency_this_node.extend(i)\n", "    for i in range(len(latency_this_node)):\n\t        latency_this_node[i][0]-=time1\n\t    latency_this_node=sorted(latency_this_node,key=(lambda x:x[0]),reverse=False)  \n\t    df=pandas.DataFrame(latency_this_node,columns=['time','latency'])\n\t    df.to_csv(f'hr_{args.nodeName}_latency.csv') \n\tdef run_on_node(cmd):\n\t    os.system(cmd)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    if args.head:\n", "        o_process_list=[]\n\t        for i in range(args.node_size):\n\t            cmd=\" ssh {} 'ulimit -n 100000;cd /home/jcshi/LoadGenerator;python3 LoadGenerator-dis-hr.py --head {} --qps {} --node_size {} -p {} -t {} -nodeName {} -d {}' \"\\\n\t                .format(node_list[i],0,args.qps,args.node_size,args.process,args.types,node_list[i],args.duration)\n\t            print(cmd)\n\t            # run_on_node(cmd)\n\t            p=Process(target=run_on_node,args=(cmd,))\n\t            o_process_list.append(p)\n\t        for p in o_process_list:\n\t            p.start()\n", "        for p in o_process_list:\n\t            p.join()\n\t    else:\n\t        print(args)\n\t        os.system(\"ulimit -n 100000\")\n\t        QPS=int(args.qps/args.node_size)\n\t        duraion=args.duration\n\t        process_num=args.process\n\t        cg_rate_list=cg_rate_list_all[args.types]\n\t        QPS_list=[]\n", "        for i in range(duraion):\n\t            QPS_list+=[QPS]\n\t        print(QPS_list)\n\t        request_test(QPS_list,duraion,process_num)"]}
{"filename": "LoadMonitor/NetTrafficMonitor_temp.py", "chunked_list": ["import os\n\timport time\n\timport copy\n\timport math\n\timport sys\n\tsys.path.append(\"..\") \n\tfrom client.client import rpc_get_net_proc\n\tPodNodeMapper={}\n\tNodeIpMapper={\n\t    \"cpu-03\":\"10.2.64.3:50052\",\n", "    \"cpu-04\":\"10.2.64.4:50052\",\n\t    \"cpu-07\":\"10.2.64.7:50052\",\n\t    \"cpu-08\":\"10.2.64.8:50052\",\n\t}\n\tclass Node:\n\t    def __init__(self,name,in_traffic,out_traffic,upper_traffic,back_traffic,children):\n\t        self.name=name\n\t        self.in_traffic=in_traffic\n\t        self.out_traffic=out_traffic\n\t        self.upper_traffic=upper_traffic\n", "        self.back_traffic=back_traffic\n\t        self.children=children\n\t    def display(self):\n\t        print(self.name,self.in_traffic,self.out_traffic,self.upper_traffic,self.back_traffic,self.children)\n\tdef buildDAG():\n\t    profilemmc=Node(\"memcached-profile\",0,0,0,0,[])\n\t    ratemmc=Node(\"memcached-rate\",0,0,0,0,[])\n\t    reservemmc=Node(\"memcached-reserve\",0,0,0,0,[])\n\t    reservemongodb=Node(\"mongodb-reservation\",0,0,0,0,[])\n\t    checkmmc=Node(\"memcached-check\",0,0,0,0,[])\n", "    rank_category=Node(\"rank-category\",0,0,0,0,[])\n\t    rank_overall=Node(\"rank-overall\",0,0,0,0,[])\n\t    recommend=Node(\"recommendation\",0,0,0,0,[rank_category,rank_overall])\n\t    geo=Node(\"geo\",0,0,0,0,[])\n\t    user=Node(\"user\",0,0,0,0,[])    \n\t    profile=Node(\"profile\",0,0,0,0,[profilemmc])\n\t    rate=Node(\"rate\",0,0,0,0,[ratemmc])\n\t    search=Node(\"search\",0,0,0,0,[geo,rate])\n\t    check=Node(\"check\",0,0,0,0,[checkmmc])\n\t    reserve=Node(\"reservation\",0,0,0,0,[reservemmc,reservemongodb])\n", "    frontendSearch=Node(\"frontend-search\",0,0,0,0,[search,check])\n\t    frontendRecommend=Node(\"frontend-recommend\",0,0,0,0,[recommend,profile])\n\t    frontendReserve=Node(\"frontend-reserve\",0,0,0,0,[user,reserve])\n\t    EnteringMS=Node(\"entering-ms\",0,0,0,0,[frontendSearch,frontendRecommend,frontendReserve])\n\t    return EnteringMS\n\tdef postOrder(Access_Record,root):\n\t    if not root:\n\t        return\n\t    for target in root.children:\n\t        postOrder(Access_Record,target)\n", "    Access_Record.append(root)\n\tms_interace_mapper={}\n\tdef ms_interace():\n\t    global ms_interace_mapper, PodNodeMapper\n\t    pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n\t    nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n\t    for i in range(len(pods)):\n\t        pod=pods[i].replace(\"\\n\",\"\")\n\t        node=nodes[i].replace(\"\\n\",\"\")\n\t        if(pod==\"NAME\"):\n", "            continue\n\t        id=os.popen(\"kubectl exec -i \"+pod+\" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n\t        print(pod,id)\n\t        idtarget=\"^'\"+id+\":\"+\" \"+\"'\"\n\t        if(node==\"cpu-06\"):\n\t            temp=os.popen(\"ip link show | grep \"+idtarget+\" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n\t        else:\n\t            temp=os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget+\"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n\t        index=temp.find('@')\n\t        iplink=temp[:index]\n", "        temp=pod.split(\"-\")\n\t        extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n\t        value=pod.replace(extra,\"\")\n\t        if(value==\"memcached-reservation\"):\n\t            value=\"memcached-reserve\"\n\t        ms_interace_mapper[iplink]=value\n\t        PodNodeMapper[value]=node\n\tdef getTraffic():\n\t    global ms_interace_mapper\n\t    traffic_mapper={}\n", "    res=[]\n\t    for k,v in NodeIpMapper.items():\n\t        res_rpc=rpc_get_net_proc(v)\n\t        res.extend(res_rpc)\n\t    for i in range(2,len(res)):\n\t        line=res[i].replace(\"\\n\",\"\").split(\" \")\n\t        while '' in line:\n\t            line.remove('')\n\t        interface=line[0].replace(\":\",\"\")\n\t        if(interface in ms_interace_mapper.keys()):\n", "            if(ms_interace_mapper[interface] not in traffic_mapper.keys()):\n\t                traffic_mapper[ms_interace_mapper[interface]]=[int(line[9]),int(line[1])]\n\t            else:\n\t                traffic_mapper[ms_interace_mapper[interface]][0]+=int(line[9])\n\t                traffic_mapper[ms_interace_mapper[interface]][1]+=int(line[1])\n\t    return traffic_mapper\n\tdef calTrafficRate(traffic_mapper_old,traffic_mapper_new,duration):\n\t    this_sample_rate={}\n\t    for key in traffic_mapper_old:\n\t        in_traffic_rate=(traffic_mapper_new[key][0]-traffic_mapper_old[key][0])/duration/1000000*8\n", "        out_traffic_rate=(traffic_mapper_new[key][1]-traffic_mapper_old[key][1])/duration/1000000*8\n\t        this_sample_rate[key]=[in_traffic_rate,out_traffic_rate,duration]\n\t    return this_sample_rate\n\tdef calUpperandBacktrafic(this_Access_Record,this_sample_rate):\n\t    for target in this_Access_Record:\n\t        target.in_traffic=this_sample_rate[target.name][0]\n\t        target.out_traffic=this_sample_rate[target.name][1]\n\t        target.upper_traffic=target.in_traffic\n\t        target.back_traffic=target.out_traffic\n\t        for child in target.children:\n", "            target.upper_traffic-=child.back_traffic\n\t            target.back_traffic-=child.upper_traffic\n\t    return this_Access_Record\n\tdef runTrafficMonitor(Access_Record):\n\t    traffic_mapper_old=getTraffic()\n\t    t_old=time.time()\n\t    while(True):\n\t        time.sleep(1)\n\t        traffic_mapper_new=getTraffic()\n\t        t_new=time.time()\n", "        this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n\t        this_Access_Record=calUpperandBacktrafic(copy.deepcopy(Access_Record),this_sample_rate)\n\t        for i in range(len(this_Access_Record)):\n\t            print(this_Access_Record[i].name,this_Access_Record[i].upper_traffic,this_sample_rate['entering-ms'][2])\n\t        print()\n\t        traffic_mapper_old=traffic_mapper_new\n\t        t_old=t_new\n\tdef runOneTime():\n\t    root=buildDAG()\n\t    Access_Record=[]\n", "    postOrder(Access_Record,root)\n\t    ms_interace()\n\t    traffic_mapper_old=getTraffic()\n\t    t_old=time.time()\n\t    time.sleep(5)\n\t    traffic_mapper_new=getTraffic()\n\t    t_new=time.time()\n\t    this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n\t    this_Access_Record=calUpperandBacktrafic(copy.deepcopy(Access_Record),this_sample_rate)\n\t    return this_Access_Record\n", "if __name__ == \"__main__\":\n\t    root=buildDAG()\n\t    Access_Record=[]\n\t    postOrder(Access_Record,root)\n\t    for target in Access_Record:\n\t        print(target.name)\n\t    ms_interace()\n\t    runTrafficMonitor(Access_Record)\n"]}
{"filename": "LoadMonitor/NetTrafficMonitor.py", "chunked_list": ["import os\n\timport time\n\timport copy\n\timport math\n\timport sys\n\tsys.path.append(\"..\") \n\tfrom client.client import rpc_get_net_proc\n\ttraffic_mapper_old={}\n\tt_old=0\n\t#get traffic data based on network interface\n", "#every 1 second\n\tdef getTraffic(ms_dag):\n\t    ms_interface_mapper=ms_dag.ms_interface_mapper\n\t    traffic_mapper={}\n\t    res=[]\n\t    for k,v in ms_dag.NodeIpMapper.items():\n\t        res_rpc=rpc_get_net_proc(v)\n\t        res.extend(res_rpc)\n\t    for i in range(2,len(res)):\n\t        line=res[i].replace(\"\\n\",\"\").split(\" \")\n", "        while '' in line:\n\t            line.remove('')\n\t        interface=line[0].replace(\":\",\"\")\n\t        if(interface in ms_interface_mapper.keys()):\n\t            if(ms_interface_mapper[interface] not in traffic_mapper.keys()):\n\t                traffic_mapper[ms_interface_mapper[interface]]=[int(line[9]),int(line[1])]\n\t            else:\n\t                traffic_mapper[ms_interface_mapper[interface]][0]+=int(line[9])\n\t                traffic_mapper[ms_interface_mapper[interface]][1]+=int(line[1])\n\t    return traffic_mapper\n", "#call traffic difference\n\tdef calTrafficRate(traffic_mapper_old,traffic_mapper_new,duration):\n\t    this_sample_rate={}\n\t    for key in traffic_mapper_old:\n\t        in_traffic_rate=(traffic_mapper_new[key][0]-traffic_mapper_old[key][0])/duration/1000000*8\n\t        out_traffic_rate=(traffic_mapper_new[key][1]-traffic_mapper_old[key][1])/duration/1000000*8\n\t        this_sample_rate[key]=[in_traffic_rate,out_traffic_rate,duration]\n\t    return this_sample_rate\n\t#Post order update based on structure\n\tdef calUpperandBacktrafic(ms_dag,this_sample_rate):\n", "    for target in ms_dag.Access_Record:\n\t        target.in_traffic=this_sample_rate[target.name][0]\n\t        target.out_traffic=this_sample_rate[target.name][1]\n\t        target.upper_traffic=target.in_traffic\n\t        target.back_traffic=target.out_traffic\n\t        for child in target.children:\n\t            target.upper_traffic-=child.back_traffic\n\t            target.back_traffic-=child.upper_traffic\n\t            #deal with fluncations\n\t            if(target.upper_traffic<0):\n", "                target.upper_traffic=0\n\t            if(target.back_traffic<0):\n\t                target.back_traffic=0\n\t#For initialization\n\tdef initialize(ms_dag):\n\t    global traffic_mapper_old,t_old\n\t    traffic_mapper_old=getTraffic(ms_dag)\n\t    t_old=time.time()\n\t    print(t_old,traffic_mapper_old)\n\t    # print(\"==========\")\n", "def runOneTime(ms_dag):\n\t    global t_old,traffic_mapper_old\n\t    # print(t_old,traffic_mapper_old)\n\t    traffic_mapper_new=getTraffic(ms_dag)\n\t    t_new=time.time()\n\t    # print(t_new,traffic_mapper_new)\n\t    this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n\t    # this_Access_Record=calUpperandBacktrafic(copy.deepcopy(ms_dag.Access_Record),this_sample_rate)\n\t    calUpperandBacktrafic(ms_dag,this_sample_rate)\n\t    # print(ms_dag.Access_Record[11].upper_traffic,this_sample_rate['entering-ms'][2])\n", "    # update the last record\n\t    t_old=t_new\n\t    traffic_mapper_old=traffic_mapper_new\n\t    return this_sample_rate['entering-ms'][2]\n\t#For separable test\n\tdef runTrafficMonitor(Access_Record):\n\t    traffic_mapper_old=getTraffic()\n\t    t_old=time.time()\n\t    while(True):\n\t        time.sleep(1)\n", "        traffic_mapper_new=getTraffic()\n\t        t_new=time.time()\n\t        this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n\t        this_Access_Record=calUpperandBacktrafic(copy.deepcopy(Access_Record),this_sample_rate)\n\t        # obtain upper traffic\n\t        print(this_Access_Record[11].name,this_Access_Record[11].upper_traffic,this_sample_rate['entering-ms'][2])\n\t        # temp_Record.append(this_Access_Record[11].upper_traffic)\n\t        traffic_mapper_old=traffic_mapper_new\n\t        t_old=t_new\n\t    # print(len(temp_Record))"]}
