{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t    name=\"seqrecord\",\n\t    version=\"0.1.8\",\n\t    description=\"various bugs fix\",\n\t    author_email=\"shuhang0chen@gmail.com\",\n\t    maintainer_email=\"shuhang0chen@gmail.com\",\n\t    packages=find_packages(),\n\t    install_requires=[\"numpy\", \"torch\", \"torchdata\", \"aiofiles\"],\n\t)\n"]}
{"filename": "seqrecord/__init__.py", "chunked_list": ["from .weather.seqrecord import WSeqRecord\n"]}
{"filename": "seqrecord/utils.py", "chunked_list": ["from collections import OrderedDict\n\timport io\n\timport os\n\tfrom typing import Callable, List, Tuple\n\tPRODUCER_SLEEP_INTERVAL = 0.0001  # Interval between buffer fullfilment checks\n\tCONSUMER_SLEEP_INTERVAL = (\n\t    0.0001  # Interval between checking items availablitity in buffer\n\t)\n\tdef distribute_loads(works: int, num_processes: int) -> List[Tuple[int, int]]:\n\t    \"\"\"Given the overall works and number of processes, allocate evenly the loads that each process should take.\n", "    Args:\n\t        works (int): amount of over all work\n\t        num_processes (int): number of processes available\n\t    Returns:\n\t        List[Tuple[int, int]]: indices of work each process is responsible for\n\t    \"\"\"\n\t    assert (\n\t        works >= num_processes\n\t    ), \"The amount of works is less than number of processes.\"\n\t    ans = []\n", "    start = 0\n\t    works_remain = works\n\t    loads_per_process = round(works / num_processes)\n\t    for i in range(num_processes):\n\t        if works_remain % (num_processes - i) == 0:\n\t            loads_per_process = works_remain // (num_processes - i)\n\t        else:\n\t            loads_per_process = round(works_remain / (num_processes - i))\n\t        end = min(start + loads_per_process, works)\n\t        ans.append((start, end))\n", "        works_remain -= end - start\n\t        start = end\n\t    return ans\n\tclass LRUCache:\n\t    \"\"\"LRU cache using OrderedDict\n\t    Returns:\n\t        _type_: _description_\n\t    \"\"\"\n\t    def __init__(self, capacity: int, monitoring: bool = False) -> None:\n\t        self.capacity = capacity\n", "        self.values = OrderedDict()  # Dict[int, io.BufferedReader] = {}\n\t        self.monitoring = monitoring\n\t        if self.monitoring:\n\t            self.hits = 0\n\t            self.misses = 0\n\t    def put(self, key: int, value):\n\t        \"\"\"add (key, value) to cache following lru policy using ordered dict\n\t        Args:\n\t            ele (_type_): _description_\n\t        Returns:\n", "            evicted element in case there is post-processing needed.\n\t        \"\"\"\n\t        evicted = None\n\t        if key not in self.values:\n\t            if len(self.values) == self.capacity:\n\t                # popitem() returns key, value pair\n\t                _, evicted = self.values.popitem(last=False)\n\t        else:\n\t            self.values.pop(key)\n\t        self.values[key] = value\n", "        return evicted\n\t    def get(self, key: int):\n\t        \"\"\"retrieve element from cache\n\t        Args:\n\t            key (_type_): _description_\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        res = None\n\t        if key in self.values:\n", "            self.values[key] = self.values.pop(key)\n\t            res = self.values[key]\n\t        if self.monitoring:\n\t            self.hits = self.hits + (res is not None)\n\t            self.misses = self.misses + (res is None)\n\t        return res\n\t    def pop(self, key: int):\n\t        return self.values.pop(key)\n\t    def keys(self):\n\t        return [key for key in self.values.keys()]\n", "    def stats(self) -> str:\n\t        if self.monitoring:\n\t            return f\"Cache hitting rate is {self.hits / (self.hits + self.misses)} with overall number of reads {self.hits + self.misses}\"\n\t        else:\n\t            return \"Monitoring was not enabled for cache, hence no stats available\"\n\tclass FileManager:\n\t    \"\"\"manager opening and closing of file descriptors, with a LRU Cache for opened files (remote files, connection).\"\"\"\n\t    # todo: add stats for monitoring cache miss and hits\n\t    def __init__(\n\t        self,\n", "        cache_capacity: int,\n\t        monitoring: bool = False,\n\t    ) -> None:\n\t        self.cache = LRUCache(cache_capacity, monitoring)\n\t    def open_file(self, file_idx: int, file_path: str) -> io.BufferedReader:\n\t        f = self.cache.get(file_idx)\n\t        if f is None:\n\t            # todo: make this a variable or function or something fixed\n\t            file_path = file_path\n\t            f = open(file_path, \"rb\")\n", "            evicted = self.cache.put(key=file_idx, value=f)\n\t            if evicted is not None:\n\t                evicted.close()\n\t        return f\n\t    def close_all_files(self) -> None:\n\t        \"\"\"Close all open file descriptors in cache\"\"\"\n\t        for key in self.cache.keys():\n\t            self.cache.pop(key).close()\n\t        return\n\tclass WriterBuffer:\n", "    def __init__(self) -> None:\n\t        self.buffer = io.BytesIO()\n\t    def write(self, buffer) -> None:\n\t        self.buffer.write(buffer)\n\t    def is_empty(self) -> bool:\n\t        return self.buffer.tell() == 0\n\t    def getbuffer(self) -> memoryview:\n\t        return self.buffer.getbuffer()\n\t    def getvalue(self):\n\t        return self.buffer.getvalue()\n", "    def clear(self) -> None:\n\t        self.buffer.flush()\n\t        self.buffer.seek(0)\n\t    def close(self) -> None:\n\t        self.buffer.close()\n\tclass TimeTracker:\n\t    def __init__(self) -> None:\n\t        self.time: float = 0.0\n\t        self.nbytes: int = 0\n\t    def add(self, time: float, nbytes: int) -> None:\n", "        self.time += time\n\t        self.nbytes += nbytes\n\t    def summarize(self) -> str:\n\t        return f\"Took {self.time} to read {self.nbytes} bytes, with average rate {self.nbytes / self.time} bytes/s\"\n"]}
{"filename": "seqrecord/robot/datapipes.py", "chunked_list": ["\"\"\"Iterative datapipes built from seqrecord\"\"\"\n\tfrom typing import Callable, Dict, List, Optional\n\timport numpy as np\n\timport torch\n\timport torchdata.datapipes as dp\n\tfrom tqdm import tqdm\n\tfrom .seqrecord import SeqRecord\n\t@dp.functional_datapipe(\"video_datapipe\")\n\tclass VideoDatapipeFromSeqRecord(dp.iter.IterDataPipe):\n\t    \"\"\"A torch datapiple class that iteratively read video(episode) segment from record files.\"\"\"\n", "    def __init__(\n\t        self,\n\t        record: SeqRecord,\n\t        segment_len: int,\n\t        features_rename: Dict[str, str],\n\t        shuffle_recordfiles: bool = False,\n\t    ) -> None:\n\t        super().__init__()\n\t        self.segmentproto = record.get_proto4segment(\n\t            segment_len, [feature for feature in features_rename]\n", "        )\n\t        self.record = record\n\t        self.features_rename = features_rename\n\t        self.shuffle_recordfiles = shuffle_recordfiles\n\t    def __iter__(self):\n\t        for segment in self.record.read_segments(\n\t            self.segmentproto, shuffle_recordfiles=self.shuffle_recordfiles\n\t        ):\n\t            res = {}\n\t            for feature in self.features_rename:\n", "                res[self.features_rename[feature]] = segment[feature]\n\t            yield res\n\t@dp.functional_datapipe(\"item_datapipe\")\n\tclass ItemDatapipeFromSeqRecord(dp.iter.IterDataPipe):\n\t    \"\"\"A torch datapiple class that iteratively read item (frame) from record files.\"\"\"\n\t    def __init__(\n\t        self,\n\t        record: SeqRecord,\n\t        features_rename: Dict[str, str],\n\t        shuffle_recordfiles: bool = False,\n", "    ) -> None:\n\t        super().__init__()\n\t        self.record = record\n\t        self.features_rename = features_rename\n\t        self.shuffle_recordfiles = shuffle_recordfiles\n\t    def __iter__(self):\n\t        res = {}\n\t        for item in self.record.read_items(\n\t            features=[feature for feature in self.features_rename],\n\t            shuffle_recordfiles=self.shuffle_recordfiles,\n", "        ):\n\t            res = {}\n\t            for feature in self.features_rename:\n\t                res[self.features_rename[feature]] = item[feature]\n\t            yield res\n\tdef collate_fn(batch: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n\t    collated_batch: Dict[str, torch.Tensor] = {}\n\t    for feature in batch[0]:\n\t        collated_batch[feature] = torch.from_numpy(\n\t            np.stack([batch[i][feature] for i in range(len(batch))], axis=0)\n", "        )\n\t    return collated_batch\n\tdef list2array(data_list: Dict[str, List[np.ndarray]]) -> Dict[str, np.ndarray]:\n\t    \"\"\"transform data from list of np.array to a single numpy array. Only needed for video datapipes.\n\t    Args:\n\t        data_np (Dict[str, List[np.ndarray]]): _description_\n\t    Returns:\n\t        Dict[str, np.ndarray]: _description_\n\t    \"\"\"\n\t    data_array: Dict[str, np.ndarray] = {}\n", "    for feature in data_list:\n\t        data_array[feature] = np.stack(data_list[feature], axis=0)\n\t    return data_array\n\tdef build_datapipes(\n\t    datapipe: dp.iter.IterDataPipe,\n\t    shuffle_buffer_size: Optional[int],\n\t    batch_size: int,\n\t    mappings: List[Callable],\n\t) -> dp.iter.IterDataPipe:\n\t    \"\"\"Iteratively apply operations to datapipe: shuffle, sharding, map, batch, collator\n", "    Args:\n\t        datapipe (dp.datapipe.IterDataPipe): entry datapipe\n\t        shuffle_buffer_size (Optional[int]): buffer size for pseudo-shuffle\n\t        batch_size (int):\n\t        mappings (List[Callable]): a list of transforms applied to datapipe, between sharding and batch\n\t    Returns:\n\t        dp.datapipe.IterDataPipe: transformed datapipe ready to be sent to dataloader\n\t    \"\"\"\n\t    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n\t    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n", "    if shuffle_buffer_size is not None:\n\t        datapipe = datapipe.shuffle(buffer_size=shuffle_buffer_size)\n\t    # sharding: Place ShardingFilter (datapipe.sharding_filter) as early as possible in the pipeline,\n\t    # especially before expensive operations such as decoding, in order to avoid repeating these expensive operations across worker/distributed processes.\n\t    datapipe = datapipe.sharding_filter()\n\t    for i, mapping in enumerate(mappings):\n\t        datapipe = datapipe.map(fn=mapping)\n\t    # Note that if you choose to use Batcher while setting batch_size > 1 for DataLoader,\n\t    # your samples will be batched more than once. You should choose one or the other.\n\t    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n", "    datapipe = datapipe.batch(batch_size=batch_size, drop_last=True)\n\t    datapipe = datapipe.collate(collate_fn=collate_fn)\n\t    return datapipe\n"]}
{"filename": "seqrecord/robot/__init__.py", "chunked_list": []}
{"filename": "seqrecord/robot/seqrecord.py", "chunked_list": ["\"\"\"A package for decoding and encoding each item in data file.\"\"\"\n\timport collections\n\timport io\n\timport os\n\timport pickle\n\tfrom typing import (\n\t    Any,\n\t    BinaryIO,\n\t    Dict,\n\t    Generator,\n", "    List,\n\t    Optional,\n\t    Sequence,\n\t    Tuple,\n\t    TypeVar,\n\t    Union,\n\t    Deque,\n\t)\n\timport numpy as np\n\timport yaml\n", "from seqrecord.utils import WriterBuffer\n\timport copy\n\tfrom collections import deque\n\timport time\n\tfrom seqrecord.utils import PRODUCER_SLEEP_INTERVAL, CONSUMER_SLEEP_INTERVAL\n\timport threading\n\tMAX_RECORDFILE_SIZE = 1e9  # 1e8, 100 mb, maximum size of a single record file\n\tRSR = TypeVar(\"RSR\", bound=\"RSeqRecord\")\n\tdef recordfileidx2path(recorddir, recordfile_idx: int) -> str:\n\t    return os.path.join(recorddir, f\"records_{recordfile_idx}.bin\")\n", "# todo: add metadata for each episode\n\tclass RSeqRecord:\n\t    \"\"\"A serialization protocal that stores sequences of episodes into record files, while provides metadata of each episode/frame\n\t    to read segments from dataset.\"\"\"\n\t    def __init__(\n\t        self,\n\t        recorddir: str,\n\t    ) -> None:\n\t        # folder that stores data in a separate directory (subfolder)\n\t        self.recorddir: str = recorddir\n", "        os.makedirs(self.recorddir, exist_ok=True)\n\t        self.features_written = None\n\t        self.num_bytes: int = 0  # number of bytes written into current record file\n\t        self.file_idx: int = 0  # number of record file created for dataset\n\t        # track the idx endpoints for each record file, [[start_idx, end_idx]], both are inclusive\n\t        self.idx_range_of_files: List[Tuple[int, int]] = []\n\t        # file object for current record file\n\t        self.file_desc: Optional[BinaryIO] = None\n\t        # serialization proto info of each data item\n\t        self.metadata: Dict[int, dict] = {}\n", "        # index of current data item to be processed\n\t        self.frame_idx: int = 0\n\t        # a cache dict that stores protocal info for each (segment_len, sub features)\n\t        self.metadata_segment_cache: Dict[str, dict] = {}\n\t        self.write_buffer = WriterBuffer()\n\t    def get_recordfiles(self) -> List[str]:\n\t        return [self.recordfile_idx_to_path(i) for i in range(self.file_idx)]\n\t    def write_item(\n\t        self,\n\t        frame: Dict[str, np.ndarray],\n", "        is_seq_start: bool,\n\t    ) -> None:\n\t        \"\"\"write one item data dict(feature->np.ndarray) into bytes and write encoded bytes into\n\t        current record files.\n\t        Args:\n\t            item (Dict[str, np.ndarray]): feature to data (np.ndarray)\n\t            is_seq_start (bool): denote if the item is the beginning of a sequence\n\t        \"\"\"\n\t        if self.features_written is None:\n\t            self.features_written = [key for key in frame]\n", "        if is_seq_start:\n\t            self.seq_start()\n\t        # get file name and start position for item\n\t        self.metadata[self.frame_idx] = {\n\t            \"frame_idx\": self.frame_idx,\n\t            \"file_idx\": self.file_idx,\n\t            \"bytes_offset\": self.num_bytes,\n\t            \"is_seq_start\": is_seq_start,\n\t        }\n\t        num_bytes_in_frame = 0\n", "        for feature, data in frame.items():\n\t            self.metadata[self.frame_idx][feature] = {\n\t                \"is_none\": (\n\t                    data.dtype == np.dtype(\"O\") and data == None\n\t                ),  # this feature is essentially missing, and\n\t                \"dtype\": data.dtype,\n\t                \"shape\": data.shape,\n\t                \"bytes_offset\": num_bytes_in_frame,\n\t                \"nbytes\": data.nbytes,\n\t            }\n", "            self.write_buffer.write(data.tobytes())\n\t            num_bytes_in_frame += data.nbytes\n\t        self.num_bytes += num_bytes_in_frame\n\t        self.frame_idx += 1\n\t        return\n\t    def seq_start(self) -> None:\n\t        \"\"\"Notify the record that a new sequence is being written, let the record decide if we need\n\t        a new record file to write into.\n\t        Two cases we need to open new file:\n\t            1. we currently do not have record file to write into\n", "            2. current file size is big enough (larger than MAX_RECORDFILE_SIZE)\n\t        \"\"\"\n\t        if self.num_bytes > MAX_RECORDFILE_SIZE:\n\t            # current record file big enough\n\t            self.num_bytes = 0\n\t            self.file_idx += 1\n\t            self.idx_range_of_files[-1].append(self.frame_idx - 1)\n\t            self.idx_range_of_files.append([self.frame_idx])\n\t            self.file_desc.write(self.write_buffer.getbuffer())\n\t            self.file_desc.flush()\n", "            self.file_desc.close()\n\t            self.write_buffer.clear()\n\t            self.file_desc = open(\n\t                recordfileidx2path(self.recorddir, self.file_idx),\n\t                mode=\"wb\",\n\t            )\n\t        elif self.file_desc == None:\n\t            # no opened record file to write into\n\t            self.idx_range_of_files.append([self.frame_idx])\n\t            self.file_desc = open(\n", "                recordfileidx2path(self.recorddir, self.file_idx), mode=\"wb\"\n\t            )\n\t    def read_frame(\n\t        self,\n\t        file_desc: Union[io.BufferedReader, BinaryIO],\n\t        metadata_frame: Dict[str, Union[int, dict]],\n\t        features: List[str],\n\t    ) -> Dict[str, np.ndarray]:\n\t        \"\"\"Given record file descriptor and serialization proto of a single data item, return the\n\t        decoded dictionary(feature->data(np.ndarray)) of the item.\n", "        Args:\n\t            recordfile_desc (io.BufferedReader): python file object of the record file (required by numpy)\n\t            itemproto (Dict[str, Any]): dict that contains protocal info of a specific data item\n\t        Returns:\n\t            Dict[str, np.ndarray]: data\n\t        \"\"\"\n\t        frame = {}\n\t        frame_offset = metadata_frame[\"bytes_offset\"]\n\t        for feature in features:\n\t            frame[feature] = np.memmap(\n", "                file_desc,\n\t                dtype=metadata_frame[feature][\"dtype\"],\n\t                mode=\"r\",\n\t                offset=frame_offset + metadata_frame[feature][\"bytes_offset\"],\n\t                shape=metadata_frame[feature][\"shape\"],\n\t            )\n\t        # * do we need to close the memmap?\n\t        return frame\n\t    def read_frame_frombuffer(\n\t        self,\n", "        file_desc: Union[io.BufferedReader, BinaryIO],\n\t        metadata_frame: Dict[str, Union[int, dict]],\n\t        features: List[str],\n\t    ) -> Dict[str, np.ndarray]:\n\t        \"\"\"Given record file descriptor and serialization proto of a single data item, return the\n\t        decoded dictionary(feature->data(np.ndarray)) of the item, where decoding is done by\n\t        np.frombuffer()\n\t        Args:\n\t            recordfile_desc (io.BufferedReader): python file object of the record file (required by numpy)\n\t            itemproto (Dict[str, Any]): dict that contains protocal info of a specific data item\n", "        Returns:\n\t            Dict[str, np.ndarray]: data\n\t        \"\"\"\n\t        frame = {}\n\t        file_desc.seek(metadata_frame[\"bytes_offset\"])\n\t        for feature in features:\n\t            bytes = file_desc.read(metadata_frame[feature][\"nbytes\"])\n\t            array1d = np.frombuffer(\n\t                bytes,\n\t                dtype=metadata_frame[feature][\"dtype\"],\n", "            )\n\t            frame[feature] = array1d.reshape(metadata_frame[feature][\"shape\"])\n\t        return frame\n\t    def read_frames(\n\t        self, features: List[str]\n\t    ) -> Generator[Dict[str, np.ndarray], None, None]:\n\t        \"\"\"Given that the dataset has been recored, decode the record sequentially, each time\n\t        returning a dict that contains the data item.\n\t        Args:\n\t            features [List[str]]: a list of features requested to read from item\n", "            shuffle_recordfile: bool: if we shuffle at the record file level when reading items in the record\n\t        Yields:\n\t            Generator[Dict[str, np.ndarray], None, None]: data item [feature->data]. All data items are being returned sequentially\n\t        \"\"\"\n\t        recordfiles = list(range(self.file_idx))\n\t        for i in recordfiles:\n\t            recordfile_path = recordfileidx2path(self.recorddir, i)\n\t            endpoints = self.idx_range_of_files[i]\n\t            with open(recordfile_path, mode=\"rb\") as f:\n\t                for idx in range(endpoints[0], endpoints[1] + 1):\n", "                    item = self.read_frame(f, self.metadata[idx], features)\n\t                    yield {feature: item[feature] for feature in features}\n\t    def get_metadata4segment(\n\t        self, segment_len: int, sub_features: Optional[List[str]] = None\n\t    ) -> Dict[str, Any]:\n\t        \"\"\"Generate a protocal for reading segments from records. Each data item of segment should\n\t        contain all features in sub_features.\n\t        Note:\n\t        Only call this function when record has scanned all data in dataset, and record has valid attributes: rootdir, recordfile_idx\n\t        Args:\n", "            segment_len (int): length of segment we are reading, 1< segment_len < sequence length\n\t            sub_features: (Optional[List[str]]): features (modalities data) we need for each data item in segment to contain. If it is None,\n\t            then we read all features.\n\t        Returns:\n\t            Dict[str, Any]: protocal needed for reading segments from data\n\t        \"\"\"\n\t        def has_sub_features(itemproto: Dict[str, Any]) -> bool:\n\t            return all(not itemproto[feature][\"is_none\"] for feature in sub_features)\n\t        def update_segmentproto(item_idx: int, is_segment_start: bool) -> None:\n\t            if is_segment_start:\n", "                head4segment.append(item_idx)\n\t            recordfile_idx = self.metadata[item_idx][\"file_idx\"]\n\t            file2segment_items[recordfile_idx].append((is_segment_start, item_idx))\n\t            return\n\t        if sub_features is None:\n\t            sub_features = list(self.features_written)\n\t        else:\n\t            assert all(\n\t                feature in self.features_written for feature in sub_features\n\t            ), \"Unknow features requested\"\n", "        cache_key = str(segment_len) + \"#\" + \"#\".join(sorted(sub_features))\n\t        if cache_key in self.metadata_segment_cache:\n\t            return self.metadata_segment_cache[cache_key]\n\t        head4segment: List[int] = []\n\t        file2segment_items: dict[int, List[Tuple[bool, int]]] = collections.defaultdict(\n\t            list\n\t        )\n\t        q = collections.deque()\n\t        q_has_seg_tail = False  # indicates if the elements currently in queue are tail of some segment\n\t        for idx in range(self.frame_idx):\n", "            itemproto = self.metadata[idx]\n\t            if (not has_sub_features(itemproto)) or (itemproto[\"is_seq_start\"]):\n\t                # new seq start\n\t                while q:\n\t                    if q_has_seg_tail:\n\t                        update_segmentproto(q.popleft(), is_segment_start=False)\n\t                    else:\n\t                        q.popleft()\n\t                q_has_seg_tail = False\n\t                if has_sub_features(itemproto):\n", "                    # a valid start of sequence\n\t                    q.append(idx)\n\t            else:\n\t                q.append(idx)\n\t                if len(q) == segment_len:\n\t                    # claim: elements in the queue must be from the same sequence\n\t                    update_segmentproto(q.popleft(), is_segment_start=True)\n\t                    q_has_seg_tail = True\n\t        if q and q_has_seg_tail:\n\t            # front element in queue is need as last element of some segment\n", "            update_segmentproto(q.popleft(), is_segment_start=False)\n\t        # 1. new seq (including broken) added before queue pops out\n\t        #       the remaining elements in queue are completely useless\n\t        # 2. new seq (including broken) added after queue has popped out\n\t        #       the remaining elements are not start of segment but are tails of some segment\n\t        self.metadata_segment_cache[cache_key] = {\n\t            \"segment_len\": segment_len,\n\t            \"features\": sub_features,\n\t            \"head4segment\": head4segment,\n\t            \"file2segment_items\": file2segment_items,\n", "        }\n\t        return self.metadata_segment_cache[cache_key]\n\t    def read_segments(self, segment_proto: dict):\n\t        \"\"\"Iterate through the whole records and return segments sequential.\n\t        Yields:\n\t            segment_proto: info on in given segment_len and features\n\t        \"\"\"\n\t        segment_len = segment_proto[\"segment_len\"]\n\t        recordfile_ids = list(segment_proto[\"file2segment_items\"].keys())\n\t        for recordfile_idx in recordfile_ids:\n", "            item_list = segment_proto[\"file2segment_items\"][recordfile_idx]\n\t            recordfile_path = recordfileidx2path(self.recorddir, recordfile_idx)\n\t            q = collections.deque()\n\t            with open(recordfile_path, mode=\"rb\") as f:\n\t                for is_segment_start, item_idx in item_list:\n\t                    q.append(\n\t                        (\n\t                            is_segment_start,\n\t                            self.read_frame(\n\t                                f, self.metadata[item_idx], segment_proto[\"features\"]\n", "                            ),\n\t                        )\n\t                    )\n\t                    while not q[0][0]:\n\t                        q.popleft()\n\t                    if len(q) == segment_len:\n\t                        yield self.collate_items(q)\n\t                        q.popleft()\n\t    def read_one_segment(\n\t        self,\n", "        segment_len: int,\n\t        head_idx: int,\n\t    ) -> Dict[str, List[np.ndarray]]:\n\t        \"\"\"Read a segment (of lenght segment_len) starting from the item index being head_idx.\n\t        Args:\n\t            segment_len (int): length of segment we need to generate\n\t            head_idx (int): item_idx of the head of the segment to be read.\n\t        Returns:\n\t            Dict[str, np.ndarray]: segment data\n\t        \"\"\"\n", "        recordfile_path = recordfileidx2path(\n\t            self.recorddir, self.metadata[head_idx][\"recordfile_idx\"]\n\t        )\n\t        q = []\n\t        with open(recordfile_path, mode=\"rb\") as f:\n\t            for idx in range(head_idx, head_idx + segment_len):\n\t                q.append(\n\t                    (\n\t                        idx == head_idx,\n\t                        self.read_frame_frombuffer(f, self.metadata[idx]),\n", "                    )\n\t                )\n\t        return self.collate_items(q)\n\t    def collate_items(\n\t        self, q: Sequence[Tuple[bool, dict]]\n\t    ) -> Dict[str, List[np.ndarray]]:\n\t        segment = {}\n\t        features = q[0][1].keys()\n\t        for feature in features:\n\t            segment[feature] = [item[feature] for _, item in q]\n", "        return segment\n\t    def close_recordfile(self):\n\t        \"\"\"Close opened file descriptor!\n\t        This needs to be called when finishes scanning over the dataset.\n\t        \"\"\"\n\t        self.idx_range_of_files[-1].append(self.frame_idx - 1)\n\t        self.file_desc.write(self.write_buffer.getbuffer())\n\t        self.write_buffer.close()\n\t        self.write_buffer = None\n\t        self.file_desc.flush()\n", "        self.file_desc.close()\n\t        self.file_idx += 1\n\t        self.file_desc = None\n\t    def dump(self) -> None:\n\t        \"\"\"save attributes of instance of record into a file.\n\t        Note:\n\t        saving attribute dict instead of pickled class: pickling class and loading it is a mess because of\n\t        path issues.\n\t        \"\"\"\n\t        dic = copy.deepcopy(self.__dict__)\n", "        with open(os.path.join(self.recorddir, \"record.dict\"), mode=\"wb\") as f:\n\t            pickle.dump(dic, file=f)\n\t        # save some attributes of the seqrecord to yaml for human inspection\n\t        dic[\"metadata_segment_cache\"] = None\n\t        for key, val in dic[\"metadata\"].items():\n\t            for feature in dic[\"features_written\"]:\n\t                val[feature][\"dtype\"] = val[feature][\"dtype\"].str\n\t                val[feature][\"shape\"] = list(val[feature][\"shape\"])\n\t        with open(os.path.join(self.recorddir, \"record_dict.yaml\"), mode=\"w\") as f:\n\t            f.write(\"# Configs for human inspection only!\\n\")\n", "            f.write(yaml.dump(dic))\n\t    @classmethod\n\t    def load_record_from_dict(cls, recorddir: str) -> RSR:\n\t        \"\"\"return an instance of sequence record from file that stores attributes of record as a\n\t        dict (stored at path).\n\t        Args:\n\t            path (str): path to the file that stores dict of attributes of seqrecord\n\t        Returns:\n\t            SR: an instance of record\n\t        \"\"\"\n", "        file_path = os.path.join(recorddir, \"record.dict\")\n\t        with open(file_path, mode=\"rb\") as f:\n\t            obj_dict = pickle.load(f)\n\t        obj = cls(\n\t            recorddir=recorddir,\n\t        )\n\t        obj_dict.pop(\"recorddir\", None)\n\t        for key, value in obj_dict.items():\n\t            setattr(obj, key, value)\n\t        return obj\n"]}
{"filename": "seqrecord/robot/tests/test_seqrecord.py", "chunked_list": ["\"\"\"Unit test for seqrecord's functionalities\"\"\"\n\timport unittest\n\tfrom typing import Dict, List\n\timport numpy as np\n\timport numpy.testing as nptest\n\tfrom seqrecord.robot.seqrecord import RSeqRecord\n\tdef concate_list(file2segment_item: Dict[str, list]):\n\t    res = []\n\t    for key in sorted(file2segment_item):\n\t        res = res + file2segment_item[key]\n", "    return res\n\tclass Test_RSeqRecord(unittest.TestCase):\n\t    def test_encode_decode(self):\n\t        \"\"\"Testing encode and decode of items, no segment involved.\"\"\"\n\t        record, dataset, features = build_simple_dataset()\n\t        # encode dataset\n\t        for i, item in enumerate(dataset):\n\t            if i % 4 == 0:\n\t                # mock start of a sequence\n\t                record.write_item(item, True)\n", "            else:\n\t                record.write_item(item, False)\n\t        record.close_recordfile()\n\t        record.dump()\n\t        # decode dataset\n\t        for i, item in enumerate(record.read_frames(features=features)):\n\t            for feature in features:\n\t                nptest.assert_equal(\n\t                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n\t                )\n", "        loaded_record = RSeqRecord.load_record_from_dict(\"./output/seqrecord_test/\")\n\t    def test_idx4segment(self):\n\t        \"\"\"Having the record written (and various attributes setup), generate an index protocal for\n\t        specific segment len.\"\"\"\n\t        record, dataset, features = build_simple_dataset()\n\t        seq_len = 4\n\t        # encode dataset\n\t        for i, item in enumerate(dataset):\n\t            if i % seq_len == 0:\n\t                # mock start of a sequence\n", "                record.write_item(item, True)\n\t            else:\n\t                record.write_item(item, False)\n\t        record.close_recordfile()\n\t        # segment len =2, sequence len =4, full features\n\t        seg_len = 2\n\t        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n\t        items = concate_list(idx4segment[\"file2segment_items\"])\n\t        self.assertEqual(len(items), 10)\n\t        ids = [item_idx for _, item_idx in items]\n", "        self.assertListEqual(ids, list(range(10)))\n\t        for i, (is_segment_start, item) in enumerate(items):\n\t            if i in [0, 1, 2, 4, 5, 6, 8]:\n\t                self.assertTrue(is_segment_start)\n\t            else:\n\t                self.assertFalse(is_segment_start)\n\t        heads = idx4segment[\"head4segment\"]\n\t        for i, segment in enumerate(record.read_segments(idx4segment)):\n\t            for j in range(seg_len):\n\t                for feature in features:\n", "                    nptest.assert_equal(\n\t                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n\t                        err_msg=\"\",\n\t                        verbose=True,\n\t                    )\n\t        # segment len =4, sequence len =4, full features\n\t        seg_len = 4\n\t        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n\t        items = concate_list(idx4segment[\"file2segment_items\"])\n", "        ids = [item_idx for _, item_idx in items]\n\t        self.assertEqual(len(ids), 8)\n\t        ids = [item_idx for _, item_idx in items]\n\t        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n\t        for i, (is_segment_start, item) in enumerate(items):\n\t            if i in [0, 4]:\n\t                self.assertTrue(is_segment_start)\n\t            else:\n\t                self.assertFalse(is_segment_start)\n\t        heads = idx4segment[\"head4segment\"]\n", "        for i, segment in enumerate(record.read_segments(idx4segment)):\n\t            for j in range(seg_len):\n\t                for feature in features:\n\t                    nptest.assert_equal(\n\t                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n\t                        err_msg=\"\",\n\t                        verbose=True,\n\t                    )\n\t        # segment len =3, sequence len =4, full feature\n", "        seg_len = 3\n\t        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n\t        self.assertEqual(len(ids), 8)\n\t        items = concate_list(idx4segment[\"file2segment_items\"])\n\t        ids = [item_idx for _, item_idx in items]\n\t        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n\t        for i, (is_segment_start, item) in enumerate(items):\n\t            if i in [0, 1, 4, 5]:\n\t                self.assertTrue(is_segment_start)\n\t            else:\n", "                self.assertFalse(is_segment_start)\n\t        heads = idx4segment[\"head4segment\"]\n\t        for i, segment in enumerate(record.read_segments(idx4segment)):\n\t            for j in range(seg_len):\n\t                for feature in features:\n\t                    nptest.assert_equal(\n\t                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n\t                        err_msg=\"\",\n\t                        verbose=True,\n", "                    )\n\t    def test_idx4segment_brokenfeatures(self):\n\t        \"\"\"Test the idx4segment with some features from dataset missing.\"\"\"\n\t        # segment len = 3, sequence len =4, break two features\n\t        record, dataset, features = build_broken_dataset([3, 4])\n\t        seq_len = 4\n\t        # encode dataset\n\t        for i, item in enumerate(dataset):\n\t            if i % seq_len == 0:\n\t                # mock start of a sequence\n", "                record.write_item(item, True)\n\t            else:\n\t                record.write_item(item, False)\n\t        record.close_recordfile()\n\t        seg_len = 3\n\t        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n\t        items = concate_list(metadata_segment[\"file2segment_items\"])\n\t        ids = [item_idx for _, item_idx in items]\n\t        self.assertEqual(len(ids), 6)\n\t        self.assertListEqual(ids, [0, 1, 2, 5, 6, 7])\n", "        for i, (is_segment_start, item) in enumerate(items):\n\t            if item in [0, 5]:\n\t                self.assertTrue(is_segment_start)\n\t            else:\n\t                self.assertFalse(is_segment_start)\n\t        heads = metadata_segment[\"head4segment\"]\n\t        for i, segment in enumerate(record.read_segments(metadata_segment)):\n\t            for j in range(seg_len):\n\t                for feature in features:\n\t                    nptest.assert_equal(\n", "                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n\t                        err_msg=\"\",\n\t                        verbose=True,\n\t                    )\n\t        seg_len = 3\n\t        metadata_segment = record.get_metadata4segment(\n\t            segment_len=seg_len, sub_features=[\"A100\"]\n\t        )\n\t        items = concate_list(metadata_segment[\"file2segment_items\"])\n", "        ids = [item_idx for _, item_idx in items]\n\t        self.assertEqual(len(items), 6)\n\t        self.assertListEqual(ids, [0, 1, 2, 5, 6, 7])\n\t        for i, (is_seg_start, item) in enumerate(items):\n\t            if item in [0, 5]:\n\t                self.assertTrue(is_seg_start)\n\t            else:\n\t                self.assertFalse(is_seg_start)\n\t                seg_len = 3\n\t        heads = metadata_segment[\"head4segment\"]\n", "        for i, segment in enumerate(record.read_segments(metadata_segment)):\n\t            for j in range(seg_len):\n\t                for feature in [\"A100\"]:\n\t                    nptest.assert_equal(\n\t                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n\t                        err_msg=\"\",\n\t                        verbose=True,\n\t                    )\n\t        metadata_segment = record.get_metadata4segment(\n", "            segment_len=seg_len, sub_features=[\"i5\", \"s1\"]\n\t        )\n\t        items = concate_list(metadata_segment[\"file2segment_items\"])\n\t        ids = [item_idx for _, item_idx in items]\n\t        self.assertEqual(len(ids), 8)\n\t        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n\t        for i, (is_seg_start, item) in enumerate(items):\n\t            if item in [0, 1, 4, 5]:\n\t                self.assertTrue(is_seg_start)\n\t            else:\n", "                self.assertFalse(is_seg_start)\n\t        heads = metadata_segment[\"head4segment\"]\n\t        for i, segment in enumerate(record.read_segments(metadata_segment)):\n\t            for j in range(seg_len):\n\t                for feature in [\"i5\", \"s1\"]:\n\t                    nptest.assert_equal(\n\t                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n\t                        err_msg=\"\",\n\t                        verbose=True,\n", "                    )\n\t        # segment len = 3, sequence len =4, break two features\n\t        record, dataset, features = build_broken_dataset([3, 6])\n\t        seq_len = 4\n\t        # encode dataset\n\t        for i, item in enumerate(dataset):\n\t            if i % seq_len == 0:\n\t                # mock start of a sequence\n\t                record.write_item(item, True)\n\t            else:\n", "                record.write_item(item, False)\n\t        record.close_recordfile()\n\t        seg_len = 3\n\t        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n\t        items = concate_list(metadata_segment[\"file2segment_items\"])\n\t        ids = [item_idx for _, item_idx in items]\n\t        self.assertEqual(len(ids), 3)\n\t        self.assertListEqual(ids, [0, 1, 2])\n\t        for i, (is_seg_start, item) in enumerate(items):\n\t            if item in [0, 5]:\n", "                self.assertTrue(is_seg_start)\n\t            else:\n\t                self.assertFalse(is_seg_start)\n\t        heads = metadata_segment[\"head4segment\"]\n\t        for i, segment in enumerate(record.read_segments(metadata_segment)):\n\t            for j in range(seg_len):\n\t                for feature in features:\n\t                    nptest.assert_equal(\n\t                        dataset[heads[i] + j][feature],\n\t                        segment[feature][j],\n", "                        err_msg=\"\",\n\t                        verbose=True,\n\t                    )\n\t        # segment len = 3, sequence len =4, break two features\n\t        record, dataset, features = build_broken_dataset([2, 6])\n\t        seq_len = 4\n\t        # encode dataset\n\t        for i, item in enumerate(dataset):\n\t            if i % seq_len == 0:\n\t                # mock start of a sequence\n", "                record.write_item(item, True)\n\t            else:\n\t                record.write_item(item, False)\n\t        record.close_recordfile()\n\t        seg_len = 3\n\t        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n\t        items = concate_list(metadata_segment[\"file2segment_items\"])\n\t        ids = [item_idx for _, item_idx in items]\n\t        self.assertEqual(len(ids), 0)\n\tdef build_simple_dataset():\n", "    \"\"\"Generate a fake dataset to test methods of Record.\n\t    Returns:\n\t        _type_: _description_\n\t    \"\"\"\n\t    rootdir = \"./output/seqrecord_test/\"\n\t    seq_len = 10\n\t    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    dataset = [{} for _ in range(seq_len)]\n\t    for i in range(seq_len):\n\t        for feature in features:\n", "            shape = (np.random.randint(1, 5), np.random.randint(2, 7))\n\t            dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n\t            dataset[i][feature] = (\n\t                np.random.rand(shape[0], shape[1])\n\t                if dtype == \"float32\"\n\t                else np.ones(shape=shape)\n\t            )\n\t    record = RSeqRecord(rootdir)\n\t    return record, dataset, features\n\tdef build_broken_dataset(feature_is_none_list: List[int]):\n", "    \"\"\"Generate a fake dataset to test methods of SeqRecord where some features does not exist.\n\t    Params:\n\t        feature_is_none_list (List[str]): indices of data-frame that have missing features\n\t    Returns:\n\t        None\n\t    \"\"\"\n\t    rootdir = \"./output/seqrecord_test/\"\n\t    seq_len = 10\n\t    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    dataset = [{} for _ in range(seq_len)]\n", "    for i in range(seq_len):\n\t        for feature in features:\n\t            shape = (np.random.randint(1, 5), np.random.randint(2, 7))\n\t            dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n\t            if feature != \"A100\" or (i not in feature_is_none_list):\n\t                dataset[i][feature] = (\n\t                    np.random.rand(shape[0], shape[1])\n\t                    if dtype == \"float32\"\n\t                    else np.ones(shape=shape)\n\t                )\n", "            else:\n\t                dataset[i][feature] = np.array(None)\n\t    record = RSeqRecord(rootdir)\n\t    return record, dataset, features\n\tdef build_seq_dataset():\n\t    \"\"\"Generate an aritificial dataset to test methods of SeqRecord, w\"\"\"\n\t    rootdir = \"./output/seqrecord_test/\"\n\t    seq_len = 10\n\t    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    dataset = [{} for _ in range(seq_len)]\n", "    for feature in features:\n\t        shape = (np.random.randint(1, 5), np.random.randint(2, 7))\n\t        dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n\t        for i in range(seq_len):\n\t            dataset[i][feature] = (\n\t                np.random.rand(shape[0], shape[1])\n\t                if dtype == \"float32\"\n\t                else np.ones(shape=shape)\n\t            )\n\t    record = RSeqRecord(rootdir)\n", "    return record, dataset, features\n\tif __name__ == \"__main__\":\n\t    np.random.seed(0)\n\t    unittest.main()\n"]}
{"filename": "seqrecord/robot/example/dataset_datapipes.py", "chunked_list": ["from seqrecorder.seqrecord import SeqRecord\n\tfrom seqrecorder.datapipes import VideoDatapipeFromSeqRecord\n\timport numpy as np\n\tfrom typing import Callable, Dict, List, Optional\n\timport torch.utils.data.datapipes as dp\n\timport torch\n\tdef list2array(data_list: Dict[str, List[np.ndarray]]) -> Dict[str, np.ndarray]:\n\t    \"\"\"transform data from list of np.array to a single numpy array.\n\t    Args:\n\t        data_np (Dict[str, List[np.ndarray]]): _description_\n", "    Returns:\n\t        Dict[str, np.ndarray]: _description_\n\t    \"\"\"\n\t    data_array: Dict[str, np.ndarray] = {}\n\t    for feature in data_list:\n\t        data_array[feature] = np.stack(data_list[feature], axis=0)\n\t    return data_array\n\tdef collate_fn(batch: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n\t    collated_batch: Dict[str, torch.Tensor] = {}\n\t    for feature in batch[0]:\n", "        collated_batch[feature] = torch.from_numpy(\n\t            np.stack([batch[i][feature] for i in range(len(batch))], axis=0)\n\t        )\n\t    return collated_batch\n\tdef build_iter_datapipe(\n\t    recorddir, segment_len, features, transform: Optional[Callable]\n\t):\n\t    record = SeqRecord.load_record_from_dict(recorddir)\n\t    datapipe = VideoDatapipeFromSeqRecord(record, segment_len, features)\n\t    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n", "    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n\t    datapipe = dp.iter.Shuffler(datapipe, buffer_size=1000)\n\t    # sharding: Place ShardingFilter (datapipe.sharding_filter) as early as possible in the pipeline,\n\t    # especially before expensive operations such as decoding, in order to avoid repeating these expensive operations across worker/distributed processes.\n\t    datapipe = dp.iter.ShardingFilter(datapipe)\n\t    datapipe = dp.iter.Mapper(datapipe, fn=list2array)\n\t    if transform is not None:\n\t        datapipe = dp.iter.Mapper(datapipe, fn=transform)\n\t    # Note that if you choose to use Batcher while setting batch_size > 1 for DataLoader,\n\t    # your samples will be batched more than once. You should choose one or the other.\n", "    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n\t    datapipe = dp.iter.Batcher(datapipe, batch_size=2, drop_last=True)\n\t    datapipe = dp.iter.Collator(datapipe, collate_fn=collate_fn)\n\t    return datapipe\n\tif __name__ == \"__main__\":\n\t    recorddir = \"./output/recorddataset/\"\n\t    segment_len = 3\n\t    features = [\"image_left\", \"image_right\"]\n\t    datapipe = build_iter_datapipe(recorddir, segment_len, features, None)\n\t    for seq in datapipe:\n", "        print(seq.keys())\n"]}
{"filename": "seqrecord/robot/example/record_dataset.py", "chunked_list": ["\"\"\"Create a dummy dataset and transform it into SeqRecord format.\n\t\"\"\"\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom dataio.seqrecord import SeqRecord\n\tdef encode_dummy_dataset(\n\t    recorddir: str,\n\t) -> None:\n\t    \"\"\"create and transform an artificial dataset to SeqRecord format.\n\t    Args:\n", "        recorddir (str): directory where the seqrecord files will be saved\n\t    \"\"\"\n\t    # attributes of dataset\n\t    num_seq = 10\n\t    seq_len = 7\n\t    features = {\"image_left\": \"RGBImage\", \"image_right\": \"RGBImage\"}\n\t    record = SeqRecord(\n\t        recorddir=recorddir,\n\t        features=features,\n\t        pretransform_module_path=\"dataio.example.dataset_transform\",\n", "    )\n\t    for _ in tqdm(range(num_seq)):\n\t        for j in range(seq_len):\n\t            item = {\n\t                \"image_left\": np.random.rand(224, 224, 3),\n\t                \"image_right\": np.random.rand(224, 224, 3),\n\t            }\n\t            record.write_item(item, (j == 0))\n\t    record.close_recordfile()\n\t    record.dump()\n", "if __name__ == \"__main__\":\n\t    recorddir = \"./output/recorddataset/\"\n\t    encode_dummy_dataset(recorddir)\n"]}
{"filename": "seqrecord/robot/example/dataset_transform.py", "chunked_list": ["\"\"\"Implement modality transform utilities for habitat dataset.\"\"\"\n\tfrom dataclasses import dataclass\n\timport sys\n\tfrom typing import Any, Dict\n\timport numpy as np\n\timport torch\n\tfrom torchvision import transforms\n\tdef instantiate_modal_class(args: Dict[str, Any]) -> Any:\n\t    \"\"\"Instantiates a class [defined in this module] with the given args that contains class name\n\t    and init args.\n", "    Args:\n\t        init: Dict of the form {\"modal\":...,\"$attributes\":...}.\n\t    Returns:\n\t        The instantiated class object.\n\t    \"\"\"\n\t    args_class = getattr(\n\t        sys.modules[__name__], args[\"modal\"]\n\t    )  # get class object from this module by its class name\n\t    return args_class(**args[\"kwargs\"])\n\tclass InputTransform:\n", "    def __init__(self, modal_config: Dict[str, Any]) -> None:\n\t        self.inputs = {}\n\t        self.feature_map = {}\n\t        for key, modal in modal_config.items():\n\t            modal[\"kwargs\"][\"feature\"] = key\n\t            if modal[\"kwargs\"].get(\"feature_in_batch\", None) is not None:\n\t                # this feature is not being sent to model\n\t                self.feature_map[key] = modal[\"kwargs\"][\"feature_in_batch\"]\n\t            self.inputs[key] = instantiate_modal_class(modal)\n\t        return\n", "    def pre_transform(self, x: Dict[str, np.ndarray]):\n\t        \"\"\"apply pre-transform on frame data.\n\t        Args:\n\t            x (Dict[str, np.ndarray]): each value of the dict is one single frame data\n\t        \"\"\"\n\t        res = {}\n\t        for key, value in x.items():\n\t            # static methods can be called from instances:\n\t            # https://docs.python.org/3/library/functions.html#staticmethod\n\t            res[key] = self.inputs[key].pre_transform(value)\n", "        return res\n\t    def transform(\n\t        self, transform_type: str, x: Dict[str, np.ndarray]\n\t    ) -> Dict[str, torch.Tensor]:\n\t        res = {}\n\t        for _, input in self.inputs.items():\n\t            key = input.feature\n\t            if key not in self.feature_map:\n\t                # this input is not being sent to model in batch\n\t                continue\n", "            res[self.feature_map[key]] = self.inputs[key].transform(\n\t                transform_type, x[key]\n\t            )\n\t        return res\n\tclass RGBImage:\n\t    \"\"\"\n\t    Args:\n\t        ModalityTransform (): RGB images\n\t    \"\"\"\n\t    MEAN = [0.485, 0.456, 0.406]\n", "    STD = [0.229, 0.224, 0.225]\n\t    def __init__(self, feature: str, img_dim: int, feature_in_batch: str):\n\t        self.feature: str = feature\n\t        self.feature_in_batch: str = feature_in_batch\n\t        # transforms.Compose is not supported by torch jit script.\n\t        # see: https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms.py\n\t        # center crop and resize: If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n\t        # normalize: essential code is tensor.sub_(mean).div_(std), so it should be fine with 'sequence' (batched) data.\n\t        # transforms.Compose is not supported by torch jit script.\n\t        self.train_trans = torch.nn.Sequential(\n", "            transforms.RandomResizedCrop(img_dim, scale=(0.8, 1.0)),\n\t            transforms.Normalize(mean=RGBImage.MEAN, std=RGBImage.STD),\n\t        )\n\t    @staticmethod\n\t    def pre_transform(x: np.ndarray) -> np.ndarray:\n\t        \"\"\"Transform numpy array of image (size:[h,w,c], dtype:int8) to array (size:[c, h, w],\n\t        dtype:float32)\n\t        Args:\n\t            x (np.ndarray): (size:[h,w,c], dtype:int8)\n\t        Returns:\n", "            np.ndarray: image (size:[c, h, w], dtype:float32)\n\t        \"\"\"\n\t        x = x.transpose(2, 0, 1)  # h, w, c -> c, h, w\n\t        return x\n\t    def transform(self, transform_type: str, x: np.ndarray) -> torch.Tensor:\n\t        \"\"\"Apply transform to batch or a single frame of images represented by torch.Tensor, not\n\t        pil image!\n\t        Args:\n\t            transform_type (str): type of transform {train|val}\n\t            x (np.ndarray): batch of sequence of rgb image represented by np.ndarry with shape  h, w, c\n", "        Returns:\n\t            np.ndarray: with shape [num_units, c, unit_len, h, w]\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "seqrecord/weather/datapipes.py", "chunked_list": ["\"\"\"Iterative datapipes toread weather dataset in seqrecord format\"\"\"\n\tfrom typing import Callable, Dict, Iterator, List, Optional, Tuple\n\tfrom time import perf_counter\n\timport numpy as np\n\timport torch\n\timport torchdata.datapipes as datapipe\n\tfrom torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n\tfrom tqdm import tqdm\n\tfrom .seqrecord import WSeqRecord\n\t# todo: let collate_fn be a parameter of build_wdatapipe!\n", "@datapipe.functional_datapipe(\"gen_framepair\")\n\tclass FramePairsFromWSeqRecord(datapipe.iter.IterDataPipe):\n\t    \"\"\"A torch datapiple class that iteratively read frame pairs from weather dataset (encoded by WSeqRecord).\"\"\"\n\t    def __init__(\n\t        self,\n\t        source_dp: datapipe.iter.IterDataPipe,\n\t    ) -> None:\n\t        super().__init__()\n\t        self.source_dp = source_dp\n\t    def __iter__(self):\n", "        yield from WSeqRecord.iterate_framepairs_from_files(\n\t            self.source_dp,\n\t        )\n\t@datapipe.functional_datapipe(\"gen_fileidx\")\n\tclass FileidxFromWSeqRecord(datapipe.iter.IterDataPipe):\n\t    \"\"\"A torch datapiple class that iteratively read fileidx from weather dataset (encoded by WSeqRecord).\"\"\"\n\t    def __init__(\n\t        self,\n\t        record_dp: datapipe.iter.IterDataPipe,\n\t    ) -> None:\n", "        super().__init__()\n\t        self.record_dp = record_dp\n\t    def __iter__(self):\n\t        for record in self.record_dp:\n\t            for fileidx in range(record.num_files):\n\t                yield record, fileidx\n\tdef build_wdatapipe(\n\t    records: List[WSeqRecord],\n\t    file_shuffle_buffer_size: Optional[int],\n\t    data_shuffle_buffer_size: Optional[int],\n", "    batch_size: int,\n\t    mappings: List[Callable],\n\t    collate_fn: Callable,\n\t) -> datapipe.iter.IterDataPipe:\n\t    \"\"\"Iteratively apply operations to datapipe: shuffle, sharding, map, batch, collator\n\t    Args:\n\t        datapipe (datapipe.datapipe.IterDataPipe): entry datapipe\n\t        shuffle_buffer_size (Optional[int]): buffer size for pseudo-shuffle\n\t        batch_size (int):\n\t        mappings (List[Callable]): a list of transforms applied to datapipe, between sharding and batch\n", "    Returns:\n\t        datapipe.datapipe.IterDataPipe: transformed datapipe ready to be sent to dataloader\n\t    \"\"\"\n\t    dp = IterableWrapper(records).gen_fileidx()\n\t    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n\t    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n\t    if file_shuffle_buffer_size is not None:\n\t        dp = dp.shuffle(buffer_size=file_shuffle_buffer_size)\n\t    # sharding: Place ShardingFilter (datapipe.sharding_filter) as early as possible in the pipeline,\n\t    # especially before expensive operations such as decoding, in order to avoid repeating these expensive operations across worker/distributed processes.\n", "    # output will be a sequence of file_idx(s) distributed to different workers (\n\t    dp = dp.sharding_filter()\n\t    dp = dp.gen_framepair()\n\t    # dp = dp.shuffle(buffer_size=data_shuffle_buffer_size)\n\t    for i, mapping in enumerate(mappings):\n\t        dp = dp.map(fn=mapping)\n\t    # Note that if you choose to use Batcher while setting batch_size > 1 for DataLoader,\n\t    # your samples will be batched more than once. You should choose one or the other.\n\t    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n\t    dp = dp.batch(batch_size=batch_size, drop_last=True)\n", "    dp = dp.collate(collate_fn=collate_fn)\n\t    return dp\n"]}
{"filename": "seqrecord/weather/__init__.py", "chunked_list": []}
{"filename": "seqrecord/weather/constants.py", "chunked_list": ["NAME_TO_VAR = {\n\t    \"2m_temperature\": \"t2m\",\n\t    \"10m_u_component_of_wind\": \"u10\",\n\t    \"10m_v_component_of_wind\": \"v10\",\n\t    \"mean_sea_level_pressure\": \"msl\",\n\t    \"surface_pressure\": \"sp\",\n\t    \"toa_incident_solar_radiation\": \"tisr\",\n\t    \"total_precipitation\": \"tp\",\n\t    \"land_sea_mask\": \"lsm\",\n\t    \"orography\": \"orography\",\n", "    \"lattitude\": \"lat2d\",\n\t    \"geopotential\": \"z\",\n\t    \"u_component_of_wind\": \"u\",\n\t    \"v_component_of_wind\": \"v\",\n\t    \"temperature\": \"t\",\n\t    \"relative_humidity\": \"r\",\n\t    \"specific_humidity\": \"q\",\n\t    \"2m_dewpoint_temperature\": \"d2m\",\n\t    \"total_cloud_cover\": \"tcc\",\n\t    \"total_column_water_vapour\": \"tcwv\",\n", "    \"sea_surface_temperature\": \"sst\",\n\t    \"skin_temperature\": \"skt\",\n\t}\n\tVAR_TO_NAME = {v: k for k, v in NAME_TO_VAR.items()}\n\tVAR_TO_NAME.update({k: k for k in NAME_TO_VAR.keys()})\n\tSINGLE_LEVEL_VARS = [\n\t    \"2m_temperature\",\n\t    \"10m_u_component_of_wind\",\n\t    \"10m_v_component_of_wind\",\n\t    \"mean_sea_level_pressure\",\n", "    \"surface_pressure\",\n\t    \"toa_incident_solar_radiation\",\n\t    \"total_precipitation\",\n\t    \"land_sea_mask\",\n\t    \"orography\",\n\t    \"lattitude\",\n\t]\n\tPRESSURE_LEVEL_VARS = [\n\t    \"geopotential\",\n\t    \"u_component_of_wind\",\n", "    \"v_component_of_wind\",\n\t    \"temperature\",\n\t    \"relative_humidity\",\n\t    \"specific_humidity\",\n\t]\n\tALL_LEVELS = [\n\t    1,\n\t    2,\n\t    3,\n\t    5,\n", "    7,\n\t    10,\n\t    20,\n\t    30,\n\t    50,\n\t    70,\n\t    100,\n\t    125,\n\t    150,\n\t    175,\n", "    200,\n\t    225,\n\t    250,\n\t    300,\n\t    350,\n\t    400,\n\t    450,\n\t    500,\n\t    550,\n\t    600,\n", "    650,\n\t    700,\n\t    750,\n\t    775,\n\t    800,\n\t    825,\n\t    850,\n\t    875,\n\t    900,\n\t    925,\n", "    950,\n\t    975,\n\t    1000,\n\t]\n\tNAME_LEVEL_TO_VAR_LEVEL = {}\n\tfor var in SINGLE_LEVEL_VARS:\n\t    NAME_LEVEL_TO_VAR_LEVEL[var] = NAME_TO_VAR[var]\n\tfor var in PRESSURE_LEVEL_VARS:\n\t    for l in ALL_LEVELS:\n\t        NAME_LEVEL_TO_VAR_LEVEL[var + \"_\" + str(l)] = NAME_TO_VAR[var] + \"_\" + str(l)\n", "VAR_LEVEL_TO_NAME_LEVEL = {v: k for k, v in NAME_LEVEL_TO_VAR_LEVEL.items()}\n"]}
{"filename": "seqrecord/weather/seqrecord.py", "chunked_list": ["\"\"\"A package for encoding and decoding weather dataset.\"\"\"\n\timport asyncio\n\timport copy\n\timport io\n\timport os\n\timport pickle\n\timport random\n\timport threading\n\timport time\n\tfrom collections import deque\n", "from functools import partial\n\tfrom time import perf_counter\n\tfrom typing import (\n\t    Any,\n\t    BinaryIO,\n\t    Deque,\n\t    Dict,\n\t    Generator,\n\t    List,\n\t    Optional,\n", "    Tuple,\n\t    TypeVar,\n\t    Union,\n\t    Iterator,\n\t)\n\timport aiofiles\n\timport numpy as np\n\timport yaml\n\timport subprocess\n\timport shutil\n", "from seqrecord.utils import (\n\t    CONSUMER_SLEEP_INTERVAL,\n\t    PRODUCER_SLEEP_INTERVAL,\n\t    FileManager,\n\t    LRUCache,\n\t    TimeTracker,\n\t    WriterBuffer,\n\t)\n\tMAX_RECORDFILE_SIZE = 2e9  # 1e9  # 1e8, 100 mb, maximum size of a single record file\n\tFILE_CACHE_SIZE = 10  # 10, maximum number of record files to keep in local disk\n", "WSR = TypeVar(\"WSR\", bound=\"WSeqRecord\")\n\t# todo: add property stuff for attributes not to be changed by user\n\t# todo: MISSING, some transform work, subsample etc. Need to make sure data is same as produced by existing dataset\n\tclass _PrefetchData:\n\t    def __init__(self, source_data_generator, buffer_size: int):\n\t        self.run_prefetcher = True\n\t        # python deque is thread safe for appends and pops from opposite sides.\n\t        # ref: https://stackoverflow.com/questions/8554153/is-this-deque-thread-safe-in-python\n\t        self.prefetch_buffer: Deque = deque()\n\t        self.buffer_size: int = buffer_size\n", "        self.source_data_generator = source_data_generator\n\tclass WSeqRecord:\n\t    \"\"\"A serialization protocal that stores a single continuous long sequence of weather data into record files, while provides metadata of each frame to enable efficient random access of frames.\"\"\"\n\t    def __init__(self, recorddir: str, local_cache_dir: Optional[str] = None) -> None:\n\t        \"\"\"_summary_\n\t        Args:\n\t            recorddir (str): directory record files is placed\n\t        \"\"\"\n\t        # folder that stores data in a separate directory (subfolder)\n\t        self.recorddir: str = recorddir\n", "        os.makedirs(self.recorddir, exist_ok=True)\n\t        # in case we use realtive path '~'\n\t        self.local_cache_dir = local_cache_dir\n\t        if local_cache_dir is not None:\n\t            self.local_cache_dir = os.path.abspath(os.path.expanduser(local_cache_dir))\n\t            if (\n\t                os.path.exists(self.local_cache_dir)\n\t                and len(os.listdir(self.local_cache_dir)) > 0\n\t            ):\n\t                print(\"Warning: local cache dir is not empty. Clearing it now.\")\n", "                shutil.rmtree(self.local_cache_dir, ignore_errors=True)\n\t            os.makedirs(self.local_cache_dir, exist_ok=True)\n\t        self.features_written = None\n\t    @staticmethod\n\t    def subfolder_name(rank: int, world_size: int) -> str:\n\t        return f\"{rank}\"\n\t    @staticmethod\n\t    def fileidx2name(file_idx: int) -> str:\n\t        return f\"record_{file_idx}.bin\"\n\t    def fileidx2path(self, file_idx: int, local_cache_dir: Optional[str] = None):\n", "        \"\"\"Turn absolute file idx into relative path of the corresponding record file.\n\t        Write to self.local_dir and move.\n\t        Args:\n\t            file_idx (int): _description_\n\t            local_cache_dir (str, optional): The directory to cache the record file. Defaults to None.\n\t        Returns:\n\t            _type_: _description_\n\t        Yields:\n\t            _type_: _description_\n\t        \"\"\"\n", "        dir = self.recorddir if local_cache_dir is None else local_cache_dir\n\t        rank_id = self.meta_file[file_idx].get(\"rank_id\", -1)\n\t        if rank_id == -1:\n\t            # there is no rank hierarachy\n\t            return os.path.join(dir, self.fileidx2name(file_idx))\n\t        else:\n\t            return os.path.join(\n\t                dir,\n\t                self.subfolder_name(rank_id, self.num_ranks),\n\t                f\"record_{self.meta_file[file_idx]['rel_file_idx']}.bin\",\n", "            )\n\t    def recordfile_generator(self, frame_generator: Iterator):\n\t        \"\"\"Ignore the complexity of rank/world size from multi-processing. This method only\n\t        focus on file/frame.\n\t        Args:\n\t            frame_generator (callable): _description_\n\t        Yields:\n\t            _type_: _description_\n\t        \"\"\"\n\t        try:\n", "            write_buffer = WriterBuffer()\n\t            num_bytes = 0\n\t            self.meta_file = {}\n\t            self.meta_frame = {}\n\t            frame_idx = 0\n\t            file_idx = 0\n\t            for frame in frame_generator:\n\t                if self.features_written is None:\n\t                    self.features_written = [key for key in frame]\n\t                if num_bytes == 0:\n", "                    # new file\n\t                    self.meta_file[file_idx] = {\n\t                        \"frame_idx_start\": frame_idx,\n\t                        \"relative_path\": self.fileidx2name(file_idx),\n\t                    }\n\t                    # relative path to the record file does not contain directory of the corresponding seqrecord\n\t                self.meta_frame[frame_idx] = {\n\t                    \"file_idx\": file_idx,\n\t                    \"bytes_offset\": num_bytes,\n\t                }\n", "                num_bytes_in_frame = 0\n\t                for feature, data in frame.items():\n\t                    self.meta_frame[frame_idx][feature] = {\n\t                        \"is_none\": (\n\t                            data.dtype == np.dtype(\"O\") and data == None\n\t                        ),  # this feature is essentially missing, and\n\t                        \"dtype\": data.dtype,\n\t                        \"shape\": data.shape,\n\t                        \"bytes_offset\": num_bytes_in_frame,\n\t                        \"nbytes\": data.nbytes,\n", "                    }\n\t                    write_buffer.write(data.tobytes())\n\t                    num_bytes_in_frame += data.nbytes\n\t                self.meta_frame[frame_idx][\"nbytes\"] = num_bytes_in_frame\n\t                frame_idx += 1\n\t                num_bytes += num_bytes_in_frame\n\t                if num_bytes > MAX_RECORDFILE_SIZE:\n\t                    # current file is big enough\n\t                    num_bytes = 0\n\t                    self.meta_file[file_idx][\"frame_idx_end\"] = frame_idx\n", "                    write_buffer.clear()\n\t                    yield (\n\t                        self.fileidx2path(\n\t                            file_idx, local_cache_dir=self.local_cache_dir\n\t                        ),\n\t                        write_buffer.getvalue(),\n\t                    )\n\t                    file_idx += 1\n\t            if (\n\t                file_idx in self.meta_file\n", "                and self.meta_file[file_idx].get(\"frame_idx_end\", None) is None\n\t            ):\n\t                # there is content left in the write_buffer\n\t                self.meta_file[file_idx][\"frame_idx_end\"] = frame_idx\n\t                yield (\n\t                    self.fileidx2path(file_idx, self.local_cache_dir),\n\t                    write_buffer.getvalue(),\n\t                )\n\t                file_idx += 1\n\t        finally:\n", "            write_buffer.close()\n\t            self.num_files = file_idx\n\t            self.num_frames = frame_idx\n\t    def put_frame(self, frame_generator: callable, prefetch_buffer_size: int = 5):\n\t        # should be only adding frames here\n\t        # two threads this function keep writing and send them to buffer\n\t        # a separate thread writes the buffer to files as long as the buffer is non-empty\n\t        try:\n\t            prefetch_data = _PrefetchData(\n\t                self.recordfile_generator(frame_generator=frame_generator),\n", "                prefetch_buffer_size,\n\t            )\n\t            thread = threading.Thread(\n\t                target=WSeqRecord.prefetch_thread_worker,\n\t                args=(prefetch_data,),\n\t                daemon=True,\n\t            )\n\t            thread.start()\n\t            file_cache = []\n\t            subprocesses = deque()\n", "            while prefetch_data.run_prefetcher:\n\t                if len(prefetch_data.prefetch_buffer) > 0:\n\t                    (\n\t                        file_path,\n\t                        content,\n\t                    ) = prefetch_data.prefetch_buffer.popleft()\n\t                    with open(file_path, \"wb\") as f:\n\t                        f.write(content)\n\t                    file_cache.append(file_path)\n\t                    if (\n", "                        self.local_cache_dir is not None\n\t                        and len(file_cache) > FILE_CACHE_SIZE\n\t                    ):\n\t                        # move record files to recorddir in the background\n\t                        subprocesses.append(\n\t                            subprocess.Popen(\n\t                                [\n\t                                    \"mv\",\n\t                                ]\n\t                                + file_cache\n", "                                + [f\"{self.recorddir}/\"]\n\t                            )\n\t                        )\n\t                        file_cache = []\n\t                else:\n\t                    # TODO: Calculate sleep interval based on previous availability speed\n\t                    time.sleep(CONSUMER_SLEEP_INTERVAL)\n\t                    # todo: check if poll() is used properly\n\t                    if len(subprocesses) > 0 and subprocesses[0].poll() is not None:\n\t                        subprocesses.popleft()\n", "        finally:\n\t            prefetch_data.run_prefetcher = False\n\t            if thread is not None:\n\t                thread.join()\n\t                thread = None\n\t            if self.local_cache_dir is not None and len(file_cache) > 0:\n\t                subprocesses.append(\n\t                    subprocess.Popen(\n\t                        [\n\t                            \"mv\",\n", "                        ]\n\t                        + file_cache\n\t                        + [f\"{self.recorddir}/\"]\n\t                    )\n\t                )\n\t                file_cache = []\n\t            for p in subprocesses:\n\t                p.wait()\n\t    def read_frame(\n\t        self,\n", "        file_desc: Union[io.BufferedReader, BinaryIO],\n\t        metadata_frame: Dict[str, Union[int, dict]],\n\t        features: List[str],\n\t    ) -> Dict[str, np.ndarray]:\n\t        \"\"\"Given record file descriptor and serialization proto of a single frame, return the\n\t        decoded dictionary(feature->data(np.ndarray)) of the item.\n\t        Args:\n\t            file_desc (io.BufferedReader): python file object of the record file (required by numpy)\n\t            metadata_frame (Dict[str, Any]): dict that contains meta info of a specific frame\n\t            features (List[str]):  features requested for frame\n", "        Returns:\n\t            Dict[str, np.ndarray]: data\n\t        \"\"\"\n\t        frame = {}\n\t        frame_offset = metadata_frame[\"bytes_offset\"]\n\t        for feature in features:\n\t            frame[feature] = np.memmap(\n\t                file_desc,\n\t                dtype=metadata_frame[feature][\"dtype\"],\n\t                mode=\"r\",\n", "                offset=frame_offset + metadata_frame[feature][\"bytes_offset\"],\n\t                shape=metadata_frame[feature][\"shape\"],\n\t            )\n\t        return frame\n\t    def iterate_frames(\n\t        self, features: List[str]\n\t    ) -> Generator[Dict[str, np.ndarray], None, None]:\n\t        \"\"\"Iterate sequentially over frames in the dataset\n\t        Args:\n\t            features (List[str]): a list of feature names requested from frames\n", "        Returns:\n\t            _type_: _description_\n\t        Yields:\n\t            Generator[Dict[str, np.ndarray], None, None]: generates one-frame data\n\t        \"\"\"\n\t        for file_idx in range(self.num_files):\n\t            file_desc = open(\n\t                os.path.join(self.recorddir, self.meta_file[file_idx][\"relative_path\"]),\n\t                mode=\"rb\",\n\t            )\n", "            for idx in range(\n\t                self.meta_file[file_idx][\"frame_idx_start\"],\n\t                self.meta_file[file_idx][\"frame_idx_end\"],\n\t            ):\n\t                frame = self.read_frame(file_desc, self.meta_frame[idx], features)\n\t                yield {feature: frame[feature] for feature in features}\n\t            file_desc.close()\n\t    # todo: test effect of caching on real data\n\t    def iterate_framepairs(\n\t        self,\n", "        input_features: List[str],\n\t        target_features: List[str],\n\t        max_pred_steps: int,\n\t        filedesc_cache_cap: int = 10,\n\t        frame_cache_cap: int = 20,\n\t    ) -> Generator[Dict[str, np.ndarray], None, None]:\n\t        \"\"\"Iterate frames over the whole dataset\n\t        # todo: to think about, if we don't shuffle files, then cache based on frame idx is convenient and effective.\n\t        Args:\n\t            input_features [List[str]]: a list of features requested for input\n", "            target_features [List[str]]: a list of features requested for target\n\t            max_pred_steps [int]: maximum number of leap steps for predictive frame\n\t        Yields:\n\t            Generator[Dict[str, np.ndarray], None, None]: data item [feature->data]. All data items are being returned sequentially\n\t        \"\"\"\n\t        file_manager = FileManager(\n\t            cache_capacity=filedesc_cache_cap,\n\t        )\n\t        # given that, input and target features do not overlap, we only cache target frame\n\t        # LRU might not be suitable, evicting based on idx seems better\n", "        frame_cache = LRUCache(frame_cache_cap)\n\t        for fileidx4input in range(self.num_files):\n\t            filedesc4input = file_manager.open_file(\n\t                file_idx=fileidx4input,\n\t                file_path=os.path.join(\n\t                    self.recorddir, self.meta_file[fileidx4input][\"relative_path\"]\n\t                ),\n\t            )\n\t            endpoints = (\n\t                self.meta_file[fileidx4input][\"frame_idx_start\"],\n", "                self.meta_file[fileidx4input][\"frame_idx_end\"],\n\t            )\n\t            # no target frame to predict for the last frame\n\t            for frameidx4input in range(\n\t                endpoints[0],\n\t                min(endpoints[1], self.num_frames - 1),  # self.num_frames\n\t            ):\n\t                input_frame = self.read_frame(\n\t                    filedesc4input, self.meta_frame[frameidx4input], input_features\n\t                )\n", "                # get the target frame for prediction, both start, stop inclusive\n\t                lookahead_steps = min(\n\t                    random.randint(1, max_pred_steps),\n\t                    self.num_frames - 1 - frameidx4input,\n\t                )\n\t                frameidx4target = frameidx4input + lookahead_steps\n\t                target_frame = frame_cache.get(frameidx4target)\n\t                if target_frame is None:\n\t                    fileidx4target = self.meta_frame[frameidx4target][\"file_idx\"]\n\t                    filedesc4target = file_manager.open_file(\n", "                        file_idx=fileidx4target,\n\t                        file_path=os.path.join(\n\t                            self.recorddir,\n\t                            self.meta_file[fileidx4target][\"relative_path\"],\n\t                        ),\n\t                    )\n\t                    target_frame = self.read_frame(\n\t                        filedesc4target,\n\t                        self.meta_frame[frameidx4target],\n\t                        target_features,\n", "                    )\n\t                # colllate input and target frames so that input and target frame are np.ndarray\n\t                input_frame = np.vstack(\n\t                    [input_frame[feature] for feature in input_features]\n\t                )\n\t                target_frame = np.vstack(\n\t                    [target_frame[feature] for feature in target_features]\n\t                )\n\t                yield {\n\t                    \"input\": input_frame,\n", "                    \"target\": target_frame,\n\t                    \"lookahead_steps\": np.asarray(lookahead_steps),\n\t                    \"input_features\": input_features,\n\t                    \"target_features\": target_features,\n\t                }\n\t        file_manager.close_all_files()\n\t    def async_iterate_framepairs(\n\t        self,\n\t        input_features: List[str],\n\t        target_features: List[str],\n", "        max_pred_steps: int,\n\t        filedesc_cache_cap: int = 10,\n\t    ) -> Generator[Dict[str, np.ndarray], None, None]:\n\t        \"\"\"Asyncly read two frames from (possibly) two files.\n\t        Notes:\n\t            No frame cache\n\t        Returns:\n\t            _type_: _description_\n\t        Yields:\n\t            _type_: _description_\n", "        \"\"\"\n\t        # setup a single event loop for async read\n\t        loop = asyncio.new_event_loop()\n\t        asyncio.set_event_loop(loop)\n\t        # file_desc_cache is only used for target frame, since we are iterating for the input frame,\n\t        file_desc_cache = LRUCache(capacity=filedesc_cache_cap)\n\t        # given that, input and target features do not overlap, we only cache target frame\n\t        # LRU might not be suitable, evicting based on idx seems better\n\t        # read two frames using asyn io\n\t        # file cache should only be used for future frames, since base file desc is used continuously\n", "        try:\n\t            for fileidx4input in range(self.num_files):\n\t                filedesc4input = None\n\t                endpoints = (\n\t                    self.meta_file[fileidx4input][\"frame_idx_start\"],\n\t                    self.meta_file[fileidx4input][\"frame_idx_end\"],\n\t                )\n\t                # no target frame to predict for the last frame\n\t                for frameidx4input in range(\n\t                    endpoints[0], min(endpoints[1], self.num_frames - 1)\n", "                ):\n\t                    lookahead_steps = min(\n\t                        random.randint(1, max_pred_steps),\n\t                        self.num_frames - 1 - frameidx4input,\n\t                    )\n\t                    frameidx4target = frameidx4input + lookahead_steps\n\t                    fileidx4target = self.meta_frame[frameidx4target][\"file_idx\"]\n\t                    filedesc4target = file_desc_cache.get(fileidx4target)\n\t                    if filedesc4input is None and filedesc4target is None:\n\t                        # both files need to be opened\n", "                        file_descs = loop.run_until_complete(\n\t                            asyncio.gather(\n\t                                async_open_file(\n\t                                    fileidx4input,\n\t                                    None,\n\t                                    os.path.join(\n\t                                        self.recorddir,\n\t                                        self.meta_file[fileidx4input][\"relative_path\"],\n\t                                    ),\n\t                                ),\n", "                                async_open_file(\n\t                                    fileidx4target,\n\t                                    file_desc_cache,\n\t                                    os.path.join(\n\t                                        self.recorddir,\n\t                                        self.meta_file[fileidx4target][\"relative_path\"],\n\t                                    ),\n\t                                ),\n\t                            )\n\t                        )\n", "                        # order of return values are preserved. Ref: https://stackoverflow.com/questions/54668701/asyncio-gather-scheduling-order-guarantee\n\t                        filedesc4input, filedesc4target = file_descs[0], file_descs[1]\n\t                    elif filedesc4input is None:\n\t                        # only need files for input frame\n\t                        file_descs = loop.run_until_complete(\n\t                            async_open_file(\n\t                                fileidx4input,\n\t                                None,\n\t                                os.path.join(\n\t                                    self.recorddir,\n", "                                    self.meta_file[fileidx4input][\"relative_path\"],\n\t                                ),\n\t                            )\n\t                        )\n\t                        filedesc4input = file_descs\n\t                    elif filedesc4target is None:\n\t                        # only need files for target frame\n\t                        file_descs = loop.run_until_complete(\n\t                            async_open_file(\n\t                                fileidx4target,\n", "                                file_desc_cache,\n\t                                os.path.join(\n\t                                    self.recorddir,\n\t                                    self.meta_file[fileidx4target][\"relative_path\"],\n\t                                ),\n\t                            )\n\t                        )\n\t                        filedesc4target = file_descs\n\t                    frame_pairs = loop.run_until_complete(\n\t                        asyncio.gather(\n", "                            async_read_frame(\n\t                                filedesc4input,\n\t                                self.meta_frame[frameidx4input],\n\t                                input_features,\n\t                            ),\n\t                            async_read_frame(\n\t                                filedesc4target,\n\t                                self.meta_frame[frameidx4target],\n\t                                target_features,\n\t                            ),\n", "                        )\n\t                    )\n\t                    yield {\n\t                        \"input\": frame_pairs[0],\n\t                        \"target\": frame_pairs[1],\n\t                        \"lookahead_steps\": np.asarray(lookahead_steps),\n\t                        \"input_features\": input_features,\n\t                        \"target_features\": target_features,\n\t                    }\n\t                # close file descriptor for\n", "                if filedesc4input is not None:\n\t                    loop.run_until_complete(close_aiofile(filedesc4input))\n\t        finally:\n\t            # close open files\n\t            loop.run_until_complete(close_files_in_cache(file_desc_cache))\n\t            # wrap up async works\n\t            loop.run_until_complete(loop.shutdown_asyncgens())\n\t            loop.close()\n\t    @staticmethod\n\t    def prefetch_thread_worker(prefetch_data):\n", "        # Lazily import to prevent circular import\n\t        # shc: not sure what this is for?\n\t        from torchdata.dataloader2 import communication\n\t        itr = iter(prefetch_data.source_data_generator)\n\t        stop_iteration = False\n\t        while prefetch_data.run_prefetcher:\n\t            if (\n\t                len(prefetch_data.prefetch_buffer) < prefetch_data.buffer_size\n\t                and not stop_iteration\n\t            ):\n", "                try:\n\t                    item = next(itr)\n\t                    prefetch_data.prefetch_buffer.append(item)\n\t                except StopIteration:\n\t                    stop_iteration = True\n\t                # shc: probably not necessary for now\n\t                except communication.iter.InvalidStateResetRequired:\n\t                    stop_iteration = True\n\t                except communication.iter.TerminateRequired:\n\t                    prefetch_data.run_prefetcher = False\n", "            elif stop_iteration and len(prefetch_data.prefetch_buffer) == 0:\n\t                prefetch_data.run_prefetcher = False\n\t            else:  # Buffer is full, waiting for main thread to consume items\n\t                # TODO: Calculate sleep interval based on previous consumption speed\n\t                time.sleep(PRODUCER_SLEEP_INTERVAL)\n\t    def fetch_framepairs(\n\t        self,\n\t        input_features: List[str],\n\t        target_features: List[str],\n\t        max_pred_steps: int,\n", "        prefetch_buffer_size: int = 10,\n\t    ):\n\t        if prefetch_buffer_size < 1:\n\t            yield from self.iterate_framepairs(\n\t                input_features, target_features, max_pred_steps\n\t            )\n\t        else:\n\t            # ref: https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/util/prefetcher.py\n\t            # preftech using a separate thread\n\t            try:\n", "                prefetch_data = _PrefetchData(\n\t                    self.iterate_framepairs(\n\t                        input_features, target_features, max_pred_steps\n\t                    ),\n\t                    prefetch_buffer_size,\n\t                )\n\t                thread = threading.Thread(\n\t                    target=WSeqRecord.prefetch_thread_worker,\n\t                    args=(prefetch_data,),\n\t                    daemon=True,\n", "                )\n\t                thread.start()\n\t                while prefetch_data.run_prefetcher:\n\t                    if len(prefetch_data.prefetch_buffer) > 0:\n\t                        yield prefetch_data.prefetch_buffer.popleft()\n\t                    else:\n\t                        # TODO: Calculate sleep interval based on previous availability speed\n\t                        time.sleep(CONSUMER_SLEEP_INTERVAL)\n\t            finally:\n\t                prefetch_data.run_prefetcher = False\n", "                if thread is not None:\n\t                    thread.join()\n\t                    thread = None\n\t    @staticmethod\n\t    def iterate_framepairs_from_files(\n\t        fileidx_generator: Iterator[int],\n\t        filedesc_cache_cap: int = 10,\n\t        frame_cache_cap: int = 20,\n\t    ) -> Generator[Dict[str, np.ndarray], None, None]:\n\t        file_manager = FileManager(\n", "            cache_capacity=filedesc_cache_cap,\n\t        )\n\t        frame_cache = LRUCache(frame_cache_cap)\n\t        for record, fileidx4input in fileidx_generator:\n\t            filedesc4input = file_manager.open_file(\n\t                file_idx=fileidx4input,\n\t                file_path=os.path.join(\n\t                    record.recorddir, record.meta_file[fileidx4input][\"relative_path\"]\n\t                ),\n\t            )\n", "            endpoints = (\n\t                record.meta_file[fileidx4input][\"frame_idx_start\"],\n\t                record.meta_file[fileidx4input][\"frame_idx_end\"],\n\t            )\n\t            # no target frame to predict for the last frame\n\t            for frameidx4input in range(\n\t                endpoints[0],\n\t                min(endpoints[1], record.num_frames - 1),  # self.num_frames\n\t            ):\n\t                input_frame = record.read_frame(\n", "                    filedesc4input,\n\t                    record.meta_frame[frameidx4input],\n\t                    record.framereader_args[\"input_features\"],\n\t                )\n\t                # get the target frame for prediction, both start, stop inclusive\n\t                lookahead_steps = min(\n\t                    random.randint(1, record.framereader_args[\"max_pred_steps\"]),\n\t                    record.num_frames - 1 - frameidx4input,\n\t                )\n\t                frameidx4target = frameidx4input + lookahead_steps\n", "                target_frame = frame_cache.get(frameidx4target)\n\t                if target_frame is None:\n\t                    fileidx4target = record.meta_frame[frameidx4target][\"file_idx\"]\n\t                    filedesc4target = file_manager.open_file(\n\t                        fileidx4target,\n\t                        file_path=os.path.join(\n\t                            record.recorddir,\n\t                            record.meta_file[fileidx4target][\"relative_path\"],\n\t                        ),\n\t                    )\n", "                    target_frame = record.read_frame(\n\t                        filedesc4target,\n\t                        record.meta_frame[frameidx4target],\n\t                        record.framereader_args[\"target_features\"],\n\t                    )\n\t                # colllate input and target frames so that input and target frame are np.ndarray\n\t                # each feature is a two-dimensional np.ndarray\n\t                # output is channelxheightxwidth\n\t                input_frame = np.stack(\n\t                    [\n", "                        input_frame[feature]\n\t                        for feature in record.framereader_args[\"input_features\"]\n\t                    ],\n\t                    axis=0,\n\t                )\n\t                target_frame = np.stack(\n\t                    [\n\t                        target_frame[feature]\n\t                        for feature in record.framereader_args[\"target_features\"]\n\t                    ],\n", "                    axis=0,\n\t                )\n\t                # print(self.timer.summarize())\n\t                yield {\n\t                    \"input\": input_frame,\n\t                    \"target\": target_frame,\n\t                    \"lead_times\": np.asarray(\n\t                        lookahead_steps * record.framereader_args[\"hours_per_step\"],\n\t                        dtype=input_frame.dtype,\n\t                    ),\n", "                    \"meta_data\": record.framereader_args,\n\t                }\n\t        file_manager.close_all_files()\n\t    @staticmethod\n\t    def iterate_frames_from_file(\n\t        fileidx_generator: Iterator[int],\n\t        filedesc_cache_cap: int = 10,\n\t        frame_cache_cap: int = 20,\n\t    ):\n\t        \"\"\"read input/target frames from record files, where input consists of multiple consecutive frames and target frame is one future frame.\n", "        Notes:\n\t            # ! $lead_steps is with respect to the last frame frame in $input_frames\n\t            # ! Assume input target features are the same\n\t        Args:\n\t            fileidx_generator (Iterator[int]): _description_\n\t            filedesc_cache_cap (int, optional): _description_. Defaults to 10.\n\t            frame_cache_cap (int, optional): _description_. Defaults to 20.\n\t        Yields:\n\t            _type_: _description_\n\t        \"\"\"\n", "        file_manager = FileManager(\n\t            cache_capacity=filedesc_cache_cap,\n\t        )\n\t        frame_cache = LRUCache(frame_cache_cap)\n\t        inputframes_queue = deque()\n\t        for record, fileidx4input in fileidx_generator:\n\t            filedesc4input = file_manager.open_file(\n\t                file_idx=fileidx4input,\n\t                file_path=os.path.join(\n\t                    record.recorddir, record.meta_file[fileidx4input][\"relative_path\"]\n", "                ),\n\t            )\n\t            endpoints = (\n\t                record.meta_file[fileidx4input][\"frame_idx_start\"],\n\t                min(\n\t                    record.meta_file[fileidx4input][\"frame_idx_end\"],\n\t                    record.num_frames - 1,\n\t                    # no target frame left to predict for the last frame\n\t                ),\n\t            )\n", "            # fileidx from generator may not be consecutive\n\t            if len(inputframes_queue) > 0:\n\t                print(\"testing concatnation \", inputframes_queue[-1][0], endpoints[0])\n\t            if (\n\t                len(inputframes_queue) == 0\n\t                or inputframes_queue[-1][0] + 1 != endpoints[0]\n\t            ):\n\t                # consective files, the input frames queue from last file can be used\n\t                # do we need to worry about memory leak issues here, I guess not?\n\t                print(\"not concated!\")\n", "                inputframes_queue = deque()\n\t            print(f\"in the {fileidx4input}th file, {endpoints[0]} to {endpoints[1]}!\")\n\t            for frameidx4input in range(endpoints[0], endpoints[1]):\n\t                input_frame = frame_cache.get(frameidx4input)\n\t                if input_frame is None:\n\t                    input_frame = record.read_frame(\n\t                        filedesc4input,\n\t                        record.meta_frame[frameidx4input],\n\t                        record.framereader_args[\"input_features\"],\n\t                    )\n", "                    # no need to cache input frames since it is not likely to be reused\n\t                inputframes_queue.append((frameidx4input, input_frame))\n\t                if (\n\t                    len(inputframes_queue)\n\t                    >= record.framereader_args[\"num_frames_in_input\"]\n\t                ):\n\t                    # ready to eject input/target pairs\n\t                    max_pred_steps = min(\n\t                        record.framereader_args[\"max_pred_steps\"],\n\t                        record.num_frames - 1 - frameidx4input,\n", "                    )\n\t                    # both sides of randint are inclusive\n\t                    lookahead_steps = random.randint(1, max_pred_steps)\n\t                    frameidx4target = frameidx4input + lookahead_steps\n\t                    target_frame = frame_cache.get(frameidx4target)\n\t                    if target_frame is None:\n\t                        fileidx4target = record.meta_frame[frameidx4target][\"file_idx\"]\n\t                        filedesc4target = file_manager.open_file(\n\t                            fileidx4target,\n\t                            file_path=os.path.join(\n", "                                record.recorddir,\n\t                                record.meta_file[fileidx4target][\"relative_path\"],\n\t                            ),\n\t                        )\n\t                        target_frame = record.read_frame(\n\t                            filedesc4target,\n\t                            record.meta_frame[frameidx4target],\n\t                            record.framereader_args[\"target_features\"],\n\t                        )\n\t                    # colllate input and target frames so that input and target frame are np.ndarray\n", "                    # each feature is a two-dimensional np.ndarray\n\t                    # input_frames is Lis[cxhxw] with length num_frames_in_input\n\t                    input_frames = [\n\t                        (\n\t                            idx,\n\t                            np.stack(\n\t                                [\n\t                                    frame[feature]\n\t                                    for feature in record.framereader_args[\n\t                                        \"input_features\"\n", "                                    ]\n\t                                ],\n\t                                axis=0,\n\t                            ),\n\t                        )\n\t                        for idx, frame in inputframes_queue\n\t                    ]\n\t                    # target_frame is cxhxw\n\t                    target_frame = np.stack(\n\t                        [\n", "                            target_frame[feature]\n\t                            for feature in record.framereader_args[\"target_features\"]\n\t                        ],\n\t                        axis=0,\n\t                    )\n\t                    yield {\n\t                        \"input\": input_frames,  # List[np.ndarray with shape cxhxw]\n\t                        \"target\": target_frame,  # np.ndarray with shape cxhxw\n\t                        \"lead_time\": np.asarray(\n\t                            lookahead_steps * record.framereader_args[\"hours_per_step\"],\n", "                            dtype=input_frames[0][1].dtype,\n\t                        ),\n\t                        \"meta_data\": record.framereader_args,\n\t                    }\n\t                    inputframes_queue.popleft()\n\t        file_manager.close_all_files()\n\t    def dump_record(self, rank: Optional[int] = None) -> None:\n\t        \"\"\"save attributes of instance of record into a pickled file and yaml file for visual inspection.\n\t        Note:\n\t        saving attribute dict instead of pickled class: pickling class and loading it is a mess because of\n", "        path issues.\n\t        \"\"\"\n\t        file_name = f\"record_{rank}\" if rank is not None else \"record_all\"\n\t        dic = copy.deepcopy(self.__dict__)\n\t        # do not want to pickle a python module\n\t        with open(os.path.join(self.recorddir, f\"{file_name}.dict\"), mode=\"wb\") as f:\n\t            pickle.dump(dic, file=f)\n\t        # transform some features to make them readable in yaml\n\t        for _, val in dic[\"meta_frame\"].items():\n\t            for feature in dic[\"features_written\"]:\n", "                val[feature][\"dtype\"] = val[feature][\"dtype\"].str\n\t                val[feature][\"shape\"] = list(val[feature][\"shape\"])\n\t        with open(os.path.join(self.recorddir, f\"{file_name}.yaml\"), mode=\"w\") as f:\n\t            f.write(\"# Configs for human inspection only!\\n\")\n\t            f.write(yaml.dump(dic))\n\t    @classmethod\n\t    def load_record(cls, recorddir: str, rank: Optional[int] = None) -> WSR:\n\t        \"\"\"return an instance of sequence record from file that stores attributes of record as a\n\t        dict (stored at path).\n\t        Args:\n", "            path (str): path to the file that stores dict of attributes of seqrecord\n\t        Returns:\n\t            WSR: an instance of record\n\t        \"\"\"\n\t        file_path = os.path.join(\n\t            recorddir, \"record_all.dict\" if rank is None else f\"record_{rank}.dict\"\n\t        )\n\t        with open(file_path, mode=\"rb\") as f:\n\t            obj_dict = pickle.load(f)\n\t        obj = cls(\n", "            recorddir=recorddir,\n\t        )\n\t        obj_dict.pop(\"recorddir\", None)\n\t        for key, value in obj_dict.items():\n\t            setattr(obj, key, value)\n\t        return obj\n\t    @classmethod\n\t    def gather_subseqrecords(\n\t        cls,\n\t        recorddir: str,\n", "        world_size: int,\n\t        rank2folder: Optional[Dict[int, str]] = None,\n\t    ) -> WSR:\n\t        # make everything hierarchical to make it consistent\n\t        if rank2folder is None:\n\t            rank2folder = {\n\t                i: cls.subfolder_name(i, world_size) for i in range(world_size)\n\t            }\n\t        sub_records = []\n\t        for i in range(world_size):\n", "            sub_records.append(\n\t                cls.load_record(os.path.join(recorddir, rank2folder[i]), rank=i)\n\t            )\n\t        # combine meta data\n\t        features_written = sub_records[0].features_written\n\t        # meta data on each rank collected data\n\t        meta_rank = {}\n\t        meta_file = {}\n\t        meta_frame = {}\n\t        abs_file_idx = 0\n", "        abs_frame_idx = 0\n\t        for i in range(world_size):\n\t            meta_rank[i] = {\n\t                \"file_idx_start\": abs_file_idx,\n\t                \"file_idx_end\": abs_file_idx + sub_records[i].num_files,\n\t                \"frame_idx_start\": abs_frame_idx,\n\t            }\n\t            for j in range(sub_records[i].num_files):\n\t                meta_file[abs_file_idx] = {\n\t                    \"relative_path\": os.path.join(\n", "                        rank2folder[i],\n\t                        sub_records[i].meta_file[j][\"relative_path\"],\n\t                    ),\n\t                    \"frame_idx_start\": abs_frame_idx,\n\t                }\n\t                for k in range(\n\t                    sub_records[i].meta_file[j][\"frame_idx_start\"],\n\t                    sub_records[i].meta_file[j][\"frame_idx_end\"],\n\t                ):\n\t                    meta_frame[abs_frame_idx] = sub_records[i].meta_frame[k]\n", "                    meta_frame[abs_frame_idx][\"rel_frame_idx\"] = k\n\t                    meta_frame[abs_frame_idx][\"file_idx\"] = abs_file_idx\n\t                    abs_frame_idx += 1\n\t                meta_file[abs_file_idx][\"frame_idx_end\"] = abs_frame_idx\n\t                abs_file_idx += 1\n\t            meta_rank[i][\"frame_idx_end\"] = abs_frame_idx\n\t        record = cls(recorddir)\n\t        record.meta_file = meta_file\n\t        record.meta_frame = meta_frame\n\t        record.meta_rank = meta_rank\n", "        record.features_written = features_written\n\t        record.num_ranks = world_size\n\t        record.num_files = abs_file_idx\n\t        record.num_frames = abs_frame_idx\n\t        return record\n\t    def set_framereader_args(self, args: Dict[str, Any]) -> None:\n\t        self.framereader_args = args\n\tasync def async_open_file(\n\t    file_idx: int, file_desc_cache: Optional[LRUCache], file_path: str\n\t):\n", "    file_desc = await aiofiles.open(file_path, \"rb\")\n\t    if file_desc_cache is not None:\n\t        evicted = file_desc_cache.put(file_idx, file_desc)\n\t        if evicted is not None:\n\t            await evicted.close()\n\t    return file_desc\n\tasync def close_files_in_cache(file_desc_cache: LRUCache) -> None:\n\t    for key in file_desc_cache.keys():\n\t        await file_desc_cache.pop(key).close()\n\t    return None\n", "async def close_aiofile(file_desc: io.BufferedReader) -> None:\n\t    await file_desc.close()\n\t    return\n\t# notes: have to use file manager (instead of an lru function since we need to close files)\n\t#        how to verify we are doing async?\n\t# other approaches to be compared with:\n\t#           1. read the whole frame and extract feature data (since reading small pieces of data multiple times is probably slow)\n\t#           2. no async at all\n\tasync def async_read_frame(\n\t    file_desc: io.BufferedReader, metadata_frame: dict, features: List[str]\n", ") -> np.ndarray:\n\t    \"\"\"Given frame metadata and file object that contain frame data, read features from the frame data\n\t    according to features\n\t    Args:\n\t        file_desc (io.BufferedReader): file object that contains the frame data (file object returned by aiofiles) is a subtype of BufferedReader\n\t        metadata_frame (dict): _description_\n\t        features (List[str]): _description_\n\t    Returns:\n\t        np.ndarray: _description_\n\t    \"\"\"\n", "    await file_desc.seek(metadata_frame[\"bytes_offset\"])\n\t    data_bytes = await file_desc.read(metadata_frame[\"nbytes\"])\n\t    frame = {}\n\t    # read the whole chunk or we read each file separately\n\t    for feature in features:\n\t        # b = file_desc.read(metadata[feature][\"nbytes\"])\n\t        # array1d = np.frombuffer(\n\t        #     bytes,\n\t        #     dtype=metadata[feature][\"dtype\"],\n\t        # )\n", "        # frame[feature] = array1d\n\t        # `await` halts `async_read_frame` and gives control back\n\t        start = metadata_frame[feature][\"bytes_offset\"]\n\t        end = start + metadata_frame[feature][\"nbytes\"]\n\t        frame[feature] = np.frombuffer(\n\t            data_bytes[start:end],\n\t            dtype=metadata_frame[feature][\"dtype\"],\n\t        ).reshape(metadata_frame[feature][\"shape\"])\n\t    frame_array = np.vstack([frame[feature] for feature in features])\n\t    return frame_array\n"]}
{"filename": "seqrecord/weather/test/test_wseqrecord_mp.py", "chunked_list": ["\"\"\"test parallel write, gather meta information, and read frame pairs\"\"\"\n\timport unittest\n\tfrom typing import Dict, List\n\timport random\n\timport os\n\timport numpy as np\n\timport numpy.testing as nptest\n\timport seqrecord.weather.seqrecord as wseqrecord\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tfrom seqrecord.weather.format.weathernp2seq_mp import distribute_loads\n", "from itertools import islice\n\tfrom multiprocessing import Process\n\tclass TestWSeqRecord(unittest.TestCase):\n\t    def test_read_frame(self):\n\t        print(\"Test read frame\")\n\t        dataset, rootdir, features = build_weather_dataset()\n\t        record = parallel_write(dataset, rootdir, num_processes=3)\n\t        for i, item in enumerate(record.iterate_frames(features=features)):\n\t            for feature in features:\n\t                nptest.assert_equal(\n", "                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n\t                )\n\t    def test_read_frame_pair(self):\n\t        print(\"Test read frame pairs\")\n\t        dataset, rootdir, features = build_weather_dataset()\n\t        record = parallel_write(dataset, rootdir, num_processes=3)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.iterate_framepairs(\n\t                input_features, output_features, max_pred_steps=max_pred_steps\n\t            )\n\t        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n", "            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n\t            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n", "                verbose=True,\n\t            )\n\t    def test_fetch_frame_pair(self):\n\t        print(\"Test fetch frame pairs\")\n\t        dataset, rootdir, features = build_weather_dataset()\n\t        record = parallel_write(dataset, rootdir, num_processes=3)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.fetch_framepairs(\n\t                input_features, output_features, max_pred_steps=max_pred_steps\n\t            )\n\t        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n", "            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n\t            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n", "                verbose=True,\n\t            )\n\t    def test_async_iterate_frame_pair(self):\n\t        print(\"Test async iterate frame pairs\")\n\t        dataset, rootdir, features = build_weather_dataset()\n\t        record = parallel_write(dataset, rootdir, num_processes=3)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.async_iterate_framepairs(\n\t                input_features, output_features, max_pred_steps=max_pred_steps\n\t            )\n\t        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n", "            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n\t            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n", "                verbose=True,\n\t            )\n\t    def test_iterate_frame_pair_from_files(self):\n\t        print(\"Test iterate frame pairs from files\")\n\t        dataset, rootdir, features = build_weather_dataset()\n\t        record = parallel_write(dataset, rootdir, num_processes=3)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        read_args = {\n\t            \"input_features\": features[:random_partition],\n\t            \"target_features\": features[random_partition:],\n\t            \"max_pred_steps\": max_pred_steps,\n\t        }\n\t        record.add_read_args(read_args)\n\t        fileidx_generator = ((record, i) for i in range(record.num_files))\n\t        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.iterate_framepairs_from_files(fileidx_generator)\n", "        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n\t            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n", "            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n\t                verbose=True,\n\t            )\n\t    def test_local_cache_writer(self):\n\t        print(\"Test iterate frame pairs from files\")\n\t        wseqrecord.MAX_RECORDFILE_SIZE = 1e8\n", "        dataset, rootdir, features = build_weather_dataset()\n\t        record = parallel_write(dataset, rootdir, num_processes=3)\n\tdef build_weather_dataset():\n\t    \"\"\"Generate an aritificial weather dataset so that each feature has the same size\"\"\"\n\t    rootdir = \"./output/wseqrecord_test/\"\n\t    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    time_horizon = 10000\n\t    dataset = [{} for _ in range(time_horizon)]\n\t    shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n\t    dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n", "    for feature in features:\n\t        for i in range(time_horizon):\n\t            dataset[i][feature] = (\n\t                np.random.rand(shape[0], shape[1])\n\t                if dtype == \"float32\"\n\t                else np.random.randint(low=0, high=255, size=shape)\n\t            )\n\t    features = [feature for feature in features]\n\t    return dataset, rootdir, features\n\tdef write_frame(rank, world_size, rootdir, frame_gen, load_amound):\n", "    sub_wsrecord = WSeqRecord(\n\t        os.path.join(rootdir, WSeqRecord.subfolder_name(rank, world_size)),\n\t        local_cache_dir=os.path.join(\"./output/cache/\", str(rank)),\n\t    )\n\t    sub_wsrecord.put_frame(frame_gen)\n\t    sub_wsrecord.dump_record(rank)\n\tdef parallel_write(dataset, rootdir, num_processes):\n\t    dividens = distribute_loads(len(dataset), num_processes)\n\t    processes = []\n\t    for i in range(num_processes):\n", "        sub_dataset_generator = islice(iter(dataset), dividens[i][0], dividens[i][1])\n\t        p = Process(\n\t            target=write_frame,\n\t            args=(\n\t                i,\n\t                num_processes,\n\t                rootdir,\n\t                sub_dataset_generator,\n\t                dividens[i][1] - dividens[i][0],\n\t            ),\n", "        )\n\t        processes.append(p)\n\t        p.start()\n\t    # all process complete successfully\n\t    for i in range(num_processes):\n\t        processes[i].join()\n\t    # combine sub-seqrecord\n\t    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_processes)\n\t    record.dump_record()\n\t    return record\n", "if __name__ == \"__main__\":\n\t    np.random.seed(0)\n\t    unittest.main()\n"]}
{"filename": "seqrecord/weather/test/test_wseqrecord_dist.py", "chunked_list": ["\"\"\"test multiple (two) levels of parallel write, gather meta information\"\"\"\n\timport unittest\n\tfrom typing import Dict, List\n\timport random\n\timport os\n\timport numpy as np\n\timport numpy.testing as nptest\n\timport seqrecord.weather.seqrecord as wseqrecord\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tfrom seqrecord.weather.format.weathernp2seq_mp import distribute_loads\n", "from itertools import islice\n\tfrom multiprocessing import Process\n\tclass TestWSeqRecord(unittest.TestCase):\n\t    def test_read_frame(self):\n\t        print(\"Test read frame\")\n\t        num_nodes, num_processes = 2, 3\n\t        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\t        for i, item in enumerate(record.iterate_frames(features=features)):\n\t            for feature in features:\n\t                nptest.assert_equal(\n", "                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n\t                )\n\t    def test_read_frame_pair(self):\n\t        print(\"Test read frame pairs\")\n\t        num_nodes, num_processes = 3, 4\n\t        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.iterate_framepairs(\n\t                input_features, output_features, max_pred_steps=max_pred_steps\n\t            )\n\t        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n", "            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n\t            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n", "                verbose=True,\n\t            )\n\t    def test_fetch_frame_pair(self):\n\t        print(\"Test fetch frame pairs\")\n\t        num_nodes, num_processes = 2, 4\n\t        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.fetch_framepairs(\n\t                input_features, output_features, max_pred_steps=max_pred_steps\n\t            )\n\t        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n", "            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n\t            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n", "                verbose=True,\n\t            )\n\t    def test_async_iterate_frame_pair(self):\n\t        print(\"Test async iterate frame pairs\")\n\t        num_nodes, num_processes = 3, 5\n\t        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        input_features = features[:random_partition]\n\t        output_features = features[random_partition:]\n", "        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.async_iterate_framepairs(\n\t                input_features, output_features, max_pred_steps=max_pred_steps\n\t            )\n\t        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n", "            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n\t            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n", "                verbose=True,\n\t            )\n\t    def test_iterate_frame_pair_from_files(self):\n\t        print(\"Test iterate frame pairs from files\")\n\t        num_nodes, num_processes = 4, 7\n\t        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\t        max_pred_steps = 100\n\t        random_partition = random.randint(1, len(features) - 2)\n\t        read_args = {\n\t            \"input_features\": features[:random_partition],\n", "            \"target_features\": features[random_partition:],\n\t            \"max_pred_steps\": max_pred_steps,\n\t        }\n\t        record.add_read_args(read_args)\n\t        fileidx_generator = ((record, i) for i in range(record.num_files))\n\t        # read frame pairs\n\t        for i, item in enumerate(\n\t            record.iterate_framepairs_from_files(\n\t                fileidx_generator,\n\t            )\n", "        ):\n\t            x, y, lookahead_steps = (\n\t                item[\"input\"],\n\t                item[\"target\"],\n\t                item[\"lookahead_steps\"],\n\t            )\n\t            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t            y_target = np.vstack(\n\t                [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t            )\n", "            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t            nptest.assert_equal(\n\t                y,\n\t                y_target,\n\t                err_msg=\"\",\n\t                verbose=True,\n\t            )\n\tdef build_weather_dataset():\n\t    \"\"\"Generate an aritificial weather dataset so that each feature has the same size\"\"\"\n\t    rootdir = \"./output/wseqrecord_test/\"\n", "    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    time_horizon = 10000\n\t    dataset = [{} for _ in range(time_horizon)]\n\t    shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n\t    dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n\t    for feature in features:\n\t        for i in range(time_horizon):\n\t            dataset[i][feature] = (\n\t                np.random.rand(shape[0], shape[1])\n\t                if dtype == \"float32\"\n", "                else np.random.randint(low=0, high=255, size=shape)\n\t            )\n\t    features = [feature for feature in features]\n\t    return dataset, rootdir, features\n\tdef write_frame(rank, world_size, rootdir, frame_gen, load_amound):\n\t    sub_wsrecord = WSeqRecord(\n\t        os.path.join(rootdir, WSeqRecord.subfolder_name(rank, world_size))\n\t    )\n\t    sub_wsrecord.put_frame(frame_gen)\n\t    sub_wsrecord.dump_record(rank)\n", "def parallel_write(dataset, node_rank, rootdir, num_processes):\n\t    dividens = distribute_loads(len(dataset), num_processes)\n\t    processes = []\n\t    for i in range(num_processes):\n\t        sub_dataset_generator = islice(iter(dataset), dividens[i][0], dividens[i][1])\n\t        p = Process(\n\t            target=write_frame,\n\t            args=(\n\t                i,\n\t                num_processes,\n", "                rootdir,\n\t                sub_dataset_generator,\n\t                dividens[i][1] - dividens[i][0],\n\t            ),\n\t        )\n\t        processes.append(p)\n\t        p.start()\n\t    # all process complete successfully\n\t    for i in range(num_processes):\n\t        processes[i].join()\n", "    # combine sub-seqrecord\n\t    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_processes)\n\t    record.dump(node_rank)\n\t    return record\n\tdef distributed_write(num_nodes, num_processes):\n\t    complete_dataset = []\n\t    for i in range(num_nodes):\n\t        dataset, rootdir, features = build_weather_dataset()\n\t        parallel_write(dataset, i, os.path.join(rootdir, str(i)), num_processes)\n\t        complete_dataset = complete_dataset + dataset\n", "    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_nodes)\n\t    record.dump()\n\t    return record, complete_dataset, rootdir, features\n\tif __name__ == \"__main__\":\n\t    np.random.seed(0)\n\t    unittest.main()\n"]}
{"filename": "seqrecord/weather/test/test_wseqrecord.py", "chunked_list": ["\"\"\"Unit test for Wseqrecord's correctness and speed\"\"\"\n\timport unittest\n\tfrom typing import Dict, List\n\timport random\n\timport numpy as np\n\timport numpy.testing as nptest\n\timport seqrecord.weather.seqrecord as wseqrecord\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tdef test_iter_frames(file_size: int):\n\t    dataset, record, features = build_dataset()\n", "    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(frame_generator=iter(dataset))\n\t    record.dump_record()\n\t    # loaded_record = WSeqRecord.load_record_from_dict(\"./output/wseqrecord_test/\")\n\t    # decode dataset\n\t    for i, item in enumerate(record.iterate_frames(features=features)):\n\t        for feature in features:\n\t            nptest.assert_equal(\n\t                item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n", "            )\n\tdef test_iter_frame_pairs(file_size: int, max_pred_steps: int):\n\t    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n\t    dataset, record, features = build_weather_dataset()\n\t    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n\t    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(frame_generator=iter(dataset))\n\t    record.dump_record()\n\t    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n", "    random_partition = random.randint(1, len(features) - 2)\n\t    input_features = features[:random_partition]\n\t    output_features = features[random_partition:]\n\t    # read frame pairs\n\t    for i, item in enumerate(\n\t        loaded_record.iterate_framepairs(\n\t            input_features, output_features, max_pred_steps=max_pred_steps\n\t        )\n\t    ):\n\t        x, y, lookahead_steps = (\n", "            item[\"input\"],\n\t            item[\"target\"],\n\t            item[\"lookahead_steps\"],\n\t        )\n\t        lookahead_steps_stats[lookahead_steps - 1] += 1\n\t        x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t        y_target = np.vstack(\n\t            [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t        )\n\t        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n", "        nptest.assert_equal(\n\t            y,\n\t            y_target,\n\t            err_msg=\"\",\n\t            verbose=True,\n\t        )\n\t    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]\n\tdef test_fetch_frame_pairs(file_size: int, max_pred_steps: int):\n\t    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n\t    dataset, record, features = build_weather_dataset()\n", "    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n\t    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(frame_generator=iter(dataset))\n\t    record.dump_record()\n\t    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n\t    random_partition = random.randint(1, len(features) - 2)\n\t    input_features = features[:random_partition]\n\t    output_features = features[random_partition:]\n\t    # read frame pairs\n", "    for i, item in enumerate(\n\t        loaded_record.fetch_framepairs(\n\t            input_features,\n\t            output_features,\n\t            max_pred_steps=max_pred_steps,\n\t            prefetch_buffer_size=10,\n\t        )\n\t    ):\n\t        x, y, lookahead_steps = (\n\t            item[\"input\"],\n", "            item[\"target\"],\n\t            item[\"lookahead_steps\"],\n\t        )\n\t        lookahead_steps_stats[lookahead_steps - 1] += 1\n\t        x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t        y_target = np.vstack(\n\t            [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t        )\n\t        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t        nptest.assert_equal(\n", "            y,\n\t            y_target,\n\t            err_msg=\"\",\n\t            verbose=True,\n\t        )\n\t    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]\n\tdef test_async_read_frame_pairs(file_size: int, max_pred_steps: int):\n\t    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n\t    dataset, record, features = build_weather_dataset()\n\t    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n", "    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(frame_generator=iter(dataset))\n\t    record.dump_record()\n\t    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n\t    random_partition = random.randint(1, len(features) - 2)\n\t    input_features = features[:random_partition]\n\t    output_features = features[random_partition:]\n\t    # read frame pairs\n\t    for i, item in enumerate(\n", "        loaded_record.async_iterate_framepairs(\n\t            input_features, output_features, max_pred_steps=max_pred_steps\n\t        )\n\t    ):\n\t        x, y, lookahead_steps = (\n\t            item[\"input\"],\n\t            item[\"target\"],\n\t            item[\"lookahead_steps\"],\n\t        )\n\t        lookahead_steps_stats[lookahead_steps - 1] += 1\n", "        x_target = np.vstack([dataset[i][feature] for feature in input_features])\n\t        y_target = np.vstack(\n\t            [dataset[i + lookahead_steps][feature] for feature in output_features]\n\t        )\n\t        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t        nptest.assert_equal(\n\t            y,\n\t            y_target,\n\t            err_msg=\"\",\n\t            verbose=True,\n", "        )\n\t    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]\n\tdef test_threaded_write_frame(file_size: int, max_pred_steps: int):\n\t    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n\t    dataset, record, features = build_weather_dataset()\n\t    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n\t    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(iter(dataset), 5)\n\t    record.dump_record()\n", "    # loaded_record = WSeqRecord.load_record_from_dict(\"./output/wseqrecord_test/\")\n\t    # decode dataset\n\t    for i, item in enumerate(record.iterate_frames(features=features)):\n\t        for feature in features:\n\t            nptest.assert_equal(\n\t                item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n\t            )\n\tdef test_read_framepairs_from_file(file_size: int, max_pred_steps: int):\n\t    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n\t    dataset, record, features = build_weather_dataset()\n", "    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n\t    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(iter(dataset), 5)\n\t    record.dump_record()\n\t    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n\t    random_partition = random.randint(1, len(features) - 2)\n\t    read_args = {\n\t        \"input_features\": features[:random_partition],\n\t        \"target_features\": features[random_partition:],\n", "        \"max_pred_steps\": max_pred_steps,\n\t    }\n\t    loaded_record.add_read_args(read_args)\n\t    # read frame pairs\n\t    for i, item in enumerate(\n\t        WSeqRecord.iterate_framepairs_from_files(\n\t            ((loaded_record, i) for i in range(loaded_record.num_files))\n\t        )\n\t    ):\n\t        x, y, lookahead_steps = (\n", "            item[\"input\"],\n\t            item[\"target\"],\n\t            item[\"lookahead_steps\"],\n\t        )\n\t        lookahead_steps_stats[lookahead_steps - 1] += 1\n\t        x_target = np.vstack(\n\t            [dataset[i][feature] for feature in read_args[\"input_features\"]]\n\t        )\n\t        y_target = np.vstack(\n\t            [\n", "                dataset[i + lookahead_steps][feature]\n\t                for feature in read_args[\"target_features\"]\n\t            ]\n\t        )\n\t        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n\t        nptest.assert_equal(\n\t            y,\n\t            y_target,\n\t            err_msg=\"\",\n\t            verbose=True,\n", "        )\n\t    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]\n\tdef test_read_frames_from_file(\n\t    num_frames_in_input: int,\n\t    max_pred_steps: int,\n\t    num_frames: int,\n\t    file_size: int,\n\t):\n\t    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n\t    dataset, record, features = build_weather_dataset(num_frames)\n", "    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n\t    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n\t    # encode dataset\n\t    record.put_frame(iter(dataset), 5)\n\t    record.dump_record()\n\t    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n\t    read_args = {\n\t        \"input_features\": features,\n\t        \"target_features\": features,\n\t        \"max_pred_steps\": max_pred_steps,\n", "        \"hours_per_step\": 1,\n\t        \"num_frames_in_input\": num_frames_in_input,\n\t    }\n\t    loaded_record.set_framereader_args(read_args)\n\t    # read frame pairs\n\t    num_input_target_pairs = 0\n\t    for i, item in enumerate(\n\t        WSeqRecord.iterate_frames_from_file(\n\t            ((loaded_record, i) for i in range(loaded_record.num_files))\n\t        )\n", "    ):\n\t        print(f\"testing the {i}-th input-target pair\")\n\t        num_input_target_pairs += 1\n\t        x, y, lookahead_steps = (\n\t            item[\"input\"],\n\t            item[\"target\"],\n\t            item[\"lead_time\"],\n\t        )\n\t        x_target = [\n\t            np.stack(\n", "                [dataset[i + j][feature] for feature in read_args[\"input_features\"]],\n\t                axis=0,\n\t            )\n\t            for j in range(num_frames_in_input)\n\t        ]\n\t        for j in range(num_frames_in_input):\n\t            frame_idx, frame = x[j]\n\t            print(\n\t                f\"inside {i}-th input_frames, testing the {j}-th (rel) and {frame_idx}-th (abs) frame\"\n\t            )\n", "            nptest.assert_equal(\n\t                frame,\n\t                x_target[j],\n\t                err_msg=\"\",\n\t                verbose=True,\n\t            )\n\t        print(\n\t            f\"with {num_frames_in_input=} and {num_frames}, {lookahead_steps=} and target frame idx is {i + num_frames_in_input+ lookahead_steps-1}\"\n\t        )\n\t        y_target = np.stack(\n", "            [\n\t                dataset[i + num_frames_in_input + lookahead_steps - 1][feature]\n\t                for feature in read_args[\"target_features\"]\n\t            ]\n\t        )\n\t        nptest.assert_equal(\n\t            y,\n\t            y_target,\n\t            err_msg=\"\",\n\t            verbose=True,\n", "        )\n\t    print(\"num_input_target_pairs: \", num_input_target_pairs)\n\t    return num_input_target_pairs\n\tclass TestWSeqRecord(unittest.TestCase):\n\t    def test_encode_decode(self):\n\t        \"\"\"Testing encode and decode of frames.\"\"\"\n\t        print(\"testing reading frame\")\n\t        test_iter_frames(1e4)\n\t        test_iter_frames(1e6)\n\t        test_iter_frames(1e8)\n", "        test_iter_frames(1e10)\n\t    def test_read_frame_pairs(self):\n\t        \"\"\"Testing iteratively read frame pairs\"\"\"\n\t        # todo: fix feature bug and max_steps bugs\n\t        print(\"testing reading frame pairs\")\n\t        lookahead_stats = test_iter_frame_pairs(1e6, max_pred_steps=10)\n\t        lookahead_stats = test_iter_frame_pairs(1e8, max_pred_steps=5)\n\t        lookahead_stats = test_iter_frame_pairs(1e10, max_pred_steps=13)\n\t        lookahead_stats = test_iter_frame_pairs(1e6, max_pred_steps=100)\n\t        lookahead_stats = test_iter_frame_pairs(1e8, max_pred_steps=1000)\n", "        lookahead_stats = test_iter_frame_pairs(1e10, max_pred_steps=10000)\n\t    def test_fetch_frame_pairs(self):\n\t        \"\"\"Tesing reading frame pairs with prefetching\"\"\"\n\t        print(\"testing fetching frame pairs\")\n\t        test_fetch_frame_pairs(1e6, max_pred_steps=10)\n\t        test_fetch_frame_pairs(1e8, max_pred_steps=5)\n\t        test_fetch_frame_pairs(1e10, max_pred_steps=13)\n\t        test_fetch_frame_pairs(1e6, max_pred_steps=100)\n\t        test_fetch_frame_pairs(1e8, max_pred_steps=1000)\n\t        test_fetch_frame_pairs(1e10, max_pred_steps=10000)\n", "    def test_async_read_frame_pairs(self):\n\t        \"\"\"Testing reading frame pairs using asyncio\"\"\"\n\t        print(\"testing async read frame pairs\")\n\t        test_async_read_frame_pairs(1e6, max_pred_steps=10)\n\t        test_async_read_frame_pairs(1e8, max_pred_steps=5)\n\t        test_async_read_frame_pairs(1e10, max_pred_steps=13)\n\t        test_async_read_frame_pairs(1e6, max_pred_steps=100)\n\t        test_async_read_frame_pairs(1e8, max_pred_steps=1000)\n\t        test_async_read_frame_pairs(1e10, max_pred_steps=10000)\n\t    def test_single_async(self):\n", "        print(\"Single test\")\n\t        test_async_read_frame_pairs(1e8, max_pred_steps=1000)\n\t    def test_threaded_write(self):\n\t        test_threaded_write_frame(1e6, max_pred_steps=100)\n\t    def test_read_framepairs_from_file(self):\n\t        print(\"Testing reading frame files\")\n\t        test_read_framepairs_from_file(1e6, max_pred_steps=100)\n\t    def test_read_frames_from_file(self):\n\t        print(\"Testing reading frames from file\")\n\t        num_frames_in_input = 5\n", "        max_pred_steps = 10\n\t        num_frames = 15\n\t        num_inputtarget_pairs = test_read_frames_from_file(\n\t            num_frames_in_input, max_pred_steps, num_frames, 1e6\n\t        )\n\t        self.assertEqual(num_inputtarget_pairs, num_frames - num_frames_in_input)\n\t        # todo: test with inconsecutive frames\n\tdef build_dataset():\n\t    \"\"\"Generate an aritificial dataset to test methods of SeqRecord, w\"\"\"\n\t    rootdir = \"./output/wseqrecord_test/\"\n", "    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    time_horizon = 1000\n\t    dataset = [{} for _ in range(time_horizon)]\n\t    for feature in features:\n\t        shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n\t        dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n\t        for i in range(time_horizon):\n\t            dataset[i][feature] = (\n\t                np.random.rand(shape[0], shape[1])\n\t                if dtype == \"float32\"\n", "                else np.random.randint(low=0, high=255, size=shape)\n\t            )\n\t    record = WSeqRecord(rootdir)\n\t    features = [feature for feature in features]\n\t    return dataset, record, features\n\tdef build_weather_dataset(time_horizon: int = 1000):\n\t    \"\"\"Generate an aritificial weather dataset so that each feature has the same size\"\"\"\n\t    rootdir = \"./output/wseqrecord_test/\"\n\t    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n\t    dataset = [{} for _ in range(time_horizon)]\n", "    shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n\t    dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n\t    for feature in features:\n\t        for i in range(time_horizon):\n\t            dataset[i][feature] = (\n\t                np.random.rand(shape[0], shape[1])\n\t                if dtype == \"float32\"\n\t                else np.random.randint(low=0, high=255, size=shape)\n\t            )\n\t    record = WSeqRecord(rootdir)\n", "    features = [feature for feature in features]\n\t    return dataset, record, features\n\tif __name__ == \"__main__\":\n\t    np.random.seed(0)\n\t    unittest.main()\n"]}
{"filename": "seqrecord/weather/test/test_utils.py", "chunked_list": ["from seqrecord.utils import distribute_loads\n\timport unittest\n\tclass TestUtils(unittest.TestCase):\n\t    def test_distribute_loads(self):\n\t        works = 10\n\t        num_processes = 2\n\t        print(\n\t            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        num_processes = 3\n", "        print(\n\t            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        num_processes = 4\n\t        print(\n\t            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        works = 11\n\t        num_processes = 2\n\t        print(\n", "            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        works = 19\n\t        num_processes = 2\n\t        print(\n\t            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        works = 11\n\t        num_processes = 2\n\t        print(\n", "            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        works = 19\n\t        num_processes = 10\n\t        print(\n\t            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        works = 11\n\t        num_processes = 5\n\t        print(\n", "            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n\t        )\n\t        works = 44\n\t        num_processes = 10\n\t        res = distribute_loads(works, num_processes)\n\t        print(\n\t            f\"{works=}, {num_processes=}, the result is: {res}, with maximum load {max(r[1]-r[0] for r in res)}\"\n\t        )\n\t        works = 44\n\t        num_processes = 21\n", "        res = distribute_loads(works, num_processes)\n\t        print(\n\t            f\"{works=}, {num_processes=}, the result is: {res}, with maximum load {max(r[1]-r[0] for r in res)}\"\n\t        )\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord_debug.py", "chunked_list": ["import os\n\timport sys\n\timport numpy as np\n\timport xarray as xr\n\timport psutil\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tfrom seqrecord.utils import distribute_loads\n\timport os\n\tfrom typing import List, Tuple, Iterator\n\timport re\n", "import numpy as np\n\tfrom tqdm import tqdm\n\tfrom multiprocessing import Process\n\timport click\n\tfrom itertools import islice\n\tfrom seqrecord.weather.format.constants import NAME_TO_VAR\n\tsingle_vars = [\n\t    \"2m_temperature\",\n\t    # \"10m_u_component_of_wind\",\n\t    # \"10m_v_component_of_wind\",\n", "    # \"mean_sea_level_pressure\",\n\t    # # \"surface_pressure\",\n\t    # # \"2m_dewpoint_temperature\",\n\t    # # \"total_precipitation\",\n\t    # \"total_cloud_cover\",\n\t    # \"total_column_water_vapour\",\n\t]\n\tHOURS_PER_MONTH = 744\n\tHOURS_PER_SHARD = 384\n\tFRAME_BUFFER_SIZE = 50\n", "def frame_generator(rank: int, path, year, month_gen: Iterator):\n\t    def get_num_frames_in_month(m):\n\t        var = single_vars[0]\n\t        var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n\t        ds = xr.open_dataset(var_path)\n\t        return ds[NAME_TO_VAR[var]].shape[0]\n\t    for m in month_gen:\n\t        num_frames_in_month = get_num_frames_in_month(m)\n\t        print(f\"{num_frames_in_month=}\")\n\t        for frame_idx in range(0, num_frames_in_month, FRAME_BUFFER_SIZE):\n", "            np_vars = {}\n\t            end_frame_idx = min(frame_idx + FRAME_BUFFER_SIZE, num_frames_in_month)\n\t            for var in single_vars:\n\t                var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n\t                ds = xr.open_dataset(var_path)\n\t                code = NAME_TO_VAR[var]\n\t                assert len(ds[code].shape) == 3\n\t                np_vars[var] = ds[code][frame_idx:end_frame_idx].to_numpy()\n\t                del ds\n\t                # assert np_vars[var].shape[0] == HOURS_PER_MONTH\n", "            print(f\"/n RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\")\n\t            print(\n\t                f\"memory of the np array: {sum(arr.nbytes for _, arr in np_vars.items())}\"\n\t            )\n\t            for frame_idx in range(0, end_frame_idx - frame_idx):\n\t                frame = {}\n\t                for key in np_vars:\n\t                    frame[key] = np_vars[key][frame_idx]\n\t                print(\n\t                    f\"memory of the frame np array: {sum(arr.nbytes for _, arr in frame.items())} /n\"\n", "                )\n\t                yield frame\n\t                break\n\tdef grib2np(rank, world_size, config, year, month_gen):\n\t    \"\"\"\n\t    Convert grib files to numpy arrays and save them to disk.\n\t    \"\"\"\n\t    sub_wseqrecord = WSeqRecord(\n\t        os.path.join(\n\t            config[\"wseqrecord_dir\"],\n", "            WSeqRecord.subfolder_name(rank, world_size),\n\t        )\n\t    )\n\t    sub_wseqrecord.put_frame(\n\t        frame_generator(rank, config[\"grib_dataset_dir\"], year, month_gen)\n\t    )\n\t    sub_wseqrecord.dump_record(rank=rank)\n\t# notes: # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n\t@click.command()\n\t@click.option(\n", "    \"--dataset-mount-dir\", type=str, default=\"/datadrive/azure_storage/weathereastus\"\n\t)\n\t@click.option(\"--year\", type=int, required=True)\n\t@click.option(\"--num-processes\", type=int, default=6)\n\tdef main(dataset_mount_dir: str, year: int, num_processes: int):\n\t    print(f\"configs {dataset_mount_dir=}, {year=}, {num_processes=}.\\n\")\n\t    year = 1979\n\t    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n\t    DEFAULT_CONFIG = {\n\t        \"num_processes\": num_processes,\n", "        \"wseqrecord_dir\": os.path.join(\n\t            dataset_mount_dir,\n\t            f\"era5seqrecord/test/{year}\",\n\t        ),\n\t        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n\t    }\n\t    config = DEFAULT_CONFIG\n\t    month_generator = range(1, 13)\n\t    dividens = distribute_loads(12, config[\"num_processes\"])\n\t    processes = []\n", "    for i in range(config[\"num_processes\"]):\n\t        sub_month_generator = islice(month_generator, dividens[i][0], dividens[i][1])\n\t        p = Process(\n\t            target=grib2np,\n\t            args=(i, config[\"num_processes\"], config, year, sub_month_generator),\n\t        )\n\t        processes.append(p)\n\t        p.start()\n\t    # all process complete successfully\n\t    for i in range(config[\"num_processes\"]):\n", "        processes[i].join()\n\t    # combine sub-seqrecord\n\t    print(\"Combining meta-seqrecord\")\n\t    record = WSeqRecord.gather_subseqrecords(\n\t        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n\t    )\n\t    record.dump()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "seqrecord/weather/format/weathernp2seq.py", "chunked_list": ["\"\"\"transform existing weather dataset to sequence weather format.\n\t\"\"\"\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\timport os\n\tfrom typing import List\n\timport re\n\timport numpy as np\n\tfrom tqdm import tqdm\n\t# test correctness of saved seqrecord format\n\t# dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n", "DEFAULT_CONFIG = {\n\t    \"dataset_mount_dir\": \"/datadrive/weatherdatastorage2/datasets\",  # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n\t    \"wsrecord_dir\": \"CMIP6/MPI-ESM/wseqrecord/test/1.40625deg_equally_np_all_levels/train/\",\n\t    \"wdataset_dir\": \"CMIP6/MPI-ESM/1.40625deg_equally_np_all_levels/train/\",\n\t    \"wdataset_split\": \"train\",\n\t}\n\tdef sort_weatherfiles(files: List[os.DirEntry]) -> List[os.DirEntry]:\n\t    \"\"\"Return sorted files in dir, make sure files are sorted incrementally in time.\n\t    # todo: check with Jayesh this sorting is correct\n\t    Example file names: 195001010600-195501010000_33.npz from CMIP6/MPI-ESM/1.40625deg_equally_np_all_levels/train/\n", "    Args:\n\t        files (List[os.DirEntry]): each element in list is a file name\n\t    \"\"\"\n\t    def str2nums(direntry):\n\t        nums = re.split(\"-|_\", direntry.name.split(\".\")[0])\n\t        nums = tuple(map(int, nums))\n\t        return nums\n\t    return sorted(files, key=str2nums)\n\tdef main(config: dict) -> None:\n\t    wsrecord = WSeqRecord(\n", "        os.path.join(config[\"dataset_mount_dir\"], config[\"wsrecord_dir\"])\n\t    )\n\t    files_dirs = os.scandir(\n\t        os.path.join(config[\"dataset_mount_dir\"], config[\"wdataset_dir\"])\n\t    )\n\t    files = list(filter(lambda direntry: direntry.is_file(), files_dirs))\n\t    files = sort_weatherfiles(files)\n\t    def frame_generator(files):\n\t        i = 0\n\t        for file_path in tqdm(files):\n", "            if i >= 50:\n\t                break\n\t            data = np.load(file_path)\n\t            num_frames = data[data.files[0]].shape[0]\n\t            for rel_frame_idx in range(num_frames):\n\t                frame = {}\n\t                for key in data.files:\n\t                    frame[key] = data[key][rel_frame_idx]\n\t                yield frame\n\t            i += 1\n", "    wsrecord.put_frame(frame_generator(files), 5)\n\t    wsrecord.dump()\n\tif __name__ == \"__main__\":\n\t    config = DEFAULT_CONFIG\n\t    main(config)\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord_mp.py", "chunked_list": ["import os\n\timport math\n\timport numpy as np\n\timport xarray as xr\n\timport psutil\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tfrom seqrecord.utils import distribute_loads\n\timport os\n\tfrom typing import List, Tuple, Iterator\n\timport click\n", "import numpy as np\n\tfrom tqdm import tqdm\n\tfrom multiprocessing import Process\n\tfrom itertools import islice\n\tfrom seqrecord.weather.constants import NAME_TO_VAR\n\tsingle_vars = [\n\t    \"2m_temperature\",\n\t    \"10m_u_component_of_wind\",\n\t    \"10m_v_component_of_wind\",\n\t    \"mean_sea_level_pressure\",\n", "    \"surface_pressure\",\n\t    \"2m_dewpoint_temperature\",\n\t    # \"total_precipitation\",\n\t    \"skin_temperature\",\n\t    \"sea_surface_temperature\",\n\t    \"total_cloud_cover\",\n\t    \"total_column_water_vapour\",\n\t]\n\tpressure_vars = [\n\t    \"geopotential\",\n", "    \"specific_humidity\",\n\t    \"temperature\",\n\t    \"u_component_of_wind\",\n\t    \"v_component_of_wind\",\n\t]\n\tLEVELS = [\n\t    1,\n\t    2,\n\t    3,\n\t    5,\n", "    7,\n\t    10,\n\t    20,\n\t    30,\n\t    50,\n\t    70,\n\t    100,\n\t    125,\n\t    150,\n\t    175,\n", "    200,\n\t    225,\n\t    250,\n\t    300,\n\t    350,\n\t    400,\n\t    450,\n\t    500,\n\t    550,\n\t    600,\n", "    650,\n\t    700,\n\t    750,\n\t    775,\n\t    800,\n\t    825,\n\t    850,\n\t    875,\n\t    900,\n\t    925,\n", "    950,\n\t    975,\n\t    1000,\n\t]\n\t#\n\tHOURS_PER_MONTH = 744\n\tHOURS_PER_SHARD = 384\n\tdef frame_generator(rank: int, path, year, month_gen: Iterator):\n\t    for m in month_gen:\n\t        ds_files = {}\n", "        num_frames_in_month = -1\n\t        for var in single_vars:\n\t            var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n\t            code = NAME_TO_VAR[var]\n\t            ds_files[var] = xr.open_dataset(var_path)[code]\n\t            if num_frames_in_month == -1:\n\t                num_frames_in_month = ds_files[var].shape[0]\n\t        for var in pressure_vars:\n\t            for level in LEVELS:\n\t                var_path = os.path.join(\n", "                    path,\n\t                    f\"{year}\",\n\t                    f\"{m:02d}\",\n\t                    f\"{var}_{level}.grib\",\n\t                )\n\t                code = NAME_TO_VAR[var]\n\t                ds_files[f\"{var}_{level}\"] = xr.open_dataset(var_path)[code]\n\t        pbar = (\n\t            tqdm(\n\t                range(num_frames_in_month),\n", "                desc=f\"formating frames in month {m}\",\n\t                total=num_frames_in_month,\n\t            )\n\t            if rank == 0\n\t            else range(num_frames_in_month)\n\t        )\n\t        if rank == 0:\n\t            print(\n\t                f\"/n When rank 0 opens one month's files, the RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\"\n\t            )\n", "        for frame_idx in pbar:\n\t            np_vars = {}\n\t            for key, item in ds_files.items():\n\t                np_vars[key] = item[frame_idx].to_numpy()\n\t            yield np_vars\n\tdef grib2np(rank, world_size, config, year, month_gen):\n\t    \"\"\"\n\t    Convert grib files to numpy arrays and save them to disk.\n\t    \"\"\"\n\t    sub_wseqrecord = WSeqRecord(\n", "        recorddir=os.path.join(\n\t            config[\"wseqrecord_dir\"],\n\t            WSeqRecord.subfolder_name(rank, world_size),\n\t        ),\n\t        local_cache_dir=os.path.join(\n\t            config[\"local_cache_dir\"],\n\t            WSeqRecord.subfolder_name(rank, world_size),\n\t        ),\n\t    )\n\t    sub_wseqrecord.put_frame(\n", "        frame_generator(rank, config[\"grib_dataset_dir\"], year, month_gen)\n\t    )\n\t    print(\"rank\", rank, \" finished, dummping metadata!\")\n\t    sub_wseqrecord.dump_record(rank=rank)\n\t@click.command()\n\t@click.option(\n\t    \"--dataset-mount-dir\", type=str, default=\"/datadrive/azure_storage/weathereastus\"\n\t)\n\t@click.option(\"--local-cache-dir\", type=str, default=\"~/record_cache\")\n\t@click.option(\"--year\", type=int, required=True)\n", "@click.option(\"--num-processes\", type=int, default=12)\n\tdef main(dataset_mount_dir: str, local_cache_dir: str, year: int, num_processes: int):\n\t    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n\t    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n\t    DEFAULT_CONFIG = {\n\t        \"num_processes\": num_processes,\n\t        \"local_cache_dir\": local_cache_dir,\n\t        \"wseqrecord_dir\": os.path.join(\n\t            dataset_mount_dir,\n\t            f\"era5seqrecord/test/{year}\",\n", "        ),\n\t        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n\t    }\n\t    config = DEFAULT_CONFIG\n\t    month_generator = range(1, 13)\n\t    dividens = distribute_loads(12, config[\"num_processes\"])\n\t    processes = []\n\t    for i in range(config[\"num_processes\"]):\n\t        sub_month_generator = islice(month_generator, dividens[i][0], dividens[i][1])\n\t        p = Process(\n", "            target=grib2np,\n\t            args=(i, config[\"num_processes\"], config, year, sub_month_generator),\n\t        )\n\t        processes.append(p)\n\t        p.start()\n\t    # all process complete successfully\n\t    for i in range(config[\"num_processes\"]):\n\t        processes[i].join()\n\t    # combine sub-seqrecord\n\t    record = WSeqRecord.gather_subseqrecords(\n", "        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n\t    )\n\t    record.dump_record()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "seqrecord/weather/format/post_dist.py", "chunked_list": ["import os\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tdef main():\n\t    years = range(1979, 2016)\n\t    recorddir = \"/datadrive/azure_storage/weathereastus/era5seqrecord/aml_dist\"\n\t    for year in years:\n\t        print(f\"Gathering {year}'s data\")\n\t        sub_dir = os.path.join(recorddir, str(year))\n\t        WSeqRecord.gather_subseqrecords(sub_dir, 12).dump(rank=year - 1979)\n\t    print(\"Gathering all years' data\")\n", "    WSeqRecord.gather_subseqrecords(\n\t        recorddir,\n\t        len(years),\n\t        rank2folder={i: str(year) for i, year in enumerate(years)},\n\t    ).dump()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord.py", "chunked_list": ["import os\n\timport math\n\timport numpy as np\n\timport xarray as xr\n\timport psutil\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tfrom seqrecord.utils import distribute_loads\n\timport os\n\tfrom typing import List, Tuple, Iterator\n\timport click\n", "import numpy as np\n\tfrom tqdm import tqdm\n\tfrom seqrecord.weather.constants import NAME_TO_VAR\n\tsingle_vars = [\n\t    \"2m_temperature\",\n\t    \"10m_u_component_of_wind\",\n\t    \"10m_v_component_of_wind\",\n\t    \"mean_sea_level_pressure\",\n\t    \"surface_pressure\",\n\t    \"2m_dewpoint_temperature\",\n", "    # \"total_precipitation\",\n\t    \"skin_temperature\",\n\t    \"sea_surface_temperature\",\n\t    \"total_cloud_cover\",\n\t    \"total_column_water_vapour\",\n\t]\n\tpressure_vars = [\n\t    \"geopotential\",\n\t    \"specific_humidity\",\n\t    \"temperature\",\n", "    \"u_component_of_wind\",\n\t    \"v_component_of_wind\",\n\t]\n\tLEVELS = [\n\t    1,\n\t    2,\n\t    3,\n\t    5,\n\t    7,\n\t    10,\n", "    20,\n\t    30,\n\t    50,\n\t    70,\n\t    100,\n\t    125,\n\t    150,\n\t    175,\n\t    200,\n\t    225,\n", "    250,\n\t    300,\n\t    350,\n\t    400,\n\t    450,\n\t    500,\n\t    550,\n\t    600,\n\t    650,\n\t    700,\n", "    750,\n\t    775,\n\t    800,\n\t    825,\n\t    850,\n\t    875,\n\t    900,\n\t    925,\n\t    950,\n\t    975,\n", "    1000,\n\t]\n\t#\n\tHOURS_PER_MONTH = 744\n\tHOURS_PER_SHARD = 384\n\tdef frame_generator(path, year):\n\t    def get_num_frames_in_month(m: int) -> int:\n\t        \"\"\"Read a single variable of one-month data and return the number of frames in that month\n\t        Assume:\n\t            within a month, every variable has the same number of frames\n", "        Args:\n\t            m (int): _description_\n\t        Returns:\n\t            int: _description_\n\t        \"\"\"\n\t        var = single_vars[0]\n\t        var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n\t        ds = xr.open_dataset(var_path)\n\t        return ds[NAME_TO_VAR[var]].shape[0]\n\t    for m in tqdm(range(1, 13)):\n", "        num_frames_in_month = get_num_frames_in_month(m)\n\t        np_vars = {}\n\t        for var in single_vars:\n\t            var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n\t            ds = xr.open_dataset(var_path)\n\t            code = NAME_TO_VAR[var]\n\t            assert len(ds[code].shape) == 3\n\t            np_vars[var] = ds[code].to_numpy()\n\t            del ds\n\t            # assert np_vars[var].shape[0] == HOURS_PER_MONTH\n", "        for var in pressure_vars:\n\t            for level in LEVELS:\n\t                var_path = os.path.join(\n\t                    path,\n\t                    f\"{year}\",\n\t                    f\"{m:02d}\",\n\t                    f\"{var}_{level}.grib\",\n\t                )\n\t                ds = xr.open_dataset(var_path)\n\t                code = NAME_TO_VAR[var]\n", "                np_vars[f\"{var}_{level}\"] = ds[code].to_numpy()\n\t                del ds\n\t                # assert np_vars[f\"{var}_{level}\"].shape[0] == HOURS_PER_MONTH\n\t        for i in range(num_frames_in_month):\n\t            frame = {}\n\t            for key in np_vars:\n\t                frame[key] = np_vars[key][i]\n\t            yield frame\n\tdef grib2np(record, config, year):\n\t    \"\"\"\n", "    Convert grib files to numpy arrays and save them to disk.\n\t    \"\"\"\n\t    record.put_frame(frame_generator(config[\"grib_dataset_dir\"], year))\n\t@click.command()\n\t@click.option(\n\t    \"--dataset-mount-dir\", type=str, default=\"/datadrive/azure_storage/weathereastus\"\n\t)\n\t@click.option(\"--year\", type=int, required=True)\n\tdef main(dataset_mount_dir: str, year: int):\n\t    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n", "    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n\t    DEFAULT_CONFIG = {\n\t        \"wseqrecord_dir\": os.path.join(\n\t            dataset_mount_dir,\n\t            f\"era5seqrecord/aml/{year}\",\n\t        ),\n\t        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n\t    }\n\t    config = DEFAULT_CONFIG\n\t    record = WSeqRecord(config[\"wseqrecord_dir\"])\n", "    grib2np(record, config, year)\n\t    print(\"Dumping\")\n\t    record.dump_record()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord_dist.py", "chunked_list": ["\"\"\"Running distributed (over nodes) transformation of gribdata to seqrecord.\n\t    No join operation is performed.\n\tReturns:\n\t    _type_: _description_\n\tYields:\n\t    _type_: _description_\n\t\"\"\"\n\timport sys\n\timport os\n\timport xarray as xr\n", "import psutil\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\tfrom seqrecord.utils import distribute_loads\n\timport os\n\tfrom typing import List, Tuple, Iterator\n\tfrom tqdm import tqdm\n\timport torch.distributed as dist\n\tfrom itertools import islice\n\tfrom seqrecord.weather.constants import NAME_TO_VAR\n\tNUM_PROCESS_PER_NODE = 12  # make sure each node can handle processing 12 processes in the same time (months)\n", "single_vars = [\n\t    \"2m_temperature\",\n\t    \"10m_u_component_of_wind\",\n\t    \"10m_v_component_of_wind\",\n\t    \"mean_sea_level_pressure\",\n\t    \"surface_pressure\",\n\t    \"2m_dewpoint_temperature\",\n\t    # \"total_precipitation\",\n\t    \"skin_temperature\",\n\t    \"sea_surface_temperature\",\n", "    \"total_cloud_cover\",\n\t    \"total_column_water_vapour\",\n\t]\n\tpressure_vars = [\n\t    \"geopotential\",\n\t    \"specific_humidity\",\n\t    \"temperature\",\n\t    \"u_component_of_wind\",\n\t    \"v_component_of_wind\",\n\t]\n", "LEVELS = [\n\t    1,\n\t    2,\n\t    3,\n\t    5,\n\t    7,\n\t    10,\n\t    20,\n\t    30,\n\t    50,\n", "    70,\n\t    100,\n\t    125,\n\t    150,\n\t    175,\n\t    200,\n\t    225,\n\t    250,\n\t    300,\n\t    350,\n", "    400,\n\t    450,\n\t    500,\n\t    550,\n\t    600,\n\t    650,\n\t    700,\n\t    750,\n\t    775,\n\t    800,\n", "    825,\n\t    850,\n\t    875,\n\t    900,\n\t    925,\n\t    950,\n\t    975,\n\t    1000,\n\t]\n\t#\n", "HOURS_PER_MONTH = 744\n\tHOURS_PER_SHARD = 384\n\tdef frame_generator(year, local_rank, path):\n\t    \"\"\"Process one month's data of $year.\n\t    Args:\n\t        year (_type_): _description_\n\t        local_rank (_type_): _description_\n\t        path (_type_): _description_\n\t    Yields:\n\t        _type_: _description_\n", "    \"\"\"\n\t    month = local_rank + 1  # local_rank : [0-12), month: [1, 13)\n\t    ds_files = {}\n\t    num_frames_in_month = -1\n\t    for var in single_vars:\n\t        var_path = os.path.join(path, f\"{year}\", f\"{month:02d}\", f\"{var}_0.grib\")\n\t        code = NAME_TO_VAR[var]\n\t        ds_files[var] = xr.open_dataset(var_path)[code]\n\t        if num_frames_in_month == -1:\n\t            num_frames_in_month = ds_files[var].shape[0]\n", "    for var in pressure_vars:\n\t        for level in LEVELS:\n\t            var_path = os.path.join(\n\t                path,\n\t                f\"{year}\",\n\t                f\"{month:02d}\",\n\t                f\"{var}_{level}.grib\",\n\t            )\n\t            code = NAME_TO_VAR[var]\n\t            ds_files[f\"{var}_{var}\"] = xr.open_dataset(var_path)[code]\n", "    pbar = (\n\t        tqdm(\n\t            range(num_frames_in_month),\n\t            desc=f\"formating frames in month {month}\",\n\t            total=num_frames_in_month,\n\t        )\n\t        if local_rank == 0\n\t        else range(num_frames_in_month)\n\t    )\n\t    if local_rank == 0:\n", "        print(\n\t            f\"/n When rank 0 opens one month's files, the RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\"\n\t        )\n\t    for frame_idx in pbar:\n\t        np_vars = {}\n\t        for key, item in ds_files.items():\n\t            np_vars[key] = item[frame_idx].to_numpy()\n\t        yield np_vars\n\tdef grib2np(year, local_rank, config):\n\t    \"\"\"\n", "    Convert grib files to numpy arrays and save them to disk.\n\t    \"\"\"\n\t    sub_wseqrecord = WSeqRecord(\n\t        os.path.join(\n\t            config[\"wseqrecord_dir\"],\n\t            f\"{year}\",\n\t            WSeqRecord.subfolder_name(local_rank, NUM_PROCESS_PER_NODE),\n\t        ),\n\t        local_cache_dir=os.path.join(\n\t            config[\"local_cache_dir\"],\n", "            f\"{year}\",\n\t            WSeqRecord.subfolder_name(local_rank, NUM_PROCESS_PER_NODE),\n\t        ),\n\t    )\n\t    sub_wseqrecord.put_frame(\n\t        frame_generator(year, local_rank, config[\"grib_dataset_dir\"])\n\t    )\n\t    sub_wseqrecord.dump_record(rank=local_rank)\n\t# def initialize():\n\t#     dist.init_process_group(backend=\"gloo\")\n", "def main(num_nodes: int):\n\t    \"\"\"Each node formats several years' data (one year at a time). Within each node, spawns 12 processes where\n\t    each process takes care of one month's data.\n\t    Args:\n\t        dataset_mount_dir (str): _description_\n\t        num_nodes (int): _description_\n\t    \"\"\"\n\t    # initialize()\n\t    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n\t    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n", "    dataset_mount_dir = \"/mnt\"\n\t    DEFAULT_CONFIG = {\n\t        \"wseqrecord_dir\": os.path.join(\n\t            dataset_mount_dir,\n\t            f\"era5seqrecord/aml_dist/\",\n\t        ),\n\t        \"local_cache_dir\": \"~/record_cache\",\n\t        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n\t    }\n\t    config = DEFAULT_CONFIG\n", "    source_generator = range(1979, 2016)  # full-dataset: 1979-2023\n\t    dividens = distribute_loads(len(source_generator), num_nodes)\n\t    # each node processes one year's data. torchrun will spawn 12 processes within each node, and each process (identified by local_rank) processes one month's data.\n\t    # ref: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-distributed-gpu\n\t    node_rank = int(os.environ[\"NODE_RANK\"])\n\t    local_rank = int(os.environ[\"LOCAL_RANK\"])\n\t    sub_datasource_generator = islice(\n\t        source_generator, dividens[node_rank][0], dividens[node_rank][1]\n\t    )\n\t    for year in sub_datasource_generator:\n", "        print(\n\t            f\"Transforming {year}'s data on the {node_rank}-th node with local rank {local_rank}\"\n\t        )\n\t        grib2np(\n\t            year,\n\t            local_rank,  # local_rank: range(0, 12) corresponds month range(1, 13)\n\t            config,\n\t        )\n\t    print(\n\t        f\"Transforming {year}'s data on the {node_rank}-th node with local rank {local_rank} is done.\"\n", "    )\n\t    # # combine sub-seqrecord\n\t    # record = WSeqRecord.gather_subseqrecords(\n\t    #     config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n\t    # )\n\t    # record.dump()\n\tif __name__ == \"__main__\":\n\t    num_nodes = int(sys.argv[1])\n\t    main(num_nodes)\n"]}
{"filename": "seqrecord/weather/format/validation.py", "chunked_list": ["from seqrecord import WSeqRecord, build_wdatapipe\n\timport torch\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom torchdata.dataloader2 import (\n\t    DataLoader2,\n\t    DistributedReadingService,\n\t    MultiProcessingReadingService,\n\t    SequentialReadingService,\n\t)\n", "from time import perf_counter\n\timport click\n\timport torch.distributed as dist\n\timport torch.multiprocessing as mp\n\timport os\n\timport sys\n\tMASTER_ADDR = \"127.0.0.1\"\n\tWORLD_SIZE = 2\n\timport socket\n\tdef _get_open_port():\n", "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t    s.bind((\"\", 0))\n\t    port = s.getsockname()[1]\n\t    s.close()\n\t    return str(port)\n\tos.environ[\"MASTER_ADDR\"] = MASTER_ADDR\n\tos.environ[\"MASTER_PORT\"] = _get_open_port()\n\t# constants\n\tVAR = [\n\t    \"2m_temperature\",\n", "    \"10m_u_component_of_wind\",\n\t    \"10m_v_component_of_wind\",\n\t    \"mean_sea_level_pressure\",\n\t    \"total_cloud_cover\",\n\t    \"total_column_water_vapour\",\n\t    \"geopotential_1\",\n\t    \"geopotential_2\",\n\t    \"geopotential_3\",\n\t    \"geopotential_5\",\n\t    \"geopotential_7\",\n", "    \"geopotential_10\",\n\t    \"geopotential_20\",\n\t    \"geopotential_30\",\n\t    \"geopotential_70\",\n\t    \"geopotential_125\",\n\t    \"geopotential_175\",\n\t    \"geopotential_225\",\n\t    \"geopotential_350\",\n\t    \"geopotential_450\",\n\t    \"geopotential_550\",\n", "    \"geopotential_650\",\n\t    \"geopotential_750\",\n\t    \"geopotential_775\",\n\t    \"geopotential_800\",\n\t    \"geopotential_825\",\n\t    \"geopotential_875\",\n\t    \"geopotential_900\",\n\t    \"geopotential_950\",\n\t    \"geopotential_975\",\n\t    \"specific_humidity_1\",\n", "    \"specific_humidity_2\",\n\t    \"specific_humidity_3\",\n\t    \"specific_humidity_5\",\n\t    \"specific_humidity_7\",\n\t    \"specific_humidity_10\",\n\t    \"specific_humidity_20\",\n\t    \"specific_humidity_30\",\n\t    \"specific_humidity_70\",\n\t    \"specific_humidity_125\",\n\t    \"specific_humidity_175\",\n", "    \"specific_humidity_225\",\n\t    \"specific_humidity_350\",\n\t    \"specific_humidity_450\",\n\t    \"specific_humidity_550\",\n\t    \"specific_humidity_650\",\n\t    \"specific_humidity_750\",\n\t    \"specific_humidity_775\",\n\t    \"specific_humidity_800\",\n\t    \"specific_humidity_825\",\n\t    \"specific_humidity_875\",\n", "    \"specific_humidity_900\",\n\t    \"specific_humidity_950\",\n\t    \"specific_humidity_975\",\n\t    \"temperature_1\",\n\t    \"temperature_2\",\n\t    \"temperature_3\",\n\t    \"temperature_5\",\n\t    \"temperature_7\",\n\t    \"temperature_10\",\n\t    \"temperature_20\",\n", "    \"temperature_30\",\n\t    \"temperature_70\",\n\t    \"temperature_125\",\n\t    \"temperature_175\",\n\t    \"temperature_225\",\n\t    \"temperature_350\",\n\t    \"temperature_450\",\n\t    \"temperature_550\",\n\t    \"temperature_650\",\n\t    \"temperature_750\",\n", "    \"temperature_775\",\n\t    \"temperature_800\",\n\t    \"temperature_825\",\n\t    \"temperature_875\",\n\t    \"temperature_900\",\n\t    \"temperature_950\",\n\t    \"temperature_975\",\n\t    \"u_component_of_wind_1\",\n\t    \"u_component_of_wind_2\",\n\t    \"u_component_of_wind_3\",\n", "    \"u_component_of_wind_5\",\n\t    \"u_component_of_wind_7\",\n\t    \"u_component_of_wind_10\",\n\t    \"u_component_of_wind_20\",\n\t    \"u_component_of_wind_30\",\n\t    \"u_component_of_wind_70\",\n\t    \"u_component_of_wind_125\",\n\t    \"u_component_of_wind_175\",\n\t    \"u_component_of_wind_225\",\n\t    \"u_component_of_wind_350\",\n", "    \"u_component_of_wind_450\",\n\t    \"u_component_of_wind_550\",\n\t    \"u_component_of_wind_650\",\n\t    \"u_component_of_wind_750\",\n\t    \"u_component_of_wind_775\",\n\t    \"u_component_of_wind_800\",\n\t    \"u_component_of_wind_825\",\n\t    \"u_component_of_wind_875\",\n\t    \"u_component_of_wind_900\",\n\t    \"u_component_of_wind_950\",\n", "    \"u_component_of_wind_975\",\n\t    \"v_component_of_wind_1\",\n\t    \"v_component_of_wind_2\",\n\t    \"v_component_of_wind_3\",\n\t    \"v_component_of_wind_5\",\n\t    \"v_component_of_wind_7\",\n\t    \"v_component_of_wind_10\",\n\t    \"v_component_of_wind_20\",\n\t    \"v_component_of_wind_30\",\n\t    \"v_component_of_wind_70\",\n", "    \"v_component_of_wind_125\",\n\t    \"v_component_of_wind_175\",\n\t    \"v_component_of_wind_225\",\n\t    \"v_component_of_wind_350\",\n\t    \"v_component_of_wind_450\",\n\t    \"v_component_of_wind_550\",\n\t    \"v_component_of_wind_650\",\n\t    \"v_component_of_wind_750\",\n\t    \"v_component_of_wind_775\",\n\t    \"v_component_of_wind_800\",\n", "    \"v_component_of_wind_825\",\n\t    \"v_component_of_wind_875\",\n\t    \"v_component_of_wind_900\",\n\t    \"v_component_of_wind_950\",\n\t    \"v_component_of_wind_975\",\n\t]\n\tVAR4TEST = [\n\t    \"2m_temperature\",\n\t    \"geopotential_1\",\n\t]\n", "def identity(x):\n\t    return x\n\tdef mp_loader(dp):\n\t    rs = MultiProcessingReadingService(num_workers=4)\n\t    dl = DataLoader2(dp, reading_service=rs)\n\t    print(\"MP reading serivce\")\n\t    num_frames = 0\n\t    for i, batch in tqdm(enumerate(dl)):\n\t        # batch_size is 1\n\t        num_frames += 1\n", "        item = batch[0]\n\t        print(\"\\n\")\n\t        for key, val in item.items():\n\t            if isinstance(val, dict):\n\t                print(key, val.keys())\n\t            elif isinstance(val, torch.Tensor):\n\t                print(key, val.size())\n\t            elif isinstance(val, np.ndarray):\n\t                print(key, val.shape)\n\t            else:\n", "                print(key, val)\n\t    print(f\"num_frames: {num_frames}\")\n\t    dl.shutdown()\n\tdef dist_loader(rank, world_size, dp, q):\n\t    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\t    rs = DistributedReadingService()\n\t    dl = DataLoader2(dp, reading_service=rs)\n\t    cnt = 0\n\t    for d in tqdm(dl, desc=f\"loading on rank {rank}\"):\n\t        cnt += 1\n", "        # Mimic distributed training step\n\t        dist.barrier()\n\t    q.put(cnt)\n\t    dl.shutdown()\n\tdef mp_dist_training(rank, world_size, dp, q):\n\t    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\t    mp_rs = MultiProcessingReadingService(num_workers=3)\n\t    dist_rs = DistributedReadingService()\n\t    rs = SequentialReadingService(dist_rs, mp_rs)\n\t    dl = DataLoader2(dp, reading_service=rs)\n", "    cnt = 0\n\t    for d in tqdm(dl, desc=f\"loading on rank {rank} with mp reading service\"):\n\t        cnt += 1\n\t        # Mimic distributed training step\n\t        dist.barrier()\n\t    q.put(cnt)\n\t    dl.shutdown()\n\tdef dist_run(loader, dp):\n\t    ctx = mp.get_context(\"fork\")  # Notebook doesn't work well with spawn\n\t    pqs = []\n", "    for rank in range(WORLD_SIZE):\n\t        q = ctx.Queue()\n\t        p = ctx.Process(target=loader, args=(rank, WORLD_SIZE, dp, q))\n\t        pqs.append((p, q))\n\t        p.start()\n\t    for rank in range(WORLD_SIZE):\n\t        cnt = pqs[rank][1].get()\n\t        print(f\"DataLoader2 on rank {rank} received {cnt} data\")\n\t        pqs[rank][0].join()\n\t@click.command()\n", "@click.option(\n\t    \"--container-dir\", default=\"/datadrive/azure_storage/weathereastus/era5seqrecord\"\n\t)\n\t@click.option(\n\t    \"--reading-service\", required=True, type=click.Choice([\"mp\", \"dist\", \"mpdist\"])\n\t)\n\t@click.option(\"--testing\", required=True, type=bool)\n\tdef main(container_dir: str, reading_service: str, testing: bool = True):\n\t    recorddir = f\"{container_dir}/local/1980\"\n\t    record = WSeqRecord.load_record(recorddir=recorddir)\n", "    var_list = VAR4TEST if testing else VAR\n\t    record.set_framereader_args(\n\t        {\n\t            \"input_features\": var_list,\n\t            \"target_features\": var_list,\n\t            \"hours_per_step\": 1,\n\t            \"max_pred_steps\": 10,\n\t        }\n\t    )\n\t    dp = build_wdatapipe(\n", "        [record], None, None, batch_size=1, mappings=[], collate_fn=identity\n\t    )\n\t    print(f\"Testing: {testing}\")\n\t    if reading_service == \"mp\":\n\t        print(\"Testing mp reading with 4 workers:\")\n\t        mp_loader(dp)\n\t    elif reading_service == \"dist\":\n\t        print(f\"Testing dist reading with {WORLD_SIZE} nodes:\")\n\t        dist_run(dist_loader, dp)\n\t    else:\n", "        print(f\"Testing dist mp reading with {WORLD_SIZE} nodes and 3 workers:\")\n\t        dist_run(mp_dist_training, dp)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "seqrecord/weather/format/weathernp2seq_mp.py", "chunked_list": ["\"\"\"transform existing weather dataset to sequence weather format.\n\tnotes:\n\t    - assume the entire dataset is stored in the single storage account.\n\t\"\"\"\n\tfrom seqrecord.weather.seqrecord import WSeqRecord\n\timport os\n\tfrom typing import List, Tuple, Iterator\n\timport re\n\timport numpy as np\n\tfrom tqdm import tqdm\n", "from multiprocessing import Process\n\tfrom itertools import islice\n\t# test correctness of saved seqrecord format\n\t# dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n\tdataset_mount_dir = \"/datadrive/weatherdatastorage2/datasets\"\n\t# \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n\tDEFAULT_CONFIG = {\n\t    \"num_processes\": 4,\n\t    \"wseqrecord_dir\": os.path.join(\n\t        dataset_mount_dir,\n", "        \"CMIP6/MPI-ESM/wseqrecord/dev/5.625deg_equally_np_all_levels/train/\",\n\t    ),\n\t    \"wdataset_dir\": os.path.join(\n\t        dataset_mount_dir, \"CMIP6/MPI-ESM/5.625deg_equally_np_all_levels/train/\"\n\t    ),\n\t}\n\tdef sort_weatherfiles(files: List[os.DirEntry]) -> List[os.DirEntry]:\n\t    \"\"\"Return sorted files in dir, make sure files are sorted incrementally in time.\n\t    # todo: check with Jayesh this sorting is correct\n\t    Example file names: 195001010600-195501010000_33.npz from CMIP6/MPI-ESM/1.40625deg_equally_np_all_levels/train/\n", "    Args:\n\t        files (List[os.DirEntry]): each element in list is a file name\n\t    \"\"\"\n\t    def str2nums(direntry):\n\t        nums = re.split(\"-|_\", direntry.name.split(\".\")[0])\n\t        nums = tuple(map(int, nums))\n\t        return nums\n\t    return sorted(files, key=str2nums)\n\tdef weatherdata2seqrecord(\n\t    rank: int,\n", "    world_size: int,\n\t    config: dict,\n\t    sub_weatherfile_generator: Iterator,\n\t    sub_loads: int,\n\t) -> None:\n\t    sub_wsrecord = WSeqRecord(\n\t        os.path.join(\n\t            config[\"wseqrecord_dir\"], WSeqRecord.subfolder_name(rank, world_size)\n\t        )\n\t    )\n", "    def frame_generator(files: Iterator):\n\t        i = 0\n\t        pbar = (\n\t            tqdm(files, desc=\"Formatting progress on rank 0:\", total=sub_loads)\n\t            if rank == 0\n\t            else files\n\t        )\n\t        for file_path in pbar:\n\t            data = np.load(file_path)\n\t            num_frames = data[data.files[0]].shape[0]\n", "            for rel_frame_idx in range(num_frames):\n\t                frame = {}\n\t                for key in data.files:\n\t                    frame[key] = data[key][rel_frame_idx]\n\t                yield frame\n\t            i += 1\n\t    sub_wsrecord.put_frame(frame_generator(sub_weatherfile_generator), 5)\n\t    sub_wsrecord.dump(rank)\n\tdef distribute_loads(works: int, num_processes: int) -> List[Tuple[int, int]]:\n\t    \"\"\"Given the overall works and number of processes, allocate evenly the loads that each process should take.\n", "    Args:\n\t        works (int): amount of over all work\n\t        num_processes (int): number of processes available\n\t    Returns:\n\t        List[Tuple[int, int]]: indices of work each process is responsible for\n\t    \"\"\"\n\t    assert (\n\t        works >= num_processes\n\t    ), \"The amount of works is less than number of processes.\"\n\t    ans = []\n", "    start = 0\n\t    loads_per_process = round(works / num_processes)\n\t    for i in range(num_processes):\n\t        end = start + loads_per_process if i < num_processes - 1 else works\n\t        ans.append((start, end))\n\t        start = end\n\t    return ans\n\tif __name__ == \"__main__\":\n\t    config = DEFAULT_CONFIG\n\t    # gather existing weather dataset files\n", "    files_dirs = os.scandir(os.path.join(config[\"wdataset_dir\"]))\n\t    files = list(filter(lambda direntry: direntry.is_file(), files_dirs))\n\t    # make sure files are ordered by time incrementally\n\t    # different datasets require different sortting methods\n\t    files = sort_weatherfiles(files)[:4]\n\t    weatherfile_generator = iter(files)\n\t    dividens = distribute_loads(len(files), config[\"num_processes\"])\n\t    processes = []\n\t    for i in range(config[\"num_processes\"]):\n\t        sub_weatherfile_generator = islice(\n", "            weatherfile_generator, dividens[i][0], dividens[i][1]\n\t        )\n\t        p = Process(\n\t            target=weatherdata2seqrecord,\n\t            args=(\n\t                i,\n\t                config[\"num_processes\"],\n\t                config,\n\t                sub_weatherfile_generator,\n\t                dividens[i][1] - dividens[i][0],\n", "            ),\n\t        )\n\t        processes.append(p)\n\t        p.start()\n\t    # all process complete successfully\n\t    for i in range(config[\"num_processes\"]):\n\t        processes[i].join()\n\t    # combine sub-seqrecord\n\t    record = WSeqRecord.gather_subseqrecords(\n\t        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n", "    )\n"]}
{"filename": "seqrecord/weather/datapipe/datapipe.py", "chunked_list": ["import os\n\tfrom torchdata.dataloader2 import (\n\t    DataLoader2,\n\t    MultiProcessingReadingService,\n\t    DistributedReadingService,\n\t    SequentialReadingService,\n\t)\n\timport torchdata\n\tfrom torchdata.datapipes import functional_datapipe\n\tfrom torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n", "import torch.distributed as dist\n\timport torch.multiprocessing as mp\n\timport socket\n\timport torch\n\tfrom torch.utils.data.datapipes.iter.sharding import SHARDING_PRIORITIES\n\tMASTER_ADDR = \"127.0.0.1\"\n\tWORLD_SIZE = 2\n\t@functional_datapipe(\"square_dp\")\n\tclass SqaureDP(IterDataPipe):\n\t    def __init__(self, source_datapipe) -> None:\n", "        self.source_datapipe = source_datapipe\n\t        self.acc = []\n\t    def __iter__(self):\n\t        for num in self.source_datapipe:\n\t            print(f\"getting number {num} and square it to get {num*num}\")\n\t            self.acc.append(num)\n\t            yield num * num\n\t        print(f\"\\n {self.acc} \\n\")\n\tdef _get_open_port():\n\t    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n", "    s.bind((\"\", 0))\n\t    port = s.getsockname()[1]\n\t    s.close()\n\t    return str(port)\n\tos.environ[\"MASTER_ADDR\"] = MASTER_ADDR\n\tos.environ[\"MASTER_PORT\"] = _get_open_port()\n\tdef dist_dl(rank, world_size):\n\t    # using the old apis of torch data to shard\n\t    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\t    dp = IterableWrapper([i for i in range(10)]).sharding_filter().square_dp()\n", "    mp_rs = MultiProcessingReadingService(num_workers=3)\n\t    dist_rs = DistributedReadingService()\n\t    rs = SequentialReadingService(dist_rs, mp_rs)\n\t    dl = DataLoader2(dp, reading_service=rs)\n\t    for d in dl:\n\t        d\n\t    dl.shutdown()\n\tdef mp_dl():\n\t    dp = (\n\t        IterableWrapper([i for i in range(10)])\n", "        .sharding_filter(sharding_group_filter=SHARDING_PRIORITIES.DEFAULT)\n\t        .sharding_filter(sharding_group_filter=SHARDING_PRIORITIES.MULTIPROCESSING)\n\t        .square_dp()\n\t    )\n\t    mp_rs = MultiProcessingReadingService(num_workers=3)\n\t    dist_rs = DistributedReadingService()\n\t    rs = SequentialReadingService(dist_rs, mp_rs)\n\t    dl = DataLoader2(dp, reading_service=rs)\n\t    for i, x in enumerate(dl):\n\t        # print(x)\n", "        x\n\t    dl.shutdown()\n\tif __name__ == \"__main__\":\n\t    ctx = mp.get_context(\"fork\")  # Notebook doesn't work well with spawn\n\tfor rank in range(WORLD_SIZE):\n\t    p = ctx.Process(target=dist_dl, args=(rank, WORLD_SIZE))\n\t    p.start()\n"]}
