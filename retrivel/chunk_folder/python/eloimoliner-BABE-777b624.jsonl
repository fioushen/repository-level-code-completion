{"filename": "train.py", "chunked_list": ["import os\n\timport re\n\timport json\n\timport hydra\n\timport torch\n\t#from utils.torch_utils import distributed as dist\n\timport utils.setup as setup\n\tfrom training.trainer import Trainer\n\timport copy\n\tdef _main(args):\n", "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t    #assert torch.cuda.is_available()\n\t    #device=\"cuda\"\n\t    global __file__\n\t    __file__ = hydra.utils.to_absolute_path(__file__)\n\t    dirname = os.path.dirname(__file__)\n\t    args.model_dir = os.path.join(dirname, str(args.model_dir))\n\t    if not os.path.exists(args.model_dir):\n\t            os.makedirs(args.model_dir)\n\t    args.exp.model_dir=args.model_dir\n", "    #dist.init()\n\t    dset=setup.setup_dataset(args)\n\t    diff_params=setup.setup_diff_parameters(args)\n\t    network=setup.setup_network(args, device)\n\t    optimizer=setup.setup_optimizer(args, network)\n\t    #try:\n\t    test_set=setup.setup_dataset_test(args)\n\t    #except:\n\t    #test_set=None\n\t    network_tester=copy.deepcopy(network)\n", "    tester=setup.setup_tester(args, network=network_tester, diff_params=diff_params, test_set=test_set, device=device) #this will be used for making demos during training\n\t    print(\"setting up trainer\")\n\t    trainer=setup.setup_trainer(args, dset=dset, network=network, optimizer=optimizer, diff_params=diff_params, tester=tester, device=device) #this will be used for making demos during training\n\t    print(\"trainer set up\")\n\t    # Print options.\n\t    print()\n\t    print('Training options:')\n\t    print()\n\t    print(f'Output directory:        {args.model_dir}')\n\t    print(f'Network architecture:    {args.network.callable}')\n", "    print(f'Dataset:    {args.dset.callable}')\n\t    print(f'Diffusion parameterization:  {args.diff_params.callable}')\n\t    print(f'Batch size:              {args.exp.batch}')\n\t    print(f'Mixed-precision:         {args.exp.use_fp16}')\n\t    print()\n\t    # Train.\n\t    #trainer=Trainer(args=args, dset=dset, network=network, optimizer=optimizer, diff_params=diff_params, tester=tester, device=device)\n\t    trainer.training_loop()\n\t@hydra.main(config_path=\"conf\", config_name=\"conf\")\n\tdef main(args):\n", "    _main(args)\n\tif __name__ == \"__main__\":\n\t    main()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "test.py", "chunked_list": ["import os\n\timport re\n\timport json\n\timport hydra\n\t#import click\n\timport torch\n\t#from utils.torch_utils import distributed as dist\n\timport utils.setup as setup\n\timport urllib.request\n\tdef _main(args):\n", "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t    #assert torch.cuda.is_available()\n\t    #device=\"cuda\"\n\t    global __file__\n\t    __file__ = hydra.utils.to_absolute_path(__file__)\n\t    dirname = os.path.dirname(__file__)\n\t    args.model_dir = os.path.join(dirname, str(args.model_dir))\n\t    if not os.path.exists(args.model_dir):\n\t            os.makedirs(args.model_dir)\n\t    args.exp.model_dir=args.model_dir\n", "    torch.multiprocessing.set_start_method('spawn')\n\t    diff_params=setup.setup_diff_parameters(args)\n\t    network=setup.setup_network(args, device)\n\t    test_set=setup.setup_dataset_test(args)\n\t    tester=setup.setup_tester(args, network=network, diff_params=diff_params, test_set=test_set, device=device) #this will be used for making demos during training\n\t    # Print options.\n\t    print()\n\t    print('Training options:')\n\t    print()\n\t    print(f'Output directory:        {args.model_dir}')\n", "    print(f'Network architecture:    {args.network.callable}')\n\t    print(f'Diffusion parameterization:  {args.diff_params.callable}')\n\t    print(f'Tester:                  {args.tester.callable}')\n\t    print(f'Experiment:                  {args.exp.exp_name}')\n\t    print()\n\t    # Train.\n\t    print(\"loading checkpoint path:\", args.tester.checkpoint)\n\t    if args.tester.checkpoint != 'None':\n\t        ckpt_path=os.path.join(dirname, args.tester.checkpoint)\n\t        if not(os.path.exists(ckpt_path)):\n", "            print(\"download ckpt\")\n\t            path, basename= os.path.split(args.tester.checkpoint)\n\t            if not(os.path.exists(os.path.join(dirname, path))):\n\t                    os.makedirs(os.path.join(dirname,path))\n\t            HF_path=\"https://huggingface.co/Eloimoliner/babe/resolve/main/\"+os.path.basename(args.tester.checkpoint)\n\t            urllib.request.urlretrieve(HF_path, filename=ckpt_path)\n\t        #relative path\n\t        ckpt_path=os.path.join(dirname, args.tester.checkpoint)\n\t        tester.load_checkpoint(ckpt_path) \n\t        #jexcept:\n", "        #j    #absolute path\n\t        #j   tester.load_checkpoint(os.path.join(args.model_dir,args.tester.checkpoint)) \n\t    else:\n\t        print(\"trying to load latest checkpoint\")\n\t        tester.load_latest_checkpoint()\n\t    tester.dodajob()\n\t@hydra.main(config_path=\"conf\", config_name=\"conf\")\n\tdef main(args):\n\t    _main(args)\n\tif __name__ == \"__main__\":\n", "    main()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "utils/setup.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport utils.dnnlib as dnnlib\n\tdef worker_init_fn(worker_id):\n\t    st=np.random.get_state()[2]\n\t    np.random.seed( st+ worker_id)\n\tdef setup_dataset(args):\n\t    try:\n\t        overfit=args.dset.overfit\n\t    except:\n", "        overfit=False\n\t    #the dataloader loads audio at the original sampling rate, then in the training loop we resample it to the target sampling rate. The mismatch between sampling rates is indicated by the resample_factor\n\t    #if resample_factor=1, then the audio is not resampled, and everything is normal\n\t    #try:\n\t    if args.dset.type==\"operator\":\n\t        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor)\n\t        dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.exp.batch,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn))\n\t        return dataset_iterator\n\t    else:\n\t        if args.dset.name==\"maestro_allyears\" or args.dset.name==\"maestro_fs\":\n", "            dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, overfit=overfit)\n\t        else:\n\t            dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor, seg_len=args.exp.audio_len*args.exp.resample_factor, overfit=overfit)\n\t        dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.exp.batch,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn, timeout=0, prefetch_factor=20))\n\t        return dataset_iterator\n\tdef setup_dataset_test(args):\n\t    if args.dset.name==\"maestro_allyears\" or args.dset.name==\"maestro_fs\":\n\t        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.test.callable, dset_args=args.dset, num_samples=args.dset.test.num_samples)\n\t    else:\n\t        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.test.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor,seg_len=args.exp.audio_len*args.exp.resample_factor, num_samples=args.dset.test.num_samples)\n", "    dataset = torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.dset.test.batch_size,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn)\n\t    return dataset\n\tdef setup_diff_parameters(args):\n\t    diff_params_obj=dnnlib.call_func_by_name(func_name=args.diff_params.callable, args=args)\n\t    return diff_params_obj\n\tdef setup_network(args, device, operator=False):\n\t        #try:\n\t        network_obj=dnnlib.call_func_by_name(func_name=args.network.callable, args=args, device=device)\n\t        #except Exception as e:\n\t        #        print(e)\n", "        #        network_obj=dnnlib.call_func_by_name(func_name=args.network.callable, args=args.network, device=device)\n\t        return network_obj.to(device)\n\tdef setup_denoiser(args, device):\n\t        #try:\n\t        network_obj=dnnlib.call_func_by_name(func_name=args.tester.denoiser.callable, unet_args=args.tester.denoiser)\n\t        #except Exception as e:\n\t        #        print(e)\n\t        #        network_obj=dnnlib.call_func_by_name(func_name=args.network.callable, args=args.network, device=device)\n\t        return network_obj.to(device)\n\tdef setup_optimizer(args, network):\n", "    # setuo optimizer for training\n\t    optimizer = torch.optim.Adam(network.parameters(), lr=args.exp.lr, betas=(args.exp.optimizer.beta1, args.exp.optimizer.beta2), eps=args.exp.optimizer.eps)\n\t    return optimizer\n\tdef setup_tester(args, network=None, network_operator=None, diff_params=None, test_set=None, device=\"cpu\"):\n\t    assert network is not None\n\t    assert diff_params is not None\n\t    if args.tester.do_test:\n\t        # setuo sampler for making demos during training\n\t        print(\"netwew operater\",network_operator)\n\t        if network_operator!=None:\n", "                sampler = dnnlib.call_func_by_name(func_name=args.tester.callable, args=args, network=network, network_operator=network_operator, test_set=test_set, diff_params=diff_params, device=device)\n\t        else:\n\t                sampler = dnnlib.call_func_by_name(func_name=args.tester.callable, args=args, network=network, test_set=test_set, diff_params=diff_params, device=device)\n\t        return sampler\n\t    else:\n\t        return None\n\t    trainer=setup.setup_trainer #this will be used for making demos during training\n\tdef setup_trainer(args, dset=None, network=None, optimizer=None, diff_params=None, tester=None, device=\"cpu\"):\n\t    assert network is not None\n\t    assert diff_params is not None\n", "    assert optimizer is not None\n\t    assert tester is not None\n\t    print(args.exp.trainer_callable)\n\t    trainer = dnnlib.call_func_by_name(func_name=args.exp.trainer_callable, args=args, dset=dset, network=network, optimizer=optimizer, diff_params=diff_params, tester=tester, device=device)\n\t    return trainer\n"]}
{"filename": "utils/utils_notebook.py", "chunked_list": ["import soundfile as sf\n\timport torch\n\timport numpy as np\n\tdef load_audio(name=\"test_dir/0.wav\",Ls=65536):\n\t    x, fs=sf.read(name)\n\t    x=torch.Tensor(x)\n\t    #x=x[1*fs:2*fs]\n\t    Ls=65536\n\t    x=x[0:Ls]\n\t    return x, fs\n", "def save_wav(x, fs=22050, filename=\"test.wav\"):\n\t    x=x.numpy()\n\t    sf.write(filename, x, fs)\n\tdef plot_stft(x):\n\t    NFFT=1024\n\t    #hamming window\n\t    window = torch.hann_window(NFFT)\n\t    #apply STFT to x\n\t    X = torch.stft(x, NFFT, hop_length=NFFT//2, win_length=NFFT, window=window, center=False, normalized=False, onesided=True)\n\t    freqs=np.fft.rfftfreq(NFFT, 1/fs)\n", "    X_abs=(X[...,0]**2+X[...,1]**2)**0.5\n\t    #plot absolute value of STFT using px\n\t    fig = px.imshow(20*np.log10(X_abs.numpy()+1e-8), labels=dict(x=\"Time\", y=\"Frequency\", color=\"Magnitude\"))\n\t    fig.show()"]}
{"filename": "utils/training_utils.py", "chunked_list": ["import torch\n\timport torchaudio\n\timport numpy as np\n\timport scipy.signal\n\tclass EMAWarmup:\n\t    \"\"\"Implements an EMA warmup using an inverse decay schedule.\n\t    If inv_gamma=1 and power=1, implements a simple average. inv_gamma=1, power=2/3 are\n\t    good values for models you plan to train for a million or more steps (reaches decay\n\t    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n\t    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n", "    215.4k steps).\n\t    Args:\n\t        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n\t        power (float): Exponential factor of EMA warmup. Default: 1.\n\t        min_value (float): The minimum EMA decay rate. Default: 0.\n\t        max_value (float): The maximum EMA decay rate. Default: 1.\n\t        start_at (int): The epoch to start averaging at. Default: 0.\n\t        last_epoch (int): The index of last epoch. Default: 0.\n\t    \"\"\"\n\t    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n", "                 last_epoch=0):\n\t        self.inv_gamma = inv_gamma\n\t        self.power = power\n\t        self.min_value = min_value\n\t        self.max_value = max_value\n\t        self.start_at = start_at\n\t        self.last_epoch = last_epoch\n\t    def state_dict(self):\n\t        \"\"\"Returns the state of the class as a :class:`dict`.\"\"\"\n\t        return dict(self.__dict__.items())\n", "    def load_state_dict(self, state_dict):\n\t        \"\"\"Loads the class's state.\n\t        Args:\n\t            state_dict (dict): scaler state. Should be an object returned\n\t                from a call to :meth:`state_dict`.\n\t        \"\"\"\n\t        self.__dict__.update(state_dict)\n\t    def get_value(self):\n\t        \"\"\"Gets the current EMA decay rate.\"\"\"\n\t        epoch = max(0, self.last_epoch - self.start_at)\n", "        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n\t        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\t    def step(self):\n\t        \"\"\"Updates the step count.\"\"\"\n\t        self.last_epoch += 1\n\t#from https://github.com/csteinmetz1/auraloss/blob/main/auraloss/perceptual.py\n\tclass FIRFilter(torch.nn.Module):\n\t    \"\"\"FIR pre-emphasis filtering module.\n\t    Args:\n\t        filter_type (str): Shape of the desired FIR filter (\"hp\", \"fd\", \"aw\"). Default: \"hp\"\n", "        coef (float): Coefficient value for the filter tap (only applicable for \"hp\" and \"fd\"). Default: 0.85\n\t        ntaps (int): Number of FIR filter taps for constructing A-weighting filters. Default: 101\n\t        plot (bool): Plot the magnitude respond of the filter. Default: False\n\t    Based upon the perceptual loss pre-empahsis filters proposed by\n\t    [Wright & Välimäki, 2019](https://arxiv.org/abs/1911.08922).\n\t    A-weighting filter - \"aw\"\n\t    First-order highpass - \"hp\"\n\t    Folded differentiator - \"fd\"\n\t    Note that the default coefficeint value of 0.85 is optimized for\n\t    a sampling rate of 44.1 kHz, considering adjusting this value at differnt sampling rates.\n", "    \"\"\"\n\t    def __init__(self, filter_type=\"hp\", coef=0.85, fs=44100, ntaps=101, plot=False): \n\t        \"\"\"Initilize FIR pre-emphasis filtering module.\"\"\"\n\t        super(FIRFilter, self).__init__()\n\t        self.filter_type = filter_type\n\t        self.coef = coef\n\t        self.fs = fs\n\t        self.ntaps = ntaps\n\t        self.plot = plot\n\t        if ntaps % 2 == 0:\n", "            raise ValueError(f\"ntaps must be odd (ntaps={ntaps}).\")\n\t        if filter_type == \"hp\":\n\t            self.fir = torch.nn.Conv1d(1, 1, kernel_size=3, bias=False, padding=1)\n\t            self.fir.weight.requires_grad = False\n\t            self.fir.weight.data = torch.tensor([1, -coef, 0]).view(1, 1, -1)\n\t        elif filter_type == \"fd\":\n\t            self.fir = torch.nn.Conv1d(1, 1, kernel_size=3, bias=False, padding=1)\n\t            self.fir.weight.requires_grad = False\n\t            self.fir.weight.data = torch.tensor([1, 0, -coef]).view(1, 1, -1)\n\t        elif filter_type == \"aw\":\n", "            # Definition of analog A-weighting filter according to IEC/CD 1672.\n\t            f1 = 20.598997\n\t            f2 = 107.65265\n\t            f3 = 737.86223\n\t            f4 = 12194.217\n\t            A1000 = 1.9997\n\t            NUMs = [(2 * np.pi * f4) ** 2 * (10 ** (A1000 / 20)), 0, 0, 0, 0]\n\t            DENs = np.polymul(\n\t                [1, 4 * np.pi * f4, (2 * np.pi * f4) ** 2],\n\t                [1, 4 * np.pi * f1, (2 * np.pi * f1) ** 2],\n", "            )\n\t            DENs = np.polymul(\n\t                np.polymul(DENs, [1, 2 * np.pi * f3]), [1, 2 * np.pi * f2]\n\t            )\n\t            # convert analog filter to digital filter\n\t            b, a = scipy.signal.bilinear(NUMs, DENs, fs=fs)\n\t            # compute the digital filter frequency response\n\t            w_iir, h_iir = scipy.signal.freqz(b, a, worN=512, fs=fs)\n\t            # then we fit to 101 tap FIR filter with least squares\n\t            taps = scipy.signal.firls(ntaps, w_iir, abs(h_iir), fs=fs)\n", "            # now implement this digital FIR filter as a Conv1d layer\n\t            self.fir = torch.nn.Conv1d(\n\t                1, 1, kernel_size=ntaps, bias=False, padding=ntaps // 2\n\t            )\n\t            self.fir.weight.requires_grad = False\n\t            self.fir.weight.data = torch.tensor(taps.astype(\"float32\")).view(1, 1, -1)\n\t    def forward(self, error):\n\t        \"\"\"Calculate forward propagation.\n\t        Args:\n\t            input (Tensor): Predicted signal (B, #channels, #samples).\n", "            target (Tensor): Groundtruth signal (B, #channels, #samples).\n\t        Returns:\n\t            Tensor: Filtered signal.\n\t        \"\"\"\n\t        self.fir.weight.data=self.fir.weight.data.to(error.device)\n\t        error=error.unsqueeze(1)\n\t        error = torch.nn.functional.conv1d(\n\t            error, self.fir.weight.data, padding=self.ntaps // 2\n\t        )\n\t        error=error.squeeze(1)\n", "        return error\n\tdef resample_batch(audio, fs, fs_target, length_target=None):\n\t        device=audio.device\n\t        dtype=audio.dtype\n\t        B=audio.shape[0]\n\t        #if possible resampe in a batched way\n\t        #check if all the fs are the same and equal to 44100\n\t        #print(fs_target)\n\t        if fs_target==22050:\n\t            if (fs==44100).all():\n", "                 audio=torchaudio.functional.resample(audio, 2,1)\n\t                 return audio[:, 0:length_target] #trow away the last samples\n\t            elif (fs==48000).all():\n\t                 #approcimate resamppleint\n\t                 audio=torchaudio.functional.resample(audio, 160*2,147)\n\t                 return audio[:, 0:length_target]\n\t            else:\n\t                #if revious is unsuccesful bccause we have examples at 441000 and 48000 in the same batch,, just iterate over the batch\n\t                proc_batch=torch.zeros((B,length_target), device=device)\n\t                for i, (a, f_s) in enumerate(zip(audio, fs)): #I hope this shit wll not slow down everythingh\n", "                    if f_s==44100:\n\t                        #resample by 2\n\t                        a=torchaudio.functional.resample(a, 2,1)\n\t                    elif f_s==48000:\n\t                        a=torchaudio.functional.resample(a, 160*2,147)\n\t                    else:\n\t                        print(\"WARNING, strange fs\", f_s)\n\t                    proc_batch[i]=a[0:length_target]\n\t                return proc_batch\n\t        elif fs_target==44100:\n", "            if (fs==44100).all():\n\t                 return audio[:, 0:length_target] #trow away the last samples\n\t            elif (fs==48000).all():\n\t                 #approcimate resamppleint\n\t                 audio=torchaudio.functional.resample(audio, 160,147)\n\t                 return audio[:, 0:length_target]\n\t            else:\n\t                #if revious is unsuccesful bccause we have examples at 441000 and 48000 in the same batch,, just iterate over the batch\n\t                B,C,L=audio.shape\n\t                proc_batch=torch.zeros((B,C,L), device=device)\n", "                #print(\"debigging resample batch\")\n\t                #print(audio.shape,fs.shape)\n\t                for i, (a, f_s) in enumerate(zip(audio, fs.tolist())): #I hope this shit wll not slow down everythingh\n\t                    #print(i,a.shape,f_s)\n\t                    if f_s==44100:\n\t                        #resample by 2\n\t                        pass\n\t                    elif f_s==22050:\n\t                        a=torchaudio.functional.resample(a, 1,2)\n\t                    elif f_s==48000:\n", "                        a=torchaudio.functional.resample(a, 160,147)\n\t                    else:\n\t                        print(\"WARNING, strange fs\", f_s)\n\t                    proc_batch[i]=a[...,0:length_target] \n\t                return proc_batch\n\t        else:\n\t            if (fs==44100).all():\n\t                 audio=torchaudio.functional.resample(audio, 44100, fs_target)\n\t                 return audio[...,0:length_target] #trow away the last samples\n\t            elif (fs==48000).all():\n", "                 print(\"resampling 48000 to 16000\", length_target, audio.shape)\n\t                 #approcimate resamppleint\n\t                 audio=torchaudio.functional.resample(audio, 48000,fs_target)\n\t                 print(audio.shape)\n\t                 return audio[..., 0:length_target]\n\t            else:\n\t                #if revious is unsuccesful bccause we have examples at 441000 and 48000 in the same batch,, just iterate over the batch\n\t                proc_batch=torch.zeros((B,length_target), device=device)\n\t                for i, (a, f_s) in enumerate(zip(audio, fs)): #I hope this shit wll not slow down everythingh\n\t                    if f_s==44100:\n", "                        #resample by 2\n\t                        a=torchaudio.functional.resample(a, 44100,fs_target)\n\t                    elif f_s==48000:\n\t                        a=torchaudio.functional.resample(a, 48000,fs_target)\n\t                    else:\n\t                        print(\"WARNING, strange fs\", f_s)\n\t                    proc_batch[i]=a[...,0:length_target] \n\t                return proc_batch\n\tdef load_state_dict( state_dict, network=None, ema=None, optimizer=None, log=True):\n\t        '''\n", "        utility for loading state dicts for different models. This function sequentially tries different strategies\n\t        args:\n\t            state_dict: the state dict to load\n\t        returns:\n\t            True if the state dict was loaded, False otherwise\n\t        Assuming the operations are don in_place, this function will not create a copy of the network and optimizer (I hope)\n\t        '''\n\t        #print(state_dict)\n\t        if log: print(\"Loading state dict\")\n\t        if log:\n", "            print(state_dict.keys())\n\t        #if there\n\t        try:\n\t            if log: print(\"Attempt 1: trying with strict=True\")\n\t            if network is not None:\n\t                network.load_state_dict(state_dict['network'])\n\t            if optimizer is not None:\n\t                optimizer.load_state_dict(state_dict['optimizer'])\n\t            if ema is not None:\n\t                ema.load_state_dict(state_dict['ema'])\n", "            return True\n\t        except Exception as e:\n\t            if log:\n\t                print(\"Could not load state dict\")\n\t                print(e)\n\t        try:\n\t            if log: print(\"Attempt 2: trying with strict=False\")\n\t            if network is not None:\n\t                network.load_state_dict(state_dict['network'], strict=False)\n\t            #we cannot load the optimizer in this setting\n", "            #self.optimizer.load_state_dict(state_dict['optimizer'], strict=False)\n\t            if ema is not None:\n\t                ema.load_state_dict(state_dict['ema'], strict=False)\n\t            return True\n\t        except Exception as e:\n\t            if log:\n\t                print(\"Could not load state dict\")\n\t                print(e)\n\t                print(\"training from scratch\")\n\t        try:\n", "            if log: print(\"Attempt 3: trying with strict=False,but making sure that the shapes are fine\")\n\t            if ema is not None:\n\t                ema_state_dict = ema.state_dict()\n\t            if network is not None:\n\t                network_state_dict = network.state_dict()\n\t            i=0 \n\t            if network is not None:\n\t                for name, param in state_dict['network'].items():\n\t                    if log: print(\"checking\",name) \n\t                    if name in network_state_dict.keys():\n", "                        if network_state_dict[name].shape==param.shape:\n\t                                network_state_dict[name]=param\n\t                                if log:\n\t                                    print(\"assigning\",name)\n\t                                i+=1\n\t            network.load_state_dict(network_state_dict)\n\t            if ema is not None:\n\t                for name, param in state_dict['ema'].items():\n\t                        if log: print(\"checking\",name) \n\t                        if name in ema_state_dict.keys():\n", "                            if ema_state_dict[name].shape==param.shape:\n\t                                ema_state_dict[name]=param\n\t                                if log:\n\t                                    print(\"assigning\",name)\n\t                                i+=1\n\t            ema.load_state_dict(ema_state_dict)\n\t            if i==0:\n\t                if log: print(\"WARNING, no parameters were loaded\")\n\t                raise Exception(\"No parameters were loaded\")\n\t            elif i>0:\n", "                if log: print(\"loaded\", i, \"parameters\")\n\t                return True\n\t        except Exception as e:\n\t            print(e)\n\t            print(\"the second strict=False failed\")\n\t        try:\n\t            if log: print(\"Attempt 4: Assuming the naming is different, with the network and ema called 'state_dict'\")\n\t            if network is not None:\n\t                network.load_state_dict(state_dict['state_dict'])\n\t            if ema is not None:\n", "                ema.load_state_dict(state_dict['state_dict'])\n\t        except Exception as e:\n\t            if log:\n\t                print(\"Could not load state dict\")\n\t                print(e)\n\t                print(\"training from scratch\")\n\t                print(\"It failed 3 times!! but not giving up\")\n\t            #print the names of the parameters in self.network\n\t        try:\n\t            if log: print(\"Attempt 5: trying to load with different names, now model='model' and ema='ema_weights'\")\n", "            if ema is not None:\n\t                dic_ema = {}\n\t                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n\t                    dic_ema[key] = tensor\n\t                    ema.load_state_dict(dic_ema)\n\t                return True\n\t        except Exception as e:\n\t            if log:\n\t                print(e)\n\t        try:\n", "            if log: print(\"Attempt 6: If there is something wrong with the name of the ema parameters, we can try to load them using the names of the parameters in the model\")\n\t            if ema is not None:\n\t                dic_ema = {}\n\t                i=0\n\t                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n\t                    if tensor.requires_grad:\n\t                        dic_ema[key]=state_dict['ema_weights'][i]\n\t                        i=i+1\n\t                    else:\n\t                        dic_ema[key]=tensor     \n", "                ema.load_state_dict(dic_ema)\n\t                return True\n\t        except Exception as e:\n\t            if log:\n\t                print(e)\n\t        try:\n\t            #assign the parameters in state_dict to self.network using a for loop\n\t            print(\"Attempt 7: Trying to load the parameters one by one. This is for the dance diffusion model, looking for parameters starting with 'diffusion.' or 'diffusion_ema.'\")\n\t            if ema is not None:\n\t                ema_state_dict = ema.state_dict()\n", "            if network is not None:\n\t                network_state_dict = ema.state_dict()\n\t            i=0 \n\t            if network is not None:\n\t                for name, param in state_dict['state_dict'].items():\n\t                    print(\"checking\",name) \n\t                    if name.startswith(\"diffusion.\"):\n\t                        i+=1\n\t                        name=name.replace(\"diffusion.\",\"\")\n\t                        if network_state_dict[name].shape==param.shape:\n", "                            #print(param.shape, network.state_dict()[name].shape)\n\t                            network_state_dict[name]=param\n\t                            #print(\"assigning\",name)\n\t                network.load_state_dict(network_state_dict, strict=False)\n\t            if ema is not None:\n\t                for name, param in state_dict['state_dict'].items():\n\t                    if name.startswith(\"diffusion_ema.\"): \n\t                        i+=1\n\t                        name=name.replace(\"diffusion_ema.\",\"\")\n\t                        if ema_state_dict[name].shape==param.shape:\n", "                            if log:\n\t                                    print(param.shape, ema.state_dict()[name].shape)\n\t                            ema_state_dict[name]=param\n\t                ema.load_state_dict(ema_state_dict, strict=False)\n\t            if i==0:\n\t                print(\"WARNING, no parameters were loaded\")\n\t                raise Exception(\"No parameters were loaded\")\n\t            elif i>0:\n\t                print(\"loaded\", i, \"parameters\")\n\t                return True\n", "        except Exception as e:\n\t            if log:\n\t                print(e)\n\t        #try:\n\t        # this is for the dmae1d mddel, assuming there is only one network\n\t        if network is not None:\n\t            network.load_state_dict(state_dict, strict=True)\n\t        if ema is not None:\n\t            ema.load_state_dict(state_dict, strict=True)\n\t        return True\n", "        #except Exception as e:\n\t        #    if log:\n\t        #        print(e)\n\t        return False\n\tdef unnormalize(x,stds, args):\n\t        #unnormalize the STN separated audio\n\t        new_std=args.exp.normalization.target_std\n\t        if new_std==\"sigma_data\":\n\t            new_std=args.diff_params.sigma_data\n\t        x=stds*x/(new_std+1e-8)\n", "        return x\n\tdef normalize( xS, xT, xN, args, return_std=False):\n\t        #normalize the STN separated audio\n\t        if args.exp.normalization.mode==\"None\":\n\t            pass\n\t        elif args.exp.normalization.mode==\"residual_noise\":\n\t            #normalize the residual noise\n\t            std=xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True)\n\t            new_std=args.exp.normalization.target_std\n\t            if new_std==\"sigma_data\":\n", "                new_std=args.diff_params.sigma_data\n\t            #print(std, new_std)\n\t            xN=new_std*xN/(std+1e-8)\n\t            #print(xN.std(dim=-1, keepdim=True))\n\t            xS=new_std*xS/(std+1e-8)\n\t            xT=new_std*xT/(std+1e-8)\n\t        elif args.exp.normalization.mode==\"residual_noise_batch\":\n\t            #normalize the residual noise per batch\n\t            #get the std of the entire batch\n\t            std=xN.std(dim=(0,1,2),unbiased=True, keepdim=False)\n", "            new_std=args.exp.normalization.target_std\n\t            if new_std==\"sigma_data\":\n\t                new_std=args.diff_params.sigma_data\n\t            #print(std, new_std)\n\t            xN=new_std*xN/(std+1e-8)\n\t            #print(xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True))\n\t            xS=new_std*xS/(std+1e-8)\n\t            xT=new_std*xT/(std+1e-8)\n\t        elif args.exp.normalization.mode==\"all\":\n\t            std=(xN+xS+xT).std(dim=-1, keepdim=True).mean(dim=1, keepdim=True)\n", "            new_std=args.exp.normalization.target_std\n\t            if new_std==\"sigma_data\":\n\t                new_std=args.diff_params.sigma_data\n\t            xN=new_std*xN/(std+1e-8)\n\t            xS=new_std*xS/(std+1e-8)\n\t            xT=new_std*xT/(std+1e-8)\n\t            #print(\"std\",xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True))\n\t        else:\n\t            print(\"normalization mode not recognized\")\n\t            pass\n", "        try:\n\t            if return_std:\n\t                return xS, xT, xN, std\n\t        except Exception as e:\n\t            print(e)\n\t            print(\"warning!, std cannot be returned\")\n\t            pass\n\t        return xS, xT, xN\n"]}
{"filename": "utils/blind_bwe_utils.py", "chunked_list": ["import torch\n\timport plotly.express as px\n\timport pandas as pd\n\tdef apply_filter(x, H, NFFT):\n\t    '''\n\t    '''\n\t    X=apply_stft(x, NFFT)\n\t    xrec=apply_filter_istft(X, H, NFFT)\n\t    xrec=xrec[:,:x.shape[-1]]\n\t    return xrec\n", "def apply_stft(x, NFFT):\n\t    '''\n\t    '''\n\t    #hamming window\n\t    window = torch.hamming_window(window_length=NFFT)\n\t    window=window.to(x.device)\n\t    x=torch.cat((x, torch.zeros(*x.shape[:-1],NFFT).to(x.device)),1) #is padding necessary?\n\t    X = torch.stft(x, NFFT, hop_length=NFFT//2,  window=window,  center=False, onesided=True, return_complex=True)\n\t    X=torch.view_as_real(X)\n\t    return X\n", "def apply_filter_istft(X, H, NFFT):\n\t    '''\n\t    '''\n\t    #hamming window\n\t    window = torch.hamming_window(window_length=NFFT)\n\t    window=window.to(X.device)\n\t    X=X*H.unsqueeze(-1).unsqueeze(-1).expand(X.shape)\n\t    X=torch.view_as_complex(X)\n\t    x=torch.istft(X, NFFT, hop_length=NFFT//2,  window=window, center=False, return_complex=False)\n\t    return x\n", "def design_filter_G(fc, A, G, f):\n\t    \"\"\"\n\t    fc: cutoff frequency \n\t        if fc is a scalar, the filter has one slopw\n\t        if fc is a list of scalars, the filter has multiple slopes\n\t    A: attenuation in dB\n\t        if A is a scalar, the filter has one slopw\n\t        if A is a list of scalars, the filter has multiple slopes\n\t    \"\"\"\n\t    multiple_slopes=False\n", "    #check if fc and A are lists\n\t    if isinstance(fc, list) and isinstance(A, list):\n\t        multiple_slopes=True\n\t    #check if fc is a tensor and A is a tensor\n\t    try:\n\t        if fc.shape[0]>1:\n\t            multiple_slopes=True\n\t    except:\n\t        pass\n\t    if multiple_slopes:\n", "        H=torch.zeros(f.shape).to(f.device)\n\t        H[f<fc[0]]=1\n\t        H[f>=fc[0]]=10**(A[0]*torch.log2(f[f>=fc[0]]/fc[0])/20)\n\t        for i in range(1,len(fc)):\n\t            #find the index of the first frequency that is greater than fc[i]\n\t            #fix=torch.where(f>=fc[i])[0][0]\n\t            #print(fc[i],fix)\n\t            #H[f>=fc[i]]=H[fix]-10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)\n\t            H[f>=fc[i]]=10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)*H[f>=fc[i]][0]\n\t        #apply the gain (G is in dB)\n", "        H=H*10**(G/20)\n\t        #return H\n\t    else:\n\t        #if fc and A are scalars\n\t        H=torch.zeros(f.shape).to(f.device)\n\t        H[f<fc]=1\n\t        H[f>=fc]=10**(A*torch.log2(f[f>=fc]/fc)/20)\n\t        H=H*10**(G/20)\n\t    return H\n\tdef design_filter(fc, A, f):\n", "    \"\"\"\n\t    fc: cutoff frequency \n\t        if fc is a scalar, the filter has one slopw\n\t        if fc is a list of scalars, the filter has multiple slopes\n\t    A: attenuation in dB\n\t        if A is a scalar, the filter has one slopw\n\t        if A is a list of scalars, the filter has multiple slopes\n\t    \"\"\"\n\t    multiple_slopes=False\n\t    #check if fc and A are lists\n", "    if isinstance(fc, list) and isinstance(A, list):\n\t        multiple_slopes=True\n\t    #check if fc is a tensor and A is a tensor\n\t    try:\n\t        if fc.shape[0]>1:\n\t            multiple_slopes=True\n\t    except:\n\t        pass\n\t    if multiple_slopes:\n\t        H=torch.zeros(f.shape).to(f.device)\n", "        H[f<fc[0]]=1\n\t        H[f>=fc[0]]=10**(A[0]*torch.log2(f[f>=fc[0]]/fc[0])/20)\n\t        for i in range(1,len(fc)):\n\t            #find the index of the first frequency that is greater than fc[i]\n\t            #fix=torch.where(f>=fc[i])[0][0]\n\t            #print(fc[i],fix)\n\t            #H[f>=fc[i]]=H[fix]-10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)\n\t            H[f>=fc[i]]=10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)*H[f>=fc[i]][0]\n\t        #return H\n\t    else:\n", "        #if fc and A are scalars\n\t        H=torch.zeros(f.shape).to(f.device)\n\t        H[f<fc]=1\n\t        H[f>=fc]=10**(A*torch.log2(f[f>=fc]/fc)/20)\n\t    return H\n\t#def design_filter(fc, A, f):\n\t#    H=torch.zeros(f.shape).to(f.device)\n\t#    H[f<fc]=1\n\t#    H[f>=fc]=10**(A*torch.log2(f[f>=fc]/fc)/20)\n\t#    return H\n", "def apply_filter_and_norm_STFTmag(X,Xref, H):\n\t    #X: (N,513, T) \"clean\" example\n\t    #Xref: (N,513, T)  observations\n\t    #H: (513,) filter\n\t    #get the absolute value of the STFT\n\t    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n\t    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\t    X=X*H.unsqueeze(-1).expand(X.shape)\n\t    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n\t    return norm\n", "def apply_norm_filter(H,H2):\n\t    norm=torch.linalg.norm(H.reshape(-1)-H2.reshape(-1),ord=2)\n\t    return norm\n\tdef apply_norm_STFT_fweighted(y,den_rec, freq_weight=\"linear\", NFFT=1024):\n\t    #X: (N,513, T) \"clean\" example\n\t    #Xref: (N,513, T)  observations\n\t    #H: (513,) filter\n\t    X=apply_stft(den_rec, NFFT)\n\t    Xref=apply_stft(y, NFFT)\n\t    #get the absolute value of the STFT\n", "    #X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n\t    #Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\t    freqs=torch.linspace(0, 1, X.shape[1]).to(X.device).unsqueeze(-1)\n\t    #print(X.shape, Xref.shape, freqs.shape)\n\t    #apply frequency weighting to the cost function\n\t    if freq_weight==\"linear\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)\n\t    elif freq_weight==\"None\":\n\t        pass\n", "    elif freq_weight==\"log\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"sqrt\":\n\t        X=X*torch.sqrt(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.sqrt(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"log2\":\n\t        X=X*torch.log2(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log2(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"log10\":\n", "        X=X*torch.log10(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log10(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"cubic\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**3\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**3\n\t    elif freq_weight==\"quadratic\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**2\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**2\n\t    elif freq_weight==\"logcubic\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**3)\n", "        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**3)\n\t    elif freq_weight==\"logquadratic\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**2)\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**2)\n\t    elif freq_weight==\"squared\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**4\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**4\n\t    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n\t    return norm\n\tdef apply_norm_STFTmag_fweighted(y,den_rec, freq_weight=\"linear\", NFFT=1024, logmag=False):\n", "    #X: (N,513, T) \"clean\" example\n\t    #Xref: (N,513, T)  observations\n\t    #H: (513,) filter\n\t    X=apply_stft(den_rec, NFFT)\n\t    Xref=apply_stft(y, NFFT)\n\t    #get the absolute value of the STFT\n\t    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n\t    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\t    freqs=torch.linspace(0, 1, X.shape[1]).to(X.device)\n\t    #apply frequency weighting to the cost function\n", "    if freq_weight==\"linear\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)\n\t    elif freq_weight==\"None\":\n\t        pass\n\t    elif freq_weight==\"log\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"sqrt\":\n\t        X=X*torch.sqrt(freqs.unsqueeze(-1).expand(X.shape))\n", "        Xref=Xref*torch.sqrt(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"log2\":\n\t        X=X*torch.log2(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log2(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"log10\":\n\t        X=X*torch.log10(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log10(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"cubic\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**3\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**3\n", "    elif freq_weight==\"quadratic\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**2\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**2\n\t    elif freq_weight==\"logcubic\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**3)\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**3)\n\t    elif freq_weight==\"logquadratic\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**2)\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**2)\n\t    elif freq_weight==\"squared\":\n", "        X=X*freqs.unsqueeze(-1).expand(X.shape)**4\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**4\n\t    if logmag==True:\n\t        norm=torch.linalg.norm(torch.log10(X.reshape(-1)+1e-8)-torch.log10(Xref.reshape(-1)+1e-8),ord=2)\n\t    else:\n\t        norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n\t    return norm\n\tdef apply_filter_and_norm_STFTmag_fweighted(X,Xref, H, freq_weight=\"linear\"):\n\t    #X: (N,513, T) \"clean\" example\n\t    #Xref: (N,513, T)  observations\n", "    #H: (513,) filter\n\t    #get the absolute value of the STFT\n\t    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n\t    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\t    X=X*H.unsqueeze(-1).expand(X.shape)\n\t    freqs=torch.linspace(0, 1, X.shape[1]).to(X.device)\n\t    #apply frequency weighting to the cost function\n\t    if freq_weight==\"linear\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)\n", "    elif freq_weight==\"None\":\n\t        pass\n\t    elif freq_weight==\"log\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"sqrt\":\n\t        X=X*torch.sqrt(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.sqrt(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"log2\":\n\t        X=X*torch.log2(freqs.unsqueeze(-1).expand(X.shape))\n", "        Xref=Xref*torch.log2(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"log10\":\n\t        X=X*torch.log10(freqs.unsqueeze(-1).expand(X.shape))\n\t        Xref=Xref*torch.log10(freqs.unsqueeze(-1).expand(Xref.shape))\n\t    elif freq_weight==\"cubic\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**3\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**3\n\t    elif freq_weight==\"quadratic\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**2\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**2\n", "    elif freq_weight==\"logcubic\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**3)\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**3)\n\t    elif freq_weight==\"logquadratic\":\n\t        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**2)\n\t        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**2)\n\t    elif freq_weight==\"squared\":\n\t        X=X*freqs.unsqueeze(-1).expand(X.shape)**4\n\t        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**4\n\t    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n", "    return norm\n\tdef plot_filter(ref_filter, est_filter, NFFT=1024, fs=44100):\n\t    f=torch.fft.rfftfreq(NFFT, d=1/fs).to(ref_filter.device)\n\t    Href=design_filter(ref_filter[0],ref_filter[1], f)\n\t    H=design_filter(est_filter[0],est_filter[1], f)\n\t    fig=px.line(x=f.cpu(),y=20*torch.log10(H.cpu().detach()), log_x=True , title='Frequency response of a low pass filter', labels={'x':'Frequency (Hz)', 'y':'Magnitude (dB)'})\n\t    #plot the reference frequency response\n\t    fig.add_scatter(x=f.cpu(),y=20*torch.log10(Href.cpu().detach()), mode='lines', name='Reference')\n\t    return fig\n\tdef animation_filter(path, data_filters ,t,NFFT=1024, fs=44100, name=\"animation_filter\",NT=15 ):\n", "    '''\n\t    plot an animation of the reverse diffusion process of filters\n\t    args:\n\t        path: path to save the animation\n\t        x: input audio (N,T)\n\t        t: timesteps (sigma)\n\t        name: name of the animation\n\t    '''\n\t    #print(noisy.shape)\n\t    f=torch.fft.rfftfreq(NFFT, d=1/fs)\n", "    Nsteps=data_filters.shape[0]\n\t    numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n\t    tt=torch.linspace(0, Nsteps-1, numsteps)\n\t    i_s=[]\n\t    allX=None\n\t    for i in tt:\n\t        i=int(torch.floor(i))\n\t        i_s.append(i)\n\t        X=design_filter(data_filters[i,0],data_filters[i,1], f) # (513,)\n\t        X=X.unsqueeze(0) #(1,513)\n", "        if allX==None:\n\t             allX=X\n\t        else:\n\t             allX=torch.cat((allX,X), 0)\n\t    #allX shape is ( 513, numsteps)\n\t    sigma=t[i_s]\n\t    #x=x.squeeze(1)# (100,19)\n\t    print(allX.shape, f.shape, sigma.shape)\n\t    f=f.unsqueeze(0).expand(allX.shape[0], -1).reshape(-1)\n\t    sigma=sigma.unsqueeze(-1).expand(-1, allX.shape[1]).reshape(-1)\n", "    allX=allX.reshape(-1)\n\t    print(allX.shape, f.shape, sigma.shape)\n\t    df=pd.DataFrame(\n\t        {\n\t            \"f\": f.cpu().numpy(),\n\t            \"h\": 20*torch.log10(allX.cpu()).numpy(),\n\t            \"sigma\": sigma.cpu().numpy()\n\t        }\n\t    )\n\t    fig=px.line(df, x=\"f\",y=\"h\", animation_frame=\"sigma\", log_x=True) #I need\n", "    path_to_plotly_html = path+\"/\"+name+\".html\"\n\t    fig.write_html(path_to_plotly_html, auto_play = False)\n\t    return fig\n"]}
{"filename": "utils/bandwidth_extension.py", "chunked_list": ["import scipy.signal\n\timport torch\n\timport torchaudio\n\timport math\n\tdef prepare_filter(args, sample_rate):\n\t    #design the filter\n\t    order=args.tester.bandwidth_extension.filter.order\n\t    fc=args.tester.bandwidth_extension.filter.fc\n\t    if args.tester.bandwidth_extension.filter.type==\"firwin\":\n\t        beta=args.tester.bandwidth_extension.filter.beta\n", "        filter=get_FIR_lowpass(order,fc, beta,sample_rate)\n\t    elif args.tester.bandwidth_extension.filter.type==\"firwin_hpf\":\n\t        beta=args.tester.bandwidth_extension.filter.beta\n\t        filter=get_FIR_high_pass(order,fc, beta,sample_rate)\n\t    elif args.tester.bandwidth_extension.filter.type==\"cheby1\":\n\t        ripple =args.tester.bandwidth_extension.filter.ripple\n\t        b,a=get_cheby1_ba(order, ripple, 2*fc/sample_rate) \n\t        filter=(b,a)\n\t    elif args.tester.bandwidth_extension.filter.type==\"biquad\":\n\t        Q =args.tester.bandwidth_extension.filter.biquad.Q\n", "        parameters=design_biquad_lpf(fc, sample_rate, Q) \n\t        filter=parameters\n\t    elif args.tester.bandwidth_extension.filter.type==\"resample\":\n\t        filter= sample_rate/args.tester.bandwidth_extension.filter.resample.fs\n\t    elif args.tester.bandwidth_extension.filter.type==\"decimate\":\n\t        filter= int(args.tester.bandwidth_extension.decimate.factor)\n\t        args.tester.bandwidth_extension.filter.resample.fs=int(sample_rate/filter)\n\t    elif args.tester.bandwidth_extension.filter.type==\"cheby1filtfilt\":\n\t        raise NotImplementedError\n\t    elif args.tester.bandwidth_extension.filter.type==\"butter_fir\":\n", "        raise NotImplementedError\n\t    elif args.tester.bandwidth_extension.filter.type==\"cheby1_fir\":\n\t        raise NotImplementedError\n\t    else:\n\t        raise NotImplementedError\n\t    return filter\n\tdef get_FIR_high_pass(order,fc, beta, sr):\n\t    \"\"\"\n\t        This function designs a  FIR high pass filter using the window method. It uses scipy.signal\n\t        Args:\n", "            order(int): order of the filter\n\t            fc (float): cutoff frequency\n\t            sr (float): sampling rate\n\t        Returns:\n\t            B (Tensor): shape(1,1,order-1) FIR filter coefficients\n\t    \"\"\"\n\t    B=scipy.signal.firwin(numtaps=order-1,cutoff=fc, width=beta,window=\"kaiser\", fs=sr, pass_zero=\"highpass\")\n\t    B=torch.FloatTensor(B)\n\t    B=B.unsqueeze(0)\n\t    B=B.unsqueeze(0)\n", "    return B\n\tdef get_FIR_lowpass(order,fc, beta, sr):\n\t    \"\"\"\n\t        This function designs a FIR low pass filter using the window method. It uses scipy.signal\n\t        Args:\n\t            order(int): order of the filter\n\t            fc (float): cutoff frequency\n\t            sr (float): sampling rate\n\t        Returns:\n\t            B (Tensor): shape(1,1,order) FIR filter coefficients\n", "    \"\"\"\n\t    B=scipy.signal.firwin(numtaps=order,cutoff=fc, width=beta,window=\"kaiser\", fs=sr)\n\t    B=torch.FloatTensor(B)\n\t    B=B.unsqueeze(0)\n\t    B=B.unsqueeze(0)\n\t    return B\n\tdef apply_low_pass_firwin(y,filter):\n\t    \"\"\"\n\t        Utility for applying a FIR filter, usinf pytorch conv1d\n\t        Args;\n", "            y (Tensor): shape (B,T) signal to filter\n\t            filter (Tensor): shape (1,1,order) FIR filter coefficients\n\t        Returns:\n\t            y_lpf (Tensor): shape (B,T) filtered signal\n\t    \"\"\"\n\t    #ii=2\n\t    B=filter.to(y.device)\n\t    #B=filter\n\t    y=y.unsqueeze(1)\n\t    #weight=torch.nn.Parameter(B)\n", "    y_lpf=torch.nn.functional.conv1d(y,B,padding=\"same\")\n\t    y_lpf=y_lpf.squeeze(1) #some redundancy here, but its ok\n\t    #y_lpf=y\n\t    return y_lpf\n\tdef apply_decimate(y,factor):\n\t    \"\"\"\n\t        Function for applying a naive decimation for downsampling\n\t        Args:\n\t            y (Tensor): shape (B,T)\n\t            factor (int): decimation factor\n", "        Returns\n\t            y (Tensor): shape (B,T//factor)\n\t    \"\"\"\n\t    factor=factor\n\t    return y[...,0:-1:factor]\n\tdef apply_resample(y,factor):\n\t    \"\"\"\n\t        Applies torch's resmpling function\n\t        Args:\n\t            y (Tensor): shape (B,T)\n", "            factor (float): resampling factor\n\t    \"\"\"\n\t    N=100 \n\t    return torchaudio.functional.resample(y,orig_freq=int(factor*N),new_freq=N )\n\tdef apply_low_pass_biquad(y,filter):\n\t    \"\"\"\n\t        Applies torchaudio's biquad filter\n\t        Args:\n\t            y (Tensor): shape (B,T)\n\t            filter (tuple): biquad filter coefficients\n", "        Returns:        \n\t            y_lpf (Tensor) : shape (B,T) filtered signal\n\t    \"\"\"\n\t    b0,b1,b2,a0,a1,a2=filter\n\t    b0=torch.Tensor(b0).to(y.device)\n\t    b1=torch.Tensor(b1).to(y.device)\n\t    b2=torch.Tensor(b2).to(y.device)\n\t    a0=torch.Tensor(a0).to(y.device)\n\t    a1=torch.Tensor(a1).to(y.device)\n\t    a2=torch.Tensor(a2).to(y.device)\n", "    y_lpf=torchaudio.functional.biquad(y, b0,b1,b2,a0,a1,a2)\n\t    return y_lpf\n\tdef apply_low_pass_IIR(y,filter):\n\t    b,a=filter\n\t    b=torch.Tensor(b).to(y.device)\n\t    a=torch.Tensor(a).to(y.device)\n\t    y_lpf=torchaudio.functional.lfilter(y, a,b, clamp=False)\n\t    return y_lpf\n\tdef apply_low_pass(y,filter, type):\n\t    \"\"\"\n", "        Meta-function for applying a lowpass filter, maps y to another function depending on the type\n\t        Args:\n\t           y (Tensors): shape (B,T)\n\t           filter (whatever): filter coefficients, or whatever that specifies the filter\n\t           type (string): specifier of the type of filter\n\t        Returns\n\t           y_lpf (Tensor): shape (B,,T) foltered signal\n\t    \"\"\"\n\t    if type==\"firwin\":\n\t        return apply_low_pass_firwin(y,filter)\n", "    if type==\"firwin_hpf\":\n\t        return apply_low_pass_firwin(y,filter)\n\t    elif type==\"cheby1\":\n\t        return apply_low_pass_IIR(y,filter)\n\t    elif type==\"biquad\":\n\t        return apply_low_pass_biquad(y,filter)\n\t    elif type==\"resample\":\n\t        return apply_resample(y,filter)\n\t    elif type==\"decimate\":\n\t        return apply_decimate(y,filter)\n", "def get_cheby1_ba(order, ripple,hi ):\n\t    \"\"\"\n\t        Utility for designing a chebyshev type I IIR lowpass filter\n\t        Args:\n\t           order, ripple, hi: (see scipy.signal.cheby1 documentation)\n\t        Returns:\n\t           b,a: filter coefficients\n\t    \"\"\"\n\t    b,a = scipy.signal.cheby1(order, ripple, hi, btype='lowpass', output='ba')\n\t    return b,a\n", "def design_biquad_lpf(fc, fs, Q):\n\t    \"\"\"\n\t        utility for designing a biqad lowpass filter\n\t        Args:\n\t            fc (float): cutoff frequency\n\t            fs (float): sampling frequency\n\t            Q (float):  Q-factor\n\t    \"\"\"\n\t    w0 = 2 * math.pi * fc / fs\n\t    w0 = torch.as_tensor(w0, dtype=torch.float32)\n", "    alpha = torch.sin(w0) / 2 / Q\n\t    b0 = (1 - torch.cos(w0)) / 2\n\t    b1 = 1 - torch.cos(w0)\n\t    b2 = b0\n\t    a0 = 1 + alpha\n\t    a1 = -2 * torch.cos(w0)\n\t    a2 = 1 - alpha\n\t    return b0, b1, b2, a0, a1, a2\n"]}
{"filename": "utils/logging.py", "chunked_list": ["import os\n\timport torch \n\timport time\n\timport numpy as np\n\t#import torchaudio\n\timport plotly.express as px\n\timport soundfile as sf\n\t#import plotly.graph_objects as go\n\timport pandas as pd\n\timport plotly\n", "import scipy.signal as sig\n\timport plotly.graph_objects as go\n\t\"\"\"\n\tLogging related functions that I wrote for my own use\n\tThis is quite a mess, but I'm too lazy to clean it up\n\t\"\"\"\n\t#from src.CQT_nsgt import CQT_cpx\n\tdef do_stft(noisy, clean=None, win_size=2048, hop_size=512, device=\"cpu\", DC=True):\n\t    \"\"\"\n\t        applies the stft, this an ugly old function, but I'm using it for logging and I'm to lazy to modify it\n", "    \"\"\"\n\t    #window_fn = tf.signal.hamming_window\n\t    #win_size=args.stft.win_size\n\t    #hop_size=args.stft.hop_size\n\t    window=torch.hamming_window(window_length=win_size)\n\t    window=window.to(noisy.device)\n\t    noisy=torch.cat((noisy, torch.zeros(noisy.shape[0],win_size).to(noisy.device)),1)\n\t    stft_signal_noisy=torch.stft(noisy, win_size, hop_length=hop_size,window=window,center=False,return_complex=False)\n\t    stft_signal_noisy=stft_signal_noisy.permute(0,3,2,1)\n\t    #stft_signal_noisy=tf.signal.stft(noisy,frame_length=win_size, window_fn=window_fn, frame_step=hop_size)\n", "    #stft_noisy_stacked=tf.stack( values=[tf.math.real(stft_signal_noisy), tf.math.imag(stft_signal_noisy)], axis=-1)\n\t    if clean!=None:\n\t       # stft_signal_clean=tf.signal.stft(clean,frame_length=win_size, window_fn=window_fn, frame_step=hop_size)\n\t        clean=torch.cat((clean, torch.zeros(clean.shape[0],win_size).to(device)),1)\n\t        stft_signal_clean=torch.stft(clean, win_size, hop_length=hop_size,window=window, center=False,return_complex=False)\n\t        stft_signal_clean=stft_signal_clean.permute(0,3,2,1)\n\t        #stft_clean_stacked=tf.stack( values=[tf.math.real(stft_signal_clean), tf.math.imag(stft_signal_clean)], axis=-1)\n\t        if DC:\n\t            return stft_signal_noisy, stft_signal_clean\n\t        else:\n", "            return stft_signal_noisy[...,1:], stft_signal_clean[...,1:]\n\t    else:\n\t        if DC:\n\t            return stft_signal_noisy\n\t        else:\n\t            return stft_signal_noisy[...,1:]\n\tdef plot_norms(path, normsscores, normsguides, t, name):\n\t    values=t.cpu().numpy()\n\t    df=pd.DataFrame.from_dict(\n\t                {\"sigma\": values[0:-1], \"score\": normsscores.cpu().numpy(), \"guidance\": normsguides.cpu().numpy()\n", "                }\n\t                )\n\t    fig= px.line(df, x=\"sigma\", y=[\"score\", \"guidance\"],log_x=True,  log_y=True, markers=True)\n\t    path_to_plotly_html = path+\"/\"+name+\".html\"\n\t    fig.write_html(path_to_plotly_html, auto_play = False)\n\t    return fig\n\tdef plot_spectral_analysis_sampling( avgspecNF,avgspecDEN, ts, fs=22050, nfft=1024):\n\t    T,F=avgspecNF.shape\n\t    f=torch.arange(0, F)*fs/nfft\n\t    f=f.unsqueeze(1).repeat(1,T).view(-1)\n", "    avgspecNF=avgspecNF.permute(1,0).reshape(-1)\n\t    avgspecDEN=avgspecDEN.permute(1,0).reshape(-1)\n\t    ts=ts.cpu().numpy().tolist()\n\t    ts=513*ts\n\t    #ts=np.repeat(ts,513)\n\t    print(f.shape, avgspecDEN.shape, avgspecNF.shape, len(ts))\n\t    df=pd.DataFrame.from_dict(\n\t        {\"f\": f,  \"noisy\": avgspecNF, \"denoised\":  avgspecDEN, \"sigma\":ts\n\t        }\n\t        )\n", "    fig= px.line(df, x=\"f\", y=[\"noisy\", \"denoised\"],  animation_frame=\"sigma\", log_x=False,log_y=False,  markers=False)\n\t    return fig\n\tdef plot_spectral_analysis(avgspecY, avgspecNF,avgspecDEN, ts, fs=22050, nfft=1024):\n\t    T,F=avgspecNF.shape\n\t    f=torch.arange(0, F)*fs/nfft\n\t    f=f.unsqueeze(1).repeat(1,T).view(-1)\n\t    avgspecY=avgspecY.squeeze(0).unsqueeze(1).repeat(1,T).view(-1)\n\t    avgspecNF=avgspecNF.permute(1,0).reshape(-1)\n\t    avgspecDEN=avgspecDEN.permute(1,0).reshape(-1)\n\t    ts=ts.cpu().numpy().tolist()\n", "    ts=513*ts\n\t    #ts=np.repeat(ts,513)\n\t    print(f.shape, avgspecY.shape, avgspecNF.shape, len(ts))\n\t    df=pd.DataFrame.from_dict(\n\t        {\"f\": f, \"y\": avgspecY, \"noisy\": avgspecNF, \"denoised\":  avgspecDEN, \"sigma\":ts\n\t        }\n\t        )\n\t    fig= px.line(df, x=\"f\", y=[\"y\", \"noisy\", \"denoised\"],  animation_frame=\"sigma\", log_x=False,log_y=False,  markers=False)\n\t    return fig\n\tdef plot_loss_by_sigma_test_snr(average_snr, average_snr_out, t):\n", "    #write a fancy plot to log in wandb\n\t    values=np.array(t)\n\t    df=pd.DataFrame.from_dict(\n\t                {\"sigma\": values, \"SNR\": average_snr, \"SNR_denoised\": average_snr_out\n\t                }\n\t                )\n\t    fig= px.line(df, x=\"sigma\", y=[\"SNR\", \"SNR_denoised\"],log_x=True,  markers=True)\n\t    return fig\n\t# Create and style traces\n\tdef plot_loss_by_sigma(sigma_means, sigma_stds, sigma_bins):\n", "    df=pd.DataFrame.from_dict(\n\t                {\"sigma\": sigma_bins, \"loss\": sigma_means, \"std\": sigma_stds\n\t                }\n\t                )\n\t    fig= error_line('bar', data_frame=df, x=\"sigma\", y=\"loss\", error_y=\"std\", log_x=True,  markers=True, range_y=[0, 2])\n\t    return fig\n\tdef plot_loss_by_sigma_and_freq(sigma_freq_means, sigma_freq_stds, sigma_bins, freq_bins):\n\t    df=pd.DataFrame.from_dict(\n\t                {\"sigma\": np.tile(sigma_bins, len(freq_bins))}) \n\t    names=[]\n", "    means=[]\n\t    stds=[]\n\t    for i in range(len(freq_bins)):\n\t        name=[str(freq_bins[i])+\"Hz\" for j in range(len(sigma_bins))]\n\t        names.extend(name)\n\t        means.extend(sigma_freq_means[i])\n\t        stds.extend(sigma_freq_stds[i])\n\t    df['freq']=names\n\t    df['means']=means\n\t    df['stds']=stds\n", "    print(df)\n\t    #basically, we want to plot the loss as a function of sigma and freq. We do it by plotting a line for each freq.\n\t    fig= error_line('bar', data_frame=df, x=\"sigma\", y=\"means\", error_y=\"stds\", color=\"freq\", log_x=True,  markers=True, range_y=[0, 2])\n\t    return fig\n\tdef plot_melspectrogram(X, refr=1):\n\t    X=X.squeeze(1) #??\n\t    X=X.cpu().numpy()\n\t    if refr==None:\n\t        refr=np.max(np.abs(X))+1e-8\n\t    S_db = 10*np.log10(np.abs(X)/refr)\n", "    S_db=np.transpose(S_db, (0,2,1))\n\t    S_db=np.flip(S_db, axis=1)\n\t    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n\t        o=S_db[i]\n\t        if i==0:\n\t             res=o\n\t        else:\n\t             res=np.concatenate((res,o), axis=1)\n\t    fig=px.imshow( res,  zmin=-40, zmax=20)\n\t    fig.update_layout(coloraxis_showscale=False)\n", "    return fig\n\tdef plot_cpxspectrogram(X):\n\t    X=X.squeeze(1)\n\t    X=X.cpu().numpy()\n\t    #Xre=X[...,0]\n\t    #Xim=X[...,1]\n\t    #X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n\t    #if refr==None:\n\t    #    refr=np.max(np.abs(X))+1e-8\n\t    #S_db = 10*np.log10(np.abs(X)/refr)\n", "    #S_db=np.transpose(S_db, (0,2,1))\n\t    #S_db=np.flip(S_db, axis=1)\n\t    #for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n\t    #    o=S_db[i]\n\t    #    if i==0:\n\t    #         res=o\n\t    #    else:\n\t    #         res=np.concatenate((res,o), axis=1)\n\t    fig=px.imshow(X, facet_col=3, animation_frame=0)\n\t    fig.update_layout(coloraxis_showscale=False)\n", "    return fig\n\tdef print_cuda_memory():\n\t    t = torch.cuda.get_device_properties(0).total_memory\n\t    r = torch.cuda.memory_reserved(0)\n\t    a = torch.cuda.memory_allocated(0)\n\t    f = r-a  # free inside reservedk\n\t    print(\"memrylog\",t,r,a,f)\n\tdef plot_spectrogram(X, refr=None):\n\t    X=X.squeeze(1)\n\t    X=X.cpu().numpy()\n", "    X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n\t    if refr==None:\n\t        refr=np.max(np.abs(X))+1e-8\n\t    S_db = 10*np.log10(np.abs(X)/refr)\n\t    S_db=np.transpose(S_db, (0,2,1))\n\t    S_db=np.flip(S_db, axis=1)\n\t    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n\t        o=S_db[i]\n\t        if i==0:\n\t             res=o\n", "        else:\n\t             res=np.concatenate((res,o), axis=1)\n\t    fig=px.imshow( res,  zmin=-40, zmax=20)\n\t    fig.update_layout(coloraxis_showscale=False)\n\t    return fig\n\tdef plot_mag_spectrogram(X, refr=None, path=None,name=\"spec\"):\n\t    #X=X.squeeze(1)\n\t    X=X.cpu().numpy()\n\t    #X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n\t    if refr==None:\n", "        refr=np.max(np.abs(X))+1e-8\n\t    S_db = 10*np.log10(np.abs(X)/refr)\n\t    #S_db=np.transpose(S_db, (0,2,1))\n\t    S_db=np.flip(S_db, axis=1)\n\t    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n\t        o=S_db[i]\n\t        if i==0:\n\t             res=o\n\t        else:\n\t             res=np.concatenate((res,o), axis=1)\n", "    fig=px.imshow( res,  zmin=-40, zmax=20)\n\t    fig.update_layout(coloraxis_showscale=False)\n\t    path_to_plotly_png = path+\"/\"+name+\".png\"\n\t    plotly.io.write_image(fig, path_to_plotly_png)\n\t    return fig\n\tdef plot_spectrogram(X, refr=None):\n\t    X=X.squeeze(1)\n\t    X=X.cpu().numpy()\n\t    X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n\t    if refr==None:\n", "        refr=np.max(np.abs(X))+1e-8\n\t    S_db = 10*np.log10(np.abs(X)/refr)\n\t    S_db=np.transpose(S_db, (0,2,1))\n\t    S_db=np.flip(S_db, axis=1)\n\t    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n\t        o=S_db[i]\n\t        if i==0:\n\t             res=o\n\t        else:\n\t             res=np.concatenate((res,o), axis=1)\n", "    fig=px.imshow( res,  zmin=-40, zmax=20)\n\t    fig.update_layout(coloraxis_showscale=False)\n\t    return fig\n\tdef write_audio_file(x, sr, string: str, path='tmp', stereo=False):\n\t    if not(os.path.exists(path)): \n\t        os.makedirs(path)\n\t    path=os.path.join(path,string+\".wav\")\n\t    if stereo:\n\t        '''\n\t        x has shape (B,2,T)\n", "        '''\n\t        x=x.permute(0,2,1) #B,T,2\n\t        x=x.flatten(0,1) #B*T,2\n\t        x=x.cpu().numpy()\n\t        #if np.abs(np.max(x))>=1:\n\t        #    #normalize to avoid clipping\n\t        #    x=x/np.abs(np.max(x))\n\t    else:\n\t        x=x.flatten()\n\t        x=x.unsqueeze(1)\n", "        x=x.cpu().numpy()\n\t        #if np.abs(np.max(x))>=1:\n\t        #    #normalize to avoid clipping\n\t        #    x=x/np.abs(np.max(x))\n\t    sf.write(path,x,sr)\n\t    return path\n\tdef plot_trajectories(x,sr, sigma_data):\n\t    \"\"\"\n\t    x: (B,T) Batch of trajectories\n\t    sr: sampling rate\n", "    args: args object\n\t    \"\"\"\n\t    x=x.cpu()\n\t    times=np.arange(x.shape[1])/sr\n\t    fig=plot_batch_of_lines(x/sigma_data, times, log_x=False)\n\t    return fig\n\tdef plot_trajectories_fft(x, sr, sigma_data):\n\t    \"\"\"\n\t    x: (B,T) Batch of trajectories\n\t    sr: sampling rate\n", "    args: args object\n\t    Apply fft to the trajectories and plot the magnitude of the fft\n\t    \"\"\"\n\t    x=x.cpu()\n\t    times=np.arange(x.shape[1])/sr\n\t    x=x/sigma_data\n\t    x_fft=np.fft.rfft(x, axis=1)\n\t    x_fft=np.abs(x_fft)\n\t    freqs=np.fft.rfftfreq(x.shape[1], d=1/sr)\n\t    #now do it on db scale\n", "    x_fft_db=10*np.log10(x_fft)\n\t    fig=plot_batch_of_lines(x_fft_db, freqs, log_x=False)\n\t    return fig\n\t    return fig\n\tdef plot_cpxCQT_from_raw_audio(x, args, refr=None ):\n\t    #shape of input spectrogram:  (     ,T,F, )\n\t    fmax=args.sample_rate/2\n\t    fmin=fmax/(2**args.cqt.numocts)\n\t    fbins=int(args.cqt.binsoct*args.cqt.numocts) \n\t    device=x.device\n", "    CQTransform=CQT_cpx(fmin,fbins, args.sample_rate, args.audio_len, device=device, split_0_nyq=False)\n\t    x=x\n\t    X=CQTransform.fwd(x)\n\t    return plot_cpxspectrogram(X)\n\tdef plot_CQT_from_raw_audio(x, args, refr=None ):\n\t    #shape of input spectrogram:  (     ,T,F, )\n\t    fmax=args.sample_rate/2\n\t    fmin=fmax/(2**args.cqt.numocts)\n\t    fbins=int(args.cqt.binsoct*args.cqt.numocts) \n\t    device=x.device\n", "    CQTransform=CQT_cpx(fmin,fbins, args.sample_rate, args.audio_len, device=device, split_0_nyq=False)\n\t    refr=3\n\t    x=x\n\t    X=CQTransform.fwd(x)\n\t    return plot_spectrogram(X, refr)\n\tdef get_spectrogram_from_raw_audio(x, stft, refr=1):\n\t    X=do_stft(x, win_size=stft.win_size, hop_size=stft.hop_size)\n\t    X=X.permute(0,2,3,1)\n\t    X=X.squeeze(1)\n\t    #X=X.cpu().numpy()\n", "    X=torch.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n\t    #if refr==None:\n\t    #    refr=np.max(np.abs(X))+1e-8\n\t    S_db = 10*torch.log10(torch.abs(X)/refr)\n\t    S_db=S_db.permute(0,2,1)\n\t    #np.transpose(S_db, (0,2,1))\n\t    S_db=torch.flip(S_db, [1])\n\t    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n\t        o=S_db[i]\n\t        if i==0:\n", "             res=o\n\t        else:\n\t             res=torch.cat((res,o), 1)\n\t    return res\n\tdef downsample2d(inputArray, kernelSize):\n\t    \"\"\"This function downsamples a 2d numpy array by convolving with a flat\n\t    kernel and then sub-sampling the resulting array.\n\t    A kernel size of 2 means convolution with a 2x2 array [[1, 1], [1, 1]] and\n\t    a resulting downsampling of 2-fold.\n\t    :param: inputArray: 2d numpy array\n", "    :param: kernelSize: integer\n\t    \"\"\"\n\t    average_kernel = np.ones((kernelSize,kernelSize))\n\t    blurred_array = sig.convolve2d(inputArray, average_kernel, mode='same')\n\t    downsampled_array = blurred_array[::kernelSize,::kernelSize]\n\t    return downsampled_array\n\tdef diffusion_CQT_animation(path, x ,t,  args, refr=1, name=\"animation_diffusion\", resample_factor=1 ):\n\t    \"\"\"\n\t        Utility for creating an animation of the cqt diffusion process\n\t    \"\"\"\n", "    #shape of input spectrograms:  (Nsteps,B,Time,Freq)\n\t    #print(noisy.shape)\n\t    Nsteps=x.shape[0]\n\t    numsteps=10\n\t    tt=torch.linspace(0, Nsteps-1, numsteps)\n\t    i_s=[]\n\t    allX=None\n\t    device=x.device\n\t    fmax=args.sample_rate/2\n\t    fmin=fmax/(2**args.cqt.numocts)\n", "    fbins=int(args.cqt.binsoct*args.cqt.numocts) \n\t    CQTransform=CQT_cpx(fmin,fbins, args.sample_rate, args.audio_len, device=device, split_0_nyq=False)\n\t    for i in tt:\n\t        i=int(torch.floor(i))\n\t        i_s.append(i)\n\t        xx=x[i]\n\t        X=CQTransform.fwd(xx)\n\t        X=torch.sqrt(X[...,0]**2 + X[...,1]**2)\n\t        S_db = 10*torch.log10(torch.abs(X)/refr)\n\t        S_db = S_db[:,1:-1,1:-1]\n", "        S_db=S_db.permute(0,2,1)\n\t        #np.transpose(S_db, (0,2,1))\n\t        S_db=torch.flip(S_db, [1])\n\t        S_db=S_db.unsqueeze(0)\n\t        S_db=torch.nn.functional.interpolate(S_db, size= (S_db.shape[2]//resample_factor, S_db.shape[3]//resample_factor), mode=\"bilinear\")\n\t        S_db=S_db.squeeze(0)\n\t        S_db=S_db.cpu().numpy()\n\t        #S_db=S_db.squeeze(0)        \n\t        if i==0:\n\t             allX=S_db\n", "        else:\n\t             allX=np.concatenate((allX,S_db), 0)\n\t    #fig=px.imshow(S_db, animation_frame=0, facet_col=3, binary_compression_level=0) #I need to incorporate t here!!!\n\t    fig=px.imshow(allX, animation_frame=0,  zmin=-40, zmax=10, binary_compression_level=0) #I need to incorporate t here!!!\n\t    fig.update_layout(coloraxis_showscale=False)\n\t    fig.update_xaxes(showticklabels=False)\n\t    fig.update_yaxes(showticklabels=False)\n\t    t=t[i_s].cpu().numpy()\n\t    assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n\t    #print(fig)\n", "    for i, f in enumerate(fig.layout.sliders[0].steps):\n\t        #hacky way of changing the axis values\n\t        #f.args[0]=[str(t[i])]\n\t        f.label=str(t[i])\n\t    path_to_plotly_html = path+\"/\"+name+\".html\"\n\t    fig.write_html(path_to_plotly_html, auto_play = False)\n\t    return fig\n\tdef diffusion_joint_filter_animation(path, score, grads , f, t,  refr=1, name=\"animation_diffusion\", NT=15 ):\n\t    '''\n\t    plot an animation of the reverse diffusion process of filters\n", "    args:\n\t        path: path to save the animation\n\t        x: filters (Nsteps, F)\n\t        f: frequencies (F)\n\t        t: timesteps (sigma)\n\t        name: name of the animation\n\t    '''\n\t    #shape of input spectrograms:  (Nsteps,B,Time,Freq)\n\t    #print(noisy.shape)\n\t    Nsteps=score.shape[0]\n", "    #numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n\t    #tt=torch.linspace(0, Nsteps-1, numsteps)\n\t    #i_s=[]\n\t    #for i in tt:\n\t    #    i=int(torch.floor(i))\n\t    #    i_s.append(i)\n\t    print(f.shape, score.shape, grads.shape, t.shape)  #19, (100,19)\n\t    #fig=px.line(x=f.unsqueeze(0).numpy(),y=x, animation_frame=0) #I need to incorporate t here!!!\n\t    sigma=t\n\t    s=score.squeeze(1)# (100,19)\n", "    g=grads.squeeze(1)# (100,19)\n\t    f=f# (19,)\n\t    f=f.unsqueeze(0).expand(s.shape[0], -1).reshape(-1)\n\t    sigma=sigma.unsqueeze(-1).expand(-1, s.shape[1]).reshape(-1)\n\t    s=s.reshape(-1)\n\t    g=g.reshape(-1)\n\t    max_score=torch.max(s).item()\n\t    df=pd.DataFrame(\n\t        {\n\t            \"f\": f.cpu().numpy(),\n", "            \"score\": s.cpu().numpy(),\n\t            \"grads\": g.cpu().numpy(),\n\t            \"sigma\": sigma.cpu().numpy()\n\t        }\n\t    )\n\t    fig=px.line(df, x=\"f\",y=[\"score\",\"grads\"], animation_frame=\"sigma\", log_x=True) #I need to incorporate t here!!!\n\t    #set the range of th y axis\n\t    fig.update_yaxes(range=[-max_score,max_score])\n\t    #t=t[i_s].cpu().numpy()\n\t    #t=t.cpu().numpy()\n", "    #assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n\t    #print(fig)\n\t    #for i, f in enumerate(fig.layout.sliders[0].steps):\n\t        #hacky way of changing the axis values\n\t        #f.args[0]=[str(t[i])]\n\t    #    f.label=str(t[i])\n\t    path_to_plotly_html = path+\"/\"+name+\".html\"\n\t    fig.write_html(path_to_plotly_html, auto_play = False)\n\t    return fig\n\tdef diffusion_filter_animation(path, x , f, t,  refr=1, name=\"animation_diffusion\", NT=15 ):\n", "    '''\n\t    plot an animation of the reverse diffusion process of filters\n\t    args:\n\t        path: path to save the animation\n\t        x: filters (Nsteps, F)\n\t        f: frequencies (F)\n\t        t: timesteps (sigma)\n\t        name: name of the animation\n\t    '''\n\t    #shape of input spectrograms:  (Nsteps,B,Time,Freq)\n", "    #print(noisy.shape)\n\t    Nsteps=x.shape[0]\n\t    #numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n\t    #tt=torch.linspace(0, Nsteps-1, numsteps)\n\t    #i_s=[]\n\t    #for i in tt:\n\t    #    i=int(torch.floor(i))\n\t    #    i_s.append(i)\n\t    print(f.shape, x.shape, t.shape)  #19, (100,19)\n\t    #fig=px.line(x=f.unsqueeze(0).numpy(),y=x, animation_frame=0) #I need to incorporate t here!!!\n", "    sigma=t\n\t    x=x.squeeze(1)# (100,19)\n\t    f=f# (19,)\n\t    f=f.unsqueeze(0).expand(x.shape[0], -1).reshape(-1)\n\t    sigma=sigma.unsqueeze(-1).expand(-1, x.shape[1]).reshape(-1)\n\t    x=x.reshape(-1)\n\t    df=pd.DataFrame(\n\t        {\n\t            \"f\": f.cpu().numpy(),\n\t            \"x\": x.cpu().numpy(),\n", "            \"sigma\": sigma.cpu().numpy()\n\t        }\n\t    )\n\t    fig=px.line(df, x=\"f\",y=\"x\", animation_frame=\"sigma\", log_x=True) #I need to incorporate t here!!!\n\t    #t=t[i_s].cpu().numpy()\n\t    #t=t.cpu().numpy()\n\t    #assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n\t    #print(fig)\n\t    #for i, f in enumerate(fig.layout.sliders[0].steps):\n\t        #hacky way of changing the axis values\n", "        #f.args[0]=[str(t[i])]\n\t    #    f.label=str(t[i])\n\t    path_to_plotly_html = path+\"/\"+name+\".html\"\n\t    fig.write_html(path_to_plotly_html, auto_play = False)\n\t    return fig\n\tdef diffusion_spec_animation(path, x ,t,  stft, refr=1, name=\"animation_diffusion\",NT=15 ):\n\t    '''\n\t    plot an animation of the reverse diffusion process of filters\n\t    args:\n\t        path: path to save the animation\n", "        x: input audio (N,T)\n\t        t: timesteps (sigma)\n\t        name: name of the animation\n\t    '''\n\t    #print(noisy.shape)\n\t    Nsteps=x.shape[0]\n\t    numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n\t    tt=torch.linspace(0, Nsteps-1, numsteps)\n\t    i_s=[]\n\t    allX=None\n", "    for i in tt:\n\t        i=int(torch.floor(i))\n\t        i_s.append(i)\n\t        X=get_spectrogram_from_raw_audio(x[i],stft, refr)\n\t        X=X.unsqueeze(0)\n\t        if allX==None:\n\t             allX=X\n\t        else:\n\t             allX=torch.cat((allX,X), 0)\n\t    allX=allX.cpu().numpy()\n", "    fig=px.imshow(allX, animation_frame=0,  zmin=-40, zmax=20) #I need to incorporate t here!!!\n\t    fig.update_layout(coloraxis_showscale=False)\n\t    t=t[i_s].cpu().numpy()\n\t    assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n\t    #print(fig)\n\t    for i, f in enumerate(fig.layout.sliders[0].steps):\n\t        #hacky way of changing the axis values\n\t        #f.args[0]=[str(t[i])]\n\t        f.label=str(t[i])\n\t    path_to_plotly_html = path+\"/\"+name+\".html\"\n", "    fig.write_html(path_to_plotly_html, auto_play = False)\n\t    return fig\n\tdef plot_spectrogram_from_raw_audio(x, stft, refr=None ):\n\t    #shape of input spectrogram:  (     ,T,F, )\n\t    refr=3\n\t    x=x\n\t    X=do_stft(x, win_size=stft.win_size, hop_size=stft.hop_size)\n\t    X=X.permute(0,2,3,1)\n\t    return plot_spectrogram(X, refr)\n\tdef plot_spectrogram_from_cpxspec(X, refr=None):\n", "    #shape of input spectrogram:  (     ,T,F, )\n\t    return plot_spectrogram(X, refr)\n\tdef error_line(error_y_mode='band', **kwargs):\n\t    \"\"\"Extension of `plotly.express.line` to use error bands.\"\"\"\n\t    ERROR_MODES = {'bar','band','bars','bands',None}\n\t    if error_y_mode not in ERROR_MODES:\n\t        raise ValueError(f\"'error_y_mode' must be one of {ERROR_MODES}, received {repr(error_y_mode)}.\")\n\t    if error_y_mode in {'bar','bars',None}:\n\t        fig = px.line(**kwargs)\n\t    elif error_y_mode in {'band','bands'}:\n", "        if 'error_y' not in kwargs:\n\t            raise ValueError(f\"If you provide argument 'error_y_mode' you must also provide 'error_y'.\")\n\t        figure_with_error_bars = px.line(**kwargs)\n\t        fig = px.line(**{arg: val for arg,val in kwargs.items() if arg != 'error_y'})\n\t        for data in figure_with_error_bars.data:\n\t            x = list(data['x'])\n\t            y_upper = list(data['y'] + data['error_y']['array'])\n\t            y_lower = list(data['y'] - data['error_y']['array'] if data['error_y']['arrayminus'] is None else data['y'] - data['error_y']['arrayminus'])\n\t            color = f\"rgba({tuple(int(data['line']['color'].lstrip('#')[i:i+2], 16) for i in (0, 2, 4))},.3)\".replace('((','(').replace('),',',').replace(' ','')\n\t            fig.add_trace(\n", "                go.Scatter(\n\t                    x = x+x[::-1],\n\t                    y = y_upper+y_lower[::-1],\n\t                    fill = 'toself',\n\t                    fillcolor = color,\n\t                    line = dict(\n\t                        color = 'rgba(255,255,255,0)'\n\t                    ),\n\t                    hoverinfo = \"skip\",\n\t                    showlegend = False,\n", "                    legendgroup = data['legendgroup'],\n\t                    xaxis = data['xaxis'],\n\t                    yaxis = data['yaxis'],\n\t                )\n\t            )\n\t        # Reorder data as said here: https://stackoverflow.com/a/66854398/8849755\n\t        reordered_data = []\n\t        for i in range(int(len(fig.data)/2)):\n\t            reordered_data.append(fig.data[i+int(len(fig.data)/2)])\n\t            reordered_data.append(fig.data[i])\n", "        fig.data = tuple(reordered_data)\n\t    return fig\n\tdef plot_filters( x, freqs):\n\t    '''\n\t    This function plots a batch of lines using plotly\n\t    args:\n\t        x: (B, F)\n\t    '''\n\t    fig = px.line(x=freqs, y=x[0,:], log_x=True)\n\t    for i in range(1,x.shape[0]):\n", "        fig.add_trace(go.Scatter(x=freqs, y=x[i,:]))\n\t    return fig\n\tdef plot_batch_of_lines( x, freqs, log_x=True):\n\t    '''\n\t    This function plots a batch of lines using plotly\n\t    args:\n\t        x: (B, F)\n\t    '''\n\t    fig = px.line(x=freqs, y=x[0,:], log_x=log_x)\n\t    for i in range(1,x.shape[0]):\n", "        fig.add_trace(go.Scatter(x=freqs, y=x[i,:]))\n\t    return fig\n"]}
{"filename": "utils/dnnlib/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\tfrom .util import EasyDict, make_cache_dir_path, call_func_by_name\n"]}
{"filename": "utils/dnnlib/util.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Miscellaneous utility classes and functions.\"\"\"\n\timport ctypes\n\timport fnmatch\n\timport importlib\n", "import inspect\n\timport numpy as np\n\timport os\n\timport shutil\n\timport sys\n\timport types\n\timport io\n\timport pickle\n\timport re\n\timport requests\n", "import html\n\timport hashlib\n\timport glob\n\timport tempfile\n\timport urllib\n\timport urllib.request\n\timport uuid\n\tfrom distutils.util import strtobool\n\tfrom typing import Any, List, Tuple, Union, Optional\n\t# Util classes\n", "# ------------------------------------------------------------------------------------------\n\tclass EasyDict(dict):\n\t    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n\t    def __getattr__(self, name: str) -> Any:\n\t        try:\n\t            return self[name]\n\t        except KeyError:\n\t            raise AttributeError(name)\n\t    def __setattr__(self, name: str, value: Any) -> None:\n\t        self[name] = value\n", "    def __delattr__(self, name: str) -> None:\n\t        del self[name]\n\tclass Logger(object):\n\t    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n\t    def __init__(self, file_name: Optional[str] = None, file_mode: str = \"w\", should_flush: bool = True):\n\t        self.file = None\n\t        if file_name is not None:\n\t            self.file = open(file_name, file_mode)\n\t        self.should_flush = should_flush\n\t        self.stdout = sys.stdout\n", "        self.stderr = sys.stderr\n\t        sys.stdout = self\n\t        sys.stderr = self\n\t    def __enter__(self) -> \"Logger\":\n\t        return self\n\t    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n\t        self.close()\n\t    def write(self, text: Union[str, bytes]) -> None:\n\t        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n\t        if isinstance(text, bytes):\n", "            text = text.decode()\n\t        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n\t            return\n\t        if self.file is not None:\n\t            self.file.write(text)\n\t        self.stdout.write(text)\n\t        if self.should_flush:\n\t            self.flush()\n\t    def flush(self) -> None:\n\t        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n", "        if self.file is not None:\n\t            self.file.flush()\n\t        self.stdout.flush()\n\t    def close(self) -> None:\n\t        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n\t        self.flush()\n\t        # if using multiple loggers, prevent closing in wrong order\n\t        if sys.stdout is self:\n\t            sys.stdout = self.stdout\n\t        if sys.stderr is self:\n", "            sys.stderr = self.stderr\n\t        if self.file is not None:\n\t            self.file.close()\n\t            self.file = None\n\t# Cache directories\n\t# ------------------------------------------------------------------------------------------\n\t_dnnlib_cache_dir = None\n\tdef set_cache_dir(path: str) -> None:\n\t    global _dnnlib_cache_dir\n\t    _dnnlib_cache_dir = path\n", "def make_cache_dir_path(*paths: str) -> str:\n\t    if _dnnlib_cache_dir is not None:\n\t        return os.path.join(_dnnlib_cache_dir, *paths)\n\t    if 'DNNLIB_CACHE_DIR' in os.environ:\n\t        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n\t    if 'HOME' in os.environ:\n\t        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n\t    if 'USERPROFILE' in os.environ:\n\t        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n\t    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n", "# Small util functions\n\t# ------------------------------------------------------------------------------------------\n\tdef format_time(seconds: Union[int, float]) -> str:\n\t    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n\t    s = int(np.rint(seconds))\n\t    if s < 60:\n\t        return \"{0}s\".format(s)\n\t    elif s < 60 * 60:\n\t        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n\t    elif s < 24 * 60 * 60:\n", "        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n\t    else:\n\t        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n\tdef format_time_brief(seconds: Union[int, float]) -> str:\n\t    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n\t    s = int(np.rint(seconds))\n\t    if s < 60:\n\t        return \"{0}s\".format(s)\n\t    elif s < 60 * 60:\n\t        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n", "    elif s < 24 * 60 * 60:\n\t        return \"{0}h {1:02}m\".format(s // (60 * 60), (s // 60) % 60)\n\t    else:\n\t        return \"{0}d {1:02}h\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24)\n\tdef ask_yes_no(question: str) -> bool:\n\t    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n\t    while True:\n\t        try:\n\t            print(\"{0} [y/n]\".format(question))\n\t            return strtobool(input().lower())\n", "        except ValueError:\n\t            pass\n\tdef tuple_product(t: Tuple) -> Any:\n\t    \"\"\"Calculate the product of the tuple elements.\"\"\"\n\t    result = 1\n\t    for v in t:\n\t        result *= v\n\t    return result\n\t_str_to_ctype = {\n\t    \"uint8\": ctypes.c_ubyte,\n", "    \"uint16\": ctypes.c_uint16,\n\t    \"uint32\": ctypes.c_uint32,\n\t    \"uint64\": ctypes.c_uint64,\n\t    \"int8\": ctypes.c_byte,\n\t    \"int16\": ctypes.c_int16,\n\t    \"int32\": ctypes.c_int32,\n\t    \"int64\": ctypes.c_int64,\n\t    \"float32\": ctypes.c_float,\n\t    \"float64\": ctypes.c_double\n\t}\n", "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n\t    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n\t    type_str = None\n\t    if isinstance(type_obj, str):\n\t        type_str = type_obj\n\t    elif hasattr(type_obj, \"__name__\"):\n\t        type_str = type_obj.__name__\n\t    elif hasattr(type_obj, \"name\"):\n\t        type_str = type_obj.name\n\t    else:\n", "        raise RuntimeError(\"Cannot infer type name from input\")\n\t    assert type_str in _str_to_ctype.keys()\n\t    my_dtype = np.dtype(type_str)\n\t    my_ctype = _str_to_ctype[type_str]\n\t    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\t    return my_dtype, my_ctype\n\tdef is_pickleable(obj: Any) -> bool:\n\t    try:\n\t        with io.BytesIO() as stream:\n\t            pickle.dump(obj, stream)\n", "        return True\n\t    except:\n\t        return False\n\t# Functionality to import modules/objects by name, and call functions by name\n\t# ------------------------------------------------------------------------------------------\n\tdef get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n\t    \"\"\"Searches for the underlying module behind the name to some python object.\n\t    Returns the module and the object name (original name with module part removed).\"\"\"\n\t    # allow convenience shorthands, substitute them by full names\n\t    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n", "    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n\t    # list alternatives for (module_name, local_obj_name)\n\t    parts = obj_name.split(\".\")\n\t    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n\t    # try each alternative in turn\n\t    for module_name, local_obj_name in name_pairs:\n\t        try:\n\t            module = importlib.import_module(module_name) # may raise ImportError\n\t            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n\t            return module, local_obj_name\n", "        except:\n\t            pass\n\t    # maybe some of the modules themselves contain errors?\n\t    for module_name, _local_obj_name in name_pairs:\n\t        try:\n\t            importlib.import_module(module_name) # may raise ImportError\n\t        except ImportError:\n\t            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n\t                raise\n\t    # maybe the requested attribute is missing?\n", "    for module_name, local_obj_name in name_pairs:\n\t        try:\n\t            module = importlib.import_module(module_name) # may raise ImportError\n\t            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n\t        except ImportError:\n\t            pass\n\t    # we are out of luck, but we have no idea why\n\t    raise ImportError(obj_name)\n\tdef get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n\t    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n", "    if obj_name == '':\n\t        return module\n\t    obj = module\n\t    for part in obj_name.split(\".\"):\n\t        obj = getattr(obj, part)\n\t    return obj\n\tdef get_obj_by_name(name: str) -> Any:\n\t    \"\"\"Finds the python object with the given name.\"\"\"\n\t    module, obj_name = get_module_from_obj_name(name)\n\t    return get_obj_from_module(module, obj_name)\n", "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n\t    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n\t    assert func_name is not None\n\t    func_obj = get_obj_by_name(func_name)\n\t    assert callable(func_obj)\n\t    return func_obj(*args, **kwargs)\n\tdef construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n\t    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n\t    return call_func_by_name(*args, func_name=class_name, **kwargs)\n\tdef get_module_dir_by_obj_name(obj_name: str) -> str:\n", "    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n\t    module, _ = get_module_from_obj_name(obj_name)\n\t    return os.path.dirname(inspect.getfile(module))\n\tdef is_top_level_function(obj: Any) -> bool:\n\t    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n\t    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n\tdef get_top_level_function_name(obj: Any) -> str:\n\t    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n\t    assert is_top_level_function(obj)\n\t    module = obj.__module__\n", "    if module == '__main__':\n\t        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n\t    return module + \".\" + obj.__name__\n\t# File system helpers\n\t# ------------------------------------------------------------------------------------------\n\tdef list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n\t    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n\t    Returns list of tuples containing both absolute and relative paths.\"\"\"\n\t    assert os.path.isdir(dir_path)\n\t    base_name = os.path.basename(os.path.normpath(dir_path))\n", "    if ignores is None:\n\t        ignores = []\n\t    result = []\n\t    for root, dirs, files in os.walk(dir_path, topdown=True):\n\t        for ignore_ in ignores:\n\t            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n\t            # dirs need to be edited in-place\n\t            for d in dirs_to_remove:\n\t                dirs.remove(d)\n\t            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n", "        absolute_paths = [os.path.join(root, f) for f in files]\n\t        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n\t        if add_base_to_relative:\n\t            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n\t        assert len(absolute_paths) == len(relative_paths)\n\t        result += zip(absolute_paths, relative_paths)\n\t    return result\n\tdef copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n\t    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n\t    Will create all necessary directories.\"\"\"\n", "    for file in files:\n\t        target_dir_name = os.path.dirname(file[1])\n\t        # will create all intermediate-level directories\n\t        if not os.path.exists(target_dir_name):\n\t            os.makedirs(target_dir_name)\n\t        shutil.copyfile(file[0], file[1])\n\t# URL helpers\n\t# ------------------------------------------------------------------------------------------\n\tdef is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n\t    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n", "    if not isinstance(obj, str) or not \"://\" in obj:\n\t        return False\n\t    if allow_file_urls and obj.startswith('file://'):\n\t        return True\n\t    try:\n\t        res = requests.compat.urlparse(obj)\n\t        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n\t            return False\n\t        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n\t        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n", "            return False\n\t    except:\n\t        return False\n\t    return True\n\tdef open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n\t    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n\t    assert num_attempts >= 1\n\t    assert not (return_filename and (not cache))\n\t    # Doesn't look like an URL scheme so interpret it as a local filename.\n\t    if not re.match('^[a-z]+://', url):\n", "        return url if return_filename else open(url, \"rb\")\n\t    # Handle file URLs.  This code handles unusual file:// patterns that\n\t    # arise on Windows:\n\t    #\n\t    # file:///c:/foo.txt\n\t    #\n\t    # which would translate to a local '/c:/foo.txt' filename that's\n\t    # invalid.  Drop the forward slash for such pathnames.\n\t    #\n\t    # If you touch this code path, you should test it on both Linux and\n", "    # Windows.\n\t    #\n\t    # Some internet resources suggest using urllib.request.url2pathname() but\n\t    # but that converts forward slashes to backslashes and this causes\n\t    # its own set of problems.\n\t    if url.startswith('file://'):\n\t        filename = urllib.parse.urlparse(url).path\n\t        if re.match(r'^/[a-zA-Z]:', filename):\n\t            filename = filename[1:]\n\t        return filename if return_filename else open(filename, \"rb\")\n", "    assert is_url(url)\n\t    # Lookup from cache.\n\t    if cache_dir is None:\n\t        cache_dir = make_cache_dir_path('downloads')\n\t    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n\t    if cache:\n\t        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n\t        if len(cache_files) == 1:\n\t            filename = cache_files[0]\n\t            return filename if return_filename else open(filename, \"rb\")\n", "    # Download.\n\t    url_name = None\n\t    url_data = None\n\t    with requests.Session() as session:\n\t        if verbose:\n\t            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n\t        for attempts_left in reversed(range(num_attempts)):\n\t            try:\n\t                with session.get(url) as res:\n\t                    res.raise_for_status()\n", "                    if len(res.content) == 0:\n\t                        raise IOError(\"No data received\")\n\t                    if len(res.content) < 8192:\n\t                        content_str = res.content.decode(\"utf-8\")\n\t                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n\t                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n\t                            if len(links) == 1:\n\t                                url = requests.compat.urljoin(url, links[0])\n\t                                raise IOError(\"Google Drive virus checker nag\")\n\t                        if \"Google Drive - Quota exceeded\" in content_str:\n", "                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n\t                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n\t                    url_name = match[1] if match else url\n\t                    url_data = res.content\n\t                    if verbose:\n\t                        print(\" done\")\n\t                    break\n\t            except KeyboardInterrupt:\n\t                raise\n\t            except:\n", "                if not attempts_left:\n\t                    if verbose:\n\t                        print(\" failed\")\n\t                    raise\n\t                if verbose:\n\t                    print(\".\", end=\"\", flush=True)\n\t    # Save to cache.\n\t    if cache:\n\t        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n\t        safe_name = safe_name[:min(len(safe_name), 128)]\n", "        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n\t        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n\t        os.makedirs(cache_dir, exist_ok=True)\n\t        with open(temp_file, \"wb\") as f:\n\t            f.write(url_data)\n\t        os.replace(temp_file, cache_file) # atomic\n\t        if return_filename:\n\t            return cache_file\n\t    # Return data as file object.\n\t    assert not return_filename\n", "    return io.BytesIO(url_data)\n"]}
{"filename": "training/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t# empty\n"]}
{"filename": "training/trainer.py", "chunked_list": ["# Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Main training loop.\"\"\"\n\timport os\n\timport time\n\timport copy\n", "import numpy as np\n\timport torch\n\timport torchaudio\n\tfrom utils.torch_utils import training_stats\n\tfrom utils.torch_utils import misc\n\timport librosa\n\tfrom glob import glob\n\timport re\n\timport wandb\n\timport utils.logging as utils_logging\n", "from torch.profiler import tensorboard_trace_handler\n\timport omegaconf\n\timport utils.training_utils as t_utils\n\tfrom surgeon_pytorch import Inspect, get_layers\n\t#----------------------------------------------------------------------------\n\tclass Trainer():\n\t    def __init__(self, args, dset, network, optimizer, diff_params, tester=None, device='cpu'):\n\t        self.args=args\n\t        self.dset=dset\n\t        #self.network=torch.compile(network)\n", "        self.network=network\n\t        self.optimizer=optimizer\n\t        self.diff_params=diff_params\n\t        self.device=device\n\t        #testing means generating demos by sampling from the model\n\t        self.tester=tester\n\t        if self.tester is None or not(self.args.tester.do_test):\n\t            self.do_test=False\n\t        else:\n\t            self.do_test=True\n", "        #these are settings set by karras. I am not sure what they do\n\t        #np.random.seed((seed * dist.get_world_size() + dist.get_rank()) % (1 << 31))\n\t        torch.manual_seed(np.random.randint(1 << 31))\n\t        torch.backends.cudnn.enabled = True\n\t        torch.backends.cudnn.benchmark = True\n\t        torch.backends.cudnn.allow_tf32 = True\n\t        torch.backends.cuda.matmul.allow_tf32 = True\n\t        torch.backends.cudnn.deterministic = False\n\t        #torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\t        #S=self.args.exp.resample_factor\n", "        #if S>2.1 and S<2.2:\n\t        #    #resampling 48k to 22.05k\n\t        #    self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n\t        #elif S!=1:\n\t        #    N=int(self.args.exp.audio_len*S)\n\t        #    self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\t        # Setup augmentation.\n\t        #augment_pipe = dnnlib.util.construct_class_by_name(**augment_kwargs) if augment_kwargs is not None else None # training.augment.AugmentPipe\n\t        #print model summary\n\t        self.total_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)\n", "        print(\"total_params: \",self.total_params/1e6, \"M\")\n\t        self.layer_list=get_layers(self.network) #used for logging feature statistics\n\t        self.network_wrapped=Inspect(self.network, self.layer_list) #used for logging feature statistics\n\t        self.ema = copy.deepcopy(self.network).eval().requires_grad_(False)\n\t        #resume from checkpoint\n\t        self.latest_checkpoint=None\n\t        resuming=False\n\t        if self.args.exp.resume:\n\t            if self.args.exp.resume_checkpoint != \"None\":\n\t                resuming =self.resume_from_checkpoint(checkpoint_path=self.args.exp.resume_checkpoint)\n", "            else:\n\t                resuming =self.resume_from_checkpoint()\n\t            if not resuming:\n\t                print(\"Could not resume from checkpoint\")\n\t                print(\"training from scratch\")\n\t            else:\n\t                print(\"Resuming from iteration {}\".format(self.it))\n\t        if not resuming:\n\t            self.it=0\n\t            self.latest_checkpoint=None\n", "        if self.args.logging.print_model_summary:\n\t            #if dist.get_rank() == 0:\n\t             with torch.no_grad():\n\t                 audio=torch.zeros([args.exp.batch,args.exp.audio_len], device=device)\n\t                 sigma = torch.ones([args.exp.batch], device=device).unsqueeze(-1)\n\t                 misc.print_module_summary(self.network, [audio, sigma ], max_nesting=2)\n\t        if self.args.logging.log:\n\t            #assert self.args.logging.heavy_log_interval % self.args.logging.save_interval == 0 #sorry for that, I just want to make sure that you are not wasting your time by logging too often, as the tester is only updated with the ema weights from a checkpoint\n\t            self.setup_wandb()\n\t            if self.do_test:\n", "               self.tester.setup_wandb_run(self.wandb_run)\n\t            self.setup_logging_variables()\n\t        self.profile=False\n\t        if self.args.logging.profiling.enabled:\n\t            try:\n\t                print(\"Profiling is being enabled\")\n\t                wait=self.args.logging.profiling.wait\n\t                warmup=self.args.logging.profiling.warmup\n\t                active=self.args.logging.profiling.active\n\t                repeat=self.args.logging.profiling.repeat\n", "                schedule =  torch.profiler.schedule(\n\t                wait=wait, warmup=warmup, active=active, repeat=repeat)\n\t                self.profiler = torch.profiler.profile(\n\t                schedule=schedule, on_trace_ready=tensorboard_trace_handler(\"wandb/latest-run/tbprofile\"), profile_memory=True, with_stack=False)\n\t                self.profile=True\n\t                self.profile_total_steps = (wait + warmup + active) * (1 + repeat)\n\t            except Exception as e:\n\t                print(\"Could not setup profiler\")\n\t                print(e)\n\t                self.profile=False\n", "    def setup_wandb(self):\n\t        \"\"\"\n\t        Configure wandb, open a new run and log the configuration.\n\t        \"\"\"\n\t        config=omegaconf.OmegaConf.to_container(\n\t            self.args, resolve=True, throw_on_missing=True\n\t        )\n\t        config[\"total_params\"]=self.total_params\n\t        self.wandb_run=wandb.init(project=self.args.exp.wandb.project, entity=self.args.exp.wandb.entity, config=config)\n\t        wandb.watch(self.network, log=\"all\", log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n", "        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\t    def setup_logging_variables(self):\n\t        self.sigma_bins = np.logspace(np.log10(self.args.diff_params.sigma_min), np.log10(self.args.diff_params.sigma_max), num=self.args.logging.num_sigma_bins, base=10)\n\t        #logarithmically spaced bins for the frequency logging\n\t        self.freq_bins=np.logspace(np.log2(self.args.logging.cqt.fmin), np.log2(self.args.logging.cqt.fmin*2**(self.args.logging.cqt.num_octs)), num=self.args.logging.cqt.num_octs*self.args.logging.cqt.bins_per_oct, base=2)\n\t        self.freq_bins=self.freq_bins.astype(int)\n\t    def load_state_dict(self, state_dict):\n\t        #print(state_dict)\n\t        return t_utils.load_state_dict(state_dict, network=self.network, ema=self.ema, optimizer=self.optimizer)\n\t    def load_state_dict_legacy(self, state_dict):\n", "        #print(state_dict)\n\t        print(state_dict.keys())\n\t        try:\n\t            self.it = state_dict['it']\n\t        except:\n\t            self.it=150000 #large number to mean that we loaded somethin, but it is arbitrary\n\t        try:\n\t            self.network.load_state_dict(state_dict['network'])\n\t            self.optimizer.load_state_dict(state_dict['optimizer'])\n\t            self.ema.load_state_dict(state_dict['ema'])\n", "            return True\n\t        except Exception as e:\n\t            print(\"Could not load state dict\")\n\t            print(e)\n\t            print(\"trying with strict=False\")\n\t        try:\n\t            self.network.load_state_dict(state_dict['network'], strict=False)\n\t            #we cannot load the optimizer in this setting\n\t            #self.optimizer.load_state_dict(state_dict['optimizer'], strict=False)\n\t            self.ema.load_state_dict(state_dict['ema'], strict=False)\n", "            return True\n\t        except Exception as e:\n\t            print(\"Could not load state dict\")\n\t            print(e)\n\t            print(\"training from scratch\")\n\t        try:\n\t            self.network.load_state_dict(state_dict['state_dict'])\n\t            self.ema.load_state_dict(state_dict['state_dict'])\n\t        except Exception as e:\n\t            print(\"Could not load state dict\")\n", "            print(e)\n\t            print(\"training from scratch\")\n\t            print(\"It failed 3 times!! giving up..\")\n\t            return False\n\t    def resume_from_checkpoint(self, checkpoint_path=None, checkpoint_id=None):\n\t        # Resume training from latest checkpoint available in the output director\n\t        if checkpoint_path is not None:\n\t            try:\n\t                checkpoint=torch.load(checkpoint_path, map_location=self.device)\n\t                print(checkpoint.keys())\n", "                #if it is possible, retrieve the iteration number from the checkpoint\n\t                try:\n\t                    self.it = checkpoint['it']\n\t                except:\n\t                    self.it=157007 #large number to mean that we loaded somethin, but it is arbitrary\n\t                return self.load_state_dict(checkpoint)\n\t            except Exception as e:\n\t                print(\"Could not resume from checkpoint\")\n\t                print(e)\n\t                print(\"training from scratch\")\n", "                self.it=0\n\t            try:\n\t                checkpoint=torch.load(os.path.join(self.args.model_dir,checkpoint_path), map_location=self.device)\n\t                print(checkpoint.keys())\n\t                #if it is possible, retrieve the iteration number from the checkpoint\n\t                try:\n\t                    self.it = checkpoint['it']\n\t                except:\n\t                    self.it=157007 #large number to mean that we loaded somethin, but it is arbitrary\n\t                self.network.load_state_dict(checkpoint['ema_model'])\n", "                return True\n\t            except Exception as e:\n\t                print(\"Could not resume from checkpoint\")\n\t                print(e)\n\t                print(\"training from scratch\")\n\t                self.it=0\n\t                return False\n\t        else:\n\t            try:\n\t                print(\"trying to load a project checkpoint\")\n", "                print(\"checkpoint_id\", checkpoint_id)\n\t                if checkpoint_id is None:\n\t                    # find latest checkpoint_id\n\t                    save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n\t                    save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t                    print(save_name)\n\t                    list_weights = glob(save_name)\n\t                    id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n\t                    list_ids = [int(id_regex.search(weight_path).groups()[0])\n\t                                for weight_path in list_weights]\n", "                    checkpoint_id = max(list_ids)\n\t                    print(checkpoint_id)\n\t                checkpoint = torch.load(\n\t                    f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n\t                #if it is possible, retrieve the iteration number from the checkpoint\n\t                try:\n\t                    self.it = checkpoint['it']\n\t                except:\n\t                    self.it=159000 #large number to mean that we loaded somethin, but it is arbitrary\n\t                self.load_state_dict(checkpoint)\n", "                return True\n\t            except Exception as e:\n\t                print(e)\n\t                return False\n\t    def state_dict(self):\n\t        return {\n\t            'it': self.it,\n\t            'network': self.network.state_dict(),\n\t            'optimizer': self.optimizer.state_dict(),\n\t            'ema': self.ema.state_dict(),\n", "            'args': self.args,\n\t        }\n\t    def save_checkpoint(self):\n\t        save_basename = f\"{self.args.exp.exp_name}-{self.it}.pt\"\n\t        save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t        torch.save(self.state_dict(), save_name)\n\t        print(\"saving\",save_name)\n\t        if self.args.logging.remove_last_checkpoint:\n\t            try:\n\t                os.remove(self.latest_checkpoint)\n", "                print(\"removed last checkpoint\", self.latest_checkpoint)\n\t            except:\n\t                print(\"could not remove last checkpoint\", self.latest_checkpoint)\n\t        self.latest_checkpoint=save_name\n\t    def log_feature_stats(self):\n\t        print(\"logging feature stats\")\n\t        #this is specific for the edm diff_params, be careful if changing it\n\t        x=self.get_batch()\n\t        print(x.shape, x.std(-1))\n\t        sigma=self.diff_params.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n", "        input, target, cnoise= self.diff_params.prepare_train_preconditioning(x, sigma)\n\t        estimate, feat_list=self.network_wrapped(input,cnoise) #is this too crazy memory wise?\n\t        for name, a in zip(self.layer_list, feat_list):\n\t            #log an histogram for each layer in wandb\n\t            if a is not None:\n\t                wandb.log({f\"features/{name}\": wandb.Histogram(a[0].detach().cpu().numpy())}, step=self.it)\n\t        del feat_list #I hope this frees the memory\n\t    def process_loss_for_logging(self, error: torch.Tensor, sigma: torch.Tensor):\n\t        \"\"\"\n\t        This function is used to process the loss for logging. It is used to group the losses by the values of sigma and report them using training_stats.\n", "        args:\n\t            error: the error tensor with shape [batch, audio_len]\n\t            sigma: the sigma tensor with shape [batch]\n\t        \"\"\"\n\t        #sigma values are ranged between self.args.diff_params.sigma_min and self.args.diff_params.sigma_max. We need to quantize the values of sigma into 10 logarithmically spaced bins between self.args.diff_params.sigma_min and self.args.diff_params.sigma_max\n\t        torch.nan_to_num(error) #not tested might crash\n\t        error=error.detach().cpu().numpy()\n\t        #Now I need to report the error respect to frequency. I would like to do this by using the CQT of the error and then report the error respect to both sigma and frequency\n\t        #I will use librosa to compute the CQT\n\t        #will this be too heavy? I am not sure. I will try it and see what happens\n", "        if self.it%self.args.logging.freq_cqt_logging==0: #do that only once every 50 iterations, for efficiency\n\t            cqt_res=[]\n\t            for i in range(error.shape[0]):\n\t                #running this cqt in gpu would be faster. \n\t                cqt = librosa.cqt(error[i], sr=self.args.exp.sample_rate, hop_length=self.args.logging.cqt.hop_length, fmin=self.args.logging.cqt.fmin, n_bins=self.args.logging.cqt.num_octs*self.args.logging.cqt.bins_per_oct, bins_per_octave=self.args.logging.cqt.bins_per_oct)\n\t                cqt_res.append(np.abs(cqt.mean(axis=1)))\n\t                #now I need to report the error respect to frequency\n\t        else:\n\t            cqt_res=None\n\t        for i in range(len(self.sigma_bins)):\n", "            if i == 0:\n\t                mask = sigma <= self.sigma_bins[i]\n\t            elif i == len(self.sigma_bins)-1:\n\t                mask = (sigma <= self.sigma_bins[i]) & (sigma > self.sigma_bins[i-1])\n\t            else:\n\t                mask = (sigma <= self.sigma_bins[i]) & (sigma > self.sigma_bins[i-1])\n\t            mask=mask.squeeze(-1).cpu()\n\t            if mask.sum() > 0:\n\t                #find the index of the first element of the mask\n\t                idx = np.where(mask==True)[0][0]\n", "                training_stats.report('error_sigma_'+str(self.sigma_bins[i]),error[idx].mean())\n\t                if cqt_res is not None:\n\t                    for j in range(cqt.shape[0]):\n\t                        training_stats.report('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]),cqt_res[idx][j].mean())\n\t        if cqt_res is not None:\n\t            for j in range(cqt.shape[0]):\n\t                for i in range(len(cqt_res)):\n\t                    training_stats.report('error_freq_'+str(self.freq_bins[j]),cqt_res[i][j].mean())\n\t    def get_batch(self):\n\t        #load the data batch\n", "        if self.args.dset.name == \"maestro_allyears\":\n\t            audio, fs = next(self.dset)\n\t            audio=audio.to(self.device).to(torch.float32)\n\t            print(fs, audio.shape)\n\t            #do resampling if needed\n\t            #print(\"before resample\",audio.shape, self.args.exp.resample_factor)\n\t            return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\t        else: \n\t            audio = next(self.dset)\n\t            audio=audio.to(self.device).to(torch.float32)\n", "            #do resampling if needed\n\t            #print(\"before resample\",audio.shape, self.args.exp.resample_factor)\n\t            if self.args.exp.resample_factor != 1:\n\t                #self.resample(audio)\n\t                audio=torchaudio.functional.resample(audio, self.args.exp.resample_factor, 1)\n\t            #TODO: add augmentation\n\t            return audio\n\t    def train_step(self):\n\t        # Train step\n\t        it_start_time = time.time()\n", "        #self.optimizer.zero_grad(set_to_none=True)\n\t        self.optimizer.zero_grad()\n\t        st_time=time.time()\n\t        for round_idx in range(self.args.exp.num_accumulation_rounds):\n\t            #with misc.ddp_sync(ddp, (round_idx == num_accumulation_rounds - 1)):\n\t            audio=self.get_batch()\n\t            print(audio.shape, self.args.exp.audio_len, audio.std(-1))\n\t            #print(audio.shape, self.args.exp.audio_len)\n\t            error, sigma = self.diff_params.loss_fn(self.network, audio)\n\t            loss=error.mean()\n", "            loss.backward() #TODO: take care of the loss scaling if using mixed precision\n\t            #do I want to call this at every round? It will slow down the training. I will try it and see what happens\n\t            #loss.sum().mul(self.args.exp.loss_scaling).backward()\n\t            #print(loss.item())\n\t        if self.it <= self.args.exp.lr_rampup_it:\n\t            for g in self.optimizer.param_groups:\n\t                #learning rate ramp up\n\t                g['lr'] = self.args.exp.lr * min(self.it / max(self.args.exp.lr_rampup_it, 1e-8), 1)\n\t        #for param in self.network.parameters():\n\t        #    #take care of none gradients. Is that needed? \n", "        #    if param.grad is not None:\n\t        #        torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)#This is needed for the loss scaling. That is what causes the nan gradients. \n\t        if self.args.exp.use_grad_clip:\n\t            torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.args.exp.max_grad_norm)\n\t        # Update weights.\n\t        self.optimizer.step()\n\t        end_time=time.time()\n\t        if self.args.logging.log:\n\t            self.process_loss_for_logging(error, sigma)\n\t        it_end_time = time.time()\n", "        print(\"it :\",self.it, \"time:, \",end_time-st_time, \"total_time: \",training_stats.report('it_time',it_end_time-it_start_time) ,\"loss: \", training_stats.report('loss', loss.item())) #TODO: take care of the logging\n\t    def update_ema(self):\n\t        \"\"\"Update exponential moving average of self.network weights.\"\"\"\n\t        ema_rampup = self.args.exp.ema_rampup  #ema_rampup should be set to 10000 in the config file\n\t        ema_rate=self.args.exp.ema_rate #ema_rate should be set to 0.9999 in the config file\n\t        t = self.it * self.args.exp.batch\n\t        with torch.no_grad():\n\t            if t < ema_rampup:\n\t                s = np.clip(t / ema_rampup, 0.0, ema_rate)\n\t                for dst, src in zip(self.ema.parameters(), self.network.parameters()):\n", "                    dst.copy_(dst * s + src * (1-s))\n\t            else:\n\t                for dst, src in zip(self.ema.parameters(), self.network.parameters()):\n\t                    dst.copy_(dst * ema_rate + src * (1-ema_rate))\n\t    def easy_logging(self):\n\t        \"\"\"\n\t         Do the simplest logging here. This will be called every 1000 iterations or so\n\t        I will use the training_stats.report function for this, and aim to report the means and stds of the losses in wandb\n\t        \"\"\"\n\t        training_stats.default_collector.update()\n", "        #Is it a good idea to log the stds of the losses? I think it is not.\n\t        loss_mean=training_stats.default_collector.mean('loss')\n\t        self.wandb_run.log({'loss':loss_mean}, step=self.it)\n\t        loss_std=training_stats.default_collector.std('loss')\n\t        self.wandb_run.log({'loss_std':loss_std}, step=self.it)\n\t        it_time_mean=training_stats.default_collector.mean('it_time')\n\t        self.wandb_run.log({'it_time_mean':it_time_mean}, step=self.it)\n\t        it_time_std=training_stats.default_collector.std('it_time')\n\t        self.wandb_run.log({'it_time_std':it_time_std}, step=self.it)\n\t        #here reporting the error respect to sigma. I should make a fancier plot too, with mean and std\n", "        sigma_means=[]\n\t        sigma_stds=[]\n\t        for i in range(len(self.sigma_bins)):\n\t            a=training_stats.default_collector.mean('error_sigma_'+str(self.sigma_bins[i]))\n\t            sigma_means.append(a)\n\t            self.wandb_run.log({'error_sigma_'+str(self.sigma_bins[i]):a}, step=self.it)\n\t            a=training_stats.default_collector.std('error_sigma_'+str(self.sigma_bins[i]))\n\t            sigma_stds.append(a)\n\t        figure=utils_logging.plot_loss_by_sigma(sigma_means,sigma_stds, self.sigma_bins)\n\t        wandb.log({\"loss_dependent_on_sigma\": figure}, step=self.it, commit=True)\n", "        #TODO log here the losses at different noise levels. I don't know if these should be heavy\n\t        #TODO also log here the losses at different frequencies if we are reporting them. same as above\n\t    def heavy_logging(self):\n\t        \"\"\"\n\t        Do the heavy logging here. This will be called every 10000 iterations or so\n\t        \"\"\"\n\t        freq_means=[]\n\t        freq_stds=[]\n\t        freq_sigma_means=[]\n\t        freq_sigma_stds=[]\n", "        for j in range(len(self.freq_bins)):\n\t            a=training_stats.default_collector.mean('error_freq_'+str(self.freq_bins[j]))\n\t            freq_means.append(a)\n\t            self.wandb_run.log({'error_freq_'+str(self.freq_bins[j]):a}, step=self.it)\n\t            a=training_stats.default_collector.std('error_freq_'+str(self.freq_bins[j]))\n\t            freq_stds.append(a)\n\t            #I need to take care of this in some other way that is easier to visualize\n\t            omeans=[]\n\t            ostds=[]\n\t            for i in range(len(self.sigma_bins)):\n", "                a=training_stats.default_collector.mean('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]))\n\t                omeans.append(a)\n\t                a=training_stats.default_collector.std('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]))\n\t                ostds.append(a)\n\t                #wandb.log({'error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]):a}, step=self.it)\n\t                #this logging will not be so handy. I create a figure using plotly to nicely visualize the results\n\t            freq_sigma_means.append(omeans)\n\t            freq_sigma_stds.append(ostds)\n\t        figure=utils_logging.plot_loss_by_sigma(freq_means,freq_stds, self.freq_bins)\n\t        wandb.log({\"loss_dependent_on_freq\": figure}, step=self.it)\n", "        figure=utils_logging.plot_loss_by_sigma_and_freq(freq_sigma_means,freq_sigma_stds, self.sigma_bins, self.freq_bins)#TODO!!!\n\t        wandb.log({\"loss_dependent_on_freq_and_sigma\": figure}, step=self.it)\n\t        if self.do_test:\n\t            if self.latest_checkpoint is not None:\n\t                self.tester.load_checkpoint(self.latest_checkpoint)\n\t            preds=self.tester.sample_unconditional()\n\t            if \"inpainting\" in self.args.tester.modes:\n\t                preds=self.tester.test_inpainting()\n\t            if \"bwe\" in self.args.tester.modes:\n\t                preds=self.tester.test_bwe()\n", "            #self.log_audio(preds, \"unconditional_sampling\")\n\t        #TODO: call the unconditional generation function and log the audio samples\n\t    def log_audio(self,x, name):\n\t        string=name+\"_\"+self.args.tester.name\n\t        audio_path=utils_logging.write_audio_file(x,self.args.exp.sample_rate, string,path=self.args.model_dir)\n\t        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it)\n\t        #TODO: log spectrogram of the audio file to wandb\n\t        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(x, self.args.logging.stft)\n\t        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it)\n\t    def conditional_demos(self):\n", "        \"\"\"\n\t        Do the conditional demos here. This will be called every 10000 iterations or so\n\t        \"\"\"\n\t        #TODO: call the conditional generation function and log the audio samples\n\t        pass\n\t    def training_loop(self):\n\t        # Initialize.\n\t        #ddp = torch.nn.parallel.DistributedDataParallel(net, device_ids=[device], broadcast_buffers=False)\n\t        while True:\n\t            # Accumulate gradients.\n", "            self.train_step()\n\t            self.update_ema()\n\t            if self.profile and self.args.logging.log:\n\t                print(self.profile, self.profile_total_steps, self.it)\n\t                if self.it<self.profile_total_steps:\n\t                    self.profiler.step()\n\t                elif self.it==self.profile_total_steps +1:\n\t                    #log trace as an artifact in wandb\n\t                    profile_art = wandb.Artifact(f\"trace-{wandb.run.id}\", type=\"profile\")\n\t                    profile_art.add_file(glob(\"wandb/latest-run/tbprofile/*.pt.trace.json\")[0], \"trace.pt.trace.json\")\n", "                    wandb.log_artifact(profile_art)\n\t                    print(\"proiling done\")\n\t                elif self.it>self.profile_total_steps +1:\n\t                    self.profile=False\n\t            if self.it>0 and self.it%self.args.logging.save_interval==0 and self.args.logging.save_model:\n\t                #self.save_snapshot() #are the snapshots necessary? I think they are not.\n\t                self.save_checkpoint()\n\t            if self.it>0 and self.it%self.args.logging.log_feature_stats_interval==0 and self.args.logging.log_feature_stats:\n\t                self.log_feature_stats()\n\t            if self.it>0 and self.it%self.args.logging.heavy_log_interval==0 and self.args.logging.log:\n", "                self.heavy_logging()\n\t                #self.conditional_demos()\n\t            if self.it>0 and self.it%self.args.logging.log_interval==0 and self.args.logging.log:\n\t                self.easy_logging()\n\t            # Update state.\n\t            self.it += 1\n\t    #----------------------------------------------------------------------------\n"]}
{"filename": "networks/cqtdiff+.py", "chunked_list": ["import torch.nn as nn\n\timport numpy as np\n\timport torch.nn.functional as F\n\timport math as m\n\timport torch\n\t#import torchaudio\n\ttorch.pi = torch.acos(torch.zeros(1)).item() * 2 # which is 3.1415927410125732\n\tfrom cqt_nsgt_pytorch import CQT_nsgt\n\timport torchaudio\n\timport einops\n", "import math\n\t\"\"\"\n\tAs similar as possible to the original CQTdiff architecture, but using the octave-base representation of the CQT\n\tThis should be more memory efficient, and also more efficient in terms of computation, specially when using higher sampling rates.\n\tI am expecting similar performance to the original CQTdiff architecture, but faster. \n\tPerhaps the fact that I am using powers of 2 for the time sizes is critical for transient reconstruction. I should thest CQT matrix model with powers of 2, this requires modifying the CQT_nsgt_pytorch.py file.\n\t\"\"\"\n\tdef weight_init(shape, mode, fan_in, fan_out):\n\t    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n\t    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n", "    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n\t    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n\t    raise ValueError(f'Invalid init mode \"{mode}\"')\n\tclass Linear(torch.nn.Module):\n\t    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n\t        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n", "        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n\t    def forward(self, x):\n\t        x = x @ self.weight.to(x.dtype).t()\n\t        if self.bias is not None:\n\t            x = x.add_(self.bias.to(x.dtype))\n\t        return x\n\tclass Conv1d(torch.nn.Module):\n\t    def __init__(self,\n\t        in_channels, out_channels, kernel=1, bias=False, dilation=1,\n\t        init_mode='kaiming_normal', init_weight=1, init_bias=0,\n", "    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.dilation = dilation\n\t        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel, fan_out=out_channels*kernel)\n\t        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel], **init_kwargs) * init_weight) \n\t        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if bias else None\n\t    def forward(self, x):\n\t        w = self.weight.to(x.dtype) if self.weight is not None else None\n", "        b = self.bias.to(x.dtype) if self.bias is not None else None\n\t        w_pad = w.shape[-1] // 2 if w is not None else 0\n\t        #f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n\t        #print(x.shape, w.shape)\n\t        if w is not None:\n\t                x = torch.nn.functional.conv1d(x, w, padding=\"same\", dilation=self.dilation)\n\t        if b is not None:\n\t            x = x.add_(b.reshape(1, -1, 1))\n\t        return x\n\tclass Conv2d(torch.nn.Module):\n", "    def __init__(self,\n\t        in_channels, out_channels, kernel=(1,1), bias=False, dilation=1,\n\t        init_mode='kaiming_normal', init_weight=1, init_bias=0,\n\t    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.dilation = dilation\n\t        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel[0]*kernel[1], fan_out=out_channels*kernel[0]*kernel[1])\n\t        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel[0], kernel[1]], **init_kwargs) * init_weight) \n", "        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if bias else None\n\t    def forward(self, x):\n\t        w = self.weight.to(x.dtype) if self.weight is not None else None\n\t        b = self.bias.to(x.dtype) if self.bias is not None else None\n\t        w_pad = w.shape[-1] // 2 if w is not None else 0\n\t        #f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n\t        if w is not None:\n\t                x = torch.nn.functional.conv2d(x, w, padding=\"same\", dilation=self.dilation)\n\t        if b is not None:\n\t            x = x.add_(b.reshape(1, -1, 1, 1))\n", "        return x\n\tclass LayerScale(nn.Module):\n\t    \"\"\"Layer scale from [Touvron et al 2021] (https://arxiv.org/pdf/2103.17239.pdf).\n\t    This rescales diagonaly residual outputs close to 0 initially, then learnt.\n\t    \"\"\"\n\t    def __init__(self, channels: int, init: float = 1e-4, channel_last=True):\n\t        \"\"\"\n\t        channel_last = False corresponds to (B, C, T) tensors\n\t        channel_last = True corresponds to (T, B, C) tensors\n\t        \"\"\"\n", "        super().__init__()\n\t        self.channel_last = channel_last\n\t        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n\t        self.scale.data[:] = init\n\t    def forward(self, x):\n\t        if self.channel_last:\n\t            return self.scale * x\n\t        else:\n\t            return self.scale[:, None] * x\n\tclass BiasFreeLayerNorm(nn.Module):\n", "    def __init__(self, num_features, eps=1e-7):\n\t        super(BiasFreeLayerNorm, self).__init__()\n\t        self.gamma = nn.Parameter(torch.ones(1,1,num_features))\n\t        #self.beta = nn.Parameter(torch.zeros(1,num_features,1,1))\n\t        #self.beta = torch.zeros(1,num_features,1,1)\n\t        self.eps = eps\n\t    def forward(self, x):\n\t        N, T, C = x.size()\n\t        #x = x.view(N, self.num_groups ,-1,H,W)\n\t        #x=einops.rearrange(x, 'n t c -> n (t c)')\n", "        #mean = x.mean(-1, keepdim=True)\n\t        #var = x.var(-1, keepdim=True)\n\t        std=x.std(-1, keepdim=True) #reduce over channels and time\n\t        #var = x.var(-1, keepdim=True)\n\t        ## normalize\n\t        x = (x) / (std+self.eps)\n\t        # normalize\n\t        #x=einops.rearrange(x, 'n (t c) -> n t c', t=T)\n\t        #x = x.view(N,C,H,W)\n\t        return x * self.gamma\n", "class BiasFreeGroupNorm(nn.Module):\n\t    def __init__(self, num_features, num_groups=32, eps=1e-7):\n\t        super(BiasFreeGroupNorm, self).__init__()\n\t        self.gamma = nn.Parameter(torch.ones(1,num_features,1,1))\n\t        #self.beta = nn.Parameter(torch.zeros(1,num_features,1,1))\n\t        #self.beta = torch.zeros(1,num_features,1,1)\n\t        self.num_groups = num_groups\n\t        self.eps = eps\n\t    def forward(self, x):\n\t        N, C, F, T = x.size()\n", "        #x = x.view(N, self.num_groups ,-1,H,W)\n\t        gc=C//self.num_groups\n\t        x=einops.rearrange(x, 'n (g gc) f t -> n g (gc f t)', g=self.num_groups, gc=gc)\n\t        #mean = x.mean(-1, keepdim=True)\n\t        #var = x.var(-1, keepdim=True)\n\t        std=x.std(-1, keepdim=True) #reduce over channels and time\n\t        #var = x.var(-1, keepdim=True)\n\t        ## normalize\n\t        x = (x) / (std+self.eps)\n\t        # normalize\n", "        x=einops.rearrange(x, 'n g (gc f t) -> n (g gc) f t', g=self.num_groups, gc=gc, f=F, t=T)\n\t        #x = x.view(N,C,H,W)\n\t        return x * self.gamma\n\tclass RFF_MLP_Block(nn.Module):\n\t    \"\"\"\n\t        Encoder of the noise level embedding\n\t        Consists of:\n\t            -Random Fourier Feature embedding\n\t            -MLP\n\t    \"\"\"\n", "    def __init__(self, emb_dim=512, rff_dim=32, init=None):\n\t        super().__init__()\n\t        self.RFF_freq = nn.Parameter(\n\t            16 * torch.randn([1, rff_dim]), requires_grad=False)\n\t        self.MLP = nn.ModuleList([\n\t            Linear(2*rff_dim, 128, **init),\n\t            Linear(128, 256, **init),\n\t            Linear(256, emb_dim, **init),\n\t        ])\n\t    def forward(self, sigma):\n", "        \"\"\"\n\t        Arguments:\n\t          sigma:\n\t              (shape: [B, 1], dtype: float32)\n\t        Returns:\n\t          x: embedding of sigma\n\t              (shape: [B, 512], dtype: float32)\n\t        \"\"\"\n\t        x = self._build_RFF_embedding(sigma)\n\t        for layer in self.MLP:\n", "            x = F.relu(layer(x))\n\t        return x\n\t    def _build_RFF_embedding(self, sigma):\n\t        \"\"\"\n\t        Arguments:\n\t          sigma:\n\t              (shape: [B, 1], dtype: float32)\n\t        Returns:\n\t          table:\n\t              (shape: [B, 64], dtype: float32)\n", "        \"\"\"\n\t        freqs = self.RFF_freq\n\t        table = 2 * np.pi * sigma * freqs\n\t        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n\t        return table\n\tclass AddFreqEncodingRFF(nn.Module):\n\t    '''\n\t    [B, T, F, 2] => [B, T, F, 12]  \n\t    Generates frequency positional embeddings and concatenates them as 10 extra channels\n\t    This function is optimized for F=1025\n", "    '''\n\t    def __init__(self, f_dim, N):\n\t        super(AddFreqEncodingRFF, self).__init__()\n\t        self.N=N\n\t        self.RFF_freq = nn.Parameter(\n\t            16 * torch.randn([1, N]), requires_grad=False)\n\t        self.f_dim=f_dim #f_dim is fixed\n\t        embeddings=self.build_RFF_embedding()\n\t        self.embeddings=nn.Parameter(embeddings, requires_grad=False) \n\t    def build_RFF_embedding(self):\n", "        \"\"\"\n\t        Returns:\n\t          table:\n\t              (shape: [C,F], dtype: float32)\n\t        \"\"\"\n\t        freqs = self.RFF_freq\n\t        #freqs = freqs.to(device=torch.device(\"cuda\"))\n\t        freqs=freqs.unsqueeze(-1) # [1, 32, 1]\n\t        self.n=torch.arange(start=0,end=self.f_dim)\n\t        self.n=self.n.unsqueeze(0).unsqueeze(0)  #[1,1,F]\n", "        table = 2 * np.pi * self.n * freqs\n\t        #print(freqs.shape, x.shape, table.shape)\n\t        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1) #[1,32,F]\n\t        return table\n\t    def forward(self, input_tensor):\n\t        #print(input_tensor.shape)\n\t        batch_size_tensor = input_tensor.shape[0]  # get batch size\n\t        time_dim = input_tensor.shape[-1]  # get time dimension\n\t        fembeddings_2 = torch.broadcast_to(self.embeddings, [batch_size_tensor, time_dim,self.N*2, self.f_dim])\n\t        fembeddings_2=fembeddings_2.permute(0,2,3,1)\n", "        #print(input_tensor.shape, fembeddings_2.shape)\n\t        return torch.cat((input_tensor,fembeddings_2),1)  \n\tclass RelativePositionBias(nn.Module):\n\t    def __init__(self, num_buckets: int, max_distance: int, num_heads: int):\n\t        super().__init__()\n\t        self.num_buckets = num_buckets\n\t        self.max_distance = max_distance\n\t        self.num_heads = num_heads\n\t        self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n\t    @staticmethod\n", "    def _relative_position_bucket(\n\t        relative_position, num_buckets: int, max_distance: int\n\t    ):\n\t        num_buckets //= 2\n\t        ret = (relative_position >= 0).to(torch.long) * num_buckets\n\t        n = torch.abs(relative_position)\n\t        max_exact = num_buckets // 2\n\t        is_small = n < max_exact\n\t        val_if_large = (\n\t            max_exact\n", "            + (\n\t                torch.log(n.float() / max_exact)\n\t                / math.log(max_distance / max_exact)\n\t                * (num_buckets - max_exact)\n\t            ).long()\n\t        )\n\t        val_if_large = torch.min(\n\t            val_if_large, torch.full_like(val_if_large, num_buckets - 1)\n\t        )\n\t        ret += torch.where(is_small, n, val_if_large)\n", "        return ret\n\t    def forward(self, num_queries: int, num_keys: int):\n\t        i, j, device = num_queries, num_keys, self.relative_attention_bias.weight.device\n\t        q_pos = torch.arange(j - i, j, dtype=torch.long, device=device)\n\t        k_pos = torch.arange(j, dtype=torch.long, device=device)\n\t        rel_pos = einops.rearrange(k_pos, \"j -> 1 j\") - einops.rearrange(q_pos, \"i -> i 1\")\n\t        relative_position_bucket = self._relative_position_bucket(\n\t            rel_pos, num_buckets=self.num_buckets, max_distance=self.max_distance\n\t        )\n\t        bias = self.relative_attention_bias(relative_position_bucket)\n", "        bias = einops.rearrange(bias, \"m n h -> 1 h m n\")\n\t        return bias\n\tclass TimeAttentionBlock(nn.Module):\n\t    def __init__(self, Nin,attention_dict, init, init_zero, Fdim) -> None:\n\t        super().__init__()\n\t        #NA=attention_dict.N\n\t        self.attention_dict=attention_dict\n\t        self.Fdim=Fdim\n\t        N=attention_dict.num_heads*Fdim \n\t        self.qk = Conv1d(N, N*2, bias=self.attention_dict.bias_qkv, **init )\n", "        self.proj_in=Conv2d(Nin, attention_dict.num_heads, (1,1), bias=False, **init)\n\t        self.proj_out=Conv2d(attention_dict.num_heads, Nin, (1,1), bias=False, **init)\n\t        #not sure if a bias is a good idea here\n\t        #self.v = Conv2d(N, N*2, (1,1), bias=False,**init )\n\t        #I think that as long as the main signal path layers are bias free, we should be safe from artifacts\n\t        #self.proj = Conv1d(NA, NA, 1, bias=False, **init)\n\t        self.scale=(N/self.attention_dict.num_heads)**-0.5\n\t        self.use_rel_pos = self.attention_dict.use_rel_pos\n\t        if self.use_rel_pos:\n\t            self.rel_pos = RelativePositionBias(\n", "                num_buckets=attention_dict.rel_pos_num_buckets,\n\t                max_distance=attention_dict.rel_pos_max_distance,\n\t                num_heads=attention_dict.num_heads,\n\t            )\n\t    def forward(self, x):\n\t        #shape of x is [batch, C,F, T]\n\t        #we need shape: [batch, heads, T, D]\n\t        #with heands on different (original) channels\n\t        #print(x.shape, self.Fdim)\n\t        x=self.proj_in(x) #reduce the C dimensionality\n", "        #print(x.shape, self.Fdim)\n\t        #normalize everyting (easy)\n\t        #split into heads\n\t        x=einops.rearrange(x, \"b h f t -> b (h f) t\")\n\t        v=einops.rearrange(x,\"b (h f) t -> b h t f\", f=self.Fdim) #identity layer for the values\n\t        qk=self.qk(x) #linear layer\n\t        #for now, f are features (all merged) but still represents frequency\n\t        qk=einops.rearrange(qk, \"b (h d) t -> b h t d\", h=self.attention_dict.num_heads)\n\t        q,k=qk.chunk(2,dim=-1)\n\t        #print(\"qk\",q.shape, k.shape)\n", "        sim = torch.einsum(\"... n d, ... m d -> ... n m\", q, k)\n\t        #print(\"sim\",sim.shape)\n\t        sim = (sim + self.rel_pos(*sim.shape[-2:])) if self.use_rel_pos else sim\n\t        #print(\"sim\",sim.shape)\n\t        sim = sim * self.scale\n\t        # Get attention matrix with softmax\n\t        attn = sim.softmax(dim=-1)\n\t        # Compute values\n\t        #print(\"attn\",attn.shape, v.shape)\n\t        out = torch.einsum(\"... n m, ... m d -> ... n d\", attn, v)\n", "        #print(\"out\",out.shape)\n\t        out = einops.rearrange(out, \"b h t f -> b h f t\", f=self.Fdim)\n\t        #out = einops.rearrange(out, \"b (h f) t -> b h f t\", f=self.Fdim)\n\t        #reverse step\n\t        out=self.proj_out(out)\n\t        return out\n\tclass ResnetBlock(nn.Module):\n\t    def __init__(\n\t        self,\n\t        dim,\n", "        dim_out,\n\t        use_norm=True,\n\t        num_dils = 6,\n\t        bias=False,\n\t        kernel_size=(5,3),\n\t        emb_dim=512,\n\t        proj_place='before', #using 'after' in the decoder out blocks\n\t        init=None,\n\t        init_zero=None,\n\t        attention_dict=None,\n", "        Fdim=128, #number of frequency bins\n\t    ):\n\t        super().__init__()\n\t        self.bias=bias\n\t        self.use_norm=use_norm\n\t        self.num_dils=num_dils\n\t        self.proj_place=proj_place\n\t        self.Fdim=Fdim\n\t        if self.proj_place=='before':\n\t            #dim_out is the block dimension\n", "            N=dim_out\n\t        else:\n\t            #dim in is the block dimension\n\t            N=dim\n\t            self.proj_out = Conv2d(N, dim_out,   bias=bias, **init) if N!=dim_out else nn.Identity() #linear projection\n\t        self.res_conv = Conv2d(dim, dim_out, bias=bias, **init) if dim!= dim_out else nn.Identity() #linear projection\n\t        self.proj_in = Conv2d(dim, N,   bias=bias, **init) if dim!=N else nn.Identity()#linear projection\n\t        self.H=nn.ModuleList()\n\t        self.affine=nn.ModuleList()\n\t        self.gate=nn.ModuleList()\n", "        if self.use_norm:\n\t            self.norm=nn.ModuleList()\n\t        for i in range(self.num_dils):\n\t            if self.use_norm:\n\t                self.norm.append(BiasFreeGroupNorm(N,8))\n\t            self.affine.append(Linear(emb_dim, N, **init))\n\t            self.gate.append(Linear(emb_dim, N, **init_zero))\n\t            #self.H.append(Gated_residual_layer(dim_out, (5,3), (2**i,1), bias=bias)) #sometimes I changed this 1,5 to 3,5. be careful!!! (in exp 80 as far as I remember)\n\t            self.H.append(Conv2d(N,N,    \n\t                                    kernel=kernel_size,\n", "                                    dilation=(2**i,1),\n\t                                    bias=bias, **init)) #freq convolution (dilated) \n\t        self.attention_dict=attention_dict\n\t        if self.attention_dict is not None:\n\t            #NA=self.attention_dict.N\n\t            self.norm2=BiasFreeGroupNorm(N,8)\n\t            self.affine2=Linear(emb_dim, N, **init)\n\t            self.gate2=Linear(emb_dim, N, **init_zero)\n\t            #self.norm2 = BiasFreeGroupNorm(N,8)\n\t            #self.proj_attn_in = Conv1d(N*Fdim, NA,   bias=bias, **init) if (N*Fdim)!=NA else nn.Identity()#linear projection\n", "            #self.proj_attn_out = Conv1d(NA, N*Fdim,   bias=bias, **init_zero) if NA!=(N*Fdim) else nn.Identity() #linear projection\n\t            ##the attention is applied time-wise, since channels times frequency is too much, we need to reduce the dimensionality using a linear projection\n\t            self.attn_block=TimeAttentionBlock(N,self.attention_dict, init,init_zero, self.Fdim)\n\t    def forward(self, input_x, sigma):\n\t        x=input_x\n\t        x=self.proj_in(x)\n\t        if self.attention_dict is not None:\n\t            i_x=x\n\t            gamma=self.affine2(sigma)\n\t            scale=self.gate2(sigma)\n", "            x=self.norm2(x)\n\t            x=x*(gamma.unsqueeze(2).unsqueeze(3)+1) #no bias\n\t            x=self.attn_block(x)*scale.unsqueeze(2).unsqueeze(3)\n\t            #x=(x+i_x)\n\t            x=(x+i_x)/(2**0.5)\n\t        for norm, affine, gate, conv in zip(self.norm, self.affine, self.gate, self.H):\n\t            x0=x\n\t            if self.use_norm:\n\t                x=norm(x)\n\t            gamma =affine(sigma)\n", "            scale=gate(sigma)\n\t            x=x*(gamma.unsqueeze(2).unsqueeze(3)+1) #no bias\n\t            x=(x0+conv(F.gelu(x))*scale.unsqueeze(2).unsqueeze(3))/(2**0.5) \n\t            #x=(x0+conv(F.gelu(x))*scale.unsqueeze(2).unsqueeze(3))\n\t        #one residual connection here after the dilated convolutions\n\t        if self.proj_place=='after':\n\t            x=self.proj_out(x)\n\t        x=(x + self.res_conv(input_x))/(2**0.5)\n\t        return x\n\tclass AttentionOp(torch.autograd.Function):\n", "    def forward(ctx, q, k):\n\t        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n\t        ctx.save_for_backward(q, k, w)\n\t        return w\n\t    def backward(ctx, dw):\n\t        q, k, w = ctx.saved_tensors\n\t        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n\t        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n\t        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n\t        return dq, dk\n", "_kernels = {\n\t    'linear':\n\t        [1 / 8, 3 / 8, 3 / 8, 1 / 8],\n\t    'cubic': \n\t        [-0.01171875, -0.03515625, 0.11328125, 0.43359375,\n\t        0.43359375, 0.11328125, -0.03515625, -0.01171875],\n\t    'lanczos3': \n\t        [0.003689131001010537, 0.015056144446134567, -0.03399861603975296,\n\t        -0.066637322306633, 0.13550527393817902, 0.44638532400131226,\n\t        0.44638532400131226, 0.13550527393817902, -0.066637322306633,\n", "        -0.03399861603975296, 0.015056144446134567, 0.003689131001010537]\n\t}\n\tclass UpDownResample(nn.Module):\n\t    def __init__(self,\n\t        up=False, \n\t        down=False,\n\t        mode_resample=\"T\", #T for time, F for freq, TF for both\n\t        resample_filter='cubic', \n\t        pad_mode='reflect'\n\t        ):\n", "        super().__init__()\n\t        assert not (up and down) #you cannot upsample and downsample at the same time\n\t        assert up or down #you must upsample or downsample\n\t        self.down=down\n\t        self.up=up\n\t        if up or down:\n\t            #upsample block\n\t            self.pad_mode = pad_mode #I think reflect is a goof choice for padding\n\t            self.mode_resample=mode_resample\n\t            if mode_resample==\"T\":\n", "                kernel_1d = torch.tensor(_kernels[resample_filter], dtype=torch.float32)\n\t            elif mode_resample==\"F\":\n\t                #kerel shouuld be the same\n\t                kernel_1d = torch.tensor(_kernels[resample_filter], dtype=torch.float32)\n\t            else:\n\t                raise NotImplementedError(\"Only time upsampling is implemented\")\n\t                #TODO implement freq upsampling and downsampling\n\t            self.pad = kernel_1d.shape[0] // 2 - 1\n\t            self.register_buffer('kernel', kernel_1d)\n\t    def forward(self, x):\n", "        shapeorig=x.shape\n\t        #x=x.view(x.shape[0],-1,x.shape[-1])\n\t        x=x.view(-1,x.shape[-2],x.shape[-1]) #I have the feeling the reshape makes everything consume too much memory. There is no need to have the channel dimension different than 1. I leave it like this because otherwise it requires a contiguous() call, but I should check if the memory gain / speed, would be significant.\n\t        if self.mode_resample==\"F\":\n\t            x=x.permute(0,2,1)#call contiguous() here?\n\t        #print(\"after view\",x.shape)\n\t        if self.down:\n\t            x = F.pad(x, (self.pad,) * 2, self.pad_mode)\n\t        elif self.up:\n\t            x = F.pad(x, ((self.pad + 1) // 2,) * 2, self.pad_mode)\n", "        #print(\"after pad\",x.shape)\n\t        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0]])\n\t        #print(\"weight\",weight.shape)\n\t        indices = torch.arange(x.shape[1], device=x.device)\n\t        #print(\"indices\",indices.shape)\n\t        #weight = self.kernel.to(x.device).unsqueeze(0).unsqueeze(0).expand(x.shape[1], x.shape[1], -1)\n\t        #print(\"weight\",weight.shape)\n\t        weight[indices, indices] = self.kernel.to(weight)\n\t        if self.down:\n\t            x_out= F.conv1d(x, weight, stride=2)\n", "        elif self.up:\n\t            x_out =F.conv_transpose1d(x, weight, stride=2, padding=self.pad * 2 + 1)\n\t        if self.mode_resample==\"F\":\n\t            x_out=x_out.permute(0,2,1).contiguous()\n\t            return x_out.view(shapeorig[0],-1,x_out.shape[-2], shapeorig[-1])\n\t        else:\n\t            return x_out.view(shapeorig[0],-1,shapeorig[2], x_out.shape[-1])\n\tclass Unet_CQT_oct_with_attention(nn.Module):\n\t    \"\"\"\n\t        Main U-Net model based on the CQT\n", "    \"\"\"\n\t    def __init__(self, args, device):\n\t        \"\"\"\n\t        Args:\n\t            args (dictionary): hydra dictionary\n\t            device: torch device (\"cuda\" or \"cpu\")\n\t        \"\"\"\n\t        super(Unet_CQT_oct_with_attention, self).__init__()\n\t        self.args=args\n\t        self.depth=args.network.cqt.num_octs\n", "        #self.depth=args.network.inner_depth+self.args.network.cqt.num_octs\n\t        #assert self.depth==args.network.depth, \"The depth of the network should be the sum of the inner depth and the number of octaves\" #make sure we are aware of the depth of the network\n\t        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3)) #same as ADM, according to edm implementation\n\t        init_zero = dict(init_mode='kaiming_uniform', init_weight=1e-7) #I think it is safer to initialize the last layer with a small weight, rather than zero. Breaking symmetry and all that. \n\t        self.emb_dim=args.network.emb_dim\n\t        self.embedding = RFF_MLP_Block(emb_dim=args.network.emb_dim, init=init)\n\t        self.use_norm=args.network.use_norm\n\t        #fmax=self.args.exp.sample_rate/2\n\t        #self.fmin=fmax/(2**self.args.cqt.numocts)\n\t        self.fbins=int(self.args.network.cqt.bins_per_oct*self.args.network.cqt.num_octs) \n", "        self.device=device\n\t        self.bins_per_oct=self.args.network.cqt.bins_per_oct\n\t        self.num_octs=self.args.network.cqt.num_octs\n\t        #self.CQTransform=CQT_nsgt(self.args.network.cqt.num_octs,self.args.network.cqt.bins_per_oct, \"oct\",  self.args.exp.sample_rate, self.args.exp.audio_len, device=self.device)\n\t        if self.args.network.cqt.window==\"kaiser\":\n\t            win=(\"kaiser\",self.args.network.cqt.beta)\n\t        else:\n\t            win=self.args.network.cqt.window\n\t        self.CQTransform=CQT_nsgt(self.args.network.cqt.num_octs, self.args.network.cqt.bins_per_oct, mode=\"oct\",window=win,fs=self.args.exp.sample_rate, audio_len=self.args.exp.audio_len, dtype=torch.float32, device=self.device)\n\t        self.f_dim=self.fbins #assuming we have thrown away the DC component and the Nyquist frequency\n", "        self.use_fencoding=self.args.network.use_fencoding\n\t        if self.use_fencoding:\n\t            N_freq_encoding=32\n\t            self.freq_encodings=nn.ModuleList([])\n\t            for i in range(self.num_octs):\n\t                self.freq_encodings.append(AddFreqEncodingRFF(self.bins_per_oct,N_freq_encoding))\n\t            Nin=2*N_freq_encoding+2\n\t        else:\n\t            Nin=2\n\t        #Encoder\n", "        self.Ns= self.args.network.Ns\n\t        self.Ss= self.args.network.Ss\n\t        self.num_dils= self.args.network.num_dils #intuition: less dilations for the first layers and more for the deeper layers\n\t        #self.inner_num_dils=self.args.network.inner_num_dils\n\t        self.attention_dict=self.args.network.attention_dict\n\t        #self.attention_Ns=self.args.network.attention_Ns\n\t        self.downsamplerT=UpDownResample(down=True, mode_resample=\"T\")\n\t        #self.downsamplerF=UpDownResample(down=True, mode_resample=\"F\")\n\t        self.upsamplerT=UpDownResample(up=True, mode_resample=\"T\")\n\t        #self.upsamplerF=UpDownResample(up=True, mode_resample=\"F\")\n", "        self.downs=nn.ModuleList([])\n\t        self.middle=nn.ModuleList([])\n\t        self.ups=nn.ModuleList([])\n\t        self.attention_layers=self.args.network.attention_layers\n\t        #sth like [0,0,0,0,0,0,1,1]\n\t        for i in range(self.num_octs):\n\t            if i==0:\n\t                dim_in=self.Ns[i]\n\t                dim_out=self.Ns[i]\n\t            else:\n", "                dim_in=self.Ns[i-1]\n\t                dim_out=self.Ns[i]\n\t            if self.attention_layers[i]:\n\t                print(\"Attention layer at (down) octave {}\".format(i))\n\t                attn_dict=self.attention_dict\n\t                #attn_dict.N=self.attention_Ns[i]\n\t                #assert attn_dict.N > 0\n\t            else:\n\t                attn_dict=None\n\t            self.downs.append(\n", "                               nn.ModuleList([\n\t                                        ResnetBlock(Nin, dim_in, self.use_norm,num_dils=1, bias=False, kernel_size=(1,1), emb_dim=self.emb_dim, init=init, init_zero=init_zero),\n\t                                        Conv2d(2, dim_out, kernel=(5,3), bias=False, **init),\n\t                                        ResnetBlock(dim_in, dim_out, self.use_norm,num_dils=self.num_dils[i], bias=False , attention_dict=attn_dict, emb_dim=self.emb_dim, init=init, init_zero=init_zero, Fdim=(i+1)*self.bins_per_oct)\n\t                                        ]))\n\t        if self.args.network.bottleneck_type==\"res_dil_convs\":\n\t            for i in range(self.args.network.num_bottleneck_layers):\n\t                if self.attention_layers[-1]:\n\t                    attn_dict=self.attention_dict\n\t                    #attn_dict.N=self.attention_Ns[-1]\n", "                    #assert attn_dict.N > 0\n\t                else:\n\t                    attn_dict=None\n\t                self.middle.append(nn.ModuleList([\n\t                                ResnetBlock(self.Ns[-1], 2, use_norm=self.use_norm,num_dils= 1,bias=False, kernel_size=(1,1), proj_place=\"after\", emb_dim=self.emb_dim, init=init, init_zero=init_zero),\n\t                                ResnetBlock(self.Ns[-1], self.Ns[-1], self.use_norm, num_dils=self.num_dils[-1], bias=False, emb_dim=self.emb_dim,attention_dict=attn_dict, init=init, init_zero=init_zero,\n\t                                Fdim=(self.num_octs)*self.bins_per_oct)]))\n\t        else:\n\t            raise NotImplementedError(\"bottleneck type not implemented\")\n\t        #self.pyr_up_proj_first=nn.Conv2d(dim_out, 2, (5,3), padding=\"same\", padding_mode=\"zeros\", bias=False)\n", "        for i in range(self.num_octs-1,-1,-1):\n\t            if i==0:\n\t                dim_in=self.Ns[i]*2\n\t                dim_out=self.Ns[i]\n\t            else:\n\t                dim_in=self.Ns[i]*2\n\t                dim_out=self.Ns[i-1]\n\t            if self.attention_layers[i]:\n\t                print(\"Attention layer at (up) oct layer {}\".format(i))\n\t                attn_dict=self.attention_dict\n", "                #attn_dict.N=self.attention_Ns[i]\n\t                #assert attn_dict.N > 0\n\t            else:\n\t                attn_dict=None\n\t            self.ups.append(nn.ModuleList(\n\t                                        [\n\t                                        ResnetBlock(dim_out, 2, use_norm=self.use_norm,num_dils= 1,bias=False, kernel_size=(1,1), proj_place=\"after\", emb_dim=self.emb_dim, init=init, init_zero=init_zero),\n\t                                        ResnetBlock(dim_in, dim_out, use_norm=self.use_norm,num_dils= self.num_dils[i],attention_dict=attn_dict, bias=False, emb_dim=self.emb_dim, init=init, init_zero=init_zero, Fdim=(i+1)*self.bins_per_oct),\n\t                                        ]))\n\t        #self.cropconcat = CropConcatBlock()\n", "    def forward(self, inputs, sigma):\n\t        \"\"\"\n\t        Args: \n\t            inputs (Tensor):  Input signal in time-domsin, shape (B,T)\n\t            sigma (Tensor): noise levels,  shape (B,1)\n\t        Returns:\n\t            pred (Tensor): predicted signal in time-domain, shape (B,T)\n\t        \"\"\"\n\t        #apply RFF embedding+MLP of the noise level\n\t        sigma = self.embedding(sigma)\n", "        #apply CQT to the inputs\n\t        X_list =self.CQTransform.fwd(inputs.unsqueeze(1))\n\t        X_list_out=X_list\n\t        hs=[]\n\t        for i,modules in enumerate(self.downs):\n\t            #print(\"downsampler\", i)\n\t            if i <=(self.num_octs-1):\n\t                C=X_list[-1-i]#get the corresponding CQT octave\n\t                C=C.squeeze(1)\n\t                C=torch.view_as_real(C)\n", "                C=C.permute(0,3,1,2).contiguous() # call contiguous() here?\n\t                if self.use_fencoding:\n\t                    #Cfreq=self.freq_encoding(C)\n\t                    C2=self.freq_encodings[i](C) #B, C + Nfreq*2, F,T\n\t                else:\n\t                    C2=C\n\t                init_block, pyr_down_proj, ResBlock=modules\n\t                C2=init_block(C2,sigma)\n\t            else:\n\t                pyr_down_proj, ResBlock=modules\n", "            if i==0:\n\t                X=C2 #starting the main signal path\n\t                pyr=self.downsamplerT(C) #starting the auxiliary path\n\t            elif i<(self.num_octs-1):\n\t                pyr=torch.cat((self.downsamplerT(C),self.downsamplerT(pyr)),dim=2) #updating the auxiliary path\n\t                X=torch.cat((C2,X),dim=2) #updating the main signal path with the new octave\n\t            elif i==(self.num_octs-1):# last layer\n\t                #pyr=torch.cat((self.downsamplerF(C),self.downsamplerF(pyr)),dim=2) #updating the auxiliary path\n\t                pyr=torch.cat((C,pyr), dim=2) #no downsampling in the last layer\n\t                X=torch.cat((C2,X),dim=2) #updating the main signal path with the new octave\n", "            else: #last layer\n\t                pass\n\t                #pyr=pyr\n\t                #X=X\n\t            X=ResBlock(X, sigma)\n\t            hs.append(X)\n\t            #downsample the main signal path\n\t            #we do not need to downsample in the inner layer\n\t            if i<(self.num_octs-1): \n\t                X=self.downsamplerT(X)\n", "                #apply the residual connection\n\t                #X=(X+pyr_down_proj(pyr))/(2**0.5) #I'll my need to put that inside a combiner block??\n\t            else: #last layer\n\t                #no downsampling in the last layer\n\t                pass\n\t            #apply the residual connection\n\t            X=(X+pyr_down_proj(pyr))/(2**0.5) #I'll my need to put that inside a combiner block??\n\t            #print(\"encoder \", i, X.shape, X.mean().item(), X.std().item())\n\t        #middle layers\n\t        #print(\"bttleneck\")\n", "        if self.args.network.bottleneck_type==\"res_dil_convs\":\n\t            for i in range(self.args.network.num_bottleneck_layers):\n\t                OutBlock, ResBlock =self.middle[i]\n\t                X=ResBlock(X, sigma)   \n\t                Xout=OutBlock(X,sigma)\n\t        for i,modules in enumerate(self.ups):\n\t            j=len(self.ups) -i-1\n\t            #print(\"upsampler\", j)\n\t            OutBlock,  ResBlock=modules\n\t            skip=hs.pop()\n", "            X=torch.cat((X,skip),dim=1)\n\t            X=ResBlock(X, sigma)\n\t            Xout=(Xout+OutBlock(X,sigma))/(2**0.5)\n\t            if j<=(self.num_octs-1):\n\t                X= X[:,:,self.bins_per_oct::,:]\n\t                Out, Xout= Xout[:,:,0:self.bins_per_oct,:], Xout[:,:,self.bins_per_oct::,:]\n\t                #pyr_out, pyr= pyr[:,:,0:self.bins_per_oct,:], pyr[:,:,self.bins_per_oct::,:]\n\t                #X_out=(pyr_up_proj(X_out)+pyr_out)/(2**0.5)\n\t                Out=Out.permute(0,2,3,1).contiguous() #call contiguous() here?\n\t                Out=torch.view_as_complex(Out)\n", "                #save output\n\t                X_list_out[i]=Out.unsqueeze(1)\n\t            elif j>(self.num_octs-1):\n\t                print(\"We should not be here\")\n\t                pass\n\t            if j>0 and j<=(self.num_octs-1):\n\t                #pyr=self.upsampler(pyr) #call contiguous() here?\n\t                X=self.upsamplerT(X) #call contiguous() here?\n\t                Xout=self.upsamplerT(Xout) #call contiguous() here?\n\t        pred_time=self.CQTransform.bwd(X_list_out)\n", "        pred_time=pred_time.squeeze(1)\n\t        pred_time=pred_time[:,0:inputs.shape[-1]]\n\t        assert pred_time.shape==inputs.shape, \"bad shapes\"\n\t        return pred_time\n\tclass CropAddBlock(nn.Module):\n\t    def forward(self,down_layer, x,  **kwargs):\n\t        x1_shape = down_layer.shape\n\t        x2_shape = x.shape\n\t        #print(x1_shape,x2_shape)\n\t        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n", "        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n\t        down_layer_cropped = down_layer[:,\n\t                                        :,\n\t                                        height_diff: (x2_shape[2] + height_diff),\n\t                                        width_diff: (x2_shape[3] + width_diff),:]\n\t        x = torch.add(down_layer_cropped, x)\n\t        return x\n\tclass CropConcatBlock(nn.Module):\n\t    def forward(self, down_layer, x, **kwargs):\n\t        x1_shape = down_layer.shape\n", "        x2_shape = x.shape\n\t        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n\t        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n\t        down_layer_cropped = down_layer[:,\n\t                                        :,\n\t                                        height_diff: (x2_shape[2] + height_diff),\n\t                                        width_diff: (x2_shape[3] + width_diff)]\n\t        x = torch.cat((down_layer_cropped, x),1)\n\t        return x\n"]}
{"filename": "networks/denoiser.py", "chunked_list": ["import torch.nn as nn\n\timport math as m\n\timport torch\n\t#import torchaudio\n\ttorch.pi = torch.acos(torch.zeros(1)).item() * 2 # which is 3.1415927410125732\n\t#def build_model_denoise(unet_args=None):\n\t#\n\t#    inputs=Input(shape=(None, None,2))\n\t#\n\t#    outputs_stage_2,outputs_stage_1=MultiStage_denoise(unet_args=unet_args)(inputs)\n", "#\n\t#    #Encapsulating MultiStage_denoise in a keras.Model object\n\t#    model= tf.keras.Model(inputs=inputs,outputs=[outputs_stage_2, outputs_stage_1])\n\t#    return model\n\tclass DenseBlock(nn.Module):\n\t    '''\n\t    [B, T, F, N] => [B, T, F, N] \n\t    DenseNet Block consisting of \"num_layers\" densely connected convolutional layers\n\t    '''\n\t    def __init__(self, num_layers,N0, N, ksize):\n", "        '''\n\t        num_layers:     number of densely connected conv. layers\n\t        N:              Number of filters (same in each layer) \n\t        ksize:          Kernel size (same in each layer) \n\t        '''\n\t        super(DenseBlock, self).__init__()\n\t        self.H=nn.ModuleList()\n\t        self.num_layers=num_layers\n\t        for i in range(num_layers):\n\t            if i==0:   \n", "                Nin=N0\n\t            else:\n\t                Nin=N0+i*N\n\t            self.H.append(nn.Sequential(\n\t                                nn.Conv2d(Nin,N,\n\t                                      kernel_size=ksize,\n\t                                      stride=1,\n\t                                      padding='same',\n\t                                      padding_mode='reflect',\n\t                                      ),\n", "                                nn.ELU()        ))\n\t    def forward(self, x):\n\t        x_ = self.H[0](x)\n\t        if self.num_layers>1:\n\t            for h in self.H[1:]:\n\t                x = torch.cat((x_, x), 1)\n\t                #x_=tf.pad(x, self.padding_modes_1, mode='SYMMETRIC')\n\t                x_ = h(x)  \n\t                #add elu here\n\t        return x_\n", "class FinalBlock(nn.Module):\n\t    '''\n\t    [B, T, F, N] => [B, T, F, 2] \n\t    Final block. Basiforwardy, a 3x3 conv. layer to map the output features to the output complex spectrogram.\n\t    '''\n\t    def __init__(self, N0):\n\t        super(FinalBlock, self).__init__()\n\t        ksize=(3,3)\n\t        self.conv2=nn.Conv2d(N0,out_channels=2,\n\t                      kernel_size=ksize,\n", "                      stride=1, \n\t                      padding='same',\n\t                      padding_mode='reflect')\n\t    def forward(self, inputs ):\n\t        pred=self.conv2(inputs)\n\t        return pred\n\tclass SAM(nn.Module):\n\t    '''\n\t    [B, T, F, N] => [B, T, F, N] , [B, T, F, N]\n\t    Supervised Attention Module:\n", "    The purpose of SAM is to make the network only propagate the most relevant features to the second stage, discarding the less useful ones.\n\t    The estimated residual noise signal is generated from the U-Net output features by means of a 3x3 convolutional layer. \n\t    The first stage output is then calculated adding the original input spectrogram to the residual noise. \n\t    The attention-guided features are computed using the attention masks M, which are directly calculated from the first stage output with a 1x1 convolution and a sigmoid function. \n\t    '''\n\t    def __init__(self, n_feat):\n\t        super(SAM, self).__init__()\n\t        ksize=(3,3)\n\t        self.conv1 = nn.Conv2d(n_feat,out_channels=n_feat,\n\t                      kernel_size=ksize,\n", "                      stride=1, \n\t                      padding='same',\n\t                      padding_mode='reflect')\n\t        ksize=(3,3)\n\t        self.conv2=nn.Conv2d( n_feat,2,\n\t                      kernel_size=ksize,\n\t                      stride=1, \n\t                      padding='same',\n\t                      padding_mode='reflect')\n\t        ksize=(3,3)\n", "        self.conv3 = nn.Conv2d(2,n_feat,\n\t                      kernel_size=ksize,\n\t                      stride=1, \n\t                      padding='same',\n\t                      padding_mode='reflect')\n\t        #self.cropadd=CropAddBlock()\n\t    def forward(self, inputs, input_spectrogram):\n\t        x1 = self.conv1(inputs)\n\t        x=self.conv2(inputs)\n\t        #residual prediction\n", "        pred = torch.add(x, input_spectrogram) #features to next stage\n\t        M=self.conv3(pred)\n\t        M= torch.sigmoid(M)\n\t        x1=torch.multiply(x1, M)\n\t        x1 = torch.add(x1, inputs) #features to next stage\n\t        return x1, pred\n\tclass AddFreqEncoding(nn.Module):\n\t    '''\n\t    [B, T, F, 2] => [B, T, F, 12]  \n\t    Generates frequency positional embeddings and concatenates them as 10 extra channels\n", "    This function is optimized for F=1025\n\t    '''\n\t    def __init__(self, f_dim):\n\t        super(AddFreqEncoding, self).__init__()\n\t        pi=torch.pi\n\t        self.f_dim=f_dim #f_dim is fixed\n\t        n=torch.arange(start=0,end=f_dim)/(f_dim-1)\n\t        # n=n.type(torch.FloatTensor)\n\t        coss=torch.cos(pi*n)\n\t        f_channel = torch.unsqueeze(coss, -1) #(1025,1)\n", "        self.fembeddings= f_channel\n\t        for k in range(1,10):   \n\t            coss=torch.cos(2**k*pi*n)\n\t            f_channel = torch.unsqueeze(coss, -1) #(1025,1)\n\t            self.fembeddings=torch.cat((self.fembeddings,f_channel),-1) #(1025,10)\n\t        self.fembeddings=nn.Parameter(self.fembeddings)\n\t        #self.register_buffer('fembeddings_const', self.fembeddings)\n\t    def forward(self, input_tensor):\n\t        batch_size_tensor = input_tensor.shape[0]  # get batch size\n\t        time_dim = input_tensor.shape[2]  # get time dimension\n", "        fembeddings_2 = torch.broadcast_to(self.fembeddings, [batch_size_tensor, time_dim, self.f_dim, 10])\n\t        fembeddings_2=fembeddings_2.permute(0,3,1,2)\n\t        return torch.cat((input_tensor,fembeddings_2),1)  #(batch,12,427,1025)\n\tclass Decoder(nn.Module):\n\t    '''\n\t    [B, T, F, N] , skip connections => [B, T, F, N]  \n\t    Decoder side of the U-Net subnetwork.\n\t    '''\n\t    def __init__(self, Ns, Ss, unet_args):\n\t        super(Decoder, self).__init__()\n", "        self.Ns=Ns\n\t        self.Ss=Ss\n\t        self.depth=unet_args.depth\n\t        self.dblocks=nn.ModuleList()\n\t        for i in range(self.depth):\n\t            self.dblocks.append(D_Block(layer_idx=i,N0=self.Ns[i+1] ,N=self.Ns[i], S=self.Ss[i],num_tfc=unet_args.num_tfc))\n\t    def forward(self,inputs, contracting_layers):\n\t        x=inputs\n\t        for i in range(self.depth,0,-1):\n\t            x=self.dblocks[i-1](x, contracting_layers[i-1])\n", "        return x \n\tclass Encoder(nn.Module):\n\t    '''\n\t    [B, T, F, N] => skip connections , [B, T, F, N_4]  \n\t    Encoder side of the U-Net subnetwork.\n\t    '''\n\t    def __init__(self,N0, Ns, Ss, unet_args):\n\t        super(Encoder, self).__init__()\n\t        self.Ns=Ns\n\t        self.Ss=Ss\n", "        self.depth=unet_args.depth\n\t        self.contracting_layers = {}\n\t        self.eblocks=nn.ModuleList()\n\t        for i in range(self.depth):\n\t            if i==0:\n\t                Nin=N0\n\t            else:\n\t                Nin=self.Ns[i]\n\t            self.eblocks.append(E_Block(layer_idx=i,N0=Nin,N01=self.Ns[i],N=self.Ns[i+1],S=self.Ss[i], num_tfc=unet_args.num_tfc))\n\t        self.i_block=I_Block(self.Ns[self.depth],self.Ns[self.depth],unet_args.num_tfc)\n", "    def forward(self, inputs):\n\t        x=inputs\n\t        for i in range(self.depth):\n\t            x, x_contract=self.eblocks[i](x)\n\t            self.contracting_layers[i] = x_contract #if remove 0, correct this\n\t        x=self.i_block(x)\n\t        return x, self.contracting_layers\n\tclass MultiStage_denoise(nn.Module):\n\t    def __init__(self,  unet_args=None):\n\t        super(MultiStage_denoise, self).__init__()\n", "        self.depth=unet_args.depth\n\t        Nin=2\n\t        if unet_args.use_fencoding:\n\t            self.freq_encoding=AddFreqEncoding(unet_args.f_dim)\n\t            Nin=12 #hardcoded\n\t        self.use_sam=unet_args.use_SAM\n\t        self.use_fencoding=unet_args.use_fencoding\n\t        self.num_stages=unet_args.num_stages\n\t        #Encoder\n\t        self.Ns= [64,64,64,128,128,256,512] \n", "        self.Ss= [(2,2),(2,2),(2,2),(2,2),(2,2),(2,2)]\n\t        #initial feature extractor\n\t        ksize=(7,7)\n\t        self.conv2d_1 = nn.Sequential(nn.Conv2d(Nin,self.Ns[0],\n\t                      kernel_size=ksize,\n\t                      padding='same',\n\t                      padding_mode='reflect'),\n\t                      nn.ELU())\n\t        self.encoder_s1=Encoder(self.Ns[0],self.Ns, self.Ss, unet_args)\n\t        self.decoder_s1=Decoder(self.Ns, self.Ss, unet_args)\n", "        self.cropconcat = CropConcatBlock()\n\t        #self.cropadd = CropAddBlock()\n\t        self.finalblock=FinalBlock(self.Ns[0])\n\t        if self.num_stages>1:\n\t            self.sam_1=SAM(self.Ns[0])\n\t            #initial feature extractor\n\t            ksize=(7,7)\n\t            self.conv2d_2 =nn.Sequential(\n\t                                 nn.Conv2d(Nin,self.Ns[0],\n\t                                 kernel_size=ksize,\n", "                                 stride=1, \n\t                                 padding='same',\n\t                                 padding_mode='reflect'),\n\t                                 nn.ELU())\n\t            self.encoder_s2=Encoder(2*self.Ns[0],self.Ns, self.Ss, unet_args)\n\t            self.decoder_s2=Decoder(self.Ns, self.Ss, unet_args)\n\t    def forward(self, inputs):\n\t        if self.use_fencoding:\n\t            x_w_freq=self.freq_encoding(inputs)   #None, None, 1025, 12 \n\t        else:\n", "            x_w_freq=inputs\n\t        #intitial feature extractor\n\t        x=self.conv2d_1(x_w_freq) #None, None, 1025, 32\n\t        x, contracting_layers_s1= self.encoder_s1(x)\n\t        #decoder\n\t        feats_s1 =self.decoder_s1(x, contracting_layers_s1) #None, None, 1025, 32 features\n\t        if self.num_stages>1:        \n\t            #SAM module\n\t            Fout, pred_stage_1=self.sam_1(feats_s1,inputs)\n\t            #intitial feature extractor\n", "            x=self.conv2d_2(x_w_freq)\n\t            if self.use_sam:\n\t                x = torch.cat((x, Fout), 1)\n\t            else:\n\t                x = torch.cat((x,feats_s1), 1)\n\t            x, contracting_layers_s2= self.encoder_s2(x)\n\t            feats_s2=self.decoder_s2(x, contracting_layers_s2) #None, None, 1025, 32 features\n\t            #consider implementing a third stage?\n\t            pred_stage_2=self.finalblock(feats_s2) \n\t            return pred_stage_2, pred_stage_1\n", "        else:             \n\t            pred_stage_1=self.finalblock(feats_s1) \n\t            return pred_stage_1\n\tclass I_Block(nn.Module):\n\t    '''\n\t    [B, T, F, N] => [B, T, F, N] \n\t    Intermediate block:\n\t    Basiforwardy, a densenet block with a residual connection\n\t    '''\n\t    def __init__(self,N0,N, num_tfc, **kwargs):\n", "        super(I_Block, self).__init__(**kwargs)\n\t        ksize=(3,3)\n\t        self.tfc=DenseBlock(num_tfc,N0,N,ksize)\n\t        self.conv2d_res= nn.Conv2d(N0,N,\n\t                                      kernel_size=(1,1),\n\t                                      stride=1,\n\t                                      padding='same',\n\t                                      padding_mode='reflect')\n\t    def forward(self,inputs):\n\t        x=self.tfc(inputs)\n", "        inputs_proj=self.conv2d_res(inputs)\n\t        return torch.add(x,inputs_proj)\n\tclass E_Block(nn.Module):\n\t    def __init__(self, layer_idx,N0,N01, N,  S, num_tfc, **kwargs):\n\t        super(E_Block, self).__init__(**kwargs)\n\t        self.layer_idx=layer_idx\n\t        self.N0=N0\n\t        self.N=N\n\t        self.S=S\n\t        self.i_block=I_Block(N0,N01,num_tfc)\n", "        ksize=(S[0]+2,S[1]+2)\n\t        self.conv2d_2 = nn.Sequential(nn.Conv2d(N01,N,\n\t                                          kernel_size=(S[0]+2,S[1]+2),\n\t                                          padding=(2,2),\n\t                                          stride=S,\n\t                                          padding_mode='reflect'),\n\t                                      nn.ELU())\n\t    def forward(self, inputs, training=None, **kwargs):\n\t        x=self.i_block(inputs)\n\t        x_down = self.conv2d_2(x)\n", "        return x_down, x\n\tclass D_Block(nn.Module):\n\t    def __init__(self, layer_idx,N0, N,  S,  num_tfc, **kwargs):\n\t        super(D_Block, self).__init__(**kwargs)\n\t        self.layer_idx=layer_idx\n\t        self.N=N\n\t        self.S=S\n\t        ksize=(S[0]+2, S[1]+2)\n\t        self.tconv_1= nn.Sequential(\n\t                                nn.ConvTranspose2d(N0,N,\n", "                                             kernel_size=(S[0]+2, S[1]+2),\n\t                                             stride=S,\n\t                                             padding_mode='zeros'),\n\t                                nn.ELU())\n\t        self.upsampling = nn.Upsample(scale_factor=S, mode=\"nearest\")\n\t        self.projection =nn.Conv2d(N0,N,\n\t                                      kernel_size=(1,1),\n\t                                      stride=1,\n\t                                      padding='same',\n\t                                      padding_mode='reflect')\n", "        self.cropadd=CropAddBlock()\n\t        self.cropconcat=CropConcatBlock()\n\t        self.i_block=I_Block(2*N,N,num_tfc)\n\t    def forward(self, inputs, bridge, **kwargs):\n\t        x = self.tconv_1(inputs)\n\t        x2= self.upsampling(inputs)\n\t        if x2.shape[-1]!=x.shape[-1]:\n\t            x2= self.projection(x2)\n\t        x= self.cropadd(x,x2)\n\t        x=self.cropconcat(x,bridge)\n", "        x=self.i_block(x)\n\t        return x\n\tclass CropAddBlock(nn.Module):\n\t    def forward(self,down_layer, x,  **kwargs):\n\t        x1_shape = down_layer.shape\n\t        x2_shape = x.shape\n\t        #print(x1_shape,x2_shape)\n\t        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n\t        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n\t        down_layer_cropped = down_layer[:,\n", "                                        :,\n\t                                        height_diff: (x2_shape[2] + height_diff),\n\t                                        width_diff: (x2_shape[3] + width_diff)]\n\t        x = torch.add(down_layer_cropped, x)\n\t        return x\n\tclass CropConcatBlock(nn.Module):\n\t    def forward(self, down_layer, x, **kwargs):\n\t        x1_shape = down_layer.shape\n\t        x2_shape = x.shape\n\t        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n", "        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n\t        down_layer_cropped = down_layer[:,\n\t                                        :,\n\t                                        height_diff: (x2_shape[2] + height_diff),\n\t                                        width_diff: (x2_shape[3] + width_diff)]\n\t        x = torch.cat((down_layer_cropped, x),1)\n\t        return x\n"]}
{"filename": "diff_params/edm_PD.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport utils.training_utils as utils\n\tclass EDM():\n\t    \"\"\"\n\t        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n\t    \"\"\"\n\t    def __init__(self, args):\n\t        \"\"\"\n\t        Args:\n", "            args (dictionary): hydra arguments\n\t            sigma_data (float): \n\t        \"\"\"\n\t        self.args=args\n\t        self.sigma_min = args.diff_params.sigma_min\n\t        self.sigma_max =args.diff_params.sigma_max\n\t        self.P_mean=args.diff_params.P_mean\n\t        self.P_std=args.diff_params.P_std\n\t        self.ro=args.diff_params.ro\n\t        self.ro_train=args.diff_params.ro_train\n", "        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n\t        #parameters stochastic sampling\n\t        self.Schurn=args.diff_params.Schurn\n\t        self.Stmin=args.diff_params.Stmin\n\t        self.Stmax=args.diff_params.Stmax\n\t        self.Snoise=args.diff_params.Snoise\n\t        #perceptual filter\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\t        self.boundaries=self.create_schedule(self.args.diff_params.PD.boundaries.T)\n", "        #print(self.boundaries)\n\t        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\t        self.boundaries=self.boundaries.to(self.device)\n\t        #self.boundaries=torch.flip(self.boundaries,(0,)).to(self.device)\n\t        #print(\"real T\", len(self.boundaries))\n\t        #print(self.boundaries)\n\t    def get_gamma(self, t): \n\t        \"\"\"\n\t        Get the parameter gamma that defines the stochasticity of the sampler\n\t        Args\n", "            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n\t        \"\"\"\n\t        N=t.shape[0]\n\t        gamma=torch.zeros(t.shape).to(t.device)\n\t        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n\t        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n\t        #We use Schurn=5 as the default in our experiments\n\t        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n\t        return gamma\n\t    def create_schedule(self,nb_steps):\n", "        \"\"\"\n\t        Define the schedule of timesteps\n\t        Args:\n\t           nb_steps (int): Number of discretized steps\n\t        \"\"\"\n\t        i=torch.arange(0,nb_steps+1)\n\t        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n\t        t[-1]=0\n\t        return t\n\t    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n", "        \"\"\"\n\t        Define the schedule of timesteps\n\t        Args:\n\t           nb_steps (int): Number of discretized steps\n\t        \"\"\"\n\t        i=torch.arange(0,nb_steps+1)\n\t        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n\t        t[-1]=0\n\t        return t\n\t    def sample_ptrain(self,N):\n", "        \"\"\"\n\t        For training, getting t as a normal distribution, folowing Karras et al. \n\t        I'm not using this\n\t        Args:\n\t            N (int): batch size\n\t        \"\"\"\n\t        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n\t        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n\t    def sample_ptrain_safe(self,N):\n\t        \"\"\"\n", "        For training, getting  t according to the same criteria as sampling\n\t        Args:\n\t            N (int): batch size\n\t        \"\"\"\n\t        a=torch.rand(N)\n\t        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n\t        return t\n\t    def sample_prior(self,shape,sigma):\n\t        \"\"\"\n\t        Just sample some gaussian noise, nothing more\n", "        Args:\n\t            shape (tuple): shape of the noise to sample, something like (B,T)\n\t            sigma (float): noise level of the noise\n\t        \"\"\"\n\t        n=torch.randn(shape).to(sigma.device)*sigma\n\t        return n\n\t    def cskip(self, sigma):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n", "            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\t    def cout(self,sigma ):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n", "    def cin(self, sigma):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return (self.sigma_data**2+sigma**2)**(-0.5)\n\t    def cnoise(self,sigma ):\n\t        \"\"\"\n\t        preconditioning of the noise embedding\n", "        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return (1/4)*torch.log(sigma)\n\t    def lambda_w(self,sigma):\n\t        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n\t    def denoiser(self, xn , net, sigma):\n\t        \"\"\"\n\t        This method does the whole denoising step, which implies applying the model and the preconditioning\n\t        Args:\n", "            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            model (nn.Module): Model of the denoiser\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        if len(sigma.shape)==1:\n\t            sigma=sigma.unsqueeze(-1)\n\t        cskip=self.cskip(sigma)\n\t        cout=self.cout(sigma)\n\t        cin=self.cin(sigma)\n\t        cnoise=self.cnoise(sigma)\n", "        return cskip * xn +cout*net(cin*xn, cnoise)  #this will crash because of broadcasting problems, debug later!\n\t    def prepare_train_preconditioning(self, x, sigma):\n\t        #weight=self.lambda_w(sigma)\n\t        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n\t        #print(x.shape)\n\t        noise=self.sample_prior(x.shape,sigma)\n\t        cskip=self.cskip(sigma)\n\t        cout=self.cout(sigma)\n\t        cin=self.cin(sigma)\n\t        cnoise=self.cnoise(sigma)\n", "        target=(1/cout)*(x-cskip*(x+noise))\n\t        return cin*(x+noise), target, cnoise\n\t    def loss_fn(self, net, x):\n\t        \"\"\"\n\t        Loss function, which is the mean squared error between the denoised latent and the clean latent\n\t        Args:\n\t            net (nn.Module): Model of the denoiser\n\t            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n", "        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\t        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n\t        #print(\"inputs to net\", input.shape, cnoise.shape)\n\t        estimate=net(input,cnoise)\n\t        error=(estimate-target)\n\t        try:\n\t            #this will only happen if the model is cqt-based, if it crashes it is normal\n\t            if self.args.net.use_cqt_DC_correction:\n\t                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n\t        except:\n", "            pass \n\t        #APPLY A-WEIGHTING\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            error=self.AW(error)\n\t        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n\t        return error**2, sigma\n\t    def ode_update(self, x, sigma_1, sigma_0, net_teacher):\n\t        x_0_hat=self.denoiser(x, net_teacher, sigma_0)\n\t        score=(x_0_hat-x)/sigma_0**2\n\t        return x-(sigma_1-sigma_0)*sigma_0*score\n", "    def loss_fn_PD(self, net, net_teacher, x, stage):\n\t        \"\"\"\n\t        Loss function, which is the mean squared error between the denoised latent and the clean latent\n\t        Args:\n\t            net (nn.Module): Model of the denoiser\n\t            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        #sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\t        #sample sigma using self.boundaries\n", "        if stage==0:\n\t            schedule=self.boundaries\n\t        else:\n\t            #if stage ==1 we are in the second stage, so we downsample the schedule by a factor of 2\n\t            schedule=self.boundaries[::(2**(stage))]\n\t        schedule=torch.flip(schedule,(0,))\n\t        print(schedule, \"stage\", stage, len(schedule), len(self.boundaries))\n\t        #print(\"stage\",stage,\"train schedule\", schedule, \"original boundaries\", self.boundaries)\n\t        if len(schedule)>3:\n\t            j = torch.randint(1, len(schedule)//2, (x.shape[0], 1), device=x.device)\n", "            i=j*2+1\n\t        else:\n\t            i=2\n\t        sigma_0=schedule[i]\n\t        sigma_1=schedule[i-1]\n\t        sigma_2=schedule[i-2]\n\t        #print(j,i, sigma_0, sigma_1, sigma_2)\n\t        #input_0, _ , cnoise_0= self.prepare_train_preconditioning(x, sigma_0)\n\t        noise=self.sample_prior(x.shape,sigma_0)\n\t        cskip_0=self.cskip(sigma_0)\n", "        cout_0=self.cout(sigma_0)\n\t        cin_0=self.cin(sigma_0)\n\t        #cin_1=self.cin(sigma_1)\n\t        cnoise_0=self.cnoise(sigma_0)\n\t        #input_1, _ , cnoise_1= self.prepare_train_preconditioning(x, sigma_1)\n\t        #2 ode steps with the teacher network\n\t        zn=x+noise\n\t        with torch.no_grad():\n\t            z_teacher=self.ode_update(zn, sigma_1,sigma_0, net_teacher)\n\t            z_teacher=self.ode_update(z_teacher, sigma_2, sigma_1, net_teacher)\n", "            #x_0_student=self.denoiser(cin_0*xn, net,sigma_0)\n\t            x_0_student=(z_teacher-(sigma_2/sigma_0)*zn)/(1-(sigma_2/sigma_0))\n\t            target=(1/cout_0)*(x_0_student-cskip_0*zn)\n\t        estimate=net(cin_0*zn, cnoise_0)\n\t        error=(estimate-target.detach())\n\t        try:\n\t            #this will only happen if the model is cqt-based, if it crashes it is normal\n\t            if self.args.net.use_cqt_DC_correction:\n\t                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n\t        except:\n", "            pass \n\t        #APPLY A-WEIGHTING\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            error=self.AW(error)\n\t        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n\t        return error**2, sigma_0\n\t    def PD_sample(self, N, L, net, device, stage):\n\t        \"\"\"\n\t           N: batch size\n\t           L: length\n", "           device\n\t        \"\"\"\n\t        #if stage ==1 we are in the second stage, so we downsample the schedule by a factor of 2\n\t        schedule=self.boundaries[::(2**(stage+1))]\n\t        schedule=torch.flip(schedule,(0,))\n\t        print(\"test schedule\", schedule)\n\t        z=torch.randn((N,L), device=device)*schedule[-1]\n\t        for i in range(len(schedule)-1):\n\t            sigma_0=schedule[-i-1]\n\t            sigma_1=schedule[-i-2]\n", "            print(i, sigma_0, sigma_1)\n\t            z=self.ode_update(z, sigma_1,sigma_0, net)\n\t        #x0=self.denoiser(z, net, sigma_1)\n\t        return z\n"]}
{"filename": "diff_params/edm.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport utils.training_utils as utils\n\tclass EDM():\n\t    \"\"\"\n\t        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n\t    \"\"\"\n\t    def __init__(self, args):\n\t        \"\"\"\n\t        Args:\n", "            args (dictionary): hydra arguments\n\t            sigma_data (float): \n\t        \"\"\"\n\t        self.args=args\n\t        self.sigma_min = args.diff_params.sigma_min\n\t        self.sigma_max =args.diff_params.sigma_max\n\t        self.P_mean=args.diff_params.P_mean\n\t        self.P_std=args.diff_params.P_std\n\t        self.ro=args.diff_params.ro\n\t        self.ro_train=args.diff_params.ro_train\n", "        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n\t        #parameters stochastic sampling\n\t        self.Schurn=args.diff_params.Schurn\n\t        self.Stmin=args.diff_params.Stmin\n\t        self.Stmax=args.diff_params.Stmax\n\t        self.Snoise=args.diff_params.Snoise\n\t        #perceptual filter\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\t    def get_gamma(self, t): \n", "        \"\"\"\n\t        Get the parameter gamma that defines the stochasticity of the sampler\n\t        Args\n\t            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n\t        \"\"\"\n\t        N=t.shape[0]\n\t        gamma=torch.zeros(t.shape).to(t.device)\n\t        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n\t        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n\t        #We use Schurn=5 as the default in our experiments\n", "        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n\t        return gamma\n\t    def create_schedule(self,nb_steps):\n\t        \"\"\"\n\t        Define the schedule of timesteps\n\t        Args:\n\t           nb_steps (int): Number of discretized steps\n\t        \"\"\"\n\t        i=torch.arange(0,nb_steps+1)\n\t        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n", "        t[-1]=0\n\t        return t\n\t    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n\t        \"\"\"\n\t        Define the schedule of timesteps\n\t        Args:\n\t           nb_steps (int): Number of discretized steps\n\t        \"\"\"\n\t        i=torch.arange(0,nb_steps+1)\n\t        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n", "        t[-1]=0\n\t        return t\n\t    def sample_ptrain(self,N):\n\t        \"\"\"\n\t        For training, getting t as a normal distribution, folowing Karras et al. \n\t        I'm not using this\n\t        Args:\n\t            N (int): batch size\n\t        \"\"\"\n\t        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n", "        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n\t    def sample_ptrain_safe(self,N):\n\t        \"\"\"\n\t        For training, getting  t according to the same criteria as sampling\n\t        Args:\n\t            N (int): batch size\n\t        \"\"\"\n\t        a=torch.rand(N)\n\t        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n\t        return t\n", "    def sample_prior(self,shape,sigma):\n\t        \"\"\"\n\t        Just sample some gaussian noise, nothing more\n\t        Args:\n\t            shape (tuple): shape of the noise to sample, something like (B,T)\n\t            sigma (float): noise level of the noise\n\t        \"\"\"\n\t        n=torch.randn(shape).to(sigma.device)*sigma\n\t        return n\n\t    def cskip(self, sigma):\n", "        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\t    def cout(self,sigma ):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n", "            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n\t    def cin(self, sigma):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return (self.sigma_data**2+sigma**2)**(-0.5)\n", "    def cnoise(self,sigma ):\n\t        \"\"\"\n\t        preconditioning of the noise embedding\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return (1/4)*torch.log(sigma)\n\t    def lambda_w(self,sigma):\n\t        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n\t    def denoiser(self, xn , net, sigma):\n", "        \"\"\"\n\t        This method does the whole denoising step, which implies applying the model and the preconditioning\n\t        Args:\n\t            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            model (nn.Module): Model of the denoiser\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        if len(sigma.shape)==1:\n\t            sigma=sigma.unsqueeze(-1)\n\t        cskip=self.cskip(sigma)\n", "        cout=self.cout(sigma)\n\t        cin=self.cin(sigma)\n\t        cnoise=self.cnoise(sigma)\n\t        return cskip * xn +cout*net(cin*xn, cnoise)  #this will crash because of broadcasting problems, debug later!\n\t    def prepare_train_preconditioning(self, x, sigma):\n\t        #weight=self.lambda_w(sigma)\n\t        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n\t        print(x.shape)\n\t        noise=self.sample_prior(x.shape,sigma)\n\t        cskip=self.cskip(sigma)\n", "        cout=self.cout(sigma)\n\t        cin=self.cin(sigma)\n\t        cnoise=self.cnoise(sigma)\n\t        target=(1/cout)*(x-cskip*(x+noise))\n\t        return cin*(x+noise), target, cnoise\n\t    def loss_fn(self, net, x):\n\t        \"\"\"\n\t        Loss function, which is the mean squared error between the denoised latent and the clean latent\n\t        Args:\n\t            net (nn.Module): Model of the denoiser\n", "            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\t        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n\t        print(\"inputs to net\", input.shape, cnoise.shape)\n\t        estimate=net(input,cnoise)\n\t        error=(estimate-target)\n\t        try:\n\t            #this will only happen if the model is cqt-based, if it crashes it is normal\n", "            if self.args.net.use_cqt_DC_correction:\n\t                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n\t        except:\n\t            pass \n\t        #APPLY A-WEIGHTING\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            error=self.AW(error)\n\t        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n\t        return error**2, sigma\n"]}
{"filename": "diff_params/edm_eps.py", "chunked_list": ["import torch\n\timport torch.nn.functional as F\n\timport numpy as np\n\tfrom tqdm import tqdm\n\timport utils.training_utils as utils\n\tclass EDM():\n\t    \"\"\"\n\t        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n\t    \"\"\"\n\t    def __init__(self, args):\n", "        \"\"\"\n\t        Args:\n\t            args (dictionary): hydra arguments\n\t            sigma_data (float): \n\t        \"\"\"\n\t        self.args=args\n\t        #parameters from: https://github.com/yoyololicon/diffwave-sr/blob/main/ckpt/vctk_48k_udm/.hydra/config.yaml\n\t        self.T=self.args.diff_params.T\n\t        self.gamma1=torch.Tensor([self.args.diff_params.scheduler.gamma1])\n\t        self.gamma0=torch.Tensor([self.args.diff_params.scheduler.gamma0])\n", "        print(\"gamam0, 1\",self.gamma0, self.gamma1)\n\t        t = torch.linspace(0, 1, self.T + 1)\n\t        self.gamma, t=self.LogSNRLinearScheduler(self.gamma1, self.gamma0, t)\n\t        self.sigma_min = args.diff_params.sigma_min\n\t        self.sigma_max =args.diff_params.sigma_max\n\t        self.P_mean=args.diff_params.P_mean\n\t        self.P_std=args.diff_params.P_std\n\t        self.ro=args.diff_params.ro\n\t        self.ro_train=args.diff_params.ro_train\n\t        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n", "        #parameters stochastic sampling\n\t        self.Schurn=args.diff_params.Schurn\n\t        self.Stmin=args.diff_params.Stmin\n\t        self.Stmax=args.diff_params.Stmax\n\t        self.Snoise=args.diff_params.Snoise\n\t        #perceptual filter\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\t    def LogSNRLinearScheduler(self, gamma1, gamma0, t):\n\t        t = t.clamp(0, 1)\n", "        gamma = gamma0 * (1 - t) + gamma1 * t\n\t        return gamma, t\n\t    def gamma_to_t(self, gamma):\n\t        \"\"\"\n\t        Convert the parameter gamma to the parameter t\n\t        Args:\n\t            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n\t        \"\"\"\n\t        return (gamma - self.gamma0) / (self.gamma1 - self.gamma0)\n\t    def t_to_gamma(self, t):\n", "        \"\"\"\n\t        Convert the parameter t to the parameter gamma\"\"\"\n\t        return self.gamma0 + t * (self.gamma1 - self.gamma0)\n\t    def gamma_2_as(self, gamma):\n\t        \"\"\"\n\t        Convert the parameter gamma to the parameter alpha and s\n\t        Args:\n\t            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n\t        \"\"\"\n\t        var = gamma.sigmoid()\n", "        return (1 - var).sqrt(), var.sqrt()\n\t    def t_2_as(self, t):\n\t        \"\"\"\n\t        Convert the parameter t to the parameter alpha and s\n\t        Args:\n\t            t (Tensor): shape: (N_steps, ) Tensor of t values\n\t        \"\"\"\n\t        gamma=self.t_to_gamma(t)\n\t        return self.gamma_2_as(gamma)\n\t    def gamma_to_sigma(self, gamma):\n", "        \"\"\"\n\t        Convert the parameter gamma to the parameter sigma\n\t        Args:\n\t            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n\t        \"\"\"\n\t        return torch.sqrt(1/torch.exp(-gamma))\n\t    def sigma_to_gamma(self, sigma):\n\t        \"\"\"\n\t        Convert the parameter sigma to the parameter gamma\n\t        Args:\n", "            sigma (Tensor): shape: (N_steps, ) Tensor of sigma values\n\t        \"\"\"\n\t        return torch.log(sigma**2)\n\t    def sigma_to_t(self, sigma):\n\t        gamma=self.sigma_to_gamma(sigma)\n\t        print(\"gamma\",gamma)\n\t        return self.gamma_to_t(gamma)\n\t    def gamma2logas(self,g):\n\t        log_var = -F.softplus(-g)\n\t        return 0.5 * (-g + log_var), log_var\n", "    def reverse_process_ddim(self,z_1,  model):\n\t        tt = torch.linspace(0, 1, self.T + 1)\n\t        gamma, steps =self.LogSNRLinearScheduler(self.gamma1, self.gamma0, tt)\n\t        #print(gamma, steps)\n\t        Pm1 = -torch.expm1((gamma[1:] - gamma[:-1]) * 0.5)\n\t        log_alpha, log_var = self.gamma2logas(gamma)\n\t        #print(\"log_alpha\", log_alpha, \"log_var\", log_var)\n\t        alpha_st = torch.exp(log_alpha[:-1] - log_alpha[1:])\n\t        #print(\"alpha_st\", alpha_st)\n\t        std = log_var.mul(0.5).exp()\n", "        T = gamma.numel() - 1\n\t        z_t = z_1\n\t        for t in tqdm(range(T, 0, -1)):\n\t            #print(\"z_t std\", z_t.std(-1))\n\t            s = t - 1\n\t            #print(\"steps\",steps[t:t+1],\"gamma\", gamma[t], \"alpha_st\", alpha_st[s], \"std\",std[s], \"pm1\", Pm1[s])\n\t            #print(z_t.shape, steps[t:t+1].shape)\n\t            noise_hat = model(z_t, steps[t:t+1])\n\t            noise_hat = noise_hat.float()\n\t            z_t.mul_(alpha_st[s]).add_(std[s] * Pm1[s] * noise_hat)\n", "        return z_t\n\t    def get_gamma(self, t): \n\t        \"\"\"\n\t        Get the parameter gamma that defines the stochasticity of the sampler, it is not the same as the parameter gamma in the scheduler\n\t        Args\n\t            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n\t        \"\"\"\n\t        N=t.shape[0]\n\t        gamma=torch.zeros(t.shape).to(t.device)\n\t        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n", "        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n\t        #We use Schurn=5 as the default in our experiments\n\t        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n\t        return gamma\n\t    def create_schedule(self,nb_steps):\n\t        \"\"\"\n\t        Define the schedule of timesteps\n\t        Args:\n\t           nb_steps (int): Number of discretized steps\n\t        \"\"\"\n", "        i=torch.arange(0,nb_steps+1)\n\t        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n\t        t[-1]=0\n\t        return t\n\t    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n\t        \"\"\"\n\t        Define the schedule of timesteps\n\t        Args:\n\t           nb_steps (int): Number of discretized steps\n\t        \"\"\"\n", "        i=torch.arange(0,nb_steps+1)\n\t        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n\t        t[-1]=0\n\t        return t\n\t    def sample_ptrain(self,N):\n\t        \"\"\"\n\t        For training, getting t as a normal distribution, folowing Karras et al. \n\t        I'm not using this\n\t        Args:\n\t            N (int): batch size\n", "        \"\"\"\n\t        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n\t        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n\t    def sample_ptrain_safe(self,N):\n\t        \"\"\"\n\t        For training, getting  t according to the same criteria as sampling\n\t        Args:\n\t            N (int): batch size\n\t        \"\"\"\n\t        a=torch.rand(N)\n", "        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n\t        return t\n\t    def sample_prior(self,shape,sigma):\n\t        \"\"\"\n\t        Just sample some gaussian noise, nothing more\n\t        Args:\n\t            shape (tuple): shape of the noise to sample, something like (B,T)\n\t            sigma (float): noise level of the noise\n\t        \"\"\"\n\t        n=torch.randn(shape).to(sigma.device)*sigma\n", "        return n\n\t    def cskip(self, sigma):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\t    def cout(self,sigma ):\n\t        \"\"\"\n", "        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n\t    def cin(self, sigma):\n\t        \"\"\"\n\t        Just one of the preconditioning parameters\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n", "        \"\"\"\n\t        return (self.sigma_data**2+sigma**2)**(-0.5)\n\t    def cnoise(self,sigma ):\n\t        \"\"\"\n\t        preconditioning of the noise embedding\n\t        Args:\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        return (1/4)*torch.log(sigma)\n\t    def lambda_w(self,sigma):\n", "        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n\t    def denoiser(self, xn , net, sigma):\n\t        \"\"\"\n\t        This method does the whole denoising step, which implies applying the model and the preconditioning\n\t        Args:\n\t            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            model (nn.Module): Model of the denoiser\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        if len(sigma.shape)==1:\n", "            sigma=sigma.unsqueeze(-1)\n\t        #cskip=self.cskip(sigma)\n\t        #cout=self.cout(sigma)\n\t        #cin=self.cin(sigma)\n\t        #cnoise=self.cnoise(sigma)\n\t        self.gamma0=self.gamma0.to(sigma.device)\n\t        self.gamma1=self.gamma1.to(sigma.device)\n\t        #print(sigma.device)\n\t        #t=self.sigma_to_t(sigma)\n\t        gamma=self.sigma_to_gamma(sigma)\n", "        #print(\"gamma\", gamma)\n\t        t=self.gamma_to_t(gamma)\n\t        a, s=self.gamma_2_as(gamma)\n\t        #print(a.shape, s.shape, sigma.shape, t.shape)\n\t        #print(\"sigma\", sigma, \"t\", t)\n\t        #print(\"a\", a,\"s\", s)\n\t        z_t=a*xn #this is equivalent to cin\n\t        #print(\"z_t std\",z_t.std(-1))\n\t        t=t.expand(z_t.shape[0],1).squeeze(-1) \n\t        #print(\"before net\", z_t.shape, t.shape)\n", "        eps_hat=net(z_t, t)\n\t        eps_hat=eps_hat\n\t        x0_hat=(-s*eps_hat+z_t)/a #equvalent to cout and cskip\n\t        return x0_hat#this will crash because of broadcasting problems, debug later!\n\t    def prepare_train_preconditioning(self, x, sigma):\n\t        #weight=self.lambda_w(sigma)\n\t        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n\t        print(x.shape)\n\t        noise=self.sample_prior(x.shape,sigma)\n\t        cskip=self.cskip(sigma)\n", "        cout=self.cout(sigma)\n\t        cin=self.cin(sigma)\n\t        cnoise=self.cnoise(sigma)\n\t        target=(1/cout)*(x-cskip*(x+noise))\n\t        return cin*(x+noise), target, cnoise\n\t    def loss_fn(self, net, x):\n\t        \"\"\"\n\t        Loss function, which is the mean squared error between the denoised latent and the clean latent\n\t        Args:\n\t            net (nn.Module): Model of the denoiser\n", "            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n\t            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n\t        \"\"\"\n\t        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\t        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n\t        estimate=net(input,cnoise)\n\t        error=(estimate-target)\n\t        try:\n\t            #this will only happen if the model is cqt-based, if it crashes it is normal\n\t            if self.args.net.use_cqt_DC_correction:\n", "                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n\t        except:\n\t            pass \n\t        #APPLY A-WEIGHTING\n\t        if self.args.diff_params.aweighting.use_aweighting:\n\t            error=self.AW(error)\n\t        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n\t        return error**2, sigma\n"]}
{"filename": "datasets/audiofolder_test.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\timport os\n\timport numpy as np\n\timport zipfile\n", "#import PIL.Image\n\timport json\n\timport torch\n\timport utils.dnnlib as dnnlib\n\timport random\n\timport pandas as pd\n\timport glob\n\timport soundfile as sf\n\t#try:\n\t#    import pyspng\n", "#except ImportError:\n\t#    pyspng = None\n\t#----------------------------------------------------------------------------\n\t# Dataset subclass that loads images recursively from the specified directory\n\t# or ZIP file.\n\tclass AudioFolderDatasetTest(torch.utils.data.Dataset):\n\t    def __init__(self,\n\t        dset_args,\n\t        fs=44100,\n\t        seg_len=131072,\n", "        num_samples=4,\n\t        seed=42 ):\n\t        super().__init__()\n\t        random.seed(seed)\n\t        np.random.seed(seed)\n\t        path=dset_args.test.path\n\t        filelist=glob.glob(os.path.join(path,\"*.wav\"))\n\t        assert len(filelist)>0 , \"error in dataloading: empty or nonexistent folder\"\n\t        self.train_samples=filelist\n\t        self.seg_len=int(seg_len)\n", "        self.fs=fs\n\t        self.test_samples=[]\n\t        self.filenames=[]\n\t        self._fs=[]\n\t        for i in range(num_samples):\n\t            file=self.train_samples[i]\n\t            self.filenames.append(os.path.basename(file))\n\t            data, samplerate = sf.read(file)\n\t            data=data.T\n\t            self._fs.append(samplerate)\n", "            if data.shape[-1]>=self.seg_len:\n\t                idx=np.random.randint(0,data.shape[-1]-self.seg_len)\n\t                data=data[...,idx:idx+self.seg_len]\n\t            else:\n\t                idx=0\n\t                data=np.tile(data,(self.seg_len//data.shape[-1]+1))[...,idx:idx+self.seg_len]\n\t            if not dset_args.test.stereo and len(data.shape)>1 :\n\t                data=np.mean(data,axis=1)\n\t            self.test_samples.append(data[...,0:self.seg_len]) #use only 50s\n\t    def __getitem__(self, idx):\n", "        #return self.test_samples[idx]\n\t        return self.test_samples[idx], self._fs[idx], self.filenames[idx]\n\t    def __len__(self):\n\t        return len(self.test_samples)\n"]}
{"filename": "datasets/maestro_dataset.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\timport os\n\timport numpy as np\n\timport zipfile\n", "#import PIL.Image\n\timport json\n\timport torch\n\timport utils.dnnlib as dnnlib\n\timport random\n\timport pandas as pd\n\timport glob\n\timport soundfile as sf\n\t#try:\n\t#    import pyspng\n", "#except ImportError:\n\t#    pyspng = None\n\t#----------------------------------------------------------------------------\n\t# Dataset subclass that loads images recursively from the specified directory\n\t# or ZIP file.\n\tclass MaestroDataset_fs(torch.utils.data.IterableDataset):\n\t    def __init__(self,\n\t        dset_args,\n\t        overfit=False, #set to True for overfitting dataset (lightweight tests to vccheck that the dataloading is not bottlenecking)\n\t        seed=42 ):\n", "        super().__init__()\n\t        self.overfit=overfit\n\t        random.seed(seed)\n\t        np.random.seed(seed)\n\t        path=dset_args.path\n\t        years=dset_args.years\n\t        metadata_file=os.path.join(path,\"maestro-v3.0.0.csv\")\n\t        metadata=pd.read_csv(metadata_file)\n\t        metadata=metadata[metadata[\"year\"].isin(years)]\n\t        metadata=metadata[metadata[\"split\"]==\"train\"]\n", "        filelist=metadata[\"audio_filename\"]\n\t        filelist=filelist.map(lambda x:  os.path.join(path,x)     , na_action='ignore')\n\t        self.train_samples=filelist.to_list()\n\t        self.seg_len=int(dset_args.load_len)\n\t    def __iter__(self):\n\t        if self.overfit:\n\t           data_clean=self.overfit_sample\n\t        while True:\n\t            if not self.overfit:\n\t                num=random.randint(0,len(self.train_samples)-1)\n", "                #for file in self.train_samples:  \n\t                file=self.train_samples[num]\n\t                data, samplerate = sf.read(file)\n\t                #print(file,samplerate)\n\t                data_clean=data\n\t                #Stereo to mono\n\t                if len(data.shape)>1 :\n\t                    data_clean=np.mean(data_clean,axis=1)\n\t            #normalize\n\t            #no normalization!!\n", "            #data_clean=data_clean/np.max(np.abs(data_clean))\n\t            #framify data clean files\n\t            num_frames=np.floor(len(data_clean)/self.seg_len) \n\t            if num_frames>4:\n\t                for i in range(8):\n\t                    #get 8 random batches to be a bit faster\n\t                    if not self.overfit:\n\t                        idx=np.random.randint(0,len(data_clean)-self.seg_len)\n\t                    else: \n\t                        idx=0\n", "                    segment=data_clean[idx:idx+self.seg_len]\n\t                    segment=segment.astype('float32')\n\t                    #b=np.mean(np.abs(segment))\n\t                    #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n\t                    #let's make this shit a bit robust to input scale\n\t                    #scale=np.random.uniform(1.75,2.25)\n\t                    #this way I estimage sigma_data (after pre_emph) to be around 1\n\t                    #segment=10.0**(scale) *segment\n\t                    yield  segment, samplerate\n\t            else:\n", "                pass\n\tclass MaestroDataset(torch.utils.data.IterableDataset):\n\t    def __init__(self,\n\t        dset_args,\n\t        fs=44100,\n\t        seg_len=131072,\n\t        overfit=False, #set to True for overfitting dataset (lightweight tests to vccheck that the dataloading is not bottlenecking)\n\t        seed=42 ):\n\t        super().__init__()\n\t        self.overfit=overfit\n", "        random.seed(seed)\n\t        np.random.seed(seed)\n\t        path=dset_args.path\n\t        years=dset_args.years\n\t        metadata_file=os.path.join(path,\"maestro-v3.0.0.csv\")\n\t        metadata=pd.read_csv(metadata_file)\n\t        metadata=metadata[metadata[\"year\"].isin(years)]\n\t        metadata=metadata[metadata[\"split\"]==\"train\"]\n\t        filelist=metadata[\"audio_filename\"]\n\t        filelist=filelist.map(lambda x:  os.path.join(path,x)     , na_action='ignore')\n", "        self.train_samples=filelist.to_list()\n\t        self.seg_len=int(seg_len)\n\t        self.fs=fs\n\t        if self.overfit:\n\t            file=self.train_samples[0]\n\t            data, samplerate = sf.read(file)\n\t            assert samplerate==self.fs, \"wrong sampling rate\"\n\t            if len(data.shape)>1 :\n\t                data=np.mean(data,axis=1)\n\t            self.overfit_sample=data[10*samplerate:60*samplerate] #use only 50s\n", "    def __iter__(self):\n\t        if self.overfit:\n\t           data_clean=self.overfit_sample\n\t        while True:\n\t            if not self.overfit:\n\t                num=random.randint(0,len(self.train_samples)-1)\n\t                #for file in self.train_samples:  \n\t                file=self.train_samples[num]\n\t                data, samplerate = sf.read(file)\n\t                assert(samplerate==self.fs, \"wrong sampling rate\")\n", "                data_clean=data\n\t                #Stereo to mono\n\t                if len(data.shape)>1 :\n\t                    data_clean=np.mean(data_clean,axis=1)\n\t            #normalize\n\t            #no normalization!!\n\t            #data_clean=data_clean/np.max(np.abs(data_clean))\n\t            #framify data clean files\n\t            num_frames=np.floor(len(data_clean)/self.seg_len) \n\t            if num_frames>4:\n", "                for i in range(8):\n\t                    #get 8 random batches to be a bit faster\n\t                    if not self.overfit:\n\t                        idx=np.random.randint(0,len(data_clean)-self.seg_len)\n\t                    else: \n\t                        idx=0\n\t                    segment=data_clean[idx:idx+self.seg_len]\n\t                    segment=segment.astype('float32')\n\t                    #b=np.mean(np.abs(segment))\n\t                    #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n", "                    #let's make this shit a bit robust to input scale\n\t                    #scale=np.random.uniform(1.75,2.25)\n\t                    #this way I estimage sigma_data (after pre_emph) to be around 1\n\t                    #segment=10.0**(scale) *segment\n\t                    yield  segment\n\t            else:\n\t                pass\n"]}
{"filename": "datasets/audiofolder.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\timport os\n\timport numpy as np\n\timport zipfile\n", "#import PIL.Image\n\timport json\n\timport torch\n\timport utils.dnnlib as dnnlib\n\timport random\n\timport pandas as pd\n\timport glob\n\timport soundfile as sf\n\t#try:\n\t#    import pyspng\n", "#except ImportError:\n\t#    pyspng = None\n\t#----------------------------------------------------------------------------\n\t# Dataset subclass that loads images recursively from the specified directory\n\t# or ZIP file.\n\tclass AudioFolderDataset(torch.utils.data.IterableDataset):\n\t    def __init__(self,\n\t        dset_args,\n\t        fs=44100,\n\t        seg_len=131072,\n", "        overfit=False,\n\t        seed=42 ):\n\t        self.overfit=overfit\n\t        super().__init__()\n\t        random.seed(seed)\n\t        np.random.seed(seed)\n\t        path=dset_args.path\n\t        filelist=glob.glob(os.path.join(path,\"*.wav\"))\n\t        assert len(filelist)>0 , \"error in dataloading: empty or nonexistent folder\"\n\t        self.train_samples=filelist\n", "        self.seg_len=int(seg_len)\n\t        self.fs=fs\n\t        if self.overfit:\n\t            file=self.train_samples[0]\n\t            data, samplerate = sf.read(file)\n\t            if len(data.shape)>1 :\n\t                data=np.mean(data,axis=1)\n\t            self.overfit_sample=data[10*samplerate:60*samplerate] #use only 50s\n\t    def __iter__(self):\n\t        if self.overfit:\n", "           data_clean=self.overfit_sample\n\t        while True:\n\t            if not self.overfit:\n\t                num=random.randint(0,len(self.train_samples)-1)\n\t                #for file in self.train_samples:  \n\t                file=self.train_samples[num]\n\t                data, samplerate = sf.read(file)\n\t                assert(samplerate==self.fs, \"wrong sampling rate\")\n\t                data_clean=data\n\t                #Stereo to mono\n", "                if len(data.shape)>1 :\n\t                    data_clean=np.mean(data_clean,axis=1)\n\t            #normalize\n\t            #no normalization!!\n\t            #data_clean=data_clean/np.max(np.abs(data_clean))\n\t            #framify data clean files\n\t            num_frames=np.floor(len(data_clean)/self.seg_len) \n\t            #if num_frames>4:\n\t            for i in range(8):\n\t                #get 8 random batches to be a bit faster\n", "                if not self.overfit:\n\t                    idx=np.random.randint(0,len(data_clean)-self.seg_len)\n\t                else:\n\t                    idx=0\n\t                segment=data_clean[idx:idx+self.seg_len]\n\t                segment=segment.astype('float32')\n\t                #b=np.mean(np.abs(segment))\n\t                #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n\t                #let's make this shit a bit robust to input scale\n\t                #scale=np.random.uniform(1.75,2.25)\n", "                #this way I estimage sigma_data (after pre_emph) to be around 1\n\t                #segment=10.0**(scale) *segment\n\t                yield  segment\n\t            #else:\n\t            #    pass\n"]}
{"filename": "datasets/cocochorales.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\timport os\n\timport numpy as np\n\timport zipfile\n", "#import PIL.Image\n\timport json\n\timport torch\n\timport utils.dnnlib as dnnlib\n\timport random\n\timport pandas as pd\n\timport glob\n\timport soundfile as sf\n\t#try:\n\t#    import pyspng\n", "#except ImportError:\n\t#    pyspng = None\n\t#----------------------------------------------------------------------------\n\t# Dataset subclass that loads images recursively from the specified directory\n\t# or ZIP file.\n\tclass AudioFolderDataset(torch.utils.data.IterableDataset):\n\t    def __init__(self,\n\t        dset_args,\n\t        fs=44100,\n\t        seg_len=131072,\n", "        overfit=False,\n\t        seed=42 ):\n\t        self.overfit=overfit\n\t        super().__init__()\n\t        random.seed(seed)\n\t        np.random.seed(seed)\n\t        path=dset_args.path\n\t        self.dset_args=dset_args\n\t        filelist=glob.glob(os.path.join(path,\"*/\"))\n\t        assert len(filelist)>0 , \"error in dataloading: empty or nonexistent folder\"\n", "        self.train_samples=filelist\n\t        self.seg_len=int(seg_len)\n\t        self.fs=fs\n\t        if self.overfit:\n\t            raise NotImplementedError\n\t            file=self.train_samples[0]\n\t            data, samplerate = sf.read(file)\n\t            if len(data.shape)>1 :\n\t                data=np.mean(data,axis=1)\n\t            self.overfit_sample=data[10*samplerate:60*samplerate] #use only 50s\n", "    def load_audio_file(self,file):\n\t                data, samplerate = sf.read(file)\n\t                assert(samplerate==self.fs, \"wrong sampling rate\")\n\t                data_clean=data\n\t                #Stereo to mono\n\t                if len(data.shape)>1 :\n\t                    data_clean=np.mean(data_clean,axis=1)\n\t                return data_clean\n\t    def __iter__(self):\n\t        if self.overfit:\n", "           data_clean=self.overfit_sample\n\t        while True:\n\t            if not self.overfit:\n\t                num=random.randint(0,len(self.train_samples)-1)\n\t                #for file in self.train_samples:  \n\t                file=self.train_samples[num]\n\t                audio=np.zeros(self.seg_len)\n\t                #get random number between 0 and 1\n\t                rand_num=random.random()\n\t                if rand_num<self.dset_args.prob_quartet:\n", "                    #load the 4 stems\n\t                    print(\"load 4 stems\")\n\t                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n\t                    if not( len(stems)==4):\n\t                         \"error in dataloading: wrong number of stems\"\n\t                    audio=[]\n\t                    print(stems)\n\t                    for s in stems:\n\t                        audio+=[self.load_audio_file(s)]\n\t                elif rand_num<self.dset_args.prob_quartet+self.dset_args.prob_trio:\n", "                    #load 3 stems\n\t                    print(\"load 3 stems\")\n\t                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n\t                    #assert len(stems)==4, \"error in dataloading: wrong number of stems\"\n\t                    #remove one random stem\n\t                    stems.pop(random.randrange(len(stems)))\n\t                    #assert len(stems)==3, \"error in dataloading: wrong number of stems\"\n\t                    print(stems)\n\t                    audio=[]\n\t                    for s in stems:\n", "                        audio+=[self.load_audio_file(s)]\n\t                elif rand_num<self.dset_args.prob_quartet+self.dset_args.prob_trio+self.dset_args.prob_duo:\n\t                    #load 2 stems\n\t                    print(\"load 2 stems\")\n\t                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n\t                    #assert len(stems)==4, \"error in dataloading: wrong number of stems\"\n\t                    #remove two random stems\n\t                    stems.pop(random.randrange(len(stems)))\n\t                    stems.pop(random.randrange(len(stems)))\n\t                    #assert len(stems)==2, \"error in dataloading: wrong number of stems\"\n", "                    audio=[]\n\t                    print(stems)\n\t                    for s in stems:\n\t                        audio+=[self.load_audio_file(s)]\n\t                else:\n\t                    #load 1 stem\n\t                    print(\"load 1 stem\")\n\t                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n\t                    #assert len(stems)==4, \"error in dataloading: wrong number of stems\"\n\t                    #remove two random stems\n", "                    stems.pop(random.randrange(len(stems)))\n\t                    stems.pop(random.randrange(len(stems)))\n\t                    stems.pop(random.randrange(len(stems)))\n\t                    #assert len(stems)==1, \"error in dataloading: wrong number of stems\"\n\t                    audio=[]\n\t                    print(stems)\n\t                    for s in stems:\n\t                        audio+=[self.load_audio_file(s)]\n\t            #normalize\n\t            #no normalization!!\n", "            #data_clean=data_clean/np.max(np.abs(data_clean))\n\t            #framify data clean files\n\t            num_frames=np.floor(len(audio[0])/self.seg_len) \n\t            #if num_frames>4:\n\t            for i in range(8):\n\t                #get 8 random batches to be a bit faster\n\t                if not self.overfit:\n\t                    idx=np.random.randint(0,len(audio[0])-self.seg_len)\n\t                else:\n\t                    idx=0\n", "                segment=audio[0][idx:idx+self.seg_len]\n\t                if len(audio)>1:\n\t                    for d in audio[1:]:\n\t                        try:\n\t                            segment+=d[idx:idx+self.seg_len]\n\t                        except:\n\t                            pass\n\t                segment=segment.astype('float32')\n\t                #b=np.mean(np.abs(segment))\n\t                #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n", "                #let's make this shit a bit robust to input scale\n\t                #scale=np.random.uniform(1.75,2.25)\n\t                #this way I estimage sigma_data (after pre_emph) to be around 1\n\t                #segment=10.0**(scale) *segment\n\t                yield  segment\n\t            #else:\n\t            #    pass\n"]}
{"filename": "datasets/maestro_dataset_test.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\timport os\n\timport numpy as np\n\timport zipfile\n", "#import PIL.Image\n\timport json\n\timport torch\n\timport utils.dnnlib as dnnlib\n\timport random\n\timport pandas as pd\n\timport glob\n\timport soundfile as sf\n\t#try:\n\t#    import pyspng\n", "#except ImportError:\n\t#    pyspng = None\n\t#----------------------------------------------------------------------------\n\t# Dataset subclass that loads images recursively from the specified directory\n\t# or ZIP file.\n\tclass MaestroDatasetTestChunks(torch.utils.data.Dataset):\n\t    def __init__(self,\n\t        dset_args,\n\t        num_samples=4,\n\t        seed=42 ):\n", "        super().__init__()\n\t        random.seed(seed)\n\t        np.random.seed(seed)\n\t        path=dset_args.path\n\t        years=dset_args.years\n\t        self.seg_len=int(dset_args.load_len)\n\t        metadata_file=os.path.join(path,\"maestro-v3.0.0.csv\")\n\t        metadata=pd.read_csv(metadata_file)\n\t        metadata=metadata[metadata[\"year\"].isin(years)]\n\t        metadata=metadata[metadata[\"split\"]==\"test\"]\n", "        filelist=metadata[\"audio_filename\"]\n\t        filelist=filelist.map(lambda x:  os.path.join(path,x)     , na_action='ignore')\n\t        self.filelist=filelist.to_list()\n\t        self.test_samples=[]\n\t        self.filenames=[]\n\t        self.f_s=[]\n\t        for i in range(num_samples):\n\t            file=self.filelist[i]\n\t            self.filenames.append(os.path.basename(file))\n\t            data, samplerate = sf.read(file)\n", "            if len(data.shape)>1 :\n\t                data=np.mean(data,axis=1)\n\t            self.test_samples.append(data[10*samplerate:10*samplerate+self.seg_len]) #use only 50s\n\t            self.f_s.append(samplerate)\n\t    def __getitem__(self, idx):\n\t        return self.test_samples[idx], self.f_s[idx], self.filenames[idx]\n\t    def __len__(self):\n\t        return len(self.test_samples)\n"]}
{"filename": "testing/blind_bwe_tester_mushra.py", "chunked_list": ["from datetime import date\n\timport pickle\n\timport re\n\timport torch\n\timport torchaudio\n\t#from src.models.unet_cqt import Unet_CQT\n\t#from src.models.unet_stft import Unet_STFT\n\t#from src.models.unet_1d import Unet_1d\n\t#import src.utils.setup as utils_setup\n\t#from src.sde import  VE_Sde_Elucidating\n", "import numpy as np\n\timport utils.dnnlib as dnnlib\n\timport os\n\timport utils.logging as utils_logging\n\timport wandb\n\timport copy\n\tfrom glob import glob\n\tfrom tqdm import tqdm\n\timport utils.bandwidth_extension as utils_bwe\n\timport omegaconf\n", "#import utils.filter_generation_utils as f_utils\n\timport utils.blind_bwe_utils as blind_bwe_utils\n\timport utils.training_utils as t_utils\n\timport soundfile as sf\n\t#from utils.spectral_analysis import LTAS_processor\n\tclass BlindTester():\n\t    def __init__(\n\t        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n\t    ):\n\t        self.args=args\n", "        self.network=torch.compile(network)\n\t        #self.network=network\n\t        #prnt number of parameters\n\t        self.diff_params=copy.copy(diff_params)\n\t        self.device=device\n\t        #choose gpu as the device if possible\n\t        if self.device is None:\n\t            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        self.network=network\n\t        torch.backends.cudnn.benchmark = True\n", "        today=date.today() \n\t        if it is None:\n\t            self.it=0\n\t        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n\t        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n\t        if not os.path.exists(self.path_sampling):\n\t            os.makedirs(self.path_sampling)\n\t        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n\t        self.setup_sampler()\n\t        self.use_wandb=False #hardcoded for now\n", "        S=self.args.exp.resample_factor\n\t        if S>2.1 and S<2.2:\n\t            #resampling 48k to 22.05k\n\t            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n\t        elif S!=1:\n\t            N=int(self.args.exp.audio_len*S)\n\t            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\t        if test_set is not None:\n\t            self.test_set=test_set\n\t            self.do_inpainting=True\n", "            self.do_bwe=True\n\t            self.do_blind_bwe=True\n\t        else:\n\t            self.test_set=None\n\t            self.do_inpainting=False\n\t            self.do_bwe=False #these need to be set up in the config file\n\t            self.do_blind_bwe=False\n\t        self.paths={}\n\t        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n\t            self.do_inpainting=True\n", "            mode=\"inpainting\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n\t            #TODO add more information in the subirectory names\n\t        else: self.do_inpainting=False\n\t        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n\t            self.do_bwe=True\n\t            mode=\"bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n\t            #TODO add more information in the subirectory names\n\t        else:\n", "            self.do_bwe=False\n\t        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n\t            self.do_blind_bwe=True\n\t            mode=\"blind_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n\t            #TODO add more information in the subirectory names\n\t        if \"real_blind_bwe\" in self.args.tester.modes:\n\t            self.do_blind_bwe=True\n\t            mode=\"real_blind_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n", "            #TODO add more information in the subirectory names\n\t        if \"formal_test_bwe\" in self.args.tester.modes:\n\t            self.do_formal_test_bwe=True\n\t            mode=\"formal_test_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n\t        if (\"unconditional\" in self.args.tester.modes):\n\t            mode=\"unconditional\"\n\t            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\t        if (\"filter_bwe\" in self.args.tester.modes):\n\t            mode=\"filter_bwe\"\n", "            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\t        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n\t        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\t    def prepare_unc_experiment(self, str):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            return path_exp\n\t    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n", "            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n\t            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n\t                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n\t            #ensure the path exists\n\t            if not os.path.exists(path_original):\n", "                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n\t            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n\t            return path_exp, path_degraded, path_original, path_reconstructed\n\t    def resample_audio(self, audio, fs):\n\t        #this has been reused from the trainer.py\n\t        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n", "    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n\t            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n\t                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n", "            #ensure the path exists\n\t            if not os.path.exists(path_original):\n\t                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n\t            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n\t            n=str_degraded_estimate\n\t            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n", "            #ensure the path exists\n\t            if not os.path.exists(path_degraded_estimate):\n\t                os.makedirs(path_degraded_estimate)\n\t            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\t    def setup_wandb(self):\n\t        \"\"\"\n\t        Configure wandb, open a new run and log the configuration.\n\t        \"\"\"\n\t        config=omegaconf.OmegaConf.to_container(\n\t            self.args, resolve=True, throw_on_missing=True\n", "        )\n\t        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n\t        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n\t        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\t        self.use_wandb=True\n\t    def setup_wandb_run(self, run):\n\t        #get the wandb run object from outside (in trainer.py or somewhere else)\n\t        self.wandb_run=run\n\t        self.use_wandb=True\n\t    def setup_sampler(self):\n", "        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\t    def load_latest_checkpoint(self ):\n\t        #load the latest checkpoint from self.args.model_dir\n\t        try:\n\t            # find latest checkpoint_id\n\t            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n\t            save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t            list_weights = glob(save_name)\n\t            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n\t            list_ids = [int(id_regex.search(weight_path).groups()[0])\n", "                        for weight_path in list_weights]\n\t            checkpoint_id = max(list_ids)\n\t            state_dict = torch.load(\n\t                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n\t            self.network.load_state_dict(state_dict['ema'])\n\t            print(f\"Loaded checkpoint {checkpoint_id}\")\n\t            return True\n\t        except (FileNotFoundError, ValueError):\n\t            raise ValueError(\"No checkpoint found\")\n\t    def load_checkpoint(self, path):\n", "        state_dict = torch.load(path, map_location=self.device)\n\t        if self.args.exp.exp_name==\"diffwave-sr\":\n\t            print(state_dict.keys())\n\t            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n\t            self.network.load_state_dict(state_dict['ema_model'])\n\t            self.network.eval()\n\t            print(\"ckpt loaded\")\n\t        else:\n\t            try:\n\t                print(\"load try 1\")\n", "                self.network.load_state_dict(state_dict['ema'])\n\t            except:\n\t                #self.network.load_state_dict(state_dict['model'])\n\t                try:\n\t                    print(\"load try 2\")\n\t                    dic_ema = {}\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n\t                        dic_ema[key] = tensor\n\t                    self.network.load_state_dict(dic_ema)\n\t                except:\n", "                    print(\"load try 3\")\n\t                    dic_ema = {}\n\t                    i=0\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n\t                        if tensor.requires_grad:\n\t                            dic_ema[key]=state_dict['ema_weights'][i]\n\t                            i=i+1\n\t                        else:\n\t                            dic_ema[key]=tensor     \n\t                    self.network.load_state_dict(dic_ema)\n", "        try:\n\t            self.it=state_dict['it']\n\t        except:\n\t            self.it=0\n\t    def log_filter(self,preds, f, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n\t        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\t        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\t    def log_audio(self,preds, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n", "        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n\t        print(audio_path)\n\t        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n\t        #TODO: log spectrogram of the audio file to wandb\n\t        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\t    def sample_unconditional_diffwavesr(self):\n\t        #print some parameters of self.network\n\t        #print(\"self.network\", self.network.input_projection[0].weight)\n\t        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n", "        #TODO assert that the audio_len is consistent with the model\n\t        rid=False\n\t        z_1=torch.randn(shape, device=self.device)\n\t        #print(\"sd\",z_1.std(-1))\n\t        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n\t        preds=outputs\n\t        self.log_audio(preds.detach(), \"unconditional\")\n\t        return preds\n\t    def sample_unconditional(self):\n\t        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n", "        #TODO assert that the audio_len is consistent with the model\n\t        rid=False\n\t        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n\t        if rid:\n\t            preds, data_denoised, t=outputs\n\t            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n\t        else:\n\t            preds=outputs\n\t        self.log_audio(preds, \"unconditional\")\n\t        return preds\n", "    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False):\n\t        print(\"BLIND\", blind)\n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n\t        test_bwe_table_audio = wandb.Table(columns=columns)\n\t        if not self.do_formal_test_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n", "        if typefilter==\"fc_A\":\n\t            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        elif typefilter==\"3rdoct\":\n\t            type=\"3rdoct\"\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n\t            da_filter=da_filter.to(self.device)\n\t        else:\n\t            type=self.args.tester.bandwidth_extension.filter.type\n", "            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n\t            da_filter=da_filter.to(self.device)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.formal_test.path\n\t        filenames=glob(path+\"/*.wav\")\n\t        segL=self.args.exp.audio_len\n\t        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\t        for filename in filenames:\n\t            path, basename=os.path.split(filename)\n", "            print(path, basename)\n\t            #open audio file\n\t            d,fs=sf.read(filename)\n\t            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n\t            print(\"D\", D.shape, fs)\n\t            path_out=self.args.tester.formal_test.folder\n\t            print(\"skippint?\", os.path.join(path_out, basename))\n\t            if os.path.exists(os.path.join(path_out, basename)):\n\t                print(\"yes skippint\", os.path.join(path_out, basename))\n\t                continue\n", "            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n\t            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n\t                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n\t                continue\n\t            if type==\"fc_A\":\n\t                degraded=self.apply_lowpass_fcA(D, da_filter)\n\t            else:\n\t                degraded=self.apply_low_pass(D, da_filter, type)\n\t            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\t            print(\"filename\",filename)\n", "            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n\t            n=os.path.splitext(os.path.basename(filename))[0]\n\t            #degraded=degraded.float().to(self.device).unsqueeze(0)\n\t            print(n)\n\t            final_pred=torch.zeros_like(degraded)\n\t            print(\"dsds FS\",fs)\n\t            print(\"seg shape\",degraded.shape)\n\t            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t            print(\"seg shape\",degraded.shape)\n\t            std= degraded.std(-1)\n", "            rid=False\n\t            L=degraded.shape[-1]\n\t            #modify the sampler, so that it is computationally cheaper\n\t            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n\t            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\t            #first segment\n\t            ix=0\n\t            seg=degraded[...,ix:ix+segL]\n\t            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t            filter_data=[]\n", "            rid=False\n\t            if blind:\n\t                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                pred, estimated_filter =outputs\n\t                filter_data.append(((ix, ix+segL), estimated_filter))\n\t            else:\n\t                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False)\n\t            if self.args.tester.formal_test.use_AR:\n\t                assert not blind\n\t                previous_pred=pred[..., 0:segL-discard_end]\n", "                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t                ix+=segL-overlap-discard_end\n\t                y_masked=torch.zeros_like(pred, device=self.device)\n\t                mask=torch.ones_like(seg, device=self.device)\n\t                mask[...,overlap::]=0\n\t            else:\n\t                print(\"noar\")\n\t                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n\t                win_pred=pred[...,0:segL-discard_end]\n\t                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n", "                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n\t                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\t                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\t            if blind:\n\t                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                        pickle.dump(filter_data, f)\n\t            while ix<L-segL-discard_end-discard_start:\n\t                seg=degraded[...,ix:ix+segL]\n\t                if self.args.tester.formal_test.use_AR:\n", "                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n\t                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t                else:\n\t                    if blind:\n\t                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                        pred, estimated_filter =outputs\n\t                        filter_data.append(((ix, ix+segL), estimated_filter))\n\t                    else:\n\t                        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t                previous_pred_win=pred[..., 0:segL-discard_end]\n", "                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n\t                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n\t                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\t                #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n\t                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\t                path, basename=os.path.split(filename)\n\t                print(path, basename)\n\t                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\t                if self.args.tester.formal_test.use_AR:\n", "                    ix+=segL-overlap-discard_end\n\t                else:\n\t                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\t                if blind:\n\t                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                        pickle.dump(filter_data, f)\n\t            #skipping the last segment, which is not complete, I am lazy\n\t            seg=degraded[...,ix::]\n\t            if self.args.tester.formal_test.use_AR:\n\t                y_masked[...,0:overlap]=pred[...,-overlap::]\n", "            if seg.shape[-1]<segL:\n\t                #cat zeros\n\t                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\t                if self.args.tester.formal_test.use_AR:\n\t                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t                    mask[...,seg.shape[-1]:segL]=0\n\t            else:\n\t                seg_zp=seg[...,0:segL]\n\t            if self.args.tester.formal_test.use_AR:\n", "                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t            else:\n\t                if blind:\n\t                    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n\t                    pred, estimated_filter =outputs\n\t                    filter_data.append(((ix, ix+segL), estimated_filter))\n\t                else:\n\t                    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t            if not self.args.tester.formal_test.use_AR:\n\t                win_pred=pred[...,0:seg.shape[-1]]\n", "                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n\t                final_pred[...,ix::]+=win_pred\n\t            else:\n\t                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t            #final_pred=final_pred*10**(-scale/20)\n\t            #extract path from filename\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n\t            #save filter_data in a pickle file\n\t            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n", "                pickle.dump(filter_data, f)\n\t    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n\t        test_bwe_table_audio = wandb.Table(columns=columns)\n\t        if not self.do_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n", "        if typefilter==\"fc_A\":\n\t            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        elif typefilter==\"3rdoct\":\n\t            type=\"3rdoct\"\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n\t            da_filter=da_filter.to(self.device)\n\t        else:\n\t            type=self.args.tester.bandwidth_extension.filter.type\n", "            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n\t            da_filter=da_filter.to(self.device)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t            n=os.path.splitext(filename[0])[0]\n\t            seg=original.float().to(self.device)\n\t            seg=self.resample_audio(seg, fs)\n\t            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n\t            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n", "            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t            #        #add gain boost (in dB)\n\t            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\t            if type==\"fc_A\":\n\t                y=self.apply_lowpass_fcA(seg, da_filter)\n\t            else:\n\t                y=self.apply_low_pass(seg, da_filter, type)\n\t            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\t            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n\t            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n", "            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n\t                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n\t                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t            print(\"y\", y.shape)\n\t            if test_filter_fit:\n\t                if compute_sweep:\n", "                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n\t                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n\t                    #save the data_norms and data_grads as a .npy file\n\t                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                else:\n\t                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n\t                    pred, data_denoised, data_score, t, data_filters =out\n\t            else:\n\t                rid=True\n", "                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n\t                pred, data_denoised, data_score, t =out\n\t            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n\t            #    #compensate gain boost\n\t            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t            #    #add gain boost (in dB)\n\t            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            res[i,:]=pred\n", "            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\t            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\t            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\t            test_bwe_table_audio.add_data(i, \n\t                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t            if rid:\n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n", "            if test_filter_fit:\n\t                #expecting to crash here\n\t                print(data_filters.shape)\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\t        if self.use_wandb:\n\t            self.log_audio(res, \"bwe\")\n\t    def apply_low_pass(self, seg, filter, typefilter):\n\t        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n\t        return y\n", "    def apply_lowpass_fcA(self, seg, params):\n\t        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n\t        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n\t        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n\t        return xfilt\n\t    def prepare_filter(self, sample_rate, typefilter):\n\t        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n\t        return filter\n\t    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #raise NotImplementedError\n", "        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        filename=self.args.tester.complete_recording.path\n", "        d,fs=sf.read(filename)\n\t        degraded=torch.Tensor(d)\n\t        segL=self.args.exp.audio_len\n\t        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\t        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\t        print(\"filename\",filename)\n\t        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t        degraded=degraded.float().to(self.device).unsqueeze(0)\n\t        print(n)\n\t        print(\"dsds FS\",fs)\n", "        print(\"seg shape\",degraded.shape)\n\t        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t        print(\"seg shape\",degraded.shape)\n\t        std= degraded.std(-1)\n\t        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n\t        #add noise\n\t        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n\t            #contaminate a bit with white noise\n\t            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n\t            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n", "            sigma=torch.sqrt(sigma2_s/SNR)\n\t            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\t        if self.args.tester.complete_recording.n_segments_blindstep==1:\n\t            y=degraded[...,ix_first:ix_first+segL]\n\t        else:\n\t            #initialize y with the first segment and repeat it\n\t            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n\t            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n\t                #random index\n\t                ix=np.random.randint(0, degraded.shape[-1]-segL)\n", "                y[j,...]=degraded[...,ix:ix+segL]\n\t        print(\"y shape\",y.shape)\n\t        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n\t        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\t        #y=y*10**(scale/20)\n\t        #degraded=degraded*10**(scale/20)\n\t        rid=False\n\t        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t        pred, estimated_filter =outputs\n\t        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n", "        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\t        hop=segL-overlap\n\t        final_pred=torch.zeros_like(degraded)\n\t        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\t        path, basename=os.path.split(filename)\n\t        print(path, basename)\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t        L=degraded.shape[-1]\n\t        #modify the sampler, so that it is computationally cheaper\n\t        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n", "        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\t        #first segment\n\t        ix=0\n\t        seg=degraded[...,ix:ix+segL]\n\t        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t        previous_pred=pred[..., 0:segL-discard_end]\n\t        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t        ix+=segL-overlap-discard_end\n\t        y_masked=torch.zeros_like(pred, device=self.device)\n\t        mask=torch.ones_like(seg, device=self.device)\n", "        mask[...,overlap::]=0\n\t        hann_window=torch.hann_window(overlap*2, device=self.device)\n\t        while ix<L-segL-discard_end-discard_start:\n\t            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n\t            seg=degraded[...,ix:ix+segL]\n\t            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t            previous_pred=pred[..., 0:segL-discard_end]\n\t            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t            #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n", "            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\t            path, basename=os.path.split(filename)\n\t            print(path, basename)\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t            ix+=segL-overlap-discard_end\n\t        #skipping the last segment, which is not complete, I am lazy\n\t        seg=degraded[...,ix::]\n\t        y_masked[...,0:overlap]=pred[...,-overlap::]\n\t        if seg.shape[-1]<segL:\n\t            #cat zeros\n", "            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\t            #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t            mask[...,seg.shape[-1]:segL]=0\n\t        else:\n\t            seg_zp=seg[...,0:segL]\n\t        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t        #final_pred=final_pred*10**(-scale/20)\n", "        #extract path from filename\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n", "            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n\t            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n", "            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.blind_bwe.real_recordings.path\n\t        audio_files=glob(path+\"/*.wav\")\n\t        print(audio_files, path)\n", "        test_set_data=[]\n\t        test_set_fs=[]\n\t        test_set_names=[]\n\t        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n\t            d,fs=sf.read(audio_files[i])\n\t            #print(d.shape, self.args.exp.audio_len)\n\t            #if d.shape[-1] >= self.args.exp.audio_len:\n\t            #    d=d[...,0:self.args.exp.audio_len]\n\t            test_set_data.append(torch.Tensor(d))\n\t            test_set_fs.append(fs)\n", "            print(\"fs\",fs)\n\t            print(\"len\",len(d))\n\t            test_set_names.append(audio_files[i])\n\t        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n\t                print(\"filename\",filename)\n\t                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t                seg=degraded.float().to(self.device).unsqueeze(0)\n\t                print(n)\n\t                print(\"dsds FS\",fs)\n\t                print(\"seg shape\",seg.shape)\n", "                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n\t                print(\"seg shape\",seg.shape)\n\t                ix_start=self.args.tester.blind_bwe\n\t                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n\t                y=seg\n\t                print(\"y shape\",y.shape)\n\t                #normalize???\n\t                std= y.std(-1)\n\t                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n", "                #print(\"scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n\t                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n", "                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n", "                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n\t                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pred, estimated_filter =outputs\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n", "                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n", "                #y=y*10**(-scale/20)\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n\t                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n\t                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t                if typefilter==\"fc_A\":\n", "                    test_blind_bwe_table_filters.add_data(i, \n\t                        wandb.Html(path_est_filter),\n\t                    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters)\n", "                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t    def test_blind_bwe(self, typefilter=\"firwin\", compute_sweep=False, blind=True):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n", "            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n\t            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n", "            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        if typefilter==\"fc_A\":\n\t            fc=self.args.tester.blind_bwe.test_filter.fc\n", "            A=self.args.tester.blind_bwe.test_filter.A\n\t            da_filter=torch.Tensor([fc, A]).to(self.device)\n\t        else:\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n\t            da_filter=da_filter.to(self.device)\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.blind_bwe.real_recordings.path\n\t        audio_files=glob(path+\"/*.wav\")\n", "        print(audio_files, path)\n\t        test_set_data=[]\n\t        test_set_fs=[]\n\t        test_set_names=[]\n\t        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n\t            d,fs=sf.read(audio_files[i])\n\t            #print(d.shape, self.args.exp.audio_len)\n\t            #if d.shape[-1] >= self.args.exp.audio_len:\n\t            #    d=d[...,0:self.args.exp.audio_len]\n\t            test_set_data.append(torch.Tensor(d))\n", "            test_set_fs.append(fs)\n\t            print(\"fs\",fs)\n\t            print(\"len\",len(d))\n\t            test_set_names.append(audio_files[i])\n\t        for i,( original,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n\t        #for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t                n=os.path.basename(filename)+typefilter\n\t                print(\"n\",n, filename)\n\t                seg=original.float().to(self.device).unsqueeze(0)\n\t                #seg=self.resample_audio(seg, fs)\n", "                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n\t                #sigma_norm=0.07 #hardcoded\n\t                #orig_std=seg.std(-1)\n\t                #seg=sigma_norm*seg/orig_std\n\t                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                #    orig_std=seg.std(-1)\n\t                #    seg=sigma_norm*seg/orig_std\n\t                #elif self.args.tester.blind_bwe.gain_boost != 0:\n\t                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n", "                #    #add gain boost (in dB)\n\t                #    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n\t                #apply lowpass filter\n\t                print(seg.shape)\n\t                if typefilter==\"fc_A\":\n\t                    y=self.apply_lowpass_fcA(seg, da_filter)\n\t                else:\n\t                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\t                #add noise to the observations for regularization\n\t                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n", "                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n\t                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n\t                #print(\"applied scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n", "                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n\t                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                if blind:\n\t                    rid=True\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                else:\n\t                    rid=False\n\t                    pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=rid, test_filter_fit=False, compute_sweep=False)\n", "                if blind:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n", "                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n\t                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pass\n\t                #y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\t                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n", "                #    assert orig_std is not None\n\t                #seg=orig_std*seg/sigma_norm\n\t                #pred=orig_std*pred/sigma_norm\n\t                #elif self.args.tester.blind_bwe.gain_boost != 0:\n\t                #    #compensate gain boost #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.  #    #add gain boost (in dB) #    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20) #    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20) ##    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20) #    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20) \n\t                #y_est=y_est*10**(-scale/20)\n\t               # pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n\t                #y=y*10**(-scale/20)\n\t                #if self.use_wandb:\n", "                #add to principal wandb table\n\t                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n\t                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n\t                #path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n", "                #will probably crash here!\n\t                #fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                #path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                #fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                #test_blind_bwe_table_audio.add_data(i, \n\t                #        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                #        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                #        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n\t                #        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n\t                #if typefilter==\"fc_A\":\n", "                #    test_blind_bwe_table_filters.add_data(i, \n\t                #        wandb.Html(path_est_filter),\n\t                #    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n\t                #print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                #fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                #print(data_filters.shape)\n", "                #will crash here\n\t                #fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        #self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        #self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n\t    def dodajob(self):\n\t        #self.setup_wandb()\n\t        for m in self.args.tester.modes:\n", "            if m==\"unconditional\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional()\n\t            if m==\"unconditional_diffwavesr\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional_diffwavesr()\n\t            self.it+=1\n\t            if m==\"blind_bwe\":\n\t                print(\"TESTING BLIND BWE\")\n\t                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep, typefilter=\"firwin\")\n", "            if m==\"real_blind_bwe\":\n\t                print(\"TESTING REAL BLIND BWE\")\n\t                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe_complete\":\n\t                #process the whole audio file\n\t                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n\t                print(\"TESTING REAL BLIND BWE COMPLETE\")\n\t                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n", "                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n\t            if m==\"formal_test_bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n\t        self.it+=1\n"]}
{"filename": "testing/blind_bwe_tester.py", "chunked_list": ["from datetime import date\n\timport pickle\n\timport re\n\timport torch\n\timport torchaudio\n\t#from src.models.unet_cqt import Unet_CQT\n\t#from src.models.unet_stft import Unet_STFT\n\t#from src.models.unet_1d import Unet_1d\n\t#import src.utils.setup as utils_setup\n\t#from src.sde import  VE_Sde_Elucidating\n", "import numpy as np\n\timport utils.dnnlib as dnnlib\n\timport os\n\timport utils.logging as utils_logging\n\timport wandb\n\timport copy\n\tfrom glob import glob\n\tfrom tqdm import tqdm\n\timport utils.bandwidth_extension as utils_bwe\n\timport omegaconf\n", "#import utils.filter_generation_utils as f_utils\n\timport utils.blind_bwe_utils as blind_bwe_utils\n\timport utils.training_utils as t_utils\n\timport soundfile as sf\n\t#from utils.spectral_analysis import LTAS_processor\n\tclass BlindTester():\n\t    def __init__(\n\t        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n\t    ):\n\t        self.args=args\n", "        self.network=torch.compile(network)\n\t        #self.network=network\n\t        #prnt number of parameters\n\t        self.diff_params=copy.copy(diff_params)\n\t        self.device=device\n\t        #choose gpu as the device if possible\n\t        if self.device is None:\n\t            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        self.network=network\n\t        torch.backends.cudnn.benchmark = True\n", "        today=date.today() \n\t        if it is None:\n\t            self.it=0\n\t        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n\t        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n\t        if not os.path.exists(self.path_sampling):\n\t            os.makedirs(self.path_sampling)\n\t        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n\t        self.setup_sampler()\n\t        self.use_wandb=False #hardcoded for now\n", "        S=self.args.exp.resample_factor\n\t        if S>2.1 and S<2.2:\n\t            #resampling 48k to 22.05k\n\t            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n\t        elif S!=1:\n\t            N=int(self.args.exp.audio_len*S)\n\t            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\t        if test_set is not None:\n\t            self.test_set=test_set\n\t            self.do_inpainting=True\n", "            self.do_bwe=True\n\t            self.do_blind_bwe=True\n\t        else:\n\t            self.test_set=None\n\t            self.do_inpainting=False\n\t            self.do_bwe=False #these need to be set up in the config file\n\t            self.do_blind_bwe=False\n\t        self.paths={}\n\t        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n\t            self.do_inpainting=True\n", "            mode=\"inpainting\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n\t            #TODO add more information in the subirectory names\n\t        else: self.do_inpainting=False\n\t        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n\t            self.do_bwe=True\n\t            mode=\"bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n\t            #TODO add more information in the subirectory names\n\t        else:\n", "            self.do_bwe=False\n\t        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n\t            self.do_blind_bwe=True\n\t            mode=\"blind_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n\t            #TODO add more information in the subirectory names\n\t        if \"real_blind_bwe\" in self.args.tester.modes:\n\t            self.do_blind_bwe=True\n\t            mode=\"real_blind_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n", "            #TODO add more information in the subirectory names\n\t        if \"formal_test_bwe\" in self.args.tester.modes:\n\t            self.do_formal_test_bwe=True\n\t            mode=\"formal_test_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n\t        if (\"unconditional\" in self.args.tester.modes):\n\t            mode=\"unconditional\"\n\t            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\t        if (\"filter_bwe\" in self.args.tester.modes):\n\t            mode=\"filter_bwe\"\n", "            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\t        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n\t        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\t    def prepare_unc_experiment(self, str):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            return path_exp\n\t    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n", "            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n\t            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n\t                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n\t            #ensure the path exists\n\t            if not os.path.exists(path_original):\n", "                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n\t            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n\t            return path_exp, path_degraded, path_original, path_reconstructed\n\t    def resample_audio(self, audio, fs):\n\t        #this has been reused from the trainer.py\n\t        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n", "    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n\t            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n\t                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n", "            #ensure the path exists\n\t            if not os.path.exists(path_original):\n\t                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n\t            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n\t            n=str_degraded_estimate\n\t            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n", "            #ensure the path exists\n\t            if not os.path.exists(path_degraded_estimate):\n\t                os.makedirs(path_degraded_estimate)\n\t            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\t    def setup_wandb(self):\n\t        \"\"\"\n\t        Configure wandb, open a new run and log the configuration.\n\t        \"\"\"\n\t        config=omegaconf.OmegaConf.to_container(\n\t            self.args, resolve=True, throw_on_missing=True\n", "        )\n\t        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n\t        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n\t        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\t        self.use_wandb=True\n\t    def setup_wandb_run(self, run):\n\t        #get the wandb run object from outside (in trainer.py or somewhere else)\n\t        self.wandb_run=run\n\t        self.use_wandb=True\n\t    def setup_sampler(self):\n", "        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\t    def load_latest_checkpoint(self ):\n\t        #load the latest checkpoint from self.args.model_dir\n\t        try:\n\t            # find latest checkpoint_id\n\t            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n\t            save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t            list_weights = glob(save_name)\n\t            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n\t            list_ids = [int(id_regex.search(weight_path).groups()[0])\n", "                        for weight_path in list_weights]\n\t            checkpoint_id = max(list_ids)\n\t            state_dict = torch.load(\n\t                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n\t            self.network.load_state_dict(state_dict['ema'])\n\t            print(f\"Loaded checkpoint {checkpoint_id}\")\n\t            return True\n\t        except (FileNotFoundError, ValueError):\n\t            raise ValueError(\"No checkpoint found\")\n\t    def load_checkpoint(self, path):\n", "        state_dict = torch.load(path, map_location=self.device)\n\t        if self.args.exp.exp_name==\"diffwave-sr\":\n\t            print(state_dict.keys())\n\t            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n\t            self.network.load_state_dict(state_dict['ema_model'])\n\t            self.network.eval()\n\t            print(\"ckpt loaded\")\n\t        else:\n\t            try:\n\t                print(\"load try 1\")\n", "                self.network.load_state_dict(state_dict['ema'])\n\t            except:\n\t                #self.network.load_state_dict(state_dict['model'])\n\t                try:\n\t                    print(\"load try 2\")\n\t                    dic_ema = {}\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n\t                        dic_ema[key] = tensor\n\t                    self.network.load_state_dict(dic_ema)\n\t                except:\n", "                    print(\"load try 3\")\n\t                    dic_ema = {}\n\t                    i=0\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n\t                        if tensor.requires_grad:\n\t                            dic_ema[key]=state_dict['ema_weights'][i]\n\t                            i=i+1\n\t                        else:\n\t                            dic_ema[key]=tensor     \n\t                    self.network.load_state_dict(dic_ema)\n", "        try:\n\t            self.it=state_dict['it']\n\t        except:\n\t            self.it=0\n\t    def log_filter(self,preds, f, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n\t        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\t        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\t    def log_audio(self,preds, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n", "        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n\t        print(audio_path)\n\t        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n\t        #TODO: log spectrogram of the audio file to wandb\n\t        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\t    def sample_unconditional_diffwavesr(self):\n\t        #print some parameters of self.network\n\t        #print(\"self.network\", self.network.input_projection[0].weight)\n\t        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n", "        #TODO assert that the audio_len is consistent with the model\n\t        rid=False\n\t        z_1=torch.randn(shape, device=self.device)\n\t        #print(\"sd\",z_1.std(-1))\n\t        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n\t        preds=outputs\n\t        self.log_audio(preds.detach(), \"unconditional\")\n\t        return preds\n\t    def sample_unconditional(self):\n\t        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n", "        #TODO assert that the audio_len is consistent with the model\n\t        rid=False\n\t        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n\t        if rid:\n\t            preds, data_denoised, t=outputs\n\t            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n\t        else:\n\t            preds=outputs\n\t        self.log_audio(preds, \"unconditional\")\n\t        return preds\n", "    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False, robustness=False):\n\t        print(\"BLIND\", blind)\n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n\t        test_bwe_table_audio = wandb.Table(columns=columns)\n\t        if not self.do_formal_test_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n", "        if typefilter==\"fc_A\":\n\t            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        elif typefilter==\"3rdoct\":\n\t            type=\"3rdoct\"\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n\t            da_filter=da_filter.to(self.device)\n\t        else:\n\t            type=self.args.tester.bandwidth_extension.filter.type\n", "            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n\t            da_filter=da_filter.to(self.device)\n\t        if robustness:\n\t            order=self.args.tester.formal_test.robustness_filter.order\n\t            fc=self.args.tester.formal_test.robustness_filter.fc\n\t            beta=self.args.tester.formal_test.robustness_filter.beta\n\t            da_other_filter=utils_bwe.get_FIR_lowpass(order,fc, beta,self.args.exp.sample_rate)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.formal_test.path\n", "        filenames=glob(path+\"/*.wav\")\n\t        segL=self.args.exp.audio_len\n\t        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\t        for filename in filenames:\n\t            path, basename=os.path.split(filename)\n\t            print(path, basename)\n\t            #open audio file\n\t            d,fs=sf.read(filename)\n\t            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n\t            print(\"D\", D.shape, fs)\n", "            path_out=self.args.tester.formal_test.folder\n\t            print(\"skippint?\", os.path.join(path_out, basename))\n\t            if os.path.exists(os.path.join(path_out, basename)):\n\t                print(\"yes skippint\", os.path.join(path_out, basename))\n\t                continue\n\t            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n\t            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n\t                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n\t                continue\n\t            if not(robustness):\n", "                if type==\"fc_A\":\n\t                    degraded=self.apply_lowpass_fcA(D, da_filter)\n\t                else:\n\t                    degraded=self.apply_low_pass(D, da_filter, type)\n\t            else:\n\t                print(\"test robustness, using a different filter\")\n\t                degraded=self.apply_low_pass(D, da_other_filter, type)\n\t            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\t            print(\"filename\",filename)\n\t            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n", "            n=os.path.splitext(os.path.basename(filename))[0]\n\t            #degraded=degraded.float().to(self.device).unsqueeze(0)\n\t            print(n)\n\t            final_pred=torch.zeros_like(degraded)\n\t            print(\"dsds FS\",fs)\n\t            print(\"seg shape\",degraded.shape)\n\t            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t            print(\"seg shape\",degraded.shape)\n\t            std= degraded.std(-1)\n\t            rid=False\n", "            L=degraded.shape[-1]\n\t            #modify the sampler, so that it is computationally cheaper\n\t            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n\t            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\t            #first segment\n\t            ix=0\n\t            seg=degraded[...,ix:ix+segL]\n\t            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t            filter_data=[]\n\t            rid=False\n", "            if blind:\n\t                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                pred, estimated_filter =outputs\n\t                filter_data.append(((ix, ix+segL), estimated_filter))\n\t            else:\n\t                if robustness:\n\t                        pred=self.sampler.predict_bwe(seg, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\t                else:\n\t                        pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\t            if self.args.tester.formal_test.use_AR:\n", "                assert not blind\n\t                previous_pred=pred[..., 0:segL-discard_end]\n\t                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t                ix+=segL-overlap-discard_end\n\t                y_masked=torch.zeros_like(pred, device=self.device)\n\t                mask=torch.ones_like(seg, device=self.device)\n\t                mask[...,overlap::]=0\n\t            else:\n\t                print(\"noar\")\n\t                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n", "                win_pred=pred[...,0:segL-discard_end]\n\t                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n\t                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n\t                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\t                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\t            if blind:\n\t                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                        pickle.dump(filter_data, f)\n\t            while ix<L-segL-discard_end-discard_start:\n", "                seg=degraded[...,ix:ix+segL]\n\t                if self.args.tester.formal_test.use_AR:\n\t                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n\t                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t                else:\n\t                    if blind:\n\t                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                        pred, estimated_filter =outputs\n\t                        filter_data.append(((ix, ix+segL), estimated_filter))\n\t                    else:\n", "                        if robustness:\n\t                                pred=self.sampler.predict_bwe(seg, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\t                        else:\n\t                                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\t                #if blind:\n\t                #        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                #        pred, estimated_filter =outputs\n\t                #        filter_data.append(((ix, ix+segL), estimated_filter))\n\t                #    else:\n\t                #        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n", "                previous_pred_win=pred[..., 0:segL-discard_end]\n\t                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n\t                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n\t                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\t                #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n\t                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\t                path, basename=os.path.split(filename)\n\t                print(path, basename)\n\t                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n", "                if self.args.tester.formal_test.use_AR:\n\t                    ix+=segL-overlap-discard_end\n\t                else:\n\t                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\t                if blind:\n\t                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                        pickle.dump(filter_data, f)\n\t            #skipping the last segment, which is not complete, I am lazy\n\t            seg=degraded[...,ix::]\n\t            if self.args.tester.formal_test.use_AR:\n", "                y_masked[...,0:overlap]=pred[...,-overlap::]\n\t            if seg.shape[-1]<segL:\n\t                #cat zeros\n\t                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\t                if self.args.tester.formal_test.use_AR:\n\t                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t                    mask[...,seg.shape[-1]:segL]=0\n\t            else:\n\t                seg_zp=seg[...,0:segL]\n", "            if self.args.tester.formal_test.use_AR:\n\t                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t            else:\n\t                if blind:\n\t                        outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n\t                        pred, estimated_filter =outputs\n\t                        filter_data.append(((ix, ix+segL), estimated_filter))\n\t                else:\n\t                        if robustness:\n\t                                pred=self.sampler.predict_bwe(seg_zp, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n", "                        else:\n\t                                pred=self.sampler.predict_bwe(seg_zp, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\t                #if blind:\n\t                #    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n\t                #    pred, estimated_filter =outputs\n\t                #    filter_data.append(((ix, ix+segL), estimated_filter))\n\t                #else:\n\t                #    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t            if not self.args.tester.formal_test.use_AR:\n\t                win_pred=pred[...,0:seg.shape[-1]]\n", "                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n\t                final_pred[...,ix::]+=win_pred\n\t            else:\n\t                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t            #final_pred=final_pred*10**(-scale/20)\n\t            #extract path from filename\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n\t            #save filter_data in a pickle file\n\t            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n", "                pickle.dump(filter_data, f)\n\t    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n\t        test_bwe_table_audio = wandb.Table(columns=columns)\n\t        if not self.do_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n", "        if typefilter==\"fc_A\":\n\t            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        elif typefilter==\"3rdoct\":\n\t            type=\"3rdoct\"\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n\t            da_filter=da_filter.to(self.device)\n\t        else:\n\t            type=self.args.tester.bandwidth_extension.filter.type\n", "            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n\t            da_filter=da_filter.to(self.device)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t            n=os.path.splitext(filename[0])[0]\n\t            seg=original.float().to(self.device)\n\t            seg=self.resample_audio(seg, fs)\n\t            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n\t            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n", "            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t            #        #add gain boost (in dB)\n\t            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\t            if type==\"fc_A\":\n\t                y=self.apply_lowpass_fcA(seg, da_filter)\n\t            else:\n\t                y=self.apply_low_pass(seg, da_filter, type)\n\t            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\t            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n\t            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n", "            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n\t                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n\t                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t            print(\"y\", y.shape)\n\t            if test_filter_fit:\n\t                if compute_sweep:\n", "                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n\t                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n\t                    #save the data_norms and data_grads as a .npy file\n\t                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                else:\n\t                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n\t                    pred, data_denoised, data_score, t, data_filters =out\n\t            else:\n\t                rid=True\n", "                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n\t                pred, data_denoised, data_score, t =out\n\t            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n\t            #    #compensate gain boost\n\t            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t            #    #add gain boost (in dB)\n\t            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            res[i,:]=pred\n", "            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\t            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\t            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\t            test_bwe_table_audio.add_data(i, \n\t                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t            if rid:\n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n", "            if test_filter_fit:\n\t                #expecting to crash here\n\t                print(data_filters.shape)\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\t        if self.use_wandb:\n\t            self.log_audio(res, \"bwe\")\n\t    def apply_low_pass(self, seg, filter, typefilter):\n\t        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n\t        return y\n", "    def apply_lowpass_fcA(self, seg, params):\n\t        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n\t        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n\t        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n\t        return xfilt\n\t    def prepare_filter(self, sample_rate, typefilter):\n\t        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n\t        return filter\n\t    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #raise NotImplementedError\n", "        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        filename=self.args.tester.complete_recording.path\n", "        d,fs=sf.read(filename)\n\t        degraded=torch.Tensor(d)\n\t        segL=self.args.exp.audio_len\n\t        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\t        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\t        print(\"filename\",filename)\n\t        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t        degraded=degraded.float().to(self.device).unsqueeze(0)\n\t        print(n)\n\t        print(\"dsds FS\",fs)\n", "        print(\"seg shape\",degraded.shape)\n\t        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t        print(\"seg shape\",degraded.shape)\n\t        std= degraded.std(-1)\n\t        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n\t        #add noise\n\t        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n\t            #contaminate a bit with white noise\n\t            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n\t            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n", "            sigma=torch.sqrt(sigma2_s/SNR)\n\t            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\t        if self.args.tester.complete_recording.n_segments_blindstep==1:\n\t            y=degraded[...,ix_first:ix_first+segL]\n\t        else:\n\t            #initialize y with the first segment and repeat it\n\t            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n\t            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n\t                #random index\n\t                ix=np.random.randint(0, degraded.shape[-1]-segL)\n", "                y[j,...]=degraded[...,ix:ix+segL]\n\t        print(\"y shape\",y.shape)\n\t        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n\t        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\t        #y=y*10**(scale/20)\n\t        #degraded=degraded*10**(scale/20)\n\t        rid=False\n\t        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t        pred, estimated_filter =outputs\n\t        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n", "        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\t        hop=segL-overlap\n\t        final_pred=torch.zeros_like(degraded)\n\t        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\t        path, basename=os.path.split(filename)\n\t        print(path, basename)\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t        L=degraded.shape[-1]\n\t        #modify the sampler, so that it is computationally cheaper\n\t        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n", "        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\t        #first segment\n\t        ix=0\n\t        seg=degraded[...,ix:ix+segL]\n\t        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t        previous_pred=pred[..., 0:segL-discard_end]\n\t        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t        ix+=segL-overlap-discard_end\n\t        y_masked=torch.zeros_like(pred, device=self.device)\n\t        mask=torch.ones_like(seg, device=self.device)\n", "        mask[...,overlap::]=0\n\t        hann_window=torch.hann_window(overlap*2, device=self.device)\n\t        while ix<L-segL-discard_end-discard_start:\n\t            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n\t            seg=degraded[...,ix:ix+segL]\n\t            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t            previous_pred=pred[..., 0:segL-discard_end]\n\t            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t            #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n", "            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\t            path, basename=os.path.split(filename)\n\t            print(path, basename)\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t            ix+=segL-overlap-discard_end\n\t        #skipping the last segment, which is not complete, I am lazy\n\t        seg=degraded[...,ix::]\n\t        y_masked[...,0:overlap]=pred[...,-overlap::]\n\t        if seg.shape[-1]<segL:\n\t            #cat zeros\n", "            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\t            #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t            mask[...,seg.shape[-1]:segL]=0\n\t        else:\n\t            seg_zp=seg[...,0:segL]\n\t        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t        #final_pred=final_pred*10**(-scale/20)\n", "        #extract path from filename\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n", "            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n\t            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n", "            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.blind_bwe.real_recordings.path\n\t        audio_files=glob(path+\"/*.wav\")\n\t        print(audio_files, path)\n", "        test_set_data=[]\n\t        test_set_fs=[]\n\t        test_set_names=[]\n\t        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n\t            d,fs=sf.read(audio_files[i])\n\t            #print(d.shape, self.args.exp.audio_len)\n\t            #if d.shape[-1] >= self.args.exp.audio_len:\n\t            #    d=d[...,0:self.args.exp.audio_len]\n\t            test_set_data.append(torch.Tensor(d))\n\t            test_set_fs.append(fs)\n", "            print(\"fs\",fs)\n\t            print(\"len\",len(d))\n\t            test_set_names.append(audio_files[i])\n\t        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n\t                print(\"filename\",filename)\n\t                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t                seg=degraded.float().to(self.device).unsqueeze(0)\n\t                print(n)\n\t                print(\"dsds FS\",fs)\n\t                print(\"seg shape\",seg.shape)\n", "                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n\t                print(\"seg shape\",seg.shape)\n\t                ix_start=self.args.tester.blind_bwe\n\t                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n\t                y=seg\n\t                print(\"y shape\",y.shape)\n\t                #normalize???\n\t                std= y.std(-1)\n\t                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n", "                #print(\"scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n\t                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n", "                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n", "                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n\t                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pred, estimated_filter =outputs\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n", "                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n", "                #y=y*10**(-scale/20)\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n\t                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n\t                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t                if typefilter==\"fc_A\":\n", "                    test_blind_bwe_table_filters.add_data(i, \n\t                        wandb.Html(path_est_filter),\n\t                    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters)\n", "                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n", "            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n\t            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n", "            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        if typefilter==\"fc_A\":\n\t            fc=self.args.tester.blind_bwe.test_filter.fc\n", "            A=self.args.tester.blind_bwe.test_filter.A\n\t            da_filter=torch.Tensor([fc, A]).to(self.device)\n\t        else:\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n\t            da_filter=da_filter.to(self.device)\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t                n=os.path.splitext(filename[0])[0]+typefilter\n", "                seg=original.float().to(self.device)\n\t                seg=self.resample_audio(seg, fs)\n\t                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                    orig_std=seg.std(-1)\n\t                    seg=sigma_norm*seg/orig_std\n\t                elif self.args.tester.blind_bwe.gain_boost != 0:\n\t                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t                    #add gain boost (in dB)\n\t                    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n", "                #apply lowpass filter\n\t                if typefilter==\"fc_A\":\n\t                    y=self.apply_lowpass_fcA(seg, da_filter)\n\t                else:\n\t                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\t                #add noise to the observations for regularization\n\t                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n\t                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n", "                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n\t                #print(\"applied scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n", "                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n", "                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n\t                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n", "                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pred, estimated_filter =outputs\n\t                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\t                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                    assert orig_std is not None\n\t                    seg=orig_std*seg/sigma_norm\n\t                elif self.args.tester.blind_bwe.gain_boost != 0:\n", "                    #compensate gain boost\n\t                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t                    #add gain boost (in dB)\n\t                    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n", "                #y=y*10**(-scale/20)\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n\t                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n", "                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n\t                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n\t                #will probably crash here!\n\t                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n", "                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n\t                #if typefilter==\"fc_A\":\n\t                #    test_blind_bwe_table_filters.add_data(i, \n\t                #        wandb.Html(path_est_filter),\n\t                #    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n", "                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters.shape)\n\t                #will crash here\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n\t    def dodajob(self):\n", "        #self.setup_wandb()\n\t        for m in self.args.tester.modes:\n\t            if m==\"unconditional\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional()\n\t            if m==\"unconditional_diffwavesr\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional_diffwavesr()\n\t            self.it+=1\n\t            if m==\"blind_bwe\":\n", "                print(\"TESTING BLIND BWE\")\n\t                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe\":\n\t                print(\"TESTING REAL BLIND BWE\")\n\t                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe_complete\":\n\t                #process the whole audio file\n\t                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n\t                print(\"TESTING REAL BLIND BWE COMPLETE\")\n\t                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n", "            if m==\"bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n\t            if m==\"formal_test_bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind, robustness=self.args.tester.formal_test.robustness)\n\t        self.it+=1\n"]}
{"filename": "testing/edm_sampler.py", "chunked_list": ["from tqdm import tqdm\n\timport torch\n\timport torchaudio\n\t#import scipy.signal\n\t#import numpy as np\n\tclass Sampler():\n\t    def __init__(self, model, diff_params, args, rid=False):\n\t        self.model = model\n\t        self.diff_params = diff_params #same as training, useful if we need to apply a wrapper or something\n\t        self.args=args\n", "        if not(self.args.tester.diff_params.same_as_training):\n\t            self.update_diff_params()\n\t        self.order=self.args.tester.order\n\t        self.xi=self.args.tester.posterior_sampling.xi\n\t         #hyperparameter for the reconstruction guidance\n\t        self.data_consistency=self.args.tester.posterior_sampling.data_consistency #use reconstruction gudance without replacement\n\t        self.nb_steps=self.args.tester.T\n\t        #self.treshold_on_grads=args.tester.inference.max_thresh_grads\n\t        self.rid=rid #this is for logging, ignore for now\n\t        #try:\n", "        #    self.stereo=self.args.tester.stereo\n\t        #except:\n\t        #    self.stereo=False\n\t    def update_diff_params(self):\n\t        #the parameters for testing might not be necesarily the same as the ones used for training\n\t        self.diff_params.sigma_min=self.args.tester.diff_params.sigma_min\n\t        self.diff_params.sigma_max =self.args.tester.diff_params.sigma_max\n\t        self.diff_params.ro=self.args.tester.diff_params.ro\n\t        self.diff_params.sigma_data=self.args.tester.diff_params.sigma_data\n\t        #par.diff_params.meters stochastic sampling\n", "        self.diff_params.Schurn=self.args.tester.diff_params.Schurn\n\t        self.diff_params.Stmin=self.args.tester.diff_params.Stmin\n\t        self.diff_params.Stmax=self.args.tester.diff_params.Stmax\n\t        self.diff_params.Snoise=self.args.tester.diff_params.Snoise\n\t    def data_consistency_step(self, x_hat, y, degradation):\n\t        \"\"\"\n\t        Simple replacement method, used for inpainting and FIR bwe\n\t        \"\"\"\n\t        #get reconstruction estimate\n\t        den_rec= degradation(x_hat)     \n", "        #apply replacment (valid for linear degradations)\n\t        return y+x_hat-den_rec \n\t    def get_score_rec_guidance(self, x, y, t_i, degradation):\n\t        x.requires_grad_()\n\t        x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\t        if self.args.tester.filter_out_cqt_DC_Nyq:\n\t            x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n\t        den_rec= degradation(x_hat) \n\t        if len(y.shape)==3:\n\t            dim=(1,2)\n", "        elif len(y.shape)==2:\n\t            dim=1\n\t        norm=torch.linalg.norm(y-den_rec,dim=dim, ord=2)\n\t        rec_grads=torch.autograd.grad(outputs=norm,\n\t                                      inputs=x)\n\t        rec_grads=rec_grads[0]\n\t        normguide=torch.linalg.norm(rec_grads)/self.args.exp.audio_len**0.5\n\t        #normalize scaling\n\t        s=self.xi/(normguide*t_i+1e-6)\n\t        #optionally apply a treshold to the gradients\n", "        if False:\n\t            #pply tresholding to the gradients. It is a dirty trick but helps avoiding bad artifacts \n\t            rec_grads=torch.clip(rec_grads, min=-self.treshold_on_grads, max=self.treshold_on_grads)\n\t        score=(x_hat.detach()-x)/t_i**2\n\t        #apply scaled guidance to the score\n\t        score=score-s*rec_grads\n\t        return score\n\t    def get_score(self,x, y, t_i, degradation):\n\t        if y==None:\n\t            assert degradation==None\n", "            #unconditional sampling\n\t            with torch.no_grad():\n\t                #print(\"In sampling\", x.shape, t_i.shape)\n\t                x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\t                if self.args.tester.filter_out_cqt_DC_Nyq:\n\t                    x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n\t                score=(x_hat-x)/t_i**2\n\t            return score\n\t        else:\n\t            if self.xi>0:\n", "                #apply rec. guidance\n\t                score=self.get_score_rec_guidance(x, y, t_i, degradation)\n\t                #optionally apply replacement or consistency step\n\t                if self.data_consistency:\n\t                    #convert score to denoised estimate using Tweedie's formula\n\t                    x_hat=score*t_i**2+x\n\t                    if self.args.inference.mode==\"phase_retrieval\":\n\t                        x_hat=self.data_consistency_step_phase_retrieval(x_hat,y)\n\t                    else:\n\t                        x_hat=self.data_consistency_step(x_hat,y, degradation)\n", "                    #convert back to score\n\t                    score=(x_hat-x)/t_i**2\n\t            else:\n\t                #denoised with replacement method\n\t                with torch.no_grad():\n\t                    x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\t                    x_hat=self.data_consistency_step(x_hat,y, degradation)\n\t                    score=(x_hat-x)/t_i**2\n\t            return score\n\t    def predict_unconditional(\n", "        self,\n\t        shape,  #observations (lowpssed signal) Tensor with shape ??\n\t        device\n\t    ):\n\t        self.y=None\n\t        self.degradation=None\n\t        return self.predict(shape, device)\n\t    def predict_resample(\n\t        self,\n\t        y,  #observations (lowpssed signal) Tensor with shape ??\n", "        shape,\n\t        degradation, #lambda function\n\t    ):\n\t        self.degradation=degradation \n\t        self.y=y\n\t        print(shape)\n\t        return self.predict(shape, y.device)\n\t    def predict_conditional(\n\t        self,\n\t        y,  #observations (lowpssed signal) Tensor with shape ??\n", "        degradation, #lambda function\n\t    ):\n\t        self.degradation=degradation \n\t        self.y=y\n\t        return self.predict(y.shape, y.device)\n\t    def predict(\n\t        self,\n\t        shape,  #observations (lowpssed signal) Tensor with shape ??\n\t        device, #lambda function\n\t    ):\n", "        if self.rid:\n\t            data_denoised=torch.zeros((self.nb_steps,shape[0], shape[1]))\n\t        #get the noise schedule\n\t        t = self.diff_params.create_schedule(self.nb_steps).to(device)\n\t        #sample from gaussian distribution with sigma_max variance\n\t        x = self.diff_params.sample_prior(shape,t[0]).to(device)\n\t        #parameter for langevin stochasticity, if Schurn is 0, gamma will be 0 to, so the sampler will be deterministic\n\t        gamma=self.diff_params.get_gamma(t).to(device)\n\t        for i in tqdm(range(0, self.nb_steps, 1)):\n\t            #print(\"sampling step \",i,\" from \",self.nb_steps)\n", "            if gamma[i]==0:\n\t                #deterministic sampling, do nothing\n\t                t_hat=t[i] \n\t                x_hat=x\n\t            else:\n\t                #stochastic sampling\n\t                #move timestep\n\t                t_hat=t[i]+gamma[i]*t[i] \n\t                #sample noise, Snoise is 1 by default\n\t                epsilon=torch.randn(shape).to(device)*self.diff_params.Snoise\n", "                #add extra noise\n\t                x_hat=x+((t_hat**2 - t[i]**2)**(1/2))*epsilon \n\t            score=self.get_score(x_hat, self.y, t_hat, self.degradation)    \n\t            #d=-t_hat*((denoised-x_hat)/t_hat**2)\n\t            d=-t_hat*score\n\t            #apply second order correction\n\t            h=t[i+1]-t_hat\n\t            if t[i+1]!=0 and self.order==2:  #always except last step\n\t                #second order correction2\n\t                #h=t[i+1]-t_hat\n", "                t_prime=t[i+1]\n\t                x_prime=x_hat+h*d\n\t                score=self.get_score(x_prime, self.y, t_prime, self.degradation)\n\t                d_prime=-t_prime*score\n\t                x=(x_hat+h*((1/2)*d +(1/2)*d_prime))\n\t            elif t[i+1]==0 or self.order==1: #first condition  is to avoid dividing by 0\n\t                #first order Euler step\n\t                x=x_hat+h*d\n\t            if self.rid: data_denoised[i]=x\n\t        if self.rid:\n", "            return x.detach(), data_denoised.detach(), t.detach()\n\t        else:\n\t            return x.detach()\n\t    def apply_mask(self, x):\n\t        return self.mask*x\n\t    def predict_inpainting(\n\t        self,\n\t        y_masked,\n\t        mask\n\t        ):\n", "        self.mask=mask.to(y_masked.device)\n\t        degradation=lambda x: self.apply_mask(x)\n\t        return self.predict_conditional(y_masked, degradation)\n\t    def apply_FIR_filter(self,y):\n\t        y=y.unsqueeze(1)\n\t        #apply the filter with a convolution (it is an FIR)\n\t        y_lpf=torch.nn.functional.conv1d(y,self.filt,padding=\"same\")\n\t        y_lpf=y_lpf.squeeze(1) \n\t        return y_lpf\n\t    def apply_IIR_filter(self,y):\n", "        y_lpf=torchaudio.functional.lfilter(y, self.a,self.b, clamp=False)\n\t        return y_lpf\n\t    def apply_biquad(self,y):\n\t        y_lpf=torchaudio.functional.biquad(y, self.b0, self.b1, self.b2, self.a0, self.a1, self.a2)\n\t        return y_lpf\n\t    def decimate(self,x):\n\t        return x[...,0:-1:self.factor]\n\t    def resample(self,x):\n\t        N=100\n\t        return torchaudio.functional.resample(x,orig_freq=int(N*self.factor), new_freq=N)\n", "    def predict_bwe(\n\t        self,\n\t        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n\t        filt, #filter Tensor with shape ??\n\t        filt_type\n\t        ):\n\t        #define the degradation model as a lambda\n\t        if filt_type==\"firwin\":\n\t            self.filt=filt.to(ylpf.device)\n\t            degradation=lambda x: self.apply_FIR_filter(x)\n", "        elif filt_type==\"firwin_hpf\":\n\t            self.filt=filt.to(ylpf.device)\n\t            degradation=lambda x: self.apply_FIR_filter(x)\n\t        elif filt_type==\"cheby1\":\n\t            b,a=filt\n\t            self.a=torch.Tensor(a).to(ylpf.device)\n\t            self.b=torch.Tensor(b).to(ylpf.device)\n\t            degradation=lambda x: self.apply_IIR_filter(x)\n\t        elif filt_type==\"biquad\":\n\t            b0, b1, b2, a0, a1, a2=filt\n", "            self.b0=torch.Tensor(b0).to(ylpf.device)\n\t            self.b1=torch.Tensor(b1).to(ylpf.device)\n\t            self.b2=torch.Tensor(b2).to(ylpf.device)\n\t            self.a0=torch.Tensor(a0).to(ylpf.device)\n\t            self.a1=torch.Tensor(a1).to(ylpf.device)\n\t            self.a2=torch.Tensor(a2).to(ylpf.device)\n\t            degradation=lambda x: self.apply_biquad(x)\n\t        elif filt_type==\"resample\":\n\t            self.factor =filt\n\t            degradation= lambda x: self.resample(x)\n", "            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n\t        elif filt_type==\"decimate\":\n\t            self.factor =filt\n\t            degradation= lambda x: self.decimate(x)\n\t            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n\t        else:\n\t           raise NotImplementedError\n\t        return self.predict_conditional(ylpf, degradation)\n\tclass SamplerPhaseRetrieval(Sampler):\n\t    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n", "        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\t        #assert data_consistency==False\n\t        assert xi>0\n\t    def apply_stft(self,x):\n\t        x2=torch.cat((x,self.zeropad ),-1)\n\t        X=torch.stft(x2, self.win_size, hop_length=self.hop_size,window=self.window,center=False,return_complex=False)\n\t        Y=torch.sqrt(X[...,0]**2 + X[...,1]**2)\n\t        return Y\n\t    def predict_pr(\n\t        self,\n", "        y\n\t        ):\n\t        self.win_size=self.args.inference.phase_retrieval.win_size\n\t        self.hop_size=self.args.inference.phase_retrieval.hop_size\n\t        print(y.shape)\n\t        self.zeropad=torch.zeros(y.shape[0],self.win_size ).to(y.device)\n\t        self.window=torch.hamming_window(window_length=self.win_size).to(y.device)\n\t        degradation=lambda x: self.apply_stft(x)\n\t        return self.predict_resample(y, (y.shape[0], self.args.exp.audio_len), degradation)\n\tclass SamplerCompSens(Sampler):\n", "    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n\t        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\t        assert data_consistency==False\n\t        assert xi>0\n\t    def apply_mask(self, x):\n\t        return self.mask*x\n\t    def predict_compsens(\n\t        self,\n\t        y_masked,\n\t        mask\n", "        ):\n\t        self.mask=mask.to(y_masked.device)\n\t        degradation=lambda x: self.apply_mask(x)\n\t        return self.predict_conditional(y_masked, degradation)\n\tclass SamplerDeclipping(Sampler):\n\t    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n\t        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\t        assert data_consistency==False\n\t        assert xi>0\n\t    def apply_clip(self,x):\n", "        x_hat=torch.clip(x,min=-self.clip_value, max=self.clip_value)\n\t        return x_hat\n\t    def predict_declipping(\n\t        self,\n\t        y_clipped,\n\t        clip_value\n\t        ):\n\t        self.clip_value=clip_value\n\t        degradation=lambda x: self.apply_clip(x)\n\t        if self.rid:\n", "            res, denoised, t=self.predict_conditional(y_clipped, degradation)\n\t            return res, denoised, t\n\t        else: \n\t            res=self.predict_conditional(y_clipped, degradation)\n\t            return res\n\tclass SamplerAutoregressive(Sampler):\n\t    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n\t        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\t        assert rid==False\n\t        self.ov=self.args.inference.autoregressive.overlap\n", "    def apply_mask(self, x):\n\t        return self.mask*x\n\t    def predict_autoregressive(\n\t        self,\n\t        shape,\n\t        N,\n\t        device\n\t        ):\n\t        endmask=int(self.ov*shape[-1])\n\t        self.mask=torch.ones((1,self.args.exp.audio_len)).to(device) #assume between 5 and 6s of total length\n", "        self.mask[:,endmask::]=0\n\t        degradation=lambda x: self.apply_mask(x)\n\t        x= self.predict_unconditional(shape, device)\n\t        xcat=x\n\t        x_masked=torch.zeros((1,self.args.exp.audio_len)).to(device)\n\t        for i in range(N-1):\n\t            x_masked[:,0:endmask]=x[:,-endmask::]\n\t            x=self.predict_conditional(x_masked, degradation)\n\t            xcat=torch.cat((xcat,x[...,endmask::]),-1)\n\t        return xcat\n", "class SamplerInpainting(Sampler):\n\t    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n\t        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\t    def apply_mask(self, x):\n\t        return self.mask*x\n\t    def predict_inpainting(\n\t        self,\n\t        y_masked,\n\t        mask\n\t        ):\n", "        self.mask=mask.to(y_masked.device)\n\t        degradation=lambda x: self.apply_mask(x)\n\t        return self.predict_conditional(y_masked, degradation)\n\tclass SamplerBWE(Sampler):\n\t    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n\t        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\t    def apply_FIR_filter(self,y):\n\t        y=y.unsqueeze(1)\n\t        #apply the filter with a convolution (it is an FIR)\n\t        y_lpf=torch.nn.functional.conv1d(y,self.filt,padding=\"same\")\n", "        y_lpf=y_lpf.squeeze(1) \n\t        return y_lpf\n\t    def apply_IIR_filter(self,y):\n\t        y_lpf=torchaudio.functional.lfilter(y, self.a,self.b, clamp=False)\n\t        return y_lpf\n\t    def apply_biquad(self,y):\n\t        y_lpf=torchaudio.functional.biquad(y, self.b0, self.b1, self.b2, self.a0, self.a1, self.a2)\n\t        return y_lpf\n\t    def decimate(self,x):\n\t        return x[...,0:-1:self.factor]\n", "    def resample(self,x):\n\t        N=100\n\t        return torchaudio.functional.resample(x,orig_freq=int(N*self.factor), new_freq=N)\n\t    def predict_bwe(\n\t        self,\n\t        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n\t        filt, #filter Tensor with shape ??\n\t        filt_type\n\t        ):\n\t        #define the degradation model as a lambda\n", "        if filt_type==\"firwin\":\n\t            self.filt=filt.to(ylpf.device)\n\t            degradation=lambda x: self.apply_FIR_filter(x)\n\t        elif filt_type==\"firwin_hpf\":\n\t            self.filt=filt.to(ylpf.device)\n\t            degradation=lambda x: self.apply_FIR_filter(x)\n\t        elif filt_type==\"cheby1\":\n\t            b,a=filt\n\t            self.a=torch.Tensor(a).to(ylpf.device)\n\t            self.b=torch.Tensor(b).to(ylpf.device)\n", "            degradation=lambda x: self.apply_IIR_filter(x)\n\t        elif filt_type==\"biquad\":\n\t            b0, b1, b2, a0, a1, a2=filt\n\t            self.b0=torch.Tensor(b0).to(ylpf.device)\n\t            self.b1=torch.Tensor(b1).to(ylpf.device)\n\t            self.b2=torch.Tensor(b2).to(ylpf.device)\n\t            self.a0=torch.Tensor(a0).to(ylpf.device)\n\t            self.a1=torch.Tensor(a1).to(ylpf.device)\n\t            self.a2=torch.Tensor(a2).to(ylpf.device)\n\t            degradation=lambda x: self.apply_biquad(x)\n", "        elif filt_type==\"resample\":\n\t            self.factor =filt\n\t            degradation= lambda x: self.resample(x)\n\t            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.audio_len), degradation)\n\t        elif filt_type==\"decimate\":\n\t            self.factor =filt\n\t            degradation= lambda x: self.decimate(x)\n\t            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.audio_len), degradation)\n\t        else:\n\t           raise NotImplementedError\n", "        return self.predict_conditional(ylpf, degradation)\n"]}
{"filename": "testing/blind_bwe_sampler.py", "chunked_list": ["from tqdm import tqdm\n\timport torch\n\timport torchaudio\n\t#import scipy.signal\n\timport copy\n\t#import numpy as np\n\t#import utils.filter_generation_utils as f_utils\n\timport utils.blind_bwe_utils as blind_bwe_utils\n\tclass BlindSampler():\n\t    def __init__(self, model,  diff_params, args, rid=False):\n", "        self.model = model\n\t        self.diff_params = diff_params #same as training, useful if we need to apply a wrapper or something\n\t        self.args=args\n\t        if not(self.args.tester.diff_params.same_as_training):\n\t            self.update_diff_params()\n\t        self.order=self.args.tester.order\n\t        self.xi=self.args.tester.posterior_sampling.xi\n\t        #hyperparameter for the reconstruction guidance\n\t        self.data_consistency=self.args.tester.posterior_sampling.data_consistency #use reconstruction gudance without replacement\n\t        self.nb_steps=self.args.tester.T\n", "        #prepare optimization parameters\n\t        self.mu=torch.Tensor([self.args.tester.blind_bwe.optimization.mu[0], self.args.tester.blind_bwe.optimization.mu[1]])\n\t        #clamping parameters\n\t        self.fcmin=self.args.tester.blind_bwe.fcmin\n\t        if self.args.tester.blind_bwe.fcmax ==\"nyquist\":\n\t                self.fcmax=self.args.exp.sample_rate//2\n\t        else:\n\t                self.fcmax=self.args.tester.blind_bwe.fcmax\n\t        self.Amin=self.args.tester.blind_bwe.Amin\n\t        self.Amax=self.args.tester.blind_bwe.Amax\n", "        #used for congerence checking\n\t        self.tol=self.args.tester.blind_bwe.optimization.tol\n\t        self.start_sigma=self.args.tester.posterior_sampling.start_sigma\n\t        if self.start_sigma ==\"None\":\n\t            self.start_sigma=None\n\t        print(\"start sigma\", self.start_sigma)\n\t    def update_diff_params(self):\n\t        #the parameters for testing might not be necesarily the same as the ones used for training\n\t        self.diff_params.sigma_min=self.args.tester.diff_params.sigma_min\n\t        self.diff_params.sigma_max =self.args.tester.diff_params.sigma_max\n", "        self.diff_params.ro=self.args.tester.diff_params.ro\n\t        self.diff_params.sigma_data=self.args.tester.diff_params.sigma_data\n\t        #par.diff_params.meters stochastic sampling\n\t        self.diff_params.Schurn=self.args.tester.diff_params.Schurn\n\t        self.diff_params.Stmin=self.args.tester.diff_params.Stmin\n\t        self.diff_params.Stmax=self.args.tester.diff_params.Stmax\n\t        self.diff_params.Snoise=self.args.tester.diff_params.Snoise\n\t    def data_consistency_step_classic(self, x_hat, y, degradation, filter_params=None):\n\t        \"\"\"\n\t        Simple replacement method, used for inpainting and FIR bwe\n", "        \"\"\"\n\t        #get reconstruction estimate\n\t        if filter_params is not None:\n\t            den_rec= degradation(x_hat, filter_params)     \n\t        else:\n\t            den_rec= degradation(x_hat)     \n\t        #apply replacment (valid for linear degradations)\n\t        return y+x_hat-den_rec \n\t    def get_rec_grads(self, x_hat, y, x, t_i, degradation, filter_params=None):\n\t        \"\"\"\n", "        Compute the gradients of the reconstruction error with respect to the input\n\t        \"\"\" \n\t        if self.args.tester.posterior_sampling.SNR_observations !=\"None\":\n\t            snr=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n\t            sigma2_s=torch.var(y, -1)\n\t            sigma=torch.sqrt(sigma2_s/snr).unsqueeze(-1)\n\t            #sigma=torch.tensor([self.args.tester.posterior_sampling.sigma_observations]).unsqueeze(-1).to(y.device)\n\t            #print(y.shape, sigma.shape)\n\t            y+=sigma*torch.randn(y.shape).to(y.device)\n\t        if filter_params is not None:\n", "            den_rec= degradation(x_hat, filter_params) \n\t        else:\n\t            den_rec= degradation(x_hat) \n\t        if len(y.shape)==3:\n\t            dim=(1,2)\n\t        elif len(y.shape)==2:\n\t            dim=1\n\t        if self.args.tester.posterior_sampling.norm==\"smoothl1\":\n\t            norm=torch.nn.functional.smooth_l1_loss(y, den_rec, reduction='sum', beta=self.args.tester.posterior_sampling.smoothl1_beta)\n\t        elif self.args.tester.posterior_sampling.norm==\"cosine\":\n", "            cos = torch.nn.CosineSimilarity(dim=dim, eps=1e-6)\n\t            norm = (1-cos(den_rec, y)).clamp(min=0)\n\t            print(\"norm\",norm)\n\t        elif self.args.tester.posterior_sampling.stft_distance.use:\n\t            if self.args.tester.posterior_sampling.stft_distance.use_multires:\n\t                print(\" applying multires \")\n\t                norm1, norm2=self.norm(y, den_rec)\n\t                norm=norm1+norm2\n\t            elif self.args.tester.posterior_sampling.stft_distance.mag:\n\t                print(\"logmag\", self.args.tester.posterior_sampling.stft_distance.logmag)\n", "                norm=blind_bwe_utils.apply_norm_STFTmag_fweighted(y, den_rec, self.args.tester.posterior_sampling.freq_weighting, self.args.tester.posterior_sampling.stft_distance.nfft, logmag=self.args.tester.posterior_sampling.stft_distance.logmag)\n\t                print(\"norm\", norm)\n\t            else:\n\t                norm=blind_bwe_utils.apply_norm_STFT_fweighted(y, den_rec, self.args.tester.posterior_sampling.freq_weighting, self.args.tester.posterior_sampling.stft_distance.nfft)\n\t        else:\n\t            norm=torch.linalg.norm(y-den_rec,dim=dim, ord=self.args.tester.posterior_sampling.norm)\n\t        rec_grads=torch.autograd.grad(outputs=norm.sum(),\n\t                                      inputs=x)\n\t        rec_grads=rec_grads[0]\n\t        normguide=torch.linalg.norm(rec_grads)/self.args.exp.audio_len**0.5\n", "        #normalize scaling\n\t        s=self.xi/(normguide+1e-6)\n\t        #optionally apply a treshold to the gradients\n\t        if False:\n\t            #pply tresholding to the gradients. It is a dirty trick but helps avoiding bad artifacts \n\t            rec_grads=torch.clip(rec_grads, min=-self.treshold_on_grads, max=self.treshold_on_grads)\n\t        return s*rec_grads/t_i\n\t    def get_score_rec_guidance(self, x, y, t_i, degradation, filter_params=None):\n\t        x.requires_grad_()\n\t        x_hat=self.get_denoised_estimate(x, t_i)\n", "        #add noise to y\n\t        rec_grads=self.get_rec_grads(x_hat, y, x, t_i, degradation, filter_params)\n\t        score=self.denoised2score(x_hat, x, t_i)\n\t        #apply scaled guidance to the score\n\t        score=score-rec_grads\n\t        return score\n\t    def get_denoised_estimate(self, x, t_i):\n\t        x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\t        if self.args.tester.filter_out_cqt_DC_Nyq:\n\t            x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n", "        return x_hat\n\t    def get_score(self,x, y, t_i, degradation, filter_params=None):\n\t        if y==None:\n\t            assert degradation==None\n\t            #unconditional sampling\n\t            with torch.no_grad():\n\t                #print(\"In sampling\", x.shape, t_i.shape)\n\t                #print(\"before denoiser\", x.shape)\n\t                x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\t                if self.args.tester.filter_out_cqt_DC_Nyq:\n", "                    x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n\t                score=(x_hat-x)/t_i**2\n\t            return score\n\t        else:\n\t            if self.xi>0:\n\t                #apply rec. guidance\n\t                score=self.get_score_rec_guidance(x, y, t_i, degradation, filter_params=filter_params)\n\t                #optionally apply replacement or consistency step\n\t                if self.data_consistency:\n\t                    #convert score to denoised estimate using Tweedie's formula\n", "                    x_hat=score*t_i**2+x\n\t                    try:\n\t                        x_hat=self.data_consistency_step(x_hat)\n\t                    except:\n\t                        x_hat=self.data_consistency_step(x_hat,y, degradation)\n\t                    #convert back to score\n\t                    score=(x_hat-x)/t_i**2\n\t            else:\n\t                #raise NotImplementedError\n\t                #denoised with replacement method\n", "                with torch.no_grad():\n\t                    x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\t                    #x_hat=self.data_consistency_step(x_hat,y, degradation)\n\t                    if self.data_consistency:\n\t                        try:\n\t                            x_hat=self.data_consistency_step(x_hat)\n\t                        except:\n\t                            try:\n\t                                x_hat=self.data_consistency_step(x_hat,y, degradation)\n\t                            except:\n", "                                x_hat=self.data_consistency_step(x_hat,y, degradation, filter_params)\n\t                    score=(x_hat-x)/t_i**2\n\t            return score\n\t    def apply_FIR_filter(self,y):\n\t        y=y.unsqueeze(1)\n\t        #apply the filter with a convolution (it is an FIR)\n\t        y_lpf=torch.nn.functional.conv1d(y,self.filt,padding=\"same\")\n\t        y_lpf=y_lpf.squeeze(1) \n\t        return y_lpf\n\t    def apply_IIR_filter(self,y):\n", "        y_lpf=torchaudio.functional.lfilter(y, self.a,self.b, clamp=False)\n\t        return y_lpf\n\t    def apply_biquad(self,y):\n\t        y_lpf=torchaudio.functional.biquad(y, self.b0, self.b1, self.b2, self.a0, self.a1, self.a2)\n\t        return y_lpf\n\t    def decimate(self,x):\n\t        return x[...,0:-1:self.factor]\n\t    def resample(self,x):\n\t        N=100\n\t        return torchaudio.functional.resample(x,orig_freq=int(N*self.factor), new_freq=N)\n", "    def prepare_smooth_mask(self, mask, size=10):\n\t        hann=torch.hann_window(size*2)\n\t        hann_left=hann[0:size]\n\t        hann_right=hann[size::]\n\t        B,N=mask.shape\n\t        mask=mask[0]\n\t        prev=1\n\t        new_mask=mask.clone()\n\t        #print(hann.shape)\n\t        for i in range(len(mask)):\n", "            if mask[i] != prev:\n\t                #print(i, mask.shape, mask[i], prev)\n\t                #transition\n\t                if mask[i]==0:\n\t                   print(\"apply right\")\n\t                   #gap encountered, apply hann right before\n\t                   new_mask[i-size:i]=hann_right\n\t                if mask[i]==1:\n\t                   print(\"apply left\")\n\t                   #gap encountered, apply hann left after\n", "                   new_mask[i:i+size]=hann_left\n\t                #print(mask[i-2*size:i+2*size])\n\t                #print(new_mask[i-2*size:i+2*size])\n\t            prev=mask[i]\n\t        return new_mask.unsqueeze(0).expand(B,-1)\n\t    def predict_bwe_AR(\n\t        self,\n\t        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n\t        y_masked,\n\t        filt, #filter Tensor with shape ??\n", "        filt_type,\n\t        rid=False,\n\t        test_filter_fit=False,\n\t        compute_sweep=False,\n\t        mask=None\n\t        ):\n\t        assert mask is not None\n\t        #define the degradation model as a lambda\n\t        if filt_type==\"fc_A\":\n\t            print(\"fc_A\")\n", "            self.freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(ylpf.device)\n\t            self.params=filt\n\t            print(self.params)\n\t            y=mask*y_masked+(1-mask)*ylpf\n\t            degradation=lambda x: mask*x +(1-mask)*self.apply_filter_fcA(x, self.params)\n\t        elif filt_type==\"firwin\":\n\t            self.filt=filt.to(ylpf.device)\n\t            y=mask*y_masked+(1-mask)*ylpf\n\t            degradation=lambda x: mask*x +(1-mask)*self.apply_FIR_filter(x)\n\t            #degradation=lambda x: self.apply_FIR_filter(x)\n", "        else:\n\t           raise NotImplementedError\n\t        if self.args.tester.complete_recording.inpaint_DC:\n\t            smooth_mask=self.prepare_smooth_mask(mask, 50)\n\t            y_smooth_masked=smooth_mask*y_masked\n\t            mask_degradation=lambda x: smooth_mask*x \n\t            self.data_consistency_step=lambda x_hat: self.data_consistency_step_classic(x_hat,y_smooth_masked, mask_degradation)\n\t            self.data_consistency=True\n\t        return self.predict_conditional(y, degradation, rid, test_filter_fit, compute_sweep)\n\t    def predict_bwe(\n", "        self,\n\t        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n\t        filt, #filter Tensor with shape ??\n\t        filt_type,\n\t        rid=False,\n\t        test_filter_fit=False,\n\t        compute_sweep=False\n\t        ):\n\t        print(\"test_filter_fit\", test_filter_fit)\n\t        print(\"compute_sweep\", compute_sweep)\n", "        #define the degradation model as a lambda\n\t        if filt_type==\"firwin\":\n\t            self.filt=filt.to(ylpf.device)\n\t            degradation=lambda x: self.apply_FIR_filter(x)\n\t        elif filt_type==\"firwin_hpf\":\n\t            self.filt=filt.to(ylpf.device)\n\t            degradation=lambda x: self.apply_FIR_filter(x)\n\t        elif filt_type==\"cheby1\":\n\t            b,a=filt\n\t            self.a=torch.Tensor(a).to(ylpf.device)\n", "            self.b=torch.Tensor(b).to(ylpf.device)\n\t            degradation=lambda x: self.apply_IIR_filter(x)\n\t        elif filt_type==\"biquad\":\n\t            b0, b1, b2, a0, a1, a2=filt\n\t            self.b0=torch.Tensor(b0).to(ylpf.device)\n\t            self.b1=torch.Tensor(b1).to(ylpf.device)\n\t            self.b2=torch.Tensor(b2).to(ylpf.device)\n\t            self.a0=torch.Tensor(a0).to(ylpf.device)\n\t            self.a1=torch.Tensor(a1).to(ylpf.device)\n\t            self.a2=torch.Tensor(a2).to(ylpf.device)\n", "            degradation=lambda x: self.apply_biquad(x)\n\t        elif filt_type==\"resample\":\n\t            self.factor =filt\n\t            degradation= lambda x: self.resample(x)\n\t            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n\t        elif filt_type==\"decimate\":\n\t            self.factor =filt\n\t            degradation= lambda x: self.decimate(x)\n\t            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n\t            #elif filt_type==\"3rdoct\":\n", "            #    freq_octs=torch.tensor(f_utils.get_third_octave_bands(self.args.exp.sample_rate, fmin=self.args.tester.blind_bwe.range.fmin, fmax=self.args.exp.sample_rate/2))\n\t            #    filt=f_utils.normalize_filter(filt)\n\t            #    degradation= lambda x: self.apply_3rdoct_filt(x, filt, freq_octs)\n\t        elif filt_type==\"fc_A\":\n\t            print(\"fc_A\")\n\t            self.freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(ylpf.device)\n\t            self.params=filt\n\t            print(self.params)\n\t            degradation=lambda x:  self.apply_filter_fcA(x, self.params)\n\t        else:\n", "           raise NotImplementedError\n\t        if self.data_consistency:\n\t            #normal data consistency\n\t            self.data_consistency_step=lambda x,y,degradation: self.data_consistency_step_classic(x,y, degradation)\n\t        return self.predict_conditional(ylpf, degradation, rid, test_filter_fit, compute_sweep)\n\t    def predict_unconditional(\n\t        self,\n\t        shape,  #observations (lowpssed signal) Tensor with shape ??\n\t        device,\n\t        rid=False\n", "    ):\n\t        self.y=None\n\t        self.degradation=None\n\t        return self.predict(shape, device, rid)\n\t    def predict_resample(\n\t        self,\n\t        y,  #observations (lowpssed signal) Tensor with shape ??\n\t        shape,\n\t        degradation, #lambda function\n\t    ):\n", "        self.degradation=degradation \n\t        self.y=y\n\t        return self.predict(shape, y.device)\n\t    def predict_conditional(\n\t        self,\n\t        y,  #observations (lowpssed signal) Tensor with shape ??\n\t        degradation, #lambda function\n\t        rid=False,\n\t        test_filter_fit=False,\n\t        compute_sweep=False\n", "    ):\n\t        self.degradation=degradation \n\t        #if self.args.tester.posterior_sampling.SNR_observations is not None:\n\t        #    SNR=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n\t        #    sigma2_s=torch.var(y, -1)\n\t        #    sigma=torch.sqrt(sigma2_s/SNR)\n\t        #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t        self.y=y\n\t        return self.predict(y.shape, y.device, rid, test_filter_fit, compute_sweep)\n\t    def predict(\n", "        self,\n\t        shape,  #observations (lowpssed signal) Tensor with shape ??\n\t        device, #lambda function\n\t        rid=False,\n\t        test_filter_fit=False,\n\t        compute_sweep=False\n\t    ):\n\t        if rid:\n\t            data_denoised=torch.zeros((self.nb_steps,shape[0], shape[1]))\n\t            data_score=torch.zeros((self.nb_steps,shape[0], shape[1]))\n", "        if test_filter_fit:\n\t            filter_params=torch.Tensor([self.args.tester.blind_bwe.initial_conditions.fc, self.args.tester.blind_bwe.initial_conditions.A]).to(device)\n\t            if rid:\n\t                data_filters=torch.zeros((self.nb_steps, filter_params.shape[0]))\n\t        if self.start_sigma is None or self.y is None:\n\t            t=self.diff_params.create_schedule(self.nb_steps).to(device)\n\t            x=self.diff_params.sample_prior(shape, t[0]).to(device)\n\t        else:\n\t            #get the noise schedule\n\t            t = self.diff_params.create_schedule_from_initial_t(self.start_sigma,self.nb_steps).to(device)\n", "            #sample from gaussian distribution with sigma_max variance\n\t            x = self.y + self.diff_params.sample_prior(shape,t[0]).to(device)\n\t        #if self.args.tester.bandwidth_extension.sigma_observations>0 and self.y is not None:\n\t        #    self.y=self.y+self.args.tester.bandwidth_extension.sigma_observations*torch.randn_like(self.y)\n\t        #parameter for langevin stochasticity, if Schurn is 0, gamma will be 0 to, so the sampler will be deterministic\n\t        gamma=self.diff_params.get_gamma(t).to(device)\n\t        if compute_sweep:\n\t            self.fc_s=torch.logspace(2.5, 4, 15).to(device)\n\t            self.A_s=torch.linspace(-80, -5, 12).to(device)\n\t            if rid:\n", "                data_norms=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0]))\n\t                data_grads=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0], 2))\n\t        for i in tqdm(range(0, self.nb_steps, 1)):\n\t            #print(\"sampling step \",i,\" from \",self.nb_steps)\n\t            x_hat, t_hat=self.move_timestep(x, t[i], gamma[i],self.diff_params.Snoise)\n\t            score=self.get_score(x_hat, self.y, t_hat, self.degradation)    \n\t            if test_filter_fit:\n\t                denoised_estimate=self.score2denoised(score, x_hat, t_hat)\n\t                est_params=self.fit_params(denoised_estimate, self.y,  filter_params)\n\t                ##print(\"estimated params\",est_params.shape)\n", "            if compute_sweep:\n\t                denoised_estimate=self.score2denoised(score, x_hat, t_hat)\n\t                norms, grads=self.compute_sweep(denoised_estimate, self.y)\n\t            d=-t_hat*score\n\t            if rid: \n\t                data_denoised[i]=self.score2denoised(score, x_hat, t_hat)\n\t                data_score[i]=score\n\t                if test_filter_fit:\n\t                    data_filters[i]=est_params\n\t                if compute_sweep:\n", "                    data_norms[i]=norms\n\t                    data_grads[i]=grads\n\t            #apply second order correction\n\t            h=t[i+1]-t_hat\n\t            if t[i+1]!=0 and self.order==2:  #always except last step\n\t                #second order correction2\n\t                #h=t[i+1]-t_hat\n\t                t_prime=t[i+1]\n\t                x_prime=x_hat+h*d\n\t                score=self.get_score(x_prime, self.y, t_prime, self.degradation)\n", "                d_prime=-t_prime*score\n\t                x=(x_hat+h*((1/2)*d +(1/2)*d_prime))\n\t            elif t[i+1]==0 or self.order==1: #first condition  is to avoid dividing by 0\n\t                #first order Euler step\n\t                x=x_hat+h*d\n\t        if rid:\n\t            list_out=(x.detach(), data_denoised.detach(), data_score.detach(),t.detach())\n\t            if test_filter_fit:\n\t                list_out=list_out+(data_filters.detach(),)\n\t            if compute_sweep:\n", "                list_out=list_out+(data_norms.detach(), data_grads.detach())\n\t            return list_out\n\t        else:\n\t            return x.detach()\n\t    def denoised2score(self,  x_d0, x, t):\n\t        #tweedie's score function\n\t        return (x_d0-x)/t**2\n\t    def score2denoised(self, score, x, t):\n\t        return score*t**2+x\n\t    def move_timestep(self, x, t, gamma, Snoise=1):\n", "        #if gamma_sig[i]==0 this is a deterministic step, make sure it doed not crash\n\t        t_hat=t+gamma*t\n\t        #sample noise, Snoise is 1 by default\n\t        epsilon=torch.randn(x.shape).to(x.device)*Snoise\n\t        #add extra noise\n\t        x_hat=x+((t_hat**2 - t**2)**(1/2))*epsilon\n\t        return x_hat, t_hat\n\t    def apply_filter_fcA(self, x, filter_params):\n\t        H=blind_bwe_utils.design_filter(filter_params[0], filter_params[1], self.freqs)\n\t        return blind_bwe_utils.apply_filter(x, H,self.args.tester.blind_bwe.NFFT)\n", "    def optimizer_func(self, Xden, Y, params):\n\t        \"\"\"\n\t        Xden: STFT of denoised estimate\n\t        y: observations\n\t        params: parameters of the degradation model (fc, A)\n\t        \"\"\"\n\t        #print(\"before design filter\", params)\n\t        H=blind_bwe_utils.design_filter(params[0],params[1], self.freqs)\n\t        return blind_bwe_utils.apply_filter_and_norm_STFTmag_fweighted(Xden, Y, H, self.args.tester.posterior_sampling.freq_weighting_filter)\n\t    def fit_params(self, denoised_estimate, y, filter_params):\n", "        #fit the parameters of the degradation model\n\t        #denoised_estimate: denoised estimate of the signal\n\t        #y: observations\n\t        #degradation: degradation function\n\t        #filter_params: initial estimate of parameters of the degradation model\n\t        #return: reestimated parameters of the degradation model\n\t        if self.args.tester.posterior_sampling.SNR_observations !=\"None\":\n\t            snr=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n\t            sigma2_s=torch.var(y, -1)\n\t            sigma=torch.sqrt(sigma2_s/snr).unsqueeze(-1)\n", "            #sigma=torch.tensor([self.args.tester.posterior_sampling.sigma_observations]).unsqueeze(-1).to(y.device)\n\t            #print(y.shape, sigma.shape)\n\t            y+=sigma*torch.randn(y.shape).to(y.device)\n\t        #add noise to the denoised estimate for regularization\n\t        if self.args.tester.blind_bwe.sigma_den_estimate:\n\t            denoised_estimate=denoised_estimate+torch.randn(denoised_estimate.shape).to(denoised_estimate.device)*self.args.tester.blind_bwe.sigma_den_estimate\n\t        Xden=blind_bwe_utils.apply_stft(denoised_estimate, self.args.tester.blind_bwe.NFFT)\n\t        Y=blind_bwe_utils.apply_stft(y, self.args.tester.blind_bwe.NFFT)\n\t        func=lambda  params: self.optimizer_func( Xden, Y, params)\n\t        self.mu=self.mu.to(y.device)\n", "        for i in tqdm(range(self.args.tester.blind_bwe.optimization.max_iter)):\n\t            filter_params.requires_grad=True\n\t                #fc.requires_grad=True\n\t            norm=func(filter_params)\n\t            grad=torch.autograd.grad(norm,filter_params,create_graph=True)\n\t            #update params with gradient descent, using backtracking line search\n\t            t=self.mu\n\t            newparams=filter_params-t.unsqueeze(1)*grad[0]\n\t            #update with the found step size\n\t            filter_params=newparams\n", "            filter_params.detach_()\n\t            #limit params to help stability\n\t            if self.args.tester.blind_bwe.optimization.clamp_fc:\n\t                    filter_params[0,0]=torch.clamp(filter_params[0,0],min=self.fcmin,max=self.fcmax)\n\t                    for k in range(1,len(filter_params[0])):\n\t                        filter_params[0,k]=torch.clamp(filter_params[0,k],min=filter_params[0,k-1]+1,max=self.fcmax)\n\t            if self.args.tester.blind_bwe.optimization.clamp_A:\n\t                    filter_params[1,0]=torch.clamp(filter_params[1,0],min=self.Amin,max=-1 if self.args.tester.blind_bwe.optimization.only_negative_A else self.Amax)\n\t                    for k in range(1,len(filter_params[0])):\n\t                        filter_params[1,k]=torch.clamp(filter_params[1,k],min=self.Amin,max=filter_params[1,k-1] if self.args.tester.blind_bwe.optimization.only_negative_A else self.Amax)\n", "            if i>0:\n\t                if (torch.abs(filter_params[0]-prev_params[0]).mean()<self.tol[0]) and (torch.abs(filter_params[1]-prev_params[1]).mean()<self.tol[1]):\n\t                     break\n\t            prev_params=filter_params.clone().detach()\n\t        #print(\"fc: \",filter_params[0].item(),\" A: \", filter_params[1].item())\n\t        print(filter_params)\n\t        return filter_params\n\t    def compute_sweep(self, denoised_estimate, y):\n\t        Xden=blind_bwe_utils.apply_stft(denoised_estimate, self.args.tester.blind_bwe.NFFT)\n\t        Y=blind_bwe_utils.apply_stft(y, self.args.tester.blind_bwe.NFFT)\n", "        func=lambda  params: self.optimizer_func( Xden, Y, params)\n\t        grads=torch.zeros(self.fc_s.shape[0], self.A_s.shape[0], 2)\n\t        norms=torch.zeros(self.fc_s.shape[0], self.A_s.shape[0])\n\t        #iterate over fc and A values\n\t        for fc in range(self.fc_s.shape[0]):\n\t            for A in range(self.A_s.shape[0]):\n\t                #print(\"fc: \",self.fc_s[fc].item(),\"A: \",self.A_s[A].item())\n\t                params=torch.Tensor([self.fc_s[fc], self.A_s[A]]).requires_grad_(True)\n\t                norm=func(params)\n\t                grads[fc,A,:]=torch.autograd.grad(norm,params,create_graph=True)[0]\n", "                norms[fc,A]=norm\n\t        return norms.detach(), grads.detach()\n\t    def predict_blind_bwe(\n\t        self,\n\t        y,  #observations (lowpssed signal) Tensor with shape (L,)\n\t        rid=False,\n\t        compute_sweep=False,\n\t        ):\n\t        self.freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(y.device)\n\t        self.degradation=lambda x, filter_params: self.apply_filter_fcA(x, filter_params)\n", "        if self.data_consistency:\n\t            #normal data consistency\n\t            self.data_consistency_step=lambda x,y,degradation, filter_params: self.data_consistency_step_classic(x,y, degradation, filter_params)\n\t        #get shape and device from the observations tensor\n\t        shape=y.shape\n\t        device=y.device\n\t        #initialise filter parameters\n\t        filter_params=torch.Tensor([self.args.tester.blind_bwe.initial_conditions.fc, self.args.tester.blind_bwe.initial_conditions.A]).to(device)\n\t        if len(filter_params.shape)==1:\n\t            filter_params.unsqueeze_(1)\n", "        print(filter_params.shape)\n\t        shape_filter_params=filter_params.shape #fc and A\n\t        #retrieve the shape from the initial estimate of the parameters\n\t        if compute_sweep:\n\t            self.fc_s=torch.logspace(2.5, 4, 15).to(device)\n\t            self.A_s=torch.linspace(-80, -5, 12).to(device)\n\t            if rid:\n\t                data_norms=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0]))\n\t                data_grads=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0], 2))\n\t        if rid:\n", "            data_denoised=torch.zeros((self.nb_steps,shape[0], shape[1]))\n\t            data_filters=torch.zeros((self.nb_steps,*shape_filter_params))\n\t            print(data_filters.shape)\n\t        if self.start_sigma is None:\n\t            t=self.diff_params.create_schedule(self.nb_steps).to(device)\n\t            x=self.diff_params.sample_prior(shape, t[0]).to(device)\n\t        else:\n\t            #get the noise schedule\n\t            t = self.diff_params.create_schedule_from_initial_t(self.start_sigma,self.nb_steps).to(y.device)\n\t            #sample from gaussian distribution with sigma_max variance\n", "            x = y + self.diff_params.sample_prior(shape,t[0]).to(device)\n\t        #if self.args.tester.posterior_sampling.SNR_observations !=\"none\":\n\t        #    snr=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n\t        #    sigma2_s=torch.var(y, -1)\n\t        #    sigma=torch.sqrt(sigma2_s/snr).unsqueeze(-1)\n\t        #    #sigma=torch.tensor([self.args.tester.posterior_sampling.sigma_observations]).unsqueeze(-1).to(y.device)\n\t        #    #print(y.shape, sigma.shape)\n\t        #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t        #parameter for langevin stochasticity, if Schurn is 0, gamma will be 0 to, so the sampler will be deterministic\n\t        gamma=self.diff_params.get_gamma(t).to(device)\n", "        for i in tqdm(range(0, self.nb_steps, 1)):\n\t            #print(\"sampling step \",i,\" from \",self.nb_steps)\n\t            x_hat, t_hat=self.move_timestep(x, t[i], gamma[i])\n\t            x_hat.requires_grad_(True)\n\t            x_den=self.get_denoised_estimate(x_hat, t_hat)\n\t            x_den_2=x_den.clone().detach()\n\t            filter_params=self.fit_params(x_den_2, y,  filter_params)\n\t            rec_grads=self.get_rec_grads(x_den, y, x_hat, t_hat, self.degradation, filter_params)\n\t            x_hat.detach_()\n\t            score=self.denoised2score(x_den_2, x_hat, t_hat)-rec_grads\n", "            if self.args.tester.posterior_sampling.data_consistency:\n\t                #apply data consistency here!\n\t                #it is a bit ugly, but I need to convert the score to denoied estimate again\n\t                x_den_3=self.score2denoised(score, x_hat, t_hat)\n\t                x_den_3=self.data_consistency_step(x_den_3, y, self.degradation, filter_params)\n\t                score=self.denoised2score(x_den_3, x_hat, t_hat)\n\t            if compute_sweep:\n\t                norms, grads=self.compute_sweep(x_den_2, y)\n\t            #d=-t_hat*((denoised-x_hat)/t_hat**2)\n\t            d=-t_hat*score\n", "            if rid: \n\t                data_denoised[i]=x_den_2\n\t                data_filters[i]=filter_params\n\t                if compute_sweep:\n\t                    data_norms[i]=norms\n\t                    data_grads[i]=grads\n\t            #apply second order correction\n\t            h=t[i+1]-t_hat\n\t            if t[i+1]!=0 and self.order==2:  #always except last step\n\t                #second order correction2\n", "                #h=t[i+1]-t_hat\n\t                t_prime=t[i+1]\n\t                x_prime=x_hat+h*d\n\t                x_prime.requires_grad_(True)\n\t                x_den=self.get_denoised_estimate(x_prime, t_prime)\n\t                x_den_2=x_den.clone().detach()\n\t                filter_params=self.fit_params(x_den_2, y,  filter_params)\n\t                rec_grads=self.get_rec_grads(x_den, y, x_prime, t_prime, self.degradation, filter_params)\n\t                x_prime.detach_()\n\t                score=self.denoised2score(x_den_2, x_prime, t_prime)-rec_grads\n", "                if self.args.tester.posterior_sampling.data_consistency:\n\t                    #apply data consistency here!\n\t                    #it is a bit ugly, but I need to convert the score to denoied estimate again\n\t                    x_den_3=self.score2denoised(score, x_prime, t_prime)\n\t                    x_den_3=self.data_consistency_step(x_den_3, y, self.degradation, filter_params)\n\t                    score=self.denoised2score(x_den_3, x_prime, t_prime)\n\t                d_prime=-t_prime*score\n\t                x=(x_hat+h*((1/2)*d +(1/2)*d_prime))\n\t            elif t[i+1]==0 or self.order==1: #first condition  is to avoid dividing by 0\n\t                #first order Euler step\n", "                x=x_hat+h*d\n\t        if rid:\n\t            list_out=(x.detach(), filter_params.detach(), data_denoised.detach(),t.detach(), data_filters.detach())\n\t            if compute_sweep:\n\t                list_out=list_out+(data_norms.detach(), data_grads.detach())\n\t            return list_out\n\t        else:\n\t            return x.detach() , filter_params.detach()\n"]}
{"filename": "testing/blind_bwe_tester_small.py", "chunked_list": ["from datetime import date\n\timport pickle\n\timport re\n\timport torch\n\timport torchaudio\n\t#from src.models.unet_cqt import Unet_CQT\n\t#from src.models.unet_stft import Unet_STFT\n\t#from src.models.unet_1d import Unet_1d\n\t#import src.utils.setup as utils_setup\n\t#from src.sde import  VE_Sde_Elucidating\n", "import numpy as np\n\timport utils.dnnlib as dnnlib\n\timport os\n\timport utils.logging as utils_logging\n\timport wandb\n\timport copy\n\tfrom glob import glob\n\tfrom tqdm import tqdm\n\timport utils.bandwidth_extension as utils_bwe\n\timport omegaconf\n", "#import utils.filter_generation_utils as f_utils\n\timport utils.blind_bwe_utils as blind_bwe_utils\n\timport utils.training_utils as t_utils\n\timport soundfile as sf\n\t#from utils.spectral_analysis import LTAS_processor\n\tclass BlindTester():\n\t    def __init__(\n\t        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n\t    ):\n\t        self.args=args\n", "        self.network=torch.compile(network)\n\t        #self.network=network\n\t        #prnt number of parameters\n\t        self.diff_params=copy.copy(diff_params)\n\t        self.device=device\n\t        #choose gpu as the device if possible\n\t        if self.device is None:\n\t            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        self.network=network\n\t        torch.backends.cudnn.benchmark = True\n", "        today=date.today() \n\t        if it is None:\n\t            self.it=0\n\t        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n\t        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n\t        if not os.path.exists(self.path_sampling):\n\t            os.makedirs(self.path_sampling)\n\t        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n\t        self.setup_sampler()\n\t        self.use_wandb=False #hardcoded for now\n", "        S=self.args.exp.resample_factor\n\t        if S>2.1 and S<2.2:\n\t            #resampling 48k to 22.05k\n\t            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n\t        elif S!=1:\n\t            N=int(self.args.exp.audio_len*S)\n\t            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\t        if test_set is not None:\n\t            self.test_set=test_set\n\t            self.do_inpainting=True\n", "            self.do_bwe=True\n\t            self.do_blind_bwe=True\n\t        else:\n\t            self.test_set=None\n\t            self.do_inpainting=False\n\t            self.do_bwe=False #these need to be set up in the config file\n\t            self.do_blind_bwe=False\n\t        self.paths={}\n\t        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n\t            self.do_inpainting=True\n", "            mode=\"inpainting\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n\t            #TODO add more information in the subirectory names\n\t        else: self.do_inpainting=False\n\t        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n\t            self.do_bwe=True\n\t            mode=\"bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n\t            #TODO add more information in the subirectory names\n\t        else:\n", "            self.do_bwe=False\n\t        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n\t            self.do_blind_bwe=True\n\t            mode=\"blind_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n\t            #TODO add more information in the subirectory names\n\t        if \"real_blind_bwe\" in self.args.tester.modes:\n\t            self.do_blind_bwe=True\n\t            mode=\"real_blind_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n", "            #TODO add more information in the subirectory names\n\t        if \"formal_test_bwe\" in self.args.tester.modes:\n\t            self.do_formal_test_bwe=True\n\t            mode=\"formal_test_bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n\t        if \"formal_test_bwe_small\" in self.args.tester.modes:\n\t            self.do_formal_test_bwe=True\n\t            mode=\"formal_test_bwe_small\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe_small\",\"degraded\",\"reconstructed\")\n\t        if (\"unconditional\" in self.args.tester.modes):\n", "            mode=\"unconditional\"\n\t            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\t        if (\"filter_bwe\" in self.args.tester.modes):\n\t            mode=\"filter_bwe\"\n\t            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\t        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n\t        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\t    def prepare_unc_experiment(self, str):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n", "                os.makedirs(path_exp)\n\t            return path_exp\n\t    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n\t            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n", "                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n\t            #ensure the path exists\n\t            if not os.path.exists(path_original):\n\t                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n\t            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n", "            return path_exp, path_degraded, path_original, path_reconstructed\n\t    def resample_audio(self, audio, fs):\n\t        #this has been reused from the trainer.py\n\t        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\t    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n", "            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n\t                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n\t            #ensure the path exists\n\t            if not os.path.exists(path_original):\n\t                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n", "            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n\t            n=str_degraded_estimate\n\t            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n\t            #ensure the path exists\n\t            if not os.path.exists(path_degraded_estimate):\n\t                os.makedirs(path_degraded_estimate)\n\t            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\t    def setup_wandb(self):\n\t        \"\"\"\n", "        Configure wandb, open a new run and log the configuration.\n\t        \"\"\"\n\t        config=omegaconf.OmegaConf.to_container(\n\t            self.args, resolve=True, throw_on_missing=True\n\t        )\n\t        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n\t        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n\t        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\t        self.use_wandb=True\n\t    def setup_wandb_run(self, run):\n", "        #get the wandb run object from outside (in trainer.py or somewhere else)\n\t        self.wandb_run=run\n\t        self.use_wandb=True\n\t    def setup_sampler(self):\n\t        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\t    def load_latest_checkpoint(self ):\n\t        #load the latest checkpoint from self.args.model_dir\n\t        try:\n\t            # find latest checkpoint_id\n\t            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n", "            save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t            list_weights = glob(save_name)\n\t            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n\t            list_ids = [int(id_regex.search(weight_path).groups()[0])\n\t                        for weight_path in list_weights]\n\t            checkpoint_id = max(list_ids)\n\t            state_dict = torch.load(\n\t                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n\t            self.network.load_state_dict(state_dict['ema'])\n\t            print(f\"Loaded checkpoint {checkpoint_id}\")\n", "            return True\n\t        except (FileNotFoundError, ValueError):\n\t            raise ValueError(\"No checkpoint found\")\n\t    def load_checkpoint(self, path):\n\t        state_dict = torch.load(path, map_location=self.device)\n\t        if self.args.exp.exp_name==\"diffwave-sr\":\n\t            print(state_dict.keys())\n\t            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n\t            self.network.load_state_dict(state_dict['ema_model'])\n\t            self.network.eval()\n", "            print(\"ckpt loaded\")\n\t        else:\n\t            try:\n\t                print(\"load try 1\")\n\t                self.network.load_state_dict(state_dict['ema'])\n\t            except:\n\t                #self.network.load_state_dict(state_dict['model'])\n\t                try:\n\t                    print(\"load try 2\")\n\t                    dic_ema = {}\n", "                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n\t                        dic_ema[key] = tensor\n\t                    self.network.load_state_dict(dic_ema)\n\t                except:\n\t                    print(\"load try 3\")\n\t                    dic_ema = {}\n\t                    i=0\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n\t                        if tensor.requires_grad:\n\t                            dic_ema[key]=state_dict['ema_weights'][i]\n", "                            i=i+1\n\t                        else:\n\t                            dic_ema[key]=tensor     \n\t                    self.network.load_state_dict(dic_ema)\n\t        try:\n\t            self.it=state_dict['it']\n\t        except:\n\t            self.it=0\n\t    def log_filter(self,preds, f, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n", "        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\t        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\t    def log_audio(self,preds, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n\t        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n\t        print(audio_path)\n\t        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n\t        #TODO: log spectrogram of the audio file to wandb\n\t        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n", "    def sample_unconditional_diffwavesr(self):\n\t        #print some parameters of self.network\n\t        #print(\"self.network\", self.network.input_projection[0].weight)\n\t        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n\t        #TODO assert that the audio_len is consistent with the model\n\t        rid=False\n\t        z_1=torch.randn(shape, device=self.device)\n\t        #print(\"sd\",z_1.std(-1))\n\t        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n\t        preds=outputs\n", "        self.log_audio(preds.detach(), \"unconditional\")\n\t        return preds\n\t    def sample_unconditional(self):\n\t        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n\t        #TODO assert that the audio_len is consistent with the model\n\t        rid=False\n\t        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n\t        if rid:\n\t            preds, data_denoised, t=outputs\n\t            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n", "        else:\n\t            preds=outputs\n\t        self.log_audio(preds, \"unconditional\")\n\t        return preds\n\t    def formal_test_bwe_small(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False, blind=False):\n\t        print(\"BLIND\", blind)\n\t        if typefilter==\"fc_A\":\n\t            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        else:\n", "            raise NotImplementedError\n\t        path=self.args.tester.formal_test_small.path\n\t        path_out=self.args.tester.formal_test_small.path_out\n\t        filenames=glob(path+\"/*.wav\")\n\t        assert len(filenames)>0, \"No examples found in path \"+path\n\t        for filename in filenames:\n\t            path, basename=os.path.split(filename)\n\t            n=os.path.splitext(basename)[0]\n\t            print(path, basename)\n\t            #open audio file\n", "            d,fs=sf.read(filename)\n\t            seg=torch.Tensor(d).to(self.device).unsqueeze(0)\n\t            assert fs==self.args.exp.sample_rate, \"Sample rate of audio file is not consistent with the one specified in the config file\"\n\t            assert seg.shape[-1]==self.args.exp.audio_len, \"Audio length of audio file is not consistent with the one specified in the config file\"\n\t            print(\"skippint?\", os.path.join(path_out,\"reconstructed\", basename))\n\t            if os.path.exists(os.path.join(path_out, \"reconstructed\",basename)):\n\t                print(\"yes skippint\", os.path.join(path_out, basename))\n\t                continue\n\t            print(\"skippint?\", os.path.join(path_out,\"reconstructed\", basename+\".wav\"))\n\t            if os.path.exists(os.path.join(path_out, \"reconstructed\",basename+\".wav\")):\n", "                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n\t                continue\n\t            if type==\"fc_A\":\n\t                y=self.apply_lowpass_fcA(seg, da_filter)\n\t            else:\n\t                raise NotImplementedError\n\t                y=self.apply_low_pass(D, da_filter, type)\n\t            rid=True\n\t            if blind:\n\t                outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n", "            else:\n\t                rid=False\n\t                outputs=self.sampler.predict_bwe(y, da_filter, type, rid=rid)\n\t            if rid:\n\t                pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n\t                #the logged outputs are:\n\t                #   pred: the reconstructed audio\n\t                #   estimated_filter: the estimated filter ([fc, A])\n\t                #   t: the time step vector\n\t                #   data_denoised: a vector with the denoised audio for each time step\n", "                #   data_filters: a vector with the estimated filters for each time step\n\t            else:\n\t                if blind:\n\t                    pred, estimated_filter =outputs\n\t                else:\n\t                    pred= outputs\n\t            #y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\t            #path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n\t            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=os.path.join(path_out, \"degraded\"))\n\t            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=os.path.join(path_out, \"reconstructed\"))\n", "            if blind:\n\t                with open(os.path.join(path_out,\"filters\", n+\".filter_data.pkl\"), \"wb\") as f:\n\t                    pickle.dump(estimated_filter, f)\n\t                freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n\t                H_true=blind_bwe_utils.design_filter(da_filter[0], da_filter[1], freqs)\n\t                H_Pred=blind_bwe_utils.design_filter(estimated_filter[0], estimated_filter[1], freqs)\n\t                #compute dB MSE between the true and the estimated filter\n\t                dB_MSE=torch.mean((20*torch.log10(H_true)-20*torch.log10(H_Pred))**2)\n\t                print(\"dB MSE\", dB_MSE)\n\t    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False):\n", "        print(\"BLIND\", blind)\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n\t        #test_bwe_table_audio = wandb.Table(columns=columns)\n\t        if not self.do_formal_test_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        if typefilter==\"fc_A\":\n", "            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        elif typefilter==\"3rdoct\":\n\t            type=\"3rdoct\"\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n\t            da_filter=da_filter.to(self.device)\n\t        else:\n\t            type=self.args.tester.bandwidth_extension.filter.type\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n", "            da_filter=da_filter.to(self.device)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.formal_test.path\n\t        filenames=glob(path+\"/*.wav\")\n\t        segL=self.args.exp.audio_len\n\t        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\t        for filename in filenames:\n\t            path, basename=os.path.split(filename)\n\t            print(path, basename)\n", "            #open audio file\n\t            d,fs=sf.read(filename)\n\t            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n\t            print(\"D\", D.shape, fs)\n\t            path_out=self.args.tester.formal_test.folder\n\t            print(\"skippint?\", os.path.join(path_out, basename))\n\t            if os.path.exists(os.path.join(path_out, basename)):\n\t                print(\"yes skippint\", os.path.join(path_out, basename))\n\t                continue\n\t            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n", "            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n\t                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n\t                continue\n\t            if type==\"fc_A\":\n\t                degraded=self.apply_lowpass_fcA(D, da_filter)\n\t            else:\n\t                degraded=self.apply_low_pass(D, da_filter, type)\n\t            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\t            print(\"filename\",filename)\n\t            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n", "            n=os.path.splitext(os.path.basename(filename))[0]\n\t            #degraded=degraded.float().to(self.device).unsqueeze(0)\n\t            print(n)\n\t            final_pred=torch.zeros_like(degraded)\n\t            print(\"dsds FS\",fs)\n\t            print(\"seg shape\",degraded.shape)\n\t            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t            print(\"seg shape\",degraded.shape)\n\t            std= degraded.std(-1)\n\t            rid=False\n", "            L=degraded.shape[-1]\n\t            #modify the sampler, so that it is computationally cheaper\n\t            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n\t            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\t            #first segment\n\t            ix=0\n\t            seg=degraded[...,ix:ix+segL]\n\t            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t            filter_data=[]\n\t            rid=False\n", "            if blind:\n\t                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                pred, estimated_filter =outputs\n\t                filter_data.append(((ix, ix+segL), estimated_filter))\n\t            else:\n\t                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False)\n\t            if self.args.tester.formal_test.use_AR:\n\t                assert not blind\n\t                previous_pred=pred[..., 0:segL-discard_end]\n\t                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n", "                ix+=segL-overlap-discard_end\n\t                y_masked=torch.zeros_like(pred, device=self.device)\n\t                mask=torch.ones_like(seg, device=self.device)\n\t                mask[...,overlap::]=0\n\t            else:\n\t                print(\"noar\")\n\t                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n\t                win_pred=pred[...,0:segL-discard_end]\n\t                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n\t                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n", "                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\t                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\t            if blind:\n\t                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                        pickle.dump(filter_data, f)\n\t            while ix<L-segL-discard_end-discard_start:\n\t                seg=degraded[...,ix:ix+segL]\n\t                if self.args.tester.formal_test.use_AR:\n\t                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n", "                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t                else:\n\t                    if blind:\n\t                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n\t                        pred, estimated_filter =outputs\n\t                        filter_data.append(((ix, ix+segL), estimated_filter))\n\t                    else:\n\t                        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t                previous_pred_win=pred[..., 0:segL-discard_end]\n\t                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n", "                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n\t                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\t                #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n\t                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\t                path, basename=os.path.split(filename)\n\t                print(path, basename)\n\t                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\t                if self.args.tester.formal_test.use_AR:\n\t                    ix+=segL-overlap-discard_end\n", "                else:\n\t                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\t                if blind:\n\t                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                        pickle.dump(filter_data, f)\n\t            #skipping the last segment, which is not complete, I am lazy\n\t            seg=degraded[...,ix::]\n\t            if self.args.tester.formal_test.use_AR:\n\t                y_masked[...,0:overlap]=pred[...,-overlap::]\n\t            if seg.shape[-1]<segL:\n", "                #cat zeros\n\t                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\t                if self.args.tester.formal_test.use_AR:\n\t                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t                    mask[...,seg.shape[-1]:segL]=0\n\t            else:\n\t                seg_zp=seg[...,0:segL]\n\t            if self.args.tester.formal_test.use_AR:\n\t                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n", "            else:\n\t                if blind:\n\t                    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n\t                    pred, estimated_filter =outputs\n\t                    filter_data.append(((ix, ix+segL), estimated_filter))\n\t                else:\n\t                    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t            if not self.args.tester.formal_test.use_AR:\n\t                win_pred=pred[...,0:seg.shape[-1]]\n\t                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n", "                final_pred[...,ix::]+=win_pred\n\t            else:\n\t                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t            #final_pred=final_pred*10**(-scale/20)\n\t            #extract path from filename\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n\t            #save filter_data in a pickle file\n\t            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n\t                pickle.dump(filter_data, f)\n", "    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n\t        test_bwe_table_audio = wandb.Table(columns=columns)\n\t        if not self.do_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        if typefilter==\"fc_A\":\n", "            type=\"fc_A\"\n\t            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n\t        elif typefilter==\"3rdoct\":\n\t            type=\"3rdoct\"\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n\t            da_filter=da_filter.to(self.device)\n\t        else:\n\t            type=self.args.tester.bandwidth_extension.filter.type\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n", "            da_filter=da_filter.to(self.device)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t            n=os.path.splitext(filename[0])[0]\n\t            seg=original.float().to(self.device)\n\t            seg=self.resample_audio(seg, fs)\n\t            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n\t            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n\t            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n", "            #        #add gain boost (in dB)\n\t            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\t            if type==\"fc_A\":\n\t                y=self.apply_lowpass_fcA(seg, da_filter)\n\t            else:\n\t                y=self.apply_low_pass(seg, da_filter, type)\n\t            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\t            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n\t            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n\t            #    y+=sigma*torch.randn(y.shape).to(y.device)\n", "            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n\t                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n\t                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t            print(\"y\", y.shape)\n\t            if test_filter_fit:\n\t                if compute_sweep:\n\t                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n", "                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n\t                    #save the data_norms and data_grads as a .npy file\n\t                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                else:\n\t                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n\t                    pred, data_denoised, data_score, t, data_filters =out\n\t            else:\n\t                rid=True\n\t                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n", "                pred, data_denoised, data_score, t =out\n\t            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n\t            #    #compensate gain boost\n\t            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t            #    #add gain boost (in dB)\n\t            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\t            res[i,:]=pred\n\t            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n", "            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\t            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\t            test_bwe_table_audio.add_data(i, \n\t                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t            if rid:\n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t            if test_filter_fit:\n", "                #expecting to crash here\n\t                print(data_filters.shape)\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\t        if self.use_wandb:\n\t            self.log_audio(res, \"bwe\")\n\t    def apply_low_pass(self, seg, filter, typefilter):\n\t        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n\t        return y\n\t    def apply_lowpass_fcA(self, seg, params):\n", "        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n\t        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n\t        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n\t        return xfilt\n\t    def prepare_filter(self, sample_rate, typefilter):\n\t        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n\t        return filter\n\t    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #raise NotImplementedError\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n", "        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        filename=self.args.tester.complete_recording.path\n\t        d,fs=sf.read(filename)\n", "        degraded=torch.Tensor(d)\n\t        segL=self.args.exp.audio_len\n\t        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\t        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\t        print(\"filename\",filename)\n\t        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t        degraded=degraded.float().to(self.device).unsqueeze(0)\n\t        print(n)\n\t        print(\"dsds FS\",fs)\n\t        print(\"seg shape\",degraded.shape)\n", "        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t        print(\"seg shape\",degraded.shape)\n\t        std= degraded.std(-1)\n\t        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n\t        #add noise\n\t        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n\t            #contaminate a bit with white noise\n\t            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n\t            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n\t            sigma=torch.sqrt(sigma2_s/SNR)\n", "            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\t        if self.args.tester.complete_recording.n_segments_blindstep==1:\n\t            y=degraded[...,ix_first:ix_first+segL]\n\t        else:\n\t            #initialize y with the first segment and repeat it\n\t            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n\t            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n\t                #random index\n\t                ix=np.random.randint(0, degraded.shape[-1]-segL)\n\t                y[j,...]=degraded[...,ix:ix+segL]\n", "        print(\"y shape\",y.shape)\n\t        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n\t        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\t        #y=y*10**(scale/20)\n\t        #degraded=degraded*10**(scale/20)\n\t        rid=False\n\t        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t        pred, estimated_filter =outputs\n\t        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\t        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n", "        hop=segL-overlap\n\t        final_pred=torch.zeros_like(degraded)\n\t        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\t        path, basename=os.path.split(filename)\n\t        print(path, basename)\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t        L=degraded.shape[-1]\n\t        #modify the sampler, so that it is computationally cheaper\n\t        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n\t        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n", "        #first segment\n\t        ix=0\n\t        seg=degraded[...,ix:ix+segL]\n\t        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t        previous_pred=pred[..., 0:segL-discard_end]\n\t        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t        ix+=segL-overlap-discard_end\n\t        y_masked=torch.zeros_like(pred, device=self.device)\n\t        mask=torch.ones_like(seg, device=self.device)\n\t        mask[...,overlap::]=0\n", "        hann_window=torch.hann_window(overlap*2, device=self.device)\n\t        while ix<L-segL-discard_end-discard_start:\n\t            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n\t            seg=degraded[...,ix:ix+segL]\n\t            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t            previous_pred=pred[..., 0:segL-discard_end]\n\t            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t            #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n\t            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n", "            path, basename=os.path.split(filename)\n\t            print(path, basename)\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t            ix+=segL-overlap-discard_end\n\t        #skipping the last segment, which is not complete, I am lazy\n\t        seg=degraded[...,ix::]\n\t        y_masked[...,0:overlap]=pred[...,-overlap::]\n\t        if seg.shape[-1]<segL:\n\t            #cat zeros\n\t            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n", "            #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t            mask[...,seg.shape[-1]:segL]=0\n\t        else:\n\t            seg_zp=seg[...,0:segL]\n\t        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t        #final_pred=final_pred*10**(-scale/20)\n\t        #extract path from filename\n", "        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n\t            columns=[\"id\", \"estimate_filter\"]\n", "            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n\t            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n", "            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.blind_bwe.real_recordings.path\n\t        audio_files=glob(path+\"/*.wav\")\n\t        test_set_data=[]\n\t        test_set_fs=[]\n", "        test_set_names=[]\n\t        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n\t            d,fs=sf.read(audio_files[i])\n\t            test_set_data.append(torch.Tensor(d))\n\t            print(\"fs\",fs)\n\t            print(\"len\",len(d))\n\t            test_set_names.append(audio_files[i])\n\t        for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\t                print(\"filename\",filename)\n\t                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n", "                seg=degraded.float().to(self.device).unsqueeze(0)\n\t                print(n)\n\t                print(\"dsds FS\",fs)\n\t                print(\"seg shape\",seg.shape)\n\t                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n\t                print(\"seg shape\",seg.shape)\n\t                ix_start=self.args.tester.blind_bwe\n\t                seg=seg[...,self.args.exp.sample_rate*5:self.args.exp.sample_rate*5+self.args.exp.audio_len]\n\t                y=seg\n\t                print(\"y shape\",y.shape)\n", "                #normalize???\n\t                std= y.std(-1)\n\t                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n\t                #print(\"scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n", "                #    sigma=torch.sqrt(sigma2_s/SNR)\n\t                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n", "                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n\t                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n\t                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n", "                else:\n\t                    pred, estimated_filter =outputs\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n\t                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n", "                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n\t                #y=y*10**(-scale/20)\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n\t                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n\t                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n", "                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t                if typefilter==\"fc_A\":\n\t                    test_blind_bwe_table_filters.add_data(i, \n\t                        wandb.Html(path_est_filter),\n\t                    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n", "                    #test_blind_bwe_table_spec.add_data(i, \n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters)\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n", "        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n\t            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n", "            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n\t            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n", "        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        if typefilter==\"fc_A\":\n\t            fc=self.args.tester.blind_bwe.test_filter.fc\n\t            A=self.args.tester.blind_bwe.test_filter.A\n\t            da_filter=torch.Tensor([fc, A]).to(self.device)\n\t        else:\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n\t            da_filter=da_filter.to(self.device)\n", "        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t                n=os.path.splitext(filename[0])[0]+typefilter\n\t                seg=original.float().to(self.device)\n\t                seg=self.resample_audio(seg, fs)\n\t                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                #    orig_std=seg.std(-1)\n\t                #    seg=sigma_norm*seg/orig_std\n", "                #elif self.args.tester.blind_bwe.gain_boost != 0:\n\t                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t                #    #add gain boost (in dB)\n\t                #    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n\t                #apply lowpass filter\n\t                if typefilter==\"fc_A\":\n\t                    y=self.apply_lowpass_fcA(seg, da_filter)\n\t                else:\n\t                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\t                #add noise to the observations for regularization\n", "                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n\t                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n\t                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n\t                #print(\"applied scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n", "                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n\t                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n", "                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n\t                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n", "                    #   t: the time step vector\n\t                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pred, estimated_filter =outputs\n\t                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\t                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                #    assert orig_std is not None\n\t                #    seg=orig_std*seg/sigma_norm\n", "                #elif self.args.tester.blind_bwe.gain_boost != 0:\n\t                #    #compensate gain boost\n\t                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t                #    #add gain boost (in dB)\n\t                #    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                #    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                #    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                #    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n", "                #seg=seg*10**(-scale/20)\n\t                #y=y*10**(-scale/20)\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n\t                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n", "                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n\t                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n\t                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n\t                #will probably crash here!\n\t                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n", "                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n\t                #if typefilter==\"fc_A\":\n\t                #    test_blind_bwe_table_filters.add_data(i, \n\t                #        wandb.Html(path_est_filter),\n\t                #    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n", "                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters.shape)\n\t                #will crash here\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n", "    def dodajob(self):\n\t        self.setup_wandb()\n\t        for m in self.args.tester.modes:\n\t            if m==\"unconditional\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional()\n\t            if m==\"unconditional_diffwavesr\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional_diffwavesr()\n\t            self.it+=1\n", "            if m==\"blind_bwe\":\n\t                print(\"TESTING BLIND BWE\")\n\t                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe\":\n\t                print(\"TESTING REAL BLIND BWE\")\n\t                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe_complete\":\n\t                #process the whole audio file\n\t                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n\t                print(\"TESTING REAL BLIND BWE COMPLETE\")\n", "                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n\t            if m==\"formal_test_bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n\t            if m==\"formal_test_bwe_small\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.formal_test_bwe_small(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"fc_A\", blind=self.args.tester.formal_test.blind)\n", "        self.it+=1\n"]}
{"filename": "testing/tester.py", "chunked_list": ["from datetime import date\n\timport re\n\timport torch\n\timport torchaudio\n\t#from src.models.unet_cqt import Unet_CQT\n\t#from src.models.unet_stft import Unet_STFT\n\t#from src.models.unet_1d import Unet_1d\n\t#import src.utils.setup as utils_setup\n\t#from src.sde import  VE_Sde_Elucidating\n\timport utils.dnnlib as dnnlib\n", "import os\n\timport utils.logging as utils_logging\n\timport wandb\n\timport copy\n\tfrom glob import glob\n\tfrom tqdm import tqdm\n\timport utils.bandwidth_extension as utils_bwe\n\timport utils.training_utils as t_utils\n\timport omegaconf\n\tclass Tester():\n", "    def __init__(\n\t        self, args, network, diff_params, test_set=None, device=None, it=None\n\t    ):\n\t        self.args=args\n\t        self.network=network\n\t        self.diff_params=copy.copy(diff_params)\n\t        self.device=device\n\t        #choose gpu as the device if possible\n\t        if self.device is None:\n\t            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "        self.network=network\n\t        torch.backends.cudnn.benchmark = True\n\t        today=date.today() \n\t        if it is None:\n\t            self.it=0\n\t        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n\t        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n\t        if not os.path.exists(self.path_sampling):\n\t            os.makedirs(self.path_sampling)\n\t        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n", "        self.setup_sampler()\n\t        self.use_wandb=False #hardcoded for now\n\t        #S=2\n\t        #if S>2.1 and S<2.2:\n\t        #    #resampling 48k to 22.05k\n\t        #    self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n\t        #elif S!=1:\n\t        #    N=int(self.args.exp.audio_len*S)\n\t        #    self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\t        if test_set is not None:\n", "            self.test_set=test_set\n\t            self.do_inpainting=True\n\t            self.do_bwe=True\n\t        else:\n\t            self.test_set=None\n\t            self.do_inpainting=False\n\t            self.do_bwe=False #these need to be set up in the config file\n\t        self.paths={}\n\t        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n\t            self.do_inpainting=True\n", "            mode=\"inpainting\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n\t            #TODO add more information in the subirectory names\n\t        else:\n\t            self.do_inpainting=False\n\t        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n\t            self.do_bwe=True\n\t            mode=\"bwe\"\n\t            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n\t            #TODO add more information in the subirectory names\n", "        else:\n\t            self.do_bwe=False\n\t        if (\"unconditional\" in self.args.tester.modes):\n\t            mode=\"unconditional\"\n\t            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\t        try:\n\t            self.stereo=self.args.tester.stereo\n\t        except:\n\t            self.stereo=False\n\t    def prepare_unc_experiment(self, str):\n", "            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            return path_exp\n\t    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"recosntucted\"):\n\t            path_exp=os.path.join(self.path_sampling,str)\n\t            if not os.path.exists(path_exp):\n\t                os.makedirs(path_exp)\n\t            n=str_degraded\n\t            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n", "            #ensure the path exists\n\t            if not os.path.exists(path_degraded):\n\t                os.makedirs(path_degraded)\n\t            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n\t            #ensure the path exists\n\t            if not os.path.exists(path_original):\n\t                os.makedirs(path_original)\n\t            n=str_reconstruced\n\t            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n\t            #ensure the path exists\n", "            if not os.path.exists(path_reconstructed):\n\t                os.makedirs(path_reconstructed)\n\t            return path_exp, path_degraded, path_original, path_reconstructed\n\t    def setup_wandb(self):\n\t        \"\"\"\n\t        Configure wandb, open a new run and log the configuration.\n\t        \"\"\"\n\t        config=omegaconf.OmegaConf.to_container(\n\t            self.args, resolve=True, throw_on_missing=True\n\t        )\n", "        self.wandb_run=wandb.init(project=\"testing\"+self.args.exp.wandb.project, entity=self.args.exp.wandb.entity, config=config)\n\t        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n\t        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\t        self.use_wandb=True\n\t    def setup_wandb_run(self, run):\n\t        #get the wandb run object from outside (in trainer.py or somewhere else)\n\t        self.wandb_run=run\n\t        self.use_wandb=True\n\t    def setup_sampler(self):\n\t        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network, diff_params=self.diff_params, args=self.args)\n", "    def load_latest_checkpoint(self ):\n\t        #load the latest checkpoint from self.args.model_dir\n\t        try:\n\t            # find latest checkpoint_id\n\t            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n\t            save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t            list_weights = glob(save_name)\n\t            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n\t            list_ids = [int(id_regex.search(weight_path).groups()[0])\n\t                        for weight_path in list_weights]\n", "            checkpoint_id = max(list_ids)\n\t            state_dict = torch.load(\n\t                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n\t            try:\n\t                self.network.load_state_dict(state_dict['ema'])\n\t            except Exception as e:\n\t                print(e)\n\t                print(\"Failed to load in strict mode, trying again without strict mode\")\n\t                self.network.load_state_dict(state_dict['model'], strict=False)\n\t            print(f\"Loaded checkpoint {checkpoint_id}\")\n", "            return True\n\t        except (FileNotFoundError, ValueError):\n\t            raise ValueError(\"No checkpoint found\")\n\t    def load_checkpoint(self, path):\n\t        state_dict = torch.load(path, map_location=self.device)\n\t        try:\n\t            self.it=state_dict['it']\n\t        except:\n\t            self.it=0\n\t        print(\"loading checkpoint\")\n", "        return t_utils.load_state_dict(state_dict, ema=self.network)\n\t    def load_checkpoint_legacy(self, path):\n\t        state_dict = torch.load(path, map_location=self.device)\n\t        try:\n\t            print(\"load try 1\")\n\t            self.network.load_state_dict(state_dict['ema'])\n\t        except:\n\t            #self.network.load_state_dict(state_dict['model'])\n\t            try:\n\t                print(\"load try 2\")\n", "                dic_ema = {}\n\t                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n\t                    dic_ema[key] = tensor\n\t                self.network.load_state_dict(dic_ema)\n\t            except:\n\t                print(\"load try 3\")\n\t                dic_ema = {}\n\t                i=0\n\t                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n\t                    if tensor.requires_grad:\n", "                        dic_ema[key]=state_dict['ema_weights'][i]\n\t                        i=i+1\n\t                    else:\n\t                        dic_ema[key]=tensor     \n\t                self.network.load_state_dict(dic_ema)\n\t        try:\n\t            self.it=state_dict['it']\n\t        except:\n\t            self.it=0\n\t    def log_audio(self,preds, mode:str):\n", "        string=mode+\"_\"+self.args.tester.name\n\t        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=os.path.join(self.args.model_dir, self.paths[mode]),stereo=self.stereo)\n\t        print(audio_path)\n\t        if self.use_wandb:\n\t            self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it)\n\t        #TODO: log spectrogram of the audio file to wandb\n\t        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t        if self.use_wandb:\n\t            self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it)\n\t    def sample_unconditional(self):\n", "        #the audio length is specified in the args.exp, doesnt depend on the tester\n\t        if self.stereo: \n\t            shape=[self.args.tester.unconditional.num_samples,2, self.args.exp.audio_len]\n\t        else:\n\t            shape=[self.args.tester.unconditional.num_samples, self.args.exp.audio_len]\n\t        #TODO assert that the audio_len is consistent with the model\n\t        preds=self.sampler.predict_unconditional(shape, self.device)\n\t        if self.use_wandb:\n\t            self.log_audio(preds, \"unconditional\")\n\t        else:\n", "            #TODO do something else if wandb is not used, like saving the audio file to the model directory\n\t            pass\n\t        return preds\n\t    def test_inpainting(self):\n\t        if not self.do_inpainting or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        self.inpainting_mask=torch.ones((1,self.args.exp.audio_len)).to(self.device) #assume between 5 and 6s of total length\n\t        gap=int(self.args.tester.inpainting.gap_length*self.args.exp.sample_rate/1000)      \n", "        if self.args.tester.inpainting.start_gap_idx ==\"None\": #we were crashing here!\n\t            #the gap is placed at the center\n\t            start_gap_index=int(self.args.exp.audio_len//2 - gap//2) \n\t        else:\n\t            start_gap_index=int(self.args.tester.inpainting.start_gap_idx*self.args.exp.sample_rate/1000)\n\t        self.inpainting_mask[...,start_gap_index:(start_gap_index+gap)]=0\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n", "        for i, (original, fs, filename) in enumerate(tqdm(self.test_set)):\n\t            n=os.path.splitext(filename[0])[0]\n\t            original=original.float().to(self.device)\n\t            seg=self.resample_audio(original, fs)\n\t            #seg=torchaudio.functional.resample(seg, self.args.exp.resample_factor, 1)\n\t            utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"original\"])\n\t            masked=seg*self.inpainting_mask\n\t            utils_logging.write_audio_file(masked, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"degraded\"])\n\t            pred=self.sampler.predict_inpainting(masked, self.inpainting_mask)\n\t            utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"reconstructed\"])\n", "            res[i,:]=pred\n\t        if self.use_wandb:\n\t            self.log_audio(res, \"inpainting\")\n\t        #TODO save the files in the subdirectory inpainting of the model directory\n\t    def resample_audio(self, audio, fs):\n\t        #this has been reused from the trainer.py\n\t        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\t    def sample_inpainting(self, y, mask):\n\t        y_masked=y*mask\n\t        #shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n", "        #TODO assert that the audio_len is consistent with the model\n\t        preds=self.sampler.predict_inpainting(y_masked, mask)\n\t        return preds\n\t    def test_bwe(self, typefilter=\"whateverIignoreit\"):\n\t        if not self.do_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n", "        #prepare lowpass filters\n\t        self.filter=utils_bwe.prepare_filter(self.args, self.args.exp.sample_rate)\n\t        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs, filename) in enumerate(tqdm(self.test_set)):\n\t            n=os.path.splitext(filename[0])[0]\n\t            original=original.float().to(self.device)\n\t            seg=self.resample_audio(original, fs)\n\t            utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\t            y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n", "            if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                sigma2_s=torch.var(y, -1)\n\t                sigma=torch.sqrt(sigma2_s/SNR)\n\t                y+=sigma*torch.randn(y.shape).to(y.device)\n\t            utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\t            pred=self.sampler.predict_bwe(y, self.filter, self.args.tester.bandwidth_extension.filter.type)\n\t            utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\t            res[i,:]=pred\n\t        if self.use_wandb:\n", "            self.log_audio(res, \"bwe\")\n\t            #preprocess the audio file if necessary\n\t    def dodajob(self):\n\t        self.setup_wandb()\n\t        if \"unconditional\" in self.args.tester.modes:\n\t            print(\"testing unconditional\")\n\t            self.sample_unconditional()\n\t        self.it+=1\n\t        if \"blind_bwe\" in self.args.tester.modes:\n\t            print(\"testing blind bwe\")\n", "            #tester.test_blind_bwe(typefilter=\"whatever\")\n\t            self.tester.test_blind_bwe(typefilter=\"3rdoct\")\n\t        self.it+=1\n\t        if \"filter_bwe\" in self.args.tester.modes:\n\t            print(\"testing filter bwe\")\n\t            self.test_filter_bwe(typefilter=\"3rdoct\")\n\t        self.it+=1\n\t        if \"unconditional_operator\" in self.args.tester.modes:\n\t            print(\"testing unconditional operator\")\n\t            self.sample_unconditional_operator()\n", "        self.it+=1\n\t        if \"bwe\" in self.args.tester.modes:\n\t            print(\"testing bwe\")\n\t            self.test_bwe(typefilter=\"3rdoct\")\n\t        self.it+=1\n\t        if \"inpainting\" in self.args.tester.modes:\n\t            self.test_inpainting()\n\t        self.it+=1\n\t        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n"]}
{"filename": "testing/denoise_and_bwe_tester.py", "chunked_list": ["from datetime import date\n\timport pickle\n\timport re\n\timport torch\n\timport torchaudio\n\t#from src.models.unet_cqt import Unet_CQT\n\t#from src.models.unet_stft import Unet_STFT\n\t#from src.models.unet_1d import Unet_1d\n\t#import src.utils.setup as utils_setup\n\t#from src.sde import  VE_Sde_Elucidating\n", "import numpy as np\n\timport utils.dnnlib as dnnlib\n\timport os\n\timport utils.logging as utils_logging\n\timport wandb\n\timport copy\n\tfrom glob import glob\n\tfrom tqdm import tqdm\n\timport utils.bandwidth_extension as utils_bwe\n\timport utils.setup as utils_setup\n", "import omegaconf\n\t#import utils.filter_generation_utils as f_utils\n\timport utils.blind_bwe_utils as blind_bwe_utils\n\timport utils.training_utils as t_utils\n\timport soundfile as sf\n\t#from utils.spectral_analysis import LTAS_processor\n\tclass BlindTester():\n\t    def __init__(\n\t        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n\t    ):\n", "        self.args=args\n\t        self.network=torch.compile(network)\n\t        #self.network=network\n\t        #prnt number of parameters\n\t        self.diff_params=copy.copy(diff_params)\n\t        self.device=device\n\t        #choose gpu as the device if possible\n\t        if self.device is None:\n\t            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        self.network=network\n", "        torch.backends.cudnn.benchmark = True\n\t        today=date.today() \n\t        if it is None:\n\t            self.it=0\n\t        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n\t        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n\t        if not os.path.exists(self.path_sampling):\n\t            os.makedirs(self.path_sampling)\n\t        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n\t        self.setup_sampler()\n", "        self.use_wandb=False #hardcoded for now\n\t        S=self.args.exp.resample_factor\n\t        if S>2.1 and S<2.2:\n\t            #resampling 48k to 22.05k\n\t            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n\t        elif S!=1:\n\t            N=int(self.args.exp.audio_len*S)\n\t            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\t        self.denoiser=utils_setup.setup_denoiser(self.args, self.device)\n\t        self.denoiser.load_state_dict(torch.load(self.args.tester.denoiser.checkpoint, map_location=self.device))\n", "        self.denoiser.to(self.device)\n\t    def resample_audio(self, audio, fs):\n\t        #this has been reused from the trainer.py\n\t        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\t    def setup_wandb(self):\n\t        \"\"\"\n\t        Configure wandb, open a new run and log the configuration.\n\t        \"\"\"\n\t        config=omegaconf.OmegaConf.to_container(\n\t            self.args, resolve=True, throw_on_missing=True\n", "        )\n\t        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n\t        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n\t        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\t        self.use_wandb=True\n\t    def setup_wandb_run(self, run):\n\t        #get the wandb run object from outside (in trainer.py or somewhere else)\n\t        self.wandb_run=run\n\t        self.use_wandb=True\n\t    def setup_sampler(self):\n", "        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\t    def apply_denoiser(self,x):\n\t        segment_size=self.args.tester.denoiser.sample_rate_denoiser*self.args.tester.denoiser.segment_size\n\t        length_data=x.shape[-1]\n\t        overlapsize=1024 #hardcoded for now\n\t        window=torch.hamming_window(window_length=2*overlapsize).to(self.device)\n\t        window_left=window[:overlapsize]\n\t        window_right=window[overlapsize:]\n\t        audio_finished=False\n\t        pointer=0\n", "        denoised_data=torch.zeros_like(x)\n\t        #numchunks=torch.ceil(torch.Tensor(lenght_data/segment_size)).int()\n\t        while (not(audio_finished)):\n\t            #for i in tqdm(range(numchunks)):\n\t            if pointer+segment_size<length_data:\n\t                segment=x[:,pointer:pointer+segment_size]\n\t                x_den=self.apply_denoiser_model(segment)\n\t                if pointer==0:\n\t                    x_den=torch.cat((x_den[:,0:int(segment_size-overlapsize)], x_den[:,int(segment_size-overlapsize):segment_size]*window_right), axis=-1)\n\t                else:\n", "                    x_den=torch.cat((x_den[...,0:int(overlapsize)]*window_left, x_den[:,int(overlapsize):int(segment_size-overlapsize)], x_den[:,int(segment_size-overlapsize):segment_size]*window_right), axis=-1)\n\t                denoised_data[:,pointer:pointer+segment_size]+=x_den\n\t                pointer+=(segment_size-overlapsize)\n\t            else:\n\t                segment=x[:,pointer:]\n\t                lensegment=segment.shape[-1]\n\t                segment=torch.cat((segment, torch.zeros(segment.shape[0],segment_size-lensegment).to(self.device)),-1)\n\t                audio_finished=True\n\t                x_den=self.apply_denoiser_model(segment)\n\t                if pointer!=0:\n", "                    x_den=torch.cat((x_den[...,0:int(overlapsize)]*window_left, x_den[:,int(overlapsize):int(segment_size-overlapsize)]), axis=-1)\n\t                denoised_data[:,pointer:]+=x_den[...,0:lensegment]\n\t        return denoised_data\n\t    def apply_denoiser_model(self, x):\n\t        win_size=self.args.tester.denoiser.stft_win_size\n\t        hop_size=self.args.tester.denoiser.stft_hop_size\n\t        window=torch.hamming_window(window_length=win_size).to(self.device)\n\t        x=torch.cat((x, torch.zeros(x.shape[0],win_size).to(self.device)),-1)\n\t        X=torch.stft(x, win_size, hop_length=hop_size,window=window,center=False,return_complex=False)\n\t        X=X.permute(0,3,2,1) #shape= (batch_size, R/I, time, freq)\n", "        #segment_TF_ds=tf.data.Dataset.from_tensors(segment_TF)\n\t        with torch.no_grad():\n\t            #print( X.shape, X.dtype)\n\t            pred = self.denoiser(X)\n\t        if self.args.tester.denoiser.num_stages>1:\n\t            pred=pred[0]\n\t        pred=pred.permute(0,3,2,1).contiguous()\n\t        pred=torch.view_as_complex(pred)\n\t        pred_time=torch.istft(pred, win_size, hop_length=hop_size, window=window, center=False, return_complex=False)\n\t        return pred_time[...,0:x.shape[-1]]\n", "    def load_latest_checkpoint(self ):\n\t        #load the latest checkpoint from self.args.model_dir\n\t        try:\n\t            # find latest checkpoint_id\n\t            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n\t            save_name = f\"{self.args.model_dir}/{save_basename}\"\n\t            list_weights = glob(save_name)\n\t            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n\t            list_ids = [int(id_regex.search(weight_path).groups()[0])\n\t                        for weight_path in list_weights]\n", "            checkpoint_id = max(list_ids)\n\t            state_dict = torch.load(\n\t                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n\t            self.network.load_state_dict(state_dict['ema'])\n\t            print(f\"Loaded checkpoint {checkpoint_id}\")\n\t            return True\n\t        except (FileNotFoundError, ValueError):\n\t            raise ValueError(\"No checkpoint found\")\n\t    def load_checkpoint(self, path):\n\t        state_dict = torch.load(path, map_location=self.device)\n", "        if self.args.exp.exp_name==\"diffwave-sr\":\n\t            print(state_dict.keys())\n\t            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n\t            self.network.load_state_dict(state_dict['ema_model'])\n\t            self.network.eval()\n\t            print(\"ckpt loaded\")\n\t        else:\n\t            try:\n\t                print(\"load try 1\")\n\t                self.network.load_state_dict(state_dict['ema'])\n", "            except:\n\t                #self.network.load_state_dict(state_dict['model'])\n\t                try:\n\t                    print(\"load try 2\")\n\t                    dic_ema = {}\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n\t                        dic_ema[key] = tensor\n\t                    self.network.load_state_dict(dic_ema)\n\t                except:\n\t                    print(\"load try 3\")\n", "                    dic_ema = {}\n\t                    i=0\n\t                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n\t                        if tensor.requires_grad:\n\t                            dic_ema[key]=state_dict['ema_weights'][i]\n\t                            i=i+1\n\t                        else:\n\t                            dic_ema[key]=tensor     \n\t                    self.network.load_state_dict(dic_ema)\n\t        try:\n", "            self.it=state_dict['it']\n\t        except:\n\t            self.it=0\n\t    def log_audio(self,preds, mode:str):\n\t        string=mode+\"_\"+self.args.tester.name\n\t        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n\t        print(audio_path)\n\t        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n\t        #TODO: log spectrogram of the audio file to wandb\n\t        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n", "        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\t    def apply_low_pass(self, seg, filter, typefilter):\n\t        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n\t        return y\n\t    def apply_lowpass_fcA(self, seg, params):\n\t        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n\t        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n\t        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n\t        return xfilt\n\t    def prepare_filter(self, sample_rate, typefilter):\n", "        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n\t        return filter\n\t    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False, use_denoiser=True):\n\t        #raise NotImplementedError\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        filename=self.args.tester.complete_recording.path\n\t        path, basename=os.path.split(filename)\n\t        print(path, basename)\n", "        d,fs=sf.read(filename)\n\t        #stereo to mono\n\t        print(\"d.shape\",d.shape)\n\t        if len(d.shape)>1:\n\t            d=d.mean(axis=1)\n\t        degraded=torch.Tensor(d)\n\t        segL=self.args.exp.audio_len\n\t        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\t        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\t        print(\"filename\",filename)\n", "        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t        degraded=degraded.float().to(self.device).unsqueeze(0)\n\t        print(n)\n\t        if self.args.tester.complete_recording.use_denoiser:\n\t                print(\"denoising\")\n\t                if fs!=self.args.tester.denoiser.sample_rate_denoiser:\n\t                    degraded=torchaudio.functional.resample(degraded, fs, self.args.tester.denoiser.sample_rate_denoiser)\n\t                degraded=self.apply_denoiser(degraded)\n\t                if fs!=self.args.tester.denoiser.sample_rate_denoiser:\n\t                    degraded=torchaudio.functional.resample(degraded,  self.args.tester.denoiser.sample_rate_denoiser, self.args.exp.sample_rate)\n", "                path_reconstructed=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".denoised.wav\", path=path)\n\t        else:\n\t                print(\"no denoising\")\n\t                degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\t        print(\"seg shape\",degraded.shape)\n\t        std= degraded.std(-1)\n\t        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n\t        #add noise\n\t        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n\t            #contaminate a bit with white noise\n", "            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n\t            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n\t            sigma=torch.sqrt(sigma2_s/SNR)\n\t            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\t        if self.args.tester.complete_recording.n_segments_blindstep==1:\n\t            y=degraded[...,ix_first:ix_first+segL]\n\t        else:\n\t            #initialize y with the first segment and repeat it\n\t            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n\t            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n", "                #random index\n\t                ix=np.random.randint(0, degraded.shape[-1]-segL)\n\t                y[j,...]=degraded[...,ix:ix+segL]\n\t        print(\"y shape\",y.shape)\n\t        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n\t        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\t        #y=y*10**(scale/20)\n\t        #degraded=degraded*10**(scale/20)\n\t        rid=False\n\t        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n", "        pred, estimated_filter =outputs\n\t        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\t        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\t        hop=segL-overlap\n\t        final_pred=torch.zeros_like(degraded)\n\t        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t        L=degraded.shape[-1]\n\t        #modify the sampler, so that it is computationally cheaper\n\t        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n", "        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\t        #first segment\n\t        ix=0\n\t        seg=degraded[...,ix:ix+segL]\n\t        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\t        previous_pred=pred[..., 0:segL-discard_end]\n\t        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t        ix+=segL-overlap-discard_end\n\t        y_masked=torch.zeros_like(pred, device=self.device)\n\t        mask=torch.ones_like(seg, device=self.device)\n", "        mask[...,overlap::]=0\n\t        hann_window=torch.hann_window(overlap*2, device=self.device)\n\t        while ix<L-segL-discard_end-discard_start:\n\t            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n\t            seg=degraded[...,ix:ix+segL]\n\t            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t            previous_pred=pred[..., 0:segL-discard_end]\n\t            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\t            #do a little bit of overlap and add with a hann window to avoid discontinuities\n\t            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n", "            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\t            path, basename=os.path.split(filename)\n\t            print(path, basename)\n\t            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t            ix+=segL-overlap-discard_end\n\t        #skipping the last segment, which is not complete, I am lazy\n\t        seg=degraded[...,ix::]\n\t        y_masked[...,0:overlap]=pred[...,-overlap::]\n\t        if seg.shape[-1]<segL:\n\t            #cat zeros\n", "            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\t            #the cat zeroes will also be part of the observed signal, so I need to mask them\n\t            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n\t            mask[...,seg.shape[-1]:segL]=0\n\t        else:\n\t            seg_zp=seg[...,0:segL]\n\t        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\t        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\t        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n\t        #final_pred=final_pred*10**(-scale/20)\n", "        #extract path from filename\n\t        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\t    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n", "            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n\t            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n", "            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        path=self.args.tester.blind_bwe.real_recordings.path\n\t        audio_files=glob(path+\"/*.wav\")\n\t        print(audio_files, path)\n", "        test_set_data=[]\n\t        test_set_fs=[]\n\t        test_set_names=[]\n\t        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n\t            d,fs=sf.read(audio_files[i])\n\t            #print(d.shape, self.args.exp.audio_len)\n\t            #if d.shape[-1] >= self.args.exp.audio_len:\n\t            #    d=d[...,0:self.args.exp.audio_len]\n\t            test_set_data.append(torch.Tensor(d))\n\t            test_set_fs.append(fs)\n", "            print(\"fs\",fs)\n\t            print(\"len\",len(d))\n\t            test_set_names.append(audio_files[i])\n\t        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n\t                print(\"filename\",filename)\n\t                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n\t                seg=degraded.float().to(self.device).unsqueeze(0)\n\t                print(n)\n\t                print(\"dsds FS\",fs)\n\t                print(\"seg shape\",seg.shape)\n", "                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n\t                print(\"seg shape\",seg.shape)\n\t                ix_start=self.args.tester.blind_bwe\n\t                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n\t                y=seg\n\t                print(\"y shape\",y.shape)\n\t                #normalize???\n\t                std= y.std(-1)\n\t                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n", "                #print(\"scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n\t                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n", "                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n", "                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n\t                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pred, estimated_filter =outputs\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n", "                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n", "                #y=y*10**(-scale/20)\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n\t                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n\t                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\t                if typefilter==\"fc_A\":\n", "                    test_blind_bwe_table_filters.add_data(i, \n\t                        wandb.Html(path_est_filter),\n\t                    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n\t                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters)\n", "                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\t        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n\t        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n\t        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\t        if typefilter==\"3rdoct\":\n", "            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        elif typefilter==\"fc_A\":\n\t            columns=[\"id\", \"estimate_filter\"]\n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        else:\n\t            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n\t            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\t        log_spec=False\n\t        if log_spec:\n", "            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n\t            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\t        if not self.do_blind_bwe or self.test_set is None:\n\t            print(\"No test set specified, skipping inpainting test\")\n\t            return\n\t        assert self.test_set is not None\n\t        if len(self.test_set) == 0:\n\t            print(\"No samples found in test set\")\n\t        if typefilter==\"fc_A\":\n\t            fc=self.args.tester.blind_bwe.test_filter.fc\n", "            A=self.args.tester.blind_bwe.test_filter.A\n\t            da_filter=torch.Tensor([fc, A]).to(self.device)\n\t        else:\n\t            #prepare lowpass filters\n\t            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n\t            da_filter=da_filter.to(self.device)\n\t        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n\t        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\t        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n\t                n=os.path.splitext(filename[0])[0]+typefilter\n", "                seg=original.float().to(self.device)\n\t                seg=self.resample_audio(seg, fs)\n\t                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                    orig_std=seg.std(-1)\n\t                    seg=sigma_norm*seg/orig_std\n\t                elif self.args.tester.blind_bwe.gain_boost != 0:\n\t                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t                    #add gain boost (in dB)\n\t                    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n", "                #apply lowpass filter\n\t                if typefilter==\"fc_A\":\n\t                    y=self.apply_lowpass_fcA(seg, da_filter)\n\t                else:\n\t                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\t                #add noise to the observations for regularization\n\t                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n\t                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n\t                    sigma2_s=torch.var(y, -1)\n\t                    sigma=torch.sqrt(sigma2_s/SNR)\n", "                    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\t                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n\t                #print(\"applied scale\",scale)\n\t                #y=y*10**(scale/20)\n\t                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\t                #if self.args.tester.noise_in_observations_SNR != \"None\":\n\t                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n\t                #    sigma2_s=torch.var(y, -1)\n\t                #    sigma=torch.sqrt(sigma2_s/SNR)\n", "                #    y+=sigma*torch.randn(y.shape).to(y.device)\n\t                rid=True\n\t                if compute_sweep:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n\t                else:\n\t                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n\t                if rid:\n\t                    if compute_sweep:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n", "                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n\t                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n\t                    else:\n\t                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n\t                    #the logged outputs are:\n\t                    #   pred: the reconstructed audio\n\t                    #   estimated_filter: the estimated filter ([fc, A])\n\t                    #   t: the time step vector\n", "                    #   data_denoised: a vector with the denoised audio for each time step\n\t                    #   data_filters: a vector with the estimated filters for each time step\n\t                else:\n\t                    pred, estimated_filter =outputs\n\t                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\t                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n\t                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n\t                    assert orig_std is not None\n\t                    seg=orig_std*seg/sigma_norm\n\t                elif self.args.tester.blind_bwe.gain_boost != 0:\n", "                    #compensate gain boost\n\t                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n\t                    #add gain boost (in dB)\n\t                    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\t                #y_est=y_est*10**(-scale/20)\n\t                #pred=pred*10**(-scale/20)\n\t                #seg=seg*10**(-scale/20)\n", "                #y=y*10**(-scale/20)\n\t                #if self.use_wandb:\n\t                #add to principal wandb table\n\t                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n\t                #acum_orig[i,:]=seg\n\t                #acum_deg[i,:]=y\n\t                #acum_bwe[i,:]=pred\n\t                #acum_ded_est[i,:]=y_est\n\t                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n\t                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n", "                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n\t                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n\t                #will probably crash here!\n\t                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n\t                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n\t                fig_est_filter.write_html(path_est_filter, auto_play = False)\n\t                test_blind_bwe_table_audio.add_data(i, \n\t                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n\t                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n", "                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n\t                #if typefilter==\"fc_A\":\n\t                #    test_blind_bwe_table_filters.add_data(i, \n\t                #        wandb.Html(path_est_filter),\n\t                #    )\n\t                if log_spec:\n\t                    pass\n\t                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\t                    #test_blind_bwe_table_spec.add_data(i, \n\t                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n", "                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n\t                print(data_filters.shape)\n\t                #will crash here\n\t                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\t                #fig_join_animation=utils_logging.diffusion_joint_animation()\n\t                #log the \n\t        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n\t        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\t        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n\t    def dodajob(self):\n", "        self.setup_wandb()\n\t        for m in self.args.tester.modes:\n\t            if m==\"unconditional\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional()\n\t            if m==\"unconditional_diffwavesr\":\n\t                print(\"testing unconditional\")\n\t                self.sample_unconditional_diffwavesr()\n\t            self.it+=1\n\t            if m==\"blind_bwe\":\n", "                print(\"TESTING BLIND BWE\")\n\t                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe\":\n\t                print(\"TESTING REAL BLIND BWE\")\n\t                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n\t            if m==\"real_blind_bwe_complete\":\n\t                #process the whole audio file\n\t                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n\t                print(\"TESTING REAL BLIND BWE COMPLETE\")\n\t                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n", "            if m==\"bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n\t            if m==\"formal_test_bwe\": \n\t                print(\"TESTING NORMAL BWE\")\n\t                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n\t        self.it+=1\n"]}
