{"filename": "train_base.py", "chunked_list": ["import os\n\timport datetime\n\timport random\n\timport time\n\timport cv2\n\tfrom cv2 import mean\n\timport numpy as np\n\timport logging\n\timport argparse\n\tfrom visdom import Visdom\n", "import os.path as osp\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.nn.parallel\n\timport torch.optim\n\timport torch.utils.data\n\timport torch.multiprocessing as mp\n\timport torch.distributed as dist\n", "from torch.cuda.amp import autocast as autocast\n\tfrom torch.cuda import amp\n\tfrom torch.utils.data.distributed import DistributedSampler\n\tfrom model.util import PSPNet\n\tfrom dataset import iSAID, iSAID_1\n\tfrom util import config\n\tfrom util.util import AverageMeter, intersectionAndUnionGPU, get_model_para_number, setup_seed, get_logger, get_save_path, \\\n\t                             fix_bn, check_makedirs,lr_decay, Special_characters\n\tcv2.ocl.setUseOpenCL(False)\n\tcv2.setNumThreads(0)\n", "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\tdef get_parser():\n\t    parser = argparse.ArgumentParser(description='PyTorch Few-Shot Semantic Segmentation')\n\t    parser.add_argument('--arch', type=str, default='PSPNet', help='') # \n\t    parser.add_argument('--split', type=int, default=1, help='') # \n\t    parser.add_argument('--dataset', type=str, default='iSAID', help='') # \n\t    parser.add_argument('--backbone', type=str, default='vgg', help='') # \n\t    parser.add_argument('--variable1', type=str, default='', help='') #\n\t    parser.add_argument('--variable2', type=str, default='', help='') #\n\t    parser.add_argument('--local_rank', type=int, default=-1, help='number of cpu threads to use during batch generation')    \n", "    parser.add_argument('--opts', help='see config/ade20k/ade20k_pspnet50.yaml for all options', default=None, nargs=argparse.REMAINDER)\n\t    args = parser.parse_args()\n\t    base_config = 'config/pretrain/{}.yaml'.format(args.dataset)\n\t    # data_config = 'config/dataset/{}.yaml'.format(args.dataset)\n\t    # assert args.config is not None\n\t    cfg = config.load_cfg_from_cfg_file([base_config])\n\t    cfg = config.merge_cfg_from_args(cfg, args)\n\t    if args.opts is not None:\n\t        cfg = config.merge_cfg_from_list(cfg, args.opts)\n\t    cfg.snapshot_path = 'initmodel/PSPNet/{}/split{}/{}/'.format(args.dataset,  args.split, args.backbone)\n", "    cfg.result_path = 'initmodel/PSPNet/{}/split{}/{}/result/'.format(args.dataset,  args.split, args.backbone)\n\t    return cfg\n\tdef get_model(args):\n\t    model = eval(args.arch).OneModel(args)\n\t    optimizer = model.get_optim(model, args.lr_decay, LR=args.base_lr)\n\t    if args.distributed:\n\t        # Initialize Process Group\n\t        dist.init_process_group(backend='nccl')\n\t        print('args.local_rank: ', args.local_rank)\n\t        torch.cuda.set_device(args.local_rank)\n", "        device = torch.device('cuda', args.local_rank)\n\t        model.to(device)\n\t        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n\t        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n\t    else:\n\t        model = model.cuda()\n\t    # Resume\n\t    check_makedirs(args.snapshot_path)\n\t    check_makedirs(args.result_path)\n\t    if args.resume:\n", "        resume_path = osp.join(args.snapshot_path, args.resume)\n\t        if os.path.isfile(resume_path):\n\t            if main_process():\n\t                logger.info(\"=> loading checkpoint '{}'\".format(resume_path))\n\t            checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n\t            args.start_epoch = checkpoint['epoch']\n\t            new_param = checkpoint['state_dict']\n\t            try: \n\t                model.load_state_dict(new_param)\n\t            except RuntimeError:                   # 1GPU loads mGPU model\n", "                for key in list(new_param.keys()):\n\t                    new_param[key[7:]] = new_param.pop(key)\n\t                model.load_state_dict(new_param)\n\t            optimizer.load_state_dict(checkpoint['optimizer'])\n\t            if main_process():\n\t                logger.info(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n\t        else:\n\t            if main_process():       \n\t                logger.info(\"=> no checkpoint found at '{}'\".format(resume_path))\n\t    # Get model para.\n", "    total_number, learnable_number = get_model_para_number(model)\n\t    if main_process():\n\t        print('Number of Parameters: %d' % (total_number))\n\t        print('Number of Learnable Parameters: %d' % (learnable_number))\n\t    time.sleep(5)\n\t    return model, optimizer\n\tdef main_process():\n\t    return not args.distributed or (args.distributed and (args.local_rank == 0))\n\tdef main():\n\t    global args, logger\n", "    args = get_parser()\n\t    logger = get_logger()\n\t    args.logger = logger\n\t    # args.distributed = False # Debug\n\t    args.distributed = True if torch.cuda.device_count() > 1 else False\n\t    shuffle = False if args.distributed else True\n\t    if main_process():\n\t        print(args)\n\t    if args.manual_seed is not None:\n\t        setup_seed(args.manual_seed, args.seed_deterministic)\n", "    if main_process():\n\t        logger.info(\"=> creating dataset ...\")\n\t# ----------------------  DATASET  ----------------------\n\t    train_data = eval('{}.{}_base_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n\t                                             mode='train', transform_dict=args.train_transform)\n\t    train_sampler = DistributedSampler(train_data) if args.distributed else None\n\t    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=shuffle, \\\n\t                                            num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n\t    # Val\n\t    val_data =  eval('{}.{}_base_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n", "                                             mode='val', transform_dict=args.val_transform)            \n\t    val_sampler = DistributedSampler(val_data) if args.distributed else None                  \n\t    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size_val, shuffle=False, \\\n\t                                            num_workers=args.workers, pin_memory=False, sampler=val_sampler)\n\t    args.base_class_num = len(train_data.list)\n\t    logger.info('train_list: {}'.format(train_data.list))\n\t    logger.info('num_train_data: {}'.format(len(train_data)))\n\t    logger.info('val_list: {}'.format(val_data.list))\n\t    logger.info('num_val_data: {}'.format(len(val_data)))\n\t    time.sleep(2)\n", "    if main_process():\n\t        logger.info(\"=> creating model ...\")\n\t    model, optimizer = get_model(args)\n\t    logger.info(model)\n\t# ----------------------  TRAINVAL  ----------------------\n\t    global best_miou, best_FBiou, best_epoch, keep_epoch, val_num\n\t    global best_name, grow_name, all_name, latest_name\n\t    best_miou = 0.\n\t    best_FBiou = 0.\n\t    best_epoch = 0\n", "    keep_epoch = 0\n\t    val_num = 0\n\t    start_time = time.time()\n\t    scaler = amp.GradScaler()\n\t#--------------------------- FilenamePrepare -----------------------------\n\t    latest_name = args.snapshot_path + 'latest.pth'\n\t    best_name = args.snapshot_path + 'best.pth'\n\t    grow_name = args.snapshot_path + 'grow.txt'\n\t    all_name = args.snapshot_path + 'all.txt'\n\t    for epoch in range(args.start_epoch, args.epochs):\n", "        if keep_epoch == args.stop_interval:\n\t            break\n\t        if args.fix_random_seed_val:\n\t            setup_seed(args.manual_seed + epoch, args.seed_deterministic)\n\t        epoch_log = epoch + 1\n\t        keep_epoch += 1\n\t        # ----------------------  TRAIN  ----------------------\n\t        train(train_loader, val_loader, model, optimizer, epoch, scaler)\n\t        # save model for <resuming>\n\t        if ((epoch + 1) % args.save_freq == 0) and main_process():\n", "            logger.info('Saving checkpoint to: ' + latest_name)\n\t            if osp.exists(latest_name):\n\t                os.remove(latest_name)            \n\t            torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, latest_name)\n\t        # -----------------------  VAL  -----------------------\n\t        if args.evaluate and (epoch + 1)% args.val_freq == 0:\n\t            _,fbIou, _,_, mIoU,_ , recall, precision = validate(val_loader, model)   \n\t            val_num += 1\n\t            with open(all_name, 'a') as f:\n\t                f.write('[{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch, mIoU, fbIou, recall, precision))\n", "        # save model for <testing> and <fine-tuning>\n\t            if mIoU > best_miou:\n\t                best_miou, best_epoch = mIoU, epoch\n\t                keep_epoch = 0\n\t                with open(grow_name, 'a') as f:\n\t                    f.write('Best_epoch:{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch , best_miou, fbIou, recall, precision)) \n\t                logger.info('Saving checkpoint to: ' + best_name)\n\t                if osp.exists(best_name):\n\t                    os.remove(best_name)    \n\t                torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name)  \n", "    total_time = time.time() - start_time\n\t    t_m, t_s = divmod(total_time, 60)\n\t    t_h, t_m = divmod(t_m, 60)\n\t    total_time = '{:02d}h {:02d}m {:02d}s'.format(int(t_h), int(t_m), int(t_s))\n\t    print('\\nEpoch: {}/{} \\t Total running time: {}'.format(epoch_log, args.epochs, total_time))\n\t    print('The number of models validated: {}'.format(val_num))            \n\t    print('\\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Final Best Result   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n\t    print(args.arch + '\\t Group:{} \\t Best_mIoU:{:.4f} \\t Best_FBIoU:{:.4f} \\t Best_step:{}'.format(args.split, best_miou, best_FBiou, best_epoch))\n\t    print('>'*80)\n\t    print ('当前的日期和时间是 %s' % datetime.datetime.now())\n", "def train(train_loader, val_loader, model, optimizer, epoch ,scaler):\n\t    global best_miou, best_epoch, keep_epoch, val_num\n\t    batch_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    main_loss_meter = AverageMeter()\n\t    aux_loss_meter_1 = AverageMeter()\n\t    aux_loss_meter_2 = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    intersection_meter = AverageMeter()\n\t    union_meter = AverageMeter()\n", "    target_meter = AverageMeter()\n\t    tmp_num = 0\n\t    model.train()\n\t    if args.fix_bn:\n\t        model.apply(fix_bn) # fix batchnorm\n\t    end = time.time()\n\t    val_time = 0.\n\t    max_iter = args.epochs * len(train_loader)\n\t    current_characters = Special_characters[random.randint(1,len(Special_characters)-1)]\n\t    current_GPU = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n", "    for i, (input, target) in enumerate(train_loader):\n\t        data_time.update(time.time() - end - val_time)\n\t        current_iter = epoch * len(train_loader) + i + 1\n\t        lr_decay(optimizer, args.base_lr, current_iter, max_iter, args.lr_decay, current_characters )\n\t        if current_iter % 50 == 0 and main_process():\n\t            print(current_characters[0]*3 +' '*5 + '{}_{}_{}_split{} Pretrain: {} GPU_id: {}'.format(args.arch,\\\n\t                             args.dataset ,args.backbone, args.split, args.pretrain, current_GPU) + ' '*5 + current_characters[1]*3)\n\t        input = input.cuda(non_blocking=True)\n\t        target = target.cuda(non_blocking=True)\n\t        optimizer.zero_grad()\n", "        output, main_loss, aux_loss_1, aux_loss_2= model(x=input, y=target)\n\t        loss = main_loss + aux_loss_1 * args.aux_weight1 +aux_loss_2 * args.aux_weight2\n\t        scaler.scale(loss).backward()\n\t        scaler.step(optimizer)\n\t        scaler.update()\n\t        n = input.size(0) # batch_size\n\t        intersection, union, target = intersectionAndUnionGPU(output, target, 2, args.ignore_label)\n\t        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()\n\t        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)\n\t        Iou = sum(intersection_meter.val[1:]) / (sum(union_meter.val[1:]) + 1e-10)  # allAcc\n", "        main_loss_meter.update(main_loss.item(), n)\n\t        if isinstance(aux_loss_1, torch.Tensor):\n\t            aux_loss_meter_1.update(aux_loss_1.item(), n)\n\t        if isinstance(aux_loss_2, torch.Tensor):\n\t            aux_loss_meter_2.update(aux_loss_2.item(), n)\n\t        loss_meter.update(loss.item(), n)\n\t        batch_time.update(time.time() - end - val_time)\n\t        end = time.time()\n\t        remain_iter = max_iter - current_iter\n\t        remain_time = remain_iter * batch_time.avg\n", "        t_m, t_s = divmod(remain_time, 60)\n\t        t_h, t_m = divmod(t_m, 60)\n\t        remain_time = '{:02d}:{:02d}:{:02d}'.format(int(t_h), int(t_m), int(t_s))\n\t        if (i + 1) % args.print_freq == 0 and main_process():\n\t            logger.info('Epoch: [{}/{}][{}/{}] '\n\t                        'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n\t                        'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n\t                        'Remain {remain_time} '\n\t                        'MainLoss {main_loss_meter.val:.4f} '\n\t                        'AuxLoss_1 {aux_loss_meter_1.val:.4f} '  \n", "                        'AuxLoss_2 {aux_loss_meter_2.val:.4f} ' \n\t                        'Loss {loss_meter.val:.4f} '\n\t                        'Iou {Iou:.4f}.'.format(epoch+1, args.epochs, i + 1, len(train_loader),\n\t                                                        batch_time=batch_time,\n\t                                                        data_time=data_time,\n\t                                                        remain_time=remain_time,\n\t                                                        main_loss_meter=main_loss_meter,\n\t                                                        aux_loss_meter_1=aux_loss_meter_1,\n\t                                                        aux_loss_meter_2=aux_loss_meter_2,\n\t                                                        loss_meter=loss_meter,\n", "                                                        Iou=Iou))\n\t        # -----------------------  SubEpoch VAL  -----------------------\n\t        if args.evaluate and args.SubEpoch_val and (args.epochs<=100 and (epoch + 1)%args.val_freq==0) and (i==round(len(train_loader)/2)): # max_epoch<=100时进行half_epoch Val\n\t            _,fbIou, _,_, mIoU,_ , recall, precision = validate(val_loader, model)   \n\t            model.train()\n\t            val_num += 1 \n\t            # save model for <testing> and <fine-tuning>\n\t            with open(all_name, 'a') as f:\n\t                f.write('[{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch, mIoU, fbIou, recall, precision))\n\t            if mIoU > best_miou:\n", "                best_miou, best_epoch = mIoU, (epoch-0.5)\n\t                keep_epoch = 0\n\t                with open(grow_name, 'a') as f:\n\t                    f.write('Best_epoch:{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch , best_miou, fbIou, recall, precision)) \n\t                logger.info('Saving checkpoint to: ' + best_name)\n\t                if osp.exists(best_name):\n\t                    os.remove(best_name) \n\t                torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name) \n\t            tmp_num += 1\n\t    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n", "    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n\t    mIoU = np.mean(iou_class)\n\t    mAcc = np.mean(accuracy_class)\n\t    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\t    logger.info('Train result at epoch [{}/{}]: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(epoch, args.epochs, mIoU, mAcc, allAcc))\n\t    for i in range(2):\n\t        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\t    return main_loss_meter.avg, mIoU, mAcc, allAcc\n\tdef validate(val_loader, model):\n\t    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')\n", "    batch_time = AverageMeter()\n\t    model_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    intersection_meter = AverageMeter()\n\t    union_meter = AverageMeter()\n\t    target_meter = AverageMeter()\n\t    split_gap = len(val_loader.dataset.list)\n\t    test_num = min(len(val_loader), 1000) # 20000 \n\t    class_intersection_meter = [0]*split_gap\n", "    class_union_meter = [0]*split_gap   \n\t    class_target_meter = [0]*split_gap \n\t    if args.manual_seed is not None and args.fix_random_seed_val:\n\t        setup_seed(args.manual_seed, args.seed_deterministic)\n\t    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\t    model.eval()\n\t    end = time.time()\n\t    val_start = end\n\t    iter_num = 0\n\t    total_time = 0\n", "    for e in range(10):\n\t        for i, (input, target) in enumerate(val_loader):\n\t            if iter_num * args.batch_size_val >= test_num:\n\t                break\n\t            iter_num += 1\n\t            data_time.update(time.time() - end)\n\t            input = input.cuda(non_blocking=True)\n\t            target = target.cuda(non_blocking=True)\n\t            start_time = time.time()\n\t            # with autocast():\n", "            with torch.no_grad():\n\t                output = model(x=input, y=target)\n\t            total_time = total_time + 1\n\t            model_time.update(time.time() - start_time)\n\t            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n\t            output = output.float()\n\t            loss = criterion(output, target)    \n\t            n = input.size(0)\n\t            loss = torch.mean(loss)\n\t            output = output.max(1)[1]\n", "            intersection, union, new_target = intersectionAndUnionGPU(output, target, split_gap+1, args.ignore_label)\n\t            intersection, union, new_target = intersection.cpu().numpy(), union.cpu().numpy(), new_target.cpu().numpy()\n\t            intersection_meter.update(intersection), union_meter.update(union), target_meter.update(new_target)\n\t            for idx in range(1,len(intersection)):\n\t                class_intersection_meter[idx-1] += intersection[idx]\n\t                class_union_meter[idx-1] += union[idx]\n\t                class_target_meter[idx-1] += new_target[idx]\n\t            Iou = np.mean(intersection_meter.val[1:] / (union_meter.val[1:] + 1e-10))\n\t            recall = np.mean(intersection_meter.val[1:] / (target_meter.val[1:]+ 1e-10))\n\t            precision = np.mean(intersection_meter.val[1:] /(union_meter.val[1:] - target_meter.val[1:] + intersection_meter.val[1:] + 1e-10) )\n", "            loss_meter.update(loss.item(), input.size(0))\n\t            batch_time.update(time.time() - end)\n\t            end = time.time()\n\t            if ((i + 1) % round((test_num/40)) == 0):\n\t                logger.info('Test: [{}/{}] '\n\t                            'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n\t                            'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n\t                            'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n\t                            'recall {recall:.4f} '\n\t                            'precision {precision:.4f} '\n", "                            'Iou {Iou:.4f}.'.format(iter_num* args.batch_size_val, test_num,\n\t                                                            data_time=data_time,\n\t                                                            batch_time=batch_time,\n\t                                                            loss_meter=loss_meter,\n\t                                                            recall=recall,\n\t                                                            precision=precision,\n\t                                                            Iou=Iou))\n\t    val_time = time.time()-val_start\n\t    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n\t    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n", "    mIoU = np.mean(iou_class)\n\t    mAcc = np.mean(accuracy_class)\n\t    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\t    class_iou_class = []\n\t    class_miou = 0\n\t    class_recall_class = []\n\t    class_mrecall = 0\n\t    class_precisoin_class = []\n\t    class_mprecision = 0\n\t    for i in range(len(class_intersection_meter)):\n", "        class_iou = class_intersection_meter[i]/(class_union_meter[i]+ 1e-10)\n\t        class_iou_class.append(class_iou)\n\t        class_miou += class_iou\n\t        class_recall = class_intersection_meter[i]/(class_target_meter[i]+ 1e-10)\n\t        class_recall_class.append(class_recall)\n\t        class_mrecall += class_recall\n\t        class_precision = class_intersection_meter[i]/(class_union_meter[i] - class_target_meter[i] + class_intersection_meter[i]+ 1e-10)\n\t        class_precisoin_class.append(class_precision)\n\t        class_mprecision += class_precision\n\t    class_mrecall = class_mrecall*1.0 / len(class_intersection_meter)  \n", "    class_miou = class_miou*1.0 / len(class_intersection_meter)\n\t    class_mprecision = class_mprecision*1.0 / len(class_intersection_meter)\n\t    logger.info('mean IoU---Val result: mIoU {:.4f}.'.format(class_miou))\n\t    logger.info('mean recall---Val result: mrecall {:.4f}.'.format(class_mrecall))\n\t    logger.info('mean precisoin---Val result: mprecisoin {:.4f}.'.format(class_mprecision))\n\t    for i in range(split_gap):\n\t        logger.info('Class_{}: \\t Result: iou {:.4f}. \\t recall {:.4f}. \\t precision {:.4f}. \\t {}'.format(i+1, class_iou_class[i], class_recall_class[i], class_precisoin_class[i],\\\n\t                         val_loader.dataset.class_id[val_loader.dataset.list[i]]))     \n\t    logger.info('FBIoU---Val result: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(mIoU, mAcc, allAcc))\n\t    for i in range(2):\n", "        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\t    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')\n\t    print('total time: {:.4f}, avg inference time: {:.4f}, count: {}'.format(val_time, model_time.avg, test_num))\n\t    return loss_meter.avg, mIoU, mAcc, allAcc, class_miou, iou_class[1], class_mrecall, class_mprecision\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "train.py", "chunked_list": ["import os\n\timport datetime\n\timport random\n\timport time\n\timport cv2\n\timport numpy as np\n\timport logging\n\timport argparse\n\tfrom visdom import Visdom\n\timport os.path as osp\n", "import torch\n\timport torch.backends.cudnn as cudnn\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.nn.parallel\n\timport torch.optim\n\timport torch.utils.data\n\timport torch.multiprocessing as mp\n\timport torch.distributed as dist\n\tfrom torch.cuda.amp import autocast as autocast\n", "from torch.cuda import amp\n\tfrom torch.utils.data.distributed import DistributedSampler\n\tfrom model.few_seg import R2Net\n\t# from model.workdir import\n\tfrom dataset import iSAID, iSAID_1\n\tfrom util import config\n\tfrom util.util import AverageMeter,  intersectionAndUnionGPU, get_model_para_number, setup_seed, get_logger, get_save_path, \\\n\t                                     fix_bn, check_makedirs,freeze_modules,lr_decay, Special_characters\n\tcv2.ocl.setUseOpenCL(False)\n\tcv2.setNumThreads(0)\n", "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\tdef get_parser():\n\t    parser = argparse.ArgumentParser(description='PyTorch Few-Shot Semantic Segmentation')\n\t    parser.add_argument('--arch', type=str, default='R2Net', help='') # \n\t    parser.add_argument('--shot', type=int, default=1, help='') # \n\t    parser.add_argument('--split', type=int, default=0, help='') # \n\t    parser.add_argument('--dataset', type=str, default='iSAID', help='') # \n\t    parser.add_argument('--backbone', type=str, default='vgg', help='') # \n\t    parser.add_argument('--variable1', type=str, default='', help='') #\n\t    parser.add_argument('--variable2', type=str, default='', help='') #\n", "    parser.add_argument('--local_rank', type=int, default=-1, help='number of cpu threads to use during batch generation')    \n\t    parser.add_argument('--opts', help='see config/ade20k/ade20k_pspnet50.yaml for all options', default=None, nargs=argparse.REMAINDER)\n\t    args = parser.parse_args()\n\t    base_config = 'config/base.yaml'\n\t    data_config = 'config/dataset/{}.yaml'.format(args.dataset)\n\t    if args.arch in ['R2Net']:\n\t        model_config = 'config/model/few_seg/{}.yaml'.format(args.arch)\n\t    else:\n\t        model_config = 'config/model/workdir/{}.yaml'.format(args.arch)\n\t    if os.path.exists(model_config):\n", "        cfg = config.load_cfg_from_cfg_file([base_config, data_config, model_config])\n\t    else:\n\t        cfg = config.load_cfg_from_cfg_file([base_config, data_config])\n\t    cfg = config.merge_cfg_from_args(cfg, args)\n\t    if args.opts is not None:\n\t        cfg = config.merge_cfg_from_list(cfg, args.opts)\n\t    return cfg\n\tdef get_model(args):\n\t    model = eval(args.arch).OneModel(args, cls_type='Base')\n\t    optimizer = model.get_optim(model, args.lr_decay, LR=args.base_lr)\n", "    freeze_modules(model, args.freeze_layer)\n\t    time.sleep(2)\n\t    if args.distributed:\n\t        # Initialize Process Group\n\t        device = torch.device('cuda', args.local_rank)\n\t        model.to(device)\n\t        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n\t        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n\t    else:\n\t        model = model.cuda()\n", "    # Resume\n\t    if args.resume:\n\t        resume_path = osp.join(args.snapshot_path, args.resume)\n\t        if os.path.isfile(resume_path):\n\t            if main_process():\n\t                logger.info(\"=> loading checkpoint '{}'\".format(resume_path))\n\t            checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n\t            args.start_epoch = checkpoint['epoch']\n\t            new_param = checkpoint['state_dict']\n\t            try: \n", "                model.load_state_dict(new_param)\n\t            except RuntimeError:                   # 1GPU loads mGPU model\n\t                for key in list(new_param.keys()):\n\t                    new_param[key[7:]] = new_param.pop(key)\n\t                model.load_state_dict(new_param)\n\t            optimizer.load_state_dict(checkpoint['optimizer'])\n\t            if main_process():\n\t                logger.info(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n\t        else:\n\t            if main_process():       \n", "                logger.info(\"=> no checkpoint found at '{}'\".format(resume_path))\n\t    # Get model para.\n\t    total_number, learnable_number = get_model_para_number(model)\n\t    if main_process():\n\t        print('Number of Parameters: %d' % (total_number))\n\t        print('Number of Learnable Parameters: %d' % (learnable_number))\n\t    time.sleep(5)\n\t    return model, optimizer\n\tdef main_process():\n\t    return not args.distributed or (args.distributed and (args.local_rank == 0))\n", "def main():\n\t    global args, logger\n\t    args = get_parser()\n\t    logger = get_logger()\n\t    args.logger = logger\n\t    args.distributed = False # Debug\n\t    # args.distributed = True if torch.cuda.device_count() > 1 else False\n\t    shuffle = False if args.distributed else True\n\t    get_save_path(args)\n\t    check_makedirs(args.snapshot_path)\n", "    check_makedirs(args.result_path)\n\t    if main_process():\n\t        print(args)\n\t    if args.manual_seed is not None:\n\t        setup_seed(args.manual_seed, args.seed_deterministic)\n\t    if args.distributed:\n\t        dist.init_process_group(backend='nccl')\n\t        print('args.local_rank: ', args.local_rank)\n\t        torch.cuda.set_device(args.local_rank)\n\t    if main_process():\n", "        logger.info(\"=> creating dataset ...\")\n\t# ----------------------  DATASET  ----------------------\n\t    train_data = eval('{}.{}_few_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n\t                                            shot=args.shot, mode='train', transform_dict=args.train_transform)\n\t    train_sampler = DistributedSampler(train_data) if args.distributed else None\n\t    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=shuffle, \\\n\t                                            num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n\t    # Val\n\t    val_data =  eval('{}.{}_few_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n\t                                            shot=args.shot, mode='val', transform_dict=args.val_transform)            \n", "    val_sampler = DistributedSampler(val_data) if args.distributed else None                  \n\t    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size_val, shuffle=False, \\\n\t                                            num_workers=args.workers, pin_memory=False, sampler=val_sampler)\n\t    logger.info('train_list: {}'.format(train_data.list))\n\t    logger.info('num_train_data: {}'.format(len(train_data)))\n\t    logger.info('val_list: {}'.format(val_data.list))\n\t    logger.info('num_val_data: {}'.format(len(val_data)))\n\t    args.base_class_num = len(train_data.list)\n\t    args.novel_class_num = len(val_data.list)\n\t    config_file = args.snapshot_path + 'config.yaml'\n", "    with open(config_file, 'w', encoding='utf-8') as f:\n\t        f.write(str(args))\n\t    time.sleep(2)\n\t    if main_process():\n\t        logger.info(\"=> creating model ...\")\n\t    model, optimizer = get_model(args)\n\t    logger.info(model)\n\t# ----------------------  TRAINVAL  ----------------------\n\t    global best_miou, best_FBiou, best_epoch, keep_epoch, val_num\n\t    global best_name, grow_name, all_name, latest_name, best_class_iou\n", "    best_miou = 0.\n\t    best_FBiou = 0.\n\t    best_class_iou = []\n\t    best_epoch = 0\n\t    keep_epoch = 0\n\t    val_num = 0\n\t    start_time = time.time()\n\t    scaler = amp.GradScaler()\n\t#--------------------------- FilenamePrepare -----------------------------\n\t    latest_name = args.snapshot_path + 'latest.pth'\n", "    best_name = args.snapshot_path + 'best.pth'\n\t    grow_name = args.snapshot_path + 'grow.txt'\n\t    all_name = args.snapshot_path + 'all.txt'\n\t    for epoch in range(args.start_epoch, args.epochs):\n\t        if keep_epoch == args.stop_interval:\n\t            break\n\t        if args.fix_random_seed_val:\n\t            setup_seed(args.manual_seed + epoch, args.seed_deterministic)\n\t        epoch_log = epoch + 1\n\t        keep_epoch += 1\n", "        # ----------------------  TRAIN  ----------------------\n\t        train(train_loader, val_loader, model, optimizer, epoch, scaler)\n\t        torch.cuda.empty_cache()\n\t        # save model for <resuming>\n\t        if ((epoch + 1) % args.save_freq == 0) and main_process():\n\t            logger.info('Saving checkpoint to: ' + latest_name)\n\t            if osp.exists(latest_name):\n\t                os.remove(latest_name)            \n\t            torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, latest_name)\n\t        # -----------------------  VAL  -----------------------\n", "        if args.evaluate and (epoch + 1)% args.val_freq == 0:\n\t            _,fbIou, _,_, mIoU,_ , recall, precision, class_miou = validate(val_loader, model)   \n\t            torch.cuda.empty_cache()\n\t            val_num += 1\n\t            with open(all_name, 'a') as f:\n\t                f.write('[{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch+1, mIoU, fbIou, recall, precision))\n\t        # save model for <testing> and <fine-tuning>\n\t            if mIoU > best_miou:\n\t                best_miou, best_epoch, best_class_iou, best_FBiou = mIoU, epoch, class_miou, fbIou\n\t                keep_epoch = 0\n", "                with open(grow_name, 'a') as f:\n\t                    f.write('Best_epoch:{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch+1 , best_miou, fbIou, recall, precision)) \n\t                logger.info('Saving checkpoint to: ' + best_name + '  miou: {:.4f}'.format(best_miou))\n\t                if osp.exists(best_name):\n\t                    os.remove(best_name)    \n\t                torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name)  \n\t    with open(args.snapshot_path + 'class.txt', 'a') as f:\n\t        for i in range(len(best_class_iou)):\n\t            f.write('{:.4f}\\t'.format(best_class_iou[i]))\n\t        f.write('\\nmiou: {:.4f}, fb_iou: {:.4f}'.format(best_miou, best_FBiou))\n", "    total_time = time.time() - start_time\n\t    t_m, t_s = divmod(total_time, 60)\n\t    t_h, t_m = divmod(t_m, 60)\n\t    total_time = '{:02d}h {:02d}m {:02d}s'.format(int(t_h), int(t_m), int(t_s))\n\t    print('\\nEpoch: {}/{} \\t Total running time: {}'.format(epoch_log, args.epochs, total_time))\n\t    print('The number of models validated: {}'.format(val_num))            \n\t    print('\\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Final Best Result   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n\t    print(args.arch + '\\t Group:{} \\t Best_mIoU:{:.4f} \\t Best_FBIoU:{:.4f} \\t Best_step:{}'.format(args.split, best_miou, best_FBiou, best_epoch + 1 ))\n\t    print('>'*80)\n\t    print ('当前的日期和时间是 %s' % datetime.datetime.now())\n", "def train(train_loader, val_loader, model, optimizer, epoch ,scaler):\n\t    global best_miou, best_epoch, keep_epoch, val_num\n\t    batch_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    main_loss_meter = AverageMeter()\n\t    aux_loss_meter_1 = AverageMeter()\n\t    aux_loss_meter_2 = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    intersection_meter = AverageMeter()\n\t    union_meter = AverageMeter()\n", "    target_meter = AverageMeter()\n\t    recall_meter = AverageMeter()\n\t    acc_meter = AverageMeter()\n\t    tmp_num = 0\n\t    model.train()\n\t    if args.fix_bn:\n\t        model.apply(fix_bn) # fix batchnorm\n\t    end = time.time()\n\t    val_time = 0.\n\t    max_iter = args.epochs * len(train_loader)\n", "    current_characters = Special_characters[random.randint(0,len(Special_characters)-1)]\n\t    current_GPU = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n\t    # GPU_name = torch.cuda.get_device_name()\n\t    for i, (input, target, s_input, s_mask, subcls) in enumerate(train_loader):\n\t        data_time.update(time.time() - end - val_time)\n\t        current_iter = epoch * len(train_loader) + i + 1\n\t        lr_decay(optimizer, args.base_lr, current_iter, max_iter, args.lr_decay, current_characters )\n\t        if current_iter % 50 == 0 and main_process():\n\t            print(current_characters[0]*3 +' '*5 + '{}_{}_{}_split{}_{}shot Pretrain: {} GPU_id: {}'.format(args.arch,\\\n\t                             args.dataset ,args.backbone, args.split, args.shot, args.pretrain, current_GPU) + ' '*5 + current_characters[1]*3)\n", "        s_input = s_input.cuda(non_blocking=True)\n\t        s_mask = s_mask.cuda(non_blocking=True)\n\t        input = input.cuda(non_blocking=True)\n\t        target = target.cuda(non_blocking=True)\n\t        optimizer.zero_grad()\n\t        # with torch.autograd.set_detect_anomaly(True):      # debug\n\t        output, main_loss, aux_loss_1, aux_loss_2= model(s_x=s_input, s_y=s_mask, x=input, y=target, cat_idx=subcls)\n\t        loss = main_loss + aux_loss_1 * args.aux_weight1 +aux_loss_2 * args.aux_weight2\n\t        scaler.scale(loss).backward()\n\t        scaler.step(optimizer)\n", "        scaler.update()\n\t        if 'para_limit' in args.keys():\n\t            for item_id in range(len(args.para_limit.name)):\n\t                item = args.para_limit.name[item_id]\n\t                tmp_limit = args.para_limit.limit[item_id]\n\t                eval('model.{}'.format(item)).data.clamp_(tmp_limit[0], tmp_limit[1])\n\t        n = input.size(0) # batch_size\n\t        intersection, union, target = intersectionAndUnionGPU(output, target, 2, args.ignore_label)\n\t        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()\n\t        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)\n", "        accuracy = sum(intersection_meter.val[1:]) / (sum(target_meter.val[1:]) + 1e-10)  # allAcc\n\t        main_loss_meter.update(main_loss.item(), n)\n\t        if isinstance(aux_loss_1, torch.Tensor):\n\t            aux_loss_meter_1.update(aux_loss_1.item(), n)\n\t        if isinstance(aux_loss_2, torch.Tensor):\n\t            aux_loss_meter_2.update(aux_loss_2.item(), n)\n\t        loss_meter.update(loss.item(), n)\n\t        batch_time.update(time.time() - end - val_time)\n\t        end = time.time()\n\t        remain_iter = max_iter - current_iter\n", "        remain_time = remain_iter * batch_time.avg\n\t        t_m, t_s = divmod(remain_time, 60)\n\t        t_h, t_m = divmod(t_m, 60)\n\t        remain_time = '{:02d}:{:02d}:{:02d}'.format(int(t_h), int(t_m), int(t_s))\n\t        if (i + 1) % args.print_freq == 0 and main_process():\n\t            logger.info('Epoch: [{}/{}][{}/{}] '\n\t                        'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n\t                        'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n\t                        'Remain {remain_time} '\n\t                        'MainLoss {main_loss_meter.val:.4f} '\n", "                        'AuxLoss_1 {aux_loss_meter_1.val:.4f} '  \n\t                        'AuxLoss_2 {aux_loss_meter_2.val:.4f} ' \n\t                        'Loss {loss_meter.val:.4f} '\n\t                        'Accuracy {accuracy:.4f}.'.format(epoch+1, args.epochs, i + 1, len(train_loader),\n\t                                                        batch_time=batch_time,\n\t                                                        data_time=data_time,\n\t                                                        remain_time=remain_time,\n\t                                                        main_loss_meter=main_loss_meter,\n\t                                                        aux_loss_meter_1=aux_loss_meter_1,\n\t                                                        aux_loss_meter_2=aux_loss_meter_2,\n", "                                                        loss_meter=loss_meter,\n\t                                                        accuracy=accuracy))\n\t        # -----------------------  SubEpoch VAL  -----------------------\n\t        if args.evaluate and args.SubEpoch_val and ((epoch + 1)%args.val_freq==0) and (i in torch.arange(1,args.sub_freq)*round(len(train_loader)/args.sub_freq)): # max_epoch<=100时进行half_epoch Val\n\t            _,fbIou, _,_, mIoU,_ , recall, precision, class_miou = validate(val_loader, model)    \n\t            torch.cuda.empty_cache()\n\t            model.train()\n\t            if args.fix_bn:\n\t                model.apply(fix_bn)\n\t            val_num += 1 \n", "            tmp_num += 1\n\t            # save model for <testing> and <fine-tuning>\n\t            with open(all_name, 'a') as f:\n\t                f.write('[{}_{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch, tmp_num, mIoU, fbIou, recall, precision))\n\t            if mIoU > best_miou:\n\t                best_miou, best_epoch, best_class_iou, best_FBiou = mIoU, epoch+(1/args.sub_freq)*tmp_num, class_miou, fbIou\n\t                keep_epoch = 0\n\t                with open(grow_name, 'a') as f:\n\t                    f.write('Best_epoch:{}_{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch, tmp_num, best_miou, fbIou, recall, precision)) \n\t                logger.info('Saving checkpoint to: ' + best_name + '  miou: {:.4f}'.format(best_miou))\n", "                if osp.exists(best_name):\n\t                    os.remove(best_name) \n\t                torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name) \n\t    if hasattr(model, 'out_data'):\n\t        with open(args.snapshot_path + 'out_data.txt', 'a') as f:\n\t            for item in model.out_data:\n\t                f.write(item + '\\n')\n\t            model.out_data = []\n\t    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n\t    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n", "    mIoU = np.mean(iou_class)\n\t    mAcc = np.mean(accuracy_class)\n\t    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\t    logger.info('Train result at epoch [{}/{}]: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(epoch+1, args.epochs, mIoU, mAcc, allAcc))\n\t    for i in range(2):\n\t        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\t    return main_loss_meter.avg, mIoU, mAcc, allAcc\n\tdef validate(val_loader, model):\n\t    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')\n\t    batch_time = AverageMeter()\n", "    model_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    intersection_meter = AverageMeter()\n\t    union_meter = AverageMeter()\n\t    target_meter = AverageMeter()\n\t    split_gap = len(val_loader.dataset.list)\n\t    test_num = 1000 # 20000 \n\t    class_intersection_meter = [0]*split_gap\n\t    class_union_meter = [0]*split_gap   \n", "    class_target_meter = [0]*split_gap   \n\t    if args.manual_seed is not None and args.fix_random_seed_val:\n\t        setup_seed(args.manual_seed, args.seed_deterministic)\n\t    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\t    model.eval()\n\t    end = time.time()\n\t    val_start = end\n\t    assert test_num % args.batch_size_val == 0\n\t    alpha = round(test_num / args.batch_size_val)\n\t    iter_num = 0\n", "    total_time = 0\n\t    for e in range(10):\n\t        for i, (input, target, s_input, s_mask, subcls, ori_label) in enumerate(val_loader):\n\t            if iter_num * args.batch_size_val >= test_num:\n\t                break\n\t            iter_num += 1\n\t            data_time.update(time.time() - end)\n\t            s_input = s_input.cuda(non_blocking=True)\n\t            s_mask = s_mask.cuda(non_blocking=True)\n\t            input = input.cuda(non_blocking=True)\n", "            target = target.cuda(non_blocking=True)\n\t            ori_label = ori_label.cuda(non_blocking=True)\n\t            start_time = time.time()\n\t            # with autocast():\n\t            with torch.no_grad():\n\t                output = model(s_x=s_input, s_y=s_mask, x=input, y=target, cat_idx=subcls)\n\t            total_time = total_time + 1\n\t            model_time.update(time.time() - start_time)\n\t            if args.ori_resize:\n\t                longerside = max(ori_label.size(1), ori_label.size(2))\n", "                backmask = torch.ones(ori_label.size(0), longerside, longerside, device='cuda')*255\n\t                backmask[:, :ori_label.size(1), :ori_label.size(2)] = ori_label\n\t                target = backmask.clone().long()\n\t            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n\t            output = output.float()\n\t            loss = criterion(output, target)    \n\t            n = input.size(0)\n\t            loss = torch.mean(loss)\n\t            output = output.max(1)[1]\n\t            # for b_id in range(output.size(0)):\n", "            intersection, union, new_target = intersectionAndUnionGPU(output, target, 2, args.ignore_label)\n\t            intersection, union, new_target = intersection.cpu().numpy(), union.cpu().numpy(), new_target.cpu().numpy()\n\t            intersection_meter.update(intersection), union_meter.update(union), target_meter.update(new_target)\n\t            tmp_id = subcls[0].cpu().numpy()[0]\n\t            class_intersection_meter[tmp_id] += intersection[1]\n\t            class_union_meter[tmp_id] += union[1] \n\t            class_target_meter[tmp_id] += new_target[1]\n\t            recall = np.mean(intersection_meter.val[1:] / (target_meter.val[1:]+ 1e-10))\n\t            precision = np.mean(intersection_meter.val[1:] /(union_meter.val[1:] - target_meter.val[1:] + intersection_meter.val[1:] + 1e-10) ) \n\t            Iou = sum(intersection_meter.val[1:]) / (sum(union_meter.val[1:]) + 1e-10)\n", "            loss_meter.update(loss.item(), input.size(0))\n\t            batch_time.update(time.time() - end)\n\t            end = time.time()\n\t            if ((iter_num) % round((alpha/20)) == 0):\n\t                logger.info('Test: [{}/{}] '\n\t                            'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n\t                            'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n\t                            'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n\t                            'recall {recall:.4f} '\n\t                            'precision {precision:.4f} '\n", "                            'Iou {Iou:.4f}.'.format(iter_num* args.batch_size_val, test_num,\n\t                                                            data_time=data_time,\n\t                                                            batch_time=batch_time,\n\t                                                            loss_meter=loss_meter,\n\t                                                            recall=recall,\n\t                                                            precision=precision,\n\t                                                            Iou=Iou))\n\t    val_time = time.time()-val_start\n\t    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n\t    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n", "    mIoU = np.mean(iou_class)\n\t    mAcc = np.mean(accuracy_class)\n\t    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\t    class_iou_class = []\n\t    class_miou = 0\n\t    class_recall_class = []\n\t    class_mrecall = 0\n\t    class_precisoin_class = []\n\t    class_mprecision = 0\n\t    for i in range(len(class_intersection_meter)):\n", "        class_iou = class_intersection_meter[i]/(class_union_meter[i]+ 1e-10)\n\t        class_iou_class.append(class_iou)\n\t        class_miou += class_iou\n\t        class_recall = class_intersection_meter[i]/(class_target_meter[i]+ 1e-10)\n\t        class_recall_class.append(class_recall)\n\t        class_mrecall += class_recall\n\t        class_precision = class_intersection_meter[i]/(class_union_meter[i] - class_target_meter[i] + class_intersection_meter[i]+ 1e-10)\n\t        class_precisoin_class.append(class_precision)\n\t        class_mprecision += class_precision\n\t    class_mrecall = class_mrecall*1.0 / len(class_intersection_meter)  \n", "    class_miou = class_miou*1.0 / len(class_intersection_meter)\n\t    class_mprecision = class_mprecision*1.0 / len(class_intersection_meter)\n\t    logger.info('mean IoU---Val result: mIoU {:.4f}.'.format(class_miou))\n\t    logger.info('mean recall---Val result: mrecall {:.4f}.'.format(class_mrecall))\n\t    logger.info('mean precisoin---Val result: mprecisoin {:.4f}.'.format(class_mprecision))\n\t    for i in range(split_gap):\n\t        logger.info('Class_{}: \\t Result: iou {:.4f}. \\t recall {:.4f}. \\t precision {:.4f}. \\t {}'.format(i+1, \\\n\t                        class_iou_class[i], class_recall_class[i], class_precisoin_class[i],\\\n\t                         val_loader.dataset.class_id[val_loader.dataset.list[i]]))     \n\t    logger.info('FBIoU---Val result: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(mIoU, mAcc, allAcc))\n", "    for i in range(2):\n\t        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\t    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')\n\t    print('total time: {:.4f}, avg inference time: {:.4f}, count: {}'.format(val_time, model_time.avg, test_num))\n\t    return loss_meter.avg, mIoU, mAcc, allAcc, class_miou, iou_class[1], class_mrecall, class_mprecision, class_iou_class\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "test.py", "chunked_list": ["import os\n\timport datetime\n\timport random\n\timport time\n\timport cv2\n\timport numpy as np\n\timport logging\n\timport argparse\n\tfrom visdom import Visdom\n\timport os.path as osp\n", "import torch\n\timport torch.backends.cudnn as cudnn\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.nn.parallel\n\timport torch.optim\n\timport torch.utils.data\n\timport torch.multiprocessing as mp\n\timport torch.distributed as dist\n\tfrom torch.cuda.amp import autocast as autocast\n", "from torch.cuda import amp\n\tfrom torch.utils.data.distributed import DistributedSampler\n\tfrom model.few_seg import R2Net\n\t# from model.workdir import\n\tfrom dataset import iSAID, iSAID_1\n\tfrom util import config\n\tfrom util.util import AverageMeter, poly_learning_rate, intersectionAndUnionGPU, get_model_para_number, setup_seed, get_logger, get_save_path, \\\n\t                                    is_same_model, fix_bn, sum_list, check_makedirs,freeze_modules, adjust_learning_rate_poly,lr_decay\n\tcv2.ocl.setUseOpenCL(False)\n\tcv2.setNumThreads(0)\n", "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\tdef get_parser():\n\t    parser = argparse.ArgumentParser(description='PyTorch Few-Shot Semantic Segmentation')\n\t    parser.add_argument('--arch', type=str, default='R2Net', help='') # \n\t    parser.add_argument('--shot', type=int, default=1, help='') # \n\t    parser.add_argument('--split', type=int, default=0, help='') # \n\t    parser.add_argument('--dataset', type=str, default='iSAID', help='') # \n\t    parser.add_argument('--backbone', type=str, default='vgg', help='') # \n\t    parser.add_argument('--s_q', default='False', help='') #\n\t    parser.add_argument('--cross_domain', default=None, help='') #\n", "    parser.add_argument('--variable1', type=str, default='', help='') #\n\t    parser.add_argument('--variable2', type=str, default='', help='') #\n\t    parser.add_argument('--local_rank', type=int, default=-1, help='number of cpu threads to use during batch generation')    \n\t    parser.add_argument('--opts', help='see config/ade20k/ade20k_pspnet50.yaml for all options', default=None, nargs=argparse.REMAINDER)\n\t    args = parser.parse_args()\n\t    base_config = 'config/base.yaml'\n\t    data_config = 'config/dataset/{}.yaml'.format(args.cross_domain)\n\t    if args.arch in ['R2Net']:\n\t        model_config = 'config/model/few_seg/{}.yaml'.format(args.arch)\n\t    else:\n", "        model_config = 'config/model/workdir/{}.yaml'.format(args.arch)\n\t    if os.path.exists(model_config):\n\t        cfg = config.load_cfg_from_cfg_file([base_config, data_config, model_config])\n\t    else:\n\t        cfg = config.load_cfg_from_cfg_file([base_config, data_config])\n\t    cfg = config.merge_cfg_from_args(cfg, args)\n\t    if args.opts is not None:\n\t        cfg = config.merge_cfg_from_list(cfg, args.opts)\n\t    return cfg\n\tdef get_model(args):\n", "    model = eval(args.arch).OneModel(args, cls_type='Base')\n\t    optimizer = model.get_optim(model, args.lr_decay, LR=args.base_lr)\n\t    model = model.cuda()\n\t    # Resume\n\t    get_save_path(args)\n\t    check_makedirs(args.snapshot_path)\n\t    check_makedirs(args.result_path)\n\t    weight_path = osp.join(args.snapshot_path, 'best.pth')\n\t    if os.path.isfile(weight_path):\n\t        logger.info(\"=> loading checkpoint '{}'\".format(weight_path))\n", "        checkpoint = torch.load(weight_path, map_location=torch.device('cpu'))\n\t        args.start_epoch = checkpoint['epoch']\n\t        new_param = checkpoint['state_dict']\n\t        try: \n\t            model.load_state_dict(new_param)\n\t        except RuntimeError:                   # 1GPU loads mGPU model\n\t            for key in list(new_param.keys()):\n\t                new_param[key[7:]] = new_param.pop(key)\n\t            model.load_state_dict(new_param)\n\t        optimizer.load_state_dict(checkpoint['optimizer'])\n", "        logger.info(\"=> loaded checkpoint '{}' (epoch {})\".format(weight_path, checkpoint['epoch']))\n\t    else:\n\t        logger.info(\"=> no checkpoint found at '{}'\".format(weight_path))\n\t    # Get model para.\n\t    total_number, learnable_number = get_model_para_number(model)\n\t    print('Number of Parameters: %d' % (total_number))\n\t    print('Number of Learnable Parameters: %d' % (learnable_number))\n\t    time.sleep(5)\n\t    return model, optimizer\n\tdef main_process():\n", "    return not args.distributed or (args.distributed and (args.local_rank == 0))\n\tdef main():\n\t    global args, logger\n\t    args = get_parser()\n\t    logger = get_logger()\n\t    args.logger = logger\n\t    args.distributed = True if torch.cuda.device_count() > 1 else False\n\t    if main_process():\n\t        print(args)\n\t    if args.manual_seed is not None:\n", "        setup_seed(args.manual_seed, args.seed_deterministic)\n\t    if main_process():\n\t        logger.info(\"=> creating dataset ...\")\n\t    train_data = eval('{}.{}_few_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n\t                                        shot=args.shot, mode='train', transform_dict=args.train_transform)\n\t    train_id = []\n\t    for i in range(len(train_data.list)):\n\t        train_id.append(train_data.class_id[train_data.list[i]])\n\t    val_data =  eval('{}.{}_few_dataset'.format(args.cross_domain, args.cross_domain))(split=args.split, \\\n\t                                        shot=args.shot, mode='val', transform_dict=args.val_transform)\n", "    val_data.list = []\n\t    for id in val_data.all_class:\n\t        if val_data.class_id[id] not in train_id:\n\t            val_data.list.append(id)\n\t    tmp = set() \n\t    for item in val_data.list:\n\t        tmp = tmp | set(val_data.sub_class_file_list[item])\n\t    val_data.data_list = list(tmp)\n\t    val_sampler = DistributedSampler(val_data) if args.distributed else None                  \n\t    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size_val, shuffle=False, \\\n", "                                            num_workers=args.workers, pin_memory=False, sampler=val_sampler)\n\t    args.base_class_num =len(train_data.list)\n\t    args.novel_class_num = len(val_data.list)\n\t    # if args.cross_dataset:\n\t    #     train_data = eval('{}.{}_few_dataset'.format(args.train_dataset, args.train_dataset)).class_id\n\t    logger.info('val_list: {}'.format(val_data.list))\n\t    logger.info('num_val_data: {}'.format(len(val_data)))\n\t    if main_process():\n\t        logger.info(\"=> creating model ...\")\n\t    model, _ = get_model(args)\n", "    logger.info(model)\n\t    val_manual_seed = args.manual_seed\n\t    if eval(args.s_q):\n\t        val_num = 2\n\t    else:\n\t        val_num = 5\n\t    setup_seed(val_manual_seed, False)\n\t    seed_array = np.random.randint(0,1000,val_num) \n\t    start_time = time.time()\n\t    FBIoU_array = np.zeros(val_num)\n", "    mIoU_array = np.zeros(val_num)\n\t    pIoU_array = np.zeros(val_num)\n\t    class_array = np.zeros([val_num, len(val_data.list)])\n\t    txt_root = 'exp/{}/test_result/'.format(args.arch)\n\t    check_makedirs(txt_root)\n\t    for val_id in range(val_num):\n\t        val_seed = seed_array[val_id]\n\t        print('Val: [{}/{}] \\t Seed: {}'.format(val_id+1, val_num, val_seed))\n\t        loss_val, mIoU_val, mAcc_val, allAcc_val, class_miou, pIoU, class_iou = validate(val_loader, model, val_seed) \n\t        FBIoU_array[val_id], mIoU_array[val_id], pIoU_array[val_id] = mIoU_val, class_miou, pIoU\n", "        for class_id in range(len(class_iou)):\n\t            class_array[val_id, class_id] = class_iou[class_id]\n\t    class_marray = np.mean(class_array, 0)\n\t    total_time = time.time() - start_time\n\t    t_m, t_s = divmod(total_time, 60)\n\t    t_h, t_m = divmod(t_m, 60)\n\t    total_time = '{:02d}h {:02d}m {:02d}s'.format(int(t_h), int(t_m), int(t_s))\n\t    print('\\nTotal running time: {}'.format(total_time))\n\t    print('Seed0: {}'.format(val_manual_seed))\n\t    print('mIoU:  {}'.format(np.round(mIoU_array, 4)))\n", "    print('FBIoU: {}'.format(np.round(FBIoU_array, 4)))\n\t    print('pIoU:  {}'.format(np.round(pIoU_array, 4)))\n\t    print('-'*43)\n\t    print('Best_Seed_m: {} \\t Best_Seed_F: {} \\t Best_Seed_p: {}'.format(seed_array[mIoU_array.argmax()], seed_array[FBIoU_array.argmax()], seed_array[pIoU_array.argmax()]))\n\t    print('Best_mIoU: {:.4f} \\t Best_FBIoU: {:.4f} \\t Best_pIoU: {:.4f}'.format(mIoU_array.max(), FBIoU_array.max(), pIoU_array.max()))\n\t    print('Mean_mIoU: {:.4f} \\t Mean_FBIoU: {:.4f} \\t Mean_pIoU: {:.4f}'.format(mIoU_array.mean(), FBIoU_array.mean(), pIoU_array.mean()))\n\t    with open(txt_root + '{}_{}.txt'.format(args.dataset, args.cross_domain), 'a') as f:\n\t        f.write('\\nsupp=query : {}  '.format(args.s_q)+ '\\n')\n\t        f.write('{} {} split{} {} shot'.format(args.arch, args.backbone, args.split, args.shot) + '\\n')\n\t        f.write('Seed0: {}\\n'.format(val_manual_seed))\n", "        f.write('Seed:  {}\\n'.format(seed_array))\n\t        f.write('mIoU:  {}\\n'.format(np.round(mIoU_array, 4)))\n\t        f.write('FBIoU: {}\\n'.format(np.round(FBIoU_array, 4)))\n\t        f.write('pIoU:  {}\\n'.format(np.round(pIoU_array, 4)))\n\t        f.write('Best_Seed_m: {} \\t Best_Seed_F: {} \\t Best_Seed_p: {} \\n'.format(seed_array[mIoU_array.argmax()], seed_array[FBIoU_array.argmax()], seed_array[pIoU_array.argmax()]))\n\t        f.write('Best_mIoU: {:.4f} \\t Best_FBIoU: {:.4f} \\t Best_pIoU: {:.4f} \\n'.format(mIoU_array.max(), FBIoU_array.max(), pIoU_array.max()))\n\t        f.write('Mean_mIoU: {:.4f} \\t Mean_FBIoU: {:.4f} \\t Mean_pIoU: {:.4f} \\n'.format(mIoU_array.mean(), FBIoU_array.mean(), pIoU_array.mean()))\n\t        for id in range(len(val_data.list)):\n\t            f.write('{}\\t'.format(val_data.list[id]))\n\t        f.write('\\n')\n", "        for id in range(len(val_data.list)):\n\t            f.write('{:.2f}\\t'.format(class_marray[id]*100))\n\t        f.write('\\n' + '-'*47 + '\\n')\n\t        f.write(str(datetime.datetime.now()) + '\\n')\n\tdef validate(val_loader, model, val_seed):\n\t    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')\n\t    batch_time = AverageMeter()\n\t    model_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n", "    intersection_meter = AverageMeter()\n\t    union_meter = AverageMeter()\n\t    target_meter = AverageMeter()\n\t    split_gap = len(val_loader.dataset.list)\n\t    if args.s_q:\n\t        test_num = min(len(val_loader)*2, 1000)\n\t    else:\n\t        test_num = 1000\n\t    class_intersection_meter = [0]*split_gap\n\t    class_union_meter = [0]*split_gap   \n", "    if args.manual_seed is not None and args.fix_random_seed_val:\n\t        setup_seed(args.manual_seed, args.seed_deterministic)\n\t    setup_seed(val_seed, args.seed_deterministic)\n\t    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\t    model.eval()\n\t    end = time.time()\n\t    val_start = end\n\t    alpha = round(test_num / args.batch_size_val)\n\t    iter_num = 0\n\t    total_time = 0\n", "    for e in range(10):\n\t        for i, (input, target, s_input, s_mask, subcls, ori_label) in enumerate(val_loader):\n\t            if iter_num * args.batch_size_val >= test_num:\n\t                break\n\t            iter_num += 1\n\t            data_time.update(time.time() - end)\n\t            s_input = s_input.cuda(non_blocking=True)\n\t            s_mask = s_mask.cuda(non_blocking=True)\n\t            input = input.cuda(non_blocking=True)\n\t            target = target.cuda(non_blocking=True)\n", "            ori_label = ori_label.cuda(non_blocking=True)\n\t            start_time = time.time()\n\t            # with autocast():\n\t            if eval(args.s_q):\n\t                assert (args.shot==1)\n\t                with torch.no_grad():\n\t                    output = model(s_x=input.unsqueeze(1), s_y=target.unsqueeze(1), x=input, y=target, cat_idx=subcls)\n\t            else:\n\t                with torch.no_grad():\n\t                    output = model(s_x=s_input, s_y=s_mask, x=input, y=target, cat_idx=subcls)\n", "            total_time = total_time + 1\n\t            model_time.update(time.time() - start_time)\n\t            if args.ori_resize:\n\t                longerside = max(ori_label.size(1), ori_label.size(2))\n\t                backmask = torch.ones(ori_label.size(0), longerside, longerside, device='cuda')*255\n\t                backmask[:, :ori_label.size(1), :ori_label.size(2)] = ori_label\n\t                target = backmask.clone().long()\n\t            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n\t            output = output.float()\n\t            loss = criterion(output, target)    \n", "            n = input.size(0)\n\t            loss = torch.mean(loss)\n\t            output = output.max(1)[1]\n\t            for b_id in range(output.size(0)):\n\t                intersection, union, new_target = intersectionAndUnionGPU(output[b_id,], target[b_id,], 2, args.ignore_label)\n\t                intersection, union, new_target = intersection.cpu().numpy(), union.cpu().numpy(), new_target.cpu().numpy()\n\t                intersection_meter.update(intersection), union_meter.update(union), target_meter.update(new_target)\n\t                tmp_id = subcls[0].cpu().numpy()[b_id]\n\t                class_intersection_meter[tmp_id] += intersection[1]\n\t                class_union_meter[tmp_id] += union[1] \n", "            accuracy = sum(intersection_meter.val[1:]) / (sum(target_meter.val[1:]) + 1e-10)\n\t            loss_meter.update(loss.item(), input.size(0))\n\t            batch_time.update(time.time() - end)\n\t            end = time.time()\n\t            if ((iter_num ) % round((alpha/20)) == 0):\n\t                logger.info('Test: [{}/{}] '\n\t                            'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n\t                            'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n\t                            'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n\t                            'Accuracy {accuracy:.4f}.'.format(iter_num* args.batch_size_val, test_num,\n", "                                                            data_time=data_time,\n\t                                                            batch_time=batch_time,\n\t                                                            loss_meter=loss_meter,\n\t                                                            accuracy=accuracy))\n\t    val_time = time.time()-val_start\n\t    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n\t    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n\t    mIoU = np.mean(iou_class)\n\t    mAcc = np.mean(accuracy_class)\n\t    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n", "    class_iou_class = []\n\t    class_miou = 0\n\t    for i in range(len(class_intersection_meter)):\n\t        class_iou = class_intersection_meter[i]/(class_union_meter[i]+ 1e-10)\n\t        class_iou_class.append(class_iou)\n\t        class_miou += class_iou\n\t    class_miou = class_miou*1.0 / len(class_intersection_meter)\n\t    logger.info('meanIoU---Val result: mIoU {:.4f}.'.format(class_miou))\n\t    for i in range(split_gap):\n\t        logger.info('Class_{}: \\t Result: iou {:.4f}. \\t {}'.format(i+1, class_iou_class[i],\\\n", "                         val_loader.dataset.class_id[val_loader.dataset.list[i]]))            \n\t    logger.info('FBIoU---Val result: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(mIoU, mAcc, allAcc))\n\t    for i in range(2):\n\t        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\t    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')\n\t    print('total time: {:.4f}, avg inference time: {:.4f}, count: {}'.format(val_time, model_time.avg, test_num))\n\t    return loss_meter.avg, mIoU, mAcc, allAcc, class_miou, iou_class[1], class_iou_class\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "lists/iSAID/gen_list.py", "chunked_list": ["import numpy as np\n\timport cv2\n\timport os\n\timport time\n\tfrom tqdm import tqdm\n\t# from dataset.base import classId2className\n\tclassId2className = {\n\t                    'coco': {\n\t                         0: 'background',\n\t                         1: 'person',\n", "                         2: 'bicycle',\n\t                         3: 'car',\n\t                         4: 'motorcycle',\n\t                         5: 'airplane',\n\t                         6: 'bus',\n\t                         7: 'train',\n\t                         8: 'truck',\n\t                         9: 'boat',\n\t                         10: 'traffic light',\n\t                         11: 'fire hydrant',\n", "                         12: 'stop sign',\n\t                         13: 'parking meter',\n\t                         14: 'bench',\n\t                         15: 'bird',\n\t                         16: 'cat',\n\t                         17: 'dog',\n\t                         18: 'horse',\n\t                         19: 'sheep',\n\t                         20: 'cow',\n\t                         21: 'elephant',\n", "                         22: 'bear',\n\t                         23: 'zebra',\n\t                         24: 'giraffe',\n\t                         25: 'backpack',\n\t                         26: 'umbrella',\n\t                         27: 'handbag',\n\t                         28: 'tie',\n\t                         29: 'suitcase',\n\t                         30: 'frisbee',\n\t                         31: 'skis',\n", "                         32: 'snowboard',\n\t                         33: 'sports ball',\n\t                         34: 'kite',\n\t                         35: 'baseball bat',\n\t                         36: 'baseball glove',\n\t                         37: 'skateboard',\n\t                         38: 'surfboard',\n\t                         39: 'tennis racket',\n\t                         40: 'bottle',\n\t                         41: 'wine glass',\n", "                         42: 'cup',\n\t                         43: 'fork',\n\t                         44: 'knife',\n\t                         45: 'spoon',\n\t                         46: 'bowl',\n\t                         47: 'banana',\n\t                         48: 'apple',\n\t                         49: 'sandwich',\n\t                         50: 'orange',\n\t                         51: 'broccoli',\n", "                         52: 'carrot',\n\t                         53: 'hot dog',\n\t                         54: 'pizza',\n\t                         55: 'donut',\n\t                         56: 'cake',\n\t                         57: 'chair',\n\t                         58: 'sofa',\n\t                         59: 'pottedplant',\n\t                         60: 'bed',\n\t                         61: 'diningtable',\n", "                         62: 'toilet',\n\t                         63: 'tv',\n\t                         64: 'laptop',\n\t                         65: 'mouse',\n\t                         66: 'remote',\n\t                         67: 'keyboard',\n\t                         68: 'cell phone',\n\t                         69: 'microwave',\n\t                         70: 'oven',\n\t                         71: 'toaster',\n", "                         72: 'sink',\n\t                         73: 'refrigerator',\n\t                         74: 'book',\n\t                         75: 'clock',\n\t                         76: 'vase',\n\t                         77: 'scissors',\n\t                         78: 'teddy bear',\n\t                         79: 'hair drier',\n\t                         80: 'toothbrush'},\n\t                    'pascal': {\n", "                        0: 'background',\n\t                        1: 'airplane',\n\t                        2: 'bicycle',\n\t                        3: 'bird',\n\t                        4: 'boat',\n\t                        5: 'bottle',\n\t                        6: 'bus',\n\t                        7: 'cat',\n\t                        8: 'car',\n\t                        9: 'chair',\n", "                        10: 'cow',\n\t                        11: 'diningtable',\n\t                        12: 'dog',\n\t                        13: 'horse',\n\t                        14: 'motorcycle',\n\t                        15: 'person',\n\t                        16: 'pottedplant',\n\t                        17: 'sheep',\n\t                        18: 'sofa',\n\t                        19: 'train',\n", "                        20: 'tv'\n\t                        },\n\t                    'iSAID':{\n\t                        0: 'unlabeled',\n\t                        1: 'ship',\n\t                        2: 'storage_tank',\n\t                        3: 'baseball_diamond',\n\t                        4: 'tennis_court',\n\t                        5: 'basketball_court',\n\t                        6: 'Ground_Track_Field',\n", "                        7: 'Bridge',\n\t                        8: 'Large_Vehicle',\n\t                        9: 'Small_Vehicle',\n\t                        10: 'Helicopter',\n\t                        11: 'Swimming_pool',\n\t                        12: 'Roundabout',\n\t                        13: 'Soccer_ball_field',\n\t                        14: 'plane',\n\t                        15: 'Harbor'\n\t                    },\n", "                    'few_shot':{\n\t                        0: 'Background',\n\t                        1: 'Foreground',\n\t                    }\n\t                     }\n\tdataset = 'iSAID'\n\tdata_root = '/disk2/lcb/datasets/{}/ann_dir/'.format(dataset)\n\tlist_dir = 'lists/{}/'.format(dataset)\n\tif not os.path.exists(list_dir):\n\t    os.makedirs(list_dir)\n", "class_num = len(classId2className[dataset]) -1\n\tdef gen_list(dataset, data_root, list_dir, class_num):\n\t    for mode in ['train', 'val']:\n\t        mutli_class_num = np.zeros(class_num)\n\t        pot_num = np.zeros(class_num)\n\t        pic_num= np.zeros(class_num)\n\t        tmp_root = data_root + mode\n\t        new_root = tmp_root #.replace('ann_dir', 'ann_dir_1')\n\t        file_list = os.listdir(new_root)\n\t        pair_list = []\n", "        for idx in tqdm(range(len(file_list))):\n\t            file = file_list[idx]\n\t            label_name = 'ann_dir/{}/'.format(mode) +  file\n\t            image_name = ('img_dir/{}/'.format(mode) + file.replace('_instance_color_RGB', ''))\n\t            label = cv2.imread(new_root + '/' + file, cv2.IMREAD_GRAYSCALE)\n\t            label_list = np.unique(label).tolist()\n\t            if 0 in label_list:\n\t                label_list.remove(0)\n\t            if 255 in label_list:\n\t                label_list.remove(255)\n", "            if len(label_list) != 0 :  #and len(label_list) <3\n\t                mutli_class_num[len(label_list)-1] += 1\n\t                for cls in label_list:\n\t                    if len(np.where(label == cls)[0]) > 2*32*32 :\n\t                        pair_list.append(image_name + ' ' + label_name)\n\t                        pic_num[cls-1] += 1\n\t                        pot_num[cls-1] += len(np.where(label == cls)[0])/1000000\n\t        pair_list = list(set(pair_list))\n\t        with open(list_dir + '{}_num.txt'.format(mode), 'a') as f:\n\t            f.write('-'*23 +'{}_{}'.format( dataset, mode) + '-'*23  + '\\n')\n", "            for i in range(class_num):\n\t                f.write('同时含有{}个类别的图像数目：{}'.format(i+1, mutli_class_num[i]) + '\\n')\n\t            f.write('图像总数：{}'.format(np.sum(mutli_class_num) )+ '\\n')\n\t            for i in range(class_num):\n\t                f.write('类别{} \\t 图像数目：{} \\t {} '.format(i+1,  pic_num[i], classId2className[dataset][i+1]) + '\\n')\n\t            f.write('图像数目类别比列为：{}'. format(np.round(pic_num/np.min(pic_num), 2))  + '\\n')\n\t            for i in range(class_num):\n\t                f.write('类别{} \\t 像素点数目：{:.1f} million \\t {}'.format(i+1, pot_num[i], classId2className[dataset][i+1]) + '\\n')\n\t            f.write('像素点类别比列为：{}'. format(np.round(pot_num/np.min(pot_num), 2))  + '\\n')\n\t        with open(list_dir + '{}.txt'.format(mode), 'a') as f:\n", "            for pair in pair_list:\n\t                f.write(pair + '\\n')\n\tdef change_label(data_root):\n\t    for mode in ['train', 'val']:\n\t        tmp_root = data_root + mode\n\t        new_root = tmp_root.replace('ann_dir', 'ann_dir_1')\n\t        if not os.path.exists(new_root):\n\t            os.makedirs(new_root)\n\t        file_list = os.listdir(tmp_root)\n\t        for idx in tqdm(range(len(file_list))):\n", "            file = file_list[idx]\n\t            label_name = new_root + '/' + file\n\t            label = cv2.imread(tmp_root + '/' + file, cv2.IMREAD_GRAYSCALE)\n\t            label[label==0] = 255\n\t            label[label==6] = 0\n\t            cv2.imwrite(label_name, label)\n\t# change_label(data_root)\n\tgen_list(dataset, data_root, list_dir, class_num)\n"]}
{"filename": "model/util/ASPP.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.utils.data\n\tclass ASPP(nn.Module):\n\t    def __init__(self, out_channels=256):\n\t        super(ASPP, self).__init__()\n\t        self.layer6_0 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n\t            nn.ReLU(),\n", "        )\n\t        self.layer6_1 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n\t            nn.ReLU(),\n\t            )\n\t        self.layer6_2 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=6,dilation=6, bias=True),\n\t            nn.ReLU(),\n\t            )\n\t        self.layer6_3 = nn.Sequential(\n", "            nn.Conv2d(out_channels , out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=True),\n\t            nn.ReLU(),\n\t            )\n\t        self.layer6_4 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=18, dilation=18, bias=True),\n\t            nn.ReLU(),\n\t            )\n\t        self._init_weight()\n\t    def _init_weight(self):\n\t        for m in self.modules():\n", "            if isinstance(m, nn.Conv2d):\n\t                torch.nn.init.kaiming_normal_(m.weight)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                m.weight.data.fill_(1)\n\t                m.bias.data.zero_()\n\t    def forward(self, x):\n\t        feature_size = x.shape[-2:]\n\t        global_feature = F.avg_pool2d(x, kernel_size=feature_size)\n\t        global_feature = self.layer6_0(global_feature)\n\t        global_feature = global_feature.expand(-1, -1, feature_size[0], feature_size[1])\n", "        out = torch.cat(\n\t            [global_feature, self.layer6_1(x), self.layer6_2(x), self.layer6_3(x), self.layer6_4(x)], dim=1)\n\t        return out\n\tclass ASPP_Drop(nn.Module):\n\t    def __init__(self, out_channels=256):\n\t        super(ASPP_Drop, self).__init__()\n\t        self.layer6_0 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n\t            nn.ReLU(),\n\t            nn.Dropout2d(p=0.5),\n", "        )\n\t        self.layer6_1 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n\t            nn.ReLU(),\n\t            nn.Dropout2d(p=0.5),\n\t            )\n\t        self.layer6_2 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=6,dilation=6, bias=True),\n\t            nn.ReLU(),\n\t            nn.Dropout2d(p=0.5)\n", "            )\n\t        self.layer6_3 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=True),\n\t            nn.ReLU(),\n\t            nn.Dropout2d(p=0.5)\n\t            )\n\t        self.layer6_4 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=18, dilation=18, bias=True),\n\t            nn.ReLU(),\n\t            nn.Dropout2d(p=0.5)\n", "            )\n\t        self._init_weight()\n\t    def _init_weight(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                torch.nn.init.kaiming_normal_(m.weight)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                m.weight.data.fill_(1)\n\t                m.bias.data.zero_()\n\t    def forward(self, x):\n", "        feature_size = x.shape[-2:]\n\t        global_feature = F.avg_pool2d(x, kernel_size=feature_size)\n\t        global_feature = self.layer6_0(global_feature)\n\t        global_feature = global_feature.expand(-1, -1, feature_size[0], feature_size[1])\n\t        out = torch.cat(\n\t            [global_feature, self.layer6_1(x), self.layer6_2(x), self.layer6_3(x), self.layer6_4(x)], dim=1)\n\t        return out\n\tclass ASPP_BN(nn.Module):\n\t    def __init__(self, out_channels=256):\n\t        super(ASPP_BN, self).__init__()\n", "        self.layer6_0 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU()            \n\t        )\n\t        self.layer6_1 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU()  \n\t            )\n", "        self.layer6_2 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=6,dilation=6, bias=True),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU()  \n\t            )\n\t        self.layer6_3 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=True),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU()  \n\t            )\n", "        self.layer6_4 = nn.Sequential(\n\t            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=18, dilation=18, bias=True),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU()  \n\t            )\n\t        self._init_weight()\n\t    def _init_weight(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                torch.nn.init.kaiming_normal_(m.weight)\n", "            elif isinstance(m, nn.BatchNorm2d):\n\t                m.weight.data.fill_(1)\n\t                m.bias.data.zero_()\n\t    def forward(self, x):\n\t        feature_size = x.shape[-2:]\n\t        global_feature = F.avg_pool2d(x, kernel_size=feature_size)\n\t        global_feature = self.layer6_0(global_feature)\n\t        global_feature = global_feature.expand(-1, -1, feature_size[0], feature_size[1])\n\t        out = torch.cat(\n\t            [global_feature, self.layer6_1(x), self.layer6_2(x), self.layer6_3(x), self.layer6_4(x)], dim=1)\n", "        return out"]}
{"filename": "model/util/cls.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom torch.nn import BatchNorm2d as BatchNorm        \n\tfrom torch.cuda.amp import autocast as autocast\n\tfrom model.backbone.layer_extrator import layer_extrator\n\tclass cls(nn.Module):\n\t    def __init__(self, backbone, fp16=True):\n\t        super(cls, self).__init__()\n\t        self.backbone = backbone\n", "        self.criterion = nn.CrossEntropyLoss()\n\t        self.pretrained = True\n\t        self.classes = 46\n\t        self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=self.backbone, pretrained=True)\n\t        self.fp16 = fp16\n\t        if self.backbone == 'resnet50' or self.backbone == 'resnet101':\n\t            self.avgpool = nn.AdaptiveAvgPool2d(1)\n\t            self.fc = nn.Linear(512 * 4, self.classes )\n\t        else:\n\t            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n", "            self.fc = nn.Sequential(\n\t            nn.Linear(512 * 7 * 7, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, self.classes),\n\t        )\n\t        self.GAP = nn.AdaptiveAvgPool2d(1)\n", "    def get_optim(self, model, lr_dict, LR):\n\t        optimizer = torch.optim.SGD(model.parameters(),\\\n\t            lr=LR, momentum=lr_dict['momentum'], weight_decay=lr_dict['weight_decay'])\n\t        return optimizer\n\t    def forward(self, x,  y):\n\t        with autocast(enabled=self.fp16):\n\t            x = self.layer0(x)\n\t            x = self.layer1(x)\n\t            x = self.layer2(x)\n\t            x = self.layer3(x)\n", "            x = self.layer4(x)\n\t            x = self.avgpool(x)\n\t            x = x.view(x.size(0), -1)\n\t            x = self.fc(x)\n\t            # x = F.log_softmax(x, 1,)\n\t            if self.training:\n\t                loss = self.criterion(x, y.long())\n\t                # loss_1 = F.nll_loss(x,y.long())\n\t                return x, loss\n\t            else:\n", "                return x\n\t    #   \n"]}
{"filename": "model/util/PSPNet.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tfrom torch._C import device\n\timport torch.nn.functional as F\n\tfrom torch.nn import BatchNorm2d as BatchNorm        \n\tfrom model.backbone.layer_extrator import layer_extrator\n\tfrom torch.cuda.amp import autocast\n\tclass PPM(nn.Module):\n\t    def __init__(self, in_dim, reduction_dim, bins):\n\t        super(PPM, self).__init__()\n", "        self.features = []\n\t        for bin in bins:\n\t            self.features.append(nn.Sequential(\n\t                nn.AdaptiveAvgPool2d(bin),\n\t                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n\t                nn.BatchNorm2d(reduction_dim),\n\t                nn.ReLU(inplace=True)\n\t            ))\n\t        self.features = nn.ModuleList(self.features)\n\t    def forward(self, x):\n", "        x_size = x.size()\n\t        out = [x]\n\t        for f in self.features:\n\t            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n\t        return torch.cat(out, 1)\n\tclass OneModel(nn.Module):\n\t    def __init__(self, args):\n\t        super(OneModel, self).__init__()\n\t        self.criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\t        self.classes = args.base_class_num +1\n", "        self.backbone = args.backbone\n\t        self.fp16 = args.fp16\n\t        if args.backbone in ['vgg', 'resnet50', 'resnet101']:\n\t            self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=args.backbone, pretrained = True)\n\t            self.encoder = nn.Sequential(self.layer0, self.layer1, self.layer2, self.layer3, self.layer4)\n\t            fea_dim = 512 if args.backbone == 'vgg' else 2048\n\t        # Base Learner\n\t        bins=(1, 2, 3, 6)\n\t        self.ppm = PPM(fea_dim, int(fea_dim/len(bins)), bins)\n\t        self.cls = nn.Sequential(\n", "            nn.Conv2d(fea_dim*2, 512, kernel_size=3, padding=1, bias=False),\n\t            nn.BatchNorm2d(512),\n\t            nn.ReLU(inplace=True),\n\t            nn.Dropout2d(p=0.1),\n\t            nn.Conv2d(512, self.classes, kernel_size=1))\n\t    def get_optim(self, model, args, LR):\n\t        optimizer = torch.optim.SGD(\n\t            [     \n\t            {'params': model.encoder.parameters(), 'lr' : LR},\n\t            {'params': model.ppm.parameters(), 'lr' : LR*10},\n", "            {'params': model.cls.parameters(), 'lr' : LR*10},\n\t            ], lr=LR, momentum=args.momentum, weight_decay=args.weight_decay)\n\t        return optimizer\n\t    def forward(self, x, y):\n\t        with autocast(enabled=self.fp16):\n\t            x_size = x.size()\n\t            h = x_size[2]\n\t            w = x_size[3]\n\t            x = self.encoder(x)\n\t            if self.backbone == 'swin':\n", "                x = self.ppm(x.permute(0, 3, 1, 2))\n\t            else:\n\t                x = self.ppm(x)\n\t            x = self.cls(x)\n\t            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n\t            if self.training:\n\t                main_loss = self.criterion(x, y.long())\n\t                return x.max(1)[1], main_loss, 0, 0\n\t            else:\n\t                return x"]}
{"filename": "model/few_seg/R2Net.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom torch.nn import BatchNorm2d as BatchNorm        \n\timport numpy as np\n\timport random\n\timport time\n\timport cv2\n\tfrom model.backbone.layer_extrator import layer_extrator\n\tfrom model.util.ASPP import ASPP, ASPP_Drop ,ASPP_BN\n", "from model.util.PSPNet import OneModel as PSPNet\n\tfrom torch.cuda.amp import autocast as autocast\n\tdef Cor_Map(query_feat, supp_feat_list, mask_list):\n\t    corr_query_mask_list = []\n\t    cosine_eps = 1e-7\n\t    for i, tmp_supp_feat in enumerate(supp_feat_list):\n\t        resize_size = tmp_supp_feat.size(2)\n\t        tmp_mask = F.interpolate(mask_list[i], size=(resize_size, resize_size), mode='bilinear', align_corners=True)\n\t        tmp_supp_feat_4 = tmp_supp_feat * tmp_mask\n\t        q = query_feat\n", "        s = tmp_supp_feat_4\n\t        bsize, ch_sz, sp_sz, _ = q.size()[:]\n\t        tmp_query = q\n\t        tmp_query = tmp_query.contiguous().view(bsize, ch_sz, -1)\n\t        tmp_query_norm = torch.norm(tmp_query, 2, 1, True)\n\t        tmp_supp = s               \n\t        tmp_supp = tmp_supp.contiguous().view(bsize, ch_sz, -1) \n\t        tmp_supp = tmp_supp.contiguous().permute(0, 2, 1)\n\t        tmp_supp_norm = torch.norm(tmp_supp, 2, 2, True) \n\t        similarity = torch.bmm(tmp_supp, tmp_query)/(torch.bmm(tmp_supp_norm, tmp_query_norm) + cosine_eps)   \n", "        similarity = similarity.max(1)[0].view(bsize, sp_sz*sp_sz)\n\t        similarity = (similarity - similarity.min(1)[0].unsqueeze(1))/(similarity.max(1)[0].unsqueeze(1) - similarity.min(1)[0].unsqueeze(1) + cosine_eps)\n\t        corr_query = similarity.view(bsize, 1, sp_sz, sp_sz)\n\t        corr_query_mask_list.append(corr_query)  \n\t    corr_query_mask = torch.cat(corr_query_mask_list, 1).mean(1).unsqueeze(1)\n\t    return corr_query_mask\n\tdef Weighted_GAP(supp_feat, mask):\n\t    supp_feat = supp_feat * mask\n\t    feat_h, feat_w = supp_feat.shape[-2:][0], supp_feat.shape[-2:][1]\n\t    area = F.avg_pool2d(mask, (supp_feat.size()[2], supp_feat.size()[3])) * feat_h * feat_w + 0.0005\n", "    supp_feat = F.avg_pool2d(input=supp_feat, kernel_size=supp_feat.shape[-2:]) * feat_h * feat_w / area\n\t    return supp_feat\n\tdef pro_select(query_feat, pro_list):\n\t    \"\"\"\n\t    query_feat  b*c*h*w\n\t    pro_list  [b*1*c*1*1]*n  \n\t    \"\"\"\n\t    query_base = query_feat.unsqueeze(1) # b*1*c*h*w\n\t    pro_gather = torch.cat(pro_list, 1)  # b*n*c*1*1\n\t    pro_gather = pro_gather.expand(-1,-1,-1,query_base.size(3), query_base.size(4)) # b*n*c*h*w\n", "    index_map = nn.CosineSimilarity(2)(pro_gather, query_base).unsqueeze(2) # b*n*1*h*w\n\t    index_pro = index_map.max(1)[1].unsqueeze(1).expand_as(query_base)  # b*1*c*h*w\n\t    out_pro = torch.gather(pro_gather, 1, index_pro).squeeze(1)\n\t    out_map = torch.sum(index_map, 1)\n\t    return out_pro, out_map\n\tclass feat_decode(nn.Module):\n\t    def __init__(self, inchannel):\n\t        super(feat_decode, self).__init__()\n\t        self.ASPP = ASPP()\n\t        self.res1 = nn.Sequential(\n", "            nn.Conv2d(inchannel*5, inchannel, kernel_size=1, padding=0, bias=False),\n\t            nn.ReLU(inplace=True),                          \n\t        )\n\t        self.res2 = nn.Sequential(\n\t            nn.Conv2d(inchannel, inchannel, kernel_size=3, padding=1, bias=False),\n\t            nn.ReLU(inplace=True),   \n\t            nn.Conv2d(inchannel, inchannel, kernel_size=3, padding=1, bias=False),\n\t            nn.ReLU(inplace=True),                             \n\t        )    \n\t        self.cls = nn.Sequential(\n", "            nn.Conv2d(inchannel, inchannel, kernel_size=3, padding=1, bias=False),\n\t            nn.ReLU(inplace=True),\n\t            nn.Dropout2d(p=0.1),                 \n\t            nn.Conv2d(inchannel, 1, kernel_size=1)\n\t        )\n\t    def forward(self,x):\n\t        x =self.ASPP(x)\n\t        x = self.res1(x)\n\t        x = self.res2(x) + x\n\t        x = self.cls(x)\n", "        return x\n\tclass OneModel(nn.Module):\n\t    def __init__(self, args, cls_type=None):\n\t        super(OneModel, self).__init__()\n\t        self.shot = args.shot\n\t        self.criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\t        self.pretrained = args.pretrain\n\t        self.classes = 2\n\t        self.fp16 = args.fp16\n\t        self.backbone = args.backbone\n", "        self.base_class_num = args.base_class_num\n\t        self.alpha = torch.nn.Parameter(torch.FloatTensor(self.base_class_num+1,1), requires_grad=True)\n\t        self.beta = torch.nn.Parameter(torch.FloatTensor(self.base_class_num+1,1), requires_grad=True)\n\t        nn.init.normal_(self.alpha, mean=1.0)\n\t        nn.init.normal_(self.beta)\n\t        self.pro_global = torch.FloatTensor(self.base_class_num+1, 256).cuda()\n\t        nn.init.normal_(self.pro_global)\n\t        # self.pro_global.data.fill_(0.0)\n\t        if self.pretrained:\n\t            BaseNet = PSPNet(args)\n", "            weight_path = 'initmodel/PSPNet/{}/split{}/{}/best.pth'.format(args.dataset, args.split, args.backbone)\n\t            new_param = torch.load(weight_path, map_location=torch.device('cpu'))['state_dict']\n\t            print('load <base> weights from: {}'.format(weight_path))\n\t            try: \n\t                BaseNet.load_state_dict(new_param)\n\t            except RuntimeError:                   # 1GPU loads mGPU model\n\t                for key in list(new_param.keys()):\n\t                    new_param[key[7:]] = new_param.pop(key)\n\t                BaseNet.load_state_dict(new_param)\n\t            self.layer0, self.layer1, self.layer2, \\\n", "                self.layer3, self.layer4 = BaseNet.layer0, BaseNet.layer1, BaseNet.layer2, BaseNet.layer3, BaseNet.layer4\n\t            self.base_layer = nn.Sequential(BaseNet.ppm, BaseNet.cls)\n\t        else:\n\t            self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=args.backbone, pretrained=True)\n\t        reduce_dim = 256\n\t        if self.backbone == 'vgg':\n\t            fea_dim = 512 + 256\n\t        else:\n\t            fea_dim = 1024 + 512       \n\t        self.down_query = nn.Sequential(\n", "            nn.Conv2d(fea_dim, reduce_dim, kernel_size=1, padding=0, bias=False),\n\t            nn.ReLU(inplace=True),\n\t            nn.Dropout2d(p=0.5)                  \n\t        )\n\t        self.down_supp = nn.Sequential(\n\t            nn.Conv2d(fea_dim, reduce_dim, kernel_size=1, padding=0, bias=False),\n\t            nn.ReLU(inplace=True),\n\t            nn.Dropout2d(p=0.5)                   \n\t        )  \n\t        self.init_merge = nn.Sequential(\n", "            nn.Conv2d(reduce_dim*2 +  1 , reduce_dim, kernel_size=1, padding=0, bias=False),\n\t            nn.ReLU(inplace=True))\n\t        self.init_merge_bg = nn.Sequential(\n\t            nn.Conv2d(reduce_dim*2 , reduce_dim, kernel_size=1, padding=0, bias=False),\n\t            nn.ReLU(inplace=True))\n\t        self.GAP = nn.AdaptiveAvgPool2d(1)\n\t        self.decode = feat_decode(inchannel= reduce_dim)\n\t        self.iter = 0\n\t    def get_optim(self, model, lr_dict, LR):\n\t        optimizer = torch.optim.SGD(\n", "            [\n\t            {'params': model.alpha},\n\t            {'params': model.beta},\n\t            {'params': model.pro_global},\n\t            {'params': model.down_query.parameters()},\n\t            {'params': model.down_supp.parameters()},\n\t            {'params': model.init_merge.parameters()},\n\t            {'params': model.init_merge_bg.parameters()},\n\t            {'params': model.decode.parameters()},\n\t            ],\n", "            lr=LR, momentum=lr_dict['momentum'], weight_decay=lr_dict['weight_decay'])\n\t        return optimizer\n\t    def forward(self, x, s_x, s_y, y, cat_idx=None):\n\t        with autocast(enabled=self.fp16):\n\t            x_size = x.size()\n\t            h = x_size[2]\n\t            w = x_size[3]\n\t            # Query Feature\n\t            with torch.no_grad():\n\t                query_feat_0 = self.layer0(x)\n", "                query_feat_1 = self.layer1(query_feat_0)\n\t                query_feat_2 = self.layer2(query_feat_1)\n\t                query_feat_3 = self.layer3(query_feat_2)\n\t                query_feat_4 = self.layer4(query_feat_3)\n\t                query_out = self.base_layer(query_feat_4)\n\t                query_out = nn.Softmax2d()(query_out)\n\t                if self.backbone == 'vgg':\n\t                    query_feat_2 = F.interpolate(query_feat_2, size=(query_feat_3.size(2),query_feat_3.size(3)), mode='bilinear', align_corners=True)\n\t            query_feat = torch.cat([query_feat_3, query_feat_2], 1)\n\t            query_feat = self.down_query(query_feat)\n", "            no_base_map = self.get_no_base_map(query_out, cat_idx)\n\t            # Support Feature     \n\t            final_supp_list = []\n\t            mask_list = []\n\t            act_fg_list = []\n\t            act_bg_list = []\n\t            feat_fg_list = []\n\t            feat_bg_list = []\n\t            aux_loss_1 = 0\n\t            aux_loss_2 = 0\n", "            for i in range(self.shot):\n\t                pro_fg_list = []\n\t                pro_bg_list = []\n\t                mask = (s_y[:,i,:,:] == 1).float().unsqueeze(1)\n\t                mask_list.append(mask)\n\t                with torch.no_grad():\n\t                    supp_feat_0 = self.layer0(s_x[:,i,:,:,:])\n\t                    supp_feat_1 = self.layer1(supp_feat_0)\n\t                    supp_feat_2 = self.layer2(supp_feat_1)\n\t                    supp_feat_3 = self.layer3(supp_feat_2)\n", "                    supp_feat_4_true = self.layer4(supp_feat_3)\n\t                    mask = F.interpolate(mask, size=(supp_feat_3.size(2), supp_feat_3.size(3)), mode='bilinear', align_corners=True)\n\t                    supp_feat_4 = self.layer4(supp_feat_3*mask)\n\t                    final_supp_list.append(supp_feat_4)\n\t                    if self.backbone == 'vgg':\n\t                        supp_feat_2 = F.interpolate(supp_feat_2, size=(supp_feat_3.size(2),supp_feat_3.size(3)), mode='bilinear', align_corners=True)\n\t                    supp_base_out = self.base_layer(supp_feat_4_true.clone())\n\t                    supp_base_out = nn.Softmax2d()(supp_base_out) # b*(c+1)*h*w\n\t                supp_feat = torch.cat([supp_feat_3, supp_feat_2], 1)\n\t                supp_feat_tmp = self.down_supp(supp_feat)\n", "                # gen pro\n\t                pro_fg = Weighted_GAP(supp_feat_tmp , mask)\n\t                pro_fg_list.append(pro_fg.unsqueeze(1) )\n\t                pro_bg = Weighted_GAP(supp_feat_tmp , 1-mask)\n\t                pro_bg_list.append(pro_bg.unsqueeze(1))\n\t                # gen pro by act_map\n\t                pro_list = []\n\t                for j in range(self.base_class_num +1 ):\n\t                    pro = Weighted_GAP(supp_feat_tmp , supp_base_out[:,j,].unsqueeze(1)).unsqueeze(1) # b*1*256*1*1\n\t                    pro_list.append(pro)\n", "                    if not self.training and j ==0 :\n\t                        pro_fg_list.append(pro)\n\t                local_pro = torch.cat(pro_list, 1)  #b*(c+1)*256*1*1\n\t                # global 2 local\n\t                cur_local_pro = ((self.alpha * self.pro_global).unsqueeze(0) + (self.beta).unsqueeze(0) * local_pro.squeeze(3).squeeze(3)).unsqueeze(3).unsqueeze(3) # b*(c+1)*256*1*1\n\t                # local 2 global\n\t                # with torch.no_grad():\n\t                # new_pro_global = self.pro_global * self.beta + torch.mean(local_pro, 0).squeeze(2).squeeze(2) * (1-self.beta)\n\t                # with torch.no_grad():\n\t                #     self.pro_global = new_pro_global.clone()\n", "                base_fg_list = []\n\t                base_bg_list = []\n\t                # select pro\n\t                for b_id in range(query_feat.size(0)):\n\t                    c_id_array = torch.arange(self.base_class_num+1, device='cuda')\n\t                    c_id = cat_idx[0][b_id] + 1\n\t                    c_mask = (c_id_array!=c_id)\n\t                    if self.training:\n\t                        base_fg_list.append(cur_local_pro[b_id, c_id,:,: ].unsqueeze(0))  # b*256*1*1\n\t                        base_bg_list.append(cur_local_pro[b_id,c_mask,:,:].unsqueeze(0)) # b*c*256*1*1\n", "                    else:\n\t                        base_bg_list.append(cur_local_pro[b_id,:,:,:].unsqueeze(0))  # b*(c+1)*1*1\n\t                if self.training:\n\t                    base_fg = torch.cat(base_fg_list, 0)  # b*1*256*1*1\n\t                    base_bg = torch.cat(base_bg_list, 0)  # b*c(c+1)*256*1*1\n\t                    pro_fg_list.append(base_fg.unsqueeze(1))\n\t                    pro_bg_list.append(base_bg)\n\t                    tmp_pro = torch.mean(cur_local_pro, 0 ).squeeze(2)\n\t                    # tmp_pro = self.pro_global.unsqueeze(2) # (c+1)*256*1\n\t                    crs = nn.CosineSimilarity(1)(tmp_pro, tmp_pro.transpose(0,2))  # (c+1)*(c+1)\n", "                    crs[crs==1] = 0\n\t                    crs_1 = 1-nn.CosineSimilarity(1)(torch.mean(local_pro, 0).squeeze(2), tmp_pro)\n\t                    gamma = (self.base_class_num+1) * self.base_class_num  \n\t                    aux_loss_1 +=  (torch.sum(crs) / gamma )\n\t                    aux_loss_2 += torch.mean(crs_1)\n\t                else:\n\t                    base_bg = torch.cat(base_bg_list, 0)  # b*c(c+1)*256*1*1\n\t                    pro_bg_list.append(base_bg)\n\t                fg_feat, fg_map = pro_select(query_feat, pro_fg_list)\n\t                bg_feat, bg_map = pro_select(query_feat, pro_bg_list)\n", "                feat_bg_list.append(bg_feat.unsqueeze(1))\n\t                feat_fg_list.append(fg_feat.unsqueeze(1))\n\t                act_bg_list.append(bg_map.unsqueeze(1))\n\t                act_fg_list.append(fg_map.unsqueeze(1))\n\t            if self.shot>1:\n\t                aux_loss_1 /= self.shot\n\t                aux_loss_2 /= self.shot\n\t                fg_dis = torch.cat(act_fg_list, 1)\n\t                bg_dis = torch.cat(act_bg_list, 1)  # b*k*1*h*w\n\t                fg_dis =F.softmax(fg_dis, 1)\n", "                bg_dis =F.softmax(bg_dis, 1)\n\t                fg_feat = torch.mean(torch.cat(feat_fg_list, 1)*fg_dis, 1)\n\t                bg_feat = torch.mean(torch.cat(feat_bg_list, 1)*bg_dis, 1)\n\t                # bg_map = torch.mean(torch.cat(act_bg_list, 1), 1)\n\t                # fg_map = torch.mean(torch.cat(act_fg_list, 1), 1)\n\t            corr_query_mask = Cor_Map(query_feat_4, final_supp_list, mask_list )\n\t            corr_query_mask = F.interpolate(corr_query_mask, size=(query_feat.size(2), query_feat.size(3)), mode='bilinear', align_corners=True)\n\t            query_feat_bin = query_feat\n\t            merge_feat_bin = torch.cat([query_feat_bin, fg_feat, corr_query_mask], 1)\n\t            merge_feat_bin = self.init_merge(merge_feat_bin)     \n", "            merge_feat_bg_bin = torch.cat([query_feat_bin, bg_feat], 1)\n\t            merge_feat_bg_bin = self.init_merge_bg(merge_feat_bg_bin)   \n\t            out_fg = self.decode(merge_feat_bin * no_base_map)\n\t            out_bg = self.decode(merge_feat_bg_bin)\n\t            output_fin = torch.cat([out_bg, out_fg], 1)\n\t            #   Output Part\n\t            output_fin = F.interpolate(output_fin, size=(h, w), mode='bilinear', align_corners=True)\n\t            if self.training:\n\t                act_map = nn.Softmax(1)(output_fin)\n\t                alpha = self.GAP(act_map[:,1].unsqueeze(1))\n", "                main_loss = self.criterion(output_fin, y.long()) \n\t                mask_y = (y==1).float().unsqueeze(1)\n\t                alpha_1 = self.GAP(mask_y)\n\t                beta = (alpha - alpha_1)**2\n\t                aux_loss = -(1-alpha)*torch.log(alpha) - beta * torch.log(1-beta)\n\t                return output_fin.max(1)[1], main_loss, torch.mean(aux_loss), aux_loss_1 + aux_loss_2\n\t            else:\n\t                return output_fin\n\t    def get_no_base_map(self, query_out, cat_idx):\n\t        map_list = []\n", "        if self.training:\n\t            for b_id in range(query_out.size(0)):\n\t                c_id = cat_idx[0][b_id] + 1\n\t                current_map = query_out[b_id,c_id,]\n\t                bg_map = query_out[b_id,0,]\n\t                map_list.append((bg_map + current_map).unsqueeze(0))\n\t            out_map = torch.cat(map_list, 0).unsqueeze(1)\n\t        else:\n\t            out_map = query_out[:,0,]\n\t        return out_map"]}
{"filename": "model/backbone/transform.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport numpy as np\n\timport torch.nn.functional as F\n\timport math, copy\n\tfrom torch.autograd import Variable\n\tclass MultiHeadedAttention(nn.Module):\n\t    def __init__(self, h, d_model, dropout=0.1):\n\t        \"Take in model size and number of heads.\"\n\t        super(MultiHeadedAttention, self).__init__()\n", "        assert d_model % h == 0\n\t        # We assume d_v always equals d_k\n\t        self.d_k = d_model // h\n\t        self.h = h\n\t        self.linears = clones(nn.Linear(d_model, d_model), 2)\n\t        self.attn = None\n\t        self.dropout = nn.Dropout(p=dropout)\n\t    def forward(self, query, key, value, mask=None):\n\t        if mask is not None:\n\t            # Same mask applied to all h heads.\n", "            mask = mask.unsqueeze(1)\n\t        nbatches = query.size(0)\n\t        # 1) Do all the linear projections in batch from d_model => h x d_k\n\t        query, key = \\\n\t            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n\t             for l, x in zip(self.linears, (query, key))]\n\t        value = value.repeat(self.h, 1, 1).transpose(0, 1).contiguous().unsqueeze(-1)\n\t        # 2) Apply attention on all the projected vectors in batch.\n\t        x, self.attn = attention(query, key, value, mask=mask,\n\t                                 dropout=self.dropout)\n", "        # 3) \"Concat\" using a view and apply a final linear.\n\t        return torch.mean(x, -3)\n\tclass PositionalEncoding(nn.Module):\n\t    \"Implement the PE function.\"\n\t    def __init__(self, d_model, dropout, max_len=10000):\n\t        super(PositionalEncoding, self).__init__()\n\t        self.dropout = nn.Dropout(p=dropout)\n\t        # Compute the positional encodings once in log space.\n\t        pe = torch.zeros(max_len, d_model)\n\t        position = torch.arange(0, max_len).unsqueeze(1)\n", "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n\t                             -(math.log(10000.0) / d_model))\n\t        pe[:, 0::2] = torch.sin(position * div_term)\n\t        pe[:, 1::2] = torch.cos(position * div_term)\n\t        pe = pe.unsqueeze(0)\n\t        self.register_buffer('pe', pe)\n\t    def forward(self, x):\n\t        x = x + Variable(self.pe[:, :x.size(1)],\n\t                         requires_grad=False)\n\t        return self.dropout(x)\n", "def attention(query, key, value, mask=None, dropout=None):\n\t    \"Compute 'Scaled Dot Product Attention'\"\n\t    d_k = query.size(-1)\n\t    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n\t             / math.sqrt(d_k)\n\t    if mask is not None:\n\t        scores = scores.masked_fill(mask == 0, -1e9)\n\t    p_attn = F.softmax(scores, dim=-1)\n\t    if dropout is not None:\n\t        p_attn = dropout(p_attn)\n", "    return torch.matmul(p_attn, value), p_attn\n\tdef clones(module, N):\n\t    \"Produce N identical layers.\"\n\t    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"]}
{"filename": "model/backbone/utils.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom itertools import repeat\n\timport collections.abc\n\timport math\n\timport warnings\n\tfrom torch.nn.init import _calculate_fan_in_and_fan_out\n\tdef _trunc_normal_(tensor, mean, std, a, b):\n\t    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n", "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n\t    def norm_cdf(x):\n\t        # Computes standard normal cumulative distribution function\n\t        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\t    if (mean < a - 2 * std) or (mean > b + 2 * std):\n\t        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n\t                      \"The distribution of values may be incorrect.\",\n\t                      stacklevel=2)\n\t    # Values are generated by using a truncated uniform distribution and\n\t    # then using the inverse CDF for the normal distribution.\n", "    # Get upper and lower cdf values\n\t    l = norm_cdf((a - mean) / std)\n\t    u = norm_cdf((b - mean) / std)\n\t    # Uniformly fill tensor with values from [l, u], then translate to\n\t    # [2l-1, 2u-1].\n\t    tensor.uniform_(2 * l - 1, 2 * u - 1)\n\t    # Use inverse cdf transform for normal distribution to get truncated\n\t    # standard normal\n\t    tensor.erfinv_()\n\t    # Transform to proper mean, std\n", "    tensor.mul_(std * math.sqrt(2.))\n\t    tensor.add_(mean)\n\t    # Clamp to ensure it's in the proper range\n\t    tensor.clamp_(min=a, max=b)\n\t    return tensor\n\tdef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # type: (Tensor, float, float, float, float) -> Tensor\n\t    r\"\"\"Fills the input Tensor with values drawn from a truncated\n\t    normal distribution. The values are effectively drawn from the\n\t    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n", "    with values outside :math:`[a, b]` redrawn until they are within\n\t    the bounds. The method used for generating the random values works\n\t    best when :math:`a \\leq \\text{mean} \\leq b`.\n\t    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n\t    applied while sampling the normal with mean/std applied, therefore a, b args\n\t    should be adjusted to match the range of mean, std args.\n\t    Args:\n\t        tensor: an n-dimensional `torch.Tensor`\n\t        mean: the mean of the normal distribution\n\t        std: the standard deviation of the normal distribution\n", "        a: the minimum cutoff value\n\t        b: the maximum cutoff value\n\t    Examples:\n\t        >>> w = torch.empty(3, 5)\n\t        >>> nn.init.trunc_normal_(w)\n\t    \"\"\"\n\t    with torch.no_grad():\n\t        return _trunc_normal_(tensor, mean, std, a, b)\n\tdef trunc_normal_tf_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # type: (Tensor, float, float, float, float) -> Tensor\n", "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n\t    normal distribution. The values are effectively drawn from the\n\t    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n\t    with values outside :math:`[a, b]` redrawn until they are within\n\t    the bounds. The method used for generating the random values works\n\t    best when :math:`a \\leq \\text{mean} \\leq b`.\n\t    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the\n\t    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0\n\t    and the result is subsquently scaled and shifted by the mean and std args.\n\t    Args:\n", "        tensor: an n-dimensional `torch.Tensor`\n\t        mean: the mean of the normal distribution\n\t        std: the standard deviation of the normal distribution\n\t        a: the minimum cutoff value\n\t        b: the maximum cutoff value\n\t    Examples:\n\t        >>> w = torch.empty(3, 5)\n\t        >>> nn.init.trunc_normal_(w)\n\t    \"\"\"\n\t    with torch.no_grad():\n", "        _trunc_normal_(tensor, 0, 1.0, a, b)\n\t        tensor.mul_(std).add_(mean)\n\t    return tensor\n\tdef variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):\n\t    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n\t    if mode == 'fan_in':\n\t        denom = fan_in\n\t    elif mode == 'fan_out':\n\t        denom = fan_out\n\t    elif mode == 'fan_avg':\n", "        denom = (fan_in + fan_out) / 2\n\t    variance = scale / denom\n\t    if distribution == \"truncated_normal\":\n\t        # constant is stddev of standard normal truncated to (-2, 2)\n\t        trunc_normal_tf_(tensor, std=math.sqrt(variance) / .87962566103423978)\n\t    elif distribution == \"normal\":\n\t        with torch.no_grad():\n\t            tensor.normal_(std=math.sqrt(variance))\n\t    elif distribution == \"uniform\":\n\t        bound = math.sqrt(3 * variance)\n", "        with torch.no_grad():\n\t            tensor.uniform_(-bound, bound)\n\t    else:\n\t        raise ValueError(f\"invalid distribution {distribution}\")\n\tdef lecun_normal_(tensor):\n\t    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')\n\t# From PyTorch internals\n\tdef _ntuple(n):\n\t    def parse(x):\n\t        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n", "            return tuple(x)\n\t        return tuple(repeat(x, n))\n\t    return parse\n\tto_1tuple = _ntuple(1)\n\tto_2tuple = _ntuple(2)\n\tto_3tuple = _ntuple(3)\n\tto_4tuple = _ntuple(4)\n\tto_ntuple = _ntuple\n\tdef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n", "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n\t    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n\t    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n\t    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n\t    'survival rate' as the argument.\n\t    \"\"\"\n\t    if drop_prob == 0. or not training:\n\t        return x\n\t    keep_prob = 1 - drop_prob\n\t    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n", "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n\t    if keep_prob > 0.0 and scale_by_keep:\n\t        random_tensor.div_(keep_prob)\n\t    return x * random_tensor\n\tclass DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\t    \"\"\"\n\t    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n", "        self.scale_by_keep = scale_by_keep\n\t    def forward(self, x):\n\t        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\t    def extra_repr(self):\n\t        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n"]}
{"filename": "model/backbone/vgg.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.utils.model_zoo as model_zoo\n\tBatchNorm = nn.BatchNorm2d\n\t__all__ = [\n\t    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n\t    'vgg19_bn', 'vgg19',\n\t]\n\tmodel_urls = {\n\t    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n", "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n\t    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n\t    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n\t    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n\t    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n\t    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n\t    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n\t}\n\tclass VGG(nn.Module):\n\t    def __init__(self, features, num_classes=1000, init_weights=True):\n", "        super(VGG, self).__init__()\n\t        self.features = features\n\t        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n\t        self.classifier = nn.Sequential(\n\t            nn.Linear(512 * 7 * 7, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n\t            nn.Linear(4096, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(),\n", "            nn.Linear(4096, num_classes),\n\t        )\n\t        if init_weights:\n\t            self._initialize_weights()\n\t    def forward(self, x):\n\t        x = self.features(x)\n\t        x = self.avgpool(x)\n\t        x = x.view(x.size(0), -1)\n\t        x = self.classifier(x)\n\t        return x\n", "    def _initialize_weights(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\t                if m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, BatchNorm):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n", "                nn.init.normal_(m.weight, 0, 0.01)\n\t                nn.init.constant_(m.bias, 0)\n\tdef make_layers(cfg, batch_norm=False):\n\t    layers = []\n\t    in_channels = 3\n\t    for v in cfg:\n\t        if v == 'M':\n\t            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n\t        else:\n\t            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n", "            if batch_norm:\n\t                layers += [conv2d, BatchNorm(v), nn.ReLU(inplace=True)]\n\t            else:\n\t                layers += [conv2d, nn.ReLU(inplace=True)]\n\t            in_channels = v\n\t    return nn.Sequential(*layers)\n\tcfg = {\n\t    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n\t    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n\t    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n", "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n\t}\n\tdef vgg11(pretrained=False, **kwargs):\n\t    \"\"\"VGG 11-layer model (configuration \"A\")\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    if pretrained:\n\t        kwargs['init_weights'] = False\n\t    model = VGG(make_layers(cfg['A']), **kwargs)\n", "    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n\t    return model\n\tdef vgg11_bn(pretrained=False, **kwargs):\n\t    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    if pretrained:\n\t        kwargs['init_weights'] = False\n", "    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n\t    return model\n\tdef vgg13(pretrained=False, **kwargs):\n\t    \"\"\"VGG 13-layer model (configuration \"B\")\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    if pretrained:\n", "        kwargs['init_weights'] = False\n\t    model = VGG(make_layers(cfg['B']), **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n\t    return model\n\tdef vgg13_bn(pretrained=False, **kwargs):\n\t    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n", "    if pretrained:\n\t        kwargs['init_weights'] = False\n\t    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\n\t    return model\n\tdef vgg16(pretrained=False, **kwargs):\n\t    \"\"\"VGG 16-layer model (configuration \"D\")\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n", "    \"\"\"\n\t    if pretrained:\n\t        kwargs['init_weights'] = False\n\t    model = VGG(make_layers(cfg['D']), **kwargs)\n\t    if pretrained:\n\t        #model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n\t        model_path = './initmodel/vgg16.pth'\n\t        model.load_state_dict(torch.load(model_path), strict=False) \n\t    return model\n\tdef vgg16_bn(pretrained=False, **kwargs):\n", "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    if pretrained:\n\t        kwargs['init_weights'] = False\n\t    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n\t    if pretrained:\n\t        #model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n\t        model_path = './initmodel/vgg16_bn.pth'\n", "        model.load_state_dict(torch.load(model_path), strict=False)        \n\t    return model\n\tdef vgg19(pretrained=False, **kwargs):\n\t    \"\"\"VGG 19-layer model (configuration \"E\")\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    if pretrained:\n\t        kwargs['init_weights'] = False\n\t    model = VGG(make_layers(cfg['E']), **kwargs)\n", "    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\n\t    return model\n\tdef vgg19_bn(pretrained=False, **kwargs):\n\t    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    if pretrained:\n\t        kwargs['init_weights'] = False\n", "    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\n\t    return model\n\tif __name__ =='__main__':\n\t    import os\n\t    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n\t    input = torch.rand(4, 3, 473, 473).cuda()\n\t    target = torch.rand(4, 473, 473).cuda()*1.0\n\t    model = vgg16_bn(pretrained=False).cuda()\n", "    model.train()\n\t    layer0_idx = range(0,6)\n\t    layer1_idx = range(6,13)\n\t    layer2_idx = range(13,23)\n\t    layer3_idx = range(23,33)\n\t    layer4_idx = range(34,43)\n\t    #layer4_idx = range(34,43)\n\t    print(model.features)\n\t    layers_0 = []\n\t    layers_1 = []\n", "    layers_2 = []\n\t    layers_3 = []\n\t    layers_4 = []\n\t    for idx in layer0_idx:\n\t        layers_0 += [model.features[idx]]\n\t    for idx in layer1_idx:\n\t        layers_1 += [model.features[idx]]\n\t    for idx in layer2_idx:\n\t        layers_2 += [model.features[idx]]\n\t    for idx in layer3_idx:\n", "        layers_3 += [model.features[idx]] \n\t    for idx in layer4_idx:\n\t        layers_4 += [model.features[idx]]         \n\t    layer0 = nn.Sequential(*layers_0) \n\t    layer1 = nn.Sequential(*layers_1) \n\t    layer2 = nn.Sequential(*layers_2) \n\t    layer3 = nn.Sequential(*layers_3) \n\t    layer4 = nn.Sequential(*layers_4) \n\t    output = layer0(input)\n\t    print(layer0)\n", "    print('layer 0: {}'.format(output.size()))\n\t    output = layer1(output)\n\t    print(layer1)\n\t    print('layer 1: {}'.format(output.size()))\n\t    output = layer2(output)\n\t    print(layer2)\n\t    print('layer 2: {}'.format(output.size()))\n\t    output = layer3(output)\n\t    print(layer3)\n\t    print('layer 3: {}'.format(output.size()))\n", "    output = layer4(output)\n\t    print(layer4)\n\t    print('layer 4: {}'.format(output.size()))\n"]}
{"filename": "model/backbone/resnet.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport math\n\timport torch.utils.model_zoo as model_zoo\n\tBatchNorm = nn.BatchNorm2d\n\t__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n\t           'resnet152']\n\tmodel_urls = {\n\t    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n\t    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n", "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n\t    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n\t    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n\t}\n\tdef conv3x3(in_planes, out_planes, stride=1):\n\t    \"\"\"3x3 convolution with padding\"\"\"\n\t    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n\t                     padding=1, bias=False)\n\tclass BasicBlock(nn.Module):\n\t    expansion = 1\n", "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n\t        super(BasicBlock, self).__init__()\n\t        self.conv1 = conv3x3(inplanes, planes, stride)\n\t        self.bn1 = BatchNorm(planes)\n\t        self.relu = nn.ReLU(inplace=True)\n\t        self.conv2 = conv3x3(planes, planes)\n\t        self.bn2 = BatchNorm(planes)\n\t        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x):\n", "        residual = x\n\t        out = self.conv1(x)\n\t        out = self.bn1(out)\n\t        out = self.relu(out)\n\t        out = self.conv2(out)\n\t        out = self.bn2(out)\n\t        if self.downsample is not None:\n\t            residual = self.downsample(x)\n\t        out += residual\n\t        out = self.relu(out)\n", "        return out\n\tclass Bottleneck(nn.Module):\n\t    expansion = 4\n\t    def __init__(self, inplanes, planes, stride=1, downsample=None):\n\t        super(Bottleneck, self).__init__()\n\t        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n\t        self.bn1 = BatchNorm(planes)\n\t        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n\t                            padding=1, bias=False)\n\t        self.bn2 = BatchNorm(planes)\n", "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n\t        self.bn3 = BatchNorm(planes * self.expansion)\n\t        self.relu = nn.ReLU(inplace=True)\n\t        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x):\n\t        residual = x\n\t        out = self.conv1(x)\n\t        out = self.bn1(out)\n\t        out = self.relu(out)\n", "        out = self.conv2(out)\n\t        out = self.bn2(out)\n\t        out = self.relu(out)\n\t        out = self.conv3(out)\n\t        out = self.bn3(out)\n\t        if self.downsample is not None:\n\t            residual = self.downsample(x)\n\t        out += residual\n\t        out = self.relu(out)\n\t        return out\n", "class ResNet(nn.Module):\n\t    def __init__(self, block, layers, num_classes=1000, deep_base=True):\n\t        super(ResNet, self).__init__()\n\t        self.deep_base = deep_base\n\t        if not self.deep_base:\n\t            self.inplanes = 64\n\t            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\t            self.bn1 = BatchNorm(64)\n\t            self.relu = nn.ReLU(inplace=True)\n\t        else:\n", "            self.inplanes = 128\n\t            self.conv1 = conv3x3(3, 64, stride=2)\n\t            self.bn1 = BatchNorm(64)\n\t            self.relu1 = nn.ReLU(inplace=True)\n\t            self.conv2 = conv3x3(64, 64)\n\t            self.bn2 = BatchNorm(64)\n\t            self.relu2 = nn.ReLU(inplace=True)\n\t            self.conv3 = conv3x3(64, 128)\n\t            self.bn3 = BatchNorm(128)\n\t            self.relu3 = nn.ReLU(inplace=True)\n", "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\t        self.layer1 = self._make_layer(block, 64, layers[0])\n\t        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n\t        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n\t        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\t        self.avgpool = nn.AvgPool2d(7, stride=1)\n\t        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n", "            elif isinstance(m, BatchNorm):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t    def _make_layer(self, block, planes, blocks, stride=1):\n\t        downsample = None\n\t        if stride != 1 or self.inplanes != planes * block.expansion:\n\t            downsample = nn.Sequential(\n\t                nn.Conv2d(self.inplanes, planes * block.expansion,\n\t                        kernel_size=1, stride=stride, bias=False),\n\t                BatchNorm(planes * block.expansion),\n", "            )\n\t        layers = []\n\t        layers.append(block(self.inplanes, planes, stride, downsample))\n\t        self.inplanes = planes * block.expansion\n\t        for i in range(1, blocks):\n\t            layers.append(block(self.inplanes, planes))\n\t        return nn.Sequential(*layers)\n\t    def forward(self, x):\n\t        x = self.relu1(self.bn1(self.conv1(x)))\n\t        if self.deep_base:\n", "            x = self.relu2(self.bn2(self.conv2(x)))\n\t            x = self.relu3(self.bn3(self.conv3(x)))\n\t        x = self.maxpool(x)\n\t        x = self.layer1(x)\n\t        x = self.layer2(x)\n\t        x = self.layer3(x)\n\t        x = self.layer4(x)\n\t        x = self.avgpool(x)\n\t        x = x.view(x.size(0), -1)\n\t        x = self.fc(x)\n", "        return x\n\tdef resnet18(pretrained=False, **kwargs):\n\t    \"\"\"Constructs a ResNet-18 model.\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n\t    return model\n", "def resnet34(pretrained=False, **kwargs):\n\t    \"\"\"Constructs a ResNet-34 model.\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n\t    if pretrained:\n\t        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n\t    return model\n\tdef resnet50(pretrained=True, **kwargs):\n", "    \"\"\"Constructs a ResNet-50 model.\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n\t    if pretrained:\n\t        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n\t        model_path = './initmodel/resnet50_v2.pth'\n\t        model.load_state_dict(torch.load(model_path), strict=False)\n\t    return model\n", "def resnet101(pretrained=False, **kwargs):\n\t    \"\"\"Constructs a ResNet-101 model.\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n\t    if pretrained:\n\t        # model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n\t        model_path = './initmodel/resnet101_v2.pth'\n\t        model.load_state_dict(torch.load(model_path), strict=False)\n", "    return model\n\tdef resnet152(pretrained=False, **kwargs):\n\t    \"\"\"Constructs a ResNet-152 model.\n\t    Args:\n\t        pretrained (bool): If True, returns a model pre-trained on ImageNet\n\t    \"\"\"\n\t    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n\t    if pretrained:\n\t        # model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n\t        model_path = './initmodel/resnet152_v2.pth'\n", "        model.load_state_dict(torch.load(model_path), strict=False)\n\t    return model\n"]}
{"filename": "model/backbone/layer_extrator.py", "chunked_list": ["from turtle import forward\n\timport torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom torch.nn import BatchNorm2d as BatchNorm        \n\timport numpy as np\n\timport random\n\timport time\n\timport cv2\n\timport model.backbone.resnet as models\n", "import model.backbone.vgg as vgg_models\n\tdef get_vgg16_layer(model):\n\t    layer0_idx = range(0,7)\n\t    layer1_idx = range(7,14)\n\t    layer2_idx = range(14,24)\n\t    layer3_idx = range(24,34)\n\t    layer4_idx = range(34,43)\n\t    layers_0 = []\n\t    layers_1 = []\n\t    layers_2 = []\n", "    layers_3 = []\n\t    layers_4 = []\n\t    for idx in layer0_idx:\n\t        layers_0 += [model.features[idx]]\n\t    for idx in layer1_idx:\n\t        layers_1 += [model.features[idx]]\n\t    for idx in layer2_idx:\n\t        layers_2 += [model.features[idx]]\n\t    for idx in layer3_idx:\n\t        layers_3 += [model.features[idx]]\n", "    for idx in layer4_idx:\n\t        layers_4 += [model.features[idx]]  \n\t    layer0 = nn.Sequential(*layers_0) \n\t    layer1 = nn.Sequential(*layers_1) \n\t    layer2 = nn.Sequential(*layers_2) \n\t    layer3 = nn.Sequential(*layers_3) \n\t    layer4 = nn.Sequential(*layers_4)\n\t    return layer0,layer1,layer2,layer3,layer4\n\tdef layer_extrator(backbone = None, pretrained = True,):\n\t    \"\"\"\n", "    \"\"\"\n\t    if backbone == 'vgg':\n\t        print('INFO: Using VGG_16 bn')\n\t        vgg_models.BatchNorm = BatchNorm\n\t        vgg16 = vgg_models.vgg16_bn(pretrained=pretrained)\n\t        print(vgg16)\n\t        layer0, layer1, layer2, layer3, layer4 = get_vgg16_layer(vgg16)\n\t    else:\n\t        print('INFO: Using {}'.format(backbone))\n\t        if backbone == 'resnet50':\n", "            resnet = models.resnet50(pretrained=pretrained)\n\t        elif backbone == 'resnet101':\n\t            resnet = models.resnet101(pretrained=pretrained)\n\t        else:\n\t            resnet = models.resnet152(pretrained=pretrained)\n\t        layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu1, resnet.conv2, resnet.bn2, resnet.relu2, resnet.conv3, resnet.bn3, resnet.relu3, resnet.maxpool)\n\t        layer1, layer2, layer3, layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n\t        for n, m in layer3.named_modules():\n\t            if 'conv2' in n:\n\t                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n", "            elif 'downsample.0' in n:\n\t                m.stride = (1, 1)\n\t        for n, m in layer4.named_modules():\n\t            if 'conv2' in n:\n\t                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n\t            elif 'downsample.0' in n:\n\t                m.stride = (1, 1)\n\t    return layer0, layer1, layer2, layer3, layer4"]}
{"filename": "dataset/base_data.py", "chunked_list": ["from lib2to3.pgen2.token import N_TOKENS\n\timport os\n\timport os.path as osp\n\timport cv2\n\timport numpy as np\n\timport copy\n\tfrom torch.utils.data import Dataset\n\timport torch.nn.functional as F\n\timport torch\n\timport random\n", "from PIL import Image\n\tfrom util.util import  make_dict, gen_list\n\tfrom util.get_transform import get_transform\n\tfrom util.get_weak_anns import transform_anns\n\tdef label_trans(label, target_cls, mode):\n\t    label_class = np.unique(label).tolist()\n\t    if 0 in label_class:\n\t        label_class.remove(0)\n\t    target_pix = np.where(label == target_cls)\n\t    new_label = np.zeros_like(label)\n", "    new_label[target_pix[0],target_pix[1]] = 1 \n\t    if mode == 'ignore':\n\t        for cls in label_class:\n\t            if cls != target_cls:\n\t                ignore_pix = np.where(label == cls)\n\t                new_label[ignore_pix[0],ignore_pix[1]] = 255\n\t    elif mode == 'To_zero':\n\t        ignore_pix = np.where(label == 255)\n\t        new_label[ignore_pix[0],ignore_pix[1]] = 255\n\t    return new_label\n", "class Few_Data(Dataset):\n\t    class_id = None\n\t    all_class = None\n\t    val_class = None\n\t    data_root = None\n\t    val_list =None\n\t    train_list =None\n\t    def __init__(self, split=0, shot=1, dataset=None, mode='train', ann_type='mask', transform_dict=None, ori_resize=False):\n\t        assert mode in ['train', 'val', 'demo']\n\t        self.mode = mode\n", "        self.shot = shot\n\t        self.ann_type = ann_type\n\t        self.sample_mode = transform_dict.pop('sample_mode')\n\t        self.fliter_mode = transform_dict.pop('fliter_mode')\n\t        if self.mode == 'train':\n\t            self.list = list(set(self.all_class) - set(self.val_class[split]))\n\t        else:\n\t            self.list = self.val_class[split]\n\t        if self.mode == 'demo':\n\t            dict_name = './lists/{}/train_dict.txt'.format(dataset)\n", "            self.sample_mode = 'class'\n\t        else:\n\t            dict_name = './lists/{}/{}_dict.txt'.format(dataset, mode)\n\t        if not os.path.exists(dict_name):\n\t            make_dict(data_root=self.data_root, data_list=eval('self.{}_list'.format(self.mode)), \\\n\t                        all_class= self.all_class, dataset=dataset, mode=self.mode)\n\t        self.data_list, self.sub_class_file_list = gen_list(dict_name, self.list, fliter=self.fliter_mode)\n\t        self.transform_dict = transform_dict\n\t        self.AUG = get_transform(transform_dict)\n\t    def transform(self, image, label):\n", "        if self.transform_dict['type'] == 'albumentations':\n\t            aug = self.AUG(image=image, mask=label)\n\t            return aug['image'], aug['mask']\n\t        else:\n\t            image, label = self.AUG(image=image, label=label)\n\t            return image, label\n\t    def __len__(self):\n\t        return len(self.data_list)\n\t    def __getitem__(self, index):\n\t        label_class = []\n", "        if self.sample_mode == 'rand':\n\t            image_path, label_path = self.data_list[index]\n\t        elif self.sample_mode == 'class':\n\t            tmp_class = self.list[random.randint(1, len(self.list) ) -1]\n\t            file_all = list(set(self.sub_class_file_list[tmp_class]) & set(self.data_list))\n\t            image_path, label_path = file_all[random.randint(1,len(file_all))-1]\n\t        image = cv2.imread(image_path, cv2.IMREAD_COLOR) \n\t        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n\t        image = np.float32(image)\n\t        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n", "        label_class = np.unique(label).tolist()\n\t        if 0 in label_class:\n\t            label_class.remove(0)\n\t        if 255 in label_class:\n\t            label_class.remove(255) \n\t        new_label_class = []       \n\t        for c in label_class:\n\t            if c in self.list:\n\t                new_label_class.append(c)\n\t        label_class = new_label_class    \n", "        assert len(label_class) > 0\n\t        # supp_query dataset\n\t        class_chosen = label_class[random.randint(1,len(label_class))-1]\n\t        label = label_trans(label, class_chosen, mode='To_zero')\n\t        file_class_chosen = self.sub_class_file_list[class_chosen]\n\t        num_file = len(file_class_chosen)\n\t        if self.mode == 'demo':\n\t            s_x_list = []\n\t            s_y_list = []\n\t            s_ori_x_list = []\n", "            s_ori_y_list = []\n\t            raw_image = image.copy()\n\t            raw_label = label.copy()\n\t            if self.transform is not None:\n\t                image, label = self.transform(image, label)\n\t            for i in range(10):\n\t                support_image_path_list = []\n\t                support_label_path_list = []\n\t                support_idx_list = []\n\t                for k in range(self.shot):\n", "                    support_idx = random.randint(1,num_file)-1\n\t                    support_image_path = image_path\n\t                    support_label_path = label_path\n\t                    while((support_image_path == image_path and support_label_path == label_path) or support_idx in support_idx_list):\n\t                        support_idx = random.randint(1,num_file)-1\n\t                        support_image_path, support_label_path = file_class_chosen[support_idx]                \n\t                    support_idx_list.append(support_idx)\n\t                    support_image_path_list.append(support_image_path)\n\t                    support_label_path_list.append(support_label_path)\n\t                support_image_list_ori = []\n", "                support_label_list_ori = [] \n\t                support_label_list_ori_mask = []\n\t                subcls_list = []\n\t                for k in range(self.shot):\n\t                    subcls_list.append(self.list.index(class_chosen))\n\t                    support_image_path = support_image_path_list[k]\n\t                    support_label_path = support_label_path_list[k] \n\t                    support_image = cv2.imread(support_image_path, cv2.IMREAD_COLOR)      \n\t                    support_image = cv2.cvtColor(support_image, cv2.COLOR_BGR2RGB)\n\t                    support_image = np.float32(support_image)\n", "                    support_label = cv2.imread(support_label_path, cv2.IMREAD_GRAYSCALE)\n\t                    target_pix = np.where(support_label == class_chosen)\n\t                    ignore_pix = np.where(support_label == 255)\n\t                    support_label[:,:] = 0\n\t                    support_label[target_pix[0],target_pix[1]] = 1 \n\t                    support_label, support_label_mask = transform_anns(support_label, self.ann_type)\n\t                    support_label[ignore_pix[0],ignore_pix[1]] = 255\n\t                    support_label_mask[ignore_pix[0],ignore_pix[1]] = 255\n\t                    support_image_list_ori.append(support_image)\n\t                    support_label_list_ori.append(support_label)\n", "                    support_label_list_ori_mask.append(support_label_mask)\n\t                assert len(support_label_list_ori) == self.shot and len(support_image_list_ori) == self.shot    \n\t                support_image_list = [[] for _ in range(self.shot)]\n\t                support_label_list = [[] for _ in range(self.shot)]\n\t                if self.transform is not None:\n\t                    for k in range(self.shot):\n\t                        support_image_list[k], support_label_list[k] = self.transform(support_image_list_ori[k], support_label_list_ori[k])\n\t                s_xs = support_image_list\n\t                s_ys = support_label_list\n\t                s_x = s_xs[0].unsqueeze(0)\n", "                for i in range(1, self.shot):\n\t                    s_x = torch.cat([s_xs[i].unsqueeze(0), s_x], 0)\n\t                s_y = s_ys[0].unsqueeze(0)\n\t                for i in range(1, self.shot):\n\t                    s_y = torch.cat([s_ys[i].unsqueeze(0), s_y], 0)\n\t                s_x_list.append(s_x)\n\t                s_y_list.append(s_y)\n\t                s_ori_x_list.append(support_image_list_ori)\n\t                s_ori_y_list.append(support_label_list_ori)\n\t        else:\n", "            support_image_path_list = []\n\t            support_label_path_list = []\n\t            support_idx_list = []\n\t            for k in range(self.shot):\n\t                support_idx = random.randint(1,num_file)-1\n\t                support_image_path = image_path\n\t                support_label_path = label_path\n\t                while((support_image_path == image_path and support_label_path == label_path) or support_idx in support_idx_list):  # 保证与query不同\n\t                    support_idx = random.randint(1,num_file)-1\n\t                    support_image_path, support_label_path = file_class_chosen[support_idx]                \n", "                support_idx_list.append(support_idx)\n\t                support_image_path_list.append(support_image_path)\n\t                support_label_path_list.append(support_label_path)\n\t            support_image_list_ori = []\n\t            support_label_list_ori = []\n\t            support_label_list_ori_mask = []\n\t            subcls_list = []\n\t            for k in range(self.shot):\n\t                subcls_list.append(self.list.index(class_chosen))\n\t                support_image_path = support_image_path_list[k]\n", "                support_label_path = support_label_path_list[k] \n\t                support_image = cv2.imread(support_image_path, cv2.IMREAD_COLOR)      \n\t                support_image = cv2.cvtColor(support_image, cv2.COLOR_BGR2RGB)\n\t                support_image = np.float32(support_image)\n\t                support_label = cv2.imread(support_label_path, cv2.IMREAD_GRAYSCALE)\n\t                support_label = label_trans(support_label, class_chosen, mode='To_zero')\n\t                support_label, support_label_mask = transform_anns(support_label, self.ann_type)\n\t                support_label_mask = label_trans(support_label_mask, class_chosen, mode='To_zero')\n\t                if support_image.shape[0] != support_label.shape[0] or support_image.shape[1] != support_label.shape[1]:\n\t                    raise (RuntimeError(\"Support Image & label shape mismatch: \" + support_image_path + \" \" + support_label_path + \"\\n\"))            \n", "                support_image_list_ori.append(support_image)\n\t                support_label_list_ori.append(support_label)\n\t                support_label_list_ori_mask.append(support_label_mask)\n\t            assert len(support_label_list_ori) == self.shot and len(support_image_list_ori) == self.shot    \n\t            raw_image = image.copy()\n\t            raw_label = label.copy()\n\t            support_image_list = [[] for _ in range(self.shot)]\n\t            support_label_list = [[] for _ in range(self.shot)]\n\t            image, label = self.transform(image, label)\n\t            for k in range(self.shot):\n", "                support_image_list[k], support_label_list[k] = self.transform(support_image_list_ori[k], support_label_list_ori[k])\n\t            s_xs = support_image_list\n\t            s_ys = support_label_list\n\t            s_x = s_xs[0].unsqueeze(0)\n\t            for i in range(1, self.shot):\n\t                s_x = torch.cat([s_xs[i].unsqueeze(0), s_x], 0)\n\t            s_y = s_ys[0].unsqueeze(0)\n\t            for i in range(1, self.shot):\n\t                s_y = torch.cat([s_ys[i].unsqueeze(0), s_y], 0)\n\t            total_image_list = support_image_list_ori.copy()\n", "            total_image_list.append(raw_image)\n\t        # Return\n\t        if self.mode == 'train':\n\t            return image, label, s_x, s_y, subcls_list\n\t        elif self.mode == 'val':\n\t            return image, label, s_x, s_y, subcls_list, raw_label\n\t        elif self.mode == 'demo':\n\t            return image, label, s_x_list, s_y_list, subcls_list, s_ori_x_list, s_ori_y_list, raw_image, raw_label\n\tclass Base_Data(Dataset):\n\t    class_id = None\n", "    all_class = None\n\t    val_class = None\n\t    data_root = None\n\t    val_list =None\n\t    train_list =None\n\t    def __init__(self, split=0,  data_root=None, dataset=None, mode='train', transform_dict=None):\n\t        assert mode in ['train', 'val']\n\t        self.mode = mode\n\t        self.transform_dict = transform_dict\n\t        self.AUG = get_transform(transform_dict)\n", "        if split == -1 :\n\t            self.list = self.all_class\n\t        else:\n\t            self.list = list(set(self.all_class) - set(self.val_class[split]))\n\t        dict_name = './lists/{}/{}_dict.txt'.format(dataset, mode)\n\t        if not os.path.exists(dict_name):\n\t            make_dict(data_root=self.data_root, data_list=eval('self.{}_list'.format(self.mode)), \\\n\t                        all_class= self.all_class, dataset=dataset, mode=self.mode)\n\t        self.data_list, _ = gen_list(dict_name, self.list, fliter=False)\n\t        # if split == -1 :\n", "        #     self.list = self.all_class\n\t        #     list_path = './lists/{}/{}.txt'.format(dataset, self.mode)\n\t        #     with open(list_path, 'r') as f:\n\t        #         f_str = f.readlines()\n\t        #     self.data_list = []\n\t        #     for line in f_str:\n\t        #         img, mask = line.split(' ')\n\t        #         img = '../data/{}/'.format(dataset) + img\n\t        #         mask = '../data/{}/'.format(dataset) + mask\n\t        #         self.data_list.append((img, mask.strip()))\n", "        # else:\n\t        #     self.list = list(set(self.all_class) - set(self.val_class[split]))\n\t        #     list_root = './lists/{}/fss_list/{}/'.format(dataset, self.mode)\n\t        #     # dict_path = list_root + '{}_dict.txt'.format(self.mode)\n\t        #     if self.mode == 'train':\n\t        #         list_path = list_root + 'train_split{}.txt'.format(split)\n\t        #     elif self.mode == 'val':\n\t        #         list_path = list_root + 'val_base{}.txt'.format(split)\n\t        #     with open(list_path, 'r') as f:\n\t        #         f_str = f.readlines()\n", "        #     self.data_list = []\n\t        #     for line in f_str:\n\t        #         img, mask = line.split(' ')\n\t        #         self.data_list.append((img, mask.strip()))\n\t    def transform(self, image, label):\n\t        if self.transform_dict['type'] == 'albumentations':\n\t            aug = self.AUG(image=image, mask=label)\n\t            return aug['image'], aug['mask']\n\t        else:\n\t            image, label = self.AUG(image=image, label=label)\n", "            return image, label\n\t    def __len__(self):\n\t        return len(self.data_list)\n\t    def __getitem__(self, index):\n\t        image_path, label_path = self.data_list[index]\n\t        image = cv2.imread(image_path, cv2.IMREAD_COLOR) \n\t        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n\t        image = np.float32(image)\n\t        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n\t        label_tmp = label.copy()\n", "        label_class = np.unique(label).tolist()\n\t        if 0 in label_class:\n\t            label_class.remove(0)   \n\t        if 255 in label_class:\n\t            label_class.remove(255) \n\t        for cls in label_class:\n\t            select_pix = np.where(label_tmp == cls)\n\t            if cls in self.list:\n\t                label[select_pix[0],select_pix[1]] = self.list.index(cls) + 1\n\t            else:\n", "                label[select_pix[0],select_pix[1]] = 0\n\t        raw_label = label.copy()\n\t        image, label = self.transform(image, label)\n\t        # Return\n\t        if self.mode == 'val':\n\t            # return image, label, raw_label\n\t            return image, label\n\t        else:\n\t            return image, label"]}
{"filename": "dataset/iSAID.py", "chunked_list": ["from dataset.base_data import Few_Data, Base_Data\n\tclass iSAID_few_dataset(Few_Data):\n\t    class_id = {\n\t                0: 'unlabeled',\n\t                1: 'ship',\n\t                2: 'storage_tank',\n\t                3: 'baseball_diamond',  \n\t                4: 'tennis_court',\n\t                5: 'basketball_court',\n\t                6: 'Ground_Track_Field',\n", "                7: 'Bridge',\n\t                8: 'Large_Vehicle',\n\t                9: 'Small_Vehicle',\n\t                10: 'Helicopter',\n\t                11: 'Swimming_pool',\n\t                12: 'Roundabout',\n\t                13: 'Soccer_ball_field',\n\t                14: 'plane',\n\t                15: 'Harbor'\n\t                    }\n", "    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n\t               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n\t               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n\t               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n\t    all_class = list(range(1, 16))\n\t    val_class = [list(range(1, 6)), list(range(6, 11)), list(range(11, 16))]\n\t    data_root = '../data/iSAID'\n\t    train_list ='./lists/iSAID/train.txt'\n\t    val_list ='./lists/iSAID/val.txt'\n\t    def __init__(self, split=0, shot=1, dataset='iSAID', mode='train', ann_type='mask', transform_dict=None):\n", "        super().__init__(split, shot, dataset, mode, ann_type, transform_dict)\n\tclass iSAID_base_dataset(Base_Data):\n\t    class_id = {\n\t                0: 'unlabeled',\n\t                1: 'ship',\n\t                2: 'storage_tank',\n\t                3: 'baseball_diamond',\n\t                4: 'tennis_court',\n\t                5: 'basketball_court',\n\t                6: 'Ground_Track_Field',\n", "                7: 'Bridge',\n\t                8: 'Large_Vehicle',\n\t                9: 'Small_Vehicle',\n\t                10: 'Helicopter',\n\t                11: 'Swimming_pool',\n\t                12: 'Roundabout',\n\t                13: 'Soccer_ball_field',\n\t                14: 'plane',\n\t                15: 'Harbor'\n\t                    }\n", "    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n\t               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n\t               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n\t               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n\t    all_class = list(range(1, 16))\n\t    val_class = [list(range(1, 6)), list(range(6, 11)), list(range(11, 16))]\n\t    data_root = '../data/iSAID'\n\t    train_list ='./lists/iSAID/train.txt'\n\t    val_list ='./lists/iSAID/val.txt'\n\t    def __init__(self, split=0, shot=1, data_root=None, dataset='iSAID', mode='train', transform_dict=None):\n", "        super().__init__(split,  data_root, dataset, mode, transform_dict)"]}
{"filename": "dataset/iSAID_1.py", "chunked_list": ["from dataset.base_data import Few_Data, Base_Data\n\tclass iSAID_1_few_dataset(Few_Data):\n\t    class_id = {\n\t                0: 'unlabeled',\n\t                1: 'ship',\n\t                2: 'storage_tank',\n\t                3: 'baseball_diamond',  \n\t                4: 'tennis_court',\n\t                5: 'basketball_court',\n\t                6: 'Ground_Track_Field',\n", "                7: 'Bridge',\n\t                8: 'Large_Vehicle',\n\t                9: 'Small_Vehicle',\n\t                10: 'Helicopter',\n\t                11: 'Swimming_pool',\n\t                12: 'Roundabout',\n\t                13: 'Soccer_ball_field',\n\t                14: 'plane',\n\t                15: 'Harbor'\n\t                    }\n", "    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n\t               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n\t               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n\t               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n\t    all_class = list(range(1, 16))\n\t    val_class = [list(range(1, 16, 3)), list(range(2, 16, 3)), list(range(3, 16, 3))]\n\t    data_root = '../data/iSAID'\n\t    train_list ='./lists/iSAID/train.txt'\n\t    val_list ='./lists/iSAID/val.txt'\n\t    def __init__(self, split=0, shot=1, dataset='iSAID', mode='train', ann_type='mask', transform_dict=None):\n", "        super().__init__(split, shot, dataset, mode, ann_type, transform_dict)\n\tclass iSAID_1_base_dataset(Base_Data):\n\t    class_id = {\n\t                0: 'unlabeled',\n\t                1: 'ship',\n\t                2: 'storage_tank',\n\t                3: 'baseball_diamond',\n\t                4: 'tennis_court',\n\t                5: 'basketball_court',\n\t                6: 'Ground_Track_Field',\n", "                7: 'Bridge',\n\t                8: 'Large_Vehicle',\n\t                9: 'Small_Vehicle',\n\t                10: 'Helicopter',\n\t                11: 'Swimming_pool',\n\t                12: 'Roundabout',\n\t                13: 'Soccer_ball_field',\n\t                14: 'plane',\n\t                15: 'Harbor'\n\t                    }\n", "    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n\t               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n\t               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n\t               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n\t    all_class = list(range(1, 16))\n\t    val_class = [list(range(1, 16, 3)), list(range(2, 16, 3)), list(range(3, 16, 3))]\n\t    data_root = '../data/iSAID'\n\t    train_list ='./lists/iSAID/train.txt'\n\t    val_list ='./lists/iSAID/val.txt'\n\t    def __init__(self, split=0, shot=1, data_root=None, dataset='iSAID', mode='train', transform_dict=None):\n", "        super().__init__(split,  data_root, dataset, mode, transform_dict)"]}
{"filename": "util/config.py", "chunked_list": ["# -----------------------------------------------------------------------------\n\t# Functions for parsing args\n\t# -----------------------------------------------------------------------------\n\timport yaml\n\timport os\n\tfrom ast import literal_eval\n\timport copy\n\tclass CfgNode(dict):\n\t    \"\"\"\n\t    CfgNode represents an internal node in the configuration tree. It's a simple\n", "    dict-like container that allows for attribute-based access to keys.\n\t    \"\"\"\n\t    def __init__(self, init_dict=None, key_list=None, new_allowed=False):\n\t        # Recursively convert nested dictionaries in init_dict into CfgNodes\n\t        init_dict = {} if init_dict is None else init_dict\n\t        key_list = [] if key_list is None else key_list\n\t        for k, v in init_dict.items():\n\t            if type(v) is dict:\n\t                # Convert dict to CfgNode\n\t                init_dict[k] = CfgNode(v, key_list=key_list + [k])\n", "        super(CfgNode, self).__init__(init_dict)\n\t    def __getattr__(self, name):\n\t        if name in self:\n\t            return self[name]\n\t        else:\n\t            raise AttributeError(name)\n\t    def __setattr__(self, name, value):\n\t        self[name] = value\n\t    def __str__(self):\n\t        def _indent(s_, num_spaces):\n", "            s = s_.split(\"\\n\")\n\t            if len(s) == 1:\n\t                return s_\n\t            first = s.pop(0)\n\t            s = [(num_spaces * \" \") + line for line in s]\n\t            s = \"\\n\".join(s)\n\t            s = first + \"\\n\" + s\n\t            return s\n\t        r = \"\"\n\t        s = []\n", "        for k, v in sorted(self.items()):\n\t            seperator = \"\\n\" if isinstance(v, CfgNode) else \" \"\n\t            attr_str = \"{}:{}{}\".format(str(k), seperator, str(v))\n\t            attr_str = _indent(attr_str, 2)\n\t            s.append(attr_str)\n\t        r += \"\\n\".join(s)\n\t        return r\n\t    def __repr__(self):                # print\n\t        return \"{}({})\".format(self.__class__.__name__, super(CfgNode, self).__repr__())\n\tdef load_cfg_from_cfg_file(file_list):\n", "    cfg = {}\n\t    for file in file_list:\n\t        assert os.path.isfile(file) and file.endswith('.yaml'), \\\n\t            '{} is not a yaml file'.format(file)\n\t        with open(file, 'r') as f:\n\t            cfg_from_file = yaml.safe_load(f)\n\t        for key in cfg_from_file:\n\t            cfg[key] = cfg_from_file[key]\n\t    # for key in cfg_from_file:\n\t    #     for k, v in cfg_from_file[key].items():\n", "    #         cfg[k] = v\n\t    cfg = CfgNode(cfg)\n\t    return cfg\n\tdef merge_cfg_from_args(cfg, args):\n\t    args_dict = args.__dict__\n\t    for k ,v in args_dict.items():\n\t        if not k == 'config' or k == 'opts':\n\t            cfg[k] = v\n\t    return cfg\n\tdef merge_cfg_from_list(cfg, cfg_list):\n", "    new_cfg = copy.deepcopy(cfg)\n\t    assert len(cfg_list) % 2 == 0\n\t    for full_key, v in zip(cfg_list[0::2], cfg_list[1::2]):\n\t        subkey = full_key.split('.')[-1]\n\t        assert subkey in cfg, 'Non-existent key: {}'.format(full_key)\n\t        value = _decode_cfg_value(v)\n\t        value = _check_and_coerce_cfg_value_type(\n\t            value, cfg[subkey], subkey, full_key\n\t        )\n\t        setattr(new_cfg, subkey, value)\n", "    return new_cfg\n\tdef _decode_cfg_value(v):\n\t    \"\"\"Decodes a raw config value (e.g., from a yaml config files or command\n\t    line argument) into a Python object.\n\t    \"\"\"\n\t    # All remaining processing is only applied to strings\n\t    if not isinstance(v, str):\n\t        return v\n\t    # Try to interpret `v` as a:\n\t    #   string, number, tuple, list, dict, boolean, or None\n", "    try:\n\t        v = literal_eval(v)\n\t    # The following two excepts allow v to pass through when it represents a\n\t    # string.\n\t    #\n\t    # Longer explanation:\n\t    # The type of v is always a string (before calling literal_eval), but\n\t    # sometimes it *represents* a string and other times a data structure, like\n\t    # a list. In the case that v represents a string, what we got back from the\n\t    # yaml parser is 'foo' *without quotes* (so, not '\"foo\"'). literal_eval is\n", "    # ok with '\"foo\"', but will raise a ValueError if given 'foo'. In other\n\t    # cases, like paths (v = 'foo/bar' and not v = '\"foo/bar\"'), literal_eval\n\t    # will raise a SyntaxError.\n\t    except ValueError:\n\t        pass\n\t    except SyntaxError:\n\t        pass\n\t    return v\n\tdef _check_and_coerce_cfg_value_type(replacement, original, key, full_key):\n\t    \"\"\"Checks that `replacement`, which is intended to replace `original` is of\n", "    the right type. The type is correct if it matches exactly or is one of a few\n\t    cases in which the type can be easily coerced.\n\t    \"\"\"\n\t    original_type = type(original)\n\t    replacement_type = type(replacement)\n\t    # The types must match (with some exceptions)\n\t    if replacement_type == original_type:\n\t        return replacement\n\t    # Cast replacement from from_type to to_type if the replacement and original\n\t    # types match from_type and to_type\n", "    def conditional_cast(from_type, to_type):\n\t        if replacement_type == from_type and original_type == to_type:\n\t            return True, to_type(replacement)\n\t        else:\n\t            return False, None\n\t    # Conditionally casts\n\t    # list <-> tuple\n\t    casts = [(tuple, list), (list, tuple)]\n\t    # For py2: allow converting from str (bytes) to a unicode string\n\t    try:\n", "        casts.append((str, unicode))  # noqa: F821\n\t    except Exception:\n\t        pass\n\t    for (from_type, to_type) in casts:\n\t        converted, converted_value = conditional_cast(from_type, to_type)\n\t        if converted:\n\t            return converted_value\n\t    raise ValueError(\n\t        \"Type mismatch ({} vs. {}) with values ({} vs. {}) for config \"\n\t        \"key: {}\".format(\n", "            original_type, replacement_type, original, replacement, full_key\n\t        )\n\t    )\n\t# def _assert_with_logging(cond, msg):\n\t#     if not cond:\n\t#         logger.debug(msg)\n\t#     assert cond, msg\n"]}
{"filename": "util/get_weak_anns.py", "chunked_list": ["from __future__ import absolute_import, division\n\timport networkx as nx\n\timport numpy as np\n\tfrom scipy.ndimage import binary_dilation, binary_erosion, maximum_filter\n\tfrom scipy.special import comb\n\tfrom skimage.filters import rank\n\tfrom skimage.morphology import dilation, disk, erosion, medial_axis\n\tfrom sklearn.neighbors import radius_neighbors_graph\n\timport cv2\n\timport matplotlib.pyplot as plt\n", "import matplotlib.patches as mpatches\n\tfrom scipy import ndimage\n\tdef bezier_curve(points, nb_points=1000):\n\t    \"\"\" Given a list of points compute a bezier curve from it.\n\t    # Arguments\n\t        points: ndarray. Array of points with shape (N, 2) with N being the\n\t            number of points and the second dimension representing the\n\t            (x, y) coordinates.\n\t        nb_points: Integer. Number of points to sample from the bezier curve.\n\t            This value must be larger than the number of points given in\n", "            `points`. Maximum value 10000.\n\t    # Returns\n\t        ndarray: Array of shape (1000, 2) with the bezier curve of the\n\t            given path of points.\n\t    \"\"\"\n\t    nb_points = min(nb_points, 1000)\n\t    points = np.asarray(points, dtype=np.float)\n\t    if points.ndim != 2 or points.shape[1] != 2:\n\t        raise ValueError(\n\t            '`points` should be two dimensional and have shape: (N, 2)')\n", "    n_points = len(points)\n\t    if n_points > nb_points:\n\t        # We are downsampling points\n\t        return points\n\t    t = np.linspace(0., 1., nb_points).reshape(1, -1)\n\t    # Compute the Bernstein polynomial of n, i as a function of t\n\t    i = np.arange(n_points).reshape(-1, 1)\n\t    n = n_points - 1\n\t    polynomial_array = comb(n, i) * (t**(n - i)) * (1 - t)**i\n\t    bezier_curve_points = polynomial_array.T.dot(points)\n", "    return bezier_curve_points\n\tdef bresenham(points):\n\t    \"\"\" Apply Bresenham algorithm for a list points.\n\t    More info: https://en.wikipedia.org/wiki/Bresenham's_line_algorithm\n\t    # Arguments\n\t        points: ndarray. Array of points with shape (N, 2) with N being the number\n\t            if points and the second coordinate representing the (x, y)\n\t            coordinates.\n\t    # Returns\n\t        ndarray: Array of points after having applied the bresenham algorithm.\n", "    \"\"\"\n\t    points = np.asarray(points, dtype=np.int)\n\t    def line(x0, y0, x1, y1):\n\t        \"\"\" Bresenham line algorithm.\n\t        \"\"\"\n\t        d_x = x1 - x0\n\t        d_y = y1 - y0\n\t        x_sign = 1 if d_x > 0 else -1\n\t        y_sign = 1 if d_y > 0 else -1\n\t        d_x = np.abs(d_x)\n", "        d_y = np.abs(d_y)\n\t        if d_x > d_y:\n\t            xx, xy, yx, yy = x_sign, 0, 0, y_sign\n\t        else:\n\t            d_x, d_y = d_y, d_x\n\t            xx, xy, yx, yy = 0, y_sign, x_sign, 0\n\t        D = 2 * d_y - d_x\n\t        y = 0\n\t        line = np.empty((d_x + 1, 2), dtype=points.dtype)\n\t        for x in range(d_x + 1):\n", "            line[x] = [x0 + x * xx + y * yx, y0 + x * xy + y * yy]\n\t            if D >= 0:\n\t                y += 1\n\t                D -= 2 * d_x\n\t            D += 2 * d_y\n\t        return line\n\t    nb_points = len(points)\n\t    if nb_points < 2:\n\t        return points\n\t    new_points = []\n", "    for i in range(nb_points - 1):\n\t        p = points[i:i + 2].ravel().tolist()\n\t        new_points.append(line(*p))\n\t    new_points = np.concatenate(new_points, axis=0)\n\t    return new_points\n\tdef scribbles2mask(scribbles,\n\t                   output_resolution,\n\t                   bezier_curve_sampling=False,\n\t                   nb_points=1000,\n\t                   compute_bresenham=True,\n", "                   default_value=0):\n\t    \"\"\" Convert the scribbles data into a mask.\n\t    # Arguments\n\t        scribbles: Dictionary. Scribbles in the default format.\n\t        output_resolution: Tuple. Output resolution (H, W).\n\t        bezier_curve_sampling: Boolean. Weather to sample first the returned\n\t            scribbles using bezier curve or not.\n\t        nb_points: Integer. If `bezier_curve_sampling` is `True` set the number\n\t            of points to sample from the bezier curve.\n\t        compute_bresenham: Boolean. Whether to compute bresenham algorithm for the\n", "            scribbles lines.\n\t        default_value: Integer. Default value for the pixels which do not belong\n\t            to any scribble.\n\t    # Returns\n\t        ndarray: Array with the mask of the scribbles with the index of the\n\t            object ids. The shape of the returned array is (B x H x W) by\n\t            default or (H x W) if `only_annotated_frame==True`.\n\t    \"\"\"\n\t    if len(output_resolution) != 2:\n\t        raise ValueError(\n", "            'Invalid output resolution: {}'.format(output_resolution))\n\t    for r in output_resolution:\n\t        if r < 1:\n\t            raise ValueError(\n\t                'Invalid output resolution: {}'.format(output_resolution))\n\t    size_array = np.asarray(output_resolution[::-1], dtype=np.float) - 1\n\t    m = np.full(output_resolution, default_value, dtype=np.int)\n\t    for p in scribbles:\n\t        p /= output_resolution[::-1]\n\t        path = p.tolist()\n", "        path = np.asarray(path, dtype=np.float)\n\t        if bezier_curve_sampling:\n\t            path = bezier_curve(path, nb_points=nb_points)\n\t        path *= size_array\n\t        path = path.astype(np.int)\n\t        if compute_bresenham:\n\t            path = bresenham(path)\n\t        m[path[:, 1], path[:, 0]] = 1\n\t    return m\n\tclass ScribblesRobot(object):\n", "    \"\"\"Robot that generates realistic scribbles simulating human interaction.\n\t    # Attributes:\n\t        kernel_size: Float. Fraction of the square root of the area used\n\t            to compute the dilation and erosion before computing the\n\t            skeleton of the error masks.\n\t        max_kernel_radius: Float. Maximum kernel radius when applying\n\t            dilation and erosion. Default 16 pixels.\n\t        min_nb_nodes: Integer. Number of nodes necessary to keep a connected\n\t            graph and convert it into a scribble.\n\t        nb_points: Integer. Number of points to sample the bezier curve\n", "            when converting the final paths into curves.\n\t    Reference:\n\t    [1] Sergi et al., \"The 2018 DAVIS Challenge on Video Object Segmentation\", arxiv 2018\n\t    [2] Jordi et al., \"The 2017 DAVIS Challenge on Video Object Segmentation\", arxiv 2017\n\t    \"\"\"\n\t    def __init__(self,\n\t                 kernel_size=.15,\n\t                 max_kernel_radius=16,\n\t                 min_nb_nodes=4,\n\t                 nb_points=1000):\n", "        if kernel_size >= 1. or kernel_size < 0:\n\t            raise ValueError('kernel_size must be a value between [0, 1).')\n\t        self.kernel_size = kernel_size\n\t        self.max_kernel_radius = max_kernel_radius\n\t        self.min_nb_nodes = min_nb_nodes\n\t        self.nb_points = nb_points\n\t    def _generate_scribble_mask(self, mask):\n\t        \"\"\" Generate the skeleton from a mask\n\t        Given an error mask, the medial axis is computed to obtain the\n\t        skeleton of the objects. In order to obtain smoother skeleton and\n", "        remove small objects, an erosion and dilation operations are performed.\n\t        The kernel size used is proportional the squared of the area.\n\t        # Arguments\n\t            mask: Numpy Array. Error mask\n\t        Returns:\n\t            skel: Numpy Array. Skeleton mask\n\t        \"\"\"\n\t        mask = np.asarray(mask, dtype=np.uint8)\n\t        side = np.sqrt(np.sum(mask > 0))\n\t        mask_ = mask\n", "        # kernel_size = int(self.kernel_size * side)\n\t        kernel_radius = self.kernel_size * side * .5\n\t        kernel_radius = min(kernel_radius, self.max_kernel_radius)\n\t        # logging.verbose(\n\t        #     'Erosion and dilation with kernel radius: {:.1f}'.format(\n\t        #         kernel_radius), 2)\n\t        compute = True\n\t        while kernel_radius > 1. and compute:\n\t            kernel = disk(kernel_radius)\n\t            mask_ = rank.minimum(mask.copy(), kernel)\n", "            mask_ = rank.maximum(mask_, kernel)\n\t            compute = False\n\t            if mask_.astype(np.bool).sum() == 0:\n\t                compute = True\n\t                prev_kernel_radius = kernel_radius\n\t                kernel_radius *= .9\n\t                # logging.verbose('Reducing kernel radius from {:.1f} '.format(\n\t                #     prev_kernel_radius) +\n\t                #                 'pixels to {:.1f}'.format(kernel_radius), 1)\n\t        mask_ = np.pad(\n", "            mask_, ((1, 1), (1, 1)), mode='constant', constant_values=False)\n\t        skel = medial_axis(mask_.astype(np.bool))\n\t        skel = skel[1:-1, 1:-1]\n\t        return skel\n\t    def _mask2graph(self, skeleton_mask):\n\t        \"\"\" Transforms a skeleton mask into a graph\n\t        Args:\n\t            skeleton_mask (ndarray): Skeleton mask\n\t        Returns:\n\t            tuple(nx.Graph, ndarray): Returns a tuple where the first element\n", "                is a Graph and the second element is an array of xy coordinates\n\t                indicating the coordinates for each Graph node.\n\t                If an empty mask is given, None is returned.\n\t        \"\"\"\n\t        mask = np.asarray(skeleton_mask, dtype=np.bool)\n\t        if np.sum(mask) == 0:\n\t            return None\n\t        h, w = mask.shape\n\t        x, y = np.arange(w), np.arange(h)\n\t        X, Y = np.meshgrid(x, y)\n", "        X, Y = X.ravel(), Y.ravel()\n\t        M = mask.ravel()\n\t        X, Y = X[M], Y[M]\n\t        points = np.c_[X, Y]\n\t        G = radius_neighbors_graph(points, np.sqrt(2), mode='distance')\n\t        T = nx.from_scipy_sparse_matrix(G)\n\t        return T, points\n\t    def _acyclics_subgraphs(self, G):\n\t        \"\"\" Divide a graph into connected components subgraphs\n\t        Divide a graph into connected components subgraphs and remove its\n", "        cycles removing the edge with higher weight inside the cycle. Also\n\t        prune the graphs by number of nodes in case the graph has not enought\n\t        nodes.\n\t        Args:\n\t            G (nx.Graph): Graph\n\t        Returns:\n\t            list(nx.Graph): Returns a list of graphs which are subgraphs of G\n\t                with cycles removed.\n\t        \"\"\"\n\t        if not isinstance(G, nx.Graph):\n", "            raise TypeError('G must be a nx.Graph instance')\n\t        S = []  # List of subgraphs of G\n\t        for g in nx.connected_component_subgraphs(G):  # Make sure the version of package \"networkx\" < 2.4\n\t        # for g in (G.subgraph(c) for c in nx.connected_components(G)):\n\t            # Remove all cycles that we may find\n\t            has_cycles = True\n\t            while has_cycles:\n\t                try:\n\t                    cycle = nx.find_cycle(g)\n\t                    weights = np.asarray([G[u][v]['weight'] for u, v in cycle])\n", "                    idx = weights.argmax()\n\t                    # Remove the edge with highest weight at cycle\n\t                    g.remove_edge(*cycle[idx])\n\t                except nx.NetworkXNoCycle:\n\t                    has_cycles = False\n\t            if len(g) < self.min_nb_nodes:\n\t                # Prune small subgraphs\n\t                # logging.verbose('Remove a small line with {} nodes'.format(\n\t                #     len(g)), 1)\n\t                continue\n", "            S.append(g)\n\t        return S\n\t    def _longest_path_in_tree(self, G):\n\t        \"\"\" Given a tree graph, compute the longest path and return it\n\t        Given an undirected tree graph, compute the longest path and return it.\n\t        The approach use two shortest path transversals (shortest path in a\n\t        tree is the same as longest path). This could be improve but would\n\t        require implement it:\n\t        https://cs.stackexchange.com/questions/11263/longest-path-in-an-undirected-tree-with-only-one-traversal\n\t        Args:\n", "            G (nx.Graph): Graph which should be an undirected tree graph\n\t        Returns:\n\t            list(int): Returns a list of indexes of the nodes belonging to the\n\t                longest path.\n\t        \"\"\"\n\t        if not isinstance(G, nx.Graph):\n\t            raise TypeError('G must be a nx.Graph instance')\n\t        if not nx.is_tree(G):\n\t            raise ValueError('Graph G must be a tree (graph without cycles)')\n\t        # Compute the furthest node to the random node v\n", "        v = list(G.nodes())[0]\n\t        distance = nx.single_source_shortest_path_length(G, v)\n\t        vp = max(distance.items(), key=lambda x: x[1])[0]\n\t        # From this furthest point v' find again the longest path from it\n\t        distance = nx.single_source_shortest_path(G, vp)\n\t        longest_path = max(distance.values(), key=len)\n\t        # Return the longest path\n\t        return list(longest_path)\n\t    def generate_scribbles(self, mask):\n\t        \"\"\"Given a binary mask, the robot will return a scribble in the region\"\"\"\n", "        # generate scribbles\n\t        skel_mask = self._generate_scribble_mask(mask)\n\t        G, P = self._mask2graph(skel_mask)\n\t        S = self._acyclics_subgraphs(G)\n\t        longest_paths_idx = [self._longest_path_in_tree(s) for s in S]\n\t        longest_paths = [P[idx] for idx in longest_paths_idx]\n\t        scribbles_paths = [\n\t                bezier_curve(p, self.nb_points) for p in longest_paths\n\t        ]\n\t        output_resolution = tuple([mask.shape[0], mask.shape[1]])\n", "        scribble_mask = scribbles2mask(scribbles_paths, output_resolution)\n\t        return scribble_mask\n\tdef find_bbox(mask):\n\t    _, labels, stats, centroids = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n\t    return stats[1:]  # remove bg stat\n\tdef transform_anns(mask, ann_type):\n\t    mask_ori = mask.copy()\n\t    if ann_type == 'scribble':\n\t        dilated_size = 20\n\t        Scribble_Expert = ScribblesRobot()\n", "        scribble_mask = Scribble_Expert.generate_scribbles(mask)\n\t        scribble_mask = ndimage.maximum_fliter(scribble_mask, size=dilated_size)\n\t        return scribble_mask.astype(np.uint8), mask_ori\n\t    elif ann_type == 'bbox':\n\t        bboxs = find_bbox(mask)\n\t        for j in bboxs: \n\t            cv2.rectangle(mask, (j[0], j[1]), (j[0] + j[2], j[1] + j[3]), 1, -1) # -1->fill; 2->draw_rec        \n\t        return mask, mask_ori\n\t    elif ann_type == 'mask':\n\t        return mask, mask_ori\n", "if __name__ == '__main__':\n\t    label_path = '2008_001227.png'\n\t    Scribble_Expert = ScribblesRobot()\n\t    mask = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n\t    dilated_size = 20\n\t    scribble_mask = Scribble_Expert.generate_scribbles(mask)\n\t    scribble_mask = ndimage.maximum_fliter(scribble_mask, size=dilated_size) # \n\t    scribble_mask[scribble_mask==1] = 255\n\t    cv2.imwrite('scribble.png', scribble_mask)\n\t    bboxs = find_bbox(mask)\n", "    mask_color = cv2.imread(label_path, cv2.IMREAD_COLOR)\n\t    for j in bboxs: \n\t        cv2.rectangle(mask_color, (j[0], j[1]), (j[0] + j[2], j[1] + j[3]), (0,255,0), -1) # -1->fill; 2->draw_rec\n\t    cv2.imwrite('bbox.png', mask_color)\n\t    print('done')\n"]}
{"filename": "util/get_transform.py", "chunked_list": ["import random\n\timport math\n\timport numpy as np\n\timport numbers\n\timport collections\n\timport cv2\n\timport torch\n\timport albumentations as Albu\n\tfrom albumentations.pytorch import ToTensorV2\n\timport util.transform as base\n", "from torchvision import transforms as pytorch\n\t\"\"\"\n\talbumentations \n\t\"\"\"\n\tdef get_transform(transform_dict):\n\t    \"\"\"\n\t    a dict \n\t    \"\"\"\n\t    pip_line = []\n\t    if transform_dict['type'] == 'albumentations':\n", "        tmp = 'Albu.'\n\t    elif transform_dict['type'] == 'pytorch':\n\t        tmp = 'pytorch.'\n\t    else:\n\t        tmp = 'base.'\n\t    for key in transform_dict:\n\t        if key != 'type':\n\t            if key == 'OneOf' or key == 'SomeOf':\n\t                tmp_pip_line = []\n\t                for item in transform_dict[key]['transforms']:\n", "                    tmp_pip_line.append(eval(tmp+ item.pop('type'))(**item))\n\t                pip_line.append(eval(tmp+ key)(transforms=tmp_pip_line, p=transform_dict[key]['p']))\n\t            elif key == 'ToTensorV2':\n\t                pip_line.append(eval(key)())\n\t            elif key == 'ToTensor' and tmp == 'pytorch.':\n\t                pip_line.append(eval(tmp+ key)())\n\t            else:\n\t                pip_line.append(eval(tmp+ key)(**transform_dict[key]))\n\t    transformer = eval(tmp + 'Compose')(pip_line)\n\t    return transformer\n"]}
{"filename": "util/transform.py", "chunked_list": ["import random\n\timport math\n\timport numpy as np\n\timport numbers\n\timport collections\n\timport cv2\n\timport time\n\timport torch\n\tmanual_seed = 123\n\ttorch.manual_seed(manual_seed)\n", "np.random.seed(manual_seed)\n\ttorch.manual_seed(manual_seed)\n\ttorch.cuda.manual_seed_all(manual_seed)\n\trandom.seed(manual_seed)\n\tclass Compose(object):\n\t    # Composes segtransforms: segtransform.Compose([segtransform.RandScale([0.5, 2.0]), segtransform.ToTensor()])\n\t    def __init__(self, segtransform):\n\t        self.segtransform = segtransform\n\t    def __call__(self, image, label):\n\t        for t in self.segtransform:\n", "            image, label = t(image, label)\n\t        return image, label\n\tclass ToTensor(object):\n\t    def __init__(self, enabled = True):\n\t        self.enabled = enabled\n\t    # Converts numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n\t    def __call__(self, image, label):\n\t        if not isinstance(image, np.ndarray) or not isinstance(label, np.ndarray):\n\t            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray\"\n\t                                \"[eg: data readed by cv2.imread()].\\n\"))\n", "        if len(image.shape) > 3 or len(image.shape) < 2:\n\t            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray with 3 dims or 2 dims.\\n\"))\n\t        if len(image.shape) == 2:\n\t            image = np.expand_dims(image, axis=2)\n\t        if not len(label.shape) == 2:\n\t            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray labellabel with 2 dims.\\n\"))\n\t        image = torch.from_numpy(image.transpose((2, 0, 1)))\n\t        if not isinstance(image, torch.FloatTensor):\n\t            image = image.float()\n\t        label = torch.from_numpy(label)\n", "        if not isinstance(label, torch.LongTensor):\n\t            label = label.long()\n\t        return image, label\n\tclass ToNumpy(object):\n\t    # Converts torch.FloatTensor of shape (C x H x W) to a numpy.ndarray (H x W x C).\n\t    def __call__(self, image, label):\n\t        if not isinstance(image, torch.Tensor) or not isinstance(label, torch.Tensor):\n\t            raise (RuntimeError(\"segtransform.ToNumpy() only handle torch.tensor\"))\n\t        image = image.cpu().numpy().transpose((1, 2, 0))\n\t        if not image.dtype == np.uint8:\n", "            image = image.astype(np.uint8)\n\t        label = label.cpu().numpy().transpose((1, 2, 0))\n\t        if not label.dtype == np.uint8:\n\t            label = label.astype(np.uint8)\n\t        return image, label\n\tclass Normalize(object):\n\t    # Normalize tensor with mean and standard deviation along channel: channel = (channel - mean) / std\n\t    def __init__(self, mean, std=None):\n\t        if std is None:\n\t            assert len(mean) > 0\n", "        else:\n\t            assert len(mean) == len(std)\n\t        self.mean = mean\n\t        self.std = std\n\t    def __call__(self, image, label):\n\t        if self.std is None:\n\t            for t, m in zip(image, self.mean):\n\t                t.sub_(m)\n\t        else:\n\t            for t, m, s in zip(image, self.mean, self.std):\n", "                t.sub_(m).div_(s)\n\t        return image, label\n\tclass UnNormalize(object):\n\t    # UnNormalize tensor with mean and standard deviation along channel: channel = (channel * std) + mean\n\t    def __init__(self, mean, std=None):\n\t        if std is None:\n\t            assert len(mean) > 0\n\t        else:\n\t            assert len(mean) == len(std)\n\t        self.mean = mean\n", "        self.std = std\n\t    def __call__(self, image, label):\n\t        if self.std is None:\n\t            for t, m in zip(image, self.mean):\n\t                t.add_(m)\n\t        else:\n\t            for t, m, s in zip(image, self.mean, self.std):\n\t                t.mul_(s).add_(m)\n\t        return image, label\n\tclass Resize(object):\n", "    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n\t    def __init__(self, size):\n\t        self.size = size\n\t    def __call__(self, image, label):\n\t        value_scale = 255\n\t        mean = [0.485, 0.456, 0.406]\n\t        mean = [item * value_scale for item in mean]\n\t        std = [0.229, 0.224, 0.225]\n\t        std = [item * value_scale for item in std]\n\t        def find_new_hw(ori_h, ori_w, test_size):\n", "            if ori_h >= ori_w:\n\t                ratio = test_size*1.0 / ori_h\n\t                new_h = test_size\n\t                new_w = int(ori_w * ratio)\n\t            elif ori_w > ori_h:\n\t                ratio = test_size*1.0 / ori_w\n\t                new_h = int(ori_h * ratio)\n\t                new_w = test_size\n\t            if new_h % 8 != 0:\n\t                new_h = (int(new_h /8))*8\n", "            else:\n\t                new_h = new_h\n\t            if new_w % 8 != 0:\n\t                new_w = (int(new_w /8))*8\n\t            else:\n\t                new_w = new_w    \n\t            return new_h, new_w           \n\t        test_size = self.size\n\t        new_h, new_w = find_new_hw(image.shape[0], image.shape[1], test_size)\n\t        #new_h, new_w = test_size, test_size\n", "        image_crop = cv2.resize(image, dsize=(int(new_w), int(new_h)), interpolation=cv2.INTER_LINEAR)\n\t        back_crop = np.zeros((test_size, test_size, 3)) \n\t        # back_crop[:,:,0] = mean[0]\n\t        # back_crop[:,:,1] = mean[1]\n\t        # back_crop[:,:,2] = mean[2]\n\t        back_crop[:new_h, :new_w, :] = image_crop\n\t        image = back_crop \n\t        s_mask = label\n\t        new_h, new_w = find_new_hw(s_mask.shape[0], s_mask.shape[1], test_size)\n\t        #new_h, new_w = test_size, test_size\n", "        s_mask = cv2.resize(s_mask.astype(np.float32), dsize=(int(new_w), int(new_h)),interpolation=cv2.INTER_NEAREST)\n\t        back_crop_s_mask = np.ones((test_size, test_size)) * 255\n\t        back_crop_s_mask[:new_h, :new_w] = s_mask\n\t        label = back_crop_s_mask\n\t        return image, label\n\tclass test_Resize(object):\n\t    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n\t    def __init__(self, size):\n\t        self.size = size\n\t    def __call__(self, image, label):\n", "        value_scale = 255\n\t        mean = [0.485, 0.456, 0.406]\n\t        mean = [item * value_scale for item in mean]\n\t        std = [0.229, 0.224, 0.225]\n\t        std = [item * value_scale for item in std]\n\t        def find_new_hw(ori_h, ori_w, test_size):\n\t            if max(ori_h, ori_w) > test_size:\n\t                if ori_h >= ori_w:\n\t                    ratio = test_size*1.0 / ori_h\n\t                    new_h = test_size\n", "                    new_w = int(ori_w * ratio)\n\t                elif ori_w > ori_h:\n\t                    ratio = test_size*1.0 / ori_w\n\t                    new_h = int(ori_h * ratio)\n\t                    new_w = test_size\n\t                if new_h % 8 != 0:\n\t                    new_h = (int(new_h /8))*8\n\t                else:\n\t                    new_h = new_h\n\t                if new_w % 8 != 0:\n", "                    new_w = (int(new_w /8))*8\n\t                else:\n\t                    new_w = new_w    \n\t                return new_h, new_w     \n\t            else:\n\t                return ori_h, ori_w      \n\t        test_size = self.size\n\t        new_h, new_w = find_new_hw(image.shape[0], image.shape[1], test_size)\n\t        if new_w != image.shape[0] or new_h != image.shape[1]:\n\t            image_crop = cv2.resize(image, dsize=(int(new_w), int(new_h)), interpolation=cv2.INTER_LINEAR)\n", "        else:\n\t            image_crop = image.copy()\n\t        back_crop = np.zeros((test_size, test_size, 3)) \n\t        back_crop[:new_h, :new_w, :] = image_crop\n\t        image = back_crop \n\t        s_mask = label\n\t        new_h, new_w = find_new_hw(s_mask.shape[0], s_mask.shape[1], test_size)\n\t        if new_w != s_mask.shape[0] or new_h != s_mask.shape[1]:\n\t            s_mask = cv2.resize(s_mask.astype(np.float32), dsize=(int(new_w), int(new_h)),interpolation=cv2.INTER_NEAREST)\n\t        back_crop_s_mask = np.ones((test_size, test_size)) * 255\n", "        back_crop_s_mask[:new_h, :new_w] = s_mask\n\t        label = back_crop_s_mask\n\t        return image, label\n\tclass Direct_Resize(object):\n\t    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n\t    def __init__(self, size):\n\t        self.size = size\n\t    def __call__(self, image, label):\n\t        test_size = self.size\n\t        image = cv2.resize(image, dsize=(test_size, test_size), interpolation=cv2.INTER_LINEAR)\n", "        label = cv2.resize(label.astype(np.float32), dsize=(test_size, test_size),interpolation=cv2.INTER_NEAREST)\n\t        return image, label\n\tclass RandScale(object):\n\t    # Randomly resize image & label with scale factor in [scale_min, scale_max]\n\t    def __init__(self, scale, aspect_ratio=None):\n\t        assert (isinstance(scale, collections.Iterable) and len(scale) == 2)\n\t        if isinstance(scale, collections.Iterable) and len(scale) == 2 \\\n\t                and isinstance(scale[0], numbers.Number) and isinstance(scale[1], numbers.Number) \\\n\t                and 0 < scale[0] < scale[1]:\n\t            self.scale = scale\n", "        else:\n\t            raise (RuntimeError(\"segtransform.RandScale() scale param error.\\n\"))\n\t        if aspect_ratio is None:\n\t            self.aspect_ratio = aspect_ratio\n\t        elif isinstance(aspect_ratio, collections.Iterable) and len(aspect_ratio) == 2 \\\n\t                and isinstance(aspect_ratio[0], numbers.Number) and isinstance(aspect_ratio[1], numbers.Number) \\\n\t                and 0 < aspect_ratio[0] < aspect_ratio[1]:\n\t            self.aspect_ratio = aspect_ratio\n\t        else:\n\t            raise (RuntimeError(\"segtransform.RandScale() aspect_ratio param error.\\n\"))\n", "    def __call__(self, image, label):\n\t        temp_scale = self.scale[0] + (self.scale[1] - self.scale[0]) * random.random()\n\t        temp_aspect_ratio = 1.0\n\t        if self.aspect_ratio is not None:\n\t            temp_aspect_ratio = self.aspect_ratio[0] + (self.aspect_ratio[1] - self.aspect_ratio[0]) * random.random()\n\t            temp_aspect_ratio = math.sqrt(temp_aspect_ratio)\n\t        scale_factor_x = temp_scale * temp_aspect_ratio\n\t        scale_factor_y = temp_scale / temp_aspect_ratio\n\t        image = cv2.resize(image, None, fx=scale_factor_x, fy=scale_factor_y, interpolation=cv2.INTER_LINEAR)\n\t        label = cv2.resize(label, None, fx=scale_factor_x, fy=scale_factor_y, interpolation=cv2.INTER_NEAREST)\n", "        return image, label\n\tclass Crop(object):\n\t    \"\"\"Crops the given ndarray image (H*W*C or H*W).\n\t    Args:\n\t        size (sequence or int): Desired output size of the crop. If size is an\n\t        int instead of sequence like (h, w), a square crop (size, size) is made.\n\t    \"\"\"\n\t    def __init__(self, size, crop_type='center', padding=None, ignore_label=255):\n\t        self.size = size\n\t        if isinstance(size, int):\n", "            self.crop_h = size\n\t            self.crop_w = size\n\t        elif isinstance(size, collections.Iterable) and len(size) == 2 \\\n\t                and isinstance(size[0], int) and isinstance(size[1], int) \\\n\t                and size[0] > 0 and size[1] > 0:\n\t            self.crop_h = size[0]\n\t            self.crop_w = size[1]\n\t        else:\n\t            raise (RuntimeError(\"crop size error.\\n\"))\n\t        if crop_type == 'center' or crop_type == 'rand':\n", "            self.crop_type = crop_type\n\t        else:\n\t            raise (RuntimeError(\"crop type error: rand | center\\n\"))\n\t        if padding is None:\n\t            self.padding = padding\n\t        elif isinstance(padding, list):\n\t            if all(isinstance(i, numbers.Number) for i in padding):\n\t                self.padding = padding\n\t            else:\n\t                raise (RuntimeError(\"padding in Crop() should be a number list\\n\"))\n", "            if len(padding) != 3:\n\t                raise (RuntimeError(\"padding channel is not equal with 3\\n\"))\n\t        else:\n\t            raise (RuntimeError(\"padding in Crop() should be a number list\\n\"))\n\t        if isinstance(ignore_label, int):\n\t            self.ignore_label = ignore_label\n\t        else:\n\t            raise (RuntimeError(\"ignore_label should be an integer number\\n\"))\n\t    def __call__(self, image, label):\n\t        h, w = label.shape\n", "        pad_h = max(self.crop_h - h, 0)\n\t        pad_w = max(self.crop_w - w, 0)\n\t        pad_h_half = int(pad_h / 2)\n\t        pad_w_half = int(pad_w / 2)\n\t        if pad_h > 0 or pad_w > 0:\n\t            if self.padding is None:\n\t                raise (RuntimeError(\"segtransform.Crop() need padding while padding argument is None\\n\"))\n\t            image = cv2.copyMakeBorder(image, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=self.padding)\n\t            label = cv2.copyMakeBorder(label, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=self.ignore_label)\n\t        h, w = label.shape\n", "        raw_label = label\n\t        raw_image = image\n\t        if self.crop_type == 'rand':\n\t            h_off = random.randint(0, h - self.crop_h)\n\t            w_off = random.randint(0, w - self.crop_w)\n\t        else:\n\t            h_off = int((h - self.crop_h) / 2)\n\t            w_off = int((w - self.crop_w) / 2)\n\t        image = image[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n\t        label = label[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n", "        raw_pos_num = np.sum(raw_label == 1)\n\t        pos_num = np.sum(label == 1)\n\t        crop_cnt = 0\n\t        while(pos_num < 0.85*raw_pos_num and crop_cnt<=30):\n\t            image = raw_image\n\t            label = raw_label\n\t            if self.crop_type == 'rand':\n\t                h_off = random.randint(0, h - self.crop_h)\n\t                w_off = random.randint(0, w - self.crop_w)\n\t            else:\n", "                h_off = int((h - self.crop_h) / 2)\n\t                w_off = int((w - self.crop_w) / 2)\n\t            image = image[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n\t            label = label[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]   \n\t            raw_pos_num = np.sum(raw_label == 1)\n\t            pos_num = np.sum(label == 1)  \n\t            crop_cnt += 1\n\t        if crop_cnt >= 50:\n\t            image = cv2.resize(raw_image, (self.size[0], self.size[0]), interpolation=cv2.INTER_LINEAR)\n\t            label = cv2.resize(raw_label, (self.size[0], self.size[0]), interpolation=cv2.INTER_NEAREST)            \n", "        if image.shape != (self.size[0], self.size[0], 3):\n\t            image = cv2.resize(image, (self.size[0], self.size[0]), interpolation=cv2.INTER_LINEAR)\n\t            label = cv2.resize(label, (self.size[0], self.size[0]), interpolation=cv2.INTER_NEAREST)\n\t        return image, label\n\tclass RandRotate(object):\n\t    # Randomly rotate image & label with rotate factor in [rotate_min, rotate_max]\n\t    def __init__(self, rotate, padding, ignore_label=255, p=0.5):\n\t        assert (isinstance(rotate, collections.Iterable) and len(rotate) == 2)\n\t        if isinstance(rotate[0], numbers.Number) and isinstance(rotate[1], numbers.Number) and rotate[0] < rotate[1]:\n\t            self.rotate = rotate\n", "        else:\n\t            raise (RuntimeError(\"segtransform.RandRotate() scale param error.\\n\"))\n\t        assert padding is not None\n\t        assert isinstance(padding, list) and len(padding) == 3\n\t        if all(isinstance(i, numbers.Number) for i in padding):\n\t            self.padding = padding\n\t        else:\n\t            raise (RuntimeError(\"padding in RandRotate() should be a number list\\n\"))\n\t        assert isinstance(ignore_label, int)\n\t        self.ignore_label = ignore_label\n", "        self.p = p\n\t    def __call__(self, image, label):\n\t        if random.random() < self.p:\n\t            angle = self.rotate[0] + (self.rotate[1] - self.rotate[0]) * random.random()\n\t            h, w = label.shape\n\t            matrix = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n\t            image = cv2.warpAffine(image, matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=self.padding)\n\t            label = cv2.warpAffine(label, matrix, (w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=self.ignore_label)\n\t        return image, label\n\tclass RandomHorizontalFlip(object):\n", "    def __init__(self, p=0.5):\n\t        self.p = p\n\t    def __call__(self, image, label):\n\t        if random.random() < self.p:\n\t            image = cv2.flip(image, 1)\n\t            label = cv2.flip(label, 1)\n\t        return image, label\n\tclass RandomVerticalFlip(object):\n\t    def __init__(self, p=0.5):\n\t        self.p = p\n", "    def __call__(self, image, label):\n\t        if random.random() < self.p:\n\t            image = cv2.flip(image, 0)\n\t            label = cv2.flip(label, 0)\n\t        return image, label\n\tclass RandomGaussianBlur(object):\n\t    def __init__(self, radius=5):\n\t        self.radius = radius\n\t    def __call__(self, image, label):\n\t        if random.random() < 0.5:\n", "            image = cv2.GaussianBlur(image, (self.radius, self.radius), 0)\n\t        return image, label\n\tclass RGB2BGR(object):\n\t    # Converts image from RGB order to BGR order, for model initialized from Caffe\n\t    def __call__(self, image, label):\n\t        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\t        return image, label\n\tclass BGR2RGB(object):\n\t    # Converts image from BGR order to RGB order, for model initialized from Pytorch\n\t    def __call__(self, image, label):\n", "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t        return image, label\n"]}
{"filename": "util/util.py", "chunked_list": ["import os\n\timport numpy as np\n\tfrom PIL import Image\n\timport random\n\timport logging\n\timport cv2\n\timport matplotlib.pyplot as plt\n\tfrom matplotlib.pyplot import MultipleLocator\n\tfrom matplotlib.ticker import FuncFormatter, FormatStrFormatter\n\tfrom matplotlib import font_manager\n", "from matplotlib import rcParams\n\timport seaborn as sns\n\timport pandas as pd\n\timport math\n\tfrom seaborn.distributions import distplot\n\tfrom tqdm import tqdm\n\tfrom scipy import ndimage\n\t# from get_weak_anns import find_bbox, ScribblesRobot\n\timport torch\n\tfrom torch import nn\n", "import torch.backends.cudnn as cudnn\n\timport torch.nn.init as initer\n\tSpecial_characters = [\n\t    ['▁▂▃▄▅▆▇█', '█▇▆▅▄▃▂▁'],\n\t    ['( * ^ _ ^ * )', '( * ^ _ ^ * )'],\n\t    ['( $ _ $ )', '( $ _ $ )'],\n\t    ['(？ ‧ _ ‧ )', '( ‧ _ ‧ ？)'],\n\t    ['( T___T )', '( T___T )'],\n\t    [' ⌒ _ ⌒ ☆ ', ' ☆ ⌒ _ ⌒ '],\n\t    ['( = ^ o ^ = )', '( = ^ o ^ = )'],\n", "    [' ㊣ ㊣ ㊣ ', ' ㊣ ㊣ ㊣ '],\n\t    ['.¸.·´¯`·', '.¸.·´¯`·'],\n\t    ['( ¯ □ ¯ )', '( ¯ □ ¯ )'],\n\t    ['( ⊙ o ⊙ )', '( ⊙ o ⊙ )'],\n\t    [' ◕ ‿ ◕ ｡ ', ' ｡ ◕ ‿ ◕ '],\n\t    ['( ◡ ‿ ◡ ✿)', '(✿ ◡ ‿ ◡ )']\n\t]\n\tclass AverageMeter(object):\n\t    \"\"\"Computes and stores the average and current value\"\"\"\n\t    def __init__(self):\n", "        self.reset()\n\t    def reset(self):\n\t        self.val = 0\n\t        self.avg = 0\n\t        self.sum = 0\n\t        self.count = 0\n\t    def update(self, val, n=1):\n\t        self.val = val\n\t        self.sum += val * n\n\t        self.count += n\n", "        self.avg = self.sum / self.count\n\tdef lr_decay(optimizer, base_lr, curr_iter, max_iter, decay_dict, current_characters=None):\n\t    if decay_dict['type'] == 'poly_learning_rate':\n\t        scale_lr=10.\n\t        lr = base_lr * (1 - float(curr_iter) / max_iter) ** decay_dict['power']\n\t        for index, param_group in enumerate(optimizer.param_groups):\n\t            if index <= decay_dict['index_split']:\n\t                param_group['lr'] = lr\n\t            else:\n\t                param_group['lr'] = lr * scale_lr\n", "    elif decay_dict['type'] == 'adjust_learning_rate_poly':\n\t        reduce = ((1-float(curr_iter)/max_iter)**(decay_dict['power']))\n\t        lr = base_lr * reduce\n\t        optimizer.param_groups[0]['lr'] = lr * 1\n\t        optimizer.param_groups[1]['lr'] = lr * 2\n\t        optimizer.param_groups[2]['lr'] = lr * 10\n\t        optimizer.param_groups[3]['lr'] = lr * 20\n\t    elif decay_dict['type'] == 'half_learning_rate':\n\t        scale_lr=10.\n\t        lr = base_lr * (1 - float(decay_dict['rate']*curr_iter) / (max_iter)) ** decay_dict['power']\n", "        for index, param_group in enumerate(optimizer.param_groups):\n\t            if index <= decay_dict['index_split']:\n\t                param_group['lr'] = lr\n\t            else:\n\t                param_group['lr'] = lr * scale_lr\n\t    elif decay_dict['type'] == 'split_learning_rate_poly':\n\t        reduce = ((1-float(curr_iter)/max_iter)**(decay_dict['power']))\n\t        lr = base_lr * reduce\n\t        for i in range(len(optimizer.param_groups)):\n\t            optimizer.param_groups[i]['lr'] = lr * decay_dict['scale_lr'][i]\n", "    elif decay_dict['type'] == 'step_learning_rate_poly':\n\t        # reduce = ((1-float(curr_iter)/max_iter)**(decay_dict['power']))\n\t        # tmp_lr =base_lr\n\t        for i in len(decay_dict['step']):\n\t            if float(curr_iter)/max_iter > decay_dict['step'][i]:\n\t                tmp_lr = base_lr*decay_dict['step_rate'][i]\n\t        # lr = tmp_lr * reduce\n\t        for param_group in optimizer.param_groups:\n\t            param_group['lr'] = tmp_lr\n\t    elif decay_dict['type'] == 'adam':\n", "        lr = base_lr\n\t        if curr_iter % 50 == 0:   \n\t            print(' Using Adam optim')\n\t    if curr_iter % 50 == 0:   \n\t        print(' '*len(current_characters[0])*3 +' '*10  + 'Base LR: {:.8f}, Curr LR: {:.8f}'.format(base_lr, lr) )\n\tdef step_learning_rate(optimizer, base_lr, epoch, step_epoch, multiplier=0.1):\n\t    \"\"\"Sets the learning rate to the base LR decayed by 10 every step epochs\"\"\"\n\t    lr = base_lr * (multiplier ** (epoch // step_epoch))\n\t    for param_group in optimizer.param_groups:\n\t        param_group['lr'] = lr\n", "def poly_learning_rate(optimizer, base_lr, curr_iter, max_iter, power=0.9, index_split=-1, scale_lr=10., warmup=False, warmup_step=500):\n\t    \"\"\"poly learning rate policy\"\"\"\n\t    if warmup and curr_iter < warmup_step:\n\t        lr = base_lr * (0.1 + 0.9 * (curr_iter/warmup_step))\n\t    else:\n\t        lr = base_lr * (1 - float(curr_iter) / max_iter) ** power\n\t    if curr_iter % 50 == 0:   \n\t        print('Base LR: {:.4f}, Curr LR: {:.4f}, Warmup: {}.'.format(base_lr, lr, (warmup and curr_iter < warmup_step)))     \n\t    for index, param_group in enumerate(optimizer.param_groups):\n\t        if index <= index_split:\n", "            param_group['lr'] = lr\n\t        else:\n\t            param_group['lr'] = lr * scale_lr\n\tdef adjust_learning_rate_poly(optimizer, base_lr, curr_iter, max_iter,  power=0.9):\n\t    # base_lr = 3.5e-4\n\t    # max_iter = args.max_steps\n\t    reduce = ((1-float(curr_iter)/max_iter)**(power))\n\t    lr = base_lr * reduce\n\t    optimizer.param_groups[0]['lr'] = lr * 1\n\t    optimizer.param_groups[1]['lr'] = lr * 2\n", "    optimizer.param_groups[2]['lr'] = lr * 10\n\t    optimizer.param_groups[3]['lr'] = lr * 20\n\t    if curr_iter % 50 == 0:   \n\t        print('Base LR: {:.8f}, Curr LR: {:.8f}, '.format(base_lr, lr)) \n\tdef intersectionAndUnion(output, target, K, ignore_index=255):\n\t    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n\t    assert (output.ndim in [1, 2, 3])\n\t    assert output.shape == target.shape\n\t    output = output.reshape(output.size).copy()\n\t    target = target.reshape(target.size)\n", "    output[np.where(target == ignore_index)[0]] = ignore_index\n\t    intersection = output[np.where(output == target)[0]]\n\t    area_intersection, _ = np.histogram(intersection, bins=np.arange(K+1))\n\t    area_output, _ = np.histogram(output, bins=np.arange(K+1))\n\t    area_target, _ = np.histogram(target, bins=np.arange(K+1))\n\t    area_union = area_output + area_target - area_intersection\n\t    return area_intersection, area_union, area_target\n\tdef intersectionAndUnionGPU(output, target, K, ignore_index=255):\n\t    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n\t    assert (output.dim() in [1, 2, 3])\n", "    assert output.shape == target.shape\n\t    output = output.view(-1)\n\t    # target = target.view(-1)\n\t    target = target.reshape(-1)\n\t    output[target == ignore_index] = ignore_index\n\t    intersection = output[output == target]\n\t    area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)\n\t    area_output = torch.histc(output, bins=K, min=0, max=K-1)\n\t    area_target = torch.histc(target, bins=K, min=0, max=K-1)\n\t    area_union = area_output + area_target - area_intersection\n", "    return area_intersection, area_union, area_target\n\tdef check_mkdir(dir_name):\n\t    if not os.path.exists(dir_name):\n\t        os.mkdir(dir_name)\n\tdef check_makedirs(dir_name):\n\t    if not os.path.exists(dir_name):\n\t        os.makedirs(dir_name)\n\tdef del_file(path):\n\t    for i in os.listdir(path):\n\t        path_file = os.path.join(path,i)\n", "        if os.path.isfile(path_file):\n\t            os.remove(path_file)\n\t        else:\n\t            del_file(path_file)\n\tdef init_weights(model, conv='kaiming', batchnorm='normal', linear='kaiming', lstm='kaiming'):\n\t    \"\"\"\n\t    :param model: Pytorch Model which is nn.Module\n\t    :param conv:  'kaiming' or 'xavier'\n\t    :param batchnorm: 'normal' or 'constant'\n\t    :param linear: 'kaiming' or 'xavier'\n", "    :param lstm: 'kaiming' or 'xavier'\n\t    \"\"\"\n\t    for m in model.modules():\n\t        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n\t            if conv == 'kaiming':\n\t                initer.kaiming_normal_(m.weight)\n\t            elif conv == 'xavier':\n\t                initer.xavier_normal_(m.weight)\n\t            else:\n\t                raise ValueError(\"init type of conv error.\\n\")\n", "            if m.bias is not None:\n\t                initer.constant_(m.bias, 0)\n\t        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):#, BatchNorm1d, BatchNorm2d, BatchNorm3d)):\n\t            if batchnorm == 'normal':\n\t                initer.normal_(m.weight, 1.0, 0.02)\n\t            elif batchnorm == 'constant':\n\t                initer.constant_(m.weight, 1.0)\n\t            else:\n\t                raise ValueError(\"init type of batchnorm error.\\n\")\n\t            initer.constant_(m.bias, 0.0)\n", "        elif isinstance(m, nn.Linear):\n\t            if linear == 'kaiming':\n\t                initer.kaiming_normal_(m.weight)\n\t            elif linear == 'xavier':\n\t                initer.xavier_normal_(m.weight)\n\t            else:\n\t                raise ValueError(\"init type of linear error.\\n\")\n\t            if m.bias is not None:\n\t                initer.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LSTM):\n", "            for name, param in m.named_parameters():\n\t                if 'weight' in name:\n\t                    if lstm == 'kaiming':\n\t                        initer.kaiming_normal_(param)\n\t                    elif lstm == 'xavier':\n\t                        initer.xavier_normal_(param)\n\t                    else:\n\t                        raise ValueError(\"init type of lstm error.\\n\")\n\t                elif 'bias' in name:\n\t                    initer.constant_(param, 0)\n", "def colorize(gray, palette):\n\t    # gray: numpy array of the label and 1*3N size list palette\n\t    color = Image.fromarray(gray.astype(np.uint8)).convert('P')\n\t    color.putpalette(palette)\n\t    return color\n\t# ------------------------------------------------------\n\tdef get_model_para_number(model):\n\t    total_number = 0\n\t    learnable_number = 0 \n\t    for para in model.parameters():\n", "        total_number += torch.numel(para)\n\t        if para.requires_grad == True:\n\t            learnable_number+= torch.numel(para)\n\t    return total_number, learnable_number\n\tdef setup_seed(seed=2021, deterministic=False):\n\t    if deterministic:\n\t        cudnn.benchmark = False\n\t        cudnn.deterministic = True\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    os.environ['PYTHONHASHSEED'] = str(seed)\n\tdef get_logger():\n\t    logger_name = \"main-logger\"\n\t    logger = logging.getLogger()\n\t    logger.setLevel(logging.INFO)\n\t    handler = logging.StreamHandler()\n\t    fmt = \"[%(asctime)s %(levelname)s %(filename)s line %(lineno)d %(process)d] %(message)s\"\n", "    handler.setFormatter(logging.Formatter(fmt))\n\t    logger.addHandler(handler)\n\t    return logger\n\tdef get_metirc(output, target, K, ignore_index=255):\n\t    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n\t    #交并比\n\t    assert (output.dim() in [1, 2, 3])\n\t    assert output.shape == target.shape\n\t    output = output.view(-1)\n\t    target = target.view(-1)\n", "    output[target == ignore_index] = ignore_index\n\t    intersection = output[output == target]\n\t    if intersection.shape[0] == 0:\n\t        area_intersection = torch.tensor([0.,0.],device='cuda')\n\t    else:\n\t        area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)\n\t    area_output = torch.histc(output, bins=K, min=0, max=K-1)\n\t    area_target = torch.histc(target, bins=K, min=0, max=K-1)\n\t    # area_union = area_output + area_target - area_intersection\n\t    Pre = area_intersection / (area_output + 1e-10)\n", "    Rec = area_intersection / (area_target + 1e-10)\n\t    return Pre, Rec\n\tdef get_save_path_1(args):\n\t    if len(args.variable1) != 0 :\n\t        variable1 = eval('args.{}'.format(args.variable1))\n\t        args[args.variable1] = eval(args.aux1)\n\t    else :\n\t        variable1 = 0\n\t    if len(args.variable2) != 0 :\n\t        variable2 = eval('args.{}'.format(args.variable2))\n", "        args[args.variable2] = eval(args.aux2)\n\t    else :\n\t        variable2 = 0\n\t    args.snapshot_path = 'exp/{}/{}/{}/split{}/{}shot/'.format( args.arch,  args.dataset, args.backbone, args.split, args.shot)\n\t    args.result_path = 'exp/{}/{}/{}/split{}/result/'.format( args.arch,  args.dataset, args.backbone, args.split)\n\t    if len(args.variable1) != 0 or len(args.variable2) != 0  :\n\t        args.snapshot_path = args.snapshot_path+'{}_{}/'.format(eval(args.aux1), eval(args.aux2))\n\tdef get_save_path(args):\n\t    if len(args.variable1) != 0 :\n\t        variable1 = eval('args.{}'.format(args.variable1))\n", "    else :\n\t        variable1 = 0\n\t    if len(args.variable2) != 0 :\n\t        variable2 = eval('args.{}'.format(args.variable2))\n\t    else :\n\t        variable2 = 0\n\t    args.snapshot_path = 'exp/{}/{}/{}/split{}/{}shot/'.format( args.arch,  args.dataset, args.backbone, args.split, args.shot)\n\t    args.result_path = 'exp/{}/{}/{}/split{}/result/'.format( args.arch,  args.dataset, args.backbone, args.split)\n\t    if len(args.variable1) != 0 or len(args.variable2) != 0  :\n\t        args.snapshot_path = args.snapshot_path+'{}_{}/'.format(variable1, variable2)\n", "def is_same_model(model1, model2):\n\t    flag = 0\n\t    count = 0\n\t    for k, v in model1.state_dict().items():\n\t        model1_val = v\n\t        model2_val = model2.state_dict()[k]\n\t        if (model1_val==model2_val).all():\n\t            pass\n\t        else:\n\t            flag+=1\n", "            print('value of key <{}> mismatch'.format(k))\n\t        count+=1\n\t    return True if flag==0 else False\n\tdef fix_bn(m):\n\t    classname = m.__class__.__name__\n\t    if classname.find('BatchNorm') != -1:\n\t        m.eval()\n\t    if classname.find('LayerNorm') != -1:\n\t        m.eval()\n\tdef freeze_modules(model, freeze_layer):\n", "    for item in freeze_layer:\n\t        if hasattr(model, item):\n\t            for param in eval('model.' + item).parameters():\n\t                param.requires_grad = False\n\t            print('model.{} has been frozen'.format(item))\n\t        else:\n\t            print('model has no part named {} , please check the input'.format(item))\n\tdef sum_list(list):\n\t    sum = 0\n\t    for item in list:\n", "        sum += item\n\t    return sum\n\tdef make_dataset(data_root=None, data_list=None, all_class=None, split_list=None, fliter_intersection=True):    \n\t    # if not os.path.isfile(data_list):\n\t    #     raise (RuntimeError(\"Image list file do not exist: \" + data_list + \"\\n\"))\n\t    # Shaban uses these lines to remove small objects:\n\t    # if util.change_coordinates(mask, 32.0, 0.0).sum() > 2:\n\t    #    flitered_item.append(item)\n\t    # which means the mask will be downsampled to 1/32 of the original size and the valid area should be larger than 2, \n\t    # therefore the area in original size should be accordingly larger than 2 * 32 * 32    \n", "    image_label_list_0 = []  \n\t    image_label_list_1 = [] \n\t    image_label_list_2 = [] \n\t    image_label_list_3 = [] \n\t    fin_list = []\n\t    list_read = open(data_list).readlines()\n\t    sub_class_file_list = {}\n\t    for sub_c in all_class:\n\t        sub_class_file_list[sub_c] = []\n\t    for l_idx in tqdm(range(len(list_read))):\n", "        line = list_read[l_idx]\n\t        line = line.strip()\n\t        line_split = line.split(' ')\n\t        image_name = os.path.join(data_root, line_split[0])\n\t        label_name = os.path.join(data_root, line_split[1])\n\t        item = (image_name, label_name)\n\t        label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n\t        label_class = np.unique(label).tolist()\n\t        if 0 in label_class:\n\t            label_class.remove(0)\n", "        if 255 in label_class:\n\t            label_class.remove(255)\n\t        # all_label_list = []\n\t        new_label_class_0 = []\n\t        new_label_class_1 = []\n\t        new_label_class_2 = []\n\t        new_label_class_3 = []\n\t        all_label_class = []\n\t        if fliter_intersection:\n\t            for c in label_class:\n", "                tmp_label = np.zeros_like(label)\n\t                target_pix = np.where(label == c)\n\t                tmp_label[target_pix[0],target_pix[1]] = 1 \n\t                if tmp_label.sum() >= 2 * 32 * 32:   \n\t                    all_label_class.append(c)\n\t                    for i in range(len(split_list)):\n\t                        sub_list = split_list[i]\n\t                        tmp_label_class = eval('new_label_class_{}'.format(i))\n\t                        if set(label_class).issubset(set(sub_list)):\n\t                            if c in sub_list:\n", "                                tmp_label_class.append(c)\n\t        else:\n\t            for c in label_class:\n\t                tmp_label = np.zeros_like(label)\n\t                target_pix = np.where(label == c)\n\t                tmp_label[target_pix[0],target_pix[1]] = 1 \n\t                if tmp_label.sum() >= 2 * 32 * 32:   \n\t                    all_label_class.append(c)\n\t                    for i in range(len(split_list)):\n\t                        sub_list = split_list[i]\n", "                        tmp_label_class = eval('new_label_class_{}'.format(i))\n\t                        if c in sub_list:\n\t                            tmp_label_class.append(c)    \n\t        for i in range(len(split_list)):\n\t            new_label_class = eval('new_label_class_{}'.format(i))\n\t            image_label_list = eval('image_label_list_{}'.format(i))\n\t            if len(new_label_class) > 0:\n\t                image_label_list.append(item)\n\t        for c in all_label_class:\n\t            sub_class_file_list[c].append(item)\n", "    for i in range(len(split_list)):\n\t        image_label_list = eval('image_label_list_{}'.format(i))\n\t        fin_list.append(image_label_list)  \n\t    # print(\"Checking image&label pair {} list done! \".format(split))\n\t    return fin_list, sub_class_file_list\n\tdef make_data_list(data_root=None, dataset=None, train_list=None, val_list=None, all_class=None, val_class=None, ):\n\t    list_root = './lists/{}/fss_list/'.format(dataset)\n\t    if not os.path.exists(list_root):\n\t        train_root = list_root + 'train/'\n\t        val_root = list_root + 'val/'\n", "        print('{} has been created!'.format(list_root))\n\t        os.makedirs(train_root)\n\t        os.makedirs(val_root)\n\t    data_root = data_root\n\t    train_class_list = []\n\t    for i in range(len(val_class)):\n\t        tmp_list = list(set(all_class) - set(val_class[i]))\n\t        train_class_list.append(tmp_list)\n\t    print('Processing train_split')\n\t    train_txt, train_dict = make_dataset(data_root=data_root, data_list=train_list, all_class=all_class, split_list=train_class_list, fliter_intersection=True)\n", "    with open (train_root + 'train_dict.txt', 'w') as f:\n\t        f.write(str(train_dict))\n\t    for i in range(len(train_txt)):\n\t        data_list = train_txt[i]\n\t        with open(train_root + 'train_split{}.txt'.format(i), 'w')as f:\n\t            for item in data_list:\n\t                img, label = item\n\t                f.write(img + ' ')\n\t                f.write(label + '\\n')\n\t    print('Processing val_split')\n", "    val_txt, val_dict = make_dataset(data_root=data_root, data_list=val_list, all_class=all_class, split_list=val_class, fliter_intersection=False)\n\t    with open (val_root + 'val_dict.txt', 'w') as f:\n\t        f.write(str(val_dict))\n\t    for i in range(len(val_txt)):\n\t        data_list = val_txt[i]\n\t        with open(val_root + 'val_split{}.txt'.format(i), 'w')as f:\n\t            for item in data_list:\n\t                img, label = item\n\t                f.write(img + ' ')\n\t                f.write(label + '\\n')\n", "    print('Processing val_base')\n\t    base_txt, _ = make_dataset(data_root=data_root, data_list=val_list, all_class=all_class, split_list=val_class, fliter_intersection=False)\n\t    for i in range(len(base_txt)):\n\t        data_list = base_txt[i]\n\t        with open(val_root + 'val_base{}.txt'.format(i), 'w')as f:\n\t            for item in data_list:\n\t                img, label = item\n\t                f.write(img + ' ')\n\t                f.write(label + '\\n')\n\tdef make_dict(data_root=None, data_list=None, all_class=None, dataset=None, mode=None):    \n", "    \"\"\"\n\t    data_list: all data in train/val\n\t    all_class: all class in dataset\n\t    return the dict that contains the data_list of each classes\n\t    \"\"\"\n\t    dict_name = './lists/{}/{}_dict.txt'.format(dataset, mode)\n\t    list_read = open(data_list).readlines()\n\t    sub_class_file_list = {}\n\t    for sub_c in all_class:\n\t        sub_class_file_list[sub_c] = []\n", "    print('Processing {} data' .format(mode))\n\t    for l_idx in tqdm(range(len(list_read))):\n\t        line = list_read[l_idx]\n\t        line = line.strip()\n\t        line_split = line.split(' ')\n\t        image_name = os.path.join(data_root, line_split[0])\n\t        label_name = os.path.join(data_root, line_split[1])\n\t        item = (image_name, label_name)\n\t        label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n\t        label_class = np.unique(label).tolist()\n", "        if 0 in label_class:\n\t            label_class.remove(0)\n\t        if 255 in label_class:\n\t            label_class.remove(255)\n\t        all_label_class = []\n\t        for c in label_class:\n\t            tmp_label = np.zeros_like(label)\n\t            target_pix = np.where(label == c)\n\t            tmp_label[target_pix[0],target_pix[1]] = 1 \n\t            if tmp_label.sum() >= 2 * 32 * 32:   \n", "                all_label_class.append(c)\n\t        for c in all_label_class:\n\t            sub_class_file_list[c].append(item)\n\t    with open (dict_name, 'w') as f:\n\t        f.write(str(sub_class_file_list))\n\tdef gen_list(class_dict_name, class_list, fliter):\n\t    with open(class_dict_name, 'r') as f:\n\t        f_str = f.read()\n\t    class_dict = eval(f_str)\n\t    Discard = set()\n", "    Adopt = set()\n\t    for id in class_dict:\n\t        if id in class_list:\n\t            Adopt = set(class_dict[id]) | Adopt\n\t        else:\n\t            Discard = set(class_dict[id]) | Discard\n\t    if fliter:\n\t        out_list = Adopt - Discard\n\t    else:\n\t        out_list = Adopt\n", "    return list(out_list), class_dict\n\t    # return sub_class_file_list\n"]}
