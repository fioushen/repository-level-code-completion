{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages\n\tfrom setuptools import setup\n\tsetup(name='darod',\n\t      version='1.0.0',\n\t      author='Colin Decourt',\n\t      license='Apache 2.0',\n\t      packages=find_packages())\n"]}
{"filename": "train.py", "chunked_list": ["import time\n\timport tensorflow as tf\n\tfrom darod.models import model\n\tfrom darod.trainers.trainer import Trainer\n\tfrom darod.utils import data_utils, bbox_utils, io_utils\n\tseed = 42\n\t# Set memory growth to avoid GPU to crash\n\tphysical_devices = tf.config.experimental.list_physical_devices('GPU')\n\ttf.config.experimental.set_memory_growth(physical_devices[0], True)\n\t# Load config from arguments\n", "args = io_utils.handle_args()\n\tconfig = io_utils.args2config(args)\n\tepochs = config[\"training\"]['epochs']\n\tbatch_size = config[\"training\"][\"batch_size\"]\n\t# Prepare dataset\n\tif config[\"data\"][\"dataset\"] == \"carrada\":\n\t    batched_train_dataset, dataset_info = data_utils.prepare_dataset(split=\"train\", config=config, seed=seed)\n\t    num_train_example = data_utils.get_total_item_size(dataset_info, \"train\")\n\t    #\n\t    batched_test_dataset, _ = data_utils.prepare_dataset(split=\"test\", config=config, seed=seed)\n", "    batched_val_dataset, _ = data_utils.prepare_dataset(split=\"val\", config=config, seed=seed)\n\telse:\n\t    batched_train_dataset, dataset_info = data_utils.prepare_dataset(split=\"train[:90%]\", config=config, seed=seed)\n\t    num_train_example = data_utils.get_total_item_size(dataset_info, \"train[:90%]\")\n\t    #\n\t    batched_val_dataset, _ = data_utils.prepare_dataset(split=\"train[90%:]\", config=config, seed=seed)\n\t    batched_test_dataset, _ = data_utils.prepare_dataset(split=\"test\", config=config, seed=seed)\n\tlabels = data_utils.get_labels(dataset_info)\n\tconfig[\"data\"][\"total_labels\"] = len(labels) + 1\n\tlabels = [\"bg\"] + labels\n", "config[\"training\"][\"num_steps_epoch\"] = num_train_example\n\tconfig[\"training\"][\"seed\"] = seed\n\t# Generate anchors\n\tanchors = bbox_utils.anchors_generation(config, train=True)\n\t# Load model\n\tfaster_rcnn_model = model.DAROD(config, anchors)\n\t# Build the model \n\t_ = faster_rcnn_model(tf.zeros([1, 256, 64, config[\"model\"][\"input_size\"][-1]]))\n\tfaster_rcnn_model.summary()\n\t# Train model \n", "trainer = Trainer(config=config, model=faster_rcnn_model, experiment_name=config[\"log\"][\"exp\"],\n\t                  backbone=config[\"model\"][\"backbone\"], labels=labels, backup_dir=args.backup_dir)\n\ttrainer.train(anchors, batched_train_dataset, batched_val_dataset)\n"]}
{"filename": "vizualize_testdata.py", "chunked_list": ["import json\n\timport os\n\timport matplotlib.pyplot as plt\n\timport tensorflow as tf\n\tfrom r2d2.models import model\n\tfrom r2d2.utils import io_utils, data_utils, bbox_utils, viz_utils\n\t# Set memory growth to avoid GPU to crash\n\tphysical_devices = tf.config.experimental.list_physical_devices('GPU')\n\ttf.config.experimental.set_memory_growth(physical_devices[0], True)\n\t# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n", "def main():\n\t    args = io_utils.handle_args_viz()\n\t    config_pth = os.path.join(args.path, \"config.json\")\n\t    with open(config_pth, 'r') as file:\n\t        config = json.load(file)\n\t    dataset = config[\"data\"][\"dataset\"]\n\t    labels = config[\"data\"][\"labels\"]\n\t    labels = [\"bg\"] + labels\n\t    seed = config[\"training\"][\"seed\"]\n\t    layout = config[\"model\"][\"layout\"]\n", "    target_id = args.seq_id\n\t    eval_best = args.eval_best if args.eval_best is not None else False\n\t    # Prepare dataset\n\t    if dataset == \"carrada\":\n\t        batched_test_dataset, dataset_info = data_utils.prepare_dataset(split=\"test\", config=config, seed=seed)\n\t    elif dataset == \"raddet\":\n\t        batched_test_dataset, dataset_info = data_utils.prepare_dataset(split=\"test\", config=config, seed=seed)\n\t    else:\n\t        raise NotImplementedError(\"This dataset doesn't exist.\")\n\t    anchors = bbox_utils.anchors_generation(config, train=False)\n", "    faster_rcnn_model = model.R2D2(config, anchors)\n\t    if layout == \"2D\":\n\t        faster_rcnn_model.build(input_shape=(None, 256, 64, 1))\n\t    else:\n\t        fake_input = tf.zeros(shape=(config[\"training\"][\"batch_size\"], config[\"model\"][\"sequence_len\"], 256, 64, 1))\n\t        _ = faster_rcnn_model(fake_input)\n\t    faster_rcnn_model.summary()\n\t    def restore_ckpt(config, log_pth):\n\t        optimizer = config[\"training\"][\"optimizer\"]\n\t        lr = config[\"training\"][\"lr\"]\n", "        use_scheduler = config[\"training\"][\"scheduler\"]\n\t        momentum = config[\"training\"][\"momentum\"]\n\t        if optimizer == \"SGD\":\n\t            optimizer = tf.optimizers.SGD(learning_rate=lr, momentum=momentum)\n\t        elif optimizer == \"adam\":\n\t            optimizer = tf.optimizers.Adam(learning_rate=lr)\n\t        elif optimizer == \"adad\":\n\t            optimizer = tf.optimizers.Adadelta(learning_rate=1.0)\n\t        elif optimizer == \"adag\":\n\t            optimizer = tf.optimizers.Adagrad(learning_rate=lr)\n", "        else:\n\t            raise NotImplemented(\"Not supported optimizer {}\".format(optimizer))\n\t        global_step = tf.Variable(1, trainable=False, dtype=tf.int64)\n\t        ckpt = tf.train.Checkpoint(optimizer=optimizer, model=faster_rcnn_model, step=global_step)\n\t        manager = tf.train.CheckpointManager(ckpt, log_pth, max_to_keep=5)\n\t        ckpt.restore(manager.latest_checkpoint)\n\t    # ------ Start evaluation with latest ckpt ------ #\n\t    print(\"Start creation of animation using the latest checkpoint.\")\n\t    if eval_best:\n\t        faster_rcnn_model.load_weights(os.path.join(args.path, \"best-model.h5\"))\n", "    else:\n\t        restore_ckpt(config, log_pth=args.path)\n\t    idx = 0\n\t    start_idx = 0\n\t    end_idx = 250\n\t    for data in batched_test_dataset:\n\t        spectrums, gt_boxes, gt_labels, is_same_seq, seq_id, _, images = data\n\t        if seq_id.numpy()[0] != target_id and dataset == \"carrada\":\n\t            continue\n\t        else:\n", "            valid_idxs = tf.where(is_same_seq == 1)\n\t            spectrums, gt_boxes, gt_labels = tf.gather_nd(spectrums, valid_idxs), tf.gather_nd(gt_boxes,\n\t                                                                                               valid_idxs), tf.gather_nd(\n\t                gt_labels, valid_idxs)\n\t            if spectrums.shape[0] != 0:\n\t                _, _, _, _, _, decoder_output = faster_rcnn_model(spectrums, training=False)\n\t                pred_boxes, pred_labels, pred_scores = decoder_output\n\t                if start_idx <= idx <= end_idx:\n\t                    if layout == \"2D\":\n\t                        fig = viz_utils.showCameraRD(camera_image=images[0], rd_spectrum=spectrums[0],\n", "                                                     boxes=pred_boxes[0], labels=pred_labels[0], scores=pred_scores[0],\n\t                                                     gt_boxes=None, gt_labels=None, dataset=dataset,\n\t                                                     class_names=config[\"data\"][\"labels\"])\n\t                    else:\n\t                        fig = viz_utils.showCameraRD(camera_image=images[0], rd_spectrum=spectrums[0][-1],\n\t                                                     boxes=pred_boxes[0], labels=pred_labels[0], scores=pred_scores[0],\n\t                                                     gt_boxes=None, gt_labels=None, dataset=dataset,\n\t                                                     class_names=config[\"data\"][\"labels\"])\n\t                    plt.savefig(\"./images/\" + \"pred_\" + str(idx) + \".png\", bbox_inches=\"tight\")\n\t                    plt.clf()\n", "                idx += 1\n\t                if idx >= end_idx:\n\t                    break\n\t    import imageio\n\t    png_dir = './images'\n\t    images = []\n\t    for file_name in sorted(os.listdir(png_dir)):\n\t        if file_name.endswith('.png'):\n\t            file_path = os.path.join(png_dir, file_name)\n\t            images.append(imageio.imread(file_path))\n", "            os.remove(file_path)\n\t    imageio.mimsave('./images/predictions_' + str(target_id) + '.gif', images, fps=4)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "eval.py", "chunked_list": ["import json\n\timport os\n\timport tensorflow as tf\n\tfrom tqdm import tqdm\n\tfrom darod.metrics import mAP\n\tfrom darod.models import model\n\tfrom darod.utils import io_utils, data_utils, bbox_utils\n\t# Set memory growth to avoid GPU to crash\n\tphysical_devices = tf.config.experimental.list_physical_devices('GPU')\n\ttf.config.experimental.set_memory_growth(physical_devices[0], True)\n", "def main():\n\t    args = io_utils.handle_args_eval()\n\t    # summary_writer = tf.summary.create_file_writer(args.path)\n\t    config_pth = os.path.join(args.path, \"config.json\")\n\t    with open(config_pth, 'r') as file:\n\t        config = json.load(file)\n\t    dataset = config[\"data\"][\"dataset\"]\n\t    labels = config[\"data\"][\"labels\"]\n\t    labels = [\"bg\"] + labels\n\t    seed = config[\"training\"][\"seed\"]\n", "    # Prepare dataset\n\t    if dataset == \"carrada\":\n\t        batched_test_dataset, dataset_info = data_utils.prepare_dataset(split=\"test\", config=config, seed=seed)\n\t    elif dataset == \"raddet\":\n\t        batched_test_dataset, dataset_info = data_utils.prepare_dataset(split=\"test\", config=config, seed=seed)\n\t    else:\n\t        raise NotImplementedError(\"This dataset doesn't exist.\")\n\t    len_test_dataset = data_utils.get_total_item_size(dataset_info, \"test\")\n\t    anchors = bbox_utils.anchors_generation(config, train=False)\n\t    faster_rcnn_model = model.DAROD(config, anchors)\n", "    faster_rcnn_model.build(input_shape=(None, 256, 64, config[\"model\"][\"input_size\"][-1]))\n\t    faster_rcnn_model.summary()\n\t    def restore_ckpt(config, log_pth):\n\t        optimizer = config[\"training\"][\"optimizer\"]\n\t        lr = config[\"training\"][\"lr\"]\n\t        use_scheduler = config[\"training\"][\"scheduler\"]\n\t        momentum = config[\"training\"][\"momentum\"]\n\t        if optimizer == \"SGD\":\n\t            optimizer = tf.optimizers.SGD(learning_rate=lr, momentum=momentum)\n\t        elif optimizer == \"adam\":\n", "            optimizer = tf.optimizers.Adam(learning_rate=lr)\n\t        elif optimizer == \"adad\":\n\t            optimizer = tf.optimizers.Adadelta(learning_rate=1.0)\n\t        elif optimizer == \"adag\":\n\t            optimizer = tf.optimizers.Adagrad(learning_rate=lr)\n\t        else:\n\t            raise NotImplemented(\"Not supported optimizer {}\".format(optimizer))\n\t        global_step = tf.Variable(1, trainable=False, dtype=tf.int64)\n\t        ckpt = tf.train.Checkpoint(optimizer=optimizer, model=faster_rcnn_model, step=global_step)\n\t        manager = tf.train.CheckpointManager(ckpt, log_pth, max_to_keep=5)\n", "        ckpt.restore(manager.latest_checkpoint)\n\t    def test_step(batched_test_dataset, map_iou_threshold_list=None, num_classes=3):\n\t        \"\"\"\n\t        Test step for model evaluation.\n\t        :param batched_test_dataset: test dataset\n\t        :param map_iou_threshold_list: list of threshold to test over the predictions\n\t        :return: dictionary with AP per class at different thresholds\n\t        \"\"\"\n\t        if map_iou_threshold_list is None:\n\t            map_iou_threshold_list = [0.1, 0.3, 0.5, 0.7]\n", "        pbar = tqdm(total=int(len_test_dataset))\n\t        tp_dict = mAP.init_tp_dict(num_classes, iou_threshold=map_iou_threshold_list)\n\t        import time\n\t        total_time = 0\n\t        for input_data in batched_test_dataset:\n\t            spectrums, gt_boxes, gt_labels, is_same_seq, _, _, images = input_data\n\t            # Take valid data (frames from the same record in a window)\n\t            valid_idxs = tf.where(is_same_seq == 1)\n\t            spectrums, gt_boxes, gt_labels = tf.gather_nd(spectrums, valid_idxs), tf.gather_nd(gt_boxes,\n\t                                                                                               valid_idxs), tf.gather_nd(\n", "                gt_labels, valid_idxs)\n\t            if spectrums.shape[0] != 0:\n\t                start = time.time()\n\t                _, _, _, _, _, decoder_output = faster_rcnn_model(spectrums, training=False)\n\t                end = time.time() - start\n\t                total_time += end\n\t                pred_boxes, pred_labels, pred_scores = decoder_output\n\t                pred_labels = pred_labels - 1\n\t                gt_labels = gt_labels - 1\n\t                for batch_id in range(gt_labels.shape[0]):\n", "                    tp_dict = mAP.accumulate_tp_fp(pred_boxes.numpy()[batch_id], pred_labels.numpy()[batch_id],\n\t                                                   pred_scores.numpy()[batch_id], gt_boxes.numpy()[batch_id],\n\t                                                   gt_labels.numpy()[batch_id], tp_dict,\n\t                                                   iou_thresholds=map_iou_threshold_list)\n\t            pbar.update(1)\n\t        print(\"Inference time on GPU: {} s\".format(total_time / 1391))\n\t        ap_dict = mAP.AP(tp_dict, num_classes, iou_th=map_iou_threshold_list)\n\t        return ap_dict\n\t    # ------ Start evaluation with latest ckpt ------ #\n\t    print(\"Start evaluation of the model with latest checkpoint.\")\n", "    restore_ckpt(config, log_pth=args.path)\n\t    out_ap = test_step(batched_test_dataset, num_classes=len(labels) - 1)\n\t    mAP.write_ap(out_ap, args.path)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "datasets/carrada_builder/__init__.py", "chunked_list": ["\"\"\"carrada dataset.\"\"\"\n"]}
{"filename": "datasets/carrada_builder/carrada_test.py", "chunked_list": ["\"\"\"carrada dataset.\"\"\"\n\timport tensorflow_datasets.public_api as tfds\n\tfrom tensorflow_datasets.carrada import carrada\n\tclass CarradaTest(tfds.testing.DatasetBuilderTestCase):\n\t    \"\"\"Tests for carrada dataset.\"\"\"\n\t    # TODO(carrada):\n\t    DATASET_CLASS = carrada.Carrada\n\t    SPLITS = {\n\t        'train': 3,  # Number of fake train example\n\t        'test': 1,  # Number of fake test example\n", "    }\n\t    # If you are calling `download/download_and_extract` with a dict, like:\n\t    #   dl_manager.download({'some_key': 'http://a.org/out.txt', ...})\n\t    # then the tests needs to provide the fake output paths relative to the\n\t    # fake data directory\n\t    # DL_EXTRACT_RESULT = {'some_key': 'output_file1.txt', ...}\n\tif __name__ == '__main__':\n\t    tfds.testing.test_main()\n"]}
{"filename": "datasets/carrada_builder/carrada.py", "chunked_list": ["\"\"\"carrada dataset.\"\"\"\n\timport json\n\timport os\n\timport numpy as np\n\timport tensorflow as tf\n\timport tensorflow_datasets.public_api as tfds\n\t_DESCRIPTION = \"\"\"\n\t# CARRADA dataset \n\tThe CARRADA dataset contains camera images, raw radar data and generated annotations for scene understanding\n\tin autonomous driving. \n", "*Full dataset can be found here: https://github.com/valeoai/carrada_dataset*\n\t\"\"\"\n\t_CITATION = \"\"\"\n\t@misc{ouaknine2020carrada,\n\t    title={CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations},\n\t    author={A. Ouaknine and A. Newson and J. Rebut and F. Tupin and P. Pérez},\n\t    year={2020},\n\t    eprint={2005.01456},\n\t    archivePrefix={arXiv},\n\t    primaryClass={cs.CV}\n", "}\n\t\"\"\"\n\t# @dataclasses.dataclass\n\t# class CarradaConfig(tfds.core.BuilderConfig):\n\t#   sequence_length = 1\n\tclass Carrada(tfds.core.GeneratorBasedBuilder):\n\t    \"\"\"DatasetBuilder for carrada dataset.\"\"\"\n\t    VERSION = tfds.core.Version('1.0.0')\n\t    RELEASE_NOTES = {\n\t        '1.0.0': 'Initial release.',\n", "    }\n\t    # BUILDER_CONFIG = [\n\t    #     # `name` (and optionally `description`) are required for each config\n\t    #     CarradaConfig(name='sl_1', description='Sequence length of 1', sequence_length=1),\n\t    #     CarradaConfig(name='sl_2', description='Sequence length of 2', sequence_length=2),\n\t    #     CarradaConfig(name='sl_3', description='Sequence length of 3', sequence_length=3),\n\t    #     CarradaConfig(name='sl_4', description='Sequence length of 4', sequence_length=4),\n\t    #     CarradaConfig(name='sl_5', description='Sequence length of 5', sequence_length=5),\n\t    #     CarradaConfig(name='sl_6', description='Sequence length of 6', sequence_length=6),\n\t    #     CarradaConfig(name='sl_7', description='Sequence length of 7', sequence_length=7),\n", "    #     CarradaConfig(name='sl_8', description='Sequence length of 8', sequence_length=8),\n\t    #     CarradaConfig(name='sl_9', description='Sequence length of 9', sequence_length=9),\n\t    #     CarradaConfig(name='sl_10', description='Sequence length of 10', sequence_length=10),\n\t    # ]\n\t    def _info(self) -> tfds.core.DatasetInfo:\n\t        \"\"\"Returns the dataset metadata.\"\"\"\n\t        return tfds.core.DatasetInfo(\n\t            builder=self,\n\t            description=_DESCRIPTION,\n\t            features=tfds.features.FeaturesDict({\n", "                # No need to keep 'spectrum' and 'image' field. Serialize their filenames is\n\t                # enough to load it and allows to save disk space.\n\t                'spectrum': tfds.features.Tensor(shape=(256, 64), dtype=tf.float32),\n\t                'image': tfds.features.Image(shape=(None, None, 3)),\n\t                'spectrum/filename': tfds.features.Text(),\n\t                'image/filename': tfds.features.Text(),\n\t                'spectrum/id': tf.int64,\n\t                'sequence/id': tf.int64,\n\t                'objects': tfds.features.Sequence({\n\t                    'area': tf.int64,\n", "                    'bbox': tfds.features.BBoxFeature(),\n\t                    'id': tf.int64,\n\t                    'label': tfds.features.ClassLabel(names=[\"pedestrian\", \"bicyclist\", \"car\"])  # Keep 0 class for BG\n\t                }),\n\t            }),\n\t            # If there's a common (input, target) tuple from the\n\t            # features, specify them here. They'll be used if\n\t            # `as_supervised=True` in `builder.as_dataset`.\n\t            supervised_keys=('spectrum', 'objects'),  # Set to `None` to disable\n\t            homepage=\"\",\n", "            citation=_CITATION,\n\t            disable_shuffling=True,\n\t        )\n\t    def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n\t        \"\"\"Returns SplitGenerators.\"\"\"\n\t        # TODO(carrada): Downloads the data and defines the splits\n\t        path = \"/opt/dataset_ssd/radar1/Carrada/\"\n\t        # TODO(carrada): Returns the Dict[split names, Iterator[Key, Example]]\n\t        return {\n\t            'train': self._generate_examples(path, 'train'),\n", "            'test': self._generate_examples(path, 'test'),\n\t            'val': self._generate_examples(path, 'val'),\n\t        }\n\t    def _generate_examples(self, path, split, input_type=\"RD\"):\n\t        \"\"\"Yields examples.\"\"\"\n\t        exceptions = ['2019-09-16-12-52-12/camera_images/001015.jpg', '2019-09-16-12-52-12/camera_images/001016.jpg',\n\t                      '2019-09-16-13-14-29/camera_images/000456.jpg', '2020-02-28-12-12-16/camera_images/000489.jpg',\n\t                      '2020-02-28-12-12-16/camera_images/000490.jpg', '2020-02-28-12-12-16/camera_images/000491.jpg',\n\t                      '2020-02-28-13-06-53/camera_images/000225.jpg', '2020-02-28-13-06-53/camera_images/000226.jpg',\n\t                      '2020-02-28-13-10-51/camera_images/000247.jpg', '2020-02-28-13-10-51/camera_images/000248.jpg',\n", "                      '2020-02-28-13-10-51/camera_images/000249.jpg', '2020-02-28-13-13-43/camera_images/000216.jpg',\n\t                      '2020-02-28-13-13-43/camera_images/000217.jpg', '2020-02-28-13-15-36/camera_images/000169.jpg',\n\t                      '2020-02-28-12-13-54/camera_images/000659.jpg', '2020-02-28-12-13-54/camera_images/000660.jpg',\n\t                      '2020-02-28-13-07-38/camera_images/000279.jpg', '2020-02-28-13-07-38/camera_images/000280.jpg']\n\t        annotations_path = os.path.join(path, 'annotations_frame_oriented.json')\n\t        annotation, seq_ref = self._load_annotation(path, annotations_path)\n\t        spectrum_type = 'range_doppler' if input_type == \"RD\" else 'range_angle'\n\t        h, w = (256, 64) if input_type == \"RD\" else (256, 256)\n\t        #\n\t        # sequence_length = self.builder_config.sequence_length\n", "        if split == \"train\":\n\t            sequences = [seq for seq in seq_ref if seq_ref[seq]['split'] == \"Train\"]\n\t        elif split == \"val\":\n\t            sequences = [seq for seq in seq_ref if seq_ref[seq]['split'] == \"Validation\"]\n\t        elif split == \"test\":\n\t            sequences = [seq for seq in seq_ref if seq_ref[seq]['split'] == \"Test\"]\n\t        s_id = 0\n\t        a_id = 0\n\t        sequence_id = 0\n\t        for sequence in sequences:\n", "            sequence_id += 1\n\t            # Load all annotations for current sequence\n\t            current_annotations = annotation[sequence]\n\t            # Iterate over all frames in the current sequence\n\t            for frame in current_annotations:\n\t                objects = []\n\t                # Load all annotations fdor all instances in the frame\n\t                sequence_annot = current_annotations[frame]\n\t                # Define file_name for camera and spectrum data\n\t                spectrum_fn = os.path.join(sequence, spectrum_type + '_processed', frame.zfill(6) + '.npy')\n", "                image_fn = os.path.join(sequence, 'camera_images', frame.zfill(6) + '.jpg')\n\t                if image_fn not in exceptions:\n\t                    if len(current_annotations[frame]) != 0:\n\t                        instances_annot = current_annotations[frame]\n\t                        for instance in instances_annot:\n\t                            in_polygon = instances_annot[instance][spectrum_type]['dense']\n\t                            bbox, area = self._build_bbox(in_polygon, h, w)\n\t                            label = instances_annot[instance][spectrum_type]['label']\n\t                            objects.append({\n\t                                'bbox': bbox,\n", "                                'label': label - 1,\n\t                                'area': area,\n\t                                'id': a_id\n\t                            })\n\t                            a_id += 1\n\t                        example = {\n\t                            'spectrum': np.load(os.path.join(path, spectrum_fn)).astype(np.float32),\n\t                            'spectrum/filename': spectrum_fn,\n\t                            'sequence/id': sequence_id,\n\t                            'image': os.path.join(path, image_fn),\n", "                            'image/filename': image_fn,\n\t                            'spectrum/id': s_id,\n\t                            'objects': objects\n\t                        }\n\t                        s_id += 1\n\t                        yield s_id - 1, example\n\t    def _build_bbox(self, polygon, h, w):\n\t        x, y = [], []\n\t        for y_, x_ in polygon:\n\t            x.append(x_)\n", "            y.append(y_)\n\t        y1, x1, y2, x2 = min(y), min(x), max(y), max(x)\n\t        bbox = [y1, x1, y2, x2]\n\t        area = self._compute_area(bbox)\n\t        return tfds.features.BBox(\n\t            ymin=y1 / h,\n\t            xmin=x1 / w,\n\t            ymax=y2 / h,\n\t            xmax=x2 / w,\n\t        ), area\n", "    def _compute_area(self, box):\n\t        y1, x1, y2, x2 = box\n\t        return (x2 - x1) * (y2 - y1)\n\t    def _load_annotation(self, path, annotation_path):\n\t        \"\"\"\n\t        Load annotations file and sequence distribution file\n\t        \"\"\"\n\t        with open(annotation_path) as annotations:\n\t            annotations = json.load(annotations)\n\t        with open(os.path.join(path, \"data_seq_ref.json\")) as seq_ref:\n", "            seq_ref = json.load(seq_ref)\n\t        return annotations, seq_ref\n"]}
{"filename": "datasets/raddet_builder/raddet_test.py", "chunked_list": ["\"\"\"raddet dataset.\"\"\"\n\timport tensorflow_datasets.public_api as tfds\n\tfrom tensorflow_datasets.raddet_builder.raddet import raddet\n\tclass RaddetTest(tfds.testing.DatasetBuilderTestCase):\n\t    \"\"\"Tests for raddet dataset.\"\"\"\n\t    # TODO(raddet):\n\t    DATASET_CLASS = raddet.Raddet\n\t    SPLITS = {\n\t        'train': 3,  # Number of fake train example\n\t        'test': 1,  # Number of fake test example\n", "    }\n\t    # If you are calling `download/download_and_extract` with a dict, like:\n\t    #   dl_manager.download({'some_key': 'http://a.org/out.txt', ...})\n\t    # then the tests needs to provide the fake output paths relative to the\n\t    # fake data directory\n\t    # DL_EXTRACT_RESULT = {'some_key': 'output_file1.txt', ...}\n\tif __name__ == '__main__':\n\t    tfds.testing.test_main()\n"]}
{"filename": "datasets/raddet_builder/__init__.py", "chunked_list": ["\"\"\"raddet dataset.\"\"\"\n"]}
{"filename": "datasets/raddet_builder/raddet.py", "chunked_list": ["\"\"\"raddet dataset.\"\"\"\n\timport numpy as np\n\timport tensorflow as tf\n\timport tensorflow_datasets.public_api as tfds\n\tfrom utils import loader, helper\n\t# TODO(raddet): Markdown description  that will appear on the catalog page.\n\t_DESCRIPTION = \"\"\"\n\tDescription is **formatted** as markdown.\n\tIt should also contain any processing which has been applied (if any),\n\t(e.g. corrupted example skipped, images cropped,...):\n", "\"\"\"\n\t# TODO(raddet): BibTeX citation\n\t_CITATION = \"\"\"\n\t\"\"\"\n\tclass Raddet(tfds.core.GeneratorBasedBuilder):\n\t    \"\"\"DatasetBuilder for raddet dataset.\"\"\"\n\t    VERSION = tfds.core.Version('1.1.0')\n\t    RELEASE_NOTES = {\n\t        '1.0.0': 'Initial release.',\n\t    }\n", "    def _info(self) -> tfds.core.DatasetInfo:\n\t        \"\"\"Returns the dataset metadata.\"\"\"\n\t        return tfds.core.DatasetInfo(\n\t            builder=self,\n\t            description=_DESCRIPTION,\n\t            features=tfds.features.FeaturesDict({\n\t                # No need to keep 'spectrum' and 'image' field. Serialize their filenames is\n\t                # enough to load it and allows to save disk space. \n\t                'spectrum': tfds.features.Tensor(shape=(256, 64), dtype=tf.float32),\n\t                'image': tfds.features.Image(shape=(None, None, 3)),\n", "                'spectrum/filename': tfds.features.Text(),\n\t                'spectrum/id': tf.int64,\n\t                'sequence/id': tf.int64,\n\t                'objects': tfds.features.Sequence({\n\t                    'area': tf.int64,\n\t                    'bbox': tfds.features.BBoxFeature(),\n\t                    'id': tf.int64,\n\t                    'label': tfds.features.ClassLabel(names=[\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"])\n\t                    # Keep 0 class for BG\n\t                }),\n", "            }),\n\t            # If there's a common (input, target) tuple from the\n\t            # features, specify them here. They'll be used if\n\t            # `as_supervised=True` in `builder.as_dataset`.\n\t            supervised_keys=('spectrum', 'objects'),  # Set to `None` to disable\n\t            homepage=\"\",\n\t            citation=_CITATION,\n\t            disable_shuffling=True,\n\t        )\n\t    def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n", "        \"\"\"Returns SplitGenerators.\"\"\"\n\t        # TODO(carrada): Downloads the data and defines the splits\n\t        train_path = \"/opt/stomach/radar1/RADDet dataset/train/\"\n\t        test_path = \"/opt/stomach/radar1/RADDet dataset/test/\"\n\t        # TODO(carrada): Returns the Dict[split names, Iterator[Key, Example]]\n\t        return {\n\t            'train': self._generate_examples(train_path, 'train'),\n\t            'test': self._generate_examples(test_path, 'test'),\n\t        }\n\t    def _generate_examples(self, path, input_type=\"RD\"):\n", "        classes_list = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"]\n\t        if path.split('/')[-2] == \"train\":\n\t            RAD_sequences = loader.readSequences(path)\n\t        else:\n\t            RAD_sequences = sorted(loader.readSequences(path))\n\t        count = 0\n\t        a_id = 0\n\t        s_id = 0\n\t        global_mean_log = 3.2438383\n\t        global_variance_log = 6.8367246\n", "        global_max_log = 10.0805629\n\t        global_min_log = 0.0\n\t        while count < len(RAD_sequences):\n\t            objects = []\n\t            RAD_filename = RAD_sequences[count]\n\t            RAD_complex = loader.readRAD(RAD_filename)\n\t            if RAD_complex is None:\n\t                raise ValueError(\"RAD file not found, please double check the path.\")\n\t            RAD_data = helper.complexTo2channels(RAD_complex)\n\t            # Normalize data\n", "            RAD_data = (RAD_data - global_mean_log) / global_variance_log\n\t            # RAD_data = (RAD_data - global_min_log) / (global_max_log - global_min_log)\n\t            # Load GT instances\n\t            gt_filename = loader.gtfileFromRADfile(RAD_filename, path)\n\t            gt_instances = loader.readRadarInstances(gt_filename)\n\t            if gt_instances is None:\n\t                raise ValueError(\"gt file not found, please double check the path\")\n\t            # Get RD spectrum\n\t            RD_data = helper.getSumDim(RAD_data, target_axis=1)\n\t            # Get RD bboxes\n", "            bboxes, classes = helper.readAndEncodeGtRD(gt_instances, RD_data.shape)\n\t            seq_id = RAD_filename.split('/')[-2].split('_')[-1]\n\t            for (box, class_) in zip(bboxes, classes):\n\t                bbox, area = helper.buildTfdsBoxes(box)\n\t                objects.append({\n\t                    'bbox': bbox,\n\t                    'label': classes_list.index(class_),\n\t                    'area': area,\n\t                    'id': a_id\n\t                })\n", "                a_id += 1\n\t            image_filename = loader.imgfileFromRADfile(RAD_filename, path)\n\t            example = {\n\t                'spectrum': RD_data.astype(np.float32),\n\t                'spectrum/filename': RAD_filename,\n\t                'sequence/id': int(seq_id),\n\t                'image': image_filename,\n\t                'spectrum/id': count,\n\t                'objects': objects\n\t            }\n", "            count += 1\n\t            yield count, example\n"]}
{"filename": "datasets/raddet_builder/utils/loader.py", "chunked_list": ["import glob\n\timport os\n\timport pickle\n\timport numpy as np\n\tdef readSequences(path):\n\t    \"\"\" Read sequences from PROJECT_ROOT/sequences.txt \"\"\"\n\t    sequences = glob.glob(os.path.join(path, \\\n\t                                       \"RAD/*/*.npy\"))\n\t    if len(sequences) == 0:\n\t        raise ValueError(\"Cannot read sequences.txt. \\\n", "                    Please check if the file is organized properly.\")\n\t    return sequences\n\tdef readRAD(filename):\n\t    if os.path.exists(filename):\n\t        return np.load(filename)\n\t    else:\n\t        return None\n\tdef gtfileFromRADfile(RAD_file, prefix):\n\t    \"\"\" Transfer RAD filename to gt filename \"\"\"\n\t    RAD_file_spec = RAD_file.split(\"RAD\")[-1]\n", "    gt_file = os.path.join(prefix, \"gt\") + RAD_file_spec.replace(\"npy\", \"pickle\")\n\t    return gt_file\n\tdef imgfileFromRADfile(RAD_file, prefix):\n\t    \"\"\" Transfer RAD filename to gt filename \"\"\"\n\t    RAD_file_spec = RAD_file.split(\"RAD\")[-1]\n\t    gt_file = os.path.join(prefix, \"stereo_image\") + RAD_file_spec.replace(\"npy\", \"jpg\")\n\t    return gt_file\n\tdef readRadarInstances(pickle_file):\n\t    \"\"\" read output radar instances. \"\"\"\n\t    if os.path.exists(pickle_file):\n", "        with open(pickle_file, \"rb\") as f:\n\t            radar_instances = pickle.load(f)\n\t        if len(radar_instances['classes']) == 0:\n\t            radar_instances = None\n\t    else:\n\t        radar_instances = None\n\t    return radar_instances\n"]}
{"filename": "datasets/raddet_builder/utils/__init__.py", "chunked_list": []}
{"filename": "datasets/raddet_builder/utils/helper.py", "chunked_list": ["import numpy as np\n\timport tensorflow_datasets as tfds\n\tdef readAndEncodeGtRD(gt_instance, rd_shape):\n\t    \"\"\" read gt_instance and return bbox\n\t    and class for RD \"\"\"\n\t    x_shape, y_shape = rd_shape[1], rd_shape[0]\n\t    boxes = gt_instance[\"boxes\"]\n\t    classes = gt_instance[\"classes\"]\n\t    new_boxes = []\n\t    new_classes = []\n", "    for (box, class_) in zip(boxes, classes):\n\t        yc, xc, h, w = box[0], box[2], box[3], box[5]\n\t        y1, y2, x1, x2 = int(yc - h / 2), int(yc + h / 2), int(xc - w / 2), int(xc + w / 2)\n\t        if x1 < 0:\n\t            # Create 2 boxes\n\t            x1 += x_shape\n\t            box1 = [y1 / y_shape, x1 / x_shape, y2 / y_shape, x_shape / x_shape]\n\t            box2 = [y1 / y_shape, 0 / x_shape, y2 / y_shape, x2 / x_shape]\n\t            #\n\t            new_boxes.append(box1)\n", "            new_classes.append(class_)\n\t            #\n\t            new_boxes.append(box2)\n\t            new_classes.append(class_)\n\t        elif x2 >= x_shape:\n\t            x2 -= x_shape\n\t            box1 = [y1 / y_shape, x1 / x_shape, y2 / y_shape, x_shape / x_shape]\n\t            box2 = [y1 / y_shape, 0 / x_shape, y2 / y_shape, x2 / x_shape]\n\t            #\n\t            new_boxes.append(box1)\n", "            new_classes.append(class_)\n\t            #\n\t            new_boxes.append(box2)\n\t            new_classes.append(class_)\n\t        else:\n\t            new_boxes.append([y1 / y_shape, x1 / x_shape, y2 / y_shape, x2 / x_shape])\n\t            new_classes.append(class_)\n\t    return new_boxes, new_classes\n\tdef buildTfdsBoxes(box):\n\t    ymin, xmin, ymax, xmax = box\n", "    area = (xmax - xmin) * (ymax - ymin)\n\t    return tfds.features.BBox(\n\t        ymin=ymin,\n\t        xmin=xmin,\n\t        ymax=ymax,\n\t        xmax=xmax\n\t    ), area\n\tdef complexTo2channels(target_array):\n\t    \"\"\" transfer complex a + bi to [a, b]\"\"\"\n\t    assert target_array.dtype == np.complex64\n", "    ### NOTE: transfer complex to (magnitude) ###\n\t    output_array = getMagnitude(target_array)\n\t    output_array = getLog(output_array)\n\t    return output_array\n\tdef getMagnitude(target_array, power_order=2):\n\t    \"\"\" get magnitude out of complex number \"\"\"\n\t    target_array = np.abs(target_array)\n\t    target_array = pow(target_array, power_order)\n\t    return target_array\n\tdef getLog(target_array, scalar=1., log_10=True):\n", "    \"\"\" get Log values \"\"\"\n\t    if log_10:\n\t        return scalar * np.log10(target_array + 1.)\n\t    else:\n\t        return target_array\n\tdef getSumDim(target_array, target_axis):\n\t    \"\"\" sum up one dimension \"\"\"\n\t    output = np.sum(target_array, axis=target_axis)\n\t    return output\n"]}
{"filename": "darod/__init__.py", "chunked_list": []}
{"filename": "darod/utils/pretraining_utils.py", "chunked_list": ["import os\n\timport tensorflow as tf\n\tdef load_from_imagenet(model_source, source_path, model_dest):\n\t    \"\"\"\n\t    Load backbone weights pretrained on image net\n\t    :param model_source: the pre-trained classification model on ImageNet dataset\n\t    :param source_path: path to the weights of the source model\n\t    :param model_dest: the model to initialize with\n\t    :return: initialized model\n\t    \"\"\"\n", "    model_source.load_weights(source_path)\n\t    fake_input = tf.zeros((1, 256, 64, 3))\n\t    _ = model_source(fake_input)\n\t    for layer in model_dest.layers:\n\t        if \"rgg_block\" in layer.name:\n\t            # assert model_source.get_layer(layer.name).get_weights() != model_dest.get_layer(layer.name).get_weights()\n\t            print(\"Initialize layer {} of the backbone with ImageNet weights...\".format(layer.name))\n\t            pretrained_weigths = model_source.get_layer(layer.name).get_weights()\n\t            model_dest.get_layer(layer.name).set_weights(pretrained_weigths)\n\t            # assert model_source.get_layer(layer.name).get_weights() == model_dest.get_layer(layer.name).get_weights()\n", "            print(\"Layer {} initialized with ImageNet weights.\".format(layer.name))\n\t        else:\n\t            continue\n\t    print(\"Backbone successfully initialized!\")\n\t    return model_dest\n\tdef load_from_radar_ds(weights_path, model_dest):\n\t    \"\"\"\n\t    Load a model trained on a other radar dataset (except classification and regression heads\n\t    :param weights_path: path to the weights of the model\n\t    :param model_dest: the model to initialize with\n", "    :return: the intialized model\n\t    \"\"\"\n\t    # Copy RPN weights before initialization\n\t    weights_rpn = model_dest.get_layer(\"rpn_conv\").get_weights()\n\t    weights_cls = model_dest.get_layer(\"rpn_cls\").get_weights()\n\t    weights_reg = model_dest.get_layer(\"rpn_reg\").get_weights()\n\t    print(\"Try to initialiaze model using pretrained model on {} dataset...\".format(weights_path.split(\"/\")[-2]))\n\t    weights_path = os.path.join(weights_path, \"best-model.h5\")\n\t    model_dest.load_weights(weights_path, by_name=True, skip_mismatch=True)\n\t    model_dest.get_layer(\"rpn_conv\").set_weights(weights_rpn)\n", "    model_dest.get_layer(\"rpn_cls\").set_weights(weights_cls)\n\t    model_dest.get_layer(\"rpn_reg\").set_weights(weights_reg)\n\t    print(\"Succesfully initialiazed model using pretrained model on {} dataset!\".format(weights_path.split(\"/\")[-3]))\n\t    return model_dest\n"]}
{"filename": "darod/utils/data_utils.py", "chunked_list": ["import tensorflow as tf\n\timport tensorflow_datasets as tfds\n\tdef make_window_dataset(ds, window_size=5, shift=1, stride=1, viz=False):\n\t    \"\"\"\n\t    Create a window dataset of window size and flatten it to\n\t    return a window of data for sequential processing. Only for temporal processing.\n\t    :param ds: the dataset to window\n\t    :param window_size: the size of the window\n\t    :param shift: the shit between windows\n\t    :param stride: the stride between windows\n", "    :param viz: return camera images or not\n\t    :return: a new dataset with windowed data\n\t    \"\"\"\n\t    windows = ds.window(window_size, shift=shift, stride=stride)\n\t    def sub_to_batch(sub):\n\t        \"\"\"Return a batched zip dataset containing the current sequence_id,\n\t        window_size spectrums and the associated bounding boxes and labels\n\t        inputs:\n\t            sub: sub dataset to batch (or window)\n\t        output: a zip dataset with sequence id, spectrum, bboxes and labels\n", "        \"\"\"\n\t        if viz:\n\t            return tf.data.Dataset.zip((sub['sequence/id'].padded_batch(window_size, padded_shapes=[],\n\t                                                                        padding_values=tf.constant(-1, tf.int64),\n\t                                                                        drop_remainder=True),\n\t                                        sub['spectrum'].padded_batch(window_size, padded_shapes=[None, None],\n\t                                                                     padding_values=tf.constant(0, tf.float32),\n\t                                                                     drop_remainder=True),\n\t                                        sub['objects']['bbox'].padded_batch(window_size, padded_shapes=[None, 4],\n\t                                                                            padding_values=tf.constant(0, tf.float32),\n", "                                                                            drop_remainder=True),\n\t                                        sub['objects']['label'].padded_batch(window_size, padded_shapes=[None],\n\t                                                                             padding_values=tf.constant(-2, tf.int64),\n\t                                                                             drop_remainder=True),\n\t                                        sub[\"image\"].padded_batch(window_size, padded_shapes=[None, None, 3],\n\t                                                                  padding_values=tf.constant(0, tf.uint8),\n\t                                                                  drop_remainder=True),\n\t                                        sub['spectrum/filename'].padded_batch(window_size, padded_shapes=[],\n\t                                                                              padding_values=tf.constant(\"-1\",\n\t                                                                                                         tf.string),\n", "                                                                              drop_remainder=True)))\n\t        return tf.data.Dataset.zip((sub['sequence/id'].padded_batch(window_size, padded_shapes=[],\n\t                                                                    padding_values=tf.constant(-1, tf.int64),\n\t                                                                    drop_remainder=True),\n\t                                    sub['spectrum'].padded_batch(window_size, padded_shapes=[None, None],\n\t                                                                 padding_values=tf.constant(0, tf.float32),\n\t                                                                 drop_remainder=True),\n\t                                    sub['objects']['bbox'].padded_batch(window_size, padded_shapes=[None, 4],\n\t                                                                        padding_values=tf.constant(0, tf.float32),\n\t                                                                        drop_remainder=True),\n", "                                    sub['objects']['label'].padded_batch(window_size, padded_shapes=[None],\n\t                                                                         padding_values=tf.constant(-2, tf.int64),\n\t                                                                         drop_remainder=True),\n\t                                    sub['spectrum/filename'].padded_batch(window_size, padded_shapes=[],\n\t                                                                          padding_values=tf.constant(\"-1\", tf.string),\n\t                                                                          drop_remainder=True)))\n\t    windows = windows.flat_map(sub_to_batch)\n\t    return windows\n\tdef data_preprocessing(sequence_id, spectrums, gt_boxes, gt_labels, filename, config, train=True, viz=False,\n\t                       image=None):\n", "    \"\"\"\n\t    Preprocess the data before training i.e. normalize spectrums between 0 and 1, standardize the\n\t    data, augment it if necessary.\n\t    :param sequence_id: id of the current sequence\n\t    :param spectrums: serie of sequence_len spectrums\n\t    :param gt_boxes: serie of ground truth boxes\n\t    :param gt_labels: serie of ground truth labels\n\t    :param filename: name of the file to process\n\t    :param config: config file\n\t    :param train: transform data for training (True) or for test/val (False)\n", "    :param viz: return camera image if train is False and viz is True and only the last spectrum\n\t    :param image: return camera image if True\n\t    :return: spectrum, ground truth, labels, sequence id, filename, camera image (optional)\n\t    \"\"\"\n\t    apply_augmentation = config[\"training\"][\"use_aug\"]\n\t    layout = config[\"model\"][\"layout\"]\n\t    # Only take the last label of the window i.e. the spectrum we want to detect objects\n\t    gt_labels = tf.cast(gt_labels[-1] + 1, dtype=tf.int32)\n\t    gt_boxes = gt_boxes[-1]\n\t    if layout == \"2D\":\n", "        spectrums = spectrums[-1]\n\t    spectrums = tf.expand_dims(spectrums, -1)\n\t    if config[\"training\"][\"pretraining\"] == \"imagenet\":\n\t        mean = [0.485, 0.456, 0.406]\n\t        variance = [0.229, 0.224, 0.225]\n\t        spectrums = (spectrums - tf.reduce_min(spectrums, axis=[0, 1])) / (\n\t                    tf.reduce_max(spectrums, axis=[0, 1]) - tf.reduce_min(spectrums, axis=[0,\n\t                                                                                           1]))  # Normalize spectrums between 0 and 1\n\t        spectrums = (spectrums - mean) / variance  # Standardize spectrums\n\t    elif config[\"model\"][\"backbone\"] in [\"resnet\", \"efficientnet\", \"mobilenet\", \"vgg16\"]:\n", "        spectrums = (spectrums - config[\"data\"][\"data_mean\"]) / config[\"data\"][\"data_std\"]\n\t        spectrums = tf.image.grayscale_to_rgb(spectrums)\n\t    else:\n\t        # Standardize input\n\t        spectrums = (spectrums - config[\"data\"][\"data_mean\"]) / config[\"data\"][\"data_std\"]\n\t    if apply_augmentation and train:\n\t        random_val = get_random_bool()\n\t        if tf.greater_equal(random_val, 0) and tf.less(random_val, 0.25):\n\t            spectrums, gt_boxes = flip_horizontally(spectrums, gt_boxes)\n\t            print(\"Horizontal flipping\")\n", "        elif tf.greater_equal(random_val, 0.25) and tf.less(random_val, 0.5):\n\t            spectrums, gt_boxes = flip_vertically(spectrums, gt_boxes)\n\t            print(\"Vertical flipping\")\n\t    if viz and image is not None:\n\t        image = image[-1]\n\t    #\n\t    is_same_seq = tf.cast(sequence_id[0] == sequence_id[-1], tf.int64)\n\t    if viz:\n\t        return spectrums, gt_boxes, gt_labels, is_same_seq, sequence_id[-1], filename, image\n\t    return spectrums, gt_boxes, gt_labels, is_same_seq, sequence_id[-1], filename\n", "def transforms(spectrum, config):\n\t    \"\"\"\n\t    Standardize data for each frame of the sequence.\n\t    :param spectrum: input data to standardize\n\t    :param config: mean of the dataset\n\t    :return:  standardized spectrums\n\t    \"\"\"\n\t    spectrum = tf.expand_dims(spectrum, axis=-1)\n\t    spectrum = (spectrum - config[\"data\"][\"data_mean\"]) / config[\"data\"][\"data_std\"]\n\t    return spectrum\n", "def get_random_bool():\n\t    \"\"\"\n\t    Generating random boolean.\n\t    :return: random boolean 0d tensor\n\t    \"\"\"\n\t    return tf.random.uniform((), dtype=tf.float32)\n\tdef randomly_apply_operation(operation, img, gt_boxes):\n\t    \"\"\"\n\t    Randomly applying given method to image and ground truth boxes.\n\t    :param operation: callable method\n", "    :param img: (height, width, depth)\n\t    :param gt_boxes: (ground_truth_object_count, [y1, x1, y2, x2])\n\t    :return: modified_or_not_img = (final_height, final_width, depth)\n\t             modified_or_not_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n\t    \"\"\"\n\t    return tf.cond(\n\t        tf.greater(tf.random.uniform((), dtype=tf.float32), 0.5),\n\t        lambda: operation(img, gt_boxes),\n\t        lambda: (img, gt_boxes)\n\t    )\n", "def flip_horizontally(img, gt_boxes):\n\t    \"\"\"\n\t    Flip image horizontally and adjust the ground truth boxes.\n\t    :param img: (height, width, depth)\n\t    :param gt_boxes: (ground_truth_object_count, [y1, x1, y2, x2])\n\t    :return: modified_img = (height, width, depth)\n\t             modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n\t    \"\"\"\n\t    flipped_img = tf.image.flip_left_right(img)\n\t    flipped_gt_boxes = tf.stack([gt_boxes[..., 0],\n", "                                 1.0 - gt_boxes[..., 3],\n\t                                 gt_boxes[..., 2],\n\t                                 1.0 - gt_boxes[..., 1]], -1)\n\t    return flipped_img, flipped_gt_boxes\n\tdef flip_vertically(img, gt_boxes):\n\t    \"\"\"\n\t    Flip image horizontally and adjust the ground truth boxes.\n\t    :param img: (height, width, depth)\n\t    :param gt_boxes: (ground_truth_object_count, [y1, x1, y2, x2])\n\t    :return: modified_img = (height, width, depth)\n", "             modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n\t    \"\"\"\n\t    flipped_img = tf.image.flip_up_down(img)\n\t    flipped_gt_boxes = tf.stack([1.0 - gt_boxes[..., 2],\n\t                                 gt_boxes[..., 1],\n\t                                 1.0 - gt_boxes[..., 0],\n\t                                 gt_boxes[..., 3]], -1)\n\t    return flipped_img, flipped_gt_boxes\n\tdef get_dataset(name, version, split, data_dir):\n\t    \"\"\"\n", "    Get tensorflow dataset split and info.\n\t    :param name: name of the dataset, carrada/raddet\n\t    :param version:  version of the dataset\n\t    :param split: data split string, should be one of [\"train\", \"validation\", \"test\"]\n\t    :param data_dir: read/write path for tensorflow datasets\n\t    :return: dataset: tensorflow dataset split\n\t             info: tensorflow dataset info\n\t    \"\"\"\n\t    dataset, info = tfds.load(name=name + \":\" + version, data_dir=data_dir, split=split, as_supervised=False,\n\t                              with_info=True)\n", "    return dataset, info\n\tdef get_total_item_size(info, split):\n\t    \"\"\"\n\t    Get total item size for given split.\n\t    :param info: tensorflow dataset info\n\t    :param split:  data split string, should be one of [\"train\", \"test\"]\n\t    :return: total_item_size = number of total items\n\t    \"\"\"\n\t    return info.splits[split].num_examples\n\tdef get_labels(info):\n", "    \"\"\"\n\t    Get label names list.\n\t    :param info: tensorflow dataset info\n\t    :return:  labels = [labels list]\n\t    \"\"\"\n\t    return info.features[\"objects\"][\"label\"].names\n\tdef prepare_dataset(split, config, seed):\n\t    \"\"\"\n\t    Prepare dataset for training\n\t    :param split: train/test/val\n", "    :param config: configuration dictionary with training settings\n\t    :param seed: seed\n\t    :return: batched dataset and dataset info\n\t    \"\"\"\n\t    #\n\t    train = True if split == \"train\" or split == \"train[:90%]\" else False\n\t    viz = True if split == \"test\" else False\n\t    buffer_size = 4000 if config[\"data\"][\"dataset\"] == \"carrada\" else 10000\n\t    dataset, dataset_info = get_dataset(name=config[\"data\"][\"dataset\"], version=config[\"data\"][\"dataset_version\"],\n\t                                        split=split, data_dir=config[\"data\"][\"tfds_path\"])\n", "    dataset_w = make_window_dataset(dataset, window_size=config[\"model\"][\"sequence_len\"], viz=viz)\n\t    #\n\t    if viz:\n\t        dataset_w = dataset_w.map(\n\t            lambda seq_ids, spectrums, gt_boxes, gt_labels, image, filename: data_preprocessing(seq_ids, spectrums,\n\t                                                                                                gt_boxes, gt_labels,\n\t                                                                                                filename, config,\n\t                                                                                                train=train, viz=True,\n\t                                                                                                image=image))\n\t        padding_values = (\n", "        tf.constant(0, tf.float32), tf.constant(0, tf.float32), tf.constant(-1, tf.int32), tf.constant(-1, tf.int64),\n\t        tf.constant(-1, tf.int64), tf.constant(\"-1\", tf.string), tf.constant(0, tf.uint8))\n\t    #\n\t    else:\n\t        dataset_w = dataset_w.map(\n\t            lambda seq_ids, spectrums, gt_boxes, gt_labels, filename: data_preprocessing(seq_ids, spectrums, gt_boxes,\n\t                                                                                         gt_labels, filename, config,\n\t                                                                                         train=train))\n\t        padding_values = (\n\t        tf.constant(0, tf.float32), tf.constant(0, tf.float32), tf.constant(-1, tf.int32), tf.constant(-1, tf.int64),\n", "        tf.constant(-1, tf.int64), tf.constant(\"-1\", tf.string))\n\t    #\n\t    if train:\n\t        print(\"Shuffle the dataset\")\n\t        dataset_w = dataset_w.shuffle(buffer_size, seed=seed, reshuffle_each_iteration=True)\n\t    if config[\"model\"][\"layout\"] == \"2D\":\n\t        if viz:\n\t            padded_shapes = (\n\t            [None, None, config[\"model\"][\"input_size\"][-1]], [None, 4], [None], [], [], [None], [None, None, 3])\n\t        else:\n", "            padded_shapes = ([None, None, config[\"model\"][\"input_size\"][-1]], [None, 4], [None], [], [], [None])\n\t    else:\n\t        if viz:\n\t            padded_shapes = (\n\t            [None, None, None, config[\"model\"][\"input_size\"][-1]], [None, 4], [None], [], [], [None], [None, None, 3])\n\t        else:\n\t            padded_shapes = ([None, None, None, config[\"model\"][\"input_size\"][-1]], [None, 4], [None], [], [], [None])\n\t    if split == \"test\":\n\t        batched_dataset = dataset_w.padded_batch(1, padded_shapes=padded_shapes,\n\t                                                 padding_values=padding_values,\n", "                                                 drop_remainder=True)\n\t    else:\n\t        batched_dataset = dataset_w.padded_batch(config[\"training\"][\"batch_size\"], padded_shapes=padded_shapes,\n\t                                                 padding_values=padding_values,\n\t                                                 drop_remainder=True)\n\t    return batched_dataset, dataset_info\n"]}
{"filename": "darod/utils/io_utils.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom datetime import datetime\n\tdef get_log_path(model_type, backbone=\"vgg16\", custom_postfix=\"\"):\n\t    \"\"\"\n\t    Generating log path from model_type value for tensorboard.\n\t    :param model_type:  \"rpn\", \"faster_rcnn\"\n\t    :param backbone: backbone used\n\t    :param custom_postfix: any custom string for log folder name\n", "    :return: tensorboard log path, for example: \"logs/rpn_mobilenet_v2/{date}\"\n\t    \"\"\"\n\t    return \"logs/{}_{}{}/{}\".format(model_type, backbone, custom_postfix, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\tdef get_model_path(model_type, backbone=\"vgg16\"):\n\t    \"\"\"\n\t    Generating model path from model_type value for save/load model weights.\n\t    :param model_type: \"rpn\", \"faster_rcnn\"\n\t    :param backbone: backbone used\n\t    :return: os model path, for example: \"trained/rpn_vgg16_model_weights.h5\"\n\t    \"\"\"\n", "    main_path = \"trained\"\n\t    if not os.path.exists(main_path):\n\t        os.makedirs(main_path)\n\t    model_path = os.path.join(main_path, \"{}_{}_model_weights.h5\".format(model_type, backbone))\n\t    return model_path\n\tdef handle_args():\n\t    \"\"\"\n\t    Parse arguments from command line\n\t    :return: args\n\t    \"\"\"\n", "    parser = argparse.ArgumentParser(description=\"DAROD implementation\")\n\t    parser.add_argument(\"--config\", help=\"Path of the config file\")\n\t    parser.add_argument(\"--backup-dir\", help=\"Path to backup dir\")\n\t    parser.add_argument(\"--sequence-length\", default=None, type=int,\n\t                        help=\"The length of the sequence for temporal information\")\n\t    parser.add_argument(\"--batch-size\", default=None, type=int)\n\t    parser.add_argument(\"--use-bn\", action=\"store_true\", default=None)\n\t    parser.add_argument(\"--n-epochs\", type=int, default=None)\n\t    parser.add_argument(\"--backbone\", default=None)\n\t    parser.add_argument(\"--rpn-pth\", default=None, type=str)\n", "    parser.add_argument(\"--frcnn-pth\", default=None, type=str)\n\t    parser.add_argument(\"--use-aug\", action=\"store_true\", default=None)\n\t    parser.add_argument(\"--layout\", default=None)\n\t    parser.add_argument(\"--use-doppler\", action=\"store_true\", default=None)\n\t    parser.add_argument(\"--use-dropout\", action=\"store_true\", default=None)\n\t    parser.add_argument(\"--dataset\", default=None)\n\t    parser.add_argument(\"--optimizer\", default=None)\n\t    parser.add_argument(\"--lr\", default=None, type=float)\n\t    parser.add_argument(\"--use-scheduler\", action=\"store_true\", default=None)\n\t    parser.add_argument(\"--exp\")\n", "    parser.add_argument(\"--init\", type=str)\n\t    parser.add_argument(\"--pt\", action=\"store_true\", default=None)\n\t    args = parser.parse_args()\n\t    return args\n\tdef args2config(args):\n\t    \"\"\"\n\t    Parse arguments to a new config file\n\t    :param args: arguments\n\t    :return: updated configuration dictionary\n\t    \"\"\"\n", "    with open(args.config) as file:\n\t        config = json.load(file)\n\t    config[\"log\"][\"exp\"] = args.exp\n\t    # Model hyper parameters\n\t    config[\"model\"][\"backbone\"] = args.backbone if args.backbone is not None else config[\"model\"][\"backbone\"]\n\t    config[\"model\"][\"layout\"] = args.layout if args.layout is not None else config[\"model\"][\"layout\"]\n\t    config[\"model\"][\"sequence_len\"] = args.sequence_length if args.sequence_length is not None else config[\"model\"][\n\t        \"sequence_len\"]\n\t    # Training hyper parameters\n\t    config[\"training\"][\"batch_size\"] = args.batch_size if args.batch_size is not None else config[\"training\"][\n", "        \"batch_size\"]\n\t    config[\"training\"][\"epochs\"] = args.n_epochs if args.n_epochs is not None else config[\"training\"][\"epochs\"]\n\t    config[\"training\"][\"use_bn\"] = args.use_bn if args.use_bn is not None else config[\"training\"][\"use_bn\"]\n\t    config[\"training\"][\"use_aug\"] = args.use_aug if args.use_aug is not None else config[\"training\"][\"use_aug\"]\n\t    config[\"training\"][\"use_doppler\"] = args.use_doppler if args.use_doppler is not None else config[\"training\"][\n\t        \"use_doppler\"]\n\t    config[\"training\"][\"use_dropout\"] = args.use_dropout if args.use_dropout is not None else config[\"training\"][\n\t        \"use_dropout\"]\n\t    config[\"training\"][\"optimizer\"] = args.optimizer if args.optimizer is not None else config[\"training\"][\"optimizer\"]\n\t    config[\"training\"][\"lr\"] = args.lr if args.lr is not None else config[\"training\"][\"lr\"]\n", "    config[\"training\"][\"scheduler\"] = args.use_scheduler if args.use_scheduler is not None else config[\"training\"][\n\t        \"scheduler\"]\n\t    config[\"training\"][\"pretraining\"] = \"imagenet\" if args.pt is not None else \"None\"\n\t    # Dataset hyper parameters\n\t    config[\"data\"][\"dataset\"] = args.dataset.split(\":\")[0] if args.dataset is not None else config[\"data\"][\"dataset\"]\n\t    config[\"data\"][\"dataset_version\"] = args.dataset.split(\":\")[1] if args.dataset is not None else config[\"data\"][\n\t        \"dataset_version\"]\n\t    return config\n\tdef handle_args_eval():\n\t    \"\"\"\n", "    Parse command line arguments for evaluation script\n\t    :return: args\n\t    \"\"\"\n\t    parser = argparse.ArgumentParser(description=\"R2D2 evaluation and inference\")\n\t    parser.add_argument(\"--path\", help=\"Path to the logs\")\n\t    parser.add_argument(\"--show-res\", action=\"store_true\",\n\t                        help=\"Print predictions on the range-Doppler spectrum and upload it in Tensorboard\")\n\t    parser.add_argument(\"--iou-th\", action=\"append\", type=float,\n\t                        help=\"Store a list of IoU threshold to evaluate the model\")\n\t    parser.add_argument(\"--eval-best\", action=\"store_true\",\n", "                        help=\"Eval both the model on the model saved at the best val loss and on the last ckpt.\")\n\t    args = parser.parse_args()\n\t    return args\n\tdef handle_args_viz():\n\t    parser = argparse.ArgumentParser(description=\"R2D2 evaluation and inference\")\n\t    parser.add_argument(\"--path\", help=\"Path to the logs\")\n\t    args = parser.parse_args()\n\t    return args\n"]}
{"filename": "darod/utils/__init__.py", "chunked_list": []}
{"filename": "darod/utils/bbox_utils.py", "chunked_list": ["import tensorflow as tf\n\tdef anchors_generation(config, train=True):\n\t    \"\"\"\n\t    Generating anchors boxes with different shapes centered on\n\t    each pixel.\n\t    :param config: config file with input data, anchor scales/ratios\n\t    :param train: anchors for training or inference\n\t    :return: base_anchors = (anchor_count * fm_h * fm_w, [y1, x1, y2, x2]\n\t    \"\"\"\n\t    in_height, in_width = config[\"model\"][\"feature_map_shape\"]\n", "    scales, ratios = config[\"rpn\"][\"anchor_scales\"], config[\"rpn\"][\"anchor_ratios\"]\n\t    num_scales, num_ratios = len(scales), len(ratios)\n\t    scale_tensor = tf.convert_to_tensor(scales, dtype=tf.float32)\n\t    ratio_tensor = tf.convert_to_tensor(ratios, dtype=tf.float32)\n\t    boxes_per_pixel = (num_scales + num_ratios - 1)\n\t    #\n\t    offset_h, offset_w = 0.5, 0.5\n\t    steps_h = 1.0 / in_height\n\t    steps_w = 1.0 / in_width\n\t    # Generate all center points for the anchor boxes\n", "    center_h = (tf.range(in_height, dtype=tf.float32) + offset_h) * steps_h\n\t    center_w = (tf.range(in_width, dtype=tf.float32) + offset_w) * steps_w\n\t    shift_y, shift_x = tf.meshgrid(center_h, center_w)\n\t    shift_y, shift_x = tf.reshape(shift_y, shape=(-1)), tf.reshape(shift_x, shape=(-1))\n\t    # Generate \"boxes_per_pixel\" number of heights and widths that are later\n\t    # used to create anchor box corner coordinates xmin, xmax, ymin ymax\n\t    w = tf.concat((scale_tensor * tf.sqrt(ratio_tensor[0]), scales[0] * tf.sqrt(ratio_tensor[1:])),\n\t                  axis=-1) * in_height / in_width\n\t    h = tf.concat((scale_tensor / tf.sqrt(ratio_tensor[0]), scales[0] / tf.sqrt(ratio_tensor[1:])),\n\t                  axis=-1) * in_height / in_width\n", "    # Divide by 2 to get the half height and half width\n\t    anchor_manipulation = tf.tile(tf.transpose(tf.stack([-w, -h, w, h])), [in_height * in_width, 1]) / 2\n\t    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n\t    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n\t    out_grid = tf.repeat(tf.stack([shift_x, shift_y, shift_x, shift_y], axis=1), boxes_per_pixel, axis=0)\n\t    output = out_grid + anchor_manipulation\n\t    if train:\n\t        output = tf.where(\n\t            tf.reduce_any(tf.logical_or(tf.less_equal(output, 0.0), tf.greater_equal(output, 1.0)), axis=-1,\n\t                          keepdims=True) == False, output, 0.0)\n", "    else:\n\t        output = tf.clip_by_value(output, clip_value_min=0.0, clip_value_max=1.0)\n\t    return output\n\tdef non_max_suppression(pred_bboxes, pred_labels, **kwargs):\n\t    \"\"\"\n\t    Applying non maximum suppression.\n\t    Details could be found on tensorflow documentation.\n\t    https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression\n\t    :param pred_bboxes: (batch_size, total_bboxes, total_labels, [y1, x1, y2, x2]), total_labels should be 1 for binary operations like in rpn\n\t    :param pred_labels: (batch_size, total_bboxes, total_labels)\n", "    :param kwargs: other parameters\n\t    :return: nms_boxes = (batch_size, max_detections, [y1, x1, y2, x2])\n\t            nmsed_scores = (batch_size, max_detections)\n\t            nmsed_classes = (batch_size, max_detections)\n\t            valid_detections = (batch_size)\n\t                Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid.\n\t                The rest of the entries are zero paddings.\n\t    \"\"\"\n\t    return tf.image.combined_non_max_suppression(\n\t        pred_bboxes,\n", "        pred_labels,\n\t        **kwargs\n\t    )\n\tdef get_bboxes_from_deltas(anchors, deltas):\n\t    \"\"\"\n\t    Calculating bounding boxes for given bounding box and delta values.\n\t    :param anchors: (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    :param deltas: (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n\t    :return:  final_boxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    \"\"\"\n", "    all_anc_width = anchors[..., 3] - anchors[..., 1]\n\t    all_anc_height = anchors[..., 2] - anchors[..., 0]\n\t    all_anc_ctr_x = anchors[..., 1] + 0.5 * all_anc_width\n\t    all_anc_ctr_y = anchors[..., 0] + 0.5 * all_anc_height\n\t    #\n\t    all_bbox_width = tf.exp(deltas[..., 3]) * all_anc_width\n\t    all_bbox_height = tf.exp(deltas[..., 2]) * all_anc_height\n\t    all_bbox_ctr_x = (deltas[..., 1] * all_anc_width) + all_anc_ctr_x\n\t    all_bbox_ctr_y = (deltas[..., 0] * all_anc_height) + all_anc_ctr_y\n\t    #\n", "    y1 = all_bbox_ctr_y - (0.5 * all_bbox_height)\n\t    x1 = all_bbox_ctr_x - (0.5 * all_bbox_width)\n\t    y2 = all_bbox_height + y1\n\t    x2 = all_bbox_width + x1\n\t    #\n\t    return tf.stack([y1, x1, y2, x2], axis=-1)\n\tdef get_bboxes_transforms(deltas):\n\t    \"\"\"\n\t    Calculating bounding boxes for given delta values.\n\t    :param deltas:  (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n", "    :return:  final_boxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    \"\"\"\n\t    dy = deltas[..., 0]\n\t    dx = deltas[..., 1]\n\t    dh = deltas[..., 2]\n\t    dw = deltas[..., 3]\n\t    pred_ctr_x = dx\n\t    pred_ctr_y = dy\n\t    pred_w = tf.exp(dw)\n\t    pred_h = tf.exp(dh)\n", "    y1 = pred_ctr_y - 0.5 * pred_h\n\t    x1 = pred_ctr_x - 0.5 * pred_w\n\t    y2 = pred_ctr_y + 0.5 * pred_h\n\t    x2 = pred_ctr_y + 0.5 * pred_w\n\t    return tf.stack([y1, x1, y2, x2], axis=-1)\n\tdef get_deltas_from_bboxes(bboxes, gt_boxes):\n\t    \"\"\"\n\t    Calculating bounding box deltas for given bounding box and ground truth boxes.\n\t    :param bboxes: (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    :param gt_boxes: (batch_size, total_bboxes, [y1, x1, y2, x2])\n", "    :return:  final_deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n\t    \"\"\"\n\t    bbox_width = bboxes[..., 3] - bboxes[..., 1]\n\t    bbox_height = bboxes[..., 2] - bboxes[..., 0]\n\t    bbox_ctr_x = bboxes[..., 1] + 0.5 * bbox_width\n\t    bbox_ctr_y = bboxes[..., 0] + 0.5 * bbox_height\n\t    #\n\t    gt_width = gt_boxes[..., 3] - gt_boxes[..., 1]\n\t    gt_height = gt_boxes[..., 2] - gt_boxes[..., 0]\n\t    gt_ctr_x = gt_boxes[..., 1] + 0.5 * gt_width\n", "    gt_ctr_y = gt_boxes[..., 0] + 0.5 * gt_height\n\t    #\n\t    bbox_width = tf.where(tf.equal(bbox_width, 0), 1e-3, bbox_width)\n\t    bbox_height = tf.where(tf.equal(bbox_height, 0), 1e-3, bbox_height)\n\t    delta_x = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.truediv((gt_ctr_x - bbox_ctr_x), bbox_width))\n\t    delta_y = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height),\n\t                       tf.truediv((gt_ctr_y - bbox_ctr_y), bbox_height))\n\t    delta_w = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.math.log(gt_width / bbox_width))\n\t    delta_h = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.math.log(gt_height / bbox_height))\n\t    #\n", "    return tf.stack([delta_y, delta_x, delta_h, delta_w], axis=-1)\n\tdef generate_iou_map(bboxes, gt_boxes):\n\t    \"\"\"\n\t    Calculating iou values for each ground truth boxes in batched manner.\n\t    :param bboxes: (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    :param gt_boxes: (batch_size, total_gt_boxes, [y1, x1, y2, x2])\n\t    :return: iou_map = (batch_size, total_bboxes, total_gt_boxes)\n\t    \"\"\"\n\t    bbox_y1, bbox_x1, bbox_y2, bbox_x2 = tf.split(bboxes, 4, axis=-1)\n\t    gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(gt_boxes, 4, axis=-1)\n", "    # Calculate bbox and ground truth boxes areas\n\t    gt_area = tf.squeeze((gt_y2 - gt_y1) * (gt_x2 - gt_x1), axis=-1)\n\t    bbox_area = tf.squeeze((bbox_y2 - bbox_y1) * (bbox_x2 - bbox_x1), axis=-1)\n\t    #\n\t    x_top = tf.maximum(bbox_x1, tf.transpose(gt_x1, [0, 2, 1]))\n\t    y_top = tf.maximum(bbox_y1, tf.transpose(gt_y1, [0, 2, 1]))\n\t    x_bottom = tf.minimum(bbox_x2, tf.transpose(gt_x2, [0, 2, 1]))\n\t    y_bottom = tf.minimum(bbox_y2, tf.transpose(gt_y2, [0, 2, 1]))\n\t    ### Calculate intersection area\n\t    intersection_area = tf.maximum(x_bottom - x_top, 0) * tf.maximum(y_bottom - y_top, 0)\n", "    ### Calculate union area\n\t    union_area = (tf.expand_dims(bbox_area, -1) + tf.expand_dims(gt_area, 1) - intersection_area)\n\t    # Intersection over Union\n\t    return tf.math.divide_no_nan(intersection_area, union_area)\n\tdef generate_delta_map(bboxes, gt_boxes):\n\t    \"\"\"\n\t    Calculating delta values between center for each ground truth boxes in batched manner.\n\t    :param bboxes:  (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    :param gt_boxes:  (batch_size, total_gt_boxes, [y1, x1, y2, x2])\n\t    :return: delta_map = (batch_size, total_bboxes, total_gt_boxes)\n", "    \"\"\"\n\t    # Denormalize bboxes for better computation\n\t    bboxes = denormalize_bboxes(bboxes, 256, 64)\n\t    gt_boxes = denormalize_bboxes(gt_boxes, 256, 64)\n\t    #\n\t    bbox_y1, bbox_x1, bbox_y2, bbox_x2 = tf.split(bboxes, 4, axis=-1)\n\t    gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(gt_boxes, 4, axis=-1)\n\t    #  Calculate bbox and groud truth centers\n\t    bbox_center_x = bbox_x1 + (bbox_x2 - bbox_x1) / 2\n\t    bbox_center_y = bbox_y1 + (bbox_y2 - bbox_y1) / 2\n", "    #\n\t    gt_center_x = gt_x1 + (gt_x2 - gt_x1) / 2\n\t    gt_center_y = gt_y1 + (gt_y2 - gt_y1) / 2\n\t    #\n\t    deltas_x = tf.pow((tf.transpose(gt_center_x, [0, 2, 1]) - bbox_center_x), 2)\n\t    deltas_y = tf.pow((tf.transpose(gt_center_y, [0, 2, 1]) - bbox_center_y), 2)\n\t    deltas = tf.add(deltas_x, deltas_y)\n\t    deltas = tf.math.sqrt(deltas)\n\t    return deltas\n\tdef normalize_bboxes(bboxes, height, width):\n", "    \"\"\"\n\t    Normalizing bounding boxes.\n\t    :param bboxes: (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    :param height: image height\n\t    :param width: image width\n\t    :return: normalized_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2]) in normalized form [0, 1]\n\t    \"\"\"\n\t    y1 = bboxes[..., 0] / height\n\t    x1 = bboxes[..., 1] / width\n\t    y2 = bboxes[..., 2] / height\n", "    x2 = bboxes[..., 3] / width\n\t    return tf.stack([y1, x1, y2, x2], axis=-1)\n\tdef denormalize_bboxes(bboxes, height, width):\n\t    \"\"\"\n\t    Denormalizing bounding boxes.\n\t    :param bboxes: (batch_size, total_bboxes, [y1, x1, y2, x2]) in normalized form [0, 1]\n\t    :param height: image height\n\t    :param width: image width\n\t    :return: denormalized_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    \"\"\"\n", "    y1 = bboxes[..., 0] * height\n\t    x1 = bboxes[..., 1] * width\n\t    y2 = bboxes[..., 2] * height\n\t    x2 = bboxes[..., 3] * width\n\t    return tf.round(tf.stack([y1, x1, y2, x2], axis=-1))\n\tdef rotate_bboxes(bboxes, height, width):\n\t    \"\"\"\n\t    Rotate bounding boxes\n\t    :param bboxes: (batch_size, total_bboxes, [y1, x1, y2, x2]) in denormalize form [0, heigth/width]\n\t    :param height: image height\n", "    :param width: image width\n\t    :return: rotated_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n\t    \"\"\"\n\t    y1 = height - bboxes[..., 0]\n\t    x1 = width - bboxes[..., 1]\n\t    y2 = height - bboxes[..., 2]\n\t    x2 = width - bboxes[..., 3]\n\t    return tf.round(tf.stack([y1, x1, y2, x2], axis=-1))\n"]}
{"filename": "darod/utils/train_utils.py", "chunked_list": ["import time\n\timport numpy as np\n\timport tensorflow as tf\n\timport tensorflow_addons as tfa\n\tfrom ..utils import bbox_utils\n\tdef compute_eta(total_duration, epoch_duration, epoch, total_epochs):\n\t    \"\"\"Compute training ETA\"\"\"\n\t    total_duration += epoch_duration\n\t    eta = (total_epochs - epoch) * total_duration / (epoch)\n\t    return time.strftime(\"%H:%M:%S\", time.gmtime(eta)), total_duration\n", "def randomly_select_xyz_mask(mask, select_xyz, seed):\n\t    \"\"\"\n\t    Selecting x, y, z number of True elements for corresponding batch and replacing others to False\n\t    :param mask: (batch_size, [m_bool_value])\n\t    :param select_xyz: (batch_size, [m_bool_value])\n\t    :param seed: seed\n\t    :return:  selected_valid_mask = (batch_size, [m_bool_value])\n\t    \"\"\"\n\t    maxval = tf.reduce_max(select_xyz) * 10\n\t    random_mask = tf.random.uniform(tf.shape(mask), minval=1, maxval=maxval, dtype=tf.int32, seed=seed)\n", "    multiplied_mask = tf.cast(mask, tf.int32) * random_mask\n\t    sorted_mask = tf.argsort(multiplied_mask, direction=\"DESCENDING\")\n\t    sorted_mask_indices = tf.argsort(sorted_mask)\n\t    selected_mask = tf.less(sorted_mask_indices, tf.expand_dims(select_xyz, 1))\n\t    return tf.logical_and(mask, selected_mask)\n\tdef calculate_rpn_actual_outputs(anchors, gt_boxes, gt_labels, config):\n\t    \"\"\"\n\t    Generating one step data for training or inference. Batch operations supported.\n\t    :param anchors: (total_anchors, [y1, x1, y2, x2]) these values in normalized format between [0, 1]\n\t    :param gt_boxes: (batch_size, gt_box_size, [y1, x1, y2, x2]) these values in normalized format between [0, 1]\n", "    :param gt_labels: (batch_size, gt_box_size)\n\t    :param config: dictionary\n\t    :return: bbox_deltas = (batch_size, total_anchors, [delta_y, delta_x, delta_h, delta_w])\n\t             bbox_labels = (batch_size, feature_map_shape, feature_map_shape, anchor_count)\n\t    \"\"\"\n\t    batch_size = tf.shape(gt_boxes)[0]\n\t    anchor_count = config[\"rpn\"][\"anchor_count\"]\n\t    total_pos_bboxes = int(config[\"rpn\"][\"rpn_boxes\"] / 2)\n\t    total_neg_bboxes = int(config[\"rpn\"][\"rpn_boxes\"] / 2)\n\t    variances = config[\"rpn\"][\"variances\"]\n", "    adaptive_ratio = config[\"rpn\"][\"adaptive_ratio\"]\n\t    postive_th = config[\"rpn\"][\"positive_th\"]\n\t    #\n\t    output_height, output_width = config[\"model\"][\"feature_map_shape\"][0], config[\"model\"][\"feature_map_shape\"][1]\n\t    # Calculate iou values between each bboxes and ground truth boxes\n\t    iou_map = bbox_utils.generate_iou_map(anchors, gt_boxes)\n\t    # Get max index value for each row\n\t    max_indices_each_row = tf.argmax(iou_map, axis=2, output_type=tf.int32)\n\t    # Get max index value for each column\n\t    max_indices_each_column = tf.argmax(iou_map, axis=1, output_type=tf.int32)\n", "    # IoU map has iou values for every gt boxes and we merge these values column wise\n\t    merged_iou_map = tf.reduce_max(iou_map, axis=2)\n\t    #\n\t    pos_mask = tf.greater(merged_iou_map, postive_th)\n\t    #\n\t    valid_indices_cond = tf.not_equal(gt_labels, -1)\n\t    valid_indices = tf.cast(tf.where(valid_indices_cond), tf.int32)\n\t    valid_max_indices = max_indices_each_column[valid_indices_cond]\n\t    #\n\t    scatter_bbox_indices = tf.stack([valid_indices[..., 0], valid_max_indices], 1)\n", "    max_pos_mask = tf.scatter_nd(scatter_bbox_indices, tf.fill((tf.shape(valid_indices)[0],), True), tf.shape(pos_mask))\n\t    pos_mask = tf.logical_and(tf.logical_or(pos_mask, max_pos_mask), tf.reduce_sum(anchors, axis=-1) != 0.0)\n\t    pos_mask = randomly_select_xyz_mask(pos_mask, tf.constant([total_pos_bboxes], dtype=tf.int32),\n\t                                        seed=config[\"training\"][\"seed\"])\n\t    #\n\t    pos_count = tf.reduce_sum(tf.cast(pos_mask, tf.int32), axis=-1)\n\t    # Keep a 50%/50% ratio of positive/negative samples\n\t    if adaptive_ratio:\n\t        neg_count = 2 * pos_count\n\t    else:\n", "        neg_count = (total_pos_bboxes + total_neg_bboxes) - pos_count\n\t    #\n\t    neg_mask = tf.logical_and(tf.logical_and(tf.less(merged_iou_map, 0.3), tf.logical_not(pos_mask)),\n\t                              tf.reduce_sum(anchors, axis=-1) != 0.0)\n\t    neg_mask = randomly_select_xyz_mask(neg_mask, neg_count, seed=config[\"training\"][\"seed\"])\n\t    #\n\t    pos_labels = tf.where(pos_mask, tf.ones_like(pos_mask, dtype=tf.float32), tf.constant(-1.0, dtype=tf.float32))\n\t    neg_labels = tf.cast(neg_mask, dtype=tf.float32)\n\t    bbox_labels = tf.add(pos_labels, neg_labels)\n\t    #\n", "    gt_boxes_map = tf.gather(gt_boxes, max_indices_each_row, batch_dims=1)\n\t    # Replace negative bboxes with zeros\n\t    expanded_gt_boxes = tf.where(tf.expand_dims(pos_mask, -1), gt_boxes_map, tf.zeros_like(gt_boxes_map))\n\t    # Calculate delta values between anchors and ground truth bboxes\n\t    bbox_deltas = bbox_utils.get_deltas_from_bboxes(anchors, expanded_gt_boxes) / variances\n\t    #\n\t    bbox_labels = tf.reshape(bbox_labels, (batch_size, output_height, output_width, anchor_count))\n\t    #\n\t    return bbox_deltas, bbox_labels\n\tdef frcnn_cls_loss(*args):\n", "    \"\"\"\n\t    Calculating faster rcnn class loss value.\n\t    :param args: could be (y_true, y_pred) or ((y_true, y_pred), )\n\t    :return: CE loss\n\t    \"\"\"\n\t    y_true, y_pred = args if len(args) == 2 else args[0]\n\t    loss_fn = tf.losses.CategoricalCrossentropy(reduction=tf.losses.Reduction.NONE, from_logits=True)\n\t    loss_for_all = loss_fn(y_true, y_pred)\n\t    #\n\t    cond = tf.reduce_any(tf.not_equal(y_true, tf.constant(0.0)), axis=-1)\n", "    mask = tf.cast(cond, dtype=tf.float32)\n\t    #\n\t    conf_loss = tf.reduce_sum(mask * loss_for_all)\n\t    total_boxes = tf.maximum(1.0, tf.reduce_sum(mask))\n\t    return tf.math.divide_no_nan(conf_loss, total_boxes)\n\tdef rpn_cls_loss(*args):\n\t    \"\"\"\n\t    Calculating rpn class loss value.\n\t    :param args: could be (y_true, y_pred) or ((y_true, y_pred), )\n\t    :return: CE loss\n", "    \"\"\"\n\t    y_true, y_pred = args if len(args) == 2 else args[0]\n\t    indices = tf.where(tf.not_equal(y_true, tf.constant(-1.0, dtype=tf.float32)))\n\t    target = tf.gather_nd(y_true, indices)\n\t    output = tf.gather_nd(y_pred, indices)\n\t    lf = tf.losses.BinaryCrossentropy(from_logits=True)\n\t    return lf(target, output)\n\tdef reg_loss(*args):\n\t    \"\"\"\n\t    Calculating rpn / faster rcnn regression loss value.\n", "    :param args: could be (y_true, y_pred) or ((y_true, y_pred), )\n\t    :return: regression loss val\n\t    \"\"\"\n\t    y_true, y_pred = args if len(args) == 2 else args[0]\n\t    y_pred = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1, 4))\n\t    #\n\t    loss_fn = tf.losses.Huber(reduction=tf.losses.Reduction.NONE, delta=1 / 9)\n\t    loss_for_all = loss_fn(y_true, y_pred)\n\t    # loss_for_all = tf.reduce_sum(loss_for_all, axis=-1)\n\t    #\n", "    pos_cond = tf.reduce_any(tf.not_equal(y_true, tf.constant(0.0)), axis=-1)\n\t    pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n\t    #\n\t    loc_loss = tf.reduce_sum(tf.math.multiply_no_nan(pos_mask, loss_for_all))\n\t    total_pos_bboxes = tf.maximum(1.0, tf.reduce_sum(pos_mask))\n\t    return tf.math.divide_no_nan(loc_loss, total_pos_bboxes)\n\tdef giou_loss(y_true, y_pred, roi_bboxes):\n\t    \"\"\"\n\t    Calculating rpn / faster rcnn regression loss value.\n\t    :param y_true: ground truth\n", "    :param y_pred: predictied deltas\n\t    :param roi_bboxes: predicted boxes\n\t    :return: regression loss val\n\t    \"\"\"\n\t    y_pred = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1, 4))\n\t    y_pred = bbox_utils.get_bboxes_from_deltas(roi_bboxes, y_pred)\n\t    #\n\t    loss_fn = tfa.losses.GIoULoss(reduction=tf.losses.Reduction.NONE)\n\t    loss_for_all = loss_fn(y_true, y_pred)\n\t    # loss_for_all = tf.reduce_sum(loss_for_all, axis=-1)\n", "    #\n\t    pos_cond = tf.reduce_any(tf.not_equal(y_true, tf.constant(0.0)), axis=-1)\n\t    pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n\t    #\n\t    loc_loss = tf.reduce_sum(tf.math.multiply_no_nan(pos_mask, loss_for_all))\n\t    total_pos_bboxes = tf.maximum(1.0, tf.reduce_sum(pos_mask))\n\t    return tf.math.divide_no_nan(loc_loss, total_pos_bboxes)\n\tdef darod_loss(rpn_cls_pred, rpn_delta_pred, frcnn_cls_pred, frcnn_reg_pred,\n\t              bbox_labels, bbox_deltas, frcnn_reg_actuals, frcnn_cls_actuals):\n\t    \"\"\"\n", "    Calculate loss function for DAROD model\n\t    :param rpn_cls_pred: RPN classification pred\n\t    :param rpn_delta_pred: RPN regression pred\n\t    :param frcnn_cls_pred: FRCNN classification pred\n\t    :param frcnn_reg_pred: FRCNN regression pred\n\t    :param bbox_labels: bounding boxes labels\n\t    :param bbox_deltas: bounding boxes deltas\n\t    :param frcnn_reg_actuals: faster rcnn regression labels\n\t    :param frcnn_cls_actuals: faster rcnn classification labels\n\t    :return: faster rcnn loss\n", "    \"\"\"\n\t    rpn_regression_loss = reg_loss(bbox_deltas, rpn_delta_pred)\n\t    rpn_classif_loss = rpn_cls_loss(bbox_labels, rpn_cls_pred)\n\t    frcnn_regression_loss = reg_loss(frcnn_reg_actuals, frcnn_reg_pred)\n\t    frcnn_classif_loss = frcnn_cls_loss(frcnn_cls_actuals, frcnn_cls_pred)\n\t    return rpn_regression_loss, rpn_classif_loss, frcnn_regression_loss, frcnn_classif_loss\n\tdef darod_loss_giou(rpn_cls_pred, rpn_delta_pred, frcnn_cls_pred, frcnn_reg_pred,\n\t                   bbox_labels, bbox_deltas, frcnn_reg_actuals, frcnn_cls_actuals, roi_bboxes):\n\t    rpn_regression_loss = reg_loss(bbox_deltas, rpn_delta_pred)\n\t    rpn_classif_loss = rpn_cls_loss(bbox_labels, rpn_cls_pred)\n", "    frcnn_regression_loss = giou_loss(frcnn_reg_actuals, frcnn_reg_pred, roi_bboxes) / 10\n\t    frcnn_classif_loss = frcnn_cls_loss(frcnn_cls_actuals, frcnn_cls_pred)\n\t    return rpn_regression_loss, rpn_classif_loss, frcnn_regression_loss, frcnn_classif_loss\n"]}
{"filename": "darod/utils/log_utils.py", "chunked_list": ["import os\n\timport tensorflow as tf\n\tdef get_log_path(model_type, exp, backbone, custom_postfix=\"\"):\n\t    \"\"\"\n\t    Generating log path from model_type value for tensorboard.\n\t    :param model_type: the model used\n\t    :param exp: experiment name\n\t    :param backbone: the backbone used\n\t    :param custom_postfix:  any custom string for log folder name\n\t    :return: tensorboard log path, for example: \"logs/rpn_mobilenet_v2/{date}\"\n", "    \"\"\"\n\t    log_path = \"logs/{}_{}{}/{}/\".format(model_type, backbone, custom_postfix, exp)\n\t    if not os.path.exists(log_path):\n\t        os.makedirs(log_path)\n\t    return log_path\n\tdef tensorboard_val_stats(writer, ap_dict, labels, step):\n\t    \"\"\"\n\t    Write evaluation metric to tensorboard\n\t    :param writer: TB writer\n\t    :param ap_dict: dictionary with AP\n", "    :param labels: labels list\n\t    :param step: epoch number\n\t    :return:\n\t    \"\"\"\n\t    with writer.as_default():\n\t        for class_id in ap_dict:\n\t            if class_id != \"mean\":\n\t                tf.summary.scalar(\"mAP@0.5/\" + labels[class_id], ap_dict[class_id][\"AP\"][0], step=step)\n\t                tf.summary.scalar(\"precision@0.5/\" + labels[class_id], ap_dict[class_id][\"precision\"][0], step=step)\n\t                tf.summary.scalar(\"recall@0.5/\" + labels[class_id], ap_dict[class_id][\"recall\"][0], step=step)\n", "                tf.summary.scalar(\"F1@0.5/\" + labels[class_id], ap_dict[class_id][\"F1\"][0], step=step)\n\t        tf.summary.scalar(\"mAP@0.5/Mean\", ap_dict[\"mean\"][\"AP\"][0], step=step)\n\t        tf.summary.scalar(\"precision@0.5/Mean\", ap_dict[\"mean\"][\"precision\"][0], step=step)\n\t        tf.summary.scalar(\"recall@0.5/Mean\", ap_dict[\"mean\"][\"recall\"][0], step=step)\n\t        tf.summary.scalar(\"F1@0.5/Mean\", ap_dict[\"mean\"][\"F1\"][0], step=step)\n"]}
{"filename": "darod/utils/viz_utils.py", "chunked_list": ["import io\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport seaborn as sb\n\timport tensorflow as tf\n\tfrom PIL import Image\n\tfrom matplotlib.patches import Rectangle\n\tfrom .bbox_utils import denormalize_bboxes\n\tdef set_seaborn_style():\n\t    sb.set_style(style='darkgrid')\n", "    colors = [\"#FCB316\", \"#6DACDE\", \"#BFD730\", \"#320E3B\", \"#E56399\", \"#393D3F\", \"#A97C73\", \"#AF3E4D\", \"#2F4B26\"]\n\t    sb.set_context(\"paper\")\n\t    sb.set_palette(sb.color_palette(colors))\n\tdef to_PIL(spectrum):\n\t    spectrum = ((spectrum - np.min(spectrum)) / (np.max(spectrum) - np.min(spectrum)))\n\t    cm = plt.get_cmap('plasma')\n\t    colored_spectrum = cm(spectrum)\n\t    pil_spectrum = Image.fromarray((colored_spectrum[:, :, :3] * 255).astype(np.uint8))\n\t    return pil_spectrum\n\tdef norm2Image(array):\n", "    \"\"\" normalize to image format (uint8) \"\"\"\n\t    norm_sig = plt.Normalize()\n\t    img = plt.cm.plasma(norm_sig(array))\n\t    img *= 255.\n\t    img = img.astype(np.uint8)\n\t    return img\n\tdef drawRDboxes(boxes, labels, ax, use_facecolor=True, color=\"#E56399\", class_names=[\"pedestrian\", \"bicyclist\", \"car\"]):\n\t    \"\"\"Draw bounding boxes on RD spectrum\n\t    Set labels to None for RPN boxes only.\"\"\"\n\t    if use_facecolor:\n", "        facecolor = color\n\t    else:\n\t        facecolor = \"none\"\n\t    if labels is not None:\n\t        for (box, label) in zip(boxes, labels):\n\t            if label != -1 or tf.reduce_sum(box, axis=0) != 0:\n\t                print(box)\n\t                y1, x1, y2, x2 = denormalize_bboxes(box, 256, 64)\n\t                h, w = y2 - y1, x2 - x1\n\t                rect = Rectangle((x1, y1), width=w, height=h, linewidth=1.0, alpha=0.9,\n", "                                 linestyle=\"dashed\", color=color, facecolor=facecolor, edgecolor=color, fill=False)\n\t                ax.add_patch(rect)\n\t                label = int(label.numpy())\n\t                ax.text(x1 - 1 - (x2 - x1), y1 - 3 - (y2 - y1), class_names[label - 1], size=10,\n\t                        verticalalignment='baseline',\n\t                        color='w', backgroundcolor=\"none\",\n\t                        bbox={'facecolor': color, 'alpha': 0.5,\n\t                              'pad': 2, 'edgecolor': 'none'})\n\t    else:\n\t        for box in boxes:\n", "            if tf.reduce_sum(box, axis=0) != 0:\n\t                y1, x1, y2, x2 = denormalize_bboxes(box, 256, 64)\n\t                h, w = y2 - y1, x2 - x1\n\t                rect = Rectangle((x1, y1), width=w, height=h, linewidth=1.0, alpha=0.9,\n\t                                 linestyle=\"dashed\", color=color, facecolor=facecolor, fill=False)\n\t                ax.add_patch(rect)\n\tdef drawRDboxes_with_scores(boxes, labels, scores, ax, use_facecolor=True, color=\"#E56399\",\n\t                            class_names=[\"pedestrian\", \"bicyclist\", \"car\"]):\n\t    \"\"\"Draw bounding boxes on RD spectrum with scores.\n\t    This function must be use ONLY with predictions\"\"\"\n", "    if use_facecolor:\n\t        facecolor = color\n\t    else:\n\t        facecolor = \"none\"\n\t    for (box, label, score) in zip(boxes, labels, scores):\n\t        if label != 0 or tf.reduce_sum(box, axis=0) != 0:\n\t            y1, x1, y2, x2 = denormalize_bboxes(box, 256, 64)\n\t            h, w = y2 - y1, x2 - x1\n\t            score = np.round(score.numpy(), 4)\n\t            rect = Rectangle((x1, y1), width=w, height=h, linewidth=1.0, alpha=0.9,\n", "                             linestyle=\"dashed\", color=color, facecolor=facecolor, edgecolor=color, fill=False)\n\t            ax.add_patch(rect)\n\t            label = int(label.numpy())\n\t            ax.text(x1 + 1, y1 - 3, class_names[label - 1] + \" \" + str(score), size=10, verticalalignment='baseline',\n\t                    color='w', backgroundcolor=\"none\",\n\t                    bbox={'facecolor': color, 'alpha': 0.5,\n\t                          'pad': 2, 'edgecolor': 'none'})\n\tdef showCameraRD(camera_image, rd_spectrum, boxes, labels, class_names, scores=None, gt_boxes=None, gt_labels=None,\n\t                 dataset=\"carrada\"):\n\t    fig = plt.figure(figsize=(15, 5))\n", "    ax1 = fig.add_subplot(121)\n\t    ax2 = fig.add_subplot(122)\n\t    rd_spectrum = norm2Image(rd_spectrum.numpy().squeeze())\n\t    ax1.imshow(rd_spectrum)\n\t    title = \"Range-Doppler\"\n\t    ax1.set_xticks([0, 16, 32, 48, 63])\n\t    ax1.set_xticklabels([-13, -6.5, 0, 6.5, 13])\n\t    ax1.set_yticks([0, 64, 128, 192, 255])\n\t    ax1.set_yticklabels([50, 37.5, 25, 12.5, 0])\n\t    ax1.set_xlabel(\"velocity (m/s)\")\n", "    ax1.set_ylabel(\"range (m)\")\n\t    ax1.set_title(title)\n\t    if boxes is not None and labels is not None and scores is None:\n\t        drawRDboxes(boxes, labels, ax1, class_names=class_names, color=\"#02D0F0\")\n\t    elif scores is not None and boxes is not None and labels is not None:\n\t        drawRDboxes_with_scores(boxes, labels, scores, ax1, class_names=class_names, color=\"#02D0F0\")\n\t    else:\n\t        raise ValueError(\"Please use at least boxes and labels as arguments.\")\n\t    if gt_boxes is not None and gt_labels is not None:\n\t        drawRDboxes(gt_boxes, gt_labels, ax1, class_names=class_names, color=\"#ECE8EF\")\n", "    # Show only left camera image\n\t    if dataset == \"raddet\":\n\t        ax2.imshow(camera_image[:, :camera_image.shape[1] // 2, ...])\n\t    else:\n\t        ax2.imshow(camera_image)\n\t    ax2.axis('off')\n\t    ax2.set_title(\"camera image\")\n\t    return fig\n\tdef plot_to_image(figure):\n\t    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n", "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n\t    # Save the plot to a PNG in memory.\n\t    buf = io.BytesIO()\n\t    plt.savefig(buf, format='png', bbox_inches=\"tight\")\n\t    # Closing the figure prevents it from being displayed directly inside\n\t    # the notebook.\n\t    plt.close(figure)\n\t    buf.seek(0)\n\t    # Convert PNG buffer to TF image\n\t    image = tf.image.decode_png(buf.getvalue(), channels=4)\n", "    # Add the batch dimension\n\t    image = tf.expand_dims(image, 0)\n\t    return image\n"]}
{"filename": "darod/layers/frcnn_layers.py", "chunked_list": ["import tensorflow as tf\n\tfrom tensorflow.keras.layers import Layer\n\tfrom ..utils import bbox_utils, train_utils\n\tclass RoIBBox(Layer):\n\t    \"\"\"\n\t    Generating bounding boxes from rpn predictions.\n\t    First calculating the boxes from predicted deltas and label probs.\n\t    Then applied non max suppression and selecting \"train or test nms_topn\" boxes.\n\t    \"\"\"\n\t    def __init__(self, anchors, config, **kwargs):\n", "        \"\"\"\n\t        :param anchors: generated anchors\n\t        :param config: configuration dictionary\n\t        :param kwargs:\n\t        \"\"\"\n\t        super(RoIBBox, self).__init__(**kwargs)\n\t        self.config = config\n\t        # self.mode = mode\n\t        self.anchors = tf.constant(anchors, dtype=tf.float32)\n\t    def get_config(self):\n", "        config = super(RoIBBox, self).get_config()\n\t        # config.update({\"config\": self.config, \"anchors\": self.anchors.numpy(), \"mode\": self.mode})\n\t        return config\n\t    def call(self, inputs, training=True):\n\t        \"\"\"\n\t        :param inputs: rpn_bbox_deltas and rpn_labels\n\t        :param training:\n\t        :return: roi boxes\n\t        \"\"\"\n\t        rpn_bbox_deltas = inputs[0]\n", "        rpn_labels = inputs[1]\n\t        anchors = self.anchors\n\t        #\n\t        pre_nms_topn = self.config[\"fastrcnn\"][\"pre_nms_topn_train\"] if training else self.config[\"fastrcnn\"][\n\t            \"pre_nms_topn_test\"]\n\t        post_nms_topn = self.config[\"fastrcnn\"][\"post_nms_topn_train\"] if training else self.config[\"fastrcnn\"][\n\t            \"post_nms_topn_test\"]\n\t        nms_iou_threshold = self.config[\"rpn\"][\"rpn_nms_iou\"]\n\t        variances = self.config[\"rpn\"][\"variances\"]\n\t        total_anchors = anchors.shape[0]\n", "        batch_size = tf.shape(rpn_bbox_deltas)[0]\n\t        rpn_bbox_deltas = tf.reshape(rpn_bbox_deltas, (batch_size, total_anchors, 4))\n\t        # Apply softmax to rpn_labels to obtain probabilities\n\t        rpn_labels = tf.nn.softmax(rpn_labels)\n\t        rpn_labels = tf.reshape(rpn_labels, (batch_size, total_anchors))\n\t        #\n\t        rpn_bbox_deltas *= variances\n\t        rpn_bboxes = bbox_utils.get_bboxes_from_deltas(anchors, rpn_bbox_deltas)\n\t        _, pre_indices = tf.nn.top_k(rpn_labels, pre_nms_topn)\n\t        #\n", "        pre_roi_bboxes = tf.gather(rpn_bboxes, pre_indices, batch_dims=1)\n\t        pre_roi_labels = tf.gather(rpn_labels, pre_indices, batch_dims=1)\n\t        #\n\t        pre_roi_bboxes = tf.reshape(pre_roi_bboxes, (batch_size, pre_nms_topn, 1, 4))\n\t        pre_roi_labels = tf.reshape(pre_roi_labels, (batch_size, pre_nms_topn, 1))\n\t        roi_bboxes, roi_scores, _, _ = bbox_utils.non_max_suppression(pre_roi_bboxes, pre_roi_labels,\n\t                                                                      max_output_size_per_class=post_nms_topn,\n\t                                                                      max_total_size=post_nms_topn,\n\t                                                                      iou_threshold=nms_iou_threshold)\n\t        return tf.stop_gradient(roi_bboxes), tf.stop_gradient(roi_scores)\n", "class RadarFeatures(Layer):\n\t    \"\"\"\n\t    Extracting radar feature from RPN proposed boxes.\n\t    This layer extracts range and Doppler values from RPN proposed\n\t    boxes which have scores > 0.5. Otherwise, range and Doppler values\n\t    are set to -1\n\t    \"\"\"\n\t    def __init__(self, config, **kwargs):\n\t        super(RadarFeatures, self).__init__(**kwargs)\n\t        self.config = config\n", "    def get_config(self):\n\t        config = super(RadarFeatures, self).get_config()\n\t        config.update({\"config\": self.config})\n\t        return config\n\t    def call(self, inputs):\n\t        \"\"\"\n\t        :param inputs: roi_bboxes and roi_scores\n\t        :return: radar_features\n\t        \"\"\"\n\t        # Get inputs\n", "        roi_bboxes = bbox_utils.denormalize_bboxes(inputs[0],\n\t                                                   height=self.config[\"model\"][\"input_size\"][0],\n\t                                                   width=self.config[\"model\"][\"input_size\"][1])\n\t        roi_scores = inputs[1]\n\t        # Get centers of roi boxes\n\t        h = roi_bboxes[..., 2] - roi_bboxes[..., 0]\n\t        w = roi_bboxes[..., 3] - roi_bboxes[..., 1]\n\t        ctr_y = roi_bboxes[..., 0] + h / 2\n\t        ctr_x = roi_bboxes[..., 1] + w / 2\n\t        # Get radar feature\n", "        range_values = self.config[\"data\"][\"range_res\"] * tf.cast(ctr_y, tf.float32)\n\t        doppler_values = tf.abs(32 - (self.config[\"data\"][\"doppler_res\"] * tf.cast(ctr_x, tf.float32)))\n\t        radar_features = tf.stack([range_values, doppler_values], axis=-1)\n\t        # Get valid radar features (i.e. boxes with height > 0 and width > 0 and scores > 0.5)\n\t        radar_features = tf.where(tf.expand_dims(tf.logical_and(roi_scores > 0.5, tf.logical_and(h > 0, w > 0)), -1),\n\t                                  radar_features, tf.ones_like(radar_features) * -1)\n\t        return radar_features\n\tclass RoIDelta(Layer):\n\t    \"\"\"\n\t    Calculating faster rcnn actual bounding box deltas and labels.\n", "    This layer only running on the training phase.\n\t    \"\"\"\n\t    def __init__(self, config, **kwargs):\n\t        super(RoIDelta, self).__init__(**kwargs)\n\t        self.config = config\n\t    def get_config(self):\n\t        config = super(RoIDelta, self).get_config()\n\t        config.update({\"config\": self.config})\n\t        return config\n\t    def call(self, inputs):\n", "        \"\"\"\n\t        :param inputs: roi boxes, gt boxes and gt labels\n\t        :return: roi boxes deltas and roi boxes labels\n\t        \"\"\"\n\t        roi_bboxes = inputs[0]\n\t        gt_boxes = inputs[1]\n\t        gt_labels = inputs[2]\n\t        total_labels = self.config[\"data\"][\"total_labels\"]\n\t        total_pos_bboxes = int(self.config[\"fastrcnn\"][\"frcnn_boxes\"] / 3)\n\t        total_neg_bboxes = int(self.config[\"fastrcnn\"][\"frcnn_boxes\"] * (2 / 3))\n", "        variances = self.config[\"fastrcnn\"][\"variances_boxes\"]\n\t        adaptive_ratio = self.config[\"fastrcnn\"][\"adaptive_ratio\"]\n\t        positive_th = self.config[\"fastrcnn\"][\"positive_th\"]\n\t        batch_size, total_bboxes = tf.shape(roi_bboxes)[0], tf.shape(roi_bboxes)[1]\n\t        # Calculate iou values between each bboxes and ground truth boxes\n\t        iou_map = bbox_utils.generate_iou_map(roi_bboxes, gt_boxes)\n\t        # Get max index value for each row\n\t        max_indices_each_gt_box = tf.argmax(iou_map, axis=2, output_type=tf.int32)\n\t        # IoU map has iou values for every gt boxes and we merge these values column wise\n\t        merged_iou_map = tf.reduce_max(iou_map, axis=2)\n", "        #\n\t        pos_mask = tf.greater(merged_iou_map, positive_th)\n\t        pos_mask = train_utils.randomly_select_xyz_mask(pos_mask, tf.constant([total_pos_bboxes], dtype=tf.int32),\n\t                                                        seed=self.config[\"training\"][\"seed\"])\n\t        #\n\t        neg_mask = tf.logical_and(tf.less(merged_iou_map, positive_th), tf.greater_equal(merged_iou_map, 0.0))\n\t        if adaptive_ratio:\n\t            pos_count = tf.reduce_sum(tf.cast(pos_mask, tf.int32), axis=-1)\n\t            # Keep a 33%/66% ratio of positive/negative bboxes\n\t            total_neg_bboxes = tf.multiply(pos_count + 1, 3)\n", "            neg_mask = train_utils.randomly_select_xyz_mask(neg_mask, total_neg_bboxes,\n\t                                                            seed=self.config[\"training\"][\"seed\"])\n\t        else:\n\t            neg_mask = train_utils.randomly_select_xyz_mask(neg_mask, tf.constant([total_neg_bboxes], dtype=tf.int32),\n\t                                                            seed=self.config[\"training\"][\"seed\"])\n\t        #\n\t        gt_boxes_map = tf.gather(gt_boxes, max_indices_each_gt_box, batch_dims=1)\n\t        expanded_gt_boxes = tf.where(tf.expand_dims(pos_mask, axis=-1), gt_boxes_map, tf.zeros_like(gt_boxes_map))\n\t        #\n\t        gt_labels_map = tf.gather(gt_labels, max_indices_each_gt_box, batch_dims=1)\n", "        pos_gt_labels = tf.where(pos_mask, gt_labels_map, tf.constant(-1, dtype=tf.int32))\n\t        neg_gt_labels = tf.cast(neg_mask, dtype=tf.int32)\n\t        expanded_gt_labels = pos_gt_labels + neg_gt_labels\n\t        #\n\t        roi_bbox_deltas = bbox_utils.get_deltas_from_bboxes(roi_bboxes, expanded_gt_boxes) / variances\n\t        #\n\t        roi_bbox_labels = tf.one_hot(expanded_gt_labels, total_labels)\n\t        scatter_indices = tf.tile(tf.expand_dims(roi_bbox_labels, -1), (1, 1, 1, 4))\n\t        roi_bbox_deltas = scatter_indices * tf.expand_dims(roi_bbox_deltas, -2)\n\t        roi_bbox_deltas = tf.reshape(roi_bbox_deltas, (batch_size, total_bboxes * total_labels, 4))\n", "        #\n\t        if self.config[\"fastrcnn\"][\"reg_loss\"] == \"sl1\":\n\t            return tf.stop_gradient(roi_bbox_deltas), tf.stop_gradient(roi_bbox_labels)\n\t        elif self.config[\"fastrcnn\"][\"reg_loss\"] == \"giou\":\n\t            expanded_roi_boxes = scatter_indices * tf.expand_dims(roi_bboxes, -2)\n\t            expanded_roi_boxes = tf.reshape(expanded_roi_boxes, (batch_size, total_bboxes * total_labels, 4))\n\t            #\n\t            expanded_gt_boxes = scatter_indices * tf.expand_dims(expanded_gt_boxes, -2)\n\t            expanded_gt_boxes = tf.reshape(expanded_gt_boxes, (batch_size, total_bboxes * total_labels, 4))\n\t            return tf.stop_gradient(expanded_roi_boxes), tf.stop_gradient(expanded_gt_boxes), tf.stop_gradient(\n", "                roi_bbox_labels)\n\tclass RoIPooling(Layer):\n\t    \"\"\"\n\t    Reducing all feature maps to same size.\n\t    Firstly cropping bounding boxes from the feature maps and then resizing it to the pooling size.\n\t    \"\"\"\n\t    def __init__(self, config, **kwargs):\n\t        super(RoIPooling, self).__init__(**kwargs)\n\t        self.config = config\n\t    def get_config(self):\n", "        config = super(RoIPooling, self).get_config()\n\t        config.update({\"config\": self.config})\n\t        return config\n\t    def call(self, inputs):\n\t        \"\"\"\n\t        :param inputs: feature map and roi boxes\n\t        :return: pooled features\n\t        \"\"\"\n\t        feature_map = inputs[0]\n\t        roi_bboxes = inputs[1]\n", "        pooling_size = self.config[\"fastrcnn\"][\"pooling_size\"]\n\t        batch_size, total_bboxes = tf.shape(roi_bboxes)[0], tf.shape(roi_bboxes)[1]\n\t        #\n\t        row_size = batch_size * total_bboxes\n\t        # We need to arange bbox indices for each batch\n\t        pooling_bbox_indices = tf.tile(tf.expand_dims(tf.range(batch_size), axis=1), (1, total_bboxes))\n\t        pooling_bbox_indices = tf.reshape(pooling_bbox_indices, (-1,))\n\t        pooling_bboxes = tf.reshape(roi_bboxes, (row_size, 4))\n\t        # Crop to bounding box size then resize to pooling size\n\t        pooling_feature_map = tf.image.crop_and_resize(\n", "            feature_map,\n\t            pooling_bboxes,\n\t            pooling_bbox_indices,\n\t            pooling_size\n\t        )\n\t        final_pooling_feature_map = tf.reshape(pooling_feature_map, (\n\t        batch_size, total_bboxes, pooling_feature_map.shape[1], pooling_feature_map.shape[2],\n\t        pooling_feature_map.shape[3]))\n\t        return final_pooling_feature_map\n"]}
{"filename": "darod/layers/__init__.py", "chunked_list": []}
{"filename": "darod/metrics/mAP.py", "chunked_list": ["import json\n\timport os\n\timport numpy as np\n\tdef giou2d(bboxes1, bboxes2):\n\t    \"\"\"\n\t    Calculate the gious between each bbox of bboxes1 and bboxes2.\n\t    :param bboxes1: [x1, y1, x2, y2]\n\t    :param bboxes2: [x1, y1, x2, y2]\n\t    :return: Generalised IoU between boxes1 and boxes2\n\t    \"\"\"\n", "    bboxes1 = bboxes1.astype(np.float32)\n\t    bboxes2 = bboxes2.astype(np.float32)\n\t    area1 = (bboxes1[2] - bboxes1[0] + 1) * (\n\t            bboxes1[3] - bboxes1[1] + 1)\n\t    area2 = (bboxes2[2] - bboxes2[0] + 1) * (\n\t            bboxes2[3] - bboxes2[1] + 1)\n\t    x_start = np.maximum(bboxes1[0], bboxes2[0])\n\t    x_min = np.minimum(bboxes1[0], bboxes2[0])\n\t    y_start = np.maximum(bboxes1[1], bboxes2[1])\n\t    y_min = np.minimum(bboxes1[1], bboxes2[1])\n", "    x_end = np.minimum(bboxes1[2], bboxes2[2])\n\t    x_max = np.maximum(bboxes1[2], bboxes2[2])\n\t    y_end = np.minimum(bboxes1[3], bboxes2[3])\n\t    y_max = np.maximum(bboxes1[3], bboxes2[3])\n\t    overlap = np.maximum(x_end - x_start + 1, 0) * np.maximum(y_end - y_start + 1, 0)\n\t    closure = np.maximum(x_max - x_min + 1, 0) * np.maximum(y_max - y_min + 1, 0)\n\t    union = area1 + area2 - overlap\n\t    ious = overlap / union - (closure - union) / closure\n\t    return ious\n\tdef iou2d(box_xywh_1, box_xywh_2):\n", "    \"\"\"\n\t    Numpy version of 3D bounding box IOU calculation\n\t    :param box_xywh_1: [x1, y1, x2, y2]\n\t    :param box_xywh_2: [x1, y1, x2, y2]\n\t    :return:\n\t    \"\"\"\n\t    assert box_xywh_1.shape[-1] == 4\n\t    assert box_xywh_2.shape[-1] == 4\n\t    box1_w = box_xywh_1[..., 2] - box_xywh_1[..., 0]\n\t    box1_h = box_xywh_1[..., 3] - box_xywh_1[..., 1]\n", "    #\n\t    box2_w = box_xywh_2[..., 2] - box_xywh_2[..., 0]\n\t    box2_h = box_xywh_2[..., 3] - box_xywh_2[..., 1]\n\t    ### areas of both boxes\n\t    box1_area = box1_h * box1_w\n\t    box2_area = box2_h * box2_w\n\t    ### find the intersection box\n\t    box1_min = [box_xywh_1[..., 0], box_xywh_1[..., 1]]\n\t    box1_max = [box_xywh_1[..., 2], box_xywh_1[..., 3]]\n\t    box2_min = [box_xywh_2[..., 0], box_xywh_2[..., 1]]\n", "    box2_max = [box_xywh_2[..., 2], box_xywh_2[..., 3]]\n\t    x_top = np.maximum(box1_min[0], box2_min[0])\n\t    y_top = np.maximum(box1_min[1], box2_min[1])\n\t    x_bottom = np.minimum(box1_max[0], box2_max[0])\n\t    y_bottom = np.minimum(box1_max[1], box2_max[1])\n\t    intersection_area = np.maximum(x_bottom - x_top, 0) * np.maximum(y_bottom - y_top, 0)\n\t    ### get union area\n\t    union_area = box1_area + box2_area - intersection_area\n\t    ### get iou\n\t    iou = np.nan_to_num(intersection_area / (union_area + 1e-10))\n", "    return iou\n\tdef init_tp_dict(n_classes, iou_threshold=[0.5]):\n\t    \"\"\"\n\t    Init a dictionary containing true positive and false positive per\n\t    class and total gt per class\n\t    :param n_classes: the number of classes\n\t    :param iou_threshold: a dictionary containing empty list for tp, fp and total_gts\n\t    :return: empty score dictionary\n\t    \"\"\"\n\t    tp_dict = dict()\n", "    for class_id in range(n_classes):\n\t        tp_dict[class_id] = {\n\t            \"tp\": [[] for _ in range(len(iou_threshold))],\n\t            \"fp\": [[] for _ in range(len(iou_threshold))],\n\t            \"scores\": [[] for _ in range(len(iou_threshold))],\n\t            \"total_gt\": 0\n\t        }\n\t    return tp_dict\n\tdef accumulate_tp_fp(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_classes, tp_dict, iou_thresholds):\n\t    \"\"\"\n", "    Count the number of false positive and true positive for a single image\n\t    :param pred_boxes: the predicted boxes for the image i\n\t    :param pred_labels: the predicted labels for the image i\n\t    :param pred_scores: the predicted scores for the image i\n\t    :param gt_boxes: the ground truth boxes for the image i\n\t    :param gt_classes: the ground truth labels for the image i\n\t    :param tp_dict: dictionary containing accumulated statistics on the test set\n\t    :param iou_thresholds: threshold to use for tp detections\n\t    :return: the updated ap_dict\n\t    \"\"\"\n", "    # Remove background detections\n\t    keep_idx = pred_boxes[..., :].any(axis=-1) > 0\n\t    pred_boxes = pred_boxes[keep_idx]\n\t    pred_labels = pred_labels[keep_idx]\n\t    pred_scores = pred_scores[keep_idx]\n\t    detected_gts = [[] for _ in range(len(iou_thresholds))]\n\t    # Count number of GT for each class\n\t    for i in range(len(gt_classes)):\n\t        gt_temp = gt_classes[i]\n\t        if gt_temp == -2:\n", "            continue\n\t        tp_dict[gt_temp][\"total_gt\"] += 1\n\t    for i in range(len(pred_boxes)):\n\t        pred_box = pred_boxes[i]\n\t        pred_label = int(pred_labels[i])\n\t        pred_score = pred_scores[i]\n\t        iou = iou2d(pred_box, gt_boxes)\n\t        max_idx_iou = np.argmax(iou)\n\t        gt_box = gt_boxes[max_idx_iou]\n\t        gt_label = gt_classes[max_idx_iou]\n", "        for iou_idx in range(len(iou_thresholds)):\n\t            tp_dict[pred_label][\"scores\"][iou_idx].append(pred_score)\n\t            tp_dict[pred_label][\"tp\"][iou_idx].append(0)\n\t            tp_dict[pred_label][\"fp\"][iou_idx].append(0)\n\t            if pred_label == gt_label and iou[max_idx_iou] >= iou_thresholds[iou_idx] and \\\n\t                    list(gt_box) not in detected_gts[iou_idx]:\n\t                tp_dict[pred_label][\"tp\"][iou_idx][-1] = 1\n\t                detected_gts[iou_idx].append(list(gt_box))\n\t            else:\n\t                tp_dict[pred_label][\"fp\"][iou_idx][-1] = 1\n", "    return tp_dict\n\tdef compute_ap(recall, precision):\n\t    \"\"\"\n\t    Compute the average precision, given the recall and precision curves.\n\t    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n\t    :param recall: The recall curve (list).\n\t    :param precision: The precision curve (list).\n\t    :return: The average precision as computed in py-faster-rcnn.\n\t    \"\"\"\n\t    # correct AP calculation\n", "    # first append sentinel values at the end\n\t    mrec = np.concatenate(([0.0], recall, [1.0]))\n\t    mpre = np.concatenate(([0.0], precision, [0.0]))\n\t    # compute the precision envelope\n\t    for i in range(mpre.size - 1, 0, -1):\n\t        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\t    # to calculate area under PR curve, look for points\n\t    # where X axis (recall) changes value\n\t    i = np.where(mrec[1:] != mrec[:-1])[0]\n\t    # and sum (\\Delta recall) * prec\n", "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\t    return ap\n\tdef compute_ap_class(tp, fp, scores, total_gt):\n\t    \"\"\"\n\t    Compute the AP given tp, fp, scores and number of ground truth of a class\n\t    See: https://github.com/keshik6/KITTI-2d-object-detection for some part of this code.\n\t    :param tp: true positives list\n\t    :param fp: false positives list\n\t    :param scores: scores list\n\t    :param total_gt: number of total GT in the dataset\n", "    :return: average precision of a class\n\t    \"\"\"\n\t    ap = 0\n\t    # Array manipulation\n\t    tp = np.array(tp)\n\t    fp = np.array(fp)\n\t    scores = np.array(scores)\n\t    # Sort detection by scores\n\t    indices = np.argsort(-scores)\n\t    fp = fp[indices]\n", "    tp = tp[indices]\n\t    # compute false positives and true positives\n\t    fp = np.cumsum(fp)\n\t    tp = np.cumsum(tp)\n\t    # compute precision/recall\n\t    recall = tp / total_gt\n\t    precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n\t    # compute ap\n\t    average_precision = compute_ap(recall, precision)\n\t    if len(precision) == 0:\n", "        mean_pre = 0.0\n\t    else:\n\t        mean_pre = np.mean(precision)\n\t    if len(recall) == 0:\n\t        mean_rec = 0.0\n\t    else:\n\t        mean_rec = np.mean(recall)\n\t    return average_precision, mean_pre, mean_rec\n\tdef compute_f1(precision, recall):\n\t    \"\"\"\n", "    Compute F1 score\n\t    :param precision:\n\t    :param recall:\n\t    :return: F1-score\n\t    \"\"\"\n\t    return 2 * (precision * recall) / (precision + recall + 1e-16)\n\tdef AP(tp_dict, n_classes, iou_th=[0.5]):\n\t    \"\"\"\n\t    Create a dictionary containing ap per class\n\t    :param tp_dict:\n", "    :param n_classes:\n\t    :return: A dictionary with the AP for each class\n\t    \"\"\"\n\t    ap_dict = dict()\n\t    # Ignore 0 class which is BG\n\t    for class_id in range(n_classes):\n\t        tp, fp, scores, total_gt = tp_dict[class_id][\"tp\"], tp_dict[class_id][\"fp\"], tp_dict[class_id][\"scores\"], \\\n\t            tp_dict[class_id][\"total_gt\"]\n\t        ap_dict[class_id] = [[] for _ in range(len(iou_th))]\n\t        ap_dict[class_id] = {\n", "            \"AP\": [0.0 for _ in range(len(iou_th))],\n\t            \"precision\": [0.0 for _ in range(len(iou_th))],\n\t            \"recall\": [0.0 for _ in range(len(iou_th))],\n\t            \"F1\": [0.0 for _ in range(len(iou_th))]\n\t        }\n\t        for iou_idx in range(len(iou_th)):\n\t            ap_dict[class_id][\"AP\"][iou_idx], ap_dict[class_id][\"precision\"][iou_idx], ap_dict[class_id][\"recall\"][\n\t                iou_idx] \\\n\t                = compute_ap_class(tp[iou_idx], fp[iou_idx], scores[iou_idx], total_gt)\n\t            ap_dict[class_id][\"F1\"][iou_idx] = compute_f1(ap_dict[class_id][\"precision\"][iou_idx],\n", "                                                          ap_dict[class_id][\"recall\"][iou_idx])\n\t    ap_dict[\"mean\"] = {\n\t        \"AP\": [np.mean([ap_dict[class_id][\"AP\"][iou_th] for class_id in range(n_classes)])\n\t               for iou_th in range(len(ap_dict[0][\"AP\"]))],\n\t        \"precision\": [np.mean([ap_dict[class_id][\"precision\"][iou_th] for class_id in range(n_classes)])\n\t                      for iou_th in range(len(ap_dict[0][\"precision\"]))],\n\t        \"recall\": [np.mean([ap_dict[class_id][\"recall\"][iou_th] for class_id in range(n_classes)])\n\t                   for iou_th in range(len(ap_dict[0][\"recall\"]))],\n\t        \"F1\": [np.mean([ap_dict[class_id][\"F1\"][iou_th] for class_id in range(n_classes)])\n\t               for iou_th in range(len(ap_dict[0][\"F1\"]))]\n", "    }\n\t    return ap_dict\n\tdef write_ap(ap_dict, save_path):\n\t    with open(os.path.join(save_path, \"test_results.json\"), 'w') as ap_file:\n\t        json.dump(ap_dict, ap_file, indent=2)\n"]}
{"filename": "darod/metrics/__init__.py", "chunked_list": []}
{"filename": "darod/models/darod_blocks.py", "chunked_list": ["import tensorflow_addons as tfa\n\tfrom tensorflow.keras import layers\n\tclass DARODBlock2D(layers.Layer):\n\t    \"\"\"\n\t    Build a 2D convolutional block with num_conv convolution operations.\n\t    \"\"\"\n\t    def __init__(self, filters, padding=\"same\", kernel_size=(3, 3), num_conv=2, dilation_rate=(1, 1),\n\t                 strides=(1, 1), activation=\"leaky_relu\", block_norm=None,\n\t                 pooling_size=(2, 2), pooling_strides=(2, 2), name=None):\n\t        \"\"\"\n", "        :param filters: number of filters inside the block\n\t        :param padding:  \"same\" or \"valid\". Same padding is applied to all convolutions\n\t        :param kernel_size: the size of the kernel\n\t        :param num_conv: number of convolution operations (2 or 3)\n\t        :param dilation_rate: dilation rate to use inside the block\n\t        :param strides: strides for the convolution operations\n\t        :param activation: activation function to apply. Possible operations: ReLu, Leaky ReLu\n\t        :param block_norm: normalization operation during training. Possible operations: batch norm, group norm\n\t        :param pooling_size: size of the last pooling operation. If None, no pooling is applied.\n\t        :param pooling_strides: strides of the last pooling opeation.\n", "        :param name: name of the block\n\t        \"\"\"\n\t        super(DARODBlock2D, self).__init__(name=name)\n\t        #\n\t        self.filters = filters\n\t        self.padding = padding\n\t        self.kernel_size = kernel_size\n\t        self.num_conv = num_conv\n\t        self.dilation_rate = dilation_rate\n\t        self.strides = strides\n", "        self.pooling_size = pooling_size\n\t        self.pooling_strides = pooling_strides\n\t        self.activation = activation\n\t        self.block_norm = block_norm\n\t        #\n\t        assert self.num_conv == 2 or self.num_conv == 3, print(\n\t            ValueError(\"Inappropriate number of convolutions used for this block.\"))\n\t        self.conv1 = layers.Conv2D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides,\n\t                                   padding=self.padding, dilation_rate=self.dilation_rate, name=self.name + \"_conv_1\")\n\t        #\n", "        if block_norm == \"batch_norm\":\n\t            self.norm1 = layers.BatchNormalization(name=name + \"_normalization_1\")\n\t        elif block_norm == \"group_norm\":\n\t            self.norm1 = tfa.layers.GroupNormalization(name=name + \"_normalization_1\")\n\t        elif block_norm == \"layer_norm\":\n\t            self.norm1 = tfa.layers.GroupNormalization(groups=1, name=name + \"_normalization_1\")\n\t        elif block_norm == \"instance_norm\":\n\t            assert filters is not None, print(\n\t                \"Error. Please provide input number of filters for instance normalization.\")\n\t            self.norm1 = tfa.layers.GroupNormalization(groups=filters, name=name + \"_normalization_1\")\n", "        elif block_norm is None:\n\t            pass\n\t        else:\n\t            raise NotImplementedError(\"Error. Please ue either batch norm or group norm or layer norm or instance norm or None \\\n\t                for normalization.\")\n\t        #\n\t        if activation == \"relu\":\n\t            self.activation1 = layers.ReLU(name=name + \"_activation_1\")\n\t        elif activation == \"leaky_relu\":\n\t            self.activation1 = layers.LeakyReLU(alpha=0.001, name=name + \"_activation_1\")\n", "        else:\n\t            raise NotImplementedError(\"Please use ReLu or Leaky ReLu activation function.\")\n\t        self.conv2 = layers.Conv2D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides,\n\t                                   padding=self.padding, dilation_rate=self.dilation_rate, name=self.name + \"_conv_2\")\n\t        #\n\t        if block_norm == \"batch_norm\":\n\t            self.norm2 = layers.BatchNormalization(name=name + \"_normalization_2\")\n\t        elif block_norm == \"group_norm\":\n\t            self.norm2 = tfa.layers.GroupNormalization(name=name + \"_normalization_2\")\n\t        elif block_norm == \"layer_norm\":\n", "            self.norm2 = tfa.layers.GroupNormalization(groups=1, name=name + \"_normalization_2\")\n\t        elif block_norm == \"instance_norm\":\n\t            assert filters is not None, print(\n\t                \"Error. Please provide input number of filters for instance normalization.\")\n\t            self.norm2 = tfa.layers.GroupNormalization(groups=filters, name=name + \"_normalization_2\")\n\t        elif block_norm is None:\n\t            pass\n\t        else:\n\t            raise NotImplementedError(\"Error. Please ue either batch norm or group norm or layer norm or instance norm or None \\\n\t                for normalization.\")\n", "        #\n\t        if activation == \"relu\":\n\t            self.activation2 = layers.ReLU(name=name + \"_activation_2\")\n\t        elif activation == \"leaky_relu\":\n\t            self.activation2 = layers.LeakyReLU(alpha=0.001, name=name + \"_activation_2\")\n\t        else:\n\t            raise NotImplementedError(\"Please use ReLu or Leaky ReLu activation function.\")\n\t        if num_conv == 3:\n\t            self.conv3 = layers.Conv2D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides,\n\t                                       padding=self.padding, dilation_rate=self.dilation_rate,\n", "                                       name=self.name + \"_conv_3\")\n\t            #\n\t            if block_norm == \"batch_norm\":\n\t                self.norm3 = layers.BatchNormalization(name=name + \"_normalization_3\")\n\t            elif block_norm == \"group_norm\":\n\t                self.norm3 = tfa.layers.GroupNormalization(name=name + \"_normalization_3\")\n\t            elif block_norm == \"layer_norm\":\n\t                self.norm3 = tfa.layers.GroupNormalization(groups=1, name=name + \"_normalization_3\")\n\t            elif block_norm == \"instance_norm\":\n\t                assert filters is not None, print(\n", "                    \"Error. Please provide input number of filters for instance normalization.\")\n\t                self.norm3 = tfa.layers.GroupNormalization(groups=filters, name=name + \"_normalization_3\")\n\t            elif block_norm is None:\n\t                pass\n\t            else:\n\t                raise NotImplementedError(\"Error. Please ue either batch norm or group norm or layer norm or instance norm or None \\\n\t                    for normalization.\")\n\t            #\n\t            if activation == \"relu\":\n\t                self.activation3 = layers.ReLU(name=name + \"_activation_3\")\n", "            elif activation == \"leaky_relu\":\n\t                self.activation3 = layers.LeakyReLU(alpha=0.001, name=name + \"_activation_3\")\n\t            else:\n\t                raise NotImplementedError(\"Please use ReLu or Leaky ReLu activation function.\")\n\t        self.maxpooling = layers.MaxPooling2D(pool_size=pooling_size, strides=pooling_strides, name=name + \"_maxpool\")\n\t    def call(self, inputs, training=False):\n\t        \"\"\"\n\t        DAROD forward pass\n\t        :param inputs: RD spectrum\n\t        :param training: training or inference\n", "        :return: features\n\t        \"\"\"\n\t        x = self.conv1(inputs)\n\t        if self.block_norm is not None:\n\t            x = self.norm1(x, training=training)\n\t        x = self.activation1(x)\n\t        x = self.conv2(x)\n\t        if self.block_norm is not None:\n\t            x = self.norm2(x, training=training)\n\t        x = self.activation2(x)\n", "        if self.num_conv == 3:\n\t            x = self.conv3(x)\n\t            if self.block_norm is not None:\n\t                x = self.norm3(x, training=training)\n\t            x = self.activation3(x)\n\t        x = self.maxpooling(x)\n\t        return x\n"]}
{"filename": "darod/models/model.py", "chunked_list": ["import tensorflow as tf\n\tfrom tensorflow.keras import layers\n\tfrom .darod_blocks import DARODBlock2D\n\tfrom ..layers.frcnn_layers import RoIBBox, RoIPooling, RadarFeatures\n\tfrom ..utils import bbox_utils\n\tclass Decoder(tf.keras.layers.Layer):\n\t    \"\"\"\n\t    Generating bounding boxes and labels from faster rcnn predictions.\n\t    First calculating the boxes from predicted deltas and label probs.\n\t    Then applied non max suppression and selecting top_n boxes by scores.\n", "    \"\"\"\n\t    \"\"\"\n\t    inputs:\n\t        roi_bboxes = (batch_size, roi_bbox_size, [y1, x1, y2, x2])\n\t        pred_deltas = (batch_size, roi_bbox_size, total_labels * [delta_y, delta_x, delta_h, delta_w])\n\t        pred_label_probs = (batch_size, roi_bbox_size, total_labels)\n\t    outputs:\n\t        pred_bboxes = (batch_size, top_n, [y1, x1, y2, x2])\n\t        pred_labels = (batch_size, top_n)\n\t            1 to total label number\n", "        pred_scores = (batch_size, top_n)\n\t    \"\"\"\n\t    def __init__(self, variances, total_labels, max_total_size=100, score_threshold=0.05, iou_threshold=0.5, **kwargs):\n\t        \"\"\"\n\t        :param variances: bbox variances\n\t        :param total_labels: number of classes\n\t        :param max_total_size: max number of predictions\n\t        :param score_threshold: score threshold\n\t        :param iou_threshold: iou threshold\n\t        :param kwargs: other args\n", "        \"\"\"\n\t        super(Decoder, self).__init__(**kwargs)\n\t        self.variances = variances\n\t        self.total_labels = total_labels\n\t        self.max_total_size = max_total_size\n\t        self.score_threshold = score_threshold\n\t        self.iou_threshold = iou_threshold\n\t    def get_config(self):\n\t        config = super(Decoder, self).get_config()\n\t        config.update({\n", "            \"variances\": self.variances,\n\t            \"total_labels\": self.total_labels,\n\t            \"max_total_size\": self.max_total_size,\n\t            \"score_threshold\": self.score_threshold,\n\t            \"iou_threshold\": self.iou_threshold\n\t        })\n\t        return config\n\t    def call(self, inputs):\n\t        \"\"\"\n\t        Make final predictions from DAROD outputs\n", "        :param inputs: DAROD outputs (roi boxes, deltas and probas)\n\t        :return: final predictions (boxes, classes, scores)\n\t        \"\"\"\n\t        roi_bboxes = inputs[0]\n\t        pred_deltas = inputs[1]\n\t        pred_label_probs = inputs[2]\n\t        batch_size = tf.shape(pred_deltas)[0]\n\t        #\n\t        pred_deltas = tf.reshape(pred_deltas, (batch_size, -1, self.total_labels, 4))\n\t        pred_deltas *= self.variances\n", "        #\n\t        expanded_roi_bboxes = tf.tile(tf.expand_dims(roi_bboxes, -2), (1, 1, self.total_labels, 1))\n\t        pred_bboxes = bbox_utils.get_bboxes_from_deltas(expanded_roi_bboxes, pred_deltas)\n\t        pred_bboxes = tf.clip_by_value(pred_bboxes, clip_value_min=0.0, clip_value_max=1.0)\n\t        #\n\t        pred_labels_map = tf.expand_dims(tf.argmax(pred_label_probs, -1), -1)\n\t        pred_labels = tf.where(tf.not_equal(pred_labels_map, 0), pred_label_probs, tf.zeros_like(pred_label_probs))\n\t        #\n\t        final_bboxes, final_scores, final_labels, _ = bbox_utils.non_max_suppression(\n\t            pred_bboxes, pred_labels,\n", "            iou_threshold=self.iou_threshold,\n\t            max_output_size_per_class=self.max_total_size,\n\t            max_total_size=self.max_total_size,\n\t            score_threshold=self.score_threshold)\n\t        #\n\t        return tf.stop_gradient(final_bboxes), tf.stop_gradient(final_labels), tf.stop_gradient(final_scores)\n\tclass DAROD(tf.keras.Model):\n\t    def __init__(self, config, anchors):\n\t        \"\"\"\n\t        Implement DAROD model\n", "        :param config: configuration dictionary to build the model\n\t        :param anchors: generated anchors.\n\t        \"\"\"\n\t        super(DAROD, self).__init__()\n\t        self.config = config\n\t        self.anchors = anchors\n\t        #\n\t        self.use_doppler = config[\"training\"][\"use_doppler\"]\n\t        self.use_dropout = config[\"training\"][\"use_dropout\"]\n\t        self.variances = config[\"fastrcnn\"][\"variances_boxes\"]\n", "        self.total_labels = config[\"data\"][\"total_labels\"]\n\t        self.frcnn_num_pred = config[\"fastrcnn\"][\"frcnn_num_pred\"]\n\t        self.box_nms_iou = config[\"fastrcnn\"][\"box_nms_iou\"]\n\t        self.box_nms_score = config[\"fastrcnn\"][\"box_nms_score\"]\n\t        self.dropout_rate = config[\"training\"][\"dropout_rate\"]\n\t        self.layout = config[\"model\"][\"layout\"]\n\t        self.use_bn = config[\"training\"][\"use_bn\"]\n\t        self.dilation_rate = config[\"model\"][\"dilation_rate\"]\n\t        #\n\t        if self.use_bn:\n", "            block_norm = \"group_norm\"  # By default for this model\n\t        else:\n\t            block_norm = None\n\t        # Backbone\n\t        self.block1 = DARODBlock2D(filters=64, padding=\"same\", kernel_size=(3, 3),\n\t                                   num_conv=2, dilation_rate=self.dilation_rate, activation=\"leaky_relu\",\n\t                                   block_norm=block_norm, pooling_size=(2, 2),\n\t                                   pooling_strides=(2, 2), name=\"darod_block1\")\n\t        self.block2 = DARODBlock2D(filters=128, padding=\"same\", kernel_size=(3, 3),\n\t                                   num_conv=2, dilation_rate=self.dilation_rate, activation=\"leaky_relu\",\n", "                                   block_norm=block_norm, pooling_size=(2, 1),\n\t                                   pooling_strides=(2, 1), name=\"darod_block2\")\n\t        self.block3 = DARODBlock2D(filters=256, padding=\"same\", kernel_size=(3, 3),\n\t                                   num_conv=3, dilation_rate=(1, 1), activation=\"leaky_relu\",\n\t                                   block_norm=block_norm, pooling_size=(2, 1),\n\t                                   pooling_strides=(2, 1), name=\"darod_block3\")\n\t        # RPN\n\t        self.rpn_conv = layers.Conv2D(config[\"rpn\"][\"rpn_channels\"], config[\"rpn\"][\"rpn_window\"], padding=\"same\",\n\t                                      activation=\"relu\", name=\"rpn_conv\")\n\t        self.rpn_cls_output = layers.Conv2D(config[\"rpn\"][\"anchor_count\"], (1, 1), activation=\"linear\", name=\"rpn_cls\")\n", "        self.rpn_reg_output = layers.Conv2D(4 * config[\"rpn\"][\"anchor_count\"], (1, 1), activation=\"linear\",\n\t                                            name=\"rpn_reg\")\n\t        # Fast RCNN\n\t        self.roi_bbox = RoIBBox(anchors, config, name=\"roi_bboxes\")\n\t        self.radar_features = RadarFeatures(config, name=\"radar_features\")\n\t        self.roi_pooled = RoIPooling(config, name=\"roi_pooling\")\n\t        #\n\t        self.flatten = layers.Flatten(name=\"frcnn_flatten\")\n\t        if self.use_dropout:\n\t            self.dropout = layers.Dropout(rate=self.dropout_rate)\n", "        self.fc1 = layers.Dense(config[\"fastrcnn\"][\"in_channels_1\"], activation=\"relu\", name=\"frcnn_fc1\")\n\t        self.fc2 = layers.Dense(config[\"fastrcnn\"][\"in_channels_2\"], activation=\"relu\", name=\"frcnn_fc2\")\n\t        self.frcnn_cls = layers.Dense(config[\"data\"][\"total_labels\"], activation='linear', name=\"frcnn_cls\")\n\t        self.frcnn_reg = layers.Dense(config[\"data\"][\"total_labels\"] * 4, activation=\"linear\", name=\"frcnn_reg\")\n\t        self.decoder = Decoder(variances=self.variances, total_labels=self.total_labels,\n\t                               max_total_size=self.frcnn_num_pred,\n\t                               score_threshold=self.box_nms_score, iou_threshold=self.box_nms_iou)\n\t    def call(self, inputs, step=\"frcnn\", training=False):\n\t        \"\"\"\n\t        DAROD forward pass\n", "        :param inputs: RD spectrum\n\t        :param step: RPN (for RPN pretraining) or Fast R-CNN for end to end training\n\t        :param training: training or inference\n\t        :return: RPN predictions (bbox regression and class) and/or Fast R-CNN predictions (bbox regression and object class)\n\t        \"\"\"\n\t        x = self.block1(inputs, training=training)\n\t        x = self.block2(x, training=training)\n\t        x = self.block3(x, training=training)\n\t        # RPN forward pass\n\t        rpn_out = self.rpn_conv(x)\n", "        rpn_cls_pred = self.rpn_cls_output(rpn_out)\n\t        rpn_delta_pred = self.rpn_reg_output(rpn_out)\n\t        if step == \"rpn\":\n\t            return rpn_cls_pred, rpn_delta_pred\n\t        # Fast RCNN part\n\t        roi_bboxes_out, roi_bboxes_scores = self.roi_bbox([rpn_delta_pred, rpn_cls_pred], training=training)\n\t        roi_pooled_out = self.roi_pooled([x, roi_bboxes_out])\n\t        output = layers.TimeDistributed(self.flatten)(roi_pooled_out)\n\t        features = self.radar_features([roi_bboxes_out, roi_bboxes_scores], training=training)\n\t        output = tf.concat([output, features], -1)\n", "        output = layers.TimeDistributed(self.fc1)(output)\n\t        if self.use_dropout:\n\t            output = layers.TimeDistributed(self.dropout, name=\"dropout_1\")(output)\n\t        output = layers.TimeDistributed(self.fc2)(output)\n\t        if self.use_dropout:\n\t            output = layers.TimeDistributed(self.dropout, name=\"dropout_2\")(output)\n\t        #\n\t        frcnn_cls_pred = layers.TimeDistributed(self.frcnn_cls)(output)\n\t        frcnn_reg_pred = layers.TimeDistributed(self.frcnn_reg)(output)\n\t        # Decoder part\n", "        decoder_output = self.decoder([roi_bboxes_out, frcnn_reg_pred, tf.nn.softmax(frcnn_cls_pred)])\n\t        return rpn_cls_pred, rpn_delta_pred, frcnn_cls_pred, frcnn_reg_pred, roi_bboxes_out, decoder_output\n"]}
{"filename": "darod/models/__init__.py", "chunked_list": []}
{"filename": "darod/trainers/__init__.py", "chunked_list": []}
{"filename": "darod/trainers/trainer.py", "chunked_list": ["import json\n\timport os\n\timport numpy as np\n\timport tensorflow as tf\n\timport tensorflow_addons as tfa\n\tfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\tfrom ..layers.frcnn_layers import RoIDelta\n\tfrom ..metrics import mAP\n\tfrom ..utils import train_utils, log_utils\n\tclass Trainer:\n", "    \"\"\"\n\t    Class to train DAROD model.\n\t    \"\"\"\n\t    def __init__(self, config, model, labels, experiment_name, backbone,\n\t                 backup_dir=\"/home/nxf67149/Documents/codes/DAROD/saves/\"):\n\t        \"\"\"\n\t        :param config: configuration dictionary with training settings\n\t        :param model: the model to train\n\t        :param labels (list): class labels\n\t        :param experiment_name: name of the experiment\n", "        :param backbone: backbone to use (DAROD or vision based one)\n\t        :param backup_dir: directory to save logs\n\t        \"\"\"\n\t        self.config = config\n\t        self.eval_every = self.config[\"training\"][\"eval_every\"]\n\t        self.lr = self.config[\"training\"][\"lr\"]\n\t        optimizer = self.config[\"training\"]['optimizer']\n\t        self.scheduler = self.config[\"training\"][\"scheduler\"]\n\t        momentum = self.config[\"training\"][\"momentum\"]\n\t        self.variances_boxes = self.config[\"fastrcnn\"][\"variances_boxes\"]\n", "        self.total_labels = self.config[\"data\"][\"total_labels\"]\n\t        self.frcnn_num_pred = self.config[\"fastrcnn\"][\"frcnn_num_pred\"]\n\t        self.box_nms_score = self.config[\"fastrcnn\"]['box_nms_score']\n\t        self.box_nms_iou = self.config[\"fastrcnn\"]['box_nms_iou']\n\t        #\n\t        decay_step = self.config[\"training\"][\"scheduler_step\"]\n\t        if self.scheduler:\n\t            self.lr = ExponentialDecay(self.lr, decay_rate=0.9, staircase=True,\n\t                                       decay_steps=(config[\"training\"][\"num_steps_epoch\"] / config[\"training\"][\n\t                                           \"batch_size\"]) * decay_step)\n", "        if optimizer == \"SGD\":\n\t            self.optimizer = tf.optimizers.SGD(learning_rate=self.lr, momentum=momentum)\n\t        elif optimizer == \"WSGD\":\n\t            self.optimizer = tfa.optimizers.SGDW(learning_rate=self.lr, momentum=momentum, weight_decay=0.0005)\n\t        elif optimizer == \"adam\":\n\t            self.optimizer = tf.optimizers.Adam(learning_rate=self.lr)\n\t        elif optimizer == \"adad\":\n\t            self.optimizer = tf.optimizers.Adadelta(learning_rate=1.0)\n\t        elif optimizer == \"adag\":\n\t            self.optimizer = tf.optimizers.Adagrad(learning_rate=self.lr)\n", "        else:\n\t            raise NotImplemented(\"Not supported optimizer {}\".format(optimizer))\n\t        #\n\t        self.experience_name = experiment_name\n\t        self.backbone = backbone\n\t        #\n\t        self.labels = labels\n\t        self.n_epochs = self.config[\"training\"][\"epochs\"]\n\t        self.model = model\n\t        self.global_step = tf.Variable(0, trainable=False, dtype=tf.int64)\n", "        self.backup_dir = os.path.join(backup_dir, experiment_name)\n\t        if not os.path.exists(self.backup_dir):\n\t            os.makedirs(self.backup_dir)\n\t        with open(os.path.join(self.backup_dir, 'config.json'), 'w') as file:\n\t            json.dump(config, file, indent=2)\n\t        # Tensorboard setting\n\t        self.summary_writer = tf.summary.create_file_writer(self.backup_dir)\n\t        # Checkpoint manager\n\t        self.ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model, step=self.global_step)\n\t        self.manager = tf.train.CheckpointManager(self.ckpt, self.backup_dir, max_to_keep=5)\n", "        self.ckpt.restore(self.manager.latest_checkpoint)\n\t        if self.manager.latest_checkpoint:\n\t            print(\"Restored from {}\".format(self.manager.latest_checkpoint))\n\t            self.global_step.assign(self.ckpt.step.numpy())\n\t    def init_metrics(self):\n\t        \"\"\"\n\t        Initialise metrics for training\n\t        :return:\n\t        \"\"\"\n\t        self.train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n", "        self.train_rpn_reg_loss = tf.keras.metrics.Mean(name=\"train_rpn_reg_loss\")\n\t        self.train_rpn_cls_loss = tf.keras.metrics.Mean(name=\"train_rpn_cls_loss\")\n\t        self.train_frcnn_reg_loss = tf.keras.metrics.Mean(name=\"train_frcnn_reg_loss\")\n\t        self.train_frcnn_cls_loss = tf.keras.metrics.Mean(name=\"train_frcnn_cls_loss\")\n\t        self.val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n\t        self.val_rpn_reg_loss = tf.keras.metrics.Mean(name=\"val_rpn_reg_loss\")\n\t        self.val_rpn_cls_loss = tf.keras.metrics.Mean(name=\"val_rpn_cls_loss\")\n\t        self.val_frcnn_reg_loss = tf.keras.metrics.Mean(name=\"val_frcnn_reg_loss\")\n\t        self.val_frcnn_cls_loss = tf.keras.metrics.Mean(name=\"val_frcnn_cls_loss\")\n\t    def reset_all_metrics(self):\n", "        \"\"\"\n\t        Reset all metrics\n\t        :return:\n\t        \"\"\"\n\t        self.train_loss.reset_states()\n\t        self.train_rpn_reg_loss.reset_states()\n\t        self.train_rpn_cls_loss.reset_states()\n\t        self.train_frcnn_reg_loss.reset_states()\n\t        self.train_frcnn_cls_loss.reset_states()\n\t        self.val_loss.reset_states()\n", "        self.val_rpn_reg_loss.reset_states()\n\t        self.val_rpn_cls_loss.reset_states()\n\t        self.val_frcnn_reg_loss.reset_states()\n\t        self.val_frcnn_cls_loss.reset_states()\n\t    def update_train_metrics(self, loss, rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss):\n\t        \"\"\"\n\t        Update train metrics\n\t        :param loss: total loss\n\t        :param rpn_reg_loss: rpn regression loss\n\t        :param rpn_cls_loss: rpn classification loss\n", "        :param frcnn_reg_loss: fast rcnn regression loss\n\t        :param frcnn_cls_loss: fast rcnn classification loss\n\t        :return:\n\t        \"\"\"\n\t        self.train_loss(loss)\n\t        self.train_rpn_reg_loss(rpn_reg_loss)\n\t        self.train_rpn_cls_loss(rpn_cls_loss)\n\t        self.train_frcnn_reg_loss(frcnn_reg_loss)\n\t        self.train_frcnn_cls_loss(frcnn_cls_loss)\n\t    def update_val_metrics(self, loss, rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss):\n", "        \"\"\"\n\t        Update val metrics\n\t        :param loss: total loss\n\t        :param rpn_reg_loss: rpn regression loss\n\t        :param rpn_cls_loss: rpn classification loss\n\t        :param frcnn_reg_loss: fast rcnn regression loss\n\t        :param frcnn_cls_loss: fast rcnn classification loss\n\t        :return:\n\t        \"\"\"\n\t        self.val_loss(loss)\n", "        self.val_rpn_reg_loss(rpn_reg_loss)\n\t        self.val_rpn_cls_loss(rpn_cls_loss)\n\t        self.val_frcnn_reg_loss(frcnn_reg_loss)\n\t        self.val_frcnn_cls_loss(frcnn_cls_loss)\n\t    def train_step(self, input_data, anchors, epoch):\n\t        \"\"\"\n\t        Performs one training step (forward pass + optimisation)\n\t        :param input_data: RD spectrum\n\t        :param anchors: anchors\n\t        :param epoch: epoch number\n", "        :return:\n\t        \"\"\"\n\t        spectrums, gt_boxes, gt_labels, is_same_seq, _, _ = input_data\n\t        # Take only valid idxs\n\t        valid_idxs = tf.where(is_same_seq == 1)\n\t        spectrums, gt_boxes, gt_labels = tf.gather_nd(spectrums, valid_idxs), tf.gather_nd(gt_boxes,\n\t                                                                                           valid_idxs), tf.gather_nd(\n\t            gt_labels, valid_idxs)\n\t        if spectrums.shape[0] != 0:\n\t            # Get RPN labels\n", "            bbox_deltas, bbox_labels = train_utils.calculate_rpn_actual_outputs(anchors, gt_boxes, gt_labels,\n\t                                                                                self.config)\n\t            # Train step\n\t            with tf.GradientTape() as tape:\n\t                rpn_cls_pred, rpn_delta_pred, frcnn_cls_pred, frcnn_reg_pred, roi_bboxes, _ = self.model(spectrums,\n\t                                                                                                         training=True)\n\t                frcnn_reg_actuals, frcnn_cls_actuals = RoIDelta(self.config)([roi_bboxes, gt_boxes, gt_labels])\n\t                rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss = train_utils.darod_loss(rpn_cls_pred,\n\t                                                                                                   rpn_delta_pred,\n\t                                                                                                   frcnn_cls_pred,\n", "                                                                                                   frcnn_reg_pred,\n\t                                                                                                   bbox_labels,\n\t                                                                                                   bbox_deltas,\n\t                                                                                                   frcnn_reg_actuals,\n\t                                                                                                   frcnn_cls_actuals)\n\t            losses = rpn_reg_loss + rpn_cls_loss + frcnn_reg_loss + frcnn_cls_loss\n\t            self.update_train_metrics(losses, rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss)\n\t            gradients = tape.gradient([rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss],\n\t                                      self.model.trainable_variables)\n\t            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n", "            if self.scheduler:\n\t                cur_lr = self.optimizer.lr(self.global_step)\n\t            print(\"===> ite(epoch) {}({})/{}({})  <==> train loss {} <==> rpn_reg_loss {} <==> rpn_cls_loss {} \\ \"\n\t                  \"<==> frcnn_reg_loss {} <==> frcnn_cls_loss {}\".format(self.global_step.value().numpy(), epoch,\n\t                                                                         self.total_ite, self.n_epochs, losses.numpy(),\n\t                                                                         rpn_reg_loss.numpy(), rpn_cls_loss.numpy(),\n\t                                                                         frcnn_reg_loss.numpy(),\n\t                                                                         frcnn_cls_loss.numpy()))\n\t            with self.summary_writer.as_default():\n\t                tf.summary.scalar(\"train_loss/total_loss\", losses, step=self.global_step)\n", "                tf.summary.scalar(\"train_loss/rpn_reg_loss\", rpn_reg_loss, step=self.global_step)\n\t                tf.summary.scalar(\"train_loss/rpn_cls_loss\", rpn_cls_loss, step=self.global_step)\n\t                tf.summary.scalar(\"train_loss/frcnn_reg_loss\", frcnn_reg_loss, step=self.global_step)\n\t                tf.summary.scalar(\"train_loss/frcnn_cls_loss\", frcnn_cls_loss, step=self.global_step)\n\t                if not self.scheduler:\n\t                    tf.summary.scalar(\"lr\", self.optimizer.learning_rate.numpy(), step=self.global_step)\n\t                else:\n\t                    tf.summary.scalar(\"lr\", cur_lr.numpy(), step=self.global_step)\n\t            if np.isnan(losses):\n\t                raise ValueError(\"Get NaN at iteration \", self.ite)\n", "            self.global_step.assign_add(1)\n\t    def val_step(self, input_data, anchors):\n\t        \"\"\"\n\t        Performs one validation step\n\t        :param input_data: RD spectrum\n\t        :param anchors: anchors\n\t        :return: ground truth boxes, labels and predictions\n\t        \"\"\"\n\t        spectrums, gt_boxes, gt_labels, is_same_seq, _, _ = input_data\n\t        # Take only valid idxs\n", "        valid_idxs = tf.where(is_same_seq == 1)\n\t        spectrums, gt_boxes, gt_labels = tf.gather_nd(spectrums, valid_idxs), tf.gather_nd(gt_boxes,\n\t                                                                                           valid_idxs), tf.gather_nd(\n\t            gt_labels, valid_idxs)\n\t        if spectrums.shape[0] != 0:\n\t            # Get RPN labels\n\t            bbox_deltas, bbox_labels = train_utils.calculate_rpn_actual_outputs(anchors, gt_boxes, gt_labels,\n\t                                                                                self.config)\n\t            rpn_cls_pred, rpn_delta_pred, frcnn_cls_pred, frcnn_reg_pred, roi_bboxes, decoder_output = self.model(\n\t                spectrums, training=True)\n", "            if self.config[\"fastrcnn\"][\"reg_loss\"] == \"sl1\":\n\t                # Smooth L1-loss\n\t                frcnn_reg_actuals, frcnn_cls_actuals = RoIDelta(self.config)([roi_bboxes, gt_boxes, gt_labels])\n\t                rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss = train_utils.darod_loss(rpn_cls_pred,\n\t                                                                                                   rpn_delta_pred,\n\t                                                                                                   frcnn_cls_pred,\n\t                                                                                                   frcnn_reg_pred,\n\t                                                                                                   bbox_labels,\n\t                                                                                                   bbox_deltas,\n\t                                                                                                   frcnn_reg_actuals,\n", "                                                                                                   frcnn_cls_actuals)\n\t            elif self.config[\"fastrcnn\"][\"reg_loss\"] == \"giou\":\n\t                # Generalized IoU loss\n\t                expanded_roi_bboxes, expanded_gt_boxes, frcnn_cls_actuals = RoIDelta(self.config)(\n\t                    [roi_bboxes, gt_boxes, gt_labels])\n\t                rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss = train_utils.darod_loss_giou(rpn_cls_pred,\n\t                                                                                                        rpn_delta_pred,\n\t                                                                                                        frcnn_cls_pred,\n\t                                                                                                        frcnn_reg_pred,\n\t                                                                                                        bbox_labels,\n", "                                                                                                        bbox_deltas,\n\t                                                                                                        expanded_gt_boxes,\n\t                                                                                                        frcnn_cls_actuals,\n\t                                                                                                        expanded_roi_bboxes)\n\t            losses = rpn_reg_loss + rpn_cls_loss + frcnn_reg_loss + frcnn_cls_loss\n\t            self.update_val_metrics(losses, rpn_reg_loss, rpn_cls_loss, frcnn_reg_loss, frcnn_cls_loss)\n\t            return gt_boxes, gt_labels, decoder_output\n\t        else:\n\t            return gt_boxes, gt_labels, None\n\t    def train(self, anchors, train_dataset, val_dataset):\n", "        \"\"\"\n\t        Train the model\n\t        :param anchors: anchors\n\t        :param train_dataset: train dataset\n\t        :param val_dataset: validation dataset\n\t        :return:\n\t        \"\"\"\n\t        best_loss = np.inf\n\t        self.ite = 0\n\t        self.total_ite = int(\n", "            (self.config[\"training\"][\"num_steps_epoch\"] / self.config[\"training\"][\"batch_size\"]) * self.n_epochs)\n\t        self.init_metrics()\n\t        start_epoch = int(self.global_step.numpy() / (\n\t                    self.config[\"training\"][\"num_steps_epoch\"] / self.config[\"training\"][\"batch_size\"]))\n\t        for epoch in range(start_epoch, self.n_epochs):\n\t            # Reset metrics\n\t            self.reset_all_metrics()\n\t            # Perform one train step\n\t            for input_data in train_dataset:\n\t                self.train_step(input_data, anchors, epoch)\n", "            # Testing phase\n\t            if (epoch % self.eval_every == 0 and epoch != 0) or epoch == self.n_epochs - 1:\n\t                tp_dict = mAP.init_tp_dict(len(self.labels) - 1, iou_threshold=[0.5])\n\t            # Validation phase\n\t            for input_data in val_dataset:\n\t                gt_boxes, gt_labels, decoder_output = self.val_step(input_data, anchors)\n\t                if decoder_output is not None and (\n\t                        epoch % self.eval_every == 0 and epoch != 0) or epoch == self.n_epochs - 1:\n\t                    pred_boxes, pred_labels, pred_scores = decoder_output\n\t                    pred_labels = pred_labels - 1\n", "                    gt_labels = gt_labels - 1\n\t                    for batch_id in range(pred_boxes.shape[0]):\n\t                        tp_dict = mAP.accumulate_tp_fp(pred_boxes.numpy()[batch_id], pred_labels.numpy()[batch_id],\n\t                                                       pred_scores.numpy()[batch_id], gt_boxes.numpy()[batch_id],\n\t                                                       gt_labels.numpy()[batch_id], tp_dict,\n\t                                                       iou_thresholds=[0.5])\n\t            if (epoch % self.eval_every == 0 and epoch != 0) or epoch == self.n_epochs - 1:\n\t                ap_dict = mAP.AP(tp_dict, n_classes=len(self.labels) - 1)\n\t                log_utils.tensorboard_val_stats(self.summary_writer, ap_dict, self.labels[1:], self.global_step)\n\t                if self.val_loss.result().numpy() < best_loss:\n", "                    best_loss = self.val_loss.result().numpy()\n\t                    self.model.save_weights(os.path.join(self.backup_dir, \"best-model.h5\"))\n\t                    self.config[\"training\"][\"best_epoch\"] = epoch\n\t            # Log metrics\n\t            with self.summary_writer.as_default():\n\t                tf.summary.scalar(\"val_loss/total_loss\", self.val_loss.result().numpy(), step=self.global_step)\n\t                tf.summary.scalar(\"val_loss/rpn_reg_loss\", self.val_rpn_reg_loss.result().numpy(),\n\t                                  step=self.global_step)\n\t                tf.summary.scalar(\"val_loss/rpn_cls_loss\", self.val_rpn_cls_loss.result().numpy(),\n\t                                  step=self.global_step)\n", "                tf.summary.scalar(\"val_loss/frcnn_reg_loss\", self.val_frcnn_reg_loss.result().numpy(),\n\t                                  step=self.global_step)\n\t                tf.summary.scalar(\"val_loss/frcnn_cls_loss\", self.val_frcnn_cls_loss.result().numpy(),\n\t                                  step=self.global_step)\n\t            print(\"<=============== Validation ===============>\")\n\t            print(\"===> epoch {}/{}  <==> val loss {} <==> val rpn_reg_loss {} <==> val rpn_cls_loss {}\"\n\t                  \" <==> val frcnn_reg_loss {} <==> val frcnn_cls_loss {}\".format(\n\t                epoch, self.n_epochs, self.val_loss.result().numpy(), self.val_rpn_reg_loss.result().numpy(),\n\t                self.val_rpn_cls_loss.result().numpy(), self.val_frcnn_reg_loss.result().numpy(),\n\t                self.val_frcnn_cls_loss.result().numpy()))\n", "            print(\"<==========================================>\")\n\t            save_path = self.manager.save()\n\t            print(\"Saved checkpoint for step {}: {}\".format(int(self.ckpt.step), save_path))\n"]}
