{"filename": "stats_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport logging\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom datetime import timedelta\n\tfrom enum import Enum, auto\n\timport regexes\n\timport utils\n", "format_err_msg = utils.format_err_msg\n\tParsingAssertion = utils.ParsingAssertion\n\tErrContext = utils.ErrorContext\n\tformat_line_num_from_entry = utils.format_line_num_from_entry\n\tformat_line_num_from_line_idx = utils.format_line_num_from_line_idx\n\tget_line_num_from_entry = utils.get_line_num_from_entry\n\tdef is_empty_line(line):\n\t    return re.fullmatch(regexes.EMPTY_LINE, line) is not None\n\tdef parse_uptime_line(line, allow_mismatch=False):\n\t    # Uptime(secs): 603.0 total, 600.0 interval\n", "    match = re.search(regexes.UPTIME_STATS_LINE, line)\n\t    if not match:\n\t        if allow_mismatch:\n\t            return None\n\t        if not match:\n\t            raise ParsingAssertion(\"Failed parsing uptime line\",\n\t                                   ErrContext(**{\"line\": line}))\n\t    total_sec = float(match.group('total'))\n\t    interval_sec = float(match.group('interval'))\n\t    return total_sec, interval_sec\n", "def parse_line_with_cf(line, regex_str, allow_mismatch=False):\n\t    line_parts = re.findall(regex_str, line)\n\t    if not line_parts:\n\t        if allow_mismatch:\n\t            return None\n\t        if not line_parts:\n\t            raise ParsingAssertion(\"Failed parsing line with column-family\",\n\t                                   ErrContext(**{\"line\": line}))\n\t    cf_name = line_parts[0]\n\t    return cf_name\n", "class DbWideStatsMngr:\n\t    \"\"\" Parses and stores dumps of the db-wide stats at the top of the\n\t     DUMPING STATS dump\n\t    \"\"\"\n\t    @dataclass\n\t    class CumulativeWritesInfo:\n\t        num_writes: int = 0\n\t        num_keys: int = 0\n\t        ingest: int = 0\n\t        ingest_rate_mbps: float = 0.0\n", "    @staticmethod\n\t    def is_start_line(line):\n\t        return re.fullmatch(regexes.DB_STATS, line) is not None\n\t    def __init__(self):\n\t        self.stalls = {}\n\t        self.cumulative_writes = {}\n\t    def add_lines(self, time, db_stats_lines):\n\t        assert len(db_stats_lines) > 0\n\t        self.stalls[time] = {}\n\t        for line in db_stats_lines[1:]:\n", "            if self.try_parse_as_interval_stall_line(time, line):\n\t                continue\n\t            elif self.try_parse_as_cumulative_stall_line(time, line):\n\t                continue\n\t            elif self.try_parse_as_cumulative_writes_line(time, line):\n\t                continue\n\t        if DbWideStatsMngr.is_all_zeroes_entry(self.stalls[time]):\n\t            del self.stalls[time]\n\t    @staticmethod\n\t    def try_parse_as_stalls_line(regex, line):\n", "        line_parts = re.findall(regex, line)\n\t        if not line_parts:\n\t            return None\n\t        assert len(line_parts) == 1 and len(line_parts[0]) == 5\n\t        hours, minutes, seconds, ms, stall_percent = line_parts[0]\n\t        stall_duration = timedelta(hours=int(hours),\n\t                                   minutes=int(minutes),\n\t                                   seconds=int(seconds),\n\t                                   milliseconds=int(ms))\n\t        return stall_duration, stall_percent\n", "    def try_parse_as_interval_stall_line(self, time, line):\n\t        stall_info = DbWideStatsMngr.try_parse_as_stalls_line(\n\t            regexes.DB_WIDE_INTERVAL_STALL, line)\n\t        if stall_info is None:\n\t            return None\n\t        stall_duration, stall_percent = stall_info\n\t        self.stalls[time].update({\"interval_duration\": stall_duration,\n\t                                  \"interval_percent\": float(stall_percent)})\n\t    def try_parse_as_cumulative_stall_line(self, time, line):\n\t        stall_info = DbWideStatsMngr.try_parse_as_stalls_line(\n", "            regexes.DB_WIDE_CUMULATIVE_STALL, line)\n\t        if stall_info is None:\n\t            return None\n\t        stall_duration, stall_percent = stall_info\n\t        self.stalls[time].update({\"cumulative_duration\": stall_duration,\n\t                                  \"cumulative_percent\": float(stall_percent)})\n\t    def try_parse_as_cumulative_writes_line(self, time, line):\n\t        writes_info = re.findall(regexes.DB_WIDE_CUMULATIVE_WRITES, line)\n\t        if not writes_info:\n\t            return None\n", "        assert len(writes_info) == 1 and len(writes_info[0]) == 6\n\t        writes_info = writes_info[0]\n\t        info = DbWideStatsMngr.CumulativeWritesInfo()\n\t        info.num_writes = utils.get_number_from_human_readable_components(\n\t            writes_info[0], writes_info[1])\n\t        info.num_keys = utils.get_number_from_human_readable_components(\n\t            writes_info[2], writes_info[3])\n\t        # Although ingest is a counter it's printed in units of GB (bytes)\n\t        info.ingest = \\\n\t            utils.get_num_bytes_from_human_readable_str(f\"{writes_info[4]} GB\")\n", "        # Keep the ingest rate in MBPS as it is printed\n\t        info.ingest_rate_mbps = float(writes_info[5])\n\t        self.cumulative_writes[time] = info\n\t    @staticmethod\n\t    def is_all_zeroes_entry(entry):\n\t        interval_duration_total_secs = 0.0\n\t        interval_percent = 0.0\n\t        cumulative_duration_total_secs = 0\n\t        cumulative_percent = 0.0\n\t        if \"interval_duration\" in entry:\n", "            interval_duration_total_secs = \\\n\t                entry[\"interval_duration\"].total_seconds()\n\t        if \"interval_percent\" in entry:\n\t            interval_percent = entry[\"interval_percent\"]\n\t        if \"cumulative_duration\" in entry:\n\t            cumulative_duration_total_secs = \\\n\t                entry[\"cumulative_duration\"].total_seconds()\n\t        if \"cumulative_percent\" in entry:\n\t            interval_percent = entry[\"cumulative_percent\"]\n\t        return interval_duration_total_secs == 0.0 and \\\n", "            interval_percent == 0.0 and \\\n\t            cumulative_duration_total_secs == 0.0 and \\\n\t            cumulative_percent == 0.0\n\t    def get_stalls_entries(self):\n\t        return self.stalls\n\t    def get_cumulative_writes_entries(self):\n\t        return self.cumulative_writes\n\t    def get_last_cumulative_writes_entry(self):\n\t        if not self.cumulative_writes:\n\t            return None\n", "        return utils.get_last_dict_entry(self.cumulative_writes)\n\tclass CompactionStatsMngr:\n\t    class LineType(Enum):\n\t        LEVEL = auto()\n\t        SUM = auto()\n\t        INTERVAL = auto()\n\t        USER = auto()\n\t    class LevelFields(str, Enum):\n\t        SIZE_BYTES = 'size_bytes'\n\t        WRITE_AMP = 'W-Amp'\n", "        COMP_SEC = 'Comp(sec)'\n\t        COMP_MERGE_CPU = 'CompMergeCPU(sec)'\n\t        @staticmethod\n\t        def get_fields_list():\n\t            return list(CompactionStatsMngr.LevelFields.__members__)\n\t        @staticmethod\n\t        def has_field(field):\n\t            assert isinstance(field, CompactionStatsMngr.LevelFields)\n\t            return field.name in \\\n\t                CompactionStatsMngr.LevelFields.get_fields_list()\n", "    class CfLevelEntry:\n\t        def __init__(self, entry):\n\t            self.time = list(entry.keys())[0]\n\t            self.lines = entry[self.time]\n\t        @staticmethod\n\t        def get_level_key(level):\n\t            return f\"LEVEL-{level}\"\n\t        def get_time(self):\n\t            return self.time\n\t        def get_levels(self):\n", "            level_regex = fr\"LEVEL-{regexes.INT_C}\"\n\t            levels = []\n\t            for key in self.lines.keys():\n\t                match = re.findall(level_regex, key)\n\t                if match:\n\t                    levels.append(int(match[0]))\n\t            return levels\n\t        def get_level_line(self, level):\n\t            level_key = __class__.get_level_key(level)\n\t            if level_key not in self.lines:\n", "                return None\n\t            return self.lines[level_key]\n\t        def get_sum_line(self):\n\t            return self.lines['SUM']\n\t        def get_total_size_bytes(self):\n\t            return\n\t    @staticmethod\n\t    def parse_start_line(line, allow_mismatch=False):\n\t        return parse_line_with_cf(line, regexes.COMPACTION_STATS,\n\t                                  allow_mismatch)\n", "    @staticmethod\n\t    def is_start_line(line):\n\t        return CompactionStatsMngr.parse_start_line(line,\n\t                                                    allow_mismatch=True) \\\n\t               is not None\n\t    def __init__(self):\n\t        self.level_entries = dict()\n\t        self.priority_entries = dict()\n\t    def add_lines(self, time, cf_name, stats_lines, line_num):\n\t        stats_lines = [line.strip() for line in stats_lines]\n", "        assert cf_name == \\\n\t               CompactionStatsMngr.parse_start_line(stats_lines[0])\n\t        try:\n\t            if stats_lines[1].startswith('Level'):\n\t                self.parse_level_lines(time, cf_name, stats_lines[1:])\n\t            elif stats_lines[1].startswith('Priority'):\n\t                self.parse_priority_lines(time, cf_name, stats_lines[1:])\n\t            else:\n\t                assert 0\n\t        except utils.ParsingError as e:\n", "            logging.warning(f\"Failed parsing compaction stats lines ({e}) - \"\n\t                            f\"Ignoring these lines.\"\n\t                            f\".\\n\"\n\t                            f\"time:{time}, cf:{cf_name}, \"\n\t                            f\"lines:\\n{stats_lines[1:]}\")\n\t    @staticmethod\n\t    def parse_header_line(header_line, separator_line):\n\t        # separator line is expected to be all \"-\"-s\n\t        if set(separator_line.strip()) != {\"-\"}:\n\t            # TODO - Issue an error / warning\n", "            return None\n\t        header_fields = header_line.split()\n\t        if header_fields[0] != 'Level' or header_fields[1] != \"Files\" or \\\n\t                header_fields[2] != \"Size\":\n\t            # TODO - Issue an error / warning\n\t            return None\n\t        return header_fields\n\t    @staticmethod\n\t    def determine_line_type(type_field_str):\n\t        type_field_str = type_field_str.strip()\n", "        level_num = None\n\t        line_type = None\n\t        if type_field_str == \"Sum\":\n\t            line_type = CompactionStatsMngr.LineType.SUM\n\t        elif type_field_str == \"Int\":\n\t            line_type = CompactionStatsMngr.LineType.INTERVAL\n\t        elif type_field_str == \"User\":\n\t            line_type = CompactionStatsMngr.LineType.USER\n\t        else:\n\t            level_match = re.findall(r\"L(\\d+)\", type_field_str)\n", "            if level_match:\n\t                line_type = CompactionStatsMngr.LineType.LEVEL\n\t                level_num = int(level_match[0])\n\t            else:\n\t                # TODO - Error\n\t                pass\n\t        return line_type, level_num\n\t    @staticmethod\n\t    def parse_files_field(files_field):\n\t        files_parts = re.findall(r\"(\\d+)/(\\d+)\", files_field)\n", "        if not files_parts:\n\t            # TODO - Error\n\t            return None\n\t        return files_parts[0][0], files_parts[0][1]\n\t    @staticmethod\n\t    def parse_size_field(size_value, size_units):\n\t        return utils.get_num_bytes_from_human_readable_components(\n\t            size_value,\n\t            size_units)\n\t    def parse_level_lines(self, time, cf_name, stats_lines):\n", "        if len(stats_lines) < 2:\n\t            raise utils.ParsingError(\"Missing lines\")\n\t        header_fields = CompactionStatsMngr.parse_header_line(stats_lines[0],\n\t                                                              stats_lines[1])\n\t        if header_fields is None:\n\t            raise utils.ParsingError(\"Failed parsing compaction stats header\")\n\t        new_entry = {}\n\t        for line in stats_lines[2:]:\n\t            line_fields = line.strip().split()\n\t            if not line_fields:\n", "                continue\n\t            line_type, level_num = \\\n\t                CompactionStatsMngr.determine_line_type(line_fields[0])\n\t            if line_type is None:\n\t                raise utils.ParsingError(\n\t                    f\"Failed determining line type ({line_fields[0]})\")\n\t            num_files, files_in_comp = \\\n\t                CompactionStatsMngr.parse_files_field(line_fields[1])\n\t            if files_in_comp is None:\n\t                raise utils.ParsingError(\n", "                    f\"Failed parsing files field ({line_fields[1]})\")\n\t            size_in_units = line_fields[2]\n\t            size_units = line_fields[3]\n\t            key = line_type.name\n\t            if line_type is CompactionStatsMngr.LineType.LEVEL:\n\t                key += f\"-{level_num}\"\n\t            new_entry[key] = {\n\t                \"Num-Files\": num_files,\n\t                \"Files-In-Comp\": files_in_comp,\n\t                \"size_bytes\":\n", "                    CompactionStatsMngr.parse_size_field(size_in_units,\n\t                                                         size_units)\n\t            }\n\t            # A valid line must have one more field than in the header line\n\t            if len(line_fields) != len(header_fields) + 1:\n\t                logging.error(\n\t                    f\"Expected #{len(header_fields)+1} fields in line, \"\n\t                    f\"when there are {len(line_fields)} in compaction level \"\n\t                    f\"stats. time:{time}, cf:{cf_name}.\\n\"\n\t                    f\"line:{line}\")\n", "                return\n\t            new_entry[key].update({\n\t                header_fields[i]: line_fields[i+1]\n\t                for i in range(3, len(header_fields))\n\t            })\n\t        if CompactionStatsMngr.LineType.SUM.name not in new_entry:\n\t            logging.error(\n\t                f\"Error parsing compaction stats level lines. \"\n\t                f\"time:{time}, cf:{cf_name}.\\n\"\n\t                f\"stats lines:{stats_lines}\")\n", "            return\n\t        if time not in self.level_entries:\n\t            self.level_entries[time] = {}\n\t        self.level_entries[time][cf_name] = new_entry\n\t    def parse_priority_lines(self, time, cf_name, stats_lines):\n\t        # TODO - Consider issuing an info message as Redis (e.g.) don not\n\t        #  have any content here\n\t        if len(stats_lines) < 4:\n\t            return\n\t        # TODO: Parse when doing something with the data\n", "        pass\n\t    def get_level_entries(self):\n\t        return self.level_entries\n\t    def get_cf_level_entries(self, cf_name):\n\t        cf_entries = []\n\t        for time, time_entries in self.level_entries.items():\n\t            if cf_name in time_entries:\n\t                cf_entries.append({time: time_entries[cf_name]})\n\t        return cf_entries\n\t    def get_first_cf_level_entry(self, cf_name):\n", "        all_entries = self.get_cf_level_entries(cf_name)\n\t        if not all_entries:\n\t            return None\n\t        return all_entries[0]\n\t    def get_last_cf_level_entry(self, cf_name):\n\t        all_entries = self.get_cf_level_entries(cf_name)\n\t        if not all_entries:\n\t            return None\n\t        return all_entries[-1]\n\t    def get_first_level_entry_all_cfs(self):\n", "        all_entries = self.get_level_entries()\n\t        if not all_entries:\n\t            return None\n\t        time, first_all_cfs_dumps = \\\n\t            utils.get_first_dict_entry_components(all_entries)\n\t        return {cf_name: {time: cf_entry} for cf_name, cf_entry in\n\t                first_all_cfs_dumps.items()}\n\t    def get_last_level_entry_all_cfs(self):\n\t        all_entries = self.get_level_entries()\n\t        if not all_entries:\n", "            return None\n\t        time, last_all_cfs_dumps = \\\n\t            utils.get_last_dict_entry_components(all_entries)\n\t        return {cf_name: {time: cf_entry} for cf_name, cf_entry in\n\t                last_all_cfs_dumps.items()}\n\t    @staticmethod\n\t    def get_time_of_entry(stats_table):\n\t        keys = list(stats_table.keys())\n\t        assert len(keys) == 1\n\t        return keys[0]\n", "    @staticmethod\n\t    def get_level_entry_uptime_seconds(entry, log_start_time):\n\t        entry_time = CompactionStatsMngr.get_time_of_entry(entry)\n\t        return utils.get_times_strs_diff_seconds(log_start_time, entry_time)\n\t    @staticmethod\n\t    def get_field_value_for_line_in_entry(stats_table, line_key, field):\n\t        assert CompactionStatsMngr.LevelFields.has_field(field)\n\t        time = CompactionStatsMngr.get_time_of_entry(stats_table)\n\t        if line_key not in list(stats_table[time].keys()):\n\t            logging.info(f\"{line_key} not in entry. time:{time}\")\n", "            return None\n\t        level_line = stats_table[time][line_key]\n\t        if field.value not in level_line:\n\t            logging.info(f\"{field.value} not in entry's line. time:{time}\\n\"\n\t                         f\"{level_line}\")\n\t            return None\n\t        return level_line[field.value]\n\t    @staticmethod\n\t    def get_field_value_for_all_levels(stats_table, field):\n\t        assert CompactionStatsMngr.LevelFields.has_field(field)\n", "        per_level_value = {}\n\t        time = CompactionStatsMngr.get_time_of_entry(stats_table)\n\t        line_key_regex = fr\"LEVEL-{regexes.INT_C}\"\n\t        for key, value in stats_table[time].items():\n\t            match = re.findall(line_key_regex, key)\n\t            if not match:\n\t                continue\n\t            level = int(match[0])\n\t            per_level_value[level] = \\\n\t                CompactionStatsMngr.get_field_value_for_line_in_entry(\n", "                    stats_table, key, field)\n\t        if not per_level_value:\n\t            return None\n\t        return per_level_value\n\t    @staticmethod\n\t    def get_level_field_value(stats_table, level, field):\n\t        per_level_value = \\\n\t            CompactionStatsMngr.get_field_value_for_all_levels(\n\t                stats_table, field)\n\t        if per_level_value is None:\n", "            return None\n\t        if level not in per_level_value:\n\t            logging.info(f\"No info for level {level} in table.\")\n\t            return None\n\t        return per_level_value[level]\n\t    @staticmethod\n\t    def get_sum_value(stats_table, field):\n\t        line_key = CompactionStatsMngr.LineType.SUM.name\n\t        value = \\\n\t            CompactionStatsMngr.get_field_value_for_line_in_entry(\n", "                stats_table, line_key, field)\n\t        if value is None:\n\t            # SUM must be present always\n\t            raise utils.ParsingError(\n\t                f\"{line_key} is missing from compaction stats.\\n{stats_table}\")\n\t        return value\n\t    def get_cf_size_bytes_at_end(self, cf_name):\n\t        last_cf_entry = self.get_last_cf_level_entry(cf_name)\n\t        if not last_cf_entry:\n\t            return None\n", "        return CompactionStatsMngr.get_field_value_for_line_in_entry(\n\t            last_cf_entry, CompactionStatsMngr.LineType.SUM.name,\n\t            CompactionStatsMngr.LevelFields.SIZE_BYTES)\n\t    def get_cf_level_size_bytes(self, cf_name, level):\n\t        last_cf_entry = self.get_last_cf_level_entry(cf_name)\n\t        if not last_cf_entry:\n\t            return None\n\t        # TODO - Turn this into a utility\n\t        level_key = f\"LEVEL-{level}\"\n\t        return CompactionStatsMngr.get_field_value_for_line_in_entry(\n", "            last_cf_entry, level_key,\n\t            CompactionStatsMngr.LevelFields.SIZE_BYTES)\n\tclass BlobStatsMngr:\n\t    @staticmethod\n\t    def parse_blob_stats_line(line, allow_mismatch=False):\n\t        line_parts = re.findall(regexes.BLOB_STATS_LINE, line)\n\t        if not line_parts:\n\t            if allow_mismatch:\n\t                return None\n\t            assert line_parts\n", "        file_count, total_size_gb, garbage_size_gb, space_amp = line_parts[0]\n\t        return \\\n\t            int(file_count), float(total_size_gb), float(garbage_size_gb), \\\n\t            float(space_amp)\n\t    @staticmethod\n\t    def is_start_line(line):\n\t        return \\\n\t            BlobStatsMngr.parse_blob_stats_line(line, allow_mismatch=True) \\\n\t            is not None\n\t    def __init__(self):\n", "        self.entries = dict()\n\t    def add_lines(self, time, cf_name, db_stats_lines):\n\t        assert len(db_stats_lines) > 0\n\t        line = db_stats_lines[0]\n\t        line_parts = re.findall(regexes.BLOB_STATS_LINE, line)\n\t        assert line_parts and len(line_parts) == 1 and len(line_parts[0]) == 4\n\t        components = line_parts[0]\n\t        file_count = int(components[0])\n\t        total_size_bytes = \\\n\t            utils.get_num_bytes_from_human_readable_components(\n", "                components[1],\n\t                \"GB\")\n\t        garbage_size_bytes = \\\n\t            utils.get_num_bytes_from_human_readable_components(\n\t                components[2],\n\t                \"GB\")\n\t        space_amp = float(components[3])\n\t        if cf_name not in self.entries:\n\t            self.entries[cf_name] = dict()\n\t        self.entries[cf_name][time] = {\n", "            \"File Count\": file_count,\n\t            \"Total Size\": total_size_bytes,\n\t            \"Garbage Size\": garbage_size_bytes,\n\t            \"Space Amp\": space_amp\n\t        }\n\t    def get_cf_stats(self, cf_name):\n\t        if cf_name not in self.entries:\n\t            return []\n\t        return self.entries[cf_name]\n\tclass CfNoFileStatsMngr:\n", "    @staticmethod\n\t    def is_start_line(line):\n\t        return parse_uptime_line(line, allow_mismatch=True)\n\t    def __init__(self):\n\t        self.stall_counts = {}\n\t    def try_parse_as_stalls_count_line(self, time, cf_name, line):\n\t        if not line.startswith(regexes.CF_STALLS_LINE_START):\n\t            return None\n\t        if cf_name not in self.stall_counts:\n\t            self.stall_counts[cf_name] = {}\n", "        # TODO - I have seen compaction stats for the same cf twice - WHY?\n\t        #######assert time not in self.stall_counts[cf_name] # noqa\n\t        self.stall_counts[cf_name][time] = {}\n\t        stall_count_and_reason_matches = \\\n\t            re.compile(regexes.CF_STALLS_COUNT_AND_REASON)\n\t        sum_fields_count = 0\n\t        for match in stall_count_and_reason_matches.finditer(line):\n\t            count = int(match[1])\n\t            self.stall_counts[cf_name][time][match[2]] = count\n\t            sum_fields_count += count\n", "        assert self.stall_counts[cf_name][time]\n\t        total_count_match = re.findall(\n\t            regexes.CF_STALLS_INTERVAL_COUNT, line)\n\t        # TODO - Last line of Redis's log was cropped in the middle\n\t        ###### assert total_count_match and len(total_count_match) == 1 # noqa\n\t        if not total_count_match or len(total_count_match) != 1:\n\t            del self.stall_counts[cf_name][time]\n\t            return None\n\t        total_count = int(total_count_match[0])\n\t        self.stall_counts[cf_name][time][\"interval_total_count\"] = total_count\n", "        sum_fields_count += total_count\n\t        if sum_fields_count == 0:\n\t            del self.stall_counts[cf_name][time]\n\t    def add_lines(self, time, cf_name, stats_lines):\n\t        for line in stats_lines:\n\t            line = line.strip()\n\t            if self.try_parse_as_stalls_count_line(time, cf_name, line):\n\t                continue\n\t    def get_stall_counts(self):\n\t        return self.stall_counts\n", "class CfFileHistogramStatsMngr:\n\t    @dataclass\n\t    class CfLevelStats:\n\t        count: int = 0\n\t        average: float = 0.0\n\t        std_dev: float = 0.0\n\t        min: int = 0\n\t        median: float = 0.0\n\t        max: int = 0\n\t    class CfEntry:\n", "        def __init__(self, entry):\n\t            self.time = __class__.get_time(entry)\n\t            self.levels_stats = entry[self.time]\n\t        @staticmethod\n\t        def get_time(entry):\n\t            return list(entry.keys())[0]\n\t        def get_levels(self):\n\t            return list(self.levels_stats.keys())\n\t        def get_all_levels_stats(self):\n\t            return self.levels_stats\n", "        def get_level_stats(self, level):\n\t            if level not in self.levels_stats:\n\t                return None\n\t            return self.levels_stats[level]\n\t    @staticmethod\n\t    def parse_start_line(line, allow_mismatch=False):\n\t        return parse_line_with_cf(line,\n\t                                  regexes.FILE_READ_LATENCY_STATS,\n\t                                  allow_mismatch)\n\t    @staticmethod\n", "    def is_start_line(line):\n\t        return CfFileHistogramStatsMngr.parse_start_line(line,\n\t                                                         allow_mismatch=True) \\\n\t               is not None\n\t    def __init__(self):\n\t        # Format: {<cf name>: {time: {<level>: CfLevelStats}}}\n\t        self.stats = dict()\n\t    def add_lines(self, time, cf_name, db_stats_lines):\n\t        # The lines are organized as follows:\n\t        # <cf-1> Header\n", "        # Level-<X> stats\n\t        # Level-<Y> stats\n\t        # ...\n\t        # A cf header is:\n\t        # ** File Read Latency Histogram By Level [<cf-name>] **\n\t        #\n\t        # The per level dumps are:\n\t        #  ** Level 0 read latency histogram (<cf-name>):\n\t        # Count: 25 Average: 1571.7200  StdDev: 5194.93\n\t        # Min: 1  Median: 2.8333  Max: 26097\n", "        # Percentiles: <Percentiles Line> (Not parsed currently)\n\t        # Per-Bucket Histogram Table (Not parsed currently)\n\t        #\n\t        assert isinstance(db_stats_lines, list)\n\t        num_lines = len(db_stats_lines)\n\t        assert num_lines > 0\n\t        # First line must be the start of a cf section\n\t        parsed_cf_name = CfFileHistogramStatsMngr.parse_start_line(\n\t            db_stats_lines[0])\n\t        assert cf_name == parsed_cf_name, \\\n", "            f\"cf_name:{cf_name}, parsed_cf_name:{parsed_cf_name}\"\n\t        line_idx = 1\n\t        while line_idx < num_lines:\n\t            next_line_idx = \\\n\t                CfFileHistogramStatsMngr.find_next_level_stats_start(\n\t                    db_stats_lines, line_idx + 1)\n\t            self.parse_next_level_stats(\n\t                time, cf_name, db_stats_lines[line_idx:next_line_idx])\n\t            line_idx = next_line_idx\n\t    @staticmethod\n", "    def find_next_level_stats_start(db_stats_lines, line_idx):\n\t        while line_idx < len(db_stats_lines):\n\t            if CfFileHistogramStatsMngr.\\\n\t                    is_level_stats_start_line(db_stats_lines[line_idx]):\n\t                break\n\t            line_idx += 1\n\t        return line_idx\n\t    def parse_next_level_stats(self, time, cf_name, level_stats_lines):\n\t        if len(level_stats_lines) < 3:\n\t            raise utils.ParsingError(\n", "                f\"Expecting at least 3 lines. time:{time}\\n\"\n\t                f\"{level_stats_lines}\")\n\t        new_stats = CfFileHistogramStatsMngr.CfLevelStats()\n\t        level = CfFileHistogramStatsMngr.parse_level_line(\n\t            time, cf_name, level_stats_lines[0])\n\t        CfFileHistogramStatsMngr.parse_stats_line_1(\n\t            time, cf_name, level_stats_lines[1], new_stats)\n\t        CfFileHistogramStatsMngr.parse_stats_line_2(\n\t            time, cf_name, level_stats_lines[2], new_stats)\n\t        self.add_cf_if_necessary(cf_name)\n", "        self.add_cf_time_if_necessary(cf_name, time)\n\t        if level in self.stats[cf_name][time]:\n\t            logging.warning(\n\t                f\"Duplicate file read latency for level, Ignoring. \"\n\t                f\"tims:{time}, cf:{cf_name}. level:{level}\\n\"\n\t                f\"{level_stats_lines}\")\n\t            return\n\t        self.stats[cf_name][time][level] = new_stats\n\t    def add_cf_if_necessary(self, cf_name):\n\t        if cf_name not in self.stats:\n", "            self.stats[cf_name] = {}\n\t    def add_cf_time_if_necessary(self, cf_name, time):\n\t        if time not in self.stats[cf_name]:\n\t            self.stats[cf_name][time] = {}\n\t    @staticmethod\n\t    def parse_level_line(time, cf_name, line):\n\t        match = re.findall(regexes.LEVEL_READ_LATENCY_LEVEL_LINE, line)\n\t        if not match:\n\t            raise utils.ParsingError(\n\t                f\"Failed parsing read latency level line. \"\n", "                f\"time:{time}, cf:{cf_name}\\n\"\n\t                f\"{line}\")\n\t        assert len(match) == 1\n\t        # Return the level\n\t        return int(match[0])\n\t    @staticmethod\n\t    def is_level_stats_start_line(line):\n\t        match = re.findall(regexes.LEVEL_READ_LATENCY_LEVEL_LINE, line)\n\t        return True if match else False\n\t    @staticmethod\n", "    def parse_stats_line_1(time, cf_name, line, new_stats):\n\t        match = re.findall(regexes.LEVEL_READ_LATENCY_STATS_LINE1, line)\n\t        if not match:\n\t            raise utils.ParsingError(\n\t                f\"Failed parsing read latency level line. \"\n\t                f\"time:{time}, cf:{cf_name}\\n\"\n\t                f\"{line}\")\n\t        assert len(match) == 1 or len(match[0]) == 3\n\t        new_stats.count = int(match[0][0])\n\t        new_stats.average = float(match[0][1])\n", "        new_stats.std_dev = float(match[0][2])\n\t    @staticmethod\n\t    def parse_stats_line_2(time, cf_name, line, new_stats):\n\t        match = re.findall(regexes.LEVEL_READ_LATENCY_STATS_LINE2, line)\n\t        if not match:\n\t            raise utils.ParsingError(\n\t                f\"Failed parsing read latency level line. \"\n\t                f\"time:{time}, cf:{cf_name}\\n\"\n\t                f\"{line}\")\n\t        assert len(match) == 1 or len(match[0]) == 3\n", "        new_stats.min = int(match[0][0])\n\t        new_stats.median = float(match[0][1])\n\t        new_stats.max = int(match[0][2])\n\t    def get_all_entries(self):\n\t        if not self.stats:\n\t            return None\n\t        return self.stats\n\t    def get_cf_entries(self, cf_name):\n\t        if cf_name not in self.stats:\n\t            return None\n", "        return self.stats[cf_name]\n\t    def get_last_cf_entry(self, cf_name):\n\t        all_entries = self.get_cf_entries(cf_name)\n\t        if not all_entries:\n\t            return None\n\t        return utils.get_last_dict_entry(all_entries)\n\tclass BlockCacheStatsMngr:\n\t    @staticmethod\n\t    def is_start_line(line):\n\t        return re.findall(regexes.BLOCK_CACHE_STATS_START, line)\n", "    def __init__(self):\n\t        self.caches = dict()\n\t    def add_lines(self, time, cf_name, db_stats_lines):\n\t        if len(db_stats_lines) < 2:\n\t            return\n\t        cache_id = self.parse_cache_id_line(db_stats_lines[0])\n\t        self.parse_global_entry_stats_line(time, cache_id, db_stats_lines[1])\n\t        if len(db_stats_lines) > 2:\n\t            self.parse_cf_entry_stats_line(time, cache_id, db_stats_lines[2])\n\t        return cache_id\n", "    def parse_cache_id_line(self, line):\n\t        line_parts = re.findall(regexes.BLOCK_CACHE_STATS_START, line)\n\t        assert line_parts and len(line_parts) == 1 and len(line_parts[0]) == 3\n\t        cache_id, cache_capacity, capacity_units = line_parts[0]\n\t        capacity_bytes = utils.\\\n\t            get_num_bytes_from_human_readable_components(cache_capacity,\n\t                                                         capacity_units)\n\t        if cache_id not in self.caches:\n\t            self.caches[cache_id] = {\"Capacity\": capacity_bytes,\n\t                                     \"Usage\": 0}\n", "        return cache_id\n\t    def parse_global_entry_stats_line(self, time, cache_id, line):\n\t        line_parts = re.findall(regexes.BLOCK_CACHE_ENTRY_STATS, line)\n\t        assert line_parts and len(line_parts) == 1\n\t        roles, roles_stats = BlockCacheStatsMngr.parse_entry_stats_line(\n\t            line_parts[0])\n\t        self.add_time_if_necessary(cache_id, time)\n\t        self.caches[cache_id][time][\"Usage\"] = 0\n\t        usage = 0\n\t        for i, role in enumerate(roles):\n", "            count, size_with_unit, portion = roles_stats[i].split(',')\n\t            size_bytes = \\\n\t                utils.get_num_bytes_from_human_readable_str(\n\t                    size_with_unit)\n\t            portion = f\"{float(portion.split('%')[0]):.2f}%\"\n\t            self.caches[cache_id][time][role] = \\\n\t                {\"Count\": int(count), \"Size\": size_bytes, \"Portion\": portion}\n\t            usage += size_bytes\n\t        self.caches[cache_id][time][\"Usage\"] = usage\n\t        self.caches[cache_id][\"Usage\"] = usage\n", "    def parse_cf_entry_stats_line(self, time, cache_id, line):\n\t        line_parts = re.findall(regexes.BLOCK_CACHE_CF_ENTRY_STATS, line)\n\t        if not line_parts:\n\t            return\n\t        assert len(line_parts) == 1 and len(line_parts[0]) == 2\n\t        cf_name, roles_info_part = line_parts[0]\n\t        roles, roles_stats = BlockCacheStatsMngr.parse_entry_stats_line(\n\t            roles_info_part)\n\t        cf_entry = {}\n\t        for i, role in enumerate(roles):\n", "            size_bytes = \\\n\t                utils.get_num_bytes_from_human_readable_str(\n\t                    roles_stats[i])\n\t            if size_bytes > 0:\n\t                cf_entry[role] = size_bytes\n\t        if cf_entry:\n\t            if \"CF-s\" not in self.caches[cache_id][time]:\n\t                self.add_time_if_necessary(cache_id, time)\n\t                self.caches[cache_id][time][\"CF-s\"] = {}\n\t            self.caches[cache_id][time][\"CF-s\"][cf_name] = cf_entry\n", "    @staticmethod\n\t    def parse_entry_stats_line(line):\n\t        roles = re.findall(regexes.BLOCK_CACHE_ENTRY_ROLES_NAMES,\n\t                           line)\n\t        roles_stats = re.findall(regexes.BLOCK_CACHE_ENTRY_ROLES_STATS,\n\t                                 line)\n\t        if len(roles) != len(roles_stats):\n\t            assert False, str(ParsingAssertion(\n\t                f\"Error Parsing block cache stats line. \"\n\t                f\"roles:{roles}, roles_stats:{roles_stats}\",\n", "                ErrContext(**{'log_line': line})))\n\t        return roles, roles_stats\n\t    def add_time_if_necessary(self, cache_id, time):\n\t        if time not in self.caches[cache_id]:\n\t            self.caches[cache_id][time] = {}\n\t    def get_cache_entries(self, cache_id):\n\t        if cache_id not in self.caches:\n\t            return {}\n\t        return self.caches[cache_id]\n\t    def get_cf_cache_entries(self, cache_id, cf_name):\n", "        cf_entries = {}\n\t        all_cache_entries = self.get_cache_entries(cache_id)\n\t        if not all_cache_entries:\n\t            return cf_entries\n\t        cf_entries = {}\n\t        for key in all_cache_entries.keys():\n\t            time = utils.parse_time_str(key, expect_valid_str=False)\n\t            if time:\n\t                time = key\n\t                if \"CF-s\" in all_cache_entries[time]:\n", "                    if cf_name in all_cache_entries[time][\"CF-s\"]:\n\t                        cf_entries[time] = \\\n\t                            all_cache_entries[time][\"CF-s\"][cf_name]\n\t        return cf_entries\n\t    def get_all_cache_entries(self):\n\t        return self.caches\n\t    def get_last_usage(self, cache_id):\n\t        usage = 0\n\t        if self.caches:\n\t            usage = self.caches[cache_id][\"Usage\"]\n", "        return usage\n\tclass StatsMngr:\n\t    class StatsType(Enum):\n\t        DB_WIDE = auto()\n\t        COMPACTION = auto()\n\t        BLOB = auto()\n\t        BLOCK_CACHE = auto()\n\t        CF_NO_FILE = auto()\n\t        CF_FILE_HISTOGRAM = auto()\n\t        COUNTERS = auto()\n", "    def __init__(self):\n\t        self.dump_stats_entry_found = False\n\t        self.db_wide_stats_mngr = DbWideStatsMngr()\n\t        self.compaction_stats_mngr = CompactionStatsMngr()\n\t        self.blob_stats_mngr = BlobStatsMngr()\n\t        self.block_cache_stats_mngr = BlockCacheStatsMngr()\n\t        self.cf_no_file_stats_mngr = CfNoFileStatsMngr()\n\t        self.cf_file_histogram_stats_mngr = CfFileHistogramStatsMngr()\n\t    @staticmethod\n\t    def is_dump_stats_start(entry):\n", "        return entry.get_msg().startswith(regexes.DUMP_STATS_STR)\n\t    @staticmethod\n\t    def find_next_start_line_in_db_stats(db_stats_lines,\n\t                                         start_line_idx,\n\t                                         curr_stats_type):\n\t        line_idx = start_line_idx + 1\n\t        next_stats_type = None\n\t        cf_name = None\n\t        # DB Wide Stats must be the first and were verified above\n\t        while line_idx < len(db_stats_lines) and next_stats_type is None:\n", "            line = db_stats_lines[line_idx]\n\t            if CompactionStatsMngr.is_start_line(line):\n\t                next_stats_type = StatsMngr.StatsType.COMPACTION\n\t                cf_name = CompactionStatsMngr.parse_start_line(line)\n\t            elif BlobStatsMngr.is_start_line(line):\n\t                next_stats_type = StatsMngr.StatsType.BLOB\n\t            elif BlockCacheStatsMngr.is_start_line(line):\n\t                next_stats_type = StatsMngr.StatsType.BLOCK_CACHE\n\t            elif CfFileHistogramStatsMngr.is_start_line(line):\n\t                next_stats_type = StatsMngr.StatsType.CF_FILE_HISTOGRAM\n", "                cf_name = CfFileHistogramStatsMngr.parse_start_line(line)\n\t            elif CfNoFileStatsMngr.is_start_line(line) and \\\n\t                    curr_stats_type != StatsMngr.StatsType.DB_WIDE:\n\t                next_stats_type = StatsMngr.StatsType.CF_NO_FILE\n\t            else:\n\t                line_idx += 1\n\t        return line_idx, next_stats_type, cf_name\n\t    def parse_next_db_stats_entry_lines(self, time, cf_name, stats_type,\n\t                                        entry_start_line_num,\n\t                                        db_stats_lines, start_line_idx,\n", "                                        end_line_idx):\n\t        assert end_line_idx <= len(db_stats_lines)\n\t        stats_lines_to_parse = db_stats_lines[start_line_idx:end_line_idx]\n\t        stats_lines_to_parse = [line.strip() for line in stats_lines_to_parse]\n\t        line_num = entry_start_line_num + start_line_idx + 1\n\t        try:\n\t            logging.debug(f\"Parsing Stats Component ({stats_type.name}) \"\n\t                          f\"[line# {line_num}]\")\n\t            valid_stats_type = True\n\t            if stats_type == StatsMngr.StatsType.DB_WIDE:\n", "                self.db_wide_stats_mngr.add_lines(time, stats_lines_to_parse)\n\t            elif stats_type == StatsMngr.StatsType.COMPACTION:\n\t                self.compaction_stats_mngr.add_lines(\n\t                    time, cf_name, stats_lines_to_parse, line_num)\n\t            elif stats_type == StatsMngr.StatsType.BLOB:\n\t                self.blob_stats_mngr.add_lines(time, cf_name,\n\t                                               stats_lines_to_parse)\n\t            elif stats_type == StatsMngr.StatsType.BLOCK_CACHE:\n\t                self.block_cache_stats_mngr.add_lines(time, cf_name,\n\t                                                      stats_lines_to_parse)\n", "            elif stats_type == StatsMngr.StatsType.CF_NO_FILE:\n\t                self.cf_no_file_stats_mngr.add_lines(time, cf_name,\n\t                                                     stats_lines_to_parse)\n\t            elif stats_type == StatsMngr.StatsType.CF_FILE_HISTOGRAM:\n\t                self.cf_file_histogram_stats_mngr.add_lines(\n\t                    time, cf_name, stats_lines_to_parse)\n\t            else:\n\t                valid_stats_type = False\n\t        except utils.ParsingError as e:  # noqa\n\t            logging.exception(format_err_msg(\n", "                f\"Error parsing a Stats Entry. time:{time}, cf:{cf_name}\" +\n\t                str(ErrContext(**{\n\t                    \"log_line_idx\": line_num - 1,\n\t                    \"log_line\": db_stats_lines[start_line_idx]\n\t                }))))\n\t            valid_stats_type = True\n\t        if not valid_stats_type:\n\t            assert False, f\"Unexpected stats type ({stats_type})\"\n\t    def try_adding_entries(self, log_entries, start_entry_idx):\n\t        cf_names_found = set()\n", "        entry_idx = start_entry_idx\n\t        # A stats entry starts with the\n\t        # \"------- DUMPING STATS -------\" entry, however, there may be\n\t        # unrelated entries between this entry and the entry with\n\t        # the actual stats (the one starting with \"DB Stats\").\n\t        dump_stats_entry_found_now = False\n\t        if StatsMngr.is_dump_stats_start(log_entries[entry_idx]):\n\t            logging.debug(\n\t                f\"Found Stats Dump Entry Start (\"\n\t                f\"{format_line_num_from_entry(log_entries[entry_idx])}\")\n", "            self.dump_stats_entry_found = True\n\t            dump_stats_entry_found_now = True\n\t            entry_idx += 1\n\t        elif not self.dump_stats_entry_found:\n\t            return False, entry_idx, cf_names_found\n\t        db_stats_entry = log_entries[entry_idx]\n\t        db_stats_lines = \\\n\t            utils.remove_empty_lines_at_start(\n\t                db_stats_entry.get_msg_lines())\n\t        db_stats_time = db_stats_entry.get_time()\n", "        # Now check if the actual stats follow immediately, or, if not, wait\n\t        # for a later entry with the actual stats\n\t        if len(db_stats_lines) == 0 or \\\n\t                not DbWideStatsMngr.is_start_line(db_stats_lines[0]):\n\t            # Found the start => return True\n\t            return dump_stats_entry_found_now, entry_idx, cf_names_found\n\t        # \"** DB Stats **\" is immediately following \"DUMP STATS\"\n\t        logging.debug(f\"Parsing Stats Dump Entry (\"\n\t                      f\"{format_line_num_from_entry(log_entries[entry_idx])}\")\n\t        self.dump_stats_entry_found = False\n", "        def log_parsing_error(msg_prefix):\n\t            logging.error(format_err_msg(\n\t                f\"{msg_prefix} While parsing Stats Entry. time:\"\n\t                f\"{db_stats_time}, cf:{curr_cf_name}\",\n\t                ErrContext(**{\n\t                    \"log_line_idx\":\n\t                        db_stats_entry.get_start_line_num() + line_idx})))\n\t        line_idx = 0\n\t        stats_type = StatsMngr.StatsType.DB_WIDE\n\t        curr_cf_name = utils.NO_CF\n", "        try:\n\t            while line_idx < len(db_stats_lines):\n\t                next_line_num, next_stats_type, next_cf_name = \\\n\t                    StatsMngr.find_next_start_line_in_db_stats(db_stats_lines,\n\t                                                               line_idx,\n\t                                                               stats_type)\n\t                # parsing must progress\n\t                assert next_line_num > line_idx\n\t                self.parse_next_db_stats_entry_lines(\n\t                    db_stats_time,\n", "                    curr_cf_name,\n\t                    stats_type,\n\t                    db_stats_entry.get_start_line_num(),\n\t                    db_stats_lines,\n\t                    line_idx,\n\t                    next_line_num)\n\t                line_idx = next_line_num\n\t                stats_type = next_stats_type\n\t                if next_cf_name is not None:\n\t                    curr_cf_name = next_cf_name\n", "                    if next_cf_name != utils.NO_CF:\n\t                        cf_names_found.add(curr_cf_name)\n\t        except AssertionError:\n\t            log_parsing_error(\"Assertion\")\n\t            raise\n\t        except Exception:  # noqa\n\t            log_parsing_error(\"Exception\")\n\t            raise\n\t        # Done parsing the stats entry\n\t        entry_idx += 1\n", "        line_num = format_line_num_from_entry(log_entries[entry_idx]) \\\n\t            if entry_idx < len(log_entries) else \\\n\t            format_line_num_from_line_idx(log_entries[-1].get_end_line_idx())\n\t        logging.debug(f\"Completed Parsing Stats Dump Entry ({line_num})\")\n\t        return True, entry_idx, cf_names_found\n\t    def get_db_wide_stats_mngr(self):\n\t        return self.db_wide_stats_mngr\n\t    def get_compactions_stats_mngr(self):\n\t        return self.compaction_stats_mngr\n\t    def get_blob_stats_mngr(self):\n", "        return self.blob_stats_mngr\n\t    def get_block_cache_stats_mngr(self):\n\t        return self.block_cache_stats_mngr\n\t    def get_cf_no_file_stats_mngr(self):\n\t        return self.cf_no_file_stats_mngr\n\t    def get_cf_file_histogram_stats_mngr(self):\n\t        return self.cf_file_histogram_stats_mngr\n"]}
{"filename": "log_file_options_parser.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport logging\n\timport re\n\timport regexes\n\timport utils\n\tfrom log_entry import LogEntry\n\tformat_lines_range_from_entries_idxs = \\\n\t    utils.format_lines_range_from_entries_idxs\n", "format_line_num_from_entry = utils.format_line_num_from_entry\n\tTABLE_OPTIONS_TOPIC_TITLES = [(\"metadata_cache_options\", \"metadata_cache_\"),\n\t                              (\"block_cache_options\", \"block_cache_\")]\n\tdef get_table_options_topic_info(topic_name):\n\t    for topic_info in TABLE_OPTIONS_TOPIC_TITLES:\n\t        if topic_info[0] == topic_name:\n\t            return topic_info\n\t    return None\n\tclass LogFileOptionsParser:\n\t    @staticmethod\n", "    def try_parsing_as_options_entry(log_entry):\n\t        assert isinstance(log_entry, LogEntry)\n\t        option_parts_match = re.findall(regexes.OPTION_LINE,\n\t                                        log_entry.get_msg())\n\t        if len(option_parts_match) != 1 or len(option_parts_match[0]) != 2:\n\t            return None\n\t        assert len(option_parts_match) == 1 or len(option_parts_match[0]) == 2\n\t        option_name = option_parts_match[0][0].strip()\n\t        option_value = option_parts_match[0][1].strip()\n\t        return option_name, option_value\n", "    @staticmethod\n\t    def is_options_entry(line):\n\t        if LogFileOptionsParser.try_parsing_as_options_entry(line):\n\t            return True\n\t        else:\n\t            return False\n\t    @staticmethod\n\t    def try_parsing_as_table_options_entry(log_entry):\n\t        assert isinstance(log_entry, LogEntry)\n\t        options_lines = log_entry.get_non_stripped_msg_lines()\n", "        if len(options_lines) < 1:\n\t            # TODO - Maybe a bug - consider asserting\n\t            return None\n\t        options_dict = dict()\n\t        # first line has the \"table_factory options:\" prefix\n\t        # example:\n\t        # options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory\n\t        option_parts_match = re.findall(regexes.TABLE_OPTIONS_START_LINE,\n\t                                        options_lines[0])\n\t        if len(option_parts_match) != 1 or len(option_parts_match[0]) != 2:\n", "            return None\n\t        options_dict[option_parts_match[0][0].strip()] = \\\n\t            option_parts_match[0][1].strip()\n\t        options_lines_to_parse = options_lines[1:]\n\t        line_idx = 0\n\t        while line_idx < len(options_lines_to_parse):\n\t            line = options_lines_to_parse[line_idx]\n\t            option_name, option_value = \\\n\t                LogFileOptionsParser.parse_table_options_line(line)\n\t            if option_name is None:\n", "                line_idx += 1\n\t                continue\n\t            topic_info = \\\n\t                get_table_options_topic_info(option_name)\n\t            if topic_info is None:\n\t                options_dict[option_name] = option_value\n\t                line_idx += 1\n\t            else:\n\t                line_idx = \\\n\t                    LogFileOptionsParser.parse_table_options_topic_options(\n", "                        topic_info[1], options_lines_to_parse, line_idx,\n\t                        options_dict)\n\t        return options_dict\n\t    @staticmethod\n\t    def parse_table_options_line(line):\n\t        option_parts_match = re.findall(\n\t            regexes.TABLE_OPTIONS_CONTINUATION_LINE, line)\n\t        if not option_parts_match:\n\t            return None, None\n\t        assert len(option_parts_match) == 1 and \\\n", "               len(option_parts_match[0]) == 2\n\t        option_name = option_parts_match[0][0].strip()\n\t        option_value = option_parts_match[0][1].strip()\n\t        return option_name, option_value\n\t    @staticmethod\n\t    def parse_table_options_topic_options(topic_prefix,\n\t                                          options_lines_to_parse, line_idx,\n\t                                          options_dict):\n\t        topic_line = options_lines_to_parse[line_idx]\n\t        topic_line_indentation_size =\\\n", "            utils.get_num_leading_spaces(topic_line)\n\t        line_idx += 1\n\t        while line_idx < len(options_lines_to_parse):\n\t            checked_line = options_lines_to_parse[line_idx]\n\t            checked_line_indentation_size = \\\n\t                utils.get_num_leading_spaces(checked_line)\n\t            if checked_line_indentation_size <=   \\\n\t                    topic_line_indentation_size:\n\t                break\n\t            option_name, option_value = \\\n", "                LogFileOptionsParser.parse_table_options_line(checked_line)\n\t            if option_name is None:\n\t                break\n\t            options_dict[f\"{topic_prefix}{option_name}\"] = option_value\n\t            line_idx += 1\n\t        return line_idx\n\t    @staticmethod\n\t    def try_parsing_as_cf_options_start_entry(log_entry):\n\t        parts = re.findall(regexes.CF_OPTIONS_START, log_entry.get_msg())\n\t        if not parts or len(parts) != 1:\n", "            return None\n\t        # In case of match, we return the column-family name\n\t        return parts[0]\n\t    @staticmethod\n\t    def is_cf_options_start_entry(log_entry):\n\t        result = LogFileOptionsParser.try_parsing_as_cf_options_start_entry(\n\t            log_entry)\n\t        return result is not None\n\t    @staticmethod\n\t    def parse_db_wide_wbm_sub_pseudo_options(entry, options_dict):\n", "        wbm_pseudo_options = \\\n\t            re.findall(regexes.DB_WIDE_WBM_PSEUDO_OPTION_LINE,\n\t                       entry.get_msg(), re.MULTILINE)\n\t        for pseudo_option_name, pseudo_option_value in wbm_pseudo_options:\n\t            options_dict[f\"write_buffer_manager_{pseudo_option_name}\"] =\\\n\t                pseudo_option_value\n\t    @staticmethod\n\t    def parse_db_wide_options(log_entries, start_entry_idx, end_entry_idx):\n\t        \"\"\"\n\t        Parses all of the entries in the specified range of\n", "        [start_entry_idx, end_entry_idx)\n\t        Returns:\n\t            options_dict: The parsed options:\n\t                dict(<option name>: <option value>)\n\t            entry_idx: the index of the entry\n\t        \"\"\"\n\t        logging.debug(f\"Parsing DB-Wide Options (\"\n\t                      f\"{format_lines_range_from_entries_idxs(log_entries,start_entry_idx, end_entry_idx)})\") # noqa\n\t        options_dict = {}\n\t        entry_idx = start_entry_idx\n", "        while entry_idx < end_entry_idx:\n\t            entry = log_entries[entry_idx]\n\t            options_kv = \\\n\t                LogFileOptionsParser.try_parsing_as_options_entry(entry)\n\t            if options_kv:\n\t                option_name, option_value = options_kv\n\t                options_dict[option_name] = option_value\n\t                if option_name == \\\n\t                    utils.\\\n\t                        DB_WIDE_WRITE_BUFFER_MANAGER_OPTIONS_NAME:\n", "                    # Special case write buffer manager \"Options\"\n\t                    LogFileOptionsParser.parse_db_wide_wbm_sub_pseudo_options(\n\t                        entry, options_dict)\n\t            else:\n\t                # TODO - Add info to Error\n\t                logging.error(f\"TODO - ERROR In DB Wide Entry \"\n\t                              f\"(idx:{entry_idx}), {entry}\")\n\t            entry_idx += 1\n\t        return options_dict\n\t    @staticmethod\n", "    def parse_cf_options(log_entries, start_entry_idx, cf_name=None):\n\t        logging.debug(\n\t            f\"Parsing CF Options (\"\n\t            f\"{format_line_num_from_entry(log_entries[start_entry_idx])}\")\n\t        entry_idx = start_entry_idx\n\t        # If cf_name was specified, it means we have received cf options\n\t        # without the CF options header entry\n\t        if cf_name is None:\n\t            cf_name = LogFileOptionsParser. \\\n\t                try_parsing_as_cf_options_start_entry(log_entries[entry_idx])\n", "            entry_idx += 1\n\t        # cf_name may be the emtpy string, but not None\n\t        assert cf_name is not None\n\t        options_dict = {}\n\t        table_options_dict = None\n\t        duplicate_option = False\n\t        while entry_idx < len(log_entries):\n\t            entry = log_entries[entry_idx]\n\t            options_kv = \\\n\t                LogFileOptionsParser.try_parsing_as_options_entry(entry)\n", "            if options_kv:\n\t                option_name, option_value = options_kv\n\t                if option_name in options_dict:\n\t                    # finding the same option twice implies that the options\n\t                    # for this cf are over.\n\t                    duplicate_option = True\n\t                    break\n\t                options_dict[option_name] = option_value\n\t            else:\n\t                temp_table_options_dict = \\\n", "                    LogFileOptionsParser.try_parsing_as_table_options_entry(\n\t                           entry)\n\t                if temp_table_options_dict:\n\t                    assert table_options_dict is None\n\t                    table_options_dict = temp_table_options_dict\n\t                else:\n\t                    # The entry is a different type of entry => done\n\t                    break\n\t            entry_idx += 1\n\t        assert options_dict, \"No Options for Column Family\"\n", "        assert table_options_dict, \"Missing table options in CF options\"\n\t        line_num = \"\"\n\t        if entry_idx < len(log_entries):\n\t            line_num = format_line_num_from_entry(log_entries[entry_idx])\n\t        logging.debug(f\"Completed Parsing CF Options ([{cf_name}] {line_num}\"\n\t                      f\"duplicate:{duplicate_option})\")\n\t        return cf_name, options_dict, table_options_dict, entry_idx, \\\n\t            duplicate_option\n"]}
{"filename": "log_entry.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport re\n\timport regexes\n\timport utils\n\tclass LogEntry:\n\t    @staticmethod\n\t    def is_entry_start(log_line, regex=None):\n\t        token_list = log_line.strip().split()\n", "        if not token_list:\n\t            return False\n\t        # The assumption is that a new log will start with a date\n\t        if not re.findall(regexes.TIMESTAMP, token_list[0]):\n\t            return False\n\t        if regex:\n\t            # token_list[1] should be the context\n\t            if not re.findall(regex, \" \".join(token_list[2:])):\n\t                return False\n\t        return True\n", "    @staticmethod\n\t    def validate_entry_start(line_idx, log_line):\n\t        if not LogEntry.is_entry_start(log_line):\n\t            raise utils.ParsingAssertion(\n\t                \"Line isn't entry's start.\",\n\t                utils.ErrorContext(**{'log_line': log_line,\n\t                                      'log_line_idx': line_idx}))\n\t    def validate_finalized(self):\n\t        if not self.is_finalized:\n\t            raise utils.ParsingAssertion(\n", "                f\"Entry already finalized. {self}\")\n\t    def __init__(self, line_idx, log_line, last_line=False):\n\t        LogEntry.validate_entry_start(line_idx, log_line)\n\t        self.is_finalized = False\n\t        self.start_line_idx = line_idx\n\t        # Try to parse as a warning line\n\t        parts = re.findall(regexes.START_LINE_WITH_WARN_PARTS, log_line)\n\t        if parts:\n\t            num_parts_expected = 6\n\t        else:\n", "            # Not a warning line => Parse as \"normal\" line\n\t            parts = re.findall(regexes.START_LINE_PARTS, log_line)\n\t            if not parts:\n\t                raise utils.ParsingError(\n\t                    \"Failed parsing Log Entry start line.\",\n\t                    utils.ErrorContext(**{'log_line': log_line,\n\t                                          'log_line_idx': line_idx}))\n\t            num_parts_expected = 5\n\t        assert len(parts) == 1 and len(parts[0]) == num_parts_expected, \\\n\t            f\"Unexpected # of parts (expected {num_parts_expected}) ({parts})\"\n", "        parts = parts[0]\n\t        self.time = parts[0]\n\t        self.context = parts[1]\n\t        self.orig_time = parts[2]\n\t        # warn msg\n\t        if num_parts_expected == 6:\n\t            self.warning_type = utils.WarningType(parts[3])\n\t            part_increment = 1\n\t        else:\n\t            self.warning_type = None\n", "            part_increment = 0\n\t        # File + Line in a source file\n\t        # example: '... [/column_family.cc: 932] ...'\n\t        self.code_pos = parts[3 + part_increment]\n\t        if self.code_pos:\n\t            code_pos_value_match = re.findall(r\"\\[(.*)\\]\", self.code_pos)\n\t            if code_pos_value_match:\n\t                self.code_pos = code_pos_value_match[0]\n\t        # Rest of 1st line's text starts the msg_lines part\n\t        self.msg_lines = list()\n", "        if parts[4 + part_increment]:\n\t            # self.msg_lines.append(parts[4 + part_increment].strip())\n\t            self.msg_lines.append(parts[4 + part_increment])\n\t        self.cf_name = None\n\t        self.job_id = None\n\t        self.try_parsing_cf_name_and_job_id(log_line)\n\t        if last_line:\n\t            self.all_lines_added()\n\t    def __str__(self):\n\t        return f\"LogEntry (lines:{self.get_lines_idxs_range()}), Start:\\n\" \\\n", "               f\"{self.msg_lines[0]}\"\n\t    def try_parsing_cf_name_and_job_id(self, log_line):\n\t        match = re.findall(regexes.CF_WITH_JOB_ID, log_line)\n\t        if not match:\n\t            return\n\t        assert len(match) == 1 and len(match[0]) == 2\n\t        self.cf_name, self.job_id = match[0]\n\t        self.job_id = int(self.job_id)\n\t    def validate_not_finalized(self, log_line=None):\n\t        if self.is_finalized:\n", "            msg = \"Entry already finalized.\"\n\t            if log_line:\n\t                msg += f\". Added line:\\n{log_line}\\n\"\n\t            msg += f\"\\n{self}\"\n\t            raise utils.ParsingAssertion(msg, self.start_line_idx)\n\t    def validate_not_adding_entry_start_line(self, log_line):\n\t        if LogEntry.is_entry_start(log_line):\n\t            raise utils.ParsingAssertion(\n\t                f\"Illegal attempt to add an entry start as a line to \"\n\t                f\"an existing entry. Line:\\n{log_line}\\n{self}\")\n", "    def add_line(self, log_line, last_line=False):\n\t        self.validate_not_finalized(log_line)\n\t        self.validate_not_adding_entry_start_line(log_line)\n\t        # self.msg_lines.append(log_line.strip())\n\t        self.msg_lines.append(log_line)\n\t        if last_line:\n\t            self.all_lines_added()\n\t    def all_lines_added(self):\n\t        self.validate_not_finalized()\n\t        assert not self.is_finalized\n", "        self.is_finalized = True\n\t        return self\n\t    def have_all_lines_been_added(self):\n\t        return self.is_finalized\n\t    def get_start_line_idx(self):\n\t        return self.start_line_idx\n\t    def get_start_line_num(self):\n\t        return self.get_start_line_idx() + 1\n\t    def get_end_line_idx(self):\n\t        return self.start_line_idx + len(self.msg_lines) - 1\n", "    def get_end_line_num(self):\n\t        return self.get_end_line_idx() + 1\n\t    def get_lines_idxs_range(self):\n\t        return self.start_line_idx, self.start_line_idx+len(self.msg_lines)\n\t    def get_time(self):\n\t        return self.time\n\t    def get_gmt_timestamp(self):\n\t        return utils.get_gmt_timestamp_us(self.time)\n\t    def get_code_pos(self):\n\t        return self.code_pos\n", "    def get_msg_lines(self):\n\t        return [line.strip() for line in self.msg_lines]\n\t    def get_non_stripped_msg_lines(self):\n\t        return self.msg_lines\n\t    def get_msg(self):\n\t        return \"\\n\".join(self.get_msg_lines()).strip()\n\t    def get_non_stripped_msg(self):\n\t        return \"\\n\".join(self.msg_lines)\n\t    def is_warn_msg(self):\n\t        return self.warning_type\n", "    def get_warning_type(self):\n\t        return self.warning_type\n\t    def get_cf_name(self):\n\t        return self.cf_name\n\t    def get_job_id(self):\n\t        return self.job_id\n"]}
{"filename": "console_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport io\n\timport logging\n\timport display_utils\n\timport log_file\n\timport utils\n\tfrom log_file import ParsedLog\n\tdef get_title(log_file_path, parsed_log):\n", "    return f\"Parsing of: {log_file_path}\"\n\tdef print_title(f, log_file_path, parsed_log):\n\t    title = get_title(log_file_path, parsed_log)\n\t    print(f\"{title}\", file=f)\n\t    print(len(title) * \"=\", file=f)\n\tdef print_cf_console_printout(f, parsed_log, db_size_msg_suffix):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    cfs_info_for_display = \\\n\t        display_utils.prepare_general_cf_info_for_display(parsed_log)\n\t    cfs_display_values = []\n", "    for cf_name, cf_info in cfs_info_for_display.items():\n\t        row = [\n\t            cf_name,\n\t            cf_info[\"CF Size\"],\n\t            cf_info[\"Avg. Key Size\"],\n\t            cf_info[\"Avg. Value Size\"],\n\t            cf_info[\"Compaction Style\"],\n\t            cf_info[\"Compression\"],\n\t            cf_info[\"Filter-Policy\"]\n\t        ]\n", "        cfs_display_values.append(row)\n\t    size_suffix = \"\"\n\t    if db_size_msg_suffix is not None:\n\t        size_suffix = f\"({db_size_msg_suffix})\"\n\t    table_header = [\"Column Family\",\n\t                    f\"Size {size_suffix}\",\n\t                    \"Avg. Key Size\",\n\t                    \"Avg. Value Size\",\n\t                    \"Compaction Style\",\n\t                    \"Compression\",\n", "                    \"Filter-Policy\"]\n\t    ascii_table = display_utils.generate_ascii_table(table_header,\n\t                                                     cfs_display_values)\n\t    print(ascii_table, file=f)\n\tdef print_general_info(f, parsed_log: ParsedLog):\n\t    disp_dict = \\\n\t        display_utils.prepare_db_wide_info_for_display(parsed_log)\n\t    if isinstance(disp_dict[\"Error Messages\"], dict):\n\t        error_lines = \"\"\n\t        for error_time, error_msg in \\\n", "                disp_dict[\"Error Messages\"].items():\n\t            error_lines += f\"\\n{error_time} {error_msg}\"\n\t        disp_dict[\"Error Messages\"] = error_lines\n\t    if isinstance(disp_dict[\"Fatal Messages\"], dict):\n\t        error_lines = \"\"\n\t        for error_time, error_msg in \\\n\t                disp_dict[\"Fatal Messages\"].items():\n\t            error_lines += f\"\\n{error_time} {error_msg}\"\n\t        disp_dict[\"Fatal Messages\"] = error_lines\n\t    suffix = \"\"\n", "    msg1 = None\n\t    db_size_msg_suffix = None\n\t    db_size_time = disp_dict[\"DB Size Time\"]\n\t    if disp_dict[\"DB Size Time\"] is not None:\n\t        suffix += \"*\"\n\t        db_size_msg_suffix = suffix\n\t        disp_dict = \\\n\t            utils.replace_key_keep_order(disp_dict,\n\t                                         \"DB Size\",\n\t                                         f\"DB Size ({suffix})\")\n", "        msg1 = f\"({suffix}) Data is calculated at: {db_size_time}\"\n\t    del disp_dict[\"DB Size Time\"]\n\t    msg2 = None\n\t    ingest_time = disp_dict[\"Ingest Time\"]\n\t    if ingest_time is not None:\n\t        if db_size_time != ingest_time:\n\t            suffix += \"*\"\n\t            msg2 = f\"({suffix}) Ingest Data are calculated at: {ingest_time}\"\n\t        disp_dict = \\\n\t            utils.replace_key_keep_order(\n", "                disp_dict, \"Ingest\", f\"Ingest ({suffix})\")\n\t    del disp_dict[\"Ingest Time\"]\n\t    msg3 = None\n\t    num_cfs_info_key = \"Num CF-s Info\"\n\t    if \"Num CF-s Info\" in disp_dict:\n\t        suffix += \"*\"\n\t        disp_dict = \\\n\t            utils.replace_key_keep_order(\n\t                disp_dict, \"Num CF-s\", f\"Num CF-s ({suffix})\")\n\t        num_cfs_info = disp_dict[num_cfs_info_key]\n", "        msg3 = f\"({suffix}) {num_cfs_info}\"\n\t        del disp_dict[num_cfs_info_key]\n\t    width = 25\n\t    for field_name, value in disp_dict.items():\n\t        print(f\"{field_name.ljust(width)}: {value}\", file=f)\n\t    print_cf_console_printout(f, parsed_log, db_size_msg_suffix)\n\t    if msg1 is not None:\n\t        print(msg1, file=f)\n\t    if msg2 is not None:\n\t        print(msg2, file=f)\n", "    if msg3 is not None:\n\t        print(msg3, file=f)\n\tdef get_console_output(log_file_path, parsed_log, output_type):\n\t    logging.debug(f\"Preparing {output_type} Console Output\")\n\t    f = io.StringIO()\n\t    if output_type == utils.ConsoleOutputType.SHORT:\n\t        print_title(f, log_file_path, parsed_log)\n\t        print_general_info(f, parsed_log)\n\t    return f.getvalue()\n"]}
{"filename": "log_parser.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport argparse\n\timport io\n\timport logging\n\timport logging.config\n\timport os\n\timport pathlib\n\timport shutil\n", "import sys\n\timport textwrap\n\timport threading\n\timport time\n\tfrom pathlib import Path\n\timport console_outputter\n\timport csv_outputter\n\timport json_outputter\n\timport utils\n\tfrom log_file import ParsedLog\n", "DEBUG_MODE = True\n\t# event to signal the process bar thread to exit\n\texit_event = threading.Event()\n\tdef exit_program(exit_code):\n\t    exit_event.set()\n\t    exit(exit_code)\n\tdef display_process_bar():\n\t    # Simple textual process bar that starts \"playing\" once parsing\n\t    # exceeds 1 second\n\t    time_from_last_print = 0\n", "    line = \"Parsing \"\n\t    while True:\n\t        time.sleep(0.05)\n\t        if exit_event.is_set():\n\t            break\n\t        time_from_last_print += 50\n\t        if time_from_last_print >= 1000:\n\t            line += \".\"\n\t            print(line, end='\\r')\n\t            sys.stdout.flush()\n", "            time_from_last_print = 0\n\tdef parse_log(log_file_path):\n\t    if not os.path.isfile(log_file_path):\n\t        raise utils.LogFileNotFoundError(log_file_path)\n\t    logging.debug(f\"Parsing {log_file_path}\")\n\t    with open(log_file_path) as log_file:\n\t        logging.debug(f\"Starting to read the contents of {log_file_path}\")\n\t        log_lines = log_file.readlines()\n\t        logging.debug(f\"Completed reading the contents of {log_file_path}\")\n\t        return ParsedLog(log_file_path, log_lines,\n", "                         should_init_baseline_info=True)\n\tdef setup_cmd_line_parser():\n\t    epilog = textwrap.dedent('''\\\n\t    Notes:\n\t    - The default it to print to the console in a short format.\n\t    - It is possible to specify both json and console outputs. Both will be generated.\n\t    ''')  # noqa\n\t    parser = argparse.ArgumentParser(\n\t        formatter_class=argparse.RawDescriptionHelpFormatter,\n\t        epilog=epilog)\n", "    parser.add_argument(\"log_file_path\",\n\t                        metavar=\"log-file-path\",\n\t                        help=\"A path to a log file to parse \"\n\t                             \"(default: %(default)s)\")\n\t    parser.add_argument(\"-c\", \"--console\",\n\t                        choices=[\"short\", \"long\"],\n\t                        help=\"Print to console a summary (short) or a \"\n\t                             \"detailed (long) output (default: %(default)s)\")\n\t    parser.add_argument(\"-j\", \"--generate-json\",\n\t                        action=\"store_true\",\n", "                        default=False,\n\t                        help=f\"Optionally generate a JSON file in the \"\n\t                             f\"output folder with detailed information. \"\n\t                             f\"If generated, it will be called \"\n\t                             f\"{utils.DEFAULT_JSON_FILE_NAME}.\"\n\t                             f\"(default: %(default)s)\")\n\t    parser.add_argument(\"-o\", \"--output-folder\",\n\t                        default=utils.DEFAULT_OUTPUT_FOLDER,\n\t                        help=\"The name of the folder where output \"\n\t                             \"files will be stored in SUB-FOLDERS \"\n", "                             f\"named \"\n\t                             f\"{utils.OUTPUT_SUB_FOLDER_PREFIX}dddd.\"\n\t                             \"'dddd' is the run number (default: %(default)s)\")\n\t    parser.add_argument(\"-l\", \"--generate-log\",\n\t                        action=\"store_true\",\n\t                        default=False,\n\t                        help=\"Generate a log file for the parser's log \"\n\t                             \"messages (default: %(default)s)\")\n\t    return parser\n\tdef validate_and_sanitize_cmd_line_args(cmdline_args):\n", "    # The default is short console output\n\t    if not cmdline_args.console and not cmdline_args.generate_json:\n\t        cmdline_args.console = utils.ConsoleOutputType.SHORT\n\tdef handle_exception(exception, console, should_exit):\n\t    logging.exception(f\"\\n{exception}\")\n\t    if console:\n\t        if hasattr(exception, 'msg'):\n\t            print(exception.msg, file=sys.stderr)\n\t        else:\n\t            print(exception, file=sys.stderr)\n", "    if should_exit:\n\t        exit_program(1)\n\tdef report_exception(exception, console):\n\t    handle_exception(exception, console, should_exit=True)\n\tdef fatal_exception(exception, console=True):\n\t    handle_exception(exception, console, should_exit=True)\n\tdef verify_min_python_version():\n\t    if sys.version_info.major < utils.MIN_PYTHON_VERSION_MAJOR or \\\n\t            sys.version_info.minor < utils.MIN_PYTHON_VERSION_MINOR:\n\t        msg = f\"The log parser tool requires Python Version >= \" \\\n", "              f\"{utils.MIN_PYTHON_VERSION_MAJOR}.\" \\\n\t              f\"{utils.MIN_PYTHON_VERSION_MINOR} \" \\\n\t              f\"(Yours is {sys.version_info.major}.{sys.version_info.minor})\"\n\t        print(msg, file=sys.stderr)\n\t        exit_program(1)\n\tdef setup_logger(generate_log, output_folder):\n\t    if not generate_log:\n\t        logging.root.setLevel(logging.FATAL)\n\t        return\n\t    my_log_file_path = utils.get_log_file_path(output_folder)\n", "    logging.basicConfig(filename=my_log_file_path,\n\t                        format=\"%(asctime)s - %(levelname)s [%(filename)s \"\n\t                               \"(%(funcName)s:%(lineno)d)] %(message)s)\",\n\t                        level=logging.DEBUG)\n\t    return my_log_file_path\n\tdef prepare_output_folder(output_folder_parent):\n\t    output_path_parent = pathlib.Path(output_folder_parent)\n\t    largest_num = 0\n\t    if output_path_parent.exists():\n\t        for file in output_path_parent.iterdir():\n", "            name = file.name\n\t            if name.startswith(utils.OUTPUT_SUB_FOLDER_PREFIX):\n\t                name = name[len(utils.OUTPUT_SUB_FOLDER_PREFIX):]\n\t                if name.isnumeric() and len(name) == 4:\n\t                    num = int(name)\n\t                    largest_num = max(largest_num, num)\n\t        if largest_num == 9999:\n\t            largest_num = 1\n\t    output_folder = f\"{output_folder_parent}/\" \\\n\t                    f\"{utils.OUTPUT_SUB_FOLDER_PREFIX}\" \\\n", "                    f\"{largest_num + 1:04}\"\n\t    if not output_path_parent.exists():\n\t        os.makedirs(output_folder_parent)\n\t    shutil.rmtree(output_folder, ignore_errors=True)\n\t    os.makedirs(output_folder)\n\t    return output_folder\n\tdef print_to_console_if_applicable(cmdline_args, log_file_path,\n\t                                   parsed_log, json_content):\n\t    f = io.StringIO()\n\t    console_content = None\n", "    if cmdline_args.console == utils.ConsoleOutputType.SHORT:\n\t        f.write(\n\t            console_outputter.get_console_output(\n\t                log_file_path,\n\t                parsed_log,\n\t                cmdline_args.console))\n\t        console_content = f.getvalue()\n\t    elif cmdline_args.console == utils.ConsoleOutputType.LONG:\n\t        console_content = json_outputter.get_json_dump_str(json_content)\n\t    if console_content is not None:\n", "        print()\n\t        print(console_content)\n\tdef generate_json_if_applicable(\n\t        cmdline_args, parsed_log, csvs_paths, output_folder,\n\t        report_to_console):\n\t    json_content = None\n\t    if cmdline_args.generate_json or \\\n\t            cmdline_args.console == utils.ConsoleOutputType.LONG:\n\t        json_content = json_outputter.get_json(parsed_log)\n\t        json_content[\"CSV-s\"] = csvs_paths\n", "        if cmdline_args.generate_json:\n\t            json_file_name = utils.DEFAULT_JSON_FILE_NAME\n\t            json_outputter.write_json(json_file_name,\n\t                                      json_content, output_folder,\n\t                                      report_to_console)\n\t    return json_content\n\tdef generate_csvs_if_applicable(parsed_log, output_folder, report_to_console):\n\t    assert isinstance(parsed_log, ParsedLog)\n\t    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    events_mngr = parsed_log.get_events_mngr()\n", "    stats_mngr = parsed_log.get_stats_mngr()\n\t    compactions_monitor = parsed_log.get_compactions_monitor()\n\t    counters_mngr = \\\n\t        parsed_log.get_counters_mngr()\n\t    counters_csv_path = csv_outputter.generate_counters_csv(\n\t        counters_mngr, output_folder, report_to_console)\n\t    human_readable_histograms_csv_file_path, tools_histograms_csv_file_path = \\\n\t        csv_outputter.generate_histograms_csv(\n\t            counters_mngr, output_folder, report_to_console)\n\t    compaction_stats_mngr = stats_mngr.get_compactions_stats_mngr()\n", "    compactions_stats_csv_path = csv_outputter.generate_compactions_stats_csv(\n\t        compaction_stats_mngr, output_folder, report_to_console)\n\t    compactions_csv_path = csv_outputter.generate_compactions_csv(\n\t        compactions_monitor, output_folder, report_to_console)\n\t    flushes_csv_path = csv_outputter.generate_flushes_csv(\n\t        cfs_names, events_mngr, output_folder, report_to_console)\n\t    def generate_disp_path(path):\n\t        if path is None:\n\t            return utils.FILE_NOT_GENERATED_TEXT\n\t        assert isinstance(path, Path)\n", "        return str(path)\n\t    return {\n\t        \"Counters\": generate_disp_path(counters_csv_path),\n\t        \"Histograms (Human-Readable)\":\n\t            generate_disp_path(human_readable_histograms_csv_file_path),\n\t        \"Histograms (Tools)\":\n\t            generate_disp_path(tools_histograms_csv_file_path),\n\t        \"Compactions-Stats\": generate_disp_path(compactions_stats_csv_path),\n\t        \"Compactions\": generate_disp_path(compactions_csv_path),\n\t        \"Flushes\": generate_disp_path(flushes_csv_path)\n", "    }\n\tdef main():\n\t    verify_min_python_version()\n\t    parser = setup_cmd_line_parser()\n\t    cmdline_args = parser.parse_args()\n\t    output_folder = cmdline_args.output_folder\n\t    output_folder = prepare_output_folder(output_folder)\n\t    my_log_file_path = setup_logger(cmdline_args.generate_log,\n\t                                    output_folder)\n\t    validate_and_sanitize_cmd_line_args(cmdline_args)\n", "    t = threading.Thread(target=display_process_bar)\n\t    t.start()\n\t    try:\n\t        log_file_path = cmdline_args.log_file_path\n\t        log_file_path = os.path.abspath(log_file_path)\n\t        parsed_log = parse_log(log_file_path)\n\t        exit_event.set()\n\t        if cmdline_args.console == utils.ConsoleOutputType.LONG:\n\t            report_to_console = False\n\t        else:\n", "            report_to_console = True\n\t        if report_to_console:\n\t            log_file_path_str = parsed_log.get_log_file_path()\n\t            print(f\"Log file: {str(Path(log_file_path_str).as_uri())}\")\n\t            baseline_info = parsed_log.get_baseline_info()\n\t            if baseline_info is not None:\n\t                print(f\"Baseline Log: \"\n\t                      f\"{str(baseline_info.baseline_log_path.as_uri())}\")\n\t            else:\n\t                print(\"No Available Baseline Log\")\n", "        csvs_paths = generate_csvs_if_applicable(parsed_log, output_folder,\n\t                                                 report_to_console)\n\t        json_content = generate_json_if_applicable(\n\t            cmdline_args, parsed_log, csvs_paths, output_folder,\n\t            report_to_console)\n\t        print_to_console_if_applicable(cmdline_args, log_file_path, parsed_log,\n\t                                       json_content)\n\t    except utils.ParsingError as exception:\n\t        report_exception(exception, console=True)\n\t    except utils.LogFileNotFoundError as exception:\n", "        report_exception(exception, console=True)\n\t    except utils.EmptyLogFile as exception:\n\t        report_exception(exception, console=True)\n\t    except utils.InvalidLogFile as exception:\n\t        report_exception(exception, console=True)\n\t    except ValueError as exception:\n\t        fatal_exception(exception)\n\t    except AssertionError as exception:\n\t        if not DEBUG_MODE:\n\t            fatal_exception(exception)\n", "        else:\n\t            exit_event.set()\n\t            raise\n\t    except Exception as exception:  # noqa\n\t        if not DEBUG_MODE:\n\t            print(f\"An unrecoverable error occurred while parsing \"\n\t                  f\"{log_file_path}.\", file=sys.stderr)\n\t            print(\"Please open an issue in the tool's GitHub repository \"\n\t                  \"(https://github.com/speedb-io/log-parser)\", file=sys.stderr)\n\t            if my_log_file_path:\n", "                print(f\"\\nMore details may be found in {my_log_file_path}\",\n\t                      file=sys.stderr)\n\t            fatal_exception(exception, console=False)\n\t        else:\n\t            exit_event.set()\n\t            raise\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "display_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport io\n\timport logging\n\tfrom dataclasses import dataclass, asdict\n\tfrom pathlib import Path\n\timport baseline_log_files_utils\n\timport cache_utils\n\timport calc_utils\n", "import db_files\n\timport db_options\n\timport log_file\n\timport utils\n\tfrom counters import CountersMngr\n\tfrom db_options import DatabaseOptions, CfsOptionsDiff, SectionType\n\tfrom db_options import SanitizedValueType\n\tfrom stats_mngr import CompactionStatsMngr, StatsMngr\n\tfrom warnings_mngr import WarningType, WarningElementInfo, WarningsMngr\n\tnum_for_display = utils.get_human_readable_number\n", "num_bytes_for_display = utils.get_human_readable_num_bytes\n\tCFS_COMMON_KEY = \"CF-s (Common)\"\n\tCFS_SPECIFIC_KEY = \"CF-s (Specific)\"\n\tTABLE_KEY = \"Block-Based Table\"\n\tdef format_value(value, suffix=None, conv_func=None):\n\t    if value is not None:\n\t        if conv_func is not None:\n\t            value = conv_func(value)\n\t        if suffix is not None:\n\t            suffix = \" \" + suffix\n", "        else:\n\t            suffix = \"\"\n\t        return f\"{value}{suffix}\"\n\t    else:\n\t        return \"No Information\"\n\tdef prepare_db_wide_user_opers_stats_for_display(db_wide_info):\n\t    display_info = dict()\n\t    def get_disp_value(percent, num, total_num, oper_name,\n\t                       unavailability_reason):\n\t        if unavailability_reason is None:\n", "            assert total_num is not None\n\t            if total_num > 0 and num > 0:\n\t                return f\"{percent:.1f}% ({num}/{total_num})\"\n\t            else:\n\t                return f\"0 (No {oper_name} Operations)\"\n\t        else:\n\t            return f\"{utils.DATA_UNAVAILABLE_TEXT} ({unavailability_reason})\"\n\t    user_opers_stats = db_wide_info[\"user_opers_stats\"]\n\t    assert isinstance(user_opers_stats, calc_utils.UserOpersStats)\n\t    total_num_user_opers = user_opers_stats.total_num_user_opers\n", "    reason = user_opers_stats.unavailability_reason\n\t    display_info['Writes'] = \\\n\t        get_disp_value(user_opers_stats.percent_written,\n\t                       user_opers_stats.num_written,\n\t                       total_num_user_opers,\n\t                       \"Write\", reason)\n\t    display_info['Reads'] = \\\n\t        get_disp_value(user_opers_stats.percent_read,\n\t                       user_opers_stats.num_read,\n\t                       total_num_user_opers,\n", "                       \"Read\", reason)\n\t    display_info['Seeks'] = \\\n\t        get_disp_value(user_opers_stats.percent_seek,\n\t                       user_opers_stats.num_seek,\n\t                       total_num_user_opers,\n\t                       \"Seek\", reason)\n\t    delete_opers_stats = db_wide_info[\"delete_opers_stats\"]\n\t    assert isinstance(delete_opers_stats, calc_utils.DeleteOpersStats)\n\t    display_info['Deleted (Flushed) Entries'] = \\\n\t        get_disp_value(delete_opers_stats.total_percent_deletes,\n", "                       delete_opers_stats.total_num_deletes,\n\t                       delete_opers_stats.total_num_flushed_entries,\n\t                       \"Delete\",\n\t                       delete_opers_stats.unavailability_reason)\n\t    return display_info\n\t@dataclass\n\tclass NotableEntityInfo:\n\t    display_title: str\n\t    display_text: str\n\t    special_value_type: SanitizedValueType\n", "    special_value_text: str\n\t    display_value: bool\n\tnotable_entities = {\n\t    \"statistics\": NotableEntityInfo(\"Statistics\",\n\t                                    \"Available\",\n\t                                    SanitizedValueType.NULL_PTR,\n\t                                    utils.NO_STATS_TEXT,\n\t                                    display_value=False)\n\t}\n\tdef get_db_wide_notable_entities_display_info(parsed_log):\n", "    display_info = {}\n\t    db_opts = parsed_log.get_database_options()\n\t    for option_name, info in notable_entities.items():\n\t        option_value = db_opts.get_db_wide_option(option_name)\n\t        if option_value is None:\n\t            logging.warning(f\"Option {option_name} not found in \"\n\t                            f\"{parsed_log.get_log_file_path()}\")\n\t            continue\n\t        option_value_type = SanitizedValueType.get_type_from_str(option_value)\n\t        if option_value_type == info.special_value_type:\n", "            display_value = info.special_value_text\n\t        else:\n\t            if info.display_text is None:\n\t                display_value = option_value\n\t            else:\n\t                if info.display_value:\n\t                    display_value = f\"{info.display_text} ({option_value})\"\n\t                else:\n\t                    display_value = info.display_text\n\t        display_info[info.display_title] = display_value\n", "    return display_info\n\tdef prepare_error_or_fatal_warnings_for_display(warnings_mngr, prepare_error):\n\t    assert isinstance(warnings_mngr, WarningsMngr)\n\t    if prepare_error:\n\t        all_type_warnings = \\\n\t            warnings_mngr.get_warnings_of_type(WarningType.ERROR)\n\t        if not all_type_warnings:\n\t            return \"No Errors\"\n\t    else:\n\t        all_type_warnings = \\\n", "            warnings_mngr.get_warnings_of_type(WarningType.FATAL)\n\t        if not all_type_warnings:\n\t            return \"No Fatals\"\n\t    warnings_tuples = list()\n\t    for cf_name, cf_info in all_type_warnings.items():\n\t        for category, err_infos in cf_info.items():\n\t            for info in err_infos:\n\t                assert isinstance(info, WarningElementInfo)\n\t                warnings_tuples.append((info.time, info.warning_msg))\n\t    # First item in every tuple is time => will be sorted on time\n", "    warnings_tuples.sort()\n\t    return {time: msg for time, msg in warnings_tuples}\n\tdef prepare_ingest_info_for_db_wide_info_display(db_wide_info):\n\t    ingest_info = db_wide_info[\"ingest_info\"]\n\t    if ingest_info is not None:\n\t        assert isinstance(ingest_info, calc_utils.DbIngestInfo)\n\t        return prepare_db_ingest_info_for_display(ingest_info)\n\t    else:\n\t        unavailability_reason = utils.NO_INGEST_TEXT\n\t        return {\n", "            \"Ingest\": unavailability_reason,\n\t            \"Ingest Rate\": unavailability_reason,\n\t            \"Ingest Time\": None\n\t        }\n\tdef prepare_db_wide_info_for_display(parsed_log):\n\t    log_file_time_info = calc_utils.get_log_file_time_info(parsed_log)\n\t    assert isinstance(log_file_time_info, calc_utils.LogFileTimeInfo)\n\t    display_info = {}\n\t    db_wide_info = calc_utils.get_db_wide_info(parsed_log)\n\t    db_wide_notable_entities = \\\n", "        get_db_wide_notable_entities_display_info(parsed_log)\n\t    display_info[\"Name\"] = str(Path(parsed_log.get_log_file_path()))\n\t    display_info[\"Start Time\"] = log_file_time_info.start_time\n\t    display_info[\"End Time\"] = log_file_time_info.end_time\n\t    display_info[\"Log Time Span\"] = \\\n\t        utils.convert_seconds_to_dd_hh_mm_ss(log_file_time_info.span_seconds)\n\t    display_info[\"Creator\"] = db_wide_info['creator']\n\t    display_info[\"Version\"] = f\"{db_wide_info['version']} \" \\\n\t                              f\"[{db_wide_info['git_hash']}]\"\n\t    db_size_bytes_time = db_wide_info['db_size_bytes_time']\n", "    if db_wide_info['db_size_bytes'] is not None:\n\t        display_info[\"DB Size\"] = \\\n\t            num_bytes_for_display(db_wide_info['db_size_bytes'])\n\t    else:\n\t        display_info[\"DB Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\t    display_info[\"DB Size Time\"] = db_size_bytes_time\n\t    if db_wide_info[\"num_keys_written\"] is not None:\n\t        display_info[\"Num Keys Written\"] = \\\n\t            num_for_display(db_wide_info['num_keys_written'])\n\t    else:\n", "        display_info[\"Num Keys Written\"] = utils.DATA_UNAVAILABLE_TEXT\n\t    if db_wide_info['avg_key_size_bytes'] is not None:\n\t        display_info[\"Avg. Written Key Size\"] = \\\n\t            num_bytes_for_display(db_wide_info['avg_key_size_bytes'])\n\t    else:\n\t        display_info[\"Avg. Written Key Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\t    if db_wide_info['avg_value_size_bytes'] is not None:\n\t        display_info[\"Avg. Written Value Size\"] = \\\n\t            num_bytes_for_display(db_wide_info['avg_value_size_bytes'])\n\t    else:\n", "        display_info[\"Avg. Written Value Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\t    display_info[\"Num Warnings\"] = db_wide_info['num_warnings']\n\t    if db_wide_info['errors'] is not None:\n\t        display_info[\"Error Messages\"] = db_wide_info['errors']\n\t    else:\n\t        display_info[\"Error Messages\"] = \"No Error Messages\"\n\t    if db_wide_info['fatals'] is not None:\n\t        display_info[\"Fatal Messages\"] = db_wide_info['fatals']\n\t    else:\n\t        display_info[\"Fatal Messages\"] = \"No Fatal Messages\"\n", "    display_info.update(\n\t        prepare_ingest_info_for_db_wide_info_display(db_wide_info))\n\t    display_info.update(db_wide_notable_entities)\n\t    display_info.update(\n\t        prepare_db_wide_user_opers_stats_for_display(db_wide_info))\n\t    num_cfs_info_msg = \\\n\t        \"Please see the 'Ability to determine the number of cf-s' section in the log parser's documentation for more information\" # noqa\n\t    if db_wide_info['num_cfs'] is not None:\n\t        total_num_cfs = db_wide_info['num_cfs']\n\t        num_non_auto_gen_cfs_with_options = \\\n", "            parsed_log.get_cfs_names_that_have_options(\n\t                include_auto_generated=False)\n\t        display_info['Num CF-s'] = total_num_cfs\n\t        if total_num_cfs != len(num_non_auto_gen_cfs_with_options):\n\t            display_info[\"Num CF-s Info\"] = num_cfs_info_msg\n\t    else:\n\t        display_info['Num CF-s'] = \"Can't be accurately determined\"\n\t        display_info[\"Num CF-s Info\"] = num_cfs_info_msg\n\t    return display_info\n\tdef prepare_general_cf_info_for_display(parsed_log):\n", "    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    filter_stats = \\\n\t        calc_utils.calc_filter_stats(cfs_names,\n\t                                     parsed_log.get_database_options(),\n\t                                     parsed_log.get_files_monitor(),\n\t                                     parsed_log.get_counters_mngr())\n\t    display_info = {}\n\t    events_mngr = parsed_log.get_events_mngr()\n\t    compaction_stats_mngr = \\\n", "        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\t    for cf_name in cfs_names:\n\t        table_creation_stats = \\\n\t            calc_utils.calc_cf_table_creation_stats(cf_name, events_mngr)\n\t        cf_options = calc_utils.get_applicable_cf_options(\n\t            parsed_log.get_database_options())\n\t        cf_size_bytes = compaction_stats_mngr.get_cf_size_bytes_at_end(cf_name)\n\t        display_info[cf_name] = {}\n\t        cf_display_info = display_info[cf_name]\n\t        if cf_size_bytes is not None:\n", "            cf_display_info[\"CF Size\"] = num_bytes_for_display(cf_size_bytes)\n\t        else:\n\t            cf_display_info[\"CF Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\t        cf_display_info[\"Avg. Key Size\"] = \\\n\t            num_bytes_for_display(table_creation_stats['avg_key_size'])\n\t        cf_display_info[\"Avg. Value Size\"] = \\\n\t            num_bytes_for_display(table_creation_stats['avg_value_size'])\n\t        if cf_name in cf_options['compaction_style']:\n\t            if cf_options['compaction_style'][cf_name] is not None:\n\t                cf_display_info[\"Compaction Style\"] = \\\n", "                    cf_options['compaction_style'][cf_name]\n\t            else:\n\t                cf_display_info[\"Compaction Style\"] = utils.UNKNOWN_VALUE_TEXT\n\t        else:\n\t            cf_display_info[\"Compaction Style\"] = utils.UNKNOWN_VALUE_TEXT\n\t        if cf_name in cf_options['compression']:\n\t            if cf_options['compression'][cf_name] is not None:\n\t                cf_display_info[\"Compression\"] = \\\n\t                    cf_options['compression'][cf_name]\n\t            else:\n", "                cf_display_info[\"Compression\"] = utils.UNKNOWN_VALUE_TEXT\n\t        elif calc_utils.is_cf_compression_by_level(parsed_log, cf_name):\n\t            cf_display_info[\"Compression\"] = \"Per-Level\"\n\t        else:\n\t            cf_display_info[\"Compression\"] = utils.UNKNOWN_VALUE_TEXT\n\t        if cf_name in filter_stats.files_filter_stats:\n\t            cf_files_filter_stats = filter_stats.files_filter_stats[cf_name]\n\t        else:\n\t            cf_files_filter_stats = None\n\t        cf_display_info[\"Filter-Policy\"] = \\\n", "            prepare_cf_filter_stats_for_display(cf_files_filter_stats,\n\t                                                format_as_dict=False)\n\t    return display_info\n\tdef prepare_warn_warnings_for_display(warn_warnings_info):\n\t    # The input is in the following format:\n\t    # {<warning-type>: {<cf-name>: {<category>: <number of messages>}}\n\t    disp = dict()\n\t    disp_db = dict()\n\t    disp_cfs = dict()\n\t    for cf_name, cf_info in warn_warnings_info.items():\n", "        if cf_name == utils.NO_CF:\n\t            disp_dict = disp_db\n\t        else:\n\t            disp_cfs[cf_name] = dict()\n\t            disp_dict = disp_cfs[cf_name]\n\t        for category, num_in_category in cf_info.items():\n\t            disp_category = category.value\n\t            disp_dict[disp_category] = num_in_category\n\t    if not disp_db and not disp_cfs:\n\t        return None\n", "    if disp_db:\n\t        disp[\"DB\"] = disp_db\n\t    else:\n\t        disp[\"DB\"] = \"No DB Warnings\"\n\t    if disp_cfs:\n\t        disp[\"CF-s\"] = disp_cfs\n\t    else:\n\t        disp[\"CF-s\"] = \"No CF-s Warnings\"\n\t    return disp\n\tdef prepare_cfs_common_options_for_display(cfs_common_options):\n", "    if cfs_common_options:\n\t        options, table_options = \\\n\t            DatabaseOptions.prepare_flat_full_names_cf_options_for_display(\n\t                cfs_common_options, None)\n\t        return {\n\t            \"CF\": options,\n\t            TABLE_KEY: table_options\n\t        }\n\t    else:\n\t        return \"No Common Options to All CF-s\"\n", "def prepare_cfs_specific_options_for_display(cfs_specific_options):\n\t    disp = {}\n\t    if cfs_specific_options:\n\t        for cf_name, cf_options in cfs_specific_options.items():\n\t            if cf_options:\n\t                disp_cf_options, disp_cf_table_options =\\\n\t                    DatabaseOptions.\\\n\t                    prepare_flat_full_names_cf_options_for_display(\n\t                        cf_options, None)\n\t                disp[cf_name] = {}\n", "                if disp_cf_options:\n\t                    disp[cf_name][\"CF\"] = \\\n\t                        disp_cf_options\n\t                else:\n\t                    disp[cf_name][\"CF\"] = \\\n\t                        \"No Specific Options\"\n\t                if disp_cf_table_options:\n\t                    disp[cf_name][TABLE_KEY] = \\\n\t                        disp_cf_table_options\n\t                else:\n", "                    disp[cf_name][TABLE_KEY] = \\\n\t                        \"No Specific Table Options\"\n\t                if not disp[cf_name]:\n\t                    del(disp[cf_name])\n\t    if not disp:\n\t        disp = \"No Specific CF-s Options\"\n\t    return disp\n\tdef get_all_options_for_display(parsed_log):\n\t    all_options = {}\n\t    db_opts = parsed_log.get_database_options()\n", "    cfs_common_options, cfs_specific_options =  \\\n\t        calc_utils.get_cfs_common_and_specific_options(db_opts)\n\t    db_disp_opts = db_opts.get_db_wide_options_for_display()\n\t    cfs_disp_opts = dict()\n\t    cfs_disp_opts[CFS_COMMON_KEY] = \\\n\t        prepare_cfs_common_options_for_display(cfs_common_options)\n\t    cfs_disp_opts[CFS_SPECIFIC_KEY] = \\\n\t        prepare_cfs_specific_options_for_display(cfs_specific_options)\n\t    all_options[\"DB\"] = db_disp_opts\n\t    all_options[\"CF-s\"] = cfs_disp_opts\n", "    return all_options\n\tdef get_diff_tuple_for_display(raw_diff_tuple):\n\t    return {utils.DIFF_BASELINE_NAME: raw_diff_tuple[0],\n\t            utils.DIFF_LOG_NAME: raw_diff_tuple[1]}\n\tdef prepare_db_wide_diff_dict_for_display(\n\t        product_name, baseline_log_path, baseline_version, db_wide_diff):\n\t    display_db_wide_diff = {\n\t        \"Baseline\": f\"{str(baseline_version)} ({product_name})\",\n\t        \"Baseline Log\": str(baseline_log_path)\n\t    }\n", "    if db_wide_diff is None:\n\t        display_db_wide_diff[\"DB\"] = \"No Diff\"\n\t        return display_db_wide_diff\n\t    del (db_wide_diff[CfsOptionsDiff.CF_NAMES_KEY])\n\t    display_db_wide_diff[\"DB\"] = {}\n\t    for full_option_name in db_wide_diff:\n\t        section_type = SectionType.extract_section_type(full_option_name)\n\t        option_name = \\\n\t            db_options.extract_option_name(full_option_name)\n\t        if section_type == SectionType.DB_WIDE:\n", "            display_db_wide_diff[\"DB\"][option_name] = \\\n\t                get_diff_tuple_for_display(db_wide_diff[full_option_name])\n\t        elif section_type == SectionType.VERSION:\n\t            pass\n\t        else:\n\t            assert False, \"Unexpected section type\"\n\t    if not display_db_wide_diff[\"DB\"]:\n\t        del (display_db_wide_diff[\"DB\"])\n\t    if list(display_db_wide_diff.keys()) == [\"Baseline\"]:\n\t        display_db_wide_diff = {}\n", "    return display_db_wide_diff\n\tdef prepare_cfs_diff_dict_for_display(common_diff, cfs_specific_diffs):\n\t    display_cfs_diff = dict()\n\t    if common_diff:\n\t        assert isinstance(common_diff, db_options.CfsOptionsDiff)\n\t        common_diff_dict = common_diff.get_diff_dict()\n\t        del(common_diff_dict[db_options.CfsOptionsDiff.CF_NAMES_KEY])\n\t        options, table_options = \\\n\t            DatabaseOptions.prepare_flat_full_names_cf_options_for_display(\n\t                common_diff_dict, get_diff_tuple_for_display)\n", "        display_cfs_diff[CFS_COMMON_KEY] = {\n\t            \"CF\": options,\n\t            TABLE_KEY: table_options\n\t        }\n\t    else:\n\t        display_cfs_diff[CFS_COMMON_KEY] = \"No Common Diff\"\n\t    display_cfs_diff[CFS_SPECIFIC_KEY] = dict()\n\t    if cfs_specific_diffs:\n\t        for cf_name, cf_specific_diff in cfs_specific_diffs.items():\n\t            if cf_specific_diff is not None:\n", "                assert isinstance(cf_specific_diff, db_options.CfsOptionsDiff)\n\t                cf_specific_diff_dict = cf_specific_diff.get_diff_dict()\n\t                del (cf_specific_diff_dict[\n\t                    db_options.CfsOptionsDiff.CF_NAMES_KEY])\n\t                options, table_options = \\\n\t                    DatabaseOptions.\\\n\t                    prepare_flat_full_names_cf_options_for_display(\n\t                        cf_specific_diff_dict, get_diff_tuple_for_display)\n\t                display_cfs_diff[CFS_SPECIFIC_KEY][cf_name] = {\n\t                    \"CF\": options,\n", "                    TABLE_KEY: table_options\n\t                }\n\t    if not display_cfs_diff[CFS_SPECIFIC_KEY]:\n\t        display_cfs_diff[CFS_SPECIFIC_KEY] = \"No CF-s Specific Diff\"\n\t    return display_cfs_diff\n\tdef get_options_baseline_diff_for_display(parsed_log):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    log_metadata = parsed_log.get_metadata()\n\t    log_database_options = parsed_log.get_database_options()\n\t    baseline_info = parsed_log.get_baseline_info()\n", "    if baseline_info is None:\n\t        return \"NO BASELINE FOUND\"\n\t    assert isinstance(baseline_info,\n\t                      baseline_log_files_utils.BaselineDBOptionsInfo)\n\t    baseline_opts = baseline_info.baseline_options.get_all_options()\n\t    log_opts = log_database_options.get_all_options()\n\t    db_wide_diff = \\\n\t        DatabaseOptions.get_db_wide_options_diff(baseline_opts, log_opts)\n\t    if db_wide_diff is not None:\n\t        db_wide_diff = db_wide_diff.get_diff_dict()\n", "    display_diff = prepare_db_wide_diff_dict_for_display(\n\t        log_metadata.get_product_name(), baseline_info.baseline_log_path,\n\t        baseline_info.closest_version, db_wide_diff)\n\t    common_diff, cfs_specific_diffs = \\\n\t        calc_utils.get_cfs_common_and_specific_diff_dicts(\n\t            baseline_info.baseline_options, log_database_options)\n\t    display_diff[\"CF-s\"] = \\\n\t        prepare_cfs_diff_dict_for_display(common_diff, cfs_specific_diffs)\n\t    return display_diff\n\tdef prepare_cf_flushes_stats_for_display(parsed_log):\n", "    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    disp = {}\n\t    def calc_sizes_histogram():\n\t        sizes_histogram = {}\n\t        bucket_min_size_mb = 0\n\t        for i, num_in_bucket in \\\n\t                enumerate(reason_stats.sizes_histogram):\n\t            if i < len(calc_utils.FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB):\n\t                bucket_max_size_mb = \\\n\t                    calc_utils.FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB[i]\n", "                bucket_title = f\"{bucket_min_size_mb} - \" \\\n\t                               f\"{bucket_max_size_mb} [MB]\"\n\t            else:\n\t                bucket_title = f\"> {bucket_min_size_mb} [MB]\"\n\t            bucket_min_size_mb = bucket_max_size_mb\n\t            sizes_histogram[bucket_title] = num_in_bucket\n\t        return sizes_histogram\n\t    def get_write_amp_level1():\n\t        cf_compaction_stats =\\\n\t            compactions_stats_mngr.get_cf_level_entries(cf_name)\n", "        if not cf_compaction_stats:\n\t            return None\n\t        last_dump_stats = cf_compaction_stats[-1]\n\t        return CompactionStatsMngr.get_level_field_value(\n\t            last_dump_stats, level=1,\n\t            field=CompactionStatsMngr.LevelFields.WRITE_AMP)\n\t    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    events_mngr = parsed_log.get_events_mngr()\n\t    stats_mngr = parsed_log.get_stats_mngr()\n\t    compactions_stats_mngr = stats_mngr.get_compactions_stats_mngr()\n", "    for cf_name in cfs_names:\n\t        cf_disp = dict()\n\t        cf_flushes_stats = \\\n\t            calc_utils.calc_cf_flushes_stats(cf_name, events_mngr)\n\t        if not cf_flushes_stats:\n\t            continue\n\t        write_amp_level1 = get_write_amp_level1()\n\t        if not write_amp_level1:\n\t            write_amp_level1 = utils.DATA_UNAVAILABLE_TEXT\n\t        cf_disp[\"L0->L1 Write-Amp\"] = write_amp_level1\n", "        for reason, reason_stats in cf_flushes_stats.items():\n\t            assert isinstance(reason_stats, calc_utils.PerFlushReasonStats)\n\t            cf_reason_disp = dict()\n\t            cf_reason_disp[\"Sizes Histogram\"] = calc_sizes_histogram()\n\t            cf_reason_disp[\"Num Flushes\"] = \\\n\t                num_for_display(reason_stats.num_flushes)\n\t            cf_reason_disp[\"Min Duration\"] = \\\n\t                format_value(reason_stats.min_duration_ms,\n\t                             suffix=\"ms\",\n\t                             conv_func=None)\n", "            cf_reason_disp[\"Max Duration\"] = \\\n\t                format_value(reason_stats.max_duration_ms,\n\t                             suffix=\"ms\",\n\t                             conv_func=None)\n\t            cf_reason_disp[\"Min Num Memtables\"] = \\\n\t                format_value(reason_stats.min_num_memtables,\n\t                             suffix=None,\n\t                             conv_func=None)\n\t            cf_reason_disp[\"Max Num Memtables\"] = \\\n\t                format_value(reason_stats.max_num_memtables,\n", "                             suffix=None,\n\t                             conv_func=None)\n\t            cf_reason_disp[\"Min Total Data Size\"] = \\\n\t                format_value(reason_stats.min_total_data_size_bytes,\n\t                             suffix=None,\n\t                             conv_func=num_bytes_for_display)\n\t            cf_reason_disp[\"Max Total Data Size\"] = \\\n\t                format_value(reason_stats.max_total_data_size_bytes,\n\t                             suffix=None,\n\t                             conv_func=num_bytes_for_display)\n", "            cf_disp[reason] = cf_reason_disp\n\t        disp[cf_name] = cf_disp\n\t    return disp\n\tdef prepare_global_compactions_stats_for_display(parsed_log):\n\t    disp = {}\n\t    compactions_monitor = parsed_log.get_compactions_monitor()\n\t    largest_compaction_size_bytes = \\\n\t        calc_utils.get_largest_compaction_size_bytes(compactions_monitor)\n\t    disp[\"Largest compaction size\"] = \\\n\t        num_bytes_for_display(largest_compaction_size_bytes)\n", "    return disp\n\tdef prepare_cf_compactions_stats_for_display(parsed_log):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    disp = {}\n\t    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    log_start_time = parsed_log.get_metadata().get_start_time()\n\t    compactions_monitor = parsed_log.get_compactions_monitor()\n\t    compactions_stats_mngr = \\\n\t        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\t    for cf_name in cfs_names:\n", "        cf_compactions_stats = \\\n\t            calc_utils.calc_cf_compactions_stats(\n\t                cf_name, log_start_time, compactions_monitor,\n\t                compactions_stats_mngr)\n\t        if cf_compactions_stats:\n\t            assert isinstance(cf_compactions_stats,\n\t                              calc_utils.CfCompactionStats)\n\t            s = cf_compactions_stats\n\t            if s.per_level_write_amp is not None:\n\t                per_level_write_amp = s.per_level_write_amp\n", "            else:\n\t                per_level_write_amp = \"No Write-Amp Info Found\"\n\t            disp[cf_name] = {\n\t                \"Num Compactions\": s.num_compactions,\n\t                \"Min Compactions BW\":\n\t                    format_value(s.min_compaction_bw_mbps, \"MBPS\"),\n\t                \"Max Compactions BW\":\n\t                    format_value(s.max_compaction_bw_mbps, \"MBPS\"),\n\t                \"Comp\": format_value(s.comp_sec, \"seconds\"),\n\t                \"Comp Merge CPU\": format_value(s.comp_merge_cpu_sec,\n", "                                               \"seconds\"),\n\t                \"Per-Level Write-Amp\": per_level_write_amp\n\t            }\n\t        else:\n\t            disp[cf_name] = \"No Compaction Stats\"\n\t    return disp\n\tdef prepare_cf_stalls_entries_for_display(parsed_log):\n\t    mngr = parsed_log.get_stats_mngr().get_cf_no_file_stats_mngr()\n\t    stall_counts = mngr.get_stall_counts()\n\t    display_stall_counts = {}\n", "    for cf_name in stall_counts.keys():\n\t        if stall_counts[cf_name]:\n\t            display_stall_counts[cf_name] = stall_counts[cf_name]\n\t    return display_stall_counts if display_stall_counts \\\n\t        else \"No Stalls\"\n\tdef generate_ascii_table(columns_names, table):\n\t    f = io.StringIO()\n\t    if len(table) < 1:\n\t        return\n\t    max_columns_widths = []\n", "    num_columns = len(columns_names)\n\t    for i in range(num_columns):\n\t        max_value_len = max([len(str(row[i])) for row in table])\n\t        column_name_len = len(columns_names[i])\n\t        max_columns_widths.append(2 + max([max_value_len, column_name_len]))\n\t    header_line = \"\"\n\t    for i, name in enumerate(columns_names):\n\t        width = max_columns_widths[i]\n\t        header_line += f'|{name.center(width)}'\n\t    header_line += '|'\n", "    print('-' * len(header_line), file=f)\n\t    print(header_line, file=f)\n\t    print('-' * len(header_line), file=f)\n\t    for row in table:\n\t        row_line = \"\"\n\t        for i, value in enumerate(row):\n\t            width = max_columns_widths[i]\n\t            row_line += f'|{str(value).center(width)}'\n\t        row_line += '|'\n\t        print(row_line, file=f)\n", "    print('-' * len(header_line), file=f)\n\t    return f.getvalue()\n\tdef prepare_cfs_size_bytes_growth_for_display(growth):\n\t    disp = {}\n\t    if not growth:\n\t        return utils.NO_GROWTH_INFO_TEXT\n\t    def get_delta_str(delta_value):\n\t        abs_delta_str = num_bytes_for_display(abs(delta_value))\n\t        if delta_value >= 0:\n\t            return f\"(+{abs_delta_str})\"\n", "        else:\n\t            return f\"(-{abs_delta_str})\"\n\t    def get_growth_str(start_value, end_value):\n\t        start_size_str = num_bytes_for_display(start_value)\n\t        if end_value is not None:\n\t            if start_value == end_value:\n\t                if start_value > 0:\n\t                    value_str = f\"{start_size_str} (No Change)\"\n\t                else:\n\t                    value_str = \"Empty Level\"\n", "            else:\n\t                end_size_str = num_bytes_for_display(end_value)\n\t                delta = end_value - start_value\n\t                delta_str = get_delta_str(delta)\n\t                value_str = \\\n\t                    f\"{start_size_str} -> {end_size_str}  {delta_str}\"\n\t        else:\n\t            # End size is unknown\n\t            value_str = f\"{start_size_str} -> (UNKNOWN SIZE)\"\n\t        return value_str\n", "    for cf_name in growth:\n\t        if growth[cf_name] is None:\n\t            disp[cf_name] = utils.NO_GROWTH_INFO_TEXT\n\t            continue\n\t        disp[cf_name] = {}\n\t        if not growth[cf_name]:\n\t            disp[cf_name] = utils.NO_GROWTH_INFO_TEXT\n\t            continue\n\t        total_bytes_start = 0\n\t        total_bytes_end = None\n", "        # The levelt are not ordered within growth[cf_name]\n\t        levels_and_sizes = list(growth[cf_name].items())\n\t        levels_and_sizes.sort()\n\t        for level, sizes_bytes in levels_and_sizes:\n\t            start_size_bytes = sizes_bytes[0]\n\t            end_size_bytes = sizes_bytes[1]\n\t            if start_size_bytes is None:\n\t                start_size_bytes = 0\n\t            # if end_size_bytes is None:\n\t            #     end_size_bytes = 0\n", "            disp[cf_name][f\"Level {level}\"] = get_growth_str(\n\t                start_size_bytes, end_size_bytes)\n\t            total_bytes_start += start_size_bytes\n\t            if end_size_bytes is not None:\n\t                if total_bytes_end is None:\n\t                    total_bytes_end = end_size_bytes\n\t                else:\n\t                    total_bytes_end += end_size_bytes\n\t        disp[cf_name][\"Sum\"] =\\\n\t            get_growth_str(total_bytes_start, total_bytes_end)\n", "    return disp\n\tdef prepare_db_ingest_info_for_display(ingest_info):\n\t    assert isinstance(ingest_info, calc_utils.DbIngestInfo)\n\t    disp = {}\n\t    if not ingest_info:\n\t        return \"No Ingest Info\"\n\t    disp[\"Ingest\"] = utils.get_human_readable_num_bytes(ingest_info.ingest)\n\t    disp[\"Ingest Rate\"] = f\"{ingest_info.ingest_rate_mbps} MBps\"\n\t    disp[\"Ingest Time\"] = ingest_info.time\n\t    return disp\n", "def prepare_seek_stats_for_display(seek_stats):\n\t    assert isinstance(seek_stats, calc_utils.SeekStats)\n\t    disp = dict()\n\t    disp[\"Num Seeks\"] = num_for_display(seek_stats.num_seeks)\n\t    disp[\"Num Found Seeks\"] = num_for_display(seek_stats.num_found_seeks)\n\t    disp[\"Num Nexts\"] = num_for_display(seek_stats.num_nexts)\n\t    disp[\"Num Prevs\"] = num_for_display(seek_stats.num_prevs)\n\t    disp[\"Avg. Seek Range Size\"] = f\"{seek_stats.avg_seek_range_size:.1f}\"\n\t    disp[\"Avg. Seeks Rate Per Second\"] = \\\n\t        num_for_display(seek_stats.avg_seek_rate_per_second)\n", "    disp[\"Avg. Seek Latency\"] = f\"{seek_stats.avg_seek_latency_us:.1f} us\"\n\t    return disp\n\tdef prepare_cache_id_options_for_display(options):\n\t    assert isinstance(options, cache_utils.CacheOptions)\n\t    disp = dict()\n\t    disp[\"Capacity\"] = num_bytes_for_display(options.cache_capacity_bytes)\n\t    disp[\"Num Shards\"] = 2 ** options.num_shard_bits\n\t    disp[\"Shard Size\"] = num_bytes_for_display(options.shard_size_bytes)\n\t    disp[\"CF-s\"] = \\\n\t        {cf_name: asdict(cf_options) for cf_name, cf_options in\n", "         options.cfs_specific_options.items()}\n\t    return disp\n\tdef prepare_block_stats_of_cache_for_display(block_stats):\n\t    assert isinstance(block_stats, db_files.BlockLiveFileStats)\n\t    disp = dict()\n\t    disp[\"Avg. Size\"] = \\\n\t        num_bytes_for_display(int(block_stats.get_avg_block_size()))\n\t    disp[\"Max Size\"] = num_bytes_for_display(block_stats.max_size_bytes)\n\t    disp[\"Max Size At\"] = block_stats.max_size_time\n\t    return disp\n", "def prepare_block_cache_info_for_display(cache_info):\n\t    assert isinstance(cache_info, cache_utils.CacheIdInfo)\n\t    disp = dict()\n\t    disp.update(prepare_cache_id_options_for_display(cache_info.options))\n\t    blocks_stats = cache_info.files_stats.blocks_stats\n\t    disp[\"Index Block\"] = \\\n\t        prepare_block_stats_of_cache_for_display(\n\t            blocks_stats[db_files.BlockType.INDEX])\n\t    if blocks_stats[db_files.BlockType.FILTER].num_created > 0:\n\t        disp[\"Filter Block\"] = \\\n", "            prepare_block_stats_of_cache_for_display(\n\t                blocks_stats[db_files.BlockType.FILTER])\n\t    else:\n\t        disp[\"Filter Block\"] = \"No Stats (Filters not in use)\"\n\t    return disp\n\tdef prepare_block_counters_for_display(cache_counters):\n\t    assert isinstance(cache_counters, cache_utils.CacheCounters)\n\t    disp = asdict(cache_counters)\n\t    disp = {key: num_for_display(value) for key, value in disp.items()}\n\t    return disp\n", "# TODO: The detailed display should be moved to its own csv\n\t# TODO: The cache id in the detailed should not include the process id so it\n\t#  matches the non-detailed display\n\tdef prepare_detailed_block_cache_stats_for_display(detailed_block_cache_stats):\n\t    disp_stats = detailed_block_cache_stats.copy()\n\t    for cache_stats in disp_stats.values():\n\t        cache_stats['Capacity'] = \\\n\t            num_bytes_for_display(cache_stats['Capacity'])\n\t        cache_stats['Usage'] = num_bytes_for_display(cache_stats['Usage'])\n\t        for cache_stats_key, entry in cache_stats.items():\n", "            if utils.parse_time_str(cache_stats_key, expect_valid_str=False):\n\t                entry['Usage'] = num_bytes_for_display(entry['Usage'])\n\t                for entry_key, role_values in entry.items():\n\t                    if entry_key == 'CF-s':\n\t                        for cf_name in entry['CF-s']:\n\t                            for role, cf_role_value in \\\n\t                                    entry['CF-s'][cf_name].items():\n\t                                entry['CF-s'][cf_name][role] =\\\n\t                                    num_bytes_for_display(cf_role_value)\n\t                    elif entry_key != 'Usage':\n", "                        role_values['Size'] =\\\n\t                            num_bytes_for_display(role_values['Size'])\n\t    return disp_stats\n\tdef prepare_block_cache_stats_for_display(cache_stats,\n\t                                          detailed_block_cache_stats):\n\t    assert isinstance(cache_stats, cache_utils.CacheStats)\n\t    disp = dict()\n\t    if cache_stats.per_cache_id_info:\n\t        disp[\"Caches\"] = {}\n\t        for cache_id, cache_info in cache_stats.per_cache_id_info.items():\n", "            disp[\"Caches\"][cache_id] = \\\n\t                prepare_block_cache_info_for_display(cache_info)\n\t    if cache_stats.global_cache_counters:\n\t        disp[\"DB Counters\"] = \\\n\t            prepare_block_counters_for_display(\n\t                cache_stats.global_cache_counters)\n\t    else:\n\t        disp[\"DB Counters\"] = utils.NO_COUNTERS_DUMPS_TEXT\n\t    detailed_disp_block_cache_stats = None\n\t    if detailed_block_cache_stats:\n", "        detailed_disp_block_cache_stats = \\\n\t            prepare_detailed_block_cache_stats_for_display(\n\t                detailed_block_cache_stats)\n\t    if detailed_disp_block_cache_stats:\n\t        disp[\"Detailed\"] = detailed_disp_block_cache_stats\n\t    else:\n\t        disp[\"Detailed\"] = \"No Detailed Block Cache Stats Available\"\n\t    return disp\n\tdef prepare_cf_filter_stats_for_display(cf_filter_stats, format_as_dict):\n\t    if cf_filter_stats.filter_policy:\n", "        assert isinstance(cf_filter_stats, calc_utils.CfFilterFilesStats)\n\t        if cf_filter_stats.filter_policy != utils.INVALID_FILTER_POLICY:\n\t            sanitized_filter_policy = \\\n\t                SanitizedValueType.get_type_from_str(\n\t                    cf_filter_stats.filter_policy)\n\t            if sanitized_filter_policy == SanitizedValueType.NULL_PTR:\n\t                cf_disp_stats = \"No Filter\"\n\t            else:\n\t                if cf_filter_stats.avg_bpk is not None:\n\t                    bpk_str = f\"{cf_filter_stats.avg_bpk:.1f}\"\n", "                else:\n\t                    bpk_str = \"unknown bpk\"\n\t                if format_as_dict:\n\t                    cf_disp_stats = {\n\t                        \"Filter-Policy\": cf_filter_stats.filter_policy,\n\t                        \"Avg. BPK\": bpk_str\n\t                    }\n\t                else:\n\t                    cf_disp_stats = \\\n\t                        f\"{cf_filter_stats.filter_policy} ({bpk_str})\"\n", "        else:\n\t            cf_disp_stats = \"Filter Data Not Available\"\n\t    else:\n\t        cf_disp_stats = \"Filter Data Not Available\"\n\t    return cf_disp_stats\n\tdef prepare_filter_stats_for_display(filter_stats):\n\t    assert isinstance(filter_stats, calc_utils.FilterStats)\n\t    disp = {}\n\t    if filter_stats.files_filter_stats:\n\t        disp[\"CF-s\"] = {}\n", "        for cf_name, cf_stats in filter_stats.files_filter_stats.items():\n\t            assert isinstance(cf_stats, calc_utils.CfFilterFilesStats)\n\t            disp[\"CF-s\"][cf_name] = \\\n\t                prepare_cf_filter_stats_for_display(cf_stats,\n\t                                                    format_as_dict=True)\n\t    else:\n\t        disp[\"CF-s\"] = \"No Filters used In SST-s\"\n\t    if filter_stats.filter_counters and not \\\n\t            filter_stats.filter_counters.are_all_zeroes():\n\t        disp_counters = {\n", "            \"False-Positive-Rate\":\n\t                f\"1 in {filter_stats.filter_counters.one_in_n_fpr}\",\n\t            \"False-Positives\":\n\t                num_for_display(filter_stats.filter_counters.false_positives),\n\t            \"Negatives\":\n\t                num_for_display(filter_stats.filter_counters.negatives),\n\t            \"True-Positives\":\n\t                num_for_display(filter_stats.filter_counters.true_positives)\n\t        }\n\t        disp[\"Counters\"] = disp_counters\n", "    else:\n\t        disp[\"Counters\"] = \"No Filter Counters Available\"\n\t    return disp\n\tdef prepare_filter_effectiveness_stats_for_display(\n\t        cfs_names, db_opts, counters_mngr, files_monitor):\n\t    assert isinstance(db_opts, db_options.DatabaseOptions)\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\t    filter_stats = calc_utils.calc_filter_stats(\n\t        cfs_names, db_opts, files_monitor, counters_mngr)\n", "    if filter_stats:\n\t        return prepare_filter_stats_for_display(filter_stats)\n\t    else:\n\t        return \"No Filter Stats Available\"\n\tdef prepare_get_histogram_for_display(counters_mngr):\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    get_counter_name = \"rocksdb.db.get.micros\"\n\t    get_histogram = \\\n\t        counters_mngr.get_last_histogram_entry(\n\t            get_counter_name, non_zero=True)\n", "    if get_histogram:\n\t        return CountersMngr.\\\n\t            get_histogram_entry_display_values(get_histogram)\n\t    else:\n\t        logging.info(\"No Get latency histogram (maybe no stats)\")\n\t        return \"No Get Info\"\n\tdef prepare_multi_get_histogram_for_display(counters_mngr):\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    multi_get_counter_name = \"rocksdb.db.multiget.micros\"\n\t    multi_get_histogram = \\\n", "        counters_mngr.get_last_histogram_entry(\n\t            multi_get_counter_name, non_zero=True)\n\t    if multi_get_histogram:\n\t        return counters_mngr.\\\n\t            get_histogram_entry_display_values(multi_get_histogram)\n\t    else:\n\t        logging.info(\"No Multi-Get latency histogram (maybe no stats)\")\n\t        return \"No Multi-Get Info\"\n\tdef prepare_per_cf_read_latency_for_display(cf_file_histogram_stats_mngr):\n\t    per_cf_stats = \\\n", "        calc_utils.calc_read_latency_per_cf_stats(cf_file_histogram_stats_mngr)\n\t    stats = dict()\n\t    if per_cf_stats:\n\t        for cf_name, cf_stats in per_cf_stats.items():\n\t            stats[cf_name] = {\n\t                \"Num Reads\": num_for_display(cf_stats.num_reads),\n\t                \"Avg. Read Latency\": f\"{cf_stats.avg_read_latency_us:.1f} us\",\n\t                \"Max Read Latency\": f\"{cf_stats.max_read_latency_us:.1f} us\",\n\t                \"Read % of All CF-s\":\n\t                    f\"{cf_stats.read_percent_of_all_cfs:.1f}%\"\n", "            }\n\t    return stats\n\tdef prepare_applicable_read_stats(\n\t        cfs_names, db_opts, counters_mngr, stats_mngr, files_monitor):\n\t    assert isinstance(db_opts, db_options.DatabaseOptions)\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    assert isinstance(stats_mngr, StatsMngr)\n\t    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\t    stats = dict()\n\t    stats[\"Get Histogram\"] = prepare_get_histogram_for_display(counters_mngr)\n", "    stats[\"Multi-Get Histogram\"] = \\\n\t        prepare_multi_get_histogram_for_display(counters_mngr)\n\t    cf_file_histogram_stats_mngr =\\\n\t        stats_mngr.get_cf_file_histogram_stats_mngr()\n\t    stats[\"Per CF Read Latency\"] = \\\n\t        prepare_per_cf_read_latency_for_display(cf_file_histogram_stats_mngr)\n\t    stats[\"Filter Effectiveness\"] = \\\n\t        prepare_filter_effectiveness_stats_for_display(\n\t            cfs_names, db_opts, counters_mngr, files_monitor)\n\t    return stats if stats else None\n"]}
{"filename": "calc_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport copy\n\timport logging\n\tfrom bisect import bisect\n\tfrom dataclasses import dataclass, field\n\timport db_files\n\timport db_options\n\timport utils\n", "from counters import CountersMngr\n\tfrom events import EventType\n\tfrom events import FlowType, MatchingEventInfo, EventsMngr\n\tfrom log_file import ParsedLog\n\tfrom stats_mngr import CompactionStatsMngr, CfFileHistogramStatsMngr, \\\n\t    DbWideStatsMngr\n\tfrom warnings_mngr import WarningType, WarningsMngr, WarningElementInfo\n\tdef get_db_size_bytes_at_start(compaction_stats_mngr):\n\t    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n\t    first_entry_all_cfs = compaction_stats_mngr.get_first_level_entry_all_cfs()\n", "    if not first_entry_all_cfs:\n\t        return 0\n\t    size_bytes = 0\n\t    for cf_name, cf_entry in first_entry_all_cfs.items():\n\t        size_bytes += \\\n\t            int(CompactionStatsMngr.get_sum_value(\n\t                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES))\n\t@dataclass\n\tclass DbSizeBytesInfo:\n\t    size_bytes: int = None\n", "    size_time: str = None\n\tdef get_db_size_bytes_info_at_end(cfs_names, compaction_stats_mngr):\n\t    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n\t    last_entry_all_cfs = compaction_stats_mngr.get_last_level_entry_all_cfs()\n\t    if not last_entry_all_cfs:\n\t        return DbSizeBytesInfo(size_bytes=None, size_time=None)\n\t    size_bytes = 0\n\t    size_time = None\n\t    for cf_name in cfs_names:\n\t        if cf_name not in last_entry_all_cfs:\n", "            continue\n\t        cf_entry = last_entry_all_cfs[cf_name]\n\t        if size_time is None:\n\t            size_time, _ = utils.get_last_dict_entry_components(cf_entry)\n\t        size_bytes += \\\n\t            int(CompactionStatsMngr.get_sum_value(\n\t                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES))\n\t    return DbSizeBytesInfo(size_time=size_time, size_bytes=size_bytes)\n\tdef get_per_cf_per_level_size_bytes(entry_all_cfs):\n\t    if not entry_all_cfs:\n", "        return {}\n\t    growth = dict()\n\t    for cf_name, cf_entry in entry_all_cfs.items():\n\t        size_bytes_per_level = \\\n\t            CompactionStatsMngr.get_field_value_for_all_levels(\n\t                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES)\n\t        growth[cf_name] = size_bytes_per_level\n\t    return growth\n\tdef calc_cfs_size_bytes_growth(cfs_names, compaction_stats_mngr):\n\t    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n", "    growth = {cf_name: None for cf_name in cfs_names}\n\t    first_entry_all_cfs = compaction_stats_mngr.get_first_level_entry_all_cfs()\n\t    if not first_entry_all_cfs:\n\t        return {}\n\t    start_per_cf_and_level_sizes_bytes = \\\n\t        get_per_cf_per_level_size_bytes(first_entry_all_cfs)\n\t    last_entry_all_cfs = compaction_stats_mngr.get_last_level_entry_all_cfs()\n\t    assert last_entry_all_cfs\n\t    start_cf_names = list(start_per_cf_and_level_sizes_bytes.keys())\n\t    for cf_name in start_cf_names:\n", "        growth[cf_name] = {}\n\t        start_cf_level_sizes = start_per_cf_and_level_sizes_bytes[cf_name]\n\t        if start_cf_level_sizes:\n\t            for level in start_cf_level_sizes:\n\t                growth[cf_name][level] = (start_cf_level_sizes[level], None)\n\t    end_per_cf_and_level_sizes_bytes = \\\n\t        get_per_cf_per_level_size_bytes(last_entry_all_cfs)\n\t    end_cf_names = list(end_per_cf_and_level_sizes_bytes.keys())\n\t    for cf_name in end_cf_names:\n\t        if cf_name not in growth:\n", "            growth[cf_name] = {}\n\t        end_cf_level_sizes = end_per_cf_and_level_sizes_bytes[cf_name]\n\t        if end_cf_level_sizes:\n\t            for level in end_cf_level_sizes:\n\t                if level in growth[cf_name]:\n\t                    start_value = growth[cf_name][level][0]\n\t                else:\n\t                    start_value = None\n\t                growth[cf_name][level] = (start_value,\n\t                                          end_cf_level_sizes[level])\n", "    return growth\n\tdef calc_cf_table_creation_stats(cf_name, events_mngr):\n\t    assert isinstance(events_mngr, EventsMngr)\n\t    creation_events = \\\n\t        events_mngr.get_cf_events_by_type(cf_name,\n\t                                          EventType.TABLE_FILE_CREATION)\n\t    total_num_entries = 0\n\t    total_keys_sizes = 0\n\t    total_values_sizes = 0\n\t    for event in creation_events:\n", "        table_properties = event.event_details_dict[\"table_properties\"]\n\t        total_num_entries += table_properties[\"num_entries\"]\n\t        total_keys_sizes += table_properties[\"raw_key_size\"]\n\t        total_values_sizes += table_properties[\"raw_value_size\"]\n\t    num_tables_created = len(creation_events)\n\t    avg_num_table_entries = 0\n\t    avg_key_size = 0\n\t    avg_value_size = 0\n\t    if num_tables_created > 0:\n\t        avg_num_table_entries = int(total_num_entries / num_tables_created)\n", "        avg_key_size = int(total_keys_sizes / total_num_entries)\n\t        avg_value_size = int(total_values_sizes / total_num_entries)\n\t    return {\"num_tables_created\": num_tables_created,\n\t            \"total_num_entries\": total_num_entries,\n\t            \"total_keys_sizes\": total_keys_sizes,\n\t            \"total_values_sizes\": total_values_sizes,\n\t            \"avg_num_table_entries\": avg_num_table_entries,\n\t            \"avg_key_size\": avg_key_size,\n\t            \"avg_value_size\": avg_value_size}\n\t@dataclass\n", "class DeleteOpersStats:\n\t    total_num_flushed_entries: int = None\n\t    total_num_deletes: int = None\n\t    total_percent_deletes: float = None\n\t    unavailability_reason: str = None\n\tdef calc_cf_delete_opers_stats(cf_name, events_mngr):\n\t    flush_started_events = \\\n\t        events_mngr.get_cf_events_by_type(cf_name, EventType.FLUSH_STARTED)\n\t    if not flush_started_events:\n\t        return DeleteOpersStats(\n", "            unavailability_reason=utils.NO_FLUSHES_TEXT)\n\t    stats = DeleteOpersStats(total_num_flushed_entries=0, total_num_deletes=0)\n\t    for flush_event in flush_started_events:\n\t        stats.total_num_flushed_entries += flush_event.get_num_entries()\n\t        stats.total_num_deletes += flush_event.get_num_deletes()\n\t    return stats\n\tdef calc_delete_opers_stats(cfs_names, events_mngr):\n\t    assert cfs_names\n\t    assert isinstance(events_mngr, EventsMngr)\n\t    stats = DeleteOpersStats(total_num_flushed_entries=0, total_num_deletes=0)\n", "    unavailability_reason = None\n\t    has_any_data = False\n\t    for cf_name in cfs_names:\n\t        cf_stats = calc_cf_delete_opers_stats(cf_name, events_mngr)\n\t        assert isinstance(cf_stats, DeleteOpersStats)\n\t        if cf_stats.total_num_flushed_entries:\n\t            has_any_data = True\n\t            stats.total_num_flushed_entries += \\\n\t                cf_stats.total_num_flushed_entries\n\t            assert cf_stats.total_num_deletes is not None\n", "            stats.total_num_deletes += cf_stats.total_num_deletes\n\t        else:\n\t            # arbitrarily use the first reason for all\n\t            assert cf_stats.unavailability_reason is not None\n\t            unavailability_reason = cf_stats.unavailability_reason\n\t    if not has_any_data:\n\t        return DeleteOpersStats(unavailability_reason=unavailability_reason)\n\t    if stats.total_num_flushed_entries > 0:\n\t        stats.total_percent_deletes = \\\n\t            float(100 * stats.total_num_deletes /\n", "                  stats.total_num_flushed_entries)\n\t    return stats\n\t@dataclass\n\tclass UserOpersStats:\n\t    num_written: int = None\n\t    num_read: int = None\n\t    num_seek: int = None\n\t    total_num_user_opers: int = None\n\t    percent_written: float = None\n\t    percent_read: float = None\n", "    percent_seek: float = None\n\t    unavailability_reason: str = None\n\tdef get_user_operations_stats(counters_mngr):\n\t    if not counters_mngr.does_have_counters_values():\n\t        return UserOpersStats(unavailability_reason=utils.NO_STATS_TEXT)\n\t    stats = UserOpersStats()\n\t    mngr = counters_mngr\n\t    stats.num_written = \\\n\t        mngr.get_last_counter_value(\"rocksdb.number.keys.written\")\n\t    stats.num_read = mngr.get_last_counter_value(\"rocksdb.number.keys.read\")\n", "    stats.num_seek = mngr.get_last_counter_value(\"rocksdb.number.db.seek\")\n\t    stats.total_num_user_opers = \\\n\t        stats.num_written + stats.num_read + stats.num_seek\n\t    if stats.total_num_user_opers > 0:\n\t        stats.percent_written = float(100 * stats.num_written /\n\t                                      stats.total_num_user_opers)\n\t        stats.percent_read = float(100 * stats.num_read /\n\t                                   stats.total_num_user_opers)\n\t        stats.percent_seek = float(100 * stats.num_seek /\n\t                                   stats.total_num_user_opers)\n", "    return stats\n\t@dataclass\n\tclass LogFileTimeInfo:\n\t    start_time: str = None\n\t    end_time: str = None\n\t    span_seconds: float = 0.0\n\tdef get_log_file_time_info(parsed_log):\n\t    metadata = parsed_log.get_metadata()\n\t    return LogFileTimeInfo(start_time=metadata.get_start_time(),\n\t                           end_time=metadata.get_end_time(),\n", "                           span_seconds=metadata.get_log_time_span_seconds())\n\tdef get_warn_messages(raw_elements):\n\t    if not raw_elements:\n\t        return None\n\t    returned_errors = dict()\n\t    for cf_errors in raw_elements.values():\n\t        for category_errors in cf_errors.values():\n\t            for error_info in category_errors:\n\t                assert isinstance(error_info, WarningElementInfo)\n\t                returned_errors[error_info.time] = error_info.warning_msg\n", "    return returned_errors\n\tdef get_error_warnings(warnings_mngr):\n\t    assert isinstance(warnings_mngr, WarningsMngr)\n\t    return get_warn_messages(warnings_mngr.get_error_warnings())\n\tdef get_fatal_warnings(warnings_mngr):\n\t    assert isinstance(warnings_mngr, WarningsMngr)\n\t    return get_warn_messages(warnings_mngr.get_fatal_warnings())\n\tdef get_db_wide_info(parsed_log: ParsedLog):\n\t    metadata = parsed_log.get_metadata()\n\t    warns_mngr = parsed_log.get_warnings_mngr()\n", "    stats_mngr = parsed_log.get_stats_mngr()\n\t    db_wide_stats_mngr = stats_mngr.get_db_wide_stats_mngr()\n\t    counters_mngr = parsed_log.get_counters_mngr()\n\t    user_opers_stats = get_user_operations_stats(counters_mngr)\n\t    assert isinstance(user_opers_stats, UserOpersStats)\n\t    cumulative_writes_stats_dict = \\\n\t        stats_mngr.get_db_wide_stats_mngr(). \\\n\t        get_last_cumulative_writes_entry()\n\t    num_keys_written = None\n\t    if cumulative_writes_stats_dict:\n", "        _, cumulative_writes_stats = \\\n\t            utils.get_first_dict_entry_components(cumulative_writes_stats_dict)\n\t        if user_opers_stats.num_written:\n\t            num_keys_written = max(user_opers_stats.num_written,\n\t                                   cumulative_writes_stats.num_keys)\n\t        else:\n\t            num_keys_written = cumulative_writes_stats.num_keys\n\t    total_num_table_created_entries = 0\n\t    total_keys_sizes = 0\n\t    total_values_size = 0\n", "    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    events_mngr = parsed_log.get_events_mngr()\n\t    delete_opers_stats = calc_delete_opers_stats(cfs_names, events_mngr)\n\t    assert isinstance(delete_opers_stats, DeleteOpersStats)\n\t    for cf_name in cfs_names:\n\t        table_creation_stats = calc_cf_table_creation_stats(cf_name,\n\t                                                            events_mngr)\n\t        total_num_table_created_entries += \\\n\t            table_creation_stats[\"total_num_entries\"]\n\t        total_keys_sizes += table_creation_stats[\"total_keys_sizes\"]\n", "        total_values_size += table_creation_stats[\"total_values_sizes\"]\n\t    # TODO - Add unit test when total_num_table_created_entries == 0\n\t    # TODO - Consider whether this means data is not available\n\t    avg_key_size_bytes = None\n\t    avg_value_size_bytes = None\n\t    if total_num_table_created_entries > 0:\n\t        avg_key_size_bytes = \\\n\t            int(total_keys_sizes / total_num_table_created_entries)\n\t        avg_value_size_bytes = \\\n\t            int(total_values_size / total_num_table_created_entries)\n", "    compactions_stats_mngr = \\\n\t        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\t    db_size_bytes_info = get_db_size_bytes_info_at_end(cfs_names,\n\t                                                       compactions_stats_mngr)\n\t    assert isinstance(db_size_bytes_info, DbSizeBytesInfo)\n\t    ingest_info = get_db_ingest_info(db_wide_stats_mngr)\n\t    info = {\n\t        \"creator\": metadata.get_product_name(),\n\t        \"version\": metadata.get_version(),\n\t        \"git_hash\": metadata.get_git_hash(),\n", "        \"db_size_bytes\": db_size_bytes_info.size_bytes,\n\t        \"db_size_bytes_time\": db_size_bytes_info.size_time,\n\t        \"num_cfs\": parsed_log.get_num_cfs_when_certain(),\n\t        \"avg_key_size_bytes\": avg_key_size_bytes,\n\t        \"avg_value_size_bytes\": avg_value_size_bytes,\n\t        \"num_warnings\": warns_mngr.get_total_num_warns(),\n\t        \"errors\": get_error_warnings(warns_mngr),\n\t        \"fatals\": get_fatal_warnings(warns_mngr),\n\t        \"total_num_table_created_entries\": total_num_table_created_entries,\n\t        \"num_keys_written\": num_keys_written,\n", "        \"user_opers_stats\": user_opers_stats,\n\t        \"delete_opers_stats\": delete_opers_stats,\n\t        \"ingest_info\": ingest_info\n\t    }\n\t    return info\n\t@dataclass\n\tclass DbIngestInfo:\n\t    time: str\n\t    ingest: int = 0\n\t    ingest_rate_mbps: float = 0.0\n", "def get_db_ingest_info(db_wide_stats_mngr):\n\t    assert isinstance(db_wide_stats_mngr, DbWideStatsMngr)\n\t    cumulative_writes_stats_dict = \\\n\t        db_wide_stats_mngr.get_last_cumulative_writes_entry()\n\t    if not cumulative_writes_stats_dict:\n\t        return None\n\t    time, cumulative_writes_stats = \\\n\t        utils.get_first_dict_entry_components(cumulative_writes_stats_dict)\n\t    return DbIngestInfo(\n\t        time=time,\n", "        ingest=cumulative_writes_stats.ingest,\n\t        ingest_rate_mbps=cumulative_writes_stats.ingest_rate_mbps)\n\tdef calc_event_histogram(cf_name, events_mngr, event_type, group_by_field):\n\t    events = events_mngr.get_cf_events_by_type(cf_name, event_type)\n\t    histogram = dict()\n\t    for event in events:\n\t        event_grouping = event.event_details_dict[group_by_field]\n\t        if event_grouping not in histogram:\n\t            histogram[event_grouping] = 0\n\t        histogram[event_grouping] += 1\n", "    return histogram\n\t# A list of flushed data sizes (in MB) to use for the generation of the\n\t# flush sizes histogram\n\t# The sizes are integers and must be ordered and always increasing\n\tFLUSHED_SIZES_HISTOGRAM_BUCKETS_MB = [2, 10, 32, 64]\n\t@dataclass\n\tclass PerFlushReasonStats:\n\t    num_flushes: int = 0\n\t    # Prepare a count per bucket (+1 for sizes > last)\n\t    sizes_histogram: list = \\\n", "        field(default_factory=lambda: ([0] * (len(\n\t            FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB)+1)))\n\t    min_duration_ms: int = None\n\t    max_duration_ms: int = None\n\t    min_num_memtables: int = None\n\t    max_num_memtables: int = None\n\t    min_total_data_size_bytes: int = None\n\t    max_total_data_size_bytes: int = None\n\tdef calc_cf_flushes_stats(cf_name, events_mngr):\n\t    cf_flush_events = events_mngr.get_cf_flow_events(FlowType.FLUSH,\n", "                                                     cf_name)\n\t    if not cf_flush_events:\n\t        return {}\n\t    def get_min(curr, new):\n\t        return min(curr, new) if curr is not None else new\n\t    def get_max(curr, new):\n\t        return max(curr, new) if curr is not None else new\n\t    stats = {}\n\t    for events_pair in cf_flush_events:\n\t        start_flush_event = events_pair[0]\n", "        flush_reason = start_flush_event.get_flush_reason()\n\t        num_memtables = start_flush_event.get_num_memtables()\n\t        total_data_size_bytes = start_flush_event.get_total_data_size_bytes()\n\t        flush_duration_ms = 0\n\t        # It's possible that there is no matching end event\n\t        end_event = events_pair[1]\n\t        if end_event:\n\t            start_event_info = MatchingEventInfo(start_flush_event,\n\t                                                 FlowType.FLUSH, True)\n\t            end_event_info = MatchingEventInfo(end_event,\n", "                                               FlowType.FLUSH, False)\n\t            flush_duration_ms = \\\n\t                start_event_info.get_duration_ms(end_event_info)\n\t        # Find the bucket for the flushed size\n\t        bucket_idx = bisect(FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB,\n\t                            total_data_size_bytes / (2**20))\n\t        if flush_reason not in stats:\n\t            stats[flush_reason] = PerFlushReasonStats()\n\t        reason_stats = stats[flush_reason]\n\t        reason_stats.num_flushes += 1\n", "        reason_stats.sizes_histogram[bucket_idx] += 1\n\t        reason_stats.min_duration_ms = \\\n\t            get_min(reason_stats.min_duration_ms, flush_duration_ms)\n\t        reason_stats.max_duration_ms = \\\n\t            get_max(reason_stats.max_duration_ms, flush_duration_ms)\n\t        reason_stats.min_num_memtables = \\\n\t            get_min(reason_stats.min_num_memtables, num_memtables)\n\t        reason_stats.max_num_memtables = \\\n\t            get_max(reason_stats.max_num_memtables, num_memtables)\n\t        reason_stats.min_total_data_size_bytes = \\\n", "            get_min(reason_stats.min_total_data_size_bytes,\n\t                    total_data_size_bytes)\n\t        reason_stats.max_total_data_size_bytes = \\\n\t            get_max(reason_stats.max_total_data_size_bytes,\n\t                    total_data_size_bytes)\n\t    if stats:\n\t        for reason in stats:\n\t            stats[reason] = stats[reason]\n\t    return stats\n\tdef get_largest_compaction_size_bytes(compactions_monitor):\n", "    jobs = compactions_monitor.get_finished_jobs()\n\t    largest_size_bytes = 0\n\t    for job in jobs.values():\n\t        largest_size_bytes = max(largest_size_bytes,\n\t                                 job.start_event.get_input_data_size_bytes())\n\t    return largest_size_bytes\n\tdef get_cf_per_level_write_amp(compactions_stats_mngr, cf_name):\n\t    cf_compaction_stats = \\\n\t        compactions_stats_mngr.get_cf_level_entries(cf_name)\n\t    if not cf_compaction_stats:\n", "        return None\n\t@dataclass\n\tclass CfCompactionStats:\n\t    num_compactions: int = 0\n\t    min_compaction_bw_mbps: float = None\n\t    max_compaction_bw_mbps: float = None\n\t    per_level_write_amp: dict = None\n\t    comp_sec: float = None\n\t    comp_merge_cpu_sec: float = None\n\tdef calc_cf_compactions_stats(cf_name, log_start_time, compactions_monitor,\n", "                              compactions_stats_mngr):\n\t    cf_jobs = compactions_monitor.get_cf_finished_jobs(cf_name)\n\t    if not cf_jobs:\n\t        return None\n\t    stats = CfCompactionStats()\n\t    for job in cf_jobs.values():\n\t        stats.num_compactions += 1\n\t        if job.pre_finish_info:\n\t            if stats.min_compaction_bw_mbps is None:\n\t                stats.min_compaction_bw_mbps = \\\n", "                    job.pre_finish_info.write_rate_mbps\n\t            else:\n\t                stats.min_compaction_bw_mbps = \\\n\t                    min(stats.min_compaction_bw_mbps,\n\t                        job.pre_finish_info.write_rate_mbps)\n\t            if stats.max_compaction_bw_mbps is None:\n\t                stats.max_compaction_bw_mbps = \\\n\t                    job.pre_finish_info.write_rate_mbps\n\t            else:\n\t                stats.max_compaction_bw_mbps = \\\n", "                    max(stats.max_compaction_bw_mbps,\n\t                        job.pre_finish_info.write_rate_mbps)\n\t    last_entry = compactions_stats_mngr.get_last_cf_level_entry(cf_name)\n\t    if last_entry:\n\t        per_level_write_amp = \\\n\t            CompactionStatsMngr.get_field_value_for_all_levels(\n\t                last_entry, CompactionStatsMngr.LevelFields.WRITE_AMP)\n\t        if per_level_write_amp:\n\t            sum_write_amp = \\\n\t                CompactionStatsMngr.get_sum_value(\n", "                    last_entry, CompactionStatsMngr.LevelFields.WRITE_AMP)\n\t            per_level_write_amp[\"SUM\"] = sum_write_amp\n\t            stats.per_level_write_amp = per_level_write_amp\n\t        uptime = \\\n\t            CompactionStatsMngr.get_level_entry_uptime_seconds(last_entry,\n\t                                                               log_start_time)\n\t        if uptime > 0.0:\n\t            stats.comp_sec = \\\n\t                float(CompactionStatsMngr.get_sum_value(\n\t                    last_entry, CompactionStatsMngr.LevelFields.COMP_SEC))\n", "            stats.comp_merge_cpu_sec =\\\n\t                float(CompactionStatsMngr.get_sum_value(\n\t                    last_entry,\n\t                    CompactionStatsMngr.LevelFields.COMP_MERGE_CPU))\n\t    return stats\n\tdef calc_all_events_histogram(cf_names, events_mngr):\n\t    # Returns a dictionary of:\n\t    # {<cf_name>: {<event_type>: [events]}}   # noqa\n\t    histogram = {}\n\t    for cf_name in cf_names:\n", "        for event_type in EventType:\n\t            cf_events_of_type = events_mngr.get_cf_events_by_type(cf_name,\n\t                                                                  event_type)\n\t            if cf_name not in histogram:\n\t                histogram[cf_name] = {}\n\t            if cf_events_of_type:\n\t                histogram[cf_name][event_type] = len(cf_events_of_type)\n\t    return histogram\n\tdef is_cf_compression_by_level(parsed_log, cf_name):\n\t    db_opts = parsed_log.get_database_options()\n", "    return db_opts.get_cf_option(cf_name, \"compression[0]\") is not None\n\tdef get_applicable_cf_options(db_opts):\n\t    assert isinstance(db_opts, db_options.DatabaseOptions)\n\t    cf_names = db_opts.get_cfs_names()\n\t    cfs_options = {\"compaction_style\": {},\n\t                   \"compression\": {},\n\t                   \"filter_policy\": {}}\n\t    for cf_name in cf_names:\n\t        cfs_options[\"compaction_style\"][cf_name] = \\\n\t            db_opts.get_cf_option(cf_name, \"compaction_style\")\n", "        cfs_options[\"compression\"][cf_name] = \\\n\t            db_opts.get_cf_option(cf_name, \"compression\")\n\t        cfs_options[\"filter_policy\"][cf_name] = \\\n\t            db_opts.get_cf_table_option(cf_name, \"filter_policy\")\n\t    compaction_styles = list(set(cfs_options[\"compaction_style\"].values()))\n\t    if len(compaction_styles) == 1 and compaction_styles[0] is not None:\n\t        common_compaction_style = compaction_styles[0]\n\t    else:\n\t        common_compaction_style = \"Per Column Family\"\n\t    cfs_options[\"compaction_style\"][\"common\"] = common_compaction_style\n", "    compressions = list(set(cfs_options[\"compression\"].values()))\n\t    if len(compressions) == 1 and compressions[0] is not None:\n\t        common_compression = compressions[0]\n\t    else:\n\t        common_compression = \"Per Column Family\"\n\t    cfs_options[\"compression\"][\"common\"] = common_compression\n\t    filter_policies = list(set(cfs_options[\"filter_policy\"].values()))\n\t    if len(filter_policies) == 1 and filter_policies[0] is not None:\n\t        common_filter_policy = filter_policies[0]\n\t    else:\n", "        common_filter_policy = \"Per Column Family\"\n\t    cfs_options[\"filter_policy\"][\"common\"] = common_filter_policy\n\t    return cfs_options\n\t@dataclass\n\tclass CfReadLatencyStats:\n\t    num_reads: int = 0\n\t    avg_read_latency_us: float = 0.0\n\t    max_read_latency_us: float = 0.0\n\t    read_percent_of_all_cfs: float = 0.0\n\tdef calc_read_latency_per_cf_stats(cf_file_histogram_stats_mngr):\n", "    stats = {}\n\t    all_entries = cf_file_histogram_stats_mngr.get_all_entries()\n\t    if not all_entries:\n\t        return {}\n\t    total_num_reads = 0\n\t    for cf_name in all_entries:\n\t        last_cf_entry = cf_file_histogram_stats_mngr.get_last_cf_entry(cf_name)\n\t        if not last_cf_entry:\n\t            continue\n\t        total_cf_num_reads = 0\n", "        total_cf_read_latency_us = 0.0\n\t        max_cf_read_latency_us = 0.0\n\t        levels_stats = CfFileHistogramStatsMngr.CfEntry(last_cf_entry)\n\t        for level_stats in levels_stats.get_all_levels_stats().values():\n\t            total_cf_num_reads += level_stats.count\n\t            total_cf_read_latency_us += \\\n\t                level_stats.count * level_stats.average\n\t            max_cf_read_latency_us = \\\n\t                max(max_cf_read_latency_us, level_stats.max)\n\t        avg_cf_read_latency_us = total_cf_read_latency_us / total_cf_num_reads\n", "        stats[cf_name] = \\\n\t            CfReadLatencyStats(\n\t                num_reads=total_cf_num_reads,\n\t                avg_read_latency_us=avg_cf_read_latency_us,\n\t                max_read_latency_us=max_cf_read_latency_us)\n\t        total_num_reads += total_cf_num_reads\n\t    for cf_stats in stats.values():\n\t        cf_stats.read_percent_of_all_cfs = \\\n\t            (cf_stats.num_reads/total_num_reads) * 100\n\t    return stats if stats else None\n", "def calc_cf_read_density(compactions_stats_mngr, cf_file_histogram_stats_mngr,\n\t                         cf_read_latecny_stats, cf_name):\n\t    assert isinstance(cf_read_latecny_stats, CfReadLatencyStats)\n\t    last_cf_compaction_stats_entry = \\\n\t        compactions_stats_mngr.get_last_cf_level_entry(cf_name)\n\t    if not last_cf_compaction_stats_entry:\n\t        return None\n\t    last_cf_read_latency_raw_entry = \\\n\t        cf_file_histogram_stats_mngr.get_last_cf_entry(cf_name)\n\t    last_cf_read_latency_entry = \\\n", "        CfFileHistogramStatsMngr.CfEntry(last_cf_read_latency_raw_entry)\n\t    total_num_cf_reads = cf_read_latecny_stats.num_reads\n\t    if total_num_cf_reads == 0:\n\t        logging.info(f\"No read density as no reads for cf ({cf_name}). \"\n\t                     f\"time:{last_cf_read_latency_entry.get_time()} \")\n\t        return {}\n\t    # Calculate per level READ-NORM\n\t    per_level_read_norm = dict()\n\t    for level, level_stats in \\\n\t            last_cf_read_latency_entry.get_all_levels_stats().items():\n", "        per_level_read_norm[level] = level_stats.count / total_num_cf_reads\n\t    compaction_entry = CompactionStatsMngr.CfLevelEntry(\n\t        last_cf_compaction_stats_entry)\n\t    total_cf_size_bytes = \\\n\t        compactions_stats_mngr.get_cf_size_bytes_at_end(cf_name)\n\t    if total_cf_size_bytes == 0:\n\t        logging.info(f\"No read density as cf ({cf_name}) size if 0 \"\n\t                     f\"time:{compaction_entry.get_time()} \")\n\t        return None\n\t    # Calculate per level SIZE-NORM\n", "    per_level_size_norm = dict()\n\t    compaction_levels = compaction_entry.get_levels()\n\t    for level in compaction_levels:\n\t        # TODO - If this ever gets used again - level_size_bytes could be None\n\t        level_size_bytes = \\\n\t            compactions_stats_mngr.get_cf_level_size_bytes(cf_name, level)\n\t        per_level_size_norm[level] = level_size_bytes / total_cf_size_bytes\n\t    per_level_read_density = dict()\n\t    for level, level_size_bytes in per_level_size_norm.items():\n\t        if level_size_bytes == 0:\n", "            logging.info(f\"0 Size level {level} skipped.\"\n\t                         f\"cf:{cf_name}. time:{compaction_entry.get_time()}\")\n\t            continue\n\t        if level not in per_level_read_norm:\n\t            logging.info(f\"Level {level} skipped since it's missing in \"\n\t                         f\"compaction levels dump. cf:{cf_name}. time:\"\n\t                         f\"{compaction_entry.get_time()}\")\n\t            continue\n\t        per_level_read_density[level] =\\\n\t            per_level_read_norm[level] / per_level_size_norm[level]\n", "    # Calculate the weighted-average of the norms\n\t    sum_densities = sum(per_level_read_density.values())\n\t    per_level_weighted_avg_density = \\\n\t        {level: density / sum_densities for level, density in\n\t         per_level_read_density.items()}\n\t    return per_level_weighted_avg_density\n\t@dataclass\n\tclass SeekStats:\n\t    num_seeks: int = 0\n\t    num_found_seeks: int = 0\n", "    num_nexts: int = 0\n\t    num_prevs: int = 0\n\t    avg_seek_range_size: float = 0.0\n\t    avg_seek_rate_per_second: float = 0.0\n\t    avg_seek_latency_us: float = 0.0\n\tdef get_applicable_seek_stats(counters_mngr):\n\t    # The names of the counters in the stats dump in the log\n\t    prefix = 'rocksdb.number.db'\n\t    seek_name = f\"{prefix}.seek\"\n\t    seek_found_name = f\"{prefix}.seek.found\"\n", "    seek_next_name = f\"{prefix}.next\"\n\t    seek_prev_name = f\"{prefix}.prev\"\n\t    seek_latency_hist_us = \"rocksdb.db.seek.micros\"\n\t    mngr = counters_mngr\n\t    # First see if there are any seeks recorded\n\t    last_seek_entry = mngr.get_last_counter_entry(seek_name)\n\t    if not last_seek_entry:\n\t        logging.info(\"No seeks (maybe no stats) or no actual seeks in log.\")\n\t        return None\n\t    last_seek_time = last_seek_entry[\"time\"]\n", "    last_seek_count = last_seek_entry[\"value\"]\n\t    first_seek_entry = mngr.get_first_counter_entry(seek_name)\n\t    first_seek_time = first_seek_entry[\"time\"]\n\t    first_seek_count = first_seek_entry[\"value\"]\n\t    num_seeks = last_seek_count - first_seek_count\n\t    if num_seeks == 0:\n\t        logging.info(\"No seeks log (seek counter is 0).\")\n\t        return None\n\t    seek_time_span_seconds = \\\n\t        utils.get_times_strs_diff_seconds(\n", "            first_seek_time, last_seek_time)\n\t    stats = SeekStats()\n\t    last_seek_found_count = mngr.get_last_counter_value(seek_found_name)\n\t    last_num_nexts = mngr.get_last_counter_value(seek_next_name)\n\t    last_num_prevs = mngr.get_last_counter_value(seek_prev_name)\n\t    first_seek_found_count = mngr.get_first_counter_value(seek_found_name)\n\t    first_num_nexts = mngr.get_first_counter_value(seek_next_name)\n\t    first_num_prevs = mngr.get_first_counter_value(seek_prev_name)\n\t    stats.num_seeks = num_seeks\n\t    stats.num_found_seeks = last_seek_found_count - first_seek_found_count\n", "    stats.num_nexts = last_num_nexts - first_num_nexts\n\t    stats.num_prevs = last_num_prevs - first_num_prevs\n\t    if stats.num_seeks > 0:\n\t        stats.avg_seek_range_size = \\\n\t            (stats.num_prevs + stats.num_nexts) / stats.num_seeks\n\t    avg_seek_latency_us = \\\n\t        mngr.get_last_histogram_entry(seek_latency_hist_us, non_zero=True)\n\t    if avg_seek_latency_us:\n\t        if seek_time_span_seconds > 0.0:\n\t            stats.avg_seek_rate_per_second = \\\n", "                num_seeks / seek_time_span_seconds\n\t        seek_latency_hist = avg_seek_latency_us[\"values\"]\n\t        stats.avg_seek_latency_us = seek_latency_hist[\"Average\"]\n\t    return stats\n\tdef get_warn_warnings_info(cfs_names, warnings_mngr):\n\t    assert isinstance(warnings_mngr, WarningsMngr)\n\t    all_warn_warnings = warnings_mngr.get_warnings_of_type(WarningType.WARN)\n\t    if not all_warn_warnings:\n\t        return None\n\t    returned_warn_warnings = {}\n", "    cfs_names_including_db = [utils.NO_CF] + cfs_names\n\t    for cf_name in cfs_names_including_db:\n\t        if cf_name in all_warn_warnings:\n\t            cf_info = copy.deepcopy(all_warn_warnings[cf_name])\n\t            for category in list(cf_info.keys()):\n\t                cf_info[category] = len(cf_info[category])\n\t            returned_warn_warnings[cf_name] = cf_info\n\t        else:\n\t            returned_warn_warnings[cf_name] = {}\n\t    return returned_warn_warnings\n", "@dataclass\n\tclass CfFilterFilesStats:\n\t    filter_policy: str = None\n\t    avg_bpk: float = None\n\tdef calc_files_filter_stats(cfs_names, db_opts, files_monitor):\n\t    assert isinstance(db_opts, db_options.DatabaseOptions)\n\t    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\t    stats = dict()\n\t    cfs_options = get_applicable_cf_options(db_opts)\n\t    cfs_filters_from_options = dict()\n", "    for cf_name in cfs_names:\n\t        if cf_name in cfs_options['filter_policy']:\n\t            cfs_filters_from_options[cf_name] = \\\n\t                cfs_options['filter_policy'][cf_name]\n\t    for cf_name in cfs_names:\n\t        cf_filter_files_stats = \\\n\t            db_files.calc_cf_files_stats([cf_name], files_monitor)\n\t        if cf_filter_files_stats:\n\t            assert isinstance(cf_filter_files_stats, db_files.CfsFilesStats)\n\t            filter_policy = \\\n", "                cf_filter_files_stats.cfs_filter_specific[\n\t                    cf_name].filter_policy\n\t            avg_bpk = \\\n\t                cf_filter_files_stats.cfs_filter_specific[cf_name].avg_bpk\n\t            stats[cf_name] = CfFilterFilesStats(filter_policy=filter_policy,\n\t                                                avg_bpk=avg_bpk)\n\t        elif cf_name in cfs_filters_from_options:\n\t            filter_policy = cfs_filters_from_options[cf_name]\n\t            stats[cf_name] = CfFilterFilesStats(filter_policy=filter_policy,\n\t                                                avg_bpk=None)\n", "        else:\n\t            # INVALID_FILTER_POLICY to indicate this cf's filter policy\n\t            # can't be deduced (instead of None which means - we know it has\n\t            # no filter policy)\n\t            stats[cf_name] = CfFilterFilesStats(\n\t                filter_policy=utils.INVALID_FILTER_POLICY, avg_bpk=None)\n\t    return stats\n\t@dataclass\n\tclass FilterCounters:\n\t    negatives: int = 0\n", "    positives: int = 0\n\t    true_positives: int = 0\n\t    false_positives: int = 0\n\t    one_in_n_fpr: int = 0\n\t    def are_all_zeroes(self):\n\t        return self.negatives + self.positives + self.true_positives + \\\n\t               self.false_positives + self.one_in_n_fpr == 0\n\tdef collect_filter_counters(counters_mngr):\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    if not counters_mngr.does_have_counters_values():\n", "        logging.info(\"Can't collect Filter counters. No counters available\")\n\t        return None\n\t    filter_counters_names = {\n\t        \"negatives\": \"rocksdb.bloom.filter.useful\",\n\t        \"positives\":  \"rocksdb.bloom.filter.full.positive\",\n\t        \"true_positives\": \"rocksdb.bloom.filter.full.true.positive\"}\n\t    counters = FilterCounters()\n\t    for field_name, counter_name in filter_counters_names.items():\n\t        counter_value = counters_mngr.get_last_counter_value(counter_name)\n\t        setattr(counters, field_name, counter_value)\n", "    assert counters.positives >= counters.true_positives\n\t    counters.false_positives = counters.positives - counters.true_positives\n\t    return counters\n\tdef calc_bloom_1_in_fpr(counters):\n\t    assert isinstance(counters, FilterCounters)\n\t    total = counters.negatives + counters.positives\n\t    false_positives = counters.false_positives\n\t    if false_positives == 0:\n\t        return 0\n\t    return int(total / false_positives)\n", "@dataclass\n\tclass FilterStats:\n\t    files_filter_stats: dict = None\n\t    filter_counters: FilterCounters = None\n\tdef calc_filter_stats(cfs_names, db_opts, files_monitor, counters_mngr):\n\t    assert isinstance(db_opts, db_options.DatabaseOptions)\n\t    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    stats = FilterStats()\n\t    files_filter_stats = calc_files_filter_stats(cfs_names,\n", "                                                 db_opts,\n\t                                                 files_monitor)\n\t    if files_filter_stats:\n\t        stats.files_filter_stats = files_filter_stats\n\t    filter_counters = collect_filter_counters(counters_mngr)\n\t    if filter_counters:\n\t        assert isinstance(filter_counters, FilterCounters)\n\t        one_in_n_fpr = calc_bloom_1_in_fpr(filter_counters)\n\t        filter_counters.one_in_n_fpr = one_in_n_fpr\n\t        stats.filter_counters = filter_counters\n", "    return stats\n\tdef get_cfs_common_and_specific_options(db_opts):\n\t    assert isinstance(db_opts, db_options.DatabaseOptions)\n\t    cfs_names = db_opts.get_cfs_names()\n\t    cfs_options = {cf_name: db_opts.get_cf_options(cf_name)\n\t                   for cf_name in cfs_names}\n\t    cfs_common_options, cfs_specific_options = \\\n\t        db_opts.get_unified_cfs_options(cfs_options)\n\t    return cfs_common_options, cfs_specific_options\n\tdef get_cfs_common_and_specific_diff_dicts(\n", "        baseline_options, log_database_options):\n\t    assert isinstance(baseline_options, db_options.DatabaseOptions)\n\t    assert isinstance(log_database_options, db_options.DatabaseOptions)\n\t    baseline_opts = baseline_options.get_all_options()\n\t    cfs_common_options, cfs_specific_options =  \\\n\t        get_cfs_common_and_specific_options(log_database_options)\n\t    common_dummy_cf_name = \"COMMON-DUMMY-CF-NAME\"\n\t    common_log_file_full_name_options = db_options.FullNamesOptionsDict()\n\t    common_log_file_full_name_options.\\\n\t        init_from_full_names_options_no_cf_dict(common_dummy_cf_name,\n", "                                                cfs_common_options)\n\t    # We need to compare the common to the baseline,\n\t    # but a baseline that only contains the options common\n\t    # to all the cf-s\n\t    baseline_opts_for_diff_dict = dict()\n\t    baseline_opts_dict = baseline_opts.get_options_dict()\n\t    for full_common_option_name in cfs_common_options.keys():\n\t        if full_common_option_name in baseline_opts_dict:\n\t            baseline_opts_for_diff_dict[full_common_option_name] = \\\n\t                baseline_opts_dict[full_common_option_name]\n", "    baseline_opts_for_diff =\\\n\t        db_options.FullNamesOptionsDict(baseline_opts_for_diff_dict)\n\t    common_diff = db_options.DatabaseOptions.get_cfs_options_diff(\n\t        baseline_opts_for_diff,\n\t        utils.DEFAULT_CF_NAME,\n\t        common_log_file_full_name_options,\n\t        common_dummy_cf_name)\n\t    if common_diff is None or common_diff.is_empty_diff():\n\t        common_diff = {}\n\t    # We need to compare every cf to the baseline, but a baseline that doesn't\n", "    # have the options common to all the cf-s (they are missing in the\n\t    # cfs_specific_options)\n\t    baseline_opts_for_diff_dict = \\\n\t        copy.deepcopy(baseline_opts.get_options_dict())\n\t    utils.delete_dict_keys(baseline_opts_for_diff_dict,\n\t                           cfs_common_options.keys())\n\t    baseline_opts_for_diff =\\\n\t        db_options.FullNamesOptionsDict(baseline_opts_for_diff_dict)\n\t    cfs_specific_diffs = dict()\n\t    for cf_name, cf_options in cfs_specific_options.items():\n", "        cf_full_name_options = db_options.FullNamesOptionsDict()\n\t        cf_full_name_options. \\\n\t            init_from_full_names_options_no_cf_dict(cf_name, cf_options)\n\t        cf_diff = db_options.DatabaseOptions.get_cfs_options_diff(\n\t            baseline_opts_for_diff,\n\t            utils.DEFAULT_CF_NAME,\n\t            cf_full_name_options,\n\t            cf_name)\n\t        cfs_specific_diffs[cf_name] = cf_diff\n\t    return common_diff, cfs_specific_diffs\n"]}
{"filename": "events.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport json\n\timport logging\n\timport re\n\timport typing\n\tfrom dataclasses import dataclass\n\tfrom enum import Enum\n\timport regexes\n", "import utils\n\tfrom log_entry import LogEntry\n\tclass EventType(str, Enum):\n\t    FLUSH_STARTED = \"flush_started\"\n\t    FLUSH_FINISHED = \"flush_finished\"\n\t    COMPACTION_STARTED = \"compaction_started\"\n\t    COMPACTION_FINISHED = \"compaction_finished\"\n\t    TABLE_FILE_CREATION = 'table_file_creation'\n\t    TABLE_FILE_DELETION = \"table_file_deletion\"\n\t    TRIVIAL_MOVE = \"trivial_move\"\n", "    RECOVERY_STARTED = \"recovery_started\"\n\t    RECOVERY_FINISHED = \"recovery_finished\"\n\t    INGEST_FINISHED = \"ingest_finished\"\n\t    BLOB_FILE_CREATION = \"blob_file_creation\"\n\t    BLOB_FILE_DELETION = \"blob_file_deletion\"\n\t    UNKNOWN = \"UNKNOWN\"\n\t    def __str__(self):\n\t        return str(self.value)\n\t    @staticmethod\n\t    def type_from_str(event_type_str):\n", "        try:\n\t            return EventType(event_type_str)\n\t        except ValueError:\n\t            return EventType.UNKNOWN\n\tclass EventField(str, Enum):\n\t    EVENT_TYPE = \"event\"\n\t    JOB_ID = \"job\"\n\t    TIME_MICROS = \"time_micros\"\n\t    CF_NAME = \"cf_name\"\n\t    FLUSH_REASON = \"flush_reason\"\n", "    COMPACTION_REASON = \"compaction_reason\"\n\t    FILE_NUMBER = \"file_number\"\n\t    FILE_SIZE = \"file_size\"\n\t    TABLE_PROPERTIES = \"table_properties\"\n\t    WAL_ID = \"wal_id\",\n\t    NUM_ENTRIES = \"num_entries\"\n\t    NUM_DELETES = \"num_deletes\"\n\t    NUM_MEMTABLES = \"num_memtables\"\n\t    TOTAL_DATA_SIZE = \"total_data_size\",\n\t    INPUT_DATA_SIZE = \"input_data_size\"\n", "    COMPACTION_TIME_MICROS = \"compaction_time_micros\"\n\t    TOTAL_OUTPUT_SIZE = \"total_output_size\"\n\t    # Compaction Finished\n\t    OUTPUT_LEVEL = \"output_level\"\n\t    NUM_OUTPUT_FILES = \"num_output_files\"\n\t    NUM_INPUT_RECORDS = \"num_input_records\"\n\t    RECORDS_IN = \"records_in\"\n\t    RECORDS_DROPPED = \"records_dropped\"\n\tclass TablePropertiesField(str, Enum):\n\t    CF_ID = \"column_family_id\"\n", "    DATA_SIZE = \"data_size\"\n\t    INDEX_SIZE = \"index_size\"\n\t    FILTER_SIZE = \"filter_size\"\n\t    FILTER_POLICY = \"filter_policy\"\n\t    NUM_FILTER_ENTRIES = \"num_filter_entries\"\n\t    NUM_DATA_BLOCKS = \"num_data_blocks\"\n\t    TOTAL_KEY_SIZE = \"raw_key_size\"\n\t    TOTAL_VALUE_SIZE = \"raw_value_size\"\n\t    COMPRESSION_TYPE = \"compression\"\n\tclass FlowType(str, Enum):\n", "    FLUSH = \"Flush\"\n\t    COMPACTION = \"Compaction\"\n\t    RECOVERY = \"Recovery\"\n\t    def __str__(self):\n\t        return str(self.value)\n\t# Events that have a start and finish events that should be matched\n\t# based on their job-id\n\t# Every tuple consists of:\n\t# - The event in the log marking the start\n\t# - The event in the log marking the end\n", "# - The type of the associated operation / flow (e.g., Flush)\n\t# The duration of the associated operation is the time difference\n\t# between the 2\n\tMATCHING_EVENTS = [\n\t    (EventType.FLUSH_STARTED, EventType.FLUSH_FINISHED, FlowType.FLUSH),\n\t    (EventType.COMPACTION_STARTED, EventType.COMPACTION_FINISHED,\n\t     FlowType.COMPACTION),\n\t    (EventType.RECOVERY_STARTED, EventType.RECOVERY_FINISHED,\n\t     FlowType.RECOVERY)\n\t]\n", "Event = typing.NewType(\"Event\", None)\n\t@dataclass\n\tclass MatchingEventTypeInfo:\n\t    event_type: EventType\n\t    associated_flow_type: FlowType\n\t    is_start: bool\n\t@dataclass\n\tclass MatchingEventInfo:\n\t    event: Event\n\t    associated_flow_type: FlowType\n", "    is_start: bool\n\t    def get_duration_ms(self, matching_event_info):\n\t        if self.is_start:\n\t            start_time_epoch_micro = \\\n\t                self.event.get_time_since_epoch_microseconds()\n\t            end_time_epoch_micro = \\\n\t                matching_event_info.event.get_time_since_epoch_microseconds()\n\t        else:\n\t            start_time_epoch_micro = \\\n\t                matching_event_info.event.get_time_since_epoch_microseconds()\n", "            end_time_epoch_micro = \\\n\t                self.event.get_time_since_epoch_microseconds()\n\t        assert end_time_epoch_micro >= start_time_epoch_micro\n\t        return int((end_time_epoch_micro - start_time_epoch_micro) / 1000)\n\tdef get_flow_start_event_type(flow_type):\n\t    for match in MATCHING_EVENTS:\n\t        if flow_type == match[2]:\n\t            return match[0]\n\t    return None\n\tclass Event:\n", "    @dataclass\n\t    class EventPreambleInfo:\n\t        cf_name: str\n\t        type: str\n\t        job_id: str\n\t        wal_id: str = None\n\t        def are_equal_ignoring_wal_id(self, other):\n\t            return self.cf_name == other.cf_name and \\\n\t                   self.type == other.type and \\\n\t                   self.job_id == other.job_id\n", "    @staticmethod\n\t    def is_an_event_entry(log_entry):\n\t        assert isinstance(log_entry, LogEntry)\n\t        return re.findall(regexes.EVENT, log_entry.get_msg()) != []\n\t    @staticmethod\n\t    def try_parse_as_preamble(log_entry):\n\t        event_msg = log_entry.get_msg()\n\t        is_preamble, cf_name, job_id, wal_id = \\\n\t            FlushStartedEvent.try_parse_as_preamble(event_msg)\n\t        if is_preamble:\n", "            event_type = EventType.FLUSH_STARTED\n\t        else:\n\t            wal_id = None\n\t        if not is_preamble:\n\t            is_preamble, cf_name, job_id = \\\n\t                CompactionStartedEvent.try_parse_as_preamble(event_msg)\n\t            if is_preamble:\n\t                event_type = EventType.COMPACTION_STARTED\n\t        if not is_preamble:\n\t            return None\n", "        return Event.EventPreambleInfo(cf_name, event_type, job_id, wal_id)\n\t    @staticmethod\n\t    def create_event(log_entry):\n\t        assert Event.is_an_event_entry(log_entry)\n\t        entry_msg = log_entry.get_msg()\n\t        event_json_str = entry_msg[entry_msg.find(\"{\"):]\n\t        try:\n\t            event_details_dict = json.loads(event_json_str)\n\t        except json.JSONDecodeError:\n\t            raise utils.ParsingError(\n", "                f\"Error decoding event's json fields.n{log_entry}\")\n\t        event_type = Event.get_event_data_field(\n\t            event_details_dict, EventField.EVENT_TYPE)\n\t        if event_type == EventType.FLUSH_STARTED:\n\t            event = FlushStartedEvent(log_entry)\n\t        elif event_type == EventType.FLUSH_FINISHED:\n\t            event = FlushFinishedEvent(log_entry)\n\t        elif event_type == EventType.COMPACTION_STARTED:\n\t            event = CompactionStartedEvent(log_entry)\n\t        elif event_type == EventType.COMPACTION_FINISHED:\n", "            event = CompactionFinishedEvent(log_entry)\n\t        elif event_type == EventType.TABLE_FILE_CREATION:\n\t            event = TableFileCreationEvent(log_entry)\n\t        elif event_type == EventType.TABLE_FILE_DELETION:\n\t            event = TableFileDeletionEvent(log_entry)\n\t        else:\n\t            raise utils.ParsingError(\n\t                f\"Unsupported event type ({event_type}).\\n{log_entry}\")\n\t        if not event.is_valid():\n\t            raise utils.ParsingError(\n", "                f\"Invalid event (Probably missing an Event Field) (\"\n\t                f\"Mandatory: {event.get_all_mandatory_fields()}.\\\n\t                n{log_entry}\")\n\t        return event\n\t    def __init__(self, log_entry):\n\t        assert Event.is_an_event_entry(log_entry)\n\t        entry_msg = log_entry.get_msg()\n\t        event_json_str = entry_msg[entry_msg.find(\"{\"):]\n\t        self.log_time_str = log_entry.get_time()\n\t        self.matching_event_info = None\n", "        self.event_details_dict = None\n\t        self.cf_name = None\n\t        try:\n\t            self.event_details_dict = json.loads(event_json_str)\n\t        except json.JSONDecodeError:\n\t            raise utils.ParsingError(\n\t                f\"Error decoding event's json fields.n{log_entry}\")\n\t        self.cf_name = self.get_event_data_field1(\n\t            EventField.CF_NAME, default=utils.NO_CF, field_expected=False)\n\t    def __str__(self):\n", "        if not self.does_have_details():\n\t            return \"Event: No Details\"\n\t        # Accessing all fields via their methods and not logging errors\n\t        # to avoid endless recursion\n\t        return f\"Event: type: {self.get_type(default=EventType.UNKNOWN)},\" \\\n\t               f\"job-id: {self.get_job_id(default=utils.INVALID_JOB_ID)}, \" \\\n\t               f\"cf: {self.get_cf_name(utils.INVALID_CF)}\"\n\t    # By default, sort events based on their time\n\t    def __lt__(self, other):\n\t        return self.get_log_time() < other.get_log_time()\n", "    def __eq__(self, other):\n\t        return self.get_log_time() == other.get_log_time() and \\\n\t               self.get_type() == other.get_type() and \\\n\t               self.get_cf_name() == other.get_cf_name()\n\t    def does_have_details(self):\n\t        return self.event_details_dict is not None\n\t    def does_have_field(self, field):\n\t        return self.get_event_data_field1(\n\t            field, default=None, field_expected=False) is not None\n\t    @staticmethod\n", "    def get_common_mandatory_fields():\n\t        return [EventField.EVENT_TYPE,\n\t                EventField.JOB_ID]\n\t    def get_all_mandatory_fields(self):\n\t        all_mandatory_fields = self.get_mandatory_fields()\n\t        all_mandatory_fields.extend(Event.get_common_mandatory_fields())\n\t        return all_mandatory_fields\n\t    def is_valid(self):\n\t        mandatory_fields = self.get_all_mandatory_fields()\n\t        for field in mandatory_fields:\n", "            if not self.does_have_field(field):\n\t                return False\n\t        return True\n\t    @staticmethod\n\t    def get_dict_field(field, fields_dict, default=None, field_expected=True):\n\t        field_str = field.value\n\t        if field_str not in fields_dict:\n\t            if default is None and field_expected:\n\t                raise utils.ParsingError(\n\t                    f\"Can't find field ({field.value}) in dict.\\\n", "                    n{fields_dict}\")\n\t            return default\n\t        return fields_dict[field_str]\n\t    @staticmethod\n\t    def get_event_data_field(\n\t            event_details_dict, field, default=None, field_expected=True):\n\t        assert isinstance(field, EventField)\n\t        try:\n\t            return Event.get_dict_field(\n\t                field, event_details_dict, default, field_expected)\n", "        except utils.ParsingError:\n\t            raise utils.ParsingError(\n\t                f\"Can't find field ({field.value}) in event details.\"\n\t                f\"\\n{event_details_dict}\")\n\t    def get_event_data_field1(self, field, default=None, field_expected=True):\n\t        return Event.get_event_data_field(self.event_details_dict, field,\n\t                                          default, field_expected)\n\t    def get_log_time(self):\n\t        return self.log_time_str\n\t    def get_type(self, default=None):\n", "        return self.get_event_data_field1(EventField.EVENT_TYPE, default)\n\t    def get_job_id(self, default=None):\n\t        job_id = self.get_event_data_field1(EventField.JOB_ID, default)\n\t        return job_id\n\t    def get_time_since_epoch_microseconds(self, default=None):\n\t        return self.get_event_data_field1(EventField.TIME_MICROS, default)\n\t    def get_event_data_dict(self):\n\t        return self.event_details_dict\n\t    def is_db_wide_event(self):\n\t        return self.get_cf_name() == utils.NO_CF\n", "    def is_cf_event(self):\n\t        return not self.is_db_wide_event()\n\t    def get_cf_name(self, default=utils.INVALID_CF):\n\t        return self.cf_name\n\t    def set_cf_name(self, cf_name):\n\t        curr_cf_name = self.get_cf_name()\n\t        if curr_cf_name == cf_name:\n\t            return\n\t        if curr_cf_name != utils.NO_CF:\n\t            raise utils.ParsingError(\n", "                f\"Trying to set cf name {cf_name} to an event that already \"\n\t                f\"has a cf name ({curr_cf_name}.\"\n\t                f\"\\n{self}\")\n\t        self.cf_name = cf_name\n\t    def set_wal_id(self, wal_id):\n\t        curr_wal_id = self.get_wal_id_if_available()\n\t        if curr_wal_id and curr_wal_id != wal_id:\n\t            raise utils.ParsingError(\n\t                f\"Trying to set wal id {wal_id} to an event that already \"\n\t                f\"has a wal id ({curr_wal_id}.\"\n", "                f\"\\n{self}\")\n\t        self.event_details_dict[EventField.WAL_ID.value] = wal_id\n\t    def get_wal_id_if_available(self, default=None):\n\t        return self.get_event_data_field1(EventField.WAL_ID, default,\n\t                                          field_expected=False)\n\t    def get_matching_event_info_if_exists(self):\n\t        return self.matching_event_info\n\t    def try_adding_preamble_event(self, preamble_info):\n\t        if self.get_type() != preamble_info.type:\n\t            return False\n", "        # Add the cf_name as if it was part of the event\n\t        self.set_cf_name(preamble_info.cf_name)\n\t        if preamble_info.wal_id is not None:\n\t            self.set_wal_id(preamble_info.wal_id)\n\t        return True\n\t    def set_matching_event_info(self, matching_event_info):\n\t        assert isinstance(matching_event_info, MatchingEventInfo)\n\t        matching_event = matching_event_info.event\n\t        assert self.get_job_id() == matching_event.get_job_id()\n\t        assert self.get_cf_name(default=utils.NO_CF) == \\\n", "               matching_event.get_cf_name(default=utils.NO_CF)\n\t        if self.matching_event_info:\n\t            logging.error(f\"Already have a matching event.\"\n\t                          f\"\\nMe:{self}\"\n\t                          f\"\\nCandidate:{matching_event_info.event}\")\n\t        self.matching_event_info = matching_event_info\n\t    def get_my_matching_type_info_if_exists(self):\n\t        return Event.get_matching_type_info_if_exists(self.get_type())\n\t    @staticmethod\n\t    def get_matching_type_info_if_exists(event_type):\n", "        for match in MATCHING_EVENTS:\n\t            if event_type == match[0]:\n\t                return MatchingEventTypeInfo(event_type=match[1],\n\t                                             associated_flow_type=match[2],\n\t                                             is_start=False)\n\t            elif event_type == match[1]:\n\t                return MatchingEventTypeInfo(event_type=match[0],\n\t                                             associated_flow_type=match[2],\n\t                                             is_start=True)\n\t        return None\n", "    def try_adding_matching_event(self, candidate_event):\n\t        assert isinstance(candidate_event, Event)\n\t        my_matching_type_info = self.get_my_matching_type_info_if_exists()\n\t        if my_matching_type_info is None:\n\t            return False\n\t        if my_matching_type_info.event_type != candidate_event.get_type():\n\t            return False\n\t        if self.get_job_id() != candidate_event.get_job_id():\n\t            return False\n\t        if self.get_cf_name() != candidate_event.get_cf_name():\n", "            return False\n\t        # The candidate is matching. Record it\n\t        associated_flow_type = my_matching_type_info.associated_flow_type\n\t        my_matching_event_info = \\\n\t            MatchingEventInfo(event=candidate_event,\n\t                              associated_flow_type=associated_flow_type,\n\t                              is_start=my_matching_type_info.is_start)\n\t        self.set_matching_event_info(my_matching_event_info)\n\t        candidate_matching_event_info =\\\n\t            MatchingEventInfo(event=self,\n", "                              associated_flow_type=associated_flow_type,\n\t                              is_start=not my_matching_event_info.is_start)\n\t        candidate_event.set_matching_event_info(candidate_matching_event_info)\n\t        return True\n\tclass FlushStartedEvent(Event):\n\t    @staticmethod\n\t    def try_parse_as_preamble(event_msg):\n\t        # [column_family_name_000018] [JOB 38] Flushing memtable with next log file: 5 # noqa\n\t        # Returns is_preamble, cf_name, job_id, wal_id\n\t        match = re.search(regexes.FLUSH_EVENT_PREAMBLE, event_msg)\n", "        if not match:\n\t            return False, None, None, None\n\t        assert len(match.groups()) == 3\n\t        return True, \\\n\t            match.group('cf'), int(match.group('job_id')), \\\n\t            int(match.group('wal_id'))\n\t    # 2023/01/04-08:55:00.625647 27424 EVENT_LOG_v1\n\t    # {\"time_micros\": 1672822500625643, \"job\": 8,\n\t    # \"event\": \"flush_started\",\n\t    # \"num_memtables\": 1, \"num_entries\": 59913, \"num_deletes\": 0,\n", "    # \"total_data_size\": 61530651, \"memory_usage\": 66349552,\n\t    # \"flush_reason\": \"Write Buffer Full\"}\n\t    #\n\t    def __init__(self, log_entry):\n\t        super().__init__(log_entry)\n\t    @staticmethod\n\t    def get_mandatory_fields():\n\t        return [EventField.TIME_MICROS,\n\t                EventField.FLUSH_REASON]\n\t    def get_flush_reason(self, default=None):\n", "        return self.get_event_data_field1(EventField.FLUSH_REASON, default)\n\t    def get_num_entries(self, default=0):\n\t        return self.get_event_data_field1(EventField.NUM_ENTRIES, default)\n\t    def get_num_deletes(self, default=0):\n\t        return self.get_event_data_field1(EventField.NUM_DELETES, default)\n\t    def get_num_memtables(self, default=0):\n\t        return self.get_event_data_field1(EventField.NUM_MEMTABLES, default)\n\t    def get_total_data_size_bytes(self, default=0):\n\t        return self.get_event_data_field1(EventField.TOTAL_DATA_SIZE, default)\n\tclass FlushFinishedEvent(Event):\n", "    # 2023/01/04-08:55:00.743632 27424\n\t    # (Original Log Time 2023/01/04-08:55:00.743481)\n\t    # EVENT_LOG_v1 {\"time_micros\": 1672822500743473, \"job\": 8,\n\t    # \"event\": \"flush_finished\", \"output_compression\": \"NoCompression\",\n\t    # \"lsm_state\": [8, 3, 45, 427, 822, 0, 0], \"immutable_memtables\": 0}\n\t    #\n\t    def __init__(self, log_entry):\n\t        super().__init__(log_entry)\n\t    @staticmethod\n\t    def get_mandatory_fields():\n", "        return [EventField.TIME_MICROS]\n\tclass CompactionStartedEvent(Event):\n\t    @staticmethod\n\t    def try_parse_as_preamble(event_msg):\n\t        # [default] [JOB 13] Compacting 1@1 + 5@2 files to L2, score 1.63\n\t        # Returns is_preamble, cf_name, job_id\n\t        match = re.search(regexes.COMPACTION_EVENT_PREAMBLE, event_msg)\n\t        if not match:\n\t            return False, None, None\n\t        assert len(match.groups()) == 2\n", "        return True, match.group('cf'), int(match.group('job_id'))\n\t    # 2023/01/04-08:55:00.743718 27420 EVENT_LOG_v1\n\t    # {\"time_micros\": 1672822500743711, \"job\": 9,\n\t    # \"event\": \"compaction_started\", \"compaction_reason\": \"LevelL0FilesNum\",\n\t    # \"files_L0\": [17250, 17247, 17243, 17239], \"score\": 1,\n\t    # \"input_data_size\": 251316602}\n\t    #\n\t    def __init__(self, log_entry):\n\t        super().__init__(log_entry)\n\t    @staticmethod\n", "    def get_mandatory_fields():\n\t        return [EventField.TIME_MICROS,\n\t                EventField.COMPACTION_REASON]\n\t    def get_compaction_reason(self, default=None):\n\t        return self.get_event_data_field1(EventField.COMPACTION_REASON,\n\t                                          default)\n\t    def get_input_data_size_bytes(self, default=0):\n\t        return self.get_event_data_field1(EventField.INPUT_DATA_SIZE, default)\n\t    def get_input_files(self):\n\t        # returns {<level>: list(files)}\n", "        input_files = dict()\n\t        for level_str, files_list in self.get_event_data_dict().items():\n\t            if level_str.startswith('files_L'):\n\t                level = int(level_str.lstrip('files_L'))\n\t                input_files[level] = files_list\n\t        return input_files\n\tclass CompactionFinishedEvent(Event):\n\t    # 2023/01/04-08:55:00.746783 27413\n\t    # (Original Log Time 2023/01/04-08:55:00.746653) EVENT_LOG_v1\n\t    # {\"time_micros\": 1672822500746645, \"job\": 4,\n", "    # \"event\": \"compaction_finished\",\n\t    # \"compaction_time_micros\": 971568, \"compaction_time_cpu_micros\": 935180,\n\t    # \"output_level\": 1, \"num_output_files\": 7, \"total_output_size\": 437263613,\n\t    # \"num_input_records\": 424286, \"num_output_records\": 423497,\n\t    # \"num_subcompactions\": 1, \"output_compression\": \"NoCompression\",\n\t    # \"num_single_delete_mismatches\": 0, \"num_single_delete_fallthrough\": 0,\n\t    # \"lsm_state\": [4, 7, 45, 427, 822, 0, 0]}\n\t    #\n\t    def __init__(self, log_entry):\n\t        super().__init__(log_entry)\n", "    @staticmethod\n\t    def get_mandatory_fields():\n\t        return [EventField.TIME_MICROS]\n\t    def get_num_input_records(self, default=0):\n\t        return self.get_event_data_field1(EventField.NUM_INPUT_RECORDS,\n\t                                          default)\n\t    def get_compaction_duration_micros(self, default=0):\n\t        return self.get_event_data_field1(\n\t            EventField.COMPACTION_TIME_MICROS, default)\n\t    def get_compaction_duration_seconds(self, default=0):\n", "        return int(self.get_compaction_duration_micros() / 1000)\n\t    def get_total_output_size_bytes(self, default=0):\n\t        return self.get_event_data_field1(EventField.TOTAL_OUTPUT_SIZE,\n\t                                          default)\n\t    def get_output_level(self, default=utils.INVALID_LEVEL):\n\t        return self.get_event_data_field1(EventField.OUTPUT_LEVEL, default)\n\t    def get_num_output_files(self, default=0):\n\t        return self.get_event_data_field1(EventField.NUM_OUTPUT_FILES, default)\n\tclass TableFileCreationEvent(Event):\n\t    # Sample Event:\n", "    # 2023/01/04-09:04:59.399021 27424 EVENT_LOG_v1\n\t    # {\"time_micros\": 1672823099398998, \"cf_name\": \"default\", \"job\": 8564,\n\t    # \"event\": \"table_file_creation\", \"file_number\": 37155, \"file_size\":\n\t    # 62762756, \"file_checksum\": \"\", \"file_checksum_func_name\": \"Unknown\",\n\t    #\n\t    # \"table_properties\":\n\t    # {\"data_size\": 62396458, \"index_size\": 289284,\n\t    # \"index_partitions\": 0, \"top_level_index_size\": 0,\n\t    # \"index_key_is_user_key\": 1, \"index_value_is_delta_encoded\": 1,\n\t    # \"filter_size\": 75973, \"raw_key_size\": 1458576,\"raw_average_key_size\": 24,\n", "    # \"raw_value_size\": 60774000, \"raw_average_value_size\": 1000,\n\t    # \"num_data_blocks\": 15194, \"num_entries\": 60774, \"num_filter_entries\":\n\t    # 60774, \"num_deletions\": 0, \"num_merge_operands\": 0,\n\t    # \"num_range_deletions\": 0, \"format_version\": 0, \"fixed_key_len\": 0,\n\t    # \"filter_policy\": \"bloomfilter\", \"column_family_name\": \"default\",\n\t    # \"column_family_id\": 0, \"comparator\": \"leveldb.BytewiseComparator\",\n\t    # \"merge_operator\": \"nullptr\", \"prefix_extractor_name\": \"nullptr\",\n\t    # \"property_collectors\": \"[]\", \"compression\": \"NoCompression\",\n\t    # \"compression_options\": \"window_bits=-14; level=32767; strategy=0;\n\t    # max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0;\n", "    # max_dict_buffer_bytes=0; \", \"creation_time\": 1672823099,\n\t    # \"oldest_key_time\": 1672823099, \"file_creation_time\": 1672823099,\n\t    # \"slow_compression_estimated_data_size\": 0,\n\t    # \"fast_compression_estimated_data_size\": 0,\n\t    # \"db_id\": \"c100448c-dc04-4c74-8ab2-65d72f3aa3a8\",\n\t    # \"db_session_id\": \"4GAWIG5RIF8PQWM3NOQG\", \"orig_file_number\": 37155}}\n\t    #\n\t    NO_FILTER = \"\"\n\t    def __init__(self, log_entry):\n\t        super().__init__(log_entry)\n", "    @staticmethod\n\t    def get_mandatory_fields():\n\t        return [EventField.TIME_MICROS,\n\t                EventField.CF_NAME,\n\t                EventField.FILE_NUMBER,\n\t                EventField.TABLE_PROPERTIES]\n\t    def get_cf_id(self, default=utils.INVALID_CF_ID):\n\t        cf_id =\\\n\t            self.get_table_properties_field(\n\t                TablePropertiesField.CF_ID, default)\n", "        if cf_id == utils.INVALID_CF_ID:\n\t            return None\n\t        return cf_id\n\t    def get_created_file_number(self, default=None):\n\t        return self.get_event_data_field1(EventField.FILE_NUMBER, default)\n\t    def get_compressed_file_size_bytes(self, default=0):\n\t        return self.get_event_data_field1(EventField.FILE_SIZE, default)\n\t    def get_compressed_data_size_bytes(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.DATA_SIZE, default)\n", "    def get_num_data_blocks(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.NUM_DATA_BLOCKS, default)\n\t    def get_total_keys_sizes_bytes(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.TOTAL_KEY_SIZE, default)\n\t    def get_total_values_sizes_bytes(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.TOTAL_VALUE_SIZE, default)\n\t    def get_data_size_bytes(self, default=0):\n", "        return self.get_total_keys_sizes_bytes() + \\\n\t            self.get_total_values_sizes_bytes()\n\t    def get_index_size_bytes(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.INDEX_SIZE, default)\n\t    def get_filter_size_bytes(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.FILTER_SIZE, default)\n\t    def does_use_filter(self):\n\t        filter_policy = \\\n", "            self.get_table_properties_field(\n\t                TablePropertiesField.FILTER_POLICY,\n\t                default=None, field_expected=False)\n\t        return filter_policy is not None and filter_policy != \\\n\t            TableFileCreationEvent.NO_FILTER\n\t    def get_filter_policy(self):\n\t        if not self.does_use_filter():\n\t            return None\n\t        return \\\n\t            self.get_table_properties_field(TablePropertiesField.FILTER_POLICY)\n", "    def get_num_filter_entries(self, default=0):\n\t        return self.get_table_properties_field(\n\t            TablePropertiesField.NUM_FILTER_ENTRIES, default)\n\t    def get_compression_type(self, default=None):\n\t        compression_type = \\\n\t            self.get_table_properties_field(\n\t                TablePropertiesField.COMPRESSION_TYPE, default=\"\")\n\t        if compression_type == \"\":\n\t            return None\n\t        return compression_type\n", "    def get_table_properties_field(self, table_field, default=None,\n\t                                   field_expected=True):\n\t        assert isinstance(table_field, TablePropertiesField)\n\t        table_properties_dict = \\\n\t            self.get_event_data_field1(EventField.TABLE_PROPERTIES)\n\t        try:\n\t            return self.get_dict_field(\n\t                table_field, table_properties_dict, default, field_expected)\n\t        except utils.ParsingError:\n\t            raise utils.ParsingError(\n", "                f\"{table_field} not in table properties.\\n{self}\")\n\tclass TableFileDeletionEvent(Event):\n\t    # Sample Event:\n\t    # 2023/01/04-09:05:00.808463 27416 EVENT_LOG_v1\n\t    # {\"time_micros\": 1672823100808460, \"job\": 8423,\n\t    # \"event\": \"table_file_deletion\", \"file_number\": 37162}\n\t    #\n\t    def __init__(self, log_entry):\n\t        super().__init__(log_entry)\n\t    @staticmethod\n", "    def get_mandatory_fields():\n\t        return [EventField.TIME_MICROS,\n\t                EventField.FILE_NUMBER]\n\t    def get_deleted_file_number(self, default=None):\n\t        return self.get_event_data_field1(EventField.FILE_NUMBER, default)\n\tclass EventsMngr:\n\t    \"\"\"\n\t    The events manager contains all of the events.\n\t    It stores them in a dictionary of the following format:\n\t    <cf-name>: Dictionary of cf events\n", "    (The db-wide events are stored under the \"cf name\" No_COL_NAME)\n\t    Dictionary of cf events is itself a dictionary of the following format:\n\t    <event-type>: List of Event-s, ordered by their time\n\t    \"\"\"\n\t    def __init__(self, job_id_to_cf_name_map):\n\t        assert isinstance(job_id_to_cf_name_map, dict)\n\t        self.job_id_to_cf_name_map = job_id_to_cf_name_map\n\t        self.preambles = dict()\n\t        self.events = dict()\n\t    def try_parsing_as_preamble(self, entry):\n", "        preamble_info = Event.try_parse_as_preamble(entry)\n\t        if not preamble_info:\n\t            return None\n\t        assert isinstance(preamble_info, Event.EventPreambleInfo)\n\t        # If a preamble was already encountered, it must be for the same\n\t        # parameters\n\t        job_id = preamble_info.job_id\n\t        if preamble_info.job_id not in self.preambles:\n\t            self.preambles[job_id] = preamble_info\n\t        else:\n", "            if not self.preambles[job_id].are_equal_ignoring_wal_id(\n\t                    preamble_info):\n\t                logging.error(\n\t                    f\"A preamble with same job id exists, but other preamble\"\n\t                    f\" info mismatching (IGNORING NEW). \"\n\t                    f\"Existing:{self.preambles[job_id]}. New:{preamble_info}\")\n\t                # Indicating as a preamble, but returning the existing one\n\t                return self.preambles[job_id]\n\t        return preamble_info\n\t    def try_adding_entry(self, entry):\n", "        assert isinstance(entry, LogEntry)\n\t        # A preamble event is an entry that will be pending for its\n\t        # associated event entry to provide the event with its cf name\n\t        preamble_info = self.try_parsing_as_preamble(entry)\n\t        if preamble_info:\n\t            return True, None, preamble_info.cf_name\n\t        if not Event.is_an_event_entry(entry):\n\t            return False, None, None\n\t        try:\n\t            event = Event.create_event(entry)\n", "        except utils.ParsingError as e:\n\t            # telling caller I have added it as an event since it's\n\t            # supposedly an event, but badly formatted somehow\n\t            logging.error(f\"Discarding badly constructed event.\\n{entry} \"\n\t                          f\"(exception: {e})\")\n\t            return True, None, None\n\t        # Combine associated event preamble, if any exists\n\t        event_job_id = event.get_job_id()\n\t        # The preamble may provide the cf_name and possible other info\n\t        if event_job_id in self.preambles:\n", "            preamble_info = self.preambles[event_job_id]\n\t            if event.try_adding_preamble_event(preamble_info):\n\t                del(self.preambles[event_job_id])\n\t        if event.is_db_wide_event():\n\t            self.__try_finding_cf_for_newly_added_event(event_job_id, event)\n\t        cf_name = event.get_cf_name()\n\t        try:\n\t            self.__add_event(event_job_id, cf_name, event.get_type(), event)\n\t        except utils.ParsingError:\n\t            logging.error(f\"Error adding an event.Discarding it. \\n\"\n", "                          f\"event:{event}\")\n\t            return True, None, None\n\t        self.__try_to_match_newly_added_event(event)\n\t        if cf_name == utils.NO_CF:\n\t            cf_name = None\n\t        return True, event, cf_name\n\t    def __try_finding_cf_for_newly_added_event(self, event_job_id, event):\n\t        assert event.is_db_wide_event()\n\t        if event_job_id not in self.events:\n\t            return None\n", "        # Existing (earlier) events with the same job id should have\n\t        # their cf name set. Try to find one and use it\n\t        # Assuming a job id is unique to a cf\n\t        job_cfs_names = [cf_name for cf_name in\n\t                         self.events[event_job_id].keys() if cf_name !=\n\t                         utils.NO_CF]\n\t        if not job_cfs_names:\n\t            return None\n\t        assert len(job_cfs_names) == 1\n\t        cf_name = job_cfs_names[0]\n", "        event.set_cf_name(cf_name)\n\t        return cf_name\n\t    def __add_event(self, job_id, cf_name, event_type, event):\n\t        if job_id not in self.events:\n\t            self.events[job_id] = dict()\n\t        job_events = self.events[job_id]\n\t        EventsMngr.__validate_job_has_cf_or_no_other(job_events, cf_name,\n\t                                                     event)\n\t        if cf_name not in job_events:\n\t            job_events[cf_name] = dict()\n", "        if event_type not in job_events[cf_name]:\n\t            job_events[cf_name][event_type] = []\n\t        job_events[cf_name][event_type].append(event)\n\t    @staticmethod\n\t    def __validate_job_has_cf_or_no_other(job_events, cf_name, event):\n\t        # It is illegal to have a job id with events in multiple cf-s.\n\t        # The only other allowed \"cf\" is the no-col-family cf\n\t        if not job_events or \\\n\t                cf_name == utils.NO_CF or \\\n\t                cf_name in job_events:\n", "            return\n\t        # The only valid option is that job_events will only have\n\t        # the NO_COL_FAMILY or be empty (checked above)\n\t        job_cf_names = list(job_events.keys())\n\t        if job_cf_names != [utils.NO_CF]:\n\t            raise utils.ParsingError(\n\t                f\"Job has events for more than one cf. \"\n\t                f\"CF-s: ({job_cf_names}). cf_name:{cf_name}, \"\n\t                f\"\\nEvent:{event}\")\n\t    def __try_to_match_newly_added_event(self, new_event):\n", "        cf_name = new_event.get_cf_name()\n\t        new_event_type = new_event.get_type()\n\t        matching_event_type_info = \\\n\t            Event.get_matching_type_info_if_exists(new_event_type)\n\t        if not matching_event_type_info:\n\t            return\n\t        if not matching_event_type_info.is_start:\n\t            # Attempt to match only the end event (the matching will be the\n\t            # start)\n\t            return\n", "        potentially_matching_events =\\\n\t            self.get_cf_events_by_type(cf_name,\n\t                                       matching_event_type_info.event_type)\n\t        # Try to find a match from the most recent to the first\n\t        for potential_event in reversed(potentially_matching_events):\n\t            if new_event.try_adding_matching_event(potential_event):\n\t                break\n\t    def get_cf_events(self, cf_name):\n\t        all_cf_events = []\n\t        for job_events in self.events.values():\n", "            if cf_name not in job_events:\n\t                continue\n\t            for cf_events in job_events[cf_name].values():\n\t                all_cf_events.extend(list(cf_events))\n\t        # Return the events sorted by their time\n\t        all_cf_events.sort()\n\t        return all_cf_events\n\t    def get_cf_events_by_type(self, cf_name, event_type):\n\t        assert isinstance(event_type, EventType)\n\t        all_cf_events = []\n", "        for job_events in self.events.values():\n\t            if cf_name not in job_events:\n\t                continue\n\t            if event_type not in job_events[cf_name]:\n\t                continue\n\t            all_cf_events.extend(job_events[cf_name][event_type])\n\t        # The list may not be ordered due to the original time issue\n\t        # or having event preambles matched to their events somehow\n\t        # out of order. Sorting will insure correctness even if the list\n\t        # is already sorted\n", "        all_cf_events.sort()\n\t        return all_cf_events\n\t    def get_cf_flow_events(self, flow_type, cf_name):\n\t        flow_events = []\n\t        event_start_type = get_flow_start_event_type(flow_type)\n\t        starting_events = self.get_cf_events_by_type(cf_name,\n\t                                                     event_start_type)\n\t        for start_event in starting_events:\n\t            end_event = None\n\t            matching_end_event_info = \\\n", "                start_event.get_matching_event_info_if_exists()\n\t            if matching_end_event_info:\n\t                end_event = matching_end_event_info.event\n\t            flow_events.append((start_event, end_event))\n\t        return flow_events\n\t    def get_all_flow_events(self, flow_type, cfs_names):\n\t        # The cf_name is in the event data => no need to have a per-cf\n\t        # dictionary and we will sort the events based on their time\n\t        flow_events = []\n\t        for cf_name in cfs_names:\n", "            cf_flow_events = self.get_cf_flow_events(flow_type, cf_name)\n\t            if cf_flow_events:\n\t                flow_events.extend(cf_flow_events)\n\t        # Sort the events based on the start event (\n\t        flow_events.sort(key=lambda a: a[0])\n\t        return flow_events\n\t    def debug_get_all_events(self):\n\t        return self.events\n"]}
{"filename": "json_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport io\n\timport json\n\timport logging\n\timport cache_utils\n\timport calc_utils\n\timport display_utils\n\timport log_file\n", "import utils\n\tdef get_general_json(parsed_log):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    general_json = display_utils.prepare_db_wide_info_for_display(parsed_log)\n\t    if general_json[\"DB Size Time\"] is None:\n\t        del(general_json[\"DB Size Time\"])\n\t    if general_json[\"Ingest Time\"] is None:\n\t        del(general_json[\"Ingest Time\"])\n\t    return general_json\n\tdef get_db_size_json(parsed_log):\n", "    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    stats_mngr = parsed_log.get_stats_mngr()\n\t    db_wide_stats_mngr = stats_mngr.get_db_wide_stats_mngr()\n\t    compactions_stats_mngr = stats_mngr.get_compactions_stats_mngr()\n\t    db_size_json = {}\n\t    ingest_info = calc_utils.get_db_ingest_info(db_wide_stats_mngr)\n\t    if ingest_info:\n\t        ingest_json = \\\n\t            display_utils.prepare_db_ingest_info_for_display(ingest_info)\n", "        db_size_json[\"Ingest\"] = ingest_json\n\t    else:\n\t        db_size_json[\"Ingest\"] = utils.DATA_UNAVAILABLE_TEXT\n\t    growth_info = \\\n\t        calc_utils.calc_cfs_size_bytes_growth(cfs_names,\n\t                                              compactions_stats_mngr)\n\t    growth_json =\\\n\t        display_utils.prepare_cfs_size_bytes_growth_for_display(growth_info)\n\t    db_size_json[\"CF-s Growth\"] = growth_json\n\t    return db_size_json\n", "def get_flushes_json(parsed_log):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    flushes_json = {}\n\t    flushes_stats =\\\n\t        display_utils.prepare_cf_flushes_stats_for_display(parsed_log)\n\t    if flushes_stats:\n\t        for cf_name in parsed_log.get_cfs_names(include_auto_generated=False):\n\t            if cf_name in flushes_stats:\n\t                flushes_json[cf_name] = flushes_stats[cf_name]\n\t    else:\n", "        flushes_json = utils.NO_FLUSHES_TEXT\n\t    return flushes_json\n\tdef get_compactions_json(parsed_log):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n\t    compactions_json = {}\n\t    compactions_stats = \\\n\t        display_utils.prepare_cf_compactions_stats_for_display(parsed_log)\n\t    if compactions_stats:\n\t        compactions_json.update(\n\t            display_utils.prepare_global_compactions_stats_for_display(\n", "                parsed_log))\n\t        compactions_json[\"CF-s\"] = {}\n\t        for cf_name in parsed_log.get_cfs_names(include_auto_generated=False):\n\t            if cf_name in compactions_stats:\n\t                compactions_json[\"CF-s\"][cf_name] = compactions_stats[cf_name]\n\t    else:\n\t        compactions_json = utils.NO_COMPACTIONS_TEXT\n\t    return compactions_json\n\tdef get_reads_json(parsed_log):\n\t    assert isinstance(parsed_log, log_file.ParsedLog)\n", "    db_options = parsed_log.get_database_options()\n\t    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    stats_mngr = parsed_log.get_stats_mngr()\n\t    counters_mngr = parsed_log.get_counters_mngr()\n\t    files_monitor = parsed_log.get_files_monitor()\n\t    read_stats = \\\n\t        display_utils.prepare_applicable_read_stats(cfs_names,\n\t                                                    db_options,\n\t                                                    counters_mngr,\n\t                                                    stats_mngr,\n", "                                                    files_monitor)\n\t    if read_stats:\n\t        return read_stats\n\t    else:\n\t        return utils.NO_READS_TEXT\n\tdef get_seeks_json(parsed_log):\n\t    counters_mngr = \\\n\t        parsed_log.get_counters_mngr()\n\t    seek_stats = calc_utils.get_applicable_seek_stats(\n\t        counters_mngr)\n", "    if seek_stats:\n\t        disp_dict = display_utils.prepare_seek_stats_for_display(seek_stats)\n\t        return disp_dict\n\t    else:\n\t        return utils.NO_SEEKS_TEXT\n\tdef get_warn_warnings_json(cfs_names, warnings_mngr):\n\t    warnings_info = calc_utils.get_warn_warnings_info(cfs_names, warnings_mngr)\n\t    if warnings_info:\n\t        disp_dict = \\\n\t            display_utils.prepare_warn_warnings_for_display(warnings_info)\n", "        return disp_dict\n\t    else:\n\t        return utils.NO_WARNS_TEXT\n\tdef get_error_warnings_json(warnings_mngr):\n\t    return display_utils.prepare_error_or_fatal_warnings_for_display(\n\t        warnings_mngr, prepare_error=True)\n\tdef get_fatal_warnings_json(warnings_mngr):\n\t    return display_utils.prepare_error_or_fatal_warnings_for_display(\n\t        warnings_mngr, prepare_error=False)\n\tdef get_warnings_json(parsed_log):\n", "    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\t    warnings_mngr = parsed_log.get_warnings_mngr()\n\t    warnings_json = get_warn_warnings_json(cfs_names, warnings_mngr)\n\t    return warnings_json\n\tdef get_block_cache_json(parsed_log):\n\t    cache_stats = \\\n\t        cache_utils.calc_block_cache_stats(\n\t            parsed_log.get_database_options(),\n\t            parsed_log.get_counters_mngr(),\n\t            parsed_log.get_files_monitor())\n", "    if cache_stats:\n\t        stats_mngr = parsed_log.get_stats_mngr()\n\t        detailed_block_cache_stats = \\\n\t            stats_mngr.get_block_cache_stats_mngr().get_all_cache_entries()\n\t        display_stats = \\\n\t            display_utils.prepare_block_cache_stats_for_display(\n\t                cache_stats, detailed_block_cache_stats)\n\t        return display_stats\n\t    else:\n\t        return utils.NO_BLOCK_CACHE_STATS\n", "def get_json(parsed_log):\n\t    j = dict()\n\t    j[\"General\"] = get_general_json(parsed_log)\n\t    j[\"General\"][\"CF-s\"] = \\\n\t        display_utils.prepare_general_cf_info_for_display(parsed_log)\n\t    j[\"Options\"] = {\n\t        \"Diff\":\n\t            display_utils.get_options_baseline_diff_for_display(parsed_log),\n\t        \"All Options\": display_utils.get_all_options_for_display(parsed_log)\n\t    }\n", "    j[\"DB-Size\"] = get_db_size_json(parsed_log)\n\t    j[\"Flushes\"] = get_flushes_json(parsed_log)\n\t    j[\"Compactions\"] = get_compactions_json(parsed_log)\n\t    j[\"Reads\"] = get_reads_json(parsed_log)\n\t    j[\"Seeks\"] = get_seeks_json(parsed_log)\n\t    j[\"Warnings\"] = get_warnings_json(parsed_log)\n\t    j[\"Block-Cache-Stats\"] = get_block_cache_json(parsed_log)\n\t    return j\n\tdef get_json_dump_str(json_content):\n\t    f = io.StringIO()\n", "    json.dump(json_content, f, indent=1)\n\t    return f.getvalue()\n\tdef write_json(json_file_name, json_content, output_folder, report_to_console):\n\t    json_path = utils.get_json_file_path(output_folder, json_file_name)\n\t    with json_path.open(mode='w') as json_file:\n\t        json.dump(json_content, json_file)\n\t    logging.info(f\"JSON Output is in {json_path}\")\n\t    if report_to_console:\n\t        print(f\"JSON Output is in {json_path.as_uri()}\")\n"]}
{"filename": "baseline_log_files_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport bisect\n\timport pathlib\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom pathlib import Path\n\timport log_file\n\timport regexes\n", "import utils\n\tfrom db_options import OptionsDiff, DatabaseOptions\n\t@dataclass\n\tclass Version:\n\t    major: int\n\t    minor: int\n\t    patch: int\n\t    def __init__(self, version_str):\n\t        version_parts = re.findall(regexes.VERSION, version_str)\n\t        assert len(version_parts) == 1 and len(version_parts[0]) == 3\n", "        self.major = int(version_parts[0][0])\n\t        self.minor = int(version_parts[0][1])\n\t        self.patch = int(version_parts[0][2]) if version_parts[0][2] else None\n\t    def get_patch_for_comparison(self):\n\t        if self.patch is None:\n\t            return -1\n\t        return self.patch\n\t    def __eq__(self, other):\n\t        return self.major == other.major and \\\n\t               self.minor == other.minor and \\\n", "               self.get_patch_for_comparison() == \\\n\t               other.get_patch_for_comparison()\n\t    def __lt__(self, other):\n\t        if self.major != other.major:\n\t            return self.major < other.major\n\t        elif self.minor != other.minor:\n\t            return self.minor < other.minor\n\t        else:\n\t            return self.get_patch_for_comparison() < \\\n\t                   other.get_patch_for_comparison()\n", "    def __repr__(self):\n\t        if self.patch is not None:\n\t            patch = f\".{self.patch}\"\n\t        else:\n\t            patch = \"\"\n\t        return f\"{self.major}.{self.minor}{patch}\"\n\t@dataclass\n\tclass BaselineLogFileInfo:\n\t    file_path: Path\n\t    version: Version\n", "    def __lt__(self, other):\n\t        return self.version < other.version\n\tdef find_all_baseline_log_files(baselines_logs_folder, product_name):\n\t    if product_name == utils.ProductName.ROCKSDB:\n\t        logs_regex = regexes.ROCKSDB_BASELINE_LOG_FILE\n\t    elif product_name == utils.ProductName.SPEEDB:\n\t        logs_regex = regexes.SPEEDB_BASELINE_LOG_FILE\n\t    else:\n\t        assert False\n\t    logs_folder_path = pathlib.Path(baselines_logs_folder).resolve()\n", "    files = []\n\t    for file_path in logs_folder_path.iterdir():\n\t        file_match = re.findall(logs_regex, file_path.name)\n\t        if file_match:\n\t            assert len(file_match) == 1\n\t            files.append(\n\t                BaselineLogFileInfo(file_path, Version(file_match[0])))\n\t    files.sort()\n\t    return files\n\tdef find_closest_version_idx(baseline_versions, version):\n", "    if isinstance(version, str):\n\t        version = Version(version)\n\t    if baseline_versions[0] == version:\n\t        return 0\n\t    baseline_versions.sort()\n\t    closest_version_idx = bisect.bisect_right(baseline_versions, version)\n\t    if closest_version_idx:\n\t        return closest_version_idx-1\n\t    else:\n\t        return None\n", "def find_closest_baseline_info(baselines_logs_folder, product_name,\n\t                               version_str):\n\t    baseline_files = find_all_baseline_log_files(baselines_logs_folder,\n\t                                                 product_name)\n\t    baseline_versions = [file_info.version for file_info in baseline_files]\n\t    if not baseline_versions:\n\t        return None\n\t    closest_version_idx = find_closest_version_idx(baseline_versions,\n\t                                                   version_str)\n\t    if closest_version_idx is None:\n", "        return None\n\t    return baseline_files[closest_version_idx]\n\t@dataclass\n\tclass BaselineDBOptionsInfo:\n\t    baseline_log_path: Path = None\n\t    baseline_options: DatabaseOptions = None\n\t    closest_version: Version = None\n\tdef get_baseline_database_options(baselines_logs_folder,\n\t                                  product_name,\n\t                                  version_str):\n", "    closest_baseline_info = \\\n\t        find_closest_baseline_info(baselines_logs_folder,\n\t                                   product_name,\n\t                                   version_str)\n\t    if closest_baseline_info is None:\n\t        return None\n\t    assert isinstance(closest_baseline_info, BaselineLogFileInfo)\n\t    baseline_log_path = closest_baseline_info.file_path\n\t    with baseline_log_path.open() as baseline_log_file:\n\t        log_lines = baseline_log_file.readlines()\n", "        log_lines = [line for line in log_lines]\n\t        main_parsing_context = utils.parsing_context\n\t        parsed_log = log_file.ParsedLog(str(baseline_log_path), log_lines,\n\t                                        should_init_baseline_info=False)\n\t        utils.parsing_context = main_parsing_context\n\t        baseline_options = parsed_log.get_database_options()\n\t        return BaselineDBOptionsInfo(baseline_log_path,\n\t                                     baseline_options,\n\t                                     closest_baseline_info.version)\n\t@dataclass\n", "class OptionsDiffRelToBaselineInfo:\n\t    baseline_log_path: Path = None\n\t    diff: OptionsDiff = None\n\t    closest_version: Version = None\n\tdef find_options_diff_relative_to_baseline(baselines_logs_folder,\n\t                                           product_name,\n\t                                           version,\n\t                                           db_options):\n\t    baseline_info = get_baseline_database_options(baselines_logs_folder,\n\t                                                  product_name,\n", "                                                  version)\n\t    assert isinstance(baseline_info, BaselineDBOptionsInfo)\n\t    if baseline_info is None:\n\t        return None\n\t    diff = DatabaseOptions.get_options_diff(\n\t        baseline_info.baseline_options.get_all_options(),\n\t        db_options.get_all_options())\n\t    return OptionsDiffRelToBaselineInfo(baseline_info.baseline_log_path,\n\t                                        diff,\n\t                                        baseline_info.closest_version)\n"]}
{"filename": "db_files.py", "chunked_list": ["import copy\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom enum import Enum, auto\n\timport events\n\timport utils\n\t@dataclass\n\tclass DbFileInfo:\n\t    file_number: int\n\t    cf_name: str\n", "    creation_time: str\n\t    deletion_time: str = None\n\t    size_bytes: int = 0\n\t    compressed_size_bytes: int = 0\n\t    compressed_data_size_bytes: int = 0\n\t    data_size_bytes: int = 0\n\t    index_size_bytes: int = 0\n\t    filter_size_bytes: int = 0\n\t    filter_policy: str = None\n\t    num_filter_entries: int = 0\n", "    compression_type: str = None\n\t    level: int = None\n\t    def is_alive(self):\n\t        return self.deletion_time is None\n\t    def file_deleted(self, deletion_time):\n\t        assert self.is_alive()\n\t        self.deletion_time = deletion_time\n\tclass BlockType(Enum):\n\t    INDEX = auto()\n\t    FILTER = auto()\n", "class BlockLiveFileStats:\n\t    def __init__(self):\n\t        self.num_created = 0\n\t        self.num_live = 0\n\t        # Total size of all the blocks in created files (never decreasing)\n\t        self.total_created_size_bytes = 0\n\t        # Current total size of live indexes / filters\n\t        self.curr_total_live_size_bytes = 0\n\t        # The size of the largest block created and its creation time\n\t        self.max_size_bytes = 0\n", "        self.max_size_time = None\n\t        # The max total size at some point in time\n\t        self.max_total_live_size_bytes = 0\n\t        self.max_total_live_size_time = None\n\t    def __eq__(self, other):\n\t        assert isinstance(other, BlockLiveFileStats)\n\t        return self.num_created == other.num_created and \\\n\t            self.num_live == other.num_live and \\\n\t            self. total_created_size_bytes == \\\n\t            other.total_created_size_bytes and \\\n", "            self.curr_total_live_size_bytes == \\\n\t            other.curr_total_live_size_bytes and \\\n\t            self.max_size_bytes == other.max_size_bytes and \\\n\t            self.max_size_time == other.max_size_time\n\t        # These are incorrect when there is more than a single cf\n\t        # and \\\n\t        # self.max_total_live_size_bytes == \\\n\t        # other.max_total_live_size_bytes and \\\n\t        # self.max_total_live_size_time == other.max_total_live_size_time\n\t    def block_created(self, size_bytes, time):\n", "        if size_bytes == 0:\n\t            return\n\t        self.num_created += 1\n\t        self.num_live += 1\n\t        self.total_created_size_bytes += size_bytes\n\t        self.curr_total_live_size_bytes += size_bytes\n\t        if self.max_size_bytes < size_bytes:\n\t            self.max_size_bytes = size_bytes\n\t            self.max_size_time = time\n\t        if self.max_total_live_size_bytes <\\\n", "                self.curr_total_live_size_bytes:\n\t            self.max_total_live_size_bytes = \\\n\t                self.curr_total_live_size_bytes\n\t            self.max_total_live_size_time = time\n\t    def block_deleted(self, size_bytes):\n\t        if size_bytes == 0:\n\t            return\n\t        assert self.num_live > 0\n\t        assert self.num_live <= self.num_created\n\t        self.num_live -= 1\n", "        assert self.curr_total_live_size_bytes >= \\\n\t               size_bytes\n\t        self.curr_total_live_size_bytes -= size_bytes\n\t    def get_avg_block_size(self):\n\t        if self.num_created == 0:\n\t            return 0\n\t        return self.total_created_size_bytes / self.num_created\n\tclass DbLiveFilesStats:\n\t    def __init__(self):\n\t        self.num_created = 0\n", "        self.num_live = 0\n\t        self.blocks_stats = {\n\t            BlockType.INDEX: BlockLiveFileStats(),\n\t            BlockType.FILTER: BlockLiveFileStats()\n\t        }\n\t    def file_created(self, file_info):\n\t        assert isinstance(file_info, DbFileInfo)\n\t        self.num_created += 1\n\t        self.num_live += 1\n\t        self.blocks_stats[BlockType.INDEX].block_created(\n", "            file_info.index_size_bytes, file_info.creation_time)\n\t        self.blocks_stats[BlockType.FILTER].block_created(\n\t            file_info.filter_size_bytes, file_info.creation_time)\n\t    def file_deleted(self, file_info):\n\t        assert isinstance(file_info, DbFileInfo)\n\t        assert self.num_live > 0\n\t        assert self.num_live <= self.num_created\n\t        self.num_live -= 1\n\t        self.blocks_stats[BlockType.INDEX].block_deleted(\n\t            file_info.index_size_bytes)\n", "        self.blocks_stats[BlockType.FILTER].block_deleted(\n\t            file_info.filter_size_bytes)\n\tclass DbFilesMonitor:\n\t    def __init__(self):\n\t        # Format: {<file_number>\n\t        self.files = dict()\n\t        self.live_files_stats = dict()\n\t        self.cfs_names = list()\n\t    def new_event(self, event):\n\t        event_type = event.get_type()\n", "        if event_type == events.EventType.TABLE_FILE_CREATION:\n\t            return self.file_created(event)\n\t        elif event_type == events.EventType.TABLE_FILE_DELETION:\n\t            return self.file_deleted(event)\n\t        return True\n\t    def file_created(self, event):\n\t        assert isinstance(event, events.TableFileCreationEvent)\n\t        file_number = event.get_created_file_number()\n\t        if file_number in self.files:\n\t            logging.error(\n", "                f\"Created File (number {file_number}) already exists, \"\n\t                f\"Ignoring Event.\\n{event}\")\n\t            return False\n\t        cf_name = event.get_cf_name()\n\t        file_info = DbFileInfo(file_number,\n\t                               cf_name=cf_name,\n\t                               creation_time=event.get_log_time())\n\t        file_info.compressed_size_bytes = \\\n\t            event.get_compressed_file_size_bytes()\n\t        file_info.compressed_data_size_bytes = \\\n", "            event.get_compressed_data_size_bytes()\n\t        file_info.data_size_bytes = event.get_data_size_bytes()\n\t        file_info.index_size_bytes = event.get_index_size_bytes()\n\t        file_info.filter_size_bytes = event.get_filter_size_bytes()\n\t        file_info.compression_type = event.get_compression_type()\n\t        if event.does_use_filter():\n\t            file_info.filter_policy = event.get_filter_policy()\n\t            file_info.num_filter_entries = event.get_num_filter_entries()\n\t        self.files[file_number] = file_info\n\t        if cf_name not in self.cfs_names:\n", "            self.cfs_names.append(cf_name)\n\t            assert cf_name not in self.live_files_stats\n\t            self.live_files_stats[cf_name] = DbLiveFilesStats()\n\t        self.live_files_stats[cf_name].file_created(file_info)\n\t        return True\n\t    def file_deleted(self, event):\n\t        assert isinstance(event, events.TableFileDeletionEvent)\n\t        file_number = event.get_deleted_file_number()\n\t        if file_number not in self.files:\n\t            logging.info(f\"File deleted event ((number {file_number}) \"\n", "                         f\"without a previous file created event, Ignoring \"\n\t                         f\"Event.\\n{event}\")\n\t            return False\n\t        file_info = self.files[file_number]\n\t        if not file_info.is_alive():\n\t            logging.error(\n\t                f\"Deleted File (number {file_number}) already \"\n\t                f\"deleted, Ignoring Event.\\n{event}\")\n\t            return False\n\t        file_info.file_deleted(event.get_log_time())\n", "        assert file_info.cf_name in self.live_files_stats\n\t        self.live_files_stats[file_info.cf_name].file_deleted(file_info)\n\t        return True\n\t    def get_all_files(self, file_filter=lambda info: True):\n\t        # Returns {<cf>: [<live files for this cf>\n\t        files = {}\n\t        for info in self.files.values():\n\t            if file_filter(info):\n\t                if info.cf_name not in files:\n\t                    files[info.cf_name] = list()\n", "                files[info.cf_name].append(info)\n\t        return files\n\t    def get_all_cf_files(self, cf_name):\n\t        all_cf_files = \\\n\t            self.get_all_files(lambda info: info.cf_name == cf_name)\n\t        if not all_cf_files:\n\t            return []\n\t        return all_cf_files[cf_name]\n\t    def get_all_live_files(self):\n\t        return self.get_all_files(lambda info: info.is_alive())\n", "    def get_cf_live_files(self, cf_name):\n\t        cf_live_files = \\\n\t            self.get_all_files(\n\t                lambda info: info.is_alive() and info.cf_name == cf_name)\n\t        if not cf_live_files:\n\t            return []\n\t        return cf_live_files[cf_name]\n\t    def get_blocks_stats(self):\n\t        stats = {}\n\t        for cf_name, cf_blocks_stats in self.live_files_stats.items():\n", "            stats[cf_name] = {\n\t                block_type: block_stats for block_type, block_stats in\n\t                cf_blocks_stats.blocks_stats.items()}\n\t        return stats\n\t    def get_cfs_names(self):\n\t        return self.cfs_names\n\t#\n\t# Files Stats\n\t#\n\t@dataclass\n", "class CfFilterSpecificStats:\n\t    filter_policy: str = None\n\t    avg_bpk: float = 0.0\n\tclass CfsFilesStats:\n\t    def __init__(self):\n\t        self.blocks_stats = \\\n\t            {block_type: BlockLiveFileStats() for block_type in BlockType}\n\t        # {<cf_name>: FilterSpecificStats}\n\t        self.cfs_filter_specific = dict()\n\tdef get_block_stats_for_cfs_group(cfs_names, files_monitor, block_type):\n", "    assert isinstance(files_monitor, DbFilesMonitor)\n\t    assert isinstance(block_type, BlockType)\n\t    blocks_stats_all_cfs = files_monitor.get_blocks_stats()\n\t    stats = None\n\t    for cf_name in cfs_names:\n\t        if cf_name not in blocks_stats_all_cfs:\n\t            continue\n\t        assert block_type in blocks_stats_all_cfs[cf_name]\n\t        if stats is None:\n\t            stats = copy.deepcopy(blocks_stats_all_cfs[cf_name][block_type])\n", "        else:\n\t            block_stats = blocks_stats_all_cfs[cf_name][block_type]\n\t            assert isinstance(block_stats, BlockLiveFileStats)\n\t            stats.num_created += block_stats.num_created\n\t            stats.num_live += block_stats.num_live\n\t            stats.total_created_size_bytes += \\\n\t                block_stats.total_created_size_bytes\n\t            stats.curr_total_live_size_bytes += \\\n\t                block_stats.curr_total_live_size_bytes\n\t            if stats.max_size_bytes < block_stats.max_size_bytes:\n", "                stats.max_size_bytes = block_stats.max_size_bytes\n\t                stats.max_size_time = block_stats.max_size_time\n\t            stats.max_total_live_size_bytes += \\\n\t                block_stats.max_total_live_size_bytes\n\t    return stats\n\tdef calc_cf_files_stats(cfs_names, files_monitor):\n\t    assert isinstance(files_monitor, DbFilesMonitor)\n\t    stats = CfsFilesStats()\n\t    for block_type in BlockType:\n\t        stats.blocks_stats[block_type] = get_block_stats_for_cfs_group(\n", "            cfs_names, files_monitor, block_type)\n\t    num_cf_files = 0\n\t    for cf_name in cfs_names:\n\t        cf_files = files_monitor.get_all_cf_files(cf_name)\n\t        num_cf_files += len(cf_files) if cf_files else 0\n\t        if num_cf_files == 0:\n\t            logging.info(f\"No files for cf {cf_name}\")\n\t            continue\n\t        total_filters_sizes_bytes = 0\n\t        total_num_filters_entries = 0\n", "        filter_policy = None\n\t        for i, file_info in enumerate(cf_files):\n\t            if i == 0:\n\t                # Set the filter policy for the cf on the first. Changing\n\t                # it later is either a bug, or an unsupported (by the\n\t                # parser) dynamic change of the filter policy\n\t                filter_policy = file_info.filter_policy\n\t            else:\n\t                if filter_policy != utils.INVALID_FILTER_POLICY \\\n\t                        and filter_policy != file_info.filter_policy:\n", "                    logging.warning(\n\t                        f\"Filter policy changed for CF. Not supported\"\n\t                        f\"currently. cf:{cf_name}\\nfile_info:{file_info}\")\n\t                    filter_policy = utils.INVALID_FILTER_POLICY\n\t                    continue\n\t            total_filters_sizes_bytes += file_info.filter_size_bytes\n\t            total_num_filters_entries += file_info.num_filter_entries\n\t        avg_bpk = 0\n\t        if filter_policy is not None and filter_policy != \\\n\t                utils.INVALID_FILTER_POLICY:\n", "            if total_num_filters_entries > 0:\n\t                avg_bpk =\\\n\t                    (8 * total_filters_sizes_bytes) / total_num_filters_entries\n\t            else:\n\t                logging.warning(f\"No filter entries. cf:{cf_name}\\n\"\n\t                                f\"file info:{file_info}\")\n\t        stats.cfs_filter_specific[cf_name] =\\\n\t            CfFilterSpecificStats(filter_policy=filter_policy, avg_bpk=avg_bpk)\n\t    if num_cf_files == 0:\n\t        logging.info(f\"No files for all cf-s ({[cfs_names]}\")\n", "        return None\n\t    return stats\n"]}
{"filename": "log_file.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport logging\n\timport re\n\timport baseline_log_files_utils\n\timport regexes\n\timport utils\n\tfrom cfs_infos import CfsMetadata\n\tfrom compactions import CompactionsMonitor\n", "from counters import CountersMngr\n\tfrom db_files import DbFilesMonitor\n\tfrom db_options import DatabaseOptions\n\tfrom events import EventsMngr\n\tfrom log_entry import LogEntry\n\tfrom log_file_options_parser import LogFileOptionsParser\n\tfrom stats_mngr import StatsMngr\n\tfrom warnings_mngr import WarningsMngr\n\tget_error_context = utils.get_error_context_from_entry\n\tclass LogFileMetadata:\n", "    \"\"\"\n\t    Contains the metadata information about a log file:\n\t    - Product Name (RocksDB / Speedb)\n\t    - S/W Version\n\t    - Git Hash\n\t    - DB Session Id\n\t    - Times of first and last log entries in the file\n\t    \"\"\"\n\t    def __init__(self, log_entries, start_entry_idx):\n\t        self.product_name = None\n", "        self.version = None\n\t        self.db_session_id = None\n\t        self.git_hash = None\n\t        self.start_time = None\n\t        # Will be set later (when file is fully parsed)\n\t        self.end_time = None\n\t        if len(log_entries) == 0:\n\t            logging.warning(\"Empty Metadata pars (no entries)\")\n\t            return\n\t        self.start_time = log_entries[0].get_time()\n", "        # Parsing all entries and searching for predefined metadata\n\t        # entities. Some may or may not exist (e.g., the DB Session Id is\n\t        # not present in rolled logs). Also, no fixed order is assumed.\n\t        # However, it is a parsing error if the same entitiy is found more\n\t        # than once\n\t        for i, entry in enumerate(log_entries):\n\t            if self.try_parse_as_product_and_version_entry(entry):\n\t                continue\n\t            elif self.try_parse_as_git_hash_entry(entry):\n\t                continue\n", "            elif self.try_parse_as_db_session_id_entry(entry):\n\t                continue\n\t    def __str__(self):\n\t        start_time = self.start_time if self.start_time else \"UNKNOWN\"\n\t        end_time = self.end_time if self.end_time else \"UNKNOWN\"\n\t        return f\"LogFileMetadata: Start:{start_time}, End:{end_time}\"\n\t    def try_parse_as_product_and_version_entry(self, log_entry):\n\t        lines = str(log_entry.msg_lines[0]).strip()\n\t        match_parts = re.findall(regexes.PRODUCT_AND_VERSION, lines)\n\t        if not match_parts or len(match_parts) != 1:\n", "            return False\n\t        if self.product_name or self.version:\n\t            raise utils.ParsingError(\n\t                    f\"Product / Version already parsed. Product:\"\n\t                    f\"{self.product_name}, Version:{self.version}).\"\n\t                    f\"\\n{log_entry}\")\n\t        self.product_name, self.version = match_parts[0]\n\t        return True\n\t    def try_parse_as_git_hash_entry(self, log_entry):\n\t        lines = str(log_entry.msg_lines[0]).strip()\n", "        match_parts = re.findall(regexes.GIT_HASH_LINE, lines)\n\t        if not match_parts or len(match_parts) != 1:\n\t            return False\n\t        if self.git_hash:\n\t            raise utils.ParsingError(\n\t                f\"Git Hash Already Parsed ({self.git_hash})\\n{log_entry}\")\n\t        self.git_hash = match_parts[0]\n\t        return True\n\t    def try_parse_as_db_session_id_entry(self, log_entry):\n\t        lines = str(log_entry.msg_lines[0]).strip()\n", "        match_parts = re.findall(regexes.DB_SESSION_ID, lines)\n\t        if not match_parts or len(match_parts) != 1:\n\t            return False\n\t        if self.db_session_id:\n\t            raise utils.ParsingError(\n\t                f\"DB Session Id Already Parsed ({self.db_session_id})\"\n\t                f\"n{log_entry}\")\n\t        self.db_session_id = match_parts[0]\n\t        return True\n\t    def set_end_time(self, end_time):\n", "        assert not self.end_time,\\\n\t            f\"End time already set ({self.end_time})\"\n\t        assert utils.compare_times_strs(end_time, self.start_time) > 0\n\t        self.end_time = end_time\n\t    def is_valid(self):\n\t        return self.product_name and self.version\n\t    def get_product_name(self):\n\t        return self.product_name\n\t    def get_version(self):\n\t        return self.version\n", "    def get_git_hash(self):\n\t        return self.git_hash\n\t    def get_db_session_id(self):\n\t        return self.db_session_id\n\t    def get_start_time(self):\n\t        return self.start_time\n\t    def get_end_time(self):\n\t        return self.end_time\n\t    def get_log_time_span_seconds(self):\n\t        if not self.end_time:\n", "            raise utils.ParsingAssertion(f\"Unknown end time.\\n{self}\")\n\t        return int(utils.get_times_strs_diff_seconds(self.start_time,\n\t                                                     self.end_time))\n\tclass ParsedLog:\n\t    def __init__(self, log_file_path, log_lines, should_init_baseline_info):\n\t        logging.debug(f\"Starting to parse {log_file_path}\")\n\t        utils.parsing_context = utils.ParsingContext()\n\t        utils.parsing_context.parsing_starts(log_file_path)\n\t        self.log_file_path = log_file_path\n\t        self.metadata = None\n", "        self.db_options = DatabaseOptions()\n\t        self.cfs_metadata = CfsMetadata(self.log_file_path)\n\t        self.cfs_names = {}\n\t        self.next_unknown_cf_name_suffix = None\n\t        self.entry_idx = 0\n\t        self.log_entries, self.job_id_to_cf_name_map = \\\n\t            self.parse_log_to_entries(log_file_path, log_lines)\n\t        self.events_mngr = EventsMngr(self.job_id_to_cf_name_map)\n\t        self.compactions_monitor = CompactionsMonitor()\n\t        self.files_monitor = DbFilesMonitor()\n", "        self.warnings_mngr = WarningsMngr()\n\t        self.stats_mngr = StatsMngr()\n\t        self.counters_mngr = CountersMngr()\n\t        self.not_parsed_entries = []\n\t        self.parse_metadata()\n\t        self.set_end_time()\n\t        self.parse_rest_of_log()\n\t        # no need to take up the memory for that\n\t        self.log_entries = None\n\t        utils.parsing_context.parsing_ends()\n", "        self.cfs_metadata.parsing_complete()\n\t        self.warnings_mngr.set_cfs_names_on_parsing_complete(\n\t            self.get_cfs_names(include_auto_generated=False))\n\t        self.baseline_info = None\n\t        if should_init_baseline_info:\n\t            self.init_baseline_info()\n\t        logging.debug(f\"Parsing of {self.log_file_path} Complete\")\n\t    def __str__(self):\n\t        return f\"ParsedLog ({self.log_file_path})\"\n\t    @staticmethod\n", "    def parse_log_to_entries(log_file_path, log_lines):\n\t        if len(log_lines) < 1:\n\t            raise utils.EmptyLogFile(log_file_path)\n\t        # first line must be the beginning of a log entry\n\t        if not LogEntry.is_entry_start(log_lines[0]):\n\t            raise utils.InvalidLogFile(log_file_path)\n\t        logging.debug(\"Parsing log to entries\")\n\t        # Failure to parse an entry should just skip that entry\n\t        # (best effort)\n\t        log_entries = list()\n", "        job_id_to_cf_name_map = dict()\n\t        new_entry = None\n\t        skip_until_next_entry_start = False\n\t        for line_idx, line in enumerate(log_lines):\n\t            try:\n\t                if LogEntry.is_entry_start(line):\n\t                    skip_until_next_entry_start = False\n\t                    if new_entry:\n\t                        log_entries.append(new_entry.all_lines_added())\n\t                    new_entry = LogEntry(line_idx, line)\n", "                    ParsedLog.add_job_id_to_cf_mapping_if_available(\n\t                        new_entry, job_id_to_cf_name_map)\n\t                else:\n\t                    # To account for logs split into multiple lines\n\t                    if new_entry:\n\t                        new_entry.add_line(line)\n\t                    else:\n\t                        if not skip_until_next_entry_start:\n\t                            raise utils.ParsingAssertion(\n\t                                \"Bug while parsing log to entries.\",\n", "                                log_file_path, line_idx)\n\t            except utils.ParsingError as e:\n\t                logging.error(str(e.value))\n\t                # Discarding the \"bad\" entry and skipping all lines until\n\t                # finding the start of the next one.\n\t                new_entry = None\n\t                skip_until_next_entry_start = True\n\t        # Handle the last entry in the file.\n\t        if new_entry:\n\t            log_entries.append(new_entry.all_lines_added())\n", "        logging.debug(\"Completed Parsing log to entries\")\n\t        return log_entries, job_id_to_cf_name_map\n\t    @staticmethod\n\t    def add_job_id_to_cf_mapping_if_available(entry, job_id_to_cf_name_map):\n\t        job_id = entry.get_job_id()\n\t        if not job_id:\n\t            return\n\t        cf_name = entry.get_cf_name()\n\t        if not cf_name:\n\t            return\n", "        if job_id in job_id_to_cf_name_map:\n\t            assert job_id_to_cf_name_map[job_id] == cf_name\n\t        else:\n\t            job_id_to_cf_name_map[job_id] = cf_name\n\t    @staticmethod\n\t    def find_next_options_entry(log_entries, start_entry_idx):\n\t        entry_idx = start_entry_idx\n\t        while entry_idx < len(log_entries) and \\\n\t                not LogFileOptionsParser.is_options_entry(\n\t                    log_entries[entry_idx]):\n", "            entry_idx += 1\n\t        return (entry_idx < len(log_entries)), entry_idx\n\t    def parse_metadata(self):\n\t        # Metadata must be at the top of the log and surely doesn't extend\n\t        # beyond the first options line\n\t        has_found, options_entry_idx = \\\n\t            ParsedLog.find_next_options_entry(self.log_entries, self.entry_idx)\n\t        self.metadata = \\\n\t            LogFileMetadata(self.log_entries[self.entry_idx:options_entry_idx],\n\t                            self.entry_idx)\n", "        if not self.metadata.is_valid():\n\t            raise utils.InvalidLogFile(self.log_file_path)\n\t        self.entry_idx = options_entry_idx\n\t    def generate_next_unknown_cf_name(self):\n\t        # The first one is always \"default\" - NOT considered auto-generated\n\t        if self.next_unknown_cf_name_suffix is None:\n\t            self.next_unknown_cf_name_suffix = 1\n\t            return False, utils.DEFAULT_CF_NAME\n\t        else:\n\t            next_cf_name = f\"Unknown-CF-#{self.next_unknown_cf_name_suffix}\"\n", "            self.next_unknown_cf_name_suffix += 1\n\t            return True, next_cf_name\n\t    def parse_cf_options(self, cf_options_header_available):\n\t        if cf_options_header_available:\n\t            is_auto_generated = False\n\t            auto_generated_cf_name = None\n\t        else:\n\t            is_auto_generated, auto_generated_cf_name = \\\n\t                self.generate_next_unknown_cf_name()\n\t        cf_entry_idx = self.entry_idx\n", "        cf_name, options_dict, table_options_dict, self.entry_idx, \\\n\t            duplicate_option = \\\n\t            LogFileOptionsParser.parse_cf_options(self.log_entries,\n\t                                                  self.entry_idx,\n\t                                                  auto_generated_cf_name)\n\t        self.db_options.set_cf_options(cf_name,\n\t                                       options_dict,\n\t                                       table_options_dict)\n\t        # TODO - Handle failure in add_cf_found_during_cf_options_parsing\n\t        cf_id = None\n", "        self.cfs_metadata.add_cf_found_during_cf_options_parsing(\n\t            cf_name, cf_id, is_auto_generated, self.log_entries[cf_entry_idx])\n\t    @staticmethod\n\t    def find_support_info_start_index(log_entries, start_entry_idx):\n\t        entry_idx = start_entry_idx\n\t        while entry_idx < len(log_entries):\n\t            if re.findall(regexes.SUPPORT_INFO_START_LINE,\n\t                          log_entries[entry_idx].get_msg_lines()[0]):\n\t                return entry_idx\n\t            entry_idx += 1\n", "        raise utils.ParsingError(\n\t            f\"Failed finding Support Info. start-idx:{start_entry_idx}\")\n\t    def try_parse_as_cf_lifetime_entry(self):\n\t        parse_result, self.entry_idx = \\\n\t            self.cfs_metadata.try_parse_as_cf_lifetime_entries(\n\t                self.log_entries, self.entry_idx)\n\t        return parse_result\n\t    def are_dw_wide_options_set(self):\n\t        return self.db_options.are_db_wide_options_set()\n\t    def try_parse_as_db_wide_options(self):\n", "        if self.are_dw_wide_options_set() or \\\n\t                not LogFileOptionsParser.is_options_entry(\n\t                    self.get_curr_entry()):\n\t            return False\n\t        support_info_entry_idx = \\\n\t            ParsedLog.find_support_info_start_index(self.log_entries,\n\t                                                    self.entry_idx)\n\t        options_dict =\\\n\t            LogFileOptionsParser.parse_db_wide_options(self.log_entries,\n\t                                                       self.entry_idx,\n", "                                                       support_info_entry_idx)\n\t        if not options_dict:\n\t            raise utils.ParsingError(\n\t                f\"Empy db-wide options dictionary ({self}).\",\n\t                self.get_curr_error_context())\n\t        self.db_options.set_db_wide_options(options_dict)\n\t        self.entry_idx = support_info_entry_idx\n\t        return True\n\t    def try_parse_as_cf_options(self):\n\t        entry = self.get_curr_entry()\n", "        result = False\n\t        if LogFileOptionsParser.is_cf_options_start_entry(entry):\n\t            self.parse_cf_options(cf_options_header_available=True)\n\t            result = True\n\t        elif LogFileOptionsParser.is_options_entry(entry):\n\t            assert self.are_dw_wide_options_set()\n\t            self.parse_cf_options(cf_options_header_available=False)\n\t            result = True\n\t        return result\n\t    def try_parse_as_warning_entries(self):\n", "        entry = self.log_entries[self.entry_idx]\n\t        result = self.warnings_mngr.try_adding_entry(entry)\n\t        if result:\n\t            self.entry_idx += 1\n\t        return result\n\t    def try_parse_as_event_entries(self):\n\t        entry = self.get_curr_entry()\n\t        result, event, cf_name = self.events_mngr.try_adding_entry(entry)\n\t        if not result:\n\t            return False\n", "        self.add_cf_name_found_during_parsing(cf_name, entry)\n\t        if event:\n\t            self.compactions_monitor.new_event(event)\n\t            self.files_monitor.new_event(event)\n\t        self.entry_idx += 1\n\t        return True\n\t    def try_parse_as_stats_entries(self):\n\t        entry_idx_on_entry = self.entry_idx\n\t        result, self.entry_idx, cfs_names = \\\n\t            self.stats_mngr.try_adding_entries(self.log_entries,\n", "                                               self.entry_idx)\n\t        if result:\n\t            self.add_cfs_names_found_during_parsing(\n\t                cfs_names, self.get_entry(entry_idx_on_entry))\n\t        return result\n\t    def try_parse_as_counters_stats_entries(self):\n\t        result, self.entry_idx = \\\n\t            self.counters_mngr.try_adding_entries(\n\t                self.log_entries, self.entry_idx)\n\t        return result\n", "    def try_processing_in_monitors(self):\n\t        curr_entry = self.get_curr_entry()\n\t        processed, cf_name = \\\n\t            self.compactions_monitor.consider_entry(curr_entry)\n\t        if cf_name:\n\t            self.add_cf_name_found_during_parsing(cf_name, curr_entry)\n\t        return processed\n\t    def add_cf_name_found_during_parsing(self, cf_name, entry):\n\t        if cf_name is None:\n\t            return\n", "        self.add_cfs_names_found_during_parsing([cf_name], entry)\n\t    def add_cfs_names_found_during_parsing(self, cfs_names, entry):\n\t        if not cfs_names:\n\t            return\n\t        for cf_name in cfs_names:\n\t            self.cfs_metadata.handle_cf_name_found_during_parsing(\n\t                cf_name, entry)\n\t    def parse_rest_of_log(self):\n\t        # Parse all the entries and process those that are required\n\t        try:\n", "            while self.entry_idx < len(self.log_entries):\n\t                curr_entry_idx = self.entry_idx\n\t                try:\n\t                    if self.try_parse_as_cf_lifetime_entry():\n\t                        continue\n\t                    if self.try_parse_as_db_wide_options():\n\t                        continue\n\t                    if self.try_parse_as_cf_options():\n\t                        continue\n\t                    if self.try_parse_as_warning_entries():\n", "                        continue\n\t                    if self.try_parse_as_event_entries():\n\t                        continue\n\t                    if self.try_parse_as_stats_entries():\n\t                        continue\n\t                    if self.try_parse_as_counters_stats_entries():\n\t                        continue\n\t                    if not self.try_processing_in_monitors():\n\t                        self.not_parsed_entries.append(self.get_curr_entry())\n\t                    self.entry_idx += 1\n", "                except utils.ParsingError:\n\t                    logging.error(\"Error while parsing, skipping.\")\n\t                    # Make sure we are not stuck forever\n\t                    if curr_entry_idx == self.entry_idx:\n\t                        self.entry_idx += 1\n\t        except AssertionError:\n\t            logging.error(f\"Assertion While Parsing {self.log_file_path}\")\n\t            raise\n\t    def handle_cf_name_found_during_parsing(self, cf_name):\n\t        if not self.cfs_metadata.handle_cf_name_found_during_parsing(\n", "                cf_name, self.get_curr_entry()):\n\t            return\n\t    def init_baseline_info(self):\n\t        self.baseline_info = \\\n\t            baseline_log_files_utils.get_baseline_database_options(\n\t                utils.BASELINE_LOGS_FOLDER,\n\t                self.metadata.get_product_name(),\n\t                self.metadata.get_version())\n\t    def get_start_time(self):\n\t        return self.metadata.get_start_time()\n", "    def set_end_time(self):\n\t        last_entry = self.log_entries[-1]\n\t        end_time = last_entry.get_time()\n\t        self.metadata.set_end_time(end_time)\n\t    def get_num_seconds_from_start(self, time_str):\n\t        num_seconds =\\\n\t            utils.get_times_strs_diff_seconds(\n\t                self.get_start_time(), time_str)\n\t        if num_seconds < 0:\n\t            logging.warning(\n", "                f\"time ({time_str}) is before log start\\n{self.metadata}\")\n\t            return 0\n\t        return num_seconds\n\t    def get_log_file_path(self):\n\t        return self.log_file_path\n\t    def get_metadata(self):\n\t        return self.metadata\n\t    def get_cfs_names(self, include_auto_generated):\n\t        if include_auto_generated:\n\t            return self.cfs_metadata.get_all_cfs_names()\n", "        else:\n\t            return self.cfs_metadata.get_non_auto_generated_cfs_names()\n\t    def get_cfs_names_that_have_options(self, include_auto_generated):\n\t        # Return only the names of cf-s for which options exist\n\t        return self.cfs_metadata.get_cfs_names_that_have_options(\n\t            include_auto_generated)\n\t    def get_auto_generated_cfs_names(self):\n\t        return self.cfs_metadata.get_auto_generated_cf_names()\n\t    def does_have_auto_generated_cfs_names(self):\n\t        return len(self.get_auto_generated_cfs_names()) > 0\n", "    def get_num_cfs_when_certain(self):\n\t        return self.cfs_metadata.get_num_cfs_when_certain()\n\t    def get_database_options(self):\n\t        return self.db_options\n\t    def get_events_mngr(self):\n\t        return self.events_mngr\n\t    def get_compactions_monitor(self):\n\t        return self.compactions_monitor\n\t    def get_stats_mngr(self):\n\t        return self.stats_mngr\n", "    def get_counters_mngr(self):\n\t        return self.counters_mngr\n\t    def get_files_monitor(self):\n\t        return self.files_monitor\n\t    def get_warnings_mngr(self):\n\t        return self.warnings_mngr\n\t    def get_entry(self, entry_idx):\n\t        return self.log_entries[entry_idx]\n\t    def get_curr_entry(self):\n\t        if self.entry_idx < len(self.log_entries):\n", "            return self.log_entries[self.entry_idx]\n\t        else:\n\t            return None\n\t    def get_curr_error_context(self):\n\t        curr_entry = self.get_curr_entry()\n\t        if curr_entry is not None:\n\t            return get_error_context(curr_entry, self.log_file_path)\n\t        else:\n\t            return None\n\t    def get_baseline_info(self):\n", "        return self.baseline_info\n"]}
{"filename": "warnings_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport logging\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom enum import Enum\n\timport regexes\n\timport utils\n\tfrom log_entry import LogEntry\n", "class WarningType(str, Enum):\n\t    WARN = \"WARN\"\n\t    ERROR = \"ERROR\"\n\t    FATAL = \"FATAL\"\n\t@dataclass\n\tclass WarningElementInfo:\n\t    time: str\n\t    code_pos: str\n\t    warning_msg: str\n\t    def __str__(self):\n", "        return f\"{self.time} [{self.code_pos}] {self.warning_msg}\"\n\tclass WarningCategory(str, Enum):\n\t    WRITE_DELAY = \"Write-Delay\"\n\t    WRITE_STOP = \"Write-Stop\"\n\t    OTHER = \"Other\"\n\tclass WarningsMngr:\n\t    def __init__(self):\n\t        self.cfs_names = None\n\t        # Stores all warnings during parsing (and before all cfs-names are\n\t        # known)\n", "        self.unprocessed_warnings = \\\n\t            {warning_type: list() for warning_type in WarningType}\n\t        # Replaces unprocessed_warnings once parsing completes\n\t        # Format:\n\t        # {<warning-type>: {<cf-name>: {<category>: [warn-info]}}}\n\t        self.processed_warnings = None\n\t    def try_adding_entry(self, entry):\n\t        assert isinstance(entry, LogEntry)\n\t        if not entry.is_warn_msg():\n\t            return False\n", "        warning_type = entry.get_warning_type()\n\t        warning_time = entry.get_time()\n\t        code_pos = entry.get_code_pos()\n\t        warning_msg = entry.get_msg()\n\t        warning_info = WarningElementInfo(warning_time, code_pos, warning_msg)\n\t        self.unprocessed_warnings[warning_type].append(warning_info)\n\t        return True\n\t    @staticmethod\n\t    def is_write_delay_msg(warn_msg):\n\t        return re.search(regexes.WRITE_DELAY_WARN_MSG, warn_msg.strip()) is \\\n", "               not None\n\t    @staticmethod\n\t    def is_write_stop_msg(warn_msg):\n\t        return re.search(regexes.WRITE_STOP_WARN_MSG, warn_msg.strip()) is \\\n\t               not None\n\t    @staticmethod\n\t    def classify_warning_msg(msg):\n\t        if WarningsMngr.is_write_delay_msg(msg):\n\t            return WarningCategory.WRITE_DELAY\n\t        elif WarningsMngr.is_write_stop_msg(msg):\n", "            return WarningCategory.WRITE_STOP\n\t        else:\n\t            return WarningCategory.OTHER\n\t    @staticmethod\n\t    def determine_warning_msg_cf(cfs_names, msg):\n\t        cfs_in_warn = utils.try_find_cfs_in_lines(cfs_names, msg.splitlines())\n\t        if cfs_in_warn is None:\n\t            return utils.NO_CF\n\t        elif isinstance(cfs_in_warn, list):\n\t            logging.info(f\"Warning msg with multiple cfs. Can't determine. \"\n", "                         f\"Placing in DB's bucket. cfs:{cfs_in_warn}\\nmsg\")\n\t            return utils.NO_CF\n\t        else:\n\t            return cfs_in_warn\n\t    def set_cfs_names_on_parsing_complete(self, cfs_names):\n\t        assert self.processed_warnings is None\n\t        self.cfs_names = cfs_names\n\t        # Now process all the warnings\n\t        # Processed warnings are organized as follows:\n\t        # {<warning-type>: {<cf-name>: {<category>: [warn-info]}}}\n", "        #\n\t        self.processed_warnings = \\\n\t            {warning_type: dict() for warning_type in WarningType}\n\t        for warning_type in self.unprocessed_warnings:\n\t            for info in self.unprocessed_warnings[warning_type]:\n\t                assert isinstance(info, WarningElementInfo)\n\t                category = WarningsMngr.classify_warning_msg(info.warning_msg)\n\t                assert isinstance(category, WarningCategory)\n\t                cf_name = \\\n\t                    WarningsMngr.determine_warning_msg_cf(\n", "                        self.cfs_names, info.warning_msg)\n\t                assert cf_name is not None\n\t                if cf_name not in self.processed_warnings[warning_type]:\n\t                    self.processed_warnings[warning_type][cf_name] = dict()\n\t                if category not in \\\n\t                        self.processed_warnings[warning_type][cf_name]:\n\t                    self.processed_warnings[warning_type][cf_name][\n\t                        category] = list()\n\t                self.processed_warnings[warning_type][cf_name][\n\t                    category].append(info)\n", "        for warning_type in list(self.processed_warnings.keys()):\n\t            if not self.processed_warnings[warning_type]:\n\t                del(self.processed_warnings[warning_type])\n\t        self.unprocessed_warnings = None\n\t    def is_parsing_complete(self):\n\t        return self.unprocessed_warnings is None\n\t    def verify_parsing_complete(self):\n\t        assert self.is_parsing_complete()\n\t    def get_all_warnings(self):\n\t        self.verify_parsing_complete()\n", "        return self.processed_warnings\n\t    def get_warnings_of_type(self, warning_type):\n\t        assert isinstance(warning_type, WarningType)\n\t        all_warnings = self.get_all_warnings()\n\t        if not all_warnings:\n\t            return None\n\t        if warning_type not in all_warnings:\n\t            return None\n\t        return all_warnings[warning_type]\n\t    def get_warn_warnings(self):\n", "        return self.get_warnings_of_type(WarningType.WARN)\n\t    def get_error_warnings(self):\n\t        return self.get_warnings_of_type(WarningType.ERROR)\n\t    def get_fatal_warnings(self):\n\t        return self.get_warnings_of_type(WarningType.FATAL)\n\t    def get_cf_warnings_of_type(self, cf_name, warning_type):\n\t        all_warnings_of_type = self.get_warnings_of_type(warning_type)\n\t        if not all_warnings_of_type:\n\t            return None\n\t        if cf_name not in all_warnings_of_type:\n", "            return None\n\t        return all_warnings_of_type[cf_name]\n\t    def get_cf_warn_warnings(self, cf_name):\n\t        return self.get_cf_warnings_of_type(cf_name, WarningType.WARN)\n\t    def get_cf_error_warnings(self, cf_name):\n\t        return self.get_cf_warnings_of_type(cf_name, WarningType.ERROR)\n\t    def get_cf_fatal_warnings(self, cf_name):\n\t        return self.get_cf_warnings_of_type(cf_name, WarningType.FATAL)\n\t    def get_cf_warnings_of_type_and_category(\n\t            self, cf_name, warning_type, category):\n", "        assert isinstance(category, WarningCategory)\n\t        all_cf_warnings_of_type = \\\n\t            self.get_cf_warnings_of_type(cf_name, warning_type)\n\t        if not all_cf_warnings_of_type:\n\t            return None\n\t        if category not in all_cf_warnings_of_type:\n\t            return None\n\t        return all_cf_warnings_of_type[category]\n\t    def get_total_num_of_type(self, warn_type):\n\t        all_of_type = self.get_warnings_of_type(warn_type)\n", "        if not all_of_type:\n\t            return 0\n\t        total_num = 0\n\t        for cf_name, cf_data in all_of_type.items():\n\t            for category in cf_data.keys():\n\t                total_num += len(cf_data[category])\n\t        return total_num\n\t    def get_total_num_warns(self):\n\t        return self.get_total_num_of_type(WarningType.WARN)\n\t    def get_total_num_errors(self):\n", "        return self.get_total_num_of_type(WarningType.ERROR)\n\t    def get_total_num_fatals(self):\n\t        return self.get_total_num_of_type(WarningType.FATAL)\n"]}
{"filename": "cfs_infos.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport logging\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom enum import Enum, auto\n\timport regexes\n\timport utils\n\tget_error_context = utils.get_error_context_from_entry\n", "class CfDiscoveryType(Enum):\n\t    OPTIONS = 1\n\t    CREATE_NO_OPTIONS = 2\n\t    RECOVERED_NO_OPTIONS = 3\n\t    DURING_PARSING = 4\n\t@dataclass\n\tclass CfMetadata:\n\t    discovery_type: CfDiscoveryType\n\t    name: str\n\t    discovery_time: str\n", "    has_options: bool\n\t    auto_generated: bool\n\t    id: int = None\n\t    drop_time: str = None\n\t    def __lt__(self, other):\n\t        assert isinstance(other, CfMetadata)\n\t        if self.id is not None and other.id is not None:\n\t            return self.id < other.id\n\t        if self.discovery_type.value != other.discovery_type.value:\n\t            return self.discovery_type.value < other.discovery_type.value\n", "        if self.discovery_type == CfDiscoveryType.OPTIONS or \\\n\t                self.discovery_type == CfDiscoveryType.CREATE_NO_OPTIONS or \\\n\t                self.discovery_type == CfDiscoveryType.RECOVERED_NO_OPTIONS:\n\t            return self.discovery_time < other.discovery_time\n\t        return self.name < other.name\n\tclass CfOper(Enum):\n\t    CREATE = auto()\n\t    RECOVER = auto()\n\t    DROP = auto()\n\tclass CfsMetadata:\n", "    def __init__(self, log_file_path):\n\t        self.cfs_info = {}\n\t        self.log_file_path = log_file_path\n\t    def add_cf_found_during_cf_options_parsing(self, cf_name, cf_id,\n\t                                               is_auto_generated, entry):\n\t        if not self._validate_cf_doesnt_exist(cf_name, entry):\n\t            return False\n\t        self.cfs_info[cf_name] = \\\n\t            CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n\t                       name=cf_name,\n", "                       discovery_time=entry.get_time(),\n\t                       has_options=True,\n\t                       auto_generated=is_auto_generated,\n\t                       id=cf_id)\n\t        return True\n\t    def handle_cf_name_found_during_parsing(self, cf_name, entry, cf_id=None):\n\t        if cf_name in self.cfs_info:\n\t            return False\n\t        logging.info(f\"Found new cf name during parsing [{cf_name}]\")\n\t        self.cfs_info[cf_name] = \\\n", "            CfMetadata(discovery_type=CfDiscoveryType.DURING_PARSING,\n\t                       name=cf_name,\n\t                       discovery_time=entry.get_time(),\n\t                       has_options=False,\n\t                       auto_generated=False,\n\t                       id=cf_id)\n\t        return True\n\t    def try_parse_as_cf_lifetime_entries(self, log_entries, entry_idx):\n\t        # CF lifetime messages in log are currently always single liners\n\t        entry = log_entries[entry_idx]\n", "        next_entry_idx = entry_idx + 1\n\t        try:\n\t            # The cf lifetime entries are printed when the CF will have been\n\t            # discovered\n\t            if self.try_parse_as_drop_cf(entry):\n\t                return True, next_entry_idx\n\t            if self.try_parse_as_recover_cf(entry):\n\t                return True, next_entry_idx\n\t            if self.try_parse_as_create_cf(entry):\n\t                return True, next_entry_idx\n", "            return None, entry_idx\n\t        except utils.ParsingError as e:\n\t            logging.error(\n\t                f\"Failed parsing entry as cf lifetime entry ({e}).\\n \"\n\t                f\"entry:{entry}\")\n\t            return True, next_entry_idx\n\t    def try_parse_as_drop_cf(self, entry):\n\t        cf_id_match = re.findall(regexes.DROP_CF, entry.get_msg())\n\t        if not cf_id_match:\n\t            return False\n", "        assert len(cf_id_match) == 1\n\t        dropped_cf_id = cf_id_match[0]\n\t        # A dropped cf may have been discovered without knowing its id (\n\t        # e.g., auto-generated one)\n\t        cf_info = self.get_cf_info_by_id(dropped_cf_id)\n\t        if not cf_info:\n\t            logging.info(utils.format_err_msg(\n\t                f\"CF with Id {dropped_cf_id} \"\n\t                f\"dropped but no cf with matching id.\", error_context=None,\n\t                entry=entry, file_path=self.log_file_path))\n", "            return True\n\t        self._validate_cf_wasnt_dropped(cf_info, entry, raise_exception=True)\n\t        cf_info.drop_time = entry.get_time()\n\t        return True\n\t    def try_parse_as_recover_cf(self, entry):\n\t        cf_match = re.search(regexes.RECOVERED_CF, entry.get_msg())\n\t        if not cf_match:\n\t            return False\n\t        assert len(cf_match.groups()) == 3\n\t        cf_name = cf_match.group('cf')\n", "        cf_id = int(cf_match.group('cf_id'))\n\t        if self._does_cf_exist(cf_name):\n\t            self._update_cf_id(cf_name, cf_id, entry, raise_exception=True)\n\t        else:\n\t            logging.info(f\"Created cf without options [{cf_name}]\")\n\t            self.cfs_info[cf_name] = \\\n\t                CfMetadata(discovery_type=CfDiscoveryType.RECOVERED_NO_OPTIONS,\n\t                           name=cf_name,\n\t                           discovery_time=entry.get_time(),\n\t                           has_options=False,\n", "                           auto_generated=False,\n\t                           id=cf_id)\n\t        return True\n\t    def try_parse_as_create_cf(self, entry):\n\t        cf_match = re.search(regexes.CREATE_CF, entry.get_msg())\n\t        if not cf_match:\n\t            return False\n\t        assert len(cf_match.groups()) == 2\n\t        cf_name = cf_match.group('cf')\n\t        cf_id = int(cf_match.group('cf_id'))\n", "        if self._does_cf_exist(cf_name):\n\t            self._update_cf_id(cf_name, cf_id, entry, raise_exception=True)\n\t        else:\n\t            logging.info(f\"Created cf without options [{cf_name}]\")\n\t            self.cfs_info[cf_name] = \\\n\t                CfMetadata(discovery_type=CfDiscoveryType.CREATE_NO_OPTIONS,\n\t                           name=cf_name,\n\t                           discovery_time=entry.get_time(),\n\t                           has_options=False,\n\t                           auto_generated=False,\n", "                           id=cf_id)\n\t        return True\n\t    def parsing_complete(self):\n\t        self.cfs_info = utils.sort_dict_on_values(self.cfs_info)\n\t    def set_cf_id(self, cf_name, cf_id, line, line_idx):\n\t        self._validate_cf_exists(cf_name, line, line_idx)\n\t        cf_id = int(cf_id)\n\t        self._update_cf_id(cf_name, cf_id, line, line_idx)\n\t        self.cfs_info[cf_name].id = int(cf_id)\n\t    def get_cf_id(self, cf_name):\n", "        cf_id = None\n\t        if cf_name in self.cfs_info:\n\t            cf_id = self.cfs_info[cf_name].id\n\t        return cf_id\n\t    def get_cf_info_by_name(self, cf_name):\n\t        cf_info = None\n\t        if cf_name in self.cfs_info:\n\t            cf_info = self.cfs_info[cf_name]\n\t        return cf_info\n\t    def get_cf_info_by_id(self, cf_id):\n", "        cf_id = int(cf_id)\n\t        for cf_info in self.cfs_info.values():\n\t            if cf_info.id == cf_id:\n\t                return cf_info\n\t        return None\n\t    def get_non_auto_generated_cfs_names(self):\n\t        return [cf_name for cf_name in self.cfs_info.keys() if\n\t                not self.cfs_info[cf_name].auto_generated]\n\t    def get_auto_generated_cf_names(self):\n\t        return [cf_name for cf_name in self.cfs_info.keys() if\n", "                self.cfs_info[cf_name].auto_generated]\n\t    def get_cfs_names_that_have_options(self, include_auto_generated):\n\t        returned_cfs_names = []\n\t        for cf_name in self.cfs_info.keys():\n\t            if self.cfs_info[cf_name].has_options:\n\t                if include_auto_generated or \\\n\t                        not self.cfs_info[cf_name].auto_generated:\n\t                    returned_cfs_names.append(cf_name)\n\t        return returned_cfs_names\n\t    def get_all_cfs_names(self):\n", "        return list(self.cfs_info.keys())\n\t    def get_num_cfs(self):\n\t        return len(self.cfs_info) if self.cfs_info else 0\n\t    def are_there_auto_generated_cf_names(self):\n\t        return len(self.get_auto_generated_cf_names()) > 0\n\t    def get_num_cfs_when_certain(self):\n\t        # Once a log has auto-generated cf names we have no certain way of\n\t        # knowing how many cf-s there are\n\t        if self.are_there_auto_generated_cf_names():\n\t            return None\n", "        else:\n\t            return self.get_num_cfs()\n\t    def was_cf_dropped(self, cf_name):\n\t        cf_info = self.get_cf_info_by_name(cf_name)\n\t        if not cf_info:\n\t            return False\n\t        return cf_info.drop_time is not None\n\t    def get_cf_drop_time(self, cf_name):\n\t        drop_time = None\n\t        cf_info = self.get_cf_info_by_name(cf_name)\n", "        if cf_info:\n\t            drop_time = cf_info.drop_time\n\t        return drop_time\n\t    def _validate_cf_exists(self, cf_name, entry):\n\t        if cf_name not in self.cfs_info:\n\t            raise utils.ParsingError(\n\t                f\"cf ({cf_name}) doesn't exist.\",\n\t                self.get_error_context(entry))\n\t    def _does_cf_exist(self, cf_name):\n\t        return cf_name in self.cfs_info\n", "    def _validate_cf_doesnt_exist(self, cf_name, entry):\n\t        if self._does_cf_exist(cf_name):\n\t            context = self.get_error_context(entry)\n\t            logging.error(f\"cf ({cf_name}) already exists.{context}\")\n\t            return False\n\t        return True\n\t    def _update_cf_id(self, cf_name, cf_id, entry, raise_exception):\n\t        curr_cf_id = self.cfs_info[cf_name].id\n\t        if curr_cf_id and curr_cf_id != int(cf_id):\n\t            context = self.get_error_context(entry)\n", "            if raise_exception:\n\t                raise utils.ParsingError(\n\t                    f\"id ({cf_id}) of cf ({cf_name}) already exists.\", context)\n\t            else:\n\t                logging.error(\n\t                    f\"id ({cf_id}) of cf ({cf_name}) already exists.{context}\")\n\t                return False\n\t        self.cfs_info[cf_name].id = cf_id\n\t        return True\n\t    def _validate_cf_wasnt_dropped(self, cf_info, entry, raise_exception):\n", "        if cf_info.drop_time:\n\t            context = self.get_error_context(entry)\n\t            if raise_exception:\n\t                raise utils.ParsingError(\n\t                    f\"CF ({cf_info.name}) already dropped at \"\n\t                    f\"({cf_info.drop_time}).\", context)\n\t            else:\n\t                logging.error(f\"CF ({cf_info.name}) already dropped at \"\n\t                              f\"({cf_info.drop_time}).{context}\")\n\t                return False\n", "        return True\n\t    def get_error_context(self, entry):\n\t        if entry is None:\n\t            return None\n\t        return utils.get_error_context_from_entry(entry, self.log_file_path)\n"]}
{"filename": "db_options.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport copy\n\timport re\n\tfrom enum import Enum, auto\n\timport regexes\n\timport utils\n\tDB_WIDE_CF_NAME = utils.NO_CF\n\tSANITIZED_NO_VALUE = \"Missing\"\n", "RAW_NULL_PTR = \"Uninitialised\"\n\tSANITIZED_NULL_PTR = f\"Pointer ({RAW_NULL_PTR})\"\n\tSANITIZED_FALSE = str(False)\n\tSANITIZED_TRUE = str(False)\n\tclass SectionType(str, Enum):\n\t    VERSION = \"Version\"\n\t    DB_WIDE = \"DBOptions\"\n\t    CF = \"CFOptions\"\n\t    TABLE_OPTIONS = \"TableOptions.BlockBasedTable\"\n\t    def __str__(self):\n", "        return str(self.value)\n\t    @staticmethod\n\t    def extract_section_type(full_option_name):\n\t        # Try to match a section type by combining dot-delimited words\n\t        # in the full name. Longer combinations are tested before shorter\n\t        # ones (\"TableOptions.BlockBasedTable\" will be tested before\n\t        # \"CFOptions\")\n\t        name_parts = full_option_name.split('.')\n\t        for i in range(len(name_parts)):\n\t            try:\n", "                potential_section_type = '.'.join(name_parts[:-i])\n\t                return SectionType(potential_section_type)\n\t            except Exception: # noqa\n\t                continue\n\t        raise utils.ParsingError(\n\t            f\"Invalid full option name ({full_option_name}\")\n\tdef validate_section(section_type, expected_type=None):\n\t    SectionType.extract_section_type(f\"{section_type}.dummy\")\n\t    if expected_type is not None:\n\t        if section_type != expected_type:\n", "            raise utils.ParsingError(\n\t                f\"Not DB Wide section name ({section_type}\")\n\tdef parse_full_option_name(full_option_name):\n\t    section_type = SectionType.extract_section_type(full_option_name)\n\t    option_name = full_option_name.split('.')[-1]\n\t    return section_type, option_name\n\tdef extract_option_name(full_option_name):\n\t    section_type, option_name = parse_full_option_name(full_option_name)\n\t    return option_name\n\tdef get_full_option_name(section_type, option_name):\n", "    validate_section(section_type)\n\t    return f\"{section_type}.{option_name}\"\n\tdef get_db_wide_full_option_name(option_name):\n\t    return get_full_option_name(SectionType.DB_WIDE, option_name)\n\tdef extract_db_wide_option_name(full_option_name):\n\t    section_type, option_name = parse_full_option_name(full_option_name)\n\t    validate_section(section_type, SectionType.DB_WIDE)\n\t    return option_name\n\tdef get_cf_full_option_name(option_name):\n\t    return get_full_option_name(SectionType.CF, option_name)\n", "def extract_cf_option_name(full_option_name):\n\t    section_type, option_name = parse_full_option_name(full_option_name)\n\t    validate_section(section_type, SectionType.CF)\n\t    return option_name\n\tdef get_cf_table_full_option_name(option_name):\n\t    return get_full_option_name(SectionType.TABLE_OPTIONS, option_name)\n\tdef extract_cf_table_option_name(full_option_name):\n\t    section_type, option_name = parse_full_option_name(full_option_name)\n\t    validate_section(section_type, SectionType.TABLE_OPTIONS)\n\t    return option_name\n", "def is_sanitized_pointer_value(value):\n\t    return re.findall(regexes.SANITIZED_POINTER, value.strip())\n\tdef sanitized_to_raw_ptr_value(sanitized_value):\n\t    if sanitized_value.strip() == SANITIZED_NULL_PTR:\n\t        return RAW_NULL_PTR\n\t    assert is_sanitized_pointer_value(sanitized_value)\n\t    match = re.fullmatch(regexes.SANITIZED_POINTER, sanitized_value.strip())\n\t    assert match\n\t    return match.group('ptr')\n\tclass SanitizedValueType(Enum):\n", "    NO_VALUE = auto()\n\t    BOOL = auto()\n\t    NULL_PTR = auto()\n\t    POINTER = auto()\n\t    OTHER = auto()\n\t    @staticmethod\n\t    def get_type_from_str(value):\n\t        if value == SANITIZED_NO_VALUE:\n\t            return SanitizedValueType.NO_VALUE\n\t        elif value == SANITIZED_NULL_PTR:\n", "            return SanitizedValueType.NULL_PTR\n\t        elif value == SANITIZED_FALSE or value == SANITIZED_TRUE:\n\t            return SanitizedValueType.BOOL\n\t        elif is_sanitized_pointer_value(value):\n\t            return SanitizedValueType.POINTER\n\t        else:\n\t            return SanitizedValueType.OTHER\n\tdef check_and_sanitize_if_null_ptr(value):\n\t    if isinstance(value, str):\n\t        temp_value = value.lower()\n", "        if temp_value == \"none\" or \\\n\t                temp_value == \"(nil)\" or \\\n\t                temp_value == \"nil\" or \\\n\t                temp_value == \"nullptr\" or \\\n\t                temp_value == \"null\" or \\\n\t                temp_value == \"0x0\":\n\t            return True, SANITIZED_NULL_PTR\n\t    return False, value\n\tdef check_and_sanitize_if_bool_value(value, include_int):\n\t    if isinstance(value, bool):\n", "        return True, str(value)\n\t    elif isinstance(value, str):\n\t        temp_value = value.lower()\n\t        if temp_value == \"false\":\n\t            return True, str(False)\n\t        elif temp_value == \"true\":\n\t            return True, str(True)\n\t        elif include_int and temp_value == '0':\n\t            return True, str(False)\n\t        elif include_int and temp_value == '1':\n", "            return True, str(True)\n\t        else:\n\t            return False, value\n\t    elif include_int and isinstance(value, int):\n\t        is_bool = value == 0 or value == 1\n\t        if is_bool:\n\t            return True, str(bool(value))\n\t    return False, value\n\tdef check_and_sanitize_if_pointer_value(value):\n\t    is_null, sanitized_value = check_and_sanitize_if_null_ptr(value)\n", "    if is_null:\n\t        return False, value\n\t    if not isinstance(value, str):\n\t        return False, value\n\t    pointer_match = re.findall(regexes.POINTER, value.strip())\n\t    if not pointer_match:\n\t        return False, value\n\t    assert len(pointer_match) == 1\n\t    return True, f\"Pointer ({pointer_match[0]})\"\n\tdef get_sanitized_value_with_type(value):\n", "    if value is None:\n\t        return SANITIZED_NO_VALUE, SanitizedValueType.NO_VALUE\n\t    is_bool_value, sanitized_value =\\\n\t        check_and_sanitize_if_bool_value(value, include_int=False)\n\t    if is_bool_value:\n\t        return sanitized_value, SanitizedValueType.BOOL\n\t    is_null_ptr, sanitized_value = check_and_sanitize_if_null_ptr(value)\n\t    if is_null_ptr:\n\t        return sanitized_value, SanitizedValueType.NULL_PTR\n\t    is_pointer_value, sanitized_value =\\\n", "        check_and_sanitize_if_pointer_value(value)\n\t    if is_pointer_value:\n\t        return sanitized_value, SanitizedValueType.POINTER\n\t    return value, SanitizedValueType.OTHER\n\tdef get_sanitized_value(value):\n\t    sanitized_value, _ = get_sanitized_value_with_type(value)\n\t    return sanitized_value\n\tdef get_sanitized_options_diff(base_value, new_value, expect_diff):\n\t    sanitized_base_value, base_type = \\\n\t        get_sanitized_value_with_type(base_value)\n", "    sanitized_new_value, new_type = \\\n\t        get_sanitized_value_with_type(new_value)\n\t    # We expect a diff => It's a bug if both are no-value (=> equal)\n\t    if expect_diff:\n\t        assert base_type != SanitizedValueType.NO_VALUE or \\\n\t           new_type != SanitizedValueType.NO_VALUE\n\t        # Same - 2 pointers are considered equal => bug\n\t        assert base_type != SanitizedValueType.POINTER or \\\n\t               new_type != SanitizedValueType.POINTER\n\t    if base_type == SanitizedValueType.BOOL or \\\n", "            new_type == SanitizedValueType.BOOL:\n\t        _, sanitized_base_value = check_and_sanitize_if_bool_value(\n\t            sanitized_base_value, include_int=True)\n\t        _, sanitized_new_value = check_and_sanitize_if_bool_value(\n\t            sanitized_new_value, include_int=True)\n\t    if base_type == new_type and base_type == SanitizedValueType.POINTER:\n\t        are_diff = False\n\t    else:\n\t        are_diff = sanitized_base_value != sanitized_new_value\n\t    if expect_diff:\n", "        assert are_diff\n\t        return sanitized_base_value, sanitized_new_value\n\t    else:\n\t        return are_diff, sanitized_base_value, sanitized_new_value\n\tdef are_non_sanitized_values_different(base_value, new_value):\n\t    are_diff, _, _ = get_sanitized_options_diff(base_value, new_value,\n\t                                                expect_diff=False)\n\t    return are_diff\n\tclass FullNamesOptionsDict:\n\t    # {Full-option-name: {<cf>: <option-value>}}\n", "    def __init__(self, options_dict=None):\n\t        self.options_dict = {}\n\t        if options_dict is not None:\n\t            self.options_dict = copy.deepcopy(options_dict)\n\t    def __eq__(self, other):\n\t        if isinstance(other, FullNamesOptionsDict):\n\t            return self.options_dict == other.options_dict\n\t        elif isinstance(other, dict):\n\t            return self.options_dict == other\n\t        else:\n", "            assert False, \"Comparing to an invalid type \" \\\n\t                          f\"({type(other)})\"\n\t    def init_from_full_names_options_no_cf_dict(\n\t            self, cf_name, options_dict_no_cf):\n\t        # Input: {Full-option-name: <option-value>}\n\t        # Converts to the format of this class using the specified cf_name\n\t        assert self.options_dict == {}\n\t        for full_option_name, option_value in options_dict_no_cf.items():\n\t            self.options_dict[full_option_name] = {cf_name: option_value}\n\t    def set_option(self, section_type, cf_name, option_name, option_value):\n", "        if section_type == SectionType.DB_WIDE:\n\t            assert cf_name == DB_WIDE_CF_NAME\n\t        full_option_name = get_full_option_name(section_type, option_name)\n\t        if full_option_name not in self.options_dict:\n\t            self.options_dict[full_option_name] = {}\n\t        self.options_dict[full_option_name].update({\n\t            cf_name: get_sanitized_value(option_value)})\n\t    def get_option_by_full_name(self, full_option_name):\n\t        if full_option_name not in self.options_dict:\n\t            return None\n", "        return self.options_dict[full_option_name]\n\t    def get_option_by_parts(self, section_type, option_name, cf_name=None):\n\t        value = self.get_option_by_full_name(get_full_option_name(\n\t            section_type, option_name))\n\t        if value is None or cf_name is None:\n\t            return value\n\t        if cf_name not in value:\n\t            return None\n\t        return value[cf_name]\n\t    def set_db_wide_option(self, option_name, option_value):\n", "        self.set_option(SectionType.DB_WIDE, DB_WIDE_CF_NAME, option_name,\n\t                        option_value)\n\t    def get_db_wide_option(self, option_name):\n\t        return self.get_option_by_parts(SectionType.DB_WIDE, option_name,\n\t                                        DB_WIDE_CF_NAME)\n\t    def set_cf_option(self, cf_name, option_name, option_value):\n\t        self.set_option(SectionType.CF, cf_name, option_name,\n\t                        option_value)\n\t    def get_cf_option(self, option_name, cf_name=None):\n\t        return self.get_option_by_parts(SectionType.CF, option_name, cf_name)\n", "    def set_cf_table_option(self, cf_name, option_name, option_value):\n\t        self.set_option(SectionType.TABLE_OPTIONS, cf_name, option_name,\n\t                        option_value)\n\t    def get_cf_table_option(self, option_name, cf_name=None):\n\t        return self.get_option_by_parts(SectionType.TABLE_OPTIONS,\n\t                                        option_name, cf_name)\n\t    def get_options_dict(self):\n\t        return self.options_dict\n\tclass OptionsDiff:\n\t    def __init__(self, baseline_options, new_options):\n", "        self.baseline_options = baseline_options\n\t        self.new_options = new_options\n\t        self.diff_dict = {}\n\t    def __eq__(self, other):\n\t        if isinstance(other, OptionsDiff):\n\t            return self.diff_dict == other.diff_dict\n\t        elif isinstance(other, dict):\n\t            return self.diff_dict == other\n\t        else:\n\t            assert False, \"Comparing to an invalid type \" \\\n", "                          f\"({type(other)})\"\n\t    def diff_in_base(self, cf_name, full_option_name):\n\t        self.add_option_if_necessary(full_option_name)\n\t        self.diff_dict[full_option_name][cf_name] =\\\n\t            get_sanitized_options_diff(\n\t                self.baseline_options[full_option_name][cf_name], None,\n\t                expect_diff=True)\n\t    def diff_in_new(self, cf_name, full_option_name):\n\t        self.add_option_if_necessary(full_option_name)\n\t        self.diff_dict[full_option_name][cf_name] =\\\n", "            get_sanitized_options_diff(\n\t                None, self.new_options[full_option_name][cf_name],\n\t                expect_diff=True)\n\t    def diff_between(self, cf_name, full_option_name):\n\t        self.add_option_if_necessary(full_option_name)\n\t        self.diff_dict[full_option_name][cf_name] =\\\n\t            get_sanitized_options_diff(\n\t                self.baseline_options[full_option_name][cf_name],\n\t                self.new_options[full_option_name][cf_name],\n\t                expect_diff=True)\n", "    def add_option_if_necessary(self, full_option_name):\n\t        if full_option_name not in self.diff_dict:\n\t            self.diff_dict[full_option_name] = {}\n\t    def is_empty_diff(self):\n\t        return self.diff_dict == {}\n\t    def get_diff_dict(self):\n\t        return self.diff_dict\n\tclass CfsOptionsDiff:\n\t    CF_NAMES_KEY = \"cf names\"\n\t    def __init__(self, baseline_options, baseline_cf_name,\n", "                 new_options, new_cf_name, diff_dict=None):\n\t        self.baseline_options = baseline_options\n\t        self.baseline_cf_name = baseline_cf_name\n\t        self.new_options = new_options\n\t        self.new_cf_name = new_cf_name\n\t        self.diff_dict = {}\n\t        if diff_dict is not None:\n\t            self.diff_dict = diff_dict\n\t    def __eq__(self, other):\n\t        new_dict = None\n", "        if isinstance(other, CfsOptionsDiff):\n\t            new_dict = other.get_diff_dict()\n\t        elif isinstance(other, dict):\n\t            new_dict = other\n\t        else:\n\t            assert False, \"Comparing to an invalid type \" \\\n\t                          f\"({type(other)})\"\n\t        assert CfsOptionsDiff.CF_NAMES_KEY in new_dict, \\\n\t            f\"{CfsOptionsDiff.CF_NAMES_KEY} key missing in other\"\n\t        baseline_dict = self.get_diff_dict()\n", "        assert baseline_dict[CfsOptionsDiff.CF_NAMES_KEY] == \\\n\t               new_dict[CfsOptionsDiff.CF_NAMES_KEY], (\n\t            \"Comparing diff entities for mismatching cf-s: \"\n\t            f\"baseline:{baseline_dict[CfsOptionsDiff.CF_NAMES_KEY]}, \"\n\t            f\"new:{new_dict[CfsOptionsDiff.CF_NAMES_KEY]}\")\n\t        return self.diff_dict == new_dict\n\t    def diff_in_base(self, full_option_name):\n\t        self.add_option_if_necessary(full_option_name)\n\t        self.diff_dict[full_option_name] = \\\n\t            get_sanitized_options_diff(\n", "                self.baseline_options[full_option_name][self.baseline_cf_name],\n\t                None, expect_diff=True)\n\t    def diff_in_new(self, full_option_name):\n\t        self.add_option_if_necessary(full_option_name)\n\t        self.diff_dict[full_option_name] = \\\n\t            get_sanitized_options_diff(\n\t                None,\n\t                self.new_options[full_option_name][self.new_cf_name],\n\t                expect_diff=True)\n\t    def diff_between(self, full_option_name):\n", "        self.add_option_if_necessary(full_option_name)\n\t        self.diff_dict[full_option_name] = \\\n\t            get_sanitized_options_diff(\n\t                self.baseline_options[full_option_name][self.baseline_cf_name],\n\t                self.new_options[full_option_name][self.new_cf_name],\n\t                expect_diff=True)\n\t    def add_option_if_necessary(self, full_option_name):\n\t        if full_option_name not in self.diff_dict:\n\t            self.diff_dict[full_option_name] = {}\n\t    def is_empty_diff(self):\n", "        return self.diff_dict == {}\n\t    def get_diff_dict(self, exclude_compared_cfs_names=False):\n\t        if self.is_empty_diff():\n\t            return {}\n\t        result_dict = self.diff_dict\n\t        if not exclude_compared_cfs_names:\n\t            result_dict.update({CfsOptionsDiff.CF_NAMES_KEY:\n\t                                {\"Base\": self.baseline_cf_name,\n\t                                 \"New\": self.new_cf_name}})\n\t        return result_dict\n", "class DatabaseOptions:\n\t    def __init__(self, section_based_options_dict=None):\n\t        # The options are stored in the following data structure:\n\t        # {DBOptions: {DB_WIDE: {<option-name>: <option-value>, ...}},\n\t        # {CFOptions: {<cf-name>: {<option-name>: <option-value>, ...}},\n\t        # {TableOptions.BlockBasedTable:\n\t        #   {<cf-name>: {<option-name>: <option-value>, ...}}}\n\t        # If section_based_options_dict is specified, it must be in the\n\t        # internal format (e.g., used when loading options from an options\n\t        # file)\n", "        self.options_dict = {}\n\t        if section_based_options_dict:\n\t            # TODO - Verify the format of the options_dict\n\t            self.options_dict = section_based_options_dict\n\t        self.cfs_names = None\n\t        self.setup_column_families()\n\t    def __str__(self):\n\t        return \"DatabaseOptions\"\n\t    def set_db_wide_options(self, options_dict):\n\t        # The input dictionary is expected to be like this:\n", "        # {<option name>: <option_value>}\n\t        if self.are_db_wide_options_set():\n\t            raise utils.ParsingAssertion(\n\t                \"DB Wide Options Already Set\")\n\t        self.options_dict[SectionType.DB_WIDE] = {\n\t            DB_WIDE_CF_NAME: options_dict}\n\t        self.setup_column_families()\n\t    def create_section_if_necessary(self, section_type):\n\t        if section_type not in self.options_dict:\n\t            self.options_dict[section_type] = dict()\n", "    def create_cf_in_section_if_necessary(self, section_type, cf_name):\n\t        if cf_name not in self.options_dict[section_type]:\n\t            self.cfs_names.append(cf_name)\n\t            self.options_dict[section_type][cf_name] = dict()\n\t    def create_section_and_cf_in_section_if_necessary(self, section_type,\n\t                                                      cf_name):\n\t        self.create_section_if_necessary(section_type)\n\t        self.create_cf_in_section_if_necessary(section_type, cf_name)\n\t    def validate_no_section(self, section_type):\n\t        if section_type in self.options_dict:\n", "            raise utils.ParsingAssertion(\n\t                f\"{section_type} Already Set\")\n\t    def validate_no_cf_name_in_section(self, section_type, cf_name):\n\t        if cf_name in self.options_dict[section_type]:\n\t            raise utils.ParsingAssertion(\n\t                f\"{section_type} Already Set for this CF ({cf_name})\")\n\t    def set_cf_options(self, cf_name, cf_non_table_options, table_options):\n\t        # Both input dictionaries are expected to be like this:\n\t        # {<option name>: <option_value>}\n\t        self.create_section_if_necessary(SectionType.CF)\n", "        self.validate_no_cf_name_in_section(SectionType.CF, cf_name)\n\t        self.create_section_if_necessary(SectionType.TABLE_OPTIONS)\n\t        self.validate_no_cf_name_in_section(SectionType.TABLE_OPTIONS,\n\t                                            cf_name)\n\t        self.options_dict[SectionType.CF][cf_name] = cf_non_table_options\n\t        self.options_dict[SectionType.TABLE_OPTIONS][cf_name] = table_options\n\t        self.setup_column_families()\n\t    def setup_column_families(self):\n\t        self.cfs_names = []\n\t        if SectionType.CF in self.options_dict:\n", "            self.cfs_names =\\\n\t                list(self.options_dict[SectionType.CF].keys())\n\t        if SectionType.DB_WIDE in self.options_dict:\n\t            self.cfs_names.extend(\n\t                list(self.options_dict[SectionType.DB_WIDE].keys()))\n\t    def are_db_wide_options_set(self):\n\t        return SectionType.DB_WIDE in self.options_dict\n\t    def get_cfs_names(self):\n\t        cf_names_no_db_wide = copy.deepcopy(self.cfs_names)\n\t        if DB_WIDE_CF_NAME in cf_names_no_db_wide:\n", "            cf_names_no_db_wide.remove(DB_WIDE_CF_NAME)\n\t        return cf_names_no_db_wide\n\t    def set_db_wide_option(self, option_name, option_value,\n\t                           allow_new_option=False):\n\t        if not allow_new_option:\n\t            assert self.get_db_wide_option(option_name) is not None,\\\n\t                \"Trying to update a non-existent DB Wide Option.\" \\\n\t                f\"{option_name} = {option_value}\"\n\t        self.create_section_and_cf_in_section_if_necessary(\n\t            SectionType.DB_WIDE, DB_WIDE_CF_NAME)\n", "        self.options_dict[SectionType.DB_WIDE][\n\t            DB_WIDE_CF_NAME][option_name] = option_value\n\t    def get_all_options(self):\n\t        # Returns all the options that as a FullNamesOptionsDict\n\t        full_options_names = []\n\t        for section_type in self.options_dict:\n\t            for cf_name in self.options_dict[section_type]:\n\t                for option_name in self.options_dict[section_type][cf_name]:\n\t                    full_option_name = get_full_option_name(section_type,\n\t                                                            option_name)\n", "                    full_options_names.append(full_option_name)\n\t        return self.get_options(full_options_names)\n\t    def get_db_wide_options(self):\n\t        # Returns the DB-Wide options as a FullNamesOptionsDict\n\t        full_options_names = []\n\t        section_type = SectionType.DB_WIDE\n\t        if section_type in self.options_dict:\n\t            sec_dict = self.options_dict[section_type]\n\t            for option_name in sec_dict[DB_WIDE_CF_NAME]:\n\t                full_option_name = get_full_option_name(section_type,\n", "                                                        option_name)\n\t                full_options_names.append(full_option_name)\n\t        return self.get_options(full_options_names, DB_WIDE_CF_NAME)\n\t    def get_db_wide_options_for_display(self):\n\t        options_for_display = {}\n\t        db_wide_options = self.get_db_wide_options().get_options_dict()\n\t        for full_option_name, option_value_with_cf in db_wide_options.items():\n\t            option_name = extract_db_wide_option_name(full_option_name)\n\t            option_value = option_value_with_cf[DB_WIDE_CF_NAME]\n\t            options_for_display[option_name] = option_value\n", "        return options_for_display\n\t    def get_db_wide_option(self, option_name):\n\t        # Returns the raw value of a single DB-Wide option\n\t        full_option_name = get_full_option_name(SectionType.DB_WIDE,\n\t                                                option_name)\n\t        option_value_dict = \\\n\t            self.get_options([full_option_name]).get_options_dict()\n\t        if not option_value_dict:\n\t            return None\n\t        else:\n", "            return option_value_dict[full_option_name][DB_WIDE_CF_NAME]\n\t    def get_cf_options(self, cf_name):\n\t        # Returns the options for cf-name as a FullNamesOptionsDict\n\t        full_options_names = []\n\t        for section_type in self.options_dict:\n\t            if cf_name in self.options_dict[section_type]:\n\t                for option_name in self.options_dict[section_type][cf_name]:\n\t                    full_option_name = \\\n\t                        get_full_option_name(section_type, option_name)\n\t                    full_options_names.append(full_option_name)\n", "        return self.get_options(full_options_names, cf_name)\n\t    @staticmethod\n\t    def get_unified_cfs_options(cfs_options):\n\t        # dictionaries that will contain the results\n\t        common_cfs_options = {}\n\t        unique_cfs_options = {}\n\t        if not cfs_options:\n\t            return common_cfs_options, unique_cfs_options\n\t        cfs_names = [cf_name for cf_name in cfs_options.keys()]\n\t        # First assume no common options, later remove common\n", "        # Remove cf-name from values to allow comparison\n\t        for cf_name in cfs_names:\n\t            assert isinstance(cfs_options[cf_name], FullNamesOptionsDict)\n\t            unique_cfs_options[cf_name] = {}\n\t            for option_name, option_value_with_cf_name in \\\n\t                    cfs_options[cf_name].options_dict.items():\n\t                unique_cfs_options[cf_name][option_name] = \\\n\t                    option_value_with_cf_name[cf_name]\n\t        # Check all options\n\t        first_cf_name = cfs_names[0]\n", "        for option_name in list(unique_cfs_options[first_cf_name].keys()):\n\t            try:\n\t                options_values =\\\n\t                    [unique_cfs_options[cf_name][option_name] for\n\t                     cf_name in unique_cfs_options.keys()]\n\t                different_options_values = set(options_values)\n\t            except KeyError:\n\t                # At least one cf doesn't have this option => not common\n\t                continue\n\t            if len(different_options_values) != 1:\n", "                # At least one cf has a different value for this options =>\n\t                # not common\n\t                continue\n\t            # The option is common to all cf-s => place in common\n\t            # dict, and remove from all unique dicts\n\t            common_cfs_options[option_name] = options_values[0]\n\t            for cf_name in cfs_names:\n\t                del(unique_cfs_options[cf_name][option_name])\n\t        return common_cfs_options, unique_cfs_options\n\t    @staticmethod\n", "    def prepare_flat_full_names_cf_options_for_display(\n\t            cf_options, option_value_prepare_func):\n\t        if option_value_prepare_func is None:\n\t            def option_value_prepare_func(value):\n\t                return value\n\t        options_for_display = {}\n\t        table_options_for_display = {}\n\t        for full_option_name, option_value in cf_options.items():\n\t            section_type, option_name = \\\n\t                parse_full_option_name(full_option_name)\n", "            if section_type == SectionType.CF:\n\t                options_for_display[option_name] = \\\n\t                    option_value_prepare_func(option_value)\n\t            else:\n\t                assert section_type == SectionType.TABLE_OPTIONS\n\t                table_options_for_display[option_name] = \\\n\t                    option_value_prepare_func(option_value)\n\t        return options_for_display, table_options_for_display\n\t    def get_cf_options_for_display(self, cf_name):\n\t        options_for_display = {}\n", "        table_options_for_display = {}\n\t        cf_options = self.get_cf_options(cf_name)\n\t        for full_option_name, option_value_with_cf in \\\n\t                cf_options.get_options_dict().items():\n\t            option_value = option_value_with_cf[cf_name]\n\t            section_type, option_name =\\\n\t                parse_full_option_name(full_option_name)\n\t            if section_type == SectionType.CF:\n\t                options_for_display[option_name] = option_value\n\t            else:\n", "                assert section_type == SectionType.TABLE_OPTIONS\n\t                table_options_for_display[option_name] = option_value\n\t        return options_for_display, table_options_for_display\n\t    def get_cf_option(self, cf_name, option_name):\n\t        # Returns the raw value of a single CF option\n\t        full_option_name = get_full_option_name(SectionType.CF, option_name)\n\t        option_value_dict = self.get_options([full_option_name], cf_name). \\\n\t            get_options_dict()\n\t        if not option_value_dict:\n\t            return None\n", "        else:\n\t            return option_value_dict[full_option_name][cf_name]\n\t    def set_cf_option(self, cf_name, option_name, option_value,\n\t                      allow_new_option=False):\n\t        if not allow_new_option:\n\t            assert self.get_cf_option(cf_name, option_name) is not None,\\\n\t                \"Trying to update a non-existent CF Option.\" \\\n\t                f\"cf:{cf_name} - {option_name} = {option_value}\"\n\t        self.create_section_and_cf_in_section_if_necessary(SectionType.CF,\n\t                                                           cf_name)\n", "        self.options_dict[SectionType.CF][cf_name][option_name] = \\\n\t            option_value\n\t    def get_cf_table_option(self, cf_name, option_name):\n\t        # Returns the raw value of a single CF Table option\n\t        full_option_name = get_full_option_name(SectionType.TABLE_OPTIONS,\n\t                                                option_name)\n\t        option_value_dict = self.get_options([full_option_name], cf_name). \\\n\t            get_options_dict()\n\t        if not option_value_dict:\n\t            return None\n", "        else:\n\t            return option_value_dict[full_option_name][cf_name]\n\t    def get_cf_table_raw_ptr_str(self, cf_name, options_name):\n\t        sanitized_ptr = self.get_cf_table_option(cf_name, options_name)\n\t        if sanitized_ptr is None:\n\t            return None\n\t        return sanitized_to_raw_ptr_value(sanitized_ptr)\n\t    def set_cf_table_option(self, cf_name, option_name, option_value,\n\t                            allow_new_option=False):\n\t        if not allow_new_option:\n", "            assert self.get_cf_table_option(cf_name, option_name) is not None,\\\n\t                \"Trying to update a non-existent CF Table Option.\" \\\n\t                f\"cf:{cf_name} - {option_name} = {option_value}\"\n\t        self.create_section_and_cf_in_section_if_necessary(\n\t            SectionType.TABLE_OPTIONS, cf_name)\n\t        self.options_dict[SectionType.TABLE_OPTIONS][cf_name][option_name] = \\\n\t            option_value\n\t    def get_options(self, requested_full_options_names,\n\t                    requested_cf_name=None):\n\t        # The input is expected to be a set or a list of the format:\n", "        # [<full option name>, ...]\n\t        # Returns a FullNamesOptionsDict for the requested options\n\t        assert isinstance(requested_full_options_names, list) or isinstance(\n\t            requested_full_options_names, set),\\\n\t            f\"Illegal requested_full_options_names type \" \\\n\t            f\"({type(requested_full_options_names)}\"\n\t        options = FullNamesOptionsDict()\n\t        for full_option_name in requested_full_options_names:\n\t            section_type, option_name = \\\n\t                parse_full_option_name(full_option_name)\n", "            if section_type not in self.options_dict:\n\t                continue\n\t            if requested_cf_name is None:\n\t                cf_names = self.options_dict[section_type].keys()\n\t            else:\n\t                cf_names = [requested_cf_name]\n\t            for cf_name in cf_names:\n\t                if cf_name not in self.options_dict[section_type]:\n\t                    continue\n\t                if option_name in self.options_dict[section_type][cf_name]:\n", "                    option_value = \\\n\t                        self.options_dict[section_type][cf_name][\n\t                            option_name]\n\t                    options.set_option(section_type, cf_name, option_name,\n\t                                       option_value)\n\t        return options\n\t    @staticmethod\n\t    def get_options_diff(baseline, new):\n\t        # Receives 2 sets of options and returns the difference between them.\n\t        # Three cases exist:\n", "        # 1. An option exists in the old but not in the new\n\t        # 2. An option exists in the new but not in the old\n\t        # 3. The option exists in both but the values differ\n\t        # The inputs must be FullNamesOptionsDict instances. These may be\n\t        # obtained\n\t        # The resulting diff is an OptionsDiff with only actual\n\t        # differences\n\t        assert isinstance(baseline, FullNamesOptionsDict)\n\t        assert isinstance(new, FullNamesOptionsDict)\n\t        baseline_options = baseline.get_options_dict()\n", "        new_options = new.get_options_dict()\n\t        diff = OptionsDiff(baseline_options, new_options)\n\t        full_options_names_union =\\\n\t            set(baseline_options.keys()).union(set(new_options.keys()))\n\t        for full_option_name in full_options_names_union:\n\t            # if option in options_union, then it must be in one of the configs\n\t            if full_option_name not in baseline_options:\n\t                for cf_name in new_options[full_option_name]:\n\t                    diff.diff_in_new(cf_name, full_option_name)\n\t            elif full_option_name not in new_options:\n", "                for cf_name in baseline_options[full_option_name]:\n\t                    diff.diff_in_base(cf_name, full_option_name)\n\t            else:\n\t                for cf_name in baseline_options[full_option_name]:\n\t                    if cf_name in new_options[full_option_name]:\n\t                        are_different = \\\n\t                            are_non_sanitized_values_different(\n\t                                baseline_options[full_option_name][cf_name],\n\t                                new_options[full_option_name][cf_name])\n\t                        if are_different:\n", "                            diff.diff_between(cf_name, full_option_name)\n\t                    else:\n\t                        diff.diff_in_base(cf_name, full_option_name)\n\t                for cf_name in new_options[full_option_name]:\n\t                    if cf_name in baseline_options[full_option_name]:\n\t                        are_different = are_non_sanitized_values_different(\n\t                            baseline_options[full_option_name][cf_name],\n\t                            new_options[full_option_name][cf_name])\n\t                        if are_different:\n\t                            diff.diff_between(cf_name, full_option_name)\n", "                    else:\n\t                        diff.diff_in_new(cf_name, full_option_name)\n\t        return diff if not diff.is_empty_diff() else None\n\t    def get_options_diff_relative_to_me(self, new):\n\t        baseline = self.get_all_options()\n\t        return DatabaseOptions.get_options_diff(baseline, new)\n\t    @staticmethod\n\t    def get_cfs_options_diff(baseline, baseline_cf_name, new, new_cf_name):\n\t        assert isinstance(baseline, FullNamesOptionsDict)\n\t        assert isinstance(new, FullNamesOptionsDict)\n", "        # Same as get_options_diff, but for specific column families.\n\t        # This is needed to compare a parsed log file with a baseline version.\n\t        # The baseline version contains options only for the default column\n\t        # family. So, there is a need to compare the options for all column\n\t        # families in the parsed log with the same default column family in\n\t        # the base version\n\t        baseline_opts_dict = baseline.get_options_dict()\n\t        new_opts_dict = new.get_options_dict()\n\t        diff = CfsOptionsDiff(baseline_opts_dict, baseline_cf_name,\n\t                              new_opts_dict, new_cf_name)\n", "        # Unify the names of the options in both, remove the duplicates, but\n\t        # preserve the order of the options\n\t        baseline_keys_list = list(baseline_opts_dict.keys())\n\t        new_keys_list = list(new_opts_dict.keys())\n\t        options_union = \\\n\t            utils.unify_lists_preserve_order(baseline_keys_list, new_keys_list)\n\t        for full_option_name in options_union:\n\t            # if option in options_union, then it must be in one of the configs\n\t            if full_option_name not in baseline_opts_dict:\n\t                new_option_values = new_opts_dict[full_option_name]\n", "                if new_cf_name in new_option_values:\n\t                    diff.diff_in_new(full_option_name)\n\t            elif full_option_name not in new_opts_dict:\n\t                baseline_option_values = baseline_opts_dict[full_option_name]\n\t                if baseline_cf_name in baseline_option_values:\n\t                    diff.diff_in_base(full_option_name)\n\t            else:\n\t                baseline_option_values = baseline_opts_dict[full_option_name]\n\t                new_option_values = new_opts_dict[full_option_name]\n\t                if baseline_cf_name in baseline_option_values:\n", "                    if new_cf_name in new_option_values:\n\t                        are_different = are_non_sanitized_values_different(\n\t                            baseline_option_values[baseline_cf_name],\n\t                            new_option_values[new_cf_name])\n\t                        if are_different:\n\t                            diff.diff_between(full_option_name)\n\t                    else:\n\t                        diff.diff_in_base(full_option_name)\n\t                elif new_cf_name in new_option_values:\n\t                    diff.diff_in_new(full_option_name)\n", "        return diff if not diff.is_empty_diff() else None\n\t    @staticmethod\n\t    def get_db_wide_options_diff(opt_old, opt_new):\n\t        return DatabaseOptions.get_cfs_options_diff(\n\t            opt_old,\n\t            DB_WIDE_CF_NAME,\n\t            opt_new,\n\t            DB_WIDE_CF_NAME)\n\t    @staticmethod\n\t    def get_unified_cfs_diffs(cfs_diffs):\n", "        # Input [CfsOptionsDiff-1, CfsOptionsDiff-2...]\n\t        # dictionaries that will contain the results\n\t        common_cfs_diffs = {}\n\t        unique_cfs_diffs = {}\n\t        if not cfs_diffs:\n\t            return common_cfs_diffs, unique_cfs_diffs\n\t        # First assume no common options, later remove common\n\t        # unique_cfs_diffs = copy.deepcopy(cfs_diffs)\n\t        unique_cfs_diffs = copy.deepcopy(cfs_diffs)\n\t        # Check all options diffs\n", "        for option_name in list(unique_cfs_diffs[0].keys()):\n\t            try:\n\t                if option_name == CfsOptionsDiff.CF_NAMES_KEY:\n\t                    continue\n\t                option_diffs =\\\n\t                    [unique_cfs_diffs[cf_idx][option_name] for\n\t                     cf_idx in range(len(unique_cfs_diffs))]\n\t                different_options_diffs = set(option_diffs)\n\t            except KeyError:\n\t                # At least one cf doesn't have this option => not common\n", "                continue\n\t            if len(different_options_diffs) != 1:\n\t                # At least one cf has a different diff for this option =>\n\t                # not common\n\t                continue\n\t            # The option is common to all cf-s => place in common\n\t            # dict, and remove from all unique dicts\n\t            common_cfs_diffs[option_name] = option_diffs[0]\n\t            for cf_diff in unique_cfs_diffs:\n\t                del(cf_diff[option_name])\n", "        return common_cfs_diffs, unique_cfs_diffs\n"]}
{"filename": "__init__.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n"]}
{"filename": "counters.py", "chunked_list": ["import logging\n\timport re\n\timport regexes\n\timport utils\n\tformat_err_msg = utils.format_err_msg\n\tParsingAssertion = utils.ParsingAssertion\n\tErrContext = utils.ErrorContext\n\tformat_line_num_from_entry = utils.format_line_num_from_entry\n\tformat_line_num_from_line_idx = utils.format_line_num_from_line_idx\n\tget_line_num_from_entry = utils.get_line_num_from_entry\n", "class CountersMngr:\n\t    @staticmethod\n\t    def is_start_line(line):\n\t        return re.findall(regexes.STATS_COUNTERS_AND_HISTOGRAMS, line)\n\t    @staticmethod\n\t    def is_your_entry(entry):\n\t        entry_lines = entry.get_msg_lines()\n\t        return CountersMngr.is_start_line(entry_lines[0])\n\t    def try_adding_entries(self, log_entries, start_entry_idx):\n\t        entry_idx = start_entry_idx\n", "        entry = log_entries[entry_idx]\n\t        if not CountersMngr.is_your_entry(entry):\n\t            return False, entry_idx\n\t        try:\n\t            self.add_entry(entry)\n\t        except utils.ParsingError:\n\t            logging.error(f\"Error while parsing Counters entry, Skipping.\\n\"\n\t                          f\"entry:{entry}\")\n\t        entry_idx += 1\n\t        return True, entry_idx\n", "    def __init__(self):\n\t        # list of counters names in the order of their appearance\n\t        # in the log file (retaining this order assuming it is\n\t        # convenient for the user)\n\t        self.counters_names = []\n\t        self.counters = dict()\n\t        self.histogram_counters_names = []\n\t        self.histograms = dict()\n\t    def add_entry(self, entry):\n\t        time = entry.get_time()\n", "        lines = entry.get_msg_lines()\n\t        assert CountersMngr.is_start_line(lines[0])\n\t        logging.debug(f\"Parsing Counter and Histograms Entry (\"\n\t                      f\"{format_line_num_from_entry(entry)}\")\n\t        for i, line in enumerate(lines[1:]):\n\t            if self.try_parse_counter_line(time, line):\n\t                continue\n\t            if self.try_parse_histogram_line(time, line):\n\t                continue\n\t            # Skip badly formed lines\n", "            logging.error(format_err_msg(\n\t                \"Failed parsing Counters / Histogram line\"\n\t                f\"Entry. time:{time}\",\n\t                ErrContext(**{\n\t                    \"log_line_idx\": get_line_num_from_entry(entry, i + 1),\n\t                    \"log_line\": line})))\n\t    def try_parse_counter_line(self, time, line):\n\t        line_parts = re.findall(regexes.STATS_COUNTER, line)\n\t        if not line_parts:\n\t            return False\n", "        assert len(line_parts) == 1 and len(line_parts[0]) == 2\n\t        value = int(line_parts[0][1])\n\t        counter_name = line_parts[0][0]\n\t        if counter_name not in self.counters:\n\t            self.counters_names.append(counter_name)\n\t            self.counters[counter_name] = list()\n\t        entries = self.counters[counter_name]\n\t        if entries:\n\t            prev_entry = entries[-1]\n\t            prev_value = prev_entry[\"value\"]\n", "            if value < prev_value:\n\t                logging.error(format_err_msg(\n\t                    f\"count or sum DECREASED during interval - Ignoring Entry.\"\n\t                    f\"prev_value:{prev_value}, count:{value}\"\n\t                    f\" (counter:{counter_name}), \"\n\t                    f\"prev_time:{prev_entry['time']}, time:{time}\",\n\t                    ErrContext(**{\"log_line\": line})))\n\t                return True\n\t        self.counters[counter_name].append({\n\t            \"time\": time,\n", "            \"value\": value})\n\t        return True\n\t    def try_parse_histogram_line(self, time, line):\n\t        match = re.fullmatch(regexes.STATS_HISTOGRAM, line)\n\t        if not match:\n\t            return False\n\t        assert len(match.groups()) == 7\n\t        counter_name = match.group('name')\n\t        count = int(match.group('count'))\n\t        total = int(match.group('sum'))\n", "        if total > 0 and count == 0:\n\t            logging.error(format_err_msg(\n\t                f\"0 Count but total > 0 in a histogram (counter:\"\n\t                f\"{counter_name}), time:{time}\",\n\t                ErrContext(**{\"log_line\": line})))\n\t        if counter_name not in self.histograms:\n\t            self.histograms[counter_name] = list()\n\t            self.histogram_counters_names.append(counter_name)\n\t        # There are cases where the count is > 0 but the\n\t        # total is 0 (e.g., 'rocksdb.prefetched.bytes.discarded')\n", "        if total > 0:\n\t            average = float(f\"{(total / count):.2f}\")\n\t        else:\n\t            average = float(f\"{0.0:.2f}\")\n\t        entries = self.histograms[counter_name]\n\t        prev_count = 0\n\t        prev_total = 0\n\t        if entries:\n\t            prev_entry = entries[-1]\n\t            prev_count = prev_entry[\"values\"][\"Count\"]\n", "            prev_total = prev_entry[\"values\"][\"Sum\"]\n\t            if count < prev_count or total < prev_total:\n\t                logging.error(format_err_msg(\n\t                    f\"count or sum DECREASED during interval - Ignoring Entry.\"\n\t                    f\"prev_count:{prev_count}, count:{count}\"\n\t                    f\"prev_sum:{prev_total}, sum:{total},\"\n\t                    f\" (counter:{counter_name}), \"\n\t                    f\"prev_time:{prev_entry['time']}, time:{time}\",\n\t                    ErrContext(**{\"log_line\": line})))\n\t                return True\n", "        entries.append(\n\t            {\"time\": time,\n\t             \"values\": {\"P50\": float(match.group('P50')),\n\t                        \"P95\": float(match.group('P95')),\n\t                        \"P99\": float(match.group('P99')),\n\t                        \"P100\": float(match.group('P100')),\n\t                        \"Count\": count,\n\t                        \"Sum\": total,\n\t                        \"Average\": average,\n\t                        \"Interval Count\": count - prev_count,\n", "                        \"Interval Sum\": total - prev_total}})\n\t        return True\n\t    def does_have_counters_values(self):\n\t        return self.counters != {}\n\t    def does_have_histograms_values(self):\n\t        return self.histograms != {}\n\t    def get_counters_names(self):\n\t        return self.counters_names\n\t    def get_counters_times(self):\n\t        all_entries = self.get_all_counters_entries()\n", "        times = list(\n\t            {counter_entry[\"time\"]\n\t             for counter_entries in all_entries.values()\n\t             for counter_entry in counter_entries})\n\t        times.sort()\n\t        return times\n\t    def get_counter_entries(self, counter_name):\n\t        if counter_name not in self.counters:\n\t            return {}\n\t        return self.counters[counter_name]\n", "    def get_non_zeroes_counter_entries(self, counter_name):\n\t        counter_entries = self.get_counter_entries(counter_name)\n\t        return list(filter(lambda entry: entry['value'] > 0,\n\t                           counter_entries))\n\t    def are_all_counter_entries_zero(self, counter_name):\n\t        return len(self.get_non_zeroes_counter_entries(counter_name)) == 0\n\t    def get_all_counters_entries(self):\n\t        return self.counters\n\t    def get_counters_entries_not_all_zeroes(self):\n\t        result = {}\n", "        for counter_name, counter_entries in self.counters.items():\n\t            if not self.are_all_counter_entries_zero(counter_name):\n\t                result.update({counter_name: counter_entries})\n\t        return result\n\t    def get_first_counter_entry(self, counter_name):\n\t        entries = self.get_counter_entries(counter_name)\n\t        if not entries:\n\t            return {}\n\t        return entries[0]\n\t    def get_first_counter_value(self, counter_name, default=0):\n", "        last_entry = self.get_first_counter_entry(counter_name)\n\t        if not last_entry:\n\t            return default\n\t        return last_entry[\"value\"]\n\t    def get_last_counter_entry(self, counter_name):\n\t        entries = self.get_counter_entries(counter_name)\n\t        if not entries:\n\t            return {}\n\t        return entries[-1]\n\t    def get_last_counter_value(self, counter_name, default=0):\n", "        last_entry = self.get_last_counter_entry(counter_name)\n\t        if not last_entry:\n\t            return default\n\t        return last_entry[\"value\"]\n\t    def get_histogram_counters_names(self):\n\t        return self.histogram_counters_names\n\t    def get_histogram_counters_times(self):\n\t        all_entries = self.get_all_histogram_entries()\n\t        times = list(\n\t            {counter_entry[\"time\"]\n", "             for counter_entries in all_entries.values()\n\t             for counter_entry in counter_entries})\n\t        times.sort()\n\t        return times\n\t    def get_histogram_entries(self, counter_name):\n\t        if counter_name not in self.histograms:\n\t            return {}\n\t        return self.histograms[counter_name]\n\t    def get_all_histogram_entries(self):\n\t        return self.histograms\n", "    def get_last_histogram_entry(self, counter_name, non_zero):\n\t        entries = self.get_histogram_entries(counter_name)\n\t        if not entries:\n\t            return {}\n\t        last_entry = entries[-1]\n\t        is_zero_entry_func = \\\n\t            CountersMngr.is_histogram_entry_count_zero\n\t        if non_zero and is_zero_entry_func(last_entry):\n\t            return {}\n\t        return entries[-1]\n", "    @staticmethod\n\t    def is_histogram_entry_count_zero(entry):\n\t        return entry['values']['Count'] == 0\n\t    @staticmethod\n\t    def is_histogram_entry_count_not_zero(entry):\n\t        return entry['values']['Count'] > 0\n\t    def get_non_zeroes_histogram_entries(self, counter_name):\n\t        histogram_entries = self.get_histogram_entries(counter_name)\n\t        return list(filter(lambda entry: entry['values']['Count'] > 0,\n\t                           histogram_entries))\n", "    def are_all_histogram_entries_zero(self, counter_name):\n\t        return len(self.get_non_zeroes_histogram_entries(counter_name)) == 0\n\t    def get_histogram_entries_not_all_zeroes(self):\n\t        result = {}\n\t        for counter_name, histogram_entries in self.histograms.items():\n\t            if not self.are_all_histogram_entries_zero(counter_name):\n\t                result.update({counter_name: histogram_entries})\n\t        return result\n\t    @staticmethod\n\t    def get_histogram_entry_display_values(entry):\n", "        disp_values = {}\n\t        values = entry[\"values\"]\n\t        disp_values[\"Count\"] = \\\n\t            utils.get_human_readable_number(values[\"Count\"])\n\t        disp_values[\"Sum\"] = \\\n\t            utils.get_human_readable_number(values[\"Sum\"])\n\t        disp_values[\"Avg. Read Latency\"] = f'{values[\"Average\"]:.1f} us'\n\t        disp_values[\"P50\"] = f'{values[\"P50\"]:.1f} us'\n\t        disp_values[\"P95\"] = f'{values[\"P95\"]:.1f} us'\n\t        disp_values[\"P99\"] = f'{values[\"P99\"]:.1f} us'\n", "        disp_values[\"P100\"] = f'{values[\"P100\"]:.1f} us'\n\t        return disp_values\n"]}
{"filename": "utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\t\"\"\" Common constants and utilities used in the log parser's modules \"\"\"\n\timport copy\n\timport logging\n\timport pathlib\n\timport re\n\timport time\n\tfrom calendar import timegm\n", "from dataclasses import dataclass\n\tfrom datetime import datetime, timedelta\n\tfrom enum import Enum\n\tfrom typing import Optional\n\timport regexes\n\tMIN_PYTHON_VERSION_MAJOR = 3\n\tMIN_PYTHON_VERSION_MINOR = 8\n\tNO_CF = 'DB_WIDE'\n\tINVALID_CF = \"UNKNOWN-CF\"\n\tDEFAULT_CF_NAME = \"default\"\n", "INVALID_CF_ID = -1\n\tINVALID_JOB_ID = -1\n\tINVALID_FILE_NUMBER = -1\n\tINVALID_LEVEL = -1\n\tINVALID_FILTER_POLICY = \"INVALID-FILTER-POLICY\"\n\tDB_WIDE_WRITE_BUFFER_MANAGER_OPTIONS_NAME = \"write_buffer_manager\"\n\tBASELINE_LOGS_FOLDER = \"baseline_logs\"\n\tDIFF_BASELINE_NAME = \"Baseline\"\n\tDIFF_LOG_NAME = \"Parsed Log\"\n\tDEFAULT_OUTPUT_FOLDER = \"output_files\"\n", "OUTPUT_SUB_FOLDER_PREFIX = \"run_\"\n\tDEFAULT_LOG_FILE_NAME = \"log_parser.log\"\n\tDEFAULT_JSON_FILE_NAME = \"log.json\"\n\tDEFAULT_COUNTERS_FILE_NAME = \"counters.csv\"\n\tDEFAULT_HUMAN_READABLE_HISTOGRAMS_FILE_NAME = \"histograms_human_readable.csv\"\n\tDEFAULT_TOOLS_HISTOGRAMS_FILE_NAME = \"histograms_tools.csv\"\n\tDEFAULT_COMPACTIONS_STATS_FILE_NAME = \"compactions_stats.csv\"\n\tDEFAULT_COMPACTIONS_FILE_NAME = \"compactions.csv\"\n\tDEFAULT_FLUSHES_FILE_NAME = \"flushes.csv\"\n\tFILE_NOT_GENERATED_TEXT = \"File Not Generated\"\n", "DATA_UNAVAILABLE_TEXT = \"Data Unavailable\"\n\tUNKNOWN_VALUE_TEXT = \"UNKNOWN\"\n\tNO_INGEST_TEXT = \"No Ingest Info Available\"\n\tNO_STATS_TEXT = \"No Statistics\"\n\tNO_COUNTERS_DUMPS_TEXT = \"No Counters Dumps Available\"\n\tNO_FLUSHES_TEXT = \"No Flush Started Events\"\n\tNO_SEEKS_TEXT = \"No Seeks\"\n\tNO_WARNS_TEXT = \"No Warnings\"\n\tNO_READS_TEXT = \"No Reads\"\n\tNO_COMPACTIONS_TEXT = \"No Compactions\"\n", "NO_BLOCK_CACHE_STATS = \"No Block Cache Statistics\"\n\tNO_GROWTH_INFO_TEXT = \"No Growth Information Available\"\n\t# =====================================\n\t#           MISC UTILS\n\t# =====================================\n\tdef get_last_dict_entry(d):\n\t    assert d is None or isinstance(d, dict)\n\t    if not d:\n\t        return None\n\t    key = list(d.keys())[-1]\n", "    return {key: d[key]}\n\tdef get_first_dict_entry_components(d):\n\t    assert d is None or isinstance(d, dict)\n\t    if not d:\n\t        return None\n\t    key = list(d.keys())[0]\n\t    return key, d[key]\n\tdef get_last_dict_entry_components(d):\n\t    assert d is None or isinstance(d, dict)\n\t    if not d:\n", "        return None\n\t    key = list(d.keys())[-1]\n\t    return key, d[key]\n\tdef delete_dict_keys(in_dict, keys_to_delete):\n\t    \"\"\" Delete specific keys from an dictionary\"\"\"\n\t    for key in keys_to_delete:\n\t        if key in in_dict:\n\t            del in_dict[key]\n\tdef unify_dicts(dict1, dict2, favor_first):\n\t    # Avoid mutating the input dictionary\n", "    unified_dict = copy.deepcopy(dict1)\n\t    for key, value in dict2.items():\n\t        if key in unified_dict:\n\t            if not favor_first:\n\t                unified_dict[key] = dict2[key]\n\t        else:\n\t            unified_dict[key] = value\n\t    return unified_dict\n\tdef replace_key_keep_order(d, existing_key, new_key):\n\t    if existing_key not in d:\n", "        return\n\t    pos = list(d.keys()).index(existing_key)\n\t    value = d[existing_key]\n\t    items = list(d.items())\n\t    items.insert(pos, (new_key, value))\n\t    updated_d = dict(items)\n\t    del(updated_d[existing_key])\n\t    return updated_d\n\tdef insert_dict_entry_before_key(d, existing_key, new_key, new_value):\n\t    if existing_key not in d:\n", "        return\n\t    pos = list(d.keys()).index(existing_key)\n\t    value = d[existing_key]\n\t    items = list(d.items())\n\t    items.insert(pos, (new_key, value))\n\t    updated_d = dict(items)\n\t    return updated_d\n\tdef insert_dict_entry_after_key(d, existing_key, new_key, new_value):\n\t    if existing_key not in d:\n\t        return\n", "    pos = list(d.keys()).index(existing_key)\n\t    value = d[existing_key]\n\t    items = list(d.items())\n\t    items.insert(pos, (new_key, value))\n\t    updated_d = dict(items)\n\t    return updated_d\n\tdef find_dict_keys_matching_prefix(d, key_prefix):\n\t    assert isinstance(d, dict)\n\t    matching = list()\n\t    for key in d.keys():\n", "        if isinstance(key, str) and key.startswith(key_prefix):\n\t            matching.append(key)\n\t    return matching\n\tdef find_list_items_matching_prefix(lst, prefix):\n\t    assert isinstance(lst, list)\n\t    return [e for e in lst if isinstance(e, str) and e.startswith(prefix)]\n\tdef are_dicts_equal_and_in_same_keys_order(d1, d2):\n\t    if d1 != d2:\n\t        return False\n\t    return list(d1.keys()) == list(d2.keys())\n", "def unify_lists_preserve_order(l1, l2):\n\t    # Unifies the lists, removes duplicates, but maintains the order\n\t    unified_with_duplicates = l1 + l2\n\t    seen = set()\n\t    return [x for x in unified_with_duplicates\n\t            if not (x in seen or seen.add(x))]\n\tdef sort_dict_on_values(d):\n\t    return dict(sorted(d.items(), key=lambda x: x[1]))\n\tdef has_method(obj, method_name):\n\t    \"\"\" Checks if a method exists in an obj \"\"\"\n", "    return method_name in dir(obj)\n\tdef get_gmt_timestamp_us(time_str):\n\t    \"\"\"\n\t    Converts a time string in the log's format to a GMT Unix Timestamp.\n\t    The resolution is in seconds\n\t    Example: '2018/07/25-11:25:45.782710' will be converted into 1532517945\n\t    \"\"\"\n\t    hr_time = time_str + 'GMT'\n\t    dt = datetime.strptime(hr_time,  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\t    us = dt.microsecond\n", "    epoch_seconds = timegm(time.strptime(hr_time, \"%Y/%m/%d-%H:%M:%S.%f%Z\"))\n\t    return epoch_seconds * 10**6 + us\n\tdef get_time_relative_to(base_time, num_seconds, num_us=0):\n\t    base_dt = datetime.strptime(base_time + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\t    num_seconds = num_seconds + num_us / 10**6\n\t    dt = timedelta(seconds=num_seconds)\n\t    rel_time_dt = base_dt + dt\n\t    return rel_time_dt.strftime(\"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\tdef get_times_strs_diff(time_str1, time_str2):\n\t    \"\"\" Calculates the difference between 2 log time string \"\"\"\n", "    dt1 = datetime.strptime(time_str1 + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\t    dt2 = datetime.strptime(time_str2 + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\t    return dt2 - dt1\n\tdef get_times_strs_diff_seconds(time_str1, time_str2):\n\t    return get_times_strs_diff(time_str1, time_str2).total_seconds()\n\tdef compare_times_strs(time1_str, time2_str):\n\t    \"\"\" Compares 2 log time strings\n\t    Returns:\n\t        -1: time1_str < time2_str\n\t        0: time1_str == time2_str\n", "        1: time1_str > time2_str\n\t    \"\"\"\n\t    diff_total_seconds = get_times_strs_diff_seconds(time2_str, time1_str)\n\t    if diff_total_seconds < 0:\n\t        return -1\n\t    elif diff_total_seconds > 0:\n\t        return 1\n\t    else:\n\t        return 0\n\tdef convert_seconds_to_dd_hh_mm_ss(seconds):\n", "    seconds = int(seconds)\n\t    days = int(seconds / 86400)\n\t    return time.strftime(f'{days}d %Hh %Mm %Ss', time.gmtime(seconds))\n\t# =====================================\n\t#       LOGGING TYPES & UTILS\n\t# =====================================\n\t@dataclass\n\tclass ParsingContext:\n\t    file_path: str = None\n\t    parsing_done: bool = False\n", "    line_idx: int = 0\n\t    line: str = None\n\t    def parsing_starts(self, file_path):\n\t        self.file_path = file_path\n\t    def parsing_ends(self):\n\t        assert not self.parsing_done\n\t        self.parsing_done = True\n\t    def is_parsing_done(self):\n\t        return self.parsing_done\n\t    def update_line(self, line_idx, new_line=None):\n", "        self.line_idx = line_idx\n\t        if new_line:\n\t            self.line = new_line\n\t        else:\n\t            self.line = None\n\t    def increment_line(self, increment=1, new_line=None):\n\t        new_line_idx = self.line_idx + increment\n\t        self.update_line(new_line_idx, new_line)\n\tparsing_context = None\n\t@dataclass\n", "class ErrorContext:\n\t    file_path: Optional[str] = None\n\t    log_line_idx: Optional[int] = None\n\t    log_line: Optional[str] = None\n\t    def __str__(self):\n\t        file_path = self.file_path if self.file_path is not None else \"?\"\n\t        line_num = self.log_line_idx + 1 \\\n\t            if self.log_line_idx is not None else \"?\"\n\t        result_str = f\"[File:{file_path} (line#:{line_num})]\"\n\t        if self.log_line is not None:\n", "            result_str += f\"\\n{self.log_line}\"\n\t        return result_str\n\tdef get_error_context_from_entry(entry, file_path=None):\n\t    error_context = ErrorContext()\n\t    if not error_context:\n\t        error_context = ErrorContext()\n\t    if file_path:\n\t        error_context.file_path = file_path\n\t    error_context.log_line = entry.get_msg()\n\t    error_context.log_line_idx = entry.get_start_line_idx()\n", "    return error_context\n\tdef format_err_msg(msg, error_context=None, entry=None, file_path=None):\n\t    if entry:\n\t        error_context = get_error_context_from_entry(entry, file_path)\n\t    result_str = msg\n\t    if error_context is not None:\n\t        result_str += \" - \" + str(error_context)\n\t    return result_str\n\tdef get_line_num_from_entry(entry, rel_line_idx=None):\n\t    line_idx = entry.get_start_line_idx()\n", "    if rel_line_idx is not None:\n\t        line_idx += rel_line_idx\n\t    return line_idx + 1\n\tdef format_line_num_from_line_idx(line_idx):\n\t    return f\"[line# {line_idx + 1}]\"\n\tdef format_line_num_from_entry(entry, rel_line_idx=None):\n\t    line_idx = entry.get_start_line_idx()\n\t    if rel_line_idx is not None:\n\t        line_idx += rel_line_idx\n\t    return format_line_num_from_line_idx(line_idx)\n", "def format_lines_range_from_entries(start_entry, end_entry):\n\t    result = \"[lines#\"\n\t    result += f\"{start_entry.get_start_line_idx() + 1}\"\n\t    result += \"-\"\n\t    result += f\"{end_entry.get_end_line_idx() + 1}\"\n\t    result += \"]\"\n\t    return result\n\tdef format_lines_range_from_entries_idxs(log_entries, start_idx, end_idx):\n\t    return format_lines_range_from_entries(log_entries[start_idx],\n\t                                           log_entries[end_idx])\n", "def print_msg(msg, to_console=True, console_msg=None):\n\t    logging.info(msg)\n\t    if to_console:\n\t        if console_msg:\n\t            print(console_msg)\n\t        else:\n\t            print(msg)\n\t# =====================================\n\t#       EXCEPTIONS\n\t# =====================================\n", "class ParsingError(Exception):\n\t    def __init__(self, msg, error_context=None):\n\t        self.msg = msg\n\t        self.context = error_context\n\t    def __str__(self):\n\t        result_str = self.msg\n\t        if self.context is not None:\n\t            result_str += str(self.context)\n\t        return result_str\n\t    def set_context(self, error_context):\n", "        self.context = error_context\n\tclass ParsingAssertion(ParsingError):\n\t    def __init__(self, msg, error_context=None):\n\t        super().__init__(msg, error_context)\n\tclass LogFileNotFoundError(Exception):\n\t    def __init__(self, file_path):\n\t        self.msg = f\"Log file to parse ({file_path}) Not Found\"\n\tclass EmptyLogFile(Exception):\n\t    def __init__(self, file_path):\n\t        self.msg = f\"{file_path} Is Empty\"\n", "class InvalidLogFile(Exception):\n\t    def __init__(self, file_path):\n\t        self.msg = f\"{file_path} is not a valid log File\"\n\tclass WarningType(str, Enum):\n\t    WARN = \"WARN\"\n\t    ERROR = \"ERROR\"\n\t    FATAL = \"FATAL\"\n\tclass ConsoleOutputType(str, Enum):\n\t    SHORT = \"short\"\n\t    LONG = \"long\"\n", "class ProductName(str, Enum):\n\t    ROCKSDB = \"RocksDB\"\n\t    SPEEDB = \"Speedb\"\n\t    def __eq__(self, other):\n\t        return self.lower() == other.lower()\n\t# =====================================\n\t#       PARSING UTILITIES\n\t# =====================================\n\tdef parse_time_str(time_str, expect_valid_str=True):\n\t    try:\n", "        return datetime.strptime(time_str, '%Y/%m/%d-%H:%M:%S.%f')\n\t    except ValueError:\n\t        if expect_valid_str:\n\t            raise ParsingError(f\"Invalid time str ({time_str}\")\n\t        return None\n\tdef is_valid_time_str(time_str):\n\t    return parse_time_str(time_str, expect_valid_str=False) is not None\n\tNUM_BYTES_UNITS_STRS = [\"KB\", \"MB\", \"GB\", \"TB\"]\n\tNUM_UNITS_STRS = [\"K\", \"M\", \"G\"]\n\tdef __convert_human_readable_components(\n", "        num_bytes_without_unit_str, size_units_str, units_list, factor):\n\t    try:\n\t        num_bytes_without_unit_str = float(num_bytes_without_unit_str)\n\t    except ValueError:\n\t        raise ParsingError(\n\t            f\"Num bytes is not a nummer: {num_bytes_without_unit_str}\")\n\t    size_units_str = size_units_str.strip()\n\t    try:\n\t        unit_idx = units_list.index(size_units_str)\n\t        multiplier = factor ** (unit_idx + 1)\n", "    except ValueError:\n\t        if size_units_str != '':\n\t            raise ParsingAssertion(\n\t                f\"Unexpected size units ({size_units_str}\")\n\t        multiplier = 1\n\t    result = float(num_bytes_without_unit_str) * multiplier\n\t    return int(result)\n\tdef get_num_bytes_from_human_readable_components(num_bytes_without_unit_str,\n\t                                                 size_units_str):\n\t    return __convert_human_readable_components(\n", "        num_bytes_without_unit_str,\n\t        size_units_str,\n\t        NUM_BYTES_UNITS_STRS,\n\t        1024)\n\tdef get_num_bytes_from_human_readable_str(size_with_unit_str):\n\t    match = re.findall(f\"{regexes.NUM_BYTES_WITH_UNIT_ONLY}\",\n\t                       size_with_unit_str)\n\t    if not match:\n\t        raise ParsingError(f\"Invalid size with unit str ({size_with_unit_str}\")\n\t    size, size_unit = match[0]\n", "    return get_num_bytes_from_human_readable_components(size, size_unit)\n\tdef get_human_readable_num_bytes(size_in_bytes):\n\t    if size_in_bytes < 2 ** 10:\n\t        return str(size_in_bytes) + \" B\"\n\t    elif size_in_bytes < 2 ** 20:\n\t        size_units_str = \"KB\"\n\t        divider = 2 ** 10\n\t    elif size_in_bytes < 2 ** 30:\n\t        size_units_str = \"MB\"\n\t        divider = 2 ** 20\n", "    elif size_in_bytes < 2 ** 40:\n\t        size_units_str = \"GB\"\n\t        divider = 2 ** 30\n\t    else:\n\t        size_units_str = \"TB\"\n\t        divider = 2 ** 40\n\t    return f\"{float(size_in_bytes) / divider:.1f} {size_units_str}\"\n\tdef get_number_from_human_readable_components(num_bytes_without_unit_str,\n\t                                              size_units_str):\n\t    return __convert_human_readable_components(\n", "        num_bytes_without_unit_str,\n\t        size_units_str,\n\t        NUM_UNITS_STRS,\n\t        1000)\n\tdef get_number_from_human_readable_str(number_with_units_str):\n\t    match = re.findall(f\"{regexes.NUM_WITH_UNIT_ONLY}\", number_with_units_str)\n\t    if not match:\n\t        raise ParsingError(\n\t            f\"Invalid size with unit str ({number_with_units_str}\")\n\t    size, size_unit = match[0]\n", "    return get_number_from_human_readable_components(size, size_unit)\n\tdef get_human_readable_number(number):\n\t    assert number >= 0\n\t    if number < 10 ** 4:\n\t        return str(number)\n\t    elif number < 10 ** 7:\n\t        size_units_str = \"K\"\n\t        divider = 10**3\n\t    elif number < 10 ** 10:\n\t        size_units_str = \"M\"\n", "        divider = 10**6\n\t    else:\n\t        size_units_str = \"G\"\n\t        divider = 10**9\n\t    return f\"{float(number) / divider:.1f} {size_units_str}\"\n\tdef get_num_leading_spaces(line):\n\t    return len(line) - len(line.lstrip())\n\tdef remove_empty_lines_at_start(lines):\n\t    return [line for line in lines if line.strip()]\n\tdef try_find_cfs_in_lines(cfs_names, lines):\n", "    \"\"\" Try to find the first cf name in cfs_names that appears in lines\n\t    cf names are searched as \"[<cf-name>]\"\n\t    Returns either the cf-name(s) (if found) or None (if not)\n\t    \"\"\"\n\t    if isinstance(lines, list):\n\t        lines = \"\\n\".join(lines)\n\t    match = re.findall(regexes.CF_NAME_OLD, lines, re.MULTILINE)\n\t    if not match:\n\t        return None\n\t    potential_cfs_names_set = set(match)\n", "    cfs_names_set = set(cfs_names)\n\t    found_cfs_names = list(potential_cfs_names_set.intersection(cfs_names_set))\n\t    if not found_cfs_names:\n\t        return None\n\t    if len(found_cfs_names) == 1:\n\t        return found_cfs_names[0]\n\t    else:\n\t        return found_cfs_names\n\t# =====================================\n\t#           FILE PATHS UTILS\n", "# =====================================\n\tdef get_file_path(file_folder_name, file_basename):\n\t    return pathlib.Path(f\"{file_folder_name}/{file_basename}\").resolve()\n\tdef get_json_file_path(output_folder, json_file_name):\n\t    return get_file_path(output_folder, json_file_name)\n\tdef get_log_file_path(output_folder):\n\t    return get_file_path(output_folder, DEFAULT_LOG_FILE_NAME)\n\tdef get_counters_csv_file_path(output_folder):\n\t    return get_file_path(output_folder, DEFAULT_COUNTERS_FILE_NAME)\n\tdef get_human_readable_histograms_csv_file_path(output_folder):\n", "    return get_file_path(output_folder,\n\t                         DEFAULT_HUMAN_READABLE_HISTOGRAMS_FILE_NAME)\n\tdef get_tools_histograms_csv_file_path(output_folder):\n\t    return get_file_path(output_folder,\n\t                         DEFAULT_TOOLS_HISTOGRAMS_FILE_NAME)\n\tdef get_compactions_stats_csv_file_path(output_folder):\n\t    return get_file_path(output_folder,\n\t                         DEFAULT_COMPACTIONS_STATS_FILE_NAME)\n\tdef get_compactions_csv_file_path(output_folder):\n\t    return get_file_path(output_folder,\n", "                         DEFAULT_COMPACTIONS_FILE_NAME)\n\tdef get_flushes_csv_file_path(output_folder):\n\t    return get_file_path(output_folder, DEFAULT_FLUSHES_FILE_NAME)\n"]}
{"filename": "compactions.py", "chunked_list": ["from __future__ import annotations\n\timport logging\n\timport re\n\tfrom dataclasses import asdict, dataclass\n\tfrom enum import auto\n\timport events\n\timport regexes\n\timport utils\n\tclass CompactionState:\n\t    WAITING_START = auto()\n", "    STARTED = auto()\n\t    FINISHED = auto()\n\t@dataclass\n\tclass PreFinishStatsInfo:\n\t    cf_name: str\n\t    read_rate_mbps: float = 0.0\n\t    write_rate_mbps: float = 0.0\n\t    read_write_amplify: float = 0.0\n\t    write_amplify: float = 0.0\n\t    records_in: int = 0\n", "    records_dropped: int = 0\n\t    def as_dict(self):\n\t        return asdict(self)\n\t@dataclass\n\tclass CompactionJobInfo:\n\t    job_id: int\n\t    cf_name: str = None\n\t    start_event: events.CompactionStartedEvent = None\n\t    finish_event: events.CompactionFinishedEvent = None\n\t    pre_finish_info: PreFinishStatsInfo = None\n", "    def __str__(self):\n\t        return f\"Compaction Info: \" \\\n\t               f\"job_id:{self.job_id}, \" \\\n\t               f\"cf_name:{self.cf_name}\" \\\n\t               f\"start_time:{self.get_start_time()}\" \\\n\t               f\"finish_time:{self.get_finish_time()}\"\n\t    def get_state(self):\n\t        if self.finish_event is not None:\n\t            assert self.start_event is not None\n\t            return CompactionState.FINISHED\n", "        elif self.start_event is not None:\n\t            return CompactionState.STARTED\n\t        else:\n\t            return CompactionState.WAITING_START\n\t    def has_finished(self):\n\t        return self.get_state() == CompactionState.FINISHED\n\t    def get_start_time(self):\n\t        return self.start_event.get_log_time() if self.start_event else \"\"\n\t    def get_finish_time(self):\n\t        return self.finish_event.get_log_time() if self.finish_event else \"\"\n", "    def set_start_event(self, start_event):\n\t        if self.get_state() != CompactionState.WAITING_START:\n\t            raise utils.ParsingError(f\"Unexpected Compaction's state to \"\n\t                                     f\"set start event ({start_event}. {self}\")\n\t        self.start_event = start_event\n\t    def set_finish_event(self, finish_event):\n\t        if self.get_state() != CompactionState.STARTED:\n\t            raise utils.ParsingError(f\"Unexpected Compaction's state to set \"\n\t                                     f\"finish event ({finish_event}. {self}\")\n\t        if utils.compare_times_strs(self.get_start_time(),\n", "                                    finish_event.get_log_time()) > 0:\n\t            raise utils.ParsingError(\n\t                f\"finish event's time ({finish_event} before \"\n\t                f\"start event time.\\n{finish_event}\")\n\t        self.finish_event = finish_event\n\tclass CompactionsMonitor:\n\t    def __init__(self):\n\t        self.jobs = dict()\n\t        self.pre_finish_stats_infos = list()\n\t    def consider_entry(self, entry):\n", "        try:\n\t            is_finish_stats_line, cf_name = \\\n\t                self.try_parse_as_pre_finish_stats_line(entry)\n\t            if is_finish_stats_line:\n\t                return True, cf_name\n\t        except utils.ParsingError as e:\n\t            logging.WARN(f\"Error adding entry to compaction jobs.\\n\"\n\t                         f\"error: {e}\\n\"\n\t                         f\"entry:{entry}\")\n\t        return False, None\n", "    def add_job_if_applicable(self, job_id, cf_name=None, entry=None):\n\t        entry_cf_name = entry.get_cf_name() if entry else cf_name\n\t        entry_job_id = entry.get_job_id() if entry else job_id\n\t        if cf_name is not None and entry_cf_name != cf_name or \\\n\t                entry_job_id != job_id:\n\t            raise utils.ParsingError(\n\t                f\"entry's cf name ({entry_cf_name}) or entry_job_id \"\n\t                f\"({entry_job_id}) != what was parsed here.\\nentry: {entry}\")\n\t        if job_id not in self.jobs:\n\t            self.jobs[job_id] = CompactionJobInfo(job_id, cf_name)\n", "        else:\n\t            if self.jobs[job_id].cf_name != cf_name:\n\t                raise utils.ParsingError(\n\t                    f\"new cf_name ({cf_name}) != existing (\"\n\t                    f\"{self.jobs[job_id].cf_name}) for same job.\")\n\t    def try_parse_as_score_entry(self, entry):\n\t        match = \\\n\t            re.findall(regexes.COMPACTION_BEFORE_SCORE_LINE, entry.get_msg())\n\t        if not match:\n\t            return False\n", "        assert len(match) == 1 and len(match[0]) == 4\n\t        cf_name, job_id, _, before_score = match[0]\n\t        job_id = int(job_id)\n\t        before_score = float(before_score)\n\t        self.add_job_if_applicable(job_id, cf_name, entry)\n\t        self.jobs[job_id].before_score = before_score\n\t        return True\n\t    def try_parse_as_pre_finish_stats_line(self, entry):\n\t        match = re.findall(regexes.COMPACTION_JOB_FINISH_STATS_LINE,\n\t                           entry.get_msg())\n", "        if not match:\n\t            return False, None\n\t        assert len(match) == 1 and len(match[0]) == 7\n\t        info = PreFinishStatsInfo(*match[0])\n\t        info.read_rate_mbps = float(info.read_rate_mbps)\n\t        info.write_rate_mbps = float(info.write_rate_mbps)\n\t        info.read_write_amplify = float(info.read_write_amplify)\n\t        info.write_amplify = float(info.write_amplify)\n\t        info.records_in = int(info.records_in)\n\t        info.records_dropped = int(info.records_dropped)\n", "        self.pre_finish_stats_infos.append(info)\n\t        return True, info.cf_name\n\t    def new_event(self, event):\n\t        event_type = event.get_type()\n\t        if event_type == events.EventType.COMPACTION_STARTED:\n\t            self.compaction_started(event)\n\t        elif event_type == events.EventType.COMPACTION_FINISHED:\n\t            self.compaction_finished(event)\n\t    def compaction_started(self, event):\n\t        assert isinstance(event, events.CompactionStartedEvent)\n", "        # 2023/01/04-08:55:00.743718 27420 EVENT_LOG_v1\n\t        # {\"time_micros\": 1672822500743711, \"job\": 9,\n\t        # \"event\": \"compaction_started\",\n\t        # \"compaction_reason\": \"LevelL0FilesNum\",\n\t        # \"files_L0\": [17250, 17247, 17243, 17239], \"score\": 1,\n\t        # \"input_data_size\": 251316602}\n\t        #\n\t        job_id = event.get_job_id()\n\t        self.add_job_if_applicable(job_id, event.get_cf_name())\n\t        self.jobs[job_id].set_start_event(event)\n", "    def compaction_finished(self, event):\n\t        # 2023/01/04-08:55:00.746783 27413\n\t        # (Original Log Time 2023/01/04-08:55:00.746653) EVENT_LOG_v1\n\t        # {\"time_micros\": 1672822500746645, \"job\": 4,\n\t        # \"event\": \"compaction_finished\",\n\t        # \"compaction_time_micros\": 971568,\n\t        # \"compaction_time_cpu_micros\": 935180,\n\t        # \"output_level\": 1, \"num_output_files\": 7,\n\t        # \"total_output_size\": 437263613,\n\t        # \"num_input_records\": 424286, \"num_output_records\": 423497,\n", "        # \"num_subcompactions\": 1, \"output_compression\": \"NoCompression\",\n\t        # \"num_single_delete_mismatches\": 0,\n\t        # \"num_single_delete_fallthrough\": 0,\n\t        # \"lsm_state\": [4, 7, 45, 427, 822, 0, 0]}\n\t        #\n\t        assert isinstance(event, events.CompactionFinishedEvent)\n\t        job_id = event.get_job_id()\n\t        if job_id not in self.jobs:\n\t            logging.info(\n\t                f\"Compaction finished event for job for which there is no \"\n", "                f\"recorded compaction started. job-id:{job_id}\\n\"\n\t                f\"event:{event}\")\n\t            return\n\t        job = self.jobs[job_id]\n\t        job.set_finish_event(event)\n\t        # Try to match the pre-finish info based on the number of\n\t        # input records and cf name\n\t        num_input_records = event.get_num_input_records()\n\t        for idx, info in enumerate(self.pre_finish_stats_infos):\n\t            if num_input_records == info.records_in:\n", "                if job.cf_name == info.cf_name:\n\t                    job.pre_finish_info = info\n\t                    self.pre_finish_stats_infos.pop(idx)\n\t                    break\n\t                else:\n\t                    logging.info(\n\t                        f\"# input records match, but different cf-s.\\n\"\n\t                        f\"pre-finish info:{info}\\n\"\n\t                        f\"job:{asdict(job)}\\n\"\n\t                        f\"event:{event}\")\n", "    def get_finished_jobs(self):\n\t        finished_jobs = dict()\n\t        for job_id, job_info in self.jobs.items():\n\t            if job_info.has_finished():\n\t                finished_jobs[job_id] = job_info\n\t        return finished_jobs\n\t    def get_cf_finished_jobs(self, cf_name):\n\t        finished_cf_jobs = dict()\n\t        for job_id, job_info in self.jobs.items():\n\t            if job_info.has_finished() and job_info.cf_name == cf_name:\n", "                finished_cf_jobs[job_id] = job_info\n\t        return finished_cf_jobs\n"]}
{"filename": "csv_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport copy\n\timport csv\n\timport io\n\timport logging\n\tfrom dataclasses import dataclass\n\timport utils\n\tfrom events import FlowType, EventField\n", "def get_counters_csv(counter_and_histograms_mngr):\n\t    f = io.StringIO()\n\t    writer = csv.writer(f)\n\t    mngr = counter_and_histograms_mngr\n\t    # Get all counters for which at least one entry is not 0 (=> there is at\n\t    # least one value that should be included for them in the CSV)\n\t    all_applicable_entries = mngr.get_counters_entries_not_all_zeroes()\n\t    if not all_applicable_entries:\n\t        logging.info(\"No counters with non-zero values => NO CSV\")\n\t        return None\n", "    counters_names = list(all_applicable_entries.keys())\n\t    times = mngr.get_counters_times()\n\t    # Support counter entries with missing entries for some time point\n\t    # Maintain an index per counter that advances only when the counter has\n\t    # a value per csv row (one row per time point)\n\t    counters_idx = {name: 0 for name in counters_names}\n\t    # csv header line (counter names)\n\t    writer.writerow([\"Time\"] + counters_names)\n\t    # Write one line per time:\n\t    for time_idx, time in enumerate(times):\n", "        csv_line = list()\n\t        csv_line.append(time)\n\t        for counter_name in counters_names:\n\t            counter_entries = all_applicable_entries[counter_name]\n\t            value = 0\n\t            if counters_idx[counter_name] < len(counter_entries):\n\t                counter_entry_time =\\\n\t                    counter_entries[counters_idx[counter_name]][\"time\"]\n\t                time_diff = \\\n\t                    utils.compare_times_strs(counter_entry_time, time)\n", "                assert time_diff >= 0\n\t                if time_diff == 0:\n\t                    value =\\\n\t                        counter_entries[counters_idx[counter_name]][\"value\"]\n\t                    counters_idx[counter_name] += 1\n\t            csv_line.append(value)\n\t        writer.writerow(csv_line)\n\t    return f.getvalue()\n\tdef get_human_readable_histogram_csv(counter_and_histograms_mngr):\n\t    f = io.StringIO()\n", "    writer = csv.writer(f)\n\t    mngr = counter_and_histograms_mngr\n\t    # Get all histograms for which at least one entry is not 0 (=> there is at\n\t    # least one value that should be included for them in the CSV)\n\t    all_applicable_entries = mngr.get_histogram_entries_not_all_zeroes()\n\t    if not all_applicable_entries:\n\t        logging.info(\"No Histograms with non-zero values => NO CSV\")\n\t        return None\n\t    counters_names = list(all_applicable_entries.keys())\n\t    times = mngr.get_histogram_counters_times()\n", "    # Support histogram entries with missing entries for some time point\n\t    # Maintain an index per histogram that advances only when the histogram has\n\t    # a value per csv row (one row per time point)\n\t    histograms_idx = {name: 0 for name in counters_names}\n\t    # csv header lines (counter names)\n\t    header_line1 = [\"\"]\n\t    header_line2 = [\"\"]\n\t    counter_histogram_columns =\\\n\t        list(all_applicable_entries[counters_names[0]][0][\"values\"].keys())\n\t    counter_histogram_columns.remove(\"Average\")\n", "    counter_histogram_columns.remove(\"Interval Count\")\n\t    counter_histogram_columns.remove(\"Interval Sum\")\n\t    num_counter_columns = len(counter_histogram_columns)\n\t    for counter_name in counters_names:\n\t        name_columns = [\".\" for i in range(num_counter_columns-1)]\n\t        name_columns.insert(0, counter_name)\n\t        # name_columns[int(num_counter_columns/2)] = counter_name\n\t        header_line1.extend(name_columns)\n\t        header_line2.extend(counter_histogram_columns)\n\t    writer.writerow(header_line1)\n", "    writer.writerow(header_line2)\n\t    # Write one line per time:\n\t    zero_values = [0 for i in range(num_counter_columns)]\n\t    for time_idx, time in enumerate(times):\n\t        csv_line = list()\n\t        csv_line.append(time)\n\t        for counter_name in counters_names:\n\t            histogram_entries = all_applicable_entries[counter_name]\n\t            values = zero_values\n\t            idx = histograms_idx[counter_name]\n", "            if idx < len(histogram_entries):\n\t                counter_entry_time = histogram_entries[idx][\"time\"]\n\t                time_diff = \\\n\t                    utils.compare_times_strs(counter_entry_time, time)\n\t                assert time_diff >= 0\n\t                if time_diff == 0:\n\t                    values = list(histogram_entries[idx][\"values\"].values())\n\t                    histograms_idx[counter_name] += 1\n\t            csv_line.extend(values)\n\t        writer.writerow(csv_line)\n", "    return f.getvalue()\n\tdef get_tools_histogram_csv(counter_and_histograms_mngr):\n\t    f = io.StringIO()\n\t    writer = csv.writer(f)\n\t    mngr = counter_and_histograms_mngr\n\t    # Get all histograms for which at least one entry is not 0 (=> there is at\n\t    # least one value that should be included for them in the CSV)\n\t    all_applicable_entries = mngr.get_histogram_entries_not_all_zeroes()\n\t    if not all_applicable_entries:\n\t        logging.info(\"No Histograms with non-zero values => NO CSV\")\n", "        return None\n\t    counters_names = list(all_applicable_entries.keys())\n\t    times = mngr.get_histogram_counters_times()\n\t    # Support histogram entries with missing entries for some time point\n\t    # Maintain an index per histogram that advances only when the histogram has\n\t    # a value per csv row (one row per time point)\n\t    histograms_idx = {name: 0 for name in counters_names}\n\t    # csv header lines (counter names)\n\t    header_line = [\"Name\", \"Time\"]\n\t    counter_histogram_columns =\\\n", "        list(all_applicable_entries[counters_names[0]][0][\"values\"].keys())\n\t    header_line.extend(counter_histogram_columns)\n\t    num_counter_columns = len(counter_histogram_columns)\n\t    writer.writerow(header_line)\n\t    # Write one line per time:\n\t    zero_values = [0 for i in range(num_counter_columns)]\n\t    for counter_name in counters_names:\n\t        for time_idx, time in enumerate(times):\n\t            csv_line = [counter_name, time]\n\t            histogram_entries = all_applicable_entries[counter_name]\n", "            values = zero_values\n\t            idx = histograms_idx[counter_name]\n\t            if idx < len(histogram_entries):\n\t                counter_entry_time = histogram_entries[idx][\"time\"]\n\t                time_diff = \\\n\t                    utils.compare_times_strs(counter_entry_time, time)\n\t                assert time_diff >= 0\n\t                if time_diff == 0:\n\t                    values = list(histogram_entries[idx][\"values\"].values())\n\t                    histograms_idx[counter_name] += 1\n", "                csv_line.extend(values)\n\t            writer.writerow(csv_line)\n\t    return f.getvalue()\n\tdef get_compaction_stats_csv(compaction_stats_mngr):\n\t    f = io.StringIO()\n\t    writer = csv.writer(f)\n\t    entries = compaction_stats_mngr.get_level_entries()\n\t    if not entries:\n\t        logging.info(\"No Compaction Stats => NO CSV\")\n\t        return None\n", "    temp = list(list(entries.values())[0].values())[0]\n\t    columns_names = list(list(temp.values())[0].keys())\n\t    header_line = [\"Time\", \"Column Family\", \"Level\"] + columns_names\n\t    writer.writerow(header_line)\n\t    for time, time_entry in entries.items():\n\t        for cf_name, cf_entry in time_entry.items():\n\t            for level, level_values in cf_entry.items():\n\t                row = [time, cf_name, level]\n\t                row += list(level_values.values())\n\t                writer.writerow(row)\n", "    return f.getvalue()\n\tdef get_flow_events_csv(cfs_names, events_mngr, flow_type):\n\t    f = io.StringIO()\n\t    writer = csv.writer(f)\n\t    immutable_events = events_mngr.get_all_flow_events(flow_type, cfs_names)\n\t    if not immutable_events:\n\t        return None\n\t    # Going to modify the events so make a modifiable copy first\n\t    events = copy.deepcopy(immutable_events)\n\t    first_event = True\n", "    for event_pair in events:\n\t        start_event = event_pair[0]\n\t        finish_event = event_pair[1]\n\t        start_event_data = start_event.get_event_data_dict()\n\t        cf_name = start_event.get_cf_name()\n\t        event_start_time = start_event.get_log_time()\n\t        if not finish_event:\n\t            event_finish_time = \"UNKNOWN\"\n\t            event_data = start_event_data\n\t        else:\n", "            event_finish_time = finish_event.get_log_time()\n\t            finish_event_data = finish_event.get_event_data_dict()\n\t            event_data = utils.unify_dicts(\n\t                start_event_data, finish_event_data, favor_first=True)\n\t        fields_to_del = [EventField.CF_NAME,\n\t                         EventField.TIME_MICROS,\n\t                         EventField.EVENT_TYPE]\n\t        utils.delete_dict_keys(event_data, fields_to_del)\n\t        if first_event:\n\t            first_event = False\n", "            event_columns_names = list(list(event_data.keys()))\n\t            header_line = [\"Start Time\", \"Finish Time\", \"Column Family\"] + \\\n\t                event_columns_names\n\t            writer.writerow(header_line)\n\t        row = [event_start_time, event_finish_time, cf_name]\n\t        row += list(event_data.values())\n\t        writer.writerow(row)\n\t    return f.getvalue()\n\t@dataclass\n\tclass CompactionsCsvInputFilesInfo:\n", "    updated_columns_names: list = None\n\t    first_column_idx: int = None\n\t    first_level: int = None\n\t    second_level: int = None\n\tdef process_compactions_csv_header(columns_names):\n\t    # Assume that, in general, compactions potentially have 2 \"files_\" columns\n\t    # (They may have one, and, maybe more than 2)\n\t    # Name them:\n\t    # 1. The first: \"Input Level Files\"\n\t    # 2. The second: \"Input Files from Output Level\"\n", "    prefix = \"files_L\"\n\t    prefix_len = len(prefix)\n\t    updated_columns_names = copy.deepcopy(columns_names)\n\t    input_files_columns = \\\n\t        utils.find_list_items_matching_prefix(updated_columns_names, prefix)\n\t    if not input_files_columns:\n\t        return None\n\t    if len(input_files_columns) > 2:\n\t        logging.warning(\n\t            f\"Compactions have more than 2 'files_' columns. Including only \"\n", "            f\"the first 2. columns_names:{columns_names}\")\n\t        for to_remove in input_files_columns[2:]:\n\t            updated_columns_names.remove(to_remove)\n\t        input_files_columns = input_files_columns[:2]\n\t    def extract_level(column_idx):\n\t        level_str = columns_names[column_idx][prefix_len:]\n\t        try:\n\t            return int(level_str)\n\t        except ValueError:\n\t            logging.warning(f\"Unexpected column name (\"\n", "                            f\"{columns_names[column_idx]}\")\n\t            return None\n\t    first_column_idx = updated_columns_names.index(input_files_columns[0])\n\t    first_level = extract_level(first_column_idx)\n\t    if first_level is None:\n\t        return None\n\t    updated_columns_names[first_column_idx] = \"Input Level Files\"\n\t    second_level = None\n\t    if len(input_files_columns) > 1:\n\t        second_column_idx = updated_columns_names.index(input_files_columns[1])\n", "        if second_column_idx != first_column_idx+1:\n\t            # Currently, support only consecutive columns\n\t            logging.warning(\n\t                f\"non-consecutive file_<Level> columns ({columns_names})\")\n\t            return None\n\t        second_level = extract_level(second_column_idx)\n\t        if not second_level:\n\t            return None\n\t        updated_columns_names[second_column_idx] = \\\n\t            \"Input Files from Output Level\"\n", "    else:\n\t        updated_columns_names.insert(first_column_idx + 1,\n\t                                     \"Input Files from Output Level\")\n\t    return CompactionsCsvInputFilesInfo(\n\t        updated_columns_names=updated_columns_names,\n\t        first_column_idx=first_column_idx,\n\t        first_level=first_level,\n\t        second_level=second_level\n\t    )\n\tdef get_compactions_csv(compactions_monitor):\n", "    # return get_flow_events_csv(events_mngr, FlowType.COMPACTION)\n\t    f = io.StringIO()\n\t    writer = csv.writer(f)\n\t    jobs = compactions_monitor.get_finished_jobs()\n\t    if not jobs:\n\t        return None\n\t    updated_header_columns_info = None\n\t    for job_id, job_info in jobs.items():\n\t        # Skipping incomplete jobs\n\t        if not job_info.has_finished():\n", "            logging.info(\"Compaction job hasn't finished, Not including in \"\n\t                         \"csv (skipping).\\n{job_info}\")\n\t            continue\n\t        start_event = job_info.start_event\n\t        finish_event = job_info.finish_event\n\t        job_info_dict = {}\n\t        if job_info.pre_finish_info:\n\t            job_info_dict = job_info.pre_finish_info.as_dict()\n\t        job_info_dict = utils.unify_dicts(job_info_dict,\n\t                                          start_event.get_event_data_dict(),\n", "                                          favor_first=True)\n\t        job_info_dict = \\\n\t            utils.unify_dicts(job_info_dict,\n\t                              finish_event.get_event_data_dict(),\n\t                              favor_first=True)\n\t        fields_to_del = [EventField.CF_NAME,\n\t                         EventField.TIME_MICROS,\n\t                         EventField.EVENT_TYPE,\n\t                         EventField.RECORDS_IN,\n\t                         EventField.RECORDS_DROPPED]\n", "        utils.delete_dict_keys(job_info_dict, fields_to_del)\n\t        columns_names = list(list(job_info_dict.keys()))\n\t        if updated_header_columns_info is None:\n\t            updated_header_columns_info = \\\n\t                process_compactions_csv_header(columns_names)\n\t            if updated_header_columns_info is None:\n\t                logging.warning(\"Failed processing CSV's header. Aborting\")\n\t                return None\n\t            curr_updated_columns_info = updated_header_columns_info\n\t            header_line = [\"Start Time\", \"Finish Time\", \"Column Family\"] + \\\n", "                updated_header_columns_info.updated_columns_names\n\t            writer.writerow(header_line)\n\t        else:\n\t            curr_updated_columns_info = \\\n\t                process_compactions_csv_header(columns_names)\n\t            if updated_header_columns_info.first_column_idx != \\\n\t                    curr_updated_columns_info.first_column_idx:\n\t                logging.warning(\n\t                    f\"Mismatching compaction job fields. \"\n\t                    f\"Skipping entry:{job_info}\")\n", "                continue\n\t        job_values = list(job_info_dict.values())\n\t        first_idx = curr_updated_columns_info.first_column_idx\n\t        first_level_str = f\"Level{curr_updated_columns_info.first_level}: \"\n\t        job_values[first_idx] = first_level_str + str(job_values[first_idx])\n\t        if curr_updated_columns_info.second_level is not None:\n\t            second_level_str = \\\n\t                f\"Level{curr_updated_columns_info.second_level}: \"\n\t            job_values[first_idx+1] = \\\n\t                second_level_str + str(job_values[first_idx+1])\n", "        else:\n\t            job_values.insert(curr_updated_columns_info.first_column_idx+1, \"\")\n\t        row = [job_info.get_start_time(),\n\t               job_info.get_finish_time(),\n\t               job_info.cf_name]\n\t        row += job_values\n\t        writer.writerow(row)\n\t    if updated_header_columns_info is None:\n\t        return None\n\t    return f.getvalue()\n", "def get_flushes_csv(cfs_names, events_mngr):\n\t    return get_flow_events_csv(cfs_names, events_mngr, FlowType.FLUSH)\n\tdef generate_counters_csv(mngr, output_folder, report_to_console):\n\t    counters_csv = get_counters_csv(mngr)\n\t    if counters_csv:\n\t        counters_csv_path = \\\n\t            utils.get_counters_csv_file_path(output_folder)\n\t        with open(counters_csv_path, \"w\") as f:\n\t            f.write(counters_csv)\n\t        msg_start = \"Counters CSV Is in \"\n", "        utils.print_msg(\n\t            f\"{msg_start}{counters_csv_path}\", report_to_console,\n\t            f\"{msg_start}{counters_csv_path.as_uri()}\")\n\t        return counters_csv_path\n\t    else:\n\t        utils.print_msg(\"No Counters to report\", report_to_console)\n\t        return None\n\tdef generate_human_readable_histograms_csv(mngr, output_folder,\n\t                                           report_to_console):\n\t    histograms_csv = \\\n", "        get_human_readable_histogram_csv(mngr)\n\t    if not histograms_csv:\n\t        utils.print_msg(\"No Counters Histograms to report\", report_to_console)\n\t        return None\n\t    histograms_csv_file_name = \\\n\t        utils. \\\n\t        get_human_readable_histograms_csv_file_path(output_folder)\n\t    with open(histograms_csv_file_name, \"w\") as f:\n\t        f.write(histograms_csv)\n\t    msg_start = \"Human Readable Counters Histograms CSV Is in \"\n", "    utils.print_msg(\n\t        f\"{msg_start}{histograms_csv_file_name}\", report_to_console,\n\t        f\"{msg_start}{histograms_csv_file_name.as_uri()}\")\n\t    return histograms_csv_file_name\n\tdef generate_tools_histograms_csv(mngr, output_folder, report_to_console):\n\t    histograms_csv = get_tools_histogram_csv(mngr)\n\t    if not histograms_csv:\n\t        logging.info(\"No Counters Histograms to report\")\n\t        return None\n\t    histograms_csv_file_name = \\\n", "        utils.get_tools_histograms_csv_file_path(output_folder)\n\t    with open(histograms_csv_file_name, \"w\") as f:\n\t        f.write(histograms_csv)\n\t    logging.info(f\"Tools Counters Histograms CSV Is in\"\n\t                 f\" {histograms_csv_file_name}\")\n\t    return histograms_csv_file_name\n\tdef generate_histograms_csv(mngr, output_folder, report_to_console):\n\t    human_readable_csv_file_path = \\\n\t        generate_human_readable_histograms_csv(\n\t            mngr, output_folder, report_to_console)\n", "    if human_readable_csv_file_path is None:\n\t        return None, None\n\t    tools_csv_file_path = generate_tools_histograms_csv(\n\t        mngr, output_folder, report_to_console)\n\t    return human_readable_csv_file_path, tools_csv_file_path\n\tdef generate_compactions_stats_csv(compaction_stats_mngr, output_folder,\n\t                                   report_to_console):\n\t    compaction_stats_csv = get_compaction_stats_csv(compaction_stats_mngr)\n\t    if compaction_stats_csv is None:\n\t        utils.print_msg(\"No Compaction Stats to report\", report_to_console)\n", "        return None\n\t    compactions_stats_csv_path = \\\n\t        utils.get_compactions_stats_csv_file_path(output_folder)\n\t    with open(compactions_stats_csv_path, \"w\") as f:\n\t        f.write(compaction_stats_csv)\n\t    msg_start = \"Compactions Stats CSV Is in \"\n\t    utils.print_msg(\n\t        f\"{msg_start}{compactions_stats_csv_path}\", report_to_console,\n\t        f\"{msg_start}{compactions_stats_csv_path.as_uri()}\")\n\t    return compactions_stats_csv_path\n", "def generate_compactions_csv(\n\t        compactions_monitor, output_folder, report_to_console):\n\t    compaction_csv = get_compactions_csv(compactions_monitor)\n\t    if compaction_csv is None:\n\t        utils.print_msg(\"No Compactions to report\", report_to_console)\n\t        return None\n\t    compactions_csv_path = \\\n\t        utils.get_compactions_csv_file_path(output_folder)\n\t    with open(compactions_csv_path, \"w\") as f:\n\t        f.write(compaction_csv)\n", "    msg_start = \"Compactions CSV Is in \"\n\t    utils.print_msg(\n\t        f\"{msg_start}{compactions_csv_path}\", report_to_console,\n\t        f\"{msg_start}{compactions_csv_path.as_uri()}\")\n\t    return compactions_csv_path\n\tdef generate_flushes_csv(\n\t        cfs_names, events_mngr, output_folder, report_to_console):\n\t    flushes_csv = get_flushes_csv(cfs_names, events_mngr)\n\t    if flushes_csv is None:\n\t        utils.print_msg(\"No Flushes to report\", report_to_console)\n", "        return None\n\t    flushes_csv_path = utils.get_flushes_csv_file_path(output_folder)\n\t    with open(flushes_csv_path, \"w\") as f:\n\t        f.write(flushes_csv)\n\t    msg_start = \"Flushes CSV Is in \"\n\t    utils.print_msg(\n\t        f\"{msg_start}{flushes_csv_path}\", report_to_console,\n\t        f\"{msg_start}{flushes_csv_path.as_uri()}\")\n\t    return flushes_csv_path\n"]}
{"filename": "regexes.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\t#\n\t# BASIC TYPES AND CONSTRUCTS\n\t#\n\tWS = r'\\s*'\n\tINT = r'[\\d]+'\n\tINT_C = r\"([\\d]+)\"\n\tFLOAT = r'[-+]?(?:\\d+(?:[.,]\\d*)?|[.,]\\d+)(?:[eE][-+]?\\d+)?'\n", "FLOAT_C = fr'({FLOAT})'\n\tNUM_UNIT = r'(K|M|G)'\n\tBYTES_UNIT = r'(KB|MB|GB|TB)'\n\tNUM_WITH_UNIT = fr'{FLOAT_C}\\s*{NUM_UNIT}?\\s*'\n\tNUM_WITH_UNIT_ONLY = fr\"{NUM_WITH_UNIT}\\Z\"\n\tNUM_BYTES_WITH_UNIT = fr'{FLOAT_C}\\s*{BYTES_UNIT}?\\s*'\n\tNUM_BYTES_WITH_UNIT_ONLY = fr'{NUM_BYTES_WITH_UNIT}\\Z'\n\tCF_NAME_OLD = r'\\[(?P<cf>[\\w\\]]*)\\]'\n\tCF_NAME = r'\\[(?P<cf>.*)\\]'\n\tCF_ID = fr'\\(ID\\s+(?P<cf_id>{INT})\\)'\n", "JOB_ID = r\"\\[JOB (?P<job_id>[\\d+]+)\\]\"\n\tPOINTER_NC = r'0x[\\dA-Fa-f]+'\n\tPOINTER = fr'({POINTER_NC})'\n\tSANITIZED_POINTER = fr\"Pointer \\((?P<ptr>{POINTER_NC})\\)\"\n\t#\n\t# LOG ENTRY PARTS REGEXES\n\t#\n\tEMPTY_LINE = r'^\\s*$'\n\tTIMESTAMP = r'\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}'\n\tORIG_TIME = fr\"\\(Original Log Time ({TIMESTAMP})\\)\"\n", "# Example: [/flush_job.cc:858]\n\tCODE_POS = r\"\\[\\/?.*?\\.[\\w:]+:\\d+\\]\"\n\tSTART_LINE_WITH_WARN_PARTS = \\\n\t    fr\"({TIMESTAMP}) (\\w+)\\s*(?:{ORIG_TIME})?\\s*\" \\\n\t    fr\"\\[(WARN|ERROR|FATAL)\\]\\s*({CODE_POS})?(.*)\"\n\tSTART_LINE_PARTS = \\\n\t    fr\"({TIMESTAMP}) (\\w+)\\s*\" \\\n\t    fr\"(?:{ORIG_TIME})?\\s*({CODE_POS})?(.*)\"\n\t# A log entry that has the cf-name followed by the job id\n\t# Example [column_family_name_000001] [JOB 31]\n", "CF_WITH_JOB_ID = fr\"{CF_NAME_OLD}\\s*{JOB_ID}\"\n\t#\n\t# METADATA REGEXES\n\tDB_SESSION_ID = r\"DB Session ID:\\s*([0-9A-Z]+)\"\n\t# <product name> version: <version number>\n\tPRODUCT_AND_VERSION = r\"(\\S+) version: ([0-9.]+)\"\n\tGIT_HASH_LINE = r\"Git sha \\s*(\\S+)\"\n\t#\n\t# OPTIONS REGEXES\n\tOPTION_LINE = r\"\\s*Options\\.(\\S+)\\s*:\\s*(.+)?\"\n", "DB_WIDE_WBM_PSEUDO_OPTION_LINE = r\"\\s*wbm\\.(\\S+)\\s*:\\s*(.+)\"\n\t# example:\n\t# --------------- Options for column family [default]:\n\t# In case of a match the result will be [<column-family name>] (List)\n\tCF_OPTIONS_START = \\\n\t    r\"--------------- Options for column family \\[(.*)\\]:.*\"\n\tTABLE_OPTIONS_START_LINE = \\\n\t    r\"^\\s*table_factory options:\\s*(\\S+)\\s*:(.*)\"\n\tTABLE_OPTIONS_CONTINUATION_LINE = r\"^\\s*(\\S+)\\s*:(.*)\"\n\t#\n", "# EVENTS\n\t#\n\tPREAMBLE_EVENT = r\"\\[(.*?)\\] \\[JOB ([0-9]+)\\]\\s*(.*)\"\n\t# [column_family_name_000018] [JOB 38] Flushing memtable with next log file: 5\n\t# Capturing: cf_name, job_id, wal-id\n\tFLUSH_EVENT_PREAMBLE = \\\n\t    fr\"^{WS}{CF_NAME}{WS}{JOB_ID}{WS}Flushing memtable \" \\\n\t    fr\"with next log file:{WS}(?P<wal_id>{INT})\"\n\t# [default] [JOB 13] Compacting 1@1 + 5@2 files to L2, score 1.63\n\t# Capturing: cf_name, job_id\n", "COMPACTION_EVENT_PREAMBLE = \\\n\t    fr\"^{WS}{CF_NAME}{WS}{JOB_ID}{WS}Compacting.*score\"\n\tEVENT = r\"\\s*EVENT_LOG_v1\"\n\tWRITE_DELAY_WARN_MSG = fr\"{CF_NAME_OLD}{WS}Stalling writes\"\n\tWRITE_STOP_WARN_MSG = fr\"{CF_NAME_OLD}{WS}Stopping writes\"\n\t#\n\t# STATISTICS RELATED\n\t#\n\tDUMP_STATS_STR = r'------- DUMPING STATS -------'\n\tDB_STATS = fr'^{WS}\\*\\* DB Stats \\*\\*{WS}$'\n", "COMPACTION_STATS = fr'^{WS}\\*\\* Compaction Stats{WS}{CF_NAME}{WS}\\*\\*{WS}$'\n\tFILE_READ_LATENCY_STATS = \\\n\t    fr'^{WS}\\*\\* File Read Latency Histogram By Level{WS}{CF_NAME}' \\\n\t    fr'{WS}\\*\\*{WS}$'\n\tLEVEL_READ_LATENCY_LEVEL_LINE = \\\n\t    fr'\\*\\* Level {INT_C} read latency histogram \\(micros\\):'\n\t# Count: 26687513 Average: 3.1169  StdDev: 34.39\n\tLEVEL_READ_LATENCY_STATS_LINE1 = \\\n\t    fr\"Count:{WS}{INT_C}{WS}Average:{WS}{FLOAT_C}{WS}StdDev:{WS}{FLOAT_C}\"\n\t# Min: 0  Median: 2.4427  Max: 56365\n", "LEVEL_READ_LATENCY_STATS_LINE2 = \\\n\t    fr\"Min:{WS}{INT_C}{WS}Median:{WS}{FLOAT_C}{WS}Max:{WS}{INT_C}\"\n\tSTATS_COUNTERS_AND_HISTOGRAMS = r'^\\s*STATISTICS:\\s*$'\n\t# Uptime(secs): 603.0 total, 600.0 interval\n\tUPTIME_STATS_LINE = \\\n\t    fr'^{WS}Uptime\\(secs\\):{WS}(?P<total>{FLOAT}){WS}total,' \\\n\t    fr'{WS}(?P<interval>{FLOAT}){WS}interval'\n\t#\n\t# BLOCK CACHE STATS REGEXES\n\t#\n", "CACHE_ID = r\"(\\S+)\"\n\t# LRUCache@0x556ae9ce9770#27411\n\t# <name>@<ptr>#<process-id>\n\tCACHE_ID_PARTS = fr\"(?P<name>.*?)@(?P<ptr>{POINTER_NC})#(?P<pid>{INT})\"\n\tBLOCK_CACHE_STATS_START = \\\n\t    fr'Block cache {CACHE_ID} capacity: {FLOAT_C} {BYTES_UNIT} '\n\tBLOCK_CACHE_ENTRY_STATS = \\\n\t    r\"Block cache entry stats\\(count,size,portion\\): (.*)\"\n\tBLOCK_CACHE_CF_ENTRY_STATS = fr\"Block cache {CF_NAME} (.*)\"\n\tBLOCK_CACHE_ENTRY_ROLES_NAMES = r\"([A-Za-z]+)\\(\"\n", "BLOCK_CACHE_ENTRY_ROLES_STATS = r\"[a-zA-Z]+\\(([^\\)]+?)\\)\"\n\tSTATS_COUNTER = fr\"^{WS}([\\w\\.]+){WS}COUNT{WS}:{WS} {INT_C}{WS}$\"\n\tSTATS_HISTOGRAM = \\\n\t    fr'^\\s*([\\w\\.]+) P50 : {FLOAT_C} P95 : {FLOAT_C} P99 : {FLOAT_C} ' \\\n\t    fr'P100 : {FLOAT_C} COUNT : {INT_C} SUM : {INT_C}'\n\tSTATS_HISTOGRAM = \\\n\t    fr'^{WS}(?P<name>[\\w\\.]+){WS}P50{WS}:{WS}(?P<P50>{FLOAT})' \\\n\t    fr'{WS}P95{WS}:{WS}(?P<P95>{FLOAT}){WS}P99{WS}:{WS}(?P<P99>{FLOAT})' \\\n\t    fr'{WS}P100{WS}:{WS}(?P<P100>{FLOAT})' \\\n\t    fr'{WS}COUNT{WS}:{WS}(?P<count>{INT}){WS}SUM{WS}:{WS}(?P<sum>{INT})'\n", "BLOB_STATS_LINE = \\\n\t    fr'Blob file count: ([\\d]+), total size: {FLOAT_C} GB, ' \\\n\t    fr'garbage size: {FLOAT_C} GB, space amp: {FLOAT_C}'\n\tSUPPORT_INFO_START_LINE = r'\\s*Compression algorithms supported:\\s*$'\n\tVERSION = r'(\\d+)\\.(\\d+)\\.?(\\d+)?'\n\t# Interval stall: 00:00:0.000 H:M:S, 0.0 percent\n\tDB_WIDE_INTERVAL_STALL = \\\n\t    fr\"Interval stall: (\\d+):(\\d+):(\\d+)\\.(\\d+) H:M:S, {FLOAT_C} percent\"\n\t#  Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent\n\tDB_WIDE_CUMULATIVE_STALL = \\\n", "    fr\"Cumulative stall: (\\d+):(\\d+):(\\d+)\\.(\\d+) H:M:S, {FLOAT_C} percent\"\n\t# Cumulative writes: 819K writes, 1821M keys, 788K commit groups, 1.0 writes per commit group, ingest: 80.67 GB, 68.66 MB/s # noqa\n\tDB_WIDE_CUMULATIVE_WRITES = \\\n\t    fr\"Cumulative writes:\\s*{NUM_WITH_UNIT} writes,\\s*{NUM_WITH_UNIT} keys.*\"\\\n\t    fr\"ingest: {FLOAT_C}\\s*GB,\\s*{FLOAT_C}\\s*MB/s\"\n\tCF_STALLS_LINE_START = \"Stalls(count):\"\n\tCF_STALLS_COUNT_AND_REASON = r\"\\b(\\d+) (.*?),\"\n\tCF_STALLS_INTERVAL_COUNT = r\".*interval (\\d+) total count$\"\n\t# 2022/12/17-11:12:17.399948 38990 [/version_set.cc:4980] Column family [column_family_name_000009] (ID 9), log number is 41576 # noqa\n\tRECOVERED_CF = \\\n", "    fr\"Column family {CF_NAME}\\s*{CF_ID},{WS}log number is (?P<log_num>{INT})\"\n\t# 2022/12/17-06:02:37.695926 24554 [/db_impl/db_impl.cc:2799] Created column family [column_family_name_000001] (ID 1) # noqa\n\tCREATE_CF = fr\"Created column family {CF_NAME}\\s*{CF_ID}\"\n\tDROP_CF = fr\"Dropped column family with id {INT_C}\\s*\"\n\tROCKSDB_BASELINE_LOG_FILE = r\"LOG-rocksdb-(\\d+\\.\\d+\\.?\\d*)\"\n\tSPEEDB_BASELINE_LOG_FILE = r\"LOG-speedb-(\\d+\\.\\d+\\.?\\d*)\"\n\t# 2021/05/04-20:54:25.385487 7fa4245ff700 [/compaction/compaction_job.cc:1762] [default] [JOB 38680] Compacting 1@5 + 1@6 files to L6, score 0.9 # noqa\n\tCOMPACTION_BEFORE_SCORE_LINE = \\\n\t    fr\"{CF_NAME}\\s*{JOB_ID}\\s*Compacting .*\" \\\n\t    fr\"files to L{INT_C},\\s*score\\s*{FLOAT_C}\"\n", "# A line in a compaction job preceding the compaction_finished event\n\t# [default] compacted to: files[4 7 45 427 822 0 0] max score 1.63,\n\t# MB/sec: 450.9 rd, 450.1 wr, level 1, files in(4, 3) out(7 +0 blob)\n\t# MB in(224.7, 193.2 +0.0 blob) out(417.0 +0.0 blob), read-write-amplify(3.7)\n\t# write-amplify(1.9) OK, records in: 424286, records dropped: 789\n\t# output_compression: NoCompression\n\t# The following regex matches this line and extracts the following fields:\n\t# 0: cf name\n\t# 1: MB/sec - <Rate> rd\n\t# 2: MB/sec - <Rate> wr\n", "# 3: read-write-amplify\n\t# 4: write-amplify\n\t# 5: records in\n\t# 6: records dropped\n\t#\n\tCOMPACTION_JOB_FINISH_STATS_LINE = \\\n\t    fr\"{CF_NAME_OLD}.*,\\s*MB\\/sec:\\s*{FLOAT_C}\\s*rd,\" \\\n\t    fr\"\\s*{FLOAT_C}\\s*wr,.*read-write-amplify\\({FLOAT_C}\\)\\s*write-amplify\\(\" \\\n\t    fr\"{FLOAT_C}\\).*records in:\\s*{INT_C},\\s*records dropped:\\s*{INT_C}\"\n"]}
{"filename": "cache_utils.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\timport db_files\n\tfrom counters import CountersMngr\n\tfrom db_files import DbFilesMonitor\n\tfrom db_options import RAW_NULL_PTR, DatabaseOptions, \\\n\t    sanitized_to_raw_ptr_value\n\tdef get_cf_raw_cache_ptr_str(cf_name, db_options):\n\t    assert isinstance(db_options, DatabaseOptions)\n\t    sanitized_cache_ptr = \\\n", "        db_options.get_cf_table_option(cf_name, \"block_cache\")\n\t    return sanitized_to_raw_ptr_value(sanitized_cache_ptr)\n\tdef does_cf_has_cache(cf_name, db_options):\n\t    raw_ptr_str = db_options.get_cf_table_raw_ptr_str(cf_name, \"block_cache\")\n\t    return raw_ptr_str is not None and raw_ptr_str != RAW_NULL_PTR\n\tdef get_cache_id(cf_name, db_options):\n\t    assert isinstance(db_options, DatabaseOptions)\n\t    cache_ptr = db_options.get_cf_table_raw_ptr_str(cf_name, \"block_cache\")\n\t    cache_name = db_options.get_cf_table_option(cf_name, \"block_cache_name\")\n\t    if cache_ptr is None or cache_name is None:\n", "        return None\n\t    return f\"{cache_name}@{cache_ptr}\"\n\t@dataclass\n\tclass CfCacheOptions:\n\t    cf_name: str\n\t    cache_id: str = None\n\t    cache_index_and_filter_blocks: bool = None\n\t    cache_capacity_bytes: int = 0\n\t    num_shard_bits: int = 0\n\t@dataclass\n", "class CfSpecificCacheOptions:\n\t    cache_index_and_filter_blocks: bool = None\n\t@dataclass\n\tclass CacheOptions:\n\t    cache_capacity_bytes: int = 0\n\t    num_shard_bits: int = 0\n\t    shard_size_bytes: int = 0\n\t    # {<cf-name>: CfSpecificCacheOptions}\n\t    cfs_specific_options: dict = None\n\tdef collect_cf_cache_options(cf_name, db_options):\n", "    assert isinstance(db_options, DatabaseOptions)\n\t    if not does_cf_has_cache(cf_name, db_options):\n\t        return None\n\t    cache_id = get_cache_id(cf_name, db_options)\n\t    cache_index_and_filter_blocks = \\\n\t        db_options.get_cf_table_option(\n\t            cf_name, \"cache_index_and_filter_blocks\")\n\t    cache_capacity_bytes_str = \\\n\t        db_options.get_cf_table_option(cf_name, \"block_cache_capacity\")\n\t    num_shard_bits_str = \\\n", "        db_options.get_cf_table_option(cf_name, \"block_cache_num_shard_bits\")\n\t    if cache_id is None or cache_index_and_filter_blocks is None or \\\n\t            cache_capacity_bytes_str is None or num_shard_bits_str is None:\n\t        logging.warning(\n\t            f\"{cf_name} has cache but its cache options are \"\n\t            f\"corrupted. cache_id:{cache_id}, \"\n\t            f\"cache_index_and_filter_blocks:{cache_index_and_filter_blocks}\"\n\t            f\"cache_capacity_bytes_str:{cache_capacity_bytes_str}\"\n\t            f\"num_shard_bits_str:{num_shard_bits_str}\")\n\t        return None\n", "    options = CfCacheOptions(cf_name)\n\t    options.cache_id = cache_id\n\t    options.cache_index_and_filter_blocks = cache_index_and_filter_blocks\n\t    options.cache_capacity_bytes = int(cache_capacity_bytes_str)\n\t    options.num_shard_bits = int(num_shard_bits_str)\n\t    return options\n\tdef calc_shard_size_bytes(cache_capacity_bytes, num_shard_bits):\n\t    num_shards = 2 ** num_shard_bits\n\t    return int((cache_capacity_bytes + (num_shards - 1)) / num_shards)\n\tdef collect_cache_options_info(db_options):\n", "    assert isinstance(db_options, DatabaseOptions)\n\t    # returns {<cache-id>: [<cf-cache-options>]\n\t    cache_options = dict()\n\t    cfs_names = db_options.get_cfs_names()\n\t    for cf_name in cfs_names:\n\t        cf_cache_options = collect_cf_cache_options(cf_name, db_options)\n\t        if cf_cache_options is None:\n\t            continue\n\t        cache_id = cf_cache_options.cache_id\n\t        cfs_specific_options =\\\n", "            CfSpecificCacheOptions(\n\t                cf_cache_options.cache_index_and_filter_blocks)\n\t        if cache_id not in cache_options:\n\t            shard_size_bytes = \\\n\t                calc_shard_size_bytes(cf_cache_options.cache_capacity_bytes,\n\t                                      cf_cache_options.num_shard_bits)\n\t            cfs_specific_options =\\\n\t                {cf_cache_options.cf_name: cfs_specific_options}\n\t            cache_options[cache_id] = \\\n\t                CacheOptions(\n", "                    cache_capacity_bytes=cf_cache_options.cache_capacity_bytes,\n\t                    num_shard_bits=cf_cache_options.num_shard_bits,\n\t                    shard_size_bytes=shard_size_bytes,\n\t                    cfs_specific_options=cfs_specific_options)\n\t        else:\n\t            assert cache_options[cache_id].cache_capacity_bytes == \\\n\t                   cf_cache_options.cache_capacity_bytes\n\t            assert cache_options[cache_id].num_shard_bits == \\\n\t                   cf_cache_options.num_shard_bits\n\t            cache_options[cache_id].cfs_specific_options[\n", "                cf_cache_options.cf_name] = cfs_specific_options\n\t    if not cache_options:\n\t        return None\n\t    return cache_options\n\t@dataclass\n\tclass CacheCounters:\n\t    cache_add: int = 0\n\t    cache_miss: int = 0\n\t    cache_hit: int = 0\n\t    index_add: int = 0\n", "    index_miss: int = 0\n\t    index_hit: int = 0\n\t    filter_add: int = 0\n\t    filter_miss: int = 0\n\t    filter_hit: int = 0\n\t    data_add: int = 0\n\t    data_miss: int = 0\n\t    data_hit: int = 0\n\tdef collect_cache_counters(counters_mngr):\n\t    assert isinstance(counters_mngr, CountersMngr)\n", "    if not counters_mngr.does_have_counters_values():\n\t        logging.info(\"Can't collect cache counters. No counters available\")\n\t        return None\n\t    cache_counters_names = {\n\t        \"cache_miss\": \"rocksdb.block.cache.miss\",\n\t        \"cache_hit\": \"rocksdb.block.cache.hit\",\n\t        \"cache_add\": \"rocksdb.block.cache.add\",\n\t        \"index_miss\": \"rocksdb.block.cache.index.miss\",\n\t        \"index_hit\": \"rocksdb.block.cache.index.hit\",\n\t        \"index_add\": \"rocksdb.block.cache.index.add\",\n", "        \"filter_miss\": \"rocksdb.block.cache.filter.miss\",\n\t        \"filter_hit\": \"rocksdb.block.cache.filter.hit\",\n\t        \"filter_add\": \"rocksdb.block.cache.filter.add\",\n\t        \"data_miss\": \"rocksdb.block.cache.data.miss\",\n\t        \"data_hit\": \"rocksdb.block.cache.data.hit\",\n\t        \"data_add\": \"rocksdb.block.cache.data.add\"}\n\t    counters = CacheCounters()\n\t    for field_name, counter_name in cache_counters_names.items():\n\t        counter_value = counters_mngr.get_last_counter_value(counter_name)\n\t        setattr(counters, field_name, counter_value)\n", "    return counters\n\t@dataclass\n\tclass CacheIdInfo:\n\t    options: CacheOptions = None\n\t    files_stats: db_files.CfsFilesStats = None\n\t@dataclass\n\tclass CacheStats:\n\t    # {<cache-id>: CacheIdInfo}\n\t    per_cache_id_info: dict = None\n\t    global_cache_counters: CacheCounters = None\n", "def calc_block_cache_stats(db_options: object, counters_mngr,\n\t                           files_monitor) -> object:\n\t    assert isinstance(db_options, DatabaseOptions)\n\t    assert isinstance(counters_mngr, CountersMngr)\n\t    assert isinstance(files_monitor, DbFilesMonitor)\n\t    stats = CacheStats()\n\t    cache_options = collect_cache_options_info(db_options)\n\t    if cache_options is None:\n\t        return None\n\t    stats.per_cache_id_info = dict()\n", "    for cache_id, options in cache_options.items():\n\t        assert isinstance(options, CacheOptions)\n\t        cache_cfs_names = list(options.cfs_specific_options.keys())\n\t        cache_files_stats =\\\n\t            db_files.calc_cf_files_stats(cache_cfs_names, files_monitor)\n\t        if not cache_files_stats:\n\t            return None\n\t        assert isinstance(cache_files_stats, db_files.CfsFilesStats)\n\t        stats.per_cache_id_info[cache_id] = \\\n\t            CacheIdInfo(options=options, files_stats=cache_files_stats)\n", "    cache_counters = collect_cache_counters(counters_mngr)\n\t    if cache_counters:\n\t        stats.global_cache_counters = cache_counters\n\t    return stats\n"]}
{"filename": "test/test_db_options.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport pytest\n\timport db_options as db_opts\n\timport utils\n\tdefault = 'default'\n\tcf1 = 'cf1'\n\tcf2 = 'cf2'\n\tcf3 = 'cf3'\n", "EMPTY_FULL_NAMES_OPTIONS_DICT = db_opts.FullNamesOptionsDict()\n\tDB_WIDE_CF_NAME = utils.NO_CF\n\tCF_SECTION_TYPE = db_opts.SectionType.CF\n\tTABLE_SECTION_TYPE = db_opts.SectionType.TABLE_OPTIONS\n\tbaseline_options = {\n\t    'DBOptions.stats_dump_freq_sec': {utils.NO_CF: '20'},\n\t    'DBOptions.enable_pipelined_write': {utils.NO_CF: '0'},\n\t    'CFOptions.write_buffer_size': {\n\t        default: '1024000',\n\t        cf1: '128000',\n", "        cf2: '128000000'\n\t    },\n\t    'TableOptions.BlockBasedTable.index_type': {cf2: '1'},\n\t    'TableOptions.BlockBasedTable.format_version': {cf1: '4', cf2: '5'},\n\t    'TableOptions.BlockBasedTable.block_size': {cf1: '4096', cf2: '4096'},\n\t    'DBOptions.use_fsync': {utils.NO_CF: 'true'},\n\t    'DBOptions.max_log_file_size':\n\t        {utils.NO_CF: '128000000'},\n\t    'DBOptions.ptr_option1': {utils.NO_CF: '0x12345678'}\n\t}\n", "new_options = {\n\t    'bloom_bits': {utils.NO_CF: '4'},\n\t    'DBOptions.enable_pipelined_write': {utils.NO_CF: 'True'},\n\t    'CFOptions.write_buffer_size': {\n\t        default: '128000000',\n\t        cf1: '128000',\n\t        cf3: '128000000'\n\t    },\n\t    'TableOptions.BlockBasedTable.checksum': {cf1: 'true'},\n\t    'TableOptions.BlockBasedTable.format_version': {cf1: '5', cf2: '5'},\n", "    'TableOptions.BlockBasedTable.block_size': {cf1: '4096', cf2: '4096'},\n\t    'DBOptions.use_fsync': {utils.NO_CF: 'true'},\n\t    'DBOptions.max_log_file_size': {utils.NO_CF: '0'},\n\t    'DBOptions.ptr_option1': {utils.NO_CF: '0x55555555'}\n\t}\n\tbaseline = db_opts.FullNamesOptionsDict(baseline_options)\n\tnew = db_opts.FullNamesOptionsDict(new_options)\n\tdef test_sanitized_to_raw_ptr_value():\n\t    assert db_opts.sanitized_to_raw_ptr_value(db_opts.SANITIZED_NULL_PTR) == \\\n\t           db_opts.RAW_NULL_PTR\n", "    assert db_opts.sanitized_to_raw_ptr_value(\"Pointer (0x1234)\") == '0x1234'\n\t    assert db_opts.sanitized_to_raw_ptr_value(\"Pointer (0xa0FAA55)\") == \\\n\t           '0xa0FAA55'\n\tdef test_extract_section_type():\n\t    assert db_opts.SectionType.\\\n\t           extract_section_type(\"Version.option-name1\") == \"Version\"\n\t    assert db_opts.SectionType.\\\n\t           extract_section_type(\"DBOptions.option-name1\") == \"DBOptions\"\n\t    assert db_opts.SectionType.\\\n\t           extract_section_type(\"CFOptions.option-name1\") == \"CFOptions\"\n", "    assert db_opts.SectionType.extract_section_type(\n\t        \"TableOptions.BlockBasedTable.option-name1\") ==\\\n\t        \"TableOptions.BlockBasedTable\"\n\t    with pytest.raises(utils.ParsingError):\n\t        db_opts.SectionType.extract_section_type(\"Dummy.option-name1\")\n\tdef test_misc_option_name_utils():\n\t    # parse_full_option_name\n\t    assert db_opts.parse_full_option_name(\"Version.option-name1\") \\\n\t           == (\"Version\", \"option-name1\")\n\t    with pytest.raises(utils.ParsingError):\n", "        db_opts.SectionType.extract_section_type(\"Dummy.option-name1\")\n\t    # get_full_option_name\n\t    assert db_opts.get_full_option_name(\"DBOptions\", \"option1\") == \\\n\t           \"DBOptions.option1\"\n\t    with pytest.raises(utils.ParsingError):\n\t        db_opts.get_full_option_name(\"Dummy\", \"option-name1\")\n\t    # get_db_wide_full_option_name\n\t    assert db_opts.get_db_wide_full_option_name(\"option1\") == \\\n\t           \"DBOptions.option1\"\n\t    # extract_option_name\n", "    assert db_opts.extract_option_name(\"DBOptions.option1\") == \\\n\t           \"option1\"\n\t    with pytest.raises(utils.ParsingError):\n\t        db_opts.extract_option_name(\"Dummy.option-name1\")\n\t    # extract_db_wide_option_name\n\t    assert db_opts.extract_db_wide_option_name(\"DBOptions.option1\")\\\n\t           == \"option1\"\n\t    with pytest.raises(utils.ParsingError):\n\t        db_opts.extract_db_wide_option_name(\"CFOptions.option1\")\n\t    # get_cf_full_option_name\n", "    assert db_opts.get_cf_full_option_name(\"option1\") == \\\n\t           \"CFOptions.option1\"\n\t    # extract_db_wide_option_name\n\t    assert db_opts.extract_cf_option_name(\"CFOptions.option1\")\\\n\t           == \"option1\"\n\t    with pytest.raises(utils.ParsingError):\n\t        db_opts.extract_cf_option_name(\"DBOptions.option1\")\n\t    # get_cf_table_full_option_name\n\t    assert db_opts.get_cf_table_full_option_name(\"option1\") == \\\n\t           \"TableOptions.BlockBasedTable.option1\"\n", "    # extract_db_wide_option_name\n\t    assert db_opts.extract_cf_table_option_name(\n\t        \"TableOptions.BlockBasedTable.option1\") == \"option1\"\n\t    with pytest.raises(utils.ParsingError):\n\t        db_opts.extract_cf_table_option_name(\"DBOptions.option1\")\n\tdef test_misc_options_sanitization_utils():\n\t    # check_and_sanitize_if_no_value\n\t    assert db_opts.check_and_sanitize_if_null_ptr(None) == \\\n\t           (False, None)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"None\") == \\\n", "           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"NONE\") == \\\n\t           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"(nil)\") == \\\n\t           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"nil\") == \\\n\t           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"nullptr\") == \\\n\t           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"null\") == \\\n", "           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"NullPtr\") == \\\n\t           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"0x0\") == \\\n\t           (True, db_opts.SANITIZED_NULL_PTR)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"\") == \\\n\t           (False, \"\")\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"XXX\") == \\\n\t           (False, \"XXX\")\n\t    assert db_opts.check_and_sanitize_if_null_ptr(0) == \\\n", "           (False, 0)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(False) == \\\n\t           (False, False)\n\t    assert db_opts.check_and_sanitize_if_null_ptr(\"0x1234\") == \\\n\t           (False, \"0x1234\")\n\t    # is_bool_value\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        True, include_int=False) == (True, \"True\")\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        False, include_int=False) == (True, \"False\")\n", "    assert db_opts.check_and_sanitize_if_bool_value(\n\t        \"true\", include_int=False) == (True, \"True\")\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        \"TRUE\", include_int=False) == (True, \"True\")\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        \"false\", include_int=False) == (True, \"False\")\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        \"FALSE\", include_int=False) == (True, \"False\")\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        0, include_int=False) == (False, 0)\n", "    assert db_opts.check_and_sanitize_if_bool_value(\n\t        1, include_int=False) == (False, 1)\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        0, include_int=True) == (True, \"False\")\n\t    assert db_opts.check_and_sanitize_if_bool_value(\n\t        1, include_int=True) == (True, \"True\")\n\t    assert db_opts.check_and_sanitize_if_pointer_value(None) == \\\n\t           (False, None)\n\t    assert db_opts.check_and_sanitize_if_pointer_value(\"(nil)\") ==\\\n\t           (False, \"(nil)\")\n", "    assert db_opts.check_and_sanitize_if_pointer_value(0) ==\\\n\t           (False, 0)\n\t    assert db_opts.check_and_sanitize_if_pointer_value(\"0x0\") ==\\\n\t           (False, \"0x0\")\n\t    assert db_opts.check_and_sanitize_if_pointer_value(\"0x123\") ==\\\n\t           (True, \"Pointer (0x123)\")\n\t    # get_sanitized_value\n\t    assert db_opts.get_sanitized_value(None) == db_opts.SANITIZED_NO_VALUE\n\t    assert db_opts.get_sanitized_value(\"None\") == db_opts.SANITIZED_NULL_PTR\n\t    assert db_opts.get_sanitized_value(\"None\") == db_opts.SANITIZED_NULL_PTR\n", "    assert db_opts.get_sanitized_value(\"0x0\") == db_opts.SANITIZED_NULL_PTR\n\t    assert db_opts.get_sanitized_value(True) == \"True\"\n\t    assert db_opts.get_sanitized_value(False) == \"False\"\n\t    assert db_opts.get_sanitized_value(False) == \"False\"\n\t    assert db_opts.get_sanitized_value(\"False\") == \"False\"\n\t    assert db_opts.get_sanitized_value(\"100\") == \"100\"\n\t    assert db_opts.get_sanitized_value(\"0x123\") == \"Pointer (0x123)\"\n\t    # get_sanitized_options_diff\n\t    assert db_opts.are_non_sanitized_values_different(0, 1)\n\t    assert db_opts.are_non_sanitized_values_different(False, 1)\n", "    assert db_opts.are_non_sanitized_values_different(True, 0)\n\t    assert db_opts.are_non_sanitized_values_different(\"True\", 0)\n\t    assert db_opts.are_non_sanitized_values_different(\"True\", \"0\")\n\t    assert db_opts.are_non_sanitized_values_different(\"False\", 1)\n\t    assert db_opts.are_non_sanitized_values_different(\"False\", \"1\")\n\t    assert not db_opts.are_non_sanitized_values_different(False, 0)\n\t    assert not db_opts.are_non_sanitized_values_different(True, 1)\n\t    assert not db_opts.are_non_sanitized_values_different(\"False\", 0)\n\t    assert not db_opts.are_non_sanitized_values_different(\"False\", \"0\")\n\t    assert not db_opts.are_non_sanitized_values_different(\"True\", 1)\n", "    assert not db_opts.are_non_sanitized_values_different(\"True\", \"1\")\n\t    assert not db_opts.are_non_sanitized_values_different(0, \"False\")\n\t    assert not db_opts.are_non_sanitized_values_different(1, \"TRUE\")\n\t    assert not db_opts.are_non_sanitized_values_different(None, None)\n\t    assert db_opts.are_non_sanitized_values_different(None, \"nil\")\n\t    assert db_opts.are_non_sanitized_values_different(None, \"False\")\n\t    assert db_opts.are_non_sanitized_values_different(False, None)\n\t    assert not db_opts.are_non_sanitized_values_different(\"0x0\", \"(nil)\")\n\t    assert not db_opts.are_non_sanitized_values_different(\"0x1234\", \"0x5678\")\n\t    assert db_opts.are_non_sanitized_values_different(\"0x0\", \"0x5678\")\n", "    # get_sanitized_options_diff\n\t    assert db_opts.get_sanitized_options_diff(\n\t        \"None\", None, expect_diff=False) == \\\n\t        (True, db_opts.SANITIZED_NULL_PTR, db_opts.SANITIZED_NO_VALUE)\n\t    assert \\\n\t        db_opts.get_sanitized_options_diff(\n\t            \"None\", \"false\", expect_diff=True) == \\\n\t        (db_opts.SANITIZED_NULL_PTR, \"False\")\n\t    assert db_opts.get_sanitized_options_diff(\n\t        \"false\", True, expect_diff=True) == (\"False\", \"True\")\n", "    assert db_opts.get_sanitized_options_diff(\n\t        0, True, expect_diff=True) == (\"False\", \"True\")\n\t    assert db_opts.get_sanitized_options_diff(\n\t        0, 1, expect_diff=True) == (0, 1)\n\t    assert db_opts.get_sanitized_options_diff(\n\t        0, \"1\", expect_diff=True) == (0, \"1\")\n\t    assert db_opts.get_sanitized_options_diff(\n\t        \"0x1234\", \"0x5678\", expect_diff=False) == \\\n\t        (False, \"Pointer (0x1234)\", \"Pointer (0x5678)\")\n\tdef test_full_name_options_dict():\n", "    db_option1 = \"DB-OPTION-1\"\n\t    db_value1 = \"DB-VALUE-1\"\n\t    db_value1_1 = \"DB-VALUE-1-1\"\n\t    cf1 = \"CF-1\"\n\t    option1 = \"OPTION-1\"\n\t    value1 = \"VALUE-1\"\n\t    value1_1 = \"VALUE-1-1\"\n\t    cf2 = \"CF-2\"\n\t    option2 = \"OPTION-2\"\n\t    value2 = \"VALUE-2\"\n", "    value2_1 = \"VALUE-2-1\"\n\t    fnd1 = db_opts.FullNamesOptionsDict()\n\t    fnd2 = db_opts.FullNamesOptionsDict()\n\t    assert fnd1 == fnd2\n\t    assert fnd1 == fnd2.get_options_dict()\n\t    assert fnd1.get_option_by_full_name(f\"DBOptions.{db_option1}\") is None\n\t    fnd1.set_option(db_opts.SectionType.DB_WIDE, DB_WIDE_CF_NAME, db_option1,\n\t                    db_value1)\n\t    assert fnd1 != fnd2\n\t    assert fnd1.get_option_by_full_name(f\"DBOptions.{db_option1}\") == {\n", "        DB_WIDE_CF_NAME: db_value1}\n\t    assert fnd1.\\\n\t           get_option_by_parts(db_opts.SectionType.DB_WIDE, db_option1) == \\\n\t           {DB_WIDE_CF_NAME: db_value1}\n\t    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, db_option1,\n\t                                    DB_WIDE_CF_NAME) == db_value1\n\t    assert fnd1.get_option_by_parts(db_opts.SectionType.CF, db_option1,\n\t                                    DB_WIDE_CF_NAME) is None\n\t    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, db_option1,\n\t                                    cf1) is None\n", "    assert fnd1.get_db_wide_option(db_option1) == db_value1\n\t    assert fnd1.get_cf_option(db_option1) is None\n\t    assert fnd1.get_cf_option(db_option1, cf1) is None\n\t    assert fnd1.get_cf_table_option(db_option1) is None\n\t    assert fnd1.get_cf_table_option(db_option1, cf1) is None\n\t    fnd1.set_db_wide_option(db_option1, db_value1_1)\n\t    assert fnd1.get_db_wide_option(db_option1) == db_value1_1\n\t    fnd1.set_cf_option(cf1, option1, value1)\n\t    assert fnd1.\\\n\t           get_option_by_parts(db_opts.SectionType.CF, option1) == \\\n", "           {cf1: value1}\n\t    assert fnd1.\\\n\t           get_option_by_parts(db_opts.SectionType.CF, option1, cf1) == value1\n\t    assert fnd1.\\\n\t           get_option_by_parts(db_opts.SectionType.DB_WIDE, option1) is None\n\t    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, option1,\n\t                                    DB_WIDE_CF_NAME) is None\n\t    assert fnd1.get_cf_option(option1) == {cf1: value1}\n\t    assert fnd1.get_cf_option(option1, cf1) == value1\n\t    assert fnd1.get_cf_table_option(option1) is None\n", "    assert fnd1.get_cf_table_option(option1, cf1) is None\n\t    fnd1.set_cf_option(cf1, option1, value1_1)\n\t    assert fnd1.get_cf_option(option1, cf1) == value1_1\n\t    fnd1.set_cf_table_option(cf2, option2, value2)\n\t    assert fnd1.\\\n\t           get_option_by_parts(db_opts.SectionType.TABLE_OPTIONS, option2) == \\\n\t           {cf2: value2}\n\t    assert fnd1.get_option_by_parts(db_opts.SectionType.TABLE_OPTIONS, option2,\n\t                                    cf2) == value2\n\t    assert fnd1.\\\n", "           get_option_by_parts(db_opts.SectionType.DB_WIDE, option2) is None\n\t    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, option2,\n\t                                    DB_WIDE_CF_NAME) is None\n\t    assert fnd1.get_cf_table_option(option2) == {cf2: value2}\n\t    assert fnd1.get_cf_table_option(option2, cf2) == value2\n\t    assert fnd1.get_cf_option(option2) is None\n\t    assert fnd1.get_cf_option(option2, cf2) is None\n\t    fnd1.set_cf_table_option(cf2, option2, value2_1)\n\t    assert fnd1.get_cf_table_option(option2, cf2) == value2_1\n\t    expected_dict = {'DBOptions.DB-OPTION-1': {'DB_WIDE': 'DB-VALUE-1-1'},\n", "                     'CFOptions.OPTION-1': {'CF-1': 'VALUE-1-1'},\n\t                     'TableOptions.BlockBasedTable.OPTION-2':\n\t                         {'CF-2': 'VALUE-2-1'}}\n\t    assert fnd1.get_options_dict() == expected_dict\n\t    fnd1.set_db_wide_option(option2, value1_1)\n\t    fnd1.set_cf_option(cf2, option1, db_value1)\n\t    fnd1.set_cf_table_option(cf1, option2, value2)\n\t    expected_dict = {'DBOptions.DB-OPTION-1': {'DB_WIDE': 'DB-VALUE-1-1'},\n\t                     'DBOptions.OPTION-2': {'DB_WIDE': 'VALUE-1-1'},\n\t                     'CFOptions.OPTION-1': {'CF-1': 'VALUE-1-1',\n", "                                            'CF-2': 'DB-VALUE-1'},\n\t                     'TableOptions.BlockBasedTable.OPTION-2':\n\t                         {'CF-2': 'VALUE-2-1', 'CF-1': 'VALUE-2'}}\n\t    assert fnd1.get_options_dict() == expected_dict\n\tdef test_empty_db_options():\n\t    dbo = db_opts.DatabaseOptions()\n\t    assert dbo.get_all_options() == EMPTY_FULL_NAMES_OPTIONS_DICT\n\t    assert dbo.get_db_wide_options() == EMPTY_FULL_NAMES_OPTIONS_DICT\n\t    assert not dbo.are_db_wide_options_set()\n\t    assert dbo.get_cfs_names() == []\n", "    assert dbo.get_db_wide_options_for_display() == {}\n\t    assert dbo.get_db_wide_option(\"manual_wal_flush\") is None\n\t    with pytest.raises(AssertionError):\n\t        dbo.set_db_wide_option(\"manual_wal_flush\", \"1\",\n\t                               allow_new_option=False)\n\t    dbo.set_db_wide_option(\"manual_wal_flush\", \"1\",\n\t                           allow_new_option=True)\n\t    assert dbo.get_db_wide_option(\"manual_wal_flush\") == \"1\"\n\t    assert dbo.get_db_wide_option(\"Dummy-Options\") is None\n\t    assert dbo.get_options({\"DBOptions.manual_wal_flush\"}).\\\n", "        get_options_dict() == {\n\t        'DBOptions.manual_wal_flush': {\"DB_WIDE\": \"1\"}}\n\t    assert dbo.get_options({\"DBOptions.Dummy-Option\"}) == \\\n\t           EMPTY_FULL_NAMES_OPTIONS_DICT\n\t    assert dbo.get_cf_options(default) == EMPTY_FULL_NAMES_OPTIONS_DICT\n\t    assert dbo.get_cf_options_for_display(default) == ({}, {})\n\t    assert dbo.get_cf_option(default, \"write_buffer_size\") is None\n\t    with pytest.raises(AssertionError):\n\t        dbo.set_cf_option(default, \"write_buffer_size\", \"100\",\n\t                          allow_new_option=False)\n", "    dbo.set_cf_option(default, \"write_buffer_size\", \"100\",\n\t                      allow_new_option=True)\n\t    assert dbo.get_cf_option(default, \"write_buffer_size\") == \"100\"\n\t    assert dbo.get_cf_option(default, \"Dummmy-Options\") is None\n\t    assert dbo.get_cf_option(\"Dummy-CF\", \"write_buffer_size\") is None\n\t    assert dbo.get_options({\"CFOptions.write_buffer_size\"}) == \\\n\t           {'CFOptions.write_buffer_size': {default: '100'}}\n\t    assert dbo.get_options({\"CFOptions.write_buffer_size\"}, default) \\\n\t           == {'CFOptions.write_buffer_size': {default: '100'}}\n\t    assert dbo.get_options({\"CFOptions.write_buffer_size\"}, \"Dummy-CF\")\\\n", "           == {}\n\t    assert dbo.get_options({\"CFOptions.Dummy-Options\"}, default) == {}\n\t    assert dbo.get_cf_table_option(default, \"write_buffer_size\") is \\\n\t           None\n\t    assert dbo.\\\n\t           get_options([\"TableOptions.BlockBasedTable.block_align\"], cf1) == {}\n\t    with pytest.raises(AssertionError):\n\t        dbo.set_cf_table_option(cf1, \"index_type\", \"3\", allow_new_option=False)\n\t    dbo.set_cf_table_option(cf1, \"index_type\", \"3\", allow_new_option=True)\n\t    assert dbo.get_cf_table_option(cf1, \"index_type\") == \"3\"\n", "    assert dbo.get_cf_table_option(cf1, \"Dummy-Options\") is None\n\t    assert dbo.get_cf_table_option(default, \"index_type\") is None\n\t    assert dbo.get_options({\"TableOptions.BlockBasedTable.index_type\"})\\\n\t           == {'TableOptions.BlockBasedTable.index_type': {cf1: '3'}}\n\t    assert dbo.get_options({\"TableOptions.BlockBasedTable.index_type\"},\n\t                           cf1) == \\\n\t           {'TableOptions.BlockBasedTable.index_type': {cf1: '3'}}\n\t    assert dbo.get_options({\n\t        \"TableOptions.BlockBasedTable.index_type\"}, \"Dummy-CF\") == {}\n\t    assert dbo.get_options({\n", "        \"TableOptions.BlockBasedTable.Dummy-Option\"}) == {}\n\tdef test_set_db_wide_options():\n\t    input_dict = {\"manual_wal_flush\": \"false\",\n\t                  \"db_write_buffer_size\": \"0\"}\n\t    expected_options = {\n\t        'DBOptions.manual_wal_flush': {utils.NO_CF: 'False'},\n\t        'DBOptions.db_write_buffer_size': {utils.NO_CF: '0'},\n\t    }\n\t    dbo = db_opts.DatabaseOptions()\n\t    dbo.set_db_wide_options(input_dict)\n", "    assert dbo.get_all_options() == expected_options\n\t    assert dbo.get_db_wide_options() == expected_options\n\t    assert dbo.get_db_wide_option(\"manual_wal_flush\") == 'False'\n\t    assert dbo.get_db_wide_option(\"manual_wal_flush-X\") is None\n\t    assert dbo.get_cfs_names() == []\n\t    dbo.set_db_wide_option('manual_wal_flush', 'true', allow_new_option=False)\n\t    assert dbo.get_db_wide_option(\"manual_wal_flush\") == 'True'\n\t    dbo.set_db_wide_option('manual_wal_flush', 'false', allow_new_option=False)\n\t    assert dbo.get_db_wide_option(\"manual_wal_flush\") == 'False'\n\t    assert dbo.get_db_wide_option(\"DUMMY\") is None\n", "    with pytest.raises(AssertionError):\n\t        dbo.set_db_wide_option('DUMMY', 'XXX', allow_new_option=False)\n\t    dbo.set_db_wide_option('DUMMY', 'XXX', True)\n\t    assert dbo.get_db_wide_option(\"DUMMY\") == \"XXX\"\n\t    expected_db_wide_display_options = {'manual_wal_flush': 'False',\n\t                                        'db_write_buffer_size': '0',\n\t                                        'DUMMY': 'XXX'}\n\t    assert dbo.get_db_wide_options_for_display() == \\\n\t           expected_db_wide_display_options\n\tdef test_set_cf_options():\n", "    default_input_dict = {\"write_buffer_size\": \"1000\"}\n\t    default_table_input_dict = {\"index_type\": \"1\"}\n\t    cf1_input_dict = {\"write_buffer_size\": \"2000\"}\n\t    cf1_table_input_dict = {\"index_type\": \"2\"}\n\t    cf2_input_dict = {\"write_buffer_size\": \"3000\"}\n\t    cf2_table_input_dict = {\"index_type\": \"3\"}\n\t    dbo = db_opts.DatabaseOptions()\n\t    expected_options = dict()\n\t    expected_options['CFOptions.write_buffer_size'] = {default: '1000'}\n\t    expected_options['TableOptions.BlockBasedTable.index_type'] = \\\n", "        {default: '1'}\n\t    dbo.set_cf_options(default, default_input_dict, default_table_input_dict)\n\t    assert dbo.get_all_options() == expected_options\n\t    assert dbo.get_cfs_names() == [default]\n\t    expected_options['CFOptions.write_buffer_size'][cf1] = \"2000\"\n\t    expected_options['TableOptions.BlockBasedTable.index_type'][cf1] = \"2\"\n\t    dbo.set_cf_options(cf1, cf1_input_dict, cf1_table_input_dict)\n\t    assert dbo.get_all_options().get_options_dict() == expected_options\n\t    assert dbo.get_cfs_names() == [default, cf1]\n\t    expected_options['CFOptions.write_buffer_size'][cf2] = \"3000\"\n", "    expected_options['TableOptions.BlockBasedTable.index_type'][cf2] = \"3\"\n\t    dbo.set_cf_options(cf2, cf2_input_dict, cf2_table_input_dict)\n\t    assert dbo.get_all_options() == expected_options\n\t    assert dbo.get_cfs_names() == [default, cf1, cf2]\n\t    expected_dict_default = \\\n\t        {'CFOptions.write_buffer_size': {default: '1000'},\n\t         'TableOptions.BlockBasedTable.index_type': {default: '1'}}\n\t    expected_dict_cf1 = \\\n\t        {'CFOptions.write_buffer_size': {cf1: '2000'},\n\t         'TableOptions.BlockBasedTable.index_type': {cf1: '2'}}\n", "    expected_dict_cf2 = \\\n\t        {'CFOptions.write_buffer_size': {cf2: '3000'},\n\t         'TableOptions.BlockBasedTable.index_type': {cf2: '3'}}\n\t    assert dbo.get_db_wide_options().get_options_dict() == {}\n\t    assert expected_dict_default == dbo.get_cf_options(default)\n\t    assert expected_dict_cf1 == dbo.get_cf_options(cf1)\n\t    assert expected_dict_cf2 == dbo.get_cf_options(cf2)\n\t    assert dbo.get_cf_option(default, \"write_buffer_size\") == '1000'\n\t    assert dbo.get_cf_table_option(default, \"index_type\", ) == '1'\n\t    assert dbo.get_cf_option(cf1, \"write_buffer_size\") == '2000'\n", "    assert dbo.get_cf_table_option(cf1, \"index_type\", ) == '2'\n\t    dbo.set_cf_table_option(cf1, \"index_type\", '100')\n\t    assert dbo.get_cf_table_option(cf1, \"index_type\", ) == '100'\n\t    assert dbo.get_cf_option(cf2, \"write_buffer_size\") == '3000'\n\t    assert dbo.get_cf_table_option(cf2, \"index_type\") == '3'\n\t    dbo.set_cf_table_option(cf2, \"index_type\", 'XXXX')\n\t    assert dbo.get_cf_table_option(cf2, \"index_type\") == 'XXXX'\n\t    assert dbo.get_cf_option(default, \"index_type\") is None\n\t    dbo.set_cf_option(default, \"index_type\", \"200\", True)\n\t    assert dbo.get_cf_option(default, \"index_type\") == \"200\"\n", "    assert dbo.get_cf_table_option(cf1, \"write_buffer_size\") is None\n\t    expected_default_display_options = {'write_buffer_size': '1000',\n\t                                        'index_type': '200'}\n\t    expected_default_display_table_options = {'index_type': '1'}\n\t    expected_cf1_display_options = {'write_buffer_size': '2000'}\n\t    expected_cf1_display_table_options = {'index_type': '100'}\n\t    expected_cf2_display_options = {'write_buffer_size': '3000'}\n\t    expected_cf2_display_table_options = {'index_type': 'XXXX'}\n\t    assert dbo.get_cf_options_for_display(default) == \\\n\t           (expected_default_display_options,\n", "            expected_default_display_table_options)\n\t    assert dbo.get_cf_options_for_display(cf1) == \\\n\t           (expected_cf1_display_options,\n\t            expected_cf1_display_table_options)\n\t    assert dbo.get_cf_options_for_display(cf2) == \\\n\t           (expected_cf2_display_options,\n\t            expected_cf2_display_table_options)\n\tdef test_get_unified_options():\n\t    get_unified = db_opts.DatabaseOptions.get_unified_cfs_options\n\t    assert get_unified({}) == ({}, {})\n", "    cf1_options =\\\n\t        db_opts.FullNamesOptionsDict({\n\t            'CFOptions.write_buffer_size': {cf1: 1000}})\n\t    cfs_options = {cf1: cf1_options}\n\t    assert get_unified(cfs_options) == \\\n\t           ({\"CFOptions.write_buffer_size\": 1000},\n\t            {cf1: {}})\n\t    cf2_options =\\\n\t        db_opts.FullNamesOptionsDict({\n\t            'CFOptions.write_buffer_size': {cf2: 1000}})\n", "    cfs_options = {cf1: cf1_options, cf2: cf2_options}\n\t    assert get_unified(cfs_options) == \\\n\t           ({\"CFOptions.write_buffer_size\": 1000},\n\t            {cf1: {}, cf2: {}})\n\t    cf2_options.set_cf_option(cf2, \"write_buffer_size\", 2000)\n\t    assert get_unified(cfs_options) == \\\n\t           ({},\n\t            {cf1: {\"CFOptions.write_buffer_size\": 1000},\n\t             cf2: {\"CFOptions.write_buffer_size\": 2000}})\n\t    cf1_options.set_cf_option(cf1, \"cf1_only_option\", \"1234\")\n", "    assert get_unified(cfs_options) == \\\n\t           ({},\n\t            {cf1: {\"CFOptions.write_buffer_size\": 1000,\n\t                   \"CFOptions.cf1_only_option\": \"1234\"},\n\t             cf2: {\"CFOptions.write_buffer_size\": 2000}})\n\t    cf2_options.set_cf_option(cf2, \"cf2_only_option\", \"5678\")\n\t    assert get_unified(cfs_options) == \\\n\t           ({},\n\t            {cf1: {\"CFOptions.write_buffer_size\": 1000,\n\t                   \"CFOptions.cf1_only_option\": \"1234\"},\n", "             cf2: {\"CFOptions.write_buffer_size\": 2000,\n\t                   \"CFOptions.cf2_only_option\": \"5678\"}})\n\t    cf2_options.set_cf_option(cf2, \"write_buffer_size\", 1000)\n\t    assert get_unified(cfs_options) == \\\n\t           ({\"CFOptions.write_buffer_size\": 1000},\n\t            {cf1: {\"CFOptions.cf1_only_option\": \"1234\"},\n\t             cf2: {\"CFOptions.cf2_only_option\": \"5678\"}})\n\tdef test_options_diff_class():\n\t    diff = db_opts.OptionsDiff(baseline_options, new_options)\n\t    assert diff.get_diff_dict() == {}\n", "    diff.diff_in_base(cf2, 'CFOptions.write_buffer_size')\n\t    diff.diff_in_new(cf3, 'CFOptions.write_buffer_size')\n\t    diff.diff_between(default, 'CFOptions.write_buffer_size')\n\t    assert diff.get_diff_dict() == {\n\t        'CFOptions.write_buffer_size': {\n\t            cf2: ('128000000', db_opts.SANITIZED_NO_VALUE),\n\t            cf3: (db_opts.SANITIZED_NO_VALUE, '128000000'),\n\t            default: ('1024000', '128000000')\n\t        }\n\t    }\n", "def test_get_options_diff():\n\t    expected_diff = {\n\t        'DBOptions.stats_dump_freq_sec':\n\t            {utils.NO_CF: ('20', db_opts.SANITIZED_NO_VALUE)},\n\t        'DBOptions.enable_pipelined_write':\n\t            {utils.NO_CF: ('False', 'True')},\n\t        'bloom_bits': {utils.NO_CF: (db_opts.SANITIZED_NO_VALUE, '4')},\n\t        'CFOptions.write_buffer_size': {\n\t            default: ('1024000', '128000000'),\n\t            cf2: ('128000000', db_opts.SANITIZED_NO_VALUE),\n", "            cf3: (db_opts.SANITIZED_NO_VALUE, '128000000')},\n\t        'TableOptions.BlockBasedTable.index_type':\n\t            {'cf2': ('1', db_opts.SANITIZED_NO_VALUE)},\n\t        'TableOptions.BlockBasedTable.checksum':\n\t            {cf1: (db_opts.SANITIZED_NO_VALUE, 'True')},\n\t        'TableOptions.BlockBasedTable.format_version': {'cf1': ('4', '5')},\n\t        'DBOptions.max_log_file_size':\n\t            {utils.NO_CF: ('128000000', '0')}\n\t    }\n\t    full_name_baseline_dict = db_opts.FullNamesOptionsDict(baseline_options)\n", "    full_name_new_dict = db_opts.FullNamesOptionsDict(new_options)\n\t    diff = db_opts.DatabaseOptions.\\\n\t        get_options_diff(full_name_baseline_dict, full_name_new_dict)\n\t    assert diff.get_diff_dict() == expected_diff\n\tdef test_cfs_options_diff_class():\n\t    diff = db_opts.CfsOptionsDiff(baseline_options, default, new_options, cf1)\n\t    assert diff.get_diff_dict() == {}\n\t    diff.diff_between('CFOptions.write_buffer_size')\n\t    assert diff.get_diff_dict() == {\n\t        db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default,\n", "                                              \"New\": cf1},\n\t        'CFOptions.write_buffer_size': ('1024000', '128000')\n\t    }\n\tdef test_get_db_wide_options_diff():\n\t    assert db_opts.DatabaseOptions.\\\n\t               get_db_wide_options_diff(baseline, baseline) is None\n\t    expected_diff = {'bloom_bits': (db_opts.SANITIZED_NO_VALUE, '4'),\n\t                     'DBOptions.enable_pipelined_write': ('False', 'True'),\n\t                     'DBOptions.max_log_file_size': ('128000000', '0'),\n\t                     'DBOptions.stats_dump_freq_sec':\n", "                         ('20', db_opts.SANITIZED_NO_VALUE),\n\t                     db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n\t                         {\"Base\": 'DB_WIDE',\n\t                          \"New\": 'DB_WIDE'}}\n\t    assert db_opts.DatabaseOptions.get_db_wide_options_diff(baseline, new).\\\n\t        get_diff_dict() == expected_diff\n\tdef test_get_cfs_options_diff():\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(\n\t            baseline, utils.NO_CF, baseline, utils.NO_CF) is None\n", "    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(baseline, default, baseline, default) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(baseline, cf1, baseline, cf1) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(baseline, cf2, baseline, cf2) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(baseline, cf3, baseline, cf3) is None\n\t    assert db_opts.DatabaseOptions.get_cfs_options_diff(\n\t        new, utils.NO_CF,\n", "        new, utils.NO_CF) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(new, default, new, default) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(new, cf1, new, cf1) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(new, cf2, new, cf2) is None\n\t    assert db_opts.DatabaseOptions.\\\n\t           get_cfs_options_diff(new, cf3, new, cf3) is None\n\t    def compare_cf_actual_and_expected_diffs(actual_diff, expected_diff_dict):\n", "        assert isinstance(actual_diff, db_opts.CfsOptionsDiff)\n\t        assert isinstance(expected_diff_dict, dict)\n\t        assert actual_diff == expected_diff_dict\n\t        assert utils.are_dicts_equal_and_in_same_keys_order(\n\t            actual_diff.diff_dict, expected_diff)\n\t    expected_diff = {\n\t        'CFOptions.write_buffer_size': ('1024000', '128000000'),\n\t        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n\t            {\"Base\": default,\n\t             \"New\": default}\n", "    }\n\t    actual_diff = db_opts.DatabaseOptions.\\\n\t        get_cfs_options_diff(baseline, default, new, default)\n\t    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\t    expected_diff = {\n\t        'CFOptions.write_buffer_size': ('1024000', '128000'),\n\t        'TableOptions.BlockBasedTable.format_version':\n\t            (db_opts.SANITIZED_NO_VALUE, '5'),\n\t        'TableOptions.BlockBasedTable.block_size':\n\t            (db_opts.SANITIZED_NO_VALUE, '4096'),\n", "        'TableOptions.BlockBasedTable.checksum':\n\t            (db_opts.SANITIZED_NO_VALUE, 'True'),\n\t        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n\t            {\"Base\": default,\n\t             \"New\": cf1}\n\t    }\n\t    actual_diff = db_opts.DatabaseOptions.\\\n\t        get_cfs_options_diff(baseline, default, new, cf1)\n\t    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\t    expected_diff = {\n", "        'CFOptions.write_buffer_size': ('128000', '1024000'),\n\t        'TableOptions.BlockBasedTable.checksum':\n\t            ('True', db_opts.SANITIZED_NO_VALUE),\n\t        'TableOptions.BlockBasedTable.format_version':\n\t            ('5', db_opts.SANITIZED_NO_VALUE),\n\t        'TableOptions.BlockBasedTable.block_size':\n\t            ('4096', db_opts.SANITIZED_NO_VALUE),\n\t        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n\t            {\"Base\": cf1,\n\t             \"New\": default}\n", "    }\n\t    actual_diff = db_opts.DatabaseOptions. \\\n\t        get_cfs_options_diff(new, cf1, baseline, default)\n\t    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\t    expected_diff = {\n\t        'CFOptions.write_buffer_size':\n\t            ('128000000', db_opts.SANITIZED_NO_VALUE),\n\t        'TableOptions.BlockBasedTable.index_type':\n\t            ('1', db_opts.SANITIZED_NO_VALUE),\n\t        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n", "            {\"Base\": cf2,\n\t             \"New\": cf2},\n\t    }\n\t    actual_diff = db_opts.DatabaseOptions.\\\n\t        get_cfs_options_diff(baseline, cf2, new, cf2)\n\t    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\t    expected_diff = {\n\t        'CFOptions.write_buffer_size':\n\t            (db_opts.SANITIZED_NO_VALUE, '128000000'),\n\t        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n", "            {\"Base\": cf3,\n\t             \"New\": cf3}\n\t    }\n\t    actual_diff = db_opts.DatabaseOptions.\\\n\t        get_cfs_options_diff(baseline, cf3, new, cf3)\n\t    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\tdef test_get_unified_cfs_diffs():\n\t    cfs_diffs = {}\n\t    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == ({}, {})\n\t    cf1_vs_default_diff = {\n", "        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n\t            {\"Base\": default,\n\t             \"New\": cf1},\n\t        'CFOptions.write_buffer_size': ('1024000', '128000'),\n\t    }\n\t    cfs_diffs = [cf1_vs_default_diff]\n\t    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n\t        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n\t        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1}}])\n\t    cf2_vs_default_diff = {\n", "        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n\t            {\"Base\": default,\n\t             \"New\": cf2},\n\t        'CFOptions.write_buffer_size': ('1024000', '128000'),\n\t    }\n\t    cfs_diffs = [cf1_vs_default_diff, cf2_vs_default_diff]\n\t    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n\t        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n\t        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1}},\n\t         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2}}])\n", "    cf1_vs_default_diff['CFOptions.write_buffer_size'] = (1000, 2000)\n\t    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n\t        {},\n\t        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1},\n\t          'CFOptions.write_buffer_size': (1000, 2000)},\n\t         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2},\n\t          'CFOptions.write_buffer_size': ('1024000', '128000')}])\n\t    cf1_vs_default_diff['CFOptions.write_buffer_size'] = \\\n\t        cf2_vs_default_diff['CFOptions.write_buffer_size']\n\t    cf2_vs_default_diff['TableOptions.BlockBasedTable.cf2_option'] = (200, 300)\n", "    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n\t        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n\t        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1}},\n\t         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2},\n\t          'TableOptions.BlockBasedTable.cf2_option': (200, 300)}])\n\t    cf1_vs_default_diff['TableOptions.BlockBasedTable.cf1_option'] = (True,\n\t                                                                      False)\n\t    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n\t        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n\t        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1},\n", "          'TableOptions.BlockBasedTable.cf1_option': (True, False)},\n\t         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2},\n\t          'TableOptions.BlockBasedTable.cf2_option': (200, 300)}])\n"]}
{"filename": "test/test_csv_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport counters\n\timport csv_outputter\n\tfrom test.testing_utils import \\\n\t    add_stats_entry_lines_to_counters_mngr\n\tadd_stats_entry_lines_to_mngr = \\\n\t    add_stats_entry_lines_to_counters_mngr\n\tdef test_get_counters_csv():\n", "    counter1_entry_lines = [\n\t        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 0'''.splitlines(),\n\t        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 10'''.splitlines(),\n\t        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 0'''.splitlines()\n\t    ]\n\t    # All 0-s => Should be in the CSV at all\n\t    counter2_entry_lines = [\n", "        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter2 COUNT : 0'''.splitlines()\n\t    ]\n\t    # Has an entry only once. Later doesn't even have an entry\n\t    counter3_entry_lines = [\n\t        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter3 COUNT : 100'''.splitlines()\n\t    ]\n\t    mngr = counters.CountersMngr()\n\t    assert csv_outputter.get_counters_csv(mngr) is None\n", "    for entry in counter1_entry_lines:\n\t        add_stats_entry_lines_to_mngr(entry, mngr)\n\t    for entry in counter2_entry_lines:\n\t        add_stats_entry_lines_to_mngr(entry, mngr)\n\t    for entry in counter3_entry_lines:\n\t        add_stats_entry_lines_to_mngr(entry, mngr)\n\t    expected_csv = 'Time,counter1,counter3\\r\\n'\\\n\t                   '2022/11/24-15:50:09.512106,0,0\\r\\n'\\\n\t                   '2022/11/24-15:50:10.512106,10,0\\r\\n'\\\n\t                   '2022/11/24-15:50:12.512106,0,100\\r\\n'\n", "    assert csv_outputter.get_counters_csv(mngr) == expected_csv\n\tdef test_get_human_readable_histograms_csv():\n\t    counter1_entry_lines = [\n\t        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n\t        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 1 SUM : 10'''.splitlines(), # noqa\n\t        '''2022/11/24-15:50:11.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 7 SUM : 24'''.splitlines(), # noqa\n\t    ]\n", "    # All 0-s => Should be in the CSV at all\n\t    counter2_entry_lines = [\n\t        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter2 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n\t    ]\n\t    # Has an entry only once. Later doesn't even have an entry\n\t    counter3_entry_lines = [\n\t        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter3 P50 : 0.500000 P95 : 0.500000 P99 : 0.000000 P100 : 0.000000 COUNT : 12 SUM : 36'''.splitlines(), # noqa\n\t    ]\n", "    mngr = counters.CountersMngr()\n\t    assert csv_outputter.get_counters_csv(mngr) is None\n\t    for entry in counter1_entry_lines:\n\t        add_stats_entry_lines_to_mngr(entry, mngr)\n\t    for entry in counter2_entry_lines:\n\t        add_stats_entry_lines_to_mngr(entry, mngr)\n\t    for entry in counter3_entry_lines:\n\t        add_stats_entry_lines_to_mngr(entry, mngr)\n\t    csv = csv_outputter.get_human_readable_histogram_csv(mngr)\n\t    assert csv == \\\n", "        ',counter1,.,.,.,.,.,counter3,.,.,.,.,.\\r\\n' \\\n\t        ',P50,P95,P99,P100,Count,Sum,P50,P95,P99,P100,Count,Sum\\r\\n' \\\n\t        '2022/11/24-15:50:09.512106,0.0,0.0,0.0,0.0,0,0,0.0,0,0,0,0,0,0,0,0\\r\\n' \\\n\t        '2022/11/24-15:50:10.512106,1.0,0.0,0.0,0.0,1,10,10.0,1,10,0,0,0,0,0,0\\r\\n' \\\n\t        '2022/11/24-15:50:11.512106,1.0,0.0,0.0,0.0,7,24,3.43,6,14,0,0,0,0,0,0\\r\\n' \\\n\t        '2022/11/24-15:50:12.512106,0,0,0,0,0,0,0.5,0.5,0.0,0.0,12,36,3.0,12,36\\r\\n' # noqa\n\t    csv = csv_outputter.get_tools_histogram_csv(mngr)\n\t    assert csv == \\\n\t        'Name,Time,P50,P95,P99,P100,Count,Sum,Average,Interval Count,Interval Sum\\r\\n' \\\n\t        'counter1,2022/11/24-15:50:09.512106,0.0,0.0,0.0,0.0,0,0,0.0,0,0\\r\\n' \\\n", "        'counter1,2022/11/24-15:50:10.512106,1.0,0.0,0.0,0.0,1,10,10.0,1,10\\r\\n' \\\n\t        'counter1,2022/11/24-15:50:11.512106,1.0,0.0,0.0,0.0,7,24,3.43,6,14\\r\\n' \\\n\t        'counter1,2022/11/24-15:50:12.512106\\r\\n' \\\n\t        'counter3,2022/11/24-15:50:09.512106,0,0,0,0,0,0,0,0,0\\r\\n' \\\n\t        'counter3,2022/11/24-15:50:10.512106,0,0,0,0,0,0,0,0,0\\r\\n' \\\n\t        'counter3,2022/11/24-15:50:11.512106,0,0,0,0,0,0,0,0,0\\r\\n' \\\n\t        'counter3,2022/11/24-15:50:12.512106,0.5,0.5,0.0,0.0,12,36,3.0,12,36\\r\\n' # noqa\n\tdef test_process_compactions_csv_header():\n\t    test_func = csv_outputter.process_compactions_csv_header\n\t    result_type = csv_outputter.CompactionsCsvInputFilesInfo\n", "    assert test_func([]) is None\n\t    assert test_func([\"files_L\", \"files1\"]) is None\n\t    assert test_func([\"files_\"]) is None\n\t    assert test_func([\"files_L0\"]) == \\\n\t           result_type([\"Input Level Files\", \"Input Files from Output Level\"],\n\t                       first_column_idx=0,\n\t                       first_level=0,\n\t                       second_level=None)\n\t    assert test_func([\"files_L1\", \"files_L2\"]) == \\\n\t           result_type([\"Input Level Files\", \"Input Files from Output Level\"],\n", "                       first_column_idx=0,\n\t                       first_level=1,\n\t                       second_level=2)\n\t    assert test_func([\"COL1\", \"files_L1\", \"files_L2\"]) == \\\n\t           result_type(\n\t               [\"COL1\", \"Input Level Files\", \"Input Files from Output Level\"],\n\t               first_column_idx=1,\n\t               first_level=1,\n\t               second_level=2)\n\t    assert test_func([\"COL1\", \"files_L1\", \"COL2\"]) == \\\n", "           result_type(\n\t               [\"COL1\", \"Input Level Files\", \"Input Files from Output \"\n\t                                             \"Level\", \"COL2\"],\n\t               first_column_idx=1,\n\t               first_level=1,\n\t               second_level=None)\n\t    assert test_func([\"COL1\", \"files_L10\", \"files_L20\", \"files_L30\"]) == \\\n\t           result_type(\n\t               [\"COL1\", \"Input Level Files\", \"Input Files from Output Level\"],\n\t               first_column_idx=1,\n", "               first_level=10,\n\t               second_level=20)\n\t    # Non Consecutive \"files_<Level>\" columns\n\t    assert test_func([\"COL1\", \"files_L10\", \"COL2\", \"files_L20\", \"files_L30\",\n\t                      \"COL4\", \"COL5\", \"files_40\"]) is None\n"]}
{"filename": "test/sample_log_info.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\tfrom datetime import timedelta\n\timport utils\n\tclass SampleLogInfo:\n\t    FILE_PATH = \"input_files/LOG_sample\"\n\t    START_TIME = \"2022/04/17-14:13:10.723796\"\n\t    END_TIME = \"2022/04/17-14:14:32.645120\"\n\t    PRODUCT_NAME = \"SpeeDB\"\n", "    GIT_HASH = \"UNKNOWN:0a396684d6c08f6fe4a37572c0429d91176c51d1\"\n\t    VERSION = \"6.22.1\"\n\t    NUM_ENTRIES = 73\n\t    CF_DEFAULT = 'default'\n\t    CF_NAMES = [CF_DEFAULT, '_sample/CF_1', '_sample/CF-2', '']\n\t    DB_WIDE_OPTIONS_START_ENTRY_IDX = 7\n\t    SUPPORT_INFO_START_ENTRY_IDX = 15\n\t    CF_DEFAULT_LEVEL_SIZES = {\n\t        0: utils.get_num_bytes_from_human_readable_components(\"149.99\", \"MB\"),\n\t        1: utils.get_num_bytes_from_human_readable_components(\"271.79\", \"MB\"),\n", "        2: utils.get_num_bytes_from_human_readable_components(\"2.73\", \"GB\"),\n\t        3: utils.get_num_bytes_from_human_readable_components(\"24.96\", \"GB\"),\n\t        4: utils.get_num_bytes_from_human_readable_components(\"54.33\", \"GB\"),\n\t    }\n\t    CF_SIZE_BYTES = \\\n\t        {\n\t            'default':\n\t                utils.get_num_bytes_from_human_readable_components(\n\t                    \"82.43\", \"GB\"),\n\t            '_sample/CF_1': None,\n", "            '_sample/CF-2': None,\n\t            '': None,\n\t        }\n\t    DB_SIZE_BYTES = sum([size if size is not None else 0 for size in\n\t                         CF_SIZE_BYTES.values()])\n\t    NUM_WARNS = 1\n\t    OPTIONS_ENTRIES_INDICES = [21, 33, 42, 50]\n\t    TABLE_OPTIONS_ENTRIES_INDICES = [24, 38, 45, 55]\n\t    DB_WIDE_OPTIONS_DICT = {\n\t        'error_if_exists': '0',\n", "        'create_if_missing': '1',\n\t        'db_log_dir': '',\n\t        'wal_dir': '',\n\t        'track_and_verify_wals_in_manifest': '0',\n\t        'env': '0x7f4a9117d5c0',\n\t        'fs': 'Posix File System',\n\t        'info_log': '0x7f4af4020bf0'\n\t    }\n\t    DEFAULT_OPTIONS_DICT = {\n\t        'comparator': 'leveldb.BytewiseComparator',\n", "        'merge_operator': 'StringAppendTESTOperator',\n\t        'write_buffer_size': '67108864',\n\t        'max_write_buffer_number': '2',\n\t        'ttl': '2592000'\n\t    }\n\t    SAMPLE_CF1_OPTIONS_DICT = {\n\t        'comparator': 'leveldb.BytewiseComparator-XXX',\n\t        'merge_operator': 'StringAppendTESTOperator-XXX',\n\t        'compaction_filter': 'None',\n\t        'table_factory': 'BlockBasedTable',\n", "        'write_buffer_size': '67108864',\n\t        'max_write_buffer_number': '2',\n\t    }\n\t    SAMPLE_CF2_OPTIONS_DICT = {\n\t        'comparator': 'leveldb.BytewiseComparator-YYY',\n\t        'table_factory': 'BlockBasedTable-YYY',\n\t        'write_buffer_size': '123467108864',\n\t        'max_write_buffer_number': '10',\n\t        'compression': 'Snappy'\n\t    }\n", "    EMPTY_CF_OPTIONS_DICT = {\n\t        'comparator': 'leveldb.BytewiseComparator-ZZZ',\n\t        'merge_operator': 'StringAppendTESTOperator-ZZZ',\n\t        'compaction_filter': 'None',\n\t        'table_factory': 'BlockBasedTable'\n\t    }\n\t    DEFAULT_TABLE_OPTIONS_DICT = {\n\t        'flush_block_policy_factory':\n\t            'FlushBlockBySizePolicyFactory (0x7f4af401f570)',\n\t        'cache_index_and_filter_blocks': '1',\n", "        'cache_index_and_filter_blocks_with_high_priority': '1',\n\t        'pin_l0_filter_and_index_blocks_in_cache': '1',\n\t        'pin_top_level_index_and_filter': '1',\n\t        'metadata_cache_top_level_index_pinning': '0',\n\t        'block_cache_capacity': '209715200',\n\t        'block_cache_compressed': '(nil)',\n\t        'prepopulate_block_cache': '0'}\n\t    SAMPLE_CF1_TABLE_OPTIONS_DICT = {\n\t        'flush_block_policy_factory':\n\t            'FlushBlockBySizePolicyFactory (0x7f4af4031090)',\n", "        'pin_top_level_index_and_filter': '1',\n\t        'metadata_cache_unpartitioned_pinning': '3',\n\t        'no_block_cache': '0',\n\t        'block_cache': '0x7f4bc07214d0',\n\t        'block_cache_memory_allocator': 'None',\n\t        'block_cache_high_pri_pool_ratio': '0.100',\n\t        'block_cache_compressed': '(nil)'}\n\t    SAMPLE_CF2_TABLE_OPTIONS_DICT = {\n\t        'flush_block_policy_factory':\n\t            'FlushBlockBySizePolicyFactory (0x7f4af4091b90)',\n", "        'cache_index_and_filter_blocks': '1',\n\t        'cache_index_and_filter_blocks_with_high_priority': '1'\n\t    }\n\t    EMPTY_CF_TABLE_OPTIONS_DICT = {\n\t        'flush_block_policy_factory':\n\t            'FlushBlockBySizePolicyFactory (0x7f4af4030f30)',\n\t        'pin_top_level_index_and_filter': '1'}\n\t    OPTIONS_DICTS = [\n\t        DEFAULT_OPTIONS_DICT,\n\t        SAMPLE_CF1_OPTIONS_DICT,\n", "        SAMPLE_CF2_OPTIONS_DICT,\n\t        EMPTY_CF_OPTIONS_DICT\n\t    ]\n\t    TABLE_OPTIONS_DICTS = [\n\t        DEFAULT_TABLE_OPTIONS_DICT,\n\t        SAMPLE_CF1_TABLE_OPTIONS_DICT,\n\t        SAMPLE_CF2_TABLE_OPTIONS_DICT,\n\t        EMPTY_CF_TABLE_OPTIONS_DICT\n\t    ]\n\t    DB_STATS_ENTRY_TIME = \"2022/04/17-14:14:28.645150\"\n", "    CUMULATIVE_DURATION = \\\n\t        timedelta(hours=12, minutes=10, seconds=56, milliseconds=123)\n\t    INTERVAL_DURATION = \\\n\t        timedelta(hours=45, minutes=34, seconds=12, milliseconds=789)\n\t    DB_WIDE_STALLS_ENTRIES = \\\n\t        {DB_STATS_ENTRY_TIME: {\"cumulative_duration\": CUMULATIVE_DURATION,\n\t                               \"cumulative_percent\": 98.7,\n\t                               \"interval_duration\": INTERVAL_DURATION,\n\t                               \"interval_percent\": 12.3}}\n\t    EVENTS_HISTOGRAM = {'default': {\"table_file_creation\": 2},\n", "                        '_sample/CF_1': {},\n\t                        '_sample/CF-2': {},\n\t                        '': {'flush_started': 1,\n\t                             \"table_file_creation\": 1}}\n\tclass SampleRolledLogInfo:\n\t    FILE_PATH = \"input_files/Rolled_LOG_sample.txt\"\n\t    START_TIME = \"2022/04/17-14:13:10.723796\"\n\t    END_TIME = \"2022/04/17-14:14:32.645120\"\n\t    PRODUCT_NAME = \"SpeeDB\"\n\t    GIT_HASH = \"UNKNOWN:0a396684d6c08f6fe4a37572c0429d91176c51d1\"\n", "    VERSION = \"6.22.1\"\n\t    NUM_ENTRIES = 59\n\t    CF_NAMES = [\"default\", \"\", \"CF1\"]\n\t    AUTO_GENERATED_CF_NAMES = [\"Unknown-CF-#1\",\n\t                               \"Unknown-CF-#2\",\n\t                               \"Unknown-CF-#3\"]\n\t    DB_WIDE_OPTIONS_START_ENTRY_IDX = 7\n\t    SUPPORT_INFO_START_ENTRY_IDX = 15\n\t    NUM_WARNS = 1\n\t    OPTIONS_ENTRIES_INDICES = [19, 25, 32, 38]\n", "    TABLE_OPTIONS_ENTRIES_INDICES = [21, 29, 34, 42]\n\t    DB_WIDE_OPTIONS_DICT = SampleLogInfo.DB_WIDE_OPTIONS_DICT\n\t    DEFAULT_OPTIONS_DICT = SampleLogInfo.DEFAULT_OPTIONS_DICT\n\t    SAMPLE_CF1_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF1_OPTIONS_DICT\n\t    SAMPLE_CF2_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF2_OPTIONS_DICT\n\t    EMPTY_CF_OPTIONS_DICT = SampleLogInfo.EMPTY_CF_OPTIONS_DICT\n\t    DEFAULT_TABLE_OPTIONS_DICT = SampleLogInfo.DEFAULT_TABLE_OPTIONS_DICT\n\t    SAMPLE_CF1_TABLE_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF1_TABLE_OPTIONS_DICT\n\t    SAMPLE_CF2_TABLE_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF2_TABLE_OPTIONS_DICT\n\t    EMPTY_CF_TABLE_OPTIONS_DICT = SampleLogInfo.EMPTY_CF_TABLE_OPTIONS_DICT\n", "    OPTIONS_DICTS = [\n\t        DEFAULT_OPTIONS_DICT,\n\t        SAMPLE_CF1_OPTIONS_DICT,\n\t        SAMPLE_CF2_OPTIONS_DICT,\n\t        EMPTY_CF_OPTIONS_DICT\n\t    ]\n\t    TABLE_OPTIONS_DICTS = [\n\t        DEFAULT_TABLE_OPTIONS_DICT,\n\t        SAMPLE_CF1_TABLE_OPTIONS_DICT,\n\t        SAMPLE_CF2_TABLE_OPTIONS_DICT,\n", "        EMPTY_CF_TABLE_OPTIONS_DICT\n\t    ]\n\t    DB_STATS_ENTRY_TIME = \"2022/04/17-14:14:28.645150\"\n\t    CUMULATIVE_DURATION = \\\n\t        timedelta(hours=12, minutes=10, seconds=56, milliseconds=123)\n\t    INTERVAL_DURATION = \\\n\t        timedelta(hours=45, minutes=34, seconds=12, milliseconds=789)\n\t    DB_WIDE_STALLS_ENTRIES = \\\n\t        {DB_STATS_ENTRY_TIME: {\"cumulative_duration\": CUMULATIVE_DURATION,\n\t                               \"cumulative_percent\": 98.7,\n", "                               \"interval_duration\": INTERVAL_DURATION,\n\t                               \"interval_percent\": 12.3}}\n"]}
{"filename": "test/test_counters.py", "chunked_list": ["import counters\n\tfrom log_entry import LogEntry\n\tfrom test.testing_utils import \\\n\t    add_stats_entry_lines_to_counters_mngr\n\tadd_stats_entry_lines_to_mngr = \\\n\t    add_stats_entry_lines_to_counters_mngr\n\tdef test_stats_counter_and_histograms_is_your_entry():\n\t    lines = \\\n\t        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n\t        rocksdb.block.cache.miss COUNT : 61\n", "        '''.splitlines()  # noqa\n\t    entry = LogEntry(0, lines[0])\n\t    entry.add_line(lines[1], True)\n\t    assert counters.CountersMngr.is_your_entry(entry)\n\tdef test_counters_mngr():\n\t    entry_lines = \\\n\t        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n\t         rocksdb.block.cache.miss COUNT : 61\n\t        rocksdb.block.cache.hit COUNT : 0\n\t        rocksdb.block.cache.add COUNT : 0\n", "        rocksdb.block.cache.data.miss COUNT : 71\n\t        rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0\n\t        rocksdb.read.block.compaction.micros P50 : 1.321429 P95 : 3.650000 P99 : 17.000000 P100 : 17.000000 COUNT : 67 SUM : 140\n\t        rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines()  # noqa\n\t    mngr = counters.CountersMngr()\n\t    assert mngr.get_counters_names() == []\n\t    assert mngr.get_counters_times() == []\n\t    assert mngr.get_histogram_counters_names() == []\n\t    assert mngr.get_histogram_counters_times() == []\n\t    add_stats_entry_lines_to_mngr(entry_lines, mngr)\n", "    assert mngr.get_counter_entries(\"rocksdb.block.cache.hit\") == \\\n\t           [{'time': '2022/11/24-15:58:09.512106',\n\t             'value': 0}]\n\t    assert mngr.get_counter_entries(\"rocksdb.block.cache.data.miss\") == \\\n\t           [{'time': '2022/11/24-15:58:09.512106',\n\t             'value': 71}]\n\t    assert mngr.get_counter_entries('XXXXX') == {}\n\t    assert mngr.get_counters_names() == ['rocksdb.block.cache.miss',\n\t                                         'rocksdb.block.cache.hit',\n\t                                         'rocksdb.block.cache.add',\n", "                                         'rocksdb.block.cache.data.miss']\n\t    assert mngr.get_counters_times() == ['2022/11/24-15:58:09.512106']\n\t    assert mngr.get_histogram_entries('rocksdb.db.get.micros') == \\\n\t           [{'time': '2022/11/24-15:58:09.512106',\n\t             'values': {'P100': 0.0,\n\t                        'P50': 0.0,\n\t                        'P95': 0.0,\n\t                        'P99': 0.0,\n\t                        'Count': 0,\n\t                        'Sum': 0,\n", "                        'Average': 0.0,\n\t                        'Interval Count': 0,\n\t                        'Interval Sum': 0}}]\n\t    assert \\\n\t        mngr.get_histogram_entries('rocksdb.read.block.compaction.micros') ==\\\n\t        [{'time': '2022/11/24-15:58:09.512106',\n\t          'values': {'P100': 17.0,\n\t                     'P50': 1.321429,\n\t                     'P95': 3.65,\n\t                     'P99': 17.0,\n", "                     'Count': 67,\n\t                     'Sum': 140,\n\t                     'Average': 2.09,\n\t                     'Interval Count': 67,\n\t                     'Interval Sum': 140}}]\n\t    assert mngr.get_histogram_counters_names() == [\n\t        'rocksdb.db.get.micros',\n\t        'rocksdb.read.block.compaction.micros',\n\t        'rocksdb.blobdb.next.micros'\n\t    ]\n", "    assert mngr.get_histogram_counters_times() == \\\n\t           ['2022/11/24-15:58:09.512106']\n\t    assert mngr.get_histogram_entries('YYYY') == {}\n\t    entry_lines1 = \\\n\t        '''2022/11/24-15:58:10.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n\t         rocksdb.block.cache.miss COUNT : 61\n\t        rocksdb.block.cache.hit COUNT : 10\n\t        rocksdb.block.cache.add COUNT : 20\n\t        rocksdb.block.cache.data.miss COUNT : 81\n\t        rocksdb.db.get.micros P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0\n", "        rocksdb.read.block.compaction.micros P50 : 1.321429 P95 : 3.650000 P99 : 17.000000 P100 : 17.000000 COUNT : 75 SUM : 180\n\t        rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines()  # noqa\n\t    entry1 = LogEntry(0, entry_lines1[0])\n\t    for line in entry_lines1[1:]:\n\t        entry1.add_line(line)\n\t    mngr.add_entry(entry1.all_lines_added())\n\t    assert mngr.get_all_counters_entries() == {\n\t        \"rocksdb.block.cache.miss\": [{'time': '2022/11/24-15:58:09.512106',\n\t                                      'value': 61},\n\t                                     {'time': '2022/11/24-15:58:10.512106',\n", "                                      'value': 61}],\n\t        \"rocksdb.block.cache.hit\": [{'time': '2022/11/24-15:58:09.512106',\n\t                                     'value': 0},\n\t                                    {'time': '2022/11/24-15:58:10.512106',\n\t                                     'value': 10}],\n\t        \"rocksdb.block.cache.add\": [{'time': '2022/11/24-15:58:09.512106',\n\t                                     'value': 0},\n\t                                    {'time': '2022/11/24-15:58:10.512106',\n\t                                     'value': 20}],\n\t        \"rocksdb.block.cache.data.miss\":\n", "            [{'time': '2022/11/24-15:58:09.512106',\n\t              'value': 71},\n\t             {'time': '2022/11/24-15:58:10.512106',\n\t              'value': 81}]\n\t    }\n\t    assert mngr.get_all_histogram_entries() == {\n\t        \"rocksdb.db.get.micros\":\n\t            [{'time': '2022/11/24-15:58:09.512106',\n\t              'values': {'P100': 0.0,\n\t                         'P50': 0.0,\n", "                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 0.0,\n\t                         'Count': 0,\n\t                         'Sum': 0,\n\t                         'Interval Count': 0,\n\t                         'Interval Sum': 0}},\n\t             {'time': '2022/11/24-15:58:10.512106',\n\t              'values': {'P100': 0.0,\n\t                         'P50': 0.0,\n", "                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 0.0,\n\t                         'Count': 0,\n\t                         'Sum': 0,\n\t                         'Interval Count': 0,\n\t                         'Interval Sum': 0}}],\n\t        \"rocksdb.read.block.compaction.micros\":\n\t            [{'time': '2022/11/24-15:58:09.512106',\n\t              'values': {'P100': 17.0,\n", "                         'P50': 1.321429,\n\t                         'P95': 3.65,\n\t                         'P99': 17.0,\n\t                         'Average': 2.09,\n\t                         'Count': 67,\n\t                         'Sum': 140,\n\t                         'Interval Count': 67,\n\t                         'Interval Sum': 140}},\n\t             {'time': '2022/11/24-15:58:10.512106',\n\t              'values': {'P100': 17.0,\n", "                         'P50': 1.321429,\n\t                         'P95': 3.65,\n\t                         'P99': 17.0,\n\t                         'Average': 2.4,\n\t                         'Count': 75,\n\t                         'Sum': 180,\n\t                         'Interval Count': 8,\n\t                         'Interval Sum': 40}}],\n\t        \"rocksdb.blobdb.next.micros\":\n\t            [{'time': '2022/11/24-15:58:09.512106',\n", "              'values': {'P100': 0.0,\n\t                         'P50': 0.0,\n\t                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 0.0,\n\t                         'Count': 0,\n\t                         'Sum': 0,\n\t                         'Interval Count': 0,\n\t                         'Interval Sum': 0}},\n\t             {'time': '2022/11/24-15:58:10.512106',\n", "              'values': {'P100': 0.0,\n\t                         'P50': 0.0,\n\t                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 0.0,\n\t                         'Count': 0,\n\t                         'Sum': 0,\n\t                         'Interval Count': 0,\n\t                         'Interval Sum': 0}}]\n\t    }\n", "    assert mngr.get_histogram_counters_names() == [\n\t        'rocksdb.db.get.micros',\n\t        'rocksdb.read.block.compaction.micros',\n\t        'rocksdb.blobdb.next.micros'\n\t    ]\n\t    assert mngr.get_histogram_counters_times() == [\n\t        '2022/11/24-15:58:09.512106',\n\t        '2022/11/24-15:58:10.512106'\n\t    ]\n\tdef test_counters_zero_handling():\n", "    # Just for testing, allowing the counter to return to 0 after being > 0\n\t    entry_lines = [\n\t        '''2022/11/24-15:50:08.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 0'''.splitlines(),\n\t        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 0'''.splitlines(),\n\t        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 10'''.splitlines(),\n\t        '''2022/11/24-15:50:11.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 11'''.splitlines(),\n", "        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 COUNT : 0'''.splitlines()\n\t    ]\n\t    mngr = counters.CountersMngr()\n\t    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == []\n\t    assert mngr.are_all_counter_entries_zero(\"counter1\")\n\t    assert mngr.get_counters_entries_not_all_zeroes() == {}\n\t    add_stats_entry_lines_to_mngr(entry_lines[0], mngr)\n\t    assert mngr.are_all_counter_entries_zero(\"counter1\")\n\t    assert mngr.get_counters_entries_not_all_zeroes() == {}\n", "    add_stats_entry_lines_to_mngr(entry_lines[1], mngr)\n\t    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == []\n\t    assert mngr.are_all_counter_entries_zero(\"counter1\")\n\t    assert mngr.get_counters_entries_not_all_zeroes() == {}\n\t    add_stats_entry_lines_to_mngr(entry_lines[2], mngr)\n\t    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == \\\n\t           [{'time': '2022/11/24-15:50:10.512106', 'value': 10}]\n\t    assert not mngr.are_all_counter_entries_zero(\"counter1\")\n\t    assert mngr.get_counters_entries_not_all_zeroes() == \\\n\t           {'counter1': [{'time': '2022/11/24-15:50:08.512106', 'value': 0},\n", "                         {'time': '2022/11/24-15:50:09.512106', 'value': 0},\n\t                         {'time': '2022/11/24-15:50:10.512106', 'value': 10}]}\n\t    add_stats_entry_lines_to_mngr(entry_lines[3], mngr)\n\t    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == \\\n\t           [{'time': '2022/11/24-15:50:10.512106', 'value': 10},\n\t            {'time': '2022/11/24-15:50:11.512106', 'value': 11}]\n\t    assert not mngr.are_all_counter_entries_zero(\"counter1\")\n\t    assert mngr.get_counters_entries_not_all_zeroes() == \\\n\t           {'counter1': [{'time': '2022/11/24-15:50:08.512106', 'value': 0},\n\t                         {'time': '2022/11/24-15:50:09.512106', 'value': 0},\n", "                         {'time': '2022/11/24-15:50:10.512106', 'value': 10},\n\t                         {'time': '2022/11/24-15:50:11.512106', 'value': 11}]}\n\t    add_stats_entry_lines_to_mngr(entry_lines[4], mngr)\n\t    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == \\\n\t           [{'time': '2022/11/24-15:50:10.512106', 'value': 10},\n\t            {'time': '2022/11/24-15:50:11.512106', 'value': 11}]\n\t    assert not mngr.are_all_counter_entries_zero(\"counter1\")\n\t    assert mngr.get_counters_entries_not_all_zeroes() == \\\n\t           {'counter1': [{'time': '2022/11/24-15:50:08.512106', 'value': 0},\n\t                         {'time': '2022/11/24-15:50:09.512106', 'value': 0},\n", "                         {'time': '2022/11/24-15:50:10.512106', 'value': 10},\n\t                         {'time': '2022/11/24-15:50:11.512106', 'value': 11}]}\n\tdef test_histograms_zero_handling():\n\t    # Just for testing, allowing the counter to return to 0 after being > 0\n\t    entry_lines = [\n\t        '''2022/11/24-15:50:08.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n\t        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n\t        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n", "        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 1 SUM : 10'''.splitlines(), # noqa\n\t        '''2022/11/24-15:50:11.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 7 SUM : 24'''.splitlines(), # noqa\n\t        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n\t        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines() # noqa\n\t    ]\n\t    mngr = counters.CountersMngr()\n\t    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == []\n\t    assert mngr.are_all_histogram_entries_zero(\"counter1\")\n\t    assert mngr.get_histogram_entries_not_all_zeroes() == {}\n", "    add_stats_entry_lines_to_mngr(entry_lines[0], mngr)\n\t    assert mngr.are_all_histogram_entries_zero(\"counter1\")\n\t    assert mngr.get_histogram_entries_not_all_zeroes() == {}\n\t    add_stats_entry_lines_to_mngr(entry_lines[1], mngr)\n\t    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == []\n\t    assert mngr.are_all_histogram_entries_zero(\"counter1\")\n\t    assert mngr.get_histogram_entries_not_all_zeroes() == {}\n\t    add_stats_entry_lines_to_mngr(entry_lines[2], mngr)\n\t    expected_non_zero_entries = \\\n\t        [{'time': '2022/11/24-15:50:10.512106',\n", "          'values': {'P100': 0.0,\n\t                     'P50': 1.0,\n\t                     'P95': 0.0,\n\t                     'P99': 0.0,\n\t                     'Average': 10.0,\n\t                     'Count': 1,\n\t                     'Sum': 10,\n\t                     'Interval Count': 1,\n\t                     'Interval Sum': 10}}]\n\t    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == \\\n", "           expected_non_zero_entries\n\t    assert not mngr.are_all_histogram_entries_zero(\"counter1\")\n\t    expected_not_all_zeroes = {\n\t        'counter1':\n\t            [{'time': '2022/11/24-15:50:08.512106',\n\t              'values': {'P100': 0.0,\n\t                         'P50': 0.0,\n\t                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 0.0,\n", "                         'Count': 0,\n\t                         'Sum': 0,\n\t                         'Interval Count': 0,\n\t                         'Interval Sum': 0}},\n\t             {'time': '2022/11/24-15:50:09.512106',\n\t              'values': {'P100': 0.0,\n\t                         'P50': 0.0,\n\t                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 0.0,\n", "                         'Count': 0,\n\t                         'Sum': 0,\n\t                         'Interval Count': 0,\n\t                         'Interval Sum': 0}},\n\t             {'time': '2022/11/24-15:50:10.512106',\n\t              'values': {'P100': 0.0,\n\t                         'P50': 1.0,\n\t                         'P95': 0.0,\n\t                         'P99': 0.0,\n\t                         'Average': 10.0,\n", "                         'Count': 1,\n\t                         'Sum': 10,\n\t                         'Interval Count': 1,\n\t                         'Interval Sum': 10}}\n\t             ]\n\t    }\n\t    assert mngr.get_histogram_entries_not_all_zeroes() == \\\n\t           expected_not_all_zeroes\n\t    add_stats_entry_lines_to_mngr(entry_lines[3], mngr)\n\t    expected_non_zero_entries.append(\n", "        {'time': '2022/11/24-15:50:11.512106',\n\t         'values': {'P100': 0.0,\n\t                    'P50': 1.0,\n\t                    'P95': 0.0,\n\t                    'P99': 0.0,\n\t                    'Average': 3.43,\n\t                    'Count': 7,\n\t                    'Sum': 24,\n\t                    'Interval Count': 6,\n\t                    'Interval Sum': 14}})\n", "    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == \\\n\t           expected_non_zero_entries\n\t    assert not mngr.are_all_histogram_entries_zero(\"counter1\")\n\t    expected_not_all_zeroes[\"counter1\"].append(\n\t        {'time': '2022/11/24-15:50:11.512106',\n\t         'values': {'P100': 0.0,\n\t                    'P50': 1.0,\n\t                    'P95': 0.0,\n\t                    'P99': 0.0,\n\t                    'Average': 3.43,\n", "                    'Count': 7,\n\t                    'Sum': 24,\n\t                    'Interval Count': 6,\n\t                    'Interval Sum': 14}})\n\t    assert mngr.get_histogram_entries_not_all_zeroes() == \\\n\t           expected_not_all_zeroes\n\t    # Line 4's count / sum are < line 3 => Line is ignored\n\t    add_stats_entry_lines_to_mngr(entry_lines[4], mngr)\n\t    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == \\\n\t           expected_non_zero_entries\n", "    assert not mngr.are_all_histogram_entries_zero(\"counter1\")\n\t    assert mngr.get_histogram_entries_not_all_zeroes() == \\\n\t           expected_not_all_zeroes\n\tdef test_badly_formed_counters_and_histograms_entries():\n\t    # There is only a single valid counter line and a single histogram line\n\t    # Issues:\n\t    # - spaces within a counter / histogram name\n\t    # - missing values\n\t    # - Unexpected text at the end of the line\n\t    entry_lines = \\\n", "        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n\t        rocksdb.block.cache.miss COUNT : 61\n\t        rocksdb. block.cache.hit COUNT : 100\n\t        rocksdb.block.cache.XXX COUNT : \n\t        rocksdb.block.cache.YYY COUNT : 61  UNEXPECTED TEXT\n\t        rocksdb. db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0\n\t        rocksdb.db.get.micros.XXX P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM :\n\t        rocksdb.db.get.micros.XXX P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM :            UNEXPECTED TEXT\n\t        rocksdb.read.block.compaction.micros P50 : 1.321429 P95 : 3.650000 P99 : 17.000000 P100 : 17.000000 COUNT : 75 SUM : 180'''.splitlines()  # noqa\n\t    entry = LogEntry(0, entry_lines[0])\n", "    for line in entry_lines[1:]:\n\t        entry.add_line(line)\n\t    mngr = counters.CountersMngr()\n\t    mngr.add_entry(entry.all_lines_added())\n\t    assert mngr.get_all_counters_entries() == \\\n\t           {\"rocksdb.block.cache.miss\": [{'time': '2022/11/24-15:58:09.512106',\n\t                                          'value': 61}]}\n\t    assert mngr.get_all_histogram_entries() ==\\\n\t           {\"rocksdb.read.block.compaction.micros\":\n\t            [{'time': '2022/11/24-15:58:09.512106',\n", "              'values': {'P100': 17.0,\n\t                         'P50': 1.321429,\n\t                         'P95': 3.65,\n\t                         'P99': 17.0,\n\t                         'Average': 2.4,\n\t                         'Count': 75,\n\t                         'Sum': 180,\n\t                         'Interval Count': 75,\n\t                         'Interval Sum': 180}}]\n\t            }\n"]}
{"filename": "test/test_events.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\tfrom datetime import datetime, timedelta\n\timport pytest\n\timport events\n\timport utils\n\tfrom log_entry import LogEntry\n\tfrom test.testing_utils import create_event_entry, create_event, \\\n\t    entry_to_event, entry_msg_to_entry\n", "def test_try_parse_as_flush_preamble():\n\t    cf = \"cf1\"\n\t    job_id = 38\n\t    wal_id = 55\n\t    valid_preamble = \\\n\t        f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file: {wal_id}\"\n\t    partial1 = f\"[{cf}] [JOB {job_id}] Flushing\"\n\t    partial2 = f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file:\"\n\t    test_func = events.FlushStartedEvent.try_parse_as_preamble\n\t    assert test_func(\"\") == (False, None, None, None)\n", "    assert test_func(valid_preamble) == (True, cf, job_id, wal_id)\n\t    assert test_func(partial1) == (False, None, None, None)\n\t    assert test_func(partial2) == (False, None, None, None)\n\tdef test_try_parse_as_compaction_preamble():\n\t    cf = \"cf2\"\n\t    job_id = 157\n\t    valid_preamble = \\\n\t        f\"[{cf}] [JOB {job_id}] Compacting 1@1 + 5@2 files to L2, score 1.63\"\n\t    partial1 = \\\n\t        f\"[{cf}] [JOB {job_id}] Compacting\"\n", "    partial2 = \\\n\t        f\"[{cf}] [JOB {job_id}] Compacting 1@1 + 5@2 files\"\n\t    test_func = events.CompactionStartedEvent.try_parse_as_preamble\n\t    assert test_func(\"\") == (False, None, None)\n\t    assert test_func(valid_preamble) == (True, cf, job_id)\n\t    assert test_func(partial1) == (False, None, None)\n\t    assert test_func(partial2) == (False, None, None)\n\tdef test_try_parse_event_preamble():\n\t    cf = \"cf1\"\n\t    job_id = 38\n", "    wal_id = 55\n\t    time_str = \"2022/04/17-14:42:19.220573\"\n\t    valid_flush_preamble = \\\n\t        f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file: {wal_id}\"\n\t    flush_preamble_entry = entry_msg_to_entry(time_str, valid_flush_preamble)\n\t    valid_compaction_preamble = \\\n\t        f\"[{cf}] [JOB {job_id}] Compacting 1@1 + 5@2 files to L2, score 1.63\"\n\t    compaction_preamble_entry =\\\n\t        entry_msg_to_entry(time_str, valid_compaction_preamble)\n\t    partial_compaction_preamble = f\"[{cf}] [JOB {job_id}] Compacting\"\n", "    invalid_compaction_preamble_entry =\\\n\t        entry_msg_to_entry(time_str, partial_compaction_preamble)\n\t    test_func = events.Event.try_parse_as_preamble\n\t    info = events.Event.EventPreambleInfo\n\t    assert test_func(flush_preamble_entry) == \\\n\t           info(cf, events.EventType.FLUSH_STARTED, job_id, wal_id=wal_id)\n\t    assert test_func(compaction_preamble_entry) == \\\n\t           info(cf, events.EventType.COMPACTION_STARTED, job_id, wal_id=None)\n\t    assert test_func(invalid_compaction_preamble_entry) is None\n\tdef test_compaction_started_event():\n", "    time = \"2023/01/24-08:54:40.130553\"\n\t    job_id = 1\n\t    cf = \"cf1\"\n\t    cf_names = [cf]\n\t    reason = \"Reason1\"\n\t    files_l1 = [17248]\n\t    files_l2 = [16778, 16779, 16780, 16781, 17022]\n\t    event = create_event(job_id, cf_names, time,\n\t                         events.EventType.COMPACTION_STARTED, cf,\n\t                         compaction_reason=reason,\n", "                         files_L1=files_l1, files_L2=files_l2)\n\t    assert isinstance(event, events.CompactionStartedEvent)\n\t    assert event.get_compaction_reason() == reason\n\t    assert event.get_input_files() == {\n\t        1: files_l1,\n\t        2: files_l2\n\t    }\n\tdef test_table_file_creation_event():\n\t    time = \"2023/01/24-08:54:40.130553\"\n\t    file_number = 1234\n", "    job_id = 1\n\t    cf = \"cf1\"\n\t    cf_names = [cf]\n\t    table_properties = {}\n\t    event = create_event(job_id, cf_names, time,\n\t                         events.EventType.TABLE_FILE_CREATION, cf,\n\t                         file_number=file_number,\n\t                         table_properties=table_properties)\n\t    assert event.get_type() == events.EventType.TABLE_FILE_CREATION.value\n\t    assert event.get_created_file_number() == file_number\n", "    assert event.get_cf_id() is None\n\t    assert event.get_compressed_data_size_bytes() == 0\n\t    assert event.get_num_data_blocks() == 0\n\t    assert event.get_total_keys_sizes_bytes() == 0\n\t    assert event.get_total_values_sizes_bytes() == 0\n\t    assert event.get_data_size_bytes() == 0\n\t    assert event.get_index_size_bytes() == 0\n\t    assert event.get_filter_size_bytes() == 0\n\t    assert not event.does_use_filter()\n\t    assert event.get_num_filter_entries() == 0\n", "    assert event.get_compression_type() is None\n\t    table_properties2 = {\n\t        \"column_family_id\": 1,\n\t        \"data_size\": 100,\n\t        \"index_size\": 200,\n\t        \"filter_size\": 300,\n\t        \"raw_key_size\": 400,\n\t        \"raw_value_size\": 500,\n\t        \"num_data_blocks\": 600,\n\t        \"num_entries\": 700,\n", "        \"compression\": \"NoCompression\",\n\t        \"filter_policy\": \"BloomFilter\",\n\t        \"num_filter_entries\": 800}\n\t    event2 = create_event(job_id, cf_names, time,\n\t                          events.EventType.TABLE_FILE_CREATION, cf,\n\t                          file_number=file_number,\n\t                          table_properties=table_properties2)\n\t    assert event2.get_type() == events.EventType.TABLE_FILE_CREATION.value\n\t    assert event2.get_created_file_number() == file_number\n\t    assert event2.get_cf_id() == 1\n", "    assert event2.get_compressed_data_size_bytes() == 100\n\t    assert event2.get_num_data_blocks() == 600\n\t    assert event2.get_total_keys_sizes_bytes() == 400\n\t    assert event2.get_total_values_sizes_bytes() == 500\n\t    assert event2.get_data_size_bytes() == 900\n\t    assert event2.get_index_size_bytes() == 200\n\t    assert event2.get_filter_size_bytes() == 300\n\t    assert event2.does_use_filter()\n\t    assert event2.get_filter_policy() == \"BloomFilter\"\n\t    assert event2.get_num_filter_entries() == 800\n", "    assert event2.get_compression_type() == \"NoCompression\"\n\tdef test_table_file_deletion_event():\n\t    time = \"2023/01/24-08:54:40.130553\"\n\t    file_number = 1234\n\t    job_id = 1\n\t    cf = \"cf1\"\n\t    cf_names = [cf]\n\t    event = create_event(job_id, cf_names, time,\n\t                         events.EventType.TABLE_FILE_DELETION, cf,\n\t                         file_number=file_number)\n", "    assert event.get_type() == events.EventType.TABLE_FILE_DELETION.value\n\t    assert event.get_deleted_file_number() == file_number\n\tdef test_table_file_creation_event1():\n\t    time = \"2023/01/24-08:54:40.130553\"\n\t    file_number = 1234\n\t    job_id = 1\n\t    cf = \"cf1\"\n\t    cf_names = [cf]\n\t    cf_id = 100\n\t    compressed_data_size_bytes = 62396458\n", "    num_data_blocks = 1000\n\t    total_keys_sizes_bytes = 2000\n\t    total_values_sizes_bytes = 3333\n\t    index_size = 3000\n\t    filter_size = 4000\n\t    num_entries = 5555\n\t    compression_type = \"NoCompression\"\n\t    table_properties = {\n\t        \"data_size\": compressed_data_size_bytes,\n\t        \"index_size\": index_size,\n", "        \"index_partitions\": 0,\n\t        \"top_level_index_size\": 0,\n\t        \"index_key_is_user_key\": 1,\n\t        \"index_value_is_delta_encoded\": 1,\n\t        \"filter_size\": filter_size,\n\t        \"raw_key_size\": total_keys_sizes_bytes,\n\t        \"raw_average_key_size\": 24,\n\t        \"raw_value_size\": total_values_sizes_bytes,\n\t        \"raw_average_value_size\": 1000,\n\t        \"num_data_blocks\": num_data_blocks,\n", "        \"num_entries\": num_entries,\n\t        \"num_filter_entries\": 60774,\n\t        \"num_deletions\": 0,\n\t        \"num_merge_operands\": 0,\n\t        \"num_range_deletions\": 0,\n\t        \"format_version\": 0,\n\t        \"fixed_key_len\": 0,\n\t        \"filter_policy\": \"bloomfilter\",\n\t        \"column_family_name\": \"default\",\n\t        \"column_family_id\": cf_id,\n", "        \"comparator\": \"leveldb.BytewiseComparator\",\n\t        \"merge_operator\": \"nullptr\",\n\t        \"prefix_extractor_name\": \"nullptr\",\n\t        \"property_collectors\": \"[]\",\n\t        \"compression\": compression_type,\n\t        \"oldest_key_time\": 1672823099,\n\t        \"file_creation_time\": 1672823099,\n\t        \"slow_compression_estimated_data_size\": 0,\n\t        \"fast_compression_estimated_data_size\": 0,\n\t        \"db_id\": \"c100448c-dc04-4c74-8ab2-65d72f3aa3a8\",\n", "        \"db_session_id\": \"4GAWIG5RIF8PQWM3NOQG\",\n\t        \"orig_file_number\": 37155}\n\t    event = create_event(job_id, cf_names, time,\n\t                         events.EventType.TABLE_FILE_CREATION, cf,\n\t                         file_number=file_number,\n\t                         table_properties=table_properties)\n\t    assert event.get_type() == events.EventType.TABLE_FILE_CREATION.value\n\t    assert event.get_created_file_number() == file_number\n\t    assert event.get_cf_id() == cf_id\n\t    assert event.get_compressed_data_size_bytes() == compressed_data_size_bytes\n", "    assert event.get_num_data_blocks() == num_data_blocks\n\t    assert event.get_total_keys_sizes_bytes() == total_keys_sizes_bytes\n\t    assert event.get_total_values_sizes_bytes() == total_values_sizes_bytes\n\t    assert event.get_data_size_bytes() ==\\\n\t           total_keys_sizes_bytes + total_values_sizes_bytes\n\t    assert event.get_index_size_bytes() == index_size\n\t    assert event.get_filter_size_bytes() == filter_size\n\t    assert event.get_compression_type() == compression_type\n\tdef verify_expected_events(events_mngr, expected_events_dict):\n\t    # Expecting a dictionary of:\n", "    # {<cf_name>: [<events entries for this cf>]}\n\t    cf_names = list(expected_events_dict.keys())\n\t    # prepare the expected events per (cf, event type)\n\t    expected_cf_events_per_type = dict()\n\t    for name in cf_names:\n\t        expected_cf_events_per_type[name] = {event_type: [] for event_type\n\t                                             in events.EventType}\n\t    for cf_name, cf_events_entries in expected_events_dict.items():\n\t        expected_cf_events = \\\n\t            [events.Event(event_entry) for event_entry in\n", "             cf_events_entries]\n\t        assert events_mngr.get_cf_events(cf_name) == expected_cf_events\n\t        for event in expected_cf_events:\n\t            expected_cf_events_per_type[cf_name][event.get_type()].append(\n\t                event)\n\t        for event_type in events.EventType:\n\t            assert events_mngr.get_cf_events_by_type(cf_name, event_type) ==\\\n\t                   expected_cf_events_per_type[cf_name][event_type]\n\tdef test_event_type():\n\t    assert events.EventType.type_from_str(\"flush_finished\") == \\\n", "           events.EventType.FLUSH_FINISHED\n\t    assert str(events.EventType.type_from_str(\"flush_finished\")) == \\\n\t           \"flush_finished\"\n\t    assert events.EventType.type_from_str(\"Dummy\") == events.EventType.UNKNOWN\n\t    assert str(events.EventType.type_from_str(\"Dummy\")) == \"UNKNOWN\"\n\tdef test_get_matching_type_info_if_exists():\n\t    assert events.Event.\\\n\t           get_matching_type_info_if_exists(\n\t            events.EventType.FLUSH_STARTED) == \\\n\t           events.MatchingEventTypeInfo(\n", "               events.EventType.FLUSH_FINISHED, events.FlowType.FLUSH, False)\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.FLUSH_FINISHED) == events.MatchingEventTypeInfo(\n\t        events.EventType.FLUSH_STARTED, events.FlowType.FLUSH, True)\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.COMPACTION_STARTED) == \\\n\t        events.MatchingEventTypeInfo(\n\t            events.EventType.COMPACTION_FINISHED, events.FlowType.COMPACTION,\n\t            False)\n\t    assert events.Event.get_matching_type_info_if_exists(\n", "        events.EventType.COMPACTION_FINISHED) == \\\n\t        events.MatchingEventTypeInfo(events.EventType.COMPACTION_STARTED,\n\t                                     events.FlowType.COMPACTION, True)\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.RECOVERY_STARTED) == \\\n\t        events.MatchingEventTypeInfo(events.EventType.RECOVERY_FINISHED,\n\t                                     events.FlowType.RECOVERY, False)\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.RECOVERY_FINISHED) == \\\n\t        events.MatchingEventTypeInfo(events.EventType.RECOVERY_STARTED,\n", "                                     events.FlowType.RECOVERY, True)\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.TABLE_FILE_CREATION) is None\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.TABLE_FILE_DELETION) is None\n\t    assert events.Event.\\\n\t           get_matching_type_info_if_exists(events.EventType.TRIVIAL_MOVE) \\\n\t           is None\n\t    assert events.Event.\\\n\t           get_matching_type_info_if_exists(events.EventType.INGEST_FINISHED) \\\n", "           is None\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.BLOB_FILE_CREATION) is None\n\t    assert events.Event.get_matching_type_info_if_exists(\n\t        events.EventType.BLOB_FILE_DELETION) is None\n\t    assert events.Event.\\\n\t           get_matching_type_info_if_exists(events.EventType.UNKNOWN) is None\n\tdef test_matching_event_info_get_duration_ms():\n\t    cf_names = [\"default\"]\n\t    job_id = 1\n", "    start_event_time = \"2023/01/04-08:54:59.130996\"\n\t    start_event = create_event(\n\t        job_id, cf_names, start_event_time,\n\t        event_type=events.EventType.FLUSH_STARTED, flush_reason=\"Reason1\")\n\t    start_event_info = \\\n\t        events.MatchingEventInfo(start_event, events.FlowType.FLUSH,\n\t                                 is_start=True)\n\t    start_datetime = datetime.strptime(f\"{start_event_time}GMT\",\n\t                                       \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\t    delta_ms = 1500\n", "    time_delta_ms = timedelta(milliseconds=delta_ms)\n\t    end_datetime = start_datetime + time_delta_ms\n\t    end_event_time = end_datetime.strftime(\"%Y/%m/%d-%H:%M:%S.%f\")\n\t    end_event = create_event(\n\t        job_id, cf_names, end_event_time,\n\t        event_type=events.EventType.FLUSH_FINISHED)\n\t    end_event_info = events.MatchingEventInfo(\n\t        end_event, events.FlowType.FLUSH, is_start=False)\n\t    assert start_event_info.get_duration_ms(end_event_info) == delta_ms\n\t    assert end_event_info.get_duration_ms(start_event_info) == delta_ms\n", "@pytest.mark.parametrize(\"cf_name\", [\"default\", \"\"])\n\tdef test_event(cf_name):\n\t    job_id = 35\n\t    event_entry = create_event_entry(job_id, \"2022/04/17-14:42:19.220573\",\n\t                                     events.EventType.FLUSH_FINISHED, cf_name)\n\t    assert events.Event.is_an_event_entry(event_entry)\n\t    assert not events.Event.try_parse_as_preamble(event_entry)\n\t    event1 = events.Event(event_entry)\n\t    assert event1.get_type() == events.EventType.FLUSH_FINISHED\n\t    assert event1.get_job_id() == 35\n", "    assert event1.get_cf_name() == cf_name\n\t    assert not event1.is_db_wide_event()\n\t    assert event1.is_cf_event()\n\t    assert event1.get_my_matching_type_info_if_exists() == \\\n\t           events.MatchingEventTypeInfo(\n\t               events.EventType.FLUSH_STARTED, events.FlowType.FLUSH, True)\n\t    event1 = None\n\t    # Unknown event (legal)\n\t    event_entry = create_event_entry(job_id, \"2022/04/17-14:42:19.220573\",\n\t                                     events.EventType.UNKNOWN, cf_name)\n", "    event2 = events.Event(event_entry)\n\t    assert event2.get_type() == events.EventType.UNKNOWN\n\t    assert event2.get_job_id() == 35\n\t    assert event2.get_cf_name() == cf_name\n\t    assert not event2.is_db_wide_event()\n\t    assert event2.is_cf_event()\n\t    assert event2.get_my_matching_type_info_if_exists() is None\n\t@pytest.mark.parametrize(\"cf_name\", [\"default\", \"\"])\n\tdef test_event_preamble(cf_name):\n\t    preamble_line = f\"\"\"2022/04/17-14:42:11.398681 7f4a8b5bb700 \n", "    [/flush_job.cc:333] [{cf_name}] [JOB 8] \n\t    Flushing memtable with next log file: 5\n\t    \"\"\" # noqa\n\t    preamble_line = \" \".join(preamble_line.splitlines())\n\t    cf_names = [cf_name, \"dummy_cf\"]\n\t    job_id = 8\n\t    preamble_entry = LogEntry(0, preamble_line, True)\n\t    event_entry = create_event_entry(job_id, \"2022/11/24-15:58:17.683316\",\n\t                                     events.EventType.FLUSH_STARTED,\n\t                                     utils.NO_CF, flush_reason=\"REASON1\")\n", "    assert not events.Event.try_parse_as_preamble(event_entry)\n\t    preamble_info = \\\n\t        events.Event.try_parse_as_preamble(preamble_entry)\n\t    assert preamble_info\n\t    assert preamble_info.job_id == 8\n\t    assert preamble_info.type == events.EventType.FLUSH_STARTED\n\t    assert preamble_info.cf_name == cf_name\n\t    assert events.Event.is_an_event_entry(event_entry)\n\t    assert not events.Event.try_parse_as_preamble(event_entry)\n\t    event = events.Event.create_event(event_entry)\n", "    assert event.get_type() == events.EventType.FLUSH_STARTED\n\t    assert event.get_job_id() == 8\n\t    assert event.get_cf_name() == utils.NO_CF\n\t    assert event.is_db_wide_event()\n\t    assert not event.is_cf_event()\n\t    assert event.get_wal_id_if_available() is None\n\t    assert event.get_flush_reason() == \"REASON1\"\n\t    assert event.try_adding_preamble_event(preamble_info)\n\t    assert event.get_cf_name() == cf_name\n\t    assert not event.is_db_wide_event()\n", "    assert event.is_cf_event()\n\t    assert event.get_wal_id_if_available() == 5\n\t    assert event.get_my_matching_type_info_if_exists() == \\\n\t           events.MatchingEventTypeInfo(\n\t               events.EventType.FLUSH_FINISHED, events.FlowType.FLUSH, False)\n\t    assert event.get_matching_event_info_if_exists() is None\n\t    compaction_finished_event = \\\n\t        create_event(job_id, cf_names, \"2022/11/24-15:59:17.683316\",\n\t                     events.EventType.COMPACTION_FINISHED, cf_name)\n\t    event.try_adding_matching_event(compaction_finished_event)\n", "    assert event.get_matching_event_info_if_exists() is None\n\t    flush_end_event = create_event(job_id, cf_names,\n\t                                   \"2022/11/24-15:59:17.683316\",\n\t                                   events.EventType.FLUSH_FINISHED,\n\t                                   cf_name)\n\t    event.try_adding_matching_event(flush_end_event)\n\t    assert event.get_matching_event_info_if_exists() ==\\\n\t           events.MatchingEventInfo(flush_end_event, events.FlowType.FLUSH,\n\t                                    is_start=False)\n\tdef test_illegal_events():\n", "    cf1 = \"CF1\"\n\t    # Illegal event json\n\t    event_entry = create_event_entry(35, \"2022/04/17-14:42:19.220573\",\n\t                                     events.EventType.FLUSH_FINISHED, cf1,\n\t                                     make_illegal_json=True)\n\t    assert events.Event.try_parse_as_preamble(event_entry) is None\n\t    with pytest.raises(utils.ParsingError):\n\t        events.Event.create_event(event_entry)\n\t    # Missing Job id (illegal)\n\t    event_entry = create_event_entry(None, \"2022/04/17-14:42:19.220573\",\n", "                                     events.EventType.FLUSH_FINISHED,\n\t                                     cf_name=cf1)\n\t    assert events.Event.try_parse_as_preamble(event_entry) is None\n\t    assert events.Event.is_an_event_entry(event_entry)\n\t    with pytest.raises(utils.ParsingError):\n\t        events.Event.create_event(event_entry)\n\t    # Missing events.Event Type (definitely illegal)\n\t    event_entry = create_event_entry(200, \"2022/04/17-14:42:19.220573\",\n\t                                     event_type=None, cf_name=cf1)\n\t    assert events.Event.try_parse_as_preamble(event_entry) is None\n", "    assert events.Event.is_an_event_entry(event_entry)\n\t    with pytest.raises(utils.ParsingError):\n\t        events.Event.create_event(event_entry)\n\tdef test_handling_same_job_id_multiple_cfs():\n\t    cf1 = \"cf1\"\n\t    cf2 = \"cf2\"\n\t    event1_time = \"2023/01/04-08:54:59.130996\"\n\t    event2_time = \"2023/01/04-08:55:59.130996\"\n\t    event3_time = \"2023/01/04-08:56:59.130996\"\n\t    job_id = 1\n", "    job_id_to_cf_name_map = {job_id: cf1}\n\t    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\t    event_entry_cf1 = create_event_entry(job_id, event1_time,\n\t                                         events.EventType.FLUSH_STARTED, cf1,\n\t                                         flush_reason=\"FLUSH_REASON1\")\n\t    event_cf1 = entry_to_event(event_entry_cf1)\n\t    assert events_mngr.try_adding_entry(event_entry_cf1) ==\\\n\t           (True, event_cf1, cf1)\n\t    event_entry_cf2 = create_event_entry(job_id, event2_time,\n\t                                         events.EventType.FLUSH_FINISHED, cf2)\n", "    assert events_mngr.try_adding_entry(event_entry_cf2) == (True, None, None)\n\t    # Now adding without a cf => should match the event to the 1st one\n\t    event_entry_cf3 = create_event_entry(job_id, event3_time,\n\t                                         events.EventType.FLUSH_FINISHED,\n\t                                         utils.NO_CF)\n\t    event_no_cf = entry_to_event(event_entry_cf3)\n\t    event_no_cf.set_cf_name(cf1)\n\t    assert events_mngr.try_adding_entry(event_entry_cf3) == \\\n\t           (True, event_no_cf, cf1)\n\tdef test_adding_events_to_events_mngr():\n", "    cf1 = \"cf1\"\n\t    job_id1 = 1\n\t    cf2 = \"cf2\"\n\t    job_id2 = 2\n\t    job_id_to_cf_name_map = {job_id1: cf1, job_id2: cf2}\n\t    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\t    assert not events_mngr.get_cf_events(cf1)\n\t    assert not events_mngr.\\\n\t        get_cf_events_by_type(cf2, events.EventType.FLUSH_FINISHED)\n\t    expected_events_entries = {cf1: [], cf2: []}\n", "    event1_entry = create_event_entry(job_id1, \"2022/04/17-14:42:19.220573\",\n\t                                      events.EventType.FLUSH_FINISHED, cf1)\n\t    event1 = entry_to_event(event1_entry)\n\t    assert events_mngr.try_adding_entry(event1_entry) == (True, event1, cf1)\n\t    expected_events_entries[cf1] = [event1_entry]\n\t    verify_expected_events(events_mngr, expected_events_entries)\n\t    event2_entry = create_event_entry(job_id2, \"2022/04/18-14:42:19.220573\",\n\t                                      events.EventType.FLUSH_STARTED, cf2,\n\t                                      flush_reason=\"FLUSH_REASON1\")\n\t    event2 = entry_to_event(event2_entry)\n", "    assert events_mngr.try_adding_entry(event2_entry) == (True, event2, cf2)\n\t    expected_events_entries[cf2] = [event2_entry]\n\t    verify_expected_events(events_mngr, expected_events_entries)\n\t    # Create another cf1 event, but set its time to EARLIER than event1\n\t    event3_entry = create_event_entry(job_id1, \"2022/03/17-14:42:19.220573\",\n\t                                      events.EventType.FLUSH_FINISHED, cf1)\n\t    event3 = entry_to_event(event3_entry)\n\t    assert events_mngr.try_adding_entry(event3_entry) == (True, event3, cf1)\n\t    # Expecting event3 to be before event1\n\t    expected_events_entries[cf1] = [event3_entry, event1_entry]\n", "    verify_expected_events(events_mngr, expected_events_entries)\n\t    # Create some more cf21 event, later in time\n\t    event4_entry = \\\n\t        create_event_entry(job_id2, \"2022/05/17-14:42:19.220573\",\n\t                           events.EventType.COMPACTION_STARTED, cf2,\n\t                           compaction_reason=\"Reason1\")\n\t    event4 = entry_to_event(event4_entry)\n\t    event5_entry = \\\n\t        create_event_entry(job_id2, \"2022/05/17-15:42:19.220573\",\n\t                           events.EventType.COMPACTION_STARTED, cf2,\n", "                           compaction_reason=\"Reason2\")\n\t    event5 = entry_to_event(event5_entry)\n\t    event6_entry =\\\n\t        create_event_entry(job_id2, \"2022/05/17-16:42:19.220573\",\n\t                           events.EventType.COMPACTION_FINISHED, cf2)\n\t    event6 = entry_to_event(event6_entry)\n\t    assert events_mngr.try_adding_entry(event4_entry) == (True, event4, cf2)\n\t    assert events_mngr.try_adding_entry(event5_entry) == (True, event5, cf2)\n\t    assert events_mngr.try_adding_entry(event6_entry) == (True, event6, cf2)\n\t    expected_events_entries[cf2] = [event2_entry, event4_entry,\n", "                                    event5_entry, event6_entry]\n\t    verify_expected_events(events_mngr, expected_events_entries)\n\tdef test_get_flow_events():\n\t    cf1 = \"cf1\"\n\t    cf2 = \"cf2\"\n\t    cfs_names = [cf1, cf2]\n\t    job_id = 1\n\t    job_id_to_cf_name_map = {job_id: cf1}\n\t    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\t    assert events_mngr.get_cf_flow_events(events.FlowType.FLUSH, cf1) == []\n", "    assert events_mngr.get_cf_flow_events(events.FlowType.FLUSH, cf2) == []\n\t    assert events_mngr.get_all_flow_events(events.FlowType.FLUSH, cfs_names)\\\n\t           == []\n\t    flush_started_cf1_1 = create_event_entry(job_id,\n\t                                             \"2022/04/18-14:42:19.220573\",\n\t                                             events.EventType.FLUSH_STARTED,\n\t                                             cf1,\n\t                                             flush_reason=\"FLUSH_REASON1\")\n\t    flush_started_event = entry_to_event(flush_started_cf1_1)\n\t    assert events_mngr.try_adding_entry(flush_started_cf1_1) == \\\n", "           (True, flush_started_event, cf1)\n\t    cf1_flush_started_events =\\\n\t        events_mngr.get_cf_events_by_type(cf1, events.EventType.FLUSH_STARTED)\n\t    expected_flush_events = [(cf1_flush_started_events[0], None)]\n\t    assert events_mngr.get_cf_flow_events(events.FlowType.FLUSH, cf1) == \\\n\t           expected_flush_events\n\t    assert events_mngr.get_all_flow_events(events.FlowType.FLUSH, cfs_names)\\\n\t           == expected_flush_events\n\t    flush_finished_cf1_1 = \\\n\t        create_event_entry(job_id,\n", "                           \"2022/04/18-14:43:19.220573\",\n\t                           events.EventType.FLUSH_FINISHED,\n\t                           cf1)\n\t    flush_finished_event = entry_to_event(flush_finished_cf1_1)\n\t    assert events_mngr.try_adding_entry(flush_finished_cf1_1) == \\\n\t           (True, flush_finished_event, cf1)\n\t    cf1_flush_started_events =\\\n\t        events_mngr.get_cf_events_by_type(cf1, events.EventType.FLUSH_STARTED)\n\t    cf1_flush_finished_events =\\\n\t        events_mngr.get_cf_events_by_type(cf1, events.EventType.FLUSH_FINISHED)\n", "    expected_flush_events = [(cf1_flush_started_events[0],\n\t                              cf1_flush_finished_events[0])]\n\t    assert events_mngr.get_all_flow_events(events.FlowType.FLUSH, cfs_names)\\\n\t           == expected_flush_events\n\tdef test_try_adding_invalid_event_to_events_mngr():\n\t    cf1 = \"cf1\"\n\t    job_id = 35\n\t    job_id_to_cf_name_map = {job_id: cf1}\n\t    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\t    # Illegal event json\n", "    invalid_event_entry =\\\n\t        create_event_entry(job_id,\n\t                           \"2022/04/17-14:42:19.220573\",\n\t                           events.EventType.FLUSH_FINISHED,\n\t                           cf1,\n\t                           make_illegal_json=True)\n\t    assert events_mngr.try_adding_entry(invalid_event_entry) == \\\n\t           (True, None, None)\n\t    assert events_mngr.debug_get_all_events() == {}\n\t    event1_entry = create_event_entry(job_id, \"2022/04/17-14:42:19.220573\",\n", "                                      events.EventType.FLUSH_FINISHED, cf1)\n\t    event1 = entry_to_event(event1_entry)\n\t    assert events_mngr.try_adding_entry(event1_entry) == (True, event1, cf1)\n\t    assert len(events_mngr.debug_get_all_events()) == 1\n\t    assert events_mngr.try_adding_entry(invalid_event_entry) == \\\n\t           (True, None, None)\n\t    assert len(events_mngr.debug_get_all_events()) == 1\n"]}
{"filename": "test/test_cache_utils.py", "chunked_list": ["from dataclasses import dataclass\n\timport db_files\n\tfrom db_files import DbFilesMonitor\n\tfrom events import EventType\n\tfrom test.testing_utils import create_event\n\t@dataclass\n\tclass FileCreationHelperInfo:\n\t    job_id: int\n\t    file_number: int\n\t    cf_name: str\n", "    creation_time: str\n\t    compressed_data_size_bytes: int = 10000\n\t    num_data_blocks: int = 500\n\t    total_keys_sizes_bytes: int = 800\n\t    total_values_sizes_bytes: int = 1600\n\t    index_size: int = 1000\n\t    filter_size: int = 2000\n\t    num_entries: int = 100\n\t    compression_type: str = \"NoCompression\"\n\t    filter_policy: str = \"\"\n", "    num_filter_entries: int = 50\n\tdef create_file_event_helper(cf_names, creation_info, monitor):\n\t    assert isinstance(creation_info, FileCreationHelperInfo)\n\t    table_properties = {\n\t        \"data_size\": creation_info.compressed_data_size_bytes,\n\t        \"index_size\": creation_info.index_size,\n\t        \"filter_size\": creation_info.filter_size,\n\t        \"raw_key_size\": creation_info.total_keys_sizes_bytes,\n\t        \"raw_value_size\": creation_info.total_values_sizes_bytes,\n\t        \"num_data_blocks\": creation_info.num_data_blocks,\n", "        \"num_entries\": creation_info.num_entries,\n\t        \"compression\": creation_info.compression_type,\n\t        \"filter_policy\": creation_info.filter_policy,\n\t        \"num_filter_entries\": creation_info.num_filter_entries}\n\t    creation_event = create_event(creation_info.job_id, cf_names,\n\t                                  creation_info.creation_time,\n\t                                  EventType.TABLE_FILE_CREATION,\n\t                                  creation_info.cf_name,\n\t                                  file_number=creation_info.file_number,\n\t                                  table_properties=table_properties)\n", "    assert monitor.new_event(creation_event)\n\tdef test_calc_cf_files_stats():\n\t    cf1 = \"cf1\"\n\t    cf2 = \"cf2\"\n\t    cf_names = [cf1, cf2]\n\t    time1 = \"2023/01/24-08:54:40.130553\"\n\t    time1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\n\t    time1_plus_11_sec = \"2023/01/24-08:55:50.130553\"\n\t    monitor = DbFilesMonitor()\n\t    helper_info1 = FileCreationHelperInfo(job_id=1,\n", "                                          file_number=1234,\n\t                                          cf_name=cf1,\n\t                                          creation_time=time1,\n\t                                          index_size=1000,\n\t                                          filter_size=2000,\n\t                                          filter_policy=\"Filter1\",\n\t                                          num_filter_entries=10000)\n\t    # the block statistics are tested as part of the test_db_files suite\n\t    expected_cf1_filter_specific_stats = \\\n\t        {cf1: db_files.CfFilterSpecificStats(filter_policy=\"Filter1\",\n", "                                             avg_bpk=8*2000/10000)}\n\t    create_file_event_helper(cf_names, helper_info1, monitor)\n\t    actual_cf1_stats = db_files.calc_cf_files_stats([cf1], monitor)\n\t    assert actual_cf1_stats.cfs_filter_specific == \\\n\t           expected_cf1_filter_specific_stats\n\t    assert db_files.calc_cf_files_stats([cf2], monitor) is None\n\t    helper_info2 = FileCreationHelperInfo(job_id=2,\n\t                                          file_number=5678,\n\t                                          cf_name=cf1,\n\t                                          creation_time=time1_plus_10_sec,\n", "                                          index_size=500,\n\t                                          filter_size=3000,\n\t                                          filter_policy=\"Filter1\",\n\t                                          num_filter_entries=5000)\n\t    expected_cf1_filter_specific_stats = \\\n\t        {cf1: db_files.CfFilterSpecificStats(filter_policy=\"Filter1\",\n\t                                             avg_bpk=8*5000/15000)}\n\t    create_file_event_helper(cf_names, helper_info2, monitor)\n\t    actual_cf1_stats = db_files.calc_cf_files_stats([cf1], monitor)\n\t    assert actual_cf1_stats.cfs_filter_specific == \\\n", "           expected_cf1_filter_specific_stats\n\t    assert db_files.calc_cf_files_stats([cf2], monitor) is None\n\t    helper_info3 = FileCreationHelperInfo(job_id=3,\n\t                                          file_number=9999,\n\t                                          cf_name=cf2,\n\t                                          creation_time=time1_plus_11_sec,\n\t                                          index_size=1500,\n\t                                          filter_size=1000,\n\t                                          filter_policy=\"\",\n\t                                          num_filter_entries=0)\n", "    expected_cf2_filter_specific_stats = \\\n\t        {cf2: db_files.CfFilterSpecificStats(filter_policy=None, avg_bpk=0)}\n\t    expected_all_cfs_filter_specific_stats = {\n\t        cf1: expected_cf1_filter_specific_stats[cf1],\n\t        cf2: expected_cf2_filter_specific_stats[cf2]\n\t    }\n\t    create_file_event_helper(cf_names, helper_info3, monitor)\n\t    actual_cf1_stats = db_files.calc_cf_files_stats([cf1], monitor)\n\t    assert actual_cf1_stats.cfs_filter_specific == \\\n\t           expected_cf1_filter_specific_stats\n", "    actual_cf2_stats = db_files.calc_cf_files_stats([cf2], monitor)\n\t    assert actual_cf2_stats.cfs_filter_specific == \\\n\t           expected_cf2_filter_specific_stats\n\t    actual_all_cfs_stats = db_files.calc_cf_files_stats([cf1, cf2], monitor)\n\t    assert actual_all_cfs_stats.cfs_filter_specific == \\\n\t           expected_all_cfs_filter_specific_stats\n"]}
{"filename": "test/test_calc_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\tfrom dataclasses import dataclass\n\timport calc_utils\n\timport db_options as db_opts\n\timport test.testing_utils as test_utils\n\timport utils\n\tfrom counters import CountersMngr\n\tfrom stats_mngr import CfFileHistogramStatsMngr\n", "from test.sample_log_info import SampleLogInfo\n\t# TODO: Move to the stats mngr test file\n\tdef test_get_cf_size_bytes():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\t    compactions_stats_mngr = \\\n\t        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\t    for cf_name in SampleLogInfo.CF_NAMES:\n\t        actual_size_bytes = \\\n\t            compactions_stats_mngr.get_cf_size_bytes_at_end(cf_name)\n\t        assert actual_size_bytes == SampleLogInfo.CF_SIZE_BYTES[cf_name]\n", "def test_get_db_size_bytes():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\t    compactions_stats_mngr = \\\n\t        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\t    for level, level_size_bytes in \\\n\t            SampleLogInfo.CF_DEFAULT_LEVEL_SIZES.items():\n\t        actual_level_size_bytes = \\\n\t            compactions_stats_mngr.get_cf_level_size_bytes(\n\t                SampleLogInfo.CF_DEFAULT, level)\n\t        assert actual_level_size_bytes == level_size_bytes\n", "def test_calc_all_events_histogram():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\t    events_histogram = calc_utils.calc_all_events_histogram(\n\t        SampleLogInfo.CF_NAMES, parsed_log.get_events_mngr())\n\t    assert events_histogram == SampleLogInfo.EVENTS_HISTOGRAM\n\tdef test_calc_read_latency_per_cf_stats():\n\t    time = \"2023/01/04-09:04:59.378877 27442\"\n\t    l0 = 0\n\t    l1 = 1\n\t    l4 = 4\n", "    cf1 = \"cf1\"\n\t    cf2 = \"cf2\"\n\t    stats_lines_cf1 = \\\n\t    f'''** File Read Latency Histogram By Level [{cf1}] **\n\t    ** Level {l0} read latency histogram (micros):\n\t    Count: 100 Average: 1.5  StdDev: 34.39\n\t    Min: 0  Median: 2.4427  Max: 1000\n\t    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02\n\t    ** Level {l4} read latency histogram (micros):\n\t    Count: 200 Average: 2.5  StdDev: 2.2\n", "    Min: 1000  Median: 3.3  Max: 2000\n\t    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02'''  # noqa\n\t    stats_lines_cf1 = stats_lines_cf1.splitlines()\n\t    stats_lines_cf1 = [line.strip() for line in stats_lines_cf1]\n\t    mngr = CfFileHistogramStatsMngr()\n\t    mngr.add_lines(time, cf1, stats_lines_cf1)\n\t    total_reads_cf1 = 100 + 200\n\t    expected_cf1_avg_read_latency_us = (100*1.5 + 200*2.5) / total_reads_cf1\n\t    expected_cf1_stats = \\\n\t        calc_utils.CfReadLatencyStats(\n", "            num_reads=total_reads_cf1,\n\t            avg_read_latency_us=expected_cf1_avg_read_latency_us,\n\t            max_read_latency_us=2000,\n\t            read_percent_of_all_cfs=100.0)\n\t    actual_stats = calc_utils.calc_read_latency_per_cf_stats(mngr)\n\t    assert actual_stats == {cf1: expected_cf1_stats}\n\t    stats_lines_cf2 = \\\n\t        f'''** File Read Latency Histogram By Level [{cf2}] **\n\t    ** Level {l1} read latency histogram (micros):\n\t    Count: 1000 Average: 2.5  StdDev: 34.39\n", "    Min: 0  Median: 2.4427  Max: 20000\n\t    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02\n\t    ** Level {l4} read latency histogram (micros):\n\t    Count: 500 Average: 1.1  StdDev: 2.2\n\t    Min: 1000  Median: 3.3  Max: 10000\n\t    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n\t    stats_lines_cf2 = stats_lines_cf2.splitlines()\n\t    stats_lines_cf2 = [line.strip() for line in stats_lines_cf2]\n\t    mngr.add_lines(time, cf2, stats_lines_cf2)\n\t    total_reads = 300 + 1500\n", "    total_reads_cf2 = 1000 + 500\n\t    expected_cf1_stats.read_percent_of_all_cfs = \\\n\t        (total_reads_cf1 / total_reads) * 100\n\t    expected_cf2_read_percent_of_all_cfs = \\\n\t        (total_reads_cf2 / total_reads) * 100\n\t    expected_cf2_avg_read_latency_us = (1000*2.5 + 500*1.1) / (1000+500)\n\t    expected_cf2_stats = \\\n\t        calc_utils.CfReadLatencyStats(\n\t            num_reads=total_reads_cf2,\n\t            avg_read_latency_us=expected_cf2_avg_read_latency_us,\n", "            max_read_latency_us=20000,\n\t            read_percent_of_all_cfs=expected_cf2_read_percent_of_all_cfs)\n\t    actual_stats = calc_utils.calc_read_latency_per_cf_stats(mngr)\n\t    assert actual_stats == {cf1: expected_cf1_stats,\n\t                            cf2: expected_cf2_stats}\n\t@dataclass\n\tclass HistogramInfo:\n\t    count: int = 0\n\t    sum: int = 0\n\t    def get_average(self):\n", "        assert self.sum > 0\n\t        return self.sum / self.count\n\t@dataclass\n\tclass SeekInfo:\n\t    time: str\n\t    num_seeks: int = 0\n\t    num_found: int = 0\n\t    num_next: int = 0\n\t    num_prev: int = 0\n\t    seek_micros: HistogramInfo = HistogramInfo()\n", "def get_seek_counter_and_histogram_entry(test_seek_info):\n\t    assert isinstance(test_seek_info, SeekInfo)\n\t    prefix = 'rocksdb.number.db'\n\t    lines = list()\n\t    lines.append(\n\t        f'{test_seek_info.time} 100 [db/db_impl/db_impl.cc:753] STATISTICS:')\n\t    lines.append(f'{prefix}.seek COUNT : {test_seek_info.num_seeks}')\n\t    lines.append(f'{prefix}.seek.found COUNT : {test_seek_info.num_found}')\n\t    lines.append(f'{prefix}.next COUNT : {test_seek_info.num_next}')\n\t    lines.append(f'{prefix}.prev COUNT : {test_seek_info.num_prev}')\n", "    lines.append(\n\t        f'rocksdb.db.seek.micros P50 : 0.0 P95 : 0.0 P99 : 0.0 P100 : 0.0 '\n\t        f'COUNT : {test_seek_info.seek_micros.count} '\n\t        f'SUM : {test_seek_info.seek_micros.sum}')\n\t    return test_utils.lines_to_entries(lines)\n\tdef test_get_applicable_seek_stats():\n\t    get_seek_stats = calc_utils.get_applicable_seek_stats\n\t    start_time = \"2022/06/16-15:36:02.993900\"\n\t    start_plus_1_second = utils.get_time_relative_to(start_time, num_seconds=1)\n\t    start_plus_10_seconds = \\\n", "        utils.get_time_relative_to(start_time, num_seconds=10)\n\t    start_plus_20_seconds = \\\n\t        utils.get_time_relative_to(start_time, num_seconds=20)\n\t    start_seek_stats = SeekInfo(start_time)\n\t    start_entry = get_seek_counter_and_histogram_entry(start_seek_stats)\n\t    start_plus_1_seek_stats = SeekInfo(start_plus_1_second)\n\t    start_plus_1_seek_entry = \\\n\t        get_seek_counter_and_histogram_entry(start_plus_1_seek_stats)\n\t    start_plus_10_seek_stats = \\\n\t        SeekInfo(start_plus_10_seconds, num_seeks=1000, num_found=500)\n", "    start_plus_10_seek_entry = \\\n\t        get_seek_counter_and_histogram_entry(start_plus_10_seek_stats)\n\t    start_plus_20_seek_stats = \\\n\t        SeekInfo(start_plus_20_seconds,\n\t                 num_seeks=20000,\n\t                 num_found=10000,\n\t                 num_next=1500,\n\t                 num_prev=500,\n\t                 seek_micros=HistogramInfo(count=20000, sum=2000))\n\t    start_plus_20_seek_entry = \\\n", "        get_seek_counter_and_histogram_entry(start_plus_20_seek_stats)\n\t    mngr = CountersMngr()\n\t    assert get_seek_stats(mngr) is None\n\t    assert mngr.try_adding_entries(start_entry, 0) == (True, 1)\n\t    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n\t           {'time': start_time, 'value': 0}\n\t    assert get_seek_stats(mngr) is None\n\t    assert mngr.try_adding_entries(start_plus_1_seek_entry, 0) == (True, 1)\n\t    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n\t           {'time': start_plus_1_second, 'value': 0}\n", "    assert get_seek_stats(mngr) is None\n\t    assert mngr.try_adding_entries(start_plus_10_seek_entry, 0) == (True, 1)\n\t    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n\t           {'time': start_plus_10_seconds, 'value': 1000}\n\t    get_seek_stats(mngr) == calc_utils.SeekStats(num_seeks=1000,\n\t                                                 num_found_seeks=500)\n\t    assert mngr.try_adding_entries(start_plus_20_seek_entry, 0) == (True, 1)\n\t    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n\t           {'time': start_plus_20_seconds, 'value': 20000}\n\t    assert mngr.get_last_histogram_entry('rocksdb.db.seek.micros',\n", "                                         non_zero=False) == \\\n\t           {'time': start_plus_20_seconds,\n\t            'values': {'Average': 0.1,\n\t                       'Count': 20000,\n\t                       'Interval Count': 20000,\n\t                       'Interval Sum': 2000,\n\t                       'P100': 0.0,\n\t                       'P50': 0.0,\n\t                       'P95': 0.0,\n\t                       'P99': 0.0,\n", "                       'Sum': 2000}}\n\t    assert get_seek_stats(mngr) == \\\n\t           calc_utils.SeekStats(num_seeks=20000,\n\t                                num_found_seeks=10000,\n\t                                num_nexts=1500,\n\t                                num_prevs=500,\n\t                                avg_seek_range_size=0.1,\n\t                                avg_seek_rate_per_second=(20000/20),\n\t                                avg_seek_latency_us=0.1)\n\tCF_SECTION_TYPE = db_opts.SectionType.CF\n", "TABLE_SECTION_TYPE = db_opts.SectionType.TABLE_OPTIONS\n\tdef test_get_cfs_common_and_specific_options():\n\t    get_opts = calc_utils.get_cfs_common_and_specific_options\n\t    cf1 = 'cf1'\n\t    cf2 = 'cf2'\n\t    cf_option1 = \"CF-Option1\"\n\t    cf_option2 = \"CF-Option2\"\n\t    cf_option3 = \"CF-Option3\"\n\t    cf_table_option1 = \"CF-Table-Option1\"\n\t    cf_table_option2 = \"CF-Table-Option2\"\n", "    full_cf_option1_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n\t                                                        cf_option1)\n\t    full_cf_option2_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n\t                                                        cf_option2)\n\t    full_cf_option3_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n\t                                                        cf_option3)\n\t    full_cf_table_option1_name =\\\n\t        db_opts.get_full_option_name(TABLE_SECTION_TYPE, cf_table_option1)\n\t    full_cf_table_option2_name = \\\n\t        db_opts.get_full_option_name(TABLE_SECTION_TYPE, cf_table_option2)\n", "    dbo = db_opts.DatabaseOptions()\n\t    assert get_opts(dbo) == ({}, {})\n\t    dbo.set_cf_option(cf1, cf_option1, 1, allow_new_option=True)\n\t    expected_common = {full_cf_option1_name: 1}\n\t    expected_specific = {cf1: {}}\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_option(cf1, cf_option2, 2, allow_new_option=True)\n\t    expected_common = {\n\t        full_cf_option1_name: 1,\n\t        full_cf_option2_name: 2\n", "    }\n\t    expected_specific = {cf1: {}}\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_option(cf2, cf_option3, 3, allow_new_option=True)\n\t    expected_common = {}\n\t    expected_specific = {\n\t        cf1: {full_cf_option1_name: 1,\n\t              full_cf_option2_name: 2},\n\t        cf2: {full_cf_option3_name: 3}\n\t    }\n", "    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_option(cf2, cf_option1, 1, allow_new_option=True)\n\t    expected_common = {full_cf_option1_name: 1}\n\t    expected_specific = {\n\t        cf1: {full_cf_option2_name: 2},\n\t        cf2: {full_cf_option3_name: 3}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_option(cf2, cf_option1, 5)\n\t    expected_common = {}\n", "    expected_specific = {\n\t        cf1: {full_cf_option1_name: 1,\n\t              full_cf_option2_name: 2},\n\t        cf2: {full_cf_option1_name: 5,\n\t              full_cf_option3_name: 3}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_table_option(cf2, cf_table_option1, 100, allow_new_option=True)\n\t    expected_common = {}\n\t    expected_specific = {\n", "        cf1: {full_cf_option1_name: 1,\n\t              full_cf_option2_name: 2},\n\t        cf2: {full_cf_option1_name: 5,\n\t              full_cf_option3_name: 3,\n\t              full_cf_table_option1_name: 100}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_table_option(cf1, cf_table_option1, 100, allow_new_option=True)\n\t    expected_common = {full_cf_table_option1_name: 100}\n\t    expected_specific = {\n", "        cf1: {full_cf_option1_name: 1,\n\t              full_cf_option2_name: 2},\n\t        cf2: {full_cf_option1_name: 5,\n\t              full_cf_option3_name: 3}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_table_option(cf1, cf_table_option2, 200, allow_new_option=True)\n\t    expected_common = {full_cf_table_option1_name: 100}\n\t    expected_specific = {\n\t        cf1: {full_cf_option1_name: 1,\n", "              full_cf_option2_name: 2,\n\t              full_cf_table_option2_name: 200},\n\t        cf2: {full_cf_option1_name: 5,\n\t              full_cf_option3_name: 3}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_table_option(cf2, cf_table_option2, 200, allow_new_option=True)\n\t    expected_common = {\n\t        full_cf_table_option1_name: 100,\n\t        full_cf_table_option2_name: 200\n", "    }\n\t    expected_specific = {\n\t        cf1: {full_cf_option1_name: 1,\n\t              full_cf_option2_name: 2},\n\t        cf2: {full_cf_option1_name: 5,\n\t              full_cf_option3_name: 3}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_option(cf1, cf_option3, 3, allow_new_option=True)\n\t    expected_common = {\n", "        full_cf_option3_name: 3,\n\t        full_cf_table_option1_name: 100,\n\t        full_cf_table_option2_name: 200\n\t    }\n\t    expected_specific = {\n\t        cf1: {full_cf_option1_name: 1,\n\t              full_cf_option2_name: 2},\n\t        cf2: {full_cf_option1_name: 5}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n", "    dbo.set_cf_option(cf1, cf_option1, 5)\n\t    expected_common = {\n\t        full_cf_option1_name: 5,\n\t        full_cf_option3_name: 3,\n\t        full_cf_table_option1_name: 100,\n\t        full_cf_table_option2_name: 200\n\t    }\n\t    expected_specific = {\n\t        cf1: {full_cf_option2_name: 2},\n\t        cf2: {}\n", "    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\t    dbo.set_cf_option(cf2, cf_option2, 2, allow_new_option=True)\n\t    expected_common = {\n\t        full_cf_option1_name: 5,\n\t        full_cf_option2_name: 2,\n\t        full_cf_option3_name: 3,\n\t        full_cf_table_option1_name: 100,\n\t        full_cf_table_option2_name: 200\n\t    }\n", "    expected_specific = {\n\t        cf1: {},\n\t        cf2: {}\n\t    }\n\t    assert get_opts(dbo) == (expected_common, expected_specific)\n\tdef test_get_cfs_common_and_specific_diff_dicts():\n\t    get_cfs_diff = calc_utils.get_cfs_common_and_specific_diff_dicts\n\t    # db_cf = utils.NO_CF\n\t    dflt_cf = utils.DEFAULT_CF_NAME\n\t    cf1 = 'cf1'\n", "    # cf2 = 'cf2'\n\t    db_option1 = \"DB-Options1\"\n\t    db_option2 = \"DB-Options2\"\n\t    cf_option1 = \"CF-Option1\"\n\t    cf_option2 = \"CF-Option2\"\n\t    # cf_option3 = \"CF-Option3\"\n\t    # cf_table_option1 = \"CF-Table-Option1\"\n\t    # cf_table_option2 = \"CF-Table-Option2\"\n\t    full_cf_option1_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n\t                                                        cf_option1)\n", "    def extract_dict(cfs_options_diff):\n\t        assert isinstance(cfs_options_diff, db_opts.CfsOptionsDiff)\n\t        return cfs_options_diff.get_diff_dict(exclude_compared_cfs_names=True)\n\t    base_dbo = db_opts.DatabaseOptions()\n\t    log_dbo = db_opts.DatabaseOptions()\n\t    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\t    base_dbo.set_db_wide_option(db_option1, 1, allow_new_option=True)\n\t    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\t    log_dbo.set_db_wide_option(db_option2, 1, allow_new_option=True)\n\t    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n", "    # base has a cf option but log has no cf-s => empty diff\n\t    base_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n\t    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\t    base_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n\t    log_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n\t    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {dflt_cf: None})\n\t    base_dbo.set_cf_option(dflt_cf, cf_option1, 11, allow_new_option=True)\n\t    log_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n\t    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n\t    assert actual_common.get_diff_dict(exclude_compared_cfs_names=True) == {\n", "        full_cf_option1_name: (11, 10)}\n\t    assert actual_specific == {dflt_cf: None}\n\t    base_dbo.set_cf_option(dflt_cf, cf_option2, 20, allow_new_option=True)\n\t    log_dbo.set_cf_option(dflt_cf, cf_option2, 20, allow_new_option=True)\n\t    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n\t    assert actual_common.get_diff_dict(exclude_compared_cfs_names=True) == {\n\t        full_cf_option1_name: (11, 10)}\n\t    assert actual_specific == {dflt_cf: None}\n\t    log_dbo.set_cf_option(cf1, cf_option2, 20, allow_new_option=True)\n\t    # base (dflt): opt1=11, opt2=20\n", "    # log: dflt: opt1=10,  opt2=20\n\t    #      cf1:  Missing   opt2=20\n\t    # Expected diff: Common: None (opt2 identical)\n\t    #                Specific: dflt: (11, 10), cf1: (11, Missing)\n\t    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n\t    actual_dflt = extract_dict(actual_specific[dflt_cf])\n\t    actual_cf1 = extract_dict(actual_specific[cf1])\n\t    assert actual_common == {}\n\t    assert actual_dflt == {full_cf_option1_name: (11, 10)}\n\t    assert actual_cf1 == {full_cf_option1_name: (11, \"Missing\")}\n", "    log_dbo.set_cf_option(cf1, cf_option1, 11, allow_new_option=True)\n\t    # base (dflt): opt1=11, opt2=20\n\t    # log: dflt: opt1=10,  opt2=20\n\t    #      cf1:  opt1=11   opt2=20\n\t    # Expected diff: Common: None (opt2 identical)\n\t    #                Specific: dflt: (11, 10), cf1: None\n\t    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n\t    actual_dflt = extract_dict(actual_specific[dflt_cf])\n\t    assert actual_common == {}\n\t    assert actual_dflt == {full_cf_option1_name: (11, 10)}\n", "    assert actual_specific[cf1] is None\n"]}
{"filename": "test/testing_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport utils\n\tfrom events import Event\n\tfrom log_entry import LogEntry\n\tfrom log_file import ParsedLog\n\tfrom test.sample_log_info import SampleLogInfo\n\tdef read_file(file_path):\n\t    with open(file_path, \"r\") as f:\n", "        return f.readlines()\n\tdef create_parsed_log(file_path):\n\t    log_lines = read_file(file_path)\n\t    return ParsedLog(SampleLogInfo.FILE_PATH, log_lines,\n\t                     should_init_baseline_info=False)\n\tdef entry_msg_to_entry(time_str, msg, process_id=\"0x123456\", code_pos=None):\n\t    if code_pos is not None:\n\t        line = f\"{time_str} {process_id} {code_pos} {msg}\"\n\t    else:\n\t        line = f\"{time_str} {process_id} {msg}\"\n", "    assert LogEntry.is_entry_start(line)\n\t    return LogEntry(0, line, last_line=True)\n\tdef line_to_entry(line, last_line=True):\n\t    assert LogEntry.is_entry_start(line)\n\t    return LogEntry(0, line, last_line)\n\tdef lines_to_entries(lines):\n\t    entries = []\n\t    entry = None\n\t    for i, line in enumerate(lines):\n\t        if LogEntry.is_entry_start(line):\n", "            if entry:\n\t                entries.append(entry.all_lines_added())\n\t            entry = LogEntry(i, line)\n\t        else:\n\t            assert entry\n\t            entry.add_line(line)\n\t    if entry:\n\t        entries.append(entry.all_lines_added())\n\t    return entries\n\tdef add_stats_entry_lines_to_counters_mngr(entry_lines, mngr):\n", "    entry = LogEntry(0, entry_lines[0])\n\t    for line in entry_lines[1:]:\n\t        entry.add_line(line)\n\t    mngr.add_entry(entry.all_lines_added())\n\tdef create_event_entry(job_id, time_str, event_type=None, cf_name=None,\n\t                       make_illegal_json=False, **kwargs):\n\t    event_line = time_str + \" \"\n\t    event_line += '7f4a8b5bb700 EVENT_LOG_v1 {\"time_micros\": '\n\t    event_line += str(utils.get_gmt_timestamp_us(time_str))\n\t    if event_type is not None:\n", "        event_line += f', \"event\": \"{str(event_type.value)}\"'\n\t    if cf_name is not None:\n\t        event_line += f', \"cf_name\": \"{cf_name}\"'\n\t    if job_id is not None:\n\t        event_line += f', \"job\": {job_id}'\n\t    for k, v in kwargs.items():\n\t        if isinstance(v, str):\n\t            event_line += f', \"{k}\": \"{v}\"'\n\t        elif isinstance(v, dict):\n\t            event_line += f', \"{k}\":' + ' {'\n", "            first_key = True\n\t            for k1, v1 in v.items():\n\t                if not first_key:\n\t                    event_line += \", \"\n\t                if isinstance(v1, str):\n\t                    event_line += f'\"{k1}\": \"{v1}\"'\n\t                else:\n\t                    event_line += f'\"{k1}\": {v1}'\n\t                first_key = False\n\t            event_line += \"}\"\n", "            pass\n\t        else:\n\t            event_line += f', \"{k}\": {v}'\n\t    if make_illegal_json:\n\t        event_line += \", \"\n\t    event_line += '}'\n\t    event_entry = LogEntry(0, event_line, True)\n\t    assert Event.is_an_event_entry(event_entry)\n\t    return event_entry\n\tdef entry_to_event(event_entry):\n", "    assert Event.is_an_event_entry(event_entry)\n\t    return Event(event_entry)\n\tdef create_event(job_id, cf_names, time_str, event_type=None, cf_name=None,\n\t                 make_illegal_json=False, **kwargs):\n\t    event_entry = create_event_entry(job_id, time_str, event_type, cf_name,\n\t                                     make_illegal_json, **kwargs)\n\t    assert Event.is_an_event_entry(event_entry)\n\t    return Event.create_event(event_entry)\n"]}
{"filename": "test/test_baseline_log_files_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport baseline_log_files_utils as baseline_utils\n\timport db_options\n\timport utils\n\tVer = baseline_utils.Version\n\tbaseline_logs_folder_path = \"../\" + utils.BASELINE_LOGS_FOLDER\n\tdef test_version():\n\t    assert str(Ver(\"1.2.3\")) == \"1.2.3\"\n", "    assert str(Ver(\"1.2\")) == \"1.2\"\n\t    assert str(Ver(\"10.20.30\")) == \"10.20.30\"\n\t    assert str(Ver(\"10.20\")) == \"10.20\"\n\t    v1 = Ver(\"1.2.3\")\n\t    assert v1.major == 1\n\t    assert v1.minor == 2\n\t    assert v1.patch == 3\n\t    v2 = Ver(\"4.5\")\n\t    assert v2.major == 4\n\t    assert v2.minor == 5\n", "    assert v2.patch is None\n\t    assert Ver(\"1.2.3\") < Ver(\"2.1.1\")\n\t    assert Ver(\"2.1.1\") > Ver(\"1.2.3\")\n\t    assert Ver(\"2.2.3\") < Ver(\"2.3.1\")\n\t    assert Ver(\"2.2.1\") < Ver(\"2.2.2\")\n\t    assert not Ver(\"2.2\") < Ver(\"2.2\")\n\t    assert Ver(\"2.2\") < Ver(\"2.2.0\")\n\t    assert Ver(\"2.2\") < Ver(\"2.3\")\n\t    assert Ver(\"2.2\") < Ver(\"2.2.2\")\n\t    assert Ver(\"2.2.1\") > Ver(\"2.2\")\n", "    assert Ver(\"2.2.3\") == Ver(\"2.2.3\")\n\t    assert Ver(\"2.2\") == Ver(\"2.2\")\n\t    assert Ver(\"2.2.3\") != Ver(\"2.2\")\n\tdef test_find_closest_version():\n\t    # prepare an unsorted list on purpose\n\t    baseline_versions = [Ver(\"1.2\"),\n\t                         Ver(\"1.2.0\"),\n\t                         Ver(\"1.2.1\"),\n\t                         Ver(\"2.1\"),\n\t                         Ver(\"2.1.0\"),\n", "                         Ver(\"3.0\"),\n\t                         Ver(\"3.0.0\")]\n\t    find = baseline_utils.find_closest_version_idx\n\t    assert find(baseline_versions, Ver(\"1.1\")) is None\n\t    assert find(baseline_versions, Ver(\"1.1.9\")) is None\n\t    assert find(baseline_versions, Ver(\"1.2\")) == 0  # Ver(\"1.2\")\n\t    assert find(baseline_versions, Ver(\"1.2.0\")) == 1  # Ver(\"1.2.0\")\n\t    assert find(baseline_versions, Ver(\"1.2.2\")) == 2  # Ver(\"1.2.1\")\n\t    assert find(baseline_versions, Ver(\"2.0.0\")) == 2  # Ver(\"1.2.1\")\n\t    assert find(baseline_versions, Ver(\"2.1\")) == 3  # Ver(\"2.1\")\n", "    assert find(baseline_versions, Ver(\"2.1.0\")) == 4  # Ver(\"2.1.0\")\n\t    assert find(baseline_versions, Ver(\"2.1.1\")) == 4  # Ver(\"2.1.0\")\n\t    assert find(baseline_versions, Ver(\"3.0\")) == 5  # Ver(\"3.0\")\n\t    assert find(baseline_versions, Ver(\"3.0.0\")) == 6  # Ver(\"3.0.0\")\n\t    assert find(baseline_versions, Ver(\"3.0.1\")) == 6  # Ver(\"3.0.0\")\n\t    assert find(baseline_versions, Ver(\"3.1\")) == 6  # Ver(\"3.0.0\")\n\t    assert find(baseline_versions, Ver(\"99.99\")) == 6  # Ver(\"3.0.0\")\n\t    assert find(baseline_versions, Ver(\"99.99.99\")) == 6  # Ver(\"3.0.0\")\n\tdef test_find_closest_baseline_log_file():\n\t    find = baseline_utils.find_closest_baseline_info\n", "    versions_to_test_info = [\n\t        (\"1.0.0\", utils.ProductName.SPEEDB, None, None),\n\t        (\"2.2.1\", utils.ProductName.SPEEDB, \"LOG-speedb-2.2.1\",\n\t         \"2.2.1\"),\n\t        (\"2.2.2\", utils.ProductName.SPEEDB, \"LOG-speedb-2.2.1\",\n\t         \"2.2.1\"),\n\t        (\"32.0.1\", utils.ProductName.SPEEDB, \"LOG-speedb-2.5.0\",\n\t         \"2.5.0\"),\n\t        (\"6.0.0\", utils.ProductName.ROCKSDB, None, None),\n\t        (\"6.26.0\", utils.ProductName.ROCKSDB,\n", "         \"LOG-rocksdb-6.26.0\", \"6.26.0\"),\n\t        (\"6.27.0\", utils.ProductName.ROCKSDB,\n\t         \"LOG-rocksdb-6.26.0\", \"6.26.0\"),\n\t        (\"7.10.11\", utils.ProductName.ROCKSDB,\n\t         \"LOG-rocksdb-7.9.2\", \"7.9.2\"),\n\t        (\"8.0.0\", utils.ProductName.ROCKSDB,\n\t         \"LOG-rocksdb-8.0.0\", \"8.0.0\"),\n\t        (\"9.0.0\", utils.ProductName.ROCKSDB,\n\t         \"LOG-rocksdb-8.0.0\", \"8.0.0\")\n\t    ]\n", "    for version_info in versions_to_test_info:\n\t        closest_baseline_info = \\\n\t            find(baseline_logs_folder_path,\n\t                 version_info[1],\n\t                 Ver(version_info[0]))\n\t        if version_info[2] is None:\n\t            assert closest_baseline_info is None\n\t        else:\n\t            assert closest_baseline_info is not None\n\t            assert closest_baseline_info.file_path.name == version_info[2]\n", "            assert closest_baseline_info.version == Ver(version_info[3])\n\tdef test_find_options_diff():\n\t    baseline_info_rocksdb_7_7_3 = \\\n\t        baseline_utils.get_baseline_database_options(\n\t            baseline_logs_folder_path,\n\t            utils.ProductName.ROCKSDB,\n\t            version_str=\"7.7.3\")\n\t    assert isinstance(baseline_info_rocksdb_7_7_3,\n\t                      baseline_utils.BaselineDBOptionsInfo)\n\t    database_options_rocksdb_7_7_3 = \\\n", "        baseline_info_rocksdb_7_7_3.baseline_options\n\t    diff_rocksdb_7_7_3_vs_itself = \\\n\t        baseline_utils.find_options_diff_relative_to_baseline(\n\t            baseline_logs_folder_path,\n\t            utils.ProductName.ROCKSDB,\n\t            Ver(\"7.7.3\"),\n\t            database_options_rocksdb_7_7_3)\n\t    assert isinstance(diff_rocksdb_7_7_3_vs_itself,\n\t                      baseline_utils.OptionsDiffRelToBaselineInfo)\n\t    assert diff_rocksdb_7_7_3_vs_itself.diff is None\n", "    assert diff_rocksdb_7_7_3_vs_itself.closest_version == Ver(\"7.7.3\")\n\t    baseline_info_rocksdb_2_1_0 = \\\n\t        baseline_utils.get_baseline_database_options(\n\t            baseline_logs_folder_path,\n\t            utils.ProductName.SPEEDB,\n\t            version_str=\"2.1.0\")\n\t    database_options_speedb_2_1_0 = \\\n\t        baseline_info_rocksdb_2_1_0.baseline_options\n\t    diff_speedb_2_1_0_vs_itself = \\\n\t        baseline_utils.find_options_diff_relative_to_baseline(\n", "            baseline_logs_folder_path,\n\t            utils.ProductName.SPEEDB,\n\t            Ver(\"2.1.0\"),\n\t            database_options_speedb_2_1_0)\n\t    assert diff_speedb_2_1_0_vs_itself.diff is None\n\t    assert diff_speedb_2_1_0_vs_itself.closest_version == Ver(\"2.1.0\")\n\t    expected_diff = {}\n\t    updated_database_options_2_1_0 = database_options_speedb_2_1_0\n\t    curr_max_open_files_value = \\\n\t        updated_database_options_2_1_0.get_db_wide_option('max_open_files')\n", "    new_max_open_files_value = str(int(curr_max_open_files_value) + 100)\n\t    updated_database_options_2_1_0.set_db_wide_option('max_open_files',\n\t                                                      new_max_open_files_value)\n\t    expected_diff['DBOptions.max_open_files'] = \\\n\t        {utils.NO_CF: (curr_max_open_files_value,\n\t                       new_max_open_files_value)}\n\t    updated_database_options_2_1_0.set_db_wide_option(\"NEW_DB_WIDE_OPTION1\",\n\t                                                      \"NEW_DB_WIDE_VALUE1\",\n\t                                                      True)\n\t    expected_diff['DBOptions.NEW_DB_WIDE_OPTION1'] = \\\n", "        {utils.NO_CF: (db_options.SANITIZED_NO_VALUE, \"NEW_DB_WIDE_VALUE1\")}\n\t    cf_name = \"default\"\n\t    curr_ttl_value = updated_database_options_2_1_0.get_cf_option(cf_name,\n\t                                                                  \"ttl\")\n\t    new_ttl_value = str(int(curr_ttl_value) + 1000)\n\t    updated_database_options_2_1_0.set_cf_option(cf_name, \"ttl\",\n\t                                                 new_ttl_value)\n\t    updated_database_options_2_1_0.set_cf_option('default', 'ttl',\n\t                                                 new_ttl_value)\n\t    expected_diff['CFOptions.ttl'] = {'default': (curr_ttl_value,\n", "                                                  new_ttl_value)}\n\t    updated_database_options_2_1_0.set_cf_option(cf_name, \"NEW_CF_OPTION1\",\n\t                                                 \"NEW_CF_VALUE1\", \"True\")\n\t    expected_diff['CFOptions.NEW_CF_OPTION1'] =\\\n\t        {'default': (db_options.SANITIZED_NO_VALUE, \"NEW_CF_VALUE1\")}\n\t    curr_block_align_value = \\\n\t        updated_database_options_2_1_0.get_cf_table_option(cf_name,\n\t                                                           \"block_align\")\n\t    new_block_align_value = \"dummy_block_align_value\"\n\t    updated_database_options_2_1_0.set_cf_table_option(cf_name,\n", "                                                       \"block_align\",\n\t                                                       new_block_align_value)\n\t    expected_diff['TableOptions.BlockBasedTable.block_align'] = \\\n\t        {'default': (curr_block_align_value, new_block_align_value)}\n\t    updated_database_options_2_1_0.set_cf_table_option(cf_name,\n\t                                                       \"NEW_CF_TABLE_OPTION1\",\n\t                                                       \"NEW_CF_TABLE_VALUE1\",\n\t                                                       \"True\")\n\t    expected_diff['TableOptions.BlockBasedTable.NEW_CF_TABLE_OPTION1'] = \\\n\t        {'default': (db_options.SANITIZED_NO_VALUE, \"NEW_CF_TABLE_VALUE1\")}\n", "    diff_speedb_2_1_0_vs_itself = \\\n\t        baseline_utils.find_options_diff_relative_to_baseline(\n\t            baseline_logs_folder_path,\n\t            utils.ProductName.SPEEDB,\n\t            Ver(\"2.1.0\"),\n\t            updated_database_options_2_1_0)\n\t    assert diff_speedb_2_1_0_vs_itself.diff.get_diff_dict() == expected_diff\n\t    assert diff_speedb_2_1_0_vs_itself.closest_version == Ver(\"2.1.0\")\n"]}
{"filename": "test/log_analyzer_test_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport log_entry\n\tdef read_sample_file(file_name, expected_num_entries):\n\t    file_path = \"input_files/\" + file_name\n\t    f = open(file_path)\n\t    lines = f.readlines()\n\t    entries = []\n\t    entry = None\n", "    for i, line in enumerate(lines):\n\t        if log_entry.LogEntry.is_entry_start(line):\n\t            if entry:\n\t                entries.append(entry.all_lines_added())\n\t            entry = log_entry.LogEntry(i, line)\n\t        else:\n\t            assert entry\n\t            entry.add_line(line)\n\t    if entry:\n\t        entries.append(entry.all_lines_added())\n", "    assert len(entries) == expected_num_entries\n\t    return entries\n"]}
{"filename": "test/__init__.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n"]}
{"filename": "test/test_cfs_infos.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport test.testing_utils as test_utils\n\tfrom cfs_infos import CfMetadata, CfsMetadata, CfDiscoveryType\n\tdefault = \"default\"\n\tcf1 = \"cf1\"\n\tcf2 = \"cf2\"\n\tcf3 = \"cf3\"\n\tcf1_id = 10\n", "cf2_id = 20\n\tcf1_options_line = \\\n\t    '2023/03/07-16:05:55.538873 8379 [/column_family.cc:631] ' \\\n\t    f'--------------- Options for column family [{cf1}]:'\n\tcf1_recovered_line = \\\n\t    '2023/03/07-16:05:55.532774 8379 [/version_set.cc:5585] ' \\\n\t    f'Column family [{cf1}] (ID {cf1_id}), log number is 0'.strip()\n\tcf1_cf2_id_recovered_line = \\\n\t    '2023/03/07-16:05:55.532774 8379 [/version_set.cc:5585] ' \\\n\t    f'Column family [{cf1}] (ID {cf2_id}), log number is 0'.strip()\n", "cf1_created_line = \\\n\t    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n\t    f'Created column family [{cf1}] (ID {cf1_id})'\n\tcf1_cf2_id_created_line = \\\n\t    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n\t    f'Created column family [{cf1}] (ID {cf2_id})'\n\tcf2_created_line = \\\n\t    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n\t    f'Created column family [{cf2}] (ID {cf2_id})'\n\tcf2_cf1_id_created_line = \\\n", "    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n\t    f'Created column family [{cf2}] (ID {cf1_id})'\n\tcf1_drop_time = '2023/03/07-16:06:09.037627'\n\tcf1_dropped_line = \\\n\t    f'{cf1_drop_time} 8432 [/db_impl/db_impl.cc:3311] ' \\\n\t    f'Dropped column family with id {cf1_id}'\n\tcf2_dropped_line = \\\n\t    '2023/03/07-16:06:09.037627 8432 [/db_impl/db_impl.cc:3311] ' \\\n\t    f'Dropped column family with id {cf2_id}'\n\tcf1_options_entry = test_utils.line_to_entry(cf1_options_line)\n", "cf1_recovered_entry = test_utils.line_to_entry(cf1_recovered_line)\n\tcf1_cf2_id_recovered_entry = \\\n\t    test_utils.line_to_entry(cf1_cf2_id_recovered_line)\n\tcf1_created_entry = test_utils.line_to_entry(cf1_created_line)\n\tcf1_cf2_id_created_entry = test_utils.line_to_entry(cf1_cf2_id_created_line)\n\tcf2_created_entry = test_utils.line_to_entry(cf2_created_line)\n\tcf2_cf1_id_created_entry = test_utils.line_to_entry(cf2_cf1_id_created_line)\n\tcf1_dropped_entry = test_utils.line_to_entry(cf1_dropped_line)\n\tcf2_dropped_entry = test_utils.line_to_entry(cf2_dropped_line)\n\tdef test_empty():\n", "    cfs = CfsMetadata(\"dummy-path\")\n\t    assert cfs.get_cf_id(cf1) is None\n\t    assert cfs.get_cf_info_by_name(cf1) is None\n\t    assert cfs.get_cf_info_by_id(cf1_id) is None\n\t    assert cfs.get_non_auto_generated_cfs_names() == []\n\t    assert cfs.get_auto_generated_cf_names() == []\n\t    assert cfs.get_num_cfs() == 0\n\tdef test_add_cf_found_during_options_parsing_cf_name_known():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    assert cfs.add_cf_found_during_cf_options_parsing(cf1,\n", "                                                      cf1_id,\n\t                                                      is_auto_generated=False,\n\t                                                      entry=cf1_options_entry)\n\t    expected_cf_metadata = \\\n\t        CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n\t                   name=cf1,\n\t                   discovery_time='2023/03/07-16:05:55.538873',\n\t                   has_options=True,\n\t                   auto_generated=False,\n\t                   id=cf1_id,\n", "                   drop_time=None)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n\t    assert cfs.get_cf_info_by_name(cf1) == expected_cf_metadata\n\t    assert cfs.get_cf_info_by_id(cf1_id) == expected_cf_metadata\n\t    assert cfs.get_non_auto_generated_cfs_names() == [cf1]\n\t    assert cfs.get_auto_generated_cf_names() == []\n\t    assert cfs.get_num_cfs() == 1\n\t    assert not cfs.was_cf_dropped(cf1)\n\t    assert cfs.get_cf_drop_time(cf1) is None\n\tdef test_add_cf_found_during_options_parsing_auto_generated_cf_name():\n", "    cfs = CfsMetadata(\"dummy-path\")\n\t    unknown_cf_name = \"Unknonw-1\"\n\t    options_entry = \\\n\t        test_utils.line_to_entry(\n\t            '2023/01/23-22:09:21.013513 7f6bdfd54700 '\n\t            'Options.comparator: leveldb.BytewiseComparator')\n\t    assert cfs.add_cf_found_during_cf_options_parsing(cf_name=unknown_cf_name,\n\t                                                      cf_id=None,\n\t                                                      is_auto_generated=True,\n\t                                                      entry=options_entry)\n", "    expected_cf_metadata = \\\n\t        CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n\t                   name=unknown_cf_name,\n\t                   discovery_time='2023/01/23-22:09:21.013513',\n\t                   has_options=True,\n\t                   auto_generated=True,\n\t                   id=None,\n\t                   drop_time=None)\n\t    assert cfs.get_cf_id(cf1) is None\n\t    assert cfs.get_cf_info_by_name(unknown_cf_name) == expected_cf_metadata\n", "    assert cfs.get_non_auto_generated_cfs_names() == []\n\t    assert cfs.get_auto_generated_cf_names() == [unknown_cf_name]\n\t    assert cfs.get_num_cfs() == 1\n\t    assert not cfs.was_cf_dropped(cf1)\n\t    assert cfs.get_cf_drop_time(cf1) is None\n\tdef test_handle_cf_name_found_during_parsing():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    assert cfs.add_cf_found_during_cf_options_parsing(cf1,\n\t                                                      cf1_id,\n\t                                                      is_auto_generated=False,\n", "                                                      entry=cf1_options_entry)\n\t    stats_start_entry =\\\n\t        test_utils.line_to_entry('2023/01/23-22:19:21.014278 7f6bdfd54700 ['\n\t                                 '/db_impl/db_impl.cc:947]')\n\t    assert not cfs.handle_cf_name_found_during_parsing(cf1, stats_start_entry)\n\t    assert cfs.get_num_cfs() == 1\n\t    stats_start_entry =\\\n\t        test_utils.line_to_entry('2023/01/23-22:19:21.014278 7f6bdfd54700 ['\n\t                                 '/db_impl/db_impl.cc:947]')\n\t    assert cfs.handle_cf_name_found_during_parsing(cf2,\n", "                                                   stats_start_entry)\n\t    expected_cf_metadata = \\\n\t        CfMetadata(discovery_type=CfDiscoveryType.DURING_PARSING,\n\t                   name=cf2,\n\t                   discovery_time='2023/01/23-22:19:21.014278',\n\t                   has_options=False,\n\t                   auto_generated=False,\n\t                   id=None,\n\t                   drop_time=None)\n\t    assert cfs.get_cf_id(cf2) is None\n", "    assert cfs.get_cf_info_by_name(cf2) == expected_cf_metadata\n\t    assert cfs.get_cf_info_by_id(cf2_id) is None\n\t    assert cfs.get_non_auto_generated_cfs_names() == [cf1, cf2]\n\t    assert cfs.get_auto_generated_cf_names() == []\n\t    assert cfs.get_num_cfs() == 2\n\tdef test_parse_cf_recovered_entry():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    assert cfs.add_cf_found_during_cf_options_parsing(cf1,\n\t                                                      cf_id=None,\n\t                                                      is_auto_generated=False,\n", "                                                      entry=cf1_options_entry)\n\t    assert cfs.get_cf_id(cf1) is None\n\t    assert cfs.try_parse_as_cf_lifetime_entries(\n\t        [cf1_recovered_entry], entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == 10\n\t    # Illegal (different cf_id), but should be silently ignored\n\t    assert cfs.try_parse_as_cf_lifetime_entries(\n\t        [cf1_cf2_id_recovered_entry], entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == 10\n\t    assert cfs.try_parse_as_cf_lifetime_entries([cf1_recovered_entry],\n", "                                                entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == 10\n\tdef test_parse_cf_recovered_entry_2():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    # Recovered without options\n\t    assert cfs.try_parse_as_cf_lifetime_entries(\n\t        [cf1_recovered_entry], entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == 10\n\t    # Already discovered without options => Rejected\n\t    assert not cfs.add_cf_found_during_cf_options_parsing(\n", "        cf1, cf_id=None, is_auto_generated=False, entry=cf1_recovered_entry)\n\t    assert cfs.get_cf_id(cf1) == 10\n\tdef test_parse_cf_created_entry():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    assert cfs.add_cf_found_during_cf_options_parsing(\n\t        cf1, cf_id=None, is_auto_generated=False, entry=cf1_recovered_entry)\n\t    assert cfs.get_cf_id(cf1) is None\n\t    assert cfs.try_parse_as_cf_lifetime_entries([cf1_created_entry],\n\t                                                entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n", "    # Already discovered without options => Rejected\n\t    assert cfs.try_parse_as_cf_lifetime_entries([cf1_cf2_id_recovered_entry],\n\t                                                entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n\tdef test_parse_cf_created_entry_2():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    # Created without options\n\t    cfs.try_parse_as_cf_lifetime_entries(\n\t        [cf1_created_entry], entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n", "    assert not cfs.add_cf_found_during_cf_options_parsing(\n\t        cf1, cf_id=None, is_auto_generated=False, entry=cf1_recovered_entry)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n\tdef test_parse_cf_dropped_entry():\n\t    cfs = CfsMetadata(\"dummy-path\")\n\t    # Dropping a cf doesn't specify the dropped cf's name, only its id =>\n\t    # We may have an auto-generated cf (=> no id) that is later dropped\n\t    cfs.try_parse_as_cf_lifetime_entries([cf1_dropped_entry],\n\t                                         entry_idx=0)\n\t    assert not cfs.was_cf_dropped(cf1)\n", "    assert cfs.get_cf_drop_time(cf1) is None\n\t    assert cfs.add_cf_found_during_cf_options_parsing(\n\t        cf1, cf_id=cf1_id, is_auto_generated=False, entry=cf1_recovered_entry)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n\t    assert cfs.try_parse_as_cf_lifetime_entries([cf1_dropped_entry],\n\t                                                entry_idx=0) == (True, 1)\n\t    assert cfs.get_cf_id(cf1) == cf1_id\n\t    assert cfs.was_cf_dropped(cf1)\n\t    assert cfs.get_cf_drop_time(cf1) == cf1_drop_time\n\t    assert cfs.try_parse_as_cf_lifetime_entries(\n", "        [cf1_dropped_entry], entry_idx=0) == (True, 1)\n\t    assert cfs.was_cf_dropped(cf1)\n\t    assert cfs.get_cf_drop_time(cf1) == cf1_drop_time\n"]}
{"filename": "test/test_log_file_options_parser.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\tfrom log_entry import LogEntry\n\tfrom log_file_options_parser import get_table_options_topic_info, \\\n\t    LogFileOptionsParser\n\tfrom test.sample_log_info import SampleLogInfo, SampleRolledLogInfo\n\tdef test_get_table_options_topic_info():\n\t    assert get_table_options_topic_info(\n\t        \"metadata_cache_options\") == (\"metadata_cache_options\",\n", "                                      \"metadata_cache_\")\n\t    assert get_table_options_topic_info(\n\t        \"block_cache_options\") == (\"block_cache_options\", \"block_cache_\")\n\t    assert get_table_options_topic_info(\"block_cache\") is None\n\tdef read_sample_file(InfoClass):\n\t    f = open(InfoClass.FILE_PATH)\n\t    lines = f.readlines()\n\t    entries = []\n\t    entry = None\n\t    for i, line in enumerate(lines):\n", "        if LogEntry.is_entry_start(line):\n\t            if entry:\n\t                entries.append(entry.all_lines_added())\n\t            entry = LogEntry(i, line)\n\t        else:\n\t            assert entry\n\t            entry.add_line(line)\n\t    if entry:\n\t        entries.append(entry.all_lines_added())\n\t    assert len(entries) == InfoClass.NUM_ENTRIES\n", "    return entries\n\tdef test_try_parsing_as_options_entry():\n\t    date = \"2022/04/17-14:13:10.725596 7f4a9fdff700\"\n\t    context = \"7f4a9fdff700\"\n\t    line_start = date + \" \" + context + \" \"\n\t    option1 = \"Options.track_and_verify_wals_in_manifest: 0\"\n\t    option2 = \"Options.wal_dir: /data/rocksdb2/\"\n\t    option3 = \"Options.statistics: (nil)\"\n\t    option4 = \"Options.comparator: leveldb.BytewiseComparator\"\n\t    table_file_option = \"data_block_index_type: 0\"\n", "    assert (\"track_and_verify_wals_in_manifest\", \"0\") == \\\n\t           LogFileOptionsParser.try_parsing_as_options_entry(\n\t               LogEntry(0, line_start + option1, True))\n\t    assert (\"track_and_verify_wals_in_manifest\", \"0\") == \\\n\t           LogFileOptionsParser.try_parsing_as_options_entry(\n\t               LogEntry(0, line_start + option1 + \"   \", True))\n\t    assert (\"wal_dir\", \"/data/rocksdb2/\") == \\\n\t           LogFileOptionsParser.try_parsing_as_options_entry(\n\t               LogEntry(0, line_start + option2, True))\n\t    assert (\"statistics\", \"(nil)\") == \\\n", "           LogFileOptionsParser.try_parsing_as_options_entry(\n\t               LogEntry(0, line_start + option3, True))\n\t    assert (\"comparator\", \"leveldb.BytewiseComparator\") == \\\n\t           LogFileOptionsParser.try_parsing_as_options_entry(\n\t               LogEntry(0, line_start + option4, True))\n\t    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n\t        LogEntry(0, line_start, True))\n\t    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n\t        LogEntry(0, line_start + \"   \", True))\n\t    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n", "        LogEntry(0, line_start + \"Options.xxx\", True))\n\t    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n\t        LogEntry(0, line_start + table_file_option, True))\n\tdef test_try_parsing_as_table_options_entry():\n\t    date = \"2022/04/17-14:13:10.725596\"\n\t    context = \"7f4a9fdff700\"\n\t    line_start = date + \" \" + context + \" \"\n\t    option1 = \"Options.wal_dir: /data/rocksdb2/\"\n\t    table_options_start = \"table_factory options:\"\n\t    table_options_line_start = line_start + table_options_start\n", "    table_option1 = \"flush_block_policy_factory: \" \\\n\t                    \"FlushBlockBySizePolicyFactory (0x7f4af4091b90)\"\n\t    table_option2 = \"cache_index_and_filter_blocks: 1\"\n\t    table_option_special_value = \" metadata_cache_options: \"\n\t    table_option_special_value_cont1 = \"  partition_pinning: 0 \"\n\t    expected_result = dict()\n\t    table_options_entry = LogEntry(0, table_options_line_start + \" \" +\n\t                                   table_option1)\n\t    expected_result[\"flush_block_policy_factory\"] = \\\n\t        \"FlushBlockBySizePolicyFactory (0x7f4af4091b90)\"\n", "    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n\t        table_options_entry)\n\t    assert expected_result == actual_result\n\t    table_options_entry.add_line(table_option2)\n\t    expected_result[\"cache_index_and_filter_blocks\"] = '1'\n\t    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n\t        table_options_entry)\n\t    assert expected_result == actual_result\n\t    table_options_entry.add_line(table_option_special_value)\n\t    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n", "        table_options_entry)\n\t    assert expected_result == actual_result\n\t    table_options_entry.add_line(table_option_special_value_cont1)\n\t    expected_result[\"metadata_cache_partition_pinning\"] = \"0\"\n\t    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n\t        table_options_entry)\n\t    assert actual_result == expected_result\n\t    options_entry = LogEntry(0, line_start + option1, True)\n\t    assert LogFileOptionsParser.try_parsing_as_options_entry(options_entry)\n\t    assert not LogFileOptionsParser.try_parsing_as_table_options_entry(\n", "        options_entry)\n\tdef test_parse_db_wide_options():\n\t    log_entries = read_sample_file(SampleLogInfo)\n\t    start_entry_idx = SampleLogInfo.DB_WIDE_OPTIONS_START_ENTRY_IDX\n\t    actual_options_dict =\\\n\t        LogFileOptionsParser.parse_db_wide_options(\n\t            log_entries,\n\t            start_entry_idx,\n\t            SampleLogInfo.SUPPORT_INFO_START_ENTRY_IDX)\n\t    assert actual_options_dict == SampleLogInfo.DB_WIDE_OPTIONS_DICT\n", "def test_parsing_as_table_options_entry():\n\t    log_entries = read_sample_file(SampleLogInfo)\n\t    for i, idx in enumerate(SampleLogInfo.TABLE_OPTIONS_ENTRIES_INDICES):\n\t        actual_options_dict = \\\n\t            LogFileOptionsParser.try_parsing_as_table_options_entry(\n\t                log_entries[idx])\n\t        assert SampleLogInfo.TABLE_OPTIONS_DICTS[i] == actual_options_dict\n\tdef test_parse_cf_options_with_cf_options_header():\n\t    log_entries = read_sample_file(SampleLogInfo)\n\t    for i, idx in enumerate(SampleLogInfo.OPTIONS_ENTRIES_INDICES):\n", "        cf_name, options_dict, table_options_dict, entry_idx, \\\n\t            duplicate_option =\\\n\t            LogFileOptionsParser.parse_cf_options(log_entries, idx)\n\t        assert cf_name == SampleLogInfo.CF_NAMES[i]\n\t        assert options_dict == SampleLogInfo.OPTIONS_DICTS[i]\n\t        assert table_options_dict == SampleLogInfo.TABLE_OPTIONS_DICTS[i]\n\t        assert not duplicate_option\n\t        # +1 entry for the cf options start line (not a cf-options entry)\n\t        # +1 for the table options entry (single entry)\n\t        num_parsed_entries = 1 + len(options_dict) + 1\n", "        assert entry_idx == idx + num_parsed_entries\n\tdef test_parse_cf_options_without_cf_options_header():\n\t    log_entries = read_sample_file(SampleRolledLogInfo)\n\t    for i, idx in enumerate(SampleRolledLogInfo.OPTIONS_ENTRIES_INDICES):\n\t        provided_cf_name = SampleLogInfo.CF_NAMES[i]\n\t        cf_name, options_dict, table_options_dict, entry_idx, \\\n\t            duplicate_option =\\\n\t            LogFileOptionsParser.parse_cf_options(log_entries, idx,\n\t                                                  provided_cf_name)\n\t        assert cf_name == provided_cf_name\n", "        assert options_dict == SampleLogInfo.OPTIONS_DICTS[i]\n\t        assert table_options_dict == SampleLogInfo.TABLE_OPTIONS_DICTS[i]\n\t        # the last cf should NOT have a duplicate, the ones befire it should\n\t        assert duplicate_option == (i+1 < len(\n\t            SampleRolledLogInfo.OPTIONS_ENTRIES_INDICES))\n\t        # +1 for the table options entry (single entry)\n\t        num_parsed_entries = len(options_dict) + 1\n\t        assert entry_idx == idx + num_parsed_entries\n"]}
{"filename": "test/test_warnings_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport utils\n\tfrom log_entry import LogEntry\n\tfrom warnings_mngr import WarningCategory, WarningElementInfo, WarningsMngr\n\tdef create_warning_entry(warning_type, cf_name, warning_element_info):\n\t    assert isinstance(warning_type, utils.WarningType)\n\t    assert isinstance(warning_element_info, WarningElementInfo)\n\t    warning_line = warning_element_info.time + \" \"\n", "    warning_line += '7f4a8b5bb700 '\n\t    warning_line += f'[{warning_type.name}] '\n\t    warning_line += f'[{warning_element_info.code_pos}] '\n\t    if cf_name != utils.NO_CF:\n\t        warning_line += f'[{cf_name}] '\n\t    warning_line += warning_element_info.warning_msg\n\t    warning_entry = LogEntry(0, warning_line, True)\n\t    assert warning_entry.is_warn_msg()\n\t    assert warning_entry.get_warning_type() == warning_type\n\t    assert warning_entry.get_code_pos() == warning_element_info.code_pos\n", "    return warning_entry\n\tdef add_warning(mngr, warning_type, time, code_pos, cf_name, warn_msg):\n\t    assert isinstance(warning_type, utils.WarningType)\n\t    warning_element_info = WarningElementInfo(time, code_pos, warn_msg)\n\t    warn_entry = create_warning_entry(\n\t        warning_type, cf_name, warning_element_info)\n\t    assert mngr.try_adding_entry(warn_entry)\n\tdef test_non_warnings_entries():\n\t    line1 = '''2022/04/17-14:21:50.026058 7f4a8b5bb700 [/flush_job.cc:333]\n\t    [cf1] [JOB 9] Flushing memtable with next log file: 5'''\n", "    line2 = '''2022/04/17-14:21:50.026087 7f4a8b5bb700 EVENT_LOG_v1\n\t    {\"time_micros\": 1650205310026076, \"job\": 9, \"event\": \"flush_started\"'''\n\t    cfs_names = [\"cf1\", \"cf2\"]\n\t    warnings_mngr = WarningsMngr()\n\t    assert LogEntry.is_entry_start(line1)\n\t    entry1 = LogEntry(0, line1, True)\n\t    assert LogEntry.is_entry_start(line2)\n\t    entry2 = LogEntry(0, line2, True)\n\t    assert not warnings_mngr.try_adding_entry(entry1)\n\t    assert not warnings_mngr.try_adding_entry(entry2)\n", "    warnings_mngr.set_cfs_names_on_parsing_complete(cfs_names)\n\tdef test_warn_entries_empty():\n\t    mngr = WarningsMngr()\n\t    mngr.set_cfs_names_on_parsing_complete([\"cf1\", \"cf2\"])\n\t    assert mngr.get_all_warnings() == {}\n\tdef test_warn_entries_basic():\n\t    cf1 = \"cf1\"\n\t    cf2 = \"cf2\"\n\t    cf3 = \"cf3\"\n\t    cfs_names = [cf1, cf2, cf3]\n", "    time1 = \"2022/04/17-14:21:50.026058\"\n\t    time2 = \"2022/04/17-14:21:51.026058\"\n\t    time3 = \"2022/04/17-14:21:52.026058\"\n\t    time4 = \"2022/04/17-14:21:53.026058\"\n\t    time5 = \"2022/04/17-14:21:54.026058\"\n\t    warn_msg1 = \"Warning Message 1\"\n\t    cf_warn_msg1 = f\"[{cf1}] {warn_msg1}\"\n\t    warn_msg2 = \"Warning Message 2\"\n\t    cf_warn_msg2 = f\"[{cf2}] {warn_msg2}\"\n\t    warn_msg3 = \"Warning Message 3\"\n", "    cf_warn_msg3 = f\"[{cf2}] {warn_msg3}\"\n\t    delay_msg = \"Stalling writes, L0 files 2, memtables 2\"\n\t    cf_delay_msg = f\"[{cf2}] {delay_msg}\"\n\t    stop_msg = \"Stopping writes Dummy Text 1\"\n\t    cf_stop_msg = f\"[{cf1}] {stop_msg}\"\n\t    code_pos1 = \"/flush_job.cc:333\"\n\t    code_pos2 = \"/column_family.cc:932\"\n\t    code_pos3 = \"/column_family1.cc:999\"\n\t    code_pos4 = \"/column_family2.cc:1111\"\n\t    mngr = WarningsMngr()\n", "    add_warning(mngr, utils.WarningType.WARN, time1, code_pos1, cf1, warn_msg1)\n\t    add_warning(mngr, utils.WarningType.ERROR, time2,\n\t                code_pos1, cf2, warn_msg2)\n\t    add_warning(mngr, utils.WarningType.WARN, time3, code_pos2, cf2, warn_msg3)\n\t    add_warning(mngr, utils.WarningType.WARN, time4, code_pos3, cf2, delay_msg)\n\t    add_warning(mngr, utils.WarningType.WARN, time5, code_pos4, cf1, stop_msg)\n\t    mngr.set_cfs_names_on_parsing_complete(cfs_names)\n\t    expected_cf1_warn_warnings = {\n\t        WarningCategory.WRITE_STOP:\n\t            [WarningElementInfo(time5, code_pos4, cf_stop_msg)],\n", "        WarningCategory.OTHER:\n\t            [WarningElementInfo(time1, code_pos1, cf_warn_msg1)]\n\t    }\n\t    expected_cf2_warn_warnings = {\n\t        WarningCategory.WRITE_DELAY:\n\t            [WarningElementInfo(time4, code_pos3, cf_delay_msg)],\n\t        WarningCategory.OTHER:\n\t            [WarningElementInfo(time3, code_pos2, cf_warn_msg3)]\n\t    }\n\t    expected_cf2_error_warnings = {\n", "        WarningCategory.OTHER:\n\t            [WarningElementInfo(time2, code_pos1, cf_warn_msg2)]}\n\t    expected_warn_warnings = {\n\t            cf1: expected_cf1_warn_warnings,\n\t            cf2: expected_cf2_warn_warnings\n\t    }\n\t    expected_error_warnings = {cf2: expected_cf2_error_warnings}\n\t    all_expected_warnings = {\n\t        utils.WarningType.WARN: expected_warn_warnings,\n\t        utils.WarningType.ERROR: expected_error_warnings\n", "    }\n\t    actual_cf1_warn_warnings = mngr.get_cf_warn_warnings(cf1)\n\t    assert actual_cf1_warn_warnings == expected_cf1_warn_warnings\n\t    actual_cf2_warn_warnings = mngr.get_cf_warn_warnings(cf2)\n\t    assert actual_cf2_warn_warnings == expected_cf2_warn_warnings\n\t    actual_cf2_error_warnings = mngr.get_cf_error_warnings(cf2)\n\t    assert actual_cf2_error_warnings == expected_cf2_error_warnings\n\t    assert mngr.get_cf_warn_warnings(cf3) is None\n\t    assert mngr.get_cf_error_warnings(cf3) is None\n\t    actual_warn_warnings = mngr.get_warn_warnings()\n", "    assert expected_warn_warnings == actual_warn_warnings\n\t    all_actual_warnings = mngr.get_all_warnings()\n\t    assert all_actual_warnings == all_expected_warnings\n"]}
{"filename": "test/test_db_files.py", "chunked_list": ["import copy\n\tfrom dataclasses import dataclass\n\timport db_files\n\timport events\n\timport utils\n\tfrom test.testing_utils import create_event\n\tjob_id1 = 1\n\tjob_id2 = 2\n\tcf1 = \"cf1\"\n\tcf2 = \"cf2\"\n", "cf_names = [cf1, cf2]\n\ttime1_minus_10_sec = \"2023/01/24-08:54:30.130553\"\n\ttime1 = \"2023/01/24-08:54:40.130553\"\n\ttime1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\n\ttime1_plus_11_sec = \"2023/01/24-08:55:50.130553\"\n\tfile_number1 = 1234\n\tfile_number2 = 5678\n\tfile_number3 = 9999\n\t@dataclass\n\tclass GlobalTestVars:\n", "    event_time_micros: int = 0\n\t    cmprsd_data_size_bytes: int = 62396458\n\t    num_data_blocks: int = 1000\n\t    total_keys_sizes_bytes: int = 2000\n\t    total_values_sizes_bytes: int = 3333\n\t    index_size: int = 3000\n\t    filter_size: int = 4000\n\t    num_entries: int = 5555\n\t    filter_policy: str = \"bloomfilter\"\n\t    num_filter_entries: int = 6666\n", "    compression_type: str = \"NoCompression\"\n\tdef get_table_properties(global_vars):\n\t    assert isinstance(global_vars, GlobalTestVars)\n\t    return {\n\t        \"data_size\": global_vars.cmprsd_data_size_bytes,\n\t        \"index_size\": global_vars.index_size,\n\t        \"index_partitions\": 0,\n\t        \"top_level_index_size\": 0,\n\t        \"index_key_is_user_key\": 1,\n\t        \"index_value_is_delta_encoded\": 1,\n", "        \"filter_size\": global_vars.filter_size,\n\t        \"raw_key_size\": global_vars.total_keys_sizes_bytes,\n\t        \"raw_average_key_size\": 24,\n\t        \"raw_value_size\": global_vars.total_values_sizes_bytes,\n\t        \"raw_average_value_size\": 1000,\n\t        \"num_data_blocks\": global_vars.num_data_blocks,\n\t        \"num_entries\": global_vars.num_entries,\n\t        \"num_filter_entries\": global_vars.num_filter_entries,\n\t        \"num_deletions\": 0,\n\t        \"num_merge_operands\": 0,\n", "        \"num_range_deletions\": 0,\n\t        \"format_version\": 0,\n\t        \"fixed_key_len\": 0,\n\t        \"filter_policy\": global_vars.filter_policy,\n\t        \"column_family_name\": \"default\",\n\t        \"column_family_id\": 0,\n\t        \"comparator\": \"leveldb.BytewiseComparator\",\n\t        \"merge_operator\": \"nullptr\",\n\t        \"prefix_extractor_name\": \"nullptr\",\n\t        \"property_collectors\": \"[]\",\n", "        \"compression\": global_vars.compression_type,\n\t        \"oldest_key_time\": 1672823099,\n\t        \"file_creation_time\": 1672823099,\n\t        \"slow_compression_estimated_data_size\": 0,\n\t        \"fast_compression_estimated_data_size\": 0,\n\t        \"db_id\": \"c100448c-dc04-4c74-8ab2-65d72f3aa3a8\",\n\t        \"db_session_id\": \"4GAWIG5RIF8PQWM3NOQG\",\n\t        \"orig_file_number\": 37155}\n\tdef test_create_delete_file():\n\t    vars = GlobalTestVars()\n", "    creation_event1 = create_event(job_id1, cf_names, time1,\n\t                                   events.EventType.TABLE_FILE_CREATION, cf1,\n\t                                   file_number=file_number1,\n\t                                   table_properties=get_table_properties(vars))\n\t    monitor = db_files.DbFilesMonitor()\n\t    assert monitor.get_all_live_files() == {}\n\t    assert monitor.get_cf_live_files(cf1) == []\n\t    expected_data_size_bytes = \\\n\t        vars.total_keys_sizes_bytes + vars.total_values_sizes_bytes\n\t    info1 = \\\n", "        db_files.DbFileInfo(\n\t            file_number=file_number1,\n\t            cf_name=cf1,\n\t            creation_time=time1,\n\t            deletion_time=None,\n\t            size_bytes=0,\n\t            compressed_size_bytes=0,\n\t            compressed_data_size_bytes=vars.cmprsd_data_size_bytes,\n\t            data_size_bytes=expected_data_size_bytes,\n\t            index_size_bytes=vars.index_size,\n", "            filter_size_bytes=vars.filter_size,\n\t            filter_policy=vars.filter_policy,\n\t            num_filter_entries=vars.num_filter_entries,\n\t            compression_type=vars.compression_type,\n\t            level=None)\n\t    assert monitor.new_event(creation_event1)\n\t    assert monitor.get_all_files() == {cf1: [info1]}\n\t    assert monitor.get_all_cf_files(cf1) == [info1]\n\t    assert monitor.get_all_cf_files(cf2) == []\n\t    assert monitor.get_all_live_files() == {cf1: [info1]}\n", "    assert monitor.get_cf_live_files(cf1) == [info1]\n\t    assert monitor.get_cf_live_files(cf2) == []\n\t    deletion_event = create_event(job_id1, cf_names, time1_plus_10_sec,\n\t                                  events.EventType.TABLE_FILE_DELETION, cf1,\n\t                                  file_number=file_number1)\n\t    assert monitor.new_event(deletion_event)\n\t    info1.deletion_time = time1_plus_10_sec\n\t    assert monitor.get_all_files() == {cf1: [info1]}\n\t    assert monitor.get_all_cf_files(cf1) == [info1]\n\t    assert monitor.get_all_cf_files(cf2) == []\n", "    assert monitor.get_all_live_files() == {}\n\t    assert monitor.get_cf_live_files(cf1) == []\n\t    assert monitor.get_cf_live_files(cf2) == []\n\t    creation_event2 = create_event(job_id1, cf_names, time1_plus_10_sec,\n\t                                   events.EventType.TABLE_FILE_CREATION, cf1,\n\t                                   file_number=file_number2,\n\t                                   table_properties=get_table_properties(vars))\n\t    info2 = copy.deepcopy(info1)\n\t    info2.file_number = file_number2\n\t    info2.creation_time = time1_plus_10_sec\n", "    info2.deletion_time = None\n\t    assert monitor.new_event(creation_event2)\n\t    assert monitor.get_all_files() == {cf1: [info1, info2]}\n\t    assert monitor.get_all_cf_files(cf1) == [info1, info2]\n\t    assert monitor.get_all_cf_files(cf2) == []\n\t    assert monitor.get_all_live_files() == {cf1: [info2]}\n\t    assert monitor.get_cf_live_files(cf2) == []\n\t    assert monitor.get_cf_live_files(cf1) == [info2]\n\t    creation_event3 = create_event(job_id1, cf_names, time1_plus_10_sec,\n\t                                   events.EventType.TABLE_FILE_CREATION, cf2,\n", "                                   file_number=file_number3,\n\t                                   table_properties=get_table_properties(vars))\n\t    info3 = copy.deepcopy(info1)\n\t    info3.file_number = file_number3\n\t    info3.cf_name = cf2\n\t    info3.creation_time = time1_plus_10_sec\n\t    info3.deletion_time = None\n\t    assert monitor.new_event(creation_event3)\n\t    assert monitor.get_all_files() == {cf1: [info1, info2], cf2: [info3]}\n\t    assert monitor.get_all_cf_files(cf1) == [info1, info2]\n", "    assert monitor.get_all_cf_files(cf2) == [info3]\n\t    assert monitor.get_all_live_files() == {cf1: [info2], cf2: [info3]}\n\t    assert monitor.get_cf_live_files(cf1) == [info2]\n\t    assert monitor.get_cf_live_files(cf2) == [info3]\n\tcreate_job_id = 100\n\tdelete_job_id = 200\n\telapsed_seconds = 0\n\tcreated_file_number = 1\n\tdef test_live_file_stats():\n\t    @dataclass\n", "    class CreateTestVars:\n\t        job_id: int = None\n\t        file_number: int = None\n\t        time: str = None\n\t        cf: str = None\n\t        index_size: int = None\n\t        filter_size: int = None\n\t        def __init__(self, cf, index_size, filter_size):\n\t            global create_job_id, elapsed_seconds, created_file_number\n\t            create_job_id += 1\n", "            self.job_id = create_job_id\n\t            created_file_number += 1\n\t            self.file_number = created_file_number\n\t            self.cf = cf\n\t            self.index_size = index_size\n\t            self.filter_size = filter_size\n\t            global elapsed_seconds\n\t            elapsed_seconds += 1\n\t            self.time = utils.get_time_relative_to(time1, elapsed_seconds)\n\t    @dataclass\n", "    class DeleteTestVars:\n\t        job_id: int = None\n\t        file_number: int = None\n\t        time: str = None\n\t        cf: str = None\n\t        def __init__(self, create_vars):\n\t            assert isinstance(create_vars, CreateTestVars)\n\t            global delete_job_id, elapsed_seconds\n\t            delete_job_id += 1\n\t            self.job_id = delete_job_id\n", "            self.file_number = create_vars.file_number\n\t            self.cf = create_vars.cf\n\t            elapsed_seconds += 1\n\t            self.time = utils.get_time_relative_to(time1, elapsed_seconds)\n\t    def create_test_file(monitor, vars, global_vars):\n\t        assert isinstance(vars, CreateTestVars)\n\t        global_vars.index_size = vars.index_size\n\t        global_vars.filter_size = vars.filter_size\n\t        creation_event = \\\n\t            create_event(vars.job_id, cf_names, vars.time,\n", "                         events.EventType.TABLE_FILE_CREATION, vars.cf,\n\t                         file_number=vars.file_number,\n\t                         table_properties=get_table_properties(global_vars))\n\t        assert monitor.new_event(creation_event)\n\t        return creation_event\n\t    def delete_test_file(monitor, vars):\n\t        assert isinstance(vars, DeleteTestVars)\n\t        deletion_event = create_event(vars.job_id, cf_names, vars.time,\n\t                                      events.EventType.TABLE_FILE_DELETION,\n\t                                      vars.cf,\n", "                                      file_number=vars.file_number)\n\t        assert monitor.new_event(deletion_event)\n\t    global_vars = GlobalTestVars()\n\t    cf1_create_1_vars = \\\n\t        CreateTestVars(cf=cf1, index_size=100, filter_size=200)\n\t    cf1_create_2_vars = \\\n\t        CreateTestVars(cf=cf1, index_size=50, filter_size=500)\n\t    cf1_create_3_vars = \\\n\t        CreateTestVars(cf=cf1, index_size=170, filter_size=30)\n\t    cf1_delete_1_vars = DeleteTestVars(cf1_create_1_vars)\n", "    cf1_delete_2_vars = DeleteTestVars(cf1_create_2_vars)\n\t    cf2_create_1_vars = \\\n\t        CreateTestVars(cf=cf2, index_size=100, filter_size=200)\n\t    cf2_create_2_vars = \\\n\t        CreateTestVars(cf=cf2, index_size=400, filter_size=200)\n\t    cf2_delete_1_vars = \\\n\t        DeleteTestVars(cf2_create_1_vars)\n\t    monitor = db_files.DbFilesMonitor()\n\t    create_test_file(monitor, cf1_create_1_vars, global_vars)\n\t    # Live - cf1: index=100, filter=200\n", "    delete_test_file(monitor, cf1_delete_1_vars)\n\t    # Live: - cf1: index=0, filter=0\n\t    cf1_create_2_event =\\\n\t        create_test_file(monitor, cf1_create_2_vars, global_vars)\n\t    cf1_create_2_time = cf1_create_2_event.get_log_time()\n\t    # Live: - cf1: index=50, filter=500\n\t    cf2_create_1_event =\\\n\t        create_test_file(monitor, cf2_create_1_vars, global_vars)\n\t    cf2_create_1_time = cf2_create_1_event.get_log_time()\n\t    # Live: - cf1: index=50, filter=500, cf2: index=100, filter=200\n", "    cf1_create_3_event = \\\n\t        create_test_file(monitor, cf1_create_3_vars, global_vars)\n\t    cf1_create_3_time = cf1_create_3_event.get_log_time()\n\t    # Live: - cf1: index=220, filter=530, cf2: index=100, filter=200\n\t    delete_test_file(monitor, cf2_delete_1_vars)\n\t    # Live: - cf1: index=220, filter=530, cf2: index=0, filter=0\n\t    delete_test_file(monitor, cf1_delete_2_vars)\n\t    # Live: - cf1: index=170, filter=30, cf2: index=0, filter=0\n\t    cf2_create_2_event =\\\n\t        create_test_file(monitor, cf2_create_2_vars, global_vars)\n", "    cf2_create_2_time = cf2_create_2_event.get_log_time()\n\t    # Live: - cf1: index=170, filter=30, cf2: index=400, filter=200\n\t    expected_cf1_index_stats = db_files.BlockLiveFileStats()\n\t    expected_cf1_index_stats.num_created = 3\n\t    expected_cf1_index_stats.num_live = 1\n\t    expected_cf1_index_stats.total_created_size_bytes = 320\n\t    expected_cf1_index_stats.curr_total_live_size_bytes = 170\n\t    expected_cf1_index_stats.max_size_bytes = 170\n\t    expected_cf1_index_stats.max_size_time = cf1_create_3_time\n\t    expected_cf1_index_stats.max_total_live_size_bytes = 220\n", "    expected_cf1_index_stats.max_total_live_size_time = cf1_create_3_time\n\t    expected_cf1_filter_stats = db_files.BlockLiveFileStats()\n\t    expected_cf1_filter_stats.num_created = 3\n\t    expected_cf1_filter_stats.num_live = 1\n\t    expected_cf1_filter_stats.total_created_size_bytes = 730\n\t    expected_cf1_filter_stats.curr_total_live_size_bytes = 30\n\t    expected_cf1_filter_stats.max_size_bytes = 500\n\t    expected_cf1_filter_stats.max_size_time = cf1_create_2_time\n\t    expected_cf1_filter_stats.max_total_live_size_bytes = 530\n\t    expected_cf1_filter_stats.max_total_live_size_time = cf1_create_3_time\n", "    expected_cf2_index_stats = db_files.BlockLiveFileStats()\n\t    expected_cf2_index_stats.num_created = 2\n\t    expected_cf2_index_stats.num_live = 1\n\t    expected_cf2_index_stats.total_created_size_bytes = 500\n\t    expected_cf2_index_stats.curr_total_live_size_bytes = 400\n\t    expected_cf2_index_stats.max_size_bytes = 400\n\t    expected_cf2_index_stats.max_size_time = cf2_create_2_time\n\t    expected_cf2_index_stats.max_total_live_size_bytes = 400\n\t    expected_cf2_index_stats.max_total_live_size_time = cf2_create_2_time\n\t    expected_cf2_filter_stats = db_files.BlockLiveFileStats()\n", "    expected_cf2_filter_stats.num_created = 2\n\t    expected_cf2_filter_stats.num_live = 1\n\t    expected_cf2_filter_stats.total_created_size_bytes = 400\n\t    expected_cf2_filter_stats.curr_total_live_size_bytes = 200\n\t    expected_cf2_filter_stats.max_size_bytes = 200\n\t    expected_cf2_filter_stats.max_size_time = cf2_create_1_time\n\t    expected_cf2_filter_stats.max_total_live_size_bytes = 200\n\t    expected_cf2_filter_stats.max_total_live_size_time = cf2_create_1_time\n\t    actual_stats = monitor.get_blocks_stats()\n\t    assert actual_stats[cf1][db_files.BlockType.INDEX] == \\\n", "           expected_cf1_index_stats\n\t    assert actual_stats[cf1][db_files.BlockType.FILTER] == \\\n\t           expected_cf1_filter_stats\n\t    assert actual_stats[cf2][db_files.BlockType.INDEX] == \\\n\t           expected_cf2_index_stats\n\t    assert actual_stats[cf2][db_files.BlockType.FILTER] == \\\n\t           expected_cf2_filter_stats\n\t    expected_index_stats = db_files.BlockLiveFileStats()\n\t    expected_index_stats.num_created = 5\n\t    expected_index_stats.num_live = 2\n", "    expected_index_stats.total_created_size_bytes = 820\n\t    expected_index_stats.curr_total_live_size_bytes = 570\n\t    expected_index_stats.max_size_bytes = 400\n\t    expected_index_stats.max_size_time = cf2_create_2_time\n\t    # These are incorrect for more than 1 cf\n\t    expected_index_stats.max_total_live_size_bytes = 0\n\t    expected_index_stats.max_total_live_size_time = None\n\t    actual_index_stats = \\\n\t        db_files.get_block_stats_for_cfs_group(\n\t            [cf1, cf2], monitor, db_files.BlockType.INDEX)\n", "    assert actual_index_stats == expected_index_stats\n\t    expected_filter_stats = db_files.BlockLiveFileStats()\n\t    expected_filter_stats.num_created = 5\n\t    expected_filter_stats.num_live = 2\n\t    expected_filter_stats.total_created_size_bytes = 1130\n\t    expected_filter_stats.curr_total_live_size_bytes = 230\n\t    expected_filter_stats.max_size_bytes = 500\n\t    expected_filter_stats.max_size_time = cf1_create_2_time\n\t    # These are incorrect for more than 1 cf\n\t    # expected_filter_stats.max_total_live_size_bytes = 530\n", "    # expected_filter_stats.max_total_live_size_time = cf1_create_3_time\n\t    actual_filter_stats = \\\n\t        db_files.get_block_stats_for_cfs_group(\n\t            [cf1, cf2], monitor, db_files.BlockType.FILTER)\n\t    assert actual_filter_stats == expected_filter_stats\n"]}
{"filename": "test/test_log_file.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport itertools\n\timport pytest\n\timport test.testing_utils as test_utils\n\timport utils\n\tfrom db_options import DatabaseOptions\n\tfrom log_file import LogFileMetadata, ParsedLog\n\tfrom test.sample_log_info import SampleLogInfo, SampleRolledLogInfo\n", "def test_empty_md():\n\t    md = LogFileMetadata([], 0)\n\t    assert md.get_product_name() is None\n\t    assert md.get_version() is None\n\t    assert md.get_git_hash() is None\n\t    assert md.get_db_session_id() is None\n\t    assert md.get_start_time() is None\n\t    assert md.get_end_time() is None\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        md.get_log_time_span_seconds()\n", "def test_md_parse_product_and_version():\n\t    lines =\\\n\t        [\"2022/02/17-12:38:38.054710 7f574ef65f80 SpeeDB version: 6.11.4\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 RocksDB version: 6.11.5\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 SpeeDB version:\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 XXXXXX version: 6.11.6\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 version:6.11.4\",\n\t         ]\n\t    expected_values = [\n\t        (\"SpeeDB\", \"6.11.4\"),\n", "        (\"RocksDB\", \"6.11.5\"),\n\t        (None, None),\n\t        (\"XXXXXX\", \"6.11.6\"),\n\t        (None, None),\n\t    ]\n\t    for i, line in enumerate(lines):\n\t        entry = test_utils.line_to_entry(line)\n\t        md = LogFileMetadata([entry], 0)\n\t        assert md.get_product_name() == expected_values[i][0]\n\t        assert md.get_version() == expected_values[i][1]\n", "    with pytest.raises(utils.ParsingError):\n\t        lines_with_duplicates = lines[0:2]\n\t        entries_with_duplicates =\\\n\t            test_utils.lines_to_entries(lines_with_duplicates)\n\t        md = LogFileMetadata(entries_with_duplicates, 0)\n\tdef test_md_parse_git_hash():\n\t    lines =\\\n\t        [\"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha 123456\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha    123456\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git SHA 123456\",\n", "         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha XXXXX:123456\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha \",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git 123456\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 sha 123456\",\n\t         ]\n\t    expected_values = [\n\t        \"123456\",\n\t        \"123456\",\n\t        None,\n\t        \"XXXXX:123456\",\n", "        None,\n\t        None,\n\t        None,\n\t    ]\n\t    for i, line in enumerate(lines):\n\t        entry = test_utils.line_to_entry(line)\n\t        md = LogFileMetadata([entry], 0)\n\t        assert md.get_git_hash() == expected_values[i]\n\t    # Test duplicates\n\t    with pytest.raises(utils.ParsingError):\n", "        lines_with_duplicates = lines[0:2]\n\t        entries_with_duplicates =\\\n\t            test_utils.lines_to_entries(lines_with_duplicates)\n\t        md = LogFileMetadata(entries_with_duplicates, 0)\n\tdef test_md_parse_db_session_id():\n\t    lines =\\\n\t        [\"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:  21GZXO5TD\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID: 21GZXO5TD\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:21GZXO5TD\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:\",\n", "         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB ID: 21GZXO5TD\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 Session ID: 21GZXO5TD\",\n\t         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:\",\n\t         ]\n\t    expected_values = [\n\t        \"21GZXO5TD\",\n\t        \"21GZXO5TD\",\n\t        \"21GZXO5TD\",\n\t        None,\n\t        None,\n", "        None,\n\t        None,\n\t    ]\n\t    for i, line in enumerate(lines):\n\t        entry = test_utils.line_to_entry(line)\n\t        md = LogFileMetadata([entry], 0)\n\t        assert md.get_db_session_id() == expected_values[i]\n\t    # Test duplicates\n\t    with pytest.raises(utils.ParsingError):\n\t        lines_with_duplicates = lines[0:2]\n", "        entries_with_duplicates =\\\n\t            test_utils.lines_to_entries(lines_with_duplicates)\n\t        md = LogFileMetadata(entries_with_duplicates, 0)\n\tdef test_parse_md_valid_combinations():\n\t    lines = [\n\t        \"2022/02/17-12:38:38.054710 f65f80 SpeeDB version: 6.11.4\",\n\t        \"2022/02/17-12:38:38.054710 f65f80 DB Session ID: 21GZXO5TD9VAIRA\",\n\t        \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha 123456\",\n\t    ]\n\t    # Test all combinations of line orderings. Parsing should not be affected\n", "    for i, lines_permutation in enumerate(list(itertools.permutations(lines))):\n\t        entries = test_utils.lines_to_entries(lines_permutation)\n\t        md = LogFileMetadata(entries, 0)\n\t        assert md.get_product_name() == \"SpeeDB\"\n\t        assert md.get_version() == \"6.11.4\"\n\t        assert md.get_git_hash() == \"123456\"\n\t        assert md.get_db_session_id() == \"21GZXO5TD9VAIRA\"\n\tdef test_parse_log_to_entries():\n\t    lines = '''2022/04/17-14:13:10.724683 7f4a9fdff700 Entry 1\n\t    2022/04/17-14:14:10.724683 7f4a9fdff700 Entry 2\n", "    Entry 2 Continuation 1\n\t    2022/04/17-14:14:20.724683 7f4a9fdff700 Entry 3\n\t    Entry 3 Continuation 1\n\t    '''.splitlines() # noqa\n\t    entries, job_id_to_cf_name_map =\\\n\t        ParsedLog.parse_log_to_entries(\"DummyPath\", lines[:1])\n\t    assert len(entries) == 1\n\t    assert job_id_to_cf_name_map == {}\n\t    assert entries[0].get_msg() == \"Entry 1\"\n\t    entries, job_id_to_cf_name_map = \\\n", "        ParsedLog.parse_log_to_entries(\"DummyPath\", lines[:2])\n\t    assert len(entries) == 2\n\t    assert entries[0].get_msg() == \"Entry 1\"\n\t    assert entries[1].get_msg() == \"Entry 2\"\n\t    entries, job_id_to_cf_name_map = \\\n\t        ParsedLog.parse_log_to_entries(\"DummyPath\", lines)\n\t    assert len(entries) == 3\n\t    assert entries[1].get_msg_lines() == [\"Entry 2\",\n\t                                          \"Entry 2 Continuation 1\",\n\t                                          \"\",\n", "                                          \"\"]\n\t    assert entries[1].get_msg() == \"Entry 2\\nEntry 2 Continuation 1\"\n\t    assert entries[2].get_msg_lines() == [\"Entry 3\",\n\t                                          \"\",\n\t                                          \"Entry 3 Continuation 1\",\n\t                                          \"\"]\n\t    assert entries[2].get_msg() == \"Entry 3\\n\\nEntry 3 Continuation 1\"\n\tdef test_parse_metadata():\n\t    lines = '''2022/11/24-15:58:04.758352 32819 RocksDB version: 7.2.2\n\t    2022/11/24-15:58:04.758397 32819 Git sha 35f6432643807267f210eda9e9388a80ea2ecf0e\n", "    2022/11/24-15:58:04.758398 32819 Compile date\n\t    2022/11/24-15:58:04.758402 32819 DB SUMMARY\n\t    2022/11/24-15:58:04.758403 32819 DB Session ID:  V90YQ8JY6T5E5H2ES6LK\n\t    2022/11/24-15:58:04.759056 32819 CURRENT file:  CURRENT\n\t    2022/11/24-15:58:04.759058 32819 IDENTITY file:  IDENTITY\n\t    2022/11/24-15:58:04.759060 32819 MANIFEST file:  MANIFEST-025591 size: 439474 Bytes\n\t    2022/11/24-15:58:04.759061 32819 SST files in /data/ dir, Total Num: 1498,\"\n\t    2022/11/24-15:58:04.759062 32819 Write Ahead Log file in /data/: 028673.log\n\t    '''.splitlines() # noqa\n\t    entries = test_utils.lines_to_entries(lines)\n", "    metadata = LogFileMetadata(entries, start_entry_idx=0)\n\t    assert metadata.get_product_name() == \"RocksDB\"\n\t    assert metadata.get_version() == \"7.2.2\"\n\t    assert metadata.get_git_hash() == \\\n\t           \"35f6432643807267f210eda9e9388a80ea2ecf0e\"\n\t    assert metadata.get_start_time() == \"2022/11/24-15:58:04.758352\"\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        metadata.get_log_time_span_seconds()\n\t    assert str(metadata) == \\\n\t           \"LogFileMetadata: Start:2022/11/24-15:58:04.758352, End:UNKNOWN\"\n", "    metadata.set_end_time(\"2022/11/24-16:08:04.758352\")\n\t    assert metadata.get_end_time() == \"2022/11/24-16:08:04.758352\"\n\t    assert metadata.get_log_time_span_seconds() == 600\n\t    assert str(metadata) == \\\n\t           \"LogFileMetadata: Start:2022/11/24-15:58:04.758352, \" \\\n\t           \"End:2022/11/24-16:08:04.758352\"\n\tdef test_parse_metadata1():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\t    metadata = parsed_log.get_metadata()\n\t    assert metadata.get_product_name() == SampleLogInfo.PRODUCT_NAME\n", "    assert metadata.get_version() == SampleLogInfo.VERSION\n\t    assert metadata.get_git_hash() == SampleLogInfo.GIT_HASH\n\t    assert metadata.get_start_time() == SampleLogInfo.START_TIME\n\t    assert metadata.get_end_time() == SampleLogInfo.END_TIME\n\t    expected_time_span = \\\n\t        (utils.parse_time_str(SampleLogInfo.END_TIME) -\n\t         utils.parse_time_str(SampleLogInfo.START_TIME)).seconds\n\t    assert metadata.get_log_time_span_seconds() == expected_time_span\n\tdef test_parse_options():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n", "    assert parsed_log.get_cfs_names(include_auto_generated=False) == \\\n\t           SampleLogInfo.CF_NAMES\n\t    actual_db_options = parsed_log.get_database_options()\n\t    assert actual_db_options.are_db_wide_options_set()\n\t    assert actual_db_options.get_cfs_names() == \\\n\t           parsed_log.get_cfs_names(include_auto_generated=False)\n\t    # Assuming LogFileOptionsParser is fully tested and may be used\n\t    expected_db_options = DatabaseOptions()\n\t    expected_db_options.set_db_wide_options(SampleLogInfo.DB_WIDE_OPTIONS_DICT)\n\t    for i in range(len(SampleLogInfo.CF_NAMES)):\n", "        expected_db_options.set_cf_options(\n\t            SampleLogInfo.CF_NAMES[i],\n\t            SampleLogInfo.OPTIONS_DICTS[i],\n\t            SampleLogInfo.TABLE_OPTIONS_DICTS[i])\n\t    actual_db_wide_options = actual_db_options.get_db_wide_options()\n\t    expected_db_wide_options = expected_db_options.get_db_wide_options()\n\t    assert expected_db_wide_options == actual_db_wide_options\n\t    for i in range(len(SampleLogInfo.CF_NAMES)):\n\t        cf_name = SampleLogInfo.CF_NAMES[i]\n\t        actual_options = actual_db_options.get_cf_options(cf_name)\n", "        expected_options = expected_db_options.get_cf_options(cf_name)\n\t        assert expected_options == actual_options\n\tdef test_parse_options_in_rolled_log():\n\t    parsed_log = test_utils.create_parsed_log(SampleRolledLogInfo.FILE_PATH)\n\t    assert parsed_log.get_cfs_names(include_auto_generated=False) == \\\n\t           SampleRolledLogInfo.CF_NAMES\n\t    assert parsed_log.get_auto_generated_cfs_names() == \\\n\t           SampleRolledLogInfo.AUTO_GENERATED_CF_NAMES\n\t    actual_db_options = parsed_log.get_database_options()\n\t    assert actual_db_options.are_db_wide_options_set()\n", "    expected_cfs_names_with_options = [\"default\"] + \\\n\t        SampleRolledLogInfo.AUTO_GENERATED_CF_NAMES\n\t    assert actual_db_options.get_cfs_names() == \\\n\t           expected_cfs_names_with_options\n\t    # Assuming LogFileOptionsParser is fully tested and may be used\n\t    expected_db_options = DatabaseOptions()\n\t    expected_db_options.set_db_wide_options(SampleLogInfo.DB_WIDE_OPTIONS_DICT)\n\t    for i in range(len(expected_cfs_names_with_options)):\n\t        expected_db_options.set_cf_options(\n\t            expected_cfs_names_with_options[i],\n", "            SampleLogInfo.OPTIONS_DICTS[i],\n\t            SampleLogInfo.TABLE_OPTIONS_DICTS[i])\n\t    actual_db_wide_options = actual_db_options.get_db_wide_options()\n\t    expected_db_wide_options = expected_db_options.get_db_wide_options()\n\t    assert expected_db_wide_options == actual_db_wide_options\n\t    for i in range(len(expected_cfs_names_with_options)):\n\t        cf_name = expected_cfs_names_with_options[i]\n\t        actual_options = actual_db_options.get_cf_options(cf_name)\n\t        expected_options = expected_db_options.get_cf_options(cf_name)\n\t        assert expected_options == actual_options\n", "def test_parse_warns():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\t    warns_mngr = parsed_log.get_warnings_mngr()\n\t    assert warns_mngr.get_total_num_warns() == 1\n\tdef test_parse_db_wide_stats():\n\t    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\t    mngr = parsed_log.get_stats_mngr()\n\t    db_wide_stats_mngr = mngr.get_db_wide_stats_mngr()\n\t    assert db_wide_stats_mngr.get_stalls_entries() == \\\n\t           SampleLogInfo.DB_WIDE_STALLS_ENTRIES\n", "def test_empty_log_file():\n\t    with pytest.raises(utils.EmptyLogFile):\n\t        ParsedLog(\"DummyPath\", [], should_init_baseline_info=False)\n\tdef test_unexpected_1st_log_line():\n\t    with pytest.raises(utils.InvalidLogFile):\n\t        ParsedLog(\"DummyPath\", [\"Dummy Line\", \"Another Dummy Line\"],\n\t                  should_init_baseline_info=False)\n"]}
{"filename": "test/test_stats_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\tfrom datetime import timedelta\n\timport utils\n\tfrom stats_mngr import DbWideStatsMngr, CompactionStatsMngr, BlobStatsMngr, \\\n\t    CfFileHistogramStatsMngr, BlockCacheStatsMngr, \\\n\t    StatsMngr, parse_uptime_line, CfNoFileStatsMngr\n\tfrom test.testing_utils import lines_to_entries\n\tNUM_LINES = 381\n", "STATS_DUMP_IDX = 0\n\tDB_STATS_IDX = 2\n\tCOMPACTION_STATS_DEFAULT_PER_LEVEL_IDX = 11\n\tCOMPACTION_STATS_DEFAULT_BY_PRIORITY_IDX = 22\n\tBLOB_STATS_IDX = 27\n\tCF_NO_FILE_HISTOGRAM_DEFAULT_IDX = 29\n\tBLOCK_CACHE_DEFAULT_IDX = 38\n\tCF_FILE_READ_LATENCY_IDX = 41\n\tSTATISTICS_COUNTERS_IDX = 138\n\tONE_PAST_STATS_IDX = 377\n", "EMPTY_LINE1 = ''\n\tEMPTY_LINE2 = '                      '\n\tget_value_by_size_with_unit = \\\n\t    utils.get_num_bytes_from_human_readable_str\n\t# TODO  - Test that errors during parsing of a stats entry are handled\n\t#  gracefully (parsing continues, partial/corrupt data is not saved, and only\n\t#  the faulty lines are skipped)\n\t'''\n\tTODO:\n\tGeneral:\n", "- Test parse_line_with_cf\n\tDB-Wide:\n\tAdd tests for stalls lines or remove the code\n\t'''\n\tdef read_sample_stats_file():\n\t    f = open(\"input_files/LOG_sample_stats.txt\")\n\t    lines = f.readlines()\n\t    assert len(lines) == NUM_LINES\n\t    return lines\n\tdef test_parse_uptime_line():\n", "    line1 = \"Uptime(secs): 4.8 total, 8.4 interval\"\n\t    assert (4.8, 8.4) == parse_uptime_line(line1)\n\t    line2 = \"Uptime(secs): 4.8 total, XXX 8.4 interval\"\n\t    assert parse_uptime_line(line2, allow_mismatch=True) is None\n\t#\n\t#   DB Wide Stats Mngr\n\t#\n\tdef test_db_wide_is_stats_start_line():\n\t    assert DbWideStatsMngr.is_start_line(\"** DB Stats **\")\n\t    assert DbWideStatsMngr.is_start_line(\"** DB Stats **      \")\n", "    assert not DbWideStatsMngr.is_start_line(\"** DB Stats **   DUMMY TEXT\")\n\t    assert not DbWideStatsMngr.is_start_line(\"** DB XXX Stats **\")\n\tdef test_db_wide_try_parse_as_stalls_line():\n\t    pass\n\tdef test_db_wide_stats_mngr():\n\t    time1 = \"2022/11/24-15:58:09.511260\"\n\t    db_wide_stats_lines1 = \\\n\t        '''Uptime(secs): 4.8 total, 4.8 interval\n\t        Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\n\t        Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n", "        Cumulative stall: 12:10:56.123 H:M:S, 98.7 percent\n\t        Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\n\t        Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n\t        Interval stall: 45:34:12.789 H:M:S, 12.3 percent\n\t        '''.splitlines()  # noqa\n\t    time2 = \"2022/11/24-15:59:09.511260\"\n\t    db_wide_stats_lines2 = \\\n\t        '''Uptime(secs): 4.8 total, 4.8 interval\n\t        Cumulative writes: 10M writes, 2M keys, 0 commit groups, 0.0 writes per commit group, ingest: 1.23 GB, 5.67 MB/s\n\t        '''.splitlines()  # noqa\n", "    mngr = DbWideStatsMngr()\n\t    assert mngr.get_stalls_entries() == {}\n\t    mngr.add_lines(time1, db_wide_stats_lines1)\n\t    expected_cumulative_duration = \\\n\t        timedelta(hours=12, minutes=10, seconds=56, milliseconds=123)\n\t    expected_interval_duration = \\\n\t        timedelta(hours=45, minutes=34, seconds=12, milliseconds=789)\n\t    expected_stalls_entries = \\\n\t        {time1: {\"cumulative_duration\": expected_cumulative_duration,\n\t                 \"cumulative_percent\": 98.7,\n", "                 \"interval_duration\": expected_interval_duration,\n\t                 \"interval_percent\": 12.3}}\n\t    expected_cumulative_writes_entries = {\n\t        time1: DbWideStatsMngr.CumulativeWritesInfo()\n\t    }\n\t    actual_stalls_entries = mngr.get_stalls_entries()\n\t    assert actual_stalls_entries == expected_stalls_entries\n\t    actual_cumulative_writes_entries = mngr.get_cumulative_writes_entries()\n\t    assert actual_cumulative_writes_entries == \\\n\t           expected_cumulative_writes_entries\n", "    mngr.add_lines(time2, db_wide_stats_lines2)\n\t    actual_stalls_entries = mngr.get_stalls_entries()\n\t    assert actual_stalls_entries == expected_stalls_entries\n\t    expected_cumulative_writes_entries.update({\n\t        time2: DbWideStatsMngr.CumulativeWritesInfo(\n\t            num_writes=utils.get_number_from_human_readable_str(\"10 M\"),\n\t            num_keys=utils.get_number_from_human_readable_str(\"2 M\"),\n\t            ingest=utils.get_num_bytes_from_human_readable_str(\"1.23 GB\"),\n\t            ingest_rate_mbps=5.67)\n\t    })\n", "    actual_cumulative_writes_entries = mngr.get_cumulative_writes_entries()\n\t    assert actual_cumulative_writes_entries == \\\n\t           expected_cumulative_writes_entries\n\tdef test_is_compaction_stats_start_line():\n\t    line1 = \"** Compaction Stats [default] **\"\n\t    assert CompactionStatsMngr.is_start_line(line1)\n\t    assert CompactionStatsMngr.parse_start_line(line1) == \"default\"\n\t    line2 = \"** Compaction Stats [col-family] **       \"\n\t    assert CompactionStatsMngr.is_start_line(line2)\n\t    assert CompactionStatsMngr.parse_start_line(line2) == \"col-family\"\n", "    line3 = \"       ** Compaction Stats [col-family] **\"\n\t    assert CompactionStatsMngr.is_start_line(line3)\n\t    assert CompactionStatsMngr.parse_start_line(line3) == \"col-family\"\n\t    line4 = \"** Compaction Stats    [col-family]     **     \"\n\t    assert CompactionStatsMngr.is_start_line(line4)\n\t    assert CompactionStatsMngr.parse_start_line(line4) == \"col-family\"\n\t    line5 = \"** Compaction XXX Stats  [col-family] **\"\n\t    assert not CompactionStatsMngr.is_start_line(line5)\n\tdef test_is_blob_stats_start_line():\n\t    line1 = \\\n", "        'Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB,' \\\n\t        ' space amp: 0.0'\n\t    assert BlobStatsMngr.is_start_line(line1)\n\t    assert (0, 0.0, 0.0, 0.0) == BlobStatsMngr.parse_blob_stats_line(line1)\n\t    line2 = \\\n\t        'Blob file count: 100, total size: 1.5 GB, garbage size: 3.5 GB,' \\\n\t        ' space amp: 0.2'\n\t    assert BlobStatsMngr.is_start_line(line2)\n\t    assert (100, 1.5, 3.5, 0.2) == BlobStatsMngr.parse_blob_stats_line(line2)\n\tdef test_is_cf_file_histogram_stats_start_line():\n", "    cf1 = \"default\"\n\t    cf2 = \"col_family\"\n\t    line1 = f\"** File Read Latency Histogram By Level [{cf1}] **\"\n\t    assert CfFileHistogramStatsMngr.is_start_line(line1)\n\t    assert CfFileHistogramStatsMngr.parse_start_line(line1) == cf1\n\t    line2 = f\"** File Read Latency Histogram By Level [{cf2}] **       \"\n\t    assert CfFileHistogramStatsMngr.is_start_line(line2)\n\t    assert CfFileHistogramStatsMngr.parse_start_line(line2) == cf2\n\t    line3 = f\"       ** File Read Latency Histogram By Level [{cf2}] **\"\n\t    assert CfFileHistogramStatsMngr.is_start_line(line3)\n", "    assert CfFileHistogramStatsMngr.parse_start_line(line3) == cf2\n\t    line4 = \\\n\t        f\"** File Read Latency Histogram By Level    [{cf2}]     **     \"\n\t    assert CfFileHistogramStatsMngr.is_start_line(line4)\n\t    assert CfFileHistogramStatsMngr.parse_start_line(line4) == cf2\n\t    line5 = \\\n\t        f\"** File Read Latency Histogram XXX By Level Stats  [{cf2}] **\"\n\t    assert not CfFileHistogramStatsMngr.is_start_line(line5)\n\tdef test_is_block_cache_stats_start_line():\n\t    line1 = 'Block cache LRUCache@0x5600bb634770#32819 capacity: 8.00 MB ' \\\n", "            'collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0'\n\t    line2 = \\\n\t        'Block cache entry stats(count,size,portion): ' \\\n\t        'Misc(3,8.12 KB, 0.0991821%)'\n\t    assert BlockCacheStatsMngr.is_start_line(line1)\n\t    assert not BlockCacheStatsMngr.is_start_line(line2)\n\tdef test_find_next_start_line_in_db_stats():\n\t    lines = read_sample_stats_file()\n\t    assert DbWideStatsMngr.is_start_line(lines[DB_STATS_IDX])\n\t    expected_next_line_idxs = [COMPACTION_STATS_DEFAULT_PER_LEVEL_IDX,\n", "                               COMPACTION_STATS_DEFAULT_BY_PRIORITY_IDX,\n\t                               BLOB_STATS_IDX,\n\t                               CF_NO_FILE_HISTOGRAM_DEFAULT_IDX,\n\t                               BLOCK_CACHE_DEFAULT_IDX,\n\t                               CF_FILE_READ_LATENCY_IDX]\n\t    expected_next_types = [StatsMngr.StatsType.COMPACTION,\n\t                           StatsMngr.StatsType.COMPACTION,\n\t                           StatsMngr.StatsType.BLOB,\n\t                           StatsMngr.StatsType.CF_NO_FILE,\n\t                           StatsMngr.StatsType.BLOCK_CACHE,\n", "                           StatsMngr.StatsType.CF_FILE_HISTOGRAM]\n\t    expected_next_cf_names = [\"default\", \"default\", None, None,\n\t                              None, \"CF1\"]\n\t    line_idx = DB_STATS_IDX\n\t    stats_type = StatsMngr.StatsType.DB_WIDE\n\t    for i, expected_next_line_idx in enumerate(expected_next_line_idxs):\n\t        next_line_idx, next_stats_type, next_cf_name = \\\n\t            StatsMngr.find_next_start_line_in_db_stats(lines,\n\t                                                       line_idx,\n\t                                                       stats_type)\n", "        assert (next_line_idx > line_idx) or (next_stats_type is None)\n\t        assert next_line_idx == expected_next_line_idx\n\t        assert next_stats_type == expected_next_types[i]\n\t        assert next_cf_name == expected_next_cf_names[i]\n\t        line_idx = next_line_idx\n\t        stats_type = next_stats_type\n\tdef test_blob_stats_mngr():\n\t    blob_line = \\\n\t        'Blob file count: 10, total size: 1.5 GB, garbage size: 2.0 GB, ' \\\n\t        'space amp: 4.0'\n", "    blob_lines = [blob_line, EMPTY_LINE2]\n\t    time = '2022/11/24-15:58:09.511260 32851'\n\t    cf = \"cf1\"\n\t    mngr = BlobStatsMngr()\n\t    mngr.add_lines(time, cf, blob_lines)\n\t    expected_blob_entries = \\\n\t        {time: {\"File Count\": 10,\n\t                \"Total Size\": float(1.5 * 2 ** 30),\n\t                \"Garbage Size\": float(2 * 2 ** 30),\n\t                \"Space Amp\": 4.0}}\n", "    assert mngr.get_cf_stats(cf) == expected_blob_entries\n\tdef test_block_cache_stats_mngr_no_cf():\n\t    lines = \\\n\t        '''Block cache LRUCache@0x5600bb634770#32819 capacity: 8.00 GB collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0\n\t        Block cache entry stats(count,size,portion): DataBlock(1548,6.97 MB,0.136142%) IndexBlock(843,3.91 GB,78.2314%) Misc(6,16.37 KB,1.86265e-08%)\n\t        '''.splitlines()  # noqa\n\t    time = '2022/11/24-15:58:09.511260'\n\t    cf_name = \"cf1\"\n\t    mngr = BlockCacheStatsMngr()\n\t    cache_id = mngr.add_lines(time, cf_name, lines)\n", "    assert cache_id == \"LRUCache@0x5600bb634770#32819\"\n\t    entries = mngr.get_cache_entries(cache_id)\n\t    assert entries['Capacity'] == get_value_by_size_with_unit('8.00 GB')\n\t    expected_data_block_stats = \\\n\t        {'Count': 1548,\n\t         'Size': get_value_by_size_with_unit('6.97 MB'),\n\t         'Portion': '0.14%'\n\t         }\n\t    expected_index_block_stats = \\\n\t        {'Count': 843,\n", "         'Size': get_value_by_size_with_unit('3.91 GB'),\n\t         'Portion': '78.23%'\n\t         }\n\t    assert time in entries\n\t    time_entries = entries[time]\n\t    assert 'DataBlock' in time_entries\n\t    assert time_entries['DataBlock'] == expected_data_block_stats\n\t    assert 'IndexBlock' in time_entries\n\t    assert time_entries['IndexBlock'] == expected_index_block_stats\n\t    total_expected_usage = \\\n", "        get_value_by_size_with_unit('6.97 MB') + \\\n\t        get_value_by_size_with_unit('3.91 GB') + \\\n\t        get_value_by_size_with_unit('16.37 KB')\n\t    assert time_entries[\"Usage\"] == total_expected_usage\n\t    assert entries[\"Usage\"] == total_expected_usage\n\t    assert mngr.get_cf_cache_entries(cache_id, cf_name) == {}\n\t    assert mngr.get_last_usage(cache_id) == total_expected_usage\n\tdef test_block_cache_stats_mngr_with_cf():\n\t    lines = \\\n\t        '''Block cache LRUCache@0x5600bb634770#32819 capacity: 8.00 GB collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0\n", "        Block cache entry stats(count,size,portion): DataBlock(1548,6.97 MB,0.136142%) IndexBlock(843,3.91 GB,78.2314%) Misc(6,16.37 KB,1.86265e-08%)\n\t        Block cache [CF1]  DataBlock(4.50 KB) FilterBlock(0.00 KB) IndexBlock(0.91 GB)\n\t        '''.splitlines()  # noqa\n\t    time = '2022/11/24-15:58:09.511260'\n\t    cf_name = \"CF1\"\n\t    mngr = BlockCacheStatsMngr()\n\t    cache_id = mngr.add_lines(time, cf_name, lines)\n\t    assert cache_id == \"LRUCache@0x5600bb634770#32819\"\n\t    cf_entries = mngr.get_cf_cache_entries(cache_id, cf_name)\n\t    expected_cf_entries = \\\n", "        {time: {'DataBlock': get_value_by_size_with_unit('4.50 KB'),\n\t                'IndexBlock': get_value_by_size_with_unit('0.91 GB')}\n\t         }\n\t    assert cf_entries == expected_cf_entries\n\t    entries = mngr.get_cache_entries(cache_id)\n\t    total_expected_usage = \\\n\t        get_value_by_size_with_unit('6.97 MB') + \\\n\t        get_value_by_size_with_unit('3.91 GB') + \\\n\t        get_value_by_size_with_unit('16.37 KB')\n\t    assert entries[time][\"Usage\"] == total_expected_usage\n", "    assert entries[\"Usage\"] == total_expected_usage\n\tdef test_stats_mngr():\n\t    lines = read_sample_stats_file()\n\t    entries = lines_to_entries(lines)\n\t    mngr = StatsMngr()\n\t    expected_entry_idx = 1\n\t    expected_cfs_names_found = set()\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=1) == \\\n\t           (False, expected_entry_idx, expected_cfs_names_found)\n\t    expected_entry_idx = 2\n", "    expected_cfs_names_found = {\"default\", \"CF1\"}\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=0) == \\\n\t           (True, expected_entry_idx, expected_cfs_names_found)\n\tdef test_stats_mngr_non_contig_entries_1():\n\t    lines = \\\n\t'''2023/07/18-19:27:01.889729 27127 [/db_impl/db_impl.cc:1084] ------- DUMPING STATS -------\n\t2023/07/18-19:27:01.889745 26641 [/column_family.cc:1044] [default] Increasing compaction threads because of estimated pending compaction bytes 18555651178\n\t2023/07/18-19:27:01.890259 27127 [/db_impl/db_impl.cc:1086] \n\t** DB Stats **\n\tUptime(secs): 0.7 total, 0.7 interval\n", "Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\n\tCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n\tCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\n\tInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\n\tInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n\tInterval stall: 00:00:0.000 H:M:S, 0.0 percent\n\tWrite Stall (count): write-buffer-manager-limit-stops: 0,\n\t ** Compaction Stats [default] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n\t------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "  L0      2/0   322.40 MB   1.3      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n\t  L1      6/1   350.91 MB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t  L2     59/11   3.16 GB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t  L3    487/28  27.78 GB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t  L4    166/0   10.17 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.2   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t Sum    720/40  41.77 GB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n\t'''.splitlines() # noqa\n\t    entries = lines_to_entries(lines)\n\t    mngr = StatsMngr()\n", "    expected_entry_idx = 1\n\t    expected_cfs_names_found = set()\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=0) == \\\n\t           (True, expected_entry_idx, expected_cfs_names_found)\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=1) == \\\n\t           (False, expected_entry_idx, expected_cfs_names_found)\n\t    expected_entry_idx = 3\n\t    expected_cfs_names_found = {\"default\"}\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=2) == \\\n\t           (True, expected_entry_idx, expected_cfs_names_found)\n", "def test_stats_mngr_non_contig_entries_2():\n\t    lines = \\\n\t'''2023/07/18-19:27:01.889729 27127 [/db_impl/db_impl.cc:1084] ------- DUMPING STATS -------\n\t2023/07/18-19:27:01.889745 26641 [/column_family.cc:1044] [default] Increasing compaction threads because of estimated pending compaction bytes 18555651178\n\t2023/07/18-19:27:01.889806 26641 (Original Log Time 2023/07/18-19:27:01.887253) [/db_impl/db_impl_compaction_flush.cc:3428] [default] Moving #13947 to level-4 67519682 bytes\n\t2023/07/18-19:27:01.889746 27127 [/db_impl/db_impl.cc:1084] ------- DUMPING STATS -------\n\t2023/07/18-19:27:01.890259 27127 [/db_impl/db_impl.cc:1086] \n\t** DB Stats **\n\tUptime(secs): 0.7 total, 0.7 interval\n\tCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\n", "Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n\tCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\n\tInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\n\tInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n\tInterval stall: 00:00:0.000 H:M:S, 0.0 percent\n\tWrite Stall (count): write-buffer-manager-limit-stops: 0,\n\t ** Compaction Stats [default] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n\t------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t  L0      2/0   322.40 MB   1.3      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n", "  L1      6/1   350.91 MB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t  L2     59/11   3.16 GB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t  L3    487/28  27.78 GB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t  L4    166/0   10.17 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.2   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t Sum    720/40  41.77 GB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n\t'''.splitlines() # noqa\n\t    entries = lines_to_entries(lines)\n\t    mngr = StatsMngr()\n\t    expected_entry_idx = 1\n", "    expected_cfs_names_found = set()\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=0) == \\\n\t           (True, expected_entry_idx, expected_cfs_names_found)\n\t    expected_entry_idx = 1\n\t    expected_cfs_names_found = set()\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=1) == \\\n\t           (False, expected_entry_idx, expected_cfs_names_found)\n\t    expected_entry_idx = 2\n\t    expected_cfs_names_found = set()\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=2) == \\\n", "           (False, expected_entry_idx, expected_cfs_names_found)\n\t    expected_entry_idx = 5\n\t    expected_cfs_names_found = {\"default\"}\n\t    assert mngr.try_adding_entries(entries, start_entry_idx=3) == \\\n\t           (True, expected_entry_idx, expected_cfs_names_found)\n\tdef test_compaction_stats_mngr():\n\t    lines_level = \\\n\t        '''** Compaction Stats [default] **\n\t        Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n\t        ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "          L0      1/0   149.99 MB   0.6      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n\t          L1      5/0   271.79 MB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t          L2     50/1    2.73 GB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t          L3    421/6   24.96 GB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t          L4   1022/0   54.33 GB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\t         Sum   1499/7   82.43 GB   0.0      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n\t         Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n\t        '''.splitlines()  # noqa\n\t    lines_priority = \\\n\t        '''** Compaction Stats [CF1] **\n", "        Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n\t        ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t        User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.0   0.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n\t        '''.splitlines()  # noqa\n\t    lines_level = [line.strip() for line in lines_level]\n\t    lines_priority = [line.strip() for line in lines_priority]\n\t    time = \"2022/11/24-15:58:09.511260\"\n\t    mngr = CompactionStatsMngr()\n\t    assert mngr.get_cf_size_bytes_at_end(\"default\") is None\n\t    mngr.add_lines(time, \"default\", lines_level, 0)\n", "    mngr.add_lines(time, \"CF1\", lines_priority, 0)\n\t    assert mngr.get_cf_size_bytes_at_end(\"default\") == \\\n\t           utils.get_num_bytes_from_human_readable_components(\"82.43\", \"GB\")\n\t    assert mngr.get_cf_size_bytes_at_end(\"CF1\") is None\n\tdef test_cf_no_file_stats_mngr():\n\t    time = \"2022/11/24-15:58:09.511260\"\n\t    cf_name = \"cf1\"\n\t    lines = '''\n\t    Uptime(secs): 22939219.8 total, 0.0 interval\n\t    Flush(GB): cumulative 158.813, interval 0.000\n", "    AddFile(GB): cumulative 0.000, interval 0.000\n\t    AddFile(Total Files): cumulative 0, interval 0\n\t    AddFile(L0 Files): cumulative 0, interval 0\n\t    AddFile(Keys): cumulative 0, interval 0\n\t    Cumulative compaction: 364.52 GB write, 0.02 MB/s write, 363.16 GB read, 0.02 MB/s read, 1942.7 seconds\n\t    Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\t    Stalls(count): 0 level0_slowdown, 1 level0_slowdown_with_compaction, 2 level0_numfiles, 3 level0_numfiles_with_compaction, 4 stop for pending_compaction_bytes, 5 slowdown for pending_compaction_bytes, 6 memtable_compaction, 7 memtable_slowdown, interval 100 total count\n\t    '''.splitlines()  # noqa\n\t    expected_stall_counts = \\\n\t        {cf_name: {time: {'level0_slowdown': 0,\n", "                          'level0_slowdown_with_compaction': 1,\n\t                          'level0_numfiles': 2,\n\t                          'level0_numfiles_with_compaction': 3,\n\t                          'stop for pending_compaction_bytes': 4,\n\t                          'slowdown for pending_compaction_bytes': 5,\n\t                          'memtable_compaction': 6,\n\t                          'memtable_slowdown': 7,\n\t                          'interval_total_count': 100}}}\n\t    mngr = CfNoFileStatsMngr()\n\t    mngr.add_lines(time, cf_name, lines)\n", "    stall_counts = mngr.get_stall_counts()\n\t    assert stall_counts == expected_stall_counts\n\tdef test_compaction_stats_get_field_value():\n\t    stats = \\\n\t        {'2023/04/09-12:37:27.398344':\n\t         {'LEVEL-0': {'Moved(GB)': '1.2',\n\t                      'W-Amp': '1.0',\n\t                      'Comp(sec)': '74.4',\n\t                      'CompMergeCPU(sec)': '10.1',\n\t                      'Comp(cnt)': '15.03'},\n", "          'LEVEL-1': {'Moved(GB)': '1.2', 'W-Amp': '2.0',\n\t                      'Comp(sec)': '84.4',\n\t                      'CompMergeCPU(sec)': '20.2',\n\t                      'Comp(cnt)': '15.03'},\n\t          'SUM': {'Moved(GB)': '1.2',\n\t                  'W-Amp': '3.0',\n\t                  'Comp(sec)': '158.8',\n\t                  'CompMergeCPU(sec)': '30.3',\n\t                  'Comp(cnt)': '15.03'},\n\t          'INTERVAL': {'Moved(GB)': '1.2',\n", "                       'W-Amp': '1.5',\n\t                       'Comp(sec)': '74.4',\n\t                       'CompMergeCPU(sec)': '15.97',\n\t                       'Comp(cnt)': '15.03'}}}\n\t    field = CompactionStatsMngr.LevelFields.WRITE_AMP\n\t    assert CompactionStatsMngr.get_level_field_value(stats, 0, field) == '1.0'\n\t    assert CompactionStatsMngr.get_level_field_value(stats, 1, field) == '2.0'\n\t    assert CompactionStatsMngr.get_sum_value(stats, field) == '3.0'\n\t    assert CompactionStatsMngr.get_field_value_for_all_levels(stats, field) ==\\\n\t           {0: '1.0', 1: '2.0'}\n", "    field = CompactionStatsMngr.LevelFields.COMP_SEC\n\t    assert CompactionStatsMngr.get_level_field_value(stats, 0, field) == '74.4'\n\t    assert CompactionStatsMngr.get_level_field_value(stats, 1, field) == '84.4'\n\t    assert CompactionStatsMngr.get_sum_value(stats, field) == '158.8'\n\t    assert CompactionStatsMngr.get_field_value_for_all_levels(stats, field) ==\\\n\t           {0: '74.4', 1: '84.4'}\n\t    field = CompactionStatsMngr.LevelFields.COMP_MERGE_CPU\n\t    assert CompactionStatsMngr.get_level_field_value(stats, 0, field) == '10.1'\n\t    assert CompactionStatsMngr.get_level_field_value(stats, 1, field) == '20.2'\n\t    assert CompactionStatsMngr.get_sum_value(stats, field) == '30.3'\n", "    assert CompactionStatsMngr.get_field_value_for_all_levels(stats, field) ==\\\n\t           {0: '10.1', 1: '20.2'}\n\tdef test_compactions_stats_mngr():\n\t    stats_lines = \\\n\t        f'''** Compaction Stats [default] **\n\t    Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n\t    ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t      L0     16/11   3.90 GB   5.9    131.5     0.0    131.5     231.0     99.5       0.0   2.3    224.8    394.9    599.04            488.24      2319    0.258    136M   322K       0.0       0.0\n\t      L1     30/30   1.81 GB   0.0    156.4    95.8     60.6     153.8     93.2       0.0   1.6    354.8    349.0    451.24            372.78        52    8.678    162M  2648K       0.0       0.0\n\t      L2     43/0    2.47 GB   1.0    162.1    77.6     84.5     159.7     75.2      14.0   2.1    319.5    314.8    519.66            404.59      1075    0.483    168M  2502K       0.0       0.0\n", "      L3    436/0   24.96 GB   1.0    473.6    89.2    384.4     445.7     61.3       0.0   5.0    327.9    308.5   1479.17           1127.43      1470    1.006    492M    29M       0.0       0.0\n\t      L4   1369/0   66.58 GB   0.3    169.1    61.3    107.9     129.6     21.8       0.0   2.1    362.4    277.7    477.96            344.39       996    0.480    175M    41M       0.0       0.0\n\t     Sum   1894/41  99.72 GB   0.0   1092.7   323.9    768.8    1119.9    351.0      14.0  11.2    317.3    325.1   3527.07           2737.42      5912    0.597   1136M    75M       0.0       0.0\n\t     Int      0/0    0.00 KB   0.0   1092.7   323.9    768.8    1119.9    351.0      14.0  11.2    317.3    325.1   3527.07           2737.42      5912    0.597   1136M    75M       0.0       0.0'''.splitlines() # noqa\n\t    stats_lines = [line.strip() for line in stats_lines]\n\t    expected_level0_dict = \\\n\t        {'Num-Files': '16',\n\t         'Files-In-Comp': '11',\n\t         'size_bytes': 4187593113,\n\t         'Score': '5.9',\n", "         'Read(GB)': '131.5',\n\t         'Rn(GB)': '0.0',\n\t         'Rnp1(GB)': '131.5',\n\t         'Write(GB)': '231.0',\n\t         'Wnew(GB)': '99.5',\n\t         'Moved(GB)': '0.0',\n\t         'W-Amp': '2.3',\n\t         'Rd(MB/s)': '224.8',\n\t         'Wr(MB/s)': '394.9',\n\t         'Comp(sec)': '599.04',\n", "         'CompMergeCPU(sec)': '488.24',\n\t         'Comp(cnt)': '2319',\n\t         'Avg(sec)': '0.258',\n\t         'KeyIn': '136M',\n\t         'KeyDrop': '322K',\n\t         'Rblob(GB)': '0.0',\n\t         'Wblob(GB)': '0.0'}\n\t    expected_sum_dict = \\\n\t        {'Num-Files': '1894',\n\t         'Files-In-Comp': '41',\n", "         'size_bytes': 107073534689,\n\t         'Score': '0.0',\n\t         'Read(GB)': '1092.7',\n\t         'Rn(GB)': '323.9',\n\t         'Rnp1(GB)': '768.8',\n\t         'Write(GB)': '1119.9',\n\t         'Wnew(GB)': '351.0',\n\t         'Moved(GB)': '14.0',\n\t         'W-Amp': '11.2',\n\t         'Rd(MB/s)': '317.3',\n", "         'Wr(MB/s)': '325.1',\n\t         'Comp(sec)': '3527.07',\n\t         'CompMergeCPU(sec)': '2737.42',\n\t         'Comp(cnt)': '5912',\n\t         'Avg(sec)': '0.597',\n\t         'KeyIn': '1136M',\n\t         'KeyDrop': '75M',\n\t         'Rblob(GB)': '0.0',\n\t         'Wblob(GB)': '0.0'}\n\t    time = \"2023/01/04-09:04:59.378877 27442\"\n", "    cf_name = \"default\"\n\t    level0_key = 'LEVEL-0'\n\t    sum_key = \"SUM\"\n\t    mngr = CompactionStatsMngr()\n\t    mngr.add_lines(time, cf_name, stats_lines, 0)\n\t    entries = mngr.get_cf_level_entries(cf_name)\n\t    assert isinstance(entries, list)\n\t    assert len(entries) == 1\n\t    assert isinstance(entries[0], dict)\n\t    assert list(entries[0].keys()) == [time]\n", "    values = entries[0][time]\n\t    assert isinstance(values, dict)\n\t    assert len(values.keys()) == 7\n\t    assert level0_key in values\n\t    level0_values_dict = values[level0_key]\n\t    assert level0_values_dict == expected_level0_dict\n\t    assert sum_key in values\n\t    sum_values_dict = values[sum_key]\n\t    assert sum_values_dict == expected_sum_dict\n\t    field = CompactionStatsMngr.LevelFields.WRITE_AMP\n", "    assert CompactionStatsMngr.get_level_field_value(entries[0], 1, field) ==\\\n\t           '1.6'\n\t    assert CompactionStatsMngr.get_sum_value(entries[0], field) == '11.2'\n\t    assert CompactionStatsMngr.get_field_value_for_all_levels(\n\t        entries[0], field) == {0: '2.3',\n\t                               1: '1.6',\n\t                               2: '2.1',\n\t                               3: '5.0',\n\t                               4: '2.1'}\n\t    field = CompactionStatsMngr.LevelFields.COMP_SEC\n", "    assert CompactionStatsMngr.get_level_field_value(entries[0], 1, field) == \\\n\t           '451.24'\n\t    assert CompactionStatsMngr.get_sum_value(entries[0], field) == '3527.07'\n\t    field = CompactionStatsMngr.LevelFields.COMP_MERGE_CPU\n\t    assert CompactionStatsMngr.get_level_field_value(entries[0], 1, field) == \\\n\t           '372.78'\n\t    assert CompactionStatsMngr.get_sum_value(entries[0], field) == '2737.42'\n\t#\n\t# FILE READ LATENCY TESTS INFO\n\t#\n", "time1 = \"2023/01/04-09:04:59.378877 27442\"\n\ttime2 = \"2023/01/04-09:14:59.378877 27442\"\n\tl0 = 0\n\tl4 = 4\n\tcf1 = \"cf1\"\n\tcf2 = \"cf2\"\n\tstats_lines_cf1_t1_l0 = \\\n\tf'''** File Read Latency Histogram By Level [{cf1}] **\n\t** Level {l0} read latency histogram (micros):\n\tCount: 26687513 Average: 3.1169  StdDev: 34.39\n", "Min: 0  Median: 2.4427  Max: 56365\n\tPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n\tstats_lines_cf1_t1_l0 = stats_lines_cf1_t1_l0.splitlines()\n\tstats_lines_cf1_t1_l0 = [line.strip() for line in stats_lines_cf1_t1_l0]\n\tstats_lines_cf1_t1_l4 = \\\n\tf'''** File Read Latency Histogram By Level [{cf1}] **\n\t** Level {l4} read latency histogram (micros):\n\tCount: 100 Average: 1.1  StdDev: 2.2\n\tMin: 1000  Median: 3.3  Max: 2000\n\tPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n", "stats_lines_cf1_t1_l4 = stats_lines_cf1_t1_l4.splitlines()\n\tstats_lines_cf1_t1_l4 = [line.strip() for line in stats_lines_cf1_t1_l4]\n\tstats_lines_cf1_t2_l4 = \\\n\tf'''** File Read Latency Histogram By Level [{cf1}] **\n\t** Level {l4} read latency histogram (micros):\n\tCount: 500 Average: 10.10  StdDev: 20.20\n\tMin: 10000  Median: 30.30  Max: 20000\n\tPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n\tstats_lines_cf1_t2_l4 = stats_lines_cf1_t2_l4.splitlines()\n\tstats_lines_cf1_t2_l4 = [line.strip() for line in stats_lines_cf1_t2_l4]\n", "stats_lines_cf2_t1_l0 = \\\n\tf'''** File Read Latency Histogram By Level [{cf2}] **\n\t** Level {l0} read latency histogram (micros):\n\tCount: 200 Average: 6.6  StdDev: 7.7\n\tMin: 5000  Median: 8.8  Max: 6000\n\tPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n\tstats_lines_cf2_t1_l0 = stats_lines_cf2_t1_l0.splitlines()\n\tstats_lines_cf2_t1_l0 = [line.strip() for line in stats_lines_cf2_t1_l0]\n\texpected_cf1_t1_l0_stats = \\\n\t    CfFileHistogramStatsMngr.CfLevelStats(\n", "        count=26687513,\n\t        average=3.1169,\n\t        std_dev=34.39,\n\t        min=0,\n\t        median=2.4427,\n\t        max=56365)\n\texpected_cf1_t1_l4_stats = \\\n\t    CfFileHistogramStatsMngr.CfLevelStats(\n\t        count=100,\n\t        average=1.1,\n", "        std_dev=2.2,\n\t        min=1000,\n\t        median=3.3,\n\t        max=2000)\n\texpected_cf1_t2_l4_stats = \\\n\t    CfFileHistogramStatsMngr.CfLevelStats(\n\t        count=500,\n\t        average=10.10,\n\t        std_dev=20.20,\n\t        min=10000,\n", "        median=30.30,\n\t        max=20000)\n\texpected_cf2_t1_l0_stats = \\\n\t    CfFileHistogramStatsMngr.CfLevelStats(\n\t        count=200,\n\t        average=6.6,\n\t        std_dev=7.7,\n\t        min=5000,\n\t        median=8.8,\n\t        max=6000)\n", "def test_cf_file_histogram_mngr1():\n\t    mngr = CfFileHistogramStatsMngr()\n\t    expected_cf1_entries = dict()\n\t    mngr.add_lines(time1, cf1, stats_lines_cf1_t1_l0)\n\t    expected_cf1_entries[time1] = {l0:  expected_cf1_t1_l0_stats}\n\t    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n\t    assert mngr.get_last_cf_entry(cf1) == expected_cf1_entries\n\t    assert mngr.get_cf_entries(cf2) is None\n\t    mngr.add_lines(time1, cf1, stats_lines_cf1_t1_l4)\n\t    expected_cf1_entries[time1][l4] = expected_cf1_t1_l4_stats\n", "    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n\t    assert mngr.get_last_cf_entry(cf1) == expected_cf1_entries\n\t    assert mngr.get_cf_entries(cf2) is None\n\t    expected_cf2_entries = dict()\n\t    expected_cf2_entries[time1] = {}\n\t    expected_cf2_entries[time1][l0] = expected_cf2_t1_l0_stats\n\t    mngr.add_lines(time1, cf2, stats_lines_cf2_t1_l0)\n\t    assert mngr.get_cf_entries(cf2) == expected_cf2_entries\n\t    assert mngr.get_last_cf_entry(cf2) == expected_cf2_entries\n\t    expected_cf1_entries[time2] = {}\n", "    expected_cf1_entries[time2][l4] = expected_cf1_t2_l4_stats\n\t    mngr.add_lines(time2, cf1, stats_lines_cf1_t2_l4)\n\t    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n\t    assert mngr.get_last_cf_entry(cf1) == \\\n\t           {time2: {l4: expected_cf1_t2_l4_stats}}\n\tdef test_cf_file_histogram_mngr2():\n\t    empty_line = \"                      \"\n\t    all_cf1_stats_lines = list()\n\t    all_cf1_stats_lines.extend(stats_lines_cf1_t1_l0)\n\t    all_cf1_stats_lines.extend([empty_line] * 2)\n", "    all_cf1_stats_lines.extend(stats_lines_cf1_t1_l4[1:])\n\t    all_cf1_stats_lines.extend([empty_line])\n\t    mngr = CfFileHistogramStatsMngr()\n\t    expected_cf1_entries = {\n\t        time1: {l0: expected_cf1_t1_l0_stats,\n\t                l4: expected_cf1_t1_l4_stats}}\n\t    mngr.add_lines(time1, cf1, all_cf1_stats_lines)\n\t    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n\tstats_dump = \"\"\"2022/10/07-16:50:52.365328 7f68d1999700 [db/db_impl/db_impl.cc:901] ------- DUMPING STATS -------\n\t2022/10/07-16:50:52.365535 7f68d1999700 [db/db_impl/db_impl.cc:903] \n", "** DB Stats **\n\tUptime(secs): 4.1 total, 4.1 interval\n\tCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\n\tCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n\tCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\n\tInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\n\tInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s\n\tInterval stall: 00:00:0.000 H:M:S, 0.0 percent\n\t** Compaction Stats [default] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n", "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t  L0      5/0   154.26 MB   1.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\t Sum   1127/0   70.96 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\t** Compaction Stats [default] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\tUptime(secs): 4.1 total, 4.1 interval\n\tFlush(GB): cumulative 0.004, interval 0.004\n", "AddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n\tAddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n\tCumulative compaction: 0.00 GB write, 0.90 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.3 seconds\n\tInterval compaction: 0.00 GB write, 0.90 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.3 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [default] **\n\t** Level 0 read latency histogram (micros):\n\tCount: 25 Average: 1571.7200  StdDev: 5194.93\n", "Min: 1  Median: 2.8333  Max: 26097\n\tPercentiles: P50: 2.83 P75: 32.50 P99: 26097.00 P99.9: 26097.00 P99.99: 26097.00\n\t------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n\t(       3,       4 ]        1  25.000% 100.000% #####\n\t** Compaction Stats [meta-Kdm3W] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t  L0      2/0    1.95 KB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\t  L1      1/0    1.04 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n", " Sum      3/0    2.99 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\t** Compaction Stats [meta-Kdm3W] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\tUptime(secs): 4.1 total, 4.1 interval\n\tFlush(GB): cumulative 0.000, interval 0.000\n\tAddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n", "AddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n\tCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [meta-Kdm3W] **\n\t** Level 0 read latency histogram (micros):\n\tCount: 8 Average: 3.0000  StdDev: 3.20\n\tMin: 1  Median: 1.0000  Max: 11\n\tPercentiles: P50: 1.00 P75: 3.00 P99: 11.00 P99.9: 11.00 P99.99: 11.00\n", "------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n\t(       3,       4 ]        1  25.000% 100.000% #####\n\t** Level 1 read latency histogram (micros):\n\tCount: 4 Average: 2.5000  StdDev: 0.87\n\tMin: 2  Median: 2.0000  Max: 4\n\tPercentiles: P50: 2.00 P75: 2.00 P99: 3.96 P99.9: 4.00 P99.99: 4.00\n\t------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n\t(       3,       4 ]        1  25.000% 100.000% #####\n", "** Compaction Stats [txlog-Kdm3W] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t** Compaction Stats [txlog-Kdm3W] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUptime(secs): 4.1 total, 4.1 interval\n\tFlush(GB): cumulative 0.000, interval 0.000\n", "AddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n\tAddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n\tCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [txlog-Kdm3W] **\n\t** Compaction Stats [!qs-stats-nvbl9e] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n", "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t  L0      2/0    1.88 KB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n\t  L1      1/0    1.10 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t Sum      3/0    2.97 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n\t** Compaction Stats [!qs-stats-nvbl9e] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n\tUptime(secs): 4.1 total, 4.1 interval\n", "Flush(GB): cumulative 0.000, interval 0.000\n\tAddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n\tAddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n\tCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [!qs-stats-nvbl9e] **\n\t** Level 0 read latency histogram (micros):\n", "Count: 8 Average: 2.1250  StdDev: 1.27\n\tMin: 1  Median: 1.3333  Max: 5\n\tPercentiles: P50: 1.33 P75: 2.00 P99: 5.00 P99.9: 5.00 P99.99: 5.00\n\t------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n\t(       3,       4 ]        1  25.000% 100.000% #####\n\t** Compaction Stats [default] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t  L0      5/0   154.26 MB   1.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n", " Sum   1127/0   70.96 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t** Compaction Stats [default] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\tUptime(secs): 4.1 total, 0.0 interval\n\tFlush(GB): cumulative 0.004, interval 0.000\n\tAddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n", "AddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n\tCumulative compaction: 0.00 GB write, 0.90 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.3 seconds\n\tInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [default] **\n\t** Level 0 read latency histogram (micros):\n\tCount: 25 Average: 1571.7200  StdDev: 5194.93\n\tMin: 1  Median: 2.8333  Max: 26097\n\tPercentiles: P50: 2.83 P75: 32.50 P99: 26097.00 P99.9: 26097.00 P99.99: 26097.00\n", "------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n\t(       3,       4 ]        1  25.000% 100.000% #####\n\t** Compaction Stats [meta-Kdm3W] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t  L0      2/0    1.95 KB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\t  L1      1/0    1.04 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t Sum      3/0    2.99 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n", "** Compaction Stats [meta-Kdm3W] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\tUptime(secs): 4.1 total, 0.0 interval\n\tFlush(GB): cumulative 0.000, interval 0.000\n\tAddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n\tAddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n", "Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [meta-Kdm3W] **\n\t** Level 0 read latency histogram (micros):\n\tCount: 8 Average: 3.0000  StdDev: 3.20\n\tMin: 1  Median: 1.0000  Max: 11\n\tPercentiles: P50: 1.00 P75: 3.00 P99: 11.00 P99.9: 11.00 P99.99: 11.00\n\t------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n", "(       3,       4 ]        1  25.000% 100.000% #####\n\t** Level 1 read latency histogram (micros):\n\tCount: 4 Average: 2.5000  StdDev: 0.87\n\tMin: 2  Median: 2.0000  Max: 4\n\tPercentiles: P50: 2.00 P75: 2.00 P99: 3.96 P99.9: 4.00 P99.99: 4.00\n\t------------------------------------------------------\n\t(       1,       2 ]        3  75.000%  75.000% ###############\n\t(       3,       4 ]        1  25.000% 100.000% #####\n\t** Compaction Stats [txlog-Kdm3W] **\n\tLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n", "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\t** Compaction Stats [txlog-Kdm3W] **\n\tPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n\t-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\tUptime(secs): 4.1 total, 0.0 interval\n\tFlush(GB): cumulative 0.000, interval 0.000\n\tAddFile(GB): cumulative 0.000, interval 0.000\n\tAddFile(Total Files): cumulative 0, interval 0\n", "AddFile(L0 Files): cumulative 0, interval 0\n\tAddFile(Keys): cumulative 0, interval 0\n\tCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n\tStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\t** File Read Latency Histogram By Level [txlog-Kdm3W] **\n\t \"\"\" # noqa\n\tstats_dump = stats_dump.splitlines()\n\tstats_dump = [line.strip() for line in stats_dump]\n\tstats_dump_entries = lines_to_entries(stats_dump)\n", "def test_stats_mngr_parsing_ignoring_repeating_cfs():\n\t    mngr = StatsMngr()\n\t    assert mngr.is_dump_stats_start(stats_dump_entries[0])\n\t    mngr.try_adding_entries(stats_dump_entries, 0)\n"]}
{"filename": "test/test_compactions.py", "chunked_list": ["import pytest\n\timport compactions\n\timport utils\n\tfrom events import EventType\n\tfrom test.testing_utils import create_event, line_to_entry\n\tjob_id1 = 1\n\tjob_id2 = 2\n\tcf1 = \"cf1\"\n\tcf2 = \"cf2\"\n\tcf_names = [cf1, cf2]\n", "time1_minus_10_sec = \"2023/01/24-08:54:30.130553\"\n\ttime1 = \"2023/01/24-08:54:40.130553\"\n\ttime1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\n\ttime1_plus_11_sec = \"2023/01/24-08:55:50.130553\"\n\ttime2 = \"2023/01/24-08:55:40.130553\"\n\ttime2_plus_5_sec = \"2023/01/24-08:55:45.130553\"\n\treason1 = \"Reason1\"\n\treason2 = \"Reason2\"\n\tfiles_l1 = [17248]\n\tfiles_l2 = [16778, 16779, 16780, 16781, 17022]\n", "input_files = {\n\t    1: files_l1,\n\t    2: files_l2\n\t}\n\tdef test_compaction_info():\n\t    info = compactions.CompactionJobInfo(job_id1)\n\t    assert info.get_state() == compactions.CompactionState.WAITING_START\n\t    start_event1 = create_event(job_id1, cf_names, time1_minus_10_sec,\n\t                                EventType.COMPACTION_STARTED, cf1,\n\t                                compaction_reason=reason1)\n", "    start_event2 = create_event(job_id1, cf_names, time1,\n\t                                EventType.COMPACTION_STARTED, cf1,\n\t                                compaction_reason=reason1)\n\t    finish_event1 = create_event(job_id1, cf_names, time1_minus_10_sec,\n\t                                 EventType.COMPACTION_FINISHED, cf1)\n\t    finish_event2 = create_event(job_id1, cf_names, time1_plus_10_sec,\n\t                                 EventType.COMPACTION_FINISHED, cf1)\n\t    finish_event3 = create_event(job_id1, cf_names, time1_plus_11_sec,\n\t                                 EventType.COMPACTION_FINISHED, cf1)\n\t    with pytest.raises(utils.ParsingError):\n", "        info.set_finish_event(finish_event1)\n\t    info.set_start_event(start_event2)\n\t    assert info.get_state() == compactions.CompactionState.STARTED\n\t    with pytest.raises(utils.ParsingError):\n\t        info.set_start_event(start_event1)\n\t    with pytest.raises(utils.ParsingError):\n\t        info.set_finish_event(finish_event1)\n\t    info.set_finish_event(finish_event2)\n\t    assert info.get_state() == compactions.CompactionState.FINISHED\n\t    with pytest.raises(utils.ParsingError):\n", "        info.set_finish_event(finish_event3)\n\tdef test_mon_basic():\n\t    monitor = compactions.CompactionsMonitor()\n\t    assert monitor.get_finished_jobs() == {}\n\t    # Compaction Job 1 - Start + Finish Events\n\t    start_event1 = create_event(job_id1, cf_names, time1,\n\t                                EventType.COMPACTION_STARTED, cf1,\n\t                                compaction_reason=reason1,\n\t                                files_L1=files_l1, files_L2=files_l2)\n\t    monitor.new_event(start_event1)\n", "    assert monitor.get_finished_jobs() == {}\n\t    finish_event1 = create_event(job_id1, cf_names, time1_plus_10_sec,\n\t                                 EventType.COMPACTION_FINISHED, cf1)\n\t    monitor.new_event(finish_event1)\n\t    expected_jobs = {\n\t        job_id1: compactions.CompactionJobInfo(job_id1,\n\t                                               cf_name=cf1,\n\t                                               start_event=start_event1,\n\t                                               finish_event=finish_event1,\n\t                                               pre_finish_info=None)\n", "    }\n\t    assert monitor.get_finished_jobs() == expected_jobs\n\t    # Compaction Job 2 - Start + Finish Events\n\t    start_event2 = create_event(job_id2, cf_names, time2,\n\t                                EventType.COMPACTION_STARTED, cf2,\n\t                                compaction_reason=reason2)\n\t    monitor.new_event(start_event2)\n\t    assert monitor.get_finished_jobs() == expected_jobs\n\t    finish_event2 = create_event(job_id2, cf_names, time2_plus_5_sec,\n\t                                 EventType.COMPACTION_FINISHED, cf2)\n", "    monitor.new_event(finish_event2)\n\t    expected_jobs[job_id2] = \\\n\t        compactions.CompactionJobInfo(job_id2,\n\t                                      cf_name=cf2,\n\t                                      start_event=start_event2,\n\t                                      finish_event=finish_event2)\n\t    assert monitor.get_finished_jobs() == expected_jobs\n\t    expected_cf1_jobs = {\n\t        job_id1: compactions.CompactionJobInfo(job_id1,\n\t                                               cf_name=cf1,\n", "                                               start_event=start_event1,\n\t                                               finish_event=finish_event1,\n\t                                               pre_finish_info=None)\n\t    }\n\t    assert monitor.get_cf_finished_jobs(cf1) == expected_cf1_jobs\n\t    expected_cf2_jobs = {\n\t        job_id2: compactions.CompactionJobInfo(job_id2,\n\t                                               cf_name=cf2,\n\t                                               start_event=start_event2,\n\t                                               finish_event=finish_event2,\n", "                                               pre_finish_info=None)\n\t    }\n\t    assert monitor.get_cf_finished_jobs(cf2) == expected_cf2_jobs\n\tdef test_try_parse_as_pre_finish_stats_line():\n\t    max_score = 4.56\n\t    read_rate_mbps = 475.6\n\t    write_rate_mbps = 475.0\n\t    level = 10\n\t    read_write_amplify = 1.2\n\t    write_amplify = 3.4\n", "    records_in = 5678\n\t    records_dropped = 9876\n\t    pre_fihish_entry = \\\n\t        line_to_entry(\n\t            f\"{time1} 1234 (Original Log Time {time1_minus_10_sec}) \"\n\t            f\"[/compaction/compaction_job.cc:952] \"\n\t            f\"[{cf2}] compacted to: files[3 7 45 427 822 0 0] \"\n\t            f\"max score {max_score}, \"\n\t            f\"MB/sec: {read_rate_mbps} rd, {write_rate_mbps} wr, \"\n\t            f\"level {level}, files in(0, 4) out(1 +0 blob) \"\n", "            f\"MB in(0.0, 239.7 +0.0 blob) out(239.4 +0.0 blob), \"\n\t            f\"read-write-amplify({read_write_amplify}) \"\n\t            f\"write-amplify({write_amplify}) OK, \"\n\t            f\"records in: {records_in}, records dropped: {records_dropped} \"\n\t            f\"output_compression: NoCompression\")\n\t    start_event = create_event(job_id1, cf_names, time1,\n\t                               EventType.COMPACTION_STARTED, cf2,\n\t                               compaction_reason=reason1,\n\t                               files_L1=files_l1, files_L2=files_l2)\n\t    finish_event = create_event(job_id1, cf_names, time1_plus_11_sec,\n", "                                EventType.COMPACTION_FINISHED, cf2,\n\t                                num_input_records=records_in)\n\t    monitor = compactions.CompactionsMonitor()\n\t    monitor.new_event(start_event)\n\t    assert monitor.consider_entry(pre_fihish_entry) == (True, cf2)\n\t    monitor.new_event(finish_event)\n\t    expected_pre_finish_info = \\\n\t        compactions.PreFinishStatsInfo(cf_name=cf2,\n\t                                       read_rate_mbps=read_rate_mbps,\n\t                                       write_rate_mbps=write_rate_mbps,\n", "                                       read_write_amplify=read_write_amplify,\n\t                                       write_amplify=write_amplify,\n\t                                       records_in=records_in,\n\t                                       records_dropped=records_dropped)\n\t    expected_jobs = {\n\t        job_id1: compactions.CompactionJobInfo(\n\t            job_id=job_id1,\n\t            cf_name=cf2,\n\t            start_event=start_event,\n\t            finish_event=finish_event,\n", "            pre_finish_info=expected_pre_finish_info)\n\t    }\n\t    assert monitor.get_finished_jobs() == expected_jobs\n"]}
{"filename": "test/test_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport copy\n\tfrom datetime import datetime\n\timport pytest\n\timport utils\n\tdef test_unify_dicts():\n\t    unify = utils.unify_dicts\n\t    d1 = dict(a=100, b=200)\n", "    d1_copy = copy.deepcopy(d1)\n\t    d2 = dict(a=300, c=500)\n\t    d2_copy = copy.deepcopy(d2)\n\t    assert unify({}, {}, True) == {}\n\t    assert unify(d1, {}, True) == d1\n\t    assert unify({}, d2, True) == d2\n\t    assert unify(d1, d2, True) == dict(a=100, b=200, c=500)\n\t    assert unify(d1, d2, False) == dict(a=300, b=200, c=500)\n\t    # Verify no mutation of inputes\n\t    assert d1_copy == d1\n", "    assert d2_copy == d2\n\tdef test_delete_dict_keys():\n\t    d1 = dict(a=100, b=200, c=300)\n\t    keys = [\"a\"]\n\t    utils.delete_dict_keys(d1, keys)\n\t    assert d1 == dict(b=200, c=300)\n\tdef test_are_dicts_equal_and_in_same_keys_order():\n\t    d1 = dict(x=10, y=20)\n\t    d2 = dict(y=20, x=10)\n\t    are_equal = utils.are_dicts_equal_and_in_same_keys_order\n", "    assert are_equal({}, {})\n\t    assert are_equal(d1, d1)\n\t    assert d1 == d2\n\t    assert not are_equal(d1, d2)\n\t    assert not are_equal(d2, d1)\n\tdef test_unify_lists_preserve_order():\n\t    unify = utils.unify_lists_preserve_order\n\t    assert unify([], []) == []\n\t    assert unify([1, 2], [1, 2]) == [1, 2]\n\t    assert unify([2, 1], [4, 3]) == [2, 1, 4, 3]\n", "    assert unify([2, 1], [2]) == [2, 1]\n\t    assert unify([2, 1], [1]) == [2, 1]\n\t    assert unify([2, 1], [1, 2]) == [2, 1]\n\t    assert unify([2, 1], [2, 3]) == [2, 1, 3]\n\t    assert unify([2, 1, 4], [4, 3, 1, 5, 2]) == [2, 1, 4, 3, 5]\n\tdef test_get_get_times_strs_diff_seconds():\n\t    diff = utils.get_times_strs_diff_seconds\n\t    M = 10 ** 6\n\t    time1 = \"2022/11/24-15:50:09.512106\"\n\t    time1_minus_10_micros = \"2022/11/24-15:50:09.512096\"\n", "    time1_plus_1_second = \"2022/11/24-15:50:10.512106\"\n\t    assert diff(time1, time1_minus_10_micros) == -10 / M\n\t    assert diff(time1_minus_10_micros, time1) == 10 / M\n\t    assert diff(time1, time1_plus_1_second) == 1\n\tdef test_get_time_relative_to():\n\t    diff = utils.get_times_strs_diff_seconds\n\t    rel_time = utils.get_time_relative_to\n\t    time1 = \"2022/11/24-15:50:09.512106\"\n\t    assert rel_time(time1, 0) == time1\n\t    assert diff(time1, rel_time(time1, 1)) == 1\n", "    assert diff(time1, rel_time(time1, -1)) == -1\n\t    assert diff(time1, rel_time(time1, 10**6)) == 10**6\n\t    assert diff(time1, rel_time(time1, 0, num_us=1)) == 1/10**6\n\t    assert diff(time1, rel_time(time1, 0, num_us=10000)) == 1/100\n\t    assert diff(time1, rel_time(time1, 10, num_us=10000)) == 10.01\n\tdef test_compare_times_strs():\n\t    time1 = \"2022/11/24-15:50:09.512106\"\n\t    time2 = \"2022/11/24-15:51:09.512107\"\n\t    assert utils.compare_times_strs(time1, time1) == 0\n\t    assert utils.compare_times_strs(time1, time2) < 0\n", "    assert utils.compare_times_strs(time2, time1) > 0\n\tdef test_parse_time_str():\n\t    parse_time = utils.parse_time_str\n\t    now = datetime.now()\n\t    now_str = now.strftime(\"%Y/%m/%d-%H:%M:%S.%f\")\n\t    assert now == utils.parse_time_str(now_str)\n\t    assert parse_time(\"\", False) is None\n\t    assert parse_time(\"XXXX\", False) is None\n\t    assert parse_time(\"2022/11/24-15:50:09\", False) is None\n\t    with pytest.raises(utils.ParsingError):\n", "        assert parse_time(\"\", True)\n\tdef test_is_valid_time_str():\n\t    is_valid = utils.is_valid_time_str\n\t    assert is_valid(\"2022/11/24-15:50:09.123456\")\n\t    assert not is_valid(\"\")\n\t    assert not is_valid(\"XXX\")\n\t    assert not is_valid(\"2022/11/24-15:50:09\")\n\tdef test_convert_seconds_to_dd_hh_mm_ss():\n\t    one_minute = 60\n\t    one_hour = 3600\n", "    one_day = 24 * 3600\n\t    assert utils.convert_seconds_to_dd_hh_mm_ss(0) == \"0d 00h 00m 00s\"\n\t    assert utils.convert_seconds_to_dd_hh_mm_ss(1) == \"0d 00h 00m 01s\"\n\t    assert utils.convert_seconds_to_dd_hh_mm_ss(one_minute) == \"0d 00h 01m 00s\"\n\t    assert utils.convert_seconds_to_dd_hh_mm_ss(one_hour) == \"0d 01h 00m 00s\"\n\t    assert utils.convert_seconds_to_dd_hh_mm_ss(one_day) == \"1d 00h 00m 00s\"\n\t    assert utils.convert_seconds_to_dd_hh_mm_ss(\n\t        one_day + one_hour + one_minute + 30) == \"1d 01h 01m 30s\"\n\tdef test_get_num_bytes_from_human_readable_components():\n\t    num_bytes = utils.get_num_bytes_from_human_readable_components\n", "    assert num_bytes(\"1\", \"\") == 1\n\t    assert num_bytes(\"  1    \", \"\") == 1\n\t    assert num_bytes(\"1\", \"KB\") == 2**10\n\t    assert num_bytes(\"1\", \"  KB    \") == 2**10\n\t    assert num_bytes(\"1\", \"MB\") == 2**20\n\t    assert num_bytes(\"1\", \"GB\") == 2**30\n\t    assert num_bytes(\"1\", \"TB\") == 2**40\n\t    with pytest.raises(utils.ParsingError):\n\t        num_bytes(\"\", \"\")\n\t    with pytest.raises(utils.ParsingError):\n", "        num_bytes(\"1\", \"X\")\n\tdef test_get_num_bytes_from_human_readable_str():\n\t    num_bytes = utils.get_num_bytes_from_human_readable_str\n\t    assert num_bytes(\"1\") == 1\n\t    assert num_bytes(\"  1    \") == 1\n\t    assert num_bytes(\"1KB\") == 2**10\n\t    assert num_bytes(\"   1  KB    \") == 2**10\n\t    assert num_bytes(\"1 MB\") == 2**20\n\t    assert num_bytes(\"1 GB\") == 2**30\n\t    assert num_bytes(\"1 TB\") == 2**40\n", "    with pytest.raises(utils.ParsingError):\n\t        num_bytes(\"\")\n\t    with pytest.raises(utils.ParsingError):\n\t        num_bytes(\"1 X\")\n\t    with pytest.raises(utils.ParsingError):\n\t        num_bytes(\"KB\")\n\tdef test_get_human_readable_num_bytes():\n\t    human = utils.get_human_readable_num_bytes\n\t    assert human(1) == \"1 B\"\n\t    assert human(1000) == \"1000 B\"\n", "    assert human(1024) == \"1.0 KB\"\n\t    assert human(2048) == \"2.0 KB\"\n\t    assert human(2**20) == \"1.0 MB\"\n\t    assert human(2**30) == \"1.0 GB\"\n\t    assert human(2**40) == \"1.0 TB\"\n\t    assert human(2 * 2**40) == \"2.0 TB\"\n\t    assert human(2**50) == \"1024.0 TB\"\n\tdef test_get_number_from_human_readable_components():\n\t    num_bytes = utils.get_number_from_human_readable_components\n\t    assert num_bytes(\"1\", \"\") == 1\n", "    assert num_bytes(\"  1    \", \"\") == 1\n\t    assert num_bytes(\"1\", \"K\") == 1000\n\t    assert num_bytes(\"1\", \"  K    \") == 1000\n\t    assert num_bytes(\"1\", \"M\") == 10**6\n\t    assert num_bytes(\"1\", \"G\") == 10**9\n\t    with pytest.raises(utils.ParsingError):\n\t        num_bytes(\"\", \"\")\n\t    with pytest.raises(utils.ParsingError):\n\t        num_bytes(\"1\", \"X\")\n\tdef test_get_human_readable_number():\n", "    human = utils.get_human_readable_number\n\t    assert human(1) == \"1\"\n\t    assert human(1000) == \"1000\"\n\t    assert human(9999) == \"9999\"\n\t    assert human(10000) == \"10.0 K\"\n\t    assert human(20000) == \"20.0 K\"\n\t    assert human(100000) == \"100.0 K\"\n\t    assert human(1000000) == \"1000.0 K\"\n\t    assert human(10000000) == \"10.0 M\"\n\t    assert human(10**10) == \"10.0 G\"\n", "    assert human(10**11) == \"100.0 G\"\n\t    assert human(7 * 10**11) == \"700.0 G\"\n\tdef test_get_number_from_human_readable_str():\n\t    get_num = utils.get_number_from_human_readable_str\n\t    assert get_num(\"0\") == 0\n\t    assert get_num(\"    0\") == 0\n\t    assert get_num(\"0   \") == 0\n\t    assert get_num(\" 1000 \") == 1000\n\t    assert get_num(\"1K\") == 1000\n\t    assert get_num(\"1 K\") == 1000\n", "    assert get_num(\"   1  K   \") == 1000\n\t    assert get_num(\"1M\") == 10**6\n\t    assert get_num(\"1 M \") == 10**6\n\t    assert get_num(\"   1M  \") == 10**6\n\t    assert get_num(\"1G\") == 10**9\n\t    assert get_num(\"1 G\") == 10**9\n\t    assert get_num(\"   1G  \") == 10**9\n\t    with pytest.raises(utils.ParsingError):\n\t        assert get_num(\"0X\")\n\t    with pytest.raises(utils.ParsingError):\n", "        assert get_num(\"1KR\")\n\t    with pytest.raises(utils.ParsingError):\n\t        assert get_num(\"1K R\")\n\tdef test_try_find_cf_in_lines():\n\t    cf1 = \"cf1\"\n\t    cf2 = \"cf2\"\n\t    assert utils.try_find_cfs_in_lines([], \"\") is None\n\t    assert utils.try_find_cfs_in_lines([cf1], \"\") is None\n\t    assert utils.try_find_cfs_in_lines([cf1], \"cf1\") is None\n\t    assert utils.try_find_cfs_in_lines([cf1], \"[cf1]\") is cf1\n", "    assert utils.try_find_cfs_in_lines([cf2], \"cf1\") is None\n\t    assert utils.try_find_cfs_in_lines([cf1, cf2], \"[cf2]\") == cf2\n\t    assert set(utils.try_find_cfs_in_lines([cf1, cf2], \"[cf2] [cf1]\")) == \\\n\t           set([cf2, cf1])\n\t    assert set(utils.try_find_cfs_in_lines([cf2, cf1], \"[cf2] [cf1]\")) == \\\n\t           set([cf2, cf1])\n\t    lines1 = f\"Line 1 No cf\\nLine 2 with [{cf1}]\".splitlines()\n\t    assert len(lines1) == 2\n\t    assert utils.try_find_cfs_in_lines([cf1], lines1) == cf1\n\t    lines2 = f\"Line 1 No cf\\nLine 2 with [{cf2}]\\nLine3 [{cf2}]\".splitlines()\n", "    assert len(lines2) == 3\n\t    assert utils.try_find_cfs_in_lines([cf2], lines2) == cf2\n\t    assert utils.try_find_cfs_in_lines([cf1], lines2) is None\n\t    assert utils.try_find_cfs_in_lines([cf1, cf2], lines2) == cf2\n\t    lines3 = f\"Line 1 No cf\\nLine 2 with [{cf2}]\\nLine3 [{cf1}]\".splitlines()\n\t    assert len(lines3) == 3\n\t    assert utils.try_find_cfs_in_lines([cf2], lines3) == cf2\n\t    assert utils.try_find_cfs_in_lines([cf1], lines3) == cf1\n\t    assert set(utils.try_find_cfs_in_lines([cf1, cf2], lines3)) == \\\n\t           set([cf1, cf2])\n"]}
{"filename": "test/test_log_entry.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.'''\n\timport pytest\n\tfrom log_entry import LogEntry\n\timport utils\n\tdef test_is_entry_start():\n\t    # Dummy text\n\t    assert not LogEntry.is_entry_start((\"XXXX\"))\n\t    # Invalid line - timestamp missing microseconds\n", "    assert not LogEntry.is_entry_start(\"2022/11/24-15:58:04\")\n\t    # Invalid line - timestamp microseconds is cropped\n\t    assert not LogEntry.is_entry_start(\"2022/11/24-15:58:04.758\")\n\t    # Valid line\n\t    assert LogEntry.is_entry_start(\"2022/11/24-15:58:04.758352 32819 \")\n\tdef test_basic_single_line():\n\t    log_line1 = \"2022/11/24-15:58:04.758402 32819 DB SUMMARY\"\n\t    log_line2 = \"2022/11/24-15:58:05.068464 32819 [/version_set.cc:4965] \" \\\n\t                \"Recovered from manifest\"\n\t    entry = LogEntry(100, log_line1, True)\n", "    assert \"2022/11/24-15:58:04.758402\" == entry.get_time()\n\t    assert entry.get_start_line_idx() == 100\n\t    assert entry.get_lines_idxs_range() == (100, 101)\n\t    assert not entry.get_code_pos()\n\t    assert not entry.is_warn_msg()\n\t    assert entry.get_warning_type() is None\n\t    assert entry.have_all_lines_been_added()\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        entry.add_line(log_line2, last_line=True)\n\t    with pytest.raises(utils.ParsingAssertion):\n", "        entry.add_line(log_line2, last_line=False)\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        entry.all_lines_added()\n\tdef test_warn_single_line():\n\t    warn_msg = \"2022/04/17-15:24:51.089890 7f4a9fdff700 [WARN] \" \\\n\t               \"[/column_family.cc:932] Stalling writes, \" \\\n\t               \"L0 files 2, memtables 2\"\n\t    entry = LogEntry(100, warn_msg, True)\n\t    assert \"2022/04/17-15:24:51.089890\" == entry.get_time()\n\t    assert entry.get_code_pos() == \"/column_family.cc:932\"\n", "    assert entry.is_warn_msg()\n\t    assert entry.get_warning_type() == utils.WarningType.WARN\n\tdef test_multi_line_entry():\n\t    log_line1 = \"2022/11/24-15:58:04.758402 32819 DB SUMMARY\"\n\t    log_line2 = \"Continuation Line 1\"\n\t    log_line3 = \"Continuation Line 2\"\n\t    log_line4 = \"Continuation Line 2\"\n\t    log_line5 = \"2022/11/24-15:58:05.068464 32819 [/version_set.cc:4965] \" \\\n\t                \"Recovered from manifest\"\n\t    entry = LogEntry(100, log_line1, False)\n", "    assert \"2022/11/24-15:58:04.758402\" == entry.get_time()\n\t    assert not entry.have_all_lines_been_added()\n\t    entry.add_line(log_line2, last_line=False)\n\t    assert not entry.have_all_lines_been_added()\n\t    assert entry.get_lines_idxs_range() == (100, 102)\n\t    # Attempting to add the start of a new entry\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        entry.add_line(log_line5, last_line=True)\n\t    assert not entry.have_all_lines_been_added()\n\t    assert entry.get_lines_idxs_range() == (100, 102)\n", "    entry.add_line(log_line3, last_line=False)\n\t    assert not entry.have_all_lines_been_added()\n\t    assert entry.get_lines_idxs_range() == (100, 103)\n\t    entry.all_lines_added()\n\t    assert entry.have_all_lines_been_added()\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        entry.all_lines_added()\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        entry.add_line(log_line4, last_line=True)\n\t    with pytest.raises(utils.ParsingAssertion):\n", "        entry.add_line(log_line4, last_line=False)\n\tdef test_invalid_entry_start():\n\t    with pytest.raises(utils.ParsingAssertion):\n\t        LogEntry(10, \"Not an entry start line\")\n\t    log_line = \"2022/11/24-15:58:04.758402\"\n\t    with pytest.raises(utils.ParsingError):\n\t        LogEntry(10, log_line)\n"]}
{"filename": "test/test_regexes.py", "chunked_list": ["import re\n\timport regexes as r\n\tdef test_cf_name_regex():\n\t    assert re.search(r.CF_NAME, \"[CF1]\").group('cf') == 'CF1'\n\t    assert re.search(r.CF_NAME, \"[ C F 1 ]\").group('cf') == ' C F 1 '\n\t    assert re.search(r.CF_NAME, \"[]\").group('cf') == ''\n\t    assert re.search(r.CF_NAME, \"[...]\").group('cf') == '...'\n\t    assert re.search(r.CF_NAME, \"[\\0\\1]\").group('cf') == '\\0\\1'\n\t    assert re.search(r.CF_NAME, \"[!@#$%_-=]\").group('cf') == '!@#$%_-='\n\t    assert re.search(r.CF_NAME, \"CF1\") is None\n", "    assert re.search(r.CF_NAME, \"CF1]\") is None\n\t    assert re.search(r.CF_NAME, \"[CF1\") is None\n\tdef test_uptime_stats_line_regex():\n\t    line1 = \"Uptime(secs): 654.3 total, 789.1 interval\"\n\t    line2 = \" Uptime(secs):123.4 total, 56.7 interval  \"\n\t    line3 = \"Uptime(secs):123.4 total,\"\n\t    line4 = \"Uptime(secs):123.4 total, 78.9\"\n\t    m1 = re.search(r.UPTIME_STATS_LINE, line1)\n\t    assert m1\n\t    assert m1.group('total') == '654.3'\n", "    assert m1.group('interval') == '789.1'\n\t    m2 = re.search(r.UPTIME_STATS_LINE, line2)\n\t    assert m2\n\t    assert m2.group('total') == '123.4'\n\t    assert m2.group('interval') == '56.7'\n\t    assert not re.search(r.UPTIME_STATS_LINE, line3)\n\t    assert not re.search(r.UPTIME_STATS_LINE, line4)\n\tdef test_db_wide_interval_stall_regex():\n\t    line = 'Interval stall: 01:02:3.456 H:M:S, 7.8 percent'\n\t    m = re.search(r.DB_WIDE_INTERVAL_STALL, line)\n", "    assert m\n\t    assert m.groups() == ('01', '02', '3', '456', '7.8')\n\tdef test_db_wide_cumulative_stall_regex():\n\t    line = 'Cumulative stall: 01:02:3.456 H:M:S, 7.8 percent'\n\t    m = re.search(r.DB_WIDE_CUMULATIVE_STALL, line)\n\t    assert m\n\t    assert m.groups() == ('01', '02', '3', '456', '7.8')\n\tdef test_db_wide_cumulative_writes():\n\t    line = 'Cumulative writes: 819K writes, 1821M keys, 788K commit groups, ' \\\n\t           '1.0 writes per commit group, ingest: 80.67 GB, 68.66 MB/s '\n", "    m = re.search(r.DB_WIDE_CUMULATIVE_WRITES, line)\n\t    assert m\n\t    assert m.groups() == ('819', 'K', '1821', 'M', '80.67', '68.66')\n"]}
