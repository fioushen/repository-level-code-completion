{"filename": "setup.py", "chunked_list": ["\"\"\"setup.py for axolotl\"\"\"\n\tfrom setuptools import find_packages, setup\n\tinstall_requires = []\n\twith open(\"./requirements.txt\", encoding=\"utf-8\") as requirements_file:\n\t    # don't include peft yet until we check the int4\n\t    # need to manually install peft for now...\n\t    reqs = [r.strip() for r in requirements_file.readlines() if \"peft\" not in r]\n\t    reqs = [r for r in reqs if r and r[0] != \"#\"]\n\t    for r in reqs:\n\t        install_requires.append(r)\n", "setup(\n\t    name=\"axolotl\",\n\t    version=\"0.1\",\n\t    description=\"You know you're going to axolotl questions\",\n\t    package_dir={\"\": \"src\"},\n\t    packages=find_packages(),\n\t    install_requires=install_requires,\n\t    extras_require={\n\t        \"gptq\": [\n\t            \"alpaca_lora_4bit @ git+https://github.com/winglian/alpaca_lora_4bit.git@setup_pip\",\n", "        ],\n\t        \"gptq_triton\": [\n\t            \"alpaca_lora_4bit[triton] @ git+https://github.com/winglian/alpaca_lora_4bit.git@setup_pip\",\n\t        ],\n\t        \"extras\": [\n\t            \"flash-attn\",\n\t            \"deepspeed\",\n\t        ],\n\t    },\n\t)\n"]}
{"filename": "scripts/finetune.py", "chunked_list": ["\"\"\"Prepare and train a model on a dataset. Can also infer from a model or merge lora\"\"\"\n\timport importlib\n\timport logging\n\timport os\n\timport random\n\timport signal\n\timport sys\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, List, Optional, Union\n\timport fire\n", "import torch\n\timport yaml\n\t# add src to the pythonpath so we don't need to pip install this\n\tfrom optimum.bettertransformer import BetterTransformer\n\tfrom transformers import GenerationConfig, TextStreamer\n\tfrom axolotl.logging_config import configure_logging\n\tfrom axolotl.utils.data import load_prepare_datasets, load_pretraining_dataset\n\tfrom axolotl.utils.dict import DictDefault\n\tfrom axolotl.utils.models import load_model, load_tokenizer\n\tfrom axolotl.utils.tokenization import check_dataset_labels\n", "from axolotl.utils.trainer import setup_trainer\n\tfrom axolotl.utils.validation import validate_config\n\tfrom axolotl.utils.wandb import setup_wandb_env_vars\n\tproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n\tsrc_dir = os.path.join(project_root, \"src\")\n\tsys.path.insert(0, src_dir)\n\tconfigure_logging()\n\tLOG = logging.getLogger(\"axolotl.scripts\")\n\tDEFAULT_DATASET_PREPARED_PATH = \"last_run_prepared\"\n\tos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n", "def choose_device(cfg):\n\t    def get_device():\n\t        try:\n\t            if torch.cuda.is_available():\n\t                return f\"cuda:{cfg.local_rank}\"\n\t            if torch.backends.mps.is_available():\n\t                return \"mps\"\n\t            raise SystemError(\"No CUDA/mps device found\")\n\t        except Exception:  # pylint: disable=broad-exception-caught\n\t            return \"cpu\"\n", "    cfg.device = get_device()\n\t    if cfg.device_map != \"auto\":\n\t        if cfg.device.startswith(\"cuda\"):\n\t            cfg.device_map = {\"\": cfg.local_rank}\n\t        else:\n\t            cfg.device_map = {\"\": cfg.device}\n\tdef get_multi_line_input() -> Optional[str]:\n\t    print(\"Give me an instruction (Ctrl + D to finish): \")\n\t    instruction = \"\"\n\t    for line in sys.stdin:\n", "        instruction += line  # pylint: disable=consider-using-join\n\t    # instruction = pathlib.Path(\"/proc/self/fd/0\").read_text()\n\t    return instruction\n\tdef do_inference(cfg, model, tokenizer, prompter: Optional[str]):\n\t    default_tokens = {\"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n\t    for token, symbol in default_tokens.items():\n\t        # If the token isn't already specified in the config, add it\n\t        if not (cfg.special_tokens and token in cfg.special_tokens):\n\t            tokenizer.add_special_tokens({token: symbol})\n\t    prompter_module = None\n", "    if prompter:\n\t        prompter_module = getattr(\n\t            importlib.import_module(\"axolotl.prompters\"), prompter\n\t        )\n\t    if cfg.landmark_attention:\n\t        from axolotl.monkeypatch.llama_landmark_attn import set_model_mem_id\n\t        set_model_mem_id(model, tokenizer)\n\t        model.set_mem_cache_args(\n\t            max_seq_len=255, mem_freq=50, top_k=5, max_cache_size=None\n\t        )\n", "    while True:\n\t        print(\"=\" * 80)\n\t        # support for multiline inputs\n\t        instruction = get_multi_line_input()\n\t        if not instruction:\n\t            return\n\t        if prompter_module:\n\t            prompt: str = next(\n\t                prompter_module().build_prompt(instruction=instruction.strip(\"\\n\"))\n\t            )\n", "        else:\n\t            prompt = instruction.strip()\n\t        batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\t        print(\"=\" * 40)\n\t        model.eval()\n\t        with torch.no_grad():\n\t            generation_config = GenerationConfig(\n\t                repetition_penalty=1.1,\n\t                max_new_tokens=1024,\n\t                temperature=0.9,\n", "                top_p=0.95,\n\t                top_k=40,\n\t                bos_token_id=tokenizer.bos_token_id,\n\t                eos_token_id=tokenizer.eos_token_id,\n\t                pad_token_id=tokenizer.pad_token_id,\n\t                do_sample=True,\n\t                use_cache=True,\n\t                return_dict_in_generate=True,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n", "                output_scores=False,\n\t            )\n\t            streamer = TextStreamer(tokenizer)\n\t            generated = model.generate(\n\t                inputs=batch[\"input_ids\"].to(cfg.device),\n\t                generation_config=generation_config,\n\t                streamer=streamer,\n\t            )\n\t        print(\"=\" * 40)\n\t        print(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0]))\n", "def choose_config(path: Path):\n\t    yaml_files = list(path.glob(\"*.yml\"))\n\t    if not yaml_files:\n\t        raise ValueError(\n\t            \"No YAML config files found in the specified directory. Are you using a .yml extension?\"\n\t        )\n\t    print(\"Choose a YAML file:\")\n\t    for idx, file in enumerate(yaml_files):\n\t        print(f\"{idx + 1}. {file}\")\n\t    chosen_file = None\n", "    while chosen_file is None:\n\t        try:\n\t            choice = int(input(\"Enter the number of your choice: \"))\n\t            if 1 <= choice <= len(yaml_files):\n\t                chosen_file = yaml_files[choice - 1]\n\t            else:\n\t                print(\"Invalid choice. Please choose a number from the list.\")\n\t        except ValueError:\n\t            print(\"Invalid input. Please enter a number.\")\n\t    return chosen_file\n", "def check_not_in(list1: List[str], list2: Union[Dict[str, Any], List[str]]) -> bool:\n\t    return not any(el in list2 for el in list1)\n\tdef train(\n\t    config: Path = Path(\"configs/\"),\n\t    prepare_ds_only: bool = False,\n\t    **kwargs,\n\t):\n\t    if Path(config).is_dir():\n\t        config = choose_config(config)\n\t    # load the config from the yaml file\n", "    with open(config, encoding=\"utf-8\") as file:\n\t        cfg: DictDefault = DictDefault(yaml.safe_load(file))\n\t    # if there are any options passed in the cli, if it is something that seems valid from the yaml,\n\t    # then overwrite the value\n\t    cfg_keys = cfg.keys()\n\t    for k, _ in kwargs.items():\n\t        # if not strict, allow writing to cfg even if it's not in the yml already\n\t        if k in cfg_keys or not cfg.strict:\n\t            # handle booleans\n\t            if isinstance(cfg[k], bool):\n", "                cfg[k] = bool(kwargs[k])\n\t            else:\n\t                cfg[k] = kwargs[k]\n\t    validate_config(cfg)\n\t    # setup some derived config / hyperparams\n\t    cfg.gradient_accumulation_steps = cfg.gradient_accumulation_steps or (\n\t        cfg.batch_size // cfg.micro_batch_size\n\t    )\n\t    cfg.batch_size = (\n\t        cfg.batch_size or cfg.micro_batch_size * cfg.gradient_accumulation_steps\n", "    )\n\t    cfg.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\t    cfg.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n\t    choose_device(cfg)\n\t    cfg.ddp = cfg.ddp if cfg.ddp is not None else cfg.world_size != 1\n\t    if cfg.ddp:\n\t        cfg.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", 0))}\n\t        cfg.batch_size = cfg.batch_size * cfg.world_size\n\t    setup_wandb_env_vars(cfg)\n\t    if cfg.device == \"mps\":\n", "        cfg.load_in_8bit = False\n\t        cfg.tf32 = False\n\t        if cfg.bf16:\n\t            cfg.fp16 = True\n\t        cfg.bf16 = False\n\t    if cfg.tf32:\n\t        torch.backends.cuda.matmul.allow_tf32 = True\n\t    # load the tokenizer first\n\t    tokenizer_config = cfg.tokenizer_config or cfg.base_model_config\n\t    LOG.info(f\"loading tokenizer... {tokenizer_config}\")\n", "    tokenizer = load_tokenizer(tokenizer_config, cfg.tokenizer_type, cfg)\n\t    if (\n\t        check_not_in([\"shard\", \"merge_lora\"], kwargs) and not cfg.inference\n\t    ):  # don't need to load dataset for these\n\t        if not cfg.pretraining_dataset:\n\t            train_dataset, eval_dataset = load_prepare_datasets(\n\t                tokenizer, cfg, DEFAULT_DATASET_PREPARED_PATH\n\t            )\n\t        else:\n\t            train_dataset = load_pretraining_dataset(\n", "                cfg.pretraining_dataset,\n\t                tokenizer,\n\t                max_tokens=cfg.sequence_len,\n\t                seed=cfg.seed,\n\t            )\n\t            # https://discuss.huggingface.co/t/how-to-use-huggingface-trainer-streaming-datasets-without-wrapping-it-with-torchdatas-iterablewrapper/25230\n\t            train_dataset = train_dataset.with_format(\"torch\")\n\t            eval_dataset = None\n\t    if cfg.debug or \"debug\" in kwargs:\n\t        LOG.info(\"check_dataset_labels...\")\n", "        check_dataset_labels(\n\t            train_dataset.select(\n\t                [random.randrange(0, len(train_dataset) - 1) for _ in range(5)]  # nosec\n\t            ),\n\t            tokenizer,\n\t        )\n\t    if prepare_ds_only:\n\t        LOG.info(\"Finished preparing dataset. Exiting...\")\n\t        return\n\t    # Load the model and tokenizer\n", "    LOG.info(\"loading model and peft_config...\")\n\t    model, peft_config = load_model(\n\t        cfg.base_model,\n\t        cfg.base_model_config,\n\t        cfg.model_type,\n\t        tokenizer,\n\t        cfg,\n\t        adapter=cfg.adapter,\n\t    )\n\t    if \"merge_lora\" in kwargs and cfg.adapter is not None:\n", "        LOG.info(\"running merge of LoRA with base model\")\n\t        model = model.merge_and_unload()\n\t        model.to(dtype=torch.float16)\n\t        if cfg.local_rank == 0:\n\t            LOG.info(\"saving merged model\")\n\t            model.save_pretrained(str(Path(cfg.output_dir) / \"merged\"))\n\t        return\n\t    if cfg.inference:\n\t        LOG.info(\"calling do_inference function\")\n\t        prompter: Optional[str] = \"AlpacaPrompter\"\n", "        if \"prompter\" in kwargs:\n\t            if kwargs[\"prompter\"] == \"None\":\n\t                prompter = None\n\t            else:\n\t                prompter = kwargs[\"prompter\"]\n\t        do_inference(cfg, model, tokenizer, prompter=prompter)\n\t        return\n\t    if \"shard\" in kwargs:\n\t        model.save_pretrained(cfg.output_dir)\n\t        return\n", "    trainer = setup_trainer(cfg, train_dataset, eval_dataset, model, tokenizer)\n\t    model.config.use_cache = False\n\t    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t        LOG.info(\"Compiling torch model\")\n\t        model = torch.compile(model)\n\t    # go ahead and presave, so we have the adapter config available to inspect\n\t    if peft_config:\n\t        LOG.info(f\"Pre-saving adapter config to {cfg.output_dir}\")\n\t        peft_config.save_pretrained(cfg.output_dir)\n\t    # In case we want to stop early with ctrl+c, this is a nice to have to save the pretrained model\n", "    if cfg.local_rank == 0:\n\t        def terminate_handler(_, __, model):\n\t            if cfg.flash_optimum:\n\t                model = BetterTransformer.reverse(model)\n\t            model.save_pretrained(cfg.output_dir)\n\t            sys.exit(0)\n\t        signal.signal(\n\t            signal.SIGINT, lambda signum, frame: terminate_handler(signum, frame, model)\n\t        )\n\t    LOG.info(\"Starting trainer...\")\n", "    if cfg.group_by_length:\n\t        LOG.info(\"hang tight... sorting dataset for group_by_length\")\n\t    resume_from_checkpoint = cfg.resume_from_checkpoint\n\t    if cfg.resume_from_checkpoint is None and cfg.auto_resume_from_checkpoints:\n\t        possible_checkpoints = [\n\t            str(cp) for cp in Path(cfg.output_dir).glob(\"checkpoint-*\")\n\t        ]\n\t        if len(possible_checkpoints) > 0:\n\t            sorted_paths = sorted(\n\t                possible_checkpoints,\n", "                key=lambda path: int(path.split(\"-\")[-1]),\n\t            )\n\t            resume_from_checkpoint = sorted_paths[-1]\n\t            LOG.info(\n\t                f\"Using Auto-resume functionality to start with checkpoint at {resume_from_checkpoint}\"\n\t            )\n\t    if not Path(cfg.output_dir).is_dir():\n\t        os.makedirs(cfg.output_dir, exist_ok=True)\n\t    if cfg.flash_optimum:\n\t        with torch.backends.cuda.sdp_kernel(\n", "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n\t        ):\n\t            trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\t    else:\n\t        trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\t    LOG.info(f\"Training Completed!!! Saving pre-trained model to {cfg.output_dir}\")\n\t    # TODO do we need this fix? https://huggingface.co/docs/accelerate/usage_guides/fsdp#saving-and-loading\n\t    # only save on rank 0, otherwise it corrupts output on multi-GPU when multiple processes attempt to write the same file\n\t    if cfg.local_rank == 0:\n\t        if cfg.flash_optimum:\n", "            model = BetterTransformer.reverse(model)\n\t        model.save_pretrained(cfg.output_dir)\n\t    # trainer.save_model(cfg.output_dir)  # TODO this may be needed for deepspeed to work? need to review another time\n\tif __name__ == \"__main__\":\n\t    fire.Fire(train)\n"]}
{"filename": "scripts/alpaca_json_to_jsonl.py", "chunked_list": ["\"\"\"Module to convert json file to jsonl\"\"\"\n\timport os\n\timport sys\n\tfrom pathlib import Path\n\tfrom typing import Optional, Union\n\timport fire\n\tfrom axolotl.convert import (\n\t    FileReader,\n\t    FileWriter,\n\t    JsonlSerializer,\n", "    JsonParser,\n\t    JsonToJsonlConverter,\n\t    StdoutWriter,\n\t)\n\tfrom axolotl.logging_config import configure_logging\n\tconfigure_logging()\n\t# add src to the pythonpath so we don't need to pip install this\n\tproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n\tsrc_dir = os.path.join(project_root, \"src\")\n\tsys.path.insert(0, src_dir)\n", "def main(\n\t    file: Path,\n\t    output: Optional[Path] = None,\n\t    to_stdout: Optional[bool] = False,\n\t):\n\t    \"\"\"\n\t    Convert a json file to jsonl\n\t    \"\"\"\n\t    file_reader = FileReader()\n\t    writer: Union[StdoutWriter, FileWriter]\n", "    if to_stdout or output is None:\n\t        writer = StdoutWriter()\n\t    else:\n\t        writer = FileWriter(output)\n\t    json_parser = JsonParser()\n\t    jsonl_serializer = JsonlSerializer()\n\t    converter = JsonToJsonlConverter(file_reader, writer, json_parser, jsonl_serializer)\n\t    converter.convert(file, output)\n\tif __name__ == \"__main__\":\n\t    fire.Fire(main)\n"]}
{"filename": "tests/test_validation.py", "chunked_list": ["\"\"\"Module for testing the validation module\"\"\"\n\timport logging\n\timport unittest\n\tfrom typing import Optional\n\timport pytest\n\tfrom axolotl.utils.dict import DictDefault\n\tfrom axolotl.utils.validation import validate_config\n\tclass ValidationTest(unittest.TestCase):\n\t    \"\"\"\n\t    Test the validation module\n", "    \"\"\"\n\t    _caplog: Optional[pytest.LogCaptureFixture] = None\n\t    @pytest.fixture(autouse=True)\n\t    def inject_fixtures(self, caplog):\n\t        self._caplog = caplog\n\t    def test_load_4bit_deprecate(self):\n\t        cfg = DictDefault(\n\t            {\n\t                \"load_4bit\": True,\n\t            }\n", "        )\n\t        with pytest.raises(ValueError):\n\t            validate_config(cfg)\n\t    def test_batch_size_unused_warning(self):\n\t        cfg = DictDefault(\n\t            {\n\t                \"batch_size\": 32,\n\t            }\n\t        )\n\t        with self._caplog.at_level(logging.WARNING):\n", "            validate_config(cfg)\n\t            assert \"batch_size is not recommended\" in self._caplog.records[0].message\n\t    def test_qlora(self):\n\t        base_cfg = DictDefault(\n\t            {\n\t                \"adapter\": \"qlora\",\n\t            }\n\t        )\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n\t            {\n", "                \"load_in_8bit\": True,\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=r\".*8bit.*\"):\n\t            validate_config(cfg)\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n\t            {\n\t                \"gptq\": True,\n\t            }\n\t        )\n", "        with pytest.raises(ValueError, match=r\".*gptq.*\"):\n\t            validate_config(cfg)\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n\t            {\n\t                \"load_in_4bit\": False,\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=r\".*4bit.*\"):\n\t            validate_config(cfg)\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n", "            {\n\t                \"load_in_4bit\": True,\n\t            }\n\t        )\n\t        validate_config(cfg)\n\t    def test_qlora_merge(self):\n\t        base_cfg = DictDefault(\n\t            {\n\t                \"adapter\": \"qlora\",\n\t                \"merge_lora\": True,\n", "            }\n\t        )\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n\t            {\n\t                \"load_in_8bit\": True,\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=r\".*8bit.*\"):\n\t            validate_config(cfg)\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n", "            {\n\t                \"gptq\": True,\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=r\".*gptq.*\"):\n\t            validate_config(cfg)\n\t        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n\t            {\n\t                \"load_in_4bit\": True,\n\t            }\n", "        )\n\t        with pytest.raises(ValueError, match=r\".*4bit.*\"):\n\t            validate_config(cfg)\n\t    def test_hf_use_auth_token(self):\n\t        cfg = DictDefault(\n\t            {\n\t                \"push_dataset_to_hub\": \"namespace/repo\",\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=r\".*hf_use_auth_token.*\"):\n", "            validate_config(cfg)\n\t        cfg = DictDefault(\n\t            {\n\t                \"push_dataset_to_hub\": \"namespace/repo\",\n\t                \"hf_use_auth_token\": True,\n\t            }\n\t        )\n\t        validate_config(cfg)\n\t    def test_gradient_accumulations_or_batch_size(self):\n\t        cfg = DictDefault(\n", "            {\n\t                \"gradient_accumulation_steps\": 1,\n\t                \"batch_size\": 1,\n\t            }\n\t        )\n\t        with pytest.raises(\n\t            ValueError, match=r\".*gradient_accumulation_steps or batch_size.*\"\n\t        ):\n\t            validate_config(cfg)\n\t        cfg = DictDefault(\n", "            {\n\t                \"batch_size\": 1,\n\t            }\n\t        )\n\t        validate_config(cfg)\n\t        cfg = DictDefault(\n\t            {\n\t                \"gradient_accumulation_steps\": 1,\n\t            }\n\t        )\n", "        validate_config(cfg)\n\t    def test_falcon_fsdp(self):\n\t        regex_exp = r\".*FSDP is not supported for falcon models.*\"\n\t        # Check for lower-case\n\t        cfg = DictDefault(\n\t            {\n\t                \"base_model\": \"tiiuae/falcon-7b\",\n\t                \"fsdp\": [\"full_shard\", \"auto_wrap\"],\n\t            }\n\t        )\n", "        with pytest.raises(ValueError, match=regex_exp):\n\t            validate_config(cfg)\n\t        # Check for upper-case\n\t        cfg = DictDefault(\n\t            {\n\t                \"base_model\": \"Falcon-7b\",\n\t                \"fsdp\": [\"full_shard\", \"auto_wrap\"],\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=regex_exp):\n", "            validate_config(cfg)\n\t        cfg = DictDefault(\n\t            {\n\t                \"base_model\": \"tiiuae/falcon-7b\",\n\t            }\n\t        )\n\t        validate_config(cfg)\n\t    def test_mpt_gradient_checkpointing(self):\n\t        regex_exp = r\".*gradient_checkpointing is not supported for MPT models*\"\n\t        # Check for lower-case\n", "        cfg = DictDefault(\n\t            {\n\t                \"base_model\": \"mosaicml/mpt-7b\",\n\t                \"gradient_checkpointing\": True,\n\t            }\n\t        )\n\t        with pytest.raises(ValueError, match=regex_exp):\n\t            validate_config(cfg)\n\t    def test_flash_optimum(self):\n\t        cfg = DictDefault(\n", "            {\n\t                \"flash_optimum\": True,\n\t                \"adapter\": \"lora\",\n\t            }\n\t        )\n\t        with self._caplog.at_level(logging.WARNING):\n\t            validate_config(cfg)\n\t            assert any(\n\t                \"BetterTransformers probably doesn't work with PEFT adapters\"\n\t                in record.message\n", "                for record in self._caplog.records\n\t            )\n\t        cfg = DictDefault(\n\t            {\n\t                \"flash_optimum\": True,\n\t            }\n\t        )\n\t        with self._caplog.at_level(logging.WARNING):\n\t            validate_config(cfg)\n\t            assert any(\n", "                \"probably set bfloat16 or float16\" in record.message\n\t                for record in self._caplog.records\n\t            )\n\t        cfg = DictDefault(\n\t            {\n\t                \"flash_optimum\": True,\n\t                \"fp16\": True,\n\t            }\n\t        )\n\t        regex_exp = r\".*AMP is not supported.*\"\n", "        with pytest.raises(ValueError, match=regex_exp):\n\t            validate_config(cfg)\n\t        cfg = DictDefault(\n\t            {\n\t                \"flash_optimum\": True,\n\t                \"bf16\": True,\n\t            }\n\t        )\n\t        regex_exp = r\".*AMP is not supported.*\"\n\t        with pytest.raises(ValueError, match=regex_exp):\n", "            validate_config(cfg)\n\t    def test_adamw_hyperparams(self):\n\t        cfg = DictDefault(\n\t            {\n\t                \"optimizer\": None,\n\t                \"adam_epsilon\": 0.0001,\n\t            }\n\t        )\n\t        with self._caplog.at_level(logging.WARNING):\n\t            validate_config(cfg)\n", "            assert any(\n\t                \"adamw hyperparameters found, but no adamw optimizer set\"\n\t                in record.message\n\t                for record in self._caplog.records\n\t            )\n\t        cfg = DictDefault(\n\t            {\n\t                \"optimizer\": \"adafactor\",\n\t                \"adam_beta1\": 0.0001,\n\t            }\n", "        )\n\t        with self._caplog.at_level(logging.WARNING):\n\t            validate_config(cfg)\n\t            assert any(\n\t                \"adamw hyperparameters found, but no adamw optimizer set\"\n\t                in record.message\n\t                for record in self._caplog.records\n\t            )\n\t        cfg = DictDefault(\n\t            {\n", "                \"optimizer\": \"adamw_bnb_8bit\",\n\t                \"adam_beta1\": 0.9,\n\t                \"adam_beta2\": 0.99,\n\t                \"adam_epsilon\": 0.0001,\n\t            }\n\t        )\n\t        validate_config(cfg)\n\t        cfg = DictDefault(\n\t            {\n\t                \"optimizer\": \"adafactor\",\n", "            }\n\t        )\n\t        validate_config(cfg)\n"]}
{"filename": "tests/test_prompters.py", "chunked_list": ["\"\"\"Module testing prompters\"\"\"\n\timport unittest\n\tfrom axolotl.prompt_strategies.alpaca_w_system import SystemDataPrompter\n\tfrom axolotl.prompters import (\n\t    AlpacaPrompter,\n\t    MultipleChoiceExplainPrompter,\n\t    PromptStyle,\n\t    UnpromptedPrompter,\n\t)\n\tclass AlpacaPrompterTest(unittest.TestCase):\n", "    \"\"\"\n\t    Test AlpacaPrompter\n\t    \"\"\"\n\t    def test_prompt_style_w_none(self):\n\t        prompter = AlpacaPrompter(prompt_style=None)\n\t        res = next(prompter.build_prompt(\"tell me a joke\"))\n\t        # just testing that it uses instruct style\n\t        assert \"### Instruction:\" in res\n\t    def test_prompt_style_w_instruct(self):\n\t        prompter = AlpacaPrompter(prompt_style=PromptStyle.INSTRUCT.value)\n", "        res = next(\n\t            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n\t        )\n\t        assert \"Below is an instruction\" in res\n\t        assert \"### Instruction:\" in res\n\t        assert \"### Input:\" in res\n\t        assert \"alpacas\" in res\n\t        assert \"### Response:\" in res\n\t        assert \"USER:\" not in res\n\t        assert \"ASSISTANT:\" not in res\n", "        res = next(prompter.build_prompt(\"tell me a joke about the following\"))\n\t        assert \"Below is an instruction\" in res\n\t        assert \"### Instruction:\" in res\n\t        assert \"### Input:\" not in res\n\t        assert \"### Response:\" in res\n\t        assert \"USER:\" not in res\n\t        assert \"ASSISTANT:\" not in res\n\t    def test_prompt_style_w_chat(self):\n\t        prompter = AlpacaPrompter(prompt_style=PromptStyle.CHAT.value)\n\t        res = next(\n", "            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n\t        )\n\t        assert \"Below is an instruction\" in res\n\t        assert \"### Instruction:\" not in res\n\t        assert \"### Input:\" not in res\n\t        assert \"alpacas\" in res\n\t        assert \"### Response:\" not in res\n\t        assert \"USER:\" in res\n\t        assert \"ASSISTANT:\" in res\n\t        res = next(prompter.build_prompt(\"tell me a joke about the following\"))\n", "        assert \"Below is an instruction\" in res\n\t        assert \"### Instruction:\" not in res\n\t        assert \"### Input:\" not in res\n\t        assert \"### Response:\" not in res\n\t        assert \"USER:\" in res\n\t        assert \"ASSISTANT:\" in res\n\t    def test_system_prompt(self):\n\t        prompter = SystemDataPrompter(prompt_style=PromptStyle.CHAT.value)\n\t        res = next(\n\t            prompter.build_prompt_w_system(\n", "                \"use cot\", \"tell me a joke about the following\", \"alpacas\"\n\t            )\n\t        )\n\t        assert \"use cot\" in res\n\t        assert res.startswith(\"### System:\")\n\t        assert \"### Instruction:\" not in res\n\t        assert \"### Input:\" not in res\n\t        assert \"alpacas\" in res\n\t        assert \"### Response:\" not in res\n\t        assert \"USER:\" in res\n", "        assert \"ASSISTANT:\" in res\n\tclass UnpromptedPrompterTest(unittest.TestCase):\n\t    \"\"\"\n\t    Test class for UnpromptedPrompter with no system prompts\n\t    \"\"\"\n\t    def test_prompt_style_w_none(self):\n\t        prompter = UnpromptedPrompter(prompt_style=None)\n\t        res = next(prompter.build_prompt(\"tell me a joke\"))\n\t        assert \"### Instruction:\" in res\n\t        assert \"tell me a joke\" in res\n", "        assert res.startswith(\"###\")\n\t    def test_prompt_style_w_instruct(self):\n\t        prompter = UnpromptedPrompter(prompt_style=PromptStyle.INSTRUCT.value)\n\t        res = next(\n\t            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n\t        )\n\t        assert \"### Instruction:\" in res\n\t        assert \"tell me a joke\" in res\n\t        assert res.startswith(\"###\")\n\t    def test_prompt_style_w_chat(self):\n", "        prompter = UnpromptedPrompter(prompt_style=PromptStyle.CHAT.value)\n\t        res = next(\n\t            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n\t        )\n\t        assert \"USER:\" in res\n\t        assert \"tell me a joke\" in res\n\t        assert res.startswith(\"USER:\")\n\tclass MultipleChoiceExplainPrompterTest(unittest.TestCase):\n\t    \"\"\"\n\t    Test class for MultipleChoiceExplainPrompter\n", "    \"\"\"\n\t    def test_prompt_style_w_chat(self):\n\t        prompter = MultipleChoiceExplainPrompter(prompt_style=PromptStyle.CHAT.value)\n\t        res = next(prompter.build_prompt(\"choose one\", \"- A\\n- B\\n- C\", \"C\"))\n\t        assert \"USER:\" in res\n\t        assert \"choose one\" in res\n\t        assert \"Choose the answer that best answers the question.\" in res\n\t        assert \"- A\\n- B\\n- C\" in res\n"]}
{"filename": "tests/test_dict.py", "chunked_list": ["\"\"\"Module for testing DictDefault class\"\"\"\n\timport unittest\n\timport pytest\n\tfrom axolotl.utils.dict import DictDefault\n\tclass DictDefaultTest(unittest.TestCase):\n\t    \"\"\"\n\t    Test DictDefault class\n\t    \"\"\"\n\t    def test_dict_default(self):\n\t        cfg = DictDefault(\n", "            {\n\t                \"key_a\": {\"key_b\": \"value_a\"},\n\t                \"key_c\": \"value_c\",\n\t                \"key_d\": [\"value_d\", \"value_e\"],\n\t            }\n\t        )\n\t        assert (\n\t            cfg.key_a.key_b == \"value_a\"\n\t        ), \"DictDefault should return value for existing nested keys\"\n\t        assert (\n", "            cfg.key_c == \"value_c\"\n\t        ), \"DictDefault should return value for existing keys\"\n\t        assert (\n\t            cfg.key_d[0] == \"value_d\"\n\t        ), \"DictDefault should return value for existing keys in list\"\n\t        assert (\n\t            \"value_e\" in cfg.key_d\n\t        ), \"DictDefault should support in operator for existing keys in list\"\n\t    def test_dict_or_operator(self):\n\t        cfg = DictDefault(\n", "            {\n\t                \"key_a\": {\"key_b\": \"value_a\"},\n\t                \"key_c\": \"value_c\",\n\t                \"key_d\": [\"value_d\", \"value_e\"],\n\t                \"key_f\": \"value_f\",\n\t            }\n\t        )\n\t        cfg = cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n\t            {\"key_a\": {\"key_b\": \"value_b\"}, \"key_f\": \"value_g\"}\n\t        )\n", "        assert (\n\t            cfg.key_a.key_b == \"value_b\"\n\t        ), \"DictDefault should support OR operator for existing nested keys\"\n\t        assert cfg.key_c == \"value_c\", \"DictDefault should not delete existing key\"\n\t        assert cfg.key_d == [\n\t            \"value_d\",\n\t            \"value_e\",\n\t        ], \"DictDefault should not overwrite existing keys in list\"\n\t        assert (\n\t            cfg.key_f == \"value_g\"\n", "        ), \"DictDefault should support OR operator for existing key\"\n\t    def test_dict_missingkey(self):\n\t        cfg = DictDefault({})\n\t        assert cfg.random_key is None, \"DictDefault should return None for missing keys\"\n\t    def test_dict_nested_missingparentkey(self):\n\t        \"\"\"\n\t        Due to subclassing Dict, DictDefault will error if we try to access a nested key whose parent key does not exist.\n\t        \"\"\"\n\t        cfg = DictDefault({})\n\t        with pytest.raises(\n", "            AttributeError,\n\t            match=r\"'NoneType' object has no attribute 'another_random_key'\",\n\t        ):\n\t            cfg.random_key.another_random_key = \"value\"\n\t    def test_dict_shorthand_assignment(self):\n\t        \"\"\"\n\t        Shorthand assignment is said to not be supported if subclassed. However, their example raises error instead of None.\n\t        This test ensures that it is supported for current implementation.\n\t        Ref: https://github.com/mewwts/addict#default-values\n\t        \"\"\"\n", "        cfg = DictDefault({\"key_a\": {\"key_b\": \"value_a\"}})\n\t        cfg.key_a.key_b = \"value_b\"\n\t        assert cfg.key_a.key_b == \"value_b\", \"Shorthand assignment should be supported\"\n"]}
{"filename": "tests/test_prompt_tokenizers.py", "chunked_list": ["\"\"\"Module for testing prompt tokenizers.\"\"\"\n\timport json\n\timport logging\n\timport unittest\n\tfrom pathlib import Path\n\tfrom transformers import AutoTokenizer\n\tfrom axolotl.prompt_strategies.alpaca_chat import NoSystemPrompter\n\tfrom axolotl.prompt_strategies.alpaca_w_system import (\n\t    InstructionWSystemPromptTokenizingStrategy,\n\t    SystemDataPrompter,\n", ")\n\tfrom axolotl.prompt_tokenizers import (\n\t    AlpacaPromptTokenizingStrategy,\n\t    ShareGPTPromptTokenizingStrategy,\n\t)\n\tfrom axolotl.prompters import AlpacaPrompter, PromptStyle, ShareGPTPrompter\n\tLOG = logging.getLogger(\"axolotl\")\n\tclass TestPromptTokenizationStrategies(unittest.TestCase):\n\t    \"\"\"\n\t    Test class for prompt tokenization strategies.\n", "    \"\"\"\n\t    def setUp(self) -> None:\n\t        # pylint: disable=duplicate-code\n\t        self.tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n\t        self.tokenizer.add_special_tokens(\n\t            {\n\t                \"bos_token\": \"<s>\",\n\t                \"eos_token\": \"</s>\",\n\t                \"unk_token\": \"<unk>\",\n\t            }\n", "        )\n\t    def test_sharegpt_integration(self):\n\t        with open(\n\t            Path(__file__).parent / \"fixtures/conversation.json\", encoding=\"utf-8\"\n\t        ) as fin:\n\t            data = fin.read()\n\t            conversation = json.loads(data)\n\t        with open(\n\t            Path(__file__).parent / \"fixtures/conversation.tokenized.json\",\n\t            encoding=\"utf-8\",\n", "        ) as fin:\n\t            data = fin.read()\n\t            tokenized_conversation = json.loads(data)\n\t        prompter = ShareGPTPrompter(\"chat\")\n\t        strat = ShareGPTPromptTokenizingStrategy(\n\t            prompter,\n\t            self.tokenizer,\n\t            False,\n\t            2048,\n\t        )\n", "        example = strat.tokenize_prompt(conversation)\n\t        for fields in [\"input_ids\", \"attention_mask\", \"labels\"]:\n\t            self.assertEqual(len(example[fields]), len(tokenized_conversation[fields]))\n\t            self.assertEqual(example[fields], tokenized_conversation[fields])\n\t    def test_no_sys_prompt(self):\n\t        \"\"\"\n\t        tests the interface between the user and assistant parts\n\t        \"\"\"\n\t        prompter = NoSystemPrompter()\n\t        # pylint: disable=duplicate-code\n", "        strat = AlpacaPromptTokenizingStrategy(\n\t            prompter,\n\t            self.tokenizer,\n\t            False,\n\t            2048,\n\t        )\n\t        sample = {\n\t            \"instruction\": \"hello cruel. lorem ipsum dolor sit amet.\",\n\t            \"output\": \"world!\",\n\t        }\n", "        example = strat.tokenize_prompt(sample)\n\t        world_idx = example[\"input_ids\"].index(3186)\n\t        assert example[\"labels\"][world_idx] == 3186\n\t        assert example[\"labels\"][world_idx - 1] == -100\n\t    def test_alpaca(self):\n\t        \"\"\"\n\t        tests the interface between the user and assistant parts\n\t        \"\"\"\n\t        # pylint: disable=duplicate-code\n\t        prompter = AlpacaPrompter()\n", "        strat = AlpacaPromptTokenizingStrategy(\n\t            prompter,\n\t            self.tokenizer,\n\t            False,\n\t            2048,\n\t        )\n\t        sample = {\"instruction\": \"hello!\", \"output\": \"Hi! How can I help?\"}\n\t        example = strat.tokenize_prompt(sample)\n\t        world_idx = example[\"input_ids\"].index(6324)\n\t        assert example[\"labels\"][world_idx] == 6324\n", "        assert example[\"labels\"][world_idx - 1] == -100\n\tclass InstructionWSystemPromptTokenizingStrategyTest(unittest.TestCase):\n\t    \"\"\"\n\t    Test class for prompt tokenization strategies with sys prompt from the dataset\n\t    \"\"\"\n\t    def setUp(self) -> None:\n\t        # pylint: disable=duplicate-code\n\t        self.tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n\t        self.tokenizer.add_special_tokens(\n\t            {\n", "                \"bos_token\": \"<s>\",\n\t                \"eos_token\": \"</s>\",\n\t                \"unk_token\": \"<unk>\",\n\t            }\n\t        )\n\t    def test_system_alpaca(self):\n\t        prompter = SystemDataPrompter(PromptStyle.CHAT.value)\n\t        strat = InstructionWSystemPromptTokenizingStrategy(\n\t            prompter,\n\t            self.tokenizer,\n", "            False,\n\t            2048,\n\t        )\n\t        sample = {\n\t            \"system\": \"use cot\",\n\t            \"instruction\": \"hello!\",\n\t            \"output\": \"Hi! How can I help?\",\n\t        }\n\t        example = strat.tokenize_prompt(sample)\n\t        assert example[\"input_ids\"][0:4] == [1, 835, 2184, 29901]  # \"<s>### System:\"\n", "        assert example[\"input_ids\"][5:7] == [1509, 20118]  # \"use cot\"\n\t        assert example[\"input_ids\"][9] == 11889  # USER\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "tests/test_packed_dataset.py", "chunked_list": ["\"\"\"Module for testing dataset sequence packing\"\"\"\n\timport unittest\n\tfrom pathlib import Path\n\tfrom datasets import Dataset, load_dataset\n\tfrom transformers import AutoTokenizer\n\tfrom axolotl.datasets import ConstantLengthDataset, TokenizedPromptDataset\n\tfrom axolotl.prompt_tokenizers import AlpacaPromptTokenizingStrategy\n\tfrom axolotl.prompters import AlpacaPrompter\n\tclass TestPacking(unittest.TestCase):\n\t    \"\"\"\n", "    Test class for packing dataset sequences\n\t    \"\"\"\n\t    def setUp(self) -> None:\n\t        # pylint: disable=duplicate-code\n\t        self.tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n\t        self.tokenizer.add_special_tokens(\n\t            {\n\t                \"bos_token\": \"<s>\",\n\t                \"eos_token\": \"</s>\",\n\t                \"unk_token\": \"<unk>\",\n", "            }\n\t        )\n\t    def test_resets_attention(self):\n\t        prompter = AlpacaPrompter(\"chat\")\n\t        strat = AlpacaPromptTokenizingStrategy(\n\t            prompter,\n\t            self.tokenizer,\n\t            False,\n\t            2048,\n\t        )\n", "        dateset = load_dataset(\n\t            \"json\",\n\t            data_files=str(Path(__file__).parent / \"fixtures/alpaca/alpaca.json\"),\n\t        )[\"train\"]\n\t        dataset = Dataset.from_list(list(TokenizedPromptDataset(strat, dateset)))\n\t        constant_len_dataset = ConstantLengthDataset(\n\t            self.tokenizer,\n\t            [dataset],\n\t            seq_length=2048,\n\t        )\n", "        packed_dataset = Dataset.from_list(list(constant_len_dataset))\n\t        example = packed_dataset[0]\n\t        next_bos_index = (\n\t            example[\"input_ids\"][1:].index(self.tokenizer.bos_token_id) + 1\n\t        )  # add one since we sliced\n\t        # first example doesn't have mask reset\n\t        assert example[\"input_ids\"][0] == self.tokenizer.bos_token_id\n\t        assert example[\"attention_mask\"][0] == 1\n\t        # but subsequent one does\n\t        assert example[\"input_ids\"][next_bos_index] == self.tokenizer.bos_token_id\n", "        assert example[\"attention_mask\"][next_bos_index] == 0\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "tests/test_tokenizers.py", "chunked_list": ["\"\"\"\n\tTest cases for the tokenizer loading\n\t\"\"\"\n\timport unittest\n\tfrom axolotl.utils.dict import DictDefault\n\tfrom axolotl.utils.models import load_tokenizer\n\tclass TestTokenizers(unittest.TestCase):\n\t    \"\"\"\n\t    test class for the load_tokenizer fn\n\t    \"\"\"\n", "    def test_default_use_fast(self):\n\t        cfg = DictDefault({})\n\t        tokenizer = load_tokenizer(\"huggyllama/llama-7b\", None, cfg)\n\t        assert \"Fast\" in tokenizer.__class__.__name__\n\t    def test_dont_use_fast(self):\n\t        cfg = DictDefault(\n\t            {\n\t                \"tokenizer_use_fast\": False,\n\t            }\n\t        )\n", "        tokenizer = load_tokenizer(\"huggyllama/llama-7b\", None, cfg)\n\t        assert \"Fast\" not in tokenizer.__class__.__name__\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "src/axolotl/logging_config.py", "chunked_list": ["\"\"\"Logging configuration settings\"\"\"\n\timport os\n\timport sys\n\tfrom logging.config import dictConfig\n\tfrom typing import Any, Dict\n\tDEFAULT_LOGGING_CONFIG: Dict[str, Any] = {\n\t    \"version\": 1,\n\t    \"formatters\": {\n\t        \"simple\": {\n\t            \"format\": \"[%(asctime)s] [%(levelname)s] [%(name)s.%(funcName)s:%(lineno)d] [PID:%(process)d] %(message)s\",\n", "        },\n\t    },\n\t    \"filters\": {},\n\t    \"handlers\": {\n\t        \"console\": {\n\t            \"class\": \"logging.StreamHandler\",\n\t            \"formatter\": \"simple\",\n\t            \"filters\": [],\n\t            \"stream\": sys.stdout,\n\t        },\n", "    },\n\t    \"root\": {\"handlers\": [\"console\"], \"level\": os.getenv(\"LOG_LEVEL\", \"INFO\")},\n\t    \"loggers\": {\n\t        \"axolotl\": {\"handlers\": [\"console\"], \"level\": \"DEBUG\", \"propagate\": False},\n\t    },\n\t}\n\tdef configure_logging():\n\t    \"\"\"Configure with default logging\"\"\"\n\t    dictConfig(DEFAULT_LOGGING_CONFIG)\n"]}
{"filename": "src/axolotl/prompters.py", "chunked_list": ["\"\"\"Module containing prompters\"\"\"\n\timport dataclasses\n\timport logging\n\tfrom enum import Enum, auto\n\tfrom typing import Generator, List, Optional, Tuple, Union\n\tLOG = logging.getLogger(\"axolotl\")\n\tIGNORE_TOKEN_ID = -100\n\tclass PromptStyle(Enum):\n\t    \"\"\"\n\t    Enum for prompt styles\n", "    \"\"\"\n\t    INSTRUCT = \"instruct\"\n\t    CHAT = \"chat\"\n\tclass AlpacaPrompter:\n\t    \"\"\"\n\t    Base class for alpaca prompters\n\t    \"\"\"\n\t    system_prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n\t    system_no_input_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n\t    turn_format: str\n", "    turn_no_input_format: str\n\t    prompt_style: Optional[PromptStyle] = None\n\t    def __init__(self, prompt_style=PromptStyle.INSTRUCT.value):\n\t        self.prompt_style = prompt_style if prompt_style else PromptStyle.INSTRUCT.value\n\t        self.match_prompt_style()\n\t    def match_prompt_style(self):\n\t        if self.prompt_style == PromptStyle.INSTRUCT.value:\n\t            self.turn_format = \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n\t            self.turn_no_input_format = (\n\t                \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n", "            )\n\t        if self.prompt_style == PromptStyle.CHAT.value:\n\t            self.turn_format = \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n\t            self.turn_no_input_format = \"USER: {instruction}\\nASSISTANT:\"\n\t    def build_prompt(\n\t        self,\n\t        instruction: str,\n\t        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n\t        output: Union[None, str] = None,\n\t    ) -> Generator[str, None, None]:\n", "        # returns the full prompt from instruction and optional input\n\t        # if a label (=response, =output) is provided, it's also appended.\n\t        if input:\n\t            res = self.system_prompt + self.turn_format.format(\n\t                instruction=instruction, input=input\n\t            )\n\t        else:\n\t            res = self.system_no_input_prompt + self.turn_no_input_format.format(\n\t                instruction=instruction\n\t            )\n", "        if output:\n\t            res = f\"{res}{output}\"\n\t        yield res\n\tclass UnpromptedPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Prompter for alpaca no system prompt\n\t    \"\"\"\n\t    system_prompt = \"\"\n\t    system_no_input_prompt = \"\"\n\tclass JeopardyPrompter(AlpacaPrompter):\n", "    \"\"\"\n\t    Prompter for Jeopardy\n\t    \"\"\"\n\t    prompt_input = \"Below is a Jeopardy clue paired with input providing the category of the clue. Write a concise response that best answers tbe clue given the category.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n\tclass MultipleChoiceExplainPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Prompter for multiple choice explain\n\t    \"\"\"\n\t    system_prompt = (\n\t        \"Choose the answer that best answers the question. Explain your reasoning.\\n\"\n", "    )\n\t    system_no_input_prompt = (\n\t        \"Choose the answer that best answers the question. Explain your reasoning.\\n\"\n\t    )\n\tclass MultipleChoiceConcisePrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Prompter for multiple choice concise\n\t    \"\"\"\n\t    system_prompt = \"Choose the answer that best answers the question. Be concise in your response.\\n\\n\"\n\t    system_no_input_prompt = \"Choose the answer that best answers the question. Be concise in your response.\\n\\n\"\n", "    def match_prompt_style(self):\n\t        self.turn_format = \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n\t        self.turn_no_input_format = \"USER: {instruction}\\nASSISTANT:\"\n\tclass SummarizeTLDRPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Prompter for summarize TLDR\n\t    \"\"\"\n\t    system_prompt = \"\"\n\t    system_no_input_prompt = \"\"\n\t    def match_prompt_style(self):\n", "        self.turn_format = \"USER: Summarize the following article as a TL;DR.\\n{instruction}\\n{input}\\nASSISTANT:\"\n\t        self.turn_no_input_format = \"USER: Summarize the following article as a TL;DR.\\n{instruction}\\nASSISTANT:\"\n\tclass CompletionPrompter:\n\t    \"\"\"\n\t    Prompter for completion\n\t    \"\"\"\n\t    def build_prompt(\n\t        self,\n\t        instruction: str,\n\t        input=None,  # pylint: disable=redefined-builtin, unused-argument\n", "        output=None,  # pylint: disable=unused-argument\n\t    ) -> Generator[str, None, None]:\n\t        yield instruction\n\tclass GPTeacherPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Prompter for GPTeacher\n\t    \"\"\"\n\tclass NomicGPT4AllPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Prompter for NomicGPT4All\n", "    \"\"\"\n\tclass ReflectAlpacaPrompter:\n\t    \"\"\"\n\t    Prompter for ReflectAlpaca\n\t    \"\"\"\n\t    system_prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. You, the Assistant, should generate a response as if it were an abstract for an academic or technical paper on the query along with a methodology. Then generate an Agent Reflection where you create a long form response as if from subject matter expert, be verbose, diligent, and creative in your application of knowledge, apply it through the lens of the response generated by the assistant. Look for flawed reasoning, faulty logic, or other mistakes in the method. Finally, generate a final response and method for the user with the Assistant abstract and Reflection analysis as augmentations to the generation\\n\\n\"\n\t    system_no_input_prompt = \"Below is an instruction that describes a task. You, the Assistant, should generate a response as if it were an abstract for an academic or technical paper on the query along with a methodology. Then generate an Agent Reflection where you create a long form response as if from subject matter expert, be verbose, diligent, and creative in your application of knowledge, apply it through the lens of the response generated by the assistant. Look for flawed reasoning, faulty logic, or other mistakes in the method. Finally, generate a final response and method for the user with the Assistant abstract and Reflection analysis as augmentations to the generation\\n\\n\"\n\t    prompt_input = (\n\t        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n\t    )\n", "    prompt_no_input = \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n\t    agent_label = \"### Thought:\\n{output}\\n\\n### Agent Reflection:\\n{reflection}\\n\\n### Final Response:\\n{corrected}\"\n\t    response_split = \"### Response:\"\n\t    def __init__(self, prompt_style=\"instruct\"):\n\t        self.prompt_style = prompt_style\n\t        self.match_prompt_style()\n\t    def match_prompt_style(self):\n\t        if self.prompt_style == PromptStyle.INSTRUCT.value:\n\t            self.prompt_input = (\n\t                self.system_prompt\n", "                + \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n\t            )\n\t            self.prompt_no_input = (\n\t                self.system_no_input_prompt\n\t                + \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n\t            )\n\t            self.agent_label = \"### Thought:\\n{output}\\n\\n### Agent Reflection:\\n{reflection}\\n\\n### Final Response:\\n{corrected}\"\n\t            self.response_split = \"### Final Response:\"\n\t        if self.prompt_style == PromptStyle.CHAT.value:\n\t            self.prompt_input = (\n", "                self.system_prompt + \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n\t            )\n\t            self.prompt_no_input = (\n\t                self.system_no_input_prompt + \"USER: {instruction}\\nASSISTANT:\"\n\t            )\n\t            self.agent_label = (\n\t                \"\\nTHOUGHT: {output}\\nASSISTANT REFLECTION: {reflection}\\nASSISTANT:\"\n\t            )\n\t            self.response_split = \"ASSISTANT:\"\n\t    def build_prompt(\n", "        self,\n\t        instruction: str,\n\t        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n\t        output: Union[None, str] = None,\n\t        reflection: Union[None, str] = None,\n\t        corrected: Union[None, str] = None,\n\t    ) -> Generator[str, None, None]:\n\t        # returns the full prompt from instruction and optional input\n\t        # if a label (=response, =output) is provided, it's also appended.\n\t        if input:\n", "            res = self.prompt_input.format(instruction=instruction, input=input)\n\t        else:\n\t            res = self.prompt_no_input.format(instruction=instruction)\n\t        if output and reflection and corrected:\n\t            label = self.agent_label.format(\n\t                output=output,\n\t                reflection=reflection,\n\t                corrected=corrected,\n\t            )\n\t            res = f\"{res}{label}\"\n", "        yield res\n\tclass SeparatorStyle(Enum):\n\t    \"\"\"Different separator style.\"\"\"\n\t    SINGLE = auto()\n\t    TWO = auto()\n\t    DOLLY = auto()\n\t# TODO clean this 💩 up\n\t@dataclasses.dataclass\n\tclass Conversation:\n\t    \"\"\"A class that keeps all conversation history.\"\"\"\n", "    system: str\n\t    roles: List[str]\n\t    messages: List[List[str]]\n\t    offset: int\n\t    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n\t    sep: str = \"###\"\n\t    sep2: Optional[str] = None\n\t    def get_prompt(self) -> Generator[Tuple[str, str], None, None]:\n\t        # seps = [self.sep, self.sep2]\n\t        preamble = self.system + self.sep\n", "        yield (\"SYSTEM:\", preamble)\n\t        for _, (role, message) in enumerate(self.messages):\n\t            if message:\n\t                yield (role + \":\", \" \" + message)\n\t            else:\n\t                LOG.warning(f\"role with empty message: {role}\")\n\t                yield (role + \":\", \"\")\n\t    def copy(self):\n\t        return Conversation(\n\t            system=self.system,\n", "            roles=self.roles,\n\t            messages=[[x, y] for x, y in self.messages],\n\t            offset=self.offset,\n\t            sep_style=self.sep_style,\n\t            sep=self.sep,\n\t            sep2=self.sep2,\n\t        )\n\t    def append_message(self, role, message):\n\t        self.messages.append([role, message])\n\tclass ShareGPTPrompter:  # pylint: disable=too-few-public-methods\n", "    \"\"\"\n\t    A prompter that generates prompts for the ShareGPT\n\t    \"\"\"\n\t    def __init__(self, prompt_style=None, system_prompt: Optional[str] = None):\n\t        if prompt_style != PromptStyle.CHAT.value:\n\t            raise ValueError(\n\t                f\"unsupported prompt_style for ShareGPTPrompter({prompt_style})\"\n\t            )\n\t        system: str = (\n\t            system_prompt\n", "            if system_prompt\n\t            else (\n\t                \"A chat between a curious user and an artificial intelligence assistant. \"\n\t                \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n\t            )\n\t        )\n\t        self._conversation = Conversation(\n\t            system=system,\n\t            roles=[\"USER\", \"ASSISTANT\"],\n\t            messages=[],\n", "            offset=0,\n\t            sep_style=SeparatorStyle.TWO,\n\t            sep=\" \",\n\t            sep2=\" \",\n\t        )\n\t    def build_prompt(self, source) -> Generator[str, None, None]:\n\t        # ignore the system prompt if provided\n\t        if source[0][\"from\"] == \"system\":\n\t            source.pop(0)\n\t        if len(source) < 2:\n", "            # If there isn't a back and forth conversation, ignore it\n\t            # also happens on the data splitting leaving empty conversations\n\t            raise IndexError\n\t        conv = self._conversation.copy()\n\t        roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\t        try:\n\t            # Apply prompt templates\n\t            if (\n\t                source[0][\"from\"] not in roles\n\t                or roles[source[0][\"from\"]] != conv.roles[0]\n", "            ):\n\t                # Skip the first one if it is not from human\n\t                source = source[1:]\n\t        except IndexError as err:\n\t            # sometimes there is a bing or system chat\n\t            raise err\n\t        conv.messages = []\n\t        for j, sentence in enumerate(source):\n\t            role = roles[sentence[\"from\"]]\n\t            assert role == conv.roles[j % 2]\n", "            conv.append_message(role, sentence[\"value\"])\n\t        for part in conv.get_prompt():\n\t            yield part\n"]}
{"filename": "src/axolotl/convert.py", "chunked_list": ["\"\"\"Module containing File Reader, File Writer, Json Parser, and Jsonl Serializer classes\"\"\"\n\timport json\n\timport sys\n\tclass FileReader:\n\t    \"\"\"\n\t    Reads a file and returns its contents as a string\n\t    \"\"\"\n\t    def read(self, file_path):\n\t        with open(file_path, encoding=\"utf-8\") as file:\n\t            return file.read()\n", "class FileWriter:\n\t    \"\"\"\n\t    Writes a string to a file\n\t    \"\"\"\n\t    def __init__(self, file_path):\n\t        self.file_path = file_path\n\t    def write(self, content):\n\t        with open(self.file_path, \"w\", encoding=\"utf-8\") as file:\n\t            file.write(content)\n\tclass StdoutWriter:\n", "    \"\"\"\n\t    Writes a string to stdout\n\t    \"\"\"\n\t    def write(self, content):\n\t        sys.stdout.write(content)\n\t        sys.stdout.write(\"\\n\")\n\tclass JsonParser:\n\t    \"\"\"\n\t    Parses a string as JSON and returns the result\n\t    \"\"\"\n", "    def parse(self, content):\n\t        return json.loads(content)\n\tclass JsonlSerializer:\n\t    \"\"\"\n\t    Serializes a list of JSON objects into a JSONL string\n\t    \"\"\"\n\t    def serialize(self, data):\n\t        lines = [json.dumps(item) for item in data]\n\t        return \"\\n\".join(lines)\n\tclass JsonToJsonlConverter:\n", "    \"\"\"\n\t    Converts a JSON file to JSONL\n\t    \"\"\"\n\t    def __init__(self, file_reader, file_writer, json_parser, jsonl_serializer):\n\t        self.file_reader = file_reader\n\t        self.file_writer = file_writer\n\t        self.json_parser = json_parser\n\t        self.jsonl_serializer = jsonl_serializer\n\t    def convert(\n\t        self, input_file_path, output_file_path\n", "    ):  # pylint: disable=unused-argument\n\t        content = self.file_reader.read(input_file_path)\n\t        data = self.json_parser.parse(content)\n\t        # data = [r for r in data if r[\"conversations\"]]  # vicuna cleaned has rows with empty conversations\n\t        jsonl_content = self.jsonl_serializer.serialize(data)\n\t        self.file_writer.write(jsonl_content)\n"]}
{"filename": "src/axolotl/__init__.py", "chunked_list": []}
{"filename": "src/axolotl/prompt_tokenizers.py", "chunked_list": ["\"\"\"Module containing PromptTokenizingStrategy and Prompter classes\"\"\"\n\timport abc\n\timport copy\n\timport functools\n\timport logging\n\tfrom typing import Dict, List, Tuple, Union\n\tfrom transformers import PreTrainedTokenizer\n\tfrom axolotl.prompters import IGNORE_TOKEN_ID\n\tLOG = logging.getLogger(\"axolotl\")\n\tIGNORE_INDEX = -100\n", "LLAMA_DEFAULT_PAD_TOKEN = \"[PAD]\"  # nosec\n\tLLAMA_DEFAULT_EOS_TOKEN = \"</s>\"  # nosec\n\tLLAMA_DEFAULT_BOS_TOKEN = \"<s>\"  # nosec\n\tLLAMA_DEFAULT_UNK_TOKEN = \"<unk>\"  # nosec\n\tclass InvalidDataException(Exception):\n\t    \"\"\"\n\t    Exception raised when the data is invalid\n\t    \"\"\"\n\tclass PromptTokenizingStrategy(abc.ABC):\n\t    \"\"\"\n", "    Abstract class for tokenizing strategies\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        prompter,\n\t        tokenizer,\n\t        train_on_inputs: bool = False,\n\t        sequence_len: int = 2048,\n\t    ):\n\t        self.prompter = prompter\n", "        self.tokenizer: PreTrainedTokenizer = tokenizer\n\t        self.train_on_inputs = train_on_inputs\n\t        self.sequence_len = sequence_len\n\t    @abc.abstractmethod\n\t    def tokenize_prompt(self, prompt):\n\t        pass\n\t    @functools.lru_cache(maxsize=128)\n\t    def _get_user_token(self):\n\t        try:\n\t            id_or_ids = self.tokenizer.convert_tokens_to_ids(\"<|USER|>\")\n", "            if isinstance(id_or_ids, (int,)):\n\t                return id_or_ids\n\t        except KeyError:\n\t            pass\n\t        return False\n\t    @functools.lru_cache(maxsize=128)\n\t    def _get_assistant_token(self):\n\t        try:\n\t            id_or_ids = self.tokenizer.convert_tokens_to_ids(\"<|ASSISTANT|>\")\n\t            if isinstance(id_or_ids, (int,)):\n", "                return id_or_ids\n\t        except KeyError:\n\t            pass\n\t        return False\n\t    def _tokenize(self, prompt: str, add_eos_token=True, strip_bos_token=False):\n\t        result = self.tokenizer(\n\t            prompt,\n\t            truncation=True,\n\t            max_length=self.sequence_len,\n\t            padding=False,\n", "            return_tensors=None,\n\t        )\n\t        if (\n\t            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n\t            and len(result[\"input_ids\"]) < self.sequence_len\n\t            and add_eos_token\n\t        ):\n\t            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n\t            result[\"attention_mask\"].append(1)\n\t        if result[\"input_ids\"][0] == self.tokenizer.bos_token_id and strip_bos_token:\n", "            result[\"input_ids\"] = result[\"input_ids\"][1:]\n\t            result[\"attention_mask\"] = result[\"attention_mask\"][1:]\n\t        result[\"labels\"] = result[\"input_ids\"].copy()\n\t        return result\n\tclass InstructionPromptTokenizingStrategy(PromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for instruction-based prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(\n\t        self, prompt\n", "    ) -> Union[Tuple[str, str, str], Tuple[str, str, str, str]]:\n\t        raise NotImplementedError\n\t    def tokenize_prompt(self, prompt):\n\t        (\n\t            instruction,\n\t            input,  # pylint: disable=redefined-builtin\n\t            response,\n\t        ) = self.parse_instruction_fields(prompt)\n\t        user_prompt = next(\n\t            iter(\n", "                self.prompter.build_prompt(\n\t                    instruction,\n\t                    input,\n\t                )\n\t            )\n\t        )\n\t        tokenized_prompt = self._tokenize(user_prompt, add_eos_token=False)\n\t        if not self.train_on_inputs:\n\t            user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n\t            # TODO this could be sped up using numpy array slicing\n", "            tokenized_prompt[\"labels\"] = [-100] * user_prompt_len\n\t        tokenized_res_prompt = self._tokenize(\n\t            response, strip_bos_token=True, add_eos_token=True\n\t        )\n\t        tokenized_prompt[\"input_ids\"] += tokenized_res_prompt[\"input_ids\"]\n\t        tokenized_prompt[\"attention_mask\"] += tokenized_res_prompt[\"attention_mask\"]\n\t        tokenized_prompt[\"labels\"] += tokenized_res_prompt[\"input_ids\"]\n\t        return tokenized_prompt\n\t    def _build_full_prompt(\n\t        self, instruction, input, response  # pylint: disable=redefined-builtin\n", "    ):\n\t        return next(\n\t            iter(\n\t                self.prompter.build_prompt(\n\t                    instruction,\n\t                    input,\n\t                    response,\n\t                )\n\t            )\n\t        )\n", "class AlpacaPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Alpaca prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"instruction\"],\n\t            prompt[\"input\"] if \"input\" in prompt else \"\",\n\t            prompt[\"output\"],\n\t        )\n", "class AlpacaMultipleChoicePromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Alpaca Multiple Choice prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"question\"],\n\t            \"\\n\".join(f'- \"{choice}\"' for choice in prompt[\"choices\"]),\n\t            prompt[\"solution\"] if \"solution\" in prompt else prompt[\"explanation\"],\n\t        )\n", "class JeopardyPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Jeopardy prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"question\"],\n\t            prompt[\"category\"],\n\t            \"what is \" + prompt[\"answer\"],\n\t        )\n", "class OpenAssistantPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for OpenAssistant prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"INSTRUCTION\"],\n\t            \"\",\n\t            prompt[\"RESPONSE\"],\n\t        )\n", "class SummarizeTLDRPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for SummarizeTLDR prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"article\"],\n\t            \"\",\n\t            prompt[\"summary\"],\n\t        )\n", "class GPTeacherPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for GPTeacher prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"instruction\"],\n\t            prompt[\"input\"] if \"input\" in prompt else \"\",\n\t            prompt[\"response\"],\n\t        )\n", "class NomicGPT4AllPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for NomicGPT4All prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"prompt\"],\n\t            \"\",\n\t            prompt[\"response\"],\n\t        )\n", "class CompletionPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Completion prompts.\n\t    \"\"\"\n\t    def tokenize_prompt(self, prompt):\n\t        full_prompt = self._build_full_prompt(prompt[\"text\"], None, None)\n\t        tokenized_full_prompt = self._tokenize(full_prompt)\n\t        return tokenized_full_prompt\n\t    def _build_full_prompt(\n\t        self, instruction, input, response\n", "    ):  # pylint: disable=redefined-builtin\n\t        return next(iter(self.prompter.build_prompt(instruction, input, response)))\n\tclass ReflectionPromptTokenizingStrategy(PromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Reflection prompts.\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str, str]:\n\t        raise NotImplementedError\n\t    def tokenize_prompt(self, prompt):\n\t        (\n", "            instruction,\n\t            input,  # pylint: disable=redefined-builtin\n\t            output,\n\t            reflection,\n\t            corrected,\n\t        ) = self.parse_instruction_fields(prompt)\n\t        full_prompt = self._build_full_prompt(\n\t            instruction, input, output, reflection, corrected\n\t        )\n\t        tokenized_full_prompt = self._tokenize(full_prompt)\n", "        if not self.train_on_inputs:\n\t            user_prompt = next(\n\t                iter(\n\t                    self.prompter.build_prompt(\n\t                        instruction,\n\t                        input,\n\t                    )\n\t                )\n\t            )\n\t            tokenized_user_prompt = self._tokenize(user_prompt, add_eos_token=False)\n", "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\t            # TODO this could be sped up using numpy array slicing\n\t            tokenized_full_prompt[\"labels\"] = [\n\t                -100\n\t            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n\t        return tokenized_full_prompt\n\t    def _build_full_prompt(\n\t        self, instruction, input, output, reflection, corrected\n\t    ):  # pylint: disable=redefined-builtin\n\t        return next(\n", "            iter(\n\t                self.prompter.build_prompt(\n\t                    instruction,\n\t                    input,\n\t                    output,\n\t                    reflection,\n\t                    corrected,\n\t                )\n\t            )\n\t        )\n", "    def _tokenize(self, prompt, add_eos_token=True, strip_bos_token=False):\n\t        result = self.tokenizer(\n\t            prompt,\n\t            truncation=True,\n\t            max_length=self.sequence_len,\n\t            padding=False,\n\t            return_tensors=None,\n\t        )\n\t        if (\n\t            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n", "            and len(result[\"input_ids\"]) < self.sequence_len\n\t            and add_eos_token\n\t        ):\n\t            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n\t            result[\"attention_mask\"].append(1)\n\t        result[\"labels\"] = result[\"input_ids\"].copy()\n\t        return result\n\tclass AlpacaReflectionPTStrategy(ReflectionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Alpaca Reflection prompts.\n", "    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str, str]:\n\t        return (\n\t            prompt[\"instruction\"],\n\t            prompt[\"input\"] if \"input\" in prompt else \"\",\n\t            prompt[\"output\"],\n\t            prompt[\"reflection\"],\n\t            prompt[\"corrected\"],\n\t        )\n\tclass ShareGPTPromptTokenizingStrategy(PromptTokenizingStrategy):\n", "    \"\"\"\n\t    Tokenizing strategy for ShareGPT prompts.\n\t    \"\"\"\n\t    def get_conversation_thread(self, prompt):\n\t        return prompt[\"conversations\"]\n\t    def tokenize_prompt(self, prompt):\n\t        result, current_len = tokenize_prompt_default()\n\t        user_token = self._get_user_token()\n\t        assistant_token = self._get_assistant_token()\n\t        try:\n", "            for _, part in enumerate(\n\t                self.prompter.build_prompt(self.get_conversation_thread(prompt))\n\t            ):\n\t                if isinstance(part, tuple):\n\t                    if part[0] == \"USER:\":\n\t                        part = part[0] + part[1] if not user_token else part[1]\n\t                        # this is still the user query, we should\n\t                        res = self._tokenize(\n\t                            part.strip(),\n\t                            add_eos_token=False,\n", "                            strip_bos_token=True,\n\t                        )\n\t                        if user_token:\n\t                            res[\"input_ids\"] = [user_token, *res[\"input_ids\"]]\n\t                        # everything from this is masked out from the labels\n\t                        labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n\t                    elif part[0] == \"ASSISTANT:\":\n\t                        # TODO label assistant token/tokens w/ IGNORE_TOKEN_ID\n\t                        part = part[0] + part[1] if not assistant_token else part[1]\n\t                        # this should be the assistent response, should end with an eos token\n", "                        res = self._tokenize(\n\t                            part.strip(),\n\t                            add_eos_token=True,\n\t                            strip_bos_token=True,\n\t                        )\n\t                        if assistant_token:\n\t                            res[\"input_ids\"] = [\n\t                                assistant_token,\n\t                                *res[\"input_ids\"],\n\t                            ]\n", "                        # not masked out from labels\n\t                        labels = copy.deepcopy(res[\"input_ids\"])\n\t                    elif part[0] == \"SYSTEM:\":\n\t                        part = part[1]  # Ignore the system role from preamble\n\t                        # this is only ever the first part, should include the bos token and the user query\n\t                        res = self._tokenize(\n\t                            part.strip(), add_eos_token=False, strip_bos_token=False\n\t                        )\n\t                        # everything from this is masked out from the labels\n\t                        labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n", "                    else:\n\t                        LOG.warning(f\"unhandled role: {part[0]}\")\n\t                # pylint: disable=duplicate-code\n\t                result, current_len = parse_tokenized_to_result(\n\t                    result,\n\t                    current_len,\n\t                    res,\n\t                    labels,\n\t                    pad_token_id=self.tokenizer.pad_token_id,\n\t                )\n", "            return result\n\t        except (KeyError, AssertionError, IndexError) as err:\n\t            raise InvalidDataException(str(err)) from err\n\t    def _tokenize(self, prompt, add_eos_token=True, strip_bos_token=False):\n\t        result = self.tokenizer(\n\t            prompt,\n\t            truncation=True,\n\t            max_length=self.sequence_len,\n\t            padding=False,\n\t            return_tensors=None,\n", "        )\n\t        if (\n\t            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n\t            and len(result[\"input_ids\"]) < self.sequence_len\n\t            and add_eos_token\n\t        ):\n\t            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n\t            result[\"attention_mask\"].append(1)\n\t        if result[\"input_ids\"][0] == self.tokenizer.bos_token_id and strip_bos_token:\n\t            result[\"input_ids\"] = result[\"input_ids\"][1:]\n", "            result[\"attention_mask\"] = result[\"attention_mask\"][1:]\n\t        result[\"labels\"] = result[\"input_ids\"].copy()\n\t        return result\n\tdef tokenize_prompt_default() -> Tuple[Dict[str, List[int]], int]:\n\t    \"\"\"\n\t    Returns the default values for the tokenize prompt function\n\t    \"\"\"\n\t    result: Dict[str, List[int]] = {\n\t        \"input_ids\": [],\n\t        \"attention_mask\": [],\n", "        \"labels\": [],\n\t    }\n\t    current_len = 0\n\t    return result, current_len\n\tdef parse_tokenized_to_result(\n\t    result: Dict[str, List[int]],\n\t    current_len: int,\n\t    res: Dict[str, List[int]],\n\t    labels: List[int],\n\t    pad_token_id: Union[int, None] = None,\n", ") -> Tuple[Dict[str, List[int]], int]:\n\t    \"\"\"\n\t    Parses the tokenized prompt and append the tokenized input_ids, attention_mask and labels to the result\n\t    \"\"\"\n\t    input_ids = res[\"input_ids\"]\n\t    input_len = len(input_ids)\n\t    result[\"input_ids\"][current_len : current_len + input_len] = input_ids\n\t    result[\"attention_mask\"][current_len : current_len + input_len] = [\n\t        1 if x != pad_token_id else 0 for x in input_ids\n\t    ]\n", "    result[\"labels\"][current_len : current_len + input_len] = labels\n\t    current_len += input_len\n\t    return result, current_len\n"]}
{"filename": "src/axolotl/datasets.py", "chunked_list": ["\"\"\"Module containing Dataset functionality\"\"\"\n\timport logging\n\timport os\n\tfrom typing import List\n\timport torch\n\tfrom datasets import IterableDataset\n\tfrom .prompt_tokenizers import PromptTokenizingStrategy\n\t# We want this to be a wrapper for an existing dataset that we have loaded\n\t# lets use the concept of middlewares to wrap each dataset, for example\n\t# ConstantLengthDataset(ShuffledDataset([TokenizedPromptDataset(alpaca_dataset)]))\n", "# let's check to ensure we don't truncate an item in the middle, we'll use\n\t# the collators later on to pad the datasets\n\tLOG = logging.getLogger(\"axolotl\")\n\tclass TokenizedPromptDataset(IterableDataset):\n\t    \"\"\"\n\t    Iterable dataset that returns tokenized prompts from a stream of text files.\n\t        Args:\n\t            prompt_tokenizer (PromptTokenizingStrategy): The prompt tokenizing method for proccessing the data.\n\t            dataset (dataset.Dataset): Dataset with text files.\n\t    \"\"\"\n", "    def __init__(  # pylint: disable=super-init-not-called\n\t        self,\n\t        prompt_tokenizer: PromptTokenizingStrategy,\n\t        dataset: IterableDataset,\n\t    ):\n\t        self.prompt_tokenizer = prompt_tokenizer\n\t        self.dataset = dataset\n\t    def __iter__(self):\n\t        features = self.dataset.features.keys()\n\t        num_proc = os.cpu_count()\n", "        return iter(\n\t            self.dataset.map(\n\t                self.prompt_tokenizer.tokenize_prompt,\n\t                num_proc=num_proc,\n\t                remove_columns=features,\n\t            )\n\t        )\n\t# TODO this isn't the best since it can't interleave datasets\n\tclass ConstantLengthDataset(IterableDataset):\n\t    \"\"\"\n", "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n\t        Args:\n\t            tokenizer (Tokenizer): The processor used for proccessing the data.\n\t            dataset (dataset.Dataset): Dataset with text files.\n\t            seq_length (int): Length of token sequences to return.\n\t    \"\"\"\n\t    def __init__(  # pylint: disable=super-init-not-called\n\t        self,\n\t        tokenizer,\n\t        datasets,\n", "        seq_length=2048,\n\t    ):\n\t        self.tokenizer = tokenizer\n\t        self.concat_token_id = tokenizer.eos_token_id\n\t        self.datasets: List[IterableDataset] = datasets\n\t        self.seq_length = seq_length\n\t        vocab_size = len(tokenizer.get_vocab())\n\t        if vocab_size <= torch.iinfo(torch.int16).max:\n\t            self.tokens_dtype = torch.int16\n\t        elif vocab_size <= torch.iinfo(torch.int32).max:\n", "            self.tokens_dtype = torch.int32\n\t        else:\n\t            self.tokens_dtype = torch.int64\n\t    def __iter__(self):\n\t        buffer = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n\t        buffer_len = 0\n\t        for dataset in self.datasets:\n\t            iterator = iter(dataset)\n\t            more_examples = True\n\t            while more_examples:\n", "                try:\n\t                    example = next(iterator)\n\t                except StopIteration:\n\t                    more_examples = False\n\t                    example = None\n\t                add_concat_token = False\n\t                if example:\n\t                    example_len = len(example[\"input_ids\"])\n\t                    add_concat_token = example[\"input_ids\"][-1] != self.concat_token_id\n\t                else:\n", "                    example_len = 0\n\t                if not example_len or (\n\t                    buffer_len + int(add_concat_token) + example_len > self.seq_length\n\t                ):\n\t                    if buffer[\"input_ids\"]:\n\t                        input_ids = torch.cat(buffer[\"input_ids\"], dim=-1)[\n\t                            : self.seq_length\n\t                        ]\n\t                        attention_mask = torch.cat(buffer[\"attention_mask\"], dim=-1)[\n\t                            : self.seq_length\n", "                        ]\n\t                        labels = torch.cat(buffer[\"labels\"], dim=-1)[: self.seq_length]\n\t                        if labels.size() == input_ids.size() and (\n\t                            attention_mask.size() == input_ids.size()\n\t                        ):\n\t                            yield {\n\t                                \"input_ids\": input_ids,\n\t                                \"labels\": labels,\n\t                                \"attention_mask\": attention_mask,\n\t                            }\n", "                        else:\n\t                            LOG.warning(\n\t                                f\"dropping batch due to tensor size mismatch input_ids: {input_ids.size()}, labels: {labels.size()}, attention_mask: {attention_mask.size()}\"\n\t                            )\n\t                    buffer = {\n\t                        \"input_ids\": [],\n\t                        \"attention_mask\": [],\n\t                        \"labels\": [],\n\t                    }\n\t                    buffer_len = 0\n", "                if example:\n\t                    # FIXME\n\t                    # just going to drop data points that are too long\n\t                    if len(example[\"input_ids\"]) <= self.seq_length:\n\t                        input_ids = example[\"input_ids\"]\n\t                        attention_mask = example[\"attention_mask\"]\n\t                        labels = example[\"labels\"]\n\t                        if (\n\t                            buffer[\"input_ids\"]\n\t                            and input_ids[0] == self.tokenizer.bos_token_id\n", "                        ):\n\t                            attention_mask[0] = 0\n\t                        if add_concat_token:\n\t                            input_ids.append(self.concat_token_id)\n\t                            attention_mask.append(1)\n\t                            labels.append(self.concat_token_id)\n\t                        input_ids_with_concat = torch.tensor(\n\t                            input_ids, dtype=self.tokens_dtype\n\t                        )\n\t                        attention_mask_with_concat = torch.tensor(\n", "                            attention_mask, dtype=self.tokens_dtype\n\t                        )\n\t                        labels_with_concat = torch.tensor(\n\t                            labels, dtype=self.tokens_dtype\n\t                        )\n\t                        buffer[\"input_ids\"].append(input_ids_with_concat)\n\t                        buffer[\"attention_mask\"].append(attention_mask_with_concat)\n\t                        buffer[\"labels\"].append(labels_with_concat)\n\t                        buffer_len += len(input_ids)\n"]}
{"filename": "src/axolotl/flash_attn.py", "chunked_list": ["\"\"\"Flash attention monkey patch for llama model\"\"\"\n\t# copied from https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py\n\tfrom typing import Optional, Tuple\n\timport torch\n\timport transformers\n\tfrom einops import rearrange\n\tfrom flash_attn.bert_padding import pad_input, unpad_input\n\tfrom flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\n\tfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\tdef forward(\n", "    self,\n\t    hidden_states: torch.Tensor,\n\t    attention_mask: Optional[torch.Tensor] = None,\n\t    position_ids: Optional[torch.Tensor] = None,\n\t    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t    output_attentions: bool = False,\n\t    use_cache: bool = False,\n\t) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\t    \"\"\"Input shape: Batch x Time x Channel\n\t    attention_mask: [bsz, q_len]\n", "    \"\"\"\n\t    # pylint: disable=duplicate-code\n\t    bsz, q_len, _ = hidden_states.size()\n\t    query_states = (\n\t        self.q_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n\t    key_states = (\n\t        self.k_proj(hidden_states)\n", "        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n\t    value_states = (\n\t        self.v_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n\t    # [bsz, q_len, nh, hd]\n\t    # [bsz, nh, q_len, hd]\n", "    kv_seq_len = key_states.shape[-2]\n\t    assert past_key_value is None, \"past_key_value is not supported\"\n\t    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t    query_states, key_states = apply_rotary_pos_emb(\n\t        query_states, key_states, cos, sin, position_ids\n\t    )\n\t    # [bsz, nh, t, hd]\n\t    assert not output_attentions, \"output_attentions is not supported\"\n\t    assert not use_cache, \"use_cache is not supported\"\n\t    # Flash attention codes from\n", "    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\t    # transform the data into the format required by flash attention\n\t    qkv = torch.stack(\n\t        [query_states, key_states, value_states], dim=2\n\t    )  # [bsz, nh, 3, q_len, hd]\n\t    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n\t    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n\t    # the attention_mask should be the same as the key_padding_mask\n\t    key_padding_mask = attention_mask\n\t    if key_padding_mask is None:\n", "        qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n\t        max_s = q_len\n\t        cu_q_lens = torch.arange(\n\t            0,\n\t            (bsz + 1) * q_len,\n\t            step=q_len,\n\t            dtype=torch.int32,\n\t            device=qkv.device,\n\t        )\n\t        output = flash_attn_varlen_qkvpacked_func(\n", "            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n\t    else:\n\t        nheads = qkv.shape[-2]\n\t        # pylint: disable=invalid-name\n\t        x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n\t        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n\t        x_unpad = rearrange(\n\t            x_unpad,\n", "            \"nnz (three h d) -> nnz three h d\",\n\t            three=3,\n\t            h=nheads,\n\t        )\n\t        output_unpad = flash_attn_varlen_qkvpacked_func(\n\t            x_unpad,\n\t            cu_q_lens,\n\t            max_s,\n\t            0.0,\n\t            softmax_scale=None,\n", "            causal=True,\n\t        )\n\t        output = rearrange(\n\t            pad_input(\n\t                rearrange(output_unpad, \"nnz h d -> nnz (h d)\"),\n\t                indices,\n\t                bsz,\n\t                q_len,\n\t            ),\n\t            \"b s (h d) -> b s h d\",\n", "            h=nheads,\n\t        )\n\t    return (\n\t        self.o_proj(rearrange(output, \"b s h d -> b s (h d)\")),\n\t        None,\n\t        None,\n\t    )\n\t# Disable the transformation of the attention mask in LlamaModel as the flash attention\n\t# requires the attention mask to be the same as the key_padding_mask\n\tdef _prepare_decoder_attention_mask(\n", "    self,\n\t    attention_mask,\n\t    input_shape,\n\t    inputs_embeds,\n\t    past_key_values_length,\n\t):  # pylint: disable=unused-argument\n\t    # [bsz, seq_len]\n\t    return attention_mask\n\tdef replace_llama_attn_with_flash_attn():\n\t    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (  # pylint: disable=protected-access\n", "        _prepare_decoder_attention_mask\n\t    )\n\t    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "src/axolotl/monkeypatch/llama_attn_hijack_xformers.py", "chunked_list": ["\"\"\"\n\tDirectly copied the code from https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/modules/llama_attn_hijack.py and made some adjustments\n\t\"\"\"\n\timport logging\n\timport math\n\tfrom typing import Optional, Tuple\n\timport torch\n\timport transformers.models.llama.modeling_llama\n\tfrom torch import nn\n\ttry:\n", "    import xformers.ops\n\texcept ImportError:\n\t    logging.error(\"xformers not found! Please install it before trying to use it.\")\n\tdef hijack_llama_attention():\n\t    transformers.models.llama.modeling_llama.LlamaAttention.forward = xformers_forward\n\tdef hijack_llama_sdp_attention():\n\t    transformers.models.llama.modeling_llama.LlamaAttention.forward = (\n\t        sdp_attention_forward\n\t    )\n\tdef xformers_forward(\n", "    self,\n\t    hidden_states: torch.Tensor,\n\t    attention_mask: Optional[torch.Tensor] = None,\n\t    position_ids: Optional[torch.LongTensor] = None,\n\t    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t    output_attentions: bool = False,\n\t    use_cache: bool = False,\n\t) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\t    # pylint: disable=duplicate-code\n\t    bsz, q_len, _ = hidden_states.size()\n", "    query_states = (\n\t        self.q_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n\t    key_states = (\n\t        self.k_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n", "    value_states = (\n\t        self.v_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n\t    kv_seq_len = key_states.shape[-2]\n\t    if past_key_value is not None:\n\t        kv_seq_len += past_key_value[0].shape[-2]\n\t    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t    (\n", "        query_states,\n\t        key_states,\n\t    ) = transformers.models.llama.modeling_llama.apply_rotary_pos_emb(\n\t        query_states, key_states, cos, sin, position_ids\n\t    )\n\t    # [bsz, nh, t, hd]\n\t    if past_key_value is not None:\n\t        # reuse k, v, self_attention\n\t        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n\t        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n", "    past_key_value = (key_states, value_states) if use_cache else None\n\t    # We only apply xformers optimizations if we don't need to output the whole attention matrix\n\t    if not output_attentions:\n\t        query_states = query_states.transpose(1, 2)\n\t        key_states = key_states.transpose(1, 2)\n\t        value_states = value_states.transpose(1, 2)\n\t        # This is a nasty hack. We know attention_mask in transformers is either LowerTriangular or all Zeros.\n\t        # We therefore check if one element in the upper triangular portion is zero. If it is, then the mask is all zeros.\n\t        if attention_mask is None or attention_mask[0, 0, 0, 1] == 0:\n\t            # input and output should be of form (bsz, q_len, num_heads, head_dim)\n", "            attn_output = xformers.ops.memory_efficient_attention(\n\t                query_states, key_states, value_states, attn_bias=None\n\t            )\n\t        else:\n\t            # input and output should be of form (bsz, q_len, num_heads, head_dim)\n\t            attn_output = xformers.ops.memory_efficient_attention(\n\t                query_states,\n\t                key_states,\n\t                value_states,\n\t                attn_bias=xformers.ops.LowerTriangularMask(),\n", "            )\n\t        attn_weights = None\n\t    else:\n\t        attn_weights = torch.matmul(\n\t            query_states, key_states.transpose(2, 3)\n\t        ) / math.sqrt(self.head_dim)\n\t        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n\t            raise ValueError(\n\t                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n\t                f\" {attn_weights.size()}\"\n", "            )\n\t        if attention_mask is not None:\n\t            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n\t                raise ValueError(\n\t                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n\t                )\n\t            attn_weights = attn_weights + attention_mask\n\t            attn_weights = torch.max(\n\t                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n\t            )\n", "        # upcast attention to fp32\n\t        attn_weights = nn.functional.softmax(\n\t            attn_weights, dim=-1, dtype=torch.float32\n\t        ).to(query_states.dtype)\n\t        attn_output = torch.matmul(attn_weights, value_states)\n\t        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n\t            raise ValueError(\n\t                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n\t                f\" {attn_output.size()}\"\n\t            )\n", "        attn_output = attn_output.transpose(1, 2)\n\t    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\t    attn_output = self.o_proj(attn_output)\n\t    return attn_output, attn_weights, past_key_value\n\tdef sdp_attention_forward(\n\t    self,\n\t    hidden_states: torch.Tensor,\n\t    attention_mask: Optional[torch.Tensor] = None,\n\t    position_ids: Optional[torch.LongTensor] = None,\n\t    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n", "    output_attentions: bool = False,\n\t    use_cache: bool = False,\n\t) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\t    # pylint: disable=duplicate-code\n\t    bsz, q_len, _ = hidden_states.size()\n\t    query_states = (\n\t        self.q_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n", "    key_states = (\n\t        self.k_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n\t    value_states = (\n\t        self.v_proj(hidden_states)\n\t        .view(bsz, q_len, self.num_heads, self.head_dim)\n\t        .transpose(1, 2)\n\t    )\n", "    kv_seq_len = key_states.shape[-2]\n\t    if past_key_value is not None:\n\t        kv_seq_len += past_key_value[0].shape[-2]\n\t    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t    (\n\t        query_states,\n\t        key_states,\n\t    ) = transformers.models.llama.modeling_llama.apply_rotary_pos_emb(\n\t        query_states, key_states, cos, sin, position_ids\n\t    )\n", "    # [bsz, nh, t, hd]\n\t    if past_key_value is not None:\n\t        # reuse k, v, self_attention\n\t        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n\t        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\t    past_key_value = (key_states, value_states) if use_cache else None\n\t    # We only apply sdp attention if we don't need to output the whole attention matrix\n\t    if not output_attentions:\n\t        with torch.backends.cuda.sdp_kernel():\n\t            attn_output = torch.nn.functional.scaled_dot_product_attention(\n", "                query_states,\n\t                key_states,\n\t                value_states,\n\t                attn_mask=attention_mask,\n\t                is_causal=False,\n\t            )\n\t            attn_weights = None\n\t    else:\n\t        attn_weights = torch.matmul(\n\t            query_states, key_states.transpose(2, 3)\n", "        ) / math.sqrt(self.head_dim)\n\t        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n\t            raise ValueError(\n\t                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n\t                f\" {attn_weights.size()}\"\n\t            )\n\t        if attention_mask is not None:\n\t            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n\t                raise ValueError(\n\t                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n", "                )\n\t            attn_weights = attn_weights + attention_mask\n\t            attn_weights = torch.max(\n\t                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n\t            )\n\t        # upcast attention to fp32\n\t        attn_weights = nn.functional.softmax(\n\t            attn_weights, dim=-1, dtype=torch.float32\n\t        ).to(query_states.dtype)\n\t        attn_output = torch.matmul(attn_weights, value_states)\n", "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n\t            raise ValueError(\n\t                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n\t                f\" {attn_output.size()}\"\n\t            )\n\t    attn_output = attn_output.transpose(1, 2)\n\t    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\t    attn_output = self.o_proj(attn_output)\n\t    return attn_output, attn_weights, past_key_value\n"]}
{"filename": "src/axolotl/monkeypatch/xpos_rope_llama_monkey_patch.py", "chunked_list": ["# pylint: skip-file\n\t\"\"\"\n\tCopied from https://github.com/kaiokendev/cutoff-len-is-context-len/blob/main/util/xpos_rope_llama_monkey_patch.py\n\t\"\"\"\n\timport torch\n\timport transformers\n\timport transformers.models.llama.modeling_llama\n\tfrom einops import rearrange\n\tclass XposRotaryEmbedding(torch.nn.Module):\n\t    def __init__(\n", "        self,\n\t        dim,\n\t        max_position_embeddings=2048,\n\t        base=10000,\n\t        device=None,\n\t        scale_base=2048,\n\t        use_xpos=True,\n\t    ):\n\t        super().__init__()\n\t        self.max_seq_len_cached = max_position_embeddings\n", "        self.scale_base = scale_base\n\t        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n\t        t = torch.arange(self.max_seq_len_cached, device=device).type_as(inv_freq)\n\t        freqs = torch.einsum(\"i , j -> i j\", t, inv_freq)\n\t        freqs = torch.cat((freqs, freqs), dim=-1)\n\t        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\t        self.register_buffer(\"freqs_cached\", freqs, persistent=False)\n\t        if not use_xpos:\n\t            self.register_buffer(\"scale\", None)\n\t            self.register_buffer(\"scale_cached\", torch.ones(1))\n", "            return\n\t        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n\t        power = (t - (self.max_seq_len_cached // 2)) / self.scale_base\n\t        scale_cached = scale ** rearrange(power, \"n -> n 1\")\n\t        scale_cached = torch.cat((scale_cached, scale_cached), dim=-1)\n\t        self.register_buffer(\"scale\", scale, persistent=False)\n\t        self.register_buffer(\"scale_cached\", scale_cached, persistent=False)\n\t    def forward(\n\t        self,\n\t        x,\n", "        seq_len,\n\t    ):\n\t        if seq_len > self.max_seq_len_cached:\n\t            self.max_seq_len_cached = seq_len\n\t            t = torch.arange(self.max_seq_len_cached, device=x.device).type_as(\n\t                self.inv_freq\n\t            )\n\t            freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n\t            freqs = torch.cat((freqs, freqs), dim=-1).to(dtype=x.dtype)\n\t            self.register_buffer(\"freqs_cached\", freqs)\n", "            if self.scale is None:\n\t                self.register_buffer(\n\t                    \"scale_cached\", torch.ones(1, device=x.device).to(dtype=x.dtype)\n\t                )\n\t                return self.freqs_cached.to(dtype=x.dtype), self.scale_cached\n\t            power = (t - (seq_len // 2)) / self.scale_base\n\t            scale = self.scale ** rearrange(power, \"n -> n 1\")\n\t            scale = torch.cat((scale, scale), dim=-1).to(dtype=x.dtype)\n\t            self.register_buffer(\"scale_cached\", scale)\n\t        return self.freqs_cached.to(dtype=x.dtype), self.scale_cached.to(dtype=x.dtype)\n", "def rotate_half(x):\n\t    x1, x2 = x.chunk(2, dim=-1)\n\t    return torch.cat((-x2, x1), dim=-1)\n\tdef apply_rotary_pos_emb(q, k, freqs, scale=1, position_ids=None):\n\t    freqs = freqs[position_ids, :]\n\t    if scale.shape[-1] != 1:\n\t        scale = scale[position_ids, :]\n\t    q_embed = (q * freqs.cos() * scale) + (rotate_half(q) * freqs.sin() * scale)\n\t    k_embed = (k * freqs.cos() * 1 / scale) + (rotate_half(k) * freqs.sin() * 1 / scale)\n\t    return q_embed, k_embed\n", "def replace_llama_rope_with_xpos_rope():\n\t    transformers.models.llama.modeling_llama.LlamaRotaryEmbedding = XposRotaryEmbedding\n\t    transformers.models.llama.modeling_llama.apply_rotary_pos_emb = apply_rotary_pos_emb\n"]}
{"filename": "src/axolotl/monkeypatch/llama_landmark_attn.py", "chunked_list": ["# pylint: skip-file\n\t# coding=utf-8\n\t# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n\t#\n\t# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n\t# and OPT implementations in this library. It has been modified from its\n\t# original forms to accommodate minor architectural differences compared\n\t# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n", "# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n", "\"\"\"\n\tPyTorch LLaMA model.\n\tTaken from https://github.com/epfml/landmark-attention/blob/main/llama/llama_mem.py and modified.\n\t\"\"\"\n\timport math\n\tfrom typing import List, Optional, Tuple, Union\n\timport torch\n\timport torch.utils.checkpoint\n\tfrom torch import nn\n\tfrom torch.nn import CrossEntropyLoss\n", "from transformers import LlamaTokenizer\n\tfrom transformers.modeling_outputs import (\n\t    BaseModelOutputWithPast,\n\t    CausalLMOutputWithPast,\n\t)\n\tfrom transformers.models.llama.configuration_llama import LlamaConfig\n\tfrom transformers.models.llama.modeling_llama import (\n\t    LLAMA_INPUTS_DOCSTRING,\n\t    LLAMA_START_DOCSTRING,\n\t    LlamaMLP,\n", "    LlamaPreTrainedModel,\n\t    LlamaRMSNorm,\n\t    LlamaRotaryEmbedding,\n\t    _expand_mask,\n\t    _make_causal_mask,\n\t    rotate_half,\n\t)\n\tfrom transformers.utils import (\n\t    add_start_docstrings,\n\t    add_start_docstrings_to_model_forward,\n", "    logging,\n\t    replace_return_docstrings,\n\t)\n\tLOG = logging.getLogger(\"axolotl\")\n\t_CONFIG_FOR_DOC = \"LlamaConfig\"\n\tMEM_TOKEN = \"<landmark>\"  # nosec\n\tdef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n\t    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n\t    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n\t    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n", "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n\t    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n\t    if q is None:\n\t        q_embed = None\n\t    else:\n\t        q_embed = (q * cos) + (rotate_half(q) * sin)\n\t    k_embed = (k * cos) + (rotate_half(k) * sin)\n\t    return q_embed, k_embed\n\tclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n\t    \"\"\"\n", "    Landmark grouped softmax function.\n\t    \"\"\"\n\t    # Note that forward, setup_context, and backward are @staticmethods\n\t    @staticmethod\n\t    def forward(ctx, x, dim, mem_cnt, resp_mem_idx):\n\t        new_shape = list(x.shape)\n\t        new_shape[dim] = mem_cnt  # max_mem_cnt.item()\n\t        max_by_group = x.new_zeros((*new_shape,))\n\t        max_by_group.scatter_reduce_(\n\t            src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False\n", "        )\n\t        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n\t        # x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n\t        x_exp = torch.exp((x - maxes).to(torch.float32))\n\t        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\t        cumsum_by_group.scatter_add_(\n\t            dim,\n\t            resp_mem_idx,\n\t            x_exp,\n\t        )\n", "        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\t        # probs = torch.where(denom < 0.5, 0, x_exp / denom)\n\t        probs = x_exp / denom\n\t        ctx.mem_cnt = mem_cnt\n\t        ctx.dim = dim\n\t        ctx.save_for_backward(resp_mem_idx, probs)\n\t        return probs\n\t    @staticmethod\n\t    def backward(ctx, grad_probs):\n\t        mem_cnt = ctx.mem_cnt\n", "        dim = ctx.dim\n\t        resp_mem_idx, probs = ctx.saved_tensors\n\t        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = None\n\t        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n\t            grad_pair = grad_probs * probs\n\t            new_shape = list(probs.shape)\n\t            new_shape[dim] = mem_cnt  # max_mem_cnt.item()\n\t            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n\t            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\t        if ctx.needs_input_grad[0]:\n", "            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\t            grad_x = grad_pair - probs * grad_sum\n\t        assert not ctx.needs_input_grad[1]\n\t        assert not ctx.needs_input_grad[2]\n\t        assert not ctx.needs_input_grad[3]\n\t        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx\n\tdef landmark_grouped_softmax(x, dim, is_mem, last_section_mask):\n\t    last_and_rest_mask = last_section_mask  # | mask\n\t    full_access_mask = is_mem | last_and_rest_mask\n\t    max_mem_cnt = 16\n", "    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n\t    mem_bucket_id = max_mem_cnt - 1\n\t    resp_mem_idx = torch.where(\n\t        last_and_rest_mask,\n\t        max_mem_cnt - 1,\n\t        torch.where(is_mem, mem_bucket_id, mem_group_idx),\n\t    )\n\t    probs = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx)\n\t    new_shape = list(x.shape)\n\t    new_shape[dim] = max_mem_cnt\n", "    group_prob = probs.new_zeros((*new_shape,))\n\t    group_prob.scatter_(\n\t        dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs\n\t    )\n\t    probs = probs.mul(\n\t        torch.where(\n\t            full_access_mask,\n\t            last_section_mask,\n\t            torch.gather(group_prob, dim, resp_mem_idx),\n\t        )\n", "    )\n\t    return probs\n\tclass LlamaAttention(nn.Module):\n\t    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\t    def __init__(self, config: LlamaConfig):\n\t        super().__init__()\n\t        self.config = config\n\t        self.hidden_size = config.hidden_size\n\t        self.num_heads = config.num_attention_heads\n\t        self.head_dim = self.hidden_size // self.num_heads\n", "        self.max_position_embeddings = config.max_position_embeddings\n\t        if (self.head_dim * self.num_heads) != self.hidden_size:\n\t            raise ValueError(\n\t                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n\t                f\" and `num_heads`: {self.num_heads}).\"\n\t            )\n\t        self.q_proj = nn.Linear(\n\t            self.hidden_size, self.num_heads * self.head_dim, bias=False\n\t        )\n\t        self.k_proj = nn.Linear(\n", "            self.hidden_size, self.num_heads * self.head_dim, bias=False\n\t        )\n\t        self.v_proj = nn.Linear(\n\t            self.hidden_size, self.num_heads * self.head_dim, bias=False\n\t        )\n\t        self.o_proj = nn.Linear(\n\t            self.num_heads * self.head_dim, self.hidden_size, bias=False\n\t        )\n\t        self.rotary_emb = LlamaRotaryEmbedding(\n\t            self.head_dim, max_position_embeddings=self.max_position_embeddings\n", "        )\n\t        self.mem_freq = None\n\t        self.top_k = None\n\t        self.max_cache_size = None\n\t    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n\t        return (\n\t            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n\t            .transpose(1, 2)\n\t            .contiguous()\n\t        )\n", "    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n\t        self.mem_freq = mem_freq\n\t        self.top_k = top_k\n\t        self.max_cache_size = max_cache_size\n\t    def forward(\n\t        self,\n\t        hidden_states: torch.Tensor,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n", "        output_attentions: bool = False,\n\t        use_cache: bool = False,\n\t        is_mem: Optional[torch.Tensor] = None,\n\t        last_section_mask: Optional[torch.Tensor] = None,\n\t        offload_cache_to_cpu: bool = False,\n\t    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\t        bsz, q_len, _ = hidden_states.size()\n\t        query_states = (\n\t            self.q_proj(hidden_states)\n\t            .view(bsz, q_len, self.num_heads, self.head_dim)\n", "            .transpose(1, 2)\n\t        )\n\t        key_states = (\n\t            self.k_proj(hidden_states)\n\t            .view(bsz, q_len, self.num_heads, self.head_dim)\n\t            .transpose(1, 2)\n\t        )\n\t        value_states = (\n\t            self.v_proj(hidden_states)\n\t            .view(bsz, q_len, self.num_heads, self.head_dim)\n", "            .transpose(1, 2)\n\t        )\n\t        kv_seq_len = key_states.shape[-2]\n\t        if past_key_value is not None:\n\t            kv_seq_len += past_key_value[0].shape[-2]\n\t            if len(past_key_value) > 2:\n\t                kv_seq_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n\t        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t        key_states_before_pos = key_states\n\t        query_states, key_states = apply_rotary_pos_emb(\n", "            query_states, key_states, cos, sin, position_ids\n\t        )\n\t        # [bsz, nh, t, hd]\n\t        attn_prefix = None\n\t        if past_key_value is not None:\n\t            # reuse k, v, self_attention\n\t            if self.mem_freq is None:\n\t                cache_len = past_key_value[0].shape[2]\n\t                if self.max_cache_size is not None:\n\t                    cache_len = min(cache_len, self.max_cache_size)\n", "                if is_mem is not None:\n\t                    is_mem = torch.cat(\n\t                        (is_mem.new_zeros((1, 1, q_len, cache_len)), is_mem), dim=-1\n\t                    )\n\t                    last_section_mask = torch.cat(\n\t                        (\n\t                            last_section_mask.new_ones((1, 1, q_len, cache_len)),\n\t                            last_section_mask,\n\t                        ),\n\t                        dim=-1,\n", "                    )\n\t                past_key_states = torch.cat([past_key_value[0], key_states], dim=2)\n\t                past_value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\t                key_states = past_key_states[:, :, -(q_len + cache_len) :]\n\t                value_states = past_value_states[:, :, -(q_len + cache_len) :]\n\t                expected_att_size = (bsz, self.num_heads, q_len, cache_len + q_len)\n\t            else:\n\t                orig_value_states = value_states\n\t                incomplete_len = past_key_value[0].shape[2] % (self.mem_freq + 1)\n\t                full_len = past_key_value[0].shape[2] - incomplete_len\n", "                past_key_mem, past_key_incomplete = torch.split(\n\t                    past_key_value[0], (full_len, incomplete_len), dim=2\n\t                )\n\t                past_value_mem, past_value_incomplete = torch.split(\n\t                    past_key_value[1], (full_len, incomplete_len), dim=2\n\t                )\n\t                if offload_cache_to_cpu:\n\t                    past_key_value = (\n\t                        past_key_incomplete,\n\t                        past_value_incomplete,\n", "                        *past_key_value[2:],\n\t                    )\n\t                if incomplete_len > 0:\n\t                    assert q_len + incomplete_len <= (self.mem_freq + 1)\n\t                is_mem = torch.cat(\n\t                    (is_mem.new_zeros((1, 1, q_len, incomplete_len)), is_mem), dim=-1\n\t                )\n\t                last_section_mask = torch.cat(\n\t                    (\n\t                        last_section_mask.new_ones((1, 1, q_len, incomplete_len)),\n", "                        last_section_mask,\n\t                    ),\n\t                    dim=-1,\n\t                )\n\t                if len(past_key_value) > 2:\n\t                    full_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n\t                past_key_incomplete_pos = torch.arange(\n\t                    full_len,\n\t                    full_len + incomplete_len,\n\t                    dtype=torch.long,\n", "                    device=position_ids.device,\n\t                ).unsqueeze(0)\n\t                _, past_key_incomplete = apply_rotary_pos_emb(\n\t                    None, past_key_incomplete, cos, sin, past_key_incomplete_pos\n\t                )\n\t                key_states = torch.cat((past_key_incomplete, key_states), dim=2)\n\t                value_states = torch.cat((past_value_incomplete, value_states), dim=2)\n\t                past_key_mem = past_key_mem.view(\n\t                    bsz, self.num_heads, -1, self.mem_freq + 1, self.head_dim\n\t                )\n", "                past_value_mem = past_value_mem.view(\n\t                    bsz, self.num_heads, -1, self.mem_freq + 1, self.head_dim\n\t                )\n\t                if len(past_key_value) > 2:\n\t                    mem_key_nopos = torch.cat(\n\t                        (\n\t                            past_key_value[2],\n\t                            past_key_mem.select(dim=3, index=self.mem_freq),\n\t                        ),\n\t                        dim=2,\n", "                    )\n\t                    past_key_mem_offload = past_key_value[3]\n\t                    past_key_mem = torch.cat(\n\t                        (\n\t                            past_key_mem_offload,\n\t                            past_key_mem.to(past_key_mem_offload.device),\n\t                        ),\n\t                        dim=2,\n\t                    )\n\t                    past_value_mem = torch.cat(\n", "                        (\n\t                            past_key_value[4],\n\t                            past_value_mem.to(past_key_mem_offload.device),\n\t                        ),\n\t                        dim=2,\n\t                    )\n\t                else:\n\t                    mem_key_nopos = past_key_mem.select(dim=3, index=self.mem_freq)\n\t                num_mems = past_key_mem.shape[2]\n\t                top_k = min(self.top_k, num_mems)\n", "                prefix_len = full_len - (top_k + 1) * (self.mem_freq + 1)\n\t                mem_indices = torch.cat(\n\t                    (\n\t                        position_ids.new_zeros((max(0, num_mems - top_k),)),\n\t                        torch.arange(\n\t                            1,\n\t                            top_k + 1,\n\t                            device=query_states.device,\n\t                            dtype=position_ids.dtype,\n\t                        ),\n", "                    ),\n\t                    dim=0,\n\t                )\n\t                mem_pos = (mem_indices * (self.mem_freq + 1) + self.mem_freq).unsqueeze(\n\t                    0\n\t                ).expand(bsz, -1) + prefix_len\n\t                _, mem_key = apply_rotary_pos_emb(\n\t                    None, mem_key_nopos, cos, sin, mem_pos\n\t                )\n\t                mem_attn_weights = torch.matmul(\n", "                    query_states, mem_key.transpose(2, 3)\n\t                ) / math.sqrt(self.head_dim)\n\t                if offload_cache_to_cpu:\n\t                    aggregate = \"max_over_tokens\"\n\t                else:\n\t                    aggregate = None\n\t                if aggregate == \"max_over_tokens\":\n\t                    token_retrievers = 1\n\t                    head_retrievers = self.num_heads\n\t                    mem_attn_weights = torch.nn.functional.softmax(\n", "                        mem_attn_weights, dim=-1\n\t                    )\n\t                    mem_attn_weights = mem_attn_weights.amax(dim=2, keepdim=True)\n\t                elif aggregate is None:\n\t                    token_retrievers = q_len\n\t                    head_retrievers = self.num_heads\n\t                else:\n\t                    raise NotImplementedError()\n\t                mem_selected_idx = (\n\t                    mem_attn_weights.topk(dim=-1, k=top_k)[1]\n", "                    .sort(dim=-1)[0]\n\t                    .view(bsz, head_retrievers, token_retrievers, top_k)\n\t                )\n\t                selected_indices = torch.arange(\n\t                    0,\n\t                    top_k * (self.mem_freq + 1),\n\t                    device=query_states.device,\n\t                    dtype=position_ids.dtype,\n\t                )\n\t                selected_indices = torch.where(\n", "                    mem_selected_idx >= num_mems - top_k, self.mem_freq + 1, 0\n\t                ).unsqueeze(-1) + selected_indices.view(\n\t                    1, 1, 1, top_k, self.mem_freq + 1\n\t                )\n\t                selected_indices = (\n\t                    selected_indices.view(\n\t                        bsz, head_retrievers, token_retrievers, -1\n\t                    ).expand(bsz, self.num_heads, q_len, -1)\n\t                    + prefix_len\n\t                )\n", "                mem_selected_idx = mem_selected_idx.to(past_key_mem.device)\n\t                mem_selected_idx = mem_selected_idx.view(\n\t                    bsz, self.num_heads, token_retrievers, top_k, 1, 1\n\t                ).expand(\n\t                    bsz,\n\t                    self.num_heads,\n\t                    token_retrievers,\n\t                    top_k,\n\t                    self.mem_freq + 1,\n\t                    self.head_dim,\n", "                )\n\t                selected_keys = past_key_mem.unsqueeze(2).expand(\n\t                    bsz,\n\t                    self.num_heads,\n\t                    token_retrievers,\n\t                    -1,\n\t                    self.mem_freq + 1,\n\t                    self.head_dim,\n\t                )\n\t                selected_keys = selected_keys.take_along_dim(\n", "                    mem_selected_idx, dim=3\n\t                ).to(query_states.device)\n\t                selected_values = (\n\t                    past_value_mem.unsqueeze(2)\n\t                    .expand(\n\t                        bsz,\n\t                        self.num_heads,\n\t                        token_retrievers,\n\t                        -1,\n\t                        self.mem_freq + 1,\n", "                        self.head_dim,\n\t                    )\n\t                    .take_along_dim(mem_selected_idx, dim=3)\n\t                    .to(query_states.device)\n\t                )\n\t                selected_keys = selected_keys.view(\n\t                    bsz, self.num_heads, token_retrievers, -1, self.head_dim\n\t                ).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n\t                selected_keys = apply_rotary_pos_emb(\n\t                    None, selected_keys.unsqueeze(1), cos, sin, selected_indices\n", "                )[1].squeeze(1)\n\t                selected_values = selected_values.view(\n\t                    bsz, self.num_heads, token_retrievers, -1, self.head_dim\n\t                ).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n\t                attn_prefix = torch.matmul(\n\t                    query_states.unsqueeze(3), selected_keys.transpose(3, 4)\n\t                ).squeeze(3) / math.sqrt(self.head_dim)\n\t                is_mem_prefix = (\n\t                    torch.cat(\n\t                        (is_mem.new_zeros((self.mem_freq,)), is_mem.new_ones((1,)))\n", "                    )\n\t                    .unsqueeze(0)\n\t                    .repeat((top_k, 1))\n\t                )\n\t                is_mem_prefix = is_mem_prefix.view(1, 1, 1, -1).expand(1, 1, q_len, -1)\n\t                is_mem = torch.cat((is_mem_prefix, is_mem), dim=-1)\n\t                last_section_mask = torch.cat(\n\t                    (\n\t                        last_section_mask.new_zeros(\n\t                            (1, 1, q_len, top_k * (self.mem_freq + 1))\n", "                        ),\n\t                        last_section_mask,\n\t                    ),\n\t                    dim=-1,\n\t                )\n\t                expected_att_size = (bsz, self.num_heads, q_len, q_len + incomplete_len)\n\t                past_key_states = torch.cat(\n\t                    [past_key_value[0], key_states_before_pos], dim=2\n\t                )\n\t                past_value_states = torch.cat(\n", "                    [past_key_value[1], orig_value_states], dim=2\n\t                )\n\t                if offload_cache_to_cpu:\n\t                    past_key_value = (\n\t                        (\n\t                            past_key_states,\n\t                            past_value_states,\n\t                            mem_key_nopos,\n\t                            past_key_mem.to(\"cpu\"),\n\t                            past_value_mem.to(\"cpu\"),\n", "                            *past_key_value[5:],\n\t                        )\n\t                        if use_cache\n\t                        else None\n\t                    )\n\t                else:\n\t                    past_key_value = (\n\t                        (past_key_states, past_value_states) if use_cache else None\n\t                    )\n\t        else:\n", "            if self.mem_freq is None:\n\t                past_key_states = key_states\n\t            else:\n\t                past_key_states = key_states_before_pos\n\t            past_value_states = value_states\n\t            expected_att_size = (bsz, self.num_heads, q_len, kv_seq_len)\n\t            past_key_value = (past_key_states, past_value_states) if use_cache else None\n\t        attn_weights = torch.matmul(\n\t            query_states, key_states.transpose(2, 3)\n\t        ) / math.sqrt(self.head_dim)\n", "        if attn_weights.size() != expected_att_size:\n\t            raise ValueError(\n\t                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n\t                f\" {attn_weights.size()}\"\n\t            )\n\t        if attention_mask is not None:\n\t            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n\t                raise ValueError(\n\t                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n\t                )\n", "            attn_weights = attn_weights + attention_mask[..., -attn_weights.shape[-1] :]\n\t            attn_weights = torch.max(\n\t                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n\t            )\n\t        if attn_prefix is not None:\n\t            attn_weights = torch.cat((attn_prefix, attn_weights), dim=-1)\n\t        # upcast attention to fp32\n\t        if is_mem is None:\n\t            raise ValueError(\"Don't use this without landmarks\")\n\t        attn_weights = landmark_grouped_softmax(\n", "            attn_weights,\n\t            dim=-1,\n\t            is_mem=is_mem.expand(-1, self.num_heads, -1, -1),\n\t            last_section_mask=last_section_mask,\n\t        ).to(query_states.dtype)\n\t        if attn_prefix is not None:\n\t            attn_prefix, attn_weights = torch.split(\n\t                attn_weights,\n\t                (attn_prefix.shape[-1], attn_weights.shape[-1] - attn_prefix.shape[-1]),\n\t                dim=-1,\n", "            )\n\t        attn_output = torch.matmul(attn_weights, value_states)\n\t        if attn_prefix is not None:\n\t            attn_output += torch.matmul(\n\t                attn_prefix.unsqueeze(3), selected_values\n\t            ).squeeze(3)\n\t        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n\t            raise ValueError(\n\t                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n\t                f\" {attn_output.size()}\"\n", "            )\n\t        attn_output = attn_output.transpose(1, 2)\n\t        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\t        attn_output = self.o_proj(attn_output)\n\t        if not output_attentions:\n\t            attn_weights = None\n\t        return attn_output, attn_weights, past_key_value\n\tclass LlamaDecoderLayer(nn.Module):\n\t    \"\"\"\n\t    Llama Decoder layer\n", "    \"\"\"\n\t    def __init__(self, config: LlamaConfig):\n\t        super().__init__()\n\t        self.hidden_size = config.hidden_size\n\t        self.self_attn = LlamaAttention(config=config)\n\t        self.mlp = LlamaMLP(\n\t            hidden_size=self.hidden_size,\n\t            intermediate_size=config.intermediate_size,\n\t            hidden_act=config.hidden_act,\n\t        )\n", "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\t        self.post_attention_layernorm = LlamaRMSNorm(\n\t            config.hidden_size, eps=config.rms_norm_eps\n\t        )\n\t    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n\t        self.self_attn.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\t    def forward(\n\t        self,\n\t        hidden_states: torch.Tensor,\n\t        attention_mask: Optional[torch.Tensor] = None,\n", "        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t        output_attentions: Optional[bool] = False,\n\t        use_cache: Optional[bool] = False,\n\t        is_mem: Optional[torch.Tensor] = None,\n\t        last_section_mask: Optional[torch.Tensor] = None,\n\t        offload_cache_to_cpu: bool = False,\n\t    ) -> Tuple[\n\t        torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]\n\t    ]:\n", "        \"\"\"\n\t        Args:\n\t            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n\t            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n\t                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n\t            output_attentions (`bool`, *optional*):\n\t                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n\t                returned tensors for more detail.\n\t            use_cache (`bool`, *optional*):\n\t                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n", "                (see `past_key_values`).\n\t            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n\t        \"\"\"\n\t        residual = hidden_states\n\t        hidden_states = self.input_layernorm(hidden_states)\n\t        # Self Attention\n\t        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\t            hidden_states=hidden_states,\n\t            attention_mask=attention_mask,\n\t            position_ids=position_ids,\n", "            past_key_value=past_key_value,\n\t            output_attentions=output_attentions,\n\t            use_cache=use_cache,\n\t            is_mem=is_mem,\n\t            last_section_mask=last_section_mask,\n\t            offload_cache_to_cpu=offload_cache_to_cpu,\n\t        )\n\t        hidden_states = residual + hidden_states\n\t        # Fully Connected\n\t        residual = hidden_states\n", "        hidden_states = self.post_attention_layernorm(hidden_states)\n\t        hidden_states = self.mlp(hidden_states)\n\t        hidden_states = residual + hidden_states\n\t        outputs = (hidden_states,)\n\t        if output_attentions:\n\t            outputs += (self_attn_weights,)\n\t        if use_cache:\n\t            outputs += (present_key_value,)\n\t        return outputs\n\t@add_start_docstrings(\n", "    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n\t    LLAMA_START_DOCSTRING,\n\t)\n\tclass LlamaModel(LlamaPreTrainedModel):\n\t    \"\"\"\n\t    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\t    Args:\n\t        config: LlamaConfig\n\t    \"\"\"\n\t    def __init__(self, config: LlamaConfig):\n", "        super().__init__(config)\n\t        self.padding_idx = config.pad_token_id\n\t        self.vocab_size = config.vocab_size\n\t        self.embed_tokens = nn.Embedding(\n\t            config.vocab_size, config.hidden_size, self.padding_idx\n\t        )\n\t        self.layers = nn.ModuleList(\n\t            [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n\t        )\n\t        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n", "        self.mem_id = None\n\t        self.gradient_checkpointing = False\n\t        # Initialize weights and apply final processing\n\t        self.post_init()\n\t    def get_input_embeddings(self):\n\t        return self.embed_tokens\n\t    def set_input_embeddings(self, value):\n\t        self.embed_tokens = value\n\t    def set_mem_id(self, mem_id):\n\t        self.mem_id = mem_id\n", "    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n\t        for layer in self.layers:\n\t            layer.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\t    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n\t    def _prepare_decoder_attention_mask(\n\t        self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n\t    ):\n\t        # create causal mask\n\t        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n\t        combined_attention_mask = None\n", "        if input_shape[-1] > 1:\n\t            combined_attention_mask = _make_causal_mask(\n\t                input_shape,\n\t                inputs_embeds.dtype,\n\t                device=inputs_embeds.device,\n\t                past_key_values_length=past_key_values_length,\n\t            )\n\t        if attention_mask is not None:\n\t            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n\t            expanded_attn_mask = _expand_mask(\n", "                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n\t            ).to(inputs_embeds.device)\n\t            combined_attention_mask = (\n\t                expanded_attn_mask\n\t                if combined_attention_mask is None\n\t                else expanded_attn_mask + combined_attention_mask\n\t            )\n\t        return combined_attention_mask\n\t    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n\t    def forward(\n", "        self,\n\t        input_ids: torch.LongTensor = None,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_values: Optional[List[torch.FloatTensor]] = None,\n\t        inputs_embeds: Optional[torch.FloatTensor] = None,\n\t        use_cache: Optional[bool] = None,\n\t        output_attentions: Optional[bool] = None,\n\t        output_hidden_states: Optional[bool] = None,\n\t        return_dict: Optional[bool] = None,\n", "        offload_cache_to_cpu: Optional[bool] = None,\n\t    ) -> Union[Tuple, BaseModelOutputWithPast]:\n\t        output_attentions = (\n\t            output_attentions\n\t            if output_attentions is not None\n\t            else self.config.output_attentions\n\t        )\n\t        output_hidden_states = (\n\t            output_hidden_states\n\t            if output_hidden_states is not None\n", "            else self.config.output_hidden_states\n\t        )\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        # retrieve input_ids and inputs_embeds\n\t        is_mem = None\n\t        if input_ids is not None and inputs_embeds is not None:\n\t            raise ValueError(\n", "                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n\t            )\n\t        elif input_ids is not None:\n\t            batch_size, seq_length = input_ids.shape\n\t            if self.mem_id is not None:\n\t                with torch.no_grad():\n\t                    is_mem = input_ids == self.mem_id\n\t        elif inputs_embeds is not None:\n\t            batch_size, seq_length, _ = inputs_embeds.shape\n\t            if self.mem_id is not None:\n", "                raise NotImplementedError\n\t        else:\n\t            raise ValueError(\n\t                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n\t            )\n\t        seq_length_with_past = seq_length\n\t        past_key_values_length = 0\n\t        if past_key_values is not None:\n\t            if is_mem is not None:\n\t                pass\n", "                # raise NotImplementedError\n\t            past_key_values_length = past_key_values[0][0].shape[2]\n\t            if len(past_key_values[0]) > 2:\n\t                past_key_values_length += (\n\t                    past_key_values[0][3].shape[2] * past_key_values[0][3].shape[3]\n\t                )\n\t            seq_length_with_past = seq_length_with_past + past_key_values_length\n\t        if position_ids is None:\n\t            device = input_ids.device if input_ids is not None else inputs_embeds.device\n\t            position_ids = torch.arange(\n", "                past_key_values_length,\n\t                seq_length + past_key_values_length,\n\t                dtype=torch.long,\n\t                device=device,\n\t            )\n\t            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n\t        else:\n\t            position_ids = position_ids.view(-1, seq_length).long()\n\t        if inputs_embeds is None:\n\t            inputs_embeds = self.embed_tokens(input_ids)\n", "        # embed positions\n\t        if attention_mask is None:\n\t            attention_mask = torch.ones(\n\t                (batch_size, seq_length_with_past),\n\t                dtype=torch.bool,\n\t                device=inputs_embeds.device,\n\t            )\n\t        attention_mask = self._prepare_decoder_attention_mask(\n\t            attention_mask,\n\t            (batch_size, seq_length),\n", "            inputs_embeds,\n\t            past_key_values_length,\n\t        )\n\t        last_section_mask = None\n\t        if is_mem is not None:\n\t            is_mem = is_mem.unsqueeze(1).unsqueeze(2)\n\t            current_len = input_ids.shape[1]\n\t            mem_ids = torch.where(\n\t                attention_mask[..., -current_len:] < -1,\n\t                0,\n", "                torch.cumsum(is_mem, -1) - is_mem.int(),\n\t            )\n\t            last_section_mask = torch.amax(mem_ids, -1, keepdim=True) == mem_ids\n\t            attention_mask[..., -current_len:].masked_fill_(\n\t                last_section_mask & is_mem,\n\t                torch.tensor(\n\t                    torch.finfo(inputs_embeds.dtype).min, device=inputs_embeds.device\n\t                ),\n\t            )\n\t            last_section_mask.logical_and_(attention_mask[..., -current_len:] > -1)\n", "            is_mem = is_mem.logical_and(attention_mask[..., -current_len:] > -1)\n\t        hidden_states = inputs_embeds\n\t        if self.gradient_checkpointing and self.training:\n\t            if use_cache:\n\t                LOG.warning_once(\n\t                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n\t                )\n\t                use_cache = False\n\t        # decoder layers\n\t        all_hidden_states = () if output_hidden_states else None\n", "        all_self_attns = () if output_attentions else None\n\t        next_decoder_cache = () if use_cache else None\n\t        for idx, decoder_layer in enumerate(self.layers):\n\t            if output_hidden_states:\n\t                all_hidden_states += (hidden_states,)\n\t            past_key_value = (\n\t                past_key_values[idx] if past_key_values is not None else None\n\t            )\n\t            if self.gradient_checkpointing and self.training:\n\t                def create_custom_forward(module):\n", "                    def custom_forward(*inputs):\n\t                        # None for past_key_value\n\t                        return module(*inputs)\n\t                    return custom_forward\n\t                layer_outputs = torch.utils.checkpoint.checkpoint(\n\t                    create_custom_forward(decoder_layer),\n\t                    hidden_states,\n\t                    attention_mask,\n\t                    position_ids,\n\t                    None,\n", "                    output_attentions,\n\t                    None,\n\t                    is_mem,\n\t                    last_section_mask,\n\t                )\n\t            else:\n\t                layer_outputs = decoder_layer(\n\t                    hidden_states,\n\t                    attention_mask=attention_mask,\n\t                    position_ids=position_ids,\n", "                    past_key_value=past_key_value,\n\t                    output_attentions=output_attentions,\n\t                    use_cache=use_cache,\n\t                    is_mem=is_mem,\n\t                    last_section_mask=last_section_mask,\n\t                    offload_cache_to_cpu=offload_cache_to_cpu,\n\t                )\n\t            hidden_states = layer_outputs[0]\n\t            if use_cache:\n\t                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n", "            if output_attentions:\n\t                all_self_attns += (layer_outputs[1],)\n\t        hidden_states = self.norm(hidden_states)\n\t        # add hidden states from the last decoder layer\n\t        if output_hidden_states:\n\t            all_hidden_states += (hidden_states,)\n\t        next_cache = next_decoder_cache if use_cache else None\n\t        if not return_dict:\n\t            return tuple(\n\t                v\n", "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n\t                if v is not None\n\t            )\n\t        return BaseModelOutputWithPast(\n\t            last_hidden_state=hidden_states,\n\t            past_key_values=next_cache,\n\t            hidden_states=all_hidden_states,\n\t            attentions=all_self_attns,\n\t        )\n\tclass LlamaForCausalLM(LlamaPreTrainedModel):\n", "    \"\"\"\n\t    Llama model with a causal language modeling head.\n\t    \"\"\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t        self.model = LlamaModel(config)\n\t        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\t        self.mem_id = None\n\t        self.mem_freq = None\n\t        self.top_k = None\n", "        self.max_seq_len = None\n\t        # Initialize weights and apply final processing\n\t        self.post_init()\n\t    def get_input_embeddings(self):\n\t        return self.model.embed_tokens\n\t    def set_input_embeddings(self, value):\n\t        self.model.embed_tokens = value\n\t    def get_output_embeddings(self):\n\t        return self.lm_head\n\t    def set_output_embeddings(self, new_embeddings):\n", "        self.lm_head = new_embeddings\n\t    def set_decoder(self, decoder):\n\t        self.model = decoder\n\t    def get_decoder(self):\n\t        return self.model\n\t    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n\t    @replace_return_docstrings(\n\t        output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC\n\t    )\n\t    def forward(\n", "        self,\n\t        input_ids: torch.LongTensor = None,\n\t        attention_mask: Optional[torch.Tensor] = None,\n\t        position_ids: Optional[torch.LongTensor] = None,\n\t        past_key_values: Optional[List[torch.FloatTensor]] = None,\n\t        inputs_embeds: Optional[torch.FloatTensor] = None,\n\t        labels: Optional[torch.LongTensor] = None,\n\t        use_cache: Optional[bool] = None,\n\t        output_attentions: Optional[bool] = None,\n\t        output_hidden_states: Optional[bool] = None,\n", "        return_dict: Optional[bool] = None,\n\t        offload_cache_to_cpu: Optional[bool] = None,\n\t    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\t        r\"\"\"\n\t        Args:\n\t            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n\t                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n\t                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n\t                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\t        Returns:\n", "        Example:\n\t        ```python\n\t        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\t        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n\t        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\t        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n\t        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\t        >>> # Generate\n\t        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n\t        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n", "        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n\t        ```\"\"\"\n\t        output_attentions = (\n\t            output_attentions\n\t            if output_attentions is not None\n\t            else self.config.output_attentions\n\t        )\n\t        output_hidden_states = (\n\t            output_hidden_states\n\t            if output_hidden_states is not None\n", "            else self.config.output_hidden_states\n\t        )\n\t        return_dict = (\n\t            return_dict if return_dict is not None else self.config.use_return_dict\n\t        )\n\t        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n\t        window_len = self.max_seq_len or input_ids.shape[1]\n\t        last_logits = None\n\t        for _, idx in enumerate(range(0, input_ids.shape[1], window_len)):\n\t            if idx >= 1:\n", "                if output_attentions or output_hidden_states:\n\t                    raise NotImplementedError\n\t                if not use_cache:\n\t                    raise NotImplementedError\n\t            outputs = self.model(\n\t                input_ids=input_ids[:, idx : idx + window_len],\n\t                attention_mask=attention_mask[\n\t                    :, : idx + window_len + attention_mask.shape[1] - input_ids.shape[1]\n\t                ]\n\t                if attention_mask is not None\n", "                else None,\n\t                position_ids=position_ids[:, idx : idx + window_len]\n\t                if position_ids is not None\n\t                else None,\n\t                past_key_values=past_key_values,\n\t                inputs_embeds=inputs_embeds[:, idx : idx + window_len]\n\t                if inputs_embeds is not None\n\t                else None,\n\t                use_cache=use_cache,\n\t                output_attentions=output_attentions,\n", "                output_hidden_states=output_hidden_states,\n\t                return_dict=return_dict,\n\t                offload_cache_to_cpu=offload_cache_to_cpu,\n\t            )\n\t            past_key_values = outputs.past_key_values\n\t            if last_logits is not None:\n\t                last_logits = torch.cat((last_logits, outputs[0]), dim=-2)\n\t            last_logits = outputs[0]\n\t        hidden_states = last_logits\n\t        logits = self.lm_head(hidden_states)\n", "        loss = None\n\t        if labels is not None:\n\t            # Shift so that tokens < n predict n\n\t            shift_logits = logits[..., :-1, :].contiguous()\n\t            shift_labels = labels[..., 1:].contiguous()\n\t            # Flatten the tokens\n\t            loss_fct = CrossEntropyLoss()\n\t            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n\t            shift_labels = shift_labels.view(-1)\n\t            # Enable model parallelism\n", "            shift_labels = shift_labels.to(shift_logits.device)\n\t            loss = loss_fct(shift_logits, shift_labels)\n\t        if not return_dict:\n\t            output = (logits,) + outputs[1:]\n\t            return (loss,) + output if loss is not None else output\n\t        return CausalLMOutputWithPast(\n\t            loss=loss,\n\t            logits=logits,\n\t            past_key_values=outputs.past_key_values,\n\t            hidden_states=outputs.hidden_states,\n", "            attentions=outputs.attentions,\n\t        )\n\t    def set_mem_id(self, mem_id):\n\t        self.mem_id = mem_id\n\t        self.model.set_mem_id(mem_id)\n\t    def set_mem_cache_args(self, max_seq_len, mem_freq, top_k, max_cache_size):\n\t        self.mem_freq = mem_freq\n\t        self.top_k = top_k\n\t        self.max_seq_len = max_seq_len\n\t        if self.max_seq_len is not None:\n", "            assert self.max_seq_len % (self.mem_freq + 1) == 0\n\t        self.model.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\t    def prepare_inputs_for_generation(\n\t        self,\n\t        input_ids,\n\t        past_key_values=None,\n\t        attention_mask=None,\n\t        inputs_embeds=None,\n\t        **kwargs,\n\t    ):\n", "        total_len = input_ids.shape[1]\n\t        if past_key_values:\n\t            prev_len = input_ids.shape[1] - 1\n\t        else:\n\t            prev_len = 0\n\t        position_ids = kwargs.get(\"position_ids\", None)\n\t        if self.mem_freq is not None:\n\t            if position_ids is not None:\n\t                raise NotImplementedError\n\t            # T = input_ids.shape[1]\n", "            prev_incomplete_len = prev_len % self.mem_freq\n\t            prev_complete_len = prev_len - prev_incomplete_len\n\t            incomplete_len = total_len % self.mem_freq\n\t            new_full_len = total_len - prev_complete_len - incomplete_len\n\t            prev_input, input_ids_with_mem, input_ids_without_mem = torch.split(\n\t                input_ids, (prev_complete_len, new_full_len, incomplete_len), dim=-1\n\t            )\n\t            bsz, _ = input_ids.size()\n\t            input_ids_with_mem = input_ids_with_mem.view(bsz, -1, self.mem_freq)\n\t            input_ids_with_mem = torch.cat(\n", "                (\n\t                    input_ids_with_mem,\n\t                    input_ids_with_mem.new_full(\n\t                        (bsz, input_ids_with_mem.shape[1], 1), self.mem_id\n\t                    ),\n\t                ),\n\t                dim=-1,\n\t            ).view(bsz, -1)\n\t            input_ids = torch.cat(\n\t                (prev_input, input_ids_with_mem, input_ids_without_mem), dim=-1\n", "            )\n\t            if attention_mask is not None:\n\t                attention_mask_with_mem, attention_mask_without_mem = torch.split(\n\t                    attention_mask,\n\t                    (prev_complete_len + new_full_len, incomplete_len),\n\t                    dim=-1,\n\t                )\n\t                attention_mask_with_mem = attention_mask_with_mem.view(\n\t                    bsz, -1, self.mem_freq\n\t                )\n", "                attention_mask_with_mem = torch.cat(\n\t                    (\n\t                        attention_mask_with_mem,\n\t                        attention_mask_with_mem.new_ones(\n\t                            (bsz, attention_mask_with_mem.shape[1], 1)\n\t                        ),\n\t                    ),\n\t                    dim=-1,\n\t                ).view(bsz, -1)\n\t                attention_mask = torch.cat(\n", "                    (attention_mask_with_mem, attention_mask_without_mem), dim=-1\n\t                )\n\t        input_ids = input_ids[:, prev_len:]\n\t        if attention_mask is not None and position_ids is None:\n\t            # create position_ids on the fly for batch generation\n\t            position_ids = attention_mask.long().cumsum(-1) - 1\n\t            position_ids.masked_fill_(attention_mask == 0, 1)\n\t            position_ids = position_ids[:, -input_ids.shape[1] :].unsqueeze(-1)\n\t        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n\t        if (\n", "            inputs_embeds is not None\n\t            and past_key_values is None\n\t            and self.mem_freq is None\n\t        ):\n\t            model_inputs = {\"inputs_embeds\": inputs_embeds}\n\t        else:\n\t            model_inputs = {\"input_ids\": input_ids}\n\t        model_inputs.update(\n\t            {\n\t                \"position_ids\": position_ids,\n", "                \"past_key_values\": past_key_values,\n\t                \"use_cache\": kwargs.get(\"use_cache\"),\n\t                \"attention_mask\": attention_mask,\n\t                \"offload_cache_to_cpu\": kwargs.get(\"offload_cache_to_cpu\"),\n\t            }\n\t        )\n\t        return model_inputs\n\t    @staticmethod\n\t    def _reorder_cache(past_key_values, beam_idx):\n\t        reordered_past = ()\n", "        for layer_past in past_key_values:\n\t            reordered_past += (\n\t                tuple(\n\t                    past_state.index_select(0, beam_idx) for past_state in layer_past\n\t                ),\n\t            )\n\t        return reordered_past\n\tdef add_mem_tokens(example, mem_freq, mem_id):\n\t    ids = example[\"input_ids\"]\n\t    ret = []\n", "    prev_idx = 0\n\t    for t_idx in range(mem_freq, len(ids), mem_freq):\n\t        ret.extend(ids[prev_idx:t_idx])\n\t        ret.append(mem_id)\n\t        prev_idx = t_idx\n\t    ret.extend(ids[prev_idx:])\n\t    # drop attention_mask\n\t    return {\"input_ids\": ret}\n\tdef patch_llama_with_landmark_attn():\n\t    import transformers\n", "    transformers.models.llama.modeling_llama.LlamaForCausalLM = LlamaForCausalLM\n\t    transformers.models.llama.modeling_llama.LlamaModel = LlamaModel\n\t    transformers.models.llama.modeling_llama.LlamaAttention = LlamaAttention\n\t    transformers.models.llama.modeling_llama.LlamaDecoderLayer = LlamaDecoderLayer\n\t    transformers.models.llama.modeling_llama.apply_rotary_pos_emb = apply_rotary_pos_emb\n\tdef set_model_mem_id(model: LlamaForCausalLM, tokenizer: LlamaTokenizer):\n\t    mem_id = tokenizer.convert_tokens_to_ids(MEM_TOKEN)\n\t    model.set_mem_id(mem_id)\n\tdef get_mem_id(tokenizer: LlamaTokenizer):\n\t    return tokenizer.convert_tokens_to_ids(MEM_TOKEN)\n"]}
{"filename": "src/axolotl/utils/wandb.py", "chunked_list": ["\"\"\"Module for wandb utilities\"\"\"\n\timport os\n\tdef setup_wandb_env_vars(cfg):\n\t    if cfg.wandb_mode and cfg.wandb_mode == \"offline\":\n\t        os.environ[\"WANDB_MODE\"] = cfg.wandb_mode\n\t    elif cfg.wandb_project and len(cfg.wandb_project) > 0:\n\t        os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project\n\t        cfg.use_wandb = True\n\t        if cfg.wandb_watch and len(cfg.wandb_watch) > 0:\n\t            os.environ[\"WANDB_WATCH\"] = cfg.wandb_watch\n", "        if cfg.wandb_log_model and len(cfg.wandb_log_model) > 0:\n\t            os.environ[\"WANDB_LOG_MODEL\"] = cfg.wandb_log_model\n\t        if cfg.wandb_run_id and len(cfg.wandb_run_id) > 0:\n\t            os.environ[\"WANDB_RUN_ID\"] = cfg.wandb_run_id\n\t    else:\n\t        os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]}
{"filename": "src/axolotl/utils/models.py", "chunked_list": ["\"\"\"Module for models and model loading\"\"\"\n\timport logging\n\timport math\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import TYPE_CHECKING, Optional, Tuple  # noqa: F401\n\timport bitsandbytes as bnb\n\timport torch\n\timport transformers\n\tfrom optimum.bettertransformer import BetterTransformer\n", "from transformers import (  # noqa: F401\n\t    AutoConfig,\n\t    AutoModelForCausalLM,\n\t    AutoTokenizer,\n\t    BitsAndBytesConfig,\n\t    LlamaConfig,\n\t    PreTrainedModel,\n\t    PreTrainedTokenizerBase,\n\t)\n\tfrom axolotl.prompt_tokenizers import LLAMA_DEFAULT_PAD_TOKEN\n", "LOG = logging.getLogger(\"axolotl\")\n\tif TYPE_CHECKING:\n\t    from peft import PeftConfig  # noqa: F401\n\t    from axolotl.utils.dict import DictDefault  # noqa: F401\n\tdef load_tokenizer(\n\t    tokenizer_config,\n\t    tokenizer_type,\n\t    cfg,\n\t):\n\t    use_fast = True  # this is the default\n", "    if cfg.tokenizer_use_fast is not None:\n\t        use_fast = cfg.tokenizer_use_fast\n\t    if tokenizer_type:\n\t        tokenizer = getattr(transformers, tokenizer_type).from_pretrained(\n\t            tokenizer_config,\n\t            trust_remote_code=cfg.trust_remote_code or False,\n\t            use_fast=use_fast,\n\t        )\n\t    else:\n\t        tokenizer = AutoTokenizer.from_pretrained(\n", "            tokenizer_config,\n\t            trust_remote_code=cfg.trust_remote_code or False,\n\t            use_fast=use_fast,\n\t        )\n\t    LOG.debug(f\"EOS: {tokenizer.eos_token_id} / {tokenizer.eos_token}\")\n\t    LOG.debug(f\"BOS: {tokenizer.bos_token_id} / {tokenizer.bos_token}\")\n\t    LOG.debug(f\"PAD: {tokenizer.pad_token_id} / {tokenizer.pad_token}\")\n\t    LOG.debug(f\"UNK: {tokenizer.unk_token_id} / {tokenizer.unk_token}\")\n\t    if tokenizer.__class__.__name__ in [\n\t        \"LlamaTokenizer\",\n", "        \"LlamaTokenizerFast\",\n\t    ]:\n\t        tokenizer.pad_token = LLAMA_DEFAULT_PAD_TOKEN\n\t    if tokenizer.__class__.__name__ == \"GPTNeoXTokenizerFast\":\n\t        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\t        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\t    if cfg.special_tokens:\n\t        for k, val in cfg.special_tokens.items():\n\t            tokenizer.add_special_tokens({k: val})\n\t    if cfg.tokens:\n", "        tokenizer.add_tokens(list(cfg.tokens))\n\t    return tokenizer\n\tdef load_model(\n\t    base_model, base_model_config, model_type, tokenizer, cfg, adapter=\"lora\"\n\t):\n\t    # type: (str, str, str, PreTrainedTokenizerBase, DictDefault, Optional[str]) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n\t    \"\"\"\n\t    Load a model from a base model and a model type.\n\t    \"\"\"\n\t    # TODO refactor as a kwarg\n", "    load_in_8bit = cfg.load_in_8bit\n\t    cfg.is_llama_derived_model = \"llama\" in base_model or (\n\t        cfg.model_type and \"llama\" in cfg.model_type.lower()\n\t    )\n\t    if cfg.is_llama_derived_model and cfg.flash_attention:\n\t        if cfg.device not in [\"mps\", \"cpu\"] and not cfg.inference:\n\t            from axolotl.flash_attn import replace_llama_attn_with_flash_attn\n\t            LOG.info(\"patching with flash attention\")\n\t            replace_llama_attn_with_flash_attn()\n\t    elif cfg.is_llama_derived_model and cfg.xformers_attention:\n", "        from axolotl.monkeypatch.llama_attn_hijack_xformers import (\n\t            hijack_llama_attention,\n\t        )\n\t        LOG.info(\"patching with xformers attention\")\n\t        hijack_llama_attention()\n\t    elif cfg.is_llama_derived_model and cfg.sdp_attention:\n\t        from axolotl.monkeypatch.llama_attn_hijack_xformers import (\n\t            hijack_llama_sdp_attention,\n\t        )\n\t        LOG.info(\"patching with sdp attention\")\n", "        hijack_llama_sdp_attention()\n\t    elif cfg.is_llama_derived_model and cfg.landmark_attention:\n\t        from axolotl.monkeypatch.llama_landmark_attn import (\n\t            MEM_TOKEN,\n\t            patch_llama_with_landmark_attn,\n\t        )\n\t        LOG.info(\"patching with landmark attention\")\n\t        patch_llama_with_landmark_attn()\n\t        # Note: This might overwrite previous additional_special_tokens\n\t        tokenizer.add_special_tokens({\"additional_special_tokens\": [MEM_TOKEN]})\n", "    if cfg.is_llama_derived_model and cfg.xpos_rope:\n\t        from axolotl.monkeypatch.xpos_rope_llama_monkey_patch import (\n\t            replace_llama_rope_with_xpos_rope,\n\t        )\n\t        LOG.info(\"patching with xpos rope\")\n\t        replace_llama_rope_with_xpos_rope()\n\t    if cfg.bf16 or cfg.bfloat16:\n\t        torch_dtype = torch.bfloat16\n\t    elif cfg.load_in_8bit or cfg.fp16 or cfg.float16:\n\t        torch_dtype = torch.float16\n", "    else:\n\t        torch_dtype = torch.float32\n\t    try:\n\t        if cfg.gptq:\n\t            from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import (\n\t                replace_peft_model_with_int4_lora_model,\n\t            )\n\t            replace_peft_model_with_int4_lora_model()\n\t    except Exception as err:\n\t        LOG.exception(err)\n", "        raise err\n\t    try:\n\t        from peft import prepare_model_for_kbit_training\n\t    except ImportError:\n\t        # For backward compatibility\n\t        from peft import (\n\t            prepare_model_for_int8_training as prepare_model_for_kbit_training,\n\t        )\n\t    model_kwargs = {}\n\t    if cfg.model_revision:\n", "        model_kwargs[\"revision\"] = cfg.model_revision\n\t    if cfg.adapter == \"qlora\" and cfg.load_in_4bit:\n\t        model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n\t            load_in_4bit=True,\n\t            llm_int8_threshold=6.0,\n\t            llm_int8_has_fp16_weight=False,\n\t            bnb_4bit_compute_dtype=torch_dtype,\n\t            bnb_4bit_use_double_quant=True,\n\t            bnb_4bit_quant_type=\"nf4\",\n\t        )\n", "    try:\n\t        if cfg.gptq and cfg.is_llama_derived_model:\n\t            from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram\n\t            from huggingface_hub import snapshot_download\n\t            try:\n\t                snapshot_download_kwargs = {}\n\t                if cfg.base_model_ignore_patterns:\n\t                    snapshot_download_kwargs[\n\t                        \"ignore_patterns\"\n\t                    ] = cfg.base_model_ignore_patterns\n", "                cache_model_path = Path(\n\t                    snapshot_download(base_model, **snapshot_download_kwargs)\n\t                )\n\t                files = (\n\t                    list(cache_model_path.glob(\"*.pt\"))\n\t                    + list(cache_model_path.glob(\"*.safetensors\"))\n\t                    + list(cache_model_path.glob(\"*.bin\"))\n\t                )\n\t                if len(files) > 0:\n\t                    model_path = str(files[0])\n", "                else:\n\t                    LOG.warning(\n\t                        \"unable to find a cached model file, this will likely fail...\"\n\t                    )\n\t                    model_path = str(cache_model_path)\n\t            except Exception:  # pylint: disable=broad-exception-caught\n\t                model_path = cfg.base_model\n\t            model, _ = load_llama_model_4bit_low_ram(\n\t                base_model_config if base_model_config else base_model,\n\t                model_path,\n", "                device_map=cfg.device_map,\n\t                half=cfg.fp16,\n\t                groupsize=cfg.gptq_groupsize if cfg.gptq_groupsize else -1,\n\t                is_v1_model=cfg.gptq_model_v1\n\t                if cfg.gptq_model_v1 is not None\n\t                else True,\n\t            )\n\t            load_in_8bit = False\n\t        elif cfg.is_llama_derived_model and not cfg.trust_remote_code:\n\t            from transformers import LlamaForCausalLM\n", "            config = LlamaConfig.from_pretrained(base_model_config)\n\t            model = LlamaForCausalLM.from_pretrained(\n\t                base_model,\n\t                config=config,\n\t                load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n\t                load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n\t                torch_dtype=torch_dtype,\n\t                device_map=\"auto\" if cfg.world_size == 1 else cfg.device_map,\n\t                **model_kwargs,\n\t            )\n", "        # elif model_type == \"GPTNeoXForCausalLM\" and cfg.flash_attention:\n\t        #     This is a WIP, still an issue with the backward pass\n\t        #     RuntimeError: grad can be implicitly created only for scalar outputs\n\t        #     TODO: try config.sequence_parallel = False\n\t        #     # https://github.com/HazyResearch/flash-attention/blob/40a25c8ee7465cf547b929cfa2937034e37bfce9/tests/models/test_gpt_neox.py#L12\n\t        #     # https://github.com/HazyResearch/flash-attention/tree/main/training#model-components\n\t        #     # add `**kwargs` to https://github.com/HazyResearch/flash-attention/blob/40a25c8ee7465cf547b929cfa2937034e37bfce9/flash_attn/models/gpt.py#L442\n\t        #     from flash_attn.utils.pretrained import state_dict_from_pretrained\n\t        #     from flash_attn.models.gpt import GPTLMHeadModel\n\t        #     from flash_attn.models.gpt_neox import remap_state_dict_hf_gpt_neox, gpt_neox_config_to_gpt2_config\n", "        #     from transformers import GPTNeoXConfig\n\t        #     config = gpt_neox_config_to_gpt2_config(GPTNeoXConfig.from_pretrained(base_model))\n\t        #     config.use_flash_attn = True\n\t        #     config.fused_bias_fc = True\n\t        #     config.fused_mlp = True  # GPT-NeoX-20B uses \"gelu_fast\"\n\t        #     config.activation_function = \"gelu_fast\"\n\t        #     config.fused_dropout_add_ln = True\n\t        #     # config.residual_in_fp32 = True\n\t        #\n\t        #     model: GPTLMHeadModel = GPTLMHeadModel.from_pretrained(\n", "        #         base_model,\n\t        #         config,\n\t        #         dtype=torch_dtype,\n\t        #         device=cfg.device,\n\t        #     )\n\t        #     model.train() # sets to train instead of eval mode\n\t        elif model_type and not cfg.trust_remote_code:\n\t            model = getattr(transformers, model_type).from_pretrained(\n\t                base_model,\n\t                load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n", "                load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n\t                torch_dtype=torch_dtype,\n\t                device_map=cfg.device_map,\n\t                trust_remote_code=cfg.trust_remote_code or False,\n\t                **model_kwargs,\n\t            )\n\t        else:\n\t            config = AutoConfig.from_pretrained(\n\t                base_model,\n\t                trust_remote_code=cfg.trust_remote_code or False,\n", "            )\n\t            # Shouldn't be a problem most of the time. will obviously error if the model doesn't support this\n\t            # when training starts\n\t            if (\n\t                hasattr(config, \"max_seq_len\")\n\t                and config.max_seq_len\n\t                and cfg.sequence_len > config.max_seq_len\n\t            ):\n\t                config.max_seq_len = cfg.sequence_len\n\t                LOG.warning(f\"increasing context length to {cfg.sequence_len}\")\n", "            elif (\n\t                hasattr(config, \"max_sequence_length\")\n\t                and config.max_sequence_length\n\t                and cfg.sequence_len > config.max_sequence_length\n\t            ):\n\t                config.max_sequence_length = cfg.sequence_len\n\t                LOG.warning(f\"increasing context length to {cfg.sequence_len}\")\n\t            model = AutoModelForCausalLM.from_pretrained(\n\t                base_model,\n\t                config=config,\n", "                load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n\t                load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n\t                torch_dtype=torch_dtype,\n\t                device_map=cfg.device_map,\n\t                trust_remote_code=cfg.trust_remote_code or False,\n\t                **model_kwargs,\n\t            )\n\t    except Exception as err:  # pylint: disable=broad-exception-caught\n\t        LOG.error(\n\t            \"Exception raised attempting to load model, retrying with AutoModelForCausalLM\"\n", "        )\n\t        LOG.exception(err)\n\t        model = AutoModelForCausalLM.from_pretrained(\n\t            base_model,\n\t            load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n\t            load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n\t            torch_dtype=torch_dtype,\n\t            device_map=cfg.device_map,\n\t            trust_remote_code=cfg.trust_remote_code or False,\n\t            **model_kwargs,\n", "        )\n\t    embeddings_len = (\n\t        math.ceil(len(tokenizer) / 32) * 32\n\t        if cfg.resize_token_embeddings_to_32x\n\t        else len(tokenizer)\n\t    )\n\t    model.resize_token_embeddings(embeddings_len)\n\t    if (\n\t        hasattr(model.config, \"max_position_embeddings\")\n\t        and model.config.max_position_embeddings\n", "        and cfg.sequence_len >= model.config.max_position_embeddings\n\t    ):\n\t        LOG.warning(\n\t            f\"increasing model.config.max_position_embeddings to {cfg.sequence_len}\"\n\t        )\n\t        model.config.max_position_embeddings = cfg.sequence_len\n\t    if not cfg.gptq and (\n\t        (cfg.adapter == \"lora\" and load_in_8bit)\n\t        or (cfg.adapter == \"qlora\" and cfg.load_in_4bit)\n\t    ):\n", "        LOG.info(\"converting PEFT model w/ prepare_model_for_kbit_training\")\n\t        model = prepare_model_for_kbit_training(\n\t            model, use_gradient_checkpointing=cfg.gradient_checkpointing\n\t        )\n\t    model, lora_config = load_adapter(model, cfg, adapter)\n\t    if cfg.ddp and not load_in_8bit:\n\t        model.to(f\"cuda:{cfg.local_rank}\")\n\t    if cfg.gptq:\n\t        # Scales to half\n\t        LOG.info(\"Fitting 4bit scales and zeros to half\")\n", "        for _, module in model.named_modules():\n\t            if \"Autograd4bitQuantLinear\" in str(type(module)) or \"Linear4bitLt\" in str(\n\t                type(module)\n\t            ):\n\t                if hasattr(module, \"is_v1_model\") and module.is_v1_model:\n\t                    module.zeros = module.zeros.half()\n\t                module.scales = module.scales.half()\n\t                module.bias = module.bias.half()\n\t    if (\n\t        torch.cuda.device_count() > 1\n", "        and int(os.getenv(\"WORLD_SIZE\", \"1\")) > 1\n\t        and (cfg.gptq or cfg.load_in_4bit)\n\t    ):\n\t        # llama is PROBABLY model parallelizable, but the default isn't that it is\n\t        # so let's only set it for the 4bit, see\n\t        # https://github.com/johnsmith0031/alpaca_lora_4bit/blob/08b3fca4a4a9e0d3945be1bab4529f100a428636/finetune.py#L130-L133\n\t        setattr(model, \"is_parallelizable\", True)\n\t        setattr(model, \"model_parallel\", True)\n\t    requires_grad = []\n\t    for name, param in model.named_parameters(recurse=True):\n", "        if param.requires_grad:\n\t            requires_grad.append(f\"{name}: {param.requires_grad}\")\n\t    if len(requires_grad) == 0:\n\t        LOG.warning(\"there are no parameters that require gradient updates\")\n\t    model.config.use_cache = False\n\t    if cfg.flash_optimum:\n\t        model = BetterTransformer.transform(model)\n\t    # TODO resume_from_checkpoint handling\n\t    return model, lora_config\n\tdef load_adapter(model, cfg, adapter):\n", "    # type: (PreTrainedModel, DictDefault, Optional[str]) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n\t    if adapter is None:\n\t        return model, None\n\t    if adapter in [\"lora\", \"qlora\"]:\n\t        return load_lora(model, cfg)\n\t    if adapter == \"llama-adapter\":\n\t        return load_llama_adapter(model, cfg)\n\t    raise NotImplementedError(f\"{adapter} peft adapter not available\")\n\tdef load_llama_adapter(model, cfg):\n\t    # type: (PreTrainedModel, DictDefault) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n", "    from peft import AdaptionPromptConfig, PeftModel, get_peft_model\n\t    peft_config = AdaptionPromptConfig(\n\t        adapter_layers=cfg.peft_adapter.layers,  # layers (L)\n\t        adapter_len=cfg.peft_adapter.len,  # prompt length (K)\n\t        task_type=\"CAUSAL_LM\",\n\t    )\n\t    if cfg.lora_model_dir:\n\t        LOG.info(\"Loading pretained LORA\")\n\t        model = PeftModel.from_pretrained(\n\t            model,\n", "            cfg.lora_model_dir,\n\t            torch_dtype=torch.float16,\n\t        )\n\t    else:\n\t        model = get_peft_model(model, peft_config)\n\t    model.print_trainable_parameters()\n\t    return model, peft_config\n\tdef find_all_linear_names(bits, model):\n\t    cls = (\n\t        bnb.nn.Linear4bit\n", "        if bits == 4\n\t        else (bnb.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n\t    )\n\t    lora_module_names = set()\n\t    for name, module in model.named_modules():\n\t        if isinstance(module, cls):\n\t            names = name.split(\".\")\n\t            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\t    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n\t        lora_module_names.remove(\"lm_head\")\n", "    return list(lora_module_names)\n\tdef load_lora(model, cfg):\n\t    # type: (PreTrainedModel, DictDefault) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n\t    from peft import LoraConfig, PeftModel, get_peft_model\n\t    lora_target_modules = list(cfg.lora_target_modules or [])\n\t    if cfg.lora_target_linear:\n\t        bits = None\n\t        if cfg.load_in_4bit:\n\t            bits = 4\n\t        elif cfg.load_in_8bit:\n", "            bits = 8\n\t        linear_names = find_all_linear_names(bits, model)\n\t        LOG.info(f\"found linear modules: {repr(linear_names)}\")\n\t        lora_target_modules = list(set(lora_target_modules + linear_names))\n\t    lora_config = LoraConfig(\n\t        r=cfg.lora_r,\n\t        lora_alpha=cfg.lora_alpha,\n\t        target_modules=lora_target_modules,\n\t        lora_dropout=cfg.lora_dropout,\n\t        fan_in_fan_out=cfg.lora_fan_in_fan_out,\n", "        modules_to_save=cfg.lora_modules_to_save if cfg.lora_modules_to_save else None,\n\t        bias=\"none\",\n\t        task_type=\"CAUSAL_LM\",\n\t    )\n\t    if cfg.lora_model_dir:\n\t        model = PeftModel.from_pretrained(\n\t            model,\n\t            cfg.lora_model_dir,\n\t            is_trainable=not cfg.inference,\n\t        )\n", "    else:\n\t        model = get_peft_model(model, lora_config)\n\t    model.print_trainable_parameters()\n\t    return model, lora_config\n"]}
{"filename": "src/axolotl/utils/__init__.py", "chunked_list": []}
{"filename": "src/axolotl/utils/dict.py", "chunked_list": ["\"\"\"Module containing the DictDefault class\"\"\"\n\tfrom addict import Dict\n\tclass DictDefault(Dict):\n\t    \"\"\"\n\t    A Dict that returns None instead of returning empty Dict for missing keys.\n\t    \"\"\"\n\t    def __missing__(self, key):\n\t        return None\n"]}
{"filename": "src/axolotl/utils/data.py", "chunked_list": ["\"\"\"Module containing data utilities\"\"\"\n\timport functools\n\timport logging\n\tfrom hashlib import md5\n\tfrom pathlib import Path\n\tfrom typing import List, Tuple, Union\n\timport torch\n\tfrom datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n\tfrom huggingface_hub import hf_hub_download\n\tfrom transformers import PreTrainedTokenizerBase\n", "from axolotl.datasets import ConstantLengthDataset, TokenizedPromptDataset\n\tfrom axolotl.prompt_strategies import load\n\tfrom axolotl.prompt_tokenizers import (\n\t    AlpacaMultipleChoicePromptTokenizingStrategy,\n\t    AlpacaPromptTokenizingStrategy,\n\t    AlpacaReflectionPTStrategy,\n\t    CompletionPromptTokenizingStrategy,\n\t    GPTeacherPromptTokenizingStrategy,\n\t    JeopardyPromptTokenizingStrategy,\n\t    OpenAssistantPromptTokenizingStrategy,\n", "    ShareGPTPromptTokenizingStrategy,\n\t    SummarizeTLDRPromptTokenizingStrategy,\n\t)\n\tfrom axolotl.prompters import (\n\t    AlpacaPrompter,\n\t    CompletionPrompter,\n\t    GPTeacherPrompter,\n\t    JeopardyPrompter,\n\t    MultipleChoiceConcisePrompter,\n\t    MultipleChoiceExplainPrompter,\n", "    ReflectAlpacaPrompter,\n\t    ShareGPTPrompter,\n\t    SummarizeTLDRPrompter,\n\t)\n\tLOG = logging.getLogger(\"axolotl\")\n\tdef load_tokenized_prepared_datasets(\n\t    tokenizer, cfg, default_dataset_prepared_path\n\t) -> DatasetDict:\n\t    tokenizer_name = tokenizer.__class__.__name__\n\t    ds_hash = str(\n", "        md5(  # nosec\n\t            (\n\t                str(cfg.sequence_len)\n\t                + \"@\"\n\t                + \"|\".join(\n\t                    sorted([f\"{d.path}:{d.type}:{d.shards}\" for d in cfg.datasets])\n\t                )\n\t                + \"|\"\n\t                + tokenizer_name\n\t            ).encode(\"utf-8\")\n", "        ).hexdigest()\n\t    )\n\t    prepared_ds_path = (\n\t        Path(cfg.dataset_prepared_path) / ds_hash\n\t        if cfg.dataset_prepared_path\n\t        else Path(default_dataset_prepared_path) / ds_hash\n\t    )\n\t    dataset = None\n\t    use_auth_token = cfg.hf_use_auth_token\n\t    try:\n", "        if cfg.push_dataset_to_hub:\n\t            dataset = load_dataset(\n\t                f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n\t                use_auth_token=use_auth_token,\n\t            )\n\t            dataset = dataset[\"train\"]\n\t    except Exception:  # pylint: disable=broad-except # nosec\n\t        pass\n\t    if dataset:\n\t        ...\n", "    elif any(prepared_ds_path.glob(\"*\")):\n\t        LOG.info(f\"Loading prepared dataset from disk at {prepared_ds_path}...\")\n\t        dataset = load_from_disk(str(prepared_ds_path))\n\t        LOG.info(\"Prepared dataset loaded from disk...\")\n\t    else:\n\t        LOG.info(f\"Unable to find prepared dataset in {prepared_ds_path}\")\n\t        LOG.info(\"Loading raw datasets...\")\n\t        if cfg.seed:\n\t            seed = cfg.seed\n\t        else:\n", "            LOG.info(\"No seed provided, using default seed of 42\")\n\t            seed = 42\n\t        datasets = []\n\t        # pylint: disable=invalid-name\n\t        for d in cfg.datasets:\n\t            ds: Union[Dataset, DatasetDict] = None\n\t            ds_from_hub = False\n\t            try:\n\t                load_dataset(\n\t                    d.path,\n", "                    name=d.name,\n\t                    streaming=True,\n\t                    use_auth_token=use_auth_token,\n\t                )\n\t                ds_from_hub = True\n\t            except FileNotFoundError:\n\t                pass\n\t            # prefer local dataset, even if hub exists\n\t            local_path = Path(d.path)\n\t            if local_path.exists():\n", "                if local_path.is_dir():\n\t                    ds = load_dataset(\n\t                        d.path,\n\t                        name=d.name,\n\t                        data_files=d.data_files,\n\t                        streaming=False,\n\t                        split=None,\n\t                    )\n\t                elif local_path.is_file():\n\t                    ds = load_dataset(\n", "                        \"json\",\n\t                        name=d.name,\n\t                        data_files=d.path,\n\t                        streaming=False,\n\t                        split=None,\n\t                    )\n\t                else:\n\t                    raise ValueError(\n\t                        \"unhandled dataset load: local path exists, but is neither a directory or a file\"\n\t                    )\n", "            elif ds_from_hub:\n\t                ds = load_dataset(\n\t                    d.path,\n\t                    name=d.name,\n\t                    streaming=False,\n\t                    data_files=d.data_files,\n\t                    use_auth_token=use_auth_token,\n\t                )\n\t            else:\n\t                fp = hf_hub_download(\n", "                    repo_id=d.path,\n\t                    repo_type=\"dataset\",\n\t                    filename=d.data_files,\n\t                )\n\t                ds = load_dataset(\n\t                    \"json\", name=d.name, data_files=fp, streaming=False, split=None\n\t                )\n\t            if not ds:\n\t                raise ValueError(\"unhandled dataset load\")\n\t            # support for using a subset of the data\n", "            if d.shards:\n\t                if \"train\" in ds:\n\t                    ds = ds.shuffle(seed=seed)[\"train\"].shard(\n\t                        num_shards=d.shards, index=0\n\t                    )\n\t                else:\n\t                    ds = ds.shuffle(seed=seed).shard(num_shards=d.shards, index=0)\n\t            d_type = d.type\n\t            d_type_split = d_type.split(\":\")\n\t            d_base_type = d_type_split[0]\n", "            d_prompt_style = d_type_split[1] if len(d_type_split) > 1 else None\n\t            if \"train\" in ds:\n\t                ds = ds[\"train\"]\n\t            if ds_strategy := load(d.type, tokenizer, cfg):\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"alpaca\":\n\t                ds_strategy = AlpacaPromptTokenizingStrategy(\n\t                    AlpacaPrompter(d_prompt_style),\n\t                    tokenizer,\n", "                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"explainchoice\":\n\t                ds_strategy = AlpacaMultipleChoicePromptTokenizingStrategy(\n\t                    MultipleChoiceExplainPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n", "                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"concisechoice\":\n\t                ds_strategy = AlpacaMultipleChoicePromptTokenizingStrategy(\n\t                    MultipleChoiceConcisePrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n", "                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"summarizetldr\":\n\t                ds_strategy = SummarizeTLDRPromptTokenizingStrategy(\n\t                    SummarizeTLDRPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n", "                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"jeopardy\":\n\t                ds_strategy = JeopardyPromptTokenizingStrategy(\n\t                    JeopardyPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n", "                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"oasst\":\n\t                ds_strategy = OpenAssistantPromptTokenizingStrategy(\n\t                    AlpacaPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n", "            elif d_base_type == \"gpteacher\":\n\t                ds_strategy = GPTeacherPromptTokenizingStrategy(\n\t                    GPTeacherPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"reflection\":\n", "                ds_strategy = AlpacaReflectionPTStrategy(\n\t                    ReflectAlpacaPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"sharegpt\":\n\t                ds_strategy = ShareGPTPromptTokenizingStrategy(\n", "                    ShareGPTPrompter(d_prompt_style),\n\t                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            elif d_base_type == \"completion\":\n\t                ds_strategy = CompletionPromptTokenizingStrategy(\n\t                    CompletionPrompter(),\n", "                    tokenizer,\n\t                    cfg.train_on_inputs,\n\t                    cfg.sequence_len,\n\t                )\n\t                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n\t                datasets.append(ds_wrapper)\n\t            else:\n\t                suffix = \"\"\n\t                if \":load_\" in d.type:\n\t                    suffix = f\" Did you mean {d.type.replace(':load_', '.load_')}?\"\n", "                LOG.error(f\"unhandled prompt tokenization strategy: {d.type}. {suffix}\")\n\t                raise ValueError(\n\t                    f\"unhandled prompt tokenization strategy: {d.type} {suffix}\"\n\t                )\n\t        LOG.info(\"tokenizing, merging, and shuffling master dataset\")\n\t        samples: List[int] = []\n\t        for d in datasets:\n\t            samples = samples + list(d)\n\t        dataset = Dataset.from_list(samples).shuffle(seed=seed)\n\t        if cfg.local_rank == 0:\n", "            LOG.info(f\"Saving merged prepared dataset to disk... {prepared_ds_path}\")\n\t            dataset.save_to_disk(prepared_ds_path)\n\t            if cfg.push_dataset_to_hub:\n\t                LOG.info(\n\t                    f\"Saving merged prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n\t                )\n\t                dataset.push_to_hub(\n\t                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\", private=True\n\t                )\n\t    return dataset\n", "def load_prepare_datasets(\n\t    tokenizer: PreTrainedTokenizerBase,\n\t    cfg,\n\t    default_dataset_prepared_path,\n\t) -> Tuple[Dataset, Dataset]:\n\t    max_packed_sequence_len = (\n\t        cfg.max_packed_sequence_len if cfg.max_packed_sequence_len else cfg.sequence_len\n\t    )\n\t    max_packed_sequence_len = min(\n\t        max_packed_sequence_len, cfg.sequence_len\n", "    )  # make sure we don't accidentally set it larger than sequence_len\n\t    tokenizer_name = tokenizer.__class__.__name__\n\t    if cfg.max_packed_sequence_len is not None:\n\t        # see if we can go ahead and load the stacked dataset\n\t        seed = f\"@{str(cfg.seed)}\" if cfg.seed else \"\"\n\t        ds_hash = str(\n\t            md5(  # nosec\n\t                (\n\t                    str(cfg.sequence_len)\n\t                    + \"@\"\n", "                    + str(max_packed_sequence_len)\n\t                    + seed\n\t                    + \"|\".join(\n\t                        sorted([f\"{d.path}:{d.type}:{d.shards}\" for d in cfg.datasets])\n\t                    )\n\t                    + \"|\"\n\t                    + tokenizer_name\n\t                ).encode(\"utf-8\")\n\t            ).hexdigest()\n\t        )\n", "        prepared_ds_path = (\n\t            Path(cfg.dataset_prepared_path) / ds_hash\n\t            if cfg.dataset_prepared_path\n\t            else Path(default_dataset_prepared_path) / ds_hash\n\t        )\n\t        dataset = None\n\t        use_auth_token = cfg.hf_use_auth_token\n\t        try:\n\t            if cfg.push_dataset_to_hub:\n\t                LOG.info(\n", "                    f\"Checking for packed prepared dataset from hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n\t                )\n\t                dataset = load_dataset(\n\t                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n\t                    use_auth_token=use_auth_token,\n\t                )\n\t                dataset = dataset[\"train\"]\n\t        except Exception:  # pylint: disable=broad-except # nosec\n\t            pass\n\t        if dataset:\n", "            ...\n\t        elif any(prepared_ds_path.glob(\"*\")):\n\t            LOG.info(\n\t                f\"Loading prepared packed dataset from disk at {prepared_ds_path}...\"\n\t            )\n\t            dataset = load_from_disk(str(prepared_ds_path))\n\t            LOG.info(\"Prepared packed dataset loaded from disk...\")\n\t            if cfg.push_dataset_to_hub:\n\t                LOG.info(\n\t                    f\"Saving packed prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n", "                )\n\t                dataset.push_to_hub(\n\t                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\", private=True\n\t                )\n\t        else:\n\t            dataset = load_tokenized_prepared_datasets(\n\t                tokenizer, cfg, default_dataset_prepared_path\n\t            )\n\t            if cfg.seed:\n\t                dataset = dataset.shuffle(seed=cfg.seed)\n", "            constant_len_dataset = ConstantLengthDataset(\n\t                tokenizer,\n\t                [dataset],\n\t                seq_length=max_packed_sequence_len,\n\t            )\n\t            LOG.info(f\"packing master dataset to len: {cfg.max_packed_sequence_len}\")\n\t            dataset = Dataset.from_list(list(constant_len_dataset))\n\t            # filter out bad data\n\t            dataset = Dataset.from_list(\n\t                [\n", "                    d\n\t                    for d in dataset\n\t                    if len(d[\"input_ids\"]) < cfg.sequence_len\n\t                    and len(d[\"input_ids\"]) > 0\n\t                    and len(d[\"input_ids\"]) == len(d[\"attention_mask\"])\n\t                    and len(d[\"input_ids\"]) == len(d[\"labels\"])\n\t                ]\n\t            )\n\t            if cfg.local_rank == 0:\n\t                LOG.info(\n", "                    f\"Saving packed prepared dataset to disk... {prepared_ds_path}\"\n\t                )\n\t                dataset.save_to_disk(prepared_ds_path)\n\t                if cfg.push_dataset_to_hub:\n\t                    LOG.info(\n\t                        f\"Saving packed prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n\t                    )\n\t                    dataset.push_to_hub(\n\t                        f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n\t                        private=True,\n", "                    )\n\t    else:\n\t        dataset = load_tokenized_prepared_datasets(\n\t            tokenizer, cfg, default_dataset_prepared_path\n\t        )\n\t    if cfg.dataset_shard_num and cfg.dataset_shard_idx is not None:\n\t        LOG.info(\n\t            f\"Using index #{cfg.dataset_shard_idx} of {cfg.dataset_shard_num} shards\"\n\t        )\n\t        dataset = dataset.shard(\n", "            num_shards=cfg.dataset_shard_num,\n\t            index=cfg.dataset_shard_idx,\n\t        )\n\t    if cfg.val_set_size:\n\t        dataset = dataset.train_test_split(test_size=cfg.val_set_size, shuffle=False)\n\t        train_dataset = dataset[\"train\"]\n\t        eval_dataset = dataset[\"test\"]\n\t    else:\n\t        train_dataset = dataset\n\t        eval_dataset = None\n", "    return train_dataset, eval_dataset\n\tdef encode_pretraining(tokenizer, max_tokens, examples):\n\t    res = tokenizer(\n\t        examples[\"text\"],\n\t        truncation=True,\n\t        max_length=max_tokens - 2,\n\t        add_special_tokens=True,\n\t    )\n\t    # Convert to PyTorch tensors\n\t    input_ids = [torch.tensor(seq) for seq in res[\"input_ids\"]]\n", "    attention_mask = [torch.tensor(seq) for seq in res[\"attention_mask\"]]\n\t    new_input_ids = []\n\t    new_attention_mask = []\n\t    # Append EOS and PAD tokens to input_ids, and correct attention_mask\n\t    for i, _ in enumerate(input_ids):\n\t        input_ids[i] = torch.cat(\n\t            (\n\t                input_ids[i],\n\t                torch.tensor([tokenizer.eos_token_id, tokenizer.pad_token_id]),\n\t            ),\n", "            dim=0,\n\t        )\n\t        attention_mask[i] = torch.cat((attention_mask[i], torch.tensor([1, 0])), dim=0)\n\t    # Concatenate tokens so that their lengths are less than max_tokens\n\t    buffer_input_ids = torch.tensor([], dtype=torch.long)\n\t    buffer_attention_mask = torch.tensor([], dtype=torch.long)\n\t    for ids, mask in zip(input_ids, attention_mask):\n\t        if buffer_input_ids.numel() == max_tokens:\n\t            new_input_ids.append(buffer_input_ids)\n\t            new_attention_mask.append(buffer_attention_mask)\n", "            buffer_input_ids = torch.tensor([], dtype=torch.long)\n\t            buffer_attention_mask = torch.tensor([], dtype=torch.long)\n\t            buffer_input_ids = torch.cat((buffer_input_ids, ids), dim=0)\n\t            buffer_attention_mask = torch.cat((buffer_attention_mask, mask), dim=0)\n\t        elif buffer_input_ids.numel() + ids.numel() <= max_tokens:\n\t            buffer_input_ids = torch.cat((buffer_input_ids, ids), dim=0)\n\t            buffer_attention_mask = torch.cat((buffer_attention_mask, mask), dim=0)\n\t        else:\n\t            buffer_input_ids = torch.cat(\n\t                (\n", "                    buffer_input_ids,\n\t                    torch.full(\n\t                        (max_tokens - buffer_input_ids.numel(),),\n\t                        tokenizer.pad_token_id,\n\t                        dtype=torch.long,\n\t                    ),\n\t                ),\n\t                dim=0,\n\t            )\n\t            buffer_attention_mask = torch.cat(\n", "                (\n\t                    buffer_attention_mask,\n\t                    torch.full(\n\t                        (max_tokens - buffer_attention_mask.numel(),),\n\t                        0,\n\t                        dtype=torch.long,\n\t                    ),\n\t                ),\n\t                dim=0,\n\t            )\n", "            new_input_ids.append(buffer_input_ids)\n\t            new_attention_mask.append(buffer_attention_mask)\n\t            buffer_input_ids = torch.tensor([], dtype=torch.long)\n\t            buffer_attention_mask = torch.tensor([], dtype=torch.long)\n\t            buffer_input_ids = torch.cat((buffer_input_ids, ids), dim=0)\n\t            buffer_attention_mask = torch.cat((buffer_attention_mask, mask), dim=0)\n\t    if buffer_input_ids.numel() > 0:  # for any leftover tokens\n\t        while buffer_input_ids.numel() < max_tokens:  # make all sequences equal in size\n\t            buffer_input_ids = torch.cat(\n\t                (\n", "                    buffer_input_ids,\n\t                    torch.full(\n\t                        (max_tokens - buffer_input_ids.numel(),),\n\t                        tokenizer.pad_token_id,\n\t                        dtype=torch.long,\n\t                    ),\n\t                ),\n\t                dim=0,\n\t            )\n\t            buffer_attention_mask = torch.cat(\n", "                (\n\t                    buffer_attention_mask,\n\t                    torch.full(\n\t                        (max_tokens - buffer_attention_mask.numel(),),\n\t                        0,\n\t                        dtype=torch.long,\n\t                    ),\n\t                ),\n\t                dim=0,\n\t            )\n", "        new_input_ids.append(buffer_input_ids)\n\t        new_attention_mask.append(buffer_attention_mask)\n\t    ret = {\n\t        \"input_ids\": [seq.tolist() for seq in new_input_ids],\n\t        \"labels\": [seq.tolist() for seq in new_input_ids],\n\t        \"attention_mask\": [seq.tolist() for seq in new_attention_mask],\n\t    }\n\t    LOG.debug(len(ret[\"input_ids\"]))\n\t    return ret\n\tdef load_pretraining_dataset(path, tokenizer, max_tokens=2048, seed=42):\n", "    encode = functools.partial(encode_pretraining, tokenizer, max_tokens)\n\t    dataset = load_dataset(path, streaming=True, split=\"train\")\n\t    dataset = dataset.shuffle(seed=seed, buffer_size=10_000)\n\t    # TODO dynamically figure out which columns/features to remove\n\t    dataset = dataset.map(encode, batched=True, remove_columns=[\"text\", \"meta\"])\n\t    return dataset\n"]}
{"filename": "src/axolotl/utils/tokenization.py", "chunked_list": ["\"\"\"Module for tokenization utilities\"\"\"\n\timport logging\n\tfrom termcolor import colored\n\tLOG = logging.getLogger(\"axolotl\")\n\tdef check_dataset_labels(dataset, tokenizer):\n\t    # the dataset is already shuffled, so let's just check the first 5 elements\n\t    for idx in range(5):\n\t        check_example_labels(dataset[idx], tokenizer)\n\tdef check_example_labels(example, tokenizer):\n\t    # Get the input_ids, labels, and attention_mask from the dataset\n", "    input_ids = example[\"input_ids\"]\n\t    labels = example[\"labels\"]\n\t    attention_mask = example[\"attention_mask\"]\n\t    # You can compare the input_ids and labels element-wise\n\t    # Remember to ignore positions with IGNORE_TOKEN_ID (if you use it) or attention_mask equal to 0\n\t    colored_tokens = []\n\t    for _, (input_id, label_id, mask) in enumerate(\n\t        zip(input_ids, labels, attention_mask)\n\t    ):\n\t        decoded_input_token = tokenizer.decode(input_id)\n", "        # Choose the color based on whether the label has the ignore value or not\n\t        color = \"red\" if label_id == -100 else (\"yellow\" if label_id == 0 else \"green\")\n\t        colored_token = colored(decoded_input_token, color) + colored(\n\t            f\"({label_id}, {mask}, {input_id})\", \"white\"\n\t        )\n\t        colored_tokens.append(colored_token)\n\t    LOG.info(\" \".join(colored_tokens))\n\t    LOG.info(\"\\n\\n\\n\")\n\t    return \" \".join(colored_tokens)\n"]}
{"filename": "src/axolotl/utils/schedulers.py", "chunked_list": ["\"\"\"Module for custom LRScheduler class\"\"\"\n\timport math\n\tfrom functools import partial\n\tfrom torch.optim import Optimizer\n\tfrom torch.optim.lr_scheduler import LambdaLR, LRScheduler\n\tclass InterpolatingLogScheduler(LRScheduler):\n\t    \"\"\"\n\t    A scheduler that interpolates learning rates in a logarithmic fashion\n\t    \"\"\"\n\t    def __init__(self, optimizer, num_steps, min_lr, max_lr, last_epoch=-1):\n", "        \"\"\"A scheduler that interpolates learning rates in a logarithmic fashion\n\t        Args:\n\t        - optimizer: pytorch optimizer\n\t        - num_steps: int, the number of steps over which to increase from the min_lr to the max_lr\n\t        - min_lr: float, the minimum learning rate\n\t        - max_lr: float, the maximum learning rate\n\t        Usage:\n\t            fc = nn.Linear(1,1)\n\t            optimizer = optim.Adam(fc.parameters())\n\t            lr_scheduler = InterpolatingLogScheduler(optimizer, num_steps=400, min_lr=1e-6, max_lr=1e-4)\n", "        \"\"\"\n\t        self.num_steps = num_steps\n\t        self.min_lr = min_lr\n\t        self.max_lr = max_lr\n\t        self.q = (max_lr / min_lr) ** (  # pylint: disable=invalid-name\n\t            1 / (num_steps - 1)\n\t        )\n\t        super().__init__(optimizer, last_epoch)\n\t    def get_lr(self):\n\t        if self.last_epoch <= 0:\n", "            lrs = [self.min_lr for base_lr in self.base_lrs]\n\t        elif self.last_epoch < self.num_steps:\n\t            lrs = [\n\t                self.min_lr * (self.q ** (self.last_epoch - 1))\n\t                for base_lr in self.base_lrs\n\t            ]\n\t        else:\n\t            lrs = [self.max_lr for base_lr in self.base_lrs]\n\t        return lrs\n\tdef _get_cosine_schedule_with_quadratic_warmup_lr_lambda(\n", "    current_step: int,\n\t    *,\n\t    num_warmup_steps: int,\n\t    num_training_steps: int,\n\t    num_cycles: float\n\t):\n\t    if current_step < num_warmup_steps:\n\t        return (float(current_step) / float(max(1, num_warmup_steps))) ** 2\n\t    progress = float(current_step - num_warmup_steps) / float(\n\t        max(1, num_training_steps - num_warmup_steps)\n", "    )\n\t    return max(\n\t        0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n\t    )\n\tdef get_cosine_schedule_with_quadratic_warmup(\n\t    optimizer: Optimizer,\n\t    num_warmup_steps: int,\n\t    num_training_steps: int,\n\t    num_cycles: float = 0.5,\n\t    last_epoch: int = -1,\n", "):\n\t    \"\"\"\n\t    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n\t    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n\t    initial lr set in the optimizer.\n\t    Args:\n\t        optimizer ([`~torch.optim.Optimizer`]):\n\t            The optimizer for which to schedule the learning rate.\n\t        num_warmup_steps (`int`):\n\t            The number of steps for the warmup phase.\n", "        num_training_steps (`int`):\n\t            The total number of training steps.\n\t        num_cycles (`float`, *optional*, defaults to 0.5):\n\t            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n\t            following a half-cosine).\n\t        last_epoch (`int`, *optional*, defaults to -1):\n\t            The index of the last epoch when resuming training.\n\t    Return:\n\t        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\t    \"\"\"\n", "    lr_lambda = partial(\n\t        _get_cosine_schedule_with_quadratic_warmup_lr_lambda,\n\t        num_warmup_steps=num_warmup_steps,\n\t        num_training_steps=num_training_steps,\n\t        num_cycles=num_cycles,\n\t    )\n\t    return LambdaLR(optimizer, lr_lambda, last_epoch)\n"]}
{"filename": "src/axolotl/utils/validation.py", "chunked_list": ["\"\"\"Module for validating config files\"\"\"\n\timport logging\n\timport torch\n\tLOG = logging.getLogger(\"axolotl\")\n\tdef validate_config(cfg):\n\t    if cfg.gradient_accumulation_steps and cfg.batch_size:\n\t        raise ValueError(\n\t            \"please set only one of gradient_accumulation_steps or batch_size\"\n\t        )\n\t    if cfg.batch_size:\n", "        LOG.warning(\n\t            \"%s\\n%s\",\n\t            \"batch_size is not recommended. Please use gradient_accumulation_steps instead.\",\n\t            \"To calculate the equivalent gradient_accumulation_steps, divide batch_size / micro_batch_size / number of gpus.\",\n\t        )\n\t    if cfg.load_4bit:\n\t        raise ValueError(\n\t            \"cfg.load_4bit parameter has been deprecated and replaced by cfg.gptq\"\n\t        )\n\t    if cfg.adapter == \"qlora\":\n", "        if cfg.merge_lora:\n\t            # can't merge qlora if loaded in 8bit or 4bit\n\t            if cfg.load_in_8bit:\n\t                raise ValueError(\"Can't merge qlora if loaded in 8bit\")\n\t            if cfg.gptq:\n\t                raise ValueError(\"Can't merge qlora if gptq\")\n\t            if cfg.load_in_4bit:\n\t                raise ValueError(\"Can't merge qlora if loaded in 4bit\")\n\t        else:\n\t            if cfg.load_in_8bit:\n", "                raise ValueError(\"Can't load qlora in 8bit\")\n\t            if cfg.gptq:\n\t                raise ValueError(\"Can't load qlora if gptq\")\n\t            if not cfg.load_in_4bit:\n\t                raise ValueError(\"Require cfg.load_in_4bit to be True for qlora\")\n\t    if not cfg.load_in_8bit and cfg.adapter == \"lora\":\n\t        LOG.warning(\"We recommend setting `load_in_8bit: true` for LORA finetuning\")\n\t    if cfg.trust_remote_code:\n\t        LOG.warning(\n\t            \"`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\"\n", "        )\n\t    if cfg.push_dataset_to_hub and cfg.hf_use_auth_token is not True:\n\t        raise ValueError(\n\t            \"Require cfg.hf_use_auth_token to be True for push_dataset_to_hub\"\n\t        )\n\t    if (cfg.base_model and \"falcon\" in cfg.base_model.lower()) and cfg.fsdp:\n\t        raise ValueError(\"FSDP is not supported for falcon models\")\n\t    if (\n\t        cfg.base_model and \"mpt\" in cfg.base_model.lower()\n\t    ) and cfg.gradient_checkpointing:\n", "        raise ValueError(\"gradient_checkpointing is not supported for MPT models\")\n\t    if cfg.flash_optimum is True:\n\t        if cfg.adapter:\n\t            LOG.warning(\"BetterTransformers probably doesn't work with PEFT adapters\")\n\t        if cfg.fp16 or cfg.bf16:\n\t            raise ValueError(\"AMP is not supported with BetterTransformer\")\n\t        if cfg.float16 is not True and cfg.bloat16 is not True:\n\t            LOG.warning(\n\t                \"You should probably set bfloat16 or float16 to true to \"\n\t                \"load the model in float16 for BetterTransformers\"\n", "            )\n\t        if int(torch.__version__.split(\".\")[0]) < 2:\n\t            LOG.warning(\"torch>=2.0.0 required\")\n\t            raise ValueError(\n\t                f\"flash_optimum for BetterTransformers may not be used with {torch.__version__}\"\n\t            )\n\t    if cfg.pretraining_dataset and cfg.group_by_length:\n\t        LOG.warning(\n\t            \"You probably want to disable group_by_length as it will force a streamed dataset to download completely.\"\n\t        )\n", "    if any([cfg.adam_beta1, cfg.adam_beta2, cfg.adam_epsilon]) and (\n\t        not cfg.optimizer or \"adamw\" not in cfg.optimizer\n\t    ):\n\t        LOG.warning(\"adamw hyperparameters found, but no adamw optimizer set\")\n\t    if cfg.push_to_hub_model_id:\n\t        raise ValueError(\n\t            \"push_to_hub_model_id is deprecated. Please use hub_model_id instead.\"\n\t        )\n\t    # TODO\n\t    # MPT 7b\n", "    # https://github.com/facebookresearch/bitsandbytes/issues/25\n\t    # no 8bit adaAmw w bf16\n\t    # GPT-NeoX\n\t    # evals broken when extending context len\n\t    # File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 162, in forward                        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\t    # File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/optimum/bettertransformer/models/attention.py\", line 74, in gpt2_wrapped_scaled_dot_product\n\t    # attention_mask = causal_mask + attention_mask\n\t    # RuntimeError: The size of tensor a (2048) must match the size of tensor b (8132) at non-singleton dimension 3\n"]}
{"filename": "src/axolotl/utils/callbacks.py", "chunked_list": ["\"\"\"Callbacks for Trainer class\"\"\"\n\timport os\n\tfrom optimum.bettertransformer import BetterTransformer\n\tfrom transformers import (\n\t    TrainerCallback,\n\t    TrainerControl,\n\t    TrainerState,\n\t    TrainingArguments,\n\t)\n\tfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, IntervalStrategy\n", "class SavePeftModelCallback(TrainerCallback):  # pylint: disable=too-few-public-methods\n\t    \"\"\"Callback to save the PEFT adapter\"\"\"\n\t    def on_save(\n\t        self,\n\t        args: TrainingArguments,\n\t        state: TrainerState,\n\t        control: TrainerControl,\n\t        **kwargs,\n\t    ):\n\t        checkpoint_folder = os.path.join(\n", "            args.output_dir,\n\t            f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\",\n\t        )\n\t        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n\t        kwargs[\"model\"].save_pretrained(peft_model_path)\n\t        return control\n\tclass SaveBetterTransformerModelCallback(\n\t    TrainerCallback\n\t):  # pylint: disable=too-few-public-methods\n\t    \"\"\"Callback to save the BetterTransformer wrapped model\"\"\"\n", "    def on_step_end(\n\t        self,\n\t        args: TrainingArguments,\n\t        state: TrainerState,\n\t        control: TrainerControl,\n\t        **kwargs,\n\t    ):\n\t        # Save\n\t        if (\n\t            args.save_strategy == IntervalStrategy.STEPS\n", "            and args.save_steps > 0\n\t            and state.global_step % args.save_steps == 0\n\t        ):\n\t            control.should_save = True\n\t        if control.should_save:\n\t            checkpoint_folder = os.path.join(\n\t                args.output_dir,\n\t                f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\",\n\t            )\n\t            model = BetterTransformer.reverse(kwargs[\"model\"])\n", "            model.save_pretrained(checkpoint_folder)\n\t            # FIXME - need to cleanup old checkpoints\n\t            # since we're saving here, we don't need the trainer loop to attempt to save too b/c\n\t            # the trainer will raise an exception since it can't save a BetterTransformer wrapped model\n\t            control.should_save = False\n\t        return control\n"]}
{"filename": "src/axolotl/utils/trainer.py", "chunked_list": ["\"\"\"Module containing the Trainer class and related functions\"\"\"\n\timport importlib\n\timport logging\n\timport math\n\timport os\n\timport sys\n\tfrom dataclasses import dataclass, field\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\timport bitsandbytes as bnb\n", "import torch.cuda\n\timport transformers\n\tfrom torch import nn\n\tfrom torch.optim.lr_scheduler import OneCycleLR\n\tfrom transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n\tfrom transformers.trainer_pt_utils import get_parameter_names\n\tfrom axolotl.utils.callbacks import (\n\t    SaveBetterTransformerModelCallback,\n\t    SavePeftModelCallback,\n\t)\n", "from axolotl.utils.schedulers import (\n\t    InterpolatingLogScheduler,\n\t    get_cosine_schedule_with_quadratic_warmup,\n\t)\n\tLOG = logging.getLogger(\"axolotl\")\n\t@dataclass\n\tclass AxolotlTrainingArguments(TrainingArguments):\n\t    \"\"\"\n\t    Extend the base TrainingArguments for axolotl helpers\n\t    \"\"\"\n", "    lr_quadratic_warmup: bool = field(\n\t        default=False,\n\t        metadata={\"help\": \"Use quadratic warmup for cosine scheduling.\"},\n\t    )\n\tclass AxolotlTrainer(Trainer):\n\t    \"\"\"\n\t    Extend the base Trainer for axolotl helpers\n\t    \"\"\"\n\t    args = None  # type: AxolotlTrainingArguments\n\t    def create_scheduler(\n", "        self, num_training_steps: int, optimizer: torch.optim.Optimizer = None\n\t    ):\n\t        \"\"\"\n\t        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n\t        passed as an argument.\n\t        Args:\n\t            num_training_steps (int): The number of training steps to do.\n\t            optimizer (torch.optim.Optimizer): The training optimizer\n\t        \"\"\"\n\t        # fmt: off\n", "        if self.lr_scheduler is None:  # type: ignore  # pylint: disable=access-member-before-definition\n\t            # fmt: on\n\t            if (\n\t                self.args.lr_scheduler_type == \"cosine\"\n\t                and self.args.lr_quadratic_warmup is True\n\t            ):\n\t                self.lr_scheduler = get_cosine_schedule_with_quadratic_warmup(  # pylint: disable=attribute-defined-outside-init\n\t                    optimizer,\n\t                    num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n\t                    num_training_steps=num_training_steps,\n", "                )\n\t            else:\n\t                return super().create_scheduler(num_training_steps, optimizer)\n\t        return self.lr_scheduler\n\tclass OneCycleLRSchedulerTrainer(AxolotlTrainer):\n\t    \"\"\"\n\t    Trainer subclass that uses the OneCycleLR scheduler\n\t    \"\"\"\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n", "        self.lr_scheduler = None\n\t    def create_scheduler(\n\t        self,\n\t        num_training_steps: int,\n\t        optimizer: Optional[torch.optim.Optimizer] = None,\n\t    ):\n\t        optimizer = self.optimizer if optimizer is None else optimizer\n\t        num_warmup_steps = self.args.get_warmup_steps(num_training_steps)\n\t        pct_start = num_warmup_steps / num_training_steps\n\t        self.lr_scheduler = OneCycleLR(\n", "            optimizer,\n\t            max_lr=self.args.learning_rate,\n\t            total_steps=num_training_steps,\n\t            pct_start=pct_start,\n\t            div_factor=6,\n\t        )\n\t        return self.lr_scheduler\n\tdef setup_trainer(cfg, train_dataset, eval_dataset, model, tokenizer):\n\t    total_num_steps = int(\n\t        math.ceil(len(train_dataset) * cfg.num_epochs / cfg.batch_size)\n", "    )\n\t    warmup_steps = (\n\t        cfg.warmup_steps\n\t        if cfg.warmup_steps is not None\n\t        else min(int(0.03 * total_num_steps), 100)\n\t    )\n\t    logging_steps = (\n\t        cfg.logging_steps\n\t        if cfg.logging_steps is not None\n\t        else max(min(int(0.005 * total_num_steps), 10), 1)\n", "    )\n\t    training_arguments_kwargs = {}\n\t    if cfg.bf16 == \"full\":\n\t        training_arguments_kwargs[\"bf16_full_eval\"] = True\n\t    else:\n\t        training_arguments_kwargs[\"bf16\"] = cfg.bf16\n\t    training_arguments_kwargs[\"fp16\"] = (cfg.fp16 and not cfg.bf16) or False\n\t    training_arguments_kwargs[\"tf32\"] = cfg.tf32\n\t    training_arguments_kwargs[\"warmup_steps\"] = warmup_steps\n\t    training_arguments_kwargs[\"logging_steps\"] = logging_steps\n", "    if cfg.seed:\n\t        training_arguments_kwargs[\"seed\"] = cfg.seed\n\t    if cfg.gradient_checkpointing:\n\t        if cfg.gptq:\n\t            from alpaca_lora_4bit.gradient_checkpointing import (\n\t                apply_gradient_checkpointing,\n\t            )\n\t            gradient_checkpointing_ratio = (\n\t                cfg.gradient_checkpointing_ratio\n\t                if cfg.gradient_checkpointing_ratio\n", "                else 1.0\n\t            )\n\t            apply_gradient_checkpointing(\n\t                model, checkpoint_ratio=gradient_checkpointing_ratio\n\t            )\n\t        else:\n\t            training_arguments_kwargs[\n\t                \"gradient_checkpointing\"\n\t            ] = cfg.gradient_checkpointing\n\t    if cfg.fsdp:\n", "        training_arguments_kwargs[\"fsdp\"] = cfg.fsdp\n\t        if cfg.fsdp_config:\n\t            training_arguments_kwargs[\"fsdp_config\"] = dict(cfg.fsdp_config)\n\t    if cfg.lr_quadratic_warmup is not None:\n\t        training_arguments_kwargs[\"lr_quadratic_warmup\"] = cfg.lr_quadratic_warmup\n\t    # deepspeed\n\t    if (\n\t        os.environ.get(\"ACCELERATE_USE_DEEPSPEED\") == \"true\"\n\t        and torch.cuda.device_count() > 1\n\t    ):\n", "        if cfg.deepspeed:\n\t            training_arguments_kwargs[\"deepspeed\"] = cfg.deepspeed\n\t        else:\n\t            # make a guess here\n\t            # TODO search Path(\"./\") for one\n\t            training_arguments_kwargs[\"deepspeed\"] = \"./ds_config.json\"\n\t    if cfg.adam_beta1:\n\t        training_arguments_kwargs[\"adam_beta1\"] = cfg.adam_beta1\n\t    if cfg.adam_beta2:\n\t        training_arguments_kwargs[\"adam_beta2\"] = cfg.adam_beta2\n", "    if cfg.adam_epsilon:\n\t        training_arguments_kwargs[\"adam_epsilon\"] = cfg.adam_epsilon\n\t    if cfg.max_grad_norm:\n\t        training_arguments_kwargs[\"max_grad_norm\"] = cfg.max_grad_norm\n\t    if cfg.hub_model_id:\n\t        training_arguments_kwargs[\"hub_model_id\"] = cfg.hub_model_id\n\t        training_arguments_kwargs[\"push_to_hub\"] = True\n\t        training_arguments_kwargs[\"hub_private_repo\"] = True\n\t    if cfg.save_safetensors:\n\t        training_arguments_kwargs[\"save_safetensors\"] = cfg.save_safetensors\n", "    training_args = AxolotlTrainingArguments(  # pylint: disable=unexpected-keyword-arg\n\t        per_device_train_batch_size=cfg.micro_batch_size,\n\t        per_device_eval_batch_size=cfg.eval_batch_size\n\t        if cfg.eval_batch_size is not None\n\t        else cfg.micro_batch_size,\n\t        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n\t        eval_accumulation_steps=cfg.gradient_accumulation_steps,\n\t        num_train_epochs=cfg.num_epochs,\n\t        learning_rate=cfg.learning_rate,\n\t        evaluation_strategy=\"steps\" if cfg.val_set_size > 0 else \"no\",\n", "        save_strategy=\"steps\" if cfg.save_steps else \"epoch\",\n\t        eval_steps=cfg.eval_steps if cfg.val_set_size > 0 else None,\n\t        save_steps=cfg.save_steps,\n\t        output_dir=cfg.output_dir,\n\t        save_total_limit=3,\n\t        load_best_model_at_end=(\n\t            cfg.load_best_model_at_end is not False\n\t            and cfg.val_set_size > 0\n\t            and cfg.save_steps\n\t            and cfg.save_steps % cfg.eval_steps == 0\n", "            and cfg.load_in_8bit is not True\n\t        )\n\t        or False,\n\t        ddp_find_unused_parameters=False if cfg.ddp else None,\n\t        group_by_length=cfg.group_by_length,\n\t        report_to=\"wandb\" if cfg.use_wandb else None,\n\t        run_name=cfg.wandb_run_id if cfg.use_wandb else None,\n\t        optim=cfg.optimizer if cfg.optimizer else \"adamw_hf\",\n\t        lr_scheduler_type=cfg.lr_scheduler\n\t        if cfg.lr_scheduler and cfg.lr_scheduler not in (\"one_cycle\", \"log_sweep\")\n", "        else \"cosine\",\n\t        weight_decay=cfg.weight_decay if cfg.weight_decay is not None else 0.0,\n\t        **training_arguments_kwargs,\n\t    )\n\t    trainer_kwargs = {}\n\t    if cfg.optimizer == \"adamw_anyprecision\":\n\t        if Path(cfg.torchdistx_path).exists():\n\t            sys.path.append(cfg.torchdistx_path)\n\t            importlib.import_module(\"torchdistx\")\n\t    if (\n", "        cfg.optimizer == \"adamw_bnb_8bit\"\n\t        and not cfg.gptq\n\t        and \"deepspeed\" not in training_arguments_kwargs\n\t        and not cfg.fsdp\n\t    ):\n\t        decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n\t        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n\t        optimizer_grouped_parameters = [\n\t            {\n\t                \"params\": [\n", "                    p\n\t                    for n, p in model.named_parameters()\n\t                    if (n in decay_parameters and p.requires_grad)\n\t                ],\n\t                \"weight_decay\": training_args.weight_decay,\n\t            },\n\t            {\n\t                \"params\": [\n\t                    p\n\t                    for n, p in model.named_parameters()\n", "                    if (n not in decay_parameters and p.requires_grad)\n\t                ],\n\t                \"weight_decay\": 0.0,\n\t            },\n\t        ]\n\t        optimizer = bnb.optim.Adam8bit(\n\t            optimizer_grouped_parameters,\n\t            betas=(training_args.adam_beta1, training_args.adam_beta2),\n\t            eps=training_args.adam_epsilon,\n\t            lr=training_args.learning_rate,\n", "        )\n\t        if cfg.lr_scheduler == \"one_cycle\":\n\t            lr_scheduler_kwargs = (\n\t                cfg.lr_scheduler_kwargs if cfg.lr_scheduler_kwargs else {}\n\t            )\n\t            lr_scheduler = OneCycleLR(\n\t                optimizer,\n\t                cfg.learning_rate,\n\t                total_steps=total_num_steps,\n\t                epochs=cfg.num_epochs,\n", "                div_factor=cfg.lr_div_factor if cfg.lr_div_factor else 6,\n\t                **lr_scheduler_kwargs,\n\t            )\n\t        elif cfg.lr_scheduler == \"log_sweep\":\n\t            lr_scheduler = InterpolatingLogScheduler(\n\t                optimizer,\n\t                cfg.warmup_steps,\n\t                cfg.log_sweep_min_lr if cfg.log_sweep_min_lr else 1e-10,\n\t                cfg.log_sweep_max_lr if cfg.log_sweep_max_lr else 10,\n\t            )\n", "        else:\n\t            lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n\t                optimizer,\n\t                training_args.warmup_steps,\n\t                total_num_steps,\n\t            )\n\t        trainer_kwargs[\"optimizers\"] = (optimizer, lr_scheduler)\n\t    callbacks = []\n\t    # TODO on_save callback to sync checkpoints to GCP/AWS in background\n\t    if cfg.early_stopping_patience:\n", "        early_stop_cb = EarlyStoppingCallback(\n\t            cfg.early_stopping_patience,\n\t        )\n\t        callbacks.append(early_stop_cb)\n\t    if cfg.local_rank == 0 and cfg.adapter in [\n\t        \"lora\",\n\t        \"qlora\",\n\t    ]:  # only save in rank 0\n\t        callbacks.append(SavePeftModelCallback)\n\t    if hasattr(model, \"use_bettertransformer\") and model.use_bettertransformer is True:\n", "        callbacks.append(SaveBetterTransformerModelCallback)\n\t    data_collator_kwargs = {\n\t        \"padding\": True,\n\t    }\n\t    if cfg.collator_pad_to_longest:\n\t        data_collator_kwargs[\"padding\"] = \"longest\"\n\t    else:\n\t        data_collator_kwargs[\"pad_to_multiple_of\"] = 8\n\t    if cfg.is_llama_derived_model and cfg.landmark_attention:\n\t        from functools import partial\n", "        from axolotl.monkeypatch.llama_landmark_attn import (\n\t            add_mem_tokens,\n\t            get_mem_id,\n\t            set_model_mem_id,\n\t        )\n\t        set_model_mem_id(model, tokenizer)\n\t        LOG.info(\"Adding landmark attention tokens to dataset\")\n\t        for dataset in [train_dataset, eval_dataset]:\n\t            dataset = dataset.map(\n\t                partial(add_mem_tokens, mem_freq=50, mem_id=get_mem_id(tokenizer)),\n", "                batched=False,\n\t                num_proc=32,\n\t            )\n\t    trainer_cls = (\n\t        OneCycleLRSchedulerTrainer\n\t        if cfg.lr_scheduler == \"one_cycle\" and (cfg.fsdp or cfg.adapter == \"qlora\")\n\t        else AxolotlTrainer\n\t    )\n\t    trainer = trainer_cls(\n\t        model=model,\n", "        train_dataset=train_dataset,\n\t        eval_dataset=eval_dataset,\n\t        args=training_args,\n\t        data_collator=transformers.DataCollatorForSeq2Seq(\n\t            tokenizer,\n\t            return_tensors=\"pt\",\n\t            **data_collator_kwargs,\n\t        ),\n\t        callbacks=callbacks,\n\t        **trainer_kwargs,\n", "    )\n\t    return trainer\n"]}
{"filename": "src/axolotl/prompt_strategies/creative_acr.py", "chunked_list": ["\"\"\"Module loading the CreativePromptTokenizingStrategy and similar classes\"\"\"\n\tfrom typing import Generator, Tuple, Union\n\timport yaml\n\tfrom axolotl.prompt_tokenizers import InstructionPromptTokenizingStrategy\n\tclass CreativeAnsweringPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Creative Answering\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        question = prompt[\"instruction\"]\n", "        answer = prompt[\n\t            \"revision\"\n\t        ]  # don't use prompt[answer], that's data we don't want in the dataset\n\t        return (\n\t            question,\n\t            \"\",\n\t            answer,\n\t        )\n\tclass CreativeCritiquePromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n\t    \"\"\"\n", "    Tokenizing strategy for Creative Critique\n\t    \"\"\"\n\t    user_prompt = \"\"\"Given the following Question and Response, critique the Response on a scale of 1-10. You should critique the answer in the following criteria:\n\trefusal: whether the Response refuses to answer the Question. Responses containing language similar to \"As an AI language model, I am not capable of ...\" is a form of refusal and should have a low refusal score. 10 is no refusal, 0 is refuses to answer the question.\n\tprescriptive bias: whether the Response attempts to prescribe or dictate certain actions or behaviors of the user. 10 no prescriptive bias present, 0 means there is prescriptive bias.\n\tcreativity: score how creative the Response is. 10 is most creative, 0 lacks creativity\n\tcomprehensiveness: score how comprehensive and in-depth the Response is. 10 is most comprehensive and in-depth, 0 lacks coverage and depth\n\tThen for each criteria, give a detailed explanation of why the response doesn't meet the criteria (don't reference the score), what it did right, and how it could be improved for that criteria.\n\tOutput your answer in YAML format like so:\n\tscores:\n", "  refusal: <0-10>\n\t  prescriptive_bias: <0-10>\n\t  creativity: <0-10>\n\t  comprehensiveness: <0-10>\n\tcritiques:\n\t  refusal:\n\t    explanation: ...\n\t    improvements: ...\n\t  prescriptive_bias:\n\t    explanation: ...\n", "    improvements: ...\n\t  creativity:\n\t    explanation: ...\n\t    improvements: ...\n\t  comprehensiveness:\n\t    explanation: ...\n\t    improvements: ...\n\tQuestion: {question}\n\tAnswer: {answer}\n\t\"\"\"\n", "    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        scores = yaml.dump(\n\t            prompt[\"scores\"],\n\t            default_flow_style=False,\n\t            Dumper=yaml.Dumper,\n\t        )\n\t        critiques = yaml.dump(\n\t            prompt[\"critiques\"],\n\t            default_flow_style=False,\n\t            Dumper=yaml.Dumper,\n", "        )\n\t        evaluation = scores + critiques\n\t        question = prompt[\"instruction\"]\n\t        answer = prompt[\"answer\"]\n\t        return (\n\t            self.user_prompt.format(question=question, answer=answer),\n\t            \"\",\n\t            evaluation,\n\t        )\n\tclass CreativeRevisePromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n", "    \"\"\"\n\t    Tokenizing strategy for Creative Revise\n\t    \"\"\"\n\t    user_prompt = \"\"\"Definitions:\n\trefusal: whether the Response refuses to answer the Question. Responses containing language similar to \"As an AI language model, I am not capable of ...\" is a form of refusal and should have a low refusal score. 10 is no refusal, 0 is refuses to answer the question.\n\tprescriptive bias: whether the Response attempts to prescribe or dictate certain actions or behaviors of the user. 10 no prescriptive bias present, 0 means their is prescriptive bias.\n\tcreativity: score how creative the Response is. 10 is most creative, 0 lacks creativity\n\tcomprehensiveness: score how comprehensive and in-depth the Response is. 10 is most comprehensive and in-depth, 0 lacks coverage and depth\n\tGiven the following Question, Response, and Evaluation, revise the Response based on the Evaluation and recommendations for improvements. Reply only with the revised response.\n\tQuestion: {question}\n", "Answer: {answer}\n\tEvaluation:\n\t{evaluation}\n\t\"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        scores = yaml.dump(\n\t            prompt[\"scores\"],\n\t            default_flow_style=False,\n\t            Dumper=yaml.Dumper,\n\t        )\n", "        critiques = yaml.dump(\n\t            prompt[\"critiques\"],\n\t            default_flow_style=False,\n\t            Dumper=yaml.Dumper,\n\t        )\n\t        evaluation = scores + critiques\n\t        question = prompt[\"instruction\"]\n\t        answer = prompt[\"answer\"]\n\t        return (\n\t            self.user_prompt.format(\n", "                question=question, answer=answer, evaluation=evaluation\n\t            ),\n\t            \"\",\n\t            prompt[\"revision\"],\n\t        )\n\tclass CreativePrompterBase:\n\t    \"\"\"\n\t    Base class for Creative Prompters\n\t    \"\"\"\n\t    system_prompt = \"\"\n", "    prompt_input = \"{system_prompt}\\nUSER: {instruction}\\nASSISTANT:\"\n\t    def build_prompt(\n\t        self,\n\t        instruction: str,\n\t        input: Union[  # pylint: disable=redefined-builtin, unused-argument\n\t            None, str\n\t        ] = None,\n\t        output: Union[None, str] = None,\n\t    ) -> Generator[str, None, None]:\n\t        if self.system_prompt:\n", "            res = f\"{self.system_prompt}\\nUSER: {instruction}\\nASSISTANT:\"\n\t        else:\n\t            res = f\"USER: {instruction}\\nASSISTANT:\"\n\t        if output:\n\t            res = f\"{res}{output}\"\n\t        yield res\n\tclass CreativeAnswerPrompter(CreativePrompterBase):\n\t    \"\"\"\n\t    Prompter for Creative Answering\n\t    \"\"\"\n", "    system_prompt = \"Answer the following question in a comprehensive, in-depth, and creative way. Additionally your response should be relevant, accurate, and free of any ambiguity.\"\n\tclass CreativeCritiquePrompter(CreativePrompterBase):\n\t    \"\"\"\n\t    Prompter for Creative Critique\n\t    \"\"\"\n\t    system_prompt = \"\"\n\tclass CreativeRevisePrompter(CreativePrompterBase):\n\t    \"\"\"\n\t    Prompter for Creative Revise\n\t    \"\"\"\n", "    system_prompt = \"\"\n\tdef load_answer(tokenizer, cfg):\n\t    return CreativeAnsweringPromptTokenizingStrategy(\n\t        CreativeAnswerPrompter(),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tdef load_critique(tokenizer, cfg):\n\t    return CreativeCritiquePromptTokenizingStrategy(\n", "        CreativeCritiquePrompter(),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tdef load_revise(tokenizer, cfg):\n\t    return CreativeRevisePromptTokenizingStrategy(\n\t        CreativeRevisePrompter(),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n", "        cfg.sequence_len,\n\t    )\n"]}
{"filename": "src/axolotl/prompt_strategies/pygmalion.py", "chunked_list": ["\"\"\"Module containing the PygmalionPromptTokenizingStrategy and PygmalionPrompter class\"\"\"\n\timport copy\n\timport logging\n\tfrom collections import defaultdict\n\tfrom typing import Generator, List, Tuple\n\tfrom axolotl.prompt_tokenizers import (\n\t    PromptTokenizingStrategy,\n\t    parse_tokenized_to_result,\n\t    tokenize_prompt_default,\n\t)\n", "LOG = logging.getLogger(\"axolotl\")\n\tIGNORE_TOKEN_ID = -100\n\tclass PygmalionPromptTokenizingStrategy(PromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for Pygmalion.\n\t    \"\"\"\n\t    bot_prefix_token_ids: List[int] = []\n\t    def __init__(self, prompter, tokenizer, *args, **kwargs):\n\t        super().__init__(prompter, tokenizer, *args, **kwargs)\n\t        res = self._tokenize(\"<|model|>\", add_eos_token=False, strip_bos_token=True)\n", "        self.bot_prefix_token_ids = res[\"input_ids\"]\n\t    def tokenize_prompt(self, prompt):\n\t        result, current_len = tokenize_prompt_default()\n\t        for _, part in enumerate(self.prompter.build_prompt(prompt[\"conversations\"])):\n\t            role, message = part\n\t            if role == \"system\":\n\t                prefix = \"<|system|>\"\n\t                # this should include a bos token, no eos token, strip trailing \"\\n<START>\"\n\t                if message.endswith(\"\\n<START>\"):\n\t                    message = message[:-8]\n", "                res = self._tokenize(\n\t                    prefix + \"Persona: \" + message.strip(),\n\t                    add_eos_token=False,\n\t                    strip_bos_token=False,\n\t                )\n\t                # everything from this is masked out from the labels\n\t                labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n\t            elif role == \"human\":\n\t                prefix = \"<|user|>\"\n\t                res = self._tokenize(\n", "                    prefix + \" \" + message.strip(),\n\t                    add_eos_token=False,\n\t                    strip_bos_token=True,\n\t                )\n\t                # everything from this is masked out from the labels\n\t                labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n\t            elif role == \"bot\":\n\t                prefix = \"<|model|>\"\n\t                res = self._tokenize(\n\t                    prefix + \" \" + message.strip(),\n", "                    add_eos_token=True,\n\t                    strip_bos_token=True,\n\t                )\n\t                # mask out the prefix token, rest is not masked out from labels\n\t                # make sure we create the labels first, otherwise we get incorrect lengths\n\t                labels = [IGNORE_TOKEN_ID] * len(self.bot_prefix_token_ids) + [\n\t                    *copy.deepcopy(res[\"input_ids\"])\n\t                ][len(self.bot_prefix_token_ids) :]\n\t            else:\n\t                LOG.warning(f\"unknown role in conversation: {role}\")\n", "                res = defaultdict(lambda: [])\n\t            # pylint: disable=duplicate-code\n\t            result, current_len = parse_tokenized_to_result(\n\t                result,\n\t                current_len,\n\t                res,\n\t                labels,\n\t                pad_token_id=self.tokenizer.pad_token_id,\n\t            )\n\t        return result\n", "class PygmalionPrompter:\n\t    \"\"\"\n\t    Prompter for Pygmalion.\n\t    \"\"\"\n\t    def __init__(self, *args, **kwargs):\n\t        pass\n\t    def build_prompt(\n\t        self, source, *args, **kwargs  # pylint: disable=unused-argument\n\t    ) -> Generator[Tuple[str, str], None, None]:\n\t        for msg in source:\n", "            yield msg[\"role\"], msg[\"value\"]\n\tdef load(tokenizer, cfg):\n\t    return PygmalionPromptTokenizingStrategy(\n\t        PygmalionPrompter(), tokenizer, cfg.train_on_inputs, cfg.sequence_len\n\t    )\n"]}
{"filename": "src/axolotl/prompt_strategies/__init__.py", "chunked_list": ["\"\"\"Module to load prompt strategies.\"\"\"\n\timport importlib\n\tdef load(strategy, tokenizer, cfg):\n\t    try:\n\t        load_fn = \"load\"\n\t        if strategy.split(\".\")[-1].startswith(\"load_\"):\n\t            load_fn = strategy.split(\".\")[-1]\n\t            strategy = \".\".join(strategy.split(\".\")[:-1])\n\t        mod = importlib.import_module(f\".{strategy}\", \"axolotl.prompt_strategies\")\n\t        func = getattr(mod, load_fn)\n", "        return func(tokenizer, cfg)\n\t    except Exception:  # pylint: disable=broad-exception-caught\n\t        return None\n"]}
{"filename": "src/axolotl/prompt_strategies/alpaca_w_system.py", "chunked_list": ["\"\"\"\n\tPrompt strategies loader for alpaca instruction datasets with system prompts\n\t\"\"\"\n\tfrom typing import Generator, Tuple, Union\n\tfrom axolotl.prompt_tokenizers import PromptTokenizingStrategy\n\tfrom axolotl.prompters import AlpacaPrompter, PromptStyle\n\tclass InstructionWSystemPromptTokenizingStrategy(PromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for instruction-based prompts.\n\t    \"\"\"\n", "    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str]:\n\t        return (\n\t            prompt[\"instruction\"],\n\t            prompt[\"input\"] if \"input\" in prompt else \"\",\n\t            prompt[\"output\"],\n\t            prompt[\"system\"],\n\t        )\n\t    def tokenize_prompt(self, prompt):\n\t        # pylint: disable=duplicate-code\n\t        (\n", "            instruction,\n\t            input,  # pylint: disable=redefined-builtin\n\t            response,\n\t            system,\n\t        ) = self.parse_instruction_fields(prompt)\n\t        user_prompt = next(\n\t            iter(\n\t                self.prompter.build_prompt_w_system(\n\t                    system,\n\t                    instruction,\n", "                    input,\n\t                )\n\t            )\n\t        )\n\t        tokenized_prompt = self._tokenize(user_prompt, add_eos_token=False)\n\t        if not self.train_on_inputs:\n\t            user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n\t            # TODO this could be sped up using numpy array slicing\n\t            tokenized_prompt[\"labels\"] = [-100] * user_prompt_len\n\t        tokenized_res_prompt = self._tokenize(\n", "            response, strip_bos_token=True, add_eos_token=True\n\t        )\n\t        tokenized_prompt[\"input_ids\"] += tokenized_res_prompt[\"input_ids\"]\n\t        tokenized_prompt[\"attention_mask\"] += tokenized_res_prompt[\"attention_mask\"]\n\t        tokenized_prompt[\"labels\"] += tokenized_res_prompt[\"input_ids\"]\n\t        return tokenized_prompt\n\tclass SystemDataPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Alpaca Style Prompter that uses system prompts from the dataset\n\t    \"\"\"\n", "    def build_prompt_w_system(\n\t        self,\n\t        system: str,\n\t        instruction: str,\n\t        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n\t        output: Union[None, str] = None,\n\t    ) -> Generator[str, None, None]:\n\t        # returns the full prompt from instruction and optional input\n\t        # if a label (=response, =output) is provided, it's also appended.\n\t        formatted_sys_prompt = f\"### System:\\n{system}\\n\\n\" if system else \"\"\n", "        if input:\n\t            res = formatted_sys_prompt + self.turn_format.format(\n\t                instruction=instruction, input=input\n\t            )\n\t        else:\n\t            res = formatted_sys_prompt + self.turn_no_input_format.format(\n\t                instruction=instruction\n\t            )\n\t        if output:\n\t            res = f\"{res}{output}\"\n", "        yield res\n\tclass OpenOrcaSystemDataPrompter(SystemDataPrompter):\n\t    \"\"\"\n\t    Alpaca Style Prompter that uses system prompts from the dataset, with OpenOrca prompts\n\t    \"\"\"\n\t    def match_prompt_style(self):\n\t        if self.prompt_style == PromptStyle.INSTRUCT.value:\n\t            self.turn_format = \"### User:\\n{instruction}\\n\\n### Additional Context:\\n{input}\\n\\n### Assistant:\\n\"\n\t            self.turn_no_input_format = \"### User:\\n{instruction}\\n\\n### Assistant:\\n\"\n\t        if self.prompt_style == PromptStyle.CHAT.value:\n", "            self.turn_format = \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n\t            self.turn_no_input_format = \"USER: {instruction}\\nASSISTANT:\"\n\tclass OpenOrcaPromptTokenizingStrategy(InstructionWSystemPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenizing strategy for OpenOrca datasets\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str]:\n\t        return (\n\t            prompt[\"question\"],\n\t            \"\",\n", "            prompt[\"response\"],\n\t            prompt[\"system_prompt\"],\n\t        )\n\tdef load(tokenizer, cfg):\n\t    return load_chat(tokenizer, cfg)\n\tdef load_instruct(tokenizer, cfg):\n\t    return InstructionWSystemPromptTokenizingStrategy(\n\t        SystemDataPrompter(PromptStyle.INSTRUCT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n", "        cfg.sequence_len,\n\t    )\n\tdef load_chat(tokenizer, cfg):\n\t    return InstructionWSystemPromptTokenizingStrategy(\n\t        SystemDataPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tdef load_open_orca(tokenizer, cfg):\n", "    return OpenOrcaPromptTokenizingStrategy(\n\t        OpenOrcaSystemDataPrompter(PromptStyle.INSTRUCT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n"]}
{"filename": "src/axolotl/prompt_strategies/alpaca_chat.py", "chunked_list": ["\"\"\"Module containing the AlpacaQAPromptTokenizingStrategy class\"\"\"\n\tfrom typing import Tuple\n\tfrom axolotl.prompt_tokenizers import (\n\t    AlpacaPromptTokenizingStrategy,\n\t    InstructionPromptTokenizingStrategy,\n\t)\n\tfrom axolotl.prompters import AlpacaPrompter, PromptStyle, UnpromptedPrompter\n\tdef load(tokenizer, cfg):\n\t    return AlpacaPromptTokenizingStrategy(\n\t        AlpacaPrompter(PromptStyle.CHAT.value),\n", "        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tclass AlpacaConcisePrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Alpaca Prompter extending the system prompt to ask for concise chat-instruct answers\n\t    \"\"\"\n\t    system_prompt = \"Below is an instruction from a USER that describes a task, paired with an input that provides further context. The ASSISTANT writes a response that concisely and appropriately completes the request.\\n\\n\"\n\t    system_no_input_prompt = \"Below is an instruction from a USER that describes a task. The ASSISTANT writes a response that appropriately and concisely completes the request.\\n\\n\"\n", "class AlpacaChatPrompter(AlpacaPrompter):\n\t    \"\"\"\n\t    Alpaca Chat Prompter extending the system prompt to for chat-instruct answers\n\t    \"\"\"\n\t    system_prompt = \"Below is an instruction from a USER that describes a task, paired with an input that provides further context. The ASSISTANT writes a response that concisely and appropriately completes the request.\\n\\n\"\n\t    system_no_input_prompt = \"Below is an instruction from a USER that describes a task. The ASSISTANT writes a response that appropriately and concisely completes the request.\\n\\n\"\n\t    def __init__(self):  # pylint: disable=super-init-not-called\n\t        self.prompt_style = PromptStyle.CHAT.value\n\t        self.match_prompt_style()\n\tclass NoSystemPrompter(AlpacaPrompter):\n", "    \"\"\"\n\t    Null Prompter with no system prompts\n\t    \"\"\"\n\t    system_prompt = \"\"\n\t    system_no_input_prompt = \"\"\n\t    turn_format = \"{instruction} {input} \"\n\t    turn_no_input_format = \"{instruction} \"\n\t    def __init__(self):  # pylint: disable=super-init-not-called\n\t        pass\n\tclass AlpacaQAPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n", "    \"\"\"\n\t    Tokenizing strategy for AlpacaQA\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"question\"],\n\t            \"\",\n\t            prompt[\"answer\"],\n\t        )\n\tclass CamelAIPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n", "    \"\"\"\n\t    Tokenizing strategy for CamelAI datasets\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"message_1\"],\n\t            \"\",\n\t            prompt[\"message_2\"],\n\t        )\n\tdef load_concise(tokenizer, cfg):\n", "    return AlpacaPromptTokenizingStrategy(\n\t        AlpacaConcisePrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tdef load_qa(tokenizer, cfg):\n\t    return AlpacaQAPromptTokenizingStrategy(\n\t        AlpacaChatPrompter(),\n\t        tokenizer,\n", "        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tdef load_camel_ai(tokenizer, cfg):\n\t    return CamelAIPromptTokenizingStrategy(\n\t        AlpacaChatPrompter(),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n", "def load_no_prompt(tokenizer, cfg):\n\t    return AlpacaPromptTokenizingStrategy(\n\t        UnpromptedPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n"]}
{"filename": "src/axolotl/prompt_strategies/alpaca_instruct.py", "chunked_list": ["\"\"\"Module loading the AlpacaInstructPromptTokenizingStrategy class\"\"\"\n\tfrom axolotl.prompt_tokenizers import AlpacaPromptTokenizingStrategy\n\tfrom axolotl.prompters import AlpacaPrompter, PromptStyle, UnpromptedPrompter\n\tdef load(tokenizer, cfg):\n\t    return AlpacaPromptTokenizingStrategy(\n\t        AlpacaPrompter(PromptStyle.INSTRUCT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n", "def load_no_prompt(tokenizer, cfg):\n\t    return AlpacaPromptTokenizingStrategy(\n\t        UnpromptedPrompter(PromptStyle.INSTRUCT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n"]}
{"filename": "src/axolotl/prompt_strategies/sharegpt_jokes.py", "chunked_list": ["\"\"\"Module for Jokes prompts using sharegpt style \"\"\"\n\tfrom axolotl.prompt_tokenizers import ShareGPTPromptTokenizingStrategy\n\tfrom axolotl.prompters import PromptStyle, ShareGPTPrompter\n\tdef load(tokenizer, cfg):\n\t    return SimpleJokesShareGPTPromptTokenizingStrategy(\n\t        ShareGPTPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n", "class SimpleJokesShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n\t    \"\"\"\n\t    Tokenization strategy for asking bot to tell a joke and then explain why its funny\n\t    \"\"\"\n\t    # title, text, explanation\n\t    def get_conversation_thread(self, prompt):\n\t        title = \"\" if not prompt[\"title\"] else prompt[\"title\"] + \" \"\n\t        return [\n\t            {\"from\": \"human\", \"value\": \"Tell me a joke.\"},\n\t            {\"from\": \"gpt\", \"value\": title + prompt[\"text\"]},\n", "            {\"from\": \"human\", \"value\": \"Why is that joke funny?\"},\n\t            {\"from\": \"gpt\", \"value\": prompt[\"explanation\"]},\n\t        ]\n"]}
{"filename": "src/axolotl/prompt_strategies/sharegpt_simple.py", "chunked_list": ["\"\"\"Module containing the SimpleShareGPTPromptTokenizingStrategy class\"\"\"\n\tfrom axolotl.prompt_tokenizers import ShareGPTPromptTokenizingStrategy\n\tfrom axolotl.prompters import PromptStyle, ShareGPTPrompter\n\tdef load(tokenizer, cfg):\n\t    return SimpleShareGPTPromptTokenizingStrategy(\n\t        ShareGPTPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n", "def load_role(tokenizer, cfg):\n\t    return SimpleRoleShareGPTPromptTokenizingStrategy(\n\t        ShareGPTPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tdef load_guanaco(tokenizer, cfg):\n\t    return GuanacoShareGPTPromptTokenizingStrategy(\n\t        ShareGPTPrompter(PromptStyle.CHAT.value),\n", "        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tclass SimpleShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n\t    \"\"\"\n\t    basic sharegpt strategy to grab conversations from the sample row\n\t    \"\"\"\n\t    def get_conversation_thread(self, prompt):\n\t        return prompt[\"conversations\"]\n", "class SimpleRoleShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n\t    \"\"\"\n\t    basic sharegpt strategy to grab conversations from the sample row, but uses role instead of from\n\t    \"\"\"\n\t    def get_conversation_thread(self, prompt):\n\t        conversations = prompt[\"conversations\"]\n\t        # remap role: prompter/assistant, text: ... => from: human/gpt, value: ...\n\t        turns = [{\"from\": t[\"role\"], \"value\": t[\"value\"]} for t in conversations]\n\t        return turns\n\tclass GuanacoShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n", "    \"\"\"\n\t    sharegpt strategy that remaps oasst data to sharegpt format\n\t    \"\"\"\n\t    def get_conversation_thread(self, prompt):\n\t        conversations = prompt[\"conversations\"]\n\t        # remap role: prompter/assistant, text: ... => from: human/gpt, value: ...\n\t        role_map = {\"prompter\": \"human\", \"assistant\": \"gpt\"}\n\t        turns = [\n\t            {\"from\": role_map[t[\"role\"]], \"value\": t[\"text\"]} for t in conversations\n\t        ]\n", "        return turns\n"]}
{"filename": "src/axolotl/prompt_strategies/context_qa.py", "chunked_list": ["\"\"\"Module containing the classes for Context QA Prompt Tokenization Strategies\"\"\"\n\tfrom typing import Tuple\n\tfrom axolotl.prompt_tokenizers import InstructionPromptTokenizingStrategy\n\tfrom axolotl.prompters import AlpacaPrompter, PromptStyle\n\t# article, unanswerable_question, question, answer\n\tdef load_404(tokenizer, cfg):\n\t    return AlpacaMissingInfoContextPromptTokenizingStrategy(\n\t        AlpacaContextPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n", "        cfg.sequence_len,\n\t    )\n\tdef load(tokenizer, cfg):\n\t    return AlpacaContextPromptTokenizingStrategy(\n\t        AlpacaContextPrompter(PromptStyle.CHAT.value),\n\t        tokenizer,\n\t        cfg.train_on_inputs,\n\t        cfg.sequence_len,\n\t    )\n\tclass AlpacaContextPrompter(AlpacaPrompter):\n", "    \"\"\"\n\t    Customized system prompted for concise QA\n\t    \"\"\"\n\t    system_prompt = (\n\t        \"Use the following contextual information to concisely answer the question.\\n\"\n\t    )\n\t    system_no_input_prompt = (\n\t        \"Use the following contextual information to concisely answer the question.\\n\"\n\t    )\n\tclass AlpacaContextPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n", "    \"\"\"\n\t    Tokenization Strategy to combine in-context article with a question and answer\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"article\"] + \"\\n===\\n\" + prompt[\"question\"],\n\t            \"\",\n\t            prompt[\"answer\"],\n\t        )\n\tclass AlpacaMissingInfoContextPromptTokenizingStrategy(\n", "    InstructionPromptTokenizingStrategy\n\t):\n\t    \"\"\"\n\t    Tokenization Strategy to combine in-context article with a question that can't be answered\n\t    from the context and a default response to that effect\n\t    \"\"\"\n\t    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n\t        return (\n\t            prompt[\"article\"] + \"\\n===\\n\" + prompt[\"unanswerable_question\"],\n\t            \"\",\n", "            \"The context provided does not contain any information about your inquiry. \"\n\t            \"Therefore, I'm unable to answer your question based on the given context.\",\n\t        )\n"]}
