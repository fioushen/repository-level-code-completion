{"filename": "scripts/stereo_image_proc_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for stereo_image_proc::DisparityNode.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    1. PrepLeftResizeNode, PrepRightResizeNode: resizes images to quarter HD\n\t- Graph under Test:\n\t    1. DisparityNode: creates disparity images from stereo pair\n\tRequired:\n\t- Packages:\n\t    - stereo_image_proc\n\t- Datasets:\n\t    - assets/datasets/r2b_dataset/r2b_hideaway\n\t\"\"\"\n\timport os\n", "from launch_ros.actions import ComposableNodeContainer\n\tfrom launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.QUARTER_HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_hideaway'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking stereo_image_proc::DisparityNode.\"\"\"\n\t    env = os.environ.copy()\n\t    env['OSPL_VERBOSITY'] = '8'  # 8 = OS_NONE\n", "    # bare minimum formatting for console output matching\n\t    env['RCUTILS_CONSOLE_OUTPUT_FORMAT'] = '{message}'\n\t    disparity_node = ComposableNode(\n\t        name='DisparityNode',\n\t        namespace=TestDisparityNode.generate_namespace(),\n\t        package='stereo_image_proc',\n\t        plugin='stereo_image_proc::DisparityNode',\n\t        parameters=[{\n\t                'queue_size': 500,\n\t                'approx': False,\n", "                'use_system_default_qos': False\n\t        }],\n\t        remappings=[\n\t            ('/left/camera_info', '/left/camera_info'),\n\t            ('/right/camera_info', '/right/camera_info'),\n\t            ('/left/image_rect', '/left/image_rect'),\n\t            ('/right/image_rect', '/right/image_rect')],\n\t    )\n\t    data_loader_node = ComposableNode(\n\t        name='DataLoaderNode',\n", "        namespace=TestDisparityNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n\t                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info'),\n\t                    ('hawk_0_right_rgb_image', 'data_loader/right_image'),\n\t                    ('hawk_0_right_rgb_camera_info', 'data_loader/right_camera_info')]\n\t    )\n\t    prep_left_resize_node = ComposableNode(\n\t        name='PrepLeftResizeNode',\n", "        namespace=TestDisparityNode.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/left_image'),\n", "            ('image/camera_info', 'data_loader/left_camera_info'),\n\t            ('resize/image_raw', 'buffer/left/image_resized'),\n\t            ('resize/camera_info', 'buffer/left/camera_info_resized'),\n\t        ]\n\t    )\n\t    prep_right_resize_node = ComposableNode(\n\t        name='PrepRightResizeNode',\n\t        namespace=TestDisparityNode.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n", "        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/right_image'),\n\t            ('image/camera_info', 'data_loader/right_camera_info'),\n\t            ('resize/image_raw', 'buffer/right/image_resized'),\n\t            ('resize/camera_info', 'buffer/right/camera_info_resized'),\n", "        ]\n\t    )\n\t    playback_node = ComposableNode(\n\t        name='PlaybackNode',\n\t        namespace=TestDisparityNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        parameters=[{\n\t            'data_formats': [\n\t                'sensor_msgs/msg/Image',\n", "                'sensor_msgs/msg/CameraInfo',\n\t                'sensor_msgs/msg/Image',\n\t                'sensor_msgs/msg/CameraInfo'],\n\t        }],\n\t        remappings=[('buffer/input0', 'buffer/left/image_resized'),\n\t                    ('input0', 'left/image_rect'),\n\t                    ('buffer/input1', 'buffer/left/camera_info_resized'),\n\t                    ('input1', 'left/camera_info'),\n\t                    ('buffer/input2', 'buffer/right/image_resized'),\n\t                    ('input2', 'right/image_rect'),\n", "                    ('buffer/input3', 'buffer/right/camera_info_resized'),\n\t                    ('input3', 'right/camera_info')],\n\t    )\n\t    monitor_node = ComposableNode(\n\t        name='MonitorNode',\n\t        namespace=TestDisparityNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n\t            'monitor_data_format': 'stereo_msgs/msg/DisparityImage',\n", "        }],\n\t        remappings=[\n\t            ('output', 'disparity')],\n\t    )\n\t    composable_node_container = ComposableNodeContainer(\n\t        name='container',\n\t        namespace=TestDisparityNode.generate_namespace(),\n\t        package='rclcpp_components',\n\t        executable='component_container_mt',\n\t        prefix=container_prefix,\n", "        sigterm_timeout=container_sigterm_timeout,\n\t        composable_node_descriptions=[\n\t            data_loader_node,\n\t            prep_left_resize_node,\n\t            prep_right_resize_node,\n\t            playback_node,\n\t            monitor_node,\n\t            disparity_node\n\t        ],\n\t        output='screen'\n", "    )\n\t    return [composable_node_container]\n\tdef generate_test_description():\n\t    return TestDisparityNode.generate_test_description_with_nsys(launch_setup)\n\tclass TestDisparityNode(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for stereo_image_proc::DisparityNode.\"\"\"\n\t    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='stereo_image_proc::DisparityNode Benchmark',\n\t        input_data_path=ROSBAG_PATH,\n", "        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=100.0,\n\t        publisher_lower_frequency=10.0,\n\t        # The number of frames to be buffered\n\t        playback_message_buffer_size=50,\n\t        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n\t    )\n\t    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "scripts/image_transport_h264_encoder_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for image_transport H264 encoder node.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    1. PrepResizeNode: resizes images to full HD\n\t- Graph under Test:\n\t    1. RepublishNode: encodes images to h264\n\tRequired:\n\t- Packages:\n\t    - ros2_h264_encoder\n\t    - h264_msgs\n\t- Datasets:\n\t    - assets/datasets/r2b_dataset/r2b_mezzanine\n\t\"\"\"\n", "from launch_ros.actions import ComposableNodeContainer, Node\n\tfrom launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.FULL_HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_mezzanine'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking image_transport H264 encoder node.\"\"\"\n\t    republish_node = Node(\n\t        name='RepublishNode',\n", "        namespace=TestH264EncoderNode.generate_namespace(),\n\t        package='image_transport',\n\t        executable='republish',\n\t        arguments=['raw', 'h264'],\n\t        remappings=[\n\t            ('in', 'image_raw'),\n\t            ('out/h264', 'image_h264'),\n\t        ],\n\t    )\n\t    data_loader_node = ComposableNode(\n", "        name='DataLoaderNode',\n\t        namespace=TestH264EncoderNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n\t                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info')]\n\t    )\n\t    prep_resize_node = ComposableNode(\n\t        name='PrepResizeNode',\n\t        namespace=TestH264EncoderNode.generate_namespace(),\n", "        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/left_image'),\n\t            ('image/camera_info', 'data_loader/left_camera_info'),\n", "            ('resize/image_raw', 'buffer/image_raw'),\n\t            ('resize/camera_info', 'buffer/camera_info'),\n\t        ]\n\t    )\n\t    playback_node = ComposableNode(\n\t        name='PlaybackNode',\n\t        namespace=TestH264EncoderNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        parameters=[{\n", "            'data_formats': [\n\t                'sensor_msgs/msg/Image'],\n\t        }],\n\t        remappings=[('buffer/input0', 'buffer/image_raw'),\n\t                    ('input0', 'image_raw')]\n\t    )\n\t    monitor_node = ComposableNode(\n\t        name='MonitorNode',\n\t        namespace=TestH264EncoderNode.generate_namespace(),\n\t        package='ros2_benchmark',\n", "        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n\t            'monitor_data_format': 'h264_msgs/msg/Packet',\n\t        }],\n\t        remappings=[\n\t            ('output', 'image_h264')],\n\t    )\n\t    composable_node_container = ComposableNodeContainer(\n\t        name='container',\n\t        namespace=TestH264EncoderNode.generate_namespace(),\n", "        package='rclcpp_components',\n\t        executable='component_container_mt',\n\t        prefix=container_prefix,\n\t        sigterm_timeout=container_sigterm_timeout,\n\t        composable_node_descriptions=[\n\t            data_loader_node,\n\t            prep_resize_node,\n\t            playback_node,\n\t            monitor_node\n\t        ],\n", "        output='screen'\n\t    )\n\t    return [republish_node, composable_node_container]\n\tdef generate_test_description():\n\t    return TestH264EncoderNode.generate_test_description_with_nsys(launch_setup)\n\tclass TestH264EncoderNode(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for image_transport H264 encoder node.\"\"\"\n\t    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='image_transport H264 Encoder Node Benchmark',\n", "        input_data_path=ROSBAG_PATH,\n\t        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=200.0,\n\t        publisher_lower_frequency=10.0,\n\t        # The number of frames to be buffered\n\t        playback_message_buffer_size=100\n\t    )\n\t    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "scripts/apriltag_ros_apriltag_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for apriltag_ros.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    1. PrepResizeNode: resizes images to HD\n\t- Graph under Test:\n\t    1. AprilTagNode: detects Apriltags\n\tRequired:\n\t- Packages:\n\t    - apriltag_ros\n\t- Datasets:\n\t    - assets/datasets/r2b_dataset/r2b_storage\n\t\"\"\"\n\tfrom launch_ros.actions import ComposableNodeContainer\n", "from launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking apriltag_ros.\"\"\"\n\t    # Detector parameters to detect all 36h11 tags\n\t    cfg_36h11 = {\n\t        'image_transport': 'raw',\n", "        'family': '36h11',\n\t        'size': 0.162\n\t    }\n\t    apriltag_node = ComposableNode(\n\t        name='AprilTagNode',\n\t        namespace=TestAprilTagNode.generate_namespace(),\n\t        package='apriltag_ros',\n\t        plugin='AprilTagNode',\n\t        parameters=[cfg_36h11],\n\t        remappings=[\n", "            ('image_rect', 'image'),\n\t            ('detections', 'apriltag_detections')\n\t        ]\n\t    )\n\t    data_loader_node = ComposableNode(\n\t        name='DataLoaderNode',\n\t        namespace=TestAprilTagNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n", "                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n\t    )\n\t    prep_resize_node = ComposableNode(\n\t        name='PrepResizeNode',\n\t        namespace=TestAprilTagNode.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n", "            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/image'),\n\t            ('image/camera_info', 'data_loader/camera_info'),\n\t            ('resize/image_raw', 'buffer/image'),\n\t            ('resize/camera_info', 'buffer/camera_info'),\n\t        ]\n\t    )\n\t    playback_node = ComposableNode(\n", "        name='PlaybackNode',\n\t        namespace=TestAprilTagNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        parameters=[{\n\t            'data_formats': [\n\t                'sensor_msgs/msg/Image',\n\t                'sensor_msgs/msg/CameraInfo'\n\t            ],\n\t        }],\n", "        remappings=[\n\t            ('buffer/input0', 'buffer/image'),\n\t            ('input0', 'image'),\n\t            ('buffer/input1', 'buffer/camera_info'),\n\t            ('input1', 'camera_info')\n\t        ]\n\t    )\n\t    monitor_node = ComposableNode(\n\t        name='MonitorNode',\n\t        namespace=TestAprilTagNode.generate_namespace(),\n", "        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n\t            'monitor_data_format': 'apriltag_msgs/msg/AprilTagDetectionArray',\n\t        }],\n\t        remappings=[\n\t            ('output', 'apriltag_detections')\n\t        ]\n\t    )\n\t    composable_node_container = ComposableNodeContainer(\n", "        name='container',\n\t        namespace=TestAprilTagNode.generate_namespace(),\n\t        package='rclcpp_components',\n\t        executable='component_container_mt',\n\t        prefix=container_prefix,\n\t        sigterm_timeout=container_sigterm_timeout,\n\t        composable_node_descriptions=[\n\t            data_loader_node,\n\t            prep_resize_node,\n\t            playback_node,\n", "            monitor_node,\n\t            apriltag_node\n\t        ],\n\t        output='screen'\n\t    )\n\t    return [composable_node_container]\n\tdef generate_test_description():\n\t    return TestAprilTagNode.generate_test_description_with_nsys(launch_setup)\n\tclass TestAprilTagNode(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for AprilTagNode.\"\"\"\n", "    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='apriltag_ros AprilTagNode Benchmark',\n\t        input_data_path=ROSBAG_PATH,\n\t        # The slice of the rosbag to use\n\t        input_data_start_time=3.0,\n\t        input_data_end_time=3.5,\n\t        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=600.0,\n\t        publisher_lower_frequency=10.0,\n", "        # The number of frames to be buffered\n\t        playback_message_buffer_size=10,\n\t        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n\t    )\n\t    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "scripts/image_transport_h264_decoder_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for image_transport H264 decoder node.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    1. PrepResizeNode: resizes images to full HD\n\t    2. PrepRepublishEncoderNode: encodes images to h264\n\t- Graph under Test:\n\t    1. RepublishDecoderNode: decodes compressed images\n\tRequired:\n\t- Packages:\n\t    - ros2_h264_encoder\n\t    - h264_msgs\n\t    - h264_image_transport\n\t- Datasets:\n", "    - assets/datasets/r2b_dataset/r2b_mezzanine\n\t\"\"\"\n\tfrom launch_ros.actions import ComposableNodeContainer, Node\n\tfrom launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.FULL_HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_mezzanine'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking image_transport decoder node.\"\"\"\n", "    republish_node = Node(\n\t        name='RepublishDecoderNode',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n\t        package='image_transport',\n\t        executable='republish',\n\t        arguments=['h264',  'raw', '--ros-args', '--log-level', 'error'],\n\t        remappings=[\n\t            ('in/h264', 'compressed_image'),\n\t            ('out', 'image_uncompressed'),\n\t        ],\n", "    )\n\t    data_loader_node = ComposableNode(\n\t        name='DataLoaderNode',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n\t                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info')]\n\t    )\n\t    prep_resize_node = ComposableNode(\n", "        name='PrepResizeNode',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n", "            ('image/image_raw', 'data_loader/left_image'),\n\t            ('image/camera_info', 'data_loader/left_camera_info'),\n\t            ('resize/image_raw', 'resized/image_raw'),\n\t            ('resize/camera_info', 'resized/camera_info'),\n\t        ]\n\t    )\n\t    prep_encoder_node = Node(\n\t        name='PrepRepublishEncoderNode',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n\t        package='image_transport',\n", "        executable='republish',\n\t        arguments=['raw', 'h264'],\n\t        remappings=[\n\t            ('in', 'resized/image_raw'),\n\t            ('out/h264', 'image_h264'),\n\t        ],\n\t    )\n\t    playback_node = ComposableNode(\n\t        name='PlaybackNode',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n", "        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        parameters=[{\n\t            'data_formats': [\n\t                'h264_msgs/msg/Packet'],\n\t        }],\n\t        remappings=[('buffer/input0', 'image_h264'),\n\t                    ('input0', 'compressed_image')],\n\t    )\n\t    monitor_node = ComposableNode(\n", "        name='MonitorNode',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n\t            'monitor_data_format': 'sensor_msgs/msg/Image',\n\t        }],\n\t        remappings=[\n\t            ('output', 'image_uncompressed')],\n\t    )\n", "    composable_node_container = ComposableNodeContainer(\n\t        name='container',\n\t        namespace=TestH264DecoderNode.generate_namespace(),\n\t        package='rclcpp_components',\n\t        executable='component_container_mt',\n\t        prefix=container_prefix,\n\t        sigterm_timeout=container_sigterm_timeout,\n\t        composable_node_descriptions=[\n\t            data_loader_node,\n\t            prep_resize_node,\n", "            playback_node,\n\t            monitor_node\n\t        ],\n\t        output='screen'\n\t    )\n\t    return [prep_encoder_node, republish_node, composable_node_container]\n\tdef generate_test_description():\n\t    return TestH264DecoderNode.generate_test_description_with_nsys(launch_setup)\n\tclass TestH264DecoderNode(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for image_transport H264 decoder node.\"\"\"\n", "    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='image_transport H264 Decoder Node Benchmark',\n\t        input_data_path=ROSBAG_PATH,\n\t        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=200.0,\n\t        publisher_lower_frequency=10.0,\n\t        # The number of frames to be buffered\n\t        playback_message_buffer_size=1\n\t    )\n", "    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "scripts/apriltag_ros_apriltag_graph.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for apriltag_ros graph.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    1. PrepResizeNode: resizes images to HD\n\t- Graph under Test:\n\t    1. RectifyNode: rectifies images\n\t    2. AprilTagNode: detects Apriltags\n\tRequired:\n\t- Packages:\n\t    - apriltag_ros\n\t- Datasets:\n\t    - assets/datasets/r2b_dataset/r2b_storage\n\t\"\"\"\n", "from launch_ros.actions import ComposableNodeContainer\n\tfrom launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking apriltag_ros graph.\"\"\"\n\t    # Detector parameters to detect all 36h11 tags\n\t    cfg_36h11 = {\n", "        'image_transport': 'raw',\n\t        'family': '36h11',\n\t        'size': 0.162\n\t    }\n\t    rectify_node = ComposableNode(\n\t        name='RectifyNode',\n\t        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::RectifyNode'\n\t    )\n", "    apriltag_node = ComposableNode(\n\t        name='AprilTagNode',\n\t        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='apriltag_ros',\n\t        plugin='AprilTagNode',\n\t        parameters=[cfg_36h11],\n\t        remappings=[\n\t            ('detections', 'apriltag_detections')\n\t        ]\n\t    )\n", "    data_loader_node = ComposableNode(\n\t        name='DataLoaderNode',\n\t        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n\t                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n\t    )\n\t    prep_resize_node = ComposableNode(\n\t        name='PrepResizeNode',\n", "        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/image'),\n", "            ('image/camera_info', 'data_loader/camera_info'),\n\t            ('resize/image_raw', 'buffer/image'),\n\t            ('resize/camera_info', 'buffer/camera_info'),\n\t        ]\n\t    )\n\t    playback_node = ComposableNode(\n\t        name='PlaybackNode',\n\t        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n", "        parameters=[{\n\t            'data_formats': [\n\t                'sensor_msgs/msg/Image',\n\t                'sensor_msgs/msg/CameraInfo'],\n\t        }],\n\t        remappings=[('buffer/input0', 'buffer/image'),\n\t                    ('input0', 'image'),\n\t                    ('buffer/input1', 'buffer/camera_info'),\n\t                    ('input1', 'camera_info')],\n\t    )\n", "    monitor_node = ComposableNode(\n\t        name='MonitorNode',\n\t        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n\t            'monitor_data_format': 'apriltag_msgs/msg/AprilTagDetectionArray',\n\t        }],\n\t        remappings=[\n\t            ('output', 'apriltag_detections')],\n", "    )\n\t    composable_node_container = ComposableNodeContainer(\n\t        name='container',\n\t        namespace=TestAprilTagGraph.generate_namespace(),\n\t        package='rclcpp_components',\n\t        executable='component_container_mt',\n\t        prefix=container_prefix,\n\t        sigterm_timeout=container_sigterm_timeout,\n\t        composable_node_descriptions=[\n\t            data_loader_node,\n", "            prep_resize_node,\n\t            playback_node,\n\t            monitor_node,\n\t            rectify_node,\n\t            apriltag_node\n\t        ],\n\t        output='screen'\n\t    )\n\t    return [composable_node_container]\n\tdef generate_test_description():\n", "    return TestAprilTagGraph.generate_test_description_with_nsys(launch_setup)\n\tclass TestAprilTagGraph(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for AprilTag graph.\"\"\"\n\t    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='apriltag_ros AprilTag Graph Benchmark',\n\t        input_data_path=ROSBAG_PATH,\n\t        # The slice of the rosbag to use\n\t        input_data_start_time=3.0,\n\t        input_data_end_time=3.5,\n", "        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=600.0,\n\t        publisher_lower_frequency=10.0,\n\t        # The number of frames to be buffered\n\t        playback_message_buffer_size=10,\n\t        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n\t    )\n\t    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "scripts/image_proc_rectify_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for image_proc RectifyNode.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    None\n\t- Graph under Test:\n\t    1. RectifyNode: rectifies images\n\tRequired:\n\t- Packages:\n\t    - image_proc\n\t- Datasets:\n\t    - assets/datasets/r2b_dataset/r2b_storage\n\t\"\"\"\n\tfrom launch_ros.actions import ComposableNodeContainer\n", "from launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking image_proc RectifyNode.\"\"\"\n\t    rectify_node = ComposableNode(\n\t        name='RectifyNode',\n\t        namespace=TestRectifyNode.generate_namespace(),\n", "        package='image_proc',\n\t        plugin='image_proc::RectifyNode',\n\t        remappings=[('image', 'image_raw')],\n\t    )\n\t    data_loader_node = ComposableNode(\n\t        name='DataLoaderNode',\n\t        namespace=TestRectifyNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n", "                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n\t    )\n\t    prep_resize_node = ComposableNode(\n\t        name='PrepResizeNode',\n\t        namespace=TestRectifyNode.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n", "            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/image'),\n\t            ('image/camera_info', 'data_loader/camera_info'),\n\t            ('resize/image_raw', 'buffer/image'),\n\t            ('resize/camera_info', 'buffer/camera_info'),\n\t        ]\n\t    )\n\t    playback_node = ComposableNode(\n", "        name='PlaybackNode',\n\t        namespace=TestRectifyNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        parameters=[{\n\t            'data_formats': [\n\t                'sensor_msgs/msg/Image',\n\t                'sensor_msgs/msg/CameraInfo'],\n\t        }],\n\t        remappings=[('buffer/input0', 'buffer/image'),\n", "                    ('input0', 'image_raw'),\n\t                    ('buffer/input1', 'buffer/camera_info'),\n\t                    ('input1', 'camera_info')],\n\t    )\n\t    monitor_node = ComposableNode(\n\t        name='MonitorNode',\n\t        namespace=TestRectifyNode.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n", "            'monitor_data_format': 'sensor_msgs/msg/Image',\n\t        }],\n\t        remappings=[\n\t            ('output', 'image_rect')],\n\t    )\n\t    composable_node_container = ComposableNodeContainer(\n\t        name='container',\n\t        namespace=TestRectifyNode.generate_namespace(),\n\t        package='rclcpp_components',\n\t        executable='component_container_mt',\n", "        prefix=container_prefix,\n\t        sigterm_timeout=container_sigterm_timeout,\n\t        composable_node_descriptions=[\n\t            data_loader_node,\n\t            prep_resize_node,\n\t            playback_node,\n\t            monitor_node,\n\t            rectify_node\n\t        ],\n\t        output='screen'\n", "    )\n\t    return [composable_node_container]\n\tdef generate_test_description():\n\t    return TestRectifyNode.generate_test_description_with_nsys(launch_setup)\n\tclass TestRectifyNode(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for image_proc RectifyNode.\"\"\"\n\t    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='image_proc::RectifyNode Benchmark',\n\t        input_data_path=ROSBAG_PATH,\n", "        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=2500.0,\n\t        publisher_lower_frequency=10.0,\n\t        # The number of frames to be buffered\n\t        playback_message_buffer_size=100,\n\t        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n\t    )\n\t    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "scripts/stereo_image_proc_graph.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"\n\tPerformance test for stereo_image_proc point cloud graph.\n\tThe graph consists of the following:\n\t- Preprocessors:\n", "    1. PrepLeftResizeNode, PrepRightResizeNode: resizes images to quarter HD\n\t- Graph under Test:\n\t    1. DisparityNode: creates disparity images from stereo pair\n\t    2. PointCloudNode: converts disparity to pointcloud\n\tRequired:\n\t- Packages:\n\t    - stereo_image_proc\n\t- Datasets:\n\t    - assets/datasets/r2b_dataset/r2b_hideaway\n\t\"\"\"\n", "import os\n\tfrom launch_ros.actions import ComposableNodeContainer\n\tfrom launch_ros.descriptions import ComposableNode\n\tfrom ros2_benchmark import ImageResolution\n\tfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\tIMAGE_RESOLUTION = ImageResolution.QUARTER_HD\n\tROSBAG_PATH = 'datasets/r2b_dataset/r2b_hideaway'\n\tdef launch_setup(container_prefix, container_sigterm_timeout):\n\t    \"\"\"Generate launch description for benchmarking stereo_image_proc point cloud graph.\"\"\"\n\t    env = os.environ.copy()\n", "    env['OSPL_VERBOSITY'] = '8'  # 8 = OS_NONE\n\t    # bare minimum formatting for console output matching\n\t    env['RCUTILS_CONSOLE_OUTPUT_FORMAT'] = '{message}'\n\t    disparity_node = ComposableNode(\n\t        name='DisparityNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n\t        package='stereo_image_proc',\n\t        plugin='stereo_image_proc::DisparityNode',\n\t        parameters=[{\n\t                'queue_size': 500,\n", "                'approx': False,\n\t                'use_system_default_qos': False\n\t        }],\n\t        remappings=[\n\t            ('/left/camera_info', '/left/camera_info'),\n\t            ('/right/camera_info', '/right/camera_info'),\n\t            ('/left/image_rect', '/left/image_rect'),\n\t            ('/right/image_rect', '/right/image_rect')],\n\t    )\n\t    pointcloud_node = ComposableNode(\n", "        name='PointCloudNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n\t        package='stereo_image_proc',\n\t        plugin='stereo_image_proc::PointCloudNode',\n\t        parameters=[{\n\t                'approximate_sync': False,\n\t                'avoid_point_cloud_padding': False,\n\t                'use_color': False,\n\t                'use_system_default_qos': False,\n\t        }],\n", "        remappings=[\n\t            ('/left/camera_info', '/left/camera_info'),\n\t            ('/right/camera_info', '/right/camera_info'),\n\t            ('/left/image_rect', '/left/image_rect'),\n\t            ('/right/image_rect', '/right/image_rect'),\n\t            ('left/image_rect_color', 'left/image_rect')],\n\t    )\n\t    data_loader_node = ComposableNode(\n\t        name='DataLoaderNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n", "        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n\t                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info'),\n\t                    ('hawk_0_right_rgb_image', 'data_loader/right_image'),\n\t                    ('hawk_0_right_rgb_camera_info', 'data_loader/right_camera_info')]\n\t    )\n\t    prep_left_resize_node = ComposableNode(\n\t        name='PrepLeftResizeNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n", "        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n\t            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/left_image'),\n\t            ('image/camera_info', 'data_loader/left_camera_info'),\n", "            ('resize/image_raw', 'buffer/left/image_resized'),\n\t            ('resize/camera_info', 'buffer/left/camera_info_resized'),\n\t        ]\n\t    )\n\t    prep_right_resize_node = ComposableNode(\n\t        name='PrepRightResizeNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n\t        package='image_proc',\n\t        plugin='image_proc::ResizeNode',\n\t        parameters=[{\n", "            'width': IMAGE_RESOLUTION['width'],\n\t            'height': IMAGE_RESOLUTION['height'],\n\t            'use_scale': False,\n\t        }],\n\t        remappings=[\n\t            ('image/image_raw', 'data_loader/right_image'),\n\t            ('image/camera_info', 'data_loader/right_camera_info'),\n\t            ('resize/image_raw', 'buffer/right/image_resized'),\n\t            ('resize/camera_info', 'buffer/right/camera_info_resized'),\n\t        ]\n", "    )\n\t    playback_node = ComposableNode(\n\t        name='PlaybackNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        parameters=[{\n\t            'data_formats': [\n\t                'sensor_msgs/msg/Image',\n\t                'sensor_msgs/msg/CameraInfo',\n", "                'sensor_msgs/msg/Image',\n\t                'sensor_msgs/msg/CameraInfo'],\n\t        }],\n\t        remappings=[('buffer/input0', 'buffer/left/image_resized'),\n\t                    ('input0', 'left/image_rect'),\n\t                    ('buffer/input1', 'buffer/left/camera_info_resized'),\n\t                    ('input1', 'left/camera_info'),\n\t                    ('buffer/input2', 'buffer/right/image_resized'),\n\t                    ('input2', 'right/image_rect'),\n\t                    ('buffer/input3', 'buffer/right/camera_info_resized'),\n", "                    ('input3', 'right/camera_info')],\n\t    )\n\t    monitor_node = ComposableNode(\n\t        name='MonitorNode',\n\t        namespace=TestStereoGraph.generate_namespace(),\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        parameters=[{\n\t            'monitor_data_format': 'sensor_msgs/msg/PointCloud2',\n\t        }],\n", "        remappings=[\n\t            ('output', 'points2')],\n\t    )\n\t    composable_node_container = ComposableNodeContainer(\n\t        name='container',\n\t        namespace=TestStereoGraph.generate_namespace(),\n\t        package='rclcpp_components',\n\t        executable='component_container_mt',\n\t        prefix=container_prefix,\n\t        sigterm_timeout=container_sigterm_timeout,\n", "        composable_node_descriptions=[\n\t            data_loader_node,\n\t            prep_left_resize_node,\n\t            prep_right_resize_node,\n\t            playback_node,\n\t            monitor_node,\n\t            disparity_node,\n\t            pointcloud_node\n\t        ],\n\t        output='screen'\n", "    )\n\t    return [composable_node_container]\n\tdef generate_test_description():\n\t    return TestStereoGraph.generate_test_description_with_nsys(launch_setup)\n\tclass TestStereoGraph(ROS2BenchmarkTest):\n\t    \"\"\"Performance test for stereo image pointcloud graph.\"\"\"\n\t    # Custom configurations\n\t    config = ROS2BenchmarkConfig(\n\t        benchmark_name='Stereo Image Pointcloud Graph Benchmark',\n\t        input_data_path=ROSBAG_PATH,\n", "        # Upper and lower bounds of peak throughput search window\n\t        publisher_upper_frequency=100.0,\n\t        publisher_lower_frequency=10.0,\n\t        # The number of frames to be buffered\n\t        playback_message_buffer_size=100,\n\t        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n\t    )\n\t    def test_benchmark(self):\n\t        self.run_benchmark()\n"]}
{"filename": "ros2_benchmark/test/data_loader_node_pol.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\timport os\n\timport time\n\timport unittest\n\timport launch\n", "from launch_ros.actions import ComposableNodeContainer\n\tfrom launch_ros.descriptions import ComposableNode\n\timport launch_testing.actions\n\timport rclpy\n\tfrom ros2_benchmark.utils.ros2_utility import ClientUtility\n\tfrom ros2_benchmark_interfaces.srv import SetData, StartLoading, StopLoading\n\tDIR_PATH = os.path.dirname(os.path.realpath(__file__))\n\tROSBAG_PATH = os.path.join(DIR_PATH, 'pol.bag/pol.bag_0.db3')\n\tdef generate_test_description():\n\t    \"\"\"Initialize test nodes and generate test description.\"\"\"\n", "    data_loader_node = ComposableNode(\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::DataLoaderNode',\n\t        name='DataLoaderNode',\n\t    )\n\t    data_loader_container = ComposableNodeContainer(\n\t        package='rclcpp_components',\n\t        name='data_loader_container',\n\t        namespace='',\n\t        executable='component_container_mt',\n", "        composable_node_descriptions=[data_loader_node],\n\t        output='screen'\n\t    )\n\t    return launch.LaunchDescription([\n\t        data_loader_container,\n\t        launch_testing.actions.ReadyToTest()\n\t    ])\n\tclass TestDataLoaderNode(unittest.TestCase):\n\t    \"\"\"An unit test class for DataLoaderNode.\"\"\"\n\t    def test_data_loader_node_services(self):\n", "        \"\"\"Test services hosted in DataLoaderNode.\"\"\"\n\t        SERVICE_SETUP_TIMEOUT_SEC = 5\n\t        SERVICE_FUTURE_TIMEOUT_SEC = 25\n\t        # Create a test ROS node\n\t        rclpy.init()\n\t        node = rclpy.create_node('test_node')\n\t        # Create a set_data service client\n\t        set_data_client = ClientUtility.create_service_client_blocking(\n\t            node, SetData, 'set_data', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(set_data_client)\n", "        # Create a start_loading service client\n\t        start_loading_client = ClientUtility.create_service_client_blocking(\n\t            node, StartLoading, 'start_loading', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_loading_client)\n\t        # Create a stop_loading service client\n\t        stop_loading_client = ClientUtility.create_service_client_blocking(\n\t            node, StopLoading, 'stop_loading', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(stop_loading_client)\n\t        # Send a request to the set_data service\n\t        set_data_request = SetData.Request()\n", "        set_data_request.data_path = ROSBAG_PATH\n\t        set_data_future = set_data_client.call_async(\n\t            set_data_request)\n\t        # Wait for the response from the start_recording service\n\t        set_data_response = ClientUtility.get_service_response_from_future_blocking(\n\t            node, set_data_future, SERVICE_FUTURE_TIMEOUT_SEC)\n\t        self.assertIsNotNone(set_data_response)\n\t        self.assertTrue(set_data_response.success)\n\t        # Send a request to the start_loading service\n\t        start_loading_request = StartLoading.Request()\n", "        start_loading_future = start_loading_client.call_async(\n\t            start_loading_request)\n\t        time.sleep(1)\n\t        # Send a request to the stop_loading service\n\t        stop_loading_request = StopLoading.Request()\n\t        stop_loading_future = stop_loading_client.call_async(\n\t            stop_loading_request)\n\t        # Wait for the response from the start_recording service\n\t        start_loading_response = ClientUtility.get_service_response_from_future_blocking(\n\t            node, start_loading_future, SERVICE_FUTURE_TIMEOUT_SEC)\n", "        self.assertIsNotNone(start_loading_response)\n\t        node.get_logger().info('Received response from the start_loading service:')\n\t        node.get_logger().info(str(start_loading_response))\n\t        # Wait for the response from the start_recording service\n\t        stop_loading_response = ClientUtility.get_service_response_from_future_blocking(\n\t            node, stop_loading_future, SERVICE_FUTURE_TIMEOUT_SEC)\n\t        self.assertIsNotNone(stop_loading_response)\n\t        node.get_logger().info('Received response from the stop_loading service.')\n"]}
{"filename": "ros2_benchmark/test/monitor_node_pol.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\timport os\n\timport unittest\n\timport launch\n\tfrom launch.actions import ExecuteProcess\n", "from launch_ros.actions import ComposableNodeContainer\n\tfrom launch_ros.descriptions import ComposableNode\n\timport launch_testing.actions\n\timport rclpy\n\tfrom ros2_benchmark.utils.ros2_utility import ClientUtility\n\tfrom ros2_benchmark_interfaces.srv import StartMonitoring\n\tdef generate_test_description():\n\t    \"\"\"Initialize test nodes and generate test description.\"\"\"\n\t    dir_path = os.path.dirname(os.path.realpath(__file__))\n\t    rosbag_path = os.path.join(dir_path, 'pol.bag')\n", "    monitor_node0 = ComposableNode(\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        name='MonitorNode0',\n\t        parameters=[{\n\t            'monitor_data_format': 'sensor_msgs/msg/Image',\n\t            'monitor_index': 0\n\t        }],\n\t        remappings=[\n\t            ('output', '/image'),\n", "        ],\n\t    )\n\t    monitor_node1 = ComposableNode(\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::MonitorNode',\n\t        name='MonitorNode1',\n\t        parameters=[{\n\t            'monitor_data_format': 'sensor_msgs/msg/CameraInfo',\n\t            'monitor_index': 1\n\t        }],\n", "        remappings=[\n\t            ('output', '/camera_info'),\n\t        ],\n\t    )\n\t    monitor_container = ComposableNodeContainer(\n\t        package='rclcpp_components',\n\t        name='monitor_container',\n\t        namespace='',\n\t        executable='component_container_mt',\n\t        composable_node_descriptions=[monitor_node0, monitor_node1],\n", "        output='screen'\n\t    )\n\t    # Play rosbag for the monitor node to receive messages\n\t    rosbag_play = ExecuteProcess(\n\t        cmd=['ros2', 'bag', 'play', rosbag_path],\n\t        output='screen')\n\t    return launch.LaunchDescription([\n\t        rosbag_play,\n\t        monitor_container,\n\t        launch_testing.actions.ReadyToTest()\n", "    ])\n\tclass TestMonitorNode(unittest.TestCase):\n\t    \"\"\"An unit test class for MonitorNode.\"\"\"\n\t    def test_monitor_node_services(self):\n\t        \"\"\"Test services hosted in MonitorNode.\"\"\"\n\t        SERVICE_SETUP_TIMEOUT_SEC = 5\n\t        SERVICE_TIMEOUT_SEC = 20\n\t        SERVICE_FUTURE_TIMEOUT_SEC = 25\n\t        # Create a test ROS node\n\t        rclpy.init()\n", "        node = rclpy.create_node('test_node')\n\t        # Create a start_monitoring0 service client\n\t        start_monitoring_client0 = ClientUtility.create_service_client_blocking(\n\t            node, StartMonitoring, 'start_monitoring0', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_monitoring_client0)\n\t        # Create a start_monitoring1 service client\n\t        start_monitoring_client1 = ClientUtility.create_service_client_blocking(\n\t            node, StartMonitoring, 'start_monitoring1', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_monitoring_client1)\n\t        # Send a request to the start_monitoring0 service\n", "        start_monitoring_request = StartMonitoring.Request()\n\t        start_monitoring_request.timeout = SERVICE_TIMEOUT_SEC\n\t        start_monitoring_request.message_count = 1\n\t        start_monitoring_future0 = start_monitoring_client0.call_async(\n\t            start_monitoring_request)\n\t        # Send a request to the start_monitoring1 service\n\t        start_monitoring_future1 = start_monitoring_client1.call_async(\n\t            start_monitoring_request)\n\t        # Wait for the response from the start_monitoring0 service\n\t        start_monitoring_response0 = ClientUtility.get_service_response_from_future_blocking(\n", "            node, start_monitoring_future0, SERVICE_FUTURE_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_monitoring_response0)\n\t        node.get_logger().info('Received response from the start_monitoring0 service:')\n\t        node.get_logger().info(str(start_monitoring_response0))\n\t        # Wait for the response from the start_monitoring1 service\n\t        start_monitoring_response1 = ClientUtility.get_service_response_from_future_blocking(\n\t            node, start_monitoring_future1, SERVICE_FUTURE_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_monitoring_response1)\n\t        node.get_logger().info('Received response from the start_monitoring1 service:')\n\t        node.get_logger().info(str(start_monitoring_response1))\n"]}
{"filename": "ros2_benchmark/test/playback_node_pol.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\timport os\n\timport unittest\n\timport launch\n\tfrom launch.actions import ExecuteProcess\n", "from launch_ros.actions import ComposableNodeContainer\n\tfrom launch_ros.descriptions import ComposableNode\n\timport launch_testing.actions\n\timport rclpy\n\tfrom ros2_benchmark.utils.ros2_utility import ClientUtility\n\tfrom ros2_benchmark_interfaces.srv import PlayMessages, StartRecording\n\tdef generate_test_description():\n\t    \"\"\"Initialize test nodes and generate test description.\"\"\"\n\t    dir_path = os.path.dirname(os.path.realpath(__file__))\n\t    rosbag_path = os.path.join(dir_path, 'pol.bag')\n", "    playback_node = ComposableNode(\n\t        package='ros2_benchmark',\n\t        plugin='ros2_benchmark::PlaybackNode',\n\t        name='PlaybackNode',\n\t        parameters=[{\n\t            'data_formats': ['sensor_msgs/msg/Image'],\n\t        }],\n\t        remappings=[\n\t            ('buffer/input0', 'buffer/image'),\n\t            ('input0', '/image')\n", "        ],\n\t    )\n\t    playback_container = ComposableNodeContainer(\n\t        package='rclcpp_components',\n\t        name='playback_container',\n\t        namespace='',\n\t        executable='component_container_mt',\n\t        composable_node_descriptions=[playback_node],\n\t        output='screen'\n\t    )\n", "    # Play rosbag for the playback node to record messages\n\t    rosbag_play = ExecuteProcess(\n\t        cmd=['ros2', 'bag', 'play', rosbag_path, '--remap', 'image:=/buffer/image'],\n\t        output='screen')\n\t    return launch.LaunchDescription([\n\t        rosbag_play,\n\t        playback_container,\n\t        launch_testing.actions.ReadyToTest()\n\t    ])\n\tclass TestPlaybackNode(unittest.TestCase):\n", "    \"\"\"An unit test class for PlaybackNode.\"\"\"\n\t    def test_playback_node_services(self):\n\t        \"\"\"Test services hosted in PlaybackNode.\"\"\"\n\t        SERVICE_SETUP_TIMEOUT_SEC = 5\n\t        SERVICE_TIMEOUT_SEC = 20\n\t        SERVICE_FUTURE_TIMEOUT_SEC = 25\n\t        # Create a test ROS node\n\t        rclpy.init()\n\t        node = rclpy.create_node('test_node')\n\t        # Create a start_recording service client\n", "        start_recording_client = ClientUtility.create_service_client_blocking(\n\t            node, StartRecording, 'start_recording', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_recording_client)\n\t        # Create a play_messages service client\n\t        play_messages_client = ClientUtility.create_service_client_blocking(\n\t            node, PlayMessages, 'play_messages', SERVICE_SETUP_TIMEOUT_SEC)\n\t        self.assertIsNotNone(play_messages_client)\n\t        # Send a request to the start_recording service\n\t        start_recording_request = StartRecording.Request()\n\t        start_recording_request.buffer_length = 10\n", "        start_recording_request.timeout = SERVICE_TIMEOUT_SEC\n\t        start_recording_future = start_recording_client.call_async(start_recording_request)\n\t        # Wait for the response from the start_recording service\n\t        start_recording_response = ClientUtility.get_service_response_from_future_blocking(\n\t            node, start_recording_future, SERVICE_FUTURE_TIMEOUT_SEC)\n\t        self.assertIsNotNone(start_recording_response)\n\t        node.get_logger().info('Received response from the start_recording service:')\n\t        node.get_logger().info(str(start_recording_response))\n\t        # Send a request to the play_messages service\n\t        play_messages_request = PlayMessages.Request()\n", "        play_messages_request.target_publisher_rate = 30.0\n\t        play_messages_future = play_messages_client.call_async(play_messages_request)\n\t        # Wait for the response from the play_messages service\n\t        play_messages_response = ClientUtility.get_service_response_from_future_blocking(\n\t            node, play_messages_future, SERVICE_FUTURE_TIMEOUT_SEC)\n\t        self.assertIsNotNone(play_messages_response)\n\t        node.get_logger().info('Received response from the play_messages service:')\n\t        node.get_logger().info(str(play_messages_response))\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/ros2_benchmark_test.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\timport datetime\n\tfrom enum import Enum\n\tfrom functools import partial\n\timport hashlib\n", "import json\n\tfrom math import ceil\n\timport os\n\timport platform\n\timport sys\n\timport time\n\tfrom typing import Iterable\n\timport unittest\n\timport launch\n\tfrom launch.actions import OpaqueFunction\n", "import launch_testing.actions\n\timport rclpy\n\tfrom ros2_benchmark_interfaces.srv import GetTopicMessageTimestamps, PlayMessages\n\tfrom ros2_benchmark_interfaces.srv import SetData, StartLoading, StopLoading\n\tfrom ros2_benchmark_interfaces.srv import StartMonitoring, StartRecording\n\timport rosbag2_py\n\tfrom .basic_performance_calculator import BasicPerformanceMetrics\n\tfrom .ros2_benchmark_config import BenchmarkMode\n\tfrom .ros2_benchmark_config import ROS2BenchmarkConfig\n\tfrom .utils.cpu_profiler import CPUProfiler\n", "from .utils.nsys_utility import NsysUtility\n\tfrom .utils.ros2_utility import ClientUtility\n\t# The maximum allowed line width of a performance repeort displayed in the terminal\n\tMAX_REPORT_OUTPUT_WIDTH = 90\n\tidle_cpu_util = 0.0\n\tclass BenchmarkMetadata(Enum):\n\t    \"\"\"Benchmark metadata items to be included in a final report.\"\"\"\n\t    NAME = 'Test Name'\n\t    TEST_DATETIME = 'Test Datetime'\n\t    TEST_FILE_PATH = 'Test File Path'\n", "    DEVICE_ARCH = 'Device Architecture'\n\t    DEVICE_OS = 'Device OS'\n\t    DEVICE_HOSTNAME = 'Device Hostname'\n\t    BENCHMARK_MODE = 'Benchmark Mode'\n\t    INPUT_DATA_PATH = 'Input Data Path'\n\t    INPUT_DATA_HASH = 'Input Data Hash'\n\t    INPUT_DATA_SIZE = 'Input Data Size (bytes)'\n\t    INPUT_DATA_START_TIME = 'Input Data Start Time (s)'\n\t    INPUT_DATA_END_TIME = 'Input Data End Time (s)'\n\t    DATA_RESOLUTION = 'Data Resolution'\n", "    IDLE_CPU_UTIL = 'Idle System CPU Util. (%)'\n\t    PEAK_THROUGHPUT_PREDICTION = 'Peak Throughput Prediction (Hz)'\n\t    CONFIG = 'Test Configurations'\n\tclass ROS2BenchmarkTest(unittest.TestCase):\n\t    \"\"\"The main ros2_benchmark framework test class.\"\"\"\n\t    # A config object that holds benchmark-relevant settings\n\t    config = ROS2BenchmarkConfig()\n\t    def __init__(self, *args, **kwargs):\n\t        \"\"\"Initialize ros2_benchmark.\"\"\"\n\t        # Calculated hash of the data to be loaded from a data loader node\n", "        self._input_data_hash = ''\n\t        # Size of the data (file)\n\t        self._input_data_size_bytes = 0\n\t        self._test_datetime = datetime.datetime.now(datetime.timezone.utc)\n\t        # The absolute path of the top level launch script\n\t        self._test_file_path = os.path.abspath(sys.argv[1])\n\t        self._peak_throughput_prediction = 0.0\n\t        self._logger_name_stack = ['']\n\t        # Override default configs from env variablees\n\t        self.override_config_from_env()\n", "        self._cpu_profiler = CPUProfiler()\n\t        self._cpu_profiler_log_file_path = ''\n\t        super().__init__(*args, **kwargs)\n\t    @classmethod\n\t    def setUpClass(cls) -> None:\n\t        \"\"\"Set up before first test method.\"\"\"\n\t        # Initialize the ROS context for the test node\n\t        rclpy.init()\n\t    @classmethod\n\t    def tearDownClass(cls) -> None:\n", "        \"\"\"Tear down after last test method.\"\"\"\n\t        # Shutdown the ROS context\n\t        rclpy.shutdown()\n\t    def setUp(self) -> None:\n\t        \"\"\"Set up before each test method.\"\"\"\n\t        # Create a ROS node for benchmark tests\n\t        self.node = rclpy.create_node('Controller', namespace=self.generate_namespace())\n\t    def tearDown(self) -> None:\n\t        \"\"\"Tear down after each test method.\"\"\"\n\t        self.node.destroy_node()\n", "    @classmethod\n\t    def generate_namespace(cls, *tokens: Iterable[str], absolute=True) -> str:\n\t        \"\"\"\n\t        Generate a namespace with an optional list of tokens.\n\t        This function is a utility for producing namespaced topic and service names in\n\t        such a way that there are no collisions between 'dummy' nodes running for testing\n\t        and 'normal' nodes running on the same machine.\n\t        Parameters\n\t        ----------\n\t        tokens : Iterable[str]\n", "            List of tokens to include in the namespace. Often used to generate\n\t            separate namespaces for Isaac ROS and reference implementations.\n\t        absolute: bool\n\t            Whether or not to generate an absolute namespace, by default True.\n\t        Returns\n\t        -------\n\t        str\n\t            The generated namespace as a slash-delimited string\n\t        \"\"\"\n\t        return ('/' if absolute else '') + '/'.join(\n", "            filter(None, [cls.config.benchmark_namespace, *tokens]))\n\t    @staticmethod\n\t    def generate_test_description(\n\t        nodes: Iterable[launch.Action], node_startup_delay: float = 5.0\n\t    ) -> launch.LaunchDescription:\n\t        \"\"\"\n\t        Generate a test launch description.\n\t        The nodes included in this launch description will be launched as a test fixture\n\t        immediately before the first test in the test class runs. Note that the graph is\n\t        NOT shut down or re-launched between tests within the same class.\n", "        Parameters\n\t        ----------\n\t        nodes : Iterable[launch.Action]\n\t            List of Actions to launch before running the test.\n\t        node_startup_delay : float, optional\n\t            Seconds to delay by to account for node startup, by default 2.0\n\t        Returns\n\t        -------\n\t        launch.LaunchDescription\n\t            The LaunchDescription object to launch before running the test\n", "        \"\"\"\n\t        # Wait until the system CPU usage become stable\n\t        rclpy.logging.get_logger('r2b').info(\n\t            'Waiting 10 seconds for measuring idle system CPU utilization...')\n\t        time.sleep(10)\n\t        global idle_cpu_util\n\t        idle_cpu_util = CPUProfiler.get_current_cpu_usage()\n\t        return launch.LaunchDescription(\n\t            nodes + [\n\t                # Start tests after a fixed delay for node startup\n", "                launch.actions.TimerAction(\n\t                    period=node_startup_delay, actions=[launch_testing.actions.ReadyToTest()])\n\t            ]\n\t        )\n\t    @staticmethod\n\t    def generate_test_description_with_nsys(\n\t        launch_setup, node_startup_delay: float = 5.0\n\t    ) -> launch.LaunchDescription:\n\t        \"\"\"Generate a test launch description with the nsys capability built in.\"\"\"\n\t        # Wait until the system CPU usage become stable\n", "        rclpy.logging.get_logger('r2b').info(\n\t            'Waiting 10 seconds for measuring idle system CPU utilization...')\n\t        time.sleep(10)\n\t        global idle_cpu_util\n\t        idle_cpu_util = CPUProfiler.get_current_cpu_usage()\n\t        launch_args = NsysUtility.generate_launch_args()\n\t        bound_launch_setup = partial(\n\t            NsysUtility.launch_setup_wrapper,\n\t            launch_setup=launch_setup)\n\t        nodes = launch_args + [OpaqueFunction(function=bound_launch_setup)]\n", "        return launch.LaunchDescription(\n\t            nodes + [\n\t                # Start tests after a fixed delay for node startup\n\t                launch.actions.TimerAction(\n\t                    period=node_startup_delay,\n\t                    actions=[launch_testing.actions.ReadyToTest()])\n\t            ]\n\t        )\n\t    def get_logger(self, child_name=''):\n\t        \"\"\"Get logger with child layers.\"\"\"\n", "        if hasattr(self, 'node'):\n\t            base_logger = self.node.get_logger()\n\t        else:\n\t            base_logger = rclpy.logging.get_logger('ROS2BenchmarkTest')\n\t        if child_name:\n\t            return base_logger.get_child(child_name)\n\t        if len(self._logger_name_stack) == 0:\n\t            return base_logger\n\t        for logger_name in self._logger_name_stack:\n\t            if logger_name:\n", "                base_logger = base_logger.get_child(logger_name)\n\t        return base_logger\n\t    def push_logger_name(self, name):\n\t        \"\"\"Add a child layer to the logger.\"\"\"\n\t        self._logger_name_stack.append(name)\n\t    def pop_logger_name(self):\n\t        \"\"\"Pop a child layer from the logger.\"\"\"\n\t        self._logger_name_stack.pop()\n\t    def override_config_from_env(self):\n\t        \"\"\"Override config parameters with values from environment variables.\"\"\"\n", "        override_config_dict = {}\n\t        for param_key in self.config.__dict__.keys():\n\t            env_value = os.getenv(f'ROS2_BENCHMARK_OVERRIDE_{param_key.upper()}')\n\t            if env_value is not None:\n\t                override_config_dict[param_key] = env_value\n\t                self.get_logger().info(\n\t                    f'Updating a benchmark config from env: {param_key} = {env_value}')\n\t        self.config.apply_to_attributes(override_config_dict)\n\t    @classmethod\n\t    def get_assets_root_path(cls):\n", "        \"\"\"Get assets path provided in configurations.\"\"\"\n\t        return cls.config.assets_root\n\t    def get_input_data_absolute_path(self):\n\t        \"\"\"Construct the absolute path of the input data file from configurations.\"\"\"\n\t        return os.path.join(self.config.assets_root, self.config.input_data_path)\n\t    def print_report(self, report: dict, sub_heading: str = '') -> None:\n\t        \"\"\"Print the given report.\"\"\"\n\t        heading = self.config.benchmark_name\n\t        # Temporarily remove configs from the report as we don't want it to be printed\n\t        config_value = None\n", "        if 'metadata' in report:\n\t            config_value = report['metadata'].pop(BenchmarkMetadata.CONFIG, None)\n\t        is_prev_dict = False\n\t        table_blocks = []\n\t        table_block_rows = []\n\t        def construct_table_blocks_helper(prefix, data):\n\t            nonlocal is_prev_dict\n\t            nonlocal table_blocks\n\t            nonlocal table_block_rows\n\t            for key, value in data.items():\n", "                key_str = str(key.value) if isinstance(key, Enum) else str(key)\n\t                if isinstance(value, dict):\n\t                    if not is_prev_dict and len(table_block_rows) > 0:\n\t                        table_blocks.append(table_block_rows)\n\t                        table_block_rows = []\n\t                        is_prev_dict = True\n\t                    construct_table_blocks_helper(f'{prefix}[{key_str}] ', value)\n\t                    if not is_prev_dict:\n\t                        table_blocks.append(table_block_rows)\n\t                        table_block_rows = []\n", "                        is_prev_dict = True\n\t                elif isinstance(value, Enum):\n\t                    table_block_rows.append(f'{prefix}{key_str} : {value.value}')\n\t                    is_prev_dict = False\n\t                elif isinstance(value, float):\n\t                    table_block_rows.append(f'{prefix}{key_str} : {\"{:.3f}\".format(value)}')\n\t                    is_prev_dict = False\n\t                else:\n\t                    table_block_rows.append(f'{prefix}{key_str} : {value}')\n\t                    is_prev_dict = False\n", "        construct_table_blocks_helper('', report)\n\t        if len(table_block_rows) > 0:\n\t            table_blocks.append(table_block_rows)\n\t        max_row_width = max([len(row) for rows in table_blocks for row in rows] +\n\t                            [len(heading), len(sub_heading)])\n\t        max_row_width = min(max_row_width, MAX_REPORT_OUTPUT_WIDTH)\n\t        def print_line_helper():\n\t            self.get_logger().info('+-{}-+'.format('-'*max_row_width))\n\t        def print_row_helper(row):\n\t            self.get_logger().info('| {:<{width}} |'.format(row, width=max_row_width))\n", "        def print_table_helper():\n\t            print_line_helper()\n\t            self.get_logger().info('| {:^{width}} |'.format(heading, width=max_row_width))\n\t            if sub_heading:\n\t                self.get_logger().info('| {:^{width}} |'.format(sub_heading, width=max_row_width))\n\t            print_line_helper()\n\t            for rows in table_blocks:\n\t                for row in rows:\n\t                    print_row_helper(row)\n\t                print_line_helper()\n", "        print_table_helper()\n\t        if config_value is not None:\n\t            report['metadata'][BenchmarkMetadata.CONFIG] = config_value\n\t    def construct_final_report(self, report: dict) -> dict:\n\t        \"\"\"Construct and return the final report from the given report.\"\"\"\n\t        final_report = report\n\t        if len(self.config.custom_report_info) > 0:\n\t            final_report['custom'] = self.config.custom_report_info\n\t        # Add benchmark metadata\n\t        metadata = {}\n", "        # Benchmark launch info\n\t        metadata[BenchmarkMetadata.NAME] = self.config.benchmark_name\n\t        metadata[BenchmarkMetadata.TEST_FILE_PATH] = self._test_file_path\n\t        metadata[BenchmarkMetadata.TEST_DATETIME] = \\\n\t            self._test_datetime.strftime('%Y-%m-%dT%H:%M:%SZ')\n\t        # Systeme info\n\t        uname = platform.uname()\n\t        metadata[BenchmarkMetadata.DEVICE_HOSTNAME] = uname.node\n\t        metadata[BenchmarkMetadata.DEVICE_ARCH] = uname.machine\n\t        metadata[BenchmarkMetadata.DEVICE_OS] = \\\n", "            f'{uname.system} {uname.release} {uname.version}'\n\t        metadata[BenchmarkMetadata.CONFIG] = self.config.to_yaml_str()\n\t        metadata[BenchmarkMetadata.IDLE_CPU_UTIL] = idle_cpu_util\n\t        # Benchmark data info\n\t        metadata[BenchmarkMetadata.BENCHMARK_MODE] = self.config.benchmark_mode.value\n\t        if self.config.benchmark_mode in [BenchmarkMode.LOOPING, BenchmarkMode.SWEEPING]:\n\t            metadata[BenchmarkMetadata.PEAK_THROUGHPUT_PREDICTION] = \\\n\t                self._peak_throughput_prediction\n\t        metadata[BenchmarkMetadata.INPUT_DATA_PATH] = self.get_input_data_absolute_path()\n\t        metadata[BenchmarkMetadata.INPUT_DATA_SIZE] = self._input_data_size_bytes\n", "        metadata[BenchmarkMetadata.INPUT_DATA_HASH] = self._input_data_hash\n\t        if self.config.input_data_start_time != -1:\n\t            metadata[BenchmarkMetadata.INPUT_DATA_START_TIME] = \\\n\t                self.config.input_data_start_time\n\t        if self.config.input_data_end_time != -1:\n\t            metadata[BenchmarkMetadata.INPUT_DATA_END_TIME] = \\\n\t                self.config.input_data_end_time\n\t        final_report['metadata'] = metadata\n\t        return final_report\n\t    def export_report(self, report: dict) -> None:\n", "        \"\"\"Export the given report to a JSON file.\"\"\"\n\t        def to_json_compatible_helper(data):\n\t            data_out = {}\n\t            for key, value in data.items():\n\t                if isinstance(value, dict):\n\t                    data_out[str(key)] = to_json_compatible_helper(value)\n\t                elif isinstance(value, Enum):\n\t                    data_out[str(key)] = str(value)\n\t                else:\n\t                    try:\n", "                        json.dumps(value)\n\t                        data_out[str(key)] = value\n\t                    except TypeError:\n\t                        data_out[str(key)] = str(value)\n\t            return data_out\n\t        if self.config.log_file_name == '':\n\t            timestr = self._test_datetime.strftime('%Y%m%d-%H%M%S')\n\t            log_file_path = os.path.join(self.config.log_folder, f'r2b-log-{timestr}.json')\n\t        else:\n\t            log_file_path = os.path.join(self.config.log_folder, self.config.log_file_name)\n", "        with open(log_file_path, 'a') as f:\n\t            f.write(json.dumps(to_json_compatible_helper(report)))\n\t        self.get_logger().info(f'Exported benchmark report to {log_file_path}')\n\t    def create_service_client_blocking(self, service_type, service_name):\n\t        \"\"\"Create a service client and wait for it to be available.\"\"\"\n\t        namespaced_service_name = self.generate_namespace(service_name)\n\t        service_client = ClientUtility.create_service_client_blocking(\n\t            self.node, service_type, namespaced_service_name,\n\t            self.config.setup_service_client_timeout_sec)\n\t        if not service_client:\n", "            self.fail(f'Failed to create a {service_name} service client')\n\t        return service_client\n\t    def get_service_response_from_future_blocking(\n\t            self,\n\t            future,\n\t            check_success=False,\n\t            timeout_sec=None):\n\t        \"\"\"Block and wait for a service future to return.\"\"\"\n\t        if timeout_sec is None:\n\t            timeout_sec = self.config.default_service_future_timeout_sec\n", "        future_result = ClientUtility.get_service_response_from_future_blocking(\n\t            self.node, future, timeout_sec)\n\t        if not future_result:\n\t            self.fail('Failed to wait for a service future')\n\t        if check_success and not future_result.success:\n\t            self.fail('A service returned with an unsuccess response')\n\t        return future_result\n\t    def prepare_buffer(self):\n\t        \"\"\"Load data from a data loader node to a playback node.\"\"\"\n\t        self.push_logger_name('Loading')\n", "        # Check the input data file\n\t        input_data_path = self.get_input_data_absolute_path()\n\t        try:\n\t            rosbag_info = rosbag2_py.Info()\n\t            rosbag_metadata = rosbag_info.read_metadata(input_data_path, 'sqlite3')\n\t            rosbag_file_path = input_data_path\n\t            if len(rosbag_metadata.files) > 0:\n\t                rosbag_file_path = os.path.join(input_data_path, rosbag_metadata.files[0].path)\n\t            elif len(rosbag_metadata.relative_file_paths) > 0:\n\t                rosbag_file_path = os.path.join(\n", "                    input_data_path, rosbag_metadata.relative_file_paths[0])\n\t            self.get_logger().info('Checking input data file...')\n\t            self.get_logger().info(f' - Rosbag path = {rosbag_file_path}')\n\t            if rosbag_metadata.compression_mode:\n\t                self.get_logger().info(\n\t                    f' - Compression mode = {rosbag_metadata.compression_mode}')\n\t            if rosbag_metadata.compression_format:\n\t                self.get_logger().info(\n\t                    f' - Compression format = {rosbag_metadata.compression_format}')\n\t            self.get_logger().info('Computing input data file hash...')\n", "            hash_md5 = hashlib.md5()\n\t            self._input_data_size_bytes = 0\n\t            with open(rosbag_file_path, 'rb') as input_data:\n\t                while True:\n\t                    next_chunk = input_data.read(4096)\n\t                    self._input_data_size_bytes += len(next_chunk)\n\t                    if next_chunk == b'':\n\t                        break\n\t                    hash_md5.update(next_chunk)\n\t            self._input_data_hash = hash_md5.hexdigest()\n", "            self.get_logger().info(f' - File hash = \"{self._input_data_hash}\"')\n\t            self.get_logger().info(f' - File size (bytes) = {self._input_data_size_bytes}')\n\t        except FileNotFoundError:\n\t            self.fail(f'Could not open the input data file at \"{input_data_path}\"')\n\t        # Create service clients\n\t        set_data_client = self.create_service_client_blocking(\n\t            SetData, 'set_data')\n\t        start_loading_client = self.create_service_client_blocking(\n\t            StartLoading, 'start_loading')\n\t        stop_loading_client = self.create_service_client_blocking(\n", "            StopLoading, 'stop_loading')\n\t        start_recording_client = self.create_service_client_blocking(\n\t            StartRecording, 'start_recording')\n\t        # Set and initialize data\n\t        self.get_logger().info('Requesting to initialize the data loader node')\n\t        set_data_request = SetData.Request()\n\t        set_data_request.data_path = input_data_path\n\t        set_data_request.publish_tf_messages = self.config.publish_tf_messages_in_set_data\n\t        set_data_request.publish_tf_static_messages = \\\n\t            self.config.publish_tf_static_messages_in_set_data\n", "        set_data_future = set_data_client.call_async(set_data_request)\n\t        self.get_service_response_from_future_blocking(\n\t            set_data_future,\n\t            check_success=True,\n\t            timeout_sec=self.config.set_data_service_future_timeout_sec)\n\t        # Start recording\n\t        self.get_logger().info('Requesting to record messages.')\n\t        start_recording_request = StartRecording.Request()\n\t        start_recording_request.buffer_length = self.config.playback_message_buffer_size\n\t        start_recording_request.timeout = self.config.start_recording_service_timeout_sec\n", "        start_recording_request.record_data_timeline = self.config.record_data_timeline\n\t        if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n\t            self.get_logger().info('Requesting to get topic message timestamps.')\n\t            start_recording_request.topic_message_timestamps = self.get_topic_message_timestamps()\n\t        start_recording_future = start_recording_client.call_async(start_recording_request)\n\t        # Load and play messages from the data loader node\n\t        self.get_logger().info('Requesting to load messages.')\n\t        start_loading_request = StartLoading.Request()\n\t        start_loading_request.publish_in_real_time = self.config.load_data_in_real_time\n\t        if self.config.input_data_start_time != -1:\n", "            start_loading_request.start_time_offset_ns = \\\n\t                int(self.config.input_data_start_time * (10**9))\n\t        if self.config.input_data_end_time != -1:\n\t            start_loading_request.end_time_offset_ns = \\\n\t                int(self.config.input_data_end_time * (10**9))\n\t        if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n\t            start_loading_request.repeat_data = False\n\t        else:\n\t            start_loading_request.repeat_data = True\n\t        start_loading_future = start_loading_client.call_async(\n", "            start_loading_request)\n\t        # Wait for the recording request to finish (or time out)\n\t        self.get_logger().info('Waiting for the recording service to end.')\n\t        start_recording_response = self.get_service_response_from_future_blocking(\n\t            start_recording_future,\n\t            timeout_sec=self.config.start_recording_service_future_timeout_sec)\n\t        recorded_topic_message_counts = start_recording_response.recorded_topic_message_counts\n\t        recorded_message_count = start_recording_response.recorded_message_count\n\t        # Stop loading data\n\t        self.get_logger().info('Requesting to stop loading data.')\n", "        stop_loading_request = StopLoading.Request()\n\t        stop_loading_future = stop_loading_client.call_async(stop_loading_request)\n\t        self.get_service_response_from_future_blocking(stop_loading_future)\n\t        # Wait for data loader service (start_loading) to end\n\t        self.get_logger().info('Waiting for the start_loading serevice to end.')\n\t        self.get_service_response_from_future_blocking(\n\t            start_loading_future, check_success=True)\n\t        self.assertTrue(recorded_message_count > 0, 'No message was recorded')\n\t        if (not self.config.record_data_timeline):\n\t            for topic_message_count in recorded_topic_message_counts:\n", "                topic_name = topic_message_count.topic_name\n\t                recorded_count = topic_message_count.message_count\n\t                if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n\t                    target_count = 0\n\t                    for topic_message_timestamps in \\\n\t                            start_recording_request.topic_message_timestamps:\n\t                        if topic_message_timestamps.topic_name == topic_name:\n\t                            target_count = len(topic_message_timestamps.timestamps_ns)\n\t                            break\n\t                else:\n", "                    target_count = self.config.playback_message_buffer_size\n\t                self.assertTrue(\n\t                    recorded_count >= target_count,\n\t                    f'Not all messages were loaded ({topic_name}:{recorded_count}/{target_count})')\n\t        self.get_logger().info(\n\t            f'All {recorded_message_count} messages were sucessfully recorded')\n\t        self.pop_logger_name()\n\t    def get_topic_message_timestamps(self):\n\t        \"\"\"Get topic message timestamps from the service get_topic_message_timestamps.\"\"\"\n\t        get_topic_message_timestamps_client = self.create_service_client_blocking(\n", "            GetTopicMessageTimestamps, 'get_topic_message_timestamps')\n\t        get_topic_message_timestamps_request = GetTopicMessageTimestamps.Request()\n\t        if self.config.input_data_start_time != -1:\n\t            get_topic_message_timestamps_request.start_time_offset_ns = \\\n\t                int(self.config.input_data_start_time * (10**9))\n\t        if self.config.input_data_end_time != -1:\n\t            get_topic_message_timestamps_request.end_time_offset_ns = \\\n\t                int(self.config.input_data_end_time * (10**9))\n\t        get_topic_message_timestamps_future = get_topic_message_timestamps_client.call_async(\n\t            get_topic_message_timestamps_request)\n", "        get_topic_message_timestamps_response = self.get_service_response_from_future_blocking(\n\t            get_topic_message_timestamps_future, check_success=True)\n\t        return get_topic_message_timestamps_response.topic_message_timestamps\n\t    def benchmark_body(self, playback_message_count, target_freq) -> dict:\n\t        \"\"\"\n\t        Run benchmark test.\n\t        Parameters\n\t        ----------\n\t        playback_message_count :\n\t            The number of messages to be tested\n", "        target_freq :\n\t            Target test publisher rate\n\t        \"\"\"\n\t        # Create play_messages service\n\t        play_messages_client = self.create_service_client_blocking(PlayMessages, 'play_messages')\n\t        # Create and send monitor service requests\n\t        monitor_service_client_map = {}\n\t        monitor_service_future_map = {}\n\t        for monitor_info in self.config.monitor_info_list:\n\t            # Create a monitor service client\n", "            start_monitoring_client = self.create_service_client_blocking(\n\t                StartMonitoring, monitor_info.service_name)\n\t            if start_monitoring_client is None:\n\t                return\n\t            # Start monitoring messages\n\t            self.get_logger().info(\n\t                f'Requesting to monitor end messages from service \"{monitor_info.service_name}\".')\n\t            start_monitoring_request = StartMonitoring.Request()\n\t            start_monitoring_request.timeout = self.config.start_monitoring_service_timeout_sec\n\t            start_monitoring_request.message_count = playback_message_count\n", "            start_monitoring_request.revise_timestamps_as_message_ids = \\\n\t                self.config.revise_timestamps_as_message_ids\n\t            start_monitoring_future = start_monitoring_client.call_async(\n\t                start_monitoring_request)\n\t            monitor_service_client_map[monitor_info.service_name] = start_monitoring_client\n\t            monitor_service_future_map[monitor_info.service_name] = start_monitoring_future\n\t        # Start CPU profiler\n\t        if self.config.enable_cpu_profiler:\n\t            self._cpu_profiler.stop_profiling()\n\t            self._cpu_profiler.start_profiling(self.config.cpu_profiling_interval_sec)\n", "            self.get_logger().info('CPU profiling stared.')\n\t        # Start playing messages\n\t        self.get_logger().info(\n\t            f'Requesting to play messages in playback_mode = {self.config.benchmark_mode}.')\n\t        play_messages_request = PlayMessages.Request()\n\t        play_messages_request.playback_mode = self.config.benchmark_mode.value\n\t        play_messages_request.target_publisher_rate = target_freq\n\t        play_messages_request.message_count = playback_message_count\n\t        play_messages_request.enforce_publisher_rate = self.config.enforce_publisher_rate\n\t        play_messages_request.revise_timestamps_as_message_ids = \\\n", "            self.config.revise_timestamps_as_message_ids\n\t        play_messages_future = play_messages_client.call_async(play_messages_request)\n\t        # Watch playback node timeout\n\t        self.get_logger().info('Waiting for the playback service to finish.')\n\t        play_messages_response = self.get_service_response_from_future_blocking(\n\t            play_messages_future,\n\t            check_success=True,\n\t            timeout_sec=self.config.play_messages_service_future_timeout_sec)\n\t        start_timestamps = {}\n\t        for i in range(len(play_messages_response.timestamps.keys)):\n", "            key = play_messages_response.timestamps.keys[i]\n\t            start_timestamp = play_messages_response.timestamps.timestamps_ns[i]\n\t            start_timestamps[key] = start_timestamp\n\t        # Get end timestamps from all monitors\n\t        monitor_end_timestamps_map = {}\n\t        for monitor_info in self.config.monitor_info_list:\n\t            self.get_logger().info(\n\t                f'Waiting for the monitor service \"{monitor_info.service_name}\" to finish.')\n\t            monitor_response = self.get_service_response_from_future_blocking(\n\t                monitor_service_future_map[monitor_info.service_name])\n", "            end_timestamps = {}\n\t            for i in range(len(monitor_response.timestamps.keys)):\n\t                key = monitor_response.timestamps.keys[i]\n\t                end_timestamp = monitor_response.timestamps.timestamps_ns[i]\n\t                end_timestamps[key] = end_timestamp\n\t            monitor_end_timestamps_map[monitor_info.service_name] = end_timestamps\n\t        # Stop CPU profiler\n\t        if self.config.enable_cpu_profiler:\n\t            self._cpu_profiler.stop_profiling()\n\t            self.get_logger().info('CPU profiling stopped.')\n", "        # Calculate performance results\n\t        performance_results = {}\n\t        for monitor_info in self.config.monitor_info_list:\n\t            end_timestamps = monitor_end_timestamps_map[monitor_info.service_name]\n\t            if len(end_timestamps) == 0:\n\t                error_message = 'No messages were observed from the monitor node ' + \\\n\t                    monitor_info.service_name\n\t                self.get_logger().error(error_message)\n\t                raise RuntimeError(error_message)\n\t            for calculator in monitor_info.calculators:\n", "                performance_results.update(\n\t                    calculator.calculate_performance(start_timestamps, end_timestamps))\n\t        # Add CPU profiler results\n\t        if self.config.enable_cpu_profiler:\n\t            performance_results.update(self._cpu_profiler.get_results())\n\t        return performance_results\n\t    def determine_max_sustainable_framerate(self, test_func) -> float:\n\t        \"\"\"\n\t        Find the maximum sustainable test pulbisher framerate by using autotuner.\n\t        Parameters\n", "        ----------\n\t        test_func\n\t            The benchmark function to be tested\n\t        Returns\n\t        -------\n\t        float\n\t            The maximum sustainable test pulbisher framerate\n\t        \"\"\"\n\t        self.push_logger_name('Probing')\n\t        # Phase 1: Binary Search to identify interval\n", "        current_upper_freq = self.config.publisher_upper_frequency\n\t        current_lower_freq = self.config.publisher_lower_frequency\n\t        # Run a trial run to warm up the graph\n\t        try:\n\t            probe_freq = (current_upper_freq + current_lower_freq) / 2\n\t            message_count = ceil(\n\t                self.config.benchmark_duration *\n\t                self.config.binary_search_duration_fraction *\n\t                probe_freq)\n\t            self.get_logger().info(f'Starting the trial probe at {probe_freq} Hz')\n", "            probe_perf_results = test_func(message_count, probe_freq)\n\t            self.print_report(\n\t                probe_perf_results,\n\t                sub_heading=f'Trial Probe {probe_freq}Hz')\n\t        except Exception:\n\t            self.get_logger().info(\n\t                f'Ignoring an exception occured in the trial probe at {probe_freq} Hz')\n\t        finally:\n\t            self.get_logger().info(\n\t                f'Finished the first trial probe at {probe_freq} Hz')\n", "        # Continue binary search until the search window is small enough to justify a linear scan\n\t        while (\n\t            abs(current_upper_freq - current_lower_freq) >\n\t            self.config.binary_search_terminal_interval_width\n\t        ):\n\t            probe_freq = (current_upper_freq + current_lower_freq) / 2\n\t            self.get_logger().info(\n\t                f'Binary Search: Probing for max sustainable frequency at {probe_freq} Hz')\n\t            # Perform mini-benchmark at this probe frequency\n\t            message_count = ceil(\n", "                self.config.benchmark_duration *\n\t                self.config.binary_search_duration_fraction *\n\t                probe_freq)\n\t            probe_perf_results = test_func(message_count, probe_freq)\n\t            self.print_report(\n\t                probe_perf_results,\n\t                sub_heading=f'Throughput Search Probe {probe_freq}Hz')\n\t            # Check if this probe frequency was sustainable\n\t            first_monitor_perf = self.get_performance_results_of_first_monitor_calculator(\n\t                probe_perf_results)\n", "            if (first_monitor_perf[BasicPerformanceMetrics.MEAN_FRAME_RATE] >=\n\t                first_monitor_perf[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE] -\n\t                self.config.binary_search_acceptable_frame_rate_drop\n\t                ) and (\n\t                    first_monitor_perf[BasicPerformanceMetrics.NUM_MISSED_FRAMES] <=\n\t                    ceil(first_monitor_perf[BasicPerformanceMetrics.NUM_FRAMES_SENT] *\n\t                         self.config.binary_search_acceptable_frame_loss_fraction)\n\t            ):\n\t                current_lower_freq = probe_freq\n\t            else:\n", "                current_upper_freq = probe_freq\n\t        target_freq = current_lower_freq\n\t        # Phase 2: Linear scan through interval from low to high\n\t        # Increase probe frequency, until failures occur or ceiling is reached\n\t        while True:\n\t            probe_freq = target_freq + self.config.linear_scan_step_size\n\t            if probe_freq > self.config.publisher_upper_frequency:\n\t                break\n\t            self.get_logger().info(\n\t                f'Linear Scan: Probing for max sustainable frequency at {probe_freq} Hz')\n", "            # Perform mini-benchmark at this probe frequency\n\t            message_count = ceil(\n\t                self.config.benchmark_duration *\n\t                self.config.linear_scan_duration_fraction *\n\t                probe_freq)\n\t            probe_perf_results = test_func(message_count, probe_freq)\n\t            self.print_report(\n\t                probe_perf_results,\n\t                sub_heading=f'Throughput Search Probe {probe_freq}Hz')\n\t            # Check if this probe frequency was sustainable\n", "            first_monitor_perf = self.get_performance_results_of_first_monitor_calculator(\n\t                probe_perf_results)\n\t            if (first_monitor_perf[BasicPerformanceMetrics.MEAN_FRAME_RATE] >=\n\t                first_monitor_perf[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE] -\n\t                self.config.linear_scan_acceptable_frame_rate_drop\n\t                ) and (\n\t                first_monitor_perf[BasicPerformanceMetrics.NUM_MISSED_FRAMES] <=\n\t                ceil(first_monitor_perf[BasicPerformanceMetrics.NUM_FRAMES_SENT] *\n\t                     self.config.linear_scan_acceptable_frame_loss_fraction)\n\t            ):\n", "                target_freq = probe_freq\n\t            else:\n\t                # The new probe frequency is too high, so terminate the linear scan\n\t                break\n\t        self.get_logger().info(\n\t            f'Final predicted max sustainable frequency was {target_freq}Hz')\n\t        # Check if target frequency is at either lower or higher bound of range\n\t        BOUNDARY_LIMIT_EPSILON = 5  # (Hz)\n\t        if target_freq >= self.config.publisher_upper_frequency - BOUNDARY_LIMIT_EPSILON:\n\t            self.get_logger().warn(\n", "                f'Final playback framerate {target_freq} Hz is close to or above max framerate '\n\t                f'{self.config.publisher_upper_frequency} Hz used in search window. ')\n\t            self.get_logger().warn(\n\t                'Consider increasing this maximum!')\n\t        elif target_freq <= self.config.publisher_lower_frequency + BOUNDARY_LIMIT_EPSILON:\n\t            self.get_logger().warn(\n\t                f'Final playback framerate {target_freq} Hz is close to or below min framerate '\n\t                f'{self.config.publisher_lower_frequency} Hz used in search window. ')\n\t            self.get_logger().warn(\n\t                'Consider decreasing this minimum!')\n", "        self.pop_logger_name()\n\t        return target_freq\n\t    def pre_benchmark_hook(self):\n\t        \"\"\"Override for benchamrk setup.\"\"\"\n\t        pass\n\t    def post_benchmark_hook(self):\n\t        \"\"\"Override for benchmark cleanup.\"\"\"\n\t        pass\n\t    def run_benchmark(self):\n\t        \"\"\"\n", "        Run benchmarking.\n\t        Entry method for running benchmark method self.benchmark_body() under various\n\t        benchmark modes.\n\t        \"\"\"\n\t        self.get_logger().info(f'Starting {self.config.benchmark_name} Benchmark')\n\t        self.get_logger().info('Executing pre-benchmark setup')\n\t        self.pre_benchmark_hook()\n\t        # Prepare playback buffers\n\t        if self.config.enable_trial_buffer_preparation:\n\t            self.push_logger_name('Trial')\n", "            self.get_logger().info('Starting trial message buffering')\n\t            self.prepare_buffer()\n\t            self.pop_logger_name()\n\t        self.get_logger().info('Buffering test messages')\n\t        self.prepare_buffer()\n\t        self.get_logger().info('Finished buffering test messages')\n\t        self.get_logger().info(f'Running benchmark: mode={self.config.benchmark_mode}')\n\t        perf_results = {}\n\t        if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n\t            perf_results = self.run_benchmark_timeline_playback_mode()\n", "        elif (self.config.benchmark_mode == BenchmarkMode.LOOPING) or \\\n\t             (self.config.benchmark_mode == BenchmarkMode.SWEEPING):\n\t            perf_results = self.run_benchmark_looping_mode()\n\t        final_report = self.construct_final_report(perf_results)\n\t        self.print_report(final_report, sub_heading='Final Report')\n\t        self.export_report(final_report)\n\t    def run_benchmark_timeline_playback_mode(self) -> dict:\n\t        self.push_logger_name('Timeline')\n\t        playback_message_count = \\\n\t            int(self.config.benchmark_duration * self.config.publisher_upper_frequency)\n", "        perf_results = self.benchmark_body(\n\t            playback_message_count,\n\t            self.config.publisher_upper_frequency)\n\t        self.pop_logger_name()\n\t        return perf_results\n\t    def run_benchmark_looping_mode(self) -> dict:\n\t        \"\"\"\n\t        Run benchmarking.\n\t        This benchmark method aims to test the maximum output framerate for the\n\t        benchmark method self.benchmark_body().\n", "        \"\"\"\n\t        self.push_logger_name('Looping')\n\t        # Run Autotuner\n\t        self.get_logger().info('Running autotuner')\n\t        target_freq = self.determine_max_sustainable_framerate(self.benchmark_body)\n\t        self._peak_throughput_prediction = target_freq\n\t        self.get_logger().info(\n\t            'Finished autotuning. '\n\t            f'Running full-scale benchmark for predicted peak frame rate: {target_freq}')\n\t        playback_message_count = int(self.config.benchmark_duration * target_freq)\n", "        # Run benchmark iterations\n\t        self.reset_performance_calculators()\n\t        self._cpu_profiler.reset()\n\t        for i in range(self.config.test_iterations):\n\t            self.get_logger().info(f'Starting Iteration {i+1}')\n\t            performance_results = self.benchmark_body(\n\t                playback_message_count,\n\t                target_freq)\n\t            self.print_report(performance_results, sub_heading=f'{target_freq}Hz #{i+1}')\n\t        # Conclude performance measurements from the iteratoins\n", "        final_perf_results = {}\n\t        for monitor_info in self.config.monitor_info_list:\n\t            for calculator in monitor_info.calculators:\n\t                final_perf_results.update(calculator.conclude_performance())\n\t        # Conclude CPU profiler data\n\t        if self.config.enable_cpu_profiler:\n\t            final_perf_results.update(self._cpu_profiler.conclude_results())\n\t        # Run additional fixed rate test\n\t        self.push_logger_name('FixedRate')\n\t        additional_test_fixed_publisher_rates = \\\n", "            set(self.config.additional_fixed_publisher_rate_tests)\n\t        self.get_logger().info(\n\t            'Starting fixed publisher rate tests for: '\n\t            f'{additional_test_fixed_publisher_rates}')\n\t        for target_freq in additional_test_fixed_publisher_rates:\n\t            first_monitor_perf = self.get_performance_results_of_first_monitor_calculator(\n\t                final_perf_results)\n\t            mean_pub_fps = first_monitor_perf[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE]\n\t            if mean_pub_fps*1.05 < target_freq:\n\t                self.get_logger().info(\n", "                    f'Skipped testing the fixed publisher rate for {target_freq}fps '\n\t                    'as it is higher than the previously measured max sustainable '\n\t                    f'rate: {mean_pub_fps}')\n\t                continue\n\t            self.get_logger().info(\n\t                f'Testing fixed publisher rate at {target_freq}fps')\n\t            playback_message_count = int(self.config.benchmark_duration * target_freq)\n\t            performance_results = self.benchmark_body(\n\t                playback_message_count,\n\t                target_freq)\n", "            self.print_report(performance_results, sub_heading=f'Fixed {target_freq} FPS')\n\t            # Add the test result to the output metrics\n\t            final_perf_results[f'{target_freq}fps'] = performance_results\n\t        self.pop_logger_name()\n\t        return final_perf_results\n\t    def get_performance_results_of_first_monitor_calculator(self, performance_results):\n\t        \"\"\"Get dict that contains the first calculator's performance results.\"\"\"\n\t        calculator = self.config.monitor_info_list[0].calculators[0]\n\t        if 'report_prefix' in calculator.config and calculator.config['report_prefix']:\n\t            return performance_results[calculator.config['report_prefix']]\n", "        return performance_results\n\t    def reset_performance_calculators(self):\n\t        \"\"\"Reset the states of all performance calculators associated with all monitors.\"\"\"\n\t        for monitor_info in self.config.monitor_info_list:\n\t            for calculator in monitor_info.calculators:\n\t                calculator.reset()\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/__init__.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"Imports for ros2_benchmark module.\"\"\"\n\tfrom .basic_performance_calculator import BasicPerformanceCalculator, BasicPerformanceMetrics\n\tfrom .ros2_benchmark_config import BenchmarkMode, MonitorPerformanceCalculatorsInfo\n\tfrom .ros2_benchmark_config import ROS2BenchmarkConfig\n", "from .ros2_benchmark_test import BenchmarkMetadata, ROS2BenchmarkTest\n\tfrom .utils.image_utility import ImageResolution, Resolution\n\t__all__ = [\n\t    'BasicPerformanceCalculator',\n\t    'BasicPerformanceMetrics',\n\t    'BenchmarkMetadata',\n\t    'BenchmarkMode',\n\t    'ImageResolution',\n\t    'MonitorPerformanceCalculatorsInfo',\n\t    'Resolution',\n", "    'ROS2BenchmarkConfig',\n\t    'ROS2BenchmarkTest',\n\t]\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/basic_performance_calculator.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\tfrom enum import Enum\n\timport numbers\n\timport numpy\n\timport rclpy\n", "class BasicPerformanceMetrics(Enum):\n\t    \"\"\"Basic performance metrics.\"\"\"\n\t    RECEIVED_DURATION = 'Delta between First & Last Received Frames (ms)'\n\t    MEAN_PLAYBACK_FRAME_RATE = 'Mean Playback Frame Rate (fps)'\n\t    MEAN_FRAME_RATE = 'Mean Frame Rate (fps)'\n\t    NUM_MISSED_FRAMES = '# of Missed Frames'\n\t    NUM_FRAMES_SENT = '# of Frames Sent'\n\t    FIRST_SENT_RECEIVED_LATENCY = 'First Sent to First Received Latency (ms)'\n\t    LAST_SENT_RECEIVED_LATENCY = 'Last Sent to Last Received Latency (ms)'\n\t    FIRST_INPUT_LATENCY = 'First Frame End-to-end Latency (ms)'\n", "    LAST_INPUT_LATENCY = 'Last Frame End-to-end Latency (ms)'\n\t    MAX_LATENCY = 'Max. End-to-End Latency (ms)'\n\t    MIN_LATENCY = 'Min. End-to-End Latency (ms)'\n\t    MEAN_LATENCY = 'Mean End-to-End Latency (ms)'\n\t    MAX_JITTER = 'Max. Frame-to-Frame Jitter (ms)'\n\t    MIN_JITTER = 'Min. Frame-to-Frame Jitter (ms)'\n\t    MEAN_JITTER = 'Mean Frame-to-Frame Jitter (ms)'\n\t    STD_DEV_JITTER = 'Frame-to-Frame Jitter Std. Deviation (ms)'\n\tclass BasicPerformanceCalculator():\n\t    \"\"\"Calculator that computes performance with basic metrics.\"\"\"\n", "    def __init__(self, config: dict = {}) -> None:\n\t        \"\"\"Initialize the calculator.\"\"\"\n\t        self.config = config\n\t        self._report_prefix = config.get('report_prefix', '')\n\t        self._message_key_match = config.get('message_key_match', False)\n\t        self._logger = None\n\t        self._perf_data_list = []\n\t    def set_logger(self, logger):\n\t        \"\"\"Set logger that enables to print log messages.\"\"\"\n\t        self._logger = logger\n", "    def get_logger(self):\n\t        \"\"\"Get logger for printing log messages.\"\"\"\n\t        if self._logger is not None:\n\t            return self._logger\n\t        return rclpy.logging.get_logger(self.__class__.__name__)\n\t    def get_info(self):\n\t        \"\"\"Return a dict containing information for loading this calculator class.\"\"\"\n\t        info = {}\n\t        info['module_name'] = self.__class__.__module__\n\t        info['class_name'] = self.__class__.__name__\n", "        info['config'] = self.config\n\t        return info\n\t    def reset(self):\n\t        \"\"\"Reset the calculator state.\"\"\"\n\t        self._perf_data_list.clear()\n\t    def calculate_performance(self,\n\t                              start_timestamps_ns: dict,\n\t                              end_timestamps_ns: dict) -> dict:\n\t        \"\"\"Calculate performance based on message start and end timestamps.\"\"\"\n\t        perf_data = {}\n", "        num_of_frame_sent = len(start_timestamps_ns)\n\t        num_of_frame_dropped = len(start_timestamps_ns) - len(end_timestamps_ns)\n\t        # BasicPerformanceMetrics.RECEIVED_DURATION\n\t        last_end_timestamp_ms = list(end_timestamps_ns.values())[-1] / 10**6\n\t        first_end_timestamp_ms = list(end_timestamps_ns.values())[0] / 10**6\n\t        received_duration_ms = last_end_timestamp_ms - first_end_timestamp_ms\n\t        perf_data[BasicPerformanceMetrics.RECEIVED_DURATION] = received_duration_ms\n\t        # BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE\n\t        if (len(start_timestamps_ns) > 1):\n\t            last_sent_time_ms = list(start_timestamps_ns.values())[-1] / 10**6\n", "            first_sent_time_ms = list(start_timestamps_ns.values())[0] / 10**6\n\t            sent_duration_ms = last_sent_time_ms - first_sent_time_ms\n\t            perf_data[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE] = len(\n\t                start_timestamps_ns) / (sent_duration_ms / 1000.0)\n\t        else:\n\t            self.get_logger().warning(\n\t                'Could not compute MEAN_PLAYBACK_FRAME_RATE due to insufficient start '\n\t                f'timestamps received: {len(start_timestamps_ns)} was received')\n\t        # BasicPerformanceMetrics.MEAN_FRAME_RATE\n\t        if received_duration_ms > 0:\n", "            perf_data[BasicPerformanceMetrics.MEAN_FRAME_RATE] = len(\n\t                end_timestamps_ns) / (received_duration_ms / 1000.0)\n\t        else:\n\t            self.get_logger().warning(\n\t                'Could not compute MEAN_FRAME_RATE due to an invalid value of'\n\t                f'RECEIVED_DURATION = {received_duration_ms}')\n\t            self.get_logger().warning(\n\t                'This could be caused by insufficient timestamps received: '\n\t                f'#start_timestamps={len(start_timestamps_ns)} '\n\t                f'#end_timestamps = {len(end_timestamps_ns)}')\n", "        perf_data[BasicPerformanceMetrics.NUM_MISSED_FRAMES] = num_of_frame_dropped\n\t        perf_data[BasicPerformanceMetrics.NUM_FRAMES_SENT] = num_of_frame_sent\n\t        perf_data[BasicPerformanceMetrics.FIRST_SENT_RECEIVED_LATENCY] = \\\n\t            first_end_timestamp_ms - first_sent_time_ms\n\t        perf_data[BasicPerformanceMetrics.LAST_SENT_RECEIVED_LATENCY] = \\\n\t            last_end_timestamp_ms - last_sent_time_ms\n\t        if self._message_key_match:\n\t            # Calculate latency between sent and received messages\n\t            end_to_end_latencies_ms = []\n\t            for message_key, start_timestamp_ns in start_timestamps_ns.items():\n", "                if message_key in end_timestamps_ns:\n\t                    end_to_end_latencies_ms.append(\n\t                        (end_timestamps_ns[message_key] - start_timestamp_ns) / 10**6)\n\t            if len(end_to_end_latencies_ms) > 0:\n\t                perf_data[BasicPerformanceMetrics.FIRST_INPUT_LATENCY] = \\\n\t                    end_to_end_latencies_ms[0]\n\t                perf_data[BasicPerformanceMetrics.LAST_INPUT_LATENCY] = \\\n\t                    end_to_end_latencies_ms[-1]\n\t                perf_data[BasicPerformanceMetrics.MAX_LATENCY] = \\\n\t                    max(end_to_end_latencies_ms)\n", "                perf_data[BasicPerformanceMetrics.MIN_LATENCY] = \\\n\t                    min(end_to_end_latencies_ms)\n\t                perf_data[BasicPerformanceMetrics.MEAN_LATENCY] = \\\n\t                    sum(end_to_end_latencies_ms) / len(end_to_end_latencies_ms)\n\t            else:\n\t                self.get_logger().warning('No end-to-end latency data available.')\n\t        # Calculate frame-to-frame jitter if at least 3 valid end timestamps are received\n\t        if len(end_timestamps_ns) > 2:\n\t            np_end_timestamps_ms = (numpy.array(list(end_timestamps_ns.values())))/(10**6)\n\t            jitters = numpy.abs(numpy.diff(numpy.diff(np_end_timestamps_ms)))\n", "            perf_data[BasicPerformanceMetrics.MAX_JITTER] = float(numpy.max(jitters))\n\t            perf_data[BasicPerformanceMetrics.MIN_JITTER] = float(numpy.min(jitters))\n\t            perf_data[BasicPerformanceMetrics.MEAN_JITTER] = float(numpy.mean(jitters))\n\t            perf_data[BasicPerformanceMetrics.STD_DEV_JITTER] = float(numpy.std(\n\t                jitters))\n\t        else:\n\t            self.get_logger().warning(\n\t                'Received insufficient end timestamps for calculating frame-to-frame jitters.'\n\t                f'3 were needed but only {len(end_timestamps_ns)} timestamp(s) were received.')\n\t        # Store the current perf results to be concluded later\n", "        self._perf_data_list.append(perf_data)\n\t        if self._report_prefix != '':\n\t            return {self._report_prefix: perf_data}\n\t        return perf_data\n\t    def conclude_performance(self) -> dict:\n\t        \"\"\"Calculate final statistical performance outcome based on all results.\"\"\"\n\t        if len(self._perf_data_list) == 0:\n\t            self.get_logger().warn('No prior performance measurements to conclude')\n\t            return {}\n\t        MEAN_METRICS = [\n", "            BasicPerformanceMetrics.NUM_FRAMES_SENT,\n\t            BasicPerformanceMetrics.FIRST_INPUT_LATENCY,\n\t            BasicPerformanceMetrics.LAST_INPUT_LATENCY,\n\t            BasicPerformanceMetrics.FIRST_SENT_RECEIVED_LATENCY,\n\t            BasicPerformanceMetrics.LAST_SENT_RECEIVED_LATENCY,\n\t            BasicPerformanceMetrics.MEAN_LATENCY,\n\t            BasicPerformanceMetrics.NUM_MISSED_FRAMES,\n\t            BasicPerformanceMetrics.RECEIVED_DURATION,\n\t            BasicPerformanceMetrics.MEAN_FRAME_RATE,\n\t            BasicPerformanceMetrics.STD_DEV_JITTER,\n", "            BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE,\n\t        ]\n\t        MAX_METRICS = [\n\t            BasicPerformanceMetrics.MAX_LATENCY,\n\t            BasicPerformanceMetrics.MAX_JITTER,\n\t            BasicPerformanceMetrics.MEAN_JITTER,\n\t        ]\n\t        MIN_METRICS = [\n\t            BasicPerformanceMetrics.MIN_LATENCY,\n\t            BasicPerformanceMetrics.MIN_JITTER,\n", "        ]\n\t        final_perf_data = {}\n\t        for metric in BasicPerformanceMetrics:\n\t            metric_value_list = [perf_data.get(metric, None) for perf_data in self._perf_data_list]\n\t            if not all(isinstance(value, numbers.Number) for value in metric_value_list):\n\t                continue\n\t            # Remove the best and the worst before concluding the metric\n\t            metric_value_list.remove(max(metric_value_list))\n\t            metric_value_list.remove(min(metric_value_list))\n\t            if metric in MEAN_METRICS:\n", "                final_perf_data[metric] = sum(metric_value_list)/len(metric_value_list)\n\t            elif metric in MAX_METRICS:\n\t                final_perf_data[metric] = max(metric_value_list)\n\t            elif metric in MIN_METRICS:\n\t                final_perf_data[metric] = min(metric_value_list)\n\t            else:\n\t                final_perf_data[metric] = 'INVALID VALUES: NO CONCLUDED METHOD ASSIGNED'\n\t        self.reset()\n\t        return final_perf_data\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/ros2_benchmark_config.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\tfrom enum import Enum\n\timport importlib\n\timport os\n\timport sys\n", "import yaml\n\tfrom .basic_performance_calculator import BasicPerformanceCalculator\n\tfrom .utils.image_utility import Resolution\n\tBUILTIN_ROS2_BENCHMARK_CONFIG_FILE = os.path.join(\n\t    os.path.dirname(__file__),\n\t    'default_ros2_benchmark_config.yaml')\n\tclass MonitorPerformanceCalculatorsInfo:\n\t    def __init__(self,\n\t                 service_name='start_monitoring0',\n\t                 calculators=[BasicPerformanceCalculator()]) -> None:\n", "        \"\"\"Initialize a monitor's performance calculators info.\"\"\"\n\t        self.service_name = service_name\n\t        self.calculators = calculators\n\t    def get_info(self):\n\t        \"\"\"Return a dict containing information for setting this monitor info object.\"\"\"\n\t        info = {}\n\t        info['service_name'] = self.service_name\n\t        info['calculators'] = []\n\t        for calculator in self.calculators:\n\t            info['calculators'].append(calculator.get_info())\n", "        return info\n\tclass BenchmarkMode(Enum):\n\t    \"\"\"\n\t    Benchmark modes supported in the framework.\n\t    The enum values must match what is defined in ros2_benchmark_interfaces::srv::PlayMessage\n\t    \"\"\"\n\t    TIMELINE = 0\n\t    LOOPING = 1\n\t    SWEEPING = 2\n\tclass ROS2BenchmarkConfig():\n", "    \"\"\"A class that holds configurations for ros2_benchmark.\"\"\"\n\t    __builtin_config_file_path = BUILTIN_ROS2_BENCHMARK_CONFIG_FILE\n\t    # It is only necessary to add a parameter to this map if we want\n\t    # to enable overriding such a parameter from an env variable and\n\t    # its type is not string (as env only supports string values).\n\t    __config_type_map = {\n\t        'revise_timestamps_as_message_ids': bool,\n\t        'enable_cpu_profiler': bool,\n\t        'publish_tf_messages_in_set_data': bool,\n\t        'publish_tf_static_messages_in_set_data': bool,\n", "        'load_data_in_real_time': bool,\n\t        'record_data_timeline': bool,\n\t        'enable_trial_buffer_preparation': bool,\n\t        'cpu_profiling_interval_sec': float,\n\t        'benchmark_duration': float,\n\t        'setup_service_client_timeout_sec': float,\n\t        'start_recording_service_timeout_sec': int,\n\t        'start_monitoring_service_timeout_sec': int,\n\t        'default_service_future_timeout_sec': float,\n\t        'set_data_service_future_timeout_sec': float,\n", "        'start_recording_service_future_timeout_sec': float,\n\t        'play_messages_service_future_timeout_sec': float,\n\t        'test_iterations': int,\n\t        'playback_message_buffer_size': int,\n\t        'publisher_upper_frequency': float,\n\t        'publisher_lower_frequency': float,\n\t        'enforce_publisher_rate': bool,\n\t        'binary_search_terminal_interval_width': float,\n\t        'binary_search_duration_fraction': float,\n\t        'binary_search_acceptable_frame_loss_fraction': float,\n", "        'binary_search_acceptable_frame_rate_drop': float,\n\t        'linear_scan_step_size': float,\n\t        'linear_scan_duration_fraction': float,\n\t        'linear_scan_acceptable_frame_loss_fraction': float,\n\t        'linear_scan_acceptable_frame_rate_drop': float,\n\t        'input_data_start_time': float,\n\t        'input_data_end_time': float\n\t    }\n\t    def __init__(self, config_file_path: str = '', *args, **kw):\n\t        \"\"\"Initialize default and given configs and apply to attribtues.\"\"\"\n", "        self.apply_to_attributes(dict(*args, **kw))\n\t        if config_file_path != '':\n\t            try:\n\t                self.apply_to_attributes(\n\t                    load_config_file(config_file_path)['ros2_benchmark_config'],\n\t                    override=False)\n\t            except (FileNotFoundError, yaml.YAMLError, TypeError) as error:\n\t                print('Failed to load a custom benchmark config file.')\n\t                raise error\n\t        try:\n", "            self.apply_to_attributes(\n\t                load_config_file(self.__builtin_config_file_path)['ros2_benchmark_config'],\n\t                override=False)\n\t        except (FileNotFoundError, yaml.YAMLError, TypeError) as error:\n\t            print('Failed to load a default benchmark config file.')\n\t            raise error\n\t    def apply_to_attributes(self, config_dict, override=True):\n\t        \"\"\"Apply the given configuration key-value pairs to instance attributes.\"\"\"\n\t        for key, value in config_dict.items():\n\t            if override is False and hasattr(self, key):\n", "                continue\n\t            if key == 'benchmark_mode' and isinstance(value, str):\n\t                if value not in BenchmarkMode.__members__:\n\t                    raise TypeError(f'Unknown benchmark mode: \"{value}\"')\n\t                setattr(self, key, BenchmarkMode.__members__[value])\n\t            elif key == 'monitor_info_list':\n\t                monitor_info_list = []\n\t                for monitor_info in value:\n\t                    if isinstance(monitor_info, MonitorPerformanceCalculatorsInfo):\n\t                        monitor_info_list.append(monitor_info)\n", "                        continue\n\t                    calculator_list = []\n\t                    for calculator in monitor_info['calculators']:\n\t                        calculator_module = importlib.import_module(calculator['module_name'])\n\t                        calculator_class = getattr(calculator_module, calculator['class_name'])\n\t                        calculator_config = calculator['config'] if 'config' in calculator else {}\n\t                        calculator_list.append(calculator_class(calculator_config))\n\t                    monitor_info_list.append(\n\t                        MonitorPerformanceCalculatorsInfo(\n\t                            monitor_info['service_name'],\n", "                            calculator_list))\n\t                setattr(self, key, monitor_info_list)\n\t            else:\n\t                if key in self.__config_type_map:\n\t                    value_type = self.__config_type_map[key]\n\t                    if value_type is bool and isinstance(value, str):\n\t                        if value.lower() in ['false', '0']:\n\t                            value = False\n\t                        else:\n\t                            value = True\n", "                    setattr(self, key, self.__config_type_map[key](value))\n\t                else:\n\t                    setattr(self, key, value)\n\t    def to_yaml_str(self):\n\t        \"\"\"Export all configurations as a YAML string.\"\"\"\n\t        yaml.add_representer(Resolution, Resolution.yaml_representer)\n\t        config_dict = {}\n\t        for key, value in self.__dict__.items():\n\t            if key == 'benchmark_mode':\n\t                config_dict[key] = str(value)\n", "            elif key == 'monitor_info_list':\n\t                monitor_info_list_export = []\n\t                for monitor_info in value:\n\t                    monitor_info_list_export.append(monitor_info.get_info())\n\t                config_dict[key] = monitor_info_list_export\n\t            else:\n\t                config_dict[key] = value\n\t        return yaml.dump({'ros2_benchmark_config': config_dict}, allow_unicode=True)\n\tdef load_config_file(config_file_path: str):\n\t    \"\"\"Load a benchmark configuration file and return its dict object.\"\"\"\n", "    try:\n\t        with open(config_file_path) as config_file:\n\t            return yaml.safe_load(config_file.read())\n\t    except FileNotFoundError as error:\n\t        print('Could not find benchmark config file at '\n\t              f'\"{config_file_path}\".', sys.stderr)\n\t        raise error\n\t    except yaml.YAMLError as error:\n\t        print('Invalid benchmark configs detected in '\n\t              f'\"{config_file_path}\".', sys.stderr)\n", "        raise error\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/cpu_profiler.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2021-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"CPU profiler class to measure performance of benchmark tests.\"\"\"\n\tfrom enum import Enum\n\timport numbers\n\tfrom pathlib import Path\n", "from threading import Thread\n\timport numpy as np\n\timport psutil\n\tfrom .profiler import Profiler\n\tclass CPUProfilingMetrics(Enum):\n\t    \"\"\"Metrics for CPU profiling.\"\"\"\n\t    MAX_CPU_UTIL = 'Max. CPU Util. (%)'\n\t    MIN_CPU_UTIL = 'Min. CPU Util. (%)'\n\t    MEAN_CPU_UTIL = 'Mean CPU Util. (%)'\n\t    STD_DEV_CPU_UTIL = 'Std. Deviation CPU Util. (%)'\n", "    BASELINE_CPU_UTIL = 'Baseline CPU Util. (%)'\n\tclass CPUProfiler(Profiler):\n\t    \"\"\"CPU profiler class to measure CPU performance of benchmark tests.\"\"\"\n\t    def __init__(self):\n\t        \"\"\"Construct CPU profiler.\"\"\"\n\t        super().__init__()\n\t    def start_profiling(self, interval: float = 1.0) -> Path:\n\t        \"\"\"\n\t        Start CPU profiling thread to keep track of performance metrics.\n\t        Parameters\n", "        ----------\n\t        interval: float\n\t            The interval between measurements, in seconds\n\t        \"\"\"\n\t        super().start_profiling()\n\t        # While the is_running flag is true, log CPU usage\n\t        def psutil_log():\n\t            with open(self._log_file_path, 'w+') as logfile:\n\t                while self._is_running:\n\t                    logfile.write(\n", "                        f'{psutil.cpu_percent(interval=interval, percpu=True)}\\n')\n\t        self.psutil_thread = Thread(target=psutil_log)\n\t        self.psutil_thread.start()\n\t        return self._log_file_path\n\t    def stop_profiling(self):\n\t        \"\"\"Stop profiling.\"\"\"\n\t        if self._is_running:\n\t            super().stop_profiling()\n\t            # Wait for thread to stop\n\t            self.psutil_thread.join()\n", "    @staticmethod\n\t    def get_current_cpu_usage():\n\t        \"\"\"Return current CPU usage.\"\"\"\n\t        return np.mean(psutil.cpu_percent(interval=1.0, percpu=True))\n\t    def get_results(self, log_file_path=None) -> dict:\n\t        \"\"\"Return CPU profiling results.\"\"\"\n\t        assert not self._is_running, 'Cannot collect results until profiler has been stopped!'\n\t        log_file_path = self._log_file_path if log_file_path is None else log_file_path\n\t        assert self._log_file_path is not None, 'No log file for reading CPU  profiling results.'\n\t        profile_data = {}\n", "        with open(log_file_path) as logfile:\n\t            cpu_values = []\n\t            for line in logfile.readlines():\n\t                # Remove brackets from line before splitting entries by comma\n\t                cpu_values.append(np.mean([float(v)\n\t                                  for v in line[1:-2].split(',')]))\n\t            cpu_values = np.array(cpu_values)\n\t            profile_data[CPUProfilingMetrics.MAX_CPU_UTIL] = np.max(cpu_values)\n\t            profile_data[CPUProfilingMetrics.MIN_CPU_UTIL] = np.min(cpu_values)\n\t            profile_data[CPUProfilingMetrics.MEAN_CPU_UTIL] = np.mean(cpu_values)\n", "            profile_data[CPUProfilingMetrics.STD_DEV_CPU_UTIL] = np.std(cpu_values)\n\t            profile_data[CPUProfilingMetrics.BASELINE_CPU_UTIL] = cpu_values[0]\n\t        self._profile_data_list.append(profile_data)\n\t        return profile_data\n\t    def reset(self):\n\t        \"\"\"Reset the profiler state.\"\"\"\n\t        self._profile_data_list.clear()\n\t        return\n\t    def conclude_results(self) -> dict:\n\t        \"\"\"Conclude final profiling outcome based on all previous results.\"\"\"\n", "        if len(self._profile_data_list) == 0:\n\t            self.get_logger().warn('No prior profile data to conclude')\n\t            return {}\n\t        MEAN_METRICS = [\n\t            CPUProfilingMetrics.MEAN_CPU_UTIL,\n\t            CPUProfilingMetrics.STD_DEV_CPU_UTIL,\n\t            CPUProfilingMetrics.BASELINE_CPU_UTIL\n\t        ]\n\t        MAX_METRICS = [\n\t            CPUProfilingMetrics.MAX_CPU_UTIL\n", "        ]\n\t        MIN_METRICS = [\n\t            CPUProfilingMetrics.MIN_CPU_UTIL\n\t        ]\n\t        final_profile_data = {}\n\t        for metric in CPUProfilingMetrics:\n\t            metric_value_list = [profile_data.get(metric, None) for\n\t                                 profile_data in self._profile_data_list]\n\t            if not all(isinstance(value, numbers.Number) for value in metric_value_list):\n\t                continue\n", "            # Remove the best and the worst before concluding the metric\n\t            metric_value_list.remove(max(metric_value_list))\n\t            metric_value_list.remove(min(metric_value_list))\n\t            if metric in MEAN_METRICS:\n\t                final_profile_data[metric] = sum(metric_value_list)/len(metric_value_list)\n\t            elif metric in MAX_METRICS:\n\t                final_profile_data[metric] = max(metric_value_list)\n\t            elif metric in MIN_METRICS:\n\t                final_profile_data[metric] = min(metric_value_list)\n\t            else:\n", "                final_profile_data[metric] = 'INVALID VALUES: NO CONCLUDED METHOD ASSIGNED'\n\t        self.reset()\n\t        return final_profile_data\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/image_utility.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\tclass Resolution:\n\t    \"\"\"Generic resolution value holder.\"\"\"\n\t    def __init__(self, width, height, name=''):\n\t        \"\"\"Initialize the resolution.\"\"\"\n", "        self._dict = {}\n\t        self._dict['width'] = int(width)\n\t        self._dict['height'] = int(height)\n\t        self._dict['name'] = name\n\t    def __str__(self) -> str:\n\t        out_str = f'({self[\"width\"]},{self[\"height\"]})'\n\t        if self['name']:\n\t            out_str = f'{self[\"name\"]} {out_str}'\n\t        return out_str\n\t    def __setitem__(self, key, value):\n", "        self._dict[key] = value\n\t    def __getitem__(self, key):\n\t        return self._dict[key]\n\t    def __repr__(self):\n\t        return f'Resolution({self[\"width\"]}, {self[\"height\"]}, {self[\"name\"]})'\n\t    def yaml_representer(dumper, data):\n\t        \"\"\"Support dumping a Resolution object in YAML.\"\"\"\n\t        return dumper.represent_scalar('tag:yaml.org,2002:str', repr(data))\n\tclass ImageResolution:\n\t    \"\"\"Common image resolutions.\"\"\"\n", "    QUARTER_HD = Resolution(960, 540, 'Quarter HD')\n\t    VGA = Resolution(640, 480, 'VGA')\n\t    WVGA = Resolution(720, 480, 'WVGA')\n\t    HD = Resolution(1280, 720, 'HD')\n\t    FULL_HD = Resolution(1920, 1080, 'Full HD')\n\t    FOUR_K = Resolution(3840, 2160, '4K')\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/nsys_utility.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\timport datetime\n\tfrom inspect import signature\n\timport platform\n\tfrom launch.actions import DeclareLaunchArgument\n", "from launch.conditions import IfCondition\n\tfrom launch.substitutions import LaunchConfiguration\n\tclass NsysUtility():\n\t    \"\"\"Utilities for enabling Nsight System profiling.\"\"\"\n\t    @staticmethod\n\t    def generate_launch_args():\n\t        \"\"\"\n\t        Generate launch args for nsight systme profiling.\n\t        Usage Example: launch_test <path to your script>\n\t                       enable_nsys:=<true, false>\n", "                       nsys_profile_name:=<your profile output name>\n\t                       nsys_profile_flags:=<your profiling flags>\n\t        \"\"\"\n\t        return [\n\t            DeclareLaunchArgument('enable_nsys', default_value='false',\n\t                                  description='Enable nsys profiling'),\n\t            DeclareLaunchArgument('nsys_profile_name', default_value='',\n\t                                  description='Label to append for nsys profile output'),\n\t            DeclareLaunchArgument('nsys_profile_flags', default_value='--trace=osrt,nvtx,cuda',\n\t                                  description='Flags for nsys profile')\n", "        ]\n\t    @staticmethod\n\t    def generate_nsys_prefix(context):\n\t        \"\"\"Generate prefi for nsight systme profiling.\"\"\"\n\t        enable_nsys = IfCondition(LaunchConfiguration(\n\t            'enable_nsys')).evaluate(context)\n\t        nsys_profile_name = LaunchConfiguration(\n\t            'nsys_profile_name').perform(context)\n\t        nsys_profile_flags = LaunchConfiguration(\n\t            'nsys_profile_flags').perform(context)\n", "        container_prefix = ''\n\t        if enable_nsys:\n\t            if(not nsys_profile_name):\n\t                current_time = datetime.datetime.now(datetime.timezone.utc).\\\n\t                               strftime('%Y-%m-%dT%H:%M:%SZ')\n\t                nsys_profile_name = f'profile_{platform.machine()}_{current_time}'\n\t            container_prefix = f'nsys profile {nsys_profile_flags} -o {nsys_profile_name}'\n\t        return (enable_nsys, container_prefix)\n\t    @staticmethod\n\t    def launch_setup_wrapper(context, launch_setup):\n", "        \"\"\"Invoke the launch_setup method with nsys parameters for ComposableNodeContainer.\"\"\"\n\t        enable_nsys, container_prefix = NsysUtility.generate_nsys_prefix(context)\n\t        launch_setup_parameters = signature(launch_setup).parameters\n\t        if (not all(param in launch_setup_parameters for param in\n\t                    ['container_prefix', 'container_sigterm_timeout'])):\n\t            if enable_nsys:\n\t                raise RuntimeError(\n\t                    'Incorrect launch_setup signature. '\n\t                    'When Nsys is enbaled, the signature must be: '\n\t                    'def launch_setup(container_prefix, container_sigterm_timeout)')\n", "            else:\n\t                return launch_setup()\n\t        container_sigterm_timeout = '1000' if enable_nsys else '5'\n\t        return launch_setup(\n\t            container_prefix=container_prefix,\n\t            container_sigterm_timeout=container_sigterm_timeout)\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/__init__.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/ros2_utility.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\timport time\n\timport rclpy\n\tclass ClientUtility:\n\t    \"\"\"A class for hosting utility methods for ROS 2 serevice clients.\"\"\"\n", "    @staticmethod\n\t    def create_service_client_blocking(node, service_type, service_name, timeout_sec):\n\t        \"\"\"Create a service client and wait for it to be available.\"\"\"\n\t        service_client = node.create_client(service_type, service_name)\n\t        start_time = time.time()\n\t        while not service_client.wait_for_service(timeout_sec=1):\n\t            node.get_logger().info(\n\t                f'{service_name} service is not available yet, waiting...')\n\t            if (time.time() - start_time) > timeout_sec:\n\t                node.get_logger().info(\n", "                    f'Creating {service_name} service client timed out')\n\t                return None\n\t        return service_client\n\t    @staticmethod\n\t    def get_service_response_from_future_blocking(node, future, timeout_sec):\n\t        \"\"\"Block and wait for a service future to return.\"\"\"\n\t        start_time = time.time()\n\t        while not future.done():\n\t            rclpy.spin_once(node)\n\t            if (time.time() - start_time) > timeout_sec:\n", "                node.get_logger().info(\n\t                    f'Waiting for a service future timed out ({timeout_sec}s)')\n\t                return None\n\t        return future.result()\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/profiler.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n\t# Copyright (c) 2021-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\t# SPDX-License-Identifier: Apache-2.0\n\t\"\"\"Profiler base class to measure the performance of benchmark tests.\"\"\"\n\tfrom abc import ABC, abstractmethod\n\tfrom datetime import datetime\n\timport os\n", "class Profiler(ABC):\n\t    \"\"\"Profiler base class to measure the performance of benchmark tests.\"\"\"\n\t    DEFAILT_LOG_DIR = '/tmp'\n\t    @abstractmethod\n\t    def __init__(self):\n\t        \"\"\"Construct profiler.\"\"\"\n\t        self._is_running = False\n\t        # Logfile path is generated once start_profiling() is called\n\t        self._log_file_path = None\n\t        self._profile_data_list = []\n", "    @abstractmethod\n\t    def start_profiling(self, log_dir=DEFAILT_LOG_DIR) -> None:\n\t        \"\"\"\n\t        Run profiling program to keep track of performance metrics.\n\t        Parameters\n\t        ----------\n\t        log_dir\n\t            Path to write the logs to\n\t        \"\"\"\n\t        assert not self._is_running, 'Profiler has already been started!'\n", "        self._is_running = True\n\t        # Create log file folders if they don't exist already\n\t        os.makedirs(log_dir, exist_ok=True)\n\t        self._log_file_path = os.path.join(\n\t            log_dir,\n\t            f'{type(self).__name__}_{datetime.timestamp(datetime.now())}.log')\n\t        return self._log_file_path\n\t    @abstractmethod\n\t    def stop_profiling(self) -> None:\n\t        \"\"\"Stop profiling.\"\"\"\n", "        self._is_running = False\n\t    @abstractmethod\n\t    def get_results(self, log_file_path=None) -> dict:\n\t        \"\"\"Return profiling results.\"\"\"\n\t        return {}\n\t    @abstractmethod\n\t    def reset(self):\n\t        \"\"\"Reset the profiler state.\"\"\"\n\t        self._profile_data_list.clear()\n\t        return\n", "    @abstractmethod\n\t    def conclude_results(self) -> dict:\n\t        \"\"\"Conclude final profiling outcome based on all previous results.\"\"\"\n\t        return {}\n"]}
