{"filename": "tests/test_llama_index.py", "chunked_list": ["\"\"\"Tests for the Llame Index Package.\"\"\"\n\timport pytest\n\tfrom gpt_review._ask import _load_azure_openai_context\n\tfrom gpt_review._llama_index import _query_index\n\tdef ask_doc_test() -> None:\n\t    _load_azure_openai_context()\n\t    question = \"What is the name of the package?\"\n\t    files = [\"src/gpt_review/__init__.py\"]\n\t    _query_index(question, files, fast=True)\n\t    # Try again to use the cached index\n", "    _query_index(question, files, fast=True)\n\tdef test_ask_doc(mock_openai) -> None:\n\t    \"\"\"Unit Test for the ask_doc function.\"\"\"\n\t    ask_doc_test()\n\t@pytest.mark.integration\n\tdef test_int_ask_doc() -> None:\n\t    \"\"\"Integration Test for the ask_doc function.\"\"\"\n\t    ask_doc_test()\n"]}
{"filename": "tests/test_openai.py", "chunked_list": ["\"\"\"Tests for the Open AI Wrapper.\"\"\"\n\timport pytest\n\tfrom openai.error import RateLimitError\n\timport gpt_review.constants as C\n\tfrom gpt_review._openai import _call_gpt, _get_model\n\tfrom gpt_review.context import _load_azure_openai_context\n\tdef get_model_test() -> None:\n\t    prompt = \"This is a test prompt\"\n\t    context = _load_azure_openai_context()\n\t    model = _get_model(prompt=prompt, max_tokens=1000, fast=True)\n", "    assert model == context.turbo_llm_model_deployment_id\n\t    model = _get_model(prompt=prompt, max_tokens=5000)\n\t    assert model == context.smart_llm_model_deployment_id\n\t    model = _get_model(prompt=prompt, max_tokens=9000)\n\t    assert model == context.large_llm_model_deployment_id\n\tdef test_get_model() -> None:\n\t    get_model_test()\n\t@pytest.mark.integration\n\tdef test_int_get_model() -> None:\n\t    get_model_test()\n", "def rate_limit_test(monkeypatch):\n\t    def mock_get_model(prompt: str, max_tokens: int, fast: bool = False, large: bool = False):\n\t        error = RateLimitError(\"Rate Limit Error\")\n\t        error.headers[\"Retry-After\"] = 10\n\t        raise error\n\t    monkeypatch.setattr(\"gpt_review._openai._get_model\", mock_get_model)\n\t    with pytest.raises(RateLimitError):\n\t        _call_gpt(prompt=\"This is a test prompt\", retry=C.MAX_RETRIES)\n\tdef test_rate_limit(monkeypatch) -> None:\n\t    rate_limit_test(monkeypatch)\n", "@pytest.mark.integration\n\tdef test_int_rate_limit(monkeypatch) -> None:\n\t    rate_limit_test(monkeypatch)\n"]}
{"filename": "tests/test_gpt_cli.py", "chunked_list": ["\"\"\"Pytest for gpt_review/main.py\"\"\"\n\timport os\n\timport subprocess\n\timport sys\n\tfrom dataclasses import dataclass\n\timport pytest\n\timport gpt_review.constants as C\n\tfrom gpt_review._gpt_cli import cli\n\t@dataclass\n\tclass CLICase:\n", "    command: str\n\t    expected_error_message: str = \"\"\n\t    expected_error_code: int = 0\n\t@dataclass\n\tclass CLICase1(CLICase):\n\t    expected_error_code: int = 1\n\t@dataclass\n\tclass CLICase2(CLICase):\n\t    expected_error_code: int = 2\n\tSAMPLE_FILE = \"src/gpt_review/__init__.py\"\n", "QUESTION = \"how are you\"\n\tWHAT_LANGUAGE = \"'what programming language is this code written in?'\"\n\tHELP_TEXT = \"\"\"usage: gpt ask [-h] [--verbose] [--debug] [--only-show-errors]\n\t               [--output {json,jsonc,yaml,yamlc,table,tsv,none}]\n\t               [--query JMESPATH] [--max-tokens MAX_TOKENS]\n\t               [--temperature TEMPERATURE] [--top-p TOP_P]\n\t               [--frequency-penalty FREQUENCY_PENALTY]\n\t               [--presence-penalty PRESENCE_PENALTY]\n\t               <QUESTION> [<QUESTION> ...]\n\t\"\"\"\n", "ROOT_COMMANDS = [\n\t    CLICase(\"--version\"),\n\t    CLICase(\"--help\"),\n\t]\n\tASK_COMMANDS = [\n\t    CLICase(\"ask --help\"),\n\t    CLICase(f\"ask {QUESTION}\"),\n\t    CLICase(f\"ask --fast {QUESTION}\"),\n\t    CLICase(\n\t        f\"ask {QUESTION} --fast --max-tokens {C.MAX_TOKENS_DEFAULT} --temperature {C.TEMPERATURE_DEFAULT} --top-p {C.TOP_P_DEFAULT} --frequency-penalty {C.FREQUENCY_PENALTY_DEFAULT} --presence-penalty {C.FREQUENCY_PENALTY_MAX}\"\n", "    ),\n\t    CLICase1(\n\t        f\"ask {QUESTION} --fast --max-tokens {C.MAX_TOKENS_MIN-1}\",\n\t        f\"ERROR: --max-tokens must be a(n) int between {C.MAX_TOKENS_MIN} and {C.MAX_TOKENS_MAX}\\n\",\n\t    ),\n\t    CLICase1(\n\t        f\"ask {QUESTION} --temperature {C.TEMPERATURE_MAX+8}\",\n\t        f\"ERROR: --temperature must be a(n) float between {C.TEMPERATURE_MIN} and {C.TEMPERATURE_MAX}\\n\",\n\t    ),\n\t    CLICase1(\n", "        f\"ask {QUESTION} --top-p {C.TOP_P_MAX+3.5}\",\n\t        f\"ERROR: --top-p must be a(n) float between {C.TOP_P_MIN} and {C.TOP_P_MAX}\\n\",\n\t    ),\n\t    CLICase1(\n\t        f\"ask {QUESTION} --frequency-penalty {C.FREQUENCY_PENALTY_MAX+2}\",\n\t        f\"ERROR: --frequency-penalty must be a(n) float between {C.FREQUENCY_PENALTY_MIN} and {C.FREQUENCY_PENALTY_MAX}\\n\",\n\t    ),\n\t    CLICase1(\n\t        f\"ask {QUESTION} --presence-penalty {C.PRESENCE_PENALTY_MAX+7.7}\",\n\t        f\"ERROR: --presence-penalty must be a(n) float between {C.PRESENCE_PENALTY_MIN} and {C.PRESENCE_PENALTY_MAX}\\n\",\n", "    ),\n\t    CLICase2(\n\t        f\"ask {QUESTION} --fast --max-tokens\",\n\t        f\"\"\"{HELP_TEXT}\n\tgpt ask: error: argument --max-tokens: expected one argument\n\t\"\"\",\n\t    ),\n\t    CLICase2(\n\t        f\"ask {QUESTION} --fast --max-tokens 'test'\",\n\t        f\"\"\"{HELP_TEXT}\n", "gpt ask: error: argument --max-tokens: invalid int value: \\\"'test'\\\"\n\t\"\"\",\n\t    ),\n\t    CLICase(f\"ask --files {SAMPLE_FILE} --files {SAMPLE_FILE} {WHAT_LANGUAGE} --reset\"),\n\t    CLICase(f\"ask --fast -f {SAMPLE_FILE} {WHAT_LANGUAGE}\"),\n\t    CLICase(f\"ask --fast -d src/gpt_review --reset --recursive --hidden --required-exts .py {WHAT_LANGUAGE}\"),\n\t    CLICase(f\"ask --fast -repo microsoft/gpt-review --branch main {WHAT_LANGUAGE}\"),\n\t]\n\tGITHUB_COMMANDS = [\n\t    CLICase(\"github review --help\"),\n", "    CLICase(\"github review\"),\n\t]\n\tGIT_COMMANDS = [\n\t    CLICase(\"git commit --help\"),\n\t    # CLICase(\"git commit\"),\n\t    # CLICase(\"git commit --large\"),\n\t    # CLICase(\"git commit --gpt4\"),\n\t    # CLICase(\"git commit --push\"),\n\t]\n\tREVIEW_COMMANDS = [\n", "    CLICase(\"review --help\"),\n\t    CLICase(\"review diff --help\"),\n\t    CLICase(\"review diff --diff tests/mock.diff --config tests/config.summary.test.yml\"),\n\t    CLICase(\"review diff --diff tests/mock.diff --config tests/config.summary.extra.yml\"),\n\t]\n\tARGS = ROOT_COMMANDS + ASK_COMMANDS + GIT_COMMANDS + GITHUB_COMMANDS + REVIEW_COMMANDS\n\tARGS_DICT = {arg.command: arg for arg in ARGS}\n\tMODULE_COMMANDS = [\n\t    CLICase(\"python -m gpt --version\"),\n\t    CLICase(\"python -m gpt_review --version\"),\n", "]\n\tMODULE_DICT = {arg.command: arg for arg in MODULE_COMMANDS}\n\tdef gpt_cli_test(command: CLICase) -> None:\n\t    os.environ[\"GPT_ASK_COMMANDS\"] = \"1\"\n\t    sys.argv[1:] = command.command.split(\" \")\n\t    exit_code = -1\n\t    try:\n\t        exit_code = cli()\n\t    except SystemExit as e:\n\t        exit_code = e.code\n", "    finally:\n\t        assert exit_code == command.expected_error_code\n\tdef cli_test(command, command_array) -> None:\n\t    result = subprocess.run(\n\t        command_array,\n\t        stdout=subprocess.PIPE,\n\t        stderr=subprocess.PIPE,\n\t        check=False,\n\t    )\n\t    assert result.returncode == command.expected_error_code\n", "@pytest.mark.parametrize(\"command\", ARGS_DICT.keys())\n\t@pytest.mark.cli\n\tdef test_cli_gpt_cli(command: str) -> None:\n\t    \"\"\"Test gpt commands from installed CLI\"\"\"\n\t    command_array = f\"gpt {ARGS_DICT[command].command}\".split(\" \")\n\t    cli_test(ARGS_DICT[command], command_array)\n\t@pytest.mark.parametrize(\"command\", MODULE_DICT.keys())\n\t@pytest.mark.cli\n\tdef test_cli_gpt_module(command: str) -> None:\n\t    \"\"\"Test running cli as module\"\"\"\n", "    command_array = MODULE_DICT[command].command.split(\" \")\n\t    cli_test(MODULE_DICT[command], command_array)\n\t@pytest.mark.parametrize(\"command\", ARGS_DICT.keys())\n\tdef test_gpt_cli(command: str, mock_openai: None) -> None:\n\t    gpt_cli_test(ARGS_DICT[command])\n\t@pytest.mark.parametrize(\"command\", ARGS_DICT.keys())\n\t@pytest.mark.integration\n\tdef test_int_gpt_cli(command: str) -> None:\n\t    \"\"\"Test gpt commands from CLI file\"\"\"\n\t    gpt_cli_test(ARGS_DICT[command])\n"]}
{"filename": "tests/test_context.py", "chunked_list": ["\"\"\"Tests for the context grabber.\"\"\"\n\tfrom gpt_review.context import _load_context_file\n\tdef test_load_context_file(monkeypatch) -> None:\n\t    monkeypatch.setenv(\"CONTEXT_FILE\", \"azure.yaml.template\")\n\t    yaml_context = _load_context_file()\n\t    assert \"azure_api_type\" in yaml_context\n\t    assert \"azure_api_base\" in yaml_context\n\t    assert \"azure_api_version\" in yaml_context\n"]}
{"filename": "tests/test_git.py", "chunked_list": ["\"\"\"Test git functions.\"\"\"\n\timport pytest\n\tfrom gpt_review._git import _commit, _find_git_dir\n\tdef commit_test() -> None:\n\t    \"\"\"Test Case for commit wrapper.\"\"\"\n\t    message = _commit()\n\t    assert message\n\t    message = _commit(push=True)\n\t    assert message\n\tdef test_commit(mock_openai: None, mock_git_commit: None) -> None:\n", "    \"\"\"Unit test for commit function.\"\"\"\n\t    commit_test()\n\t@pytest.mark.integration\n\tdef test_int_commit(mock_git_commit: None) -> None:\n\t    \"\"\"Integration test for commit function.\"\"\"\n\t    commit_test()\n\t@pytest.mark.unit\n\t@pytest.mark.integration\n\tdef test_find_git_dir() -> None:\n\t    _find_git_dir(path=\"tests\")\n", "    with pytest.raises(FileNotFoundError):\n\t        _find_git_dir(path=\"/\")\n"]}
{"filename": "tests/test_review.py", "chunked_list": ["import pytest\n\tfrom gpt_review.repositories.github import GitHubClient\n\tdef test_get_review(mock_openai) -> None:\n\t    get_review_test()\n\t@pytest.mark.integration\n\tdef test_int_get_review() -> None:\n\t    get_review_test()\n\tdef get_review_test() -> None:\n\t    \"\"\"Test get_review.\"\"\"\n\t    # Load test data from moock.diff\n", "    with open(\"tests/mock.diff\", \"r\") as f:\n\t        diff = f.read()\n\t        GitHubClient.post_pr_summary(diff)\n\tdef test_empty_summary(empty_summary, mock_openai) -> None:\n\t    get_review_test()\n\t@pytest.mark.integration\n\tdef test_int_empty_summary(empty_summary) -> None:\n\t    get_review_test()\n\tdef test_file_summary(mock_openai, file_summary) -> None:\n\t    get_review_test()\n", "@pytest.mark.integration\n\tdef test_int_file_summary(mock_openai, file_summary) -> None:\n\t    get_review_test()\n"]}
{"filename": "tests/test_github.py", "chunked_list": ["import pytest\n\tfrom gpt_review.repositories.github import GitHubClient\n\tdef get_pr_diff_test(starts_with, patch_repo=None, patch_pr=None) -> None:\n\t    \"\"\"Test the GitHub API call.\"\"\"\n\t    diff = GitHubClient.get_pr_diff(patch_repo=patch_repo, patch_pr=patch_pr)\n\t    assert diff.startswith(starts_with)\n\tdef post_pr_comment_test() -> None:\n\t    \"\"\"Test the GitHub API call.\"\"\"\n\t    response = GitHubClient.post_pr_summary(\"test\")\n\t    assert response\n", "@pytest.mark.integration\n\tdef test_int_pr_diff(mock_github) -> None:\n\t    \"\"\"Integration Test for GitHub API diff call.\"\"\"\n\t    get_pr_diff_test(\"diff --git a/README.md b/README.md\", \"microsoft/gpt-review\", 1)\n\tdef test_pr_diff(mock_openai, mock_github) -> None:\n\t    \"\"\"Unit Test for GitHub API diff call.\"\"\"\n\t    get_pr_diff_test(\"diff --git a/README.md b/README.md\")\n\t@pytest.mark.integration\n\tdef test_int_pr_comment(mock_github) -> None:\n\t    \"\"\"Integration Test for GitHub API comment call.\"\"\"\n", "    post_pr_comment_test()\n\t@pytest.mark.integration\n\tdef test_int_pr_update(mock_github, mock_github_comment) -> None:\n\t    \"\"\"Integration Test for updating GitHub API comment call.\"\"\"\n\t    post_pr_comment_test()\n\tdef test_pr_comment(mock_openai, mock_github) -> None:\n\t    \"\"\"Unit Test for GitHub API comment call.\"\"\"\n\t    post_pr_comment_test()\n\tdef test_pr_update(mock_openai, mock_github, mock_github_comment) -> None:\n\t    \"\"\"Unit Test for updating GitHub API comment call.\"\"\"\n", "    post_pr_comment_test()\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["from collections import namedtuple\n\timport pytest\n\timport yaml\n\tfrom llama_index import SimpleDirectoryReader\n\tdef pytest_collection_modifyitems(items):\n\t    for item in items:\n\t        if \"_int_\" in item.nodeid:\n\t            item.add_marker(pytest.mark.integration)\n\t        elif \"_cli_\" in item.nodeid:\n\t            item.add_marker(pytest.mark.cli)\n", "        else:\n\t            item.add_marker(pytest.mark.unit)\n\t@pytest.fixture\n\tdef mock_openai(monkeypatch) -> None:\n\t    \"\"\"\n\t    Mock OpenAI Functions with monkeypatch\n\t    - aopenai.ChatCompletion.create\n\t    \"\"\"\n\t    monkeypatch.setenv(\"OPENAI_API_KEY\", \"MOCK\")\n\t    monkeypatch.setenv(\"AZURE_OPENAI_API\", \"MOCK\")\n", "    monkeypatch.setenv(\"AZURE_OPENAI_API_KEY\", \"MOCK\")\n\t    monkeypatch.setenv(\"FILE_SUMMARY_FULL\", \"true\")\n\t    class MockResponse:\n\t        def __init__(self) -> None:\n\t            self.choices = [namedtuple(\"mockMessage\", \"message\")(*[namedtuple(\"mockContent\", \"content\")(*[[\"test\"]])])]\n\t    class MockQueryResponse:\n\t        def __init__(self) -> None:\n\t            self.response = \"test\"\n\t    class MockStorageContext:\n\t        def persist(self, persist_dir) -> None:\n", "            pass\n\t    class MockIndex:\n\t        def __init__(self) -> None:\n\t            self.storage_context = MockStorageContext()\n\t        def query(self, question: str) -> MockQueryResponse:\n\t            assert isinstance(question, str)\n\t            return MockQueryResponse()\n\t        def as_query_engine(self):\n\t            return self\n\t    def mock_create(\n", "        model=None,\n\t        deployment_id=None,\n\t        messages=None,\n\t        temperature=0,\n\t        max_tokens=500,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0,\n\t    ) -> MockResponse:\n\t        return MockResponse()\n", "    def from_documents(documents, service_context=None) -> MockIndex:\n\t        return MockIndex()\n\t    def init_mock_reader(self, owner, repo, use_parser) -> None:\n\t        pass\n\t    def mock_load_data_from_branch(self, branch):\n\t        return SimpleDirectoryReader(input_dir=\".\").load_data()\n\t    monkeypatch.setattr(\"openai.ChatCompletion.create\", mock_create)\n\t    monkeypatch.setattr(\"llama_index.GPTVectorStoreIndex.from_documents\", from_documents)\n\t    monkeypatch.setattr(\"llama_index.GithubRepositoryReader.__init__\", init_mock_reader)\n\t    monkeypatch.setattr(\"llama_index.GithubRepositoryReader._load_data_from_branch\", mock_load_data_from_branch)\n", "    def mock_query(self, question) -> MockQueryResponse:\n\t        return MockQueryResponse()\n\t    monkeypatch.setattr(\"llama_index.indices.query.base.BaseQueryEngine.query\", mock_query)\n\t@pytest.fixture\n\tdef mock_github(monkeypatch) -> None:\n\t    \"\"\"\n\t    Mock GitHub Functions with monkeypatch\n\t    - requests.get\n\t    \"\"\"\n\t    monkeypatch.setenv(\"LINK\", \"https://github.com/microsoft/gpt-review/pull/1\")\n", "    monkeypatch.setenv(\"GIT_COMMIT_HASH\", \"a9da0c1e65f1102bc2ae16abed7b6a66400a5bde\")\n\t    class MockResponse:\n\t        def __init__(self) -> None:\n\t            self.text = \"diff --git a/README.md b/README.md\"\n\t        def json(self) -> dict:\n\t            return {\"test\": \"test\"}\n\t    def mock_get(url, headers, timeout) -> MockResponse:\n\t        return MockResponse()\n\t    def mock_put(url, headers, data, timeout) -> MockResponse:\n\t        return MockResponse()\n", "    def mock_post(url, headers, data, timeout) -> MockResponse:\n\t        return MockResponse()\n\t    monkeypatch.setattr(\"requests.get\", mock_get)\n\t    monkeypatch.setattr(\"requests.put\", mock_put)\n\t    monkeypatch.setattr(\"requests.post\", mock_post)\n\t@pytest.fixture\n\tdef mock_github_comment(monkeypatch) -> None:\n\t    class MockCommentResponse:\n\t        def json(self) -> list:\n\t            return [\n", "                {\n\t                    \"user\": {\"login\": \"github-actions[bot]\"},\n\t                    \"body\": \"Summary by GPT-4\",\n\t                    \"id\": 1,\n\t                }\n\t            ]\n\t    def mock_get(url, headers, timeout) -> MockCommentResponse:\n\t        return MockCommentResponse()\n\t    monkeypatch.setattr(\"requests.get\", mock_get)\n\t@pytest.fixture\n", "def mock_git_commit(monkeypatch) -> None:\n\t    \"\"\"Mock git.commit with pytest monkey patch\"\"\"\n\t    class MockGit:\n\t        def __init__(self) -> None:\n\t            self.git = self\n\t        def commit(self, message, push: bool = False) -> str:\n\t            return \"test commit response\"\n\t        def diff(self, message, cached) -> str:\n\t            return \"test diff response\"\n\t        def push(self) -> str:\n", "            return \"test push response\"\n\t    def mock_init(cls) -> MockGit:\n\t        return MockGit()\n\t    monkeypatch.setattr(\"git.repo.Repo.init\", mock_init)\n\t@pytest.fixture\n\tdef report_config():\n\t    \"\"\"Load sample.report.yaml file\"\"\"\n\t    return load_report_config(\"config.summary.template.yml\")\n\tdef load_report_config(file_name):\n\t    with open(file_name, \"r\") as yaml_file:\n", "        config = yaml.safe_load(yaml_file)\n\t        return config[\"report\"]\n\t@pytest.fixture\n\tdef config_yaml():\n\t    return \"tests/config.summary.test.yml\"\n\t@pytest.fixture\n\tdef git_diff() -> str:\n\t    \"\"\"Load test.diff file\"\"\"\n\t    with open(\"tests/mock.diff\", \"r\") as diff_file:\n\t        diff = diff_file.read()\n", "    return diff\n\t@pytest.fixture\n\tdef empty_summary(monkeypatch) -> None:\n\t    \"\"\"Test empty summary.\"\"\"\n\t    monkeypatch.setenv(\"FILE_SUMMARY\", \"false\")\n\t    monkeypatch.setenv(\"TEST_SUMMARY\", \"false\")\n\t    monkeypatch.setenv(\"BUG_SUMMARY\", \"false\")\n\t    monkeypatch.setenv(\"RISK_SUMMARY\", \"false\")\n\t    monkeypatch.setenv(\"FULL_SUMMARY\", \"false\")\n\t@pytest.fixture\n", "def file_summary(monkeypatch) -> None:\n\t    \"\"\"Test empty summary.\"\"\"\n\t    monkeypatch.setenv(\"FILE_SUMMARY_FULL\", \"false\")\n"]}
{"filename": "tests/test_report.py", "chunked_list": ["\"\"\"Tests for customized markdown report generation.\"\"\"\n\timport pytest\n\tfrom gpt_review._review import _process_report, _process_yaml\n\tdef test_report_generation(git_diff, report_config, mock_openai) -> None:\n\t    \"\"\"Test report generation with mocks.\"\"\"\n\t    report_generation_test(git_diff, report_config)\n\t@pytest.mark.integration\n\tdef test_int_report_generation(git_diff, report_config) -> None:\n\t    \"\"\"Test report generation.\"\"\"\n\t    report_generation_test(git_diff, report_config)\n", "def test_process_yaml(git_diff, config_yaml, mock_openai) -> None:\n\t    process_yaml_test(git_diff, config_yaml)\n\t@pytest.mark.integration\n\tdef test_int_process_yaml(git_diff, config_yaml) -> None:\n\t    process_yaml_test(git_diff, config_yaml)\n\tdef report_generation_test(git_diff, report_config) -> None:\n\t    report = _process_report(git_diff, report_config)\n\t    assert report\n\tdef process_yaml_test(git_diff, config_yaml) -> None:\n\t    \"\"\"Test process_yaml.\"\"\"\n", "    report = _process_yaml(git_diff, config_yaml)\n\t    assert report\n"]}
{"filename": "src/gpt_review/_command.py", "chunked_list": ["\"\"\"Interface for GPT CLI command groups.\"\"\"\n\tfrom knack import CLICommandsLoader\n\tclass GPTCommandGroup:\n\t    \"\"\"Command Group Interface.\"\"\"\n\t    @staticmethod\n\t    def load_command_table(loader: CLICommandsLoader) -> None:\n\t        \"\"\"Load the command table.\"\"\"\n\t    @staticmethod\n\t    def load_arguments(loader: CLICommandsLoader) -> None:\n\t        \"\"\"Load the arguments for the command group.\"\"\"\n"]}
{"filename": "src/gpt_review/_openai.py", "chunked_list": ["\"\"\"Open AI API Call Wrapper.\"\"\"\n\timport logging\n\timport os\n\timport openai\n\tfrom openai.error import RateLimitError\n\timport gpt_review.constants as C\n\tfrom gpt_review.context import _load_azure_openai_context\n\tfrom gpt_review.utils import _retry_with_exponential_backoff\n\tdef _count_tokens(prompt) -> int:\n\t    \"\"\"\n", "    Determine number of tokens in prompt.\n\t    Args:\n\t        prompt (str): The prompt to send to GPT-4.\n\t    Returns:\n\t        int: The number of tokens in the prompt.\n\t    \"\"\"\n\t    return int(len(prompt) / 4 * 3)\n\tdef _get_model(prompt: str, max_tokens: int, fast: bool = False, large: bool = False) -> str:\n\t    \"\"\"\n\t    Get the OpenAI model based on the prompt length.\n", "    - when greater then 8k use gpt-4-32k\n\t    - otherwise use gpt-4\n\t    - enable fast to use gpt-35-turbo for small prompts\n\t    Args:\n\t        prompt (str): The prompt to send to GPT-4.\n\t        max_tokens (int): The maximum number of tokens to generate.\n\t        fast (bool, optional): Whether to use the fast model. Defaults to False.\n\t        large (bool, optional): Whether to use the large model. Defaults to False.\n\t    Returns:\n\t        str: The model to use.\n", "    \"\"\"\n\t    context = _load_azure_openai_context()\n\t    tokens = _count_tokens(prompt)\n\t    if large or tokens + max_tokens > 8000:\n\t        return context.large_llm_model_deployment_id\n\t    if tokens + max_tokens > 4000:\n\t        return context.smart_llm_model_deployment_id\n\t    return context.turbo_llm_model_deployment_id if fast else context.smart_llm_model_deployment_id\n\tdef _call_gpt(\n\t    prompt: str,\n", "    temperature=0.10,\n\t    max_tokens=500,\n\t    top_p=1.0,\n\t    frequency_penalty=0.5,\n\t    presence_penalty=0.0,\n\t    retry=0,\n\t    messages=None,\n\t    fast: bool = False,\n\t    large: bool = False,\n\t) -> str:\n", "    \"\"\"\n\t    Call GPT with the given prompt.\n\t    Args:\n\t        prompt (str): The prompt to send to GPT-4.\n\t        temperature (float, optional): The temperature to use. Defaults to 0.10.\n\t        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 500.\n\t        top_p (float, optional): The top_p to use. Defaults to 1.\n\t        frequency_penalty (float, optional): The frequency penalty to use. Defaults to 0.5.\n\t        presence_penalty (float, optional): The presence penalty to use. Defaults to 0.0.\n\t        retry (int, optional): The number of times to retry the request. Defaults to 0.\n", "        messages (List[Dict[str, str]], optional): The messages to send to GPT-4. Defaults to None.\n\t        fast (bool, optional): Whether to use the fast model. Defaults to False.\n\t        large (bool, optional): Whether to use the large model. Defaults to False.\n\t    Returns:\n\t        str: The response from GPT.\n\t    \"\"\"\n\t    messages = messages or [{\"role\": \"user\", \"content\": prompt}]\n\t    logging.debug(\"Prompt sent to GPT: %s\\n\", prompt)\n\t    try:\n\t        model = _get_model(prompt, max_tokens=max_tokens, fast=fast, large=large)\n", "        logging.debug(\"Model Selected based on prompt size: %s\", model)\n\t        if os.environ.get(\"OPENAI_API_TYPE\", \"\") == C.AZURE_API_TYPE:\n\t            logging.debug(\"Using Azure Open AI.\")\n\t            completion = openai.ChatCompletion.create(\n\t                deployment_id=model,\n\t                messages=messages,\n\t                max_tokens=max_tokens,\n\t                temperature=temperature,\n\t                top_p=top_p,\n\t                frequency_penalty=frequency_penalty,\n", "                presence_penalty=presence_penalty,\n\t            )\n\t        else:\n\t            logging.debug(\"Using Open AI.\")\n\t            completion = openai.ChatCompletion.create(\n\t                model=model,\n\t                messages=messages,\n\t                max_tokens=max_tokens,\n\t                temperature=temperature,\n\t                top_p=top_p,\n", "                frequency_penalty=frequency_penalty,\n\t                presence_penalty=presence_penalty,\n\t            )\n\t        return completion.choices[0].message.content  # type: ignore\n\t    except RateLimitError as error:\n\t        if retry < C.MAX_RETRIES:\n\t            retry_after = error.headers.get(\"Retry-After\", C.DEFAULT_RETRY_AFTER)\n\t            _retry_with_exponential_backoff(retry, retry_after)\n\t            return _call_gpt(prompt, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, retry + 1)\n\t        raise RateLimitError(\"Retry limit exceeded\") from error\n"]}
{"filename": "src/gpt_review/main.py", "chunked_list": ["\"\"\"The GPT CLI entry point.\"\"\"\n\timport sys\n\tfrom knack.help_files import helps\n\tfrom gpt_review._gpt_cli import cli\n\tdef _help_text(help_type, short_summary) -> str:\n\t    return f\"\"\"\n\ttype: {help_type}\n\tshort-summary: {short_summary}\n\t\"\"\"\n\thelps[\"\"] = _help_text(\"group\", \"Easily interact with GPT APIs.\")\n", "helps[\"ask\"] = _help_text(\"group\", \"Use GPT to ask questions.\")\n\thelps[\"git\"] = _help_text(\"group\", \"Use GPT enchanced git commands.\")\n\thelps[\"git commit\"] = _help_text(\"command\", \"Run git commit with a commit message generated by GPT.\")\n\thelps[\"github\"] = _help_text(\"group\", \"Use GPT with GitHub Repositories.\")\n\thelps[\"github review\"] = _help_text(\"command\", \"Review GitHub PR with Open AI, and post response as a comment.\")\n\thelps[\"review\"] = _help_text(\"group\", \"Use GPT to perform customized reviews.\")\n\thelps[\"review diff\"] = _help_text(\"command\", \"Review a git diff from file.\")\n\texit_code = cli()\n\tsys.exit(exit_code)\n"]}
{"filename": "src/gpt_review/_ask.py", "chunked_list": ["\"\"\"Ask GPT a question.\"\"\"\n\timport logging\n\tfrom typing import Dict, List, Optional\n\tfrom knack import CLICommandsLoader\n\tfrom knack.arguments import ArgumentsContext\n\tfrom knack.commands import CommandGroup\n\tfrom knack.util import CLIError\n\timport gpt_review.constants as C\n\tfrom gpt_review._command import GPTCommandGroup\n\tfrom gpt_review._llama_index import _query_index\n", "from gpt_review._openai import _call_gpt\n\tfrom gpt_review.context import _load_azure_openai_context\n\tdef validate_parameter_range(namespace) -> None:\n\t    \"\"\"\n\t    Validate the following parameters:\n\t    - max_tokens is in [1,4000]\n\t    - temperature is in [0,1]\n\t    - top_p is in [0,1]\n\t    - frequency_penalty is in [0,2]\n\t    - presence_penalty is in [0,2]\n", "    Args:\n\t        namespace (argparse.Namespace): The namespace to validate.\n\t    Raises:\n\t        CLIError: If the parameter is not within the allowed range.\n\t    \"\"\"\n\t    _range_validation(namespace.max_tokens, \"max-tokens\", C.MAX_TOKENS_MIN, C.MAX_TOKENS_MAX)\n\t    _range_validation(namespace.temperature, \"temperature\", C.TEMPERATURE_MIN, C.TEMPERATURE_MAX)\n\t    _range_validation(namespace.top_p, \"top-p\", C.TOP_P_MIN, C.TOP_P_MAX)\n\t    _range_validation(\n\t        namespace.frequency_penalty, \"frequency-penalty\", C.FREQUENCY_PENALTY_MIN, C.FREQUENCY_PENALTY_MAX\n", "    )\n\t    _range_validation(namespace.presence_penalty, \"presence-penalty\", C.PRESENCE_PENALTY_MIN, C.PRESENCE_PENALTY_MAX)\n\tdef _range_validation(param, name, min_value, max_value) -> None:\n\t    \"\"\"Validates that the given parameter is within the allowed range\n\t    Args:\n\t        param (int or float): The parameter value to validate.\n\t        name (str): The name of the parameter.\n\t        min_value (int or float): The minimum allowed value for the parameter.\n\t        max_value (int or float): The maximum allowed value for the parameter.\n\t    Raises:\n", "        CLIError: If the parameter is not within the allowed range.\n\t    \"\"\"\n\t    if param is not None and (param < min_value or param > max_value):\n\t        raise CLIError(f\"--{name} must be a(n) {type(param).__name__} between {min_value} and {max_value}\")\n\tdef _ask(\n\t    question: List[str],\n\t    max_tokens: int = C.MAX_TOKENS_DEFAULT,\n\t    temperature: float = C.TEMPERATURE_DEFAULT,\n\t    top_p: float = C.TOP_P_DEFAULT,\n\t    frequency_penalty: float = C.FREQUENCY_PENALTY_DEFAULT,\n", "    presence_penalty: float = C.PRESENCE_PENALTY_DEFAULT,\n\t    files: Optional[List[str]] = None,\n\t    messages=None,\n\t    fast: bool = False,\n\t    large: bool = False,\n\t    directory: Optional[str] = None,\n\t    reset: bool = False,\n\t    required_exts: Optional[List[str]] = None,\n\t    hidden: bool = False,\n\t    recursive: bool = False,\n", "    repository: Optional[str] = None,\n\t    branch: str = \"main\",\n\t) -> Dict[str, str]:\n\t    \"\"\"\n\t    Ask GPT a question.\n\t    Args:\n\t        question (List[str]): The question to ask GPT.\n\t        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to C.MAX_TOKENS_DEFAULT.\n\t        temperature (float, optional): Controls randomness. Defaults to C.TEMPERATURE_DEFAULT.\n\t        top_p (float, optional): Controls diversity via nucleus sampling. Defaults to C.TOP_P_DEFAULT.\n", "        frequency_penalty (float, optional): How much to penalize new tokens based on their existing frequency in the\n\t            text so far. Defaults to C.FREQUENCY_PENALTY_DEFAULT.\n\t        presence_penalty (float, optional): How much to penalize new tokens based on whether they appear in the text so\n\t            far. Defaults to C.PRESENCE_PENALTY_DEFAULT.\n\t        files (Optional[List[str]], optional): The files to search. Defaults to None.\n\t        messages ([type], optional): [description]. Defaults to None.\n\t        fast (bool, optional): Use the fast model. Defaults to False.\n\t        large (bool, optional): Use the large model. Defaults to False.\n\t        directory (Optional[str], optional): The directory to search. Defaults to None.\n\t        reset (bool, optional): Whether to reset the index. Defaults to False.\n", "        required_exts (Optional[List[str]], optional): The required file extensions. Defaults to None.\n\t        hidden (bool, optional): Include hidden files. Defaults to False.\n\t        recursive (bool, optional): Recursively search the directory. Defaults to False.\n\t        repository (Optional[str], optional): The repository to search. Defaults to None.\n\t    Returns:\n\t            Dict[str, str]: The response from GPT.\n\t    \"\"\"\n\t    _load_azure_openai_context()\n\t    prompt = \"\".join(question)\n\t    if files or directory or repository:\n", "        response = _query_index(\n\t            prompt,\n\t            files,\n\t            input_dir=directory,\n\t            reset=reset,\n\t            exclude_hidden=not hidden,\n\t            recursive=recursive,\n\t            required_exts=required_exts,\n\t            repository=repository,\n\t            branch=branch,\n", "            fast=fast,\n\t            large=large,\n\t        )\n\t    else:\n\t        response = _call_gpt(\n\t            prompt=prompt,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            frequency_penalty=frequency_penalty,\n", "            presence_penalty=presence_penalty,\n\t            fast=fast,\n\t            large=large,\n\t            messages=messages,\n\t        )\n\t    logging.info(response)\n\t    return {\"response\": response}\n\tclass AskCommandGroup(GPTCommandGroup):\n\t    \"\"\"Ask Command Group.\"\"\"\n\t    @staticmethod\n", "    def load_command_table(loader: CLICommandsLoader) -> None:\n\t        with CommandGroup(loader, \"\", \"gpt_review._ask#{}\") as group:\n\t            group.command(\"ask\", \"_ask\", is_preview=True)\n\t    @staticmethod\n\t    def load_arguments(loader: CLICommandsLoader) -> None:\n\t        with ArgumentsContext(loader, \"ask\") as args:\n\t            args.positional(\"question\", type=str, nargs=\"+\", help=\"Provide a question to ask GPT.\")\n\t            args.argument(\n\t                \"fast\",\n\t                help=\"Use gpt-35-turbo for prompts < 4000 tokens.\",\n", "                default=False,\n\t                action=\"store_true\",\n\t            )\n\t            args.argument(\n\t                \"large\",\n\t                help=\"Use gpt-4-32k for prompts.\",\n\t                default=False,\n\t                action=\"store_true\",\n\t            )\n\t            args.argument(\n", "                \"temperature\",\n\t                type=float,\n\t                help=\"Sets the level of creativity/randomness.\",\n\t                validator=validate_parameter_range,\n\t            )\n\t            args.argument(\n\t                \"max_tokens\",\n\t                type=int,\n\t                help=\"The maximum number of tokens to generate.\",\n\t                validator=validate_parameter_range,\n", "            )\n\t            args.argument(\n\t                \"top_p\",\n\t                type=float,\n\t                help=\"Also sets the level of creativity/randomness. Adjust temperature or top p but not both.\",\n\t                validator=validate_parameter_range,\n\t            )\n\t            args.argument(\n\t                \"frequency_penalty\",\n\t                type=float,\n", "                help=\"Reduce the chance of repeating a token based on current frequency in the text.\",\n\t                validator=validate_parameter_range,\n\t            )\n\t            args.argument(\n\t                \"presence_penalty\",\n\t                type=float,\n\t                help=\"Reduce the chance of repeating any token that has appeared in the text so far.\",\n\t                validator=validate_parameter_range,\n\t            )\n\t            args.argument(\n", "                \"files\",\n\t                type=str,\n\t                help=\"Ask question about a file. Can be used multiple times.\",\n\t                default=None,\n\t                action=\"append\",\n\t                options_list=(\"--files\", \"-f\"),\n\t            )\n\t            args.argument(\n\t                \"directory\",\n\t                type=str,\n", "                help=\"Path of the directory to index. Use --recursive (or -r) to index subdirectories.\",\n\t                default=None,\n\t                options_list=(\"--directory\", \"-d\"),\n\t            )\n\t            args.argument(\n\t                \"required_exts\",\n\t                type=str,\n\t                help=\"Required extensions when indexing a directory. Requires --directory. Can be used multiple times.\",\n\t                default=None,\n\t                action=\"append\",\n", "            )\n\t            args.argument(\n\t                \"hidden\",\n\t                help=\"Include hidden files when indexing a directory. Requires --directory.\",\n\t                default=False,\n\t                action=\"store_true\",\n\t            )\n\t            args.argument(\n\t                \"recursive\",\n\t                help=\"Recursively index a directory. Requires --directory.\",\n", "                default=False,\n\t                action=\"store_true\",\n\t                options_list=(\"--recursive\", \"-r\"),\n\t            )\n\t            args.argument(\n\t                \"repository\",\n\t                type=str,\n\t                help=\"Repository to index. Default: None.\",\n\t                default=None,\n\t                options_list=(\"--repository\", \"-repo\"),\n", "            )\n\t            args.argument(\n\t                \"branch\",\n\t                type=str,\n\t                help=\"Branch to index. Default: main.\",\n\t                default=\"main\",\n\t                options_list=(\"--branch\", \"-b\"),\n\t            )\n\t            args.argument(\n\t                \"reset\",\n", "                help=\"Reset the index, overwriting the directory. Requires --directory, --files, or --repository.\",\n\t                default=False,\n\t                action=\"store_true\",\n\t            )\n"]}
{"filename": "src/gpt_review/__main__.py", "chunked_list": ["\"\"\"The GPT CLI entry point for python -m gpt\"\"\"\n\timport sys\n\tfrom gpt_review._gpt_cli import cli\n\tif __name__ == \"__main__\":\n\t    exit_code = cli()\n\t    sys.exit(exit_code)\n"]}
{"filename": "src/gpt_review/context.py", "chunked_list": ["\"\"\"Context for the Azure OpenAI API and the models.\"\"\"\n\timport os\n\tfrom dataclasses import dataclass\n\timport openai\n\timport yaml\n\tfrom azure.identity import DefaultAzureCredential\n\tfrom azure.keyvault.secrets import SecretClient\n\timport gpt_review.constants as C\n\t@dataclass\n\tclass Context:\n", "    azure_api_base: str\n\t    azure_api_type: str = C.AZURE_API_TYPE\n\t    azure_api_version: str = C.AZURE_API_VERSION\n\t    turbo_llm_model_deployment_id: str = C.AZURE_TURBO_MODEL\n\t    smart_llm_model_deployment_id: str = C.AZURE_SMART_MODEL\n\t    large_llm_model_deployment_id: str = C.AZURE_LARGE_MODEL\n\t    embedding_model_deployment_id: str = C.AZURE_EMBEDDING_MODEL\n\tdef _load_context_file():\n\t    \"\"\"Import from yaml file and return the context.\"\"\"\n\t    context_file = os.getenv(\"CONTEXT_FILE\", C.AZURE_CONFIG_FILE)\n", "    with open(context_file, \"r\", encoding=\"utf8\") as file:\n\t        return yaml.load(file, Loader=yaml.SafeLoader)\n\tdef _load_azure_openai_context() -> Context:\n\t    \"\"\"Load the context from the environment variables or the context file.\n\t    If a config file is available its values will take precedence. Otherwise\n\t    it will first check for an AZURE_OPENAI_API key, next OPENAI_API_KEY, and\n\t    lastly the Azure Key Vault.\n\t    Returns:\n\t        Context: The context for the Azure OpenAI API and the models.\n\t    \"\"\"\n", "    azure_config = _load_context_file() if os.path.exists(os.getenv(\"CONTEXT_FILE\", C.AZURE_CONFIG_FILE)) else {}\n\t    if azure_config.get(\"azure_api_type\"):\n\t        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = azure_config.get(\"azure_api_type\")\n\t    elif os.getenv(\"AZURE_OPENAI_API\"):\n\t        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n\t    elif \"OPENAI_API_TYPE\" in os.environ:\n\t        openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n\t    if azure_config.get(\"azure_api_version\"):\n\t        openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = azure_config.get(\"azure_api_version\")\n\t    elif os.getenv(\"AZURE_OPENAI_API\"):\n", "        openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = C.AZURE_API_VERSION\n\t    elif \"OPENAI_API_VERSION\" in os.environ:\n\t        openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n\t    if os.getenv(\"AZURE_OPENAI_API\"):\n\t        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n\t        openai.api_base = os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_API\") or azure_config.get(\n\t            \"azure_api_base\"\n\t        )\n\t        openai.api_key = os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")  # type: ignore\n\t    elif os.getenv(\"OPENAI_API_KEY\"):\n", "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\t    else:\n\t        kv_client = SecretClient(\n\t            vault_url=os.getenv(\"AZURE_KEY_VAULT_URL\", C.AZURE_KEY_VAULT),\n\t            credential=DefaultAzureCredential(additionally_allowed_tenants=[\"*\"]),\n\t        )\n\t        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n\t        openai.api_base = os.environ[\"OPENAI_API_BASE\"] = kv_client.get_secret(\"azure-open-ai\").value  # type: ignore\n\t        openai.api_key = os.environ[\"OPENAI_API_KEY\"] = kv_client.get_secret(\"azure-openai-key\").value  # type: ignore\n\t    return Context(\n", "        azure_api_base=openai.api_base,\n\t        azure_api_type=openai.api_type,\n\t        azure_api_version=openai.api_version,\n\t        **azure_config.get(\"azure_model_map\", {}),\n\t    )\n"]}
{"filename": "src/gpt_review/_git.py", "chunked_list": ["\"\"\"Basic Shell Commands for Git.\"\"\"\n\timport logging\n\timport os\n\tfrom typing import Dict\n\tfrom git.repo import Repo\n\tfrom knack import CLICommandsLoader\n\tfrom knack.arguments import ArgumentsContext\n\tfrom knack.commands import CommandGroup\n\tfrom gpt_review._command import GPTCommandGroup\n\tfrom gpt_review._review import _request_goal\n", "def _find_git_dir(path=\".\") -> str:\n\t    \"\"\"\n\t    Find the .git directory.\n\t    Args:\n\t        path (str): The path to start searching from.\n\t    Returns:\n\t        path (str): The path to the .git directory.\n\t    \"\"\"\n\t    while path != \"/\":\n\t        if os.path.exists(os.path.join(path, \".git\")):\n", "            return path\n\t        path = os.path.abspath(os.path.join(path, os.pardir))\n\t    raise FileNotFoundError(\".git directory not found\")\n\tdef _diff() -> str:\n\t    \"\"\"\n\t    Get the diff of the PR\n\t    - run git commands via python\n\t    Returns:\n\t        diff (str): The diff of the PR.\n\t    \"\"\"\n", "    return Repo.init(_find_git_dir()).git.diff(None, cached=True)\n\tdef _commit_message(gpt4: bool = False, large: bool = False) -> str:\n\t    \"\"\"\n\t    Create a commit message with GPT.\n\t    Args:\n\t        gpt4 (bool, optional): Whether to use gpt-4. Defaults to False.\n\t        large (bool, optional): Whether to use gpt-4-32k. Defaults to False.\n\t    Returns:\n\t        response (str): The response from GPT-4.\n\t    \"\"\"\n", "    goal = \"\"\"\n\tCreate a short, single-line, git commit message for these changes\n\t\"\"\"\n\t    diff = _diff()\n\t    logging.debug(\"Diff: %s\", diff)\n\t    return _request_goal(diff, goal, fast=not gpt4, large=large)\n\tdef _push() -> str:\n\t    \"\"\"Run git push.\"\"\"\n\t    logging.debug(\"Pushing commit to remote.\")\n\t    repo = Repo.init(_find_git_dir())\n", "    return repo.git.push()\n\tdef _commit(gpt4: bool = False, large: bool = False, push: bool = False) -> Dict[str, str]:\n\t    \"\"\"Run git commit with a commit message generated by GPT.\n\t    Args:\n\t        gpt4 (bool, optional): Whether to use gpt-4. Defaults to False.\n\t        large (bool, optional): Whether to use gpt-4-32k. Defaults to False.\n\t        push (bool, optional): Whether to push the commit to the remote. Defaults to False.\n\t    Returns:\n\t        response (Dict[str, str]): The response from git commit.\n\t    \"\"\"\n", "    message = _commit_message(gpt4=gpt4, large=large)\n\t    logging.debug(\"Commit Message: %s\", message)\n\t    repo = Repo.init(_find_git_dir())\n\t    commit = repo.git.commit(message=message)\n\t    if push:\n\t        commit += f\"\\n{_push()}\"\n\t    return {\"response\": commit}\n\tclass GitCommandGroup(GPTCommandGroup):\n\t    \"\"\"Ask Command Group.\"\"\"\n\t    @staticmethod\n", "    def load_command_table(loader: CLICommandsLoader) -> None:\n\t        with CommandGroup(loader, \"git\", \"gpt_review._git#{}\", is_preview=True) as group:\n\t            group.command(\"commit\", \"_commit\", is_preview=True)\n\t    @staticmethod\n\t    def load_arguments(loader: CLICommandsLoader) -> None:\n\t        with ArgumentsContext(loader, \"git commit\") as args:\n\t            args.argument(\n\t                \"gpt4\",\n\t                help=\"Use gpt-4 for generating commit messages instead of gpt-35-turbo.\",\n\t                default=False,\n", "                action=\"store_true\",\n\t            )\n\t            args.argument(\n\t                \"large\",\n\t                help=\"Use gpt-4-32k model for generating commit messages.\",\n\t                default=False,\n\t                action=\"store_true\",\n\t            )\n\t            args.argument(\n\t                \"push\",\n", "                help=\"Push the commit to the remote.\",\n\t                default=False,\n\t                action=\"store_true\",\n\t            )\n"]}
{"filename": "src/gpt_review/_gpt_cli.py", "chunked_list": ["\"\"\"The GPT CLI configuration and utilities.\"\"\"\n\timport os\n\timport sys\n\tfrom collections import OrderedDict\n\tfrom knack import CLI, CLICommandsLoader\n\tfrom gpt_review import __version__\n\tfrom gpt_review._ask import AskCommandGroup\n\tfrom gpt_review._git import GitCommandGroup\n\tfrom gpt_review._review import ReviewCommandGroup\n\tfrom gpt_review.repositories.github import GitHubCommandGroup\n", "CLI_NAME = \"gpt\"\n\tclass GPTCLI(CLI):\n\t    \"\"\"Custom CLI implemntation to set version for the GPT CLI.\"\"\"\n\t    def get_cli_version(self) -> str:\n\t        return __version__\n\tclass GPTCommandsLoader(CLICommandsLoader):\n\t    \"\"\"The GPT CLI Commands Loader.\"\"\"\n\t    _CommandGroups = [AskCommandGroup, GitHubCommandGroup, GitCommandGroup, ReviewCommandGroup]\n\t    def load_command_table(self, args) -> OrderedDict:\n\t        for command_group in self._CommandGroups:\n", "            command_group.load_command_table(self)\n\t        return OrderedDict(self.command_table)\n\t    def load_arguments(self, command) -> None:\n\t        for argument_group in self._CommandGroups:\n\t            argument_group.load_arguments(self)\n\t        super(GPTCommandsLoader, self).load_arguments(command)\n\tdef cli() -> int:\n\t    \"\"\"The GPT CLI entry point.\"\"\"\n\t    gpt = GPTCLI(\n\t        cli_name=CLI_NAME,\n", "        config_dir=os.path.expanduser(os.path.join(\"~\", f\".{CLI_NAME}\")),\n\t        config_env_var_prefix=CLI_NAME,\n\t        commands_loader_cls=GPTCommandsLoader,\n\t    )\n\t    return gpt.invoke(sys.argv[1:])\n"]}
{"filename": "src/gpt_review/__init__.py", "chunked_list": ["#   -------------------------------------------------------------\n\t#   Copyright (c) Microsoft Corporation. All rights reserved.\n\t#   Licensed under the MIT License. See LICENSE in project root for information.\n\t#   -------------------------------------------------------------\n\t\"\"\"Easy GPT CLI\"\"\"\n\tfrom __future__ import annotations\n\t__version__ = \"0.9.5\"\n"]}
{"filename": "src/gpt_review/utils.py", "chunked_list": ["\"\"\"Utility functions\"\"\"\n\timport logging\n\timport time\n\tfrom typing import Optional\n\timport gpt_review.constants as C\n\tdef _retry_with_exponential_backoff(current_retry: int, retry_after: Optional[str]) -> None:\n\t    \"\"\"\n\t    Use exponential backoff to retry a request after specific time while staying under the retry count\n\t    Args:\n\t        current_retry (int): The current retry count.\n", "        retry_after (Optional[str]): The time to wait before retrying.\n\t    \"\"\"\n\t    logging.warning(\"Call to GPT failed due to rate limit, retry attempt %s of %s\", current_retry, C.MAX_RETRIES)\n\t    multiplication_factor = 2 * (1 + current_retry / C.MAX_RETRIES)\n\t    wait_time = int(retry_after) * multiplication_factor if retry_after else current_retry * multiplication_factor\n\t    logging.warning(\"Waiting for %s seconds before retrying.\", wait_time)\n\t    time.sleep(wait_time)\n"]}
{"filename": "src/gpt_review/constants.py", "chunked_list": ["\"\"\"Contains constants for minimum and maximum values of various parameters used in GPT Review.\"\"\"\n\timport os\n\timport sys\n\tMAX_TOKENS_DEFAULT = 100\n\tMAX_TOKENS_MIN = 1\n\tMAX_TOKENS_MAX = sys.maxsize\n\tTEMPERATURE_DEFAULT = 0.7\n\tTEMPERATURE_MIN = 0\n\tTEMPERATURE_MAX = 1\n\tTOP_P_DEFAULT = 0.5\n", "TOP_P_MIN = 0\n\tTOP_P_MAX = 1\n\tFREQUENCY_PENALTY_DEFAULT = 0.5\n\tFREQUENCY_PENALTY_MIN = 0\n\tFREQUENCY_PENALTY_MAX = 2\n\tPRESENCE_PENALTY_DEFAULT = 0\n\tPRESENCE_PENALTY_MIN = 0\n\tPRESENCE_PENALTY_MAX = 2\n\tMAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", 15))\n\tDEFAULT_RETRY_AFTER = 30\n", "AZURE_API_TYPE = \"azure\"\n\tAZURE_API_VERSION = \"2023-03-15-preview\"\n\tAZURE_CONFIG_FILE = \"azure.yaml\"\n\tAZURE_TURBO_MODEL = \"gpt-35-turbo\"\n\tAZURE_SMART_MODEL = \"gpt-4\"\n\tAZURE_LARGE_MODEL = \"gpt-4-32k\"\n\tAZURE_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n\tAZURE_KEY_VAULT = \"https://dciborow-openai.vault.azure.net/\"\n\tBUG_PROMPT_YAML = \"prompt_bug.yaml\"\n\tCOVERAGE_PROMPT_YAML = \"prompt_coverage.yaml\"\n", "SUMMARY_PROMPT_YAML = \"prompt_summary.yaml\"\n"]}
{"filename": "src/gpt_review/_review.py", "chunked_list": ["\"\"\"Basic functions for requesting review based goals from GPT-4.\"\"\"\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom typing import Dict\n\timport yaml\n\tfrom knack import CLICommandsLoader\n\tfrom knack.arguments import ArgumentsContext\n\tfrom knack.commands import CommandGroup\n\tfrom gpt_review._ask import _ask\n\tfrom gpt_review._command import GPTCommandGroup\n", "from gpt_review.prompts._prompt import (\n\t    load_bug_yaml,\n\t    load_coverage_yaml,\n\t    load_summary_yaml,\n\t)\n\t_CHECKS = {\n\t    \"SUMMARY_CHECKS\": [\n\t        {\n\t            \"flag\": \"SUMMARY_SUGGEST\",\n\t            \"header\": \"Suggestions\",\n", "            \"goal\": \"\"\"\n\t            Provide suggestions for improving the changes in this PR.\n\t            If the PR has no clear issues, mention that no suggestions are needed.\n\t            \"\"\",\n\t        },\n\t    ],\n\t    \"RISK_CHECKS\": [\n\t        {\n\t            \"flag\": \"RISK_BREAKING\",\n\t            \"header\": \"Breaking Changes\",\n", "            \"goal\": \"\"\"Detect breaking changes in a git diff. Here are some things that can cause a breaking change.\n\t- new parameters to public functions which are required and have no default value.\n\t\"\"\",\n\t        },\n\t    ],\n\t}\n\t@dataclass\n\tclass GitFile:\n\t    \"\"\"A git file with its diff contents.\"\"\"\n\t    file_name: str\n", "    diff: str\n\tdef _request_goal(git_diff, goal, fast: bool = False, large: bool = False, temperature: float = 0) -> str:\n\t    \"\"\"\n\t    Request a goal from GPT-4.\n\t    Args:\n\t        git_diff (str): The git diff to split.\n\t        goal (str): The goal to request from GPT-4.\n\t        fast (bool, optional): Whether to use the fast model. Defaults to False.\n\t        large (bool, optional): Whether to use the large model. Defaults to False.\n\t        temperature (float, optional): The temperature to use. Defaults to 0.\n", "    Returns:\n\t        response (str): The response from GPT-4.\n\t    \"\"\"\n\t    prompt = f\"\"\"\n\t{goal}\n\t{git_diff}\n\t\"\"\"\n\t    return _ask([prompt], max_tokens=1500, fast=fast, large=large, temperature=temperature)[\"response\"]\n\tdef _check_goals(git_diff, checks, indent=\"###\") -> str:\n\t    \"\"\"\n", "    Check goals.\n\t    Args:\n\t        git_diff (str): The git diff to check.\n\t        checks (list): The checks to run.\n\t    Returns:\n\t        str: The output of the checks.\n\t    \"\"\"\n\t    return \"\".join(\n\t        f\"\"\"\n\t{indent} {check[\"header\"]}\n", "{_request_goal(git_diff, goal=check[\"goal\"])}\n\t\"\"\"\n\t        for check in checks\n\t        if os.getenv(check[\"flag\"], \"true\").lower() == \"true\"\n\t    )\n\tdef _summarize_pr(git_diff) -> str:\n\t    \"\"\"\n\t    Summarize a PR.\n\t    Args:\n\t        git_diff (str): The git diff to summarize.\n", "    Returns:\n\t        str: The summary of the PR.\n\t    \"\"\"\n\t    text = \"\"\n\t    if os.getenv(\"FULL_SUMMARY\", \"true\").lower() == \"true\":\n\t        text += f\"\"\"\n\t{_request_goal(git_diff, goal=\"\")}\n\t\"\"\"\n\t        text += _check_goals(git_diff, _CHECKS[\"SUMMARY_CHECKS\"])\n\t    return text\n", "def _summarize_file(diff) -> str:\n\t    \"\"\"Summarize a file in a git diff.\n\t    Args:\n\t        diff (str): The file to summarize.\n\t    Returns:\n\t        str: The summary of the file.\n\t    \"\"\"\n\t    git_file = GitFile(diff.split(\" b/\")[0], diff)\n\t    question = load_summary_yaml().format(diff=diff)\n\t    response = _ask(question=[question], temperature=0.0)\n", "    return f\"\"\"\n\t### {git_file.file_name}\n\t{response}\n\t\"\"\"\n\tdef _split_diff(git_diff):\n\t    \"\"\"Split a git diff into a list of files and their diff contents.\n\t    Args:\n\t        git_diff (str): The git diff to split.\n\t    Returns:\n\t        list: A list of tuples containing the file name and diff contents.\n", "    \"\"\"\n\t    diff = \"diff\"\n\t    git = \"--git a/\"\n\t    return git_diff.split(f\"{diff} {git}\")[1:]  # Use formated string to prevent splitting\n\tdef _summarize_test_coverage(git_diff) -> str:\n\t    \"\"\"Summarize the test coverage of a git diff.\n\t    Args:\n\t        git_diff (str): The git diff to summarize.\n\t    Returns:\n\t        str: The summary of the test coverage.\n", "    \"\"\"\n\t    files = {}\n\t    for diff in _split_diff(git_diff):\n\t        path = diff.split(\" b/\")[0]\n\t        git_file = GitFile(path.split(\"/\")[len(path.split(\"/\")) - 1], diff)\n\t        files[git_file.file_name] = git_file\n\t    question = load_coverage_yaml().format(diff=git_diff)\n\t    return _ask([question], temperature=0.0, max_tokens=1500)[\"response\"]\n\tdef _summarize_risk(git_diff) -> str:\n\t    \"\"\"\n", "    Summarize potential risks.\n\t    Args:\n\t        git_diff (str): The git diff to split.\n\t    Returns:\n\t        response (str): The response from GPT-4.\n\t    \"\"\"\n\t    text = \"\"\n\t    if os.getenv(\"RISK_SUMMARY\", \"true\").lower() == \"true\":\n\t        text += \"\"\"\n\t## Potential Risks\n", "\"\"\"\n\t        text += _check_goals(git_diff, _CHECKS[\"RISK_CHECKS\"])\n\t    return text\n\tdef _summarize_files(git_diff) -> str:\n\t    \"\"\"Summarize git files.\"\"\"\n\t    summary = \"\"\"\n\t# Summary by GPT-4\n\t\"\"\"\n\t    summary += _summarize_pr(git_diff)\n\t    if os.getenv(\"FILE_SUMMARY\", \"true\").lower() == \"true\":\n", "        file_summary = \"\"\"\n\t## Changes\n\t\"\"\"\n\t        file_summary += \"\".join(_summarize_file(diff) for diff in _split_diff(git_diff))\n\t        if os.getenv(\"FILE_SUMMARY_FULL\", \"true\").lower() == \"true\":\n\t            summary += file_summary\n\t        summary += f\"\"\"\n\t### Summary of File Changes\n\t{_request_goal(file_summary, goal=\"Summarize the changes to the files.\")}\n\t\"\"\"\n", "    if os.getenv(\"TEST_SUMMARY\", \"true\").lower() == \"true\":\n\t        summary += f\"\"\"\n\t## Test Coverage\n\t{_summarize_test_coverage(git_diff)}\n\t\"\"\"\n\t    if os.getenv(\"BUG_SUMMARY\", \"true\").lower() == \"true\":\n\t        question = load_bug_yaml().format(diff=git_diff)\n\t        pr_bugs = _ask([question])[\"response\"]\n\t        summary += f\"\"\"\n\t## Potential Bugs\n", "{pr_bugs}\n\t\"\"\"\n\t    summary += _summarize_risk(git_diff)\n\t    return summary\n\tdef _review(diff: str = \".diff\", config: str = \"config.summary.yml\") -> Dict[str, str]:\n\t    \"\"\"Review a git diff from file\n\t    Args:\n\t        diff (str, optional): The diff to review. Defaults to \".diff\".\n\t        config (str, optional): The config to use. Defaults to \"config.summary.yml\".\n\t    Returns:\n", "        Dict[str, str]: The response from GPT-4.\n\t    \"\"\"\n\t    # If config is a file, use it\n\t    with open(diff, \"r\", encoding=\"utf8\") as file:\n\t        diff_contents = file.read()\n\t        if os.path.isfile(config):\n\t            summary = _process_yaml(git_diff=diff_contents, yaml_file=config)\n\t        else:\n\t            summary = _summarize_files(diff_contents)\n\t        return {\"response\": summary}\n", "def _process_yaml(git_diff, yaml_file, headers=True) -> str:\n\t    \"\"\"\n\t    Process a yaml file.\n\t    Args:\n\t        git_diff (str): The diff of the PR.\n\t        yaml_file (str): The path to the yaml file.\n\t        headers (bool, optional): Whether to include headers. Defaults to True.\n\t    Returns:\n\t        str: The report.\n\t    \"\"\"\n", "    with open(yaml_file, \"r\", encoding=\"utf8\") as file:\n\t        yaml_contents = file.read()\n\t        config = yaml.safe_load(yaml_contents)\n\t        report = config[\"report\"]\n\t        return _process_report(git_diff, report, headers=headers)\n\tdef _process_report(git_diff, report: dict, indent=\"#\", headers=True) -> str:\n\t    \"\"\"\n\t    for-each record in report\n\t    - if record is a string, check_goals\n\t    - else recursively call process_report\n", "    Args:\n\t        git_diff (str): The diff of the PR.\n\t        report (dict): The report to process.\n\t        indent (str, optional): The indent to use. Defaults to \"#\".\n\t        headers (bool, optional): Whether to include headers. Defaults to True.\n\t    Returns:\n\t        str: The report.\n\t    \"\"\"\n\t    text = \"\"\n\t    for key, record in report.items():\n", "        if isinstance(record, str) or record is None:\n\t            if headers and key != \"_\":\n\t                text += f\"\"\"\n\t{indent} {key}\n\t\"\"\"\n\t            text += f\"{_request_goal(git_diff, goal=record)}\"\n\t        else:\n\t            text += f\"\"\"\n\t{indent} {key}\n\t\"\"\"\n", "            text += _process_report(git_diff, record, indent=f\"{indent}#\", headers=headers)\n\t    return text\n\tclass ReviewCommandGroup(GPTCommandGroup):\n\t    \"\"\"Review Command Group.\"\"\"\n\t    @staticmethod\n\t    def load_command_table(loader: CLICommandsLoader) -> None:\n\t        with CommandGroup(loader, \"review\", \"gpt_review._review#{}\", is_preview=True) as group:\n\t            group.command(\"diff\", \"_review\", is_preview=True)\n\t    @staticmethod\n\t    def load_arguments(loader: CLICommandsLoader) -> None:\n", "        \"\"\"Add patch_repo, patch_pr, and access_token arguments.\"\"\"\n\t        with ArgumentsContext(loader, \"github\") as args:\n\t            args.argument(\n\t                \"diff\",\n\t                type=str,\n\t                help=\"Git diff to review.\",\n\t                default=\".diff\",\n\t            )\n\t            args.argument(\n\t                \"config\",\n", "                type=str,\n\t                help=\"The config file to use to customize review summary.\",\n\t                default=\"config.template.yml\",\n\t            )\n"]}
{"filename": "src/gpt_review/_llama_index.py", "chunked_list": ["\"\"\"Wrapper for Llama Index.\"\"\"\n\timport logging\n\timport os\n\tfrom typing import List, Optional\n\timport openai\n\tfrom langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n\tfrom langchain.embeddings import OpenAIEmbeddings\n\tfrom langchain.llms import AzureOpenAI\n\tfrom llama_index import (\n\t    Document,\n", "    GithubRepositoryReader,\n\t    GPTVectorStoreIndex,\n\t    LangchainEmbedding,\n\t    LLMPredictor,\n\t    ServiceContext,\n\t    SimpleDirectoryReader,\n\t    StorageContext,\n\t    load_index_from_storage,\n\t)\n\tfrom llama_index.indices.base import BaseGPTIndex\n", "from llama_index.storage.storage_context import DEFAULT_PERSIST_DIR\n\tfrom typing_extensions import override\n\timport gpt_review.constants as C\n\tfrom gpt_review.context import _load_azure_openai_context\n\tlogger = logging.getLogger(__name__)\n\tdef _query_index(\n\t    question: str,\n\t    files: Optional[List[str]] = None,\n\t    input_dir: Optional[str] = None,\n\t    exclude_hidden: bool = True,\n", "    recursive: bool = True,\n\t    required_exts: Optional[List[str]] = None,\n\t    repository: Optional[str] = None,\n\t    branch: str = \"main\",\n\t    fast: bool = False,\n\t    large: bool = False,\n\t    reset: bool = False,\n\t) -> str:\n\t    \"\"\"\n\t    Query a Vector Index with GPT.\n", "    Args:\n\t        question (List[str]): The question to ask.\n\t        files (List[str], optional): The files to search.\n\t            (Optional; overrides input_dir, exclude)\n\t        input_dir (str, optional): Path to the directory.\n\t        exclude_hidden (bool): Whether to exclude hidden files.\n\t        recursive (bool): Whether to search directory recursively.\n\t        required_exts (List, optional): The required extensions for files in directory.\n\t        repository (str): The repository to search. Format: owner/repo\n\t        fast (bool, optional): Whether to use the fast model. Defaults to False.\n", "        large (bool, optional): Whether to use the large model. Defaults to False.\n\t        reset (bool, optional): Whether to reset the index. Defaults to False.\n\t    Returns:\n\t        Dict[str, str]: The response.\n\t    \"\"\"\n\t    documents = []\n\t    if files:\n\t        documents += SimpleDirectoryReader(input_files=files).load_data()\n\t    elif input_dir:\n\t        documents += SimpleDirectoryReader(\n", "            input_dir=input_dir, exclude_hidden=exclude_hidden, recursive=recursive, required_exts=required_exts\n\t        ).load_data()\n\t    if repository:\n\t        owner, repo = repository.split(\"/\")\n\t        documents += GithubRepositoryReader(owner=owner, repo=repo, use_parser=False).load_data(branch=branch)\n\t    index = _load_index(documents, fast=fast, large=large, reset=reset)\n\t    return index.as_query_engine().query(question).response  # type: ignore\n\tdef _load_index(\n\t    documents: List[Document],\n\t    fast: bool = True,\n", "    large: bool = True,\n\t    reset: bool = False,\n\t    persist_dir: str = DEFAULT_PERSIST_DIR,\n\t) -> BaseGPTIndex:\n\t    \"\"\"\n\t    Load or create a document indexer.\n\t    Args:\n\t        documents (List[Document]): The documents to index.\n\t        fast (bool, optional): Whether to use the fast model. Defaults to False.\n\t        large (bool, optional): Whether to use the large model. Defaults to False.\n", "        reset (bool, optional): Whether to reset the index. Defaults to False.\n\t        persist_dir (str, optional): The directory to persist the index to. Defaults to './storage'.\n\t    Returns:\n\t        BaseGPTIndex: The document indexer.\n\t    \"\"\"\n\t    service_context = _load_service_context(fast, large)\n\t    if os.path.isdir(f\"{persist_dir}\") and not reset:\n\t        logger.info(\"Loading index from storage\")\n\t        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n\t        return load_index_from_storage(service_context=service_context, storage_context=storage_context)\n", "    logger.info(\"Creating index\")\n\t    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\t    logger.info(\"Saving index to storage\")\n\t    index.storage_context.persist(persist_dir=persist_dir)\n\t    return index\n\tdef _load_service_context(fast: bool = False, large: bool = False) -> ServiceContext:\n\t    \"\"\"\n\t    Load the service context.\n\t    Args:\n\t        fast (bool, optional): Whether to use the fast model. Defaults to False.\n", "        large (bool, optional): Whether to use the large model. Defaults to False.\n\t    Returns:\n\t        ServiceContext: The service context.\n\t    \"\"\"\n\t    context = _load_azure_openai_context()\n\t    model_name = (\n\t        context.turbo_llm_model_deployment_id\n\t        if fast\n\t        else context.large_llm_model_deployment_id\n\t        if large\n", "        else context.smart_llm_model_deployment_id\n\t    )\n\t    if openai.api_type == C.AZURE_API_TYPE:\n\t        llm_type = AzureGPT35Turbo if fast else AzureChatOpenAI\n\t        llm = llm_type(  # type: ignore\n\t            deployment_name=model_name,\n\t            model_kwargs={\n\t                \"api_key\": openai.api_key,\n\t                \"api_base\": openai.api_base,\n\t                \"api_type\": openai.api_type,\n", "                \"api_version\": openai.api_version,\n\t            },\n\t            max_retries=C.MAX_RETRIES,\n\t        )\n\t    else:\n\t        llm = ChatOpenAI(\n\t            model_name=model_name,\n\t            model_kwargs={\n\t                \"api_key\": openai.api_key,\n\t                \"api_base\": openai.api_base,\n", "                \"api_type\": openai.api_type,\n\t                \"api_version\": openai.api_version,\n\t            },\n\t            max_retries=C.MAX_RETRIES,\n\t        )\n\t    llm_predictor = LLMPredictor(llm=llm)\n\t    embedding_llm = LangchainEmbedding(\n\t        OpenAIEmbeddings(\n\t            model=\"text-embedding-ada-002\",\n\t        ),  # type: ignore\n", "        embed_batch_size=1,\n\t    )\n\t    return ServiceContext.from_defaults(\n\t        llm_predictor=llm_predictor,\n\t        embed_model=embedding_llm,\n\t    )\n\tclass AzureGPT35Turbo(AzureOpenAI):\n\t    \"\"\"Azure OpenAI Chat API.\"\"\"\n\t    @property\n\t    @override\n", "    def _default_params(self):\n\t        \"\"\"\n\t        Get the default parameters for calling OpenAI API.\n\t        gpt-35-turbo does not support best_of, logprobs, or echo.\n\t        \"\"\"\n\t        normal_params = {\n\t            \"temperature\": self.temperature,\n\t            \"max_tokens\": self.max_tokens,\n\t            \"top_p\": self.top_p,\n\t            \"frequency_penalty\": self.frequency_penalty,\n", "            \"presence_penalty\": self.presence_penalty,\n\t            \"n\": self.n,\n\t            \"request_timeout\": self.request_timeout,\n\t            \"logit_bias\": self.logit_bias,\n\t        }\n\t        return {**normal_params, **self.model_kwargs}\n"]}
{"filename": "src/gpt_review/prompts/_prompt.py", "chunked_list": ["\"\"\"Interface for a GPT Prompts.\"\"\"\n\timport os\n\timport sys\n\tfrom dataclasses import dataclass\n\tfrom pathlib import Path\n\tfrom langchain.prompts import PromptTemplate, load_prompt\n\timport gpt_review.constants as C\n\tif sys.version_info[:2] <= (3, 10):\n\t    from typing_extensions import Self\n\telse:\n", "    from typing import Self\n\t@dataclass\n\tclass LangChainPrompt(PromptTemplate):\n\t    \"\"\"A prompt for the GPT LangChain task.\"\"\"\n\t    prompt_yaml: str\n\t    @classmethod\n\t    def load(cls, prompt_yaml) -> Self:\n\t        \"\"\"Load the prompt.\"\"\"\n\t        return load_prompt(prompt_yaml)\n\tdef load_bug_yaml() -> LangChainPrompt:\n", "    \"\"\"Load the bug yaml.\"\"\"\n\t    yaml_path = os.getenv(\"PROMPT_BUG\", str(Path(__file__).parents[0].joinpath(C.BUG_PROMPT_YAML)))\n\t    return LangChainPrompt.load(yaml_path)\n\tdef load_coverage_yaml() -> LangChainPrompt:\n\t    \"\"\"Load the coverage yaml.\"\"\"\n\t    yaml_path = os.getenv(\"PROMPT_COVERAGE\", str(Path(__file__).parents[0].joinpath(C.COVERAGE_PROMPT_YAML)))\n\t    return LangChainPrompt.load(yaml_path)\n\tdef load_summary_yaml() -> LangChainPrompt:\n\t    \"\"\"Load the summary yaml.\"\"\"\n\t    yaml_path = os.getenv(\"PROMPT_SUMMARY\", str(Path(__file__).parents[0].joinpath(C.SUMMARY_PROMPT_YAML)))\n", "    return LangChainPrompt.load(yaml_path)\n"]}
{"filename": "src/gpt_review/prompts/__init__.py", "chunked_list": ["\"\"\"Collection of GPT Prompts.\"\"\"\n"]}
{"filename": "src/gpt_review/repositories/__init__.py", "chunked_list": []}
{"filename": "src/gpt_review/repositories/github.py", "chunked_list": ["\"\"\"GitHub API helpers.\"\"\"\n\timport json\n\timport logging\n\timport os\n\tfrom typing import Dict\n\timport requests\n\tfrom knack import CLICommandsLoader\n\tfrom knack.arguments import ArgumentsContext\n\tfrom knack.commands import CommandGroup\n\tfrom gpt_review._command import GPTCommandGroup\n", "from gpt_review._review import _summarize_files\n\tfrom gpt_review.repositories._repository import _RepositoryClient\n\tclass GitHubClient(_RepositoryClient):\n\t    \"\"\"GitHub client.\"\"\"\n\t    @staticmethod\n\t    def get_pr_diff(patch_repo=None, patch_pr=None, access_token=None) -> str:\n\t        \"\"\"\n\t        Get the diff of a PR.\n\t        Args:\n\t            patch_repo (str): The repo.\n", "            patch_pr (str): The PR.\n\t            access_token (str): The GitHub access token.\n\t        Returns:\n\t            str: The diff of the PR.\n\t        \"\"\"\n\t        patch_repo = patch_repo or os.getenv(\"PATCH_REPO\")\n\t        patch_pr = patch_pr or os.getenv(\"PATCH_PR\")\n\t        access_token = access_token or os.getenv(\"GITHUB_TOKEN\")\n\t        headers = {\n\t            \"Accept\": \"application/vnd.github.v3.diff\",\n", "            \"authorization\": f\"Bearer {access_token}\",\n\t        }\n\t        response = requests.get(\n\t            f\"https://api.github.com/repos/{patch_repo}/pulls/{patch_pr}\", headers=headers, timeout=10\n\t        )\n\t        return response.text\n\t    @staticmethod\n\t    def _post_pr_comment(review, git_commit_hash: str, link: str, access_token: str) -> requests.Response:\n\t        \"\"\"\n\t        Post a comment to a PR.\n", "        Args:\n\t            review (str): The review.\n\t            git_commit_hash (str): The git commit hash.\n\t            link (str): The link to the PR.\n\t            access_token (str): The GitHub access token.\n\t        Returns:\n\t            requests.Response: The response.\n\t        \"\"\"\n\t        data = {\"body\": review, \"commit_id\": git_commit_hash, \"event\": \"COMMENT\"}\n\t        data = json.dumps(data)\n", "        owner = link.split(\"/\")[-4]\n\t        repo = link.split(\"/\")[-3]\n\t        pr_number = link.split(\"/\")[-1]\n\t        headers = {\n\t            \"Accept\": \"application/vnd.github+json\",\n\t            \"authorization\": f\"Bearer {access_token}\",\n\t        }\n\t        response = requests.get(\n\t            f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\", headers=headers, timeout=10\n\t        )\n", "        comments = response.json()\n\t        for comment in comments:\n\t            if (\n\t                \"user\" in comment\n\t                and comment[\"user\"][\"login\"] == \"github-actions[bot]\"\n\t                and \"body\" in comment\n\t                and \"Summary by GPT-4\" in comment[\"body\"]\n\t            ):\n\t                review_id = comment[\"id\"]\n\t                data = {\"body\": review}\n", "                data = json.dumps(data)\n\t                response = requests.put(\n\t                    f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews/{review_id}\",\n\t                    headers=headers,\n\t                    data=data,\n\t                    timeout=10,\n\t                )\n\t                break\n\t        else:\n\t            # https://api.github.com/repos/OWNER/REPO/pulls/PULL_NUMBER/reviews\n", "            response = requests.post(\n\t                f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\",\n\t                headers=headers,\n\t                data=data,\n\t                timeout=10,\n\t            )\n\t        logging.debug(response.json())\n\t        return response\n\t    @staticmethod\n\t    def post_pr_summary(diff) -> Dict[str, str]:\n", "        \"\"\"\n\t        Get a review of a PR.\n\t        Requires the following environment variables:\n\t            - LINK: The link to the PR.\n\t            - GIT_COMMIT_HASH: The git commit hash.\n\t            - GITHUB_TOKEN: The GitHub access token.\n\t        Args:\n\t            diff (str): The patch of the PR.\n\t        Returns:\n\t            Dict[str, str]: The review.\n", "        \"\"\"\n\t        review = _summarize_files(diff)\n\t        logging.debug(review)\n\t        link = os.getenv(\"LINK\")\n\t        git_commit_hash = os.getenv(\"GIT_COMMIT_HASH\")\n\t        access_token = os.getenv(\"GITHUB_TOKEN\")\n\t        if link and git_commit_hash and access_token:\n\t            GitHubClient._post_pr_comment(\n\t                review=review, git_commit_hash=git_commit_hash, link=link, access_token=access_token\n\t            )\n", "            return {\"response\": \"PR posted\"}\n\t        logging.warning(\"No PR to post too\")\n\t        return {\"response\": \"No PR to post too\"}\n\tdef _review(repository=None, pull_request=None, access_token=None) -> Dict[str, str]:\n\t    \"\"\"Review GitHub PR with Open AI, and post response as a comment.\n\t    Args:\n\t        repository (str): The repo of the PR.\n\t        pull_request (str): The PR number.\n\t        access_token (str): The GitHub access token.\n\t    Returns:\n", "        Dict[str, str]: The response.\n\t    \"\"\"\n\t    diff = GitHubClient.get_pr_diff(repository, pull_request, access_token)\n\t    GitHubClient.post_pr_summary(diff)\n\t    return {\"response\": \"Review posted as a comment.\"}\n\tdef _comment(question: str, comment_id: int, diff: str = \".diff\", link=None, access_token=None) -> Dict[str, str]:\n\t    \"\"\"\"\"\"\n\t    raise NotImplementedError\n\tclass GitHubCommandGroup(GPTCommandGroup):\n\t    \"\"\"Ask Command Group.\"\"\"\n", "    @staticmethod\n\t    def load_command_table(loader: CLICommandsLoader) -> None:\n\t        with CommandGroup(loader, \"github\", \"gpt_review.repositories.github#{}\", is_preview=True) as group:\n\t            group.command(\"review\", \"_review\", is_preview=True)\n\t    @staticmethod\n\t    def load_arguments(loader: CLICommandsLoader) -> None:\n\t        \"\"\"Add patch_repo, patch_pr, and access_token arguments.\"\"\"\n\t        with ArgumentsContext(loader, \"github\") as args:\n\t            args.argument(\n\t                \"access_token\",\n", "                type=str,\n\t                help=\"The GitHub access token. Set or use GITHUB_TOKEN environment variable.\",\n\t                default=None,\n\t            )\n\t            args.argument(\n\t                \"pull_request\",\n\t                type=str,\n\t                help=\"The PR number. Set or use PATCH_PR environment variable.\",\n\t                default=None,\n\t                options_list=(\"--pull-request\", \"-pr\"),\n", "            )\n\t            args.argument(\n\t                \"repository\",\n\t                type=str,\n\t                help=\"The repo of the PR. Set or use PATCH_REPO environment variable.\",\n\t                default=None,\n\t                options_list=(\"--repository\", \"-r\"),\n\t            )\n"]}
{"filename": "src/gpt_review/repositories/_repository.py", "chunked_list": ["\"\"\"Abstract class for a repository client.\"\"\"\n\tfrom abc import abstractmethod\n\tclass _RepositoryClient:\n\t    \"\"\"Abstract class for a repository client.\"\"\"\n\t    @staticmethod\n\t    @abstractmethod\n\t    def get_pr_diff(patch_repo=None, patch_pr=None, access_token=None) -> str:\n\t        \"\"\"\n\t        Get the diff of a PR.\n\t        Args:\n", "            patch_repo (str): The repo.\n\t            patch_pr (str): The PR.\n\t            access_token (str): The GitHub access token.\n\t        Returns:\n\t            str: The diff of the PR.\n\t        \"\"\"\n\t    @staticmethod\n\t    @abstractmethod\n\t    def post_pr_summary(diff) -> None:\n\t        \"\"\"\n", "        Post a summary to a PR.\n\t        Args:\n\t            diff (str): The diff of the PR.\n\t        Returns:\n\t            str: The review of the PR.\n\t        \"\"\"\n"]}
{"filename": "src/gpt/__main__.py", "chunked_list": ["\"\"\"The GPT CLI entry point for python -m gpt\"\"\"\n\timport sys\n\tfrom gpt_review._gpt_cli import cli\n\tif __name__ == \"__main__\":\n\t    exit_code = cli()\n\t    sys.exit(exit_code)\n"]}
{"filename": "src/gpt/__init__.py", "chunked_list": []}
